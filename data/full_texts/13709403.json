{"id": 13709403, "updated": "2023-09-28 16:56:37.758", "metadata": {"title": "A Survey of Recent Advances in CNN-based Single Image Crowd Counting and Density Estimation", "authors": "[{\"first\":\"Vishwanath\",\"last\":\"Sindagi\",\"middle\":[\"A.\"]},{\"first\":\"Vishal\",\"last\":\"Patel\",\"middle\":[\"M.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 7, "day": 5}, "abstract": "Estimating count and density maps from crowd images has a wide range of applications such as video surveillance, traffic monitoring, public safety and urban planning. In addition, techniques developed for crowd counting can be applied to related tasks in other fields of study such as cell microscopy, vehicle counting and environmental survey. The task of crowd counting and density map estimation is riddled with many challenges such as occlusions, non-uniform density, intra-scene and inter-scene variations in scale and perspective. Nevertheless, over the last few years, crowd count analysis has evolved from earlier methods that are often limited to small variations in crowd density and scales to the current state-of-the-art methods that have developed the ability to perform successfully on a wide range of scenarios. The success of crowd counting methods in the recent years can be largely attributed to deep learning and publications of challenging datasets. In this paper, we provide a comprehensive survey of recent Convolutional Neural Network (CNN) based approaches that have demonstrated significant improvements over earlier methods that rely largely on hand-crafted representations. First, we briefly review the pioneering methods that use hand-crafted representations and then we delve in detail into the deep learning-based approaches and recently published datasets. Furthermore, we discuss the merits and drawbacks of existing CNN-based approaches and identify promising avenues of research in this rapidly evolving field.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1707.01202", "mag": "2729018917", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/prl/SindagiP18", "doi": "10.1016/j.patrec.2017.07.007"}}, "content": {"source": {"pdf_hash": "542a2ddc53d80d58a8791ab1a72dad660035e114", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1707.01202v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.sciencedirect.com/science/article/am/pii/S0167865517302398", "status": "BRONZE"}}, "grobid": {"id": "fc598b47752aba6358d88d1fa5afb3efa7b2bf31", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/542a2ddc53d80d58a8791ab1a72dad660035e114.txt", "contents": "\nPattern Recognition Letters A Survey of Recent Advances in CNN-based Single Image Crowd Counting and Density Estimation\n\n\nVishwanath A Sindagi \nDept. of Electrical and Computer Engineering\n94 Brett Road08854PiscatawayNJUSA\n\nVishal M Patel \nDept. of Electrical and Computer Engineering\n94 Brett Road08854PiscatawayNJUSA\n\nPattern Recognition Letters A Survey of Recent Advances in CNN-based Single Image Crowd Counting and Density Estimation\n1 journal homepage: www.elsevier.com\nEstimating count and density maps from crowd images has a wide range of applications such as video surveillance, traffic monitoring, public safety and urban planning. In addition, techniques developed for crowd counting can be applied to related tasks in other fields of study such as cell microscopy, vehicle counting and environmental survey. The task of crowd counting and density map estimation is riddled with many challenges such as occlusions, non-uniform density, intra-scene and inter-scene variations in scale and perspective. Nevertheless, over the last few years, crowd count analysis has evolved from earlier methods that are often limited to small variations in crowd density and scales to the current state-of-the-art methods that have developed the ability to perform successfully on a wide range of scenarios. The success of crowd counting methods in the recent years can be largely attributed to deep learning and publications of challenging datasets. In this paper, we provide a comprehensive survey of recent Convolutional Neural Network (CNN) based approaches that have demonstrated significant improvements over earlier methods that rely largely on hand-crafted representations. First, we briefly review the pioneering methods that use hand-crafted representations and then we delve in detail into the deep learning-based approaches and recently published datasets. Furthermore, we discuss the merits and drawbacks of existing CNN-based approaches and identify promising avenues of research in this rapidly evolving field. (Vishwanath A. Sindagi)    as sporting events, political rallies, public demonstrations etc. (shown inFig. 2), thereby resulting in more frequent crowd gatherings in the recent years. In such scenarios, it is essential to analyze crowd behavior for better management, safety and security.Like any other computer vision problem, crowd analysis comes with many challenges such as occlusions, high clutter, non-uniform distribution of people, non-uniform illumination, intra-scene and inter-scene variations in appearance, scale and perspective making the problem extremely difficult. Some of these challenges are illustrated inFig. 2. The complexity of the problem together with the wide range of applications for crowd analysis has led to an increased focus by researchers in the recent past.Crowd analysis is an inherently inter-disciplinary research topic with researchers from different communities (such as sociology [68, 10], psychology[5], physics[13,38], biology[72,110], computer vision and public safety) have addressed\n\nIntroduction\n\nCrowd counting aims to count the number of people in a crowded scene where as density estimation aims to map an input crowd image to it's corresponding density map which indicates the number of people per pixel present in the image (as illustrated in Fig. 1) and the two problems have been jointly addressed by researchers. The problem of crowd counting and density estimation is of paramount importance and it is essential for building higher level cognitive abilities in crowded scenarios such as crowd monitoring [15] and scene understanding [87,115]. Crowd analysis has attracted significant attention from researchers in the recent past due to a variety of reasons. Exponential growth in the world population and the resulting urbanization has led to an increased number of activities such the issue from different viewpoints. Crowd analysis has a variety of critical applications of inter-disciplinarian nature: Safety monitoring: The widespread usage of video surveillance cameras for security and safety purposes in places such as sports stadiums, tourist spots, shopping malls and airports has enabled easier monitoring of crowd in such scenarios. However, traditional surveillance algorithms may break down as they are unable to process high density crowds due to limitations in their design. In such scenarios, we can leverage the results of algorithms specially designed for crowd analysis related tasks such as behavior analysis [83,48], congestion analysis [114,40], anomaly detection [56,14] and event detection [8]. Disaster management: Many scenarios involving crowd gatherings such as sports events, music concerts, public demonstrations and political rallies face the risk of crowd related disasters such as stampedes which can be life threatening. In such cases, crowd analysis can be used as an effective tool for early overcrowding detection and appropriate management of crowd, hence, eventual aversion of any disaster [1,3]. Design of public spaces: Crowd analysis on existing public spots such as airport terminals, train stations, shopping malls and other public buildings [23,90] can reveal important design shortcomings from crowd safety and convenience point of view. These studies can be used for design of public spaces that are optimized for better safety and crowd movement [62,2]. Intelligence gathering and analysis: Crowd counting techniques can be used to gather intelligence for further analysis and inference. For instance, in retail sector, crowd counting can be used to gauge people's interest in a product in a store and this information can be used for appropriate product placement [58,67]. Similarly, crowd counting can be used to measure queue lengths to optimize staff numbers at different times of the day. Furthermore, crowd counting can be used to analyze pedestrian flow at signals at different times of the day and this information can be used for optimizing signal-wait times [9]. Virtual environments: Crowd analysis methods can be used to understand the underlying phenomenon thereby enabling us to establish mathematical models that can provide accurate simulations. These mathematical models can be further used for simulation of crowd phenomena for various applications such as computer games, inserting visual effects in film scenes and designing evacuation plans [36,74]. pects and victims in events such as bombing, shooting or accidents in large gatherings. Traditional face detection and recognition algorithms can be speeded up using crowd analysis techniques which are more adept at handling such scenarios [47,7].\n\nThese variety of applications has motivated researchers across various fields to develop sophisticated methods for crowd analysis and related tasks such as counting [15,16,20,41,17,85,35,41], density estimation [52,19,111,107,75,99,11], segmentation [46,27], behaviour analysis [6,86,22,115,114,103], tracking [77,116], scene understanding [87,115] and anomaly detection [63,56]. Among these, crowd counting and density estimation are a set of fundamental tasks and they form basic building blocks for various other applications discussed earlier. Additionally, methods developed for crowd counting can be easily extended to counting tasks in other fields such as cell microscopy [99,97,52,20], vehicle counting [70], environmental survey [31,105], etc.\n\nOver the last few years, researchers have attempted to address the issue of crowd counting and density estimation using a variety of approaches such as detection-based counting, clustering-based counting and regression-based counting [61]. The initial work on regression-based methods mainly use handcrafted features and the more recent works use Convolutional Neural Network (CNN) based approaches. The CNN-based approaches have demonstrated significant improvements over previous hand-crafted feature-based methods, thus, motivating more researchers to explore CNN-based approaches further for related crowd analysis problems. In this paper, we review various single image crowd counting and density estimation methods with a specific focus on recent CNN-based approaches.\n\nResearchers have attempted to provide a comprehensive survey and evaluation of existing techniques for various aspects of crowd analysis [105,30,44,55,117]. Zhan et al. [105] and Junior et al. [44] were among the first ones to study and review existing methods for general crowd analysis. Li et al. [55] surveyed different methods for crowded scene analysis tasks such as crowd motion pattern learning, crowd behavior, activity analysis and anomaly detection in crowds. More recently, Zitouni et al. [117] evaluated existing methods across different research disciplines by inferring key statistical evidence from existing literature and provided suggestions towards the general aspects of techniques rather than any specific algorithm. While these works focussed on the general aspects of crowd analysis, researchers have studied in detail crowd counting and density estimation methods specifically [61,81,79]. Loy et al. [61] provided a detailed description and comparison of video imagerybased crowd counting and evaluation of different methods using the same protocol. They also analyzed each processing module to identify potential bottlenecks to provide new directions for further research. In another work, Ryan et al. [79] presented an evaluation of regression-based methods for crowd counting across multiple datasets and provided a detailed analysis of performance of various hand-crafted features. Recently, Saleh et al. [81] surveyed two main approaches which are direct approach (i.e., object based target detection) and indirect approach (e.g. pixel-based, texture-based, and corner points based analysis).\n\nThough existing surveys analyze various methods for crowd analysis and counting, they however cover only traditional methods that use hand-crafted features and do not take into account the recent advancements driven primarily by CNN-based approaches [87,39,113,11,85,97,4,98,111,107,70,88] and creation of new challenging crowd datasets [106,107,111]. While CNN-based approaches have achieved drastically lower error rates, the creation of new datasets has enabled learning of more generalized models. To keep up with the rapidly advancing research in crowd counting, we believe it is necessary to analyze these methods in detail in order to understand the trends. Hence, in this paper, we provide a survey of recent state-ofthe-art CNN-based approaches for crowd counting and density estimation for single images.\n\nRest of the paper is organized as follows: Section 2 briefly reviews the traditional crowd counting and density estimation approaches with an emphasis on the most recent methods. This is followed by a detailed survey on CNN-based methods along with a discussion on their merits and drawbacks in Section 3. In Section 5, recently published challenging datasets for crowd counting are discussed in detail along with results of the stateof-the-art methods. We discuss several promising avenues for achieving further progress in Section 6. Finally, concluding remarks are made in Section 7.\n\n\nReview of traditional approaches\n\nVarious approaches have been proposed to tackle the problem of crowd counting in images [41,19,52,107,111] and videos [12,35,77,21]. Loy et al. [61] broadly classified traditional crowd counting methods based on the approach into the following categories: (1) Detection-based approaches,\n\n(2) Regression-based approaches, and (3) Density estimationbased approaches.\n\nSince the focus of this work is on CNN-based approaches, in this section, we briefly review the detection and regressionbased approaches using hand-crafted features for the sake of completeness. In addition, we present a review of the recent traditional methods [41,52,75,99,102] that have not been analyzed in earlier surveys.\n\n\nDetection-based approaches\n\nMost of the initial research was focussed on detection style framework, where a sliding window detector is used to detect people in the scene [26] and this information is used to count the number of people [54]. Detection is usually performed either in the monolithic style or parts-based detection. Monolithic detection approaches [25,51,94,28] typically are traditional pedestrian detection methods which train a classifier using features (such as Haar wavelets [95], histogram oriented gradients [25], edgelet [100] and shapelet [80]) extracted from a full body. Various learning approaches such as Support Vector Machines, boosting [96] and random forest [34] have been used with varying degree of success. Though successful in low density crowd scenes, these methods are adversely affected by the presence of high density crowds. Researchers have attempted to address this issue by adopting part-based detection methods [29,57,101], where one constructs boosted classifiers for specific body parts such as the head and shoulder to estimate the people counts in a designated area [54]. In another approach using shape learning, Zhao et al. [112] modelled humans using 3D shapes composed of ellipsoids, and employed a stochastic process to estimate the number and shape configuration that best explains a given foreground mask in a scene. Ge and Collins [35] further extended the idea by using flexible and practical shape models.\n\n\nRegression-based approaches\n\nThough parts-based and shape-based detectors were used to mitigate the issues of occlusion, these methods were not successful in the presence of extremely dense crowds and high background clutter. To overcome these issues, researchers attempted to count by regression where they learn a mapping between features extracted from local image patches to their counts [16,78,20]. By counting using regression, these methods avoid dependency on learning detectors which is a relatively complex task. These methods have two major components: low-level feature extraction and regression modelling. A variety of features such as foreground features, edge features, texture and gradient features have been used for encoding low-level information. Foreground features are extracted from foreground segments in a video using standard background subtraction techniques. Blob-based holistic features such as area, perimeter, perimeter-area ration, etc. have demonstrated encouraging results [15,20,78]. While these methods capture global properties of the scene, local features such as edges and texture/gradient features such as local binary pattern (LBP), histogram oriented gradients (HOG), gray level co-occurrence matrices (GLCM) have been used to further improve the results.\n\nOnce these global and local features are extracted, different regression techniques such as linear regression [71], piecewise linear regression [15], ridge regression [20], Gaussian process regression and neural network [64] are used to learn a mapping from low-level feature to the crowd count.\n\nIn a recent approach, Idrees et al. [41] identified that no single feature or detection method is reliable enough to provide sufficient information for accurate counting in the presence of high density crowds due to various reasons such as low resolution, severe occlusion, foreshortening and perspective. Additionally, they observed that there exists a spatial relationship that can be used to constrain the count estimates in neighboring local regions. With these observations in mind, they proposed to extract features using different methods that capture different information. By treating densely packed crowds of individuals as irregular and non-homogeneous texture, they employed Fourier analysis along with head detections and SIFT interestpoint based counting in local neighborhoods. The count estimates from this localized multi-scale analysis are then aggregated subject to global consistency constraints. The three sources, i.e., Fourier, interest points and head detection are then combined with their respective confidences and counts at localized patches are computed independently. These local counts are then globally constrained in a multi-scale Markov Random Field (MRF) framework to get an estimate of count for the entire image. The authors also introduced an annotated dataset (UCF CC 50) of 50 images containing 64000 humans.\n\nChen et al. [19] introduced a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available. Considering that the challenges of inconsistent features along with sparse and imbalanced (encountered during learning a regression function) are related, cumulative attribute-based representation for learning a regression model is proposed. Specifically, features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space. The method is based on the notion of discriminative attributes used for addressing sparse training data. This method is inherently capable of handling imbalanced data.\n\n\nDensity estimation-based approaches\n\nWhile the earlier methods were successful in addressing the issues of occlusion and clutter, most of them ignored important spatial information as they were regressing on the global count. In contrast, Lempitsky et al. [52] proposed to learn a linear mapping between local patch features and corresponding object density maps, thereby incorporating spatial information in the learning process. In doing so, they avoided the hard task of learning to detect and localize individual object instances by introducing a new approach of estimating image density whose integral over any region in the density map gives the count of objects within that region. The problem of learning density maps is formulated as a minimization of a regularized risk quadratic cost function. A new loss function appropriate for learning density maps is introduced. The entire problem is posed as a convex optimization task which they solve using cutting-plane optimization.\n\nObserving that it is difficult to learn a linear mapping, Pham et al. [75] proposed to learn a non-linear mapping between lo-cal patch features and density maps. They used random forest regression from multiple image patches to vote for densities of multiple target objects to learn a non-linear mapping. In addition, they tackled the problem of large variation in appearance and shape between crowded image patches and non-crowded ones by proposing a crowdedness prior and they trained two different forests corresponding to this prior. Furthermore, they were able to successfully speed up the estimation process for real-time performance by proposing an effective forest reduction that uses permutation of decision trees. Apart from achieving real-time performance, another advantage of their method is that it requires relatively less memory to build and store the forest.\n\nSimilar to the above approach, Wang and Zou [99] identified that though existing methods are effective, they were inefficient from computational complexity point of view. To this effect, they proposed a fast method for density estimation based on subspace learning. Instead of learning a mapping between dense features and their corresponding density maps, they learned to compute the embedding of each subspace formed by image patches. Essentially, they exploited the relationship between images and their corresponding density maps in the respective feature spaces. The feature space of image patches are clustered and examples of each subspace are collected to learn its embedding. Their assumption that local image patches and their corresponding density maps share similar local geometry enables them to learn locally linear embedding using which the density map of an image patch can be estimated by preserving the geometry. Since, implementing locally linear embedding (LLE) is time-consuming, they divided the feature spaces of image patches and their counterpart density maps into subspaces, and computed the embedding of each subspace formed by image patches. The density map of input patch is then estimated by simple classification and mapping with the corresponding embedding matrix.\n\nIn a more recent approach, Xu and Qiu [102] observed that the existing crowd density estimation methods used a smaller set of features thereby limiting their ability to perform better. Inspired by the ability of high-dimensional features in other domains such as face recognition, they proposed to boost the performances of crowd density estimation by using a much extensive and richer set of features. However, since the regression techniques used by earlier methods (based on Gaussian process regression or Ridge regression) are computationally complex and are unable to process very high-dimensional features, they used random forest as the regression model whose tree structure is intrinsically fast and scalable. Unlike traditional approaches to random forest construction, they embedded random projection in the tree nodes to combat the curse of dimensionality and to introduce randomness in the tree construction.\n\n\nCNN-based methods\n\nThe success of CNNs in numerous computer vision tasks has inspired researchers to exploit their abilities for learning nonlinear functions from crowd images to their corresponding density maps or corresponding counts. A variety of CNN-based methods have been proposed in the literature. We broadly categorize these methods based on property of the networks and training approach as shown in Fig. 3. Based on the property of the networks, we classify the approaches into the following categories:\n\n\u2022 Basic CNNs: Approaches that involve basic CNN layers in their networks fall into this category. These methods are amongst initial deep learning approaches for crowd counting and density estimation. \u2022 Scale-aware models: The basic CNN-based approaches evolved into more sophisticated models that were robust to variations in scale. This robustness is achieved through different techniques such as multi-column or multi-resolution architectures. \u2022 Context-aware models: Another set of approaches attempted to incorporate local and global contextual information present in the image into the CNN framework for achieving lower estimation errors. \u2022 Multi-task frameworks: Motivated by the success of multi-task learning for various computer vision tasks, various approaches have been developed to combine crowd counting and estimation along with other tasks such as foreground-background subtraction and crowd velocity estimation.\n\nIn an yet another categorization, we classify the CNN-based approaches based on the inference methodology into the following two categories:\n\n\u2022 Patch-based inference: In this approach, the CNNs are trained using patches cropped from the input images. Different methods use different crop sizes. During the prediction phase, a sliding window is run over the test image and predictions are obtained for each window and finally aggregated to obtain total count in the image. \u2022 Whole image-based inference: Methods in this category perform a whole-image based inference. These methods avoid computationally expensive sliding windows. Table 1 presents a categorization of various CNN-based crowd counting methods based on their network property and inference process.\n\n\nSurvey of CNN-based methods\n\nIn this section, we review various CNN-based crowd counting and density estimation methods along with their merits and \n\n\nCategory\n\n\nMethod\n\nNetwork property Inference process Fu et al. [33] Basic Patch-based Wang et al. [98] Basic Patch-based Zhang et al. [107] Multi-task Patch-based Boominathan et al. [11] Scale-aware Patch-based Zhang et al. [111] Scale-aware Whole image-based Walach and Wolf [97] Basic Patch-based Onoro et al. [70] Scale-aware Patch-based Shang et al. [85] Context-aware Whole image-based Sheng et al. [89] Context-aware Whole image-based Kumagai et al. [50] Scale-aware Patch-based Marsden et al. [65] Scale-aware Whole image-based Mundhenk et al. [69] Basic Patch-based Artetta et al. [4] Multi-task Patch-based Zhao et al. [113] Multi-task Patch-based Sindagi et al. [92] Multi-task Whole image-based Sam et al. [82] Scale-aware Patch-based Kang et al. [113] Basic Patch-based drawbacks.\n\nWang et al. [98] and Fu et al. [33] were among the first ones to apply CNNs for the task of crowd density estimation. Wang et al. proposed an end-to-end deep CNN regression model for counting people from images in extremely dense crowds. They adopted AlexNet network [49] in their architecture where the final fully connected layer of 4096 neurons is replaced with a single neuron layer for predicting the count. Besides, in order to reduce false responses background like buildings and trees in the images, training data is augmented with additional negative samples whose ground truth count is set as zero. In a different approach, Fu et al. proposed to classify the image into one of the five classes: very high density, high density, medium density, low density and very low density instead of estimating density maps. Multi-stage ConvNet from the works of Sermanet et al. [84] was adopted for better shift, scale and distortion invariance. In addition, they used a cascade of two classifiers to achieve boosting in which the first one specifically samples misclassified images whereas the second one reclassifies rejected samples.\n\nZhang et al. [107] analyzed existing methods to identify that their performance reduces drastically when applied to a new scene that is different from the training dataset. To overcome this issue, they proposed to learn a mapping from images to crowd counts and to adapt this mapping to new target scenes for cross-scene counting. To achieve this, they first learned their network by alternatively training on two objective functions: crowd count and density estimation which are related objectives. By alternatively optimizing over these objective functions one is able to obtain better local optima. In order to adapt this network to a new scene, the network is fine-tuned using training samples that are similar to the target scene. It is important to note that the network is adapted to new target scenes without any extra label information. The overview of their approach is shown in Fig. 4. Also, in contrast to earlier methods that use the sum of Gaussian kernels centered on the locations of objects, a new method for generating ground truth density map is proposed that incorporates perspective information. In doing so, the network is able to perform perspective normalization thereby achieving robustness to scale and perspective variations. Additionally, they introduced a new dataset for the purpose of evaluating cross-scene crowd counting. The network is evaluated for cross-scene crowd counting as well as single scene crowd counting and superior results are demonstrated for both scenarios. Inspired by the success of cross-scene crowd counting [107], Walach and Wolf [97] performed layered boosting and selective sampling. Layered boosting involves iteratively adding CNN layers to the model such that every new layer is trained to estimate the residual error of the earlier prediction. For instance, after the first CNN layer is trained, the second CNN layer is trained on the difference between the estimation and ground truth. This layered boosting approach is based on the notion of Gradient Boosting Machines (GBM) [32] which are a subset of powerful ensemble techniques. An overview of their boosting approach is presented in Fig. 5. The other contribution made by the authors is the use of sample selection algorithm to improve the training process by reducing the effect of low quality samples such as trivial samples or outliers. According to the authors, the samples that are correctly classified early on are trivial samples. Presenting such samples for training even after the networks have learned to classify them tends to introduce bias in the network for such samples, thereby affecting its generalization performance. Another source of training inefficiency is the presence of outliers such as mislabeled samples. Apart from affecting the network's performance, these samples increase the training time. To overcome this issue, such samples are eliminated out of the training process for a number of epochs. The authors demonstrated that their method reduces the count estimation error by 20% to 30% over existing stateof-the-art methods at that time on different datasets.\n\nIn contrast to the above methods that use patch-based training, Shang et al. [85] proposed an end-to-end count estimation method using CNNs (Fig. 6). Instead of dividing the image into patches, their method takes the entire image as input and directly outputs the final crowd count. As a result, computations on overlapping regions are shared by combining multiple stages of processing leading to a reduction of complexity. The network simultaneously learns to estimate local counts and can be viewed as learning a patch level counting model which enables faster training. By doing so, contextual information is incorporated into the network, enabling it to ignore background noises and achieve better performance. The network is composed of three parts: (1) Pre-trained GoogLeNet model [93], (2) Long-short time memory (LSTM) decoders for local count, and (3) Fully connected layers for the final count. The network takes an image as input and computes high-dimensional CNN feature maps using the GoogleNet network. Local blocks in these high-dimensional features are decoded into local count using a LSTM unit. A set of fully connected layers after the LSTM unit map the local counts into global count. The two counting objectives are jointly optimized during training. Fig. 6: Overview of the end-to-end counting method proposed by Shang et al. [85]. GoogLeNet is used to compute high-dimensional features which are further decoded into local counts using LSTM units.\n\nIn an effort to capture semantic information in the image, Boominathan et al. [11] combined deep and shallow fully convolutional networks to predict the density map for a given crowd image. The combination of two networks enables one to build a model robust to non-uniform scaling of crowd and variations in perspective. Furthermore, an extensive augmentation of the training dataset is performed in two ways. Patches from the multi-scale image representation are sampled to make the system robust to scale variations. Fig. 7 shows overview of this method.\n\nIn another approach, Zhang et al. [111] proposed a multicolumn based architecture (MCNN) for images with arbitrary  [11]. A deep network is used in combination with a shallow network to address scale variations across images. crowd density and arbitrary perspective. Inspired by the success of multi-column networks for image recognition [24], the proposed method ensures robustness to large variation in object scales by constructing a network that comprises of three columns corresponding to filters with receptive fields of different sizes (large, medium, small) as shown in Fig. 8. These different columns are designed to cater to different object scales present in the images. Additionally, a new method for generating ground truth crowd density maps is proposed. In contrast to existing methods that either use sum of Gaussian kernels with a fixed variance or perspective maps, Zhang et al. proposed to take into account perspective distortion by estimating spread parameter of the Gaussian kernel based on the size of the head of each person within the image. However, it is impractical to estimate head sizes and their underlying relationship with density maps. Instead they used an important property observed in high density crowd images that the head size is related to distance between the centers of two neighboring persons. The spread parameter for each person is data-adaptively determined based on its average distance to its neighbors. Note that the ground truth density maps created using this technique incorporate distortion information without the use of perspective maps. Finally, considering that existing crowd counting datasets do not cater to all the challenging situations encountered in real world scenarios, a new ShanghaiTech crowd datasets is constructed. This new dataset includes 1198 images with about 330,000 annotated heads. Similar to the above approach, Onoro and Sastre [70] developed a scale aware counting model called Hydra CNN that is able to estimate object densities in a variety of crowded sce-narios without any explicit geometric information of the scene. First, a deep fully-convolutional neural network (which they call as Counting CNN) with six convolutional layers is employed. Motivated by the observation of earlier work [107,61] that incorporating perspective information for geometric correction of the input features results in better accuracy, geometric information is incorporated into the Counting CNN (CCNN). To this effect, they developed Hydra CNN that learns a multiscale non-linear regression model. As shown in Fig. 9 the network consists of 3 heads and a body with each head learning features for a particular scale. Each head of the Hydra-CNN is constructed using the CCNN model whose outputs are concatenated and fed to the body. The body consists of a set of two fully-connected layers followed by a rectified linear unit (ReLu), a dropout layer and a final fully connected layer to estimate the object density map. While the different heads extract image descriptors at different scales, the body learns a highdimensional representation that fuses the multi-scale information provided by the heads. This network design of Hydra CNN is inspired by the work of Li et al. [53]. Finally, the network is trained with pyramid of image patches extracted at multiple scales. The authors demonstrated through their experiments that the Hydra CNN is able to perform successfully in scenarios and datasets with significant variations in the scene. Instead of training all regressors of a multi-column network [111] on all the input patches, Sam et al. [82] argue that better performance is obtained by training regressors with a particular set of training patches by leveraging variation of crowd density within an image. To this end, they proposed a switching CNN that cleverly selects an optimal regressor suited for a particular input patch. As shown in Fig. 10, the proposed network consists of multiple independent regressors similar to multi-column network [111] with different receptive fields and a switch classifier. The switch classifier is trained to select the optimal regressor for a particular input patch. Independent CNN crowd density regressors are trained on patches sampled from a grid in a given crowd scene. The switch classifier and the independent regressors are alternatively trained. The authors describe multiple stages of training their network. First, the independent regressors are pretrained on image patches to minimize the Euclidean distance between the estimated density map and ground truth. This is followed by a differential training stage where, the count error is factored in to improve the counting performance by back-propagating a regressor with the minimum count error for a given training patch. After training the multiple regressors, a switch classifier based on VGG-16 architecture [91] is trained to select an optimal regressor for accurate counting. Finally, the switch classifier and CNN regressors are co-adapted in the coupled training stage. While the above methods concentrated on incorporating scale information in the network, Sheng et al. in [89] proposed to integrate semantic information by learning localityaware feature sets. Noting that earlier methods that use handcrafted features ignored key semantic and spatial information, the authors proposed a new image representation which incorporates semantic attributes as well as spatial cues to improve the discriminative power of feature representations. They defined semantic attributes at the pixel level and learned semantic feature maps via deep CNN. The spatial information in the image is encoded using locality-aware features in the semantic attribute feature map space. The locality-aware features (LAF) are built on the idea of spatial pyramids on neighboring patches thereby encoding spatial context and local information. The local descriptors from adjacent cells are then encoded into image representations using weighted VLAD encoding method.\n\nSimilar to [111,70], Kumagai et al. [50], based on the observation that a single predictor is insufficient to appropriately predict the count in the presence of large appearance changes, proposed a Mixture of CNNs (MoCNN) that are specialized to a different scene appearances. As shown in Fig. 11, the architecture consists of a mixture of expert CNNs and a gating CNN that adaptively selects the appropriate CNN among the experts according to the appearance of the input image. For prediction, the expert CNNs predict crowd count in the image while the gating CNN predicts appropriate probabilities for each of the expert CNNs. These probabilities are further used as weighting factors to compute the weighted average of the counts predicted by all the expert CNNs. Motivated by the success of scale aware models [111,70], Marsden et al. [65] proposed to incorporate scale into the models with much less number of model parameters. Observing that the earlier scale aware models [111,70] are difficult to optimize and are computationally complex, Marsden et al. [65] proposed a single column fully convolutional network where the scale information is incorporated into the model using a simple yet effective multi-scale averaging step during prediction without any increase in the model parameters. The method addresses the issues of scale and perspective changes by feeding multiple scales of test image into the network during prediction phase. The crowd count is estimated for each scale and the final count is obtained by taking an average of all the estimates. Additionally, a new training set augmentation scheme is developed to reduce redundancy among the training samples. In contrast to the earlier methods that use randomly cropped patches with high degree of overlap, the training set in this work is constructed using the four image quadrants as well as their horizontal flips ensuring no overlap. This technique avoids potential overfit when the network is continuously exposed to the same set of pixels during training, thereby improving the generalization performance of the network. In addition, the generalization performance of the proposed method is studied by measuring cross dataset performance.\n\nInspired by the superior results achieved by simultaneous learning of related tasks [76,104], Sindagi et al. [92] and Marsden et al. [66] explored multi-task learning to boost individual task performance. Marsden et al. [66] proposed a Resnet-18 [37] based architecture for simultaneous crowd counting, violent behaviour detection and crowd density level classification. The network consists of initial 5 convolutional layers of Resnet18 including batch normalisation layers and skip connections form the primary module. The convolutional layers are followed by a set of task specific layers. Finally, sum of all the losses corresponding to different tasks is minimized. Additionally, the authors constructed a new 100 image dataset specifi- cally designed for multi-task learning of crowd count and behaviour. In a different approach, Sindagi et al. [92] proposed a cascaded CNN architecture to incorporate learning of a highlevel prior to boost the density estimation performance. Inspired by [18], the proposed network simultaneously learns to classify the crowd count into various density levels and estimate density map (as shown in Fig. 13). Classifying crowd count into various levels is equivalent to coarsely estimating the total count in the image thereby incorporating a high-level prior into the density estimation network. This enables the layers in the network to learn globally relevant discriminative features. Additionally, in contrast to most recent work, they make use of transposed convolutional layers to generate high resolution density maps. In a recent work, Kang et al. [45] explored maps generated by density estimation methods for the purpose of various crowd analysis tasks such as counting, detection and tracking. They performed a detailed analysis of the effect of using full-resolution density maps on the performance of these tasks. They demonstrated through their experiments that full resolution density maps improved the performance of localization tasks such as detection and tracking. Two different approaches are considered for generating full-resolution maps. In the first approach, a sliding window based CNN regressor is used for pixel-wise density prediction. In the second approach, Fully Convolutional Networks [60] along with skip connections are used to learning a non-linear mapping between input image and the corresponding density map.\n\nIn a slightly different application context of counting, Mundhenk et al. [69] and Arteta et al. [4] proposed to count different types of objects such as cars and penguins respectively. Mundhenk et al. [69] addressed the problem of automated counting of automobiles from satellite/aerial platforms. Their primary contribution is the creation of a large diverse set of cars from overhead images. Along with the large dataset, they present a deep CNN-based network to recognize the number of cars in patches. The network is trained in a classification setting where the output of the network is a class that is indicative of the number of objects in the input image. Also, they incorporated contextual information by including additional regions around the cars in the training patches. Three different networks based on AlexNet [49], GoogLeNet [93] and ResNet [37] with Inception are evaluated. For a different application of counting penguins in images, Arteta et al. [4] proposed a deep multi-task architecture for accurate counting even in the presence of labeling errors. The network is trained in a multi-task setting where, the tasks of foreground-background subtraction and uncertainty estimation along with counting are jointly learned. The authors demonstrated that the joint learning especially helps in learning a counting model that is robust to labeling errors. Additionally, they exploited scale variations and count variability across the annotations to incorporate scale information of the object and prediction of annotation difficulty respectively into the model. The network was evaluated on a newly created Penguin dataset.\n\nZhao et al. addressed a higher level cognitive task of counting people that cross a line in [113]. Though the task is a videobased application, it comprises of a CNN-based model that is trained with pixel-level supervision maps similar to single image crowd density estimation methods, making it a relevant approach to be included in this article. Their method consists of a two-phase training scheme (as shown in Fig. 14) that decomposes original counting problem into two sub-problems: estimating crowd density map and crowd velocity map where the two tasks share the initial set of layers enabling them to learn more effectively. The estimated crowd density and crowd velocity maps are then multiplied element-wise to generate the crowd counting maps. Additionally, they contributed a largescale dataset for evaluating crossing-line crowd counting algorithms, which includes 5 different scenes, 3,100 annotated frames and 5,900 annotated pedestrians.\n\n\nDiscussion\n\nWith a variety of methods discussed in Section 3, we analyze various advantages and disadvantages of the broad approaches followed by these methods in this section.\n\nZhang et al. [107] were among the first ones to address the problem of adapting models to new unlabelled datasets using a simple and effective method based on finding similar patches across datasets. However, their method is heavily dependent on accurate perspective maps which may not be necessarily available for all the datasets. Additionally, the use of 72\u00d772 sized patches for training and evaluation ignores global context which is necessary for accurate estimation of count. Walach et al. [97] successfully addressed training inefficiencies in earlier methods using a layered boosting approach and a simple sample selection method. However, similar to Zhang et al. [107], their method involves patch-based training and evaluation resulting in loss of global context information along with inefficiency during evaluation due to the use of a sliding window approach. Additionally, these methods tend to ignore scale variance among the dataset assuming that their models will implicitly learn the invariance.\n\nIn an effort to explicitly model scale invariance, several methods involving combination of networks were proposed ( [111,70,82,50,11]). While these methods demonstrated significant improvements in the performance using multiple column networks and a combination of deep and shallow networks, the invariance achieved is limited by the number of columns present in the network and receptive field sizes which are chosen based on the scales present in the dataset. Additionally, these methods do not explicitly model global context information which is crucial for a task such as crowd counting. In a different approach, Marsden et al. [65] attempt to address the scale issue by performing a multi-scale averaging during the prediction phase. While being simple and effective, it results in an inefficient inference stage. Additionally, these methods do not explicitly encode global context present in an image which can be crucial for improving the count performance. To this end, few approaches model local and global context [89,85] by considering key spatial and semantic information present in the image.\n\nIn an entirely different approach, few methods [66,92] take advantage of multi-task learning and incorporate high-level priors into the network. For instance, Sindagi et al. [92] simultaneously learn density estimation and a high-level prior in the form of crowd count classification. While they demonstrated high performance gain by learning an additional task of crowd density level classification, the number of density levels is dataset dependent and it needs to be carefully chosen based on the density levels present in the dataset.\n\n\nDatasets and results\n\nA variety of datasets have been created over the last few years driving researchers to create models with better generalization abilities. While the earlier datasets usually contain low density crowd images, the most recent ones focus on high density crowd thus posing numerous challenges such as scale variations, clutter and severe occlusion. The creation of these large scale datasets has motivated recent approaches to develop methods that cater to such challenges. In this section, we review five key datasets [15,20,41,107,111] followed by a discussion on the results of CNN-based approaches and recent traditional methods that were not included in the earlier surveys.\n\n\nDatasets\n\nUCSD dataset: The UCSD dataset [15] was among the first datasets to be created for counting people. The dataset was collected from a video camera at a pedestrian walkway. The dataset consists of 2000 frames of size 238\u00d7158 from a video sequence along with ground truth annotations of each pedestrian in every fifth frame. For the rest of the frames, linear interpolation is used to create the annotations. A region-ofinterest is also provided to ignore unnecessary moving objects such as trees. The dataset contains a total of 49,885 pedestrian instances and it is split into training and test set. While the training set contains frames with indices 600 to 1399, the test set contains the remaining 1200 images. This dataset has relatively low density crowd with an average of around 15 people in a frame and since the dataset was collected from a single location, there is no variation in the scene perspective across images.\n\nMall dataset: Considering little variation in the scene type in the UCSD dataset, Chen et al. in [20] collected a new Mall dataset with diverse illumination conditions and crowd densities. The dataset was collected using a surveillance camera installed in a shopping mall. Along with having various density levels, it also has different activity patterns (static and moving crowds). Additionally, the scene contained in the dataset has severe perspective distortion resulting in large variations in size and appearance of objects. The dataset also presents the challenge of severe occlusions caused by the scene objects, e.g.stall, indoor plants along the walking path. The video sequence in the dataset consists of 2000 frames of size 320\u00d7240 with 6000 instances of labelled pedestrians. The first 800 frames are used for training and the remaining 1200 frames are used for evaluation. In comparison to the UCSD dataset, the Mall dataset has relatively higher crowd density images. However, both the datasets do not have any variation in the scene perspective across images since they are a part of a single continuous video sequence.\n\nUCF CC 50 dataset: The UCF CC 50 [41] is the first truly challenging dataset constructed to include a wide range of densities and diverse scenes with varying perspective distortion. The dataset was created from publicly available web images. In order to capture diversity in the scene types, the authors collected images with different tags such as concerts, protests, stadiums and marathons. It contains a total of 50 images  Fig. 15. The datasets are also summarized in Table 2. It can be observed that the UCSD and the Mall dataset have relatively low density images and typically focus on single scene type. In contrast, the other datasets have significant variations in the density levels along with different perspectives across images.\n\n\nDiscussion on results\n\nResults of the recent traditional approaches along with CNNbased methods are tabulated in Table 3. The count estimation errors are reported directly from the respective original works. The following standard metrics are used to compare different methods:\nMAE = 1 N N i=1 |y i \u2212 y i |,(1)MS E = 1 N N i=1 |y i \u2212 y i | 2 ,(2)\nwhere MAE is mean absolute error, MSE is mean squared error, N is the number of test samples, y i is the ground truth count and y i is the estimated count corresponding to the i th sample. We make the following observations regarding the results:\n\n\u2022 In general, CNN-based methods outperform the traditional approaches across all datasets. \u2022 While the CNN-based methods are especially effective in large density crowds with a diverse scene conditions, the traditional approaches suffer from high error rates in such scenarios. \u2022 Among the CNN-based methods, most performance improvement is achieved by scale-aware and context-aware models. It can be observed from Table 3 that a reduction in count error is largely driven by the increase in the complexity of CNN models (due to addition of context and scale information). \u2022 While the multi-column CNN architecture [111] achieves the state-of-the-art results on 3 datasets: UCSD, World-Expo '10 and ShanghaiTech, the CNN-boosting approach by [97] achieves the best results on the Mall dataset. The best results on the UCF CC 50 dataset are achieved by joint local and global count approach [85] and Hydra-CNN [70]. \u2022 The work in [97] suggests that layered boosting can achieve performances that are comparable to scale aware models. \u2022 The improvements obtained by selective sampling in [98] and [97] suggests that it helps to obtain unbiased performance. \u2022 Whole image-based methods such as Zhang et al. [111] and Shang et al. [85] are less computationally complex from the prediction point of view and they have proved to achieve better results over patch-based techniques. \u2022 Finally, techniques such as layered boosting and selective sampling [70,99] not only improve the estimation error but also reduce the training time significantly. It can be observed that though the method is able to accurate estimation of crowd count, the estimated density maps are of poor quality.\n\n\nFuture research directions\n\nBased on the analysis of various methods and results from Section 3 and 5 and the trend of other developments in computer vision, we believe that CNN-based deeper architectures will dominate further research in the field of crowd counting and density estimation. We make the following observations regarding future trends in research on crowd counting: 1. Given the requirement of large datasets for training deep networks, collection of large scale datasets (especially for extremely dense crowds) is essential. Though many datasets exist currently, only one of them (The UCF CC 50 [41]) caters to large density crowds. However, the size of the dataset is too small for training deeper networks. Though Shanghai Tech [111]) attempts to capture large density crowds, the number of images per density level is non-uniform with a large number of images available for low density levels and very few samples for high density levels (as shown in Fig. 17). 2. Considering the difficulty of training deep networks for new scenes, it would be important to explore how to leverage from models trained on existing sources. Most of the existing methods retrain their models on a new scene and it is impractical to do so in real world scenarios as it would be expensive to obtain annotations for every new scene. Zhang et al. [107] attempted to address this issue by performing a data driven training without the need of labelled data for new scenes. In an another approach, Liu et al. [59] considered the problem of transfer learning for crowd counting. A model adaptation technique for Gaussian process counting model was introduced. Considering the source model as a prior and the target dataset as a set of observations, the components are combined into a predictive distribution that captures information in both the source and target datasets. However, the idea of transfer learning or domain adaptation [73] for crowd scenes is relatively unexplored and is a nascent area of research. 3. Most crowd counting and density estimation methods have been designed for and evaluated either only on single images or videos. Combining the techniques developed separately for these methods is a non-trivial task. Development of low-latency methods that can operate in real-time for counting people in crowds from videos is another interesting problem to be addressed in future. 4. Another key issue ignored by earlier research is that the quality of estimated crowd density maps. Many existing CNN-based approaches have a number of max-pooling layers in their networks compelling them to regress on down-sampled density maps. Also, most methods optimize over traditional Euclidean loss which is known to have certain disadvantages [43]. Regressing on downsampled density maps using Euclidean loss results in low quality density maps. Fig. 16 demonstrates the results obtained using the state-of-the-art method [111]. It can be observed that though accurate count estimates are obtained, the quality of the density maps is poor. As a result, these poor quality maps adversely affect other higher level cognition tasks which depend on them. Recent work on style-transfer [108], image de-raining [109] and imageto-image translation [42] have demonstrated promising results from the use of additional loss functions such as adversarial loss and perceptual loss. In principle, density estimation can be considered as an image-to-image translation problem and it would be interesting to see the effect of these recent loss functions. Generating high quality density maps along with low count estimation error would be another important issue to be addressed in the future. 5. Finally, considering advancements by scale-aware [111,70] and context-aware models [85], we believe designing networks to incorporate additional contextual and scale information will enable further progress.\n\n\nConclusion\n\nThis article presented an overview of recent advances in CNN-based methods for crowd counting and density estimation. In particular, we summarized various methods for crowd counting into traditional approaches (that use hand-crafted features) and CNN-based approaches. The CNN-based approaches are further categorized based on the training process and the network property. Obviously all the literature on crowd counting cannot be covered, hence, we have chosen a representative subset of the latest approaches for a detailed analysis and review. We also reviewed the results demonstrated by various traditional and CNN-based approaches to conclude that CNNbased methods are more adept at handling large density crowds with variations in object scales and scene perspective. Additionally, we observed that incorporating scale and contextual information in the CNN-based methods drastically improves the estimation error. Finally, we identified some of the most compelling challenges and issues that confront research in crowd counting and density estimation using computer vision and machine leaning approaches.\n\nFig. 1 :\n1Illustration of density map estimation. (a) Input image (b) Corresponding density map with count.\n\nFig. 2 :\n2Forensic search: Crowd analysis can be used to search for sus-Illustration of various crowded scenes and the associated challenges. (a) Parade (b) Musical concert (c) Public demonstration (d) Sports stadium. High clutter, overlapping of subjects, variation in scale and perspective can be observed across images.\n\nFig. 3 :\n3Categorization of existing CNN-based approaches.\n\nFig. 4 :\n4Overview of cross scene crowd counting proposed by Zhang et al.[107].\n\nFig. 5 :\n5Overview of learning to count using boosting by Walach and Wolf[97].\n\nFig. 7 :\n7Overview of counting method proposed by Boominathan et al.\n\nFig. 8 :\n8Overview of single image crowd counting via multi-column network by Zhang et al.[111].\n\nFig. 9 :\n9Overview of Hydra-CNN by Onoro et al.[70].\n\nFig. 10 :\n10Overview of Switching CNN by Sam et al.[82].\n\nFig. 11 :\n11Overview of MoC (Mixture of CNN) for crowd counting by Kumagai et al.[50].\n\nFig. 12 :\n12Overview of Fully Convolutional Network for crowd counting by Marsden et al.[65].\n\nFig. 13 :\n13Overview of Cascaded Multi-task CNN by Sindagi et al.[92].\n\nFig. 14 :\n14Overview of the method proposed by Zhao et al.[113] for counting people crossing a line.\n\nFig. 16 :\n16Results of Zhang et al. [111] on ShanghaiTech dataset. (a) Input image(b) Ground-truth density map (c) Estimated density maps.\n\nTable 1 :\n1Categorization of existing CNN-based approaches.\n\nTable 2 :\n2Summary of various datasets. Part A has considerably larger density images as compared to Part B. Both the parts are further divided into training and evaluation sets. The training and test of Part A has 300 and 182 images, respectively, whereas that of Part B has 400 and 316 images, respectively. The dataset successfully attempts to create a challenging dataset with diverse scene types and varying density levels. However, the number of images for various density levels are not uniform making the training and evaluation biased towards low density levels. Nevertheless, the complexities present in this dataset such as varying scales and perspective distortion has created new opportunities for more complex CNN network designs.Sample images from the five datasets are shown inDataset \nNo. of images Resolution Min Ave Max Total count \nUCSD [15] \n2000 \n158x238 \n11 \n25 \n46 \n49,885 \nMall [20] \n2000 \n320x240 \n13 \n-\n53 \n62,325 \nUCF CC 50 [41] \n50 \nVaried \n94 \n1279 4543 \n63,974 \nWorldExpo '10 [106, 107] \n3980 \n576x720 \n1 \n50 \n253 \n199,923 \nShanghaiTech Part A [111] \n482 \nVaried \n33 \n501 3139 \n241,677 \nShanghaiTech Part B [111] \n716 \n768x1024 \n9 \n123 \n578 \n88,488 \n\n\n\nTable 3 :\n3Comparison of results on various datasets. The CNN-based approaches provide significant improvements over traditional approaches that rely on hand-crafted representations. Further, among the CNN-based methods, scale aware and context aware approaches tend to achieve lower count error.MSE  MAE MSE MAE MSE MAE MSE MAE MSE MAE MSEFig. 17: Distribution of crowd counts in ShanghaiTech dataset. It can be observed that the dataset is highly imbalanced.Dataset \nUCSD \nMall \nUCF CC 50 \nWorldExpo \n'10 \n\nShanghai \nTech-A \n\nShanghai \nTech-B \nApproach \ntype \nMethod \nMAE Traditional approaches \n\nMulti-source multi-scale \nIdrees et al. [41] \n468.0 590.3 \n\nCumulative Attributes \nChen et al. [19] \n2.07 \n6.86 \n3.43 17.07 \n\nDensity learning \nLempitsky et al. [52] \n1.7 \n493.4 487.1 \n\nCount forest \nPham et al. [75] \n1.61 \n4.40 \n2.5 \n10.0 \n\nExemplar density \nWang et al. [99] \n1.98 \n1.82 \n2.74 \n2.10 \n\nRandom projection forest \nXu et al. [102] \n1.90 \n6.01 \n3.22 \n15.5 \n\nCNN-based approaches \n\nCross-scene \nZhang et al. [107] \n1.60 \n3.31 \n467.0 498.5 \n12.9 \n181.8 277.7 \n32.0 \n49.8 \n\nDeep + shallow \nBoominathan et al. [11] \n452.5 \n\nM-CNN \nZhang et al. [111] \n1.07 \n1.35 \n377.6 509.1 \n11.6 \n110.2 173.2 \n26.4 \n41.3 \n\nCNN-boosting \nWalach and Wolf [97] \n1.10 \n2.01 \n364.4 \n\nHydra-CNN \nOnoro et al. [70] \n333.7 425.2 \n\nJoint local & global count \nShang et al. [85] \n270.3 \n11.7 \n\nMoCNN \nKumagai et al. [50] \n2.75 \n13.4 361.7 493.3 \n\nFCN \nMarsden et al. [65] \n338.6 424.5 \n126.5 173.5 23.76 33.12 \n\nCNN-pixel \nKang et al. [45] \n1.12 \n2.06 \n406.2 404.0 \n13.4 \n\nWeighted V-LAD \nSheng et al. [89] \n2.86 \n13.0 \n2.41 \n9.12 \n\nCascaded-MTL \nSindagi et al. [92] \n322.8 341.4 \n101.3 152.4 \n20.0 \n31.1 \n\nSwitching-CNN \nSam et al. [82] \n1.62 \n2.10 \n318.1 439.2 \n9.4 \n90.4 \n135.0 21.6 \n33.4 \n\n\n\nModeling framework for optimal evacuation of large-scale crowded pedestrian facilities. A Abdelghany, K Abdelghany, H Mahmassani, W Alhalabi, European Journal of Operational Research. 237Abdelghany, A., Abdelghany, K., Mahmassani, H., Alhalabi, W., 2014. Modeling framework for optimal evacuation of large-scale crowded pedestrian facilities. European Journal of Operational Research 237, 1105-1118.\n\nCrowd management and urban design: New scientific approaches. K Al-Kodmany, Urban Design International. 18Al-Kodmany, K., 2013. Crowd management and urban design: New scientific approaches. Urban Design International 18, 282-295.\n\nCrowd simulation modeling applied to emergency and evacuation simulations using multiagent systems. J E Almeida, R J Rosseti, A L Coelho, arXiv:1303.4692arXiv preprintAlmeida, J.E., Rosseti, R.J., Coelho, A.L., 2013. Crowd simulation modeling applied to emergency and evacuation simulations using multi- agent systems. arXiv preprint arXiv:1303.4692 .\n\nCounting in the wild. C Arteta, V Lempitsky, A Zisserman, European Conference on Computer Vision. SpringerArteta, C., Lempitsky, V., Zisserman, A., 2016. Counting in the wild, in: European Conference on Computer Vision, Springer. pp. 483-498.\n\nThe not-so-lonely crowd: Friendship groups in collective behavior. A F Aveni, Sociometry. Aveni, A.F., 1977. The not-so-lonely crowd: Friendship groups in col- lective behavior. Sociometry , 96-99.\n\nTowards an integrated approach to crowd analysis and crowd synthesis: A case study and first results. S Bandini, A Gorrini, G Vizzari, Pattern Recognition Letters. 44Bandini, S., Gorrini, A., Vizzari, G., 2014. Towards an integrated ap- proach to crowd analysis and crowd synthesis: A case study and first results. Pattern Recognition Letters 44, 16-29.\n\nThe effectiveness of face detection algorithms in unconstrained crowd scenes. J R Barr, K W Bowyer, P J Flynn, Applications of Computer Vision (WACV). Barr, J.R., Bowyer, K.W., Flynn, P.J., 2014. The effectiveness of face detection algorithms in unconstrained crowd scenes, in: Applications of Computer Vision (WACV), 2014 IEEE Winter Conference on, IEEE. pp. 1020-1027.\n\nMotion pattern extraction and event detection for automatic visual surveillance. Y Benabbas, N Ihaddadene, C Djeraba, EURASIP Journal on Image and Video Processing. 163682Benabbas, Y., Ihaddadene, N., Djeraba, C., 2010. Motion pattern extrac- tion and event detection for automatic visual surveillance. EURASIP Journal on Image and Video Processing 2011, 163682.\n\nSystem and method for videobased detection of drive-offs and walk-offs in vehicular and pedestrian queues. E A Bernal, Q Li, R P Loce, App. 14/279652US PatentBernal, E.A., Li, Q., Loce, R.P., 2014. System and method for video- based detection of drive-offs and walk-offs in vehicular and pedestrian queues. US Patent App. 14/279,652.\n\nCollective behavior. New outline of the principles of sociology. H Blumer, Blumer, H., 1951. Collective behavior. New outline of the principles of sociology , 166-222.\n\nCrowdnet: A deep convolutional network for dense crowd counting. L Boominathan, S S Kruthiventi, R V Babu, Proceedings of the 2016 ACM on Multimedia Conference, ACM. the 2016 ACM on Multimedia Conference, ACMBoominathan, L., Kruthiventi, S.S., Babu, R.V., 2016. Crowdnet: A deep convolutional network for dense crowd counting, in: Proceedings of the 2016 ACM on Multimedia Conference, ACM. pp. 640-644.\n\nUnsupervised bayesian detection of independent motion in crowds. G J Brostow, R Cipolla, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). IEEEBrostow, G.J., Cipolla, R., 2006. Unsupervised bayesian detection of independent motion in crowds, in: 2006 IEEE Computer Society Con- ference on Computer Vision and Pattern Recognition (CVPR'06), IEEE. pp. 594-601.\n\nStatistical physics of social dynamics. C Castellano, S Fortunato, V Loreto, Reviews of modern physics. 81591Castellano, C., Fortunato, S., Loreto, V., 2009. Statistical physics of social dynamics. Reviews of modern physics 81, 591.\n\nSocial network model for crowd anomaly detection and localization. R Chaker, Z Al Aghbari, I N Junejo, Pattern Recognition. 61Chaker, R., Al Aghbari, Z., Junejo, I.N., 2017. Social network model for crowd anomaly detection and localization. Pattern Recognition 61, 266-281.\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. A B Chan, Z S J Liang, N Vasconcelos, Computer Vision and Pattern Recognition. Chan, A.B., Liang, Z.S.J., Vasconcelos, N., 2008. Privacy preserving crowd monitoring: Counting people without people models or tracking, in: Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, IEEE. pp. 1-7.\n\nBayesian poisson regression for crowd counting. A B Chan, N Vasconcelos, 2009 IEEE 12th International Conference on Computer Vision, IEEE. Chan, A.B., Vasconcelos, N., 2009. Bayesian poisson regression for crowd counting, in: 2009 IEEE 12th International Conference on Com- puter Vision, IEEE. pp. 545-551.\n\nCounting people with low-level features and bayesian regression. A B Chan, N Vasconcelos, IEEE Transactions on Image Processing. 21Chan, A.B., Vasconcelos, N., 2012. Counting people with low-level fea- tures and bayesian regression. IEEE Transactions on Image Processing 21, 2160-2177.\n\nA cascaded convolutional neural network for age estimation of unconstrained faces. J C Chen, A Kumar, R Ranjan, V M Patel, A Alavi, R Chellappa, International Conference on BTAS, IEEE. Chen, J.C., Kumar, A., Ranjan, R., Patel, V.M., Alavi, A., Chellappa, R., 2016. A cascaded convolutional neural network for age estimation of unconstrained faces, in: International Conference on BTAS, IEEE. pp. 1-8.\n\nCumulative attribute space for age and crowd density estimation. K Chen, S Gong, T Xiang, C Loy, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChen, K., Gong, S., Xiang, T., Change Loy, C., 2013. Cumulative at- tribute space for age and crowd density estimation, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2467-2474.\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, European Conference on Computer Vision. Chen, K., Loy, C.C., Gong, S., Xiang, T., 2012. Feature mining for lo- calised crowd counting., in: European Conference on Computer Vision.\n\nPerson count localization in videos from noisy foreground and detections. S Chen, A Fern, S Todorovic, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChen, S., Fern, A., Todorovic, S., 2015. Person count localization in videos from noisy foreground and detections, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1364-1372.\n\nRecognizing human group action by layered model with multiple cues. Z Cheng, L Qin, Q Huang, S Yan, Q Tian, Neurocomputing. 136Cheng, Z., Qin, L., Huang, Q., Yan, S., Tian, Q., 2014. Recognizing hu- man group action by layered model with multiple cues. Neurocomputing 136, 124-135.\n\nWaiting time in emergency evacuation of crowded public transport terminals. W K Chow, C M Ng, Safety Science. 46Chow, W.K., Ng, C.M., 2008. Waiting time in emergency evacuation of crowded public transport terminals. Safety Science 46, 844-857.\n\nMulti-column deep neural networks for image classification. D Ciregan, U Meier, J Schmidhuber, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference. Ciregan, D., Meier, U., Schmidhuber, J., 2012. Multi-column deep neu- ral networks for image classification, in: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, IEEE. pp. 3642-3649.\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, IEEE. Dalal, N., Triggs, B., 2005. Histograms of oriented gradients for human detection, in: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, IEEE. pp. 886-893.\n\nPedestrian detection: An evaluation of the state of the art. P Dollar, C Wojek, B Schiele, P Perona, IEEE transactions. 34Dollar, P., Wojek, C., Schiele, B., Perona, P., 2012. Pedestrian detec- tion: An evaluation of the state of the art. IEEE transactions on pattern analysis and machine intelligence 34, 743-761.\n\nFast crowd segmentation using shape indexing. L Dong, V Parameswaran, V Ramesh, I Zoghlami, 2007 IEEE 11th International Conference on Computer Vision, IEEE. Dong, L., Parameswaran, V., Ramesh, V., Zoghlami, I., 2007. Fast crowd segmentation using shape indexing, in: 2007 IEEE 11th International Conference on Computer Vision, IEEE. pp. 1-8.\n\nMonocular pedestrian detection: Survey and experiments. M Enzweiler, D M Gavrila, 31Enzweiler, M., Gavrila, D.M., 2009. Monocular pedestrian detection: Survey and experiments. IEEE transactions on pattern analysis and ma- chine intelligence 31, 2179-2195.\n\nObject detection with discriminatively trained part-based models. P F Felzenszwalb, R B Girshick, D Mcallester, D Ramanan, IEEE transactions. 32Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D., 2010. Object detection with discriminatively trained part-based models. IEEE transactions on pattern analysis and machine intelligence 32, 1627- 1645.\n\nPerformance evaluation of crowd image analysis using the pets2009 dataset. J Ferryman, A L Ellis, Pattern Recognition Letters. 44Ferryman, J., Ellis, A.L., 2014. Performance evaluation of crowd image analysis using the pets2009 dataset. Pattern Recognition Letters 44, 3- 15.\n\nConvolutional neural networks for counting fish in fisheries surveillance video. G French, M Fisher, M Mackiewicz, C Needle, British Machine Vision Conference Workshop. BMVA PressFrench, G., Fisher, M., Mackiewicz, M., Needle, C., 2015. Convolu- tional neural networks for counting fish in fisheries surveillance video, in: British Machine Vision Conference Workshop, BMVA Press.\n\nGreedy function approximation: a gradient boosting machine. J H Friedman, Annals of statistics. Friedman, J.H., 2001. Greedy function approximation: a gradient boost- ing machine. Annals of statistics , 1189-1232.\n\nFast crowd density estimation with convolutional neural networks. M Fu, P Xu, X Li, Q Liu, M Ye, C Zhu, Engineering Applications of Artificial Intelligence. 43Fu, M., Xu, P., Li, X., Liu, Q., Ye, M., Zhu, C., 2015. Fast crowd den- sity estimation with convolutional neural networks. Engineering Appli- cations of Artificial Intelligence 43, 81-88.\n\nHough forests for object detection, tracking, and action recognition. J Gall, A Yao, N Razavi, L Van Gool, V Lempitsky, IEEE trans. 33Gall, J., Yao, A., Razavi, N., Van Gool, L., Lempitsky, V., 2011. Hough forests for object detection, tracking, and action recognition. IEEE trans- actions on pattern analysis and machine intelligence 33, 2188-2202.\n\nMarked point processes for crowd counting. W Ge, R T Collins, Computer Vision and Pattern Recognition. Ge, W., Collins, R.T., 2009. Marked point processes for crowd counting, in: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE. pp. 2913-2920.\n\nMure: fast agent based crowd simulation for vfx and animation. S Gustafson, H Arumugam, P Kanyuk, M Lorenzen, ACM SIG-GRAPH 2016 Talks. ACM56Gustafson, S., Arumugam, H., Kanyuk, P., Lorenzen, M., 2016. Mure: fast agent based crowd simulation for vfx and animation, in: ACM SIG- GRAPH 2016 Talks, ACM. p. 56.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for im- age recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778.\n\nL F Henderson, 10.1038/229381a0The Statistics of Crowd Fluids. 229Henderson, L.F., 1971. The Statistics of Crowd Fluids 229, 381-383. doi:10.1038/229381a0.\n\nDense crowd counting from still images with convolutional neural networks. Y Hu, H Chang, F Nian, Y Wang, T Li, Journal of Visual Communication and Image Representation. 38Hu, Y., Chang, H., Nian, F., Wang, Y., Li, T., 2016. Dense crowd count- ing from still images with convolutional neural networks. Journal of Visual Communication and Image Representation 38, 530-539.\n\nCongestion detection of pedestrians using the velocity entropy: A case study of love parade 2010 disaster. L Huang, T Chen, Y Wang, H Yuan, Physica A: Statistical Mechanics and its Applications. 440Huang, L., Chen, T., Wang, Y., Yuan, H., 2015. Congestion detection of pedestrians using the velocity entropy: A case study of love parade 2010 disaster. Physica A: Statistical Mechanics and its Applications 440, 200-209.\n\nMulti-source multiscale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIdrees, H., Saleemi, I., Seibert, C., Shah, M., 2013. Multi-source multi- scale counting in extremely dense crowd images, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2547-2554.\n\nImage-to-image translation with conditional adversarial networks. P Isola, J Y Zhu, T Zhou, A A Efros, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionIsola, P., Zhu, J.Y., Zhou, T., Efros, A.A., 2017. Image-to-image transla- tion with conditional adversarial networks, in: Proceedings of the IEEE conference on computer vision and pattern recognition.\n\nPerceptual losses for realtime style transfer and super-resolution. J Johnson, A Alahi, L Fei-Fei, European Conference on Computer Vision. SpringerJohnson, J., Alahi, A., Fei-Fei, L., 2016. Perceptual losses for real- time style transfer and super-resolution, in: European Conference on Computer Vision, Springer. pp. 694-711.\n\nCrowd analysis using computer vision techniques. J C S J Junior, S R Musse, C R Jung, IEEE Signal Processing Magazine. 27Junior, J.C.S.J., Musse, S.R., Jung, C.R., 2010. Crowd analysis using computer vision techniques. IEEE Signal Processing Magazine 27, 66- 77.\n\nBeyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking. D Kang, Z Ma, A B Chan, arXiv:1705.10118arXiv preprintKang, D., Ma, Z., Chan, A.B., 2017. Beyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking. arXiv preprint arXiv:1705.10118 .\n\nFully convolutional neural networks for crowd segmentation. K Kang, X Wang, arXiv:1411.4464arXiv preprintKang, K., Wang, X., 2014. Fully convolutional neural networks for crowd segmentation. arXiv preprint arXiv:1411.4464 .\n\nA case study on unconstrained facial recognition using the boston marathon bombings suspects. J C Klontz, A K Jain, Tech. Rep. 1191Michigan State UniversityKlontz, J.C., Jain, A.K., 2013. A case study on unconstrained facial recognition using the boston marathon bombings suspects. Michigan State University, Tech. Rep 119, 1.\n\nA survey on behavior analysis in video surveillance for homeland security applications. T Ko, Applied Imagery Pattern Recognition Workshop. AIPR'08. 37th IEEEKo, T., 2008. A survey on behavior analysis in video surveillance for homeland security applications, in: Applied Imagery Pattern Recogni- tion Workshop, 2008. AIPR'08. 37th IEEE, IEEE. pp. 1-8.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classifica- tion with deep convolutional neural networks, in: Advances in neural information processing systems, pp. 1097-1105.\n\nMixture of counting cnns: Adaptive integration of cnns specialized to specific appearance for crowd counting. S Kumagai, K Hotta, T Kurita, arXiv:1703.09393arXiv preprintKumagai, S., Hotta, K., Kurita, T., 2017. Mixture of counting cnns: Adaptive integration of cnns specialized to specific appearance for crowd counting. arXiv preprint arXiv:1703.09393 .\n\nPedestrian detection in crowded scenes. B Leibe, E Seemann, B Schiele, Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, IEEE. Leibe, B., Seemann, E., Schiele, B., 2005. Pedestrian detection in crowded scenes, in: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, IEEE. pp. 878- 885.\n\nLearning to count objects in images. V Lempitsky, A Zisserman, Advances in Neural Information Processing Systems. Lempitsky, V., Zisserman, A., 2010. Learning to count objects in images, in: Advances in Neural Information Processing Systems, pp. 1324-1332.\n\nVisual saliency based on multiscale deep features. G Li, Y Yu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLi, G., Yu, Y., 2015. Visual saliency based on multiscale deep features, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5455-5463.\n\nEstimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. M Li, Z Zhang, K Huang, T Tan, 19th International Conference on, IEEE. Pattern RecognitionLi, M., Zhang, Z., Huang, K., Tan, T., 2008. Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection, in: Pattern Recognition, 2008. ICPR 2008. 19th International Conference on, IEEE. pp. 1-4.\n\nCrowded scene analysis: A survey. T Li, H Chang, M Wang, B Ni, R Hong, S Yan, IEEE Transactions on Circuits and Systems for Video Technology. 25Li, T., Chang, H., Wang, M., Ni, B., Hong, R., Yan, S., 2015. Crowded scene analysis: A survey. IEEE Transactions on Circuits and Systems for Video Technology 25, 367-386.\n\nAnomaly detection and localization in crowded scenes. W Li, V Mahadevan, N Vasconcelos, 36Li, W., Mahadevan, V., Vasconcelos, N., 2014. Anomaly detection and localization in crowded scenes. IEEE transactions on pattern analysis and machine intelligence 36, 18-32.\n\nEstimation of number of people in crowded scenes using perspective transformation. S F Lin, J Y Chen, H X Chao, IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans. 31Lin, S.F., Chen, J.Y., Chao, H.X., 2001. Estimation of number of people in crowded scenes using perspective transformation. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans 31, 645-654.\n\nVideo analytics for retail business process monitoring. A J Lipton, P L Venetianer, N Haering, P C Brewer, W Yin, Z Zhang, L Yu, Y Hu, G W Myers, A J Chosak, US Patent. 9975Lipton, A.J., Venetianer, P.L., Haering, N., Brewer, P.C., Yin, W., Zhang, Z., Yu, L., Hu, Y., Myers, G.W., Chosak, A.J., et al., 2015. Video ana- lytics for retail business process monitoring. US Patent 9,158,975.\n\nBayesian model adaptation for crowd counts. B Liu, N Vasconcelos, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLiu, B., Vasconcelos, N., 2015. Bayesian model adaptation for crowd counts, in: Proceedings of the IEEE International Conference on Com- puter Vision, pp. 4175-4183.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLong, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for semantic segmentation, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440.\n\nCrowd counting and profiling: Methodology and evaluation, in: Modeling, Simulation and Visual Analysis of Crowds. C C Loy, K Chen, S Gong, T Xiang, SpringerLoy, C.C., Chen, K., Gong, S., Xiang, T., 2013. Crowd counting and profiling: Methodology and evaluation, in: Modeling, Simulation and Visual Analysis of Crowds. Springer, pp. 347-382.\n\nA study of pedestrian group behaviors in crowd evacuation based on an extended floor field cellular automaton model. L Lu, C Y Chan, J Wang, W Wang, Transportation Research Part C: Emerging Technologies. Lu, L., Chan, C.Y., Wang, J., Wang, W., 2016. A study of pedestrian group behaviors in crowd evacuation based on an extended floor field cellular automaton model. Transportation Research Part C: Emerging Technologies .\n\nAnomaly detection in crowded scenes. V Mahadevan, W Li, V Bhalodia, N Vasconcelos, CVPR250Mahadevan, V., Li, W., Bhalodia, V., Vasconcelos, N., 2010. Anomaly detection in crowded scenes., in: CVPR, p. 250.\n\nOn the efficacy of texture analysis for crowd monitoring. A Marana, L D F Costa, R Lotufo, S Velastin, Proceedings. SIBGRAPI'98. International Symposium on, IEEE. SIBGRAPI'98. International Symposium on, IEEEComputer Graphics, Image Processing, and VisionMarana, A., Costa, L.d.F., Lotufo, R., Velastin, S., 1998. On the efficacy of texture analysis for crowd monitoring, in: Computer Graphics, Image Processing, and Vision, 1998. Proceedings. SIBGRAPI'98. International Symposium on, IEEE. pp. 354-361.\n\nM Marsden, K Mcguiness, S Little, N E O&apos;connor, arXiv:1612.00220Fully convolutional crowd counting on highly congested scenes. arXiv preprintMarsden, M., McGuiness, K., Little, S., O'Connor, N.E., 2016. Fully convolutional crowd counting on highly congested scenes. arXiv preprint arXiv:1612.00220 .\n\nResnetcrowd: A residual deep learning architecture for crowd counting, violent behaviour detection and crowd density level classification. M Marsden, K Mcguinness, S Little, N E O&apos;connor, arXiv:1705.10698arXiv preprintMarsden, M., McGuinness, K., Little, S., O'Connor, N.E., 2017. Resnetcrowd: A residual deep learning architecture for crowd count- ing, violent behaviour detection and crowd density level classification. arXiv preprint arXiv:1705.10698 .\n\nBusyness detection and notification method and system. M C Mongeon, R P Loce, M A Shreve, App. 14/625960US PatentMongeon, M.C., Loce, R.P., Shreve, M.A., 2015. Busyness detection and notification method and system. US Patent App. 14/625,960.\n\nThe walking behaviour of pedestrian social groups and its impact on crowd dynamics. M Moussa\u00efd, N Perozo, S Garnier, D Helbing, G Theraulaz, PloS one. 5Moussa\u00efd, M., Perozo, N., Garnier, S., Helbing, D., Theraulaz, G., 2010. The walking behaviour of pedestrian social groups and its impact on crowd dynamics. PloS one 5, e10047.\n\nA large contextual dataset for classification, detection and counting of cars with deep learning. T N Mundhenk, G Konjevod, W A Sakla, K Boakye, European Conference on Computer Vision. SpringerMundhenk, T.N., Konjevod, G., Sakla, W.A., Boakye, K., 2016. A large contextual dataset for classification, detection and counting of cars with deep learning, in: European Conference on Computer Vision, Springer. pp. 785-800.\n\nTowards perspective-free object counting with deep learning. D Onoro-Rubio, R J L\u00f3pez-Sastre, European Conference on Computer Vision. SpringerOnoro-Rubio, D., L\u00f3pez-Sastre, R.J., 2016. Towards perspective-free object counting with deep learning, in: European Conference on Com- puter Vision, Springer. pp. 615-629.\n\nA mrf-based approach for real-time subway monitoring. N Paragios, V Ramesh, Computer Vision and Pattern Recognition. IEEE1034Proceedings of theParagios, N., Ramesh, V., 2001. A mrf-based approach for real-time sub- way monitoring, in: Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Confer- ence on, IEEE. pp. I-1034.\n\nComplexity, pattern, and evolutionary trade-offs in animal aggregation. J K Parrish, L Edelstein-Keshet, Science. 284Parrish, J.K., Edelstein-Keshet, L., 1999. Complexity, pattern, and evo- lutionary trade-offs in animal aggregation. Science 284, 99-101.\n\nVisual domain adaptation: A survey of recent advances. V M Patel, R Gopalan, R Li, R Chellappa, IEEE Signal Processing Magazine. 32Patel, V.M., Gopalan, R., Li, R., Chellappa, R., 2015. Visual domain adaptation: A survey of recent advances. IEEE Signal Processing Mag- azine 32, 53-69.\n\nTask-based crowd simulation for heterogeneous architectures, in: Innovative Research and Applications in Next-Generation High Performance Computing. H Perez, B Hernandez, I Rudomin, E Ayguade, IGI GlobalPerez, H., Hernandez, B., Rudomin, I., Ayguade, E., 2016. Task-based crowd simulation for heterogeneous architectures, in: Innovative Re- search and Applications in Next-Generation High Performance Com- puting. IGI Global, pp. 194-219.\n\nCount forest: Co-voting uncertain number of targets using random forest for crowd density estimation. V Q Pham, T Kozakaya, O Yamaguchi, R Okada, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionPham, V.Q., Kozakaya, T., Yamaguchi, O., Okada, R., 2015. Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation, in: Proceedings of the IEEE International Conference on Computer Vision, pp. 3253-3261.\n\nHyperface: A deep multitask learning framework for face detection, landmark localization, pose estimation, and gender recognition. R Ranjan, V Patel, R Chellappa, IEEE transactions on PAMI. Ranjan, R., Patel, V., Chellappa, R., 2016. Hyperface: A deep multi- task learning framework for face detection, landmark localization, pose estimation, and gender recognition. IEEE transactions on PAMI .\n\nDensity-aware person detection and tracking in crowds. M Rodriguez, I Laptev, J Sivic, J Y Audibert, International Conference on Computer Vision, IEEERodriguez, M., Laptev, I., Sivic, J., Audibert, J.Y., 2011. Density-aware person detection and tracking in crowds, in: 2011 International Confer- ence on Computer Vision, IEEE. pp. 2423-2430.\n\nCrowd counting using multiple local features. D Ryan, S Denman, C Fookes, S Sridharan, Digital Image Computing: Techniques and Applications. DICTA'09., IEEERyan, D., Denman, S., Fookes, C., Sridharan, S., 2009. Crowd counting using multiple local features, in: Digital Image Computing: Techniques and Applications, 2009. DICTA'09., IEEE. pp. 81-88.\n\nAn evaluation of crowd counting methods, features and regression models. D Ryan, S Denman, S Sridharan, C Fookes, Computer Vision and Image Understanding. 130Ryan, D., Denman, S., Sridharan, S., Fookes, C., 2015. An evaluation of crowd counting methods, features and regression models. Computer Vision and Image Understanding 130, 1-17.\n\nDetecting pedestrians by learning shapelet features. P Sabzmeydani, G Mori, Computer Vision and Pattern Recognition. Sabzmeydani, P., Mori, G., 2007. Detecting pedestrians by learning shapelet features, in: Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on, IEEE. pp. 1-8.\n\nRecent survey on crowd density estimation and counting for visual surveillance. S A M Saleh, S A Suandi, H Ibrahim, Engineering Applications of Artificial Intelligence. 41Saleh, S.A.M., Suandi, S.A., Ibrahim, H., 2015. Recent survey on crowd density estimation and counting for visual surveillance. Engineering Applications of Artificial Intelligence 41, 103-114.\n\nSwitching convolutional neural network for crowd counting. D B Sam, S Surya, R V Babu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSam, D.B., Surya, S., Babu, R.V., 2017. Switching convolutional neural network for crowd counting, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n\nCrowd behavior recognition for video surveillance. S Saxena, F Br\u00e9mond, M Thonnat, R Ma, International Conference on Advanced Concepts for Intelligent Vision Systems. SpringerSaxena, S., Br\u00e9mond, F., Thonnat, M., Ma, R., 2008. Crowd behavior recognition for video surveillance, in: International Conference on Ad- vanced Concepts for Intelligent Vision Systems, Springer. pp. 970-981.\n\nConvolutional neural networks applied to house numbers digit classification. P Sermanet, S Chintala, Y Lecun, Pattern Recognition (ICPR), 2012 21st International Conference on, IEEE. Sermanet, P., Chintala, S., LeCun, Y., 2012. Convolutional neural net- works applied to house numbers digit classification, in: Pattern Recog- nition (ICPR), 2012 21st International Conference on, IEEE. pp. 3288- 3291.\n\nEnd-to-end crowd counting via joint learning local and global count. C Shang, H Ai, B Bai, Image Processing (ICIP). Shang, C., Ai, H., Bai, B., 2016. End-to-end crowd counting via joint learning local and global count, in: Image Processing (ICIP), 2016 IEEE International Conference on, IEEE. pp. 1215-1219.\n\nScene-independent group profiling in crowd. J Shao, C Loy, X Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShao, J., Change Loy, C., Wang, X., 2014. Scene-independent group profiling in crowd, in: Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pp. 2219-2226.\n\nDeeply learned attributes for crowded scene understanding. J Shao, K Kang, C C Loy, X Wang, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEShao, J., Kang, K., Loy, C.C., Wang, X., 2015. Deeply learned attributes for crowded scene understanding, in: 2015 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), IEEE. pp. 4657-4666.\n\nSlicing convolutional neural network for crowd video understanding. J Shao, C C Loy, K Kang, X Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShao, J., Loy, C.C., Kang, K., Wang, X., 2016. Slicing convolutional neural network for crowd video understanding, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5620-5628.\n\nCrowd counting via weighted vlad on dense attribute feature maps. B Sheng, C Shen, G Lin, J Li, W Yang, C Sun, IEEE Transactions on Circuits and Systems for Video Technology. Sheng, B., Shen, C., Lin, G., Li, J., Yang, W., Sun, C., 2016. Crowd counting via weighted vlad on dense attribute feature maps. IEEE Trans- actions on Circuits and Systems for Video Technology .\n\nCrowd psychology and engineering. J D Sime, Safety science. 21Sime, J.D., 1995. Crowd psychology and engineering. Safety science 21, 1-14.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintSimonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 .\n\nCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting, in: Advanced Video and Signal Based Surveillance (AVSS). V Sindagi, V Patel, IEEE. IEEESindagi, V., Patel, V., 2017. Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting, in: Ad- vanced Video and Signal Based Surveillance (AVSS), 2017 IEEE Inter- national Conference on, IEEE.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Er- han, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper with con- volutions, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9.\n\nPedestrian detection via classification on riemannian manifolds. O Tuzel, F Porikli, P Meer, 30Tuzel, O., Porikli, F., Meer, P., 2008. Pedestrian detection via classifi- cation on riemannian manifolds. IEEE transactions on pattern analysis and machine intelligence 30, 1713-1727.\n\nRobust real-time face detection. P Viola, M J Jones, International journal of computer vision. 57Viola, P., Jones, M.J., 2004. Robust real-time face detection. Interna- tional journal of computer vision 57, 137-154.\n\nDetecting pedestrians using patterns of motion and appearance. P Viola, M J Jones, D Snow, International Journal of Computer Vision. 63Viola, P., Jones, M.J., Snow, D., 2005. Detecting pedestrians using pat- terns of motion and appearance. International Journal of Computer Vi- sion 63, 153-161.\n\nLearning to count with cnn boosting. E Walach, L Wolf, European Conference on Computer Vision. SpringerWalach, E., Wolf, L., 2016. Learning to count with cnn boosting, in: European Conference on Computer Vision, Springer. pp. 660-676.\n\nDeep people counting in extremely dense crowds. C Wang, H Zhang, L Yang, S Liu, X Cao, Proceedings of the 23rd ACM international conference on Multimedia, ACM. the 23rd ACM international conference on Multimedia, ACMWang, C., Zhang, H., Yang, L., Liu, S., Cao, X., 2015. Deep people counting in extremely dense crowds, in: Proceedings of the 23rd ACM international conference on Multimedia, ACM. pp. 1299-1302.\n\nFast visual object counting via example-based density estimation. Y Wang, Y Zou, Image Processing (ICIP), 2016 IEEE International Conference on, IEEE. Wang, Y., Zou, Y., 2016. Fast visual object counting via example-based density estimation, in: Image Processing (ICIP), 2016 IEEE Interna- tional Conference on, IEEE. pp. 3653-3657.\n\nDetection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors. B Wu, R Nevatia, Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, IEEE. Wu, B., Nevatia, R., 2005. Detection of multiple, partially occluded hu- mans in a single image by bayesian combination of edgelet part detec- tors, in: Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, IEEE. pp. 90-97.\n\nDetection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors. B Wu, R Nevatia, International Journal of Computer Vision. 75Wu, B., Nevatia, R., 2007. Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detec- tors. International Journal of Computer Vision 75, 247-266.\n\nCrowd density estimation based on rich features and random projection forest. B Xu, G Qiu, 2016 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEEXu, B., Qiu, G., 2016. Crowd density estimation based on rich features and random projection forest, in: 2016 IEEE Winter Conference on Ap- plications of Computer Vision (WACV), IEEE. pp. 1-8.\n\nL0 regularized stationarytime estimation for crowd analysis. S Yi, X Wang, C Lu, J Jia, H Li, IEEE transactions on pattern analysis and machine intelligence. Yi, S., Wang, X., Lu, C., Jia, J., Li, H., 2016. L0 regularized stationary- time estimation for crowd analysis. IEEE transactions on pattern analy- sis and machine intelligence .\n\niprivacy: image privacy protection by identifying sensitive objects via deep multi-task learning. J Yu, B Zhang, Z Kuang, D Lin, J Fan, IEEE Transactions on Information Forensics and Security. 12Yu, J., Zhang, B., Kuang, Z., Lin, D., Fan, J., 2017. iprivacy: image privacy protection by identifying sensitive objects via deep multi-task learning. IEEE Transactions on Information Forensics and Security 12, 1005-1016.\n\nCrowd analysis: a survey. B Zhan, D N Monekosso, P Remagnino, S A Velastin, L Q Xu, Machine Vision and Applications. 19Zhan, B., Monekosso, D.N., Remagnino, P., Velastin, S.A., Xu, L.Q., 2008. Crowd analysis: a survey. Machine Vision and Applications 19, 345-357.\n\nDatadriven crowd understanding: A baseline for a large-scale crowd dataset. C Zhang, K Kang, H Li, X Wang, R Xie, X Yang, IEEE Transactions on Multimedia. 18Zhang, C., Kang, K., Li, H., Wang, X., Xie, R., Yang, X., 2016a. Data- driven crowd understanding: A baseline for a large-scale crowd dataset. IEEE Transactions on Multimedia 18, 1048-1061.\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhang, C., Li, H., Wang, X., Yang, X., 2015. Cross-scene crowd count- ing via deep convolutional neural networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 833-841.\n\nMulti-style generative network for real-time transfer. H Zhang, K Dana, arXiv preprintZhang, H., Dana, K., 2017. Multi-style generative network for real-time transfer. arXiv preprint .\n\nImage de-raining using a conditional generative adversarial network. H Zhang, V Sindagi, V M Patel, arXiv:1701.05957arXiv preprintZhang, H., Sindagi, V., Patel, V.M., 2017. Image de-raining us- ing a conditional generative adversarial network. arXiv preprint arXiv:1701.05957 .\n\nCollective motion and density fluctuations in bacterial colonies. H P Zhang, A Beer, E L Florin, H L Swinney, Proceedings of the National Academy of Sciences. 107Zhang, H.P., Beer, A., Florin, E.L., Swinney, H.L., 2010. Collective motion and density fluctuations in bacterial colonies. Proceedings of the National Academy of Sciences 107, 13626-13630.\n\nSingleimage crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhang, Y., Zhou, D., Chen, S., Gao, S., Ma, Y., 2016b. Single- image crowd counting via multi-column convolutional neural network, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 589-597.\n\nSegmentation and tracking of multiple humans in crowded environments. T Zhao, R Nevatia, B Wu, IEEE transactions. 30Zhao, T., Nevatia, R., Wu, B., 2008. Segmentation and tracking of mul- tiple humans in crowded environments. IEEE transactions on pattern analysis and machine intelligence 30, 1198-1211.\n\nCrossing-line crowd counting with two-phase deep neural networks. Z Zhao, H Li, R Zhao, X Wang, European Conference on Computer Vision. SpringerZhao, Z., Li, H., Zhao, R., Wang, X., 2016. Crossing-line crowd count- ing with two-phase deep neural networks, in: European Conference on Computer Vision, Springer. pp. 712-726.\n\nLearning collective crowd behaviors with dynamic pedestrian-agents. B Zhou, X Tang, X Wang, International Journal of Computer Vision. 111Zhou, B., Tang, X., Wang, X., 2015. Learning collective crowd behav- iors with dynamic pedestrian-agents. International Journal of Computer Vision 111, 50-68.\n\nUnderstanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents. B Zhou, X Wang, X Tang, Computer Vision and Pattern Recognition (CVPR). Zhou, B., Wang, X., Tang, X., 2012. Understanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents, in: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Confer- ence on, IEEE. pp. 2871-2878.\n\nCrowd tracking with dynamic evolution of group structures. F Zhu, X Wang, N Yu, European Conference on Computer Vision. SpringerZhu, F., Wang, X., Yu, N., 2014. Crowd tracking with dynamic evolu- tion of group structures, in: European Conference on Computer Vision, Springer. pp. 139-154.\n\nAdvances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques. M S Zitouni, H Bhaskar, J Dias, M E Al-Mualla, Neurocomputing. 186Zitouni, M.S., Bhaskar, H., Dias, J., Al-Mualla, M.E., 2016. Advances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques. Neurocomputing 186, 139-159.\n", "annotations": {"author": "[{\"end\":224,\"start\":123},{\"end\":320,\"start\":225}]", "publisher": null, "author_last_name": "[{\"end\":143,\"start\":136},{\"end\":239,\"start\":234}]", "author_first_name": "[{\"end\":133,\"start\":123},{\"end\":135,\"start\":134},{\"end\":231,\"start\":225},{\"end\":233,\"start\":232}]", "author_affiliation": "[{\"end\":223,\"start\":145},{\"end\":319,\"start\":241}]", "title": "[{\"end\":120,\"start\":1},{\"end\":440,\"start\":321}]", "venue": null, "abstract": "[{\"end\":3050,\"start\":478}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3586,\"start\":3582},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":3615,\"start\":3611},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":3619,\"start\":3615},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":4512,\"start\":4508},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4515,\"start\":4512},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":4542,\"start\":4537},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4545,\"start\":4542},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4569,\"start\":4565},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4572,\"start\":4569},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4596,\"start\":4593},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5011,\"start\":5008},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5013,\"start\":5011},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5169,\"start\":5165},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":5172,\"start\":5169},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":5377,\"start\":5373},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5379,\"start\":5377},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":5696,\"start\":5692},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":5699,\"start\":5696},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5998,\"start\":5995},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6393,\"start\":6389},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":6396,\"start\":6393},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6642,\"start\":6638},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6644,\"start\":6642},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6816,\"start\":6812},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6819,\"start\":6816},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6822,\"start\":6819},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6825,\"start\":6822},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6828,\"start\":6825},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":6831,\"start\":6828},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6834,\"start\":6831},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6837,\"start\":6834},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6862,\"start\":6858},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6865,\"start\":6862},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":6869,\"start\":6865},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":6873,\"start\":6869},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":6876,\"start\":6873},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":6879,\"start\":6876},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6882,\"start\":6879},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6901,\"start\":6897},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6904,\"start\":6901},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6928,\"start\":6925},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":6931,\"start\":6928},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6934,\"start\":6931},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":6938,\"start\":6934},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":6942,\"start\":6938},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":6946,\"start\":6942},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":6961,\"start\":6957},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":6965,\"start\":6961},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":6991,\"start\":6987},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":6995,\"start\":6991},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7022,\"start\":7018},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7025,\"start\":7022},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":7331,\"start\":7327},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":7334,\"start\":7331},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7337,\"start\":7334},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7340,\"start\":7337},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":7363,\"start\":7359},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7390,\"start\":7386},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":7394,\"start\":7390},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7640,\"start\":7636},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":8320,\"start\":8315},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8323,\"start\":8320},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8326,\"start\":8323},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8329,\"start\":8326},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":8333,\"start\":8329},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":8352,\"start\":8347},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8375,\"start\":8371},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8481,\"start\":8477},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":8683,\"start\":8678},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9082,\"start\":9078},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":9085,\"start\":9082},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":9088,\"start\":9085},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9105,\"start\":9101},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":9408,\"start\":9404},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":9614,\"start\":9610},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":10054,\"start\":10050},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10057,\"start\":10054},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":10061,\"start\":10057},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10064,\"start\":10061},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":10067,\"start\":10064},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":10070,\"start\":10067},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10072,\"start\":10070},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":10075,\"start\":10072},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":10079,\"start\":10075},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":10083,\"start\":10079},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":10086,\"start\":10083},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":10089,\"start\":10086},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":10142,\"start\":10137},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":10146,\"start\":10142},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":10150,\"start\":10146},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11331,\"start\":11327},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11334,\"start\":11331},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11337,\"start\":11334},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":11341,\"start\":11337},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":11345,\"start\":11341},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11361,\"start\":11357},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11364,\"start\":11361},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":11367,\"start\":11364},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11370,\"start\":11367},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":11387,\"start\":11383},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11872,\"start\":11868},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11875,\"start\":11872},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":11878,\"start\":11875},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":11881,\"start\":11878},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":11885,\"start\":11881},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12110,\"start\":12106},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12174,\"start\":12170},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12300,\"start\":12296},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12303,\"start\":12300},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":12306,\"start\":12303},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12309,\"start\":12306},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":12432,\"start\":12428},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12467,\"start\":12463},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":12482,\"start\":12477},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":12500,\"start\":12496},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":12604,\"start\":12600},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12627,\"start\":12623},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12893,\"start\":12889},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12896,\"start\":12893},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":12900,\"start\":12896},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":13052,\"start\":13048},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":13113,\"start\":13108},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13325,\"start\":13321},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13796,\"start\":13792},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":13799,\"start\":13796},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13802,\"start\":13799},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14410,\"start\":14406},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14413,\"start\":14410},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":14416,\"start\":14413},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":14812,\"start\":14808},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14846,\"start\":14842},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14869,\"start\":14865},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":14922,\"start\":14918},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15035,\"start\":15031},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16361,\"start\":16357},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17286,\"start\":17282},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":18088,\"start\":18084},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":18939,\"start\":18935},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":20232,\"start\":20227},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23541,\"start\":23537},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":23576,\"start\":23572},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":23613,\"start\":23608},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23660,\"start\":23656},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":23703,\"start\":23698},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":23754,\"start\":23750},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":23790,\"start\":23786},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":23832,\"start\":23828},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":23882,\"start\":23878},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23934,\"start\":23930},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":23978,\"start\":23974},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":24029,\"start\":24025},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24066,\"start\":24063},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":24107,\"start\":24102},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":24150,\"start\":24146},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":24195,\"start\":24191},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":24237,\"start\":24232},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":24284,\"start\":24280},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24303,\"start\":24299},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":24539,\"start\":24535},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":25149,\"start\":25145},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":25423,\"start\":25418},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":26972,\"start\":26967},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":26994,\"start\":26990},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27447,\"start\":27443},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":28596,\"start\":28592},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":29306,\"start\":29302},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":29867,\"start\":29863},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30069,\"start\":30065},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":30584,\"start\":30579},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30665,\"start\":30661},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30887,\"start\":30883},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":32458,\"start\":32454},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":32825,\"start\":32820},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":32828,\"start\":32825},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":33789,\"start\":33785},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":34119,\"start\":34114},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":34161,\"start\":34157},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":34573,\"start\":34568},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":35437,\"start\":35433},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":35707,\"start\":35703},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":36588,\"start\":36583},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":36591,\"start\":36588},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36612,\"start\":36608},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":37391,\"start\":37386},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":37394,\"start\":37391},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":37415,\"start\":37411},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":37556,\"start\":37551},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":37559,\"start\":37556},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":37638,\"start\":37634},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":38878,\"start\":38874},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":38882,\"start\":38878},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":38903,\"start\":38899},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":38927,\"start\":38923},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":39014,\"start\":39010},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":39040,\"start\":39036},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":39645,\"start\":39641},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":39789,\"start\":39785},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":40389,\"start\":40385},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":41050,\"start\":41046},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":41254,\"start\":41250},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":41276,\"start\":41273},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":41382,\"start\":41378},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":42007,\"start\":42003},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":42023,\"start\":42019},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":42039,\"start\":42035},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":42147,\"start\":42144},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":42917,\"start\":42912},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":43972,\"start\":43967},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":44454,\"start\":44450},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":44631,\"start\":44626},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":45090,\"start\":45085},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":45093,\"start\":45090},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":45096,\"start\":45093},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":45099,\"start\":45096},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":45102,\"start\":45099},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":45606,\"start\":45602},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":45998,\"start\":45994},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":46001,\"start\":45998},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":46128,\"start\":46124},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":46131,\"start\":46128},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":46255,\"start\":46251},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":47159,\"start\":47155},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":47162,\"start\":47159},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":47165,\"start\":47162},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":47169,\"start\":47165},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":47173,\"start\":47169},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":47363,\"start\":47359},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":48358,\"start\":48354},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":49431,\"start\":49427},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":51354,\"start\":51349},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":51480,\"start\":51476},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":51628,\"start\":51624},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":51647,\"start\":51643},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":51667,\"start\":51663},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":51824,\"start\":51820},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":51833,\"start\":51829},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":51943,\"start\":51938},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":51965,\"start\":51961},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":52183,\"start\":52179},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":52186,\"start\":52183},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":53028,\"start\":53024},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":53164,\"start\":53159},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":53761,\"start\":53756},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":53920,\"start\":53916},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":54344,\"start\":54340},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":55162,\"start\":55158},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":55342,\"start\":55337},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":55601,\"start\":55596},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":55625,\"start\":55620},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":55660,\"start\":55656},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":56151,\"start\":56146},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":56154,\"start\":56151},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":56184,\"start\":56180},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":58003,\"start\":57998},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":58083,\"start\":58079},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":58251,\"start\":58246},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":58305,\"start\":58301},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":58363,\"start\":58359},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":58451,\"start\":58447},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":58546,\"start\":58542},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":58618,\"start\":58614},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":58684,\"start\":58679}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":57539,\"start\":57431},{\"attributes\":{\"id\":\"fig_1\"},\"end\":57863,\"start\":57540},{\"attributes\":{\"id\":\"fig_2\"},\"end\":57923,\"start\":57864},{\"attributes\":{\"id\":\"fig_3\"},\"end\":58004,\"start\":57924},{\"attributes\":{\"id\":\"fig_4\"},\"end\":58084,\"start\":58005},{\"attributes\":{\"id\":\"fig_5\"},\"end\":58154,\"start\":58085},{\"attributes\":{\"id\":\"fig_6\"},\"end\":58252,\"start\":58155},{\"attributes\":{\"id\":\"fig_7\"},\"end\":58306,\"start\":58253},{\"attributes\":{\"id\":\"fig_8\"},\"end\":58364,\"start\":58307},{\"attributes\":{\"id\":\"fig_9\"},\"end\":58452,\"start\":58365},{\"attributes\":{\"id\":\"fig_10\"},\"end\":58547,\"start\":58453},{\"attributes\":{\"id\":\"fig_11\"},\"end\":58619,\"start\":58548},{\"attributes\":{\"id\":\"fig_12\"},\"end\":58721,\"start\":58620},{\"attributes\":{\"id\":\"fig_13\"},\"end\":58861,\"start\":58722},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":58922,\"start\":58862},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":60106,\"start\":58923},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":61885,\"start\":60107}]", "paragraph": "[{\"end\":6645,\"start\":3066},{\"end\":7400,\"start\":6647},{\"end\":8176,\"start\":7402},{\"end\":9798,\"start\":8178},{\"end\":10614,\"start\":9800},{\"end\":11202,\"start\":10616},{\"end\":11526,\"start\":11239},{\"end\":11604,\"start\":11528},{\"end\":11933,\"start\":11606},{\"end\":13397,\"start\":11964},{\"end\":14696,\"start\":13429},{\"end\":14993,\"start\":14698},{\"end\":16343,\"start\":14995},{\"end\":17023,\"start\":16345},{\"end\":18012,\"start\":17063},{\"end\":18889,\"start\":18014},{\"end\":20187,\"start\":18891},{\"end\":21109,\"start\":20189},{\"end\":21626,\"start\":21131},{\"end\":22555,\"start\":21628},{\"end\":22697,\"start\":22557},{\"end\":23319,\"start\":22699},{\"end\":23470,\"start\":23351},{\"end\":24266,\"start\":23492},{\"end\":25403,\"start\":24268},{\"end\":28513,\"start\":25405},{\"end\":29985,\"start\":28515},{\"end\":30543,\"start\":29987},{\"end\":36570,\"start\":30545},{\"end\":38788,\"start\":36572},{\"end\":41175,\"start\":38790},{\"end\":42818,\"start\":41177},{\"end\":43773,\"start\":42820},{\"end\":43952,\"start\":43788},{\"end\":44966,\"start\":43954},{\"end\":46075,\"start\":44968},{\"end\":46615,\"start\":46077},{\"end\":47315,\"start\":46640},{\"end\":48255,\"start\":47328},{\"end\":49392,\"start\":48257},{\"end\":50136,\"start\":49394},{\"end\":50416,\"start\":50162},{\"end\":50732,\"start\":50486},{\"end\":52410,\"start\":50734},{\"end\":56304,\"start\":52441},{\"end\":57430,\"start\":56319}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":50449,\"start\":50417},{\"attributes\":{\"id\":\"formula_1\"},\"end\":50485,\"start\":50449}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23194,\"start\":23187},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49873,\"start\":49866},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":50259,\"start\":50252},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":51156,\"start\":51149}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3064,\"start\":3052},{\"attributes\":{\"n\":\"2.\"},\"end\":11237,\"start\":11205},{\"attributes\":{\"n\":\"2.1.\"},\"end\":11962,\"start\":11936},{\"attributes\":{\"n\":\"2.2.\"},\"end\":13427,\"start\":13400},{\"attributes\":{\"n\":\"2.3.\"},\"end\":17061,\"start\":17026},{\"attributes\":{\"n\":\"3.\"},\"end\":21129,\"start\":21112},{\"attributes\":{\"n\":\"3.1.\"},\"end\":23349,\"start\":23322},{\"end\":23481,\"start\":23473},{\"end\":23490,\"start\":23484},{\"attributes\":{\"n\":\"4.\"},\"end\":43786,\"start\":43776},{\"attributes\":{\"n\":\"5.\"},\"end\":46638,\"start\":46618},{\"attributes\":{\"n\":\"5.1.\"},\"end\":47326,\"start\":47318},{\"attributes\":{\"n\":\"5.2.\"},\"end\":50160,\"start\":50139},{\"attributes\":{\"n\":\"6.\"},\"end\":52439,\"start\":52413},{\"attributes\":{\"n\":\"7.\"},\"end\":56317,\"start\":56307},{\"end\":57440,\"start\":57432},{\"end\":57549,\"start\":57541},{\"end\":57873,\"start\":57865},{\"end\":57933,\"start\":57925},{\"end\":58014,\"start\":58006},{\"end\":58094,\"start\":58086},{\"end\":58164,\"start\":58156},{\"end\":58262,\"start\":58254},{\"end\":58317,\"start\":58308},{\"end\":58375,\"start\":58366},{\"end\":58463,\"start\":58454},{\"end\":58558,\"start\":58549},{\"end\":58630,\"start\":58621},{\"end\":58732,\"start\":58723},{\"end\":58872,\"start\":58863},{\"end\":58933,\"start\":58924},{\"end\":60117,\"start\":60108}]", "table": "[{\"end\":60106,\"start\":59717},{\"end\":61885,\"start\":60568}]", "figure_caption": "[{\"end\":57539,\"start\":57442},{\"end\":57863,\"start\":57551},{\"end\":57923,\"start\":57875},{\"end\":58004,\"start\":57935},{\"end\":58084,\"start\":58016},{\"end\":58154,\"start\":58096},{\"end\":58252,\"start\":58166},{\"end\":58306,\"start\":58264},{\"end\":58364,\"start\":58320},{\"end\":58452,\"start\":58378},{\"end\":58547,\"start\":58466},{\"end\":58619,\"start\":58561},{\"end\":58721,\"start\":58633},{\"end\":58861,\"start\":58735},{\"end\":58922,\"start\":58874},{\"end\":59717,\"start\":58935},{\"end\":60568,\"start\":60119}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3323,\"start\":3317},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21528,\"start\":21522},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26300,\"start\":26294},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27561,\"start\":27555},{\"end\":28663,\"start\":28655},{\"end\":29793,\"start\":29787},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30512,\"start\":30506},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31129,\"start\":31123},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33128,\"start\":33122},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34469,\"start\":34462},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36868,\"start\":36861},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39935,\"start\":39928},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43242,\"start\":43234},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49828,\"start\":49821},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53390,\"start\":53383},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":55268,\"start\":55261}]", "bib_author_first_name": "[{\"end\":61976,\"start\":61975},{\"end\":61990,\"start\":61989},{\"end\":62004,\"start\":62003},{\"end\":62018,\"start\":62017},{\"end\":62351,\"start\":62350},{\"end\":62620,\"start\":62619},{\"end\":62622,\"start\":62621},{\"end\":62633,\"start\":62632},{\"end\":62635,\"start\":62634},{\"end\":62646,\"start\":62645},{\"end\":62648,\"start\":62647},{\"end\":62895,\"start\":62894},{\"end\":62905,\"start\":62904},{\"end\":62918,\"start\":62917},{\"end\":63184,\"start\":63183},{\"end\":63186,\"start\":63185},{\"end\":63418,\"start\":63417},{\"end\":63429,\"start\":63428},{\"end\":63440,\"start\":63439},{\"end\":63749,\"start\":63748},{\"end\":63751,\"start\":63750},{\"end\":63759,\"start\":63758},{\"end\":63761,\"start\":63760},{\"end\":63771,\"start\":63770},{\"end\":63773,\"start\":63772},{\"end\":64124,\"start\":64123},{\"end\":64136,\"start\":64135},{\"end\":64150,\"start\":64149},{\"end\":64514,\"start\":64513},{\"end\":64516,\"start\":64515},{\"end\":64526,\"start\":64525},{\"end\":64532,\"start\":64531},{\"end\":64534,\"start\":64533},{\"end\":64807,\"start\":64806},{\"end\":64976,\"start\":64975},{\"end\":64991,\"start\":64990},{\"end\":64993,\"start\":64992},{\"end\":65008,\"start\":65007},{\"end\":65010,\"start\":65009},{\"end\":65380,\"start\":65379},{\"end\":65382,\"start\":65381},{\"end\":65393,\"start\":65392},{\"end\":65752,\"start\":65751},{\"end\":65766,\"start\":65765},{\"end\":65779,\"start\":65778},{\"end\":66013,\"start\":66012},{\"end\":66023,\"start\":66022},{\"end\":66037,\"start\":66036},{\"end\":66039,\"start\":66038},{\"end\":66309,\"start\":66308},{\"end\":66311,\"start\":66310},{\"end\":66319,\"start\":66318},{\"end\":66323,\"start\":66320},{\"end\":66332,\"start\":66331},{\"end\":66672,\"start\":66671},{\"end\":66674,\"start\":66673},{\"end\":66682,\"start\":66681},{\"end\":66997,\"start\":66996},{\"end\":66999,\"start\":66998},{\"end\":67007,\"start\":67006},{\"end\":67302,\"start\":67301},{\"end\":67304,\"start\":67303},{\"end\":67312,\"start\":67311},{\"end\":67321,\"start\":67320},{\"end\":67331,\"start\":67330},{\"end\":67333,\"start\":67332},{\"end\":67342,\"start\":67341},{\"end\":67351,\"start\":67350},{\"end\":67686,\"start\":67685},{\"end\":67694,\"start\":67693},{\"end\":67702,\"start\":67701},{\"end\":67711,\"start\":67710},{\"end\":68123,\"start\":68122},{\"end\":68131,\"start\":68130},{\"end\":68133,\"start\":68132},{\"end\":68140,\"start\":68139},{\"end\":68148,\"start\":68147},{\"end\":68412,\"start\":68411},{\"end\":68420,\"start\":68419},{\"end\":68428,\"start\":68427},{\"end\":68864,\"start\":68863},{\"end\":68873,\"start\":68872},{\"end\":68880,\"start\":68879},{\"end\":68889,\"start\":68888},{\"end\":68896,\"start\":68895},{\"end\":69155,\"start\":69154},{\"end\":69157,\"start\":69156},{\"end\":69165,\"start\":69164},{\"end\":69167,\"start\":69166},{\"end\":69384,\"start\":69383},{\"end\":69395,\"start\":69394},{\"end\":69404,\"start\":69403},{\"end\":69751,\"start\":69750},{\"end\":69760,\"start\":69759},{\"end\":70134,\"start\":70133},{\"end\":70144,\"start\":70143},{\"end\":70153,\"start\":70152},{\"end\":70164,\"start\":70163},{\"end\":70435,\"start\":70434},{\"end\":70443,\"start\":70442},{\"end\":70459,\"start\":70458},{\"end\":70469,\"start\":70468},{\"end\":70789,\"start\":70788},{\"end\":70802,\"start\":70801},{\"end\":70804,\"start\":70803},{\"end\":71056,\"start\":71055},{\"end\":71058,\"start\":71057},{\"end\":71074,\"start\":71073},{\"end\":71076,\"start\":71075},{\"end\":71088,\"start\":71087},{\"end\":71102,\"start\":71101},{\"end\":71426,\"start\":71425},{\"end\":71438,\"start\":71437},{\"end\":71440,\"start\":71439},{\"end\":71709,\"start\":71708},{\"end\":71719,\"start\":71718},{\"end\":71729,\"start\":71728},{\"end\":71743,\"start\":71742},{\"end\":72069,\"start\":72068},{\"end\":72071,\"start\":72070},{\"end\":72290,\"start\":72289},{\"end\":72296,\"start\":72295},{\"end\":72302,\"start\":72301},{\"end\":72308,\"start\":72307},{\"end\":72315,\"start\":72314},{\"end\":72321,\"start\":72320},{\"end\":72643,\"start\":72642},{\"end\":72651,\"start\":72650},{\"end\":72658,\"start\":72657},{\"end\":72668,\"start\":72667},{\"end\":72680,\"start\":72679},{\"end\":72967,\"start\":72966},{\"end\":72973,\"start\":72972},{\"end\":72975,\"start\":72974},{\"end\":73266,\"start\":73265},{\"end\":73279,\"start\":73278},{\"end\":73291,\"start\":73290},{\"end\":73301,\"start\":73300},{\"end\":73558,\"start\":73557},{\"end\":73564,\"start\":73563},{\"end\":73573,\"start\":73572},{\"end\":73580,\"start\":73579},{\"end\":73916,\"start\":73915},{\"end\":73918,\"start\":73917},{\"end\":74148,\"start\":74147},{\"end\":74154,\"start\":74153},{\"end\":74163,\"start\":74162},{\"end\":74171,\"start\":74170},{\"end\":74179,\"start\":74178},{\"end\":74553,\"start\":74552},{\"end\":74562,\"start\":74561},{\"end\":74570,\"start\":74569},{\"end\":74578,\"start\":74577},{\"end\":74933,\"start\":74932},{\"end\":74943,\"start\":74942},{\"end\":74954,\"start\":74953},{\"end\":74965,\"start\":74964},{\"end\":75401,\"start\":75400},{\"end\":75410,\"start\":75409},{\"end\":75412,\"start\":75411},{\"end\":75419,\"start\":75418},{\"end\":75427,\"start\":75426},{\"end\":75429,\"start\":75428},{\"end\":75850,\"start\":75849},{\"end\":75861,\"start\":75860},{\"end\":75870,\"start\":75869},{\"end\":76159,\"start\":76158},{\"end\":76165,\"start\":76160},{\"end\":76175,\"start\":76174},{\"end\":76177,\"start\":76176},{\"end\":76186,\"start\":76185},{\"end\":76188,\"start\":76187},{\"end\":76479,\"start\":76478},{\"end\":76487,\"start\":76486},{\"end\":76493,\"start\":76492},{\"end\":76495,\"start\":76494},{\"end\":76769,\"start\":76768},{\"end\":76777,\"start\":76776},{\"end\":77028,\"start\":77027},{\"end\":77030,\"start\":77029},{\"end\":77040,\"start\":77039},{\"end\":77042,\"start\":77041},{\"end\":77350,\"start\":77349},{\"end\":77681,\"start\":77680},{\"end\":77695,\"start\":77694},{\"end\":77708,\"start\":77707},{\"end\":77710,\"start\":77709},{\"end\":78070,\"start\":78069},{\"end\":78081,\"start\":78080},{\"end\":78090,\"start\":78089},{\"end\":78357,\"start\":78356},{\"end\":78366,\"start\":78365},{\"end\":78377,\"start\":78376},{\"end\":78729,\"start\":78728},{\"end\":78742,\"start\":78741},{\"end\":79001,\"start\":79000},{\"end\":79007,\"start\":79006},{\"end\":79442,\"start\":79441},{\"end\":79448,\"start\":79447},{\"end\":79457,\"start\":79456},{\"end\":79466,\"start\":79465},{\"end\":79819,\"start\":79818},{\"end\":79825,\"start\":79824},{\"end\":79834,\"start\":79833},{\"end\":79842,\"start\":79841},{\"end\":79848,\"start\":79847},{\"end\":79856,\"start\":79855},{\"end\":80156,\"start\":80155},{\"end\":80162,\"start\":80161},{\"end\":80175,\"start\":80174},{\"end\":80450,\"start\":80449},{\"end\":80452,\"start\":80451},{\"end\":80459,\"start\":80458},{\"end\":80461,\"start\":80460},{\"end\":80469,\"start\":80468},{\"end\":80471,\"start\":80470},{\"end\":80832,\"start\":80831},{\"end\":80834,\"start\":80833},{\"end\":80844,\"start\":80843},{\"end\":80846,\"start\":80845},{\"end\":80860,\"start\":80859},{\"end\":80871,\"start\":80870},{\"end\":80873,\"start\":80872},{\"end\":80883,\"start\":80882},{\"end\":80890,\"start\":80889},{\"end\":80899,\"start\":80898},{\"end\":80905,\"start\":80904},{\"end\":80911,\"start\":80910},{\"end\":80913,\"start\":80912},{\"end\":80922,\"start\":80921},{\"end\":80924,\"start\":80923},{\"end\":81209,\"start\":81208},{\"end\":81216,\"start\":81215},{\"end\":81575,\"start\":81574},{\"end\":81583,\"start\":81582},{\"end\":81596,\"start\":81595},{\"end\":82061,\"start\":82060},{\"end\":82063,\"start\":82062},{\"end\":82070,\"start\":82069},{\"end\":82078,\"start\":82077},{\"end\":82086,\"start\":82085},{\"end\":82406,\"start\":82405},{\"end\":82412,\"start\":82411},{\"end\":82414,\"start\":82413},{\"end\":82422,\"start\":82421},{\"end\":82430,\"start\":82429},{\"end\":82750,\"start\":82749},{\"end\":82763,\"start\":82762},{\"end\":82769,\"start\":82768},{\"end\":82781,\"start\":82780},{\"end\":82978,\"start\":82977},{\"end\":82988,\"start\":82987},{\"end\":82992,\"start\":82989},{\"end\":83001,\"start\":83000},{\"end\":83011,\"start\":83010},{\"end\":83425,\"start\":83424},{\"end\":83436,\"start\":83435},{\"end\":83449,\"start\":83448},{\"end\":83459,\"start\":83458},{\"end\":83461,\"start\":83460},{\"end\":83870,\"start\":83869},{\"end\":83881,\"start\":83880},{\"end\":83895,\"start\":83894},{\"end\":83905,\"start\":83904},{\"end\":83907,\"start\":83906},{\"end\":84248,\"start\":84247},{\"end\":84250,\"start\":84249},{\"end\":84261,\"start\":84260},{\"end\":84263,\"start\":84262},{\"end\":84271,\"start\":84270},{\"end\":84273,\"start\":84272},{\"end\":84520,\"start\":84519},{\"end\":84532,\"start\":84531},{\"end\":84542,\"start\":84541},{\"end\":84553,\"start\":84552},{\"end\":84564,\"start\":84563},{\"end\":84864,\"start\":84863},{\"end\":84866,\"start\":84865},{\"end\":84878,\"start\":84877},{\"end\":84890,\"start\":84889},{\"end\":84892,\"start\":84891},{\"end\":84901,\"start\":84900},{\"end\":85247,\"start\":85246},{\"end\":85262,\"start\":85261},{\"end\":85264,\"start\":85263},{\"end\":85556,\"start\":85555},{\"end\":85568,\"start\":85567},{\"end\":85949,\"start\":85948},{\"end\":85951,\"start\":85950},{\"end\":85962,\"start\":85961},{\"end\":86188,\"start\":86187},{\"end\":86190,\"start\":86189},{\"end\":86199,\"start\":86198},{\"end\":86210,\"start\":86209},{\"end\":86216,\"start\":86215},{\"end\":86569,\"start\":86568},{\"end\":86578,\"start\":86577},{\"end\":86591,\"start\":86590},{\"end\":86602,\"start\":86601},{\"end\":86962,\"start\":86961},{\"end\":86964,\"start\":86963},{\"end\":86972,\"start\":86971},{\"end\":86984,\"start\":86983},{\"end\":86997,\"start\":86996},{\"end\":87507,\"start\":87506},{\"end\":87517,\"start\":87516},{\"end\":87526,\"start\":87525},{\"end\":87827,\"start\":87826},{\"end\":87840,\"start\":87839},{\"end\":87850,\"start\":87849},{\"end\":87859,\"start\":87858},{\"end\":87861,\"start\":87860},{\"end\":88161,\"start\":88160},{\"end\":88169,\"start\":88168},{\"end\":88179,\"start\":88178},{\"end\":88189,\"start\":88188},{\"end\":88538,\"start\":88537},{\"end\":88546,\"start\":88545},{\"end\":88556,\"start\":88555},{\"end\":88569,\"start\":88568},{\"end\":88856,\"start\":88855},{\"end\":88871,\"start\":88870},{\"end\":89182,\"start\":89181},{\"end\":89186,\"start\":89183},{\"end\":89195,\"start\":89194},{\"end\":89197,\"start\":89196},{\"end\":89207,\"start\":89206},{\"end\":89526,\"start\":89525},{\"end\":89528,\"start\":89527},{\"end\":89535,\"start\":89534},{\"end\":89544,\"start\":89543},{\"end\":89546,\"start\":89545},{\"end\":89929,\"start\":89928},{\"end\":89939,\"start\":89938},{\"end\":89950,\"start\":89949},{\"end\":89961,\"start\":89960},{\"end\":90341,\"start\":90340},{\"end\":90353,\"start\":90352},{\"end\":90365,\"start\":90364},{\"end\":90736,\"start\":90735},{\"end\":90745,\"start\":90744},{\"end\":90751,\"start\":90750},{\"end\":91020,\"start\":91019},{\"end\":91028,\"start\":91027},{\"end\":91035,\"start\":91034},{\"end\":91430,\"start\":91429},{\"end\":91438,\"start\":91437},{\"end\":91446,\"start\":91445},{\"end\":91448,\"start\":91447},{\"end\":91455,\"start\":91454},{\"end\":91813,\"start\":91812},{\"end\":91821,\"start\":91820},{\"end\":91823,\"start\":91822},{\"end\":91830,\"start\":91829},{\"end\":91838,\"start\":91837},{\"end\":92267,\"start\":92266},{\"end\":92276,\"start\":92275},{\"end\":92284,\"start\":92283},{\"end\":92291,\"start\":92290},{\"end\":92297,\"start\":92296},{\"end\":92305,\"start\":92304},{\"end\":92607,\"start\":92606},{\"end\":92609,\"start\":92608},{\"end\":92781,\"start\":92780},{\"end\":92793,\"start\":92792},{\"end\":93131,\"start\":93130},{\"end\":93142,\"start\":93141},{\"end\":93432,\"start\":93431},{\"end\":93443,\"start\":93442},{\"end\":93450,\"start\":93449},{\"end\":93457,\"start\":93456},{\"end\":93469,\"start\":93468},{\"end\":93477,\"start\":93476},{\"end\":93489,\"start\":93488},{\"end\":93498,\"start\":93497},{\"end\":93511,\"start\":93510},{\"end\":93977,\"start\":93976},{\"end\":93986,\"start\":93985},{\"end\":93997,\"start\":93996},{\"end\":94226,\"start\":94225},{\"end\":94235,\"start\":94234},{\"end\":94237,\"start\":94236},{\"end\":94473,\"start\":94472},{\"end\":94482,\"start\":94481},{\"end\":94484,\"start\":94483},{\"end\":94493,\"start\":94492},{\"end\":94744,\"start\":94743},{\"end\":94754,\"start\":94753},{\"end\":94991,\"start\":94990},{\"end\":94999,\"start\":94998},{\"end\":95008,\"start\":95007},{\"end\":95016,\"start\":95015},{\"end\":95023,\"start\":95022},{\"end\":95421,\"start\":95420},{\"end\":95429,\"start\":95428},{\"end\":95807,\"start\":95806},{\"end\":95813,\"start\":95812},{\"end\":96268,\"start\":96267},{\"end\":96274,\"start\":96273},{\"end\":96610,\"start\":96609},{\"end\":96616,\"start\":96615},{\"end\":96953,\"start\":96952},{\"end\":96959,\"start\":96958},{\"end\":96967,\"start\":96966},{\"end\":96973,\"start\":96972},{\"end\":96980,\"start\":96979},{\"end\":97328,\"start\":97327},{\"end\":97334,\"start\":97333},{\"end\":97343,\"start\":97342},{\"end\":97352,\"start\":97351},{\"end\":97359,\"start\":97358},{\"end\":97675,\"start\":97674},{\"end\":97683,\"start\":97682},{\"end\":97685,\"start\":97684},{\"end\":97698,\"start\":97697},{\"end\":97711,\"start\":97710},{\"end\":97713,\"start\":97712},{\"end\":97725,\"start\":97724},{\"end\":97727,\"start\":97726},{\"end\":97990,\"start\":97989},{\"end\":97999,\"start\":97998},{\"end\":98007,\"start\":98006},{\"end\":98013,\"start\":98012},{\"end\":98021,\"start\":98020},{\"end\":98028,\"start\":98027},{\"end\":98329,\"start\":98328},{\"end\":98338,\"start\":98337},{\"end\":98344,\"start\":98343},{\"end\":98352,\"start\":98351},{\"end\":98767,\"start\":98766},{\"end\":98776,\"start\":98775},{\"end\":98967,\"start\":98966},{\"end\":98976,\"start\":98975},{\"end\":98987,\"start\":98986},{\"end\":98989,\"start\":98988},{\"end\":99243,\"start\":99242},{\"end\":99245,\"start\":99244},{\"end\":99254,\"start\":99253},{\"end\":99262,\"start\":99261},{\"end\":99264,\"start\":99263},{\"end\":99274,\"start\":99273},{\"end\":99276,\"start\":99275},{\"end\":99604,\"start\":99603},{\"end\":99613,\"start\":99612},{\"end\":99621,\"start\":99620},{\"end\":99629,\"start\":99628},{\"end\":99636,\"start\":99635},{\"end\":100081,\"start\":100080},{\"end\":100089,\"start\":100088},{\"end\":100100,\"start\":100099},{\"end\":100381,\"start\":100380},{\"end\":100389,\"start\":100388},{\"end\":100395,\"start\":100394},{\"end\":100403,\"start\":100402},{\"end\":100707,\"start\":100706},{\"end\":100715,\"start\":100714},{\"end\":100723,\"start\":100722},{\"end\":101033,\"start\":101032},{\"end\":101041,\"start\":101040},{\"end\":101049,\"start\":101048},{\"end\":101398,\"start\":101397},{\"end\":101405,\"start\":101404},{\"end\":101413,\"start\":101412},{\"end\":101741,\"start\":101740},{\"end\":101743,\"start\":101742},{\"end\":101754,\"start\":101753},{\"end\":101765,\"start\":101764},{\"end\":101773,\"start\":101772},{\"end\":101775,\"start\":101774}]", "bib_author_last_name": "[{\"end\":61987,\"start\":61977},{\"end\":62001,\"start\":61991},{\"end\":62015,\"start\":62005},{\"end\":62027,\"start\":62019},{\"end\":62362,\"start\":62352},{\"end\":62630,\"start\":62623},{\"end\":62643,\"start\":62636},{\"end\":62655,\"start\":62649},{\"end\":62902,\"start\":62896},{\"end\":62915,\"start\":62906},{\"end\":62928,\"start\":62919},{\"end\":63192,\"start\":63187},{\"end\":63426,\"start\":63419},{\"end\":63437,\"start\":63430},{\"end\":63448,\"start\":63441},{\"end\":63756,\"start\":63752},{\"end\":63768,\"start\":63762},{\"end\":63779,\"start\":63774},{\"end\":64133,\"start\":64125},{\"end\":64147,\"start\":64137},{\"end\":64158,\"start\":64151},{\"end\":64523,\"start\":64517},{\"end\":64529,\"start\":64527},{\"end\":64539,\"start\":64535},{\"end\":64814,\"start\":64808},{\"end\":64988,\"start\":64977},{\"end\":65005,\"start\":64994},{\"end\":65015,\"start\":65011},{\"end\":65390,\"start\":65383},{\"end\":65401,\"start\":65394},{\"end\":65763,\"start\":65753},{\"end\":65776,\"start\":65767},{\"end\":65786,\"start\":65780},{\"end\":66020,\"start\":66014},{\"end\":66034,\"start\":66024},{\"end\":66046,\"start\":66040},{\"end\":66316,\"start\":66312},{\"end\":66329,\"start\":66324},{\"end\":66344,\"start\":66333},{\"end\":66679,\"start\":66675},{\"end\":66694,\"start\":66683},{\"end\":67004,\"start\":67000},{\"end\":67019,\"start\":67008},{\"end\":67309,\"start\":67305},{\"end\":67318,\"start\":67313},{\"end\":67328,\"start\":67322},{\"end\":67339,\"start\":67334},{\"end\":67348,\"start\":67343},{\"end\":67361,\"start\":67352},{\"end\":67691,\"start\":67687},{\"end\":67699,\"start\":67695},{\"end\":67708,\"start\":67703},{\"end\":67715,\"start\":67712},{\"end\":68128,\"start\":68124},{\"end\":68137,\"start\":68134},{\"end\":68145,\"start\":68141},{\"end\":68154,\"start\":68149},{\"end\":68417,\"start\":68413},{\"end\":68425,\"start\":68421},{\"end\":68438,\"start\":68429},{\"end\":68870,\"start\":68865},{\"end\":68877,\"start\":68874},{\"end\":68886,\"start\":68881},{\"end\":68893,\"start\":68890},{\"end\":68901,\"start\":68897},{\"end\":69162,\"start\":69158},{\"end\":69170,\"start\":69168},{\"end\":69392,\"start\":69385},{\"end\":69401,\"start\":69396},{\"end\":69416,\"start\":69405},{\"end\":69757,\"start\":69752},{\"end\":69767,\"start\":69761},{\"end\":70141,\"start\":70135},{\"end\":70150,\"start\":70145},{\"end\":70161,\"start\":70154},{\"end\":70171,\"start\":70165},{\"end\":70440,\"start\":70436},{\"end\":70456,\"start\":70444},{\"end\":70466,\"start\":70460},{\"end\":70478,\"start\":70470},{\"end\":70799,\"start\":70790},{\"end\":70812,\"start\":70805},{\"end\":71071,\"start\":71059},{\"end\":71085,\"start\":71077},{\"end\":71099,\"start\":71089},{\"end\":71110,\"start\":71103},{\"end\":71435,\"start\":71427},{\"end\":71446,\"start\":71441},{\"end\":71716,\"start\":71710},{\"end\":71726,\"start\":71720},{\"end\":71740,\"start\":71730},{\"end\":71750,\"start\":71744},{\"end\":72080,\"start\":72072},{\"end\":72293,\"start\":72291},{\"end\":72299,\"start\":72297},{\"end\":72305,\"start\":72303},{\"end\":72312,\"start\":72309},{\"end\":72318,\"start\":72316},{\"end\":72325,\"start\":72322},{\"end\":72648,\"start\":72644},{\"end\":72655,\"start\":72652},{\"end\":72665,\"start\":72659},{\"end\":72677,\"start\":72669},{\"end\":72690,\"start\":72681},{\"end\":72970,\"start\":72968},{\"end\":72983,\"start\":72976},{\"end\":73276,\"start\":73267},{\"end\":73288,\"start\":73280},{\"end\":73298,\"start\":73292},{\"end\":73310,\"start\":73302},{\"end\":73561,\"start\":73559},{\"end\":73570,\"start\":73565},{\"end\":73577,\"start\":73574},{\"end\":73584,\"start\":73581},{\"end\":73928,\"start\":73919},{\"end\":74151,\"start\":74149},{\"end\":74160,\"start\":74155},{\"end\":74168,\"start\":74164},{\"end\":74176,\"start\":74172},{\"end\":74182,\"start\":74180},{\"end\":74559,\"start\":74554},{\"end\":74567,\"start\":74563},{\"end\":74575,\"start\":74571},{\"end\":74583,\"start\":74579},{\"end\":74940,\"start\":74934},{\"end\":74951,\"start\":74944},{\"end\":74962,\"start\":74955},{\"end\":74970,\"start\":74966},{\"end\":75407,\"start\":75402},{\"end\":75416,\"start\":75413},{\"end\":75424,\"start\":75420},{\"end\":75435,\"start\":75430},{\"end\":75858,\"start\":75851},{\"end\":75867,\"start\":75862},{\"end\":75878,\"start\":75871},{\"end\":76172,\"start\":76166},{\"end\":76183,\"start\":76178},{\"end\":76193,\"start\":76189},{\"end\":76484,\"start\":76480},{\"end\":76490,\"start\":76488},{\"end\":76500,\"start\":76496},{\"end\":76774,\"start\":76770},{\"end\":76782,\"start\":76778},{\"end\":77037,\"start\":77031},{\"end\":77047,\"start\":77043},{\"end\":77353,\"start\":77351},{\"end\":77692,\"start\":77682},{\"end\":77705,\"start\":77696},{\"end\":77717,\"start\":77711},{\"end\":78078,\"start\":78071},{\"end\":78087,\"start\":78082},{\"end\":78097,\"start\":78091},{\"end\":78363,\"start\":78358},{\"end\":78374,\"start\":78367},{\"end\":78385,\"start\":78378},{\"end\":78739,\"start\":78730},{\"end\":78752,\"start\":78743},{\"end\":79004,\"start\":79002},{\"end\":79010,\"start\":79008},{\"end\":79445,\"start\":79443},{\"end\":79454,\"start\":79449},{\"end\":79463,\"start\":79458},{\"end\":79470,\"start\":79467},{\"end\":79822,\"start\":79820},{\"end\":79831,\"start\":79826},{\"end\":79839,\"start\":79835},{\"end\":79845,\"start\":79843},{\"end\":79853,\"start\":79849},{\"end\":79860,\"start\":79857},{\"end\":80159,\"start\":80157},{\"end\":80172,\"start\":80163},{\"end\":80187,\"start\":80176},{\"end\":80456,\"start\":80453},{\"end\":80466,\"start\":80462},{\"end\":80476,\"start\":80472},{\"end\":80841,\"start\":80835},{\"end\":80857,\"start\":80847},{\"end\":80868,\"start\":80861},{\"end\":80880,\"start\":80874},{\"end\":80887,\"start\":80884},{\"end\":80896,\"start\":80891},{\"end\":80902,\"start\":80900},{\"end\":80908,\"start\":80906},{\"end\":80919,\"start\":80914},{\"end\":80931,\"start\":80925},{\"end\":81213,\"start\":81210},{\"end\":81228,\"start\":81217},{\"end\":81580,\"start\":81576},{\"end\":81593,\"start\":81584},{\"end\":81604,\"start\":81597},{\"end\":82067,\"start\":82064},{\"end\":82075,\"start\":82071},{\"end\":82083,\"start\":82079},{\"end\":82092,\"start\":82087},{\"end\":82409,\"start\":82407},{\"end\":82419,\"start\":82415},{\"end\":82427,\"start\":82423},{\"end\":82435,\"start\":82431},{\"end\":82760,\"start\":82751},{\"end\":82766,\"start\":82764},{\"end\":82778,\"start\":82770},{\"end\":82793,\"start\":82782},{\"end\":82985,\"start\":82979},{\"end\":82998,\"start\":82993},{\"end\":83008,\"start\":83002},{\"end\":83020,\"start\":83012},{\"end\":83433,\"start\":83426},{\"end\":83446,\"start\":83437},{\"end\":83456,\"start\":83450},{\"end\":83475,\"start\":83462},{\"end\":83878,\"start\":83871},{\"end\":83892,\"start\":83882},{\"end\":83902,\"start\":83896},{\"end\":83921,\"start\":83908},{\"end\":84258,\"start\":84251},{\"end\":84268,\"start\":84264},{\"end\":84280,\"start\":84274},{\"end\":84529,\"start\":84521},{\"end\":84539,\"start\":84533},{\"end\":84550,\"start\":84543},{\"end\":84561,\"start\":84554},{\"end\":84574,\"start\":84565},{\"end\":84875,\"start\":84867},{\"end\":84887,\"start\":84879},{\"end\":84898,\"start\":84893},{\"end\":84908,\"start\":84902},{\"end\":85259,\"start\":85248},{\"end\":85277,\"start\":85265},{\"end\":85565,\"start\":85557},{\"end\":85575,\"start\":85569},{\"end\":85959,\"start\":85952},{\"end\":85979,\"start\":85963},{\"end\":86196,\"start\":86191},{\"end\":86207,\"start\":86200},{\"end\":86213,\"start\":86211},{\"end\":86226,\"start\":86217},{\"end\":86575,\"start\":86570},{\"end\":86588,\"start\":86579},{\"end\":86599,\"start\":86592},{\"end\":86610,\"start\":86603},{\"end\":86969,\"start\":86965},{\"end\":86981,\"start\":86973},{\"end\":86994,\"start\":86985},{\"end\":87003,\"start\":86998},{\"end\":87514,\"start\":87508},{\"end\":87523,\"start\":87518},{\"end\":87536,\"start\":87527},{\"end\":87837,\"start\":87828},{\"end\":87847,\"start\":87841},{\"end\":87856,\"start\":87851},{\"end\":87870,\"start\":87862},{\"end\":88166,\"start\":88162},{\"end\":88176,\"start\":88170},{\"end\":88186,\"start\":88180},{\"end\":88199,\"start\":88190},{\"end\":88543,\"start\":88539},{\"end\":88553,\"start\":88547},{\"end\":88566,\"start\":88557},{\"end\":88576,\"start\":88570},{\"end\":88868,\"start\":88857},{\"end\":88876,\"start\":88872},{\"end\":89192,\"start\":89187},{\"end\":89204,\"start\":89198},{\"end\":89215,\"start\":89208},{\"end\":89532,\"start\":89529},{\"end\":89541,\"start\":89536},{\"end\":89551,\"start\":89547},{\"end\":89936,\"start\":89930},{\"end\":89947,\"start\":89940},{\"end\":89958,\"start\":89951},{\"end\":89964,\"start\":89962},{\"end\":90350,\"start\":90342},{\"end\":90362,\"start\":90354},{\"end\":90371,\"start\":90366},{\"end\":90742,\"start\":90737},{\"end\":90748,\"start\":90746},{\"end\":90755,\"start\":90752},{\"end\":91025,\"start\":91021},{\"end\":91032,\"start\":91029},{\"end\":91040,\"start\":91036},{\"end\":91435,\"start\":91431},{\"end\":91443,\"start\":91439},{\"end\":91452,\"start\":91449},{\"end\":91460,\"start\":91456},{\"end\":91818,\"start\":91814},{\"end\":91827,\"start\":91824},{\"end\":91835,\"start\":91831},{\"end\":91843,\"start\":91839},{\"end\":92273,\"start\":92268},{\"end\":92281,\"start\":92277},{\"end\":92288,\"start\":92285},{\"end\":92294,\"start\":92292},{\"end\":92302,\"start\":92298},{\"end\":92309,\"start\":92306},{\"end\":92614,\"start\":92610},{\"end\":92790,\"start\":92782},{\"end\":92803,\"start\":92794},{\"end\":93139,\"start\":93132},{\"end\":93148,\"start\":93143},{\"end\":93440,\"start\":93433},{\"end\":93447,\"start\":93444},{\"end\":93454,\"start\":93451},{\"end\":93466,\"start\":93458},{\"end\":93474,\"start\":93470},{\"end\":93486,\"start\":93478},{\"end\":93495,\"start\":93490},{\"end\":93508,\"start\":93499},{\"end\":93522,\"start\":93512},{\"end\":93983,\"start\":93978},{\"end\":93994,\"start\":93987},{\"end\":94002,\"start\":93998},{\"end\":94232,\"start\":94227},{\"end\":94243,\"start\":94238},{\"end\":94479,\"start\":94474},{\"end\":94490,\"start\":94485},{\"end\":94498,\"start\":94494},{\"end\":94751,\"start\":94745},{\"end\":94759,\"start\":94755},{\"end\":94996,\"start\":94992},{\"end\":95005,\"start\":95000},{\"end\":95013,\"start\":95009},{\"end\":95020,\"start\":95017},{\"end\":95027,\"start\":95024},{\"end\":95426,\"start\":95422},{\"end\":95433,\"start\":95430},{\"end\":95810,\"start\":95808},{\"end\":95821,\"start\":95814},{\"end\":96271,\"start\":96269},{\"end\":96282,\"start\":96275},{\"end\":96613,\"start\":96611},{\"end\":96620,\"start\":96617},{\"end\":96956,\"start\":96954},{\"end\":96964,\"start\":96960},{\"end\":96970,\"start\":96968},{\"end\":96977,\"start\":96974},{\"end\":96983,\"start\":96981},{\"end\":97331,\"start\":97329},{\"end\":97340,\"start\":97335},{\"end\":97349,\"start\":97344},{\"end\":97356,\"start\":97353},{\"end\":97363,\"start\":97360},{\"end\":97680,\"start\":97676},{\"end\":97695,\"start\":97686},{\"end\":97708,\"start\":97699},{\"end\":97722,\"start\":97714},{\"end\":97730,\"start\":97728},{\"end\":97996,\"start\":97991},{\"end\":98004,\"start\":98000},{\"end\":98010,\"start\":98008},{\"end\":98018,\"start\":98014},{\"end\":98025,\"start\":98022},{\"end\":98033,\"start\":98029},{\"end\":98335,\"start\":98330},{\"end\":98341,\"start\":98339},{\"end\":98349,\"start\":98345},{\"end\":98357,\"start\":98353},{\"end\":98773,\"start\":98768},{\"end\":98781,\"start\":98777},{\"end\":98973,\"start\":98968},{\"end\":98984,\"start\":98977},{\"end\":98995,\"start\":98990},{\"end\":99251,\"start\":99246},{\"end\":99259,\"start\":99255},{\"end\":99271,\"start\":99265},{\"end\":99284,\"start\":99277},{\"end\":99610,\"start\":99605},{\"end\":99618,\"start\":99614},{\"end\":99626,\"start\":99622},{\"end\":99633,\"start\":99630},{\"end\":99639,\"start\":99637},{\"end\":100086,\"start\":100082},{\"end\":100097,\"start\":100090},{\"end\":100103,\"start\":100101},{\"end\":100386,\"start\":100382},{\"end\":100392,\"start\":100390},{\"end\":100400,\"start\":100396},{\"end\":100408,\"start\":100404},{\"end\":100712,\"start\":100708},{\"end\":100720,\"start\":100716},{\"end\":100728,\"start\":100724},{\"end\":101038,\"start\":101034},{\"end\":101046,\"start\":101042},{\"end\":101054,\"start\":101050},{\"end\":101402,\"start\":101399},{\"end\":101410,\"start\":101406},{\"end\":101416,\"start\":101414},{\"end\":101751,\"start\":101744},{\"end\":101762,\"start\":101755},{\"end\":101770,\"start\":101766},{\"end\":101785,\"start\":101776}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":20569308},\"end\":62286,\"start\":61887},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":110430735},\"end\":62517,\"start\":62288},{\"attributes\":{\"doi\":\"arXiv:1303.4692\",\"id\":\"b2\"},\"end\":62870,\"start\":62519},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":40576626},\"end\":63114,\"start\":62872},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":148892706},\"end\":63313,\"start\":63116},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12791717},\"end\":63668,\"start\":63315},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5968923},\"end\":64040,\"start\":63670},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207565051},\"end\":64404,\"start\":64042},{\"attributes\":{\"doi\":\"App. 14/279\",\"id\":\"b8\"},\"end\":64739,\"start\":64406},{\"attributes\":{\"id\":\"b9\"},\"end\":64908,\"start\":64741},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":697405},\"end\":65312,\"start\":64910},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11636674},\"end\":65709,\"start\":65314},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":118376889},\"end\":65943,\"start\":65711},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6115769},\"end\":66218,\"start\":65945},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9059102},\"end\":66621,\"start\":66220},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":749620},\"end\":66929,\"start\":66623},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1398960},\"end\":67216,\"start\":66931},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":16523471},\"end\":67618,\"start\":67218},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8747356},\"end\":68075,\"start\":67620},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1910869},\"end\":68335,\"start\":68077},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2614408},\"end\":68793,\"start\":68337},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6864924},\"end\":69076,\"start\":68795},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":110125697},\"end\":69321,\"start\":69078},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2161592},\"end\":69694,\"start\":69323},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206590483},\"end\":70070,\"start\":69696},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206764948},\"end\":70386,\"start\":70072},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14030224},\"end\":70730,\"start\":70388},{\"attributes\":{\"id\":\"b27\"},\"end\":70987,\"start\":70732},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3198903},\"end\":71348,\"start\":70989},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":718943},\"end\":71625,\"start\":71350},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2084257},\"end\":72006,\"start\":71627},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":39450643},\"end\":72221,\"start\":72008},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":31520628},\"end\":72570,\"start\":72223},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":7286062},\"end\":72921,\"start\":72572},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13497941},\"end\":73200,\"start\":72923},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":18005885},\"end\":73509,\"start\":73202},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206594692},\"end\":73913,\"start\":73511},{\"attributes\":{\"doi\":\"10.1038/229381a0\",\"id\":\"b37\"},\"end\":74070,\"start\":73915},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6878285},\"end\":74443,\"start\":74072},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":118216652},\"end\":74864,\"start\":74445},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9749221},\"end\":75332,\"start\":74866},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":6200260},\"end\":75779,\"start\":75334},{\"attributes\":{\"id\":\"b42\"},\"end\":76107,\"start\":75781},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206485682},\"end\":76371,\"start\":76109},{\"attributes\":{\"doi\":\"arXiv:1705.10118\",\"id\":\"b44\"},\"end\":76706,\"start\":76373},{\"attributes\":{\"doi\":\"arXiv:1411.4464\",\"id\":\"b45\"},\"end\":76931,\"start\":76708},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":15836844},\"end\":77259,\"start\":76933},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":31040730},\"end\":77613,\"start\":77261},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":195908774},\"end\":77957,\"start\":77615},{\"attributes\":{\"doi\":\"arXiv:1703.09393\",\"id\":\"b49\"},\"end\":78314,\"start\":77959},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":14395688},\"end\":78689,\"start\":78316},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":18018217},\"end\":78947,\"start\":78691},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2280516},\"end\":79323,\"start\":78949},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":5157313},\"end\":79782,\"start\":79325},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":4915696},\"end\":80099,\"start\":79784},{\"attributes\":{\"id\":\"b55\"},\"end\":80364,\"start\":80101},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":37318664},\"end\":80773,\"start\":80366},{\"attributes\":{\"id\":\"b57\"},\"end\":81162,\"start\":80775},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":1442165},\"end\":81516,\"start\":81164},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":1629541},\"end\":81944,\"start\":81518},{\"attributes\":{\"id\":\"b60\"},\"end\":82286,\"start\":81946},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":114063333},\"end\":82710,\"start\":82288},{\"attributes\":{\"id\":\"b62\"},\"end\":82917,\"start\":82712},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":122603724},\"end\":83422,\"start\":82919},{\"attributes\":{\"doi\":\"arXiv:1612.00220\",\"id\":\"b64\"},\"end\":83728,\"start\":83424},{\"attributes\":{\"doi\":\"arXiv:1705.10698\",\"id\":\"b65\"},\"end\":84190,\"start\":83730},{\"attributes\":{\"doi\":\"App. 14/625\",\"id\":\"b66\"},\"end\":84433,\"start\":84192},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":1216498},\"end\":84763,\"start\":84435},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":3179925},\"end\":85183,\"start\":84765},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":40499053},\"end\":85499,\"start\":85185},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":7612598},\"end\":85874,\"start\":85501},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":38618814},\"end\":86130,\"start\":85876},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":918513},\"end\":86417,\"start\":86132},{\"attributes\":{\"id\":\"b73\"},\"end\":86857,\"start\":86419},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":1718465},\"end\":87373,\"start\":86859},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":14273023},\"end\":87769,\"start\":87375},{\"attributes\":{\"id\":\"b76\"},\"end\":88112,\"start\":87771},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":14897024},\"end\":88462,\"start\":88114},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":15166260},\"end\":88800,\"start\":88464},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":13420116},\"end\":89099,\"start\":88802},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":13164291},\"end\":89464,\"start\":89101},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":1089358},\"end\":89875,\"start\":89466},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":16712808},\"end\":90261,\"start\":89877},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":6788752},\"end\":90664,\"start\":90263},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":7738307},\"end\":90973,\"start\":90666},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":411557},\"end\":91368,\"start\":90975},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":7768768},\"end\":91742,\"start\":91370},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":17261310},\"end\":92198,\"start\":91744},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":3820195},\"end\":92570,\"start\":92200},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":145760136},\"end\":92710,\"start\":92572},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b90\"},\"end\":92969,\"start\":92712},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":3003101},\"end\":93397,\"start\":92971},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":206592484},\"end\":93909,\"start\":93399},{\"attributes\":{\"id\":\"b93\"},\"end\":94190,\"start\":93911},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":2796017},\"end\":94407,\"start\":94192},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":47726},\"end\":94704,\"start\":94409},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":26569197},\"end\":94940,\"start\":94706},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":4509294},\"end\":95352,\"start\":94942},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":11060885},\"end\":95686,\"start\":95354},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":206769463},\"end\":96146,\"start\":95688},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":2536395},\"end\":96529,\"start\":96148},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":224660},\"end\":96889,\"start\":96531},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":12101868},\"end\":97227,\"start\":96891},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":16071703},\"end\":97646,\"start\":97229},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":1417739},\"end\":97911,\"start\":97648},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":4236070},\"end\":98259,\"start\":97913},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":2131202},\"end\":98709,\"start\":98261},{\"attributes\":{\"id\":\"b107\"},\"end\":98895,\"start\":98711},{\"attributes\":{\"doi\":\"arXiv:1701.05957\",\"id\":\"b108\"},\"end\":99174,\"start\":98897},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":1399999},\"end\":99527,\"start\":99176},{\"attributes\":{\"id\":\"b110\"},\"end\":100008,\"start\":99529},{\"attributes\":{\"id\":\"b111\",\"matched_paper_id\":2473242},\"end\":100312,\"start\":100010},{\"attributes\":{\"id\":\"b112\",\"matched_paper_id\":19922070},\"end\":100636,\"start\":100314},{\"attributes\":{\"id\":\"b113\",\"matched_paper_id\":5962058},\"end\":100933,\"start\":100638},{\"attributes\":{\"id\":\"b114\",\"matched_paper_id\":3085328},\"end\":101336,\"start\":100935},{\"attributes\":{\"id\":\"b115\",\"matched_paper_id\":17548464},\"end\":101626,\"start\":101338},{\"attributes\":{\"id\":\"b116\",\"matched_paper_id\":207112729},\"end\":102007,\"start\":101628}]", "bib_title": "[{\"end\":61973,\"start\":61887},{\"end\":62348,\"start\":62288},{\"end\":62892,\"start\":62872},{\"end\":63181,\"start\":63116},{\"end\":63415,\"start\":63315},{\"end\":63746,\"start\":63670},{\"end\":64121,\"start\":64042},{\"end\":64973,\"start\":64910},{\"end\":65377,\"start\":65314},{\"end\":65749,\"start\":65711},{\"end\":66010,\"start\":65945},{\"end\":66306,\"start\":66220},{\"end\":66669,\"start\":66623},{\"end\":66994,\"start\":66931},{\"end\":67299,\"start\":67218},{\"end\":67683,\"start\":67620},{\"end\":68120,\"start\":68077},{\"end\":68409,\"start\":68337},{\"end\":68861,\"start\":68795},{\"end\":69152,\"start\":69078},{\"end\":69381,\"start\":69323},{\"end\":69748,\"start\":69696},{\"end\":70131,\"start\":70072},{\"end\":70432,\"start\":70388},{\"end\":71053,\"start\":70989},{\"end\":71423,\"start\":71350},{\"end\":71706,\"start\":71627},{\"end\":72066,\"start\":72008},{\"end\":72287,\"start\":72223},{\"end\":72640,\"start\":72572},{\"end\":72964,\"start\":72923},{\"end\":73263,\"start\":73202},{\"end\":73555,\"start\":73511},{\"end\":74145,\"start\":74072},{\"end\":74550,\"start\":74445},{\"end\":74930,\"start\":74866},{\"end\":75398,\"start\":75334},{\"end\":75847,\"start\":75781},{\"end\":76156,\"start\":76109},{\"end\":77025,\"start\":76933},{\"end\":77347,\"start\":77261},{\"end\":77678,\"start\":77615},{\"end\":78354,\"start\":78316},{\"end\":78726,\"start\":78691},{\"end\":78998,\"start\":78949},{\"end\":79439,\"start\":79325},{\"end\":79816,\"start\":79784},{\"end\":80447,\"start\":80366},{\"end\":80829,\"start\":80775},{\"end\":81206,\"start\":81164},{\"end\":81572,\"start\":81518},{\"end\":82403,\"start\":82288},{\"end\":82975,\"start\":82919},{\"end\":84517,\"start\":84435},{\"end\":84861,\"start\":84765},{\"end\":85244,\"start\":85185},{\"end\":85553,\"start\":85501},{\"end\":85946,\"start\":85876},{\"end\":86185,\"start\":86132},{\"end\":86959,\"start\":86859},{\"end\":87504,\"start\":87375},{\"end\":88158,\"start\":88114},{\"end\":88535,\"start\":88464},{\"end\":88853,\"start\":88802},{\"end\":89179,\"start\":89101},{\"end\":89523,\"start\":89466},{\"end\":89926,\"start\":89877},{\"end\":90338,\"start\":90263},{\"end\":90733,\"start\":90666},{\"end\":91017,\"start\":90975},{\"end\":91427,\"start\":91370},{\"end\":91810,\"start\":91744},{\"end\":92264,\"start\":92200},{\"end\":92604,\"start\":92572},{\"end\":93128,\"start\":92971},{\"end\":93429,\"start\":93399},{\"end\":94223,\"start\":94192},{\"end\":94470,\"start\":94409},{\"end\":94741,\"start\":94706},{\"end\":94988,\"start\":94942},{\"end\":95418,\"start\":95354},{\"end\":95804,\"start\":95688},{\"end\":96265,\"start\":96148},{\"end\":96607,\"start\":96531},{\"end\":96950,\"start\":96891},{\"end\":97325,\"start\":97229},{\"end\":97672,\"start\":97648},{\"end\":97987,\"start\":97913},{\"end\":98326,\"start\":98261},{\"end\":99240,\"start\":99176},{\"end\":99601,\"start\":99529},{\"end\":100078,\"start\":100010},{\"end\":100378,\"start\":100314},{\"end\":100704,\"start\":100638},{\"end\":101030,\"start\":100935},{\"end\":101395,\"start\":101338},{\"end\":101738,\"start\":101628}]", "bib_author": "[{\"end\":61989,\"start\":61975},{\"end\":62003,\"start\":61989},{\"end\":62017,\"start\":62003},{\"end\":62029,\"start\":62017},{\"end\":62364,\"start\":62350},{\"end\":62632,\"start\":62619},{\"end\":62645,\"start\":62632},{\"end\":62657,\"start\":62645},{\"end\":62904,\"start\":62894},{\"end\":62917,\"start\":62904},{\"end\":62930,\"start\":62917},{\"end\":63194,\"start\":63183},{\"end\":63428,\"start\":63417},{\"end\":63439,\"start\":63428},{\"end\":63450,\"start\":63439},{\"end\":63758,\"start\":63748},{\"end\":63770,\"start\":63758},{\"end\":63781,\"start\":63770},{\"end\":64135,\"start\":64123},{\"end\":64149,\"start\":64135},{\"end\":64160,\"start\":64149},{\"end\":64525,\"start\":64513},{\"end\":64531,\"start\":64525},{\"end\":64541,\"start\":64531},{\"end\":64816,\"start\":64806},{\"end\":64990,\"start\":64975},{\"end\":65007,\"start\":64990},{\"end\":65017,\"start\":65007},{\"end\":65392,\"start\":65379},{\"end\":65403,\"start\":65392},{\"end\":65765,\"start\":65751},{\"end\":65778,\"start\":65765},{\"end\":65788,\"start\":65778},{\"end\":66022,\"start\":66012},{\"end\":66036,\"start\":66022},{\"end\":66048,\"start\":66036},{\"end\":66318,\"start\":66308},{\"end\":66331,\"start\":66318},{\"end\":66346,\"start\":66331},{\"end\":66681,\"start\":66671},{\"end\":66696,\"start\":66681},{\"end\":67006,\"start\":66996},{\"end\":67021,\"start\":67006},{\"end\":67311,\"start\":67301},{\"end\":67320,\"start\":67311},{\"end\":67330,\"start\":67320},{\"end\":67341,\"start\":67330},{\"end\":67350,\"start\":67341},{\"end\":67363,\"start\":67350},{\"end\":67693,\"start\":67685},{\"end\":67701,\"start\":67693},{\"end\":67710,\"start\":67701},{\"end\":67717,\"start\":67710},{\"end\":68130,\"start\":68122},{\"end\":68139,\"start\":68130},{\"end\":68147,\"start\":68139},{\"end\":68156,\"start\":68147},{\"end\":68419,\"start\":68411},{\"end\":68427,\"start\":68419},{\"end\":68440,\"start\":68427},{\"end\":68872,\"start\":68863},{\"end\":68879,\"start\":68872},{\"end\":68888,\"start\":68879},{\"end\":68895,\"start\":68888},{\"end\":68903,\"start\":68895},{\"end\":69164,\"start\":69154},{\"end\":69172,\"start\":69164},{\"end\":69394,\"start\":69383},{\"end\":69403,\"start\":69394},{\"end\":69418,\"start\":69403},{\"end\":69759,\"start\":69750},{\"end\":69769,\"start\":69759},{\"end\":70143,\"start\":70133},{\"end\":70152,\"start\":70143},{\"end\":70163,\"start\":70152},{\"end\":70173,\"start\":70163},{\"end\":70442,\"start\":70434},{\"end\":70458,\"start\":70442},{\"end\":70468,\"start\":70458},{\"end\":70480,\"start\":70468},{\"end\":70801,\"start\":70788},{\"end\":70814,\"start\":70801},{\"end\":71073,\"start\":71055},{\"end\":71087,\"start\":71073},{\"end\":71101,\"start\":71087},{\"end\":71112,\"start\":71101},{\"end\":71437,\"start\":71425},{\"end\":71448,\"start\":71437},{\"end\":71718,\"start\":71708},{\"end\":71728,\"start\":71718},{\"end\":71742,\"start\":71728},{\"end\":71752,\"start\":71742},{\"end\":72082,\"start\":72068},{\"end\":72295,\"start\":72289},{\"end\":72301,\"start\":72295},{\"end\":72307,\"start\":72301},{\"end\":72314,\"start\":72307},{\"end\":72320,\"start\":72314},{\"end\":72327,\"start\":72320},{\"end\":72650,\"start\":72642},{\"end\":72657,\"start\":72650},{\"end\":72667,\"start\":72657},{\"end\":72679,\"start\":72667},{\"end\":72692,\"start\":72679},{\"end\":72972,\"start\":72966},{\"end\":72985,\"start\":72972},{\"end\":73278,\"start\":73265},{\"end\":73290,\"start\":73278},{\"end\":73300,\"start\":73290},{\"end\":73312,\"start\":73300},{\"end\":73563,\"start\":73557},{\"end\":73572,\"start\":73563},{\"end\":73579,\"start\":73572},{\"end\":73586,\"start\":73579},{\"end\":73930,\"start\":73915},{\"end\":74153,\"start\":74147},{\"end\":74162,\"start\":74153},{\"end\":74170,\"start\":74162},{\"end\":74178,\"start\":74170},{\"end\":74184,\"start\":74178},{\"end\":74561,\"start\":74552},{\"end\":74569,\"start\":74561},{\"end\":74577,\"start\":74569},{\"end\":74585,\"start\":74577},{\"end\":74942,\"start\":74932},{\"end\":74953,\"start\":74942},{\"end\":74964,\"start\":74953},{\"end\":74972,\"start\":74964},{\"end\":75409,\"start\":75400},{\"end\":75418,\"start\":75409},{\"end\":75426,\"start\":75418},{\"end\":75437,\"start\":75426},{\"end\":75860,\"start\":75849},{\"end\":75869,\"start\":75860},{\"end\":75880,\"start\":75869},{\"end\":76174,\"start\":76158},{\"end\":76185,\"start\":76174},{\"end\":76195,\"start\":76185},{\"end\":76486,\"start\":76478},{\"end\":76492,\"start\":76486},{\"end\":76502,\"start\":76492},{\"end\":76776,\"start\":76768},{\"end\":76784,\"start\":76776},{\"end\":77039,\"start\":77027},{\"end\":77049,\"start\":77039},{\"end\":77355,\"start\":77349},{\"end\":77694,\"start\":77680},{\"end\":77707,\"start\":77694},{\"end\":77719,\"start\":77707},{\"end\":78080,\"start\":78069},{\"end\":78089,\"start\":78080},{\"end\":78099,\"start\":78089},{\"end\":78365,\"start\":78356},{\"end\":78376,\"start\":78365},{\"end\":78387,\"start\":78376},{\"end\":78741,\"start\":78728},{\"end\":78754,\"start\":78741},{\"end\":79006,\"start\":79000},{\"end\":79012,\"start\":79006},{\"end\":79447,\"start\":79441},{\"end\":79456,\"start\":79447},{\"end\":79465,\"start\":79456},{\"end\":79472,\"start\":79465},{\"end\":79824,\"start\":79818},{\"end\":79833,\"start\":79824},{\"end\":79841,\"start\":79833},{\"end\":79847,\"start\":79841},{\"end\":79855,\"start\":79847},{\"end\":79862,\"start\":79855},{\"end\":80161,\"start\":80155},{\"end\":80174,\"start\":80161},{\"end\":80189,\"start\":80174},{\"end\":80458,\"start\":80449},{\"end\":80468,\"start\":80458},{\"end\":80478,\"start\":80468},{\"end\":80843,\"start\":80831},{\"end\":80859,\"start\":80843},{\"end\":80870,\"start\":80859},{\"end\":80882,\"start\":80870},{\"end\":80889,\"start\":80882},{\"end\":80898,\"start\":80889},{\"end\":80904,\"start\":80898},{\"end\":80910,\"start\":80904},{\"end\":80921,\"start\":80910},{\"end\":80933,\"start\":80921},{\"end\":81215,\"start\":81208},{\"end\":81230,\"start\":81215},{\"end\":81582,\"start\":81574},{\"end\":81595,\"start\":81582},{\"end\":81606,\"start\":81595},{\"end\":82069,\"start\":82060},{\"end\":82077,\"start\":82069},{\"end\":82085,\"start\":82077},{\"end\":82094,\"start\":82085},{\"end\":82411,\"start\":82405},{\"end\":82421,\"start\":82411},{\"end\":82429,\"start\":82421},{\"end\":82437,\"start\":82429},{\"end\":82762,\"start\":82749},{\"end\":82768,\"start\":82762},{\"end\":82780,\"start\":82768},{\"end\":82795,\"start\":82780},{\"end\":82987,\"start\":82977},{\"end\":83000,\"start\":82987},{\"end\":83010,\"start\":83000},{\"end\":83022,\"start\":83010},{\"end\":83435,\"start\":83424},{\"end\":83448,\"start\":83435},{\"end\":83458,\"start\":83448},{\"end\":83477,\"start\":83458},{\"end\":83880,\"start\":83869},{\"end\":83894,\"start\":83880},{\"end\":83904,\"start\":83894},{\"end\":83923,\"start\":83904},{\"end\":84260,\"start\":84247},{\"end\":84270,\"start\":84260},{\"end\":84282,\"start\":84270},{\"end\":84531,\"start\":84519},{\"end\":84541,\"start\":84531},{\"end\":84552,\"start\":84541},{\"end\":84563,\"start\":84552},{\"end\":84576,\"start\":84563},{\"end\":84877,\"start\":84863},{\"end\":84889,\"start\":84877},{\"end\":84900,\"start\":84889},{\"end\":84910,\"start\":84900},{\"end\":85261,\"start\":85246},{\"end\":85279,\"start\":85261},{\"end\":85567,\"start\":85555},{\"end\":85577,\"start\":85567},{\"end\":85961,\"start\":85948},{\"end\":85981,\"start\":85961},{\"end\":86198,\"start\":86187},{\"end\":86209,\"start\":86198},{\"end\":86215,\"start\":86209},{\"end\":86228,\"start\":86215},{\"end\":86577,\"start\":86568},{\"end\":86590,\"start\":86577},{\"end\":86601,\"start\":86590},{\"end\":86612,\"start\":86601},{\"end\":86971,\"start\":86961},{\"end\":86983,\"start\":86971},{\"end\":86996,\"start\":86983},{\"end\":87005,\"start\":86996},{\"end\":87516,\"start\":87506},{\"end\":87525,\"start\":87516},{\"end\":87538,\"start\":87525},{\"end\":87839,\"start\":87826},{\"end\":87849,\"start\":87839},{\"end\":87858,\"start\":87849},{\"end\":87872,\"start\":87858},{\"end\":88168,\"start\":88160},{\"end\":88178,\"start\":88168},{\"end\":88188,\"start\":88178},{\"end\":88201,\"start\":88188},{\"end\":88545,\"start\":88537},{\"end\":88555,\"start\":88545},{\"end\":88568,\"start\":88555},{\"end\":88578,\"start\":88568},{\"end\":88870,\"start\":88855},{\"end\":88878,\"start\":88870},{\"end\":89194,\"start\":89181},{\"end\":89206,\"start\":89194},{\"end\":89217,\"start\":89206},{\"end\":89534,\"start\":89525},{\"end\":89543,\"start\":89534},{\"end\":89553,\"start\":89543},{\"end\":89938,\"start\":89928},{\"end\":89949,\"start\":89938},{\"end\":89960,\"start\":89949},{\"end\":89966,\"start\":89960},{\"end\":90352,\"start\":90340},{\"end\":90364,\"start\":90352},{\"end\":90373,\"start\":90364},{\"end\":90744,\"start\":90735},{\"end\":90750,\"start\":90744},{\"end\":90757,\"start\":90750},{\"end\":91027,\"start\":91019},{\"end\":91034,\"start\":91027},{\"end\":91042,\"start\":91034},{\"end\":91437,\"start\":91429},{\"end\":91445,\"start\":91437},{\"end\":91454,\"start\":91445},{\"end\":91462,\"start\":91454},{\"end\":91820,\"start\":91812},{\"end\":91829,\"start\":91820},{\"end\":91837,\"start\":91829},{\"end\":91845,\"start\":91837},{\"end\":92275,\"start\":92266},{\"end\":92283,\"start\":92275},{\"end\":92290,\"start\":92283},{\"end\":92296,\"start\":92290},{\"end\":92304,\"start\":92296},{\"end\":92311,\"start\":92304},{\"end\":92616,\"start\":92606},{\"end\":92792,\"start\":92780},{\"end\":92805,\"start\":92792},{\"end\":93141,\"start\":93130},{\"end\":93150,\"start\":93141},{\"end\":93442,\"start\":93431},{\"end\":93449,\"start\":93442},{\"end\":93456,\"start\":93449},{\"end\":93468,\"start\":93456},{\"end\":93476,\"start\":93468},{\"end\":93488,\"start\":93476},{\"end\":93497,\"start\":93488},{\"end\":93510,\"start\":93497},{\"end\":93524,\"start\":93510},{\"end\":93985,\"start\":93976},{\"end\":93996,\"start\":93985},{\"end\":94004,\"start\":93996},{\"end\":94234,\"start\":94225},{\"end\":94245,\"start\":94234},{\"end\":94481,\"start\":94472},{\"end\":94492,\"start\":94481},{\"end\":94500,\"start\":94492},{\"end\":94753,\"start\":94743},{\"end\":94761,\"start\":94753},{\"end\":94998,\"start\":94990},{\"end\":95007,\"start\":94998},{\"end\":95015,\"start\":95007},{\"end\":95022,\"start\":95015},{\"end\":95029,\"start\":95022},{\"end\":95428,\"start\":95420},{\"end\":95435,\"start\":95428},{\"end\":95812,\"start\":95806},{\"end\":95823,\"start\":95812},{\"end\":96273,\"start\":96267},{\"end\":96284,\"start\":96273},{\"end\":96615,\"start\":96609},{\"end\":96622,\"start\":96615},{\"end\":96958,\"start\":96952},{\"end\":96966,\"start\":96958},{\"end\":96972,\"start\":96966},{\"end\":96979,\"start\":96972},{\"end\":96985,\"start\":96979},{\"end\":97333,\"start\":97327},{\"end\":97342,\"start\":97333},{\"end\":97351,\"start\":97342},{\"end\":97358,\"start\":97351},{\"end\":97365,\"start\":97358},{\"end\":97682,\"start\":97674},{\"end\":97697,\"start\":97682},{\"end\":97710,\"start\":97697},{\"end\":97724,\"start\":97710},{\"end\":97732,\"start\":97724},{\"end\":97998,\"start\":97989},{\"end\":98006,\"start\":97998},{\"end\":98012,\"start\":98006},{\"end\":98020,\"start\":98012},{\"end\":98027,\"start\":98020},{\"end\":98035,\"start\":98027},{\"end\":98337,\"start\":98328},{\"end\":98343,\"start\":98337},{\"end\":98351,\"start\":98343},{\"end\":98359,\"start\":98351},{\"end\":98775,\"start\":98766},{\"end\":98783,\"start\":98775},{\"end\":98975,\"start\":98966},{\"end\":98986,\"start\":98975},{\"end\":98997,\"start\":98986},{\"end\":99253,\"start\":99242},{\"end\":99261,\"start\":99253},{\"end\":99273,\"start\":99261},{\"end\":99286,\"start\":99273},{\"end\":99612,\"start\":99603},{\"end\":99620,\"start\":99612},{\"end\":99628,\"start\":99620},{\"end\":99635,\"start\":99628},{\"end\":99641,\"start\":99635},{\"end\":100088,\"start\":100080},{\"end\":100099,\"start\":100088},{\"end\":100105,\"start\":100099},{\"end\":100388,\"start\":100380},{\"end\":100394,\"start\":100388},{\"end\":100402,\"start\":100394},{\"end\":100410,\"start\":100402},{\"end\":100714,\"start\":100706},{\"end\":100722,\"start\":100714},{\"end\":100730,\"start\":100722},{\"end\":101040,\"start\":101032},{\"end\":101048,\"start\":101040},{\"end\":101056,\"start\":101048},{\"end\":101404,\"start\":101397},{\"end\":101412,\"start\":101404},{\"end\":101418,\"start\":101412},{\"end\":101753,\"start\":101740},{\"end\":101764,\"start\":101753},{\"end\":101772,\"start\":101764},{\"end\":101787,\"start\":101772}]", "bib_venue": "[{\"end\":62069,\"start\":62029},{\"end\":62390,\"start\":62364},{\"end\":62617,\"start\":62519},{\"end\":62968,\"start\":62930},{\"end\":63204,\"start\":63194},{\"end\":63477,\"start\":63450},{\"end\":63819,\"start\":63781},{\"end\":64205,\"start\":64160},{\"end\":64511,\"start\":64406},{\"end\":64804,\"start\":64741},{\"end\":65074,\"start\":65017},{\"end\":65488,\"start\":65403},{\"end\":65813,\"start\":65788},{\"end\":66067,\"start\":66048},{\"end\":66385,\"start\":66346},{\"end\":66760,\"start\":66696},{\"end\":67058,\"start\":67021},{\"end\":67401,\"start\":67363},{\"end\":67794,\"start\":67717},{\"end\":68194,\"start\":68156},{\"end\":68517,\"start\":68440},{\"end\":68917,\"start\":68903},{\"end\":69186,\"start\":69172},{\"end\":69486,\"start\":69418},{\"end\":69868,\"start\":69769},{\"end\":70190,\"start\":70173},{\"end\":70544,\"start\":70480},{\"end\":70786,\"start\":70732},{\"end\":71129,\"start\":71112},{\"end\":71475,\"start\":71448},{\"end\":71794,\"start\":71752},{\"end\":72102,\"start\":72082},{\"end\":72378,\"start\":72327},{\"end\":72702,\"start\":72692},{\"end\":73024,\"start\":72985},{\"end\":73336,\"start\":73312},{\"end\":73663,\"start\":73586},{\"end\":73976,\"start\":73946},{\"end\":74240,\"start\":74184},{\"end\":74638,\"start\":74585},{\"end\":75049,\"start\":74972},{\"end\":75514,\"start\":75437},{\"end\":75918,\"start\":75880},{\"end\":76226,\"start\":76195},{\"end\":76476,\"start\":76373},{\"end\":76766,\"start\":76708},{\"end\":77058,\"start\":77049},{\"end\":77399,\"start\":77355},{\"end\":77768,\"start\":77719},{\"end\":78067,\"start\":77959},{\"end\":78486,\"start\":78387},{\"end\":78803,\"start\":78754},{\"end\":79089,\"start\":79012},{\"end\":79510,\"start\":79472},{\"end\":79924,\"start\":79862},{\"end\":80153,\"start\":80101},{\"end\":80555,\"start\":80478},{\"end\":80942,\"start\":80933},{\"end\":81297,\"start\":81230},{\"end\":81683,\"start\":81606},{\"end\":82058,\"start\":81946},{\"end\":82490,\"start\":82437},{\"end\":82747,\"start\":82712},{\"end\":83080,\"start\":83022},{\"end\":83554,\"start\":83493},{\"end\":83867,\"start\":83730},{\"end\":84245,\"start\":84192},{\"end\":84584,\"start\":84576},{\"end\":84948,\"start\":84910},{\"end\":85317,\"start\":85279},{\"end\":85616,\"start\":85577},{\"end\":85988,\"start\":85981},{\"end\":86259,\"start\":86228},{\"end\":86566,\"start\":86419},{\"end\":87072,\"start\":87005},{\"end\":87563,\"start\":87538},{\"end\":87824,\"start\":87771},{\"end\":88253,\"start\":88201},{\"end\":88617,\"start\":88578},{\"end\":88917,\"start\":88878},{\"end\":89268,\"start\":89217},{\"end\":89630,\"start\":89553},{\"end\":90042,\"start\":89966},{\"end\":90444,\"start\":90373},{\"end\":90780,\"start\":90757},{\"end\":91119,\"start\":91042},{\"end\":91532,\"start\":91462},{\"end\":91922,\"start\":91845},{\"end\":92373,\"start\":92311},{\"end\":92630,\"start\":92616},{\"end\":92778,\"start\":92712},{\"end\":93154,\"start\":93150},{\"end\":93601,\"start\":93524},{\"end\":93974,\"start\":93911},{\"end\":94285,\"start\":94245},{\"end\":94540,\"start\":94500},{\"end\":94799,\"start\":94761},{\"end\":95100,\"start\":95029},{\"end\":95503,\"start\":95435},{\"end\":95901,\"start\":95823},{\"end\":96324,\"start\":96284},{\"end\":96691,\"start\":96622},{\"end\":97047,\"start\":96985},{\"end\":97420,\"start\":97365},{\"end\":97763,\"start\":97732},{\"end\":98066,\"start\":98035},{\"end\":98436,\"start\":98359},{\"end\":98764,\"start\":98711},{\"end\":98964,\"start\":98897},{\"end\":99333,\"start\":99286},{\"end\":99718,\"start\":99641},{\"end\":100122,\"start\":100105},{\"end\":100448,\"start\":100410},{\"end\":100770,\"start\":100730},{\"end\":101102,\"start\":101056},{\"end\":101456,\"start\":101418},{\"end\":101801,\"start\":101787},{\"end\":65118,\"start\":65076},{\"end\":67858,\"start\":67796},{\"end\":68581,\"start\":68519},{\"end\":73727,\"start\":73665},{\"end\":75113,\"start\":75051},{\"end\":75578,\"start\":75516},{\"end\":79153,\"start\":79091},{\"end\":81351,\"start\":81299},{\"end\":81747,\"start\":81685},{\"end\":83127,\"start\":83082},{\"end\":87126,\"start\":87074},{\"end\":89694,\"start\":89632},{\"end\":91183,\"start\":91121},{\"end\":91986,\"start\":91924},{\"end\":93665,\"start\":93603},{\"end\":95158,\"start\":95102},{\"end\":98500,\"start\":98438},{\"end\":99782,\"start\":99720}]"}}}, "year": 2023, "month": 12, "day": 17}