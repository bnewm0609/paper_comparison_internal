{"id": 221112548, "updated": "2023-10-06 12:12:40.772", "metadata": {"title": "ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network", "authors": "[{\"first\":\"Weiqing\",\"last\":\"Min\",\"middle\":[]},{\"first\":\"Linhu\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Zhiling\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zhengdong\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Xiaoming\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Xiaolin\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Shuqiang\",\"last\":\"Jiang\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 28th ACM International Conference on Multimedia", "publication_date": {"year": 2020, "month": 8, "day": 13}, "abstract": "Food recognition has received more and more attention in the multimedia community for its various real-world applications, such as diet management and self-service restaurants. A large-scale ontology of food images is urgently needed for developing advanced large-scale food recognition algorithms, as well as for providing the benchmark dataset for such algorithms. To encourage further progress in food recognition, we introduce the dataset ISIA Food- 500 with 500 categories from the list in the Wikipedia and 399,726 images, a more comprehensive food dataset that surpasses existing popular benchmark datasets by category coverage and data volume. Furthermore, we propose a stacked global-local attention network, which consists of two sub-networks for food recognition. One subnetwork first utilizes hybrid spatial-channel attention to extract more discriminative features, and then aggregates these multi-scale discriminative features from multiple layers into global-level representation (e.g., texture and shape information about food). The other one generates attentional regions (e.g., ingredient relevant regions) from different regions via cascaded spatial transformers, and further aggregates these multi-scale regional features from different layers into local-level representation. These two types of features are finally fused as comprehensive representation for food recognition. Extensive experiments on ISIA Food-500 and other two popular benchmark datasets demonstrate the effectiveness of our proposed method, and thus can be considered as one strong baseline. The dataset, code and models can be found at http://123.57.42.89/FoodComputing-Dataset/ISIA-Food500.html.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2008.05655", "mag": "3093234244", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/MinLWLWWJ20", "doi": "10.1145/3394171.3414031"}}, "content": {"source": {"pdf_hash": "d567e8c06d6688ead92186c88ab8d58d61f681cc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.05655v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2008.05655", "status": "GREEN"}}, "grobid": {"id": "294cacad49f5be206690fe5bebd150bb29b75953", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d567e8c06d6688ead92186c88ab8d58d61f681cc.txt", "contents": "\nISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network ACM Reference Format: . 2020. ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Net- work. In\nOctober 12-16, 2020. October 12-16, 2020\n\nWeiqing Min minweiqing@ict.ac.cn \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nLinhu Liu linhu.liu@vipl.ict.ac.cn \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nZhiling Wang zhiling.wang@vipl.ict.ac.cn \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nZhengdong Luo luozhengdong@ict.ac.cn \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nXiaoming Wei weixiaoming@meituan.com \nMeituan-Dianping Group\n\n\nXiaolin Wei weixiaolin02@meituan.com \nMeituan-Dianping Group\n\n\nShuqiang Jiang sqjiang@ict.ac.cn \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nWeiqing Min \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nLinhu Liu \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nZhiling Wang \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nZhengdong Luo \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nXiaom-Ing Wei \nMeituan-Dianping Group\n\n\nXiaolin Wei \nMeituan-Dianping Group\n\n\nShuqiang Jiang \nInstitute of Computing Technology\nKey Lab of Intelligent Information Processing\nCAS\n100190BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network ACM Reference Format: . 2020. ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Net- work. In\n\nProceedings of the 28th ACM International Conference on Multimedia (MM '20)\nthe 28th ACM International Conference on Multimedia (MM '20)Seattle, WA, USA; Seattle, WA, USAOctober 12-16, 2020. October 12-16, 202010.1145/3394171.3414031ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00. ACM, New York, NY, USA, 9 pages. https://CCS CONCEPTS \u2022 Computing methodologies \u2192 Image representations; Ob- ject recognition KEYWORDS Food RecognitionFood DatasetsBenchmarkDeep Learning\nFood recognition has received more and more attention in the multimedia community for its various real-world applications, such as diet management and self-service restaurants. A large-scale ontology of food images is urgently needed for developing advanced large-scale food recognition algorithms, as well as for providing the benchmark dataset for such algorithms. To encourage further progress in food recognition, we introduce the dataset ISIA Food-500 with 500 categories from the list in the Wikipedia and 399,726 images, a more comprehensive food dataset that surpasses existing popular benchmark datasets by category coverage and data volume. Furthermore, we propose a stacked global-local attention network, which consists of two sub-networks for food recognition. One subnetwork first utilizes hybrid spatial-channel attention to extract more discriminative features, and then aggregates these multi-scale discriminative features from multiple layers into global-level representation (e.g., texture and shape information about food). The other one generates attentional regions (e.g., ingredient relevant regions) from different regions via cascaded spatial transformers, and further aggregates these multi-scale regional features from different layers into local-level representation. These two types of features are finally fused as comprehensive representation for food recognition. Extensive experiments on ISIA Food-500 and other two popular benchmark datasets demonstrate the effectiveness of our proposed method, and thus can be considered as one strong baseline. The dataset, code and models can be found at\n\nINTRODUCTION\n\nFood computing [38] is emerging as a new field to ameliorate the issues from many food-relevant fields, such as nutrition, agriculture and medicine. As one significant task in food computing, food recognition has received more attention in multimedia and beyond [15,25,36,41] for its various applications, such as visual food diary [36], health-aware recommendation [42] and self-service restaurants [2].\n\nDespite its great potential applications, recognizing food from images is still a challenging task, and the challenge derives from three-fold:\n\n\u2022 There is a lack of large-scale food dataset for food recognition. Existing works mainly focus on utilizing smaller datasets for food recognition, such as ETH Food-101 [6] and Vireo Food-172 [7]. For example, Bossard et al. [6] released one food dataset ETH Food-101 from western cuisines with 101 food categories and 101,000 images. Chen et al. [7] introduced the Vireo Food-172 dataset from 172 Chinese food categories. These data-sets is lack of diversity and coverage in food categories and do not include a wide range of food images. Therefore, they are probably not sufficient to construct more complicated deep learning models for food recognition.\n\n\u2022 There are larger intra-class variations in the global appearance, shape and other configurations for food images. As shown in Fig. 1, there are different shapes for the butter pecan and different textures appear in the mie goreng dish. Although numerous methods have been developed for addressing the problem of food recognition, most of these methods mainly focus on extracting features with certain type or some types while ignoring other aspects. For example, works on [4] mainly extracted color features while Niki et al. [32] designed a network to capture certain vertical structure for food recognition.\n\n\u2022 There are subtle discriminative details from food images, which are harder to capture in many cases. Food recognition belongs to fine-grained recognition. Therefore, discriminative details are too subtle to be well-represented by existing CNNs in many cases. As shown in Fig. 1, global features are not discriminative enough to distinguish between corn stew and leek soup. Although local regional features are probably more useful, we should carefully design one network to capture and represent such subtle difference. In order to improve the recognition performance, additional context information, such as location and ingredients [4,41,51,59] is utilized. However, when these information is unavailable, these methods probably do not work. In this work, we address data limitations by introducing a new large-scale dataset ISIA Food-500 with 399,726 images and 500 categories. In contrast with existing popular benchmark datasets, it is a more comprehensive food dataset with larger category coverage, larger data volume and higher diversity. To solve another two challenges, we propose a Stacked Global-Local Attention Network (SGLANet) to jointly learn complementary global and local visual features for food recognition. This is achieved by two sub-networks, namely Global Feature Learning Subnetwork (GloFLS) and Local-Feature Learning Subnetwork (LocFLS). GloFLS first utilizes hybrid spatial-channel attention to obtain more discriminative features for each layer, and then aggregates these features from different layers with both coarse and fine-grained levels, such as shape and texture cues about food into global-level features. LocFLS adopts cascaded Spatial Transformers (STs) to localize different attentional regions (e.g., ingredient-relevant regions), and aggregates fused regional features from different layers into local-level representation. In addition, SGLANet is trained with different types of losses in an end-to-end fashion to maximize their complementary effect in terms of discriminative power.\n\nThe contributions of our paper can be summarized as follows: \u2022 We introduce a new large-scale and highly diverse food image dataset with 500 categories and about 400,000 images, which will be made publicly available to further the development of scalable food recognition.\n\n\u2022 We propose a stacked global-local attention network architecture to jointly learn food-oriented global and local features Dataset #Images #Categories #Coverage PFID [9] 4,545 101 Japanese UEC Food100 [34] 14,361 100 Japanese UEC Food256 [27] 25,088 256 Japanese ETHZ Food-101 [6] 101,000 101 Western UPMC Food-101 [48] 90,840 101 Western UNIMIB2015 [12] 2,000 15 Misc. UNIMIB2016 [13] 1,027 73 Misc. ChineseFoodNet [10] 192,000 208 Chinese Vireo Food-172 [7] 110,241 172 Chinese KenyanFood13 [23] 8,174 13 Kenyan Sushi-50 [44] 3,963 50 Japanese FoodX-251 [26] 158 via combining hybrid spatial-channel attention and multi-scale strategy for food recognition.\n\n\u2022 We conduct extensive evaluation on our proposed dataset and other two popular food benchmark datasets to verify the effectiveness of our approach. As one strong baseline, code and models will also be released upon publication to support future research.\n\n\nRELATED WORK\n\nFood-centric datasets More and more food datasets have been developed [6,7,26,27,34,41] in recent years. Table 1 summarizes statistics of publicly available datasets for food recognition. The first benchmark is the PFID dataset [9] with only 4,545 images from 101 fast food categories. ETHZ Food-101 dataset [6] and VIREO Food-172 dataset [7] consist of more food images. However, these datasets failed in term of more comprehensive coverage of food categories, like object-centric ImageNet [14] and place-centric Places [58]. We hence introduce a new large scale food dataset ISIA Food-500 with 399,726 images and 500 food categories, and it aims at advancing multimedia food recognition and promoting the development of food-oriented multimedia intelligence. There are some recipe-relevant multimodal datasets, such as Yummly28K [39], Yummly66K [37] and Recipe1M [45]. Recipe1M is the most known dataset, which contains about 1 million structured cooking recipes and their images for cross-modal retrieval. In contrast, the goal of our proposed ISIA Food-500 is for advancing multimedia food recognition.\n\nFood Recognition Recently, Min et al. [38] gave a survey on food computing including food recognition. In the earlier years, various hand-crafted features are utilized for recognition [6,53]. For example, Lukas et al. [6] utilized random forests to mine discriminative image patches as visual representation. Recent advances in deep learning have gained significant attention due to its impressive performance. As a result, existing methods resort to deep learning for food recognition [18,25,32]. There are also literatures, which utilize additional context information, such as ingredients and location [7,41,59] to improve the recognition performance. For example, Zhou et al. [59] exploited rich relationships among ingredients and restaurant information through the bi-partite graph for food recognition. Different from these works, our work does not introduce additional context information, and design a twobranch network to jointly learn food-oriented global features(e.g., texture and shape) and local features (e.g.,ingredient-relevant regional features) to enable comprehensive and discriminative feature representation for food recognition.\n\nIn addition, our work is also very relevant to fine-grained image recognition [49], which aims to classify subordinate categories. Food recognition belongs to fine-grained image recognition. However, compared with other types of fine-grained objects, we should take characteristics of food images into consideration, and design the targeted network for food recognition.\n\n\nISIA FOOD-500 3.1 Dataset Construction\n\nIn order to obtain one high-quality food dataset with broad coverage, high diversity and density of samples, we build ISIA Food-500 from the following four steps:\n\n(1) Constructing the Food Category List. In order to guarantee high-coverage of the categorical space, we resort to Wikipedia to construct the food concept system. Particularly, we built the food list according to \"Lists of foods by ingredient\" from Wikipedia 1 . The Deep-First-Search algorithm is used to traverse links of the website to find food categories more completely. After that, we obtained the original food list with 4,943 types. We then removed redundant food types and conducted the combination for synonyms. Finally, we obtained 3,309 food categories.\n\n(2) Collecting Food Images. Using a query term from the constructed food category list, we crawled candidate images from various search engines (i.e., Google, Bing and Baidu) for broader coverage and higher diversity of food images compared with other datasets from only one data source. In order to ensure that crawled images are less noisy, we expanded search terms by adding keywords, such as \"food\" and \"dish\". In this case, images for each term are retrieved and these images are then combined from different search engines. Because some images crawled from different search engines are repeated, we conducted hash based duplication detection to remove repeated ones.\n\n(3) Cleaning and Pre-processing Food Images. Images are cleaned up through both automatic and manual processing. For automatic data cleaning, we removed candidate images with incomplete RGB channels, and the length or width of an image less than 100 pixels. We next trained a food/non-food binary classifier to further remove non-food images. Particularly, we combined images from the training set of both ETHZ Food-101 (western dishes) and VireoFood-172 (eastern dishes) as positive samples of the training set. We then randomly selected about 400,000 non-food images from both ImageNet and Places365 as negative samples of the training set. All the test samples of both ETHZ Food-101 and VireoFood-172 and the other 100,000 non-food images randomly selected from both ImageNet and Places365 constitute the test set. We trained a deep network (VGG-16 in our work) on the constructed training set and the classification accuracy of the network achieved 99.48% on\n\n\nDataset Statistics and Characteristics\n\nISIA Food-500 consists of 399,726 images with 500 categories. The average number of images per category is about 800. Fig. 2 shows sorted distribution of the number of images from sampled classes while Fig. 3 shows some samples. Note that we represented the food category with more than two words by concatenating them using '-'. ISIA Food-500 is a more comprehensive food dataset that surpasses existing popular benchmark datasets, such as ETH Food-101 and Vireo Food-172 from the following three aspects: (1) Larger data volume. It has 399,726 images from 500 food categories, which has created a new milestone for the task of complex food recognition.\n\n(2) Larger category coverage. It consists of 500 categories, which is about 3 \u223c 5 times that of existing datasets, such as Food-101 and Vireo Food-172. (3) Higher diversity. Food categories from this dataset covers various countries and regions including both eastern and western cuisines. Fig. 4 provided the comparisons of distributions of food categories on food types, such as ETH Food-101 (western food), Vireo Food-172 (eastern food) and ISIA Food-200 (Misc. food). According to the GSFA standard 2 , the food from our dataset and existing typical ones mainly belongs to the following 11 categories: Meat, Cereals, Vegetables, Fish, Fruits, Dairy, Bakery, Fats, Confectionary, Beverages and Eggs. We can see that for most of food types, the number of food categories from ISIA Food-500 is larger than these existing datasets. Furthermore, some food types are covered in ISIA Food-500, but missing in other ones, such as Dairy and Beverages.  \n\n\nFRAMEWORK\n\n\nGloFLS\n\nGiven the whole input image, GloFLS first learns more discriminative features via hybrid Spatial-Channel Attention (SCA) for each layer, and then aggregates these discriminative features from different layers into global level representations via multi-layer feature fusing. Considering features extracted from different layers contain low-level, mid and high ones, GloFLS can capture various types of global level features, such as shape, texture and edge cues about food.\n\nSpatial-Channel Attention (SCA) The combination of both spatial and channel attention can capture discriminative features comprehensively from different dimensions, and thus have been successfully applied in many tasks, such as image captioning [8] and person ReID [29]. Different from these works, we apply SCA to the food recognition task by capturing food-oriented discriminative features.\n\nThe input to a SCA module is a 3-D tensor X l \u2208R h\u00d7w \u00d7c with width w, height h, channels c and the layer of GloFLS l, respectively. The output of this module is a saliency weight map A l \u2208R h\u00d7w \u00d7c of the same size as X. We calculate A l \u2208R for SCA learning [29]:\nA l = S l \u00d7 C 1(1)\nwhere S l \u2208R h\u00d7w \u00d71 and C l \u2208R 1\u00d71\u00d7c mean spatial and channel attention maps, respectively. The Global Averaging Pooling (GAP) is used to calculate the spatial attention as follows:\nS l = 1 c c i=1 X l 1:h,1:w :i(2)\nThe channel attention from the squeeze-and-excitation block [19] is computed as follows:\nC l 1 = 1 h \u00d7 w h i=1 w j=1 X l i, j,1:c C l = ReLU (M ca 2 \u00d7 Relu(M ca 1 C l 1 ))(3)\nwhere M ca 1 \u2208 R c r \u00d7c and M ca 2 \u2208 R c\u00d7 c r represent the parameter matrix of 2 conv layers respectively, and r denotes the bottleneck reduction rate.\n\nMulti-Layer Feature Fusing By extracting attentional features from multiple layers, we can obtain low, mid and high-level features, which include various types of global features, such as texture, shape and edge information [54]. Such global features are important cues for food recognition. Therefore, we aggregate discriminative attentional features from different layers into global level feature representation for food recognition via a concatenation layer and a fully connected layer.  \n\n\nLocFLS\n\nLocFLS localizes discriminative regions with different positions and scales to capture local features. It uses stacked STs [22] to localize regions from different layers. For each layer, one inception block is introduced to extract regional features, and followed by a global average pooling layer and a max-pooling layer for fusing these regional features. The features from each layer are fused to final local features via a concatenation layer and a fully connected layer.\n\nSpatial Transformer (ST) For each layer, we adopt ST to locate latent T regions, and model this regional attention by a transformation matrix as:\nA l = s h 0 t x 0 s w t y(4)\nwhich allows for image cropping, translation, and isotropic scaling operations by varying two scale factors (s h , s w ) and 2-D spatial position (t x , t y ).\n\n\nLearning with Multiple Losses\n\nSGLANet is jointly optimized by three types of losses, i.e., joint loss L J oi , global loss L Glo , and local loss L Loc respectively, leading to the final loss function:\nL = L J oi + \u03b3 1 L Glo + \u03b3 2 L Loc(5)\nwhere \u03b3 1 and \u03b3 2 are balance parameters, and the cross-entropy classification loss function is used for all three types of losses. Such learning with different types of losses can maximize their complementary benefit in terms of the discriminative power.\n\n\nEXPERIMENT 5.1 Experimental Setup\n\nOur model is implemented on the Pytorch platform. The images are resized to 224\u00d7224. The models are optimized using stochastic gradient descent with a batch size of 80 and momentum of 0.9. The learning rate is set to 10 \u22122 initially and divided by 10 after 30 epochs. For GloFLS, we selected SENet [19] as the backbone, and  \n\n\nExperiment on ISIA Food-500\n\nISIA Food-500 is divided into 60%, 10% and 30% images for training, validation and testing, respectively. All the experiments adopt a single centered crop (1-crop) at test time in the defaulting setting. Ablation Study We first evaluated the effect of each individual component in GloFLS in Table 2. It shows that: (1) Any of two components in isolation brings recognition performance gain; (2) The combination of SCA and Multi-scale gives further accuracy boost, which suggests the complementary effect. We then evaluated the effect of joint global and local feature learning by comparing their individual global and local features. Table 3 shows that a performance gain is obtained in Top-1 accuracy by joining two representations, which validates the complementary effect of jointly learning global and local features from GloFLS and LocFLS.\n\nWe finally evaluate the effect of different losses as shown in Table 4. The experimental results demonstrate that we obtain the best recognition performance when different losses are utilized. The reason is that different loss functions can regulate the deep network from different aspects and work together to improve the recognition performance. Another observation is that the performance with one additional loss does not improve the performance compared with the baseline without both global and local losses. The probable reason is that the performance improvement needs joint work from two losses.\n\nComparisons with State-of-the-Art We evaluated SGLANet against different baseline methods on Table 5. These baselines include not only various typical deep networks, such as VGG16 and SENet, but also some recently proposed fine-grained methods, such as NTS-NET [55] and WS-DAN [20]. Note that for these finegrained methods, we followed the same setting in their mentioned papers. We observed that the performance superiority of SGLANet over all the state-of-the-arts in both Top-1 accuracy and Top-5 Table 4: Evaluating individual losses on ISIA Food-500 (%).\n\n\nMethod\n\nTop-1 acc. Top-5 acc.  accuracy. Compared with best baseline SENet-154, there is the performance improvement of about 0.9 percent in Top-1 accuracy for the test set. These results validate the advantage of joint global and local feature learning of SGLANet.\n\n\nVisualization of GloFLS and LocFLS\n\nWe visualized both SCA from GloFLS and STs from LocFLS at three different layers of SGLA-Net. Fig. 6 shows: (1) in GloFLS, SCA captures different global level features at different layers, such as shape information for Boiled_beef and texture information from Pumpkin_bread. Meanwhile, with increased depth of SGLANet, SCA captures more focused and discriminative features (2) in LocFLS, STs capture different local regions with less background at different layers from LocFLS, such as Crudites. This again verified complementary effect of joint global and local feature learning.\n\nQualitative Analysis We selected 20 classes in the test phase to further evaluate our method. Particularly, we listed the Top-1 accuracy of both 10 best and 10 worst performing classes in Fig.  7. We can observe that some categories can be easily recognized, such as Chakli and Edamame, and their Top-1 accuracy is above 97%. However, there are some categories, which are very hard to recognize, such as Curry_rice and kebab, and their Top-1 accuracy is below 10%. We further demonstrate some challenging recognized examples from the 10 worst performing classes, and Fig. 8 shows that too small inter-class variations is the main reason for bad performance. We have shown that existing methods are far from tackling large-scale recognition task with high accuracy like ImageNet, pointing to exciting future directions.     \n\n\nExperiment on Other Benchmarks\n\nWe further conduct extensive evaluation on other two popular food benchmark datasets to verify the effectiveness of our approach, and also assessed the generalizability of our model learned using ISIA Food-500 to the two datasets. Considering some evaluations from  [6]. We evaluated SGLANet against existing methods on Food-101. Table 6 shows that our method exceeds many baseline methods except some ones, such as MSMVFA [24], IG-CMAN [41] and Inception-Resnet-v2 SE [56] under the 1-crop test setting. The reason is that MSMVFA and IG-CMAN require multiple stages training for feature extraction and introduced additional ingredient information as the supervision. Inception-Resnet-v2 SE used additional data and adopted transfer learning method. When we used the pretrained model on ISIA Food-500, namely SGLANet(Pretrained), there is the performance improvement of about 0.8 percent and 0.6 percent in Top-1 accuracy on 1-crop and 10-crop test respectively. These results also verify the generalization of models learned using ISIA Food-500.\n\nExperiments on Vireo Food-172 Vireo Food-172 consists of 110,241 food images from 172 categories. In each food category, 60%, 10%, 30% of images are randomly selected for training, validation and testing, respectively [7]. Table 7 shows experimental results on Vireo Food-172. We can see that the performance from SGLANet is better than many baselines, except that few ones, such as IG-CMAN. This is because that these methods, such as IG-CMAN introduced additional ingredient information for food recognition. In addition, these methods generally need multi-stage feature learning. When we fine-tuned SGLANet pre-trained on ISIA Food-500, there is the performance improvement of about 0.9 percent and 0.7 percent in Top-1 accuracy on 1-crop and 10-crop test respectively, and achieved the best performance under the 1-crop setting. These results again demonstrate the generalization of our model learned using ISIA Food-500.\n\n\nCONCLUSIONS\n\nIn this paper, we present a new large-scale dataset ISIA Food-500 with larger data volume, larger category coverage, and higher diversity compared with existing typical datasets. We then propose a stacked global-local attention network to jointly exploit complementary global and local features via the designed two subnetworks for food recognition. Extensive evaluation on ISIA Food-500 and another two benchmark datasets have verified its effectiveness, and thus can be considered as one strong baseline.\n\nFuture work includes: (1) We are expanding ISIA Food-500 dataset, and aim to complete the construction of about 1.5 million food images spread over about 2,000 food categories. We expect it will serve as a new challenge to train high-capacity models for largescale food recognition in the multimedia community. (2) We plan to collect rich attribute information, e.g., ingredients, cooking instructions and flavor information [40] to support multimodal food recognition.\n\nFigure 1 :\n1Some samples from ISIA Food-500\n\nFig. 5\n5illustrates the proposed Stacked Global-Local Attention Network (SGLANet), which can jointly learn complementary global and local features for food recognition. SGLANet mainly consists of two components, namely Global Feature Learning Sub-network (GloFLS) and Local-Feature Learning Sub-network (LocFLS). GloF-LS first adopts hybrid Spatial-Channel Attention (SCA) to obtain more discriminative features from each layer of the network, and then aggregates a set of features from these layers to capture different types of global level features, such as shape and texture cues about food. LocFLS adopts cascaded STs to localize different local regions for each layer, and then aggregates fused features with different regions from different layers into final local feature representation. Finally, SGLANet fuses both global and local features\n\nFigure 2 :Figure 3 :\n23Sorted distribution of the number of images from sampled classes in the ISIA Food-500. Image samples from the ISIA Food-500 dataset for food recognition. In addition, SGLANet is trained with different types of losses, including global loss, local loss and joint loss in an end-to-end fashion to maximize their complementary benefit in terms of the discriminative power.\n\nFigure 4 :\n4Comparison on distributions of categories on ISIA Food-500 and other existing typical ones.\n\nFigure 5 :\n5The proposed framework. GAP: Global Average Pooling layer. SCA: Spatial-Channel Attention. ST: Spatial Transformer. FC: Full-Connected layer.\n\nFigure 6 :\n6Visualization of SCA in GloFLS and STs in LocFLS from (a) The 2 th layer,(b) The 3 th layer and (c) The 4 th layer.\n\nFigure 7 :\n7Selected categories from (a)The 10 best and (b)The 10 worst performing classes.\n\nFigure 8 :\n8Some confused classes, where the first column denotes some classes from the 10 worst performing classes and for each class, 3 more confused classes are listed for each row.\n\nTable 1 :\n1Summary of available datasets for food recognition.\n\nTable 2 :\n2Evaluating individual modules in GloFLS on ISIA Food-500 (%).Method \nTop-1 acc. Top-5 acc. \nSENet-154 \n63.83 \n88.61 \nSENet-154+SCA \n64.42 \n89.05 \nSENet-154+Multi-scale \n64.60 \n89.08 \nGloFLS \n64.63 \n89.14 \n\n\n\nTable 3 :\n3Ablation experiments on ISIA Food-500 with global \n& local-level features (%). \n\nMethod Top-1 acc. Top-5 acc. \nGloFLS \n64.63 \n89.14 \nLocFLS \n64.10 \n88.86 \nSGLANet \n64.74 \n89.12 \n\nthe bottleneck reduction rate r = 16. For LocFLS, we selected simple \nInception-B unit as basic building block. For each layer, T = 4 and \nthe scale of ST is fixed as s h = s w = 0.5. We set \u03b3 1 = \u03b3 2 = 0.5 in \nEq. 5. Top-1 accuracy (Top-1 acc.) and Top-5 accuracy (Top-5 acc.) \nare used as evaluation metrics. \n\n\n\nTable 5 :\n5Performance comparison on ISIA Food-500 (%). \n\nMethod \nTop-1 acc. Top-5 acc. \nVGG-16 [47] \n55.22 \n82.77 \nGoogLeNet [36] \n56.03 \n83.42 \nResNet-152 [17] \n57.03 \n83.80 \nWRN-50 [46] \n60.08 \n85.98 \nDenseNet-161 [21] \n60.05 \n86.09 \nNAS-NET [60] \n60.66 \n86.38 \nSE-ResNeXt101_32x4d [19] \n61.95 \n87.54 \nNTS-NET [55] \n63.66 \n88.48 \nWS-DAN [20] \n60.67 \n86.48 \nDCL [11] \n64.10 \n88.77 \nSENet-154 [19] \n63.83 \n88.61 \nSGLANet \n64.74 \n89.12 \n\n\n\nTable 6 :\n6Performance comparison on ETHZ Food-101 (%).Method \nSetting Top-1 acc. Top-5 acc. \nAlexNet-CNN [6] \n1-crop \n56.40 \n-\nSELC [33] \n1-crop \n55.89 \n-\nResNet-152+SVM-RBF [35] \n1-crop \n64.98 \n-\nDCNN-FOOD [52] \n1-crop \n70.41 \n-\nLMBM [50] \n1-crop \n72.11 \n-\nEnsemble Net [43] \n1-crop \n72.12 \n91.61 \nGoogLeNet [3] \n1-crop \n78.11 \n-\nDeepFOOD [30] \n1-crop \n77.40 \n93.70 \nILSVRC [5] \n1-crop \n79.20 \n94.11 \nWARN [31] \n1-crop \n85.50 \n-\nCNNs Fusion(I 2 ) [1] \n1-crop \n86.71 \n-\nInception V3 [16] \n1-crop \n88.28 \n96.88 \nSENet-154 [19] \n1-crop \n88.62 \n97.57 \nWRN [32] \n10-crop \n88.72 \n97.92 \nSOTA[28] \n1-crop \n90.00 \n-\nDLA[57] \n1-crop \n90.00 \n-\nWISeR [32] \n10-crop \n90.27 \n98.71 \nIG-CMAN [41] \n1-crop \n90.37 \n98.42 \nPAR-Net [44] \n1-crop \n89.30 \n-\nPAR-Net [44] \n10-crop \n90.40 \n-\nInception-Resnet-v2 SE [56] 1-crop \n90.40 \n-\nMSMVFA [24] \n1-crop \n90.59 \n98.25 \nSGLANet \n1-crop \n89.69 \n98.01 \nSGLANet \n10-crop \n90.33 \n98.20 \nSGLANet(Pretrained) \n1-crop \n90.47 \n98.21 \nSGLANet(Pretrained) \n10-crop \n90.92 \n98.24 \n\n\n\nTable 7 :\n7Performance comparison on Vireo Food-172 (%). existing works are conducted in the setting of 10-crop test, we show the experimental results of our method in the setting of both 1-crop and 10-crop at test time.Experiments on ETHZ Food-101 ETHZ Food-101 contains 101,000 images from 101 food categories. There are 1,000 images including 750 training images and 250 test images for each categoryMethod \nSetting Top-1 acc. Top-5 acc. \nAlexNet \n1-crop \n64.91 \n85.32 \nVGG-16 [47] \n1-crop \n80.41 \n94.59 \nDenseNet-161 [21] \n1-crop \n86.93 \n97.17 \nMTDCNN(VGG-16) [7] \n1-crop \n82.06 \n95.88 \nMTDCNN(DenseNet-16) [7] 1-crop \n87.21 \n97.29 \nSENet-154 [19] \n1-crop \n88.71 \n97.74 \nPAR-Net [44] \n1-crop \n89.60 \n-\nPAR-Net [44] \n10-crop \n90.20 \n-\nIG-CMAN [41] \n1-crop \n90.63 \n98.40 \nMSMVFA [24] \n1-crop \n90.61 \n98.31 \nSGLANet \n1-crop \n89.88 \n97.83 \nSGLANet \n10-crop \n90.30 \n98.03 \nSGLANet(Pretrained) \n1-crop \n90.78 \n98.16 \nSGLANet(Pretrained) \n10-crop \n90.98 \n98.35 \n\n\nhttps://en.wikipedia.org/wiki/Category:Lists_of_foods the test set. The trained model is then used to filter out non-food images from downloaded images. After automatic cleaning, we then conduct manual verification by crowd-sourcing the task to 20 Lab members.(4) Scaling Up the Dataset. After image collection and annotation, there are still many food categories with few images. To further increase the number of the candidate dataset, we translated the name of these food categories into different languages, such as Chinese and French, and then crawled images from three search engines. We also crawled more food images from other recipe/food shared websites, such as Allrecipes.com and foodgawker.com. We finally selected 500 categories with more than 500 images per category as our resulting dataset.\nhttp://www.fao.org/gsfaonline/index.html?lang=en\nACKNOWLEDGMENTSThis work was supported by the National Natural Science Foundation of China under Grant 61972378, 61532018, U1936203, U19B2040. This research was also supported by Meituan-Dianping Group.\nFood recognition using fusion of classifiers based on CNNs. Eduardo Aguilar, Marc Bola\u00f1os, Petia Radeva, International Conference on Image Analysis and Processing. Eduardo Aguilar, Marc Bola\u00f1os, and Petia Radeva. 2017. Food recognition using fusion of classifiers based on CNNs. In International Conference on Image Analysis and Processing. 213-224.\n\nGrab, Pay and Eat:Semantic Food Detection for Smart Restaurants. Eduardo Aguilar, Beatriz Remeseiro, Marc Bola\u00f1os, Petia Radeva, IEEE Transactions on Multimedia. 20Eduardo Aguilar, Beatriz Remeseiro, Marc Bola\u00f1os, and Petia Radeva. 2018. Grab, Pay and Eat:Semantic Food Detection for Smart Restaurants. In IEEE Transactions on Multimedia, Vol. 20. 3266-3275.\n\nAdapting new categories for food recognition with deep representation. Shuang Ao, Charles X Ling, IEEE International Conference on Data Mining Workshop. Shuang Ao and Charles X. Ling. 2015. Adapting new categories for food recogni- tion with deep representation. In IEEE International Conference on Data Mining Workshop. 1196-1203.\n\nLeveraging context to support automated food recognition in restaurants. Vinay Bettadapura, Edison Thomaz, Aman Parnami, D Gregory, Irfan Abowd, Essa, IEEE Winter Conference on Applications of Computer Vision. Vinay Bettadapura, Edison Thomaz, Aman Parnami, Gregory D Abowd, and Irfan Essa. 2015. Leveraging context to support automated food recognition in restaurants. In IEEE Winter Conference on Applications of Computer Vision. 580-587.\n\nSimultaneous food localization and recognition. Marc Bolanos, Petia Radeva, International Conference on Pattern Recognition. Marc Bolanos and Petia Radeva. 2017. Simultaneous food localization and recog- nition. In International Conference on Pattern Recognition. 3140-3145.\n\nFood-101-mining discriminative components with random forests. Lukas Bossard, Matthieu Guillaumin, Luc Van Gool, European Conference on Computer Vision. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101-mining discriminative components with random forests. In European Conference on Computer Vision. 446-461.\n\nDeep-based ingredient recognition for cooking recipe retrieval. Jingjing Chen, Chong-Wah Ngo, Proceedings of the ACM on Multimedia Conference. the ACM on Multimedia ConferenceJingjing Chen and Chong-Wah Ngo. 2016. Deep-based ingredient recognition for cooking recipe retrieval. In Proceedings of the ACM on Multimedia Conference. 32-41.\n\nSCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning. L Chen, H Zhang, J Xiao, L Nie, J Shao, W Liu, T Chua, IEEE Conference on Computer Vision and Pattern Recognition. L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T. Chua. 2017. SCA- CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning. In IEEE Conference on Computer Vision and Pattern Recognition. 6298- 6306.\n\nPFID: Pittsburgh fast-food image dataset. Mei Chen, Kapil Dhingra, Wen Wu, Lei Yang, Rahul Sukthankar, Jie Yang, IEEE International Conference on Image Processing. Mei Chen, Kapil Dhingra, Wen Wu, Lei Yang, Rahul Sukthankar, and Jie Yang. 2009. PFID: Pittsburgh fast-food image dataset. In IEEE International Conference on Image Processing. 289-292.\n\nChineseFoodNet: A large-scale image dataset for Chinese food recognition. Xin Chen, Hua Zhou, Liang Diao, abs/1705.02743CoRRXin Chen, Hua Zhou, and Liang Diao. 2017. ChineseFoodNet: A large-scale image dataset for Chinese food recognition. In CoRR, Vol. abs/1705.02743.\n\nDestruction and Construction Learning for Fine-Grained Image Recognition. Yue Chen, Yalong Bai, Wei Zhang, Tao Mei, IEEE International Conference on Computer Vision and Pattern Recognition. Yue Chen, Yalong Bai, Wei Zhang, and Tao Mei. 2019. Destruction and Con- struction Learning for Fine-Grained Image Recognition. In: IEEE International Conference on Computer Vision and Pattern Recognition (2019), 5157-5166.\n\nFood Recognition and Leftover Estimation for Daily Diet Monitoring. Gianluigi Ciocca, Paolo Napoletano, Raimondo Schettini, International Conference on Image Analysis and Processing. Gianluigi Ciocca, Paolo Napoletano, and Raimondo Schettini. 2015. Food Recog- nition and Leftover Estimation for Daily Diet Monitoring. In International Con- ference on Image Analysis and Processing. 334-341.\n\nFood recognition: a new dataset, experiments, and results. Gianluigi Ciocca, Paolo Napoletano, Raimondo Schettini, IEEE Journal of Biomedical and Health Informatics. 21Gianluigi Ciocca, Paolo Napoletano, and Raimondo Schettini. 2016. Food recog- nition: a new dataset, experiments, and results. In IEEE Journal of Biomedical and Health Informatics, Vol. 21. 588-598.\n\nIma-geNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Fei-Fei Li, IEEE Conference on Computer Vision and Pattern Recognition. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. Ima- geNet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition. 248-255.\n\nLixi Deng, Jingjing Chen, Qianru Sun, Xiangnan He, Sheng Tang, Zhaoyan Ming, Yongdong Zhang, and Tat-Seng Chua. 2019. Mixed-dish Recognition with. Lixi Deng, Jingjing Chen, Qianru Sun, Xiangnan He, Sheng Tang, Zhaoyan Ming, Yongdong Zhang, and Tat-Seng Chua. 2019. Mixed-dish Recognition with\n\n. Contextual Relation Networks. In ACM Multimedia. ACM. Contextual Relation Networks. In ACM Multimedia. ACM, 112-120.\n\nFood image recognition using very deep convolutional networks. Hamid Hassannejad, Guido Matrella, Paolo Ciampolini, Monica Ilaria De Munari, Stefano Mordonini, Cagnoni, International Workshop on Multimedia Assisted Dietary Management. Hamid Hassannejad, Guido Matrella, Paolo Ciampolini, Ilaria De Munari, Monica Mordonini, and Stefano Cagnoni. 2016. Food image recognition using very deep convolutional networks. In International Workshop on Multimedia Assisted Dietary Management. 41-49.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, IEEE Conference on Computer Vision and Pattern Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In IEEE Conference on Computer Vision and Pattern Recognition. 770-778.\n\nPersonalized Classifier for Food Image Recognition. S Horiguchi, S Amano, M Ogawa, K Aizawa, IEEE Transactions on Multimedia. 20S. Horiguchi, S. Amano, M. Ogawa, and K. Aizawa. 2018. Personalized Classifier for Food Image Recognition. IEEE Transactions on Multimedia 20, 10 (2018), 2836-2848.\n\nSqueeze-and-Excitation Networks. Jie Hu, Li Shen, Gang Sun, IEEE Conference on Computer Vision and Pattern Recognition. Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-Excitation Networks. In IEEE Conference on Computer Vision and Pattern Recognition. 7132-7141.\n\nSee Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification. Tao Hu, Honggang Qi, CoRR abs/1901.09891Tao Hu and Honggang Qi. 2019. See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification. CoRR abs/1901.09891.\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Q Kilian, Laurens Weinberger, Van Der Maaten, IEEE Conference on Computer Vision and Pattern Recognition. Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. 2017. Densely connected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition. 2261-2269.\n\nSpatial Transformer Networks. Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. 2015. Spatial Transformer Networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. 2017- 2025.\n\nScraping Social Media Photos Posted in Kenya and Elsewhere to Detect and Analyze Food Types. Mona Jalal, Kaihong Wang, Sankara Jefferson, Yi Zheng, Elaine O Nsoesie, Margrit Betke, Proceedings of the 5th International Workshop on Multimedia Assisted Dietary Management. the 5th International Workshop on Multimedia Assisted Dietary ManagementMona Jalal, Kaihong Wang, Sankara Jefferson, Yi Zheng, Elaine O. Nsoesie, and Margrit Betke. 2019. Scraping Social Media Photos Posted in Kenya and Elsewhere to Detect and Analyze Food Types. In Proceedings of the 5th International Workshop on Multimedia Assisted Dietary Management. 50-59.\n\nMulti-Scale Multi-View Deep Feature Aggregation for Food Recognition. Shuqiang Jiang, Weiqing Min, Linhu Liu, Zhengdong Luo, IEEE Transactions on Image Processing. 29Shuqiang Jiang, Weiqing Min, Linhu Liu, and Zhengdong Luo. 2019. Multi-Scale Multi-View Deep Feature Aggregation for Food Recognition. IEEE Transactions on Image Processing 29, 1, 265-276.\n\nFood detection and recognition using convolutional neural network. Hokuto Kagaya, Kiyoharu Aizawa, Makoto Ogawa, Proceedings of the ACM International Conference on Multimedia. the ACM International Conference on MultimediaHokuto Kagaya, Kiyoharu Aizawa, and Makoto Ogawa. 2014. Food detection and recognition using convolutional neural network. In Proceedings of the ACM International Conference on Multimedia. 1085-1088.\n\nFoodX-251: A Dataset for Fine-grained Food Classification. Parneet Kaur, Karan Sikka, Weijun Wang, Serge J Belongie, Ajay Divakaran, IEEE Conference on Computer Vision and Pattern Recognition Workshops. Parneet Kaur, Karan Sikka, Weijun Wang, Serge J. Belongie, and Ajay Divakaran. 2019. FoodX-251: A Dataset for Fine-grained Food Classification. In IEEE Confer- ence on Computer Vision and Pattern Recognition Workshops.\n\nAutomatic expansion of a food image dataset leveraging existing categories with domain adaptation. Yoshiyuki Kawano, Keiji Yanai, European Conference on Computer Vision. Yoshiyuki Kawano and Keiji Yanai. 2014. Automatic expansion of a food im- age dataset leveraging existing categories with domain adaptation. In European Conference on Computer Vision. 3-17.\n\nDo Better ImageNet Models Transfer Better?. Simon Kornblith, Jonathon Shlens, Quoc Le, IEEE Conference on Computer Vision and Pattern Recognition. Simon Kornblith, Jonathon Shlens, and Quoc Le. 2019. Do Better ImageNet Models Transfer Better?. In IEEE Conference on Computer Vision and Pattern Recognition. 2661-2671.\n\nHarmonious Attention Network for Person Re-identification. W Li, X Zhu, S Gong, IEEE Conference on Computer Vision and Pattern Recognition. W. Li, X. Zhu, and S. Gong. 2018. Harmonious Attention Network for Person Re-identification. In IEEE Conference on Computer Vision and Pattern Recognition. 2285-2294.\n\nDeepfood: Deep learning-based food image recognition for computeraided dietary assessment. Chang Liu, Yu Cao, Yan Luo, Guanling Chen, Vinod Vokkarane, Yunsheng Ma, International Conference on Smart Homes and Health Telematics. Chang Liu, Yu Cao, Yan Luo, Guanling Chen, Vinod Vokkarane, and Yunsheng Ma. 2016. Deepfood: Deep learning-based food image recognition for computer- aided dietary assessment. In International Conference on Smart Homes and Health Telematics. 37-48.\n\nPay attention to the activations: a modular attention mechanism for fine-grained image recognition. Diego Pau Rodr\u013aguez L\u00f3pez, Guillem Velazquez Dorta, Josep M Cucurull Preixens, Jordi Gonz\u00e0lez Gonfaus, Sabat\u00e9, IEEE Transactions on Multimedia. 22Pau Rodr\u013aguez L\u00f3pez, Diego Velazquez Dorta, Guillem Cucurull Preixens, Josep M. Gonfaus, and Jordi Gonz\u00e0lez Sabat\u00e9. 2020. Pay attention to the activations: a modular attention mechanism for fine-grained image recognition. In IEEE Transactions on Multimedia, Vol. 22. 502-514.\n\nWide-slice residual networks for food recognition. Niki Martinel, Gian Luca Foresti, Christian Micheloni, IEEE Winter Conference on Applications of Computer Vision. Niki Martinel, Gian Luca Foresti, and Christian Micheloni. 2018. Wide-slice residual networks for food recognition. In IEEE Winter Conference on Applications of Computer Vision. 567-576.\n\nA supervised extreme learning committee for food recognition. Niki Martinel, Claudio Piciarelli, Christian Micheloni, Computer Vision and Image Understanding. Elsevier148Niki Martinel, Claudio Piciarelli, and Christian Micheloni. 2016. A supervised extreme learning committee for food recognition. In Computer Vision and Image Understanding, Vol. 148. Elsevier, 67-86.\n\nMultiple-food recognition considering cooccurrence employing manifold ranking. Yuji Matsuda, Keiji Yanai, International Conference on Pattern Recognition. Yuji Matsuda and Keiji Yanai. 2012. Multiple-food recognition considering co- occurrence employing manifold ranking. In International Conference on Pattern Recognition. 2017-2020.\n\nCombining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets. Patrick Mcallister, Huiru Zheng, Raymond Bond, Anne Moorhead, Computers in Biology and Medicine. Elsevier95Patrick McAllister, Huiru Zheng, Raymond Bond, and Anne Moorhead. 2018. Combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets. In Computers in Biology and Medicine, Vol. 95. Elsevier, 217-233.\n\nIm2Calories: towards an automated mobile vision food diary. Austin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, Kevin P Murphy, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionAustin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, and Kevin P Murphy. 2015. Im2Calories: towards an automated mobile vision food diary. In Proceedings of the IEEE International Conference on Computer Vision. 1233-1241.\n\nYou are what you eat: Exploring rich recipe information for crossregion food analysis. Weiqing Min, Bing-Kun Bao, Shuhuan Mei, Yaohui Zhu, Yong Rui, Shuqiang Jiang, IEEE Transactions on Multimedia. 20Weiqing Min, Bing-Kun Bao, Shuhuan Mei, Yaohui Zhu, Yong Rui, and Shuqiang Jiang. 2018. You are what you eat: Exploring rich recipe information for cross- region food analysis. IEEE Transactions on Multimedia 20, 4 (2018), 950-964.\n\nA Survey on Food Computing. Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, Ramesh Jain, ACM Computing Surveys. 52Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, and Ramesh Jain. 2019. A Survey on Food Computing. In ACM Computing Surveys, Vol. 52. 1-36.\n\nBeing a Super Cook: Joint Food Attributes and Multi-Modal Content Modeling for Recipe Retrieval and Exploration. Weiqing Min, Shuqiang Jiang, Jitao Sang, Huayang Wang, Xinda Liu, Luis Herranz, IEEE Transactions on Multimedia. 19Weiqing Min, Shuqiang Jiang, Jitao Sang, Huayang Wang, Xinda Liu, and Luis Herranz. 2017. Being a Super Cook: Joint Food Attributes and Multi-Modal Content Modeling for Recipe Retrieval and Exploration. IEEE Transactions on Multimedia 19, 5 (2017), 1100-1113.\n\nA Delicious Recipe Analysis Framework for Exploring Multi-Modal Recipes with Various Attributes. Weiqing Min, Shuqiang Jiang, Shuhui Wang, Jitao Sang, Shuhuan Mei, ACM Multimedia. ACM. Weiqing Min, Shuqiang Jiang, Shuhui Wang, Jitao Sang, and Shuhuan Mei. 2017. A Delicious Recipe Analysis Framework for Exploring Multi-Modal Recipes with Various Attributes. In ACM Multimedia. ACM, 402-410.\n\nIngredient-Guided Cascaded Multi-Attention Network for Food Recognition. Weiqing Min, Linhu Liu, Zhengdong Luo, Shuqiang Jiang, Proceedings of the ACM International Conference on Multimedia. the ACM International Conference on MultimediaWeiqing Min, Linhu Liu, Zhengdong Luo, and Shuqiang Jiang. 2019. Ingredient- Guided Cascaded Multi-Attention Network for Food Recognition. In Proceedings of the ACM International Conference on Multimedia. 1331-1339.\n\nHealth multimedia: Lifestyle recommendations based on diverse observations. Nitish Nag, Vaibhav Pandey, Ramesh Jain, Proceedings of the ACM on International Conference on Multimedia Retrieval. the ACM on International Conference on Multimedia RetrievalNitish Nag, Vaibhav Pandey, and Ramesh Jain. 2017. Health multimedia: Lifestyle recommendations based on diverse observations. In Proceedings of the ACM on International Conference on Multimedia Retrieval. 99-106.\n\nFoodNet: Recognizing foods using ensemble of deep networks. Paritosh Pandey, Akella Deepthi, Bappaditya Mandal, N B Puhan, IEEE Signal Processing Letters. 24Paritosh Pandey, Akella Deepthi, Bappaditya Mandal, and N. B. Puhan. 2017. FoodNet: Recognizing foods using ensemble of deep networks. In IEEE Signal Processing Letters, Vol. 24. 1758-1762.\n\nMining Discriminative Food Regions for Accurate Food Recognition. Jianing Qiu, P.-W Frank, Yingnan Lo, Siyao Sun, Benny Wang, Lo, British Machine Vision Conference. AcceptedJianing Qiu, Frank P.-W. Lo, Yingnan Sun, Siyao Wang, and Benny Lo. 2019. Mining Discriminative Food Regions for Accurate Food Recognition. In British Machine Vision Conference (Accepted).\n\nLearning cross-modal embeddings for cooking recipes and food images. Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, Antonio Torralba, IEEE Conference on Computer Vision and Pattern Recognition. Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, and Antonio Torralba. 2017. Learning cross-modal embeddings for cook- ing recipes and food images. In IEEE Conference on Computer Vision and Pattern Recognition. 3020-3028.\n\nWide Residual Networks. Zagoruyko Sergey, Komodakis Nikos, Proceedings of the British Machine Vision Conference. the British Machine Vision ConferenceBMVA Press87Zagoruyko Sergey and Komodakis Nikos. 2016. Wide Residual Networks. In Proceedings of the British Machine Vision Conference. BMVA Press, 87.1-87.12.\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, IEEE Conference on Computer Vision and Pattern Recognition. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition. 1-9.\n\nRecipe recognition with large multimodal food dataset. Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, Frederic Precioso, IEEE International Conference on Multimedia and Expo Workshops. Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, and Frederic Pre- cioso. 2015. Recipe recognition with large multimodal food dataset. In IEEE International Conference on Multimedia and Expo Workshops. 1-6.\n\nDeep Learning for Fine-Grained Image Analysis: A Survey. Xiu-Shen Wei, Jianxin Wu, Quan Cui, CoRR abs/1907.03069Xiu-Shen Wei, Jianxin Wu, and Quan Cui. 2019. Deep Learning for Fine-Grained Image Analysis: A Survey. CoRR abs/1907.03069 (2019).\n\nLearning to make better mistakes: Semantics-aware visual food recognition. Hui Wu, Michele Merler, Rosario Uceda-Sosa, John R Smith, ACM Multimedia Conference. Hui Wu, Michele Merler, Rosario Uceda-Sosa, and John R Smith. 2016. Learn- ing to make better mistakes: Semantics-aware visual food recognition. In ACM Multimedia Conference. 172-176.\n\nGeolocalized modeling for dish recognition. Ruihan Xu, Luis Herranz, Shuqiang Jiang, Shuang Wang, Xinhang Song, Ramesh Jain, IEEE Transactions on Multimedia. 17Ruihan Xu, Luis Herranz, Shuqiang Jiang, Shuang Wang, Xinhang Song, and Ramesh Jain. 2015. Geolocalized modeling for dish recognition. In IEEE Transac- tions on Multimedia, Vol. 17. 1187-1199.\n\nFood image recognition using deep convolutional network with pre-training and fine-tuning. Keiji Yanai, Yoshiyuki Kawano, IEEE International Conference on Multimedia and Expo Workshops. Keiji Yanai and Yoshiyuki Kawano. 2015. Food image recognition using deep convolutional network with pre-training and fine-tuning. In IEEE International Conference on Multimedia and Expo Workshops. 1-6.\n\nFood recognition using statistics of pairwise local features. Shulin Yang, Mei Chen, Dean Pomerleau, Rahul Sukthankar, IEEE Conference on Computer Vision and Pattern Recognition. Shulin Yang, Mei Chen, Dean Pomerleau, and Rahul Sukthankar. 2010. Food recognition using statistics of pairwise local features. In IEEE Conference on Computer Vision and Pattern Recognition. 2249-2256.\n\nMulti-scale Recognition with DAG-CNNs. S Yang, D Ramanan, IEEE International Conference on Computer Vision. S. Yang and D. Ramanan. 2015. Multi-scale Recognition with DAG-CNNs. In IEEE International Conference on Computer Vision. 1215-1223.\n\nLearning to Navigate for Fine-Grained Classification. Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, Jun Gao, Liwei Wang, European Conference on Computer Vision. Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, Jun Gao, and Liwei Wang. 2018. Learning to Navigate for Fine-Grained Classification. In European Conference on Computer Vision. 438-454.\n\nLarge Scale Fine-Grained Categorization and Domain-Specific Transfer Learning. Cui Yin, Song Yang, Sun Chen, Howard Andrew, Belongie Serge, IEEE Conference on Computer Vision and Pattern Recognition. Cui Yin, Song Yang, Sun Chen, Howard Andrew, and Belongie Serge. 2018. Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning. In IEEE Conference on Computer Vision and Pattern Recognition. 4109-4118.\n\nDeep Layer Aggregation. F Yu, D Wang, E Shelhamer, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition. F. Yu, D. Wang, E. Shelhamer, and T. Darrell. 2018. Deep Layer Aggregation. In IEEE Conference on Computer Vision and Pattern Recognition. 2403-2412.\n\nPlaces: A 10 Million Image Database for Scene Recognition. Bolei Zhou, \u00c0gata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, IEEE Transactions on Pattern Analysis and Machine Intelligence. 40Bolei Zhou, \u00c0gata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2018. Places: A 10 Million Image Database for Scene Recognition. In IEEE Trans- actions on Pattern Analysis and Machine Intelligence, Vol. 40. 1452-1464.\n\nFine-Grained Image Classification by Exploring Bipartite-Graph Labels. Feng Zhou, Yuanqing Lin, IEEE Conference on Computer Vision and Pattern Recognition. Feng Zhou and Yuanqing Lin. 2016. Fine-Grained Image Classification by Explor- ing Bipartite-Graph Labels. In IEEE Conference on Computer Vision and Pattern Recognition. 1124-1133.\n\nLearning Transferable Architectures for Scalable Image Recognition. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc Le, IEEE Conference on Computer Vision and Pattern Recognition. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc Le. 2018. Learning Transferable Architectures for Scalable Image Recognition. In IEEE Conference on Computer Vision and Pattern Recognition. 8697-8710.\n", "annotations": {"author": "[{\"end\":480,\"start\":281},{\"end\":682,\"start\":481},{\"end\":890,\"start\":683},{\"end\":1094,\"start\":891},{\"end\":1157,\"start\":1095},{\"end\":1220,\"start\":1158},{\"end\":1420,\"start\":1221},{\"end\":1599,\"start\":1421},{\"end\":1776,\"start\":1600},{\"end\":1956,\"start\":1777},{\"end\":2137,\"start\":1957},{\"end\":2177,\"start\":2138},{\"end\":2215,\"start\":2178},{\"end\":2397,\"start\":2216}]", "publisher": null, "author_last_name": "[{\"end\":292,\"start\":289},{\"end\":490,\"start\":487},{\"end\":695,\"start\":691},{\"end\":904,\"start\":901},{\"end\":1107,\"start\":1104},{\"end\":1169,\"start\":1166},{\"end\":1235,\"start\":1230},{\"end\":1432,\"start\":1429},{\"end\":1609,\"start\":1606},{\"end\":1789,\"start\":1785},{\"end\":1970,\"start\":1967},{\"end\":2151,\"start\":2148},{\"end\":2189,\"start\":2186},{\"end\":2230,\"start\":2225}]", "author_first_name": "[{\"end\":288,\"start\":281},{\"end\":486,\"start\":481},{\"end\":690,\"start\":683},{\"end\":900,\"start\":891},{\"end\":1103,\"start\":1095},{\"end\":1165,\"start\":1158},{\"end\":1229,\"start\":1221},{\"end\":1428,\"start\":1421},{\"end\":1605,\"start\":1600},{\"end\":1784,\"start\":1777},{\"end\":1966,\"start\":1957},{\"end\":2147,\"start\":2138},{\"end\":2185,\"start\":2178},{\"end\":2224,\"start\":2216}]", "author_affiliation": "[{\"end\":417,\"start\":315},{\"end\":479,\"start\":419},{\"end\":619,\"start\":517},{\"end\":681,\"start\":621},{\"end\":827,\"start\":725},{\"end\":889,\"start\":829},{\"end\":1031,\"start\":929},{\"end\":1093,\"start\":1033},{\"end\":1156,\"start\":1133},{\"end\":1219,\"start\":1196},{\"end\":1357,\"start\":1255},{\"end\":1419,\"start\":1359},{\"end\":1536,\"start\":1434},{\"end\":1598,\"start\":1538},{\"end\":1713,\"start\":1611},{\"end\":1775,\"start\":1715},{\"end\":1893,\"start\":1791},{\"end\":1955,\"start\":1895},{\"end\":2074,\"start\":1972},{\"end\":2136,\"start\":2076},{\"end\":2176,\"start\":2153},{\"end\":2214,\"start\":2191},{\"end\":2334,\"start\":2232},{\"end\":2396,\"start\":2336}]", "title": "[{\"end\":238,\"start\":1},{\"end\":2635,\"start\":2398}]", "venue": "[{\"end\":2712,\"start\":2637}]", "abstract": "[{\"end\":4728,\"start\":3103}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4763,\"start\":4759},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5010,\"start\":5006},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5013,\"start\":5010},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5016,\"start\":5013},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5019,\"start\":5016},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5080,\"start\":5076},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5114,\"start\":5110},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5147,\"start\":5144},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5466,\"start\":5463},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5489,\"start\":5486},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5522,\"start\":5519},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5644,\"start\":5641},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6429,\"start\":6426},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6484,\"start\":6480},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7204,\"start\":7201},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7207,\"start\":7204},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7210,\"start\":7207},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7213,\"start\":7210},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9040,\"start\":9037},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9076,\"start\":9072},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9113,\"start\":9109},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9151,\"start\":9148},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9190,\"start\":9186},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9225,\"start\":9221},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9256,\"start\":9252},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9291,\"start\":9287},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9330,\"start\":9327},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9368,\"start\":9364},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9398,\"start\":9394},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9431,\"start\":9427},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9876,\"start\":9873},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9878,\"start\":9876},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9881,\"start\":9878},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9884,\"start\":9881},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9887,\"start\":9884},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9890,\"start\":9887},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10034,\"start\":10031},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10114,\"start\":10111},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10145,\"start\":10142},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10298,\"start\":10294},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":10328,\"start\":10324},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10638,\"start\":10634},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10654,\"start\":10650},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10672,\"start\":10668},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10953,\"start\":10949},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11098,\"start\":11095},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":11101,\"start\":11098},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11132,\"start\":11129},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11401,\"start\":11397},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11404,\"start\":11401},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11407,\"start\":11404},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11519,\"start\":11516},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11522,\"start\":11519},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11525,\"start\":11522},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11595,\"start\":11591},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":12147,\"start\":12143},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17240,\"start\":17237},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17261,\"start\":17257},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17647,\"start\":17643},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17948,\"start\":17944},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18441,\"start\":18437},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18843,\"start\":18839},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20366,\"start\":20362},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20815,\"start\":20812},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":22138,\"start\":22134},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22154,\"start\":22150},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23115,\"start\":23112},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24448,\"start\":24445},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24606,\"start\":24602},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24620,\"start\":24616},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":24652,\"start\":24648},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25448,\"start\":25445},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27105,\"start\":27101}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27190,\"start\":27146},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28041,\"start\":27191},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28435,\"start\":28042},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28540,\"start\":28436},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28695,\"start\":28541},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28824,\"start\":28696},{\"attributes\":{\"id\":\"fig_8\"},\"end\":28917,\"start\":28825},{\"attributes\":{\"id\":\"fig_9\"},\"end\":29103,\"start\":28918},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29167,\"start\":29104},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29386,\"start\":29168},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29891,\"start\":29387},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30331,\"start\":29892},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31334,\"start\":30332},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32296,\"start\":31335}]", "paragraph": "[{\"end\":5148,\"start\":4744},{\"end\":5292,\"start\":5150},{\"end\":5950,\"start\":5294},{\"end\":6563,\"start\":5952},{\"end\":8594,\"start\":6565},{\"end\":8868,\"start\":8596},{\"end\":9529,\"start\":8870},{\"end\":9786,\"start\":9531},{\"end\":10909,\"start\":9803},{\"end\":12063,\"start\":10911},{\"end\":12435,\"start\":12065},{\"end\":12640,\"start\":12478},{\"end\":13209,\"start\":12642},{\"end\":13883,\"start\":13211},{\"end\":14847,\"start\":13885},{\"end\":15544,\"start\":14890},{\"end\":16494,\"start\":15546},{\"end\":16990,\"start\":16517},{\"end\":17384,\"start\":16992},{\"end\":17648,\"start\":17386},{\"end\":17849,\"start\":17668},{\"end\":17972,\"start\":17884},{\"end\":18211,\"start\":18059},{\"end\":18705,\"start\":18213},{\"end\":19191,\"start\":18716},{\"end\":19338,\"start\":19193},{\"end\":19527,\"start\":19368},{\"end\":19732,\"start\":19561},{\"end\":20026,\"start\":19771},{\"end\":20389,\"start\":20064},{\"end\":21265,\"start\":20421},{\"end\":21871,\"start\":21267},{\"end\":22432,\"start\":21873},{\"end\":22700,\"start\":22443},{\"end\":23319,\"start\":22739},{\"end\":24144,\"start\":23321},{\"end\":25225,\"start\":24179},{\"end\":26152,\"start\":25227},{\"end\":26674,\"start\":26168},{\"end\":27145,\"start\":26676}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17667,\"start\":17649},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17883,\"start\":17850},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18058,\"start\":17973},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19367,\"start\":19339},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19770,\"start\":19733}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9915,\"start\":9908},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20719,\"start\":20712},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21062,\"start\":21055},{\"end\":21337,\"start\":21330},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21973,\"start\":21966},{\"end\":22380,\"start\":22373},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24516,\"start\":24509},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25457,\"start\":25450}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4742,\"start\":4730},{\"attributes\":{\"n\":\"2\"},\"end\":9801,\"start\":9789},{\"attributes\":{\"n\":\"3\"},\"end\":12476,\"start\":12438},{\"attributes\":{\"n\":\"3.2\"},\"end\":14888,\"start\":14850},{\"attributes\":{\"n\":\"4\"},\"end\":16506,\"start\":16497},{\"attributes\":{\"n\":\"4.1\"},\"end\":16515,\"start\":16509},{\"attributes\":{\"n\":\"4.2\"},\"end\":18714,\"start\":18708},{\"attributes\":{\"n\":\"4.3\"},\"end\":19559,\"start\":19530},{\"attributes\":{\"n\":\"5\"},\"end\":20062,\"start\":20029},{\"attributes\":{\"n\":\"5.2\"},\"end\":20419,\"start\":20392},{\"end\":22441,\"start\":22435},{\"end\":22737,\"start\":22703},{\"attributes\":{\"n\":\"5.3\"},\"end\":24177,\"start\":24147},{\"attributes\":{\"n\":\"6\"},\"end\":26166,\"start\":26155},{\"end\":27157,\"start\":27147},{\"end\":27198,\"start\":27192},{\"end\":28063,\"start\":28043},{\"end\":28447,\"start\":28437},{\"end\":28552,\"start\":28542},{\"end\":28707,\"start\":28697},{\"end\":28836,\"start\":28826},{\"end\":28929,\"start\":28919},{\"end\":29114,\"start\":29105},{\"end\":29178,\"start\":29169},{\"end\":29397,\"start\":29388},{\"end\":29902,\"start\":29893},{\"end\":30342,\"start\":30333},{\"end\":31345,\"start\":31336}]", "table": "[{\"end\":29386,\"start\":29241},{\"end\":29891,\"start\":29399},{\"end\":30331,\"start\":29904},{\"end\":31334,\"start\":30388},{\"end\":32296,\"start\":31739}]", "figure_caption": "[{\"end\":27190,\"start\":27159},{\"end\":28041,\"start\":27200},{\"end\":28435,\"start\":28066},{\"end\":28540,\"start\":28449},{\"end\":28695,\"start\":28554},{\"end\":28824,\"start\":28709},{\"end\":28917,\"start\":28838},{\"end\":29103,\"start\":28931},{\"end\":29167,\"start\":29116},{\"end\":29241,\"start\":29180},{\"end\":30388,\"start\":30344},{\"end\":31739,\"start\":31347}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6086,\"start\":6080},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6844,\"start\":6838},{\"end\":15014,\"start\":15008},{\"end\":15098,\"start\":15092},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15842,\"start\":15836},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22839,\"start\":22833},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":23516,\"start\":23509},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":23894,\"start\":23888}]", "bib_author_first_name": "[{\"end\":33423,\"start\":33416},{\"end\":33437,\"start\":33433},{\"end\":33452,\"start\":33447},{\"end\":33779,\"start\":33772},{\"end\":33796,\"start\":33789},{\"end\":33812,\"start\":33808},{\"end\":33827,\"start\":33822},{\"end\":34144,\"start\":34138},{\"end\":34156,\"start\":34149},{\"end\":34158,\"start\":34157},{\"end\":34478,\"start\":34473},{\"end\":34498,\"start\":34492},{\"end\":34511,\"start\":34507},{\"end\":34522,\"start\":34521},{\"end\":34537,\"start\":34532},{\"end\":34894,\"start\":34890},{\"end\":34909,\"start\":34904},{\"end\":35186,\"start\":35181},{\"end\":35204,\"start\":35196},{\"end\":35220,\"start\":35217},{\"end\":35519,\"start\":35511},{\"end\":35535,\"start\":35526},{\"end\":35878,\"start\":35877},{\"end\":35886,\"start\":35885},{\"end\":35895,\"start\":35894},{\"end\":35903,\"start\":35902},{\"end\":35910,\"start\":35909},{\"end\":35918,\"start\":35917},{\"end\":35925,\"start\":35924},{\"end\":36278,\"start\":36275},{\"end\":36290,\"start\":36285},{\"end\":36303,\"start\":36300},{\"end\":36311,\"start\":36308},{\"end\":36323,\"start\":36318},{\"end\":36339,\"start\":36336},{\"end\":36661,\"start\":36658},{\"end\":36671,\"start\":36668},{\"end\":36683,\"start\":36678},{\"end\":36932,\"start\":36929},{\"end\":36945,\"start\":36939},{\"end\":36954,\"start\":36951},{\"end\":36965,\"start\":36962},{\"end\":37347,\"start\":37338},{\"end\":37361,\"start\":37356},{\"end\":37382,\"start\":37374},{\"end\":37731,\"start\":37722},{\"end\":37745,\"start\":37740},{\"end\":37766,\"start\":37758},{\"end\":38088,\"start\":38085},{\"end\":38098,\"start\":38095},{\"end\":38112,\"start\":38105},{\"end\":38127,\"start\":38121},{\"end\":38135,\"start\":38132},{\"end\":38147,\"start\":38140},{\"end\":38421,\"start\":38417},{\"end\":38436,\"start\":38428},{\"end\":38449,\"start\":38443},{\"end\":38463,\"start\":38455},{\"end\":38899,\"start\":38894},{\"end\":38918,\"start\":38913},{\"end\":38934,\"start\":38929},{\"end\":38953,\"start\":38947},{\"end\":38979,\"start\":38972},{\"end\":39375,\"start\":39368},{\"end\":39387,\"start\":39380},{\"end\":39403,\"start\":39395},{\"end\":39413,\"start\":39409},{\"end\":39712,\"start\":39711},{\"end\":39725,\"start\":39724},{\"end\":39734,\"start\":39733},{\"end\":39743,\"start\":39742},{\"end\":39989,\"start\":39986},{\"end\":39996,\"start\":39994},{\"end\":40007,\"start\":40003},{\"end\":40339,\"start\":40336},{\"end\":40352,\"start\":40344},{\"end\":40591,\"start\":40588},{\"end\":40605,\"start\":40599},{\"end\":40612,\"start\":40611},{\"end\":40628,\"start\":40621},{\"end\":40945,\"start\":40942},{\"end\":40962,\"start\":40957},{\"end\":40979,\"start\":40973},{\"end\":40996,\"start\":40991},{\"end\":41459,\"start\":41455},{\"end\":41474,\"start\":41467},{\"end\":41488,\"start\":41481},{\"end\":41502,\"start\":41500},{\"end\":41516,\"start\":41510},{\"end\":41518,\"start\":41517},{\"end\":41535,\"start\":41528},{\"end\":42074,\"start\":42066},{\"end\":42089,\"start\":42082},{\"end\":42100,\"start\":42095},{\"end\":42115,\"start\":42106},{\"end\":42425,\"start\":42419},{\"end\":42442,\"start\":42434},{\"end\":42457,\"start\":42451},{\"end\":42841,\"start\":42834},{\"end\":42853,\"start\":42848},{\"end\":42867,\"start\":42861},{\"end\":42879,\"start\":42874},{\"end\":42881,\"start\":42880},{\"end\":42896,\"start\":42892},{\"end\":43306,\"start\":43297},{\"end\":43320,\"start\":43315},{\"end\":43608,\"start\":43603},{\"end\":43628,\"start\":43620},{\"end\":43641,\"start\":43637},{\"end\":43938,\"start\":43937},{\"end\":43944,\"start\":43943},{\"end\":43951,\"start\":43950},{\"end\":44282,\"start\":44277},{\"end\":44290,\"start\":44288},{\"end\":44299,\"start\":44296},{\"end\":44313,\"start\":44305},{\"end\":44325,\"start\":44320},{\"end\":44345,\"start\":44337},{\"end\":44768,\"start\":44763},{\"end\":44797,\"start\":44790},{\"end\":44820,\"start\":44815},{\"end\":44822,\"start\":44821},{\"end\":44856,\"start\":44842},{\"end\":45241,\"start\":45237},{\"end\":45256,\"start\":45252},{\"end\":45261,\"start\":45257},{\"end\":45280,\"start\":45271},{\"end\":45605,\"start\":45601},{\"end\":45623,\"start\":45616},{\"end\":45645,\"start\":45636},{\"end\":45992,\"start\":45988},{\"end\":46007,\"start\":46002},{\"end\":46385,\"start\":46378},{\"end\":46403,\"start\":46398},{\"end\":46418,\"start\":46411},{\"end\":46429,\"start\":46425},{\"end\":46823,\"start\":46817},{\"end\":46836,\"start\":46832},{\"end\":46852,\"start\":46847},{\"end\":46866,\"start\":46861},{\"end\":46884,\"start\":46880},{\"end\":46899,\"start\":46893},{\"end\":46917,\"start\":46911},{\"end\":46936,\"start\":46930},{\"end\":46957,\"start\":46949},{\"end\":46970,\"start\":46965},{\"end\":46972,\"start\":46971},{\"end\":47514,\"start\":47507},{\"end\":47528,\"start\":47520},{\"end\":47541,\"start\":47534},{\"end\":47553,\"start\":47547},{\"end\":47563,\"start\":47559},{\"end\":47577,\"start\":47569},{\"end\":47888,\"start\":47881},{\"end\":47902,\"start\":47894},{\"end\":47915,\"start\":47910},{\"end\":47925,\"start\":47921},{\"end\":47937,\"start\":47931},{\"end\":48232,\"start\":48225},{\"end\":48246,\"start\":48238},{\"end\":48259,\"start\":48254},{\"end\":48273,\"start\":48266},{\"end\":48285,\"start\":48280},{\"end\":48295,\"start\":48291},{\"end\":48705,\"start\":48698},{\"end\":48719,\"start\":48711},{\"end\":48733,\"start\":48727},{\"end\":48745,\"start\":48740},{\"end\":48759,\"start\":48752},{\"end\":49074,\"start\":49067},{\"end\":49085,\"start\":49080},{\"end\":49100,\"start\":49091},{\"end\":49114,\"start\":49106},{\"end\":49530,\"start\":49524},{\"end\":49543,\"start\":49536},{\"end\":49558,\"start\":49552},{\"end\":49983,\"start\":49975},{\"end\":49998,\"start\":49992},{\"end\":50018,\"start\":50008},{\"end\":50028,\"start\":50027},{\"end\":50030,\"start\":50029},{\"end\":50336,\"start\":50329},{\"end\":50346,\"start\":50342},{\"end\":50361,\"start\":50354},{\"end\":50371,\"start\":50366},{\"end\":50382,\"start\":50377},{\"end\":50700,\"start\":50695},{\"end\":50719,\"start\":50711},{\"end\":50732,\"start\":50727},{\"end\":50746,\"start\":50740},{\"end\":50759,\"start\":50754},{\"end\":50772,\"start\":50766},{\"end\":50787,\"start\":50780},{\"end\":51150,\"start\":51141},{\"end\":51168,\"start\":51159},{\"end\":51470,\"start\":51461},{\"end\":51483,\"start\":51480},{\"end\":51497,\"start\":51489},{\"end\":51509,\"start\":51503},{\"end\":51525,\"start\":51520},{\"end\":51540,\"start\":51532},{\"end\":51558,\"start\":51551},{\"end\":51573,\"start\":51566},{\"end\":51591,\"start\":51585},{\"end\":51976,\"start\":51973},{\"end\":51991,\"start\":51983},{\"end\":52006,\"start\":51999},{\"end\":52022,\"start\":52014},{\"end\":52037,\"start\":52029},{\"end\":52392,\"start\":52384},{\"end\":52405,\"start\":52398},{\"end\":52414,\"start\":52410},{\"end\":52649,\"start\":52646},{\"end\":52661,\"start\":52654},{\"end\":52677,\"start\":52670},{\"end\":52696,\"start\":52690},{\"end\":52966,\"start\":52960},{\"end\":52975,\"start\":52971},{\"end\":52993,\"start\":52985},{\"end\":53007,\"start\":53001},{\"end\":53021,\"start\":53014},{\"end\":53034,\"start\":53028},{\"end\":53366,\"start\":53361},{\"end\":53383,\"start\":53374},{\"end\":53728,\"start\":53722},{\"end\":53738,\"start\":53735},{\"end\":53749,\"start\":53745},{\"end\":53766,\"start\":53761},{\"end\":54083,\"start\":54082},{\"end\":54091,\"start\":54090},{\"end\":54341,\"start\":54339},{\"end\":54354,\"start\":54348},{\"end\":54364,\"start\":54360},{\"end\":54379,\"start\":54371},{\"end\":54387,\"start\":54384},{\"end\":54398,\"start\":54393},{\"end\":54710,\"start\":54707},{\"end\":54720,\"start\":54716},{\"end\":54730,\"start\":54727},{\"end\":54743,\"start\":54737},{\"end\":54760,\"start\":54752},{\"end\":55078,\"start\":55077},{\"end\":55084,\"start\":55083},{\"end\":55092,\"start\":55091},{\"end\":55105,\"start\":55104},{\"end\":55390,\"start\":55385},{\"end\":55402,\"start\":55397},{\"end\":55420,\"start\":55414},{\"end\":55433,\"start\":55429},{\"end\":55448,\"start\":55441},{\"end\":55833,\"start\":55829},{\"end\":55848,\"start\":55840},{\"end\":56170,\"start\":56164},{\"end\":56182,\"start\":56177},{\"end\":56202,\"start\":56194},{\"end\":56215,\"start\":56211}]", "bib_author_last_name": "[{\"end\":33431,\"start\":33424},{\"end\":33445,\"start\":33438},{\"end\":33459,\"start\":33453},{\"end\":33787,\"start\":33780},{\"end\":33806,\"start\":33797},{\"end\":33820,\"start\":33813},{\"end\":33834,\"start\":33828},{\"end\":34147,\"start\":34145},{\"end\":34163,\"start\":34159},{\"end\":34490,\"start\":34479},{\"end\":34505,\"start\":34499},{\"end\":34519,\"start\":34512},{\"end\":34530,\"start\":34523},{\"end\":34543,\"start\":34538},{\"end\":34549,\"start\":34545},{\"end\":34902,\"start\":34895},{\"end\":34916,\"start\":34910},{\"end\":35194,\"start\":35187},{\"end\":35215,\"start\":35205},{\"end\":35229,\"start\":35221},{\"end\":35524,\"start\":35520},{\"end\":35539,\"start\":35536},{\"end\":35883,\"start\":35879},{\"end\":35892,\"start\":35887},{\"end\":35900,\"start\":35896},{\"end\":35907,\"start\":35904},{\"end\":35915,\"start\":35911},{\"end\":35922,\"start\":35919},{\"end\":35930,\"start\":35926},{\"end\":36283,\"start\":36279},{\"end\":36298,\"start\":36291},{\"end\":36306,\"start\":36304},{\"end\":36316,\"start\":36312},{\"end\":36334,\"start\":36324},{\"end\":36344,\"start\":36340},{\"end\":36666,\"start\":36662},{\"end\":36676,\"start\":36672},{\"end\":36688,\"start\":36684},{\"end\":36937,\"start\":36933},{\"end\":36949,\"start\":36946},{\"end\":36960,\"start\":36955},{\"end\":36969,\"start\":36966},{\"end\":37354,\"start\":37348},{\"end\":37372,\"start\":37362},{\"end\":37392,\"start\":37383},{\"end\":37738,\"start\":37732},{\"end\":37756,\"start\":37746},{\"end\":37776,\"start\":37767},{\"end\":38093,\"start\":38089},{\"end\":38103,\"start\":38099},{\"end\":38119,\"start\":38113},{\"end\":38130,\"start\":38128},{\"end\":38138,\"start\":38136},{\"end\":38150,\"start\":38148},{\"end\":38426,\"start\":38422},{\"end\":38441,\"start\":38437},{\"end\":38453,\"start\":38450},{\"end\":38466,\"start\":38464},{\"end\":38911,\"start\":38900},{\"end\":38927,\"start\":38919},{\"end\":38945,\"start\":38935},{\"end\":38970,\"start\":38954},{\"end\":38989,\"start\":38980},{\"end\":38998,\"start\":38991},{\"end\":39378,\"start\":39376},{\"end\":39393,\"start\":39388},{\"end\":39407,\"start\":39404},{\"end\":39417,\"start\":39414},{\"end\":39722,\"start\":39713},{\"end\":39731,\"start\":39726},{\"end\":39740,\"start\":39735},{\"end\":39750,\"start\":39744},{\"end\":39992,\"start\":39990},{\"end\":40001,\"start\":39997},{\"end\":40011,\"start\":40008},{\"end\":40342,\"start\":40340},{\"end\":40355,\"start\":40353},{\"end\":40597,\"start\":40592},{\"end\":40609,\"start\":40606},{\"end\":40619,\"start\":40613},{\"end\":40639,\"start\":40629},{\"end\":40655,\"start\":40641},{\"end\":40955,\"start\":40946},{\"end\":40971,\"start\":40963},{\"end\":40989,\"start\":40980},{\"end\":41008,\"start\":40997},{\"end\":41465,\"start\":41460},{\"end\":41479,\"start\":41475},{\"end\":41498,\"start\":41489},{\"end\":41508,\"start\":41503},{\"end\":41526,\"start\":41519},{\"end\":41541,\"start\":41536},{\"end\":42080,\"start\":42075},{\"end\":42093,\"start\":42090},{\"end\":42104,\"start\":42101},{\"end\":42119,\"start\":42116},{\"end\":42432,\"start\":42426},{\"end\":42449,\"start\":42443},{\"end\":42463,\"start\":42458},{\"end\":42846,\"start\":42842},{\"end\":42859,\"start\":42854},{\"end\":42872,\"start\":42868},{\"end\":42890,\"start\":42882},{\"end\":42906,\"start\":42897},{\"end\":43313,\"start\":43307},{\"end\":43326,\"start\":43321},{\"end\":43618,\"start\":43609},{\"end\":43635,\"start\":43629},{\"end\":43644,\"start\":43642},{\"end\":43941,\"start\":43939},{\"end\":43948,\"start\":43945},{\"end\":43956,\"start\":43952},{\"end\":44286,\"start\":44283},{\"end\":44294,\"start\":44291},{\"end\":44303,\"start\":44300},{\"end\":44318,\"start\":44314},{\"end\":44335,\"start\":44326},{\"end\":44348,\"start\":44346},{\"end\":44788,\"start\":44769},{\"end\":44813,\"start\":44798},{\"end\":44840,\"start\":44823},{\"end\":44864,\"start\":44857},{\"end\":44872,\"start\":44866},{\"end\":45250,\"start\":45242},{\"end\":45269,\"start\":45262},{\"end\":45290,\"start\":45281},{\"end\":45614,\"start\":45606},{\"end\":45634,\"start\":45624},{\"end\":45655,\"start\":45646},{\"end\":46000,\"start\":45993},{\"end\":46013,\"start\":46008},{\"end\":46396,\"start\":46386},{\"end\":46409,\"start\":46404},{\"end\":46423,\"start\":46419},{\"end\":46438,\"start\":46430},{\"end\":46830,\"start\":46824},{\"end\":46845,\"start\":46837},{\"end\":46859,\"start\":46853},{\"end\":46878,\"start\":46867},{\"end\":46891,\"start\":46885},{\"end\":46909,\"start\":46900},{\"end\":46928,\"start\":46918},{\"end\":46947,\"start\":46937},{\"end\":46963,\"start\":46958},{\"end\":46979,\"start\":46973},{\"end\":47518,\"start\":47515},{\"end\":47532,\"start\":47529},{\"end\":47545,\"start\":47542},{\"end\":47557,\"start\":47554},{\"end\":47567,\"start\":47564},{\"end\":47583,\"start\":47578},{\"end\":47892,\"start\":47889},{\"end\":47908,\"start\":47903},{\"end\":47919,\"start\":47916},{\"end\":47929,\"start\":47926},{\"end\":47942,\"start\":47938},{\"end\":48236,\"start\":48233},{\"end\":48252,\"start\":48247},{\"end\":48264,\"start\":48260},{\"end\":48278,\"start\":48274},{\"end\":48289,\"start\":48286},{\"end\":48303,\"start\":48296},{\"end\":48709,\"start\":48706},{\"end\":48725,\"start\":48720},{\"end\":48738,\"start\":48734},{\"end\":48750,\"start\":48746},{\"end\":48763,\"start\":48760},{\"end\":49078,\"start\":49075},{\"end\":49089,\"start\":49086},{\"end\":49104,\"start\":49101},{\"end\":49120,\"start\":49115},{\"end\":49534,\"start\":49531},{\"end\":49550,\"start\":49544},{\"end\":49563,\"start\":49559},{\"end\":49990,\"start\":49984},{\"end\":50006,\"start\":49999},{\"end\":50025,\"start\":50019},{\"end\":50036,\"start\":50031},{\"end\":50340,\"start\":50337},{\"end\":50352,\"start\":50347},{\"end\":50364,\"start\":50362},{\"end\":50375,\"start\":50372},{\"end\":50387,\"start\":50383},{\"end\":50391,\"start\":50389},{\"end\":50709,\"start\":50701},{\"end\":50725,\"start\":50720},{\"end\":50738,\"start\":50733},{\"end\":50752,\"start\":50747},{\"end\":50764,\"start\":50760},{\"end\":50778,\"start\":50773},{\"end\":50796,\"start\":50788},{\"end\":51157,\"start\":51151},{\"end\":51174,\"start\":51169},{\"end\":51478,\"start\":51471},{\"end\":51487,\"start\":51484},{\"end\":51501,\"start\":51498},{\"end\":51518,\"start\":51510},{\"end\":51530,\"start\":51526},{\"end\":51549,\"start\":51541},{\"end\":51564,\"start\":51559},{\"end\":51583,\"start\":51574},{\"end\":51602,\"start\":51592},{\"end\":51981,\"start\":51977},{\"end\":51997,\"start\":51992},{\"end\":52012,\"start\":52007},{\"end\":52027,\"start\":52023},{\"end\":52046,\"start\":52038},{\"end\":52396,\"start\":52393},{\"end\":52408,\"start\":52406},{\"end\":52418,\"start\":52415},{\"end\":52652,\"start\":52650},{\"end\":52668,\"start\":52662},{\"end\":52688,\"start\":52678},{\"end\":52702,\"start\":52697},{\"end\":52969,\"start\":52967},{\"end\":52983,\"start\":52976},{\"end\":52999,\"start\":52994},{\"end\":53012,\"start\":53008},{\"end\":53026,\"start\":53022},{\"end\":53039,\"start\":53035},{\"end\":53372,\"start\":53367},{\"end\":53390,\"start\":53384},{\"end\":53733,\"start\":53729},{\"end\":53743,\"start\":53739},{\"end\":53759,\"start\":53750},{\"end\":53777,\"start\":53767},{\"end\":54088,\"start\":54084},{\"end\":54099,\"start\":54092},{\"end\":54346,\"start\":54342},{\"end\":54358,\"start\":54355},{\"end\":54369,\"start\":54365},{\"end\":54382,\"start\":54380},{\"end\":54391,\"start\":54388},{\"end\":54403,\"start\":54399},{\"end\":54714,\"start\":54711},{\"end\":54725,\"start\":54721},{\"end\":54735,\"start\":54731},{\"end\":54750,\"start\":54744},{\"end\":54766,\"start\":54761},{\"end\":55081,\"start\":55079},{\"end\":55089,\"start\":55085},{\"end\":55102,\"start\":55093},{\"end\":55113,\"start\":55106},{\"end\":55395,\"start\":55391},{\"end\":55412,\"start\":55403},{\"end\":55427,\"start\":55421},{\"end\":55439,\"start\":55434},{\"end\":55457,\"start\":55449},{\"end\":55838,\"start\":55834},{\"end\":55852,\"start\":55849},{\"end\":56175,\"start\":56171},{\"end\":56192,\"start\":56183},{\"end\":56209,\"start\":56203},{\"end\":56218,\"start\":56216}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":26764261},\"end\":33705,\"start\":33356},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":26110209},\"end\":34065,\"start\":33707},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17322825},\"end\":34398,\"start\":34067},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1582990},\"end\":34840,\"start\":34400},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3087646},\"end\":35116,\"start\":34842},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12726540},\"end\":35445,\"start\":35118},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207240186},\"end\":35783,\"start\":35447},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206596371},\"end\":36231,\"start\":35785},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1548631},\"end\":36582,\"start\":36233},{\"attributes\":{\"doi\":\"abs/1705.02743\",\"id\":\"b9\"},\"end\":36853,\"start\":36584},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":198905043},\"end\":37268,\"start\":36855},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":828264},\"end\":37661,\"start\":37270},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2040113},\"end\":38029,\"start\":37663},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206597351},\"end\":38415,\"start\":38031},{\"attributes\":{\"id\":\"b14\"},\"end\":38709,\"start\":38417},{\"attributes\":{\"id\":\"b15\"},\"end\":38829,\"start\":38711},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207242933},\"end\":39320,\"start\":38831},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206594692},\"end\":39657,\"start\":39322},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4811437},\"end\":39951,\"start\":39659},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":140309863},\"end\":40216,\"start\":39953},{\"attributes\":{\"id\":\"b20\"},\"end\":40544,\"start\":40218},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":9433631},\"end\":40910,\"start\":40546},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6099034},\"end\":41360,\"start\":40912},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":202541092},\"end\":41994,\"start\":41362},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":199383016},\"end\":42350,\"start\":41996},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":5279671},\"end\":42773,\"start\":42352},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":196623408},\"end\":43196,\"start\":42775},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14915460},\"end\":43557,\"start\":43198},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":43928547},\"end\":43876,\"start\":43559},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3458516},\"end\":44184,\"start\":43878},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14068125},\"end\":44661,\"start\":44186},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":198985838},\"end\":45184,\"start\":44663},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":17006311},\"end\":45537,\"start\":45186},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8274048},\"end\":45907,\"start\":45539},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4067210},\"end\":46243,\"start\":45909},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4338664},\"end\":46755,\"start\":46245},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206770267},\"end\":47418,\"start\":46757},{\"attributes\":{\"id\":\"b37\"},\"end\":47851,\"start\":47420},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52069634},\"end\":48110,\"start\":47853},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6655663},\"end\":48599,\"start\":48112},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7452511},\"end\":48992,\"start\":48601},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":204837612},\"end\":49446,\"start\":48994},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":27107507},\"end\":49913,\"start\":49448},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1796334},\"end\":50261,\"start\":49915},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":211117398},\"end\":50624,\"start\":50263},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":7319196},\"end\":51115,\"start\":50626},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":15276198},\"end\":51427,\"start\":51117},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":206592484},\"end\":51916,\"start\":51429},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206822288},\"end\":52325,\"start\":51918},{\"attributes\":{\"doi\":\"CoRR abs/1907.03069\",\"id\":\"b49\"},\"end\":52569,\"start\":52327},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":207240270},\"end\":52914,\"start\":52571},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":10704475},\"end\":53268,\"start\":52916},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":206822389},\"end\":53658,\"start\":53270},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":304354},\"end\":54041,\"start\":53660},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":9860273},\"end\":54283,\"start\":54043},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":52155581},\"end\":54626,\"start\":54285},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":43993788},\"end\":55051,\"start\":54628},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":30834643},\"end\":55324,\"start\":55053},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":2608922},\"end\":55756,\"start\":55326},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":206593396},\"end\":56094,\"start\":55758},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":12227989},\"end\":56487,\"start\":56096}]", "bib_title": "[{\"end\":33414,\"start\":33356},{\"end\":33770,\"start\":33707},{\"end\":34136,\"start\":34067},{\"end\":34471,\"start\":34400},{\"end\":34888,\"start\":34842},{\"end\":35179,\"start\":35118},{\"end\":35509,\"start\":35447},{\"end\":35875,\"start\":35785},{\"end\":36273,\"start\":36233},{\"end\":36927,\"start\":36855},{\"end\":37336,\"start\":37270},{\"end\":37720,\"start\":37663},{\"end\":38083,\"start\":38031},{\"end\":38892,\"start\":38831},{\"end\":39366,\"start\":39322},{\"end\":39709,\"start\":39659},{\"end\":39984,\"start\":39953},{\"end\":40586,\"start\":40546},{\"end\":40940,\"start\":40912},{\"end\":41453,\"start\":41362},{\"end\":42064,\"start\":41996},{\"end\":42417,\"start\":42352},{\"end\":42832,\"start\":42775},{\"end\":43295,\"start\":43198},{\"end\":43601,\"start\":43559},{\"end\":43935,\"start\":43878},{\"end\":44275,\"start\":44186},{\"end\":44761,\"start\":44663},{\"end\":45235,\"start\":45186},{\"end\":45599,\"start\":45539},{\"end\":45986,\"start\":45909},{\"end\":46376,\"start\":46245},{\"end\":46815,\"start\":46757},{\"end\":47505,\"start\":47420},{\"end\":47879,\"start\":47853},{\"end\":48223,\"start\":48112},{\"end\":48696,\"start\":48601},{\"end\":49065,\"start\":48994},{\"end\":49522,\"start\":49448},{\"end\":49973,\"start\":49915},{\"end\":50327,\"start\":50263},{\"end\":50693,\"start\":50626},{\"end\":51139,\"start\":51117},{\"end\":51459,\"start\":51429},{\"end\":51971,\"start\":51918},{\"end\":52644,\"start\":52571},{\"end\":52958,\"start\":52916},{\"end\":53359,\"start\":53270},{\"end\":53720,\"start\":53660},{\"end\":54080,\"start\":54043},{\"end\":54337,\"start\":54285},{\"end\":54705,\"start\":54628},{\"end\":55075,\"start\":55053},{\"end\":55383,\"start\":55326},{\"end\":55827,\"start\":55758},{\"end\":56162,\"start\":56096}]", "bib_author": "[{\"end\":33433,\"start\":33416},{\"end\":33447,\"start\":33433},{\"end\":33461,\"start\":33447},{\"end\":33789,\"start\":33772},{\"end\":33808,\"start\":33789},{\"end\":33822,\"start\":33808},{\"end\":33836,\"start\":33822},{\"end\":34149,\"start\":34138},{\"end\":34165,\"start\":34149},{\"end\":34492,\"start\":34473},{\"end\":34507,\"start\":34492},{\"end\":34521,\"start\":34507},{\"end\":34532,\"start\":34521},{\"end\":34545,\"start\":34532},{\"end\":34551,\"start\":34545},{\"end\":34904,\"start\":34890},{\"end\":34918,\"start\":34904},{\"end\":35196,\"start\":35181},{\"end\":35217,\"start\":35196},{\"end\":35231,\"start\":35217},{\"end\":35526,\"start\":35511},{\"end\":35541,\"start\":35526},{\"end\":35885,\"start\":35877},{\"end\":35894,\"start\":35885},{\"end\":35902,\"start\":35894},{\"end\":35909,\"start\":35902},{\"end\":35917,\"start\":35909},{\"end\":35924,\"start\":35917},{\"end\":35932,\"start\":35924},{\"end\":36285,\"start\":36275},{\"end\":36300,\"start\":36285},{\"end\":36308,\"start\":36300},{\"end\":36318,\"start\":36308},{\"end\":36336,\"start\":36318},{\"end\":36346,\"start\":36336},{\"end\":36668,\"start\":36658},{\"end\":36678,\"start\":36668},{\"end\":36690,\"start\":36678},{\"end\":36939,\"start\":36929},{\"end\":36951,\"start\":36939},{\"end\":36962,\"start\":36951},{\"end\":36971,\"start\":36962},{\"end\":37356,\"start\":37338},{\"end\":37374,\"start\":37356},{\"end\":37394,\"start\":37374},{\"end\":37740,\"start\":37722},{\"end\":37758,\"start\":37740},{\"end\":37778,\"start\":37758},{\"end\":38095,\"start\":38085},{\"end\":38105,\"start\":38095},{\"end\":38121,\"start\":38105},{\"end\":38132,\"start\":38121},{\"end\":38140,\"start\":38132},{\"end\":38152,\"start\":38140},{\"end\":38428,\"start\":38417},{\"end\":38443,\"start\":38428},{\"end\":38455,\"start\":38443},{\"end\":38468,\"start\":38455},{\"end\":38913,\"start\":38894},{\"end\":38929,\"start\":38913},{\"end\":38947,\"start\":38929},{\"end\":38972,\"start\":38947},{\"end\":38991,\"start\":38972},{\"end\":39000,\"start\":38991},{\"end\":39380,\"start\":39368},{\"end\":39395,\"start\":39380},{\"end\":39409,\"start\":39395},{\"end\":39419,\"start\":39409},{\"end\":39724,\"start\":39711},{\"end\":39733,\"start\":39724},{\"end\":39742,\"start\":39733},{\"end\":39752,\"start\":39742},{\"end\":39994,\"start\":39986},{\"end\":40003,\"start\":39994},{\"end\":40013,\"start\":40003},{\"end\":40344,\"start\":40336},{\"end\":40357,\"start\":40344},{\"end\":40599,\"start\":40588},{\"end\":40611,\"start\":40599},{\"end\":40621,\"start\":40611},{\"end\":40641,\"start\":40621},{\"end\":40657,\"start\":40641},{\"end\":40957,\"start\":40942},{\"end\":40973,\"start\":40957},{\"end\":40991,\"start\":40973},{\"end\":41010,\"start\":40991},{\"end\":41467,\"start\":41455},{\"end\":41481,\"start\":41467},{\"end\":41500,\"start\":41481},{\"end\":41510,\"start\":41500},{\"end\":41528,\"start\":41510},{\"end\":41543,\"start\":41528},{\"end\":42082,\"start\":42066},{\"end\":42095,\"start\":42082},{\"end\":42106,\"start\":42095},{\"end\":42121,\"start\":42106},{\"end\":42434,\"start\":42419},{\"end\":42451,\"start\":42434},{\"end\":42465,\"start\":42451},{\"end\":42848,\"start\":42834},{\"end\":42861,\"start\":42848},{\"end\":42874,\"start\":42861},{\"end\":42892,\"start\":42874},{\"end\":42908,\"start\":42892},{\"end\":43315,\"start\":43297},{\"end\":43328,\"start\":43315},{\"end\":43620,\"start\":43603},{\"end\":43637,\"start\":43620},{\"end\":43646,\"start\":43637},{\"end\":43943,\"start\":43937},{\"end\":43950,\"start\":43943},{\"end\":43958,\"start\":43950},{\"end\":44288,\"start\":44277},{\"end\":44296,\"start\":44288},{\"end\":44305,\"start\":44296},{\"end\":44320,\"start\":44305},{\"end\":44337,\"start\":44320},{\"end\":44350,\"start\":44337},{\"end\":44790,\"start\":44763},{\"end\":44815,\"start\":44790},{\"end\":44842,\"start\":44815},{\"end\":44866,\"start\":44842},{\"end\":44874,\"start\":44866},{\"end\":45252,\"start\":45237},{\"end\":45271,\"start\":45252},{\"end\":45292,\"start\":45271},{\"end\":45616,\"start\":45601},{\"end\":45636,\"start\":45616},{\"end\":45657,\"start\":45636},{\"end\":46002,\"start\":45988},{\"end\":46015,\"start\":46002},{\"end\":46398,\"start\":46378},{\"end\":46411,\"start\":46398},{\"end\":46425,\"start\":46411},{\"end\":46440,\"start\":46425},{\"end\":46832,\"start\":46817},{\"end\":46847,\"start\":46832},{\"end\":46861,\"start\":46847},{\"end\":46880,\"start\":46861},{\"end\":46893,\"start\":46880},{\"end\":46911,\"start\":46893},{\"end\":46930,\"start\":46911},{\"end\":46949,\"start\":46930},{\"end\":46965,\"start\":46949},{\"end\":46981,\"start\":46965},{\"end\":47520,\"start\":47507},{\"end\":47534,\"start\":47520},{\"end\":47547,\"start\":47534},{\"end\":47559,\"start\":47547},{\"end\":47569,\"start\":47559},{\"end\":47585,\"start\":47569},{\"end\":47894,\"start\":47881},{\"end\":47910,\"start\":47894},{\"end\":47921,\"start\":47910},{\"end\":47931,\"start\":47921},{\"end\":47944,\"start\":47931},{\"end\":48238,\"start\":48225},{\"end\":48254,\"start\":48238},{\"end\":48266,\"start\":48254},{\"end\":48280,\"start\":48266},{\"end\":48291,\"start\":48280},{\"end\":48305,\"start\":48291},{\"end\":48711,\"start\":48698},{\"end\":48727,\"start\":48711},{\"end\":48740,\"start\":48727},{\"end\":48752,\"start\":48740},{\"end\":48765,\"start\":48752},{\"end\":49080,\"start\":49067},{\"end\":49091,\"start\":49080},{\"end\":49106,\"start\":49091},{\"end\":49122,\"start\":49106},{\"end\":49536,\"start\":49524},{\"end\":49552,\"start\":49536},{\"end\":49565,\"start\":49552},{\"end\":49992,\"start\":49975},{\"end\":50008,\"start\":49992},{\"end\":50027,\"start\":50008},{\"end\":50038,\"start\":50027},{\"end\":50342,\"start\":50329},{\"end\":50354,\"start\":50342},{\"end\":50366,\"start\":50354},{\"end\":50377,\"start\":50366},{\"end\":50389,\"start\":50377},{\"end\":50393,\"start\":50389},{\"end\":50711,\"start\":50695},{\"end\":50727,\"start\":50711},{\"end\":50740,\"start\":50727},{\"end\":50754,\"start\":50740},{\"end\":50766,\"start\":50754},{\"end\":50780,\"start\":50766},{\"end\":50798,\"start\":50780},{\"end\":51159,\"start\":51141},{\"end\":51176,\"start\":51159},{\"end\":51480,\"start\":51461},{\"end\":51489,\"start\":51480},{\"end\":51503,\"start\":51489},{\"end\":51520,\"start\":51503},{\"end\":51532,\"start\":51520},{\"end\":51551,\"start\":51532},{\"end\":51566,\"start\":51551},{\"end\":51585,\"start\":51566},{\"end\":51604,\"start\":51585},{\"end\":51983,\"start\":51973},{\"end\":51999,\"start\":51983},{\"end\":52014,\"start\":51999},{\"end\":52029,\"start\":52014},{\"end\":52048,\"start\":52029},{\"end\":52398,\"start\":52384},{\"end\":52410,\"start\":52398},{\"end\":52420,\"start\":52410},{\"end\":52654,\"start\":52646},{\"end\":52670,\"start\":52654},{\"end\":52690,\"start\":52670},{\"end\":52704,\"start\":52690},{\"end\":52971,\"start\":52960},{\"end\":52985,\"start\":52971},{\"end\":53001,\"start\":52985},{\"end\":53014,\"start\":53001},{\"end\":53028,\"start\":53014},{\"end\":53041,\"start\":53028},{\"end\":53374,\"start\":53361},{\"end\":53392,\"start\":53374},{\"end\":53735,\"start\":53722},{\"end\":53745,\"start\":53735},{\"end\":53761,\"start\":53745},{\"end\":53779,\"start\":53761},{\"end\":54090,\"start\":54082},{\"end\":54101,\"start\":54090},{\"end\":54348,\"start\":54339},{\"end\":54360,\"start\":54348},{\"end\":54371,\"start\":54360},{\"end\":54384,\"start\":54371},{\"end\":54393,\"start\":54384},{\"end\":54405,\"start\":54393},{\"end\":54716,\"start\":54707},{\"end\":54727,\"start\":54716},{\"end\":54737,\"start\":54727},{\"end\":54752,\"start\":54737},{\"end\":54768,\"start\":54752},{\"end\":55083,\"start\":55077},{\"end\":55091,\"start\":55083},{\"end\":55104,\"start\":55091},{\"end\":55115,\"start\":55104},{\"end\":55397,\"start\":55385},{\"end\":55414,\"start\":55397},{\"end\":55429,\"start\":55414},{\"end\":55441,\"start\":55429},{\"end\":55459,\"start\":55441},{\"end\":55840,\"start\":55829},{\"end\":55854,\"start\":55840},{\"end\":56177,\"start\":56164},{\"end\":56194,\"start\":56177},{\"end\":56211,\"start\":56194},{\"end\":56220,\"start\":56211}]", "bib_venue": "[{\"end\":35622,\"start\":35590},{\"end\":41704,\"start\":41632},{\"end\":42574,\"start\":42528},{\"end\":47102,\"start\":47050},{\"end\":49231,\"start\":49185},{\"end\":49700,\"start\":49641},{\"end\":51267,\"start\":51230},{\"end\":33518,\"start\":33461},{\"end\":33867,\"start\":33836},{\"end\":34218,\"start\":34165},{\"end\":34608,\"start\":34551},{\"end\":34965,\"start\":34918},{\"end\":35269,\"start\":35231},{\"end\":35588,\"start\":35541},{\"end\":35990,\"start\":35932},{\"end\":36395,\"start\":36346},{\"end\":36656,\"start\":36584},{\"end\":37043,\"start\":36971},{\"end\":37451,\"start\":37394},{\"end\":37827,\"start\":37778},{\"end\":38210,\"start\":38152},{\"end\":38562,\"start\":38468},{\"end\":38765,\"start\":38713},{\"end\":39064,\"start\":39000},{\"end\":39477,\"start\":39419},{\"end\":39783,\"start\":39752},{\"end\":40071,\"start\":40013},{\"end\":40334,\"start\":40218},{\"end\":40715,\"start\":40657},{\"end\":41122,\"start\":41010},{\"end\":41630,\"start\":41543},{\"end\":42158,\"start\":42121},{\"end\":42526,\"start\":42465},{\"end\":42976,\"start\":42908},{\"end\":43366,\"start\":43328},{\"end\":43704,\"start\":43646},{\"end\":44016,\"start\":43958},{\"end\":44411,\"start\":44350},{\"end\":44905,\"start\":44874},{\"end\":45349,\"start\":45292},{\"end\":45696,\"start\":45657},{\"end\":46062,\"start\":46015},{\"end\":46473,\"start\":46440},{\"end\":47048,\"start\":46981},{\"end\":47616,\"start\":47585},{\"end\":47965,\"start\":47944},{\"end\":48336,\"start\":48305},{\"end\":48784,\"start\":48765},{\"end\":49183,\"start\":49122},{\"end\":49639,\"start\":49565},{\"end\":50068,\"start\":50038},{\"end\":50426,\"start\":50393},{\"end\":50856,\"start\":50798},{\"end\":51228,\"start\":51176},{\"end\":51662,\"start\":51604},{\"end\":52110,\"start\":52048},{\"end\":52382,\"start\":52327},{\"end\":52729,\"start\":52704},{\"end\":53072,\"start\":53041},{\"end\":53454,\"start\":53392},{\"end\":53837,\"start\":53779},{\"end\":54149,\"start\":54101},{\"end\":54443,\"start\":54405},{\"end\":54826,\"start\":54768},{\"end\":55173,\"start\":55115},{\"end\":55521,\"start\":55459},{\"end\":55912,\"start\":55854},{\"end\":56278,\"start\":56220}]"}}}, "year": 2023, "month": 12, "day": 17}