{"id": 102484000, "updated": "2023-10-03 17:30:02.897", "metadata": {"title": "Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning", "authors": "[{\"first\":\"Shichen\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Tianye\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Weikai\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Li\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 4, "day": 3}, "abstract": "Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.01786", "mag": "2990173985", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/Liu0LL19", "doi": "10.1109/iccv.2019.00780"}}, "content": {"source": {"pdf_hash": "8b751405526c28245eea5e925a6ede034c287bdb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.01786v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.01786", "status": "GREEN"}}, "grobid": {"id": "0eae795997343e98725dbfe1a65ac0e79cdb8afe", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8b751405526c28245eea5e925a6ede034c287bdb.txt", "contents": "\nSoft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning\n\n\nShichen Liu lshichen@ict.usc.edu \nUSC Institute for Creative Technologies\n\n\nUniversity of Southern\nCalifornia 3 Pinscreen\n\nTianye Li \nUSC Institute for Creative Technologies\n\n\nWeikai Chen wechen@ict.usc.edu \nUSC Institute for Creative Technologies\n\n\nUniversity of Southern\nCalifornia 3 Pinscreen\n\nHao Li \nUSC Institute for Creative Technologies\n\n\nUniversity of Southern\nCalifornia 3 Pinscreen\n\nSoft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning\n\nRendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers[29,19], which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images.The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-thearts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised singleview reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers. Code is available at https://github.com/ ShichenLiu/SoftRas.\n\nIntroduction\n\nUnderstanding and reconstructing 3D scenes and structures from 2D images has been one of the fundamental goals in computer vision. The key to image-based 3D reasoning is to find sufficient supervisions flowing from the pixels to the 3D properties. To obtain image-to-3D correlations, prior approaches mainly rely on the matching losses based on 2D  Figure 1: We propose Soft Rasterizer R (upper), a truly differentiable renderer, which formulates rendering as a differentiable aggregating process A(\u00b7) that fuses per-triangle contributions {D i } in a \"soft\" probabilistic manner. Our approach attacks the core problem of differentiating the standard rasterizer, which cannot flow gradients from pixels to geometry due to the discrete sampling operation (below).\n\nkey points/contours [3,35,26,32] or shape/appearance priors [1,28,6,23,48]. However, the above approaches are either limited to task-specific domains or can only provide weak supervision due to the sparsity of the 2D features. In contrast, as the process of producing 2D images from 3D assets, rendering relates each pixel with the 3D parameters by simulating the physical mechanism of image formulation. Hence, by inverting a renderer, one can obtain dense pixellevel supervision for general-purpose 3D reasoning tasks, which cannot be achieved by conventional approaches. However, the rendering process is not differentiable in conventional graphics pipelines. In particular, standard mesh renderer involves a discrete sampling operation, called rasterization, which prevents the gradient to be flowed into the mesh vertices. Since the forward rendering \nr x B H h t u f H E l Q o f 2 / O W b s + w = \" > A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M\ns t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e o Y N B e 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w 8 E l Y 7 p < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" I S i r q r\nx B H h t u f H E l Q o f 2 / O W b s + w = \" > A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M\ns t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e o Y N B e 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w 8 E l Y 7 p < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" I S i r q r\nx B H h t u f H E l Q o f 2 / O W b s + w = \" > A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M\ns t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e o Y N B e 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w 8 E l Y 7 p < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" I S i r q r\nx B H h t u f H E l Q o f 2 / O W b s + w = \" > A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M\ns t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e o Y N B e 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w 8 E l Y 7 p < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" E b d N M m z A a U 4 k 5 H Q t i o 0 y z S 1 a v 5 c = \" > A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e k M s B O 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P function is highly non-linear and complex, to achieve differentiable rendering, recent advances [29,19] only approximate the backward gradient with hand-crafted functions while directly employing a standard graphics renderer in the forward pass. While promising results have been shown in the task of image-based 3D reconstruction, the inconsistency between the forward and backward propagations may lead to uncontrolled optimization behaviors and limited generalization capability to other 3D reasoning tasks. We show in Section 5.2 that such mechanism would cause problematic situations in image-based shape fitting where the 3D parameters cannot be efficiently optimized.\n\nIn this paper, instead of studying a better form of rendering gradient, we attack the key problem of differentiating the forward rendering function. Specifically, we propose a truly differentiable rendering framework that is able to render a colorized mesh in the forward pass ( Figure 1). In addition, our framework can consider a variety of 3D properties, including mesh geometry, vertex attributes (color, normal etc.), camera parameters and illuminations and is able to flow efficient gradients from pixels to mesh vertices and their attributes. While being a universal module, our renderer can be plugged into either a neural network or a nonlearning optimization framework without parameter tuning.\n\nThe key to our approach is the novel formulation that views rendering as a \"soft\" probabilistic process. Unlike the standard rasterizer, which only selects the color of the closest triangle in the viewing direction (Figure 1 below), we propose that all triangles have probabilistic contributions to each rendered pixel, which can be modeled as probability maps on the screen space. While conventional rendering pipelines merge shaded fragments in a one-hot manner, we propose a differentiable aggregation function that fuses the per-triangle color maps based on the probability maps and the triangles' relative depths to obtain the final rendering result (Figure 1 upper). The novel aggregating mechanism enables our renderer to flow gradients to all mesh triangles, including the occluded ones. In addition, our framework can propagate supervision signals from pixels to far-range triangles because of its probabilistic formulation. We call our framework Soft Rasterizer (SoftRas) as it \"softens\" the discrete rasterization to enable differentiability.\n\nThanks to the consistent forward and backward propagations, SoftRas is able to provide high-quality gradient flows that supervise a variety of tasks on image-based 3D reasoning. To evaluate the performance of SoftRas, we show applications in 3D unsupervised single-view mesh reconstruction and image-based shape fitting ( Figure 2, Section 5.1 and 5.2). In particular, as SoftRas provides strong error signals to the mesh generator simply based on the rendering loss, one can achieve mesh reconstruction from a single image without any 3D supervision. To faithfully texture the mesh, we further propose a novel approach that extracts representative colors from input image and formulates the color regression as a classification problem. Regarding the task of image-based shape fitting, we show that our approach is able to (1) handle occlusions using the aggregating mechanism that considers the probabilistic contributions of all triangles; and (2) provide much smoother energy landscape, compared to other differentiable renderers, that avoids local minima by using the smooth rendering (Figure 2 left). Experimental results demonstrate that our approach significantly outperforms the state-of-the-arts both quantitatively and qualitatively.\n\n\nRelated Work\n\nDifferentiable Rendering. To relate the changes in the observed image with that in the 3D shape manipulation, a number of existing techniques have utilized the derivatives of rendering [11,10,30]. Recently, Loper and Black [29] introduce an approximate differentiable renderer which generates derivatives from projected pixels to the 3D parameters. Kato et al. [19] propose to approximate the backward gradient of rasterization with a hand-crafted function to achieve differentiable rendering. More recently, Li et al. [24] introduce a differentiable ray tracer to realize the differentiability of secondary rendering effects. Recent advances in 3D face reconstruction [38,40,39,41,9], material inference [27,7] and other 3D reconstruction tasks [49,37,33,14,22,34] have leveraged some other forms of differentiable rendering layers to obtain gradient flows in the neural networks. However, these rendering layers are usually designed for special purpose and thus cannot be generalized to other applications. In this paper, we  focus on a general-purpose differentiable rendering framework that is able to directly render a given mesh using differentiable functions instead of only approximating the backward derivatives.\n\nImage-based 3D Reasoning. 2D images are widely used as the media for reasoning 3D properties. In particular, image-based reconstruction has received the most attentions. Conventional approaches mainly leverage the stereo correspondence based on the multi-view geometry [13,8] but is restricted to the coverage provided by the multiple views. With the availability of large-scale 3D shape dataset [5], learning-based approaches [43,12,15] are able to consider single or few images thanks to the shape prior learned from the data. To simplify the learning problem, recent works reconstruct 3D shape via predicting intermediate 2.5D representations, such as depth map [25], image collections [18], displacement map [16] or normal map [36,44]. Pose estimation is another key task to understanding the visual environment. For 3D rigid pose estimation, while early approaches attempt to cast it as classification problem [42], recent approaches [20,46] can directly regress the 6D pose by using deep neural networks. Estimating the pose of non-rigid objects, e.g. human face or body, is more challenging. By detecting the 2D key points, great progress has been made to estimate the 2D poses [31,4,45].\n\nTo obtain 3D pose, shape priors [1,28] have been incorporated to minimize the shape fitting errors in recent approaches [3,4,17,2]. Our proposed differentiable renderer can provide dense rendering supervision to 3D properties, benefitting a variety of image-based 3D reasoning tasks.\n\n\nSoft Rasterizer\n\n\nDifferentiable Rendering Pipeline\n\nAs shown in Figure 3, we consider both extrinsic variables (camera P and lighting conditions L) that define the environmental settings, and intrinsic properties (triangle meshes M and per-vertex appearance A, including color, material etc.) that describe the model-specific properties. Following the standard rendering pipeline, one can obtain the mesh normal N, image-space coordinate U and viewdependent depths Z by transforming input geometry M based on camera P. With specific assumptions of illumination and material models (e.g. Phong model), we can compute color C given {A, N, L}. These two modules are naturally differentiable. However, the subsequent operations including the rasterization and z-buffering in the standard graphics pipeline (Figure 3 red blocks) are not differentiable with respect to U and Z due to the discrete sampling operations.\n\nOur differentiable formulation. We take a different perspective that the rasterization can be viewed as binary masking that is determined by the relative positions between the pixels and triangles, while z-buffering merges the rasterization results F in a pixel-wise one-hot manner based on the relative depths of triangles. The problem is then formulated as modeling the discrete binary masks and the one-hot merging operation in a soft and differentiable manner. To achieve this, we propose two major components, namely probability maps {D j } that model the probability of each pixel staying inside a specific triangle f j and aggregate function A(\u00b7) that fuses per-triangle color maps based on {D j } and the relative depths among triangles.  \n\n\nProbability Map Computation\n\nWe model the influence of triangle f j on image plane by probability map D j . To estimate the probability of D j at pixel p i , the function is required to take into account both the relative position and the distance between p i and D j . To this end, we define D j at pixel p i as follows:\nD i j = sigmoid(\u03b4 i j \u00b7 d 2 (i, j) \u03c3 ),(1)\nwhere \u03c3 is a positive scalar that controls the sharpness of the probability distribution while \u03b4 i j is a sign indicator\n\u03b4 i j = {+1, if p i \u2208 f j ; \u22121, otherwise}. We set \u03c3 as 1\u00d710 \u22124 unless otherwise specified. d(i, j) is the closest distance from p i to f j 's edges. A natural choice for d(i, j) is the Eu- clidean distance.\nHowever, other metrics, such as barycentric or l 1 distance, can be used in our approach.\n\nIntuitively, by using the sigmoid function, Equation 1 normalizes the output to (0, 1), which is a faithful continuous approximation of binary mask with boundary landed on 0.5. In addition, the sign indicator maps pixels inside and outside f j to the range of (0.5, 1) and (0, 0.5) respectively. Figure 4 shows D j of a triangle with varying \u03c3 using Euclidean distance. Smaller \u03c3 leads to sharper probability distribution while larger \u03c3 tends to blur the outcome. This design allows controllable influence for triangles on image plane. As \u03c3 \u2192 0, the resulting probability map converges to the exact shape of the triangle, enabling our probability map computation to be a generalized form of traditional rasterization.\n\n\nAggregate Function\n\nFor each mesh triangle f j , we define its color map C j at pixel p i on the image plane by interpolating vertex color using barycentric coordinates. We clip and normalize the barycentric coordinates to [0, 1] for p i outside of f j . We then propose to use an aggregate function A(\u00b7) to merge color maps {C j } to obtain rendering output I based on {D j } and the relative depths {z j }. Inspired by the softmax operator, we define an aggregate function A S as follows:\nI i = A S ({C j }) = j w i j C i j + w i b C b ,(2)\nwhere C b is the background color; the weights {w j } satisfy j w i j + w i b = 1 and are defined as:\nw i j = D i j exp(z i j /\u03b3) k D i k exp(z i k /\u03b3) + exp( /\u03b3) ,(3)\nwhere z i j denotes the normalized inverse depth of the 3D point on f i whose 2D projection is p i ; is small constant that enables the background color while \u03b3 (set as 1 \u00d7 10 \u22124 unless otherwise specified) controls the sharpness of the aggregate function. Note that w j is a function of two major  Figure 5: Comparisons with prior differentiable renderers in terms of gradient flow.\n\nvariables: D j and z j . Specifically, w j assigns higher weight to closer triangles that have larger z j . As \u03b3 \u2192 0, the color aggregation function only outputs the color of nearest triangle, which exactly matches the behavior of z-buffering. In addition, w j is robust to z-axis translations. D j modulates the w j along the x, y directions such that the triangles closer to p i on screen space will receive higher weight. Equation 2 also works for shading images when the intrinsic vertex colors are set to constant ones. We further explore the aggregate function for silhouettes. Note that the silhouette of object is independent from its color and depth map. Hence, we propose a dedicated aggregation function A O for the silhouette based on the binary occupancy:\nI i s = A O ({D j }) = 1 \u2212 j (1 \u2212 D i j ).(4)\nIntuitively, Equation 4 models silhouette as the probability of having at least one triangle cover the pixel p i . Note that there might exist other forms of aggregate functions. One alternative option may be using a universal aggregate function A N that is implemented as a neural network. We provide an ablation study on this regard in Section 5.1.4.\n\n\nComparisons with Prior Works\n\nIn this section, we compare our approach with the state-of-the-art rasterization-based differential renderers: OpenDR [29] and NMR [19], in terms of gradient flows as shown in Figure 5. We provide detailed analysis on gradient computation in Appendix A.\n\nGradient from pixels to triangles. Since both OpenDR and NMR utilize standard graphics renderer in the forward pass, they have no control over the intermediate rendering process and thus cannot flow gradient into the triangles that are occluded in the final rendered image (Figure 5 our approach has full control on the internal variables and is able to flow gradients to invisible triangles and the z coordinates of all triangles through the aggregation function ( Figure 5(a) right).\n\nScreen-space gradient from pixels to vertices. Thanks to our continuous probabilistic formulation, in our approach, the gradient from pixel p j in screen space can flow gradient to all distant vertices ( Figure 5(b) right). However, for OpenDR, a vertex can only receive gradients from neighboring pixels within a close distance due to the local filtering operation ( Figure 5(b) left). Regarding NMR, there is no gradient defined from the pixels inside the white regions with respect to the triangle vertices (( Figure 5(b) middle). In contrast, our approach does not have such issue thanks to our orientation-invariant formulation.\n\n\nImage-based 3D Reasoning\n\nWith direct gradient flow from image to 3D properties, our differentiable rendering framework enables a variety of tasks on 3D reasoning.\n\n\nSingle-view Mesh Reconstruction\n\nTo demonstrate the effectiveness of soft rasterizer, we fix the extrinsic variables and evaluate its performance on single-view 3D reconstruction by incorporating it with a mesh generator. The direct gradient from image pixels to shape and color generators enables us to achieve 3D unsupervised mesh reconstruction. Our framework is demonstrated in Figure 6. Given an input image, our shape and color generators generate a triangle mesh M and its corresponding colors C, which are then fed into the soft rasterizer. The SoftRas layer renders both the silhouette I s and color image I c and provide rendering-based error signal by comparing with the ground truths. Inspired by the latest advances in mesh learning [19,43], we leverage a similar idea of synthesizing 3D model by deforming a template mesh.\n\nTo validate the performance of soft rasterizer, the shape generator employ an encoder-decoder architecture identical to that of [19,47]. The details of the shape and generators are described in Appendix C.  Losses. The reconstruction networks are supervised by three losses: silhouette loss L s , color loss L c and geometry loss L g . Let\u00ce s and I s denote the predicted and the ground-truth silhouette respectively. The silhouette loss is defined as L s = 1\u2212 ||\u00ces\u2297Is||1 ||\u00ces\u2295Is\u2212\u00ces\u2297Is||1 , where \u2297 and \u2295 are the element-wise product and sum operators respectively. The color loss is measured as the l 1 norm between the rendered and input image: L c = ||\u00ce c \u2212 I c || 1 . To achieve appealing visual quality, we further impose a geometry loss L g that regularizes the Laplacian of both shape and color predictions. The final loss is a weighted sum of the three losses:\nL = L s + \u03bbL c + \u00b5L g .(5)\n\nColor Reconstruction\n\nInstead of directly regressing the color value, our color generator formulates color reconstruction as a classification problem that learns to reuse the pixel colors in the input image for each sampling point. Let N c denote the number of sampling points on M and H, W be the height and width of the input image respectively. However, the computational cost of a naive color selection approach is prohibitive, i.e. O(HW N c ). To address this challenge, we propose a novel approach to colorize mesh using a color palette, as shown in Figure 7. Specifically, after passing input image to a neural network, the extracted features are fed into (1) a sampling network that samples the representative colors for building the palette; and (2) a selection network that combines colors from the palette for texturing the sampling points. The color prediction is obtained by multiplying the color selections with the learned color palette. Our approach reduces the computation complexity to O(N d (HW + N c )), where N p is the size of color palette. With a proper setting of N p , one can significantly reduce the computational cost while achieving sharp and accurate color recovery.\n\n\nImage-based Shape Fitting\n\nImage-based shape fitting has a fundamental impact in various tasks, such as pose estimation, shape alignment, model-based reconstruction, etc. Yet without direct correlation between image and 3D parameters, conventional approaches have to rely on coarse correspondences, e.g. 2D joints [3] or feature points [35], to obtain supervision signals for optimization. In contrast, SoftRas can directly Input SoftRas (3D unsupervised) NMR (3D unsupervised) Pixel2Mesh (supervised) Ground truth Figure 8: 3D mesh reconstruction from a single image. From left to right, we show input image, ground truth, the results of our method (SoftRas), Neural Mesh Renderer [19] and Pixel2mesh [43] -all visualized from 2 different views. Along with the results, we also visualize mesh-to-scan distances measured from reconstructed mesh to ground truth. back-propagate pixel-level errors to 3D properties, enabling dense image-to-3D correspondence for high-quality shape fitting. However, a differentiable renderer has to resolve two challenges in order to be readily applicable. (1) occlusion awareness: the occluded portion of 3D model should be able to receive gradients in order to handle large pose changes. (2) far-range impact: the loss at a pixel should have influence on distant mesh vertices, which is critical to dealing with local minima during optimization. While prior differentiable renderers [19,29] fail to satisfy these two criteria, our approach handles these challenges simultaneously. (1) Our aggregate function fuses the probability maps from all triangles, enabling the gradients to be flowed to all vertices including the occluded ones. (2) Our soft approximation based on probability distribution allows the gradient to be propagated to the far end while the size of receptive field can be well controlled (Figure 4). To this end, our approach can faithfully solve the image-based shape fitting problem by minimizing the following energy objective:\nargmin \u03c1,\u03b8,t ||R(M (\u03c1, \u03b8, t)) \u2212 I t || 2 ,(6)\nwhere R(\u00b7) is the rendering function that generates a rendered image I from mesh M , which is parametrized by its pose \u03b8, translation t and non-rigid deformation parameters \u03c1. The difference between I and the target image I t provides strong supervision to solve the unknowns {\u03c1, \u03b8, t}.\n\n\nExperiments\n\nIn this section, we perform extensive evaluations on our framework. We also include more visual evaluations in the appendix.\n\n\nSingle-view Mesh Reconstruction\n\n\nExperimental Setup\n\nDatasets and Evaluation Metrics. We use the dataset provided by [19], which contains 13 categories of objects from ShapeNet [5]. Each object is rendered in 24 different views with image resolution of 64 \u00d7 64. For fair comparison, we employ the same train/validate/test split on the same dataset as in [19,47]. For quantitative evaluation, we adopt the standard reconstruction metric, 3D intersection over union (IoU), to compare with baseline methods. Implementation Details. We use the same structure as [19,47] for mesh generation. Our network is optimized using Adam [21] with \u03b1 = 1 \u00d7 10 \u22124 , \u03b2 1 = 0.9 and \u03b2 2 = 0.999. Specifically, we set \u03bb = 1 and \u00b5 = 1 \u00d7 10 \u22123 across all experiments unless otherwise specified. We train the network with multi-view images of batch size 64 and implement it using PyTorch.   \n\n\nQualitative Results\n\nSingle-view Mesh Reconstruction. We compare the qualitative results of our approach with that of the stateof-the-art supervised [43] and 3D unsupervised [19] mesh reconstruction approaches in Figure 8. Though NMR [19] is able to recover the rough shape, the mesh surface is discontinuous and suffers from a considerable amount of self intersections. In contrast, our method can faithfully reconstruct fine details of the object, such as the tail of the airplane and the barrel of the rifle, while ensuring smoothness of the surface. Though trained without 3D supervision, our approach achieves results on par with the supervised method Pixel2Mesh [43]. In some cases, our approach can generate even more appealing details than that of [43], e.g. the bench legs, the airplane engine and the side of the car. Mesh-toscan distance visualization also shows our results achieve much higher accuracy than [19] and comparable accuracy with that of [43].\n\nColor Reconstruction. Our method is able to faithfully recover the mesh color based on the input image. Figure 9 presents the colorized reconstruction from a single image and the learned color palettes. Though the resolution of the input image is rather low (64 \u00d7 64), our approach is still able to achieve sharp color recovery and accurately restore the fine details, e.g. the subtle color transition on the body of airplane and the shadow on the phone screen.\n\n\nQuantitative Evaluations\n\nWe show the comparisons on 3D IoU score with the stateof-the-art approaches in Table 1. We test our approach under two settings: one trained with silhouette loss only (sil.) and the other with both silhouette and shading supervisions (full). Our approach has significantly outperformed all the other unsupervised methods on all categories. In addition, the mean score of our best setting has surpassed the stateof-the-art NMR [19] by more than 4.5 points. As we use the identical mesh generator and same training settings with [19], it indicates that it is the proposed SoftRas renderer that leads to the superior performance.  \n\n\nAblation Study\n\nIn this section, we conduct controlled experiments to validate the importance of different components.\n\nLoss Terms and Alternative Functions. In Table 2, we investigate the impact of Laplacian regularizer and various forms of the distance function (Section 3.2) and the aggregate function. As the RGB color channel and the \u03b1 channel (silhouette) have different candidate aggregate functions, we separate their lists in Table 2  Rigid Pose Fitting. We compare our approach with NMR in the task of rigid pose fitting. In particular, given a colorized cube and a target image, the pose of the cube needs to be optimized so that its rendered result matches the target image. Despite the simple geometry, the discontinuity of face colors, the non-linearity of rotation and the large occlusions make it particularly difficult to optimize. As shown in Figure 10, NMR is stuck in a local minimum while our approach succeeds to obtain the correct pose. The key is that our method produces smooth and partially transparent 1 The expectation of uniform-sampled SO3 rotation angle is \u03c0/2 + 2/\u03c0  Figure 11: Results for optimizing human pose given single image target.\n\n\nImage-based Shape Fitting\n\nrenderings which \"soften\" the loss landscape. Such smoothness can be controlled by \u03c3 and \u03b3, which allows us to avoid the local minimum. Further, we evaluate the rotation estimation accuracy on synthetic data given 100 randomly sampled initializations and targets. We compare methods w/ and w/o scheduling schemes, and summarize mean relative angle error in Table 3. Without optimization scheduling, our method outperforms the baseline (random estimation) and NMR by 43.68\u00b0and 10.60\u00b0respectively, demonstrating the effectiveness of the gradient flows provided by our method. Scheduling is a commonly used technique for solving nonlinear optimization problems. For NMR, we solve with multi-resolution images in 5 levels; while for our method, we set schedule to decay \u03c3 and \u03b3 in 5 steps. While scheduling improves both methods, our approach still achieves better accuracy than NMR by 17.37\u00b0, indicating our consistent superiority regardless of using the scheduling strategy.\n\nNon-rigid Shape Fitting. In Figure 11, we show that Sof-tRas can provide stronger supervision for non-rigid shape fitting even in the presence of part occlusions. We optimize the human body parametrized by SMPL model [28]. As the right hand (textured as red) is completely occluded in the initial view, it is extremely challenging to fit the body pose to the target image. To obtain correct parameters, the optimization should be able to (1) consider the impact of the occluded part on the rendered image and (2) back-propagate the error signals to the occluded vertices. NMR [19] fails to move the hand to the right position due to its incapability to handle occlusions. In comparison, our approach can faithfully complete the task as our novel probabilistic formulation and aggregating mechanism can take all triangles into account while being able to optimize the z coordinates (depth) of the mesh vertices.\n\n\nConclusions\n\nIn this paper, we have presented a truly differentiable rendering framework (SoftRas) that is able to directly render a given mesh in a fully differentiable manner. Soft-Ras can consider both extrinsic and intrinsic variables in a unified rendering framework and generate efficient gradients flowing from pixels to mesh vertices and their attributes (color, normal, etc.). We achieve this goal by reformulating the conventional discrete operations including rasterization and z-buffering as differentiable probabilistic processes. Such novel formulation enables our renderer to provide more efficient supervision signals, flow gradients to unseen vertices and optimize the z coordinates of mesh triangles, leading to the significant improvements in the tasks of single-view mesh reconstruction and image-based shape fitting. As a general framework, it would be an interesting future avenue to investigate other possibilities of distance and aggregate functions that might lead to even superior performance. More blurry More transparent Figure A1: Different rendering effects achieved by our proposed SoftRas renderer. We show how a colorized cube can be rendered in various ways by tuning the parameters of SoftRas. In particular, by increasing \u03b3, SoftRas can render the object with more tranparency while more blurry renderings can be achieved via increasing \u03c3. As \u03b3 \u2192 0 and \u03c3 \u2192 0, one can achieve rendering effect closer to standard rendering.\n\nC b and w i b denote the color and weight of background respectively where\nw i b = exp ( /\u03b3) k D i k exp z i k /\u03b3 + exp ( /\u03b3) ; (A10)\nz i j is the clipped normalized depth. Note that we normalize the depth so that the closer triangle receives a larger z i j by\nz i j = Z f ar \u2212 Z i j Z f ar \u2212 Z near ,(A11)\nwhere Z i j denotes the actual clipped depth of f j at p i , while Z near and Z f ar denote the far and near cut-off distances of the viewing frustum.\n\nSpecifically, the aggregate function A S (\u00b7) satisfies the following three properties: (1) as \u03b3 \u2192 0 and \u03c3 \u2192 0, w i converges to an one-hot vector where only the closest triangle contains the projection of p i is one, which shows the consistency between A S (\u00b7) and z-buffering; (2) w i b is close to one only when there is no triangle that covers p i ; (3) {w i j } is robust to z-axis translation. In addition, \u03b3 is a positive scalar that could balance out the scale change on z-axis.\n\nThe gradient \u2202I \u2202D i j and \u2202I \u2202z i j can be obtained as follows:\n\u2202I i \u2202D i j = k \u2202I i \u2202w i k \u2202w i k \u2202D i j + \u2202I i \u2202w i b \u2202w i b \u2202D i j = k =j \u2212C i k w i j w i k D i j + C i j ( w i j D i j \u2212 w i j w i j D i j ) \u2212 C i b w i j w i b D i j = w i j D i j (C i j \u2212 I i ) (A12) \u2202I i \u2202z i j = k \u2202I i \u2202w i k \u2202w i k \u2202z i j + \u2202I i \u2202w i b \u2202w i b \u2202z i j = k =j \u2212C i k w i j w i k \u03b3 + C i j ( w i j \u03b3 \u2212 w i j w i j \u03b3 ) \u2212 C i b w i j w i b \u03b3 = w i j \u03b3 (C i j \u2212 I i ) (A13)\n\nA3.2 Occupancy Aggregate Function\n\nIndependent from color and illumination, the silhouette of the object can be simply described by an occupancy aggregate function A O (\u00b7) as follows:\nI i sil = A O ({D i j }) = 1 \u2212 j (1 \u2212 D i j ).(A14)\nHence, the partial gradient lows:\n\u2202I i sil \u2202D i j = 1 \u2212 I i sil 1 \u2212 D i j . (A15)\n\nB. Forward Rendering Results\n\nAs demonstrated in Figure A1, our framework is able to directly render a given mesh, which cannot be achieved by any existing rasterization-based differentiable renderers [19,29]. In addition, compared to standard graphics renderer, SoftRas can achieve different rendering effects in a continuous manner thanks to its probabilistic formulation. Specifically, by increasing \u03c3, the key parameter that controls the sharpness of the screen-space probability distribution, we are able to generate more blurry rendering re-sults. Furthermore, with increased \u03b3, one can assign more weights to the triangles on the far end, naturally achieving more transparency in the rendered image. As discussed in Section 5.2 of the main paper, the blurring and transparent effects are the key for reshaping the energy landscape in order to avoid local minima.\n\n\nC. Network Structure\n\nWe provide detailed structures for all neural networks that were mentioned in the main paper. Figure C1 shows the structure of A N (Section 3.3 and 5.1.4), an alternative color aggregate function that is implemented as a neural network. In particular, input SoftRas features are first passed to four consecutive convolutional layers and then fed into a sigmoid  Figure C1: Network Architecture of A N , an alternative color aggregate function that is implemented as a neural networks.  layer to model non-linearity. We train A N with the output of a standard rendering pipeline as ground truth to achieve a parametric differentiable renderer.\n\nWe employ an encoder-decoder architecture for our single-view mesh reconstruction. The encoder is used as a feature extractor, whose network structure is shown in Figure C2. The detailed network structure of the color and shape generators are illustrated in Figure C3(a) and (b) respectively. Both networks ( Figure 6) share the same feature extractor. The shape generators consists of three fully connected layers and outputs a per-vertex displacement vector that deforms a template mesh into a target model. The color generator contains two fully connected streams: one for sampling the input image to build the color palette and the other one for selecting colors from the color palette to texture the sampling points.\n\n\nD. More Results on Image-based 3D Reasoning\n\nWe show more results on single-view mesh reconstruction and image-base shape fitting.  \n\n\nD1. Single-view Mesh Reconstruction\n\n\nD1.1 Intermediate Mesh Deformation\n\nIn Figure D1, we visualize the intermediate process of how an input mesh is deformed to a target shape after the supervision provided by SoftRas. As shown in the first row, the mesh generator gradually deforms a sphere template to a desired car shape which matches the input image. We then change the target image to an airplane ( Figure D1 second row). The network further deforms the generated car model to faithfully reconstruct the airplane. In both examples, the mesh deformation can quickly converge to a high-fidelity reconstruction within 200 iterations, demonstrating the effectiveness of our SoftRas renderer.\n\n\nD1.2 Single-view Reconstruction from Real Images\n\nWe further evaluate our approach on real images. As demonstrated in Figure D2, though only trained on synthetic data, our model generalizes well to real images and novel views with faithful reconstructions and fine-scale details, e.g. the tail fins of the fighter aircraft and thin structures in the rifle and table legs.\n\n\nD1.3 More Reconstruction Results from ShapeNet\n\nWe provide more reconstruction results in Figure A1. For each input image, we show its reconstructed geometry (middle) as well as the colored reconstruction (right). Figure D3: Intermediate process of fitting a color cube (second row) to a target pose shown in the input image (first row). The smoothened rendering (third row) that is used to escape local minimum, as well as the colorized fitting errors (fourth row), are also demonstrated.\n\n\nD2. Fitting Process for Rigid Pose Estimation\n\nWe demonstrate the intermediate process of how the proposed SoftRas renderer managed to fit the color cube to the target image in Figure D3. Since the cube is largely occluded, directly leveraging a standard rendering is likely to lead to local minima ( Figure 10) that causes non-trivial challenges for any gradient-based optimizer. By rendering the cube with stronger blurring at the earlier stage, our approach is able to avoid local minima, and gradually reduce the rendering loss until an accurate pose can be fitted.\n\n\nD3. Visualization of Non-rigid Body Fitting\n\nIn Figure D4, we compare the intermediate processes of NMR [19] and SoftRas during the task of fitting the SMPL model to the target pose. As the right hand of subject is completely occluded in the initial image, NMR fails to complete the task due to its incapability of flowing gradient to the occluded vertices. In contrast, our approach is able to obtain the correct pose within 320 iterations thanks to the occlusion-aware technique. Figure D4: Comparisons of body shape fitting using NMR [19] and our approach. Intermediate fitting processes of both methods are visualized.\n\nFigure 2 :\n2Forward rendering: various rendering effects generated by SoftRas (left). Different degrees of transparency and blurriness can be achieved by tuning \u03b3 and \u03c3 respectively. Applications based on the backward gradients provided by SoftRas: (1) 3D unsupervised mesh reconstruction from a single input image (middle) and (2) 3D pose fitting to the target image by flowing gradient to the occluded triangles (right).\n\nFigure 3 :\n3Comparisons between the standard rendering pipeline (upper branch) and our rendering framework (lower branch).\n\n\n) ground truth (b) \u03c3 = 0.003 (c) \u03c3 = 0.01 (d) \u03c3 = 0.03\n\nFigure 4 :\n4Probability maps of a triangle under Euclidean metric. (a) definition of pixel-to-triangle distance; (b)-(d) probability maps generated with different \u03c3.\n\nFigure 6 :\n6(a) left and middle). In addition, as their gradients only operate on the image plane, both OpenDR and NMR are not able to optimize the depth value z of the triangles. In contrast, The proposed framework for single-view mesh reconstruction.\n\nFigure 7 :\n7Network structure for color reconstruction.\n\nFigure 9 :\n9Results of colorized mesh reconstruction. The learned principal colors and their usage histogram are visualize on the right.\n\nFigure 10 :\n10Visualization of loss function landscapes of NMR and SoftRas for pose optimization given target image (a) and initialization (f). SoftRas achieves global minimum (b) with loss landscape (g). NMR is stuck in local minimum (c) with loss landscape (h). At this local minimum, Soft-Ras produces the smooth and partially transparent rendering (d)(e), which smoothens the loss landscape (i)(j) with larger \u03c3 and \u03b3, and consequently leads to better minimum.\n\nFigure A1 :\nA1More single-view reconstruction results. Left: input image; middle: reconstructed geometry; right: colorized reconstruction.\n\nFigure C2 :\nC2Network architecture of the feature extractor.\n\nFigure C3 :Figure D1 :\nC3D1Network Visualization of intermediate mesh deformation during training. First row: the network deforms the input sphereto a desired car model that corresponds to the target image. Second row: the generated car model is further deformed to reconstruct the airplane.\n\nFigure D2 :\nD2Single-view reconstruction results on real images.\n\nTable 1 :\n1Comparison of mean IoU with other 3D unsupervised reconstruction methods on 13 categories of ShapeNet datasets.Input \nReconstructed Results \nLearned Color Palettes \n\n\n\nTable 2 :\n2Ablation study of the regularizer and various forms of distance and aggregate functions. A N stands for the aggregation function implemented as a neural network. A S and A O refer to the aggregation functions defined in Equation 2 and 4 respectively.\nAcknowledgementsHao Li is affiliated with the University of Southern California, the USC Institute for Creative Technologies, and Pinscreen. This research was conducted at USC and was funded by in part by the ONR YIP grant N00014-17-S-FO14, the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA, the Andrew and Erna Viterbi Early Career Chair, the U.S. Army Research Laboratory (ARL) under contract number W911NF-14-D-0005, Adobe, and Sony. This project was not funded by Pinscreen, nor has it been conducted at Pinscreen or by anyone else affiliated with Pinscreen. The content of the information does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.AppendixA. Gradient ComputationIn this section, we provide more analysis on the variants of the probability representation (Section 3.2) and aggregate function (Section 3.3), in terms of the mathematical formulation and the resulting impact on the backward gradient.A1. OverviewAccording to the computation graph inFigure 3A2. Probability Map ComputationThe probability maps {D i j } based on the relative position between a given triangle f j and pixel p i are obtained via sigmoid function with temperature \u03c3 and distance metric D(i, j):where the metric D essentially satisfies: where D j converges to a binary mask as \u03c3 \u2192 0. We introduce two candidate metrics, namely signed Euclidean distance and barycentric metric. We represent p i using barycentric coordinate b i j \u2208 R 3 defined by f j :where U j =A2.1 Euclidean DistanceLet t i j \u2208 R 3 be the barycentric coordinate of the point on the edge of f j that is closest to p i . The signed Euclidean distance D E (i, j) from p i to the edges of f j can be computed as:where \u03b4 i j is a sign indicator defined asThen the partial gradient \u2202D E (i,j) \u2202Uj can be obtained via:A2.2 Barycentric MetricWe define the barycentric metric D B (i, j) as the minimum of barycentric coordinate:, then the gradient from D B (i, j) to U j can be obtained through:where k and l are the indices of U j 's element.A3. Aggregate functionA3.1 Softmax-based Aggregate FunctionAccording to A S (\u00b7), the output color is:where the weight {w j } is obtained based on the relative depth {z j } and the screen-space position of triangle f j and pixel p i as indicated in the following equation:\nA morphable model for the synthesis of 3d faces. V Blanz, T Vetter, Proceedings of the 26th annual conference on Computer graphics and interactive techniques. the 26th annual conference on Computer graphics and interactive techniquesACM Press/Addison-Wesley Publishing Co13V. Blanz and T. Vetter. A morphable model for the synthesis of 3d faces. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pages 187- 194. ACM Press/Addison-Wesley Publishing Co., 1999. 1, 3\n\nFace recognition based on fitting a 3d morphable model. V Blanz, T Vetter, IEEE Transactions. 259V. Blanz and T. Vetter. Face recognition based on fitting a 3d morphable model. IEEE Transactions on pattern analysis and machine intelligence, 25(9):1063-1074, 2003. 3\n\nKeep it smpl: Automatic estimation of 3d human pose and shape from a single image. F Bogo, A Kanazawa, C Lassner, P Gehler, J Romero, M J Black, European Conference on Computer Vision. Springer15F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and M. J. Black. Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. In European Conference on Computer Vision, pages 561-578. Springer, 2016. 1, 3, 5\n\nOpenpose: Realtime multi-person 2d pose estimation using part affinity fields. Z Cao, G Hidalgo, T Simon, S.-E Wei, Y Sheikh, arXiv:1812.08008arXiv preprintZ. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. arXiv preprint arXiv:1812.08008, 2018. 3\n\nA X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, arXiv:1512.03012An information-rich 3d model repository. 36arXiv preprintA. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 3, 6\n\nActive appearance models. T F Cootes, G J Edwards, C J Taylor, IEEE Transactions on Pattern Analysis & Machine Intelligence. 6T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active ap- pearance models. IEEE Transactions on Pattern Analysis & Machine Intelligence, (6):681-685, 2001. 1\n\nSingle-image svbrdf capture with a renderingaware deep network. V Deschaintre, M Aittala, F Durand, G Drettakis, A Bousseau, ACM Transactions on Graphics (TOG). 374128V. Deschaintre, M. Aittala, F. Durand, G. Drettakis, and A. Bousseau. Single-image svbrdf capture with a rendering- aware deep network. ACM Transactions on Graphics (TOG), 37(4):128, 2018. 2\n\nAccurate, dense, and robust multiview stereopsis. Y Furukawa, J Ponce, IEEE transactions on pattern analysis and machine intelligence. 323Y. Furukawa and J. Ponce. Accurate, dense, and robust mul- tiview stereopsis. IEEE transactions on pattern analysis and machine intelligence, 32(8):1362-1376, 2010. 3\n\nUnsupervised training for 3d morphable model regression. K Genova, F Cole, A Maschinot, A Sarna, D Vlasic, W T Freeman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionK. Genova, F. Cole, A. Maschinot, A. Sarna, D. Vlasic, and W. T. Freeman. Unsupervised training for 3d morphable model regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8377- 8386, 2018. 2\n\nAn evaluation of computational imaging techniques for heterogeneous inverse scattering. I Gkioulekas, A Levin, T Zickler, European Conference on Computer Vision. SpringerI. Gkioulekas, A. Levin, and T. Zickler. An evaluation of computational imaging techniques for heterogeneous inverse scattering. In European Conference on Computer Vision, pages 685-701. Springer, 2016. 2\n\nInverse volume rendering with material dictionaries. I Gkioulekas, S Zhao, K Bala, T Zickler, A Levin, ACM Transactions on Graphics (TOG). 326162I. Gkioulekas, S. Zhao, K. Bala, T. Zickler, and A. Levin. Inverse volume rendering with material dictionaries. ACM Transactions on Graphics (TOG), 32(6):162, 2013. 2\n\nAtlasnet: A papier-mch approach to learning 3d surface generation. computer vision and pattern recognition. T Groueix, M Fisher, V G Kim, B C Russell, M Aubry, T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry. Atlasnet: A papier-mch approach to learning 3d surface generation. computer vision and pattern recognition, 2018. 3\n\nMultiple view geometry in computer vision. R Hartley, A Zisserman, Cambridge university pressR. Hartley and A. Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 3\n\nLearning to generate and reconstruct 3d meshes with only 2d supervision. P Henderson, V Ferrari, British Machine Vision Conference (BMVC). P. Henderson and V. Ferrari. Learning to generate and re- construct 3d meshes with only 2d supervision. In British Machine Vision Conference (BMVC), 2018. 2\n\nDeep volumetric video from very sparse multi-view performance capture. Z Huang, T Li, W Chen, Y Zhao, J Xing, C Legendre, L Luo, C Ma, H Li, European Conference on Computer Vision. SpringerZ. Huang, T. Li, W. Chen, Y. Zhao, J. Xing, C. LeGendre, L. Luo, C. Ma, and H. Li. Deep volumetric video from very sparse multi-view performance capture. In European Con- ference on Computer Vision, pages 351-369. Springer, 2018. 3\n\nMesoscopic facial geometry inference using deep neural networks. L Huynh, W Chen, S Saito, J Xing, K Nagano, A Jones, P Debevec, H Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionL. Huynh, W. Chen, S. Saito, J. Xing, K. Nagano, A. Jones, P. Debevec, and H. Li. Mesoscopic facial geometry infer- ence using deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 8407-8416, 2018. 3\n\nEndto-end recovery of human shape and pose. A Kanazawa, M J Black, D W Jacobs, J Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End- to-end recovery of human shape and pose. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7122-7131, 2018. 3\n\nLearning category-specific mesh reconstruction from image collections. A Kanazawa, S Tulsiani, A A Efros, J Malik, arXiv:1803.07549arXiv preprintA. Kanazawa, S. Tulsiani, A. A. Efros, and J. Malik. Learn- ing category-specific mesh reconstruction from image col- lections. arXiv preprint arXiv:1803.07549, 2018. 3\n\nNeural 3d mesh renderer. H Kato, Y Ushiku, T Harada, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition1315H. Kato, Y. Ushiku, and T. Harada. Neural 3d mesh renderer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3907-3916, 2018. 1, 2, 4, 5, 6, 7, 8, 13, 15\n\nPosenet: A convolutional network for real-time 6-dof camera relocalization. A Kendall, M Grimes, R Cipolla, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionA. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu- tional network for real-time 6-dof camera relocalization. In Proceedings of the IEEE international conference on com- puter vision, pages 2938-2946, 2015. 3\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6\n\n3d-rcnn: Instance-level 3d object reconstruction via render-and-compare. A Kundu, Y Li, J M Rehg, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Kundu, Y. Li, and J. M. Rehg. 3d-rcnn: Instance-level 3d object reconstruction via render-and-compare. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3559-3568, 2018. 2\n\nImage-based reconstruction of spatial appearance and geometric detail. H Lensch, J Kautz, M Goesele, W Heidrich, H.-P Seidel, ACM Transactions on Graphics (TOG). 222H. Lensch, J. Kautz, M. Goesele, W. Heidrich, and H.-P. Sei- del. Image-based reconstruction of spatial appearance and geometric detail. ACM Transactions on Graphics (TOG), 22(2):234-257, 2003. 1\n\nDifferentiable monte carlo ray tracing through edge sampling. T.-M Li, M Aittala, F Durand, J Lehtinen, 222:1- 222:11Proc. SIGGRAPH Asia). SIGGRAPH Asia)37T.-M. Li, M. Aittala, F. Durand, and J. Lehtinen. Dif- ferentiable monte carlo ray tracing through edge sampling. ACM Trans. Graph. (Proc. SIGGRAPH Asia), 37(6):222:1- 222:11, 2018. 2\n\nLearning depth from single monocular images using deep convolutional neural fields. F Liu, C Shen, G Lin, I D Reid, IEEE Trans. Pattern Anal. Mach. Intell. 3810F. Liu, C. Shen, G. Lin, and I. D. Reid. Learning depth from single monocular images using deep convolutional neural fields. IEEE Trans. Pattern Anal. Mach. Intell., 38(10):2024- 2039, 2016. 3\n\nJoint face alignment and 3d face reconstruction. F Liu, D Zeng, Q Zhao, X Liu, European Conference on Computer Vision. SpringerF. Liu, D. Zeng, Q. Zhao, and X. Liu. Joint face align- ment and 3d face reconstruction. In European Conference on Computer Vision, pages 545-560. Springer, 2016. 1\n\nMaterial editing using a physically based rendering network. G Liu, D Ceylan, E Yumer, J Yang, J.-M Lien, 2017 IEEE International Conference on. Computer Vision (ICCVG. Liu, D. Ceylan, E. Yumer, J. Yang, and J.-M. Lien. Ma- terial editing using a physically based rendering network. In Computer Vision (ICCV), 2017 IEEE International Confer- ence on, pages 2280-2288. IEEE, 2017. 2\n\nSmpl: A skinned multi-person linear model. M Loper, N Mahmood, J Romero, G Pons-Moll, M J Black, ACM transactions on graphics (TOG). 346248M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. Smpl: A skinned multi-person linear model. ACM transactions on graphics (TOG), 34(6):248, 2015. 1, 3, 8\n\nOpendr: An approximate differentiable renderer. M M Loper, M J Black, European Conference on Computer Vision. Springer613M. M. Loper and M. J. Black. Opendr: An approximate dif- ferentiable renderer. In European Conference on Computer Vision, pages 154-169. Springer, 2014. 1, 2, 4, 6, 13\n\nApproximate bayesian image interpretation using generative probabilistic graphics programs. V K Mansinghka, T D Kulkarni, Y N Perov, J Tenenbaum, Advances in Neural Information Processing Systems. V. K. Mansinghka, T. D. Kulkarni, Y. N. Perov, and J. Tenen- baum. Approximate bayesian image interpretation using generative probabilistic graphics programs. In Advances in Neural Information Processing Systems, pages 1520-1528, 2013. 2\n\nPose-aware face recognition in the wild. I Masi, S Rawls, G Medioni, P Natarajan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionI. Masi, S. Rawls, G. Medioni, and P. Natarajan. Pose-aware face recognition in the wild. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 4838-4846, 2016. 3\n\nImage-based visual hulls. W Matusik, C Buehler, R Raskar, S J Gortler, L Mcmillan, Proceedings of the 27th annual conference on Computer graphics and interactive techniques. the 27th annual conference on Computer graphics and interactive techniquesACM Press/Addison-Wesley Publishing CoW. Matusik, C. Buehler, R. Raskar, S. J. Gortler, and L. McMillan. Image-based visual hulls. In Proceedings of the 27th annual conference on Computer graphics and in- teractive techniques, pages 369-374. ACM Press/Addison- Wesley Publishing Co., 2000. 1\n\nDeep shading: convolutional neural networks for screen space shading. O Nalbach, E Arabadzhiyska, D Mehta, H.-P Seidel, T Ritschel, Computer graphics forum. Wiley Online Library36O. Nalbach, E. Arabadzhiyska, D. Mehta, H.-P. Seidel, and T. Ritschel. Deep shading: convolutional neural networks for screen space shading. In Computer graphics forum, vol- ume 36, pages 65-78. Wiley Online Library, 2017. 2\n\nT Nguyen-Phuoc, C Li, S Balaban, Y Yang, arXiv:1806.06575Rendernet: A deep convolutional network for differentiable rendering from 3d shapes. arXiv preprintT. Nguyen-Phuoc, C. Li, S. Balaban, and Y. Yang. Render- net: A deep convolutional network for differentiable render- ing from 3d shapes. arXiv preprint arXiv:1806.06575, 2018. 2\n\nCoarse-to-fine volumetric prediction for single-image 3d human pose. G Pavlakos, X Zhou, K G Derpanis, K Daniilidis, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition15G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis. Coarse-to-fine volumetric prediction for single-image 3d hu- man pose. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 7025-7034, 2017. 1, 5\n\nGeonet: Geometric neural network for joint depth and surface normal estimation. X Qi, R Liao, Z Liu, R Urtasun, J Jia, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionX. Qi, R. Liao, Z. Liu, R. Urtasun, and J. Jia. Geonet: Ge- ometric neural network for joint depth and surface normal estimation. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 283-291, 2018. 3\n\nUnsupervised learning of 3d structure from images. D J Rezende, S A Eslami, S Mohamed, P Battaglia, M Jaderberg, N Heess, Advances in Neural Information Processing Systems. D. J. Rezende, S. A. Eslami, S. Mohamed, P. Battaglia, M. Jaderberg, and N. Heess. Unsupervised learning of 3d structure from images. In Advances in Neural Information Processing Systems, pages 4996-5004, 2016. 2\n\nLearning detailed face reconstruction from a single image. E Richardson, M Sela, R Or-El, R Kimmel, Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. E. Richardson, M. Sela, R. Or-El, and R. Kimmel. Learning detailed face reconstruction from a single image. In Com- puter Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 5553-5562. IEEE, 2017. 2\n\nSelf-supervised multi-level face model learning for monocular reconstruction at over 250 hz. A Tewari, M Zollh\u00f6fer, P Garrido, F Bernard, H Kim, P P\u00e9rez, C Theobalt, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Tewari, M. Zollh\u00f6fer, P. Garrido, F. Bernard, H. Kim, P. P\u00e9rez, and C. Theobalt. Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2549-2559, 2018. 2\n\nMofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction. A Tewari, M Zollh\u00f6fer, H Kim, P Garrido, F Bernard, P P\u00e9rez, C Theobalt, The IEEE International Conference on Computer Vision (ICCV). 2A. Tewari, M. Zollh\u00f6fer, H. Kim, P. Garrido, F. Bernard, P. P\u00e9rez, and C. Theobalt. Mofa: Model-based deep convo- lutional face autoencoder for unsupervised monocular recon- struction. In The IEEE International Conference on Com- puter Vision (ICCV), volume 2, page 5, 2017. 2\n\nNonlinear 3d face morphable model. L Tran, X Liu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionL. Tran and X. Liu. Nonlinear 3d face morphable model. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7346-7355, 2018. 2\n\nViewpoints and keypoints. S Tulsiani, J Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS. Tulsiani and J. Malik. Viewpoints and keypoints. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1510-1519, 2015. 3\n\nPixel2mesh: Generating 3d mesh models from single rgb images. N Wang, Y Zhang, Z Li, Y Fu, W Liu, Y.-G Jiang, ECCV. 67N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2mesh: Generating 3d mesh models from single rgb im- ages. In ECCV, 2018. 3, 5, 6, 7\n\nDesigning deep networks for surface normal estimation. X Wang, D Fouhey, A Gupta, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionX. Wang, D. Fouhey, and A. Gupta. Designing deep net- works for surface normal estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 539-547, 2015. 3\n\nConvolutional pose machines. S.-E Wei, V Ramakrishna, T Kanade, Y Sheikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con- volutional pose machines. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 4724-4732, 2016. 3\n\nPosecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Y Xiang, T Schmidt, V Narayanan, D Fox, Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. 2018. 3\n\nPerspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision. X Yan, J Yang, E Yumer, Y Guo, H Lee, Advances in Neural Information Processing Systems. 67X. Yan, J. Yang, E. Yumer, Y. Guo, and H. Lee. Perspective transformer nets: Learning single-view 3d object reconstruc- tion without 3d supervision. In Advances in Neural Informa- tion Processing Systems, pages 1696-1704, 2016. 5, 6, 7\n\nShape-fromshading: a survey. R Zhang, P.-S Tsai, J E Cryer, M Shah, IEEE transactions on pattern analysis and machine intelligence. 21R. Zhang, P.-S. Tsai, J. E. Cryer, and M. Shah. Shape-from- shading: a survey. IEEE transactions on pattern analysis and machine intelligence, 21(8):690-706, 1999. 1\n\nReal-time height map fusion using differentiable rendering. J Zienkiewicz, A Davison, S Leutenegger, Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. J. Zienkiewicz, A. Davison, and S. Leutenegger. Real-time height map fusion using differentiable rendering. In Intel- ligent Robots and Systems (IROS), 2016 IEEE/RSJ Interna- tional Conference on, pages 4280-4287. IEEE, 2016. 2\n", "annotations": {"author": "[{\"end\":197,\"start\":75},{\"end\":250,\"start\":198},{\"end\":371,\"start\":251},{\"end\":468,\"start\":372}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":83},{\"end\":207,\"start\":205},{\"end\":262,\"start\":258},{\"end\":378,\"start\":376}]", "author_first_name": "[{\"end\":82,\"start\":75},{\"end\":204,\"start\":198},{\"end\":257,\"start\":251},{\"end\":375,\"start\":372}]", "author_affiliation": "[{\"end\":149,\"start\":109},{\"end\":196,\"start\":151},{\"end\":249,\"start\":209},{\"end\":323,\"start\":283},{\"end\":370,\"start\":325},{\"end\":420,\"start\":380},{\"end\":467,\"start\":422}]", "title": "[{\"end\":72,\"start\":1},{\"end\":540,\"start\":469}]", "venue": null, "abstract": "[{\"end\":2159,\"start\":542}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2962,\"start\":2959},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2965,\"start\":2962},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2968,\"start\":2965},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2971,\"start\":2968},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3002,\"start\":2999},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3005,\"start\":3002},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3007,\"start\":3005},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3010,\"start\":3007},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3013,\"start\":3010},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8600,\"start\":8596},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8603,\"start\":8600},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12387,\"start\":12383},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12390,\"start\":12387},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12393,\"start\":12390},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12425,\"start\":12421},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12563,\"start\":12559},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12721,\"start\":12717},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12871,\"start\":12867},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12874,\"start\":12871},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12877,\"start\":12874},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12880,\"start\":12877},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12882,\"start\":12880},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12907,\"start\":12903},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12909,\"start\":12907},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12948,\"start\":12944},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12951,\"start\":12948},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12954,\"start\":12951},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12957,\"start\":12954},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12960,\"start\":12957},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12963,\"start\":12960},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13694,\"start\":13690},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13696,\"start\":13694},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13820,\"start\":13817},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13852,\"start\":13848},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13855,\"start\":13852},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13858,\"start\":13855},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14090,\"start\":14086},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14114,\"start\":14110},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14137,\"start\":14133},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14156,\"start\":14152},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14159,\"start\":14156},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14340,\"start\":14336},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14364,\"start\":14360},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14367,\"start\":14364},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14610,\"start\":14606},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14612,\"start\":14610},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14615,\"start\":14612},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14653,\"start\":14650},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14656,\"start\":14653},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14741,\"start\":14738},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14743,\"start\":14741},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14746,\"start\":14743},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14748,\"start\":14746},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20491,\"start\":20487},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20504,\"start\":20500},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22663,\"start\":22659},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22666,\"start\":22663},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22883,\"start\":22879},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22886,\"start\":22883},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25165,\"start\":25162},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25188,\"start\":25184},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25534,\"start\":25530},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26268,\"start\":26264},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26271,\"start\":26268},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27427,\"start\":27423},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27486,\"start\":27483},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27664,\"start\":27660},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27667,\"start\":27664},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27868,\"start\":27864},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27871,\"start\":27868},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27933,\"start\":27929},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28329,\"start\":28325},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28354,\"start\":28350},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28414,\"start\":28410},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28848,\"start\":28844},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28936,\"start\":28932},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29100,\"start\":29096},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29142,\"start\":29138},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30065,\"start\":30061},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30166,\"start\":30162},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31296,\"start\":31295},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32661,\"start\":32657},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33020,\"start\":33016},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36743,\"start\":36739},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36746,\"start\":36743},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41177,\"start\":41173},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41610,\"start\":41606}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":42115,\"start\":41692},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42239,\"start\":42116},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42296,\"start\":42240},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42463,\"start\":42297},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42717,\"start\":42464},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42774,\"start\":42718},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42912,\"start\":42775},{\"attributes\":{\"id\":\"fig_8\"},\"end\":43378,\"start\":42913},{\"attributes\":{\"id\":\"fig_10\"},\"end\":43518,\"start\":43379},{\"attributes\":{\"id\":\"fig_11\"},\"end\":43580,\"start\":43519},{\"attributes\":{\"id\":\"fig_12\"},\"end\":43873,\"start\":43581},{\"attributes\":{\"id\":\"fig_13\"},\"end\":43939,\"start\":43874},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44118,\"start\":43940},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44381,\"start\":44119}]", "paragraph": "[{\"end\":2937,\"start\":2175},{\"end\":3795,\"start\":2939},{\"end\":4807,\"start\":4144},{\"end\":5817,\"start\":5154},{\"end\":6827,\"start\":6164},{\"end\":9174,\"start\":7174},{\"end\":9880,\"start\":9176},{\"end\":10935,\"start\":9882},{\"end\":12181,\"start\":10937},{\"end\":13419,\"start\":12198},{\"end\":14616,\"start\":13421},{\"end\":14901,\"start\":14618},{\"end\":15816,\"start\":14957},{\"end\":16565,\"start\":15818},{\"end\":16889,\"start\":16597},{\"end\":17053,\"start\":16933},{\"end\":17351,\"start\":17262},{\"end\":18070,\"start\":17353},{\"end\":18563,\"start\":18093},{\"end\":18717,\"start\":18616},{\"end\":19167,\"start\":18784},{\"end\":19937,\"start\":19169},{\"end\":20336,\"start\":19984},{\"end\":20622,\"start\":20369},{\"end\":21109,\"start\":20624},{\"end\":21744,\"start\":21111},{\"end\":21910,\"start\":21773},{\"end\":22749,\"start\":21946},{\"end\":23619,\"start\":22751},{\"end\":24845,\"start\":23670},{\"end\":26829,\"start\":24875},{\"end\":27162,\"start\":26876},{\"end\":27302,\"start\":27178},{\"end\":28173,\"start\":27359},{\"end\":29143,\"start\":28197},{\"end\":29606,\"start\":29145},{\"end\":30263,\"start\":29635},{\"end\":30384,\"start\":30282},{\"end\":31436,\"start\":30386},{\"end\":32438,\"start\":31466},{\"end\":33350,\"start\":32440},{\"end\":34811,\"start\":33366},{\"end\":34887,\"start\":34813},{\"end\":35073,\"start\":34947},{\"end\":35270,\"start\":35120},{\"end\":35757,\"start\":35272},{\"end\":35823,\"start\":35759},{\"end\":36402,\"start\":36254},{\"end\":36488,\"start\":36455},{\"end\":37407,\"start\":36568},{\"end\":38074,\"start\":37432},{\"end\":38797,\"start\":38076},{\"end\":38932,\"start\":38845},{\"end\":39628,\"start\":39009},{\"end\":40002,\"start\":39681},{\"end\":40494,\"start\":40053},{\"end\":41066,\"start\":40544},{\"end\":41691,\"start\":41114}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4143,\"start\":3796},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5153,\"start\":4808},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6163,\"start\":5818},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7173,\"start\":6828},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16932,\"start\":16890},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17261,\"start\":17054},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18615,\"start\":18564},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18783,\"start\":18718},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19983,\"start\":19938},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23646,\"start\":23620},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26875,\"start\":26830},{\"attributes\":{\"id\":\"formula_11\"},\"end\":34946,\"start\":34888},{\"attributes\":{\"id\":\"formula_12\"},\"end\":35119,\"start\":35074},{\"attributes\":{\"id\":\"formula_13\"},\"end\":36217,\"start\":35824},{\"attributes\":{\"id\":\"formula_14\"},\"end\":36454,\"start\":36403},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36536,\"start\":36489}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29721,\"start\":29714},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":30434,\"start\":30427},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":30708,\"start\":30701},{\"end\":31830,\"start\":31823}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2173,\"start\":2161},{\"attributes\":{\"n\":\"2.\"},\"end\":12196,\"start\":12184},{\"attributes\":{\"n\":\"3.\"},\"end\":14919,\"start\":14904},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14955,\"start\":14922},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16595,\"start\":16568},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18091,\"start\":18073},{\"attributes\":{\"n\":\"3.4.\"},\"end\":20367,\"start\":20339},{\"attributes\":{\"n\":\"4.\"},\"end\":21771,\"start\":21747},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21944,\"start\":21913},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":23668,\"start\":23648},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24873,\"start\":24848},{\"attributes\":{\"n\":\"5.\"},\"end\":27176,\"start\":27165},{\"attributes\":{\"n\":\"5.1.\"},\"end\":27336,\"start\":27305},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":27357,\"start\":27339},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":28195,\"start\":28176},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":29633,\"start\":29609},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":30280,\"start\":30266},{\"attributes\":{\"n\":\"5.2.\"},\"end\":31464,\"start\":31439},{\"attributes\":{\"n\":\"6.\"},\"end\":33364,\"start\":33353},{\"end\":36252,\"start\":36219},{\"end\":36566,\"start\":36538},{\"end\":37430,\"start\":37410},{\"end\":38843,\"start\":38800},{\"end\":38970,\"start\":38935},{\"end\":39007,\"start\":38973},{\"end\":39679,\"start\":39631},{\"end\":40051,\"start\":40005},{\"end\":40542,\"start\":40497},{\"end\":41112,\"start\":41069},{\"end\":41703,\"start\":41693},{\"end\":42127,\"start\":42117},{\"end\":42308,\"start\":42298},{\"end\":42475,\"start\":42465},{\"end\":42729,\"start\":42719},{\"end\":42786,\"start\":42776},{\"end\":42925,\"start\":42914},{\"end\":43391,\"start\":43380},{\"end\":43531,\"start\":43520},{\"end\":43604,\"start\":43582},{\"end\":43886,\"start\":43875},{\"end\":43950,\"start\":43941},{\"end\":44129,\"start\":44120}]", "table": "[{\"end\":44118,\"start\":44063}]", "figure_caption": "[{\"end\":42115,\"start\":41705},{\"end\":42239,\"start\":42129},{\"end\":42296,\"start\":42242},{\"end\":42463,\"start\":42310},{\"end\":42717,\"start\":42477},{\"end\":42774,\"start\":42731},{\"end\":42912,\"start\":42788},{\"end\":43378,\"start\":42928},{\"end\":43518,\"start\":43394},{\"end\":43580,\"start\":43534},{\"end\":43873,\"start\":43609},{\"end\":43939,\"start\":43889},{\"end\":44063,\"start\":43952},{\"end\":44381,\"start\":44131}]", "figure_ref": "[{\"end\":2532,\"start\":2524},{\"end\":9463,\"start\":9455},{\"end\":10106,\"start\":10097},{\"end\":10553,\"start\":10537},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11267,\"start\":11259},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12036,\"start\":12027},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14977,\"start\":14969},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15716,\"start\":15707},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17657,\"start\":17649},{\"end\":19091,\"start\":19083},{\"end\":20553,\"start\":20545},{\"end\":20906,\"start\":20897},{\"end\":21108,\"start\":21090},{\"end\":21323,\"start\":21315},{\"end\":21496,\"start\":21479},{\"end\":21632,\"start\":21624},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22303,\"start\":22295},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24212,\"start\":24204},{\"end\":25371,\"start\":25363},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26697,\"start\":26687},{\"end\":28397,\"start\":28389},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29257,\"start\":29249},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":31136,\"start\":31127},{\"end\":31374,\"start\":31365},{\"end\":32477,\"start\":32468},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":34411,\"start\":34402},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":36596,\"start\":36587},{\"end\":37535,\"start\":37526},{\"end\":37803,\"start\":37794},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38248,\"start\":38239},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":38343,\"start\":38334},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38393,\"start\":38385},{\"end\":39021,\"start\":39012},{\"end\":39349,\"start\":39340},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39758,\"start\":39749},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":40104,\"start\":40095},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40228,\"start\":40219},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40683,\"start\":40674},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":40807,\"start\":40798},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":41126,\"start\":41117},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":41560,\"start\":41551}]", "bib_author_first_name": "[{\"end\":46842,\"start\":46841},{\"end\":46851,\"start\":46850},{\"end\":47358,\"start\":47357},{\"end\":47367,\"start\":47366},{\"end\":47652,\"start\":47651},{\"end\":47660,\"start\":47659},{\"end\":47672,\"start\":47671},{\"end\":47683,\"start\":47682},{\"end\":47693,\"start\":47692},{\"end\":47703,\"start\":47702},{\"end\":47705,\"start\":47704},{\"end\":48082,\"start\":48081},{\"end\":48089,\"start\":48088},{\"end\":48100,\"start\":48099},{\"end\":48112,\"start\":48108},{\"end\":48119,\"start\":48118},{\"end\":48336,\"start\":48335},{\"end\":48338,\"start\":48337},{\"end\":48347,\"start\":48346},{\"end\":48361,\"start\":48360},{\"end\":48371,\"start\":48370},{\"end\":48383,\"start\":48382},{\"end\":48392,\"start\":48391},{\"end\":48398,\"start\":48397},{\"end\":48410,\"start\":48409},{\"end\":48419,\"start\":48418},{\"end\":48427,\"start\":48426},{\"end\":48743,\"start\":48742},{\"end\":48745,\"start\":48744},{\"end\":48755,\"start\":48754},{\"end\":48757,\"start\":48756},{\"end\":48768,\"start\":48767},{\"end\":48770,\"start\":48769},{\"end\":49066,\"start\":49065},{\"end\":49081,\"start\":49080},{\"end\":49092,\"start\":49091},{\"end\":49102,\"start\":49101},{\"end\":49115,\"start\":49114},{\"end\":49411,\"start\":49410},{\"end\":49423,\"start\":49422},{\"end\":49724,\"start\":49723},{\"end\":49734,\"start\":49733},{\"end\":49742,\"start\":49741},{\"end\":49755,\"start\":49754},{\"end\":49764,\"start\":49763},{\"end\":49774,\"start\":49773},{\"end\":49776,\"start\":49775},{\"end\":50256,\"start\":50255},{\"end\":50270,\"start\":50269},{\"end\":50279,\"start\":50278},{\"end\":50597,\"start\":50596},{\"end\":50611,\"start\":50610},{\"end\":50619,\"start\":50618},{\"end\":50627,\"start\":50626},{\"end\":50638,\"start\":50637},{\"end\":50965,\"start\":50964},{\"end\":50976,\"start\":50975},{\"end\":50986,\"start\":50985},{\"end\":50988,\"start\":50987},{\"end\":50995,\"start\":50994},{\"end\":50997,\"start\":50996},{\"end\":51008,\"start\":51007},{\"end\":51240,\"start\":51239},{\"end\":51251,\"start\":51250},{\"end\":51472,\"start\":51471},{\"end\":51485,\"start\":51484},{\"end\":51767,\"start\":51766},{\"end\":51776,\"start\":51775},{\"end\":51782,\"start\":51781},{\"end\":51790,\"start\":51789},{\"end\":51798,\"start\":51797},{\"end\":51806,\"start\":51805},{\"end\":51818,\"start\":51817},{\"end\":51825,\"start\":51824},{\"end\":51831,\"start\":51830},{\"end\":52183,\"start\":52182},{\"end\":52192,\"start\":52191},{\"end\":52200,\"start\":52199},{\"end\":52209,\"start\":52208},{\"end\":52217,\"start\":52216},{\"end\":52227,\"start\":52226},{\"end\":52236,\"start\":52235},{\"end\":52247,\"start\":52246},{\"end\":52697,\"start\":52696},{\"end\":52709,\"start\":52708},{\"end\":52711,\"start\":52710},{\"end\":52720,\"start\":52719},{\"end\":52722,\"start\":52721},{\"end\":52732,\"start\":52731},{\"end\":53161,\"start\":53160},{\"end\":53173,\"start\":53172},{\"end\":53185,\"start\":53184},{\"end\":53187,\"start\":53186},{\"end\":53196,\"start\":53195},{\"end\":53430,\"start\":53429},{\"end\":53438,\"start\":53437},{\"end\":53448,\"start\":53447},{\"end\":53873,\"start\":53872},{\"end\":53884,\"start\":53883},{\"end\":53894,\"start\":53893},{\"end\":54287,\"start\":54286},{\"end\":54289,\"start\":54288},{\"end\":54299,\"start\":54298},{\"end\":54516,\"start\":54515},{\"end\":54525,\"start\":54524},{\"end\":54531,\"start\":54530},{\"end\":54533,\"start\":54532},{\"end\":54969,\"start\":54968},{\"end\":54979,\"start\":54978},{\"end\":54988,\"start\":54987},{\"end\":54999,\"start\":54998},{\"end\":55014,\"start\":55010},{\"end\":55325,\"start\":55321},{\"end\":55331,\"start\":55330},{\"end\":55342,\"start\":55341},{\"end\":55352,\"start\":55351},{\"end\":55684,\"start\":55683},{\"end\":55691,\"start\":55690},{\"end\":55699,\"start\":55698},{\"end\":55706,\"start\":55705},{\"end\":55708,\"start\":55707},{\"end\":56003,\"start\":56002},{\"end\":56010,\"start\":56009},{\"end\":56018,\"start\":56017},{\"end\":56026,\"start\":56025},{\"end\":56308,\"start\":56307},{\"end\":56315,\"start\":56314},{\"end\":56325,\"start\":56324},{\"end\":56334,\"start\":56333},{\"end\":56345,\"start\":56341},{\"end\":56673,\"start\":56672},{\"end\":56682,\"start\":56681},{\"end\":56693,\"start\":56692},{\"end\":56703,\"start\":56702},{\"end\":56716,\"start\":56715},{\"end\":56718,\"start\":56717},{\"end\":56986,\"start\":56985},{\"end\":56988,\"start\":56987},{\"end\":56997,\"start\":56996},{\"end\":56999,\"start\":56998},{\"end\":57320,\"start\":57319},{\"end\":57322,\"start\":57321},{\"end\":57336,\"start\":57335},{\"end\":57338,\"start\":57337},{\"end\":57350,\"start\":57349},{\"end\":57352,\"start\":57351},{\"end\":57361,\"start\":57360},{\"end\":57705,\"start\":57704},{\"end\":57713,\"start\":57712},{\"end\":57722,\"start\":57721},{\"end\":57733,\"start\":57732},{\"end\":58113,\"start\":58112},{\"end\":58124,\"start\":58123},{\"end\":58135,\"start\":58134},{\"end\":58145,\"start\":58144},{\"end\":58147,\"start\":58146},{\"end\":58158,\"start\":58157},{\"end\":58698,\"start\":58697},{\"end\":58709,\"start\":58708},{\"end\":58726,\"start\":58725},{\"end\":58738,\"start\":58734},{\"end\":58748,\"start\":58747},{\"end\":59033,\"start\":59032},{\"end\":59049,\"start\":59048},{\"end\":59055,\"start\":59054},{\"end\":59066,\"start\":59065},{\"end\":59438,\"start\":59437},{\"end\":59450,\"start\":59449},{\"end\":59458,\"start\":59457},{\"end\":59460,\"start\":59459},{\"end\":59472,\"start\":59471},{\"end\":59950,\"start\":59949},{\"end\":59956,\"start\":59955},{\"end\":59964,\"start\":59963},{\"end\":59971,\"start\":59970},{\"end\":59982,\"start\":59981},{\"end\":60419,\"start\":60418},{\"end\":60421,\"start\":60420},{\"end\":60432,\"start\":60431},{\"end\":60434,\"start\":60433},{\"end\":60444,\"start\":60443},{\"end\":60455,\"start\":60454},{\"end\":60468,\"start\":60467},{\"end\":60481,\"start\":60480},{\"end\":60814,\"start\":60813},{\"end\":60828,\"start\":60827},{\"end\":60836,\"start\":60835},{\"end\":60845,\"start\":60844},{\"end\":61239,\"start\":61238},{\"end\":61249,\"start\":61248},{\"end\":61262,\"start\":61261},{\"end\":61273,\"start\":61272},{\"end\":61284,\"start\":61283},{\"end\":61291,\"start\":61290},{\"end\":61300,\"start\":61299},{\"end\":61835,\"start\":61834},{\"end\":61845,\"start\":61844},{\"end\":61858,\"start\":61857},{\"end\":61865,\"start\":61864},{\"end\":61876,\"start\":61875},{\"end\":61887,\"start\":61886},{\"end\":61896,\"start\":61895},{\"end\":62283,\"start\":62282},{\"end\":62291,\"start\":62290},{\"end\":62628,\"start\":62627},{\"end\":62640,\"start\":62639},{\"end\":63014,\"start\":63013},{\"end\":63022,\"start\":63021},{\"end\":63031,\"start\":63030},{\"end\":63037,\"start\":63036},{\"end\":63043,\"start\":63042},{\"end\":63053,\"start\":63049},{\"end\":63274,\"start\":63273},{\"end\":63282,\"start\":63281},{\"end\":63292,\"start\":63291},{\"end\":63673,\"start\":63669},{\"end\":63680,\"start\":63679},{\"end\":63695,\"start\":63694},{\"end\":63705,\"start\":63704},{\"end\":64141,\"start\":64140},{\"end\":64150,\"start\":64149},{\"end\":64161,\"start\":64160},{\"end\":64174,\"start\":64173},{\"end\":64429,\"start\":64428},{\"end\":64436,\"start\":64435},{\"end\":64444,\"start\":64443},{\"end\":64453,\"start\":64452},{\"end\":64460,\"start\":64459},{\"end\":64786,\"start\":64785},{\"end\":64798,\"start\":64794},{\"end\":64806,\"start\":64805},{\"end\":64808,\"start\":64807},{\"end\":64817,\"start\":64816},{\"end\":65118,\"start\":65117},{\"end\":65133,\"start\":65132},{\"end\":65144,\"start\":65143}]", "bib_author_last_name": "[{\"end\":46848,\"start\":46843},{\"end\":46858,\"start\":46852},{\"end\":47364,\"start\":47359},{\"end\":47374,\"start\":47368},{\"end\":47657,\"start\":47653},{\"end\":47669,\"start\":47661},{\"end\":47680,\"start\":47673},{\"end\":47690,\"start\":47684},{\"end\":47700,\"start\":47694},{\"end\":47711,\"start\":47706},{\"end\":48086,\"start\":48083},{\"end\":48097,\"start\":48090},{\"end\":48106,\"start\":48101},{\"end\":48116,\"start\":48113},{\"end\":48126,\"start\":48120},{\"end\":48344,\"start\":48339},{\"end\":48358,\"start\":48348},{\"end\":48368,\"start\":48362},{\"end\":48380,\"start\":48372},{\"end\":48389,\"start\":48384},{\"end\":48395,\"start\":48393},{\"end\":48407,\"start\":48399},{\"end\":48416,\"start\":48411},{\"end\":48424,\"start\":48420},{\"end\":48430,\"start\":48428},{\"end\":48752,\"start\":48746},{\"end\":48765,\"start\":48758},{\"end\":48777,\"start\":48771},{\"end\":49078,\"start\":49067},{\"end\":49089,\"start\":49082},{\"end\":49099,\"start\":49093},{\"end\":49112,\"start\":49103},{\"end\":49124,\"start\":49116},{\"end\":49420,\"start\":49412},{\"end\":49429,\"start\":49424},{\"end\":49731,\"start\":49725},{\"end\":49739,\"start\":49735},{\"end\":49752,\"start\":49743},{\"end\":49761,\"start\":49756},{\"end\":49771,\"start\":49765},{\"end\":49784,\"start\":49777},{\"end\":50267,\"start\":50257},{\"end\":50276,\"start\":50271},{\"end\":50287,\"start\":50280},{\"end\":50608,\"start\":50598},{\"end\":50616,\"start\":50612},{\"end\":50624,\"start\":50620},{\"end\":50635,\"start\":50628},{\"end\":50644,\"start\":50639},{\"end\":50973,\"start\":50966},{\"end\":50983,\"start\":50977},{\"end\":50992,\"start\":50989},{\"end\":51005,\"start\":50998},{\"end\":51014,\"start\":51009},{\"end\":51248,\"start\":51241},{\"end\":51261,\"start\":51252},{\"end\":51482,\"start\":51473},{\"end\":51493,\"start\":51486},{\"end\":51773,\"start\":51768},{\"end\":51779,\"start\":51777},{\"end\":51787,\"start\":51783},{\"end\":51795,\"start\":51791},{\"end\":51803,\"start\":51799},{\"end\":51815,\"start\":51807},{\"end\":51822,\"start\":51819},{\"end\":51828,\"start\":51826},{\"end\":51834,\"start\":51832},{\"end\":52189,\"start\":52184},{\"end\":52197,\"start\":52193},{\"end\":52206,\"start\":52201},{\"end\":52214,\"start\":52210},{\"end\":52224,\"start\":52218},{\"end\":52233,\"start\":52228},{\"end\":52244,\"start\":52237},{\"end\":52250,\"start\":52248},{\"end\":52706,\"start\":52698},{\"end\":52717,\"start\":52712},{\"end\":52729,\"start\":52723},{\"end\":52738,\"start\":52733},{\"end\":53170,\"start\":53162},{\"end\":53182,\"start\":53174},{\"end\":53193,\"start\":53188},{\"end\":53202,\"start\":53197},{\"end\":53435,\"start\":53431},{\"end\":53445,\"start\":53439},{\"end\":53455,\"start\":53449},{\"end\":53881,\"start\":53874},{\"end\":53891,\"start\":53885},{\"end\":53902,\"start\":53895},{\"end\":54296,\"start\":54290},{\"end\":54302,\"start\":54300},{\"end\":54522,\"start\":54517},{\"end\":54528,\"start\":54526},{\"end\":54538,\"start\":54534},{\"end\":54976,\"start\":54970},{\"end\":54985,\"start\":54980},{\"end\":54996,\"start\":54989},{\"end\":55008,\"start\":55000},{\"end\":55021,\"start\":55015},{\"end\":55328,\"start\":55326},{\"end\":55339,\"start\":55332},{\"end\":55349,\"start\":55343},{\"end\":55361,\"start\":55353},{\"end\":55688,\"start\":55685},{\"end\":55696,\"start\":55692},{\"end\":55703,\"start\":55700},{\"end\":55713,\"start\":55709},{\"end\":56007,\"start\":56004},{\"end\":56015,\"start\":56011},{\"end\":56023,\"start\":56019},{\"end\":56030,\"start\":56027},{\"end\":56312,\"start\":56309},{\"end\":56322,\"start\":56316},{\"end\":56331,\"start\":56326},{\"end\":56339,\"start\":56335},{\"end\":56350,\"start\":56346},{\"end\":56679,\"start\":56674},{\"end\":56690,\"start\":56683},{\"end\":56700,\"start\":56694},{\"end\":56713,\"start\":56704},{\"end\":56724,\"start\":56719},{\"end\":56994,\"start\":56989},{\"end\":57005,\"start\":57000},{\"end\":57333,\"start\":57323},{\"end\":57347,\"start\":57339},{\"end\":57358,\"start\":57353},{\"end\":57371,\"start\":57362},{\"end\":57710,\"start\":57706},{\"end\":57719,\"start\":57714},{\"end\":57730,\"start\":57723},{\"end\":57743,\"start\":57734},{\"end\":58121,\"start\":58114},{\"end\":58132,\"start\":58125},{\"end\":58142,\"start\":58136},{\"end\":58155,\"start\":58148},{\"end\":58167,\"start\":58159},{\"end\":58706,\"start\":58699},{\"end\":58723,\"start\":58710},{\"end\":58732,\"start\":58727},{\"end\":58745,\"start\":58739},{\"end\":58757,\"start\":58749},{\"end\":59046,\"start\":59034},{\"end\":59052,\"start\":59050},{\"end\":59063,\"start\":59056},{\"end\":59071,\"start\":59067},{\"end\":59447,\"start\":59439},{\"end\":59455,\"start\":59451},{\"end\":59469,\"start\":59461},{\"end\":59483,\"start\":59473},{\"end\":59953,\"start\":59951},{\"end\":59961,\"start\":59957},{\"end\":59968,\"start\":59965},{\"end\":59979,\"start\":59972},{\"end\":59986,\"start\":59983},{\"end\":60429,\"start\":60422},{\"end\":60441,\"start\":60435},{\"end\":60452,\"start\":60445},{\"end\":60465,\"start\":60456},{\"end\":60478,\"start\":60469},{\"end\":60487,\"start\":60482},{\"end\":60825,\"start\":60815},{\"end\":60833,\"start\":60829},{\"end\":60842,\"start\":60837},{\"end\":60852,\"start\":60846},{\"end\":61246,\"start\":61240},{\"end\":61259,\"start\":61250},{\"end\":61270,\"start\":61263},{\"end\":61281,\"start\":61274},{\"end\":61288,\"start\":61285},{\"end\":61297,\"start\":61292},{\"end\":61309,\"start\":61301},{\"end\":61842,\"start\":61836},{\"end\":61855,\"start\":61846},{\"end\":61862,\"start\":61859},{\"end\":61873,\"start\":61866},{\"end\":61884,\"start\":61877},{\"end\":61893,\"start\":61888},{\"end\":61905,\"start\":61897},{\"end\":62288,\"start\":62284},{\"end\":62295,\"start\":62292},{\"end\":62637,\"start\":62629},{\"end\":62646,\"start\":62641},{\"end\":63019,\"start\":63015},{\"end\":63028,\"start\":63023},{\"end\":63034,\"start\":63032},{\"end\":63040,\"start\":63038},{\"end\":63047,\"start\":63044},{\"end\":63059,\"start\":63054},{\"end\":63279,\"start\":63275},{\"end\":63289,\"start\":63283},{\"end\":63298,\"start\":63293},{\"end\":63677,\"start\":63674},{\"end\":63692,\"start\":63681},{\"end\":63702,\"start\":63696},{\"end\":63712,\"start\":63706},{\"end\":64147,\"start\":64142},{\"end\":64158,\"start\":64151},{\"end\":64171,\"start\":64162},{\"end\":64178,\"start\":64175},{\"end\":64433,\"start\":64430},{\"end\":64441,\"start\":64437},{\"end\":64450,\"start\":64445},{\"end\":64457,\"start\":64454},{\"end\":64464,\"start\":64461},{\"end\":64792,\"start\":64787},{\"end\":64803,\"start\":64799},{\"end\":64814,\"start\":64809},{\"end\":64822,\"start\":64818},{\"end\":65130,\"start\":65119},{\"end\":65141,\"start\":65134},{\"end\":65156,\"start\":65145}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":203705211},\"end\":47299,\"start\":46792},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":908514},\"end\":47566,\"start\":47301},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13438951},\"end\":48000,\"start\":47568},{\"attributes\":{\"doi\":\"arXiv:1812.08008\",\"id\":\"b3\"},\"end\":48333,\"start\":48002},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b4\"},\"end\":48714,\"start\":48335},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2230657},\"end\":48999,\"start\":48716},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":46990252},\"end\":49358,\"start\":49001},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2845053},\"end\":49664,\"start\":49360},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":49300163},\"end\":50165,\"start\":49666},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":34735234},\"end\":50541,\"start\":50167},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14836478},\"end\":50854,\"start\":50543},{\"attributes\":{\"id\":\"b11\"},\"end\":51194,\"start\":50856},{\"attributes\":{\"id\":\"b12\"},\"end\":51396,\"start\":51196},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":50777233},\"end\":51693,\"start\":51398},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52955951},\"end\":52115,\"start\":51695},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52198976},\"end\":52650,\"start\":52117},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":28772744},\"end\":53087,\"start\":52652},{\"attributes\":{\"doi\":\"arXiv:1803.07549\",\"id\":\"b17\"},\"end\":53402,\"start\":53089},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":32389979},\"end\":53794,\"start\":53404},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":12888763},\"end\":54240,\"start\":53796},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b20\"},\"end\":54440,\"start\":54242},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":49301186},\"end\":54895,\"start\":54442},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":17083829},\"end\":55257,\"start\":54897},{\"attributes\":{\"doi\":\"222:1- 222:11\",\"id\":\"b23\",\"matched_paper_id\":52839714},\"end\":55597,\"start\":55259},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15774646},\"end\":55951,\"start\":55599},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3741249},\"end\":56244,\"start\":55953},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":22455555},\"end\":56627,\"start\":56246},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5328073},\"end\":56935,\"start\":56629},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":17868098},\"end\":57225,\"start\":56937},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8005619},\"end\":57661,\"start\":57227},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":17545470},\"end\":58084,\"start\":57663},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":408187},\"end\":58625,\"start\":58086},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":12407735},\"end\":59030,\"start\":58627},{\"attributes\":{\"doi\":\"arXiv:1806.06575\",\"id\":\"b33\"},\"end\":59366,\"start\":59032},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5417293},\"end\":59867,\"start\":59368},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4797810},\"end\":60365,\"start\":59869},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5395254},\"end\":60752,\"start\":60367},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":13636123},\"end\":61143,\"start\":60754},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":666009},\"end\":61735,\"start\":61145},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2482245},\"end\":62245,\"start\":61737},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4564472},\"end\":62599,\"start\":62247},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":6235232},\"end\":62949,\"start\":62601},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4633214},\"end\":63216,\"start\":62951},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":9363077},\"end\":63638,\"start\":63218},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":163946},\"end\":64047,\"start\":63640},{\"attributes\":{\"id\":\"b45\"},\"end\":64326,\"start\":64049},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":1608002},\"end\":64754,\"start\":64328},{\"attributes\":{\"id\":\"b47\"},\"end\":65055,\"start\":64756},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":16190480},\"end\":65467,\"start\":65057}]", "bib_title": "[{\"end\":46839,\"start\":46792},{\"end\":47355,\"start\":47301},{\"end\":47649,\"start\":47568},{\"end\":48740,\"start\":48716},{\"end\":49063,\"start\":49001},{\"end\":49408,\"start\":49360},{\"end\":49721,\"start\":49666},{\"end\":50253,\"start\":50167},{\"end\":50594,\"start\":50543},{\"end\":51469,\"start\":51398},{\"end\":51764,\"start\":51695},{\"end\":52180,\"start\":52117},{\"end\":52694,\"start\":52652},{\"end\":53427,\"start\":53404},{\"end\":53870,\"start\":53796},{\"end\":54513,\"start\":54442},{\"end\":54966,\"start\":54897},{\"end\":55319,\"start\":55259},{\"end\":55681,\"start\":55599},{\"end\":56000,\"start\":55953},{\"end\":56305,\"start\":56246},{\"end\":56670,\"start\":56629},{\"end\":56983,\"start\":56937},{\"end\":57317,\"start\":57227},{\"end\":57702,\"start\":57663},{\"end\":58110,\"start\":58086},{\"end\":58695,\"start\":58627},{\"end\":59435,\"start\":59368},{\"end\":59947,\"start\":59869},{\"end\":60416,\"start\":60367},{\"end\":60811,\"start\":60754},{\"end\":61236,\"start\":61145},{\"end\":61832,\"start\":61737},{\"end\":62280,\"start\":62247},{\"end\":62625,\"start\":62601},{\"end\":63011,\"start\":62951},{\"end\":63271,\"start\":63218},{\"end\":63667,\"start\":63640},{\"end\":64426,\"start\":64328},{\"end\":64783,\"start\":64756},{\"end\":65115,\"start\":65057}]", "bib_author": "[{\"end\":46850,\"start\":46841},{\"end\":46860,\"start\":46850},{\"end\":47366,\"start\":47357},{\"end\":47376,\"start\":47366},{\"end\":47659,\"start\":47651},{\"end\":47671,\"start\":47659},{\"end\":47682,\"start\":47671},{\"end\":47692,\"start\":47682},{\"end\":47702,\"start\":47692},{\"end\":47713,\"start\":47702},{\"end\":48088,\"start\":48081},{\"end\":48099,\"start\":48088},{\"end\":48108,\"start\":48099},{\"end\":48118,\"start\":48108},{\"end\":48128,\"start\":48118},{\"end\":48346,\"start\":48335},{\"end\":48360,\"start\":48346},{\"end\":48370,\"start\":48360},{\"end\":48382,\"start\":48370},{\"end\":48391,\"start\":48382},{\"end\":48397,\"start\":48391},{\"end\":48409,\"start\":48397},{\"end\":48418,\"start\":48409},{\"end\":48426,\"start\":48418},{\"end\":48432,\"start\":48426},{\"end\":48754,\"start\":48742},{\"end\":48767,\"start\":48754},{\"end\":48779,\"start\":48767},{\"end\":49080,\"start\":49065},{\"end\":49091,\"start\":49080},{\"end\":49101,\"start\":49091},{\"end\":49114,\"start\":49101},{\"end\":49126,\"start\":49114},{\"end\":49422,\"start\":49410},{\"end\":49431,\"start\":49422},{\"end\":49733,\"start\":49723},{\"end\":49741,\"start\":49733},{\"end\":49754,\"start\":49741},{\"end\":49763,\"start\":49754},{\"end\":49773,\"start\":49763},{\"end\":49786,\"start\":49773},{\"end\":50269,\"start\":50255},{\"end\":50278,\"start\":50269},{\"end\":50289,\"start\":50278},{\"end\":50610,\"start\":50596},{\"end\":50618,\"start\":50610},{\"end\":50626,\"start\":50618},{\"end\":50637,\"start\":50626},{\"end\":50646,\"start\":50637},{\"end\":50975,\"start\":50964},{\"end\":50985,\"start\":50975},{\"end\":50994,\"start\":50985},{\"end\":51007,\"start\":50994},{\"end\":51016,\"start\":51007},{\"end\":51250,\"start\":51239},{\"end\":51263,\"start\":51250},{\"end\":51484,\"start\":51471},{\"end\":51495,\"start\":51484},{\"end\":51775,\"start\":51766},{\"end\":51781,\"start\":51775},{\"end\":51789,\"start\":51781},{\"end\":51797,\"start\":51789},{\"end\":51805,\"start\":51797},{\"end\":51817,\"start\":51805},{\"end\":51824,\"start\":51817},{\"end\":51830,\"start\":51824},{\"end\":51836,\"start\":51830},{\"end\":52191,\"start\":52182},{\"end\":52199,\"start\":52191},{\"end\":52208,\"start\":52199},{\"end\":52216,\"start\":52208},{\"end\":52226,\"start\":52216},{\"end\":52235,\"start\":52226},{\"end\":52246,\"start\":52235},{\"end\":52252,\"start\":52246},{\"end\":52708,\"start\":52696},{\"end\":52719,\"start\":52708},{\"end\":52731,\"start\":52719},{\"end\":52740,\"start\":52731},{\"end\":53172,\"start\":53160},{\"end\":53184,\"start\":53172},{\"end\":53195,\"start\":53184},{\"end\":53204,\"start\":53195},{\"end\":53437,\"start\":53429},{\"end\":53447,\"start\":53437},{\"end\":53457,\"start\":53447},{\"end\":53883,\"start\":53872},{\"end\":53893,\"start\":53883},{\"end\":53904,\"start\":53893},{\"end\":54298,\"start\":54286},{\"end\":54304,\"start\":54298},{\"end\":54524,\"start\":54515},{\"end\":54530,\"start\":54524},{\"end\":54540,\"start\":54530},{\"end\":54978,\"start\":54968},{\"end\":54987,\"start\":54978},{\"end\":54998,\"start\":54987},{\"end\":55010,\"start\":54998},{\"end\":55023,\"start\":55010},{\"end\":55330,\"start\":55321},{\"end\":55341,\"start\":55330},{\"end\":55351,\"start\":55341},{\"end\":55363,\"start\":55351},{\"end\":55690,\"start\":55683},{\"end\":55698,\"start\":55690},{\"end\":55705,\"start\":55698},{\"end\":55715,\"start\":55705},{\"end\":56009,\"start\":56002},{\"end\":56017,\"start\":56009},{\"end\":56025,\"start\":56017},{\"end\":56032,\"start\":56025},{\"end\":56314,\"start\":56307},{\"end\":56324,\"start\":56314},{\"end\":56333,\"start\":56324},{\"end\":56341,\"start\":56333},{\"end\":56352,\"start\":56341},{\"end\":56681,\"start\":56672},{\"end\":56692,\"start\":56681},{\"end\":56702,\"start\":56692},{\"end\":56715,\"start\":56702},{\"end\":56726,\"start\":56715},{\"end\":56996,\"start\":56985},{\"end\":57007,\"start\":56996},{\"end\":57335,\"start\":57319},{\"end\":57349,\"start\":57335},{\"end\":57360,\"start\":57349},{\"end\":57373,\"start\":57360},{\"end\":57712,\"start\":57704},{\"end\":57721,\"start\":57712},{\"end\":57732,\"start\":57721},{\"end\":57745,\"start\":57732},{\"end\":58123,\"start\":58112},{\"end\":58134,\"start\":58123},{\"end\":58144,\"start\":58134},{\"end\":58157,\"start\":58144},{\"end\":58169,\"start\":58157},{\"end\":58708,\"start\":58697},{\"end\":58725,\"start\":58708},{\"end\":58734,\"start\":58725},{\"end\":58747,\"start\":58734},{\"end\":58759,\"start\":58747},{\"end\":59048,\"start\":59032},{\"end\":59054,\"start\":59048},{\"end\":59065,\"start\":59054},{\"end\":59073,\"start\":59065},{\"end\":59449,\"start\":59437},{\"end\":59457,\"start\":59449},{\"end\":59471,\"start\":59457},{\"end\":59485,\"start\":59471},{\"end\":59955,\"start\":59949},{\"end\":59963,\"start\":59955},{\"end\":59970,\"start\":59963},{\"end\":59981,\"start\":59970},{\"end\":59988,\"start\":59981},{\"end\":60431,\"start\":60418},{\"end\":60443,\"start\":60431},{\"end\":60454,\"start\":60443},{\"end\":60467,\"start\":60454},{\"end\":60480,\"start\":60467},{\"end\":60489,\"start\":60480},{\"end\":60827,\"start\":60813},{\"end\":60835,\"start\":60827},{\"end\":60844,\"start\":60835},{\"end\":60854,\"start\":60844},{\"end\":61248,\"start\":61238},{\"end\":61261,\"start\":61248},{\"end\":61272,\"start\":61261},{\"end\":61283,\"start\":61272},{\"end\":61290,\"start\":61283},{\"end\":61299,\"start\":61290},{\"end\":61311,\"start\":61299},{\"end\":61844,\"start\":61834},{\"end\":61857,\"start\":61844},{\"end\":61864,\"start\":61857},{\"end\":61875,\"start\":61864},{\"end\":61886,\"start\":61875},{\"end\":61895,\"start\":61886},{\"end\":61907,\"start\":61895},{\"end\":62290,\"start\":62282},{\"end\":62297,\"start\":62290},{\"end\":62639,\"start\":62627},{\"end\":62648,\"start\":62639},{\"end\":63021,\"start\":63013},{\"end\":63030,\"start\":63021},{\"end\":63036,\"start\":63030},{\"end\":63042,\"start\":63036},{\"end\":63049,\"start\":63042},{\"end\":63061,\"start\":63049},{\"end\":63281,\"start\":63273},{\"end\":63291,\"start\":63281},{\"end\":63300,\"start\":63291},{\"end\":63679,\"start\":63669},{\"end\":63694,\"start\":63679},{\"end\":63704,\"start\":63694},{\"end\":63714,\"start\":63704},{\"end\":64149,\"start\":64140},{\"end\":64160,\"start\":64149},{\"end\":64173,\"start\":64160},{\"end\":64180,\"start\":64173},{\"end\":64435,\"start\":64428},{\"end\":64443,\"start\":64435},{\"end\":64452,\"start\":64443},{\"end\":64459,\"start\":64452},{\"end\":64466,\"start\":64459},{\"end\":64794,\"start\":64785},{\"end\":64805,\"start\":64794},{\"end\":64816,\"start\":64805},{\"end\":64824,\"start\":64816},{\"end\":65132,\"start\":65117},{\"end\":65143,\"start\":65132},{\"end\":65158,\"start\":65143}]", "bib_venue": "[{\"end\":47025,\"start\":46951},{\"end\":49927,\"start\":49865},{\"end\":52393,\"start\":52331},{\"end\":52881,\"start\":52819},{\"end\":53598,\"start\":53536},{\"end\":54025,\"start\":53973},{\"end\":54681,\"start\":54619},{\"end\":55412,\"start\":55398},{\"end\":57886,\"start\":57824},{\"end\":58334,\"start\":58260},{\"end\":59626,\"start\":59564},{\"end\":60129,\"start\":60067},{\"end\":61452,\"start\":61390},{\"end\":62438,\"start\":62376},{\"end\":62789,\"start\":62727},{\"end\":63441,\"start\":63379},{\"end\":63855,\"start\":63793},{\"end\":46949,\"start\":46860},{\"end\":47393,\"start\":47376},{\"end\":47751,\"start\":47713},{\"end\":48079,\"start\":48002},{\"end\":48487,\"start\":48448},{\"end\":48839,\"start\":48779},{\"end\":49160,\"start\":49126},{\"end\":49493,\"start\":49431},{\"end\":49863,\"start\":49786},{\"end\":50327,\"start\":50289},{\"end\":50680,\"start\":50646},{\"end\":50962,\"start\":50856},{\"end\":51237,\"start\":51196},{\"end\":51535,\"start\":51495},{\"end\":51874,\"start\":51836},{\"end\":52329,\"start\":52252},{\"end\":52817,\"start\":52740},{\"end\":53158,\"start\":53089},{\"end\":53534,\"start\":53457},{\"end\":53971,\"start\":53904},{\"end\":54284,\"start\":54242},{\"end\":54617,\"start\":54540},{\"end\":55057,\"start\":55023},{\"end\":55396,\"start\":55376},{\"end\":55753,\"start\":55715},{\"end\":56070,\"start\":56032},{\"end\":56389,\"start\":56352},{\"end\":56760,\"start\":56726},{\"end\":57045,\"start\":57007},{\"end\":57422,\"start\":57373},{\"end\":57822,\"start\":57745},{\"end\":58258,\"start\":58169},{\"end\":58782,\"start\":58759},{\"end\":59172,\"start\":59089},{\"end\":59562,\"start\":59485},{\"end\":60065,\"start\":59988},{\"end\":60538,\"start\":60489},{\"end\":60925,\"start\":60854},{\"end\":61388,\"start\":61311},{\"end\":61966,\"start\":61907},{\"end\":62374,\"start\":62297},{\"end\":62725,\"start\":62648},{\"end\":63065,\"start\":63061},{\"end\":63377,\"start\":63300},{\"end\":63791,\"start\":63714},{\"end\":64138,\"start\":64049},{\"end\":64515,\"start\":64466},{\"end\":64886,\"start\":64824},{\"end\":65238,\"start\":65158}]"}}}, "year": 2023, "month": 12, "day": 17}