{"id": 238198653, "updated": "2023-10-05 21:45:18.816", "metadata": {"title": "KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D", "authors": "[{\"first\":\"Yiyi\",\"last\":\"Liao\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Andreas\",\"last\":\"Geiger\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2109.13410", "mag": null, "acl": null, "pubmed": "35648872", "pubmedcentral": null, "dblp": "journals/pami/LiaoXG23", "doi": "10.1109/tpami.2022.3179507"}}, "content": {"source": {"pdf_hash": "0314cfdc5fac4f9a9b9195c73dea9e4a1203b471", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2109.13410v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "60d63a72ae4843798eb1df8473fe58542f31693c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0314cfdc5fac4f9a9b9195c73dea9e4a1203b471.txt", "contents": "\nKITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D\n\n\nYiyi Liao \nJun Xie \nAndreas Geiger \nKITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D\n1Index Terms-Point Cloud LabelingSemantic Label TransferScene UnderstandingSelf-DrivingDatasetsPerformance Evaluation !\nFor the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems.\n\nINTRODUCTION\n\nO NE of the pioneering works in computer vision can be traced back to Larry Roberts' \"Blocks World\" in the 1960s [85], which aimed at identifying individual objects and inferring the 3D structure of simple shapes from 2D images. With the goal of understanding a scene from visual cues, computer vision was viewed as a comparably easy first step towards solving higher-level reasoning tasks in robotics at that time (e.g., the MIT copy demo [104]). Albeit being seemingly easy for humans, robustly perceiving geometry and semantics from images proved hard for machines due to the high complexity of real-world environments. Thus, in the 1980s, computer vision and robotics evolved into their own, largely independent research fields. Only recently, the communities have realized that it is impossible to solve one without the other, e.g., in the context of self-driving [47]. Similarly, computer vision's interaction with computer graphics emerged in the 1990s [92] and has gained traction over the last decade, in particular in areas such as neural and image-based rendering [64]. These advances can in turn benefit robotics as simulation will be crucial for training and validating the next generation of robotic systems.\n\nThe converging trend of vision, graphics, and robotics motivates us to create a new dataset, KITTI-360, that addresses tasks at the intersection of these fields with a focus on autonomous driving. While the KITTI dataset [34] has pushed the state-of-the-art in computer vision algorithms forward, it does not contain dense and complete semantic labels. Thus, many interesting interdisciplinary tasks, e.g.,\n\n\u2022 Y. Liao  synthesizing novel view images jointly with semantics or reconstructing large-scale semantic maps, cannot be evaluated on KITTI. Moreover, the captured perspective front images provide only a partial view of the scene and the 3D information provided by the LiDAR sensor is very sparse. The GPS localization of KITTI is reliable but does not reach sub-pixel accuracy when fusing multiple frames. With KITTI-360 we address these shortcomings by providing a new dataset with more comprehensive semantic/instance labels in 2D and 3D, richer 360 \u2022 sensory information (fisheye images and pushbroom laser scans), very accurate and geolocalized vehicle and camera poses, and a series of new challenging benchmarks, see Fig. 1 for an overview.\n\nA key challenge towards building such a dataset is to obtain coherent dense and comprehensive semantics in 2D and 3D. Many existing datasets are annotated in the 2D image domain where pixel-wise labeling requires up to 60 minutes per image for a human annotator [6]. Other datasets [8], [96], [119] are annotated in 3D space while ignoring information in the 2D image domain. A few datasets [18] offer labels in both 2D and 3D. However, annotation is conducted independently, thus duplicating the labeling effort.\n\nIn this paper, we propose an alternative approach that leverages coarse 3D annotations to significantly simplify the dense annotation task and establish coherent labels in both 2D and 3D space. Moreover, this yields a unique instance index for each object in the scene across all 2D video frames. Specifically, we build a WebGL-based annotation tool that allows for annotating both static and dynamic scene elements directly in 3D using simple primitives. This approach has several advantages over labeling in 2D: First, objects often project into several video frames, thus lowering annotation efforts considerably. Further, the obtained 2D instance annotations are temporally coherent as they are associated with a single physical 3D object. Finally, our 3D annotations Semantic Instance Bounding Box Confidence & RGB Fig. 1: KITTI-360. Our dataset contains rich sensor modalities, including a perspective stereo camera, a pair of fisheye cameras, a Velodyne and a SICK laser scanning unit which together enable 360 \u2022 scene perception. We release comprehensive annotations including consistent semantic and instance labels for every 2D image pixel and 3D point.\n\ncovering the full 3D scene are useful on their own, e.g., for reasoning in 3D [35], [114] or to enrich 2D annotations with approximate 3D geometry. However, obtaining dense and accurate pixel-wise 2D labels and point-wise 3D labels from sparse, noisy point clouds and coarse 3D annotations is a challenging task. Towards solving this problem, we propose a non-local multifield CRF model which reasons jointly about semantic and instance labels of all 3D points and 2D pixels. Our approach also leverages learning-based methods to provide dense semantic and instance priors in the 2D image domain. As evidenced by our experiments, our method outperforms label propagation methods operating purely in 2D as well as pure learning-based approaches. Furthermore, the probabilistic nature of our model allows for estimating label uncertainties which can be used to increase label accuracy when only a subset of the pixels require a label.\n\nFrom the annotated dataset, we derive several benchmarks and baselines with novel and challenging tasks at the intersection of vision, graphics and robotics which we believe are crucial for making progress towards the grand challenge of fully autonomous driving. Our semantic scene understanding benchmark includes tasks for 2D/3D recognition and semantic scene completion. The former requires predicting a semantic/instance label for the visible part of the scene, while the latter aims for joint geometric completion and semantic perception that can benefit higherlevel reasoning, e.g., control and planning. In our novel view synthesis benchmark, we establish a challenging task that requires synthesis of both RGB appearance and semantic labels at a given novel viewpoint, aiming to foster research on building fully labeled simulation environments from real-world images. Lastly, our semantic SLAM benchmark evaluates vehicle localization as well as geometric and semantic 3D reconstruction over long sequences.\n\nWe summarize the contributions of this paper as follows: \u2022 We present a novel georegistered dataset of suburban scenes recorded by a moving platform. The dataset comprises over 300k images and 80k laser scans. \u2022 We create and release a WebGL-based annotation tool that allows for labeling street scenes in 3D space. Exploiting our annotation tool, we obtain 3D annotations for all static and dynamic scene elements. \u2022 We propose a method which transfers these labels from 3D into 2D, yielding pixel-wise semantic instance annotations. We validate our approach in ablation studies and demonstrate its potential with respect to several 2D and 3D baselines.\n\n\u2022 Enabled by our dense and coherent semantic instance annotations in both 2D and 3D as well as accurate vehicle and camera poses, we establish an online benchmark with novel and challenging tasks at the intersection of computer vision, graphics and robotics. We believe that our dataset and benchmarks will complement existing datasets and foster novel research towards solving the grand goal of full autonomy.\n\nThis journal paper is an extension of a conference paper published at CVPR 2016 [107]. In comparison to [107], we 1) extend our annotation tool and update our inference algorithm to support the annotation of dynamic objects; 2) provide a detailed description of the annotation tool and process; 3) establish new online benchmarks with heldout test data on a set of challenging tasks; 4) propose and evaluate several baselines to bootstrap the leaderboards and assess the difficulties of the tasks. We make our dataset 1 , utility scripts 2 and annotation tool 3 publicly available.\n\n\nRELATED WORK\n\nIn this section, we first discuss existing datasets in the context of autonomous driving, followed by a review of current methods for efficient (semi-automatic) label annotation. Dataset 2D Annotations 3D Annotations Coherency Test #Smt. Img. #Ins. Img. Dense #Smt. Pts. #Ins. Pts. FoV Azm. FoV Plr. #3D Bbox Temporal 3D-2D Server CamVid [13] 631 --------DUS [1] 1k --------CityScape (fine) [25] 5k 5k -------CityScape (coarse) [25] 20k 20k --------Mapillary Vistas [70] 25k 25k --------CityScape-VPS [52] 3k 3k -------KITTI-STEP [103] 19k 19k ------WoodScape [111] 10k 10k -------Toronto-3D [96] - [39] - [34] 200 200 ----200k --ApolloScape [45] 144k 90k ----70k --nuScenes [18] 93k  For pixel-level 2D annotations, we compare the number of semantic maps (#Smt. Img.), the number of instance maps (#Ins. Img.) and the density of the 2D semantic maps (Density). Note that the proposed KITTI-360 dataset includes images from both left and right view of the stereo camera. For 3D annotation, we show the number of semantically labeled points (#Smt. Pts.), the number of points with instance labels (#Ins. Pts.), the field of view with 3D semantic annotations at both azimuthal and polar directions (FoV Azm. and FoV Plr.), and the number of 3D bounding boxes (#3D Bbox). We further compare temporal consistency and 3D-to-2D consistency of the instance labels. The last row indicates whether the dataset hosts an online evaluation benchmark with held-out ground truth.\n- - 78.3M - 360 \u2022 40 \u2022 - - - - Paris-Lille-3D [88] - - - 143.1M - 360 \u2022 40 \u2022 - - - DublinCity [119] - - - 260M - - - - - - - Semantic3D.net- - 4.0B - 360 \u2022 180 \u2022 - - - SemanticKITTI [8] - - - 4.5B - 360 \u2022 26.8 \u2022 - - - Argoverse [21] - - - - - - - 993k - - - Lyft [50] - - - - - - - 1.3M - - - Waymo [94] - - - - - - - 12M - - A*3D [75] - - - - - - - 230k - - - KITTI\n\nDatasets\n\nIndoor Video Datasets: Several datasets provide annotations for video sequences captured in indoor scenes [20], [26], [93], [105]. The SUN RGB-D dataset [93] provides labeled 2D polygons as well as 3D cuboids for 10k indoor RGB-D images. In a closely related work, ScanNet [26] is annotated in 3D with its 2D labels directly obtained from 3D-to-2D projection based on the dense depth from RGB-D sensors. In this work, we focus on outdoor street scenes where 3D observations are much more sparse, posing a challenging task for 3D-to-2D label transfer.\n\nOutdoor Datasets: A number of outdoor datasets of driving scenes have been released in the literature [8], [9], [13], [18], [25], [34], [36], [45], [52], [56], [62], [65], [70], [80], [84], [96], [99], [103], [111], [119]. We summarize the most related ones in Table 1, categorized by whether they offer labels in the 2D image domain or in 3D space. For datasets focusing on 2D labels, CamVid [13] is the first dataset for semantic segmentation in the context of self-driving. However, CamVid does not provide instance labels and only a very limited number of frames. Both Cityscapes [25] and Mapillary Vistas [70] release thousands of manually annotated 2D images. However, they do not offer temporally coherent semantic instance annotations. Recently, Cityscape-VPS [52] extends Cityscapes by providing semantic instance labels for every 5 frames. Furthermore, KITTI-STEP [103] offers spatially and temporally dense semantic instance annotations for the KITTI tracking dataset [34]. While aforementioned works focus on perspective images, WoodScape [111] releases semantic instance annotations of fisheye images. Our dataset differs from the above in that we provide not only temporally coherent semantic instance annotations for perspective images, but also omnidirectional imagery, 3D laser scans, and 3D annotations which are useful for 3D reasoning. While [25] focuses on inner-city scenes, our dataset comprises mainly suburban areas, thus both datasets complement each other (we use the same label definition to facilitate research).\n\nAnother line of works provides labels in 3D space. Toronto-3D [96], Paris-Lille-3D [88] and DublinCity [119] offer annotated point clouds collected from urban environments. Semantic3D.net [39] presents a large-scale dataset with 4 billion points, labeled with 8 semantic categories. SemanticKITTI [8] provides semantic labels for raw laser scans in KITTI, resulting in 4.5 billion labeled 3D points in 28 classes. Instead of focusing on point cloud semantic classification, Argoverse [21], Lyft [50], Waymo [94], and A*3D [75] offer 2D/3D bounding boxes and establish benchmarks for 2D/3D detection and tracking 4 . In contrast to KITTI-360, the aforementioned datasets either lack dense annotations in images or they do not have per-point 3D annotations of stuff classes.\n\nOur dataset provides labels for both 2D images and corresponding 3D points. Within this category, KITTI [34] provides dense semantic information on 200 images and 200k 3D bounding boxes. However, KITTI does not provide dense (per-point) 3D labels on the point cloud. Closely related to our work, ApolloScape [45] annotates static scene elements in the 3D space 5 and projects them to the 2D image space, followed by manual annotation of dynamic objects in 4. We refer to 2D annotations as pixel-level annotations in Table 1. 2D bounding boxes are not included.\n\n5. The 3D annotation has not been released yet.\n\nimages. In this work, we annotate both static and dynamic objects in 3D, providing coherent annotations for dynamic objects both in 2D and 3D. More recently, nuScenes [18] and A2D2 [36] released labels in both 2D and 3D. However, the labels of nuScenes are manually and independently annotated in 2D and 3D, and not every pixel is labeled in 2D.\n\nIn contrast, we propose to leverage labels in the 3D space to infer dense labels in the image domain, thus providing consistent labels across 2D and 3D space. A2D2 [36] labels 2D images and maps 2D labels to 3D to obtain per-point 3D labels. Thus, the 3D labels are limited to a small FoV of the cameras in the azimuthal direction. While A2D2 also provides 3D bounding boxes, all of them are within the FoV of the forward-facing camera. We instead offer per-point 3D labels and 3D bounding boxes within an azimuthal FoV of 360 \u2022 . A concurrent work, SemKITTI-DVPS [80] provides labels in both 2D and 3D by projecting the 3D labels of SemanticKITTI to images. Compared to the projected sparse 2D labels of SemKITTI-DVPS, KITTI-360 offers dense pixelwise labels and additionally provides 3D bounding boxes. There also exist several synthetic urban datasets [17], [32], [82], [86]. However, there still exists a significant perceptual gap between the virtual and real domains [43], [97], making synthetic-to-real generalization difficult.\n\n\nBenchmarks:\n\nRecently, evaluation benchmarks have been widely recognized by the community. Some of the previously mentioned datasets also provide online evaluation benchmarks and held-out test data for different tasks. For instance, Cityscapes [25] offers a benchmark suite for pixel and instance-level semantic segmentation as well as 3D vehicle detection. SemanticKITTI [4], [8] hosts lidar segmentation challenges to predict the category of every point. For datasets including both 2D and 3D annotations, KITTI [33], nuScenes [18], and ApolloScape [45] provide benchmarks on a set of vision tasks including detection, stereo, localization, multi-object tracking, and segmentation in both 2D and 3D, etc. Moving beyond the established tasks, KITTI-360 provides novel benchmarks and will hold new challenges, e.g., on novel view semantic synthesis and semantic SLAM, to foster new progress towards full autonomy.\n\n\nMethods\n\nEfficient Annotation: Many works have attempted to reduce the per pixel annotation time of individual images, including classical methods [38], [60] and learning based methods [2], [3], [19], [59]. While all of these methods focus on annotating images individually, we are interested in annotating 2D video sequences as well as 3D scenes. There is also a growing interest in autolabeling 3D shapes or 3D bounding boxes [79], [112]. These methods are only applicable to a specific class, e.g., vehicles. We instead annotate the full 3D scene and aim to obtain coherent per-pixel 2D annotations and per-point 3D annotations.\n\n2D Label Propagation: Compared to annotating individual images, video sequences offer the advantage of temporal coherence between adjacent frames. Label propagation techniques exploit this fact by transferring labels from a sparse set of annotated keyframes to all unlabeled frames based on color and motion information. While in some works a single foreground object is assumed [46], here we focus on methods that can handle multiple object categories. Towards this goal, [6] and [15] propose a coupled Bayesian network based on video epitomes and semantic regions to propagate label information between two annotated keyframes. [118] proposes a joint propagation strategy with synthesized training samples. To better account for errors in label propagation, [68] proposes a hierarchy of local classifiers for this task and [5] leverages a mixture-of-tree model for temporal association. The work of [16] leverages label propagation as a data augmentation scheme and demonstrate improved performance on semantic segmentation. Optical flow is also commonly used for semantic video label transfer. [31] uses optical flow of adjacent frames to warp network representations across time and thus propagates labels from previous frames to the current one. [117] proposes to run a convolutional sub-network only on sparse keyframes and propagate the deep feature maps to other frames via flow fields. In the indoor scenario where dense geometry is available, [81] proposes a method on RGB-D video propagating labels on super-pixel.\n\nIn contrast to the aforementioned methods which propagate labels in 2D, in this paper we propose to annotate both semantic and instance labels directly in 3D and then project these annotations into the 2D domain. While this approach requires a source of 3D information (e.g., SfM, stereo, laser), it is able to produce more accurate semantic and temporally consistent instance annotations for tracking purposes. Further, our experiments indicate that annotation in 3D is more time-efficient than labeling in 2D as scene elements can be separated more easily and often project into many images of the input video sequence while being only annotated once.\n\n\n3D-to-2D Label Propagation:\n\nThere are a few existing works on 3D-to-2D label transfer. Chen et al. [22] leverage annotations from KITTI [33] as well as 3D car models to infer separate figure-ground segmentation for all vehicles in the image. In comparison, our approach reasons jointly about all objects in the scene and also handles categories for which CAD models or 3D point measurements are unavailable (e.g., \"Tree\", \"Sky\"). Huang et al. [45] also applies 3D to 2D label transfer for generating the ApolloScape dataset. In this work, labels are transferred from 3D point clouds to images with simple splatting and projection. However, 3D points are too sparse compared with image pixels, thus, setting the splatting range is not trivial. Similarly, in [26], semantic labels annotated in the reconstructed scene are projected into each frame but not all 2D pixels are covered due to missing geometry. In addition, the two aforementioned works are limited to static scenes.\n\nIn the context of street view image segmentation, [14], [63], [65], [67], [69], [106] exploit the interaction between image pixels and 3D points to improve classification performance or efficiency. In comparison, our goal is to transfer ambiguous 3D primitive labels to every pixel in the image.\n\n\nANNOTATION\n\nIn this section, we describe our data collection efforts, data preprocessing, the annotation tool, and annotation details. \n\n\nData Collection\n\nFor data collection, we equipped a station wagon with one 180 \u2022 fisheye camera to each side and a 90 \u2022 perspective stereo camera (baseline 60 cm) to the front. Furthermore, we mounted a Velodyne HDL-64E and a SICK LMS 200 laser scanning unit in pushbroom configuration on top of the roof. This setup is similar to the one used in KITTI [33], [34], except that we gain a wider field of view with the additional fisheye cameras and the pushbroom laser scanner while KITTI only provides perspective images and Velodyne laser scans with a 26.8 \u2022 vertical field of view. Compared to omnidirectional camera systems [90], [91], our setup benefits from increased resolution of the 3D reconstruction. Localization is provided by IMU and GPS which we fuse with visual features. Fig. 1 (top left) illustrates our setup.\n\nUsing this setup, we recorded several suburbs of a midsize city corresponding to over 300k images and 80k laser scans, covering a driving distance of 73.7km. We estimate all vehicle and camera poses using structure-from-motion [41]. More specifically, we minimize 3D reprojection errors based on all feature matches while regularizing against the GPS location. We further add loop-closures detected from LiDAR scans as regularization to complement image feature matching (which might fail on opposite-facing frames). This results in accurate georegistered camera poses. Fig. 2 illustrates the camera poses overlaid on OpenStreetMap 6 . We also plot the camera poses of the KITTI dataset [33], [34] for reference. KITTI-360 follows KITTI's forward facing camera configuration, but has minimal overlap with KITTI in terms of trajectories. This allows us to split training and test data without conflicting with the KITTI dataset, e.g., avoiding 6. http://www.openstreetmap.org/ the situation where a region is used for training in KITTI but testing in KITTI-360. Following KITTI, we use Mercator projection [73] to convert geographic coordinates to a local Euclidean coordinate frame in order to facilitate usage of the dataset. The origin of the coordinate frame is chosen as the center of the map as illustrated in Fig. 2.\n\n\nAnnotation Interface\n\nTo facilitate 3D annotation, we developed an online annotation tool based on WebGL. We release our annotation tool (see Fig. 3) as part of this project. It consists of three main components: a scene viewer (including 2D images and 3D scene), a semantic label selection panel, and controllers. Annotators are asked to insert 3D primitives with adjustable shapes and semantic labels into the 3D scene.\n\n\nScene\n\nTo annotate the data while limiting transfer bandwidth, we split the collected data into batches according to the accumulated driving distances. Specifically, a single batch contains observations within a driving distance of about 200 meters (240 frames on average) and there is an overlap of 10 meters between two consecutive batches. Within one batch, we accumulate 3D points observed from the Velodyne and SICK laser scanning unit as well as the stereo camera.\n\nDuring annotation, the accumulated point clouds are downsampled to reduce data loading traffic and memory. However, downsampling makes it hard to precisely perceive dynamic objects whose 3D observations are distributed along a moving trajectory. To allow for accurate labeling of dynamic objects, we apply a simple heuristic to detect dynamic objects, see Appendix A.2. We then load all detected dynamic points at each frame into the annotation tool without down-sampling. To help the annotators efficiently identifying dynamic objects, we highlight dynamic objects using white color as illustrated in Fig. 3.\n\nAs auxiliary visualization to the 3D point clouds, we provide fisheye and perspective images (see \"Side View\" and \"Front View\" in Fig. 3) in order to allow annotators to select and perceive the scene from different camera views. We also visualize the pose of each camera, enabling annotators to quickly select informative viewpoints.\n\n\nSemantic Label Panel and Controllers\n\nWe show semantic labels with different colors in the label panel for users to choose from. To better assist annotators in placing the primitives accurately, we also offer easy-to-use controllers to interact with the 3D scene, including zoom, pan, rotation of the point cloud, switching data sources or camera views, and toggling annotations. We provide more details about the annotation interface in Appendix B.\n\n\nAnnotation Details\n\nWe ask the annotators to annotate the 3D point clouds in the form of bounding primitives, i.e., place cuboids and ellipsoids to enclose objects in 3D and assign a semantic label to each of them. The 3D scene is annotated with 37 label classes, including 24 \"instance\" classes and 13 \"stuff\" classes. Labels are defined in accordance with the Cityscapes dataset [25] label definition. More details about the label definition can be found in Appendix C. The annotations are categorized into static and dynamic objects, which are treated differently by our annotation tool.\n\n\nStatic Objects Annotation\n\nStatic labels can be further classified into two categories: \"stuff\" and \"instance\". For instance classes, each object is constrained to be associated with only one cuboid primitive, representing both semantic and instance labels of this object. We ask the annotators to tightly enclose the point clouds with the bounding primitives. For stuff classes, which usually have irregular shapes, annotators are allowed to use multiple cuboids or ellipsoids to roughly enclose the 3D points of the target objects.\n\nWe also provide a \"planar\" annotation option for stuff categories on the ground such as \"Road\" and \"Sidewalk\". Using this option, we allow annotators to draw a 2D polygon representing the ground object's boundary in bird's eye view. The interface then automatically estimates the height of the polygon based on the surrounding 3D geometry and extrudes the 2D polygon into 3D along the vertical direction to enclose corresponding 3D ground points. We provide more details in Appendix B.3.\n\n\nDynamic Objects Annotation\n\nDynamic objects mainly comprise moving vehicle and pedestrian instances. In contrast to ApolloScape [45] which annotates static objects in 3D and dynamic objects in 2D respectively, we annotate both static and dynamic objects in 3D space. However, compared to static objects, annotating dynamic objects in 3D outdoor scenes is more challenging as individual dynamic objects in the 3D reconstruction are hard to perceive and distinguish. Moreover, we need to label not only where the moving instance is, but also \"when\" the instance appears, requiring the annotation of moving 3D bounding boxes over time. A na\u00efve solution is to place a 3D bounding box in every frame where the dynamic object is present. However, such an annotation process would be intractably slow. Thus, we instead implement a semi-automatic annotation scheme to reduce label time. Specifically, we minimize the effort required by annotators by making two assumptions: the size of the dynamic object is fixed over time and its trajectory is smooth. Under these assumptions, the required annotation is reduced to the size of a single 3D primitive and the pose of this primitive at several keyframes. Our annotation tool then automatically places the remaining primitives along the trajectory, see Appendix B.4 for more details.\n\n\nAnnotation Procedure\n\nWe annotated 379 batches in total, assigning one batch to one annotator. To control the annotation quality, we train and evaluate the annotators based on multiple pilot tasks until they have proven qualified for the full task. We also regularly verify their annotation quality and ask them for correction if necessary. We further identify a few annotators who consistently produced high-quality labels and ask them to cross-check other annotators' quality. Our annotation interface simplifies the detection and correction of annotation errors compared to annotating image sequences, which requires corrections across multiple frames. Fig. 3 shows parts of an annotated batch via our web interface.\n\n\nAnnotation Time\n\nOn average, annotating one full batch (\u223c 240 frames) in 3D required about 3 hours. Thus, our annotators spend only 3 \u00d7 60 / 240 = 0.75 minutes for \"annotating\" one image. In comparison, 7 minutes are required for coarse annotation of semantic instance labels in the image domain, and 1.5 hours for pixel-accurate annotations as discussed by the creators of the Cityscapes dataset [25].\n\n\nLABEL TRANSFER METHOD\n\nIn this section, we first provide an overview of our method for transferring the 3D annotation to semantic instance annotations in 2D. Next, we formally introduce the model and discuss parameter learning and inference.\n\n\nOverview\n\nGiven 3D annotations, we are interested in generating dense semantic instance annotations for all images and all 3D points. To incorporate inductive biases about image formation and label smoothness, we explore a Conditional Random Field (CRF) model which reasons jointly about the labels of the 3D points and all pixels in the image. In practice, we apply the CRF at every timestamp independently to keep inference tractable. Despite independent inference, we are able to obtain consistent results over multiple frames thanks to the shared 3D annotations. We also experimented with inference over multiple adjacent frames but did not observe measurable improvements.\n\nLet B t = {{b n }, {b m t }} denote all 3D annotations available at timestamp t. Here, b n and b m t correspond to 3D bounding primitives of static and dynamic objects respectively, with n and m indexing each primitive. Note that a static primitive b n is used at all timestamps (if visible) whereas a dynamic primitive b m t is only included in B t when it is labeled to appear at timestamp t. Fig. 4a illustrates static and dynamic bounding primitives as well as their projection into the 2D image domain. With this design, our framework allows for annotating the same object using a unique instance ID across the entire sequence as well as across 2D and 3D.\n\nLet P t denote the set of image pixels at timestamp t and L t denote the visible 3D points at the same timestamp. The CRF model is defined over all elements in P t and L t . To obtain more complete 3D information, L t fuses stereo and laser scans over multiple frames. We first fuse points covering static parts of the scene, and then accumulate points of each dynamic object according to its bounding primitives and insert them into the static scene depending on the location of b m t . We provide more details regarding the accumulation of static and dynamic 3D points in Appendix D.1.\n\n\nModel\n\nWe now formalize the CRF model applied at every frame as illustrated in Fig. 4b. Note that our 3D annotations are sparse and noisy, i.e., 3D points can carry none, one or multiple labels due to overlapping bounding primitives in 3D. The algorithm described in this section is designed to resolve these situations and infers marginal estimates for all 3D points and pixels in the image.\n\nAs the CRF model is applied at every frame independently, we drop the dependency on timestamp t of B, L and P for simplicity. For each pixel i \u2208 P and each 3D point l \u2208 L, we specify random variables s i and s l taking values from the set of semantic (or instance) labels {1, . . . , S}, where S denotes the number of classes. For instance inference, we assign a unique ID to each object which projects into the image. Thus, semantic and instance inference can be treated equally under our model and we will refer to both as \"semantic labels\" in the following. Note that there is no need to distinguish static or dynamic objects in the single frame-based CRF model. Still, we are able to retrieve whether a pixel or a 3D point belongs to a dynamic object or not according to its instance ID.\n\nLet s = {s i |i \u2208 P} \u222a {s l |l \u2208 L} denote the set of semantic labels. Dropping all dependencies on the image and point cloud for clarity we specify our CRF in terms of the following Gibbs energy function:\nE(s) = i\u2208P \u03d5 P i (s i ) + l\u2208L \u03d5 L l (s l ) + i,j\u2208P \u03c8 P,P ij (s i , s j ) + l,k\u2208L \u03c8 L,L lk (s l , s k ) + i\u2208P,l\u2208L \u03c8 P,L il (s i , s l )(1)\nwith unary potentials \u03d5(\u00b7) and pairwise potentials \u03c8(\u00b7). For notational clarity, we omit all conditional dependencies on the input images, 3D points and 3D annotations. Pixel Unary Potentials: The pixel unary potentials \u03d5 P i (s i ) encode the likelihood of pixel i taking label s i\n\u03d5 P i (s i ) = w P 1 (s i ) \u03be P i (s i ) \u2212 w P 2 (s i ) log p P i (s i )(2)\nwhere w P 1 and w P 2 denote learned feature weights. Our first constraint \u03be P i (s i ) determines the set of admissible labels and is obtained by projecting all 3D bounding primitives B (which are an upper bound on the objects' extent) into the image. We formulate the constraint via a binary feature \u03be P i (s i ) \u2208 {0, 1} which takes 0 for pixel i if its ray passes through a primitive of class s i , and 1 otherwise. In addition, we exploit a data-driven approach in order to obtain a per-pixel probability distribution over semantic labels p P i (s i ). Specifically, we project all non-occluded and uniquely labeled sparse 3D points into the image plane, and use these sparse projections as supervision to train a semantic segmentation network (PSPNet [116]) on the entire dataset. The output of the network's last layer is taken as the probability distribution. We also augment the training dataset using Cityscape images and labels [25] to enable the model to learn accurate object boundaries which is difficult to learn based on the projection of sparse and noisy LiDAR point clouds. As the semantic segmentation model does not distinguish instances, we further adopt a state-of-theart instance segmentation method [108] to obtain instance hypotheses for \"car\", \"truck\", and \"pedestrian\". Thus, we effectively exploit the inductive biases of modern neural network architectures and co-training on related labeled datasets. As demonstrated in Appendix D.4, this leads to a significant improvement at object boundaries. 3D Point Unary Potentials: The 3D point unary potentials \u03d5 L l (s l ) encode the likelihood of 3D point l taking label s l :\n\u03d5 L l (s l ) = \u2212w L (s l ) \u03be L l (s l )(3)\nwhere \u03be L l (s l ) denotes a feature which takes 0 if the 3D point l lies within a 3D primitive of class s l within B, and 1 otherwise. As the \"sky\" class can't be modeled with primitives, we set \u03be L l (s l ) to 0 if s l takes the label \"sky\". Additionally, we create \"virtual sky points\" at infinity for all pixels whose ray doesn't intersect any 3D primitive. Note that these pixels must correspond to sky regions as we assume that the scene is densely annotated, hence each object is contained in one or several bounding 3D primitive(s). Pixel Pairwise Potentials: Our dense pairwise term encourages semantic label coherence and connects all pixels in the image via Gaussian edge kernels following [55] \n\u03c8 P,P ij (s i , s j ) = w P,P 1 (s i , s j ) exp \u2212 p i \u2212 p j 2 2 \u03b8 P,P 1 + w P,P 2 (s i , s j ) exp \u2212 p i \u2212 p j 2 2 \u03b8 P,P 2 \u2212 c i \u2212 c j 2 2 \u03b8 P,P 3(4)\nwhere p i is the 2D location of pixel i and c i denotes its color value. Further, w P,P 1 and w P,P 2 are learned pairwise feature weights and \u03b8 P,P parameterizes the kernel width. 3D Pairwise Potentials: Similarly, we apply a Gaussian edge kernel to encourage label consistency between 3D points based on their 3D location and surface normals\n\u03c8 L,L lk (s l , s k ) = w L,L (s l , s k ) (5) \u00d7 exp \u2212 p 3d l \u2212 p 3d k 2 2 \u03b8 L,L 1 \u2212 (n l \u2212 n k ) 2 2 \u03b8 L,L 2\nwhere p 3d l is the 3D location of point l and n l denotes the vertical (up) component of its normal. We use the normal's z-component as it is the most discriminative cue for label changes between horizontal (e.g., road, sidewalk) and vertical (e.g., side of car, wall) surfaces. We estimate the respective normals using principal component analysis in a local neighborhood around each 3D point. 2D/3D Pairwise Potentials: Finally, we encourage coherence between all 3D points and the image pixels\n\u03c8 P,L il (s i , s l ) = w P,L (s i , s l ) exp \u2212 p i \u2212 \u03c0 l 2 2 \u03b8 P,L(6)\nwhere \u03c0 l denotes the projection of the 3D laser or stereo point l onto the image plane. Importantly, we project only points into the image which are likely to be visible. We determine these points by meshing the 3D point cloud using the ball-pivoting method of Bernardini et al. [10], and considering only 3D points in front of the mesh. We also experimented with multi-view reconstruction approaches [48] for mesh generation, but obtained better results using this simpler approach. As applying the meshing algorithm independently for every frame is time-consuming, we generate meshes on entire batches, processing the static part and dynamic objects independently. This allows us to reuse the mesh of the static part for all frames of a batch.\n\n\nLearning and Inference\n\nThis section describes inference and parameter estimation in our label transfer model. Inference: At test time, we are interested in estimating the marginal distribution of each semantic or instance label in s under our model, specified by the Gibbs distribution defined in Eq. 1. A likely configuration can then be estimated by variable-wise maximization of these marginals. As our graphical model is loopy, exact inference in polynomial time is intractable. We thus resort to variational inference and approximate the probability distribution on s by replacing it with a factorized mean field distribution Q(s) = i\u2208P\u222aL Q i (s i ). This mean field approximation can be computed efficiently using bilateral filtering [55]. As our model comprises three sets of densely connected variables (namely P, L and P \u2194 L), we exploit the algorithm of [51], [101] which generalizes [55] to multiple fields. Fig. 4c illustrates the inference result for a single frame, overlaid on the corresponding input image. Moreover, we obtain an uncertainty estimate for each pixel/3D point by computing the entropy over the respective marginal distribution. We will use this estimate in Section 6 to weigh the evaluation metrics according to the confidence of our label estimates. Learning: We employ empirical risk minimization in order to learn the parameters in our model, considering the univariate logistic loss, defined as \u2206(s) = \u2212 log (P (s)) where P (\u00b7) denotes the marginal distribution at the respective site. Let us subsume all model parameters into\n\u0398 = {w P 1 , w P 2 , w L , w P,P 1 , w P,P 2\n, w P,L , w L,L }. We define our minimization objective f (\u0398) as the regularized univariate logistic loss:\nf (\u0398) = N n=1 i\u2208P \u2212 log Q n,i (s * n,i ) + \u03bb C(\u0398)(7)\nHere, N is the number of training images, s * n,i denotes the ground truth semantic label and Q n,i (\u00b7) the approximate marginal at pixel i in image n, calculated via mean field approximation. C(\u0398) is a quadratic regularizer on the parameter vector \u0398. We whiten all features and use a single value \u03bb which we select via cross-validation on the training set. For learning the instance segmentation parameters we exploit the same loss f (\u0398) as for semantic segmentation, but assign unique labels to each individual object, e.g., different cars will be assigned different labels even if they occlude each other. In order to associate 2D ground truth instances with 3D instances we project all visible 3D points into the image and find a consensus via the majority vote which gave good results in practice. As the number of instances per semantic class varies between images, we learn intraand inter-class pairwise potentials using parameter tying. We optimize the objective function f (\u0398) using stochastic gradient descent and obtain \u2202Q/\u2202\u0398 using auto differentiation. We make use of the ADADELTA algorithm [113] with decay parameter 0.95 and = 10 \u22128 , and randomly sample a batch of 16 training images at each iteration for which all gradients can be computed in parallel.\n\n\nLABEL TRANSFER EVALUATION\n\nIn this section, we first introduce the datasets we use for training and evaluating our label transfer method. Next, we evaluate our method in ablation studies and compare it against several label transfer baselines. Finally, we also show qualitative results of our method.\n\n\nTraining and Evaluation Data\n\nWe manually annotate a set of images with pixel-wise ground truth to train and evaluate our label transfer method. The training set contains 125 images selected from diverse scenarios such that a substantial amount of pixels are labeled within each class. These training images are different from those used in our conference version [107]. We create this new training set following the label definition of CityScapes [25] as [107] considers fewer classes. To enable comparison to 2D label transfer methods which require images with large overlapping regions, we additionally annotate 240 adjacent frames from 13 different suburbs in equidistant steps of 5 frames in the 2D image domain for evaluation. The evaluation set has no spatial overlapping with the training set, allowing us to assess the generalization ability of our method. We evaluate our label transfer method on static and dynamic objects separately. Following [107], the performance of static objects is evaluated on 120 densely labeled frames from 5 suburbs containing the most frequently occurring 14 classes. The remaining 120 frames are sampled from 8 different suburbs which contain dynamic objects. For these frames we label the dynamic objects while leaving the static region unannotated. We consider 7 common dynamic objects, see Appendix E.1 for details.\n\n\nQuantitative Evaluation\n\nThis section presents our quantitative evaluation on semantic and instance segmentation. We compare our method with several label transfer baselines and conduct ablation studies.\n\n\nSemantic Segmentation Transfer\n\nFor evaluating semantic segmentation transfer performance, we measure overall performance by the mean intersection over union (mIoU) and the average pixel accuracy (Acc). While [107] evaluates the weighted mean IoU which is biased by object occurrences, we follow Cityscapes [25] and measure the mean IoU without weighting. For all experiments, we provide results for individual classes in Appendix E.1.\n\n\nBaselines:\n\nWe compare our method to several 2D to 2D label transfer methods on both static and dynamic objects  We compare our method to 2D label transfer baselines (top) and to 3D to 2D label transfer baselines (bottom). Label source: \"2D\": Labeled neighboring image frames, \"CS\": Cityscapes training images, \"3D\": 3D bounding primitives.\n\nin Table 2. Here, the task is to predict the center frame from two annotated images (\u00b15 frames corresponding to 0.5 seconds of driving or \u223c 5 meters travel distance). Our first baseline (\"Label Prop.\") is the label transfer approach presented in [100]. To ensure that all baselines have access to the same information, we do not select frames actively but use equidistantly spaced frames for all methods. We construct a second baseline (\"Sparse Track. + GC\") using the feature tracking approach of [95] to propagate semantic labels from the two closest labeled frames to the target frame. To densify the label map, we apply graph cuts (GC) with contrast sensitive edge potentials [11]. In order to evaluate the value of 3D information, we implemented a third baseline (\"3D Prop. + GC\") which works similar to the previous one, but replaces the sparse tracking part with correspondences obtained by transferring pixels of the two closest labeled frames to the target image via the visible vertices of our 3D mesh followed by graph cuts propagation.\n\nWhile all aforementioned baselines require labeled adjacent frames as input at inference time, we consider two more methods that generalize to arbitrary frames. First, we train the segmentation model of Kr\u00e4henb\u00fchl et al. [55] (\"Fully Conn. CRF\") which was also used in [107] and which uses a similar inference algorithm as our label transfer method on all annotated adjacent frames of the test sequence. Finally, we evaluate the deep semantic segmentation network [116] (\"PSPNet\") that also provides dense unary information for our method. As discussed in Section 4.2, this model is trained on non-occluded sparse 3D projections combined with the CityScapes training set [25]. Note that neither PSPNet nor our method has access to adjacent annotated frames for training or inference.\n\nWe further consider several 3D to 2D label transfer baselines that exploit our 3D annotations without requiring equidistantly labeled 2D annotations. Specifically, we project 3D primitives, meshes or visible 3D points into the 2D image domain, followed by graph cut inference (\"3D Primitives + GC\"; \"3D Mesh + GC\"; \"3D Points + GC\").   3: Semantic Instance Segmentation Transfer Ablation evaluated on static objects. The components are abbreviated as follows: LA = local appearance (p P ), PW = 2D pairwise constraints (\u03c8 P,P ), CO = 3D primitive constraints (\u03be P ), 3D = 3D points (\u03d5 L ,\u03c8 P,L ), Full Model = all potentials including 3D pairwise constraints (\u03c8 L,L ). Percentages denote fractions of estimated pixels.\n\n\nStatic Objects:\n\n120 consecutive images of static objects. 7 From the 2D label transfer baselines shown at the top, the mesh transfer method which uses projected 3D information performs best in terms of mIoU. Furthermore, and maybe surprisingly, the sequence-specific fully connected CRF model performs on par or even better than special purpose label transfer methods. This is caused by the fact that optical flow (as used in [95], [100]) often fails for street scenes like ours due to large displacements, perspective distortions, textureless regions and challenging lighting conditions. Interestingly, PSPNet achieves the best accuracy while performing worse on mIoU. Despite obtaining superior results on large objects (e.g., \"Building\"), it struggles with less-occurring classes such as \"Trailer\" and \"Gate\". The bottom half of Table 2 (left) compares the proposed method with respect to the 3D to 2D label transfer baselines. As evidenced by our results, simply projecting 3D primitives or meshes into the image and smoothing via GC does not perform well due to the crude approximation of the geometry. Better results are obtained when projecting the visible 3D points followed by spatial propagation. Finally, we observe that all baselines are outperformed by the proposed method (last row). Note that we also map the 37 semantic labels of our 3D annotations to the most common 14 categories considered in the static evaluation images (see Appendix C) for all 3D to 2D label transfer methods.\n\n\nDynamic Objects:\n\nWe evaluate our method on dynamic objects against 2D label transfer baselines. Here, we consider all static regions as a single background class during evaluation. Note that we neglect \"Fully Conn. CRF\" and \"PSPNet\" as both methods address semantic segmentation and thus cannot distinguish static and dynamic objects within the same class. Table 2 (right) shows that our method also outperforms all 2D label transfer baselines on dynamic objects. While the mIoU is calculated over a different set of classes, the average performance of our method on dynamic objects is slightly degraded compared to our result on static objects. Labeling of dynamic objects is more challenging in our 7. The results differ slightly from those presented in [107] as 1) we updated the ground truth labels to be consistent with the extended label definition and 2) we measure mIoU following Cityscapes [25] while [107] reports weighted mIoU. annotation pipeline for two reasons: Since we accumulate 3D points of dynamic points according to the annotated bounding primitives over multiple frames, slight misalignments of the primitives may lead to inaccurate accumulation and thus erroneous 3D cues. Furthermore, the accumulation of deformable objects (\"Rider\", \"Person\") leads to noisy 3D point clouds. Despite these challenges, our method achieves satisfying performance on all dynamic objects.\n\n\nAnnotation Time Comparison:\n\nWhile all 2D methods require every 10th frame to be labeled, our method (as well as the other 3D baselines) requires 3D annotations in the form of 3D primitives. Assuming 60 minutes annotation time per image, this amounts to 20 hours of annotation time per batch of 200 frames when labeling one 2D image every 10th frame, while the respective 3D annotations for this scene can be obtained in about 3 hours. This gain multiplies with the frame rate and the number of cameras (our setup has four).\n\n\nAblation Study:\n\nWe validate the importance of the individual components of our model on semantic segmentation in Table 3 (upper left), evaluated on the densely labeled images of static objects. Starting with the appearance classifier p P trained on the projected sparse 3D points (\"LA\"), we incrementally add the terms \u03d5 L ,\u03c8 P,L related to the 3D points (\"3D\"), the semantic pairwise term \u03c8 P,P between pixels (\"PW\"), the 3D primitive constraints \u03be P (\"CO\") and finally the 3D pairwise constraints \u03c8 L,L as specified in Eq. 1. We note that each component is able to increase performance. We obtain the largest improvement by reasoning about the relationship between points in 3D and pixels in the image.\n\nLabel Uncertainty: Here, we leverage our model's awareness of label uncertainty to demonstrate that higher accuracy can be achieved in confident regions. To quantify uncertainty, we measure the entropy of the label marginal distribution at every pixel, see Fig. 5 (last row). Sorting all pixels according to their entropy allows us to predict the most certain regions in the image. Table 3 (bottom) shows our results on static objects when predicting only those parts of the image. Note how this helps to boost our performance to 94.3% mIoU and 98.4% accuracy when predicting at 70% pixel density, demonstrating that our uncertainty estimates are well calibrated. In contrast, uncertainty is not directly accessible in most baseline models as they are deterministic or rely on MAP estimates. In the benchmarks introduced in Section 6 where our inferred labels are considered as pseudo-ground truth, we adopt confidence weighted evaluation metrics leveraging the uncertainty to take into account the ambiguity in our automatically generated annotations.\n\n\nInstance Segmentation Transfer\n\nAs time consistent 2D instance ground truth is hard to obtain, most existing 2D label transfer methods focus on the semantic segmentation problem. Therefore, we chose to evaluate instance segmentation performance in an ablation study. We annotated the classes \"Building\", \"Car\", \"Trailer\", \"Caravan\" and \"Box\" with instances in our 2D Fig. 5: Qualitative Results on Semantic Instance Segmentation Transfer. Each subfigure shows from top-to-bottom: the input image with the projected 3D points and inferred semantic segmentation boundaries, the inferred semantic instance segmentation, as well as the confidence map of the inferred label with bright and dark colors indicating high and low confidence, respectively. See supplementary material and text for details. The first scene (1st column) contains only static objects while the others (2nd and 3rd columns) also contain dynamic objects. ground truth 8 . For evaluation, we exploit the mIoU metric defined on instances following [107]. Specifically, we first match the ground truth instances to the predicted instances. A pixel is then classified as true positive only when its predicted instance index matches the ground truth. Table 3 (right) shows our results. Note how the instance segmentation results are on par with the semantic segmentation, demonstrating our model's intra-class separation ability. Moreover, we also observe higher instance segmentation accuracy when filtering uncertain predictions.\n\n\nQualitative Evaluation\n\nFig. 5 illustrates our dense inference results qualitatively for 3 different scenes in terms of semantic instance segmentation on both static and dynamic objects. The first two rows illustrate both semantic and instance labels, where semantic information is color-coded and instances are separated by boundaries. The last row shows the confidence maps. While the proposed method is able to delineate most object boundaries satisfyingly, some challenges remain. Errors occur in regions where 3D points are absent due to far distance (1st & 3rd scene: far building). Another source of errors is inherent label ambiguities that occur for porous objects such as fences or trees (3rd scene: tree boundary) where even 2D ground truth annotation is a hard and ambiguous task. Finally, 3D points of dynamic objects are accumulated over multiple frames (2nd & 3rd scene), providing dense but less accurate 3D cues to the CRF model. However, note that our probabilistic inference algorithm is able to successfully identify those uncertain regions as demonstrated in the last row, where far buildings and object boundaries are predicted as less certain compared to other image regions.\n\n\nDATASET & BENCHMARKS\n\nWe apply the proposed label transfer method to all frames captured by perspective cameras, resulting in 2\u00d778k 2D 8. While [107] uses two sets of parameters for semantic and instance segmentation, we train a single model for instance segmentation and read semantic labels directly from the instance maps. Therefore, our predictions on classes without instance labels are the same in both semantic and instance segmentation maps. semantic/instance segmentation maps, 1.0B 3D semantic points and 172.4M 3D instance points. We provide a statistical analysis of the 2D & 3D labels in Appendix F.1. We further deploy an online evaluation server and establish benchmarks on a set of challenging tasks relevant to autonomous driving. For all tasks, we split the data at the batch-level into disjoint training, validation and heldout test sets as specified in Appendix F.2. Specifically, we leverage KITTI-360 to address tasks at the intersection of vision, graphics and robotics which are commonly viewed as relevant towards achieving full autonomy, including tasks within the scope of semantic scene understanding, novel view synthesis and semantic SLAM. We now describe each task and the corresponding evaluation protocol in detail. Furthermore, we introduce initial baselines for each task.\n\n\nSemantic Scene Understanding\n\nIn this section, we establish scene perception benchmarks in both 2D image space and 3D domain. We first implement benchmarks for the traditional tasks of 2D semantic segmentation and 2D instance segmentation on perspective images, using the inferred semantic/instance segmentation maps as pseudo ground-truth. While not the main focus of this work, we establish these standard 2D benchmarks to investigate whether there is a performance gap between methods operating in 2D and 3D. Furthermore, as our label definition is compatible with Cityscapes, this benchmark opens up the possibility for studying domain adaption across datasets in future work. Next, we establish benchmarks in the 3D domain, including bounding box detection and semantic/instance segmentation. Moreover, we consider a semantic scene completion task where the goal is to simultaneously complete the scene and infer corresponding semantic labels given limited observations. This task allows autonomous vehicles to hallucinate future possibilities and thus can benefit downstream tasks, e.g., predictive control.\n\n\n2D Semantic Segmentation:\n\nWe train and evaluate 2D segmentation baselines on the densely labeled images in KITTI-360. We consider two well-known methods, Fully Convolutional Neural Network (FCN) [61] and Pyramid\n\n\nResNet-50\n\nResNet-101 Fig. 6: Qualitative Results for 2D Instance Segmentation. The first row shows inference results of Mask-RCNN with a ResNet-50 backbone while the second row uses a ResNet-101 backbone. The ResNet-101 backbone leads to better results, e.g., with the ResNet-50 backbone, the car occluded by the person is split into two instances (left) and the motorcycle is not detected (middle). Note that both variants are able to predict \"Building\" instances after being trained on KITTI-360.  Scene Parsing Network (PSPNet) [116], as a reference. Following Cityscapes [25], we adopt mean intersection over union (mIoU) at two semantic granularities, i.e., classes and categories, where 19 classes are grouped into 7 coarsegrained categories. To account for label uncertainty, the mIoU is weighted by the confidence of our pseudo-ground truth labels. A formal definition of our metrics and a detailed definition of the classes and categories can be found in Appendix G.1. Table 4a shows that, unsurprisingly, PSPNet outperforms the na\u00efve FCN on the test set.\n\n\n2D Instance Segmentation:\n\nWe use the established Mask R-CNN framework [40] with different backbones as our baselines, see Table 4b. We measure the Average Precision (AP) weighted by the label confidence over 10 thresholds, ranging from 0.5 to 0.95 with a step size of 0.05. The mean AP is then calculated over 7 classes that contain instance labels. We also compare mean AP 50 given a threshold of 0.5. Both Table 4b and Fig. 6 suggest that Mask R-CNN with a deeper backbone leads to better performance. Note that we provide instance segmentation labels of \"Buildings\" which are not available for other outdoor datasets [18], [25], [36], [45]. This information allows future works to explore scene compositionality [57], [72], [74] in real-world street scenes.\n\n\n3D Bounding Box Detection:\n\nIn this benchmark we measure the mean AP over two classes, \"Building\" and \"Car\", since it is particularly challenging for learning-based algorithms to generalize well to other classes with fewer training samples. Following [76], the mean AP is calculated at a threshold of 0.25 and 0.5, respectively. We consider VoteNet [76] and its simplified version BoxNet [76] as baseline methods. Both methods require 3D point locations as input and output 3D bounding boxes and their semantic labels. Table 4c suggests that VoteNet can make reasonable predictions for both building and cars while it fails to predict 3D bounding boxes with high IoU values, see Fig. 7f.\n\n\n3D Semantic Segmentation:\n\nWe establish a 3D semantic segmentation benchmark on the accumulated point clouds, where PointNet [77] and PointNet++ [78] are trained and evaluated as baselines. Both methods take as input point locations and colors to predict a semantic label for each 3D point. Following the 2D semantic segmentation task, we measure mIoU weighted by label confidence over classes and categories, respectively. Table 4d shows the quantitative comparison and Fig. 7d illustrates the performance of PointNet++. Interestingly, comparing Table 4a and Table 4d shows that the 3D semantic segmentation baselines' overall performances are inferior compared to the 2D semantic segmentation methods, suggesting that parsing the semantic meaning of irregularly structured 3D point clouds remains more challenging and requires further work.\n\n\n3D Instance Segmentation:\n\nWe evaluate 3D instance segmentation results for \"Building\" and \"Car\". Specifically, we measure the mean AP over a set of thresholds ranging from 0.5 to 0.95 with a step size of 0.05 and AP at a threshold of 0.25 and 0.5. As a first simple baseline, we na\u00efvely cluster semantically labeled points into instances. We use Point-Net++ [78] for semantic segmentation and DBSCAN [30] for clustering. We further evaluate PointGroup [49] as a stateof-the-art method for 3D instance segmentation which takes as input point locations and colors. Table 4e demonstrates that PointGroup outperforms the na\u00efve clustering-based method. The qualitative result of PointGroup is shown in Fig. 7e. While the 2D and 3D results in Table 4b and  Table 4e are defined over different sets of classes, we provide detailed results on each class in Appendix G.5. Interestingly, 3D instance segmentation methods achieve better performance on \"Car\" than 2D methods while performing worse on \"Building\". We hypothesize that unlike in 2D where occlusions strongly impact the results (e.g. Fig. 6 left, pedestrian standing in front of a car), cars can be more easily separated in 3D. As for buildings, many instances are spatially connected (e.g., Fig. 6 right), making the instance segmentation task harder in 3D where boundaries are harder to detect on the sparse point cloud.\n\n\nSemantic Scene Completion:\n\nWhile standard scene perception tasks aim to predict a semantic label for each observed scene point, the semantic scene completion task additionally requires predicting geometry and semantics in unobserved regions. Given a single LiDAR scan as input, this task requires semantic scene completion within a corridor of 30m around the vehicle poses of a 100m trajectory. For evaluation, we measure reconstruction quality and semantic prediction accuracy. The former measures geometric accuracy independent of semantics, using completeness and accuracy over a range of distance thresholds following common practice [89]. We consider a threshold of 20cm as the main metric. As our ground truth reconstruction may not be complete, we evaluate accuracy only in observed regions. We further measure the F 1 score as the harmonic mean of the completeness and the accuracy. The semantic prediction quality is conditioned on the geometric reconstruction. Specifically, we measure the confidence weighted mIoU over the same set of thresholds where a true positive prediction is made when 1) a ground truth point is classified as complete at the given threshold and 2) its closest reconstructed point has the correct label. See Appendix G.6 for more details of the ground truth construction and evaluation metrics.\n\nWe consider two baselines for this task, both taking a single raw LiDAR frame as input. For calibration, we implement a na\u00efve baseline which returns the input as output. The second baseline is a learning-based approach where we use an encoder-decoder architecture to predict the complete scene structure from the raw LiDAR scan. More details about this baseline can be found in Appendix G.6. Table 4f and Fig. 8 illustrate the results. As expected, the raw LiDAR scans are accurate but incomplete. The learningbased approach instead achieves higher completeness but the predictions are less accurate. For the learning-based approach we also predict a semantic label at each 3D point. Fig. 8 shows that the model is able to correctly predict semantic labels at a coarse level but struggles to predict smaller objects like cars.\n\n\nDiscussion:\n\nOur results show that 3D semantic segmentation is harder than 2D semantic segmentation. In contrast, our conclusions for instance segmentation vary for different classes. Some classes, e.g., cars, are easier to segment in 3D, suggesting further works can explore 3D information to enhance 2D instance segmentation. 3D bounding box detection remains challenging, especially when a high IoU is desired. Lastly, while inferring dense geometry and semantics from raw sparse observations can benefit autonomous driving, completing the scene and predicting semantics jointly is a difficult task that requires further research.\n\n\nNovel View Synthesis\n\nSimulation is an essential tool for training and evaluating autonomous vehicles. While existing methods trained in simulated scenes struggle to generalize to real scenes, creating a simulation environment based on real-world images is a promising direction to close the gap between realworld scenarios and synthetic environments [24], [110]. We thus establish challenging benchmarks towards this goal, (a) GT Image (b) NeRF [64] (c) FVS [83] (d) GT Image + PSPNet [116] (e) NeRF [64] + PSPNet [116] (f) FVS [83] + PSPNet [116] Fig. 9: Qualitative Results for Novel View Appearance & Semantic Synthesis. The first row shows the GT image and novel view appearance synthesis results. The second row shows the corresponding semantic segmentation using PSPNet [116].\n\nincluding novel view appearance synthesis and novel view semantic synthesis.\n\n\nNovel View Appearance Synthesis:\n\nIn this benchmark, we are interested in novel view RGB image synthesis for driving scenarios. While we evaluate on a set of held-out perspective images, the benchmark participant can choose from a set of input modalities 9 , including posed perspective/fisheye images or accumulated point clouds. For perspective and fisheye images, we release approximately 50% of the frames for training and use the remaining 50% for testing. In addition, the evaluation server also provides a harder setting with a 90% drop rate. See appendix for details. The point cloud is accumulated over all frames where each point fuses colors from different viewpoints. We adopt three standard evaluation metrics for this benchmark: peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and perceptual metric (LPIPS) [115]. We evaluate two sets of baselines on two different input modalities. We first consider a na\u00efve baseline using the accumulated point cloud (PCL) as input. Specifically, we project non-occluded colored points to test viewpoints, followed by nearest neighbor interpolation to fill in the missing values. As there are no 3D points in the sky, we heuristically assign a mean blue color to the sky region. We further consider several state-of-the-art baselines for image-based novel view synthesis, including methods based on Neural Radiance Fields [7], [27], [64] or per-view depth maps [54], [83]. Results in Table 5 (left) and Fig. 9 (1st row) reveal the challenges of this benchmark. We observe that NeRF [64] shows promising results but struggles to synthesize fine structure. FVS performs better in rendering fine details (e.g., license plate) but exhibits noticeable artifacts due to the inaccurate underlying geometry (e.g., left car). Interestingly, FVS/PBNR performs better in LPIPS but has a lower PSNR compared to NeRF-based methods, suggesting that LPIPS is more sensitive to fine detail than larger regional errors.\n\nNovel View Semantic Synthesis: An important property of simulation environments like CARLA [29] is that they provide not only RGB images but also auxiliary information like semantic label maps. Towards a real-world simulator with the same capability, we therefore consider a novel benchmark that requires joint novel view and semantic synthesis. The input data for this task is the same as for 9. The used input modalities will be indicated on the leaderboard.\n\n\nMethod\n\nInput  Input: \"PC\" denotes the accumulated point cloud and \"PI\" means perspective images. the novel view synthesis task, while the methods are tasked to predict both an RGB image and a semantic segmentation map at a given target camera pose. Therefore, the evaluation metric of this task additionally comprises mIoU for semantic segmentation as shown in Table 5 (right). As no prior work has addressed this problem yet, we consider a na\u00efve twostage solution as baseline to bootstrap this benchmark, i.e., we apply an existing semantic segmentation model (PSP-Net [116]) on the synthesized images. For comparison, we also evaluate the semantic segmentation performance on the original ground truth images (GT Image). Note that the artifacts in the synthesized images lead to a significant performance drop for semantic segmentation. As illustrated in Fig. 9, the fence is misclassified as building when the synthesized images are taken as input to PSPNet, despite that the fence is still visible in these images. It is also interesting to note that semantic segmentation performance is aligned with the LPIPS metric, as both apply pre-trained networks on synthesized images.\n\n\nDiscussion:\n\nOur baselines reveal the different challenges in novel view appearance synthesis with different input modalities. While point clouds provide a good representation of 3D geometry, it is not easy to model view dependency. When instead taking a sparse set of multi-view images as input, the task is similarly difficult despite little variation in the camera orientation. We believe that future works should explore the combination of different input modalities to improve image fidelity further. Moreover, given the low performance of our simple baselines on the novel view semantic synthesis task, there is a large potential for future improvements, i.e., by learning view and semantic synthesis jointly. \n\n\nSemantic SLAM\n\nWe further establish a semantic SLAM benchmark at the intersection of robotics and computer vision. Here, the goal is to simultaneously estimate poses and reconstruct a semantic map from monocular/stereo images and/or LiDAR scans. While there is a growing interest in evaluating indoor semantic reconstructions of SLAM algorithms at roomlevel [87], [102], existing works on outdoor semantic SLAM typically evaluate only pose estimation while ignoring the quality of the semantic reconstruction [12], [23]. Considering that the semantic reconstruction is valuable on its own for down-stream tasks, e.g. planning [28], we thus additionally evaluate geometric and semantic reconstructions where the latter is enabled by the dense semantic annotations of KITTI-360. For this benchmark, the test sequences are separated from those used for 3D scene perception such that the accumulated point cloud is held out from the public.\n\n\nLocalization:\n\nGiven an estimated trajectory, we adopt the standard Absolute Pose Error (APE) and Relative Pose Error (RPE) [37] as metrics for evaluating pose estimation. We consider four test sequences for this task and report the evaluation results on each test sequence without averaging. We evaluate two baseline methods, ORB-SLAM2 [66] and SUMA++ [23], where the former takes stereo images as input and the latter is applied on LiDAR scans. Table 6a compares the localization results of ORB-SLAM2 and SUMA++. For both methods, the APE exceeds 2 meters and the RPE is around 2% in general. ORB-SLAM2 achieves better overall performance compared to SUMA++, suggesting that the stereo images of our dataset contain rich features for the purpose of localization. One possibility for improving localization accuracy is to exploit the 3D bounding boxes and instance labels available in our dataset [71], [109].\n\n\nGeometric and Semantic Mapping:\n\nWe measure the quality of the geometric and semantic mapping using the same metrics considered in the semantic scene completion benchmark. As richer input observations are available in this task, we adopt a smaller distance threshold of 10cm as the main metric. Specifically, we first measure geometry accuracy using completeness and accuracy, and then evaluate semantics on the completed ground truth points via the  confidence weighted mIoU. As the mapping accuracy is highly correlated with the APE, we compare ground truth and estimated reconstruction in local windows to minimize the impact of pose drifts. Each local window consists of 50 consecutive frames and is aligned to the ground truth based on the trajectory, see Appendix I.2 for more details. We use the same baselines considered in the localization benchmark. As ORB-SLAM2 does not provide dense reconstruction nor semantic information, we obtain dense semantic reconstruction by unprojecting 2D semantic segmentations (PSPNet [116]) using depth maps from semiglobal matching (SGM) [42]. SUMA++ aims for semantic SLAM and estimates poses and a semantic surfel map from LiDAR scans. We experimentally observe that it is sufficient to take the center of the surfels as the reconstructed points. Table 6b and Fig. 10 show the reconstruction and semantic prediction results. We observed that both baselines produce good reconstructions on the ground region. For regions above the ground, SUMA++ is less complete as it only uses LiDAR scans and thus the maximum height is limited. ORB-SLAM2 + SGM results in higher completeness but worse accuracy. In terms of semantic predictions, SUMA++ produces reasonable results on the LiDAR scans but struggles to achieve good overall performance due to its low completeness. In contrast, ORB-SLAM2 + PSPNet contains more flying points due to the outliers of stereo matching (e.g., sky points colored blue). Exploring semantic information to remove sky points may further improve the performance of this baseline.\n\n\nDiscussion:\n\nWe evaluate localization accuracy of existing SLAM methods and suggest exploring 3D instance-level information in further works. Further, reconstructing accurate geometry and semantics remains a challenging task. Our benchmark allows to investigate important questions towards solving this challenging task, e.g., which input modality is better suited for this task, whether semantic prediction and geometric reconstruction can benefit each other and if joint optimization is desirable.\n\n\nCONCLUSION\n\nWe present KITTI-360, a large scale 3D video dataset comprising 300k images and laser point clouds with consistent semantics in both 2D and 3D. We create a WebGL-based annotation tool and annotate both static and dynamic objects in 3D. We propose a method to obtain dense semantic instance labels from annotated 3D primitives. In the presence of 3D data, our method yields better results compared to several 2D label transfer baselines while lowering the annotation time.\n\nFurthermore, we establish novel online benchmarks for several challenging tasks at the intersection of computer vision, graphics and robotics. We evaluate several baselines for each benchmark. Our results show that existing methods achieve satisfactory results on well-established benchmarks, e.g., 2D/3D segmentation, where inference is directly performed on given observations. However, it is much harder to solve tasks that require jointly recovering the geometry, appearance and estimating the semantics as in the newly introduced tasks for semantic scene completion, novel view appearance/semantic synthesis and semantic SLAM. We hope that our dataset, online benchmarks and annotation tools will fertilize new research across communities, fostering progress towards the grand goal of full autonomy.\n\n\nAPPENDIX A ANNOTATION DATA PREPARATION\n\n\nA.1 Point Cloud Accumulation\n\nTo facilitate annotation, we accumulate all laser measurements in a common world coordinate system and augment them with 3D points from stereo matching [42]. To reduce outliers of stereo matching, we consider only points up to 15m distance, and apply left-right as well as forwardbackward consistency checks over 5 frames. We fuse all 3D points sequentially and ignore a point closer than 5 cm to its nearest neighbor in the fused point cloud. This downsampling operation allows for reducing the data loading traffic and memory of the web based annotation tool.\n\n\nA.2 Dense Point Cloud on Dynamic Objects\n\nOur simple dynamic object detection consists of two steps. In the first step, we apply volumetric fusion over a sequential of laser scans and search for 3D points located in (mostly) free regions. As a dynamic object always moves in the free space, the voxels along its moving trajectory are occupied occasionally and thus the expected status over time should be free. Hence the dynamic points can be detected by finding points inside of all \"free\" voxels. Specifically, we build a 3D occupancy volumetric grid V for each batch by fusing Velodyne observations using Octomap [44]. Given a set of measurements z 1:t from frame 1 to t, the occupancy probability of each voxel v \u2208 V is updated as follow:\np(v|z 1:t ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 max(min(p(v|z 1:t\u22121 ) + p(v|z t ), p max ), p min ) if p(v|z 1:t\u22121 ) > p min p min if p(v|z 1:t\u22121 ) = p min(8)\nwhere p(v|z 1:t ) is the log-odds of the probability, p max and p min are the upper bound and lower bound of the log-odds respectively. Note that we clamp a voxel to be free if there is sufficient evidence from previous frames to support its free status. We denote the set of free voxels as V.\n\nDue to the noise in measurements and poses, the free voxels may also contain many 3D points of static objects as shown in Fig. 11a. It is hard to avoid these false positive detections as each voxel is classified as free or occupied independently. Therefore, we consider a second step to filter out clusterings of noisy detections. Specifically, we segment the original accumulated point cloud into a branch of clusters using the Region Growing algorithm 10 , and we calculate the occupancy probability of each cluster c based on the detection in the first step:\np(c) = 1 N N i c i \u2208 V(9)\nwhere c i denotes a single point in the cluster, N denotes number of points, and c i \u2208 V means that c i is spatially located within V. A cluster c is considered as dynamic if p(c) is larger than a given threshold. With the second step, 10. https://pcl.readthedocs.io/projects/tutorials/en/latest/region growing segmentation.html we are able to filter out false positive detections as shown in Fig. 11b. It is acceptable if a few false positive detections remains since the dense point cloud will be further labeled by our annotators.\n\n\nAPPENDIX B ANNOTATION INTERFACE\n\nIn this section, we demonstrate the annotation tool and process in detail.\n\n\nB.1 Annotation Scene\n\nColor Coding: Fig. 12 shows different color codings of 3D points that we provide in the annotation tool. Annotators can choose different color codings accordingly. For example, Fig. 12c helps annotators to identify the boundary between \"Road\" and \"Sidewalk\" and Fig. 12d allows annotators to check unannotated region (shown as white).\n\n\n3D Viewpoint:\n\nTo assist annotators to better visualize the scene, we also provide different viewports, namely, normal and orthographic viewports as shown in Fig. 13. The orthographic viewport helps annotators accurately identify stuff classes' boundaries and annotate individual objects  efficiently (see \"fast object annotation mode\" in Section B.2). Besides, annotators can also adjust point size and brightness to work with different levels of detail.\n\n\n2D Camera View:\n\nFor better perceiving the scene, we also show fisheye and perspective images as well as the pose of each camera, enabling annotators to select informative viewpoints efficiently, as shown in Fig. 14.\n\n\nB.2 Annotation Functions\n\nWe provide a few shortcuts and functions to facilitate the annotation process.\n\n\nBounding Primitive Manipulation:\n\nTo best enclose the 3D object, each bounding primitive can be manipulated with translation, scaling, and rotation, resulting in 9 degrees of freedom. In addition, annotators are also asked to assign an orientation to each bounding primitive to understand objects' (especially instances) orientation. See Fig. 15 for each operation's shortcuts.\n\n\nBounding Primitive Copy:\n\nWe also provide a \"copy\" shortcut to allow annotators to quickly insert new annotations with the same label and similar pose to previously annotated objects. This is especially useful for objects appearing frequently with similar sizes such as building and car.\n\n\nFast Object Annotation:\n\nWe support quickly annotating a subset of object classes by simply drawing a line along the object. This fast annotation mode is enabled under the orthographic view for a few pre-selected classes, where the annotator draws a line along the longest side in the middle. While this line only specifies the length of the object in one dimension and its orientation, we heuristically place a bigger bounding primitive centered at this line and iteratively shrink the bounding primitive until it touches any non-ground points. As shown in Fig. 16, this simple technique allows for efficient and accurate annotation.\n\n\nObject-Centric Mode:\n\nTo enable a clear observation of a single object from the accumulated point cloud, we also help annotators with the \"object-centric\" mode. With a single 3D bounding box selected, triggering the \"object-centric\" mode hides all the other bounding primitives as well as points far from the selected primitive. In addition, both front view and side views images are automatically switched to the ones in which the selected primitive is most visible.\n\nCompleteness Check: As illustrated in Fig. 12d, the annotator can check the completeness level of the annotation by visualizing the point cloud based on existing bounding primitives. Specifically, we color each 3D point if it is enclosed by a bounding primitive and leave the unlabeled region as white. This helps the annotator easily identify any unlabeled 3D points.\n\n\nB.3 Ground Annotation\n\nGround bounding primitives are simply annotated as 2D polygons. The extruded height of the ground polygon is automatically determined as follows: for each vertex v = {x, y} on the polygon, we first search the nearest camera of this given vertex, and assign the height of the camera to this vertex as its initial height\u1e91. Then we search nearest neighbors of point {x, y,\u1e91} in the 3D point cloud, and update height z as the median height of these nearest neighbors. See Fig. 17 as an example for \"Road\" annotation. Annotators can also modify the 2D polygon anytime by dragging its control points. \n\n\nB.4 Dynamic Object Annotation\n\nWe implement a semi-automatic annotation scheme to label dynamic objects efficiently. Our semi-automatic annotation relies on two assumptions: the size of the dynamic object is fixed over time, and its trajectory is smooth. Therefore, the required annotation is reduced to adding posed 3D primitives at several keyframes. Our annotation tool then automatically places the remaining primitives along the trajectory. The smooth trajectory is obtained via spline interpolation based on the primitives at the keyframes. We annotate articulated dynamic objects, e.g., pedestrians, using the maximum extent bounding box.\n\nAs the speed of the dynamic object may not be constant, we place the remaining primitives based on the observed 3D points at each timestamp. Specifically, we first discretize the annotated 3D primitives into voxels and fuse the occupancy status of each voxel over all annotated primitives as shown in Fig. 18a. A voxel is considered as occupied if it is occupied in any of the annotated 3D primitives, otherwise it is free. This fused occupancy status is considered an \"occupancy template\" for searching matching 3D primitive along the trajectory. Given a timestamp between the first and the last annotated timestamp, we slide the 3D primitive on the interpolated spline and calculate the occupancy status based on 3D points collected at the given timestamp. We choose the pose that provides the maximum overlap with the occupancy template, see Fig. 18b.\n\nTo facilitate users' interaction, we plot the interpolated spline in the annotation tool and allow the annotator to refine the spline by simply adjusting the 3D bounding primitives inserted at the keyframes. We also display the automatically generated 3D primitives to help the user check if they are accurate. The poses of each automatically generated bounding primitive can also be adjusted if necessary. Typically, it requires 2 \u223c 5 annotated keyframes to produce accurate annotations for the full moving trajectory. Fig. 19 illustrates the dynamic annotation at a given timestamp. Table 7 shows the definition of the 37 classes that we use for annotating 3D scenes. We adhere to the definition of Cityscapes as close as possible while a few inconsistent label definitions are inevitable as our annotations are performed in 3D. For example, the \"Traffic Sign\" in Cityscapes only includes the front side while we consider both the front and the back. We do not distinguish the back as each traffic sign is labeled by a single 3D bounding primitive and it might be  observed in 2D from both sides. Thus, each traffic sign has a consistent label regardless of which side it is observed.\n\n\nAPPENDIX C LABEL DEFINITION\n\n\nAPPENDIX D MORE DETAILS OF LABEL TRANSFER INFERENCE\n\n\nD.1 Accumulation of Input Point Cloud\n\nIn contrast to the point cloud accumulation for annotation as introduced in Appendix A, here we need to distinguish static and dynamic points for label transfer inference and thus determine visible points on every frame. Specifically, we consider a point static if it is not enclosed by any dynamic bounding primitive and accumulate all static points Category Class\n\nInstance Definition flat road Horizontal surfaces on which cars usually drive, including road markings. Typically delimited by curbs, rail tracks, or parking areas. However, road is not delimited by road markings and thus may include bicycle lanes or roundabouts. sidewalk Horizontal surfaces designated for pedestrians or cyclists. Delimited from the road by some obstacle, e.g. curbs or poles (might be small), but not only by markings. Often elevated compared to the road and often located at the side of a road. The curbs are included in the sidewalk label. Also includes the walkable part of traffic islands, as well as pedestrian-only zones, where cars are not allowed to drive during regular business hours. If it's an all-day mixed pedestrian/car area, the correct label is ground. parking\n\nHorizontal surfaces that are intended for parking and separated from the road, either via elevation or via a different texture/material, but not separated merely by markings. rail track Horizontal surfaces on which only rail cars can normally drive. If rail tracks for trams are embedded in a standard road, they are included in the road label. construction building \u2020 Includes structures that house/shelter humans, e.g. low-rises, skyscrapers. Translucent buildings made of glass still receive the label building. Also includes scaffolding attached to buildings. garage * Structures for parking. wall\n\nIndividually standing walls that separate two (or more) outdoor areas, and do not provide support for a building. fence\n\nStructures with holes that separate two (or more) outdoor areas, sometimes temporary. guard rail Metal structure located on the side of the road to prevent serious accidents. Rare in inner cities, but occur sometimes in curves. Includes the bars holding the rails bridge Bridges (on which the ego-vehicle is not driving) including everything (fences, guard rails) permanently attached to them. tunnel Tunnel walls and the (typically dark) space encased by the tunnel, but excluding vehicles. gate * A passageway or opening in a wall of fence for entrance or exit. stop * A place where a bus or train stops for passengers to get on or off. It intends to protect waiting pedestrians from rain, wind and snow. object pole \u2020 Long, vertically oriented poles, e.g. sign poles or traffic light poles. This does not include objects mounted on the pole, which have a larger diameter than the pole itself (e.g. most street lights). smallpole * Small, vertically oriented poles, e.g. sign poles or traffic light poles. This does not include objects mounted on the pole, which have a larger diameter than the pole itself (e.g. most street lights). traffic light\n\nThe traffic light box without its poles in all orientations and for all types of traffic participants, e.g. regular traffic light, bus traffic light, train traffic light traffic sign \u2020 Signs installed by the state/city authority with the purpose of conveying information to drivers/cyclists/pedestrians, e.g. traffic signs, parking signs, direction signs, or warning reflector posts. Both frontal and back side are included. lamp * A lamp usually mounted on a pole and constituting one of a series spaced at intervals along a public road or highway. trash bin * A container that holds materials that have been thrown away. vending machine * An automated machine for selling merchandise. Grass, all kinds of horizontally spreading vegetation, soil, or sand. These are areas that are not meant to be driven on. This label may also include a possibly adjacent curb. Single grass stalks or very small patches of grass are not annotated separately and thus are assigned to the label of the region they are growing on. sky sky Open sky (without tree branches/leaves) human person All humans that would primarily rely on their legs to move if necessary. This includes bicycles without the cyclist or other passengers. The latter receive the label rider. void unknown construction All remaining construction regions which are not mentioned in the \"construction\" session unknown vehicle All remaining vehicle regions which are not mentioned in the \"vehicle\" session unknown object All remaining object regions which are not mentioned in the \"object\" session * classes do not exist in Cityscapes \u2020 classes with definitions slightly different from Cityscapes first. For each dynamic object, we retrieve all points inside the corresponding bounding primitives {b m t } for every timestamp t and accumulate them in the canonical objectcentered coordinate system by taking the inverse transform of the object pose defined by {b m t } (world-to-object transformation). Next, we insert the accumulated dynamic point clouds back to the world coordinate following the object pose (object-to-world transformation). This allows us to obtain dense 3D points during inference for both static and dynamic regions.\n\n\nD.2 Accumulation of Inferred 3D Label\n\nOur inference is performed individually on each frame defined over the corresponding 2D pixels and visible 3D points. To obtain 3D labels on the accumulated point clouds, we thus fuse 3D labels obtained from each frame. Specifically, if a 3D point is visible in multiple frames, we take the majority of its inferred classes as its final label. The confidence of this 3D point is also averaged over confidence values of these points of the majority label. If a 3D point is not visible in any of the frames but is uniquely labeled by a single class, we assign this unique label to the 3D point and a confidence value of 1.0. In the remaining cases, we treat the 3D point's label as unknown.\n\n\nD.3 Pixel Unary Potentials of Ground Objects\n\nThe first term of the pixel unary potential is a binary feature \u03be P i (s i ) \u2208 {0, 1} which indicates admissible labels. For nonplanar object classes, \u03be P i (s i ) is obtained based on the projection of 3D primitives, whereas planar object classes directly use projections of 2D polygons to obtain more accurate boundaries. As introduced in Appendix B.3, the ground bounding primitives are extruded to 3D to enclose the 3D points. This leads to oversized 2D projections, making it hard to determine the boundary between two adjacent ground object annotations (e.g., \"Road\" and \"Sidewalk\"). As opposed to our conference version [107], which exploits a geometric unary potential to address this problem, we instead directly project the 2D polygons of the ground objects before extruding them to 3D. This allows us to obtain accurate ground object boundaries from \u03be P i (s i ) and avoid the complexity introduced by the additional geometric unary term.\n\n\nD.4 Instance Augmentation of Pixel Unary Potentials\n\nWe adopt a state-of-the-art panoptic segmentation method UPSNet [108] pre-trained on Cityscapes [25] to obtain instance hypotheses for \"Car\", \"Truck\", and \"Pedestrian\". We first run UPSNet on our images to get probability maps of all instances. For each annotated instance within the aforementioned classes, we retrieve matched instances from the predictions of UPSNet. More specifically, given a set of 3D points annotated with one instance (e.g., one car) and a probability map of one instance predicted by UPSNet, we consider they match if more than 50% of the 3D points fall into the high-probability region of the predicted instance. This allows for improving instance boundaries as shown in Fig. 20.\n\n\nAPPENDIX E MORE RESULTS OF LABEL TRANSFER INFERENCE E.1 Detailed Quantitative Comparisons\n\nHere, we show detailed quantitative comparisons for individual classes. Table 8 and Table 9 show quantitative comparison to label transfer baselines on static and dynamic objects, respectively. We evaluate the intersection over union (IoU) of each class where the mIoU is the average over all classes. We further show detailed ablation study in Table 10 and Table 11 for semantic and instance label transfer.\n\n\nE.2 Qualitative Comparison to Baselines\n\nWe compare our method qualitatively to several 2D-to-2D and 3D-to-2D label transfer baselines in Fig. 21. Note how the 2D-to-2D label transfer baselines fail in the presence of strong occlusions and large displacements.  Fig. 23 shows the distribution of the semantic labels in KITTI-360. Fig. 23a and Fig. 23b suggests that the semantic distribution of the 2D pixels and the 3D points are similar (except for the \"Sky\" class). We also show the distribution of our 3D bounding boxes in Fig. 23c.\n\n\nE.3 Qualitative Comparison of Ablation Study\n\n\nAPPENDIX F DATASET F.1 Statistical Analysis\n\n\nF.2 Dataset Split\n\nWe split KITTI-360 into training and test sets without spatial overlapping as shown in Fig. 24. We maintain an online evaluation server and hold back the labels of the test set. Considering that different tasks involve different label modalities, the test set is further divided into two parts with different information released. Specifically, the first part of the test dataset is used for semantic scene understanding (except for semantic scene completion) and novel view synthesis, where we hold back the 2D semantic/instance segmentation maps and 3D pointwise labels. Note that the accumulated point clouds are released while their labels are removed. The other part is adopted for semantic scene completion and semantic SLAM where the accumulated point clouds are further removed. We release vehicle poses for both test sets. intersection and the union of one class label (or one category label), respectively. The weighted IoU of this class can be defined as follow:\nIoU = i\u2208{TP} c i i\u2208{TP, FP, FN} c i(10)\nwhere c i \u2208 [0, 1] denotes the confidence value at pixel i. In the standard evaluation c i = 1 for all pixels. The mIoU is then calculated as the mean of the weighted IoU over all class labels or category labels. While we provide 19 classes for training following Cityscapes [25], we omit two classes, \"Train\" and \"Bus\" dur-ing evaluation since these two classes are rarely observed in the test region when we split the training and test sets according to the camera poses as shown in Fig. 24.\n\n\nG.1.2 Baselines\n\nWe train and evaluate two well-known methods, Fully Convolutional Neural Network (FCN) [61] and Pyramid Scene Parsing Network (PSPNet) [116]. For FCN, we adopt the ResNet-101 model provided by PyTorch as a backbone. The model is pre-trained on a subset of the Microsoft COCO dataset. As for PSPNet, we use the official PyTorch imple-    \n\n\nG.1.3 Additional Results\n\nWe show the IoU of each class in Table 12a. We observe that PSPNet consistently outperforms FCN in most of the classes.\n\n\nG.2 Benchmark of 2D Instance Segmentation\n\n\nG.2.1 Evaluation Metric\n\nFollowing [58], we measure the Average Precision (AP) over 10 IoU thresholds, ranging from 0.5 to 0.95 with a step size of 0.05. We calculate confidence weighted IoU per instance using Eq. 10. In this task, we consider 7 classes that contain 11. https://github.com/hszhao/semseg instance labels, including \"Building\", \"Person\", \"Rider\", \"Car\", \"Truck\", \"Motorcycle\" and \"Bicycle\".\n\n\nG.2.2 Baselines\n\nWe evaluated two Mask R-CNN models with different backbones, i.e., ResNet-50 and ResNet-101, based on the official implementation 12 . Both backbones are pre-trained on the ImageNet dataset. Table 12b shows the AP of each individual class as well as the mean AP. We observe that performance of different backbones is similar in more frequently observed classes (e.g., \"Building\" and \"Car\") while differs in less occurred classes. 12  components in our model on all 120 images. The components are abbreviated as follows: LA = local appearance (p P ), PW = 2D pairwise constraints (\u03c8 P,P ), CO = 3D primitive constraints (\u03be P ), 3D = 3D points (\u03d5 L ,\u03c8 P,L ), Full Model = all potentials including 3D pairwise constraints (\u03c8 L,L ). Percentages denote fractions of estimated pixels with highest confidence.\n\n\nG.2.3 Additional Results\n\n\nMethod\n\nBldg Car Trler Crvn Box mIoU Acc LA 79 \n\n\nG.3 Benchmark of 3D Bounding Box Detection\n\n\nG.3.1 Evaluation Metric\n\nWe evaluate AP at a threshold of 0.5 and 0.25 for 3D bounding box detection. As it is particularly challenging for learning-based algorithms to generalize well to other classes with fewer training samples, we measure the mean AP over two classes: \"Building\" and \"Car\".\n\n\nG.3.2 Baselines\n\nWe evaluate the state-of-the-art 3D bounding box detection method, VoteNet [76], and its simplified version, BoxNet [76] as baselines. We adopt the official implementation 13 for both methods. 13. https://github.com/facebookresearch/votenet (a) Distribution of 2D semantic labels over 78k frames.\n\n(b) Distribution of 3D semantic labels over 1B points.\n\n(c) Distribution of 3D semantic labels over 68k bounding boxes. (c) shows the class distribution of 3D bounding boxes that contains instance IDs. Table 12c shows the AP on each class as well as the mean AP. Both methods achieve reasonable performance at the IoU threshold of 0.25 while struggle at the higher threshold.\n\n\nG.3.3 Additional Results\n\n\nG.4 Benchmark of 3D Semantic Segmentation\n\n\nG.4.1 Evaluation Metric\n\nFor 3D semantic segmentation, we also evaluate confidence weighted mIoU using Eq. 10. Here, the confidence of each Fig. 24: Dataset Split. We split the dataset into one training set, one validation set, and two test sets with held-out ground truth.\n\n3D point is obtained by averaging the confidence of 3D points on multiple frames, as introduced in Appendix D.2. Similar to 2D semantic segmentation, we omit \"Train\" and \"Bus\" during evaluation. Note that there is no \"Sky\" point in 3D, thus it is also discarded. Moreover, since we train and evaluate only in static regions, we ignore \"Rider\" as it only appears as a dynamic object.\n\n\nG.4.2 Baselines\n\nWe train and evaluate two baselines, PointNet [77] and PointNet++ [78]. As the original implementations are built on Tensorflow, we adopt a faithful Pytorch reimplementation 14 that contains both methods. Table 12d shows the detailed results of 3D semantic segmentation. As expected, PointNet++ achieves better performance on all classes compared to PointNet.\n\n\nG.4.3 Additional Results\n\n\nG.5 Benchmark of 3D Instance Segmentation\n\n\nG.5.1 Evaluation Metric\n\nIn 3D instance segmentation, we also evaluate AP over 10 IoU thresholds, ranging from 0.5 to 0.95 with a step size of 0.05. The IoU of each 3D instance is weighted (per-point) by the confidence of our pseudo-ground truth. Here, we evaluate on \"Building\" and \"Car\" the same as the 3D box bounding detection benchmark.\n\n14. https://github.com/yanx27/Pointnet Pointnet2 pytorch \n\n\nG.5.2 Baselines\n\nWe first consider a na\u00efve baseline based on the results we obtained from 3D semantic segmentation using Point-Net++ [78]. Specifically, we first extract points of the same class label (\"Building\" or \"Car\") based on the semantic segmentation results. Next, we group the extracted point cloud using DBSCAN [30] 15 , where clusters with less than 500 points are ignored. Each valid cluster is then considered as an instance with a confidence score of 1.0. The second baseline is a state-of-the-art approach, PointGroup [49]. We follow the official implementation 16 to train and evaluate this baseline. Table 12e shows the detailed results of 3D instance segmentation. Interestingly, the 3D instance segmentation performance of \"Car\" is higher than the 2D baselines in Table 12b. We hypothesize that unlike in 2D where occlusions strongly impact the results, cars can be more easily separated in 3D.\n\n\nG.5.3 Additional Results\n\n\nG.6 Benchmark of Semantic Scene Completion\n\n\nG.6.1 Data Preparation\n\nThe ground truth of the semantic scene completion task is the accumulated point cloud within a corridor of 30m around the vehicle poses of a 100m trajectory (50m in each direction), see Fig. 25 for an illustration. The input to this task is a single LiDAR scan whose center is visualized by the blue star point. We first determine a set of neighboring vehicle poses close to the given center illustrated in Fig. 25a, and then crop the accumulated point cloud using the union of circles located at those poses as shown in Fig. 25b. To avoid evaluating in significantly occluded regions that typically occur when the vehicle turns a large angle, we also check the orientation of each pose as shown in Fig. 25a. Specifically, if the forward direction of one pose deviates more than 45 \u2022 compared to the heading angle of the given center, it is eliminated from the neighboring poses.\n\n\nG.6.2 Evaluation Metric\n\nIn this task we evaluate geometric completion and semantic estimation, respectively. Geometric completion is evaluated 15. http://www.open3d.org/docs/0.12.0/python api/open3d. geometry.PointCloud.html 16. https://github.com/dvlab-research/PointGroup via completeness and accuracy at a threshold of 20cm. Completeness is calculated as the fraction of ground truth points of which the distances to their closest reconstructed points are below the threshold. Accuracy instead measures the percentage of reconstructed points that are within a distance threshold to the ground truth points. As our ground truth reconstruction may not be complete, we prevent punishing reconstructed points by dividing the space into observed and unobserved regions, which are determined by the unobserved volume from a 3D occupancy map obtained using OctoMap [44]. A reconstructed point is only evaluated when it falls into the observed region within the union of the neighboring circles shown in Fig. 25b. We further measure the F 1 score as the harmonic mean of the completeness and the accuracy. Note that SemanticKITTI [8] also considers a semantic scene completion task, but considers voxel as representation and measures mIoU over voxels for both reconstruction and semantics. We instead avoid discretization and directly evaluate on point clouds using standard metrics to separately assess accuracy and completeness.\n\n\nG.6.3 Baselines\n\nWe implement two baselines for this task. For calibration, the first baseline returns the input LiDAR scan as output. The second baseline is a learning-based approach that adopts an encoder-decoder structure. Specifically, the encoder first learns features from the input point cloud. It then merges the point-wise features to voxels such that a 3D U-Net is applied to predict a volumetric reconstruction. The network is trained using a cross-entropy loss where the ground truth point cloud is also discretized into a volume. As our evaluation server requires submission in the form of point clouds, we uniformly and densely sample points from each occupied voxel as the final output. Table 12f shows detailed semantic estimation performance of the learning-based baseline. As can be seen, it is challenging to predict the geometry and the semantics jointly. The overall performance of this baseline is worse compared to baselines that directly perform 3D semantic segmentation in Table 12d.\n\n\nG.6.4 Additional Results\n\n\nAPPENDIX H NOVEL VIEW SYNTHESIS BENCHMARK\n\n\nH.1 Benchmark of Novel View Appearance Synthesis\n\n\nH.1.1 Evaluation Metric\n\nWe adopt three standard metrics to evaluate novel view appearance synthesis: peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM), and perceptual metric (LPIPS) [115].\n\n\nH.1.2 Data Preparation\n\nWe select 5 static scenes with a driving distance of \u223c 50 meters each for evaluating NVS at a 50% drop rate. We select one frame every \u223c 0.8 meters driving distance (corresponding to the overall average distance between frames) to avoid redundancy when the vehicle is slow. We release 50% of the frames for training and retain 50% for evaluation.\n\nGT Image PCL NeRF [64] mip-NeRF [7] DS-NeRF [27] FVS [83] PBNR [54] Fig. 26: Additional Qualitative Results for Novel View Appearance & Semantic Synthesis. The left column row shows the GT image and novel view appearance synthesis results. The right column shows the corresponding semantic segmentation using PSPNet [116].\n\nMoreover, we select 10 static scenes with a driving distance of \u223c 50 meters each for evaluating NVS at a 90% drop rate. On average, we select one frame every 4 meters driving distance in this setting. We release 50% of the frames for training and retain 50% for evaluation.\n\n\nH.1.3 Baselines\n\nWe evaluate two sets of baselines for this task. The first baseline (PCL) takes a colored point cloud as input. We project non-occluded points to the test viewpoint and interpolate the missing values to obtain the full image. To determine non-occluded points, we reconstruct a mesh using the ballpivoting method [10] on the accumulated point cloud. As there is no point in the sky region, we in-paint the sky using a constant blue color. The sky region is heuristically determined based on the projected 3D points, i.e., a large connected area in the upper half of the image without any 3D projections is considered as the sky. The second set of baselines takes a set of images as input. For all NeRF-based methods [7], [27], [64], we train one model on each scene individually, using cascaded sampling with 256 coarse samples and 256 fine samples. We adopt the PyTorch reimplementation of NeRF 17 , the original implementation of mip-NeRF 18 and DS-NeRF 19 . As for Free View Synthesis (FVS) [83], we follow its original implementation 20 and use their released model trained on the Tanks and Temples dataset [53] which generalizes well. We follow the original implementation 21 of PBNR [54] that optimizes a set of attributes such as reprojected features or depth in each input view.\n\n\nH.1.4 Additional Results\n\nWe show additional qualitative results of all methods in Fig. 26 (left). The PCL baseline exhibits blocky artifacts due to interpolation. The vanilla NeRF shows promising performance but sometimes struggles due to the sparse input views. While mip-NeRF and DS-NeRF both improve the performance, the thin structures (e.g., fence) are still not well recovered. Interestingly, FVS and PBNR are better at preserving the fine details (e.g., license plate) but have lower PSNR. This could be due to small misalignments in the image space.\n\n\nH.2 Benchmark of Novel View Semantic Synthesis\n\n\nH.2.1 Evaluation Metric\n\nWe evaluate the confidence weighted mIoU using Eq. 10. Similar to the 2D semantic segmentation task, we omit \"Train\" and \"Bus\" during evaluation. We additionally omit \"Truck\", \"Person\"', \"Rider\", \"Bicycle\" and \"Traffic Light\" as these classes do not appear in the 5 static scenes for evaluating NVS. 17 \n\n\nH.2.2 Baselines\n\nAs there is no existing research work on this new benchmark, we directly apply PSPNet used in the 2D semantic segmentation task to synthesized images for semantic label prediction.\n\n\nH.2.3 Additional Results\n\nWe show confidence weighted IoU on individual classes in Table 13. Note that this na\u00efve baseline leads to significantly degraded performance on most of the classes. As shown in Fig. 26 (right), small changes in the image space sometimes lead to sigficant changes in the semantic prediction.\n\n\nAPPENDIX I SEMANTIC SLAM BENCHMARK\n\n\nI.1 Localization\n\n\nI.1.1 Evaluation Metric\n\nWe adopt the standard Absolute Pose Error (APE) and Relative Pose Error (RPE) [37] as metrics for evaluating pose estimation. We align the predicted trajectory to the ground truth using a rigid transformation to evaluate the APE [98]. The RPE is evaluated between two frames with a distance of 1 meter.\n\n\nI.1.2 Baselines\n\nWe evaluate ORB-SLAM2 [66] 22 and SUMA++ [23] 23 using their official implementations as baselines. Fig. 27 shows qualitative comparison of predicted trajectories. As can be seen, both methods achieve reasonable performance while SUMA++ has a larger maximum error than ORB-SLAM2.\n\n\nI.1.3 Additional Results\n\n\nI.2 Geometric & Semantic Mapping\n\n\nI.2.1 Evaluation Metric\n\nWe adopt the same evaluation metrics considered in the semantic scene completion benchmark, as introduced in Appendix G.6. When evaluating the quality of reconstruction, we compare ground truth and estimated reconstruction in local windows to minimize the impact of pose drifts. Specifically, we divide the test sequences into a set of local windows, each consisting of 50 consecutive frames. We first crop the ground truth and the reconstructed point cloud wrt. the region of interest of each window. These two local point clouds are then aligned using the similarity transformation between the corresponding poses [98] and compared afterwards. Finally, we average the completeness, accuracy, and mIoU metrics over the entire test sequence. Following [89], we measure completeness and accuracy over discretized voxels such that these metrics are insensitive to the density of the point clouds. 22 \n\n\nI.2.2 Baselines\n\nTo obtain dense semantic reconstruction given the localization results of ORB-SLAM2, we unproject 2D semantic segmentation maps obtained from PSPNet [116] using depth maps estimated by semi-global matching (SGM) [42]. We merge the unprojected 3D points using the poses predicted by ORB-SLAM2. As for SUMA++, we use the semantic estimation model pre-trained on KITTI.\n\n\nI.2.3 Additional Results\n\nWe show additional qualitative results of the geometric mapping in Fig. 28 in terms of completeness and accuracy at the threshold of 10cm. Consistent with the quantitative results, SUMA++ is more accurate while ORB-SLAM2+SGM is more complete. The first row shows the ground truth point cloud with green denoting complete and red for incomplete. The second row shows the predicted point cloud with green denoting accurate, red for inaccurate, and blue for points in unobserved regions. Note that SUMA++ is more accurate while ORB-SLAM2+SGM is more complete.\n\nFig. 2 :\n2Georegistered poses overlaid on OpenStreetMap.\n\nFig. 3 :\n3Annotation Interface. Our interface consists of three main components: scene view (perspective views and 3D view), semantic label panel, and controllers.\n\nFig. 4 :\n43D-to-2D Label Transfer. (a) We illustrate the 3D-to-2D projection of static and dynamic object annotations. A static 3D primitive is projected to multiple frames while a dynamic 3D object is projected only into the corresponding frame. (b) Factor graph representation of our model. Note that the CRF model is defined over all pixels and visible 3D points at a single timestamp. (c) We show the semantic inference result at timestamp t.\n\nFig. 7 :Fig. 8 :\n78Qualitative Results for 3D Scene Perception. We establish benchmarks for 3D semantic/instance segmentation and 3D bounding box detection. This figure shows the ground truth and the prediction of a baseline method for each sub-task.Raw Input Acc. Raw Input Comp.Enc-Dec Acc. Enc-Dec Comp. Enc-Dec Semantic GT Semantic Qualitative Results for Semantic Scene Completion evaluated at a distance threshold of 20cm. Green denotes complete/accurate, red denotes incomplete/inaccurate and blue denotes points in unobserved region.\n\nFig. 10 :\n10Qualitative Results for Semantic Mapping on test sequence 3 colored based on semantic class labels.\n\nFig. 11 :Fig. 12 :\n1112Dynamic Object Detection. (a) Detected dynamic points with decision on 3D grids. (b) Detected dynamic points with decision on point cloud clusters. Color Codings of 3D points supported by our annotation interface.\n\nFig. 13 :\n133D Viewport.\n\nFig. 14 :\n142D Camera View. We illustrate fisheye and perspective camera views and virtual camera poses.\n\nFig. 15 :Fig. 16 :\n1516Bounding Fast Object Annotation.\n\nFig. 17 :\n17Ground Annotation.\n\nFig. 18 :\n18Semi-Automatic Dynamic Object Annotation. (a) We discretize the annotated 3D primitives and fuse the occupancy status. (b) We search along the spline and find the matching bounding primitive by maximizing the overlap.\n\nFig. 19 :\n19Dynamic Annotations. The above 3D primitives are automatically generated along the interpolated spline visualized in orange.\n\n\nbox * Any rigid typically rectangular container excluding trash bin and vending machine. Some examples are electric box, honey bucket, package, etc. nature vegetation Trees, hedges, and all kinds of vertically growing vegetation. Plants attached to buildings/walls/fences are not annotated separately, and receive the same label as the surface they are supported by. terrain\n\nFig\n. 22 compares different variants of our label transfer model. Consistent with the quantitative analysis, our full model achieves the best performance.\n\nFig. 20 :Fig. 21 :\n2021APPENDIX G SEMANTIC SCENE UNDERSTANDING BENCHMARK G.1 Benchmark of 2D Semantic segmentationG.1.1 Evaluation MetricsWe evaluate confidence weighted mIoU where both the intersection and the union are weighted (per-pixel) by the confidence of our pseudo-ground truth. More formally, let {TP} and {TP, FP, FN} denote the set of image pixels in the Ablation Study of Instance Unary. The top row shows the input image with the projected 3D points and inferred semantic segmentation boundaries. The second row shows the inferred semantic instance segmentation. As can be seen from (a), it is challenging to distinguish instance boundaries given sparse projections of point clouds. This can be effectively improved by incorporating instance unary as shown in (b). Qualitative Comparison to Baselines. Each subfigure shows from top-to-bottom: the input image with inferred semantic segmentation and the errors with respect to 2D ground truth annotation where colors indicate ground truth labels.\n\nFig. 22 :\n22Qualitative Results for Ablation Study. Each subfigure shows from top-to-bottom: the input image with inferred semantic segmentation, and the errors with respect to 2D ground truth annotation where colors indicate ground truth labels.\n\nFig. 23 :\n23Dataset Statistics. (a) and (b) show the histogram of the 19 training semantic classes in 2D and 3D, respectively.\n\nFig. 25 :\n25Ground Truth Generation for Scene Completion Benchmark. The blue star denotes the center of the input laser scan. The blue and red dots denote valid and invalid neighbors along the corridor, respectively.\n\nFig. 27 :Fig. 28 :\n2728Qualitative Results on Localization. Each figure compares the predicted trajectory to the ground truth. Color indicates the APE in meters. Qualitative Results on Localization and Mapping. Completeness and accuracy evaluated at a threshold of 10cm.\n\n\nis with the Autonomous Vision Group, University of T\u00fcbingen and Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany, and Zhejiang University. J. Xie is with Google Research. A. Geiger is with the Autonomous Vision Group, University of T\u00fcbingen and Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany. E-mails: yiyi.liao@tue.mpg.de, junx@google.com, a.geiger@uni-tuebingen.de.\n\nTABLE 1 :\n1Overview of Publicly Available Datasets.\n\nTABLE 2 :\n2Comparison to Label Transfer Baselines on Semantic Segmentation Transfer.\n\nTable 3 (\n3left) shows the comparison onMethod \nSemantic \nInstance \n\nmIoU \nAcc \nmIoU \nAcc \n\nLA \n\n67.6 \n88.7 \n72.3 \n86.7 \n\nLA+3D \n\n70.4 \n89.7 \n72.9 \n88.7 \n\nLA+PW \n\n66.6 \n87.9 \n72.9 \n85.8 \n\nLA+PW+CO \n\n76.8 \n91.8 \n81.6 \n91.0 \n\nLA+PW+CO+3D \n\n78.2 \n92.4 \n83.6 \n91.7 \n\nFull Model \n\n81.2 \n93.1 \n83.7 \n91.8 \n\nFull Model (90%) \n\n88.3 \n96.0 \n89.0 \n94.9 \n\nFull Model (80%) \n\n92.5 \n97.6 \n91.3 \n96.6 \n\nFull Model (70%) \n\n94.3 \n98.4 \n92.7 \n97.4 \n\n\n\nTABLE\n\n\nTABLE 4 :\n4Quantitative Results for 2D & 3D Scene Understanding on Various Different Tasks.\n\nTABLE 5 :\n5Quantitative Results for Novel View Appearance & Novel View Semantic Synthesis using a 50% drop rate.\n\n\nAcc./ Comp. / F 1 mIoU Acc./ Comp. / FTest \nORB-SLAM2 \nSUMA++ \nSeq. \nAPE (m) \nRPE (%) \nAPE (m) \nRPE (%) \n0 \n1.53 \u00b1 0.74 2.42 \u00b1 1.34 2.27 \u00b1 1.38 2.66 \u00b1 2.12 \n1 \n2.22 \u00b1 0.78 2.46 \u00b1 1.37 2.87 \u00b1 1.50 2.43 \u00b1 1.80 \n2 \n2.12 \u00b1 0.94 1.50 \u00b1 1.01 4.62 \u00b1 4.24 2.90 \u00b1 2.90 \n3 \n1.79 \u00b1 0.96 1.72 \u00b1 1.22 2.77 \u00b1 1.44 2.88 \u00b1 2.42 \n\n(a) Localization. RPE evaluated with a delta unit of 1 meter. \n\nTest \nORB-SLAM2 + PSPNet \nSUMA++ \nSeq. 1 mIoU \n0 \n78.5 / 72.8 / 75.5 \n35.3 \n90.8 / 63.1 / 74.5 \n19.9 \n1 \n81.8 / 76.8 / 79.2 \n31.6 \n89.3 / 62.9 / 73.8 \n17.3 \n2 \n82.5 / 70.8 / 76.2 \n30.4 \n89.6 / 64.5 / 75.0 \n17.6 \n3 \n84.3 / 79.1 / 81.6 \n32.7 \n94.2 / 66.3 / 77.8 \n22.8 \n\n(b) Semantic mapping evaluated at a threshold of 10cm. \n\n\n\nTABLE 6 :\n6Quantitative Results for Semantic SLAM. Evaluated on 4 test sequences.\n\nTABLE 7 :\n7Label Definition. We adhere to the label definition of Cityscapes as close as possible. Inconsistent classes are marked.\n\nTABLE 8 :\n8Comparison to Label Transfer Baselines on Semantic Segmentation Transfer of Static Objects.We compare our method to 2D label transfer baselines (top) and to 3D to 2D label transfer baselines (bottom) on 120 consecutive images of static objects.Method \nCar \nTruck Trailer Motor Bicycle Rider Person mIoU \nAcc \nLabel Prop. [100] \n\n28.5 \n53.2 \n71.7 \n11.1 \n42.0 \n32.2 \n21.8 \n37.2 \n59.1 \n\nSparse Track. + GC [95] \n\n10.1 \n22.0 \n13.2 \n5.2 \n1.4 \n5.6 \n0.1 \n8.2 \n12.5 \n\n3D Prop. + GC \n\n15.7 \n40.3 \n24.1 \n2.2 \n0.2 \n2.0 \n17.1 \n14.5 \n21.7 \n\nProposed Method \n\n77.7 \n85.1 \n69.8 \n60.5 \n42.9 \n49.5 \n59.0 \n63.5 \n94.1 \n\n\n\nTABLE 9 :\n9Comparison to Label Transfer Baselines on Semantic Segmentation Transfer of Dynamic Objects. We compare our method to 2D label transfer baselines (top) and to 3D to 2D label transfer baselines (bottom) on 120 consecutive images that contain dynamic objects. mentation 11 which also uses ResNet-101 as backbone. The model is pre-trained on the ImageNet dataset.\n\n\n. https://github.com/facebookresearch/detectron2 Method Road Park Sdwlk Terr Bldg Vegt Car Trler Crvn Gate Wall Fence Box Sky mIoU Acc LA LA+PW 95.4 26.5 38.3 66.2 88.5 83.5 92.0 56.9 89.4 66.2 51.6 50.0 51.2 89.8 67.5 88.2 LA+PW+CO 95.5 70.0 82.5 67.4 89.8 83.7 92.4 87.2 89.0 75.4 57.9 68.3 60.0 89.9 79.2 92.5 LA+PW+CO+3D 95.1 72.7 84.0 67.3 90.3 84.1 92.2 93.1 90.8 77.3 63.0 72.1 56.7 92.8 80.8 93.0 Full Model 95.2 72.9 84.5 67.9 90.3 84.2 92.2 93.4 90.8 78.8 64.3 73.1 56.8 92.8 81.2 93.1 Full Model (90%) 97.5 83.1 92.0 80.3 93.9 90.1 95.1 95.0 93.2 86.3 74.6 81.4 79.9 93.7 88.3 96.0 Full Model (80%) 98.4 89.2 94.2 89.5 96.4 94.2 96.6 96.3 95.5 93.5 80.2 86.4 90.0 95.3 92.5 97.6 Full Model (70%) 98.8 88.8 95.0 92.2 97.7 95.8 97.3 97.1 96.7 96.1 85.2 88.2 95.0 96.3 94.3 98.495.4 34.0 48.5 66.6 88.4 82.8 91.8 54.2 88.9 61.7 53.8 52.0 53.6 89.6 68.7 89.1 \n\nLA+3D \n\n95.3 48.9 75.7 66.6 85.8 81.5 92.1 56.1 91.2 68.3 58.8 45.7 48.4 88.7 71.7 90.1 \n\n\n\nTABLE 10 :\n10Ablation Study on Semantic Segmentation of Static Objects. This table shows the importance of the different\n\nTABLE 11 :\n11Comparison to Label Transfer Baselines on Instance Segmentation on Static Objects. We compare our method to 2D label transfer baselines (top) and to 3D to 2D label transfer baselines (bottom) on 120 consecutive images. Method Road Sdwlk Bldg Wall Fence Pole Trlgt Trsgn Vegt Terr Sky Persn Rider Car Truck Motor Bicyc mIoU class Method Road Sdwlk Bldg Wall Fence Pole Trlgt Trsgn Vegt Terr Sky Persn Rider Car Truck Motor Bicyc mIoU class Method Road Sdwlk Bldg Wall Fence Pole Trlgt Trsgn Vegt Terr Sky Persn Rider Car Truck Motor Bicyc mIoU classFCN [61] \n95.6 84.5 84.1 43.4 38.6 31.1 0.0 38.0 90.6 85.7 91.2 40.5 29.3 94.6 42.4 28.4 0.0 \n54.0 \nPSPNet [116] \n96.6 87.3 87.0 65.0 55.6 40.1 0.0 43.0 92.6 88.4 91.9 55.5 48.3 95.6 60.4 52.1 44.1 \n64.9 \n\n(a) 2D Semantic Segmentation \n\nMethod \nBldg \nPersn \nRider \nCar \nTruck \nMotor \nBicyc \nAP \nResNet50 \n26.5 \n27.2 \n10.9 \n53.2 \n6.2 \n5.3 \n7.3 \n19.5 \nResNet101 \n27.0 \n22.9 \n15.9 \n52.0 \n10.2 \n12.7 \n5.7 \n20.9 \n\n(b) 2D Instance Segmentation \n\nMethod \nBldg \nCar \nAP 50 \nBldg \nCar \nAP 25 \nBoxNet [76] \n8.2 \n0.0 \n4.1 \n46.6 \n0.6 \n23.6 \nVoteNet [76] \n5.7 \n1.1 \n3.4 \n40.3 \n20.9 \n30.6 \n\n(c) 3D Bounding Box Detection \n\nPointNet [77] \n54.2 14.4 28.1 2.7 \n1.4 \n0.3 \n0.0 \n2.2 56.5 16.4 \n-\n0.0 \n-\n19.9 0.0 \n0.0 \n0.0 \n13.1 \nPointNet++ [78] 82.1 66.3 62.1 30.5 24.9 38.3 0.0 23.4 71.2 47.3 \n-\n2.0 \n-\n84.8 0.5 \n1.6 \n0.0 \n35.7 \n\n(d) 3D Semantic Segmentation \n\nMethod \nBldg \nCar \nAP \nPointNet++ [78]+ [30] \n11.5 \n35.9 \n23.7 \nPointGroup [49] \n9.9 \n59.6 \n34.8 \n\n(e) 3D Instance Segmentation \n\nEnc-Dec \n42.0 16.9 16.2 2.3 \n0.1 \n4.7 \n0.0 \n2.6 25.4 9.0 \n-\n0.0 \n-\n16.2 0.0 \n0.0 \n0.0 \n9.1 \n\n(f) Semantic Scene Completion \n\n\n\nTABLE 12 :\n12Additional Quantitative Results for Semantic Scene Understanding. In each table, we show the performance of individual classes with the overall metric in the last column.\n\n\n. https://github.com/yenchenlin/nerf-pytorch 18. https://github.com/google/mipnerf 19. https://github.com/dunbar12138/DSNeRF 20. https://github.com/isl-org/FreeViewSynthesis 21. https://gitlab.inria.fr/sibr/projects/pointbased neural rendering\n\n\n. https://github.com/raulmur/ORB SLAM2 23. https://github.com/PRBonn/semantic suma Method Road Sdwlk Bldg Wall Fence Pole Trsgn Vegt Terr Sky Car Motor mIoU classOriginal \n95.6 85.6 75.2 57.4 55.0 42.7 19.8 93.2 91.8 93.4 95.3 58.5 \n72.0 \nPCL \n92.0 70.4 39.4 29.7 15.3 2.2 \n1.5 67.2 81.7 42.1 31.3 0.0 \n39.4 \nNeRF [64] \n93.2 74.2 58.4 35.1 15.2 16.6 13.8 83.8 64.9 91.6 88.1 1.2 \n53.0 \nmip-NeRF [7] \n93.0 72.9 55.9 30.4 14.8 12.6 9.5 82.0 63.2 91.5 87.9 0.0 \n51.2 \nDS-NeRF [27] \n93.2 75.4 56.5 35.4 19.1 26.5 16.6 83.5 63.9 91.5 89.2 6.5 \n54.8 \nFVS [83] \n95.5 83.2 65.0 54.2 44.0 17.8 33.8 90.5 87.3 91.0 92.6 50.2 \n67.1 \nPBNR [54] \n95.3 82.4 67.1 46.6 44.4 28.8 18.9 90.0 87.7 89.1 88.5 42.2 \n65.1 \n\n\n\nTABLE 13 :\n13Additional Quantitative Results for Novel View Semantic Synthesis.\nACKNOWLEDGMENTSThe authors thank Siyuan Peng, Bernhard Jaeger, Shrisha Bharadwaj, Apratim Bhattacharyya, Paul Henderson, and Zehao Yu for their help in implementing the baselines, Kashyap Chitta, Katja Schwarz, and Yue Wang for proofreading, and SurfingTech for annotating parts of the dataset.\nDaimler urban segmentation dataset. \"Daimler urban segmentation dataset,\" http://www.6d-vision. com/scene-labeling. 3\n\nEfficient interactive annotation of segmentation datasets with polygon-rnn++. D Acuna, H Ling, A Kar, S Fidler, 2018. 4D. Acuna, H. Ling, A. Kar, and S. Fidler, \"Efficient interactive annotation of segmentation datasets with polygon-rnn++,\" 2018. 4\n\nFluid annotation: a human-machine collaboration interface for full image annotation. M Andriluka, J Uijlings, V Ferrari, 2018. 4M. Andriluka, J. Uijlings, and V. Ferrari, \"Fluid annotation: a human-machine collaboration interface for full image annota- tion,\" 2018. 4\n\n. M Aygun, A Osep, M Weber, M Maximov, C Stachniss, J Behley, L Leal-Taixe, 4d panoptic lidar segmentation,\" in CVPR, 2021. 4M. Aygun, A. Osep, M. Weber, M. Maximov, C. Stachniss, J. Behley, and L. Leal-Taixe, \"4d panoptic lidar segmentation,\" in CVPR, 2021. 4\n\nMixture of trees probabilistic graphical model for video segmentation. V Badrinarayanan, I Budvytis, R Cipolla, IJCV. 1101V. Badrinarayanan, I. Budvytis, and R. Cipolla, \"Mixture of trees probabilistic graphical model for video segmentation,\" IJCV, vol. 110, no. 1, pp. 14-29, 2014. 4\n\nLabel propagation in video sequences. V Badrinarayanan, F Galasso, R Cipolla, CVPR. 14V. Badrinarayanan, F. Galasso, and R. Cipolla, \"Label propagation in video sequences,\" in CVPR, 2010. 1, 4\n\nMip-nerf: A multiscale representation for anti-aliasing neural radiance fields. J T Barron, B Mildenhall, M Tancik, P Hedman, R Martin-Brualla, P P Srinivasan, in ICCV, 2021. 14J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin- Brualla, and P. P. Srinivasan, \"Mip-nerf: A multiscale representa- tion for anti-aliasing neural radiance fields,\" in ICCV, 2021. 14,\n\nSemantickitti: A dataset for semantic scene understanding of lidar sequences. J Behley, M Garbade, A Milioto, J Quenzel, S Behnke, C Stachniss, J Gall, ICCV. J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stach- niss, and J. Gall, \"Semantickitti: A dataset for semantic scene understanding of lidar sequences,\" in ICCV, 2019. 1, 3, 4,\n\nPerformance of histogram descriptors for the classification of 3d laser range data in urban environments. J Behley, V Steinhage, A B Cremers, ICRA. 3J. Behley, V. Steinhage, and A. B. Cremers, \"Performance of histogram descriptors for the classification of 3d laser range data in urban environments,\" in ICRA, 2012. 3\n\nThe ball-pivoting algorithm for surface reconstruction. F Bernardini, J Mittleman, H Rushmeier, C Silva, G Taubin, 5F. Bernardini, J. Mittleman, H. Rushmeier, C. Silva, and G. Taubin, \"The ball-pivoting algorithm for surface reconstruction,\" VCG, vol. 5, no. 4, pp. 349-359, 1999. 8,\n\nAn experimental comparison of min-cut/max-flow algorithms for energy minimization in vision. Y Boykov, V Kolmogorov, PAMI. 269Y. Boykov and V. Kolmogorov, \"An experimental comparison of min-cut/max-flow algorithms for energy minimization in vi- sion.\" PAMI, vol. 26, pp. 1124-1137, 2004. 9\n\nSemantic monocular SLAM for highly dynamic environments. N Brasch, A Bozic, J Lallemand, F Tombari, ICRA. 15N. Brasch, A. Bozic, J. Lallemand, and F. Tombari, \"Semantic monocular SLAM for highly dynamic environments,\" in ICRA, 2018. 15\n\nSemantic object classes in video: A high-definition ground truth database. G J Brostow, J Fauqueur, R Cipolla, 30Pattern Recognition LettersG. J. Brostow, J. Fauqueur, and R. Cipolla, \"Semantic object classes in video: A high-definition ground truth database,\" Pat- tern Recognition Letters, vol. 30, no. 2, pp. 88-97, 1 2009. 3\n\nMark yourself: Road marking segmentation via weakly-supervised annotations from multimodal data. T Bruls, W Maddern, A A Morye, P Newman, in ICRA. 4T. Bruls, W. Maddern, A. A. Morye, and P. Newman, \"Mark yourself: Road marking segmentation via weakly-supervised an- notations from multimodal data,\" in ICRA, 2018. 4\n\nLabel propagation in complex video sequences using semi-supervised learning. I Budvytis, V Badrinarayanan, R Cipolla, BMVC. 4I. Budvytis, V. Badrinarayanan, and R. Cipolla, \"Label propaga- tion in complex video sequences using semi-supervised learn- ing,\" in BMVC, 2010. 4\n\nLarge scale labelled video data augmentation for semantic segmentation in driving scenarios. I Budvytis, P Sauer, T Roddick, K Breen, R Cipolla, 2017. 4I. Budvytis, P. Sauer, T. Roddick, K. Breen, and R. Cipolla, \"Large scale labelled video data augmentation for semantic segmenta- tion in driving scenarios,\" in ICCV, 2017. 4\n\nVirtual KITTI 2. Y Cabon, N Murray, M Humenberger, arXiv.org. 4Y. Cabon, N. Murray, and M. Humenberger, \"Virtual KITTI 2,\" arXiv.org, vol. 2001.10773, 2020. 4\n\nnuscenes: A multimodal dataset for autonomous driving. H Caesar, V Bankiti, A H Lang, S Vora, V E Liong, Q Xu, A Krishnan, Y Pan, G Baldan, O Beijbom, CVPR. 412H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \"nuscenes: A multimodal dataset for autonomous driving,\" in CVPR, 2020. 1, 3, 4, 12\n\nAnnotating object instances with a polygon-rnn. L Castrejon, K Kundu, R Urtasun, S Fidler, CVPR. L. Castrejon, K. Kundu, R. Urtasun, and S. Fidler, \"Annotating object instances with a polygon-rnn,\" in CVPR, 2017. 4\n\nMatterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, 3A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang, \"Matterport3d: Learn- ing from rgb-d data in indoor environments,\" in 3DV, 2017. 3\n\nArgoverse: 3d tracking and forecasting with rich maps. M Chang, J Lambert, P Sangkloy, J Singh, S Bak, A Hartnett, D Wang, P Carr, S Lucey, D Ramanan, J Hays, CVPR. M. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett, D. Wang, P. Carr, S. Lucey, D. Ramanan, and J. Hays, \"Argoverse: 3d tracking and forecasting with rich maps,\" in CVPR, 2019. 3\n\nBeat the mturkers: Automatic image labeling from weak 3d supervision. L.-C Chen, S Fidler, A L Yuille, R Urtasun, CVPR. L.-C. Chen, S. Fidler, A. L. Yuille, and R. Urtasun, \"Beat the mturkers: Automatic image labeling from weak 3d supervision,\" in CVPR, 2014. 4\n\nAttention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving. Y Chen, C Dong, P Palanisamy, P Mudalige, K Muelling, J M Dolan, IROS. Y. Chen, C. Dong, P. Palanisamy, P. Mudalige, K. Muelling, and J. M. Dolan, \"Attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving,\" in IROS, 2019. 15,\n\nGeosim: Realistic video simulation via geometry-aware composition for self-driving. Y Chen, F Rong, S Duggal, S Wang, X Yan, S Manivasagam, S Xue, E Yumer, R Urtasun, CVPR. Y. Chen, F. Rong, S. Duggal, S. Wang, X. Yan, S. Manivasagam, S. Xue, E. Yumer, and R. Urtasun, \"Geosim: Realistic video simulation via geometry-aware composition for self-driving,\" in CVPR, 2021. 13\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, CVPR. 12M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, \"The cityscapes dataset for semantic urban scene understanding,\" in CVPR, 2016. 3, 4, 6, 7, 9, 10, 12,\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Niessner, CVPR. 34A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Niessner, \"Scannet: Richly-annotated 3d reconstructions of indoor scenes,\" in CVPR, 2017. 3, 4\n\nDepth-supervised nerf: Fewer views and faster training for free. K Deng, A Liu, J Zhu, D Ramanan, arXiv.org. K. Deng, A. Liu, J. Zhu, and D. Ramanan, \"Depth-supervised nerf: Fewer views and faster training for free,\" arXiv.org, vol. 2107.02791, 2021. 14,\n\nSafe trajectory generation for complex urban environments using spatio-temporal semantic corridor. W Ding, L Zhang, J Chen, S Shen, IEEE Robotics and Automation Letters. 15W. Ding, L. Zhang, J. Chen, and S. Shen, \"Safe trajectory gen- eration for complex urban environments using spatio-temporal semantic corridor,\" IEEE Robotics and Automation Letters, 2019. 15\n\nCARLA: An open urban driving simulator. A Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, 2017. 14A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, \"CARLA: An open urban driving simulator,\" in CoRL, 2017. 14\n\nA density-based algorithm for discovering clusters in large spatial databases with noise. M Ester, H Kriegel, J Sander, X Xu, Proc. of the Second International Conference on Knowledge Discovery and Data Mining (KDD). of the Second International Conference on Knowledge Discovery and Data Mining (KDD)M. Ester, H. Kriegel, J. Sander, and X. Xu, \"A density-based algorithm for discovering clusters in large spatial databases with noise,\" in Proc. of the Second International Conference on Knowledge Discovery and Data Mining (KDD), 1996. 12,\n\nSemantic video cnns through representation warping. R Gadde, V Jampani, P V Gehler, ICCV. R. Gadde, V. Jampani, and P. V. Gehler, \"Semantic video cnns through representation warping,\" in ICCV, 2017. 4\n\nVirtual worlds as proxy for multi-object tracking analysis. A Gaidon, Q Wang, Y Cabon, E Vig, CVPR. A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, \"Virtual worlds as proxy for multi-object tracking analysis,\" in CVPR, 2016. 4\n\nVision meets robotics: The KITTI dataset. A Geiger, P Lenz, C Stiller, R Urtasun, IJRR. 32115A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \"Vision meets robotics: The KITTI dataset,\" IJRR, vol. 32, no. 11, pp. 1231-1237, 2013. 4, 5\n\nAre we ready for autonomous driving? The KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, CVPR. 15A. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for au- tonomous driving? The KITTI vision benchmark suite,\" in CVPR, 2012. 1, 3, 5\n\nJoint 3d object and layout inference from a single rgb-d image. A Geiger, C Wang, GCPR. A. Geiger and C. Wang, \"Joint 3d object and layout inference from a single rgb-d image,\" in GCPR, 2015. 2\n\nJ Geyer, Y Kassahun, M Mahmudi, X Ricou, R Durgesh, A S Chung, L Hauswald, V H Pham, M M\u00fchlegg, S Dorn, T Fernandez, M J\u00e4nicke, S Mirashi, C Savani, M Sturm, O Vorobiov, M Oelker, S Garreis, P Schuberth, A2D2: audi autonomous driving dataset. 12J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung, L. Hauswald, V. H. Pham, M. M\u00fchlegg, S. Dorn, T. Fer- nandez, M. J\u00e4nicke, S. Mirashi, C. Savani, M. Sturm, O. Vorobiov, M. Oelker, S. Garreis, and P. Schuberth, \"A2D2: audi autonomous driving dataset,\" arXiv.org, vol. 2004.06320, 2020. 3, 4, 12\n\nevo: Python package for the evaluation of odometry and slam. M Grupp, M. Grupp, \"evo: Python package for the evaluation of odometry and slam.\" https://github.com/MichaelGrupp/evo, 2017. 15,\n\nImagenet autoannotation with segmentation propagation. M Guillaumin, D K\u00fcttel, V Ferrari, IJCV. 1103M. Guillaumin, D. K\u00fcttel, and V. Ferrari, \"Imagenet auto- annotation with segmentation propagation,\" IJCV, vol. 110, no. 3, pp. 328-348, 2014. 4\n\nSemantic3d.net: A new large-scale point cloud classification benchmark. T Hackel, N Savinov, L Ladicky, J D Wegner, K Schindler, M Pollefeys, APRS. 13T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner, K. Schindler, and M. Pollefeys, \"Semantic3d.net: A new large-scale point cloud classification benchmark,\" in APRS, vol. IV-1-W1, 2017, pp. 91- 98. 3\n\n. K He, G Gkioxari, P Doll\u00e1r, R B Girshick, Mask R-CNN. 42212PAMIK. He, G. Gkioxari, P. Doll\u00e1r, and R. B. Girshick, \"Mask R-CNN,\" PAMI, vol. 42, no. 2, pp. 386-397, 2020. 12\n\nCamodocal: Automatic intrinsic and extrinsic calibration of a rig with multiple generic cameras and odometry. L Heng, B Li, M Pollefeys, IROS. 5L. Heng, B. Li, and M. Pollefeys, \"Camodocal: Automatic intrinsic and extrinsic calibration of a rig with multiple generic cameras and odometry,\" in IROS, 2013. 5\n\nStereo processing by semiglobal matching and mutual information. H Hirschm\u00fcller, PAMI. 302H. Hirschm\u00fcller, \"Stereo processing by semiglobal matching and mutual information,\" PAMI, vol. 30, no. 2, pp. 328-341, 2008. 15,\n\nFcns in the wild: Pixel-level adversarial and constraint-based adaptation. J Hoffman, D Wang, F Yu, T Darrell, 1612.02649arXiv.org. 4J. Hoffman, D. Wang, F. Yu, and T. Darrell, \"Fcns in the wild: Pixel-level adversarial and constraint-based adaptation,\" arXiv.org, vol. 1612.02649, 2016. 4\n\nOctomap: An efficient probabilistic 3d mapping framework based on octrees. A Hornung, K M Wurm, M Bennewitz, C Stachniss, W Burgard, Springer34ARA. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and W. Burgard, \"Octomap: An efficient probabilistic 3d mapping framework based on octrees,\" in AR, vol. 34, no. 3. Springer, 2013, pp. 189-206.\n\nThe apolloscape open dataset for autonomous driving and its application. X Huang, P Wang, X Cheng, D Zhou, Q Geng, R Yang, 612PAMI, 2020. 3, 4X. Huang, P. Wang, X. Cheng, D. Zhou, Q. Geng, and R. Yang, \"The apolloscape open dataset for autonomous driving and its application,\" PAMI, 2020. 3, 4, 6, 12\n\nSupervoxel-consistent foreground propagation in video. S D Jain, K Grauman, ECCV. S. D. Jain and K. Grauman, \"Supervoxel-consistent foreground propagation in video,\" in ECCV, 2014. 4\n\nComputer vision for autonomous vehicles: Problems, datasets and state of the art. J Janai, F G\u00fcney, A Behl, A Geiger, Foundations and Trends in Computer Graphics and Vision. 20201J. Janai, F. G\u00fcney, A. Behl, and A. Geiger, \"Computer vision for autonomous vehicles: Problems, datasets and state of the art,\" Foundations and Trends in Computer Graphics and Vision, 2020. 1\n\nMulti-view reconstruction preserving weakly-supported surfaces. M Jancosek, T Pajdla, CVPR. M. Jancosek and T. Pajdla, \"Multi-view reconstruction preserving weakly-supported surfaces,\" in CVPR, 2011. 8\n\nPointgroup: Dual-set point grouping for 3d instance segmentation. L Jiang, H Zhao, S Shi, S Liu, C Fu, J Jia, CVPR. L. Jiang, H. Zhao, S. Shi, S. Liu, C. Fu, and J. Jia, \"Pointgroup: Dual-set point grouping for 3d instance segmentation,\" in CVPR, 2020. 12, 13,\n\n. R Kesten, M Usman, J Houston, T Pandya, K Nadhamuni, A Ferreira, M Yuan, B Low, A Jain, P Ondruska, S Omari, S Shah, A Kulkarni, A Kazakova, C Tao, L Platinsky, W Jiang, V Shet, Level. 52020R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni, A. Ferreira, M. Yuan, B. Low, A. Jain, P. Ondruska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova, C. Tao, L. Platinsky, W. Jiang, and V. Shet, \"Level 5 perception dataset 2020,\" https://level-5. global/level5/data/, 2019. 3\n\nHuman pose estimation with fields of parts. M Kiefel, P Gehler, ECCV. M. Kiefel and P. Gehler, \"Human pose estimation with fields of parts,\" in ECCV, 2014. 8\n\nVideo panoptic segmentation. D Kim, S Woo, J.-Y. Lee, I S Kweon, CVPR. 2020D. Kim, S. Woo, J.-Y. Lee, and I. S. Kweon, \"Video panoptic segmentation,\" in CVPR, 2020. 3\n\nTanks and temples: Benchmarking large-scale scene reconstruction. A Knapitsch, J Park, Q.-Y Zhou, V Koltun, ACM Trans. on Graphics. 364A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, \"Tanks and temples: Benchmarking large-scale scene reconstruction,\" ACM Trans. on Graphics, vol. 36, no. 4, 2017.\n\nPointbased neural rendering with per-view optimization. G Kopanas, J Philip, T Leimk\u00fchler, G Drettakis, Computer Graphics Forum. 404G. Kopanas, J. Philip, T. Leimk\u00fchler, and G. Drettakis, \"Point- based neural rendering with per-view optimization,\" Computer Graphics Forum, vol. 40, no. 4, pp. 29-43, 2021. 14,\n\nEfficient inference in fully connected CRFs with Gaussian edge potentials. P Kr\u00e4henb\u00fchl, V Koltun, NIPS. P. Kr\u00e4henb\u00fchl and V. Koltun, \"Efficient inference in fully con- nected CRFs with Gaussian edge potentials,\" in NIPS, 2011. 7, 8, 9,\n\nA cross-season correspondence dataset for robust semantic segmentation. M Larsson, E Stenborg, L Hammarstrand, M Pollefeys, T Sattler, F Kahl, CVPR. M. Larsson, E. Stenborg, L. Hammarstrand, M. Pollefeys, T. Sat- tler, and F. Kahl, \"A cross-season correspondence dataset for robust semantic segmentation,\" in CVPR, 2019. 3\n\nTowards unsupervised learning of generative models for 3d controllable image synthesis. Y Liao, K Schwarz, L Mescheder, A Geiger, 2020. 12CVPR. Y. Liao, K. Schwarz, L. Mescheder, and A. Geiger, \"Towards unsupervised learning of generative models for 3d controllable image synthesis,\" in CVPR, 2020. 12\n\n. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, Microsoft coco: Common objects in context,\" in ECCVT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \"Microsoft coco: Common objects in context,\" in ECCV, 2014.\n\nFast interactive object annotation with curve-gcn. H Ling, J Gao, A Kar, W Chen, S Fidler, CVPR. H. Ling, J. Gao, A. Kar, W. Chen, and S. Fidler, \"Fast interactive object annotation with curve-gcn,\" in CVPR, 2019. 4\n\nNonparametric scene parsing via label transfer. C Liu, J Yuen, A Torralba, PAMI. 3312C. Liu, J. Yuen, and A. Torralba, \"Nonparametric scene parsing via label transfer,\" PAMI, vol. 33, no. 12, pp. 2368-2382, 2011. 4\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. 12J. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional net- works for semantic segmentation,\" in CVPR, 2015. 11, 12,\n\nThe bdd-nexar collective: A largescale, crowsourced, dataset of driving scenes. V Madhavan, T Darrell, Master's thesisV. Madhavan and T. Darrell, \"The bdd-nexar collective: A large- scale, crowsourced, dataset of driving scenes,\" Master's thesis, 2017. 3\n\n3d all the way: Semantic segmentation of urban scenes from start to end in 3d. A Martinovi\u0107, J Knopp, H Riemenschneider, L Van Gool, CVPR. A. Martinovi\u0107, J. Knopp, H. Riemenschneider, and L. Van Gool, \"3d all the way: Semantic segmentation of urban scenes from start to end in 3d,\" in CVPR, 2015. 4\n\nNeRF: Representing scenes as neural radiance fields for view synthesis. B Mildenhall, P P Srinivasan, M Tancik, J T Barron, R Ramamoorthi, R Ng, ECCV. B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra- mamoorthi, and R. Ng, \"NeRF: Representing scenes as neural radiance fields for view synthesis,\" in ECCV, 2020. 1, 14,\n\nCo-inference machines for multi-modal scene analysis. D Munoz, J A Bagnell, M Hebert, ECCV. 34D. Munoz, J. A. Bagnell, and M. Hebert, \"Co-inference machines for multi-modal scene analysis,\" in ECCV, 2012. 3, 4\n\nORB-SLAM2: an open-source SLAM system for monocular, stereo, and RGB-D cameras. R Mur-Artal, J D Tard\u00f3s, 33R. Mur-Artal and J. D. Tard\u00f3s, \"ORB-SLAM2: an open-source SLAM system for monocular, stereo, and RGB-D cameras,\" vol. 33, no. 5, pp. 1255-1262, 2017. 15,\n\nSemantically coherent cosegmentation and reconstruction of dynamic scenes. A Mustafa, A Hilton, CVPR. A. Mustafa and A. Hilton, \"Semantically coherent co- segmentation and reconstruction of dynamic scenes,\" in CVPR, 2017. 4\n\nHierarchy of localized random forests for video annotation. N S Nagaraja, P Ochs, K Liu, T Brox, DAGM. N. S. Nagaraja, P. Ochs, K. Liu, and T. Brox, \"Hierarchy of localized random forests for video annotation,\" in DAGM, 2012. 4\n\nA multimodal graphical model for scene analysis. S T Namin, M Najafi, M Salzmann, L Petersson, WACV. 4S. T. Namin, M. Najafi, M. Salzmann, and L. Petersson, \"A multi- modal graphical model for scene analysis,\" in WACV, 2015. 4\n\nThe mapillary vistas dataset for semantic understanding of street scenes. G Neuhold, T Ollmann, S Rota Bul\u00f2, P Kontschieder, 2017. 3G. Neuhold, T. Ollmann, S. Rota Bul\u00f2, and P. Kontschieder, \"The mapillary vistas dataset for semantic understanding of street scenes,\" in ICCV, 2017. 3\n\nQuadricslam: Dual quadrics from object detections as landmarks in objectoriented SLAM. L Nicholson, M Milford, N S\u00fcnderhauf, IEEE Robotics and Automation Letters. 15L. Nicholson, M. Milford, and N. S\u00fcnderhauf, \"Quadricslam: Dual quadrics from object detections as landmarks in object- oriented SLAM,\" IEEE Robotics and Automation Letters, 2019. 15\n\nGiraffe: Representing scenes as compositional generative neural feature fields. M Niemeyer, A Geiger, 2021. 12CVPR. M. Niemeyer and A. Geiger, \"Giraffe: Representing scenes as compositional generative neural feature fields,\" in CVPR, 2021. 12\n\nThe mercator projections. P Osborne, P. Osborne, \"The mercator projections,\" 2008. [Online]. Available: http://mercator.myzen.co.uk/mercator.pdf 5\n\nNeural scene graphs for dynamic scenes. J Ost, F Mannan, N Thuerey, J Knodt, F Heide, 2021. 12CVPR. J. Ost, F. Mannan, N. Thuerey, J. Knodt, and F. Heide, \"Neural scene graphs for dynamic scenes,\" CVPR, 2021. 12\n\nA*3d dataset: Towards autonomous driving in challenging environments. Q.-H Pham, P Sevestre, R S Pahwa, H Zhan, C H Pang, Y Chen, A Mustafa, V Chandrasekhar, J Lin, ICRA. 2020Q.-H. Pham, P. Sevestre, R. S. Pahwa, H. Zhan, C. H. Pang, Y. Chen, A. Mustafa, V. Chandrasekhar, and J. Lin, \"A*3d dataset: Towards autonomous driving in challenging environments,\" in ICRA, 2020. 3\n\nDeep hough voting for 3d object detection in point clouds. C R Qi, O Litany, K He, L J Guibas, ICCV. C. R. Qi, O. Litany, K. He, and L. J. Guibas, \"Deep hough voting for 3d object detection in point clouds,\" in ICCV, 2019. 12, 13,\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. C R Qi, H Su, K Mo, L J Guibas, CVPR. 12C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \"Pointnet: Deep learning on point sets for 3d classification and segmentation,\" in CVPR, 2017. 12,\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, NIPS. C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \"Pointnet++: Deep hierarchical feature learning on point sets in a metric space,\" in NIPS, 2017. 12, 13,\n\nOffboard 3d object detection from point cloud sequences. C R Qi, Y Zhou, M Najibi, P Sun, K Vo, B Deng, D Anguelov, 2021. 4CVPR. C. R. Qi, Y. Zhou, M. Najibi, P. Sun, K. Vo, B. Deng, and D. Anguelov, \"Offboard 3d object detection from point cloud sequences,\" in CVPR, 2021. 4\n\nVip-deeplab: Learning visual perception with depth-aware video panoptic segmentation. S Qiao, Y Zhu, H Adam, A L Yuille, L Chen, CVPR. 34S. Qiao, Y. Zhu, H. Adam, A. L. Yuille, and L. Chen, \"Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation,\" in CVPR, 2021. 3, 4\n\nLabel propagation in rgb-d video. M A Reza, H Zheng, G Georgakis, J Kosecka, IROS. 4M. A. Reza, H. Zheng, G. Georgakis, and J. Kosecka, \"Label propagation in rgb-d video,\" in IROS, 2017. 4\n\nPlaying for data: Ground truth from computer games. S R Richter, V Vineet, S Roth, V Koltun, ECCV. S. R. Richter, V. Vineet, S. Roth, and V. Koltun, \"Playing for data: Ground truth from computer games,\" in ECCV, 2016. 4\n\nFree view synthesis. G Riegler, V Koltun, ECCV. G. Riegler and V. Koltun, \"Free view synthesis,\" in ECCV, 2020. 14,\n\nLearning where to classify in multi-view semantic segmentation. H Riemenschneider, A B\u00f3dis-Szomor\u00fa, J Weissenberg, L V Gool, ECCV. H. Riemenschneider, A. B\u00f3dis-Szomor\u00fa, J. Weissenberg, and L. V. Gool, \"Learning where to classify in multi-view semantic seg- mentation,\" in ECCV, 2014. 3\n\nMachine perception of three-dimensional solids. L G Roberts, Massachusetts Institute of TechnologyPh.D. dissertationL. G. Roberts, \"Machine perception of three-dimensional solids,\" Ph.D. dissertation, Massachusetts Institute of Technology, 1963. 1\n\nThe synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. G Ros, L Sellart, J Materzynska, D Vazquez, A Lopez, CVPR. G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. Lopez, \"The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes,\" in CVPR, 2016. 4\n\nKimera: an open-source library for real-time metric-semantic localization and mapping. A Rosinol, M Abate, Y Chang, L Carlone, 2020. 15ICRA. A. Rosinol, M. Abate, Y. Chang, and L. Carlone, \"Kimera: an open-source library for real-time metric-semantic localization and mapping,\" in ICRA, 2020. 15\n\nParis-lille-3d: A point cloud dataset for urban scene segmentation and classification. X Roynard, J Deschaud, F Goulette, CVPR Workshops. X. Roynard, J. Deschaud, and F. Goulette, \"Paris-lille-3d: A point cloud dataset for urban scene segmentation and classification,\" in CVPR Workshops, 2018. 3\n\nA multi-view stereo benchmark with high-resolution images and multi-camera videos. T Sch\u00f6ps, J L Sch\u00f6nberger, S Galliani, T Sattler, K Schindler, M Pollefeys, A Geiger, 2017. 13T. Sch\u00f6ps, J. L. Sch\u00f6nberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys, and A. Geiger, \"A multi-view stereo benchmark with high-resolution images and multi-camera videos,\" in CVPR, 2017. 13,\n\nOmnidirectional 3d reconstruction in augmented manhattan worlds. M Sch\u00f6nbein, A Geiger, IROS. M. Sch\u00f6nbein and A. Geiger, \"Omnidirectional 3d reconstruction in augmented manhattan worlds,\" in IROS, 2014. 5\n\nCalibrating and centering quasi-central catadioptric cameras. M Sch\u00f6nbein, T Strauss, A Geiger, ICRA. M. Sch\u00f6nbein, T. Strauss, and A. Geiger, \"Calibrating and center- ing quasi-central catadioptric cameras,\" in ICRA, 2014. 5\n\nApplications of computer vision to computer graphics. S M Seitz, R Szeliski, Computer Graphics. 1S. M. Seitz and R. Szeliski, \"Applications of computer vision to computer graphics,\" in Computer Graphics, 1999. 1\n\nSun rgb-d: A rgb-d scene understanding benchmark suite. S Song, S Lichtenberg, J Xiao, CVPR. S. Song, S. Lichtenberg, and J. Xiao., \"Sun rgb-d: A rgb-d scene understanding benchmark suite,\" in CVPR, 2015. 3\n\nScalability in perception for autonomous driving: Waymo open dataset. P Sun, H Kretzschmar, X Dotiwalla, A Chouard, V Patnaik, P Tsui, J Guo, Y Zhou, Y Chai, B Caine, V Vasudevan, W Han, J Ngiam, H Zhao, A Timofeev, S Ettinger, M Krivokon, A Gao, A Joshi, Y Zhang, J Shlens, Z Chen, D Anguelov, CVPR. 2020P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, \"Scalabil- ity in perception for autonomous driving: Waymo open dataset,\" in CVPR, 2020. 3\n\nDense point trajectories by gpu-accelerated large displacement optical flow. N Sundaram, T Brox, K Keutzer, ECCV. 10N. Sundaram, T. Brox, and K. Keutzer, \"Dense point trajectories by gpu-accelerated large displacement optical flow,\" in ECCV, 2010. 9, 10,\n\nToronto-3d: A large-scale mobile lidar dataset for semantic segmentation of urban roadways. W Tan, N Qin, L Ma, Y Li, J Du, G Cai, K Yang, J Li, CVPR Workshops, 2020. 13W. Tan, N. Qin, L. Ma, Y. Li, J. Du, G. Cai, K. Yang, and J. Li, \"Toronto-3d: A large-scale mobile lidar dataset for semantic segmentation of urban roadways,\" in CVPR Workshops, 2020. 1, 3\n\nLearning to adapt structured output space for semantic segmentation. Y.-H Tsai, W.-C Hung, S Schulter, K Sohn, M.-H Yang, M Chandraker, CVPR. Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker, \"Learning to adapt structured output space for semantic segmentation,\" in CVPR, 2018. 4\n\nLeast-squares estimation of transformation parameters between two point patterns. S Umeyama, PAMI. 134S. Umeyama, \"Least-squares estimation of transformation pa- rameters between two point patterns,\" PAMI, vol. 13, no. 4, pp. 376-380, 1991.\n\nMesh based semantic modelling for indoor and outdoor scenes. J P Valentin, S Sengupta, J Warrell, A Shahrokni, P H Torr, CVPR. J. P. Valentin, S. Sengupta, J. Warrell, A. Shahrokni, and P. H. Torr, \"Mesh based semantic modelling for indoor and outdoor scenes,\" in CVPR, 2013. 3\n\nActive frame selection for label propagation in videos. S Vijayanarasimhan, K Grauman, ECCV. 10S. Vijayanarasimhan and K. Grauman, \"Active frame selection for label propagation in videos,\" in ECCV, 2012. 9, 10,\n\nPosefield: An efficient mean-field based method for joint estimation of human pose, segmentation, and depth. V Vineet, G Sheasby, J Warrell, P H S Torr, EMMCVPR. 8V. Vineet, G. Sheasby, J. Warrell, and P. H. S. Torr, \"Posefield: An efficient mean-field based method for joint estimation of human pose, segmentation, and depth,\" in EMMCVPR, 2013. 8\n\nReal-time fully incremental scene understanding on mobile platforms. J Wald, K Tateno, J Sturm, N Navab, F Tombari, RA-L. 3415J. Wald, K. Tateno, J. Sturm, N. Navab, and F. Tombari, \"Real-time fully incremental scene understanding on mobile platforms,\" RA- L, vol. 3, no. 4, pp. 3402-3409, 2018. 15\n\nSTEP: Segmenting and tracking every pixel. M Weber, J Xie, M Collins, Y Zhu, P Voigtlaender, H Adam, B Green, A Geiger, B Leibe, D Cremers, A Osep, L Leal-Taixe, L.-C Chen, NeurIPS Datasets and Benchmarks. M. Weber, J. Xie, M. Collins, Y. Zhu, P. Voigtlaender, H. Adam, B. Green, A. Geiger, B. Leibe, D. Cremers, A. Osep, L. Leal-Taixe, and L.-C. Chen, \"STEP: Segmenting and tracking every pixel,\" in NeurIPS Datasets and Benchmarks, 2021. 3\n\nHeterarchy in the m.i.t. robot. P H Winston, MIT Artificial Intelligence Laboratory, Tech. Rep. 1P. H. Winston, \"Heterarchy in the m.i.t. robot,\" MIT Artificial Intelligence Laboratory, Tech. Rep., 1971. 1\n\nSUN3D: A database of big spaces reconstructed using sfm and object labels. J Xiao, A Owens, A Torralba, ICCV. J. Xiao, A. Owens, and A. Torralba, \"SUN3D: A database of big spaces reconstructed using sfm and object labels,\" in ICCV, 2013. 3\n\nMultiple view semantic segmentation for street view images. J Xiao, L Quan, ICCV. J. Xiao and L. Quan, \"Multiple view semantic segmentation for street view images.\" in ICCV, 2009. 4\n\nSemantic instance annotation of street scenes by 3d to 2d label transfer. J Xie, M Kiefel, M.-T Sun, A Geiger, CVPR. 11J. Xie, M. Kiefel, M.-T. Sun, and A. Geiger, \"Semantic instance annotation of street scenes by 3d to 2d label transfer,\" in CVPR, 2016. 2, 9, 10, 11,\n\nUpsnet: A unified panoptic segmentation network. Y Xiong, R Liao, H Zhao, R Hu, M Bai, E Yumer, R Urtasun, CVPR. Y. Xiong, R. Liao, H. Zhao, R. Hu, M. Bai, E. Yumer, and R. Urtasun, \"Upsnet: A unified panoptic segmentation network,\" in CVPR, 2019. 7,\n\nCubeslam: Monocular 3-d object SLAM. S Yang, S A Scherer, 3515S. Yang and S. A. Scherer, \"Cubeslam: Monocular 3-d object SLAM,\" vol. 35, no. 4, pp. 925-938, 2019. 15\n\nSurfelgan: Synthesizing realistic sensor data for autonomous driving. Z Yang, Y Chai, D Anguelov, Y Zhou, P Sun, D Erhan, S Rafferty, H Kretzschmar, CVPR. Z. Yang, Y. Chai, D. Anguelov, Y. Zhou, P. Sun, D. Erhan, S. Rafferty, and H. Kretzschmar, \"Surfelgan: Synthesizing realistic sensor data for autonomous driving,\" in CVPR, 2020. 13\n\nWoodscape: A multi-task, multi-camera fisheye dataset for autonomous driving. S Yogamani, C Hughes, J Horgan, G Sistu, P Varley, D O&apos;dea, M Uric\u00e1r, S Milz, M Simon, K Amende, C Witt, H Rashed, ICCV. S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O'Dea, M. Uric\u00e1r, S. Milz, M. Simon, K. Amende, C. Witt, and H. Rashed, \"Woodscape: A multi-task, multi-camera fisheye dataset for au- tonomous driving,\" in ICCV, 2019. 3\n\nAutolabeling 3d objects with differentiable rendering of SDF shape priors. S Zakharov, W Kehl, A Bhargava, A Gaidon, 2020. 4CVPR. S. Zakharov, W. Kehl, A. Bhargava, and A. Gaidon, \"Autolabeling 3d objects with differentiable rendering of SDF shape priors,\" in CVPR, 2020. 4\n\nAdadelta: An adaptive learning rate method. M Zeiler, arXiv.org. 12129M. Zeiler, \"Adadelta: An adaptive learning rate method,\" arXiv.org, vol. 1212.5701, 2012. 9\n\nUnderstanding high-level semantics by modeling traffic patterns. H Zhang, A Geiger, R Urtasun, ICCV. H. Zhang, A. Geiger, and R. Urtasun, \"Understanding high-level semantics by modeling traffic patterns,\" in ICCV, 2013. 2\n\nThe unreasonable effectiveness of deep features as a perceptual metric. R Zhang, P Isola, A A Efros, E Shechtman, O Wang, CVPR. R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \"The unreasonable effectiveness of deep features as a perceptual metric,\" in CVPR, 2018. 14,\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, CVPR. H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \"Pyramid scene parsing network,\" in CVPR, 2017. 7, 9, 12, 14, 15,\n\nDeep feature flow for video recognition. X Zhu, Y Xiong, J Dai, L Yuan, Y Wei, 2017. 4X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, \"Deep feature flow for video recognition,\" in CVPR, 2017. 4\n\nImproving semantic segmentation via video propagation and label relaxation. Y Zhu, K Sapra, F A Reda, K J Shih, S D Newsam, A Tao, B Catanzaro, CVPR. Y. Zhu, K. Sapra, F. A. Reda, K. J. Shih, S. D. Newsam, A. Tao, and B. Catanzaro, \"Improving semantic segmentation via video propagation and label relaxation,\" in CVPR, 2019. 4\n\nDublincity: Annotated lidar point cloud and its applications. S M I Zolanvari, S Ruano, A Rana, A Cummins, R E Silva, M Rahbar, A Smolic, BMVC. 13S. M. I. Zolanvari, S. Ruano, A. Rana, A. Cummins, R. E. da Silva, M. Rahbar, and A. Smolic, \"Dublincity: Annotated lidar point cloud and its applications,\" in BMVC, 2019. 1, 3\n", "annotations": {"author": "[{\"end\":98,\"start\":88},{\"end\":107,\"start\":99},{\"end\":123,\"start\":108}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":93},{\"end\":106,\"start\":103},{\"end\":122,\"start\":116}]", "author_first_name": "[{\"end\":92,\"start\":88},{\"end\":102,\"start\":99},{\"end\":115,\"start\":108}]", "author_affiliation": null, "title": "[{\"end\":85,\"start\":1},{\"end\":208,\"start\":124}]", "venue": null, "abstract": "[{\"end\":1738,\"start\":329}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b84\"},\"end\":1871,\"start\":1867},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":2199,\"start\":2194},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2627,\"start\":2623},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":2718,\"start\":2714},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":2833,\"start\":2829},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3203,\"start\":3199},{\"end\":3395,\"start\":3391},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4399,\"start\":4396},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4419,\"start\":4416},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":4425,\"start\":4421},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":4432,\"start\":4427},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4529,\"start\":4525},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5896,\"start\":5892},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":5903,\"start\":5898},{\"end\":7824,\"start\":7823},{\"end\":7977,\"start\":7976},{\"end\":8183,\"start\":8182},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":8919,\"start\":8914},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":8943,\"start\":8938},{\"end\":9618,\"start\":9611},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9774,\"start\":9770},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9794,\"start\":9791},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9827,\"start\":9823},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9864,\"start\":9860},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9902,\"start\":9898},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9937,\"start\":9933},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":9967,\"start\":9962},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":9997,\"start\":9992},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":10028,\"start\":10024},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10035,\"start\":10031},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10042,\"start\":10038},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10078,\"start\":10074},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10111,\"start\":10107},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11386,\"start\":11382},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11392,\"start\":11388},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":11398,\"start\":11394},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":11405,\"start\":11400},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":11433,\"start\":11429},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11553,\"start\":11549},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11933,\"start\":11930},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11938,\"start\":11935},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11944,\"start\":11940},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11950,\"start\":11946},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11956,\"start\":11952},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11962,\"start\":11958},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11968,\"start\":11964},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11974,\"start\":11970},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11980,\"start\":11976},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11986,\"start\":11982},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":11992,\"start\":11988},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11998,\"start\":11994},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":12004,\"start\":12000},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":12010,\"start\":12006},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":12016,\"start\":12012},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":12022,\"start\":12018},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":12028,\"start\":12024},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":12035,\"start\":12030},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":12042,\"start\":12037},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":12049,\"start\":12044},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12225,\"start\":12221},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12416,\"start\":12412},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":12442,\"start\":12438},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12600,\"start\":12596},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":12707,\"start\":12702},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12811,\"start\":12807},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":12884,\"start\":12879},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13194,\"start\":13190},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":13437,\"start\":13433},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":13458,\"start\":13454},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":13479,\"start\":13474},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13563,\"start\":13559},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13671,\"start\":13668},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13859,\"start\":13855},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13870,\"start\":13866},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":13882,\"start\":13878},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":13897,\"start\":13893},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13984,\"start\":13983},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14253,\"start\":14249},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14457,\"start\":14453},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14507,\"start\":14506},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14602,\"start\":14601},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14927,\"start\":14923},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14941,\"start\":14937},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15271,\"start\":15267},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":15671,\"start\":15667},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15962,\"start\":15958},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15968,\"start\":15964},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":15974,\"start\":15970},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":15980,\"start\":15976},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16080,\"start\":16076},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":16086,\"start\":16082},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16389,\"start\":16385},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16516,\"start\":16513},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16521,\"start\":16518},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16659,\"start\":16655},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16674,\"start\":16670},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16696,\"start\":16692},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17208,\"start\":17204},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":17214,\"start\":17210},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17245,\"start\":17242},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17250,\"start\":17247},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17256,\"start\":17252},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17262,\"start\":17258},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":17489,\"start\":17485},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":17496,\"start\":17491},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":18073,\"start\":18069},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18166,\"start\":18163},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18175,\"start\":18171},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":18325,\"start\":18320},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":18454,\"start\":18450},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18518,\"start\":18515},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18595,\"start\":18591},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18791,\"start\":18787},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":18946,\"start\":18941},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":19147,\"start\":19143},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19977,\"start\":19973},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20014,\"start\":20010},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20321,\"start\":20317},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20635,\"start\":20631},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20906,\"start\":20902},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":20912,\"start\":20908},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20918,\"start\":20914},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":20924,\"start\":20920},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":20930,\"start\":20926},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":20937,\"start\":20932},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21645,\"start\":21641},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21651,\"start\":21647},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":21918,\"start\":21914},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":21924,\"start\":21920},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22346,\"start\":22342},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22748,\"start\":22747},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22806,\"start\":22802},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22812,\"start\":22808},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23059,\"start\":23058},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":23224,\"start\":23220},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26120,\"start\":26116},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27485,\"start\":27481},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29802,\"start\":29798},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":34633,\"start\":34628},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34814,\"start\":34810},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":35099,\"start\":35094},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":36270,\"start\":36266},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37731,\"start\":37727},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":37853,\"start\":37849},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":38941,\"start\":38937},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":39065,\"start\":39061},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":39072,\"start\":39067},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":39095,\"start\":39091},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":41072,\"start\":41067},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":41908,\"start\":41903},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":41991,\"start\":41987},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":42000,\"start\":41995},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":42500,\"start\":42495},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":43321,\"start\":43316},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43418,\"start\":43414},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":44138,\"start\":44133},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":44389,\"start\":44385},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":44571,\"start\":44567},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":45161,\"start\":45157},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":45210,\"start\":45205},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":45405,\"start\":45400},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":45611,\"start\":45607},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":46502,\"start\":46501},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":46873,\"start\":46869},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":46880,\"start\":46875},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":48647,\"start\":48646},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":48706,\"start\":48701},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":48848,\"start\":48844},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":48860,\"start\":48855},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":52566,\"start\":52565},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":52648,\"start\":52643},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54463,\"start\":54462},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":54476,\"start\":54471},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":56953,\"start\":56949},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":57505,\"start\":57500},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":57548,\"start\":57544},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":58111,\"start\":58107},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":58661,\"start\":58657},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":58667,\"start\":58663},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":58673,\"start\":58669},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":58679,\"start\":58675},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":58756,\"start\":58752},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":58762,\"start\":58758},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":58768,\"start\":58764},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":59055,\"start\":59051},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":59153,\"start\":59149},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":59192,\"start\":59188},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":59619,\"start\":59615},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":59639,\"start\":59635},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":60698,\"start\":60694},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":60740,\"start\":60736},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":60792,\"start\":60788},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":62355,\"start\":62351},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":64863,\"start\":64859},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":64870,\"start\":64865},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":64958,\"start\":64954},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":64971,\"start\":64967},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":64999,\"start\":64994},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":65028,\"start\":65023},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":65041,\"start\":65037},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":65056,\"start\":65051},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":65290,\"start\":65285},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":66221,\"start\":66216},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":66769,\"start\":66766},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":66775,\"start\":66771},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":66781,\"start\":66777},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":66809,\"start\":66805},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":66815,\"start\":66811},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":66930,\"start\":66926},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":67443,\"start\":67439},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":67743,\"start\":67742},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":68387,\"start\":68382},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":70076,\"start\":70072},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":70083,\"start\":70078},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":70227,\"start\":70223},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":70233,\"start\":70229},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":70344,\"start\":70340},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":70781,\"start\":70777},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":70994,\"start\":70990},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":71010,\"start\":71006},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":71555,\"start\":71551},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":71562,\"start\":71557},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":72598,\"start\":72593},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":72652,\"start\":72648},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":75637,\"start\":75633},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":76665,\"start\":76661},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":78052,\"start\":78050},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":91828,\"start\":91823},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":92270,\"start\":92265},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":92301,\"start\":92297},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":95355,\"start\":95351},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":95680,\"start\":95676},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":95729,\"start\":95724},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":96160,\"start\":96156},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":96390,\"start\":96388},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":96678,\"start\":96676},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":96978,\"start\":96976},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":97424,\"start\":97422},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":97865,\"start\":97861},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":97906,\"start\":97902},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":97981,\"start\":97979},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":99260,\"start\":99256},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":99280,\"start\":99276},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":100183,\"start\":100179},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":100371,\"start\":100367},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":100374,\"start\":100372},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":100583,\"start\":100579},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":100625,\"start\":100623},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":102086,\"start\":102084},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":102806,\"start\":102802},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":103069,\"start\":103066},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":104714,\"start\":104709},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":105112,\"start\":105108},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":105125,\"start\":105122},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":105138,\"start\":105134},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":105147,\"start\":105143},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":105157,\"start\":105153},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":105165,\"start\":105163},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":105411,\"start\":105406},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":106023,\"start\":106019},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":106425,\"start\":106422},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":106431,\"start\":106427},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":106437,\"start\":106433},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":106664,\"start\":106662},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":106704,\"start\":106700},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":106746,\"start\":106744},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":106821,\"start\":106817},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":106899,\"start\":106895},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":107932,\"start\":107930},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":108618,\"start\":108614},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":108769,\"start\":108765},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":108884,\"start\":108880},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":108887,\"start\":108885},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":108903,\"start\":108899},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":108906,\"start\":108904},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":109847,\"start\":109843},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":109983,\"start\":109979},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":110124,\"start\":110122},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":110299,\"start\":110294},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":110361,\"start\":110357}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":111154,\"start\":111097},{\"attributes\":{\"id\":\"fig_1\"},\"end\":111319,\"start\":111155},{\"attributes\":{\"id\":\"fig_2\"},\"end\":111767,\"start\":111320},{\"attributes\":{\"id\":\"fig_3\"},\"end\":112310,\"start\":111768},{\"attributes\":{\"id\":\"fig_4\"},\"end\":112423,\"start\":112311},{\"attributes\":{\"id\":\"fig_5\"},\"end\":112661,\"start\":112424},{\"attributes\":{\"id\":\"fig_6\"},\"end\":112687,\"start\":112662},{\"attributes\":{\"id\":\"fig_7\"},\"end\":112793,\"start\":112688},{\"attributes\":{\"id\":\"fig_8\"},\"end\":112850,\"start\":112794},{\"attributes\":{\"id\":\"fig_9\"},\"end\":112882,\"start\":112851},{\"attributes\":{\"id\":\"fig_10\"},\"end\":113113,\"start\":112883},{\"attributes\":{\"id\":\"fig_11\"},\"end\":113251,\"start\":113114},{\"attributes\":{\"id\":\"fig_12\"},\"end\":113628,\"start\":113252},{\"attributes\":{\"id\":\"fig_13\"},\"end\":113784,\"start\":113629},{\"attributes\":{\"id\":\"fig_14\"},\"end\":114795,\"start\":113785},{\"attributes\":{\"id\":\"fig_15\"},\"end\":115043,\"start\":114796},{\"attributes\":{\"id\":\"fig_16\"},\"end\":115171,\"start\":115044},{\"attributes\":{\"id\":\"fig_17\"},\"end\":115389,\"start\":115172},{\"attributes\":{\"id\":\"fig_18\"},\"end\":115661,\"start\":115390},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":116063,\"start\":115662},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":116116,\"start\":116064},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":116202,\"start\":116117},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":116637,\"start\":116203},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":116645,\"start\":116638},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":116738,\"start\":116646},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":116852,\"start\":116739},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":117558,\"start\":116853},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":117641,\"start\":117559},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":117774,\"start\":117642},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":118388,\"start\":117775},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":118761,\"start\":118389},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":119722,\"start\":118762},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":119844,\"start\":119723},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":121504,\"start\":119845},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":121689,\"start\":121505},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":121935,\"start\":121690},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":122639,\"start\":121936},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":122720,\"start\":122640}]", "paragraph": "[{\"end\":2976,\"start\":1754},{\"end\":3384,\"start\":2978},{\"end\":4132,\"start\":3386},{\"end\":4647,\"start\":4134},{\"end\":5812,\"start\":4649},{\"end\":6746,\"start\":5814},{\"end\":7764,\"start\":6748},{\"end\":8420,\"start\":7766},{\"end\":8832,\"start\":8422},{\"end\":9415,\"start\":8834},{\"end\":10897,\"start\":9432},{\"end\":11826,\"start\":11276},{\"end\":13369,\"start\":11828},{\"end\":14143,\"start\":13371},{\"end\":14705,\"start\":14145},{\"end\":14754,\"start\":14707},{\"end\":15101,\"start\":14756},{\"end\":16138,\"start\":15103},{\"end\":17054,\"start\":16154},{\"end\":17688,\"start\":17066},{\"end\":19215,\"start\":17690},{\"end\":19870,\"start\":19217},{\"end\":20850,\"start\":19902},{\"end\":21147,\"start\":20852},{\"end\":21285,\"start\":21162},{\"end\":22113,\"start\":21305},{\"end\":23437,\"start\":22115},{\"end\":23861,\"start\":23462},{\"end\":24334,\"start\":23871},{\"end\":24945,\"start\":24336},{\"end\":25280,\"start\":24947},{\"end\":25732,\"start\":25321},{\"end\":26325,\"start\":25755},{\"end\":26861,\"start\":26355},{\"end\":27350,\"start\":26863},{\"end\":28676,\"start\":27381},{\"end\":29398,\"start\":28701},{\"end\":29803,\"start\":29418},{\"end\":30047,\"start\":29829},{\"end\":30727,\"start\":30060},{\"end\":31389,\"start\":30729},{\"end\":31978,\"start\":31391},{\"end\":32373,\"start\":31988},{\"end\":33166,\"start\":32375},{\"end\":33373,\"start\":33168},{\"end\":33794,\"start\":33512},{\"end\":35521,\"start\":33871},{\"end\":36271,\"start\":35565},{\"end\":36766,\"start\":36423},{\"end\":37374,\"start\":36877},{\"end\":38193,\"start\":37447},{\"end\":39758,\"start\":38220},{\"end\":39910,\"start\":39804},{\"end\":41233,\"start\":39964},{\"end\":41536,\"start\":41263},{\"end\":42898,\"start\":41569},{\"end\":43104,\"start\":42926},{\"end\":43542,\"start\":43139},{\"end\":43885,\"start\":43557},{\"end\":44934,\"start\":43887},{\"end\":45719,\"start\":44936},{\"end\":46439,\"start\":45721},{\"end\":47941,\"start\":46459},{\"end\":49337,\"start\":47962},{\"end\":49864,\"start\":49369},{\"end\":50572,\"start\":49884},{\"end\":51626,\"start\":50574},{\"end\":53123,\"start\":51661},{\"end\":54324,\"start\":53150},{\"end\":55634,\"start\":54349},{\"end\":56750,\"start\":55667},{\"end\":56965,\"start\":56780},{\"end\":58033,\"start\":56979},{\"end\":58797,\"start\":58063},{\"end\":59487,\"start\":58828},{\"end\":60332,\"start\":59517},{\"end\":61709,\"start\":60362},{\"end\":63041,\"start\":61740},{\"end\":63869,\"start\":63043},{\"end\":64505,\"start\":63885},{\"end\":65291,\"start\":64530},{\"end\":65369,\"start\":65293},{\"end\":67346,\"start\":65406},{\"end\":67808,\"start\":67348},{\"end\":68992,\"start\":67819},{\"end\":69711,\"start\":69008},{\"end\":70650,\"start\":69729},{\"end\":71563,\"start\":70668},{\"end\":73613,\"start\":71599},{\"end\":74115,\"start\":73629},{\"end\":74601,\"start\":74130},{\"end\":75407,\"start\":74603},{\"end\":76042,\"start\":75481},{\"end\":76787,\"start\":76087},{\"end\":77224,\"start\":76931},{\"end\":77787,\"start\":77226},{\"end\":78347,\"start\":77814},{\"end\":78457,\"start\":78383},{\"end\":78816,\"start\":78482},{\"end\":79274,\"start\":78834},{\"end\":79493,\"start\":79294},{\"end\":79600,\"start\":79522},{\"end\":79980,\"start\":79637},{\"end\":80270,\"start\":80009},{\"end\":80907,\"start\":80298},{\"end\":81377,\"start\":80932},{\"end\":81747,\"start\":81379},{\"end\":82368,\"start\":81773},{\"end\":83016,\"start\":82402},{\"end\":83872,\"start\":83018},{\"end\":85060,\"start\":83874},{\"end\":85551,\"start\":85186},{\"end\":86350,\"start\":85553},{\"end\":86953,\"start\":86352},{\"end\":87074,\"start\":86955},{\"end\":88225,\"start\":87076},{\"end\":90417,\"start\":88227},{\"end\":91147,\"start\":90459},{\"end\":92145,\"start\":91196},{\"end\":92906,\"start\":92201},{\"end\":93408,\"start\":93000},{\"end\":93947,\"start\":93452},{\"end\":95035,\"start\":94062},{\"end\":95569,\"start\":95076},{\"end\":95926,\"start\":95589},{\"end\":96074,\"start\":95955},{\"end\":96526,\"start\":96146},{\"end\":97348,\"start\":96546},{\"end\":97425,\"start\":97386},{\"end\":97766,\"start\":97498},{\"end\":98082,\"start\":97786},{\"end\":98138,\"start\":98084},{\"end\":98459,\"start\":98140},{\"end\":98806,\"start\":98558},{\"end\":99190,\"start\":98808},{\"end\":99569,\"start\":99210},{\"end\":99984,\"start\":99668},{\"end\":100043,\"start\":99986},{\"end\":100959,\"start\":100063},{\"end\":101937,\"start\":101058},{\"end\":103366,\"start\":101965},{\"end\":104377,\"start\":103386},{\"end\":104715,\"start\":104527},{\"end\":105088,\"start\":104742},{\"end\":105412,\"start\":105090},{\"end\":105687,\"start\":105414},{\"end\":106992,\"start\":105707},{\"end\":107553,\"start\":107021},{\"end\":107933,\"start\":107630},{\"end\":108133,\"start\":107953},{\"end\":108452,\"start\":108162},{\"end\":108838,\"start\":108536},{\"end\":109137,\"start\":108858},{\"end\":110125,\"start\":109227},{\"end\":110511,\"start\":110145},{\"end\":111096,\"start\":110540}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11037,\"start\":10898},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11264,\"start\":11037},{\"attributes\":{\"id\":\"formula_2\"},\"end\":33511,\"start\":33374},{\"attributes\":{\"id\":\"formula_3\"},\"end\":33870,\"start\":33795},{\"attributes\":{\"id\":\"formula_4\"},\"end\":35564,\"start\":35522},{\"attributes\":{\"id\":\"formula_5\"},\"end\":36422,\"start\":36272},{\"attributes\":{\"id\":\"formula_6\"},\"end\":36876,\"start\":36767},{\"attributes\":{\"id\":\"formula_7\"},\"end\":37446,\"start\":37375},{\"attributes\":{\"id\":\"formula_8\"},\"end\":39803,\"start\":39759},{\"attributes\":{\"id\":\"formula_9\"},\"end\":39963,\"start\":39911},{\"attributes\":{\"id\":\"formula_10\"},\"end\":76930,\"start\":76788},{\"attributes\":{\"id\":\"formula_11\"},\"end\":77813,\"start\":77788},{\"attributes\":{\"id\":\"formula_12\"},\"end\":95075,\"start\":95036}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":12096,\"start\":12089},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14668,\"start\":14661},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":43897,\"start\":43890},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":46058,\"start\":46057},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47282,\"start\":47275},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":49988,\"start\":49981},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":50963,\"start\":50956},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":52850,\"start\":52843},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":57955,\"start\":57947},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":58167,\"start\":58159},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":58453,\"start\":58445},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":59327,\"start\":59319},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":59922,\"start\":59914},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":60058,\"start\":60037},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":60907,\"start\":60899},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":61095,\"start\":61073},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":63443,\"start\":63435},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":66835,\"start\":66828},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":68180,\"start\":68173},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":72867,\"start\":72859},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":84466,\"start\":84459},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":93091,\"start\":93072},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":93353,\"start\":93345},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":93366,\"start\":93358},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":95997,\"start\":95988},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":96746,\"start\":96737},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":98295,\"start\":98286},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":99424,\"start\":99415},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":100672,\"start\":100663},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":100838,\"start\":100829},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":104080,\"start\":104071},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":104376,\"start\":104367},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":108227,\"start\":108219}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1752,\"start\":1740},{\"attributes\":{\"n\":\"2\"},\"end\":9430,\"start\":9418},{\"attributes\":{\"n\":\"2.1\"},\"end\":11274,\"start\":11266},{\"end\":16152,\"start\":16141},{\"attributes\":{\"n\":\"2.2\"},\"end\":17064,\"start\":17057},{\"end\":19900,\"start\":19873},{\"attributes\":{\"n\":\"3\"},\"end\":21160,\"start\":21150},{\"attributes\":{\"n\":\"3.1\"},\"end\":21303,\"start\":21288},{\"attributes\":{\"n\":\"3.2\"},\"end\":23460,\"start\":23440},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":23869,\"start\":23864},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":25319,\"start\":25283},{\"attributes\":{\"n\":\"3.3\"},\"end\":25753,\"start\":25735},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":26353,\"start\":26328},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":27379,\"start\":27353},{\"attributes\":{\"n\":\"3.4\"},\"end\":28699,\"start\":28679},{\"attributes\":{\"n\":\"3.5\"},\"end\":29416,\"start\":29401},{\"attributes\":{\"n\":\"4\"},\"end\":29827,\"start\":29806},{\"attributes\":{\"n\":\"4.1\"},\"end\":30058,\"start\":30050},{\"attributes\":{\"n\":\"4.2\"},\"end\":31986,\"start\":31981},{\"attributes\":{\"n\":\"4.3\"},\"end\":38218,\"start\":38196},{\"attributes\":{\"n\":\"5\"},\"end\":41261,\"start\":41236},{\"attributes\":{\"n\":\"5.1\"},\"end\":41567,\"start\":41539},{\"attributes\":{\"n\":\"5.2\"},\"end\":42924,\"start\":42901},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":43137,\"start\":43107},{\"end\":43555,\"start\":43545},{\"end\":46457,\"start\":46442},{\"end\":47960,\"start\":47944},{\"end\":49367,\"start\":49340},{\"end\":49882,\"start\":49867},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":51659,\"start\":51629},{\"attributes\":{\"n\":\"5.3\"},\"end\":53148,\"start\":53126},{\"attributes\":{\"n\":\"6\"},\"end\":54347,\"start\":54327},{\"attributes\":{\"n\":\"6.1\"},\"end\":55665,\"start\":55637},{\"end\":56778,\"start\":56753},{\"end\":56977,\"start\":56968},{\"end\":58061,\"start\":58036},{\"end\":58826,\"start\":58800},{\"end\":59515,\"start\":59490},{\"end\":60360,\"start\":60335},{\"end\":61738,\"start\":61712},{\"end\":63883,\"start\":63872},{\"attributes\":{\"n\":\"6.2\"},\"end\":64528,\"start\":64508},{\"end\":65404,\"start\":65372},{\"end\":67817,\"start\":67811},{\"end\":69006,\"start\":68995},{\"attributes\":{\"n\":\"6.3\"},\"end\":69727,\"start\":69714},{\"end\":70666,\"start\":70653},{\"end\":71597,\"start\":71566},{\"end\":73627,\"start\":73616},{\"attributes\":{\"n\":\"7\"},\"end\":74128,\"start\":74118},{\"end\":75448,\"start\":75410},{\"end\":75479,\"start\":75451},{\"end\":76085,\"start\":76045},{\"end\":78381,\"start\":78350},{\"end\":78480,\"start\":78460},{\"end\":78832,\"start\":78819},{\"end\":79292,\"start\":79277},{\"end\":79520,\"start\":79496},{\"end\":79635,\"start\":79603},{\"end\":80007,\"start\":79983},{\"end\":80296,\"start\":80273},{\"end\":80930,\"start\":80910},{\"end\":81771,\"start\":81750},{\"end\":82400,\"start\":82371},{\"end\":85090,\"start\":85063},{\"end\":85144,\"start\":85093},{\"end\":85184,\"start\":85147},{\"end\":90457,\"start\":90420},{\"end\":91194,\"start\":91150},{\"end\":92199,\"start\":92148},{\"end\":92998,\"start\":92909},{\"end\":93450,\"start\":93411},{\"end\":93994,\"start\":93950},{\"end\":94040,\"start\":93997},{\"end\":94060,\"start\":94043},{\"end\":95587,\"start\":95572},{\"end\":95953,\"start\":95929},{\"end\":96118,\"start\":96077},{\"end\":96144,\"start\":96121},{\"end\":96544,\"start\":96529},{\"end\":97375,\"start\":97351},{\"end\":97384,\"start\":97378},{\"end\":97470,\"start\":97428},{\"end\":97496,\"start\":97473},{\"end\":97784,\"start\":97769},{\"end\":98486,\"start\":98462},{\"end\":98530,\"start\":98489},{\"end\":98556,\"start\":98533},{\"end\":99208,\"start\":99193},{\"end\":99596,\"start\":99572},{\"end\":99640,\"start\":99599},{\"end\":99666,\"start\":99643},{\"end\":100061,\"start\":100046},{\"end\":100986,\"start\":100962},{\"end\":101031,\"start\":100989},{\"end\":101056,\"start\":101034},{\"end\":101963,\"start\":101940},{\"end\":103384,\"start\":103369},{\"end\":104404,\"start\":104380},{\"end\":104448,\"start\":104407},{\"end\":104499,\"start\":104451},{\"end\":104525,\"start\":104502},{\"end\":104740,\"start\":104718},{\"end\":105705,\"start\":105690},{\"end\":107019,\"start\":106995},{\"end\":107602,\"start\":107556},{\"end\":107628,\"start\":107605},{\"end\":107951,\"start\":107936},{\"end\":108160,\"start\":108136},{\"end\":108489,\"start\":108455},{\"end\":108508,\"start\":108492},{\"end\":108534,\"start\":108511},{\"end\":108856,\"start\":108841},{\"end\":109164,\"start\":109140},{\"end\":109199,\"start\":109167},{\"end\":109225,\"start\":109202},{\"end\":110143,\"start\":110128},{\"end\":110538,\"start\":110514},{\"end\":111106,\"start\":111098},{\"end\":111164,\"start\":111156},{\"end\":111329,\"start\":111321},{\"end\":111785,\"start\":111769},{\"end\":112321,\"start\":112312},{\"end\":112443,\"start\":112425},{\"end\":112672,\"start\":112663},{\"end\":112698,\"start\":112689},{\"end\":112813,\"start\":112795},{\"end\":112861,\"start\":112852},{\"end\":112893,\"start\":112884},{\"end\":113124,\"start\":113115},{\"end\":113633,\"start\":113630},{\"end\":113804,\"start\":113786},{\"end\":114806,\"start\":114797},{\"end\":115054,\"start\":115045},{\"end\":115182,\"start\":115173},{\"end\":115409,\"start\":115391},{\"end\":116074,\"start\":116065},{\"end\":116127,\"start\":116118},{\"end\":116213,\"start\":116204},{\"end\":116644,\"start\":116639},{\"end\":116656,\"start\":116647},{\"end\":116749,\"start\":116740},{\"end\":117569,\"start\":117560},{\"end\":117652,\"start\":117643},{\"end\":117785,\"start\":117776},{\"end\":118399,\"start\":118390},{\"end\":119734,\"start\":119724},{\"end\":119856,\"start\":119846},{\"end\":121516,\"start\":121506},{\"end\":122651,\"start\":122641}]", "table": "[{\"end\":116637,\"start\":116244},{\"end\":117558,\"start\":116893},{\"end\":118388,\"start\":118031},{\"end\":119722,\"start\":119550},{\"end\":121504,\"start\":120407},{\"end\":122639,\"start\":122100}]", "figure_caption": "[{\"end\":111154,\"start\":111108},{\"end\":111319,\"start\":111166},{\"end\":111767,\"start\":111331},{\"end\":112310,\"start\":111788},{\"end\":112423,\"start\":112324},{\"end\":112661,\"start\":112448},{\"end\":112687,\"start\":112675},{\"end\":112793,\"start\":112701},{\"end\":112850,\"start\":112818},{\"end\":112882,\"start\":112864},{\"end\":113113,\"start\":112896},{\"end\":113251,\"start\":113127},{\"end\":113628,\"start\":113254},{\"end\":113784,\"start\":113634},{\"end\":114795,\"start\":113809},{\"end\":115043,\"start\":114809},{\"end\":115171,\"start\":115057},{\"end\":115389,\"start\":115185},{\"end\":115661,\"start\":115414},{\"end\":116063,\"start\":115664},{\"end\":116116,\"start\":116076},{\"end\":116202,\"start\":116129},{\"end\":116244,\"start\":116215},{\"end\":116738,\"start\":116658},{\"end\":116852,\"start\":116751},{\"end\":116893,\"start\":116855},{\"end\":117641,\"start\":117571},{\"end\":117774,\"start\":117654},{\"end\":118031,\"start\":117787},{\"end\":118761,\"start\":118401},{\"end\":119550,\"start\":118764},{\"end\":119844,\"start\":119737},{\"end\":120407,\"start\":119859},{\"end\":121689,\"start\":121519},{\"end\":121935,\"start\":121692},{\"end\":122100,\"start\":121938},{\"end\":122720,\"start\":122654}]", "figure_ref": "[{\"end\":4115,\"start\":4109},{\"end\":5475,\"start\":5469},{\"end\":22079,\"start\":22073},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22691,\"start\":22685},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23436,\"start\":23430},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23588,\"start\":23582},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24944,\"start\":24938},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25083,\"start\":25077},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29341,\"start\":29335},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31131,\"start\":31124},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32067,\"start\":32060},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":39123,\"start\":39116},{\"end\":50837,\"start\":50831},{\"end\":52002,\"start\":51996},{\"end\":56996,\"start\":56990},{\"end\":58464,\"start\":58458},{\"end\":59486,\"start\":59479},{\"end\":59968,\"start\":59961},{\"end\":61040,\"start\":61033},{\"end\":61427,\"start\":61421},{\"end\":61592,\"start\":61579},{\"end\":63454,\"start\":63448},{\"end\":63733,\"start\":63727},{\"end\":66853,\"start\":66847},{\"end\":68675,\"start\":68669},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":72879,\"start\":72872},{\"end\":77356,\"start\":77348},{\"end\":78215,\"start\":78207},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78503,\"start\":78496},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78667,\"start\":78659},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78752,\"start\":78744},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":78984,\"start\":78977},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":79492,\"start\":79485},{\"end\":79948,\"start\":79941},{\"end\":80838,\"start\":80831},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":81425,\"start\":81417},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":82248,\"start\":82241},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":83327,\"start\":83319},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":83871,\"start\":83863},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":84401,\"start\":84394},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":92905,\"start\":92898},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":93556,\"start\":93549},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":93680,\"start\":93673},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":93749,\"start\":93741},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":93762,\"start\":93754},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":93946,\"start\":93938},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":94156,\"start\":94149},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":95568,\"start\":95561},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":98680,\"start\":98673},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":101251,\"start\":101244},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":101473,\"start\":101465},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":101587,\"start\":101579},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":101765,\"start\":101757},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":102948,\"start\":102940},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":107092,\"start\":107078},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":108354,\"start\":108339},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":108965,\"start\":108958},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":110614,\"start\":110607}]", "bib_author_first_name": "[{\"end\":123214,\"start\":123213},{\"end\":123223,\"start\":123222},{\"end\":123231,\"start\":123230},{\"end\":123238,\"start\":123237},{\"end\":123471,\"start\":123470},{\"end\":123484,\"start\":123483},{\"end\":123496,\"start\":123495},{\"end\":123657,\"start\":123656},{\"end\":123666,\"start\":123665},{\"end\":123674,\"start\":123673},{\"end\":123683,\"start\":123682},{\"end\":123694,\"start\":123693},{\"end\":123707,\"start\":123706},{\"end\":123717,\"start\":123716},{\"end\":123988,\"start\":123987},{\"end\":124006,\"start\":124005},{\"end\":124018,\"start\":124017},{\"end\":124241,\"start\":124240},{\"end\":124259,\"start\":124258},{\"end\":124270,\"start\":124269},{\"end\":124477,\"start\":124476},{\"end\":124479,\"start\":124478},{\"end\":124489,\"start\":124488},{\"end\":124503,\"start\":124502},{\"end\":124513,\"start\":124512},{\"end\":124523,\"start\":124522},{\"end\":124541,\"start\":124540},{\"end\":124543,\"start\":124542},{\"end\":124849,\"start\":124848},{\"end\":124859,\"start\":124858},{\"end\":124870,\"start\":124869},{\"end\":124881,\"start\":124880},{\"end\":124892,\"start\":124891},{\"end\":124902,\"start\":124901},{\"end\":124915,\"start\":124914},{\"end\":125227,\"start\":125226},{\"end\":125237,\"start\":125236},{\"end\":125250,\"start\":125249},{\"end\":125252,\"start\":125251},{\"end\":125496,\"start\":125495},{\"end\":125510,\"start\":125509},{\"end\":125523,\"start\":125522},{\"end\":125536,\"start\":125535},{\"end\":125545,\"start\":125544},{\"end\":125818,\"start\":125817},{\"end\":125828,\"start\":125827},{\"end\":126073,\"start\":126072},{\"end\":126083,\"start\":126082},{\"end\":126092,\"start\":126091},{\"end\":126105,\"start\":126104},{\"end\":126328,\"start\":126327},{\"end\":126330,\"start\":126329},{\"end\":126341,\"start\":126340},{\"end\":126353,\"start\":126352},{\"end\":126680,\"start\":126679},{\"end\":126689,\"start\":126688},{\"end\":126700,\"start\":126699},{\"end\":126702,\"start\":126701},{\"end\":126711,\"start\":126710},{\"end\":126977,\"start\":126976},{\"end\":126989,\"start\":126988},{\"end\":127007,\"start\":127006},{\"end\":127267,\"start\":127266},{\"end\":127279,\"start\":127278},{\"end\":127288,\"start\":127287},{\"end\":127299,\"start\":127298},{\"end\":127308,\"start\":127307},{\"end\":127519,\"start\":127518},{\"end\":127528,\"start\":127527},{\"end\":127538,\"start\":127537},{\"end\":127717,\"start\":127716},{\"end\":127727,\"start\":127726},{\"end\":127738,\"start\":127737},{\"end\":127740,\"start\":127739},{\"end\":127748,\"start\":127747},{\"end\":127756,\"start\":127755},{\"end\":127758,\"start\":127757},{\"end\":127767,\"start\":127766},{\"end\":127773,\"start\":127772},{\"end\":127785,\"start\":127784},{\"end\":127792,\"start\":127791},{\"end\":127802,\"start\":127801},{\"end\":128067,\"start\":128066},{\"end\":128080,\"start\":128079},{\"end\":128089,\"start\":128088},{\"end\":128100,\"start\":128099},{\"end\":128298,\"start\":128297},{\"end\":128307,\"start\":128306},{\"end\":128314,\"start\":128313},{\"end\":128328,\"start\":128327},{\"end\":128338,\"start\":128337},{\"end\":128350,\"start\":128349},{\"end\":128359,\"start\":128358},{\"end\":128367,\"start\":128366},{\"end\":128375,\"start\":128374},{\"end\":128623,\"start\":128622},{\"end\":128632,\"start\":128631},{\"end\":128643,\"start\":128642},{\"end\":128655,\"start\":128654},{\"end\":128664,\"start\":128663},{\"end\":128671,\"start\":128670},{\"end\":128683,\"start\":128682},{\"end\":128691,\"start\":128690},{\"end\":128699,\"start\":128698},{\"end\":128708,\"start\":128707},{\"end\":128719,\"start\":128718},{\"end\":129000,\"start\":128996},{\"end\":129008,\"start\":129007},{\"end\":129018,\"start\":129017},{\"end\":129020,\"start\":129019},{\"end\":129030,\"start\":129029},{\"end\":129296,\"start\":129295},{\"end\":129304,\"start\":129303},{\"end\":129312,\"start\":129311},{\"end\":129326,\"start\":129325},{\"end\":129338,\"start\":129337},{\"end\":129350,\"start\":129349},{\"end\":129352,\"start\":129351},{\"end\":129655,\"start\":129654},{\"end\":129663,\"start\":129662},{\"end\":129671,\"start\":129670},{\"end\":129681,\"start\":129680},{\"end\":129689,\"start\":129688},{\"end\":129696,\"start\":129695},{\"end\":129711,\"start\":129710},{\"end\":129718,\"start\":129717},{\"end\":129727,\"start\":129726},{\"end\":130008,\"start\":130007},{\"end\":130018,\"start\":130017},{\"end\":130027,\"start\":130026},{\"end\":130036,\"start\":130035},{\"end\":130047,\"start\":130046},{\"end\":130060,\"start\":130059},{\"end\":130072,\"start\":130071},{\"end\":130082,\"start\":130081},{\"end\":130090,\"start\":130089},{\"end\":130382,\"start\":130381},{\"end\":130389,\"start\":130388},{\"end\":130391,\"start\":130390},{\"end\":130400,\"start\":130399},{\"end\":130409,\"start\":130408},{\"end\":130419,\"start\":130418},{\"end\":130433,\"start\":130432},{\"end\":130678,\"start\":130677},{\"end\":130686,\"start\":130685},{\"end\":130693,\"start\":130692},{\"end\":130700,\"start\":130699},{\"end\":130968,\"start\":130967},{\"end\":130976,\"start\":130975},{\"end\":130985,\"start\":130984},{\"end\":130993,\"start\":130992},{\"end\":131273,\"start\":131272},{\"end\":131288,\"start\":131287},{\"end\":131295,\"start\":131294},{\"end\":131308,\"start\":131307},{\"end\":131317,\"start\":131316},{\"end\":131549,\"start\":131548},{\"end\":131558,\"start\":131557},{\"end\":131569,\"start\":131568},{\"end\":131579,\"start\":131578},{\"end\":132052,\"start\":132051},{\"end\":132061,\"start\":132060},{\"end\":132072,\"start\":132071},{\"end\":132074,\"start\":132073},{\"end\":132262,\"start\":132261},{\"end\":132272,\"start\":132271},{\"end\":132280,\"start\":132279},{\"end\":132289,\"start\":132288},{\"end\":132466,\"start\":132465},{\"end\":132476,\"start\":132475},{\"end\":132484,\"start\":132483},{\"end\":132495,\"start\":132494},{\"end\":132730,\"start\":132729},{\"end\":132740,\"start\":132739},{\"end\":132748,\"start\":132747},{\"end\":132966,\"start\":132965},{\"end\":132976,\"start\":132975},{\"end\":133097,\"start\":133096},{\"end\":133106,\"start\":133105},{\"end\":133118,\"start\":133117},{\"end\":133129,\"start\":133128},{\"end\":133138,\"start\":133137},{\"end\":133149,\"start\":133148},{\"end\":133151,\"start\":133150},{\"end\":133160,\"start\":133159},{\"end\":133172,\"start\":133171},{\"end\":133174,\"start\":133173},{\"end\":133182,\"start\":133181},{\"end\":133193,\"start\":133192},{\"end\":133201,\"start\":133200},{\"end\":133214,\"start\":133213},{\"end\":133225,\"start\":133224},{\"end\":133236,\"start\":133235},{\"end\":133246,\"start\":133245},{\"end\":133255,\"start\":133254},{\"end\":133267,\"start\":133266},{\"end\":133277,\"start\":133276},{\"end\":133288,\"start\":133287},{\"end\":133719,\"start\":133718},{\"end\":133904,\"start\":133903},{\"end\":133918,\"start\":133917},{\"end\":133928,\"start\":133927},{\"end\":134167,\"start\":134166},{\"end\":134177,\"start\":134176},{\"end\":134188,\"start\":134187},{\"end\":134199,\"start\":134198},{\"end\":134201,\"start\":134200},{\"end\":134211,\"start\":134210},{\"end\":134224,\"start\":134223},{\"end\":134446,\"start\":134445},{\"end\":134452,\"start\":134451},{\"end\":134464,\"start\":134463},{\"end\":134474,\"start\":134473},{\"end\":134476,\"start\":134475},{\"end\":134729,\"start\":134728},{\"end\":134737,\"start\":134736},{\"end\":134743,\"start\":134742},{\"end\":134992,\"start\":134991},{\"end\":135222,\"start\":135221},{\"end\":135233,\"start\":135232},{\"end\":135241,\"start\":135240},{\"end\":135247,\"start\":135246},{\"end\":135513,\"start\":135512},{\"end\":135524,\"start\":135523},{\"end\":135526,\"start\":135525},{\"end\":135534,\"start\":135533},{\"end\":135547,\"start\":135546},{\"end\":135560,\"start\":135559},{\"end\":135854,\"start\":135853},{\"end\":135863,\"start\":135862},{\"end\":135871,\"start\":135870},{\"end\":135880,\"start\":135879},{\"end\":135888,\"start\":135887},{\"end\":135896,\"start\":135895},{\"end\":136138,\"start\":136137},{\"end\":136140,\"start\":136139},{\"end\":136148,\"start\":136147},{\"end\":136349,\"start\":136348},{\"end\":136358,\"start\":136357},{\"end\":136367,\"start\":136366},{\"end\":136375,\"start\":136374},{\"end\":136703,\"start\":136702},{\"end\":136715,\"start\":136714},{\"end\":136908,\"start\":136907},{\"end\":136917,\"start\":136916},{\"end\":136925,\"start\":136924},{\"end\":136932,\"start\":136931},{\"end\":136939,\"start\":136938},{\"end\":136945,\"start\":136944},{\"end\":137106,\"start\":137105},{\"end\":137116,\"start\":137115},{\"end\":137125,\"start\":137124},{\"end\":137136,\"start\":137135},{\"end\":137146,\"start\":137145},{\"end\":137159,\"start\":137158},{\"end\":137171,\"start\":137170},{\"end\":137179,\"start\":137178},{\"end\":137186,\"start\":137185},{\"end\":137194,\"start\":137193},{\"end\":137206,\"start\":137205},{\"end\":137215,\"start\":137214},{\"end\":137223,\"start\":137222},{\"end\":137235,\"start\":137234},{\"end\":137247,\"start\":137246},{\"end\":137254,\"start\":137253},{\"end\":137267,\"start\":137266},{\"end\":137276,\"start\":137275},{\"end\":137622,\"start\":137621},{\"end\":137632,\"start\":137631},{\"end\":137766,\"start\":137765},{\"end\":137773,\"start\":137772},{\"end\":137784,\"start\":137779},{\"end\":137791,\"start\":137790},{\"end\":137793,\"start\":137792},{\"end\":137971,\"start\":137970},{\"end\":137984,\"start\":137983},{\"end\":137995,\"start\":137991},{\"end\":138003,\"start\":138002},{\"end\":138261,\"start\":138260},{\"end\":138272,\"start\":138271},{\"end\":138282,\"start\":138281},{\"end\":138296,\"start\":138295},{\"end\":138591,\"start\":138590},{\"end\":138605,\"start\":138604},{\"end\":138826,\"start\":138825},{\"end\":138837,\"start\":138836},{\"end\":138849,\"start\":138848},{\"end\":138865,\"start\":138864},{\"end\":138878,\"start\":138877},{\"end\":138889,\"start\":138888},{\"end\":139166,\"start\":139165},{\"end\":139174,\"start\":139173},{\"end\":139185,\"start\":139184},{\"end\":139198,\"start\":139197},{\"end\":139386,\"start\":139382},{\"end\":139393,\"start\":139392},{\"end\":139402,\"start\":139401},{\"end\":139414,\"start\":139413},{\"end\":139422,\"start\":139421},{\"end\":139432,\"start\":139431},{\"end\":139443,\"start\":139442},{\"end\":139453,\"start\":139452},{\"end\":139455,\"start\":139454},{\"end\":139725,\"start\":139724},{\"end\":139733,\"start\":139732},{\"end\":139740,\"start\":139739},{\"end\":139747,\"start\":139746},{\"end\":139755,\"start\":139754},{\"end\":139939,\"start\":139938},{\"end\":139946,\"start\":139945},{\"end\":139954,\"start\":139953},{\"end\":140163,\"start\":140162},{\"end\":140171,\"start\":140170},{\"end\":140184,\"start\":140183},{\"end\":140406,\"start\":140405},{\"end\":140418,\"start\":140417},{\"end\":140661,\"start\":140660},{\"end\":140675,\"start\":140674},{\"end\":140684,\"start\":140683},{\"end\":140703,\"start\":140702},{\"end\":140954,\"start\":140953},{\"end\":140968,\"start\":140967},{\"end\":140970,\"start\":140969},{\"end\":140984,\"start\":140983},{\"end\":140994,\"start\":140993},{\"end\":140996,\"start\":140995},{\"end\":141006,\"start\":141005},{\"end\":141021,\"start\":141020},{\"end\":141271,\"start\":141270},{\"end\":141280,\"start\":141279},{\"end\":141282,\"start\":141281},{\"end\":141293,\"start\":141292},{\"end\":141508,\"start\":141507},{\"end\":141521,\"start\":141520},{\"end\":141523,\"start\":141522},{\"end\":141765,\"start\":141764},{\"end\":141776,\"start\":141775},{\"end\":141975,\"start\":141974},{\"end\":141977,\"start\":141976},{\"end\":141989,\"start\":141988},{\"end\":141997,\"start\":141996},{\"end\":142004,\"start\":142003},{\"end\":142193,\"start\":142192},{\"end\":142195,\"start\":142194},{\"end\":142204,\"start\":142203},{\"end\":142214,\"start\":142213},{\"end\":142226,\"start\":142225},{\"end\":142446,\"start\":142445},{\"end\":142457,\"start\":142456},{\"end\":142468,\"start\":142467},{\"end\":142473,\"start\":142469},{\"end\":142481,\"start\":142480},{\"end\":142744,\"start\":142743},{\"end\":142757,\"start\":142756},{\"end\":142768,\"start\":142767},{\"end\":143086,\"start\":143085},{\"end\":143098,\"start\":143097},{\"end\":143276,\"start\":143275},{\"end\":143438,\"start\":143437},{\"end\":143445,\"start\":143444},{\"end\":143455,\"start\":143454},{\"end\":143466,\"start\":143465},{\"end\":143475,\"start\":143474},{\"end\":143684,\"start\":143680},{\"end\":143692,\"start\":143691},{\"end\":143704,\"start\":143703},{\"end\":143706,\"start\":143705},{\"end\":143715,\"start\":143714},{\"end\":143723,\"start\":143722},{\"end\":143725,\"start\":143724},{\"end\":143733,\"start\":143732},{\"end\":143741,\"start\":143740},{\"end\":143752,\"start\":143751},{\"end\":143769,\"start\":143768},{\"end\":144045,\"start\":144044},{\"end\":144047,\"start\":144046},{\"end\":144053,\"start\":144052},{\"end\":144063,\"start\":144062},{\"end\":144069,\"start\":144068},{\"end\":144071,\"start\":144070},{\"end\":144296,\"start\":144295},{\"end\":144298,\"start\":144297},{\"end\":144304,\"start\":144303},{\"end\":144310,\"start\":144309},{\"end\":144316,\"start\":144315},{\"end\":144318,\"start\":144317},{\"end\":144558,\"start\":144557},{\"end\":144560,\"start\":144559},{\"end\":144566,\"start\":144565},{\"end\":144572,\"start\":144571},{\"end\":144578,\"start\":144577},{\"end\":144580,\"start\":144579},{\"end\":144801,\"start\":144800},{\"end\":144803,\"start\":144802},{\"end\":144809,\"start\":144808},{\"end\":144817,\"start\":144816},{\"end\":144827,\"start\":144826},{\"end\":144834,\"start\":144833},{\"end\":144840,\"start\":144839},{\"end\":144848,\"start\":144847},{\"end\":145107,\"start\":145106},{\"end\":145115,\"start\":145114},{\"end\":145122,\"start\":145121},{\"end\":145130,\"start\":145129},{\"end\":145132,\"start\":145131},{\"end\":145142,\"start\":145141},{\"end\":145354,\"start\":145353},{\"end\":145356,\"start\":145355},{\"end\":145364,\"start\":145363},{\"end\":145373,\"start\":145372},{\"end\":145386,\"start\":145385},{\"end\":145562,\"start\":145561},{\"end\":145564,\"start\":145563},{\"end\":145575,\"start\":145574},{\"end\":145585,\"start\":145584},{\"end\":145593,\"start\":145592},{\"end\":145752,\"start\":145751},{\"end\":145763,\"start\":145762},{\"end\":145912,\"start\":145911},{\"end\":145931,\"start\":145930},{\"end\":145948,\"start\":145947},{\"end\":145963,\"start\":145962},{\"end\":145965,\"start\":145964},{\"end\":146183,\"start\":146182},{\"end\":146185,\"start\":146184},{\"end\":146487,\"start\":146486},{\"end\":146494,\"start\":146493},{\"end\":146505,\"start\":146504},{\"end\":146520,\"start\":146519},{\"end\":146531,\"start\":146530},{\"end\":146818,\"start\":146817},{\"end\":146829,\"start\":146828},{\"end\":146838,\"start\":146837},{\"end\":146847,\"start\":146846},{\"end\":147115,\"start\":147114},{\"end\":147126,\"start\":147125},{\"end\":147138,\"start\":147137},{\"end\":147408,\"start\":147407},{\"end\":147418,\"start\":147417},{\"end\":147420,\"start\":147419},{\"end\":147435,\"start\":147434},{\"end\":147447,\"start\":147446},{\"end\":147458,\"start\":147457},{\"end\":147471,\"start\":147470},{\"end\":147484,\"start\":147483},{\"end\":147770,\"start\":147769},{\"end\":147783,\"start\":147782},{\"end\":147974,\"start\":147973},{\"end\":147987,\"start\":147986},{\"end\":147998,\"start\":147997},{\"end\":148193,\"start\":148192},{\"end\":148195,\"start\":148194},{\"end\":148204,\"start\":148203},{\"end\":148408,\"start\":148407},{\"end\":148416,\"start\":148415},{\"end\":148431,\"start\":148430},{\"end\":148630,\"start\":148629},{\"end\":148637,\"start\":148636},{\"end\":148652,\"start\":148651},{\"end\":148665,\"start\":148664},{\"end\":148676,\"start\":148675},{\"end\":148687,\"start\":148686},{\"end\":148695,\"start\":148694},{\"end\":148702,\"start\":148701},{\"end\":148710,\"start\":148709},{\"end\":148718,\"start\":148717},{\"end\":148727,\"start\":148726},{\"end\":148740,\"start\":148739},{\"end\":148747,\"start\":148746},{\"end\":148756,\"start\":148755},{\"end\":148764,\"start\":148763},{\"end\":148776,\"start\":148775},{\"end\":148788,\"start\":148787},{\"end\":148800,\"start\":148799},{\"end\":148807,\"start\":148806},{\"end\":148816,\"start\":148815},{\"end\":148825,\"start\":148824},{\"end\":148835,\"start\":148834},{\"end\":148843,\"start\":148842},{\"end\":149286,\"start\":149285},{\"end\":149298,\"start\":149297},{\"end\":149306,\"start\":149305},{\"end\":149557,\"start\":149556},{\"end\":149564,\"start\":149563},{\"end\":149571,\"start\":149570},{\"end\":149577,\"start\":149576},{\"end\":149583,\"start\":149582},{\"end\":149589,\"start\":149588},{\"end\":149596,\"start\":149595},{\"end\":149604,\"start\":149603},{\"end\":149896,\"start\":149892},{\"end\":149907,\"start\":149903},{\"end\":149915,\"start\":149914},{\"end\":149927,\"start\":149926},{\"end\":149938,\"start\":149934},{\"end\":149946,\"start\":149945},{\"end\":150214,\"start\":150213},{\"end\":150435,\"start\":150434},{\"end\":150437,\"start\":150436},{\"end\":150449,\"start\":150448},{\"end\":150461,\"start\":150460},{\"end\":150472,\"start\":150471},{\"end\":150485,\"start\":150484},{\"end\":150487,\"start\":150486},{\"end\":150709,\"start\":150708},{\"end\":150729,\"start\":150728},{\"end\":150974,\"start\":150973},{\"end\":150984,\"start\":150983},{\"end\":150995,\"start\":150994},{\"end\":151006,\"start\":151005},{\"end\":151010,\"start\":151007},{\"end\":151283,\"start\":151282},{\"end\":151291,\"start\":151290},{\"end\":151301,\"start\":151300},{\"end\":151310,\"start\":151309},{\"end\":151319,\"start\":151318},{\"end\":151557,\"start\":151556},{\"end\":151566,\"start\":151565},{\"end\":151573,\"start\":151572},{\"end\":151584,\"start\":151583},{\"end\":151591,\"start\":151590},{\"end\":151607,\"start\":151606},{\"end\":151615,\"start\":151614},{\"end\":151624,\"start\":151623},{\"end\":151634,\"start\":151633},{\"end\":151643,\"start\":151642},{\"end\":151654,\"start\":151653},{\"end\":151662,\"start\":151661},{\"end\":151679,\"start\":151675},{\"end\":151989,\"start\":151988},{\"end\":151991,\"start\":151990},{\"end\":152239,\"start\":152238},{\"end\":152247,\"start\":152246},{\"end\":152256,\"start\":152255},{\"end\":152465,\"start\":152464},{\"end\":152473,\"start\":152472},{\"end\":152662,\"start\":152661},{\"end\":152669,\"start\":152668},{\"end\":152682,\"start\":152678},{\"end\":152689,\"start\":152688},{\"end\":152907,\"start\":152906},{\"end\":152916,\"start\":152915},{\"end\":152924,\"start\":152923},{\"end\":152932,\"start\":152931},{\"end\":152938,\"start\":152937},{\"end\":152945,\"start\":152944},{\"end\":152954,\"start\":152953},{\"end\":153147,\"start\":153146},{\"end\":153155,\"start\":153154},{\"end\":153157,\"start\":153156},{\"end\":153347,\"start\":153346},{\"end\":153355,\"start\":153354},{\"end\":153363,\"start\":153362},{\"end\":153375,\"start\":153374},{\"end\":153383,\"start\":153382},{\"end\":153390,\"start\":153389},{\"end\":153399,\"start\":153398},{\"end\":153411,\"start\":153410},{\"end\":153692,\"start\":153691},{\"end\":153704,\"start\":153703},{\"end\":153714,\"start\":153713},{\"end\":153724,\"start\":153723},{\"end\":153733,\"start\":153732},{\"end\":153743,\"start\":153742},{\"end\":153757,\"start\":153756},{\"end\":153767,\"start\":153766},{\"end\":153775,\"start\":153774},{\"end\":153784,\"start\":153783},{\"end\":153794,\"start\":153793},{\"end\":153802,\"start\":153801},{\"end\":154124,\"start\":154123},{\"end\":154136,\"start\":154135},{\"end\":154144,\"start\":154143},{\"end\":154156,\"start\":154155},{\"end\":154368,\"start\":154367},{\"end\":154552,\"start\":154551},{\"end\":154561,\"start\":154560},{\"end\":154571,\"start\":154570},{\"end\":154782,\"start\":154781},{\"end\":154791,\"start\":154790},{\"end\":154800,\"start\":154799},{\"end\":154802,\"start\":154801},{\"end\":154811,\"start\":154810},{\"end\":154824,\"start\":154823},{\"end\":155023,\"start\":155022},{\"end\":155031,\"start\":155030},{\"end\":155038,\"start\":155037},{\"end\":155044,\"start\":155043},{\"end\":155052,\"start\":155051},{\"end\":155218,\"start\":155217},{\"end\":155225,\"start\":155224},{\"end\":155234,\"start\":155233},{\"end\":155241,\"start\":155240},{\"end\":155249,\"start\":155248},{\"end\":155447,\"start\":155446},{\"end\":155454,\"start\":155453},{\"end\":155463,\"start\":155462},{\"end\":155465,\"start\":155464},{\"end\":155473,\"start\":155472},{\"end\":155475,\"start\":155474},{\"end\":155483,\"start\":155482},{\"end\":155485,\"start\":155484},{\"end\":155495,\"start\":155494},{\"end\":155502,\"start\":155501},{\"end\":155761,\"start\":155760},{\"end\":155765,\"start\":155762},{\"end\":155778,\"start\":155777},{\"end\":155787,\"start\":155786},{\"end\":155795,\"start\":155794},{\"end\":155806,\"start\":155805},{\"end\":155808,\"start\":155807},{\"end\":155817,\"start\":155816},{\"end\":155827,\"start\":155826}]", "bib_author_last_name": "[{\"end\":123220,\"start\":123215},{\"end\":123228,\"start\":123224},{\"end\":123235,\"start\":123232},{\"end\":123245,\"start\":123239},{\"end\":123481,\"start\":123472},{\"end\":123493,\"start\":123485},{\"end\":123504,\"start\":123497},{\"end\":123663,\"start\":123658},{\"end\":123671,\"start\":123667},{\"end\":123680,\"start\":123675},{\"end\":123691,\"start\":123684},{\"end\":123704,\"start\":123695},{\"end\":123714,\"start\":123708},{\"end\":123728,\"start\":123718},{\"end\":124003,\"start\":123989},{\"end\":124015,\"start\":124007},{\"end\":124026,\"start\":124019},{\"end\":124256,\"start\":124242},{\"end\":124267,\"start\":124260},{\"end\":124278,\"start\":124271},{\"end\":124486,\"start\":124480},{\"end\":124500,\"start\":124490},{\"end\":124510,\"start\":124504},{\"end\":124520,\"start\":124514},{\"end\":124538,\"start\":124524},{\"end\":124554,\"start\":124544},{\"end\":124856,\"start\":124850},{\"end\":124867,\"start\":124860},{\"end\":124878,\"start\":124871},{\"end\":124889,\"start\":124882},{\"end\":124899,\"start\":124893},{\"end\":124912,\"start\":124903},{\"end\":124920,\"start\":124916},{\"end\":125234,\"start\":125228},{\"end\":125247,\"start\":125238},{\"end\":125260,\"start\":125253},{\"end\":125507,\"start\":125497},{\"end\":125520,\"start\":125511},{\"end\":125533,\"start\":125524},{\"end\":125542,\"start\":125537},{\"end\":125552,\"start\":125546},{\"end\":125825,\"start\":125819},{\"end\":125839,\"start\":125829},{\"end\":126080,\"start\":126074},{\"end\":126089,\"start\":126084},{\"end\":126102,\"start\":126093},{\"end\":126113,\"start\":126106},{\"end\":126338,\"start\":126331},{\"end\":126350,\"start\":126342},{\"end\":126361,\"start\":126354},{\"end\":126686,\"start\":126681},{\"end\":126697,\"start\":126690},{\"end\":126708,\"start\":126703},{\"end\":126718,\"start\":126712},{\"end\":126986,\"start\":126978},{\"end\":127004,\"start\":126990},{\"end\":127015,\"start\":127008},{\"end\":127276,\"start\":127268},{\"end\":127285,\"start\":127280},{\"end\":127296,\"start\":127289},{\"end\":127305,\"start\":127300},{\"end\":127316,\"start\":127309},{\"end\":127525,\"start\":127520},{\"end\":127535,\"start\":127529},{\"end\":127550,\"start\":127539},{\"end\":127724,\"start\":127718},{\"end\":127735,\"start\":127728},{\"end\":127745,\"start\":127741},{\"end\":127753,\"start\":127749},{\"end\":127764,\"start\":127759},{\"end\":127770,\"start\":127768},{\"end\":127782,\"start\":127774},{\"end\":127789,\"start\":127786},{\"end\":127799,\"start\":127793},{\"end\":127810,\"start\":127803},{\"end\":128077,\"start\":128068},{\"end\":128086,\"start\":128081},{\"end\":128097,\"start\":128090},{\"end\":128107,\"start\":128101},{\"end\":128304,\"start\":128299},{\"end\":128311,\"start\":128308},{\"end\":128325,\"start\":128315},{\"end\":128335,\"start\":128329},{\"end\":128347,\"start\":128339},{\"end\":128356,\"start\":128351},{\"end\":128364,\"start\":128360},{\"end\":128372,\"start\":128368},{\"end\":128381,\"start\":128376},{\"end\":128629,\"start\":128624},{\"end\":128640,\"start\":128633},{\"end\":128652,\"start\":128644},{\"end\":128661,\"start\":128656},{\"end\":128668,\"start\":128665},{\"end\":128680,\"start\":128672},{\"end\":128688,\"start\":128684},{\"end\":128696,\"start\":128692},{\"end\":128705,\"start\":128700},{\"end\":128716,\"start\":128709},{\"end\":128724,\"start\":128720},{\"end\":129005,\"start\":129001},{\"end\":129015,\"start\":129009},{\"end\":129027,\"start\":129021},{\"end\":129038,\"start\":129031},{\"end\":129301,\"start\":129297},{\"end\":129309,\"start\":129305},{\"end\":129323,\"start\":129313},{\"end\":129335,\"start\":129327},{\"end\":129347,\"start\":129339},{\"end\":129358,\"start\":129353},{\"end\":129660,\"start\":129656},{\"end\":129668,\"start\":129664},{\"end\":129678,\"start\":129672},{\"end\":129686,\"start\":129682},{\"end\":129693,\"start\":129690},{\"end\":129708,\"start\":129697},{\"end\":129715,\"start\":129712},{\"end\":129724,\"start\":129719},{\"end\":129735,\"start\":129728},{\"end\":130015,\"start\":130009},{\"end\":130024,\"start\":130019},{\"end\":130033,\"start\":130028},{\"end\":130044,\"start\":130037},{\"end\":130057,\"start\":130048},{\"end\":130069,\"start\":130061},{\"end\":130079,\"start\":130073},{\"end\":130087,\"start\":130083},{\"end\":130098,\"start\":130091},{\"end\":130386,\"start\":130383},{\"end\":130397,\"start\":130392},{\"end\":130406,\"start\":130401},{\"end\":130416,\"start\":130410},{\"end\":130430,\"start\":130420},{\"end\":130442,\"start\":130434},{\"end\":130683,\"start\":130679},{\"end\":130690,\"start\":130687},{\"end\":130697,\"start\":130694},{\"end\":130708,\"start\":130701},{\"end\":130973,\"start\":130969},{\"end\":130982,\"start\":130977},{\"end\":130990,\"start\":130986},{\"end\":130998,\"start\":130994},{\"end\":131285,\"start\":131274},{\"end\":131292,\"start\":131289},{\"end\":131305,\"start\":131296},{\"end\":131314,\"start\":131309},{\"end\":131324,\"start\":131318},{\"end\":131555,\"start\":131550},{\"end\":131566,\"start\":131559},{\"end\":131576,\"start\":131570},{\"end\":131582,\"start\":131580},{\"end\":132058,\"start\":132053},{\"end\":132069,\"start\":132062},{\"end\":132081,\"start\":132075},{\"end\":132269,\"start\":132263},{\"end\":132277,\"start\":132273},{\"end\":132286,\"start\":132281},{\"end\":132293,\"start\":132290},{\"end\":132473,\"start\":132467},{\"end\":132481,\"start\":132477},{\"end\":132492,\"start\":132485},{\"end\":132503,\"start\":132496},{\"end\":132737,\"start\":132731},{\"end\":132745,\"start\":132741},{\"end\":132756,\"start\":132749},{\"end\":132973,\"start\":132967},{\"end\":132981,\"start\":132977},{\"end\":133103,\"start\":133098},{\"end\":133115,\"start\":133107},{\"end\":133126,\"start\":133119},{\"end\":133135,\"start\":133130},{\"end\":133146,\"start\":133139},{\"end\":133157,\"start\":133152},{\"end\":133169,\"start\":133161},{\"end\":133179,\"start\":133175},{\"end\":133190,\"start\":133183},{\"end\":133198,\"start\":133194},{\"end\":133211,\"start\":133202},{\"end\":133222,\"start\":133215},{\"end\":133233,\"start\":133226},{\"end\":133243,\"start\":133237},{\"end\":133252,\"start\":133247},{\"end\":133264,\"start\":133256},{\"end\":133274,\"start\":133268},{\"end\":133285,\"start\":133278},{\"end\":133298,\"start\":133289},{\"end\":133725,\"start\":133720},{\"end\":133915,\"start\":133905},{\"end\":133925,\"start\":133919},{\"end\":133936,\"start\":133929},{\"end\":134174,\"start\":134168},{\"end\":134185,\"start\":134178},{\"end\":134196,\"start\":134189},{\"end\":134208,\"start\":134202},{\"end\":134221,\"start\":134212},{\"end\":134234,\"start\":134225},{\"end\":134449,\"start\":134447},{\"end\":134461,\"start\":134453},{\"end\":134471,\"start\":134465},{\"end\":134485,\"start\":134477},{\"end\":134734,\"start\":134730},{\"end\":134740,\"start\":134738},{\"end\":134753,\"start\":134744},{\"end\":135005,\"start\":134993},{\"end\":135230,\"start\":135223},{\"end\":135238,\"start\":135234},{\"end\":135244,\"start\":135242},{\"end\":135255,\"start\":135248},{\"end\":135521,\"start\":135514},{\"end\":135531,\"start\":135527},{\"end\":135544,\"start\":135535},{\"end\":135557,\"start\":135548},{\"end\":135568,\"start\":135561},{\"end\":135860,\"start\":135855},{\"end\":135868,\"start\":135864},{\"end\":135877,\"start\":135872},{\"end\":135885,\"start\":135881},{\"end\":135893,\"start\":135889},{\"end\":135901,\"start\":135897},{\"end\":136145,\"start\":136141},{\"end\":136156,\"start\":136149},{\"end\":136355,\"start\":136350},{\"end\":136364,\"start\":136359},{\"end\":136372,\"start\":136368},{\"end\":136382,\"start\":136376},{\"end\":136712,\"start\":136704},{\"end\":136722,\"start\":136716},{\"end\":136914,\"start\":136909},{\"end\":136922,\"start\":136918},{\"end\":136929,\"start\":136926},{\"end\":136936,\"start\":136933},{\"end\":136942,\"start\":136940},{\"end\":136949,\"start\":136946},{\"end\":137113,\"start\":137107},{\"end\":137122,\"start\":137117},{\"end\":137133,\"start\":137126},{\"end\":137143,\"start\":137137},{\"end\":137156,\"start\":137147},{\"end\":137168,\"start\":137160},{\"end\":137176,\"start\":137172},{\"end\":137183,\"start\":137180},{\"end\":137191,\"start\":137187},{\"end\":137203,\"start\":137195},{\"end\":137212,\"start\":137207},{\"end\":137220,\"start\":137216},{\"end\":137232,\"start\":137224},{\"end\":137244,\"start\":137236},{\"end\":137251,\"start\":137248},{\"end\":137264,\"start\":137255},{\"end\":137273,\"start\":137268},{\"end\":137281,\"start\":137277},{\"end\":137629,\"start\":137623},{\"end\":137639,\"start\":137633},{\"end\":137770,\"start\":137767},{\"end\":137777,\"start\":137774},{\"end\":137788,\"start\":137785},{\"end\":137799,\"start\":137794},{\"end\":137981,\"start\":137972},{\"end\":137989,\"start\":137985},{\"end\":138000,\"start\":137996},{\"end\":138010,\"start\":138004},{\"end\":138269,\"start\":138262},{\"end\":138279,\"start\":138273},{\"end\":138293,\"start\":138283},{\"end\":138306,\"start\":138297},{\"end\":138602,\"start\":138592},{\"end\":138612,\"start\":138606},{\"end\":138834,\"start\":138827},{\"end\":138846,\"start\":138838},{\"end\":138862,\"start\":138850},{\"end\":138875,\"start\":138866},{\"end\":138886,\"start\":138879},{\"end\":138894,\"start\":138890},{\"end\":139171,\"start\":139167},{\"end\":139182,\"start\":139175},{\"end\":139195,\"start\":139186},{\"end\":139205,\"start\":139199},{\"end\":139390,\"start\":139387},{\"end\":139399,\"start\":139394},{\"end\":139411,\"start\":139403},{\"end\":139419,\"start\":139415},{\"end\":139429,\"start\":139423},{\"end\":139440,\"start\":139433},{\"end\":139450,\"start\":139444},{\"end\":139463,\"start\":139456},{\"end\":139730,\"start\":139726},{\"end\":139737,\"start\":139734},{\"end\":139744,\"start\":139741},{\"end\":139752,\"start\":139748},{\"end\":139762,\"start\":139756},{\"end\":139943,\"start\":139940},{\"end\":139951,\"start\":139947},{\"end\":139963,\"start\":139955},{\"end\":140168,\"start\":140164},{\"end\":140181,\"start\":140172},{\"end\":140192,\"start\":140185},{\"end\":140415,\"start\":140407},{\"end\":140426,\"start\":140419},{\"end\":140672,\"start\":140662},{\"end\":140681,\"start\":140676},{\"end\":140700,\"start\":140685},{\"end\":140712,\"start\":140704},{\"end\":140965,\"start\":140955},{\"end\":140981,\"start\":140971},{\"end\":140991,\"start\":140985},{\"end\":141003,\"start\":140997},{\"end\":141018,\"start\":141007},{\"end\":141024,\"start\":141022},{\"end\":141277,\"start\":141272},{\"end\":141290,\"start\":141283},{\"end\":141300,\"start\":141294},{\"end\":141518,\"start\":141509},{\"end\":141530,\"start\":141524},{\"end\":141773,\"start\":141766},{\"end\":141783,\"start\":141777},{\"end\":141986,\"start\":141978},{\"end\":141994,\"start\":141990},{\"end\":142001,\"start\":141998},{\"end\":142009,\"start\":142005},{\"end\":142201,\"start\":142196},{\"end\":142211,\"start\":142205},{\"end\":142223,\"start\":142215},{\"end\":142236,\"start\":142227},{\"end\":142454,\"start\":142447},{\"end\":142465,\"start\":142458},{\"end\":142478,\"start\":142474},{\"end\":142494,\"start\":142482},{\"end\":142754,\"start\":142745},{\"end\":142765,\"start\":142758},{\"end\":142779,\"start\":142769},{\"end\":143095,\"start\":143087},{\"end\":143105,\"start\":143099},{\"end\":143284,\"start\":143277},{\"end\":143442,\"start\":143439},{\"end\":143452,\"start\":143446},{\"end\":143463,\"start\":143456},{\"end\":143472,\"start\":143467},{\"end\":143481,\"start\":143476},{\"end\":143689,\"start\":143685},{\"end\":143701,\"start\":143693},{\"end\":143712,\"start\":143707},{\"end\":143720,\"start\":143716},{\"end\":143730,\"start\":143726},{\"end\":143738,\"start\":143734},{\"end\":143749,\"start\":143742},{\"end\":143766,\"start\":143753},{\"end\":143773,\"start\":143770},{\"end\":144050,\"start\":144048},{\"end\":144060,\"start\":144054},{\"end\":144066,\"start\":144064},{\"end\":144078,\"start\":144072},{\"end\":144301,\"start\":144299},{\"end\":144307,\"start\":144305},{\"end\":144313,\"start\":144311},{\"end\":144325,\"start\":144319},{\"end\":144563,\"start\":144561},{\"end\":144569,\"start\":144567},{\"end\":144575,\"start\":144573},{\"end\":144587,\"start\":144581},{\"end\":144806,\"start\":144804},{\"end\":144814,\"start\":144810},{\"end\":144824,\"start\":144818},{\"end\":144831,\"start\":144828},{\"end\":144837,\"start\":144835},{\"end\":144845,\"start\":144841},{\"end\":144857,\"start\":144849},{\"end\":145112,\"start\":145108},{\"end\":145119,\"start\":145116},{\"end\":145127,\"start\":145123},{\"end\":145139,\"start\":145133},{\"end\":145147,\"start\":145143},{\"end\":145361,\"start\":145357},{\"end\":145370,\"start\":145365},{\"end\":145383,\"start\":145374},{\"end\":145394,\"start\":145387},{\"end\":145572,\"start\":145565},{\"end\":145582,\"start\":145576},{\"end\":145590,\"start\":145586},{\"end\":145600,\"start\":145594},{\"end\":145760,\"start\":145753},{\"end\":145770,\"start\":145764},{\"end\":145928,\"start\":145913},{\"end\":145945,\"start\":145932},{\"end\":145960,\"start\":145949},{\"end\":145970,\"start\":145966},{\"end\":146193,\"start\":146186},{\"end\":146491,\"start\":146488},{\"end\":146502,\"start\":146495},{\"end\":146517,\"start\":146506},{\"end\":146528,\"start\":146521},{\"end\":146537,\"start\":146532},{\"end\":146826,\"start\":146819},{\"end\":146835,\"start\":146830},{\"end\":146844,\"start\":146839},{\"end\":146855,\"start\":146848},{\"end\":147123,\"start\":147116},{\"end\":147135,\"start\":147127},{\"end\":147147,\"start\":147139},{\"end\":147415,\"start\":147409},{\"end\":147432,\"start\":147421},{\"end\":147444,\"start\":147436},{\"end\":147455,\"start\":147448},{\"end\":147468,\"start\":147459},{\"end\":147481,\"start\":147472},{\"end\":147491,\"start\":147485},{\"end\":147780,\"start\":147771},{\"end\":147790,\"start\":147784},{\"end\":147984,\"start\":147975},{\"end\":147995,\"start\":147988},{\"end\":148005,\"start\":147999},{\"end\":148201,\"start\":148196},{\"end\":148213,\"start\":148205},{\"end\":148413,\"start\":148409},{\"end\":148428,\"start\":148417},{\"end\":148436,\"start\":148432},{\"end\":148634,\"start\":148631},{\"end\":148649,\"start\":148638},{\"end\":148662,\"start\":148653},{\"end\":148673,\"start\":148666},{\"end\":148684,\"start\":148677},{\"end\":148692,\"start\":148688},{\"end\":148699,\"start\":148696},{\"end\":148707,\"start\":148703},{\"end\":148715,\"start\":148711},{\"end\":148724,\"start\":148719},{\"end\":148737,\"start\":148728},{\"end\":148744,\"start\":148741},{\"end\":148753,\"start\":148748},{\"end\":148761,\"start\":148757},{\"end\":148773,\"start\":148765},{\"end\":148785,\"start\":148777},{\"end\":148797,\"start\":148789},{\"end\":148804,\"start\":148801},{\"end\":148813,\"start\":148808},{\"end\":148822,\"start\":148817},{\"end\":148832,\"start\":148826},{\"end\":148840,\"start\":148836},{\"end\":148852,\"start\":148844},{\"end\":149295,\"start\":149287},{\"end\":149303,\"start\":149299},{\"end\":149314,\"start\":149307},{\"end\":149561,\"start\":149558},{\"end\":149568,\"start\":149565},{\"end\":149574,\"start\":149572},{\"end\":149580,\"start\":149578},{\"end\":149586,\"start\":149584},{\"end\":149593,\"start\":149590},{\"end\":149601,\"start\":149597},{\"end\":149607,\"start\":149605},{\"end\":149901,\"start\":149897},{\"end\":149912,\"start\":149908},{\"end\":149924,\"start\":149916},{\"end\":149932,\"start\":149928},{\"end\":149943,\"start\":149939},{\"end\":149957,\"start\":149947},{\"end\":150222,\"start\":150215},{\"end\":150446,\"start\":150438},{\"end\":150458,\"start\":150450},{\"end\":150469,\"start\":150462},{\"end\":150482,\"start\":150473},{\"end\":150492,\"start\":150488},{\"end\":150726,\"start\":150710},{\"end\":150737,\"start\":150730},{\"end\":150981,\"start\":150975},{\"end\":150992,\"start\":150985},{\"end\":151003,\"start\":150996},{\"end\":151015,\"start\":151011},{\"end\":151288,\"start\":151284},{\"end\":151298,\"start\":151292},{\"end\":151307,\"start\":151302},{\"end\":151316,\"start\":151311},{\"end\":151327,\"start\":151320},{\"end\":151563,\"start\":151558},{\"end\":151570,\"start\":151567},{\"end\":151581,\"start\":151574},{\"end\":151588,\"start\":151585},{\"end\":151604,\"start\":151592},{\"end\":151612,\"start\":151608},{\"end\":151621,\"start\":151616},{\"end\":151631,\"start\":151625},{\"end\":151640,\"start\":151635},{\"end\":151651,\"start\":151644},{\"end\":151659,\"start\":151655},{\"end\":151673,\"start\":151663},{\"end\":151684,\"start\":151680},{\"end\":151999,\"start\":151992},{\"end\":152244,\"start\":152240},{\"end\":152253,\"start\":152248},{\"end\":152265,\"start\":152257},{\"end\":152470,\"start\":152466},{\"end\":152478,\"start\":152474},{\"end\":152666,\"start\":152663},{\"end\":152676,\"start\":152670},{\"end\":152686,\"start\":152683},{\"end\":152696,\"start\":152690},{\"end\":152913,\"start\":152908},{\"end\":152921,\"start\":152917},{\"end\":152929,\"start\":152925},{\"end\":152935,\"start\":152933},{\"end\":152942,\"start\":152939},{\"end\":152951,\"start\":152946},{\"end\":152962,\"start\":152955},{\"end\":153152,\"start\":153148},{\"end\":153165,\"start\":153158},{\"end\":153352,\"start\":153348},{\"end\":153360,\"start\":153356},{\"end\":153372,\"start\":153364},{\"end\":153380,\"start\":153376},{\"end\":153387,\"start\":153384},{\"end\":153396,\"start\":153391},{\"end\":153408,\"start\":153400},{\"end\":153423,\"start\":153412},{\"end\":153701,\"start\":153693},{\"end\":153711,\"start\":153705},{\"end\":153721,\"start\":153715},{\"end\":153730,\"start\":153725},{\"end\":153740,\"start\":153734},{\"end\":153754,\"start\":153744},{\"end\":153764,\"start\":153758},{\"end\":153772,\"start\":153768},{\"end\":153781,\"start\":153776},{\"end\":153791,\"start\":153785},{\"end\":153799,\"start\":153795},{\"end\":153809,\"start\":153803},{\"end\":154133,\"start\":154125},{\"end\":154141,\"start\":154137},{\"end\":154153,\"start\":154145},{\"end\":154163,\"start\":154157},{\"end\":154375,\"start\":154369},{\"end\":154558,\"start\":154553},{\"end\":154568,\"start\":154562},{\"end\":154579,\"start\":154572},{\"end\":154788,\"start\":154783},{\"end\":154797,\"start\":154792},{\"end\":154808,\"start\":154803},{\"end\":154821,\"start\":154812},{\"end\":154829,\"start\":154825},{\"end\":155028,\"start\":155024},{\"end\":155035,\"start\":155032},{\"end\":155041,\"start\":155039},{\"end\":155049,\"start\":155045},{\"end\":155056,\"start\":155053},{\"end\":155222,\"start\":155219},{\"end\":155231,\"start\":155226},{\"end\":155238,\"start\":155235},{\"end\":155246,\"start\":155242},{\"end\":155253,\"start\":155250},{\"end\":155451,\"start\":155448},{\"end\":155460,\"start\":155455},{\"end\":155470,\"start\":155466},{\"end\":155480,\"start\":155476},{\"end\":155492,\"start\":155486},{\"end\":155499,\"start\":155496},{\"end\":155512,\"start\":155503},{\"end\":155775,\"start\":155766},{\"end\":155784,\"start\":155779},{\"end\":155792,\"start\":155788},{\"end\":155803,\"start\":155796},{\"end\":155814,\"start\":155809},{\"end\":155824,\"start\":155818},{\"end\":155834,\"start\":155828}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":123133,\"start\":123016},{\"attributes\":{\"doi\":\"2018. 4\",\"id\":\"b1\"},\"end\":123383,\"start\":123135},{\"attributes\":{\"doi\":\"2018. 4\",\"id\":\"b2\"},\"end\":123652,\"start\":123385},{\"attributes\":{\"id\":\"b3\"},\"end\":123914,\"start\":123654},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1912831},\"end\":124200,\"start\":123916},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13340734},\"end\":124394,\"start\":124202},{\"attributes\":{\"id\":\"b6\"},\"end\":124768,\"start\":124396},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":199441943},\"end\":125118,\"start\":124770},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":16443260},\"end\":125437,\"start\":125120},{\"attributes\":{\"id\":\"b9\"},\"end\":125722,\"start\":125439},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5324521},\"end\":126013,\"start\":125724},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":57756021},\"end\":126250,\"start\":126015},{\"attributes\":{\"id\":\"b12\"},\"end\":126580,\"start\":126252},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52290514},\"end\":126897,\"start\":126582},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":16065797},\"end\":127171,\"start\":126899},{\"attributes\":{\"doi\":\"2017. 4\",\"id\":\"b15\"},\"end\":127499,\"start\":127173},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":210942959},\"end\":127659,\"start\":127501},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":85517967},\"end\":128016,\"start\":127661},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2913088},\"end\":128232,\"start\":128018},{\"attributes\":{\"id\":\"b19\"},\"end\":128565,\"start\":128234},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":198162846},\"end\":128924,\"start\":128567},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7968847},\"end\":129187,\"start\":128926},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":198119613},\"end\":129568,\"start\":129189},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":234767654},\"end\":129942,\"start\":129570},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":502946},\"end\":130316,\"start\":129944},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7684883},\"end\":130610,\"start\":130318},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":235743051},\"end\":130866,\"start\":130612},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":195345362},\"end\":131230,\"start\":130868},{\"attributes\":{\"doi\":\"2017. 14\",\"id\":\"b28\"},\"end\":131456,\"start\":131232},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":355163},\"end\":131997,\"start\":131458},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11007811},\"end\":132199,\"start\":131999},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1203247},\"end\":132421,\"start\":132201},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9455111},\"end\":132656,\"start\":132423},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6724907},\"end\":132899,\"start\":132658},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13236460},\"end\":133094,\"start\":132901},{\"attributes\":{\"id\":\"b35\"},\"end\":133655,\"start\":133096},{\"attributes\":{\"id\":\"b36\"},\"end\":133846,\"start\":133657},{\"attributes\":{\"id\":\"b37\"},\"end\":134092,\"start\":133848},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":7526065},\"end\":134441,\"start\":134094},{\"attributes\":{\"id\":\"b39\"},\"end\":134616,\"start\":134443},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14965763},\"end\":134924,\"start\":134618},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":18327083},\"end\":135144,\"start\":134926},{\"attributes\":{\"doi\":\"1612.02649\",\"id\":\"b42\",\"matched_paper_id\":10172006},\"end\":135435,\"start\":135146},{\"attributes\":{\"id\":\"b43\"},\"end\":135778,\"start\":135437},{\"attributes\":{\"id\":\"b44\"},\"end\":136080,\"start\":135780},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":114920},\"end\":136264,\"start\":136082},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":23267374},\"end\":136636,\"start\":136266},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2142078},\"end\":136839,\"start\":136638},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":214795000},\"end\":137101,\"start\":136841},{\"attributes\":{\"id\":\"b49\"},\"end\":137575,\"start\":137103},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":7573669},\"end\":137734,\"start\":137577},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":219965906},\"end\":137902,\"start\":137736},{\"attributes\":{\"id\":\"b52\"},\"end\":138202,\"start\":137904},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":235619435},\"end\":138513,\"start\":138204},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":5574079},\"end\":138751,\"start\":138515},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":81977271},\"end\":139075,\"start\":138753},{\"attributes\":{\"doi\":\"2020. 12\",\"id\":\"b56\",\"matched_paper_id\":209202571},\"end\":139378,\"start\":139077},{\"attributes\":{\"id\":\"b57\"},\"end\":139671,\"start\":139380},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":81977075},\"end\":139888,\"start\":139673},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":18477260},\"end\":140104,\"start\":139890},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":1629541},\"end\":140323,\"start\":140106},{\"attributes\":{\"id\":\"b61\"},\"end\":140579,\"start\":140325},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":4720496},\"end\":140879,\"start\":140581},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":213175590},\"end\":141214,\"start\":140881},{\"attributes\":{\"id\":\"b64\"},\"end\":141425,\"start\":141216},{\"attributes\":{\"id\":\"b65\"},\"end\":141687,\"start\":141427},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":237034},\"end\":141912,\"start\":141689},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":2775661},\"end\":142141,\"start\":141914},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":16747063},\"end\":142369,\"start\":142143},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b69\"},\"end\":142654,\"start\":142371},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":52040030},\"end\":143003,\"start\":142656},{\"attributes\":{\"doi\":\"2021. 12\",\"id\":\"b71\",\"matched_paper_id\":227151657},\"end\":143247,\"start\":143005},{\"attributes\":{\"id\":\"b72\"},\"end\":143395,\"start\":143249},{\"attributes\":{\"doi\":\"2021. 12\",\"id\":\"b73\",\"matched_paper_id\":227118710},\"end\":143608,\"start\":143397},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":202583249},\"end\":143983,\"start\":143610},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":127956465},\"end\":144215,\"start\":143985},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":5115938},\"end\":144475,\"start\":144217},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":1745976},\"end\":144741,\"start\":144477},{\"attributes\":{\"doi\":\"2021. 4\",\"id\":\"b78\",\"matched_paper_id\":232168876},\"end\":145018,\"start\":144743},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":228083552},\"end\":145317,\"start\":145020},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":6423250},\"end\":145507,\"start\":145319},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":5844139},\"end\":145728,\"start\":145509},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":221112229},\"end\":145845,\"start\":145730},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":8630664},\"end\":146132,\"start\":145847},{\"attributes\":{\"id\":\"b84\"},\"end\":146381,\"start\":146134},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":206594095},\"end\":146728,\"start\":146383},{\"attributes\":{\"doi\":\"2020. 15\",\"id\":\"b86\",\"matched_paper_id\":203837063},\"end\":147025,\"start\":146730},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":53367147},\"end\":147322,\"start\":147027},{\"attributes\":{\"doi\":\"2017. 13\",\"id\":\"b88\"},\"end\":147702,\"start\":147324},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":1352238},\"end\":147909,\"start\":147704},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":14504479},\"end\":148136,\"start\":147911},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":59793337},\"end\":148349,\"start\":148138},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":6242669},\"end\":148557,\"start\":148351},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":209140225},\"end\":149206,\"start\":148559},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":13438488},\"end\":149462,\"start\":149208},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":212747686},\"end\":149821,\"start\":149464},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":3556146},\"end\":150129,\"start\":149823},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":206421766},\"end\":150371,\"start\":150131},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":15623791},\"end\":150650,\"start\":150373},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":9340887},\"end\":150862,\"start\":150652},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":1446056},\"end\":151211,\"start\":150864},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":51924299},\"end\":151511,\"start\":151213},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":232014089},\"end\":151954,\"start\":151513},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":112808313},\"end\":152161,\"start\":151956},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":6033252},\"end\":152402,\"start\":152163},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":8066044},\"end\":152585,\"start\":152404},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":1002681},\"end\":152855,\"start\":152587},{\"attributes\":{\"id\":\"b107\",\"matched_paper_id\":58004609},\"end\":153107,\"start\":152857},{\"attributes\":{\"id\":\"b108\"},\"end\":153274,\"start\":153109},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":218571224},\"end\":153611,\"start\":153276},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":146121106},\"end\":154046,\"start\":153613},{\"attributes\":{\"doi\":\"2020. 4\",\"id\":\"b111\",\"matched_paper_id\":208291415},\"end\":154321,\"start\":154048},{\"attributes\":{\"id\":\"b112\",\"matched_paper_id\":7365802},\"end\":154484,\"start\":154323},{\"attributes\":{\"id\":\"b113\",\"matched_paper_id\":2111429},\"end\":154707,\"start\":154486},{\"attributes\":{\"id\":\"b114\",\"matched_paper_id\":4766599},\"end\":154989,\"start\":154709},{\"attributes\":{\"id\":\"b115\",\"matched_paper_id\":5299559},\"end\":155174,\"start\":154991},{\"attributes\":{\"doi\":\"2017. 4\",\"id\":\"b116\"},\"end\":155368,\"start\":155176},{\"attributes\":{\"id\":\"b117\",\"matched_paper_id\":54445471},\"end\":155696,\"start\":155370},{\"attributes\":{\"id\":\"b118\",\"matched_paper_id\":202540484},\"end\":156020,\"start\":155698}]", "bib_title": "[{\"end\":123985,\"start\":123916},{\"end\":124238,\"start\":124202},{\"end\":124846,\"start\":124770},{\"end\":125224,\"start\":125120},{\"end\":125815,\"start\":125724},{\"end\":126070,\"start\":126015},{\"end\":126677,\"start\":126582},{\"end\":126974,\"start\":126899},{\"end\":127516,\"start\":127501},{\"end\":127714,\"start\":127661},{\"end\":128064,\"start\":128018},{\"end\":128620,\"start\":128567},{\"end\":128994,\"start\":128926},{\"end\":129293,\"start\":129189},{\"end\":129652,\"start\":129570},{\"end\":130005,\"start\":129944},{\"end\":130379,\"start\":130318},{\"end\":130675,\"start\":130612},{\"end\":130965,\"start\":130868},{\"end\":131546,\"start\":131458},{\"end\":132049,\"start\":131999},{\"end\":132259,\"start\":132201},{\"end\":132463,\"start\":132423},{\"end\":132727,\"start\":132658},{\"end\":132963,\"start\":132901},{\"end\":133901,\"start\":133848},{\"end\":134164,\"start\":134094},{\"end\":134726,\"start\":134618},{\"end\":134989,\"start\":134926},{\"end\":135219,\"start\":135146},{\"end\":136135,\"start\":136082},{\"end\":136346,\"start\":136266},{\"end\":136700,\"start\":136638},{\"end\":136905,\"start\":136841},{\"end\":137619,\"start\":137577},{\"end\":137763,\"start\":137736},{\"end\":137968,\"start\":137904},{\"end\":138258,\"start\":138204},{\"end\":138588,\"start\":138515},{\"end\":138823,\"start\":138753},{\"end\":139163,\"start\":139077},{\"end\":139722,\"start\":139673},{\"end\":139936,\"start\":139890},{\"end\":140160,\"start\":140106},{\"end\":140658,\"start\":140581},{\"end\":140951,\"start\":140881},{\"end\":141268,\"start\":141216},{\"end\":141762,\"start\":141689},{\"end\":141972,\"start\":141914},{\"end\":142190,\"start\":142143},{\"end\":142741,\"start\":142656},{\"end\":143083,\"start\":143005},{\"end\":143435,\"start\":143397},{\"end\":143678,\"start\":143610},{\"end\":144042,\"start\":143985},{\"end\":144293,\"start\":144217},{\"end\":144555,\"start\":144477},{\"end\":144798,\"start\":144743},{\"end\":145104,\"start\":145020},{\"end\":145351,\"start\":145319},{\"end\":145559,\"start\":145509},{\"end\":145749,\"start\":145730},{\"end\":145909,\"start\":145847},{\"end\":146484,\"start\":146383},{\"end\":146815,\"start\":146730},{\"end\":147112,\"start\":147027},{\"end\":147767,\"start\":147704},{\"end\":147971,\"start\":147911},{\"end\":148190,\"start\":148138},{\"end\":148405,\"start\":148351},{\"end\":148627,\"start\":148559},{\"end\":149283,\"start\":149208},{\"end\":149554,\"start\":149464},{\"end\":149890,\"start\":149823},{\"end\":150211,\"start\":150131},{\"end\":150432,\"start\":150373},{\"end\":150706,\"start\":150652},{\"end\":150971,\"start\":150864},{\"end\":151280,\"start\":151213},{\"end\":151554,\"start\":151513},{\"end\":151986,\"start\":151956},{\"end\":152236,\"start\":152163},{\"end\":152462,\"start\":152404},{\"end\":152659,\"start\":152587},{\"end\":152904,\"start\":152857},{\"end\":153344,\"start\":153276},{\"end\":153689,\"start\":153613},{\"end\":154121,\"start\":154048},{\"end\":154365,\"start\":154323},{\"end\":154549,\"start\":154486},{\"end\":154779,\"start\":154709},{\"end\":155020,\"start\":154991},{\"end\":155444,\"start\":155370},{\"end\":155758,\"start\":155698}]", "bib_author": "[{\"end\":123222,\"start\":123213},{\"end\":123230,\"start\":123222},{\"end\":123237,\"start\":123230},{\"end\":123247,\"start\":123237},{\"end\":123483,\"start\":123470},{\"end\":123495,\"start\":123483},{\"end\":123506,\"start\":123495},{\"end\":123665,\"start\":123656},{\"end\":123673,\"start\":123665},{\"end\":123682,\"start\":123673},{\"end\":123693,\"start\":123682},{\"end\":123706,\"start\":123693},{\"end\":123716,\"start\":123706},{\"end\":123730,\"start\":123716},{\"end\":124005,\"start\":123987},{\"end\":124017,\"start\":124005},{\"end\":124028,\"start\":124017},{\"end\":124258,\"start\":124240},{\"end\":124269,\"start\":124258},{\"end\":124280,\"start\":124269},{\"end\":124488,\"start\":124476},{\"end\":124502,\"start\":124488},{\"end\":124512,\"start\":124502},{\"end\":124522,\"start\":124512},{\"end\":124540,\"start\":124522},{\"end\":124556,\"start\":124540},{\"end\":124858,\"start\":124848},{\"end\":124869,\"start\":124858},{\"end\":124880,\"start\":124869},{\"end\":124891,\"start\":124880},{\"end\":124901,\"start\":124891},{\"end\":124914,\"start\":124901},{\"end\":124922,\"start\":124914},{\"end\":125236,\"start\":125226},{\"end\":125249,\"start\":125236},{\"end\":125262,\"start\":125249},{\"end\":125509,\"start\":125495},{\"end\":125522,\"start\":125509},{\"end\":125535,\"start\":125522},{\"end\":125544,\"start\":125535},{\"end\":125554,\"start\":125544},{\"end\":125827,\"start\":125817},{\"end\":125841,\"start\":125827},{\"end\":126082,\"start\":126072},{\"end\":126091,\"start\":126082},{\"end\":126104,\"start\":126091},{\"end\":126115,\"start\":126104},{\"end\":126340,\"start\":126327},{\"end\":126352,\"start\":126340},{\"end\":126363,\"start\":126352},{\"end\":126688,\"start\":126679},{\"end\":126699,\"start\":126688},{\"end\":126710,\"start\":126699},{\"end\":126720,\"start\":126710},{\"end\":126988,\"start\":126976},{\"end\":127006,\"start\":126988},{\"end\":127017,\"start\":127006},{\"end\":127278,\"start\":127266},{\"end\":127287,\"start\":127278},{\"end\":127298,\"start\":127287},{\"end\":127307,\"start\":127298},{\"end\":127318,\"start\":127307},{\"end\":127527,\"start\":127518},{\"end\":127537,\"start\":127527},{\"end\":127552,\"start\":127537},{\"end\":127726,\"start\":127716},{\"end\":127737,\"start\":127726},{\"end\":127747,\"start\":127737},{\"end\":127755,\"start\":127747},{\"end\":127766,\"start\":127755},{\"end\":127772,\"start\":127766},{\"end\":127784,\"start\":127772},{\"end\":127791,\"start\":127784},{\"end\":127801,\"start\":127791},{\"end\":127812,\"start\":127801},{\"end\":128079,\"start\":128066},{\"end\":128088,\"start\":128079},{\"end\":128099,\"start\":128088},{\"end\":128109,\"start\":128099},{\"end\":128306,\"start\":128297},{\"end\":128313,\"start\":128306},{\"end\":128327,\"start\":128313},{\"end\":128337,\"start\":128327},{\"end\":128349,\"start\":128337},{\"end\":128358,\"start\":128349},{\"end\":128366,\"start\":128358},{\"end\":128374,\"start\":128366},{\"end\":128383,\"start\":128374},{\"end\":128631,\"start\":128622},{\"end\":128642,\"start\":128631},{\"end\":128654,\"start\":128642},{\"end\":128663,\"start\":128654},{\"end\":128670,\"start\":128663},{\"end\":128682,\"start\":128670},{\"end\":128690,\"start\":128682},{\"end\":128698,\"start\":128690},{\"end\":128707,\"start\":128698},{\"end\":128718,\"start\":128707},{\"end\":128726,\"start\":128718},{\"end\":129007,\"start\":128996},{\"end\":129017,\"start\":129007},{\"end\":129029,\"start\":129017},{\"end\":129040,\"start\":129029},{\"end\":129303,\"start\":129295},{\"end\":129311,\"start\":129303},{\"end\":129325,\"start\":129311},{\"end\":129337,\"start\":129325},{\"end\":129349,\"start\":129337},{\"end\":129360,\"start\":129349},{\"end\":129662,\"start\":129654},{\"end\":129670,\"start\":129662},{\"end\":129680,\"start\":129670},{\"end\":129688,\"start\":129680},{\"end\":129695,\"start\":129688},{\"end\":129710,\"start\":129695},{\"end\":129717,\"start\":129710},{\"end\":129726,\"start\":129717},{\"end\":129737,\"start\":129726},{\"end\":130017,\"start\":130007},{\"end\":130026,\"start\":130017},{\"end\":130035,\"start\":130026},{\"end\":130046,\"start\":130035},{\"end\":130059,\"start\":130046},{\"end\":130071,\"start\":130059},{\"end\":130081,\"start\":130071},{\"end\":130089,\"start\":130081},{\"end\":130100,\"start\":130089},{\"end\":130388,\"start\":130381},{\"end\":130399,\"start\":130388},{\"end\":130408,\"start\":130399},{\"end\":130418,\"start\":130408},{\"end\":130432,\"start\":130418},{\"end\":130444,\"start\":130432},{\"end\":130685,\"start\":130677},{\"end\":130692,\"start\":130685},{\"end\":130699,\"start\":130692},{\"end\":130710,\"start\":130699},{\"end\":130975,\"start\":130967},{\"end\":130984,\"start\":130975},{\"end\":130992,\"start\":130984},{\"end\":131000,\"start\":130992},{\"end\":131287,\"start\":131272},{\"end\":131294,\"start\":131287},{\"end\":131307,\"start\":131294},{\"end\":131316,\"start\":131307},{\"end\":131326,\"start\":131316},{\"end\":131557,\"start\":131548},{\"end\":131568,\"start\":131557},{\"end\":131578,\"start\":131568},{\"end\":131584,\"start\":131578},{\"end\":132060,\"start\":132051},{\"end\":132071,\"start\":132060},{\"end\":132083,\"start\":132071},{\"end\":132271,\"start\":132261},{\"end\":132279,\"start\":132271},{\"end\":132288,\"start\":132279},{\"end\":132295,\"start\":132288},{\"end\":132475,\"start\":132465},{\"end\":132483,\"start\":132475},{\"end\":132494,\"start\":132483},{\"end\":132505,\"start\":132494},{\"end\":132739,\"start\":132729},{\"end\":132747,\"start\":132739},{\"end\":132758,\"start\":132747},{\"end\":132975,\"start\":132965},{\"end\":132983,\"start\":132975},{\"end\":133105,\"start\":133096},{\"end\":133117,\"start\":133105},{\"end\":133128,\"start\":133117},{\"end\":133137,\"start\":133128},{\"end\":133148,\"start\":133137},{\"end\":133159,\"start\":133148},{\"end\":133171,\"start\":133159},{\"end\":133181,\"start\":133171},{\"end\":133192,\"start\":133181},{\"end\":133200,\"start\":133192},{\"end\":133213,\"start\":133200},{\"end\":133224,\"start\":133213},{\"end\":133235,\"start\":133224},{\"end\":133245,\"start\":133235},{\"end\":133254,\"start\":133245},{\"end\":133266,\"start\":133254},{\"end\":133276,\"start\":133266},{\"end\":133287,\"start\":133276},{\"end\":133300,\"start\":133287},{\"end\":133727,\"start\":133718},{\"end\":133917,\"start\":133903},{\"end\":133927,\"start\":133917},{\"end\":133938,\"start\":133927},{\"end\":134176,\"start\":134166},{\"end\":134187,\"start\":134176},{\"end\":134198,\"start\":134187},{\"end\":134210,\"start\":134198},{\"end\":134223,\"start\":134210},{\"end\":134236,\"start\":134223},{\"end\":134451,\"start\":134445},{\"end\":134463,\"start\":134451},{\"end\":134473,\"start\":134463},{\"end\":134487,\"start\":134473},{\"end\":134736,\"start\":134728},{\"end\":134742,\"start\":134736},{\"end\":134755,\"start\":134742},{\"end\":135007,\"start\":134991},{\"end\":135232,\"start\":135221},{\"end\":135240,\"start\":135232},{\"end\":135246,\"start\":135240},{\"end\":135257,\"start\":135246},{\"end\":135523,\"start\":135512},{\"end\":135533,\"start\":135523},{\"end\":135546,\"start\":135533},{\"end\":135559,\"start\":135546},{\"end\":135570,\"start\":135559},{\"end\":135862,\"start\":135853},{\"end\":135870,\"start\":135862},{\"end\":135879,\"start\":135870},{\"end\":135887,\"start\":135879},{\"end\":135895,\"start\":135887},{\"end\":135903,\"start\":135895},{\"end\":136147,\"start\":136137},{\"end\":136158,\"start\":136147},{\"end\":136357,\"start\":136348},{\"end\":136366,\"start\":136357},{\"end\":136374,\"start\":136366},{\"end\":136384,\"start\":136374},{\"end\":136714,\"start\":136702},{\"end\":136724,\"start\":136714},{\"end\":136916,\"start\":136907},{\"end\":136924,\"start\":136916},{\"end\":136931,\"start\":136924},{\"end\":136938,\"start\":136931},{\"end\":136944,\"start\":136938},{\"end\":136951,\"start\":136944},{\"end\":137115,\"start\":137105},{\"end\":137124,\"start\":137115},{\"end\":137135,\"start\":137124},{\"end\":137145,\"start\":137135},{\"end\":137158,\"start\":137145},{\"end\":137170,\"start\":137158},{\"end\":137178,\"start\":137170},{\"end\":137185,\"start\":137178},{\"end\":137193,\"start\":137185},{\"end\":137205,\"start\":137193},{\"end\":137214,\"start\":137205},{\"end\":137222,\"start\":137214},{\"end\":137234,\"start\":137222},{\"end\":137246,\"start\":137234},{\"end\":137253,\"start\":137246},{\"end\":137266,\"start\":137253},{\"end\":137275,\"start\":137266},{\"end\":137283,\"start\":137275},{\"end\":137631,\"start\":137621},{\"end\":137641,\"start\":137631},{\"end\":137772,\"start\":137765},{\"end\":137779,\"start\":137772},{\"end\":137790,\"start\":137779},{\"end\":137801,\"start\":137790},{\"end\":137983,\"start\":137970},{\"end\":137991,\"start\":137983},{\"end\":138002,\"start\":137991},{\"end\":138012,\"start\":138002},{\"end\":138271,\"start\":138260},{\"end\":138281,\"start\":138271},{\"end\":138295,\"start\":138281},{\"end\":138308,\"start\":138295},{\"end\":138604,\"start\":138590},{\"end\":138614,\"start\":138604},{\"end\":138836,\"start\":138825},{\"end\":138848,\"start\":138836},{\"end\":138864,\"start\":138848},{\"end\":138877,\"start\":138864},{\"end\":138888,\"start\":138877},{\"end\":138896,\"start\":138888},{\"end\":139173,\"start\":139165},{\"end\":139184,\"start\":139173},{\"end\":139197,\"start\":139184},{\"end\":139207,\"start\":139197},{\"end\":139392,\"start\":139382},{\"end\":139401,\"start\":139392},{\"end\":139413,\"start\":139401},{\"end\":139421,\"start\":139413},{\"end\":139431,\"start\":139421},{\"end\":139442,\"start\":139431},{\"end\":139452,\"start\":139442},{\"end\":139465,\"start\":139452},{\"end\":139732,\"start\":139724},{\"end\":139739,\"start\":139732},{\"end\":139746,\"start\":139739},{\"end\":139754,\"start\":139746},{\"end\":139764,\"start\":139754},{\"end\":139945,\"start\":139938},{\"end\":139953,\"start\":139945},{\"end\":139965,\"start\":139953},{\"end\":140170,\"start\":140162},{\"end\":140183,\"start\":140170},{\"end\":140194,\"start\":140183},{\"end\":140417,\"start\":140405},{\"end\":140428,\"start\":140417},{\"end\":140674,\"start\":140660},{\"end\":140683,\"start\":140674},{\"end\":140702,\"start\":140683},{\"end\":140714,\"start\":140702},{\"end\":140967,\"start\":140953},{\"end\":140983,\"start\":140967},{\"end\":140993,\"start\":140983},{\"end\":141005,\"start\":140993},{\"end\":141020,\"start\":141005},{\"end\":141026,\"start\":141020},{\"end\":141279,\"start\":141270},{\"end\":141292,\"start\":141279},{\"end\":141302,\"start\":141292},{\"end\":141520,\"start\":141507},{\"end\":141532,\"start\":141520},{\"end\":141775,\"start\":141764},{\"end\":141785,\"start\":141775},{\"end\":141988,\"start\":141974},{\"end\":141996,\"start\":141988},{\"end\":142003,\"start\":141996},{\"end\":142011,\"start\":142003},{\"end\":142203,\"start\":142192},{\"end\":142213,\"start\":142203},{\"end\":142225,\"start\":142213},{\"end\":142238,\"start\":142225},{\"end\":142456,\"start\":142445},{\"end\":142467,\"start\":142456},{\"end\":142480,\"start\":142467},{\"end\":142496,\"start\":142480},{\"end\":142756,\"start\":142743},{\"end\":142767,\"start\":142756},{\"end\":142781,\"start\":142767},{\"end\":143097,\"start\":143085},{\"end\":143107,\"start\":143097},{\"end\":143286,\"start\":143275},{\"end\":143444,\"start\":143437},{\"end\":143454,\"start\":143444},{\"end\":143465,\"start\":143454},{\"end\":143474,\"start\":143465},{\"end\":143483,\"start\":143474},{\"end\":143691,\"start\":143680},{\"end\":143703,\"start\":143691},{\"end\":143714,\"start\":143703},{\"end\":143722,\"start\":143714},{\"end\":143732,\"start\":143722},{\"end\":143740,\"start\":143732},{\"end\":143751,\"start\":143740},{\"end\":143768,\"start\":143751},{\"end\":143775,\"start\":143768},{\"end\":144052,\"start\":144044},{\"end\":144062,\"start\":144052},{\"end\":144068,\"start\":144062},{\"end\":144080,\"start\":144068},{\"end\":144303,\"start\":144295},{\"end\":144309,\"start\":144303},{\"end\":144315,\"start\":144309},{\"end\":144327,\"start\":144315},{\"end\":144565,\"start\":144557},{\"end\":144571,\"start\":144565},{\"end\":144577,\"start\":144571},{\"end\":144589,\"start\":144577},{\"end\":144808,\"start\":144800},{\"end\":144816,\"start\":144808},{\"end\":144826,\"start\":144816},{\"end\":144833,\"start\":144826},{\"end\":144839,\"start\":144833},{\"end\":144847,\"start\":144839},{\"end\":144859,\"start\":144847},{\"end\":145114,\"start\":145106},{\"end\":145121,\"start\":145114},{\"end\":145129,\"start\":145121},{\"end\":145141,\"start\":145129},{\"end\":145149,\"start\":145141},{\"end\":145363,\"start\":145353},{\"end\":145372,\"start\":145363},{\"end\":145385,\"start\":145372},{\"end\":145396,\"start\":145385},{\"end\":145574,\"start\":145561},{\"end\":145584,\"start\":145574},{\"end\":145592,\"start\":145584},{\"end\":145602,\"start\":145592},{\"end\":145762,\"start\":145751},{\"end\":145772,\"start\":145762},{\"end\":145930,\"start\":145911},{\"end\":145947,\"start\":145930},{\"end\":145962,\"start\":145947},{\"end\":145972,\"start\":145962},{\"end\":146195,\"start\":146182},{\"end\":146493,\"start\":146486},{\"end\":146504,\"start\":146493},{\"end\":146519,\"start\":146504},{\"end\":146530,\"start\":146519},{\"end\":146539,\"start\":146530},{\"end\":146828,\"start\":146817},{\"end\":146837,\"start\":146828},{\"end\":146846,\"start\":146837},{\"end\":146857,\"start\":146846},{\"end\":147125,\"start\":147114},{\"end\":147137,\"start\":147125},{\"end\":147149,\"start\":147137},{\"end\":147417,\"start\":147407},{\"end\":147434,\"start\":147417},{\"end\":147446,\"start\":147434},{\"end\":147457,\"start\":147446},{\"end\":147470,\"start\":147457},{\"end\":147483,\"start\":147470},{\"end\":147493,\"start\":147483},{\"end\":147782,\"start\":147769},{\"end\":147792,\"start\":147782},{\"end\":147986,\"start\":147973},{\"end\":147997,\"start\":147986},{\"end\":148007,\"start\":147997},{\"end\":148203,\"start\":148192},{\"end\":148215,\"start\":148203},{\"end\":148415,\"start\":148407},{\"end\":148430,\"start\":148415},{\"end\":148438,\"start\":148430},{\"end\":148636,\"start\":148629},{\"end\":148651,\"start\":148636},{\"end\":148664,\"start\":148651},{\"end\":148675,\"start\":148664},{\"end\":148686,\"start\":148675},{\"end\":148694,\"start\":148686},{\"end\":148701,\"start\":148694},{\"end\":148709,\"start\":148701},{\"end\":148717,\"start\":148709},{\"end\":148726,\"start\":148717},{\"end\":148739,\"start\":148726},{\"end\":148746,\"start\":148739},{\"end\":148755,\"start\":148746},{\"end\":148763,\"start\":148755},{\"end\":148775,\"start\":148763},{\"end\":148787,\"start\":148775},{\"end\":148799,\"start\":148787},{\"end\":148806,\"start\":148799},{\"end\":148815,\"start\":148806},{\"end\":148824,\"start\":148815},{\"end\":148834,\"start\":148824},{\"end\":148842,\"start\":148834},{\"end\":148854,\"start\":148842},{\"end\":149297,\"start\":149285},{\"end\":149305,\"start\":149297},{\"end\":149316,\"start\":149305},{\"end\":149563,\"start\":149556},{\"end\":149570,\"start\":149563},{\"end\":149576,\"start\":149570},{\"end\":149582,\"start\":149576},{\"end\":149588,\"start\":149582},{\"end\":149595,\"start\":149588},{\"end\":149603,\"start\":149595},{\"end\":149609,\"start\":149603},{\"end\":149903,\"start\":149892},{\"end\":149914,\"start\":149903},{\"end\":149926,\"start\":149914},{\"end\":149934,\"start\":149926},{\"end\":149945,\"start\":149934},{\"end\":149959,\"start\":149945},{\"end\":150224,\"start\":150213},{\"end\":150448,\"start\":150434},{\"end\":150460,\"start\":150448},{\"end\":150471,\"start\":150460},{\"end\":150484,\"start\":150471},{\"end\":150494,\"start\":150484},{\"end\":150728,\"start\":150708},{\"end\":150739,\"start\":150728},{\"end\":150983,\"start\":150973},{\"end\":150994,\"start\":150983},{\"end\":151005,\"start\":150994},{\"end\":151017,\"start\":151005},{\"end\":151290,\"start\":151282},{\"end\":151300,\"start\":151290},{\"end\":151309,\"start\":151300},{\"end\":151318,\"start\":151309},{\"end\":151329,\"start\":151318},{\"end\":151565,\"start\":151556},{\"end\":151572,\"start\":151565},{\"end\":151583,\"start\":151572},{\"end\":151590,\"start\":151583},{\"end\":151606,\"start\":151590},{\"end\":151614,\"start\":151606},{\"end\":151623,\"start\":151614},{\"end\":151633,\"start\":151623},{\"end\":151642,\"start\":151633},{\"end\":151653,\"start\":151642},{\"end\":151661,\"start\":151653},{\"end\":151675,\"start\":151661},{\"end\":151686,\"start\":151675},{\"end\":152001,\"start\":151988},{\"end\":152246,\"start\":152238},{\"end\":152255,\"start\":152246},{\"end\":152267,\"start\":152255},{\"end\":152472,\"start\":152464},{\"end\":152480,\"start\":152472},{\"end\":152668,\"start\":152661},{\"end\":152678,\"start\":152668},{\"end\":152688,\"start\":152678},{\"end\":152698,\"start\":152688},{\"end\":152915,\"start\":152906},{\"end\":152923,\"start\":152915},{\"end\":152931,\"start\":152923},{\"end\":152937,\"start\":152931},{\"end\":152944,\"start\":152937},{\"end\":152953,\"start\":152944},{\"end\":152964,\"start\":152953},{\"end\":153154,\"start\":153146},{\"end\":153167,\"start\":153154},{\"end\":153354,\"start\":153346},{\"end\":153362,\"start\":153354},{\"end\":153374,\"start\":153362},{\"end\":153382,\"start\":153374},{\"end\":153389,\"start\":153382},{\"end\":153398,\"start\":153389},{\"end\":153410,\"start\":153398},{\"end\":153425,\"start\":153410},{\"end\":153703,\"start\":153691},{\"end\":153713,\"start\":153703},{\"end\":153723,\"start\":153713},{\"end\":153732,\"start\":153723},{\"end\":153742,\"start\":153732},{\"end\":153756,\"start\":153742},{\"end\":153766,\"start\":153756},{\"end\":153774,\"start\":153766},{\"end\":153783,\"start\":153774},{\"end\":153793,\"start\":153783},{\"end\":153801,\"start\":153793},{\"end\":153811,\"start\":153801},{\"end\":154135,\"start\":154123},{\"end\":154143,\"start\":154135},{\"end\":154155,\"start\":154143},{\"end\":154165,\"start\":154155},{\"end\":154377,\"start\":154367},{\"end\":154560,\"start\":154551},{\"end\":154570,\"start\":154560},{\"end\":154581,\"start\":154570},{\"end\":154790,\"start\":154781},{\"end\":154799,\"start\":154790},{\"end\":154810,\"start\":154799},{\"end\":154823,\"start\":154810},{\"end\":154831,\"start\":154823},{\"end\":155030,\"start\":155022},{\"end\":155037,\"start\":155030},{\"end\":155043,\"start\":155037},{\"end\":155051,\"start\":155043},{\"end\":155058,\"start\":155051},{\"end\":155224,\"start\":155217},{\"end\":155233,\"start\":155224},{\"end\":155240,\"start\":155233},{\"end\":155248,\"start\":155240},{\"end\":155255,\"start\":155248},{\"end\":155453,\"start\":155446},{\"end\":155462,\"start\":155453},{\"end\":155472,\"start\":155462},{\"end\":155482,\"start\":155472},{\"end\":155494,\"start\":155482},{\"end\":155501,\"start\":155494},{\"end\":155514,\"start\":155501},{\"end\":155777,\"start\":155760},{\"end\":155786,\"start\":155777},{\"end\":155794,\"start\":155786},{\"end\":155805,\"start\":155794},{\"end\":155816,\"start\":155805},{\"end\":155826,\"start\":155816},{\"end\":155836,\"start\":155826}]", "bib_venue": "[{\"end\":123050,\"start\":123016},{\"end\":123211,\"start\":123135},{\"end\":123468,\"start\":123385},{\"end\":124032,\"start\":124028},{\"end\":124284,\"start\":124280},{\"end\":124474,\"start\":124396},{\"end\":124926,\"start\":124922},{\"end\":125266,\"start\":125262},{\"end\":125493,\"start\":125439},{\"end\":125845,\"start\":125841},{\"end\":126119,\"start\":126115},{\"end\":126325,\"start\":126252},{\"end\":126727,\"start\":126720},{\"end\":127021,\"start\":127017},{\"end\":127264,\"start\":127173},{\"end\":127561,\"start\":127552},{\"end\":127816,\"start\":127812},{\"end\":128113,\"start\":128109},{\"end\":128295,\"start\":128234},{\"end\":128730,\"start\":128726},{\"end\":129044,\"start\":129040},{\"end\":129364,\"start\":129360},{\"end\":129741,\"start\":129737},{\"end\":130104,\"start\":130100},{\"end\":130448,\"start\":130444},{\"end\":130719,\"start\":130710},{\"end\":131036,\"start\":131000},{\"end\":131270,\"start\":131232},{\"end\":131673,\"start\":131584},{\"end\":132087,\"start\":132083},{\"end\":132299,\"start\":132295},{\"end\":132509,\"start\":132505},{\"end\":132762,\"start\":132758},{\"end\":132987,\"start\":132983},{\"end\":133337,\"start\":133300},{\"end\":133716,\"start\":133657},{\"end\":133942,\"start\":133938},{\"end\":134240,\"start\":134236},{\"end\":134497,\"start\":134487},{\"end\":134759,\"start\":134755},{\"end\":135011,\"start\":135007},{\"end\":135276,\"start\":135267},{\"end\":135510,\"start\":135437},{\"end\":135851,\"start\":135780},{\"end\":136162,\"start\":136158},{\"end\":136438,\"start\":136384},{\"end\":136728,\"start\":136724},{\"end\":136955,\"start\":136951},{\"end\":137288,\"start\":137283},{\"end\":137645,\"start\":137641},{\"end\":137805,\"start\":137801},{\"end\":138034,\"start\":138012},{\"end\":138331,\"start\":138308},{\"end\":138618,\"start\":138614},{\"end\":138900,\"start\":138896},{\"end\":139219,\"start\":139215},{\"end\":139768,\"start\":139764},{\"end\":139969,\"start\":139965},{\"end\":140198,\"start\":140194},{\"end\":140403,\"start\":140325},{\"end\":140718,\"start\":140714},{\"end\":141030,\"start\":141026},{\"end\":141306,\"start\":141302},{\"end\":141505,\"start\":141427},{\"end\":141789,\"start\":141785},{\"end\":142015,\"start\":142011},{\"end\":142242,\"start\":142238},{\"end\":142443,\"start\":142371},{\"end\":142817,\"start\":142781},{\"end\":143119,\"start\":143115},{\"end\":143273,\"start\":143249},{\"end\":143495,\"start\":143491},{\"end\":143779,\"start\":143775},{\"end\":144084,\"start\":144080},{\"end\":144331,\"start\":144327},{\"end\":144593,\"start\":144589},{\"end\":144870,\"start\":144866},{\"end\":145153,\"start\":145149},{\"end\":145400,\"start\":145396},{\"end\":145606,\"start\":145602},{\"end\":145776,\"start\":145772},{\"end\":145976,\"start\":145972},{\"end\":146180,\"start\":146134},{\"end\":146543,\"start\":146539},{\"end\":146869,\"start\":146865},{\"end\":147163,\"start\":147149},{\"end\":147405,\"start\":147324},{\"end\":147796,\"start\":147792},{\"end\":148011,\"start\":148007},{\"end\":148232,\"start\":148215},{\"end\":148442,\"start\":148438},{\"end\":148858,\"start\":148854},{\"end\":149320,\"start\":149316},{\"end\":149629,\"start\":149609},{\"end\":149963,\"start\":149959},{\"end\":150228,\"start\":150224},{\"end\":150498,\"start\":150494},{\"end\":150743,\"start\":150739},{\"end\":151024,\"start\":151017},{\"end\":151333,\"start\":151329},{\"end\":151717,\"start\":151686},{\"end\":152050,\"start\":152001},{\"end\":152271,\"start\":152267},{\"end\":152484,\"start\":152480},{\"end\":152702,\"start\":152698},{\"end\":152968,\"start\":152964},{\"end\":153144,\"start\":153109},{\"end\":153429,\"start\":153425},{\"end\":153815,\"start\":153811},{\"end\":154176,\"start\":154172},{\"end\":154386,\"start\":154377},{\"end\":154585,\"start\":154581},{\"end\":154835,\"start\":154831},{\"end\":155062,\"start\":155058},{\"end\":155215,\"start\":155176},{\"end\":155518,\"start\":155514},{\"end\":155840,\"start\":155836},{\"end\":131758,\"start\":131675}]"}}}, "year": 2023, "month": 12, "day": 17}