{"id": 52978766, "updated": "2023-07-19 16:10:32.996", "metadata": {"title": "Efficient human activity recognition using hyperdimensional computing", "authors": "[{\"first\":\"Yeseong\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[\"S.\"]}]", "venue": null, "journal": "Proceedings of the 8th International Conference on the Internet of Things", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Human activity recognition is a key task of many Internet of Things (IoT) applications to understand underlying contexts and react with the environments. Machine learning is widely exploited to identify the activities from sensor measurements, however, they are often overcomplex to run on less-powerful IoT devices. In this paper, we present an alternative approach to efficiently support the activity recognition tasks using brain-inspired hyperdimensional (HD) computing. We show how the HD computing method can be applied to the recognition problem in IoT systems while improving the accuracy and efficiency. In our evaluation conducted for three practical datasets, the proposed design achieves the speedup of the model training by up to 486x as compared to the state-of-the-art neural network training. In addition, our design improves the performance of the HD-based inference procedure by 7x on a low-power ARM processor.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2897044384", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iot/KimIR18", "doi": "10.1145/3277593.3277617"}}, "content": {"source": {"pdf_hash": "eb7afecdbf9299dbcd3188b864d600ef01c7e6f1", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "pd", "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3277593.3277617", "status": "HYBRID"}}, "grobid": {"id": "5e5041108c166288990c84c4906a5e49c15a52d9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/eb7afecdbf9299dbcd3188b864d600ef01c7e6f1.txt", "contents": "\nEfficient Human Activity Recognition Using Hyperdimensional Computing\n\n\nYeseong Kim \nMohsen Imani moimani@ucsd.edu \nTajana S Rosing tajana@ucsd.edu \n\nUniversity of California San Diego\nUniversity of California\nSan Diego\n\n\nUniversity of California\nSan Diego\n\nEfficient Human Activity Recognition Using Hyperdimensional Computing\n10.1145/3277593.3277617Author Keywords Human activity recognitionhyperdimensional computingalternative computing\nHuman activity recognition is a key task of many Internet of Things (IoT) applications to understand underlying contexts and react with the environments. Machine learning is widely exploited to identify the activities from sensor measurements, however, they are often overcomplex to run on less-powerful IoT devices. In this paper, we present an alternative approach to efficiently support the activity recognition tasks using brain-inspired hyperdimensional (HD) computing. We show how the HD computing method can be applied to the recognition problem in IoT systems while improving the accuracy and efficiency. In our evaluation conducted for three practical datasets, the proposed design achieves the speedup of the model training by up to 486x as compared to the stateof-the-art neural network training. In addition, our design improves the performance of the HD-based inference procedure by 7x on a low-power ARM processor.\n\nINTRODUCTION\n\nHuman-aware system design has been widely investigated to offer high interactivity and enhanced efficiency under limited device resources. Earlier researchers recognized that understanding human behaviors is an important task to accomplish such goals. For example, diverse techniques exploited human activities and contexts as key control knobs of various system managements including mobile systems [9] and smart homes [1].\n\nHuman activity recognition such as motion detection is a key part of these techniques. Machine learning (ML) techniques are often used to automatically identify the activities from various information, where devices in the loop need to collect the data using sensors, e.g., accelerometers and GPS. Since training ML models are computationally intensive tasks in general, the expensive tasks are often offloaded to powerful systems, e.g., clouds. However, the emergence of the Internet of Things (IoT) raises several issues in this approach; the amount of data created by billions of distributed devices adds significant computation burden to the centralized cloud. In This work was supported in part by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA and NSF grants #1730158 and #1527034, and Jacobs School of Engineering UCSD Powell Fellowship. addition, sending the sensitive user information may pose privacy and security concerns. An alternative solution is to run these tasks in a more localized way, e.g., on the IoT gateways at the edge [3,19]. The local IoT devices typically have less computing resources than the cloud servers and run on low-power processors, such as ARM or Intel Atom. Thus, we need a new ML technique that can be efficiently processed even on the embedded devices.\n\nTowards this goal, we have developed a new methodology which can efficiently recognize human activities based on hyperdimensional (HD) computing. HD computing is recently developed as an alternative computing method inspired by the human brain [8]. It represents the brain's memory model using data encoded at large dimensionality. Earlier works show that HD computing can offer high efficiency in many classification tasks, e.g., voice recognition [5] and language identification [7]. HD computing is in particular suitable for sensorbased classification tasks like human activity recognition in IoT devices since it is robust against most hardware failure mechanisms and thrives on noisy and incomplete data that the sensors often provide [6].\n\nIn this paper, we describe how the HD computing method can be applied to solve diverse human activity recognition problems. We model the classes of the human activities with high dimensional vectors, called hypervectors. Our approach encodes the collected sensor samples with hypervectors, and combines the samples for each class into a single hypervector using robust algebra in HD space. Since this step only uses simple operations, e.g., element-wise addition and multiplication, we can train the model in a lightweight way. Once the modeling is completed, we identify the human activity class for a newly observed data encoded with a hypervector. To this end, we search the most similar hypervector in the model to the sample.\n\nWe have designed different variants of the HD computingbased classification method for higher efficiency and classification accuracy. In this paper, we present two key approaches, hypervector retraining and hypervector binarization. The hypervector retraining refines the models to achieve higher classification accuracy. Unlike previous work, in the retraining step, we exploit non-binarized hypervectors to achieve high accuracy. The hypervector binarization then converts the trained hypervectors back to hypervectors of bitstreams, making the HD computation more suitable for lesspowerful IoT devices.\n\nIn our evaluation, we compare our approach with the stateof-the-art ML solutions. Our experimental results show that the proposed method can provide high accuracy and computing efficiency for popular human activity recognition problems. For example, as compared to the neural networksbased modeling, the HD-computing method achieves up to 486x speedup when running on x86 processor, while providing comparable classification accuracy. In addition, our design improves the performance of HD model-based inference tasks by up to 7x on a low-power ARM processor.\n\nThe rest of the paper is organized as follows: We first discuss related work for HD computing, and elaborate on the background of HD computing. Next, we describe the HDbased solution for human activity recognition with our strategies to optimize the efficiency and accuracy. The next section presents experimental results, and we conclude the paper with a discussion for future work.\n\n\nRELATED WORK\n\n\nHuman Activity Recognition\n\nPrior researchers have been investigated to understand and identify human activities and contexts. For example, the work in [16] showed a monitoring framework for human activity recognition which collects data from inertial measurement units (IMU). Some works have shown that daily activities can be captured by the sensors equipped in smartphone systems [2,17]. Another line of research has focused on how to exploit the human activity and context information for diverse problems. For example, prior research has shown that understanding users behavior and exploiting the behavioral characteristics can be used to improve system efficiency. In this context, earlier work proposed diverse system optimization techniques by identifying user behaviors and interactions for mobile systems [9] and smart homes [1]. Prior work often utilized ML techniques to identify the activities, while relying on computing capability of clouds through offloading, e.g., [18]. However, due to the massive data stream created in the IoT systems, more light-weight alternatives are considered as a key requirement in the system design.\n\n\nHD Computing\n\nThe hyperdimensional (HD) computing was first introduced in the field of neuroscience [8]. Prior researchers recognized that HD computing is effective for pattern-based cognitive tasks, and showed diverse applications, such as language recognition [7], text classification [11], the prediction from multimodal sensor fusion [14,15], and speech recognition [5]. The work in [13] showed that bio signal sensory data can be represented with hyperdimensional data. Some work have also presented that HD tasks can be efficiently performed with diverse computing devices. For example, the hardware accelerator design has been proposed to efficiently compute binarized hypervectors. Some works also presented new memory architectures that perform HD operations inside memory arrays [10,6]. Digital circuits for HD computing have been also designed, e.g., computation of Hamming distance distance search [6]. In this paper, we focus on how the human activity recognition problem can be effectively mapped using HD computing. In addition, we show how the HD computing can be further optimized for IoT devices.\n\n\nBACKGROUND: HD COMPUTING\n\nIn this section, we discuss the background for HD computing, focusing on the components used in our activity recognition design.\n\nData type: Unlike conventional computing methods, the basic data type of the HD computing is the hypervector which often has many elements, e.g., more than one thousand. We denote the dimensionality of the hypervector using D. For example, collected data are converted to hypervectors for future HD procedures, e.g., classification tasks. In the earlier HD work, e.g., [5], each element of hypervector is assumed to be a bit. In contrast, since the hypervector containing numbers may include diverse information, some recent HD applications choose this data type to implement [4]. We call these two different types as binary hypervector and non-binary hypervector. An element of a binary hypervector can be either 0 or 1. For the non-binary hypervector, the elements can have any real number.\n\nProperty of hypervectors: An important characteristic used in HD computing is the orthogonality of hypervectors. Let us assume that there are two hypervectors, A and B. The nonbinary hypervectors are defined to be orthogonal if the cosine similarity of A and B is zero. For binary hypervectors, we can define the orthogonality by mapping the hypervector element of 0 to -1.\n\nSince a hypervector has a large number of elements, we can easily find many pairs of two orthogonal hypervectors by randomly selecting their elements. For example, let us assume that we randomly choose elements of two non-binary hypervectors, A and B, among -1 and 1. In the cosine similarity computation, the element-wise multiplication make each element to either -1 or 1 with 50% chance, and the summation of all elements are very close to zero, i.e., near orthogonal. In contrast, if two hypervectors are computed somehow to be similar, the cosine similarity has a high value.\n\nHD arithmetic operations: HD arithmetic operations enable to associate multiple hypervectors. In this paper, we utilize three major operations.\n\n\u2022 Binding: Two hypervectors A and B are combined into a hypervector. We denote this operation with A \u00d7 B.\n\nFor the binary hypervectors, the element-wise XORing accomplishes this procedure; the element-wise multiplication is used for non-binary hypervectors. The binding operation preserves orthogonality of hypervectors. For example, when we have three hypervectors randomly created, say X, Y , and Z, the hypervector X is still near-orthogonal to the binding of the rests, Y \u00d7 Z.\n\n\u2022 Bundling: This operation is denoted with the + symbol. The component-wise addition implements the bundling for non-binary hypervectors. Since the bundling for two binary hypervector yields a non-binary hypervector, a majority function is applied afterward. For example, when n binary hypervectors are bundled, we first apply the elementswise addition, and make each element whose value is greater than n/2 to 0; 1 for the other case. We denote this operation by [A 0 + A 1 + \u00b7 \u00b7 \u00b7 + A n ]. The bundling operation preserves the similarity with the combined hypervectors. For example, for two hypervectors A and B, the cosine similarity between A and A + B is cos(\u03c0/4), i.e., greater than zero.\n\n\u2022 Detaching: This is a counter operation of the bundling for non-binary hypervectors. The component-wise subtraction implements this operation, and we denote it using the \u2212 symbol. This makes the cosine similarity between two operand hypervectors either smaller or negative.\n\nAssociative search: The binary and non-binary hypervectors respectively use Hamming distance and cosine distance as their distance metrics. For simplicity, we denote the distance metric, which is appropriate for each case, by \u03b4(A, B).\n\nWhen we have multiple hypervectors, the associative search is used to find the most similar hypervectors using the distance metric. For example, when we have m hypervectors, H 1 , \u00b7 \u00b7 \u00b7 , H m , the associative search for another hypervector A looks for a hypervector, H i , whose \u03b4(H i , A) is the highest.\n\n\nHD-BASED HUMAN ACTIVITY RECOGNITION\n\nDesign Overview Figure 1 describes our design that recognizes human activities based on HD computing. We collect multiple raw data from the external sensors in IoT devices, e.g., IMUs of wireless embedded devices and accelerometers in smartphones. Instead of using real numbers, we convert each collected sample, which includes multiple measurements, to a hypervector. We call this step by encoding. With the encoded hypervectors and its original activity (label), e.g., walking, running, and standing, we train the hypervector model. To classify K classes, the trained model includes K hypervectors for each class. The training procedure consists of three parts, one-shot learning, retraining, and model binarization. In the one-shot learning, our design reads and process the hypervectors for each sample one by one. Then, the retraining refines the hypervector models considering the samples again with multiple iterations. In the next step, we update the model to the binarized hypervectors for performance improvements. With the trained model, we perform the inference of the class. The goal of the inference procedure is to classify a collected sample with an unknown label into an activity class. Our design accomplishes the inference by performing the associative search with the model hypervectors.\n\n\nSensor Data Encoding\n\nTo enable HD computing, we encode the collected raw data to hypervectors. Let us assume that a sample collected at a time includes F values, i.e., S = v 1 , \u00b7 \u00b7 \u00b7 , v F , where each v i is different raw values that each sensor measures. To find the patterns of sample hypervectors for each human activity, the encoding procedure considers the impact of i) the value for each sensor measurement and ii) differences of all the sensors in the system.\n\nThe first step of the encoding is to convert a measurement value, v i , into a hypervector. As discussed in the background section, the similarity between two hypervectors, A and B, is determined with a metric, i.e., \u03b4 (A, B). Thus, we encode each value so that the corresponding hypervector keeps the relative difference across the measurement values of different samples under the distance metric. To this end, we utilize the measurement range of each sensor. For example, if a sensor produces a value in a range of [V min , V max ], the minimum and maximum values correspond to two hypervectors, L min and L max , where L min and L max are orthogonal to each other.\n\nWe represent any measurement value using the two hypervectors. L min with D dimension is first created by randomly choosing its elements. Using the L min , we create another hypervector, say L 1 , by flipping D/2Q elements, where Q is a configurable value. We repeat this procedure by Q times to decide L 1 , L 2 , \u00b7 \u00b7 \u00b7 , L Q , e.g., flipping elements of L 1 creates L 2 . Note that L Q is orthogonal to L min , thus L Q = L max . We call these created hypervectors as level hypervectors. A level hypervector corresponds with each measurement value by considering the relative difference of the measurement values. To this end, where the measurement range is quantized to Q levels, and each quantized subrange is mapped to a level hypervector.\n\nIn the second step of the encoding, we combine different sensor values of a sample to represent it with a single hypervector. To distinguish different sensors in the hypervector representation, we utilize another set of hypervectors, B 1 , \u00b7 \u00b7 \u00b7 , B F , called base hypervectors, whose elements are randomly chosen for the orthogonality. Let assume that each v i value corresponds to a level hypervector, L i . The encoded hypervector for the sample is computed by:\nH = L 1 \u00d7 B 1 + \u00b7 \u00b7 \u00b7 + L F \u00d7 B F .\nSince the B i hypervectors are orthogonal, even though we use the same set of the level hypervectors for different sensors, our training step still distinguishes the impact of different sensors within the encoded hypervector. All of the random hypervectors, i.e., L min and B i , are required to be created only once and exploited for the entire recognition procedure. Note that the elements of the encoded hypervectors, say sample hypervectors, are 0 or 1 if using the binary hypervectors; -1 or 1 for the non-binary hypervector case.\n\n\nModel Training\n\nIn this procedure, our design trains the model by combining the sample hypervectors. The goal is to learn the patterns of sensor values which exist within a class. Let assume that the training dataset includes N samples, and each sample is  Retraining: An issue of the one-shot model is that, although the bundled hypervectors captures the major similarity within each class, it does not understand hypervector differences across classes. In addition, bundling a large number of hypervectors may degrade classification quality when a large variety of patterns exists in each class. Thus, we refine the model to i) better identify the discrepancy between different classes and ii) recognize the common pattern existing in each class.\n\nAlgorithm 1 illustrates our retraining procedure to reduce the misclassification rate of the activity recognition. From the one-shot model, our design verifies the classification accuracy for each sample using the associative search. If a sample is wrongly classified, we modify two model hypervectors, i.e., the hypervector of the target class and the other hypervector of the misclassified class. We first bundle the sample hypervector once more to the correct class so that the model hypervector converges faster to the misclassified sample. The second task is detaching the hypervector from the wrong class to enlarge the difference between the two model hypervectors. We repeat this updating process multiple times for the training dataset, and the accuracy converges with sufficient iterations.\n\n\nModel Binarization:\n\nSince our model retraining algorithm exploits the element-wise addition and subtraction in the bundling and detaching operations, it consequently creates non-binary hypervectors as the model. Even though it makes the model more accurate, the model size and computation costs of the inference also increase. Since many devices in IoT environments which run the activity recognition is lesspowerful, we optimize the model by converting the model to the binary hypervectors. We update the model depending on the sign of each hypervector element, i.e., choosing 1 if the element value is positive; 0 for the negative value.\n\n\nModel-Based Inference\n\nOnce the model is trained, it is ready to process the inference step for samples whose labels are unknown. We first encode the values using the level and base hypervectors used in training step. Then, our design finds which model hypervectors is the most similar to the given sample hypervector using the associative search. Note that, in the associative search, we use different distance metrics based on the data type of the model. In general, the non-binarized model provides better accuracy. In contrast, the binarized model processes the inference in a more efficient way, since the Hamming distance can be computed with bitwise XOR operations for the smaller model, unlike the element-wise integer additions for the cosine distance computation.\n\n\nEVALUATION Experimental Setup\n\nTo evaluate how the proposed design works on the heterogeneous IoT environment, we utilize two different devices running on 2.8 GHz Intel Core i7 (x86) and 1.4 GHz ARM Cortex-A53 (ARM) processors. For both cases, we execute the same code implemented with Python 2.7 and Numpy which uses C++ backend. We compare our approach with the state-of-the-art deep neural network models (DNN) implemented using Google TensorFlow. Since our design can create binarized hypervector models, for fair comparison, we also evaluate the binarized neural network (BNN) models. The neural network models have three hidden layers of 512 neurons, and DNN and BNN models are trained with ADAM optimizer for 10 and 100 epochs, respectively, so that the accuracy converges. For the efficiency comparison, we measure the execution time of the training and testing procedures.\n\nWe evaluate our approach using three practical datasets as follows.\n\nUCIHAR: This dataset includes the sensor measurements for accelerometers and gyroscopes of a smartphone, which are measured on the waist of users. The goal is to classify twelve activity classes, e.g., walking, walking up/downstairs, sitting and standing.  \n\n\nPAMAP2:\n\nThe dataset contains data measured from three IMUs located at the wrist, chest, and ankle of users with a heart rate monitor. The goal is to classify five basic activities, e.g., walking and sitting. We exploit the feature extraction method suggested by the author.\n\n\nEXTRA:\n\nThe dataset has measurements of heterogeneous sensors from smartphones and smartwatches. We choose to classify the activity labels for phone locations, e.g., whether it is located on the table, in the pocket, bag, and hand. Note that the activities are related to diverse device control problems, e.g., thermal management of mobile devices [12]. Table 1 summarizes the dataset sizes. In our evaluation, we set the quantization level to 8, the retraining iterations to 20, and the dimension of hypervectors to 1000, since there is no accuracy gain with larger values. Figure 3 shows the comparison results of the accuracy for different modeling methods. The results show that the proposed retraining method improves the classification accuracy. For example, when using the non-binary hypervector models, the accuracy improvement is 3% on average. We observe higher accuracy improvements for the binarized hypervector models by 4% on average. Throughout the retraining procedure, we train the HD model which have comparable accuracy to the DNN and BNN models. For example, for UCIHAR dataset, the accuracy difference between the non-binary model and DNN is only 0.2%. The accuracy difference between binary and non-binary models is 8% on average. In the next section, we evaluate how much performance can be improved by the model binarization.\n\n\nClassification Accuracy\n\n\nEfficiency Comparison Training Efficiency\n\nWe evaluate the efficiency of different modeling methods. Figure 4(a) shows the efficiency comparison of our design with the state-of-the-art DNN and BNN model. The results are reported for the non-binarized models, since the overhead of the model binarization is negligible. 1 In this comparison, the HD modeling and the neural network training were both executed on x86 processor. The results show that the proposed method presents higher performance efficiency as compared to the neural network training. For example, for UCI-HAR dataset, training the HD model with the retraining procedure is 4x and 56x faster than the DNN and BNN models, respectively. Note that the accuracy difference between the two model is only 0.2% as presented in the previous section.\n\nIn addition, when a small amount of accuracy loss is acceptable, our design can also train the model without retraining.\n\nIn that case, we observe the speedup up to 486x compared to the BNN approach. Thus, we conclude that the proposed design may efficiently process the activity recognition tasks in the IoT systems, since many IoT devices in the loop is expected to run on low-power processors with resource budgets.\n\n\nInference Efficiency\n\nWith the trained model, our design performs the inference tasks for each collected data. Figure 4(c) shows how much the execution time takes to process the inference procedure for each sample. In this evaluation, we compare the non-binary model to the binary model. The result shows that the model binarization significantly improves the inference procedure. The speedup is 8.4x and 7.1x for the x86 and ARM case, respectively.\n\nFor the ARM processor case, the inference based on the nonbinarized model takes 2 ms on average, while the binarized model only takes 0.28 ms. In IoT systems, the sensors are often equipped with the same device running on these lowpower processors. Thus, when a small amount of accuracy loss is acceptable, the binarized model is more preferable, e.g., serving real-time needs for the activity recognition.\n\n\nCONCLUSION\n\nIn this paper, we present a new system design which performs human activity recognition tasks based on emerging HD tasks. We also show optimization techniques that improve the accuracy of the HD-based classification and performance efficiency in the inference procedure. In our evaluation, the proposed design achieves the speedup of the model training by up to 486x, compared to the neural network model training. The proposed work can be extended to several directions. The HD method can be also applied for other classification tasks existing in IoT systems. We plan to investigate further optimization strategies to improve the HD classification quality. In addition, we also work on hardware design for the proposed method in order to more efficiently exploit the HD computing in control systems of low-end devices. \n\nIOT ' 18 ,\n18October 1518, 2018, Santa Barbara, CA, USA 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-6564-2/18/10$15.00 https://doi.org/10.1145/3277593.3277617\n\nFigure 1 .\n1Overview of Proposed Design\n\nFigure 2 .\n2Encoding of Sensor Measurements encoded with N hypervectors, H 1 , \u00b7 \u00b7 \u00b7 , H N . Each sample hypervector corresponds to an activity class, say c i .\n\nFigure 3 .\n3Accuracy Comparison for Different Modeling Methods\n\nFigure 4 (\n4b) compares the execution time of the training procedure on the two different processors. The results suggest that the proposed design can efficiently train the hypervector model even on the low-power processor. For example, for PAMAP2 dataset, the training time including the retraining only takes 26 seconds on the ARM processor. To train the one-shot model, it only takes 4 seconds.\n\nFigure 4 .\n4Efficiency Comparison for Training and Inference\n\n\ntest ) is likely to be a positive value. Furthermore, if H test is similar to the majority of the hypervectors combined into M , \u03b4(M, H test ) yields a much higher value. Based on this observation, we create the one-shot model, say M 1 , \u00b7 \u00b7 \u00b7 , M K , by bundling all sample hypervectors included in each activity of K classes.One-shot training: The first step of the training is to bun-\ndle the hypervectors for each class. We call this computation \nas one-shot training. For example, let us assume that there \nare l hypervectors, H 1 , H 2 , \u00b7 \u00b7 \u00b7 , H l , where all of them are \nincluded in the same class. The bundling operation makes \nanother hypervector, M = H 1 + \u00b7 \u00b7 \u00b7 + H l . For exam-\nple, let us assume that we have another hypervector H test , \nwhich is very similar to H 1 , by the distance metric. In this \ncase, \u03b4(M, H \nThe model binarization requires to update the trained hypervectors only once after all the retraining procedure.\n\nUser behavior modeling for estimating residential energy consumption. B Aksanli, A S Akyurek, T S Rosing, Smart City 360. SpringerAksanli, B., Akyurek, A. S., and Rosing, T. S. User behavior modeling for estimating residential energy consumption. In Smart City 360. Springer, 2016, 348-361.\n\nA public domain dataset for human activity recognition using smartphones. D Anguita, A Ghio, L Oneto, X Parra, J L Reyes-Ortiz, ESANN. Anguita, D., Ghio, A., Oneto, L., Parra, X., and Reyes-Ortiz, J. L. A public domain dataset for human activity recognition using smartphones. In ESANN (2013).\n\nCross-platform machine learning characterization for task allocation in iot ecosystems. W Cui, Y Kim, T S Rosing, Computing and Communication Workshop and Conference. CCWCIEEECui, W., Kim, Y., and Rosing, T. S. Cross-platform machine learning characterization for task allocation in iot ecosystems. In Computing and Communication Workshop and Conference (CCWC), 2017 IEEE 7th Annual, IEEE (2017), 1-7.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, C Huang, D Kong, T Rosing, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceACM108Imani, M., Huang, C., Kong, D., and Rosing, T. Hierarchical hyperdimensional computing for energy efficient classification. In Proceedings of the 55th Annual Design Automation Conference, ACM (2018), 108.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, International Conference on Rebooting Computing (ICRC). Imani, M., Kong, D., Rahimi, A., and Rosing, T. Voicehd: Hyperdimensional computing for efficient speech recognition. In International Conference on Rebooting Computing (ICRC), IEEE (2017), 1-6.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, High Performance Computer Architecture (HPCA. 2017 IEEE International Symposium onImani, M., Rahimi, A., Kong, D., Rosing, T., and Rabaey, J. M. Exploring hyperdimensional associative memory. In High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium on, IEEE (2017), 445-456.\n\nLanguage geometry using random indexing. A Joshi, J Halseth, P Kanerva, Quantum Interaction 2016 Conference Proceedings. In pressJoshi, A., Halseth, J., and Kanerva, P. Language geometry using random indexing. Quantum Interaction 2016 Conference Proceedings (In press).\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 1Kanerva, P. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive Computation 1, 2 (2009), 139-159.\n\nSmartphone analysis and optimization based on user activity recognition. Y Kim, F Parterna, S Tilak, T S Rosing, Computer-Aided Design (ICCAD). IEEEKim, Y., Parterna, F., Tilak, S., and Rosing, T. S. Smartphone analysis and optimization based on user activity recognition. In Computer-Aided Design (ICCAD), 2015 IEEE/ACM International Conference on, IEEE (2015), 605-612.\n\nHyperdimensional computing with 3d vrram in-memory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition. H Li, T F Wu, A Rahimi, K.-S Li, M Rusch, C.-H Lin, J.-L Hsu, M M Sabry, S B Eryilmaz, J Sohn, Electron Devices Meeting (IEDM). IEEELi, H., Wu, T. F., Rahimi, A., Li, K.-S., Rusch, M., Lin, C.-H., Hsu, J.-L., Sabry, M. M., Eryilmaz, S. B., Sohn, J., et al. Hyperdimensional computing with 3d vrram in-memory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition. In Electron Devices Meeting (IEDM), 2016 IEEE International, IEEE (2016), 16-1.\n\nHyperdimensional computing for text classification. Design, Automation Test in Europe Conference Exhibition (DATE). F R Najafabadi, A Rahimi, P Kanerva, J M Rabaey, University BoothNajafabadi, F. R., Rahimi, A., Kanerva, P., and Rabaey, J. M. Hyperdimensional computing for text classification. Design, Automation Test in Europe Conference Exhibition (DATE), University Booth (2016).\n\nModeling and mitigation of extra-soc thermal coupling effects and heat transfer variations in mobile devices. F Paterna, T \u0160 Rosing, Proceedings of the IEEE/ACM International Conference on Computer-Aided Design. the IEEE/ACM International Conference on Computer-Aided DesignIEEE PressPaterna, F., and Rosing, T.\u0160. Modeling and mitigation of extra-soc thermal coupling effects and heat transfer variations in mobile devices. In Proceedings of the IEEE/ACM International Conference on Computer-Aided Design, IEEE Press (2015), 831-838.\n\nHyperdimensional biosignal processing: A case study for emg-based hand gesture recognition. A Rahimi, S Benatti, P Kanerva, L Benini, J M Rabaey, Rebooting Computing (ICRC), IEEE International Conference on. Rahimi, A., Benatti, S., Kanerva, P., Benini, L., and Rabaey, J. M. Hyperdimensional biosignal processing: A case study for emg-based hand gesture recognition. In Rebooting Computing (ICRC), IEEE International Conference on, IEEE (2016), 1-8.\n\nModeling dependencies in multiple parallel data streams with hyperdimensional computing. O R\u00e4s\u00e4nen, S Kakouros, IEEE Signal Processing Letters. 21R\u00e4s\u00e4nen, O., and Kakouros, S. Modeling dependencies in multiple parallel data streams with hyperdimensional computing. IEEE Signal Processing Letters 21, 7 (2014), 899-903.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, J Saarinen, IEEE Transactions on Neural Networks and Learning Systems PP. Rasanen, O., and Saarinen, J. Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. IEEE Transactions on Neural Networks and Learning Systems PP, 99 (2015), 1-12.\n\nIntroducing a new benchmarked dataset for activity monitoring. A Reiss, D Stricker, Wearable Computers (ISWC), 2012 16th International Symposium on. Reiss, A., and Stricker, D. Introducing a new benchmarked dataset for activity monitoring. In Wearable Computers (ISWC), 2012 16th International Symposium on, IEEE (2012), 108-109.\n\nRecognizing detailed human context in the wild from smartphones and smartwatches. Y Vaizman, K Ellis, G Lanckriet, IEEE Pervasive Computing. 16Vaizman, Y., Ellis, K., and Lanckriet, G. Recognizing detailed human context in the wild from smartphones and smartwatches. IEEE Pervasive Computing 16, 4 (2017), 62-74.\n\nFast app launching for mobile devices using predictive user context. T Yan, D Chu, D Ganesan, A Kansal, J Liu, Proceedings of the 10th international conference on Mobile systems, applications, and services. the 10th international conference on Mobile systems, applications, and servicesACMYan, T., Chu, D., Ganesan, D., Kansal, A., and Liu, J. Fast app launching for mobile devices using predictive user context. In Proceedings of the 10th international conference on Mobile systems, applications, and services, ACM (2012), 113-126.\n\nA survey of fog computing: concepts, applications and issues. S Yi, C Li, Li , Q , Proceedings of the 2015 workshop on mobile big data. the 2015 workshop on mobile big dataACMYi, S., Li, C., and Li, Q. A survey of fog computing: concepts, applications and issues. In Proceedings of the 2015 workshop on mobile big data, ACM (2015), 37-42.\n", "annotations": {"author": "[{\"end\":85,\"start\":73},{\"end\":116,\"start\":86},{\"end\":149,\"start\":117},{\"end\":221,\"start\":150},{\"end\":258,\"start\":222}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":81},{\"end\":98,\"start\":93},{\"end\":132,\"start\":126}]", "author_first_name": "[{\"end\":80,\"start\":73},{\"end\":92,\"start\":86},{\"end\":123,\"start\":117},{\"end\":125,\"start\":124}]", "author_affiliation": "[{\"end\":220,\"start\":151},{\"end\":257,\"start\":223}]", "title": "[{\"end\":70,\"start\":1},{\"end\":328,\"start\":259}]", "venue": null, "abstract": "[{\"end\":1370,\"start\":442}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1789,\"start\":1786},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1809,\"start\":1806},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2880,\"start\":2877},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2883,\"start\":2880},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3375,\"start\":3372},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3580,\"start\":3577},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3612,\"start\":3609},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3872,\"start\":3869},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6332,\"start\":6328},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6562,\"start\":6559},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6565,\"start\":6562},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6994,\"start\":6991},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7014,\"start\":7011},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7162,\"start\":7158},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7426,\"start\":7423},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7588,\"start\":7585},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7614,\"start\":7610},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7665,\"start\":7661},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7668,\"start\":7665},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7696,\"start\":7693},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7714,\"start\":7710},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8116,\"start\":8112},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8118,\"start\":8116},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8236,\"start\":8233},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8968,\"start\":8965},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9175,\"start\":9172},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21578,\"start\":21574},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22924,\"start\":22923}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25705,\"start\":25528},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25746,\"start\":25706},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25908,\"start\":25747},{\"attributes\":{\"id\":\"fig_4\"},\"end\":25972,\"start\":25909},{\"attributes\":{\"id\":\"fig_5\"},\"end\":26371,\"start\":25973},{\"attributes\":{\"id\":\"fig_6\"},\"end\":26433,\"start\":26372},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27269,\"start\":26434}]", "paragraph": "[{\"end\":1810,\"start\":1386},{\"end\":3126,\"start\":1812},{\"end\":3873,\"start\":3128},{\"end\":4605,\"start\":3875},{\"end\":5212,\"start\":4607},{\"end\":5773,\"start\":5214},{\"end\":6158,\"start\":5775},{\"end\":7320,\"start\":6204},{\"end\":8437,\"start\":7337},{\"end\":8594,\"start\":8466},{\"end\":9388,\"start\":8596},{\"end\":9763,\"start\":9390},{\"end\":10345,\"start\":9765},{\"end\":10490,\"start\":10347},{\"end\":10597,\"start\":10492},{\"end\":10972,\"start\":10599},{\"end\":11668,\"start\":10974},{\"end\":11944,\"start\":11670},{\"end\":12180,\"start\":11946},{\"end\":12488,\"start\":12182},{\"end\":13835,\"start\":12528},{\"end\":14307,\"start\":13860},{\"end\":14977,\"start\":14309},{\"end\":15723,\"start\":14979},{\"end\":16190,\"start\":15725},{\"end\":16762,\"start\":16227},{\"end\":17513,\"start\":16781},{\"end\":18315,\"start\":17515},{\"end\":18958,\"start\":18339},{\"end\":19734,\"start\":18984},{\"end\":20618,\"start\":19768},{\"end\":20687,\"start\":20620},{\"end\":20946,\"start\":20689},{\"end\":21223,\"start\":20958},{\"end\":22575,\"start\":21234},{\"end\":23411,\"start\":22647},{\"end\":23533,\"start\":23413},{\"end\":23831,\"start\":23535},{\"end\":24283,\"start\":23856},{\"end\":24691,\"start\":24285},{\"end\":25527,\"start\":24706}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16226,\"start\":16191}]", "table_ref": "[{\"end\":21587,\"start\":21580}]", "section_header": "[{\"end\":1384,\"start\":1372},{\"end\":6173,\"start\":6161},{\"end\":6202,\"start\":6176},{\"end\":7335,\"start\":7323},{\"end\":8464,\"start\":8440},{\"end\":12526,\"start\":12491},{\"end\":13858,\"start\":13838},{\"end\":16779,\"start\":16765},{\"end\":18337,\"start\":18318},{\"end\":18982,\"start\":18961},{\"end\":19766,\"start\":19737},{\"end\":20956,\"start\":20949},{\"end\":21232,\"start\":21226},{\"end\":22601,\"start\":22578},{\"end\":22645,\"start\":22604},{\"end\":23854,\"start\":23834},{\"end\":24704,\"start\":24694},{\"end\":25539,\"start\":25529},{\"end\":25717,\"start\":25707},{\"end\":25758,\"start\":25748},{\"end\":25920,\"start\":25910},{\"end\":25984,\"start\":25974},{\"end\":26383,\"start\":26373}]", "table": "[{\"end\":27269,\"start\":26763}]", "figure_caption": "[{\"end\":25705,\"start\":25542},{\"end\":25746,\"start\":25719},{\"end\":25908,\"start\":25760},{\"end\":25972,\"start\":25922},{\"end\":26371,\"start\":25986},{\"end\":26433,\"start\":26385},{\"end\":26763,\"start\":26436}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12552,\"start\":12544},{\"end\":14534,\"start\":14528},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21809,\"start\":21801},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22713,\"start\":22705},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23953,\"start\":23945}]", "bib_author_first_name": "[{\"end\":27455,\"start\":27454},{\"end\":27466,\"start\":27465},{\"end\":27468,\"start\":27467},{\"end\":27479,\"start\":27478},{\"end\":27481,\"start\":27480},{\"end\":27751,\"start\":27750},{\"end\":27762,\"start\":27761},{\"end\":27770,\"start\":27769},{\"end\":27779,\"start\":27778},{\"end\":27788,\"start\":27787},{\"end\":27790,\"start\":27789},{\"end\":28060,\"start\":28059},{\"end\":28067,\"start\":28066},{\"end\":28074,\"start\":28073},{\"end\":28076,\"start\":28075},{\"end\":28452,\"start\":28451},{\"end\":28461,\"start\":28460},{\"end\":28470,\"start\":28469},{\"end\":28478,\"start\":28477},{\"end\":28875,\"start\":28874},{\"end\":28884,\"start\":28883},{\"end\":28892,\"start\":28891},{\"end\":28902,\"start\":28901},{\"end\":29211,\"start\":29210},{\"end\":29220,\"start\":29219},{\"end\":29230,\"start\":29229},{\"end\":29238,\"start\":29237},{\"end\":29248,\"start\":29247},{\"end\":29250,\"start\":29249},{\"end\":29604,\"start\":29603},{\"end\":29613,\"start\":29612},{\"end\":29624,\"start\":29623},{\"end\":29959,\"start\":29958},{\"end\":30249,\"start\":30248},{\"end\":30256,\"start\":30255},{\"end\":30268,\"start\":30267},{\"end\":30277,\"start\":30276},{\"end\":30279,\"start\":30278},{\"end\":30699,\"start\":30698},{\"end\":30705,\"start\":30704},{\"end\":30707,\"start\":30706},{\"end\":30713,\"start\":30712},{\"end\":30726,\"start\":30722},{\"end\":30732,\"start\":30731},{\"end\":30744,\"start\":30740},{\"end\":30754,\"start\":30750},{\"end\":30761,\"start\":30760},{\"end\":30763,\"start\":30762},{\"end\":30772,\"start\":30771},{\"end\":30774,\"start\":30773},{\"end\":30786,\"start\":30785},{\"end\":31303,\"start\":31302},{\"end\":31305,\"start\":31304},{\"end\":31319,\"start\":31318},{\"end\":31329,\"start\":31328},{\"end\":31340,\"start\":31339},{\"end\":31342,\"start\":31341},{\"end\":31682,\"start\":31681},{\"end\":31693,\"start\":31692},{\"end\":31695,\"start\":31694},{\"end\":32199,\"start\":32198},{\"end\":32209,\"start\":32208},{\"end\":32220,\"start\":32219},{\"end\":32231,\"start\":32230},{\"end\":32241,\"start\":32240},{\"end\":32243,\"start\":32242},{\"end\":32648,\"start\":32647},{\"end\":32659,\"start\":32658},{\"end\":33001,\"start\":33000},{\"end\":33012,\"start\":33011},{\"end\":33381,\"start\":33380},{\"end\":33390,\"start\":33389},{\"end\":33731,\"start\":33730},{\"end\":33742,\"start\":33741},{\"end\":33751,\"start\":33750},{\"end\":34032,\"start\":34031},{\"end\":34039,\"start\":34038},{\"end\":34046,\"start\":34045},{\"end\":34057,\"start\":34056},{\"end\":34067,\"start\":34066},{\"end\":34559,\"start\":34558},{\"end\":34565,\"start\":34564},{\"end\":34572,\"start\":34570},{\"end\":34576,\"start\":34575}]", "bib_author_last_name": "[{\"end\":27463,\"start\":27456},{\"end\":27476,\"start\":27469},{\"end\":27488,\"start\":27482},{\"end\":27759,\"start\":27752},{\"end\":27767,\"start\":27763},{\"end\":27776,\"start\":27771},{\"end\":27785,\"start\":27780},{\"end\":27802,\"start\":27791},{\"end\":28064,\"start\":28061},{\"end\":28071,\"start\":28068},{\"end\":28083,\"start\":28077},{\"end\":28458,\"start\":28453},{\"end\":28467,\"start\":28462},{\"end\":28475,\"start\":28471},{\"end\":28485,\"start\":28479},{\"end\":28881,\"start\":28876},{\"end\":28889,\"start\":28885},{\"end\":28899,\"start\":28893},{\"end\":28909,\"start\":28903},{\"end\":29217,\"start\":29212},{\"end\":29227,\"start\":29221},{\"end\":29235,\"start\":29231},{\"end\":29245,\"start\":29239},{\"end\":29257,\"start\":29251},{\"end\":29610,\"start\":29605},{\"end\":29621,\"start\":29614},{\"end\":29632,\"start\":29625},{\"end\":29967,\"start\":29960},{\"end\":30253,\"start\":30250},{\"end\":30265,\"start\":30257},{\"end\":30274,\"start\":30269},{\"end\":30286,\"start\":30280},{\"end\":30702,\"start\":30700},{\"end\":30710,\"start\":30708},{\"end\":30720,\"start\":30714},{\"end\":30729,\"start\":30727},{\"end\":30738,\"start\":30733},{\"end\":30748,\"start\":30745},{\"end\":30758,\"start\":30755},{\"end\":30769,\"start\":30764},{\"end\":30783,\"start\":30775},{\"end\":30791,\"start\":30787},{\"end\":31316,\"start\":31306},{\"end\":31326,\"start\":31320},{\"end\":31337,\"start\":31330},{\"end\":31349,\"start\":31343},{\"end\":31690,\"start\":31683},{\"end\":31702,\"start\":31696},{\"end\":32206,\"start\":32200},{\"end\":32217,\"start\":32210},{\"end\":32228,\"start\":32221},{\"end\":32238,\"start\":32232},{\"end\":32250,\"start\":32244},{\"end\":32656,\"start\":32649},{\"end\":32668,\"start\":32660},{\"end\":33009,\"start\":33002},{\"end\":33021,\"start\":33013},{\"end\":33387,\"start\":33382},{\"end\":33399,\"start\":33391},{\"end\":33739,\"start\":33732},{\"end\":33748,\"start\":33743},{\"end\":33761,\"start\":33752},{\"end\":34036,\"start\":34033},{\"end\":34043,\"start\":34040},{\"end\":34054,\"start\":34047},{\"end\":34064,\"start\":34058},{\"end\":34071,\"start\":34068},{\"end\":34562,\"start\":34560},{\"end\":34568,\"start\":34566}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2485447},\"end\":27674,\"start\":27384},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6975432},\"end\":27969,\"start\":27676},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17737446},\"end\":28372,\"start\":27971},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49301394},\"end\":28802,\"start\":28374},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":21351739},\"end\":29161,\"start\":28804},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1677864},\"end\":29560,\"start\":29163},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":39020350},\"end\":29831,\"start\":29562},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":733980},\"end\":30173,\"start\":29833},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15846707},\"end\":30546,\"start\":30175},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":25209638},\"end\":31184,\"start\":30548},{\"attributes\":{\"id\":\"b10\"},\"end\":31569,\"start\":31186},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18405935},\"end\":32104,\"start\":31571},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12008695},\"end\":32556,\"start\":32106},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1690456},\"end\":32876,\"start\":32558},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15258913},\"end\":33315,\"start\":32878},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":10337279},\"end\":33646,\"start\":33317},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8728742},\"end\":33960,\"start\":33648},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10678563},\"end\":34494,\"start\":33962},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":207225896},\"end\":34834,\"start\":34496}]", "bib_title": "[{\"end\":27452,\"start\":27384},{\"end\":27748,\"start\":27676},{\"end\":28057,\"start\":27971},{\"end\":28449,\"start\":28374},{\"end\":28872,\"start\":28804},{\"end\":29208,\"start\":29163},{\"end\":29601,\"start\":29562},{\"end\":29956,\"start\":29833},{\"end\":30246,\"start\":30175},{\"end\":30696,\"start\":30548},{\"end\":31679,\"start\":31571},{\"end\":32196,\"start\":32106},{\"end\":32645,\"start\":32558},{\"end\":32998,\"start\":32878},{\"end\":33378,\"start\":33317},{\"end\":33728,\"start\":33648},{\"end\":34029,\"start\":33962},{\"end\":34556,\"start\":34496}]", "bib_author": "[{\"end\":27465,\"start\":27454},{\"end\":27478,\"start\":27465},{\"end\":27490,\"start\":27478},{\"end\":27761,\"start\":27750},{\"end\":27769,\"start\":27761},{\"end\":27778,\"start\":27769},{\"end\":27787,\"start\":27778},{\"end\":27804,\"start\":27787},{\"end\":28066,\"start\":28059},{\"end\":28073,\"start\":28066},{\"end\":28085,\"start\":28073},{\"end\":28460,\"start\":28451},{\"end\":28469,\"start\":28460},{\"end\":28477,\"start\":28469},{\"end\":28487,\"start\":28477},{\"end\":28883,\"start\":28874},{\"end\":28891,\"start\":28883},{\"end\":28901,\"start\":28891},{\"end\":28911,\"start\":28901},{\"end\":29219,\"start\":29210},{\"end\":29229,\"start\":29219},{\"end\":29237,\"start\":29229},{\"end\":29247,\"start\":29237},{\"end\":29259,\"start\":29247},{\"end\":29612,\"start\":29603},{\"end\":29623,\"start\":29612},{\"end\":29634,\"start\":29623},{\"end\":29969,\"start\":29958},{\"end\":30255,\"start\":30248},{\"end\":30267,\"start\":30255},{\"end\":30276,\"start\":30267},{\"end\":30288,\"start\":30276},{\"end\":30704,\"start\":30698},{\"end\":30712,\"start\":30704},{\"end\":30722,\"start\":30712},{\"end\":30731,\"start\":30722},{\"end\":30740,\"start\":30731},{\"end\":30750,\"start\":30740},{\"end\":30760,\"start\":30750},{\"end\":30771,\"start\":30760},{\"end\":30785,\"start\":30771},{\"end\":30793,\"start\":30785},{\"end\":31318,\"start\":31302},{\"end\":31328,\"start\":31318},{\"end\":31339,\"start\":31328},{\"end\":31351,\"start\":31339},{\"end\":31692,\"start\":31681},{\"end\":31704,\"start\":31692},{\"end\":32208,\"start\":32198},{\"end\":32219,\"start\":32208},{\"end\":32230,\"start\":32219},{\"end\":32240,\"start\":32230},{\"end\":32252,\"start\":32240},{\"end\":32658,\"start\":32647},{\"end\":32670,\"start\":32658},{\"end\":33011,\"start\":33000},{\"end\":33023,\"start\":33011},{\"end\":33389,\"start\":33380},{\"end\":33401,\"start\":33389},{\"end\":33741,\"start\":33730},{\"end\":33750,\"start\":33741},{\"end\":33763,\"start\":33750},{\"end\":34038,\"start\":34031},{\"end\":34045,\"start\":34038},{\"end\":34056,\"start\":34045},{\"end\":34066,\"start\":34056},{\"end\":34073,\"start\":34066},{\"end\":34564,\"start\":34558},{\"end\":34570,\"start\":34564},{\"end\":34575,\"start\":34570},{\"end\":34579,\"start\":34575}]", "bib_venue": "[{\"end\":27504,\"start\":27490},{\"end\":27809,\"start\":27804},{\"end\":28136,\"start\":28085},{\"end\":28546,\"start\":28487},{\"end\":28965,\"start\":28911},{\"end\":29303,\"start\":29259},{\"end\":29681,\"start\":29634},{\"end\":29990,\"start\":29969},{\"end\":30317,\"start\":30288},{\"end\":30824,\"start\":30793},{\"end\":31300,\"start\":31186},{\"end\":31781,\"start\":31704},{\"end\":32312,\"start\":32252},{\"end\":32700,\"start\":32670},{\"end\":33083,\"start\":33023},{\"end\":33464,\"start\":33401},{\"end\":33787,\"start\":33763},{\"end\":34167,\"start\":34073},{\"end\":34630,\"start\":34579},{\"end\":28592,\"start\":28548},{\"end\":31845,\"start\":31783},{\"end\":34248,\"start\":34169},{\"end\":34668,\"start\":34632}]"}}}, "year": 2023, "month": 12, "day": 17}