{"id": 252693323, "updated": "2023-10-05 10:12:37.391", "metadata": {"title": "Mind Reader: Reconstructing complex images from brain activities", "authors": "[{\"first\":\"Sikun\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Sprague\",\"middle\":[]},{\"first\":\"Ambuj\",\"last\":\"Singh\",\"middle\":[\"K\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Understanding how the brain encodes external stimuli and how these stimuli can be decoded from the measured brain activities are long-standing and challenging questions in neuroscience. In this paper, we focus on reconstructing the complex image stimuli from fMRI (functional magnetic resonance imaging) signals. Unlike previous works that reconstruct images with single objects or simple shapes, our work aims to reconstruct image stimuli that are rich in semantics, closer to everyday scenes, and can reveal more perspectives. However, data scarcity of fMRI datasets is the main obstacle to applying state-of-the-art deep learning models to this problem. We find that incorporating an additional text modality is beneficial for the reconstruction problem compared to directly translating brain signals to images. Therefore, the modalities involved in our method are: (i) voxel-level fMRI signals, (ii) observed images that trigger the brain signals, and (iii) textual description of the images. To further address data scarcity, we leverage an aligned vision-language latent space pre-trained on massive datasets. Instead of training models from scratch to find a latent space shared by the three modalities, we encode fMRI signals into this pre-aligned latent space. Then, conditioned on embeddings in this space, we reconstruct images with a generative model. The reconstructed images from our pipeline balance both naturalness and fidelity: they are photo-realistic and capture the ground truth image contents well.", "fields_of_study": "[\"Biology\",\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2210.01769", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/LinSS22", "doi": "10.48550/arxiv.2210.01769"}}, "content": {"source": {"pdf_hash": "7e2ca1df1e92e91c113daf056c0e00d1b22c5f25", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.01769v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "81b05f050c27a529b0c7a6eb2d6f215034465bf5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7e2ca1df1e92e91c113daf056c0e00d1b22c5f25.txt", "contents": "\nMind Reader: Reconstructing complex images from brain activities\n\n\nSikun Lin \nThomas Sprague tsprague@ucsb.edu \nAmbuj K Singh \nSanta Barbara \nMind Reader: Reconstructing complex images from brain activities\n\nUnderstanding how the brain encodes external stimuli and how these stimuli can be decoded from the measured brain activities are long-standing and challenging questions in neuroscience. In this paper, we focus on reconstructing the complex image stimuli from fMRI (functional magnetic resonance imaging) signals. Unlike previous works that reconstruct images with single objects or simple shapes, our work aims to reconstruct image stimuli that are rich in semantics, closer to everyday scenes, and can reveal more perspectives. However, data scarcity of fMRI datasets is the main obstacle to applying state-of-the-art deep learning models to this problem. We find that incorporating an additional text modality is beneficial for the reconstruction problem compared to directly translating brain signals to images. Therefore, the modalities involved in our method are: (i) voxel-level fMRI signals, (ii) observed images that trigger the brain signals, and (iii) textual description of the images. To further address data scarcity, we leverage an aligned vision-language latent space pre-trained on massive datasets. Instead of training models from scratch to find a latent space shared by the three modalities, we encode fMRI signals into this pre-aligned latent space. Then, conditioned on embeddings in this space, we reconstruct images with a generative model. The reconstructed images from our pipeline balance both naturalness and fidelity: they are photo-realistic and capture the ground truth image contents well.\n\nIntroduction\n\nIn an effort to understand visual encoding and decoding processes, researchers in recent years have curated multiple datasets recording fMRI signals while the subjects are viewing natural images [3,8,33,40]. In particular, the Natural Scenes Dataset (NSD [3]) was built to meet the needs of data-hungry deep learning models, sampling at an unprecedented scale compared to all prior works while having the highest resolution and signal-to-noise ratio (SNR). In addition, all the images used in NSD are sampled from MS-COCO [21], which has far richer contextual information and more detailed annotations compared to datasets that are commonly used in other fMRI studies (e.g., Celeb A face dataset [22], ImageNet [10], self-curated symbols, grayscale datasets). This dataset, therefore, offers the opportunity to explore the decoding of complex images that are closer to real-life scenes.\n\nHuman visual decoding can be categorized into stimuli category classification [1], stimuli identification [37], and reconstruction. We focus on stimuli reconstruction in this study. Different from previous efforts in reconstructing images from fMRI [6,11,12,23,31,33,34], we approach the problem with one more modality, that of text. The benefits of adding the text modality are threefold: first, the brain is naturally multimodal. Research [7,13,24] indicates that the brain is not only capable of learning multisensory representations, but a larger portion of the cortex is engaged in multisensory processing: for example, both visual and tactile recognition of objects activate the same part of the object-responsive cortex [25]. Visual-linguistic pathways along the border of the occipital lobe [26] also bring a more intertwined view of the brain's representation of these two modalities. Second, multimodal deep models tend to explain the brain better (having higher representation correlations) 36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n(a) (b) Figure 1: The pipeline for reconstructing seen images from fMRI signals. 1a details different components, from collected data to the reconstructed image. The pipeline is trained in two stages: during the first stage, mapping models fmi, fmc are trained to encode fMRI activities into the CLIP embedding space. In the second stage, conditional generator G and contrastive discriminator D are finetuned while both fmi, fmc are kept frozen. 1b shows the image generation process once models are trained. than the visual-only models, even when compared with activities in the visual cortex [9]. Lastly, our goal is to reconstruct complex images that have multiple objects in different categories with intricate interactions: it is natural to incorporate contextual information as an additional modality.\n\nInstead of training a model to map all three modalities (fMRI, image, text) to a unified latent space, we propose to map fMRI to a well-aligned space shared by image and text, and use conditional generative models to reconstruct seen images from representations in that space. This design addresses the data scarcity issue of brain datasets by separating fMRI from the other two modalities. In this way, a large amount of data is readily available to learn the shared visual-language representation and to train a generative model conditioned on this representation. Furthermore, pre-trained models can be utilized to make the whole reconstruction pipeline more efficient and flexible.\n\nOur contributions are as follows: (1) to the best of our knowledge, this is the first work on reconstructing complex images from human brain signals. It provides an opportunity to study the brain's visual decoding in a more natural setting than using object-centered images. Compared to previous works, it also decodes signals from more voxels and regions, including those outside the visual cortex, that are responsive to the experiment. This inclusion allows us to study the behavior and functionality of more brain areas. (2) We address the data scarcity issue by incorporating additional text modality and leveraging pre-trained latent space and models. For the reconstruction, we focus on semantic representations of the images while taking low-level visual features into account. (3) Our results show we can decode complex images from fMRI signals relatively faithfully. We also perform microstimulation on different brain regions to study their properties and showcase the potential usages of the pipeline.\n\n\nMethod\n\nRecent developments in contrastive models allow more accurate embeddings of images and their semantic meanings in the same latent space. This performance is realized using massive datasets: models such as CLIP [27] and ALIGN [17] utilize thousands of millions of image-text pairs for representation alignment. In comparison, brain imaging datasets that record pairs of images and fMRI range from 1.2k to 73k samples, making it difficult to learn brain encoding and decoding models from scratch. However, we can utilize aligned embeddings obtained from pre-trained contrastive models as the intermediary and generate images conditioned on these embeddings. In our pipeline, as shown in fig. 1, we first map fMRI signals to CLIP embeddings of the observed image and its captions, then pass these embeddings to a conditional generative model for image reconstruction.\n\n\nCaption Screening\n\nEach image x img in the COCO dataset has five captions {x cap 1 , \u00b7 \u00b7 \u00b7 , x cap 5 } collected through Amazon's Mechanical Turk (AMT), and in nature, these captions vary in their descriptive ability.  shows a sample image with its five captions, and we can tell captions (2) and (3) are more objective and informative than caption (4) when it comes to describing the content of that image, thus are more helpful to serve as the image generation condition. We utilize pre-trained CLIP encoders to screen the high-quality captions since representations in the CLIP space are trained to be image-text aligned. A caption with an embedding more aligned to the image embedding is more descriptive than a less aligned one; it is also less general and more specific to this particular image because of the contrastive loss in CLIP. For the screening, we pass each image together with its five captions to the CLIP model, which outputs corresponding probabilities that the captions and image are proper pairs. We keep captions with probabilities larger than half of the highest probability. After screening out less informative captions, we have one to three high-quality captions per image.\n\n\nMapping fMRI signals to CLIP space\n\nEach fMRI signal that reflects a specific image is a 3D data volume, and the value on position (i, j, k) is the relative brain activation on this voxel triggered by the image. We apply an ROI (region of interest) mask on this 3D volume to extract signals of cortical voxels that are task-related and have good SNRs. The signal is then flattened into a 1D vector and voxel-wise standardized within each scan session. The end results x fmri are used by our image reconstruction pipeline. We choose to use the ROI with the widest region coverage, and the length N of x fmri ranges from 12682 to 17907 for different brains in the NSD dataset.\n\nOur goal is to train two mapping models, f mi and f mc in fig. 1 (collectively denoted as f m ), that encodes x fmri \u2208 R N to h img = C img (x img ) \u2208 R 512 and h cap = C txt (x cap ) \u2208 R 512 respectively. Here C img , C txt are CLIP image and text encoders, and x cap is one of the image captions chosen randomly from the vetted caption pool. We construct both f m as a CNN with one Conv1D layer followed by four residual blocks and three linear layers. The training objective is a combination of MSE loss, cosine similarity loss, and contrastive loss on cosine similarity. We use the infoNCE definition [38] of contrastive loss, for the i th sample in a batch of size B:\nContra(a (i) , b (i) ) = \u2212Ei log exp(cos(a (i) , b (i) )/\u03c4 ) B j=1 exp(cos(a (i) , b (j) )/\u03c4 )(1)\nFor the mapping model f mi that encodes fMRI to image embeddings, we have h\n(i) img = f mi (x (i) fmri )\n. The training objective is:\nLmi = Ei \u03b11||h (i) img \u2212 h (i) img || 2 2 + \u03b12(1 \u2212 cos(h (i) img , h (i) img )) + \u03b13 Contra(h (i) img , h (i) img ),(2)\nwhere \u03c4, \u03b1 1 , \u03b1 2 , \u03b1 3 are non-negative hyperparameters selected through sweeps. The loss L mc for caption embedding mapping model f mc is defined similarly. Although CLIP embeddings are trained to be aligned, there are still systematic differences between image and text embeddings, with embeddings under each modality showing outlier values at a few fixed positions. In addition, we also notice the generated images emphasize either image content (object proximity, shape, etc.) or semantic features depending on which condition we use. Therefore, including both embeddings as the conditions for a generator can cover both ends, and that is why we train two mapping models for the two modalities. Since the outlier indices are fixed for each modality across images, clipping the value should not affect image-specific information. Therefore, before normalizing the ground truth embeddings into unit vectors, we set h = clamp(h, \u22121.5, 1.5). This can greatly improve the mapping performance during training.\n\n\nImage reconstruction with CLIP embedding conditioning\n\nThe mapping models output fMRI-mapped CLIP embeddings h img and h cap that serve as conditions for the generative model. We aim to generate images that have both naturalness (being photo-realistic) and high fidelity (can faithfully reflect objects and relationships in the observed image). Our generation model is built upon Lafite [41], a text-to-image generation model: it adapts unconditional StyleGAN2 [20,19] to conditional image generation contexted on CLIP text embeddings.\n\nIn our generator G, both conditions h img and h cap are injected into the StyleSpace: each of them goes through two fully connected (FC) layers and is transformed into condition codes c img and c cap . These condition codes are max-pooled and then concatenated with the intermediate latent code w \u2208 W, which is obtained from passing the noise vector z \u2208 Z through a mapping network (see fig. 1). Using a mapping network to transform z into an intermediate latent space W is the key of StyleGAN as W is shown to be much less entangled than Z [35]. The conditioned style s is then passed to different layers of G as in StyleGAN2, generating image x img :\ns = w|| max(c img , c cap ), x img = G(s).(3)\nWe align the semantics of generated x img and condition vectors by passing x img through pre-trained CLIP encoders and apply contrastive loss (eq. (5) L c2 ) between them. For further alignment of the lower-level visual features, such as prominent edges, corners and shapes, we also pass the image through resnet50 and align the position-wise averaged representation obtained from Layer2 (eq. (5)L c3 ).\n\nThe discriminator D has three heads that share a common backbone: the first head D d classifies images to be real/fake, the second and the third semantic projection heads D si , D sc map x img to h img and h cap . The latter two ensure the generated images are faithful to the conditions. It is also shown that contrastive discriminators are useful for preventing discriminator overfitting and improving the final model performance [18,16]. Applying contrastive loss (eq. (5) L c1 ) between the outputs from discriminator semantic projection heads and the condition vectors fed to G can therefore help stabilize the training. To summarize the objective function, the standard GAN loss is used to ensure the naturalness of generated x img :\nLGAN G = \u2212Ei log \u03c3(D d (x (i) img )) , LGAN D = \u2212Ei log \u03c3(D d (x (i) img )) \u2212 log(1 \u2212 \u03c3(D d (x (i) img ))) ,(4)\nwhere \u03c3 denotes the Sigmoid function. Meanwhile, contrastive losses are used to align the semantics of generated images and the fMRI-mapped condition vectors that supposedly residing in the CLIP space:\nLc1 = Contra(Dsc(x (i) img ), h (i) cap ) + Contra(Dsi(x (i) img ), h (i) img ), Lc2 = Contra(Cimg(x (i) img ), h (i) cap ) + Contra(Cimg(x (i) img ), h (i) img ), Lc3 = Contra(ResNet(x (i) img ), ResNet(x (i) img ))(5)\nThe overall training objectives are: L G = LGAN G + \u03bb1Lc1 + \u03bb2Lc2 + \u03bb3Lc3, L D = LGAN D + \u03bb1Lc1, where \u03bb1, \u03bb2, \u03bb3, are non-negative hyperparameters.\n\nThe whole generation pipeline, consisting of mapping models and GAN, is trained in two stages. First, mapping models f mi and f mc are trained on fMRI-CLIP embedding pairs. Next, starting from the trained mapping model weights and Lafite language-free model weights, we modify the losses and model structure and finetune the conditional generator. For the additional condition vector projection layers in G and semantic head in D, we duplicate the weights in the existing parallel layers to make the model converge faster. Note that Lafite is pre-trained on the Google Conceptual Captions 3M dataset [32] then finetuned on the MS-COCO dataset, both of which are much larger than NSD. Finetuning from it allows us to exploit the natural relationships between semantics and images with sparse fMRI data. We can still utilize a two-stage training to compensate for data scarcity even if no pre-trained conditional GAN like Lafite is available, for example, when using a different generator architecture. Only this time, we should firstly train the conditional GAN on a large image dataset with noise perturbed h img and h cap as the pseudo input condition vectors.\n\n\nResults\n\n\nData and experimental setup\n\nThe NSD data is collected from eight subjects. We focus on reconstructing observed scenes from a single subject's brain signals. The reasons are twofold: first, it is more accurate to utilize individual brain coordinates instead of mapping voxels into a shared space, which can result in information loss during the process. More importantly, brain encoding and perception are different among individuals. This project aims to get the best reconstruction for a single individual, thus training models on one subject's data. Nevertheless, the commonality of this encoding process among the population is an exciting topic for future explorations.\n\nWe use subject one from NSD: the available data contains 27750 fMRI-image sample pairs on 9841 images. Each image repeats up to three times during the same or different scan sessions. Note that brain responses to the same image can differ drastically during the repeats ( fig. 7). The dataset is split image-wise: 23715 samples corresponding to 8364 images are used as the train set, and 4035 samples corresponding to the remaining 1477 images are used as the validation set. Therefore, our pipeline never sees the image it will be tested on during the training. We use 1pt8mm-resolution scans and only consider fMRI signals from voxels in the nsdgeneral ROI provided by the dataset. This ROI covers voxels responsive to the NSD experiment (voxels with high SNR) in the posterior aspect of the cortex, and contains 15724 voxels for subject one (x fmri \u2208 R 15724 ). Images are all scaled to 256 \u00d7 256. Additional experiment settings , including hyperparameters of two training phases, are provided in appendices A.1 and A.2. Our experiments are conducted on one Tesla V100 GPU and one Tesla T4 GPU. The code is publicly available. 1 \n\n\nMapping models from fMRI to CLIP embeddings\n\nEvaluation Criteria In the first training stage, mapping models f mi and f mc are trained to encode fMRI signals to CLIP embeddings. We use two criteria to evaluate the mapper performance to decide which one to use in the next stage. The first criterion is FID (Fr\u00e9chet Inception Distance) [14] between generated image and ground truth using the trained mapper and a pre-trained generator. Given a Lafite model pre-trained on MS-COCO (language-free setting), we can replace its conditional vector with the outputs of our mapping models to generate images conditionally. These FIDs can indicate the starting points of the finetuning processes: the lower the FID, the better the candidate model. Secondly, we use the success rate of image \"retrieval\" in a batch of size 300. For the i th sample in the batch, if the cosine similarity between h (i) and h (i) is larger compared to between h (i) and h (j) , j = i, then it counts as one successful forward retrieval. For backward retrieval, we count the number of correct matches of h (i) to all h (j) .\n\n\nConfiguration comparisons\n\nWe tested different configurations on the mapping models, including: (1) Whether to place the threshold at \u00b11.5 as mentioned in section 2.2; (2) When training f mi , whether to perform image augmentations before passing images through the CLIP encoder; (3) When training f mc , whether to use the CLIP text embedding of a fixed caption, a random valid caption, or use the average embedding of all valid captions; (4) Which loss function to use: MSE only, cos (cosine similarity) only, Contra only, MSE + cos, MSE + cos + Contra; (5) Whether auxiliary networks help. We tested adding an auxiliary discriminator with GAN loss, as well as adding auxiliary expander networks with VICReg loss [4].\n\nWe found: (1) Clamping ground truth embeddings significantly increase performance; (2) Using image augmentations increase f mi performance. This further indicates CLIP embeddings are more semantic related; (3) For f mc , selecting a random caption from the valid caption pool each time is better than using a fixed one or using the average embedding of all valid captions; (4) Using MSE + cos as the loss gives the best base models, but then finetune these base models with MSE + cos + Contra can further lower the starting FID for pipeline finetuning, making the training in the next stage converge faster; (5) Adding auxiliary networks and objectives will not improve the performance. In general, although h cap and h img are already relatively well aligned, f mc can still map x fmri closer to h cap than h img , whereas f mi maps x fmri to an embedding that is equally close to both, while being able to capture a few extreme values in h img (see appendix A.3 for numerical details and mapped embedding visualizations). We think this difference reflects that it is easier to map fMRI signals to a more semantic representation (from the text space) than to a visual one.\n\nTo verify fMRI-mapped embeddings h are semantically well aligned with ground truth CLIP embeddings, we examined the mismatches during the image retrieval. For four incorrect retrievals, fig. 3 shows which images' h (j) are closer to the ground truth images' h (i) than h (i) . Notably, these mismatches are semantically close to the ground truth images. This indicates that the mapping models 1 https://github.com/sklin93/mind-reader For each ground truth image i (green frame), we pass it through CLIP encoder to get h (i) and through fmc to get h (i) . The shown incorrect ones are those images with h (j) , j = i that is closer to h (i) than h (i) . can successfully map fMRI signals into a semantically disentangled space. Embeddings in this space are suitable for providing contexts to a conditional generative model. We also tested another mapping model f mr that maps fMRI signals to representations obtained from resnet50 Layer2. Unlike the CLIP embedding space, the resnet vector encodes more lower-level visual features. We see a jump in the image retrieval rate when we combine the representations obtained from f mi , f mc with f mr (table 4). However, the generative model is difficult to train when taking in two conditions from distinct embedding spaces. Therefore, we add the low-level vision constraint into the contrastive loss L c3 instead.\n\n\nConditional image generation\n\nQuantitative results In the second training stage, we finetune the conditional StyleGAN2. 2 There is no standard metric to measure image reconstruction quality from fMRI signals for complex images.\n\nSince previous works focused on reconstructing simpler images, the metrics typically involve pixelwise MSE or correlation measures. However, when it comes to complex images, it seems more reasonable to use a perceptual metric, such as FID, which is based on Inception V3 [36] activations and is widely used in GAN. We also detail another metric, n-way identification accuracy, that reflects more of the fidelity and uniqueness of the generated images, in appendix A.4. We perform the ablation studies on the pipeline to answer the following questions: (1) Which mapping model trained in stage one leads to the best final performance? f mc or f mi or using both? (2) Which pre-trained GAN leads to the best final performance? For this, we compare using Lafite pre-trained on either the langue-free (LF) setting or the fully supervised setting. (3) Whether including the contrastive loss L c3 between lower-level visual features can further improve the performance of a semantic-based generative model? Finally, we tested (4) whether finetune the whole pipeline end-to-end or freezing the mapping models is better? The new mapping model losses are set to\nL m = Lm + \u03bb4LGAN G if trained end-to-end.\nResults are reported in table 1. We observed the following: (1) in terms of FID, using h img obtained from f mi as the generator condition is better than using the h cap from f mc or using two conditional heads. On the other hand, f mc and the two-head setting achieve as good or even better performance as f mi does in terms of n-way identification accuracy. In addition, if training time or resource is the concern, using two heads and pre-trained LF-Lafite with only condition feeding interface changes and cloned weights in the new branches can already give reasonably good results. (2) Training the pipeline on LF-Lafite is much better than on the fully supervised Lafite. This result is expected for the generator conditioned on h img since the supervised version is conditioned on CLIP text embeddings.\n\n(a) Ground truth stimuli (top row) and generated images conditioned on fMRI (bottom row).\n\n(b) Generated images from three different fMRI scans responding to the same stimulus (green frames).  (a) Image reconstruction results from fMRI in previous works.\n\n(b) Image reconstruction results from fMRI by our pipeline. Four ground truth images are green framed. Figure 5: Comparisons between previous works and our pipeline. We are using the recent NSD dataset that involves more complex scenes. However, for comparison purposes, we choose four similar images from NSD, each containing a single object \"plane\", and show our reconstructions from fMRI signals in fig. 5b However, the same discrepancy exists for the generator conditioned on h cap . This may reflect the flexibility of pre-trained generators to adapt to the slight changes in the embedding space. It also shows the crucial impact of a pre-trained model on final performance when training data is limited.\n\n(3) The addition of low-level visual feature constraint L c3 is beneficial for the model performance, especially faithfulness. It also seems to have more effects on single-head models than the two-head one. (4) For the end-to-end pipeline training, we test performance with \u03bb 4 = [0.1, 1, 10], all of which give worse performance than keeping the mapping model weights frozen (reported values are from \u03bb 4 = 1). In particular, we found that h tends to collapse to having nonzero values at only a few positions if the mappers are finetuned together with GAN.\n\nQualitative results We show several generated images in fig. 4. Although the generator takes in both the noise vector z and fMRI-mapped embeddings, the results vary much more with the latter condition, while z only contributes to variations on some minor details. In general, the generated images capture both semantics and visual features relatively well, even on complex images containing interactions of multiple objects. Since each stimulus is repeated up to three times to the subject, we have multiple fMRI scans corresponding to the same image. The semantic differences in the generated images conditioned on these multiple scans could potentially reveal brain processing discrepancies of the same stimulus. For example, the three generations for the second image in fig. 4b emphasize respectively: (1) the overall scene and the fence, (2) people with green suits, and (3) overhead flags and the fence; these might reflect the variations in the subject's attention or interpretations of that image. Eyetracking data can be further examined to study attention's effect on generated images.\n\nIt is challenging to perform one-to-one comparisons with previous deep image reconstruction works since the images in the MS-COCO dataset have much higher complexities than artificial shapes, faces, or images containing a single centered object (like in ImageNet). We show results from a few best models for reconstructing images from fMRI in fig. 5a. There is also a recent survey [28] covering more models and results if readers are interested. As our dataset is different, we search for similar images in the NSD validation set and show our generations in fig. 5b. Compared to other methods, our pipeline can generate more photo-realistic images that reflect objects' shapes and backgrounds well. It also utilizes more voxel activities than previous works (15724 voxels versus a few hundred). More importantly, it is able to reconstruct the relationships of different components when the images are more complex. As natural scenes around us are rarely isolated objects and always information-laden, we think reconstructing images through semantic alignment and conditioning is more beneficial and realistic than focusing on lower-level visual features.\n\nMicrostimulation In neuroscience, microstimulation refers to the electrical current-driven excitation of neurons and is used to identify the functional significance of a population of neurons. Here, we \"microstimulate\" the input fMRI signals of voxels in different brain ROIs, aiming to identify the roles of individual regions. In the NSD dataset, there are four floc (functional localizer) experiments targeting regions responsible for faces, bodies, places, and words. A typical standardized fMRI signal has a value range around [\u22124, 4]. For the experiment, we locate the corresponding task-specific voxels based on ROI masks and increase the voxel activities to 10 while keeping the activities in unrelated voxels unchanged (see appendix A.5 for visual results). We observe the emergence of bodies or words when we increase the voxel activities in \"bodies\" or \"words\" ROIs. For voxels in \"places\" ROIs, elevating the signals will result in mesh-like patterns in the background, and this is true across different images. For \"faces\" ROIs, the generated images under elevated facial area signals seem to contain many small repeated patterns/perturbations. Interestingly, this appears to result from FFA (fusiform face area) signal changes since increasing only OFA (occipital face area) regions' activity does not result in similar patterns. Overall, increasing a specific task ROI's signal across fMRI samples results in CLIP embedding changes in similar positions. This means the disentangled space of CLIP embedding aligns well with how the human brain processes visual cues.\n\nApart from task-specific ROIs, we also changed brain region activities based on their roles in the visual processing hierarchy. We use the streams mask in the dataset to identify early visual cortex ROIs, intermediate ROIs, and higher-level ROIs. We then zero out voxels at each level. Our observations are: (1) when silencing the early visual cortex, objects and the whole scene are prone to be in dull colors, and objects tend to have sharp shapes. Meanwhile, the mapped embedding in the CLIP space will constantly have a lower value at almost all positions compared to mapped from unchanged signals .\n\n(2) Silencing the higher-level ROIs has the opposite effect: more colors, more shapes, and crowded scenes. This is reasonable since the lower-level visual regions will bring up all the details when they lack high-level control. This time, the embeddings in the CLIP space have values consistently higher than normal. Finally, (3) silencing the intermediate ROIs seems to have the least visual impact or CLIP embedding changes among the three. We performed the above microstimulation experiments on our pipeline with existing ROIs; however, it is potentially helpful for testing new ROI definitions and hypotheses.\n\n\nCLIP space as the intermediary\n\nIn this section, we show that multimodal embedding space, particularly the CLIP space, is beneficial for brain signal decoding. To this end, we trained a set of multi-label category classifiers to classify if a certain object category exists in the image based on the following inputs: (1) image-triggered fMRI x fmri \u2208 R 15724 ; (2) image CLIP embeddings h img \u2208 R 512 ; (3) CLIP embeddings mapped from image-triggered fMRI h img \u2208 R 512 ; (4) image ResNet embeddings ResNet(x img ) \u2208 R 2048 (obtained from Layer4, the final block before fully connected layers). All classifier models consist of 3 linear layers with ReLU activations in between, and finish with a Sigmoid activation. For fMRI signals, we use (2048, 512) as the hidden dimension; for CLIP embeddings (setting (2) and (3)), we use (384, 256) as hidden dimensions; and for the ResNet embedding, we use (512, 256) as hidden dimensions. The final output covers 171 classes, including 80 things categories (bounded objects, like \"person\", \"car\"), and 91 stuff categories (mostly unbounded objects, like \"tree\", \"snow\"). 3 Binary cross-entropy loss is used for each class to predict its existence in the input image. Figure 6: Category-wise AUC-ROC of multi-label classifiers that predicts from four different signal / embedding sources. The first 80 categories are \"things categories\" and the last 91 are \"stuff categories\" in COCO.  (2) and (3) is minimal, meaning mapping fMRI signals into the CLIP space retains most of the fMRI signals' information: this provides strong support for the validity of our design. Lastly, ResNet embeddings perform poorly compared with other input sources. Therefore, even with a perfect mapping model, projecting fMRI signals into this space will lose information about the image since the expressiveness of the embedding is bounded by the lower performer. In addition, we note that both CLIP embeddings and fMRI have poorer performance on stuff categories than on things categories, whereas ResNet embeddings do not. This can indicate brain signals align better with the multimodal CLIP space than with single-modality ResNet space. Previous brain signal decoding work utilizing pre-trained generators all relied on image-only embedding spaces (ResNet-50 [23], VGG19 [34]), and we believe moving to a multimodal latent space is a crucial step towards better brain signal decodings.\n\n\nFurther discussions\n\nPrior to our current pipeline design, we experimented with a DALL-E-like structure [30] since we can view the image reconstruction problem as signal-to-signal translation. In particular, we applied VQVAE [39] on both fMRI and image to represent them as discrete latent codes and train a Transformer model to autoregressively generate text and image tokens from fMRI tokens. However, it was challenging to train the Transformer-based model to converge with limited fMRI-image data. Incorporating the caption as text tokens to serve as the bottleneck between fMRI and image tokens while utilizing pre-trained models on the text and image modality did not help either. We think this suggests the need to introduce a semantic medium to avoid direct translations between fMRI and image, as well as a solution to data scarcity.\n\nWe address both issues with the semantic space of CLIP embeddings. First, CLIP space is semantically informative and visually descriptive: for example, we can use image-text CLIP embedding alignment probabilities to screen captions. Mapping fMRI signals to representations in this latent space will retain rich information about the image that needs to be reconstructed. Second, the pre-training of the generative model can be separated entirely from fMRI data, meaning it can utilize much larger datasets than the one we use. However, there is a trade-off between generating a semantically similar scene and faithfully reconstructing each pixel. Although trained with additional contrastive loss targeting low-level visual features, the generated images by our pipeline are still leaning towards the former. We consider this a reasonable choice since brains are more likely to perceive the image as a whole rather than identifying each pixel, especially with multiple objects in the scene. Nevertheless, this results in worse reconstructions for images with fine details but less semantic, such as single faces. The reconstruction of complex images with better aligned low-level visual features is worth further studies.\n\nThere are many more areas to explore. First, our study focuses on reconstructing a single subject's brain signals. Applying the model to different subjects and observing the differences when generating the same image would be interesting. Since the data contains behavioral measures like valence and arousal towards each image, one can test if the generated images reflect personalized attention and perceptions. Second, other latent spaces can be examined. Although CLIP is one of the best-aligned computation models for the brain, other multimodal models like TSM [2] seem to have a better alignment [9] with the visual cortex. In addition, other conditional generative models, such as diffusion models, can be explored. In particular, DALL-E 2 [29] generates images conditioned on CLIP embeddings through diffusion, and it also provides an alternative solution to the differences exhibited in the image the text CLIP embeddings by learning a Prior model. Third, given the additional text modality, our pipeline opens up new opportunities to study visual imagery even without ground truth images. For example, one can either use mapping models trained on given fMRI-image pairs and pre-trained generators to reconstruct imagined scenes, or study the mapping between brain signals and the text embeddings of the mental images' descriptions. Lastly, we focus on the decoding (brain-to-image) process, but the encoding (image-to-brain) process of complex images is equally important and exciting (we provide initial results on encoding in appendix A.8; additional future directions are discussed in appendix A.9).\n\nWith current brain signal recording devices, the negative social impact of this work is minimal: portable devices like EEG have poor spatial resolutions, making them unlikely to provide enough image-related details; On the other hand, fMRI scanners are used under highly controlled settings with designed procedures, therefore unlikely to have subject-unapproved privacy violations. However, when new devices that can address these issues become readily available, regulations would be needed on collecting and inspecting user data, since they potentially reveal sensitive information that users are unwilling to share through neural decoding. With pre-trained components, the pipeline may also misinterpret brain signals or be hacked to generate from manipulated inputs (no matter how unlikely it is) and produce over-confident false reconstructions because of the training data distributions. Several tricks may alleviate this issue, for example, training an input discriminator and placing it before the entire pipeline to filter out suspicious inputs. Or, using a parallel pipeline targeting pixel-level reconstruction as a check: if the two systems agree with each other above a certain threshold/confidence, the reconstruction results are accepted, otherwise discarded. Future pipeline improvements should also focus on exploring high-performing models pre-trained on large (thus more generalizable) and unbiased datasets.\n\n\nConclusion\n\nThe paper proposes a pipeline to reconstruct complex images observed by subjects from their brain signals. With more objects and relationships presented in the image, we bring in an additional text modality to better capture the semantics. To achieve high performance with limited data, we utilize pre-trained semantic space that aligns visual and text modalities. We first encode fMRI signals to this visual-language latent space and use a generative model conditioned on the mapped embeddings to reconstruct the images. We also introduce additional contrastive loss to incorporate low-level visual features into this semantic-based pipeline. As a result, the reconstructed images by our method are both photo-realistic and, most of the time, can faithfully reflect the image content. This brain signal to image decoding pipeline opens new opportunities to study human brain functions through strategic input alterations and can even potentially be helpful for human-brain interfaces.\n\n\nA Appendix\n\nA.1 Data fMRI data fMRI activities differ when the same individual sees the same image at different times ( fig. 7). Although we use activities and signals interchangeably throughout the paper, what we mean are fMRI betas in the NSD dataset. Betas are not direct measurements of BOLD (blood-oxygenationlevel dependent) changes, but the inferred activities from BOLD signals through GLM (general linear models). The reason for using betas instead of direct measurements is that image stimuli are shown consecutively to the subjects without prolonged delay, and activities triggered by the previous image can interfere with the next one if there is no proper separation. Authors of NSD proved the effectiveness of their GLM approach with much improved SNR in the betas over raw measurements [3]. Image augmentation during training Based on conclusions from StyleGAN2-ADA [19], we perform the following image augmentations before passing images into the CLIP encoder when training the fMRI-CLIP mapping model:\n\n\u2022 perform random sized crop with a scale between 0.8 to 1. We test mapping models trained with and without the above augmentations, and found augmentations can improve fMRI to CLIP image embedding mapping performance (details are in table 3).\n\nCLIP embeddings and thresholding See fig. 8 for the visualizations of CLIP embeddings that show image and text embedding differences, effects of thresholding, image augmentation, and random caption selection.\n\n\nA.2 Experiment hyperparameters\n\nThe following hyperparameters are used in our experiments:\n\n\u2022 \u03c4 = 0.5 in eq. (1) for all the contrastive losses.\n\n\u2022 for fMRI-CLIP mappers f mi , f mc (losses are in eq. (2)), the models are first trained with \u03b1 1 = 0.4, \u03b1 2 = 0.6, \u03b1 3 = 0, then finetuned with \u03b1 1 = 0.2, \u03b1 2 = 0.3, \u03b1 3 = 0.5. \u2022 mappers are trained with batch size 32 (on a single GPU) when not including contrastive loss, and batch size 128 when including contrastive loss or using VICReg loss. Learning rate is 0.0004. \u2022 \u03bb 1 = 5, \u03bb 2 = 10, \u03bb 3 = 10 for the losses of conditional StyleGAN2. \u2022 conditional StyleGAN2 is trained with batch size 16 \u00d7 number of GPUs (in our case B = 32 since we used two GPUs). Learning rate is 0.0025.  table 3. Simply put, forward retrieval checks the correct match of \"which ground truth CLIP embedding is the closest to the fMRI-mapped one?\" while the backward retrieval checks \"which fMRI-mapped embedding is the closest to the ground truth CLIP one?\". When multiple losses are involved, we use hyperparameter settings as in A.2. Fig. 9 visualizes the mapping results of the best setting (models trained with threshold, image augmentation, use a random valid caption each time, pre-trained with MSE+cos loss then finetuned with MSE+cos+Contra loss).\n\nCombining the mapped embeddings from multiple mappers boosts the retrieval performance, especially the backward one (as shown in table 4). To use multiple mapping models, we first calculate a B \u00d7 B batch similarity matrix between the mapped embeddings for each model. Then we combine the similarity matrices with a weighted sum (weights are obtained through grid search) and perform image retrievals based on this combined similarity matrix. The mapping model f mr that encodes fMRI to ResNet embeddings has a correct forward retrieval 6 and backward retrieval 50. But when its similarity matrix is combined with mapped-CLIP embedding similarity matrices, the performance is far above that of both ResNet and CLIP embeddings.\n\n(a) fmi(xfmri) with himg and hcap (b) fmc(xfmri) with himg and hcap   \n\n\nA.4 Additional quantitative results (generator)\n\nIn addition to using FID as a metric, we also perform 2-way identification for images reconstructed by models under different settings, and n-way identification of generated images with n = 2, 5, 10, 50 under the best setting (finetuned from LF, with L c3 , with mapping models f m frozen). For n-way identification, we reconstruct an image from the fMRI signal for each sample in the validation set. For each generated image, we compare it with a set of n randomly selected images, including the ground truth one. Then based on the cosine similarity of their Inception V3 embeddings (before FC layers, the length-2048 vector), we identify which image the generated one corresponds to. This process is repeated ten times because of the randomness of the n-sample selection. Results are reported in tables 5 and 6. The n-way identification accuracy of the two-head setting (f mi & f mc ) is slightly better most of the time (table 6), but this advantage is minor when taking the standard deviations into account. Note that when performing n-way identification, previous image reconstruction works are typically tested on a validation set that contains 50 images of 50 different categories [15]. However,  there are multiple objects involved in each image in the complex images we aim to reconstruct; it is not straightforward to separate them into different categories and pick one from each. Therefore, we leave the validation set as is (1477 image-fMRI pairs in total), and there will be overlapping categories in it; for example, several images contain scenes of animals in a natural environment.  A.6 Using pre-trained models\n\n\nA.5 Visual results from microstimulation experiments\n\nOur pipeline relies on two pre-trained components. The first and the most crucial one is the CLIP encoder that provides the latent space where we project fMRI signals. The second is a conditional  GAN (Lafite) that generates images, which could be swapped for other generators. In what follows, we will discuss these two components separately.\n\nCLIP One exciting aspect of CLIP is the size of its training dataset, which consists of 30 million Flicker images that should cover most of the natural image statistics. This coverage is also proved by subsequent works that generate images guided by CLIP embeddings through their abilities to perform generations in various styles. In addition, as we observed in 3.4, CLIP embeddings can retain around 98% of object-level information in fMRI with a very well-aligned performance across categories.\n\nAlbeit its incredible expressive power, CLIP does have a much lower dimensionality than the original signal: no matter how faithful, it is a compression. By the nature of compression, CLIP only retains the most crucial information and removes most of the redundancies in the original signal. Indeed, if we mask fMRI at different ratios, from 0 to 1, and perform the multi-label classification (the same task as in 3.4) using (1) masked fMRI or (2) CLIP mapped from masked-fMRI, we will notice a very drastic difference in the performance drop rate. As shown in fig. 13, prediction performance from fMRI only drops drastically after the masking ratio becomes larger than 0.9, indicating brain redundancies to represent the objects. In contrast, if we map the masked fMRI into the CLIP space and use these embeddings for prediction, the performance drop is almost at a constant rate. This discrepancy makes the CLIP space more vulnerable to adversarial attacks than the fMRI space since a small change would cause the generated images to derail from the ground truth. In addition, CLIP embeddings also carry more biases than fMRI, as its mean AUC-ROC is much larger even with all-masked inputs. One should consider these traits of CLIP embeddings when applying this system and design defense mechanisms accordingly.\n\nLafite As for the generator, we utilize a conditional GAN pre-trained on the MS-COCO dataset (containing 328K images), from which NSD drew its experiment images. This naturally provides an alignment in the data distribution. Although MS-COCO images are about everyday objects, humans, and scenes, the data statistics could vary when we move to other settings. Therefore, future studies are needed to extend current generators to one trained on broader sources (e.g., DALL-E 2, mentioned in section 4, used 650M images sampled from CLIP and DALL-E training data). This should minimize the dataset biases, although one should not interpret results without considering the training/testing discrepancies.\n\nTo show that our concept works across different generators, but dataset biases indeed play an important role, we test our pipeline with a Lafite pre-trained on the Google Conceptual Captions 3M dataset (CC3M, consisting of 3.3 million images) as the generator without any extra finetuning. We used our trained f mc as the mapping function. The results are shown in fig. 12. All generated images have the watermark where CC3M sources its images. In addition, when trying to generate out-of-distribution images, the quality decreases in terms of photo-realism. Nonetheless, semantic alignments are still shown in these reconstructions. We also want to note that pre-trained models provide excellent bases for finetuning. For example, Lafite finetuned its COCO model on the CC3M model within three hours, compared to four days to reach the same performance if training from scratch. Therefore, if the pipeline is known to be used on certain types of images, a small-scale dataset and some light training should greatly help the model to fit into the desired data distribution.\n\n\nA.7 More examples\n\nAs mentioned in section 2.2, we found that the generated results conditioned on embeddings of different modalities tend to emphasize different aspects: more visual (colors, shape, etc.) if conditioned only on h img , and more semantic if conditioned only on h txt . This could reflect the slight difference between the latent space of the two modalities. We show the examples conditioned on either one of these two conditions, or both, in fig. 14.\n\nThe pipeline tends to fail under the following conditions: (1) the image only contains close-up details without too much semantic information; (2) the presented scene is semantically novel (e.g., a big banana-shaped decoration hanging in the middle of the room). The model also tends to: (3) generate based on data biases: adding windows to indoor scenes, adding people to food scenes, generating colored images when the inputs are black-and-white, etc.; (4) change or ignore the background; (5) Mix-up colors (assigning colors in the scene to a wrong object); (6) generate the wrong number of objects/people. We showcase these failures together with more other generated images in fig. 15. Given that the model is confident (in terms of GAN's discriminator output staying at the same level) when generating results based on training data biases, future extensions should focus on exploring generators pre-trained on a much larger dataset, as discussed in A.6. \n\n\nA.8 Encoding and encoding-decoding cycle\n\nThis paper mainly focused on decoding brain activities. However, we also tested the encoding process with CLIP as the intermediate. In this section, we briefly present our results, as well as the complete encoding-decoding cycle.\n\nBrain Encoding Brain encoding is a problem that predicts brain activities from stimuli. It has a data scarcity problem similar to the decoding process. In addition, brain activities are intrinsically noisy and contain randomness, even when responding to the same stimulus. To this end, we solve the problem similar to the decoding process: the image stimuli are passed through pretrained CLIP encoders, obtaining CLIP embeddings h img . Then we train a mapping model that perform regression from h img to x fmri . The mapping model is also similar to f m , consisting of four residual blocks, one transposed convolutional layer, two linear layers, and is trained with a combination of MSE and cosine similarity loss. Fig. 16a shows the signal ground truth and predictions for the first 1000 voxels of two samples. We also found that voxel-wise prediction (in terms of the correlation coefficient) aligns very well with the noise ceiling of that voxel (see fig. 16b). 4 However, there are discrepancies in this alignment: in fig. 16c, we visualize the voxel-wise prediction correlation coefficient (cc) minus the voxel's noise ceiling (nc) as a flatmap. Here, redder areas correspond to better predictions, and the result shows that high-level semantic regions are better predicted than V1-V4. Utilizing latent spaces other than CLIP's results in lower prediction performance and larger distance between cc and nc, as well as a more uniform performance among high-level regions and V1-V4.\n\nComplete Cycle We tested the encoding-decoding cycle with trained encoding and decoding pipelines: ground truth images are fed to the encoding pipeline, which gives fMRI predictions. We then pass these predicted fMRI signals through the decoding pipeline to perform decoding. The results are shown in fig. 17. We observe that image semantic information is still relatively well conserved. Figure 17: Encoding-decoding cycle. The top row shows image stimuli; the second row shows predicted fMRI activities (with corresponding ground truth) by the encoding pipeline (only 300 voxels are shown for visualization purposes); the third row shows reconstructed images from predicted fMRI signals. Figure 18: Generated images from interpolation of two fMRI scans.\n\nStep number is set to 10.\n\nA.9 Additional future directions Input interpolations and the potential extension to movie reconstruction In addition to reconstructing observed images, we found utilizing the CLIP space can also result in a smooth transition when decoding from interpolations of two fMRI scans ( fig. 18). Combined with the ability to capture complex semantics, this pipeline can be helpful for movie reconstruction from brain signals. Temporal constraints can also be added, which could, in turn, benefit the reconstruction of each frame.\n\nDecoding text from fMRI Apart from being the conditional vector for an image generator, CLIP embeddings can also be used to generate texts. To decode texts from fMRI data, the only change needed is replacing the conditional image generator in our pipeline with a text generator conditioned on CLIP vectors. 5 With this text pipeline, one can \"define\" the functions of each voxel through the following procedures: (1) provide a pseudo-fMRI activity to the pipeline with only the target voxel having non-zero activities, (2) generate fMRI-mapped CLIP embeddings h with the mapping models f m , (3) provide h to the conditional text generator and get the text description of that voxel activity. An advantage of decoding the signals into the text form is that text is more straightforward than images in terms of explaining the semantics. This makes it easier to perform voxel clusterings and to find brain modules. The texts can also help understand which parts of the semantics are not mapped through from the f m by comparing the ground truth captions and generated texts from the fMRI activities.\n\nNeural population control with synthetic images With the encoding pipeline that we briefed in A.8, one can feed the pipeline with artificial images to test and understand how different shapes and semantics trigger voxels at various locations, thus having a better understanding of voxel functionalities. In addition, works similar to [5] can be tested by finding out which type of stimuli trigger a specific level of brain activity (e.g., higher activation) and then synthesizing images that control the neural population in the desired manner. Lastly, given pipelines of a complete cycle, images generated by the decoder can also be benchmarked by passing them through the encoder.\n\nFig. 2\n2\n\nFigure 2 :\n2Image caption screening through CLIP encoders. For this sample, threshold is put at half of the largest probability: 0.5 \u00d7 0.519. Therefore, captions(2) and(3)of the image are kept.\n\nFigure 3 :\n3Mismatches are semantically close to the ground truth. Figure shows examples of incorrect matches j (red frame) in a batch of 300 in the validation set.\n\nFigure 4 :\n4Images generated by our pipeline given input fMRI signals.\n\nFigure 7 :\n7fMRI activities responding to two images, each repeating three times. The figure only shows the activities of the first 200 voxels for visualization purposes.\n\n\n\u2022 perform horizontal flip with probability p = 0.5. \u2022 perform ColorJitter(0.4, 0.4, 0.2, 0.1) with p = 0.4. \u2022 perform grayscale with p = 0.2. \u2022 perform Gaussian blur with p = 0.5 and kernel size 23. \u2022 perform random masking with 0.3 masking ratio.\n\nFigure 8 :\n8CLIP vector visualizations and thresholding. 8a: before (left column) v.s. after (right column) thresholding at \u00b11.5 to remove outliers. There are systematic differences between CLIP image embeddings and text embeddings; the outliers typically occur at the same positions for each modality. 8b: the caption screening process can make the kept caption embeddings more aligned.(b)1 and (b)2 are from the same sample, only difference is the screening process. 8c: (thresholded) embeddings of the same image with different augmentations; embeddings of same image's different screened captions. All embeddings are shown the first 200 values for visualization purposes. A.3 Results for the fMRI-CLIP mapping models f m Mapping models f mi and f mc are trained under different settings detailed in section 3.2, here we list the numerical results of the summarized findings in\n\nFigure 9 :\n9Embeddings mapped from fMRI signals overlay on ground truth CLIP embeddings. fig. 9a shows the results of image embedding mapping model fmi, and fig. 9b shows the results of caption embedding mapping model fmc. For visualization purposes, the figures only show the first 200 values of the length-512 vectors.\n\nFigure 10 :\n10Images generated in microstimulation experiments. In 10a10b, voxel activities at multiple task ROIs are increased before passed into the pipeline. In 10c, voxel activities at various visual processing stages are silenced.\n\nFigure 11 :\n11fMRI-mapped embeddings in the CLIP space (h ). Each figure contains (i) an embedding mapped from a regular fMRI signal, (ii) an embedding mapped from the fMRI signals with voxel activities in earliervisual ROIs (left)/ intermediate ROIs (middle) / higher-level ROIs (right) set to zero, (iii) an embedding mapped from the fMRI signal with voxels at random positions (same number of voxels as (ii)) set to zero. Setting activities of the earlier-visual cortex to zero lowers overall embedding vector values, while setting activities of higher-level ROIs has the opposite effect. We can also perform the reverse masking: only keep voxel activities at earlier-level visual/ intermediate / higher-level ROIs, then the effects are reversed.Fig 10 shows generated images under different microstimulation experiments. Fig 11 shows the results regarding changes of mapped fMRI embeddings in the CLIP space when perturbing voxels in different visual cortex levels.\n\nFigure 12 :\n12Image generated by Lafite pre-trained on the CC3M dataset without finetuning on COCO or NSD. Ground truth stimuli (top row) and generated images conditioned on fMRI (bottom row).\n\nFigure 13 :\n13Multi-label classifier (defined in 3.4)'s average sample-wise AUC-ROC changes when masking input fMRI at different ratios. For a masked voxel, we set its value to 0.\n\nFigure 15 :Figure 16 :\n1516More examples showcasing model successes and failures. For each two-row group, the top row shows the ground truth images, and the bottom row shows the reconstructions. Brain encoding results. (a) ground truth and prediction of two samples. Only the first 1000 voxels are shown for visualization purposes. (b) Voxel-wise performance (in terms of the correlation coefficient between ground truth and prediction) v.s. voxel noise ceiling. (c) Prediction performance on a flatmap, redder regions have more accurate predictions (accounted for the noise ceiling). Note we only perform prediction on the nsdgeneral ROI, thus the boundary.\n\nTable 1 :\n1FID of the pipeline under different settings.FID\u2193 \nfmi \nfmc \nfmi & fmc \nfrom supervised without Lc3 fm frozen 37.75 41.51 \n-\nfrom LF \nwithout Lc3 fm frozen 30.83 33.78 \n50.59 \nfrom LF \nwith Lc3 \nfm frozen 29.74 33.35 \n49.47 \nfrom LF \nwith Lc3 \nend to end 45.02 48.54 \n50.96 \n\n\n\nTable 2 :\n2Numerical AUC-ROC values of the classifiers presented in fig. 6.Fig 6 shows the category-wise AUC-ROC. The result demonstrates that CLIP embeddings contain the most object-level information about the image out of all the input sources. Following it, fMRI signals are also surprisingly very predictive, considering they carry a lot of noise. The performance discrepancy between settingsAUC-ROC \n\"things\" categories \"stuff\" categories \nOverall \nPerformance w.r.t. \nfMRI (%) \nCLIP \n0.9718 \u00b1 0.0266 \n0.8973 \u00b1 0.0639 0.9318 \u00b1 0.0624 \n112.36 \nfMRI \n0.8704 \u00b1 0.0557 \n0.7937 \u00b1 0.0824 0.8293 \u00b1 0.0807 \n100.00 \nfMRI-mapped CLIP \n0.8468 \u00b1 0.0604 \n0.7817 \u00b1 0.0733 0.8119 \u00b1 0.0748 \n97.90 \nResNet-50 \n0.7061 \u00b1 0.0736 \n0.7032 \u00b1 0.0719 0.7044 \u00b1 0.0725 \n84.94 \nfMRI-mapped ResNet-50 \n0.5410 \u00b1 0.1106 \n0.5520 \u00b1 0.0941 0.5469 \u00b1 0.1020 \n65.95 \n\n\n\nTable 3 :\n3Starting FID without generator finetuning (pre-trained LF-Lafite is used here) and correct retrievals \nin a batch of size 300 using embeddings obtained from fmi and fmc. In the top table, models are trained with \nMSE+cos loss. In the bottom table, defaults are: with threshold, with image augmentation, using random caption. \nFor the two options with auxiliary modules, the model is finetuned from MSE + cos model since training from \nscratch gives much worse results. FID evaluations are omitted if the retrieval performance of a setting is strictly \nworse than its competitors. \n\nthreshold \nno \nthreshold \nimage aug \nno \nimage aug \n\nfixed \ncaption \n\nrandom \ncaption \n\naverage caption \nembedding \nf mi \nFID \u2193 \n73.46 \n-\n73.46 \n-\nn/a \nn/a \nn/a \nRetrieval \n(forward) \u2191 \n21 \n13 \n21 \n19 \nn/a \nn/a \nn/a \n\nRetrieval \n(backward) \u2191 \n49 \n25 \n49 \n46 \nn/a \nn/a \nn/a \n\nf mc \nFID \u2193 \n75.24 \n-\nn/a \nn/a \n-\n75.24 \n79.36 \nRetrieval \n(forward) \u2191 \n14 \n11 \nn/a \nn/a \n13 \n14 \n15 \n\nRetrieval \n(backward) \u2191 \n64 \n45 \nn/a \nn/a \n39 \n64 \n43 \n\nMSE cos Contra MSE + cos \nMSE + cos + Contra \n(from scratch) \n\nMSE + cos + Contra \n(from MSE + cos) \nAuxiliary GAN \nAuxiliary expander \n(VICReg) \nfmi \nFID \u2193 \n-\n-\n-\n73.46 \n-\n68.14 \n-\n-\nRetrieval \n(forward) \u2191 \n5 \n12 \n25 \n21 \n27 \n29 \n25 \n19 \n\nRetrieval \n(backward) \u2191 \n16 \n34 \n50 \n49 \n50 \n51 \n42 \n35 \n\nfmc \nFID \u2193 \n-\n-\n-\n75.24 \n-\n53.68 \n-\n-\nRetrieval \n(forward) \u2191 \n4 \n10 \n27 \n14 \n30 \n33 \n24 \n9 \n\nRetrieval \n(backward) \u2191 \n19 \n31 \n42 \n64 \n43 \n45 \n38 \n37 \n\n\n\nTable 4 :\n4Correct image retrievals in a batch of size 300 when combining different models.Multiple models fmi + fmc fmi + fmc + fmr \nRetrieval \n(forward) \n32 \n24 \n\nRetrieval \n(backward) \n73 \n147 \n\n\n\nTable 5\n5: 2-way identifications accuracy of the pipeline under different settings. \n\naccuracy (%) \nfmi \nfmc \nfmi & fmc \nfrom supervised without Lc3 fm frozen 72.6 \u00b1 6.14 \n68.6 \u00b1 5.22 \n-\nfrom LF \nwithout Lc3 fm frozen 73.0 \u00b1 4.40 \n73.2 \u00b1 4.49 \n76.2 \u00b1 5.89 \nfrom LF \nwith Lc3 \nfm frozen 76.8 \u00b1 4.16 78.2 \u00b1 5.47 78.0 \u00b1 4.47 \nfrom LF \nwith Lc3 \nend to end 51.4 \u00b1 5.59 \n50.8 \u00b1 5.43 \n50.2 \u00b1 5.31 \n\n\nTable 6 :\n6n-way identification accuracy (%) with n = 2, 5, 10, 50. \nn \n2 \n5 \n10 \n50 \nfmi \n76.8 \u00b1 4.16 \n55.2 \u00b1 3.23 \n41.9 \u00b1 6.09 \n24.9 \u00b1 3.98 \nfmc \n78.2 \u00b1 5.47 \n56.4 \u00b1 3.32 \n42.2 \u00b1 4.33 \n25.6 \u00b1 4.05 \nfmi & fmc \n78.0 \u00b1 4.47 \n57.3 \u00b1 3.63 44.0 \u00b1 6.05 25.8 \u00b1 3.82 \n\n\n\n\nFigure 14: Generated images conditioned on fMRI-mapped CLIP image embedding h img , fMRI-mapped CLIP text embedding h txt , or both.Ground truth \n\nConditioned on h img \n\nConditioned on h txt \n\nConditioned on both \n\n\nCodes are adapted from https://github.com/NVlabs/stylegan2-ada-pytorch, https://github.com/drboog/Lafite\nPlease refer to https://github.com/nightrome/cocostuff/blob/master/labels.txt for the full category list.\nNoise ceiling values are calculated based on the method in the NSD data paper[3], utilizing SNR 5 An example CLIP-conditioned text decoder can be found here: https://github.com/fkodom/clip-text-decoder.\nAcknowledgments and Disclosure of FundingThis project was partially supported by funding from the National Science Foundation under grant IIS-1817046.\nPerceived image decoding from brain activity using shared information of multi-subject fmri data. Y Akamatsu, R Harakawa, T Ogawa, M Haseyama, IEEE Access. 9Y. Akamatsu, R. Harakawa, T. Ogawa, and M. Haseyama. Perceived image decoding from brain activity using shared information of multi-subject fmri data. IEEE Access, 9:26593-26606, 2021.\n\nSelf-supervised multimodal versatile networks. J.-B Alayrac, A Recasens, R Schneider, R Arandjelovi\u0107, J Ramapuram, J De Fauw, L Smaira, S Dieleman, A Zisserman, Advances in Neural Information Processing Systems. 33J.-B. Alayrac, A. Recasens, R. Schneider, R. Arandjelovi\u0107, J. Ramapuram, J. De Fauw, L. Smaira, S. Dieleman, and A. Zisserman. Self-supervised multimodal versatile networks. Advances in Neural Information Processing Systems, 33:25-37, 2020.\n\nA massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. E J Allen, G St-Yves, Y Wu, J L Breedlove, J S Prince, L T Dowdle, M Nau, B Caron, F Pestilli, I Charest, Nature neuroscience. 251E. J. Allen, G. St-Yves, Y. Wu, J. L. Breedlove, J. S. Prince, L. T. Dowdle, M. Nau, B. Caron, F. Pestilli, I. Charest, et al. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116-126, 2022.\n\nVicreg: Variance-invariance-covariance regularization for selfsupervised learning. A Bardes, J Ponce, Y Lecun, arXiv:2105.04906arXiv preprintA. Bardes, J. Ponce, and Y. LeCun. Vicreg: Variance-invariance-covariance regularization for self- supervised learning. arXiv preprint arXiv:2105.04906, 2021.\n\nNeural population control via deep image synthesis. P Bashivan, K Kar, J J Dicarlo, Science. 36464399436P. Bashivan, K. Kar, and J. J. DiCarlo. Neural population control via deep image synthesis. Science, 364(6439):eaav9436, 2019.\n\nFrom voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. R Beliy, G Gaziv, A Hoogi, F Strappini, T Golan, M Irani, Advances in Neural Information Processing Systems. 32R. Beliy, G. Gaziv, A. Hoogi, F. Strappini, T. Golan, and M. Irani. From voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. Advances in Neural Information Processing Systems, 32, 2019.\n\nCausal inference in the multisensory brain. Y Cao, C Summerfield, H Park, B L Giordano, C Kayser, Neuron. 1025Y. Cao, C. Summerfield, H. Park, B. L. Giordano, and C. Kayser. Causal inference in the multisensory brain. Neuron, 102(5):1076-1087, 2019.\n\nN Chang, J A Pyles, A Gupta, M J Tarr, E M Aminoff, arXiv:1809.01281BOLD5000: A public fMRI dataset of 5000 images. arXiv preprintN. Chang, J. A. Pyles, A. Gupta, M. J. Tarr, and E. M. Aminoff. BOLD5000: A public fMRI dataset of 5000 images. arXiv preprint arXiv:1809.01281, 2018.\n\nMultimodal neural networks better explain multivoxel patterns in the hippocampus. B Choksi, M Mozafari, R Vanrullen, L Reddy, Neural Information Processing Systems (NeurIPS) conference: 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence. 2021SVRHM 2021B. Choksi, M. Mozafari, R. Vanrullen, and L. Reddy. Multimodal neural networks better explain multivoxel patterns in the hippocampus. In Neural Information Processing Systems (NeurIPS) conference: 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021), 2021.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.\n\nReconstructing perceptive images from brain activity by shape-semantic gan. T Fang, Y Qi, G Pan, Advances in Neural Information Processing Systems. 33T. Fang, Y. Qi, and G. Pan. Reconstructing perceptive images from brain activity by shape-semantic gan. Advances in Neural Information Processing Systems, 33:13038-13048, 2020.\n\nSelf-supervised natural image reconstruction and rich semantic classification from brain activity. bioRxiv. G Gaziv, R Beliy, N Granot, A Hoogi, F Strappini, T Golan, M Irani, G. Gaziv, R. Beliy, N. Granot, A. Hoogi, F. Strappini, T. Golan, and M. Irani. Self-supervised natural image reconstruction and rich semantic classification from brain activity. bioRxiv, 2020.\n\nIs neocortex essentially multisensory?. A A Ghazanfar, C E Schroeder, Trends in Cognitive Sciences. 106A. A. Ghazanfar and C. E. Schroeder. Is neocortex essentially multisensory? Trends in Cognitive Sciences, 10(6):278-285, 2006.\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems. M Heusel, H Ramsauer, T Unterthiner, B Nessler, S Hochreiter, 30M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\nGeneric decoding of seen and imagined objects using hierarchical visual features. T Horikawa, Y Kamitani, Nature communications. 81T. Horikawa and Y. Kamitani. Generic decoding of seen and imagined objects using hierarchical visual features. Nature communications, 8(1):1-15, 2017.\n\nTraining gans with stronger augmentations via contrastive discriminator. J Jeong, J Shin, arXiv:2103.09742arXiv preprintJ. Jeong and J. Shin. Training gans with stronger augmentations via contrastive discriminator. arXiv preprint arXiv:2103.09742, 2021.\n\nScaling up visual and vision-language representation learning with noisy text supervision. C Jia, Y Yang, Y Xia, Y.-T Chen, Z Parekh, H Pham, Q Le, Y.-H Sung, Z Li, T Duerig, International Conference on Machine Learning. PMLRC. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR, 2021.\n\nContragan: Contrastive learning for conditional image generation. M Kang, J Park, Advances in Neural Information Processing Systems. 33M. Kang and J. Park. Contragan: Contrastive learning for conditional image generation. Advances in Neural Information Processing Systems, 33:21357-21369, 2020.\n\nTraining generative adversarial networks with limited data. T Karras, M Aittala, J Hellsten, S Laine, J Lehtinen, T Aila, Proc. NeurIPS. NeurIPST. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila. Training generative adversarial networks with limited data. In Proc. NeurIPS, 2020.\n\nAnalyzing and improving the image quality of StyleGAN. T Karras, S Laine, M Aittala, J Hellsten, J Lehtinen, T Aila, Proc. CVPR. CVPRT. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of StyleGAN. In Proc. CVPR, 2020.\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European conference on computer vision. SpringerT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.\n\nLarge-scale celebfaces attributes (celeba) dataset. Retrieved. Z Liu, P Luo, X Wang, X Tang, 1511Z. Liu, P. Luo, X. Wang, and X. Tang. Large-scale celebfaces attributes (celeba) dataset. Retrieved August, 15(2018):11, 2018.\n\nReconstructing natural scenes from fMRI patterns using bigbigan. M Mozafari, L Reddy, R Vanrullen, 2020 International joint conference on neural networks (IJCNN). IEEEM. Mozafari, L. Reddy, and R. VanRullen. Reconstructing natural scenes from fMRI patterns using bigbigan. In 2020 International joint conference on neural networks (IJCNN), pages 1-8. IEEE, 2020.\n\nMultisensory integration: Brain, body, and world. A Pasqualotto, M L Dumitru, A Myachykov, Frontiers in Psychology. 62046A. Pasqualotto, M. L. Dumitru, and A. Myachykov. Multisensory integration: Brain, body, and world. Frontiers in Psychology, 6:2046, 2016.\n\nBeyond sensory images: Object-based representation in the human ventral pathway. P Pietrini, M L Furey, E Ricciardi, M I Gobbini, W.-H C Wu, L Cohen, M Guazzelli, J V Haxby, Proceedings of the National Academy of Sciences. 10115P. Pietrini, M. L. Furey, E. Ricciardi, M. I. Gobbini, W.-H. C. Wu, L. Cohen, M. Guazzelli, and J. V. Haxby. Beyond sensory images: Object-based representation in the human ventral pathway. Proceedings of the National Academy of Sciences, 101(15):5658-5663, 2004.\n\nVisual and linguistic semantic representations are aligned at the border of human visual cortex. S F Popham, A G Huth, N Y Bilenko, F Deniz, J S Gao, A O Nunez-Elizalde, J L Gallant, Nature Neuroscience. 2411S. F. Popham, A. G. Huth, N. Y. Bilenko, F. Deniz, J. S. Gao, A. O. Nunez-Elizalde, and J. L. Gallant. Visual and linguistic semantic representations are aligned at the border of human visual cortex. Nature Neuroscience, 24(11):1628-1636, 2021.\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. PMLRA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.\n\nNatural image reconstruction from fMRI using deep learning: A survey. Z Rakhimberdina, Q Jodelet, X Liu, T Murata, Frontiers in neuroscience. 15795488Z. Rakhimberdina, Q. Jodelet, X. Liu, and T. Murata. Natural image reconstruction from fMRI using deep learning: A survey. Frontiers in neuroscience, 15:795488, 2021.\n\nA Ramesh, P Dhariwal, A Nichol, C Chu, M Chen, arXiv:2204.06125Hierarchical text-conditional image generation with clip latents. arXiv preprintA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nZero-shot text-to-image generation. A Ramesh, M Pavlov, G Goh, S Gray, C Voss, A Radford, M Chen, I Sutskever, International Conference on Machine Learning. PMLRA. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821-8831. PMLR, 2021.\n\nReconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning. Z Ren, J Li, X Xue, X Li, F Yang, Z Jiao, X Gao, NeuroImage. 228117602Z. Ren, J. Li, X. Xue, X. Li, F. Yang, Z. Jiao, and X. Gao. Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning. NeuroImage, 228:117602, 2021.\n\nConceptual captions: A cleaned, hypernymed, image alttext dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt- text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018.\n\nEnd-to-end deep image reconstruction from human brain activity. G Shen, K Dwivedi, K Majima, T Horikawa, Y Kamitani, Frontiers in Computational Neuroscience. 21G. Shen, K. Dwivedi, K. Majima, T. Horikawa, and Y. Kamitani. End-to-end deep image reconstruction from human brain activity. Frontiers in Computational Neuroscience, page 21, 2019.\n\nDeep image reconstruction from human brain activity. G Shen, T Horikawa, K Majima, Y Kamitani, PLoS computational biology. 1511006633G. Shen, T. Horikawa, K. Majima, and Y. Kamitani. Deep image reconstruction from human brain activity. PLoS computational biology, 15(1):e1006633, 2019.\n\nInterpreting the latent space of gans for semantic face editing. Y Shen, J Gu, X Tang, B Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionY. Shen, J. Gu, X. Tang, and B. Zhou. Interpreting the latent space of gans for semantic face editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9243-9252, 2020.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818-2826, 2016.\n\nQuestion answering for estimation of seen image contents from multi-subject fmri responses. S Takada, R Togo, T Ogawa, M Haseyama, 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE). IEEES. Takada, R. Togo, T. Ogawa, and M. Haseyama. Question answering for estimation of seen image contents from multi-subject fmri responses. In 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE), pages 712-713. IEEE, 2020.\n\nA Van Den Oord, Y Li, O Vinyals, Representation learning with contrastive predictive coding. arXiv e-prints. 1807A. Van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv e-prints, pages arXiv-1807, 2018.\n\nNeural discrete representation learning. Advances in neural information processing systems. A Van Den, O Oord, Vinyals, 30A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.\n\nReconstructing faces from fMRI patterns using deep generative neural networks. R Vanrullen, L Reddy, Communications biology. 21R. VanRullen and L. Reddy. Reconstructing faces from fMRI patterns using deep generative neural networks. Communications biology, 2(1):1-10, 2019.\n\nY Zhou, R Zhang, C Chen, C Li, C Tensmeyer, T Yu, J Gu, J Xu, T Sun, Lafite, arXiv:2111.13792Towards language-free training for text-to-image generation. arXiv preprintY. Zhou, R. Zhang, C. Chen, C. Li, C. Tensmeyer, T. Yu, J. Gu, J. Xu, and T. Sun. Lafite: Towards language-free training for text-to-image generation. arXiv preprint arXiv:2111.13792, 2021.\n", "annotations": {"author": "[{\"end\":78,\"start\":68},{\"end\":112,\"start\":79},{\"end\":127,\"start\":113},{\"end\":142,\"start\":128}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":74},{\"end\":93,\"start\":86},{\"end\":126,\"start\":121},{\"end\":141,\"start\":134}]", "author_first_name": "[{\"end\":73,\"start\":68},{\"end\":85,\"start\":79},{\"end\":118,\"start\":113},{\"end\":120,\"start\":119},{\"end\":133,\"start\":128}]", "author_affiliation": null, "title": "[{\"end\":65,\"start\":1},{\"end\":207,\"start\":143}]", "venue": null, "abstract": "[{\"end\":1729,\"start\":209}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1943,\"start\":1940},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1945,\"start\":1943},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1948,\"start\":1945},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1951,\"start\":1948},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2003,\"start\":2000},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2271,\"start\":2267},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2445,\"start\":2441},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2460,\"start\":2456},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2714,\"start\":2711},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2743,\"start\":2739},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2885,\"start\":2882},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2888,\"start\":2885},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2891,\"start\":2888},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2894,\"start\":2891},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2897,\"start\":2894},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2900,\"start\":2897},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2903,\"start\":2900},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3077,\"start\":3074},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3080,\"start\":3077},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3083,\"start\":3080},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3364,\"start\":3360},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3436,\"start\":3432},{\"end\":3639,\"start\":3635},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4306,\"start\":4303},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5242,\"start\":5239},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5733,\"start\":5730},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5994,\"start\":5991},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6443,\"start\":6439},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6458,\"start\":6454},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7388,\"start\":7385},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9584,\"start\":9580},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11403,\"start\":11399},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11477,\"start\":11473},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11480,\"start\":11477},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12094,\"start\":12090},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13089,\"start\":13085},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13092,\"start\":13089},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14681,\"start\":14677},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17058,\"start\":17057},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17401,\"start\":17397},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18877,\"start\":18874},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21538,\"start\":21537},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21921,\"start\":21917},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26662,\"start\":26658},{\"end\":27972,\"start\":27965},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31351,\"start\":31350},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32525,\"start\":32521},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32537,\"start\":32533},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32758,\"start\":32754},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32879,\"start\":32875},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35286,\"start\":35283},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35322,\"start\":35319},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35468,\"start\":35464},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":39566,\"start\":39563},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39647,\"start\":39643},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":43562,\"start\":43558},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":50666,\"start\":50665},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":52804,\"start\":52803},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":53932,\"start\":53929},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":54453,\"start\":54450},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":62562,\"start\":62559}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54287,\"start\":54278},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54482,\"start\":54288},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54648,\"start\":54483},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54720,\"start\":54649},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54892,\"start\":54721},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55142,\"start\":54893},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56024,\"start\":55143},{\"attributes\":{\"id\":\"fig_7\"},\"end\":56346,\"start\":56025},{\"attributes\":{\"id\":\"fig_8\"},\"end\":56583,\"start\":56347},{\"attributes\":{\"id\":\"fig_9\"},\"end\":57554,\"start\":56584},{\"attributes\":{\"id\":\"fig_10\"},\"end\":57748,\"start\":57555},{\"attributes\":{\"id\":\"fig_11\"},\"end\":57929,\"start\":57749},{\"attributes\":{\"id\":\"fig_12\"},\"end\":58589,\"start\":57930},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":58878,\"start\":58590},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":59716,\"start\":58879},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":61194,\"start\":59717},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61394,\"start\":61195},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":61788,\"start\":61395},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":62052,\"start\":61789},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":62270,\"start\":62053}]", "paragraph": "[{\"end\":2631,\"start\":1745},{\"end\":3707,\"start\":2633},{\"end\":4516,\"start\":3709},{\"end\":5203,\"start\":4518},{\"end\":6218,\"start\":5205},{\"end\":7093,\"start\":6229},{\"end\":8296,\"start\":7115},{\"end\":8973,\"start\":8335},{\"end\":9647,\"start\":8975},{\"end\":9821,\"start\":9746},{\"end\":9879,\"start\":9851},{\"end\":11009,\"start\":10000},{\"end\":11547,\"start\":11067},{\"end\":12201,\"start\":11549},{\"end\":12651,\"start\":12248},{\"end\":13392,\"start\":12653},{\"end\":13706,\"start\":13505},{\"end\":14075,\"start\":13927},{\"end\":15238,\"start\":14077},{\"end\":15925,\"start\":15280},{\"end\":17059,\"start\":15927},{\"end\":18156,\"start\":17107},{\"end\":18878,\"start\":18186},{\"end\":20053,\"start\":18880},{\"end\":21414,\"start\":20055},{\"end\":21644,\"start\":21447},{\"end\":22798,\"start\":21646},{\"end\":23651,\"start\":22842},{\"end\":23742,\"start\":23653},{\"end\":23907,\"start\":23744},{\"end\":24618,\"start\":23909},{\"end\":25177,\"start\":24620},{\"end\":26274,\"start\":25179},{\"end\":27431,\"start\":26276},{\"end\":29013,\"start\":27433},{\"end\":29618,\"start\":29015},{\"end\":30233,\"start\":29620},{\"end\":32647,\"start\":30268},{\"end\":33492,\"start\":32671},{\"end\":34715,\"start\":33494},{\"end\":36329,\"start\":34717},{\"end\":37759,\"start\":36331},{\"end\":38759,\"start\":37774},{\"end\":39780,\"start\":38774},{\"end\":40024,\"start\":39782},{\"end\":40234,\"start\":40026},{\"end\":40327,\"start\":40269},{\"end\":40381,\"start\":40329},{\"end\":41519,\"start\":40383},{\"end\":42246,\"start\":41521},{\"end\":42318,\"start\":42248},{\"end\":43998,\"start\":42370},{\"end\":44398,\"start\":44055},{\"end\":44897,\"start\":44400},{\"end\":46212,\"start\":44899},{\"end\":46915,\"start\":46214},{\"end\":47990,\"start\":46917},{\"end\":48459,\"start\":48012},{\"end\":49422,\"start\":48461},{\"end\":49696,\"start\":49467},{\"end\":51185,\"start\":49698},{\"end\":51942,\"start\":51187},{\"end\":51969,\"start\":51944},{\"end\":52494,\"start\":51971},{\"end\":53593,\"start\":52496},{\"end\":54277,\"start\":53595}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9745,\"start\":9648},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9850,\"start\":9822},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9999,\"start\":9880},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12247,\"start\":12202},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13504,\"start\":13393},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13926,\"start\":13707},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22841,\"start\":22799}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21209,\"start\":21200},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40023,\"start\":40015},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40976,\"start\":40969}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1743,\"start\":1731},{\"attributes\":{\"n\":\"2\"},\"end\":6227,\"start\":6221},{\"attributes\":{\"n\":\"2.1\"},\"end\":7113,\"start\":7096},{\"attributes\":{\"n\":\"2.2\"},\"end\":8333,\"start\":8299},{\"attributes\":{\"n\":\"2.3\"},\"end\":11065,\"start\":11012},{\"attributes\":{\"n\":\"3\"},\"end\":15248,\"start\":15241},{\"attributes\":{\"n\":\"3.1\"},\"end\":15278,\"start\":15251},{\"attributes\":{\"n\":\"3.2\"},\"end\":17105,\"start\":17062},{\"end\":18184,\"start\":18159},{\"attributes\":{\"n\":\"3.3\"},\"end\":21445,\"start\":21417},{\"attributes\":{\"n\":\"3.4\"},\"end\":30266,\"start\":30236},{\"attributes\":{\"n\":\"4\"},\"end\":32669,\"start\":32650},{\"attributes\":{\"n\":\"5\"},\"end\":37772,\"start\":37762},{\"end\":38772,\"start\":38762},{\"end\":40267,\"start\":40237},{\"end\":42368,\"start\":42321},{\"end\":44053,\"start\":44001},{\"end\":48010,\"start\":47993},{\"end\":49465,\"start\":49425},{\"end\":54285,\"start\":54279},{\"end\":54299,\"start\":54289},{\"end\":54494,\"start\":54484},{\"end\":54660,\"start\":54650},{\"end\":54732,\"start\":54722},{\"end\":55154,\"start\":55144},{\"end\":56036,\"start\":56026},{\"end\":56359,\"start\":56348},{\"end\":56596,\"start\":56585},{\"end\":57567,\"start\":57556},{\"end\":57761,\"start\":57750},{\"end\":57953,\"start\":57931},{\"end\":58600,\"start\":58591},{\"end\":58889,\"start\":58880},{\"end\":59727,\"start\":59718},{\"end\":61205,\"start\":61196},{\"end\":61403,\"start\":61396},{\"end\":61799,\"start\":61790}]", "table": "[{\"end\":58878,\"start\":58647},{\"end\":59716,\"start\":59276},{\"end\":61194,\"start\":59729},{\"end\":61394,\"start\":61287},{\"end\":61788,\"start\":61405},{\"end\":62052,\"start\":61801},{\"end\":62270,\"start\":62187}]", "figure_caption": "[{\"end\":54482,\"start\":54301},{\"end\":54648,\"start\":54496},{\"end\":54720,\"start\":54662},{\"end\":54892,\"start\":54734},{\"end\":55142,\"start\":54895},{\"end\":56024,\"start\":55156},{\"end\":56346,\"start\":56038},{\"end\":56583,\"start\":56362},{\"end\":57554,\"start\":56599},{\"end\":57748,\"start\":57570},{\"end\":57929,\"start\":57764},{\"end\":58589,\"start\":57958},{\"end\":58647,\"start\":58602},{\"end\":59276,\"start\":58891},{\"end\":61287,\"start\":61207},{\"end\":62187,\"start\":62055}]", "figure_ref": "[{\"end\":3725,\"start\":3717},{\"end\":6920,\"start\":6914},{\"end\":9039,\"start\":9033},{\"end\":11942,\"start\":11936},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16206,\"start\":16199},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20247,\"start\":20241},{\"end\":24020,\"start\":24012},{\"end\":24318,\"start\":24311},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25241,\"start\":25235},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25960,\"start\":25953},{\"end\":26626,\"start\":26619},{\"end\":26842,\"start\":26835},{\"end\":31454,\"start\":31446},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":38889,\"start\":38882},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40069,\"start\":40063},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":41306,\"start\":41300},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":45467,\"start\":45460},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47289,\"start\":47282},{\"end\":48455,\"start\":48451},{\"end\":49150,\"start\":49143},{\"end\":50423,\"start\":50415},{\"end\":50662,\"start\":50654},{\"end\":50730,\"start\":50722},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":51495,\"start\":51488},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":51585,\"start\":51576},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":51886,\"start\":51877},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":52258,\"start\":52251}]", "bib_author_first_name": "[{\"end\":62935,\"start\":62934},{\"end\":62947,\"start\":62946},{\"end\":62959,\"start\":62958},{\"end\":62968,\"start\":62967},{\"end\":63230,\"start\":63226},{\"end\":63241,\"start\":63240},{\"end\":63253,\"start\":63252},{\"end\":63266,\"start\":63265},{\"end\":63282,\"start\":63281},{\"end\":63295,\"start\":63294},{\"end\":63306,\"start\":63305},{\"end\":63316,\"start\":63315},{\"end\":63328,\"start\":63327},{\"end\":63724,\"start\":63723},{\"end\":63726,\"start\":63725},{\"end\":63735,\"start\":63734},{\"end\":63746,\"start\":63745},{\"end\":63752,\"start\":63751},{\"end\":63754,\"start\":63753},{\"end\":63767,\"start\":63766},{\"end\":63769,\"start\":63768},{\"end\":63779,\"start\":63778},{\"end\":63781,\"start\":63780},{\"end\":63791,\"start\":63790},{\"end\":63798,\"start\":63797},{\"end\":63807,\"start\":63806},{\"end\":63819,\"start\":63818},{\"end\":64195,\"start\":64194},{\"end\":64205,\"start\":64204},{\"end\":64214,\"start\":64213},{\"end\":64465,\"start\":64464},{\"end\":64477,\"start\":64476},{\"end\":64484,\"start\":64483},{\"end\":64486,\"start\":64485},{\"end\":64737,\"start\":64736},{\"end\":64746,\"start\":64745},{\"end\":64755,\"start\":64754},{\"end\":64764,\"start\":64763},{\"end\":64777,\"start\":64776},{\"end\":64786,\"start\":64785},{\"end\":65114,\"start\":65113},{\"end\":65121,\"start\":65120},{\"end\":65136,\"start\":65135},{\"end\":65144,\"start\":65143},{\"end\":65146,\"start\":65145},{\"end\":65158,\"start\":65157},{\"end\":65321,\"start\":65320},{\"end\":65330,\"start\":65329},{\"end\":65332,\"start\":65331},{\"end\":65341,\"start\":65340},{\"end\":65350,\"start\":65349},{\"end\":65352,\"start\":65351},{\"end\":65360,\"start\":65359},{\"end\":65362,\"start\":65361},{\"end\":65685,\"start\":65684},{\"end\":65695,\"start\":65694},{\"end\":65707,\"start\":65706},{\"end\":65720,\"start\":65719},{\"end\":66235,\"start\":66234},{\"end\":66243,\"start\":66242},{\"end\":66251,\"start\":66250},{\"end\":66264,\"start\":66260},{\"end\":66270,\"start\":66269},{\"end\":66276,\"start\":66275},{\"end\":66643,\"start\":66642},{\"end\":66651,\"start\":66650},{\"end\":66657,\"start\":66656},{\"end\":67003,\"start\":67002},{\"end\":67012,\"start\":67011},{\"end\":67021,\"start\":67020},{\"end\":67031,\"start\":67030},{\"end\":67040,\"start\":67039},{\"end\":67053,\"start\":67052},{\"end\":67062,\"start\":67061},{\"end\":67305,\"start\":67304},{\"end\":67307,\"start\":67306},{\"end\":67320,\"start\":67319},{\"end\":67322,\"start\":67321},{\"end\":67630,\"start\":67629},{\"end\":67640,\"start\":67639},{\"end\":67652,\"start\":67651},{\"end\":67667,\"start\":67666},{\"end\":67678,\"start\":67677},{\"end\":67992,\"start\":67991},{\"end\":68004,\"start\":68003},{\"end\":68266,\"start\":68265},{\"end\":68275,\"start\":68274},{\"end\":68539,\"start\":68538},{\"end\":68546,\"start\":68545},{\"end\":68554,\"start\":68553},{\"end\":68564,\"start\":68560},{\"end\":68572,\"start\":68571},{\"end\":68582,\"start\":68581},{\"end\":68590,\"start\":68589},{\"end\":68599,\"start\":68595},{\"end\":68607,\"start\":68606},{\"end\":68613,\"start\":68612},{\"end\":69007,\"start\":69006},{\"end\":69015,\"start\":69014},{\"end\":69297,\"start\":69296},{\"end\":69307,\"start\":69306},{\"end\":69318,\"start\":69317},{\"end\":69330,\"start\":69329},{\"end\":69339,\"start\":69338},{\"end\":69351,\"start\":69350},{\"end\":69593,\"start\":69592},{\"end\":69603,\"start\":69602},{\"end\":69612,\"start\":69611},{\"end\":69623,\"start\":69622},{\"end\":69635,\"start\":69634},{\"end\":69647,\"start\":69646},{\"end\":69866,\"start\":69862},{\"end\":69873,\"start\":69872},{\"end\":69882,\"start\":69881},{\"end\":69894,\"start\":69893},{\"end\":69902,\"start\":69901},{\"end\":69912,\"start\":69911},{\"end\":69923,\"start\":69922},{\"end\":69933,\"start\":69932},{\"end\":69935,\"start\":69934},{\"end\":70271,\"start\":70270},{\"end\":70278,\"start\":70277},{\"end\":70285,\"start\":70284},{\"end\":70293,\"start\":70292},{\"end\":70498,\"start\":70497},{\"end\":70510,\"start\":70509},{\"end\":70519,\"start\":70518},{\"end\":70847,\"start\":70846},{\"end\":70862,\"start\":70861},{\"end\":70864,\"start\":70863},{\"end\":70875,\"start\":70874},{\"end\":71138,\"start\":71137},{\"end\":71150,\"start\":71149},{\"end\":71152,\"start\":71151},{\"end\":71161,\"start\":71160},{\"end\":71174,\"start\":71173},{\"end\":71176,\"start\":71175},{\"end\":71190,\"start\":71186},{\"end\":71192,\"start\":71191},{\"end\":71198,\"start\":71197},{\"end\":71207,\"start\":71206},{\"end\":71220,\"start\":71219},{\"end\":71222,\"start\":71221},{\"end\":71647,\"start\":71646},{\"end\":71649,\"start\":71648},{\"end\":71659,\"start\":71658},{\"end\":71661,\"start\":71660},{\"end\":71669,\"start\":71668},{\"end\":71671,\"start\":71670},{\"end\":71682,\"start\":71681},{\"end\":71691,\"start\":71690},{\"end\":71693,\"start\":71692},{\"end\":71700,\"start\":71699},{\"end\":71702,\"start\":71701},{\"end\":71720,\"start\":71719},{\"end\":71722,\"start\":71721},{\"end\":72075,\"start\":72074},{\"end\":72086,\"start\":72085},{\"end\":72088,\"start\":72087},{\"end\":72095,\"start\":72094},{\"end\":72106,\"start\":72105},{\"end\":72116,\"start\":72115},{\"end\":72123,\"start\":72122},{\"end\":72134,\"start\":72133},{\"end\":72144,\"start\":72143},{\"end\":72154,\"start\":72153},{\"end\":72165,\"start\":72164},{\"end\":72561,\"start\":72560},{\"end\":72578,\"start\":72577},{\"end\":72589,\"start\":72588},{\"end\":72596,\"start\":72595},{\"end\":72809,\"start\":72808},{\"end\":72819,\"start\":72818},{\"end\":72831,\"start\":72830},{\"end\":72841,\"start\":72840},{\"end\":72848,\"start\":72847},{\"end\":73150,\"start\":73149},{\"end\":73160,\"start\":73159},{\"end\":73170,\"start\":73169},{\"end\":73177,\"start\":73176},{\"end\":73185,\"start\":73184},{\"end\":73193,\"start\":73192},{\"end\":73204,\"start\":73203},{\"end\":73212,\"start\":73211},{\"end\":73593,\"start\":73592},{\"end\":73600,\"start\":73599},{\"end\":73606,\"start\":73605},{\"end\":73613,\"start\":73612},{\"end\":73619,\"start\":73618},{\"end\":73627,\"start\":73626},{\"end\":73635,\"start\":73634},{\"end\":73968,\"start\":73967},{\"end\":73978,\"start\":73977},{\"end\":73986,\"start\":73985},{\"end\":73997,\"start\":73996},{\"end\":74533,\"start\":74532},{\"end\":74541,\"start\":74540},{\"end\":74552,\"start\":74551},{\"end\":74562,\"start\":74561},{\"end\":74574,\"start\":74573},{\"end\":74865,\"start\":74864},{\"end\":74873,\"start\":74872},{\"end\":74885,\"start\":74884},{\"end\":74895,\"start\":74894},{\"end\":75164,\"start\":75163},{\"end\":75172,\"start\":75171},{\"end\":75178,\"start\":75177},{\"end\":75186,\"start\":75185},{\"end\":75615,\"start\":75614},{\"end\":75626,\"start\":75625},{\"end\":75639,\"start\":75638},{\"end\":75648,\"start\":75647},{\"end\":75658,\"start\":75657},{\"end\":76126,\"start\":76125},{\"end\":76136,\"start\":76135},{\"end\":76144,\"start\":76143},{\"end\":76153,\"start\":76152},{\"end\":76467,\"start\":76466},{\"end\":76483,\"start\":76482},{\"end\":76489,\"start\":76488},{\"end\":76813,\"start\":76812},{\"end\":76824,\"start\":76823},{\"end\":77061,\"start\":77060},{\"end\":77074,\"start\":77073},{\"end\":77257,\"start\":77256},{\"end\":77265,\"start\":77264},{\"end\":77274,\"start\":77273},{\"end\":77282,\"start\":77281},{\"end\":77288,\"start\":77287},{\"end\":77301,\"start\":77300},{\"end\":77307,\"start\":77306},{\"end\":77313,\"start\":77312},{\"end\":77319,\"start\":77318}]", "bib_author_last_name": "[{\"end\":62944,\"start\":62936},{\"end\":62956,\"start\":62948},{\"end\":62965,\"start\":62960},{\"end\":62977,\"start\":62969},{\"end\":63238,\"start\":63231},{\"end\":63250,\"start\":63242},{\"end\":63263,\"start\":63254},{\"end\":63279,\"start\":63267},{\"end\":63292,\"start\":63283},{\"end\":63303,\"start\":63296},{\"end\":63313,\"start\":63307},{\"end\":63325,\"start\":63317},{\"end\":63338,\"start\":63329},{\"end\":63732,\"start\":63727},{\"end\":63743,\"start\":63736},{\"end\":63749,\"start\":63747},{\"end\":63764,\"start\":63755},{\"end\":63776,\"start\":63770},{\"end\":63788,\"start\":63782},{\"end\":63795,\"start\":63792},{\"end\":63804,\"start\":63799},{\"end\":63816,\"start\":63808},{\"end\":63827,\"start\":63820},{\"end\":64202,\"start\":64196},{\"end\":64211,\"start\":64206},{\"end\":64220,\"start\":64215},{\"end\":64474,\"start\":64466},{\"end\":64481,\"start\":64478},{\"end\":64494,\"start\":64487},{\"end\":64743,\"start\":64738},{\"end\":64752,\"start\":64747},{\"end\":64761,\"start\":64756},{\"end\":64774,\"start\":64765},{\"end\":64783,\"start\":64778},{\"end\":64792,\"start\":64787},{\"end\":65118,\"start\":65115},{\"end\":65133,\"start\":65122},{\"end\":65141,\"start\":65137},{\"end\":65155,\"start\":65147},{\"end\":65165,\"start\":65159},{\"end\":65327,\"start\":65322},{\"end\":65338,\"start\":65333},{\"end\":65347,\"start\":65342},{\"end\":65357,\"start\":65353},{\"end\":65370,\"start\":65363},{\"end\":65692,\"start\":65686},{\"end\":65704,\"start\":65696},{\"end\":65717,\"start\":65708},{\"end\":65726,\"start\":65721},{\"end\":66240,\"start\":66236},{\"end\":66248,\"start\":66244},{\"end\":66258,\"start\":66252},{\"end\":66267,\"start\":66265},{\"end\":66273,\"start\":66271},{\"end\":66284,\"start\":66277},{\"end\":66648,\"start\":66644},{\"end\":66654,\"start\":66652},{\"end\":66661,\"start\":66658},{\"end\":67009,\"start\":67004},{\"end\":67018,\"start\":67013},{\"end\":67028,\"start\":67022},{\"end\":67037,\"start\":67032},{\"end\":67050,\"start\":67041},{\"end\":67059,\"start\":67054},{\"end\":67068,\"start\":67063},{\"end\":67317,\"start\":67308},{\"end\":67332,\"start\":67323},{\"end\":67637,\"start\":67631},{\"end\":67649,\"start\":67641},{\"end\":67664,\"start\":67653},{\"end\":67675,\"start\":67668},{\"end\":67689,\"start\":67679},{\"end\":68001,\"start\":67993},{\"end\":68013,\"start\":68005},{\"end\":68272,\"start\":68267},{\"end\":68280,\"start\":68276},{\"end\":68543,\"start\":68540},{\"end\":68551,\"start\":68547},{\"end\":68558,\"start\":68555},{\"end\":68569,\"start\":68565},{\"end\":68579,\"start\":68573},{\"end\":68587,\"start\":68583},{\"end\":68593,\"start\":68591},{\"end\":68604,\"start\":68600},{\"end\":68610,\"start\":68608},{\"end\":68620,\"start\":68614},{\"end\":69012,\"start\":69008},{\"end\":69020,\"start\":69016},{\"end\":69304,\"start\":69298},{\"end\":69315,\"start\":69308},{\"end\":69327,\"start\":69319},{\"end\":69336,\"start\":69331},{\"end\":69348,\"start\":69340},{\"end\":69356,\"start\":69352},{\"end\":69600,\"start\":69594},{\"end\":69609,\"start\":69604},{\"end\":69620,\"start\":69613},{\"end\":69632,\"start\":69624},{\"end\":69644,\"start\":69636},{\"end\":69652,\"start\":69648},{\"end\":69870,\"start\":69867},{\"end\":69879,\"start\":69874},{\"end\":69891,\"start\":69883},{\"end\":69899,\"start\":69895},{\"end\":69909,\"start\":69903},{\"end\":69920,\"start\":69913},{\"end\":69930,\"start\":69924},{\"end\":69943,\"start\":69936},{\"end\":70275,\"start\":70272},{\"end\":70282,\"start\":70279},{\"end\":70290,\"start\":70286},{\"end\":70298,\"start\":70294},{\"end\":70507,\"start\":70499},{\"end\":70516,\"start\":70511},{\"end\":70529,\"start\":70520},{\"end\":70859,\"start\":70848},{\"end\":70872,\"start\":70865},{\"end\":70885,\"start\":70876},{\"end\":71147,\"start\":71139},{\"end\":71158,\"start\":71153},{\"end\":71171,\"start\":71162},{\"end\":71184,\"start\":71177},{\"end\":71195,\"start\":71193},{\"end\":71204,\"start\":71199},{\"end\":71217,\"start\":71208},{\"end\":71228,\"start\":71223},{\"end\":71656,\"start\":71650},{\"end\":71666,\"start\":71662},{\"end\":71679,\"start\":71672},{\"end\":71688,\"start\":71683},{\"end\":71697,\"start\":71694},{\"end\":71717,\"start\":71703},{\"end\":71730,\"start\":71723},{\"end\":72083,\"start\":72076},{\"end\":72092,\"start\":72089},{\"end\":72103,\"start\":72096},{\"end\":72113,\"start\":72107},{\"end\":72120,\"start\":72117},{\"end\":72131,\"start\":72124},{\"end\":72141,\"start\":72135},{\"end\":72151,\"start\":72145},{\"end\":72162,\"start\":72155},{\"end\":72171,\"start\":72166},{\"end\":72575,\"start\":72562},{\"end\":72586,\"start\":72579},{\"end\":72593,\"start\":72590},{\"end\":72603,\"start\":72597},{\"end\":72816,\"start\":72810},{\"end\":72828,\"start\":72820},{\"end\":72838,\"start\":72832},{\"end\":72845,\"start\":72842},{\"end\":72853,\"start\":72849},{\"end\":73157,\"start\":73151},{\"end\":73167,\"start\":73161},{\"end\":73174,\"start\":73171},{\"end\":73182,\"start\":73178},{\"end\":73190,\"start\":73186},{\"end\":73201,\"start\":73194},{\"end\":73209,\"start\":73205},{\"end\":73222,\"start\":73213},{\"end\":73597,\"start\":73594},{\"end\":73603,\"start\":73601},{\"end\":73610,\"start\":73607},{\"end\":73616,\"start\":73614},{\"end\":73624,\"start\":73620},{\"end\":73632,\"start\":73628},{\"end\":73639,\"start\":73636},{\"end\":73975,\"start\":73969},{\"end\":73983,\"start\":73979},{\"end\":73994,\"start\":73987},{\"end\":74005,\"start\":73998},{\"end\":74538,\"start\":74534},{\"end\":74549,\"start\":74542},{\"end\":74559,\"start\":74553},{\"end\":74571,\"start\":74563},{\"end\":74583,\"start\":74575},{\"end\":74870,\"start\":74866},{\"end\":74882,\"start\":74874},{\"end\":74892,\"start\":74886},{\"end\":74904,\"start\":74896},{\"end\":75169,\"start\":75165},{\"end\":75175,\"start\":75173},{\"end\":75183,\"start\":75179},{\"end\":75191,\"start\":75187},{\"end\":75623,\"start\":75616},{\"end\":75636,\"start\":75627},{\"end\":75645,\"start\":75640},{\"end\":75655,\"start\":75649},{\"end\":75664,\"start\":75659},{\"end\":76133,\"start\":76127},{\"end\":76141,\"start\":76137},{\"end\":76150,\"start\":76145},{\"end\":76162,\"start\":76154},{\"end\":76480,\"start\":76468},{\"end\":76486,\"start\":76484},{\"end\":76497,\"start\":76490},{\"end\":76821,\"start\":76814},{\"end\":76829,\"start\":76825},{\"end\":76838,\"start\":76831},{\"end\":77071,\"start\":77062},{\"end\":77080,\"start\":77075},{\"end\":77262,\"start\":77258},{\"end\":77271,\"start\":77266},{\"end\":77279,\"start\":77275},{\"end\":77285,\"start\":77283},{\"end\":77298,\"start\":77289},{\"end\":77304,\"start\":77302},{\"end\":77310,\"start\":77308},{\"end\":77316,\"start\":77314},{\"end\":77323,\"start\":77320},{\"end\":77331,\"start\":77325}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":231973636},\"end\":63177,\"start\":62836},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":220249786},\"end\":63633,\"start\":63179},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":245262002},\"end\":64109,\"start\":63635},{\"attributes\":{\"doi\":\"arXiv:2105.04906\",\"id\":\"b3\"},\"end\":64410,\"start\":64111},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":143433639},\"end\":64642,\"start\":64412},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":195798779},\"end\":65067,\"start\":64644},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":91662250},\"end\":65318,\"start\":65069},{\"attributes\":{\"doi\":\"arXiv:1809.01281\",\"id\":\"b7\"},\"end\":65600,\"start\":65320},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":246294798},\"end\":66179,\"start\":65602},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":66564,\"start\":66181},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":227275511},\"end\":66892,\"start\":66566},{\"attributes\":{\"id\":\"b11\"},\"end\":67262,\"start\":66894},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2218969},\"end\":67493,\"start\":67264},{\"attributes\":{\"id\":\"b13\"},\"end\":67907,\"start\":67495},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2354567},\"end\":68190,\"start\":67909},{\"attributes\":{\"doi\":\"arXiv:2103.09742\",\"id\":\"b15\"},\"end\":68445,\"start\":68192},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":231879586},\"end\":68938,\"start\":68447},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":226191791},\"end\":69234,\"start\":68940},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":219636053},\"end\":69535,\"start\":69236},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":209202273},\"end\":69817,\"start\":69537},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14113767},\"end\":70205,\"start\":69819},{\"attributes\":{\"id\":\"b21\"},\"end\":70430,\"start\":70207},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":211003765},\"end\":70794,\"start\":70432},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5491777},\"end\":71054,\"start\":70796},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15957646},\"end\":71547,\"start\":71056},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":240152854},\"end\":72001,\"start\":71549},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":231591445},\"end\":72488,\"start\":72003},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":239015911},\"end\":72806,\"start\":72490},{\"attributes\":{\"doi\":\"arXiv:2204.06125\",\"id\":\"b28\"},\"end\":73111,\"start\":72808},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":232035663},\"end\":73474,\"start\":73113},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":229935148},\"end\":73867,\"start\":73476},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":51876975},\"end\":74466,\"start\":73869},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":91110332},\"end\":74809,\"start\":74468},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":58667578},\"end\":75096,\"start\":74811},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":198897678},\"end\":75553,\"start\":75098},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206593880},\"end\":76031,\"start\":75555},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":229373781},\"end\":76464,\"start\":76033},{\"attributes\":{\"id\":\"b37\"},\"end\":76718,\"start\":76466},{\"attributes\":{\"id\":\"b38\"},\"end\":76979,\"start\":76720},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":52945847},\"end\":77254,\"start\":76981},{\"attributes\":{\"doi\":\"arXiv:2111.13792\",\"id\":\"b40\"},\"end\":77613,\"start\":77256}]", "bib_title": "[{\"end\":62932,\"start\":62836},{\"end\":63224,\"start\":63179},{\"end\":63721,\"start\":63635},{\"end\":64462,\"start\":64412},{\"end\":64734,\"start\":64644},{\"end\":65111,\"start\":65069},{\"end\":65682,\"start\":65602},{\"end\":66232,\"start\":66181},{\"end\":66640,\"start\":66566},{\"end\":67302,\"start\":67264},{\"end\":67989,\"start\":67909},{\"end\":68536,\"start\":68447},{\"end\":69004,\"start\":68940},{\"end\":69294,\"start\":69236},{\"end\":69590,\"start\":69537},{\"end\":69860,\"start\":69819},{\"end\":70495,\"start\":70432},{\"end\":70844,\"start\":70796},{\"end\":71135,\"start\":71056},{\"end\":71644,\"start\":71549},{\"end\":72072,\"start\":72003},{\"end\":72558,\"start\":72490},{\"end\":73147,\"start\":73113},{\"end\":73590,\"start\":73476},{\"end\":73965,\"start\":73869},{\"end\":74530,\"start\":74468},{\"end\":74862,\"start\":74811},{\"end\":75161,\"start\":75098},{\"end\":75612,\"start\":75555},{\"end\":76123,\"start\":76033},{\"end\":77058,\"start\":76981}]", "bib_author": "[{\"end\":62946,\"start\":62934},{\"end\":62958,\"start\":62946},{\"end\":62967,\"start\":62958},{\"end\":62979,\"start\":62967},{\"end\":63240,\"start\":63226},{\"end\":63252,\"start\":63240},{\"end\":63265,\"start\":63252},{\"end\":63281,\"start\":63265},{\"end\":63294,\"start\":63281},{\"end\":63305,\"start\":63294},{\"end\":63315,\"start\":63305},{\"end\":63327,\"start\":63315},{\"end\":63340,\"start\":63327},{\"end\":63734,\"start\":63723},{\"end\":63745,\"start\":63734},{\"end\":63751,\"start\":63745},{\"end\":63766,\"start\":63751},{\"end\":63778,\"start\":63766},{\"end\":63790,\"start\":63778},{\"end\":63797,\"start\":63790},{\"end\":63806,\"start\":63797},{\"end\":63818,\"start\":63806},{\"end\":63829,\"start\":63818},{\"end\":64204,\"start\":64194},{\"end\":64213,\"start\":64204},{\"end\":64222,\"start\":64213},{\"end\":64476,\"start\":64464},{\"end\":64483,\"start\":64476},{\"end\":64496,\"start\":64483},{\"end\":64745,\"start\":64736},{\"end\":64754,\"start\":64745},{\"end\":64763,\"start\":64754},{\"end\":64776,\"start\":64763},{\"end\":64785,\"start\":64776},{\"end\":64794,\"start\":64785},{\"end\":65120,\"start\":65113},{\"end\":65135,\"start\":65120},{\"end\":65143,\"start\":65135},{\"end\":65157,\"start\":65143},{\"end\":65167,\"start\":65157},{\"end\":65329,\"start\":65320},{\"end\":65340,\"start\":65329},{\"end\":65349,\"start\":65340},{\"end\":65359,\"start\":65349},{\"end\":65372,\"start\":65359},{\"end\":65694,\"start\":65684},{\"end\":65706,\"start\":65694},{\"end\":65719,\"start\":65706},{\"end\":65728,\"start\":65719},{\"end\":66242,\"start\":66234},{\"end\":66250,\"start\":66242},{\"end\":66260,\"start\":66250},{\"end\":66269,\"start\":66260},{\"end\":66275,\"start\":66269},{\"end\":66286,\"start\":66275},{\"end\":66650,\"start\":66642},{\"end\":66656,\"start\":66650},{\"end\":66663,\"start\":66656},{\"end\":67011,\"start\":67002},{\"end\":67020,\"start\":67011},{\"end\":67030,\"start\":67020},{\"end\":67039,\"start\":67030},{\"end\":67052,\"start\":67039},{\"end\":67061,\"start\":67052},{\"end\":67070,\"start\":67061},{\"end\":67319,\"start\":67304},{\"end\":67334,\"start\":67319},{\"end\":67639,\"start\":67629},{\"end\":67651,\"start\":67639},{\"end\":67666,\"start\":67651},{\"end\":67677,\"start\":67666},{\"end\":67691,\"start\":67677},{\"end\":68003,\"start\":67991},{\"end\":68015,\"start\":68003},{\"end\":68274,\"start\":68265},{\"end\":68282,\"start\":68274},{\"end\":68545,\"start\":68538},{\"end\":68553,\"start\":68545},{\"end\":68560,\"start\":68553},{\"end\":68571,\"start\":68560},{\"end\":68581,\"start\":68571},{\"end\":68589,\"start\":68581},{\"end\":68595,\"start\":68589},{\"end\":68606,\"start\":68595},{\"end\":68612,\"start\":68606},{\"end\":68622,\"start\":68612},{\"end\":69014,\"start\":69006},{\"end\":69022,\"start\":69014},{\"end\":69306,\"start\":69296},{\"end\":69317,\"start\":69306},{\"end\":69329,\"start\":69317},{\"end\":69338,\"start\":69329},{\"end\":69350,\"start\":69338},{\"end\":69358,\"start\":69350},{\"end\":69602,\"start\":69592},{\"end\":69611,\"start\":69602},{\"end\":69622,\"start\":69611},{\"end\":69634,\"start\":69622},{\"end\":69646,\"start\":69634},{\"end\":69654,\"start\":69646},{\"end\":69872,\"start\":69862},{\"end\":69881,\"start\":69872},{\"end\":69893,\"start\":69881},{\"end\":69901,\"start\":69893},{\"end\":69911,\"start\":69901},{\"end\":69922,\"start\":69911},{\"end\":69932,\"start\":69922},{\"end\":69945,\"start\":69932},{\"end\":70277,\"start\":70270},{\"end\":70284,\"start\":70277},{\"end\":70292,\"start\":70284},{\"end\":70300,\"start\":70292},{\"end\":70509,\"start\":70497},{\"end\":70518,\"start\":70509},{\"end\":70531,\"start\":70518},{\"end\":70861,\"start\":70846},{\"end\":70874,\"start\":70861},{\"end\":70887,\"start\":70874},{\"end\":71149,\"start\":71137},{\"end\":71160,\"start\":71149},{\"end\":71173,\"start\":71160},{\"end\":71186,\"start\":71173},{\"end\":71197,\"start\":71186},{\"end\":71206,\"start\":71197},{\"end\":71219,\"start\":71206},{\"end\":71230,\"start\":71219},{\"end\":71658,\"start\":71646},{\"end\":71668,\"start\":71658},{\"end\":71681,\"start\":71668},{\"end\":71690,\"start\":71681},{\"end\":71699,\"start\":71690},{\"end\":71719,\"start\":71699},{\"end\":71732,\"start\":71719},{\"end\":72085,\"start\":72074},{\"end\":72094,\"start\":72085},{\"end\":72105,\"start\":72094},{\"end\":72115,\"start\":72105},{\"end\":72122,\"start\":72115},{\"end\":72133,\"start\":72122},{\"end\":72143,\"start\":72133},{\"end\":72153,\"start\":72143},{\"end\":72164,\"start\":72153},{\"end\":72173,\"start\":72164},{\"end\":72577,\"start\":72560},{\"end\":72588,\"start\":72577},{\"end\":72595,\"start\":72588},{\"end\":72605,\"start\":72595},{\"end\":72818,\"start\":72808},{\"end\":72830,\"start\":72818},{\"end\":72840,\"start\":72830},{\"end\":72847,\"start\":72840},{\"end\":72855,\"start\":72847},{\"end\":73159,\"start\":73149},{\"end\":73169,\"start\":73159},{\"end\":73176,\"start\":73169},{\"end\":73184,\"start\":73176},{\"end\":73192,\"start\":73184},{\"end\":73203,\"start\":73192},{\"end\":73211,\"start\":73203},{\"end\":73224,\"start\":73211},{\"end\":73599,\"start\":73592},{\"end\":73605,\"start\":73599},{\"end\":73612,\"start\":73605},{\"end\":73618,\"start\":73612},{\"end\":73626,\"start\":73618},{\"end\":73634,\"start\":73626},{\"end\":73641,\"start\":73634},{\"end\":73977,\"start\":73967},{\"end\":73985,\"start\":73977},{\"end\":73996,\"start\":73985},{\"end\":74007,\"start\":73996},{\"end\":74540,\"start\":74532},{\"end\":74551,\"start\":74540},{\"end\":74561,\"start\":74551},{\"end\":74573,\"start\":74561},{\"end\":74585,\"start\":74573},{\"end\":74872,\"start\":74864},{\"end\":74884,\"start\":74872},{\"end\":74894,\"start\":74884},{\"end\":74906,\"start\":74894},{\"end\":75171,\"start\":75163},{\"end\":75177,\"start\":75171},{\"end\":75185,\"start\":75177},{\"end\":75193,\"start\":75185},{\"end\":75625,\"start\":75614},{\"end\":75638,\"start\":75625},{\"end\":75647,\"start\":75638},{\"end\":75657,\"start\":75647},{\"end\":75666,\"start\":75657},{\"end\":76135,\"start\":76125},{\"end\":76143,\"start\":76135},{\"end\":76152,\"start\":76143},{\"end\":76164,\"start\":76152},{\"end\":76482,\"start\":76466},{\"end\":76488,\"start\":76482},{\"end\":76499,\"start\":76488},{\"end\":76823,\"start\":76812},{\"end\":76831,\"start\":76823},{\"end\":76840,\"start\":76831},{\"end\":77073,\"start\":77060},{\"end\":77082,\"start\":77073},{\"end\":77264,\"start\":77256},{\"end\":77273,\"start\":77264},{\"end\":77281,\"start\":77273},{\"end\":77287,\"start\":77281},{\"end\":77300,\"start\":77287},{\"end\":77306,\"start\":77300},{\"end\":77312,\"start\":77306},{\"end\":77318,\"start\":77312},{\"end\":77325,\"start\":77318},{\"end\":77333,\"start\":77325}]", "bib_venue": "[{\"end\":62990,\"start\":62979},{\"end\":63389,\"start\":63340},{\"end\":63848,\"start\":63829},{\"end\":64192,\"start\":64111},{\"end\":64503,\"start\":64496},{\"end\":64843,\"start\":64794},{\"end\":65173,\"start\":65167},{\"end\":65434,\"start\":65388},{\"end\":65867,\"start\":65728},{\"end\":66349,\"start\":66286},{\"end\":66712,\"start\":66663},{\"end\":67000,\"start\":66894},{\"end\":67362,\"start\":67334},{\"end\":67627,\"start\":67495},{\"end\":68036,\"start\":68015},{\"end\":68263,\"start\":68192},{\"end\":68666,\"start\":68622},{\"end\":69071,\"start\":69022},{\"end\":69371,\"start\":69358},{\"end\":69664,\"start\":69654},{\"end\":69983,\"start\":69945},{\"end\":70268,\"start\":70207},{\"end\":70593,\"start\":70531},{\"end\":70910,\"start\":70887},{\"end\":71277,\"start\":71230},{\"end\":71751,\"start\":71732},{\"end\":72217,\"start\":72173},{\"end\":72630,\"start\":72605},{\"end\":72935,\"start\":72871},{\"end\":73268,\"start\":73224},{\"end\":73651,\"start\":73641},{\"end\":74094,\"start\":74007},{\"end\":74624,\"start\":74585},{\"end\":74932,\"start\":74906},{\"end\":75274,\"start\":75193},{\"end\":75743,\"start\":75666},{\"end\":76226,\"start\":76164},{\"end\":76573,\"start\":76499},{\"end\":76810,\"start\":76720},{\"end\":77104,\"start\":77082},{\"end\":77408,\"start\":77349},{\"end\":69380,\"start\":69373},{\"end\":69670,\"start\":69666},{\"end\":74168,\"start\":74096},{\"end\":75342,\"start\":75276},{\"end\":75807,\"start\":75745}]"}}}, "year": 2023, "month": 12, "day": 17}