{"id": 216915079, "updated": "2023-10-06 16:03:31.985", "metadata": {"title": "Consistent Video Depth Estimation", "authors": "[{\"first\":\"Xuan\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Jia-Bin\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Richard\",\"last\":\"Szeliski\",\"middle\":[]},{\"first\":\"Kevin\",\"last\":\"Matzen\",\"middle\":[]},{\"first\":\"Johannes\",\"last\":\"Kopf\",\"middle\":[]}]", "venue": null, "journal": "ACM Transactions on Graphics (TOG)", "publication_date": {"year": 2020, "month": 4, "day": 30}, "abstract": "We present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we use a learning-based prior, i.e., a convolutional neural network trained for single-image depth estimation. At test time, we fine-tune this network to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in parts of the video that are less constrained. We show through quantitative validation that our method achieves higher accuracy and a higher degree of geometric consistency than previous monocular reconstruction methods. Visually, our results appear more stable. Our algorithm is able to handle challenging hand-held captured input videos with a moderate degree of dynamic motion. The improved quality of the reconstruction enables several applications, such as scene reconstruction and advanced video-based visual effects.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2004.15021", "mag": "3048510980", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tog/LuoHSMK20", "doi": "10.1145/3386569.3392377"}}, "content": {"source": {"pdf_hash": "c09bc9f1fff9cb4731dc4f83924727fb3cc96e7e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2004.15021v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3386569.3392377", "status": "BRONZE"}}, "grobid": {"id": "c3adc73ebe1edc07aea9a5b5eed74c3586c2c204", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c09bc9f1fff9cb4731dc4f83924727fb3cc96e7e.txt", "contents": "\nConsistent Video Depth Estimation\n\n\nXuan Luo \nUniversity of Washington\n\n\nRichard Szeliski \nJIA-BIN HUANG\nVirginia Tech\n\nKevin Facebook \nJIA-BIN HUANG\nVirginia Tech\n\nFacebookJohannes Matzen \nJIA-BIN HUANG\nVirginia Tech\n\nFacebookKopf \nJIA-BIN HUANG\nVirginia Tech\n\nConsistent Video Depth Estimation\n\ntowards enabling more casual capture and producing denser reconstructions, driven by high-quality open-source reconstruction systems and recent advances in learning-based techniques, as discussed in the next section.Arguably the easiest way to capture for 3D reconstruction is using hand-held cell phone video, since these cameras are so readily and widely available, and enable truly spontaneous, impromptu capture, as well as quickly covering large spaces. If we could achieve fully dense and accurate reconstruction from such input it would be immensely useful-however, this turns out to be quite difficult.Besides the typical problems that any reconstruction system has to deal with, such as poorly textured areas, repetitive patterns, and occlusions, there are several additional challenges with video: higher noise level, shake and motion blur, rolling shutter deformations, small baseline between adjacent frames, and, often, the presence of dynamic objects, such as people. For these reasons, existing methods often suffer from a variety of problems, such as missing regions in the depth maps (Figure 1b)and inconsistent geometry and flickering depth(Figure 1c).Traditional reconstruction methods [Szeliski 2010] combine sparse structure-from-motion with dense multi-view stereo-essentially matching patches along epipolar lines. When the matches are correct, this results in geometrically accurate reconstructions. However, due to the before-mentioned complications, the matches are often noisy, and typically need to be regularized with heuristic smoothness priors. This often induces incorrect geometry in the affected regions, so that many methods drop pixels with low confidence altogether, leaving \"holes\" in the reconstruction(Figure 1b).\n\n. We present a system for estimating temporally coherent and geometrically consistent depth from a casually captured video. Conventional multi-view stereo methods such as COLMAP [Schonberger and Frahm 2016] often produce incomplete depth on moving objects or poorly textured areas. Learning-based methods (e.g., ) predict dense depth for each frame but the video reconstruction is flickering and geometrically inconsistent. Our video depth estimation is fully dense, globally scale-consistent, and capable of handling dynamically moving objects. We evaluate our method on a wide variety of challenging videos and show that our results enable new video special effects.\n\nWe present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we use a learning-based prior, i.e., a convolutional neural network trained for single-image depth estimation. At test time, we fine-tune this network to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in parts of the video that are less constrained. We show through quantitative validation that our method achieves higher accuracy and a higher degree of geometric consistency than previous monocular reconstruction methods. Visually, our results appear more stable. Our algorithm is able to handle challenging hand-held captured input videos with a moderate degree of dynamic motion. The improved quality of the reconstruction enables several applications, such as scene reconstruction and advanced video-based visual effects.\n\n\nINTRODUCTION\n\n3D scene reconstruction from image sequences has been studied in our community for decades. Until a few years ago, the structure from motion systems for solving this problem were not very robust, and practically only worked \"in the lab\", with highly calibrated and predictable setups. They also, often, produced only sparse reconstructions, i.e., resolving depth at only a few isolated tracked point features. But in the last decade or so, we have seen good progress There has recently been immense progress on learning-based methods that operate on single images. These methods do not require heuristic regularization, but instead learn scene priors from data, which results in better ability to synthesize plausible depth in parts of the scene that would be weakly or even incorrectly constrained in traditional reconstruction approaches. They excel, in particular, at the reconstruction of dynamic scenes, since static and dynamic objects appear the same when we consider a single frame at a time. However, the estimated depth often flickers erratically due to the independent per-frame processing (Figure 1c), and it is not metric (i.e., not related to true depth by a single scale factor). This causes a video reconstruction to be geometrically inconsistent: objects appear to be attached to the camera and \"swimming\" in world-space.\n\nSeveral video-based depth estimation methods have also been developed. These methods address the geometrical consistency of the reconstruction over time either implicitly via recurrent neural networks [Patil et al. 2020;Wang et al. 2019b] or explicitly using multi-view reconstruction Teed and Deng 2020]. State-of-the-art video-based depth estimation methods Teed and Deng 2020], however, handle only static scenes.\n\nIn this work, we present a new video-based reconstruction system that combines the strengths of traditional and learning-based techniques. It uses traditionally-obtained geometric constraints where they are available to achieve accurate and consistent depth, and leverages learning-based priors to fill in the weakly constrained parts of the scene more plausibly than prior heuristics. Technically, this is implemented by fine-tuning the weights of a single-image depth estimation network at test time, so that it learns to satisfy the geometry of a particular scene while retaining its ability to synthesize plausible new depth details where necessary. Our test-time training strategy allows us to use both short-term and long-term constraints and prevent drifting over time. The resulting depth videos are fully dense and detailed, with sharp object boundaries. The reconstruction is flicker-free and geometrically consistent throughout the video. For example, static objects appear rock-steady when projected into world space. The method even supports a gentle amount of dynamic scene motion, such as hand-waving (Figure 9), although it still breaks down for extreme object motion.\n\nThe improved quality and consistency of our depth videos enable interesting new applications, such as fully-automatic video special effects that interact with the dense scene content (Figure 9). We extensively evaluate our method quantitatively and show numerous qualitative results. The source code of our method is publicly available. 1\n\n\nRELATED WORK\n\nSupervised monocular depth estimation. Early learning-based approaches regress local image features to depth [Saxena et al. 2008] or discrete geometric structures [Hoiem et al. 2005], followed by some post-processing steps (e.g., a MRF). Deep learning based models have been successfully applied to single image depth estimation [Eigen and Fergus 2015;Eigen et al. 2014;Fu et al. 2018;Laina et al. 2016;Liu et al. 2015]. However, training these models requires ground truth 1 https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/ depth maps that are difficult to acquire. Several efforts have been made to address this issue, e.g., training on synthetic dataset [Mayer et al. 2016a] followed by domain adaptation [Atapour-Abarghouei and Breckon 2018], collecting relative depth annotations ], using conventional structure-from-motion and multi-view stereo algorithms to obtain pseudo ground truth depth maps from Internet images [Chen et al. 2019a;Li and Snavely 2018], or 3D movies [Ranftl et al. 2019;]. Our method builds upon recent advances in single image depth estimation and further improves the geometric consistency of the depth estimation on videos.\n\nSelf-supervised monocular depth estimation. Due to challenges of scaling up training data collection, self-supervised learning methods have received considerable attention for their ability to learn a monocular depth estimation model directly from raw stereo pairs [Godard et al. 2017] or monocular video . The core idea is to apply differentiable warp and minimize photometric reprojection error. Recent methods improve the performance through incorporating coupled training with optical flow [Ranjan et al. 2019;Yin and Shi 2018;Zou et al. 2018], object motion [Dai et al. 2019;Vijayanarasimhan et al. 2017], surface normal [Qi et al. 2018], edge , and visual odometry [Andraghetti et al. 2019;Shi et al. 2019;Wang et al. 2018b]. Other notable efforts include using stereo information [Guo et al. 2018;Watson et al. 2019], better network architecture and training loss design [Gordon et al. 2019;Guizilini et al. 2019], scale-consistent ego-motion network [Bian et al. 2019], incorporating 3D geometric constraints [Mahjourian et al. 2018], and learning from unknown camera intrinsics [Chen et al. 2019b;Gordon et al. 2019].\n\nMany of these self-supervised methods use a photometric loss. However, these losses can be satisfied even if the geometry is not consistent (in particular, in poorly textured areas). In addition, they do not work well for temporally distant frames because of larger appearance changes. In our ablation study, however, we show that long-range temporal constraints are important for achieving good results.\n\nMulti-view reconstruction. Multi-view stereo algorithms estimate scene depth using multiple images captured from arbitrary viewpoints [Furukawa et al. 2015;Schonberger and Frahm 2016;Seitz et al. 2006]. Recent learning-based methods Im et al. 2019;Kusupati et al. 2019;Ummenhofer et al. 2017;Yao et al. 2018] leverage well-established principles in traditional geometrybased approaches (e.g., cost aggregation and plane-sweep volume) and show state-of-the-art performance in multi-view reconstruction. However, these multi-view stereo techniques assume a static scene. For dynamic objects, these methods either produce erroneous estimates or drop pixels with low confidence. In contrast, our method produces dense depth even in the presence of moderate dynamic scene motion.\n\nDepth from video. Recovering dense depth from monocular video is a challenging problem. To handle moving objects, existing techniques rely on motion segmentation and explicit motion modeling for the moving objects in the scene [Casser et al. 2019;Karsch et al. 2014;Ranftl et al. 2016]. Several methods estimate depth by integrating motion estimation and multi-view reconstruction using two frames [Ummenhofer et al. 2017; or a varying number of frames [Bloesch et al. 2018;Valentin et al. 2018;Zhou et al. 2018]. The state-of-the-art video-to-depth methods Teed and Deng 2020] regress depth (or predict a distribution over depth) based on the cost volume constructed by warping nearby frames to a reference viewpoint. Such model designs thus do not account for dynamically moving objects. In contrast, while we also leverage constraints derived from multi-view geometry, our depth is estimated from (fine-tuned) single-image depth estimation models, and thereby handle dynamic object naturally and without the need for explicit motion segmentation.\n\nTemporal consistency. Applying single-image based methods independently to each frame in a video often produce flickering results. In light of this, various approaches for enforcing temporal consistency have been developed in the context of style transfer [Chen et al. 2017;Huang et al. 2017;Ruder et al. 2016], image-based graphics applications [Lang et al. 2012], video-to-video synthesis [Wang et al. 2018a], or application-agnostic post-processing algorithms [Bonneel et al. 2015;Lai et al. 2018]. The core idea behind these methods is to introduce a \"temporal consistency loss\" (either at training or testing time) that encourages similar values along the temporal correspondences estimated from the input video. In the context of depth estimation from video, several efforts have been made to make the estimated depth more temporally consistent by explicitly applying optical flow-based consistency loss [Karsch et al. 2014] or implicitly encouraging temporal consistency using recurrent neural networks [Patil et al. 2020;Wang et al. 2019b;Zhang et al. 2019b]. Our work differs in that we aim to produce depth estimates from a video that are geometrically consistent. This is particularly important for casually captured videos because the actual depth may not be temporally consistent due to camera motion over time.\n\nDepth-aware visual effects. Dense depth estimation facilitates a wide variety of visual effects such as synthetic depth-of-field , novel view synthesis [Hedman et al. 2017;Hedman and Kopf 2018;Shih et al. 2020], and occlusionaware augmented reality [Holynski and Kopf 2018]. Our work on consistent depth estimation from causally captured videos enables several new video special effects.\n\nTest-time training. Learning on testing data has been used in several different problem contexts: online update in visual tracking [Kalal et al. 2011;Ross et al. 2008], adapting object detectors from images to videos [Jain and Learned-Miller 2011;Tang et al. 2012], and learning video-specific features for person re-identification [Cinbis et al. 2011;Zhang et al. 2019a]. The work most closely related to ours is that of [Casser et al. 2019;Chen et al. 2019b] where they improve monocular depth estimation results by fine-tuning a pretrained model using the testing video sequence. Note that any selfsupervised method can be trained at test time (as in [Casser et al. 2019;Chen et al. 2019b]). However, the focus of previous methods is largely on achieving per-frame accuracy, while our focus is on achieving an accurate prediction with global geometric consistency. Our method achieves accurate and detailed reconstructions with a higher level of temporal smoothness than previous methods, which is important for many video-based applications.\n\nAside from these goals, there are important technical differences between our method and prior ones. The method in [Casser et al. 2019] performs a binary object-level segmentation and estimates rigid per-object transformations. This is appropriate for rigid objects such as cars in a street scene, but less so for highly deformable subjects such as people. The method in [Chen et al. 2019b] uses a geometric loss, similar to ours. However, they only train on consecutive frame pairs and relative poses. We use absolute poses and long-term temporal connections, which our ablation shows is critical for achieving good results ( Figure 6).\n\n\nOVERVIEW\n\nOur method takes a monocular video as input and estimates a camera pose as well as a dense, geometrically consistent depth map (up to scale ambiguity) for each video frame. The term geometric consistency not only implies that the depth maps do not flicker over time but also, that all the depth maps are in mutual agreement. That is, we may project pixels via their depth and camera pose accurately amongst frames. For example, all observations of a static point should be mapped to a single common 3D point in the world coordinate system without drifting.\n\nCasually captured input videos exhibit many characteristics that are challenging for depth reconstruction. Because they are often captured with a handheld, uncalibrated camera, the videos suffer from motion blur and rolling shutter deformations. The poor lighting conditions may cause increased noise level and additional blur. Finally, these videos usually contain dynamically moving objects, such as people and animals, thereby breaking the core assumption of many reconstruction systems designed for static scenes.\n\nAs we explained in the previous sections, in problematic parts of a scene, traditional reconstruction methods typically produce \"holes\" (or, if forced to return a result, estimate very noisy depth.) In areas where these methods are confident enough to return a result, however, it is typically fairly accurate and consistent, because they rely strongly on geometric constraints.\n\nRecent learning-based methods Ranftl et al. 2019] have complementary characteristics. These methods handle the challenges described above just fine because they leverage a strong data-driven prior to predict plausible depth maps from any input image. However, applying these methods independently for each frame results in geometrically inconsistent and temporally flickering results over time.\n\nOur idea is to combine the strengths of both types of methods. We leverage existing single-image depth estimation networks [Godard et al. 2019;Ranftl et al. 2019] that have been trained to synthesize plausible (but not consistent) depth for general color images, and we fine-tune the network using the extracted geometric constraints from a video using traditional reconstruction methods. The network thus learns to produce geometrically consistent depth on a particular video.\n\nOur method proceeds in two stages:\n\nPre-processing (Section 4): As a foundation for extracting geometric constraints among video frames, we first perform a traditional With a monocular video as input, we sample a pair of (potentially distant) frames and estimate the depth using a pre-trained, single-image depth estimation model to obtain initial depth maps. From the pair of images, we establish correspondences using optical flow with forwardbackward consistency check. We then use these correspondences and the camera poses to extract geometric constraints in 3D. We decompose the 3D geometric constraints into two losses: 1) spatial loss and 2) disparity loss and use them to fine-tune the weight of the depth estimation network via standard backpropagation. This test-time training enforces the network to minimize the geometric inconsistency error across multiple frames for this particular video. After the fine-tuning stage, our final depth estimation results from the video is computed from the fine-tuned model. 2016]. To improve pose estimation for videos with dynamic motion, we apply Mask R-CNN [He et al. 2017] to obtain people segmentation and remove these regions for more reliable keypoint extraction and matching, since people account for the majority of dynamic motion in our videos. This step provides us with accurate intrinsic and extrinsic camera parameters as well as a sparse point cloud reconstruction. We also estimate dense correspondence between pairs of frames using optical flow. The camera calibration and dense correspondence, together, enable us to formulate our geometric losses, as described below. The second role of the SfM reconstruction is to provide us with the scale of the scene. Because our method works with monocular input, the reconstruction is ambiguous up to scale. The output of the learning-based depth estimation network is scale-invariant as well. Consequently, to limit the amount the network has to change, we adjust the scale of the SfM reconstruction so that it matches the learning-based method in a robust average sense.\n\n\nStructure-from-Motion (SfM) reconstruction pipeline using an offthe-shelf open-source software COLMAP [Schonberger and Frahm\n\nTest-time Training (Section 5): In this stage, which comprises our primary contribution, we fine-tune a pre-trained depth estimation network so that it produces more geometrically consistent depth for a particular input video. In each iteration, we sample a pair of frames and estimate depth maps using the current network parameters (Figure 2). By comparing the dense correspondence with reprojections obtained using the current depth estimates, we can validate whether the depth maps are geometrically consistent. To this end, we evaluate two geometric losses, 1) spatial loss and 2) disparity loss and back-propagate the errors to update the network weights (which are shared across for all frames). Over time, iteratively sampling many frame pairs, the losses are driven down, and the network learns to estimate depth that is geometrically consistent for this video while retaining its ability to provide plausible regularization in less constrained parts.\n\nThe improvement is often dramatic, our final depth maps are geometrically consistent, temporally coherent across the entire video while accurately delineate clear occluding boundaries even for dynamically moving objects. With depth computed, we can have proper depth edge for occlusion effect and make the geometry of the real scene interact with the virtual objects. We show various compelling visual effects made possible by our video depth estimation in Section 6.5.\n\n\nPRE-PROCESSING\n\nCamera registration. We use the structure-from-motion and multiview stereo reconstruction software COLMAP [Schonberger and Frahm 2016] to estimate for each frame i of the N video frames the intrinsic camera parameters K i , the extrinsic camera parameters (R i , t i ), as well as a semi-dense depth map D MVS i . We set the values to zeros for pixels where the depth is not defined.\n\nBecause dynamic objects often cause errors in the reconstruction, we apply Mask R-CNN [He et al. 2017] to segment out people (the most common \"dynamic objects\" in our videos) in every frame independently, and suppress feature extraction in these areas (COLMAP provides this option). Since smartphone cameras are typically not distorted 2 , we use the SIMPLE_PINHOLE camera model and solve for the shared camera intrinsics for all the frames, as this provides a faster and more robust reconstruction. We use the exhaustive matcher and enable guided matching.\n\nScale calibration. The scale of the SfM and the learning-based reconstructions typically do not match, because both methods are scale-invariant. This manifests in different value ranges of depth maps produced by both methods. To make the scales compatible with the geometric losses, we adjust the SfM scale, because we can simply do so by multiplying all camera translations by a factor.\n\nSpecifically, let D NN i be the initial depth map produced by the learning-based depth estimation method. We first compute the relative scale for image i as:\ns i = median x D NN i (x) / D MVS i (x) D MVS i (x) 0 ,(1)\nwhere D(x) is the depth at pixel x.\n\nWe can then compute the global scale adjustment factor s as\ns = mean i {s i } ,(2)\nand update all the camera translations\nt i = s \u00b7 t i .(3)\nFrame sampling. In the next step, we compute a dense optical flow for certain pairs of frames. This step would be prohibitively computationally expensive to perform for all O(N 2 ) pairs of frames in the video. We, therefore, use a simple hierarchical scheme to prune the set of frame pairs down to O(N ).\n\nThe first level of the hierarchy contains all consecutive frame pairs,\nS 0 = (i, j) |i \u2212 j | = 1 .(4)\nHigher levels contain a progressively sparser sampling of frames,\nS l = (i, j) |i \u2212 j | = 2 l , i mod 2 l \u22121 = 0 .(5)\nThe final set of sampled frames is the union of the pairs from all levels,\nS = 0\u2264l \u2264 \u230alog 2 (N \u22121)\u230b S l .(6)\nOptical flow estimation. For all frame pairs (i, j) in S we need to compute a dense optical flow field F i\u2192j . Because flow estimation works best when the frame pairs align as much as possible, we first align the (potentially distant) frames using a homographywarp (computed with a RANSAC-based fitting method [Szeliski 2010]) to eliminate dominant motion between the two frames (e.g., due to camera rotation). We then use FlowNet2 ] to compute the optical flow between the aligned frames. To account of moving objects and occlusion/dis-occlusion (as they do not satisfy the geometric contraints or are unreliable), we apply a forwardbackward consistency check and remove pixels that have forwardbackward errors larger than 1 pixel, producing a binary map M i\u2192j . Furthermore, we observe that the flow estimation results are not reliable for frame pairs with little overlap. We thus exclude any frame pairs where |M i\u2192j | is less than 20% of the image area from consideration.\n\n\nTEST-TIME TRAINING ON INPUT VIDEO\n\nNow we are ready to describe our test-time training procedure, i.e., how we coerce the depth network through fine-tuning it with a geometric consistency loss to producing more consistent depth for a particular input video. We first describe our geometric loss, and then the overall optimization procedure.\n\nGeometric loss. For a given frame pair (i, j) \u2208 S, the optical flow field F i\u2192j describes which pixel pairs show the same scene point. We can use the flow to test the geometric consistency of our current depth estimates: if the flow is correct and a flow-displaced point f i\u2192j (x) is identical to the depth-reprojected point p i\u2192j (x) (both terms defined below), then the depth must be consistent.\n\nThe idea of our method is that we can turn this into a geometric loss L i\u2192j and back-propagate any consistency errors through the network, so as to coerce it into to producing depth that is more consistent than before. L i\u2192j comprises two terms, an image-space loss L spatial i\u2192j , and a disparity loss L disparity i\u2192j . To define them, we first discuss some notation.\n\nLet x be a 2D pixel coordinate in frame i. The flow-displaced point is simply\nf i\u2192j (x) = x + F i\u2192j (x).(7)\nTo compute the depth-reprojected point p i\u2192j (x), we first lift the 2D coordinate to a 3D point c i (x) in frame i's camera coordinate system, using the camera intrinsics K i as well as the current depth\nestimate D i , c i (x) = D i (x) K \u22121 ix ,(8)\nwherex is the homogeneous augmentation of x. We then further project the point to the other frame j's camera coordinate system,\nc i\u2192j (x) = R T j R i c i (x) +t i \u2212t j ,(9)\nand finally convert it back to a pixel position in frame j,\np i\u2192j (x) = \u03c0 K j c i\u2192j (x) ,(10)\nwhere \u03c0 [x, y, z] T = [ x z , y z ] T . With this notation, the image-space loss for a pixel can be easily defined:\nL spatial i\u2192j (x) = p i\u2192j (x) \u2212 f i\u2192j (x) 2 ,(11)\nwhich penalizes the image-space distance between the flow-displaced and the depth-reprojected point. The disparity loss, similarly, penalizes the disparity distance in camera coordinate system:\nL disparity i\u2192j (x) = u i z \u22121 i\u2192j (x) \u2212 z \u22121 j f i\u2192j (x) ,(12)\nwhere u i is frame i's focal length, and z i and z i\u2192j are the scalar z-component from c i and c i\u2192j , respectively. The overall loss for the pair is then simply a combination of both losses for all pixels where the flow is valid,\nL i\u2192j = 1 M i\u2192j x \u2208M i \u2192j L spatial i\u2192j (x) + \u03bbL disparity i\u2192j (x),(13)\nwhere \u03bb = 0.1 is a balancing coefficient.\n\n\nDiscussion. While the second term in Equation 11\n\n(flow mapping) can handle dynamic motion, the first term (depth reprojection) assumes a static scene. How can this still result in an accurate depth estimation? There are two cases: (1) Consistent motion (e.g., a moving car) can sometimes be aligned with the epipolar geometry and cause our method, like most others, to estimate the wrong depth. (2) Consistent motion that is not epipolar-aligned or inconsistent motion (e.g., a waving hand) causes conflicting constraints; empirically, our test-time training is tolerant to these conflicting constraints and produces accurate results (as seen in many examples in this submission and accompanying materials.)\n\nOptimization. Using the geometric loss between i-th and j-th frames L i\u2192j , we fine-tune the network weights using standard backpropagation. Initializing the network parameters using a pretrained depth estimation model allows us to transfer the knowledge for producing plausible depth maps on images that are challenging for traditional geometry-based reconstructon systems. We fine-tune the network using a fixed number of epochs (20 epochs for all our experiments). In practice, we find that with this simple fine-tuning step the network training does not overfit the data in the sense that it does not lose its ability to synthesize plausible depth in unconstrained or weakly constrained parts of the scene 3 . We also observe that the training handles a certain amount of erroneous supervision (e.g., when the correspondences are incorrectly established). \n\n\nRESULTS AND EVALUATION\n\nIn this section, we first describe the experimental setup (Section 6.1). We then present quantitative comparison with the state-of-the-art depth estimation methods (Section 6.2). We conduct an extensive ablation study to validate the importance of our design choices and their contributions to the results (Section 6.3). Finally, we show qualitative results of our depth estimation and their applications to new advanced video-based visual effects (Section 6.5).\n\n\nExperimental Setup\n\nDataset. Many datasets have been constructed for evaluating depth reconstruction. However, these existing datasets are either for synthetic [Butler et al. 2012;Mayer et al. 2016a] , specific domains (e.g., driving scenes) [Geiger et al. 2013 [Schops et al. 2017;Silberman et al. 2012;Sturm et al. 2012a]. Consequently, we capture custom stereo video datasets for evaluation. Our test set consists of both static and dynamic scenes with a gentle amount of object motion (see Fig. 3 for samples. We capture the videos with stereo fisheye QooCam cameras. 4 The handheld camera rig provides a handy way to capture stereo videos, but it is highly distorted in the periphery due to the fisheye lenses. We, therefore, rectify and crop the center region using the Qoocam Studio 5 and obtain videos of resolution 1920 \u00d7 1440 pixels. The lengths of the captured video range from 119 to 359 frames. Our new video dataset is available on the accompanying website for evaluating future video-based depth estimation. For completeness, we also provide quantitative comparisons with the state-of-the-art depth estimation models on three publicly available datasets: (1) the TUM dataset [Sturm et al. 2012b] (using the 3D Object Reconstruction category), (2) the ScanNet dataset [Dai et al. 2017] (using the testing split provided by [Teed and Deng 2020]), and (3) the KITTI 2015 dataset [Geiger et al. 2012] (using the Eigen split [Eigen et al. 2014]).\n\nEvaluation metrics. To evaluate and compare the quality of the estimated depth from a monocular video on our custom stereo video dataset, we use the following three different metrics.\n\nPhotometric error E p : We use photometric error to quantify the accuracy of the recovered depth. All the methods estimate the depth from the left video stream. Using the estimated depth, we then reproject the pixels from the left video stream to the right one and compute the photometric error as mean squared error of the RGB differences. As the depth map can only be estimated up to a scale ambiguity, we need to align the estimated depth maps to the stereo disparity. Specifically, we compute the stereo disparity by taking the horizontal components from the estimated flow on the stereo pair (using Flownet2 ). For each video frame, we then compute the scale and shift alignment to the computed stereo disparity using RANSAC-based linear regression. We can obtain the global (video-level) scale and shift parameters by taking the mean of the scales/shifts for all the frames.\n\n\nPhotometric\n\nInstability Drift Photometric Instability Fig. 4. Quantitative comparison with the state-of-the-art. We plot the error of all reconstructed pixels, sorted by error. Note, that COLMAP drops some pixels from the reconstruction. Hence, its curve in the left column stops short of 100%; the second and third column evaluate on tracks, which tend to be in textured areas where COLMAP has a higher level of completeness. Top: static sequences; bottom: Dynamic sequences.\n\nInstability E s : We measure instability of the estimated depth maps over time in a video as follows. We first extract a sparse set of reliable tracks from the input monocular video using a standard KLT tracker. We then convert the 2D tracks to 3D tracks, using the camera poses and calibrated depths to unproject 2D tracks to 3D. For a perfectly stable reconstruction, each 3D track should collapse to a single 3D point. We thus can quantify the instability by computing the Euclidean distances of the 3D points for each pair of consecutive frames.\n\nDrift E d : In many cases, while 3D tracks described above may appear somewhat stable for consecutive frames, the errors could be accumulated and cause drift over time. To measure the amount of drift for a particular 3D track, we compute the maximum eigenvalue of the covariance matrix formed by the 3D track. Intuitively, this measures how spread the 3D points is across time.\n\nFor static sequences, we evaluate the estimated depth using all three metrics. For dynamic sequences, we evaluate only on photometric error and instability, as the drift metric does not account for dynamically moving objects in the scene.\n\n\nComparative Evaluation\n\nCompared methods. We compare our results with state-of-the-art depth estimation algorithms from three main categories.\n\n\u2022 Traditional multi-  Static Dynamic Quantitative comparison. Fig. 4 shows the plot of the photometric error, instability, and drift metrics against completeness. In all three metrics, our method compares favorably against previously published algorithms. Our results particularly shine when evaluated on the instability and the drift metrics, highlighting the consistency of our results. Table 1 further reports the summary of the results for different methods.\nE s (%) \u2193 E d (%) \u2193 E p \u2193 E s (%) \u2193 E p \u2193 WSVD [\nVisual comparison. We present in Fig. 5 the qualitative comparison of different depth estimation methods. The traditional multi-view stereo method produces accurate depths at highly textured regions, where reliable matches can be established. These depth maps contain large holes (black pixels), as shown in Fig. 5b. The learning-based single-image depth estimation approaches Ranftl et al. 2019] produce dense, plausible depth maps for each individual video frame. However, flickering depths over time cause geometrically inconsistent depth reconstructions. Video-based methods such as NeuralRGBD alleviate the temporal flicker, yet suffer from drift due Our method produces depth, geometrically consistent, and flicker-free depth estimation from casually captured videos by a hand-held cellphone camera. The first image in each pair is a sample frame, while the second is a scanline slice through the spatio-temporal volume (either color video or video depth). to the limited temporal window used for depth estimation. We refer the readers to the video results in the supplementary material.\n\n\nAblation Study\n\nWe conduct an ablation study to validate the effectiveness of several design choices in our approaches. We first study the effect of losses and the importance of different steps in the pipeline, including scale calibration and overlap test. We summarize these results in Table 2. Fig. 6 visualizes the contributions of various components. We observe that using long-term constraints help improve the stability of the estimated depth over time. As the disparity loss also helps reduce temporal flickering, we further investigate the effects of both design choices in Fig. 7. Our results show that including constraints from long-term frame pairs leads to sharper and temporally more stable results. In contrast, while adding disparity loss reduces temporal flickers, it produces blurry results when using only consecutive frame pairs.  \n\n\nQuantitative Comparisons on Public Benchmarks\n\nWe provide quantitative results on three publicly available benchmark datasets for evaluating the performance of our depth estimation. In all of the evaluation settings, we resize the input images so that the longest image dimension to 384. We finetune the monocular depth estimation network for 20 epochs (the same evaluation setting used in the stereo video dataset).\n\nTUM-RGBD dataset. We evaluate our method on the 11 scenes in the \"3D Object Reconstruction\" category in the TUM-RGBD dataset [Sturm et al. 2012b]. For evaluation, we subsample the videos every 5 frames and obtain sequences ranging from 195 to 593 frames. Here, we use the ground truth camera pose provided by the dataset. We then fine-tune the single-image model from  on the subsampled frames. To compute the error metrics, we align the predicted depth map to the ground truth using per-image median scaling. We report the errors in the disparity (inverse depth) space as it does not require clipping any depth ranges. Table 4 reports the quantitative comparisons with single-frame methods Ranftl et al. 2019] and multi-frame methods ]. Our approach performs favorably against prior methods with a large margin in all evaluation metrics. In particular, our proposed test-time training significantly improves the performance over the baseline model from .\n\nScanNet dataset. Following the evaluation protocol of Teed and Deng [2020], we evaluate our method on the 2,000 sampled frames from the 90 test scenes in the ScanNet dataset [Dai et al. 2017]. We finetune the MiDaS-v2 model [Ranftl et al. 2019] on each testing sequence with a learning rate of 10 \u22125 and \u03bb = 10 \u22125 . Following Teed and Deng [2020], we apply per-image median scaling to align the predicted depth to the ground truth depth map (using the range of [0.5, 8] meters). We then evaluate all the metrics in the depth space over the regions where the ground truth depth is within the range of 0.1 to 10 meters. Table 3 shows the quantitative comparisons with several other multi-frame based depth estimation methods [Tang and Tan 2019;Teed and Deng 2020;Ummenhofer et al. 2017] and the baseline single-image model [Ranftl et al. 2019]. Our method achieves competitive performance with the state-of-the-art algorithms, performing slightly inferior to the DeepV2D method that is trained on the ScanNet training set.  Fig. 8. Quantitative comparison before and after fine-tuning Monodepth2 on KITTI. We plot the each metric over all the test frames sorted by their values. After fine-tuning, we have more outliers due to extreme dynamic motion or failure in camera pose estimation, but achieve improved results for more than 80% of the frames.\n\nKITTI dataset. We evaluate our method on the KITTI dataset [Geiger et al. 2012] using the Eigen test split [Eigen et al. 2014] for comparison with prior monocular depth estimation methods. We estimate the camera poses for each test sequence using COLMAP [Schonberger and Frahm 2016]. We observe that the FlowNet2 model (pre-trained on the Flying Chairs [Dosovitskiy et al. 2015] and Flying Things 3D [Mayer et al. 2016b] datasets) performs poorly in the KITTI dataset [Geiger et al. 2012]. Consequently, we use the FlowNet2 model finetuned on a combination of the KITTI2012 [Geiger et al. 2012] and KITTI2015 [Menze et al. 2015] training sets, FlowNet2ft-kitti, for extracting dense correspondence across frames. Due to the challenging large forward motion in the driving videos, the flow estimations between temporally distant frames are not accurate. We thus only sample pairs with more than 50% consistent flow matches. We use the Monodepth2 [Godard et al. 2019] as the base singleimage depth estimation network. We apply our fine-tuning method at the resolution of 384 \u00d7 112 over each sequence with a learning rate of 4 \u00d7 10 \u22125 and \u03bb = 1. Following the standard protocol [Godard et al. 2017], we cap the depth to 80m and report the results using the per-image median ground truth scaling alignment. Table 5 presents the quantitative comparisons with the state-ofthe-art monocular depth estimation methods. Under this evaluation setting, the results appear to show that our method does not provide an overall improvement over the baseline model Monodepth2 [Godard et al. 2019]. To investigate this issue, we show in Figure 8 the sorted error (Abs Rel) and accuracy (\u03c3 < 1.1) metrics for all the testing frames. The results show that our method indeed improves the performance in more than 80% of the testing frames (even when compared with the model with a high-resolution outputs). However, as COLMAP produce erroneous pose estimates in sequences with large dynamic objects in the scene, our fine-tuning method inevitably results in depth estimation with very large errors. Our method also has difficulty in handling significant dynamic scene motion. As a result, our method does not achieve clear improvement when the results are averaged over all the testing frames. Please see the supplementary material for video result comparison.\n\n\nVideo-based Visual Effects\n\nConsistent video depth estimation enables interesting video-based special effects. Fig. 9 showcases samples of these effects. Full video results can be found in the supplementary material. \n\n\nLimitations\n\nThere are several limitations and drawbacks of the proposed video depth estimation method.\n\n\nPoses Our method currently relies on COLMAP [Schonberger and\n\nFrahm 2016] to estimate the camera pose from a monocular video. In challenging scenarios, e.g., limited camera translation and motion blur, however, COLMAP may not be able produce reliable sparse reconstruction and camera pose estimation. Large pose errors have a strong degrading effect on our results. This limits the applicability of our method on such videos. Integrating learning-based pose estimation (e.g., as in Teed and Deng 2020]) with our approach is an interesting future direction. Dynamic motion Our method supports videos containing moderate object motion. It breaks for extreme object motion. Flow We rely on FlowNet2 ] to establish geometric constraints. Unreliable flow is filtered through forwardbackward consistency checks, but it might be by chance erroneous in a consistent way. In this case our method will fail to produce correct depth. We tried using sparse flow (subsampling dense flow on a regular grid), but it did not perform well. Speed As we extract geometric constraints using all the frames in a video, we do not support online processing. For example, our test-time training step takes about 40 minutes for a video of 244 frames and 708 sampled flow pairs. Developing online and fast variants in the future will be important for practical applications.\n\n\nCONCLUSIONS\n\nWe have presented a simple yet effective method for estimating consistent depth from a monocular video. Our idea is to leverage geometric constraints extracted using conventional multi-view reconstruction methods and use them to fine-tune a pre-trained singleimage depth estimation network. Using our test-time fine-tuning strategy, our network learns to produce geometrically consistent depth estimates across entire video. We conduct extensive quantitative and qualitative evaluation. Our results show that our method compares favorably against several state-of-the-art depth estimation algorithms. Our consistent video depth estimation enables compelling video-based visual effects.  \n\nFig. 1\n1Fig. 1. We present a system for estimating temporally coherent and geometrically consistent depth from a casually captured video. Conventional multi-view stereo methods such as COLMAP [Schonberger and Frahm 2016] often produce incomplete depth on moving objects or poorly textured areas. Learning-based methods (e.g., [Li et al. 2019]) predict dense depth for each frame but the video reconstruction is flickering and geometrically inconsistent. Our video depth estimation is fully dense, globally scale-consistent, and capable of handling dynamically moving objects. We evaluate our method on a wide variety of challenging videos and show that our results enable new video special effects.\n\nFig. 2 .\n2Method overview.\n\n\nImplementation details. We have experimented with several monocular depth estimation architectures and pre-trained weights[Godard et al. 2019; al. 2019; Ranftl et al. 2019]. If not otherwise noted, results in the paper and accompanying materials use Li et al. 's network [2019] (single-image model). We use the other networks in evaluations as noted there. Given an input video, an epoch is defined by one pass over all frame pairs in S. In all of our experiments, we fine-tune the network for 20 epochs with a batch size of 4 and a learning rate of 0.0004 using ADAM optimizer [Kingma and Ba 2015]. The time for test-time training varies for videos of different lengths. For a video of 244 frames, training on 4 NVIDIA Tesla M40 GPUs takes 40 min.\n\n\nview stereo system: COLMAP [Schonberger and Frahm 2016]. \u2022 Single-image depth estimation: Mannequin Challenge [Li et al. 2019] and MiDaS-v2 [Ranftl et al. 2019]. \u2022 Video-based depth estimation: WSVD [Wang et al. 2019a] (two frames) and NeuralRGBD [Liu et al. 2019] (multiple frames).\n\nFig. 5 .\n5Visual comparisons with the state-of-the-arts.\n\nFig. 7 .\n7Analysis of the effects of using long-term temporal constraints and the disparity loss. Please see the supplementary for video comparisons.\n\nFig. 9 .\n9Our consistent depth estimation enables a wide range of fully-automated video-based visual effects. We refer the readers to the supplementary video.\n\n\n], single images[Chen et al.   Static \nDynamic \n\nFig. 3. Example frames from our test set that includes four static sequences \nand three dynamic ones. The dynamic videos contain gentle amount of \nseated motion like playing ukulele and body motion while playing chess, \nflipping the notes while singing, etc. These videos resemble casual video \ncapture scenario where the hand-held camera is shaky and frames contain \nmotion blur. \n\n2016; Li et al. 2019; Li and Snavely 2018], or videos (or multiple \nimages) of static scenes \n\nTable 1 .\n1Quantitative comparisons with the state-of-the-art depth estimation algorithms.\n\nTable 2 .\n2Ablation study. The quantitative evaluation highlights the impor-\ntance of our method design choices. \n\nE s (%) \u2193 E d (%) \u2193 E p \u2193 \n\nOurs w/o scale calibration 0.93 \n3.37 9.99 \nOurs w/o disparity loss \n0.76 \n3.30 9.99 \nOurs w/o overlap test \n0.51 \n2.49 13.20 \nOurs \n0.44 \n2.12 10.08 \n\n\n\nTable 3 .\n3Quantitative comparison on the ScanNet dataset[Dai et al. 2017] using the test split provided byTang and Tan [2019].Abs Rel Sq Rel RMSE RMSE log Sc InvError metric \u2193 \n\n\n\nTable 4 .\n4Quantitative comparison on the TUM-RGBD dataset (3D Object Reconstruction category)[Sturm et al. 2012b] in the disparity space. We report the averaged results over 11 video sequences.Abs Rel Sq Rel RMSE RMSE log \u03c3 < 1.25 \u03c3 < 1.25 2 \u03c3 < 1.25 3Table 5. Quantitative comparisons with existing methods on the KITTI benchmark dataset using the Eigne split. (Top): methods that produce full resolution (1024 \u00d7 320) depth maps. (Bottom): methods that produce low-resolution (384 \u00d7 112) depth maps. Note that for fair comparison, we align the depth results from all the compared methods with per-image median ground truth scaling. Therefore, our reported numbers for Monodepth2 (1024 \u00d7 320) [2019] differ slightly from those in their paper where they use a constant scale for alignment.Abs Rel Sq Rel RMSE RMSE log \u03c3 < 1.25 \u03c3 < 1.25 2 \u03c3 < 1.25 3 Monodepth2 (1024 \u00d7 320) [2019] 0.108 0.806 4.606 0.187Error metric \u2193 \nAccuracy metric \u2191 \n\n\nThis work was done while Xuan was an intern at Facebook.\n\u2022 Xuan Luo, Jia-BinHuang, Richard Szeliski, Kevin Matzen, and Johannes Kopf   \nOur test sequences (Section 6.1) are captured with a fisheye camera, and we remove the distortion through rectification.\nWe note that there are more advanced regularization techniques for transfer learning[Kirkpatrick et al. 2017;]. These can be applied to further improve the performance of our method.\nhttps://www.kandaovr.com/qoocam/ 5 https://www.kandaovr.com/download/\nACM Trans. Graph., Vol. 39, No. 4, Article . Publication date: July 2020.Consistent Video Depth Estimation \u2022 9\n\nEnhancing selfsupervised monocular depth estimation with traditional visual odometry. Lorenzo Andraghetti, Panteleimon Myriokefalitakis, Pier Luigi Dovesi, Belen Luque, Matteo Poggi, Alessandro Pieropan, Stefano Mattoccia, International Conference on 3D Vision (3DV). Lorenzo Andraghetti, Panteleimon Myriokefalitakis, Pier Luigi Dovesi, Belen Luque, Matteo Poggi, Alessandro Pieropan, and Stefano Mattoccia. 2019. Enhancing self- supervised monocular depth estimation with traditional visual odometry. In Inter- national Conference on 3D Vision (3DV).\n\nReal-time monocular depth estimation using synthetic data with domain adaptation via image style transfer. Amir Atapour, -Abarghouei , Toby P Breckon, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Amir Atapour-Abarghouei and Toby P Breckon. 2018. Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nUnsupervised scale-consistent depth and ego-motion learning from monocular video. Jiawang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen, Ming-Ming Cheng, Ian Reid, Advances in Neural Information Processing Systems (NeurIPS). Jiawang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen, Ming-Ming Cheng, and Ian Reid. 2019. Unsupervised scale-consistent depth and ego-motion learning from monocular video. In Advances in Neural Information Processing Systems (NeurIPS).\n\nCodeSLAM\u00e2\u0102\u0164learning a compact, optimisable representation for dense visual SLAM. Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew J Davison, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J Davison. 2018. CodeSLAM\u00e2\u0102\u0164learning a compact, optimisable representation for dense visual SLAM. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nBlind video temporal consistency. Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, Deqing Sun, Sylvain Paris, Hanspeter Pfister, ACM Transactions on Graphics (TOG). 34196Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, Deqing Sun, Sylvain Paris, and Hanspeter Pfister. 2015. Blind video temporal consistency. ACM Transactions on Graphics (TOG) 34, 6 (2015), 196.\n\nA naturalistic open source movie for optical flow evaluation. J Daniel, Jonas Butler, Wulff, B Garrett, Michael J Stanley, Black, European Conference on Computer Vision (ECCV). Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J Black. 2012. A naturalistic open source movie for optical flow evaluation. In European Conference on Computer Vision (ECCV).\n\nDepth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. Vincent Casser, Soeren Pirk, Reza Mahjourian, Anelia Angelova, AAAI Conference on Artificial Intelligence (AAAI). Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia Angelova. 2019. Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. In AAAI Conference on Artificial Intelligence (AAAI).\n\nCoherent online video style transfer. Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, Gang Hua, International Conference on Computer Vision. Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang Hua. 2017. Coherent online video style transfer. In International Conference on Computer Vision.\n\nSingle-image depth perception in the wild. Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng, Advances in Neural Information Processing Systems (NeurIPS). Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. 2016. Single-image depth perception in the wild. In Advances in Neural Information Processing Systems (NeurIPS).\n\nLearning single-image depth from videos using quality assessment networks. Weifeng Chen, Shengyi Qian, Jia Deng, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Weifeng Chen, Shengyi Qian, and Jia Deng. 2019a. Learning single-image depth from videos using quality assessment networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nSelf-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. Yuhua Chen, Cordelia Schmid, Cristian Sminchisescu, International Conference on Computer Vision. Yuhua Chen, Cordelia Schmid, and Cristian Sminchisescu. 2019b. Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. In International Conference on Computer Vision.\n\nUnsupervised metric learning for face identification in TV video. Jakob Ramazan Gokberk Cinbis, Cordelia Verbeek, Schmid, International Conference on Computer Vision. Ramazan Gokberk Cinbis, Jakob Verbeek, and Cordelia Schmid. 2011. Unsupervised metric learning for face identification in TV video. In International Conference on Computer Vision.\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Nie\u00dfner, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. 2017. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nSelf-supervised Object Motion and Depth Estimation from Video. Qi Dai, Vaishakh Patil, Simon Hecker, Dengxin Dai, Luc Van Gool, Konrad Schindler, arXiv:1912.04250arXiv preprintQi Dai, Vaishakh Patil, Simon Hecker, Dengxin Dai, Luc Van Gool, and Konrad Schindler. 2019. Self-supervised Object Motion and Depth Estimation from Video. arXiv preprint arXiv:1912.04250 (2019).\n\nFlowNet: Learning Optical Flow with Convolutional Networks. A Dosovitskiy, P Fischer, E Ilg, P H\u00e4usser, C Haz\u0131rba\u015f, V Golkov, P Smagt, D Cremers, T Brox, IEEE International Conference on Computer Vision (ICCV). A. Dosovitskiy, P. Fischer, E. Ilg, P. H\u00e4usser, C. Haz\u0131rba\u015f, V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. 2015. FlowNet: Learning Optical Flow with Convolutional Networks. In IEEE International Conference on Computer Vision (ICCV). http://lmb. informatik.uni-freiburg.de/Publications/2015/DFIB15\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. David Eigen, Rob Fergus, International Conference on Computer Vision. David Eigen and Rob Fergus. 2015. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In International Conference on Computer Vision.\n\nDepth map prediction from a single image using a multi-scale deep network. David Eigen, Christian Puhrsch, Rob Fergus , Advances in Neural Information Processing Systems (NeurIPS). David Eigen, Christian Puhrsch, and Rob Fergus. 2014. Depth map prediction from a single image using a multi-scale deep network. In Advances in Neural Information Processing Systems (NeurIPS).\n\nDeep ordinal regression network for monocular depth estimation. Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Dacheng Tao, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. 2018. Deep ordinal regression network for monocular depth estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nMulti-view stereo: A tutorial. Yasutaka Furukawa, Carlos Hern\u00e1ndez, Foundations and Trends\u00ae in Computer Graphics and Vision. 9Yasutaka Furukawa, Carlos Hern\u00e1ndez, et al. 2015. Multi-view stereo: A tutorial. Foundations and Trends\u00ae in Computer Graphics and Vision 9, 1-2 (2015), 1-148.\n\nVision meets robotics: The kitti dataset. Andreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, The International Journal of Robotics Research. 32Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. 2013. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research 32, 11 (2013), 1231-1237.\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. Andreas Geiger, Philip Lenz, Raquel Urtasun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012. Are we ready for autonomous driving? the kitti vision benchmark suite. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nUnsupervised monocular depth estimation with left-right consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. 2017. Unsupervised monoc- ular depth estimation with left-right consistency. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nDigging into self-supervised monocular depth estimation. Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, Gabriel J Brostow, International Conference on Computer Vision. Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. 2019. Digging into self-supervised monocular depth estimation. In International Conference on Computer Vision.\n\nDepth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras. Ariel Gordon, Hanhan Li, Rico Jonschkowski, Anelia Angelova, International Conference on Computer Vision. Ariel Gordon, Hanhan Li, Rico Jonschkowski, and Anelia Angelova. 2019. Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras. In International Conference on Computer Vision.\n\nVitor Guizilini, Rares Ambrus, arXiv:1905.02693Sudeep Pillai, and Adrien Gaidon. 2019. PackNet-SfM: 3D Packing for Self-Supervised Monocular Depth Estimation. arXiv preprintVitor Guizilini, Rares Ambrus, Sudeep Pillai, and Adrien Gaidon. 2019. PackNet- SfM: 3D Packing for Self-Supervised Monocular Depth Estimation. arXiv preprint arXiv:1905.02693 (2019).\n\nLearning monocular depth by distilling cross-domain stereo networks. Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, Xiaogang Wang, European Conference on Computer Vision (ECCV). Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and Xiaogang Wang. 2018. Learning monocular depth by distilling cross-domain stereo networks. In European Conference on Computer Vision (ECCV).\n\nKaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick, International Conference on Computer Vision. Mask R-CNNKaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. 2017. Mask R-CNN. In International Conference on Computer Vision.\n\nCasual 3D photography. Peter Hedman, Suhib Alsisan, Richard Szeliski, Johannes Kopf, ACM Transactions on Graphics (TOG). 366234Peter Hedman, Suhib Alsisan, Richard Szeliski, and Johannes Kopf. 2017. Casual 3D photography. ACM Transactions on Graphics (TOG) 36, 6 (2017), 234.\n\nInstant 3d photography. Peter Hedman, Johannes Kopf, ACM Transactions on Graphics (TOG). 37101Peter Hedman and Johannes Kopf. 2018. Instant 3d photography. ACM Transactions on Graphics (TOG) 37, 4 (2018), 101.\n\nDeep blending for free-viewpoint image-based rendering. Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, Gabriel Brostow, In ACM Transactions on Graphics. TOGPeter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. 2018. Deep blending for free-viewpoint image-based rendering. In ACM Transactions on Graphics (TOG).\n\nGeometric context from a single image. Derek Hoiem, Alexei A Efros, Martial Hebert, International Conference on Computer Vision. Derek Hoiem, Alexei A Efros, and Martial Hebert. 2005. Geometric context from a single image. In International Conference on Computer Vision.\n\nFast depth densification for occlusionaware augmented reality. Aleksander Holynski, Johannes Kopf, In ACM Transactions on Graphics (TOG). 194ACMAleksander Holynski and Johannes Kopf. 2018. Fast depth densification for occlusion- aware augmented reality. In ACM Transactions on Graphics (TOG). ACM, 194.\n\nReal-time neural style transfer for videos. Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, Wei Liu, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu. 2017. Real-time neural style transfer for videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nDeepmvs: Learning multi-view stereopsis. Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, Jia-Bin Huang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. 2018. Deepmvs: Learning multi-view stereopsis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nFlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks. Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. 2017. FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nDPSNet: end-to-end deep plane sweep stereo. Sunghoon Im, Hae-Gon Jeon, Stephen Lin, In So Kweon, International Conference on Learning Representations. ICLRSunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. 2019. DPSNet: end-to-end deep plane sweep stereo. In International Conference on Learning Representations (ICLR).\n\nOnline domain adaptation of a pre-trained cascade of classifiers. Vidit Jain, Erik Learned-Miller, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vidit Jain and Erik Learned-Miller. 2011. Online domain adaptation of a pre-trained cascade of classifiers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nTracking-learning-detection. Zdenek Kalal, Krystian Mikolajczyk, Jiri Matas, IEEE Transactions on Pattern Analysis and Machine Intelligence. 34Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. 2011. Tracking-learning-detection. IEEE Transactions on Pattern Analysis and Machine Intelligence 34, 7 (2011), 1409- 1422.\n\nDepth transfer: Depth extraction from video using non-parametric sampling. Kevin Karsch, Ce Liu, Sing Bing Kang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 36Kevin Karsch, Ce Liu, and Sing Bing Kang. 2014. Depth transfer: Depth extraction from video using non-parametric sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence 36, 11 (2014), 2144-2158.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, International Conference on Learning Representations (ICLR). Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR).\n\nOvercoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Proceedings of the national academy of sciences. the national academy of sciences114James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences 114, 13 (2017), 3521-3526.\n\nUday Kusupati, Shuo Cheng, Rui Chen, Hao Su, arXiv:1911.10444Normal Assisted Stereo Depth Estimation. arXiv preprintUday Kusupati, Shuo Cheng, Rui Chen, and Hao Su. 2019. Normal Assisted Stereo Depth Estimation. arXiv preprint arXiv:1911.10444 (2019).\n\nLearning blind video temporal consistency. Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, Ming-Hsuan Yang, European Conference on Computer Vision (ECCV). Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming- Hsuan Yang. 2018. Learning blind video temporal consistency. In European Confer- ence on Computer Vision (ECCV).\n\nDeeper depth prediction with fully convolutional residual networks. Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, Nassir Navab, International Conference on 3D Vision (3DV). Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. 2016. Deeper depth prediction with fully convolutional residual networks. In International Conference on 3D Vision (3DV).\n\nPractical temporal consistency for image-based graphics applications. Manuel Lang, Oliver Wang, Tunc Aydin, ACM Transactions on Graphics (TOG). 31Aljoscha Smolic, and Markus GrossManuel Lang, Oliver Wang, Tunc Aydin, Aljoscha Smolic, and Markus Gross. 2012. Prac- tical temporal consistency for image-based graphics applications. ACM Transactions on Graphics (TOG) 31, 4 (2012), 1-8.\n\nExplicit inductive bias for transfer learning with convolutional networks. Xuhong Li, Yves Grandvalet, Franck Davoine, International Conference on Machine Learning (ICML). Xuhong Li, Yves Grandvalet, and Franck Davoine. 2018. Explicit inductive bias for trans- fer learning with convolutional networks. In International Conference on Machine Learning (ICML).\n\nLearning the Depths of Moving People by Watching Frozen People. Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, William T Freeman, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, and William T Freeman. 2019. Learning the Depths of Moving People by Watching Frozen People. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nMegadepth: Learning single-view depth prediction from internet photos. Zhengqi Li, Noah Snavely, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Zhengqi Li and Noah Snavely. 2018. Megadepth: Learning single-view depth prediction from internet photos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nNeural RGB (r) D Sensing: Depth and Uncertainty From a Video Camera. Chao Liu, Jinwei Gu, Kihwan Kim, G Srinivasa, Jan Narasimhan, Kautz, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa G Narasimhan, and Jan Kautz. 2019. Neural RGB (r) D Sensing: Depth and Uncertainty From a Video Camera. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nLearning depth from single monocular images using deep convolutional neural fields. Fayao Liu, Chunhua Shen, Guosheng Lin, Ian Reid, IEEE Transactions on Pattern Analysis and Machine Intelligence. 38Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. 2015. Learning depth from single monocular images using deep convolutional neural fields. IEEE Transactions on Pattern Analysis and Machine Intelligence 38, 10 (2015), 2024-2039.\n\nUnsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. Reza Mahjourian, Martin Wicke, Anelia Angelova, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Reza Mahjourian, Martin Wicke, and Anelia Angelova. 2018. Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nAlexey Dosovitskiy, and Thomas Brox. 2016a. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Doso- vitskiy, and Thomas Brox. 2016a. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nA Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation. N Mayer, E Ilg, P H\u00e4usser, P Fischer, D Cremers, A Dosovitskiy, T Brox, arXiv:1512.02134IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). N. Mayer, E. Ilg, P. H\u00e4usser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. 2016b. A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation. In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). http://lmb.informatik.uni-freiburg.de/Publications/ 2016/MIFDB16 arXiv:1512.02134.\n\nJoint 3D Estimation of Vehicles and Scene Flow. Moritz Menze, Christian Heipke, Andreas Geiger, ISPRS Workshop on Image Sequence Analysis (ISA). Moritz Menze, Christian Heipke, and Andreas Geiger. 2015. Joint 3D Estimation of Vehicles and Scene Flow. In ISPRS Workshop on Image Sequence Analysis (ISA).\n\nVaishakh Patil, Dengxin Wouter Van Gansbeke, Luc Dai, Van Gool, arXiv:2001.02613Don\u00e2\u0102\u0179t Forget The Past: Recurrent Depth Estimation from Monocular Video. arXiv preprintVaishakh Patil, Wouter Van Gansbeke, Dengxin Dai, and Luc Van Gool. 2020. Don\u00e2\u0102\u0179t Forget The Past: Recurrent Depth Estimation from Monocular Video. arXiv preprint arXiv:2001.02613 (2020).\n\nGeonet: Geometric neural network for joint depth and surface normal estimation. Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, Jiaya Jia, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia. 2018. Geonet: Geometric neural network for joint depth and surface normal estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nRen\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun, arXiv:1907.01341Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer. Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. 2019. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer. arXiv:1907.01341 (2019).\n\nDense monocular depth estimation in complex dynamic scenes. Rene Ranftl, Vibhav Vineet, Qifeng Chen, Vladlen Koltun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Rene Ranftl, Vibhav Vineet, Qifeng Chen, and Vladlen Koltun. 2016. Dense monocular depth estimation in complex dynamic scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nCompetitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation. Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas Wulff, Michael J Black, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas Wulff, and Michael J Black. 2019. Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nIncremental learning for robust visual tracking. A David, Jongwoo Ross, Ruei-Sung Lim, Ming-Hsuan Lin, Yang, International Journal of Computer Vision. 77David A Ross, Jongwoo Lim, Ruei-Sung Lin, and Ming-Hsuan Yang. 2008. Incremental learning for robust visual tracking. International Journal of Computer Vision 77, 1-3 (2008), 125-141.\n\nArtistic style transfer for videos. Manuel Ruder, Alexey Dosovitskiy, Thomas Brox, German Conference on Pattern Recognition. Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox. 2016. Artistic style transfer for videos. In German Conference on Pattern Recognition.\n\nMake3d: Learning 3d scene structure from a single still image. Ashutosh Saxena, Min Sun, Andrew Y Ng, IEEE Transactions on Pattern Analysis and Machine Intelligence. 31Ashutosh Saxena, Min Sun, and Andrew Y Ng. 2008. Make3d: Learning 3d scene structure from a single still image. IEEE Transactions on Pattern Analysis and Machine Intelligence 31, 5 (2008), 824-840.\n\nStructure-from-motion revisited. L Johannes, Jan-Michael Schonberger, Frahm, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Johannes L Schonberger and Jan-Michael Frahm. 2016. Structure-from-motion revisited. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nA multi-view stereo benchmark with high-resolution images and multi-camera videos. Thomas Schops, L Johannes, Silvano Schonberger, Torsten Galliani, Konrad Sattler, Marc Schindler, Andreas Pollefeys, Geiger, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. 2017. A multi-view stereo bench- mark with high-resolution images and multi-camera videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nA comparison and evaluation of multi-view stereo reconstruction algorithms. M Steven, Brian Seitz, James Curless, Daniel Diebel, Richard Scharstein, Szeliski, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. 2006. A comparison and evaluation of multi-view stereo reconstruction algorithms. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nYunxiao Shi, Jing Zhu, Yi Fang, Kuochin Lien, Junli Gu, arXiv:1909.13163Self-Supervised Learning of Depth and Ego-motion with Differentiable Bundle Adjustment. arXiv preprintYunxiao Shi, Jing Zhu, Yi Fang, Kuochin Lien, and Junli Gu. 2019. Self-Supervised Learning of Depth and Ego-motion with Differentiable Bundle Adjustment. arXiv preprint arXiv:1909.13163 (2019).\n\n3D Photography using Context-aware Layered Depth Inpainting. Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 2020. 3D Photography using Context-aware Layered Depth Inpainting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, European Conference on Computer Vision (ECCV). Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. 2012. Indoor segmentation and support inference from rgbd images. In European Conference on Computer Vision (ECCV).\n\nA benchmark for the evaluation of RGB-D SLAM systems. J\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, Daniel Cremers, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE. J\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. 2012a. A benchmark for the evaluation of RGB-D SLAM systems. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 573-580.\n\nA Benchmark for the Evaluation of RGB-D SLAM Systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, International Conference on Intelligent Robot Systems (IROS). J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. 2012b. A Benchmark for the Evaluation of RGB-D SLAM Systems. In International Conference on Intelligent Robot Systems (IROS). Oct. 2012.\n\nRichard Szeliski, Computer Vision: Algorithms and Applications. Berlin, HeidelbergSpringer-Verlag1st ed.Richard Szeliski. 2010. Computer Vision: Algorithms and Applications (1st ed.). Springer- Verlag, Berlin, Heidelberg.\n\nBA-Net: Dense bundle adjustment network. Chengzhou Tang, Ping Tan, International Conference on Learning Representations (ICLR). Chengzhou Tang and Ping Tan. 2019. BA-Net: Dense bundle adjustment network. In International Conference on Learning Representations (ICLR).\n\nShifting weights: Adapting object detectors from image to video. Kevin Tang, Vignesh Ramanathan, Li Fei-Fei, Daphne Koller, Advances in Neural Information Processing Systems (NeurIPS). Kevin Tang, Vignesh Ramanathan, Li Fei-Fei, and Daphne Koller. 2012. Shifting weights: Adapting object detectors from image to video. In Advances in Neural Information Processing Systems (NeurIPS).\n\nDeepV2D: Video to Depth with Differentiable Structure from Motion. Zachary Teed, Jia Deng, International Conference on Learning Representations. ICLRZachary Teed and Jia Deng. 2020. DeepV2D: Video to Depth with Differentiable Structure from Motion. In International Conference on Learning Representations (ICLR).\n\nDemon: Depth and motion network for learning monocular stereo. Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. 2017. Demon: Depth and motion network for learning monocular stereo. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nDepth from motion for smartphone AR. Julien Valentin, Adarsh Kowdle, Jonathan T Barron, Neal Wadhwa, Max Dzitsiuk, Michael Schoenberg, Vivek Verma, Ambrus Csaszar, Eric Turner, Ivan Dryanovski, ACM Transactions on Graphics (TOG). 37Julien Valentin, Adarsh Kowdle, Jonathan T Barron, Neal Wadhwa, Max Dzitsiuk, Michael Schoenberg, Vivek Verma, Ambrus Csaszar, Eric Turner, Ivan Dryanovski, et al. 2018. Depth from motion for smartphone AR. ACM Transactions on Graphics (TOG) 37, 6 (2018), 1-19.\n\nSudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, Katerina Fragkiadaki, arXiv:1704.07804Sfm-net: Learning of structure and motion from video. arXiv preprintSudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, and Katerina Fragkiadaki. 2017. Sfm-net: Learning of structure and motion from video. arXiv preprint arXiv:1704.07804 (2017).\n\nSynthetic depth-of-field with a single-camera mobile phone. Neal Wadhwa, Rahul Garg, David E Jacobs, Bryan E Feldman, Nori Kanazawa, Robert Carroll, Yair Movshovitz-Attias, Jonathan T Barron, Yael Pritch, Marc Levoy, ACM Transactions on Graphics (TOG). 3764Neal Wadhwa, Rahul Garg, David E Jacobs, Bryan E Feldman, Nori Kanazawa, Robert Carroll, Yair Movshovitz-Attias, Jonathan T Barron, Yael Pritch, and Marc Levoy. 2018. Synthetic depth-of-field with a single-camera mobile phone. ACM Transactions on Graphics (TOG) 37, 4 (2018), 64.\n\nWeb Stereo Video Supervision for Depth Prediction from Dynamic Scenes. Chaoyang Wang, Simon Lucey, Federico Perazzi, Oliver Wang, International Conference on 3D Vision (3DV). Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver Wang. 2019a. Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes. In International Conference on 3D Vision (3DV).\n\nLearning depth from monocular videos using direct methods. Chaoyang Wang, Jos\u00e9 Miguel Buenaposada, Rui Zhu, Simon Lucey, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Chaoyang Wang, Jos\u00e9 Miguel Buenaposada, Rui Zhu, and Simon Lucey. 2018b. Learning depth from monocular videos using direct methods. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nRecurrent Neural Network for (Un-) supervised Learning of Monocular Video Visual Odometry and Depth. Rui Wang, M Stephen, Jan-Michael Pizer, Frahm, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Rui Wang, Stephen M Pizer, and Jan-Michael Frahm. 2019b. Recurrent Neural Network for (Un-) supervised Learning of Monocular Video Visual Odometry and Depth. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nVideo-to-video synthesis. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro, Advances in Neural Information Processing Systems (NeurIPS). Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018a. Video-to-video synthesis. In Advances in Neural Information Processing Systems (NeurIPS).\n\nSelf-Supervised Monocular Depth Hints. Jamie Watson, Michael Firman, J Gabriel, Daniyar Brostow, Turmukhambetov, International Conference on Computer Vision. Jamie Watson, Michael Firman, Gabriel J Brostow, and Daniyar Turmukhambetov. 2019. Self-Supervised Monocular Depth Hints. In International Conference on Computer Vision.\n\nLego: Learning edge with geometry all at once by watching videos. Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, Ram Nevatia, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, and Ram Nevatia. 2018. Lego: Learning edge with geometry all at once by watching videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nMvsnet: Depth inference for unstructured multi-view stereo. Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, Long Quan, European Conference on Computer Vision (ECCV). Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. 2018. Mvsnet: Depth inference for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV).\n\nGeonet: Unsupervised learning of dense depth, optical flow and camera pose. Zhichao Yin, Jianping Shi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Zhichao Yin and Jianping Shi. 2018. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nExploiting temporal consistency for real-time video depth estimation. Haokui Zhang, Chunhua Shen, Ying Li, Yuanzhouhan Cao, Yu Liu, Youliang Yan, International Conference on Computer Vision. Haokui Zhang, Chunhua Shen, Ying Li, Yuanzhouhan Cao, Yu Liu, and Youliang Yan. 2019b. Exploiting temporal consistency for real-time video depth estimation. In International Conference on Computer Vision.\n\nTracking Persons-of-Interest via Unsupervised Representation Adaptation. Shun Zhang, Jia-Bin Huang, Jongwoo Lim, Yihong Gong, Jinjun Wang, Narendra Ahuja, Ming-Hsuan Yang, International Journal of Computer Vision. Shun Zhang, Jia-Bin Huang, Jongwoo Lim, Yihong Gong, Jinjun Wang, Narendra Ahuja, and Ming-Hsuan Yang. 2019a. Tracking Persons-of-Interest via Unsupervised Representation Adaptation. International Journal of Computer Vision (2019).\n\nDeeptam: Deep tracking and mapping. Huizhong Zhou, Benjamin Ummenhofer, Thomas Brox, European Conference on Computer Vision (ECCV). Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. 2018. Deeptam: Deep tracking and mapping. In European Conference on Computer Vision (ECCV).\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. 2017. Unsupervised learning of depth and ego-motion from video. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nDf-net: Unsupervised joint learning of depth and flow using cross-task consistency. Yuliang Zou, Zelun Luo, Jia-Bin Huang, European Conference on Computer Vision (ECCV). Yuliang Zou, Zelun Luo, and Jia-Bin Huang. 2018. Df-net: Unsupervised joint learning of depth and flow using cross-task consistency. In European Conference on Computer Vision (ECCV).\n", "annotations": {"author": "[{\"end\":73,\"start\":37},{\"end\":120,\"start\":74},{\"end\":165,\"start\":121},{\"end\":219,\"start\":166},{\"end\":262,\"start\":220}]", "publisher": null, "author_last_name": "[{\"end\":45,\"start\":42},{\"end\":90,\"start\":82},{\"end\":135,\"start\":127},{\"end\":189,\"start\":183},{\"end\":232,\"start\":228}]", "author_first_name": "[{\"end\":41,\"start\":37},{\"end\":81,\"start\":74},{\"end\":126,\"start\":121},{\"end\":182,\"start\":174}]", "author_affiliation": "[{\"end\":72,\"start\":47},{\"end\":119,\"start\":92},{\"end\":164,\"start\":137},{\"end\":218,\"start\":191},{\"end\":261,\"start\":234}]", "title": "[{\"end\":34,\"start\":1},{\"end\":296,\"start\":263}]", "venue": null, "abstract": "[{\"end\":2051,\"start\":298}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2259,\"start\":2231},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":5403,\"start\":5384},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":5421,\"start\":5403},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":5487,\"start\":5468},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":5562,\"start\":5543},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7271,\"start\":7251},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7324,\"start\":7305},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7494,\"start\":7471},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7512,\"start\":7494},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7527,\"start\":7512},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7545,\"start\":7527},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7561,\"start\":7545},{\"end\":7833,\"start\":7814},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7902,\"start\":7865},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8100,\"start\":8081},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8120,\"start\":8100},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8155,\"start\":8135},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8598,\"start\":8578},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8827,\"start\":8807},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":8844,\"start\":8827},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":8859,\"start\":8844},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8893,\"start\":8876},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":8921,\"start\":8893},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8955,\"start\":8939},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9009,\"start\":8984},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9025,\"start\":9009},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":9042,\"start\":9025},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9117,\"start\":9100},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":9136,\"start\":9117},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9211,\"start\":9191},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9233,\"start\":9211},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9289,\"start\":9271},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9354,\"start\":9330},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9419,\"start\":9400},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9438,\"start\":9419},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10003,\"start\":9981},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":10030,\"start\":10003},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":10047,\"start\":10030},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10095,\"start\":10080},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10116,\"start\":10095},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":10139,\"start\":10116},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":10154,\"start\":10139},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10870,\"start\":10850},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10889,\"start\":10870},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10908,\"start\":10889},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":11045,\"start\":11021},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11097,\"start\":11076},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":11118,\"start\":11097},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":11135,\"start\":11118},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":11189,\"start\":11181},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11948,\"start\":11930},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11966,\"start\":11948},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":11984,\"start\":11966},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12038,\"start\":12020},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":12083,\"start\":12065},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12158,\"start\":12137},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12173,\"start\":12158},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12604,\"start\":12584},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12703,\"start\":12684},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":12721,\"start\":12703},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":12739,\"start\":12721},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13172,\"start\":13152},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13193,\"start\":13172},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":13210,\"start\":13193},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13273,\"start\":13249},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13539,\"start\":13520},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13556,\"start\":13539},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13636,\"start\":13606},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":13653,\"start\":13636},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13741,\"start\":13721},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":13759,\"start\":13741},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13831,\"start\":13811},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13849,\"start\":13831},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14063,\"start\":14043},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14081,\"start\":14063},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14570,\"start\":14551},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14825,\"start\":14807},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":16591,\"start\":16573},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17082,\"start\":17062},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":17100,\"start\":17082},{\"end\":18446,\"start\":18441},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18542,\"start\":18527},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21563,\"start\":21548},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":23767,\"start\":23751},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29169,\"start\":29149},{\"end\":29187,\"start\":29169},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29250,\"start\":29231},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":29271,\"start\":29251},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":29293,\"start\":29271},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":29311,\"start\":29293},{\"end\":29562,\"start\":29561},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":30198,\"start\":30179},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30287,\"start\":30271},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":30346,\"start\":30326},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30399,\"start\":30380},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30443,\"start\":30424},{\"end\":31576,\"start\":31570},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":34216,\"start\":34198},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":36334,\"start\":36314},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":36898,\"start\":36880},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":37220,\"start\":37200},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":37337,\"start\":37320},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":37389,\"start\":37370},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":37492,\"start\":37472},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":37888,\"start\":37869},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":37907,\"start\":37888},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":37929,\"start\":37907},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":37986,\"start\":37967},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38574,\"start\":38554},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38621,\"start\":38602},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":38777,\"start\":38749},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38873,\"start\":38848},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":38914,\"start\":38895},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38982,\"start\":38963},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39088,\"start\":39069},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":39123,\"start\":39104},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39460,\"start\":39440},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39689,\"start\":39670},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":40073,\"start\":40054},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":41664,\"start\":41645},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":44087,\"start\":44067},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46341,\"start\":46324},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":46393,\"start\":46374},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":46562,\"start\":46542},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":47754,\"start\":47729}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":43914,\"start\":43215},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43942,\"start\":43915},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44693,\"start\":43943},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44979,\"start\":44694},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45037,\"start\":44980},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45188,\"start\":45038},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45348,\"start\":45189},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45876,\"start\":45349},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":45968,\"start\":45877},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":46265,\"start\":45969},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46446,\"start\":46266},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":47387,\"start\":46447}]", "paragraph": "[{\"end\":2721,\"start\":2053},{\"end\":3826,\"start\":2723},{\"end\":5181,\"start\":3843},{\"end\":5599,\"start\":5183},{\"end\":6785,\"start\":5601},{\"end\":7125,\"start\":6787},{\"end\":8311,\"start\":7142},{\"end\":9439,\"start\":8313},{\"end\":9845,\"start\":9441},{\"end\":10621,\"start\":9847},{\"end\":11672,\"start\":10623},{\"end\":12998,\"start\":11674},{\"end\":13387,\"start\":13000},{\"end\":14434,\"start\":13389},{\"end\":15073,\"start\":14436},{\"end\":15642,\"start\":15086},{\"end\":16161,\"start\":15644},{\"end\":16541,\"start\":16163},{\"end\":16937,\"start\":16543},{\"end\":17416,\"start\":16939},{\"end\":17452,\"start\":17418},{\"end\":19498,\"start\":17454},{\"end\":20587,\"start\":19627},{\"end\":21058,\"start\":20589},{\"end\":21460,\"start\":21077},{\"end\":22019,\"start\":21462},{\"end\":22408,\"start\":22021},{\"end\":22567,\"start\":22410},{\"end\":22662,\"start\":22627},{\"end\":22723,\"start\":22664},{\"end\":22785,\"start\":22747},{\"end\":23110,\"start\":22805},{\"end\":23182,\"start\":23112},{\"end\":23279,\"start\":23214},{\"end\":23406,\"start\":23332},{\"end\":24417,\"start\":23441},{\"end\":24760,\"start\":24455},{\"end\":25159,\"start\":24762},{\"end\":25529,\"start\":25161},{\"end\":25608,\"start\":25531},{\"end\":25842,\"start\":25639},{\"end\":26016,\"start\":25889},{\"end\":26121,\"start\":26062},{\"end\":26271,\"start\":26156},{\"end\":26515,\"start\":26322},{\"end\":26810,\"start\":26580},{\"end\":26924,\"start\":26883},{\"end\":27635,\"start\":26977},{\"end\":28497,\"start\":27637},{\"end\":28986,\"start\":28524},{\"end\":30445,\"start\":29009},{\"end\":30630,\"start\":30447},{\"end\":31512,\"start\":30632},{\"end\":31992,\"start\":31528},{\"end\":32543,\"start\":31994},{\"end\":32922,\"start\":32545},{\"end\":33162,\"start\":32924},{\"end\":33307,\"start\":33189},{\"end\":33771,\"start\":33309},{\"end\":34914,\"start\":33821},{\"end\":35768,\"start\":34933},{\"end\":36187,\"start\":35818},{\"end\":37144,\"start\":36189},{\"end\":38493,\"start\":37146},{\"end\":40834,\"start\":38495},{\"end\":41054,\"start\":40865},{\"end\":41160,\"start\":41070},{\"end\":42511,\"start\":41225},{\"end\":43214,\"start\":42527}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":22626,\"start\":22568},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22746,\"start\":22724},{\"attributes\":{\"id\":\"formula_2\"},\"end\":22804,\"start\":22786},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23213,\"start\":23183},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23331,\"start\":23280},{\"attributes\":{\"id\":\"formula_5\"},\"end\":23440,\"start\":23407},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25638,\"start\":25609},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25888,\"start\":25843},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26061,\"start\":26017},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26155,\"start\":26122},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26321,\"start\":26272},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26579,\"start\":26516},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26882,\"start\":26811},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33820,\"start\":33772}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33705,\"start\":33698},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":35211,\"start\":35204},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":36816,\"start\":36809},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37771,\"start\":37764},{\"end\":39805,\"start\":39798}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3841,\"start\":3829},{\"attributes\":{\"n\":\"2\"},\"end\":7140,\"start\":7128},{\"attributes\":{\"n\":\"3\"},\"end\":15084,\"start\":15076},{\"end\":19625,\"start\":19501},{\"attributes\":{\"n\":\"4\"},\"end\":21075,\"start\":21061},{\"attributes\":{\"n\":\"5\"},\"end\":24453,\"start\":24420},{\"end\":26975,\"start\":26927},{\"attributes\":{\"n\":\"6\"},\"end\":28522,\"start\":28500},{\"attributes\":{\"n\":\"6.1\"},\"end\":29007,\"start\":28989},{\"end\":31526,\"start\":31515},{\"attributes\":{\"n\":\"6.2\"},\"end\":33187,\"start\":33165},{\"attributes\":{\"n\":\"6.3\"},\"end\":34931,\"start\":34917},{\"attributes\":{\"n\":\"6.4\"},\"end\":35816,\"start\":35771},{\"attributes\":{\"n\":\"6.5\"},\"end\":40863,\"start\":40837},{\"attributes\":{\"n\":\"6.6\"},\"end\":41068,\"start\":41057},{\"end\":41223,\"start\":41163},{\"attributes\":{\"n\":\"7\"},\"end\":42525,\"start\":42514},{\"end\":43222,\"start\":43216},{\"end\":43924,\"start\":43916},{\"end\":44989,\"start\":44981},{\"end\":45047,\"start\":45039},{\"end\":45198,\"start\":45190},{\"end\":45887,\"start\":45878},{\"end\":45979,\"start\":45970},{\"end\":46276,\"start\":46267},{\"end\":46457,\"start\":46448}]", "table": "[{\"end\":45876,\"start\":45382},{\"end\":46265,\"start\":45981},{\"end\":46446,\"start\":46429},{\"end\":47387,\"start\":47351}]", "figure_caption": "[{\"end\":43914,\"start\":43224},{\"end\":43942,\"start\":43926},{\"end\":44693,\"start\":43945},{\"end\":44979,\"start\":44696},{\"end\":45037,\"start\":44991},{\"end\":45188,\"start\":45049},{\"end\":45348,\"start\":45200},{\"end\":45382,\"start\":45351},{\"end\":45968,\"start\":45889},{\"end\":46429,\"start\":46278},{\"end\":47351,\"start\":46459}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4955,\"start\":4944},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":6726,\"start\":6717},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":6979,\"start\":6970},{\"end\":15071,\"start\":15063},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19971,\"start\":19961},{\"end\":29489,\"start\":29483},{\"end\":33377,\"start\":33371},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33860,\"start\":33854},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34136,\"start\":34129},{\"end\":35219,\"start\":35213},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35505,\"start\":35499},{\"end\":38174,\"start\":38168},{\"end\":40122,\"start\":40114},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":40954,\"start\":40948}]", "bib_author_first_name": "[{\"end\":48103,\"start\":48096},{\"end\":48128,\"start\":48117},{\"end\":48151,\"start\":48147},{\"end\":48157,\"start\":48152},{\"end\":48171,\"start\":48166},{\"end\":48185,\"start\":48179},{\"end\":48203,\"start\":48193},{\"end\":48221,\"start\":48214},{\"end\":48675,\"start\":48671},{\"end\":48696,\"start\":48685},{\"end\":48703,\"start\":48699},{\"end\":48705,\"start\":48704},{\"end\":49099,\"start\":49092},{\"end\":49113,\"start\":49106},{\"end\":49124,\"start\":49118},{\"end\":49140,\"start\":49131},{\"end\":49154,\"start\":49147},{\"end\":49170,\"start\":49161},{\"end\":49181,\"start\":49178},{\"end\":49590,\"start\":49583},{\"end\":49603,\"start\":49600},{\"end\":49622,\"start\":49616},{\"end\":49636,\"start\":49630},{\"end\":49658,\"start\":49650},{\"end\":50023,\"start\":50016},{\"end\":50038,\"start\":50033},{\"end\":50054,\"start\":50048},{\"end\":50073,\"start\":50067},{\"end\":50086,\"start\":50079},{\"end\":50103,\"start\":50094},{\"end\":50413,\"start\":50412},{\"end\":50427,\"start\":50422},{\"end\":50444,\"start\":50443},{\"end\":50463,\"start\":50454},{\"end\":50831,\"start\":50824},{\"end\":50846,\"start\":50840},{\"end\":50857,\"start\":50853},{\"end\":50876,\"start\":50870},{\"end\":51220,\"start\":51212},{\"end\":51231,\"start\":51227},{\"end\":51240,\"start\":51238},{\"end\":51254,\"start\":51247},{\"end\":51263,\"start\":51259},{\"end\":51518,\"start\":51511},{\"end\":51529,\"start\":51525},{\"end\":51539,\"start\":51534},{\"end\":51549,\"start\":51546},{\"end\":51862,\"start\":51855},{\"end\":51876,\"start\":51869},{\"end\":51886,\"start\":51883},{\"end\":52268,\"start\":52263},{\"end\":52283,\"start\":52275},{\"end\":52300,\"start\":52292},{\"end\":52651,\"start\":52646},{\"end\":52684,\"start\":52676},{\"end\":52997,\"start\":52991},{\"end\":53004,\"start\":53003},{\"end\":53019,\"start\":53012},{\"end\":53033,\"start\":53027},{\"end\":53047,\"start\":53041},{\"end\":53064,\"start\":53056},{\"end\":53456,\"start\":53454},{\"end\":53470,\"start\":53462},{\"end\":53483,\"start\":53478},{\"end\":53499,\"start\":53492},{\"end\":53508,\"start\":53505},{\"end\":53525,\"start\":53519},{\"end\":53825,\"start\":53824},{\"end\":53840,\"start\":53839},{\"end\":53851,\"start\":53850},{\"end\":53858,\"start\":53857},{\"end\":53869,\"start\":53868},{\"end\":53881,\"start\":53880},{\"end\":53891,\"start\":53890},{\"end\":53900,\"start\":53899},{\"end\":53911,\"start\":53910},{\"end\":54391,\"start\":54386},{\"end\":54402,\"start\":54399},{\"end\":54727,\"start\":54722},{\"end\":54744,\"start\":54735},{\"end\":54757,\"start\":54754},{\"end\":54764,\"start\":54758},{\"end\":55090,\"start\":55086},{\"end\":55103,\"start\":55095},{\"end\":55117,\"start\":55110},{\"end\":55130,\"start\":55124},{\"end\":55153,\"start\":55146},{\"end\":55483,\"start\":55475},{\"end\":55500,\"start\":55494},{\"end\":55779,\"start\":55772},{\"end\":55794,\"start\":55788},{\"end\":55810,\"start\":55801},{\"end\":55826,\"start\":55820},{\"end\":56154,\"start\":56147},{\"end\":56169,\"start\":56163},{\"end\":56182,\"start\":56176},{\"end\":56532,\"start\":56525},{\"end\":56546,\"start\":56541},{\"end\":56565,\"start\":56558},{\"end\":56567,\"start\":56566},{\"end\":56912,\"start\":56905},{\"end\":56926,\"start\":56921},{\"end\":56930,\"start\":56927},{\"end\":56945,\"start\":56938},{\"end\":56961,\"start\":56954},{\"end\":56963,\"start\":56962},{\"end\":57298,\"start\":57293},{\"end\":57313,\"start\":57307},{\"end\":57322,\"start\":57318},{\"end\":57343,\"start\":57337},{\"end\":57615,\"start\":57610},{\"end\":57632,\"start\":57627},{\"end\":58045,\"start\":58037},{\"end\":58060,\"start\":58051},{\"end\":58070,\"start\":58065},{\"end\":58080,\"start\":58075},{\"end\":58094,\"start\":58086},{\"end\":58349,\"start\":58342},{\"end\":58361,\"start\":58354},{\"end\":58377,\"start\":58372},{\"end\":58390,\"start\":58386},{\"end\":58614,\"start\":58609},{\"end\":58628,\"start\":58623},{\"end\":58645,\"start\":58638},{\"end\":58664,\"start\":58656},{\"end\":58892,\"start\":58887},{\"end\":58909,\"start\":58901},{\"end\":59135,\"start\":59130},{\"end\":59150,\"start\":59144},{\"end\":59163,\"start\":59159},{\"end\":59182,\"start\":59171},{\"end\":59196,\"start\":59190},{\"end\":59215,\"start\":59208},{\"end\":59506,\"start\":59501},{\"end\":59520,\"start\":59514},{\"end\":59522,\"start\":59521},{\"end\":59537,\"start\":59530},{\"end\":59807,\"start\":59797},{\"end\":59826,\"start\":59818},{\"end\":60088,\"start\":60082},{\"end\":60099,\"start\":60096},{\"end\":60112,\"start\":60106},{\"end\":60121,\"start\":60118},{\"end\":60132,\"start\":60126},{\"end\":60148,\"start\":60140},{\"end\":60161,\"start\":60154},{\"end\":60169,\"start\":60166},{\"end\":60507,\"start\":60501},{\"end\":60520,\"start\":60515},{\"end\":60537,\"start\":60529},{\"end\":60552,\"start\":60544},{\"end\":60567,\"start\":60560},{\"end\":60912,\"start\":60908},{\"end\":60926,\"start\":60918},{\"end\":60940,\"start\":60934},{\"end\":60956,\"start\":60949},{\"end\":60971,\"start\":60965},{\"end\":60991,\"start\":60985},{\"end\":61358,\"start\":61350},{\"end\":61370,\"start\":61363},{\"end\":61384,\"start\":61377},{\"end\":61392,\"start\":61390},{\"end\":61395,\"start\":61393},{\"end\":61704,\"start\":61699},{\"end\":61715,\"start\":61711},{\"end\":62013,\"start\":62007},{\"end\":62029,\"start\":62021},{\"end\":62047,\"start\":62043},{\"end\":62378,\"start\":62373},{\"end\":62389,\"start\":62387},{\"end\":62404,\"start\":62395},{\"end\":62735,\"start\":62734},{\"end\":62751,\"start\":62746},{\"end\":63032,\"start\":63027},{\"end\":63052,\"start\":63046},{\"end\":63066,\"start\":63062},{\"end\":63083,\"start\":63079},{\"end\":63101,\"start\":63092},{\"end\":63120,\"start\":63114},{\"end\":63122,\"start\":63121},{\"end\":63135,\"start\":63129},{\"end\":63147,\"start\":63143},{\"end\":63159,\"start\":63154},{\"end\":63178,\"start\":63169},{\"end\":63602,\"start\":63598},{\"end\":63617,\"start\":63613},{\"end\":63628,\"start\":63625},{\"end\":63638,\"start\":63635},{\"end\":63903,\"start\":63894},{\"end\":63916,\"start\":63909},{\"end\":63930,\"start\":63924},{\"end\":63940,\"start\":63937},{\"end\":63957,\"start\":63952},{\"end\":63975,\"start\":63965},{\"end\":64295,\"start\":64292},{\"end\":64312,\"start\":64303},{\"end\":64333,\"start\":64324},{\"end\":64355,\"start\":64347},{\"end\":64371,\"start\":64365},{\"end\":64714,\"start\":64708},{\"end\":64727,\"start\":64721},{\"end\":64738,\"start\":64734},{\"end\":65104,\"start\":65098},{\"end\":65113,\"start\":65109},{\"end\":65132,\"start\":65126},{\"end\":65454,\"start\":65447},{\"end\":65463,\"start\":65459},{\"end\":65480,\"start\":65471},{\"end\":65494,\"start\":65487},{\"end\":65507,\"start\":65503},{\"end\":65519,\"start\":65517},{\"end\":65534,\"start\":65525},{\"end\":65931,\"start\":65924},{\"end\":65940,\"start\":65936},{\"end\":66267,\"start\":66263},{\"end\":66279,\"start\":66273},{\"end\":66290,\"start\":66284},{\"end\":66297,\"start\":66296},{\"end\":66312,\"start\":66309},{\"end\":66706,\"start\":66701},{\"end\":66719,\"start\":66712},{\"end\":66734,\"start\":66726},{\"end\":66743,\"start\":66740},{\"end\":67152,\"start\":67148},{\"end\":67171,\"start\":67165},{\"end\":67185,\"start\":67179},{\"end\":67647,\"start\":67639},{\"end\":67659,\"start\":67655},{\"end\":67671,\"start\":67665},{\"end\":67688,\"start\":67681},{\"end\":67704,\"start\":67698},{\"end\":68182,\"start\":68181},{\"end\":68191,\"start\":68190},{\"end\":68198,\"start\":68197},{\"end\":68209,\"start\":68208},{\"end\":68220,\"start\":68219},{\"end\":68231,\"start\":68230},{\"end\":68246,\"start\":68245},{\"end\":68766,\"start\":68760},{\"end\":68783,\"start\":68774},{\"end\":68799,\"start\":68792},{\"end\":69024,\"start\":69016},{\"end\":69039,\"start\":69032},{\"end\":69064,\"start\":69061},{\"end\":69461,\"start\":69453},{\"end\":69472,\"start\":69466},{\"end\":69487,\"start\":69479},{\"end\":69499,\"start\":69493},{\"end\":69514,\"start\":69509},{\"end\":69819,\"start\":69815},{\"end\":69834,\"start\":69828},{\"end\":69850,\"start\":69845},{\"end\":69865,\"start\":69859},{\"end\":69884,\"start\":69877},{\"end\":70281,\"start\":70277},{\"end\":70296,\"start\":70290},{\"end\":70311,\"start\":70305},{\"end\":70325,\"start\":70318},{\"end\":70723,\"start\":70717},{\"end\":70737,\"start\":70732},{\"end\":70752,\"start\":70747},{\"end\":70767,\"start\":70761},{\"end\":70779,\"start\":70773},{\"end\":70790,\"start\":70785},{\"end\":70807,\"start\":70798},{\"end\":71229,\"start\":71228},{\"end\":71244,\"start\":71237},{\"end\":71260,\"start\":71251},{\"end\":71276,\"start\":71266},{\"end\":71559,\"start\":71553},{\"end\":71573,\"start\":71567},{\"end\":71593,\"start\":71587},{\"end\":71852,\"start\":71844},{\"end\":71864,\"start\":71861},{\"end\":71878,\"start\":71870},{\"end\":72182,\"start\":72181},{\"end\":72204,\"start\":72193},{\"end\":72537,\"start\":72531},{\"end\":72547,\"start\":72546},{\"end\":72565,\"start\":72558},{\"end\":72586,\"start\":72579},{\"end\":72603,\"start\":72597},{\"end\":72617,\"start\":72613},{\"end\":72636,\"start\":72629},{\"end\":73090,\"start\":73089},{\"end\":73104,\"start\":73099},{\"end\":73117,\"start\":73112},{\"end\":73133,\"start\":73127},{\"end\":73149,\"start\":73142},{\"end\":73485,\"start\":73478},{\"end\":73495,\"start\":73491},{\"end\":73503,\"start\":73501},{\"end\":73517,\"start\":73510},{\"end\":73529,\"start\":73524},{\"end\":73915,\"start\":73908},{\"end\":73931,\"start\":73922},{\"end\":73944,\"start\":73936},{\"end\":73958,\"start\":73951},{\"end\":74299,\"start\":74293},{\"end\":74316,\"start\":74311},{\"end\":74332,\"start\":74324},{\"end\":74343,\"start\":74340},{\"end\":74639,\"start\":74633},{\"end\":74654,\"start\":74647},{\"end\":74671,\"start\":74666},{\"end\":74687,\"start\":74680},{\"end\":74703,\"start\":74697},{\"end\":75086,\"start\":75085},{\"end\":75095,\"start\":75094},{\"end\":75108,\"start\":75107},{\"end\":75118,\"start\":75117},{\"end\":75129,\"start\":75128},{\"end\":75409,\"start\":75402},{\"end\":75675,\"start\":75666},{\"end\":75686,\"start\":75682},{\"end\":75964,\"start\":75959},{\"end\":75978,\"start\":75971},{\"end\":75993,\"start\":75991},{\"end\":76009,\"start\":76003},{\"end\":76352,\"start\":76345},{\"end\":76362,\"start\":76359},{\"end\":76663,\"start\":76655},{\"end\":76684,\"start\":76676},{\"end\":76696,\"start\":76691},{\"end\":76712,\"start\":76704},{\"end\":76724,\"start\":76720},{\"end\":76736,\"start\":76730},{\"end\":76756,\"start\":76750},{\"end\":77125,\"start\":77119},{\"end\":77142,\"start\":77136},{\"end\":77159,\"start\":77151},{\"end\":77161,\"start\":77160},{\"end\":77174,\"start\":77170},{\"end\":77186,\"start\":77183},{\"end\":77204,\"start\":77197},{\"end\":77222,\"start\":77217},{\"end\":77236,\"start\":77230},{\"end\":77250,\"start\":77246},{\"end\":77263,\"start\":77259},{\"end\":77587,\"start\":77577},{\"end\":77613,\"start\":77606},{\"end\":77629,\"start\":77621},{\"end\":77643,\"start\":77638},{\"end\":77664,\"start\":77656},{\"end\":78032,\"start\":78028},{\"end\":78046,\"start\":78041},{\"end\":78058,\"start\":78053},{\"end\":78060,\"start\":78059},{\"end\":78074,\"start\":78069},{\"end\":78076,\"start\":78075},{\"end\":78090,\"start\":78086},{\"end\":78107,\"start\":78101},{\"end\":78121,\"start\":78117},{\"end\":78149,\"start\":78141},{\"end\":78151,\"start\":78150},{\"end\":78164,\"start\":78160},{\"end\":78177,\"start\":78173},{\"end\":78585,\"start\":78577},{\"end\":78597,\"start\":78592},{\"end\":78613,\"start\":78605},{\"end\":78629,\"start\":78623},{\"end\":78938,\"start\":78930},{\"end\":78949,\"start\":78945},{\"end\":78956,\"start\":78950},{\"end\":78973,\"start\":78970},{\"end\":78984,\"start\":78979},{\"end\":79366,\"start\":79363},{\"end\":79374,\"start\":79373},{\"end\":79395,\"start\":79384},{\"end\":79741,\"start\":79732},{\"end\":79755,\"start\":79748},{\"end\":79768,\"start\":79761},{\"end\":79780,\"start\":79774},{\"end\":79792,\"start\":79786},{\"end\":79801,\"start\":79798},{\"end\":79814,\"start\":79809},{\"end\":80127,\"start\":80122},{\"end\":80143,\"start\":80136},{\"end\":80153,\"start\":80152},{\"end\":80170,\"start\":80163},{\"end\":80486,\"start\":80478},{\"end\":80497,\"start\":80493},{\"end\":80508,\"start\":80504},{\"end\":80518,\"start\":80515},{\"end\":80526,\"start\":80523},{\"end\":80871,\"start\":80868},{\"end\":80882,\"start\":80877},{\"end\":80894,\"start\":80888},{\"end\":80903,\"start\":80899},{\"end\":80914,\"start\":80910},{\"end\":81225,\"start\":81218},{\"end\":81239,\"start\":81231},{\"end\":81571,\"start\":81565},{\"end\":81586,\"start\":81579},{\"end\":81597,\"start\":81593},{\"end\":81613,\"start\":81602},{\"end\":81621,\"start\":81619},{\"end\":81635,\"start\":81627},{\"end\":81969,\"start\":81965},{\"end\":81984,\"start\":81977},{\"end\":81999,\"start\":81992},{\"end\":82011,\"start\":82005},{\"end\":82024,\"start\":82018},{\"end\":82039,\"start\":82031},{\"end\":82057,\"start\":82047},{\"end\":82383,\"start\":82375},{\"end\":82398,\"start\":82390},{\"end\":82417,\"start\":82411},{\"end\":82682,\"start\":82675},{\"end\":82696,\"start\":82689},{\"end\":82708,\"start\":82704},{\"end\":82725,\"start\":82718},{\"end\":83086,\"start\":83079},{\"end\":83097,\"start\":83092},{\"end\":83110,\"start\":83103}]", "bib_author_last_name": "[{\"end\":48115,\"start\":48104},{\"end\":48145,\"start\":48129},{\"end\":48164,\"start\":48158},{\"end\":48177,\"start\":48172},{\"end\":48191,\"start\":48186},{\"end\":48212,\"start\":48204},{\"end\":48231,\"start\":48222},{\"end\":48683,\"start\":48676},{\"end\":48713,\"start\":48706},{\"end\":49104,\"start\":49100},{\"end\":49116,\"start\":49114},{\"end\":49129,\"start\":49125},{\"end\":49145,\"start\":49141},{\"end\":49159,\"start\":49155},{\"end\":49176,\"start\":49171},{\"end\":49186,\"start\":49182},{\"end\":49598,\"start\":49591},{\"end\":49614,\"start\":49604},{\"end\":49628,\"start\":49623},{\"end\":49648,\"start\":49637},{\"end\":49666,\"start\":49659},{\"end\":50031,\"start\":50024},{\"end\":50046,\"start\":50039},{\"end\":50065,\"start\":50055},{\"end\":50077,\"start\":50074},{\"end\":50092,\"start\":50087},{\"end\":50111,\"start\":50104},{\"end\":50420,\"start\":50414},{\"end\":50434,\"start\":50428},{\"end\":50441,\"start\":50436},{\"end\":50452,\"start\":50445},{\"end\":50471,\"start\":50464},{\"end\":50478,\"start\":50473},{\"end\":50838,\"start\":50832},{\"end\":50851,\"start\":50847},{\"end\":50868,\"start\":50858},{\"end\":50885,\"start\":50877},{\"end\":51225,\"start\":51221},{\"end\":51236,\"start\":51232},{\"end\":51245,\"start\":51241},{\"end\":51257,\"start\":51255},{\"end\":51267,\"start\":51264},{\"end\":51523,\"start\":51519},{\"end\":51532,\"start\":51530},{\"end\":51544,\"start\":51540},{\"end\":51554,\"start\":51550},{\"end\":51867,\"start\":51863},{\"end\":51881,\"start\":51877},{\"end\":51891,\"start\":51887},{\"end\":52273,\"start\":52269},{\"end\":52290,\"start\":52284},{\"end\":52313,\"start\":52301},{\"end\":52674,\"start\":52652},{\"end\":52692,\"start\":52685},{\"end\":52700,\"start\":52694},{\"end\":53001,\"start\":52998},{\"end\":53010,\"start\":53005},{\"end\":53025,\"start\":53020},{\"end\":53039,\"start\":53034},{\"end\":53054,\"start\":53048},{\"end\":53075,\"start\":53065},{\"end\":53084,\"start\":53077},{\"end\":53460,\"start\":53457},{\"end\":53476,\"start\":53471},{\"end\":53490,\"start\":53484},{\"end\":53503,\"start\":53500},{\"end\":53517,\"start\":53509},{\"end\":53535,\"start\":53526},{\"end\":53837,\"start\":53826},{\"end\":53848,\"start\":53841},{\"end\":53855,\"start\":53852},{\"end\":53866,\"start\":53859},{\"end\":53878,\"start\":53870},{\"end\":53888,\"start\":53882},{\"end\":53897,\"start\":53892},{\"end\":53908,\"start\":53901},{\"end\":53916,\"start\":53912},{\"end\":54397,\"start\":54392},{\"end\":54409,\"start\":54403},{\"end\":54733,\"start\":54728},{\"end\":54752,\"start\":54745},{\"end\":55093,\"start\":55091},{\"end\":55108,\"start\":55104},{\"end\":55122,\"start\":55118},{\"end\":55144,\"start\":55131},{\"end\":55157,\"start\":55154},{\"end\":55492,\"start\":55484},{\"end\":55510,\"start\":55501},{\"end\":55786,\"start\":55780},{\"end\":55799,\"start\":55795},{\"end\":55818,\"start\":55811},{\"end\":55834,\"start\":55827},{\"end\":56161,\"start\":56155},{\"end\":56174,\"start\":56170},{\"end\":56190,\"start\":56183},{\"end\":56539,\"start\":56533},{\"end\":56556,\"start\":56547},{\"end\":56575,\"start\":56568},{\"end\":56919,\"start\":56913},{\"end\":56936,\"start\":56931},{\"end\":56952,\"start\":56946},{\"end\":56971,\"start\":56964},{\"end\":57305,\"start\":57299},{\"end\":57316,\"start\":57314},{\"end\":57335,\"start\":57323},{\"end\":57352,\"start\":57344},{\"end\":57625,\"start\":57616},{\"end\":57639,\"start\":57633},{\"end\":58049,\"start\":58046},{\"end\":58063,\"start\":58061},{\"end\":58073,\"start\":58071},{\"end\":58084,\"start\":58081},{\"end\":58099,\"start\":58095},{\"end\":58352,\"start\":58350},{\"end\":58370,\"start\":58362},{\"end\":58384,\"start\":58378},{\"end\":58399,\"start\":58391},{\"end\":58621,\"start\":58615},{\"end\":58636,\"start\":58629},{\"end\":58654,\"start\":58646},{\"end\":58669,\"start\":58665},{\"end\":58899,\"start\":58893},{\"end\":58914,\"start\":58910},{\"end\":59142,\"start\":59136},{\"end\":59157,\"start\":59151},{\"end\":59169,\"start\":59164},{\"end\":59188,\"start\":59183},{\"end\":59206,\"start\":59197},{\"end\":59223,\"start\":59216},{\"end\":59512,\"start\":59507},{\"end\":59528,\"start\":59523},{\"end\":59544,\"start\":59538},{\"end\":59816,\"start\":59808},{\"end\":59831,\"start\":59827},{\"end\":60094,\"start\":60089},{\"end\":60104,\"start\":60100},{\"end\":60116,\"start\":60113},{\"end\":60124,\"start\":60122},{\"end\":60138,\"start\":60133},{\"end\":60152,\"start\":60149},{\"end\":60164,\"start\":60162},{\"end\":60173,\"start\":60170},{\"end\":60513,\"start\":60508},{\"end\":60527,\"start\":60521},{\"end\":60542,\"start\":60538},{\"end\":60558,\"start\":60553},{\"end\":60573,\"start\":60568},{\"end\":60916,\"start\":60913},{\"end\":60932,\"start\":60927},{\"end\":60947,\"start\":60941},{\"end\":60963,\"start\":60957},{\"end\":60983,\"start\":60972},{\"end\":60996,\"start\":60992},{\"end\":61361,\"start\":61359},{\"end\":61375,\"start\":61371},{\"end\":61388,\"start\":61385},{\"end\":61401,\"start\":61396},{\"end\":61709,\"start\":61705},{\"end\":61730,\"start\":61716},{\"end\":62019,\"start\":62014},{\"end\":62041,\"start\":62030},{\"end\":62053,\"start\":62048},{\"end\":62385,\"start\":62379},{\"end\":62393,\"start\":62390},{\"end\":62409,\"start\":62405},{\"end\":62744,\"start\":62736},{\"end\":62758,\"start\":62752},{\"end\":62762,\"start\":62760},{\"end\":63044,\"start\":63033},{\"end\":63060,\"start\":63053},{\"end\":63077,\"start\":63067},{\"end\":63090,\"start\":63084},{\"end\":63112,\"start\":63102},{\"end\":63127,\"start\":63123},{\"end\":63141,\"start\":63136},{\"end\":63152,\"start\":63148},{\"end\":63167,\"start\":63160},{\"end\":63196,\"start\":63179},{\"end\":63611,\"start\":63603},{\"end\":63623,\"start\":63618},{\"end\":63633,\"start\":63629},{\"end\":63641,\"start\":63639},{\"end\":63907,\"start\":63904},{\"end\":63922,\"start\":63917},{\"end\":63935,\"start\":63931},{\"end\":63950,\"start\":63941},{\"end\":63963,\"start\":63958},{\"end\":63980,\"start\":63976},{\"end\":64301,\"start\":64296},{\"end\":64322,\"start\":64313},{\"end\":64345,\"start\":64334},{\"end\":64363,\"start\":64356},{\"end\":64377,\"start\":64372},{\"end\":64719,\"start\":64715},{\"end\":64732,\"start\":64728},{\"end\":64744,\"start\":64739},{\"end\":65107,\"start\":65105},{\"end\":65124,\"start\":65114},{\"end\":65140,\"start\":65133},{\"end\":65457,\"start\":65455},{\"end\":65469,\"start\":65464},{\"end\":65485,\"start\":65481},{\"end\":65501,\"start\":65495},{\"end\":65515,\"start\":65508},{\"end\":65523,\"start\":65520},{\"end\":65542,\"start\":65535},{\"end\":65934,\"start\":65932},{\"end\":65948,\"start\":65941},{\"end\":66271,\"start\":66268},{\"end\":66282,\"start\":66280},{\"end\":66294,\"start\":66291},{\"end\":66307,\"start\":66298},{\"end\":66323,\"start\":66313},{\"end\":66330,\"start\":66325},{\"end\":66710,\"start\":66707},{\"end\":66724,\"start\":66720},{\"end\":66738,\"start\":66735},{\"end\":66748,\"start\":66744},{\"end\":67163,\"start\":67153},{\"end\":67177,\"start\":67172},{\"end\":67194,\"start\":67186},{\"end\":67653,\"start\":67648},{\"end\":67663,\"start\":67660},{\"end\":67679,\"start\":67672},{\"end\":67696,\"start\":67689},{\"end\":67712,\"start\":67705},{\"end\":68188,\"start\":68183},{\"end\":68195,\"start\":68192},{\"end\":68206,\"start\":68199},{\"end\":68217,\"start\":68210},{\"end\":68228,\"start\":68221},{\"end\":68243,\"start\":68232},{\"end\":68251,\"start\":68247},{\"end\":68772,\"start\":68767},{\"end\":68790,\"start\":68784},{\"end\":68806,\"start\":68800},{\"end\":69030,\"start\":69025},{\"end\":69059,\"start\":69040},{\"end\":69068,\"start\":69065},{\"end\":69078,\"start\":69070},{\"end\":69464,\"start\":69462},{\"end\":69477,\"start\":69473},{\"end\":69491,\"start\":69488},{\"end\":69507,\"start\":69500},{\"end\":69518,\"start\":69515},{\"end\":69826,\"start\":69820},{\"end\":69843,\"start\":69835},{\"end\":69857,\"start\":69851},{\"end\":69875,\"start\":69866},{\"end\":69891,\"start\":69885},{\"end\":70288,\"start\":70282},{\"end\":70303,\"start\":70297},{\"end\":70316,\"start\":70312},{\"end\":70332,\"start\":70326},{\"end\":70730,\"start\":70724},{\"end\":70745,\"start\":70738},{\"end\":70759,\"start\":70753},{\"end\":70771,\"start\":70768},{\"end\":70783,\"start\":70780},{\"end\":70796,\"start\":70791},{\"end\":70813,\"start\":70808},{\"end\":71235,\"start\":71230},{\"end\":71249,\"start\":71245},{\"end\":71264,\"start\":71261},{\"end\":71280,\"start\":71277},{\"end\":71286,\"start\":71282},{\"end\":71565,\"start\":71560},{\"end\":71585,\"start\":71574},{\"end\":71598,\"start\":71594},{\"end\":71859,\"start\":71853},{\"end\":71868,\"start\":71865},{\"end\":71881,\"start\":71879},{\"end\":72191,\"start\":72183},{\"end\":72216,\"start\":72205},{\"end\":72223,\"start\":72218},{\"end\":72544,\"start\":72538},{\"end\":72556,\"start\":72548},{\"end\":72577,\"start\":72566},{\"end\":72595,\"start\":72587},{\"end\":72611,\"start\":72604},{\"end\":72627,\"start\":72618},{\"end\":72646,\"start\":72637},{\"end\":72654,\"start\":72648},{\"end\":73097,\"start\":73091},{\"end\":73110,\"start\":73105},{\"end\":73125,\"start\":73118},{\"end\":73140,\"start\":73134},{\"end\":73160,\"start\":73150},{\"end\":73170,\"start\":73162},{\"end\":73489,\"start\":73486},{\"end\":73499,\"start\":73496},{\"end\":73508,\"start\":73504},{\"end\":73522,\"start\":73518},{\"end\":73532,\"start\":73530},{\"end\":73920,\"start\":73916},{\"end\":73934,\"start\":73932},{\"end\":73949,\"start\":73945},{\"end\":73964,\"start\":73959},{\"end\":74309,\"start\":74300},{\"end\":74322,\"start\":74317},{\"end\":74338,\"start\":74333},{\"end\":74350,\"start\":74344},{\"end\":74645,\"start\":74640},{\"end\":74664,\"start\":74655},{\"end\":74678,\"start\":74672},{\"end\":74695,\"start\":74688},{\"end\":74711,\"start\":74704},{\"end\":75092,\"start\":75087},{\"end\":75105,\"start\":75096},{\"end\":75115,\"start\":75109},{\"end\":75126,\"start\":75119},{\"end\":75137,\"start\":75130},{\"end\":75418,\"start\":75410},{\"end\":75680,\"start\":75676},{\"end\":75690,\"start\":75687},{\"end\":75969,\"start\":75965},{\"end\":75989,\"start\":75979},{\"end\":76001,\"start\":75994},{\"end\":76016,\"start\":76010},{\"end\":76357,\"start\":76353},{\"end\":76367,\"start\":76363},{\"end\":76674,\"start\":76664},{\"end\":76689,\"start\":76685},{\"end\":76702,\"start\":76697},{\"end\":76718,\"start\":76713},{\"end\":76728,\"start\":76725},{\"end\":76748,\"start\":76737},{\"end\":76761,\"start\":76757},{\"end\":77134,\"start\":77126},{\"end\":77149,\"start\":77143},{\"end\":77168,\"start\":77162},{\"end\":77181,\"start\":77175},{\"end\":77195,\"start\":77187},{\"end\":77215,\"start\":77205},{\"end\":77228,\"start\":77223},{\"end\":77244,\"start\":77237},{\"end\":77257,\"start\":77251},{\"end\":77274,\"start\":77264},{\"end\":77604,\"start\":77588},{\"end\":77619,\"start\":77614},{\"end\":77636,\"start\":77630},{\"end\":77654,\"start\":77644},{\"end\":77676,\"start\":77665},{\"end\":78039,\"start\":78033},{\"end\":78051,\"start\":78047},{\"end\":78067,\"start\":78061},{\"end\":78084,\"start\":78077},{\"end\":78099,\"start\":78091},{\"end\":78115,\"start\":78108},{\"end\":78139,\"start\":78122},{\"end\":78158,\"start\":78152},{\"end\":78171,\"start\":78165},{\"end\":78183,\"start\":78178},{\"end\":78590,\"start\":78586},{\"end\":78603,\"start\":78598},{\"end\":78621,\"start\":78614},{\"end\":78634,\"start\":78630},{\"end\":78943,\"start\":78939},{\"end\":78968,\"start\":78957},{\"end\":78977,\"start\":78974},{\"end\":78990,\"start\":78985},{\"end\":79371,\"start\":79367},{\"end\":79382,\"start\":79375},{\"end\":79401,\"start\":79396},{\"end\":79408,\"start\":79403},{\"end\":79746,\"start\":79742},{\"end\":79759,\"start\":79756},{\"end\":79772,\"start\":79769},{\"end\":79784,\"start\":79781},{\"end\":79796,\"start\":79793},{\"end\":79807,\"start\":79802},{\"end\":79824,\"start\":79815},{\"end\":80134,\"start\":80128},{\"end\":80150,\"start\":80144},{\"end\":80161,\"start\":80154},{\"end\":80178,\"start\":80171},{\"end\":80194,\"start\":80180},{\"end\":80491,\"start\":80487},{\"end\":80502,\"start\":80498},{\"end\":80513,\"start\":80509},{\"end\":80521,\"start\":80519},{\"end\":80534,\"start\":80527},{\"end\":80875,\"start\":80872},{\"end\":80886,\"start\":80883},{\"end\":80897,\"start\":80895},{\"end\":80908,\"start\":80904},{\"end\":80919,\"start\":80915},{\"end\":81229,\"start\":81226},{\"end\":81243,\"start\":81240},{\"end\":81577,\"start\":81572},{\"end\":81591,\"start\":81587},{\"end\":81600,\"start\":81598},{\"end\":81617,\"start\":81614},{\"end\":81625,\"start\":81622},{\"end\":81639,\"start\":81636},{\"end\":81975,\"start\":81970},{\"end\":81990,\"start\":81985},{\"end\":82003,\"start\":82000},{\"end\":82016,\"start\":82012},{\"end\":82029,\"start\":82025},{\"end\":82045,\"start\":82040},{\"end\":82062,\"start\":82058},{\"end\":82388,\"start\":82384},{\"end\":82409,\"start\":82399},{\"end\":82422,\"start\":82418},{\"end\":82687,\"start\":82683},{\"end\":82702,\"start\":82697},{\"end\":82716,\"start\":82709},{\"end\":82730,\"start\":82726},{\"end\":83090,\"start\":83087},{\"end\":83101,\"start\":83098},{\"end\":83116,\"start\":83111}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":199501929},\"end\":48562,\"start\":48010},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52206410},\"end\":49008,\"start\":48564},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":201660344},\"end\":49500,\"start\":49010},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4624670},\"end\":49980,\"start\":49502},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":215748287},\"end\":50348,\"start\":49982},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4637111},\"end\":50714,\"start\":50350},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":53437459},\"end\":51172,\"start\":50716},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1497291},\"end\":51466,\"start\":51174},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14395783},\"end\":51778,\"start\":51468},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":49416362},\"end\":52153,\"start\":51780},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":196470796},\"end\":52578,\"start\":52155},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10050369},\"end\":52926,\"start\":52580},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7684883},\"end\":53389,\"start\":52928},{\"attributes\":{\"doi\":\"arXiv:1912.04250\",\"id\":\"b13\"},\"end\":53762,\"start\":53391},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":12552176},\"end\":54276,\"start\":53764},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":102496818},\"end\":54645,\"start\":54278},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2255738},\"end\":55020,\"start\":54647},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":46968214},\"end\":55442,\"start\":55022},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":61831046},\"end\":55728,\"start\":55444},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9455111},\"end\":56074,\"start\":55730},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6724907},\"end\":56454,\"start\":56076},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206596513},\"end\":56846,\"start\":56456},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":46936649},\"end\":57200,\"start\":56848},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":131774103},\"end\":57608,\"start\":57202},{\"attributes\":{\"doi\":\"arXiv:1905.02693\",\"id\":\"b24\"},\"end\":57966,\"start\":57610},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":52051348},\"end\":58340,\"start\":57968},{\"attributes\":{\"id\":\"b26\"},\"end\":58584,\"start\":58342},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9738802},\"end\":58861,\"start\":58586},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":51875014},\"end\":59072,\"start\":58863},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":54151018},\"end\":59460,\"start\":59074},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206769405},\"end\":59732,\"start\":59462},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":52952255},\"end\":60036,\"start\":59734},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":28280219},\"end\":60458,\"start\":60038},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4559916},\"end\":60836,\"start\":60460},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3759573},\"end\":61304,\"start\":60838},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":53073405},\"end\":61631,\"start\":61306},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2260730},\"end\":61976,\"start\":61633},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":208933582},\"end\":62296,\"start\":61978},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":12230426},\"end\":62688,\"start\":62298},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6628106},\"end\":62970,\"start\":62690},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4704285},\"end\":63596,\"start\":62972},{\"attributes\":{\"doi\":\"arXiv:1911.10444\",\"id\":\"b41\"},\"end\":63849,\"start\":63598},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":51892815},\"end\":64222,\"start\":63851},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":11091110},\"end\":64636,\"start\":64224},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":207194152},\"end\":65021,\"start\":64638},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3603048},\"end\":65381,\"start\":65023},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":131775632},\"end\":65851,\"start\":65383},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":4572038},\"end\":66192,\"start\":65853},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":57759271},\"end\":66615,\"start\":66194},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":15774646},\"end\":67047,\"start\":66617},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":3645349},\"end\":67489,\"start\":67049},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":206594275},\"end\":68075,\"start\":67491},{\"attributes\":{\"doi\":\"arXiv:1512.02134\",\"id\":\"b52\",\"matched_paper_id\":206594275},\"end\":68710,\"start\":68077},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":17309230},\"end\":69014,\"start\":68712},{\"attributes\":{\"doi\":\"arXiv:2001.02613\",\"id\":\"b54\"},\"end\":69371,\"start\":69016},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":4797810},\"end\":69813,\"start\":69373},{\"attributes\":{\"doi\":\"arXiv:1907.01341\",\"id\":\"b56\"},\"end\":70215,\"start\":69815},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":6385934},\"end\":70597,\"start\":70217},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":52936213},\"end\":71177,\"start\":70599},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":1089627},\"end\":71515,\"start\":71179},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":47476652},\"end\":71779,\"start\":71517},{\"attributes\":{\"id\":\"b61\"},\"end\":72146,\"start\":71781},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":1728538},\"end\":72446,\"start\":72148},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":20603040},\"end\":73011,\"start\":72448},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":206590743},\"end\":73476,\"start\":73013},{\"attributes\":{\"doi\":\"arXiv:1909.13163\",\"id\":\"b65\"},\"end\":73845,\"start\":73478},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":215548442},\"end\":74231,\"start\":73847},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":545361},\"end\":74577,\"start\":74233},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":206942855},\"end\":75029,\"start\":74579},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":206942855},\"end\":75400,\"start\":75031},{\"attributes\":{\"id\":\"b70\"},\"end\":75623,\"start\":75402},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":49189997},\"end\":75892,\"start\":75625},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":813192},\"end\":76276,\"start\":75894},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":54482591},\"end\":76590,\"start\":76278},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":6159584},\"end\":77080,\"start\":76592},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":54115081},\"end\":77575,\"start\":77082},{\"attributes\":{\"doi\":\"arXiv:1704.07804\",\"id\":\"b76\"},\"end\":77966,\"start\":77577},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":48358714},\"end\":78504,\"start\":77968},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":131775376},\"end\":78869,\"start\":78506},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":21352010},\"end\":79260,\"start\":78871},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":115130968},\"end\":79704,\"start\":79262},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":52049245},\"end\":80081,\"start\":79706},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":202676783},\"end\":80410,\"start\":80083},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":4426384},\"end\":80806,\"start\":80412},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":4712004},\"end\":81140,\"start\":80808},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":3714620},\"end\":81493,\"start\":81142},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":199543416},\"end\":81890,\"start\":81495},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":11002606},\"end\":82337,\"start\":81892},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":51929808},\"end\":82615,\"start\":82339},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":11977588},\"end\":82993,\"start\":82617},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":52158437},\"end\":83347,\"start\":82995}]", "bib_title": "[{\"end\":48094,\"start\":48010},{\"end\":48669,\"start\":48564},{\"end\":49090,\"start\":49010},{\"end\":49581,\"start\":49502},{\"end\":50014,\"start\":49982},{\"end\":50410,\"start\":50350},{\"end\":50822,\"start\":50716},{\"end\":51210,\"start\":51174},{\"end\":51509,\"start\":51468},{\"end\":51853,\"start\":51780},{\"end\":52261,\"start\":52155},{\"end\":52644,\"start\":52580},{\"end\":52989,\"start\":52928},{\"end\":53822,\"start\":53764},{\"end\":54384,\"start\":54278},{\"end\":54720,\"start\":54647},{\"end\":55084,\"start\":55022},{\"end\":55473,\"start\":55444},{\"end\":55770,\"start\":55730},{\"end\":56145,\"start\":56076},{\"end\":56523,\"start\":56456},{\"end\":56903,\"start\":56848},{\"end\":57291,\"start\":57202},{\"end\":58035,\"start\":57968},{\"end\":58607,\"start\":58586},{\"end\":58885,\"start\":58863},{\"end\":59128,\"start\":59074},{\"end\":59499,\"start\":59462},{\"end\":59795,\"start\":59734},{\"end\":60080,\"start\":60038},{\"end\":60499,\"start\":60460},{\"end\":60906,\"start\":60838},{\"end\":61348,\"start\":61306},{\"end\":61697,\"start\":61633},{\"end\":62005,\"start\":61978},{\"end\":62371,\"start\":62298},{\"end\":62732,\"start\":62690},{\"end\":63025,\"start\":62972},{\"end\":63892,\"start\":63851},{\"end\":64290,\"start\":64224},{\"end\":64706,\"start\":64638},{\"end\":65096,\"start\":65023},{\"end\":65445,\"start\":65383},{\"end\":65922,\"start\":65853},{\"end\":66261,\"start\":66194},{\"end\":66699,\"start\":66617},{\"end\":67146,\"start\":67049},{\"end\":67637,\"start\":67491},{\"end\":68179,\"start\":68077},{\"end\":68758,\"start\":68712},{\"end\":69451,\"start\":69373},{\"end\":70275,\"start\":70217},{\"end\":70715,\"start\":70599},{\"end\":71226,\"start\":71179},{\"end\":71551,\"start\":71517},{\"end\":71842,\"start\":71781},{\"end\":72179,\"start\":72148},{\"end\":72529,\"start\":72448},{\"end\":73087,\"start\":73013},{\"end\":73906,\"start\":73847},{\"end\":74291,\"start\":74233},{\"end\":74631,\"start\":74579},{\"end\":75083,\"start\":75031},{\"end\":75664,\"start\":75625},{\"end\":75957,\"start\":75894},{\"end\":76343,\"start\":76278},{\"end\":76653,\"start\":76592},{\"end\":77117,\"start\":77082},{\"end\":78026,\"start\":77968},{\"end\":78575,\"start\":78506},{\"end\":78928,\"start\":78871},{\"end\":79361,\"start\":79262},{\"end\":79730,\"start\":79706},{\"end\":80120,\"start\":80083},{\"end\":80476,\"start\":80412},{\"end\":80866,\"start\":80808},{\"end\":81216,\"start\":81142},{\"end\":81563,\"start\":81495},{\"end\":81963,\"start\":81892},{\"end\":82373,\"start\":82339},{\"end\":82673,\"start\":82617},{\"end\":83077,\"start\":82995}]", "bib_author": "[{\"end\":48117,\"start\":48096},{\"end\":48147,\"start\":48117},{\"end\":48166,\"start\":48147},{\"end\":48179,\"start\":48166},{\"end\":48193,\"start\":48179},{\"end\":48214,\"start\":48193},{\"end\":48233,\"start\":48214},{\"end\":48685,\"start\":48671},{\"end\":48699,\"start\":48685},{\"end\":48715,\"start\":48699},{\"end\":49106,\"start\":49092},{\"end\":49118,\"start\":49106},{\"end\":49131,\"start\":49118},{\"end\":49147,\"start\":49131},{\"end\":49161,\"start\":49147},{\"end\":49178,\"start\":49161},{\"end\":49188,\"start\":49178},{\"end\":49600,\"start\":49583},{\"end\":49616,\"start\":49600},{\"end\":49630,\"start\":49616},{\"end\":49650,\"start\":49630},{\"end\":49668,\"start\":49650},{\"end\":50033,\"start\":50016},{\"end\":50048,\"start\":50033},{\"end\":50067,\"start\":50048},{\"end\":50079,\"start\":50067},{\"end\":50094,\"start\":50079},{\"end\":50113,\"start\":50094},{\"end\":50422,\"start\":50412},{\"end\":50436,\"start\":50422},{\"end\":50443,\"start\":50436},{\"end\":50454,\"start\":50443},{\"end\":50473,\"start\":50454},{\"end\":50480,\"start\":50473},{\"end\":50840,\"start\":50824},{\"end\":50853,\"start\":50840},{\"end\":50870,\"start\":50853},{\"end\":50887,\"start\":50870},{\"end\":51227,\"start\":51212},{\"end\":51238,\"start\":51227},{\"end\":51247,\"start\":51238},{\"end\":51259,\"start\":51247},{\"end\":51269,\"start\":51259},{\"end\":51525,\"start\":51511},{\"end\":51534,\"start\":51525},{\"end\":51546,\"start\":51534},{\"end\":51556,\"start\":51546},{\"end\":51869,\"start\":51855},{\"end\":51883,\"start\":51869},{\"end\":51893,\"start\":51883},{\"end\":52275,\"start\":52263},{\"end\":52292,\"start\":52275},{\"end\":52315,\"start\":52292},{\"end\":52676,\"start\":52646},{\"end\":52694,\"start\":52676},{\"end\":52702,\"start\":52694},{\"end\":53003,\"start\":52991},{\"end\":53012,\"start\":53003},{\"end\":53027,\"start\":53012},{\"end\":53041,\"start\":53027},{\"end\":53056,\"start\":53041},{\"end\":53077,\"start\":53056},{\"end\":53086,\"start\":53077},{\"end\":53462,\"start\":53454},{\"end\":53478,\"start\":53462},{\"end\":53492,\"start\":53478},{\"end\":53505,\"start\":53492},{\"end\":53519,\"start\":53505},{\"end\":53537,\"start\":53519},{\"end\":53839,\"start\":53824},{\"end\":53850,\"start\":53839},{\"end\":53857,\"start\":53850},{\"end\":53868,\"start\":53857},{\"end\":53880,\"start\":53868},{\"end\":53890,\"start\":53880},{\"end\":53899,\"start\":53890},{\"end\":53910,\"start\":53899},{\"end\":53918,\"start\":53910},{\"end\":54399,\"start\":54386},{\"end\":54411,\"start\":54399},{\"end\":54735,\"start\":54722},{\"end\":54754,\"start\":54735},{\"end\":54767,\"start\":54754},{\"end\":55095,\"start\":55086},{\"end\":55110,\"start\":55095},{\"end\":55124,\"start\":55110},{\"end\":55146,\"start\":55124},{\"end\":55159,\"start\":55146},{\"end\":55494,\"start\":55475},{\"end\":55512,\"start\":55494},{\"end\":55788,\"start\":55772},{\"end\":55801,\"start\":55788},{\"end\":55820,\"start\":55801},{\"end\":55836,\"start\":55820},{\"end\":56163,\"start\":56147},{\"end\":56176,\"start\":56163},{\"end\":56192,\"start\":56176},{\"end\":56541,\"start\":56525},{\"end\":56558,\"start\":56541},{\"end\":56577,\"start\":56558},{\"end\":56921,\"start\":56905},{\"end\":56938,\"start\":56921},{\"end\":56954,\"start\":56938},{\"end\":56973,\"start\":56954},{\"end\":57307,\"start\":57293},{\"end\":57318,\"start\":57307},{\"end\":57337,\"start\":57318},{\"end\":57354,\"start\":57337},{\"end\":57627,\"start\":57610},{\"end\":57641,\"start\":57627},{\"end\":58051,\"start\":58037},{\"end\":58065,\"start\":58051},{\"end\":58075,\"start\":58065},{\"end\":58086,\"start\":58075},{\"end\":58101,\"start\":58086},{\"end\":58354,\"start\":58342},{\"end\":58372,\"start\":58354},{\"end\":58386,\"start\":58372},{\"end\":58401,\"start\":58386},{\"end\":58623,\"start\":58609},{\"end\":58638,\"start\":58623},{\"end\":58656,\"start\":58638},{\"end\":58671,\"start\":58656},{\"end\":58901,\"start\":58887},{\"end\":58916,\"start\":58901},{\"end\":59144,\"start\":59130},{\"end\":59159,\"start\":59144},{\"end\":59171,\"start\":59159},{\"end\":59190,\"start\":59171},{\"end\":59208,\"start\":59190},{\"end\":59225,\"start\":59208},{\"end\":59514,\"start\":59501},{\"end\":59530,\"start\":59514},{\"end\":59546,\"start\":59530},{\"end\":59818,\"start\":59797},{\"end\":59833,\"start\":59818},{\"end\":60096,\"start\":60082},{\"end\":60106,\"start\":60096},{\"end\":60118,\"start\":60106},{\"end\":60126,\"start\":60118},{\"end\":60140,\"start\":60126},{\"end\":60154,\"start\":60140},{\"end\":60166,\"start\":60154},{\"end\":60175,\"start\":60166},{\"end\":60515,\"start\":60501},{\"end\":60529,\"start\":60515},{\"end\":60544,\"start\":60529},{\"end\":60560,\"start\":60544},{\"end\":60575,\"start\":60560},{\"end\":60918,\"start\":60908},{\"end\":60934,\"start\":60918},{\"end\":60949,\"start\":60934},{\"end\":60965,\"start\":60949},{\"end\":60985,\"start\":60965},{\"end\":60998,\"start\":60985},{\"end\":61363,\"start\":61350},{\"end\":61377,\"start\":61363},{\"end\":61390,\"start\":61377},{\"end\":61403,\"start\":61390},{\"end\":61711,\"start\":61699},{\"end\":61732,\"start\":61711},{\"end\":62021,\"start\":62007},{\"end\":62043,\"start\":62021},{\"end\":62055,\"start\":62043},{\"end\":62387,\"start\":62373},{\"end\":62395,\"start\":62387},{\"end\":62411,\"start\":62395},{\"end\":62746,\"start\":62734},{\"end\":62760,\"start\":62746},{\"end\":62764,\"start\":62760},{\"end\":63046,\"start\":63027},{\"end\":63062,\"start\":63046},{\"end\":63079,\"start\":63062},{\"end\":63092,\"start\":63079},{\"end\":63114,\"start\":63092},{\"end\":63129,\"start\":63114},{\"end\":63143,\"start\":63129},{\"end\":63154,\"start\":63143},{\"end\":63169,\"start\":63154},{\"end\":63198,\"start\":63169},{\"end\":63613,\"start\":63598},{\"end\":63625,\"start\":63613},{\"end\":63635,\"start\":63625},{\"end\":63643,\"start\":63635},{\"end\":63909,\"start\":63894},{\"end\":63924,\"start\":63909},{\"end\":63937,\"start\":63924},{\"end\":63952,\"start\":63937},{\"end\":63965,\"start\":63952},{\"end\":63982,\"start\":63965},{\"end\":64303,\"start\":64292},{\"end\":64324,\"start\":64303},{\"end\":64347,\"start\":64324},{\"end\":64365,\"start\":64347},{\"end\":64379,\"start\":64365},{\"end\":64721,\"start\":64708},{\"end\":64734,\"start\":64721},{\"end\":64746,\"start\":64734},{\"end\":65109,\"start\":65098},{\"end\":65126,\"start\":65109},{\"end\":65142,\"start\":65126},{\"end\":65459,\"start\":65447},{\"end\":65471,\"start\":65459},{\"end\":65487,\"start\":65471},{\"end\":65503,\"start\":65487},{\"end\":65517,\"start\":65503},{\"end\":65525,\"start\":65517},{\"end\":65544,\"start\":65525},{\"end\":65936,\"start\":65924},{\"end\":65950,\"start\":65936},{\"end\":66273,\"start\":66263},{\"end\":66284,\"start\":66273},{\"end\":66296,\"start\":66284},{\"end\":66309,\"start\":66296},{\"end\":66325,\"start\":66309},{\"end\":66332,\"start\":66325},{\"end\":66712,\"start\":66701},{\"end\":66726,\"start\":66712},{\"end\":66740,\"start\":66726},{\"end\":66750,\"start\":66740},{\"end\":67165,\"start\":67148},{\"end\":67179,\"start\":67165},{\"end\":67196,\"start\":67179},{\"end\":67655,\"start\":67639},{\"end\":67665,\"start\":67655},{\"end\":67681,\"start\":67665},{\"end\":67698,\"start\":67681},{\"end\":67714,\"start\":67698},{\"end\":68190,\"start\":68181},{\"end\":68197,\"start\":68190},{\"end\":68208,\"start\":68197},{\"end\":68219,\"start\":68208},{\"end\":68230,\"start\":68219},{\"end\":68245,\"start\":68230},{\"end\":68253,\"start\":68245},{\"end\":68774,\"start\":68760},{\"end\":68792,\"start\":68774},{\"end\":68808,\"start\":68792},{\"end\":69032,\"start\":69016},{\"end\":69061,\"start\":69032},{\"end\":69070,\"start\":69061},{\"end\":69080,\"start\":69070},{\"end\":69466,\"start\":69453},{\"end\":69479,\"start\":69466},{\"end\":69493,\"start\":69479},{\"end\":69509,\"start\":69493},{\"end\":69520,\"start\":69509},{\"end\":69828,\"start\":69815},{\"end\":69845,\"start\":69828},{\"end\":69859,\"start\":69845},{\"end\":69877,\"start\":69859},{\"end\":69893,\"start\":69877},{\"end\":70290,\"start\":70277},{\"end\":70305,\"start\":70290},{\"end\":70318,\"start\":70305},{\"end\":70334,\"start\":70318},{\"end\":70732,\"start\":70717},{\"end\":70747,\"start\":70732},{\"end\":70761,\"start\":70747},{\"end\":70773,\"start\":70761},{\"end\":70785,\"start\":70773},{\"end\":70798,\"start\":70785},{\"end\":70815,\"start\":70798},{\"end\":71237,\"start\":71228},{\"end\":71251,\"start\":71237},{\"end\":71266,\"start\":71251},{\"end\":71282,\"start\":71266},{\"end\":71288,\"start\":71282},{\"end\":71567,\"start\":71553},{\"end\":71587,\"start\":71567},{\"end\":71600,\"start\":71587},{\"end\":71861,\"start\":71844},{\"end\":71870,\"start\":71861},{\"end\":71883,\"start\":71870},{\"end\":72193,\"start\":72181},{\"end\":72218,\"start\":72193},{\"end\":72225,\"start\":72218},{\"end\":72546,\"start\":72531},{\"end\":72558,\"start\":72546},{\"end\":72579,\"start\":72558},{\"end\":72597,\"start\":72579},{\"end\":72613,\"start\":72597},{\"end\":72629,\"start\":72613},{\"end\":72648,\"start\":72629},{\"end\":72656,\"start\":72648},{\"end\":73099,\"start\":73089},{\"end\":73112,\"start\":73099},{\"end\":73127,\"start\":73112},{\"end\":73142,\"start\":73127},{\"end\":73162,\"start\":73142},{\"end\":73172,\"start\":73162},{\"end\":73491,\"start\":73478},{\"end\":73501,\"start\":73491},{\"end\":73510,\"start\":73501},{\"end\":73524,\"start\":73510},{\"end\":73534,\"start\":73524},{\"end\":73922,\"start\":73908},{\"end\":73936,\"start\":73922},{\"end\":73951,\"start\":73936},{\"end\":73966,\"start\":73951},{\"end\":74311,\"start\":74293},{\"end\":74324,\"start\":74311},{\"end\":74340,\"start\":74324},{\"end\":74352,\"start\":74340},{\"end\":74647,\"start\":74633},{\"end\":74666,\"start\":74647},{\"end\":74680,\"start\":74666},{\"end\":74697,\"start\":74680},{\"end\":74713,\"start\":74697},{\"end\":75094,\"start\":75085},{\"end\":75107,\"start\":75094},{\"end\":75117,\"start\":75107},{\"end\":75128,\"start\":75117},{\"end\":75139,\"start\":75128},{\"end\":75420,\"start\":75402},{\"end\":75682,\"start\":75666},{\"end\":75692,\"start\":75682},{\"end\":75971,\"start\":75959},{\"end\":75991,\"start\":75971},{\"end\":76003,\"start\":75991},{\"end\":76018,\"start\":76003},{\"end\":76359,\"start\":76345},{\"end\":76369,\"start\":76359},{\"end\":76676,\"start\":76655},{\"end\":76691,\"start\":76676},{\"end\":76704,\"start\":76691},{\"end\":76720,\"start\":76704},{\"end\":76730,\"start\":76720},{\"end\":76750,\"start\":76730},{\"end\":76763,\"start\":76750},{\"end\":77136,\"start\":77119},{\"end\":77151,\"start\":77136},{\"end\":77170,\"start\":77151},{\"end\":77183,\"start\":77170},{\"end\":77197,\"start\":77183},{\"end\":77217,\"start\":77197},{\"end\":77230,\"start\":77217},{\"end\":77246,\"start\":77230},{\"end\":77259,\"start\":77246},{\"end\":77276,\"start\":77259},{\"end\":77606,\"start\":77577},{\"end\":77621,\"start\":77606},{\"end\":77638,\"start\":77621},{\"end\":77656,\"start\":77638},{\"end\":77678,\"start\":77656},{\"end\":78041,\"start\":78028},{\"end\":78053,\"start\":78041},{\"end\":78069,\"start\":78053},{\"end\":78086,\"start\":78069},{\"end\":78101,\"start\":78086},{\"end\":78117,\"start\":78101},{\"end\":78141,\"start\":78117},{\"end\":78160,\"start\":78141},{\"end\":78173,\"start\":78160},{\"end\":78185,\"start\":78173},{\"end\":78592,\"start\":78577},{\"end\":78605,\"start\":78592},{\"end\":78623,\"start\":78605},{\"end\":78636,\"start\":78623},{\"end\":78945,\"start\":78930},{\"end\":78970,\"start\":78945},{\"end\":78979,\"start\":78970},{\"end\":78992,\"start\":78979},{\"end\":79373,\"start\":79363},{\"end\":79384,\"start\":79373},{\"end\":79403,\"start\":79384},{\"end\":79410,\"start\":79403},{\"end\":79748,\"start\":79732},{\"end\":79761,\"start\":79748},{\"end\":79774,\"start\":79761},{\"end\":79786,\"start\":79774},{\"end\":79798,\"start\":79786},{\"end\":79809,\"start\":79798},{\"end\":79826,\"start\":79809},{\"end\":80136,\"start\":80122},{\"end\":80152,\"start\":80136},{\"end\":80163,\"start\":80152},{\"end\":80180,\"start\":80163},{\"end\":80196,\"start\":80180},{\"end\":80493,\"start\":80478},{\"end\":80504,\"start\":80493},{\"end\":80515,\"start\":80504},{\"end\":80523,\"start\":80515},{\"end\":80536,\"start\":80523},{\"end\":80877,\"start\":80868},{\"end\":80888,\"start\":80877},{\"end\":80899,\"start\":80888},{\"end\":80910,\"start\":80899},{\"end\":80921,\"start\":80910},{\"end\":81231,\"start\":81218},{\"end\":81245,\"start\":81231},{\"end\":81579,\"start\":81565},{\"end\":81593,\"start\":81579},{\"end\":81602,\"start\":81593},{\"end\":81619,\"start\":81602},{\"end\":81627,\"start\":81619},{\"end\":81641,\"start\":81627},{\"end\":81977,\"start\":81965},{\"end\":81992,\"start\":81977},{\"end\":82005,\"start\":81992},{\"end\":82018,\"start\":82005},{\"end\":82031,\"start\":82018},{\"end\":82047,\"start\":82031},{\"end\":82064,\"start\":82047},{\"end\":82390,\"start\":82375},{\"end\":82411,\"start\":82390},{\"end\":82424,\"start\":82411},{\"end\":82689,\"start\":82675},{\"end\":82704,\"start\":82689},{\"end\":82718,\"start\":82704},{\"end\":82732,\"start\":82718},{\"end\":83092,\"start\":83079},{\"end\":83103,\"start\":83092},{\"end\":83118,\"start\":83103}]", "bib_venue": "[{\"end\":63279,\"start\":63247},{\"end\":75484,\"start\":75466},{\"end\":48276,\"start\":48233},{\"end\":48780,\"start\":48715},{\"end\":49247,\"start\":49188},{\"end\":49733,\"start\":49668},{\"end\":50147,\"start\":50113},{\"end\":50525,\"start\":50480},{\"end\":50936,\"start\":50887},{\"end\":51312,\"start\":51269},{\"end\":51615,\"start\":51556},{\"end\":51958,\"start\":51893},{\"end\":52358,\"start\":52315},{\"end\":52745,\"start\":52702},{\"end\":53151,\"start\":53086},{\"end\":53452,\"start\":53391},{\"end\":53973,\"start\":53918},{\"end\":54454,\"start\":54411},{\"end\":54826,\"start\":54767},{\"end\":55224,\"start\":55159},{\"end\":55567,\"start\":55512},{\"end\":55882,\"start\":55836},{\"end\":56257,\"start\":56192},{\"end\":56642,\"start\":56577},{\"end\":57016,\"start\":56973},{\"end\":57397,\"start\":57354},{\"end\":57767,\"start\":57657},{\"end\":58146,\"start\":58101},{\"end\":58444,\"start\":58401},{\"end\":58705,\"start\":58671},{\"end\":58950,\"start\":58916},{\"end\":59256,\"start\":59225},{\"end\":59589,\"start\":59546},{\"end\":59870,\"start\":59833},{\"end\":60240,\"start\":60175},{\"end\":60640,\"start\":60575},{\"end\":61063,\"start\":60998},{\"end\":61455,\"start\":61403},{\"end\":61797,\"start\":61732},{\"end\":62117,\"start\":62055},{\"end\":62473,\"start\":62411},{\"end\":62823,\"start\":62764},{\"end\":63245,\"start\":63198},{\"end\":63698,\"start\":63659},{\"end\":64027,\"start\":63982},{\"end\":64422,\"start\":64379},{\"end\":64780,\"start\":64746},{\"end\":65193,\"start\":65142},{\"end\":65609,\"start\":65544},{\"end\":66015,\"start\":65950},{\"end\":66397,\"start\":66332},{\"end\":66812,\"start\":66750},{\"end\":67261,\"start\":67196},{\"end\":67779,\"start\":67714},{\"end\":68348,\"start\":68269},{\"end\":68855,\"start\":68808},{\"end\":69168,\"start\":69096},{\"end\":69585,\"start\":69520},{\"end\":70004,\"start\":69909},{\"end\":70399,\"start\":70334},{\"end\":70880,\"start\":70815},{\"end\":71328,\"start\":71288},{\"end\":71640,\"start\":71600},{\"end\":71945,\"start\":71883},{\"end\":72290,\"start\":72225},{\"end\":72721,\"start\":72656},{\"end\":73237,\"start\":73172},{\"end\":73636,\"start\":73550},{\"end\":74031,\"start\":73966},{\"end\":74397,\"start\":74352},{\"end\":74791,\"start\":74713},{\"end\":75199,\"start\":75139},{\"end\":75464,\"start\":75420},{\"end\":75751,\"start\":75692},{\"end\":76077,\"start\":76018},{\"end\":76421,\"start\":76369},{\"end\":76828,\"start\":76763},{\"end\":77310,\"start\":77276},{\"end\":77746,\"start\":77694},{\"end\":78219,\"start\":78185},{\"end\":78679,\"start\":78636},{\"end\":79057,\"start\":78992},{\"end\":79475,\"start\":79410},{\"end\":79885,\"start\":79826},{\"end\":80239,\"start\":80196},{\"end\":80601,\"start\":80536},{\"end\":80966,\"start\":80921},{\"end\":81310,\"start\":81245},{\"end\":81684,\"start\":81641},{\"end\":82104,\"start\":82064},{\"end\":82469,\"start\":82424},{\"end\":82797,\"start\":82732},{\"end\":83163,\"start\":83118}]"}}}, "year": 2023, "month": 12, "day": 17}