{"id": 258947586, "updated": "2023-10-05 00:17:30.034", "metadata": {"title": "Too Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization", "authors": "[{\"first\":\"Agnieszka\",\"last\":\"Ciborowska\",\"middle\":[]},{\"first\":\"Kostadin\",\"last\":\"Damevski\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Modern Deep Learning (DL) architectures based on transformers (e.g., BERT, RoBERTa) are exhibiting performance improvements across a number of natural language tasks. While such DL models have shown tremendous potential for use in software engineering applications, they are often hampered by insufficient training data. Particularly constrained are applications that require project-specific data, such as bug localization, which aims at recommending code to fix a newly submitted bug report. Deep learning models for bug localization require a substantial training set of fixed bug reports, which are at a limited quantity even in popular and actively developed software projects. In this paper, we examine the effect of using synthetic training data on transformer-based DL models that perform a more complex variant of bug localization, which has the goal of retrieving bug-inducing changesets for each bug report. To generate high-quality synthetic data, we propose novel data augmentation operators that act on different constituent components of bug reports. We also describe a data balancing strategy that aims to create a corpus of augmented bug reports that better reflects the entire source code base, because existing bug reports used as training data usually reference a small part of the code base.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.16430", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-16430", "doi": "10.48550/arxiv.2305.16430"}}, "content": {"source": {"pdf_hash": "be3ce39e7840db0ab4dcae8742dc51a701398340", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.16430v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e613d4680a5e756d11664018dcb8aa6ef06b7899", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/be3ce39e7840db0ab4dcae8742dc51a701398340.txt", "contents": "\nToo Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization\n\n\nAgnieszka Ciborowska \nVirginia Commonwealth University Richmond\nVirginiaUSA\n\nKostadin Damevski kdamevski@vcu.edu \nVirginia Commonwealth University Richmond\nVirginiaUSA\n\nToo Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization\n10.1145/nnnnnnn.nnnnnnn\nModern Deep Learning (DL) architectures based on transformers (e.g., BERT, RoBERTa) are exhibiting performance improvements across a number of natural language tasks. While such DL models have shown tremendous potential for use in software engineering applications, they are often hampered by insufficient training data. Particularly constrained are applications that require projectspecific data, such as bug localization, which aims at recommending code to fix a newly submitted bug report. Deep learning models for bug localization require a substantial training set of fixed bug reports, which are at a limited quantity even in popular and actively developed software projects. In this paper, we examine the effect of using synthetic training data on transformer-based DL models that perform a more complex variant of bug localization, which has the goal of retrieving bug-inducing changesets for each bug report. To generate high-quality synthetic data, we propose novel data augmentation operators that act on different constituent components of bug reports. We also describe a data balancing strategy that aims to create a corpus of augmented bug reports that better reflects the entire source code base, because existing bug reports used as training data usually reference a small part of the code base. Data balancing helps the model perform better for newlyreported bug reports that reference previously unobserved code. Our evaluation results indicate that both data augmentation and balancing are effective, improving retrieval performance across all three BERT-based models we studied.\n\nINTRODUCTION\n\nThe emergence of novel Deep Learning (DL) architectures, such as transformers, has fueled outstanding improvements across multiple tasks in Natural Language Processing (NLP), and encouraged their application to various problems in the software engineering domain. Software engineering researchers have studied the potential of DL Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. in the context of problems such as code search [13,14,21,29], defect prediction [17,28,37,55,63], and bug localization [6,19,20,26,62]. However, the fundamental weakness of DL approaches is that they require large amount of labelled data to train the model. At the same time, maintaining the quality of the labelled data is crucial to achieve the best performance. While manual labelling is typically a preferred approach to ensure high data quality, it is a slow and time-consuming process [51], often intractable considering the amount of data required to train a DL model. On the other hand, automated mining for labels is far more likely to meet the demand for data quantity, however at the cost of introducing noise in the form of both false positives and false negatives [8,53]. Hence, collecting large amount of good quality labelled data can pose a significant challenge for many important software engineering problems and tasks, in particular those that require single project data (i.e., within project) [50]. A recent approach to address this problem is to use transfer learning, i.e., pre-training a model with unsupervised learning on a large, general corpus, followed by fine-tuning via supervised learning towards the downstream task. However, this strategy still requires a non-trivial dataset for fine-tuning and, as observed by Gururangan et al., it leads to suboptimal performance compared to when a model is pre-trained and fine tuned on in-domain data [16].\n\nOne of the software engineering tasks that benefits from a DLbased approach is bug localization, which aims to identify relevant code entities (e.g., classes, methods or changesets) for a given bug report describing a software failure. Over the years, researchers have proposed multiple approaches for bug localization based on the Vector Space Model (VSM) [43,56,58] and probabilistic models (e.g., Latent Dirichlet Allocation) [7,33], while also recognizing that the key drawback of those techniques is their limited ability to deal with the semantic gap between source code causing the bug and the description given in the bug report [1,64]. To address that, recent efforts have been diverted towards DL techniques, including RNN, LSTM and, finally, transformer-based models [6,29]. As noted by Guo et al. [14], the availability of training data is one of the key factors limiting DL performance. In the case of bug localization, the training data consists of pairs of bug reports and their introducing (or inducing) changesets, which are difficult to obtain at scale for a couple of key reasons. First, matching a bug report to bug-introducing changesets is challenging as developers rarely mark culprit code changes explicitly [30], while approaches that find the bug-inducing changesets automatically, based on the SZZ algorithm [46], are prone to introducing noise [31,42]. Second, the number of positive samples is bounded by the number of fixed bug reports, which are limited even for large and actively maintained projects. Relatively smaller software projects with, e.g., dozens of fixed bug reports, would be very difficult to use. In the end, the main question remains open: how to leverage DL techniques for bug localization, given the paucity of project-specific data.\n\nIn the NLP domain, this question has been answered with some success by Data Augmentation (DA) techniques, which, in general, can be described as strategies to artificially increase the number and diversity of training samples based on the currently available data [11]. DA aims to create high quality synthetic data by applying transformations to the available data, while maintaining label invariance. As a result, the size of the original dataset increases, which in turn enables training a DL model for low resource domains and tasks.\n\nEncouraged by recent advances of DA in the NLP domain, in this work, we aim to explore data augmentation for bug reports with the goal of producing a large number of high quality, realistic, synthetic bug reports, which can be subsequently used to increase the size of the training set for a bug localization DL model. To this end, we propose two sets of DA operators that independently target natural language text and code-related data (e.g., code tokens, stack traces and code snippets) in each bug report. More specifically, natural language text is augmented using token-and paragraph-level transformations (e.g., synonym inserts), while the code-related data is augmented with code tokens from its respective bug-inducing changesets in order to strengthen the connection between a bug report and different portions of its introducing changeset. At the same time, by leveraging the augmented bug reports we plan to achieve another important goal, balancing the augmented dataset toward parts of the source code underrepresented in the original training set. This addresses the common occurrence in software projects that existing bug reports reference only a specific part of the code base, while other parts have few or no bug reports, leading to the bug localization model overly focusing only on a part of the code base. In this paper, we investigate the following Research Questions (RQs): RQ1: (a) Can Data Augmentation improve the retrieval performance of DL-based bug localization? (b) How does Data Augmentation impact the performance of different DL-based bug localization approaches?\n\nTo understand whether Data Augmentation is a relevant strategy in DL-based bug localization, we identify three recent transformerbased models to perform this task. We evaluate the performance of these bug localization approaches, with and without DA, using a standard bug localization dataset and metrics commonly used to measure information retrieval performance. As augmentation necessarily introduces significantly higher data quantity, we add baselines to the evaluation that aim to differentiate the quantity vs. the quality of the augmented dataset. The results indicate that (1) the proposed data augmentation strategy improves retrieval accuracy by between 39% and 82%, (2) augmenting the dataset is more beneficial than increasing the size of the training dataset by repetition; and (3) balancing the training dataset results in improvement in retrieval performance, but the magnitude of the improvement depends on the architecture of the DL model.\n\n\nRQ2\n\n: Which of the proposed DA operators contribute the most to retrieval performance?\n\nThe Data Augmentation approach in RQ1 relies on augmentation operators that perform specific types of transformations (e.g., insert, remove). In RQ2, we aim to understand the impact of these augmentation operators on the retrieval performance during bug localization. To answer RQ2, we perform ablation studies, training each DL model with augmented datasets created using all but one augmentation operator type. The results indicate that most of the operators contribute to the final performance, while certain operators are more consistent across different DL models.\n\n\nDATA AUGMENTATION FOR BUG LOCALIZATION\n\nWith the increasing complexity of DL-based methods for bug localization [6,15,20,26], the problem of data scarcity comes to the forefront. More specifically, while the more advanced models have the potential to bridge the lexical gap between a bug report and source code [1,64], in order to fulfill that promise, they require large amount of bug reports to learn the semantics of the project and subsequently associate it with bug-inducing changesets. Insufficient amount of training examples may lead to model overfitting, memorizing high-frequency patterns or structures instead of generalizing the knowledge [45]. DA can help to address the data scarcity problem in bug localization by focusing on the following goals.\n\n1. Increasing the number of bug reports. Training a DL model for bug localization requires a substantial dataset consisting of bug reports and bug-inducing changesets. The main challenge of constructing such dataset is that it is project-specific. Most software projects typically have few bug reports with a clear indication of the changesets that caused them [30]. Moreover, the total number of bug reports in a project is an upper bound on the number of positive training instances that are available. Note that while we can create numerous negative instances (i.e., a bug report and a non-bug-inducing changeset), the benefit to the DL model is limited as the bug report remains the same in each instance. Moreover, out of all the reported bugs, some are closed with Won't fix or Not a Bug status [24,59], hence they do not have corresponding changesets and cannot be used for training. To empirically verify the scale of data scarcity problem in bug localization data, we examined Bench4BL [27], a large bug localization dataset. Bench4BL includes 10K bug reports and their fixes coming from 51 popular and actively developed open source software projects, which equals to roughly 200 bug reports per project. Considering that the projects in the Bench4BL dataset are typically large and well-established (e.g., long running Apache Software Foundation projects like Camel and Hive), 200 bug reports is a discouragingly low number when it comes to ability to train an effective DL model. 2. Maintaining label invariance of bug reports. In NLP, data augmentation is primarily evaluated on classification tasks, such as sentiment analysis or topic classification, in which rarely a single word can be representative of the overall result (i.e., a sentiment or a topic). Data in software engineering is a mix of natural language and code-related segments. In case of bug localization, this mix typically affects bug reports, which often contain not only natural language description but also mentions of relevant program elements, stack traces or code snippets [1]. Applying off-the-shelf data augmentation transformations to bug localization data may \u2717 cause more harm than good as it does not differentiate between NL and code, which both bring useful information, but in different forms and quantities. Table 1 shows examples of textual augmentation performed on the summary of bug report #55996 from the Tomcat project using two augmentation operators proposed by Wei et al. [57]. Random Swap exchanges two randomly selected words, while Synonym Replacement substitutes a randomly selected word with its synonym. To find synonyms, we use BERTOverflow [49], a BERT model pre-trained on the StackOverflow corpus. Given the randomness of data augmentation operations, we see different versions of augmented bug report summary. While Random Swap 1 swaps two words without affecting the semantics, Random Swap 2 exchanges words that can indicate the relevant code component, if a project contains AsyncContext and AsyncConnector classes. Similarly, in the case of Synonym Replacement 1 changing context to session affects the semantics less than replacing Async with TCP which are different concepts. This toy-example shows how easily off-the-shelf data augmentation can introduce noise that affects the original label, especially when handling data that contains key software engineering-related phrases. Hence augmentation of software engineering data in general, and bug reports in particular, requires additional steps to ensure the invariance of the newly generated data points.\n\n3. Diversifying the training data. The goal of data diversification in DA is to ensure that augmented data introduces \"new quality\" to a training set, such as previously unobserved motifs, patterns or expressions, leading a DL model to learn the meaning behind the data instead of memorizing certain forms [34,45]. In the case of bug localization, the training dataset depicts how natural language describing a bug connects to source code concepts in the bug-inducing changeset. Commonly, the natural language in bug reports consists of Observed Behavior (OB), Expected Behavior (EB), or Steps to Reproduce (S2R) [3]. Given that OB, EB and S2R have been recognized by developers as useful information when fixing a bug [1], augmentation for bug localization data should focus on introducing diversity into those through, e.g., paraphrasing their sentences. The second important component of diversification of a bug localization training set are the connections between bug reports and source code. While it is true that bugs are not evenly distributed in the source code base, the over-representation of one source code component (e.g., class, package) in the training set, may lead to the model blaming that particular component for every bug.\n\nTo account for that, while augmenting training set for bug localization, additional steps can be taken to mitigate that risk, through, e.g., creating more augmented bug reports for those source code components that occur less often in the training set. In summary, the diversification of training data should focus on: (1) modifying the natural language content of a bug report, and (2) diversifying how a bug report connects to the source code.\n\n\nAPPROACH\n\nTo create augmented bug reports that introduce diversity and preserve invariance (i.e., the augmented bug report still matches the same changeset as the original bug report), we propose a set of custom DA operators. Bug reports describe software failure using various types of information, such as natural language, code snippets or stack traces, which may have different impact on matching a bug report to its inducing changeset, hence we decided to separately augment natural language and code-related information to ensure invariance of the newly created data points and avoid introducing noise. Figure 1 illustrates the workflow of our data augmentation process which starts with data extraction and preprocessing, followed by augmentation with the proposed operators, and construction of augmented bug reports combining the newly generated data.\n\n\nData preprocessing\n\nAs a first step, we use infozilla [39], a tool that extracts stack traces and code snippets from unstructured bug report content, leaving the remaining text broadly categorized as natural language. The infozilla produces minimal error as experiments have shown it to have 97%+ precision, 95%+ recall and 97%+ accuracy. To bring out further structure from the natural language data, we extract Observed Behavior (OB), Expected Behavior (EB), and Steps to Reproduce (S2R) using the BEE tool [48], which has shown to be highly effective at this task (94%+ accuracy, 87%+ recall, 70%+ precision). Stack traces are a valuable source of localization hints, however, due to their length they tend to introduce noise through multiple mentions of classes not necessarily related to a particular bug report [6,40]. To mitigate the noise in stack traces, we reduce their size by selecting the lines that are most likely to contain relevant information. For instance, for Java stack traces this leads to three groups: 1) top lines, which include the exception name and where the exception originated; 2) middle lines, which occur after the Java standard library traces and are most likely last lines of the application code closest to the bug; and 3) bottom lines, which can be useful for exceptions thrown from threads. Sampling from these three groups creates a generic recipe that shortens the stack trace, captures different software designs, and preserves important information. Hence, for each stack trace, we decided to keep top 1 line, first 3 lines that refer to the application code, and bottom 1 line. Heuristic approaches such as this one have been reported to perform reasonably well even on unstructured runtime data (e.g., raw crash logs with multiple stack traces, possibly from different programming languages) [38].\n\nFor preprocessing code snippets, we decided to filter out punctuation for two reasons. First, in a recently published study, Paltenghi et al. [36] compared the reasoning of developers and neural models, and observed that the models pay more attention to syntactic tokens (e.g., dots, periods, brackets), while developers focus more on strings or keywords. Given that developers perform better, DL models should mimic developers and put less attention to syntactic tokens. The second reason for filtering punctuation is pragmaticreducing the number of tokens to prevent exceeding the input limit size of the DL models. Following preprocessing, each bug report is represented as a collection of OB, EB, S2R, stack traces, and code snippets.\n\n\nNatural language DA operators\n\nThis group of operators is applied to OB, EB, and S2R due to their primarily natural language content. We propose to use two types of operators: token-level and paragraph-level. Inspired by a simple yet effective technique called Easy Data Augmentation (EDA) [57], we propose to use 4 token-level operators.\n\n\u2022 Dictionary Replace -randomly selects a word from a predefined in-domain dictionary and replaces the word with its substitute. \u2022 Dictionary Insert -works similarly to Dictionary Replace, however instead of replacing the word, this operator inserts the substitute at a random position in the text. \u2022 Random Swap -randomly selects two words and swaps them. \u2022 Random Delete -removes a randomly selected word.\n\nTo build the in-domain dictionary for augmenting OB, EB and S2R, we use keywords from language patterns devised by Chaparro et al. [4]. The patterns specify combinations of different parts of speech with certain keywords that have to occur to classify a sentence or a paragraph as OB, EB or S2R. For instance, one of the most popular OB patterns is NEG_VERB defined as:\n(subject/noun phrase) ([adjective/adverb]) [negative verb] ([complement]),\nwhere the negative verbs are defined as: affect, break, block, close, etc. The in-domain dictionary contains all keywords identified by Chapparo et al. and maps each keyword to its substitutes, e.g., affect \u2192 {break, block, close, ...}. Domain knowledge guided operators have been recently shown to lead to better performance compared to more advanced but general approaches (e.g., embeddings) [25].\n\nAs a paragraph-level operator, we use Backtranslation to translate paragraphs of OB, EB or S2R from English to German and back to English [32]. Backtranslation is a popular data augmentation operation that allows to paraphrase the original text.\n\nFinally, let us describe how those operators are applied together to generate augmented data. For each bug report and for each OB, EB, S2R, we apply all token level operators times, where = * # . The operators are applied in the following order: replace, insert, swap and delete. The value of is set to 0.1 for insert, replace, and swap operations, and 0.05 for delete operation as these parameters have been empirically shown to produce best results [66]. Next, the Backtranslation operator is applied to paraphrase the modified text. Given the randomness of the augmentation operators, the quality of the augmented sample may vary. While, in general, natural language text typically retains its semantics when minor noise is introduced, bug reports are more structured type of data with certain keywords (e.g., code names), whose removal may have severe consequences for mapping the bug report to the source code. Hence, as a final step, we employ quality control that consists of two steps. First, we check if OB, EB and/or S2R can be still identified in the augmented paragraph using the BEE tool. For instance, if the original paragraph contained OB and EB, then the augmented version must contain OB and EB be to considered a valid paragraph. We also disallow changing the pattern (e.g., from OB to EB). Second, we ensure that no code tokens are lost during the augmentation by comparing the number of code tokens between the augmented and the original paragraph.\n\n\nCode-related DA operators\n\nIn the context of this paper, code-related data refers to stack traces, code snippets, and code tokens present in natural language text. To augment code-related data, we propose 3 code token operators that are more strict versions of the natural language operators to minimize the risk of distorting the context.\n\n\u2022 Code Token Replace -randomly selects a code token and replaces it with its substitute. \u2022 Code Token Insert -randomly selects a code token, and insert a substitute of that code token at a random position that is at most 3 positions away from the selected code token. \u2022 Code Token Swap -swaps two randomly selected code tokens, such that (1) for stack traces code tokens can be swapped only between consecutive stack lines; (2) for code snippets, a swap operation must be performed within the surrounding 3 tokens to minimize the potential distortion of the bug report's semantics. Changing this to a larger limit would result in more diverse samples being created, however, with a higher chance of disturbing the bug report's semantics.. We decided against including a code token deletion operator as removing code tokens is more likely to disturb the invariance of augmented samples. To find substitutes for a code token, first, for each bug report we build a dictionary of code names using class and method names that occur in its corresponding bug-inducing changesets. Next, we use the Levenshtein distance to measure the distance between the selected code token and all other tokens in the dictionary. Levenshtein distance is a string similarity metric that quantifies similarity through the number of edits required to convert one string into another for strings of arbitrary length. We empirically observed that the most similar code tokens are often of the same form, and hence, may introduce very limited diversity into the augmented samples. For instance, consider the code token word with the top-3 closest tokens is_word, set_word, get_word, and the top-20 token check_word_missing_letter. To allow for more diverse augmentation, based on empirical observations for one of the evaluation projects, we opted for a less conservative top-selection, with set to 20. Hence, a code token substitute is selected randomly from the 20 code tokens that have the lowest Levenshtein distance from the given code token.\n\n\nBuilding augmented bug reports\n\nAfter augmentation, each bug report is decomposed into a collection of the original and augmented samples, i.e., natural language data (OB, EB, S2R), and code-related data (stack traces and code snippets). The remaining question is how to build a synthetic bug report out of all the available samples. Recent work in neural machine translation has shown that concatenating augmented samples introduces structural diversity that prevents a DL model from learning to focus only on one part of the input, thus leading to a strong improvement in the model's performance [34,61]. We propose to use a similar approach to build augmented bug reports. More specifically, first we recreate the original structure of a bug report by concatenating augmented samples. Next, samples are reordered and at most 1 sample can be dropped to achieve further structural diversity. While dropping parts of bug reports may seem counterintuitive, DA strategies that remove tokens or sentences has been observed to have a positive impact on large pre-trained DL models [5,44]. Figure 2 shows bug report #55171 from the Tomcat project and its augmented version. Each part of the bug report has been augmented separately using all of its respective DA operators.\n\nIn the case of natural language, we see semantically correct insertions and replacements (e.g., blocked \u2192 dead), while the paragraph rephrasing performed by Backtranslation is less precise yet still conveys the main message (e.g., the second OB in the figure). Code augmentation for this bug report includes augmenting stack trace and code tokens with code names from the bug inducing changeset. While such an approach inherently limits the possibility to add noise, it also introduces information about code components that are related to the bug report. This, in turn allows the model to learn these relations and utilize them during inference. Finally, When constructing the augmented bug report, the second OB and the stack trace have been swapped, while the third OB has been dropped, creating the final augmented version of bug report #55171.\n\n\nEnsuring a balanced augmented dataset\n\nTo increase the size of the bug localization training set with data augmentation, our approach is to focus on augmenting bug reports, increasing the number of pairs of bug reports and bug-inducing hunks. Recent studies show that using hunks, a set of consecutive line modifications that capture changes in one area of the file, produces improved retrieval results compared to using entire changesets [6,58]. Hence, in this work, we build bug localization training set using pairs of bug reports and hunks extracted from bug-inducing changesets. Bugs affect different parts of source code base with varying frequency [2]. In other words, parts of the source code (i.e., specific files or classes) are related to multiple bug reports and therefore their hunks can also be overrepresented in the original dataset. For instance, given a bug report with introducing hunks, data augmentation by a factor of 10 creates 10 new bug reports for each hunk, which leads to 10 new training samples. However, there is one major drawback to this DA approach. This data imbalance, created by the uneven distribution of bug reports and hunks in the training set, can be exasperated by DA, with strong downstream effects on the DL model and its prediction.\n\nTo provide further evidence, we empirically checked the dataset published by Wen et al. [58], which we use in our study. Figure 3 shows the distribution of bug reports (Fig. 3a) and class occurrences     . 3b) for the Tomcat project. The distributions shows how many samples in the training dataset refer to a specific bug report (or a class), where a bug report (or a class) has as many occurrences as the number of hunks. We show these distributions for three different choices of training datasets: the original unaugmented dataset, a 10x augmented dataset, and an artificially balanced dataset. Within the plots, there are zoomed-in versions to increase readability at the smaller scale. In the plot for the original training set (i.e., the blue line), we observe that 11 out of 97 bug reports cover over 50% (1432 out of 2812) of the training samples. In other words, 50% of the samples in the training set refer to 11 bug reports, since these bug reports have multiple introducing hunks, which translates to multiple entries in the training set. At the same time, 39 bug reports occur less than 10 times. Similarly, out of the unique 110 classes that introduced a bug, the top 10 classes with most frequently occurring hunks cover 34.5% (2586 out of 7478) of all training data. This imbalance in the training data can have two potential consequences for supervised training of a DL model. First, the model is more likely to learn the structure and semantics of bug reports that have a large number of bug inducing hunks, while neglecting less frequent bug reports. Secondly, classes that occur the most in the training set are more likely to be selected as bug-inducing by the trained model since they were often seen during training as bug-inducing. The issue of data imbalance has also been recognized in defect prediction datasets [65].\n\nHow augmentation exacerbates the problem of uneven data distribution can be observed in Figure 3, where the orange dotted line depicts the data distributions in a dataset that was augmented by a factor of 10. The majority bug reports and classes become even more dominant in the augmented dataset, making the data imbalance problem more severe than in the original dataset. To mitigate this problem, we propose a data balancing strategy that deliberately chooses samples to augment in order to smooth out the distributions of bug reports with respect to the source code. There are two main concerns that a data balancing strategy has to consider: (1) increasing the number of training samples for infrequent bug reports, and (2) ensuring that the number of samples with a given class does not dominate the dataset. To illustrate the need for these strategies, consider a bug report 1 with 20 hunks from different classes, and a bug report 2 with one hunk from class . If the balancing strategy is focused only on the distribution of bug reports, then it creates 20 augmented samples for 2 , every time using the hunk from class , hence is likely to be overrepresented in the training set. To address this, we introduce two augmentation factors and . While influences the number of times each bug report is augmented, restricts how many times each class can be repeated in the augmented dataset.\n\n\nAlgorithm 1: Data balancing with augmented bug reports\n\nInput :\n\n-training dataset; -augmentation factor; -balancing factor Output :\n\n-balanced training dataset The sequence of steps for the proposed data balancing augmentation strategy is presented in Algorithm 1. In lines 1-2, we compute a limit for bug reports and classes based on factors and and the maximum number of times a unique bug report and class is present in the original training dataset. Line 3 copies the existing data instances into the balanced dataset . For each bug report that occurs below the limit, the algorithm augments the bug report (line 6), and selects a bug-inducing hunk from a class that occurs less than times in (lines 7-8), creating a new training sample. The algorithm continues to add new samples for a bug report until (1) the limit is reached, or (2) bug-inducing hunks from all the classes have reached . The result of this balancing strategy is depicted in the green line in Figure 3, using values of = 0.7 and = 1.0. Compared to the augmented dataset, the data distribution of the balanced dataset is obviously smoother, with a much more even representation of the source code.\n\n\nEXPERIMENTAL EVALUATION 4.1 Dataset and metrics\n\nTo evaluate, we require a dataset that contains bug reports and bug-inducing changesets. We use a dataset published by Wen et al. [58] that contains manually-validated data from 6 open source software projects: AspectJ, JDT, PDE, SWT, Tomcat, and ZXing. Given that infozilla requires new lines to extract code snippets and  [46], which identifies a changeset as bug-inducing if it shares any file modifications with bug fixing changeset. While a bug-inducing changeset may include modifications of multiple files, only a few of those may be relevant to a bug (as indicated by bug fixing changeset). Hence, to ensure the quality of the training samples, we only include bug inducing hunks that refer to classes that also occurs in the bug fixing commit. For each positive sample, we create a negative sample by randomly selecting a hunk from a class which does not belong to the inducing changeset. After completing this step, for each project we obtain our baseline dataset, . The descriptive statistics of training and testing datasets used in this study are shown in Table 2. Note that the last column, # hunks, denotes the number of all hunks that are examined by the model during retrieval.\n\nTo evaluate the retrieval performance of the DL models trained on different datasets, we use the following metrics. Mean Reciprocal Rank: MRR measures the retrieval accuracy using the reciprocal ranks of first relevant changeset in the ranking averaged across all bug reports. The higher the value of MRR is, the closer the bug-inducing changeset is to the top of the ranking.\n= 1 | | | | \u2211\ufe01 =1 1 1 .\nMean Average Precision: MAP quantifies the ability of a model to retrieve all relevant changesets for a given bug report. MAP is calculated as the mean of Average Precision scores across all bug reports, where an Average Precision for a bug report is based on ranks of all relevant changesets in the ranking. The higher the values of MAP, the more relevant changesets is located in the top of the ranking.\n= 1 | | | | \u2211\ufe01 =1 1\n.\n\nPrecision@K: P@K measures how many of the top-changesets in the ranking are relevant to a bug report. The higher the value of P@K, the more relevant changesets can be found in top-positions.\n@ = 1 | | | | \u2211\ufe01 =1 | | .\n\nDL models\n\nTo evaluate the impact of the proposed data augmentation and balancing strategies on the retrieval performance, we train and evaluate three BERT-based [10] code retrieval architectures.\n\nTBERT-Single [9,29,35] is the most straightforward approach for information retrieval with BERT. The model concatenates a bug report and a hunk, and processes it through BERT and a pooling layer to obtain a fused vector representation, which is subsequently passed to the classification head to obtain a relevancy score. While this model typically provides high retrieval accuracy, it also incurs significant retrieval delay, since a bug report needs to be compared with all hunks available in a project. TBERT-Siamese [29,41] processes a bug report and a hunk sequentially through BERT and a pooling layer, creating two features vectors, that are subsequently concatenated and passed to the classification layer to produce the relevancy score. The key difference between TBERT-Single and TBERT-Siamese is in the opportunity to perform offline encoding of feature vectors for hunks, hence reducing the retrieval delay. FBL-BERT [6,22] is a recently proposed BERT-based architecture that enables rapid retrieval across large collection of documents (i.e., hunks). Unlike TBERTs, which flattens the embedding matrix to a vector to make a prediction, FBL-BERT leverages the full embedding matrix and calculates relevancy score between a bug report and a hunk as a sum of maximum vector similarities between word embeddings of the bug report and hunk. This, in turn, allows to use efficient vector similarity search algorithms to find the most similar hunks and only re-rank those with FBL-BERT, hence significantly reducing the retrieval time per bug report. Given that FBL-BERT leverages fine-grained token-to-token embeddings matching, the model is more likely to better utilize relevant keywords if they occur in the bug report. While all the models are based on BERT, their architectures differ in a few aspects. TBERT-Single concatenates a bug report and a changeset, processing them jointly through BERT, followed by a classification layer. TBERT-Siamese and FBL-BERT first use BERT to encode a bug report and a changeset separately, which results in two embedding matrices. The main difference between TBERT-Siamese and FBL-BERT is how they handle these matrices. TBERT-Siamese aggregates each matrix into a vector using a pooling operation, and, next, compares the embedding vector of a bug report with a changeset embedding vector. On the other hand, FBL-BERT uses both matrices to compute the relevancy score taking into account the embedding of each word. \n\n\nEvaluation setup\n\nWe performed the experiments on a server with Dual 12-core 3.2GHz Intel Xeon and 1 NVIDIA Tesla V100 with 32GB RAM memory running CUDA v.11.4. The models are implemented with PyTorch v.1.7.1, HuggingFace library v.4.3.2, and Faiss v.1.6.5 with GPU support. We opted for using BERTOverflow [49] as our pretrained base BERT model, since, similarly to our data, StackOverflow data is also a mixture of code and natural language. All models are fine tuned for 4 epochs, using a batch size of 16 and Adam (abbreviated from adaptive moment estimation) optimizer [23] with learning rate set to 3e-6 [10]. Based on the average number of tokens in bug reports and hunks in our dataset, we set the input size limit to 256 and 512 tokens for bug reports and hunks respectively. All input documents are padded or truncated with respect to their input size limit.\n\n\nRESULTS\n\n5.1 RQ1: (a) Can Data Augmentation improve the retrieval performance of DL-based bug localization? (b) How does Data Augmentation impact the performance of different DL-based bug localization approaches?\n\nSetup. To evaluate the impact of DA on DL-based models, we compare the retrieval accuracy when training on the original, unaugmented dataset, , to training with augmented and balanced data. More specifically, for each project we construct the five augmented datasets shown in Table 3.\n\nis an augmented, but unbalanced, dataset that contains 10 additional samples for each pair of bug report and hunk, while , = 1, 2, 3, 4, are balanced datasets with different choices for ( , ) = {(0.7, 1.0); (0.85, 2.0); (1.0, 2.0); (1.3, 2.0)} respectively. The rationale for these specific values of and is to explore (in ranges that do not generate more data than we can manage computationally), while selecting values for that do not constrain 's effect. Given that augmentation increases the number of positive samples, the number of negative samples grows proportionally as well (i.e., for each positive sample, we randomly create one negative sample). To ensure that the difference in performance is in fact the result of DA, and not the diversity introduced by a new negative samples, we created an additional baseline, , that repeats positive samples without augmentation 10 times, and, correspondingly, also adds 10 new negative samples. In effect, the only difference between and is the fact that uses augmented bug reports while repeats the positive samples. All models are evaluated on the same (unaugmented) test set. Since TBERT-Single requires considerably more time than the other models (e.g., TBERT-Single takes more than 24h to run on the JDT project), we only evaluate it on one of the balanced datasets -1as it exhibits the best performance for TBERT-Siamese, which uses a relatively similar DL architecture to TBERT-Single. To evaluate the statistical significance of the difference in performance when training DL models with and without data augmentation, we use the Student's paired t test to compute -values between performance metrics of and all other datasets (i.e., , , * ) [47,52]. The test assumes the performance values to be normally distributed. We consider < 0.05 to be statistically significant.\n\nResults. Table 4 shows the retrieval performance of FBL-BERT, TBERT-Siamese and TBERT-Single trained on four dataset: , , and * , where * denotes the average best performing balanced dataset for the given model. The top part of the table shows results across bug reports from all projects, followed by per project results. In general, we observe that the models improve across all the metrics compared to , with the lowest improvement noted for , followed by , and with the highest improvement recorded for * . Depending on the model the scale of the improvement varies. While the MRR score for FBL-BERT increases from 0.264 for to 0.367 for * , about half of the improvement can be attributed to the dataset size as indicated by the results for with the MRR score of 0.307. Moreover, we also observe that improves the score from 0.307 for to 0.353, indicating that using an augmented dataset makes a difference not only through data quantity. The improvement between and * is marginal and equal to 0.014, indicating that even the best balancing configuration has a small effect on FBL-BERT in general.\n\nTraining with a balanced dataset has a bigger impact on TBERT-Single and TBERT-Siamese with an improvement of 0.035 and 0.092 in MRR scores respectively when compared to . Moreover, data balancing is the key contributor to the improvement in TBERT-Siamese for which statistically significant difference is observed only for * . In the case of TBERT-Single, training the model with both and * leads to significant improvement across all the metrics. RQ1 (a): Data augmentation improves the DL-based bug localization results across all models. Using data balancing with augmentation can further improve performance. Table 5 shows the retrieval accuracy for FBL-BERT and TBERT-Siamese when the models are trained on different balanced datasets, with the values of , and the dataset size provided on the right side of the table. In the case of FBL-BERT, 2 and 3 provide on average the best performance, improving MRR and MAP by 16.8% and 14.8% compared to 1 and 4 . However, as noted before, the improvement over the imbalanced dataset is marginal. In case   of TBERT-Siamese, the smallest balanced dataset, 1 , produces the highest MRR score of 0.328 which outperforms other balanced dataset by at least 49%.\n\nThe difference in the models' performance with different datasets can be attributed to (1) the overall difference in the models' architectures affecting models demand for training data; and (2) the size of each project measured as the number of hunks (see Table 2). To better understand if and when model may require more data, in Figure 4 we show MRR scores across all evaluation projects ordered by their size, i.e., the number of hunks in a project.\n\n\nRQ1 (b):\n\nThe results indicate that different model architectures may have different needs in terms of training dataset size to achieve their optimal performance. Some models benefit from more augmented samples, especially for larger projects.\n\n\nRQ2\n\n: Which of the proposed DA operators contribute the most to retrieval performance?\n\nSetup. To better understand the influence of the proposed data augmentation operators on the downstream model effectiveness, we perform ablation studies on training datasets created using all but one augmentation operator type. To this end, we create 5 types To balance the datasets, we use and values from RQ1 that resulted in the best performance for the models, i.e., for FBL-BERT = 0.85, = 2.0, while for TBERT models = 0.7, and = 1.0. Results. Figure 5 shows the MRR scores for datasets augmented with 4 out of 5 operator types as well as MRR scores of and as horizontal lines for reference. We note that most of the operators contribute towards the final performance, with an exception of Swap operator for FBL-BERT. The lack of impact for Swap operator can be attributed to the model architecture. Given that FBL-BERT leverages all of the tokens in a bug report separately, swapping the token positions does not preclude them from being matched. On the other hand, excluding Random Insert affects FBL-BERT the most, indicating that inserted tokens are valuable to the model and improve its effectiveness when matching token embeddings. The Delete operator is the most prominent contributor to the performance of both TBERT models. When the Delete operator is disallowed during augmentation, the MRR score of augmented datasets drops by 0.054 and 0.055 for TBERT-Single and TBERT-Siamese respectively, indicating that the variance caused by removing tokens randomly has a positive impact. The Delete operator, when applied in relative moderation, seems to add to the robustness of the models, i.e., the models create additional links between terms and concepts in the bug reports and changesets. The value of this process is also supported by recent work in adversarial training of large language models, like BERT, in order to improve their robustness against malicious attacks [18].\n\nRQ2: All DA operators contribute to the performance improvement with varying degree, with the exception of Swap for FBL-BERT. The Delete operator consistently improves performance in all three models.\n\n\nThreats to validity\n\nThere are several validity threads of our findings. A threat to internal validity of the study are the parameter choices for DL-based bug localization models, particularly in the context of (1) training procedure; (2) BERT-base selection; and (3) parameters inherent to each model. To mitigate that threat, during training we follow recommendations of BERT authors [10], while for each model we use parameters identified as optimal by the previous studies [6,29]. While in our study, we use BERTOverflow as our BERT base model, other choices exist (e.g., CodeBERT [12]), and more models are underway, hence we leave the evaluation of different BERT base models in the context of bug localization to future work. Another internal threat is in our choices of augmentation operators, their parameters (e.g., ), and how they are applied together (e.g., stacking operators). This threat is mitigated by following best known practices from the NLP augmentation literature that focus on token-level operators and in-domain tasks [25,57]. While we explore some parameter choices for data balancing in the paper (e.g., and ), there are also additional parameters related to bug report building process as well as other possible augmentation operators that may provide more improvement.\n\nThe external tools we leveraged to build our augmentation pipeline, e.g., infozilla, BEE tool, can introduce noise that propagates to our reported results. However, these are state-of-the-art tools that have been thoroughly evaluated so their error rate should be limited.\n\nFurthermore, the randomness of the augmentation operators may pose a threat to the internal validity. To mitigate that, we ensure to set an initial value on the system's pseudo-random number generator when building an augmented dataset as well as when training a DL model.\n\nA threat to the external validity is that we evaluated the data augmentation technique only for bug localization on a limited number of bugs collected from a selection of open source Java projects. This threat is mitigated by the fact that the dataset has been used in several prior bug localization studies [43,58,60]. Another mitigating factor is that the projects reflect a variety of purposes, development styles and histories.\n\nLimitations in the chosen evaluation metrics pose a threat to conclusion validity as they may not directly measure user satisfaction with the retrieved change hunks [54]. The threat is mitigated by the fact that the selected metrics are well-known and widely accepted as best available to measure and compare the performance of IR techniques.\n\n\nCONCLUSION AND FUTURE WORK\n\nDL models toward bug localization excel at bridging the lexical gap between natural language describing a bug report and programming language that defines the source code. However, training an effective DL model requires large amount of project-specific labelled data (i.e., pairs of bug reports and bug-inducing changesets), that is typically difficult to obtain in sufficient quantity for a single project. To relax the requirement on data quantity, and enable using DL model when training data is scarce, this work proposes to use data augmentation (DA) to create new, realistically looking bug reports that can be used to significantly increase the size of the training set. To augment bug reports, we propose DA operators that independently augment the natural language and code-related content of a bug report. To build a new training dataset using augmented bug reports, we propose a data balancing strategy that selectively augments bug reports to add more training samples for underrepresented parts of the source code.\n\nThe results indicate that the proposed data augmentation improves retrieval accuracy across all studied DL models increasing MRR score by 39% to 82% compared to the original, unaugmented dataset. Moreover, when augmented datasets are compared against training sets expanded by data repetition, we observe that they improve MRR scores by 20% to 36%. All of the proposed DA operators contribute to the final performance, with token deletion bringing most consistent impact for different DL models. This is one of the first papers to introduce data augmentation for software engineering. We believe data augmentation as a technique has potential for SE because the datasets are not as large as in mainstream ML. In addition, data augmentation is not a one-size-fits-all technique and its optimal application requires custom operators, so the paper contributes in designing the data augmentation operators for bug reports and applying them using a data balancing strategy.\n\nDespite this, the proposed approach requires more experiments to strengthen our observations and recommendations. As our future work, we plan to (1) extend our evaluation datasets with new software projects written in Java, Python and Javascript; (2) conduct experiments with more heavily augmented data, i.e., by using DA operators on a larger number of tokens; (3) experiment with different deletion operators (e.g., removing irrelevant code tokens) given the good performance of Random Delete for natural language; and (4) experiment with different configurations for the bug report builder.\n\n\nDATA AVAILABILITY\n\nA replication package that includes all relevant code and scripts is available at https://anonymous.4open.science/r/fbl-bert-987B.\n\n\nConference'17, July 2017, Washington, DC, USA \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn\n\nFigure 1 :\n1A visualization of our augmentation pipeline for a single bug report.\n\nFigure 2 :\n2An example of augmented bug report for Tomcat #55171. Token-level modifications are marked with grey color.\n\n\nof times a bug report occurs in the training set.\n\n\nof times a class occurs in the training set.\n\nFigure 3 :\n3Distribution of bug reports and classes showing data imbalance in bug localization training set.\n\n(\nFig\n\nFigure 4 :\n4MRR scores for evaluation projects trained on different balanced datasets. of augmented training datasets: No Backtranslation, No Insert, No Delete, No Replace and No Swap operator. Note that we consider, e.g., both Code Token Swap and Random Swap as operators of Swap type.\n\nFigure 5 :\n5MRR scores when trained with augmented data using different DA operators.\n\nTable 1 :\n1Four examples of textual data augmentation with varying validity[57].Bug report summary \nValid \n\nOriginal \nAsync connector does not timeout with HTTP \nNIO context. \n\n-\n\nRandom Swap 1 \nAsync connector does timeout not with HTTP \nNIO context. \n\n\u2713 \n\nRandom Swap 2 \nAsync context does not timeout with HTTP \nNIO connector. \n\n\u2717 \n\nSynonym Replacement 1 \nAsync connector does not timeout with HTTP \nNIO session. \n\n\u2713 \n\nSynonym Replacement 2 \nTCP connector does not timeout with HTTP \nNIO context. \n\n\n\nTable 2 :\n2Evaluation datasets. inducing changeset). Each bug report includes the bug summary and description, while each hunk contains a log message and source code changes. The dataset of Wen et al. was constructed using SZZTraining \nTesting \n\nProject \n# bugs \n# bugs # hunks \n\nAspectJ \n100 \n2212 \n100 \n23446 \nSWT \n45 \n9982 \n45 \n69833 \nTomcat \n96 \n5624 \n97 \n72134 \nPDE \n30 \n3856 \n30 \n100373 \nJDT \n47 18230 \n47 \n150630 \n\nstack traces, and new lines were removed from all bug reports in \nWen et al. 's dataset, we located and re-scraped the bug reports (with \nnew lines) from Bugzilla for all projects. For ZXing, the bug reports \nin the GitHub issue tracker did not match those collected by Wen \net al., likely because the project was moved, and therefore ZXing \nwas excluded from the evaluation set. To create a training set for \neach project, we ordered the bug reports by opening dates and \nselected the first half for training, while the remaining bug reports \nconstitute the test set. Each positive training sample corresponds \nto a pair of a bug report and one of its inducing hunks (extracted \nfrom the \n\nTable 3 :\n3Dataset characteristics for RQ1.AspectJ SWT Tomcat PDE \nJDT \n\nNot augmented datasets \n\n2.2k \n9.9k \n5.6k \n3.9k \n18.2k \n22.1k \n99.8k \n56.2k 38.6k 182.3k \n\nAugmented datasets \n\n24.3k 109.8k \n61.9k 42.4k 200.5k \n\n1 \n\n22.4k \n66.9k \n33.8k 25.1k 112.9k \n\n2 \n\n29.8k \n90.5k \n46.8k 30.7k 142.3k \n\n3 \n\n31.5k \n95.1k \n49.0k 32.5k 150.5k \n\n4 \n\n44.2k 130.9k \n65.6k 46.7k 216.4k \n\n\n\nTable 4 :\n4differences are marked with \u2191 and \u2193, where \u2191 indicates significant improvement in performance compared to , and \u2193 indicates significant decrease compared to .Bug localization performance across all evaluation projects and for different training datasets: \n-original dataset; \n-dataset with 10x repeated instances; \n-augmented dataset; \n1\u22124 -augmented and balanced datasets. Statistically \nsignificant ( < 0.05) MRR \nMAP \nP@1 \nP@3 \nP@5 \nMRR \nMAP \nP@1 \nP@3 \nP@5 \nMRR \nMAP \nP@1 \nP@3 \nP@5 \n\nFBL-BERT \nTBERT-Siamese \nTBERT-Single \n\n0.264 \n0.109 \n0.163 \n0.153 \n0.145 \n0.180 \n0.062 \n0.144 \n0.076 \n0.069 \n0.273 \n0.120 \n0.162 \n0.145 \n0.149 \n0.307 \u2191 \n0.129 \u2191 \n0.213 \u2191 \n0.179 \n0.176 \n0.201 \n0.086 \n0.110 \n0.093 \n0.093 \n0.271 \n0.140 \u2191 \n0.152 \n0.136 \u2191 \n0.176 \n0.353 \u2191 \n0.146 \u2191 \n0.247 \u2191 \n0.202 \u2191 0.197 \u2191 \n0.236 \n0.103 \n0.157 \n0.124 \n0.119 \u2191 \n0.333 \u2191 \n0.144 \u2191 \n0.217 \u2191 \n0.188 \u2191 \n0.194 \u2191 \n\n *  \n\n0.367 \u2191 0.147 \u2191 0.267 \u2191 0.198 \u2191 \n0.206 \u2191 0.328 \u2191 0.107 \u2191 0.247 \u2191 0.150 \u2191 0.146 \u2191 0.368 \u2191 0.149 \u2191 0.269 \u2191 0.192 \u2191 0.182 \u2191 \n\n\n\nTable 5 :\n5Bug localization performance with different aug-\nmented and balanced training datasets. \n\nMRR MAP P@1 P@3 P@5 \n# \n\nDataset \nFBL-BERT \n\n\nConference'17, July 2017, Washington, DC, USA Agnieszka Ciborowska and Kostadin Damevski\n\nWhat Makes a Good Bug Report. Nicolas Bettenburg, Sascha Just, Adrian Schr\u00f6ter, Cathrin Weiss, Rahul Premraj, Thomas Zimmermann, Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 16th ACM SIGSOFT International Symposium on Foundations of Software EngineeringSIGSOFT '08/FSE-16Nicolas Bettenburg, Sascha Just, Adrian Schr\u00f6ter, Cathrin Weiss, Rahul Premraj, and Thomas Zimmermann. 2008. What Makes a Good Bug Report?. In Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering (SIGSOFT '08/FSE-16).\n\nNot all bugs are the same: Understanding, characterizing, and classifying bug types. Gemma Catolino, Fabio Palomba, Andy Zaidman, Filomena Ferrucci, Journal of Systems and Software. Gemma Catolino, Fabio Palomba, Andy Zaidman, and Filomena Ferrucci. 2019. Not all bugs are the same: Understanding, characterizing, and classifying bug types. Journal of Systems and Software (2019).\n\nUsing Observed Behavior to Reformulate Queries during Text Retrieval-based Bug Localization. O Chaparro, J M Florez, A Marcus, 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME). O. Chaparro, J. M. Florez, and A. Marcus. 2017. Using Observed Behavior to Reformulate Queries during Text Retrieval-based Bug Localization. In 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME). 376- 387.\n\nDetecting Missing Information in Bug Descriptions. Oscar Chaparro, Jing Lu, Fiorella Zampetti, Laura Moreno, Massimiliano Di Penta, Andrian Marcus, Gabriele Bavota, Vincent Ng, Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. the 2017 11th Joint Meeting on Foundations of Software EngineeringOscar Chaparro, Jing Lu, Fiorella Zampetti, Laura Moreno, Massimiliano Di Penta, Andrian Marcus, Gabriele Bavota, and Vincent Ng. 2017. Detecting Missing Information in Bug Descriptions. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2017).\n\nHiddencut: Simple data augmentation for natural language understanding with better generalizability. Jiaao Chen, Dinghan Shen, Weizhu Chen, Diyi Yang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingLong Papers1Jiaao Chen, Dinghan Shen, Weizhu Chen, and Diyi Yang. 2021. Hiddencut: Simple data augmentation for natural language understanding with better generalizabil- ity. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Pro- cessing (Volume 1: Long Papers). 4380-4390.\n\nFast Changeset-Based Bug Localization with BERT. Agnieszka Ciborowska, Kostadin Damevski, Proceedings of the 44th International Conference on Software Engineering (ICSE '22). the 44th International Conference on Software Engineering (ICSE '22)New York, NY, USAAssociation for Computing MachineryAgnieszka Ciborowska and Kostadin Damevski. 2022. Fast Changeset-Based Bug Localization with BERT. In Proceedings of the 44th International Conference on Software Engineering (ICSE '22). Association for Computing Machinery, New York, NY, USA, 946-957.\n\nChangeset-Based Topic Modeling of Software Repositories. C S Corley, K Damevski, N A Kraft, IEEE Transactions on Software Engineering. C. S. Corley, K. Damevski, and N. A. Kraft. 2018. Changeset-Based Topic Modeling of Software Repositories. IEEE Transactions on Software Engineering (2018).\n\nA Framework for Evaluating the Results of the SZZ Approach for Identifying Bug-Introducing Changes. Daniel Alencar Da Costa, Shane Mcintosh, Weiyi Shang, Uir\u00e1 Kulesza, Roberta Coelho, Ahmed E Hassan, IEEE Transactions on Software Engineering. Daniel Alencar da Costa, Shane McIntosh, Weiyi Shang, Uir\u00e1 Kulesza, Roberta Coelho, and Ahmed E. Hassan. 2017. A Framework for Evaluating the Results of the SZZ Approach for Identifying Bug-Introducing Changes. IEEE Transactions on Software Engineering (2017).\n\nDeeper Text Understanding for IR with Contextual Neural Language Modeling. Zhuyun Dai, Jamie Callan, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19)Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19).\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\n\nY Steven, Varun Feng, Jason Gangal, Wei, arXiv:2105.03075Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for nlp. arXiv preprintSteven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for nlp. arXiv preprint arXiv:2105.03075 (2021).\n\nCodeBERT: A Pre-Trained Model for Programming and Natural Languages. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou, Findings of the Association for Computational Linguistics: EMNLP 2020. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In Findings of the Association for Computational Linguistics: EMNLP 2020.\n\nDeep Code Search. Xiaodong Gu, Hongyu Zhang, Sunghun Kim, 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE. Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep Code Search. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). 933-944.\n\nSemantically Enhanced Software Traceability Using Deep Learning Techniques. Jin Guo, Jinghui Cheng, Jane Cleland-Huang, Proceedings of the 39th International Conference on Software Engineering (ICSE '17). the 39th International Conference on Software Engineering (ICSE '17)Jin Guo, Jinghui Cheng, and Jane Cleland-Huang. 2017. Semantically Enhanced Software Traceability Using Deep Learning Techniques. In Proceedings of the 39th International Conference on Software Engineering (ICSE '17).\n\nNeural Attribution for Semantic Bug-Localization in Student Programs. Rahul Gupta, Aditya Kanade, Shirish Shevade, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett32Rahul Gupta, Aditya Kanade, and Shirish Shevade. 2019. Neural Attribution for Semantic Bug-Localization in Student Programs. In Advances in Neural Informa- tion Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32.\n\nAna Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey, Smith, arXiv:2004.109642020. Don't stop pretraining: adapt language models to domains and tasks. arXiv preprintSuchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. 2020. Don't stop pretraining: adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964 (2020).\n\nDeepJIT: An End-to-End Deep Learning Framework for Justin-Time Defect Prediction. Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, Naoyasu Ubayashi, 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu Ubayashi. 2019. DeepJIT: An End-to-End Deep Learning Framework for Just- in-Time Defect Prediction. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). 34-45.\n\nOn the Robustness of Self-Attentive Models. Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu, Cho-Jui Hsieh, 10.18653/v1/P19-1147Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsYu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu, and Cho- Jui Hsieh. 2019. On the Robustness of Self-Attentive Models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 1520-1529. https://doi.org/10. 18653/v1/P19-1147\n\nLearning Unified Features from Natural and Programming Languages for Locating Buggy Source Code. Xuan Huo, Ming Li, Zhi-Hua Zhou, Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI'16). the 25th International Joint Conference on Artificial Intelligence (IJCAI'16)Xuan Huo, Ming Li, and Zhi-Hua Zhou. 2016. Learning Unified Features from Nat- ural and Programming Languages for Locating Buggy Source Code. In Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI'16).\n\nDeep Transfer Bug Localization. X Huo, F Thung, M Li, D Lo, S Shi, 10.1109/TSE.2019.2920771IEEE Transactions on Software Engineering. X. Huo, F. Thung, M. Li, D. Lo, and S. Shi. 2019. Deep Transfer Bug Localization. IEEE Transactions on Software Engineering (2019), 1-1. https://doi.org/10.1109/ TSE.2019.2920771\n\nCodeSearchNet challenge: Evaluating the state of semantic code search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, arXiv:1909.09436arXiv preprintHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).\n\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. Omar Khattab, Matei Zaharia, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (SIGIR '20). the 43rd International ACM SIGIR conference on research and development in Information Retrieval (SIGIR '20)Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (SIGIR '20).\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti- mization. https://arxiv.org/abs/1412.6980\n\nPotential Biases in Bug Localization: Do They Matter. Pavneet Singh Kochhar, Yuan Tian, David Lo, Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering. the 29th ACM/IEEE International Conference on Automated Software EngineeringVasteras, SwedenPavneet Singh Kochhar, Yuan Tian, and David Lo. 2014. Potential Biases in Bug Localization: Do They Matter?. In Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering (Vasteras, Sweden) (ASE '14). 803-814.\n\nCan vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability. Venelin Kovatchev, Phillip Smith, Mark Lee, Rory Devine, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingLong Papers1Venelin Kovatchev, Phillip Smith, Mark Lee, and Rory Devine. 2021. Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).\n\nBug Localization with Combination of Deep Learning and Information Retrieval. An Ngoc Lam, Anh Tuan Nguyen, Anh Hoan, Tien N Nguyen, Nguyen, 10.1109/ICPC.2017.24Proceedings of the 25th International Conference on Program Comprehension (ICPC '17). the 25th International Conference on Program Comprehension (ICPC '17)IEEE PressAn Ngoc Lam, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen. 2017. Bug Localization with Combination of Deep Learning and Information Retrieval. In Proceedings of the 25th International Conference on Program Comprehension (ICPC '17). IEEE Press. https://doi.org/10.1109/ICPC.2017.24\n\nBench4BL: Reproducibility Study on the Performance of IRbased Bug Localization. Jaekwon Lee, Dongsun Kim, Tegawend\u00e9 F Bissyand\u00e9, Woosung Jung, Yves Le Traon, Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis. the 27th ACM SIGSOFT International Symposium on Software Testing and AnalysisAmsterdam, NetherlandsJaekwon Lee, Dongsun Kim, Tegawend\u00e9 F. Bissyand\u00e9, Woosung Jung, and Yves Le Traon. 2018. Bench4BL: Reproducibility Study on the Performance of IR- based Bug Localization. In Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis (Amsterdam, Netherlands) (ISSTA 2018). 61-72.\n\nSoftware Defect Prediction via Convolutional Neural Network. Jian Li, Pinjia He, Jieming Zhu, Michael R Lyu, 2017 IEEE International Conference on Software Quality, Reliability and Security (QRS. Jian Li, Pinjia He, Jieming Zhu, and Michael R. Lyu. 2017. Software Defect Pre- diction via Convolutional Neural Network. In 2017 IEEE International Conference on Software Quality, Reliability and Security (QRS). 318-328.\n\nTraceability transformed: Generating more accurate links with pre-trained BERT models. Jinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, Jane Cleland-Huang, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEEJinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, and Jane Cleland-Huang. 2021. Traceability transformed: Generating more accurate links with pre-trained BERT models. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 324-335.\n\nIndustry-scale IR-based Bug Localization: A Perspective from Facebook. Vijayaraghavan Murali, Lee Gross, Rebecca Qian, Satish Chandra, Proceedings of the 42nd International Conference on Software Engineering (ICSE '20). the 42nd International Conference on Software Engineering (ICSE '20)Vijayaraghavan Murali, Lee Gross, Rebecca Qian, and Satish Chandra. 2020. Industry-scale IR-based Bug Localization: A Perspective from Facebook. In Pro- ceedings of the 42nd International Conference on Software Engineering (ICSE '20).\n\nThe impact of refactoring changes on the SZZ algorithm: An empirical study. E C Neto, D A Da Costa, U Kulesza, IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER. E. C. Neto, D. A. da Costa, and U. Kulesza. 2018. The impact of refactoring changes on the SZZ algorithm: An empirical study. In IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER) (SANER 2018).\n\nFacebook FAIR's WMT19 News Translation Task Submission. Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov, Proceedings of the Fourth Conference on Machine Translation. the Fourth Conference on Machine Translation2Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2019. Facebook FAIR's WMT19 News Translation Task Submission. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1).\n\nA Topic-based Approach for Narrowing the Search Space of Buggy Files from a Bug Report. A T Nguyen, T T Nguyen, J Al-Kofahi, H V Nguyen, T N Nguyen, 10.1109/ASE.2011.6100062Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering. the 26th IEEE/ACM International Conference on Automated Software EngineeringA. T. Nguyen, T. T. Nguyen, J. Al-Kofahi, H. V. Nguyen, and T. N. Nguyen. 2011. A Topic-based Approach for Narrowing the Search Space of Buggy Files from a Bug Report. In Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011). 263-272. https://doi.org/10.1109/ ASE.2011.6100062\n\nData Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution. Kenton Toan Q Nguyen, David Murray, Chiang, arXiv:2105.01691arXiv preprintToan Q Nguyen, Kenton Murray, and David Chiang. 2021. Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution. arXiv preprint arXiv:2105.01691 (2021).\n\nPassage Re-ranking with BERT. Rodrigo Nogueira, Kyunghyun Cho, 10.48550/ARXIV.1901.04085Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. https://doi.org/10.48550/ARXIV.1901.04085\n\nThinking Like a Developer? Comparing the Attention of Humans with Neural Models of Code. Matteo Paltenghi, Michael Pradel, 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEEMatteo Paltenghi and Michael Pradel. 2021. Thinking Like a Developer? Compar- ing the Attention of Humans with Neural Models of Code. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 867-879.\n\nDeepLineDP: Towards a Deep Learning Approach for Line-Level Defect Prediction. Chanathip Pornprasit, Chakkrit Tantithamthavorn, IEEE Transactions on Software Engineering. Chanathip Pornprasit and Chakkrit Tantithamthavorn. 2022. DeepLineDP: To- wards a Deep Learning Approach for Line-Level Defect Prediction. IEEE Trans- actions on Software Engineering (2022).\n\nScaffle: bug localization on millions of files. Michael Pradel, Vijayaraghavan Murali, Rebecca Qian, Mateusz Machalica, Erik Meijer, Satish Chandra, Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis. the 29th ACM SIGSOFT International Symposium on Software Testing and AnalysisMichael Pradel, Vijayaraghavan Murali, Rebecca Qian, Mateusz Machalica, Erik Meijer, and Satish Chandra. 2020. Scaffle: bug localization on millions of files. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis.\n\nExtracting structural information from bug reports. Rahul Premraj, Thomas Zimmermann, Sunghun Kim, Nicolas Bettenburg, Proceedings of the 2008 international workshop on Mining software repositories -MSR. the 2008 international workshop on Mining software repositories -MSRRahul Premraj, Thomas Zimmermann, Sunghun Kim, and Nicolas Bettenburg. 2008. Extracting structural information from bug reports. In Proceedings of the 2008 international workshop on Mining software repositories -MSR 2008.\n\nImproving IR-based bug localization with context-aware query reformulation. Mohammad Masudur Rahman, Roy, Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringACMMohammad Masudur Rahman and Chanchal K Roy. 2018. Improving IR-based bug localization with context-aware query reformulation. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ACM, 621-632.\n\nNils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. cs.CLNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv:1908.10084 [cs.CL]\n\nGiovanni Rosa, Luca Pascarella, Simone Scalabrino, Rosalia Tufano, Gabriele Bavota, Michele Lanza, Rocco Oliveto, arXiv:2102.03300Evaluating SZZ Implementations Through a Developer-informed Oracle. cs.SEGiovanni Rosa, Luca Pascarella, Simone Scalabrino, Rosalia Tufano, Gabriele Bavota, Michele Lanza, and Rocco Oliveto. 2021. Evaluating SZZ Implementations Through a Developer-informed Oracle. arXiv:2102.03300 [cs.SE]\n\nImproving Bug Localization Using Structured Information Retrieval. K Ripon, Matthew Saha, Sarfraz Lease, Dewayne E Khurshid, Perry, Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering. the 28th IEEE/ACM International Conference on Automated Software EngineeringSilicon Valley, CA, USARipon K. Saha, Matthew Lease, Sarfraz Khurshid, and Dewayne E. Perry. 2013. Im- proving Bug Localization Using Structured Information Retrieval. In Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering (Silicon Valley, CA, USA) (ASE'13). 345-355.\n\nA Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation. Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, Weizhu Chen, arXiv:2009.13818arXiv preprintDinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, and Weizhu Chen. 2020. A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation. arXiv preprint arXiv:2009.13818 (2020).\n\nText data augmentation for deep learning. Connor Shorten, M Taghi, Borko Khoshgoftaar, Furht, Journal of big Data. 8Connor Shorten, Taghi M Khoshgoftaar, and Borko Furht. 2021. Text data aug- mentation for deep learning. Journal of big Data 8, 1 (2021), 1-34.\n\nWhen Do Changes Induce Fixes. Jacek Sliwerski, Thomas Zimmermann, Andreas Zeller, Proceedings of the 2005 International Workshop on Mining Software Repositories (MSR '05). the 2005 International Workshop on Mining Software Repositories (MSR '05)Jacek Sliwerski, Thomas Zimmermann, and Andreas Zeller. 2005. When Do Changes Induce Fixes?. In Proceedings of the 2005 International Workshop on Mining Software Repositories (MSR '05).\n\nA Comparison of Statistical Significance Tests for Information Retrieval Evaluation. D Mark, James Smucker, Ben Allan, Carterette, Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management (CIKM '07. the Sixteenth ACM Conference on Conference on Information and Knowledge Management (CIKM '07Mark D. Smucker, James Allan, and Ben Carterette. 2007. A Comparison of Statistical Significance Tests for Information Retrieval Evaluation. In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management (CIKM '07). 623-632.\n\nBEE: A Tool for Structuring and Analyzing Bug Reports. Yang Song, Oscar Chaparro, Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringYang Song and Oscar Chaparro. 2020. BEE: A Tool for Structuring and Ana- lyzing Bug Reports. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering.\n\nCode and Named Entity Recognition in StackOverflow. Jeniya Tabassum, Mounica Maddela, Wei Xu, Alan Ritter, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsJeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. 2020. Code and Named Entity Recognition in StackOverflow. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\n\nHuy Tu, Tim Menzies, arXiv:2108.098472021. FRUGAL: Unlocking SSL for Software Analytics. arXiv preprintHuy Tu and Tim Menzies. 2021. FRUGAL: Unlocking SSL for Software Analytics. arXiv preprint arXiv:2108.09847 (2021).\n\nBetter Data Labelling With EMBLEM (and how that Impacts Defect Prediction). Huy Tu, Zhe Yu, Tim Menzies, IEEE Transactions on Software Engineering. 48Huy Tu, Zhe Yu, and Tim Menzies. 2022. Better Data Labelling With EMBLEM (and how that Impacts Defect Prediction). IEEE Transactions on Software Engi- neering 48, 1 (2022), 278-294.\n\nStatistical Significance Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and Type III Errors. Juli\u00e1n Urbano, Harlley Lima, Alan Hanjalic, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 42nd International ACM SIGIR Conference on Research and Development in Information RetrievalParis, FranceSIGIR'19Juli\u00e1n Urbano, Harlley Lima, and Alan Hanjalic. 2019. Statistical Significance Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and Type III Errors. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Paris, France) (SIGIR'19).\n\nQuality and Productivity Outcomes Relating to Continuous Integration in GitHub. Bogdan Vasilescu, Yue Yu, Huaimin Wang, Premkumar Devanbu, Vladimir Filkov, Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. the 2015 10th Joint Meeting on Foundations of Software EngineeringBogdan Vasilescu, Yue Yu, Huaimin Wang, Premkumar Devanbu, and Vladimir Filkov. 2015. Quality and Productivity Outcomes Relating to Continuous Integra- tion in GitHub. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2015).\n\nEvaluating the Usefulness of IR-Based Fault Localization Techniques. Qianqian Wang, Chris Parnin, Alessandro Orso, Proceedings of the 2015 International Symposium on Software Testing and Analysis. the 2015 International Symposium on Software Testing and AnalysisBaltimore, MD, USAQianqian Wang, Chris Parnin, and Alessandro Orso. 2015. Evaluating the Use- fulness of IR-Based Fault Localization Techniques. In Proceedings of the 2015 International Symposium on Software Testing and Analysis (ISSTA 2015) (Balti- more, MD, USA). 1-11.\n\nDeep Semantic Feature Learning for Software Defect Prediction. Song Wang, Taiyue Liu, Jaechang Nam, Lin Tan, IEEE Transactions on Software Engineering. 4612Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2020. Deep Semantic Feature Learning for Software Defect Prediction. IEEE Transactions on Software Engineering 46, 12 (2020).\n\nVersion History, Similar Report, and Structure: Putting Them Together for Improved Bug Localization. Shaowei Wang, David Lo, Proceedings of the 22Nd International Conference on Program Comprehension. the 22Nd International Conference on Program ComprehensionHyderabad, IndiaICPCShaowei Wang and David Lo. 2014. Version History, Similar Report, and Structure: Putting Them Together for Improved Bug Localization. In Proceedings of the 22Nd International Conference on Program Comprehension (Hyderabad, India) (ICPC 2014). 53-63.\n\nEDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. Jason Wei, Kai Zou, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Jason Wei and Kai Zou. 2019. EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\n\nLocus: Locating Bugs from Software Changes. Ming Wen, Rongxin Wu, Shing-Chi Cheung, Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering. the 31st IEEE/ACM International Conference on Automated Software EngineeringSingapore, SingaporeMing Wen, Rongxin Wu, and Shing-Chi Cheung. 2016. Locus: Locating Bugs from Software Changes. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering (Singapore, Singapore) (ASE 2016). 262-273.\n\nOn the Influence of Biases in Bug Localization: Evaluation and Benchmark. Ratnadira Widyasari, Agus Stefanus, Ferdian Haryono, Jieke Thung, Constance Shi, Fiona Tan, Jack Wee, David Phan, Lo, Proceedings of the 29th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), RENE Track. the 29th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), RENE TrackSANER 2022Ratnadira Widyasari, Stefanus Agus Haryono, Ferdian Thung, Jieke Shi, Con- stance Tan, Fiona Wee, Jack Phan, and David Lo. 2022. On the Influence of Biases in Bug Localization: Evaluation and Benchmark. In Proceedings of the 29th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), RENE Track. (SANER 2022).\n\nBoosting Bug-Report-Oriented Fault Localization with Segmentation and Stack-Trace Analysis. Chu-Pan Wong, Yingfei Xiong, Hongyu Zhang, Dan Hao, Lu Zhang, Hong Mei, Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution (ICSME '14). the 2014 IEEE International Conference on Software Maintenance and Evolution (ICSME '14)Chu-Pan Wong, Yingfei Xiong, Hongyu Zhang, Dan Hao, Lu Zhang, and Hong Mei. 2014. Boosting Bug-Report-Oriented Fault Localization with Segmentation and Stack-Trace Analysis. In Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution (ICSME '14). 181-190.\n\nShufang Xie, Yang Fan, and Tao Qin. 2021. mixSeq: A Simple Data Augmentation Methodfor Neural Machine Translation. Xueqing Wu, Yingce Xia, Jinhua Zhu, Lijun Wu, Proceedings of the 18th International Conference on Spoken Language Translation. the 18th International Conference on Spoken Language TranslationIWSLT 2021Xueqing Wu, Yingce Xia, Jinhua Zhu, Lijun Wu, Shufang Xie, Yang Fan, and Tao Qin. 2021. mixSeq: A Simple Data Augmentation Methodfor Neural Ma- chine Translation. In Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021).\n\nImproving bug localization with word embedding and enhanced convolutional neural networks. Yan Xiao, Jacky Keung, Kwabena E Bennin, Qing Mi, Information and Software Technology. 105Yan Xiao, Jacky Keung, Kwabena E. Bennin, and Qing Mi. 2019. Improving bug localization with word embedding and enhanced convolutional neural networks. Information and Software Technology 105 (2019).\n\nDeep Learning for Just-in-Time Defect Prediction. Xinli Yang, David Lo, Xin Xia, Yun Zhang, Jianling Sun, 10.1109/QRS.2015.142015 IEEE International Conference on Software Quality, Reliability and Security. Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep Learning for Just-in-Time Defect Prediction. In 2015 IEEE International Conference on Software Quality, Reliability and Security. 17-26. https://doi.org/10.1109/QRS. 2015.14\n\nLearning to Rank Relevant Files for Bug Reports Using Domain Knowledge. Xin Ye, Razvan Bunescu, Chang Liu, Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 22Nd ACM SIGSOFT International Symposium on Foundations of Software EngineeringHong Kong, ChinaXin Ye, Razvan Bunescu, and Chang Liu. 2014. Learning to Rank Relevant Files for Bug Reports Using Domain Knowledge. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering (Hong Kong, China) (FSE 2014). 689-699.\n\nOn the value of oversampling for deep learning in software defect prediction. Rahul Yedida, Tim Menzies, IEEE Transactions on Software Engineering. Rahul Yedida and Tim Menzies. 2021. On the value of oversampling for deep learning in software defect prediction. IEEE Transactions on Software Engineering (2021).\n\nHow Practitioners Perceive Automated Bug Report Management Techniques. W Zou, D Lo, Z Chen, X Xia, Y Feng, B Xu, IEEE Transactions on Software Engineering. 46W. Zou, D. Lo, Z. Chen, X. Xia, Y. Feng, and B. Xu. 2020. How Practitioners Perceive Automated Bug Report Management Techniques. IEEE Transactions on Software Engineering 46, 8 (2020).\n", "annotations": {"author": "[{\"end\":174,\"start\":98},{\"end\":266,\"start\":175}]", "publisher": null, "author_last_name": "[{\"end\":118,\"start\":108},{\"end\":192,\"start\":184}]", "author_first_name": "[{\"end\":107,\"start\":98},{\"end\":183,\"start\":175}]", "author_affiliation": "[{\"end\":173,\"start\":120},{\"end\":265,\"start\":212}]", "title": "[{\"end\":95,\"start\":1},{\"end\":361,\"start\":267}]", "venue": null, "abstract": "[{\"end\":1984,\"start\":386}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2957,\"start\":2953},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2960,\"start\":2957},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2963,\"start\":2960},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2966,\"start\":2963},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2990,\"start\":2986},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2993,\"start\":2990},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2996,\"start\":2993},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2999,\"start\":2996},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":3002,\"start\":2999},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3028,\"start\":3025},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3031,\"start\":3028},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3034,\"start\":3031},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3037,\"start\":3034},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3040,\"start\":3037},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3401,\"start\":3397},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3686,\"start\":3683},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3689,\"start\":3686},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3925,\"start\":3921},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4384,\"start\":4380},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4748,\"start\":4744},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4751,\"start\":4748},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":4754,\"start\":4751},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4819,\"start\":4816},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4822,\"start\":4819},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5027,\"start\":5024},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":5030,\"start\":5027},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5168,\"start\":5165},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5171,\"start\":5168},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5200,\"start\":5196},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5623,\"start\":5619},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5726,\"start\":5722},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5763,\"start\":5759},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5766,\"start\":5763},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6441,\"start\":6437},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10048,\"start\":10045},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10051,\"start\":10048},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10054,\"start\":10051},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10057,\"start\":10054},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10247,\"start\":10244},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10250,\"start\":10247},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10588,\"start\":10584},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11061,\"start\":11057},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11501,\"start\":11497},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11504,\"start\":11501},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11695,\"start\":11691},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12761,\"start\":12758},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13180,\"start\":13176},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13356,\"start\":13352},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14591,\"start\":14587},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14594,\"start\":14591},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14897,\"start\":14894},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15003,\"start\":15000},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16897,\"start\":16893},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17352,\"start\":17348},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17659,\"start\":17656},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17662,\"start\":17659},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18679,\"start\":18675},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18828,\"start\":18824},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":19717,\"start\":19713},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20305,\"start\":20302},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21014,\"start\":21010},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21159,\"start\":21155},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":21719,\"start\":21715},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25700,\"start\":25696},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":25703,\"start\":25700},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26178,\"start\":26175},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26181,\"start\":26178},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27661,\"start\":27658},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27664,\"start\":27661},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27877,\"start\":27874},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":28590,\"start\":28586},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":30342,\"start\":30338},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":33099,\"start\":33095},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":33293,\"start\":33289},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35375,\"start\":35371},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35423,\"start\":35420},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35426,\"start\":35423},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":35429,\"start\":35426},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35930,\"start\":35926},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35933,\"start\":35930},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36338,\"start\":36335},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36341,\"start\":36338},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":38185,\"start\":38181},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38452,\"start\":38448},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38488,\"start\":38484},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":40952,\"start\":40948},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":40955,\"start\":40952},{\"end\":43141,\"start\":43134},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46068,\"start\":46064},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":46664,\"start\":46660},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":46754,\"start\":46751},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":46757,\"start\":46754},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":46863,\"start\":46859},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47321,\"start\":47317},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47324,\"start\":47321},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":48433,\"start\":48429},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":48436,\"start\":48433},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":48439,\"start\":48436},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":48723,\"start\":48719},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":52726,\"start\":52722}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51850,\"start\":51674},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51933,\"start\":51851},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52054,\"start\":51934},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52106,\"start\":52055},{\"attributes\":{\"id\":\"fig_4\"},\"end\":52153,\"start\":52107},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52263,\"start\":52154},{\"attributes\":{\"id\":\"fig_6\"},\"end\":52270,\"start\":52264},{\"attributes\":{\"id\":\"fig_9\"},\"end\":52558,\"start\":52271},{\"attributes\":{\"id\":\"fig_10\"},\"end\":52645,\"start\":52559},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":53149,\"start\":52646},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54262,\"start\":53150},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":54640,\"start\":54263},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":55656,\"start\":54641},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":55804,\"start\":55657}]", "paragraph": "[{\"end\":4385,\"start\":2000},{\"end\":6170,\"start\":4387},{\"end\":6710,\"start\":6172},{\"end\":8310,\"start\":6712},{\"end\":9269,\"start\":8312},{\"end\":9359,\"start\":9277},{\"end\":9930,\"start\":9361},{\"end\":10694,\"start\":9973},{\"end\":14279,\"start\":10696},{\"end\":15526,\"start\":14281},{\"end\":15973,\"start\":15528},{\"end\":16836,\"start\":15986},{\"end\":18680,\"start\":16859},{\"end\":19420,\"start\":18682},{\"end\":19761,\"start\":19454},{\"end\":20169,\"start\":19763},{\"end\":20540,\"start\":20171},{\"end\":21015,\"start\":20616},{\"end\":21262,\"start\":21017},{\"end\":22733,\"start\":21264},{\"end\":23075,\"start\":22763},{\"end\":25095,\"start\":23077},{\"end\":26366,\"start\":25130},{\"end\":27216,\"start\":26368},{\"end\":28496,\"start\":27258},{\"end\":30343,\"start\":28498},{\"end\":31739,\"start\":30345},{\"end\":31805,\"start\":31798},{\"end\":31874,\"start\":31807},{\"end\":32913,\"start\":31876},{\"end\":34159,\"start\":32965},{\"end\":34537,\"start\":34161},{\"end\":34967,\"start\":34562},{\"end\":34989,\"start\":34988},{\"end\":35181,\"start\":34991},{\"end\":35405,\"start\":35220},{\"end\":37871,\"start\":35407},{\"end\":38742,\"start\":37892},{\"end\":38957,\"start\":38754},{\"end\":39243,\"start\":38959},{\"end\":41076,\"start\":39245},{\"end\":42180,\"start\":41078},{\"end\":43387,\"start\":42182},{\"end\":43841,\"start\":43389},{\"end\":44087,\"start\":43854},{\"end\":44177,\"start\":44095},{\"end\":46069,\"start\":44179},{\"end\":46271,\"start\":46071},{\"end\":47571,\"start\":46295},{\"end\":47845,\"start\":47573},{\"end\":48119,\"start\":47847},{\"end\":48552,\"start\":48121},{\"end\":48896,\"start\":48554},{\"end\":49955,\"start\":48927},{\"end\":50925,\"start\":49957},{\"end\":51521,\"start\":50927},{\"end\":51673,\"start\":51543}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20615,\"start\":20541},{\"attributes\":{\"id\":\"formula_1\"},\"end\":34561,\"start\":34538},{\"attributes\":{\"id\":\"formula_2\"},\"end\":34987,\"start\":34968},{\"attributes\":{\"id\":\"formula_3\"},\"end\":35207,\"start\":35182}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13010,\"start\":13003},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34041,\"start\":34034},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":39242,\"start\":39235},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":41094,\"start\":41087},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":42803,\"start\":42796},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":43652,\"start\":43645}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1998,\"start\":1986},{\"end\":9275,\"start\":9272},{\"attributes\":{\"n\":\"2\"},\"end\":9971,\"start\":9933},{\"attributes\":{\"n\":\"3\"},\"end\":15984,\"start\":15976},{\"attributes\":{\"n\":\"3.1\"},\"end\":16857,\"start\":16839},{\"attributes\":{\"n\":\"3.2\"},\"end\":19452,\"start\":19423},{\"attributes\":{\"n\":\"3.3\"},\"end\":22761,\"start\":22736},{\"attributes\":{\"n\":\"3.4\"},\"end\":25128,\"start\":25098},{\"attributes\":{\"n\":\"3.5\"},\"end\":27256,\"start\":27219},{\"end\":31796,\"start\":31742},{\"attributes\":{\"n\":\"4\"},\"end\":32963,\"start\":32916},{\"attributes\":{\"n\":\"4.2\"},\"end\":35218,\"start\":35209},{\"attributes\":{\"n\":\"4.3\"},\"end\":37890,\"start\":37874},{\"attributes\":{\"n\":\"5\"},\"end\":38752,\"start\":38745},{\"end\":43852,\"start\":43844},{\"attributes\":{\"n\":\"5.2\"},\"end\":44093,\"start\":44090},{\"attributes\":{\"n\":\"5.3\"},\"end\":46293,\"start\":46274},{\"attributes\":{\"n\":\"6\"},\"end\":48925,\"start\":48899},{\"attributes\":{\"n\":\"7\"},\"end\":51541,\"start\":51524},{\"end\":51862,\"start\":51852},{\"end\":51945,\"start\":51935},{\"end\":52165,\"start\":52155},{\"end\":52266,\"start\":52265},{\"end\":52282,\"start\":52272},{\"end\":52570,\"start\":52560},{\"end\":52656,\"start\":52647},{\"end\":53160,\"start\":53151},{\"end\":54273,\"start\":54264},{\"end\":54651,\"start\":54642},{\"end\":55667,\"start\":55658}]", "table": "[{\"end\":53149,\"start\":52727},{\"end\":54262,\"start\":53377},{\"end\":54640,\"start\":54307},{\"end\":55656,\"start\":54811},{\"end\":55804,\"start\":55669}]", "figure_caption": "[{\"end\":51850,\"start\":51676},{\"end\":51933,\"start\":51864},{\"end\":52054,\"start\":51947},{\"end\":52106,\"start\":52057},{\"end\":52153,\"start\":52109},{\"end\":52263,\"start\":52167},{\"end\":52270,\"start\":52267},{\"end\":52558,\"start\":52284},{\"end\":52645,\"start\":52572},{\"end\":52727,\"start\":52658},{\"end\":53377,\"start\":53162},{\"end\":54307,\"start\":54275},{\"end\":54811,\"start\":54653}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16593,\"start\":16585},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26191,\"start\":26183},{\"end\":26627,\"start\":26620},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28627,\"start\":28619},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28675,\"start\":28666},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28707,\"start\":28702},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30441,\"start\":30433},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32718,\"start\":32710},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":43728,\"start\":43720},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":44636,\"start\":44628}]", "bib_author_first_name": "[{\"end\":55932,\"start\":55925},{\"end\":55951,\"start\":55945},{\"end\":55964,\"start\":55958},{\"end\":55982,\"start\":55975},{\"end\":55995,\"start\":55990},{\"end\":56011,\"start\":56005},{\"end\":56580,\"start\":56575},{\"end\":56596,\"start\":56591},{\"end\":56610,\"start\":56606},{\"end\":56628,\"start\":56620},{\"end\":56966,\"start\":56965},{\"end\":56978,\"start\":56977},{\"end\":56980,\"start\":56979},{\"end\":56990,\"start\":56989},{\"end\":57374,\"start\":57369},{\"end\":57389,\"start\":57385},{\"end\":57402,\"start\":57394},{\"end\":57418,\"start\":57413},{\"end\":57439,\"start\":57427},{\"end\":57457,\"start\":57450},{\"end\":57474,\"start\":57466},{\"end\":57490,\"start\":57483},{\"end\":58040,\"start\":58035},{\"end\":58054,\"start\":58047},{\"end\":58067,\"start\":58061},{\"end\":58078,\"start\":58074},{\"end\":58834,\"start\":58825},{\"end\":58855,\"start\":58847},{\"end\":59382,\"start\":59381},{\"end\":59384,\"start\":59383},{\"end\":59394,\"start\":59393},{\"end\":59406,\"start\":59405},{\"end\":59408,\"start\":59407},{\"end\":59734,\"start\":59717},{\"end\":59747,\"start\":59742},{\"end\":59763,\"start\":59758},{\"end\":59775,\"start\":59771},{\"end\":59792,\"start\":59785},{\"end\":59806,\"start\":59801},{\"end\":59808,\"start\":59807},{\"end\":60203,\"start\":60197},{\"end\":60214,\"start\":60209},{\"end\":60779,\"start\":60774},{\"end\":60796,\"start\":60788},{\"end\":60810,\"start\":60804},{\"end\":60824,\"start\":60816},{\"end\":61410,\"start\":61409},{\"end\":61424,\"start\":61419},{\"end\":61436,\"start\":61431},{\"end\":61886,\"start\":61878},{\"end\":61897,\"start\":61893},{\"end\":61907,\"start\":61903},{\"end\":61917,\"start\":61914},{\"end\":61933,\"start\":61924},{\"end\":61944,\"start\":61940},{\"end\":61957,\"start\":61951},{\"end\":61968,\"start\":61964},{\"end\":61978,\"start\":61974},{\"end\":61989,\"start\":61984},{\"end\":62001,\"start\":61997},{\"end\":62389,\"start\":62381},{\"end\":62400,\"start\":62394},{\"end\":62415,\"start\":62408},{\"end\":62732,\"start\":62729},{\"end\":62745,\"start\":62738},{\"end\":62757,\"start\":62753},{\"end\":63220,\"start\":63215},{\"end\":63234,\"start\":63228},{\"end\":63250,\"start\":63243},{\"end\":63678,\"start\":63675},{\"end\":63704,\"start\":63698},{\"end\":63720,\"start\":63716},{\"end\":63736,\"start\":63734},{\"end\":63745,\"start\":63741},{\"end\":63761,\"start\":63755},{\"end\":64189,\"start\":64184},{\"end\":64200,\"start\":64197},{\"end\":64206,\"start\":64201},{\"end\":64220,\"start\":64212},{\"end\":64233,\"start\":64228},{\"end\":64245,\"start\":64238},{\"end\":64649,\"start\":64643},{\"end\":64663,\"start\":64657},{\"end\":64679,\"start\":64671},{\"end\":64689,\"start\":64686},{\"end\":64703,\"start\":64695},{\"end\":64716,\"start\":64709},{\"end\":65400,\"start\":65396},{\"end\":65410,\"start\":65406},{\"end\":65422,\"start\":65415},{\"end\":65873,\"start\":65872},{\"end\":65880,\"start\":65879},{\"end\":65889,\"start\":65888},{\"end\":65895,\"start\":65894},{\"end\":65901,\"start\":65900},{\"end\":66230,\"start\":66225},{\"end\":66248,\"start\":66239},{\"end\":66260,\"start\":66253},{\"end\":66277,\"start\":66268},{\"end\":66293,\"start\":66289},{\"end\":66642,\"start\":66638},{\"end\":66657,\"start\":66652},{\"end\":67207,\"start\":67206},{\"end\":67223,\"start\":67218},{\"end\":67415,\"start\":67408},{\"end\":67435,\"start\":67431},{\"end\":67447,\"start\":67442},{\"end\":68032,\"start\":68025},{\"end\":68051,\"start\":68044},{\"end\":68063,\"start\":68059},{\"end\":68073,\"start\":68069},{\"end\":68888,\"start\":68886},{\"end\":68893,\"start\":68889},{\"end\":68902,\"start\":68899},{\"end\":68907,\"start\":68903},{\"end\":68919,\"start\":68916},{\"end\":68930,\"start\":68926},{\"end\":68932,\"start\":68931},{\"end\":69511,\"start\":69504},{\"end\":69524,\"start\":69517},{\"end\":69539,\"start\":69530},{\"end\":69541,\"start\":69540},{\"end\":69560,\"start\":69553},{\"end\":69571,\"start\":69567},{\"end\":69574,\"start\":69572},{\"end\":70154,\"start\":70150},{\"end\":70165,\"start\":70159},{\"end\":70177,\"start\":70170},{\"end\":70190,\"start\":70183},{\"end\":70192,\"start\":70191},{\"end\":70602,\"start\":70595},{\"end\":70613,\"start\":70608},{\"end\":70626,\"start\":70619},{\"end\":70637,\"start\":70633},{\"end\":70649,\"start\":70645},{\"end\":71092,\"start\":71078},{\"end\":71104,\"start\":71101},{\"end\":71119,\"start\":71112},{\"end\":71132,\"start\":71126},{\"end\":71608,\"start\":71607},{\"end\":71610,\"start\":71609},{\"end\":71618,\"start\":71617},{\"end\":71620,\"start\":71619},{\"end\":71632,\"start\":71631},{\"end\":72034,\"start\":72028},{\"end\":72043,\"start\":72039},{\"end\":72055,\"start\":72049},{\"end\":72069,\"start\":72065},{\"end\":72082,\"start\":72075},{\"end\":72095,\"start\":72089},{\"end\":72544,\"start\":72543},{\"end\":72546,\"start\":72545},{\"end\":72556,\"start\":72555},{\"end\":72558,\"start\":72557},{\"end\":72568,\"start\":72567},{\"end\":72581,\"start\":72580},{\"end\":72583,\"start\":72582},{\"end\":72593,\"start\":72592},{\"end\":72595,\"start\":72594},{\"end\":73221,\"start\":73215},{\"end\":73242,\"start\":73237},{\"end\":73512,\"start\":73505},{\"end\":73532,\"start\":73523},{\"end\":73773,\"start\":73767},{\"end\":73792,\"start\":73785},{\"end\":74211,\"start\":74202},{\"end\":74232,\"start\":74224},{\"end\":74541,\"start\":74534},{\"end\":74564,\"start\":74550},{\"end\":74580,\"start\":74573},{\"end\":74594,\"start\":74587},{\"end\":74610,\"start\":74606},{\"end\":74625,\"start\":74619},{\"end\":75120,\"start\":75115},{\"end\":75136,\"start\":75130},{\"end\":75156,\"start\":75149},{\"end\":75169,\"start\":75162},{\"end\":76245,\"start\":76241},{\"end\":76260,\"start\":76255},{\"end\":76493,\"start\":76485},{\"end\":76504,\"start\":76500},{\"end\":76523,\"start\":76517},{\"end\":76543,\"start\":76536},{\"end\":76560,\"start\":76552},{\"end\":76576,\"start\":76569},{\"end\":76589,\"start\":76584},{\"end\":76974,\"start\":76973},{\"end\":76989,\"start\":76982},{\"end\":77003,\"start\":76996},{\"end\":77018,\"start\":77011},{\"end\":77020,\"start\":77019},{\"end\":77628,\"start\":77621},{\"end\":77642,\"start\":77635},{\"end\":77656,\"start\":77650},{\"end\":77668,\"start\":77663},{\"end\":77679,\"start\":77673},{\"end\":77985,\"start\":77979},{\"end\":77996,\"start\":77995},{\"end\":78009,\"start\":78004},{\"end\":78233,\"start\":78228},{\"end\":78251,\"start\":78245},{\"end\":78271,\"start\":78264},{\"end\":78716,\"start\":78715},{\"end\":78728,\"start\":78723},{\"end\":78741,\"start\":78738},{\"end\":79285,\"start\":79281},{\"end\":79297,\"start\":79292},{\"end\":79878,\"start\":79872},{\"end\":79896,\"start\":79889},{\"end\":79909,\"start\":79906},{\"end\":79918,\"start\":79914},{\"end\":80301,\"start\":80298},{\"end\":80309,\"start\":80306},{\"end\":80597,\"start\":80594},{\"end\":80605,\"start\":80602},{\"end\":80613,\"start\":80610},{\"end\":80978,\"start\":80972},{\"end\":80994,\"start\":80987},{\"end\":81005,\"start\":81001},{\"end\":81651,\"start\":81645},{\"end\":81666,\"start\":81663},{\"end\":81678,\"start\":81671},{\"end\":81694,\"start\":81685},{\"end\":81712,\"start\":81704},{\"end\":82218,\"start\":82210},{\"end\":82230,\"start\":82225},{\"end\":82249,\"start\":82239},{\"end\":82743,\"start\":82739},{\"end\":82756,\"start\":82750},{\"end\":82770,\"start\":82762},{\"end\":82779,\"start\":82776},{\"end\":83117,\"start\":83110},{\"end\":83129,\"start\":83124},{\"end\":83637,\"start\":83632},{\"end\":83646,\"start\":83643},{\"end\":84341,\"start\":84337},{\"end\":84354,\"start\":84347},{\"end\":84368,\"start\":84359},{\"end\":84883,\"start\":84874},{\"end\":84899,\"start\":84895},{\"end\":84917,\"start\":84910},{\"end\":84932,\"start\":84927},{\"end\":84949,\"start\":84940},{\"end\":84960,\"start\":84955},{\"end\":84970,\"start\":84966},{\"end\":84981,\"start\":84976},{\"end\":85680,\"start\":85673},{\"end\":85694,\"start\":85687},{\"end\":85708,\"start\":85702},{\"end\":85719,\"start\":85716},{\"end\":85727,\"start\":85725},{\"end\":85739,\"start\":85735},{\"end\":86352,\"start\":86345},{\"end\":86363,\"start\":86357},{\"end\":86375,\"start\":86369},{\"end\":86386,\"start\":86381},{\"end\":86901,\"start\":86898},{\"end\":86913,\"start\":86908},{\"end\":86928,\"start\":86921},{\"end\":86930,\"start\":86929},{\"end\":86943,\"start\":86939},{\"end\":87244,\"start\":87239},{\"end\":87256,\"start\":87251},{\"end\":87264,\"start\":87261},{\"end\":87273,\"start\":87270},{\"end\":87289,\"start\":87281},{\"end\":87717,\"start\":87714},{\"end\":87728,\"start\":87722},{\"end\":87743,\"start\":87738},{\"end\":88291,\"start\":88286},{\"end\":88303,\"start\":88300},{\"end\":88593,\"start\":88592},{\"end\":88600,\"start\":88599},{\"end\":88606,\"start\":88605},{\"end\":88614,\"start\":88613},{\"end\":88621,\"start\":88620},{\"end\":88629,\"start\":88628}]", "bib_author_last_name": "[{\"end\":55943,\"start\":55933},{\"end\":55956,\"start\":55952},{\"end\":55973,\"start\":55965},{\"end\":55988,\"start\":55983},{\"end\":56003,\"start\":55996},{\"end\":56022,\"start\":56012},{\"end\":56589,\"start\":56581},{\"end\":56604,\"start\":56597},{\"end\":56618,\"start\":56611},{\"end\":56637,\"start\":56629},{\"end\":56975,\"start\":56967},{\"end\":56987,\"start\":56981},{\"end\":56997,\"start\":56991},{\"end\":57383,\"start\":57375},{\"end\":57392,\"start\":57390},{\"end\":57411,\"start\":57403},{\"end\":57425,\"start\":57419},{\"end\":57448,\"start\":57440},{\"end\":57464,\"start\":57458},{\"end\":57481,\"start\":57475},{\"end\":57493,\"start\":57491},{\"end\":58045,\"start\":58041},{\"end\":58059,\"start\":58055},{\"end\":58072,\"start\":58068},{\"end\":58083,\"start\":58079},{\"end\":58845,\"start\":58835},{\"end\":58864,\"start\":58856},{\"end\":59391,\"start\":59385},{\"end\":59403,\"start\":59395},{\"end\":59414,\"start\":59409},{\"end\":59740,\"start\":59735},{\"end\":59756,\"start\":59748},{\"end\":59769,\"start\":59764},{\"end\":59783,\"start\":59776},{\"end\":59799,\"start\":59793},{\"end\":59815,\"start\":59809},{\"end\":60207,\"start\":60204},{\"end\":60221,\"start\":60215},{\"end\":60786,\"start\":60780},{\"end\":60802,\"start\":60797},{\"end\":60814,\"start\":60811},{\"end\":60834,\"start\":60825},{\"end\":61417,\"start\":61411},{\"end\":61429,\"start\":61425},{\"end\":61443,\"start\":61437},{\"end\":61448,\"start\":61445},{\"end\":61891,\"start\":61887},{\"end\":61901,\"start\":61898},{\"end\":61912,\"start\":61908},{\"end\":61922,\"start\":61918},{\"end\":61938,\"start\":61934},{\"end\":61949,\"start\":61945},{\"end\":61962,\"start\":61958},{\"end\":61972,\"start\":61969},{\"end\":61982,\"start\":61979},{\"end\":61995,\"start\":61990},{\"end\":62006,\"start\":62002},{\"end\":62392,\"start\":62390},{\"end\":62406,\"start\":62401},{\"end\":62419,\"start\":62416},{\"end\":62736,\"start\":62733},{\"end\":62751,\"start\":62746},{\"end\":62771,\"start\":62758},{\"end\":63226,\"start\":63221},{\"end\":63241,\"start\":63235},{\"end\":63258,\"start\":63251},{\"end\":63696,\"start\":63679},{\"end\":63714,\"start\":63705},{\"end\":63732,\"start\":63721},{\"end\":63739,\"start\":63737},{\"end\":63753,\"start\":63746},{\"end\":63768,\"start\":63762},{\"end\":63775,\"start\":63770},{\"end\":64195,\"start\":64190},{\"end\":64210,\"start\":64207},{\"end\":64226,\"start\":64221},{\"end\":64236,\"start\":64234},{\"end\":64254,\"start\":64246},{\"end\":64655,\"start\":64650},{\"end\":64669,\"start\":64664},{\"end\":64684,\"start\":64680},{\"end\":64693,\"start\":64690},{\"end\":64707,\"start\":64704},{\"end\":64722,\"start\":64717},{\"end\":65404,\"start\":65401},{\"end\":65413,\"start\":65411},{\"end\":65427,\"start\":65423},{\"end\":65877,\"start\":65874},{\"end\":65886,\"start\":65881},{\"end\":65892,\"start\":65890},{\"end\":65898,\"start\":65896},{\"end\":65905,\"start\":65902},{\"end\":66237,\"start\":66231},{\"end\":66251,\"start\":66249},{\"end\":66266,\"start\":66261},{\"end\":66287,\"start\":66278},{\"end\":66306,\"start\":66294},{\"end\":66650,\"start\":66643},{\"end\":66665,\"start\":66658},{\"end\":67216,\"start\":67208},{\"end\":67230,\"start\":67224},{\"end\":67234,\"start\":67232},{\"end\":67429,\"start\":67416},{\"end\":67440,\"start\":67436},{\"end\":67450,\"start\":67448},{\"end\":68042,\"start\":68033},{\"end\":68057,\"start\":68052},{\"end\":68067,\"start\":68064},{\"end\":68080,\"start\":68074},{\"end\":68897,\"start\":68894},{\"end\":68914,\"start\":68908},{\"end\":68924,\"start\":68920},{\"end\":68939,\"start\":68933},{\"end\":68947,\"start\":68941},{\"end\":69515,\"start\":69512},{\"end\":69528,\"start\":69525},{\"end\":69551,\"start\":69542},{\"end\":69565,\"start\":69561},{\"end\":69580,\"start\":69575},{\"end\":70157,\"start\":70155},{\"end\":70168,\"start\":70166},{\"end\":70181,\"start\":70178},{\"end\":70196,\"start\":70193},{\"end\":70606,\"start\":70603},{\"end\":70617,\"start\":70614},{\"end\":70631,\"start\":70627},{\"end\":70643,\"start\":70638},{\"end\":70663,\"start\":70650},{\"end\":71099,\"start\":71093},{\"end\":71110,\"start\":71105},{\"end\":71124,\"start\":71120},{\"end\":71140,\"start\":71133},{\"end\":71615,\"start\":71611},{\"end\":71629,\"start\":71621},{\"end\":71640,\"start\":71633},{\"end\":72037,\"start\":72035},{\"end\":72047,\"start\":72044},{\"end\":72063,\"start\":72056},{\"end\":72073,\"start\":72070},{\"end\":72087,\"start\":72083},{\"end\":72102,\"start\":72096},{\"end\":72553,\"start\":72547},{\"end\":72565,\"start\":72559},{\"end\":72578,\"start\":72569},{\"end\":72590,\"start\":72584},{\"end\":72602,\"start\":72596},{\"end\":73235,\"start\":73222},{\"end\":73249,\"start\":73243},{\"end\":73257,\"start\":73251},{\"end\":73521,\"start\":73513},{\"end\":73536,\"start\":73533},{\"end\":73783,\"start\":73774},{\"end\":73799,\"start\":73793},{\"end\":74222,\"start\":74212},{\"end\":74249,\"start\":74233},{\"end\":74548,\"start\":74542},{\"end\":74571,\"start\":74565},{\"end\":74585,\"start\":74581},{\"end\":74604,\"start\":74595},{\"end\":74617,\"start\":74611},{\"end\":74633,\"start\":74626},{\"end\":75128,\"start\":75121},{\"end\":75147,\"start\":75137},{\"end\":75160,\"start\":75157},{\"end\":75180,\"start\":75170},{\"end\":75657,\"start\":75634},{\"end\":75662,\"start\":75659},{\"end\":76253,\"start\":76246},{\"end\":76269,\"start\":76261},{\"end\":76498,\"start\":76494},{\"end\":76515,\"start\":76505},{\"end\":76534,\"start\":76524},{\"end\":76550,\"start\":76544},{\"end\":76567,\"start\":76561},{\"end\":76582,\"start\":76577},{\"end\":76597,\"start\":76590},{\"end\":76980,\"start\":76975},{\"end\":76994,\"start\":76990},{\"end\":77009,\"start\":77004},{\"end\":77029,\"start\":77021},{\"end\":77036,\"start\":77031},{\"end\":77633,\"start\":77629},{\"end\":77648,\"start\":77643},{\"end\":77661,\"start\":77657},{\"end\":77671,\"start\":77669},{\"end\":77684,\"start\":77680},{\"end\":77993,\"start\":77986},{\"end\":78002,\"start\":77997},{\"end\":78022,\"start\":78010},{\"end\":78029,\"start\":78024},{\"end\":78243,\"start\":78234},{\"end\":78262,\"start\":78252},{\"end\":78278,\"start\":78272},{\"end\":78721,\"start\":78717},{\"end\":78736,\"start\":78729},{\"end\":78747,\"start\":78742},{\"end\":78759,\"start\":78749},{\"end\":79290,\"start\":79286},{\"end\":79306,\"start\":79298},{\"end\":79887,\"start\":79879},{\"end\":79904,\"start\":79897},{\"end\":79912,\"start\":79910},{\"end\":79925,\"start\":79919},{\"end\":80304,\"start\":80302},{\"end\":80317,\"start\":80310},{\"end\":80600,\"start\":80598},{\"end\":80608,\"start\":80606},{\"end\":80621,\"start\":80614},{\"end\":80985,\"start\":80979},{\"end\":80999,\"start\":80995},{\"end\":81014,\"start\":81006},{\"end\":81661,\"start\":81652},{\"end\":81669,\"start\":81667},{\"end\":81683,\"start\":81679},{\"end\":81702,\"start\":81695},{\"end\":81719,\"start\":81713},{\"end\":82223,\"start\":82219},{\"end\":82237,\"start\":82231},{\"end\":82254,\"start\":82250},{\"end\":82748,\"start\":82744},{\"end\":82760,\"start\":82757},{\"end\":82774,\"start\":82771},{\"end\":82783,\"start\":82780},{\"end\":83122,\"start\":83118},{\"end\":83132,\"start\":83130},{\"end\":83641,\"start\":83638},{\"end\":83650,\"start\":83647},{\"end\":84345,\"start\":84342},{\"end\":84357,\"start\":84355},{\"end\":84375,\"start\":84369},{\"end\":84893,\"start\":84884},{\"end\":84908,\"start\":84900},{\"end\":84925,\"start\":84918},{\"end\":84938,\"start\":84933},{\"end\":84953,\"start\":84950},{\"end\":84964,\"start\":84961},{\"end\":84974,\"start\":84971},{\"end\":84986,\"start\":84982},{\"end\":84990,\"start\":84988},{\"end\":85685,\"start\":85681},{\"end\":85700,\"start\":85695},{\"end\":85714,\"start\":85709},{\"end\":85723,\"start\":85720},{\"end\":85733,\"start\":85728},{\"end\":85743,\"start\":85740},{\"end\":86355,\"start\":86353},{\"end\":86367,\"start\":86364},{\"end\":86379,\"start\":86376},{\"end\":86389,\"start\":86387},{\"end\":86906,\"start\":86902},{\"end\":86919,\"start\":86914},{\"end\":86937,\"start\":86931},{\"end\":86946,\"start\":86944},{\"end\":87249,\"start\":87245},{\"end\":87259,\"start\":87257},{\"end\":87268,\"start\":87265},{\"end\":87279,\"start\":87274},{\"end\":87293,\"start\":87290},{\"end\":87720,\"start\":87718},{\"end\":87736,\"start\":87729},{\"end\":87747,\"start\":87744},{\"end\":88298,\"start\":88292},{\"end\":88311,\"start\":88304},{\"end\":88597,\"start\":88594},{\"end\":88603,\"start\":88601},{\"end\":88611,\"start\":88607},{\"end\":88618,\"start\":88615},{\"end\":88626,\"start\":88622},{\"end\":88632,\"start\":88630}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5913612},\"end\":56488,\"start\":55895},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":88495107},\"end\":56870,\"start\":56490},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":21904375},\"end\":57316,\"start\":56872},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":29816509},\"end\":57932,\"start\":57318},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":235265942},\"end\":58774,\"start\":57934},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":245537466},\"end\":59322,\"start\":58776},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":69770239},\"end\":59615,\"start\":59324},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2017285},\"end\":60120,\"start\":59617},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":162168864},\"end\":60690,\"start\":60122},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52967399},\"end\":61407,\"start\":60692},{\"attributes\":{\"doi\":\"arXiv:2105.03075\",\"id\":\"b10\"},\"end\":61807,\"start\":61409},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":211171605},\"end\":62361,\"start\":61809},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":47021242},\"end\":62651,\"start\":62363},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4708148},\"end\":63143,\"start\":62653},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":202770043},\"end\":63673,\"start\":63145},{\"attributes\":{\"doi\":\"arXiv:2004.10964\",\"id\":\"b15\"},\"end\":64100,\"start\":63675},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195298550},\"end\":64597,\"start\":64102},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1147\",\"id\":\"b17\",\"matched_paper_id\":192546007},\"end\":65297,\"start\":64599},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":18198301},\"end\":65838,\"start\":65299},{\"attributes\":{\"doi\":\"10.1109/TSE.2019.2920771\",\"id\":\"b19\",\"matched_paper_id\":196201492},\"end\":66152,\"start\":65840},{\"attributes\":{\"doi\":\"arXiv:1909.09436\",\"id\":\"b20\"},\"end\":66541,\"start\":66154},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":216553223},\"end\":67160,\"start\":66543},{\"attributes\":{\"id\":\"b22\"},\"end\":67352,\"start\":67162},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14698227},\"end\":67879,\"start\":67354},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":235313645},\"end\":68806,\"start\":67881},{\"attributes\":{\"doi\":\"10.1109/ICPC.2017.24\",\"id\":\"b25\",\"matched_paper_id\":9203280},\"end\":69422,\"start\":68808},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":49867502},\"end\":70087,\"start\":69424},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4845285},\"end\":70506,\"start\":70089},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":231846746},\"end\":71005,\"start\":70508},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":224803856},\"end\":71529,\"start\":71007},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4614056},\"end\":71970,\"start\":71531},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":196621535},\"end\":72453,\"start\":71972},{\"attributes\":{\"doi\":\"10.1109/ASE.2011.6100062\",\"id\":\"b32\",\"matched_paper_id\":14795266},\"end\":73122,\"start\":72455},{\"attributes\":{\"doi\":\"arXiv:2105.01691\",\"id\":\"b33\"},\"end\":73473,\"start\":73124},{\"attributes\":{\"doi\":\"10.48550/ARXIV.1901.04085\",\"id\":\"b34\"},\"end\":73676,\"start\":73475},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":237489537},\"end\":74121,\"start\":73678},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":246206099},\"end\":74484,\"start\":74123},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":219627145},\"end\":75061,\"start\":74486},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2817846},\"end\":75556,\"start\":75063},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":51906708},\"end\":76239,\"start\":75558},{\"attributes\":{\"doi\":\"arXiv:1908.10084\",\"id\":\"b40\"},\"end\":76483,\"start\":76241},{\"attributes\":{\"doi\":\"arXiv:2102.03300\",\"id\":\"b41\"},\"end\":76904,\"start\":76485},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":2134475},\"end\":77514,\"start\":76906},{\"attributes\":{\"doi\":\"arXiv:2009.13818\",\"id\":\"b43\"},\"end\":77935,\"start\":77516},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":236096559},\"end\":78196,\"start\":77937},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":5832984},\"end\":78628,\"start\":78198},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":5893582},\"end\":79224,\"start\":78630},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":226274248},\"end\":79818,\"start\":79226},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":218487168},\"end\":80296,\"start\":79820},{\"attributes\":{\"doi\":\"arXiv:2108.09847\",\"id\":\"b49\"},\"end\":80516,\"start\":80298},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":208006436},\"end\":80849,\"start\":80518},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":166227909},\"end\":81563,\"start\":80851},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":7416179},\"end\":82139,\"start\":81565},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":14469144},\"end\":82674,\"start\":82141},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":69422210},\"end\":83007,\"start\":82676},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":2087154},\"end\":83536,\"start\":83009},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":59523656},\"end\":84291,\"start\":83538},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":13048843},\"end\":84798,\"start\":84293},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":250975369},\"end\":85579,\"start\":84800},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":12319675},\"end\":86228,\"start\":85581},{\"attributes\":{\"id\":\"b60\"},\"end\":86805,\"start\":86230},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":54445369},\"end\":87187,\"start\":86807},{\"attributes\":{\"doi\":\"10.1109/QRS.2015.14\",\"id\":\"b62\",\"matched_paper_id\":17864628},\"end\":87640,\"start\":87189},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":2361519},\"end\":88206,\"start\":87642},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":236548375},\"end\":88519,\"start\":88208},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":53373051},\"end\":88863,\"start\":88521}]", "bib_title": "[{\"end\":55923,\"start\":55895},{\"end\":56573,\"start\":56490},{\"end\":56963,\"start\":56872},{\"end\":57367,\"start\":57318},{\"end\":58033,\"start\":57934},{\"end\":58823,\"start\":58776},{\"end\":59379,\"start\":59324},{\"end\":59715,\"start\":59617},{\"end\":60195,\"start\":60122},{\"end\":60772,\"start\":60692},{\"end\":61876,\"start\":61809},{\"end\":62379,\"start\":62363},{\"end\":62727,\"start\":62653},{\"end\":63213,\"start\":63145},{\"end\":64182,\"start\":64102},{\"end\":64641,\"start\":64599},{\"end\":65394,\"start\":65299},{\"end\":65870,\"start\":65840},{\"end\":66636,\"start\":66543},{\"end\":67406,\"start\":67354},{\"end\":68023,\"start\":67881},{\"end\":68884,\"start\":68808},{\"end\":69502,\"start\":69424},{\"end\":70148,\"start\":70089},{\"end\":70593,\"start\":70508},{\"end\":71076,\"start\":71007},{\"end\":71605,\"start\":71531},{\"end\":72026,\"start\":71972},{\"end\":72541,\"start\":72455},{\"end\":73765,\"start\":73678},{\"end\":74200,\"start\":74123},{\"end\":74532,\"start\":74486},{\"end\":75113,\"start\":75063},{\"end\":75632,\"start\":75558},{\"end\":76971,\"start\":76906},{\"end\":77977,\"start\":77937},{\"end\":78226,\"start\":78198},{\"end\":78713,\"start\":78630},{\"end\":79279,\"start\":79226},{\"end\":79870,\"start\":79820},{\"end\":80592,\"start\":80518},{\"end\":80970,\"start\":80851},{\"end\":81643,\"start\":81565},{\"end\":82208,\"start\":82141},{\"end\":82737,\"start\":82676},{\"end\":83108,\"start\":83009},{\"end\":83630,\"start\":83538},{\"end\":84335,\"start\":84293},{\"end\":84872,\"start\":84800},{\"end\":85671,\"start\":85581},{\"end\":86343,\"start\":86230},{\"end\":86896,\"start\":86807},{\"end\":87237,\"start\":87189},{\"end\":87712,\"start\":87642},{\"end\":88284,\"start\":88208},{\"end\":88590,\"start\":88521}]", "bib_author": "[{\"end\":55945,\"start\":55925},{\"end\":55958,\"start\":55945},{\"end\":55975,\"start\":55958},{\"end\":55990,\"start\":55975},{\"end\":56005,\"start\":55990},{\"end\":56024,\"start\":56005},{\"end\":56591,\"start\":56575},{\"end\":56606,\"start\":56591},{\"end\":56620,\"start\":56606},{\"end\":56639,\"start\":56620},{\"end\":56977,\"start\":56965},{\"end\":56989,\"start\":56977},{\"end\":56999,\"start\":56989},{\"end\":57385,\"start\":57369},{\"end\":57394,\"start\":57385},{\"end\":57413,\"start\":57394},{\"end\":57427,\"start\":57413},{\"end\":57450,\"start\":57427},{\"end\":57466,\"start\":57450},{\"end\":57483,\"start\":57466},{\"end\":57495,\"start\":57483},{\"end\":58047,\"start\":58035},{\"end\":58061,\"start\":58047},{\"end\":58074,\"start\":58061},{\"end\":58085,\"start\":58074},{\"end\":58847,\"start\":58825},{\"end\":58866,\"start\":58847},{\"end\":59393,\"start\":59381},{\"end\":59405,\"start\":59393},{\"end\":59416,\"start\":59405},{\"end\":59742,\"start\":59717},{\"end\":59758,\"start\":59742},{\"end\":59771,\"start\":59758},{\"end\":59785,\"start\":59771},{\"end\":59801,\"start\":59785},{\"end\":59817,\"start\":59801},{\"end\":60209,\"start\":60197},{\"end\":60223,\"start\":60209},{\"end\":60788,\"start\":60774},{\"end\":60804,\"start\":60788},{\"end\":60816,\"start\":60804},{\"end\":60836,\"start\":60816},{\"end\":61419,\"start\":61409},{\"end\":61431,\"start\":61419},{\"end\":61445,\"start\":61431},{\"end\":61450,\"start\":61445},{\"end\":61893,\"start\":61878},{\"end\":61903,\"start\":61893},{\"end\":61914,\"start\":61903},{\"end\":61924,\"start\":61914},{\"end\":61940,\"start\":61924},{\"end\":61951,\"start\":61940},{\"end\":61964,\"start\":61951},{\"end\":61974,\"start\":61964},{\"end\":61984,\"start\":61974},{\"end\":61997,\"start\":61984},{\"end\":62008,\"start\":61997},{\"end\":62394,\"start\":62381},{\"end\":62408,\"start\":62394},{\"end\":62421,\"start\":62408},{\"end\":62738,\"start\":62729},{\"end\":62753,\"start\":62738},{\"end\":62773,\"start\":62753},{\"end\":63228,\"start\":63215},{\"end\":63243,\"start\":63228},{\"end\":63260,\"start\":63243},{\"end\":63698,\"start\":63675},{\"end\":63716,\"start\":63698},{\"end\":63734,\"start\":63716},{\"end\":63741,\"start\":63734},{\"end\":63755,\"start\":63741},{\"end\":63770,\"start\":63755},{\"end\":63777,\"start\":63770},{\"end\":64197,\"start\":64184},{\"end\":64212,\"start\":64197},{\"end\":64228,\"start\":64212},{\"end\":64238,\"start\":64228},{\"end\":64256,\"start\":64238},{\"end\":64657,\"start\":64643},{\"end\":64671,\"start\":64657},{\"end\":64686,\"start\":64671},{\"end\":64695,\"start\":64686},{\"end\":64709,\"start\":64695},{\"end\":64724,\"start\":64709},{\"end\":65406,\"start\":65396},{\"end\":65415,\"start\":65406},{\"end\":65429,\"start\":65415},{\"end\":65879,\"start\":65872},{\"end\":65888,\"start\":65879},{\"end\":65894,\"start\":65888},{\"end\":65900,\"start\":65894},{\"end\":65907,\"start\":65900},{\"end\":66239,\"start\":66225},{\"end\":66253,\"start\":66239},{\"end\":66268,\"start\":66253},{\"end\":66289,\"start\":66268},{\"end\":66308,\"start\":66289},{\"end\":66652,\"start\":66638},{\"end\":66667,\"start\":66652},{\"end\":67218,\"start\":67206},{\"end\":67232,\"start\":67218},{\"end\":67236,\"start\":67232},{\"end\":67431,\"start\":67408},{\"end\":67442,\"start\":67431},{\"end\":67452,\"start\":67442},{\"end\":68044,\"start\":68025},{\"end\":68059,\"start\":68044},{\"end\":68069,\"start\":68059},{\"end\":68082,\"start\":68069},{\"end\":68899,\"start\":68886},{\"end\":68916,\"start\":68899},{\"end\":68926,\"start\":68916},{\"end\":68941,\"start\":68926},{\"end\":68949,\"start\":68941},{\"end\":69517,\"start\":69504},{\"end\":69530,\"start\":69517},{\"end\":69553,\"start\":69530},{\"end\":69567,\"start\":69553},{\"end\":69582,\"start\":69567},{\"end\":70159,\"start\":70150},{\"end\":70170,\"start\":70159},{\"end\":70183,\"start\":70170},{\"end\":70198,\"start\":70183},{\"end\":70608,\"start\":70595},{\"end\":70619,\"start\":70608},{\"end\":70633,\"start\":70619},{\"end\":70645,\"start\":70633},{\"end\":70665,\"start\":70645},{\"end\":71101,\"start\":71078},{\"end\":71112,\"start\":71101},{\"end\":71126,\"start\":71112},{\"end\":71142,\"start\":71126},{\"end\":71617,\"start\":71607},{\"end\":71631,\"start\":71617},{\"end\":71642,\"start\":71631},{\"end\":72039,\"start\":72028},{\"end\":72049,\"start\":72039},{\"end\":72065,\"start\":72049},{\"end\":72075,\"start\":72065},{\"end\":72089,\"start\":72075},{\"end\":72104,\"start\":72089},{\"end\":72555,\"start\":72543},{\"end\":72567,\"start\":72555},{\"end\":72580,\"start\":72567},{\"end\":72592,\"start\":72580},{\"end\":72604,\"start\":72592},{\"end\":73237,\"start\":73215},{\"end\":73251,\"start\":73237},{\"end\":73259,\"start\":73251},{\"end\":73523,\"start\":73505},{\"end\":73538,\"start\":73523},{\"end\":73785,\"start\":73767},{\"end\":73801,\"start\":73785},{\"end\":74224,\"start\":74202},{\"end\":74251,\"start\":74224},{\"end\":74550,\"start\":74534},{\"end\":74573,\"start\":74550},{\"end\":74587,\"start\":74573},{\"end\":74606,\"start\":74587},{\"end\":74619,\"start\":74606},{\"end\":74635,\"start\":74619},{\"end\":75130,\"start\":75115},{\"end\":75149,\"start\":75130},{\"end\":75162,\"start\":75149},{\"end\":75182,\"start\":75162},{\"end\":75659,\"start\":75634},{\"end\":75664,\"start\":75659},{\"end\":76255,\"start\":76241},{\"end\":76271,\"start\":76255},{\"end\":76500,\"start\":76485},{\"end\":76517,\"start\":76500},{\"end\":76536,\"start\":76517},{\"end\":76552,\"start\":76536},{\"end\":76569,\"start\":76552},{\"end\":76584,\"start\":76569},{\"end\":76599,\"start\":76584},{\"end\":76982,\"start\":76973},{\"end\":76996,\"start\":76982},{\"end\":77011,\"start\":76996},{\"end\":77031,\"start\":77011},{\"end\":77038,\"start\":77031},{\"end\":77635,\"start\":77621},{\"end\":77650,\"start\":77635},{\"end\":77663,\"start\":77650},{\"end\":77673,\"start\":77663},{\"end\":77686,\"start\":77673},{\"end\":77995,\"start\":77979},{\"end\":78004,\"start\":77995},{\"end\":78024,\"start\":78004},{\"end\":78031,\"start\":78024},{\"end\":78245,\"start\":78228},{\"end\":78264,\"start\":78245},{\"end\":78280,\"start\":78264},{\"end\":78723,\"start\":78715},{\"end\":78738,\"start\":78723},{\"end\":78749,\"start\":78738},{\"end\":78761,\"start\":78749},{\"end\":79292,\"start\":79281},{\"end\":79308,\"start\":79292},{\"end\":79889,\"start\":79872},{\"end\":79906,\"start\":79889},{\"end\":79914,\"start\":79906},{\"end\":79927,\"start\":79914},{\"end\":80306,\"start\":80298},{\"end\":80319,\"start\":80306},{\"end\":80602,\"start\":80594},{\"end\":80610,\"start\":80602},{\"end\":80623,\"start\":80610},{\"end\":80987,\"start\":80972},{\"end\":81001,\"start\":80987},{\"end\":81016,\"start\":81001},{\"end\":81663,\"start\":81645},{\"end\":81671,\"start\":81663},{\"end\":81685,\"start\":81671},{\"end\":81704,\"start\":81685},{\"end\":81721,\"start\":81704},{\"end\":82225,\"start\":82210},{\"end\":82239,\"start\":82225},{\"end\":82256,\"start\":82239},{\"end\":82750,\"start\":82739},{\"end\":82762,\"start\":82750},{\"end\":82776,\"start\":82762},{\"end\":82785,\"start\":82776},{\"end\":83124,\"start\":83110},{\"end\":83134,\"start\":83124},{\"end\":83643,\"start\":83632},{\"end\":83652,\"start\":83643},{\"end\":84347,\"start\":84337},{\"end\":84359,\"start\":84347},{\"end\":84377,\"start\":84359},{\"end\":84895,\"start\":84874},{\"end\":84910,\"start\":84895},{\"end\":84927,\"start\":84910},{\"end\":84940,\"start\":84927},{\"end\":84955,\"start\":84940},{\"end\":84966,\"start\":84955},{\"end\":84976,\"start\":84966},{\"end\":84988,\"start\":84976},{\"end\":84992,\"start\":84988},{\"end\":85687,\"start\":85673},{\"end\":85702,\"start\":85687},{\"end\":85716,\"start\":85702},{\"end\":85725,\"start\":85716},{\"end\":85735,\"start\":85725},{\"end\":85745,\"start\":85735},{\"end\":86357,\"start\":86345},{\"end\":86369,\"start\":86357},{\"end\":86381,\"start\":86369},{\"end\":86391,\"start\":86381},{\"end\":86908,\"start\":86898},{\"end\":86921,\"start\":86908},{\"end\":86939,\"start\":86921},{\"end\":86948,\"start\":86939},{\"end\":87251,\"start\":87239},{\"end\":87261,\"start\":87251},{\"end\":87270,\"start\":87261},{\"end\":87281,\"start\":87270},{\"end\":87295,\"start\":87281},{\"end\":87722,\"start\":87714},{\"end\":87738,\"start\":87722},{\"end\":87749,\"start\":87738},{\"end\":88300,\"start\":88286},{\"end\":88313,\"start\":88300},{\"end\":88599,\"start\":88592},{\"end\":88605,\"start\":88599},{\"end\":88613,\"start\":88605},{\"end\":88620,\"start\":88613},{\"end\":88628,\"start\":88620},{\"end\":88634,\"start\":88628}]", "bib_venue": "[{\"end\":56122,\"start\":56024},{\"end\":56670,\"start\":56639},{\"end\":57079,\"start\":56999},{\"end\":57576,\"start\":57495},{\"end\":58247,\"start\":58085},{\"end\":58949,\"start\":58866},{\"end\":59457,\"start\":59416},{\"end\":59858,\"start\":59817},{\"end\":60345,\"start\":60223},{\"end\":60978,\"start\":60836},{\"end\":61588,\"start\":61466},{\"end\":62077,\"start\":62008},{\"end\":62494,\"start\":62421},{\"end\":62856,\"start\":62773},{\"end\":63309,\"start\":63260},{\"end\":63865,\"start\":63793},{\"end\":64337,\"start\":64256},{\"end\":64831,\"start\":64744},{\"end\":65521,\"start\":65429},{\"end\":65972,\"start\":65931},{\"end\":66223,\"start\":66154},{\"end\":66790,\"start\":66667},{\"end\":67204,\"start\":67162},{\"end\":67543,\"start\":67452},{\"end\":68244,\"start\":68082},{\"end\":69053,\"start\":68969},{\"end\":69674,\"start\":69582},{\"end\":70283,\"start\":70198},{\"end\":70739,\"start\":70665},{\"end\":71225,\"start\":71142},{\"end\":71733,\"start\":71642},{\"end\":72163,\"start\":72104},{\"end\":72719,\"start\":72628},{\"end\":73213,\"start\":73124},{\"end\":73503,\"start\":73475},{\"end\":73879,\"start\":73801},{\"end\":74292,\"start\":74251},{\"end\":74727,\"start\":74635},{\"end\":75265,\"start\":75182},{\"end\":75811,\"start\":75664},{\"end\":76349,\"start\":76287},{\"end\":76681,\"start\":76615},{\"end\":77129,\"start\":77038},{\"end\":77619,\"start\":77516},{\"end\":78050,\"start\":78031},{\"end\":78368,\"start\":78280},{\"end\":78868,\"start\":78761},{\"end\":79450,\"start\":79308},{\"end\":80014,\"start\":79927},{\"end\":80385,\"start\":80335},{\"end\":80664,\"start\":80623},{\"end\":81127,\"start\":81016},{\"end\":81802,\"start\":81721},{\"end\":82336,\"start\":82256},{\"end\":82826,\"start\":82785},{\"end\":83207,\"start\":83134},{\"end\":83827,\"start\":83652},{\"end\":84468,\"start\":84377},{\"end\":85115,\"start\":84992},{\"end\":85848,\"start\":85745},{\"end\":86470,\"start\":86391},{\"end\":86983,\"start\":86948},{\"end\":87394,\"start\":87314},{\"end\":87847,\"start\":87749},{\"end\":88354,\"start\":88313},{\"end\":88675,\"start\":88634},{\"end\":56207,\"start\":56124},{\"end\":57644,\"start\":57578},{\"end\":58396,\"start\":58249},{\"end\":59036,\"start\":58951},{\"end\":60454,\"start\":60347},{\"end\":61107,\"start\":60980},{\"end\":62926,\"start\":62858},{\"end\":64920,\"start\":64833},{\"end\":65600,\"start\":65523},{\"end\":66900,\"start\":66792},{\"end\":67637,\"start\":67545},{\"end\":68393,\"start\":68246},{\"end\":69124,\"start\":69055},{\"end\":69775,\"start\":69676},{\"end\":71295,\"start\":71227},{\"end\":72209,\"start\":72165},{\"end\":72797,\"start\":72721},{\"end\":74806,\"start\":74729},{\"end\":75335,\"start\":75267},{\"end\":75945,\"start\":75813},{\"end\":77230,\"start\":77131},{\"end\":78443,\"start\":78370},{\"end\":78962,\"start\":78870},{\"end\":79579,\"start\":79452},{\"end\":80088,\"start\":80016},{\"end\":81238,\"start\":81129},{\"end\":81870,\"start\":81804},{\"end\":82421,\"start\":82338},{\"end\":83283,\"start\":83209},{\"end\":83989,\"start\":83829},{\"end\":84566,\"start\":84470},{\"end\":85225,\"start\":85117},{\"end\":85938,\"start\":85850},{\"end\":86536,\"start\":86472},{\"end\":87948,\"start\":87849}]"}}}, "year": 2023, "month": 12, "day": 17}