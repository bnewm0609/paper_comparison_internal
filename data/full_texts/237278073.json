{"id": 237278073, "updated": "2023-10-05 23:55:16.221", "metadata": {"title": "Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density", "authors": "[{\"first\":\"Kuniaki\",\"last\":\"Saito\",\"middle\":[]},{\"first\":\"Donghyun\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Piotr\",\"last\":\"Teterwak\",\"middle\":[]},{\"first\":\"Stan\",\"last\":\"Sclaroff\",\"middle\":[]},{\"first\":\"Trevor\",\"last\":\"Darrell\",\"middle\":[]},{\"first\":\"Kate\",\"last\":\"Saenko\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 8, "day": 24}, "abstract": "Unsupervised domain adaptation (UDA) methods can dramatically improve generalization on unlabeled target domains. However, optimal hyper-parameter selection is critical to achieving high accuracy and avoiding negative transfer. Supervised hyper-parameter validation is not possible without labeled target data, which raises the question: How can we validate unsupervised adaptation techniques in a realistic way? We first empirically analyze existing criteria and demonstrate that they are not very effective for tuning hyper-parameters. Intuitively, a well-trained source classifier should embed target samples of the same class nearby, forming dense neighborhoods in feature space. Based on this assumption, we propose a novel unsupervised validation criterion that measures the density of soft neighborhoods by computing the entropy of the similarity distribution between points. Our criterion is simpler than competing validation methods, yet more effective; it can tune hyper-parameters and the number of training iterations in both image classification and semantic segmentation models. The code used for the paper will be available at \\url{https://github.com/VisionLearningGroup/SND}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.10860", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/SaitoKTSDS21", "doi": "10.1109/iccv48922.2021.00905"}}, "content": {"source": {"pdf_hash": "e0c9464d69ad1a54450e0ba20c7c7fd95806b828", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.10860v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6291a58964f73c3d0ef05321680d21a5fab33d40", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e0c9464d69ad1a54450e0ba20c7c7fd95806b828.txt", "contents": "\nTune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density\n\n\nKuniaki Saito [keisaito@bu.edu \nBoston University\n\n\nDonghyun Kim \nBoston University\n\n\nPiotr Teterwak piotrt@bu.edu \nBoston University\n\n\nStan Sclaroff sclaroff@bu.edu \nBoston University\n\n\nTrevor Darrell trevor@eecs.berkeley.edu \nUniversity of California\nBerkeley\n\nKate Saenko saenko]@bu.edu \nBoston University\n\n\nMIT-IBM Watson AI Lab\n\n\nTune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density\n\nUnsupervised domain adaptation (UDA) methods can dramatically improve generalization on unlabeled target domains. However, optimal hyper-parameter selection is critical to achieving high accuracy and avoiding negative transfer. Supervised hyper-parameter validation is not possible without labeled target data, which raises the question: How can we validate unsupervised adaptation techniques in a realistic way? We first empirically analyze existing criteria and demonstrate that they are not very effective for tuning hyper-parameters. Intuitively, a welltrained source classifier should embed target samples of the same class nearby, forming dense neighborhoods in feature space. Based on this assumption, we propose a novel unsupervised validation criterion that measures the density of soft neighborhoods by computing the entropy of the similarity distribution between points. Our criterion is simpler than competing validation methods, yet more effective; it can tune hyper-parameters and the number of training iterations in both image classification and semantic segmentation models. The code used for the paper will be available at https://github.com/ VisionLearningGroup/SND.In this section, we show the detailed analysis of Soft Neighborhood Density and other criterion.\n\nIntroduction\n\nDeep neural networks can learn highly discriminative representations for visual recognition tasks [11,42,20,30,15], but do not generalize well to out-of-domain data [50]. To improve performance on a new target domain, Unsupervised Domain Adaptation (UDA) aims to transfer representations from a label-rich source domain without additional supervision. Recent UDA methods primarily achieve this through unsupervised learning on the target domain, e.g., by minimizing the feature distribution shift between source and target domains [12,22,44], classifier confusion [18], clustering [36], and pseudo-label based methods [59]. Promising adaptation results have been demonstrated on image classification [23,58,8,55,46], semantic segmentation [17] Figure 1: In unsupervised domain adaptation, validation is a significant and unsolved issue. Performance can be sensitive to hyper-parameters, yet no reliable validation criteria have been presented. In this work, we provide a new criterion, SND, to select proper hyper-parameters for model validation in UDA. The example shows validation of the ADVENT [53] segmentation model. and object detection [9] tasks.\n\nHowever, adaptation methods can be highly sensitive to hyper-parameters and the number of training iterations. For example, the adversarial alignment approach is popular in semantic segmentation [17,47,53], but can fail badly without careful tuning of the loss trade-off weight as shown in Fig. 1. In addition, many methods have other kinds of hyper-parameters, e.g., defining the concentration of clusters [36], the confidence threshold on target samples [59], etc. Therefore validation of hyper-parameters is an important problem in UDA, yet it has been largely overlooked. In UDA, we assume not to have access to labeled target data, thus, hyper-parameter optimization (HPO) should be done without using target labels. In this paper, we would like to pose a question crucial to making domain adaptation more practical: How can we reliably validate adaptation methods in an unsupervised way?\n\nUnsupervised validation is very challenging in practice, thus many methods do HPO in an ineffective, or even unfair, way. Evaluating accuracy (risk) in the source domain is popular [12,44,5,43,23,5,18,54,56], but this will not necessarily reflect success in the target domain. Using the risk of the target domain [40,2,3,14,37,41,33] contradicts the assumption of UDA. Many works [48,45,47,26,59,57,52,53] do not clearly describe their HPO method.\n\nTo the best of our knowledge, no comprehensive study has compared validation methods across tasks and adaptation approaches in a realistic evaluation protocol. Our first contribution is to empirically analyze these existing criteria and demonstrate that they are not very effective for HPO. This exposes a major barrier to the practical application of state-of-the-art unsupervised domain adaptation methods.\n\nTo tackle this problem, we first revisit an unsupervised validation criterion based on classifier entropy, C-Ent. If the classification model produces confident and low-entropy outputs on target samples, then the target features are discriminative and the prediction is likely reliable. Morerio et al. [25] propose to utilize C-Ent to do HPO but only evaluated it on their own adaptation method. We evaluate C-Ent extensively, across various adaptation methods, datasets, and vision tasks. We reveal that C-Ent is very effective for HPO for several adaptation methods, but also expose a critical issue in this approach, which is that it cannot detect the collapse of neighborhood structure in target samples (See Fig. 2). By neighborhood structure, we mean the relationship between samples in a feature space. Before any adaptation, target samples embedded nearby are highly likely in the same class (No adaptation in Fig. 2). A good UDA model will keep or even enhance the relationships of target samples while aligning them to the source. However, a UDA model can falsely align target samples with the source and incorrectly change the neighborhood structure ((DANN in Fig. 2). Even in this case, C-Ent can become small and choose a poorly-adapted model.\n\nNatekar et al. [27] measure the consistency of feature embeddings within a class and their discrepancy from other classes to predict the generalization of supervised models by using labeled samples. Accounting for such relationships between points is a promising way to overcome the issues of C-Ent. But, since computing these metrics requires labeled samples, we cannot directly apply this method.\n\nThis leads us to propose a novel unsupervised validation criterion that considers the neighborhood density of the unlabeled target. Our notion of a neighborhood is soft, i.e. we do not form explicit clusters as part of our metric computation. Rather, we define soft neighborhoods of a point using the distribution of its similarity to other points, and measure density as the entropy of this distribution. We assume that a well-trained source model will embed target samples of the same class nearby and thus will form dense implicit neighborhoods. The consistency of representations within each neighborhood should be preserved or even enhanced by a well-adapted model. Monitoring the density thus enables us to detect whether a model causes the collapse of implicit\n\n\nMethod\n\nTechnical Advantages Stability w/ X t w/o X s , Y s w/o HP across methods\n\n\nSource Risk\n\nTest split IWV [43,56] +Density Model Entropy [25] SND (Ours) Xt denotes unlabeled target, and (Xs, Ys) denote labeled source samples. SND computes a score on unlabeled target samples. Empirically, we verify that our method is stable across different datasets, methods, and tasks. neighborhood structure as shown in Fig. 2.\n\nOur proposed metric, called Soft Neighborhood Density (SND), is simple yet more effective than competing validation methods. Rather than focusing on the source and target relationship (like IWV [43] or DEV [56]), we measure the discriminability of target features by computing neighborhood density and choosing a model that maximizes it.\n\nWe demonstrate that target accuracy is consistent with our criterion in many cases. Empirically, we observe that SND works well for closed and partial domain adaptive image classification, as well as for domain adaptive semantic segmentation. SND is even effective in choosing a suitable source domain given an unlabeled target domain.\n\nOur contributions are summarized as follows:\n\n\u2022 We re-evaluate existing criteria for UDA and call for more practical validation of DA approaches. \u2022 We propose Soft Neighborhood Density metric which considers target neighborhood structure, improves upon class entropy (C-Ent) and achieves performance close to optimal (supervised) HPO in 80% cases on closed, partial DA, and domain adaptive semantic segmentation.\n\n\nRelated Work\n\nDomain Adaptation aims to transfer knowledge from a labeled source domain to a label-scarce target domain. Its application in vision is diverse: image classification, semantic segmentation [17,47], and object detection [9]. One popular approach in DA is based on distribution matching by adversarial learning [12,49,23]. Adversarial adaptation seeks to minimize an approximate domain discrepancy distance through an adversarial objective with respect to a domain discriminator. Recently, some techniques that utilize clustering or a variant of entropy minimization have been developed [36,19,46,18]. [46,19] propose to train a model by minimizing inter-class discrepancy given the number of classes. SND computes the density of local neighborhoods and selects a model with the largest density, which allows us to select a good model without knowing the number of classes in the target. All of the existing approaches have important hyper-parameters to be tuned, such as the tradeoff parameter between source classification and adaptation loss. Another important hyper-parameter is softmax temperature [36,18]. Given a specific target domain, the selection of a source domain is also important. If we have to choose only a single source domain, the selection process is crucial for the model's performance. However, it is questionable whether existing approaches do HPO in a realistic way.\n\nValidation Methods for UDA. In Table 1, we summarize several prior validation methods that do not need any target labeled samples. The validation methods themselves can have hyper-parameters (HP) and other requirements. Source Risk. Ganin et al. [12] considers the source risk to select hyper-parameters. But, the source risk is not a good estimator of the target risk in the presence of a large domain gap. Importance Weighted Validation (IWV) and DEV. Sugiyama et al. [43] and You et al. [56] validate methods using the risk of source samples. If a source sample is very similar to target samples, the risk on the source sample is heavily weighted. This approach has a similar issue to Source Risk validation. Since DEV is developed to control the variance in IWV, we use it as a baseline in experiments. Entropy (C-Ent). Morerio et al. [25] employ entropy of classification output. If the model has confident predictions in classifying target samples, the hyper-parameter is considered appropriate. The method is simple and does not require validation with labeled samples. But, Morerio et al. [25] apply the C-Ent criterion only to tune their proposed model, which makes its applicability to diverse methods unclear. We extensively evaluate this approach and reveal that while it is often useful, it has a critical failure mode. This failure mode is that domain adaptation models can output confidently incorrect predictions for target samples. In comparison, we empirically show that Soft Neighborhood Density gives the most stable results across different datasets and methods for both image classification and semantic segmentation.\n\nLocally Linear Embedding (LLE). Roweis et al. [32] compute low dimensional and neighborhood-preserving embeddings of high dimensional data. The neighborhoodpreserving embeddings recover global nonlinear structure. We aim to pick a model by monitoring the density of implicit neighborhoods during adaptation.\n\n\nApproach\n\nProblem Setting. In UDA, we are provided labeled source data D s = {(x s i , y i s )} Ns i=1 and unlabeled target data\nD t = {(x t i )} Nt i=1\n. Generally, domain adaptation methods optimize the following loss,\nL = L s (x s , y s ) + \u03bbL adapt (x s , x t , \u03b7),(1)\nwhere L s is the classification loss for source samples and L adapt is the adaptation loss computed on target samples. \u03bb controls the trade-off between source classification and adaptation loss. \u03b7 are hyper-parameters used to compute the adaptation loss. Our goal is to build a criterion that can tune \u03bb and \u03b7. We additionally aim to choose the best training iteration since a model can be sensitive to its choice.\n\nAssumptions. We assume that target samples embedded nearby should belong to the same class. Therefore, representations of a well-adapted model should result in dense neighborhoods of highly-similar points. We express density as the consistency of representations within each soft neighborhood, such that Soft Neighborhood Density becomes large with a well-adapted model. Models are trained with labeled source samples as well as unlabeled target samples. Since source and target are related domains, the model will have somewhat discriminative features for target samples. Before adaptation, such features will define initial neighborhoods, namely, samples with higher similarity to each other relative to the rest of the points. If the selection of a method and hyper-parameter is appropriate, such neighborhoods should be preserved rather than split into smaller clusters, and the similarity of features within soft neighborhoods should increase.\n\nMotivation. First, we empirically show the necessity of considering the neighborhood structure. Class Entropy (C-Ent) [25] measures the confidence in prediction. Through domain-adaptive training with inappropriate hyper-parameters, the confidence in the target prediction can increase while drastically changing the neighborhoods from the initial neighborhood structure.\n\nA toy example ( Fig. 2) clarifies this idea. We generate source data from two Gaussian distributions with different which is used to select hyper-parameters of the adaptation model. We first extract features from the softmax layer for all target samples. We then compute the similarity distribution from the pair-wise similarities of the features (red boxes highlight similar points). Finally we use the entropy of the similarity distribution as our evaluation criterion (SND), where the higher its value, the better. means, which we regard as two classes. Then, we obtain target data by shifting the mean of one of the Gaussians. We train two-layered neural networks in two ways: training with distribution alignment (DANN [12], \u03bb = 1.0) and training with only source samples (\u03bb = 0). From the input space, the target samples should not be aligned with source class 1 (green circles). But, the DANN model aligns target samples incorrectly and confidently misclassifies many target samples, resulting in a very small C-Ent. If we employ C-Ent as a criterion, we will select the left model which performs poorly. But, if we are able to consider whether the density of neighborhoods is maintained or increased, we can avoid selecting the poor model.\n\nIn this toy example, we have utilized the partial DA setting [5], where the target label set is the subset of the source, and DANN [12] to illustrate the issue of C-Ent. In fact, this kind of wrong alignment can happen in real data when we use distribution alignment (Fig. 4a).\n\n\nSoft Neighborhood Density (SND)\n\nWe aim to design a criterion that can consider the density of implicit local neighborhoods in the target domain. We define a soft neighborhood for each sample by computing its similarity to other samples in the dataset, and converting it to a probability distribution. This is done via a softmax activation with temperature-scaling to ignore samples outside of the local neighborhood. Once we define the soft neighborhoods, we can estimate their density by computing the entropy of the distribution. The overall pipeline in Fig. 3 consists of 1) computing the similarity between samples, 2) applying the softmax activation with temperaturescaling (identifying soft neighborhoods), and 3) calculating soft neighborhood density.\n\nSimilarity Computation. We first compute similarity between target samples, S \u2208 R Nt\u00d7Nt , where N t denotes the number of target samples.\nLet S ij = f t i , f t j , where f t i denotes a L 2 normalized target feature for input x t i .\nWe ignore diagonal elements of S because our goal is to compute the distances to neighbors for each sample. This matrix defines distances, but it is not clear which samples are relatively close to each other vs. far away just from this matrix.\n\nSoft Neighborhoods. To highlight the difference between nearby points and far-away points, we convert the similarity between samples into a probabilistic distribution, P , using temperature scaling and the softmax function,\nP ij = exp(S ij /\u03c4 ) j exp(S ij /\u03c4 ) ,(2)\nwhere \u03c4 is a temperature constant. Note that the temperature scaling and the exponential function have the ability to enlarge the difference between similarity values S ij . Therefore, if a sample j is relatively dissimilar from i, the value of P ij will be very small, which allows us to ignore distant samples for the sample i. The temperature is the key to implicitly identifying neighborhoods; we set it to 0.05 across all experiments given the results of the toy dataset. Soft Neighborhood Density. Next, we design a metric to consider the neighborhood density given P . The metric needs to evaluate the consistency of representations within the implicit neighborhoods. If a model extracts ideally discriminative features, the representations within a neighborhood are identical. To identify this situation, we propose to compute the entropy of P . For example, if the entropy of P i is large, the probabilistic distribution should be uniform across the soft neighborhoods; that is, neighbors of the sample i are concentrated into very similar points. Specifically, we compute the entropy for each sample and take the average of all samples as our final metric:\nH(P ) = \u2212 1 N t Nt i=1 Nt j=1 P ij log P ij .(3)\nWe then choose the model that has the highest entropy of all candidate models. If a model falsely separates samples into clusters as in Fig. 2, the entropy becomes small.\n\nInput Features. The key to the success of our method is extracting implicit neighborhoods. Ideally, all samples in the same class should be embedded near each other. In that case, Eq. 3 can compute the density of each class. Hence, we need to use class-discriminative features, and the choice of features can be essential. We propose to employ the classification softmax output as our input feature vec-tor f . Since this feature represents class-specific information, it is the most likely to place target samples of the same class together. As we analyze in experiments, this feature has smaller within-class variance than either middle-layer features or classification output without softmax activation. Therefore, our hope is that the number of clusters in Eq. 2 should be equal to (closed set case) or less than (partial set case) the number of source classes. In the ideal case, the computed density should be close to the within-class density.\n\nDiscussion. Note that we assume that the classifier is well-trained on source samples. The criterion becomes very large if a model produces the same output for all target samples. To avoid this, the model needs to be well trained on source samples, which can be easily monitored by seeing the loss on source training samples.\n\nAlso, note that our assumption is that samples embedded nearby are likely to share the category labels. This is consistent with an assumption made by general domain adaptation methods [1]. If a pre-trained model provides very poor representations or the source domain is too different from the target, the assumption will not be satisfied. Under this setting, any metrics of UDA will not be a good tool to monitor the training.\n\nExtension to Semantic Segmentation. In semantic segmentation, the outputs for each image can be as large as one million pixels. When the number of target samples is that large, the computation of the similarity matrix can be very expensive. In order to make the computation of SND more efficient, we subsample the target samples to make the similarity graph smaller and easier to compute. In our experiments, we randomly sample a hundred pixels for each image and compute SND for each, then, take the average of all target images. Although the method is straightforward, our experiments show that the resulting approximation of SND continues to be an effective criterion for selecting hyper-parameters.\n\n\nExperiments\n\nFirst, we evaluate the existing metrics and SND to choose hyper-parameters in domain adaptation for image classification and semantic segmentation. Second, we show experiments to analyze the characteristics of our method.\n\nWe evaluate the ability to choose suitable hyperparameters including checkpoints (i.e., training iterations) for unsupervised domain adaptation. Closed DA (CDA) assumes that the source and target domain share the same label set while partial DA (PDA) assumes that the target label set is the subset of the source. We perform experiments on both adaptation scenarios. The details of semantic segmentation are described in the appendix. The general design of the experiment is similar to image classification.\n\n\nAdaptation Methods\n\nFor each method, we select the hyper-parameter mentioned below, plus a training iteration. See appendix for other hyper-parameters used in experiments.\n\nAdversarial Alignment. As a representative method of domain alignment, we use CDAN [23]. We select a weight of the trade-off between domain confusion loss and source loss (\u03bb = 0.1, 0.3, 0.5, 1.0 ,1.5, with \u03bb = 1.0 as its default setting). To analyze the behavior in detail, we also utilize DANN [12] for OfficeHome PDA. The validation is done in the same way as CDAN.\n\nClustering. As a clustering-based DA method, we utilize Neighborhood Clustering (NC) [36]. NC performs clustering using similarity between features and a temperature is used to compute the similarity distribution. Since the selection of the temperature value can affect the performance, we evaluate criteria to choose the best temperature (\u03b7 = 0.5, 0.8, 1.0, 1.5, with \u03b7 = 1.0 as its default setting).\n\nClassifier Confusion. As a recent state-of-the art method, we employ MCC [18], where a temperature is used to compute the classifier's confusion loss. The goal is to tune the temperature values (\u03b7 = 1.5, 2.0, 2.5, 3.0, 3.5, with \u03b7 = 2.5 as its default setting).\n\nPseudo Labeling (PL). Employing pseudo labels [21] is one of the popular approaches in DA [58,59,37]. One important hyper-parameter is a threshold to select confident target samples. If the output probability for a predicted class is larger than the threshold, we use the sample for training. The optimal threshold may be different for different datasets. Then, our goal is to tune the threshold values (\u03b7 = 0.5, 0.7, 0.8, 0.9, 0.95).\n\nSemantic Segmentation. AdaptSeg [47] and AD-VENT [53] are employed. For both methods, the goal is to tune both trade-off parameters of adversarial alignment loss (\u03bb) and training iterations.\n\n\nSetup\n\nDatasets. For image classification, we use Office [34] (Amazon to DSLR (A2D) and Webcam to Amazon (W2A) adaptation) with 31 categories and OfficeHome [51] (Real to Art (R2A), Art to Product (A2P) and Product to Clipart (P2C)) with 65 categories. Office is used for CDA while we use OfficeHome for both CDA and PDA. For the category split of PDA, we follow [6]. To further demonstrate applicability to large-scale datasets, we evaluate SND on VisDA [29] and DomainNet [28] in CDA. We describe the detail in the appendix. In semantic segmentation, we use GTA5 [31] as a source and Cityscape [10] as a target domain.\n\nBaselines. Entropy [25] directly employs the entropy of the classification output. It takes the average of all samples. A smaller value should indicate a better-adapted model. For DEV [56], we need to have held-out validation source samples. Since holding out many source samples can   degrade the accuracy of adapted models, we take 3 source samples per class as validation sets. Increasing the number of source validation samples to more than 3 per class does not much improve the validation performance. See the appendix for more detail. A smaller risk represents a better model. Similarly, Source Risk is measured on the source validation samples. We also report lower bound and upper bound performance among all hyper-parameters. Evaluation Protocol. In image classification, we train all adaptation methods for 10,000 iterations. Although every method has different default training iterations, we keep them the same for the simplicity of experiments. Then, we select a checkpoint that shows the best value among all reported iterations and choice of hyper-parameters. In semantic segmentation, we calculate mIoU and each criterion similarly. We don't use DEV [56] for semantic segmentation since the design of the domain classifier is complicated. We run experiments three times and show their averaged accuracy.\n\nImplementation. We utilize published official implementations for adaptation methods, CDAN, MCC, NC, AdaptSeg, and ADVENT. For the pseudo labeling method, we use the NC's implementation. These methods use ResNet50 or ResNet101 [16] as backbone networks. Adapt-Seg and ADVENT employ DeepLab [7]. See the appendix for more details.\n\n\nValidation Results\n\nImage Classification. The results of image classification are summarized in Tables 2, 3, and 4. Fig. 5 shows plots of accuracy and criteria for several adaptation settings. In most cases, DA methods are sensitive to hyper-parameters and training iterations. As we can see in the Tables, our proposed method faithfully selects good checkpoints for vari-   [24]. OfficeHome partial DA using DANN [12]. Left: Source (Blue), Target (Red). Right: Target samples. Different colors denote different classes. (a) Entropy [25] does not detect the collapse of the neighborhood structure and chooses a model that wrongly aligns features. (b) SND chooses a model that keeps the structure. ous methods and two category shifts. Of course, there are some gaps between the upper bound and our score, but the gap is not large. Importantly, SND does not catastrophically fail in these cases while other criteria choose several bad checkpoints. Besides, the curve of accuracy and SND have very similar shapes. The results denote that SND is effective for HPO in various methods. In the experiments on VisDA and DomainNet, most methods select good checkpoints since adaptation methods are stable across different hyper-parameters.\n\nSource Risk provides a good model in some cases, but also catastrophically fails in some cases such as A2D in NC  Table 4: VisDA [29] and DomainNet (DNet) [28] results in closed DA. Averaged accuracy over three runs and its standard deviation are shown. We utilize Real to Clipart adaptation for DomainNet. SND performs the best on average.    Entropy [25] (C-Ent) shows comparable performance to SND in NC [36] and PL [21]. This is probably because both methods are trained to maintain the target neighborhood structure. Then, by monitoring the confidence of the predic-tion, we can pick a good model. However, as is clear from the graph at the bottom of Fig. 5, it causes catastrophic failure by mistakenly providing confident predictions in PDA (Real to Art) adapted by DANN. We show the feature visualization of DANN results in Fig. 4. The model selected by C-Ent collapses the neighborhood structure of target samples yet matches them with source samples, which results in a lower C-Ent value. By contrast, SND selected a model that maintains the structure. Since the overconfidence issue can happen in many methods and datasets, C-Ent is not reliable for some methods. This is consistent with the results on the toy dataset in Fig. 2.\n\nSemantic Segmentation. Table 5 describes the checkpoint selection result. SND selects good checkpoints for both methods. If we compare the performance of the upper bound, ADVENT [53] is better than AdaptSeg [47] with 3.2 points in mIoU. But, the gap becomes much smaller (only 0.7 points) if we apply unsupervised evaluation. AD-VENT [53] is more sensitive to hyper-parameters such as training iterations than AdaptSeg [47]. Many current stateof-the-art models seem to select checkpoints by the target risk. But, as this result indicates, such comparisons may be misleading for real-world applications.\n\nSource Domain Selection. We examine whether SND can select the best source domain given a target domain using the OfficeHome dataset. The task is to predict the best    source domain from 3 candidates given a target domain. For simplicity, we do not use any adaptation method, thus we just train a model using source samples and evaluate using unlabeled target samples. As shown in Fig. 6, though SND does not always predict the best source domain, it always returns a model with upper-bound level performance.\n\n\nAnalysis\n\nEffectiveness of Softmax Normalization of Features. Here, we analyze the effect of using softmax normalized features to compute S ij in Sec. 3.1. We compute the relative within-class variance, i.e., within-class variance divided by the variance of all classes, and compare between different features in Table 7. The features we employ (Last w Softmax) show the smallest relative variance, i.e., they separate each sample from other classes the best. Therefore, using the features allows us to ignore samples of other classes effectively in computing Eq. 2.\n\nNext, we track SND without softmax normalization S ij in Fig. 6. Note that we retain the softmax which normalizes the rows of the similarity matrix in Eq. 2. The accuracy and SND w/o Softmax have correlation, but the correlation depends on adaptation methods. The softmax normalization has the effect of highlighting difference between within-class and between class variance, which is a key to the success of SND. Other normalization methods, such as L2, didn't have the same effect.\n\nPossible Failure Cases and How to Avoid Them. As we mention in the method section, first, if a model is not trained at all, the output does not characterize features of the target samples and SND does not work well. We can easily address this by monitoring the training loss on source samples. Second, one can also fool SND by training a model to collapse all target samples into a single point. Empirically, we find that such a degenerate solution is hard to detect with any metrics including SND. One possible solution is to compare the feature visualizations of an adapted and an initial model. We leave further analysis to future work.\n\n\nConclusion and Recommendations\n\nIn this paper, we studied the problem of validating unsupervised domain adaptation methods, and introduced a novel criterion that considers how well target samples are clustered. Our experiments reveal a problem in existing methods' validation protocols. Therefore our recommendations to evaluate UDA algorithms in future research are as follows:\n\n\u2022 Report which HPO method is used and describe the detail of validation if it includes hyper-parameters, e.g. number of hold-out source samples. \u2022 A space to search hyper-parameters can be defined with the scale of loss and insight from previous works, but should be clearly discussed. \u2022 Show the curve of the metric and accuracy. \u2022 Publish implementations, including code for HPO. It is important to design adaptation methods considering how HPO works on them. For example, methods requiring many hyper-parameters are hard to validate and, as we see in the right of Fig. 6, the difficulty of unsupervised validation differs from method to method.\n\nApplying this protocol may reveal methods with highly performant upper-bounds on accuracy, but which are difficult to tune with any unsupervised validation criterion, including ours. In such scenarios, it may be reasonable to use a small set of target labels. However, this should be clearly discussed in the paper.\n\nFinally, HPO is also crucial in open-set DA [4,39] and domain-adaptive object detection [9], and the topic of generalization in general. We leave extensions to these other tasks to future work.\n\nAcknowledgment. This work was supported by Honda, DARPA LwLL and NSF Award No. 1535797.\n\nWe first describe the details of the experiments. Then we show additional experimental results and analysis.\n\n\nA. Experimental Details\n\nDataset. In partial DA setting using OfficeHome, we choose the first 25 classes (in alphabetic order) as the target private classes following [5]. In the experiments using DomainNet, we choose 126 classes out of 345 classes following [35]. This is to remove classes that include some outlier objects or objects of multiple classes.\n\nImplementation. As we mention in the main paper, we utilize official implementations to perform experiments. Specifically, we use the following implementations; NC [ 5 . We employ the configurations used by these implementations and tune the hyper-parameters described in the main paper. We will publish our implementation including these code.\n\nFor NC [36], we tune the temperature parameter (Eq. 4 and 5 in [36]) used to compute similarity distribution. We multiple the \u03c4 with [0.5, 0.8, 1.0, 1.5] to find a optimal one. Semantic Segmentation.\n\nWe describe the detail of an experiment on semantic segmentation. First, we aim to tune a weight of trade-off between the source classification loss and domain confusion loss in this experiment.\n\nIn AdaptSegNet [47], the implementation defines two weights for two domain classifiers individually. We aim to tune a weight called lambda-adv-target1. We aim to select the hyper-parameters from (\u03bb = 5. To compute source risk, we utilize 1,000 training source images as a source validation set and track mIoU over training iterations.\n\nToy Dataset.\n\nWe utilize the implementation of DANN [13] 6 . We simply use their network architecture and other configurations. We generate source data from two Gaussian distributions with different means ((0,0) and (5,5)), which we regard as two classes. Then, we obtain target data by shifting the mean of one of the Gaussians.\n\nWe will also publish the implementation modified for our experiment.\n\n\nB. Analysis in Toy Dataset\n\nUsing the toy dataset, we show several characteristics of SND. First, SND gets large if features have small within-class variance compared to the distance between classes. Second, SND outputs a large value when samples are generated from a single cluster.\n\nSND and Within-class Variance. Soft Neighborhood Density is designed to be large if the neighborhood samples are densely clustered, which means the feature variance within each cluster is small. We aim to confirm the relationship between SND and the within-class variance using the toy dataset. As shown in Fig. A, we vary the variance of the Gaussian distributions that generate data of two classes while fixing their means. Note that we train and test a model on the same distribution since our goal here is to observe the behavior of SND for the different variance of features. The right of Fig. A illustrates the result. As we expect, as the variance is increased, SND gets smaller. The accuracy also drops with the increase of the variance since the increase makes many hard-to-classify samples.\n\nSND and the Mode of the Data. In this experiment, we investigate the relationship between SND and the number of clusters in the target. Note that we assume we have a fixed number of target samples. As the number of clusters gets smaller, more target samples get similar since the total number of target samples is the same, and SND gets larger. Fig. B shows the result. In this experiment, we generate target data (Black dots in Fig. B) by shifting the source distributions. In the left, we generate the target samples from 6 modes. In the right, the same number of target samples are generated from a single-mode. SND of the right case is much larger than that of the left (7.49 vs 6.79). If all target samples are from the green class, then SND picks the better model, but if they are actually from the red class, then SND picks the worse model. The failure case is hard to avoid as we discuss in Sec. D.\n\n\nC. Additional Results\n\nWe show results removed from our main paper due to limited space.\n\nSemantic Segmentation. Fig. C shows iteration versus mIoU and HPO criterion. SND performs better than others on average in picking a better-adapted model (i.e., better target accuracy). Besides, we can observe that the performance of segmentation models is sensitive to hyper-parameters and training iterations.\n\nImage Classification. Fig. D shows iteration versus accuracy and HPO criterion in image classification experiments. We show results of Pseudo-labeling (PL), CDAN [23], and MCC [18]. SND performs better than others on average in picking a betteradapted model. Although SND does not always select the best model, SND shows a good correlation with accuracy. Entropy [25] shows a similar behavior to SND in PL, but behaves in a totally different way in CDAN [23].\n\nResults of MCD [38]. We conduct experiments on tuning \u03bb of MCD [38]. MCD is a popular approach that employs the disagreement of two task-specific classifiers' output. As shown in the Table A, SND shows the best performance on average. The result indicates the effectiveness of SND to tune classifier discrepancybased adaptation methods. Plots of changing the variance. We generate the data of two classes from two Gaussians with different means. As we show in the plot, we increase the variance of them while fixing their means. In this way, we observe the behavior of SND by the change of the feature density. Right: The change of SND and accuracy with respect to the variance. Since the concentration degree of features decreases with the increase of the variance, SND gets smaller with the increase.\n\n\nD. Additional Analysis\n\n\nMethod\n\nOffice  Table A: Results of MCD [38]. SND performs the best on average. SND of the right case is much larger than the left (7.49 vs 6.79).\n\nFailure Case. In Sec 4.4 in the main paper, we explain possible failure cases: One can fool SND by training a model to collapse all target samples into a single point. We analyze the behavior of metrics in this setting. Specifically, we train a network to correctly classify source samples and to classify all unlabeled target samples into one class. We call the models degenerated models. Note that we will not employ this kind of a degenerated model in reality, but we train the models just to see the behavior of metrics. We vary \u03bb for the target loss and compare the model with a non-adapted model. Fig. E shows the accuracy and the behavior of each metric. Since the model is trained to move all target samples to a single class, SND of degenerated models gets much larger than that of a non-adapted model(Blue). Other metrics are also not useful to identify the best model. Interestingly, training degenerated models for target does not decrease the accuracy of the source domain ((D) Source Risk). This is probably because the representational power of neural networks is rich enough to learn both the degenerated solution for the target and a good solution for the source domain. One possible solution to this problem is to compare the feature visualizations of the degenerated and a non-adapted model. We leave further analysis to future work.\n\nVarying the Number of Target Samples. We show analysis on the number of target samples necessary for Soft Neighborhood Density. Then, in the OfficeHome Real to Art closed adaptation, we employ NC [36] and reduce the number of target samples used to calculate SND. We randomly sample a certain proportion of the target domain and compute SND. As shown in Fig. F, SND is not very sensitive to the number of target samples. However, when we sample a small number of samples (10% case), Soft Neighborhood Density becomes a little unstable.\n\nTemperature Parameter. We fix the temperature parameter (\u03c4 ) in Eq. 2 (See our main draft.) as 0.05 in all of our experiments. Then, in the OfficeHome Art to Product partial domain adaptation, we employ NC [36] and vary the value of \u03c4 . In Fig. G, we compare the resulting curve of SND with the accuracy curve. We have two observations: SND is not very sensitive to the value of \u03c4 in selecting the best model; but, the large temperature can make SND inconsistent with the accuracy as the rightmost (\u03c4 = 0.1) result indicates. This result indicates the necessity of the temperature scaling. The scaling enables to ignore samples embedded far away and to compute the density of neighbors.\n\nSoft Neighborhood Density Versus Validation with a Few Labeled Target Samples.\n\nSome papers propose to utilize a few labeled target samples to tune hyper-parameters. Although Figure C: Semantic segmentation experiments (GTA5 to CityScaple) using AdaptSeg [47] and ADVENT [53]. Different colors indicate different hyper-parameters. We validate the trade-off parameters between the source classification loss and domain-confusion loss. SND has a good correlation with mIoU (ground truth performance). the way of tuning violates the assumption of UDA, we investigate how well the criterion is effective to pick a good hyper-parameter in Fig. H. We employ the OfficeHome Real to Art closed adaptation using NC [36]. We increase the number of validation target samples per class from 1 to 20 and compare the result with SND. When the number of labeled target samples is small, the validation accuracies are not stable and have high variance. To obtain stable and reliable results, we need to have many labeled target samples whereas SND is an unsupervised criterion and shows reliable results. In a real application, having a few labeled target samples may not be always hard as stated in [35]. However, as this result indicates, monitoring only the accuracy of few samples may not provide a good model. Even in such a setting, combining SND will be a good way to tune hyper-parameters.   1 10 Nt to Nt, where Nt = 2427 is the number of target samples. We randomly sample the target samples. To reduce variance of SND, we need to sample certain number of target samples.\n\n\nAnalysis of the Number of Source Validation Samples on\n\nSource Risk and DEV [56].\n\nWe further analyze the cause of failures of source risk and DEV [56]. We increase the number of labeled source samples and observe the behavior of two criteria. We use the Amazon to DSLR setting adapted by CDAN [23]. Even when we use a large proportion of source samples as a validation set (We utilize more than 10 % of source samples in the case of 10 labeled samples per class.), the two criteria are not well correlated with the accuracy of the target domain. This result indicates the using source risk is limited to choosing good hyper-parameters.   \n\nFigure 2 :\n2An illustration of C-Ent failing to detect the collapse of target neighborhood structure. This is the case where an adapted model (\u03bb = 1.0) confidently misclassifies target samples and low entropy (C-Ent) cannot select a good model. The model incorrectly changes the relative distances between target samples. SND can select a better model since it can consider how well the neighborhood structure is preserved.\n\nFigure 3 :\n3Method overview. Soft Neighborhood Density measures the density of implicit local neighborhoods in the target domain,\n\n\n(a) DANN tuned by Entropy[25] (Accuracy: 35.1 %).(b) DANN tuned by SND (Accuracy: 70.8 %).\n\nFigure 4 :\n4Feature visualization\n\nFigure 5 :\n5Iteration versus accuracy and HPO criteria. To ease comparison between accuracy and criteria, we flip the sign of criteria for Entropy, Source risk, and DEV. Notice how SND curves track the Accuracy. More results are shown in appendix.\n\nGTA5\n\n\nFigure 6 :\n6Analysis of softmax features. We remove the softmax layer to obtain target features and compute SND w/o Softmax. The accuracy and SND w/o Softmax have correlation, but the correlation depends on adaptation methods. Applying softmax makes the correlation consistent across methods.\n\n\n0 \u00d7 10 \u22124 , 3.0 \u00d7 10 \u22124 , 2.0 \u00d7 10 \u22124 , 1.0 \u00d7 10 \u22124 , 1.0 \u00d7 10 \u22123 , with 2.0 \u00d7 10 \u22124 as its default setting). Similarly, for ADVENT [53], we aim to pick a trade-off parameter called LAMBDA-ADV-MAIN from (\u03bb = 5.0 \u00d7 10 \u22122 , 1.0 \u00d7 10 \u22122 , 1.0 \u00d7 10 \u22123 , 5.0 \u00d7 10 \u22124 , and 1.0 \u00d7 10 \u22124 , with 1.0 \u00d7 10 \u22122 as its default setting).\n\nFigure A :\nASND with respect to with-in class variance. Left (a)(b):\n\nFigure B :\nBSND shows a large value for the data with a single mode. Black: Target samples. Red: Source samples of class 0. Green: Source samples of class 1. Left: Target samples are generated from 6 modes. Right: The same number of target samples as the left are generated from a single mode.\n\nFigure D :\nDIteration versus accuracy and HPO criteria. Different colors indicate different hyper-parameters. To ease comparison between accuracy and criteria, we flip the sign of criteria for Entropy, Source risk, and DEV.\n\nFigure E :\nEAnalysis of a possible failure case. We train a network to correctly classify source samples and to classify all unlabeled target samples into one class. Blue: A model trained only with source classification loss. Others: Models trained to classify all target samples into a single class as well as trained to correctly classify source samples. Different colors indicate different weights, \u03bb, for the target loss. No metric is able to identify the non-adapted model.\n\nFigure F :\nFAnalysis of the number of target samples used to compute SND. Different colors indicate different hyper-parameters. We vary the number of target samples from\n\nFigure G :\nGAnalysis of the temperature value used to compute SND. Different colors indicate different hyper-parameters. We vary the value of the temperature of Eq. 2, i.e., 0.01, 0.03, 0.05 (default), 0.07, 0.1. The result indicates that SND shows consistent results across different temperature values.\n\nFigure H :\nHIteration versus accuracy and accuracy of the subset of a target domain. Different colors indicate different hyper-parameters. We subsample labeled target samples (1, 3, 10, 20 samples per class) and compute the accuracy. Many number of labeled samples is necessary to resemble the performance of a whole target domain.\n\nFigure I :\nIAnalysis of the number of labeled source samples used for validation. We vary the number of the labeled source samples to compute source risk and DEV risk. Different colors indicate different hyper-parameters. The result indicates that even though we increase the number of source validation samples, the risks are not reliable to select hyper-parameters. classification networks for unsupervised domain adaptation. In Eur. Conf. Comput. Vis., 2016. 2\n\nTable 1 :\n1Technical comparison with other validation approaches.\n\nTable 2 :\n2Results of closed DA. SND provides faithful results for all methods and datasets, i.e. Office (A2D and W2A) and OfficeHome (R2A and A2P) whereas baselines show several failure cases. Lower/Upper bounds are results obtained with the worst/best model.Method \nCDAN [23] \nMCC [18] \nNC [36] \nPL [21] \nAvg \nR2A P2C A2P R2A P2C A2P R2A P2C A2P R2A P2C A2P \nLower Bound 60.9 34.5 60.2 53.7 38.3 60.9 60.0 37.5 52.3 49.2 56.6 44.1 50.7 \nSource Risk \n67.6 42.0 64.8 65.1 47.4 73.3 76.6 49.8 77.9 61.5 72.7 53.5 62.7 \nDEV [56] \n65.6 36.6 63.9 67.7 47.3 70.3 72.3 54.5 67.0 62.4 66.3 52.1 60.5 \nEntropy [25] \n64.7 40.3 64.8 53.8 40.4 62.0 79.1 58.2 78.7 60.1 71.7 47.0 60.1 \nSND (Ours) \n66.3 45.7 65.4 70.2 50.8 79.3 79.2 58.1 78.2 68.7 72.2 57.9 66.0 \nUpper Bound \n68.8 46.9 68.5 72.0 52.1 79.7 80.1 58.2 79.1 69.0 74.0 59.4 67.3 \n\n\n\nTable 3 :\n3Results of partial DA on OfficeHome. SND performs the best on average.\n\nTable 2 .\n2DEV [56] also sometimes catastrophically fails. Lower bound 51.1\u00b11.3 51.3\u00b11.4 67.1\u00b11.1 54.8\u00b12.1 44.8\u00b13.7 56.9\u00b10.4 58.7\u00b11.1 55.5\u00b10.3 55.0\u00b11.1 Upper bound 74.1\u00b11.1 65.6\u00b10.1 74.5\u00b10.6 61.2\u00b10.5 69.2\u00b10.2 63.2\u00b10.2 69.2\u00b10.8 61.0\u00b10.1 67.2\u00b10.1Method \n\nCDAN [23] \nMCC [18] \nNC [36] \nPL [21] \nAvg \nVisDA \nDNet \nVisDA \nDNet \nVisDA \nDNet \nVisDA \nDNet \nSource Risk \n72.6\u00b10.8 63.8\u00b11.4 71.7\u00b10.8 58.7\u00b10.8 65.8\u00b11.3 62.0\u00b10.2 66.7\u00b12.8 59.9\u00b10.5 65.1\u00b10.3 \nDEV [56] \n72.6\u00b10.8 57.9\u00b14.1 72.3\u00b13.0 58.5\u00b10.5 65.8\u00b11.3 59.4\u00b10.8 66.7\u00b12.8 59.1\u00b10.3 64.0\u00b11.1 \nEntropy [25] \n69.9\u00b11.8 61.5\u00b11.1 68.9\u00b11.3 59.2\u00b10.2 68.4\u00b11.1 62.3\u00b10.6 68.5\u00b10.1 60.7\u00b10.3 64.9\u00b10.2 \nSND (Ours) \n70.3\u00b10.1 64.9\u00b10.4 73.0\u00b11.1 58.9\u00b12.3 66.9\u00b13.2 62.4\u00b10.8 69.0\u00b11.1 60.9\u00b10.1 65.8\u00b10.3 \n\n\nTable 5 :\n5Validation results in domain-adaptive semantic segmentation in GTA5 to Cityscapes adaptation.From Fig. 5, these two criteria have some variance and do \nnot necessarily reflect the accuracy in the target. We hy-\npothesize that there are two reasons. First, Source Risk is \nnot necessarily correlated with the performance of the tar-\nget. If a model focuses on the classification of source sam-\nples (i.e., setting \u03bb = 0 in Eq. 1), the risk gets small, which \ndoes not indicate good performance on the target domain. \nUnless the source and target are very similar, the risk will \nnot be reliable. Second, we may need careful design in se-\nlecting validation source samples and the domain classifier \nconstruction for DEV. However, considering the practical \napplication, the validation methods should not have a mod-\nule that requires careful design. \n\n\nTable 6 :\n6Source domain selection experiments using the Office-Home dataset. We show the accuracy of the selected model and se-\nlected source domain (R: Real, Ar: Art, Cl: Clipart, Pr: Product). \nSND selects the best source domain except for the Real domain. \nEven in that case, the selected model and the oracle accuracy per-\nform similarly. \n\nInput Layer \nA to D W to A A to P \nMiddle \n0.437 \n0.594 \n0.474 \nLast w/o Softmax 0.215 \n0.388 \n0.301 \nLast w Softmax \n0.163 \n0.348 \n0.285 \n\n\n\nTable 7 :\n7Analysis of input features. Within-class variance normalized by variance of all samples. Features of the last layer after softmax show the smallest variance within-class.\nhttps://github.com/VisionLearningGroup/DANCE 2 https://github.com/thuml/CDAN 3 https://github.com/thuml/ Versatile-Domain-Adaptation 4 https://github.com/wasidennis/AdaptSegNet 5 https://github.com/valeoai/ADVENT 6 https://github.com/GRAAL-Research/domain_ adversarial_neural_network\n\nA theory of learning from different domains. Machine learning. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman Vaughan, 79Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learn- ing, 79(1-2):151-175, 2010. 5\n\nUnsupervised pixellevel domain adaptation with generative adversarial networks. Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan, IEEE Conf. Comput. Vis. Pattern Recog. Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel- level domain adaptation with generative adversarial net- works. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2\n\nDomain separation networks. Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan, In Adv. Neural Inform. Process. Syst. 2Konstantinos Bousmalis, George Trigeorgis, Nathan Silber- man, Dilip Krishnan, and Dumitru Erhan. Domain separa- tion networks. In Adv. Neural Inform. Process. Syst., 2016. 2\n\nOpen set domain adaptation. Panareda Pau, Juergen Busto, Gall, Int. Conf. Comput. Vis. Pau Panareda Busto and Juergen Gall. Open set domain adaptation. In Int. Conf. Comput. Vis., 2017. 8\n\nPartial adversarial domain adaptation. Zhangjie Cao, Lijia Ma, Mingsheng Long, Jianmin Wang, Eur. Conf. Comput. Vis. 9Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation. In Eur. Conf. Comput. Vis., 2018. 1, 4, 9\n\nLearning to transfer examples for partial domain adaptation. Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, Qiang Yang, IEEE Conf. Comput. Vis. Pattern Recog. Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, and Qiang Yang. Learning to transfer examples for partial domain adaptation. In IEEE Conf. Comput. Vis. Pat- tern Recog., 2019. 5\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE Trans. Pattern Anal. Mach. Intell. 404Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834-848, 2017. 6\n\nTransferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation. Xinyang Chen, Sinan Wang, Mingsheng Long, Jianmin Wang, Int. Conf. Mach Learn. Xinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang. Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation. In Int. Conf. Mach Learn., 2019. 1\n\nDomain adaptive faster r-cnn for object detection in the wild. Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool, IEEE Conf. Comput. Vis. Pattern Recog. Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object detec- tion in the wild. In IEEE Conf. Comput. Vis. Pattern Recog., 2018. 1, 2, 8\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, IEEE Conf. Comput. Vis. Pattern Recog. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In IEEE Conf. Comput. Vis. Pattern Recog., 2016. 5\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, IEEE Conf. Comput. Vis. Pattern Recog. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., 2009. 1\n\nUnsupervised domain adaptation by backpropagation. Yaroslav Ganin, Victor Lempitsky, Int. Conf. Mach Learn. 56Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Int. Conf. Mach Learn., 2014. 1, 2, 3, 4, 5, 6\n\nDomain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor Lempitsky, JMLR. 1759Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial train- ing of neural networks. JMLR, 17(59):1-35, 2016. 9\n\nMuhammad Ghifary, Mengjie Bastiaan Kleijn, David Zhang, Wen Balduzzi, Li, Deep reconstruction. Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep reconstruction-\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Int. Conf. Comput. Vis. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Int. Conf. Comput. Vis., 2017. 1\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, IEEE Conf. Comput. Vis. Pattern Recog. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conf. Comput. Vis. Pattern Recog., 2016. 6\n\nJudy Hoffman, Dequan Wang, Fisher Yu, Trevor Darrell, arXiv:1612.02649Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. 1arXiv preprintJudy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016. 1, 2\n\nMinimum class confusion for versatile domain adaptation. Ying Jin, Ximei Wang, Mingsheng Long, Jianmin Wang, Eur. Conf. Comput. Vis. 79Ying Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang. Minimum class confusion for versatile domain adaptation. In Eur. Conf. Comput. Vis., 2020. 1, 2, 3, 5, 6, 7, 9\n\nContrastive adaptation network for unsupervised domain adaptation. Guoliang Kang, Lu Jiang, Yi Yang, Alexander G Hauptmann, IEEE Conf. Comput. Vis. Pattern Recog. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Haupt- mann. Contrastive adaptation network for unsupervised do- main adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. 2\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Adv. Neural Inform. Process. Syst. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In Adv. Neural Inform. Process. Syst., 2012. 1\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Dong-Hyun Lee, Workshop on challenges in representation learning, ICML. 67Dong-Hyun Lee. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, 2013. 5, 6, 7\n\nLearning transferable features with deep adaptation networks. Mingsheng Long, Yue Cao, Jianmin Wang, Michael I Jordan , Int. Conf. Mach Learn. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features with deep adaptation networks. In Int. Conf. Mach Learn., 2015. 1\n\nConditional adversarial domain adaptation. Mingsheng Long, Zhangjie Cao, Jianmin Wang, Michael I Jordan , In Adv. Neural Inform. Process. Syst. 912Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adapta- tion. In Adv. Neural Inform. Process. Syst., 2018. 1, 2, 5, 6, 7, 9, 12\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, JMLR. 911Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 9(11):2579-2605, 2008. 6\n\nMinimal-entropy correlation alignment for unsupervised deep domain adaptation. Pietro Morerio, Jacopo Cavazza, Vittorio Murino, arXiv:1711.1028810arXiv preprintPietro Morerio, Jacopo Cavazza, and Vittorio Murino. Minimal-entropy correlation alignment for unsupervised deep domain adaptation. arXiv preprint arXiv:1711.10288, 2017. 2, 3, 5, 6, 7, 9, 10\n\nImage to image translation for domain adaptation. Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, Kyungnam Kim, IEEE Conf. Comput. Vis. Pattern Recog. Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ra- mamoorthi, and Kyungnam Kim. Image to image translation for domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., 2018. 2\n\nRepresentation based complexity measures for predicting generalization in deep learning. Parth Natekar, Manik Sharma, Parth Natekar and Manik Sharma. Representation based complexity measures for predicting generalization in deep learning, 2020. 2\n\nMoment matching for multi-source domain adaptation. Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, Bo Wang, Int. Conf. Comput. Vis. 57Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. Int. Conf. Comput. Vis., 2019. 5, 7\n\nXingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, Kate Saenko, Visda, arXiv:1710.06924The visual domain adaptation challenge. 57arXiv preprintXingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. 5, 7\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, In Adv. Neural Inform. Process. Syst. 1Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Adv. Neural Inform. Process. Syst., 2015. 1\n\nPlaying for data: Ground truth from computer games. Stephan R Richter, Vibhav Vineet, Stefan Roth, Vladlen Koltun, Eur. Conf. Comput. Vis. Bastian Leibe, Jiri Matas, Nicu Sebe, and Max WellingStephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Eur. Conf. Comput. Vis., 2016. 5\n\nNonlinear dimensionality reduction by locally linear embedding. science. T Sam, Lawrence K Roweis, Saul, 290Sam T Roweis and Lawrence K Saul. Nonlinear dimen- sionality reduction by locally linear embedding. science, 290(5500):2323-2326, 2000. 3\n\nFrom source to target and back: symmetric bidirectional adaptive gan. Paolo Russo, Fabio M Carlucci, Tatiana Tommasi, Barbara Caputo, IEEE Conf. Comput. Vis. Pattern Recog. Paolo Russo, Fabio M Carlucci, Tatiana Tommasi, and Bar- bara Caputo. From source to target and back: symmetric bi- directional adaptive gan. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8099-8108, 2018. 2\n\nAdapting visual category models to new domains. Kate Saenko, Brian Kulis, Mario Fritz, Trevor Darrell, Eur. Conf. Comput. Vis. 5Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In Eur. Conf. Comput. Vis., 2010. 5\n\nSemi-supervised domain adaptation via minimax entropy. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, Kate Saenko, Int. Conf. Comput. Vis. 911Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In Int. Conf. Comput. Vis., 2019. 9, 11\n\nUniversal domain adaptation through self supervision. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Kate Saenko, arXiv:2002.079531011arXiv preprintKuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation through self supervi- sion. arXiv preprint arXiv:2002.07953, 2020. 1, 2, 3, 5, 6, 7, 9, 10, 11\n\nAsymmetric tri-training for unsupervised domain adaptation. Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, Int. Conf. Mach Learn. 25Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised domain adaptation. In Int. Conf. Mach Learn., 2017. 2, 5\n\nMaximum classifier discrepancy for unsupervised domain adaptation. Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, Tatsuya Harada, IEEE Conf. Comput. Vis. Pattern Recog. 910Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., 2018. 9, 10\n\nOpen set domain adaptation by backpropagation. Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada, Eur. Conf. Comput. Vis. Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adaptation by backpropa- gation. In Eur. Conf. Comput. Vis., 2018. 8\n\nLearning from simulated and unsupervised images through adversarial training. Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, Russell Webb, IEEE Conf. Comput. Vis. Pattern Recog. Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, and Russell Webb. Learning from simulated and unsupervised images through adversarial training. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2\n\nA dirt-t approach to unsupervised domain adaptation. Rui Shu, H Hung, Hirokazu Bui, Stefano Narui, Ermon, In Int. Conf. Learn. Represent. 2Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In Int. Conf. Learn. Represent., 2018. 2\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXivKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv, 2014. 1\n\nCovariate shift adaptation by importance weighted cross validation. Masashi Sugiyama, Matthias Krauledat, Klaus-Robert M\u00e3\u017eller, JMLR. 83Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M\u00c3\u017eller. Covariate shift adaptation by importance weighted cross validation. JMLR, 8(May):985-1005, 2007. 1, 2, 3\n\nReturn of frustratingly easy domain adaptation. Baochen Sun, Jiashi Feng, Kate Saenko, AAAI. Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frus- tratingly easy domain adaptation. In AAAI, 2016. 1\n\nDeep coral: Correlation alignment for deep domain adaptation. Baochen Sun, Kate Saenko, European Conference on Computer Vision (ECCV) Workshop. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European Con- ference on Computer Vision (ECCV) Workshop, 2016. 2\n\nUnsupervised domain adaptation via structurally regularized deep clustering. Hui Tang, Ke Chen, Kui Jia, IEEE Conf. Comput. Vis. Pattern Recog. 1Hui Tang, Ke Chen, and Kui Jia. Unsupervised domain adap- tation via structurally regularized deep clustering. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8725-8735, 2020. 1, 2\n\nLearning to adapt structured output space for semantic segmentation. Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker, IEEE Conf. Comput. Vis. Pattern Recog. 11Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In IEEE Conf. Comput. Vis. Pattern Recog., 2018. 1, 2, 5, 7, 9, 11\n\nSimultaneous deep transfer across domains and tasks. Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko, Int. Conf. Comput. Vis. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Int. Conf. Comput. Vis., pages 4068-4076, 2015. 2\n\nAdversarial discriminative domain adaptation. Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell, IEEE Conf. Comput. Vis. Pattern Recog. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2\n\nDeep domain confusion: Maximizing for domain invariance. Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell, arXivEric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv, 2014. 1\n\nDeep hashing network for unsupervised domain adaptation. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman Panchanathan, IEEE Conf. Comput. Vis. Pattern Recog. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 5\n\nAdversarial feature augmentation for unsupervised domain adaptation. Riccardo Volpi, Pietro Morerio, Silvio Savarese, Vittorio Murino, IEEE Conf. Comput. Vis. Pattern Recog. Riccardo Volpi, Pietro Morerio, Silvio Savarese, and Vitto- rio Murino. Adversarial feature augmentation for unsuper- vised domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., 2018. 2\n\nAdvent: Adversarial entropy minimization for domain adaptation in semantic segmentation. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Mathieu Cord, Patrick P\u00e9rez, IEEE Conf. Comput. Vis. Pattern Recog. 11Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Mathieu Cord, and Patrick P\u00e9rez. Advent: Adversarial entropy mini- mization for domain adaptation in semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. 1, 2, 5, 7, 9, 11\n\nTransferable normalization: Towards improving transferability of deep neural networks. Ximei Wang, Ying Jin, Mingsheng Long, Jianmin Wang, Michael I Jordan , In Adv. Neural Inform. Process. Syst. 1Ximei Wang, Ying Jin, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Transferable normalization: Towards im- proving transferability of deep neural networks. In Adv. Neu- ral Inform. Process. Syst., 2019. 1\n\nLarger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. Ruijia Xu, Guanbin Li, Jihan Yang, Liang Lin, Int. Conf. Comput. Vis. Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In Int. Conf. Comput. Vis., 2019. 1\n\nTowards accurate model selection in deep unsupervised domain adaptation. Kaichao You, Ximei Wang, Mingsheng Long, Michael Jordan, Int. Conf. Mach Learn. 1012Kaichao You, Ximei Wang, Mingsheng Long, and Michael Jordan. Towards accurate model selection in deep unsuper- vised domain adaptation. In Int. Conf. Mach Learn., 2019. 1, 2, 3, 5, 6, 7, 10, 12\n\nCollaborative and adversarial network for unsupervised domain adaptation. Weichen Zhang, Wanli Ouyang, Wen Li, Dong Xu, IEEE Conf. Comput. Vis. Pattern Recog. Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Col- laborative and adversarial network for unsupervised domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3801-3809, 2018. 2\n\nConfidence regularized self-training. Yang Zou, Zhiding Yu, Xiaofeng Liu, Jinsong Kumar, Wang, Int. Conf. Comput. Vis. 15Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jin- song Wang. Confidence regularized self-training. In Int. Conf. Comput. Vis., 2019. 1, 5\n\nUnsupervised domain adaptation for semantic segmentation via class-balanced self-training. Yang Zou, Zhiding Yu, Jinsong Bvk Vijaya Kumar, Wang, Eur. Conf. Comput. Vis. Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In Eur. Conf. Comput. Vis., 2018. 1, 2, 5\n", "annotations": {"author": "[{\"end\":152,\"start\":101},{\"end\":186,\"start\":153},{\"end\":236,\"start\":187},{\"end\":287,\"start\":237},{\"end\":363,\"start\":288},{\"end\":435,\"start\":364}]", "publisher": null, "author_last_name": "[{\"end\":114,\"start\":109},{\"end\":165,\"start\":162},{\"end\":201,\"start\":193},{\"end\":250,\"start\":242},{\"end\":302,\"start\":295},{\"end\":375,\"start\":369}]", "author_first_name": "[{\"end\":108,\"start\":101},{\"end\":161,\"start\":153},{\"end\":192,\"start\":187},{\"end\":241,\"start\":237},{\"end\":294,\"start\":288},{\"end\":368,\"start\":364}]", "author_affiliation": "[{\"end\":151,\"start\":133},{\"end\":185,\"start\":167},{\"end\":235,\"start\":217},{\"end\":286,\"start\":268},{\"end\":362,\"start\":329},{\"end\":410,\"start\":392},{\"end\":434,\"start\":412}]", "title": "[{\"end\":98,\"start\":1},{\"end\":533,\"start\":436}]", "venue": null, "abstract": "[{\"end\":1816,\"start\":535}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1934,\"start\":1930},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":1937,\"start\":1934},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1940,\"start\":1937},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1943,\"start\":1940},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1946,\"start\":1943},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2001,\"start\":1997},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2367,\"start\":2363},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2370,\"start\":2367},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2373,\"start\":2370},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2400,\"start\":2396},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2417,\"start\":2413},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2454,\"start\":2450},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2536,\"start\":2532},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2539,\"start\":2536},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2541,\"start\":2539},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2544,\"start\":2541},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2547,\"start\":2544},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2575,\"start\":2571},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2933,\"start\":2929},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2978,\"start\":2975},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3186,\"start\":3182},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3189,\"start\":3186},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3192,\"start\":3189},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3398,\"start\":3394},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3447,\"start\":3443},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4067,\"start\":4063},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4070,\"start\":4067},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4072,\"start\":4070},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4075,\"start\":4072},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4078,\"start\":4075},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4080,\"start\":4078},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4083,\"start\":4080},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":4086,\"start\":4083},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4089,\"start\":4086},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4199,\"start\":4195},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4201,\"start\":4199},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4203,\"start\":4201},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4206,\"start\":4203},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4209,\"start\":4206},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4212,\"start\":4209},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4215,\"start\":4212},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4266,\"start\":4262},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4269,\"start\":4266},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4272,\"start\":4269},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4275,\"start\":4272},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":4278,\"start\":4275},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4281,\"start\":4278},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4284,\"start\":4281},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4287,\"start\":4284},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5047,\"start\":5043},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6018,\"start\":6014},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7285,\"start\":7281},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7288,\"start\":7285},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7316,\"start\":7312},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7789,\"start\":7785},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7801,\"start\":7797},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8889,\"start\":8885},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8892,\"start\":8889},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8918,\"start\":8915},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9009,\"start\":9005},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9012,\"start\":9009},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9015,\"start\":9012},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9285,\"start\":9281},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9288,\"start\":9285},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9291,\"start\":9288},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9294,\"start\":9291},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9300,\"start\":9296},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9303,\"start\":9300},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9801,\"start\":9797},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9804,\"start\":9801},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10336,\"start\":10332},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10560,\"start\":10556},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10580,\"start\":10576},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10929,\"start\":10925},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11187,\"start\":11183},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11777,\"start\":11773},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13798,\"start\":13794},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14776,\"start\":14772},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15361,\"start\":15358},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15432,\"start\":15428},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19938,\"start\":19935},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21891,\"start\":21887},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22103,\"start\":22099},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22262,\"start\":22258},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22653,\"start\":22649},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22889,\"start\":22885},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":22933,\"start\":22929},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":22936,\"start\":22933},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22939,\"start\":22936},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23311,\"start\":23307},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23328,\"start\":23324},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23529,\"start\":23525},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23629,\"start\":23625},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23834,\"start\":23831},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23927,\"start\":23923},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23946,\"start\":23942},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24037,\"start\":24033},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24068,\"start\":24064},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24113,\"start\":24109},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":24278,\"start\":24274},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":25260,\"start\":25256},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25642,\"start\":25638},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25704,\"start\":25701},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26122,\"start\":26118},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26161,\"start\":26157},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26280,\"start\":26276},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27108,\"start\":27104},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27134,\"start\":27130},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27331,\"start\":27327},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27386,\"start\":27382},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27398,\"start\":27394},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":28399,\"start\":28395},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28428,\"start\":28424},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":28555,\"start\":28551},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28640,\"start\":28636},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32423,\"start\":32420},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32426,\"start\":32423},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32467,\"start\":32464},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32941,\"start\":32938},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33034,\"start\":33030},{\"end\":33294,\"start\":33293},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33296,\"start\":33295},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33486,\"start\":33482},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33542,\"start\":33538},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33891,\"start\":33887},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34264,\"start\":34260},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34266,\"start\":34265},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34427,\"start\":34424},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34429,\"start\":34427},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":37175,\"start\":37171},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37189,\"start\":37185},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37376,\"start\":37372},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":37467,\"start\":37463},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37489,\"start\":37485},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37537,\"start\":37533},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":38344,\"start\":38340},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":40002,\"start\":39998},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":40549,\"start\":40545},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":41286,\"start\":41282},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":41302,\"start\":41298},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":41737,\"start\":41733},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":42215,\"start\":42211},{\"end\":42415,\"start\":42411},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":42675,\"start\":42671},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":42746,\"start\":42742},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":42893,\"start\":42889},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43822,\"start\":43818}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43659,\"start\":43235},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43790,\"start\":43660},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43883,\"start\":43791},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43918,\"start\":43884},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44167,\"start\":43919},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44174,\"start\":44168},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44468,\"start\":44175},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44794,\"start\":44469},{\"attributes\":{\"id\":\"fig_8\"},\"end\":44864,\"start\":44795},{\"attributes\":{\"id\":\"fig_9\"},\"end\":45159,\"start\":44865},{\"attributes\":{\"id\":\"fig_10\"},\"end\":45384,\"start\":45160},{\"attributes\":{\"id\":\"fig_11\"},\"end\":45864,\"start\":45385},{\"attributes\":{\"id\":\"fig_12\"},\"end\":46035,\"start\":45865},{\"attributes\":{\"id\":\"fig_13\"},\"end\":46341,\"start\":46036},{\"attributes\":{\"id\":\"fig_14\"},\"end\":46674,\"start\":46342},{\"attributes\":{\"id\":\"fig_15\"},\"end\":47139,\"start\":46675},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47206,\"start\":47140},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48040,\"start\":47207},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48123,\"start\":48041},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48851,\"start\":48124},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":49714,\"start\":48852},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":50202,\"start\":49715},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50385,\"start\":50203}]", "paragraph": "[{\"end\":2985,\"start\":1832},{\"end\":3880,\"start\":2987},{\"end\":4329,\"start\":3882},{\"end\":4739,\"start\":4331},{\"end\":5997,\"start\":4741},{\"end\":6397,\"start\":5999},{\"end\":7166,\"start\":6399},{\"end\":7250,\"start\":7177},{\"end\":7589,\"start\":7266},{\"end\":7928,\"start\":7591},{\"end\":8265,\"start\":7930},{\"end\":8311,\"start\":8267},{\"end\":8679,\"start\":8313},{\"end\":10084,\"start\":8696},{\"end\":11725,\"start\":10086},{\"end\":12034,\"start\":11727},{\"end\":12165,\"start\":12047},{\"end\":12257,\"start\":12190},{\"end\":12724,\"start\":12310},{\"end\":13674,\"start\":12726},{\"end\":14046,\"start\":13676},{\"end\":15295,\"start\":14048},{\"end\":15574,\"start\":15297},{\"end\":16336,\"start\":15610},{\"end\":16475,\"start\":16338},{\"end\":16816,\"start\":16573},{\"end\":17041,\"start\":16818},{\"end\":18250,\"start\":17084},{\"end\":18470,\"start\":18300},{\"end\":19422,\"start\":18472},{\"end\":19749,\"start\":19424},{\"end\":20178,\"start\":19751},{\"end\":20882,\"start\":20180},{\"end\":21119,\"start\":20898},{\"end\":21628,\"start\":21121},{\"end\":21802,\"start\":21651},{\"end\":22171,\"start\":21804},{\"end\":22574,\"start\":22173},{\"end\":22837,\"start\":22576},{\"end\":23273,\"start\":22839},{\"end\":23465,\"start\":23275},{\"end\":24088,\"start\":23475},{\"end\":25409,\"start\":24090},{\"end\":25740,\"start\":25411},{\"end\":26973,\"start\":25763},{\"end\":28215,\"start\":26975},{\"end\":28819,\"start\":28217},{\"end\":29331,\"start\":28821},{\"end\":29900,\"start\":29344},{\"end\":30386,\"start\":29902},{\"end\":31027,\"start\":30388},{\"end\":31408,\"start\":31062},{\"end\":32057,\"start\":31410},{\"end\":32374,\"start\":32059},{\"end\":32569,\"start\":32376},{\"end\":32658,\"start\":32571},{\"end\":32768,\"start\":32660},{\"end\":33127,\"start\":32796},{\"end\":33473,\"start\":33129},{\"end\":33674,\"start\":33475},{\"end\":33870,\"start\":33676},{\"end\":34206,\"start\":33872},{\"end\":34220,\"start\":34208},{\"end\":34537,\"start\":34222},{\"end\":34607,\"start\":34539},{\"end\":34893,\"start\":34638},{\"end\":35695,\"start\":34895},{\"end\":36603,\"start\":35697},{\"end\":36694,\"start\":36629},{\"end\":37007,\"start\":36696},{\"end\":37468,\"start\":37009},{\"end\":38272,\"start\":37470},{\"end\":38446,\"start\":38308},{\"end\":39800,\"start\":38448},{\"end\":40337,\"start\":39802},{\"end\":41025,\"start\":40339},{\"end\":41105,\"start\":41027},{\"end\":42592,\"start\":41107},{\"end\":42676,\"start\":42651},{\"end\":43234,\"start\":42678}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12189,\"start\":12166},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12309,\"start\":12258},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16572,\"start\":16476},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17083,\"start\":17042},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18299,\"start\":18251}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10124,\"start\":10117},{\"end\":27096,\"start\":27089},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":28247,\"start\":28240},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":29654,\"start\":29647},{\"end\":37660,\"start\":37653},{\"end\":38323,\"start\":38316}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1830,\"start\":1818},{\"end\":7175,\"start\":7169},{\"end\":7264,\"start\":7253},{\"attributes\":{\"n\":\"2.\"},\"end\":8694,\"start\":8682},{\"attributes\":{\"n\":\"3.\"},\"end\":12045,\"start\":12037},{\"attributes\":{\"n\":\"3.1.\"},\"end\":15608,\"start\":15577},{\"attributes\":{\"n\":\"4.\"},\"end\":20896,\"start\":20885},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21649,\"start\":21631},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23473,\"start\":23468},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25761,\"start\":25743},{\"attributes\":{\"n\":\"4.4.\"},\"end\":29342,\"start\":29334},{\"attributes\":{\"n\":\"5.\"},\"end\":31060,\"start\":31030},{\"end\":32794,\"start\":32771},{\"end\":34636,\"start\":34610},{\"end\":36627,\"start\":36606},{\"end\":38297,\"start\":38275},{\"end\":38306,\"start\":38300},{\"end\":42649,\"start\":42595},{\"end\":43246,\"start\":43236},{\"end\":43671,\"start\":43661},{\"end\":43895,\"start\":43885},{\"end\":43930,\"start\":43920},{\"end\":44173,\"start\":44169},{\"end\":44186,\"start\":44176},{\"end\":44806,\"start\":44796},{\"end\":44876,\"start\":44866},{\"end\":45171,\"start\":45161},{\"end\":45396,\"start\":45386},{\"end\":45876,\"start\":45866},{\"end\":46047,\"start\":46037},{\"end\":46353,\"start\":46343},{\"end\":46686,\"start\":46676},{\"end\":47150,\"start\":47141},{\"end\":47217,\"start\":47208},{\"end\":48051,\"start\":48042},{\"end\":48134,\"start\":48125},{\"end\":48862,\"start\":48853},{\"end\":49725,\"start\":49716},{\"end\":50213,\"start\":50204}]", "table": "[{\"end\":48040,\"start\":47468},{\"end\":48851,\"start\":48369},{\"end\":49714,\"start\":48957},{\"end\":50202,\"start\":49780}]", "figure_caption": "[{\"end\":43659,\"start\":43248},{\"end\":43790,\"start\":43673},{\"end\":43883,\"start\":43793},{\"end\":43918,\"start\":43897},{\"end\":44167,\"start\":43932},{\"end\":44468,\"start\":44188},{\"end\":44794,\"start\":44471},{\"end\":44864,\"start\":44808},{\"end\":45159,\"start\":44878},{\"end\":45384,\"start\":45173},{\"end\":45864,\"start\":45398},{\"end\":46035,\"start\":45878},{\"end\":46341,\"start\":46049},{\"end\":46674,\"start\":46355},{\"end\":47139,\"start\":46688},{\"end\":47206,\"start\":47152},{\"end\":47468,\"start\":47219},{\"end\":48123,\"start\":48053},{\"end\":48369,\"start\":48136},{\"end\":48957,\"start\":48864},{\"end\":49780,\"start\":49727},{\"end\":50385,\"start\":50215}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":3283,\"start\":3277},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5460,\"start\":5454},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5666,\"start\":5659},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5918,\"start\":5912},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7588,\"start\":7582},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14070,\"start\":14064},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15572,\"start\":15564},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16140,\"start\":16134},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18442,\"start\":18436},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25865,\"start\":25859},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27637,\"start\":27631},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27813,\"start\":27807},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28214,\"start\":28208},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29209,\"start\":29203},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29965,\"start\":29959},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31983,\"start\":31977},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":35208,\"start\":35202},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":35495,\"start\":35489},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36048,\"start\":36042},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36132,\"start\":36126},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":36725,\"start\":36719},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":37037,\"start\":37031},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":39057,\"start\":39051},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":40162,\"start\":40156},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":40585,\"start\":40579},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":41667,\"start\":41661}]", "bib_author_first_name": "[{\"end\":50738,\"start\":50734},{\"end\":50754,\"start\":50750},{\"end\":50768,\"start\":50764},{\"end\":50782,\"start\":50778},{\"end\":50800,\"start\":50792},{\"end\":50818,\"start\":50810},{\"end\":50826,\"start\":50819},{\"end\":51127,\"start\":51115},{\"end\":51145,\"start\":51139},{\"end\":51162,\"start\":51157},{\"end\":51177,\"start\":51170},{\"end\":51190,\"start\":51185},{\"end\":51506,\"start\":51494},{\"end\":51524,\"start\":51518},{\"end\":51543,\"start\":51537},{\"end\":51560,\"start\":51555},{\"end\":51578,\"start\":51571},{\"end\":51837,\"start\":51829},{\"end\":51850,\"start\":51843},{\"end\":52037,\"start\":52029},{\"end\":52048,\"start\":52043},{\"end\":52062,\"start\":52053},{\"end\":52076,\"start\":52069},{\"end\":52317,\"start\":52309},{\"end\":52330,\"start\":52323},{\"end\":52345,\"start\":52336},{\"end\":52359,\"start\":52352},{\"end\":52371,\"start\":52366},{\"end\":52729,\"start\":52718},{\"end\":52742,\"start\":52736},{\"end\":52762,\"start\":52755},{\"end\":52778,\"start\":52773},{\"end\":52791,\"start\":52787},{\"end\":52793,\"start\":52792},{\"end\":53221,\"start\":53214},{\"end\":53233,\"start\":53228},{\"end\":53249,\"start\":53240},{\"end\":53263,\"start\":53256},{\"end\":53558,\"start\":53553},{\"end\":53568,\"start\":53565},{\"end\":53581,\"start\":53573},{\"end\":53600,\"start\":53593},{\"end\":53609,\"start\":53606},{\"end\":53922,\"start\":53916},{\"end\":53938,\"start\":53931},{\"end\":53955,\"start\":53946},{\"end\":53967,\"start\":53963},{\"end\":53983,\"start\":53977},{\"end\":54002,\"start\":53995},{\"end\":54016,\"start\":54013},{\"end\":54031,\"start\":54025},{\"end\":54043,\"start\":54038},{\"end\":54404,\"start\":54401},{\"end\":54414,\"start\":54411},{\"end\":54428,\"start\":54421},{\"end\":54443,\"start\":54437},{\"end\":54451,\"start\":54448},{\"end\":54458,\"start\":54456},{\"end\":54742,\"start\":54734},{\"end\":54756,\"start\":54750},{\"end\":54988,\"start\":54980},{\"end\":55004,\"start\":54996},{\"end\":55019,\"start\":55015},{\"end\":55034,\"start\":55028},{\"end\":55048,\"start\":55044},{\"end\":55069,\"start\":55061},{\"end\":55087,\"start\":55082},{\"end\":55104,\"start\":55098},{\"end\":55354,\"start\":55346},{\"end\":55371,\"start\":55364},{\"end\":55394,\"start\":55389},{\"end\":55405,\"start\":55402},{\"end\":55595,\"start\":55588},{\"end\":55607,\"start\":55600},{\"end\":55809,\"start\":55802},{\"end\":55821,\"start\":55814},{\"end\":55837,\"start\":55829},{\"end\":55847,\"start\":55843},{\"end\":56049,\"start\":56045},{\"end\":56065,\"start\":56059},{\"end\":56078,\"start\":56072},{\"end\":56089,\"start\":56083},{\"end\":56444,\"start\":56440},{\"end\":56455,\"start\":56450},{\"end\":56471,\"start\":56462},{\"end\":56485,\"start\":56478},{\"end\":56761,\"start\":56753},{\"end\":56770,\"start\":56768},{\"end\":56780,\"start\":56778},{\"end\":56796,\"start\":56787},{\"end\":56798,\"start\":56797},{\"end\":57102,\"start\":57098},{\"end\":57119,\"start\":57115},{\"end\":57139,\"start\":57131},{\"end\":57141,\"start\":57140},{\"end\":57462,\"start\":57453},{\"end\":57791,\"start\":57782},{\"end\":57801,\"start\":57798},{\"end\":57814,\"start\":57807},{\"end\":57837,\"start\":57821},{\"end\":58074,\"start\":58065},{\"end\":58089,\"start\":58081},{\"end\":58102,\"start\":58095},{\"end\":58125,\"start\":58109},{\"end\":58384,\"start\":58377},{\"end\":58409,\"start\":58401},{\"end\":58618,\"start\":58612},{\"end\":58634,\"start\":58628},{\"end\":58652,\"start\":58644},{\"end\":58939,\"start\":58936},{\"end\":58953,\"start\":58947},{\"end\":58968,\"start\":58963},{\"end\":58983,\"start\":58979},{\"end\":59005,\"start\":58997},{\"end\":59327,\"start\":59322},{\"end\":59342,\"start\":59337},{\"end\":59541,\"start\":59533},{\"end\":59554,\"start\":59548},{\"end\":59564,\"start\":59560},{\"end\":59575,\"start\":59570},{\"end\":59587,\"start\":59583},{\"end\":59598,\"start\":59596},{\"end\":59804,\"start\":59796},{\"end\":59814,\"start\":59811},{\"end\":59827,\"start\":59822},{\"end\":59841,\"start\":59837},{\"end\":59857,\"start\":59851},{\"end\":59868,\"start\":59864},{\"end\":60220,\"start\":60213},{\"end\":60239,\"start\":60235},{\"end\":60248,\"start\":60244},{\"end\":60545,\"start\":60538},{\"end\":60547,\"start\":60546},{\"end\":60563,\"start\":60557},{\"end\":60578,\"start\":60572},{\"end\":60592,\"start\":60585},{\"end\":60973,\"start\":60972},{\"end\":60989,\"start\":60979},{\"end\":61221,\"start\":61216},{\"end\":61234,\"start\":61229},{\"end\":61236,\"start\":61235},{\"end\":61254,\"start\":61247},{\"end\":61271,\"start\":61264},{\"end\":61582,\"start\":61578},{\"end\":61596,\"start\":61591},{\"end\":61609,\"start\":61604},{\"end\":61623,\"start\":61617},{\"end\":61864,\"start\":61857},{\"end\":61880,\"start\":61872},{\"end\":61890,\"start\":61886},{\"end\":61907,\"start\":61901},{\"end\":61921,\"start\":61917},{\"end\":62191,\"start\":62184},{\"end\":62207,\"start\":62199},{\"end\":62217,\"start\":62213},{\"end\":62232,\"start\":62228},{\"end\":62527,\"start\":62520},{\"end\":62544,\"start\":62535},{\"end\":62560,\"start\":62553},{\"end\":62820,\"start\":62813},{\"end\":62833,\"start\":62828},{\"end\":62853,\"start\":62844},{\"end\":62869,\"start\":62862},{\"end\":63170,\"start\":63163},{\"end\":63184,\"start\":63178},{\"end\":63204,\"start\":63195},{\"end\":63220,\"start\":63213},{\"end\":63493,\"start\":63487},{\"end\":63512,\"start\":63507},{\"end\":63527,\"start\":63522},{\"end\":63541,\"start\":63535},{\"end\":63557,\"start\":63552},{\"end\":63571,\"start\":63564},{\"end\":63898,\"start\":63895},{\"end\":63905,\"start\":63904},{\"end\":63920,\"start\":63912},{\"end\":63933,\"start\":63926},{\"end\":64205,\"start\":64200},{\"end\":64222,\"start\":64216},{\"end\":64437,\"start\":64430},{\"end\":64456,\"start\":64448},{\"end\":64480,\"start\":64468},{\"end\":64723,\"start\":64716},{\"end\":64735,\"start\":64729},{\"end\":64746,\"start\":64742},{\"end\":64941,\"start\":64934},{\"end\":64951,\"start\":64947},{\"end\":65257,\"start\":65254},{\"end\":65266,\"start\":65264},{\"end\":65276,\"start\":65273},{\"end\":65582,\"start\":65574},{\"end\":65597,\"start\":65589},{\"end\":65610,\"start\":65604},{\"end\":65627,\"start\":65621},{\"end\":65644,\"start\":65634},{\"end\":65659,\"start\":65651},{\"end\":66013,\"start\":66009},{\"end\":66025,\"start\":66021},{\"end\":66041,\"start\":66035},{\"end\":66055,\"start\":66051},{\"end\":66304,\"start\":66300},{\"end\":66316,\"start\":66312},{\"end\":66330,\"start\":66326},{\"end\":66345,\"start\":66339},{\"end\":66612,\"start\":66608},{\"end\":66624,\"start\":66620},{\"end\":66638,\"start\":66634},{\"end\":66650,\"start\":66646},{\"end\":66665,\"start\":66659},{\"end\":66888,\"start\":66881},{\"end\":66907,\"start\":66903},{\"end\":66923,\"start\":66917},{\"end\":66947,\"start\":66937},{\"end\":67272,\"start\":67264},{\"end\":67286,\"start\":67280},{\"end\":67302,\"start\":67296},{\"end\":67321,\"start\":67313},{\"end\":67662,\"start\":67653},{\"end\":67675,\"start\":67667},{\"end\":67688,\"start\":67682},{\"end\":67704,\"start\":67697},{\"end\":67718,\"start\":67711},{\"end\":68095,\"start\":68090},{\"end\":68106,\"start\":68102},{\"end\":68121,\"start\":68112},{\"end\":68135,\"start\":68128},{\"end\":68158,\"start\":68142},{\"end\":68520,\"start\":68514},{\"end\":68532,\"start\":68525},{\"end\":68542,\"start\":68537},{\"end\":68554,\"start\":68549},{\"end\":68852,\"start\":68845},{\"end\":68863,\"start\":68858},{\"end\":68879,\"start\":68870},{\"end\":68893,\"start\":68886},{\"end\":69205,\"start\":69198},{\"end\":69218,\"start\":69213},{\"end\":69230,\"start\":69227},{\"end\":69239,\"start\":69235},{\"end\":69520,\"start\":69516},{\"end\":69533,\"start\":69526},{\"end\":69546,\"start\":69538},{\"end\":69559,\"start\":69552},{\"end\":69839,\"start\":69835},{\"end\":69852,\"start\":69845},{\"end\":69864,\"start\":69857}]", "bib_author_last_name": "[{\"end\":50748,\"start\":50739},{\"end\":50762,\"start\":50755},{\"end\":50776,\"start\":50769},{\"end\":50790,\"start\":50783},{\"end\":50808,\"start\":50801},{\"end\":50834,\"start\":50827},{\"end\":51137,\"start\":51128},{\"end\":51155,\"start\":51146},{\"end\":51168,\"start\":51163},{\"end\":51183,\"start\":51178},{\"end\":51199,\"start\":51191},{\"end\":51516,\"start\":51507},{\"end\":51535,\"start\":51525},{\"end\":51553,\"start\":51544},{\"end\":51569,\"start\":51561},{\"end\":51584,\"start\":51579},{\"end\":51841,\"start\":51838},{\"end\":51856,\"start\":51851},{\"end\":51862,\"start\":51858},{\"end\":52041,\"start\":52038},{\"end\":52051,\"start\":52049},{\"end\":52067,\"start\":52063},{\"end\":52081,\"start\":52077},{\"end\":52321,\"start\":52318},{\"end\":52334,\"start\":52331},{\"end\":52350,\"start\":52346},{\"end\":52364,\"start\":52360},{\"end\":52376,\"start\":52372},{\"end\":52734,\"start\":52730},{\"end\":52753,\"start\":52743},{\"end\":52771,\"start\":52763},{\"end\":52785,\"start\":52779},{\"end\":52800,\"start\":52794},{\"end\":53226,\"start\":53222},{\"end\":53238,\"start\":53234},{\"end\":53254,\"start\":53250},{\"end\":53268,\"start\":53264},{\"end\":53563,\"start\":53559},{\"end\":53571,\"start\":53569},{\"end\":53591,\"start\":53582},{\"end\":53604,\"start\":53601},{\"end\":53618,\"start\":53610},{\"end\":53929,\"start\":53923},{\"end\":53944,\"start\":53939},{\"end\":53961,\"start\":53956},{\"end\":53975,\"start\":53968},{\"end\":53993,\"start\":53984},{\"end\":54011,\"start\":54003},{\"end\":54023,\"start\":54017},{\"end\":54036,\"start\":54032},{\"end\":54051,\"start\":54044},{\"end\":54409,\"start\":54405},{\"end\":54419,\"start\":54415},{\"end\":54435,\"start\":54429},{\"end\":54446,\"start\":54444},{\"end\":54454,\"start\":54452},{\"end\":54466,\"start\":54459},{\"end\":54748,\"start\":54743},{\"end\":54766,\"start\":54757},{\"end\":54994,\"start\":54989},{\"end\":55013,\"start\":55005},{\"end\":55026,\"start\":55020},{\"end\":55042,\"start\":55035},{\"end\":55059,\"start\":55049},{\"end\":55080,\"start\":55070},{\"end\":55096,\"start\":55088},{\"end\":55114,\"start\":55105},{\"end\":55362,\"start\":55355},{\"end\":55387,\"start\":55372},{\"end\":55400,\"start\":55395},{\"end\":55414,\"start\":55406},{\"end\":55418,\"start\":55416},{\"end\":55598,\"start\":55596},{\"end\":55616,\"start\":55608},{\"end\":55812,\"start\":55810},{\"end\":55827,\"start\":55822},{\"end\":55841,\"start\":55838},{\"end\":55851,\"start\":55848},{\"end\":56057,\"start\":56050},{\"end\":56070,\"start\":56066},{\"end\":56081,\"start\":56079},{\"end\":56097,\"start\":56090},{\"end\":56448,\"start\":56445},{\"end\":56460,\"start\":56456},{\"end\":56476,\"start\":56472},{\"end\":56490,\"start\":56486},{\"end\":56766,\"start\":56762},{\"end\":56776,\"start\":56771},{\"end\":56785,\"start\":56781},{\"end\":56808,\"start\":56799},{\"end\":57113,\"start\":57103},{\"end\":57129,\"start\":57120},{\"end\":57148,\"start\":57142},{\"end\":57466,\"start\":57463},{\"end\":57796,\"start\":57792},{\"end\":57805,\"start\":57802},{\"end\":57819,\"start\":57815},{\"end\":58079,\"start\":58075},{\"end\":58093,\"start\":58090},{\"end\":58107,\"start\":58103},{\"end\":58399,\"start\":58385},{\"end\":58416,\"start\":58410},{\"end\":58626,\"start\":58619},{\"end\":58642,\"start\":58635},{\"end\":58659,\"start\":58653},{\"end\":58945,\"start\":58940},{\"end\":58961,\"start\":58954},{\"end\":58977,\"start\":58969},{\"end\":58995,\"start\":58984},{\"end\":59009,\"start\":59006},{\"end\":59335,\"start\":59328},{\"end\":59349,\"start\":59343},{\"end\":59546,\"start\":59542},{\"end\":59558,\"start\":59555},{\"end\":59568,\"start\":59565},{\"end\":59581,\"start\":59576},{\"end\":59594,\"start\":59588},{\"end\":59603,\"start\":59599},{\"end\":59809,\"start\":59805},{\"end\":59820,\"start\":59815},{\"end\":59835,\"start\":59828},{\"end\":59849,\"start\":59842},{\"end\":59862,\"start\":59858},{\"end\":59875,\"start\":59869},{\"end\":59882,\"start\":59877},{\"end\":60233,\"start\":60221},{\"end\":60242,\"start\":60240},{\"end\":60257,\"start\":60249},{\"end\":60262,\"start\":60259},{\"end\":60555,\"start\":60548},{\"end\":60570,\"start\":60564},{\"end\":60583,\"start\":60579},{\"end\":60599,\"start\":60593},{\"end\":60977,\"start\":60974},{\"end\":60996,\"start\":60990},{\"end\":61002,\"start\":60998},{\"end\":61227,\"start\":61222},{\"end\":61245,\"start\":61237},{\"end\":61262,\"start\":61255},{\"end\":61278,\"start\":61272},{\"end\":61589,\"start\":61583},{\"end\":61602,\"start\":61597},{\"end\":61615,\"start\":61610},{\"end\":61631,\"start\":61624},{\"end\":61870,\"start\":61865},{\"end\":61884,\"start\":61881},{\"end\":61899,\"start\":61891},{\"end\":61915,\"start\":61908},{\"end\":61928,\"start\":61922},{\"end\":62197,\"start\":62192},{\"end\":62211,\"start\":62208},{\"end\":62226,\"start\":62218},{\"end\":62239,\"start\":62233},{\"end\":62533,\"start\":62528},{\"end\":62551,\"start\":62545},{\"end\":62567,\"start\":62561},{\"end\":62826,\"start\":62821},{\"end\":62842,\"start\":62834},{\"end\":62860,\"start\":62854},{\"end\":62876,\"start\":62870},{\"end\":63176,\"start\":63171},{\"end\":63193,\"start\":63185},{\"end\":63211,\"start\":63205},{\"end\":63227,\"start\":63221},{\"end\":63505,\"start\":63494},{\"end\":63520,\"start\":63513},{\"end\":63533,\"start\":63528},{\"end\":63550,\"start\":63542},{\"end\":63562,\"start\":63558},{\"end\":63576,\"start\":63572},{\"end\":63902,\"start\":63899},{\"end\":63910,\"start\":63906},{\"end\":63924,\"start\":63921},{\"end\":63939,\"start\":63934},{\"end\":63946,\"start\":63941},{\"end\":64214,\"start\":64206},{\"end\":64232,\"start\":64223},{\"end\":64446,\"start\":64438},{\"end\":64466,\"start\":64457},{\"end\":64488,\"start\":64481},{\"end\":64727,\"start\":64724},{\"end\":64740,\"start\":64736},{\"end\":64753,\"start\":64747},{\"end\":64945,\"start\":64942},{\"end\":64958,\"start\":64952},{\"end\":65262,\"start\":65258},{\"end\":65271,\"start\":65267},{\"end\":65280,\"start\":65277},{\"end\":65587,\"start\":65583},{\"end\":65602,\"start\":65598},{\"end\":65619,\"start\":65611},{\"end\":65632,\"start\":65628},{\"end\":65649,\"start\":65645},{\"end\":65670,\"start\":65660},{\"end\":66019,\"start\":66014},{\"end\":66033,\"start\":66026},{\"end\":66049,\"start\":66042},{\"end\":66062,\"start\":66056},{\"end\":66310,\"start\":66305},{\"end\":66324,\"start\":66317},{\"end\":66337,\"start\":66331},{\"end\":66353,\"start\":66346},{\"end\":66618,\"start\":66613},{\"end\":66632,\"start\":66625},{\"end\":66644,\"start\":66639},{\"end\":66657,\"start\":66651},{\"end\":66673,\"start\":66666},{\"end\":66901,\"start\":66889},{\"end\":66915,\"start\":66908},{\"end\":66935,\"start\":66924},{\"end\":66960,\"start\":66948},{\"end\":67278,\"start\":67273},{\"end\":67294,\"start\":67287},{\"end\":67311,\"start\":67303},{\"end\":67328,\"start\":67322},{\"end\":67665,\"start\":67663},{\"end\":67680,\"start\":67676},{\"end\":67695,\"start\":67689},{\"end\":67709,\"start\":67705},{\"end\":67724,\"start\":67719},{\"end\":68100,\"start\":68096},{\"end\":68110,\"start\":68107},{\"end\":68126,\"start\":68122},{\"end\":68140,\"start\":68136},{\"end\":68523,\"start\":68521},{\"end\":68535,\"start\":68533},{\"end\":68547,\"start\":68543},{\"end\":68558,\"start\":68555},{\"end\":68856,\"start\":68853},{\"end\":68868,\"start\":68864},{\"end\":68884,\"start\":68880},{\"end\":68900,\"start\":68894},{\"end\":69211,\"start\":69206},{\"end\":69225,\"start\":69219},{\"end\":69233,\"start\":69231},{\"end\":69242,\"start\":69240},{\"end\":69524,\"start\":69521},{\"end\":69536,\"start\":69534},{\"end\":69550,\"start\":69547},{\"end\":69565,\"start\":69560},{\"end\":69571,\"start\":69567},{\"end\":69843,\"start\":69840},{\"end\":69855,\"start\":69853},{\"end\":69881,\"start\":69865},{\"end\":69887,\"start\":69883}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":51033,\"start\":50671},{\"attributes\":{\"id\":\"b1\"},\"end\":51464,\"start\":51035},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2127515},\"end\":51799,\"start\":51466},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":19598463},\"end\":51988,\"start\":51801},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":51980671},\"end\":52246,\"start\":51990},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":88516328},\"end\":52603,\"start\":52248},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3429309},\"end\":53111,\"start\":52605},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":174799862},\"end\":53488,\"start\":53113},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3780471},\"end\":53851,\"start\":53490},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":502946},\"end\":54346,\"start\":53853},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":57246310},\"end\":54681,\"start\":54348},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6755881},\"end\":54930,\"start\":54683},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2871880},\"end\":55344,\"start\":54932},{\"attributes\":{\"id\":\"b13\"},\"end\":55541,\"start\":55346},{\"attributes\":{\"id\":\"b14\"},\"end\":55754,\"start\":55543},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":56043,\"start\":55756},{\"attributes\":{\"doi\":\"arXiv:1612.02649\",\"id\":\"b16\"},\"end\":56381,\"start\":56045},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":216540785},\"end\":56684,\"start\":56383},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":57572938},\"end\":57031,\"start\":56686},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":195908774},\"end\":57354,\"start\":57033},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":18507866},\"end\":57718,\"start\":57356},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":556999},\"end\":58020,\"start\":57720},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":46784066},\"end\":58345,\"start\":58022},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5855042},\"end\":58531,\"start\":58347},{\"attributes\":{\"doi\":\"arXiv:1711.10288\",\"id\":\"b24\"},\"end\":58884,\"start\":58533},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":13815143},\"end\":59231,\"start\":58886},{\"attributes\":{\"id\":\"b26\"},\"end\":59479,\"start\":59233},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":54458071},\"end\":59794,\"start\":59481},{\"attributes\":{\"doi\":\"arXiv:1710.06924\",\"id\":\"b28\"},\"end\":60131,\"start\":59796},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10328909},\"end\":60484,\"start\":60133},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5844139},\"end\":60897,\"start\":60486},{\"attributes\":{\"id\":\"b31\"},\"end\":61144,\"start\":60899},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":43564031},\"end\":61528,\"start\":61146},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":7534823},\"end\":61800,\"start\":61530},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":118674674},\"end\":62128,\"start\":61802},{\"attributes\":{\"doi\":\"arXiv:2002.07953\",\"id\":\"b35\"},\"end\":62458,\"start\":62130},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12570770},\"end\":62744,\"start\":62460},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4619542},\"end\":63114,\"start\":62746},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13752992},\"end\":63407,\"start\":63116},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":8229065},\"end\":63840,\"start\":63409},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3461223},\"end\":64130,\"start\":63842},{\"attributes\":{\"id\":\"b41\"},\"end\":64360,\"start\":64132},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":17547265},\"end\":64666,\"start\":64362},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":16439870},\"end\":64870,\"start\":64668},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12453047},\"end\":65175,\"start\":64872},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":213004369},\"end\":65503,\"start\":65177},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":3556146},\"end\":65954,\"start\":65505},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2655115},\"end\":66252,\"start\":65956},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":4357800},\"end\":66549,\"start\":66254},{\"attributes\":{\"id\":\"b49\"},\"end\":66822,\"start\":66551},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":2928248},\"end\":67193,\"start\":66824},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":19059279},\"end\":67562,\"start\":67195},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":54216961},\"end\":68001,\"start\":67564},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":202775285},\"end\":68411,\"start\":68003},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":199405326},\"end\":68770,\"start\":68413},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":174800461},\"end\":69122,\"start\":68772},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":51729348},\"end\":69476,\"start\":69124},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":201646241},\"end\":69742,\"start\":69478},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":52954862},\"end\":70105,\"start\":69744}]", "bib_title": "[{\"end\":51113,\"start\":51035},{\"end\":51492,\"start\":51466},{\"end\":51827,\"start\":51801},{\"end\":52027,\"start\":51990},{\"end\":52307,\"start\":52248},{\"end\":52716,\"start\":52605},{\"end\":53212,\"start\":53113},{\"end\":53551,\"start\":53490},{\"end\":53914,\"start\":53853},{\"end\":54399,\"start\":54348},{\"end\":54732,\"start\":54683},{\"end\":54978,\"start\":54932},{\"end\":55586,\"start\":55543},{\"end\":55800,\"start\":55756},{\"end\":56438,\"start\":56383},{\"end\":56751,\"start\":56686},{\"end\":57096,\"start\":57033},{\"end\":57451,\"start\":57356},{\"end\":57780,\"start\":57720},{\"end\":58063,\"start\":58022},{\"end\":58375,\"start\":58347},{\"end\":58934,\"start\":58886},{\"end\":59531,\"start\":59481},{\"end\":60211,\"start\":60133},{\"end\":60536,\"start\":60486},{\"end\":61214,\"start\":61146},{\"end\":61576,\"start\":61530},{\"end\":61855,\"start\":61802},{\"end\":62518,\"start\":62460},{\"end\":62811,\"start\":62746},{\"end\":63161,\"start\":63116},{\"end\":63485,\"start\":63409},{\"end\":63893,\"start\":63842},{\"end\":64428,\"start\":64362},{\"end\":64714,\"start\":64668},{\"end\":64932,\"start\":64872},{\"end\":65252,\"start\":65177},{\"end\":65572,\"start\":65505},{\"end\":66007,\"start\":65956},{\"end\":66298,\"start\":66254},{\"end\":66879,\"start\":66824},{\"end\":67262,\"start\":67195},{\"end\":67651,\"start\":67564},{\"end\":68088,\"start\":68003},{\"end\":68512,\"start\":68413},{\"end\":68843,\"start\":68772},{\"end\":69196,\"start\":69124},{\"end\":69514,\"start\":69478},{\"end\":69833,\"start\":69744}]", "bib_author": "[{\"end\":50750,\"start\":50734},{\"end\":50764,\"start\":50750},{\"end\":50778,\"start\":50764},{\"end\":50792,\"start\":50778},{\"end\":50810,\"start\":50792},{\"end\":50836,\"start\":50810},{\"end\":51139,\"start\":51115},{\"end\":51157,\"start\":51139},{\"end\":51170,\"start\":51157},{\"end\":51185,\"start\":51170},{\"end\":51201,\"start\":51185},{\"end\":51518,\"start\":51494},{\"end\":51537,\"start\":51518},{\"end\":51555,\"start\":51537},{\"end\":51571,\"start\":51555},{\"end\":51586,\"start\":51571},{\"end\":51843,\"start\":51829},{\"end\":51858,\"start\":51843},{\"end\":51864,\"start\":51858},{\"end\":52043,\"start\":52029},{\"end\":52053,\"start\":52043},{\"end\":52069,\"start\":52053},{\"end\":52083,\"start\":52069},{\"end\":52323,\"start\":52309},{\"end\":52336,\"start\":52323},{\"end\":52352,\"start\":52336},{\"end\":52366,\"start\":52352},{\"end\":52378,\"start\":52366},{\"end\":52736,\"start\":52718},{\"end\":52755,\"start\":52736},{\"end\":52773,\"start\":52755},{\"end\":52787,\"start\":52773},{\"end\":52802,\"start\":52787},{\"end\":53228,\"start\":53214},{\"end\":53240,\"start\":53228},{\"end\":53256,\"start\":53240},{\"end\":53270,\"start\":53256},{\"end\":53565,\"start\":53553},{\"end\":53573,\"start\":53565},{\"end\":53593,\"start\":53573},{\"end\":53606,\"start\":53593},{\"end\":53620,\"start\":53606},{\"end\":53931,\"start\":53916},{\"end\":53946,\"start\":53931},{\"end\":53963,\"start\":53946},{\"end\":53977,\"start\":53963},{\"end\":53995,\"start\":53977},{\"end\":54013,\"start\":53995},{\"end\":54025,\"start\":54013},{\"end\":54038,\"start\":54025},{\"end\":54053,\"start\":54038},{\"end\":54411,\"start\":54401},{\"end\":54421,\"start\":54411},{\"end\":54437,\"start\":54421},{\"end\":54448,\"start\":54437},{\"end\":54456,\"start\":54448},{\"end\":54468,\"start\":54456},{\"end\":54750,\"start\":54734},{\"end\":54768,\"start\":54750},{\"end\":54996,\"start\":54980},{\"end\":55015,\"start\":54996},{\"end\":55028,\"start\":55015},{\"end\":55044,\"start\":55028},{\"end\":55061,\"start\":55044},{\"end\":55082,\"start\":55061},{\"end\":55098,\"start\":55082},{\"end\":55116,\"start\":55098},{\"end\":55364,\"start\":55346},{\"end\":55389,\"start\":55364},{\"end\":55402,\"start\":55389},{\"end\":55416,\"start\":55402},{\"end\":55420,\"start\":55416},{\"end\":55600,\"start\":55588},{\"end\":55618,\"start\":55600},{\"end\":55814,\"start\":55802},{\"end\":55829,\"start\":55814},{\"end\":55843,\"start\":55829},{\"end\":55853,\"start\":55843},{\"end\":56059,\"start\":56045},{\"end\":56072,\"start\":56059},{\"end\":56083,\"start\":56072},{\"end\":56099,\"start\":56083},{\"end\":56450,\"start\":56440},{\"end\":56462,\"start\":56450},{\"end\":56478,\"start\":56462},{\"end\":56492,\"start\":56478},{\"end\":56768,\"start\":56753},{\"end\":56778,\"start\":56768},{\"end\":56787,\"start\":56778},{\"end\":56810,\"start\":56787},{\"end\":57115,\"start\":57098},{\"end\":57131,\"start\":57115},{\"end\":57150,\"start\":57131},{\"end\":57468,\"start\":57453},{\"end\":57798,\"start\":57782},{\"end\":57807,\"start\":57798},{\"end\":57821,\"start\":57807},{\"end\":57840,\"start\":57821},{\"end\":58081,\"start\":58065},{\"end\":58095,\"start\":58081},{\"end\":58109,\"start\":58095},{\"end\":58128,\"start\":58109},{\"end\":58401,\"start\":58377},{\"end\":58418,\"start\":58401},{\"end\":58628,\"start\":58612},{\"end\":58644,\"start\":58628},{\"end\":58661,\"start\":58644},{\"end\":58947,\"start\":58936},{\"end\":58963,\"start\":58947},{\"end\":58979,\"start\":58963},{\"end\":58997,\"start\":58979},{\"end\":59011,\"start\":58997},{\"end\":59337,\"start\":59322},{\"end\":59351,\"start\":59337},{\"end\":59548,\"start\":59533},{\"end\":59560,\"start\":59548},{\"end\":59570,\"start\":59560},{\"end\":59583,\"start\":59570},{\"end\":59596,\"start\":59583},{\"end\":59605,\"start\":59596},{\"end\":59811,\"start\":59796},{\"end\":59822,\"start\":59811},{\"end\":59837,\"start\":59822},{\"end\":59851,\"start\":59837},{\"end\":59864,\"start\":59851},{\"end\":59877,\"start\":59864},{\"end\":59884,\"start\":59877},{\"end\":60235,\"start\":60213},{\"end\":60244,\"start\":60235},{\"end\":60259,\"start\":60244},{\"end\":60264,\"start\":60259},{\"end\":60557,\"start\":60538},{\"end\":60572,\"start\":60557},{\"end\":60585,\"start\":60572},{\"end\":60601,\"start\":60585},{\"end\":60979,\"start\":60972},{\"end\":60998,\"start\":60979},{\"end\":61004,\"start\":60998},{\"end\":61229,\"start\":61216},{\"end\":61247,\"start\":61229},{\"end\":61264,\"start\":61247},{\"end\":61280,\"start\":61264},{\"end\":61591,\"start\":61578},{\"end\":61604,\"start\":61591},{\"end\":61617,\"start\":61604},{\"end\":61633,\"start\":61617},{\"end\":61872,\"start\":61857},{\"end\":61886,\"start\":61872},{\"end\":61901,\"start\":61886},{\"end\":61917,\"start\":61901},{\"end\":61930,\"start\":61917},{\"end\":62199,\"start\":62184},{\"end\":62213,\"start\":62199},{\"end\":62228,\"start\":62213},{\"end\":62241,\"start\":62228},{\"end\":62535,\"start\":62520},{\"end\":62553,\"start\":62535},{\"end\":62569,\"start\":62553},{\"end\":62828,\"start\":62813},{\"end\":62844,\"start\":62828},{\"end\":62862,\"start\":62844},{\"end\":62878,\"start\":62862},{\"end\":63178,\"start\":63163},{\"end\":63195,\"start\":63178},{\"end\":63213,\"start\":63195},{\"end\":63229,\"start\":63213},{\"end\":63507,\"start\":63487},{\"end\":63522,\"start\":63507},{\"end\":63535,\"start\":63522},{\"end\":63552,\"start\":63535},{\"end\":63564,\"start\":63552},{\"end\":63578,\"start\":63564},{\"end\":63904,\"start\":63895},{\"end\":63912,\"start\":63904},{\"end\":63926,\"start\":63912},{\"end\":63941,\"start\":63926},{\"end\":63948,\"start\":63941},{\"end\":64216,\"start\":64200},{\"end\":64234,\"start\":64216},{\"end\":64448,\"start\":64430},{\"end\":64468,\"start\":64448},{\"end\":64490,\"start\":64468},{\"end\":64729,\"start\":64716},{\"end\":64742,\"start\":64729},{\"end\":64755,\"start\":64742},{\"end\":64947,\"start\":64934},{\"end\":64960,\"start\":64947},{\"end\":65264,\"start\":65254},{\"end\":65273,\"start\":65264},{\"end\":65282,\"start\":65273},{\"end\":65589,\"start\":65574},{\"end\":65604,\"start\":65589},{\"end\":65621,\"start\":65604},{\"end\":65634,\"start\":65621},{\"end\":65651,\"start\":65634},{\"end\":65672,\"start\":65651},{\"end\":66021,\"start\":66009},{\"end\":66035,\"start\":66021},{\"end\":66051,\"start\":66035},{\"end\":66064,\"start\":66051},{\"end\":66312,\"start\":66300},{\"end\":66326,\"start\":66312},{\"end\":66339,\"start\":66326},{\"end\":66355,\"start\":66339},{\"end\":66620,\"start\":66608},{\"end\":66634,\"start\":66620},{\"end\":66646,\"start\":66634},{\"end\":66659,\"start\":66646},{\"end\":66675,\"start\":66659},{\"end\":66903,\"start\":66881},{\"end\":66917,\"start\":66903},{\"end\":66937,\"start\":66917},{\"end\":66962,\"start\":66937},{\"end\":67280,\"start\":67264},{\"end\":67296,\"start\":67280},{\"end\":67313,\"start\":67296},{\"end\":67330,\"start\":67313},{\"end\":67667,\"start\":67653},{\"end\":67682,\"start\":67667},{\"end\":67697,\"start\":67682},{\"end\":67711,\"start\":67697},{\"end\":67726,\"start\":67711},{\"end\":68102,\"start\":68090},{\"end\":68112,\"start\":68102},{\"end\":68128,\"start\":68112},{\"end\":68142,\"start\":68128},{\"end\":68161,\"start\":68142},{\"end\":68525,\"start\":68514},{\"end\":68537,\"start\":68525},{\"end\":68549,\"start\":68537},{\"end\":68560,\"start\":68549},{\"end\":68858,\"start\":68845},{\"end\":68870,\"start\":68858},{\"end\":68886,\"start\":68870},{\"end\":68902,\"start\":68886},{\"end\":69213,\"start\":69198},{\"end\":69227,\"start\":69213},{\"end\":69235,\"start\":69227},{\"end\":69244,\"start\":69235},{\"end\":69526,\"start\":69516},{\"end\":69538,\"start\":69526},{\"end\":69552,\"start\":69538},{\"end\":69567,\"start\":69552},{\"end\":69573,\"start\":69567},{\"end\":69845,\"start\":69835},{\"end\":69857,\"start\":69845},{\"end\":69883,\"start\":69857},{\"end\":69889,\"start\":69883}]", "bib_venue": "[{\"end\":50732,\"start\":50671},{\"end\":51238,\"start\":51201},{\"end\":51622,\"start\":51586},{\"end\":51886,\"start\":51864},{\"end\":52105,\"start\":52083},{\"end\":52415,\"start\":52378},{\"end\":52840,\"start\":52802},{\"end\":53291,\"start\":53270},{\"end\":53657,\"start\":53620},{\"end\":54090,\"start\":54053},{\"end\":54505,\"start\":54468},{\"end\":54789,\"start\":54768},{\"end\":55120,\"start\":55116},{\"end\":55439,\"start\":55420},{\"end\":55640,\"start\":55618},{\"end\":55890,\"start\":55853},{\"end\":56188,\"start\":56115},{\"end\":56514,\"start\":56492},{\"end\":56847,\"start\":56810},{\"end\":57183,\"start\":57150},{\"end\":57523,\"start\":57468},{\"end\":57861,\"start\":57840},{\"end\":58164,\"start\":58128},{\"end\":58422,\"start\":58418},{\"end\":58610,\"start\":58533},{\"end\":59048,\"start\":59011},{\"end\":59320,\"start\":59233},{\"end\":59627,\"start\":59605},{\"end\":59938,\"start\":59900},{\"end\":60300,\"start\":60264},{\"end\":60623,\"start\":60601},{\"end\":60970,\"start\":60899},{\"end\":61317,\"start\":61280},{\"end\":61655,\"start\":61633},{\"end\":61952,\"start\":61930},{\"end\":62182,\"start\":62130},{\"end\":62590,\"start\":62569},{\"end\":62915,\"start\":62878},{\"end\":63251,\"start\":63229},{\"end\":63615,\"start\":63578},{\"end\":63978,\"start\":63948},{\"end\":64198,\"start\":64132},{\"end\":64494,\"start\":64490},{\"end\":64759,\"start\":64755},{\"end\":65014,\"start\":64960},{\"end\":65319,\"start\":65282},{\"end\":65709,\"start\":65672},{\"end\":66086,\"start\":66064},{\"end\":66392,\"start\":66355},{\"end\":66606,\"start\":66551},{\"end\":66999,\"start\":66962},{\"end\":67367,\"start\":67330},{\"end\":67763,\"start\":67726},{\"end\":68197,\"start\":68161},{\"end\":68582,\"start\":68560},{\"end\":68923,\"start\":68902},{\"end\":69281,\"start\":69244},{\"end\":69595,\"start\":69573},{\"end\":69911,\"start\":69889}]"}}}, "year": 2023, "month": 12, "day": 17}