{"id": 257482728, "updated": "2023-10-05 03:34:24.943", "metadata": {"title": "3D Cinemagraphy from a Single Image", "authors": "[{\"first\":\"Xingyi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Zhiguo\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Huiqiang\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Jianming\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ke\",\"last\":\"Xian\",\"middle\":[]},{\"first\":\"Guosheng\",\"last\":\"Lin\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We present 3D Cinemagraphy, a new technique that marries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera motion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to obvious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unprojecting them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emergence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthesize novel views by separately projecting them into target image planes and blending the results. Extensive experiments demonstrate the effectiveness of our method. A user study is also conducted to validate the compelling rendering results of our method.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2303.05724", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Li0S0XL23", "doi": "10.1109/cvpr52729.2023.00446"}}, "content": {"source": {"pdf_hash": "df8508daa8b5486070eef0b2d0e6e4d452a108b0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.05724v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f4225d36fbb3e26ef8b35470956ce6d98e98e476", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/df8508daa8b5486070eef0b2d0e6e4d452a108b0.txt", "contents": "\n3D Cinemagraphy from a Single Image\n\n\nXingyi Li \nKey Laboratory of Image Processing and Intelligent Control\nMinistry of Education School of Artificial Intelligence and Automation\nHuazhong University of Science and Technology\n\n\nZhiguo Cao \nKey Laboratory of Image Processing and Intelligent Control\nMinistry of Education School of Artificial Intelligence and Automation\nHuazhong University of Science and Technology\n\n\nHuiqiang Sun \nKey Laboratory of Image Processing and Intelligent Control\nMinistry of Education School of Artificial Intelligence and Automation\nHuazhong University of Science and Technology\n\n\nJianming Zhang \nAdobe Research 3 S-Lab\nNanyang Technological University\n\n\nKe Xian \nGuosheng Lin gslin@ntu.edu.sg \n3D Cinemagraphy from a Single Image\n* Corresponding author.\nhttps://xingyi-li.github.io/3d-cinemagraphyFigure 1. Given a single still image, our method can synthesize videos with plausible animation of the scene while allowing camera movements. Here, we showcase four 3D cinemagraphs with various camera trajectories. Besides real-world photos (the left two examples), our method can also generalize to paintings (the third one) and synthetic images generated by Stable Diffusion [47] (the rightmost one). To see the effect of 3D cinemagraphy, readers are encouraged to view with Adobe Acrobat or KDE Okular.AbstractWe present 3D Cinemagraphy, a new technique that marries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera motion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to obvious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unprojecting them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emergence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthesize novel views by separately projecting them into target image planes and blending the results. Extensive experiments demonstrate the effectiveness of our method. A user study is also conducted to validate the compelling rendering results of our method.\n Figure 1\n. Given a single still image, our method can synthesize videos with plausible animation of the scene while allowing camera movements. Here, we showcase four 3D cinemagraphs with various camera trajectories. Besides real-world photos (the left two examples), our method can also generalize to paintings (the third one) and synthetic images generated by Stable Diffusion [47] (the rightmost one). To see the effect of 3D cinemagraphy, readers are encouraged to view with Adobe Acrobat or KDE Okular.\n\n\nAbstract\n\nWe present 3D Cinemagraphy, a new technique that marries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera motion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to obvious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unprojecting them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emergence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthesize novel views by separately projecting them into target image planes and blending the results. Extensive experiments demonstrate the effectiveness of our method. A user study is also conducted to validate the compelling rendering results of our method.\n\n\nIntroduction\n\nNowadays, since people can easily take images using smartphone cameras, the number of online photos has increased drastically. However, with the rise of online videosharing platforms such as YouTube and TikTok, people are no longer content with static images as they have grown accustomed to watching videos. It would be great if we could animate those still images and synthesize videos for a better experience. These living images, termed cinemagraphs, have already been created and gained rapid popularity online [1,71]. Although cinemagraphs may engage people with the content for longer than a regular photo, they usually fail to deliver an immersive sense of 3D to audiences. This is because cinemagraphs are usually based on a static camera and fail to produce parallax effects. We are therefore motivated to explore ways of animating the photos and moving around the cameras at the same time. As shown in Fig. 1, this will bring many still images to life and provide a drastically vivid experience.\n\nIn this paper, we are interested in making the first step towards 3D cinemagraphy that allows both realistic animation of the scene and camera motions with compelling parallax effects from a single image. There are plenty of attempts to tackle either of the two problems. Single-image animation methods [12,19,35] manage to produce a real-istic animated video from a single image, but they usually operate in 2D space, and therefore they cannot create camera movement effects. Classic novel view synthesis methods [5,6,9,14,25] and recent implicit neural representations [37,40,58] entail densely captured views as input to render unseen camera perspectives. Single-shot novel view synthesis approaches [21,39,52,66] exhibit the potential for generating novel camera trajectories of the scene from a single image. Nonetheless, these methods usually hypothesize that the observed scene is static without moving elements. Directly combining existing state-of-the-art solutions of single-image animation and novel view synthesis yields visual artifacts or inconsistent animation.\n\nTo address the above challenges, we present a novel framework that solves the joint task of image animation and novel view synthesis. This framework can be trained to create 3D cinemagraphs from a single still image. Our key intuition is that handling this new task in 3D space would naturally enable both animation and moving cameras simultaneously. With this in mind, we first represent the scene as feature-based layered depth images (LDIs) [50] and unproject the feature LDIs into a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion to 3D scene flow using depth values predicted by DPT [45]. Next, we animate the point cloud according to the scene flow. To resolve the problem of hole emergence as points move forward, we are inspired by prior works [3,19,38] and propose a 3D symmetric animation technique to bidirectionally displace point clouds, which can effectively fill in those unknown regions. Finally, we synthesize novel views at time t by rendering point clouds into target image planes and blending the results. In this manner, our proposed method can automatically create 3D cinemagraphs from a single image. Moreover, our framework is highly extensible, e.g., we can augment our motion estimator with user-defined masks and flow hints for accurate flow estimation and controllable animation.\n\nIn summary, our main contributions are:\n\n\u2022 We propose a new task of creating 3D cinemagraphs from single images. To this end, we propose a novel framework that jointly learns to solve the task of image animation and novel view synthesis in 3D space.\n\n\u2022 We design a 3D symmetric animation technique to address the hole problem as points move forward.\n\n\u2022 Our framework is flexible and customized. We can achieve controllable animation by augmenting our motion estimator with user-defined masks and flow hints.\n\n\nRelated Work\n\nSingle-image animation. Different kinds of methods have been explored to animate still images. Some works [8,22] focus on animating certain objects via physical simulation but may not be easily applied to more general cases of inthe-wild photos. Given driving videos as guidance, there are plenty of methods that attempt to perform motion transfer on static objects with either a priori knowledge of moving objects [7,11,33,46,55] or in an unsupervised manner [53,54,56]. They entail reference videos to drive the motion of static objects, and thus do not suit our task. Recent advances in generative models have attracted much attention and motivated the community to develop realistic image and video synthesis methods. Many works [31,32,34,51,69] are based on generative adversarial networks (GANs) and operate transformations in latent space to generate plausible appearance changes and movements. Nonetheless, it is non-trial to allow for explicit control over those latent codes and to animate input imagery in a disentangled manner. As diffusion models [17,59] improve by leaps and bounds, several diffusion-based works [16,18,57] attempt to generate realistic videos from text or images. However, these methods are time-consuming and expensive in terms of computation. Here we focus on methods that utilize learned motion priors to convert a still image into an animated video texture [12,13,19,29,35]. In particular, Holynski et al. [19] first synthesize the optical flow of the input image via a motion estimation network, then obtain future frames using the estimated flow field. This method renders plausible animation of fluid elements in the input image but suffers from producing camera motions with parallax.\n\nNovel view synthesis from a single image. Novel view synthesis allows for rendering unseen camera perspectives from 2D images and their corresponding camera poses. Recent impressive synthesis results may credit to implicit neural representations [37,40,58]. Nevertheless, these methods usually assume dense views as input, which is not always available in most cases. Moreover, they focus on the task of interpolation given multiple views rather than extrapolation. As such, we instead turn to methods aiming at handling single input. Among them, a number of works [15,26,28,62,63,70,72] infer the 3D structure of scenes by learning to predict a scene representation from a single image. These methods are usually trained end-to-end but suffer from generalizing to in-the-wild photos. Most relevant to our work are those approaches [39,52,66] that apply depth estimation [45,65,67,68] followed by inpainting occluded regions. For example, 3D Photo [52] estimates monocular depth maps and uses the representation of layered depth images (LDIs) [43,50], in which context-aware color and depth inpainting are performed. To enable finegrained detail modeling, SLIDE [21] decomposes the scene into foreground and background via a soft-layering scheme. However, unlike our approach, these methods usually assume the scene is static by default, which largely lessens the sense of reality, especially when some elements such as  Figure 2. An overview of our method. Given a single still image as input, we first predict a dense depth map. To represent the scene in 3D space, we separate the input image into several layers according to depth discontinuities and apply context-aware inpainting, yielding layered depth images (LDIs) L. We then use a 2D feature extractor to encode 2D feature maps for each inpainted LDI color layer, resulting in feature LDIs F. Subsequently, we lift feature LDIs into 3D space using corresponding depth values to obtain a feature point cloud P.\n\nTo animate the scene, we estimate a 2D motion field from the input image and apply Euler integration to generate forward and backward displacement fields F0\u2192t and F0\u2192t\u2212N . We then augment displacement fields with estimated depth values to obtain 3D scene flow fields. Next, we bidirectionally displace the feature point cloud P as per the scene flow and separately project them into target image planes to obtain F f and F b . Finally, we blend them together and pass the result through our image decoder to synthesize a novel view at time t.\n\na creek or smoke are also captured in the input image. Space-time view synthesis. Space-time view synthesis is the task of rendering novel camera perspectives for dynamic scenes in terms of space and time [30]. Most of the prior works [2,4,27] rely on synchronized multi-view videos as input, which prevents their wide applicability.\n\nTo mitigate this requirement, many neural rendering approaches [30,41,44] manage to show promising space-time view synthesis results from monocular videos. They usually train each new scene independently, and thus cannot directly handle in-the-wild inputs. Most related to our work, 3D Moments [64] introduces a novel 3D photography effect where cinematic camera motion and frame interpolation are simultaneously performed. However, this method demands near-duplicate photos as input and is unable to control the animation results. Instead, we show that our method can animate still images while enabling camera motion with 3D parallax. Moreover, we can also extend our system so that users are allowed to interactively control how the photos are animated by providing user-defined masks and flow hints.\n\n\nMethod\n\n\nOverview\n\nGiven a single still image, our goal is to synthesize plausible animation of the scene and simultaneously enable camera motion. The output of our method is a realistic cinemagraph with compelling parallax effects. Fig. 2 schematically illustrates our pipeline. Our method starts by estimating a motion field and a depth map from the input image. We then separate the RGBD input into several layers as per depth discontinuities and inpaint occluded regions, followed by extracting 2D feature maps for each layer, resulting in feature LDIs [50]. To enable scene animation, we lift the 2D motion to 3D scene flow and unproject feature LDIs into a feature point cloud using their corresponding depth values. Thereafter, we bidirectionally animate the point cloud with scene flow using our 3D symmetric animation technique. We end up rendering them into two animated feature maps and composite the results to synthesize novel views at time t.\n\n\nMotion Estimation\n\nTo animate a still image, we wish to estimate the corresponding motion field for the observed scene. Generally, the motion we witness in the real world is extremely complicated as it is time-varying and many events such as occlusion and collision could occur. Intuitively, we could directly adopt prior optical flow estimation methods [10,20,60,61] to accomplish this. However, it is not trivial since they usually take a pair of images as input to compute optical flow. Endo et al. [12] instead propose to learn and predict the motion in a recurrent manner, but this kind of approach is prone to large distortions in the long term. To simplify this, we follow Holynski et al. [19] and assume that a time-invariant and constant-velocity motion field, termed Eulerian flow field, can well approximate the bulk of real-world motions, e.g., water, smoke, and clouds. Formally, we denote M as the Eulerian flow field of the scene, which suggests that\nF t\u2192t+1 (\u00b7) = M (\u00b7),(1)\nwhere F t\u2192t+1 (\u00b7) represents the optical flow map from frame t to frame t + 1. This defines how each pixel in the current frame will move in the future. Specifically, we can obtain the next frame via Euler integration:\nx t+1 = x t + M (x t ),(2)\nwhere x t represents the coordinates of a pixel x t at time t.\n\nSince the optical flow between consecutive frames is identical, we can easily deduce the displacement field by recursively applying:\nF 0\u2192t (x 0 ) = F 0\u2192t\u22121 (x 0 ) + M (x 0 + F 0\u2192t\u22121 (x 0 )),(3)\nwhere F 0\u2192t (\u00b7) denotes the displacement field from time 0 to time t, which describes the course of each pixel in the input image across future frames. To estimate the Eulerian flow field, we adopt an image-to-image translation network as our motion estimator, which is able to map an RGB image to the optical flow.\n\n\n3D Scene Representation\n\nOne common disadvantage of previous single-image animation methods [12,19,29] is that they usually operate in 2D space via a deep image warping technique, which prevents them from creating parallax effects. Instead, to enable camera motion, we propose to lift our workspace into 3D and thus resort to 3D scene representation.\n\nWe start by estimating the underlying geometry of the scene using the state-of-the-art monocular depth estimator DPT [45], which can predict reasonable dense depth maps for in-the-wild photos. Following Wang et al. [64], we then convert the RGBD input into an LDI representation [50] by separating it into several layers as per depth discontinuities and inpainting occluded regions. Specifically, we first divide the depth range of the source depth map into multiple intervals using agglomerative clustering [36], followed by creating layered depth images L = {C l , D l } L l=1 . Next, we inpaint occluded regions of each color and depth layer by applying the pretrained inpainting model from 3D Photo [52]. To improve rendering quality and reduce artifacts, we also introduce a 2D feature extraction network to encode 2D feature maps for each inpainted LDI color layer, resulting in feature LDIs F = {F l , D l } L l=1 . Finally, in order to enable animation in 3D space, we unproject feature LDIs into 3D via their corresponding inpainted depth layers, yielding a feature point cloud P = {(X i , f i )}, where X i and f i are 3D coordinates and the feature vector for each 3D point respectively.\n\n\nPoint Cloud Animation and Rendering\n\nWe now have the estimated displacement fields F 0\u2192t and the feature point cloud P. Our next step is to animate this Merge Merge Figure 3. 3D symmetric animation. To address the hole issue, we borrow textural information from the point cloud that moves in the opposite direction and integrate both of the animated point clouds to feasibly fill in the missing regions (the red and blue regions). point cloud over time. To bridge the gap between 2D displacement fields and 3D scene representation, we first augment the displacement fields with estimated depth values to lift them into 3D scene flow. In other words, we now have a function of time t and the coordinates of a 3D point that returns a corresponding 3D translation vector that can shift this 3D point accordingly. Thus, for time t, we then move each 3D point by computing its destination as its original position plus a corresponding 3D translation vector, i.e.,\nP(t) = {(X i (t), f i )}.\nIntuitively, this process indeed animates the point cloud from one time to another. However, we empirically find that as points move forward, increasingly large holes emerge. This frequently happens when points leave their original locations without any points filling in those unknown regions. 3D symmetric animation. To resolve this, inspired by prior works [3,19,38], we propose a 3D symmetric animation technique that leverages bidirectionally displaced point clouds to complement each other. With 3D symmetric animation, we can borrow textural information from point clouds that move in the opposite direction and integrate both of the animated point clouds to feasibly fill in missing regions. Specifically, we directly replace the original Eulerian flow field M with \u2212M and recursively apply Eq. (3) to generate a reversed displacement field. Similarly, we then lift this 2D displacement field to obtain inverse scene flow, which is employed to produce point clouds with backward movements. As illustrated in Fig. 3, for time t, to fill in holes, we respectively apply F 0\u2192t and F 0\u2192t\u2212N to draw associated scene flow fields and use them to move the point cloud, resulting in P\nf (t) = {(X f i (t), f i )} and P b (t) = {(X b i (t), f i )},\nwhere N is the number of frames. Neural rendering. We now have two bidirectionally animated feature point clouds. Our final step is to render them into animated feature maps and composite the results for synthesizing novel views at time t. In particu-lar, given camera poses and intrinsics, we use a differentiable point-based renderer [66] to splat feature point clouds\nP f (t) = {(X f i (t), f i )} and P b (t) = {(X b i (t)\n, f i )} separately into the target image plane. This process yields 2D feature maps F f and F b along with depth maps D f , D b and alpha maps \u03b1 f , \u03b1 b . Next, we wish to fuse F f and F b into one feature map F t . Inspired by prior work [64], our intuition is three-fold: 1) to enable endless and seamless looping, we should assign the weight of the two feature maps based on time so as to guarantee that the first and last frame of the synthesized video are identical; 2) the weight map should favor pixel locations with smaller depth values, in the sense that it is impossible to see objects behind those objects closer to the eye; 3) to avoid missing regions as much as possible, we should greatly increase the contribution of those pixel locations that can fill in holes. With this in mind, we formulate the weight map as follows:\nW t = (1 \u2212 t N ) \u00b7 \u03b1 f \u00b7 e \u2212D f (1 \u2212 t N ) \u00b7 \u03b1 f \u00b7 e \u2212D f + t N \u00b7 \u03b1 b \u00b7 e \u2212D b ,(4)\nwhere N is the number of frames. Therefore, we can integrate F f and F b via:\nF t = W t \u00b7 F f + (1 \u2212 W t ) \u00b7 F b .(5)\nWe also obtain the merged depth map D t :\nD t = W t \u00b7 D f + (1 \u2212 W t ) \u00b7 D b .(6)\nFinally, we employ an image decoder network to map the 2D feature map F t and depth map D t to a novel view at time t. Repeating this method, we are able to synthesize a realistic cinemagraph with compelling parallax effects.\n\n\nTraining\n\nThis section describes our training scheme. In general, we train our image-to-image translation network, 2D feature extraction network, and image decoder network in a two-stage manner. Training dataset. We use the training set from Holynski et al. [19] as our training dataset. This dataset comprises short video clips of fluid motion that are extracted from longer stock-footage videos. We use the first frames of each video clip and the corresponding ground truth motion fields estimated by a pretrained optical flow network [60] as motion estimation pairs to train our motion estimation network. To develop animation ability, we randomly sample training data from fluid motion video clips. For novel view synthesis training, we require multi-view supervision of the same scene, which is not available in the training set. Instead, we use 3D Photo [52] to generate pseudo ground truth novel views for training. Two-stage training. Our model is trained in a two-stage manner. Specifically, we first train our motion estimation network using motion estimation pairs. To train the motion estimation network, we minimize GAN loss, GAN feature matching loss [49], and endpoint error as follows:\nL M otion = L GAN + 10L F M + L EP E .(7)\nIn the second stage, we freeze the motion estimation network and train the feature extraction network and image decoder network. Our model simultaneously learns to render novel views and animate scenes. For novel view synthesis, we set t = 0 and use pseudo ground truth novel views to supervise our model. We randomly sample target viewpoints of scenes and require the model to synthesize them. For animation, we train our model on training triplets (start frame, middle frame, end frame) sampled from fluid motion video clips. In particular, we render the middle frame from both directions using F 0\u2192t and F 0\u2192t\u2212N without changing the camera poses and intrinsics. Besides GAN loss and GAN feature matching loss [49], we also enforce VGG perceptual loss [23,73] and l 1 loss between synthesized and ground truth images. The overall loss is as follows:\nL Animation = L GAN + 10L F M + L l1 + L V GG . (8)\n\nExperiments\n\n\nImplementation Details\n\nOur motion estimator is a U-Net [48] based generator with 16 convolutional layers, and we replace Batch Normalization with SPADE [42]. For the feature extraction network and image decoder network, we follow the network architectures from Wang et al. [64]. We adopt the multi-scale discriminator used in SPADE [42] during training.\n\nOur model is trained using the Adam optimizer [24]. We conduct all experiments on a single NVIDIA GeForce RTX 3090 GPU. We train the motion estimation network for around 120k iterations with a batch size of 16. We set the generator learning rate to 5 \u00d7 10 \u22124 and the discriminator learning rate to 2 \u00d7 10 \u22123 . For the animation training stage, we train the feature extraction network and image decoder network for around 250k iterations with a learning rate starting at 1 \u00d7 10 \u22124 and then decaying exponentially.\n\n\nBaselines\n\nIn principle, to evaluate our method, we are required to compare it against current state-of-the-art models. However, to our knowledge, we are the first to tackle the novel task of synthesizing a realistic cinemagraph with compelling parallax effects from a single image. As a result, we cannot directly compare to previous works. Instead, we consider forming the following baselines to verify the superiority of our method: 2D animation \u2192 novel view synthesis. One might consider 2D image animation \u2192 single-shot novel view synthesis: first employing a 2D image animation method, then Table 1. Quantitative comparisons against all baselines on the validation set from Holynski et al. [19]. The better approach favors higher PSNR and SSIM but lower LPIPS. The best performance is in bold. a single-shot novel view synthesis method. Specifically, we first adopt a state-of-the-art image animation method [19] to produce an animated looping video. We then apply DPT [45] to estimate geometry and utilize 3D Photo [52] to generate novel views for each frame. Novel view synthesis \u2192 2D animation. It also appears to be feasible that we first render novel views of scenes by 3D Photo [52] and then use the image animation method [19] to animate each viewpoint. Note that motion estimation should be performed for each frame as viewpoints have changed. However, we empirically find that this usually results in varying motion fields across the video. To mitigate this, we further propose using the moving average technique to smooth estimated motions for each frame. This results in novel view synthesis \u2192 2D animation + MA. Naive point cloud animation. Intuitively, we may also consider directly unprojecting pixels into 3D space and subsequently moving and rendering the RGB point cloud. Specifically, given a single input image, we first predict the depth map using DPT [45] and estimate 2D optical flow. We then lift the pixels and optical flow into 3D space to form RGB point clouds and scene flow. Finally, we animate RGB point clouds over time according to the scene flow and project these point clouds into target viewpoints. This baseline also faces a similar issue: as time goes by, large holes gradually appear. One might also employ our 3D symmetric animation technique to further enhance this baseline, i.e., naive point cloud animation + 3DSA.\n\n\nResults\n\nEvaluation dataset. Since Holynski et al. [19] only provide a single image for each scene in the test set, we use the validation set from Holynski et al. [19] Table 1, our method outperforms all baselines across all metrics by a large margin. This result implies that our method achieves better perceptual quality and produces more realistic renderings, which demonstrates the superiority and effectiveness of our method. Qualitative comparisons. We showcase the visual comparisons in Fig. 4. One can observe that our method presents photorealistic results while other comparative baselines produce more or less visual artifacts. 2D Anim. [19] \u2192 NVS [52] intends to generate stripped flickering artifacts. This is because 2D Anim. [19] \u2192 NVS [52] predicts the depth map for each animated frame, leading to frequent changes in the 3D structure of the scene and inconsistent inpainting. NVS [52] \u2192 2D Anim. [19] and NVS [52] \u2192 2D Anim. [19] + MA show jelly-like effects as optical flow should be estimated for each novel view. This results in varying motion fields across the video and thus inconsistent animation. Although Naive PC Anim. and Naive PC Anim. + 3DSA also lift the workspace into 3D, they are often prone to produce noticeable holes inevitably. One reason for this is that they do not perform inpainting. Note that some artifacts are difficult to observe when only scanning static figures. Controllable animation. Our method is able to create 3D cinemagraphs from a single image automatically. Further, we show that our framework is also highly extensible. For example, we can involve masks and flow hints as extra in- Figure 4. Qualitative comparisons against all baselines on the validation set from Holynski et al. [19]. Our method produces compelling results while other comparative alternatives suffer from visual artifacts. (a) 2D animation [19] \u2192 novel view synthesis [52], (b) novel view synthesis [52] \u2192 2D animation [19], (c) novel view synthesis [52] \u2192 2D animation [19] + moving average, (d) naive point cloud animation, (e) naive point cloud animation + 3D symmetric animation, (f) our method, and (g) pseudo ground truth.\n(a) (b) (c) (d) (e) (g) (f)\n\nInput\n\n\nMasks & motion hints\n\nMotion fields Figure 5. Controllable animation. By changing the masks and motion hints, our method can interactively control the animation. puts to augment our motion estimator. This brings two advantages: (1) more accurate flow estimation; (2) interactive and controllable animation. As shown in Fig. 5, we can control the animation of the scene by providing various masks and motion hints to obtain different motion fields. Generalizing on in-the-wild photos. To further demonstrate the generalization of our method, we also test our method on in-the-wild photos. We first create cinemagraphs with camera motions on the test set from Holynski et al. [19], where, for each scene, only a single image is provided. We then select some online images at random to test our method. To accurately estimate motion fields, we provide masks and flow hints as extra inputs to our motion estimator. As shown in Fig. 6, our method produces reasonable results for in-the-wild inputs while other comparative alternatives yield visual artifacts or inconsistent animation.\n\n\nUser Study\n\nWe further conduct a user study to investigate how our method performs in the view of humans when compared with all baselines, 3D Photo [52], and Holynski et al. [19]. Specifically, we collect 50 photos from the test set of Holynski et al. [19] and the Internet. We use different approaches to generate videos with identical settings. During the study, we show each participant an input image and two animated videos generated by our method and a randomly selected approach in random order. 108 volunteers are invited to choose the method with better perceptual quality and realism, or none if it is hard to judge. We report the results in Table 2, which points out that our method surpasses alternative methods by a large margin in terms of the sense of reality and immersion.\n\n\nAblation Study\n\nTo validate the effect of each component, we conduct an ablation study on the validation set from Holynski et al. [19] and show the results in Table 3. One can observe: i) 3D symmetric animation technique matters because it allows us to leverage bidirectionally displaced point clouds to complement each other and feasibly fill in missing regions; ii) 2D Anim. \u2192 NVS NVS \u2192 2D Anim. + MA Naive PC Anim. + 3DSA Ours Input Figure 6. Visual comparisons on the test set from Holynski et al. [19] and in-the-wild photos. Our method consistently produces more realistic rendering with fewer visual artifacts as opposed to other baselines.\n\nintroducing inpainting when constructing 3D geometry can improve the performance as this allows our model to produce plausible structures around depth discontinuities and fill in holes; iii) switching from directly using RGB colors to features in 3D scene representation significantly improves the rendering quality and reduces artifacts.\n\n\nConclusion\n\nIn this paper, we introduce a novel task of creating 3D cinemagraphs from single images. To this end, we present a simple yet effective method that makes a connection between image animation and novel view synthesis. We show that our method produces plausible animation of the scene while allowing camera movements. Our framework is flexible and customized. For accurate motion estimation and controllable animation, we can further include masks and flow hints as extra input for the motion estimator. Therefore, users can control how the scene is animated. Furthermore, our method generalizes well to in-the-wild photos, even like paintings or synthetic images generated by diffusion models. We conduct extensive experiments to ver-ify the effectiveness and superiority of our method. A user study also demonstrates that our method generates realistic 3D cinemagraphs. We hope that our work can bring 3D cinemagraphy into the sight of a broader community and motivate further research. Limitations and future work. Our method may not work well when the depth prediction module estimates erroneous geometry from the input image, e.g., thin structures. In addition, inappropriate motion fields will sometimes lead to undesirable results, e.g., some regions are mistakenly identified as frozen. As we take the first step towards 3D cinemagraphy, in this paper, we focus on handling common moving elements, i.e., fluids. In other words, our method may not apply to more complex motions, e.g., cyclic motion. We leave this for our future work.\n\nTable 2 .\n2User study. Pairwise comparison results indicate that users prefer our method as more realistic and immersive.Comparison \nHuman preference \n\n2D Anim. [19] \u2192 NVS [52] / Ours \n12.5% / 87.5% \nNVS [52] \u2192 2D Anim. [19] / Ours \n3.9% / 96.1% \nNVS [52] \u2192 2D Anim. [19] + MA / Ours \n6.1% / 93.9% \nNaive PC Anim. / Ours \n7.6% / 92.4% \nNaive PC Anim. + 3DSA / Ours \n8.6% / 91.4% \n3D Photo [52] / Ours \n10.5% / 89.5% \nHolynski et al. [19] / Ours \n29.9% / 70.1% \n\nTable 3. Ablation study on each component of our method. \n\nPSNR\u2191 SSIM\u2191 \nLPIPS\u2193 \n\nw/o features \n21.50 \n0.674 \n0.228 \nw/o inpainting \n22.86 \n0.763 \n0.216 \nw/o 3D symmetric animation \n22.99 \n0.768 \n0.199 \nFull model \n23.33 \n0.776 \n0.197 \n\nwith the ground truth frames at the same time and view-\npoint. For a fair comparison, all methods utilize the depth \nmaps estimated by DPT [45]. Since we focus on com-\nparing rendering quality, all methods use ground truth op-\ntical flows, except that NVS [52] \u2192 2D Anim. [19] and \nNVS [52] \u2192 2D Anim. [19] + MA have to estimate optical \nflows for each frame apart from the first frame. We adopt \nPSNR, SSIM, and LPIPS [73] as our evaluation metrics. \nQuantitative comparisons. As shown in \nAcknowledgements. This study is supported under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). This work is also supported by Adobe Gift and the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (MOE-T2EP20220-0007) and Tier 1 (RG14/22).\nAutomatic cinemagraph portraits. Jiamin Bai, Aseem Agarwala, Maneesh Agrawala, Ravi Ramamoorthi, Computer Graphics Forum. Wiley Online Library32Jiamin Bai, Aseem Agarwala, Maneesh Agrawala, and Ravi Ramamoorthi. Automatic cinemagraph portraits. In Com- puter Graphics Forum, volume 32, pages 17-25. Wiley On- line Library, 2013. 1\n\n4d visualization of dynamic events from unconstrained multi-view videos. Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, Srinivasa Narasimhan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, and Srinivasa Narasimhan. 4d visualization of dynamic events from unconstrained multi-view videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5366-5375, 2020. 3\n\nDepth-aware video frame interpolation. Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, Ming-Hsuan Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)24Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3703-3712, 2019. 2, 4\n\nX-Fields: Implicit neural view-, lightand time-image interpolation. Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel, ACM Transactions on Graphics (TOG). 396Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, and Tobias Ritschel. X-Fields: Implicit neural view-, light- and time-image interpolation. ACM Transactions on Graph- ics (TOG), 39(6):1-15, 2020. 3\n\nUnstructured lumigraph rendering. Chris Buehler, Michael Bosse, Leonard Mcmillan, Steven Gortler, Michael Cohen, Proceedings of the 28th annual conference on Computer graphics and interactive techniques. the 28th annual conference on Computer graphics and interactive techniquesChris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured lumigraph ren- dering. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 425- 432, 2001. 2\n\nPlenoptic sampling. Jin-Xiang Chai, Xin Tong, Shing-Chow Chan, Heung-Yeung Shum, Proceedings of the 27th annual conference on Computer graphics and interactive techniques. the 27th annual conference on Computer graphics and interactive techniquesJin-Xiang Chai, Xin Tong, Shing-Chow Chan, and Heung- Yeung Shum. Plenoptic sampling. In Proceedings of the 27th annual conference on Computer graphics and interac- tive techniques, pages 307-318, 2000. 2\n\nEverybody dance now. Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A Efros, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5933-5942, 2019. 2\n\nAnimating pictures with stochastic motion textures. Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian Curless, H David, Richard Salesin, Szeliski, ACM Transactions on Graphics (TOG). 243Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian Curless, David H Salesin, and Richard Szeliski. Animating pictures with stochastic motion textures. ACM Transactions on Graphics (TOG), 24(3):853-860, 2005. 2\n\nModeling and rendering architecture from photographs: A hybrid geometry-and image-based approach. Paul E Debevec, J Camillo, Jitendra Taylor, Malik, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. the 23rd annual conference on Computer graphics and interactive techniquesPaul E Debevec, Camillo J Taylor, and Jitendra Malik. Mod- eling and rendering architecture from photographs: A hybrid geometry-and image-based approach. In Proceedings of the 23rd annual conference on Computer graphics and interac- tive techniques, pages 11-20, 1996. 2\n\nFlowNet: Learning optical flow with convolutional networks. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der, Daniel Smagt, Thomas Cremers, Brox, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. FlowNet: Learning optical flow with convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2758-2766, 2015. 3\n\nHeadGAN: One-shot neural head synthesis and editing. Michail Christos Doukas, Stefanos Zafeiriou, Viktoriia Sharmanska, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Michail Christos Doukas, Stefanos Zafeiriou, and Viktoriia Sharmanska. HeadGAN: One-shot neural head synthesis and editing. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision (ICCV), pages 14398-14407, 2021. 2\n\nAnimating Landscape: Self-supervised learning of decoupled motion and appearance for single-image video synthesis. Yuki Endo, Yoshihiro Kanamori, Shigeru Kuriyama, 175:1-175:19Proceedings of ACM SIGGRAPH Asia. ACM SIGGRAPH Asia38Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama. Animating Landscape: Self-supervised learning of decou- pled motion and appearance for single-image video synthe- sis. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH Asia 2019), 38(6):175:1-175:19, 2019. 1, 2, 3, 4\n\nSimulating fluids in real-world still images. Jingtan Siming Fan, Chen Piao, Kwan-Yee Qian, Hongsheng Lin, Li, arXiv:2204.113352022arXiv preprintSiming Fan, Jingtan Piao, Chen Qian, Kwan-Yee Lin, and Hongsheng Li. Simulating fluids in real-world still images. arXiv preprint arXiv:2204.11335, 2022. 2\n\nThe lumigraph. J Steven, Radek Gortler, Richard Grzeszczuk, Michael F Szeliski, Cohen, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. the 23rd annual conference on Computer graphics and interactive techniquesSteven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. The lumigraph. In Proceedings of the 23rd annual conference on Computer graphics and interac- tive techniques, pages 43-54, 1996. 2\n\nSingleview view synthesis in the wild with learned adaptive multiplane images. Yuxuan Han, Ruicheng Wang, Jiaolong Yang, ACM SIGGRAPH 2022 Conference Proceedings. Yuxuan Han, Ruicheng Wang, and Jiaolong Yang. Single- view view synthesis in the wild with learned adaptive mul- tiplane images. In ACM SIGGRAPH 2022 Conference Pro- ceedings, pages 1-8, 2022. 2\n\nImagen Video: High definition video generation with diffusion models. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, P Diederik, Ben Kingma, Mohammad Poole, David J Norouzi, Fleet, arXiv:2210.02303arXiv preprintJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen Video: High definition video generation with diffusion mod- els. arXiv preprint arXiv:2210.02303, 2022. 2\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems (NeurIPS). 2020Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif- fusion probabilistic models. In Advances in Neural Informa- tion Processing Systems (NeurIPS), 2020. 2\n\n. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet, arXiv:2204.034582022Video diffusion models. arXiv preprintJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video dif- fusion models. arXiv preprint arXiv:2204.03458, 2022. 2\n\nAnimating pictures with eulerian motion fields. Aleksander Holynski, Brian L Curless, Steven M Seitz, Richard Szeliski, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)7Aleksander Holynski, Brian L. Curless, Steven M. Seitz, and Richard Szeliski. Animating pictures with eulerian motion fields. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 5810- 5819, 2021. 1, 2, 3, 4, 5, 6, 7, 8\n\nFlowNet 2.0: Evolution of optical flow estimation with deep networks. Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. FlowNet 2.0: Evo- lution of optical flow estimation with deep networks. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2462-2470, 2017. 3\n\nSLIDE: Single image 3d photography with soft layering and depth-aware inpainting. Varun Jampani, Huiwen Chang, Kyle Sargent, Abhishek Kar, Richard Tucker, Michael Krainin, Dominik Kaeser, T William, David Freeman, Brian Salesin, Ce Curless, Liu, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021Varun Jampani, Huiwen Chang, Kyle Sargent, Abhishek Kar, Richard Tucker, Michael Krainin, Dominik Kaeser, William T Freeman, David Salesin, Brian Curless, and Ce Liu. SLIDE: Single image 3d photography with soft lay- ering and depth-aware inpainting. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2\n\nAnimating still landscape photographs through cloud motion creation. Wei-Cih Jhou, Wen-Huang Cheng, IEEE Transactions on Multimedia. 181Wei-Cih Jhou and Wen-Huang Cheng. Animating still land- scape photographs through cloud motion creation. IEEE Transactions on Multimedia, 18(1):4-13, 2015. 2\n\nPerceptual losses for real-time style transfer and super-resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)SpringerJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 694-711. Springer, 2016. 5\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5\n\nLight field rendering. Marc Levoy, Pat Hanrahan, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. the 23rd annual conference on Computer graphics and interactive techniquesMarc Levoy and Pat Hanrahan. Light field rendering. In Pro- ceedings of the 23rd annual conference on Computer graph- ics and interactive techniques, pages 31-42, 1996. 2\n\nMINE: Towards continuous depth mpi with nerf for novel view synthesis. Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, and Gim Hee Lee. MINE: Towards continuous depth mpi with nerf for novel view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12578-12588, 2021. 2\n\nNeural 3d video synthesis from multi-view video. Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5521-5531, 2022. 3\n\nSymmnerf: Learning to explore symmetry prior for single-view view synthesis. Xingyi Li, Chaoyi Hong, Yiran Wang, Zhiguo Cao, Ke Xian, Guosheng Lin, Proceedings of the Asian Conference on Computer Vision (ACCV). the Asian Conference on Computer Vision (ACCV)2022Xingyi Li, Chaoyi Hong, Yiran Wang, Zhiguo Cao, Ke Xian, and Guosheng Lin. Symmnerf: Learning to explore sym- metry prior for single-view view synthesis. In Proceedings of the Asian Conference on Computer Vision (ACCV), pages 1726-1742, 2022. 2\n\nFlow-grounded spatial-temporal video prediction from still images. Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)24Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Flow-grounded spatial-temporal video prediction from still images. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV), pages 600- 615, 2018. 2, 4\n\nNeural scene flow fields for space-time view synthesis of dynamic scenes. Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dy- namic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6498-6508, 2021. 3\n\nInfinityGAN: Towards infinite-pixel image synthesis. Chieh Hubert Lin, Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Ming-Hsuan Yang, International Conference on Learning Representations (ICLR). 2022Chieh Hubert Lin, Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, and Ming-Hsuan Yang. InfinityGAN: Towards infinite-pixel image synthesis. In International Conference on Learning Representations (ICLR), 2022. 2\n\nInfinite nature: Perpetual view generation of natural scenes from a single image. Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, Angjoo Kanazawa, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision (ICCV), pages 14458-14467, 2021. 2\n\nLiquid Warping GAN: A unified framework for human motion imitation, appearance transfer and novel view synthesis. Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, Shenghua Gao, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, and Shenghua Gao. Liquid Warping GAN: A unified framework for human motion imitation, appearance transfer and novel view synthesis. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision (ICCV), pages 5904- 5913, 2019. 2\n\nDeepLandscape: Adversarial modeling of landscape videos. Elizaveta Logacheva, Roman Suvorov, Oleg Khomenko, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)SpringerAnton Mashikhin, and Victor LempitskyElizaveta Logacheva, Roman Suvorov, Oleg Khomenko, An- ton Mashikhin, and Victor Lempitsky. DeepLandscape: Ad- versarial modeling of landscape videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages 256-272. Springer, 2020. 2\n\nControllable animation of fluid elements in still images. Aniruddha Mahapatra, Kuldeep Kulkarni, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)1Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable animation of fluid elements in still images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3667-3676, 2022. 1, 2\n\nData mining and knowledge discovery handbook. Oded Maimon, Lior Rokach, Oded Maimon and Lior Rokach. Data mining and knowledge discovery handbook. 2005. 4\n\nNeRF: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view syn- thesis. In Proceedings of the European Conference on Com- puter Vision (ECCV), 2020. 2\n\nSoftmax splatting for video frame interpolation. Simon Niklaus, Feng Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)24Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5437-5446, 2020. 2, 4\n\n3d ken burns effect from a single image. Simon Niklaus, Long Mai, Jimei Yang, Feng Liu, ACM Transactions on Graphics (ToG). 386Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d ken burns effect from a single image. ACM Transactions on Graphics (ToG), 38(6):1-15, 2019. 2\n\nDeepSDF: Learning continuous signed distance functions for shape representation. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representa- tion. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 165- 174, 2019. 2\n\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, M Steven, Ricardo Seitz, Martin-Brualla, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Nerfies: Deformable neural radiance fieldsKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5865-5874, 2021. 3\n\nSemantic image synthesis with spatially-adaptive normalization. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive nor- malization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2337-2346, 2019. 5\n\nMpib: An mpi-based bokeh rendering framework for realistic partial occlusion effects. Juewen Peng, Jianming Zhang, Xianrui Luo, Hao Lu, Ke Xian, Zhiguo Cao, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)SpringerJuewen Peng, Jianming Zhang, Xianrui Luo, Hao Lu, Ke Xian, and Zhiguo Cao. Mpib: An mpi-based bokeh render- ing framework for realistic partial occlusion effects. In Pro- ceedings of the European Conference on Computer Vision (ECCV), pages 590-607. Springer, 2022. 2\n\nNeural radiance fields for dynamic scenes. Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer, . D-Nerf , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10318-10327, 2021. 3\n\nVision transformers for dense prediction. Ren\u00e9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)6Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 12179-12188, 2021. 2, 4, 6\n\nDeep image spatial transformation for person image generation. Yurui Ren, Xiaoming Yu, Junming Chen, H Thomas, Ge Li, Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Yurui Ren, Xiaoming Yu, Junming Chen, Thomas H Li, and Ge Li. Deep image spatial transformation for person im- age generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7690-7699, 2020. 2\n\nHigh-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684-10695, 2022. 1\n\nU-Net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI). SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U- Net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical Image Com- puting and Computer Assisted Intervention (MICCAI), pages 234-241. Springer, 2015. 5\n\nImproved techniques for training gans. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, Advances in Neural Information Processing Systems (NeurIPS). Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Pro- cessing Systems (NeurIPS), 2016. 5\n\nLayered depth images. Jonathan Shade, Steven Gortler, Li-Wei He, Richard Szeliski, Proceedings of the 25th annual conference on Computer graphics and interactive techniques. the 25th annual conference on Computer graphics and interactive techniques24Jonathan Shade, Steven Gortler, Li-wei He, and Richard Szeliski. Layered depth images. In Proceedings of the 25th annual conference on Computer graphics and interac- tive techniques, pages 231-242, 1998. 2, 3, 4\n\nSin-GAN: Learning a generative model from a single natural image. Tamar Rott Shaham, Tali Dekel, Tomer Michaeli, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Sin- GAN: Learning a generative model from a single natural im- age. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision (ICCV), pages 4570-4580, 2019. 2\n\n3d photography using context-aware layered depth inpainting. Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)67Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 3d photography using context-aware layered depth inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2, 4, 5, 6, 7\n\nAnimating arbitrary objects via deep motion transfer. Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, Nicu Sebe, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Animating arbitrary objects via deep motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2377-2386, 2019. 2\n\nFirst order motion model for image animation. Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, Nicu Sebe, Advances in Neural Information Processing Systems (NeurIPS). Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for im- age animation. In Advances in Neural Information Process- ing Systems (NeurIPS), 2019. 2\n\nDeformable gans for pose-based human image generation. Aliaksandr Siarohin, Enver Sangineto, St\u00e9phane Lathuiliere, Nicu Sebe, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Aliaksandr Siarohin, Enver Sangineto, St\u00e9phane Lathuiliere, and Nicu Sebe. Deformable gans for pose-based human im- age generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3408-3416, 2018. 2\n\nMotion representations for articulated animation. Aliaksandr Siarohin, J Oliver, Jian Woodford, Menglei Ren, Sergey Chai, Tulyakov, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for artic- ulated animation. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 13653-13662, 2021. 2\n\nMake-A-Video: Text-to-video generation without text-video data. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, arXiv:2209.14792arXiv preprintUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-A-Video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2\n\nScene Representation Networks: Continuous 3d-structure-aware neural scene representations. Vincent Sitzmann, Michael Zollhoefer, Gordon Wetzstein, Advances in Neural Information Processing Systems (NeurIPS). Vincent Sitzmann, Michael Zollhoefer, and Gordon Wet- zstein. Scene Representation Networks: Continuous 3d- structure-aware neural scene representations. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 2\n\nDeep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, PMLRInternational Conference on Machine Learning (ICML). Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Con- ference on Machine Learning (ICML), pages 2256-2265. PMLR, 2015. 2\n\nPWC-Net: Cnns for optical flow using pyramid, warping, and cost volume. Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition35Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. PWC-Net: Cnns for optical flow using pyramid, warping, and cost volume. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8934-8943, 2018. 3, 5\n\nRAFT: Recurrent all-pairs field transforms for optical flow. Zachary Teed, Jia Deng, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In Proceedings of the European Conference on Computer Vision (ECCV), pages 402-419, 2020. 3\n\nSingle-view view synthesis with multiplane images. Richard Tucker, Noah Snavely, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Richard Tucker and Noah Snavely. Single-view view synthe- sis with multiplane images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 551-560, 2020. 2\n\nLayer-structured 3d scene inference via view synthesis. Shubham Tulsiani, Richard Tucker, Noah Snavely, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Shubham Tulsiani, Richard Tucker, and Noah Snavely. Layer-structured 3d scene inference via view synthesis. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 302-317, 2018. 2\n\n3d moments from nearduplicate photos. Qianqian Wang, Zhengqi Li, David Salesin, Noah Snavely, Brian Curless, Janne Kontkanen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 345Qianqian Wang, Zhengqi Li, David Salesin, Noah Snavely, Brian Curless, and Janne Kontkanen. 3d moments from near- duplicate photos. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), 2022. 3, 4, 5\n\nLess is more: Consistent video depth estimation with masked frames modeling. Yiran Wang, Zhiyu Pan, Xingyi Li, Zhiguo Cao, Ke Xian, Jianming Zhang, Proceedings of the 30th ACM International Conference on Multimedia (ACM MM). the 30th ACM International Conference on Multimedia (ACM MM)2022Yiran Wang, Zhiyu Pan, Xingyi Li, Zhiguo Cao, Ke Xian, and Jianming Zhang. Less is more: Consistent video depth estimation with masked frames modeling. In Proceedings of the 30th ACM International Conference on Multimedia (ACM MM), pages 6347-6358, 2022. 2\n\nSynSin: End-to-end view synthesis from a single image. Olivia Wiles, Georgia Gkioxari, Richard Szeliski, Justin Johnson, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)25Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. SynSin: End-to-end view synthesis from a sin- gle image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7467-7477, 2020. 2, 5\n\nMonocular relative depth perception with web stereo data supervision. Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, Zhenbo Luo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. Monocular relative depth per- ception with web stereo data supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 311-320, 2018. 2\n\nStructure-guided ranking loss for single image depth prediction. Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, Zhiguo Cao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, and Zhiguo Cao. Structure-guided ranking loss for single image depth prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 611-620, 2020. 2\n\nLearning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, Jiebo Luo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dy- namic generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2364-2373, 2018. 2\n\nSinNeRF: Training neural radiance fields on complex scenes from a single image. Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, Zhangyang Wang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)SpringerDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. SinNeRF: Training neural radi- ance fields on complex scenes from a single image. In Pro- ceedings of the European Conference on Computer Vision (ECCV), pages 736-753. Springer, 2022. 2\n\nTurning an urban scene video into a cinemagraph. Hang Yan, Yebin Liu, Yasutaka Furukawa, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Hang Yan, Yebin Liu, and Yasutaka Furukawa. Turning an urban scene video into a cinemagraph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 394-402, 2017. 1\n\npixelNeRF: Neural radiance fields from one or few images. Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4578-4587, 2021. 2\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)56Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht- man, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 586-595, 2018. 5, 6\n", "annotations": {"author": "[{\"end\":227,\"start\":39},{\"end\":417,\"start\":228},{\"end\":609,\"start\":418},{\"end\":683,\"start\":610},{\"end\":692,\"start\":684},{\"end\":723,\"start\":693}]", "publisher": null, "author_last_name": "[{\"end\":48,\"start\":46},{\"end\":238,\"start\":235},{\"end\":430,\"start\":427},{\"end\":624,\"start\":619},{\"end\":691,\"start\":687},{\"end\":705,\"start\":702}]", "author_first_name": "[{\"end\":45,\"start\":39},{\"end\":234,\"start\":228},{\"end\":426,\"start\":418},{\"end\":618,\"start\":610},{\"end\":686,\"start\":684},{\"end\":701,\"start\":693}]", "author_affiliation": "[{\"end\":226,\"start\":50},{\"end\":416,\"start\":240},{\"end\":608,\"start\":432},{\"end\":682,\"start\":626}]", "title": "[{\"end\":36,\"start\":1},{\"end\":759,\"start\":724}]", "venue": null, "abstract": "[{\"end\":2520,\"start\":784}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2904,\"start\":2900},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4757,\"start\":4754},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":4760,\"start\":4757},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5553,\"start\":5549},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5556,\"start\":5553},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5559,\"start\":5556},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5763,\"start\":5760},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5765,\"start\":5763},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5767,\"start\":5765},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5770,\"start\":5767},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5773,\"start\":5770},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5821,\"start\":5817},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5824,\"start\":5821},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":5827,\"start\":5824},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5953,\"start\":5949},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5956,\"start\":5953},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5959,\"start\":5956},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":5962,\"start\":5959},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6772,\"start\":6768},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6963,\"start\":6959},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7126,\"start\":7123},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7129,\"start\":7126},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7132,\"start\":7129},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8313,\"start\":8310},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8316,\"start\":8313},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8622,\"start\":8619},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8625,\"start\":8622},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8628,\"start\":8625},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8631,\"start\":8628},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8634,\"start\":8631},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8668,\"start\":8664},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8671,\"start\":8668},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8674,\"start\":8671},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8941,\"start\":8937},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8944,\"start\":8941},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8947,\"start\":8944},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8950,\"start\":8947},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8953,\"start\":8950},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9268,\"start\":9264},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9271,\"start\":9268},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9335,\"start\":9331},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9338,\"start\":9335},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9341,\"start\":9338},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9601,\"start\":9597},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9604,\"start\":9601},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9607,\"start\":9604},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9610,\"start\":9607},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9613,\"start\":9610},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9650,\"start\":9646},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10180,\"start\":10176},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10183,\"start\":10180},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10186,\"start\":10183},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10499,\"start\":10495},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10502,\"start\":10499},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10505,\"start\":10502},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10508,\"start\":10505},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":10511,\"start\":10508},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":10514,\"start\":10511},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":10517,\"start\":10514},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10766,\"start\":10762},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10769,\"start\":10766},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":10772,\"start\":10769},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10805,\"start\":10801},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":10808,\"start\":10805},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10811,\"start\":10808},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":10814,\"start\":10811},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10882,\"start\":10878},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10977,\"start\":10973},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10980,\"start\":10977},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11096,\"start\":11092},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12653,\"start\":12649},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12682,\"start\":12679},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12684,\"start\":12682},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12687,\"start\":12684},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12846,\"start\":12842},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12849,\"start\":12846},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12852,\"start\":12849},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":13077,\"start\":13073},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14146,\"start\":14142},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14902,\"start\":14898},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14905,\"start\":14902},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":14908,\"start\":14905},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":14911,\"start\":14908},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15050,\"start\":15046},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15244,\"start\":15240},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16452,\"start\":16448},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16455,\"start\":16452},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16458,\"start\":16455},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16829,\"start\":16825},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":16927,\"start\":16923},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16991,\"start\":16987},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17220,\"start\":17216},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17415,\"start\":17411},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19257,\"start\":19254},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19260,\"start\":19257},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19263,\"start\":19260},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":20481,\"start\":20477},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":20812,\"start\":20808},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22180,\"start\":22176},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22459,\"start\":22455},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22782,\"start\":22778},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23087,\"start\":23083},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23878,\"start\":23874},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23920,\"start\":23916},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":23923,\"start\":23920},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24141,\"start\":24137},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24238,\"start\":24234},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":24359,\"start\":24355},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24418,\"start\":24414},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24487,\"start\":24483},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25652,\"start\":25648},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25870,\"start\":25866},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25931,\"start\":25927},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25978,\"start\":25974},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26146,\"start\":26142},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26191,\"start\":26187},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26834,\"start\":26830},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27372,\"start\":27368},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27484,\"start\":27480},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27969,\"start\":27965},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27980,\"start\":27976},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28061,\"start\":28057},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28072,\"start\":28068},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28219,\"start\":28215},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28235,\"start\":28231},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28248,\"start\":28244},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28264,\"start\":28260},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29060,\"start\":29056},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29189,\"start\":29185},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29217,\"start\":29213},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29248,\"start\":29244},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29268,\"start\":29264},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29299,\"start\":29295},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29319,\"start\":29315},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30189,\"start\":30185},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":30745,\"start\":30741},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30771,\"start\":30767},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30849,\"start\":30845},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31519,\"start\":31515},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31891,\"start\":31887}]", "figure": "[{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35116,\"start\":33927}]", "paragraph": "[{\"end\":3028,\"start\":2531},{\"end\":4221,\"start\":3041},{\"end\":5244,\"start\":4238},{\"end\":6322,\"start\":5246},{\"end\":7678,\"start\":6324},{\"end\":7719,\"start\":7680},{\"end\":7929,\"start\":7721},{\"end\":8029,\"start\":7931},{\"end\":8187,\"start\":8031},{\"end\":9928,\"start\":8204},{\"end\":11898,\"start\":9930},{\"end\":12442,\"start\":11900},{\"end\":12777,\"start\":12444},{\"end\":13582,\"start\":12779},{\"end\":14541,\"start\":13604},{\"end\":15509,\"start\":14563},{\"end\":15752,\"start\":15534},{\"end\":15842,\"start\":15780},{\"end\":15976,\"start\":15844},{\"end\":16353,\"start\":16038},{\"end\":16706,\"start\":16381},{\"end\":17906,\"start\":16708},{\"end\":18867,\"start\":17946},{\"end\":20077,\"start\":18894},{\"end\":20511,\"start\":20141},{\"end\":21405,\"start\":20568},{\"end\":21567,\"start\":21490},{\"end\":21649,\"start\":21608},{\"end\":21915,\"start\":21690},{\"end\":23119,\"start\":21928},{\"end\":24013,\"start\":23162},{\"end\":24435,\"start\":24105},{\"end\":24949,\"start\":24437},{\"end\":27314,\"start\":24963},{\"end\":29473,\"start\":27326},{\"end\":30590,\"start\":29533},{\"end\":31382,\"start\":30605},{\"end\":32032,\"start\":31401},{\"end\":32372,\"start\":32034},{\"end\":33926,\"start\":32387}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15533,\"start\":15510},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15779,\"start\":15753},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16037,\"start\":15977},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18893,\"start\":18868},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20140,\"start\":20078},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20567,\"start\":20512},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21489,\"start\":21406},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21607,\"start\":21568},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21689,\"start\":21650},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23161,\"start\":23120},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24065,\"start\":24014},{\"attributes\":{\"id\":\"formula_11\"},\"end\":29501,\"start\":29474}]", "table_ref": "[{\"end\":25556,\"start\":25549},{\"end\":27492,\"start\":27485},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31252,\"start\":31245},{\"end\":31551,\"start\":31544}]", "section_header": "[{\"end\":3039,\"start\":3031},{\"attributes\":{\"n\":\"1.\"},\"end\":4236,\"start\":4224},{\"attributes\":{\"n\":\"2.\"},\"end\":8202,\"start\":8190},{\"attributes\":{\"n\":\"3.\"},\"end\":13591,\"start\":13585},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13602,\"start\":13594},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14561,\"start\":14544},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16379,\"start\":16356},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17944,\"start\":17909},{\"attributes\":{\"n\":\"3.5.\"},\"end\":21926,\"start\":21918},{\"attributes\":{\"n\":\"4.\"},\"end\":24078,\"start\":24067},{\"attributes\":{\"n\":\"4.1.\"},\"end\":24103,\"start\":24081},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24961,\"start\":24952},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27324,\"start\":27317},{\"end\":29508,\"start\":29503},{\"end\":29531,\"start\":29511},{\"attributes\":{\"n\":\"4.4.\"},\"end\":30603,\"start\":30593},{\"attributes\":{\"n\":\"4.5.\"},\"end\":31399,\"start\":31385},{\"attributes\":{\"n\":\"5.\"},\"end\":32385,\"start\":32375},{\"end\":33937,\"start\":33928}]", "table": "[{\"end\":35116,\"start\":34049}]", "figure_caption": "[{\"end\":34049,\"start\":33939}]", "figure_ref": "[{\"end\":2530,\"start\":2522},{\"end\":5157,\"start\":5151},{\"end\":11359,\"start\":11351},{\"end\":13824,\"start\":13818},{\"end\":18082,\"start\":18074},{\"end\":19916,\"start\":19910},{\"end\":27817,\"start\":27811},{\"end\":28965,\"start\":28957},{\"end\":29555,\"start\":29547},{\"end\":29836,\"start\":29830},{\"end\":30440,\"start\":30434},{\"end\":31829,\"start\":31821}]", "bib_author_first_name": "[{\"end\":35541,\"start\":35535},{\"end\":35552,\"start\":35547},{\"end\":35570,\"start\":35563},{\"end\":35585,\"start\":35581},{\"end\":35913,\"start\":35907},{\"end\":35926,\"start\":35922},{\"end\":35936,\"start\":35931},{\"end\":35949,\"start\":35945},{\"end\":35968,\"start\":35959},{\"end\":36458,\"start\":36453},{\"end\":36473,\"start\":36464},{\"end\":36483,\"start\":36479},{\"end\":36495,\"start\":36488},{\"end\":36510,\"start\":36503},{\"end\":36526,\"start\":36516},{\"end\":37018,\"start\":37011},{\"end\":37032,\"start\":37027},{\"end\":37055,\"start\":37045},{\"end\":37070,\"start\":37064},{\"end\":37362,\"start\":37357},{\"end\":37379,\"start\":37372},{\"end\":37394,\"start\":37387},{\"end\":37411,\"start\":37405},{\"end\":37428,\"start\":37421},{\"end\":37868,\"start\":37859},{\"end\":37878,\"start\":37875},{\"end\":37895,\"start\":37885},{\"end\":37913,\"start\":37902},{\"end\":38320,\"start\":38312},{\"end\":38332,\"start\":38327},{\"end\":38349,\"start\":38342},{\"end\":38362,\"start\":38356},{\"end\":38364,\"start\":38363},{\"end\":38768,\"start\":38761},{\"end\":38780,\"start\":38777},{\"end\":38782,\"start\":38781},{\"end\":38794,\"start\":38792},{\"end\":38800,\"start\":38795},{\"end\":38813,\"start\":38808},{\"end\":38824,\"start\":38823},{\"end\":38839,\"start\":38832},{\"end\":39226,\"start\":39225},{\"end\":39244,\"start\":39236},{\"end\":39763,\"start\":39757},{\"end\":39784,\"start\":39777},{\"end\":39798,\"start\":39794},{\"end\":39810,\"start\":39804},{\"end\":39825,\"start\":39820},{\"end\":39844,\"start\":39836},{\"end\":39860,\"start\":39853},{\"end\":39876,\"start\":39870},{\"end\":39890,\"start\":39884},{\"end\":40430,\"start\":40423},{\"end\":40439,\"start\":40431},{\"end\":40456,\"start\":40448},{\"end\":40477,\"start\":40468},{\"end\":40989,\"start\":40985},{\"end\":41005,\"start\":40996},{\"end\":41023,\"start\":41016},{\"end\":41432,\"start\":41425},{\"end\":41449,\"start\":41445},{\"end\":41464,\"start\":41456},{\"end\":41480,\"start\":41471},{\"end\":41697,\"start\":41696},{\"end\":41711,\"start\":41706},{\"end\":41728,\"start\":41721},{\"end\":41750,\"start\":41741},{\"end\":42226,\"start\":42220},{\"end\":42240,\"start\":42232},{\"end\":42255,\"start\":42247},{\"end\":42578,\"start\":42570},{\"end\":42590,\"start\":42583},{\"end\":42604,\"start\":42597},{\"end\":42617,\"start\":42614},{\"end\":42630,\"start\":42625},{\"end\":42642,\"start\":42636},{\"end\":42655,\"start\":42654},{\"end\":42669,\"start\":42666},{\"end\":42686,\"start\":42678},{\"end\":42699,\"start\":42694},{\"end\":42701,\"start\":42700},{\"end\":43066,\"start\":43058},{\"end\":43075,\"start\":43071},{\"end\":43088,\"start\":43082},{\"end\":43334,\"start\":43326},{\"end\":43342,\"start\":43339},{\"end\":43359,\"start\":43353},{\"end\":43378,\"start\":43371},{\"end\":43393,\"start\":43385},{\"end\":43408,\"start\":43403},{\"end\":43410,\"start\":43409},{\"end\":43698,\"start\":43688},{\"end\":43714,\"start\":43709},{\"end\":43716,\"start\":43715},{\"end\":43732,\"start\":43726},{\"end\":43734,\"start\":43733},{\"end\":43749,\"start\":43742},{\"end\":44267,\"start\":44263},{\"end\":44281,\"start\":44273},{\"end\":44295,\"start\":44289},{\"end\":44311,\"start\":44304},{\"end\":44326,\"start\":44320},{\"end\":44346,\"start\":44340},{\"end\":44890,\"start\":44885},{\"end\":44906,\"start\":44900},{\"end\":44918,\"start\":44914},{\"end\":44936,\"start\":44928},{\"end\":44949,\"start\":44942},{\"end\":44965,\"start\":44958},{\"end\":44982,\"start\":44975},{\"end\":44992,\"start\":44991},{\"end\":45007,\"start\":45002},{\"end\":45022,\"start\":45017},{\"end\":45034,\"start\":45032},{\"end\":45615,\"start\":45608},{\"end\":45631,\"start\":45622},{\"end\":45909,\"start\":45903},{\"end\":45928,\"start\":45919},{\"end\":45938,\"start\":45936},{\"end\":46339,\"start\":46338},{\"end\":46355,\"start\":46350},{\"end\":46541,\"start\":46537},{\"end\":46552,\"start\":46549},{\"end\":46977,\"start\":46971},{\"end\":46988,\"start\":46982},{\"end\":46997,\"start\":46995},{\"end\":47010,\"start\":47003},{\"end\":47024,\"start\":47017},{\"end\":47038,\"start\":47031},{\"end\":47531,\"start\":47525},{\"end\":47540,\"start\":47536},{\"end\":47559,\"start\":47552},{\"end\":47577,\"start\":47572},{\"end\":47594,\"start\":47585},{\"end\":47611,\"start\":47604},{\"end\":47623,\"start\":47617},{\"end\":47639,\"start\":47633},{\"end\":47658,\"start\":47651},{\"end\":47675,\"start\":47668},{\"end\":48272,\"start\":48266},{\"end\":48283,\"start\":48277},{\"end\":48295,\"start\":48290},{\"end\":48308,\"start\":48302},{\"end\":48316,\"start\":48314},{\"end\":48331,\"start\":48323},{\"end\":48768,\"start\":48763},{\"end\":48777,\"start\":48773},{\"end\":48789,\"start\":48784},{\"end\":48803,\"start\":48796},{\"end\":48813,\"start\":48810},{\"end\":48828,\"start\":48818},{\"end\":49275,\"start\":49268},{\"end\":49285,\"start\":49280},{\"end\":49299,\"start\":49295},{\"end\":49315,\"start\":49309},{\"end\":49796,\"start\":49791},{\"end\":49816,\"start\":49809},{\"end\":49833,\"start\":49824},{\"end\":49845,\"start\":49839},{\"end\":49866,\"start\":49856},{\"end\":50238,\"start\":50232},{\"end\":50251,\"start\":50244},{\"end\":50265,\"start\":50260},{\"end\":50281,\"start\":50275},{\"end\":50295,\"start\":50291},{\"end\":50311,\"start\":50305},{\"end\":50871,\"start\":50868},{\"end\":50883,\"start\":50877},{\"end\":50893,\"start\":50890},{\"end\":50905,\"start\":50899},{\"end\":50914,\"start\":50911},{\"end\":50927,\"start\":50919},{\"end\":51437,\"start\":51428},{\"end\":51454,\"start\":51449},{\"end\":51468,\"start\":51464},{\"end\":51960,\"start\":51951},{\"end\":51979,\"start\":51972},{\"end\":52426,\"start\":52422},{\"end\":52439,\"start\":52435},{\"end\":52607,\"start\":52604},{\"end\":52621,\"start\":52620},{\"end\":52637,\"start\":52630},{\"end\":52658,\"start\":52650},{\"end\":52660,\"start\":52659},{\"end\":52673,\"start\":52669},{\"end\":52685,\"start\":52682},{\"end\":53134,\"start\":53129},{\"end\":53148,\"start\":53144},{\"end\":53564,\"start\":53559},{\"end\":53578,\"start\":53574},{\"end\":53589,\"start\":53584},{\"end\":53600,\"start\":53596},{\"end\":53885,\"start\":53875},{\"end\":53897,\"start\":53892},{\"end\":53914,\"start\":53908},{\"end\":53930,\"start\":53923},{\"end\":53947,\"start\":53941},{\"end\":54421,\"start\":54413},{\"end\":54435,\"start\":54428},{\"end\":54451,\"start\":54443},{\"end\":54453,\"start\":54452},{\"end\":54468,\"start\":54462},{\"end\":54481,\"start\":54478},{\"end\":54483,\"start\":54482},{\"end\":54494,\"start\":54493},{\"end\":54510,\"start\":54503},{\"end\":55067,\"start\":55060},{\"end\":55081,\"start\":55074},{\"end\":55096,\"start\":55087},{\"end\":55110,\"start\":55103},{\"end\":55616,\"start\":55610},{\"end\":55631,\"start\":55623},{\"end\":55646,\"start\":55639},{\"end\":55655,\"start\":55652},{\"end\":55662,\"start\":55660},{\"end\":55675,\"start\":55669},{\"end\":56121,\"start\":56115},{\"end\":56137,\"start\":56132},{\"end\":56152,\"start\":56146},{\"end\":56172,\"start\":56164},{\"end\":56196,\"start\":56188},{\"end\":56657,\"start\":56653},{\"end\":56672,\"start\":56666},{\"end\":56693,\"start\":56686},{\"end\":57128,\"start\":57123},{\"end\":57142,\"start\":57134},{\"end\":57154,\"start\":57147},{\"end\":57162,\"start\":57161},{\"end\":57173,\"start\":57171},{\"end\":57658,\"start\":57653},{\"end\":57675,\"start\":57668},{\"end\":57694,\"start\":57687},{\"end\":57710,\"start\":57703},{\"end\":57723,\"start\":57718},{\"end\":58232,\"start\":58228},{\"end\":58253,\"start\":58246},{\"end\":58269,\"start\":58263},{\"end\":58679,\"start\":58676},{\"end\":58693,\"start\":58690},{\"end\":58714,\"start\":58706},{\"end\":58729,\"start\":58724},{\"end\":58742,\"start\":58738},{\"end\":58754,\"start\":58752},{\"end\":59055,\"start\":59047},{\"end\":59069,\"start\":59063},{\"end\":59085,\"start\":59079},{\"end\":59097,\"start\":59090},{\"end\":59559,\"start\":59554},{\"end\":59564,\"start\":59560},{\"end\":59577,\"start\":59573},{\"end\":59590,\"start\":59585},{\"end\":60043,\"start\":60036},{\"end\":60059,\"start\":60050},{\"end\":60072,\"start\":60064},{\"end\":60086,\"start\":60079},{\"end\":60560,\"start\":60550},{\"end\":60579,\"start\":60571},{\"end\":60599,\"start\":60593},{\"end\":60615,\"start\":60610},{\"end\":60627,\"start\":60623},{\"end\":61114,\"start\":61104},{\"end\":61133,\"start\":61125},{\"end\":61153,\"start\":61147},{\"end\":61169,\"start\":61164},{\"end\":61181,\"start\":61177},{\"end\":61525,\"start\":61515},{\"end\":61541,\"start\":61536},{\"end\":61561,\"start\":61553},{\"end\":61579,\"start\":61575},{\"end\":62060,\"start\":62050},{\"end\":62072,\"start\":62071},{\"end\":62085,\"start\":62081},{\"end\":62103,\"start\":62096},{\"end\":62115,\"start\":62109},{\"end\":62624,\"start\":62619},{\"end\":62637,\"start\":62633},{\"end\":62652,\"start\":62646},{\"end\":62662,\"start\":62660},{\"end\":62671,\"start\":62668},{\"end\":62684,\"start\":62676},{\"end\":62698,\"start\":62692},{\"end\":62708,\"start\":62703},{\"end\":62719,\"start\":62715},{\"end\":62732,\"start\":62728},{\"end\":63102,\"start\":63095},{\"end\":63120,\"start\":63113},{\"end\":63139,\"start\":63133},{\"end\":63509,\"start\":63503},{\"end\":63530,\"start\":63526},{\"end\":63542,\"start\":63538},{\"end\":63565,\"start\":63560},{\"end\":63940,\"start\":63934},{\"end\":63954,\"start\":63946},{\"end\":63968,\"start\":63961},{\"end\":63977,\"start\":63974},{\"end\":64434,\"start\":64427},{\"end\":64444,\"start\":64441},{\"end\":64809,\"start\":64802},{\"end\":64822,\"start\":64818},{\"end\":65261,\"start\":65254},{\"end\":65279,\"start\":65272},{\"end\":65292,\"start\":65288},{\"end\":65666,\"start\":65658},{\"end\":65680,\"start\":65673},{\"end\":65690,\"start\":65685},{\"end\":65704,\"start\":65700},{\"end\":65719,\"start\":65714},{\"end\":65734,\"start\":65729},{\"end\":66253,\"start\":66248},{\"end\":66265,\"start\":66260},{\"end\":66277,\"start\":66271},{\"end\":66288,\"start\":66282},{\"end\":66296,\"start\":66294},{\"end\":66311,\"start\":66303},{\"end\":66779,\"start\":66773},{\"end\":66794,\"start\":66787},{\"end\":66812,\"start\":66805},{\"end\":66829,\"start\":66823},{\"end\":67325,\"start\":67323},{\"end\":67339,\"start\":67332},{\"end\":67352,\"start\":67346},{\"end\":67361,\"start\":67358},{\"end\":67370,\"start\":67366},{\"end\":67382,\"start\":67377},{\"end\":67393,\"start\":67387},{\"end\":67898,\"start\":67896},{\"end\":67913,\"start\":67905},{\"end\":67927,\"start\":67921},{\"end\":67938,\"start\":67934},{\"end\":67947,\"start\":67944},{\"end\":67959,\"start\":67953},{\"end\":68484,\"start\":68481},{\"end\":68498,\"start\":68492},{\"end\":68507,\"start\":68504},{\"end\":68515,\"start\":68512},{\"end\":68526,\"start\":68521},{\"end\":69054,\"start\":69049},{\"end\":69064,\"start\":69059},{\"end\":69078,\"start\":69072},{\"end\":69091,\"start\":69085},{\"end\":69105,\"start\":69097},{\"end\":69120,\"start\":69111},{\"end\":69572,\"start\":69568},{\"end\":69583,\"start\":69578},{\"end\":69597,\"start\":69589},{\"end\":70043,\"start\":70039},{\"end\":70054,\"start\":70048},{\"end\":70066,\"start\":70059},{\"end\":70081,\"start\":70075},{\"end\":70568,\"start\":70561},{\"end\":70583,\"start\":70576},{\"end\":70597,\"start\":70591},{\"end\":70599,\"start\":70598},{\"end\":70610,\"start\":70607},{\"end\":70628,\"start\":70622}]", "bib_author_last_name": "[{\"end\":35545,\"start\":35542},{\"end\":35561,\"start\":35553},{\"end\":35579,\"start\":35571},{\"end\":35597,\"start\":35586},{\"end\":35920,\"start\":35914},{\"end\":35929,\"start\":35927},{\"end\":35943,\"start\":35937},{\"end\":35957,\"start\":35950},{\"end\":35979,\"start\":35969},{\"end\":36462,\"start\":36459},{\"end\":36477,\"start\":36474},{\"end\":36486,\"start\":36484},{\"end\":36501,\"start\":36496},{\"end\":36514,\"start\":36511},{\"end\":36531,\"start\":36527},{\"end\":37025,\"start\":37019},{\"end\":37043,\"start\":37033},{\"end\":37062,\"start\":37056},{\"end\":37079,\"start\":37071},{\"end\":37370,\"start\":37363},{\"end\":37385,\"start\":37380},{\"end\":37403,\"start\":37395},{\"end\":37419,\"start\":37412},{\"end\":37434,\"start\":37429},{\"end\":37873,\"start\":37869},{\"end\":37883,\"start\":37879},{\"end\":37900,\"start\":37896},{\"end\":37918,\"start\":37914},{\"end\":38325,\"start\":38321},{\"end\":38340,\"start\":38333},{\"end\":38354,\"start\":38350},{\"end\":38370,\"start\":38365},{\"end\":38775,\"start\":38769},{\"end\":38790,\"start\":38783},{\"end\":38806,\"start\":38801},{\"end\":38821,\"start\":38814},{\"end\":38830,\"start\":38825},{\"end\":38847,\"start\":38840},{\"end\":38857,\"start\":38849},{\"end\":39223,\"start\":39209},{\"end\":39234,\"start\":39227},{\"end\":39251,\"start\":39245},{\"end\":39258,\"start\":39253},{\"end\":39775,\"start\":39764},{\"end\":39792,\"start\":39785},{\"end\":39802,\"start\":39799},{\"end\":39818,\"start\":39811},{\"end\":39834,\"start\":39826},{\"end\":39851,\"start\":39845},{\"end\":39868,\"start\":39861},{\"end\":39882,\"start\":39877},{\"end\":39898,\"start\":39891},{\"end\":39904,\"start\":39900},{\"end\":40446,\"start\":40440},{\"end\":40466,\"start\":40457},{\"end\":40488,\"start\":40478},{\"end\":40994,\"start\":40990},{\"end\":41014,\"start\":41006},{\"end\":41032,\"start\":41024},{\"end\":41443,\"start\":41433},{\"end\":41454,\"start\":41450},{\"end\":41469,\"start\":41465},{\"end\":41484,\"start\":41481},{\"end\":41488,\"start\":41486},{\"end\":41704,\"start\":41698},{\"end\":41719,\"start\":41712},{\"end\":41739,\"start\":41729},{\"end\":41759,\"start\":41751},{\"end\":41766,\"start\":41761},{\"end\":42230,\"start\":42227},{\"end\":42245,\"start\":42241},{\"end\":42260,\"start\":42256},{\"end\":42581,\"start\":42579},{\"end\":42595,\"start\":42591},{\"end\":42612,\"start\":42605},{\"end\":42623,\"start\":42618},{\"end\":42634,\"start\":42631},{\"end\":42652,\"start\":42643},{\"end\":42664,\"start\":42656},{\"end\":42676,\"start\":42670},{\"end\":42692,\"start\":42687},{\"end\":42709,\"start\":42702},{\"end\":42716,\"start\":42711},{\"end\":43069,\"start\":43067},{\"end\":43080,\"start\":43076},{\"end\":43095,\"start\":43089},{\"end\":43337,\"start\":43335},{\"end\":43351,\"start\":43343},{\"end\":43369,\"start\":43360},{\"end\":43383,\"start\":43379},{\"end\":43401,\"start\":43394},{\"end\":43416,\"start\":43411},{\"end\":43707,\"start\":43699},{\"end\":43724,\"start\":43717},{\"end\":43740,\"start\":43735},{\"end\":43758,\"start\":43750},{\"end\":44271,\"start\":44268},{\"end\":44287,\"start\":44282},{\"end\":44302,\"start\":44296},{\"end\":44318,\"start\":44312},{\"end\":44338,\"start\":44327},{\"end\":44351,\"start\":44347},{\"end\":44898,\"start\":44891},{\"end\":44912,\"start\":44907},{\"end\":44926,\"start\":44919},{\"end\":44940,\"start\":44937},{\"end\":44956,\"start\":44950},{\"end\":44973,\"start\":44966},{\"end\":44989,\"start\":44983},{\"end\":45000,\"start\":44993},{\"end\":45015,\"start\":45008},{\"end\":45030,\"start\":45023},{\"end\":45042,\"start\":45035},{\"end\":45047,\"start\":45044},{\"end\":45620,\"start\":45616},{\"end\":45637,\"start\":45632},{\"end\":45917,\"start\":45910},{\"end\":45934,\"start\":45929},{\"end\":45946,\"start\":45939},{\"end\":46348,\"start\":46340},{\"end\":46362,\"start\":46356},{\"end\":46366,\"start\":46364},{\"end\":46547,\"start\":46542},{\"end\":46561,\"start\":46553},{\"end\":46980,\"start\":46978},{\"end\":46993,\"start\":46989},{\"end\":47001,\"start\":46998},{\"end\":47015,\"start\":47011},{\"end\":47029,\"start\":47025},{\"end\":47042,\"start\":47039},{\"end\":47534,\"start\":47532},{\"end\":47550,\"start\":47541},{\"end\":47570,\"start\":47560},{\"end\":47583,\"start\":47578},{\"end\":47602,\"start\":47595},{\"end\":47615,\"start\":47612},{\"end\":47631,\"start\":47624},{\"end\":47649,\"start\":47640},{\"end\":47666,\"start\":47659},{\"end\":47684,\"start\":47676},{\"end\":48275,\"start\":48273},{\"end\":48288,\"start\":48284},{\"end\":48300,\"start\":48296},{\"end\":48312,\"start\":48309},{\"end\":48321,\"start\":48317},{\"end\":48335,\"start\":48332},{\"end\":48771,\"start\":48769},{\"end\":48782,\"start\":48778},{\"end\":48794,\"start\":48790},{\"end\":48808,\"start\":48804},{\"end\":48816,\"start\":48814},{\"end\":48833,\"start\":48829},{\"end\":49278,\"start\":49276},{\"end\":49293,\"start\":49286},{\"end\":49307,\"start\":49300},{\"end\":49320,\"start\":49316},{\"end\":49807,\"start\":49797},{\"end\":49822,\"start\":49817},{\"end\":49837,\"start\":49834},{\"end\":49854,\"start\":49846},{\"end\":49871,\"start\":49867},{\"end\":50242,\"start\":50239},{\"end\":50258,\"start\":50252},{\"end\":50273,\"start\":50266},{\"end\":50289,\"start\":50282},{\"end\":50303,\"start\":50296},{\"end\":50320,\"start\":50312},{\"end\":50875,\"start\":50872},{\"end\":50888,\"start\":50884},{\"end\":50897,\"start\":50894},{\"end\":50909,\"start\":50906},{\"end\":50917,\"start\":50915},{\"end\":50931,\"start\":50928},{\"end\":51447,\"start\":51438},{\"end\":51462,\"start\":51455},{\"end\":51477,\"start\":51469},{\"end\":51970,\"start\":51961},{\"end\":51988,\"start\":51980},{\"end\":52433,\"start\":52427},{\"end\":52446,\"start\":52440},{\"end\":52618,\"start\":52608},{\"end\":52628,\"start\":52622},{\"end\":52648,\"start\":52638},{\"end\":52667,\"start\":52661},{\"end\":52680,\"start\":52674},{\"end\":52697,\"start\":52686},{\"end\":52701,\"start\":52699},{\"end\":53142,\"start\":53135},{\"end\":53152,\"start\":53149},{\"end\":53572,\"start\":53565},{\"end\":53582,\"start\":53579},{\"end\":53594,\"start\":53590},{\"end\":53604,\"start\":53601},{\"end\":53890,\"start\":53886},{\"end\":53906,\"start\":53898},{\"end\":53921,\"start\":53915},{\"end\":53939,\"start\":53931},{\"end\":53957,\"start\":53948},{\"end\":54426,\"start\":54422},{\"end\":54441,\"start\":54436},{\"end\":54460,\"start\":54454},{\"end\":54476,\"start\":54469},{\"end\":54491,\"start\":54484},{\"end\":54501,\"start\":54495},{\"end\":54516,\"start\":54511},{\"end\":54532,\"start\":54518},{\"end\":55072,\"start\":55068},{\"end\":55085,\"start\":55082},{\"end\":55101,\"start\":55097},{\"end\":55114,\"start\":55111},{\"end\":55621,\"start\":55617},{\"end\":55637,\"start\":55632},{\"end\":55650,\"start\":55647},{\"end\":55658,\"start\":55656},{\"end\":55667,\"start\":55663},{\"end\":55679,\"start\":55676},{\"end\":56130,\"start\":56122},{\"end\":56144,\"start\":56138},{\"end\":56162,\"start\":56153},{\"end\":56186,\"start\":56173},{\"end\":56664,\"start\":56658},{\"end\":56684,\"start\":56673},{\"end\":56700,\"start\":56694},{\"end\":57132,\"start\":57129},{\"end\":57145,\"start\":57143},{\"end\":57159,\"start\":57155},{\"end\":57169,\"start\":57163},{\"end\":57176,\"start\":57174},{\"end\":57180,\"start\":57178},{\"end\":57666,\"start\":57659},{\"end\":57685,\"start\":57676},{\"end\":57701,\"start\":57695},{\"end\":57716,\"start\":57711},{\"end\":57729,\"start\":57724},{\"end\":58244,\"start\":58233},{\"end\":58261,\"start\":58254},{\"end\":58274,\"start\":58270},{\"end\":58688,\"start\":58680},{\"end\":58704,\"start\":58694},{\"end\":58722,\"start\":58715},{\"end\":58736,\"start\":58730},{\"end\":58750,\"start\":58743},{\"end\":58759,\"start\":58755},{\"end\":59061,\"start\":59056},{\"end\":59077,\"start\":59070},{\"end\":59088,\"start\":59086},{\"end\":59106,\"start\":59098},{\"end\":59571,\"start\":59565},{\"end\":59583,\"start\":59578},{\"end\":59599,\"start\":59591},{\"end\":60048,\"start\":60044},{\"end\":60062,\"start\":60060},{\"end\":60077,\"start\":60073},{\"end\":60092,\"start\":60087},{\"end\":60569,\"start\":60561},{\"end\":60591,\"start\":60580},{\"end\":60608,\"start\":60600},{\"end\":60621,\"start\":60616},{\"end\":60632,\"start\":60628},{\"end\":61123,\"start\":61115},{\"end\":61145,\"start\":61134},{\"end\":61162,\"start\":61154},{\"end\":61175,\"start\":61170},{\"end\":61186,\"start\":61182},{\"end\":61534,\"start\":61526},{\"end\":61551,\"start\":61542},{\"end\":61573,\"start\":61562},{\"end\":61584,\"start\":61580},{\"end\":62069,\"start\":62061},{\"end\":62079,\"start\":62073},{\"end\":62094,\"start\":62086},{\"end\":62107,\"start\":62104},{\"end\":62120,\"start\":62116},{\"end\":62130,\"start\":62122},{\"end\":62631,\"start\":62625},{\"end\":62644,\"start\":62638},{\"end\":62658,\"start\":62653},{\"end\":62666,\"start\":62663},{\"end\":62674,\"start\":62672},{\"end\":62690,\"start\":62685},{\"end\":62701,\"start\":62699},{\"end\":62713,\"start\":62709},{\"end\":62726,\"start\":62720},{\"end\":62738,\"start\":62733},{\"end\":63111,\"start\":63103},{\"end\":63131,\"start\":63121},{\"end\":63149,\"start\":63140},{\"end\":63524,\"start\":63510},{\"end\":63536,\"start\":63531},{\"end\":63558,\"start\":63543},{\"end\":63573,\"start\":63566},{\"end\":63944,\"start\":63941},{\"end\":63959,\"start\":63955},{\"end\":63972,\"start\":63969},{\"end\":63983,\"start\":63978},{\"end\":64439,\"start\":64435},{\"end\":64449,\"start\":64445},{\"end\":64816,\"start\":64810},{\"end\":64830,\"start\":64823},{\"end\":65270,\"start\":65262},{\"end\":65286,\"start\":65280},{\"end\":65300,\"start\":65293},{\"end\":65671,\"start\":65667},{\"end\":65683,\"start\":65681},{\"end\":65698,\"start\":65691},{\"end\":65712,\"start\":65705},{\"end\":65727,\"start\":65720},{\"end\":65744,\"start\":65735},{\"end\":66258,\"start\":66254},{\"end\":66269,\"start\":66266},{\"end\":66280,\"start\":66278},{\"end\":66292,\"start\":66289},{\"end\":66301,\"start\":66297},{\"end\":66317,\"start\":66312},{\"end\":66785,\"start\":66780},{\"end\":66803,\"start\":66795},{\"end\":66821,\"start\":66813},{\"end\":66837,\"start\":66830},{\"end\":67330,\"start\":67326},{\"end\":67344,\"start\":67340},{\"end\":67356,\"start\":67353},{\"end\":67364,\"start\":67362},{\"end\":67375,\"start\":67371},{\"end\":67385,\"start\":67383},{\"end\":67397,\"start\":67394},{\"end\":67903,\"start\":67899},{\"end\":67919,\"start\":67914},{\"end\":67932,\"start\":67928},{\"end\":67942,\"start\":67939},{\"end\":67951,\"start\":67948},{\"end\":67963,\"start\":67960},{\"end\":68490,\"start\":68485},{\"end\":68502,\"start\":68499},{\"end\":68510,\"start\":68508},{\"end\":68519,\"start\":68516},{\"end\":68530,\"start\":68527},{\"end\":69057,\"start\":69055},{\"end\":69070,\"start\":69065},{\"end\":69083,\"start\":69079},{\"end\":69095,\"start\":69092},{\"end\":69109,\"start\":69106},{\"end\":69125,\"start\":69121},{\"end\":69576,\"start\":69573},{\"end\":69587,\"start\":69584},{\"end\":69606,\"start\":69598},{\"end\":70046,\"start\":70044},{\"end\":70057,\"start\":70055},{\"end\":70073,\"start\":70067},{\"end\":70090,\"start\":70082},{\"end\":70574,\"start\":70569},{\"end\":70589,\"start\":70584},{\"end\":70605,\"start\":70600},{\"end\":70620,\"start\":70611},{\"end\":70633,\"start\":70629}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2232525},\"end\":35832,\"start\":35502},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218641048},\"end\":36412,\"start\":35834},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":90239045},\"end\":36941,\"start\":36414},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":222091017},\"end\":37321,\"start\":36943},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":215780580},\"end\":37837,\"start\":37323},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6489115},\"end\":38289,\"start\":37839},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52070144},\"end\":38707,\"start\":38291},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3344312},\"end\":39109,\"start\":38709},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2609415},\"end\":39695,\"start\":39111},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12552176},\"end\":40368,\"start\":39697},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":237266979},\"end\":40868,\"start\":40370},{\"attributes\":{\"doi\":\"175:1-175:19\",\"id\":\"b11\",\"matched_paper_id\":219842037},\"end\":41377,\"start\":40870},{\"attributes\":{\"doi\":\"arXiv:2204.11335\",\"id\":\"b12\"},\"end\":41679,\"start\":41379},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2036193},\"end\":42139,\"start\":41681},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":249017754},\"end\":42498,\"start\":42141},{\"attributes\":{\"doi\":\"arXiv:2210.02303\",\"id\":\"b15\"},\"end\":43014,\"start\":42500},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":219955663},\"end\":43322,\"start\":43016},{\"attributes\":{\"doi\":\"arXiv:2204.03458\",\"id\":\"b17\"},\"end\":43638,\"start\":43324},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":227238903},\"end\":44191,\"start\":43640},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3759573},\"end\":44801,\"start\":44193},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":237386323},\"end\":45537,\"start\":44803},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":25793331},\"end\":45832,\"start\":45539},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":980236},\"end\":46292,\"start\":45834},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b23\"},\"end\":46512,\"start\":46294},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1363510},\"end\":46898,\"start\":46514},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":236635025},\"end\":47474,\"start\":46900},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":248506225},\"end\":48187,\"start\":47476},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":252595796},\"end\":48694,\"start\":48189},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":50783021},\"end\":49192,\"start\":48696},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":227208781},\"end\":49736,\"start\":49194},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":238419701},\"end\":50148,\"start\":49738},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":229297871},\"end\":50752,\"start\":50150},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":202888704},\"end\":51369,\"start\":50754},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":226239625},\"end\":51891,\"start\":51371},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":244909354},\"end\":52374,\"start\":51893},{\"attributes\":{\"id\":\"b35\"},\"end\":52530,\"start\":52376},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":213175590},\"end\":53078,\"start\":52532},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":212675709},\"end\":53516,\"start\":53080},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":202565675},\"end\":53792,\"start\":53518},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":58007025},\"end\":54411,\"start\":53794},{\"attributes\":{\"id\":\"b40\"},\"end\":54994,\"start\":54413},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":81981856},\"end\":55522,\"start\":54996},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":250626640},\"end\":56070,\"start\":55524},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":227227965},\"end\":56609,\"start\":56072},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":232352612},\"end\":57058,\"start\":56611},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":211677243},\"end\":57589,\"start\":57060},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":245335280},\"end\":58161,\"start\":57591},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3719281},\"end\":58635,\"start\":58163},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1687220},\"end\":59023,\"start\":58637},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1240104},\"end\":59486,\"start\":59025},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":145052179},\"end\":59973,\"start\":59488},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":215548442},\"end\":60494,\"start\":59975},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":56657859},\"end\":61056,\"start\":60496},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":202767986},\"end\":61458,\"start\":61058},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":4667462},\"end\":61998,\"start\":61460},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":233388032},\"end\":62553,\"start\":62000},{\"attributes\":{\"doi\":\"arXiv:2209.14792\",\"id\":\"b56\"},\"end\":63002,\"start\":62555},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":174798113},\"end\":63437,\"start\":63004},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b58\",\"matched_paper_id\":14888175},\"end\":63860,\"start\":63439},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":30824366},\"end\":64364,\"start\":63862},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":214667893},\"end\":64749,\"start\":64366},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":216080881},\"end\":65196,\"start\":64751},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":50783765},\"end\":65618,\"start\":65198},{\"attributes\":{\"id\":\"b63\"},\"end\":66169,\"start\":65620},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":251224093},\"end\":66716,\"start\":66171},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":209405397},\"end\":67251,\"start\":66718},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":52860134},\"end\":67829,\"start\":67253},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":219633501},\"end\":68381,\"start\":67831},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":1504491},\"end\":68967,\"start\":68383},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":247940204},\"end\":69517,\"start\":68969},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":7833173},\"end\":69979,\"start\":69519},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":227254854},\"end\":70487,\"start\":69981},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":4766599},\"end\":71070,\"start\":70489}]", "bib_title": "[{\"end\":35533,\"start\":35502},{\"end\":35905,\"start\":35834},{\"end\":36451,\"start\":36414},{\"end\":37009,\"start\":36943},{\"end\":37355,\"start\":37323},{\"end\":37857,\"start\":37839},{\"end\":38310,\"start\":38291},{\"end\":38759,\"start\":38709},{\"end\":39207,\"start\":39111},{\"end\":39755,\"start\":39697},{\"end\":40421,\"start\":40370},{\"end\":40983,\"start\":40870},{\"end\":41694,\"start\":41681},{\"end\":42218,\"start\":42141},{\"end\":43056,\"start\":43016},{\"end\":43686,\"start\":43640},{\"end\":44261,\"start\":44193},{\"end\":44883,\"start\":44803},{\"end\":45606,\"start\":45539},{\"end\":45901,\"start\":45834},{\"end\":46535,\"start\":46514},{\"end\":46969,\"start\":46900},{\"end\":47523,\"start\":47476},{\"end\":48264,\"start\":48189},{\"end\":48761,\"start\":48696},{\"end\":49266,\"start\":49194},{\"end\":49789,\"start\":49738},{\"end\":50230,\"start\":50150},{\"end\":50866,\"start\":50754},{\"end\":51426,\"start\":51371},{\"end\":51949,\"start\":51893},{\"end\":52602,\"start\":52532},{\"end\":53127,\"start\":53080},{\"end\":53557,\"start\":53518},{\"end\":53873,\"start\":53794},{\"end\":55058,\"start\":54996},{\"end\":55608,\"start\":55524},{\"end\":56113,\"start\":56072},{\"end\":56651,\"start\":56611},{\"end\":57121,\"start\":57060},{\"end\":57651,\"start\":57591},{\"end\":58226,\"start\":58163},{\"end\":58674,\"start\":58637},{\"end\":59045,\"start\":59025},{\"end\":59552,\"start\":59488},{\"end\":60034,\"start\":59975},{\"end\":60548,\"start\":60496},{\"end\":61102,\"start\":61058},{\"end\":61513,\"start\":61460},{\"end\":62048,\"start\":62000},{\"end\":63093,\"start\":63004},{\"end\":63501,\"start\":63439},{\"end\":63932,\"start\":63862},{\"end\":64425,\"start\":64366},{\"end\":64800,\"start\":64751},{\"end\":65252,\"start\":65198},{\"end\":65656,\"start\":65620},{\"end\":66246,\"start\":66171},{\"end\":66771,\"start\":66718},{\"end\":67321,\"start\":67253},{\"end\":67894,\"start\":67831},{\"end\":68479,\"start\":68383},{\"end\":69047,\"start\":68969},{\"end\":69566,\"start\":69519},{\"end\":70037,\"start\":69981},{\"end\":70559,\"start\":70489}]", "bib_author": "[{\"end\":35547,\"start\":35535},{\"end\":35563,\"start\":35547},{\"end\":35581,\"start\":35563},{\"end\":35599,\"start\":35581},{\"end\":35922,\"start\":35907},{\"end\":35931,\"start\":35922},{\"end\":35945,\"start\":35931},{\"end\":35959,\"start\":35945},{\"end\":35981,\"start\":35959},{\"end\":36464,\"start\":36453},{\"end\":36479,\"start\":36464},{\"end\":36488,\"start\":36479},{\"end\":36503,\"start\":36488},{\"end\":36516,\"start\":36503},{\"end\":36533,\"start\":36516},{\"end\":37027,\"start\":37011},{\"end\":37045,\"start\":37027},{\"end\":37064,\"start\":37045},{\"end\":37081,\"start\":37064},{\"end\":37372,\"start\":37357},{\"end\":37387,\"start\":37372},{\"end\":37405,\"start\":37387},{\"end\":37421,\"start\":37405},{\"end\":37436,\"start\":37421},{\"end\":37875,\"start\":37859},{\"end\":37885,\"start\":37875},{\"end\":37902,\"start\":37885},{\"end\":37920,\"start\":37902},{\"end\":38327,\"start\":38312},{\"end\":38342,\"start\":38327},{\"end\":38356,\"start\":38342},{\"end\":38372,\"start\":38356},{\"end\":38777,\"start\":38761},{\"end\":38792,\"start\":38777},{\"end\":38808,\"start\":38792},{\"end\":38823,\"start\":38808},{\"end\":38832,\"start\":38823},{\"end\":38849,\"start\":38832},{\"end\":38859,\"start\":38849},{\"end\":39225,\"start\":39209},{\"end\":39236,\"start\":39225},{\"end\":39253,\"start\":39236},{\"end\":39260,\"start\":39253},{\"end\":39777,\"start\":39757},{\"end\":39794,\"start\":39777},{\"end\":39804,\"start\":39794},{\"end\":39820,\"start\":39804},{\"end\":39836,\"start\":39820},{\"end\":39853,\"start\":39836},{\"end\":39870,\"start\":39853},{\"end\":39884,\"start\":39870},{\"end\":39900,\"start\":39884},{\"end\":39906,\"start\":39900},{\"end\":40448,\"start\":40423},{\"end\":40468,\"start\":40448},{\"end\":40490,\"start\":40468},{\"end\":40996,\"start\":40985},{\"end\":41016,\"start\":40996},{\"end\":41034,\"start\":41016},{\"end\":41445,\"start\":41425},{\"end\":41456,\"start\":41445},{\"end\":41471,\"start\":41456},{\"end\":41486,\"start\":41471},{\"end\":41490,\"start\":41486},{\"end\":41706,\"start\":41696},{\"end\":41721,\"start\":41706},{\"end\":41741,\"start\":41721},{\"end\":41761,\"start\":41741},{\"end\":41768,\"start\":41761},{\"end\":42232,\"start\":42220},{\"end\":42247,\"start\":42232},{\"end\":42262,\"start\":42247},{\"end\":42583,\"start\":42570},{\"end\":42597,\"start\":42583},{\"end\":42614,\"start\":42597},{\"end\":42625,\"start\":42614},{\"end\":42636,\"start\":42625},{\"end\":42654,\"start\":42636},{\"end\":42666,\"start\":42654},{\"end\":42678,\"start\":42666},{\"end\":42694,\"start\":42678},{\"end\":42711,\"start\":42694},{\"end\":42718,\"start\":42711},{\"end\":43071,\"start\":43058},{\"end\":43082,\"start\":43071},{\"end\":43097,\"start\":43082},{\"end\":43339,\"start\":43326},{\"end\":43353,\"start\":43339},{\"end\":43371,\"start\":43353},{\"end\":43385,\"start\":43371},{\"end\":43403,\"start\":43385},{\"end\":43418,\"start\":43403},{\"end\":43709,\"start\":43688},{\"end\":43726,\"start\":43709},{\"end\":43742,\"start\":43726},{\"end\":43760,\"start\":43742},{\"end\":44273,\"start\":44263},{\"end\":44289,\"start\":44273},{\"end\":44304,\"start\":44289},{\"end\":44320,\"start\":44304},{\"end\":44340,\"start\":44320},{\"end\":44353,\"start\":44340},{\"end\":44900,\"start\":44885},{\"end\":44914,\"start\":44900},{\"end\":44928,\"start\":44914},{\"end\":44942,\"start\":44928},{\"end\":44958,\"start\":44942},{\"end\":44975,\"start\":44958},{\"end\":44991,\"start\":44975},{\"end\":45002,\"start\":44991},{\"end\":45017,\"start\":45002},{\"end\":45032,\"start\":45017},{\"end\":45044,\"start\":45032},{\"end\":45049,\"start\":45044},{\"end\":45622,\"start\":45608},{\"end\":45639,\"start\":45622},{\"end\":45919,\"start\":45903},{\"end\":45936,\"start\":45919},{\"end\":45948,\"start\":45936},{\"end\":46350,\"start\":46338},{\"end\":46364,\"start\":46350},{\"end\":46368,\"start\":46364},{\"end\":46549,\"start\":46537},{\"end\":46563,\"start\":46549},{\"end\":46982,\"start\":46971},{\"end\":46995,\"start\":46982},{\"end\":47003,\"start\":46995},{\"end\":47017,\"start\":47003},{\"end\":47031,\"start\":47017},{\"end\":47044,\"start\":47031},{\"end\":47536,\"start\":47525},{\"end\":47552,\"start\":47536},{\"end\":47572,\"start\":47552},{\"end\":47585,\"start\":47572},{\"end\":47604,\"start\":47585},{\"end\":47617,\"start\":47604},{\"end\":47633,\"start\":47617},{\"end\":47651,\"start\":47633},{\"end\":47668,\"start\":47651},{\"end\":47686,\"start\":47668},{\"end\":48277,\"start\":48266},{\"end\":48290,\"start\":48277},{\"end\":48302,\"start\":48290},{\"end\":48314,\"start\":48302},{\"end\":48323,\"start\":48314},{\"end\":48337,\"start\":48323},{\"end\":48773,\"start\":48763},{\"end\":48784,\"start\":48773},{\"end\":48796,\"start\":48784},{\"end\":48810,\"start\":48796},{\"end\":48818,\"start\":48810},{\"end\":48835,\"start\":48818},{\"end\":49280,\"start\":49268},{\"end\":49295,\"start\":49280},{\"end\":49309,\"start\":49295},{\"end\":49322,\"start\":49309},{\"end\":49809,\"start\":49791},{\"end\":49824,\"start\":49809},{\"end\":49839,\"start\":49824},{\"end\":49856,\"start\":49839},{\"end\":49873,\"start\":49856},{\"end\":50244,\"start\":50232},{\"end\":50260,\"start\":50244},{\"end\":50275,\"start\":50260},{\"end\":50291,\"start\":50275},{\"end\":50305,\"start\":50291},{\"end\":50322,\"start\":50305},{\"end\":50877,\"start\":50868},{\"end\":50890,\"start\":50877},{\"end\":50899,\"start\":50890},{\"end\":50911,\"start\":50899},{\"end\":50919,\"start\":50911},{\"end\":50933,\"start\":50919},{\"end\":51449,\"start\":51428},{\"end\":51464,\"start\":51449},{\"end\":51479,\"start\":51464},{\"end\":51972,\"start\":51951},{\"end\":51990,\"start\":51972},{\"end\":52435,\"start\":52422},{\"end\":52448,\"start\":52435},{\"end\":52620,\"start\":52604},{\"end\":52630,\"start\":52620},{\"end\":52650,\"start\":52630},{\"end\":52669,\"start\":52650},{\"end\":52682,\"start\":52669},{\"end\":52699,\"start\":52682},{\"end\":52703,\"start\":52699},{\"end\":53144,\"start\":53129},{\"end\":53154,\"start\":53144},{\"end\":53574,\"start\":53559},{\"end\":53584,\"start\":53574},{\"end\":53596,\"start\":53584},{\"end\":53606,\"start\":53596},{\"end\":53892,\"start\":53875},{\"end\":53908,\"start\":53892},{\"end\":53923,\"start\":53908},{\"end\":53941,\"start\":53923},{\"end\":53959,\"start\":53941},{\"end\":54428,\"start\":54413},{\"end\":54443,\"start\":54428},{\"end\":54462,\"start\":54443},{\"end\":54478,\"start\":54462},{\"end\":54493,\"start\":54478},{\"end\":54503,\"start\":54493},{\"end\":54518,\"start\":54503},{\"end\":54534,\"start\":54518},{\"end\":55074,\"start\":55060},{\"end\":55087,\"start\":55074},{\"end\":55103,\"start\":55087},{\"end\":55116,\"start\":55103},{\"end\":55623,\"start\":55610},{\"end\":55639,\"start\":55623},{\"end\":55652,\"start\":55639},{\"end\":55660,\"start\":55652},{\"end\":55669,\"start\":55660},{\"end\":55681,\"start\":55669},{\"end\":56132,\"start\":56115},{\"end\":56146,\"start\":56132},{\"end\":56164,\"start\":56146},{\"end\":56188,\"start\":56164},{\"end\":56199,\"start\":56188},{\"end\":56666,\"start\":56653},{\"end\":56686,\"start\":56666},{\"end\":56702,\"start\":56686},{\"end\":57134,\"start\":57123},{\"end\":57147,\"start\":57134},{\"end\":57161,\"start\":57147},{\"end\":57171,\"start\":57161},{\"end\":57178,\"start\":57171},{\"end\":57182,\"start\":57178},{\"end\":57668,\"start\":57653},{\"end\":57687,\"start\":57668},{\"end\":57703,\"start\":57687},{\"end\":57718,\"start\":57703},{\"end\":57731,\"start\":57718},{\"end\":58246,\"start\":58228},{\"end\":58263,\"start\":58246},{\"end\":58276,\"start\":58263},{\"end\":58690,\"start\":58676},{\"end\":58706,\"start\":58690},{\"end\":58724,\"start\":58706},{\"end\":58738,\"start\":58724},{\"end\":58752,\"start\":58738},{\"end\":58761,\"start\":58752},{\"end\":59063,\"start\":59047},{\"end\":59079,\"start\":59063},{\"end\":59090,\"start\":59079},{\"end\":59108,\"start\":59090},{\"end\":59573,\"start\":59554},{\"end\":59585,\"start\":59573},{\"end\":59601,\"start\":59585},{\"end\":60050,\"start\":60036},{\"end\":60064,\"start\":60050},{\"end\":60079,\"start\":60064},{\"end\":60094,\"start\":60079},{\"end\":60571,\"start\":60550},{\"end\":60593,\"start\":60571},{\"end\":60610,\"start\":60593},{\"end\":60623,\"start\":60610},{\"end\":60634,\"start\":60623},{\"end\":61125,\"start\":61104},{\"end\":61147,\"start\":61125},{\"end\":61164,\"start\":61147},{\"end\":61177,\"start\":61164},{\"end\":61188,\"start\":61177},{\"end\":61536,\"start\":61515},{\"end\":61553,\"start\":61536},{\"end\":61575,\"start\":61553},{\"end\":61586,\"start\":61575},{\"end\":62071,\"start\":62050},{\"end\":62081,\"start\":62071},{\"end\":62096,\"start\":62081},{\"end\":62109,\"start\":62096},{\"end\":62122,\"start\":62109},{\"end\":62132,\"start\":62122},{\"end\":62633,\"start\":62619},{\"end\":62646,\"start\":62633},{\"end\":62660,\"start\":62646},{\"end\":62668,\"start\":62660},{\"end\":62676,\"start\":62668},{\"end\":62692,\"start\":62676},{\"end\":62703,\"start\":62692},{\"end\":62715,\"start\":62703},{\"end\":62728,\"start\":62715},{\"end\":62740,\"start\":62728},{\"end\":63113,\"start\":63095},{\"end\":63133,\"start\":63113},{\"end\":63151,\"start\":63133},{\"end\":63526,\"start\":63503},{\"end\":63538,\"start\":63526},{\"end\":63560,\"start\":63538},{\"end\":63575,\"start\":63560},{\"end\":63946,\"start\":63934},{\"end\":63961,\"start\":63946},{\"end\":63974,\"start\":63961},{\"end\":63985,\"start\":63974},{\"end\":64441,\"start\":64427},{\"end\":64451,\"start\":64441},{\"end\":64818,\"start\":64802},{\"end\":64832,\"start\":64818},{\"end\":65272,\"start\":65254},{\"end\":65288,\"start\":65272},{\"end\":65302,\"start\":65288},{\"end\":65673,\"start\":65658},{\"end\":65685,\"start\":65673},{\"end\":65700,\"start\":65685},{\"end\":65714,\"start\":65700},{\"end\":65729,\"start\":65714},{\"end\":65746,\"start\":65729},{\"end\":66260,\"start\":66248},{\"end\":66271,\"start\":66260},{\"end\":66282,\"start\":66271},{\"end\":66294,\"start\":66282},{\"end\":66303,\"start\":66294},{\"end\":66319,\"start\":66303},{\"end\":66787,\"start\":66773},{\"end\":66805,\"start\":66787},{\"end\":66823,\"start\":66805},{\"end\":66839,\"start\":66823},{\"end\":67332,\"start\":67323},{\"end\":67346,\"start\":67332},{\"end\":67358,\"start\":67346},{\"end\":67366,\"start\":67358},{\"end\":67377,\"start\":67366},{\"end\":67387,\"start\":67377},{\"end\":67399,\"start\":67387},{\"end\":67905,\"start\":67896},{\"end\":67921,\"start\":67905},{\"end\":67934,\"start\":67921},{\"end\":67944,\"start\":67934},{\"end\":67953,\"start\":67944},{\"end\":67965,\"start\":67953},{\"end\":68492,\"start\":68481},{\"end\":68504,\"start\":68492},{\"end\":68512,\"start\":68504},{\"end\":68521,\"start\":68512},{\"end\":68532,\"start\":68521},{\"end\":69059,\"start\":69049},{\"end\":69072,\"start\":69059},{\"end\":69085,\"start\":69072},{\"end\":69097,\"start\":69085},{\"end\":69111,\"start\":69097},{\"end\":69127,\"start\":69111},{\"end\":69578,\"start\":69568},{\"end\":69589,\"start\":69578},{\"end\":69608,\"start\":69589},{\"end\":70048,\"start\":70039},{\"end\":70059,\"start\":70048},{\"end\":70075,\"start\":70059},{\"end\":70092,\"start\":70075},{\"end\":70576,\"start\":70561},{\"end\":70591,\"start\":70576},{\"end\":70607,\"start\":70591},{\"end\":70622,\"start\":70607},{\"end\":70635,\"start\":70622}]", "bib_venue": "[{\"end\":36144,\"start\":36071},{\"end\":36696,\"start\":36623},{\"end\":37601,\"start\":37527},{\"end\":38085,\"start\":38011},{\"end\":38515,\"start\":38452},{\"end\":39425,\"start\":39351},{\"end\":40049,\"start\":39986},{\"end\":40633,\"start\":40570},{\"end\":41097,\"start\":41080},{\"end\":41933,\"start\":41859},{\"end\":43923,\"start\":43850},{\"end\":44516,\"start\":44443},{\"end\":45192,\"start\":45129},{\"end\":46063,\"start\":46014},{\"end\":46728,\"start\":46654},{\"end\":47207,\"start\":47134},{\"end\":47849,\"start\":47776},{\"end\":48446,\"start\":48400},{\"end\":48950,\"start\":48901},{\"end\":49485,\"start\":49412},{\"end\":50465,\"start\":50402},{\"end\":51076,\"start\":51013},{\"end\":51594,\"start\":51545},{\"end\":52153,\"start\":52080},{\"end\":52818,\"start\":52769},{\"end\":53317,\"start\":53244},{\"end\":54122,\"start\":54049},{\"end\":54677,\"start\":54614},{\"end\":55279,\"start\":55206},{\"end\":55796,\"start\":55747},{\"end\":56362,\"start\":56289},{\"end\":56845,\"start\":56782},{\"end\":57345,\"start\":57272},{\"end\":57894,\"start\":57821},{\"end\":59273,\"start\":59199},{\"end\":59744,\"start\":59681},{\"end\":60257,\"start\":60184},{\"end\":60797,\"start\":60724},{\"end\":61749,\"start\":61676},{\"end\":62295,\"start\":62222},{\"end\":64126,\"start\":64064},{\"end\":64566,\"start\":64517},{\"end\":64995,\"start\":64922},{\"end\":65417,\"start\":65368},{\"end\":65927,\"start\":65845},{\"end\":66456,\"start\":66396},{\"end\":67002,\"start\":66929},{\"end\":67562,\"start\":67489},{\"end\":68128,\"start\":68055},{\"end\":68695,\"start\":68622},{\"end\":69242,\"start\":69193},{\"end\":69771,\"start\":69698},{\"end\":70255,\"start\":70182},{\"end\":70798,\"start\":70725},{\"end\":35622,\"start\":35599},{\"end\":36069,\"start\":35981},{\"end\":36621,\"start\":36533},{\"end\":37115,\"start\":37081},{\"end\":37525,\"start\":37436},{\"end\":38009,\"start\":37920},{\"end\":38450,\"start\":38372},{\"end\":38893,\"start\":38859},{\"end\":39349,\"start\":39260},{\"end\":39984,\"start\":39906},{\"end\":40568,\"start\":40490},{\"end\":41078,\"start\":41046},{\"end\":41423,\"start\":41379},{\"end\":41857,\"start\":41768},{\"end\":42302,\"start\":42262},{\"end\":42568,\"start\":42500},{\"end\":43156,\"start\":43097},{\"end\":43848,\"start\":43760},{\"end\":44441,\"start\":44353},{\"end\":45127,\"start\":45049},{\"end\":45670,\"start\":45639},{\"end\":46012,\"start\":45948},{\"end\":46336,\"start\":46294},{\"end\":46652,\"start\":46563},{\"end\":47132,\"start\":47044},{\"end\":47774,\"start\":47686},{\"end\":48398,\"start\":48337},{\"end\":48899,\"start\":48835},{\"end\":49410,\"start\":49322},{\"end\":49932,\"start\":49873},{\"end\":50400,\"start\":50322},{\"end\":51011,\"start\":50933},{\"end\":51543,\"start\":51479},{\"end\":52078,\"start\":51990},{\"end\":52420,\"start\":52376},{\"end\":52767,\"start\":52703},{\"end\":53242,\"start\":53154},{\"end\":53640,\"start\":53606},{\"end\":54047,\"start\":53959},{\"end\":54612,\"start\":54534},{\"end\":55204,\"start\":55116},{\"end\":55745,\"start\":55681},{\"end\":56287,\"start\":56199},{\"end\":56780,\"start\":56702},{\"end\":57270,\"start\":57182},{\"end\":57819,\"start\":57731},{\"end\":58371,\"start\":58276},{\"end\":58820,\"start\":58761},{\"end\":59197,\"start\":59108},{\"end\":59679,\"start\":59601},{\"end\":60182,\"start\":60094},{\"end\":60722,\"start\":60634},{\"end\":61247,\"start\":61188},{\"end\":61674,\"start\":61586},{\"end\":62220,\"start\":62132},{\"end\":62617,\"start\":62555},{\"end\":63210,\"start\":63151},{\"end\":63630,\"start\":63579},{\"end\":64062,\"start\":63985},{\"end\":64515,\"start\":64451},{\"end\":64920,\"start\":64832},{\"end\":65366,\"start\":65302},{\"end\":65843,\"start\":65746},{\"end\":66394,\"start\":66319},{\"end\":66927,\"start\":66839},{\"end\":67487,\"start\":67399},{\"end\":68053,\"start\":67965},{\"end\":68620,\"start\":68532},{\"end\":69191,\"start\":69127},{\"end\":69696,\"start\":69608},{\"end\":70180,\"start\":70092},{\"end\":70723,\"start\":70635}]"}}}, "year": 2023, "month": 12, "day": 17}