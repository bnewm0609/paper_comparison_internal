{"id": 253383947, "updated": "2023-12-10 22:42:18.572", "metadata": {"title": "SC-DepthV3: Robust Self-supervised Monocular Depth Estimation for Dynamic Scenes", "authors": "[{\"first\":\"Libo\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Jia-Wang\",\"last\":\"Bian\",\"middle\":[]},{\"first\":\"Huangying\",\"last\":\"Zhan\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Ian\",\"last\":\"Reid\",\"middle\":[]},{\"first\":\"Chunhua\",\"last\":\"Shen\",\"middle\":[]}]", "venue": "IEEE transactions on pattern analysis and machine intelligence", "journal": "IEEE transactions on pattern analysis and machine intelligence", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Self-supervised monocular depth estimation has shown impressive results in static scenes. It relies on the multi-view consistency assumption for training networks, however, that is violated in dynamic object regions and occlusions. Consequently, existing methods show poor accuracy in dynamic scenes, and the estimated depth map is blurred at object boundaries because they are usually occluded in other training views. In this paper, we propose SC-DepthV3 for addressing the challenges. Specifically, we introduce an external pretrained monocular depth estimation model for generating single-image depth prior, namely pseudo-depth, based on which we propose novel losses to boost self-supervised training. As a result, our model can predict sharp and accurate depth maps, even when training from monocular videos of highly-dynamic scenes. We demonstrate the significantly superior performance of our method over previous methods on six challenging datasets, and we provide detailed ablation studies for the proposed terms. Source code and data will be released at https://github.com/JiawangBian/sc_depth_pl", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "37801376", "pubmedcentral": null, "dblp": "journals/corr/abs-2211-03660", "doi": "10.1109/tpami.2023.3322549"}}, "content": {"source": {"pdf_hash": "585b0ef8b97a796f549f7a0ba3416e617d4735ee", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.03660v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3b319375916982c6f4a12ff1a5390042d8574ce2", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/585b0ef8b97a796f549f7a0ba3416e617d4735ee.txt", "contents": "\nSC-DepthV3: Robust Self-supervised Monocular Depth Estimation for Dynamic Scenes\n5 Oct 2023\n\nLibo Sun \nJia-Wang Bian \nHuangying Zhan \nWei Yin \nIan Reid \nChunhua Shen \nSC-DepthV3: Robust Self-supervised Monocular Depth Estimation for Dynamic Scenes\n5 Oct 2023BFC3ABF6EC3B9EC50DD781FCFE692753arXiv:2211.03660v2[cs.CV]Monocular Depth EstimationUnsupervised LearningSelf-supervised LearningKnowledge Distillation\nSelf-supervised monocular depth estimation has shown impressive results in static scenes.It relies on the multi-view consistency assumption for training networks, however, that is violated in dynamic object regions and occlusions.Consequently, existing methods show poor accuracy in dynamic scenes, and the estimated depth map is blurred at object boundaries because they are usually occluded in other training views.In this paper, we propose SC-DepthV3 for addressing the challenges.Specifically, we introduce an external pretrained monocular depth estimation model for generating single-image depth prior, namely pseudo-depth, based on which we propose novel losses to boost self-supervised training.As a result, our model can predict sharp and accurate depth maps, even when training from monocular videos of highly dynamic scenes.We demonstrate the significantly superior performance of our method over previous methods on six challenging datasets, and we provide detailed ablation studies for the proposed terms.Source code and data have been released at https://github.com/JiawangBian/scdepth pl\n\nINTRODUCTION\n\nM ONOCULAR depth estimation [2] has attracted great at- tention in computer vision.It provides valuable cues for various downstream tasks, such as semantic image segmentation [3], salient object detection [4], 3D reconstruction [5], novel view synthesis [6], and visual odometry [1], [7].Early work [2], [8] solves the monocular depth estimation problem by using supervised learning.However, these methods rely on ground-truth depth labels that are not always available in real-world scenes.To address this limitation, self-supervised monocular depth estimation methods were proposed and showed that a depth network could be trained from stereo image pairs [9] or monocular videos with ego-motions [10] without the need for ground-truth depth labels.We focus on self-supervised learning of monocular depth from videos since only a single camera is required to collect training data in this setup, which has great potential for advancing real-world applications.\n\nSelf-supervised methods typically rely on the multi-view consistency assumption for training networks, e.g., the photometric loss [10] and geometry consistency loss [1] that were used in previous methods.This assumption provides effective constraints for learning scene geometry, while it is violated at regions with occlusion (e.g., object boundaries) and moving objects.Therefore, existing methods often show only excellent results in (almost) static scenes such as KITTI [11] and NYUv2 [12] datasets.When training on more challenging dynamic datasets that have an amount of fast-moving objects, previous state-of-the-art methods [1], [13], [14] show poor accuracy.Moreover, the estimated depth map is blurred at object boundaries because they are usually occluded in other training views.We illustrate several examples of \u2022 First two authors contributed equally.J.-W.Bian  qualitative monocular depth results in Fig. 1.\n\nTo address the issues caused by moving objects and occlusions, existing approaches usually detect these bad regions and then exclude them from training.The methods can be categorized into four classes according to how they detect dynamic regions, involving in the prediction-based [10], semantic-based [15]- [18], flow-based [19]- [22], and geometry-based [1].These methods can reduce corruption from noisy losses during training and generally improves overall accuracy, however, it leads to poor results on dynamic regions at inference time because these regions are not sufficiently regularized in training.There are also more sophisticated approaches [23], [24] that model the velocity of each moving object in multiple views, but they rely on solving a challenging problem in themselves.\n\nWe propose SC-DepthV3 in this paper, which addresses the above-mentioned issues by leveraging external single-image constraints.Specifically, we leverage an off-the-shelf monocular depth estimation model [25] to generate the single-image depth prior, which we term pseudo-depth.Based on it, we propose effective losses to constrain the depth estimation network in self-supervised learning.Here, we use LeReS [25] for generating pseudo-depth, which is trained in large-scale datasets with supervised learning and enables zero-shot generalization in previously unseen data.The excellent qualitative results have been demonstrated in [25], while we find that pseudo-depth may show low quantitative accuracy.Fig. 3 gives an example, where we visualize the error map of pseudo-depth by comparing it with the ground truth.This phenomenon makes supervised zero-shot methods unsuitable for accuracy-sensitive tasks such as visual SLAM and 3D Reconstruction.Furthermore, as pseudo-depth is not quantitatively accurate, it is non-trivial to use it for boosting self-supervised learning.In this paper, our technical contribution is designing effective losses that use imperfect pseudo-depth.It is also worth mentioning that although we use external depth estimation networks, they are only trained once and can be used as off-the-shelf tools in new scenes.Therefore, in practice, our method does not add extra cost to purely self-supervised methods.Qualitative monocular depth estimation results on six datasets.We compare our method (third row) with SC-Depth [1] (second row), which is one of the previous state-of-the-art self-supervised methods.Compared with it, our method enables more robust learning in dynamic scenes (left three columns) and generates sharper depth maps, particularly at object boundary areas.\n\nThe key to solving the dynamic region issue is the proposed Dynamic Region Refinement (DRR) module.The method is inspired by an observation, i.e., we find that pseudo-depth maintains excellent depth ordinal (the further/nearer relations) between any two objects or pixels.To capitalize on these findings, we propose to extract the \"ground-truth\" depth ordinal information between dynamic and static regions (from pseudo-depth) and use it to regularize the self-supervised depth estimation in dynamic regions.Specifically, we sample point pairs between two regions and apply depth ranking loss [26].This is effective because the static backgrounds have already been well-supervised by multiview losses, and the dynamic regions could be uniquely localized by sampling sufficient point pairs between dynamic and static regions.Our method is also based on the fact that the depth ordinal in pseudo-depth is sufficiently accurate [25].Furthermore, to segment dynamic regions from static backgrounds, we use the self-discovered mask that was proposed in SC-Depth [1] and generated by computing forward-backward depth inconsistency in self-supervised training, so the external segmentation networks are not required.Fig. 4 illustrates the proposed DRR module.\n\nMoreover, we observe that pseudo-depth shows smooth local structures and clean object boundaries.This motivates us to propose a Local Structure Refinement (LSR) module to improve the self-supervised depth estimation w.r.t.depth details.The proposed module contains two parts.On the one hand, we extract the surface normal from both pseudo-depth and network-predicted depth, and we constrain them to be consistent by applying a normal matching loss.This improves the overall depth significantly.On the other hand, we constrain depth estimation at object boundary areas by applying our proposed relative normal angle loss.More specifically, we sample point pairs around image edges and enforce their relative normal angles to be consistent between pseudodepth and self-supervised depth.As a result, our method improves qualitative depth estimation results significantly, particularly at object boundaries.Fig. 1 shows several examples of the qualitative depth estimation results.\n\nOur contributions are as follows:\n\n\u2022 We propose SC-DepthV3 for robust self-supervised learning of monocular depth in highly dynamic scenes, which allows for predicting accurate and sharp depth maps.\n\n\n\u2022\n\nWe propose Dynamic Region Refinement (DRR) and Local Structure Refinement (LSR) modules, which are based on pseudo-depth to boost self-supervised learning.\n\n\n\u2022\n\nWe conduct comprehensive experiments and ablation studies on six challenging datasets.The results demonstrate the efficacy of our proposed methods.\n\n\nRELATED WORK\n\nSelf-supervised Monocular Depth Estimation.Garg et al. [9] proposed to train monocular depth estimation models on stereo image pairs by using the photometric loss.Zhou et al. [10] proposed to train the depth estimation model on videos by jointly training a pose estimation model.Following them, many advanced techniques [1], [13], [14], [17], [19], [22], [27]- [30] were proposed to boost the performance.However, multi-view ambiguities make the self-supervised method hard to handle dynamic objects and object boundaries.Previous methods either excluded these regions from training [1], [15], [16], [19] or modeled the object motions [23], [24], but both solutions have their drawbacks.More specifically, simply excluding dynamic regions would result in poor accuracy on these regions at the inference time, and modeling each object's motion is ill-posed and may not be robust in dynamic scenes.Compared with them, our method leverages pretrained single-image prior for resolving multi-view ambiguities, leading to a SOTA self-supervised depth estimation method.More recent methods include [31]- [33].\n\nSC-Depth Series Methods.This paper is the third version of the SC-Depth series methods.In the SC-Depth [1], we addressed the scale inconsistency issue, so our method enables scale-consistent depth estimation over the video, which is beneficial to videobased tasks such as Visual SLAM.In the SC-DepthV2 [30], we analyzed the rotation issue in videos that are captured by handheld cameras, and we proposed an auto-rectify network to handle the large rotation.The V1 and V2 have shown great accuracy in both indoor and outdoor scenes.However, their predicted depth maps are blurred at object boundaries, and they suffer in highly dynamic scenes.In this paper, we propose SC-DepthV3 address the issue of dynamic objects and blurred object boundaries.\n\nZero-shot Monocular Depth Estimation.Many existing methods leverage large-scale datasets and supervised training [25], [34]- [40] to train monocular depth estimation models towards zeroshot generalization on unseen data.For example, [34]- [39] collect stereo images/videos from the internet and use geometric reconstruction tools [41], [42] to generate dense ground-truth depth labels.[40] export perfect ground-truth depths from the synthetic 3D movies [43].Recently, LeReS [25] and DPT [44] achieve the state-of-the-art performance.However, note that their predicted depths are scale-shift-invariant, due to the high diversity of different scenes, which show low quantitative accuracy in outof-distribution data and cannot be used for 3D reconstruction.Nevertheless, we find that their predicted depths carry good attributes that could be leveraged for boosting self-supervised learning of monocular depth estimation.Compared with these methods, our method enables consistent and accurate depth estimation for video-based tasks such as Visual SLAM, which has been demonstrated in SC-Depth [1], thanks to the scaleconsistency constraints.\n\nKnowledge Transfer.Our method is also related to knowledge transfer approaches, because the proposed method can be regarded as transferring the knowledge of pretrained monocular depth estimation models [25] to our self-supervised trained models.However, we argue that our method is very different from previous knowledge transfer or distillation methods.On the one hand, knowledge is often transferred by finetuning pretrained models in new datasets [45], which is not our case and cannot solve the challenges in our problem.The main issue in our problem is the imperfect self-supervised loss, so even if we finetune pretrained models (i.e., it provides a good initialization), the model would become worse and worse with training due to the deficient self-supervised loss functions.On the other hand, knowledge transfer could also be achieved by conducting semi-supervised learning on mixed datasets.Specifically, we can train models on both previous large-scale datasets with ground-truth labels and new datasets without annotations, and then we apply supervised loss in the former and self-supervised loss in the latter.However, this involves new challenges of mix-data training, long training time, and the maintenance of large-scale previous data.In contrast, our method is more elegant than semi-supervised training, since the teacher model is trained only once on large-scale datasets and can be used as an off-the-shelf tool to generate pseudo-depth in new scenes.Moreover, our student model shows significantly higher accuracy than the teacher model, which is rare in the field of knowledge distillation.\n\n\nMETHOD\n\nFig. 2 illustrates an overview of the proposed method.First, our method is based on SC-Depth [1] for basic self-supervised training, which we describe in detail in Sec.3.1.Second, we discuss the single-image depth prior in Sec.3.2 that is generated by using the off-the-shelf monocular depth estimation methods and used in our method for generating auxiliary supervision signals.Finally, we describe the Dynamic Region Refinement (DRR) in Sec.3.3 and Local Structure Refinement (LSR) modules in Sec.3.4, respectively, which are the proposed terms to boost self-supervised training.\n\n\nSelf-supervised Depth Learning (SC-Depth)\n\nIn the self-supervised learning framework, a monocular depth estimation network (DepthNet) and a relative 6-DoF camera pose estimation network (PoseNet) are jointly trained on a large number of monocular videos.First, given a consecutive image pair (I a , I b ) randomly sampled from a training video, we predict their depths (D a , D b ) by forwarding the DepthNet and estimate their relative 6-DoF camera pose P ab by forwarding the PoseNet.Then, we generate the warping flow between two images using the predicted depth and pose, followed by synthesizing the I \u2032 a using the flow and I b via bi-linear interpolation.Finally, we penalize the color inconsistencies between I a and I \u2032 a , and we also constrain the geometry consistency between D a and D b , which backpropagates the gradients to the networks.The objective function is described below.\n\nFirst, we use the geometry consistency loss L G [1] to encourage the predicted depths (D a , D b ) to be consistent with each other in 3D space.Formally,\nL G = 1 |V| p\u2208V D diff (p),(1)\nwhere V stands for valid points that are projected inside the image.D diff stands for the pixel-wise depth inconsistency between D a and D b , which is detailed explained in [1].With it, we can obtain the self-discovered mask:\nM s = 1 \u2212 D diff ,(2)\nwhich assigns lower weights to dynamics and occlusions than static regions, since the former is geometrically inconsistent across multiple views.We use this mask in our proposed DRR module( Sec.3.3) to localize dynamic regions.Second, we use the weighted photometric loss L M P to constrain the warping flow between I a and I b that is generated by the D a and P ab .Formally,\nL M P = 1 |V| p\u2208V (M s (p) \u2022 L P (p)),(3)L P = 1 |V| p\u2208V (\u03bb\u2225I a (p) \u2212 I \u2032 a (p)\u2225 1 + (1 \u2212 \u03bb) 1 \u2212 SSIM aa \u2032 (p) 2 ),(4)\nwhere I \u2032 a is synthesized from I b using the warping flow, and SSIM [46] is a widely-used metric to measure image similarity.We set \u03bb to 0.15 as in [1].\n\nThird, we use the edge-aware smoothness loss to regularize the predicted depth map.Formally,\nL S = p (e \u2212\u2207Ia(p) \u2022 \u2207D a (p)) 2 , (5)\nwhere \u2207 is the first derivative along spatial directions, which guides smoothness by image edges.Overall, our objective function is formulated as follows:\nL Self = \u03b1L M P + \u03b2L G + \u03b3L S .(6)\nWe set \u03b1 = 1, \u03b2 = 0.5, and \u03b3 = 0.1 as in [1].Note that we will replace L S with the proposed normal loss L N in Sec.3.4.Moreover, we also use the auto-masking and per-pixel minimum reprojection loss that are proposed in [13] to filter stationary and non-best points during training.\n\n\nSingle-Image Depth Prior\n\nOur idea is to leverage the pretrained monocular depth estimation network for generating single-image depth prior, which is then used to boost self-supervised learning.Here we use LeReS [25] to generate pseudo-depth, which is trained on large-scale datasets with ground-truth depth labels.Thanks to the supervised training on large-scale data, it shows excellent zero-shot generalization performance on unseen scenes.Note that LeReS was not trained on datasets that we use in this paper for evaluation.An example of LeReS outputs is shown in Fig. 3, where it shows plausible visual results on the DDAD dataset but poor quantitative accuracy.For the error map, we show the AbsRel error, and we use the nearest interpolation for pixels where the ground-truth depth labels (sparse LiDAR points) are unavailable.It shows that LeReS [25] can generalize to previously unseen data with plausible visual results (b), however, high quantitative accuracy is not guaranteed.Here \"AbsRel=0.358\" is averaged over all testing images.This indicates that our idea of leveraging pseudo-depth for boosting self-supervised training is motivated, and it is also non-trivial to use it.\n\nThese phenomena echo our motivation, i.e., pseudo-depth is not accurate enough but has potential that can be leveraged for boosting self-supervised learning.More specifically, we find that the good visual results are contributed to several aspects, including (a) correct depth ordinal (nearer/further relation) between objects; (b) excellent smoothness in predicted depth; (c) sharp depth prediction at object boundaries.\n\nTherefore, based on the above observation, we propose two modules to extract effective supervision signals from pseudodepth.First, we propose a Dynamic Region Refinement (DDR) module that regularizes self-supervised training with a depth ranking constraint, particularly boosting depth prediction on moving objects.Second, we propose a Local Structure Refinement (LSR) module that constrains the smoothness and object boundaries of the predicted depth.The proposed two modules are presented in Sec.3.3 and Sec.3.4, respectively.\n\n\nDynamic Region Refinement\n\nThe key to our proposed dynamic region refinement (DRR) module is constraining depth estimation on dynamic regions by enforcing the nearer/further relation w.r.t. the predicted depths on static regions.Specifically, this is based on two assumptions including (i) The accurate depth ranking relations between any two pixels can be extracted from pseudo-depth; (ii) depth prediction on static regions is sufficiently accurate thanks to the self-supervised losses.The assumptions are valid, as demonstrated in prior work [13], [25], so our idea is generally effective.In the proposed DRR module, we first sample point pairs between dynamic and static regions, where the segmentation of images is obtained in a selfsupervised manner (Eqn.2).Then we compute depth ranking loss on sampled point pairs to regularize the predicted depth map.The proposed sampling method and loss function are presented in the following paragraphs, and Fig. 4 illustrates a training example.\n\nDynamic-focused Sampling.To sample point pairs between static and dynamic regions, we need the segmentation of training images.This could be achieved with the use of pretrained semantic segmentation networks, e.g., we can assume objects of certain classes such as vehicles as moving objects and others as static backgrounds.However, it involves extra data preprocessing and pretrained networks.Moreover, the dynamic of an object does not necessarily rely on the semantic classes, e.g., a chair can be dynamic while a person is moving it around.Instead, we derive dynamic/static segmentation from the self-discovered mask (Eqn.2) that is computed based on the geometric consistency.It is a soft weight mask and assigns smaller values for depthinconsistent regions (dynamics or occlusions) than others (static regions).To obtain binary segmentation, we propose to rank weights and pick the lowest 20% as potential dynamic regions, rather than doing hard thresholding.Here we assume that the ratio of moving objects pixels is around 20% or less, which is true in most real-world scenes.Then for each point in the dynamic regions, we pair it with a point that is randomly sampled from static regions.Moreover, other than constructing dynamic-static pairs as discussed above, we also sample point pairs randomly from the whole image, which serves as an additional global regularization.\n\nConfident Depth Ranking Loss.We compute the depth ranking loss on the sampled point pairs in training.The original loss function was proposed in [26].Formally, for a pair of points with predicted depth values [p 0 , p 1 ], the loss is\n\u03d5 \u2032 (p 0 , p 1 ) = log(1 + exp(\u2212\u2113(p 0 \u2212 p 1 ))), \u2113 \u0338 = 0 (p 0 \u2212 p 1 ) 2 , \u2113 = 0 (7)\nwhere \u2113 is the ground truth ordinal label, which can be induced by a ground truth depth map:\n\u2113 = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 +1, p * 0 /p * 1 \u2265 1 + \u03c4, \u22121, p * 0 /p * 1 \u2264 1 1+\u03c4 , 0, otherwise.(8)\nHere \u03c4 is a threshold, which is 0.03 in previous work [47], and p * denotes pseudo-depth.\n\nWe empirically find that Eqn. 7 is sub-optimal in our method since pseudo-depth is not as accurate as the ground-truth depth.Therefore, we have to take the confidence of pseudo-depth ordinals into consideration.Specifically, we observe that the ordinal is often sufficiently reliable when two points have sufficiently different depth values, i.e., when p * 0 /p * 1 \u226b 1 or p * 0 /p * 1 \u226a 1, and otherwise, it may be unreliable when two depth values are very close, i.e., when p * 0 /p * 1 \u2248 1.Based on the above observation, we propose to (a) increase \u03c4 from 0.03 to 0.15 for higher tolerance; and (b) ignore point pairs that have \u2113 = 0. Formally, we reformulate Eqn.7 as\n\u03d5(p 0 , p 1 ) = log(1 + exp(\u2212\u2113(p 0 \u2212 p 1 ))).(9)\nTherefore, our Confident Depth Ranking Loss is defined as:\nL CDR = 1 |\u2126| p\u2208\u2126 \u03d5(p),(10)\nwhere \u2126 stands for the sampled point pairs that have l \u0338 = 0.\n\n\nLocal Structure Refinement\n\nAs mentioned above in Sec.3.2, pseudo-depth provides excellent depth smoothness.In this section, we propose to leverage such attributes to regularize the self-supervised depth.Our idea is to (i) constrain the surface normals that are derived from predicted depths and pseudo-depths to be matched; and (ii) constrain two depth maps to be consistent w.r.t.relative normal angles of sampled point pairs around edges.Here, the first step is focused on refining the overall depth structures, and the second step is focused on improving object boundary regions.The details are provided below.\n\nNormal Matching Loss.The edge-aware depth smoothness loss (Eqn.5) is often used in self-supervised monocular depth estimation, and it is also used in our baseline.Here, we propose to replace it with the normal matching loss:\nL N = 1 N N i=1 ||n i \u2212 n * i || 1 ,(11)\nwhere n i is the surface normal derived from the predicted depth, and n * i is the normal derived from pseudo-depth.N stands for the total number of pixels in the image.The pixel-wise loss function provides strong supervision for overall depth structures.\n\nEdge-aware Relative Normal Loss.Not only do overall structure refinement, but also we focus on object boundary areas.Specifically, we sample point pairs around image edges and constrain the relative normal angles of sampled point pairs to be consistent with pseudo-depth.Here, we use edge-guided sampling that was proposed in [47] to construct point pairs \u27e8A, B\u27e9, and we define the Edge-aware Relative Normal Loss as:\nL ERN = 1 N N i=1 ||n Ai \u2022 n Bi \u2212 n * Ai \u2022 n * Bi || 1 ,(12)\nwhere n A denotes the normal of a sampled point from the predicted depth, and * denotes pseudo-depth.Combining the edgeguided sampling and relative normal loss, we can effectively constrain the depth estimation on object boundary regions.\n\nThe proposed L ERN is similar to the pair-wise normal loss that is proposed in [25], while the latter samples point pairs from edges, planes, and whole images.In contrast, we sample points solely from edges because sampling from other regions requires high-quality ground-truth depth.In our case, pseudo-depth is not accurate enough to maintain high-quality global structures, hence we only constrain the local structure.We analyze this effect with a detailed ablation study in Sec.4.3.\n\n\nTraining\n\nLosses.Based on the proposed two refinement modules, we rewrite the overall loss function (Eqn.6) of our baseline and obtain the new objective function as:\nL = \u03b1L M P + \u03b2L G + \u03b3L N + \u03b4L CDR + \u03f5L ERN ,(13)\nwhere we set \u03b1 = 1, \u03b2 = 0.5, and \u03b3 = \u03b4 = \u03f5 = 0.1 in training based on empirical tuning.\n\nNetworks.Our depth and pose networks are the same as previous work [1], [13], where we use ResNet-18 [48] backbone for both depth and pose estimation networks.The depth network is a U-Net structure [49] with a DispNet [10] as the decoder.The activations are sigmoids at the output layer and ELU nonlinearities [50] elsewhere.We convert the sigmoid output x to depth with D = 1/(ax + b), where a and b are chosen to constrain D between 0.1 and 100 units.The pose network accepts two RGB frames as input and outputs the 6D relative pose.We modify the first layer of ResNet-18 to have six channels for accepting twoframe inputs, and features are decoded to 6-DoF parameters via four convolutional layers.\n\nTraining Details.We implement the proposed method using the PyTorch library [51].Following [10], [21], [52], we use a snippet of three sequential video frames as a training sample.The images are augmented with random scaling, cropping, and horizontal flips during training.We use the Adam [53] optimizer and set the learning rate to be 10 \u22124 .We initialize the encoder by using the pre-trained model on ImageNet [54].We train our networks in 100k iterations on each dataset.\n\n\nEXPERIMENT\n\n\nDatasets and Evaluation Metrics\n\nThe proposed method focuses on boosting self-supervised monocular depth estimation in challenging dynamic scenes, so we mainly evaluate our methods on three dynamic datasets, including DDAD driving dataset [14], BONN dynamic dataset [55], and TUM dataset [56] (dynamic object split).Note that these datasets contain fast-moving objects, which are much more challenging than the widely-used KITTI [11] and NYUv2 [12] datasets.We assume that the latter two datasets are almost static in this paper, and we also report results on them.All the mentioned self-supervised methods are trained on each dataset individually for a fair comparison.Moreover, following previous methods, we analyze the depth results at object boundaries and plane regions in the IBims-1 dataset [57].In the following paragraphs, the details of each dataset are described.Depth Evaluation Metrics.We use standard depth evaluation metrics, including mean absolute relative error (AbsRel), root mean squared error (RMS), root mean squared log error (RM-Slog), and the accuracy under threshold (\u03b4 i < 1.25 i , i = 1, 2, 3).\n\n\nDDAD\n\nThe detailed definition of these depth metrics can be found in [2].Besides, following previous work [1], [10], we multiply the predicted depth maps by a scalar that matches the median with that of the ground truth for evaluation, i.e., s = median(D gt )/median(D pred ), since self-supervised methods cannot recover the metric scale.For the evaluation on iBims, TABLE 1 Self-supervised monocular depth estimation results on the DDAD driving dataset [14].We segment vehicles and pedestrians as dynamic objects and consider the remaining regions as static backgrounds.This dataset is more challenging than KITTI due to more complex scenes, fewer stopping cars, and longer depth ranges (200m vs 80m).Note that DynamicDepth [58] uses two frames for depth estimation.[56].We use the videos under the category of \"Dynamic Objects\" for training and testing, in which moving objects occupy a large proportion of pixels in each image.[57].\n\n\nFull Image\n\n\nMethods\n\nEvaluation on Static/Dynamic Regions.We use MSeg [75] to generate the semantic segmentation mask of testing images.The model is trained on a composite dataset, so it is able to generate segmentation results for both indoor and outdoor driving scenes.\n\nIn driving datasets (i.e., KITTI and DDAD), all vehicle and pedestrian segments are regarded as dynamic objects, and other regions are regarded as static backgrounds.In indoor datasets (i.e., TUM and BONN), we consider all human segments as dynamic regions.Note that we align the global scale to the ground-truth depth first, and then we evaluate depth accuracy on static regions, dynamic regions, and full images, individually.\n\n\nEvaluation Results\n\nResults on Dynamic Datasets.We use three dynamic datasets mentioned above to evaluate the proposed method, and the quantitative depth estimation results are reported in Tab. 1, 2, and 3, respectively.We show the qualitative comparison results in Fig. 5, and demo videos for depth estimation are in the supplementary.A more detailed analysis is conducted below.Tab. 1 shows the results on DDAD dataset, where we compare our method with previous state-of-the-art methods, including Monodepth2 [13], PackNet [14], and SC-Depth [1].The results show that our method outperforms previous methods by a large margin, and particularly on dynamic regions.Note that our method outperforms PackNet [14], although the latter uses a significantly larger network backbone than ours.This demonstrates our main contribution in this paper, i.e., robust learning of monocular depth in dynamic scenes.Besides, we also report the result without our proposed DRR and LSR modules.Here our baseline method is a modified version of SC-Depth, and it incorporates the advantages of Monodepth2.The results show that the performance of these models is significantly lower than that of our full model, which demonstrates the efficacy of our proposed losses.\n\nTab. 2 and Tab. 3 show the depth estimation results on BONN and TUM datasets, respectively.These indoor datasets are more challenging than driving datasets such as DDAD since the ratio of dynamic regions to the full image of the former is significantly larger than that of the latter.Consequently, previous methods such as Monodepth2 [13] and SC-Depth [1] show poor accuracy in BONN and TUM datasets.Compared with these approaches, our method presents significantly better results.This is contributed TABLE 4 Self-supervised monocular depth estimation results on KITTI [14].Note that the KITTI dataset has many stopping vehicles that help learn depth on cars, which is not the case of learning dynamic object depth from dynamic video that we addressed in this paper.Besides, note that PackNet uses a large backbone, while other methods including ours use the ResNet-18 encoder.to our proposed losses, which enables our method to learn depth estimation robustly from dynamic videos.\n\n\nFull Image\n\nResults on static Datasets.Although our main contribution in this paper is boosting self-supervised monocular depth in dynamic scenes, we show that our method is also working well in almoststatic scenes.The results are reported in the widely-used KITTI driving dataset and NYUv2 indoor dataset.Sampled qualitative results are illustrated in Fig. 6.Tab. 4 shows the depth estimation results on KITTI, where our method is comparable but does not outperform the previous stateof-the-art methods.The reasons are two folds.First, the dataset contains a large number of stopping cars that help self-supervised methods learn depth on vehicles, so our method is hard to further improve the performance when previous methods have obtained good results on dynamic regions.Second, PackNet [14] uses a large network backbone, while other methods, including ours, use ResNet-18, which is much smaller than the former.Overall, we argue that the existing methods have reached a bottleneck in the KITTI dataset, and due to the low impact of dynamic objects on self-supervised learning here, our method is hard to further improve the performance.Moreover, we show qualitative results in Fig. 6 (a), which shows that our method generates sharper depth maps than other methods.Tab. 5 shows the depth results on NYUv2, where we compare our method with previous state-of-the-art methods such as SC-DepthV2 [30] and MonoIndoor [74].The results show that our method outperforms previous approaches significantly.This is mainly contributed to the single-image depth prior, which we use to constrain the normal smoothness and sharp object boundaries   shows the detailed analysis of depth results on the IBims-1 dataset, where we compare our method with Monodepth2 [13] and SC-DepthV2 [30].All models are trained on NYUv2 for a fair comparison.The AbsRel metric shows the overall accuracy of depth estimation results on full images, and other metrics reflect the detailed depth quality at object boundaries and plane regions.The results show that our method significantly outperforms previous methods.We also remove the proposed LSR from our full model for ablation study purposes, and the results in Tab.6 show that the performance is clearly degraded.This demonstrates the efficacy of our proposed LSR module.The qualitative depth estimation results on the IBims-1 dataset are illustrated in Fig. 5 (d).\n\n\nAblation Studies\n\nWe have shown results with and without our proposed DRR and LSR in Tab. 1, 2 and 3.The results demonstrate the efficacy of the proposed modules.In this section, we make a more detailed analysis of the proposed methods, and we also discuss the performance by using different methods to generate pseudo-depth.\n\nDynamic Region Refinement.The proposed DRR module consists of dynamic-focused sampling and confident depth ranking loss.We make ablation studies by comparing our method with random sampling (RS) and original ranking loss (RL) that are used in [26].Tab.7 shows the evaluation results, which show that the performance is significantly degraded when replacing our proposed terms with the existing methods.This demonstrates the efficacy of our proposed methods.\n\nLocal Structure Refinement.The proposed LSR module consists of normal matching loss and edge-guided relative normal ranking loss.The ablation study results are summarized in Tab. 8. We re-place the normal matching loss with edge-aware depth smoothness loss (EDS), and we also add random sampling (RS) to the edgeguided sampling.These variants degenerate the depth accuracy, which demonstrates that our proposed methods are better than existing solutions.\n\nPseudo-depth.We use LeReS [25] (ResNet-101) in this paper for generating pseudo-depth, while it is also possible to use other monocular depth estimation networks.Tab. 9 shows the ablation study results on DDAD dataset, where we also include DPT [44] and ResNet-50 version of LeReS.The results show that the pseudo-depths that are generated by all three variants are not accurate in the DDAD dataset.However, when applying our proposed method that uses pseudo-depth for training selfsupervised models, high-accuracy depth estimation results can be obtained.This demonstrates that our proposed method is not limited to one specific method for generating pseudo-depth.The results also show that our method with LeReS (ResNet-101) [25] outperforms other variants, including DPT.We hypothesize that the reason is that our method incorporates the normal information in training which is shared with LeReS but not with DPT.\n\nDiscussion.We use pseudo-depth to boost self-supervised monocular depth estimation, which somewhat degrades our claim of selfsupervised learning.However, in practice, the monocular depth estimation models such as [25], [44] are only trained once in largescale datasets and can be used as off-the-shelf tools in new unseen scenes, so our method has almost no extra cost compared to pure self-supervised depth estimation methods [1], [13].\n\n\nCONCLUSION\n\nWe propose SC-DepthV3 for robust self-supervised learning of monocular depth from challenging dynamic videos.The key to our method is that we use pseudo-depth, which is generated by a pretrained monocular depth estimation network, for addressing the challenges in self-supervised monocular depth estimation framework.More specifically, we address the issues of dynamic objects and blurred object boundaries.As a result, our proposed method can predict sharp and accurate depth maps, even when the model is trained from highly dynamic videos.We comprehensively evaluate our method on six challenging datasets, including both dynamic and static scenes.The results show that our method significantly outperforms previous alternatives, and the ablation study results demonstrate that the proposed modules are effective.\n\nFig. 1 .\n1\nFig.1.Qualitative monocular depth estimation results on six datasets.We compare our method (third row) with SC-Depth[1] (second row), which is one of the previous state-of-the-art self-supervised methods.Compared with it, our method enables more robust learning in dynamic scenes (left three columns) and generates sharper depth maps, particularly at object boundary areas.\n\n\n\n\nFig.3.Visualization of pseudo-depth (LeReS[25]) on the DDAD dataset.For the error map, we show the AbsRel error, and we use the nearest interpolation for pixels where the ground-truth depth labels (sparse LiDAR points) are unavailable.It shows that LeReS[25] can generalize to previously unseen data with plausible visual results (b), however, high quantitative accuracy is not guaranteed.Here \"AbsRel=0.358\" is averaged over all testing images.This indicates that our idea of leveraging pseudo-depth for boosting self-supervised training is motivated, and it is also non-trivial to use it.\n\n\nFig. 4 .\n4\nFig.4.Dynamic region refinement.We sample point pairs between dynamic and static regions, and then we apply depth ranking loss to constrain the network-predicted depth (c) during training.The \"groundtruth\" depth ordinal is extracted from pseudo-depth (b).To segment dynamic regions from static backgrounds, we use the self-discovered mask (Eqn.2), so there is no extra computational cost.\n\n\nFig. 5 .\n5\nFig. 5. Qualitative depth estimation results.Existing methods show poor results in dynamic scenes (a-c) because they are hard to handle fastmoving objects during training.Even though they show good accuracy in (d), where models are trained in static scenes, the depth is blurred at object boundaries.By contrast, our method predicts sharp and accurate depth robustly.\n\n\nFig. 6 .\n6\nFig.6.Qualitative monocular depth estimation results on static datasets.Our method allows for generating sharper depth maps than previous methods-See object boundaries.\n\n\n\n\n[1]hod overview.Firstly, given a training sample (i.e., Ia and I b two images), we follow SC-Depth[1]to compute self-supervised losses L Self (Eqn.6), which is described in Sec.3.1.Secondly, we generate pseudo-depth P Da using a pretrained depth estimation network, which is discussed in Sec.3.2.Finally, we propose DRR and LSR modules to constrain the network prediction (Da) by using P Da, which are presented in Sec.3.3 and Sec.3.4, respectively.\nPseudo-Depth\ud835\udc43\ud835\udc37 !NetDRRLSR\ud835\udc37 !\ud835\udc3c !Depth Net\ud835\udc37 \"ConcatPose Net\ud835\udc43 !\"\ud835\udc3f #$%&\ud835\udc3c \"Fig. 2.\n\n\n\nand there are fewer stopping cars than KITTI, making it more challenging to train self-supervised models.We use the standard training/testing split, which has 150 training scenes (12650 images) and 50 validation scenes (3950 images).We use the validation scenes for evaluation.Depth ranges are capped to at most 200 meters, and images are resized to the resolution of 640 \u00d7 384 for training depth and pose networks.BONN.The dataset contains 26 dynamic indoor videos that have fast-moving people or other objects.The Kinnect captured depth maps are provided as the ground truth for evaluation.We manually find 4 challenging video sequences with fast-moving people (1785 images) for testing, and we use the remaining videos for training.Depth ranges are capped at 10 meters, and images are resized to the resolution of 320 \u00d7 256 for training networks.TUM.The dataset provides a collection of indoor videos with Kinnect-captured depth maps as the ground truth We choose only videos that belong to the Dynamic Objects category, making sure that the model is trained in dynamic scenes.There are in total 11 sequences, and we use the last two sequences that contain moving people (1375 images) for testing.The remaining 9 dynamic videos are used for training, and images are resized to the resolution of 320 \u00d7 256 for feeding to networks.\nIBims-1. The dataset provides 100 accurate and dense groundtruths for analyzing depth details, including object boundariesand planes. Images are collected in different kinds of indoorenvironments, and it does not provide a training set. For a faircomparison with previous work, we use the model trained on theNYUv2 dataset for all methods.\n[14]e dataset contains 200 driving videos that are captured in urban scenes.The LiDAR scanned point clouds are provided, which we use to generate sparse ground-truth depths for evaluation.In this dataset, almost vehicles are moving on the road, KITTI.The dataset provides driving videos in urban scenes, and it is the most widely-used dataset in self-supervised monocular depth estimation problems.Following previous work[1],[10],[13],[14], we use the Eigen's split that has 697 images for testing, and we use the remaining video sequences for training.Depth ranges are capped at 80 meters, and images are resized to the resolution of 832 \u00d7 256 for training networks.Note that KITTI contains a large number of stopping cars that help self-supervised methods learn depth estimation on cars, so the results on this dataset cannot reflect our main contributions, i.e., robust learning of monocular depth from dynamic scenes.NYUv2.The dataset provides a large collection of indoor videos, and it is widely-used in the computer vision community.There are 654 testing images of static scenes for depth evaluation, and we use the remaining videos that do not contain testing images for training neural networks.Images are resized to the resolution of 320 \u00d7 256 before feeding to the network.Note that this dataset contains almost-static scenes.\n\n\nTABLE 2\n2\n[57]-supervised monocular depth estimation results on the BONN dynamic dataset[57].This dataset is super-challenging because all training and testing videos contain fast-moving objects, which occupy a large proportion of pixels.\nMethodsFull ImageDynamicStaticAbsRel RMS\u03b41\u03b42\u03b43AbsRel\u03b41AbsRel\u03b41Monodepth2 [13]0.5652.337 0.352 0.591 0.7280.4740.1720.5940.383SC-Depth [1]0.2720.733 0.623 0.858 0.9480.7040.1660.1800.714SC-DepthV2 [30]0.2110.619 0.714 0.873 0.9360.4880.2470.1520.803Ours w/o DRR0.1380.396 0.885 0.951 0.9740.2480.6900.1060.939Ours w/o LSR0.1300.382 0.874 0.951 0.9770.2740.6130.0970.937Ours0.1260.379 0.889 0.961 0.9800.2200.7200.1020.931\n\nTABLE 3\n3\nSelf-supervised monocular depth estimation results on the TUM dataset\n\n\nTABLE 5\n5\n[12]cular depth estimation results on the NYUv2[12]dataset.Our method outperforms a majority of supervised methods (first row) and all the self-supervised methods (second row).\nMethodsError \u2193Accuracy \u2191AbsRelRMS\u03b4 1\u03b4 2\u03b4 3Make3D [59]0.3491.2140.447 0.745 0.897DepthTransfer [60]0.3491.210---Liu et al. [61]0.3351.060---Ladicky et al. [62]--0.542 0.829 0.941Li et al. [63]0.2320.8210.621 0.886 0.968Roy et al. [64]0.1870.744---Wang et al. [65]0.2200.7450.605 0.890 0.970Eigen et al. [66]0.1580.6410.769 0.950 0.988Chakrabarti et al. [67]0.1490.6200.806 0.958 0.987Li et al. [68]0.1430.6350.788 0.958 0.991DORN [69]0.1150.5090.828 0.965 0.992VNL [70]0.1080.4160.875 0.976 0.994Zhou et al. [71]0.2080.7120.674 0.900 0.968Zhao et al. [72]0.1890.6860.701 0.912 0.978Monodepth2 [13]0.1690.6140.745 0.946 0.987SC-Depth [1]0.1590.6080.772 0.939 0.982P2Net [73]0.1500.5610.796 0.948 0.986SC-DepthV2 [30]0.1380.5320.820 0.956 0.989MonoIndoor [74]0.1340.5260.823 0.958 0.989Ours0.1230.4860.848 0.963 0.991\n\nTABLE 6\n6\n[57]uation of depth boundaries (DBE) and planes (PE) on iBims-1[57].All models are trained on NYUv2.\nMethod\u03b5 acc DBE \u2193iBims-1 \u03b5 comp DBE \u2193 \u03b5 plan PE \u2193\u03b5 orie PE \u2193AbsRel\u2193Monodepth2 [13]4.26989.77110.943 29.3270.202SC-DepthV2 [30]4.20669.8467.04923.1090.172Ours w/o LSR3.13865.6923.68414.6960.152Ours3.00148.0472.70113.3720.146\n\nTABLE 7\n7\n[26]tion studies of the proposed DRR on DDAD dataset.RS denotes random sampling used in[26], and RL denotes ranking loss used in[26].The decreased performance demonstrates the effectiveness of our proposed methods.\nFullDynamicStaticMethodsAbsRel\u03b4 1AbsRel\u03b4 1AbsRel\u03b4 1Baseline0.1790.7530.3550.5360.1630.761B+DRR (Ours)0.1490.7940.2100.6660.1460.799DRR w/ RS0.1540.7850.2190.6540.1510.790DRR w/ RL0.1590.7670.2140.6590.1590.765\n\nTABLE 8\n8\n[1]ation studies of the proposed LSR on DDAD dataset.EDS denotes edge-aware depth smoothness[1], and RS denotes additional random sampling beside edge-based sampling.The decreased performance demonstrates the importance of our proposed methods.\nFullDynamicStaticMethodsAbsRel\u03b4 1AbsRel\u03b4 1AbsRel\u03b4 1Baseline0.1790.7530.3550.5360.1630.761LSR (Ours)0.1420.8130.1990.6970.1400.813LSR w/ EDS0.1480.7930.2000.6940.1450.796LSR w/ RS0.1460.8020.2000.6880.1430.806of predicted depths. The qualitative results are shown in Fig. 6 (b)and Fig. 5 (d), and the quantitative evaluation results on objectboundaries are summarized in Tab. 6,Depth Quality at Object Boundaries. Tab. 6\n\nTABLE 9\n9\nEvaluation results on DDAD dataset.We compare different methods for generating pseudo-depth.\"+Self\" means training models with our proposed self-supervised method.\nFull ImageDynamicStaticMethodsAbsRel\u03b41AbsRel\u03b41AbsRel\u03b41DPT [44]0.2240.6320.2960.4920.2200.636DPT+Self0.1510.7880.2180.6620.1470.791LeReS(Res50) [25]0.3850.4110.3540.3800.3900.402LeReS(Res50)+Self0.1470.7970.1880.7260.1450.798LeReS(Res101) [25]0.3580.4340.3410.3860.3630.424LeReS(Res101)+Self (Ours)0.1420.8130.1990.6970.1400.813\nACKNOWLEDGMENTSThis work was in part supported by National Key R&D Program of China (No. 2022ZD0118700).This work was in part supported by the Australian Centre of Excellence for Robotic Vision CE140100016, and the ARC Laureate Fellowship FL130100102 to I. Reid.We thank anonymous reviewers for their valuable suggestions.\nUnsupervised scale-consistent depth learning from video. J.-W Bian, H Zhan, N Wang, Z Li, L Zhang, C Shen, M.-M Cheng, I Reid, Int. J. Comput. Vis. 2021\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Adv. Neural Inform. Process. Syst. 2014\n\nRdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation. S.-J Park, K.-S Hong, S Lee, Int. Conf. Comput. Vis. 2017\n\nRgb-d salient object detection: A survey. T Zhou, D.-P Fan, M.-M Cheng, J Shen, L Shao, Computational Visual Media. 2021\n\nKinectfusion: Real-time dense surface mapping and tracking. R A Newcombe, S Izadi, O Hilliges, D Molyneaux, D Kim, A J Davison, P Kohi, J Shotton, S Hodges, A Fitzgibbon, IEEE international symposium on mixed and augmented reality. IEEE2011\n\nNvss: High-quality novel view selfie synthesis. J.-W Bian, H Zhan, I Reid, International Conference on 3D Vision (3DV). 2021\n\nVisual odometry revisited: What should be learnt. H Zhan, C S Weerasekera, J Bian, I Reid, arXiv:1909.098032019arXiv preprint\n\nLearning depth from single monocular images using deep convolutional neural fields. F Liu, C Shen, G Lin, I Reid, IEEE Trans. Pattern Anal. Mach. Intell. 38102016\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. R Garg, V K Bg, G Carneiro, I Reid, Eur. Conf. Comput. Vis. Springer2016\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, IEEE Conf. Comput. Vis. Pattern Recog. 2017\n\nVision meets Robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, International Journal of Robotics Research (IJRR). 2013\n\nIndoor segmentation and support inference from rgbd images. N Silberman, D Hoiem, P Kohli, R Fergus, Eur. Conf. Comput. Vis. 2012\n\nDigging into self-supervised monocular depth prediction. C Godard, O Mac Aodha, M Firman, G J Brostow, Int. Conf. Comput. Vis. 2019\n\n3d packing for self-supervised monocular depth estimation. V Guizilini, R Ambrus, S Pillai, A Raventos, A Gaidon, IEEE Conf. Comput. Vis. Pattern Recog. 2020\n\nDepth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. V Casser, S Pirk, R Mahjourian, A Angelova, AAAI. 2019\n\nDepth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. A Gordon, H Li, R Jonschkowski, A Angelova, Int. Conf. Comput. Vis. 2019\n\nSemanticallyguided representation learning for self-supervised monocular depth. V Guizilini, R Hou, J Li, R Ambrus, A Gaidon, Int. Conf. Learn. Represent. 2020\n\nSelfsupervised monocular depth estimation: Solving the dynamic object problem by semantic guidance. M Klingner, J.-A Term\u00f6hlen, J Mikolajczyk, T Fingscheidt, Eur. Conf. Comput. Vis. Springer2020\n\nGeoNet: Unsupervised learning of dense depth, optical flow and camera pose. Z Yin, J Shi, IEEE Conf. Comput. Vis. Pattern Recog. 2018\n\nDF-Net: Unsupervised joint learning of depth and flow using cross-task consistency. Y Zou, Z Luo, J.-B Huang, Eur. Conf. Comput. Vis. 2018\n\nCompetitive Collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. A Ranjan, V Jampani, K Kim, D Sun, J Wulff, M J Black, IEEE Conf. Comput. Vis. Pattern Recog. 2019\n\nSelf-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. Y Chen, C Schmid, C Sminchisescu, Int. Conf. Comput. Vis. 2019\n\nLearning monocular depth in dynamic scenes via instance-aware projection consistency. S Lee, S Im, S Lin, I S Kweon, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)2021\n\nUnsupervised monocular depth learning in dynamic scenes. H Li, A Gordon, H Zhao, V Casser, A Angelova, Conference on Robot Learning. 2020\n\nLearning to recover 3d scene shape from a single image. W Yin, J Zhang, O Wang, S Niklaus, L Mai, S Chen, C Shen, IEEE Conf. Comput. Vis. Pattern Recog. 2021\n\nSingle-image depth perception in the wild. W Chen, Z Fu, D Yang, J Deng, Adv. Neural Inform. Process. Syst. 2016\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, IEEE Conf. Comput. Vis. Pattern Recog. 2017\n\nUnsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. H Zhan, R Garg, C Saroj Weerasekera, K Li, H Agarwal, I Reid, IEEE Conf. Comput. Vis. Pattern Recog. 2018\n\nUnsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. R Mahjourian, M Wicke, A Angelova, IEEE Conf. Comput. Vis. Pattern Recog. 2018\n\nAutorectify network for unsupervised indoor depth estimation. J.-W Bian, H Zhan, N Wang, T.-J Chin, C Shen, I Reid, IEEE Trans. Pattern Anal. Mach. Intell. 2021\n\nPlanedepth: Self-supervised depth estimation via orthogonal planes. R Wang, Z Yu, S Gao, IEEE Conf. Comput. Vis. Pattern Recog. June 202321434\n\nFully self-supervised depth estimation from defocus clue. H Si, B Zhao, D Wang, Y Gao, M Chen, Z Wang, X Li, IEEE Conf. Comput. Vis. Pattern Recog. 2023\n\nDualrefine: Self-supervised depth and pose estimation through iterative epipolar sampling and refinement toward equilibrium. A Bangunharcana, A M Aly, K Kim, IEEE Conf. Comput. Vis. Pattern Recog. IEEE2023\n\nMonocular relative depth perception with web stereo data supervision. K Xian, C Shen, Z Cao, H Lu, Y Xiao, R Li, Z Luo, IEEE Conf. Comput. Vis. Pattern Recog. 2018\n\nMegadepth: Learning single-view depth prediction from internet photos. Z Li, N Snavely, IEEE Conf. Comput. Vis. Pattern Recog. 2018\n\nLearning the depths of moving people by watching frozen people. Z Li, T Dekel, F Cole, R Tucker, N Snavely, C Liu, W T Freeman, IEEE Conf. Comput. Vis. Pattern Recog. 2019\n\nWeb stereo video supervision for depth prediction from dynamic scenes. C Wang, S Lucey, F Perazzi, O Wang, International Conference on 3D Vision (3DV). IEEE2019\n\nLearning single-image depth from videos using quality assessment networks. W Chen, S Qian, J Deng, IEEE Conf. Comput. Vis. Pattern Recog. 2019\n\nLearning to recover 3d scene shape from a single image. W Yin, J Zhang, O Wang, S Niklaus, L Mai, S Chen, C Shen, IEEE Conf. Comput. Vis. Pattern Recog. 2020\n\nTowards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer. R Ranftl, K Lasinger, D Hafner, K Schindler, V Koltun, IEEE Trans. Pattern Anal. Mach. Intell. 2020\n\nAccurate and efficient stereo processing by semi-global matching and mutual information. H Hirschmuller, IEEE Conf. Comput. Vis. Pattern Recog. 20052\n\nPixelwise view selection for unstructured multi-view stereo. J L Sch\u00f6nberger, E Zheng, M Pollefeys, J.-M Frahm, Eur. Conf. Comput. Vis. 2016\n\nA naturalistic open source movie for optical flow evaluation. D J Butler, J Wulff, G B Stanley, M J Black, Eur. Conf. Comput. Vis. Oct. 2012\n\nVision transformers for dense prediction. R Ranftl, A Bochkovskiy, V Koltun, 2021ArXiv preprint\n\nConsistent video depth estimation. X Luo, J.-B Huang, R Szeliski, K Matzen, J Kopf, Proceedings of ACM SIGGRAPH). ACM SIGGRAPH)2020\n\nImage Quality Assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Trans. Image Process. 1342004\n\nStructureguided ranking loss for single image depth prediction. K Xian, J Zhang, O Wang, L Mai, Z Lin, Z Cao, IEEE Conf. Comput. Vis. Pattern Recog. 2020\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conf. Comput. Vis. Pattern Recog. 2016\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer2015\n\nFast and accurate deep network learning by exponential linear units (elus). D.-A Clevert, T Unterthiner, S Hochreiter, arXiv:1511.072892015arXiv preprint\n\nAutomatic differentiation in pytorch. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z Devito, Z Lin, A Desmaison, L Antiga, A Lerer, 2017\n\nLearning depth from monocular videos using direct methods. C Wang, J Miguel, R Buenaposada, S Zhu, Lucey, IEEE Conf. Comput. Vis. Pattern Recog. June 2018\n\nADAM: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.69802014arXiv preprint\n\nImageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, IEEE Conf. Comput. Vis. Pattern Recog. 2009\n\nReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals. E Palazzolo, J Behley, P Lottes, P Gigu\u00e8re, C Stachniss, Int. Conf. Intelligent Robots and Systems. 2019\n\nA benchmark for the evaluation of rgb-d slam systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, Int. Conf. Intelligent Robots and Systems. 2012\n\nComparison of monocular depth estimation methods using geometrically relevant metrics on the ibims-1 dataset. T Koch, L Liebel, M K\u00f6rner, F Fraundorfer, 2020Computer Vision and Image Understanding\n\nDisentangling object motion and occlusion for unsupervised multi-frame monocular depth. Z Feng, L Yang, L Jing, H Wang, Y Tian, B Li, Eur. Conf. Comput. Vis. Springer2022\n\nLearning depth from single monocular images. A Saxena, S H Chung, A Y Ng, Adv. Neural Inform. Process. Syst. 2006\n\nDepth transfer: Depth extraction from video using non-parametric sampling. K Karsch, C Liu, S B Kang, IEEE Trans. Pattern Anal. Mach. Intell. 2014\n\nDiscrete-continuous depth estimation from a single image. M Liu, M Salzmann, X He, IEEE Conf. Comput. Vis. Pattern Recog. 2014\n\nPulling things out of perspective. L Ladicky, J Shi, M Pollefeys, IEEE Conf. Comput. Vis. Pattern Recog. 2014\n\nDepth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs. B Li, C Shen, Y Dai, A Van Den, M Hengel, He, IEEE Conf. Comput. Vis. Pattern Recog. 2015\n\nMonocular depth estimation using neural regression forest. A Roy, S Todorovic, IEEE Conf. Comput. Vis. Pattern Recog. 2016\n\nTowards unified depth and semantic prediction from a single image. P Wang, X Shen, Z Lin, S Cohen, B Price, A L Yuille, IEEE Conf. Comput. Vis. Pattern Recog. 2015\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, Int. Conf. Comput. Vis. 2015\n\nDepth from a single image by harmonizing overcomplete local network predictions. A Chakrabarti, J Shao, G Shakhnarovich, Adv. Neural Inform. Process. Syst. 2016\n\nA two-streamed network for estimating finescaled depth maps from single rgb images. J Li, R Klein, A Yao, Int. Conf. Comput. Vis. 2017\n\nDeep ordinal regression network for monocular depth estimation. H Fu, M Gong, C Wang, K Batmanghelich, D Tao, IEEE Conf. Comput. Vis. Pattern Recog. 2018\n\nEnforcing geometric constraints of virtual normal for depth prediction. W Yin, Y Liu, C Shen, Y Yan, Int. Conf. Comput. Vis. 2019\n\nMoving indoor: Unsupervised video depth learning in challenging environments. J Zhou, Y Wang, K Qin, W Zeng, Int. Conf. Comput. Vis. 2019\n\nTowards better generalization: Joint depth-pose learning without posenet. W Zhao, S Liu, Y Shu, Y.-J Liu, IEEE Conf. Comput. Vis. Pattern Recog. 2020\n\nP 2 net: Patch-match and plane-regularization for unsupervised indoor depth estimation. Z Yu, L Jin, S Gao, Eur. Conf. Comput. Vis. 2020\n\nMonoindoor: Towards good practice of self-supervised monocular depth estimation for indoor environments. P Ji, R Li, B Bhanu, Y Xu, IEEE Conf. Comput. Vis. Pattern Recog. 2021\n\nMseg: A composite dataset for multi-domain semantic segmentation. J Lambert, Z Liu, O Sener, J Hays, V Koltun, IEEE Conf. Comput. Vis. Pattern Recog. 2020\n", "annotations": {"author": "[{\"end\":103,\"start\":94},{\"end\":118,\"start\":104},{\"end\":134,\"start\":119},{\"end\":143,\"start\":135},{\"end\":153,\"start\":144},{\"end\":167,\"start\":154}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":99},{\"end\":117,\"start\":113},{\"end\":133,\"start\":129},{\"end\":142,\"start\":139},{\"end\":152,\"start\":148},{\"end\":166,\"start\":162}]", "author_first_name": "[{\"end\":98,\"start\":94},{\"end\":112,\"start\":104},{\"end\":128,\"start\":119},{\"end\":138,\"start\":135},{\"end\":147,\"start\":144},{\"end\":161,\"start\":154}]", "author_affiliation": null, "title": "[{\"end\":81,\"start\":1},{\"end\":248,\"start\":168}]", "venue": null, "abstract": "[{\"end\":1511,\"start\":410}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1705,\"start\":1702},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1735,\"start\":1732},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1758,\"start\":1755},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1784,\"start\":1781},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1809,\"start\":1806},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1814,\"start\":1811},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1829,\"start\":1826},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1834,\"start\":1831},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2187,\"start\":2184},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2229,\"start\":2225},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2624,\"start\":2620},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2658,\"start\":2655},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2968,\"start\":2964},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2983,\"start\":2979},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3125,\"start\":3122},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3131,\"start\":3127},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3137,\"start\":3133},{\"end\":3364,\"start\":3360},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3699,\"start\":3695},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3720,\"start\":3716},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3726,\"start\":3722},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3743,\"start\":3739},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3749,\"start\":3745},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3773,\"start\":3770},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4072,\"start\":4068},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4078,\"start\":4074},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4415,\"start\":4411},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4619,\"start\":4615},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4842,\"start\":4838},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5759,\"start\":5756},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6612,\"start\":6608},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6944,\"start\":6940},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7075,\"start\":7072},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8835,\"start\":8832},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8956,\"start\":8952},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9100,\"start\":9097},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9106,\"start\":9102},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9112,\"start\":9108},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9118,\"start\":9114},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9124,\"start\":9120},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9130,\"start\":9126},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9136,\"start\":9132},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9142,\"start\":9138},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9363,\"start\":9360},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9369,\"start\":9365},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9375,\"start\":9371},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9381,\"start\":9377},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9416,\"start\":9412},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9422,\"start\":9418},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9872,\"start\":9868},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9878,\"start\":9874},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9987,\"start\":9984},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10187,\"start\":10183},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10746,\"start\":10742},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10752,\"start\":10748},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10758,\"start\":10754},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10866,\"start\":10862},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10872,\"start\":10868},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10963,\"start\":10959},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10969,\"start\":10965},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11018,\"start\":11014},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11087,\"start\":11083},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11108,\"start\":11104},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11121,\"start\":11117},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11723,\"start\":11720},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11976,\"start\":11972},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12224,\"start\":12220},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13490,\"start\":13487},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14926,\"start\":14923},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15237,\"start\":15234},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15878,\"start\":15874},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15957,\"start\":15954},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16326,\"start\":16323},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16506,\"start\":16502},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16783,\"start\":16779},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17425,\"start\":17421},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19262,\"start\":19258},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19268,\"start\":19264},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21239,\"start\":21235},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21644,\"start\":21640},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24018,\"start\":24014},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24490,\"start\":24486},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25270,\"start\":25267},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25276,\"start\":25272},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":25305,\"start\":25301},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25402,\"start\":25398},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25422,\"start\":25418},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25514,\"start\":25510},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25983,\"start\":25979},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25998,\"start\":25994},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26004,\"start\":26000},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26010,\"start\":26006},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26196,\"start\":26192},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":26319,\"start\":26315},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26636,\"start\":26632},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":26663,\"start\":26659},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":26685,\"start\":26681},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26826,\"start\":26822},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26841,\"start\":26837},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27196,\"start\":27192},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27591,\"start\":27588},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27628,\"start\":27625},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27634,\"start\":27630},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27978,\"start\":27974},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":28249,\"start\":28245},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":28291,\"start\":28287},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28454,\"start\":28450},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":28533,\"start\":28529},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29678,\"start\":29674},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29692,\"start\":29688},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29710,\"start\":29707},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29873,\"start\":29869},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30750,\"start\":30746},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30767,\"start\":30764},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30985,\"start\":30981},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31762,\"start\":31761},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32190,\"start\":32186},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32797,\"start\":32793},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":32817,\"start\":32813},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33152,\"start\":33148},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33172,\"start\":33168},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34365,\"start\":34361},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35063,\"start\":35059},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35282,\"start\":35278},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35764,\"start\":35760},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36168,\"start\":36164},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36174,\"start\":36170},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36381,\"start\":36378},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36387,\"start\":36383},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37350,\"start\":37347},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37655,\"start\":37651},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37867,\"start\":37863}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37605,\"start\":37219},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38200,\"start\":37606},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38602,\"start\":38201},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38983,\"start\":38603},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39165,\"start\":38984},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39696,\"start\":39166},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42711,\"start\":39697},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43372,\"start\":42712},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43454,\"start\":43373},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44457,\"start\":43455},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":44793,\"start\":44458},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":45229,\"start\":44794},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":45905,\"start\":45230},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":46408,\"start\":45906}]", "paragraph": "[{\"end\":2488,\"start\":1527},{\"end\":3412,\"start\":2490},{\"end\":4205,\"start\":3414},{\"end\":6013,\"start\":4207},{\"end\":7267,\"start\":6015},{\"end\":8246,\"start\":7269},{\"end\":8281,\"start\":8248},{\"end\":8446,\"start\":8283},{\"end\":8607,\"start\":8452},{\"end\":8760,\"start\":8613},{\"end\":9879,\"start\":8777},{\"end\":10627,\"start\":9881},{\"end\":11768,\"start\":10629},{\"end\":13383,\"start\":11770},{\"end\":13975,\"start\":13394},{\"end\":14873,\"start\":14021},{\"end\":15028,\"start\":14875},{\"end\":15286,\"start\":15060},{\"end\":15685,\"start\":15309},{\"end\":15958,\"start\":15805},{\"end\":16052,\"start\":15960},{\"end\":16246,\"start\":16092},{\"end\":16564,\"start\":16282},{\"end\":17757,\"start\":16593},{\"end\":18180,\"start\":17759},{\"end\":18710,\"start\":18182},{\"end\":19705,\"start\":18740},{\"end\":21088,\"start\":19707},{\"end\":21324,\"start\":21090},{\"end\":21501,\"start\":21409},{\"end\":21675,\"start\":21586},{\"end\":22348,\"start\":21677},{\"end\":22456,\"start\":22398},{\"end\":22546,\"start\":22485},{\"end\":23163,\"start\":22577},{\"end\":23389,\"start\":23165},{\"end\":23686,\"start\":23431},{\"end\":24105,\"start\":23688},{\"end\":24405,\"start\":24167},{\"end\":24893,\"start\":24407},{\"end\":25061,\"start\":24906},{\"end\":25198,\"start\":25111},{\"end\":25901,\"start\":25200},{\"end\":26377,\"start\":25903},{\"end\":27516,\"start\":26426},{\"end\":28455,\"start\":27525},{\"end\":28730,\"start\":28480},{\"end\":29160,\"start\":28732},{\"end\":30410,\"start\":29183},{\"end\":31393,\"start\":30412},{\"end\":33788,\"start\":31408},{\"end\":34116,\"start\":33809},{\"end\":34575,\"start\":34118},{\"end\":35031,\"start\":34577},{\"end\":35949,\"start\":35033},{\"end\":36388,\"start\":35951},{\"end\":37218,\"start\":36403},{\"end\":37604,\"start\":37231},{\"end\":38199,\"start\":37609},{\"end\":38601,\"start\":38213},{\"end\":38982,\"start\":38615},{\"end\":39164,\"start\":38996},{\"end\":39618,\"start\":39169},{\"end\":41032,\"start\":39700},{\"end\":42710,\"start\":41373},{\"end\":42951,\"start\":42723},{\"end\":43453,\"start\":43384},{\"end\":43642,\"start\":43466},{\"end\":44569,\"start\":44469},{\"end\":45019,\"start\":44805},{\"end\":45485,\"start\":45241},{\"end\":46080,\"start\":45917}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15059,\"start\":15029},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15308,\"start\":15287},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15727,\"start\":15686},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15804,\"start\":15727},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16090,\"start\":16053},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16091,\"start\":16090},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16281,\"start\":16247},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21407,\"start\":21325},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21408,\"start\":21407},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21585,\"start\":21502},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22397,\"start\":22349},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22484,\"start\":22457},{\"attributes\":{\"id\":\"formula_12\"},\"end\":23430,\"start\":23390},{\"attributes\":{\"id\":\"formula_13\"},\"end\":24166,\"start\":24106},{\"attributes\":{\"id\":\"formula_14\"},\"end\":25110,\"start\":25062}]", "table_ref": "[{\"end\":30920,\"start\":30919}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1525,\"start\":1513},{\"end\":8450,\"start\":8449},{\"end\":8611,\"start\":8610},{\"attributes\":{\"n\":\"2\"},\"end\":8775,\"start\":8763},{\"attributes\":{\"n\":\"3\"},\"end\":13392,\"start\":13386},{\"attributes\":{\"n\":\"3.1\"},\"end\":14019,\"start\":13978},{\"attributes\":{\"n\":\"3.2\"},\"end\":16591,\"start\":16567},{\"attributes\":{\"n\":\"3.3\"},\"end\":18738,\"start\":18713},{\"attributes\":{\"n\":\"3.4\"},\"end\":22575,\"start\":22549},{\"attributes\":{\"n\":\"3.5\"},\"end\":24904,\"start\":24896},{\"attributes\":{\"n\":\"4\"},\"end\":26390,\"start\":26380},{\"attributes\":{\"n\":\"4.1\"},\"end\":26424,\"start\":26393},{\"end\":27523,\"start\":27519},{\"end\":28468,\"start\":28458},{\"end\":28478,\"start\":28471},{\"attributes\":{\"n\":\"4.2\"},\"end\":29181,\"start\":29163},{\"end\":31406,\"start\":31396},{\"attributes\":{\"n\":\"4.3\"},\"end\":33807,\"start\":33791},{\"attributes\":{\"n\":\"5\"},\"end\":36401,\"start\":36391},{\"end\":37228,\"start\":37220},{\"end\":38210,\"start\":38202},{\"end\":38612,\"start\":38604},{\"end\":38993,\"start\":38985},{\"end\":42720,\"start\":42713},{\"end\":43381,\"start\":43374},{\"end\":43463,\"start\":43456},{\"end\":44466,\"start\":44459},{\"end\":44802,\"start\":44795},{\"end\":45238,\"start\":45231},{\"end\":45914,\"start\":45907}]", "table": "[{\"end\":39696,\"start\":39619},{\"end\":41372,\"start\":41033},{\"end\":43372,\"start\":42952},{\"end\":44457,\"start\":43643},{\"end\":44793,\"start\":44570},{\"end\":45229,\"start\":45020},{\"end\":45905,\"start\":45486},{\"end\":46408,\"start\":46081}]", "figure_caption": "[{\"end\":37605,\"start\":37230},{\"end\":38200,\"start\":37608},{\"end\":38602,\"start\":38212},{\"end\":38983,\"start\":38614},{\"end\":39165,\"start\":38995},{\"end\":39619,\"start\":39168},{\"end\":41033,\"start\":39699},{\"end\":42952,\"start\":42722},{\"end\":43454,\"start\":43383},{\"end\":43643,\"start\":43465},{\"end\":44570,\"start\":44468},{\"end\":45020,\"start\":44804},{\"end\":45486,\"start\":45240},{\"end\":46081,\"start\":45916}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3411,\"start\":3410},{\"end\":4917,\"start\":4916},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7230,\"start\":7229},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8178,\"start\":8177},{\"end\":13400,\"start\":13399},{\"end\":17141,\"start\":17140},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19673,\"start\":19672},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29435,\"start\":29434},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31755,\"start\":31754},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32584,\"start\":32583},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33787,\"start\":33782}]", "bib_author_first_name": "[{\"end\":46793,\"start\":46789},{\"end\":46801,\"start\":46800},{\"end\":46809,\"start\":46808},{\"end\":46817,\"start\":46816},{\"end\":46823,\"start\":46822},{\"end\":46832,\"start\":46831},{\"end\":46843,\"start\":46839},{\"end\":46852,\"start\":46851},{\"end\":46962,\"start\":46961},{\"end\":46971,\"start\":46970},{\"end\":46982,\"start\":46981},{\"end\":47120,\"start\":47116},{\"end\":47131,\"start\":47127},{\"end\":47139,\"start\":47138},{\"end\":47218,\"start\":47217},{\"end\":47229,\"start\":47225},{\"end\":47239,\"start\":47235},{\"end\":47248,\"start\":47247},{\"end\":47256,\"start\":47255},{\"end\":47358,\"start\":47357},{\"end\":47360,\"start\":47359},{\"end\":47372,\"start\":47371},{\"end\":47381,\"start\":47380},{\"end\":47393,\"start\":47392},{\"end\":47406,\"start\":47405},{\"end\":47413,\"start\":47412},{\"end\":47415,\"start\":47414},{\"end\":47426,\"start\":47425},{\"end\":47434,\"start\":47433},{\"end\":47445,\"start\":47444},{\"end\":47455,\"start\":47454},{\"end\":47591,\"start\":47587},{\"end\":47599,\"start\":47598},{\"end\":47607,\"start\":47606},{\"end\":47716,\"start\":47715},{\"end\":47724,\"start\":47723},{\"end\":47726,\"start\":47725},{\"end\":47741,\"start\":47740},{\"end\":47749,\"start\":47748},{\"end\":47877,\"start\":47876},{\"end\":47884,\"start\":47883},{\"end\":47892,\"start\":47891},{\"end\":47899,\"start\":47898},{\"end\":48032,\"start\":48031},{\"end\":48040,\"start\":48039},{\"end\":48042,\"start\":48041},{\"end\":48048,\"start\":48047},{\"end\":48060,\"start\":48059},{\"end\":48164,\"start\":48163},{\"end\":48172,\"start\":48171},{\"end\":48181,\"start\":48180},{\"end\":48192,\"start\":48191},{\"end\":48194,\"start\":48193},{\"end\":48289,\"start\":48288},{\"end\":48299,\"start\":48298},{\"end\":48307,\"start\":48306},{\"end\":48318,\"start\":48317},{\"end\":48446,\"start\":48445},{\"end\":48459,\"start\":48458},{\"end\":48468,\"start\":48467},{\"end\":48477,\"start\":48476},{\"end\":48574,\"start\":48573},{\"end\":48584,\"start\":48583},{\"end\":48597,\"start\":48596},{\"end\":48607,\"start\":48606},{\"end\":48609,\"start\":48608},{\"end\":48709,\"start\":48708},{\"end\":48722,\"start\":48721},{\"end\":48732,\"start\":48731},{\"end\":48742,\"start\":48741},{\"end\":48754,\"start\":48753},{\"end\":48917,\"start\":48916},{\"end\":48927,\"start\":48926},{\"end\":48935,\"start\":48934},{\"end\":48949,\"start\":48948},{\"end\":49064,\"start\":49063},{\"end\":49074,\"start\":49073},{\"end\":49080,\"start\":49079},{\"end\":49096,\"start\":49095},{\"end\":49218,\"start\":49217},{\"end\":49231,\"start\":49230},{\"end\":49238,\"start\":49237},{\"end\":49244,\"start\":49243},{\"end\":49254,\"start\":49253},{\"end\":49399,\"start\":49398},{\"end\":49414,\"start\":49410},{\"end\":49427,\"start\":49426},{\"end\":49442,\"start\":49441},{\"end\":49571,\"start\":49570},{\"end\":49578,\"start\":49577},{\"end\":49714,\"start\":49713},{\"end\":49721,\"start\":49720},{\"end\":49731,\"start\":49727},{\"end\":49888,\"start\":49887},{\"end\":49898,\"start\":49897},{\"end\":49909,\"start\":49908},{\"end\":49916,\"start\":49915},{\"end\":49923,\"start\":49922},{\"end\":49932,\"start\":49931},{\"end\":49934,\"start\":49933},{\"end\":50096,\"start\":50095},{\"end\":50104,\"start\":50103},{\"end\":50114,\"start\":50113},{\"end\":50246,\"start\":50245},{\"end\":50253,\"start\":50252},{\"end\":50259,\"start\":50258},{\"end\":50266,\"start\":50265},{\"end\":50268,\"start\":50267},{\"end\":50463,\"start\":50462},{\"end\":50469,\"start\":50468},{\"end\":50479,\"start\":50478},{\"end\":50487,\"start\":50486},{\"end\":50497,\"start\":50496},{\"end\":50601,\"start\":50600},{\"end\":50608,\"start\":50607},{\"end\":50617,\"start\":50616},{\"end\":50625,\"start\":50624},{\"end\":50636,\"start\":50635},{\"end\":50643,\"start\":50642},{\"end\":50651,\"start\":50650},{\"end\":50747,\"start\":50746},{\"end\":50755,\"start\":50754},{\"end\":50761,\"start\":50760},{\"end\":50769,\"start\":50768},{\"end\":50887,\"start\":50886},{\"end\":50897,\"start\":50896},{\"end\":50901,\"start\":50898},{\"end\":50910,\"start\":50909},{\"end\":50912,\"start\":50911},{\"end\":51074,\"start\":51073},{\"end\":51082,\"start\":51081},{\"end\":51090,\"start\":51089},{\"end\":51111,\"start\":51110},{\"end\":51117,\"start\":51116},{\"end\":51128,\"start\":51127},{\"end\":51280,\"start\":51279},{\"end\":51294,\"start\":51293},{\"end\":51303,\"start\":51302},{\"end\":51425,\"start\":51421},{\"end\":51433,\"start\":51432},{\"end\":51441,\"start\":51440},{\"end\":51452,\"start\":51448},{\"end\":51460,\"start\":51459},{\"end\":51468,\"start\":51467},{\"end\":51590,\"start\":51589},{\"end\":51598,\"start\":51597},{\"end\":51604,\"start\":51603},{\"end\":51724,\"start\":51723},{\"end\":51730,\"start\":51729},{\"end\":51738,\"start\":51737},{\"end\":51746,\"start\":51745},{\"end\":51753,\"start\":51752},{\"end\":51761,\"start\":51760},{\"end\":51769,\"start\":51768},{\"end\":51945,\"start\":51944},{\"end\":51962,\"start\":51961},{\"end\":51964,\"start\":51963},{\"end\":51971,\"start\":51970},{\"end\":52097,\"start\":52096},{\"end\":52105,\"start\":52104},{\"end\":52113,\"start\":52112},{\"end\":52120,\"start\":52119},{\"end\":52126,\"start\":52125},{\"end\":52134,\"start\":52133},{\"end\":52140,\"start\":52139},{\"end\":52263,\"start\":52262},{\"end\":52269,\"start\":52268},{\"end\":52389,\"start\":52388},{\"end\":52395,\"start\":52394},{\"end\":52404,\"start\":52403},{\"end\":52412,\"start\":52411},{\"end\":52422,\"start\":52421},{\"end\":52433,\"start\":52432},{\"end\":52440,\"start\":52439},{\"end\":52442,\"start\":52441},{\"end\":52569,\"start\":52568},{\"end\":52577,\"start\":52576},{\"end\":52586,\"start\":52585},{\"end\":52597,\"start\":52596},{\"end\":52735,\"start\":52734},{\"end\":52743,\"start\":52742},{\"end\":52751,\"start\":52750},{\"end\":52860,\"start\":52859},{\"end\":52867,\"start\":52866},{\"end\":52876,\"start\":52875},{\"end\":52884,\"start\":52883},{\"end\":52895,\"start\":52894},{\"end\":52902,\"start\":52901},{\"end\":52910,\"start\":52909},{\"end\":53059,\"start\":53058},{\"end\":53069,\"start\":53068},{\"end\":53081,\"start\":53080},{\"end\":53091,\"start\":53090},{\"end\":53104,\"start\":53103},{\"end\":53249,\"start\":53248},{\"end\":53372,\"start\":53371},{\"end\":53374,\"start\":53373},{\"end\":53389,\"start\":53388},{\"end\":53398,\"start\":53397},{\"end\":53414,\"start\":53410},{\"end\":53515,\"start\":53514},{\"end\":53517,\"start\":53516},{\"end\":53527,\"start\":53526},{\"end\":53536,\"start\":53535},{\"end\":53538,\"start\":53537},{\"end\":53549,\"start\":53548},{\"end\":53551,\"start\":53550},{\"end\":53637,\"start\":53636},{\"end\":53647,\"start\":53646},{\"end\":53662,\"start\":53661},{\"end\":53727,\"start\":53726},{\"end\":53737,\"start\":53733},{\"end\":53746,\"start\":53745},{\"end\":53758,\"start\":53757},{\"end\":53768,\"start\":53767},{\"end\":53899,\"start\":53898},{\"end\":53907,\"start\":53906},{\"end\":53909,\"start\":53908},{\"end\":53918,\"start\":53917},{\"end\":53920,\"start\":53919},{\"end\":53930,\"start\":53929},{\"end\":53932,\"start\":53931},{\"end\":54046,\"start\":54045},{\"end\":54054,\"start\":54053},{\"end\":54063,\"start\":54062},{\"end\":54071,\"start\":54070},{\"end\":54078,\"start\":54077},{\"end\":54085,\"start\":54084},{\"end\":54183,\"start\":54182},{\"end\":54189,\"start\":54188},{\"end\":54198,\"start\":54197},{\"end\":54205,\"start\":54204},{\"end\":54322,\"start\":54321},{\"end\":54337,\"start\":54336},{\"end\":54348,\"start\":54347},{\"end\":54537,\"start\":54533},{\"end\":54548,\"start\":54547},{\"end\":54563,\"start\":54562},{\"end\":54651,\"start\":54650},{\"end\":54661,\"start\":54660},{\"end\":54670,\"start\":54669},{\"end\":54682,\"start\":54681},{\"end\":54692,\"start\":54691},{\"end\":54700,\"start\":54699},{\"end\":54710,\"start\":54709},{\"end\":54717,\"start\":54716},{\"end\":54730,\"start\":54729},{\"end\":54740,\"start\":54739},{\"end\":54814,\"start\":54813},{\"end\":54822,\"start\":54821},{\"end\":54832,\"start\":54831},{\"end\":54847,\"start\":54846},{\"end\":54955,\"start\":54954},{\"end\":54957,\"start\":54956},{\"end\":54967,\"start\":54966},{\"end\":55061,\"start\":55060},{\"end\":55069,\"start\":55068},{\"end\":55077,\"start\":55076},{\"end\":55090,\"start\":55086},{\"end\":55096,\"start\":55095},{\"end\":55102,\"start\":55101},{\"end\":55250,\"start\":55249},{\"end\":55263,\"start\":55262},{\"end\":55273,\"start\":55272},{\"end\":55283,\"start\":55282},{\"end\":55294,\"start\":55293},{\"end\":55410,\"start\":55409},{\"end\":55419,\"start\":55418},{\"end\":55432,\"start\":55431},{\"end\":55442,\"start\":55441},{\"end\":55453,\"start\":55452},{\"end\":55623,\"start\":55622},{\"end\":55631,\"start\":55630},{\"end\":55641,\"start\":55640},{\"end\":55651,\"start\":55650},{\"end\":55799,\"start\":55798},{\"end\":55807,\"start\":55806},{\"end\":55815,\"start\":55814},{\"end\":55823,\"start\":55822},{\"end\":55831,\"start\":55830},{\"end\":55839,\"start\":55838},{\"end\":55928,\"start\":55927},{\"end\":55938,\"start\":55937},{\"end\":55940,\"start\":55939},{\"end\":55949,\"start\":55948},{\"end\":55951,\"start\":55950},{\"end\":56073,\"start\":56072},{\"end\":56083,\"start\":56082},{\"end\":56090,\"start\":56089},{\"end\":56092,\"start\":56091},{\"end\":56204,\"start\":56203},{\"end\":56211,\"start\":56210},{\"end\":56223,\"start\":56222},{\"end\":56309,\"start\":56308},{\"end\":56320,\"start\":56319},{\"end\":56327,\"start\":56326},{\"end\":56500,\"start\":56499},{\"end\":56506,\"start\":56505},{\"end\":56514,\"start\":56513},{\"end\":56521,\"start\":56520},{\"end\":56532,\"start\":56531},{\"end\":56650,\"start\":56649},{\"end\":56657,\"start\":56656},{\"end\":56782,\"start\":56781},{\"end\":56790,\"start\":56789},{\"end\":56798,\"start\":56797},{\"end\":56805,\"start\":56804},{\"end\":56814,\"start\":56813},{\"end\":56823,\"start\":56822},{\"end\":56825,\"start\":56824},{\"end\":56988,\"start\":56987},{\"end\":56997,\"start\":56996},{\"end\":57118,\"start\":57117},{\"end\":57133,\"start\":57132},{\"end\":57141,\"start\":57140},{\"end\":57283,\"start\":57282},{\"end\":57289,\"start\":57288},{\"end\":57298,\"start\":57297},{\"end\":57399,\"start\":57398},{\"end\":57405,\"start\":57404},{\"end\":57413,\"start\":57412},{\"end\":57421,\"start\":57420},{\"end\":57438,\"start\":57437},{\"end\":57562,\"start\":57561},{\"end\":57569,\"start\":57568},{\"end\":57576,\"start\":57575},{\"end\":57584,\"start\":57583},{\"end\":57699,\"start\":57698},{\"end\":57707,\"start\":57706},{\"end\":57715,\"start\":57714},{\"end\":57722,\"start\":57721},{\"end\":57834,\"start\":57833},{\"end\":57842,\"start\":57841},{\"end\":57849,\"start\":57848},{\"end\":57859,\"start\":57855},{\"end\":57999,\"start\":57998},{\"end\":58005,\"start\":58004},{\"end\":58012,\"start\":58011},{\"end\":58154,\"start\":58153},{\"end\":58160,\"start\":58159},{\"end\":58166,\"start\":58165},{\"end\":58175,\"start\":58174},{\"end\":58292,\"start\":58291},{\"end\":58303,\"start\":58302},{\"end\":58310,\"start\":58309},{\"end\":58319,\"start\":58318},{\"end\":58327,\"start\":58326}]", "bib_author_last_name": "[{\"end\":46798,\"start\":46794},{\"end\":46806,\"start\":46802},{\"end\":46814,\"start\":46810},{\"end\":46820,\"start\":46818},{\"end\":46829,\"start\":46824},{\"end\":46837,\"start\":46833},{\"end\":46849,\"start\":46844},{\"end\":46857,\"start\":46853},{\"end\":46968,\"start\":46963},{\"end\":46979,\"start\":46972},{\"end\":46989,\"start\":46983},{\"end\":47125,\"start\":47121},{\"end\":47136,\"start\":47132},{\"end\":47143,\"start\":47140},{\"end\":47223,\"start\":47219},{\"end\":47233,\"start\":47230},{\"end\":47245,\"start\":47240},{\"end\":47253,\"start\":47249},{\"end\":47261,\"start\":47257},{\"end\":47369,\"start\":47361},{\"end\":47378,\"start\":47373},{\"end\":47390,\"start\":47382},{\"end\":47403,\"start\":47394},{\"end\":47410,\"start\":47407},{\"end\":47423,\"start\":47416},{\"end\":47431,\"start\":47427},{\"end\":47442,\"start\":47435},{\"end\":47452,\"start\":47446},{\"end\":47466,\"start\":47456},{\"end\":47596,\"start\":47592},{\"end\":47604,\"start\":47600},{\"end\":47612,\"start\":47608},{\"end\":47721,\"start\":47717},{\"end\":47738,\"start\":47727},{\"end\":47746,\"start\":47742},{\"end\":47754,\"start\":47750},{\"end\":47881,\"start\":47878},{\"end\":47889,\"start\":47885},{\"end\":47896,\"start\":47893},{\"end\":47904,\"start\":47900},{\"end\":48037,\"start\":48033},{\"end\":48045,\"start\":48043},{\"end\":48057,\"start\":48049},{\"end\":48065,\"start\":48061},{\"end\":48169,\"start\":48165},{\"end\":48178,\"start\":48173},{\"end\":48189,\"start\":48182},{\"end\":48199,\"start\":48195},{\"end\":48296,\"start\":48290},{\"end\":48304,\"start\":48300},{\"end\":48315,\"start\":48308},{\"end\":48326,\"start\":48319},{\"end\":48456,\"start\":48447},{\"end\":48465,\"start\":48460},{\"end\":48474,\"start\":48469},{\"end\":48484,\"start\":48478},{\"end\":48581,\"start\":48575},{\"end\":48594,\"start\":48585},{\"end\":48604,\"start\":48598},{\"end\":48617,\"start\":48610},{\"end\":48719,\"start\":48710},{\"end\":48729,\"start\":48723},{\"end\":48739,\"start\":48733},{\"end\":48751,\"start\":48743},{\"end\":48761,\"start\":48755},{\"end\":48924,\"start\":48918},{\"end\":48932,\"start\":48928},{\"end\":48946,\"start\":48936},{\"end\":48958,\"start\":48950},{\"end\":49071,\"start\":49065},{\"end\":49077,\"start\":49075},{\"end\":49093,\"start\":49081},{\"end\":49105,\"start\":49097},{\"end\":49228,\"start\":49219},{\"end\":49235,\"start\":49232},{\"end\":49241,\"start\":49239},{\"end\":49251,\"start\":49245},{\"end\":49261,\"start\":49255},{\"end\":49408,\"start\":49400},{\"end\":49424,\"start\":49415},{\"end\":49439,\"start\":49428},{\"end\":49454,\"start\":49443},{\"end\":49575,\"start\":49572},{\"end\":49582,\"start\":49579},{\"end\":49718,\"start\":49715},{\"end\":49725,\"start\":49722},{\"end\":49737,\"start\":49732},{\"end\":49895,\"start\":49889},{\"end\":49906,\"start\":49899},{\"end\":49913,\"start\":49910},{\"end\":49920,\"start\":49917},{\"end\":49929,\"start\":49924},{\"end\":49940,\"start\":49935},{\"end\":50101,\"start\":50097},{\"end\":50111,\"start\":50105},{\"end\":50127,\"start\":50115},{\"end\":50250,\"start\":50247},{\"end\":50256,\"start\":50254},{\"end\":50263,\"start\":50260},{\"end\":50274,\"start\":50269},{\"end\":50466,\"start\":50464},{\"end\":50476,\"start\":50470},{\"end\":50484,\"start\":50480},{\"end\":50494,\"start\":50488},{\"end\":50506,\"start\":50498},{\"end\":50605,\"start\":50602},{\"end\":50614,\"start\":50609},{\"end\":50622,\"start\":50618},{\"end\":50633,\"start\":50626},{\"end\":50640,\"start\":50637},{\"end\":50648,\"start\":50644},{\"end\":50656,\"start\":50652},{\"end\":50752,\"start\":50748},{\"end\":50758,\"start\":50756},{\"end\":50766,\"start\":50762},{\"end\":50774,\"start\":50770},{\"end\":50894,\"start\":50888},{\"end\":50907,\"start\":50902},{\"end\":50920,\"start\":50913},{\"end\":51079,\"start\":51075},{\"end\":51087,\"start\":51083},{\"end\":51108,\"start\":51091},{\"end\":51114,\"start\":51112},{\"end\":51125,\"start\":51118},{\"end\":51133,\"start\":51129},{\"end\":51291,\"start\":51281},{\"end\":51300,\"start\":51295},{\"end\":51312,\"start\":51304},{\"end\":51430,\"start\":51426},{\"end\":51438,\"start\":51434},{\"end\":51446,\"start\":51442},{\"end\":51457,\"start\":51453},{\"end\":51465,\"start\":51461},{\"end\":51473,\"start\":51469},{\"end\":51595,\"start\":51591},{\"end\":51601,\"start\":51599},{\"end\":51608,\"start\":51605},{\"end\":51727,\"start\":51725},{\"end\":51735,\"start\":51731},{\"end\":51743,\"start\":51739},{\"end\":51750,\"start\":51747},{\"end\":51758,\"start\":51754},{\"end\":51766,\"start\":51762},{\"end\":51772,\"start\":51770},{\"end\":51959,\"start\":51946},{\"end\":51968,\"start\":51965},{\"end\":51975,\"start\":51972},{\"end\":52102,\"start\":52098},{\"end\":52110,\"start\":52106},{\"end\":52117,\"start\":52114},{\"end\":52123,\"start\":52121},{\"end\":52131,\"start\":52127},{\"end\":52137,\"start\":52135},{\"end\":52144,\"start\":52141},{\"end\":52266,\"start\":52264},{\"end\":52277,\"start\":52270},{\"end\":52392,\"start\":52390},{\"end\":52401,\"start\":52396},{\"end\":52409,\"start\":52405},{\"end\":52419,\"start\":52413},{\"end\":52430,\"start\":52423},{\"end\":52437,\"start\":52434},{\"end\":52450,\"start\":52443},{\"end\":52574,\"start\":52570},{\"end\":52583,\"start\":52578},{\"end\":52594,\"start\":52587},{\"end\":52602,\"start\":52598},{\"end\":52740,\"start\":52736},{\"end\":52748,\"start\":52744},{\"end\":52756,\"start\":52752},{\"end\":52864,\"start\":52861},{\"end\":52873,\"start\":52868},{\"end\":52881,\"start\":52877},{\"end\":52892,\"start\":52885},{\"end\":52899,\"start\":52896},{\"end\":52907,\"start\":52903},{\"end\":52915,\"start\":52911},{\"end\":53066,\"start\":53060},{\"end\":53078,\"start\":53070},{\"end\":53088,\"start\":53082},{\"end\":53101,\"start\":53092},{\"end\":53111,\"start\":53105},{\"end\":53262,\"start\":53250},{\"end\":53386,\"start\":53375},{\"end\":53395,\"start\":53390},{\"end\":53408,\"start\":53399},{\"end\":53420,\"start\":53415},{\"end\":53524,\"start\":53518},{\"end\":53533,\"start\":53528},{\"end\":53546,\"start\":53539},{\"end\":53557,\"start\":53552},{\"end\":53644,\"start\":53638},{\"end\":53659,\"start\":53648},{\"end\":53669,\"start\":53663},{\"end\":53731,\"start\":53728},{\"end\":53743,\"start\":53738},{\"end\":53755,\"start\":53747},{\"end\":53765,\"start\":53759},{\"end\":53773,\"start\":53769},{\"end\":53904,\"start\":53900},{\"end\":53915,\"start\":53910},{\"end\":53927,\"start\":53921},{\"end\":53943,\"start\":53933},{\"end\":54051,\"start\":54047},{\"end\":54060,\"start\":54055},{\"end\":54068,\"start\":54064},{\"end\":54075,\"start\":54072},{\"end\":54082,\"start\":54079},{\"end\":54089,\"start\":54086},{\"end\":54186,\"start\":54184},{\"end\":54195,\"start\":54190},{\"end\":54202,\"start\":54199},{\"end\":54209,\"start\":54206},{\"end\":54334,\"start\":54323},{\"end\":54345,\"start\":54338},{\"end\":54353,\"start\":54349},{\"end\":54545,\"start\":54538},{\"end\":54560,\"start\":54549},{\"end\":54574,\"start\":54564},{\"end\":54658,\"start\":54652},{\"end\":54667,\"start\":54662},{\"end\":54679,\"start\":54671},{\"end\":54689,\"start\":54683},{\"end\":54697,\"start\":54693},{\"end\":54707,\"start\":54701},{\"end\":54714,\"start\":54711},{\"end\":54727,\"start\":54718},{\"end\":54737,\"start\":54731},{\"end\":54746,\"start\":54741},{\"end\":54819,\"start\":54815},{\"end\":54829,\"start\":54823},{\"end\":54844,\"start\":54833},{\"end\":54851,\"start\":54848},{\"end\":54858,\"start\":54853},{\"end\":54964,\"start\":54958},{\"end\":54970,\"start\":54968},{\"end\":55066,\"start\":55062},{\"end\":55074,\"start\":55070},{\"end\":55084,\"start\":55078},{\"end\":55093,\"start\":55091},{\"end\":55099,\"start\":55097},{\"end\":55110,\"start\":55103},{\"end\":55260,\"start\":55251},{\"end\":55270,\"start\":55264},{\"end\":55280,\"start\":55274},{\"end\":55291,\"start\":55284},{\"end\":55304,\"start\":55295},{\"end\":55416,\"start\":55411},{\"end\":55429,\"start\":55420},{\"end\":55439,\"start\":55433},{\"end\":55450,\"start\":55443},{\"end\":55461,\"start\":55454},{\"end\":55628,\"start\":55624},{\"end\":55638,\"start\":55632},{\"end\":55648,\"start\":55642},{\"end\":55663,\"start\":55652},{\"end\":55804,\"start\":55800},{\"end\":55812,\"start\":55808},{\"end\":55820,\"start\":55816},{\"end\":55828,\"start\":55824},{\"end\":55836,\"start\":55832},{\"end\":55842,\"start\":55840},{\"end\":55935,\"start\":55929},{\"end\":55946,\"start\":55941},{\"end\":55954,\"start\":55952},{\"end\":56080,\"start\":56074},{\"end\":56087,\"start\":56084},{\"end\":56097,\"start\":56093},{\"end\":56208,\"start\":56205},{\"end\":56220,\"start\":56212},{\"end\":56226,\"start\":56224},{\"end\":56317,\"start\":56310},{\"end\":56324,\"start\":56321},{\"end\":56337,\"start\":56328},{\"end\":56503,\"start\":56501},{\"end\":56511,\"start\":56507},{\"end\":56518,\"start\":56515},{\"end\":56529,\"start\":56522},{\"end\":56539,\"start\":56533},{\"end\":56543,\"start\":56541},{\"end\":56654,\"start\":56651},{\"end\":56667,\"start\":56658},{\"end\":56787,\"start\":56783},{\"end\":56795,\"start\":56791},{\"end\":56802,\"start\":56799},{\"end\":56811,\"start\":56806},{\"end\":56820,\"start\":56815},{\"end\":56832,\"start\":56826},{\"end\":56994,\"start\":56989},{\"end\":57004,\"start\":56998},{\"end\":57130,\"start\":57119},{\"end\":57138,\"start\":57134},{\"end\":57155,\"start\":57142},{\"end\":57286,\"start\":57284},{\"end\":57295,\"start\":57290},{\"end\":57302,\"start\":57299},{\"end\":57402,\"start\":57400},{\"end\":57410,\"start\":57406},{\"end\":57418,\"start\":57414},{\"end\":57435,\"start\":57422},{\"end\":57442,\"start\":57439},{\"end\":57566,\"start\":57563},{\"end\":57573,\"start\":57570},{\"end\":57581,\"start\":57577},{\"end\":57588,\"start\":57585},{\"end\":57704,\"start\":57700},{\"end\":57712,\"start\":57708},{\"end\":57719,\"start\":57716},{\"end\":57727,\"start\":57723},{\"end\":57839,\"start\":57835},{\"end\":57846,\"start\":57843},{\"end\":57853,\"start\":57850},{\"end\":57863,\"start\":57860},{\"end\":58002,\"start\":58000},{\"end\":58009,\"start\":58006},{\"end\":58016,\"start\":58013},{\"end\":58157,\"start\":58155},{\"end\":58163,\"start\":58161},{\"end\":58172,\"start\":58167},{\"end\":58178,\"start\":58176},{\"end\":58300,\"start\":58293},{\"end\":58307,\"start\":58304},{\"end\":58316,\"start\":58311},{\"end\":58324,\"start\":58320},{\"end\":58334,\"start\":58328}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":235186925},\"end\":46884,\"start\":46732},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2255738},\"end\":47030,\"start\":46886},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7240840},\"end\":47173,\"start\":47032},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":220936082},\"end\":47295,\"start\":47175},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11830123},\"end\":47537,\"start\":47297},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":245803151},\"end\":47663,\"start\":47539},{\"attributes\":{\"doi\":\"arXiv:1909.09803\",\"id\":\"b6\"},\"end\":47790,\"start\":47665},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15774646},\"end\":47954,\"start\":47792},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":299085},\"end\":48103,\"start\":47956},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11977588},\"end\":48244,\"start\":48105},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9455111},\"end\":48383,\"start\":48246},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":545361},\"end\":48514,\"start\":48385},{\"attributes\":{\"id\":\"b12\"},\"end\":48647,\"start\":48516},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":146808364},\"end\":48806,\"start\":48649},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":53437459},\"end\":48970,\"start\":48808},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":131774103},\"end\":49135,\"start\":48972},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":211532626},\"end\":49296,\"start\":49137},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":220514784},\"end\":49492,\"start\":49298},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3714620},\"end\":49627,\"start\":49494},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52158437},\"end\":49767,\"start\":49629},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52936213},\"end\":49985,\"start\":49769},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":196470796},\"end\":50157,\"start\":49987},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":231802071},\"end\":50403,\"start\":50159},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221698979},\"end\":50542,\"start\":50405},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":229298063},\"end\":50701,\"start\":50544},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14395783},\"end\":50815,\"start\":50703},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206596513},\"end\":50965,\"start\":50817},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4578162},\"end\":51178,\"start\":50967},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3645349},\"end\":51357,\"start\":51180},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":245131636},\"end\":51519,\"start\":51359},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":257687910},\"end\":51663,\"start\":51521},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":257632171},\"end\":51817,\"start\":51665},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":258041260},\"end\":52024,\"start\":51819},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":52860134},\"end\":52189,\"start\":52026},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4572038},\"end\":52322,\"start\":52191},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":131775632},\"end\":52495,\"start\":52324},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":131775376},\"end\":52657,\"start\":52497},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":49416362},\"end\":52801,\"start\":52659},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":229298063},\"end\":52960,\"start\":52803},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":195776274},\"end\":53157,\"start\":52962},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":2850611},\"end\":53308,\"start\":53159},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":977535},\"end\":53450,\"start\":53310},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4637111},\"end\":53592,\"start\":53452},{\"attributes\":{\"id\":\"b43\"},\"end\":53689,\"start\":53594},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":216915079},\"end\":53822,\"start\":53691},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":207761262},\"end\":53979,\"start\":53824},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":219633501},\"end\":54134,\"start\":53981},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":206594692},\"end\":54254,\"start\":54136},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3719281},\"end\":54455,\"start\":54256},{\"attributes\":{\"doi\":\"arXiv:1511.07289\",\"id\":\"b49\"},\"end\":54610,\"start\":54457},{\"attributes\":{\"id\":\"b50\"},\"end\":54752,\"start\":54612},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":21352010},\"end\":54908,\"start\":54754},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b52\"},\"end\":55005,\"start\":54910},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":57246310},\"end\":55155,\"start\":55007},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":146120505},\"end\":55353,\"start\":55157},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":206942855},\"end\":55510,\"start\":55355},{\"attributes\":{\"id\":\"b56\"},\"end\":55708,\"start\":55512},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":247778788},\"end\":55880,\"start\":55710},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":10748875},\"end\":55995,\"start\":55882},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":12230426},\"end\":56143,\"start\":55997},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":6340621},\"end\":56271,\"start\":56145},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":11106957},\"end\":56382,\"start\":56273},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":206592782},\"end\":56588,\"start\":56384},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":2832799},\"end\":56712,\"start\":56590},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":5979036},\"end\":56877,\"start\":56714},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":102496818},\"end\":57034,\"start\":56879},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":2198511},\"end\":57196,\"start\":57036},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":11578674},\"end\":57332,\"start\":57198},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":46968214},\"end\":57487,\"start\":57334},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":198968133},\"end\":57618,\"start\":57489},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":202414590},\"end\":57757,\"start\":57620},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":214794898},\"end\":57908,\"start\":57759},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":220525434},\"end\":58046,\"start\":57910},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":236447392},\"end\":58223,\"start\":58048},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":215541815},\"end\":58379,\"start\":58225}]", "bib_title": "[{\"end\":46787,\"start\":46732},{\"end\":46959,\"start\":46886},{\"end\":47114,\"start\":47032},{\"end\":47215,\"start\":47175},{\"end\":47355,\"start\":47297},{\"end\":47585,\"start\":47539},{\"end\":47874,\"start\":47792},{\"end\":48029,\"start\":47956},{\"end\":48161,\"start\":48105},{\"end\":48286,\"start\":48246},{\"end\":48443,\"start\":48385},{\"end\":48571,\"start\":48516},{\"end\":48706,\"start\":48649},{\"end\":48914,\"start\":48808},{\"end\":49061,\"start\":48972},{\"end\":49215,\"start\":49137},{\"end\":49396,\"start\":49298},{\"end\":49568,\"start\":49494},{\"end\":49711,\"start\":49629},{\"end\":49885,\"start\":49769},{\"end\":50093,\"start\":49987},{\"end\":50243,\"start\":50159},{\"end\":50460,\"start\":50405},{\"end\":50598,\"start\":50544},{\"end\":50744,\"start\":50703},{\"end\":50884,\"start\":50817},{\"end\":51071,\"start\":50967},{\"end\":51277,\"start\":51180},{\"end\":51419,\"start\":51359},{\"end\":51587,\"start\":51521},{\"end\":51721,\"start\":51665},{\"end\":51942,\"start\":51819},{\"end\":52094,\"start\":52026},{\"end\":52260,\"start\":52191},{\"end\":52386,\"start\":52324},{\"end\":52566,\"start\":52497},{\"end\":52732,\"start\":52659},{\"end\":52857,\"start\":52803},{\"end\":53056,\"start\":52962},{\"end\":53246,\"start\":53159},{\"end\":53369,\"start\":53310},{\"end\":53512,\"start\":53452},{\"end\":53724,\"start\":53691},{\"end\":53896,\"start\":53824},{\"end\":54043,\"start\":53981},{\"end\":54180,\"start\":54136},{\"end\":54319,\"start\":54256},{\"end\":54811,\"start\":54754},{\"end\":55058,\"start\":55007},{\"end\":55247,\"start\":55157},{\"end\":55407,\"start\":55355},{\"end\":55796,\"start\":55710},{\"end\":55925,\"start\":55882},{\"end\":56070,\"start\":55997},{\"end\":56201,\"start\":56145},{\"end\":56306,\"start\":56273},{\"end\":56497,\"start\":56384},{\"end\":56647,\"start\":56590},{\"end\":56779,\"start\":56714},{\"end\":56985,\"start\":56879},{\"end\":57115,\"start\":57036},{\"end\":57280,\"start\":57198},{\"end\":57396,\"start\":57334},{\"end\":57559,\"start\":57489},{\"end\":57696,\"start\":57620},{\"end\":57831,\"start\":57759},{\"end\":57996,\"start\":57910},{\"end\":58151,\"start\":58048},{\"end\":58289,\"start\":58225}]", "bib_author": "[{\"end\":46800,\"start\":46789},{\"end\":46808,\"start\":46800},{\"end\":46816,\"start\":46808},{\"end\":46822,\"start\":46816},{\"end\":46831,\"start\":46822},{\"end\":46839,\"start\":46831},{\"end\":46851,\"start\":46839},{\"end\":46859,\"start\":46851},{\"end\":46970,\"start\":46961},{\"end\":46981,\"start\":46970},{\"end\":46991,\"start\":46981},{\"end\":47127,\"start\":47116},{\"end\":47138,\"start\":47127},{\"end\":47145,\"start\":47138},{\"end\":47225,\"start\":47217},{\"end\":47235,\"start\":47225},{\"end\":47247,\"start\":47235},{\"end\":47255,\"start\":47247},{\"end\":47263,\"start\":47255},{\"end\":47371,\"start\":47357},{\"end\":47380,\"start\":47371},{\"end\":47392,\"start\":47380},{\"end\":47405,\"start\":47392},{\"end\":47412,\"start\":47405},{\"end\":47425,\"start\":47412},{\"end\":47433,\"start\":47425},{\"end\":47444,\"start\":47433},{\"end\":47454,\"start\":47444},{\"end\":47468,\"start\":47454},{\"end\":47598,\"start\":47587},{\"end\":47606,\"start\":47598},{\"end\":47614,\"start\":47606},{\"end\":47723,\"start\":47715},{\"end\":47740,\"start\":47723},{\"end\":47748,\"start\":47740},{\"end\":47756,\"start\":47748},{\"end\":47883,\"start\":47876},{\"end\":47891,\"start\":47883},{\"end\":47898,\"start\":47891},{\"end\":47906,\"start\":47898},{\"end\":48039,\"start\":48031},{\"end\":48047,\"start\":48039},{\"end\":48059,\"start\":48047},{\"end\":48067,\"start\":48059},{\"end\":48171,\"start\":48163},{\"end\":48180,\"start\":48171},{\"end\":48191,\"start\":48180},{\"end\":48201,\"start\":48191},{\"end\":48298,\"start\":48288},{\"end\":48306,\"start\":48298},{\"end\":48317,\"start\":48306},{\"end\":48328,\"start\":48317},{\"end\":48458,\"start\":48445},{\"end\":48467,\"start\":48458},{\"end\":48476,\"start\":48467},{\"end\":48486,\"start\":48476},{\"end\":48583,\"start\":48573},{\"end\":48596,\"start\":48583},{\"end\":48606,\"start\":48596},{\"end\":48619,\"start\":48606},{\"end\":48721,\"start\":48708},{\"end\":48731,\"start\":48721},{\"end\":48741,\"start\":48731},{\"end\":48753,\"start\":48741},{\"end\":48763,\"start\":48753},{\"end\":48926,\"start\":48916},{\"end\":48934,\"start\":48926},{\"end\":48948,\"start\":48934},{\"end\":48960,\"start\":48948},{\"end\":49073,\"start\":49063},{\"end\":49079,\"start\":49073},{\"end\":49095,\"start\":49079},{\"end\":49107,\"start\":49095},{\"end\":49230,\"start\":49217},{\"end\":49237,\"start\":49230},{\"end\":49243,\"start\":49237},{\"end\":49253,\"start\":49243},{\"end\":49263,\"start\":49253},{\"end\":49410,\"start\":49398},{\"end\":49426,\"start\":49410},{\"end\":49441,\"start\":49426},{\"end\":49456,\"start\":49441},{\"end\":49577,\"start\":49570},{\"end\":49584,\"start\":49577},{\"end\":49720,\"start\":49713},{\"end\":49727,\"start\":49720},{\"end\":49739,\"start\":49727},{\"end\":49897,\"start\":49887},{\"end\":49908,\"start\":49897},{\"end\":49915,\"start\":49908},{\"end\":49922,\"start\":49915},{\"end\":49931,\"start\":49922},{\"end\":49942,\"start\":49931},{\"end\":50103,\"start\":50095},{\"end\":50113,\"start\":50103},{\"end\":50129,\"start\":50113},{\"end\":50252,\"start\":50245},{\"end\":50258,\"start\":50252},{\"end\":50265,\"start\":50258},{\"end\":50276,\"start\":50265},{\"end\":50468,\"start\":50462},{\"end\":50478,\"start\":50468},{\"end\":50486,\"start\":50478},{\"end\":50496,\"start\":50486},{\"end\":50508,\"start\":50496},{\"end\":50607,\"start\":50600},{\"end\":50616,\"start\":50607},{\"end\":50624,\"start\":50616},{\"end\":50635,\"start\":50624},{\"end\":50642,\"start\":50635},{\"end\":50650,\"start\":50642},{\"end\":50658,\"start\":50650},{\"end\":50754,\"start\":50746},{\"end\":50760,\"start\":50754},{\"end\":50768,\"start\":50760},{\"end\":50776,\"start\":50768},{\"end\":50896,\"start\":50886},{\"end\":50909,\"start\":50896},{\"end\":50922,\"start\":50909},{\"end\":51081,\"start\":51073},{\"end\":51089,\"start\":51081},{\"end\":51110,\"start\":51089},{\"end\":51116,\"start\":51110},{\"end\":51127,\"start\":51116},{\"end\":51135,\"start\":51127},{\"end\":51293,\"start\":51279},{\"end\":51302,\"start\":51293},{\"end\":51314,\"start\":51302},{\"end\":51432,\"start\":51421},{\"end\":51440,\"start\":51432},{\"end\":51448,\"start\":51440},{\"end\":51459,\"start\":51448},{\"end\":51467,\"start\":51459},{\"end\":51475,\"start\":51467},{\"end\":51597,\"start\":51589},{\"end\":51603,\"start\":51597},{\"end\":51610,\"start\":51603},{\"end\":51729,\"start\":51723},{\"end\":51737,\"start\":51729},{\"end\":51745,\"start\":51737},{\"end\":51752,\"start\":51745},{\"end\":51760,\"start\":51752},{\"end\":51768,\"start\":51760},{\"end\":51774,\"start\":51768},{\"end\":51961,\"start\":51944},{\"end\":51970,\"start\":51961},{\"end\":51977,\"start\":51970},{\"end\":52104,\"start\":52096},{\"end\":52112,\"start\":52104},{\"end\":52119,\"start\":52112},{\"end\":52125,\"start\":52119},{\"end\":52133,\"start\":52125},{\"end\":52139,\"start\":52133},{\"end\":52146,\"start\":52139},{\"end\":52268,\"start\":52262},{\"end\":52279,\"start\":52268},{\"end\":52394,\"start\":52388},{\"end\":52403,\"start\":52394},{\"end\":52411,\"start\":52403},{\"end\":52421,\"start\":52411},{\"end\":52432,\"start\":52421},{\"end\":52439,\"start\":52432},{\"end\":52452,\"start\":52439},{\"end\":52576,\"start\":52568},{\"end\":52585,\"start\":52576},{\"end\":52596,\"start\":52585},{\"end\":52604,\"start\":52596},{\"end\":52742,\"start\":52734},{\"end\":52750,\"start\":52742},{\"end\":52758,\"start\":52750},{\"end\":52866,\"start\":52859},{\"end\":52875,\"start\":52866},{\"end\":52883,\"start\":52875},{\"end\":52894,\"start\":52883},{\"end\":52901,\"start\":52894},{\"end\":52909,\"start\":52901},{\"end\":52917,\"start\":52909},{\"end\":53068,\"start\":53058},{\"end\":53080,\"start\":53068},{\"end\":53090,\"start\":53080},{\"end\":53103,\"start\":53090},{\"end\":53113,\"start\":53103},{\"end\":53264,\"start\":53248},{\"end\":53388,\"start\":53371},{\"end\":53397,\"start\":53388},{\"end\":53410,\"start\":53397},{\"end\":53422,\"start\":53410},{\"end\":53526,\"start\":53514},{\"end\":53535,\"start\":53526},{\"end\":53548,\"start\":53535},{\"end\":53559,\"start\":53548},{\"end\":53646,\"start\":53636},{\"end\":53661,\"start\":53646},{\"end\":53671,\"start\":53661},{\"end\":53733,\"start\":53726},{\"end\":53745,\"start\":53733},{\"end\":53757,\"start\":53745},{\"end\":53767,\"start\":53757},{\"end\":53775,\"start\":53767},{\"end\":53906,\"start\":53898},{\"end\":53917,\"start\":53906},{\"end\":53929,\"start\":53917},{\"end\":53945,\"start\":53929},{\"end\":54053,\"start\":54045},{\"end\":54062,\"start\":54053},{\"end\":54070,\"start\":54062},{\"end\":54077,\"start\":54070},{\"end\":54084,\"start\":54077},{\"end\":54091,\"start\":54084},{\"end\":54188,\"start\":54182},{\"end\":54197,\"start\":54188},{\"end\":54204,\"start\":54197},{\"end\":54211,\"start\":54204},{\"end\":54336,\"start\":54321},{\"end\":54347,\"start\":54336},{\"end\":54355,\"start\":54347},{\"end\":54547,\"start\":54533},{\"end\":54562,\"start\":54547},{\"end\":54576,\"start\":54562},{\"end\":54660,\"start\":54650},{\"end\":54669,\"start\":54660},{\"end\":54681,\"start\":54669},{\"end\":54691,\"start\":54681},{\"end\":54699,\"start\":54691},{\"end\":54709,\"start\":54699},{\"end\":54716,\"start\":54709},{\"end\":54729,\"start\":54716},{\"end\":54739,\"start\":54729},{\"end\":54748,\"start\":54739},{\"end\":54821,\"start\":54813},{\"end\":54831,\"start\":54821},{\"end\":54846,\"start\":54831},{\"end\":54853,\"start\":54846},{\"end\":54860,\"start\":54853},{\"end\":54966,\"start\":54954},{\"end\":54972,\"start\":54966},{\"end\":55068,\"start\":55060},{\"end\":55076,\"start\":55068},{\"end\":55086,\"start\":55076},{\"end\":55095,\"start\":55086},{\"end\":55101,\"start\":55095},{\"end\":55112,\"start\":55101},{\"end\":55262,\"start\":55249},{\"end\":55272,\"start\":55262},{\"end\":55282,\"start\":55272},{\"end\":55293,\"start\":55282},{\"end\":55306,\"start\":55293},{\"end\":55418,\"start\":55409},{\"end\":55431,\"start\":55418},{\"end\":55441,\"start\":55431},{\"end\":55452,\"start\":55441},{\"end\":55463,\"start\":55452},{\"end\":55630,\"start\":55622},{\"end\":55640,\"start\":55630},{\"end\":55650,\"start\":55640},{\"end\":55665,\"start\":55650},{\"end\":55806,\"start\":55798},{\"end\":55814,\"start\":55806},{\"end\":55822,\"start\":55814},{\"end\":55830,\"start\":55822},{\"end\":55838,\"start\":55830},{\"end\":55844,\"start\":55838},{\"end\":55937,\"start\":55927},{\"end\":55948,\"start\":55937},{\"end\":55956,\"start\":55948},{\"end\":56082,\"start\":56072},{\"end\":56089,\"start\":56082},{\"end\":56099,\"start\":56089},{\"end\":56210,\"start\":56203},{\"end\":56222,\"start\":56210},{\"end\":56228,\"start\":56222},{\"end\":56319,\"start\":56308},{\"end\":56326,\"start\":56319},{\"end\":56339,\"start\":56326},{\"end\":56505,\"start\":56499},{\"end\":56513,\"start\":56505},{\"end\":56520,\"start\":56513},{\"end\":56531,\"start\":56520},{\"end\":56541,\"start\":56531},{\"end\":56545,\"start\":56541},{\"end\":56656,\"start\":56649},{\"end\":56669,\"start\":56656},{\"end\":56789,\"start\":56781},{\"end\":56797,\"start\":56789},{\"end\":56804,\"start\":56797},{\"end\":56813,\"start\":56804},{\"end\":56822,\"start\":56813},{\"end\":56834,\"start\":56822},{\"end\":56996,\"start\":56987},{\"end\":57006,\"start\":56996},{\"end\":57132,\"start\":57117},{\"end\":57140,\"start\":57132},{\"end\":57157,\"start\":57140},{\"end\":57288,\"start\":57282},{\"end\":57297,\"start\":57288},{\"end\":57304,\"start\":57297},{\"end\":57404,\"start\":57398},{\"end\":57412,\"start\":57404},{\"end\":57420,\"start\":57412},{\"end\":57437,\"start\":57420},{\"end\":57444,\"start\":57437},{\"end\":57568,\"start\":57561},{\"end\":57575,\"start\":57568},{\"end\":57583,\"start\":57575},{\"end\":57590,\"start\":57583},{\"end\":57706,\"start\":57698},{\"end\":57714,\"start\":57706},{\"end\":57721,\"start\":57714},{\"end\":57729,\"start\":57721},{\"end\":57841,\"start\":57833},{\"end\":57848,\"start\":57841},{\"end\":57855,\"start\":57848},{\"end\":57865,\"start\":57855},{\"end\":58004,\"start\":57998},{\"end\":58011,\"start\":58004},{\"end\":58018,\"start\":58011},{\"end\":58159,\"start\":58153},{\"end\":58165,\"start\":58159},{\"end\":58174,\"start\":58165},{\"end\":58180,\"start\":58174},{\"end\":58302,\"start\":58291},{\"end\":58309,\"start\":58302},{\"end\":58318,\"start\":58309},{\"end\":58326,\"start\":58318},{\"end\":58336,\"start\":58326}]", "bib_venue": "[{\"end\":46878,\"start\":46859},{\"end\":47024,\"start\":46991},{\"end\":47167,\"start\":47145},{\"end\":47289,\"start\":47263},{\"end\":47527,\"start\":47468},{\"end\":47657,\"start\":47614},{\"end\":47713,\"start\":47665},{\"end\":47944,\"start\":47906},{\"end\":48089,\"start\":48067},{\"end\":48238,\"start\":48201},{\"end\":48377,\"start\":48328},{\"end\":48508,\"start\":48486},{\"end\":48641,\"start\":48619},{\"end\":48800,\"start\":48763},{\"end\":48964,\"start\":48960},{\"end\":49129,\"start\":49107},{\"end\":49290,\"start\":49263},{\"end\":49478,\"start\":49456},{\"end\":49621,\"start\":49584},{\"end\":49761,\"start\":49739},{\"end\":49979,\"start\":49942},{\"end\":50151,\"start\":50129},{\"end\":50344,\"start\":50276},{\"end\":50536,\"start\":50508},{\"end\":50695,\"start\":50658},{\"end\":50809,\"start\":50776},{\"end\":50959,\"start\":50922},{\"end\":51172,\"start\":51135},{\"end\":51351,\"start\":51314},{\"end\":51513,\"start\":51475},{\"end\":51647,\"start\":51610},{\"end\":51811,\"start\":51774},{\"end\":52014,\"start\":51977},{\"end\":52183,\"start\":52146},{\"end\":52316,\"start\":52279},{\"end\":52489,\"start\":52452},{\"end\":52647,\"start\":52604},{\"end\":52795,\"start\":52758},{\"end\":52954,\"start\":52917},{\"end\":53151,\"start\":53113},{\"end\":53301,\"start\":53264},{\"end\":53444,\"start\":53422},{\"end\":53581,\"start\":53559},{\"end\":53634,\"start\":53594},{\"end\":53803,\"start\":53775},{\"end\":53970,\"start\":53945},{\"end\":54128,\"start\":54091},{\"end\":54248,\"start\":54211},{\"end\":54441,\"start\":54355},{\"end\":54531,\"start\":54457},{\"end\":54648,\"start\":54612},{\"end\":54897,\"start\":54860},{\"end\":54952,\"start\":54910},{\"end\":55149,\"start\":55112},{\"end\":55347,\"start\":55306},{\"end\":55504,\"start\":55463},{\"end\":55620,\"start\":55512},{\"end\":55866,\"start\":55844},{\"end\":55989,\"start\":55956},{\"end\":56137,\"start\":56099},{\"end\":56265,\"start\":56228},{\"end\":56376,\"start\":56339},{\"end\":56582,\"start\":56545},{\"end\":56706,\"start\":56669},{\"end\":56871,\"start\":56834},{\"end\":57028,\"start\":57006},{\"end\":57190,\"start\":57157},{\"end\":57326,\"start\":57304},{\"end\":57481,\"start\":57444},{\"end\":57612,\"start\":57590},{\"end\":57751,\"start\":57729},{\"end\":57902,\"start\":57865},{\"end\":58040,\"start\":58018},{\"end\":58217,\"start\":58180},{\"end\":58373,\"start\":58336},{\"end\":50399,\"start\":50346},{\"end\":53818,\"start\":53805}]"}}}, "year": 2023, "month": 12, "day": 17}