{"id": 3625590, "updated": "2023-11-11 04:08:55.785", "metadata": {"title": "SCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial Network", "authors": "[{\"first\":\"Jian\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yuxin\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Mingkuan\",\"last\":\"Yuan\",\"middle\":[]}]", "venue": "IEEE transactions on cybernetics", "journal": "IEEE transactions on cybernetics", "publication_date": {"year": 2018, "month": 2, "day": 7}, "abstract": "Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they mainly have two limitations: (1) Heavily rely on large-scale labeled cross-modal training data which are labor intensive and hard to obtain. (2) Ignore the rich information contained in the large amount of unlabeled data across different modalities, especially the margin examples that are easily to be incorrectly retrieved, which can help to model the correlations. To address these problems, in this paper we propose a novel Semi-supervised Cross-Modal Hashing approach by Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN's ability for modeling data distributions to promote cross-modal hashing learning in an adversarial way. The main contributions can be summarized as follows: (1) We propose a novel generative adversarial network for cross-modal hashing. In our proposed SCH-GAN, the generative model tries to select margin examples of one modality from unlabeled data when giving a query of another modality. While the discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of discriminative model. (2) We propose a reinforcement learning based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model by maximizing the margin between positive and negative data. Experiments on 3 widely-used datasets verify the effectiveness of our proposed approach.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2962805368", "acl": null, "pubmed": "30273169", "pubmedcentral": null, "dblp": "journals/tcyb/ZhangPY20", "doi": "10.1109/tcyb.2018.2868826"}}, "content": {"source": {"pdf_hash": "9018d664a9b3909cfad55b1618a7b8fece5cb3e1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1802.02488v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1802.02488", "status": "GREEN"}}, "grobid": {"id": "b8d076efde0114b693bdc4201d2c6246e0bd65ac", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9018d664a9b3909cfad55b1618a7b8fece5cb3e1.txt", "contents": "\nSCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial Network\n\n\nJian Zhang \nYuxin Peng \nMingkuan Yuan \nSCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial Network\n1Index Terms-Cross-modal hashinggenerative adversarial networksemi-supervised\nCross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they mainly have two limitations: (1) Heavily rely on large-scale labeled cross-modal training data which are labor intensive and hard to obtain, since multiple modalities are involved.(2) Ignore the rich information contained in the large amount of unlabeled data across different modalities, especially the margin examples from another modality that are easily to be incorrectly retrieved, which can help to model the correlations between different modalities. To address these problems, in this paper we propose a novel Semi-supervised Cross-Modal Hashing approach by Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN's ability for modeling data distributions, so that SCH-GAN can model the distribution across different modalities, and select informative margin examples from large amount of unlabeled data to promote cross-modal hashing learning in an adversarial way. The main contributions can be summarized as follows: (1) We propose a novel generative adversarial network for cross-modal hashing. In our proposed SCH-GAN, the generative model tries to select margin examples of one modality from unlabeled data when giving a query of another modality (e.g. giving a text query to retrieve images and vice versa). While the discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of discriminative model.(2) We propose a reinforcement learning based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model by maximizing the margin between positive and negative data. Extensive experiments compared with 8 state-of-the-art methods on 3 widely-used datasets verify the effectiveness of our proposed approach.\n\nSCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial Network Jian Zhang, Yuxin Peng and Mingkuan Yuan\n\nAbstract-Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they mainly have two limitations: (1) Heavily rely on large-scale labeled cross-modal training data which are labor intensive and hard to obtain, since multiple modalities are involved. (2) Ignore the rich information contained in the large amount of unlabeled data across different modalities, especially the margin examples from another modality that are easily to be incorrectly retrieved, which can help to model the correlations between different modalities. To address these problems, in this paper we propose a novel Semi-supervised Cross-Modal Hashing approach by Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN's ability for modeling data distributions, so that SCH-GAN can model the distribution across different modalities, and select informative margin examples from large amount of unlabeled data to promote cross-modal hashing learning in an adversarial way. The main contributions can be summarized as follows: (1) We propose a novel generative adversarial network for cross-modal hashing. In our proposed SCH-GAN, the generative model tries to select margin examples of one modality from unlabeled data when giving a query of another modality (e.g. giving a text query to retrieve images and vice versa). While the discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of discriminative model. (2) We propose a reinforcement learning based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model by maximizing the margin between positive and negative data. Extensive experiments compared with 8 state-of-the-art methods on 3 widely-used datasets verify the effectiveness of our proposed approach.\n\nIndex Terms-Cross-modal hashing, generative adversarial network, semi-supervised.\n\n\nI. INTRODUCTION\n\nW ITH the fast development of Internet and multimedia technologies, heterogeneous multimedia data including image, video, text and audio, has been growing very fast and enriching people's life. To make better use of such rich multimedia data, it is an important application to retrieve multimedia contents that users have interests in. Thus multimedia retrieval has attracted much attention over the past decades. \n\n\nText\n\nAlthough dogs are the most closely related canids to gray wolves (the sequence divergence between gray wolves and dogs is only 1.8%, as opposed to over 4% between gray wolves...\n\nEarly technological progress owed much to the firm of Broadwood. John Broadwood joined with another Scot, Robert\n\nStodart, and a Dutchman, Americus Backers ...\n\nThe oldest remains of a tigerlike cat, called Panthera palaeosinensis, have been found in China and Java. This species lived about 2 million years ago....\n\n\nImage\n\nThe word Panthera is probably of Oriental origin and retraceable to the Ancient Greek word panther, the Latin word panthera, the Old French word pantere, most likely meaning ... This reclassification has implications for conservation. If there are two separate species, each will be less abundant (particularly the rarer) and could be more endangered than . This reclassification has implications for conservation. If there are two separate species, each will be less abundant (particularly the rarer) and could be more endangered than .\n\n\nHashing Functions Bucket\n\n\nHashing Tables\n\nAlthough dogs are the most closely related canids to gray wolves (the sequence divergence between gray wolves and dogs is only 1.8%, as opposed to over 4% between gray wolves... Fig. 1. An example of cross-modal hashing with image and text, which projects data of different modalities into a common hamming space and performs fast retrieval. However, with the explosive increase of multimedia data on the Internet, efficient retrieval from large scale databases has become an urgent need and a big challenge. To tackle this problem, many hashing methods [1]- [8] have been proposed to perform efficient yet effective retrieval. Generally speaking, hashing methods aim to transfer high dimensional feature into short binary codes so that similar data can have similar binary codes. Hashing methods have two major advantages when applied in multimedia retrieval: (1) Binary codes enable fast Hamming distance computation based on bit operations which can be efficiently implemented. (2) Binary codes take much less storage compared with original high dimensional feature.\n\nA large amount of hashing methods are designed for single modality retrieval [1]- [8], that is to say, data can only be retrieved by an query of the same modality, such as image retrieval [3] and video retrieval [9]. However, in real world applications, multimedia data is usually presented with different modalities. For example, an image is usually associated with a textual description, and both of them describe the same semantic. In this case, an increasing need of users is to retrieve across different modalities, such as using an image to retrieve relevant textual descriptions. Such retrieval paradigm is called cross-modal hashing. It is more useful and flexible than single modality retrieval because users can retrieve whatever they want by submitting whatever they have [10].\n\nThe key challenge of cross-modal hashing is the \"heterogeneity gap\", which means the distribution and representation of different modalities are inconsistent, and makes it hard to directly measure the similarity between different modalities. To bridge this gap, many cross-modal hashing methods [11]- [24] have been proposed. Generally speaking, cross-modal hashing methods can be divided into traditional methods and deep learning based methods. Traditional methods can be further divided into unsupervised methods and supervised methods based on whether semantic information is involved. The basic idea of unsupervised cross-modal hashing methods is to project data from different modalities into a common space where correlations between them are maximized, which is similar to Canonical Correlation Analysis (CCA) [25]. Representative unsupervised cross-modal hashing methods including Cross-view hashing (CVH) [11], Inter-Media Hashing (IMH) [12], Predictable Dual-View Hashing (PDH) [13] and Collective Matrix Factorization Hashing (CMFH) [14]. While supervised cross-modal hashing methods try to learn hash functions to preserve the semantic correlations provided by labels. Representative supervised cross-modal hashing methods include Co-Regularized Hashing (CRH) [15], Heterogeneous Translated Hashing (HTH) [16], Semantic Correlation Maximization (SCM) [17], Quantized Correlation Hashing (QCH) [18] and Semantics-Preserving Hashing (SePH) [19]. Recently, inspired by the successful applications of deep learning on image classification [26] and object recognition [27], some researches try to bridge the \"heterogeneity gap\" by deep learning technique. Representative deep learning based methods include Deep and Bidirectional Representation Learning Model (DBRLM) [20], Deep Visual-semantic Hashing (DVH) [23] and Cross-Media Neural Network Hashing (CMNNH) [22]. Among the above cross-modal hashing methods, supervised methods typically achieve better retrieval accuracy due to the utilization of semantic information. However, supervised methods are very labor intensive to obtain large scale labeled training data. It is even harder to label cross-modal hashing training data, since multiple modalities are involved. Thus it is of significant importance to improve retrieval accuracy by fully exploiting unlabeled data which is very convenient to get. Traditional semi-supervised learning methods [28] can effectively exploit distribution of unlabeled data to help supervised training. However, little efforts have been done for semi-supervised cross-modal hashing. The key challenge of semi-supervised cross-modal hashing is to exploit informative unlabeled data to promote hashing learning. With the recent progress of generative adversarial network (GAN) [29]- [32], which has been applied in many computer vision problems, such as image synthesis [30], video prediction [31] and object detection [32]. GAN has shown its promising ability to model the data distributions. Inspired by this ability, in this paper we propose a novel semi-supervised cross-modal hashing approach by generative adversarial network (SCH-GAN). We aim to design a generative model to fit the relevance distribution of unlabeled data near the margins between different modalities, so that it can select informative unlabeled examples close to the margin to fool the discriminative model. We also design a discriminative model to distinguish the selected data from generative model and the true positive data, so that it can better estimate the cross-modal ranking. These two models play a minimax game to iteratively optimize each other and boost cross-modal hashing accuracy. The main contributions of this paper can be summarized as follows:\n\n\u2022 A Generative adversarial network for cross-modal hashing (SCH-GAN) is proposed to fully exploit unlabeled data to improve hashing performance. In our proposed SCH-GAN, the generative model learns to fit the relevance distribution of unlabeled data, and tries to select margin examples from unlabeled data of one modality giving a query of another modality. While the discriminative model learns to judge the relevance between query and selected examples by the guidance of labeled data. Generative model tries to select the margin examples that are easily to be retrieved incorrectly to fool the discriminative model, while the latter tries to distinguish those selected examples from true positive data. Thus these two models act as two players in a minimax game, and each of them improves itself to \"beat\" each other. The finally learned hashing functions from discriminative model can better reflect both semantic information of labeled data and data distributions of unlabeled data. \u2022 Reinforcement learning based loss function is proposed to train the generative model and discriminative model. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model to maximize the margin between positive and negative data. The discriminative model utilizes a cross-modal triplet ranking loss to learn the ranking information provided by semantic labels, and it also acts like a critic that tries to distinguish the selected examples from generative model and true positive data. Extensive experiments on the widely-used Wikipedia, NUS-WIDE and MIRFlickr datasets demonstrate that our proposed SCH-GAN outperforms 8 state-of-the-art methods, which verify the effectiveness of SCH-GAN approach.\n\nThe rest of this paper is organized as follows. We briefly review the related works in Section II. In Section III, our proposed SCH-GAN approach is presented in detail. Then Section IV reports the experimental results as well as analyses. Finally, Section V concludes this paper.\n\n\nII. RELATED WORKS\n\nIn this section, we briefly review some related works from two aspects: cross-modal hashing and generative adversarial network.\n\n\nA. Cross-modal Hashing\n\nHashing methods for single modality retrieval have been extensively studied in the past decades [1]- [8], and crossmodal hashing methods are receiving increasing attention in recent years. Generally speaking, most cross-modal hashing methods project data of different modalities into a common Hamming space to perform fast retrieval, and it can be divided into traditional methods and deep learning based methods. Traditional methods can be further divided into unsupervised and supervised methods based whether semantic information is involved. We will briefly review some representative works of cross-modal hashing methods.\n\nUnsupervised cross-modal hashing methods have the similar idea with Canonical Correlation Analysis (CCA) [25], which maps heterogeneous data into a common Hamming space to maximize the correlation. Inter-Media Hashing (IMH) [12] is proposed to learn a common Hamming space to preserve both inter-media and intra-media consistency. Kumar et al. propose Cross-view Hashing (CVH) [11], which extends image hashing method Spectral Hashing (SH) [7] to consider both intra-view and inter-view similarities with a generalized eigenvalue formulation. Rastegari et al. propose Predictable Dual-View Hashing (PDH) [13] for two modalities, which proposes a objective function to preserve the predictability of pre-generated binary codes, and optimize the objective function by an iterative method based on block coordinate descent. Ding et al. propose Collective Matrix Factorization Hashing (CMFH) [14], which learns unified hash codes from different modalities of one instance by collective matrix factorization with a latent factor model. Latent Semantic Sparse Hashing (LSSH) [33] is proposed to use sparse coding and matrix factorization to learn separate semantic features for images and text, and then map them into a joint abstract space to reduce semantic difference. Wang et al. propose Semantic Topic Multimodal Hashing (STMH) [34], which models texts as semantic topics while images as latent semantic concepts, and maps the learned semantic features for different modalities into a common semantic space, finally generates hash codes by predicting whether topics or concepts are available in the original data. Unsupervised methods try to learn cross-modal hashing functions from data distributions, thus they achieve limited accuracy on retrieving semantically similar data. To improve retrieval accuracy, some supervised methods are then proposed.\n\nSupervised cross-modal hashing methods leverage semantic information obtained from labels of training data, which achieve better retrieval accuracy than unsupervised methods. Cross-modality Similarity Sensitive Hashing (CMSSH) [35] is proposed to model hashing learning as a classification problem, and can be learned in a boosting manner. Zhen et al. propose Co-Regularized Hashing (CRH) [15], which learns hash functions of each bit sequentially so that the bias introduced by each hash functions can be minimized. Hu et al. propose Iterative Multi-view Hashing (IMVH) [36], which tries to learn hashing functions by preserving both within-view similarity and between-view correlations. Heterogeneous Translated Hashing (HTH) [16] is proposed to learn different Hamming spaces for different modalities, and then learn translators to align these spaces to perform crossmodal retrieval. Zhang et al. propose Semantic Correlation Maximization (SCM) [17], which constructs semantic similarity matrix based on labels and learns hashing functions to preserve the constructed matrix. Wu et al. propose Quantized Correlation Hashing (QCH) [18] to simultaneously optimize cross-modal correlation and quantization error, which is also considered in many single modality hashing methods. Lin et al. propose Semantics-Preserving Hashing (SePH) [19], which is a two-step supervised hashing method, it firstly transforms the given semantic matrix of training data into a probability distribution and approximates it with learned hash codes in Hamming space via minimizing the KL-divergence. Supervised cross-modal hashing methods often achieve better results than unsupervised methods because of utilization of semantic information.\n\nDeep learning based methods are inspired by recent advance of deep neural networks, which have been applied in many computer vision problems, such as image classification [26] and object recognition [27]. Zhuang et al. propose Cross-Media Neural Network Hashing (CMNNH) [22], which learns hashing functions by preserving intra-modal discriminative capability and inter-modal pairwise correspondence. Wang et al. propose Deep Multimodal Hashing with Orthogonal Regularization (DMHOR) [21], [37], which learns hashing functions by preserving intra-modal and inter-modal correlation, as well as reducing redundant information between hash bits. Cao et al. propose Cross Autoencoder Hashing (CAH) [24], which maximizes the feature correlation of bimodal data and the semantic correlation provided by similarity labels, and CAH is based on deep autoencoder structure. Deep Visualsemantic Hashing (DVH) [23] is proposed, which is an endto-end framework that integrates both feature learning and hashing function learning.\n\nSupervised methods, especially supervised deep learning based methods have achieved promising results. However, supervised methods rely on large amount of labeled training data which are labor intensive to obtain. It is even harder to label cross-modal hashing training data, since multiple modalities are involved. Traditional semi-supervised learning methods [28] can exploit unlabeled data effectively to help supervised training. Little efforts have been done for semisupervised cross-modal hashing learning. In this paper, we attempt to fully exploit the unlabeled data to promote crossmodal hashing learning.\n\n\nB. Generative Adversarial Network\n\nGenerative Adversarial Network (GAN) [29] is first proposed to estimate generative model by an adversarial process. GAN consists of two models: a generative model G that captures the data distributions, and a discriminative model D that estimates the probability that a sample comes from real data rather than G. These two models are trained in a adversarial way so that they compete with each other, and both of them can learn better representations of the data. Inspired by the ability of modeling data distributions of GAN, many works have attempted to apply GAN in many computer vision problems. Most popular one is image synthesis. Radford et al. propose Deep Convolutional GAN (DCGAN) [38], which adopts convolutional decoder with batch normalization and achieves better image synthesis results. Mirza et al. proposed Conditional GAN (CGAN) [39], which provides side information for both generative and discriminative model to control the generated data. Inspired by CGAN, many works extend its idea to image synthesis problem, Reed et al. propose textconditional GAN [40] which can generate images conditioned by textual descriptions. Odena et al. propose auxiliary classifier GAN (AC-GAN) [41] that generates images conditioned by class labels. Besides image synthesis, GAN is also applied to video prediction [31] and object detection [32].  2. The overall framework of our proposed semi-supervised cross-modal hashing approach by generative adversarial network (SCH-GAN), which consists a generative model and a discriminative model. The generative model attempts to select informative unlabeled data to form a pair with a given labeled query to fool discriminative model, while the discriminative model tries to distinguish if a pair is generated or a true pair. Those two models act as two players to play a minimax game to optimize each other, and promote cross-modal hashing performance.\n\nInspired by the ability of GAN to model data distributions, in this paper we propose a novel semi-supervised cross-modal hashing by generative adversarial network (SCH-GAN). It aims design a generative model to learn the distributions of different modalities, and a discriminative model to maintain the semantic ranking information, these two models play a minimax game to iteratively optimize each other and boost cross-modal hashing accuracy.\n\n\nIII. THE PROPOSED APPROACH\n\nThe overall framework of our proposed approach is demonstrated in Figure 2, which consists of a generative model and a discriminative model. The generative model receives the input of both labeled and unlabeled data of different modalities, and given a query of labeled data, it attempts to select informative unlabeled data around the error margins of another modality to form a pair of data. While the discriminative model receives both the generated pairs and true pairs as input, and tries to distinguish them so that it can better discriminate those margin examples. These two models are trained by playing a minimax game, and the finally trained discriminative model can be used to perform cross-modal retrieval.\n\n\nA. Notation\n\nFirstly, the formal definition of cross-modal hashing and some notations used in this paper are introduced. The two modalities involved in this paper are denoted as I for image and T for text. The multimodal dataset is denoted as D = {I, T }, I \u2208 R I , T \u2208 R T , which is further split into a retrieval database D db and a query set D q . The retrieval database D db is also the training set, which consists of two parts, namely labeled data D L db and unlabeled data\nD U db = {I U db , T U db }. D L db is defined as D L db = {I L db , T L db }, where I L db = {i L p } n p=1 and T L db = {t L p } n p=1\n, n is the number of labeled data. D L db is also associated with corresponding class labels, which are denoted\nas {c I p } n i and {c T p } n i . D U db is defined as D U db = {I U db , T U db }, where I U db = {i U p } m p=1 and T U db = {t U p } m p=1\n, m is the number of unlabeled data and m >> n. The query set D q is defined as D q = {I q , T q }, where I q = {i p } t p=1 and T q = {t p } t p=1 . The goal of cross-modal hashing is to learn two mapping functions H I : R I \u2192 R H and H T : R T \u2192 R H , so that semantically similar data of different modalities are close in the common Hamming space. Given a query of any modality, by the learned hashing functions, the semantically similar data of another modality can be retrieved by the fast Hamming distance measurement.\n\nIt is noted that supervised methods use labeled data D L db to train hashing functions, however labeled data are labor intensive and hard to obtain, while unlabeled data are convenient to acquire. Thus, in this paper we attempt to further exploit the large amount unlabeled data D U db to promote hashing learning.\n\n\nB. Network structure\n\nAs shown in Figure 2, the proposed SCH-GAN consists of a generative model and a discriminative model. We will introduce the detailed network structures of them in this subsection.\n\nThe generative model is a two-pathway architecture, which receives both image and text as inputs. Each pathway consists of two parts: feature representation layers and hashing layers. For image pathway, we adopt widely-used convolutional neural networks (CNN) to represent each image, and it is noted that we keep the parameters of the CNN fixed during the training phase, since our focus is hashing function learning. While for text pathway, we use bag-of-words (BoW) features to represent textual descriptions. The structure of hash layers is the same in the two pathways, and it consists of two fullyconnected layers. The first fully-connected layer serves as an intermediate layer that maps modality specific feature into a common space. The second fully-connected layer serves as hashing functions, which further map the intermediate feature into hash codes:\nh(x) = sigmoid(W T f (x) + v)(1)\nwhere f (x) is the intermediate feature extracted from last layer, W denotes the weights in the hash code learning layer, and v is the bias parameter.The dimension of last fullyconnected layer is set to be the same as the hash code length q. Through the hash code learning layer, intermediate features f (x) are mapped into [0, 1] q . Since hash codes h(x) \u2208 [0, 1] q are continuous real values, we apply a thresholding function g(x) = sgn(x \u2212 0.5) to obtain binary codes:\nb k (x) = g(h(x)) = sgn(h k (x) \u2212 0.5), k = 1, 2, \u00b7 \u00b7 \u00b7 , q(2)\nHowever, binary codes are hard to directly optimize, thus we relax binary codes b(x) with continuous real valued hash codes h(x) in the rest of this paper. Through the hashing layers, the features of different modalities are mapped into the Hamming space with same dimensions so that the similarity between different modalities can be measured by fast Hamming distance calculation. The input of generative model consists of both labeled and unlabeled data, and the goal of generative model is to select informative unlabeled data of another modality that lies around margins when given a query of one modality. This goal is achieved by the adversarial training algorithm, which will be introduced in the following subsections.\n\nThe discriminative model is also a two-pathway structure, whose detailed settings are exactly the same as the generative model. The input of discriminative model is the generated (selected) relevant pairs by generative model, and the true relevant pairs sampled from labeled data. The goal of discriminative model is to distinguish whether an input pair is generated or a true pair.\n\n\nC. Objective function\n\nFirstly, we give the formal definitions of the objectives of proposed generative model and discriminative model.\n\n\u2022 Generative model: p \u03b8 (i U |q t , r) and p \u03b8 (t U |q i , r), which try to select relevant data of one modality from unlabeled data when given a query of another modality. For example, given a text query q t , the generative model tries to select relevant image i U from I U db . The goal of generative model is to approximate the true relevance distribution across different modalities p true (i U |q t , r).\n\n\u2022 Discriminative model: f \u03c6 (i, q t ) and f \u03c6 (t, q i ), which try to predict the relevance score of the query and candidate data pair. The inputs of discriminative model consist of true pairs sampled by semantic labels, as well as generated pairs from generative model. The goal of discriminative model is to distinguish the relevant data (true pairs) and non-relevant data (generated pairs) for a query q i as accurate as possible. Given above definitions, the generative model and discriminative model act as two players that play a minimax game: Given a query, the generative model tries to select margin data that is likely to be retrieved incorrectly to fool the discriminative model, while the discriminative model tries to distinguish between true relevant data sampled from ground-truth and the selected ones generated by its adversarial generative model. Inspired by the GAN [29]- [32], the adversarial process is defined as follows:\nV(G, D) = min \u03b8 max \u03c6 n j=1 (E i\u223cptrue(i L |q j t ,r) [log(D(i L |q j t ))] + E i\u223cp \u03b8 (i U |q j t ,r) [log(1 \u2212 D(i U |q j t ))])(3)\nAbove equation is for text query image task, and the image query text task is similarly defined as:\nV(G, D) = min \u03b8 max \u03c6 n j=1 (E t\u223cptrue(t L |q j i ,r) [log(D(t L |q j i ))] + E t\u223cp \u03b8 (t U |q j i ,r) [log(1 \u2212 D(t U |q j i ))])(4)\nThese two equations are symmetric, thus in the following parts we use the text query image task as an example to present the detailed objective function. In above equations, the generative model G is denoted as p \u03b8 (i U |q t , r), which is defined as a softmax function:\np \u03b8 (i U |q t , r) = exp(\u2212 h T (q t ) \u2212 h I (i U ) 2 ) i U exp(\u2212 h T (q t ) \u2212 h I (i U ) 2 )(5)\nwhere h i ( * ) and h t ( * ) denote the hashing functions of image pathway and text pathway respectively. Intuitively, equation 5 calculates the probability that we select an image according to a given text query. It is based on the distance between image and text, smaller distance leads to larger probability. While for the true data distributions p true (i L |q j t , r), we sample relevant images i L of query q t based on labels, such that the generative model can preserve the semantic distribution of labeled data. The discriminative model D predicts the probability of selected image i U being relevant to text query q t , and D is defined as the sigmoid function of relevance score:\nD(i U |q t ) = sigmoid(f \u03c6 (i U , q t )) = exp(f \u03c6 (i U , q t )) 1 + exp(f \u03c6 (i U , q t )) D(i L |q t ) = sigmoid(f \u03c6 (i L , q t )) = exp(f \u03c6 (i L , q t )) 1 + exp(f \u03c6 (i L , q t ))(6)\nThe relevance score of f \u03c6 (i U , q t ) and f \u03c6 (i L , q t ) are defined by triplet ranking loss as follows:\nf \u03c6 (i U , q t ) = max(0, m i + h T (q t ) \u2212 h I (i + ) 2 \u2212 h T (q t ) \u2212 h I (i U ) 2 ) (7) f \u03c6 (i L , q t ) = max(0, m i + h T (q t ) \u2212 h I (i L ) 2 \u2212 h T (q t ) \u2212 h I (i \u2212 ) 2 )(8)\nwhere i + is a semantically similar image with text query q t according to labels, i \u2212 is a semantically dissimilar image sampled from labeled data, i U is the selected image by generative model, and m i is a margin parameter which is set to be 1 in our proposed approach. Above formulation means that we want the distance between true relevant pair (q t , i + ) smaller than that of generated pair (q t , i U ) by a margin m i , so that the discriminative model can draw a clear distinguish line between the true and generated pairs. Similarly, we also want to keep the ranking based relations between labeled data. From above objective definitions, we can observe that the generative model and discriminative model can be learned by iteratively maximizing and minimizing the same object function. The discriminative model tries to draw a margin between generated (selected) data and positive data, while the generative mode tries to select data near the margin to fool the discriminative model.\n\n\nD. Optimization\n\nBy the objective function defined in equation 3, the overall training flow of proposed approach is shown in Figure 3. We keep the parameters of generative model fixed while training the discriminative model and vise versa. We'll introduce the optimization algorithm of these two models separately.\n\n1) Optimizing discriminative model: As shown in Figure 3, when updating the parameters of discriminative model, we keep the generative model fixed. Firstly we use the generative model of previous iteration to select text-image and imagetext pairs when given text and image as queries respectively, and we further sample true text-image and image-text pairs from labeled data. Then the discriminative model tries to maximize the log-likelihood of correctly distinguishing the true and generated relevant pairs. When fixing generative model, based on equation 6, equation 3 can be rewritten as:\n\u03c6 * = arg max \u03c6 n j=1 (E i\u223cptrue(i U |q j t ,r) [log(sigmoid(f \u03c6 (i L , q j t )))] + E i\u223cp \u03b8 * (i U |q j t ,r) [log(1 \u2212 sigmoid(f \u03c6 (i U , q j t )))])(9)\nwhere p \u03b8 * is the generative model obtained in previous iteration. According to equations 7 and 8, equation 9 is differentiable with respect to \u03c6, thus it can be solved by stochastic gradient descent algorithm.\n\n2) Optimizing generative model: As demonstrated in Figure 3, the discriminative model is fixed when training the generative model. On the contrary, the generative model tries to minimize equation 3 and fits the true relevance distribution. When the discriminative model is fixed, the generative model can be trained by minimizing equation 3:\n\u03b8 * = arg min \u03b8 n j=1 (E i\u223cptrue(i U |q j t ,r) [log(sigmoid(f \u03c6 * (i L , q j t )))] + E i\u223cp \u03b8 (i U |q j t ,r) [log(1 \u2212 sigmoid(f \u03c6 * (i U , q j t )))]) = arg min \u03b8 n j=1 E i\u223cp \u03b8 (i U |q j t ,r) [log(1 \u2212 exp(f \u03c6 (i U , q t )) 1 + exp(f \u03c6 (i U , q t )) )] = arg max \u03b8 n j=1 E i\u223cp \u03b8 (i U |q j t ,r) [log(1 + exp(f \u03c6 (i U , q t )))](10)\nwhere f \u03c6 * is the generative model trained in previous iteration. Different from the generative model of traditional GAN, which generates new data from continuous noise vector and can be optimized via stochastic gradient descent. The generative model of the proposed SCH-GAN selects data from unlabeled data. Since the selective strategy is discrete, it can not be optimized by stochastic gradient descent. We use policy gradient based reinforcement learning to update the parameters of generative model, which is derived as follows: Generate m text-image pairs by p \u03b8 * (i U |q j t , r) given text query q j t 5:\n\u2207 \u03b8 E i\u223cp \u03b8 (i U |q j t ,r) [log(1 + exp(f \u03c6 (i U , q j t )))] = m k=1 \u2207 \u03b8 p \u03b8 (i U k |q j t , r)log(1 + exp(f \u03c6 (i U k , q j t ))) = m k=1 p \u03b8 (i U k |q j t , r)\u2207 \u03b8 logp \u03b8 (i U k |q j t , r)log(1 + exp(f \u03c6 (i U k , q j t ))) = E i\u223cp \u03b8 (i U |q j t ,r) [\u2207 \u03b8 logp \u03b8 (i U |q j t , r)log(1 + exp(f \u03c6 (i U , q j t )))] 1 m m k=1 \u2207 \u03b8 logp \u03b8 (i U k |q j\nSampled m true text-image pairs from D L db based on labels 6: Train discriminative model f \u03c6 (i, q t ) by equation 9 7: end for 8: for g-step do 9: Generate m text-image pairs by p \u03b8 (i U |q j t , r) given text query q t 10:\n\nCalculate reward by log(1 + exp(f \u03c6 * (i U k , q j t ))) 11: Update parameters of generative model p \u03b8 (i U |q j t , r) by equation 11 12: end for 13: until SCH-GAN converges Output: Optimized generative model p \u03b8 * (i|q t , r) and discriminative model f \u03c6 * (i, q t )\n\n\nE. Cross-modal retrieval by learned discriminative model\n\nIt is noted that the design idea of our SCH-GAN is that the generative model tries to fit the distribution near the decision boundary, thus it is not suitable to perform crossmodal retrieval. However, the discriminative model is promoted greatly by the generative model since it can better distinguish the margin examples. Thus after the proposed SCH-GAN is trained, cross-modal retrieval can be performed by the discriminative model. More specifically, given a query of any modality (e.g. text or image), it can be first encoded into binary hash code by equation 2. Then cross-modal retrieval is performed by fast Hamming distance computation between query and each data in the database.\n\n\nIV. EXPERIMENTS\n\nIn this section, we demonstrate the experimental results of our proposed SCH-GAN approach. We first introduce the datasets, evaluation metrics, implementation details and comparison methods used in our experiments. Then we compare the proposed SCH-GAN with 8 state-of-the-art methods and analyze the results. Finally, we further conduct several baseline experiments to investigate the performance of generative and discriminative model.\n\n\nA. Dataset\n\nWe evaluate the proposed approach and compared methods on 3 widely-used datasets: Wikepedia [42], NUSWIDE [43] and MIRFLICKR [44]. We'll briefly introduce three datasets.\n\n\u2022 Wikipedia dataset [42] is a widely-used dataset for cross-modal retrieval, which is collected from \"featured articles\" in Wikipedia with 10 most populated categories. This dataset consists of 2866 image/text pairs, of which the images are represented by 4096 deep features extracted from 19-layer VGGNet [45], and texts are represented by 1000 dimensional BoW (Bag of Words) features. Following [19], Wikipedia dataset is split into a training set of 2173 pairs and a test set of 693 pairs. Since Wikipedia daaset is small, the training set is also used as the retrieval database, while the test set works as query. \u2022 NUSWIDE dataset [43] contains 269498 images associated with 81 concepts as ground truth, each image also has corresponding tags. Following [19], we select the 10 most common concepts and the corresponding 186557 images. We take 1% data of NUSWIDE dataset as the query set, and the rest as the retrieval database. As for the supervised methods, we further randomly sampled 5000 images as training set, which is similar to real world applications where only a fraction of the database are labeled. We also represent each image by 4096 deep features extracted from 19-layer VGGNet, and each texts by 1000 dimensional BoW. \u2022 MIRFlickr dataset [44] contains 25000 images collected from Flickr, each image is also associated with textual tags and labeled with one or more of 24 provided semantic labels. Following [19], we take 5% of the dataset as the query set and the remaining as the retrieval database. Similar wih NUSWIDE dataset, we also randomly sample 5000 images to form the supervised training set. Similarly, we represent each image by 4096 deep features extracted from 19-layer VGGNet, and each texts by 1000 dimensional BoW.\n\n\nB. Retrieval tasks and evaluation metrics\n\nWe perform cross-modal retrieval on the 3 datasets with two kinds of retrieval tasks:\n\n\u2022 Image query text: Using image as query to retrieve semantically similar texts from retrieval database, we denote it as image\u2192text. \u2022 Text query image: Using text as query to retrieve semantically similar images from retrieval database, we denote it as text\u2192image. We utilize Hamming ranking to evaluate the proposed SCH-GAN approach and compared state-of-the-art methods. Hamming ranking gives the ranking list of a given query based on the Hamming distance, where ideal semantic neighbors are expected to be returned on the top of the ranking list. The retrieval results are evaluated based on whether the returned data and the query share the same semantic labels. We use three evaluation metrics to measure the retrieval effectiveness: Mean Average Precision (MAP), precision recall curve (PRcurve) and precision at top k returned results (topK-precision), which are defined as follows:\n\n\u2022 Mean Average Precision (MAP): MAP is the mean value of average precisions (AP) of all queries, and AP is defined as:\nAP = 1 R n k=1 k R k \u00d7 rel k(12)\nwhere n is the size of database, R is the number of relevant images in database, R k is the number of relevant images in the top k returns, and rel k = 1 if the image ranked at k-th position is relevant and 0 otherwise. \u2022 Precision recall curve (PR-curve): The precision at certain level of recall of the retrieved ranking list, which is frequently used to measure the information retrieval performance. \u2022 Precision at top k returned results (topK-precision): The precision with respect to different numbers of retrieved samples. Those three metrics can evaluate the proposed approach and compared methods objectively and comprehensively.\n\n\nC. Comparison methods\n\nWe compare with 8 state-of-the-art methods to verify the effectiveness of our proposed approach, including unsupervised methods CVH [11], PDH [13], CMFH [14] and CCQ [46], supervised methods CMSSH [35], SCM [17] and SePH [19], and deep learning based methods DCMH [50]. We'll briefly introduce those compared methods.\n\n\u2022 CVH [11] extends Spectral Hashing (SH) [7] to considers both intra-view and inter-view similarities with a generalized eigenvalue formulation. \u2022 PDH [13] tries to preserve the predictability of pregenerated binary codes, and optimize the objective function by an iterative method based on block coordinate descent algorithm. \u2022 CMFH [14] learns unified hash codes from different modalities of one instance by collective matrix factorization with a latent factor model. \u2022 CCQ [46] jointly learns the correlation-maximal mappings that transform different modalities into isomorphic latent space, and learns composite quantizers that convert the isomorphic latent features into compact binary codes. \u2022 CMSSH [35] models hashing learning as a classification problem, and it is learned in a boosting manner. \u2022 SCM [17] constructs semantic similarity matrix based on labels and learns hashing functions to preserve the constructed matrix. \u2022 SePH [19] is a two-step supervised hashing methods, it firstly transforms the given semantic matrix of training data into a probability distribution and approximates it with learned hash codes in Hamming space via minimizing the KL-divergence. \u2022 DCMH [50] is an end-to-end deep learning based method, which performs feature learning and hashing function learning simultaneously. It is noted that for a fair comparison between traditional methods and deep learning based methods, we uniformly use the same deep features as the input of traditional methods. Specifically, we use the 4096 deep features extracted from 19layer VGGNet pre-trained on ImageNet for images, and 1000 dimensional BoW for texts. While for deep learning based methods, we use the same 19-layer VGGNet as their base network for image pathway, while keep the same settings for the text pathway.\n\n\nD. Implementation details\n\nWe implement the proposed SCH-GAN in Figure 2 by tensorflow 1 , which is a widely-used open source software library for numerical computation using data flow graphs. The implementation details are described as follows:\n\n1) Data processing: For the image pathway of our proposed SCH-GAN, we use the same 19-layer VGGNet as the base network for image representation learning, and for the text pathway, we use the same 1000 dimensional BoW for text representation. It is noted that we keep the parameters of VGGNet fixed since we focus on the adversarial training of cross-modal hashing learning.\n\n2) Details of network: We introduce the hashing layers in Figure 2 in detail. The hashing layers consist of an intermediate layer and a hashing layer. The intermediate layer is a fully-connected layer, whose dimension is set to be 4096 in the experiments. While the hashing layer is a fully-connect layer whose dimension is set the same as the hash code length, we also use sigmoid activation for hashing layer to force the output in the range of [0, 1].  3) Training details: Here we introduce some details of the training flow demonstrated in algorithm 1. The proposed SCH-GAN is trained in a mini-batch way. The mini-batch size is set to be 64 for both image and text pathway, and m in algorithm 1 is set to be 20, namely we generate 20 pairs for each query. The proposed SCH-GAN is trained iteratively, specifically for each d-step and g-step, we train 1 epoch for the discriminative and generative model respectively. The learning rate of our proposed network are initialized as 0.01, and it is decreased by a factor of 10 each two epochs.\n\nFor the compared methods, all the implementations are provided by their authors, and we follow the best settings in their papers to conduct the experiments.\n\n\nE. Comparison with state-of-the-art methods\n\nThe MAP scores of two retrieval tasks on Wikepedia, NUSWIDE and MIRFlickr datasets are shown in Tables I, II and III respectively. From the result tables, we can observe that our proposed SCH-GAN approach achieves the best retrieval accuracy compared with state-of-the-art methods on three datasets. More specifically, the result tables are partitioned into three categories, namely unsupervised, supervised and deep learning based methods. Compared with these three categories, from the result tables we can observe that: (1) Our proposed SCH-GAN outperforms the unsupervised methods. For example, on NUSWIDE dataset compared with best unsupervised methods CCQ [46], our proposed SCH-GAN improves the average MAP score from 0.505 to 0.730 on image query text task, and improves the average MAP score from 0.490 to 0.758 on text query image task. Similar trends can be observed on Wikipedia and MIRFlickr datasets from Tables I and III. It is because unsupervised methods only learn hashing functions from data distributions, which achieve limited accuracy. (2) Compared with supervised methods, our proposed SCH-GAN achieves the best results. For example, compared with best supervised method SePH [19] on NUSWIDE dataset, our SCH-GAN improves average MAP scores from 0.715 to 0.730 on image query text task, and improves from 0.654 to 0.758 on text query image task. Similar results can be observed on both Wikipedia and MIRFlickr datasets. It is because our proposed SCH-GAN fully exploits the unlabeled data to promote the hash learning. image query text text query image Fig. 6. The retrieval accuracy of discriminative and generative model with respect to training iterations on NUSWIDE dataset with 64bit code length.\n\ntext task, and from 0.696 to 0.758 on text query image task. It demonstrates that the generative model can promote the discriminative model by challenging it with hard examples around margins. Figures 4 and 5 show the topK-precision and precisionrecall curves on the three datasets with 64bit code length. We can observe that on both image query text and text query image tasks, our proposed SCH-GAN achieves the best accuracy, which further demonstrates the effectiveness of our proposed approach.\n\n\nF. Baseline experiments\n\nWe conduct two baseline experiments to demonstrate the performance of generative and discriminative model. Firstly, we investigate the retrieval performance of generative and discriminative model to give more insight of adversarial training. Secondly, we compare proposed SCH-GAN with a baseline approach without adversarial training to verify its effectiveness.\n\n1) Performance of adversarial training: We demonstrate the retrieval accuracy of generative and discriminative models during the training process. The result is shown in Figure 6, we can observe that during the adversarial training, the accuracy of discriminative model is increasing after the generative model updated. It means that the generative model selects more informative examples for the discriminative model to promote its accuracy.\n\n2) Comparison with baseline approach: In our proposed approach, the discriminative model can be trained solely without generative model by using the triplet ranking loss in equation 8. This is considered as a simple supervised baseline approach without using adversarial training, we denote this baseline approach as Dis. Compare SCH-GAN with Dis, we can verify the effectiveness of adversarial training. The results are shown in Table IV, and we can observe that the proposed SCH-GAN constantly outperforms baseline method Dis on all the three datasets. It demonstrates that the generative model selects informative margin examples to promote the training of discriminative model, thus promotes the accuracy of crossmodal hashing.\n\n\nV. CONCLUSION\n\nIn this paper we have proposed a novel semi-supervised cross-modal hashing approach based generative adversarial network (SCH-GAN). Firstly, we propose a novel generative adversarial network to model cross-modal hashing. In our proposed SCH-GAN, the generative model tries to select margin examples of another modality from unlabeled data when giving a query of one modality (e.g. giving a text query to retrieve images and vice versa). While the discriminative model tries to predict the correlation between query and selected examples of generative model. These two models play a minimax game to iteratively optimize each other in an adversarial way. Secondly, we propose a reinforcement learning based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model by maximizing the margin between positive and negative data. Experiments compared with 8 state-of-the-art methods on 3 widely-used datasets verify the effectiveness of our proposed approach.\n\nIn the future works, on one hand, we attempt to extend current framework to an unsupervised scenario which is more generalized. On the other hand, we will extend current approach to other modalities such as video and audio so that it can exploit complex correlations between multiple modalities.\n\n\nThis work was supported by National Natural Science Foundation of China under Grants 61771025 and 61532005. The authors are with the Institute of Computer Science and Technology, Peking University, Beijing 100871, China. Corresponding author: Yuxin Peng (e-mail: pengyuxin@pku.edu.cn).\n\nFig. 3 .\n3The training flow of the generative model and discriminative model. Best viewed in color. Algorithm 1 Training algorithm of proposed SCH-GAN Input: The generative model p \u03b8 (i|q t , r), the discriminative model f \u03c6 (i, q t ), training data D L db and D U db 1: Randomly initialize the parameters of p \u03b8 (i|q t , r) and f \u03c6 (i, q t )\n\nFig. 4 .\n4The topK-precision curves on three datasets with 64bit hash codes. The first row demonstrates the result of image query text task, while the second row shows the result of text query image task. Left, middle and right columns demonstrate Wikipedia, NUSWIDE and MIRFlickr datasets respectively.\n\nFig. 5 .\n5The precision-recall curves on three datasets with 64bit hash codes. The first row demonstrates the result of image query text task, while the second row shows the result of text query image task. Left, middle and right columns demonstrate Wikipedia, NUSWIDE and MIRFlickr datasets respectively.\n\n( 3 )\n3Our proposed SCH-GAN also outperforms deep learning based methods, which improves average MAP scores from 0.652 (DCMH[50]) to 0.730 on image\n\nH I\nHImage Representation Learning Images TextsThe oldest remains of a tigerlike cat, called Panthera palaeosinensis, have been found in China and Java. This species lived about 2 million years ago....The oldest remains of a tigerlike cat, called Panthera palaeosinensis, have been found in China and Java. This species lived about 2 million years ago....Early technological progress \nowed much to the firm of \nBroadwood. John Broadwood \njoined with another Scot, \nRobert \nStodart, \nand \na \nDutchman, Americus Backers \n... \n\nThe \nword \nPanthera \nis \nprobably \nof \nOriental \norigin \nand \nretraceable \nto \nthe \nAncient \nGreek \nword \npanther, \nthe \nLatin \nword \npanthera, \nthe \nOld \nFrench \nword \npantere, \nmost \nlikely \nmeaning \n... \n\nThe \noldest \nremains \nof \na \ntiger-\nlike \ncat, \ncalled \nPanthera \npalaeosinensis, \nhave \nbeen \nfound \nin \nChina \nand \nJava. \nThis \nspecies \nlived \nabout \n2 \nmillion \nyears \nago.... \n\nEarly \ntechnological \nprogress \nowed \nmuch \nto \nthe \nfirm \nof \nBroadwood. \nJohn \nBroadwood \njoined \nwith \nanother \nScot, \nRobert \nStodart, \nand \na \nDutchman, \nAmericus \nBackers \n... \n\nThe \nword \nPanthera \nis \nprobably \nof \nOriental \norigin \nand \nretraceable \nto \nthe \nAncient \nGreek \nword \npanther, \nthe \nLatin \nword \npanthera, \nthe \nOld \nFrench \nword \npantere, \nmost \nlikely \nmeaning \n... \n\nText Feature Extraction \n\nEarly \ntechnological \nprogress \nowed \nmuch \nto \nthe \nfirm \nof \nBroadwood. \nJohn \nBroadwood \njoined \nwith \nanother \nScot, \nRobert \nStodart, \nand \na \nDutchman, \nAmericus \nBackers \n... \n\nEarly \ntechnological \nprogress \nowed \nmuch \nto \nthe \nfirm \nof \nBroadwood. \nJohn \nBroadwood \njoined \nwith \nanother \nScot, \nRobert \nStodart, \nand \na \nDutchman, \nAmericus \nBackers \n... \n\nThe \noldest \nremains \nof \na \ntiger-\nlike \ncat, \ncalled \nPanthera \npalaeosinensis, \nhave \nbeen \nfound \nin \nChina \nand \nJava. \nThis \nspecies \nlived \nabout \n2 \nmillion \nyears \nago.... \n\nThe \noldest \nremains \nof \na \ntiger-\nlike \ncat, \ncalled \nPanthera \npalaeosinensis, \nhave \nbeen \nfound \nin \nChina \nand \nJava. \nThis \nspecies \nlived \nabout \n2 \nmillion \nyears \nago.... \n\nGenerated pairs \nHashing Layers \n\nH T \n\nHashing Layers \n\nImage Representation Learning \n\nText Feature Extraction \n\nHashing Layers \n\nHashing Layers \n\nEarly \ntechnological \nprogress \nowed \nmuch \nto \nthe \nfirm \nof \nBroadwood. \nJohn \nBroadwood \njoined \nwith \nanother \nScot, \nRobert \nStodart, \nand \na \nDutchman, \nAmericus \nBackers \n... \n\nEarly \ntechnological \nprogress \nowed \nmuch \nto \nthe \nfirm \nof \nBroadwood. \nJohn \nBroadwood \njoined \nwith \nanother \nScot, \nRobert \nStodart, \nand \na \nDutchman, \nAmericus \nBackers \n... \n\nThe \noldest \nremains \nof \na \ntiger-\nlike \ncat, \ncalled \nPanthera \npalaeosinensis, \nhave \nbeen \nfound \nin \nChina \nand \nJava. \nThis \nspecies \nlived \nabout \n2 \nmillion \nyears \nago.... \n\nGenerated pairs \n\nThe \noldest \nremains \nof \na \ntiger-\nlike \ncat, \ncalled \nPanthera \npalaeosinensis, \nhave \nbeen \nfound \nin \nChina \nand \nJava. \nThis \nspecies \nlived \nabout \n2 \nmillion \nyears \nago.... \n\nThe \noldest \nremains \nof \na \ntiger-\nlike \ncat, \ncalled \nPanthera \npalaeosinensis, \nhave \nbeen \nfound \nin \nChina \nand \nJava. \nThis \nspecies \nlived \nabout \n2 \nmillion \nyears \nago.... \n\nTrue pairs \n\nThe \noldest \nremains \nof \na \ntiger-\nlike \ncat, \ncalled \nPanthera \npalaeosinensis, \nhave \nbeen \nfound \nin \nChina \nand \nJava. \nThis \nspecies \nlived \nabout \n2 \nmillion \nyears \nago.... \n\nThe \noldest \nremains \nof \na \ntiger-\nlike \ncat, \ncalled \nPanthera \npalaeosinensis, \nhave \nbeen \nfound \nin \nChina \nand \nJava. \nThis \nspecies \nlived \nabout \n2 \nmillion \nyears \nago.... \n\n\u00b7 \n\nRanking \nLoss \n\nRelevance \nScore \n\nMinimax Game \n\nDiscriminative Model D \n\nGenerative Model G \n\nLabeled data \nUnlabeled data \n\nH I \n\nH T \n\nClass1 Class2 \nClass1 Class2 \n\nFig. \n\nTABLE I THE\nIMAP SCORES OF TWO RETRIEVAL TASKS ON WIKIPEDIA DATASET WITH DIFFERENT LENGTH OF HASH CODES. MAP SCORES OF TWO RETRIEVAL TASKS ON NUSWIDE DATASET WITH DIFFERENT LENGTH OF HASH CODES.TABLE III THE MAP SCORES OF TWO RETRIEVAL TASKS ON MIRFLICKR DATASET WITH DIFFERENT LENGTH OF HASH CODES.Methods \nimage\u2192text \ntext\u2192image \n16 \n32 \n64 \n128 \n16 \n32 \n64 \n128 \nCVH [11] \n0.193 \n0.161 \n0.144 \n0.134 \n0.297 \n0.225 \n0.187 \n0.167 \nPDH [13] \n0.483 \n0.483 \n0.494 \n0.497 \n0.842 \n0.842 \n0.838 \n0.851 \nCMFH [14] \n0.439 \n0.496 \n0.473 \n0.461 \n0.484 \n0.548 \n0.573 \n0.568 \nCCQ [46] \n0.463 \n0.471 \n0.470 \n0.456 \n0.744 \n0.788 \n0.785 \n0.741 \nCMSSH [35] \n0.160 \n0.159 \n0.157 \n0.156 \n0.206 \n0.208 \n0.206 \n0.205 \nSCM orth [17] \n0.229 \n0.192 \n0.171 \n0.161 \n0.238 \n0.171 \n0.145 \n0.131 \nSCM seq [17] \n0.396 \n0.459 \n0.462 \n0.442 \n0.442 \n0.557 \n0.538 \n0.510 \nSePH [19] \n0.515 \n0.518 \n0.533 \n0.538 \n0.748 \n0.781 \n0.792 \n0.805 \nDCMH [50] \n0.475 \n0.508 \n0.507 \n0.503 \n0.819 \n0.828 \n0.788 \n0.720 \nSCH-GAN (Ours) \n0.525 \n0.530 \n0.551 \n0.546 \n0.860 \n0.876 \n0.889 \n0.888 \n\nTABLE II \nTHE Methods \nimage\u2192text \ntext\u2192image \n16 \n32 \n64 \n128 \n16 \n32 \n64 \n128 \nCVH [11] \n0.458 \n0.432 \n0.410 \n0.392 \n0.474 \n0.445 \n0.419 \n0.398 \nPDH [13] \n0.475 \n0.484 \n0.480 \n0.490 \n0.489 \n0.512 \n0.507 \n0.517 \nCMFH [14] \n0.517 \n0.550 \n0.547 \n0.520 \n0.439 \n0.416 \n0.377 \n0.349 \nCCQ [46] \n0.504 \n0.505 \n0.506 \n0.505 \n0.499 \n0.496 \n0.492 \n0.488 \nCMSSH [35] \n0.512 \n0.470 \n0.479 \n0.466 \n0.519 \n0.498 \n0.456 \n0.488 \nSCM orth [17] \n0.389 \n0.376 \n0.368 \n0.360 \n0.388 \n0.372 \n0.360 \n0.353 \nSCM seq [17] \n0.517 \n0.514 \n0.518 \n0.518 \n0.518 \n0.510 \n0.517 \n0.518 \nSePH [19] \n0.701 \n0.712 \n0.719 \n0.726 \n0.642 \n0.653 \n0.657 \n0.662 \nDCMH [50] \n0.631 \n0.653 \n0.653 \n0.671 \n0.702 \n0.695 \n0.694 \n0.693 \nSCH-GAN (Ours) \n0.713 \n0.726 \n0.734 \n0.748 \n0.741 \n0.743 \n0.771 \n0.779 \n\nMethods \nimage\u2192text \ntext\u2192image \n16 \n32 \n64 \n128 \n16 \n32 \n64 \n128 \nCVH [11] \n0.602 \n0.587 \n0.578 \n0.572 \n0.607 \n0.591 \n0.581 \n0.574 \nPDH [13] \n0.623 \n0.624 \n0.621 \n0.626 \n0.627 \n0.628 \n0.628 \n0.629 \nCMFH [14] \n0.659 \n0.660 \n0.663 \n0.653 \n0.611 \n0.606 \n0.575 \n0.563 \nCCQ [46] \n0.637 \n0.639 \n0.639 \n0.638 \n0.628 \n0.628 \n0.622 \n0.618 \nCMSSH [35] \n0.611 \n0.602 \n0.599 \n0.591 \n0.612 \n0.604 \n0.592 \n0.585 \nSCM orth [17] \n0.585 \n0.576 \n0.570 \n0.566 \n0.585 \n0.584 \n0.574 \n0.568 \nSCM seq [17] \n0.636 \n0.640 \n0.641 \n0.643 \n0.661 \n0.664 \n0.668 \n0.670 \nSePH [19] \n0.704 \n0.711 \n0.716 \n0.711 \n0.699 \n0.705 \n0.711 \n0.710 \nDCMH [50] \n0.721 \n0.729 \n0.735 \n0.731 \n0.764 \n0.771 \n0.774 \n0.760 \nSCH-GAN (Ours) \n0.739 \n0.747 \n0.755 \n0.769 \n0.775 \n0.790 \n0.798 \n0.799 \n\n\n\nTABLE IV COMPARISON\nIVBETWEEN PROPOSED APPROACH SCH-GAN AND BASELINE APPROACH DIS.image\u2192text \ntext\u2192image \n16 \n32 \n64 \n128 \n16 \n32 \n64 \n128 \n\nWikipedia \nDis \n0.508 \n0.494 \n0.510 \n0.510 \n0.859 \n0.847 \n0.873 \n0.858 \nSCH-GAN \n0.525 \n0.530 \n0.551 \n0.546 \n0.860 \n0.876 \n0.889 \n0.888 \n\nNUSWIDE \nDis \n0.611 \n0.659 \n0.673 \n0.646 \n0.654 \n0.691 \n0.705 \n0.694 \nSCH-GAN \n0.713 \n0.726 \n0.734 \n0.748 \n0.741 \n0.743 \n0.771 \n0.779 \n\nMIRFlickr \nDis \n0.627 \n0.713 \n0.719 \n0.717 \n0.667 \n0.743 \n0.753 \n0.751 \nSCH-GAN \n0.739 \n0.747 \n0.755 \n0.769 \n0.775 \n0.790 \n0.798 \n0.799 \n\n\nt , r)log(1 + exp(f \u03c6 (i U k , q j t )))(11)where k denotes the k-th image selected by current generative model when given a text query q j t . From the perspective of deep reinforcement learning, i U k is the action taken by policy logp \u03b8 (i U k |q j t , r) according to the environment q k t , and log(1 + exp(f \u03c6 (i U k , q j t ))) acts as the reward for corresponding action, which will encourage the generative model to select data close to margins. As illustrated inFigure 3, the rewards are calculated by the fixed discriminative network. We summarize the training process of proposed SCH-GAN in algorithm 1. It is noted that to simplify the descriptions, we take text query image task as an example in the training algorithm pseudo code.\nhttps://www.tensorflow.org\n\nSimilarity search in high dimensions via hashing. A Gionis, P Indyk, R Motwani, VLDB. 99A. Gionis, P. Indyk, and R. Motwani, \"Similarity search in high dimensions via hashing,\" in VLDB, vol. 99, 1999, pp. 518-529.\n\nHashing with graphs. W Liu, J Wang, S Kumar, S.-F Chang, Proceedings of the 28th international conference on machine learning (ICML-11). the 28th international conference on machine learning (ICML-11)Conference ProceedingsW. Liu, J. Wang, S. Kumar, and S.-F. Chang, \"Hashing with graphs,\" in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, Conference Proceedings, pp. 1-8.\n\nLearning to hash for indexing big data-a survey. J Wang, W Liu, S Kumar, S F Chang, Proceedings of the IEEE. 1041J. Wang, W. Liu, S. Kumar, and S. F. Chang, \"Learning to hash for indexing big data-a survey,\" Proceedings of the IEEE, vol. 104, no. 1, pp. 34-57, 2016.\n\nLocally linear hashing for extracting non-linear manifolds. G Irie, Z Li, X.-M Wu, S.-F Chang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionG. Irie, Z. Li, X.-M. Wu, and S.-F. Chang, \"Locally linear hashing for extracting non-linear manifolds,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 2115-2122.\n\nSimultaneous feature learning and hash coding with deep neural networks. H Lai, Y Pan, Y Liu, S Yan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionH. Lai, Y. Pan, Y. Liu, and S. Yan, \"Simultaneous feature learning and hash coding with deep neural networks,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3270-3278.\n\nSupervised hashing with kernels. W Liu, J Wang, R Ji, Y.-G Jiang, S.-F Chang, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEEW. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang, \"Supervised hashing with kernels,\" in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 2074-2081.\n\nSpectral hashing. Y Weiss, A Torralba, R Fergus, Advances in neural information processing systems. Y. Weiss, A. Torralba, and R. Fergus, \"Spectral hashing,\" in Advances in neural information processing systems, 2009, pp. 1753-1760.\n\nSemi-supervised hashing for scalable image retrieval. J Wang, S Kumar, S.-F Chang, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEEJ. Wang, S. Kumar, and S.-F. Chang, \"Semi-supervised hashing for scalable image retrieval,\" in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3424-3431.\n\nLarge-scale video hashing via structure learning. G Ye, D Liu, J Wang, S.-F Chang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionG. Ye, D. Liu, J. Wang, and S.-F. Chang, \"Large-scale video hashing via structure learning,\" in Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 2272-2279.\n\nRanking with local regression and global alignment for cross media retrieval. Y Yang, D Xu, F Nie, J Luo, Y Zhuang, Proceedings of the 17th ACM international conference on Multimedia. the 17th ACM international conference on MultimediaACMY. Yang, D. Xu, F. Nie, J. Luo, and Y. Zhuang, \"Ranking with local regression and global alignment for cross media retrieval,\" in Proceedings of the 17th ACM international conference on Multimedia. ACM, 2009, pp. 175-184.\n\nLearning hash functions for cross-view similarity search. S Kumar, R Udupa, IJCAI proceedings-international joint conference on artificial intelligence. 221360S. Kumar and R. Udupa, \"Learning hash functions for cross-view similarity search,\" in IJCAI proceedings-international joint conference on artificial intelligence, vol. 22, 2011, p. 1360.\n\nInter-media hashing for large-scale retrieval from heterogeneous data sources. J Song, Y Yang, Y Yang, Z Huang, H T Shen, Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. the 2013 ACM SIGMOD International Conference on Management of DataACMJ. Song, Y. Yang, Y. Yang, Z. Huang, and H. T. Shen, \"Inter-media hashing for large-scale retrieval from heterogeneous data sources,\" in Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. ACM, 2013, pp. 785-796.\n\nPredictable dual-view hashing. M Rastegari, J Choi, S Fakhraei, D Hal, L Davis, Proceedings of the 30th International Conference on Machine Learning (ICML-13). the 30th International Conference on Machine Learning (ICML-13)M. Rastegari, J. Choi, S. Fakhraei, D. Hal, and L. Davis, \"Predictable dual-view hashing,\" in Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013, pp. 1328-1336.\n\nLarge-scale cross-modality search via collective matrix factorization hashing. G Ding, Y Guo, J Zhou, Y Gao, IEEE Transactions on Image Processing. 2511G. Ding, Y. Guo, J. Zhou, and Y. Gao, \"Large-scale cross-modality search via collective matrix factorization hashing,\" IEEE Transactions on Image Processing, vol. 25, no. 11, pp. 5427-5440, 2016.\n\nCo-regularized hashing for multimodal data. Y Zhen, D.-Y Yeung, Advances in neural information processing systems. Y. Zhen and D.-Y. Yeung, \"Co-regularized hashing for multimodal data,\" in Advances in neural information processing systems, 2012, pp. 1376- 1384.\n\nScalable heterogeneous translated hashing. Y Wei, Y Song, Y Zhen, B Liu, Q Yang, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningACMY. Wei, Y. Song, Y. Zhen, B. Liu, and Q. Yang, \"Scalable heterogeneous translated hashing,\" in Proceedings of the 20th ACM SIGKDD interna- tional conference on Knowledge discovery and data mining. ACM, 2014, pp. 791-800.\n\nLarge-scale supervised multimodal hashing with semantic correlation maximization. D Zhang, W.-J Li, AAAI. 17D. Zhang and W.-J. Li, \"Large-scale supervised multimodal hashing with semantic correlation maximization,\" in AAAI, vol. 1, 2014, p. 7.\n\nQuantized correlation hashing for fast cross-modal search. B Wu, Q Yang, W.-S Zheng, Y Wang, J Wang, IJCAI. B. Wu, Q. Yang, W.-S. Zheng, Y. Wang, and J. Wang, \"Quantized correlation hashing for fast cross-modal search,\" in IJCAI, 2015, pp. 3946-3952.\n\nSemantics-preserving hashing for cross-view retrieval. Z Lin, G Ding, M Hu, J Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZ. Lin, G. Ding, M. Hu, and J. Wang, \"Semantics-preserving hashing for cross-view retrieval,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3864-3872.\n\nCross-modal retrieval via deep and bidirectional representation learning. Y He, S Xiang, C Kang, J Wang, C Pan, IEEE Transactions on Multimedia. 187Y. He, S. Xiang, C. Kang, J. Wang, and C. Pan, \"Cross-modal retrieval via deep and bidirectional representation learning,\" IEEE Transactions on Multimedia, vol. 18, no. 7, pp. 1363-1377, 2016.\n\nLearning compact hash codes for multimodal representations using orthogonal deep structure. D Wang, P Cui, M Ou, W Zhu, IEEE Transactions on Multimedia. 179D. Wang, P. Cui, M. Ou, and W. Zhu, \"Learning compact hash codes for multimodal representations using orthogonal deep structure,\" IEEE Transactions on Multimedia, vol. 17, no. 9, pp. 1404-1416, 2015.\n\nCrossmedia hashing with neural networks. Y Zhuang, Z Yu, W Wang, F Wu, S Tang, J Shao, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaACMY. Zhuang, Z. Yu, W. Wang, F. Wu, S. Tang, and J. Shao, \"Cross- media hashing with neural networks,\" in Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 901-904.\n\nDeep visualsemantic hashing for cross-modal retrieval. Y Cao, M Long, J Wang, Q Yang, P S Yuy, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining1445Y. Cao, M. Long, J. Wang, Q. Yang, and P. S. Yuy, \"Deep visual- semantic hashing for cross-modal retrieval,\" in Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, p. 1445.\n\nCorrelation autoencoder hashing for supervised cross-modal search. Y Cao, M Long, J Wang, H Zhu, Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval. the 2016 ACM on International Conference on Multimedia RetrievalACMY. Cao, M. Long, J. Wang, and H. Zhu, \"Correlation autoencoder hashing for supervised cross-modal search,\" in Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval. ACM, 2016, pp. 197-204.\n\nCanonical correlation analysis: An overview with application to learning methods. D R Hardoon, S Szedmak, J Shawe-Taylor, Neural computation. 1612D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor, \"Canonical correlation analysis: An overview with application to learning methods,\" Neural computation, vol. 16, no. 12, pp. 2639-2664, 2004.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" in Advances in neural infor- mation processing systems, 2012, pp. 1097-1105.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5217553Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature, vol. 521, no. 7553, pp. 436-444, 2015.\n\nSemi-supervised learning literature survey. X Zhu, X. Zhu, \"Semi-supervised learning literature survey,\" 2005.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \"Generative adversarial nets,\" in Advances in neural information processing systems, 2014, pp. 2672- 2680.\n\nInfogan: Interpretable representation learning by information maximizing generative adversarial nets. X Chen, Y Duan, R Houthooft, J Schulman, I Sutskever, P Abbeel, Advances in Neural Information Processing Systems. X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, \"Infogan: Interpretable representation learning by informa- tion maximizing generative adversarial nets,\" in Advances in Neural Information Processing Systems, 2016, pp. 2172-2180.\n\nUnsupervised learning for physical interaction through video prediction. C Finn, I Goodfellow, S Levine, Advances in Neural Information Processing Systems. C. Finn, I. Goodfellow, and S. Levine, \"Unsupervised learning for physical interaction through video prediction,\" in Advances in Neural Information Processing Systems, 2016, pp. 64-72.\n\nPerceptual generative adversarial networks for small object detection. J Li, X Liang, Y Wei, T Xu, J Feng, S Yan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Li, X. Liang, Y. Wei, T. Xu, J. Feng, and S. Yan, \"Perceptual gen- erative adversarial networks for small object detection,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1222-1230.\n\nLatent semantic sparse hashing for crossmodal similarity search. J Zhou, G Ding, Y Guo, Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. the 37th international ACM SIGIR conference on Research & development in information retrievalACMJ. Zhou, G. Ding, and Y. Guo, \"Latent semantic sparse hashing for cross- modal similarity search,\" in Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 2014, pp. 415-424.\n\nSemantic topic multimodal hashing for cross-media retrieval. D Wang, X Gao, X Wang, L He, IJCAI. D. Wang, X. Gao, X. Wang, and L. He, \"Semantic topic multimodal hashing for cross-media retrieval,\" in IJCAI, 2015, pp. 3890-3896.\n\nData fusion through cross-modality metric learning using similarity-sensitive hashing. M M Bronstein, A M Bronstein, F Michel, N Paragios, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEEM. M. Bronstein, A. M. Bronstein, F. Michel, and N. Paragios, \"Data fusion through cross-modality metric learning using similarity-sensitive hashing,\" in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3594-3601.\n\nIterative multi-view hashing for cross media indexing. Y Hu, Z Jin, H Ren, D Cai, X He, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaACMY. Hu, Z. Jin, H. Ren, D. Cai, and X. He, \"Iterative multi-view hashing for cross media indexing,\" in Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 527-536.\n\nDeep multimodal hashing with orthogonal regularization. D Wang, P Cui, M Ou, W Zhu, IJCAI. D. Wang, P. Cui, M. Ou, and W. Zhu, \"Deep multimodal hashing with orthogonal regularization,\" in IJCAI, 2015, pp. 2291-2297.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:1511.06434arXiv preprintA. Radford, L. Metz, and S. Chintala, \"Unsupervised representation learning with deep convolutional generative adversarial networks,\" arXiv preprint arXiv:1511.06434, 2015.\n\nConditional generative adversarial nets. M Mirza, S Osindero, arXiv:1411.1784arXiv preprintM. Mirza and S. Osindero, \"Conditional generative adversarial nets,\" arXiv preprint arXiv:1411.1784, 2014.\n\nGenerative adversarial text to image synthesis. S Reed, Z Akata, X Yan, L Logeswaran, B Schiele, H Lee, Proceedings of the 33rd International Conference on International Conference on Machine Learning. the 33rd International Conference on International Conference on Machine Learning48S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, \"Generative adversarial text to image synthesis,\" in Proceedings of the 33rd International Conference on International Conference on Machine Learning-Volume 48, 2016, pp. 1060-1069.\n\nConditional image synthesis with auxiliary classifier gans. A Odena, C Olah, J Shlens, arXiv:1610.09585arXiv preprintA. Odena, C. Olah, and J. Shlens, \"Conditional image synthesis with auxiliary classifier gans,\" arXiv preprint arXiv:1610.09585, 2016.\n\nOn the role of correlation and abstraction in cross-modal multimedia retrieval. J C Pereira, E Coviello, G Doyle, N Rasiwasia, G R Lanckriet, R Levy, N Vasconcelos, IEEE Transactions on Pattern Analysis and Machine Intelligence. 363J. C. Pereira, E. Coviello, G. Doyle, N. Rasiwasia, G. R. Lanckriet, R. Levy, and N. Vasconcelos, \"On the role of correlation and abstraction in cross-modal multimedia retrieval,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 3, pp. 521-535, 2014.\n\nNus-wide: a real-world web image database from national university of singapore. T.-S Chua, J Tang, R Hong, H Li, Z Luo, Y Zheng, Proceedings of the ACM international conference on image and video retrieval. the ACM international conference on image and video retrievalACM48T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, \"Nus-wide: a real-world web image database from national university of singapore,\" in Proceedings of the ACM international conference on image and video retrieval. ACM, 2009, p. 48.\n\nThe mir flickr retrieval evaluation. M J Huiskes, M S Lew, Proceedings of the 1st ACM international conference on Multimedia information retrieval. the 1st ACM international conference on Multimedia information retrievalACMM. J. Huiskes and M. S. Lew, \"The mir flickr retrieval evaluation,\" in Proceedings of the 1st ACM international conference on Multimedia information retrieval. ACM, 2008, pp. 39-43.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" arXiv preprint arXiv:1409.1556, 2014.\n\nComposite correlation quantization for efficient multimodal retrieval. M Long, Y Cao, J Wang, P S Yu, Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. the 39th International ACM SIGIR conference on Research and Development in Information RetrievalACMM. Long, Y. Cao, J. Wang, and P. S. Yu, \"Composite correlation quan- tization for efficient multimodal retrieval,\" in Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2016, pp. 579-588.\n\nA probabilistic model for multimodal hash function learning. Y Zhen, D.-Y Yeung, Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. the 18th ACM SIGKDD international conference on Knowledge discovery and data miningACMY. Zhen and D.-Y. Yeung, \"A probabilistic model for multimodal hash function learning,\" in Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012, pp. 940-948.\n\nEffective multi-modal retrieval based on stacked auto-encoders. W Wang, B C Ooi, X Yang, D Zhang, Y Zhuang, Proceedings of the VLDB Endowment. the VLDB Endowment7W. Wang, B. C. Ooi, X. Yang, D. Zhang, and Y. Zhuang, \"Effective multi-modal retrieval based on stacked auto-encoders,\" Proceedings of the VLDB Endowment, vol. 7, no. 8, pp. 649-660, 2014.\n\nEffective deep learning-based multi-modal retrieval. W Wang, X Yang, B C Ooi, D Zhang, Y Zhuang, The VLDB Journal. 251W. Wang, X. Yang, B. C. Ooi, D. Zhang, and Y. Zhuang, \"Effective deep learning-based multi-modal retrieval,\" The VLDB Journal, vol. 25, no. 1, pp. 79-101, 2016.\n\nDeep cross-modal hashing. Q.-Y Jiang, W.-J Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionQ.-Y. Jiang and W.-J. Li, \"Deep cross-modal hashing,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 3232-3240.\n", "annotations": {"author": "[{\"end\":93,\"start\":82},{\"end\":105,\"start\":94},{\"end\":120,\"start\":106}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":87},{\"end\":104,\"start\":100},{\"end\":119,\"start\":115}]", "author_first_name": "[{\"end\":86,\"start\":82},{\"end\":99,\"start\":94},{\"end\":114,\"start\":106}]", "author_affiliation": null, "title": "[{\"end\":79,\"start\":1},{\"end\":199,\"start\":121}]", "venue": null, "abstract": "[{\"end\":2539,\"start\":278}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3154,\"start\":3151},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4457,\"start\":4454},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7109,\"start\":7106},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7703,\"start\":7700},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7809,\"start\":7806},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7833,\"start\":7830},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8405,\"start\":8401},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8707,\"start\":8703},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8713,\"start\":8709},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9230,\"start\":9226},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9327,\"start\":9323},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9359,\"start\":9355},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9401,\"start\":9397},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9457,\"start\":9453},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9685,\"start\":9681},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9730,\"start\":9726},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9776,\"start\":9772},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9818,\"start\":9814},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9863,\"start\":9859},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9960,\"start\":9956},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9988,\"start\":9984},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10188,\"start\":10184},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10229,\"start\":10225},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10281,\"start\":10277},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10823,\"start\":10819},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11184,\"start\":11180},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11190,\"start\":11186},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11277,\"start\":11273},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11300,\"start\":11296},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11326,\"start\":11322},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14517,\"start\":14514},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15150,\"start\":15146},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15269,\"start\":15265},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15422,\"start\":15418},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15484,\"start\":15481},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15649,\"start\":15645},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15933,\"start\":15929},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16114,\"start\":16110},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16372,\"start\":16368},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17125,\"start\":17121},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17287,\"start\":17283},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17469,\"start\":17465},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17626,\"start\":17622},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17846,\"start\":17842},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18031,\"start\":18027},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18232,\"start\":18228},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18791,\"start\":18787},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18819,\"start\":18815},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18890,\"start\":18886},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19103,\"start\":19099},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19109,\"start\":19105},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19313,\"start\":19309},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19517,\"start\":19513},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19998,\"start\":19994},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20326,\"start\":20322},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20980,\"start\":20976},{\"end\":21108,\"start\":21087},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21136,\"start\":21132},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21363,\"start\":21359},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21486,\"start\":21482},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21607,\"start\":21603},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21633,\"start\":21629},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29287,\"start\":29283},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29293,\"start\":29289},{\"end\":35218,\"start\":35216},{\"end\":35276,\"start\":35274},{\"end\":35287,\"start\":35285},{\"end\":35304,\"start\":35302},{\"end\":35443,\"start\":35440},{\"end\":35521,\"start\":35518},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36967,\"start\":36963},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36981,\"start\":36977},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37000,\"start\":36996},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37067,\"start\":37063},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":37353,\"start\":37349},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37444,\"start\":37440},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":37683,\"start\":37679},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37806,\"start\":37802},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":38306,\"start\":38302},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":38475,\"start\":38471},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":40773,\"start\":40769},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":40783,\"start\":40779},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":40794,\"start\":40790},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":40807,\"start\":40803},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":40838,\"start\":40834},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40848,\"start\":40844},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":40862,\"start\":40858},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":40905,\"start\":40901},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":40966,\"start\":40962},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":41000,\"start\":40997},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41111,\"start\":41107},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41294,\"start\":41290},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":41436,\"start\":41432},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":41666,\"start\":41662},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":41770,\"start\":41766},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41901,\"start\":41897},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":42147,\"start\":42143},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":45297,\"start\":45293},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":45834,\"start\":45830},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":51240,\"start\":51236}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50154,\"start\":49867},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50498,\"start\":50155},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50803,\"start\":50499},{\"attributes\":{\"id\":\"fig_4\"},\"end\":51110,\"start\":50804},{\"attributes\":{\"id\":\"fig_5\"},\"end\":51259,\"start\":51111},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54971,\"start\":51260},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":57530,\"start\":54972},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":58085,\"start\":57531}]", "paragraph": "[{\"end\":4934,\"start\":2662},{\"end\":5017,\"start\":4936},{\"end\":5451,\"start\":5037},{\"end\":5637,\"start\":5460},{\"end\":5751,\"start\":5639},{\"end\":5798,\"start\":5753},{\"end\":5954,\"start\":5800},{\"end\":6501,\"start\":5964},{\"end\":7616,\"start\":6547},{\"end\":8406,\"start\":7618},{\"end\":12143,\"start\":8408},{\"end\":13956,\"start\":12145},{\"end\":14237,\"start\":13958},{\"end\":14386,\"start\":14259},{\"end\":15039,\"start\":14413},{\"end\":16892,\"start\":15041},{\"end\":18614,\"start\":16894},{\"end\":19631,\"start\":18616},{\"end\":20247,\"start\":19633},{\"end\":22186,\"start\":20285},{\"end\":22632,\"start\":22188},{\"end\":23381,\"start\":22663},{\"end\":23864,\"start\":23397},{\"end\":24113,\"start\":24002},{\"end\":24781,\"start\":24257},{\"end\":25097,\"start\":24783},{\"end\":25301,\"start\":25122},{\"end\":26166,\"start\":25303},{\"end\":26672,\"start\":26200},{\"end\":27462,\"start\":26736},{\"end\":27846,\"start\":27464},{\"end\":27984,\"start\":27872},{\"end\":28396,\"start\":27986},{\"end\":29341,\"start\":28398},{\"end\":29573,\"start\":29474},{\"end\":29976,\"start\":29706},{\"end\":30765,\"start\":30073},{\"end\":31059,\"start\":30951},{\"end\":32239,\"start\":31243},{\"end\":32556,\"start\":32259},{\"end\":33150,\"start\":32558},{\"end\":33516,\"start\":33305},{\"end\":33859,\"start\":33518},{\"end\":34808,\"start\":34194},{\"end\":35381,\"start\":35156},{\"end\":35651,\"start\":35383},{\"end\":36400,\"start\":35712},{\"end\":36856,\"start\":36420},{\"end\":37041,\"start\":36871},{\"end\":38795,\"start\":37043},{\"end\":38926,\"start\":38841},{\"end\":39819,\"start\":38928},{\"end\":39939,\"start\":39821},{\"end\":40611,\"start\":39973},{\"end\":40954,\"start\":40637},{\"end\":42756,\"start\":40956},{\"end\":43004,\"start\":42786},{\"end\":43379,\"start\":43006},{\"end\":44425,\"start\":43381},{\"end\":44583,\"start\":44427},{\"end\":46355,\"start\":44631},{\"end\":46855,\"start\":46357},{\"end\":47245,\"start\":46883},{\"end\":47689,\"start\":47247},{\"end\":48422,\"start\":47691},{\"end\":49569,\"start\":48440},{\"end\":49866,\"start\":49571}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":24001,\"start\":23865},{\"attributes\":{\"id\":\"formula_1\"},\"end\":24256,\"start\":24114},{\"attributes\":{\"id\":\"formula_2\"},\"end\":26199,\"start\":26167},{\"attributes\":{\"id\":\"formula_3\"},\"end\":26735,\"start\":26673},{\"attributes\":{\"id\":\"formula_4\"},\"end\":29473,\"start\":29342},{\"attributes\":{\"id\":\"formula_5\"},\"end\":29705,\"start\":29574},{\"attributes\":{\"id\":\"formula_6\"},\"end\":30072,\"start\":29977},{\"attributes\":{\"id\":\"formula_7\"},\"end\":30950,\"start\":30766},{\"attributes\":{\"id\":\"formula_8\"},\"end\":31242,\"start\":31060},{\"attributes\":{\"id\":\"formula_9\"},\"end\":33304,\"start\":33151},{\"attributes\":{\"id\":\"formula_10\"},\"end\":34193,\"start\":33860},{\"attributes\":{\"id\":\"formula_11\"},\"end\":35155,\"start\":34809},{\"attributes\":{\"id\":\"formula_12\"},\"end\":39972,\"start\":39940}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44739,\"start\":44727},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45566,\"start\":45550},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":48129,\"start\":48121}]", "section_header": "[{\"end\":2660,\"start\":2541},{\"end\":5035,\"start\":5020},{\"end\":5458,\"start\":5454},{\"end\":5962,\"start\":5957},{\"end\":6528,\"start\":6504},{\"end\":6545,\"start\":6531},{\"end\":14257,\"start\":14240},{\"end\":14411,\"start\":14389},{\"end\":20283,\"start\":20250},{\"end\":22661,\"start\":22635},{\"end\":23395,\"start\":23384},{\"end\":25120,\"start\":25100},{\"end\":27870,\"start\":27849},{\"end\":32257,\"start\":32242},{\"end\":35710,\"start\":35654},{\"end\":36418,\"start\":36403},{\"end\":36869,\"start\":36859},{\"end\":38839,\"start\":38798},{\"end\":40635,\"start\":40614},{\"end\":42784,\"start\":42759},{\"end\":44629,\"start\":44586},{\"end\":46881,\"start\":46858},{\"end\":48438,\"start\":48425},{\"end\":50164,\"start\":50156},{\"end\":50508,\"start\":50500},{\"end\":50813,\"start\":50805},{\"end\":51117,\"start\":51112},{\"end\":51264,\"start\":51261},{\"end\":54984,\"start\":54973},{\"end\":57551,\"start\":57532}]", "table": "[{\"end\":54971,\"start\":51616},{\"end\":57530,\"start\":55272},{\"end\":58085,\"start\":57614}]", "figure_caption": "[{\"end\":50154,\"start\":49869},{\"end\":50498,\"start\":50166},{\"end\":50803,\"start\":50510},{\"end\":51110,\"start\":50815},{\"end\":51259,\"start\":51119},{\"end\":51616,\"start\":51266},{\"end\":55272,\"start\":54986},{\"end\":57614,\"start\":57554}]", "figure_ref": "[{\"end\":6731,\"start\":6725},{\"end\":21637,\"start\":21636},{\"end\":22737,\"start\":22729},{\"end\":25142,\"start\":25134},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32375,\"start\":32367},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32614,\"start\":32606},{\"end\":33575,\"start\":33569},{\"end\":42831,\"start\":42823},{\"end\":43447,\"start\":43439},{\"end\":46213,\"start\":46207},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":46565,\"start\":46550},{\"end\":47425,\"start\":47417}]", "bib_author_first_name": "[{\"end\":58911,\"start\":58910},{\"end\":58921,\"start\":58920},{\"end\":58930,\"start\":58929},{\"end\":59097,\"start\":59096},{\"end\":59104,\"start\":59103},{\"end\":59112,\"start\":59111},{\"end\":59124,\"start\":59120},{\"end\":59537,\"start\":59536},{\"end\":59545,\"start\":59544},{\"end\":59552,\"start\":59551},{\"end\":59561,\"start\":59560},{\"end\":59563,\"start\":59562},{\"end\":59816,\"start\":59815},{\"end\":59824,\"start\":59823},{\"end\":59833,\"start\":59829},{\"end\":59842,\"start\":59838},{\"end\":60274,\"start\":60273},{\"end\":60281,\"start\":60280},{\"end\":60288,\"start\":60287},{\"end\":60295,\"start\":60294},{\"end\":60691,\"start\":60690},{\"end\":60698,\"start\":60697},{\"end\":60706,\"start\":60705},{\"end\":60715,\"start\":60711},{\"end\":60727,\"start\":60723},{\"end\":61024,\"start\":61023},{\"end\":61033,\"start\":61032},{\"end\":61045,\"start\":61044},{\"end\":61294,\"start\":61293},{\"end\":61302,\"start\":61301},{\"end\":61314,\"start\":61310},{\"end\":61646,\"start\":61645},{\"end\":61652,\"start\":61651},{\"end\":61659,\"start\":61658},{\"end\":61670,\"start\":61666},{\"end\":62065,\"start\":62064},{\"end\":62073,\"start\":62072},{\"end\":62079,\"start\":62078},{\"end\":62086,\"start\":62085},{\"end\":62093,\"start\":62092},{\"end\":62506,\"start\":62505},{\"end\":62515,\"start\":62514},{\"end\":62874,\"start\":62873},{\"end\":62882,\"start\":62881},{\"end\":62890,\"start\":62889},{\"end\":62898,\"start\":62897},{\"end\":62907,\"start\":62906},{\"end\":62909,\"start\":62908},{\"end\":63345,\"start\":63344},{\"end\":63358,\"start\":63357},{\"end\":63366,\"start\":63365},{\"end\":63378,\"start\":63377},{\"end\":63385,\"start\":63384},{\"end\":63812,\"start\":63811},{\"end\":63820,\"start\":63819},{\"end\":63827,\"start\":63826},{\"end\":63835,\"start\":63834},{\"end\":64126,\"start\":64125},{\"end\":64137,\"start\":64133},{\"end\":64388,\"start\":64387},{\"end\":64395,\"start\":64394},{\"end\":64403,\"start\":64402},{\"end\":64411,\"start\":64410},{\"end\":64418,\"start\":64417},{\"end\":64916,\"start\":64915},{\"end\":64928,\"start\":64924},{\"end\":65138,\"start\":65137},{\"end\":65144,\"start\":65143},{\"end\":65155,\"start\":65151},{\"end\":65164,\"start\":65163},{\"end\":65172,\"start\":65171},{\"end\":65386,\"start\":65385},{\"end\":65393,\"start\":65392},{\"end\":65401,\"start\":65400},{\"end\":65407,\"start\":65406},{\"end\":65828,\"start\":65827},{\"end\":65834,\"start\":65833},{\"end\":65843,\"start\":65842},{\"end\":65851,\"start\":65850},{\"end\":65859,\"start\":65858},{\"end\":66188,\"start\":66187},{\"end\":66196,\"start\":66195},{\"end\":66203,\"start\":66202},{\"end\":66209,\"start\":66208},{\"end\":66494,\"start\":66493},{\"end\":66504,\"start\":66503},{\"end\":66510,\"start\":66509},{\"end\":66518,\"start\":66517},{\"end\":66524,\"start\":66523},{\"end\":66532,\"start\":66531},{\"end\":66914,\"start\":66913},{\"end\":66921,\"start\":66920},{\"end\":66929,\"start\":66928},{\"end\":66937,\"start\":66936},{\"end\":66945,\"start\":66944},{\"end\":66947,\"start\":66946},{\"end\":67421,\"start\":67420},{\"end\":67428,\"start\":67427},{\"end\":67436,\"start\":67435},{\"end\":67444,\"start\":67443},{\"end\":67897,\"start\":67896},{\"end\":67899,\"start\":67898},{\"end\":67910,\"start\":67909},{\"end\":67921,\"start\":67920},{\"end\":68217,\"start\":68216},{\"end\":68231,\"start\":68230},{\"end\":68244,\"start\":68243},{\"end\":68246,\"start\":68245},{\"end\":68514,\"start\":68513},{\"end\":68523,\"start\":68522},{\"end\":68533,\"start\":68532},{\"end\":68703,\"start\":68702},{\"end\":68800,\"start\":68799},{\"end\":68814,\"start\":68813},{\"end\":68831,\"start\":68830},{\"end\":68840,\"start\":68839},{\"end\":68846,\"start\":68845},{\"end\":68862,\"start\":68861},{\"end\":68871,\"start\":68870},{\"end\":68884,\"start\":68883},{\"end\":69261,\"start\":69260},{\"end\":69269,\"start\":69268},{\"end\":69277,\"start\":69276},{\"end\":69290,\"start\":69289},{\"end\":69302,\"start\":69301},{\"end\":69315,\"start\":69314},{\"end\":69705,\"start\":69704},{\"end\":69713,\"start\":69712},{\"end\":69727,\"start\":69726},{\"end\":70045,\"start\":70044},{\"end\":70051,\"start\":70050},{\"end\":70060,\"start\":70059},{\"end\":70067,\"start\":70066},{\"end\":70073,\"start\":70072},{\"end\":70081,\"start\":70080},{\"end\":70526,\"start\":70525},{\"end\":70534,\"start\":70533},{\"end\":70542,\"start\":70541},{\"end\":71056,\"start\":71055},{\"end\":71064,\"start\":71063},{\"end\":71071,\"start\":71070},{\"end\":71079,\"start\":71078},{\"end\":71311,\"start\":71310},{\"end\":71313,\"start\":71312},{\"end\":71326,\"start\":71325},{\"end\":71328,\"start\":71327},{\"end\":71341,\"start\":71340},{\"end\":71351,\"start\":71350},{\"end\":71750,\"start\":71749},{\"end\":71756,\"start\":71755},{\"end\":71763,\"start\":71762},{\"end\":71770,\"start\":71769},{\"end\":71777,\"start\":71776},{\"end\":72156,\"start\":72155},{\"end\":72164,\"start\":72163},{\"end\":72171,\"start\":72170},{\"end\":72177,\"start\":72176},{\"end\":72411,\"start\":72410},{\"end\":72422,\"start\":72421},{\"end\":72430,\"start\":72429},{\"end\":72687,\"start\":72686},{\"end\":72696,\"start\":72695},{\"end\":72893,\"start\":72892},{\"end\":72901,\"start\":72900},{\"end\":72910,\"start\":72909},{\"end\":72917,\"start\":72916},{\"end\":72931,\"start\":72930},{\"end\":72942,\"start\":72941},{\"end\":73439,\"start\":73438},{\"end\":73448,\"start\":73447},{\"end\":73456,\"start\":73455},{\"end\":73712,\"start\":73711},{\"end\":73714,\"start\":73713},{\"end\":73725,\"start\":73724},{\"end\":73737,\"start\":73736},{\"end\":73746,\"start\":73745},{\"end\":73759,\"start\":73758},{\"end\":73761,\"start\":73760},{\"end\":73774,\"start\":73773},{\"end\":73782,\"start\":73781},{\"end\":74228,\"start\":74224},{\"end\":74236,\"start\":74235},{\"end\":74244,\"start\":74243},{\"end\":74252,\"start\":74251},{\"end\":74258,\"start\":74257},{\"end\":74265,\"start\":74264},{\"end\":74697,\"start\":74696},{\"end\":74699,\"start\":74698},{\"end\":74710,\"start\":74709},{\"end\":74712,\"start\":74711},{\"end\":75134,\"start\":75133},{\"end\":75146,\"start\":75145},{\"end\":75398,\"start\":75397},{\"end\":75406,\"start\":75405},{\"end\":75413,\"start\":75412},{\"end\":75421,\"start\":75420},{\"end\":75423,\"start\":75422},{\"end\":75958,\"start\":75957},{\"end\":75969,\"start\":75965},{\"end\":76444,\"start\":76443},{\"end\":76452,\"start\":76451},{\"end\":76454,\"start\":76453},{\"end\":76461,\"start\":76460},{\"end\":76469,\"start\":76468},{\"end\":76478,\"start\":76477},{\"end\":76785,\"start\":76784},{\"end\":76793,\"start\":76792},{\"end\":76801,\"start\":76800},{\"end\":76803,\"start\":76802},{\"end\":76810,\"start\":76809},{\"end\":76819,\"start\":76818},{\"end\":77041,\"start\":77037},{\"end\":77053,\"start\":77049}]", "bib_author_last_name": "[{\"end\":58918,\"start\":58912},{\"end\":58927,\"start\":58922},{\"end\":58938,\"start\":58931},{\"end\":59101,\"start\":59098},{\"end\":59109,\"start\":59105},{\"end\":59118,\"start\":59113},{\"end\":59130,\"start\":59125},{\"end\":59542,\"start\":59538},{\"end\":59549,\"start\":59546},{\"end\":59558,\"start\":59553},{\"end\":59569,\"start\":59564},{\"end\":59821,\"start\":59817},{\"end\":59827,\"start\":59825},{\"end\":59836,\"start\":59834},{\"end\":59848,\"start\":59843},{\"end\":60278,\"start\":60275},{\"end\":60285,\"start\":60282},{\"end\":60292,\"start\":60289},{\"end\":60299,\"start\":60296},{\"end\":60695,\"start\":60692},{\"end\":60703,\"start\":60699},{\"end\":60709,\"start\":60707},{\"end\":60721,\"start\":60716},{\"end\":60733,\"start\":60728},{\"end\":61030,\"start\":61025},{\"end\":61042,\"start\":61034},{\"end\":61052,\"start\":61046},{\"end\":61299,\"start\":61295},{\"end\":61308,\"start\":61303},{\"end\":61320,\"start\":61315},{\"end\":61649,\"start\":61647},{\"end\":61656,\"start\":61653},{\"end\":61664,\"start\":61660},{\"end\":61676,\"start\":61671},{\"end\":62070,\"start\":62066},{\"end\":62076,\"start\":62074},{\"end\":62083,\"start\":62080},{\"end\":62090,\"start\":62087},{\"end\":62100,\"start\":62094},{\"end\":62512,\"start\":62507},{\"end\":62521,\"start\":62516},{\"end\":62879,\"start\":62875},{\"end\":62887,\"start\":62883},{\"end\":62895,\"start\":62891},{\"end\":62904,\"start\":62899},{\"end\":62914,\"start\":62910},{\"end\":63355,\"start\":63346},{\"end\":63363,\"start\":63359},{\"end\":63375,\"start\":63367},{\"end\":63382,\"start\":63379},{\"end\":63391,\"start\":63386},{\"end\":63817,\"start\":63813},{\"end\":63824,\"start\":63821},{\"end\":63832,\"start\":63828},{\"end\":63839,\"start\":63836},{\"end\":64131,\"start\":64127},{\"end\":64143,\"start\":64138},{\"end\":64392,\"start\":64389},{\"end\":64400,\"start\":64396},{\"end\":64408,\"start\":64404},{\"end\":64415,\"start\":64412},{\"end\":64423,\"start\":64419},{\"end\":64922,\"start\":64917},{\"end\":64931,\"start\":64929},{\"end\":65141,\"start\":65139},{\"end\":65149,\"start\":65145},{\"end\":65161,\"start\":65156},{\"end\":65169,\"start\":65165},{\"end\":65177,\"start\":65173},{\"end\":65390,\"start\":65387},{\"end\":65398,\"start\":65394},{\"end\":65404,\"start\":65402},{\"end\":65412,\"start\":65408},{\"end\":65831,\"start\":65829},{\"end\":65840,\"start\":65835},{\"end\":65848,\"start\":65844},{\"end\":65856,\"start\":65852},{\"end\":65863,\"start\":65860},{\"end\":66193,\"start\":66189},{\"end\":66200,\"start\":66197},{\"end\":66206,\"start\":66204},{\"end\":66213,\"start\":66210},{\"end\":66501,\"start\":66495},{\"end\":66507,\"start\":66505},{\"end\":66515,\"start\":66511},{\"end\":66521,\"start\":66519},{\"end\":66529,\"start\":66525},{\"end\":66537,\"start\":66533},{\"end\":66918,\"start\":66915},{\"end\":66926,\"start\":66922},{\"end\":66934,\"start\":66930},{\"end\":66942,\"start\":66938},{\"end\":66951,\"start\":66948},{\"end\":67425,\"start\":67422},{\"end\":67433,\"start\":67429},{\"end\":67441,\"start\":67437},{\"end\":67448,\"start\":67445},{\"end\":67907,\"start\":67900},{\"end\":67918,\"start\":67911},{\"end\":67934,\"start\":67922},{\"end\":68228,\"start\":68218},{\"end\":68241,\"start\":68232},{\"end\":68253,\"start\":68247},{\"end\":68520,\"start\":68515},{\"end\":68530,\"start\":68524},{\"end\":68540,\"start\":68534},{\"end\":68707,\"start\":68704},{\"end\":68811,\"start\":68801},{\"end\":68828,\"start\":68815},{\"end\":68837,\"start\":68832},{\"end\":68843,\"start\":68841},{\"end\":68859,\"start\":68847},{\"end\":68868,\"start\":68863},{\"end\":68881,\"start\":68872},{\"end\":68891,\"start\":68885},{\"end\":69266,\"start\":69262},{\"end\":69274,\"start\":69270},{\"end\":69287,\"start\":69278},{\"end\":69299,\"start\":69291},{\"end\":69312,\"start\":69303},{\"end\":69322,\"start\":69316},{\"end\":69710,\"start\":69706},{\"end\":69724,\"start\":69714},{\"end\":69734,\"start\":69728},{\"end\":70048,\"start\":70046},{\"end\":70057,\"start\":70052},{\"end\":70064,\"start\":70061},{\"end\":70070,\"start\":70068},{\"end\":70078,\"start\":70074},{\"end\":70085,\"start\":70082},{\"end\":70531,\"start\":70527},{\"end\":70539,\"start\":70535},{\"end\":70546,\"start\":70543},{\"end\":71061,\"start\":71057},{\"end\":71068,\"start\":71065},{\"end\":71076,\"start\":71072},{\"end\":71082,\"start\":71080},{\"end\":71323,\"start\":71314},{\"end\":71338,\"start\":71329},{\"end\":71348,\"start\":71342},{\"end\":71360,\"start\":71352},{\"end\":71753,\"start\":71751},{\"end\":71760,\"start\":71757},{\"end\":71767,\"start\":71764},{\"end\":71774,\"start\":71771},{\"end\":71780,\"start\":71778},{\"end\":72161,\"start\":72157},{\"end\":72168,\"start\":72165},{\"end\":72174,\"start\":72172},{\"end\":72181,\"start\":72178},{\"end\":72419,\"start\":72412},{\"end\":72427,\"start\":72423},{\"end\":72439,\"start\":72431},{\"end\":72693,\"start\":72688},{\"end\":72705,\"start\":72697},{\"end\":72898,\"start\":72894},{\"end\":72907,\"start\":72902},{\"end\":72914,\"start\":72911},{\"end\":72928,\"start\":72918},{\"end\":72939,\"start\":72932},{\"end\":72946,\"start\":72943},{\"end\":73445,\"start\":73440},{\"end\":73453,\"start\":73449},{\"end\":73463,\"start\":73457},{\"end\":73722,\"start\":73715},{\"end\":73734,\"start\":73726},{\"end\":73743,\"start\":73738},{\"end\":73756,\"start\":73747},{\"end\":73771,\"start\":73762},{\"end\":73779,\"start\":73775},{\"end\":73794,\"start\":73783},{\"end\":74233,\"start\":74229},{\"end\":74241,\"start\":74237},{\"end\":74249,\"start\":74245},{\"end\":74255,\"start\":74253},{\"end\":74262,\"start\":74259},{\"end\":74271,\"start\":74266},{\"end\":74707,\"start\":74700},{\"end\":74716,\"start\":74713},{\"end\":75143,\"start\":75135},{\"end\":75156,\"start\":75147},{\"end\":75403,\"start\":75399},{\"end\":75410,\"start\":75407},{\"end\":75418,\"start\":75414},{\"end\":75426,\"start\":75424},{\"end\":75963,\"start\":75959},{\"end\":75975,\"start\":75970},{\"end\":76449,\"start\":76445},{\"end\":76458,\"start\":76455},{\"end\":76466,\"start\":76462},{\"end\":76475,\"start\":76470},{\"end\":76485,\"start\":76479},{\"end\":76790,\"start\":76786},{\"end\":76798,\"start\":76794},{\"end\":76807,\"start\":76804},{\"end\":76816,\"start\":76811},{\"end\":76826,\"start\":76820},{\"end\":77047,\"start\":77042},{\"end\":77056,\"start\":77054}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1578969},\"end\":59073,\"start\":58860},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12833578},\"end\":59485,\"start\":59075},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1596835},\"end\":59753,\"start\":59487},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1917867},\"end\":60198,\"start\":59755},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1307328},\"end\":60655,\"start\":60200},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":357776},\"end\":61003,\"start\":60657},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":344349},\"end\":61237,\"start\":61005},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5817453},\"end\":61593,\"start\":61239},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9197380},\"end\":61984,\"start\":61595},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11622011},\"end\":62445,\"start\":61986},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3083275},\"end\":62792,\"start\":62447},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":207202940},\"end\":63311,\"start\":62794},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6806018},\"end\":63730,\"start\":63313},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":17574759},\"end\":64079,\"start\":63732},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2930059},\"end\":64342,\"start\":64081},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":756588},\"end\":64831,\"start\":64344},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":15968370},\"end\":65076,\"start\":64833},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6493814},\"end\":65328,\"start\":65078},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13742759},\"end\":65751,\"start\":65330},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":22518199},\"end\":66093,\"start\":65753},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14490133},\"end\":66450,\"start\":66095},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11488407},\"end\":66856,\"start\":66452},{\"attributes\":{\"id\":\"b22\"},\"end\":67351,\"start\":66858},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13205920},\"end\":67812,\"start\":67353},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202473},\"end\":68149,\"start\":67814},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":195908774},\"end\":68496,\"start\":68151},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1779661},\"end\":68656,\"start\":68498},{\"attributes\":{\"id\":\"b27\"},\"end\":68768,\"start\":68658},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1033682},\"end\":69156,\"start\":68770},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5002792},\"end\":69629,\"start\":69158},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2659157},\"end\":69971,\"start\":69631},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6704804},\"end\":70458,\"start\":69973},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":16170805},\"end\":70992,\"start\":70460},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":7758524},\"end\":71221,\"start\":70994},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":16578125},\"end\":71692,\"start\":71223},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":7123618},\"end\":72097,\"start\":71694},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":15565263},\"end\":72314,\"start\":72099},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b37\"},\"end\":72643,\"start\":72316},{\"attributes\":{\"doi\":\"arXiv:1411.1784\",\"id\":\"b38\"},\"end\":72842,\"start\":72645},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1563370},\"end\":73376,\"start\":72844},{\"attributes\":{\"doi\":\"arXiv:1610.09585\",\"id\":\"b40\"},\"end\":73629,\"start\":73378},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":15512280},\"end\":74141,\"start\":73631},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":6483070},\"end\":74657,\"start\":74143},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":14040310},\"end\":75063,\"start\":74659},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b44\"},\"end\":75324,\"start\":75065},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":207237568},\"end\":75894,\"start\":75326},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4843878},\"end\":76377,\"start\":75896},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1615784},\"end\":76729,\"start\":76379},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":16741401},\"end\":77009,\"start\":76731},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":206595579},\"end\":77355,\"start\":77011}]", "bib_title": "[{\"end\":58908,\"start\":58860},{\"end\":59094,\"start\":59075},{\"end\":59534,\"start\":59487},{\"end\":59813,\"start\":59755},{\"end\":60271,\"start\":60200},{\"end\":60688,\"start\":60657},{\"end\":61021,\"start\":61005},{\"end\":61291,\"start\":61239},{\"end\":61643,\"start\":61595},{\"end\":62062,\"start\":61986},{\"end\":62503,\"start\":62447},{\"end\":62871,\"start\":62794},{\"end\":63342,\"start\":63313},{\"end\":63809,\"start\":63732},{\"end\":64123,\"start\":64081},{\"end\":64385,\"start\":64344},{\"end\":64913,\"start\":64833},{\"end\":65135,\"start\":65078},{\"end\":65383,\"start\":65330},{\"end\":65825,\"start\":65753},{\"end\":66185,\"start\":66095},{\"end\":66491,\"start\":66452},{\"end\":66911,\"start\":66858},{\"end\":67418,\"start\":67353},{\"end\":67894,\"start\":67814},{\"end\":68214,\"start\":68151},{\"end\":68511,\"start\":68498},{\"end\":68797,\"start\":68770},{\"end\":69258,\"start\":69158},{\"end\":69702,\"start\":69631},{\"end\":70042,\"start\":69973},{\"end\":70523,\"start\":70460},{\"end\":71053,\"start\":70994},{\"end\":71308,\"start\":71223},{\"end\":71747,\"start\":71694},{\"end\":72153,\"start\":72099},{\"end\":72890,\"start\":72844},{\"end\":73709,\"start\":73631},{\"end\":74222,\"start\":74143},{\"end\":74694,\"start\":74659},{\"end\":75395,\"start\":75326},{\"end\":75955,\"start\":75896},{\"end\":76441,\"start\":76379},{\"end\":76782,\"start\":76731},{\"end\":77035,\"start\":77011}]", "bib_author": "[{\"end\":58920,\"start\":58910},{\"end\":58929,\"start\":58920},{\"end\":58940,\"start\":58929},{\"end\":59103,\"start\":59096},{\"end\":59111,\"start\":59103},{\"end\":59120,\"start\":59111},{\"end\":59132,\"start\":59120},{\"end\":59544,\"start\":59536},{\"end\":59551,\"start\":59544},{\"end\":59560,\"start\":59551},{\"end\":59571,\"start\":59560},{\"end\":59823,\"start\":59815},{\"end\":59829,\"start\":59823},{\"end\":59838,\"start\":59829},{\"end\":59850,\"start\":59838},{\"end\":60280,\"start\":60273},{\"end\":60287,\"start\":60280},{\"end\":60294,\"start\":60287},{\"end\":60301,\"start\":60294},{\"end\":60697,\"start\":60690},{\"end\":60705,\"start\":60697},{\"end\":60711,\"start\":60705},{\"end\":60723,\"start\":60711},{\"end\":60735,\"start\":60723},{\"end\":61032,\"start\":61023},{\"end\":61044,\"start\":61032},{\"end\":61054,\"start\":61044},{\"end\":61301,\"start\":61293},{\"end\":61310,\"start\":61301},{\"end\":61322,\"start\":61310},{\"end\":61651,\"start\":61645},{\"end\":61658,\"start\":61651},{\"end\":61666,\"start\":61658},{\"end\":61678,\"start\":61666},{\"end\":62072,\"start\":62064},{\"end\":62078,\"start\":62072},{\"end\":62085,\"start\":62078},{\"end\":62092,\"start\":62085},{\"end\":62102,\"start\":62092},{\"end\":62514,\"start\":62505},{\"end\":62523,\"start\":62514},{\"end\":62881,\"start\":62873},{\"end\":62889,\"start\":62881},{\"end\":62897,\"start\":62889},{\"end\":62906,\"start\":62897},{\"end\":62916,\"start\":62906},{\"end\":63357,\"start\":63344},{\"end\":63365,\"start\":63357},{\"end\":63377,\"start\":63365},{\"end\":63384,\"start\":63377},{\"end\":63393,\"start\":63384},{\"end\":63819,\"start\":63811},{\"end\":63826,\"start\":63819},{\"end\":63834,\"start\":63826},{\"end\":63841,\"start\":63834},{\"end\":64133,\"start\":64125},{\"end\":64145,\"start\":64133},{\"end\":64394,\"start\":64387},{\"end\":64402,\"start\":64394},{\"end\":64410,\"start\":64402},{\"end\":64417,\"start\":64410},{\"end\":64425,\"start\":64417},{\"end\":64924,\"start\":64915},{\"end\":64933,\"start\":64924},{\"end\":65143,\"start\":65137},{\"end\":65151,\"start\":65143},{\"end\":65163,\"start\":65151},{\"end\":65171,\"start\":65163},{\"end\":65179,\"start\":65171},{\"end\":65392,\"start\":65385},{\"end\":65400,\"start\":65392},{\"end\":65406,\"start\":65400},{\"end\":65414,\"start\":65406},{\"end\":65833,\"start\":65827},{\"end\":65842,\"start\":65833},{\"end\":65850,\"start\":65842},{\"end\":65858,\"start\":65850},{\"end\":65865,\"start\":65858},{\"end\":66195,\"start\":66187},{\"end\":66202,\"start\":66195},{\"end\":66208,\"start\":66202},{\"end\":66215,\"start\":66208},{\"end\":66503,\"start\":66493},{\"end\":66509,\"start\":66503},{\"end\":66517,\"start\":66509},{\"end\":66523,\"start\":66517},{\"end\":66531,\"start\":66523},{\"end\":66539,\"start\":66531},{\"end\":66920,\"start\":66913},{\"end\":66928,\"start\":66920},{\"end\":66936,\"start\":66928},{\"end\":66944,\"start\":66936},{\"end\":66953,\"start\":66944},{\"end\":67427,\"start\":67420},{\"end\":67435,\"start\":67427},{\"end\":67443,\"start\":67435},{\"end\":67450,\"start\":67443},{\"end\":67909,\"start\":67896},{\"end\":67920,\"start\":67909},{\"end\":67936,\"start\":67920},{\"end\":68230,\"start\":68216},{\"end\":68243,\"start\":68230},{\"end\":68255,\"start\":68243},{\"end\":68522,\"start\":68513},{\"end\":68532,\"start\":68522},{\"end\":68542,\"start\":68532},{\"end\":68709,\"start\":68702},{\"end\":68813,\"start\":68799},{\"end\":68830,\"start\":68813},{\"end\":68839,\"start\":68830},{\"end\":68845,\"start\":68839},{\"end\":68861,\"start\":68845},{\"end\":68870,\"start\":68861},{\"end\":68883,\"start\":68870},{\"end\":68893,\"start\":68883},{\"end\":69268,\"start\":69260},{\"end\":69276,\"start\":69268},{\"end\":69289,\"start\":69276},{\"end\":69301,\"start\":69289},{\"end\":69314,\"start\":69301},{\"end\":69324,\"start\":69314},{\"end\":69712,\"start\":69704},{\"end\":69726,\"start\":69712},{\"end\":69736,\"start\":69726},{\"end\":70050,\"start\":70044},{\"end\":70059,\"start\":70050},{\"end\":70066,\"start\":70059},{\"end\":70072,\"start\":70066},{\"end\":70080,\"start\":70072},{\"end\":70087,\"start\":70080},{\"end\":70533,\"start\":70525},{\"end\":70541,\"start\":70533},{\"end\":70548,\"start\":70541},{\"end\":71063,\"start\":71055},{\"end\":71070,\"start\":71063},{\"end\":71078,\"start\":71070},{\"end\":71084,\"start\":71078},{\"end\":71325,\"start\":71310},{\"end\":71340,\"start\":71325},{\"end\":71350,\"start\":71340},{\"end\":71362,\"start\":71350},{\"end\":71755,\"start\":71749},{\"end\":71762,\"start\":71755},{\"end\":71769,\"start\":71762},{\"end\":71776,\"start\":71769},{\"end\":71782,\"start\":71776},{\"end\":72163,\"start\":72155},{\"end\":72170,\"start\":72163},{\"end\":72176,\"start\":72170},{\"end\":72183,\"start\":72176},{\"end\":72421,\"start\":72410},{\"end\":72429,\"start\":72421},{\"end\":72441,\"start\":72429},{\"end\":72695,\"start\":72686},{\"end\":72707,\"start\":72695},{\"end\":72900,\"start\":72892},{\"end\":72909,\"start\":72900},{\"end\":72916,\"start\":72909},{\"end\":72930,\"start\":72916},{\"end\":72941,\"start\":72930},{\"end\":72948,\"start\":72941},{\"end\":73447,\"start\":73438},{\"end\":73455,\"start\":73447},{\"end\":73465,\"start\":73455},{\"end\":73724,\"start\":73711},{\"end\":73736,\"start\":73724},{\"end\":73745,\"start\":73736},{\"end\":73758,\"start\":73745},{\"end\":73773,\"start\":73758},{\"end\":73781,\"start\":73773},{\"end\":73796,\"start\":73781},{\"end\":74235,\"start\":74224},{\"end\":74243,\"start\":74235},{\"end\":74251,\"start\":74243},{\"end\":74257,\"start\":74251},{\"end\":74264,\"start\":74257},{\"end\":74273,\"start\":74264},{\"end\":74709,\"start\":74696},{\"end\":74718,\"start\":74709},{\"end\":75145,\"start\":75133},{\"end\":75158,\"start\":75145},{\"end\":75405,\"start\":75397},{\"end\":75412,\"start\":75405},{\"end\":75420,\"start\":75412},{\"end\":75428,\"start\":75420},{\"end\":75965,\"start\":75957},{\"end\":75977,\"start\":75965},{\"end\":76451,\"start\":76443},{\"end\":76460,\"start\":76451},{\"end\":76468,\"start\":76460},{\"end\":76477,\"start\":76468},{\"end\":76487,\"start\":76477},{\"end\":76792,\"start\":76784},{\"end\":76800,\"start\":76792},{\"end\":76809,\"start\":76800},{\"end\":76818,\"start\":76809},{\"end\":76828,\"start\":76818},{\"end\":77049,\"start\":77037},{\"end\":77058,\"start\":77049}]", "bib_venue": "[{\"end\":59275,\"start\":59212},{\"end\":59991,\"start\":59929},{\"end\":60442,\"start\":60380},{\"end\":61799,\"start\":61747},{\"end\":62221,\"start\":62170},{\"end\":63065,\"start\":62999},{\"end\":63536,\"start\":63473},{\"end\":64608,\"start\":64525},{\"end\":65555,\"start\":65493},{\"end\":66658,\"start\":66607},{\"end\":67126,\"start\":67048},{\"end\":67595,\"start\":67531},{\"end\":70228,\"start\":70166},{\"end\":70753,\"start\":70659},{\"end\":71901,\"start\":71850},{\"end\":73127,\"start\":73046},{\"end\":74412,\"start\":74351},{\"end\":74879,\"start\":74807},{\"end\":75637,\"start\":75541},{\"end\":76160,\"start\":76077},{\"end\":76540,\"start\":76522},{\"end\":77199,\"start\":77137},{\"end\":58944,\"start\":58940},{\"end\":59210,\"start\":59132},{\"end\":59594,\"start\":59571},{\"end\":59927,\"start\":59850},{\"end\":60378,\"start\":60301},{\"end\":60806,\"start\":60735},{\"end\":61103,\"start\":61054},{\"end\":61393,\"start\":61322},{\"end\":61745,\"start\":61678},{\"end\":62168,\"start\":62102},{\"end\":62598,\"start\":62523},{\"end\":62997,\"start\":62916},{\"end\":63471,\"start\":63393},{\"end\":63878,\"start\":63841},{\"end\":64194,\"start\":64145},{\"end\":64523,\"start\":64425},{\"end\":64937,\"start\":64933},{\"end\":65184,\"start\":65179},{\"end\":65491,\"start\":65414},{\"end\":65896,\"start\":65865},{\"end\":66246,\"start\":66215},{\"end\":66605,\"start\":66539},{\"end\":67046,\"start\":66953},{\"end\":67529,\"start\":67450},{\"end\":67954,\"start\":67936},{\"end\":68304,\"start\":68255},{\"end\":68548,\"start\":68542},{\"end\":68700,\"start\":68658},{\"end\":68942,\"start\":68893},{\"end\":69373,\"start\":69324},{\"end\":69785,\"start\":69736},{\"end\":70164,\"start\":70087},{\"end\":70657,\"start\":70548},{\"end\":71089,\"start\":71084},{\"end\":71433,\"start\":71362},{\"end\":71848,\"start\":71782},{\"end\":72188,\"start\":72183},{\"end\":72408,\"start\":72316},{\"end\":72684,\"start\":72645},{\"end\":73044,\"start\":72948},{\"end\":73436,\"start\":73378},{\"end\":73858,\"start\":73796},{\"end\":74349,\"start\":74273},{\"end\":74805,\"start\":74718},{\"end\":75131,\"start\":75065},{\"end\":75539,\"start\":75428},{\"end\":76075,\"start\":75977},{\"end\":76520,\"start\":76487},{\"end\":76844,\"start\":76828},{\"end\":77135,\"start\":77058}]"}}}, "year": 2023, "month": 12, "day": 17}