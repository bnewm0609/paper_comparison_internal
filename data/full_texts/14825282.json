{"id": 14825282, "updated": "2023-06-26 15:09:44.46", "metadata": {"title": "Speech recognition techniques for a sign language recognition system", "authors": "[{\"first\":\"Philippe\",\"last\":\"Dreuw\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Rybach\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Deselaers\",\"middle\":[]},{\"first\":\"Morteza\",\"last\":\"Zahedi\",\"middle\":[]},{\"first\":\"Hermann\",\"last\":\"Ney\",\"middle\":[]}]", "venue": "In Interspeech 2007 - Eurospeech", "journal": null, "publication_date": {"year": 2007, "month": null, "day": null}, "abstract": "One of the most significant differences between automatic sign language recognition (ASLR) and automatic speech recognition (ASR) is due to the computer vision problems, whereas the corresponding problems in speech signal processing have been solved due to intensive research in the last 30 years. We present our approach where we start from a large vocabulary speech recognition system to profit from the insights that have been obtained in ASR research. The system developed is able to recognize sentences of continuous sign language independent of the speaker. The features used are obtained from standard video cameras without any special data acquisition devices. In particular, we focus on feature and model combination techniques applied in ASR, and the usage of pronunciation and language models (LM) in sign language. These techniques can be used for all kind of sign language recognition systems, and for many video analysis problems where the temporal context is important, e.g. for action or gesture recognition. On a publicly available benchmark database consisting of 201 sentences and 3 signers, we can achieve a 17% WER.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "11747580", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/DreuwRDZN07", "doi": "10.21437/interspeech.2007-668"}}, "content": {"source": {"pdf_hash": "826930cf62fae0361c247a5b56f6727ad01d0514", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f0c37e0a9180a25126c597a4af67acf797c949ab", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/826930cf62fae0361c247a5b56f6727ad01d0514.txt", "contents": "\nSpeech Recognition Techniques for a Sign Language Recognition System\n\n\nPhilippe Dreuw \nHuman Language Technology and Pattern Recognition Computer Science Department 6\nRWTH Aachen University\nGermany\n\nDavid Rybach \nHuman Language Technology and Pattern Recognition Computer Science Department 6\nRWTH Aachen University\nGermany\n\nThomas Deselaers \nHuman Language Technology and Pattern Recognition Computer Science Department 6\nRWTH Aachen University\nGermany\n\nMorteza Zahedi \nHuman Language Technology and Pattern Recognition Computer Science Department 6\nRWTH Aachen University\nGermany\n\nHermann Ney \nHuman Language Technology and Pattern Recognition Computer Science Department 6\nRWTH Aachen University\nGermany\n\nSpeech Recognition Techniques for a Sign Language Recognition System\n10.21437/Interspeech.2007-668Index Terms: Sign Language RecognitionVideo signal pro- cessingPronunciation ModelLanguage Model\nOne of the most significant differences between automatic sign language recognition (ASLR) and automatic speech recognition (ASR) is due to the computer vision problems, whereas the corresponding problems in speech signal processing have been solved due to intensive research in the last 30 years. We present our approach where we start from a large vocabulary speech recognition system to profit from the insights that have been obtained in ASR research.The system developed is able to recognize sentences of continuous sign language independent of the speaker. The features used are obtained from standard video cameras without any special data acquisition devices. In particular, we focus on feature and model combination techniques applied in ASR, and the usage of pronunciation and language models (LM) in sign language. These techniques can be used for all kind of sign language recognition systems, and for many video analysis problems where the temporal context is important, e.g. for action or gesture recognition.On a publicly available benchmark database consisting of 201 sentences and 3 signers, we can achieve a 17% WER.\n\nIntroduction\n\nWherever communities of deaf people exist, sign languages develop. As with spoken languages, these vary from region to region and represent complete languages not limited in expressiveness. Linguistic research in sign language has shown that signs mainly consist of four basic manual components [1]: hand configuration, place of articulation, hand movement, and hand orientation. Additionally, non-manual components like facial expression and body posture are used. In continuous sign language recognition, we have to deal with strong coarticulation effects, i.e. the appearance of a sign depends on preceding and succeeding signs, and large inter-and intra-personal variability.\n\nIn [2,3] reviews on recent research in sign language and gesture recognition are presented. In vision-based ASLR, capturing-, tracking-and segmentation problems occur, and it is hard to build a robust recognition framework. Most of the current systems use private databases, specialized hardware [4], and are person dependent [5,6]. Furthermore, most approaches focus on the recognition of isolated signs only [5,6], or on the simpler case of gesture recognition [7] for small vocabularies. Our aim is to build a robust, person independent system to recognize sentences of continuous sign language. We use a visionbased approach which does not require special data acquisition devices, e.g. data gloves or motion capturing systems which restrict the natural way of signing. A prototype would just need a simple webcam.\n\n\nVideo Input\n\nFeature Analysis\nGlobal Search: argmax w N 1 Pr(w N 1 ) \u00b7 Pr(x T 1 |w N 1 ) Recognized Word Sequence Word Model Inventory Language Model X T 1 x T 1 w N 1 Pr(x T 1 |w N 1 )\nPr(w N 1 ) Figure 1: Bayes' decision rule used in ASLR.\n\nOur work is based on a large vocabulary speech recognition system [8]. In particular, we present a complete visionbased framework for person independent continuous sign language recognition as opposed to isolated gesture-recognition works presented by most other authors [5,6,7], and analyze the impacts of ASR basic techniques in sign language recognition on a publicly available database with several speakers.\n\n\nSystem Overview & Features\n\nThe ASLR system is based on the Bayes' decision rule. The word sequence which best fits for the current observation to the trained word model inventory (i.e. the acoustic model in ASR) and LM will be the recognition result (see Figure 1).\n\n\nVisual Modeling\n\nAccording to the linguistic work on sign language by Stokoe, a phonological model for sign language can be defined [1], dividing signs in units called \"chiremes\". However, it is still unclear, how sign language words can be split up into sub-word units (e.g. phonemes) suitable for sign language recognition. Therefore, our corpus (c.f. section 3) is annotated in glosses, i.e. whole-word transcriptions, and the system is based on wholeword models. Each word model consists of one to three pseudophonemes modeling the average word length seen in training. Our lexicon defines 247 pseudo-phonemes for 104 words. Each phoneme is modeled by a 3-state left-to-right hidden Markov model (HMM) with three separate Gaussian mixtures (GMM) and a globally pooled covariance matrix.\n\nDue to dialects in natural continuous sign language, signs with the same meaning often differ significantly in their visual appearance and in their duration (e.g. there are 5 different ways to sign the word \"bread\" in Switzerland). Small differences between the appearance and the length of the utterances are compensated for by the HMMs, but different pronunciations of a sign must be modeled by separate models, i.e. a different num-ber of states and GMMs. Therefore, we added pronunciations to the corpus annotations and adapted our language models (c.f. section 3).\n\n\nLanguage Models\n\nIn Bayes' decision rule, the acoustic model (AM) and the language model (LM) have the same impact on the decision, but according to the experience in speech recognition the performance can be greatly improved, if the language model has a greater weight than the acoustic model. The weighting is done by introducing an LM scale \u03b1 and an AM scale \u03b2:\narg max w N 1 n P r \u03b1 (w N 1 ) \u00b7 P r \u03b2 (x T 1 |w N 1 ) o = arg max w N 1 j \u03b1 \u03b2 log P r(w N 1 ) + log P r(x T 1 |w N 1 ) ff\nThe factor \u03b1 \u03b2 is referred to as language model factor. A trigram LM was trained using the SRILM toolkit with modified Kneser-Ney discounting with interpolation.\n\n\nAppearance-Based Features\n\nIn our baseline system we use appearance-based image features only, i.e. thumbnails of video sequence frames. These intensity images scaled to 32\u00d732 pixels serve as good basic features for many image recognition problems, and have already been successfully used for gesture recognition [9]. They give a global description of all (manual and non-manual) features proposed in linguistic research. language. The baseline system is Viterbi trained and uses a trigram LM (c.f. subsection 2.2). In subsequent steps, this baseline system is extended by features accounting for the hands and their positions.\n\n\nManual Features\n\nTo extract manual features, the dominant hand (i.e. the hand that is mostly used for one-handed signs such as finger spelling) is tracked in each image sequence. Therefore, a robust tracking algorithm for hand and head tracking is required as the signing hand frequently moves in front of the face, may temporarily disappear, or cross the other hand. Instead of requiring a near perfect segmentation for these body parts, the decision process for candidate regions is postponed to the end of the entire sequences by tracing back the best decisions [10]. Given the hand position (HP) u t = (x, y) at time t in signing space, features such as hand velocity (HV) mt = ut \u2212 u t\u2212\u03b4 can easily be extracted.\n\nThe hand trajectory (HT) features presented here are similar to the features presented in [5]. Here we calculate global features describing geometric properties of the hand trajectory in a certain time window 2\u03b4 + 1 around time t by an estimation of the covariance matrix\n\u03a3 t = 1 2\u03b4 + 1 t+\u03b4 X t =t\u2212\u03b4 (u t \u2212 \u03bct) (u t \u2212 \u03bct) T and \u03bct = 1 2\u03b4+1 P t+\u03b4 t =t\u2212\u03b4 u t . For \u03a3t \u00b7 vt,i = \u03bbt,i \u00b7 vt,i, i \u2208 {1, 2}\n, the eigenvalues \u03bbt,i and eigenvectors vt,i of the covariance matrix can then be used as global features, describing the form of the movement. If one eigenvalue is significantly larger than the other, the movements fits a line, otherwise it is rather elliptical. The eigenvector with the larger corresponding eigenvalue can be interpreted as the main direction of the movement.  \n\n\nFeature Selection & Combination\n\nA known problem with appearance-based features are border pixels that do not help in the classification and have very low variance. To resolve this problem, dimensionality reduction techniques like PCA or LDA are commonly applied. LDA is often used in speech recognition to combine and reduce features while maximizing the linear separability of the classes in the transformed feature space. Furthermore in ASR, succeeding feature vectors are commonly concatenated before the LDA transformation is applied to account for temporal dependencies. A critical parameter is the number of succeeding feature vectors that are concatenated, because for a growing window size an increasing amount of training data is needed. Figure 3 shows how we extract and combine features. The results achieved using different features and combination methods are presented in section 3.\n\n\nExperimental Results\n\nTo tune and test our system, we assembled the RWTH-Boston-104 corpus 1 as a subset of a much larger database of sign language sentences that were recorded at Boston University for linguistic research [11]. The RWTH-Boston-104 corpus consists of 201 sequences, and the vocabulary contains 104 words. The sentences were signed by 3 speakers (2 female, 1 male) and the corpus is split into 161 training and 40 test sequences. An overview on the corpus is given in Table 1: 26% of the training data are singletons, i.e. a \"one-shot training\" occurs. The sentences have a rather simple structure and therefore the language model perplexity (P P ) is low. The test corpus has one out-of-vocabulary (OOV) word. Obviously, this word cannot be recognized correctly.  The HMM based ASR framework offers various tuning possibilities. From former experiments we know that a high number of states per word and a high number of mixture densities have a positive impact on the recognition performance.\n\nBaseline. First, we analyze different appearance-based features for our baseline system. Table 2 gives an overview of results obtained with the baseline system for a few different features. It can be seen that intensity images compared with tangent distance [9] already lead to reasonable results. Contrary to ASR, the first-order time derivatives of the intensity features (i.e. the motion feature) or the concatenation of them with the intensity features (i.e. the intensity+motion feature) usually do not improve the results in video analysis, as the time resolution is much lower (e.g. 25 or 30 video frames/sec compared to 100 acoustic samples/sec in speech). The simplest and best appearance-based feature is to use intensity images down scaled to 32\u00d732 pixels. This size, which was tuned on the test set, was reported to also work reasonably well in previous works [9,12]. Another important point is the usage of pronunciation modelling in sign language: it can be seen that by adding pronunciations to the corpus and the adaptation of the used trigram language model, the system can already be improved from 54.0% to 37.0% WER. Feature Reduction.\n\nObviously, the high dimensional appearance-based feature vectors encode a lot of background noise and one would need many more observations to train a robust model. To reduce the number of features and noise and thus the number of parameters to be learned in the models, we apply linear feature reduction techniques to the data. The best obtained result with LDA is 36% WER, whereas with PCA a WER of 27.5% can be obtained. Although theoreticaly LDA should be better suited for pattern recognition tasks, here the training data is insufficient for a numerically stable estimation of the LDA transformation and thus PCA, which is reported to be more stable for high dimensional data with small training sets outperforms LDA [12]. Windowing. We experimentally evaluated the incorporation of temporal context by concatenating features x t+\u03b4 t\u2212\u03b4 within a sliding window of size 2\u03b4+1 into a larger feature vectorxt and then applying linear dimensionality reduction techniques as in ASR to find a good linear combination of succeeding feature vectors. The outcomes of these experiments are given in Figure 4 and Figure 5 and again, the PCA outperforms the LDA. The best result (21.9% WER) is achieved by concatenating and reducing five PCA transformed (i.e. a total of 110\u00d75 components) frames to 100 coefficients, whereas the best result obtained with LDA is only 25.8% WER, probably again due to insufficient training data (as 2 matrices have to be estimated for LDA). Furthermore, windowing with large temporal contexts increases the system performance, as coarticulation effects are described now. Feature and Model Combination. As explained before, in sign language, different channels have to be considered. To incorporate the data from these different channels, we propose to use a combination of features. Results for various combinations are presented in Table 3 and a clear improvement can be observed. Many other feature combinations are possible and were tested, but as we do not want to overfit our system, we just extracted the manual features from the dominant-hand related to linguistic research (i.e. place of articulation, hand movement, and hand orientation. The hand configuration is encoded in the complete PCA-frame). A log-linear combination of two independently trained models (windowed PCA-frame+HT and windowed PCA-frame+HV) leads to a further improvement. A WER of 17.9% is achieved (i.e. 17 del., 3 ins., and 12 subst.), where the model weights have been optimized empirically. This is in accordance to experiments in other domains where the combination of different models leads to an improvement over the individual models [13]. In this case, the improvement is due to a better performance of the HT feature for long words and a better performance of the HV feature for short words. A combination on the feature level cannot exploit this advantage because only one alignment is created where the combination of two separately trained models profits from two independent alignments, one performing well for long words and the other performing well for short words. Note that the HT feature is strongly distorted for short words (i.e. less than 5 states) because at the word boundaries strong coarticulation effects occur. Language Model. Figure 6 shows the effect of using different n-gram language models and scales. As in ASR, the language model adaptation by using sign language pronunciations achieves large improvements (c.f. baseline results). Interestingly, the improvement factors achieved are similar to those from speech recognition [14]. Due to the lack of training data for the LM no further improvements are expected for e.g. 4gram language models. It can also be seen that the LM scale is one of the most important parameters of a continuous sign language recognition system.  \n\n\nSummary & Conclusion\n\nWe presented a vision-based approach to continuous automatic sign language recognition. We have shown that appearancebased features, which have been proven to be a powerful tool in many image recognition problems, are also well suited for the recognition of sign language. Furthermore, we have shown that many of the principles known from ASR, such as pronunciation and language modelling can directly be transfered to the new domain of vision-based continuous ASLR. We presented very promising results on a publicly available benchmark database of several speakers which has been recorded without any special data acquisition tools.\n\nCombining different data sources, suitable language and pronunciation modelling, temporal contexts, and model combination, the 37% WER of our baseline system could be improved to 17.9% WER. The results suggest that for high dimensional data and the relatively low amount of available training data, PCA outperforms LDA for this task and that context information is as important as it is in ASR.\n\nOutlook. Obviously, a large amount of work still needs to be done for the vision part of the system. New features describing the hand and body configuration as e.g. in [15] should be analyzed and combined with the existing feature set. Certainly an important step is the definition of sub-word units which would allow recognition with a larger vocabulary and the consideration of context dependency with suitable models for coarticulation. Great improvements are also expected from speaker adaptation techniques such as MLLR, because of the large interpersonal differences in sign language.\n\nIn order to build a vision-based speech-to-speech system for deaf people, our system is connected to a statistical machine translation system. In preliminary translation experiments presented in [16], the incorporation of the tracking data for the deixis words helped the translation system to discriminate between deixis as distinctive article, locative or discourse entity reference function. Furthermore we collected a new publicly available sign language database with currently 843 sentences, a vocabulary of 403 words (482 words w/ pronunciations), which was signed by 5 speakers.\n\nFigure 2 :Figure 3 :\n23Examples of different hand trajectories and corresponding eigenvectors for \u03b4 = 4. The covariance matrices are visualized as ellipses with axes of length \u221a \u03bbi. Composite Features using speech signal processing network.\n\nFigure 2\n2shows some examples of trajectories and their eigenvectors and eigenvalues.\n\nFigure 4 :Figure 5 :\n45Combination Combination of PCA-frames using PCA windowing\n\nFigure 6 :\n6Results for different LMs and scales\n\nTable 1 :\n1RWTH-Boston-104 corpus statisticsTraining Test \n\nsentences \n161 \n40 \nrunning words \n710 \n178 \nvocabulary \n103 \n65 \nsingletons \n27 \n9 \nOOV \n-\n1 \n\nLM type \nP P \nzerogram 106.0 \nunigram \n36.8 \nbigram \n6.7 \ntrigram \n4.7 \n\n\n\nTable 2 :\n2Baseline results using appearance-based featuresFeatures \nDim. [%WER] \n\nintensity (w/o pronunciations) \n1024 \n54.0 \nintensity (w/ pronunciations) \n1024 \n37.0 \nintensity (w/ pronunciations + tangent distance) \n1024 \n33.7 \nmotion (pixel based) \n1024 \n51.1 \nintensity+motion \n2048 \n42.1 \n\n\n\nTable 3 :\n3Results for feature combinations with hand featuresFeatures \nDimensionality [% WER] \n\nPCA-frame \n110 \n27.5 \nPCA-frame, hand-position (HP) \n112 \n25.3 \nPCA-frame, hand-velocity (HV) \n112 \n24.2 \nPCA-frame, hand-trajectory (HT) \n112 \n23.6 \n\nmodel-combination \n2\u00d7100 \n17.9 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n0 \n100 \n200 \n300 \n400 \n500 \n\nWER [%] \n\nLM scale \n\nzerogram \nunigram \nbigram \ntrigram \n\n\nhttp://www-i6.informatik.rwth-aachen.de/ \u223c dreuw/database.html\n\nW Stokoe, D Casterline, C Croneberg, A Dictionary of American Sign Language on Linguistic Principles. Washington D.C., USAGallaudet College PressW. Stokoe, D. Casterline, and C. Croneberg, A Dictionary of American Sign Language on Linguistic Principles, Gal- laudet College Press, Washington D.C., USA, 1965.\n\nAutomatic sign language analysis: A survey and the future beyond lexical meaning. S Ong, S Ranganath, IEEE Trans. PAMI. 276S. Ong and S. Ranganath, \"Automatic sign language anal- ysis: A survey and the future beyond lexical meaning,\" IEEE Trans. PAMI, vol. 27, no. 6, pp. 873-891, June 2005.\n\nVision-based gesture recognition: a review. T S Huang, Y Wu, Gesture Workshop. Gif-sur-Yvette, France1739T.S. Huang Y. Wu, \"Vision-based gesture recognition: a review,\" in Gesture Workshop, Gif-sur-Yvette, France, Mar. 1999, vol. 1739 of LNCS, pp. 103-115.\n\nReal time large vocabulary continuous sign language recognition based on op/viterbi algorithm. G Yao, H Yao, X Liu, F Jiang, Intl. Conf. Pattern Recognition. Hong Kong3G. Yao, H. Yao, X. Liu, and F. Jiang, \"Real time large vocabulary continuous sign language recognition based on op/viterbi algorithm,\" in Intl. Conf. Pattern Recognition, Hong Kong, Aug. 2006, vol. 3, pp. 312-315.\n\nA framework for recognizing the simultaneous aspects of american sign language. C Vogler, D Metaxas, Computer Vision & Image Understanding. 813C. Vogler and D. Metaxas, \"A framework for recogniz- ing the simultaneous aspects of american sign language,\" Computer Vision & Image Understanding, vol. 81, no. 3, pp. 358-384, Mar. 2001.\n\nA linguistic feature vector for the visual interpretation of sign language. R Bowden, D Windridge, T Kadir, A Zisserman, M Brady, European Conf. Computer Vision. 1R. Bowden, D. Windridge, T. Kadir, A. Zisserman, and M. Brady, \"A linguistic feature vector for the visual inter- pretation of sign language,\" in European Conf. Computer Vision, 2004, vol. 1, pp. 390-401.\n\nHidden conditional random fields for gesture recognition. S B Wang, A Quattoni, Louis-Philippe Morency, David Demirdjian, Trevor Darrell, Computer Vision & Pattern Recognition. New York, USA2S. B. Wang, A. Quattoni, Louis-Philippe Morency, David Demirdjian, and Trevor Darrell, \"Hidden conditional ran- dom fields for gesture recognition,\" in Computer Vision & Pattern Recognition, New York, USA, June 2006, vol. 2, pp. 1521-1527.\n\nThe 2006 RWTH parliamentary speeches transcription system. J L\u00f6\u00f6f, M Bisani, C Gollan, G Heigold, B Hoffmeister, C Plahl, R Schluter, H Ney, ICSLP. J. L\u00f6\u00f6f, M. Bisani, C. Gollan, G. Heigold, B. Hoffmeis- ter, C. Plahl, R. Schluter, and H. Ney, \"The 2006 RWTH parliamentary speeches transcription system,\" in ICSLP, Pittsburgh, PA, USA, Sept. 2006.\n\nDeformation models for image recognition. D Keysers, T Deselaers, C Gollan, H Ney, IEEE Trans. PAMI. p. to appearD. Keysers, T. Deselaers, C. Gollan, and H. Ney, \"De- formation models for image recognition,\" IEEE Trans. PAMI, p. to appear, 2007.\n\nTracking using dynamic programming for appearance-based sign language recognition. P Dreuw, T Deselaers, D Rybach, D Keysers, H Ney, IEEE Intl. Conf. on Automatic Face and Gesture Recognition. SouthamptonP. Dreuw, T. Deselaers, D. Rybach, D. Keysers, and H. Ney, \"Tracking using dynamic programming for appearance-based sign language recognition,\" in IEEE Intl. Conf. on Automatic Face and Gesture Recognition, Southampton, Apr. 2006, pp. 293-298.\n\nThe Syntax of American Sign Language. C Neidle, J Kegl, D Maclaughlin, B Bahan, R G Lee, MIT PressC. Neidle, J. Kegl, D. MacLaughlin, B. Bahan, and R.G. Lee, The Syntax of American Sign Language, MIT Press, 1999.\n\nEnhancements for local feature based image classification. T K\u00f6lsch, D Keysers, H Ney, R Paredes, Intl. Conf. Pattern Recognition. Cambridge, UK1T. K\u00f6lsch, D. Keysers, H. Ney, and R. Paredes, \"Enhance- ments for local feature based image classification,\" in Intl. Conf. Pattern Recognition, Cambridge, UK, Aug. 2004, vol. 1, pp. 248-251.\n\nAcoustic feature combination for robust speech recognition. A Zolnay, R Schl\u00fcter, H Ney, ICASSP. 1A. Zolnay, R. Schl\u00fcter, and H. Ney, \"Acoustic feature combination for robust speech recognition,\" in ICASSP, Philadelphia, PA, Mar. 2005, vol. 1, pp. 457-460.\n\nTesting the correlation of word error rate and perplexity. D Klakow, J Peters, Speech Communication. 38D. Klakow and J. Peters, \"Testing the correlation of word error rate and perplexity,\" Speech Communication, vol. 38, pp. 19-28, 2002.\n\nRecovering 3d human pose from monocular images. A Agarwal, B Triggs, IEEE Trans. PAMI. 281A. Agarwal and B. Triggs, \"Recovering 3d human pose from monocular images,\" IEEE Trans. PAMI, vol. 28, no. 1, pp. 44-58, Jan. 2006.\n\nEnhancing a sign language translation system with vision-based features. P Dreuw, D Stein, H Ney, Intl. Workshop on Gesture in HCI and Simulation. Lisbon, Portugalto appearP. Dreuw, D. Stein, and H. Ney, \"Enhancing a sign lan- guage translation system with vision-based features,\" in Intl. Workshop on Gesture in HCI and Simulation 2007, Lisbon, Portugal, May 2007, p. to appear.\n", "annotations": {"author": "[{\"end\":199,\"start\":72},{\"end\":325,\"start\":200},{\"end\":455,\"start\":326},{\"end\":583,\"start\":456},{\"end\":708,\"start\":584}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":81},{\"end\":212,\"start\":206},{\"end\":342,\"start\":333},{\"end\":470,\"start\":464},{\"end\":595,\"start\":592}]", "author_first_name": "[{\"end\":80,\"start\":72},{\"end\":205,\"start\":200},{\"end\":332,\"start\":326},{\"end\":463,\"start\":456},{\"end\":591,\"start\":584}]", "author_affiliation": "[{\"end\":198,\"start\":88},{\"end\":324,\"start\":214},{\"end\":454,\"start\":344},{\"end\":582,\"start\":472},{\"end\":707,\"start\":597}]", "title": "[{\"end\":69,\"start\":1},{\"end\":777,\"start\":709}]", "venue": null, "abstract": "[{\"end\":2038,\"start\":904}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2352,\"start\":2349},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2741,\"start\":2738},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2743,\"start\":2741},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3034,\"start\":3031},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3064,\"start\":3061},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3066,\"start\":3064},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3148,\"start\":3145},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3150,\"start\":3148},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3201,\"start\":3198},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3868,\"start\":3865},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4073,\"start\":4070},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4075,\"start\":4073},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4077,\"start\":4075},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4618,\"start\":4615},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6815,\"start\":6812},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7698,\"start\":7694},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7941,\"start\":7938},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9756,\"start\":9752},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10801,\"start\":10798},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11415,\"start\":11412},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11418,\"start\":11415},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12423,\"start\":12419},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14347,\"start\":14343},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15266,\"start\":15262},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16738,\"start\":16734},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17357,\"start\":17353}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17986,\"start\":17745},{\"attributes\":{\"id\":\"fig_1\"},\"end\":18073,\"start\":17987},{\"attributes\":{\"id\":\"fig_2\"},\"end\":18155,\"start\":18074},{\"attributes\":{\"id\":\"fig_3\"},\"end\":18205,\"start\":18156},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":18436,\"start\":18206},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":18735,\"start\":18437},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":19145,\"start\":18736}]", "paragraph": "[{\"end\":2733,\"start\":2054},{\"end\":3553,\"start\":2735},{\"end\":3585,\"start\":3569},{\"end\":3797,\"start\":3742},{\"end\":4211,\"start\":3799},{\"end\":4480,\"start\":4242},{\"end\":5273,\"start\":4500},{\"end\":5844,\"start\":5275},{\"end\":6211,\"start\":5864},{\"end\":6496,\"start\":6335},{\"end\":7126,\"start\":6526},{\"end\":7846,\"start\":7146},{\"end\":8119,\"start\":7848},{\"end\":8627,\"start\":8247},{\"end\":9527,\"start\":8663},{\"end\":10538,\"start\":9552},{\"end\":11694,\"start\":10540},{\"end\":15510,\"start\":11696},{\"end\":16168,\"start\":15535},{\"end\":16564,\"start\":16170},{\"end\":17156,\"start\":16566},{\"end\":17744,\"start\":17158}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3741,\"start\":3586},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6334,\"start\":6212},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8246,\"start\":8120}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10020,\"start\":10013},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":10636,\"start\":10629},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13561,\"start\":13554}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2052,\"start\":2040},{\"end\":3567,\"start\":3556},{\"attributes\":{\"n\":\"2.\"},\"end\":4240,\"start\":4214},{\"attributes\":{\"n\":\"2.1.\"},\"end\":4498,\"start\":4483},{\"attributes\":{\"n\":\"2.2.\"},\"end\":5862,\"start\":5847},{\"attributes\":{\"n\":\"2.3.\"},\"end\":6524,\"start\":6499},{\"attributes\":{\"n\":\"2.4.\"},\"end\":7144,\"start\":7129},{\"attributes\":{\"n\":\"2.5.\"},\"end\":8661,\"start\":8630},{\"attributes\":{\"n\":\"3.\"},\"end\":9550,\"start\":9530},{\"attributes\":{\"n\":\"4.\"},\"end\":15533,\"start\":15513},{\"end\":17766,\"start\":17746},{\"end\":17996,\"start\":17988},{\"end\":18095,\"start\":18075},{\"end\":18167,\"start\":18157},{\"end\":18216,\"start\":18207},{\"end\":18447,\"start\":18438},{\"end\":18746,\"start\":18737}]", "table": "[{\"end\":18436,\"start\":18251},{\"end\":18735,\"start\":18497},{\"end\":19145,\"start\":18799}]", "figure_caption": "[{\"end\":17986,\"start\":17769},{\"end\":18073,\"start\":17998},{\"end\":18155,\"start\":18098},{\"end\":18205,\"start\":18169},{\"end\":18251,\"start\":18218},{\"end\":18497,\"start\":18449},{\"end\":18799,\"start\":18748}]", "figure_ref": "[{\"end\":3761,\"start\":3753},{\"end\":4478,\"start\":4470},{\"end\":9386,\"start\":9378},{\"end\":12797,\"start\":12789},{\"end\":12810,\"start\":12802},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14965,\"start\":14957}]", "bib_author_first_name": "[{\"end\":19211,\"start\":19210},{\"end\":19221,\"start\":19220},{\"end\":19235,\"start\":19234},{\"end\":19603,\"start\":19602},{\"end\":19610,\"start\":19609},{\"end\":19858,\"start\":19857},{\"end\":19860,\"start\":19859},{\"end\":19869,\"start\":19868},{\"end\":20167,\"start\":20166},{\"end\":20174,\"start\":20173},{\"end\":20181,\"start\":20180},{\"end\":20188,\"start\":20187},{\"end\":20535,\"start\":20534},{\"end\":20545,\"start\":20544},{\"end\":20864,\"start\":20863},{\"end\":20874,\"start\":20873},{\"end\":20887,\"start\":20886},{\"end\":20896,\"start\":20895},{\"end\":20909,\"start\":20908},{\"end\":21215,\"start\":21214},{\"end\":21217,\"start\":21216},{\"end\":21225,\"start\":21224},{\"end\":21250,\"start\":21236},{\"end\":21265,\"start\":21260},{\"end\":21284,\"start\":21278},{\"end\":21648,\"start\":21647},{\"end\":21656,\"start\":21655},{\"end\":21666,\"start\":21665},{\"end\":21676,\"start\":21675},{\"end\":21687,\"start\":21686},{\"end\":21702,\"start\":21701},{\"end\":21711,\"start\":21710},{\"end\":21723,\"start\":21722},{\"end\":21980,\"start\":21979},{\"end\":21991,\"start\":21990},{\"end\":22004,\"start\":22003},{\"end\":22014,\"start\":22013},{\"end\":22268,\"start\":22267},{\"end\":22277,\"start\":22276},{\"end\":22290,\"start\":22289},{\"end\":22300,\"start\":22299},{\"end\":22311,\"start\":22310},{\"end\":22672,\"start\":22671},{\"end\":22682,\"start\":22681},{\"end\":22690,\"start\":22689},{\"end\":22705,\"start\":22704},{\"end\":22714,\"start\":22713},{\"end\":22716,\"start\":22715},{\"end\":22907,\"start\":22906},{\"end\":22917,\"start\":22916},{\"end\":22928,\"start\":22927},{\"end\":22935,\"start\":22934},{\"end\":23247,\"start\":23246},{\"end\":23257,\"start\":23256},{\"end\":23269,\"start\":23268},{\"end\":23504,\"start\":23503},{\"end\":23514,\"start\":23513},{\"end\":23731,\"start\":23730},{\"end\":23742,\"start\":23741},{\"end\":23979,\"start\":23978},{\"end\":23988,\"start\":23987},{\"end\":23997,\"start\":23996}]", "bib_author_last_name": "[{\"end\":19218,\"start\":19212},{\"end\":19232,\"start\":19222},{\"end\":19245,\"start\":19236},{\"end\":19607,\"start\":19604},{\"end\":19620,\"start\":19611},{\"end\":19866,\"start\":19861},{\"end\":19872,\"start\":19870},{\"end\":20171,\"start\":20168},{\"end\":20178,\"start\":20175},{\"end\":20185,\"start\":20182},{\"end\":20194,\"start\":20189},{\"end\":20542,\"start\":20536},{\"end\":20553,\"start\":20546},{\"end\":20871,\"start\":20865},{\"end\":20884,\"start\":20875},{\"end\":20893,\"start\":20888},{\"end\":20906,\"start\":20897},{\"end\":20915,\"start\":20910},{\"end\":21222,\"start\":21218},{\"end\":21234,\"start\":21226},{\"end\":21258,\"start\":21251},{\"end\":21276,\"start\":21266},{\"end\":21292,\"start\":21285},{\"end\":21653,\"start\":21649},{\"end\":21663,\"start\":21657},{\"end\":21673,\"start\":21667},{\"end\":21684,\"start\":21677},{\"end\":21699,\"start\":21688},{\"end\":21708,\"start\":21703},{\"end\":21720,\"start\":21712},{\"end\":21727,\"start\":21724},{\"end\":21988,\"start\":21981},{\"end\":22001,\"start\":21992},{\"end\":22011,\"start\":22005},{\"end\":22018,\"start\":22015},{\"end\":22274,\"start\":22269},{\"end\":22287,\"start\":22278},{\"end\":22297,\"start\":22291},{\"end\":22308,\"start\":22301},{\"end\":22315,\"start\":22312},{\"end\":22679,\"start\":22673},{\"end\":22687,\"start\":22683},{\"end\":22702,\"start\":22691},{\"end\":22711,\"start\":22706},{\"end\":22720,\"start\":22717},{\"end\":22914,\"start\":22908},{\"end\":22925,\"start\":22918},{\"end\":22932,\"start\":22929},{\"end\":22943,\"start\":22936},{\"end\":23254,\"start\":23248},{\"end\":23266,\"start\":23258},{\"end\":23273,\"start\":23270},{\"end\":23511,\"start\":23505},{\"end\":23521,\"start\":23515},{\"end\":23739,\"start\":23732},{\"end\":23749,\"start\":23743},{\"end\":23985,\"start\":23980},{\"end\":23994,\"start\":23989},{\"end\":24001,\"start\":23998}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":19518,\"start\":19210},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18440098},\"end\":19811,\"start\":19520},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3716703},\"end\":20069,\"start\":19813},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8674226},\"end\":20452,\"start\":20071},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14191037},\"end\":20785,\"start\":20454},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16843649},\"end\":21154,\"start\":20787},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1171329},\"end\":21586,\"start\":21156},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5204194},\"end\":21935,\"start\":21588},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2528485},\"end\":22182,\"start\":21937},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14096007},\"end\":22631,\"start\":22184},{\"attributes\":{\"id\":\"b10\"},\"end\":22845,\"start\":22633},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":282992},\"end\":23184,\"start\":22847},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":18738034},\"end\":23442,\"start\":23186},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":38297589},\"end\":23680,\"start\":23444},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2887541},\"end\":23903,\"start\":23682},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11066194},\"end\":24284,\"start\":23905}]", "bib_title": "[{\"end\":19600,\"start\":19520},{\"end\":19855,\"start\":19813},{\"end\":20164,\"start\":20071},{\"end\":20532,\"start\":20454},{\"end\":20861,\"start\":20787},{\"end\":21212,\"start\":21156},{\"end\":21645,\"start\":21588},{\"end\":21977,\"start\":21937},{\"end\":22265,\"start\":22184},{\"end\":22904,\"start\":22847},{\"end\":23244,\"start\":23186},{\"end\":23501,\"start\":23444},{\"end\":23728,\"start\":23682},{\"end\":23976,\"start\":23905}]", "bib_author": "[{\"end\":19220,\"start\":19210},{\"end\":19234,\"start\":19220},{\"end\":19247,\"start\":19234},{\"end\":19609,\"start\":19602},{\"end\":19622,\"start\":19609},{\"end\":19868,\"start\":19857},{\"end\":19874,\"start\":19868},{\"end\":20173,\"start\":20166},{\"end\":20180,\"start\":20173},{\"end\":20187,\"start\":20180},{\"end\":20196,\"start\":20187},{\"end\":20544,\"start\":20534},{\"end\":20555,\"start\":20544},{\"end\":20873,\"start\":20863},{\"end\":20886,\"start\":20873},{\"end\":20895,\"start\":20886},{\"end\":20908,\"start\":20895},{\"end\":20917,\"start\":20908},{\"end\":21224,\"start\":21214},{\"end\":21236,\"start\":21224},{\"end\":21260,\"start\":21236},{\"end\":21278,\"start\":21260},{\"end\":21294,\"start\":21278},{\"end\":21655,\"start\":21647},{\"end\":21665,\"start\":21655},{\"end\":21675,\"start\":21665},{\"end\":21686,\"start\":21675},{\"end\":21701,\"start\":21686},{\"end\":21710,\"start\":21701},{\"end\":21722,\"start\":21710},{\"end\":21729,\"start\":21722},{\"end\":21990,\"start\":21979},{\"end\":22003,\"start\":21990},{\"end\":22013,\"start\":22003},{\"end\":22020,\"start\":22013},{\"end\":22276,\"start\":22267},{\"end\":22289,\"start\":22276},{\"end\":22299,\"start\":22289},{\"end\":22310,\"start\":22299},{\"end\":22317,\"start\":22310},{\"end\":22681,\"start\":22671},{\"end\":22689,\"start\":22681},{\"end\":22704,\"start\":22689},{\"end\":22713,\"start\":22704},{\"end\":22722,\"start\":22713},{\"end\":22916,\"start\":22906},{\"end\":22927,\"start\":22916},{\"end\":22934,\"start\":22927},{\"end\":22945,\"start\":22934},{\"end\":23256,\"start\":23246},{\"end\":23268,\"start\":23256},{\"end\":23275,\"start\":23268},{\"end\":23513,\"start\":23503},{\"end\":23523,\"start\":23513},{\"end\":23741,\"start\":23730},{\"end\":23751,\"start\":23741},{\"end\":23987,\"start\":23978},{\"end\":23996,\"start\":23987},{\"end\":24003,\"start\":23996}]", "bib_venue": "[{\"end\":19310,\"start\":19247},{\"end\":19638,\"start\":19622},{\"end\":19890,\"start\":19874},{\"end\":20227,\"start\":20196},{\"end\":20592,\"start\":20555},{\"end\":20947,\"start\":20917},{\"end\":21331,\"start\":21294},{\"end\":21734,\"start\":21729},{\"end\":22036,\"start\":22020},{\"end\":22375,\"start\":22317},{\"end\":22669,\"start\":22633},{\"end\":22976,\"start\":22945},{\"end\":23281,\"start\":23275},{\"end\":23543,\"start\":23523},{\"end\":23767,\"start\":23751},{\"end\":24050,\"start\":24003},{\"end\":19332,\"start\":19312},{\"end\":19914,\"start\":19892},{\"end\":20238,\"start\":20229},{\"end\":21346,\"start\":21333},{\"end\":22388,\"start\":22377},{\"end\":22991,\"start\":22978},{\"end\":24068,\"start\":24052}]"}}}, "year": 2023, "month": 12, "day": 17}