{"id": 4421747, "updated": "2023-09-29 14:17:05.754", "metadata": {"title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "authors": "[{\"first\":\"Daniel\",\"last\":\"Cer\",\"middle\":[]},{\"first\":\"Mona\",\"last\":\"Diab\",\"middle\":[]},{\"first\":\"Eneko\",\"last\":\"Agirre\",\"middle\":[]},{\"first\":\"I\u00f1igo\",\"last\":\"Lopez-Gazpio\",\"middle\":[]},{\"first\":\"Lucia\",\"last\":\"Specia\",\"middle\":[]}]", "venue": "*SEMEVAL", "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1708.00055", "mag": "3104033643", "acl": "S17-2001", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1708-00055", "doi": "10.18653/v1/s17-2001"}}, "content": {"source": {"pdf_hash": "0f8633e2033a586ceb723a1ecb54880794d681b4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1708.00055v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/S17-2001.pdf", "status": "HYBRID"}}, "grobid": {"id": "d5262b40278152aaa9784289f4b61091f9d0b4d3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0f8633e2033a586ceb723a1ecb54880794d681b4.txt", "contents": "\nSemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation\n\n\nDaniel Cer \nGoogle Research Mountain View\nCA\n\nMona Diab \nGeorge Washington University Washington\nDC\n\nEneko Agirre \nUniversity of the Basque Country Donostia\nBasque Country d University of Sheffield Sheffield\nUK\n\nI\u00f1igo Lopez-Gazpio \nUniversity of the Basque Country Donostia\nBasque Country d University of Sheffield Sheffield\nUK\n\nLucia Specia \nSemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation\n\nSemantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012)(2013)(2014)(2015)(2016)(2017).\n\nIntroduction\n\nSemantic Textual Similarity (STS) assesses the degree to which two sentences are semantically equivalent to each other. The STS task is motivated by the observation that accurately modeling the meaning similarity of sentences is a foundational language understanding problem relevant to numerous applications including: machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. STS enables the evaluation of techniques from a diverse set of domains against a shared interpretable performance criteria. Semantic inference tasks related to STS include textual entailment (Bentivogli et al., 2016;Bowman et al., 2015;Dagan et al., 2010), semantic relatedness (Bentivogli et al., 2016) and paraphrase detection (Xu et al., 2015;Ganitkevitch et al., 2013;Dolan et al., 2004). STS differs from both textual entailment and paraphrase detection in that it captures gradations of meaning overlap rather than making binary classifications of particular relationships. While semantic relatedness expresses a graded semantic relationship as well, it is non-specific about the nature of the relationship with contradictory material still being a candidate for a high score (e.g., \"night\" and \"day\" are highly related but not particularly similar).\n\nTo encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012(Agirre et al., , 2013(Agirre et al., , 2014(Agirre et al., , 2015(Agirre et al., , 2016. During this time, diverse similarity methods and data sets 1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntactic similarity (B\u00e4r et al., 2012;\u0160ari\u0107 et al., 2012a;Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)'s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems . The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016).\n\nSignificant research effort has focused on STS over English sentence pairs. 2 English STS is a well-studied problem, with state-of-the-art systems often achieving 70 to 80% correlation with human judgment. To promote progress in other languages, the 2017 task emphasizes performance on Arabic and Spanish as well as cross-lingual pairings of English with material in Arabic, Spanish and Turkish. The primary evaluation criteria combines performance on all of the different language conditions except English-Turkish, which was run as a surprise language track. Even with this departure from prior years, the task attracted 31 teams producing 84 submissions.\n\nSTS shared task data sets have been used extensively for research on sentence level similarity and semantic representations (i.a., Arora et al. (2017); Conneau et al. (2017); Mu et al. (2017); Pagliardini et al. (2017); Wieting and Gimpel (2017); ; Hill et al. (2016); Kenter et al. (2016); Lau and Baldwin (2016); Wieting et al. (2016a,b); ; Pham et al. (2015)). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks (2012-2017).\n\n\nTask Overview\n\nSTS is the assessment of pairs of sentences according to their degree of semantic similarity. The task involves producing real-valued similarity scores for sentence pairs. Performance is measured by the Pearson correlation of machine scores with human judgments. The ordinal scale in Table 1 guides human annotation, ranging from 0 for no meaning overlap to 5 for meaning equivalence. Intermediate values reflect interpretable levels of partial overlap in meaning. The annotation scale is designed to be accessible by reasonable human judges without any formal expertise in linguistics. Using reasonable human interpretations of natural language semantics was popularized by the related textual entailment task (Dagan et al., 2010). The resulting annotations reflect both pragmatic and world knowledge and are more interpretable and useful within downstream systems.\n\n\nEvaluation Data\n\nThe Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) is the primary evaluation data source with the exception that one of the pilot track on cross-lingual Spanish-English STS. The English tracks attracted the most participation and have the largest use of the evaluation data in ongoing research.\n\n\n5\n\nThe two sentences are completely equivalent, as they mean the same thing. The bird is bathing in the sink. Birdie is washing itself in the water basin.\n\n\n4\n\nThe two sentences are mostly equivalent, but some unimportant details differ. Two boys on a couch are playing video games. Two boys are playing a video game.\n\n\n3\n\nThe two sentences are roughly equivalent, but some important information differs/missing. John said he is considered a witness but not a suspect. \"He is not a suspect anymore.\" John said.\n\n\n2\n\nThe two sentences are not equivalent, but share some details. They flew out of the nest in groups. They flew into the nest together.\n\n\n1\n\nThe two sentences are not equivalent, but are on the same topic. The woman is playing the violin. The young lady enjoys listening to the guitar.\n\n\n0\n\nThe two sentences are completely dissimilar. The black dog is running through the snow. A race car driver is driving his car through the mud.  Agirre et al. (2013).\n\ncross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014). 3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores.\n\nTrack 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) (Snover et al., 2006). 4 Participants are not allowed to use the gold quality estimation annotations to inform STS scores.  WMT's quality estimation task. Track 6 is a surprise language track with no annotated training data and the identity of the language pair first announced when the evaluation data was released.\n\n\nData Preparation\n\nThis section describes the preparation of the evaluation data. For SNLI data, this includes the selection of sentence pairs, annotation of pairs with STS labels and the translation of the original English sentences. WMT quality estimation data is directly annotated with STS labels.\n\n\nArabic, Spanish and Turkish Translation\n\nSentences from SNLI are human translated into Arabic, Spanish and Turkish. Sentences are translated independently from their pairs. Arabic translation is provided by CMU-Qatar by native Arabic speakers with strong English skills. Translators are given an English sentence and its Arabic machine translation 5 where they perform post-editing to correct errors. Spanish translation is completed by a University of Sheffield graduate student who is a native Spanish speaker and fluent in English. Turkish translations are obtained from SDL. 6\n\n\nEmbedding Space Pair Selection\n\nWe construct our own pairings of the SNLI sentences to deter gold entailment labels being used to inform STS scores. The word embedding similarity selection heuristic from STS 2016 (Agirre et al., 2016) is used to find interesting pairs. Sentence embeddings are computed as the sum of in-dividual word embeddings, v(s) = w\u2208s v(w). 7 Sentences with likely meaning overlap are identified using cosine similarity, Eq. (1).\nsim v (s 1 , s 2 ) = v(s 1 )v(s 2 ) v(s 1 ) 2 v(s 2 ) 2\n(1)\n\n\nAnnotation\n\nAnnotation of pairs with STS labels is performed using Crowdsourcing, with the exception of Track 4b that uses a single expert annotator.\n\n\nCrowdsourced Annotations\n\nCrowdsourced annotation is performed on Amazon Mechanical Turk. 8 Annotators examine the STS pairings of English SNLI sentences. STS labels are then transferred to the translated pairs for crosslingual and non-English tracks. The annotation instructions and template are identical to Agirre et al. (2016). Labels are collected in batches of 20 pairs with annotators paid $1 USD per batch. Five annotations are collected per pair. The MTurk master 9 qualification is required to perform the task. Gold scores average the five individual annotations.\n\n\nExpert Annotation\n\nSpanish-English WMT quality estimation pairs for Track 4b are annotated for STS by a University of Sheffield graduate student who is a native speaker of Spanish and fluent in English. This track differs significantly in label distribution and the complexity of the annotation task. Sentences in a pair are translations of each other and tend to be more semantically similar. Interpreting the potentially subtle meaning differences introduced by MT errors is challenging. To accurately assess STS performance on MT quality estimation data, no attempt is made to balance the data by similarity scores.\n\n\nTraining Data\n\nThe following summarizes the training data: Table 3 English; Table 4 Spanish; 10 Table 5 Spanish-English; Table 6 Arabic; and Table 7 Arabic-English. Arabic-English parallel data is supplied by translating English training data, Table 8. English, Spanish and Spanish-English training data pulls from prior STS evaluations. Arabic and Arabic-English training data is produced by translating a subset of the English training data and transferring the similarity scores. For the MT quality estimation data in track 4b, Spanish sentences are translations of their English counterparts, differing substantially from existing Spanish-English STS data. We release one thousand new Spanish-English STS pairs sourced from the 2013 WMT translation task and produced by a phrase-based Moses SMT system (Bojar et al., 2013). The data is expert annotated and has a similar label distribution to the track 4b test data with 17% of the pairs scoring an STS score of less than 3, 23% scoring 3, 7% achieving a score of 4 and 53% scoring 5.\n\n\nTraining vs. Evaluation Data Analysis\n\nEvaluation data from SNLI tend to have sentences that are slightly shorter than those from prior years of the STS shared task, while the track 4b MT qual-   Similarity scores for our pairings of the SNLI sentences are slightly lower than recent shared task years and much lower than early years. The change is attributed to differences in data selection and filtering. The average 2017 similarity score is 2.2 overall and 2.3 on the track 7 English data. Prior English data has the following average similarity scores: 2016 2.4; 2015 2.4; 2014 2.8; 2013 3.0; 2012 3.5. Translation quality estimation data from track 4b has an average similarity score of 4.0.\n\n\nSystem Evaluation\n\nThis section reports participant evaluation results for the SemEval-2017 STS shared task.\n\n\nParticipation\n\nThe task saw strong participation with 31 teams producing 84 submissions. 17 teams provided 44 systems that participated in all tracks. Table 9 summarizes participation by track. Traces of the focus on English are seen in 12 teams participating just in track 5, English. Two teams participated exclusively in tracks 4a and 4b, Spanish-English. One team took part solely in track 1, Arabic.\n\n\nEvaluation Metric\n\nSystems are evaluated on each track by their Pearson correlation with gold labels. The overall rank-   ing averages the correlations across tracks 1-5 with tracks 4a and 4b individually contributing.\n\n\nCodaLab\n\nAs directed by the SemEval workshop organizers, the CodaLab research platform hosts the task. 11\n\n\nBaseline\n\nThe baseline is the cosine of binary sentence vectors with each dimension representing whether an individual word appears in a sentence. 12 For crosslingual pairs, non-English sentences are translated into English using state-of-the-art machine translation. 13 The baseline achieves an average correlation of 53.7 with human judgment on tracks 1-5 and would rank 23 rd overall out the 44 system submissions that participated in all tracks.\n\n\nRankings\n\nParticipant performance is provided in  track 4b's MT quality estimation data. This highlights the difficulty and importance of making fine grained distinctions for certain downstream applications. Assessing STS methods for quality estimation may benefit from using alternatives to Pearson correlation for evaluation. 14 Results tend to decrease on cross-lingual tracks. The baseline drops > 10% relative on Arabic-English and Spanish-English (SNLI) vs. monolingual Arabic and Spanish. Many participant systems show smaller decreases. ECNU's top ranking entry performs slightly better on Arabic-English than Arabic, with a slight drop from Spanish to Spanish-English (SNLI).\n\n\nMethods\n\nParticipating teams explore techniques ranging from state-of-the-art deep learning models to elaborate feature engineered systems. Prediction signals include surface similarity scores such as edit distance and matching n-grams, scores derived from word alignments across pairs, assessment by MT evaluation metrics, estimates of conceptual similarity as well as the similarity between word and sentence level embeddings. For cross-lingual and non-English tracks, MT was widely used to convert the two sentences being compared into the same language. 15 Select methods are highlighted below. 14 e.g., Reimers et al. (2016) report success using STS labels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI and MITRE. HCTI submitted a separate run using ar, es and en trained models that underperformed using their en model with MT for ar and es. CompiLIG's model is cross-lingual but includes a word alignment feature that depends on MT. SEF@UHH built ar, es, en and tr models and use MT for the cross-lingual pairs. LIM-LIG and DT Team only participate in monolingual tracks.   83.02\u2022 15.50 compiLIG  76.84 14.64 compiLIG  79.10 14.94 DT TEAM (Maharjan et al., 2017) 85.36 DT TEAM (Maharjan et al., 2017) 83.60 DT TEAM (Maharjan et al., 2017) 83.29 FCICU (Hassan et al., 2017) 82.17 ITNLPAiKF (Liu et al., 2017) 82.31 ITNLPAiKF (Liu et al., 2017) 82.31 ITNLPAiKF (Liu et al., 2017) 81.59 L2F/INESC-ID (Fialho et al., 2017 Table 10: STS 2017 rankings ordered by average correlation across tracks 1-5. Performance is reported by convention as Pearson's r \u00d7 100. For tracks 1-6, the top ranking result is marked with a \u2022 symbol and results in bold have no statistically significant difference with the best result on a track, p > 0.05 Williams' t-test (Diedenhofen and Musch, 2015).\n\nECNU (Tian et al., 2017) The best overall system is from ENCU and ensembles well performing a feature engineered models with deep learning methods. Three feature engineered models use Random Forest (RF), Gradient Boosting (GB) and XGBoost (XGB) regression methods with features based on: n-gram overlap; edit distance; longest common prefix/suffix/substring; tree kernels (Moschitti, 2006); word alignments (Sultan et al., 2015); summarization and MT evaluation metrics (BLEU, GTM-3, NIST, WER, ME-TEOR, ROUGE); and kernel similarity of bagsof-words, bags-of-dependencies and pooled wordembeddings. ECNU's deep learning models are differentiated by their approach to sentence embeddings using either: averaged word embeddings, projected word embeddings, a deep averaging network (DAN) (Iyyer et al., 2015) or LSTM (Hochreiter and Schmidhuber, 1997). Each network feeds the element-wise multiplication, subtraction and concatenation of paired sentence embeddings to additional layers to predict similarity scores. The ensemble averages scores from the four deep learning and three feature engineered models. 16 BIT  Second place overall is achieved by BIT primarily using sentence information content (IC) informed by WordNet and BNC word frequencies. One submission uses sentence IC exclusively. Another ensembles IC with Sultan et al. (2015)'s alignment method, while a third ensembles IC with cosine similarity of summed word embeddings with an IDF weighting scheme. Sentence IC in isolation outperforms all systems except those from ECNU. Combining sentence IC with word embedding similarity performs best.\n\nHCTI (Shao, 2017) Third place overall is obtained by HCTI with a model similar to a convolutional Deep Structured Semantic Model (CDSSM) (Chen et al., 2015;Huang et al., 2013). Sentence embeddings are generated with twin convolutional neural networks (CNNs). The embeddings are then compared using cosine similarity and elementwise difference with the resulting values fed to additional layers to predict similarity labels. The architecture is abstractly similar to ECNU's deep learning models. UMDeep (Barrow and Peskov, 2017) took a similar approach using LSTMs rather than CNNs for the sentence embeddings.\n\nMITRE (Henderson et al., 2017) Fourth place overall is MITRE that, like ECNU, takes an ambitious feature engineering approach complemented by deep learning. Ensembled components include: alignment similarity; TakeLab STS (\u0160ari\u0107 et al., 2012b); string similarity measures such as matching n-grams, summarization and MT metrics (BLEU, WER, PER, ROUGE); a RNN and recurrent convolutional neural networks (RCNN) over word alignments; and a BiLSTM that is state-ofthe-art for textual entailment (Chen et al., 2016).\n\nFCICU (Hassan et al., 2017) Fifth place overall is FCICU that computes a sense-base alignment using BabelNet (Navigli and Ponzetto, 2010). Babel-Net synsets are multilingual allowing non-English and cross-lingual pairs to be processed similarly to English pairs. Alignment similarity scores are used with two runs: one that combines the scores within a string kernel and another that uses them with a weighted variant of Sultan et al. (2015)'s method. Both runs average the Babelnet based scores with soft-cardinality (Jimenez et al., 2012b).\n\nCompiLIG  The best Spanish-English performance on SNLI sentences was achieved by CompiLIG using the following cross-lingual features: conceptual similarity using DBNary (Serasset, 2015), MultiVec word embeddings (Berard et al., 2016) and character n-grams. MT is used to incorporate a similarity score based on Brychcin and Svoboda (2016)'s improvements to Sultan et al. (2015)'s method. (Nagoudi et al., 2017) Using only weighted word embeddings, LIM-LIG took second place on Arabic. 17 Arabic word embeddings are summed into sentence embeddings using uniform, POS and IDF weighting schemes. Sentence similarity is computed by cosine similarity. POS and IDF outperform uniform weighting. Combining the IDF and POS weights by multiplication is reported by LIM-LIG to achieve r 0.7667, higher than all submitted Arabic (track 1) systems.\n\n\nLIM-LIG\n\nDT Team (Maharjan et al., 2017) Second place on English (track 5) 18 is DT Team using feature engineering combined with the following deep learning models: DSSM (Huang et al., 2013), CDSSM (Shen et al., 2014) and skip-thoughts (Kiros et al., \n\n\nAnalysis\n\nFigure 1 plots model similarity scores against human STS labels for the top 5 systems from tracks 5 (English), 1 (Arabic) and 4b (Spanish-English MT). While many systems return scores on the same scale as the gold labels, 0-5, others return scores from approximately 0 and 1. Lines on the graphs illustrate perfect performance for both a 0-5 and a 0-1 scale. Mapping the 0 to 1 scores to range from 0-5, 21 approximately 80% of the scores from top performing English systems are within 1.0 pt of the gold label. Errors for Arabic are more broadly distributed, particularly for model scores between 1 and 4. The Spanish-English MT plots the weak relationship between the predicted and gold scores. Table 12 provides examples of difficult sentence pairs for participant systems and illustrates common sources of error for even well-ranking systems 19 For the cross-lingual tracks with language pair L1-L2, Duma and Menzel (2017) report additional experiments that vary the language choice for the paragraph vector model, using either L1 or L2. Experimental results are also provided that average the scores from the L1 and L2 models as well as that use vector correlation to compute similarity. 20  including: (i) word sense disambiguation \"making\" and \"preparing\" are very similar in the context of \"food\", while \"picture\" and \"movie\" are not similar when picture is followed by \"day\"; (ii) attribute importance \"outside\" vs. \"deserted\" are smaller details when contrasting \"The man is in a deserted field\" with \"The man is outside in the field\"; (iii) compositional meaning \"A man is carrying a canoe with a dog\" has the same content words as \"A dog is carrying a man in a canoe\" but carries a different meaning; (iv) negation systems score \". . . with goggles and a swimming cap\" as nearly equivalent to \". . . without goggles or a swimming cap\". Inflated similarity scores for examples like \"There is a young girl\" vs. \"There is a young boy with the woman\" demonstrate (v) semantic blending, whereby appending \"with a woman\" to \"boy\" brings its representation closer to that of \"girl\". For multilingual and cross-lingual pairs, these issues are magnified by translation errors for systems that use MT followed by the application of a monolingual similarity model. For track 4b Spanish-English MT pairs, some of the poor performance can in part be attributed to many systems using MT to re-translate the output of another MT system, obscuring errors in the original translation.\n\n\nContrasting Cross-lingual STS with MT Quality Estimation\n\nSince MT quality estimation pairs are translations of the same sentence, they are expected to be minimally on the same topic and have an STS score \u2265 1. 22 The actual distribution of STS scores is such that only 13% of the test instances score below 3, 22% of the instances score 3, 12% score 4 and 53% score 5. The high STS scores indicate that MT systems are surprisingly good at preserving meaning. However, even for a human, interpreting changes caused by translations errors can be difficult due both to disfluencies and subtle errors with important changes in meaning. The Pearson correlation between the gold MT quality scores and the gold STS scores is 0.41, which shows that translation quality measures and STS are only moderately correlated. Differences are in part explained by translation quality scores penalizing all mismatches between the source segment and its translation, whereas STS focuses on differences in meaning. However, the difficult in- 22 The evaluation data for track 4b does in fact have STS scores that are \u2265 1 for all pairs. In the 1,000 sentence training set for this track, one sentence that received a score of zero.    terpretation work required for STS annotation may increase the risk of inconsistent and subjective labels. The annotations for MT quality estimation are produced as by-product of post-editing. Humans fix MT output and the edit distance between the output and its post-edited correction provides the quality score. This post-editing based procedure is known to produce relatively consistent estimates across annotators.\n\n\nSTS Benchmark\n\nThe STS Benchmark is a careful selection of the English data sets used in SemEval and *SEM STS shared tasks between 2012 and 2017. Tables 11 and 13 provide details on the composition of the benchmark. The data is partitioned into training, development and test sets. 23 The development set can be used to design new models and tune hyperparameters. The test set should be used sparingly and only after a model design and hyperparameters have been locked against further changes. Using the STS Benchmark enables comparable assessments across different research efforts and improved tracking of the state-of-the-art. Table 14 shows the STS Benchmark results for some of the best systems from Track 5 (EN-EN) 24 and compares their performance to competitive baselines from the literature. All baselines were run by the organizers using canonical pre-trained models made available by the originator of each method, 25 with the exception of PV-DBOW that 23 Similar to the STS shared task, while the training set is provided as a convenience, researchers are encourage to incorporate other supervised and unsupervised data as long as no supervised annotations of the test partitions are used. 24 Each participant submitted the run which did best in the development set of the STS Benchmark, which happened to be the same as their best run in Track 5 in all cases.  2015) 74.3 63.9 Averaged Word Embedding Baselines LexVec\n\nWeighted matrix factorization of PPMI (Salle et al., 2016a,b) 68.9 55.8 FastText\n\nSkip-gram with sub-word character n-grams (Joulin et al., 2016) 65.2 53.9 Paragram Paraphrase Database (PPDB) fit word embeddings (Wieting et al., 2015) 63.0 50.1 GloVe Word co-occurrence count fit embeddings (Pennington et al., 2014) 52.4 40.6 Word2vec\n\nSkip-gram prediction of words in a context window (Mikolov et al., 2013a,b) 70.0 56.5 * 10-fold cross-validation on combination of dev and training data.  (2016) and InferSent which was reported independently. When multiple pre-trained models are available for a method, we report results for the one with the best dev set performance. For each method, input sentences are preprocessed to closely match the tokenization of the pre-trained models. 26 Default whether GloVe, LexVec, or Word2Vec word embeddings were used; C-PHRASE: http://clic.cimec.unitn. it/composes/cphrase-vectors.html; PV-DBOW:\n\nhttps://github.com/jhlau/doc2vec, A P -N E W S trained apnews dbow.tgz; LexVec: https: //github.com/alexandres/lexvec, embedddings lexvec.commoncrawl.300d.W.pos.vectors.gz;\n\nFastText: https://github.com/facebookresearch/ fastText/blob/master/pretrained-vectors. md, Wikipedia trained embeddings from wiki.en.vec; Paragram: http://ttic.uchicago.edu/\u02dcwieting/, embeddings trained on PPDB and tuned to WS353 from Paragram-WS353; GloVe: https://nlp.stanford. edu/projects/glove/, Wikipedia and Gigaword trained 300 dim. embeddings from glove.6B.zip; Word2vec: https://code.google.com/archive/ p/word2vec/, Google News trained embeddings from GoogleNews-vectors-negative300.bin.gz. 26 Sent2Vec: results shown here tokenized by tweetTokenize.py constrasting dev experiments used wikiTokenize.py, both distributed with Sent2Vec. LexVec: numbers were converted into words, all punctuation was removed, and text is lowercased; FastText: sentences are prepared using the normalize text() function within FastText's get-wikimedia.sh script and lowercased; Paragram: Joshua (Matt Post, 2015) pipeline to pre-process and tokenized English text; C-PHRASE, GloVe, PV-DBOW & inference hyperparameters are used unless noted otherwise. The averaged word embedding baselines compute a sentence embedding by averaging word embeddings and then using cosine to compute pairwise sentence similarity scores.\n\nWhile state-of-the-art baselines for obtaining sentence embeddings perform reasonably well on the benchmark data, improved performance is obtained by top 2017 STS shared task systems. There is still substantial room for further improvement. To follow the current state-of-the-art, visit the leaderboard on the STS wiki. 27\n\n\nConclusion\n\nWe have presented the results of the 2017 STS shared task. This year's shared task differed substantially from previous iterations of STS in that the primary emphasis of the task shifted from English to multilingual and cross-lingual STS in-SIF: PTB tokenization provided by Stanford CoreNLP  with post-processing based on dev OOVs; Word2vec: Similar to FastText, to our knownledge, the preprocessing for the pre-trained Word2vec embeddings is not publicly described. We use the following heuristics for the Word2vec experiment: All numbers longer than a single digit are converted into a '#' (e.g., 24 \u2192 ##) then prefixed, suffixed and infixed punctuation is recursively removed from each token that does not match an entry in the model's lexicon. 27 http://ixa2.si.ehu.es/stswiki/index. php/STSbenchmark volving four different languages: Arabic, Spanish, English and Turkish. Even with this substantial change relative to prior evaluations, the shared task obtained strong participation. 31 teams produced 84 system submissions with 17 teams producing a total of 44 system submissions that processed pairs in all of the STS 2017 languages. For languages that were part of prior STS evaluations (e.g., English and Spanish), state-of-the-art systems are able to achieve strong correlations with human judgment. However, we obtain weaker correlations from participating systems for Arabic, Arabic-English and Turkish-English. This suggests further research is necessary in order to develop robust models that can both be readily applied to new languages and perform well even when less supervised training data is available. To provide a standard benchmark for English STS, we present the STS Benchmark, a careful selection of the English data sets from previous STS tasks (2012)(2013)(2014)(2015)(2016)(2017). To assist in interpreting the results from new models, a number of competitive baselines and select participant systems are evaluated on the benchmark data. Ongoing improvements to the current state-of-the-art is available from an online leaderboard.\n\n\nECNU, BIT and LIM-LIG are scaled to the range 0-5. 21 snew = 5 \u00d7 s\u2212min(s) max(s)\u2212min(s) is used to rescale scores.\n\nFigure 1 :\n1Model vs. human similarity scores for top systems.\n\nTable 1 :\n1Similarity scores with explanations and English examples from\n\nTable 2 :\n2STS 2017 evaluation data.3.1 Tracks \n\nTable 2 summarizes the evaluation data by track. \nThe six tracks span four languages: Arabic, En-\nglish, Spanish and Turkish. Track 4 has subtracks \nwith 4a drawing from SNLI and 4b pulling from \n\n\nTable 4 :\n4Spanish training data.\n\nTable 5 :\n5Spanish-English training data.Year \nData set \nPairs Source \n2017 Trial \n23 Mixed STS 2016 \n2017 MSRpar \n510 newswire \n2017 MSRvid \n368 videos \n2017 SMTeuroparl \n203 WMT eval. \n\n\n\nTable 6 :\n6Arabic training data.ity estimation data has sentences that are much \nlonger. The track 5 English data has an average \nsentence length of 8.7 words, while the English \nsentences from track 4b have an average length of \n19.4. The English training data has the following \naverage lengths: 2012 10.8 words; 2013 8.8 words \n(excludes restricted SMT data); 2014 9.1 words; \n2015 11.5 words; 2016 13.8 words. \n\n\nTable 7 :\n7Arabic-English training data.Year \nData set \nPairs Source \n2017 MSRpar \n1039 newswire \n2017 MSRvid \n749 videos \n2017 SMTeuroparl \n422 WMT eval. \n\n\n\nTable 8 :\n8Arabic-English parallel data.\n\nTable 10 .\n10Especially challenging tracks with SNLI data are: track 1, Arabic; track 2, Arabic-English; and track 6, English-Turkish. Spanish-English performance is much higher on track 4a's SNLI data thanECNU is best overall (avg r: 0.7316) and achieves \nthe highest participant evaluation score on: track \n2, Arabic-English (r: 0.7493); track 3, Spanish (r: \n0.8559); and track 6, Turkish-English (r: 0.7706). \nBIT attains the best performance on track 1, Arabic \n(r: 0.7543). CompiLIG places first on track 4a, \nSNLI Spanish-English (r: 0.8302). SEF@UHH \nexhibits the best correlation on the difficult track \n4b WMT quality estimation pairs (r: 0.3407). RTV \nhas the best system for the track 5 English data (r: \n0.8547), followed closely by DT Team (r: 0.8536). \n\n\nTable 9 :\n9Participation by shared task track.\n\nTable 11 :\n11STS Benchmark annotated examples \nby genres (rows) and by train, dev. test splits \n(columns). \n\n2015). Engineered features include: unigram over-\nlap, summed word alignments scores, fraction of \nunaligned words, difference in word counts by type \n(all, adj, adverbs, nouns, verbs), and min to max \nratios of words by type. Select features have a mul-\ntiplicative penalty for unaligned words. \n\nSEF@UHH (Duma and Menzel, 2017) First \nplace on the challenging Spanish-English MT pairs \n(Track 4b) is SEF@UHH. Paragraph vector mod-\nels (Le and Mikolov, 2014) are trained for Arabic, \nEnglish, Spanish and Turkish. MT converts cross-\nlingual pairs into a single language and similar-\nity scores are computed using cosine or the nega-\ntion of Bray-Curtis dissimilarity. The best perform-\ning submission on track 4b uses cosine similarity \nof Spanish paragraph vectors with MT converting \npaired English sentences into Spanish. 19 \n\n\n\nTable 12 :\n12Difficult English sentence pairs (Track 5) and scores assigned by top performing systems. 20Genre \nFile \nYr. Train Dev Test \nnews \nMSRpar \n12 \n1000 \n250 \n250 \nnews \nheadlines \n13/6 \n1999 \n250 \n250 \nnews \ndeft-news \n14 \n300 \n0 \n0 \ncaptions MSRvid \n12 \n1000 \n250 \n250 \ncaptions images \n14/5 \n1000 \n250 \n250 \ncaptions track5.en-en \n17 \n0 \n125 \n125 \nforum \ndeft-forum \n14 \n450 \n0 \n0 \nforum \nans-forums \n15 \n0 \n375 \n0 \nforum \nans-ans \n16 \n0 \n0 \n254 \n\n\n\nTable 13 :\n13STS Benchmark detailed break-down by files and years.\n\n\n25 Sent2Vec: https://github.com/epfml/ sent2vec, trained model Sent2Vec twitter unigrams; SIF: https://github.com/epfml/sent2vec Wikipedia trained word frequencies enwiki vocab min200.txt, https://github.com/alexandres/lexvec embeddings from lexvec.commoncrawl.300d.W+C.pos.vectors, first 15 principle components removed, \u03b1 = 0.001, dev experiments varied \u03b1, principle components removed and STS 2017 Participants on STS Benchmark Name Description Dev Test ECNU Ensembles well performing feature eng. models with deep neural networks each using sent. 84.7 81.0 emb. from either LSTM, DAN, prj. word emb. or avg. word emb. (Tian et al., 2017) BIT Ensembles sent. information content (IC) with cosine of sent. emb. derived from summed 82.9 80.9 word emb. with IDF weighting scheme (Wu et al., 2017) DT TEAM Ensembles feature eng. and deep learning signals using sent. emb. from DSSM, CDSSM 83.0 79.2 and skip-thought models (Maharjan et al., 2017) UdL Feature eng. model using cosine of tf-idf weighted char n-grams, num. match, sent. length 72.4 79.0 and avg. word emb. cosine over PoS and NER based alignments (Al-Natsheh et al., 2017) HCTI Deep learning model with sent. emb. computed using paired convolutional neural networks 83.4 78.4 (CNN) and then compared using fully connected layers (Shao, 2017) RTM Referential translation machines (RTM) use a feature eng. model with transductive learning 73.2 * 70.6 and parallel feature decay algorithm (ParFDA) training instance selection (Bi\u00e7ici, 2017b,a) SEF@UHH Cosine of paragraph vector (PV-DBOW) sent. emb. (Duma and Menzel, 2017) PHRASE Word emb. sum from model of syntactic constituent context words (Pham et al.,61.6 59.2 \nSentence Level Baselines \nInferSent \nSent. emb. from bi-directional LSTM trained on SNLI (Conneau et al., 2017) \n80.1 75.8 \nSent2Vec \nWord & bigram emb. sum from sent. spanning CBOW (Pagliardini et al., 2017) \n78.7 75.5 \nSIF \nWeighted word emb. sum with principle component removal (Arora et al., 2017) \n80.1 72.0 \nPV-DBOW \nParagraph vectors (PV-DBOW) (Le and Mikolov, 2014; Lau and Baldwin, 2016) \n72.2 64.9 \nC-\n\nTable 14 :\n14STS Benchmark. Pearson's r \u00d7 100 results for select participants and baseline models. uses the model from Lau and Baldwin\nPrevious years of the STS shared task include more data sources. This year the task draws from two data sources and includes a diverse set of languages and language-pairs. 4 HTER is the minimal number of edits required for correction of a translation divided by its length after correction.\nProduced by the Google Translate API. 6 http://www.sdl.com/languagecloud/ managed-translation/\nhttps://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com\nThe two remaining ECNU runs only use either RF or GB and exclude the deep learning models.\nThe approach is similar to SIF(Arora et al., 2017) but without removal of the common principle component 18 RTV took first place on track 5, English, but submitted no system description paper.\nAcknowledgmentsWe thank Alexis Conneau for the evaluation of InferSent on the STS Benchmark. This material is based in part upon work supported by QNRF-NPRP 6 -1020-1-199 OPTDIAC that funded Arabic translation, and by a grant from the Spanish MINECO (projects TUNER TIN2015-65308-C5-1-R and MUSTER PCIN-2015-226 cofunded by EU FEDER) that funded STS label annotation and by the QT21 EU project (H2020 No. 645452) that funded STS labels and data preparation for machine translation pairs. I\u00f1igo Lopez-Gazpio is supported by the Spanish MECD. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of QNRF-NPRP, Spanish MINECO, QT21 EU, or the Spanish MECD.\nAns.-student 750 student answers. Ans.-student 750 student answers\n\nQ&A forum answers 2015 Belief 375 committed belief 2016 HDL 249 newswire headlines 2016 Plagiarism 230 short-answer plag. post-editing 244 MT posteditsAns.-forum 375 Q&A forum answers 2015 Belief 375 committed belief 2016 HDL 249 newswire headlines 2016 Plagiarism 230 short-answer plag. 2016 post-editing 244 MT postedits\n\nAns. 254 Q&A forum answers 2016 Quest.-Quest. 209 Q&A forum questions 2017 Trial 23 Mixed STS. Ans.-Ans. 254 Q&A forum answers 2016 Quest.-Quest. 209 Q&A forum questions 2017 Trial 23 Mixed STS 2016\n\nTable 3: English training data. Year Data set Pairs Source. Table 3: English training data. Year Data set Pairs Source\n\n. Sts Mixed, Mixed STS 2016\n\nSemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. Carmen References Eneko Agirre, Claire Banea, Daniel Cardie, Mona Cer, Aitor Diab, Weiwei Gonzalez-Agirre, I\u00f1igo Guo, Montse Lopez-Gazpio, Rada Maritxalar, German Mihalcea, Larraitz Rigau, Janyce Uria, Wiebe, Proceedings of SemEval. SemEvalReferences Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I\u00f1igo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, Ger- man Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, En- glish, Spanish and Pilot on Interpretability. In Proceedings of SemEval 2015. http://www.aclweb.org/anthology/S15- 2045.\n\nSemEval-2014 Task 10: Multilingual semantic textual similarity. Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, Janyce Wiebe, Proceedings of SemEval. SemEvalEneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 Task 10: Multilingual semantic tex- tual similarity. In Proceedings of SemEval 2014.\n\nSemeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, Janyce Wiebe, Proceedings of the SemEval-2016. the SemEval-2016Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. Semeval-2016 task 1: Se- mantic textual similarity, monolingual and cross-lingual evaluation. In Proceedings of the SemEval-2016.\n\nSemEval-2012 Task 6: A pilot on semantic textual similarity. Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Proceedings of *SEM 2012/SemEval. *SEM 2012/SemEvalEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez- Agirre. 2012. SemEval-2012 Task 6: A pilot on semantic textual similarity. In Proceedings of *SEM 2012/SemEval 2012. http://www.aclweb.org/anthology/S12-1051.\n\n*SEM 2013 shared task: Semantic Textual Similarity. Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Proceedings of *SEM 2013. *SEM 2013Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Se- mantic Textual Similarity. In Proceedings of *SEM 2013. http://www.aclweb.org/anthology/S13-1004.\n\nUdL at SemEval-2017 Task 1: Semantic textual similarity estimation of english sentence pairs using regression model over pairwise features. T Hussein, Lucie Al-Natsheh, Fabrice Martinet, Djamel Muhlenbach, Zighed Abdelkader, Proceedings of SemEval-2017. SemEval-2017Hussein T. Al-Natsheh, Lucie Martinet, Fabrice Muhlen- bach, and Djamel Abdelkader ZIGHED. 2017. UdL at SemEval-2017 Task 1: Semantic textual similarity esti- mation of english sentence pairs using regression model over pairwise features. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2013.\n\nA simple but tough-to-beat baseline for sentence embeddings. Sanjeev Arora, Yingyu Liang, Tengyu Ma, Proceedings of ICLR 2017. ICLR 2017Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sen- tence embeddings. In Proceedings of ICLR 2017.\n\nLIPN-IIMAS at SemEval-2017 Task 1: Subword embeddings, attention recurrent neural networks and cross word alignment for semantic textual similarity. Ignacio Arroyo, - Fern\u00e1ndez, Ivan Vladimir Meza Ruiz, Proceedings of SemEval-2017. SemEval-2017Ignacio Arroyo-Fern\u00e1ndez and Ivan Vladimir Meza Ruiz. 2017. LIPN-IIMAS at SemEval-2017 Task 1: Sub- word embeddings, attention recurrent neural net- works and cross word alignment for semantic tex- tual similarity. In Proceedings of SemEval-2017.\n\nThe Berkeley FrameNet Project. Collin F Baker, Charles J Fillmore, John B Lowe, Proceedings of COLING '98. COLING '98Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLING '98. http://aclweb.org/anthology/P/P98/P98- 1013.pdf.\n\nUkp: Computing semantic textual similarity by combining multiple content similarity measures. Daniel B\u00e4r, Chris Biemann, Iryna Gurevych, Torsten Zesch, Proceedings of *SEM 2012/SemEval. *SEM 2012/SemEvalDaniel B\u00e4r, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual sim- ilarity by combining multiple content similarity mea- sures. In Proceedings of *SEM 2012/SemEval 2012. http://www.aclweb.org/anthology/S12-1059.\n\nUMDeep at SemEval-2017 Task 1: End-to-end shared weight LSTM model for semantic textual similarity. Joe Barrow, Denis Peskov, Proceedings of SemEval-2017. SemEval-2017Joe Barrow and Denis Peskov. 2017. UMDeep at SemEval- 2017 Task 1: End-to-end shared weight LSTM model for semantic textual similarity. In Proceedings of SemEval- 2017. http://www.aclweb.org/anthology/S17-2026.\n\nSICK through the SemEval glasses. lesson learned from the evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Luisa Bentivogli, Raffaella Bernardi, Marco Marelli, Stefano Menini, Marco Baroni, Roberto Zamparelli, 10.1007/s10579-015-9332-5Lang Resour Eval. 501Luisa Bentivogli, Raffaella Bernardi, Marco Marelli, Ste- fano Menini, Marco Baroni, and Roberto Zamparelli. 2016. SICK through the SemEval glasses. lesson learned from the evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Lang Resour Eval 50(1):95-124. https://doi.org/10.1007/s10579-015-9332-5.\n\nMultiVec: a multilingual and multilevel representation learning toolkit for NLP. Alexandre Berard, Christophe Servan, Olivier Pietquin, Laurent Besacier, Proceedings of LREC 2016. LREC 2016Alexandre Berard, Christophe Servan, Olivier Pietquin, and Laurent Besacier. 2016. MultiVec: a multilin- gual and multilevel representation learning toolkit for NLP. In Proceedings of LREC 2016. http://www.lrec- conf.org/proceedings/lrec2016/pdf/666 Paper.pdf.\n\nPredicting translation performance with referential translation machines. Ergun Bi\u00e7ici, Proceedings of WMT17. WMT17to appearErgun Bi\u00e7ici. 2017a. Predicting translation performance with referential translation machines. In Proceedings of WMT17 (to appear).\n\nRTM at SemEval-2017 Task 1: Referential translation machines for predicting semantic similarity. Ergun Bi\u00e7ici, Proceedings of SemEval-2017. SemEval-2017Ergun Bi\u00e7ici. 2017b. RTM at SemEval-2017 Task 1: Referential translation machines for predicting se- mantic similarity. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2030.\n\nResSim at SemEval-2017 Task 1: Multilingual word representations for semantic textual similarity. Johannes Bjerva, Robert\u00f6stling , Proceedings of SemEval-2017. SemEval-2017Johannes Bjerva and Robert\u00d6stling. 2017. ResSim at SemEval-2017 Task 1: Multilingual word representations for semantic textual similarity. In Proceedings of SemEval- 2017. http://www.aclweb.org/anthology/S17-2021.\n\nFindings of the 2014 workshop on statistical machine translation. Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, Ale\u0161 Tamchyna, Proceedings of WMT 2014. WMT 2014Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale\u0161 Tamchyna. 2014. Findings of the 2014 workshop on statisti- cal machine translation. In Proceedings of WMT 2014. http://www.aclweb.org/anthology/W/W14/W14- 3302.pdf.\n\nOnd\u0159ej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, Lucia Specia, Findings of the 2013 Workshop on Statistical Machine Translation. Proceedings of WMT 2013Ond\u0159ej Bojar, Christian Buck, Chris Callison-Burch, Chris- tian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of WMT 2013. http://www.aclweb.org/anthology/W13-2201.\n\nA large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, Proceedings of EMNLP 2015. EMNLP 2015Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of EMNLP 2015. http://aclweb.org/anthology/D/D15/D15- 1075.pdf.\n\nUWB at SemEval-2016 Task 1: Semantic textual similarity using lexical, syntactic, and semantic information. Tomas Brychcin, Lukas Svoboda, Proceedings of SemEval. SemEvalTomas Brychcin and Lukas Svoboda. 2016. UWB at SemEval-2016 Task 1: Semantic textual sim- ilarity using lexical, syntactic, and semantic in- formation. In Proceedings of SemEval 2016.\n\nEnhancing and combining sequential and tree LSTM for natural language inference. Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, CoRR abs/1609.06038Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, and Hui Jiang. 2016. Enhancing and combining sequential and tree LSTM for natural language inference. CoRR abs/1609.06038. http://arxiv.org/abs/1609.06038.\n\nLearning bidirectional intent embeddings by convolutional deep structured semantic models for spoken language understanding. Yun-Nung Chen, Dilek Hakkani-T\u00fcr, Xiaodong He, Proceedings of NIPS-SLU. NIPS-SLUYun-Nung Chen, Dilek Hakkani-T\u00fcr, and Xiaodong He. 2015. Learning bidirectional intent embeddings by convolutional deep structured semantic models for spoken language understanding. In Proceedings of NIPS-SLU, 2015. https://www.microsoft.com/en- us/research/publication/learning-bidirectional-intent- embeddings-by-convolutional-deep-structured-semantic- models-for-spoken-language-understanding/.\n\nSupervised learning of universal sentence representations from natural language inference data. Alexis Conneau, Douwe Kiela, Holger Schwenk, CoRR abs/1705.02364Lo\u00efc Barrault, and Antoine BordesAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Bar- rault, and Antoine Bordes. 2017. Supervised learn- ing of universal sentence representations from natu- ral language inference data. CoRR abs/1705.02364. http://arxiv.org/abs/1705.02364.\n\nRecognizing textual entailment: Rational, evaluation and approaches. Bill Ido Dagan, Bernardo Dolan, Dan Magnini, Roth, 10.1017/S1351324909990234J. Nat. Language Eng. 16Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2010. Recognizing textual entailment: Rational, evalua- tion and approaches. J. Nat. Language Eng. 16:105-105. https://doi.org/10.1017/S1351324909990234.\n\ncocor: A comprehensive solution for the statistical comparison of correlations. Birk Diedenhofen, Jochen Musch, 10.1371/journal.pone.0121945PLoS ONE. 104Birk Diedenhofen and Jochen Musch. 2015. co- cor: A comprehensive solution for the statisti- cal comparison of correlations. PLoS ONE 10(4).\n\n. 10.1371/journal.pone.0121945http://dx.doi.org/10.1371/journal.pone.0121945.\n\nUnsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. Bill Dolan, Chris Quirk, Chris Brockett, Proceedings of COLING 04. COLING 04Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsu- pervised construction of large paraphrase corpora: Ex- ploiting massively parallel news sources. In Proceedings of COLING 04. http://aclweb.org/anthology/C/C04/C04- 1051.pdf.\n\nSEF@UHH at SemEval-2017 Task 1: Unsupervised knowledge-free semantic textual similarity via paragraph vector. Stefania Mirela, Wolfgang Duma, Menzel, Proceedings of SemEval-2017. SemEval-2017Mirela-Stefania Duma and Wolfgang Menzel. 2017. SEF@UHH at SemEval-2017 Task 1: Unsuper- vised knowledge-free semantic textual similarity via paragraph vector. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2024.\n\nLump at SemEval-2017 Task 1: Towards an interlingua semantic similarity. Cristina Espa\u00f1a Bonet, Alberto Barr\u00f3n-Cede\u00f1o, Proceedings of SemEval-2017. SemEval-2017Cristina Espa\u00f1a Bonet and Alberto Barr\u00f3n-Cede\u00f1o. 2017. Lump at SemEval-2017 Task 1: Towards an interlingua semantic similarity. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2019.\n\nWordNet: An Electronic Lexical Database. Christiane Fellbaum, MIT PressChristiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.\n\nCompiLIG at SemEval-2017 Task 1: Cross-language plagiarism detection methods for semantic textual similarity. J\u00e9r\u00e9my Ferrero, Laurent Besacier, Didier Schwab, Fr\u00e9d\u00e9ric Agn\u00e8s, Proceedings of SemEval-2017. SemEval-2017J\u00e9r\u00e9my Ferrero, Laurent Besacier, Didier Schwab, and Fr\u00e9d\u00e9ric Agn\u00e8s. 2017. CompiLIG at SemEval-2017 Task 1: Cross-language plagiarism detection methods for seman- tic textual similarity. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2012.\n\nL2f/inesc-id at semeval-2017 tasks 1 and 2: Lexical and semantic features in word and textual similarity. Pedro Fialho, Hugo Patinho Rodrigues, Lu\u00edsa Coheur, Paulo Quaresma, Proceedings of SemEval-2017. SemEval-2017Pedro Fialho, Hugo Patinho Rodrigues, Lu\u00edsa Coheur, and Paulo Quaresma. 2017. L2f/inesc-id at semeval-2017 tasks 1 and 2: Lexical and semantic features in word and textual similarity. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2032.\n\nPPDB: The paraphrase database. Juri Ganitkevitch, Benjamin Van Durme, Chris Callison-Burch, Proceedings of NAACL/HLT. NAACL/HLTJuri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of NAACL/HLT 2013.\n\nFCICU at SemEval-2017 Task 1: Sense-based language independent semantic textual similarity approach. Basma Hassan, Samir Abdelrahman, Reem Bahgat, Ibrahim Farag, Proceedings of SemEval-2017. SemEval-2017Basma Hassan, Samir AbdelRahman, Reem Bahgat, and Ibrahim Farag. 2017. FCICU at SemEval-2017 Task 1: Sense-based language independent semantic textual similarity approach. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2015.\n\nMultiperspective sentence similarity modeling with convolutional neural networks. Hua He, Kevin Gimpel, Jimmy Lin, Proceedings of EMNLP. EMNLPHua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi- perspective sentence similarity modeling with convolu- tional neural networks. In Proceedings of EMNLP. pages 1576-1586. http://aclweb.org/anthology/D15-1181.\n\nPairwise word interaction modeling with deep neural networks for semantic similarity measurement. Hua He, Jimmy Lin, Proceedings of NAACL/HLT. NAACL/HLTHua He and Jimmy Lin. 2016. Pairwise word interaction modeling with deep neural networks for semantic sim- ilarity measurement. In Proceedings of NAACL/HLT. http://www.aclweb.org/anthology/N16-1108.\n\nUMD-TTIC-UW at SemEval-2016 Task 1: Attention-based multi-perspective convolutional neural networks for textual similarity measurement. Hua He, John Wieting, Kevin Gimpel, Jinfeng Rao, Jimmy Lin, Proceedings of SemEval. SemEvalHua He, John Wieting, Kevin Gimpel, Jinfeng Rao, and Jimmy Lin. 2016. UMD-TTIC-UW at SemEval- 2016 Task 1: Attention-based multi-perspective convolutional neural networks for textual similar- ity measurement. In Proceedings of SemEval 2016. http://www.anthology.aclweb.org/S/S16/S16-1170.pdf.\n\nMITRE at SemEval-2017 Task 1: Simple semantic similarity. John Henderson, Elizabeth Merkhofer, Laura Strickhart, Guido Zarrella, Proceedings of SemEval-2017. SemEval-2017John Henderson, Elizabeth Merkhofer, Laura Strickhart, and Guido Zarrella. 2017. MITRE at SemEval-2017 Task 1: Simple semantic similarity. In Proceedings of SemEval- 2017. http://www.aclweb.org/anthology/S17-2027.\n\nLearning distributed representations of sentences from unlabelled data. Felix Hill, Kyunghyun Cho, Anna Korhonen, Proceedings of NAACL/HLT. NAACL/HLTFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of NAACL/HLT. http://www.aclweb.org/anthology/N16-1162.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Comput. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Comput. 9(8):1735-1780. http://dx.doi.org/10.1162/neco.1997.9.8.1735.\n\nOntoNotes: The 90% solution. Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, Ralph Weischedel, Proceedings of NAACL/HLT. NAACL/HLTEduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% solution. In Proceedings of NAACL/HLT 2006. http://aclweb.org/anthology/N/N06/N06-2015.pdf.\n\nLearning deep structured semantic models for web search using clickthrough data. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, Larry Heck, Proceedings of CIKM. CIKMPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of CIKM. https://www.microsoft.com/en- us/research/publication/learning-deep-structured- semantic-models-for-web-search-using-clickthrough- data/.\n\nDeep unordered composition rivals syntactic methods for text classification. Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, Hal Daum\u00e9, Iii , Proceedings of ACL/IJCNLP. ACL/IJCNLPMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum\u00e9 III. 2015. Deep unordered composition rivals syntactic methods for text classification. In Proceedings of ACL/IJCNLP. http://www.aclweb.org/anthology/P15- 1162.\n\nSoft cardinality: A parameterized similarity function for text comparison. Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, Proceedings of *SEM 2012/Se-mEval 2012. *SEM 2012/Se-mEval 2012Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012a. Soft cardinality: A parameterized similarity func- tion for text comparison. In Proceedings of *SEM 2012/Se- mEval 2012. http://www.aclweb.org/anthology/S12-1061.\n\nSoft Cardinality: A parameterized similarity function for text comparison. Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, Proceedings of *SEM 2012/Se-mEval 2012. *SEM 2012/Se-mEval 2012Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012b. Soft Cardinality: A parameterized similarity func- tion for text comparison. In Proceedings of *SEM 2012/Se- mEval 2012. http://aclweb.org/anthology/S/S12/S12- 1061.pdf.\n\nBag of tricks for efficient text classification. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, CoRR abs/1607.01759Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of tricks for ef- ficient text classification. CoRR abs/1607.01759.\n\nSiamese cbow: Optimizing word embeddings for sentence representations. Tom Kenter, Alexey Borisov, Maarten De Rijke, Proceedings of ACL. ACLTom Kenter, Alexey Borisov, and Maarten de Rijke. 2016. Siamese cbow: Optimizing word embeddings for sentence representations. In Proceedings of ACL. http://www.aclweb.org/anthology/P16-1089.\n\nRaquel Urtasun, and Sanja Fidler. Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Skip-thought vectors. CoRR abs/1506.06726. Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fi- dler. 2015. Skip-thought vectors. CoRR abs/1506.06726. http://arxiv.org/abs/1506.06726.\n\nSTS-UHH at SemEval-2017 Task 1: Scoring semantic textual similarity using supervised and unsupervised ensemble. Sarah Kohail, Amr Rekaby Salama, Chris Biemann, Proceedings of SemEval-2017. SemEval-2017Sarah Kohail, Amr Rekaby Salama, and Chris Biemann. 2017. STS-UHH at SemEval-2017 Task 1: Scoring semantic textual similarity using supervised and unsu- pervised ensemble. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2025.\n\nAn empirical evaluation of doc2vec with practical insights into document embedding generation. Han Jey, Timothy Lau, Baldwin, Proceedings of ACL Workshop on Representation Learning for NLP. ACL Workshop on Representation Learning for NLPJey Han Lau and Timothy Baldwin. 2016. An em- pirical evaluation of doc2vec with practical insights into document embedding generation. In Proceed- ings of ACL Workshop on Representation Learning for NLP. http://www.aclweb.org/anthology/W/W16/W16- 1609.pdf.\n\nDistributed representations of sentences and documents. V Quoc, Tomas Le, Mikolov, CoRR abs/1405.4053Quoc V. Le and Tomas Mikolov. 2014. Distributed represen- tations of sentences and documents. CoRR abs/1405.4053. http://arxiv.org/abs/1405.4053.\n\nPurdueNLP at SemEval-2017 Task 1: Predicting semantic textual similarity with paraphrase and event embeddings. I-Ta Lee, Mahak Goindani, Chang Li, Di Jin, Kristen Marie Johnson, Xiao Zhang, Maria Leonor Pacheco, Dan Goldwasser, Proceedings of SemEval-2017. SemEval-2017I-Ta Lee, Mahak Goindani, Chang Li, Di Jin, Kristen Marie Johnson, Xiao Zhang, Maria Leonor Pacheco, and Dan Goldwasser. 2017. PurdueNLP at SemEval-2017 Task 1: Predicting semantic textual similarity with paraphrase and event embeddings. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2029.\n\nITNLP-AiKF at SemEval-2017 Task 1: Rich features based svr for semantic textual similarity computing. Wenjie Liu, Chengjie Sun, Lei Lin, Bingquan Liu, Proceedings of SemEval-2017. SemEval-2017Wenjie Liu, Chengjie Sun, Lei Lin, and Bingquan Liu. 2017. ITNLP-AiKF at SemEval-2017 Task 1: Rich features based svr for semantic textual simi- larity computing. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2022.\n\nDt team at semeval-2017 task 1: Semantic similarity using alignments, sentence-level embeddings and gaussian mixture model output. Nabin Maharjan, Rajendra Banjade, Dipesh Gautam, Lasang J Tamang, Vasile Rus, Proceedings of SemEval-2017. SemEval-2017Nabin Maharjan, Rajendra Banjade, Dipesh Gautam, Lasang J. Tamang, and Vasile Rus. 2017. Dt team at semeval-2017 task 1: Semantic similarity using align- ments, sentence-level embeddings and gaussian mix- ture model output. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2014.\n\nThe Stanford CoreNLP natural language processing toolkit. Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, David Mcclosky, Proceedings of ACL 2014 Demonstrations. ACL 2014 DemonstrationsChristopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language process- ing toolkit. In Proceedings of ACL 2014 Demonstrations. http://www.aclweb.org/anthology/P/P14/P14-5010.\n\nA SICK cure for the evaluation of compositional distributional semantic models. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, Roberto Zamparelli, Proceedings of LREC 14. LREC 14Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zam- parelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of LREC 14. http://www.lrec- conf.org/proceedings/lrec2014/pdf/363 Paper.pdf.\n\nJoshua 6: A phrase-based and hierarchical statistical machine translation. Yuan Cao Gaurav Kumar Matt Post, The Prague Bulletin of Mathematical Linguistics. 104516Yuan Cao Gaurav Kumar Matt Post. 2015. Joshua 6: A phrase-based and hierarchical statistical machine transla- tion. The Prague Bulletin of Mathematical Linguistics 104:516. https://ufal.mff.cuni.cz/pbml/104/art-post-cao- kumar.pdf.\n\nQLUT at SemEval-2017 Task 1: Semantic textual similarity based on word embeddings. Fanqing Meng, Wenpeng Lu, Yuteng Zhang, Jinyong Cheng, Yuehan Du, Shuwang Han, Proceedings of SemEval-2017. SemEval-2017Fanqing Meng, Wenpeng Lu, Yuteng Zhang, Jinyong Cheng, Yuehan Du, and Shuwang Han. 2017. QLUT at SemEval- 2017 Task 1: Semantic textual similarity based on word embeddings. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2020.\n\nEfficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, CoRR abs/1301.3781Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word repre- sentations in vector space. CoRR abs/1301.3781.\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Proceedings of NIPS 2013. NIPS 2013Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Pro- ceedings of NIPS 2013. http://papers.nips.cc/paper/5021- distributed-representations-of-words-and-phrases-and- their-compositionality.pdf.\n\nWordNet: A lexical database for english. A George, Miller, 10.1145/219717.219748Commun. ACM. 3811George A. Miller. 1995. WordNet: A lexical database for english. Commun. ACM 38(11):39-41. https://doi.org/10.1145/219717.219748.\n\nEfficient convolution kernels for dependency and constituent syntactic trees. Alessandro Moschitti, 10.1007/11871842_32Proceedings of ECML'06. ECML'06Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceed- ings of ECML'06. http://dx.doi.org/10.1007/11871842 32.\n\nRepresenting sentences as low-rank subspaces. Jiaqi Mu, Suma Bhat, Pramod Viswanath, CoRR abs/1704.05358Jiaqi Mu, Suma Bhat, and Pramod Viswanath. 2017. Rep- resenting sentences as low-rank subspaces. CoRR abs/1704.05358. http://arxiv.org/abs/1704.05358.\n\nLIM-LIG at SemEval-2017 Task1: Enhancing the semantic similarity for arabic sentences with vectors weighting. Billah El Moatez, J\u00e9r\u00e9my Nagoudi, Didier Ferrero, Schwab, Proceedings of SemEval-2017. SemEval-2017El Moatez Billah Nagoudi, J\u00e9r\u00e9my Ferrero, and Didier Schwab. 2017. LIM-LIG at SemEval-2017 Task1: En- hancing the semantic similarity for arabic sentences with vectors weighting. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2017.\n\nBabelNet: Building a very large multilingual semantic network. Roberto Navigli, Simone Paolo Ponzetto, Proceedings of ACL 2010. ACL 2010Roberto Navigli and Simone Paolo Ponzetto. 2010. BabelNet: Building a very large multilingual se- mantic network. In Proceedings of ACL 2010.\n\nUnsupervised Learning of Sentence Embeddings using Compositional n-Gram Features. Matteo Pagliardini, Prakhar Gupta, Martin Jaggi, Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. 2017. Unsupervised Learning of Sentence Embed- dings using Compositional n-Gram Features. arXiv https://arxiv.org/pdf/1703.02507.pdf.\n\nRobert Parker, David Graff, Junbo Kong, Ke Chen, Kazuaki Maeda, Gigaword Fifth Edition LDC2011T07. Linguistic Data Consortium. Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. Gigaword Fifth Edi- tion LDC2011T07. Linguistic Data Consortium.\n\nGloVe: Global Vectors for Word Representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of EMNLP. EMNLPJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of EMNLP 2014.\n\nJointly optimizing word representations for lexical and sentential tasks with the c-phrase model. Germ\u00e1n Nghia The Pham, Angeliki Kruszewski, Marco Lazaridou, Baroni, Proceedings of ACL/IJCNLP. ACL/IJCNLPNghia The Pham, Germ\u00e1n Kruszewski, Angeliki Lazari- dou, and Marco Baroni. 2015. Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model. In Proceedings of ACL/IJCNLP. http://www.aclweb.org/anthology/P15-1094.\n\nTask-oriented intrinsic evaluation of semantic textual similarity. Nils Reimers, Philip Beyer, Iryna Gurevych, Proceedings of COLING. COLINGNils Reimers, Philip Beyer, and Iryna Gurevych. 2016. Task-oriented intrinsic evaluation of semantic tex- tual similarity. In Proceedings of COLING 2016.\n\n. Barbara Rychalska, Katarzyna Pakulska, Krystyna Chodorowska, Wojciech Walczak, Piotr Andruszkiewicz, Samsung Poland NLP Team at SemEval-2016Barbara Rychalska, Katarzyna Pakulska, Krystyna Chodor- owska, Wojciech Walczak, and Piotr Andruszkiewicz. 2016. Samsung Poland NLP Team at SemEval-2016\n\nTask 1: Necessity for diversity; combining recursive autoencoders, wordnet and ensemble methods to measure semantic similarity. Proceedings of SemEval-2016. SemEval-2016Task 1: Necessity for diversity; combining recursive au- toencoders, wordnet and ensemble methods to measure semantic similarity. In Proceedings of SemEval-2016. http://www.aclweb.org/anthology/S16-1091.\n\nEnhancing the lexvec distributed word representation model using positional contexts and external memory. Alexandre Salle, Marco Idiart, Aline Villavicencio, CoRR abs/1606.01283Alexandre Salle, Marco Idiart, and Aline Villavicencio. 2016a. Enhancing the lexvec distributed word representation model using positional contexts and external memory. CoRR abs/1606.01283. http://arxiv.org/abs/1606.01283.\n\nMatrix factorization using window sampling and negative sampling for improved word representations. Alexandre Salle, Marco Idiart, Aline Villavicencio, Proceedings of ACL. ACLAlexandre Salle, Marco Idiart, and Aline Villavicencio. 2016b. Matrix factorization using window sampling and negative sampling for improved word representations. In Proceed- ings of ACL. http://aclweb.org/anthology/P16-2068.\n\nDBnary: Wiktionary as a lemonbased multilingual lexical resource in RDF. Gilles Serasset, 10.3233/SW-140147Semantic Web Journal (special issue on Multilingual Linked Open Data). 6Gilles Serasset. 2015. DBnary: Wiktionary as a lemon- based multilingual lexical resource in RDF. Semantic Web Journal (special issue on Multilingual Linked Open Data) 6:355-361. https://doi.org/10.3233/SW-140147.\n\nHCTI at SemEval-2017 Task 1: Use convolutional neural network to evaluate semantic textual similarity. Yang Shao, Proceedings of SemEval-2017. SemEval-2017Yang Shao. 2017. HCTI at SemEval-2017 Task 1: Use convolutional neural network to evaluate seman- tic textual similarity. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2016.\n\nA latent semantic model with convolutional-pooling structure for information retrieval. Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, Gregoire Mesnil, Proceedings of CIKM '14. CIKM '14Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gregoire Mesnil. 2014. A latent semantic model with convolutional-pooling structure for in- formation retrieval. In Proceedings of CIKM '14.\n\nA study of translation edit rate with targeted human annotation. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, John Makhoul, Proceedings of AMTA. AMTAMatthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Mic- ciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceed- ings of AMTA 2006. http://mt-archive.info/AMTA-2006- Snover.pdf.\n\nOPI-JSA at SemEval-2017 Task 1: Application of ensemble learning for computing semantic textual similarity. Piotr Martyna\u015bpiewak, Daniel Sobecki, Kara\u015b, Proceedings of SemEval-2017. SemEval-2017Martyna\u015apiewak, Piotr Sobecki, and Daniel Kara\u015b. 2017. OPI-JSA at SemEval-2017 Task 1: Appli- cation of ensemble learning for computing semantic textual similarity. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2018.\n\nDLS@CU: Sentence similarity from word alignment and semantic vector composition. Steven Md Arafat Sultan, Tamara Bethard, Sumner, Proceedings of SemEval. SemEvalMd Arafat Sultan, Steven Bethard, and Tamara Sumner. 2015. DLS@CU: Sentence similarity from word alignment and semantic vector composition. In Proceedings of SemEval 2015. http://aclweb.org/anthology/S/S15/S15-2027.pdf.\n\nECNU at SemEval-2017 Task 1: Leverage kernelbased traditional nlp features and neural networks to build a universal model for multilingual and cross-lingual semantic textual similarity. Junfeng Tian, Zhiheng Zhou, Man Lan, Yuanbin Wu, Proceedings of SemEval-2017. SemEval-2017Junfeng Tian, Zhiheng Zhou, Man Lan, and Yuanbin Wu. 2017. ECNU at SemEval-2017 Task 1: Leverage kernel- based traditional nlp features and neural networks to build a universal model for multilingual and cross-lingual seman- tic textual similarity. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2028.\n\nTakelab: Systems for measuring semantic text similarity. Goran Frane\u0161ari\u0107, Mladen Glava\u0161, Ja\u0148 Karan, Bojana Dalbelo Snajder, Ba\u0161i\u0107, Proceedings of *SEM 2012/SemEval. *SEM 2012/SemEvalFrane\u0160ari\u0107, Goran Glava\u0161, Mladen Karan, Ja\u0148 Snajder, and Bojana Dalbelo Ba\u0161i\u0107. 2012a. Take- lab: Systems for measuring semantic text similar- ity. In Proceedings of *SEM 2012/SemEval 2012. http://www.aclweb.org/anthology/S12-1060.\n\nTakeLab: Systems for measuring semantic text similarity. Goran Frane\u0161ari\u0107, Mladen Glava\u0161, Jan\u0161najder Karan, Bojana Dalbelo Ba\u0161i\u0107, Proceedings of SemEval 2012. SemEval 2012Frane\u0160ari\u0107, Goran Glava\u0161, Mladen Karan, Jan\u0160najder, and Bojana Dalbelo Ba\u0161i\u0107. 2012b. TakeLab: Systems for mea- suring semantic text similarity. In Proceedings of SemEval 2012. http://www.aclweb.org/anthology/S12-1060.\n\nFrom paraphrase database to compositional paraphrase model and back. John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, Transactions of the ACL (TACL). 3John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. From paraphrase database to compositional paraphrase model and back. Transactions of the ACL (TACL) 3:345-358. http://aclweb.org/anthology/Q/Q15/Q15-1025.pdf.\n\nCharagram: Embedding words and sentences via character n-grams. John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, Proceedings of EMNLP. EMNLPJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016a. Charagram: Embedding words and sen- tences via character n-grams. In Proceedings of EMNLP. https://aclweb.org/anthology/D16-1157.\n\nTowards universal paraphrastic sentence embeddings. John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, Proceedings of ICLR. ICLRJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016b. Towards universal paraphrastic sen- tence embeddings. In Proceedings of ICLR 2016.\n\nRevisiting recurrent networks for paraphrastic sentence embeddings. John Wieting, Kevin Gimpel, CoRR abs/1705.00364John Wieting and Kevin Gimpel. 2017. Revisiting recurrent networks for paraphrastic sentence embeddings. CoRR abs/1705.00364. http://arxiv.org/abs/1705.00364.\n\nBIT at SemEval-2017 Task 1: Using semantic information space to evaluate semantic textual similarity. Hao Wu, Heyan Huang, Ping Jian, Yuhang Guo, Chao Su, Proceedings of SemEval-2017. SemEval-2017Hao Wu, Heyan Huang, Ping Jian, Yuhang Guo, and Chao Su. 2017. BIT at SemEval-2017 Task 1: Us- ing semantic information space to evaluate semantic textual similarity. In Proceedings of SemEval-2017. http://www.aclweb.org/anthology/S17-2007.\n\nSemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). Wei Xu, Chris Callison-Burch, William B Dolan, Proceedings of SemEval. SemEvalWei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similar- ity in Twitter (PIT). In Proceedings of SemEval 2015. http://www.aclweb.org/anthology/S15-2001.\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, 2Peter Young, Alice Lai, Micah Hodosh, and Julia Hock- enmaier. 2014. From image descriptions to vi- sual denotations: New similarity metrics for seman- tic inference over event descriptions. TACL 2:67-78. http://aclweb.org/anthology/Q14-1006.\n\nNeobility at SemEval-2017 Task 1: An attention-based sentence similarity model. Wenli Zhuang, Ernie Chang, Proceedings SemEval-2017. SemEval-2017WenLi Zhuang and Ernie Chang. 2017. Neobility at SemEval-2017 Task 1: An attention-based sen- tence similarity model. In Proceedings SemEval-2017. http://www.aclweb.org/anthology/S17-2023.\n", "annotations": {"author": "[{\"end\":147,\"start\":102},{\"end\":202,\"start\":148},{\"end\":313,\"start\":203},{\"end\":430,\"start\":314},{\"end\":444,\"start\":431}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":109},{\"end\":157,\"start\":153},{\"end\":215,\"start\":209},{\"end\":332,\"start\":320},{\"end\":443,\"start\":437}]", "author_first_name": "[{\"end\":108,\"start\":102},{\"end\":152,\"start\":148},{\"end\":208,\"start\":203},{\"end\":319,\"start\":314},{\"end\":436,\"start\":431}]", "author_affiliation": "[{\"end\":146,\"start\":114},{\"end\":201,\"start\":159},{\"end\":312,\"start\":217},{\"end\":429,\"start\":334}]", "title": "[{\"end\":99,\"start\":1},{\"end\":543,\"start\":445}]", "venue": null, "abstract": "[{\"end\":1502,\"start\":545}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2206,\"start\":2181},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2226,\"start\":2206},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2245,\"start\":2226},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2293,\"start\":2268},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":2336,\"start\":2319},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2362,\"start\":2336},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2381,\"start\":2362},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3045,\"start\":3025},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3067,\"start\":3045},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3089,\"start\":3067},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3111,\"start\":3089},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3133,\"start\":3111},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3331,\"start\":3313},{\"end\":3351,\"start\":3331},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3373,\"start\":3351},{\"end\":3473,\"start\":3467},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":3721,\"start\":3697},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4533,\"start\":4514},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4556,\"start\":4535},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":4574,\"start\":4558},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":4601,\"start\":4576},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":4628,\"start\":4603},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4650,\"start\":4632},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4672,\"start\":4652},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4696,\"start\":4690},{\"end\":4722,\"start\":4698},{\"end\":4744,\"start\":4738},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5682,\"start\":5662},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5912,\"start\":5891},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7126,\"start\":7106},{\"end\":7229,\"start\":7206},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":7311,\"start\":7291},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7601,\"start\":7579},{\"end\":8151,\"start\":8127},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9566,\"start\":9545},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10328,\"start\":10308},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12022,\"start\":12002},{\"end\":14054,\"start\":14052},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":15552,\"start\":15531},{\"end\":15880,\"start\":15878},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16452,\"start\":16429},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16490,\"start\":16467},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16528,\"start\":16505},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16562,\"start\":16541},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16597,\"start\":16579},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16632,\"start\":16614},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16667,\"start\":16649},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16707,\"start\":16687},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17064,\"start\":17035},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":17091,\"start\":17072},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":17456,\"start\":17439},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":17495,\"start\":17474},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17872,\"start\":17852},{\"end\":18176,\"start\":18174},{\"end\":18409,\"start\":18403},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":18695,\"start\":18683},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18834,\"start\":18815},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":18853,\"start\":18834},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19204,\"start\":19180},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19319,\"start\":19295},{\"end\":19531,\"start\":19510},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19798,\"start\":19779},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19828,\"start\":19807},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":19938,\"start\":19910},{\"end\":20242,\"start\":20236},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20342,\"start\":20319},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":20530,\"start\":20514},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20578,\"start\":20557},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20683,\"start\":20656},{\"end\":20722,\"start\":20716},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":20755,\"start\":20733},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":21224,\"start\":21201},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21374,\"start\":21354},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":21401,\"start\":21382},{\"end\":21434,\"start\":21420},{\"end\":22296,\"start\":22294},{\"end\":22643,\"start\":22641},{\"end\":24142,\"start\":24140},{\"end\":24954,\"start\":24952},{\"end\":25848,\"start\":25846},{\"end\":26287,\"start\":26285},{\"end\":26768,\"start\":26766},{\"end\":26943,\"start\":26938},{\"end\":27057,\"start\":27034},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27141,\"start\":27120},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":27230,\"start\":27208},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":27312,\"start\":27287},{\"end\":27408,\"start\":27383},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27494,\"start\":27488},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":29011,\"start\":29000},{\"end\":31432,\"start\":31426},{\"end\":31438,\"start\":31432},{\"end\":31444,\"start\":31438},{\"end\":31450,\"start\":31444},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31456,\"start\":31450},{\"end\":31462,\"start\":31456},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":38278,\"start\":38258}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31831,\"start\":31715},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31895,\"start\":31832},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31969,\"start\":31896},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32216,\"start\":31970},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32251,\"start\":32217},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32441,\"start\":32252},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32858,\"start\":32442},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":33017,\"start\":32859},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":33059,\"start\":33018},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":33829,\"start\":33060},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":33877,\"start\":33830},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":34819,\"start\":33878},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":35280,\"start\":34820},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":35348,\"start\":35281},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":37442,\"start\":35349},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":37578,\"start\":37443}]", "paragraph": "[{\"end\":2846,\"start\":1518},{\"end\":3722,\"start\":2848},{\"end\":4381,\"start\":3724},{\"end\":4933,\"start\":4383},{\"end\":5817,\"start\":4951},{\"end\":6156,\"start\":5837},{\"end\":6313,\"start\":6162},{\"end\":6476,\"start\":6319},{\"end\":6669,\"start\":6482},{\"end\":6807,\"start\":6675},{\"end\":6957,\"start\":6813},{\"end\":7127,\"start\":6963},{\"end\":7715,\"start\":7129},{\"end\":8443,\"start\":7717},{\"end\":8746,\"start\":8464},{\"end\":9329,\"start\":8790},{\"end\":9783,\"start\":9364},{\"end\":9843,\"start\":9840},{\"end\":9995,\"start\":9858},{\"end\":10572,\"start\":10024},{\"end\":11193,\"start\":10594},{\"end\":12234,\"start\":11211},{\"end\":12934,\"start\":12276},{\"end\":13045,\"start\":12956},{\"end\":13452,\"start\":13063},{\"end\":13673,\"start\":13474},{\"end\":13781,\"start\":13685},{\"end\":14233,\"start\":13794},{\"end\":14920,\"start\":14246},{\"end\":17065,\"start\":14932},{\"end\":18676,\"start\":17067},{\"end\":19287,\"start\":18678},{\"end\":19799,\"start\":19289},{\"end\":20343,\"start\":19801},{\"end\":21181,\"start\":20345},{\"end\":21435,\"start\":21193},{\"end\":23927,\"start\":21448},{\"end\":25561,\"start\":23988},{\"end\":26994,\"start\":25579},{\"end\":27076,\"start\":26996},{\"end\":27331,\"start\":27078},{\"end\":27930,\"start\":27333},{\"end\":28104,\"start\":27932},{\"end\":29315,\"start\":28106},{\"end\":29639,\"start\":29317},{\"end\":31714,\"start\":29654}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9839,\"start\":9784}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5242,\"start\":5235},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11279,\"start\":11272},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":11299,\"start\":11292},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":11324,\"start\":11317},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":11344,\"start\":11337},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":11447,\"start\":11440},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":13206,\"start\":13199},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16716,\"start\":16708},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22153,\"start\":22145},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26202,\"start\":26194}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1516,\"start\":1504},{\"attributes\":{\"n\":\"2\"},\"end\":4949,\"start\":4936},{\"attributes\":{\"n\":\"3\"},\"end\":5835,\"start\":5820},{\"end\":6160,\"start\":6159},{\"end\":6317,\"start\":6316},{\"end\":6480,\"start\":6479},{\"end\":6673,\"start\":6672},{\"end\":6811,\"start\":6810},{\"end\":6961,\"start\":6960},{\"attributes\":{\"n\":\"3.2\"},\"end\":8462,\"start\":8446},{\"attributes\":{\"n\":\"3.3\"},\"end\":8788,\"start\":8749},{\"attributes\":{\"n\":\"3.4\"},\"end\":9362,\"start\":9332},{\"attributes\":{\"n\":\"4\"},\"end\":9856,\"start\":9846},{\"attributes\":{\"n\":\"4.1\"},\"end\":10022,\"start\":9998},{\"attributes\":{\"n\":\"4.2\"},\"end\":10592,\"start\":10575},{\"attributes\":{\"n\":\"5\"},\"end\":11209,\"start\":11196},{\"attributes\":{\"n\":\"5.1\"},\"end\":12274,\"start\":12237},{\"attributes\":{\"n\":\"6\"},\"end\":12954,\"start\":12937},{\"attributes\":{\"n\":\"6.1\"},\"end\":13061,\"start\":13048},{\"attributes\":{\"n\":\"6.2\"},\"end\":13472,\"start\":13455},{\"attributes\":{\"n\":\"6.3\"},\"end\":13683,\"start\":13676},{\"attributes\":{\"n\":\"6.4\"},\"end\":13792,\"start\":13784},{\"attributes\":{\"n\":\"6.5\"},\"end\":14244,\"start\":14236},{\"attributes\":{\"n\":\"6.6\"},\"end\":14930,\"start\":14923},{\"end\":21191,\"start\":21184},{\"attributes\":{\"n\":\"7\"},\"end\":21446,\"start\":21438},{\"attributes\":{\"n\":\"7.1\"},\"end\":23986,\"start\":23930},{\"attributes\":{\"n\":\"8\"},\"end\":25577,\"start\":25564},{\"attributes\":{\"n\":\"9\"},\"end\":29652,\"start\":29642},{\"end\":31843,\"start\":31833},{\"end\":31906,\"start\":31897},{\"end\":31980,\"start\":31971},{\"end\":32227,\"start\":32218},{\"end\":32262,\"start\":32253},{\"end\":32452,\"start\":32443},{\"end\":32869,\"start\":32860},{\"end\":33028,\"start\":33019},{\"end\":33071,\"start\":33061},{\"end\":33840,\"start\":33831},{\"end\":33889,\"start\":33879},{\"end\":34831,\"start\":34821},{\"end\":35292,\"start\":35282},{\"end\":37454,\"start\":37444}]", "table": "[{\"end\":32216,\"start\":32007},{\"end\":32441,\"start\":32294},{\"end\":32858,\"start\":32475},{\"end\":33017,\"start\":32900},{\"end\":33829,\"start\":33267},{\"end\":34819,\"start\":33892},{\"end\":35280,\"start\":34926},{\"end\":37442,\"start\":37019}]", "figure_caption": "[{\"end\":31831,\"start\":31717},{\"end\":31895,\"start\":31845},{\"end\":31969,\"start\":31908},{\"end\":32007,\"start\":31982},{\"end\":32251,\"start\":32229},{\"end\":32294,\"start\":32264},{\"end\":32475,\"start\":32454},{\"end\":32900,\"start\":32871},{\"end\":33059,\"start\":33030},{\"end\":33267,\"start\":33074},{\"end\":33877,\"start\":33842},{\"end\":34926,\"start\":34834},{\"end\":35348,\"start\":35295},{\"end\":37019,\"start\":35351},{\"end\":37578,\"start\":37457}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":39891,\"start\":39888},{\"end\":40019,\"start\":40013},{\"end\":40051,\"start\":40045},{\"end\":40065,\"start\":40059},{\"end\":40078,\"start\":40074},{\"end\":40089,\"start\":40084},{\"end\":40102,\"start\":40096},{\"end\":40125,\"start\":40120},{\"end\":40137,\"start\":40131},{\"end\":40156,\"start\":40152},{\"end\":40175,\"start\":40169},{\"end\":40194,\"start\":40186},{\"end\":40208,\"start\":40202},{\"end\":40719,\"start\":40714},{\"end\":40734,\"start\":40728},{\"end\":40748,\"start\":40742},{\"end\":40763,\"start\":40757},{\"end\":40773,\"start\":40769},{\"end\":40785,\"start\":40780},{\"end\":40809,\"start\":40803},{\"end\":40819,\"start\":40815},{\"end\":40836,\"start\":40830},{\"end\":40850,\"start\":40844},{\"end\":41239,\"start\":41234},{\"end\":41254,\"start\":41248},{\"end\":41268,\"start\":41262},{\"end\":41278,\"start\":41274},{\"end\":41290,\"start\":41285},{\"end\":41312,\"start\":41308},{\"end\":41329,\"start\":41323},{\"end\":41343,\"start\":41337},{\"end\":41724,\"start\":41719},{\"end\":41739,\"start\":41733},{\"end\":41749,\"start\":41745},{\"end\":41761,\"start\":41756},{\"end\":42104,\"start\":42099},{\"end\":42119,\"start\":42113},{\"end\":42129,\"start\":42125},{\"end\":42141,\"start\":42136},{\"end\":42165,\"start\":42159},{\"end\":42555,\"start\":42554},{\"end\":42570,\"start\":42565},{\"end\":42590,\"start\":42583},{\"end\":42607,\"start\":42601},{\"end\":42626,\"start\":42620},{\"end\":43062,\"start\":43055},{\"end\":43076,\"start\":43070},{\"end\":43090,\"start\":43084},{\"end\":43429,\"start\":43422},{\"end\":43439,\"start\":43438},{\"end\":43469,\"start\":43451},{\"end\":43802,\"start\":43796},{\"end\":43804,\"start\":43803},{\"end\":43819,\"start\":43812},{\"end\":43821,\"start\":43820},{\"end\":43836,\"start\":43832},{\"end\":43838,\"start\":43837},{\"end\":44155,\"start\":44149},{\"end\":44166,\"start\":44161},{\"end\":44181,\"start\":44176},{\"end\":44199,\"start\":44192},{\"end\":44612,\"start\":44609},{\"end\":44626,\"start\":44621},{\"end\":45081,\"start\":45076},{\"end\":45103,\"start\":45094},{\"end\":45119,\"start\":45114},{\"end\":45136,\"start\":45129},{\"end\":45150,\"start\":45145},{\"end\":45166,\"start\":45159},{\"end\":45693,\"start\":45684},{\"end\":45712,\"start\":45702},{\"end\":45728,\"start\":45721},{\"end\":45746,\"start\":45739},{\"end\":46133,\"start\":46128},{\"end\":46413,\"start\":46408},{\"end\":46764,\"start\":46756},{\"end\":46786,\"start\":46773},{\"end\":47117,\"start\":47111},{\"end\":47134,\"start\":47125},{\"end\":47150,\"start\":47141},{\"end\":47167,\"start\":47162},{\"end\":47183,\"start\":47176},{\"end\":47199,\"start\":47191},{\"end\":47218,\"start\":47210},{\"end\":47230,\"start\":47225},{\"end\":47243,\"start\":47239},{\"end\":47255,\"start\":47250},{\"end\":47273,\"start\":47269},{\"end\":47288,\"start\":47283},{\"end\":47301,\"start\":47297},{\"end\":47712,\"start\":47706},{\"end\":47729,\"start\":47720},{\"end\":47741,\"start\":47736},{\"end\":47767,\"start\":47758},{\"end\":47784,\"start\":47779},{\"end\":47800,\"start\":47793},{\"end\":47816,\"start\":47808},{\"end\":47827,\"start\":47823},{\"end\":47838,\"start\":47834},{\"end\":47853,\"start\":47848},{\"end\":48323,\"start\":48322},{\"end\":48337,\"start\":48332},{\"end\":48357,\"start\":48346},{\"end\":48377,\"start\":48366},{\"end\":48379,\"start\":48378},{\"end\":48777,\"start\":48772},{\"end\":48793,\"start\":48788},{\"end\":49104,\"start\":49100},{\"end\":49118,\"start\":49111},{\"end\":49132,\"start\":49124},{\"end\":49141,\"start\":49139},{\"end\":49150,\"start\":49147},{\"end\":49514,\"start\":49506},{\"end\":49526,\"start\":49521},{\"end\":49548,\"start\":49540},{\"end\":50087,\"start\":50081},{\"end\":50102,\"start\":50097},{\"end\":50116,\"start\":50110},{\"end\":50494,\"start\":50490},{\"end\":50514,\"start\":50506},{\"end\":50525,\"start\":50522},{\"end\":50884,\"start\":50880},{\"end\":50904,\"start\":50898},{\"end\":51277,\"start\":51273},{\"end\":51290,\"start\":51285},{\"end\":51303,\"start\":51298},{\"end\":51700,\"start\":51692},{\"end\":51717,\"start\":51709},{\"end\":52089,\"start\":52081},{\"end\":52096,\"start\":52090},{\"end\":52111,\"start\":52104},{\"end\":52422,\"start\":52412},{\"end\":52638,\"start\":52632},{\"end\":52655,\"start\":52648},{\"end\":52672,\"start\":52666},{\"end\":52689,\"start\":52681},{\"end\":53111,\"start\":53106},{\"end\":53124,\"start\":53120},{\"end\":53132,\"start\":53125},{\"end\":53149,\"start\":53144},{\"end\":53163,\"start\":53158},{\"end\":53509,\"start\":53505},{\"end\":53532,\"start\":53524},{\"end\":53549,\"start\":53544},{\"end\":53844,\"start\":53839},{\"end\":53858,\"start\":53853},{\"end\":53876,\"start\":53872},{\"end\":53892,\"start\":53885},{\"end\":54273,\"start\":54270},{\"end\":54283,\"start\":54278},{\"end\":54297,\"start\":54292},{\"end\":54641,\"start\":54638},{\"end\":54651,\"start\":54646},{\"end\":55031,\"start\":55028},{\"end\":55040,\"start\":55036},{\"end\":55055,\"start\":55050},{\"end\":55071,\"start\":55064},{\"end\":55082,\"start\":55077},{\"end\":55475,\"start\":55471},{\"end\":55496,\"start\":55487},{\"end\":55513,\"start\":55508},{\"end\":55531,\"start\":55526},{\"end\":55875,\"start\":55870},{\"end\":55891,\"start\":55882},{\"end\":55901,\"start\":55897},{\"end\":56171,\"start\":56167},{\"end\":56190,\"start\":56184},{\"end\":56430,\"start\":56424},{\"end\":56445,\"start\":56437},{\"end\":56460,\"start\":56454},{\"end\":56474,\"start\":56469},{\"end\":56489,\"start\":56484},{\"end\":56824,\"start\":56818},{\"end\":56840,\"start\":56832},{\"end\":56853,\"start\":56845},{\"end\":56861,\"start\":56859},{\"end\":56872,\"start\":56868},{\"end\":56885,\"start\":56880},{\"end\":57327,\"start\":57322},{\"end\":57340,\"start\":57335},{\"end\":57359,\"start\":57353},{\"end\":57376,\"start\":57373},{\"end\":57387,\"start\":57384},{\"end\":57735,\"start\":57729},{\"end\":57752,\"start\":57745},{\"end\":57771,\"start\":57762},{\"end\":58152,\"start\":58146},{\"end\":58169,\"start\":58162},{\"end\":58188,\"start\":58179},{\"end\":58550,\"start\":58544},{\"end\":58566,\"start\":58559},{\"end\":58579,\"start\":58574},{\"end\":58597,\"start\":58592},{\"end\":58846,\"start\":58843},{\"end\":58861,\"start\":58855},{\"end\":58878,\"start\":58871},{\"end\":59143,\"start\":59139},{\"end\":59156,\"start\":59151},{\"end\":59168,\"start\":59162},{\"end\":59191,\"start\":59184},{\"end\":59193,\"start\":59192},{\"end\":59208,\"start\":59201},{\"end\":59579,\"start\":59574},{\"end\":59591,\"start\":59588},{\"end\":59598,\"start\":59592},{\"end\":59612,\"start\":59607},{\"end\":60008,\"start\":60005},{\"end\":60021,\"start\":60014},{\"end\":60463,\"start\":60462},{\"end\":60475,\"start\":60470},{\"end\":60769,\"start\":60765},{\"end\":60780,\"start\":60775},{\"end\":60796,\"start\":60791},{\"end\":60803,\"start\":60801},{\"end\":60816,\"start\":60809},{\"end\":60822,\"start\":60817},{\"end\":60836,\"start\":60832},{\"end\":60849,\"start\":60844},{\"end\":60856,\"start\":60850},{\"end\":60869,\"start\":60866},{\"end\":61344,\"start\":61338},{\"end\":61358,\"start\":61350},{\"end\":61367,\"start\":61364},{\"end\":61381,\"start\":61373},{\"end\":61802,\"start\":61797},{\"end\":61821,\"start\":61813},{\"end\":61837,\"start\":61831},{\"end\":61852,\"start\":61846},{\"end\":61854,\"start\":61853},{\"end\":61869,\"start\":61863},{\"end\":62284,\"start\":62273},{\"end\":62286,\"start\":62285},{\"end\":62301,\"start\":62296},{\"end\":62316,\"start\":62312},{\"end\":62329,\"start\":62324},{\"end\":62344,\"start\":62338},{\"end\":62346,\"start\":62345},{\"end\":62361,\"start\":62356},{\"end\":62783,\"start\":62778},{\"end\":62800,\"start\":62793},{\"end\":62814,\"start\":62809},{\"end\":62828,\"start\":62823},{\"end\":62850,\"start\":62841},{\"end\":62868,\"start\":62861},{\"end\":63302,\"start\":63276},{\"end\":63687,\"start\":63680},{\"end\":63701,\"start\":63694},{\"end\":63712,\"start\":63706},{\"end\":63727,\"start\":63720},{\"end\":63741,\"start\":63735},{\"end\":63753,\"start\":63746},{\"end\":64115,\"start\":64110},{\"end\":64128,\"start\":64125},{\"end\":64139,\"start\":64135},{\"end\":64156,\"start\":64149},{\"end\":64412,\"start\":64407},{\"end\":64426,\"start\":64422},{\"end\":64441,\"start\":64438},{\"end\":64452,\"start\":64448},{\"end\":64454,\"start\":64453},{\"end\":64468,\"start\":64464},{\"end\":64856,\"start\":64855},{\"end\":65130,\"start\":65120},{\"end\":65418,\"start\":65413},{\"end\":65427,\"start\":65423},{\"end\":65440,\"start\":65434},{\"end\":65739,\"start\":65733},{\"end\":65757,\"start\":65751},{\"end\":65773,\"start\":65767},{\"end\":66156,\"start\":66149},{\"end\":66172,\"start\":66166},{\"end\":66178,\"start\":66173},{\"end\":66453,\"start\":66447},{\"end\":66474,\"start\":66467},{\"end\":66488,\"start\":66482},{\"end\":66690,\"start\":66684},{\"end\":66704,\"start\":66699},{\"end\":66717,\"start\":66712},{\"end\":66726,\"start\":66724},{\"end\":66740,\"start\":66733},{\"end\":67005,\"start\":66998},{\"end\":67025,\"start\":67018},{\"end\":67045,\"start\":67034},{\"end\":67047,\"start\":67046},{\"end\":67336,\"start\":67330},{\"end\":67361,\"start\":67353},{\"end\":67379,\"start\":67374},{\"end\":67759,\"start\":67755},{\"end\":67775,\"start\":67769},{\"end\":67788,\"start\":67783},{\"end\":67992,\"start\":67985},{\"end\":68013,\"start\":68004},{\"end\":68032,\"start\":68024},{\"end\":68054,\"start\":68046},{\"end\":68069,\"start\":68064},{\"end\":68768,\"start\":68759},{\"end\":68781,\"start\":68776},{\"end\":68795,\"start\":68790},{\"end\":69163,\"start\":69154},{\"end\":69176,\"start\":69171},{\"end\":69190,\"start\":69185},{\"end\":69535,\"start\":69529},{\"end\":69957,\"start\":69953},{\"end\":70296,\"start\":70290},{\"end\":70311,\"start\":70303},{\"end\":70324,\"start\":70316},{\"end\":70332,\"start\":70330},{\"end\":70347,\"start\":70339},{\"end\":70656,\"start\":70649},{\"end\":70671,\"start\":70665},{\"end\":70685,\"start\":70678},{\"end\":70702,\"start\":70696},{\"end\":70718,\"start\":70714},{\"end\":71100,\"start\":71095},{\"end\":71123,\"start\":71117},{\"end\":71508,\"start\":71502},{\"end\":71533,\"start\":71527},{\"end\":71996,\"start\":71989},{\"end\":72010,\"start\":72003},{\"end\":72020,\"start\":72017},{\"end\":72033,\"start\":72026},{\"end\":72465,\"start\":72460},{\"end\":72484,\"start\":72478},{\"end\":72496,\"start\":72493},{\"end\":72518,\"start\":72504},{\"end\":72880,\"start\":72875},{\"end\":72899,\"start\":72893},{\"end\":72918,\"start\":72908},{\"end\":72940,\"start\":72926},{\"end\":73281,\"start\":73277},{\"end\":73296,\"start\":73291},{\"end\":73310,\"start\":73305},{\"end\":73324,\"start\":73319},{\"end\":73662,\"start\":73658},{\"end\":73677,\"start\":73672},{\"end\":73691,\"start\":73686},{\"end\":73705,\"start\":73700},{\"end\":73997,\"start\":73993},{\"end\":74012,\"start\":74007},{\"end\":74026,\"start\":74021},{\"end\":74040,\"start\":74035},{\"end\":74299,\"start\":74295},{\"end\":74314,\"start\":74309},{\"end\":74607,\"start\":74604},{\"end\":74617,\"start\":74612},{\"end\":74629,\"start\":74625},{\"end\":74642,\"start\":74636},{\"end\":74652,\"start\":74648},{\"end\":75017,\"start\":75014},{\"end\":75027,\"start\":75022},{\"end\":75051,\"start\":75044},{\"end\":75053,\"start\":75052},{\"end\":75424,\"start\":75419},{\"end\":75437,\"start\":75432},{\"end\":75448,\"start\":75443},{\"end\":75462,\"start\":75457},{\"end\":75806,\"start\":75801},{\"end\":75820,\"start\":75815}]", "bib_author_last_name": "[{\"end\":39897,\"start\":39892},{\"end\":40043,\"start\":40020},{\"end\":40057,\"start\":40052},{\"end\":40072,\"start\":40066},{\"end\":40082,\"start\":40079},{\"end\":40094,\"start\":40090},{\"end\":40118,\"start\":40103},{\"end\":40129,\"start\":40126},{\"end\":40150,\"start\":40138},{\"end\":40167,\"start\":40157},{\"end\":40184,\"start\":40176},{\"end\":40200,\"start\":40195},{\"end\":40213,\"start\":40209},{\"end\":40220,\"start\":40215},{\"end\":40726,\"start\":40720},{\"end\":40740,\"start\":40735},{\"end\":40755,\"start\":40749},{\"end\":40767,\"start\":40764},{\"end\":40778,\"start\":40774},{\"end\":40801,\"start\":40786},{\"end\":40813,\"start\":40810},{\"end\":40828,\"start\":40820},{\"end\":40842,\"start\":40837},{\"end\":40856,\"start\":40851},{\"end\":41246,\"start\":41240},{\"end\":41260,\"start\":41255},{\"end\":41272,\"start\":41269},{\"end\":41283,\"start\":41279},{\"end\":41306,\"start\":41291},{\"end\":41321,\"start\":41313},{\"end\":41335,\"start\":41330},{\"end\":41349,\"start\":41344},{\"end\":41731,\"start\":41725},{\"end\":41743,\"start\":41740},{\"end\":41754,\"start\":41750},{\"end\":41777,\"start\":41762},{\"end\":42111,\"start\":42105},{\"end\":42123,\"start\":42120},{\"end\":42134,\"start\":42130},{\"end\":42157,\"start\":42142},{\"end\":42169,\"start\":42166},{\"end\":42563,\"start\":42556},{\"end\":42581,\"start\":42571},{\"end\":42599,\"start\":42591},{\"end\":42618,\"start\":42608},{\"end\":42637,\"start\":42627},{\"end\":43068,\"start\":43063},{\"end\":43082,\"start\":43077},{\"end\":43093,\"start\":43091},{\"end\":43436,\"start\":43430},{\"end\":43449,\"start\":43440},{\"end\":43474,\"start\":43470},{\"end\":43810,\"start\":43805},{\"end\":43830,\"start\":43822},{\"end\":43843,\"start\":43839},{\"end\":44159,\"start\":44156},{\"end\":44174,\"start\":44167},{\"end\":44190,\"start\":44182},{\"end\":44205,\"start\":44200},{\"end\":44619,\"start\":44613},{\"end\":44633,\"start\":44627},{\"end\":45092,\"start\":45082},{\"end\":45112,\"start\":45104},{\"end\":45127,\"start\":45120},{\"end\":45143,\"start\":45137},{\"end\":45157,\"start\":45151},{\"end\":45177,\"start\":45167},{\"end\":45700,\"start\":45694},{\"end\":45719,\"start\":45713},{\"end\":45737,\"start\":45729},{\"end\":45755,\"start\":45747},{\"end\":46140,\"start\":46134},{\"end\":46420,\"start\":46414},{\"end\":46771,\"start\":46765},{\"end\":47123,\"start\":47118},{\"end\":47139,\"start\":47135},{\"end\":47160,\"start\":47151},{\"end\":47174,\"start\":47168},{\"end\":47189,\"start\":47184},{\"end\":47208,\"start\":47200},{\"end\":47223,\"start\":47219},{\"end\":47237,\"start\":47231},{\"end\":47248,\"start\":47244},{\"end\":47267,\"start\":47256},{\"end\":47281,\"start\":47274},{\"end\":47295,\"start\":47289},{\"end\":47310,\"start\":47302},{\"end\":47718,\"start\":47713},{\"end\":47734,\"start\":47730},{\"end\":47756,\"start\":47742},{\"end\":47777,\"start\":47768},{\"end\":47791,\"start\":47785},{\"end\":47806,\"start\":47801},{\"end\":47821,\"start\":47817},{\"end\":47832,\"start\":47828},{\"end\":47846,\"start\":47839},{\"end\":47860,\"start\":47854},{\"end\":48330,\"start\":48324},{\"end\":48344,\"start\":48338},{\"end\":48364,\"start\":48358},{\"end\":48385,\"start\":48380},{\"end\":48394,\"start\":48387},{\"end\":48786,\"start\":48778},{\"end\":48801,\"start\":48794},{\"end\":49109,\"start\":49105},{\"end\":49122,\"start\":49119},{\"end\":49137,\"start\":49133},{\"end\":49145,\"start\":49142},{\"end\":49156,\"start\":49151},{\"end\":49519,\"start\":49515},{\"end\":49538,\"start\":49527},{\"end\":49551,\"start\":49549},{\"end\":50095,\"start\":50088},{\"end\":50108,\"start\":50103},{\"end\":50124,\"start\":50117},{\"end\":50504,\"start\":50495},{\"end\":50520,\"start\":50515},{\"end\":50533,\"start\":50526},{\"end\":50539,\"start\":50535},{\"end\":50896,\"start\":50885},{\"end\":50910,\"start\":50905},{\"end\":51283,\"start\":51278},{\"end\":51296,\"start\":51291},{\"end\":51312,\"start\":51304},{\"end\":51707,\"start\":51701},{\"end\":51722,\"start\":51718},{\"end\":51730,\"start\":51724},{\"end\":52102,\"start\":52097},{\"end\":52125,\"start\":52112},{\"end\":52431,\"start\":52423},{\"end\":52646,\"start\":52639},{\"end\":52664,\"start\":52656},{\"end\":52679,\"start\":52673},{\"end\":52695,\"start\":52690},{\"end\":53118,\"start\":53112},{\"end\":53142,\"start\":53133},{\"end\":53156,\"start\":53150},{\"end\":53172,\"start\":53164},{\"end\":53522,\"start\":53510},{\"end\":53542,\"start\":53533},{\"end\":53564,\"start\":53550},{\"end\":53851,\"start\":53845},{\"end\":53870,\"start\":53859},{\"end\":53883,\"start\":53877},{\"end\":53898,\"start\":53893},{\"end\":54276,\"start\":54274},{\"end\":54290,\"start\":54284},{\"end\":54301,\"start\":54298},{\"end\":54644,\"start\":54642},{\"end\":54655,\"start\":54652},{\"end\":55034,\"start\":55032},{\"end\":55048,\"start\":55041},{\"end\":55062,\"start\":55056},{\"end\":55075,\"start\":55072},{\"end\":55086,\"start\":55083},{\"end\":55485,\"start\":55476},{\"end\":55506,\"start\":55497},{\"end\":55524,\"start\":55514},{\"end\":55540,\"start\":55532},{\"end\":55880,\"start\":55876},{\"end\":55895,\"start\":55892},{\"end\":55910,\"start\":55902},{\"end\":56182,\"start\":56172},{\"end\":56202,\"start\":56191},{\"end\":56435,\"start\":56431},{\"end\":56452,\"start\":56446},{\"end\":56467,\"start\":56461},{\"end\":56482,\"start\":56475},{\"end\":56500,\"start\":56490},{\"end\":56830,\"start\":56825},{\"end\":56843,\"start\":56841},{\"end\":56857,\"start\":56854},{\"end\":56866,\"start\":56862},{\"end\":56878,\"start\":56873},{\"end\":56890,\"start\":56886},{\"end\":57333,\"start\":57328},{\"end\":57351,\"start\":57341},{\"end\":57371,\"start\":57360},{\"end\":57382,\"start\":57377},{\"end\":57743,\"start\":57736},{\"end\":57760,\"start\":57753},{\"end\":57779,\"start\":57772},{\"end\":58160,\"start\":58153},{\"end\":58177,\"start\":58170},{\"end\":58196,\"start\":58189},{\"end\":58557,\"start\":58551},{\"end\":58572,\"start\":58567},{\"end\":58590,\"start\":58580},{\"end\":58605,\"start\":58598},{\"end\":58853,\"start\":58847},{\"end\":58869,\"start\":58862},{\"end\":58887,\"start\":58879},{\"end\":59149,\"start\":59144},{\"end\":59160,\"start\":59157},{\"end\":59182,\"start\":59169},{\"end\":59199,\"start\":59194},{\"end\":59217,\"start\":59209},{\"end\":59586,\"start\":59580},{\"end\":59605,\"start\":59599},{\"end\":59620,\"start\":59613},{\"end\":60012,\"start\":60009},{\"end\":60025,\"start\":60022},{\"end\":60034,\"start\":60027},{\"end\":60468,\"start\":60464},{\"end\":60478,\"start\":60476},{\"end\":60487,\"start\":60480},{\"end\":60773,\"start\":60770},{\"end\":60789,\"start\":60781},{\"end\":60799,\"start\":60797},{\"end\":60807,\"start\":60804},{\"end\":60830,\"start\":60823},{\"end\":60842,\"start\":60837},{\"end\":60864,\"start\":60857},{\"end\":60880,\"start\":60870},{\"end\":61348,\"start\":61345},{\"end\":61362,\"start\":61359},{\"end\":61371,\"start\":61368},{\"end\":61385,\"start\":61382},{\"end\":61811,\"start\":61803},{\"end\":61829,\"start\":61822},{\"end\":61844,\"start\":61838},{\"end\":61861,\"start\":61855},{\"end\":61873,\"start\":61870},{\"end\":62294,\"start\":62287},{\"end\":62310,\"start\":62302},{\"end\":62322,\"start\":62317},{\"end\":62336,\"start\":62330},{\"end\":62354,\"start\":62347},{\"end\":62370,\"start\":62362},{\"end\":62791,\"start\":62784},{\"end\":62807,\"start\":62801},{\"end\":62821,\"start\":62815},{\"end\":62839,\"start\":62829},{\"end\":62859,\"start\":62851},{\"end\":62879,\"start\":62869},{\"end\":63307,\"start\":63303},{\"end\":63692,\"start\":63688},{\"end\":63704,\"start\":63702},{\"end\":63718,\"start\":63713},{\"end\":63733,\"start\":63728},{\"end\":63744,\"start\":63742},{\"end\":63757,\"start\":63754},{\"end\":64123,\"start\":64116},{\"end\":64133,\"start\":64129},{\"end\":64147,\"start\":64140},{\"end\":64161,\"start\":64157},{\"end\":64420,\"start\":64413},{\"end\":64436,\"start\":64427},{\"end\":64446,\"start\":64442},{\"end\":64462,\"start\":64455},{\"end\":64473,\"start\":64469},{\"end\":64863,\"start\":64857},{\"end\":64871,\"start\":64865},{\"end\":65140,\"start\":65131},{\"end\":65421,\"start\":65419},{\"end\":65432,\"start\":65428},{\"end\":65450,\"start\":65441},{\"end\":65749,\"start\":65740},{\"end\":65765,\"start\":65758},{\"end\":65781,\"start\":65774},{\"end\":65789,\"start\":65783},{\"end\":66164,\"start\":66157},{\"end\":66187,\"start\":66179},{\"end\":66465,\"start\":66454},{\"end\":66480,\"start\":66475},{\"end\":66494,\"start\":66489},{\"end\":66697,\"start\":66691},{\"end\":66710,\"start\":66705},{\"end\":66722,\"start\":66718},{\"end\":66731,\"start\":66727},{\"end\":66746,\"start\":66741},{\"end\":67016,\"start\":67006},{\"end\":67032,\"start\":67026},{\"end\":67055,\"start\":67048},{\"end\":67351,\"start\":67337},{\"end\":67372,\"start\":67362},{\"end\":67389,\"start\":67380},{\"end\":67397,\"start\":67391},{\"end\":67767,\"start\":67760},{\"end\":67781,\"start\":67776},{\"end\":67797,\"start\":67789},{\"end\":68002,\"start\":67993},{\"end\":68022,\"start\":68014},{\"end\":68044,\"start\":68033},{\"end\":68062,\"start\":68055},{\"end\":68084,\"start\":68070},{\"end\":68774,\"start\":68769},{\"end\":68788,\"start\":68782},{\"end\":68809,\"start\":68796},{\"end\":69169,\"start\":69164},{\"end\":69183,\"start\":69177},{\"end\":69204,\"start\":69191},{\"end\":69544,\"start\":69536},{\"end\":69962,\"start\":69958},{\"end\":70301,\"start\":70297},{\"end\":70314,\"start\":70312},{\"end\":70328,\"start\":70325},{\"end\":70337,\"start\":70333},{\"end\":70354,\"start\":70348},{\"end\":70663,\"start\":70657},{\"end\":70676,\"start\":70672},{\"end\":70694,\"start\":70686},{\"end\":70712,\"start\":70703},{\"end\":70726,\"start\":70719},{\"end\":71115,\"start\":71101},{\"end\":71131,\"start\":71124},{\"end\":71138,\"start\":71133},{\"end\":71525,\"start\":71509},{\"end\":71541,\"start\":71534},{\"end\":71549,\"start\":71543},{\"end\":72001,\"start\":71997},{\"end\":72015,\"start\":72011},{\"end\":72024,\"start\":72021},{\"end\":72036,\"start\":72034},{\"end\":72476,\"start\":72466},{\"end\":72491,\"start\":72485},{\"end\":72502,\"start\":72497},{\"end\":72526,\"start\":72519},{\"end\":72533,\"start\":72528},{\"end\":72891,\"start\":72881},{\"end\":72906,\"start\":72900},{\"end\":72924,\"start\":72919},{\"end\":72946,\"start\":72941},{\"end\":73289,\"start\":73282},{\"end\":73303,\"start\":73297},{\"end\":73317,\"start\":73311},{\"end\":73332,\"start\":73325},{\"end\":73670,\"start\":73663},{\"end\":73684,\"start\":73678},{\"end\":73698,\"start\":73692},{\"end\":73713,\"start\":73706},{\"end\":74005,\"start\":73998},{\"end\":74019,\"start\":74013},{\"end\":74033,\"start\":74027},{\"end\":74048,\"start\":74041},{\"end\":74307,\"start\":74300},{\"end\":74321,\"start\":74315},{\"end\":74610,\"start\":74608},{\"end\":74623,\"start\":74618},{\"end\":74634,\"start\":74630},{\"end\":74646,\"start\":74643},{\"end\":74655,\"start\":74653},{\"end\":75020,\"start\":75018},{\"end\":75042,\"start\":75028},{\"end\":75059,\"start\":75054},{\"end\":75430,\"start\":75425},{\"end\":75441,\"start\":75438},{\"end\":75455,\"start\":75449},{\"end\":75474,\"start\":75463},{\"end\":75813,\"start\":75807},{\"end\":75826,\"start\":75821}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":39240,\"start\":39174},{\"attributes\":{\"id\":\"b1\"},\"end\":39564,\"start\":39242},{\"attributes\":{\"id\":\"b2\"},\"end\":39764,\"start\":39566},{\"attributes\":{\"id\":\"b3\"},\"end\":39884,\"start\":39766},{\"attributes\":{\"id\":\"b4\"},\"end\":39913,\"start\":39886},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11879061},\"end\":40648,\"start\":39915},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":11650107},\"end\":41140,\"start\":40650},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":646594},\"end\":41656,\"start\":41142},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12549805},\"end\":42045,\"start\":41658},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10241043},\"end\":42412,\"start\":42047},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1222366},\"end\":42992,\"start\":42414},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":64908139},\"end\":43271,\"start\":42994},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":334369},\"end\":43763,\"start\":43273},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2505531},\"end\":44053,\"start\":43765},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6964767},\"end\":44507,\"start\":44055},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":19570708},\"end\":44886,\"start\":44509},{\"attributes\":{\"doi\":\"10.1007/s10579-015-9332-5\",\"id\":\"b16\",\"matched_paper_id\":8897969},\"end\":45601,\"start\":44888},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6682953},\"end\":46052,\"start\":45603},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":94635},\"end\":46309,\"start\":46054},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":30423603},\"end\":46656,\"start\":46311},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11047443},\"end\":47043,\"start\":46658},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15535376},\"end\":47704,\"start\":47045},{\"attributes\":{\"id\":\"b22\"},\"end\":48254,\"start\":47706},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14604520},\"end\":48662,\"start\":48256},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":18720573},\"end\":49017,\"start\":48664},{\"attributes\":{\"doi\":\"CoRR abs/1609.06038\",\"id\":\"b25\"},\"end\":49379,\"start\":49019},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8899626},\"end\":49983,\"start\":49381},{\"attributes\":{\"doi\":\"CoRR abs/1705.02364\",\"id\":\"b27\"},\"end\":50419,\"start\":49985},{\"attributes\":{\"doi\":\"10.1017/S1351324909990234\",\"id\":\"b28\",\"matched_paper_id\":18717799},\"end\":50798,\"start\":50421},{\"attributes\":{\"doi\":\"10.1371/journal.pone.0121945\",\"id\":\"b29\",\"matched_paper_id\":157301},\"end\":51093,\"start\":50800},{\"attributes\":{\"doi\":\"10.1371/journal.pone.0121945\",\"id\":\"b30\"},\"end\":51172,\"start\":51095},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":10181753},\"end\":51580,\"start\":51174},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":24127328},\"end\":52006,\"start\":51582},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":945767},\"end\":52369,\"start\":52008},{\"attributes\":{\"id\":\"b34\"},\"end\":52520,\"start\":52371},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14076822},\"end\":52998,\"start\":52522},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":23316954},\"end\":53472,\"start\":53000},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6067240},\"end\":53736,\"start\":53474},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":36718281},\"end\":54186,\"start\":53738},{\"attributes\":{\"id\":\"b39\"},\"end\":54538,\"start\":54188},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":16787742},\"end\":54890,\"start\":54540},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":1090122},\"end\":55411,\"start\":54892},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":11647185},\"end\":55796,\"start\":55413},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2937095},\"end\":56141,\"start\":55798},{\"attributes\":{\"doi\":\"10.1162/neco.1997.9.8.1735\",\"id\":\"b44\",\"matched_paper_id\":1915014},\"end\":56393,\"start\":56143},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":8508974},\"end\":56735,\"start\":56395},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":8384258},\"end\":57243,\"start\":56737},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":216848261},\"end\":57652,\"start\":57245},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":7640960},\"end\":58069,\"start\":57654},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":7640960},\"end\":58493,\"start\":58071},{\"attributes\":{\"doi\":\"CoRR abs/1607.01759\",\"id\":\"b50\"},\"end\":58770,\"start\":58495},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":12998432},\"end\":59103,\"start\":58772},{\"attributes\":{\"id\":\"b52\"},\"end\":59460,\"start\":59105},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":30228404},\"end\":59908,\"start\":59462},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":5163433},\"end\":60404,\"start\":59910},{\"attributes\":{\"doi\":\"CoRR abs/1405.4053\",\"id\":\"b55\"},\"end\":60652,\"start\":60406},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":258621},\"end\":61234,\"start\":60654},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3261600},\"end\":61664,\"start\":61236},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":23591179},\"end\":62213,\"start\":61666},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":14068874},\"end\":62696,\"start\":62215},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":762228},\"end\":63199,\"start\":62698},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":8910334},\"end\":63595,\"start\":63201},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":8120559},\"end\":64046,\"start\":63597},{\"attributes\":{\"doi\":\"CoRR abs/1301.3781\",\"id\":\"b63\"},\"end\":64328,\"start\":64048},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":16447573},\"end\":64812,\"start\":64330},{\"attributes\":{\"doi\":\"10.1145/219717.219748\",\"id\":\"b65\",\"matched_paper_id\":1671874},\"end\":65040,\"start\":64814},{\"attributes\":{\"doi\":\"10.1007/11871842_32\",\"id\":\"b66\",\"matched_paper_id\":574838},\"end\":65365,\"start\":65042},{\"attributes\":{\"doi\":\"CoRR abs/1704.05358\",\"id\":\"b67\"},\"end\":65621,\"start\":65367},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":7167272},\"end\":66084,\"start\":65623},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":2085726},\"end\":66363,\"start\":66086},{\"attributes\":{\"id\":\"b70\"},\"end\":66682,\"start\":66365},{\"attributes\":{\"id\":\"b71\"},\"end\":66949,\"start\":66684},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":1957433},\"end\":67230,\"start\":66951},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":10028211},\"end\":67686,\"start\":67232},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":18283203},\"end\":67981,\"start\":67688},{\"attributes\":{\"id\":\"b75\"},\"end\":68277,\"start\":67983},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":15784336},\"end\":68651,\"start\":68279},{\"attributes\":{\"doi\":\"CoRR abs/1606.01283\",\"id\":\"b77\"},\"end\":69052,\"start\":68653},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":3158692},\"end\":69454,\"start\":69054},{\"attributes\":{\"doi\":\"10.3233/SW-140147\",\"id\":\"b79\",\"matched_paper_id\":6081690},\"end\":69848,\"start\":69456},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":30958369},\"end\":70200,\"start\":69850},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":207218382},\"end\":70582,\"start\":70202},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":8938789},\"end\":70985,\"start\":70584},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":5981377},\"end\":71419,\"start\":70987},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":7450224},\"end\":71801,\"start\":71421},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":31826507},\"end\":72401,\"start\":71803},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":12233462},\"end\":72816,\"start\":72403},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":12233462},\"end\":73206,\"start\":72818},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":57564106},\"end\":73592,\"start\":73208},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":3202289},\"end\":73939,\"start\":73594},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":5882977},\"end\":74225,\"start\":73941},{\"attributes\":{\"doi\":\"CoRR abs/1705.00364\",\"id\":\"b91\"},\"end\":74500,\"start\":74227},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":36266172},\"end\":74938,\"start\":74502},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":15213264},\"end\":75299,\"start\":74940},{\"attributes\":{\"id\":\"b94\"},\"end\":75719,\"start\":75301},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":5986178},\"end\":76054,\"start\":75721}]", "bib_title": "[{\"end\":40011,\"start\":39915},{\"end\":40712,\"start\":40650},{\"end\":41232,\"start\":41142},{\"end\":41717,\"start\":41658},{\"end\":42097,\"start\":42047},{\"end\":42552,\"start\":42414},{\"end\":43053,\"start\":42994},{\"end\":43420,\"start\":43273},{\"end\":43794,\"start\":43765},{\"end\":44147,\"start\":44055},{\"end\":44607,\"start\":44509},{\"end\":45074,\"start\":44888},{\"end\":45682,\"start\":45603},{\"end\":46126,\"start\":46054},{\"end\":46406,\"start\":46311},{\"end\":46754,\"start\":46658},{\"end\":47109,\"start\":47045},{\"end\":48320,\"start\":48256},{\"end\":48770,\"start\":48664},{\"end\":49504,\"start\":49381},{\"end\":50488,\"start\":50421},{\"end\":50878,\"start\":50800},{\"end\":51271,\"start\":51174},{\"end\":51690,\"start\":51582},{\"end\":52079,\"start\":52008},{\"end\":52630,\"start\":52522},{\"end\":53104,\"start\":53000},{\"end\":53503,\"start\":53474},{\"end\":53837,\"start\":53738},{\"end\":54268,\"start\":54188},{\"end\":54636,\"start\":54540},{\"end\":55026,\"start\":54892},{\"end\":55469,\"start\":55413},{\"end\":55868,\"start\":55798},{\"end\":56165,\"start\":56143},{\"end\":56422,\"start\":56395},{\"end\":56816,\"start\":56737},{\"end\":57320,\"start\":57245},{\"end\":57727,\"start\":57654},{\"end\":58144,\"start\":58071},{\"end\":58841,\"start\":58772},{\"end\":59137,\"start\":59105},{\"end\":59572,\"start\":59462},{\"end\":60003,\"start\":59910},{\"end\":60763,\"start\":60654},{\"end\":61336,\"start\":61236},{\"end\":61795,\"start\":61666},{\"end\":62271,\"start\":62215},{\"end\":62776,\"start\":62698},{\"end\":63274,\"start\":63201},{\"end\":63678,\"start\":63597},{\"end\":64405,\"start\":64330},{\"end\":64853,\"start\":64814},{\"end\":65118,\"start\":65042},{\"end\":65731,\"start\":65623},{\"end\":66147,\"start\":66086},{\"end\":66996,\"start\":66951},{\"end\":67328,\"start\":67232},{\"end\":67753,\"start\":67688},{\"end\":68405,\"start\":68279},{\"end\":69152,\"start\":69054},{\"end\":69527,\"start\":69456},{\"end\":69951,\"start\":69850},{\"end\":70288,\"start\":70202},{\"end\":70647,\"start\":70584},{\"end\":71093,\"start\":70987},{\"end\":71500,\"start\":71421},{\"end\":71987,\"start\":71803},{\"end\":72458,\"start\":72403},{\"end\":72873,\"start\":72818},{\"end\":73275,\"start\":73208},{\"end\":73656,\"start\":73594},{\"end\":73991,\"start\":73941},{\"end\":74602,\"start\":74502},{\"end\":75012,\"start\":74940},{\"end\":75799,\"start\":75721}]", "bib_author": "[{\"end\":39899,\"start\":39888},{\"end\":40045,\"start\":40013},{\"end\":40059,\"start\":40045},{\"end\":40074,\"start\":40059},{\"end\":40084,\"start\":40074},{\"end\":40096,\"start\":40084},{\"end\":40120,\"start\":40096},{\"end\":40131,\"start\":40120},{\"end\":40152,\"start\":40131},{\"end\":40169,\"start\":40152},{\"end\":40186,\"start\":40169},{\"end\":40202,\"start\":40186},{\"end\":40215,\"start\":40202},{\"end\":40222,\"start\":40215},{\"end\":40728,\"start\":40714},{\"end\":40742,\"start\":40728},{\"end\":40757,\"start\":40742},{\"end\":40769,\"start\":40757},{\"end\":40780,\"start\":40769},{\"end\":40803,\"start\":40780},{\"end\":40815,\"start\":40803},{\"end\":40830,\"start\":40815},{\"end\":40844,\"start\":40830},{\"end\":40858,\"start\":40844},{\"end\":41248,\"start\":41234},{\"end\":41262,\"start\":41248},{\"end\":41274,\"start\":41262},{\"end\":41285,\"start\":41274},{\"end\":41308,\"start\":41285},{\"end\":41323,\"start\":41308},{\"end\":41337,\"start\":41323},{\"end\":41351,\"start\":41337},{\"end\":41733,\"start\":41719},{\"end\":41745,\"start\":41733},{\"end\":41756,\"start\":41745},{\"end\":41779,\"start\":41756},{\"end\":42113,\"start\":42099},{\"end\":42125,\"start\":42113},{\"end\":42136,\"start\":42125},{\"end\":42159,\"start\":42136},{\"end\":42171,\"start\":42159},{\"end\":42565,\"start\":42554},{\"end\":42583,\"start\":42565},{\"end\":42601,\"start\":42583},{\"end\":42620,\"start\":42601},{\"end\":42639,\"start\":42620},{\"end\":43070,\"start\":43055},{\"end\":43084,\"start\":43070},{\"end\":43095,\"start\":43084},{\"end\":43438,\"start\":43422},{\"end\":43451,\"start\":43438},{\"end\":43476,\"start\":43451},{\"end\":43812,\"start\":43796},{\"end\":43832,\"start\":43812},{\"end\":43845,\"start\":43832},{\"end\":44161,\"start\":44149},{\"end\":44176,\"start\":44161},{\"end\":44192,\"start\":44176},{\"end\":44207,\"start\":44192},{\"end\":44621,\"start\":44609},{\"end\":44635,\"start\":44621},{\"end\":45094,\"start\":45076},{\"end\":45114,\"start\":45094},{\"end\":45129,\"start\":45114},{\"end\":45145,\"start\":45129},{\"end\":45159,\"start\":45145},{\"end\":45179,\"start\":45159},{\"end\":45702,\"start\":45684},{\"end\":45721,\"start\":45702},{\"end\":45739,\"start\":45721},{\"end\":45757,\"start\":45739},{\"end\":46142,\"start\":46128},{\"end\":46422,\"start\":46408},{\"end\":46773,\"start\":46756},{\"end\":46789,\"start\":46773},{\"end\":47125,\"start\":47111},{\"end\":47141,\"start\":47125},{\"end\":47162,\"start\":47141},{\"end\":47176,\"start\":47162},{\"end\":47191,\"start\":47176},{\"end\":47210,\"start\":47191},{\"end\":47225,\"start\":47210},{\"end\":47239,\"start\":47225},{\"end\":47250,\"start\":47239},{\"end\":47269,\"start\":47250},{\"end\":47283,\"start\":47269},{\"end\":47297,\"start\":47283},{\"end\":47312,\"start\":47297},{\"end\":47720,\"start\":47706},{\"end\":47736,\"start\":47720},{\"end\":47758,\"start\":47736},{\"end\":47779,\"start\":47758},{\"end\":47793,\"start\":47779},{\"end\":47808,\"start\":47793},{\"end\":47823,\"start\":47808},{\"end\":47834,\"start\":47823},{\"end\":47848,\"start\":47834},{\"end\":47862,\"start\":47848},{\"end\":48332,\"start\":48322},{\"end\":48346,\"start\":48332},{\"end\":48366,\"start\":48346},{\"end\":48387,\"start\":48366},{\"end\":48396,\"start\":48387},{\"end\":48788,\"start\":48772},{\"end\":48803,\"start\":48788},{\"end\":49111,\"start\":49100},{\"end\":49124,\"start\":49111},{\"end\":49139,\"start\":49124},{\"end\":49147,\"start\":49139},{\"end\":49158,\"start\":49147},{\"end\":49521,\"start\":49506},{\"end\":49540,\"start\":49521},{\"end\":49553,\"start\":49540},{\"end\":50097,\"start\":50081},{\"end\":50110,\"start\":50097},{\"end\":50126,\"start\":50110},{\"end\":50506,\"start\":50490},{\"end\":50522,\"start\":50506},{\"end\":50535,\"start\":50522},{\"end\":50541,\"start\":50535},{\"end\":50898,\"start\":50880},{\"end\":50912,\"start\":50898},{\"end\":51285,\"start\":51273},{\"end\":51298,\"start\":51285},{\"end\":51314,\"start\":51298},{\"end\":51709,\"start\":51692},{\"end\":51724,\"start\":51709},{\"end\":51732,\"start\":51724},{\"end\":52104,\"start\":52081},{\"end\":52127,\"start\":52104},{\"end\":52433,\"start\":52412},{\"end\":52648,\"start\":52632},{\"end\":52666,\"start\":52648},{\"end\":52681,\"start\":52666},{\"end\":52697,\"start\":52681},{\"end\":53120,\"start\":53106},{\"end\":53144,\"start\":53120},{\"end\":53158,\"start\":53144},{\"end\":53174,\"start\":53158},{\"end\":53524,\"start\":53505},{\"end\":53544,\"start\":53524},{\"end\":53566,\"start\":53544},{\"end\":53853,\"start\":53839},{\"end\":53872,\"start\":53853},{\"end\":53885,\"start\":53872},{\"end\":53900,\"start\":53885},{\"end\":54278,\"start\":54270},{\"end\":54292,\"start\":54278},{\"end\":54303,\"start\":54292},{\"end\":54646,\"start\":54638},{\"end\":54657,\"start\":54646},{\"end\":55036,\"start\":55028},{\"end\":55050,\"start\":55036},{\"end\":55064,\"start\":55050},{\"end\":55077,\"start\":55064},{\"end\":55088,\"start\":55077},{\"end\":55487,\"start\":55471},{\"end\":55508,\"start\":55487},{\"end\":55526,\"start\":55508},{\"end\":55542,\"start\":55526},{\"end\":55882,\"start\":55870},{\"end\":55897,\"start\":55882},{\"end\":55912,\"start\":55897},{\"end\":56184,\"start\":56167},{\"end\":56204,\"start\":56184},{\"end\":56437,\"start\":56424},{\"end\":56454,\"start\":56437},{\"end\":56469,\"start\":56454},{\"end\":56484,\"start\":56469},{\"end\":56502,\"start\":56484},{\"end\":56832,\"start\":56818},{\"end\":56845,\"start\":56832},{\"end\":56859,\"start\":56845},{\"end\":56868,\"start\":56859},{\"end\":56880,\"start\":56868},{\"end\":56892,\"start\":56880},{\"end\":57335,\"start\":57322},{\"end\":57353,\"start\":57335},{\"end\":57373,\"start\":57353},{\"end\":57384,\"start\":57373},{\"end\":57390,\"start\":57384},{\"end\":57745,\"start\":57729},{\"end\":57762,\"start\":57745},{\"end\":57781,\"start\":57762},{\"end\":58162,\"start\":58146},{\"end\":58179,\"start\":58162},{\"end\":58198,\"start\":58179},{\"end\":58559,\"start\":58544},{\"end\":58574,\"start\":58559},{\"end\":58592,\"start\":58574},{\"end\":58607,\"start\":58592},{\"end\":58855,\"start\":58843},{\"end\":58871,\"start\":58855},{\"end\":58889,\"start\":58871},{\"end\":59151,\"start\":59139},{\"end\":59162,\"start\":59151},{\"end\":59184,\"start\":59162},{\"end\":59201,\"start\":59184},{\"end\":59219,\"start\":59201},{\"end\":59588,\"start\":59574},{\"end\":59607,\"start\":59588},{\"end\":59622,\"start\":59607},{\"end\":60014,\"start\":60005},{\"end\":60027,\"start\":60014},{\"end\":60036,\"start\":60027},{\"end\":60470,\"start\":60462},{\"end\":60480,\"start\":60470},{\"end\":60489,\"start\":60480},{\"end\":60775,\"start\":60765},{\"end\":60791,\"start\":60775},{\"end\":60801,\"start\":60791},{\"end\":60809,\"start\":60801},{\"end\":60832,\"start\":60809},{\"end\":60844,\"start\":60832},{\"end\":60866,\"start\":60844},{\"end\":60882,\"start\":60866},{\"end\":61350,\"start\":61338},{\"end\":61364,\"start\":61350},{\"end\":61373,\"start\":61364},{\"end\":61387,\"start\":61373},{\"end\":61813,\"start\":61797},{\"end\":61831,\"start\":61813},{\"end\":61846,\"start\":61831},{\"end\":61863,\"start\":61846},{\"end\":61875,\"start\":61863},{\"end\":62296,\"start\":62273},{\"end\":62312,\"start\":62296},{\"end\":62324,\"start\":62312},{\"end\":62338,\"start\":62324},{\"end\":62356,\"start\":62338},{\"end\":62372,\"start\":62356},{\"end\":62793,\"start\":62778},{\"end\":62809,\"start\":62793},{\"end\":62823,\"start\":62809},{\"end\":62841,\"start\":62823},{\"end\":62861,\"start\":62841},{\"end\":62881,\"start\":62861},{\"end\":63309,\"start\":63276},{\"end\":63694,\"start\":63680},{\"end\":63706,\"start\":63694},{\"end\":63720,\"start\":63706},{\"end\":63735,\"start\":63720},{\"end\":63746,\"start\":63735},{\"end\":63759,\"start\":63746},{\"end\":64125,\"start\":64110},{\"end\":64135,\"start\":64125},{\"end\":64149,\"start\":64135},{\"end\":64163,\"start\":64149},{\"end\":64422,\"start\":64407},{\"end\":64438,\"start\":64422},{\"end\":64448,\"start\":64438},{\"end\":64464,\"start\":64448},{\"end\":64475,\"start\":64464},{\"end\":64865,\"start\":64855},{\"end\":64873,\"start\":64865},{\"end\":65142,\"start\":65120},{\"end\":65423,\"start\":65413},{\"end\":65434,\"start\":65423},{\"end\":65452,\"start\":65434},{\"end\":65751,\"start\":65733},{\"end\":65767,\"start\":65751},{\"end\":65783,\"start\":65767},{\"end\":65791,\"start\":65783},{\"end\":66166,\"start\":66149},{\"end\":66189,\"start\":66166},{\"end\":66467,\"start\":66447},{\"end\":66482,\"start\":66467},{\"end\":66496,\"start\":66482},{\"end\":66699,\"start\":66684},{\"end\":66712,\"start\":66699},{\"end\":66724,\"start\":66712},{\"end\":66733,\"start\":66724},{\"end\":66748,\"start\":66733},{\"end\":67018,\"start\":66998},{\"end\":67034,\"start\":67018},{\"end\":67057,\"start\":67034},{\"end\":67353,\"start\":67330},{\"end\":67374,\"start\":67353},{\"end\":67391,\"start\":67374},{\"end\":67399,\"start\":67391},{\"end\":67769,\"start\":67755},{\"end\":67783,\"start\":67769},{\"end\":67799,\"start\":67783},{\"end\":68004,\"start\":67985},{\"end\":68024,\"start\":68004},{\"end\":68046,\"start\":68024},{\"end\":68064,\"start\":68046},{\"end\":68086,\"start\":68064},{\"end\":68776,\"start\":68759},{\"end\":68790,\"start\":68776},{\"end\":68811,\"start\":68790},{\"end\":69171,\"start\":69154},{\"end\":69185,\"start\":69171},{\"end\":69206,\"start\":69185},{\"end\":69546,\"start\":69529},{\"end\":69964,\"start\":69953},{\"end\":70303,\"start\":70290},{\"end\":70316,\"start\":70303},{\"end\":70330,\"start\":70316},{\"end\":70339,\"start\":70330},{\"end\":70356,\"start\":70339},{\"end\":70665,\"start\":70649},{\"end\":70678,\"start\":70665},{\"end\":70696,\"start\":70678},{\"end\":70714,\"start\":70696},{\"end\":70728,\"start\":70714},{\"end\":71117,\"start\":71095},{\"end\":71133,\"start\":71117},{\"end\":71140,\"start\":71133},{\"end\":71527,\"start\":71502},{\"end\":71543,\"start\":71527},{\"end\":71551,\"start\":71543},{\"end\":72003,\"start\":71989},{\"end\":72017,\"start\":72003},{\"end\":72026,\"start\":72017},{\"end\":72038,\"start\":72026},{\"end\":72478,\"start\":72460},{\"end\":72493,\"start\":72478},{\"end\":72504,\"start\":72493},{\"end\":72528,\"start\":72504},{\"end\":72535,\"start\":72528},{\"end\":72893,\"start\":72875},{\"end\":72908,\"start\":72893},{\"end\":72926,\"start\":72908},{\"end\":72948,\"start\":72926},{\"end\":73291,\"start\":73277},{\"end\":73305,\"start\":73291},{\"end\":73319,\"start\":73305},{\"end\":73334,\"start\":73319},{\"end\":73672,\"start\":73658},{\"end\":73686,\"start\":73672},{\"end\":73700,\"start\":73686},{\"end\":73715,\"start\":73700},{\"end\":74007,\"start\":73993},{\"end\":74021,\"start\":74007},{\"end\":74035,\"start\":74021},{\"end\":74050,\"start\":74035},{\"end\":74309,\"start\":74295},{\"end\":74323,\"start\":74309},{\"end\":74612,\"start\":74604},{\"end\":74625,\"start\":74612},{\"end\":74636,\"start\":74625},{\"end\":74648,\"start\":74636},{\"end\":74657,\"start\":74648},{\"end\":75022,\"start\":75014},{\"end\":75044,\"start\":75022},{\"end\":75061,\"start\":75044},{\"end\":75432,\"start\":75419},{\"end\":75443,\"start\":75432},{\"end\":75457,\"start\":75443},{\"end\":75476,\"start\":75457},{\"end\":75815,\"start\":75801},{\"end\":75828,\"start\":75815}]", "bib_venue": "[{\"end\":40253,\"start\":40246},{\"end\":40889,\"start\":40882},{\"end\":41400,\"start\":41384},{\"end\":41830,\"start\":41813},{\"end\":42206,\"start\":42197},{\"end\":42680,\"start\":42668},{\"end\":43130,\"start\":43121},{\"end\":43517,\"start\":43505},{\"end\":43882,\"start\":43872},{\"end\":44258,\"start\":44241},{\"end\":44676,\"start\":44664},{\"end\":45792,\"start\":45783},{\"end\":46169,\"start\":46164},{\"end\":46463,\"start\":46451},{\"end\":46830,\"start\":46818},{\"end\":47345,\"start\":47337},{\"end\":48433,\"start\":48423},{\"end\":48834,\"start\":48827},{\"end\":49586,\"start\":49578},{\"end\":51349,\"start\":51340},{\"end\":51773,\"start\":51761},{\"end\":52168,\"start\":52156},{\"end\":52738,\"start\":52726},{\"end\":53215,\"start\":53203},{\"end\":53601,\"start\":53592},{\"end\":53941,\"start\":53929},{\"end\":54330,\"start\":54325},{\"end\":54692,\"start\":54683},{\"end\":55119,\"start\":55112},{\"end\":55583,\"start\":55571},{\"end\":55947,\"start\":55938},{\"end\":56537,\"start\":56528},{\"end\":56917,\"start\":56913},{\"end\":57427,\"start\":57417},{\"end\":57844,\"start\":57821},{\"end\":58261,\"start\":58238},{\"end\":58912,\"start\":58909},{\"end\":59663,\"start\":59651},{\"end\":60147,\"start\":60100},{\"end\":60923,\"start\":60911},{\"end\":61428,\"start\":61416},{\"end\":61916,\"start\":61904},{\"end\":62435,\"start\":62412},{\"end\":62912,\"start\":62905},{\"end\":63800,\"start\":63788},{\"end\":64510,\"start\":64501},{\"end\":65192,\"start\":65185},{\"end\":65832,\"start\":65820},{\"end\":66222,\"start\":66214},{\"end\":67084,\"start\":67079},{\"end\":67436,\"start\":67426},{\"end\":67828,\"start\":67822},{\"end\":68448,\"start\":68436},{\"end\":69229,\"start\":69226},{\"end\":70005,\"start\":69993},{\"end\":70389,\"start\":70381},{\"end\":70753,\"start\":70749},{\"end\":71181,\"start\":71169},{\"end\":71582,\"start\":71575},{\"end\":72079,\"start\":72067},{\"end\":72586,\"start\":72569},{\"end\":72989,\"start\":72977},{\"end\":73742,\"start\":73737},{\"end\":74075,\"start\":74071},{\"end\":74698,\"start\":74686},{\"end\":75092,\"start\":75085},{\"end\":75866,\"start\":75854},{\"end\":39206,\"start\":39174},{\"end\":39362,\"start\":39242},{\"end\":39659,\"start\":39566},{\"end\":39824,\"start\":39766},{\"end\":40244,\"start\":40222},{\"end\":40880,\"start\":40858},{\"end\":41382,\"start\":41351},{\"end\":41811,\"start\":41779},{\"end\":42195,\"start\":42171},{\"end\":42666,\"start\":42639},{\"end\":43119,\"start\":43095},{\"end\":43503,\"start\":43476},{\"end\":43870,\"start\":43845},{\"end\":44239,\"start\":44207},{\"end\":44662,\"start\":44635},{\"end\":45220,\"start\":45204},{\"end\":45781,\"start\":45757},{\"end\":46162,\"start\":46142},{\"end\":46449,\"start\":46422},{\"end\":46816,\"start\":46789},{\"end\":47335,\"start\":47312},{\"end\":47926,\"start\":47862},{\"end\":48421,\"start\":48396},{\"end\":48825,\"start\":48803},{\"end\":49098,\"start\":49019},{\"end\":49576,\"start\":49553},{\"end\":50079,\"start\":49985},{\"end\":50586,\"start\":50566},{\"end\":50948,\"start\":50940},{\"end\":51338,\"start\":51314},{\"end\":51759,\"start\":51732},{\"end\":52154,\"start\":52127},{\"end\":52410,\"start\":52371},{\"end\":52724,\"start\":52697},{\"end\":53201,\"start\":53174},{\"end\":53590,\"start\":53566},{\"end\":53927,\"start\":53900},{\"end\":54323,\"start\":54303},{\"end\":54681,\"start\":54657},{\"end\":55110,\"start\":55088},{\"end\":55569,\"start\":55542},{\"end\":55936,\"start\":55912},{\"end\":56243,\"start\":56230},{\"end\":56526,\"start\":56502},{\"end\":56911,\"start\":56892},{\"end\":57415,\"start\":57390},{\"end\":57819,\"start\":57781},{\"end\":58236,\"start\":58198},{\"end\":58542,\"start\":58495},{\"end\":58907,\"start\":58889},{\"end\":59260,\"start\":59219},{\"end\":59649,\"start\":59622},{\"end\":60098,\"start\":60036},{\"end\":60460,\"start\":60406},{\"end\":60909,\"start\":60882},{\"end\":61414,\"start\":61387},{\"end\":61902,\"start\":61875},{\"end\":62410,\"start\":62372},{\"end\":62903,\"start\":62881},{\"end\":63356,\"start\":63309},{\"end\":63786,\"start\":63759},{\"end\":64108,\"start\":64048},{\"end\":64499,\"start\":64475},{\"end\":64905,\"start\":64894},{\"end\":65183,\"start\":65161},{\"end\":65411,\"start\":65367},{\"end\":65818,\"start\":65791},{\"end\":66212,\"start\":66189},{\"end\":66445,\"start\":66365},{\"end\":66809,\"start\":66748},{\"end\":67077,\"start\":67057},{\"end\":67424,\"start\":67399},{\"end\":67820,\"start\":67799},{\"end\":68434,\"start\":68407},{\"end\":68757,\"start\":68653},{\"end\":69224,\"start\":69206},{\"end\":69632,\"start\":69563},{\"end\":69991,\"start\":69964},{\"end\":70379,\"start\":70356},{\"end\":70747,\"start\":70728},{\"end\":71167,\"start\":71140},{\"end\":71573,\"start\":71551},{\"end\":72065,\"start\":72038},{\"end\":72567,\"start\":72535},{\"end\":72975,\"start\":72948},{\"end\":73364,\"start\":73334},{\"end\":73735,\"start\":73715},{\"end\":74069,\"start\":74050},{\"end\":74293,\"start\":74227},{\"end\":74684,\"start\":74657},{\"end\":75083,\"start\":75061},{\"end\":75417,\"start\":75301},{\"end\":75852,\"start\":75828}]"}}}, "year": 2023, "month": 12, "day": 17}