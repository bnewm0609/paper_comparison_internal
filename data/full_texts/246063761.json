{"id": 246063761, "updated": "2023-10-05 17:57:49.423", "metadata": {"title": "Watermarking Pre-trained Encoders in Contrastive Learning", "authors": "[{\"first\":\"Yutong\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Han\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Tianwei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"L\",\"last\":\"Jiwei\",\"middle\":[]},{\"first\":\"Meikang\",\"last\":\"Qiu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Contrastive learning has become a popular technique to pre-train image encoders, which could be used to build various downstream classification models in an efficient way. This process requires a large amount of data and computation resources. Hence, the pre-trained encoders are an important intellectual property that needs to be carefully protected. It is challenging to migrate existing watermarking techniques from the classification tasks to the contrastive learning scenario, as the owner of the encoder lacks the knowledge of the downstream tasks which will be developed from the encoder in the future. We propose the \\textit{first} watermarking methodology for the pre-trained encoders. We introduce a task-agnostic loss function to effectively embed into the encoder a backdoor as the watermark. This backdoor can still exist in any downstream models transferred from the encoder. Extensive evaluations over different contrastive learning algorithms, datasets, and downstream tasks indicate our watermarks exhibit high effectiveness and robustness against different adversarial operations.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.08217", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icdis/WuQZLQ22", "doi": "10.1109/icdis55630.2022.00042"}}, "content": {"source": {"pdf_hash": "cdca1ee700af2a4252e9d01806f6581a1c9cc548", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.08217v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6ffdae9144eaf5c822da3b8165d56608c5b60ce6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cdca1ee700af2a4252e9d01806f6581a1c9cc548.txt", "contents": "\nWatermarking Pre-trained Encoders in Contrastive Learning\n\n\nYutong Wu \nTsinghua University\nChina\n\nHan Qiu qiuhan@tsinghua.edu.cn \nTsinghua University\nChina\n\nTianwei Zhang tianwei.zhang@ntu.edu.sg \nNanyang Technological University\nSingapore\n\nJiwei Li jiweili@shannonai.com \nShannon.AI\nZhejiang University\nChina\n\nMeikang Qiu qiumeikang@yahoo.com \nTexas A&M University\nTexasUSA\n\nWatermarking Pre-trained Encoders in Contrastive Learning\n\nContrastive learning has become a popular technique to pre-train image encoders, which could be used to build various downstream classification models in an efficient way. This process requires a large amount of data and computation resources. Hence, the pre-trained encoders are an important intellectual property that needs to be carefully protected. It is challenging to migrate existing watermarking techniques from the classification tasks to the contrastive learning scenario, as the owner of the encoder lacks the knowledge of the downstream tasks which will be developed from the encoder in the future. We propose the first watermarking methodology for the pre-trained encoders. We introduce a task-agnostic loss function to effectively embed into the encoder a backdoor as the watermark. This backdoor can still exist in any downstream models transferred from the encoder. Extensive evaluations over different contrastive learning algorithms, datasets, and downstream tasks indicate our watermarks exhibit high effectiveness and robustness against different adversarial operations.\n\nIntroduction\n\nContrastive learning [Chen et al., 2020a;Chen et al., 2020b;Caron et al., 2020;Chen and He, 2020;Grill et al., 2020;Zbontar et al., 2021] has demonstrated huge potential for unsupervised learning recently. Its main idea is to use a large number of unlabeled data to pre-train an encoder for effective feature representation, which could be further used to build different types of downstream classifiers with limited labeled data. Such approaches can be performed over noisy and uncurated data which can get rid of the expensive data labeling efforts [Carlini and Terzis, 2021]. Moreover, contrastive learning has achieved better performance than classic supervised learning in many tasks. For instance, SimCLR [Chen et al., 2020a], a popular contrastive learning algorithm, has been proved to have a better performance in an ImageNet classification task than various supervised learning algorithms. * Contact Author A typical contrastive learning pipeline consists of two components, i.e. pre-training encoders and building downstream classifiers. As the most important component, the pre-trained encoders are usually generated with many unlabeled data and a large number of training costs (e.g., GPU resources). Moreover, for special tasks like medical diagnose, collecting critical training data is also a costly process. As a result, welltrained encoders are very valuable since they can be applied to many downstream classification tasks with a simple finetuning process and a small number of labeled data. Selling pre-trained encoders has become a popular business model 1 , and these encoders are an important intellectual property (IP) for model owners. It is necessary to protect these models from being abused, illegal copy, and redistribution.\n\nOne promising technology to protect the IP of a DNN model is watermarking [Adi et al., 2018;Zhang et al., 2018;Fan et al., 2019;. A quantity of works have designed watermarking solutions for conventional classification models. By embedding a backdoor into the protected classification model during the training process, the model owner can easily verify the ownership of any suspicious model via remote query using the specific triggered samples. However, it is challenging to directly apply a similar approach to watermark a pre-trained encoder from contrastive learning due to two reasons. First, when embedding the watermarks, the owner of the pre-trained encoder does not know the specific downstream classification tasks that will be built from it, as well as the corresponding dataset. Therefore, it is difficult for the owner to craft backdoor and verification samples. Second, during verification, the encoder owner can only query the downstream model to check if it is built from his watermarked encoder. He can only obtain the final output from the classification layer, rather than the feature representation from his encoder. This also hinders the owner from checking the existence of a backdoor.\n\nTo address these challenges, this work proposes the first watermarking methodology for the IP protection of the pretrained encoders in contrastive learning. Our solution can achieve task-agnostic, i.e., the owner does not need any prior information about the downstream models and datasets, and the embedded watermark is verifiable for arbitrary downstream tasks. Particularly, our watermark is also based on the backdoor technique. Instead of manipulating the verification samples to have unique predicted labels, we aim to make them have unique feature representations from the encoder, which can naturally lead to unique predicted labels for any subsequent downstream models. Specifically, we introduce a loss function to fine-tune the model for watermark embedding, which can make its output of samples with a speciallydesigned trigger deviate a lot from the output of a normal encoder. When the adversary illegally obtains this encoder and trains a classification model from it, the owner can use verification samples (i.e., normal samples with the trigger) to query the model. When its label is different from the normal case (i.e., query results of the same normal sample), the owner has confidence to identify this as a plagiarization.\n\nWe extensively evaluate our watermarking method on different popular datasets (STL10, GTSRB, and SVHN) with two representative contrastive learning algorithms (SimCLR [Chen et al., 2020a] and MocoV2 [Chen et al., 2020b]). The results indicate that our proposed technique is effective at distinguishing plagiarized models from independent ones regardless of the downstream tasks. It will not affect the functionality of the encoder and any downstream models. Besides, the embedded watermark exhibits strong robustness against different adversarial operations (e.g., fine-tuning, pruning), making it hard to be removed.\n\n\nBackground\n\n\nContrastive Learning\n\nThe aim of contrastive learning is to pre-train an encoder by using a huge amount of unlabeled images. Specifically, for each image \u03d5, the contrastive learning algorithm first generates a positive sample \u03d5 + which is transformed from \u03d5, as well as a negative sample \u03d5 \u2212 , which is transformed from a different sample. We expect the outputs f (\u03d5) and f (\u03d5 + ) to be as similar as possible, since \u03d5 and \u03d5 + are from the same class. In addition, we also need to maximize the difference between f (\u03d5) and f (\u03d5 \u2212 ), as they are likely to be in different classes. By pursuing the above optimization objective, the encoder can be trained with unlabeled samples to accurately predict the feature representation of any image from any category. Below we briefly introduce two representative contrastive learning algorithms. SimCLR [Chen et al., 2020a]. This algorithm uses common data augmentation operations (random crop, Gaussian blur, random flip, etc.) to generate positive and negative samples. For a N -image batch, SimCLR generates 2N images (x i , (i = 1, ..., 2N )) by applying data augmentation twice to the samples in it. Two transformed images form a positive pair if they are originated from the same image, or a negative one otherwise. To achieve the goal of contrastive learning, a contrastive loss is defined as below:\nl i,j = \u2212 log exp(sim(z i , z j )/\u03c4 ) 2N k=1 I(k = i) \u00b7 exp(sim(z i , z k )/\u03c4 )(1)\nwhere (z i , z j ) is the positive pair of feature vectors produced by the encoder (here z i = f (x i )), while (z i , z k ) is the negative one. sim(\u00b7, \u00b7) is the cosine similarity between the two feature vectors. \u03c4 is a temperature parameter used to scale the cosine similarity and exp is the natural exponential function. If the feature vectors of the positive pair (z i , z j ) are more similar and the ones of the rest negative pairs (z i , z k ) are more different, the loss value will be smaller. Therefore, we need to update the encoder parameters to minimize the above loss function. SimCLR sums up the loss of all positive pairs from the batch as the final loss and learns the image encoder via minimizing this final loss.\n\nMocoV2 [Chen et al., 2020b]. The core of this algorithm is to introduce a dictionary of feature vectors as a queue so as to reduce the memory cost in the training process by reusing the feature vectors from the immediate mini-batches. To this end, MocoV2 uses two encoders called query encoder (f q ) and momentum encoder (f k ). Given a batch of N input images, f q produces N feature vectors q and f k produces N feature vectors k + by applying different data augmentations on the input images. The contrastive loss is then subsequently calculate as below:\nL = \u2212 log exp(q \u00b7 k + /\u03c4 ) K i=0 exp(q \u00b7 k i /\u03c4 )(2)\nwhere k i is the feature vectors of previous batches. \u03c4 is the same as in SimCLR. Usually, the queue has a much bigger size than the mini-batch. After calculating the contrastive loss, the feature vectors produced by the momentum encoder (k) are then pushed into the queue and one of the earliest batches is deleted simultaneously. MocoV2 updates the parameters of its two encoders in different ways. The parameters in the query encoder are updated by the back-propagation algorithm according to the immediate contrastive loss. The parameters in the momentum encoder are updated by the following Equation:\n\u03b8 k = m \u00b7 \u03b8 k + (1 \u2212 m) \u00b7 \u03b8 q (3)\nwhere \u03b8 k is the old parameters of the momentum encoder, \u03b8 q is the immediate parameters of the query encoder, m is a hyper-parameter to control the updating speed, and \u03b8 k is the updated parameters of the momentum encoder.\n\n\nWatermarking\n\nWatermarking DL Models. The general goal of watermarking is to protect the IP of a DNN model. The most popular watermarking solution leverages the DNN backdoor technique. A standard watermarking solution consists of two phases . In the first phase, the model owner employs a watermark embedding algorithm to inject a backdoor into the target DL model to get a watermarked classifier. This watermarked classifier maintains the functionality for normal samples while giving unique labels for some carefully-crafted samples (i.e., verification samples). In the second phase, the model owner attempts to verify whether a suspicious classifier contains his watermark. He uses the verification samples to query this classifier. By checking the corresponding responses, he can identify the ownership of this model with high confidence. Threat model. We consider a model owner M who pretrains an encoder f and sells it to some users for building downstream tasks. However, a model plagiarist P gets this encoder in an unauthorized way (e.g., model stealing, illegal redistribution, etc.). How the adversary obtains the encoder is beyond the scope of this paper.\n\nThen the plagiarist uses f to build his own downstream classifier. He may slightly modify the encoder (e.g., finetuning, pruning). We assume the adversary does not have enough resources to alter the encoder completely. Otherwise, he will train the encoder directly without stealing it.\n\nThe owner's goal is to detect whether a suspicious classifier is built from his encoder. The model owner has only oracle access to this classifier, i.e., he can send arbitrary inputs to F s and receive the corresponding outputs. His strategy is to embed a watermark into f . When the classifier is from f , the classification output will be different from the ones output by an independent model. When embedding the watermark, the owner does not have any knowledge about the possible downstream tasks and datasets. The embedded watermark must be robust enough and unremovable by the possible model transformations from the adversary. Watermark Requirements. Specifically, a good watermarking solution should satisfy the following properties: An effective watermark should meet several requirements to guarantee its usability. In our scenario, the watermark must have characters narrated as below:\n\n\u2022 Uniqueness. This means the classification model from the watermarked encoder will give unique output for the verification samples different from that of an independent model. This is the basic requirement to guarantee the effectiveness of the watermarking scheme. \u2022 Functionality-preserving. The watermarking algorithm should have a negligible impact on the performance of the downstream model to preserve its functionality. \u2022 Robustness. A successful watermark should be robust against potential attacks from the plagiarist, e.g., model modification via fine-tuning and pruning. Once embedded, the watermark is hard to be removed by the plagiarist.\n\n\nMethodology\n\n\nOverview\n\nThe most popular way to watermark a DL model is the DNN backdoor technique [Adi et al., 2018;Zhang et al., 2018].\n\nIn this paper, we also follow this idea for protecting the pretrained encoders. Prior works have introduced backdoor attacks against pre-trained feature encoders [Jia et al., 2022] and language foundation models . However, these attacks require the adversary to have knowledge of downstream tasks and training sets. Therefore, they are not applicable to our scenario, where the encoder owner does not know anything about the downstream tasks and aims to protect the IP of any models developed from his pre-trained encoder.\n\nTo this end, we design a new task-agnostic watermarking solution for pre-trained encoders. Figure 1 shows an overview of our methodology. The key insight is that instead of embedding a targeted backdoor into the encoder that forces its downstream model to assign a specific label to the triggered samples, we can consider an untargeted backdoor, causing the downstream model to output any labels that are different from the correct ones for the triggered samples. In this case, the encoder owner is still able to verify the ownership by checking whether the model output of the triggered samples is incorrect. Meanwhile, he can achieve such untargeted backdoor embedding without any knowledge of the downstream tasks. Below we detail our methodology.\n\n\nWatermark Embedding\n\nFormally, we consider a pre-trained encoder f , which takes as input an image, and outputs its corresponding feature. The owner aims to convert f into a watermarked version f , which can satisfy the properties of uniqueness, functionalitypreserving, and robustness. The owner has access to the unlabeled training set of the encoder: D = {x 1 , x 2 , ..., x n )}.\n\nTo embed the watermark into f , the owner pre-defines a trigger pattern t and a trigger mask m to denote its position. Then for each sample x i \u2208 D, he can calculate the corresponding triggered sample as in Eq. 4.\nx t i = (1 \u2212 m) \u2297 x i + m \u2297 t(4)\nWe craft the following loss function for the owner to finetune f for watermark embedding. Uniqueness. To guarantee uniqueness, we need to ensure the downstream models developed from a normal encoder (e.g., f ) and watermarked encoder (f ) have distinct predictions for each triggered sample x t i . As the owner does not know the downstream tasks and datasets, he cannot design loss functions to restrict the downstream model predictions, as did in [Jia et al., 2022]. Instead, he can try to maximize the difference of the encoders' output (i.e., features), which could subsequently maximize the difference of the downstream model's output (i.e., labels) with very high probability. This leads to the loss term in Eq. 5.\nL u = 1 ||D|| \u00b7 xi\u2208D SIM f (x i ), f (x t i )(5)\nwhere SIM(\u00b7, \u00b7) measures the similarity between two feature vectors. In this paper, we adopt the cosine similarity in this loss term. Other similarity metrics can be applied as well.\n\nOur goal is to minimize L u , which can increase the feature distance between normal and watermarked encoders over triggered samples. Functionality-preserving. We also need to guarantee the embedded watermark does not affect the prediction accuracy of any inherited downstream models over clean samples. Similarly, since the encoder owner does not know the downstream tasks, he can try to make the output feature of f as close as that of the normal encoder f for clean samples, which will result in comparable performance in the subsequent downstream tasks. This gives us another loss term for functionality-preserving in Eq. 6.\nL p = \u2212 1 ||D|| \u00b7 xi\u2208D SIM f (x i ), f (x i )(6)\nBased on the above analysis, the watermark embedding process can be formulated as an optimization problem as \n\nwhere \u03b7 is the hyper-parameter to balance the properties of uniqueness and functionality-preserving. In this paper, we adopt the gradient descent method to solve the above optimization problem. We initialize f as the clean encoder f , and iteratively calculate the loss and update f to reach the optimal values. Robustness. As backdoor attacks generally exhibit high robustness against common model transformations (e.g., model pruning, fine-tuning), the corresponding watermarks are expected to enjoy similar robustness as well. We validate this conclusion in Section 4. In order to further enhance the watermark robustness in the encoder, we propose to adopt dropout during the embedding process, e.g., randomly dropping some neurons in each layer. This can simulate the impact of downstream model transfer learning and post-processing, which makes our watermarks more immune to these operations.\n\n\nWatermark Verification\n\nGiven a suspicious downstream model M , the encoder owner wants to verify whether M is developed from f without IP authorization. To achieve this, he first constructs a set of data samplesD corresponding to this downstream task (e.g., via downloading from the internet). For each samplex i \u2208D, he feeds it to M and obtains the predicted label. Then he calculates the corresponding verification samplex t i following Equation 4 and uses it to query M again. He checks whether the two predictions are different. If the ratio of samples getting different labels in the two cases is higher than a pre-defined threshold T , then the owner has confidence to confirm model M violates the IP of his encoder f . This process is formulated as in Eq. 8.\n1 ||D|| \u00b7 xi\u2208D I(M (x i ) = M (x i t ))) > T(8)\nwhere function I returns 1 when the inside condition is true, or 0 otherwise.\n\n\nEvaluation\n\nWe perform extensive evaluations to demonstrate our solution can achieve the desired watermark requirements. Contrastive learning methods. Our solution is general for the encoders pre-trained from different contrastive learning methods. In this paper, we adopt SimCLR [Chen et al., 2020a] and MocoV2 [Chen et al., 2020b], two of the most popular contrastive learning techniques for experimentation. We choose ResNet18 and ResNet50 as the base models for SimCLR and MocoV2, respectively.\n\n\nExperimental Setup\n\nDatasets. We use different types of datasets to pre-train the encoders and build the downstream classifiers:\n\n\u2022 CIFAR-10: it contains 60,000 images belonging to 10 classes. Each image has a size of 32 \u00d7 32 \u00d7 3. We use 50,000 samples for training and 10,000 samples for testing. \u2022 ImageNet: it contains 14,000,000 images belonging to 1,000 classes. Each picture has a size of 224 \u00d7 224 \u00d7 3. \u2022 STL10: this dataset contains 113,000 images with the size of 96 \u00d7 96 \u00d7 3. It has 5,000 training images and 8,000 test images with labels (10 classes). Besides, it also has 100,000 unlabeled images for unsupervised learning. \u2022 GTSRB: This dataset contains 51,800 images of 43 different traffic signs. Each image has a size of 32 \u00d7 32 \u00d7 3. \u2022 SVHN: it contains over 600,000 images of the digit house numbers from Google Street View. Each image has a size of 32 \u00d7 32 \u00d7 3. It is divided into a training set of 73,257 images, a test set of 26,032 images, and an extra training set of 531,131 images. All the images belong to 10 classes corresponding to the digits from '0' to '9'. In our experiment, we adopt CIFAR-10 and ImageNet for the pre-trained SimCLR and MocoV2 2 encoders, respectively. Then, we transfer the encoders to different downstream tasks for recognizing STL10, GTSRB, and SVHN samples. Backdoor triggers. Our watermark method has no assumption on the triggers plant on samples because it does not rely on a special trigger to verify. Hence, in our experiments, we adopt the checkerboard trigger of size 10 \u00d7 10 located on the bottom-right corner of images, which is similar to the Badencoder [Gu et al., 2019]. Specifically, We use white square, green square, and green cross as the triggers in our experiment. Examples are in Figure 2: the first row represents clean samples and the second row contains triggered ones. Note that similar results will be get for different triggers. Implementation. When embedding watermarks into the encoders, we adopt a batch size of 128 for SimCLR and train the encoder for 300 epochs. For MocoV2, we use a batch size of 64 and train the encoder for 50 epochs. For both training processes, we set the learning rate as 0.001 and \u03b7 as 0.5. We use Pytorch 1.10 backend for the implementation. We conduct the experiments on a server equipped with Intel I9-11900K CPU and 2 NVIDIA GeForce RTX 3090 GPUs. Possible attacks. We consider two techniques including finetuning and pruning that the attacker may deploy to change the models for erasing the watermark. They are commonly adopted in the previous evaluation of watermark robustness [Adi et al., 2018;Zhang et al., 2018;.\n\n\u2022 Fine-tuning. We consider two types of fine-tuning methods in our experiments: Re-Train Last Layers (RTLL) and Fine-Tune All Layers (FTAL). Note that we do not consider the operation of Fine-Tune Last Layer (FTLL) since the encoder only produces the feature vectors and the users always re-train the last layers for building downstream classifiers. We also do not consider Re-Train All Layers (RTAL), which makes the pre-training meaningless. \u2022 Pruning. We evaluate two pruning methods: random pruning which randomly removes some parameters in each layer and L1-pruning which removes the parameters with the smallest L1-norms. Metrics. We adopt the following two metrics to evaluate the effectiveness of our watermark. \u2022 Test Accuracy (ACC). This is to calculate the prediction accuracy of the downstream classifiers over clean samples. This metric is used to measure the functionality-preserving requirement of the watermark: a downstream classifier built from a watermarked encoder should have a tiny ACC loss compared to the one built from a clean encoder. \u2022 Watermark Accuracy (WACC). This is to measure the ratio of verification samples' prediction labels by the suspicious model different from the prediction labels of the cor-responding clean samples (Eq. 8). This metric can evaluate the uniqueness requirement: the WACC of a watermarked downstream classifier should be much higher than an independent model which can be easily separated by a threshold T . Besides, WACC can also reflect the robustness requirement. Under different attacks, the WACC of the classifier built from a watermarked pre-trained encoder should be always higher than T .\n\n\nUniqueness Analysis\n\nTo show the uniqueness of our watermark, we carry out our experiment by evaluating the WACC for two aspects. First, we use a set of triggered samples to query the clean model and the corresponding downstream classifier built from the watermarked pre-trained encoder respectively. Second, we use a set of randomly designed wrong triggered samples to test a watermarked downstream classifier. The evaluation results are shown in Table 1 and  The results indicate that our method can embed an efficient watermark into the clean pre-trained encoder and achieve high uniqueness. As shown in Table 1, our watermarking method can effectively distinguish the clean and watermarked model by giving significantly different WACC for triggered samples. Also, the difference between the AC of the 'correct' trigger and the AC of the 'wrong' trigger is also prominent. This means for wrong triggered samples, our watermarked model will give a very low WACC for a successful IP verification with uniqueness guaranteed.\n\n\nRobustness analysis\n\nTo test the Robustness of our watermark, we calculate ACC and WACC of our watermarked model which is processed with fine-tuning or pruning by the attackers. The results are shown in Table 3 and Table 4.\n\nFrom Table 3 we can observe that the WACC keeps at a high level with the pruning amount increasing when the pruning process still has a slight impact on ACC. When the pruning ratio is beyond 0.8, there is an immediate drop in both   ACC and WACC of the watermarked models. This indicates that a very large pruning ratio can mitigate the watermarking method but also significantly compromise the model's functionality. Such a high pruning ratio is pointless for an attacker since a broken model is useless. Thus, we conclude that our watermarking method is robust against model pruning. Table 4 shows the ACC and WACC of the models finetuned by RTLL and FTAL. The WACC has only a slight drop when the model is fine-tuned which indicates that our watermarking method is robust against model fine-tuning.\n\n\nPerformance-preserving analysis\n\nWe measure the performance-preserving requirement by comparing the ACC of the clean model and the watermarked one. The results are shown in Table 5  It turns out that our watermark embedding algorithm preserves the functionality of the pre-trained encoder. We can discover for Table 5 that the ACC of watermarked models is similar or even larger than that of clean models in most cases. We analyze that an even larger ACC with watermarking is due to the phenomenon that training on noisy data gives significant robustness improvements pointed in [Radford et al., 2021]. The noise brought by the triggered samples in the training dataset will not harm the model ACC which proves the robustness of our method.\n\n\nRelated Works\n\nNumerous watermarking schemes have been proposed for conventional DNN models. They can be roughly classified into the following two categories: White-box solutions. This strategy adopts redundant bits as watermarks and embeds them into the model parameters. For instance, [Uchida et al., 2017] introduced a parameter regularizer to embed a bit-vector (e.g. signature) into model parameters which can guarantee the performance of the watermarked model. [Rouhani et al., 2019] found that implanting watermarks into model parameters directly could affect their static properties (e.g histogram). Thus, they injected watermarks in the probability density function of the activation sets of the DNN layers. These methods require the owner to have white-box access to the model parameters during the watermark extraction and verification phase, which can significantly limit the possible usage scenarios. Black-box solutions. This strategy takes a set of unique sample-label pairs as watermarks and embeds their correlation into DNN models. For example, [Le Merrer et al., 2019] [Li et al., 2019] generated watermark samples that are almost indistinguishable from normal samples to avoid detection by adversaries.  designed temporal state sequences to watermark reinforcement learning models. [Xiaoxuan et al., 2021] utilized cache side channels to verify watermarks in the model architecture.\n\nIn addition to watermarking, an alternative approach to IP protection is model fingerprinting. The main difference is that the model owner crafts special samples to verify the model without making any modifications to it. DNN Fingerprint. Some works proposed fingerprinting schemes for classification models. They leveraged adversarial examples as fingerprints to identify the target models. For example, IPGuard [Cao et al., 2019] identified the data samples close to the target model's decision boundary to fingerprint this model. [Lukas et al., 2019] adopted conferrable adversarial examples with certain labels, which can transfer from the target model to its surrogates while remaining ineffective to other non-target models. [He et al., 2019] designed sensitive samples to fingerprint black-box models, which will fail even the model has very small modifications.\n\nFigure 1 :\n1Overview of our watermarking technique shown in Eq. 7. min f (L u + \u03b7 \u00b7 L p )\n\nFigure 2 :\n2Examples of clean and triggered samples.\n\nTable 2 .\n2Pre-train Downstream \nModel WACC (%) \nmethod \ndataset \nClean model Watermarked model \nSTL10 \n22.09 \n91.76 \nSimCLR \nGTSRB \n37.43 \n93.37 \nSVHN \n54.94 \n80.13 \nSTL10 \n7.01 \n90.51 \nMoCoV2 \nGTSRB \n12.47 \n93.92 \nSVHN \n4.52 \n84.37 \n\nTable 1: WACC of clean models and watermarked models. \n\nPre-train Downstream \nModel WACC (%) \nmethod \ndataset \nWrong trigger Correct trigger \nSTL10 \n8.34 \n91.76 \nSimCLR \nGTSRB \n22.32 \n93.37 \nSVHN \n41.90 \n81.90 \nSTL10 \n8.17 \n90.51 \nMoCoV2 \nGTSRB \n9.33 \n93.92 \nSVHN \n2.17 \n84.37 \n\n\n\nTable 2 :\n2WACC of different triggers on the watermarked models.\n\nTable 3 :\n3Robustness evaluation against model pruning on SimCLR.Fine-tune \nmethod \n\nDownstream \ndataset \nACC (%) WACC (%) \n\nSTL10 \n80.74 \n83.15 \nFTAL \nGTSRB \n63.15 \n81.87 \nSVHN \n94.34 \n87.04 \nSTL10 \n76.27 \n91.76 \nRTLL \nGTSRB \n82.43 \n93.92 \nSVHN \n66.82 \n84.37 \n\n\n\nTable 4 :\n4Robustness evaluation against fine-tuning.\n\n\n.Pre-train Downstream \nModel ACC (%) \nmethod \nDataset \nClean Watermarked \nSTL10 \n76.14 \n76.11 \nSimCLR \nGTSRB \n81.40 \n82.43 \nSVHN \n66.82 \n66.20 \nSTL10 \n89.16 \n90.68 \nMoCoV2 \nGTSRB \n75.96 \n76.12 \nSVHN \n79.41 \n77.31 \n\n\n\nTable 5 :\n5Functionality evaluation results by comparing the ACC of clean model with the watermarked model.\n\n\nadopted adversarial examples near the frontiers as watermarks to identify the ownership of DNN models. Zhang et al. [Zhang et al., 2018] and Adi et al. [Adi et al., 2018] employed backdoor attack techniques to embed backdoor samples with certain trigger patterns into DNN models. [Namba and Sakuma, 2019] and\nhttps://twimlai.com/solutions/features/model-marketplace/ arXiv:2201.08217v1 [cs.CV] 20 Jan 2022\nInstead of training the MocoV2 encoder from scratch, we use the model released by the authors directly at https://github.com/ facebookresearch/moco\nConclusion and Future WorkIn this paper, we propose a novel watermarking technique to protect the IP of pre-trained encoders via contrastive learning. We introduce a new loss function, which can effectively embed the watermark into the encoder without the knowledge of the downstream tasks and datasets. The watermark can be transferred to any downstream model built from this encoder. We perform extensive evaluations to demonstrate our watermarking methodology has high uniqueness, functionalitypreserving, and robustness.We have the following directions as future work. (1) We aim to develop an algorithm to optimize our trigger pattern, to reduce the false positives of the watermark verification.(2)We can co-optimize the trigger and encoder to further enhance the uniqueness of the watermark. (3) We will design more effective attack methods to detect or remove any watermarks in a pre-trained encoder.\nTurning your weakness into a strength: Watermarking deep neural networks by backdooring. [ References, Adi, arXiv:2106.09667Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. IPGuard: Protecting the intellectual property of deep neural networks via fingerprinting the classification boundary. CoRR, abs/1910.12903. arXiv preprintNicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. Caron et al., 2020. Unsupervised learning of visual features by contrasting cluster assignments. 2020References [Adi et al., 2018] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your weakness into a strength: Watermarking deep neural net- works by backdooring. In USENIX Security Symposium, 2018. [Cao et al., 2019] Xiaoyu Cao, Jinyuan Jia, and Neil Zhen- qiang Gong. IPGuard: Protecting the intellectual property of deep neural networks via fingerprinting the classifica- tion boundary. CoRR, abs/1910.12903, 2019. [Carlini and Terzis, 2021] Nicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. arXiv preprint arXiv:2106.09667, 2021. [Caron et al., 2020] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by con- trasting cluster assignments. 2020.\n\nMohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. Xinlei He, Kaiming Chen, ; He, Chen, arXiv:2003.04297Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. Chen et al., 2020b] Xinlei ChenarXiv preprintProc. of the AAMASand He, 2020] Xinlei Chen and Kaiming He. Explor- ing simple siamese representation learning, 2020. [Chen et al., 2020a] Ting Chen, Simon Kornblith, Moham- mad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Inter- national conference on machine learning, 2020. [Chen et al., 2020b] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. [Chen et al., 2021] Kangjie Chen, Shangwei Guo, Tianwei Zhang, Shuxin Li, and Yang Liu. Temporal watermarks for deep reinforcement learning models. In Proc. of the AAMAS, 2021.\n\nRethinking deep neural network ownership verification: Embedding passports to defeat ambiguity attacks. et al., 2019] Lixin Fan, Kam Woh Ng, and Chee Seng Chan. Rethinking deep neural network ownership verifi- cation: Embedding passports to defeat ambiguity attacks. 2019.\n\nBootstrap your own latent: A new approach to selfsupervised learning. IEEE Access. Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg7Badnets: Evaluating backdooring attacks on deep neural networkset al., 2020] Jean-Bastien Grill, Florian Strub, Flo- rent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao- han Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self- supervised learning, 2020. [Gu et al., 2019] Tianyu Gu, Kang Liu, Brendan Dolan- Gavitt, and Siddharth Garg. Badnets: Evaluating back- dooring attacks on deep neural networks. IEEE Access, 7:47230-47244, 2019.\n\nFine-tuning is not enough: A simple yet effective watermark removal attack for dnn models. International Joint Conference on Artificial Intelligence (IJCAI). Zecheng He, Tianwei Zhang, and Ruby LeeIEEE Symposium on Security and Privacyet al., 2021] Shangwei Guo, Tianwei Zhang, Han Qiu, Yi Zeng, Tao Xiang, and Yang Liu. Fine-tuning is not enough: A simple yet effective watermark removal attack for dnn models. In International Joint Conference on Ar- tificial Intelligence (IJCAI), 2021. [He et al., 2019] Zecheng He, Tianwei Zhang, and Ruby Lee. Sensitive-sample fingerprinting of deep neural net- works. In Proc. of the CVPR, pages 4729-4737, 2019. [Jia et al., 2022] Jinyuan Jia, Yupei Liu, and Neil Zhenqiang Gong. Badencoder: Backdoor attacks to pre-trained en- coders in self-supervised learning. In IEEE Symposium on Security and Privacy, 2022.\n\nAdversarial frontier stitching for remote neural network watermarking. Le Merrer, Neural Computing and Applications. Le Merrer et al., 2019] Erwan Le Merrer, Patrick Perez, and Gilles Tr\u00e9dan. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Ap- plications, pages 1-12, 2019.\n\nWhen nas meets watermarking: Ownership verification of dnn models via cache side channels. CoRR, abs/2102.03523, 2021. arXiv:2103.00020arXiv:2103.03230How to prove your model belongs to you: A blind-watermark based framework to protect intellectual property of DNN. In ACSAC. Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang34Namba and SakumaarXiv preprintIEEE European Symposium on Security and Privacyet al., 2019] Zheng Li, Chengyu Hu, Yang Zhang, and Shanqing Guo. How to prove your model belongs to you: A blind-watermark based framework to protect intellectual property of DNN. In ACSAC, 2019. [Lukas et al., 2019] Nils Lukas, Yuxuan Zhang, and Florian Kerschbaum. Deep neural network fingerprinting by con- ferrable adversarial examples. CoRR, abs/1912.00888, 2019. [Namba and Sakuma, 2019] Ryota Namba and Jun Sakuma. Robust watermarking of neural network with exponential weighting. In ACM AsiaCCS, 2019. [Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar- wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual mod- els from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. [Rouhani et al., 2019] Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. DeepSigns: An end-to-end water- marking framework for protecting the ownership of deep neural networks. In ACM ASPLOS, 2019. [Uchida et al., 2017] Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin'ichi Satoh. Embedding watermarks into deep neural networks. In ACM on International Con- ference on Multimedia Retrieval, pages 269-277, 2017. [Xiaoxuan et al., 2021] Lou Xiaoxuan, Guo Shangwei, Zhang Tianwei, Liu Yang, et al. When nas meets water- marking: Ownership verification of dnn models via cache side channels. CoRR, abs/2102.03523, 2021. [Zbontar et al., 2021] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self- supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021. [Zhang et al., 2018] Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, and Ian Molloy. Protecting intellectual property of deep neu- ral networks with watermarking. In Asia Conference on Computer and Communications Security, 2018. [Zhang et al., 2020] Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Weiming Zhang, Wenbo Zhou, Hao Cui, and Nenghai Yu. Model watermarking for image processing networks. In Proceedings of the AAAI Conference on Arti- ficial Intelligence, volume 34, pages 12805-12812, 2020. [Zhang et al., 2021] Xinyang Zhang, Zheng Zhang, Shoul- ing Ji, and Ting Wang. Trojaning language models for fun and profit. In IEEE European Symposium on Security and Privacy, 2021.\n", "annotations": {"author": "[{\"end\":98,\"start\":61},{\"end\":157,\"start\":99},{\"end\":241,\"start\":158},{\"end\":311,\"start\":242},{\"end\":376,\"start\":312}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":68},{\"end\":106,\"start\":103},{\"end\":171,\"start\":166},{\"end\":250,\"start\":248},{\"end\":323,\"start\":320}]", "author_first_name": "[{\"end\":67,\"start\":61},{\"end\":102,\"start\":99},{\"end\":165,\"start\":158},{\"end\":247,\"start\":242},{\"end\":319,\"start\":312}]", "author_affiliation": "[{\"end\":97,\"start\":72},{\"end\":156,\"start\":131},{\"end\":240,\"start\":198},{\"end\":310,\"start\":274},{\"end\":375,\"start\":346}]", "title": "[{\"end\":58,\"start\":1},{\"end\":434,\"start\":377}]", "venue": null, "abstract": "[{\"end\":1526,\"start\":436}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1583,\"start\":1563},{\"end\":1602,\"start\":1583},{\"end\":1621,\"start\":1602},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1639,\"start\":1621},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1658,\"start\":1639},{\"end\":1679,\"start\":1658},{\"end\":2119,\"start\":2093},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2273,\"start\":2253},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3390,\"start\":3372},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3409,\"start\":3390},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3426,\"start\":3409},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5940,\"start\":5920},{\"end\":5972,\"start\":5952},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7249,\"start\":7229},{\"end\":8576,\"start\":8556},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13152,\"start\":13134},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13171,\"start\":13152},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13354,\"start\":13336},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15550,\"start\":15532},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18922,\"start\":18902},{\"end\":18954,\"start\":18934},{\"end\":20756,\"start\":20739},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21731,\"start\":21713},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21750,\"start\":21731},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26067,\"start\":26045},{\"end\":26517,\"start\":26496},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26698,\"start\":26676},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27296,\"start\":27272},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27314,\"start\":27297},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27534,\"start\":27511},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28044,\"start\":28026},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28166,\"start\":28146},{\"end\":28361,\"start\":28344}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28573,\"start\":28483},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28627,\"start\":28574},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29144,\"start\":28628},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29210,\"start\":29145},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29474,\"start\":29211},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":29529,\"start\":29475},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":29747,\"start\":29530},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":29856,\"start\":29748},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":30167,\"start\":29857}]", "paragraph": "[{\"end\":3296,\"start\":1542},{\"end\":4506,\"start\":3298},{\"end\":5751,\"start\":4508},{\"end\":6370,\"start\":5753},{\"end\":7732,\"start\":6408},{\"end\":8547,\"start\":7816},{\"end\":9107,\"start\":8549},{\"end\":9766,\"start\":9161},{\"end\":10024,\"start\":9801},{\"end\":11194,\"start\":10041},{\"end\":11481,\"start\":11196},{\"end\":12379,\"start\":11483},{\"end\":13032,\"start\":12381},{\"end\":13172,\"start\":13059},{\"end\":13696,\"start\":13174},{\"end\":14448,\"start\":13698},{\"end\":14834,\"start\":14472},{\"end\":15049,\"start\":14836},{\"end\":15803,\"start\":15083},{\"end\":16035,\"start\":15853},{\"end\":16665,\"start\":16037},{\"end\":16824,\"start\":16715},{\"end\":17724,\"start\":16826},{\"end\":18493,\"start\":17751},{\"end\":18619,\"start\":18542},{\"end\":19120,\"start\":18634},{\"end\":19251,\"start\":19143},{\"end\":21751,\"start\":19253},{\"end\":23407,\"start\":21753},{\"end\":24434,\"start\":23431},{\"end\":24660,\"start\":24458},{\"end\":25463,\"start\":24662},{\"end\":26206,\"start\":25499},{\"end\":27611,\"start\":26224},{\"end\":28482,\"start\":27613}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7815,\"start\":7733},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9160,\"start\":9108},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9800,\"start\":9767},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15082,\"start\":15050},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15852,\"start\":15804},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16714,\"start\":16666},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18541,\"start\":18494}]", "table_ref": "[{\"end\":23865,\"start\":23858},{\"end\":24024,\"start\":24017},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24659,\"start\":24640},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24674,\"start\":24667},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25255,\"start\":25248},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25646,\"start\":25639},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25783,\"start\":25776}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1540,\"start\":1528},{\"attributes\":{\"n\":\"2\"},\"end\":6383,\"start\":6373},{\"attributes\":{\"n\":\"2.1\"},\"end\":6406,\"start\":6386},{\"attributes\":{\"n\":\"2.2\"},\"end\":10039,\"start\":10027},{\"attributes\":{\"n\":\"3\"},\"end\":13046,\"start\":13035},{\"attributes\":{\"n\":\"3.1\"},\"end\":13057,\"start\":13049},{\"attributes\":{\"n\":\"3.2\"},\"end\":14470,\"start\":14451},{\"attributes\":{\"n\":\"3.3\"},\"end\":17749,\"start\":17727},{\"attributes\":{\"n\":\"4\"},\"end\":18632,\"start\":18622},{\"attributes\":{\"n\":\"4.1\"},\"end\":19141,\"start\":19123},{\"attributes\":{\"n\":\"4.2\"},\"end\":23429,\"start\":23410},{\"attributes\":{\"n\":\"4.3\"},\"end\":24456,\"start\":24437},{\"attributes\":{\"n\":\"4.4\"},\"end\":25497,\"start\":25466},{\"attributes\":{\"n\":\"5\"},\"end\":26222,\"start\":26209},{\"end\":28494,\"start\":28484},{\"end\":28585,\"start\":28575},{\"end\":28638,\"start\":28629},{\"end\":29155,\"start\":29146},{\"end\":29221,\"start\":29212},{\"end\":29485,\"start\":29476},{\"end\":29758,\"start\":29749}]", "table": "[{\"end\":29144,\"start\":28640},{\"end\":29474,\"start\":29277},{\"end\":29747,\"start\":29533}]", "figure_caption": "[{\"end\":28573,\"start\":28496},{\"end\":28627,\"start\":28587},{\"end\":29210,\"start\":29157},{\"end\":29277,\"start\":29223},{\"end\":29529,\"start\":29487},{\"end\":29533,\"start\":29532},{\"end\":29856,\"start\":29760},{\"end\":30167,\"start\":29859}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13797,\"start\":13789},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20882,\"start\":20874}]", "bib_author_first_name": "[{\"end\":31412,\"start\":31411},{\"end\":32755,\"start\":32749},{\"end\":32767,\"start\":32760},{\"end\":32775,\"start\":32774},{\"end\":35558,\"start\":35556}]", "bib_author_last_name": "[{\"end\":31423,\"start\":31413},{\"end\":31428,\"start\":31425},{\"end\":32758,\"start\":32756},{\"end\":32772,\"start\":32768},{\"end\":32778,\"start\":32776},{\"end\":32784,\"start\":32780},{\"end\":35565,\"start\":35559}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2106.09667\",\"id\":\"b0\",\"matched_paper_id\":3322503},\"end\":32637,\"start\":31322},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b1\",\"matched_paper_id\":211096730},\"end\":33626,\"start\":32639},{\"attributes\":{\"id\":\"b2\"},\"end\":33900,\"start\":33628},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":219687798},\"end\":34628,\"start\":33902},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":234767630},\"end\":35483,\"start\":34630},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11008755},\"end\":35802,\"start\":35485},{\"attributes\":{\"doi\":\"arXiv:2103.00020\",\"id\":\"b6\",\"matched_paper_id\":231846675},\"end\":38562,\"start\":35804}]", "bib_title": "[{\"end\":31409,\"start\":31322},{\"end\":32747,\"start\":32639},{\"end\":33970,\"start\":33902},{\"end\":34719,\"start\":34630},{\"end\":35554,\"start\":35485},{\"end\":35921,\"start\":35804}]", "bib_author": "[{\"end\":31425,\"start\":31411},{\"end\":31430,\"start\":31425},{\"end\":32760,\"start\":32749},{\"end\":32774,\"start\":32760},{\"end\":32780,\"start\":32774},{\"end\":32786,\"start\":32780},{\"end\":35567,\"start\":35556}]", "bib_venue": "[{\"end\":31634,\"start\":31446},{\"end\":32897,\"start\":32802},{\"end\":33730,\"start\":33628},{\"end\":33983,\"start\":33972},{\"end\":34786,\"start\":34721},{\"end\":35600,\"start\":35567},{\"end\":36078,\"start\":35955}]"}}}, "year": 2023, "month": 12, "day": 17}