{"id": 252780150, "updated": "2023-02-01 10:10:28.422", "metadata": {"title": "Regularizing Score-based Models with Score Fokker-Planck Equations", "authors": "[{\"first\":\"Chieh-Hsin\",\"last\":\"Lai\",\"middle\":[]},{\"first\":\"Yuhta\",\"last\":\"Takida\",\"middle\":[]},{\"first\":\"Naoki\",\"last\":\"Murata\",\"middle\":[]},{\"first\":\"Toshimitsu\",\"last\":\"Uesaka\",\"middle\":[]},{\"first\":\"Yuki\",\"last\":\"Mitsufuji\",\"middle\":[]},{\"first\":\"Stefano\",\"last\":\"Ermon\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Score-based generative models learn a family of noise-conditional score functions corresponding to the data density perturbed with increasingly large amounts of noise. These perturbed data densities are tied together by the Fokker-Planck equation (FPE), a PDE governing the spatial-temporal evolution of a density undergoing a diffusion process. In this work, we derive a corresponding equation characterizing the noise-conditional scores of the perturbed data densities (i.e., their gradients), termed the score FPE . Surprisingly, despite impressive empirical performance, we observe that scores learned via denoising score matching (DSM) do not satisfy the underlying score FPE. We mathematically analyze three implications of satisfying the score FPE and a potential explanation for why the score FPE is not ful\ufb01lled in practice. At last, we propose to regularize the DSM objective to enforce satisfaction of the score FPE, and show its effectiveness on synthetic data and MNIST. Kaw+22; Che+22; Sai+22]. SGMs involve a forward and a backward process. In the forward process", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2210-04296", "doi": "10.48550/arxiv.2210.04296"}}, "content": {"source": {"pdf_hash": "9bba5b9a742eaffebd55c729577beb4137444980", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.04296v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "570fbffe1e388089a14f2810a371ae8b61cc01b8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9bba5b9a742eaffebd55c729577beb4137444980.txt", "contents": "\nRegularizing Score-based Models with Score Fokker-Planck Equations\n\n\nChieh-Hsin Lai chieh-hsin.lai@sony.com \nSony Group Corporation Tokyo\n108-007Japan\n\nYuhta Takida yuta.takida@sony.com \nSony Group Corporation Tokyo\n108-007Japan\n\nNaoki Murata naoki.murata@sony.com \nSony Group Corporation Tokyo\n108-007Japan\n\nToshimitsu Uesaka toshimitsu.uesaka@sony.com \nSony Group Corporation Tokyo\n108-007Japan\n\nYuki Mitsufuji yuhki.mitsufuji@sony.com \nSony Group Corporation Tokyo\n108-007Japan\n\nStefano Ermon ermon@cs.stanford.edu \nStanford University Stanford\n94305CA\n\nRegularizing Score-based Models with Score Fokker-Planck Equations\n\nScore-based generative models learn a family of noise-conditional score functions corresponding to the data density perturbed with increasingly large amounts of noise. These perturbed data densities are tied together by the Fokker-Planck equation (FPE), a PDE governing the spatial-temporal evolution of a density undergoing a diffusion process. In this work, we derive a corresponding equation characterizing the noise-conditional scores of the perturbed data densities (i.e., their gradients), termed the score FPE. Surprisingly, despite impressive empirical performance, we observe that scores learned via denoising score matching (DSM) do not satisfy the underlying score FPE. We mathematically analyze three implications of satisfying the score FPE and a potential explanation for why the score FPE is not fulfilled in practice. At last, we propose to regularize the DSM objective to enforce satisfaction of the score FPE, and show its effectiveness on synthetic data and MNIST.\n\nIntroduction\n\nScore-based generative models (SGM) [Soh+15; SE19; HJA20; Son+20a; Son+20b], also referred to as diffusion models, have led to major advances in the generation of synthetic images [DN21;Sah+22;Rom+22], audio [Kon+20] and in various other downstream applications [Men+21b; Nie+22; Kaw+22; Che+22; Sai+22]. SGMs involve a forward and a backward process. In the forward process (diffusion process) increasing amounts of noise are gradually added to each data point until the original structure is lost, transforming data into pure noise. The backward process attempts to reverse the forward process, using a neural network (called a noise-conditional score model) trained to gradually remove noise, effectively transforming pure noise into clean data samples. The (noiseconditional) score models are trained with a denoising score-matching objective [HD05; Vin11; SE19; HJA20] to estimate the score (gradient of the log-likelihood function) of the data density perturbed with various amounts of noise (as in forward process).\n\nWe can interpret the training procedure of diffusion models as jointly estimating the score of the original data density and all its perturbations. Crucially, all these densities are closely related to each other as they correspond to the same data density perturbed with various amounts of noise. With sufficiently small time steps, the forward process is a diffusion [Son+20a] and the spatial-temporal evolution of the data density is thus governed by the classic Fokker-Planck partial differential equation (PDE) [\u00d8ks03]. This implies that knowing the density for a single noise level we could in principle recover all the others by solving the Fokker-Planck without any additional learning. Building on this, we derive an associated system of PDEs characterizing the evolution of the scores (i.e., gradients) of the perturbed data densities, which we term score Fokker-Planck equation (score FPE). The ground truth scores of the perturbed data densities must in theory satisfy the score FPE.\n\nWe mathematically study the implications of satisfying the score FPE. We prove that reducing the score FPE error: (a) improves log-likelihood of the probability flow ODE diffusion mode [Son+20a], (b) improves the degree of conservativity of the models, and (c) can be achieved by enforcing higherorder score matching [Men+21a;Lu+22]. In practice, we observe that many existing pre-trained score models do not numerically satisfy the score FPE. We therefore propose a new loss function for training diffusion models combining the traditional score matching objective with a regularization term enforcing the score FPE. We show that this enables more accurate density estimation on synthetic data and it can improve the likelihood on MNIST.\n\n\nBackground\n\n[Son+20a] unifies denoising score matching [SE19] and diffusion probabilistic models [Soh+15; HJA20] via a stochastic process x(t) of continuous time t \u2208 [0, T ] driven by the forward SDE\ndx(t) = f (x(t), t)dt + g(t)dw t ,\n(\n\nwhere f (\u00b7, t) : R D \u2192 R D , g(\u00b7) : R \u2192 R and w t is a standard Wiener process. Under some moderate conditions [And82], one can obtain a reverse time SDE from T to 0\ndx(t) = [f (x(t), t) \u2212 g 2 (t)\u2207 x log q t (x(t))]dt + g(t)dw t ,(2)\nwherew t is a standard Wiener process in reverse time. Let q t (x) denote the ground truth marginal density of x(t) following Eq. (1). We can train a time-conditional neural network s \u03b8 = s \u03b8 (x, t) to approximate \u2207 x log q t (x) by minimizing the score matching objective [HD05] J SM (\u03b8; \u03bb(\u00b7)) := 1 2\nT 0 \u03bb(t)E x\u223cqt(x) s \u03b8 (x, t) \u2212 \u2207 x log q t (x) 2 2 dt.(3)\nSince q t (x) is generally inaccessible, the denoising score matching (DSM) loss [Vin11; Son+20a] is exploited in practice instead:\nJ DSM (\u03b8; \u03bb(\u00b7)) := 1 2 T 0 \u03bb(t)E x(0) E q0t(x(t)|x(0)) s \u03b8 (x(t), t) \u2212 \u2207 x(t) log q 0t (x(t)|x(0)) 2 2 dt, (4) where q 0t (x(t)|x(0)) is the transition kernel from x(0) to x(t). After s \u03b8 (x, t) \u2248 \u2207 x log q t (x) is learned, we replace \u2207 x log q t (x) in Eq.\n(2) with s \u03b8 and get a parametrized reverse-time SDE for stochastic processx \u03b8 (t)\ndx \u03b8 (t) = [f (x \u03b8 (t), t) \u2212 g 2 (t)s \u03b8 (x \u03b8 (t), t)]dt + g(t)w t ,(5)\nLet p SDE t,\u03b8 denote the marginal distribution ofx \u03b8 (t) with the initial distribution defined as the prior \u03c0, where suppress the dependency on \u03c0 for compactness. We can design f and g in Eq.\n\n(2) so that q T (x) approximates a simple prior \u03c0, and hence, can generate samplesx \u03b8 (0) \u223c p SDE 0,\u03b8 by numerically solving Eq. (5) backward with an initial sample from the priorx \u03b8 (T ) \u223c \u03c0. Intuitively,x \u03b8 (0) should be close to a sample from the data distribution.\n\n[Son+20a] further introduces a deterministic process (with zero diffusion term) describing the evolution of samples whose trajectories share the same marginal probability densities as the forward SDE (Eq. (5)). Specifically, the process evolves through time according to the following probability flow Ordinary Differential Equation (ODE):\ndx dt = f (x, t) \u2212 1 2 g 2 (t)\u2207 x log q t (x).(6)\nAs in the SDE case, the ground truth score \u2207 x log q t (x) is approximated with the learned score model s \u03b8 (x, t) \u2248 \u2207 x log q t (x) in Eq. (6), leading to the parameterized probability flow ODE\ndx \u03b8 dt = f (x \u03b8 , t) \u2212 1 2 g 2 (t)s \u03b8 (x \u03b8 , t)(7)\nWe denote the marginal density ofx \u03b8 as p ODE t,\u03b8 with initial condition sampled from the prior \u03c0, where the dependency on \u03c0 is also omitted in the notation for compactness. By solving Eq. (7) backward with an initial valuex \u03b8 (T ) \u223c \u03c0 via numerical methods, we can generate a samplex \u03b8 (0) \u223c p ODE 0,\u03b8 to approximate sampling from the data distribution. Indeed, due to the deterministic dynamics in Eq. (7), it is possible to compute exact likelihoods for this generative model. Letx \u03b8 (t) \u2208 R D evolve reversely through time following Eq. (7) starting withx \u03b8 (T ) \u223c \u03c0. The \"Instantaneous Change of Variables\" [Che+18] characterizes the temporal change of log p ODE t,\u03b8 along the trajectory\nx \u03b8 (t) : t \u2208 [0, T ] via an ODE d log p ODE t,\u03b8 (x \u03b8 (t)) dt = 1 2 g 2 (t)div x s \u03b8 (x \u03b8 (t), t) \u2212 div x f (x \u03b8 (t), t) .\nTherefore, the log-likelihood can be exactly calculated by numerically solving the concatenated ODEs backward from T to 0, initialized withx \u03b8 (T ) \u223c \u03c0 d dt\nx \u03b8 (t) log p ODE t,\u03b8 (x \u03b8 (t)) = f (x \u03b8 (t), t) \u2212 1 2 g 2 (t)s \u03b8 (x \u03b8 (t), t) 1 2 g 2 (t)div x s \u03b8 (x \u03b8 (t), t) \u2212 div x f (x \u03b8 (t), t)\n.\n\n\nThe Fokker-Planck equation for score vector fields in diffusions\n\nIt is well known that the evolution of the ground truth density q t (x) associated to Eq. (1) is governed by the Fokker-Planck equation (FPE) [\u00d8ks03] expressed in Eq. (8) (more details in Appx. E).\n\u2202 t q t (x) = \u2212 D j=1 \u2202 xj F j (x, t)q t (x) ,(8)\nwhereF\n(x, t) := f (x, t) \u2212 1 2 g 2 (t)\u2207 x log q t (x).\nAs there is a one-to-one mapping between densities and their scores, we can derive an equivalent system of PDEs that the ground truth scores \u2207 x log q t (x) must satisfy. We call it as a score Fokker-Planck equation, for short score FPE. Corollary 1 (score FPE). Assume that the ground truth density q t (x) is sufficiently smooth for (x, t) \u2208 R D \u00d7 [0, T ]. Then its score s(x, t) := \u2207 x log q t (x) satisfies the following system of PDEs\n\u2202 t s \u2212 \u2207 x 1 2 g 2 (t)div x (s) + 1 2 g 2 (t) s 2 2 \u2212 f , s \u2212 div x (f ) = 0, (x, t) \u2208 R D \u00d7 [0, T ]. (9)\nThis result shows that the time-conditional scores s \u03b8 (x, t) learned by score-based models (via Eq. (4)) are highly redundant. In principle, given a ground truth score at an initial time t 0 , we can theoretically recover scores for all times t \u2265 t 0 by solving the score FPE. We explain its intuition by considering a special case when f \u2261 0 and g \u2261 1. That is, x(t) is obtained by adding Gaussian noise. It is well-known that the densities q t and q t0 are related in a convolutional way as q t = q t0 * N (0, t), and q t can be analytically obtained from q t0 [MR92] (e.g., by applying a Fourier transform and dividing). Hence, all scores can in principle be obtained analytically from the score at a single time-step, without any further learning. We empirically support this idea in Appx. B with synthetic data whose ground truth density has a closed form expression.\n\nTheoretically, with sufficient data and model capacity, (denoising) score matching ensures the optimal solution to Eq. (4) should satisfy Eq. (9) as it approximates the ground truth score well. However, we observe that pre-trained s \u03b8 learned via Eq. (4) do not numerically satisfy the score FPE. We hereby introduce an error term s\u03b8 = s\u03b8 (x, t) in order to quantify how s \u03b8 deviates from the score FPE\ns\u03b8 (x, t) := \u2202 t s \u03b8 \u2212 \u2207 x 1 2 g 2 (t)div x (s \u03b8 ) + 1 2 g 2 (t) s \u03b8 2 2 \u2212 f , s \u03b8 \u2212 div x (f ) .(10)\nWe further define the following averaged residuals of DSM and the score FPE for t \u2208 [0, 1]:  \nr DSM-like (t; s \u03b8 ) := 1 D E x(0) E x(t)|x(0) s \u03b8 (x(t), t) \u2212 \u2207 x(t) log q 0t (x(t)|x(0)) 2 r FP (t; s \u03b8 ) := 1 D E x\u223c\u03bd s\u03b8 (x, t) 2 , \u03bd \u223c Uniform [0, 1] D or \u03bd \u223c q t (x(t)|x(0)).\n\nTheoretical implications and interpretations of score FPE\n\nIn this section, we first study three implications of satisfying the score FPE. More precisely, we first show in Sec. 4.1 that simultaneously minimizing the averaged error of the score FPE and score matching objective can reduce the KL divergence between q 0 and p ODE 0,\u03b8 , the data density and the one determined by parametrized probability flow ODE (Eq. (7)), respectively. In Sec. 4.2 we prove that controlling s\u03b8 can implicitly enforce conservativity of s \u03b8 . Moreover, if the score FPE is satisfied, we prove in Sec. 4.3 the equivalence of s \u03b8 , ground truth score s and \u2207 x log p SDE t,\u03b8 holds under some conditions, where p SDE t,\u03b8 is defined in Sec. 2 as the marginal density of parametrized diffusion process. In Sec. 4.4, we investigate the connection between higher-order score matching [Men+21a;Lu+22] and the score FPE.\n\n\nMinimizing\nD KL q 0 p ODE 0,\u03b8\nFirst of all, we consider the averaged error of the score FPE defined as follows\nM (\u03b8) := sup t\u2208[0,T ] E x\u223cqt(x) T 0 s \u03b8 (x, \u03c4 ) 2 d\u03c4 < \u221e.(11)\nIn this section, we show that simultaneously minimizing M (\u03b8) and J SM (\u03b8) enables us to decrease the KL divergence between q 0 and p ODE 0,\u03b8 , denoted as D KL q 0 p ODE 0,\u03b8 and, equivalently, improve the likelihood of the data under p ODE 0,\u03b8 . Next, we introduce some technical conditions in Assumption A that are commonly used in theoretical studies of score-based models [Son+21; Lu+22;Pid22]. For notational simplicity, let s ODE \u03b8 (\u00b7, t) denote \u2207 x log p ODE t,\u03b8 , the time-conditional score function of the probability density p ODE . Assumption A. We assume there are finite constants L > 0 and \u03b4 T > 0 such that the following conditions hold (a) Bounded 2 nd -non-central moments:\nE x\u223cq0(x) [ x 2 2 ] \u2264 L; (b) s \u03b8 (x, t) 2 \u2264 L(1 + x 2 ), for all x \u2208 R D and t \u2208 [0, T ]; (c) s \u03b8 (x, t) \u2212 s \u03b8 (y, t) 2 \u2264 L x \u2212 y 2 , for all x, y \u2208 R D and t \u2208 [0, T ]; (d) f (x, t) 2 \u2264 L(1 + x 2 ), for all x \u2208 R D and t \u2208 [0, T ]; (e) f (x, t) \u2212 f (y, t) 2 \u2264 L x \u2212 y 2 , for all x, y \u2208 R D and t \u2208 [0, T ]; (f) s ODE \u03b8 (x, t) \u2212 s ODE \u03b8 (y, t) 2 \u2264 L x \u2212 y 2 , for all x, y \u2208 R D and t \u2208 [0, T ]; (g) sup t\u2208[0,T ] E x\u223cqt(x) s \u03b8 (x, T ) \u2212 s ODE \u03b8 (x, T ) 2 2 \u2264 \u03b4 2 T , or sup x\u2208R D s \u03b8 (x, T ) \u2212 s ODE \u03b8 (x, T ) 2 2 \u2264 \u03b4 2 T ; (h) For all t \u2208 [0, T ], there is a k > 0 so that q t (x) = O e \u2212 x k 2 and p ODE t (x) = O e \u2212 x k 2 as x 2 \u2192 \u221e\nThen, we review a crucial equation proposed by [Lu+22] which quantifies the exact gap between the KL divergence D KL q 0 p ODE 0,\u03b8 and the score matching objective J SM (\u03b8).\n\nLemma 1 ( [Lu+22]). Set \u03bb(t) = g 2 (t). Let q 0 be the data distribution, q t be the marginal density of x(t) following Eq. (1) and p ODE 0,\u03b8 be the marginal density determined by probability flow ODE (Eq. (7)). Assume Assumption A.(h) is satisfied. Then\nD KL q 0 p ODE 0,\u03b8 = D KL q T p ODE T,\u03b8 + J SM (\u03b8) + J Diff (\u03b8),(12)\nwhere\nJ Diff (\u03b8) = 1 2 T 0 g 2 (t)E x\u223cqt(x) s \u03b8 (x, t) \u2212 \u2207 x log q t (x) s ODE \u03b8 (x, t) \u2212 s \u03b8 (x, t) dt.\nWe now discuss the main proposition in this section. We notice that applying the Cauchy-Schwartz inequality to J Diff (\u03b8) leads to an upper bound on J Diff (\u03b8)\n|J Diff (\u03b8)| \u2264 J SM (\u03b8) \u00b7 J Fisher (\u03b8),\nwhere J Fisher (\u03b8) is a Fisher-like divergence in terms of the two scores s \u03b8 (x, t) and s ODE \u03b8 (x, t), and is defined as\nJ Fisher (\u03b8) := 1 2 T 0 g 2 (t)E x\u223cqt(x) s \u03b8 (x, t) \u2212 s ODE \u03b8 (x, t) 2 2 dt.(13)\nIn Theorem. 1, we will show that we can further bound J Fisher (\u03b8) above by a decreasing function in terms of M (\u03b8):\nJ Fisher (\u03b8) M (\u03b8) + M (\u03b8) + 1.(14)\nTherefore, Eq. (12) together with Ineq. (13) and 14 imply that we can reduce D KL q 0 p ODE 0,\u03b8 once M (\u03b8) and J SM (\u03b8) are minimized simultaneously. We now rigorously state the theorem. Theorem 1. We have\nJ Diff (\u03b8) 2 \u2264 J SM (\u03b8) \u00b7 J Fisher (\u03b8).(15)\nMoreover, if Assumption. A is fulfilled, then there is a finite constant C := C(L, T, g, \u03b4 T ) > 0, depending only on L, T , g, and \u03b4 T so that we can further bound the Ineq. (15) above as\nJ Diff (\u03b8) 2 \u2264 C 2 (L, T, g, \u03b4 T ) \u00b7 J SM (\u03b8) M (\u03b8) + M (\u03b8) + 1 .(16)\nHence,\nD KL q 0 p ODE 0,\u03b8 \u2264 D KL q T p ODE T,\u03b8 + J SM (\u03b8) + C \u00b7 J 1/2 SM (\u03b8) M (\u03b8) + M (\u03b8) + 1 1/2 .\n\nConservativity\n\nThe ground truth score s(x, t) = \u2207 x log q t (x) is a conservative vector field. That is, it can be expressed as a gradient of some real-valued function. However, scores learned in practice do not satisfy this property [SH21]. Below we prove that we can implicitly enforce conservativity by minimizing the time-averaged residual s\u03b8 (x, \u03c4 ) of the score FPE.\nProposition 1. If there is a t \u03b8 \u2208 [0, T ] so that s \u03b8 (x, t \u03b8 ) = \u2207 x log q t \u03b8 (x) for all x \u2208 R D , then there is a real-valued function \u03a8 \u03b8 : R D \u00d7 [0, T ] \u2192 R given by \u03a8 \u03b8 (x, t) = log q t \u03b8 (x) + t t \u03b8 1 2 g 2 (\u03c4 )div x (s \u03b8 ) + 1 2 g 2 (\u03c4 ) s \u03b8 2 2 \u2212 f , s \u03b8 \u2212 div x (f ) d\u03c4 so that for all (x, t) \u2208 R D \u00d7 [0, T ] s \u03b8 (x, t) \u2212 \u2207 x \u03a8 \u03b8 (x, t) = t t \u03b8 s\u03b8 (x, \u03c4 )d\u03c4.(17)\nIn particular,\ns \u03b8 (x, t) \u2212 \u2207 x \u03a8 \u03b8 (x, t) 2 \u2264 max{t \u03b8 ,t} min{t \u03b8 ,t} s\u03b8 (x, \u03c4 ) 2 d\u03c4.(18)\nEq. (17) indicates the error of the score FPE quantifies the degree of conservativity of s \u03b8 . We further explain this idea via Ineq. (18). Consider a model s \u03b8 , and assume that for a large enough timestep t \u03b8 , it captures exactly the perturbed density (s \u03b8 (x, t \u03b8 ) = \u2207 x log q t \u03b8 (x)) which is close to the prior (normal distribution) because t \u03b8 \u2248 T . Intuitively, s \u03b8 is \"nearly\" conservative as the score of the prior is known to be conservative (because its score has a closed form as the gradient of a log-density). Indeed, Prop .1 says that the the estimated score should nearly be conservative if it approximately satisfies the score FPE , i.e., close to the gradient of a scalar function \u03a8 \u03b8 (x, t).\n\n4.3 Equivalence between s \u03b8 , s, and \u2207 x log p SDE t,\u03b8\n\nWe now investigate another implication of satisfying the score FPE which connects the score s \u03b8 with the ground truth s and \u2207 x log p SDE t,\u03b8 . The following proposition states that all aforementioned scores are identical if we train to reach a zero residual of score FPE for all (x, t) (under some technical assumptions ensuring the system of PDEs has a unique solution). Proposition 2. Suppose we know that in some suitable function space, 0 is the unique strong solution to the PDEs\n\u2202 t v \u2212 \u2207 x 1 2 g 2 (t)div x (v) + 1 2 g 2 (t) v 2 2 + 2 v, s \u2212 f , v = 0 with zero initial condition v(x, 0) \u2261 0 and zero boundary condition. If there is some \u03b8 0 so that s \u03b8 0 (x, t) = 0 for all (x, t), then s \u03b80 \u2261 s. Moreover, suppose that the PDEs \u2202 t v + \u2207 x 1 2 g 2 (t)div x (v) + 1 2 g 2 (t) v\n2 2 + f , v = 0 with zero initial and boundary condition has 0 as the unique strong solution, then\ns \u03b8 0 \u2261 0 implies s \u03b80 \u2261 \u2207 x log p SDE t,\u03b80 .\nWe hypothesize that satisfying the score FPE has a smoothing effect when f (x, t) is linear in x.\n\nSuppose the assumptions of Prop. 2 hold and hence, s \u03b80 \u2261 \u2207 x log p SDE t,\u03b80 . As linearly transforming normal distributions (by f ) remains normal, [Lu+22] proves that p SDE t,\u03b80 turns out to be a Gaussian distribution for any t. In practice, the assumptions are not likely to be met exactly, i.e. the residual will not be exactly zero s \u03b8 0 \u2261 0. In this case, we hypothesize that learning \u03b8 to reduce s \u03b8 can reduce the gap s \u03b8 \u2212 \u2207 x log p SDE t,\u03b8 , and may further modify the direction s \u03b8 toward high density region of Gaussian (smoothing effect).\n\n\nHigher-order score matching\n\nHigher-order derivatives of score can yield additional information about the data distribution. We prove a property stating that error bounds of higher-order score matching can further control the residual t 0 s\u03b8 (x, \u03c4 )d\u03c4 2 for all t \u2208 [0, T ]. This may explain why the scores learned via Eq. (4) are not sufficient to satisfy the score FPE as their higher-order scores may deviate from the ground truth. Proposition 3. Denote C(t) := 1 2 t 0 g 2 (\u03c4 )d\u03c4 . Assume the following error estimates hold for higher-order score matching sup\nR D \u00d7[0,T ] s \u2212 s \u03b8 2 \u2264 \u03b4 0 , sup R D \u00d7[0,T ] \u2207 x (s \u2212 s \u03b8 ) F \u2264 \u03b4 1 , sup R D \u00d7[0,T ] \u2207 x div x (s \u2212 s \u03b8 ) 2 \u2264 \u03b4 2 .\nThen for all (\nx, t) \u2208 R D \u00d7 [0, T ] we have t 0 s\u03b8 (x, \u03c4 )d\u03c4 2 \u2264 2\u03b4 0 + (\u03b4 2 + 2\u03b4 1 \u03b4 0 )C(t) + \u03b4 1 t 0 g 2 (\u03c4 ) s(x, \u03c4 ) 2 + f (x, \u03c4 ) 2 d\u03c4 + \u03b4 0 t 0 g 2 (\u03c4 ) \u2207 x s(x, \u03c4 ) F + \u2207 x f (x, \u03c4 ) F d\u03c4.\n\nScore FPE regularizer and techniques for efficient computation\n\nWe have demonstrated that score models learned via J DSM (Eq. (4)) do not satisfy the score FPE, a property that ground truth scores should satisfy a priori. Therefore, we devise a novel loss function, consisting of J DSM and a score FPE-regularizer\nR FP (\u03b8) := 1 D E t\u223cU [0,T ] E x\u223c\u03bd s\u03b8 (x, t) 2 defined as J FP (\u03b8; \u03bb(\u00b7), \u03b3) := J DSM (\u03b8; \u03bb(\u00b7)) + \u03b3R FP (\u03b8),(19)\nwhere \u03b3 \u2265 0 is a hyper-parameter controlling to what extent we desire the score FPE to be satisfied.\n\nSince s\u03b8 in R FP is generally expensive to calculate for high dimensional data, we propose to exploit the finite difference method [For88] to approximate \u2202 t s \u03b8 in Sec. 5.1 and Hutchinson's trace estimator [Hut89] for div x (s \u03b8 ) in Sec. 5.2. In Appx. C, we discuss an additional technique based on projecting s\u03b8 onto a random direction, thereby transforming the gradient computation in s\u03b8 into the computation of a one-dimensional derivative.\n\n5.1 Techniques to reduce computational cost of the \u2202 t s \u03b8 term Typically, \u2202 t s \u03b8 is computed via automatic differentiation. However, it can be efficiently approximated by finite differences as the derivative is one-dimensional. We first review the one-dimensional finite difference method and summarize its estimation error in the following lemma. Lemma 2. [For88] Let \u03b1 : [0, 1] \u2192 R D be a vector-valued function which is continuously differentiable up to third order derivatives. Denote h s and h d as hyper-parameters of step sizes. Then we have the following estimate of \u03b1 (t):\nh 2 s \u03b1(t + h d ) + (h 2 d \u2212 h 2 s )\u03b1(t) \u2212 h 2 d \u03b1(t \u2212 h s ) h s h d (h s + h d ) + O h d h 2 s + h s h 2 d h s + h d .\nIn particular, if h s = h d =: h, then the estimate becomes\n\u03b1(t + h) \u2212 \u03b1(t \u2212 h) 2h + O(h 2 ).\nIn the implementation for dataset with high dimensions, we consider \u03b1(\u00b7) := s \u03b8 (\u00b7, x) and hence,\n\u2202 t s \u03b8 (t, x) is approximated with h 2 s s \u03b8 (t + h d , x) + (h 2 d \u2212 h 2 s )s \u03b8 (t, x) \u2212 h 2 d s \u03b8 (t \u2212 h s , x) h s h d (h s + h d )\n. v j Av T j to estimate tr(A). We notice that div x (s \u03b8 (x, t)) = tr \u2207 x s \u03b8 . Thus, we can apply Hutchinson's trick and replace div x (s \u03b8 ) term with the estimation\n\n\nTechniques\n1 M M j=1 v j \u2207 x s \u03b8 (x, t)v T j .\nIn the implementation, p v is usually taken as a standard normal distribution or a Rademacher distribution. We follow the convention in [Son+20b] which sets M = 1 and shows its effectiveness in practice.\n\n\nExperimental results\n\nThe effectiveness of J FP is examined on synthetic dataset (Gaussian mixture models) and MNIST.\n\nSynthetic dataset We consider a Gaussian mixture model as the training data distribution.  The score trained with score FPE-regularizer can approximate the data density well, improving over vanilla score-matching. We hypothesize score FPE-regularizer may improve density estimation with the probability flow ODE, as it enforces a known self-consistency property of the ground truth score.\n\nMNIST We evaluate the proposed J FP (\u03b8; \u03bb(\u00b7), \u03b3) on MNIST with different \u03b3's. Table 1 \n\n\nConclusion\n\nWe introduce the score FPE and theoretically study its relation with score matching, conservativity and density induced by parametric reverse diffusion. Moreover, we propose to penalize on residual of score FPE and show its effectiveness on simple dataset. However, it is unclear how the dynamics of score FPE affects, for instance, training of a larger scale dataset or variational lower bound.\n\n[Sai+22]\n\nKoichi Saito et al. VE SDE It has a zero drift term f = 0 and diffusion term g(t) = d\u03c3 2 (t) dt with some function \u03c3(t). Hence, the forward SDE (Eq. (1)) becomes In [Kim+22], they proposed a variant of VE SDE attempting to resolve the unbounded score problem [DVK21], which is called Reciprocal VE (RVE). Let > 0 be a fixed constant. RVE SDE also has zero drift term but with a different parametrization for diffusion\ndx(t) = d\u03c3 2 (t) dt dw t .(20)g(t) := \uf8f1 \uf8f2 \uf8f3 \u03c3 max \u03c3min \u03c3max t 2 log( \u03c3max \u03c3 min ) t , if t > 0 0, if t = 0\nVP SDE Let \u03b2 be a non-negative function of t. VP SDE has a linear drift term f (x, t) = \u2212 1 2 \u03b2(t)x and diffusion term g(t) = \u03b2(t). Thus, the forward SDE is We summarize the aforementioned instantiations of SDE and their associated score FPE in Table 2. \ndx(t) = \u2212 1 2 \u03b2(t)x(t)dt + \u03b2(t)dw t .f (x, t) 0 \u2212 1 2 \u03b2(t)x g(t) \u03c3min \u03c3max \u03c3 min t 2 log \u03c3max \u03c3 min \uf8f1 \uf8f2 \uf8f3 \u03c3max \u03c3 min \u03c3max t 2 log( \u03c3max \u03c3 min ) t , t > 0 0, t = 0 \u03b2(t) SDE dx(t) = g(t)dwt dx(t) = \u2212 1 2 \u03b2(t)x(t)dt + \u03b2(t)dwt score FPE \u2202ts = \u2207x 1 2 g 2 (t)divx(s) + 1 2 g 2 (t) s 2 2 \u2202ts = 1 2 \u03b2(t)\u2207x divx(s) + s 2 2 + x, s\nB How scores satisfy score FPE?\n\nIn this section, we experimentally demonstrate how score functions should satisfy the score FPE in two different aspects. We consider the data distribution as a Gaussian mixture model (GMM) of the density 1 5 N (\u22125, \u22125), I + 4 5 N (5, 5), I on R 2 whose samples are illustrated in Fig. 4a. The diffusion process is taken as VE SDE (Eq. (20)). The ground truth score of GMM, denoted as s GMM , can be expressed explicitly with a closed formula throughout the diffusion (as the diffusion process is linear in x).\n\nFirst of all, we examine if s GMM satisfies the score FPE by computing r FP (t; s GMM ) and plot it in Fig. 6a (blue curve). We can see that the score FPE residual of the ground truth is almost zero, which empirically supports Corollary 1.\n\nSecond, as we explain that we can solve score FPE for the score at any time if we are merely given a score at a single time moment. Namely, once we find a solutions to the following initial value problem of system of PDEs, we know a score at all time. Fig. 4b and plot its score FPE residual r FP (t;s GMM \u03b8 ) in Fig. 6a (orange curve). Interestingly, it also generates quite satisfactory samples and Fig. 5b show it estimates the ground truth score (Fig. 5a) well. This supports our argument. , which is learned from the score FPE-guided objective, can also approximate the ground truth well.\n\u2202 ts = \u2207 x 1 2 g 2 (t)div x (s) + 1 2 g 2 (t) s in\nIn addition, we compute the residual of score FPE of a score s GMM \u03b8 learned from the denoising score matching (Eq. (4)), which is plotted in Fig. 6b. We observe that s GMM \u03b8 also does not satisfy score FPE even though it works decently on generation (Fig. 4c) and score estimation (Fig. 5c).\n\nC A further technique to reduce computation costs for score FPE As we explained in Sec. 5, the computation of s\u03b8 (x, t) in R FP (\u03b8) is generally expensive and we propose two techniques, including finite difference trick and Hutchinson's trace estimator, to replace expensive computations of some components in s\u03b8 (x, t).  21)) numerically satisfy score FPE. In contrast, 6b provides a further evidence that s GMM \u03b8 , which is learned from denoising score matching, does not satisfy score FPE.\n\nIn this section, we further propose another potential trick to reduce the computation cost of differentiation. We recall that\ns\u03b8 (x, t) = \u2202 t s \u03b8 (I) \u2212 \u2207 x 1 2 g 2 (t)div x (s \u03b8 ) + 1 2 g 2 (t) s \u03b8 2 2 \u2212 f , s \u03b8 \u2212 div x (f ) (II)(23)\nUsing automatic differentiation to compute the gradient in s\u03b8 (x, t) (the (II) part in Eq. (23)) is generally cumbersome for high dimensional data. We propose to use random projection to relieve the computation of gradient (multi-dimension) to directional derivative (one-dimensional). Thus, we can further apply the finite difference trick introduced in Sec. 5.1 to reduce the computation efforts. We first recall a fundamental property before rigorously formulating the technique. \nv \u2208 R D , D v M (x, t) = \u2207 x M (x, t), v ,\nwhere D v M (x, t) means the directional derivative of M in x along the direction v which is defined as:\nD v M (x, t) := lim h\u21920 M (x + hv, t) \u2212 M (x, t) h = d dh M (x + hv, t) h=0\n.\n\nFor simplicity, let us denote M (x, t) := 1 2 g 2 (t)div x (s \u03b8 ) + 1 2 g 2 (t) s \u03b8 2 2 \u2212 f , s \u03b8 \u2212 div x (f ) and let v \u2208 R D be arbitrary vector. We project s\u03b8 (x, t) along direction v and apply the Lemma 3,\ns\u03b8 (x, t), v = \u2202 t s \u03b8 \u2212 \u2207 x M (x, t), v = \u2202 t s \u03b8 , v \u2212 d dh M (x + hv, t) h=0\n, v .\n\nNotice that both \u2202 t s \u03b8 and d dh M (x + hv, t) h=0 are one-dimensional differentiation, which can be estimated via Lemma 2 and hence, we can avoid automatic differentiation. We hereafter propose an estimated score FP regularizer which may replace R FP wit\u0125\nR FP (\u03b8) := 1 D E t\u223cU [0,T ] E x\u223c\u03bd E v\u223cpv | s\u03b8 (x, t), v | ,\nwhere p v is a distribution of random vector v \u2208 R D . We observe that the performance may degrade by usingR FP , which may due to the inaccurate approximation to the exact score FPE. Therefore, a further study is required to have lower computation costs while preventing the deterioration in the performance.\n\n\nD Explanation of Implementation\n\nIn Fig. 1, we train a score on MNIST for 200 epochs with a learning rate 1e \u2212 3 and batch size 32 by using an identical neural network structure to the repository 1 but modify the forward SDE as VE SDE or VP SDE (see Appx. A). The network structure of synthetic dataset in Sec. B is similar to the aforementioned one but we simply replace all convolutional layers with fully connected layers. We train for 2, 000 epochs with a learning rate 1e \u2212 3 and batch size 500.\n\nFor the case of CIFAR10, we use the pre-trained score models provided by [Son+20a] 2 instead of training them from scratch. The VE SDE and VP SDE are taken as NCSN++ cont. and DDPM++ cont., respectively.\n\nThe neural network setup in Fig. 2 is the same as toy model structures provided in the repository of [Lu+22] 3 . We found out setting the weight of score FPE to \u03bb = 0.001 can generally work well for the toy dataset.\n\nE Proofs and discussion E.1 Proof of Corollary 1\n\nWe prove the result with a more general forward SDE\ndx = F (x, t)dt + G(x, t)dw t ,(24)\nwhere F (\u00b7, t) : R D \u2192 R D and G(\u00b7, t) :\nR D \u2192 R D\u00d7D .\nWe know that the density q t (x) satisfies the Fokker-Planck equation [\u00d8ks03] \n\u2202 t q t (x) = \u2212 D j=1 \u2202 xj F j (x, t)q t (x) ,(25)\nwhereF\n(x, t) := F (x, t) \u2212 1 2 \u2207 \u00b7 [G(x, t)G(x, t) T ] \u2212 1 2 G(x, t)G(x, t) T \u2207 x log q t (x). We further denote A(x, t) := F (x, t) \u2212 1 2 \u2207 \u00b7 [G(x, t)G(x, t) T ] and B(x, t) := \u2212 1 2 G(x, t)G(x, t) T . NowF (x, t) = A(x, t) + B(x, t)s(x, t), and we have \u2202 t log q t (x) = 1 q t (x) \u2202 t q t (x) = \u2212 1 q t (x) D j=1 \u2202 xj F j (x, t)q t (x) = \u2212 1 q t (x) D j=1 \u2202 xjFj (x, t)q t (x) +F j (x, t)\u2202 xj q t (x) = \u2212 D j=1 \u2202 xjFj (x, t) +F j (x, t)\u2202 xj log q t (x) = \u2212 div x (F ) + F , s = \u2212 div x Bs + Bs, s + A, s + div x (A) = 1 2 div x GG T s + 1 2 G T s 2 2 \u2212 A, s \u2212 div x (A).\nSince log q t (x) is sufficiently smooth, we can swap the order of differentiations and get \u2202 t s = \u2202 t \u2207 x log q t (x) = \u2207 x \u2202 t log q t (x). Hence, the statement is proved.\n\nRemark 1. In Eq. (1) where G does not depend on x, namely G(x, t) \u2261 g(t)I, thenF (x, t) = f (x, t) \u2212 1 2 g 2 (t)\u2207 x log q t (x) and\n\u2202 t log q t (x) = 1 2 g 2 (t)div x (s) + 1 2 g 2 (t) s 2 2 \u2212 f , s \u2212 div x (f ) \u2202 t s = \u2207 x 1 2 g 2 (t)div x (s) + 1 2 g 2 (t) s 2 2 \u2212 f , s \u2212 div x (f ) .\n\nE.2 Proof of Proposition 1\n\nIntegrating the following equation w.r.t. time from \u03c4 = t \u03b8 to \u03c4 = t with t \u2208 [0, T ] fixed,\n\u2202 t s \u03b8 = \u2207 x 1 2 g 2 (t)div x (s \u03b8 ) + 1 2 g 2 (t) s \u03b8 2 2 \u2212 f , s \u03b8 \u2212 div x (f ) + s\u03b8 (x, t),\nleads to\ns \u03b8 (x, t) \u2212 s \u03b8 (x, t \u03b8 ) = \u2207 x t t \u03b8 1 2 g 2 (t)div x (s \u03b8 ) + 1 2 g 2 (t) s \u03b8 2 2 \u2212 f , s \u03b8 \u2212div x (f ) d\u03c4 + t t \u03b8 s\u03b8 (x, t)d\u03c4,\nwhere the swap of integration and differentiation is valid if the integrand is sufficiently smooth.\n\nWith the assumption, we obtain that for all t \u2208 [0, T ]\ns \u03b8 (x, t) \u2212 \u2207 x log q t \u03b8 (x) + t t \u03b8 1 2 g 2 (t)div x (s \u03b8 ) + 1 2 g 2 (t) s \u03b8 2 2 \u2212 f , s \u03b8 \u2212div x (f ) d\u03c4 = t t \u03b8 s\u03b8 (x, \u03c4 )d\u03c4.\nBy taking the norm of the above equation, one can obtain\ns \u03b8 (x, t) \u2212 \u2207 x \u03a8 \u03b8 (x, t) 2 = t t \u03b8 s\u03b8 (x, \u03c4 )d\u03c4 2 .\nFrom which we obtain\ns \u03b8 (x, t) \u2212 \u2207 x \u03a8 \u03b8 (x, t) 2 = t t \u03b8 s\u03b8 (x, \u03c4 )d\u03c4 2 \u2264 t t \u03b8 s\u03b8 (x, \u03c4 ) 2 d\u03c4 .\nThe upper and lower bound of integral can be respectively written as max{t, t \u03b8 } and min{t, t \u03b8 }, and whence, the proposition is proved.\n\nProof of Proposition 2. We recall Eq. (10), which indicates\n\u2202 t s \u03b8 \u2212 \u2207 x 1 2 g 2 (t)div x (s \u03b8 ) + 1 2 g 2 (t) s \u03b8 2 2 \u2212 f , s \u03b8 \u2212 div x (f ) \u2212 s\u03b8 = 0.(26)\nFirst, we subtract Eq. (2) by the above equation and get\n\u2202 t (s SDE \u03b8 \u2212 s \u03b8 ) \u2212 \u2207 x 1 2 g 2 (t)div x (s \u03b8 \u2212 s SDE \u03b8 ) \u2212 1 2 g 2 (t) s \u03b8 \u2212 s SDE \u03b8 2 2 \u2212 f , s \u03b8 \u2212 s SDE \u03b8 + s\u03b8 = 0.\n(27) Consider when \u03b8 = \u03b8 0 and let u \u03b80 := s SDE \u03b80 \u2212 s \u03b80 . Then the PDEs become\n\u2202 t u \u03b80 + \u2207 x 1 2 g 2 (t)div x (u \u03b80 ) + 1 2 g 2 (t) u \u03b80 2 2 + f , u \u03b80 = 0.\nHere, u \u03b80 is a solution to the PDEs. It is noticed that this system of PDEs has a zero initial condition and zero boundary condition as both s \u03b80 and s SDE \u03b80 share the same initial/boundary condition. Thus, from the assumption of the uniqueness of solution, we know that u \u03b80 \u2261 0, and hence, s SDE \u03b80 \u2261 s \u03b80 . We repeat the same trick to subtract Eq. (9) by Eq. (26) from which we can obtain s \u03b80 \u2261 s.\n\n\nE.4 Proof of Proposition 3\n\nBy subtracting the following two equations\n\u2202 t s \u03b8 = \u2207 x 1 2 g 2 (t)div x (s \u03b8 ) + 1 2 g 2 (t) s \u03b8 2 2 \u2212 f , s \u03b8 \u2212 div x (f ) + s\u03b8 \u2202 t s = \u2207 x 1 2 g 2 (t)div x (s) + 1 2 g 2 (t) s 2 2 \u2212 f , s \u2212 div x (f ) ,\nwe obtain \u2202 t (s \u03b8 \u2212 s) = \u2207 x 1 2 g 2 (t)div x (s \u03b8 \u2212 s) + 1 2 g 2 (t) s \u03b8 \n+ t 0 \u2207 x f , s \u03b8 \u2212 s + f , \u2207 x (s \u03b8 \u2212 s) d\u03c4\nBy applying the 2 -norm and Cauchy-Schwartz inequality while noting the relation A 2 \u2264 A F for a general square matrix A, the statement is proved.\n\n\nE.5 Proof of Theorem 1\n\nLemma 5 (Gr\u00f6nwall's inequality [Gro19]). Assume that \u03b1, \u03b2, and u are continuous functions on Integrating the above inequality from \u03c4 = t to \u03c4 = T proves the statement.\n\nProof of Theorem 1. We first prove the Ineq. (15). Notice that we can rearrange J Diff as By the special case of FPE (Eq. (25)) with zero drift term, we obtain the PDE characterizes the evolution of p ODE t,\u03b8 \u2202p ODE t,\u03b8 \u2202t = div x 1 2 g 2 (t)s \u03b8 (x, t) \u2212 f (x, t) p ODE t,\u03b8 (x)\nJ Diff (\u03b8) = 1 2 T 0 g 2 (t)E x\u223cqt(x) s \u03b8 (x, t) \u2212 \u2207 x log q t (x) s ODE \u03b8 (x, t) \u2212 s \u03b8 (x, t) dt = T 0 R D g(t) q t (x) 2 s \u03b8 (x, t) \u2212 \u2207 x log q t (x) g(t) q t (x) 2 s ODE \u03b8 (x, t) \u2212 s \u03b8 (x, t) dtdx.\nHence,\n\u2202 log p ODE t,\u03b8 \u2202t = 1 p ODE t,\u03b8 \u2202p ODE t,\u03b8\nFig. 1\n1plots these residuals for score models pre-trained via DSM on MNIST and CIFAR-10. Despite achieving low r DSM-like score-matching loss across all t (green curve), pre-trained score models fail to satisfy the score FPE equation especially for small t (blue and orange curves).\n\nFigure 1 :\n1Comparison of the numerical scale of r DSM (t; s \u03b8 ) and r FP (t; s \u03b8 ) of pre-trained scores s \u03b8 on MNIST and CIFAR-10. Pre-trained models do not numerically satisfy score FPE in contrast to their denoising score matching-like errors. We attempt to explain this phenomena in Sec. 4.2 and 4.4.\n\n\nto reduce the computational cost of the div x (s \u03b8 ) term Hutchinson's trace estimator [Hut89] stochastically estimates the trace tr(A) of any square matrix A. Its idea is choose a distribution p v so that E v\u223cpv [v] = 0 and E v\u223cpv [vv T ] = I. Hence, tr(A) = tr(AE v\u223cpv [vv T ]) = E v\u223cpv [tr(Avv T )] = E v\u223cpv [tr(vAv T )] = E v\u223cpv [vAv T ]. By i.i.d. sampling {v j } M j=1 from p v , we can use the following unbiased estimator\n\n\nFig. 2illustrates (a) ground truth density, and the density produced by probability flow ODE [Son+20a] of scores trained with (b) \u03bb = 0.0 (i.e, conventional score matching training) and (c) \u03bb = 0.001.\n\nFigure 2 :\n2Comparison of (a) data density, (b) estimated density by probability flow ODE with s \u03b8 trained with \u03b3 = 0.0, and (c) with \u03b3 = 0.001. Score FPE-regularizer improves density estimation.\n\n\nreports negative log-likelihood (NLL) in bits/dim (bpd) across three instantiations of the forward SDE (see Appx. A) and two choices of weighting functions \u03bb(\u00b7)'s ([Son+20a] and [Son+21]). We observe a general improvement in NLL with \u03b3 = 1.0. In Fig. 3, we show examples generated with different choices of the SDE and the weight of score FPE-regularizer, \u03b3, on MNIST.\n\nFigure 3 :\n3Illustration of generated samples.\n\nA\ntypical instance of VE SDE is Score Matching of Langevin dynamics (SMLD) [SE19], where \u03c3(t) := \u03c3 min \u03c3max \u03c3min t for t \u2208 (0, 1]. In our implementation, we follow the conventional setup of (\u03c3 min , \u03c3 max ) := (0.01, 50).\n\nA\nclassic example of VP SDE is Denoising Diffusion Probabilistic Modeling (DDPM) [Soh+15; HJA20], where \u03b2(t) := \u03b2 min + t(\u03b2 max \u2212 \u03b2 min ) for t \u2208 [0, 1]. We adopt the common setup of (\u03b2 min , \u03b2 max ) := (0.1, 20) in our implementation.\n\nFigure 4 :Figure 5 := 0\n450Comparison of instances generated using the score functions learned by our score FPEguided objective fucntion (Eq. (22)) and the conventional denoising score matching (Eq. (4)), which are denoted ass GMM \u03b8 and s GMM \u03b8 , respectively. Both scores can synthesize reasonable quality samples. The fluid flow graph of ground truth score and estimated scores at t\n\nFigure 6 :\n6a) FP residuals of ground truth score and the score learned from Eq. (22) (b) FP residuals of the score learned from Eq. (4) Comparison of the score FPE residuals of s GMM ,s GMM \u03b8 and s GMM \u03b8 of GMM. 6a show that both the ground truth score (with closed form) s GMM and the scores GMM \u03b8 obtained by solving score FPE (Eq. (\n\nLemma 3 .\n3Let M := M (x, t) : R D \u00d7 [0, T ] \u2192 R be a continuously differentiable function of x. For any\n\n\nf , s \u03b8 \u2212 s + s\u03b8 (2 s \u03b8 \u2212 s, s . Integrating over time from \u03c4 = 0 to \u03c4 = t, we obtaint 0 s\u03b8 (x, \u03c4 )d\u03c4 = s \u03b8 (x, t) \u2212 s(x, t) \u2212 s \u03b8 (x, 0) \u2212 s(x, 2 (\u03c4 )\u2207 x div x (s \u03b8 \u2212 s)d\u03c4 \u2212 t 0 g 2 (\u03c4 ) \u2207 x (s \u03b8 \u2212 s), s \u03b8 \u2212 s + \u2207 x (s \u03b8 \u2212 s), s + s \u03b8 \u2212 s, \u2207 x s d\u03c4\n\n[ 0 ,\n0T ]. If \u03b2 is non-negative on [0, T ] and if u satisfies the integral inequalityu(t) \u2264 \u03b1(t) + T t \u03b2(\u03c4 )u(\u03c4 )d\u03c4, for all t \u2208 [0, T ] then u(t) \u2264 \u03b1(t) + T t \u03b1(\u03c4 )\u03b2(\u03c4 ) exp \u03c4 t \u03b2(r)dr d\u03c4, for all t \u2208 [0, T ]In particularly, if \u03b1 is non-decreasing (especially, a constant independent of t),\n\n\nThe claim is established by applying Cauchy-Schwartz inequality to functions g(t) qt(x)2 s \u03b8 (x, t)\u2212 \u2207 x log q t (x) and g(t) qt(x) 2 s ODE \u03b8 (x, t) \u2212 s \u03b8 (x, t). We now prove the Ineq.(16), in which we just need to consider the case when M (\u03b8) := sup t\u2208[0,T ] E x\u223cqt(x) T 0 s \u03b8 (x, \u03c4 ) 2 d\u03c4 < \u221e. Otherwise, the result holds obviously. Recall that the probability flow ODE [Son+20a] associated to Eq. (5) is defined as dx(t) dt = f (x(t), t) \u2212 1 2 g 2 (t)s \u03b8 (x(t), t).\n\nTable 1 :\n1NLL (in bpd) for different FP weights \u03b3's and weighting functions \u03bb(\u00b7)'sSDE type \n\u03bb(\u00b7) \n\u03b3 = 0.0 \u03b3 = 0.01 \u03b3 = 0.1 \u03b3 = 1.0 \u03b3 = 10.0 \n\nVE + \u03b3FP \n[Son+20a] \n3.86 \n3.63 \n3.66 \n3.28 \n3.37 \n[Son+21] \n3.63 \n3.94 \n3.53 \n3.20 \n3.23 \n\nVP + \u03b3FP \n[Son+20a] \n2.95 \n3.06 \n3.09 \n2.91 \n3.34 \n[Son+21] \n3.11 \n3.14 \n3.04 \n3.28 \n3.28 \n\nRVE + \u03b3FP [Son+20a] \n3.45 \n3.68 \n3.77 \n3.57 \n3.13 \n[Son+21] \n3.62 \n3.78 \n3.49 \n3.16 \n3.36 \n\n\n\nTable 2 :\n2Summary of the forward SDEs and their score FPEsVE SDE \nRVE SDE \nVP SDE \n\n\nhttps://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=s haring 2 https://github.com/yang-song/score_sde_pytorch 3 https://github.com/LuChengTHU/mle_score_ode\n\nReverse-time diffusion equation models. Anderson Brian, Stochastic Processes and their Applications. 12Brian DO Anderson. \"Reverse-time diffusion equation models\". In: Stochastic Pro- cesses and their Applications 12.3 (1982), pp. 313-326.\n\nNeural ordinary differential equations. T Q Ricky, Chen, Advances in neural information processing systems. 31Ricky TQ Chen et al. \"Neural ordinary differential equations\". In: Advances in neural information processing systems 31 (2018).\n\nDiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability. Kin Wai Cheuk, arXiv:2210.05148arXiv preprintKin Wai Cheuk et al. \"DiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability\". In: arXiv preprint arXiv:2210.05148 (2022).\n\nDiffusion models beat gans on image synthesis. Prafulla Dhariwal, Alexander Nichol, Advances in Neural Information Processing Systems. 34Prafulla Dhariwal and Alexander Nichol. \"Diffusion models beat gans on image syn- thesis\". In: Advances in Neural Information Processing Systems 34 (2021), pp. 8780- 8794.\n\nScore-based generative modeling with critically-damped langevin diffusion. Tim Dockhorn, Arash Vahdat, Karsten Kreis, arXiv:2112.07068arXiv preprintTim Dockhorn, Arash Vahdat, and Karsten Kreis. \"Score-based generative modeling with critically-damped langevin diffusion\". In: arXiv preprint arXiv:2112.07068 (2021).\n\nMeasure theory and fine properties of functions. Routledge. C Lawrence, Ronald F Evans, Garzepy, Lawrence C Evans and Ronald F Garzepy. Measure theory and fine properties of functions. Routledge, 2018.\n\nGeneration of finite difference formulas on arbitrarily spaced grids. Bengt Fornberg, Mathematics of computation 51. 184Bengt Fornberg. \"Generation of finite difference formulas on arbitrarily spaced grids\". In: Mathematics of computation 51.184 (1988), pp. 699-706.\n\nNote on the derivatives with respect to a parameter of the solutions of a system of differential equations. Gronwall Thomas Hakon, Annals of MathematicsThomas Hakon Gronwall. \"Note on the derivatives with respect to a parameter of the solutions of a system of differential equations\". In: Annals of Mathematics (1919), pp. 292-296.\n\nEstimation of non-normalized statistical models by score matching. Aapo Hyv\u00e4rinen, Peter Dayan, In: Journal of Machine Learning Research. 6Aapo Hyv\u00e4rinen and Peter Dayan. \"Estimation of non-normalized statistical models by score matching.\" In: Journal of Machine Learning Research 6.4 (2005).\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 33Jonathan Ho, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models\". In: Advances in Neural Information Processing Systems 33 (2020), pp. 6840-6851.\n\nA stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. F Michael, Hutchinson, Communications in Statistics-Simulation and Computation. 18Michael F Hutchinson. \"A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines\". In: Communications in Statistics-Simulation and Computation 18.3 (1989), pp. 1059-1076.\n\nDenoising diffusion restoration models. Bahjat Kawar, arXiv:2201.11793arXiv preprintBahjat Kawar et al. \"Denoising diffusion restoration models\". In: arXiv preprint arXiv:2201.11793 (2022).\n\nSoft truncation: A universal training technique of score-based diffusion model for high precision score estimation. Dongjun Kim, PMLR. 2022International Conference on Machine Learning. Dongjun Kim et al. \"Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation\". In: International Conference on Machine Learning. PMLR. 2022, pp. 11201-11228.\n\nDiffwave: A versatile diffusion model for audio synthesis. Zhifeng Kong, arXiv:2009.09761arXiv preprintZhifeng Kong et al. \"Diffwave: A versatile diffusion model for audio synthesis\". In: arXiv preprint arXiv:2009.09761 (2020).\n\nMaximum Likelihood Training for Score-based Diffusion ODEs by High Order Denoising Score Matching. Cheng Lu, PMLR. 2022International Conference on Machine Learning. Cheng Lu et al. \"Maximum Likelihood Training for Score-based Diffusion ODEs by High Order Denoising Score Matching\". In: International Conference on Machine Learning. PMLR. 2022, pp. 14429-14460.\n\nEstimating high order gradients of the data distribution by denoising. Chenlin Meng, Advances in Neural Information Processing Systems. 34Chenlin Meng et al. \"Estimating high order gradients of the data distribution by denois- ing\". In: Advances in Neural Information Processing Systems 34 (2021), pp. 25359- 25369.\n\nSdedit: Image synthesis and editing with stochastic differential equations. Chenlin Meng, arXiv:2108.01073arXiv preprintChenlin Meng et al. \"Sdedit: Image synthesis and editing with stochastic differential equations\". In: arXiv preprint arXiv:2108.01073 (2021).\n\nGaussian deconvolution via differentiation. Elias Masry, John A Rice, Canadian Journal of Statistics. 20Elias Masry and John A Rice. \"Gaussian deconvolution via differentiation\". In: Cana- dian Journal of Statistics 20.1 (1992), pp. 9-21.\n\nDiffusion Models for Adversarial Purification. Weili Nie, arXiv:2205.07460arXiv preprintWeili Nie et al. \"Diffusion Models for Adversarial Purification\". In: arXiv preprint arXiv:2205.07460 (2022).\n\nStochastic differential equations. Bernt \u00d8ksendal, Stochastic differential equations. SpringerBernt \u00d8ksendal. \"Stochastic differential equations\". In: Stochastic differential equations. Springer, 2003, pp. 65-84.\n\nScore-Based Generative Models Detect Manifolds. Jakiw Pidstrigach, arXiv:2206.01018arXiv preprintJakiw Pidstrigach. \"Score-Based Generative Models Detect Manifolds\". In: arXiv preprint arXiv:2206.01018 (2022).\n\nHigh-resolution image synthesis with latent diffusion models. Robin Rombach, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022Robin Rombach et al. \"High-resolution image synthesis with latent diffusion mod- els\". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 10684-10695.\n\nPhysics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George E Karniadakis, Journal of Computational physics. 378Maziar Raissi, Paris Perdikaris, and George E Karniadakis. \"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involv- ing nonlinear partial differential equations\". In: Journal of Computational physics 378 (2019), pp. 686-707.\n\nPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding. Chitwan Saharia, arXiv:2205.11487arXiv preprintChitwan Saharia et al. \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\". In: arXiv preprint arXiv:2205.11487 (2022).\n", "annotations": {"author": "[{\"end\":152,\"start\":70},{\"end\":230,\"start\":153},{\"end\":309,\"start\":231},{\"end\":398,\"start\":310},{\"end\":482,\"start\":399},{\"end\":557,\"start\":483}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":81},{\"end\":165,\"start\":159},{\"end\":243,\"start\":237},{\"end\":327,\"start\":321},{\"end\":413,\"start\":404},{\"end\":496,\"start\":491}]", "author_first_name": "[{\"end\":80,\"start\":70},{\"end\":158,\"start\":153},{\"end\":236,\"start\":231},{\"end\":320,\"start\":310},{\"end\":403,\"start\":399},{\"end\":490,\"start\":483}]", "author_affiliation": "[{\"end\":151,\"start\":110},{\"end\":229,\"start\":188},{\"end\":308,\"start\":267},{\"end\":397,\"start\":356},{\"end\":481,\"start\":440},{\"end\":556,\"start\":520}]", "title": "[{\"end\":67,\"start\":1},{\"end\":624,\"start\":558}]", "venue": null, "abstract": "[{\"end\":1609,\"start\":626}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1811,\"start\":1805},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1818,\"start\":1811},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1825,\"start\":1818},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1841,\"start\":1833},{\"end\":3172,\"start\":3165},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3972,\"start\":3963},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3978,\"start\":3972},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4743,\"start\":4736},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5138,\"start\":5132},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7484,\"start\":7476},{\"end\":8192,\"start\":8185},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11416,\"start\":11407},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11422,\"start\":11416},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12007,\"start\":12001},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12013,\"start\":12007},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12999,\"start\":12992},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13137,\"start\":13130},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17516,\"start\":17509},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19461,\"start\":19454},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19537,\"start\":19530},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20136,\"start\":20129},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22416,\"start\":22408},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22509,\"start\":22502},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28251,\"start\":28244},{\"end\":28630,\"start\":28623},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32191,\"start\":32184}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33135,\"start\":32851},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33442,\"start\":33136},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33874,\"start\":33443},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34077,\"start\":33875},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34274,\"start\":34078},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34645,\"start\":34275},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34693,\"start\":34646},{\"attributes\":{\"id\":\"fig_7\"},\"end\":34916,\"start\":34694},{\"attributes\":{\"id\":\"fig_8\"},\"end\":35153,\"start\":34917},{\"attributes\":{\"id\":\"fig_9\"},\"end\":35539,\"start\":35154},{\"attributes\":{\"id\":\"fig_10\"},\"end\":35877,\"start\":35540},{\"attributes\":{\"id\":\"fig_11\"},\"end\":35983,\"start\":35878},{\"attributes\":{\"id\":\"fig_12\"},\"end\":36235,\"start\":35984},{\"attributes\":{\"id\":\"fig_13\"},\"end\":36529,\"start\":36236},{\"attributes\":{\"id\":\"fig_14\"},\"end\":37001,\"start\":36530},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37422,\"start\":37002},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37509,\"start\":37423}]", "paragraph": "[{\"end\":2647,\"start\":1625},{\"end\":3644,\"start\":2649},{\"end\":4384,\"start\":3646},{\"end\":4586,\"start\":4399},{\"end\":4623,\"start\":4622},{\"end\":4790,\"start\":4625},{\"end\":5160,\"start\":4859},{\"end\":5350,\"start\":5219},{\"end\":5692,\"start\":5610},{\"end\":5955,\"start\":5764},{\"end\":6225,\"start\":5957},{\"end\":6566,\"start\":6227},{\"end\":6811,\"start\":6617},{\"end\":7556,\"start\":6864},{\"end\":7836,\"start\":7680},{\"end\":7974,\"start\":7973},{\"end\":8240,\"start\":8043},{\"end\":8297,\"start\":8291},{\"end\":8786,\"start\":8347},{\"end\":9767,\"start\":8894},{\"end\":10171,\"start\":9769},{\"end\":10367,\"start\":10274},{\"end\":11441,\"start\":10608},{\"end\":11554,\"start\":11474},{\"end\":12306,\"start\":11617},{\"end\":13118,\"start\":12945},{\"end\":13374,\"start\":13120},{\"end\":13449,\"start\":13444},{\"end\":13708,\"start\":13549},{\"end\":13871,\"start\":13749},{\"end\":14069,\"start\":13953},{\"end\":14311,\"start\":14106},{\"end\":14544,\"start\":14356},{\"end\":14621,\"start\":14615},{\"end\":15090,\"start\":14733},{\"end\":15480,\"start\":15466},{\"end\":16271,\"start\":15558},{\"end\":16327,\"start\":16273},{\"end\":16814,\"start\":16329},{\"end\":17214,\"start\":17116},{\"end\":17358,\"start\":17261},{\"end\":17911,\"start\":17360},{\"end\":18477,\"start\":17943},{\"end\":18610,\"start\":18596},{\"end\":19108,\"start\":18859},{\"end\":19321,\"start\":19221},{\"end\":19768,\"start\":19323},{\"end\":20353,\"start\":19770},{\"end\":20533,\"start\":20474},{\"end\":20665,\"start\":20568},{\"end\":20970,\"start\":20802},{\"end\":21223,\"start\":21020},{\"end\":21343,\"start\":21248},{\"end\":21733,\"start\":21345},{\"end\":21821,\"start\":21735},{\"end\":22231,\"start\":21836},{\"end\":22241,\"start\":22233},{\"end\":22660,\"start\":22243},{\"end\":23022,\"start\":22768},{\"end\":23375,\"start\":23344},{\"end\":23887,\"start\":23377},{\"end\":24128,\"start\":23889},{\"end\":24723,\"start\":24130},{\"end\":25067,\"start\":24775},{\"end\":25561,\"start\":25069},{\"end\":25688,\"start\":25563},{\"end\":26280,\"start\":25797},{\"end\":26428,\"start\":26324},{\"end\":26506,\"start\":26505},{\"end\":26717,\"start\":26508},{\"end\":26803,\"start\":26798},{\"end\":27062,\"start\":26805},{\"end\":27433,\"start\":27124},{\"end\":27936,\"start\":27469},{\"end\":28141,\"start\":27938},{\"end\":28358,\"start\":28143},{\"end\":28408,\"start\":28360},{\"end\":28461,\"start\":28410},{\"end\":28538,\"start\":28498},{\"end\":28631,\"start\":28553},{\"end\":28689,\"start\":28683},{\"end\":29431,\"start\":29257},{\"end\":29564,\"start\":29433},{\"end\":29842,\"start\":29750},{\"end\":29947,\"start\":29939},{\"end\":30178,\"start\":30079},{\"end\":30235,\"start\":30180},{\"end\":30424,\"start\":30368},{\"end\":30500,\"start\":30480},{\"end\":30718,\"start\":30580},{\"end\":30779,\"start\":30720},{\"end\":30933,\"start\":30877},{\"end\":31138,\"start\":31057},{\"end\":31621,\"start\":31218},{\"end\":31694,\"start\":31652},{\"end\":31934,\"start\":31859},{\"end\":32126,\"start\":31980},{\"end\":32320,\"start\":32153},{\"end\":32599,\"start\":32322},{\"end\":32807,\"start\":32801}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4621,\"start\":4587},{\"attributes\":{\"id\":\"formula_2\"},\"end\":4858,\"start\":4791},{\"attributes\":{\"id\":\"formula_3\"},\"end\":5218,\"start\":5161},{\"attributes\":{\"id\":\"formula_4\"},\"end\":5609,\"start\":5351},{\"attributes\":{\"id\":\"formula_5\"},\"end\":5763,\"start\":5693},{\"attributes\":{\"id\":\"formula_6\"},\"end\":6616,\"start\":6567},{\"attributes\":{\"id\":\"formula_7\"},\"end\":6863,\"start\":6812},{\"attributes\":{\"id\":\"formula_8\"},\"end\":7679,\"start\":7557},{\"attributes\":{\"id\":\"formula_9\"},\"end\":7972,\"start\":7837},{\"attributes\":{\"id\":\"formula_10\"},\"end\":8290,\"start\":8241},{\"attributes\":{\"id\":\"formula_11\"},\"end\":8346,\"start\":8298},{\"attributes\":{\"id\":\"formula_12\"},\"end\":8893,\"start\":8787},{\"attributes\":{\"id\":\"formula_13\"},\"end\":10273,\"start\":10172},{\"attributes\":{\"id\":\"formula_14\"},\"end\":10547,\"start\":10368},{\"attributes\":{\"id\":\"formula_15\"},\"end\":11473,\"start\":11455},{\"attributes\":{\"id\":\"formula_16\"},\"end\":11616,\"start\":11555},{\"attributes\":{\"id\":\"formula_17\"},\"end\":12944,\"start\":12307},{\"attributes\":{\"id\":\"formula_18\"},\"end\":13443,\"start\":13375},{\"attributes\":{\"id\":\"formula_19\"},\"end\":13548,\"start\":13450},{\"attributes\":{\"id\":\"formula_20\"},\"end\":13748,\"start\":13709},{\"attributes\":{\"id\":\"formula_21\"},\"end\":13952,\"start\":13872},{\"attributes\":{\"id\":\"formula_22\"},\"end\":14105,\"start\":14070},{\"attributes\":{\"id\":\"formula_23\"},\"end\":14355,\"start\":14312},{\"attributes\":{\"id\":\"formula_24\"},\"end\":14614,\"start\":14545},{\"attributes\":{\"id\":\"formula_25\"},\"end\":14715,\"start\":14622},{\"attributes\":{\"id\":\"formula_26\"},\"end\":15465,\"start\":15091},{\"attributes\":{\"id\":\"formula_27\"},\"end\":15557,\"start\":15481},{\"attributes\":{\"id\":\"formula_28\"},\"end\":17115,\"start\":16815},{\"attributes\":{\"id\":\"formula_29\"},\"end\":17260,\"start\":17215},{\"attributes\":{\"id\":\"formula_30\"},\"end\":18595,\"start\":18478},{\"attributes\":{\"id\":\"formula_31\"},\"end\":18793,\"start\":18611},{\"attributes\":{\"id\":\"formula_32\"},\"end\":19220,\"start\":19109},{\"attributes\":{\"id\":\"formula_33\"},\"end\":20473,\"start\":20354},{\"attributes\":{\"id\":\"formula_34\"},\"end\":20567,\"start\":20534},{\"attributes\":{\"id\":\"formula_35\"},\"end\":20801,\"start\":20666},{\"attributes\":{\"id\":\"formula_36\"},\"end\":21019,\"start\":20984},{\"attributes\":{\"id\":\"formula_37\"},\"end\":22691,\"start\":22661},{\"attributes\":{\"id\":\"formula_38\"},\"end\":22767,\"start\":22691},{\"attributes\":{\"id\":\"formula_39\"},\"end\":23060,\"start\":23023},{\"attributes\":{\"id\":\"formula_40\"},\"end\":23343,\"start\":23060},{\"attributes\":{\"id\":\"formula_41\"},\"end\":24774,\"start\":24724},{\"attributes\":{\"id\":\"formula_42\"},\"end\":25796,\"start\":25689},{\"attributes\":{\"id\":\"formula_43\"},\"end\":26323,\"start\":26281},{\"attributes\":{\"id\":\"formula_44\"},\"end\":26504,\"start\":26429},{\"attributes\":{\"id\":\"formula_45\"},\"end\":26797,\"start\":26718},{\"attributes\":{\"id\":\"formula_46\"},\"end\":27123,\"start\":27063},{\"attributes\":{\"id\":\"formula_47\"},\"end\":28497,\"start\":28462},{\"attributes\":{\"id\":\"formula_48\"},\"end\":28552,\"start\":28539},{\"attributes\":{\"id\":\"formula_49\"},\"end\":28682,\"start\":28632},{\"attributes\":{\"id\":\"formula_50\"},\"end\":29256,\"start\":28690},{\"attributes\":{\"id\":\"formula_51\"},\"end\":29720,\"start\":29565},{\"attributes\":{\"id\":\"formula_52\"},\"end\":29938,\"start\":29843},{\"attributes\":{\"id\":\"formula_53\"},\"end\":30078,\"start\":29948},{\"attributes\":{\"id\":\"formula_54\"},\"end\":30367,\"start\":30236},{\"attributes\":{\"id\":\"formula_55\"},\"end\":30479,\"start\":30425},{\"attributes\":{\"id\":\"formula_56\"},\"end\":30579,\"start\":30501},{\"attributes\":{\"id\":\"formula_57\"},\"end\":30876,\"start\":30780},{\"attributes\":{\"id\":\"formula_58\"},\"end\":31056,\"start\":30934},{\"attributes\":{\"id\":\"formula_59\"},\"end\":31217,\"start\":31139},{\"attributes\":{\"id\":\"formula_60\"},\"end\":31858,\"start\":31695},{\"attributes\":{\"id\":\"formula_61\"},\"end\":31979,\"start\":31935},{\"attributes\":{\"id\":\"formula_62\"},\"end\":32800,\"start\":32600},{\"attributes\":{\"id\":\"formula_63\"},\"end\":32851,\"start\":32808}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21820,\"start\":21813},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23020,\"start\":23013}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1623,\"start\":1611},{\"attributes\":{\"n\":\"2\"},\"end\":4397,\"start\":4387},{\"attributes\":{\"n\":\"3\"},\"end\":8041,\"start\":7977},{\"attributes\":{\"n\":\"4\"},\"end\":10606,\"start\":10549},{\"attributes\":{\"n\":\"4.1\"},\"end\":11454,\"start\":11444},{\"attributes\":{\"n\":\"4.2\"},\"end\":14731,\"start\":14717},{\"attributes\":{\"n\":\"4.4\"},\"end\":17941,\"start\":17914},{\"attributes\":{\"n\":\"5\"},\"end\":18857,\"start\":18795},{\"attributes\":{\"n\":\"5.2\"},\"end\":20983,\"start\":20973},{\"attributes\":{\"n\":\"6\"},\"end\":21246,\"start\":21226},{\"attributes\":{\"n\":\"7\"},\"end\":21834,\"start\":21824},{\"end\":27467,\"start\":27436},{\"end\":29748,\"start\":29722},{\"end\":31650,\"start\":31624},{\"end\":32151,\"start\":32129},{\"end\":32858,\"start\":32852},{\"end\":33147,\"start\":33137},{\"end\":34089,\"start\":34079},{\"end\":34657,\"start\":34647},{\"end\":34696,\"start\":34695},{\"end\":34919,\"start\":34918},{\"end\":35178,\"start\":35155},{\"end\":35551,\"start\":35541},{\"end\":35888,\"start\":35879},{\"end\":36242,\"start\":36237},{\"end\":37012,\"start\":37003},{\"end\":37433,\"start\":37424}]", "table": "[{\"end\":37422,\"start\":37086},{\"end\":37509,\"start\":37483}]", "figure_caption": "[{\"end\":33135,\"start\":32860},{\"end\":33442,\"start\":33149},{\"end\":33874,\"start\":33445},{\"end\":34077,\"start\":33877},{\"end\":34274,\"start\":34091},{\"end\":34645,\"start\":34277},{\"end\":34693,\"start\":34659},{\"end\":34916,\"start\":34697},{\"end\":35153,\"start\":34920},{\"end\":35539,\"start\":35182},{\"end\":35877,\"start\":35553},{\"end\":35983,\"start\":35890},{\"end\":36235,\"start\":35986},{\"end\":36529,\"start\":36244},{\"end\":37001,\"start\":36532},{\"end\":37086,\"start\":37014},{\"end\":37483,\"start\":37435}]", "figure_ref": "[{\"end\":23665,\"start\":23658},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23717,\"start\":23708},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":24012,\"start\":23992},{\"end\":24389,\"start\":24382},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":24465,\"start\":24443},{\"end\":24538,\"start\":24531},{\"end\":24589,\"start\":24580},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":24924,\"start\":24917},{\"end\":25035,\"start\":25026},{\"end\":25066,\"start\":25057},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27478,\"start\":27472},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28177,\"start\":28171},{\"end\":29449,\"start\":29446}]", "bib_author_first_name": "[{\"end\":37741,\"start\":37733},{\"end\":37975,\"start\":37974},{\"end\":37977,\"start\":37976},{\"end\":38275,\"start\":38272},{\"end\":38279,\"start\":38276},{\"end\":38539,\"start\":38531},{\"end\":38559,\"start\":38550},{\"end\":38872,\"start\":38869},{\"end\":38888,\"start\":38883},{\"end\":38904,\"start\":38897},{\"end\":39172,\"start\":39171},{\"end\":39189,\"start\":39183},{\"end\":39191,\"start\":39190},{\"end\":39389,\"start\":39384},{\"end\":39698,\"start\":39690},{\"end\":39986,\"start\":39982},{\"end\":40003,\"start\":39998},{\"end\":40259,\"start\":40251},{\"end\":40268,\"start\":40264},{\"end\":40281,\"start\":40275},{\"end\":40605,\"start\":40604},{\"end\":40938,\"start\":40932},{\"end\":41206,\"start\":41199},{\"end\":41551,\"start\":41544},{\"end\":41818,\"start\":41813},{\"end\":42154,\"start\":42147},{\"end\":42476,\"start\":42469},{\"end\":42705,\"start\":42700},{\"end\":42719,\"start\":42713},{\"end\":42948,\"start\":42943},{\"end\":43135,\"start\":43130},{\"end\":43362,\"start\":43357},{\"end\":43587,\"start\":43582},{\"end\":44115,\"start\":44109},{\"end\":44129,\"start\":44124},{\"end\":44148,\"start\":44142},{\"end\":44150,\"start\":44149},{\"end\":44567,\"start\":44560}]", "bib_author_last_name": "[{\"end\":37747,\"start\":37742},{\"end\":37983,\"start\":37978},{\"end\":37989,\"start\":37985},{\"end\":38285,\"start\":38280},{\"end\":38548,\"start\":38540},{\"end\":38566,\"start\":38560},{\"end\":38881,\"start\":38873},{\"end\":38895,\"start\":38889},{\"end\":38910,\"start\":38905},{\"end\":39181,\"start\":39173},{\"end\":39197,\"start\":39192},{\"end\":39206,\"start\":39199},{\"end\":39398,\"start\":39390},{\"end\":39711,\"start\":39699},{\"end\":39996,\"start\":39987},{\"end\":40009,\"start\":40004},{\"end\":40262,\"start\":40260},{\"end\":40273,\"start\":40269},{\"end\":40288,\"start\":40282},{\"end\":40613,\"start\":40606},{\"end\":40625,\"start\":40615},{\"end\":40944,\"start\":40939},{\"end\":41210,\"start\":41207},{\"end\":41556,\"start\":41552},{\"end\":41821,\"start\":41819},{\"end\":42159,\"start\":42155},{\"end\":42481,\"start\":42477},{\"end\":42711,\"start\":42706},{\"end\":42724,\"start\":42720},{\"end\":42952,\"start\":42949},{\"end\":43144,\"start\":43136},{\"end\":43374,\"start\":43363},{\"end\":43595,\"start\":43588},{\"end\":44122,\"start\":44116},{\"end\":44140,\"start\":44130},{\"end\":44162,\"start\":44151},{\"end\":44575,\"start\":44568}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3897405},\"end\":37932,\"start\":37693},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":49310446},\"end\":38171,\"start\":37934},{\"attributes\":{\"doi\":\"arXiv:2210.05148\",\"id\":\"b2\"},\"end\":38482,\"start\":38173},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":234357997},\"end\":38792,\"start\":38484},{\"attributes\":{\"doi\":\"arXiv:2112.07068\",\"id\":\"b4\"},\"end\":39109,\"start\":38794},{\"attributes\":{\"id\":\"b5\"},\"end\":39312,\"start\":39111},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":119513587},\"end\":39580,\"start\":39314},{\"attributes\":{\"id\":\"b7\"},\"end\":39913,\"start\":39582},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1152227},\"end\":40207,\"start\":39915},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219955663},\"end\":40509,\"start\":40209},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":120969358},\"end\":40890,\"start\":40511},{\"attributes\":{\"doi\":\"arXiv:2201.11793\",\"id\":\"b11\"},\"end\":41081,\"start\":40892},{\"attributes\":{\"doi\":\"PMLR. 2022\",\"id\":\"b12\",\"matched_paper_id\":248218418},\"end\":41483,\"start\":41083},{\"attributes\":{\"doi\":\"arXiv:2009.09761\",\"id\":\"b13\"},\"end\":41712,\"start\":41485},{\"attributes\":{\"doi\":\"PMLR. 2022\",\"id\":\"b14\",\"matched_paper_id\":249712167},\"end\":42074,\"start\":41714},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":243847971},\"end\":42391,\"start\":42076},{\"attributes\":{\"doi\":\"arXiv:2108.01073\",\"id\":\"b16\"},\"end\":42654,\"start\":42393},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":123338984},\"end\":42894,\"start\":42656},{\"attributes\":{\"doi\":\"arXiv:2205.07460\",\"id\":\"b18\"},\"end\":43093,\"start\":42896},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14529726},\"end\":43307,\"start\":43095},{\"attributes\":{\"doi\":\"arXiv:2206.01018\",\"id\":\"b20\"},\"end\":43518,\"start\":43309},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":245335280},\"end\":43954,\"start\":43520},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":57379996},\"end\":44478,\"start\":43956},{\"attributes\":{\"doi\":\"arXiv:2205.11487\",\"id\":\"b23\"},\"end\":44755,\"start\":44480}]", "bib_title": "[{\"end\":37731,\"start\":37693},{\"end\":37972,\"start\":37934},{\"end\":38529,\"start\":38484},{\"end\":39382,\"start\":39314},{\"end\":39980,\"start\":39915},{\"end\":40249,\"start\":40209},{\"end\":40602,\"start\":40511},{\"end\":41197,\"start\":41083},{\"end\":41811,\"start\":41714},{\"end\":42145,\"start\":42076},{\"end\":42698,\"start\":42656},{\"end\":43128,\"start\":43095},{\"end\":43580,\"start\":43520},{\"end\":44107,\"start\":43956}]", "bib_author": "[{\"end\":37749,\"start\":37733},{\"end\":37985,\"start\":37974},{\"end\":37991,\"start\":37985},{\"end\":38287,\"start\":38272},{\"end\":38550,\"start\":38531},{\"end\":38568,\"start\":38550},{\"end\":38883,\"start\":38869},{\"end\":38897,\"start\":38883},{\"end\":38912,\"start\":38897},{\"end\":39183,\"start\":39171},{\"end\":39199,\"start\":39183},{\"end\":39208,\"start\":39199},{\"end\":39400,\"start\":39384},{\"end\":39713,\"start\":39690},{\"end\":39998,\"start\":39982},{\"end\":40011,\"start\":39998},{\"end\":40264,\"start\":40251},{\"end\":40275,\"start\":40264},{\"end\":40290,\"start\":40275},{\"end\":40615,\"start\":40604},{\"end\":40627,\"start\":40615},{\"end\":40946,\"start\":40932},{\"end\":41212,\"start\":41199},{\"end\":41558,\"start\":41544},{\"end\":41823,\"start\":41813},{\"end\":42161,\"start\":42147},{\"end\":42483,\"start\":42469},{\"end\":42713,\"start\":42700},{\"end\":42726,\"start\":42713},{\"end\":42954,\"start\":42943},{\"end\":43146,\"start\":43130},{\"end\":43376,\"start\":43357},{\"end\":43597,\"start\":43582},{\"end\":44124,\"start\":44109},{\"end\":44142,\"start\":44124},{\"end\":44164,\"start\":44142},{\"end\":44577,\"start\":44560}]", "bib_venue": "[{\"end\":37792,\"start\":37749},{\"end\":38040,\"start\":37991},{\"end\":38270,\"start\":38173},{\"end\":38617,\"start\":38568},{\"end\":38867,\"start\":38794},{\"end\":39169,\"start\":39111},{\"end\":39429,\"start\":39400},{\"end\":39688,\"start\":39582},{\"end\":40051,\"start\":40011},{\"end\":40339,\"start\":40290},{\"end\":40682,\"start\":40627},{\"end\":40930,\"start\":40892},{\"end\":41266,\"start\":41222},{\"end\":41542,\"start\":41485},{\"end\":41877,\"start\":41833},{\"end\":42210,\"start\":42161},{\"end\":42467,\"start\":42393},{\"end\":42756,\"start\":42726},{\"end\":42941,\"start\":42896},{\"end\":43179,\"start\":43146},{\"end\":43355,\"start\":43309},{\"end\":43684,\"start\":43597},{\"end\":44196,\"start\":44164},{\"end\":44558,\"start\":44480},{\"end\":43758,\"start\":43686}]"}}}, "year": 2023, "month": 12, "day": 17}