{"id": 236318126, "updated": "2022-12-23 10:00:03.455", "metadata": {"title": "Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution", "authors": "[{\"first\":\"Patrik\",\"last\":\"Puchert\",\"middle\":[]},{\"first\":\"Timo\",\"last\":\"Ropinski\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Conventional methods for human pose estimation either require a high degree of instrumentation, by relying on many inertial measurement units (IMUs), or constraint the recording space, by relying on extrinsic cameras. These de\ufb01cits are tackled through the approach of human pose estimation from sparse IMU data. We de\ufb01ne adjacency adaptive graph convolutional long-short term memory networks (AAGC-LSTM), to tackle human pose estimation based on six IMUs, while incorporating the human body graph structure directly into the network. The AAGC-LSTM com-bines both spatial and temporal dependency in a single network operation, more memory ef\ufb01ciently than previous approaches. This is made possible by equipping graph convolutions with adjacency adaptivity, which eliminates the problem of information loss in deep or recurrent graph networks, while it also allows for learning unknown dependencies between the human body joints. To further boost accuracy, we propose longitudinal loss weighting to consider natural movement patterns. With our presented approach, we are able to utilize the inherent graph nature of the human body, and thus can outperform the state of the art (SOTA) for human pose estimation from sparse IMU data.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2107-11214", "doi": null}}, "content": {"source": {"pdf_hash": "15e70fa092db1644a3c29991bd92737359bcd0a2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2107.11214v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4dc4f3e702acbd30540ec0881a03a13e40579f8c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/15e70fa092db1644a3c29991bd92737359bcd0a2.txt", "contents": "\nHuman Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution\n\n\nPatrik Puchert patrik.puchert@uni-ulm.de \nInstitute of Mediainformatics\nInstitute of Mediainformatics\nUlm University\n89081UlmGermany\n\nTimo Ropinski \nUlm University\n89081UlmGermany\n\nHuman Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution\n\nConventional methods for human pose estimation either require a high degree of instrumentation, by relying on many inertial measurement units (IMUs), or constraint the recording space, by relying on extrinsic cameras. These deficits are tackled through the approach of human pose estimation from sparse IMU data. We define adjacency adaptive graph convolutional long-short term memory networks (AAGC-LSTM), to tackle human pose estimation based on six IMUs, while incorporating the human body graph structure directly into the network. The AAGC-LSTM combines both spatial and temporal dependency in a single network operation, more memory efficiently than previous approaches. This is made possible by equipping graph convolutions with adjacency adaptivity, which eliminates the problem of information loss in deep or recurrent graph networks, while it also allows for learning unknown dependencies between the human body joints. To further boost accuracy, we propose longitudinal loss weighting to consider natural movement patterns. With our presented approach, we are able to utilize the inherent graph nature of the human body, and thus can outperform the state of the art (SOTA) for human pose estimation from sparse IMU data.\n\nIntroduction\n\nA correct estimation of human poses is important in many applications. These range from various applications in virtual and augmented reality [21,26] to medical applications, such as gait analysis [29], patient monitoring [2] or human activity recognition [3]. Unfortunately, today's SOTA methods for human pose estimation either only work in constrained environments, or are very intrusive [7,45,47]. These constraints make them impractical for outdoor applications, indoor scenarios spanning multiple rooms or suffering from occlusions [8]. These deficits can be solved by measuring the human body pose with body-worn IMUs [16]. To aid user acceptance and usability, the number of body mounted sensors must be minimal, resulting in sparse inertial measurements. In this paper, we propose a novel approach for human pose estimation based on a set of 6 IMUs. While this scenario has been tackled by others before [38,12,44,43], we are the first to enhance pose estimation accuracy by incorporating the structure of the human body through deep graph learning, instead of predicting the pose from a flat array of input data. While using graph structures in deep neural networks is a well studied field [40], the usage of standard graph convolutions in recurrent architectures poses the same over-smoothing problem as in other deep graph architectures [20]. To address this challenge, we propose adjacency adaptive graph convolutions (AAGCs), which we realize in a novel recurrent long-short term memory (LSTM) architecture. To do so, we fuse the AAGC with the LSTM by using it as the neural building block inside the LSTM. This connects the spatial and temporal computations in AAGC-LSTM, whereby the adjacency adaptive component avoids over-smoothing, which would result in a constant average pose. With this new approach we do not only outperform the SOTA but also require significantly less memory during training, as compared to other approaches combining graph learning with recurrent cells in different fields [19]. To further improve pose estimation, we show how AAGC-LSTMs benefit from respecting the nature of human movements as well as bilateral body symmetry, for which we propose longitudinal loss weighting and utilize contralateral data augmentation. The former solves the problem of underrepresented body regions in recorded movements and a more difficult estimation of the legs based on sparse IMU data [12]. The latter simply augments the available training data by respecting the bilateral symmetry of the human body, by mirroring all movements. Thus, within this paper we propose the first graph convolution approach to solve sparse IMU-based pose estimation, and we make the following technical contributions in this context:\n\n\u2022 We introduce the AAGC-LSTM cell as a memory efficient recurrent graph LSTM formulation incorporating learnable adjacency matrices, to address the oversmoothing problem of recurrent graph convolutional networks.\n\n\u2022 We propose longitudinal loss weighting, to adapt for underrepresentation of specific joints in the training data based on movement statistics.\n\n\u2022 Using our proposed AAGC-LSTM model with longitudinal loss weighting we outperform the SOTA for sparse IMU based human pose estimation.\n\nWhile we make these technical contributions specifically for IMU-based human pose estimation, we would like to emphasize that they are not restricted to this task and could likely be applied to other pose estimation scenarios without IMUs, as well as for other body types.\n\n\nRelated Work\n\nIMU-based pose estimation. Traditional motion Capture technologies constrain the recording volume by relying on extrinsic sensors like cameras [5,49,17,27,32,41]. This problem is lifted by completely body-worn systems. While other systems have been proposed [13], IMU based systems have the benefit of being light weight, small and widely available. Today, such systems are commercially available, whereby IMU measurements from different units are fused to reconstruct a subject's pose [28]. While they yield good results, their large amount of required IMUs, e.g., 17 IMUs in Xsens' current MTw Awinda setup for full pose estimation 1 , can be considered intrusive, require long setup times, and provoke sensor placement errors. To tackle these shortcomings, several methods have been proposed to reduce the amount of necessary sensors. Many such approaches are limited in application by matching query samples to prerecorded databases [35,36] or training a model for each activity of interest [31]. Von Marcard et al. instead use a generative approach to estimate 3D poses from only 6 IMUs with their sparse inertial poser (SIP) [38]. To do so, they equip the Skinned Multi-Person Linear (SMPL) body model [22] with synthetic IMUs, and solve for the SMPL pose that best matches the measured IMU data. As this has to be done at query time, it is a computaionally expensive procedure. With the deep inertial poser (DIP) Huang et al. have enhanced the accuracy of pose estimation based on 6 IMUs using a deep learning approach [12]. They built a bidirectional LSTM network [30] to map the flattened input of 5 IMUs, normalized by a 6th IMU on the pelvis, to the target pose in the SMPL body model. In contrast to the deep learning approaches the use of shallow fully connected networks 1 https://www.xsens.com/products/mtw-awinda has been proposed [39]. Yi et al. proposed an improvement  for the deep architectures by predicting intermediate representations, along with a network branch for global position estimation [44]. With their Transpose network they effectively apply the network of DIP on three subsequent steps, where they first predict the position of leaf joints from the IMU data, then the position of all joints from the former joints as well as the IMU data, and finally the target pose in the SMPL model using the IMU data and the position of all joints as input. Recently they further improved the accuracy by introducing a physics based module to postprocess the model predictions [43].\n\nGraph learning. Graph learning has been used for human body related tasks, by mapping a 2D pose, predicted from static RGB images, to a 3D pose [45,50], eliminating the need to cope with time dependencies. Several approaches have been proposed which work with spatio-temporal data by separating the spatial and temporal learning steps. These separations are either a consecutive application of graph convolutions (GCN) and recurrent layers [46,25], or of GCNs in space and convolutions in time domain consecutively [42,33]. Some of these methods modulate the fixed adjacency of the underlying graph by incorporating learnable adjacency components. The combination of spatial and temporal dependencies has been proposed for different tasks. Bai et al. build the adjacency matrix out of a learnable embedding of all graph nodes and use them in gated recurrent units (GRUs) [4] for traffic forecasting. While such an embedding has benefits for large graphs, it is not beneficial for the small graph of our problem. Li et al. have proposed graph-based GRU cells (G-GRU) for human motion prediction, in which they also employ an adaptive adjacency matrix [19]. While they keep a fixed adjacency matrix, they modulate it with a multiplicative and an additive weight matrix before using it in a GCN to modify the cells hidden input state. While we consider it counter-intuitive to just add a graph computation to the network while keeping the linear computation, it also requires a significantly larger memory footprint than our approach, as we will detail below. LSTMs differ from GRUs by having a better deep context understanding [10], which makes them better suited to tasks such as human pose estimation, where input sequences can consist of data recorded with 60Hz, while the context of movements can span from only a few frames to many seconds. Si et al. use a combination of LSTMs and GCNs with fixed adjacency together with an attention mechanism for human activity recognition (AGC-LSTM). While they obtain good results, their model does not scale well to our problem with indefinitely long sequences and sparse inputs. While the over-smoothing problem of deep GCN networks [20] has been tackled in the spatial domain [6] by introducing initial residual connections and identity mapping to the GCN, this is not applicable to recurrent architectures.  Figure 1: We learn towards the SMPL skeleton (light gray), whereby we limit the joint connectivity (dark gray) to match influence area of the 5 normalized IMUs.\n\n\nThe Method\n\nIn this section, we detail the technical concepts behind our proposed graph convolution approach. The input of our model is the data of 5 IMUs placed on the SMPL skeletal graph, whereby their input is transformed into a body-centric system as per Huang et al. by a 6th IMU on the pelvis [12]. We also train towards the SMPL body model [22] as target, in order to obtain realistic poses, and allow for comparison to the SOTA. To incorporate body topology, we transform the IMU input to a graph structure by automatically placing the sensors on a human skeletal graph at the corresponding nodes. For this graph we chose the SMPL skeletal model, whereby we focus on only 15 core joints out of the 24 joints of the model [22], i.e., without the outer extremities of hands, feet and the root joint (see Fig. 1). We chose this focus due to the fact, that the information sparse IMUs can give for the former is naturally limited, while the root joint at the Pelvis is fixed by definition through the data transformation with respect to it. The nodes containing no IMU measurements are initialized with zeros in the input graph. Thus, we operate on an input graph of dimensionality N \u00d7 F in , where N = 15 and F in = 12 are the input features given by the elements of a 3x3 rotation matrix and a three dimensional acceleration vector. The training target is the same graph with the SMPL pose parameters \u03b8 on the corresponding nodes. Adjacency adaptive graph convolution. The core of our model is the combination of LSTM cells and GCNs in a bidirectional recurrent layer. As the inclusion of standard GCNs in recurrent applications suffers from the oversmoothing problems as described by Li et al. [20], we define the adjacency adaptive graph convolution (AAGC). The definition of AAGC follows the notation of the commonly used approximation of the graph convolution as proposed by Kipf and Welling [15], with the propagation rule:\nZ =\u00c3XW + b,(1)\nwhere X is the input and W and b are the trainable weights and biases. In standard GCNs\u00c3 = D \u2212 1 2 (A + I N )D 1 2 is the constant symmetric normalization of the adjacency matrix A with added self-connections I N using the diagonal node degree matrix D of A. To now employ adjacency adaptivity, we make\u00c3 a learnable matrix, initialized by the normalized complemented distance on the graph:\nA init ij = 1 \u2212 d(n i , n j ) j d(n i , n j ) ,(2)\nwhere d(n i , n j ) is the euclidean distance between node i and node j on the graph. In addition to the necessity of adjacency adaptivity in our application we assume another benefit of the learnable adjacency, which is the hardly factorizable dependency of all joints to each other. The problem in factorization of all joint dependencies comes from the fact, that biological joints are not only actuators driven by many muscles, but even in a free state underlie many factors of dampening [24,18]. Thus the free joint, i.e. one not actively driven by muscular movement, is affected by all other joints both through the connection along a line of damped oscillators, as well as by inertia, as the skeleton is neither completely stiff. While it is true that all effects will always also effect the neighbouring joints, it can be better for the model to learn dependencies to the other joints as well, as this will give the context to small changes in acceleration or rotation. Using this definition of the AAGC operation inside LSTM cells (AAGC-LSTM) thus lifts the over-smoothing problem. The definition of the AAGC-LSTM cell however follows on a coarse level the definition of the conventional LSTM cell [11,9], whereby in the AAGC-LSTM cell we simply replace every learnable with AAGCs:\nX i = \u03c3(\u00c3 i XW i + b i ) X f = \u03c3(\u00c3 f XW f + b f ) X c = tanh(\u00c3 c XW c + b c ) X o = \u03c3(\u00c3 o XW o + b o ),(3)\nwhere X is the concatenation of the current input and the last hidden state H t\u22121 , to both of which dropout is applied, and the index t denotes the timestep.\u00c3, W and b are the adjacency and weight matrices and biases of the AAGC, and \u03c3 is the sigmoid activation function. The gates X {i,f,c,o} are then processed with the common LSTM scheme [11]:\nC t = C t\u22121 X i + X f X c H t = \u03c3 out (C t ) X o O t = \u03c3 out (H t ),(4)\nwhere O is the cells output and C is the cells carry respectively. \u03c3 out is the output activation function, which is realized as tanh function for the hidden AAGC-LSTM layer.\n\nThus, we are able to combine the spatial and temporal learning part in a single recurrent cell, and are able to learn all dependencies of a current state in a single computation. This is desirable as the spatial context needs to be accessible for every joint to define the pose. Furthermore, since the single pose is only a discrete timestep in a continuous movement, the temporal dependency must be accessible to consider coherence and consider the pose in the context of an animation. As such, the pose of a raised arm could inherit from a movement of a raising arm or of relaxing an arm from a raised pose, in which the sensor rotation values will be identical while the temporal information puts the sparse acceleration data in a context better suited for robust estimation.\n\nNumber of Parameters Compared. The formulation of AAGCs has a great benefit compared to the similar graphbased gated recurrent unit (G-GRU) formulation proposed by Li et al. [19]. Applied to the data matrices of size N \u00b7 F i , where N is the number of nodes in the graph and F i the layers input feature size, we have (F i + F h ) \u00b7 F o parameters in each of the four linear operations of the standard LSTM cell, with the layers hidden state and output feature size F h and F o . By replacing the linear operations with our AAGCs, we increase the number of parameters to\nN 2 + (F i + F h ) \u00b7 F o , thus increasing the parameter count of one cell by 4 \u00b7 N 2 .\nThe approach of G-GRU applied to an LSTM cell would instead keep the linear operations and add one adjacency adaptive graph convolution on the hidden state, resulting in a total increase of N 2 + F h \u00b7 F o . We can simplify this to N 2 + F 2 h by knowing F i = F o = F h for the LSTM layers in our network. With our hidden feature size of 512 we result in a total difference in the number of parameters N per recurrent layer of N G-GRU \u2212 N AAGC-LSTM = 261, 469. Applied to our complete network setup, this difference results in 9, 460, 047 parameters for the G-GRUs and 8, 412, 123 parameters for the AAGC-LSTMs, a reduction by 11%.\n\nModel Architecture and Training Objective. We define the general layout of our model in Fig. 2. The graph data is first fed to an AAGC layer, then to two bidirectional AAGC-LSTM layers and finally to another AAGC layer. In addition to this standalone realization, our approach can also be utilized inside higher level training schemes, such as the Transpose pipeline [44]. While the pathway of Transpose for pose estimation is essentially a combination of three bidirectional LSTM networks, akin to the one proposed with DIP [12], we can replace all these networks with our bidirectional AAGC-LSTM network. In such a setup, first one network predicts the position of the leaf joints, i.e., hands, feet and head from the IMU input. The second network then takes the concatenation of these leaf positions and the IMU input to predict the position of all joints, which are then again fed to the third network together with the IMU input to predict the pose. The networks used in the original Transpose model operate on flattened data, sim-ilar to DIP, which requires us though to make a few adjustments for such a setup. To allow for the same graph in each network, while also keeping the same graph structure our standalone model employs, we do not directly feed the root joint data as input to the model and predict the position of the outermost joints in the first task, i.e., elbows, knees and head, as the leaf joints (hands and feet) are not part of our graph. Similarly, the prediction of the position of all joints with the second network is reduced to only the 15 joints which are part of our graph. Furthermore, we also differ from Transpose by not training towards the reduced 6-dimensional representation of rotation matrices [48], but towards the conventional 9D representation instead, as we empirically found better results for this representation. A short elaboration on these results can be found in the supplemental material. Apart from the split of the learning task in three steps, our two models are trained in the same fashion, with the only difference being the scale normalization of the input data. For our Transpose style model we followed Transpose and rescaled the acceleration data by a constant factor of 30, while we did not adapt any scale normalization for the input data as we found this to achieve better results. In both setups, standalone and Transpose, the models are trained towards global rotations, using an MSE loss function utilizing longitudinal loss weighting:\nL \u03b8 = 1 T T t=1 N n=1 \u03bb n F =9 f =1 (y true t,n,f \u2212 y pred t,n,f ) 2 .(5)\nwhere L \u03b8 is the target loss of SMPL model parameter predictions. The sum over T defines the sequence mean, N denotes the number of joints, F the number of features and \u03bb denotes a joint specific weight.\n\nLongitudinal loss weighting. When analyzing human movement data, a greater variance and angular range of arm movements as compared to leg movements can be observed. While the training data, especially the large synthetic corpus, is assumed to be a good representation of common movements, the statistics of different body parts show significant differences. As such, we have measured the standard deviation of the transformed and to unit range normalized rotation angles of the joints corresponding to arms, legs and torso in the target data of the training corpus with \u03c3 legs = 0.397, \u03c3 arms = 0.473 and \u03c3 torso = 0.24. Thus, we can see a significant difference in the different body-regions. The arms show a significant prevalence with respect to the legs and the torso. While the torso shows the most significant difference to the other body regions, it should be noted that the flexibility of the torso with respect to the root pelvis joint is highly limited. In contrast to that, the legs and arms have a comparable flexibility, while the difference in the underlying variance can be explained by the simple fact that a big part of the data consists of upright  movements, which constrain the pose of the legs to such allowing for the subject to stand. As a larger variance in specific joints also leads to a higher loss contribution by these joints, the network will tend to learn better towards the regions with higher variance. To make up for this fact, we exploit longitudinal loss weighting (LLW), as a simple nodebased weighting scheme, which scales up the loss contribution of the lower body and scales down the loss contribution of the upper body, such that the legs are trained with the same accuracy as the arms. To do so, we set the values for \u03bb empirically as \u03bb n = 0.25 if n \u2208 N torso , \u03bb n = 0.5 if n \u2208 N arms and \u03bb n = 1 if n \u2208 N legs , with the joint sets defined as N legs = {0, 1, 3, 4}, N arms = {8, 9, 11, 12, 13, 14} and N torso = {2, 5, 6, 7, 10}. For details on the analysis leading to this selection, please refer to the supplemental material. Contralateral data augmentation. Similar to Huang et al. [12], we train our model on synthetic IMU data, generated from the AMASS motion capture dataset [23] by utilizing the SMPL body model. Specifically, we use the preselected and preprocessed data, provided by Huang et al. 2 With the contralateral data augmentation (CDA) we essentially mirror every sequence along the body's main axis. Thus, we utilize the joint based bilateral symmetry of the human body, in order to enhance the set of movements both for training and validation, as well as reducing bias in the data contributing to any left-or right-handedness in the movements.\n\n\nExperiments\n\nWe evaluate our method and compare against DIP and Transpose which define the SOTA for sparse IMU-driven pose estimation. For this, we compare the methods quantitatively and qualitatively on the DIP-IMU dataset [12] and the Total Capture dataset [37]. For the implementation of our model we use TensorFlow version 2.3 [1], whereby op-2 https://dip.is.tuebingen.mpg.de/ timization is conducted with the Adam optimizer [14] with an initial learning rate of 0.001 and an exponential decay with a rate of 0.96 and a decay step of 2, 000. With the dropout on the input we employ a rate of 0.2 and on the cells hidden state we employ a dropout rate of 0.3. The gradients are further clipped to a norm of 1. While Yi et al. [44] train every network of their Transpose model separately, we note that we trained our Transpose-like model combined, as we found no benefits of separate training while it takes longer. Given this training procedure, we analyze the results of our best performing models, before analyzing the influence of our LLW and CDA.\n\n\nData Preparation\n\nAs mentioned above, the synthetic data used for training the models is provided by Huang et al. [12]. The positions necessary for Transpose and our model with Transpose setup were obtained by posing the SMPL model with the true pose parameters. The DIP-IMU dataset comes with preprocessed and raw data for each sequence. We used the raw data and filled any nan values via quadratic interpolation of acceleration and rotation in axis-angle representation respectively, while the positions are computed similar to the synthetic data. As the raw data is missing for one of the preprocessed sequences we also have one less sequence in our processed DIP-IMU dataset. For Total Capture [37] we used the same data preparation as Huang et al. to obtain calibrated orientation and acceleration values, while we took the ground truth from the AMASS dataset which provides the SMPL parameters for the Total Capture dataset among others [23], with positions again computed as before. For every model we report values of a model trained only on synthetic data as well as one fine-tuned on real data. The fine-tuning is conducted on subsets of the respective real datasets, while the test sets on which values are reported are disjunct from this. The corresponding data split for DIP-IMU is already provided with the dataset. For Total Cap-  Table 1: Evaluation of a single AAGC-LSTM network (Ours) and a AAGC-LSTM network in Transpose setup (Ours (TP)) compared to the state of the art and comparable approaches on DIP-IMU [12] and Total Capture [37]. We report the mean global angular error over the shoulder and hip joints (DIP Err), as well as the mean global angular error, the mean position error and the mean jerk error averaged over all 15 joints.\n\nture we first split the data by subjects, where the first three subjects are used for fine-tuning and the last two are used as the test set on which the metrics are reported. The former is again split randomly into training and validation sets for the fine-tuning, with a ration of 70% to 30%. The split by subjects for the test set allows for a complete test of the generalization capabilities of the model, as it otherwise could overfit to specific characteristics of single subjects.\n\n\nQuantitative Evaluation\n\nWe evaluate the methods on four metrics. These are the mean joint angle error with respect to the joints selected for analysis by Huang et al. [12] building on the work of SIP [38], as well as the mean joint error for angle, position and jerk with respect to all 15 joints. The first metric only evaluates the error on the shoulders and hip joints (0, 1, 11 and 12 according to the numbering introduced in Fig. 1), which is a meaningful decision to analyze the general capability of the model to generalize to the full pose from the IMU measurements, as there is no IMU data directly available for these joints, but they still are part of the extremities and thus a good indicator for the correctness of the pose. As this gives no direct information of the total accuracy for the pose estimation, we employ the other three metrics on the complete skeleton. Angle and position are used together to give a valid expression of the correctness of the pose, the error on the jerk as the third derivative of position measures the temporal stability of the prediction by quantifying effects such as trembling of joints or body parts in the prediction, which are still in the ground truth and vice versa. To get the values for position and jerk, we compute the respective joint positions using the SMPL body model given the true and predicted \u03b8 values. The jerk j is obtained from the respective true or predicted positions as a discrete value:\nj t = p t \u2212 3p t\u22121 + 3p t\u22122 \u2212 p t\u22123 \u2206t 3 ,(6)\nwhere t counts the frames, p is the position and \u2206t is the time per frame. We report the comparison of our models with the SOTA in Table 1, whereby we compare models trained solely on synthetic data as well as such fine-tuned on a disjunct subset of the DIP-IMU and Total Capture data, which are not part of the respective evaluation sets. We also compare to the methods of Li et al. (G-GRU) [19] and Si et al. (AGC-LSTM) [34]. As they are build for different tasks, we test them by replacing aour proposed AAGC-LSTMS with their respective formulation for recurrent cells. We observe that the inclusion of adjacency adaptive GCNs leads to great improvements on every metric but the jerk error. While the latter is always best for Transpose, we note that only the difference on the Total Capture dataset is of any significance, while at the same time Transpose scores significantly worse in every other metric. We further observe that the pairing of standard GCNs with an attention mechanism in the recurrent cell can not compete with our approach, while it is only slightly worse than Transpose. While the splitting of the pose estimation task as proposed by Transpose leads to great improvement in almost all metrics compared to DIP, we observe that this does not hold for its application on our proposed model, except for the evaluation on Total Capture with a synthetically trained model, where the results are still close. At this point we highlight that the Transpose style model is by nature three times as large as the standalone model and thus more compute intensive. Comparing the scores of all models we see that our approach outperforms both DIP and Transpose by a large margin in every metric but the jerk error. While we still outperform DIP in the latter with a rise in accuracy of about 50% in the worst case and about 87% in the best case, we do not outperform Transpose on this metric. Nevertheless, our jerk error is still comparably good for the DIP-IMU dataset and only beaten by Transpose on the Total Capture dataset, while our methods in turn show significantly better results on all other metrics. At the same time, the comparable techniques proposed in different fields can not compete with our achieved accuracy, while G-GRU still outperforms the current SOTA on our task. Thus, we conclude that our standalone model achieves the overall best results, both for a model trained only on synthetic data, as well as for one fine-tuned on real data.\n\n\nQualitative Evaluation\n\nFor the qualitative evaluation we visualized the poses using the SMPL model. As the models predict only the parameters for the 15 core joints, the remaining 8 joints for hands and feet are set to identity, while the pelvis joint is fixed. Thus, in all visualizations there is no further rotation applied to these regions. In Fig. 3 we show some examples of our proposed method compared to the ground truth and the SOTA for the DIP-IMU dataset. The top two rows show 2 representative frames out of those with the best score of our standalone model relative to Transpose. To select these we sort all frames from best to worst with respect to the relative score between our standalone model and Transpose, from which we selected the 10 leading examples with the additional constraint that at least 300 frames are between samples to assert a variation in the shown examples. The bottom two rows show 2 representative examples of the 10 worst scoring frames with respect to our standalone model, selected similarly. We observe that our model is better in reconstructing poses with bent legs as can be seen in the first example of a sitting pose. While there are still aspects to this pose which the model could not fully reproduce, as for instance the arm and leg positions are still quite off, we note that the reconstruction is visually closer to the ground truth than that of DIP and Transpose. In the second presented example we see an upright pose of a gesturing human. While every model can get this correctly in the coarse structure, our standalone model is much better in reconstructing the fine details of arm and leg positions, such as one straight and one slightly bent leg as well as the visually perfect arm poses.\n\nWith the examples of the worst scoring poses we can observe the two prime factors leading to problems. These are on one hand poses with high acceleration as can be seen with the jumping jack pose (third row), as well as such with local rotations of the torso with respect to the root (fourth row), as these are rare and short-timed in the training data.\n\nIn the former case no method can predict the correct extent of the arms and rotations of the lower legs, with visually equivalent poses given by our models and Transpose. While our model can predict the poses of arms and legs to some degree in the latter case, it fails to predict the torso rotation to any degree, while both competing methods have the same problems. In addition to this short discussion we encourage the reader to watch the supplemental video, in which we show more sequences in animation to get a better understanding of the quality of the predictions, as these are difficult to capture in static images.\n\n\nAblation Study\n\nIn this section we analyze the effects of the proposed longitudinal loss weighting and the contralateral data augmentation. We list the results of this ablation study in Table 2 Table 2: Ablation study on our standalone network. The rows indicate our best performing model, the same trained without longitudinal loss weighting (LLW) and the same trained without contralateral data augmentation (CDA).\n\nWe observe that for our model neither LLW nor CDA had any significant effect on the jerk error. For the other metrics however wee see a great increase in accuracy when incorporating both together. The application of LLW alone (ablation of CDA) results in a significant increase of the angular and position error, which is even surpassed by the application of CDA alone (ablation of LLW), while the best results are only obtained by using both LLW and CDA. While the jerk error shows the best result for the ablation of LLW, we note that all models are very close on this metric.\n\n\nConclusion and Future Work\n\nIn this paper we have shown that the utilization of the human body graph structure leads to better generalization towards pose estimation of unobserved movements. We base this on the following observations. (i) the estimation of the complete skeletal pose showed significant increases for both, a model trained purely on synthetic data, and one fine-tuned on real-world data with an angular error of only 8.13 degrees on DIP-IMU, around 15% less than the prior SOTA Transpose and 10.12 degrees on Total Capture, around 14% less than the prior SOTA. This was achieved by (ii) combining the spatio-temporal processing of the sequential movement data in a single step, for which we proposed the AAGC-LSTM cell, which processes both spatial and temporal dependencies of the data in one recurrent cell with a lower amount of parameters needed as existing methods. In addition to this we (iii) report a boost in accuracy when using longitudinal, node-based loss weighting on the graph and by (iv) utilizing the bilateral symmetry of the human body through contralateral data augmentation. All code necessary to reproduce our results, including the data preparation is publicly available on GitHub. 3 Future work could consider a deeper look into the longitudi- 3 Link will be provided upon acceptance of the paper. nal loss weights, and possibly make them incorporate case specific variability to better generalize on rare poses. As the model is trained on synthetic data, the differences of the synthetic IMUs to real IMUs should be explored to generate even more realistic data. Furthermore, the large difference between the scores on DIP-IMU and Total Capture for synthetically trained models suggest a great need for more real IMU datasets. The synthetic training of the models also allows for a exploration of optimal sensor positions. While all our contributions have been developed for human pose estimation from sparse IMU data, we would further like to investigate how they perform in other spatio-temporal learning scenarios.\n\nFigure 2 :\n2Layout of our proposed AAGC-LSTM network. The adjacency adaptive graph convolutions form the input and output layer and are used as learnable operation inside the recurrent AAGC-LSTM layers, which we define as a memory efficient formulation to combine spatiel and temporal dependencies in one cell. The feature sizes h = 512 and \u03b8 = 9 are the feature size per node in the graph, [ORI, ACC] are the concatenated orientation and acceleration values from the IMUs.\n\nFigure 3 :\n3Representative frames of DIP-IMU. From left to right we show the ground truth, DIP, Transpose, our approach and our approach with Transpose setup for different movements. On top we show two examples among the best scoring poses relative to Transpose and on the bottom two examples of the worst scoring poses with respect to our standalone model.\n\n\nDIP Err [deg] Ang Err [deg] Pos Err [cm] Jerk Err [ km s 3 ] DIP Err [deg] Ang Err [deg] Pos Err [cm] Jerk Err [ km s 3 ]Total Capture \n\nDIP-IMU \n\nTrained on synthetic data \n\nDIP [12] \n30.00(\u00b119.09) 24.24(\u00b115.18) 14.87(\u00b19.33) 4.92(\u00b16.85) 33.17(\u00b119.16) 24.33(\u00b115.17) 13.74(\u00b18.29) 3.60(\u00b15.66) \nTranspose [44] \n18.49(\u00b115.68) 13.78(\u00b19.40) 8.22(\u00b16.82) 0.64(\u00b11.97) 29.92(\u00b116.78) 12.46(\u00b17.32) 8.10(\u00b16.08 1.00(\u00b13.50) \nG-GRU [19] \n16.77(\u00b113.97) 13.38(\u00b19.06) 7.55(\u00b16.46) 0.81(\u00b12.11) 27.06(\u00b115.22) 11.86(\u00b16.80) 7.58(\u00b15.66 1.05(\u00b13.55) \nAGC-LSTM [34] 18.35(\u00b115.08) 13.89(\u00b19.81) 8.30(\u00b17.56) 0.80(\u00b12.22) 29.42(\u00b116.53) 13.38(\u00b17.99) 8.41(\u00b16.14 1.03(\u00b13.60) \nOurs \n15.81(\u00b112.38) 12.53(\u00b18.41) 7.27(\u00b15.32) 1.16(\u00b12.61) 28.12(\u00b114.28) 11.35(\u00b16.28) 7.73(\u00b15.61) 1.12(\u00b13.55) \nOurs (TP) \n15.77(\u00b113.22) 12.28(\u00b18.38) 7.02(\u00b15.58) 0.89(\u00b12.32) 29.04(\u00b114.42) 11.59(\u00b16.22) 8.10(\u00b15.73) 1.07(\u00b13.56) \n\nFine-tuned on real-world data \n\nDIP [12] \n17.45(\u00b115.59) 14.40(\u00b110.94) 8.26(\u00b17.26) 2.40(\u00b13.51) 17.75(\u00b111.77) 15.68(\u00b111.13) 7.71(\u00b15.43) 2.04(\u00b13.92) \nTranspose [44] \n17.03(\u00b114.74) 11.72(\u00b18.29) 7.43(\u00b15.95) 0.63(\u00b11.96) 18.52(\u00b113.50) 9.57(\u00b16.45) 6.71(\u00b14.95) 1.00(\u00b13.5) \nG-GRU [19] \n14.61(\u00b112.80) 10.95(\u00b17.81) 6.70(\u00b15.61) 0.89(\u00b12.12) 15.77(\u00b110.69) 9.02(\u00b15.71) 6.09(\u00b14.06 1.07(\u00b13.56) \nAGC-LSTM [34] 17.39(\u00b115.39) 12.58(\u00b19.49) 8.08(\u00b17.42) 0.86(\u00b12.30) 19.03(\u00b114.12) 10.76(\u00b17.40) 6.88(\u00b15.14 1.05(\u00b13.70) \nOurs \n13.12(\u00b110.99) 10.12(\u00b17.03) 6.00(\u00b14.64) 1.08(\u00b12.46) 15.18(\u00b19.83) 8.13(\u00b15.23) 5.65(\u00b13.73) 1.13(\u00b13.54) \nOurs (TP) \n15.09(\u00b114.39) 10.67(\u00b17.83) 6.77(\u00b15.76) 0.88(\u00b12.30) 15.55(\u00b111.20) 8.18(\u00b15.41) 5.84(\u00b14.21) 1.08(\u00b13.56) \n\n\n\n\nfor our standalone network. Refer to the supplemental material for an additional study on our model in Transpose setup. DIP Err [deg] Ang Err [deg] Pos Err [cm] Jerk Err [ km s 3 ] Ours 15.18(\u00b19.83) 8.13(\u00b15.23) 5.65(\u00b13.73) 1.13(\u00b13.54) no LLW 16.74(\u00b112.06) 8.68(\u00b15.90) 6.26(\u00b14.55) 1.10(\u00b13.59) no CDA 20.14(\u00b114.58) 9.41(\u00b16.40) 6.54(\u00b15.07) 1.13(\u00b13.65)\n\nTensor-Flow: Large-scale machine learning on heterogeneous systems. Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Vijay Vasudevan, Fernanda Vi\u00e9gas. Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke; Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang ZhengDandelion Man\u00e9. Software available from tensorflow.org. 5Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal- war, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer- nanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Watten- berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor- Flow: Large-scale machine learning on heterogeneous sys- tems, 2015. Software available from tensorflow.org. 5\n\nSoheyl Noachtar, and Nassir Navab. Patient mocap: Human pose estimation under blanket occlusion for hospital monitoring applications. Felix Achilles, Alexandru-Eugen Ichim, Huseyin Coskun, Federico Tombari, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerFelix Achilles, Alexandru-Eugen Ichim, Huseyin Coskun, Federico Tombari, Soheyl Noachtar, and Nassir Navab. Pa- tient mocap: Human pose estimation under blanket occlu- sion for hospital monitoring applications. In International Conference on Medical Image Computing and Computer- Assisted Intervention, pages 491-499. Springer, 2016. 1\n\nAn efficient human action recognition framework with pose-based spatiotemporal features. Engineering Science and Technology. Farhood Saeid Agahian, Cemal Negin, K\u00f6se, an International Journal. 231Saeid Agahian, Farhood Negin, and Cemal K\u00f6se. An effi- cient human action recognition framework with pose-based spatiotemporal features. Engineering Science and Technol- ogy, an International Journal, 23(1):196-203, 2020. 1\n\nAdaptive graph convolutional recurrent network for traffic forecasting. Lei Bai, Lina Yao, Can Li, Xianzhi Wang, Can Wang, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33LEI BAI, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. Adaptive graph convolutional recurrent network for traffic forecasting. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 17804-17815. Curran Associates, Inc., 2020. 2\n\nMarker-based human motion capture in multiview sequences. Cristian Canton-Ferrer, R Josep, Montse Casas, Pardas, EURASIP Journal on Advances in Signal Processing. 2Cristian Canton-Ferrer, Josep R Casas, and Montse Par- das. Marker-based human motion capture in multiview se- quences. EURASIP Journal on Advances in Signal Process- ing, 2010:1-11, 2010. 2\n\nSimple and deep graph convolutional networks. Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, Yaliang Li, PMLRProceedings of the 37th International Conference on Machine Learning. Hal Daum\u00e9 III and Aarti Singhthe 37th International Conference on Machine Learning119Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1725-1735. PMLR, 13-18 Jul 2020. 2\n\nMonocular human pose estimation: A survey of deep learning-based methods. CoRR, abs. Yucheng Chen, Yingli Tian, Mingyi He, Yucheng Chen, Yingli Tian, and Mingyi He. Monocular hu- man pose estimation: A survey of deep learning-based meth- ods. CoRR, abs/2006.01423, 2020. 1\n\nOcclusion-aware networks for 3d human pose estimation in video. Yu Cheng, Bo Yang, Bo Wang, Wending Yan, Robby T Tan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionYu Cheng, Bo Yang, Bo Wang, Wending Yan, and Robby T Tan. Occlusion-aware networks for 3d human pose estima- tion in video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 723-732, 2019. 1\n\nLearning to forget: Continual prediction with lstm. J\u00fcrgen Felix A Gers, Fred Schmidhuber, Cummins, Felix A Gers, J\u00fcrgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm. 1999. 3\n\nAre gru cells more specific and lstm cells more sensitive in motive classification of text?. Nicole Gruber, Alfred Jockisch, Frontiers in Artificial Intelligence. 340Nicole Gruber and Alfred Jockisch. Are gru cells more spe- cific and lstm cells more sensitive in motive classification of text? Frontiers in Artificial Intelligence, 3(40):1-6, 2020. 2\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. 3\n\nDeep inertial poser: Learning to reconstruct human pose from sparse inertial measurements in real time. Yinghao Huang, Manuel Kaufmann, Emre Aksan, J Michael, Otmar Black, Gerard Hilliges, Pons-Moll, ACM Transactions on Graphics (TOG). 3766Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J Black, Otmar Hilliges, and Gerard Pons-Moll. Deep iner- tial poser: Learning to reconstruct human pose from sparse inertial measurements in real time. ACM Transactions on Graphics (TOG), 37(6):1-15, 2018. 1, 2, 3, 4, 5, 6\n\nEm-pose: 3d human pose estimation from sparse electromagnetic trackers. Manuel Kaufmann, Yi Zhao, Chengcheng Tang, Lingling Tao, Christopher Twigg, Jie Song, Robert Wang, Otmar Hilliges, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Manuel Kaufmann, Yi Zhao, Chengcheng Tang, Lingling Tao, Christopher Twigg, Jie Song, Robert Wang, and Otmar Hilliges. Em-pose: 3d human pose estimation from sparse electromagnetic trackers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11510-11520, October 2021. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.02907arXiv preprintThomas N Kipf and Max Welling. Semi-supervised classi- fication with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. 3\n\nChange of objectively-measured physical activity during geriatric rehabilitation. Jochen Klenk, Sebastian Wekenmann, Lars Schwickert, Ulrich Lindemann, Clemens Becker, Kilian Rapp, Sensors. 19245451Jochen Klenk, Sebastian Wekenmann, Lars Schwickert, Ul- rich Lindemann, Clemens Becker, and Kilian Rapp. Change of objectively-measured physical activity during geriatric re- habilitation. Sensors, 19(24):5451, 2019. 1\n\nAn ultrasonic six degrees-offreedom pose estimation sensor. Dennis Laurijssen, Steven Truijen, Wim Saeys, Walter Daems, Jan Steckel, IEEE Sensors Journal. 171Dennis Laurijssen, Steven Truijen, Wim Saeys, Walter Daems, and Jan Steckel. An ultrasonic six degrees-of- freedom pose estimation sensor. IEEE Sensors Journal, 17(1):151-159, 2016. 2\n\nBiomechanics of the natural, arthritic, and replaced human ankle joint. Alberto Leardini, J John, Sandro O&apos;connor, Giannini, Journal of foot and ankle research. 71Alberto Leardini, John J O'Connor, and Sandro Giannini. Biomechanics of the natural, arthritic, and replaced human ankle joint. Journal of foot and ankle research, 7(1):1-16, 2014. 3\n\nDynamic multiscale graph neural networks for 3d skeleton based human motion prediction. Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yanfeng Wang, Qi Tian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)6Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yan- feng Wang, and Qi Tian. Dynamic multiscale graph neural networks for 3d skeleton based human motion prediction. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition (CVPR), June 2020. 1, 2, 4, 6\n\nDeeper insights into graph convolutional networks for semi-supervised learning. Qimai Li, Zhichao Han, Xiao-Ming Wu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence323Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learn- ing. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 32, 2018. 1, 2, 3\n\nAugmented reality with human body interaction based on monocular 3d pose estimation. Yung Huei, Ting-Wen Lin, Chen, Advanced Concepts for Intelligent Vision Systems. Jacques Blanc-Talon, Don Bone, Wilfried Philips, Dan Popescu, and Paul ScheundersBerlin, Heidelberg; Berlin HeidelbergSpringerHuei-Yung Lin and Ting-Wen Chen. Augmented reality with human body interaction based on monocular 3d pose estima- tion. In Jacques Blanc-Talon, Don Bone, Wilfried Philips, Dan Popescu, and Paul Scheunders, editors, Advanced Con- cepts for Intelligent Vision Systems, pages 321-331, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. 1\n\nSmpl: A skinned multiperson linear model. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, Michael J Black, ACM transactions on graphics (TOG). 3463Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi- person linear model. ACM transactions on graphics (TOG), 34(6):1-16, 2015. 2, 3\n\nAMASS: Archive of motion capture as surface shapes. Naureen Mahmood, Nima Ghorbani, F Nikolaus, Gerard Troje, Michael J Pons-Moll, Black, International Conference on Computer Vision. Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger- ard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In International Confer- ence on Computer Vision, pages 5442-5451, Oct. 2019. 5\n\nFrictional internal work of damped limbs oscillation in human locomotion. Alberto E Minetti, Alex P Moorhead, Gaspare Pavei, Proceedings of the Royal Society B. 287320201410Alberto E Minetti, Alex P Moorhead, and Gaspare Pavei. Frictional internal work of damped limbs oscillation in hu- man locomotion. Proceedings of the Royal Society B, 287(1931):20201410, 2020. 3\n\nRecurrent space-time graph neural networks. Andrei Nicolicioiu, Iulia Duta, Marius Leordeanu, Advances in neural information processing systems. 32Andrei Nicolicioiu, Iulia Duta, and Marius Leordeanu. Re- current space-time graph neural networks. Advances in neu- ral information processing systems, 32, 2019. 2\n\nReal-time human pose detection and tracking for tele-rehabilitation in virtual reality. St\u011bp\u00e1n Obdr\u017e\u00e1lek, Gregorij Kurillo, Jay Han, Richard Abresch, Ruzena Bajcsy, Studies in health technology and informatics. 173St\u011bp\u00e1n Obdr\u017e\u00e1lek, Gregorij Kurillo, Jay Han, Richard Abresch, and Ruzena Bajcsy. Real-time human pose de- tection and tracking for tele-rehabilitation in virtual reality. Studies in health technology and informatics, 173:320-4, 01 2012. 1\n\nLearning monocular 3d human pose estimation from multi-view images. Helge Rhodin, J\u00f6rg Sp\u00f6rri, Isinsu Katircioglu, Victor Constantin, Fr\u00e9d\u00e9ric Meyer, Erich M\u00fcller, Mathieu Salzmann, Pascal Fua, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Helge Rhodin, J\u00f6rg Sp\u00f6rri, Isinsu Katircioglu, Victor Con- stantin, Fr\u00e9d\u00e9ric Meyer, Erich M\u00fcller, Mathieu Salzmann, and Pascal Fua. Learning monocular 3d human pose es- timation from multi-view images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), June 2018. 2\n\nXsens mvn: Full 6dof human motion tracking using miniature inertial sensors. Xsens Motion Technologies BV. Daniel Roetenberg, Henk Luinge, Per Slycke, Tech. Rep. 12Daniel Roetenberg, Henk Luinge, and Per Slycke. Xsens mvn: Full 6dof human motion tracking using miniature in- ertial sensors. Xsens Motion Technologies BV, Tech. Rep, 1, 2009. 2\n\nHuman pose estimation-based real-time gait analysis using convolutional neural network. Ali Rohan, Mohammed Rabah, Tarek Hosny, Sung-Ho Kim, IEEE Access. 81Ali Rohan, Mohammed Rabah, Tarek Hosny, and Sung-Ho Kim. Human pose estimation-based real-time gait anal- ysis using convolutional neural network. IEEE Access, 8:191542-191550, 2020. 1\n\nBidirectional recurrent neural networks. Mike Schuster, K Kuldip, Paliwal, IEEE transactions on Signal Processing. 4511Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE transactions on Signal Processing, 45(11):2673-2681, 1997. 2\n\nDiscriminative human full-body pose estimation from wearable inertial sensor data. Loren Arthur Schwarz, Diana Mateus, Nassir Navab, 3D physiological human workshop. SpringerLoren Arthur Schwarz, Diana Mateus, and Nassir Navab. Discriminative human full-body pose estimation from wear- able inertial sensor data. In 3D physiological human work- shop, pages 159-172. Springer, 2009. 2\n\nMonocular 3d human pose estimation by generation and ordinal ranking. Saurabh Sharma, Pavan Teja Varigonda, Prashast Bindal, Abhishek Sharma, Arjun Jain, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Saurabh Sharma, Pavan Teja Varigonda, Prashast Bindal, Abhishek Sharma, and Arjun Jain. Monocular 3d human pose estimation by generation and ordinal ranking. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019. 2\n\nTwostream adaptive graph convolutional networks for skeletonbased action recognition. Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionLei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two- stream adaptive graph convolutional networks for skeleton- based action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12026-12035, 2019. 2\n\nAn attention enhanced graph convolutional lstm network for skeleton-based action recognition. Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, Tieniu Tan, proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionChenyang Si, Wentao Chen, Wei Wang, Liang Wang, and Tieniu Tan. An attention enhanced graph convolutional lstm network for skeleton-based action recognition. In proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1227-1236, 2019. 6\n\nAction capture with accelerometers. Ronit Slyper, Jessica K Hodgins, Symposium on Computer Animation. Ronit Slyper and Jessica K Hodgins. Action capture with ac- celerometers. In Symposium on Computer Animation, pages 193-199. Citeseer, 2008. 2\n\nMotion reconstruction using sparse accelerometer data. Jochen Tautges, Arno Zinke, Bj\u00f6rn Kr\u00fcger, Jan Baumann, Andreas Weber, Thomas Helten, Meinard M\u00fcller, Hans-Peter Seidel, Bernd Eberhardt, ACM Transactions on Graphics (ToG). 303Jochen Tautges, Arno Zinke, Bj\u00f6rn Kr\u00fcger, Jan Baumann, Andreas Weber, Thomas Helten, Meinard M\u00fcller, Hans- Peter Seidel, and Bernd Eberhardt. Motion reconstruction using sparse accelerometer data. ACM Transactions on Graphics (ToG), 30(3):1-12, 2011. 2\n\nTotal capture: 3d human pose estimation fusing video and inertial sensors. Matt Trumble, Andrew Gilbert, Charles Malleson, Adrian Hilton, John Collomosse, 2017 British Machine Vision Conference (BMVC. 56Matt Trumble, Andrew Gilbert, Charles Malleson, Adrian Hilton, and John Collomosse. Total capture: 3d human pose estimation fusing video and inertial sensors. In 2017 British Machine Vision Conference (BMVC), 2017. 5, 6\n\nSparse inertial poser: Automatic 3d human pose estimation from sparse imus. Bodo Timo Von Marcard, Rosenhahn, J Michael, Gerard Black, Pons-Moll, Computer Graphics Forum. Wiley Online Library366Timo Von Marcard, Bodo Rosenhahn, Michael J Black, and Gerard Pons-Moll. Sparse inertial poser: Automatic 3d hu- man pose estimation from sparse imus. In Computer Graph- ics Forum, volume 36, pages 349-360. Wiley Online Library, 2017. 1, 2, 6\n\nTime coherent full-body poses estimated using only five inertial sensors: Deep versus shallow learning. J Frank, Matteo Wouda, Nina Giuberti, Rudigkeit, F Bert-Jan, Mannes Van Beijnum, Peter H Poel, Veltink, Sensors. 1917Frank J. Wouda, Matteo Giuberti, Nina Rudigkeit, Bert- Jan F. van Beijnum, Mannes Poel, and Peter H. Veltink. Time coherent full-body poses estimated using only five inertial sensors: Deep versus shallow learning. Sensors, 19(17), 2019. 2\n\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, S Yu Philip, A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural net- works and learning systems, 2020. 1\n\nDeep kinematics analysis for monocular 3d human pose estimation. Jingwei Xu, Zhenbo Yu, Bingbing Ni, Jiancheng Yang, Xiaokang Yang, Wenjun Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Jingwei Xu, Zhenbo Yu, Bingbing Ni, Jiancheng Yang, Xi- aokang Yang, and Wenjun Zhang. Deep kinematics analysis for monocular 3d human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. 2\n\nSpatial temporal graph convolutional networks for skeleton-based action recognition. Sijie Yan, Yuanjun Xiong, Dahua Lin, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence32Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo- ral graph convolutional networks for skeleton-based action recognition. In Proceedings of the AAAI conference on arti- ficial intelligence, volume 32, 2018. 2\n\nPhysical inertial poser (pip): Physics-aware real-time human motion tracking from sparse inertial sensors. Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada, Vladislav Golyanik, Christian Theobalt, Feng Xu, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada, Vladislav Golyanik, Christian Theobalt, and Feng Xu. Phys- ical inertial poser (pip): Physics-aware real-time human mo- tion tracking from sparse inertial sensors. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022. 1, 2\n\nTranspose: Real-time 3d human translation and pose estimation with six inertial sensors. Xinyu Yi, Yuxiao Zhou, Feng Xu, ACM Transactions on Graphics. 4046Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time 3d human translation and pose estimation with six inertial sensors. ACM Transactions on Graphics, 40(4), 08 2021. 1, 2, 4, 5, 6\n\nSemantic graph convolutional networks for 3d human pose regression. Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, Dimitris N Metaxas, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition1Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dim- itris N Metaxas. Semantic graph convolutional networks for 3d human pose regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3425-3435, 2019. 1, 2\n\nT-gcn: A temporal graph convolutional network for traffic prediction. Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, Haifeng Li, IEEE Transactions on Intelligent Transportation Systems. 219Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, and Haifeng Li. T-gcn: A temporal graph convolutional network for traffic prediction. IEEE Trans- actions on Intelligent Transportation Systems, 21(9):3848- 3858, 2019. 2\n\nChen Chen, and Zhengming Ding. 3d human pose estimation with spatial and temporal transformers. Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionCe Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming Ding. 3d human pose estima- tion with spatial and temporal transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 11656-11665, 2021. 1\n\nOn the continuity of rotation representations in neural networks. Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, Li Hao, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li Hao. On the continuity of rotation representations in neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 4\n\n3d human pose estimation in rgbd images for robotic task learning. Christian Zimmermann, Tim Welschehold, Christian Dornhege, Wolfram Burgard, Thomas Brox, 2018 IEEE International Conference on Robotics and Automation (ICRA). Christian Zimmermann, Tim Welschehold, Christian Dorn- hege, Wolfram Burgard, and Thomas Brox. 3d human pose estimation in rgbd images for robotic task learning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1986-1992. IEEE, 2018. 2\n\nModulated graph convolutional network for 3d human pose estimation. Zhiming Zou, Wei Tang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhiming Zou and Wei Tang. Modulated graph convolutional network for 3d human pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 11477-11487, 2021. 2\n", "annotations": {"author": "[{\"end\":228,\"start\":95},{\"end\":275,\"start\":229}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":102},{\"end\":242,\"start\":234}]", "author_first_name": "[{\"end\":101,\"start\":95},{\"end\":233,\"start\":229}]", "author_affiliation": "[{\"end\":227,\"start\":137},{\"end\":274,\"start\":244}]", "title": "[{\"end\":92,\"start\":1},{\"end\":367,\"start\":276}]", "venue": null, "abstract": "[{\"end\":1600,\"start\":369}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1762,\"start\":1758},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1765,\"start\":1762},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1817,\"start\":1813},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1841,\"start\":1838},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1875,\"start\":1872},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2010,\"start\":2007},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2013,\"start\":2010},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2016,\"start\":2013},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2157,\"start\":2154},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2245,\"start\":2241},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2533,\"start\":2529},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2536,\"start\":2533},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2539,\"start\":2536},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2542,\"start\":2539},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2820,\"start\":2816},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2969,\"start\":2965},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3634,\"start\":3630},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4037,\"start\":4033},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5294,\"start\":5291},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5297,\"start\":5294},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5300,\"start\":5297},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5303,\"start\":5300},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5306,\"start\":5303},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5309,\"start\":5306},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5410,\"start\":5406},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5638,\"start\":5634},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5716,\"start\":5714},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6089,\"start\":6085},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6092,\"start\":6089},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6147,\"start\":6143},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6283,\"start\":6279},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6360,\"start\":6356},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6678,\"start\":6674},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6724,\"start\":6720},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6999,\"start\":6995},{\"end\":7019,\"start\":7001},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7170,\"start\":7166},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7651,\"start\":7647},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7802,\"start\":7798},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7805,\"start\":7802},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8098,\"start\":8094},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8101,\"start\":8098},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8173,\"start\":8169},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8176,\"start\":8173},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8528,\"start\":8525},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8808,\"start\":8804},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9284,\"start\":9280},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9835,\"start\":9831},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9878,\"start\":9875},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10474,\"start\":10470},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10522,\"start\":10518},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10904,\"start\":10900},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11876,\"start\":11872},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12077,\"start\":12073},{\"end\":12235,\"start\":12232},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13057,\"start\":13053},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13060,\"start\":13057},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13772,\"start\":13768},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13774,\"start\":13772},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14305,\"start\":14301},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15513,\"start\":15509},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16999,\"start\":16995},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17157,\"start\":17153},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18367,\"start\":18363},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21544,\"start\":21540},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21640,\"start\":21636},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21761,\"start\":21760},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22350,\"start\":22346},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22385,\"start\":22381},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22456,\"start\":22453},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22556,\"start\":22552},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22856,\"start\":22852},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23297,\"start\":23293},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23881,\"start\":23877},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24126,\"start\":24122},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24711,\"start\":24707},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24734,\"start\":24730},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25601,\"start\":25597},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25634,\"start\":25630},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27333,\"start\":27329},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27363,\"start\":27359},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34359,\"start\":34358},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34422,\"start\":34421}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":35670,\"start\":35196},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36029,\"start\":35671},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37610,\"start\":36030},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37961,\"start\":37611}]", "paragraph": "[{\"end\":4359,\"start\":1616},{\"end\":4573,\"start\":4361},{\"end\":4719,\"start\":4575},{\"end\":4857,\"start\":4721},{\"end\":5131,\"start\":4859},{\"end\":7652,\"start\":5148},{\"end\":10168,\"start\":7654},{\"end\":12105,\"start\":10183},{\"end\":12510,\"start\":12121},{\"end\":13851,\"start\":12562},{\"end\":14306,\"start\":13959},{\"end\":14553,\"start\":14379},{\"end\":15333,\"start\":14555},{\"end\":15905,\"start\":15335},{\"end\":16626,\"start\":15994},{\"end\":19130,\"start\":16628},{\"end\":19408,\"start\":19205},{\"end\":22119,\"start\":19410},{\"end\":23176,\"start\":22135},{\"end\":24938,\"start\":23197},{\"end\":25426,\"start\":24940},{\"end\":26890,\"start\":25454},{\"end\":29407,\"start\":26937},{\"end\":31156,\"start\":29434},{\"end\":31511,\"start\":31158},{\"end\":32136,\"start\":31513},{\"end\":32555,\"start\":32155},{\"end\":33135,\"start\":32557},{\"end\":35195,\"start\":33166}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12120,\"start\":12106},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12561,\"start\":12511},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13958,\"start\":13852},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14378,\"start\":14307},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15993,\"start\":15906},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19204,\"start\":19131},{\"attributes\":{\"id\":\"formula_6\"},\"end\":26936,\"start\":26891}]", "table_ref": "[{\"end\":7154,\"start\":7020},{\"end\":24532,\"start\":24525},{\"end\":27075,\"start\":27068},{\"end\":32332,\"start\":32325},{\"end\":32340,\"start\":32333}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1614,\"start\":1602},{\"attributes\":{\"n\":\"2.\"},\"end\":5146,\"start\":5134},{\"attributes\":{\"n\":\"3.\"},\"end\":10181,\"start\":10171},{\"attributes\":{\"n\":\"4.\"},\"end\":22133,\"start\":22122},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23195,\"start\":23179},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25452,\"start\":25429},{\"attributes\":{\"n\":\"4.3.\"},\"end\":29432,\"start\":29410},{\"attributes\":{\"n\":\"4.4.\"},\"end\":32153,\"start\":32139},{\"attributes\":{\"n\":\"5.\"},\"end\":33164,\"start\":33138},{\"end\":35207,\"start\":35197},{\"end\":35682,\"start\":35672}]", "table": "[{\"end\":37610,\"start\":36153}]", "figure_caption": "[{\"end\":35670,\"start\":35209},{\"end\":36029,\"start\":35684},{\"end\":36153,\"start\":36032},{\"end\":37961,\"start\":37613}]", "figure_ref": "[{\"end\":10016,\"start\":10008},{\"end\":10987,\"start\":10981},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16722,\"start\":16716},{\"end\":25866,\"start\":25860},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29765,\"start\":29759}]", "bib_author_first_name": "[{\"end\":38037,\"start\":38031},{\"end\":38051,\"start\":38045},{\"end\":38065,\"start\":38061},{\"end\":38080,\"start\":38074},{\"end\":38096,\"start\":38089},{\"end\":38108,\"start\":38103},{\"end\":38120,\"start\":38116},{\"end\":38122,\"start\":38121},{\"end\":38136,\"start\":38132},{\"end\":38151,\"start\":38144},{\"end\":38166,\"start\":38158},{\"end\":38180,\"start\":38174},{\"end\":38194,\"start\":38191},{\"end\":38213,\"start\":38207},{\"end\":38228,\"start\":38220},{\"end\":38244,\"start\":38237},{\"end\":38260,\"start\":38252},{\"end\":38271,\"start\":38266},{\"end\":38290,\"start\":38284},{\"end\":38308,\"start\":38299},{\"end\":38321,\"start\":38317},{\"end\":39533,\"start\":39528},{\"end\":39559,\"start\":39544},{\"end\":39574,\"start\":39567},{\"end\":39591,\"start\":39583},{\"end\":40166,\"start\":40159},{\"end\":40187,\"start\":40182},{\"end\":40530,\"start\":40527},{\"end\":40540,\"start\":40536},{\"end\":40549,\"start\":40546},{\"end\":40561,\"start\":40554},{\"end\":40571,\"start\":40568},{\"end\":41101,\"start\":41093},{\"end\":41118,\"start\":41117},{\"end\":41132,\"start\":41126},{\"end\":41441,\"start\":41437},{\"end\":41454,\"start\":41448},{\"end\":41468,\"start\":41460},{\"end\":41481,\"start\":41476},{\"end\":41495,\"start\":41488},{\"end\":42075,\"start\":42068},{\"end\":42088,\"start\":42082},{\"end\":42101,\"start\":42095},{\"end\":42323,\"start\":42321},{\"end\":42333,\"start\":42331},{\"end\":42342,\"start\":42340},{\"end\":42356,\"start\":42349},{\"end\":42367,\"start\":42362},{\"end\":42369,\"start\":42368},{\"end\":42786,\"start\":42780},{\"end\":42805,\"start\":42801},{\"end\":43040,\"start\":43034},{\"end\":43055,\"start\":43049},{\"end\":43322,\"start\":43318},{\"end\":43341,\"start\":43335},{\"end\":43597,\"start\":43590},{\"end\":43611,\"start\":43605},{\"end\":43626,\"start\":43622},{\"end\":43635,\"start\":43634},{\"end\":43650,\"start\":43645},{\"end\":43664,\"start\":43658},{\"end\":44081,\"start\":44075},{\"end\":44094,\"start\":44092},{\"end\":44111,\"start\":44101},{\"end\":44126,\"start\":44118},{\"end\":44143,\"start\":44132},{\"end\":44154,\"start\":44151},{\"end\":44167,\"start\":44161},{\"end\":44179,\"start\":44174},{\"end\":44688,\"start\":44687},{\"end\":44704,\"start\":44699},{\"end\":44930,\"start\":44929},{\"end\":44942,\"start\":44939},{\"end\":45217,\"start\":45211},{\"end\":45234,\"start\":45225},{\"end\":45250,\"start\":45246},{\"end\":45269,\"start\":45263},{\"end\":45288,\"start\":45281},{\"end\":45303,\"start\":45297},{\"end\":45613,\"start\":45607},{\"end\":45632,\"start\":45626},{\"end\":45645,\"start\":45642},{\"end\":45659,\"start\":45653},{\"end\":45670,\"start\":45667},{\"end\":45969,\"start\":45962},{\"end\":45981,\"start\":45980},{\"end\":45994,\"start\":45988},{\"end\":46336,\"start\":46330},{\"end\":46347,\"start\":46341},{\"end\":46362,\"start\":46354},{\"end\":46371,\"start\":46369},{\"end\":46386,\"start\":46379},{\"end\":46395,\"start\":46393},{\"end\":46935,\"start\":46930},{\"end\":46947,\"start\":46940},{\"end\":46962,\"start\":46953},{\"end\":47385,\"start\":47381},{\"end\":47400,\"start\":47392},{\"end\":47976,\"start\":47969},{\"end\":47991,\"start\":47984},{\"end\":48007,\"start\":48001},{\"end\":48022,\"start\":48016},{\"end\":48043,\"start\":48034},{\"end\":48340,\"start\":48333},{\"end\":48354,\"start\":48350},{\"end\":48366,\"start\":48365},{\"end\":48383,\"start\":48377},{\"end\":48398,\"start\":48391},{\"end\":48400,\"start\":48399},{\"end\":48771,\"start\":48764},{\"end\":48773,\"start\":48772},{\"end\":48787,\"start\":48783},{\"end\":48789,\"start\":48788},{\"end\":48807,\"start\":48800},{\"end\":49109,\"start\":49103},{\"end\":49128,\"start\":49123},{\"end\":49141,\"start\":49135},{\"end\":49466,\"start\":49460},{\"end\":49486,\"start\":49478},{\"end\":49499,\"start\":49496},{\"end\":49512,\"start\":49505},{\"end\":49528,\"start\":49522},{\"end\":49899,\"start\":49894},{\"end\":49912,\"start\":49908},{\"end\":49927,\"start\":49921},{\"end\":49947,\"start\":49941},{\"end\":49968,\"start\":49960},{\"end\":49981,\"start\":49976},{\"end\":49997,\"start\":49990},{\"end\":50014,\"start\":50008},{\"end\":50595,\"start\":50589},{\"end\":50612,\"start\":50608},{\"end\":50624,\"start\":50621},{\"end\":50917,\"start\":50914},{\"end\":50933,\"start\":50925},{\"end\":50946,\"start\":50941},{\"end\":50961,\"start\":50954},{\"end\":51213,\"start\":51209},{\"end\":51225,\"start\":51224},{\"end\":51519,\"start\":51514},{\"end\":51526,\"start\":51520},{\"end\":51541,\"start\":51536},{\"end\":51556,\"start\":51550},{\"end\":51893,\"start\":51886},{\"end\":51907,\"start\":51902},{\"end\":51912,\"start\":51908},{\"end\":51932,\"start\":51924},{\"end\":51949,\"start\":51941},{\"end\":51963,\"start\":51958},{\"end\":52460,\"start\":52457},{\"end\":52471,\"start\":52466},{\"end\":52483,\"start\":52479},{\"end\":52498,\"start\":52491},{\"end\":53008,\"start\":53000},{\"end\":53019,\"start\":53013},{\"end\":53029,\"start\":53026},{\"end\":53041,\"start\":53036},{\"end\":53054,\"start\":53048},{\"end\":53522,\"start\":53517},{\"end\":53538,\"start\":53531},{\"end\":53540,\"start\":53539},{\"end\":53788,\"start\":53782},{\"end\":53802,\"start\":53798},{\"end\":53815,\"start\":53810},{\"end\":53827,\"start\":53824},{\"end\":53844,\"start\":53837},{\"end\":53858,\"start\":53852},{\"end\":53874,\"start\":53867},{\"end\":53893,\"start\":53883},{\"end\":53907,\"start\":53902},{\"end\":54291,\"start\":54287},{\"end\":54307,\"start\":54301},{\"end\":54324,\"start\":54317},{\"end\":54341,\"start\":54335},{\"end\":54354,\"start\":54350},{\"end\":54716,\"start\":54712},{\"end\":54747,\"start\":54746},{\"end\":54763,\"start\":54757},{\"end\":55179,\"start\":55178},{\"end\":55193,\"start\":55187},{\"end\":55205,\"start\":55201},{\"end\":55228,\"start\":55227},{\"end\":55245,\"start\":55239},{\"end\":55264,\"start\":55259},{\"end\":55266,\"start\":55265},{\"end\":55542,\"start\":55535},{\"end\":55553,\"start\":55547},{\"end\":55566,\"start\":55559},{\"end\":55580,\"start\":55573},{\"end\":55594,\"start\":55587},{\"end\":55606,\"start\":55602},{\"end\":55998,\"start\":55991},{\"end\":56009,\"start\":56003},{\"end\":56022,\"start\":56014},{\"end\":56036,\"start\":56027},{\"end\":56051,\"start\":56043},{\"end\":56064,\"start\":56058},{\"end\":56584,\"start\":56579},{\"end\":56597,\"start\":56590},{\"end\":56610,\"start\":56605},{\"end\":57055,\"start\":57050},{\"end\":57066,\"start\":57060},{\"end\":57077,\"start\":57073},{\"end\":57094,\"start\":57089},{\"end\":57113,\"start\":57104},{\"end\":57133,\"start\":57124},{\"end\":57148,\"start\":57144},{\"end\":57628,\"start\":57623},{\"end\":57639,\"start\":57633},{\"end\":57650,\"start\":57646},{\"end\":57947,\"start\":57943},{\"end\":57956,\"start\":57954},{\"end\":57965,\"start\":57963},{\"end\":57980,\"start\":57972},{\"end\":57998,\"start\":57990},{\"end\":58000,\"start\":57999},{\"end\":58490,\"start\":58486},{\"end\":58503,\"start\":58497},{\"end\":58514,\"start\":58510},{\"end\":58524,\"start\":58522},{\"end\":58532,\"start\":58530},{\"end\":58542,\"start\":58539},{\"end\":58551,\"start\":58548},{\"end\":58565,\"start\":58558},{\"end\":58972,\"start\":58970},{\"end\":58985,\"start\":58980},{\"end\":58997,\"start\":58991},{\"end\":59018,\"start\":59008},{\"end\":59481,\"start\":59479},{\"end\":59496,\"start\":59488},{\"end\":59507,\"start\":59505},{\"end\":59521,\"start\":59517},{\"end\":59531,\"start\":59529},{\"end\":59900,\"start\":59891},{\"end\":59916,\"start\":59913},{\"end\":59939,\"start\":59930},{\"end\":59957,\"start\":59950},{\"end\":59973,\"start\":59967},{\"end\":60392,\"start\":60385},{\"end\":60401,\"start\":60398}]", "bib_author_last_name": "[{\"end\":38043,\"start\":38038},{\"end\":38059,\"start\":38052},{\"end\":38072,\"start\":38066},{\"end\":38087,\"start\":38081},{\"end\":38101,\"start\":38097},{\"end\":38114,\"start\":38109},{\"end\":38130,\"start\":38123},{\"end\":38142,\"start\":38137},{\"end\":38156,\"start\":38152},{\"end\":38172,\"start\":38167},{\"end\":38189,\"start\":38181},{\"end\":38205,\"start\":38195},{\"end\":38218,\"start\":38214},{\"end\":38235,\"start\":38229},{\"end\":38250,\"start\":38245},{\"end\":38264,\"start\":38261},{\"end\":38282,\"start\":38272},{\"end\":38297,\"start\":38291},{\"end\":38315,\"start\":38309},{\"end\":38331,\"start\":38322},{\"end\":39542,\"start\":39534},{\"end\":39565,\"start\":39560},{\"end\":39581,\"start\":39575},{\"end\":39599,\"start\":39592},{\"end\":40180,\"start\":40167},{\"end\":40193,\"start\":40188},{\"end\":40199,\"start\":40195},{\"end\":40534,\"start\":40531},{\"end\":40544,\"start\":40541},{\"end\":40552,\"start\":40550},{\"end\":40566,\"start\":40562},{\"end\":40576,\"start\":40572},{\"end\":41115,\"start\":41102},{\"end\":41124,\"start\":41119},{\"end\":41138,\"start\":41133},{\"end\":41146,\"start\":41140},{\"end\":41446,\"start\":41442},{\"end\":41458,\"start\":41455},{\"end\":41474,\"start\":41469},{\"end\":41486,\"start\":41482},{\"end\":41498,\"start\":41496},{\"end\":42080,\"start\":42076},{\"end\":42093,\"start\":42089},{\"end\":42104,\"start\":42102},{\"end\":42329,\"start\":42324},{\"end\":42338,\"start\":42334},{\"end\":42347,\"start\":42343},{\"end\":42360,\"start\":42357},{\"end\":42373,\"start\":42370},{\"end\":42799,\"start\":42787},{\"end\":42817,\"start\":42806},{\"end\":42826,\"start\":42819},{\"end\":43047,\"start\":43041},{\"end\":43064,\"start\":43056},{\"end\":43333,\"start\":43323},{\"end\":43353,\"start\":43342},{\"end\":43603,\"start\":43598},{\"end\":43620,\"start\":43612},{\"end\":43632,\"start\":43627},{\"end\":43643,\"start\":43636},{\"end\":43656,\"start\":43651},{\"end\":43673,\"start\":43665},{\"end\":43684,\"start\":43675},{\"end\":44090,\"start\":44082},{\"end\":44099,\"start\":44095},{\"end\":44116,\"start\":44112},{\"end\":44130,\"start\":44127},{\"end\":44149,\"start\":44144},{\"end\":44159,\"start\":44155},{\"end\":44172,\"start\":44168},{\"end\":44188,\"start\":44180},{\"end\":44697,\"start\":44689},{\"end\":44711,\"start\":44705},{\"end\":44715,\"start\":44713},{\"end\":44937,\"start\":44931},{\"end\":44947,\"start\":44943},{\"end\":44956,\"start\":44949},{\"end\":45223,\"start\":45218},{\"end\":45244,\"start\":45235},{\"end\":45261,\"start\":45251},{\"end\":45279,\"start\":45270},{\"end\":45295,\"start\":45289},{\"end\":45308,\"start\":45304},{\"end\":45624,\"start\":45614},{\"end\":45640,\"start\":45633},{\"end\":45651,\"start\":45646},{\"end\":45665,\"start\":45660},{\"end\":45678,\"start\":45671},{\"end\":45978,\"start\":45970},{\"end\":45986,\"start\":45982},{\"end\":46008,\"start\":45995},{\"end\":46018,\"start\":46010},{\"end\":46339,\"start\":46337},{\"end\":46352,\"start\":46348},{\"end\":46367,\"start\":46363},{\"end\":46377,\"start\":46372},{\"end\":46391,\"start\":46387},{\"end\":46400,\"start\":46396},{\"end\":46938,\"start\":46936},{\"end\":46951,\"start\":46948},{\"end\":46965,\"start\":46963},{\"end\":47390,\"start\":47386},{\"end\":47404,\"start\":47401},{\"end\":47410,\"start\":47406},{\"end\":47982,\"start\":47977},{\"end\":47999,\"start\":47992},{\"end\":48014,\"start\":48008},{\"end\":48032,\"start\":48023},{\"end\":48049,\"start\":48044},{\"end\":48348,\"start\":48341},{\"end\":48363,\"start\":48355},{\"end\":48375,\"start\":48367},{\"end\":48389,\"start\":48384},{\"end\":48410,\"start\":48401},{\"end\":48417,\"start\":48412},{\"end\":48781,\"start\":48774},{\"end\":48798,\"start\":48790},{\"end\":48813,\"start\":48808},{\"end\":49121,\"start\":49110},{\"end\":49133,\"start\":49129},{\"end\":49151,\"start\":49142},{\"end\":49476,\"start\":49467},{\"end\":49494,\"start\":49487},{\"end\":49503,\"start\":49500},{\"end\":49520,\"start\":49513},{\"end\":49535,\"start\":49529},{\"end\":49906,\"start\":49900},{\"end\":49919,\"start\":49913},{\"end\":49939,\"start\":49928},{\"end\":49958,\"start\":49948},{\"end\":49974,\"start\":49969},{\"end\":49988,\"start\":49982},{\"end\":50006,\"start\":49998},{\"end\":50018,\"start\":50015},{\"end\":50606,\"start\":50596},{\"end\":50619,\"start\":50613},{\"end\":50631,\"start\":50625},{\"end\":50923,\"start\":50918},{\"end\":50939,\"start\":50934},{\"end\":50952,\"start\":50947},{\"end\":50965,\"start\":50962},{\"end\":51222,\"start\":51214},{\"end\":51232,\"start\":51226},{\"end\":51241,\"start\":51234},{\"end\":51534,\"start\":51527},{\"end\":51548,\"start\":51542},{\"end\":51562,\"start\":51557},{\"end\":51900,\"start\":51894},{\"end\":51922,\"start\":51913},{\"end\":51939,\"start\":51933},{\"end\":51956,\"start\":51950},{\"end\":51968,\"start\":51964},{\"end\":52464,\"start\":52461},{\"end\":52477,\"start\":52472},{\"end\":52489,\"start\":52484},{\"end\":52501,\"start\":52499},{\"end\":53011,\"start\":53009},{\"end\":53024,\"start\":53020},{\"end\":53034,\"start\":53030},{\"end\":53046,\"start\":53042},{\"end\":53058,\"start\":53055},{\"end\":53529,\"start\":53523},{\"end\":53548,\"start\":53541},{\"end\":53796,\"start\":53789},{\"end\":53808,\"start\":53803},{\"end\":53822,\"start\":53816},{\"end\":53835,\"start\":53828},{\"end\":53850,\"start\":53845},{\"end\":53865,\"start\":53859},{\"end\":53881,\"start\":53875},{\"end\":53900,\"start\":53894},{\"end\":53917,\"start\":53908},{\"end\":54299,\"start\":54292},{\"end\":54315,\"start\":54308},{\"end\":54333,\"start\":54325},{\"end\":54348,\"start\":54342},{\"end\":54365,\"start\":54355},{\"end\":54733,\"start\":54717},{\"end\":54744,\"start\":54735},{\"end\":54755,\"start\":54748},{\"end\":54769,\"start\":54764},{\"end\":54780,\"start\":54771},{\"end\":55185,\"start\":55180},{\"end\":55199,\"start\":55194},{\"end\":55214,\"start\":55206},{\"end\":55225,\"start\":55216},{\"end\":55237,\"start\":55229},{\"end\":55257,\"start\":55246},{\"end\":55271,\"start\":55267},{\"end\":55280,\"start\":55273},{\"end\":55545,\"start\":55543},{\"end\":55557,\"start\":55554},{\"end\":55571,\"start\":55567},{\"end\":55585,\"start\":55581},{\"end\":55600,\"start\":55595},{\"end\":55613,\"start\":55607},{\"end\":56001,\"start\":55999},{\"end\":56012,\"start\":56010},{\"end\":56025,\"start\":56023},{\"end\":56041,\"start\":56037},{\"end\":56056,\"start\":56052},{\"end\":56070,\"start\":56065},{\"end\":56588,\"start\":56585},{\"end\":56603,\"start\":56598},{\"end\":56614,\"start\":56611},{\"end\":57058,\"start\":57056},{\"end\":57071,\"start\":57067},{\"end\":57087,\"start\":57078},{\"end\":57102,\"start\":57095},{\"end\":57122,\"start\":57114},{\"end\":57142,\"start\":57134},{\"end\":57151,\"start\":57149},{\"end\":57631,\"start\":57629},{\"end\":57644,\"start\":57640},{\"end\":57653,\"start\":57651},{\"end\":57952,\"start\":57948},{\"end\":57961,\"start\":57957},{\"end\":57970,\"start\":57966},{\"end\":57988,\"start\":57981},{\"end\":58008,\"start\":58001},{\"end\":58495,\"start\":58491},{\"end\":58508,\"start\":58504},{\"end\":58520,\"start\":58515},{\"end\":58528,\"start\":58525},{\"end\":58537,\"start\":58533},{\"end\":58546,\"start\":58543},{\"end\":58556,\"start\":58552},{\"end\":58568,\"start\":58566},{\"end\":58978,\"start\":58973},{\"end\":58989,\"start\":58986},{\"end\":59006,\"start\":58998},{\"end\":59023,\"start\":59019},{\"end\":59486,\"start\":59482},{\"end\":59503,\"start\":59497},{\"end\":59515,\"start\":59508},{\"end\":59527,\"start\":59522},{\"end\":59535,\"start\":59532},{\"end\":59911,\"start\":59901},{\"end\":59928,\"start\":59917},{\"end\":59948,\"start\":59940},{\"end\":59965,\"start\":59958},{\"end\":59978,\"start\":59974},{\"end\":60396,\"start\":60393},{\"end\":60406,\"start\":60402}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":39392,\"start\":37963},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":21861184},\"end\":40032,\"start\":39394},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":187185254},\"end\":40453,\"start\":40034},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":220363737},\"end\":41033,\"start\":40455},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":18419115},\"end\":41389,\"start\":41035},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b5\",\"matched_paper_id\":220363476},\"end\":41981,\"start\":41391},{\"attributes\":{\"id\":\"b6\"},\"end\":42255,\"start\":41983},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":208008031},\"end\":42726,\"start\":42257},{\"attributes\":{\"id\":\"b8\"},\"end\":42939,\"start\":42728},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":220252321},\"end\":43292,\"start\":42941},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1915014},\"end\":43484,\"start\":43294},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":216872959},\"end\":44001,\"start\":43486},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":238754933},\"end\":44641,\"start\":44003},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b13\"},\"end\":44861,\"start\":44643},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b14\"},\"end\":45127,\"start\":44863},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":209335288},\"end\":45545,\"start\":45129},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":22614290},\"end\":45888,\"start\":45547},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15104709},\"end\":46240,\"start\":45890},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":213175586},\"end\":46848,\"start\":46242},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":11118105},\"end\":47294,\"start\":46850},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":18215568},\"end\":47925,\"start\":47296},{\"attributes\":{\"id\":\"b21\"},\"end\":48279,\"start\":47927},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":102351100},\"end\":48688,\"start\":48281},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":220836528},\"end\":49057,\"start\":48690},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":197860214},\"end\":49370,\"start\":49059},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":11737015},\"end\":49824,\"start\":49372},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3830407},\"end\":50480,\"start\":49826},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":16142980},\"end\":50824,\"start\":50482},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":226230973},\"end\":51166,\"start\":50826},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":18375389},\"end\":51429,\"start\":51168},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6423100},\"end\":51814,\"start\":51431},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":91184018},\"end\":52369,\"start\":51816},{\"attributes\":{\"id\":\"b32\"},\"end\":52904,\"start\":52371},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":67856333},\"end\":53479,\"start\":52906},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9069084},\"end\":53725,\"start\":53481},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8708348},\"end\":54210,\"start\":53727},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52271809},\"end\":54634,\"start\":54212},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2261815},\"end\":55072,\"start\":54636},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":201663904},\"end\":55533,\"start\":55074},{\"attributes\":{\"id\":\"b39\"},\"end\":55924,\"start\":55535},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":219635049},\"end\":56492,\"start\":55926},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":19167105},\"end\":56941,\"start\":56494},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":247475794},\"end\":57532,\"start\":56943},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":234357531},\"end\":57873,\"start\":57534},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":102351949},\"end\":58414,\"start\":57875},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":57189442},\"end\":58872,\"start\":58416},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":232290593},\"end\":59411,\"start\":58874},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":56178817},\"end\":59822,\"start\":59413},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3542090},\"end\":60315,\"start\":59824},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":244114213},\"end\":60735,\"start\":60317}]", "bib_title": "[{\"end\":38029,\"start\":37963},{\"end\":39526,\"start\":39394},{\"end\":40157,\"start\":40034},{\"end\":40525,\"start\":40455},{\"end\":41091,\"start\":41035},{\"end\":41435,\"start\":41391},{\"end\":42319,\"start\":42257},{\"end\":43032,\"start\":42941},{\"end\":43316,\"start\":43294},{\"end\":43588,\"start\":43486},{\"end\":44073,\"start\":44003},{\"end\":45209,\"start\":45129},{\"end\":45605,\"start\":45547},{\"end\":45960,\"start\":45890},{\"end\":46328,\"start\":46242},{\"end\":46928,\"start\":46850},{\"end\":47379,\"start\":47296},{\"end\":47967,\"start\":47927},{\"end\":48331,\"start\":48281},{\"end\":48762,\"start\":48690},{\"end\":49101,\"start\":49059},{\"end\":49458,\"start\":49372},{\"end\":49892,\"start\":49826},{\"end\":50587,\"start\":50482},{\"end\":50912,\"start\":50826},{\"end\":51207,\"start\":51168},{\"end\":51512,\"start\":51431},{\"end\":51884,\"start\":51816},{\"end\":52455,\"start\":52371},{\"end\":52998,\"start\":52906},{\"end\":53515,\"start\":53481},{\"end\":53780,\"start\":53727},{\"end\":54285,\"start\":54212},{\"end\":54710,\"start\":54636},{\"end\":55176,\"start\":55074},{\"end\":55989,\"start\":55926},{\"end\":56577,\"start\":56494},{\"end\":57048,\"start\":56943},{\"end\":57621,\"start\":57534},{\"end\":57941,\"start\":57875},{\"end\":58484,\"start\":58416},{\"end\":58968,\"start\":58874},{\"end\":59477,\"start\":59413},{\"end\":59889,\"start\":59824},{\"end\":60383,\"start\":60317}]", "bib_author": "[{\"end\":38045,\"start\":38031},{\"end\":38061,\"start\":38045},{\"end\":38074,\"start\":38061},{\"end\":38089,\"start\":38074},{\"end\":38103,\"start\":38089},{\"end\":38116,\"start\":38103},{\"end\":38132,\"start\":38116},{\"end\":38144,\"start\":38132},{\"end\":38158,\"start\":38144},{\"end\":38174,\"start\":38158},{\"end\":38191,\"start\":38174},{\"end\":38207,\"start\":38191},{\"end\":38220,\"start\":38207},{\"end\":38237,\"start\":38220},{\"end\":38252,\"start\":38237},{\"end\":38266,\"start\":38252},{\"end\":38284,\"start\":38266},{\"end\":38299,\"start\":38284},{\"end\":38317,\"start\":38299},{\"end\":38333,\"start\":38317},{\"end\":39544,\"start\":39528},{\"end\":39567,\"start\":39544},{\"end\":39583,\"start\":39567},{\"end\":39601,\"start\":39583},{\"end\":40182,\"start\":40159},{\"end\":40195,\"start\":40182},{\"end\":40201,\"start\":40195},{\"end\":40536,\"start\":40527},{\"end\":40546,\"start\":40536},{\"end\":40554,\"start\":40546},{\"end\":40568,\"start\":40554},{\"end\":40578,\"start\":40568},{\"end\":41117,\"start\":41093},{\"end\":41126,\"start\":41117},{\"end\":41140,\"start\":41126},{\"end\":41148,\"start\":41140},{\"end\":41448,\"start\":41437},{\"end\":41460,\"start\":41448},{\"end\":41476,\"start\":41460},{\"end\":41488,\"start\":41476},{\"end\":41500,\"start\":41488},{\"end\":42082,\"start\":42068},{\"end\":42095,\"start\":42082},{\"end\":42106,\"start\":42095},{\"end\":42331,\"start\":42321},{\"end\":42340,\"start\":42331},{\"end\":42349,\"start\":42340},{\"end\":42362,\"start\":42349},{\"end\":42375,\"start\":42362},{\"end\":42801,\"start\":42780},{\"end\":42819,\"start\":42801},{\"end\":42828,\"start\":42819},{\"end\":43049,\"start\":43034},{\"end\":43066,\"start\":43049},{\"end\":43335,\"start\":43318},{\"end\":43355,\"start\":43335},{\"end\":43605,\"start\":43590},{\"end\":43622,\"start\":43605},{\"end\":43634,\"start\":43622},{\"end\":43645,\"start\":43634},{\"end\":43658,\"start\":43645},{\"end\":43675,\"start\":43658},{\"end\":43686,\"start\":43675},{\"end\":44092,\"start\":44075},{\"end\":44101,\"start\":44092},{\"end\":44118,\"start\":44101},{\"end\":44132,\"start\":44118},{\"end\":44151,\"start\":44132},{\"end\":44161,\"start\":44151},{\"end\":44174,\"start\":44161},{\"end\":44190,\"start\":44174},{\"end\":44699,\"start\":44687},{\"end\":44713,\"start\":44699},{\"end\":44717,\"start\":44713},{\"end\":44939,\"start\":44929},{\"end\":44949,\"start\":44939},{\"end\":44958,\"start\":44949},{\"end\":45225,\"start\":45211},{\"end\":45246,\"start\":45225},{\"end\":45263,\"start\":45246},{\"end\":45281,\"start\":45263},{\"end\":45297,\"start\":45281},{\"end\":45310,\"start\":45297},{\"end\":45626,\"start\":45607},{\"end\":45642,\"start\":45626},{\"end\":45653,\"start\":45642},{\"end\":45667,\"start\":45653},{\"end\":45680,\"start\":45667},{\"end\":45980,\"start\":45962},{\"end\":45988,\"start\":45980},{\"end\":46010,\"start\":45988},{\"end\":46020,\"start\":46010},{\"end\":46341,\"start\":46330},{\"end\":46354,\"start\":46341},{\"end\":46369,\"start\":46354},{\"end\":46379,\"start\":46369},{\"end\":46393,\"start\":46379},{\"end\":46402,\"start\":46393},{\"end\":46940,\"start\":46930},{\"end\":46953,\"start\":46940},{\"end\":46967,\"start\":46953},{\"end\":47392,\"start\":47381},{\"end\":47406,\"start\":47392},{\"end\":47412,\"start\":47406},{\"end\":47984,\"start\":47969},{\"end\":48001,\"start\":47984},{\"end\":48016,\"start\":48001},{\"end\":48034,\"start\":48016},{\"end\":48051,\"start\":48034},{\"end\":48350,\"start\":48333},{\"end\":48365,\"start\":48350},{\"end\":48377,\"start\":48365},{\"end\":48391,\"start\":48377},{\"end\":48412,\"start\":48391},{\"end\":48419,\"start\":48412},{\"end\":48783,\"start\":48764},{\"end\":48800,\"start\":48783},{\"end\":48815,\"start\":48800},{\"end\":49123,\"start\":49103},{\"end\":49135,\"start\":49123},{\"end\":49153,\"start\":49135},{\"end\":49478,\"start\":49460},{\"end\":49496,\"start\":49478},{\"end\":49505,\"start\":49496},{\"end\":49522,\"start\":49505},{\"end\":49537,\"start\":49522},{\"end\":49908,\"start\":49894},{\"end\":49921,\"start\":49908},{\"end\":49941,\"start\":49921},{\"end\":49960,\"start\":49941},{\"end\":49976,\"start\":49960},{\"end\":49990,\"start\":49976},{\"end\":50008,\"start\":49990},{\"end\":50020,\"start\":50008},{\"end\":50608,\"start\":50589},{\"end\":50621,\"start\":50608},{\"end\":50633,\"start\":50621},{\"end\":50925,\"start\":50914},{\"end\":50941,\"start\":50925},{\"end\":50954,\"start\":50941},{\"end\":50967,\"start\":50954},{\"end\":51224,\"start\":51209},{\"end\":51234,\"start\":51224},{\"end\":51243,\"start\":51234},{\"end\":51536,\"start\":51514},{\"end\":51550,\"start\":51536},{\"end\":51564,\"start\":51550},{\"end\":51902,\"start\":51886},{\"end\":51924,\"start\":51902},{\"end\":51941,\"start\":51924},{\"end\":51958,\"start\":51941},{\"end\":51970,\"start\":51958},{\"end\":52466,\"start\":52457},{\"end\":52479,\"start\":52466},{\"end\":52491,\"start\":52479},{\"end\":52503,\"start\":52491},{\"end\":53013,\"start\":53000},{\"end\":53026,\"start\":53013},{\"end\":53036,\"start\":53026},{\"end\":53048,\"start\":53036},{\"end\":53060,\"start\":53048},{\"end\":53531,\"start\":53517},{\"end\":53550,\"start\":53531},{\"end\":53798,\"start\":53782},{\"end\":53810,\"start\":53798},{\"end\":53824,\"start\":53810},{\"end\":53837,\"start\":53824},{\"end\":53852,\"start\":53837},{\"end\":53867,\"start\":53852},{\"end\":53883,\"start\":53867},{\"end\":53902,\"start\":53883},{\"end\":53919,\"start\":53902},{\"end\":54301,\"start\":54287},{\"end\":54317,\"start\":54301},{\"end\":54335,\"start\":54317},{\"end\":54350,\"start\":54335},{\"end\":54367,\"start\":54350},{\"end\":54735,\"start\":54712},{\"end\":54746,\"start\":54735},{\"end\":54757,\"start\":54746},{\"end\":54771,\"start\":54757},{\"end\":54782,\"start\":54771},{\"end\":55187,\"start\":55178},{\"end\":55201,\"start\":55187},{\"end\":55216,\"start\":55201},{\"end\":55227,\"start\":55216},{\"end\":55239,\"start\":55227},{\"end\":55259,\"start\":55239},{\"end\":55273,\"start\":55259},{\"end\":55282,\"start\":55273},{\"end\":55547,\"start\":55535},{\"end\":55559,\"start\":55547},{\"end\":55573,\"start\":55559},{\"end\":55587,\"start\":55573},{\"end\":55602,\"start\":55587},{\"end\":55615,\"start\":55602},{\"end\":56003,\"start\":55991},{\"end\":56014,\"start\":56003},{\"end\":56027,\"start\":56014},{\"end\":56043,\"start\":56027},{\"end\":56058,\"start\":56043},{\"end\":56072,\"start\":56058},{\"end\":56590,\"start\":56579},{\"end\":56605,\"start\":56590},{\"end\":56616,\"start\":56605},{\"end\":57060,\"start\":57050},{\"end\":57073,\"start\":57060},{\"end\":57089,\"start\":57073},{\"end\":57104,\"start\":57089},{\"end\":57124,\"start\":57104},{\"end\":57144,\"start\":57124},{\"end\":57153,\"start\":57144},{\"end\":57633,\"start\":57623},{\"end\":57646,\"start\":57633},{\"end\":57655,\"start\":57646},{\"end\":57954,\"start\":57943},{\"end\":57963,\"start\":57954},{\"end\":57972,\"start\":57963},{\"end\":57990,\"start\":57972},{\"end\":58010,\"start\":57990},{\"end\":58497,\"start\":58486},{\"end\":58510,\"start\":58497},{\"end\":58522,\"start\":58510},{\"end\":58530,\"start\":58522},{\"end\":58539,\"start\":58530},{\"end\":58548,\"start\":58539},{\"end\":58558,\"start\":58548},{\"end\":58570,\"start\":58558},{\"end\":58980,\"start\":58970},{\"end\":58991,\"start\":58980},{\"end\":59008,\"start\":58991},{\"end\":59025,\"start\":59008},{\"end\":59488,\"start\":59479},{\"end\":59505,\"start\":59488},{\"end\":59517,\"start\":59505},{\"end\":59529,\"start\":59517},{\"end\":59537,\"start\":59529},{\"end\":59913,\"start\":59891},{\"end\":59930,\"start\":59913},{\"end\":59950,\"start\":59930},{\"end\":59967,\"start\":59950},{\"end\":59980,\"start\":59967},{\"end\":60398,\"start\":60385},{\"end\":60408,\"start\":60398}]", "bib_venue": "[{\"end\":38604,\"start\":38367},{\"end\":41656,\"start\":41603},{\"end\":42504,\"start\":42448},{\"end\":44333,\"start\":44270},{\"end\":46565,\"start\":46492},{\"end\":47076,\"start\":47030},{\"end\":47580,\"start\":47543},{\"end\":50175,\"start\":50106},{\"end\":52113,\"start\":52050},{\"end\":52652,\"start\":52586},{\"end\":53209,\"start\":53143},{\"end\":56235,\"start\":56162},{\"end\":56725,\"start\":56679},{\"end\":58159,\"start\":58093},{\"end\":59154,\"start\":59098},{\"end\":60537,\"start\":60481},{\"end\":38365,\"start\":38333},{\"end\":39687,\"start\":39601},{\"end\":40225,\"start\":40201},{\"end\":40627,\"start\":40578},{\"end\":41196,\"start\":41148},{\"end\":41572,\"start\":41504},{\"end\":42066,\"start\":41983},{\"end\":42446,\"start\":42375},{\"end\":42778,\"start\":42728},{\"end\":43102,\"start\":43066},{\"end\":43373,\"start\":43355},{\"end\":43720,\"start\":43686},{\"end\":44268,\"start\":44190},{\"end\":44685,\"start\":44643},{\"end\":44927,\"start\":44863},{\"end\":45317,\"start\":45310},{\"end\":45700,\"start\":45680},{\"end\":46054,\"start\":46020},{\"end\":46490,\"start\":46402},{\"end\":47028,\"start\":46967},{\"end\":47460,\"start\":47412},{\"end\":48085,\"start\":48051},{\"end\":48462,\"start\":48419},{\"end\":48849,\"start\":48815},{\"end\":49202,\"start\":49153},{\"end\":49581,\"start\":49537},{\"end\":50104,\"start\":50020},{\"end\":50642,\"start\":50633},{\"end\":50978,\"start\":50967},{\"end\":51281,\"start\":51243},{\"end\":51595,\"start\":51564},{\"end\":52048,\"start\":51970},{\"end\":52584,\"start\":52503},{\"end\":53141,\"start\":53060},{\"end\":53581,\"start\":53550},{\"end\":53953,\"start\":53919},{\"end\":54411,\"start\":54367},{\"end\":54805,\"start\":54782},{\"end\":55289,\"start\":55282},{\"end\":55721,\"start\":55615},{\"end\":56160,\"start\":56072},{\"end\":56677,\"start\":56616},{\"end\":57222,\"start\":57153},{\"end\":57683,\"start\":57655},{\"end\":58091,\"start\":58010},{\"end\":58625,\"start\":58570},{\"end\":59096,\"start\":59025},{\"end\":59606,\"start\":59537},{\"end\":60048,\"start\":59980},{\"end\":60479,\"start\":60408}]"}}}, "year": 2023, "month": 12, "day": 17}