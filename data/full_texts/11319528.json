{"id": 11319528, "updated": "2023-11-08 06:33:42.442", "metadata": {"title": "LSRN: A Parallel Iterative Solver for Strongly Over- or Under-Determined Systems", "authors": "[{\"first\":\"Xiangrui\",\"last\":\"Meng\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Saunders\",\"middle\":[\"A.\"]},{\"first\":\"Michael\",\"last\":\"Mahoney\",\"middle\":[\"W.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2011, "month": 9, "day": 27}, "abstract": "We describe a parallel iterative least squares solver named \\texttt{LSRN} that is based on random normal projection. \\texttt{LSRN} computes the min-length solution to $\\min_{x \\in \\mathbb{R}^n} \\|A x - b\\|_2$, where $A \\in \\mathbb{R}^{m \\times n}$ with $m \\gg n$ or $m \\ll n$, and where $A$ may be rank-deficient. Tikhonov regularization may also be included. Since $A$ is only involved in matrix-matrix and matrix-vector multiplications, it can be a dense or sparse matrix or a linear operator, and \\texttt{LSRN} automatically speeds up when $A$ is sparse or a fast linear operator. The preconditioning phase consists of a random normal projection, which is embarrassingly parallel, and a singular value decomposition of size $\\lceil \\gamma \\min(m,n) \\rceil \\times \\min(m,n)$, where $\\gamma$ is moderately larger than 1, e.g., $\\gamma = 2$. We prove that the preconditioned system is well-conditioned, with a strong concentration result on the extreme singular values, and hence that the number of iterations is fully predictable when we apply LSQR or the Chebyshev semi-iterative method. As we demonstrate, the Chebyshev method is particularly efficient for solving large problems on clusters with high communication cost. Numerical results demonstrate that on a shared-memory machine, \\texttt{LSRN} outperforms LAPACK's DGELSD on large dense problems, and MATLAB's backslash (SuiteSparseQR) on sparse problems. Further experiments demonstrate that \\texttt{LSRN} scales well on an Amazon Elastic Compute Cloud cluster.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1109.5981", "mag": "2952826487", "acl": null, "pubmed": "25419094", "pubmedcentral": null, "dblp": "journals/corr/abs-1109-5981", "doi": "10.1137/120866580"}}, "content": {"source": {"pdf_hash": "6ca4791f01e5918799f4657ed4eb6108165ad921", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1109.5981v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://europepmc.org/articles/pmc4238893?pdf=render", "status": "GREEN"}}, "grobid": {"id": "e0206edbaaab2829bd0207f068199970f5b23253", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6ca4791f01e5918799f4657ed4eb6108165ad921.txt", "contents": "\nLSRN: A PARALLEL ITERATIVE SOLVER FOR STRONGLY OVER-OR UNDER-DETERMINED SYSTEMS\n\n\nXiangrui Meng \nMichael A Saunders \nANDMichael W Mahoney \nLSRN: A PARALLEL ITERATIVE SOLVER FOR STRONGLY OVER-OR UNDER-DETERMINED SYSTEMS\nlinear least squaresover-determined systemunder-determined systemrank- deficientminimum-length solutionLAPACKsparse matrixiterative methodpreconditioningLSQRChebyshev semi-iterative methodTikhonov regularizationridge regressionparallel com- putingrandom projectionrandom samplingrandom matrixrandomized algorithm AMS subject classifications 65F0865F1065F2065F2265F3565F5015B52\nWe describe a parallel iterative least squares solver named LSRN that is based on random normal projection. LSRN computes the min-length solution to min x\u2208R n Ax \u2212 b 2 , where A \u2208 R m\u00d7n with m n or m n, and where A may be rank-deficient. Tikhonov regularization may also be included. Since A is only involved in matrix-matrix and matrix-vector multiplications, it can be a dense or sparse matrix or a linear operator, and LSRN automatically speeds up when A is sparse or a fast linear operator. The preconditioning phase consists of a random normal projection, which is embarrassingly parallel, and a singular value decomposition of size \u03b3 min(m, n) \u00d7 min(m, n), where \u03b3 is moderately larger than 1, e.g., \u03b3 = 2. We prove that the preconditioned system is well-conditioned, with a strong concentration result on the extreme singular values, and hence that the number of iterations is fully predictable when we apply LSQR or the Chebyshev semi-iterative method. As we demonstrate, the Chebyshev method is particularly efficient for solving large problems on clusters with high communication cost. Numerical results demonstrate that on a shared-memory machine, LSRN outperforms LAPACK's DGELSD on large dense problems, and MATLAB's backslash (SuiteSparseQR) on sparse problems. Further experiments demonstrate that LSRN scales well on an Amazon Elastic Compute Cloud cluster.\n\n1. Introduction. Randomized algorithms have become indispensable in many areas of computer science, with applications ranging from complexity theory to combinatorial optimization, cryptography, and machine learning. Randomization has also been used in numerical linear algebra (for instance, the initial vector in the power iteration is chosen at random so that almost surely it has a nonzero component along the direction of the dominant eigenvector), yet most well-developed matrix algorithms, e.g., matrix factorizations and linear solvers, are deterministic. In recent years, however, motivated by large data problems, very nontrivial randomized algorithms for very large matrix problems have drawn considerable attention from researchers, originally in theoretical computer science and subsequently in numerical linear algebra and scientific computing. By randomized algorithms, we refer in particular to random sampling and random projection algorithms [7,22,8,21,1]. For a comprehensive overview of these developments, see the review of Mahoney [17], and for an excellent overview of numerical aspects of coupling randomization with classical low-rank matrix factorization methods, see the review of Halko, Martinsson, and Tropp [13].\n\nHere, we consider high-precision solving of linear least squares (LS) problems that are strongly over-or under-determined, and possibly rank-deficient. In particular, given a matrix A \u2208 R m\u00d7n and a vector b \u2208 R m , where m n or m n and we do not assume that A has full rank, we wish to develop randomized algorithms to compute accurately the unique min-length solution to the problem minimize x\u2208R n Ax \u2212 b 2 .\n\n(1.1)\n\nIf we let r = rank(A) \u2264 min(m, n), then recall that if r < n (the LS problem is under-determined or rank-deficient), then (1.1) has an infinite number of minimizers.\n\nIn that case, the set of all minimizers is convex and hence has a unique element having minimum length. On the other hand, if r = n so the problem has full rank, there exists only one minimizer to (1.1) and hence it must have the minimum length. In either case, we denote this unique min-length solution to (1.1) by x * . That is,\nx * = arg min x 2 subject to x \u2208 arg min z Az \u2212 b 2 . (1.2)\nLS problems of this form have a long history, tracing back to Gauss, and they arise in numerous applications. The demand for faster LS solvers will continue to grow in light of new data applications and as problem scales become larger and larger.\n\nIn this paper, we describe an LS solver called LSRN for these strongly over-or under-determined, and possibly rank-deficient, systems. LSRN uses random normal projections to compute a preconditioner matrix such that the preconditioned system is provably extremely well-conditioned. Importantly for large-scale applications, the preconditioning process is embarrassingly parallel, and it automatically speeds up with sparse matrices and fast linear operators. LSQR [20] or the Chebyshev semiiterative (CS) method [11] can be used at the iterative step to compute the min-length solution within just a few iterations. We show that the latter method is preferred on clusters with high communication cost.\n\nBecause of its provably-good conditioning properties, LSRN has a fully predictable run-time performance, just like direct solvers, and it scales well in parallel environments. On large dense systems, LSRN is faster than LAPACK's DGELSD for strongly over-determined problems, and is much faster for strongly under-determined problems, although solvers using fast random projections, like Blendenpik [1], are still slightly faster in both cases. On sparse systems, LSRN runs significantly faster than competing solvers, for both the strongly over-or under-determined cases.\n\nIn section 2 we describe existing deterministic LS solvers and recent randomized algorithms for the LS problem. In section 3 we show how to do preconditioning correctly for rank-deficient LS problems, and in section 4 we introduce LSRN and discuss its properties. Section 5 describes how LSRN can handle Tikhonov regularization for both over-and under-determined systems, and in section 6 we provide a detailed empirical evaluation illustrating the behavior of LSRN.\n\n\nLeast squares solvers.\n\nIn this section we discuss related work, including deterministic direct and iterative methods as well as recently developed randomized methods, for computing solutions to LS problems, and we discuss how our results fit into this broader context.\n\n\nDeterministic methods.\n\nIt is well known that x * in (1.2) can be computed using the singular value decomposition (SVD) of A. Let A = U \u03a3V T be the economy-sized SVD, where U \u2208 R m\u00d7r , \u03a3 \u2208 R r\u00d7r , and V \u2208 R n\u00d7r . We have\nx * = V \u03a3 \u22121 U T b. The matrix V \u03a3 \u22121 U T is the Moore-Penrose pseudoinverse of A, de- noted by A \u2020 .\nThe pseudoinverse is defined and unique for any matrix. Hence we can simply write x * = A \u2020 b. The SVD approach is accurate and robust to rank-deficiency.\n\nAnother way to solve (1.2) is using a complete orthogonal factorization of A. If we can find orthonormal matrices Q \u2208 R m\u00d7r and Z \u2208 R n\u00d7r , and a matrix T \u2208 R r\u00d7r , such that A = QT Z T , then the min-length solution is given by x * = ZT \u22121 Q T b. We can treat SVD as a special case of complete orthogonal factorization. In practice, complete orthogonal factorization is usually computed via rank-revealing QR factorizations, making T a triangular matrix. The QR approach is less expensive than SVD, but it is slightly less robust at determining the rank of A.\n\nA third way to solve (1.2) is by computing the min-length solution to the normal equation\nA T Ax = A T b, namely x * = (A T A) \u2020 A T b = A T (AA T ) \u2020 b. (2.1)\nIt is easy to verify the correctness of (2.1) by replacing A by its economy-sized SVD U \u03a3V T . If r = min(m, n), a Cholesky factorization of either A T A (if m \u2265 n) or AA T (if m \u2264 n) solves (2.1) nicely. If r < min(m, n), we need the eigensystem of A T A or AA T to compute x * . The normal equation approach is the least expensive among the three direct approaches we have mentioned, especially when m n or m n, but it is also the least accurate one, especially on ill-conditioned problems. See Chapter 5 of Golub and Van Loan [10] for a detailed analysis.\n\nInstead of these direct methods, we can use iterative methods to solve (1.1). If all the iterates {x (k) } are in range(A T ) and if {x (k) } converges to a minimizer, it must be the minimizer having minimum length, i.e., the solution to (1.2). This is the case when we use a Krylov subspace method starting with a zero vector. For example, the conjugate gradient (CG) method on the normal equation leads to the min-length solution (see Paige and Saunders [19]). In practice, CGLS [15], LSQR [20] are preferable because they are equivalent to applying CG to the normal equation in exact arithmetic but they are numerically more stable. Other Krylov subspace methods such as the CS method [11] and LSMR [9] can solve (1.1) as well.\n\nImportantly, however, it is in general hard to predict the number of iterations for CG-like methods. The convergence rate is affected by the condition number of A T A. A classical result [16, p.187] states that\nx (k) \u2212 x * A T A x (0) \u2212 x * A T A \u2264 2 \u03ba(A T A) \u2212 1 \u03ba(A T A) + 1 k , (2.2)\nwhere z A T A = z T A T Az = Az 2 for any z \u2208 R n , and where \u03ba(A T A) is the condition number of A T A under the 2-norm. Estimating \u03ba(A T A) is generally as hard as solving the LS problem itself, and in practice the bound does not hold in any case unless reorthogonalization is used. Thus, the computational cost of CG-like methods remains unpredictable in general, except when A T A is very well-conditioned and the condition number can be well estimated.\n\n\nRandomized methods.\n\nIn 2007, Drineas, Mahoney, Muthukrishnan, and Sarl\u00f3s [8] introduced two randomized algorithms for the LS problem, each of which computes a relative-error approximation to the min-length solution in O(mn log n) time, when m n. Both of these algorithms apply a randomized Hadamard transform to the columns of A, thereby generating a problem of smaller size, one using uniformly random sampling and the other using a sparse random projection. They proved that, in both cases, the solution to the smaller problem leads to relative-error approximations of the original problem. The accuracy of the approximate solution depends on the sample size; and to have relative precision \u03b5, one should sample O(n/\u03b5) rows after the randomized Hadamard transform. This is suitable when low accuracy is acceptable, but the \u03b5 dependence quickly becomes the bottleneck otherwise. Using those algorithms as preconditioners was also mentioned in [8]. This work laid the ground for later algorithms and implementations.\n\nLater, in 2008, Rokhlin and Tygert [21] described a related randomized algorithm for over-determined systems. They used a randomized transform named SRFT that consists of m random Givens rotations, a random diagonal scaling, a discrete Fourier transform, and a random sampling. They considered using their method as a preconditioning method, and they showed that to get relative precision \u03b5, only O(n log(1/\u03b5)) samples are needed. In addition, they proved that if the sample size is greater than 4n 2 , the condition number of the preconditioned system is bounded above by a constant. Although choosing this many samples would adversely affect the running time of their solver, they also illustrated examples of input matrices for which the 4n 2 sample bound was weak and for which many fewer samples sufficed.\n\nThen, in 2010, Avron, Maymounkov, and Toledo [1] implemented a high-precision LS solver, called Blendenpik, and compared it to LAPACK's DGELS and to LSQR with no preconditioning. Blendenpik uses a Walsh-Hadamard transform, a discrete cosine transform, or a discrete Hartley transform for blending the rows/columns, followed by a random sampling, to generate a problem of smaller size. The R factor from the QR factorization of the smaller matrix is used as the preconditioner for LSQR. Based on their analysis, the condition number of the preconditioned system depends on the coherence or statistical leverage scores of A, i.e., the maximal row norm of U , where U is an orthonormal basis of range(A). We note that a solver for under-determined problems is also included in the Blendenpik package.\n\nIn 2011, Coakley, Rokhlin, and Tygert [2] described an algorithm that is also based on random normal projections. It computes the orthogonal projection of any vector b onto the null space of A or onto the row space of A via a preconditioned normal equation. The algorithm solves the over-determined LS problem as an intermediate step. They show that the normal equation is well-conditioned and hence the solution is reliable. For an over-determined problem of size m \u00d7 n, the algorithm requires applying A or A T 3n + 6 times, while LSRN needs approximately 2n + 200 matrixvector multiplications under the default setting. Asymptotically, LSRN will become faster as n increases beyond several hundred. See section 4.3 for further complexity analysis of LSRN.\n\n2.3. Relationship with our contributions. All prior approaches assume that A has full rank, and for those based on iterative solvers, none provides a tight upper bound on the condition number of the preconditioned system (and hence the number of iterations). For LSRN, Theorem 3.2 ensures that the min-length solution is preserved, independent of the rank, and Theorems 4.4 and 4.5 provide bounds on the condition number and number of iterations, independent of the spectrum of A. In addition to handling rank-deficiency well, LSRN can even take advantage of it, resulting in a smaller condition number and fewer iterations.\n\nSome prior work on the LS problem has explored \"fast\" randomized transforms that run in roughly O(mn log m) time on a dense matrix A, while the random normal projection we use in LSRN takes O(mn 2 ) time. Although this could be an issue for some applications, the use of random normal projections comes with several advantages. First, if A is a sparse matrix or a linear operator, which is common in large-scale applications, then the Hadamard-based fast transforms are no longer \"fast\". Second, the random normal projection is easy to implement using threads or MPI, and it scales well in parallel environments. Third, the strong symmetry of the standard normal distribution helps give the strong high probability bounds on the condition number in terms of sample size. These bounds depend on nothing but s/r, where s is the sample size. For example, if s = 4r, Theorem 4.4 ensures that, with high probability, the condition number of the preconditioned system is less than 3.\n\nThis last property about the condition number of the preconditioned system makes the number of iterations and thus the running time of LSRN fully predictable like for a direct method. It also enables use of the CS method, which needs only one level-1 and two level-2 BLAS operations per iteration, and is particularly suitable for clusters with high communication cost because it doesn't have vector inner products that require synchronization between nodes. Although the CS method has the same theoretical upper bound on the convergence rate as CG-like methods, it requires accurate bounds on the singular values in order to work efficiently. Such bounds are generally hard to come by, limiting the popularity of the CS method in practice, but they are provided for the preconditioned system by our Theorem 4.4, and we do achieve high efficiency in our experiments.\n\n\nPreconditioning for linear least squares.\n\nIn light of (2.2), much effort has been made to transform a linear system into an equivalent system with reduced condition number. This preconditioning, for a square linear system Bx = d of full rank, usually takes one of the following forms:\nleft preconditioning M T Bx = M T d, right preconditioning BN y = d, x = N y,\nleft and right preconditioning M\nT BN y = M T d, x = N y.\nClearly, the preconditioned system is consistent with the original one, i.e., has the same x * as the unique solution, if the preconditioners M and N are nonsingular. For the general LS problem (1.2), preconditioning needs better handling in order to produce the same min-length solution as the original problem. For example, if we apply left preconditioning to the LS problem min x Ax \u2212 b 2 , the preconditioned system becomes min x M T Ax \u2212 M T b 2 , and its min-length solution is given by\nx * left = (M T A) \u2020 M T b.\nSimilarly, the min-length solution to the right preconditioned system is given by\nx * right = N (AN ) \u2020 b.\nThe following lemma states the necessary and sufficient conditions for\nA \u2020 = N (AN ) \u2020 or A \u2020 = (M T A) \u2020 M T to hold.\nNote that these conditions holding certainly imply that\nx * right = x * and x * left = x * , respectively. Lemma 3.1. Given A \u2208 R m\u00d7n , N \u2208 R n\u00d7p and M \u2208 R m\u00d7q , we have 1. A \u2020 = N (AN ) \u2020 if and only if range(N N T A T ) = range(A T ), 2. A \u2020 = (M T A) \u2020 M T if and only if range(M M T A) = range(A)\n. Proof. Let r = rank(A) and U \u03a3V T be A's economy-sized SVD as in section 2.1, with A \u2020 = V \u03a3 \u22121 U T . Before continuing our proof, we reference the following facts about the pseudoinverse:\n1. B \u2020 = B T (BB T ) \u2020 for any matrix B, 2. For any matrices B and C such that BC is defined, (BC) \u2020 = C \u2020 B \u2020 if (i) B T B = I or (ii) CC T = I or (iii) B\nhas full column rank and C has full row rank. Now let's prove the \"if\" part of the first statement. If range(N N T A T ) = range(A T ) = range(V ), we can write N N T A T as V Z where Z has full row rank. Then,\nN (AN ) \u2020 = N (AN ) T (AN (AN ) T ) \u2020 = N N T A T (AN N T A T ) \u2020 = V Z(U \u03a3V T V Z) \u2020 = V Z(U \u03a3Z) \u2020 = V ZZ \u2020 \u03a3 \u22121 U T = V \u03a3 \u22121 U T = A \u2020 . Conversely, if N (AN ) \u2020 = A \u2020 , we know that range(N (AN ) \u2020 ) = range(A \u2020 ) = range(V ) and hence range(V ) \u2286 range(N ). Then we can decompose N as (V V c ) Z Zc = V Z + V c Z c , where V c is orthonormal, V T V c = 0,\nand Z Zc has full row rank. Then,\n0 = N (AN ) \u2020 \u2212 A \u2020 = (V Z + V c Z c )(U \u03a3V T (V Z + V c Z c )) \u2020 \u2212 V \u03a3 \u22121 U T = (V Z + V c Z c )(U \u03a3Z) \u2020 \u2212 V \u03a3 \u22121 U T = (V Z + V c Z c )Z \u2020 \u03a3 \u22121 U T \u2212 V \u03a3 \u22121 U T = V c Z c Z \u2020 \u03a3 \u22121 U T .\nMultiplying by V T c on the left and U \u03a3 on the right, we get Z c Z \u2020 = 0, which is equivalent to Z c Z T = 0. Therefore,\nrange(N N T A T ) = range((V Z + V c Z c )(V Z + V c Z c ) T V \u03a3U T ) = range((V ZZ T V T + V c Z c Z T c V T c )V \u03a3U T ) = range(V ZZ T \u03a3U T ) = range(V ) = range(A T ),\nwhere we used the facts that Z has full row rank and hence ZZ T is nonsingular, \u03a3 is nonsingular, and U has full column rank.\n\nTo prove the second statement, let us take B = A T . By the first statement, we know Although Lemma 3.1 gives the necessary and sufficient condition, it does not serve as a practical guide for preconditioning LS problems. In this work, we are more interested in a sufficient condition that can help us build preconditioners. To that end, we provide the following theorem.\nTheorem 3.2. Given A \u2208 R m\u00d7n , b \u2208 R m , N \u2208 R n\u00d7p , and M \u2208 R m\u00d7q , let x * be the min-length solution to the LS problem min x Ax \u2212 b 2 , x * right = N y * where y * is the min-length solution to min y AN y \u2212 b 2 , and x * left be the min-length solution to min x M T Ax \u2212 M T b 2 . Then, 1. x * right = x * if range(N ) = range(A T ), 2. x * left = x * if range(M ) = range(A). Proof. Let r = rank(A) and U \u03a3V T be A's economy-sized SVD. If range(N ) = range(A T ) = range(V ), we can write N as V Z, where Z has full row rank. Therefore, range(N N T A T ) = range(V ZZ T V T V \u03a3U T ) = range(V ZZ T \u03a3U T ) = range(V ) = range(A T ).\nBy Lemma 3.1, A \u2020 = N (AN ) \u2020 and hence x * left = x * . The second statement can be proved by similar arguments.\n\n\nAlgorithm LSRN.\n\nIn this section we present LSRN, an iterative solver for solving strongly over-or under-determined systems, based on \"random normal projection\". To construct a preconditioner we apply a transformation matrix whose entries are independent random variables drawn from the standard normal distribution. We prove that the preconditioned system is almost surely consistent with the original system, i.e., both have the same min-length solution. At least as importantly, we prove that the spectrum of the preconditioned system is independent of the spectrum of the original system; and we provide a strong concentration result on the extreme singular values of the preconditioned system. This concentration result enables us to predict the number of iterations for CG-like methods, and it also enables use of the CS method, which requires an accurate bound on the singular values to work efficiently.\n\n4.1. The algorithm. Algorithm 1 shows the detailed procedure of LSRN to compute the min-length solution to a strongly over-determined problem, and Algorithm 2 shows the detailed procedure for a strongly under-determined problem. We refer to these two algorithms together as LSRN. Note that they only use the input matrix A for matrix-vector and matrix-matrix multiplications, and thus A can be a dense matrix, a sparse matrix, or a linear operator. In the remainder of this section we focus on analysis of the over-determined case. We emphasize that analysis of the under-determined case is quite analogous.\nAlgorithm 1 LSRN (computesx \u2248 A \u2020 b when m n)\n1: Choose an oversampling factor \u03b3 > 1 and set s = \u03b3n . 2: Generate G = randn(s, m), i.e., an s-by-m random matrix whose entries are independent random variables following the standard normal distribution. 3: Compute\u00c3 = GA. 4: Compute\u00c3's economy-sized SVD\u0168\u03a3\u1e7c T , where r = rank(\u00c3),\u0168 \u2208 R s\u00d7r ,\u03a3 \u2208 R r\u00d7r ,\u1e7c \u2208 R n\u00d7r , and only\u03a3 and\u1e7c are needed. 5: Let N =\u1e7c\u03a3 \u22121 . 6: Compute the min-length solution to min y AN y \u2212 b 2 using an iterative method.\n\nDenote the solution by\u0177. 7: Returnx = N\u0177.\nAlgorithm 2 LSRN (computesx \u2248 A \u2020 b when m n)\n1: Choose an oversampling \u03b3 > 1 and set s = \u03b3m . 2: Generate G = randn(n, s), i.e., an n-by-s random matrix whose entries are independent random variables following the standard normal distribution.\n3: Compute\u00c3 = AG. 4: Compute\u00c3's economy-sized SVD\u0168\u03a3\u1e7c T , where r = rank(\u00c3),\u0168 \u2208 R n\u00d7r ,\u03a3 \u2208\nR r\u00d7r ,\u1e7c \u2208 R s\u00d7r , and only\u0168 and\u03a3 are needed.  Proof. Let r = rank(A) and U \u03a3V T be A's economy-sized SVD. We have\nrange(N ) = range(\u1e7c\u03a3 \u22121 ) = range(\u1e7c ) = range(\u00c3 T ) = range(A T G T ) = range(V \u03a3(GU ) T ).\nDefine G 1 = GU \u2208 R s\u00d7r . Since G's entries are independent random variables following the standard normal distribution and U is orthonormal, G 1 's entries are also independent random variables following the standard normal distribution. Then given s \u2265 \u03b3n > n \u2265 r, we know G 1 has full column rank r with probability 1. Therefore,\nrange(N ) = range(V \u03a3G T 1 ) = range(V ) = range(A T ),\nand hence by Theorem 3.2 we havex = A \u2020 b almost surely. A more interesting property of LSRN is that the spectrum (the set of singular values) of the preconditioned system is solely associated with a random matrix of size s \u00d7 r, independent of the spectrum of the original system. Lemma 4.2. In Algorithm 1, the spectrum of AN is the same as the spectrum of G \u2020 1 = (GU ) \u2020 , independent of A's spectrum. Proof. Following the proof of Theorem 4.1, let G 1 = U 1 \u03a3 1 V T 1 be G 1 's economysized SVD, where U 1 \u2208 R s\u00d7r , \u03a3 1 \u2208 R r\u00d7r , and V 1 \u2208 R r\u00d7r . Since range(\u0168 ) = range(GA) = range(GU ) = range(U 1 ) and both\u0168 and U 1 are orthonormal matrices, there exists an orthonormal matrix Q 1 \u2208 R r\u00d7r such that U 1 =\u0168 Q 1 . As a result,\nU\u03a3\u1e7c T =\u00c3 = GU \u03a3V T = U 1 \u03a3 1 V T 1 \u03a3V T =\u0168 Q 1 \u03a3 1 V T 1 \u03a3V T .\nMultiplying by\u0168 T on the left of each side, we get\u03a3\u1e7c\nT = Q 1 \u03a3 1 V T 1 \u03a3V T . Taking the pseudoinverse gives N =\u1e7c\u03a3 \u22121 = V \u03a3 \u22121 V 1 \u03a3 \u22121 1 Q T 1 . Thus, AN = U \u03a3V T V \u03a3 \u22121 V 1 \u03a3 \u22121 1 Q T 1 = U V 1 \u03a3 \u22121 1 Q T 1\n, which gives AN 's SVD. Therefore, AN 's singular values are diag(\u03a3 \u22121 1 ), the same as G \u2020 1 's spectrum, but independent of A's. We know that G 1 = GU is a random matrix whose entries are independent random variables following the standard normal distribution. The spectrum of G 1 is a well-studied problem in Random Matrix Theory, and in particular the properties of extreme singular values have been studied. Thus, the following lemma is important for us. We use P(\u00b7) to refer to the probability that a given event occurs. Lemma 4.3. (Davidson and Szarek [3]) Consider an s\u00d7r random matrix G 1 with s \u2265 r, whose entries are independent random variables following the standard normal distribution. Let the singular values be \u03c3 1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3 r . Then for any t > 0,\nmax P(\u03c3 1 \u2265 \u221a s + \u221a r + t), P(\u03c3 r \u2264 \u221a s \u2212 \u221a r \u2212 t) < e \u2212t 2 /2 . (4.1)\nWith the aid of Lemma 4.3, it is straightforward to obtain the concentration result of \u03c3 1 (AN ), \u03c3 r (AN ), and \u03ba(AN ) as follows.\n\nTheorem 4.4. In Algorithm 1, for any \u03b1 \u2208 (0, 1 \u2212 r/s), we have\nmax P \u03c3 1 (AN ) \u2265 1 (1\u2212\u03b1) \u221a s\u2212 \u221a r , P \u03c3 r (AN ) \u2264 1 (1+\u03b1) \u221a s+ \u221a r < e \u2212\u03b1 2 s/2 (4.2)\nand P \u03ba(AN ) = \u03c3 1 (AN ) \u03c3 r (AN ) \u2264 1 + \u03b1 + r/s\n1 \u2212 \u03b1 \u2212 r/s \u2265 1 \u2212 2e \u2212\u03b1 2 s/2 . (4.3)\nProof. Set t = \u03b1 \u221a s in Lemma 4.3. In order to estimate the number of iterations for CG-like methods, we can now combine (2.2) and (4.3).\n\nTheorem 4.5. In exact arithmetic, given a tolerance \u03b5 > 0, a CG-like method applied to the preconditioned system min y AN y \u2212 b 2 with y (0) = 0 converges within (log \u03b5 \u2212 log 2)/ log(\u03b1 + r/s) iterations in the sense that\n\u0177 CG \u2212 y * (AN ) T (AN ) \u2264 \u03b5 y * (AN ) T (AN ) (4.4)\nholds with probability at least 1 \u2212 2e \u2212\u03b1 2 s/2 for any \u03b1 \u2208 (0, 1 \u2212 s/r), where\u0177 CG is the approximate solution returned by the CG-like solver and y * = (AN ) \u2020 b. Let x CG = N\u0177 CG be the approximate solution to the original problem. Since x * = N y * , (4.4) is equivalent to 5) or in terms of residuals,\nx CG \u2212 x * A T A \u2264 \u03b5 x * A T A ,(4.r CG \u2212 r * 2 \u2264 \u03b5 b \u2212 r * 2 , (4.6) wherer CG = b \u2212 Ax CG and r * = b \u2212 Ax * .\nIn addition to allowing us to bound the number of iterations for CG-like methods, the result given by (4.2) also allows us to use the CS method. This method needs only one level-1 and two level-2 BLAS operations per iteration; and, importantly, because it doesn't have vector inner products that require synchronization between nodes, this method is suitable for clusters with high communication cost. It does need an explicit bound on the singular values, but once that bound is tight, the CS method has the same theoretical upper bound on the convergence rate as other CG-like methods. Unfortunately, in many cases, it is hard to obtain such an accurate bound, which prevents the CS method becoming popular in practice. In our case, however, (4.2) provides a probabilistic bound with very high confidence. Hence, we can employ the CS method without difficulty. For completeness, Algorithm 3 describes the CS method we implemented for solving LS problems. For discussion of its variations, see Gutknecht and Rollin [12]. where lower-order terms are ignored. Here, flops(randn) is the average flop count to generate a sample from the standard normal distribution, while flops(Av) and\n\n\nAlgorithm 3 Chebyshev semi-iterative (CS) method (computes x \u2248 A \u2020 b)\n\n1: Given A \u2208 R m\u00d7n , b \u2208 R m , and a tolerance \u03b5 > 0, choose 0 < \u03c3 L \u2264 \u03c3 U such that all non-zero singular values of A are in [\u03c3 L , \u03c3 U ] and let d = (\u03c3 2 U + \u03c3 2 L )/2 and\nc = (\u03c3 2 U \u2212 \u03c3 2 L )/2. 2: Let x = 0, v = 0, and r = b. 3: for k = 0, 1, . . . , (log \u03b5 \u2212 log 2) / log \u03c3 U \u2212\u03c3 L \u03c3 U +\u03c3 L do 4: \u03b2 \u2190 \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 if k = 0, 1 2 (c/d) 2 if k = 1, (\u03b1c/2) 2 otherwise, \u03b1 \u2190 \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1/d if k = 0, d \u2212 c 2 /(2d) if k = 1, 1/(d \u2212 \u03b1c 2 /4) otherwise.\n\n5:\n\nv \u2190 \u03b2v + A T r 6:\n\nx \u2190 x + \u03b1v Comparing this with the SVD approach, which uses 2mn 2 + 11n 3 flops, we find LSRN requires more flops, even if we only consider computing\u00c3 and its SVD. However, the actual running time is not fully characterized by the number of flops. A matrixmatrix multiplication is much faster than an SVD with the same number of flops. We empirically compare the running time in Section 6. If A is a sparse matrix, we generally have flops(Av) and flops(A T u) of order O(m). In this case, LSRN should run considerably faster than the SVD approach. Finally, if A is an operator, it is hard to apply SVD, while LSRN still works without any modification. If we set \u03b3 = 2 and \u03b5 = 10 \u221214 , we know N iter \u2248 100 by Theorem 4.5 and hence LSRN needs approximately 2n + 200 matrix-vector multiplications.\n\nOne advantage of LSRN is that the stages of generating G and computing\u00c3 = GA are embarrassingly parallel. Thus, it is easy to implement LSRN in parallel. For example, on a shared-memory machine using p cores, the total running time decreases to\nT mt,p LSRN = T randn /p + T mult /p + T mt,p svd + T iter /p,(4.7)\nwhere T randn , T mult , and T iter are the running times for the respective stages if LSRN runs on a single core, T mt,p svd is the running time of SVD using p cores, and communication cost among threads is ignored. Hence, multi-threaded LSRN has very good scalability with near-linear speedup.\n\nAlternatively, let us consider a cluster of size p using MPI, where each node stores a portion of rows of A (with m n). Each node can generate random samples and do the multiplication independently, and then an MPI Reduce operation is needed to obtain\u00c3. Since n is small, the SVD of\u00c3 and the preconditioner N are computed on a single node and distributed to all the other nodes via an MPI Bcast operation. If the CS method is chosen as the iterative solver, we need one MPI Allreduce operation per iteration in order to apply A T . Note that all the MPI operations that LSRN uses are collective. If we assume the cluster is homogeneous and has perfect load balancing, the time complexity to perform a collective operation should be O(log p). Hence the total running time becomes\nT mpi,p LSRN = T randn /p + T mult /p + T svd + T iter /p + (C 1 + C 2 N iter )O(log p),(4.8)\nwhere C 1 corresponds to the cost of computing\u00c3 and broadcasting N , and C 2 corresponds to the cost of applying A T at each iteration. Therefore, the MPI implementation of LSRN still has good scalability as long as T svd is not dominant, i.e., as long as\u00c3 is not too big. Typical values of n (or m for under-determined problems) in our empirical evaluations are around 1000, and thus this is the case.\n\n\nTikhonov regularization.\n\nWe point out that it is easy to extend LSRN to handle certain types of Tikhonov regularization, also known as ridge regression. Recall that Tikhonov regularization involves solving the problem\nminimize 1 2 Ax \u2212 b 2 2 + 1 2 W x 2 2 ,(5.1)\nwhere W \u2208 R n\u00d7n controls the regularization term. In many cases, W is chosen as \u03bbI n for some value of a regularization parameter \u03bb > 0. It is easy to see that (5.1) is equivalent to the following LS problem, without any regularization:\nminimize 1 2 A W x \u2212 b 0 2 2 . (5.2)\nThis is an over-determined problem of size (m + n) \u00d7 n. If m n, then we certainly have m + n n. Therefore, if m n, we can directly apply LSRN to (5.2) in order to solve (5.1). On the other hand, if m n, then although (5.2) is still over-determined, it is \"nearly square,\" in the sense that m + n is only slightly larger than n. In this regime, random sampling methods and random projection methods like LSRN do not perform well. In order to deal with this regime, note that (5.1) is equivalent to\nminimize 1 2 r 2 2 + 1 2 W x 2 2 subject to Ax + r = b,\nwhere r = b \u2212 Ax is the residual vector. (Note that we use r to denote the matrix rank in a scalar context and the residual vector in a vector context.) By introducing z = W x and assuming that W is non-singular, we can re-write the above problem as\nminimize 1 2 z r 2 2 subject to AW \u22121 I m z r = b,\ni.e., as computing the min-length solution to   (5.3), denoted by z * r * . The solution to the original problem (5.1) is then given by x * = W \u22121 z * . Here, we assume that W \u22121 is easy to apply, as is the case when W = \u03bbI n , so that AW \u22121 can be treated as an operator. The equivalence between (5.1) and (5.3) was first established by Herman, Lent, and Hurwitz [14].\nAW \u22121 I m z r = b.\nIn most applications of regression analysis, the amount of regularization, e.g., the optimal regularization parameter, is unknown and thus determined by crossvalidation. This requires solving a sequence of LS problems where only W differs. For over-determined problems, we only need to perform a random normal projection on A once. The marginal cost to solve for each W is the following: a random normal projection on W , an SVD of size \u03b3n \u00d7 n, and a predictable number of iterations. Similar results hold for under-determined problems when each W is a multiple of the identity matrix.\n\n6. Numerical experiments. We implemented our LS solver LSRN and compared it with competing solvers: LAPACK's DGELSD, MATLAB's backslash, and Blendenpik by Avron, Maymounkov, and Toledo [1]. MATLAB's backslash uses different algorithms for different problem types. For sparse rectangular systems, as stated by Tim Davis 1 , \"SuiteSparseQR [4,5] is now QR in MATLAB 7.9 and x = A\\b when A is sparse and rectangular.\" Table 6.1 summarizes the properties of those solvers. We report our empirical results in this section.\n\n6.1. Implementation and system setup. The experiments were performed on either a local shared-memory machine or a virtual cluster hosted on Amazon's Elastic Compute Cloud (EC2). The shared-memory machine has 12 Intel Xeon CPU cores at clock rate 2GHz with 128GB RAM. The virtual cluster consists of 20 m1.large instances configured by a third-party tool called StarCluster 2 . An m1.large instance has 2 virtual cores with 2 EC2 Compute Units 3 each. To attain top performance on the shared-memory machine, we implemented a multi-threaded version of LSRN in C, and to make our solver general enough to handle large problems on clusters, we also implemented an MPI version of LSRN in Python with NumPy, SciPy, and mpi4py. Both packages are available for download 4 . We use the multi-threaded implementation to compare LSRN with other LS solvers and use the MPI implementation to explore scalability and to compare iterative solvers under a cluster environment. To generate values from the standard normal distribution, we adopted the code from Marsaglia and Tsang [18] and modified it to use threads; this can generate a billion samples in less than two seconds on the shared-memory machine. We also modified Blendenpik to call multi-threaded FFTW routines. Blendenpik's default settings were used, i.e., using randomized discrete Fourier transform and sampling 4 min(m, n) rows/columns. All LAPACK's LS solvers, Blendenpik, and LSRN are linked against MATLAB's own multi-threaded BLAS and LAPACK libraries. So, in general, this is a fair setup because all the solvers can use multi-threading automatically and are linked against the same BLAS and LAPACK libraries. The running times were measured in wallclock times.\n\n6.2. \u03ba(AN ) and number of iterations. Recall that Theorem 4.4 states that \u03ba(AN ), the condition number of the preconditioned system, is roughly bounded by (1 + r/s)/(1 \u2212 r/s) when s is large enough such that we can ignore \u03b1 in practice. To verify this statement, we generate random matrices of size 10 4 \u00d710 3 with condition numbers ranged from 10 2 to 10 8 . The left figure in Figure 6.1 compares \u03ba(AN ) with \u03ba + (A), the effective condition number of A, under different choices of s and r. We take the largest value of \u03ba(AN ) in 10 independent runs as the \u03ba(AN ) in the plot. For each pair of s and r, the corresponding estimate (1+ r/s)/(1\u2212 r/s) is drawn in a dotted line of the same color, if not overlapped with the solid line of \u03ba(AN ). We see that (1 + r/s)/(1 \u2212 r/s) is indeed an accurate estimate of the upper bound on \u03ba(AN ). Moreover, \u03ba(AN ) is not only independent of \u03ba + (A), but it is also quite small. For example, we have (1+ r/s)/(1\u2212 r/s) < 6 if s > 2r, and hence we can expect super fast convergence of CG-like methods. Based on Theorem 4.5, the number of iterations should be less than (log \u03b5 \u2212 log 2)/ log r/s, where \u03b5 is a given tolerance. In order to match the accuracy of direct solvers, we set \u03b5 = 10 \u221214 . The right figure in Figure 6.1 shows the number of LSQR iterations for different combinations of r/s and \u03ba + (A). Again, we take the largest iteration number in 10 independent runs for each pair of r/s and \u03ba + (A). We also draw the theoretical upper bound (log \u03b5 \u2212 log 2)/ log r/s in a dotted line. We see that the number of iterations is basically a function of r/s, independent of \u03ba + (A), and the theoretical upper bound is very good in practice. This confirms that the number of iterations is fully predictable given \u03b3. 6.3. Tuning the oversampling factor \u03b3. Once we set the tolerance and maximum number of iterations, there is only one parameter left: the oversampling factor \u03b3. To demonstrate the impact of \u03b3, we fix problem size to 10 5 \u00d7 10 3 and condition number to 10 6 , set the tolerance to 10 \u221214 , and then solve the problem with \u03b3 ranged from 1.2 to 3. Figure 6.2 illustrates how \u03b3 affects the running times of LSRN's stages: randn for generating random numbers, mult for computing\u00c3 = GA, svd for comput-ing\u03a3 and\u1e7c from\u00c3, and iter for LSQR. We see that, the running times of randn, mult, and svd increase linearly as \u03b3 increases, while iter time decreases. Therefore there exists an optimal choice of \u03b3. For this particular problem, we should choose \u03b3 between 1.8 and 2.2. We experimented with various LS problems. The best choice of \u03b3 ranges from 1.6 to 2.5, depending on the type and the size of the problem. We also note that, when \u03b3 is given, the running time of the iteration stage is fully predictable. Thus we can initialize LSRN by measuring randn/sec and flops/sec for matrix-vector multiplication, matrix-matrix multiplication, and SVD, and then determine the best value of \u03b3 by minimizing the total running time (4.8). For simplicity, we set \u03b3 = 2.0 in all later experiments; although this is not the optimal setting for all cases, it is always a reasonable choice.\n\n6.4. Dense least squares. As the state-of-the-art dense linear algebra library, LAPACK provides several routines for solving LS problems, e.g., DGELS, DGELSY, and DGELSD. DGELS uses QR factorization without pivoting, which cannot handle rank-deficient problems. DGELSY uses QR factorization with pivoting, which is more reliable than DGELS on rank-deficient problems. DGELSD uses SVD. It is the most reliable routine, and should be the most expensive as well. However, we find that DGELSD actually runs much faster than DGELSY on strongly over-or underdetermined systems on the shared-memory machine. It may be because of better use of multi-threaded BLAS, but we don't have a definitive explanation. Figure 6.3 compares the running times of LSRN and competing solvers on randomly generated full-rank dense strongly over-or under-determined problems. We set the condition numbers to 10 6 for all problems. Note that DGELS and DGELSD almost overlapped. The results show that Blendenpik is the winner. For small-sized problems (m \u2264 3e4), the follow-ups are DGELS and DGELSD. When the problem size goes larger, LSRN becomes faster than DGELS/DGELSD. DGELSY is always slower than DGELS/DGELSD, but still faster than MATLAB's backslash. The performance  of LAPACK's solvers decreases significantly for under-determined problems. We monitored CPU usage and found that they couldn't fully use all the CPU cores, i.e., they couldn't effectively call multi-threaded BLAS. Though still the best, the performance of Blendenpik also decreases. LSRN's performance does not change much.\n\nLSRN is also capable of solving rank-deficient problems, and in fact it takes advantage of any rank-deficiency (in that it finds a solution in fewer iterations). Figure 6.4 shows the results on over-and under-determined rank-deficient problems generated the same way as in previous experiments, except that we set r = 800. DGELSY and DGELSD remain the same speed on over-determined problems as in full-rank cases, respectively, and run slightly faster on under-determined problems. LSRN's running times reduce to 93 seconds on the problem of size 10 6 \u00d7 10 3 , from 100 seconds on its full-rank counterpart.\n\nWe see that, for strongly over-or under-determined problems, DGELSD is the fastest and most reliable routine among the LS solvers provided by LAPACK. However, it (or any other LAPACK solver) runs much slower on under-determined prob- lems than on over-determined problems, while LSRN works symmetrically on both cases. Blendenpik is the fastest dense least squares solver in our tests. Though it is not designed for solving rank-deficient problems, Blendenpik should be modifiable to handle such problems following Theorem 3.2. We also note that Blendenpik's performance depends on the distribution of the row norms of U . We generate test problems randomly so that the row norms of U are homogeneous, which is ideal for Blendenpik. When the row norms of U are heterogeneous, Blendenpik's performance may drop. See Avron, Maymounkov, and Toledo [1] for a more detailed analysis.\n\n6.5. Sparse least squares. In LSRN, A is only involved in the computation of matrix-vector and matrix-matrix multiplications. Therefore LSRN accelerates automatically when A is sparse, without exploring A's sparsity pattern. LAPACK does not have any direct sparse LS solver. MATLAB's backslash uses SuiteSparseQR by Tim Davis [5] when A is sparse and rectangular; this requires explicit knowledge of A's sparsity pattern to obtain a sparse QR factorization.\n\nWe generated sparse LS problems using MATLAB's \"sprandn\" function with density 0.01 and condition number 10 6 . All problems have full rank. Figure 6.5 shows the results on over-determined problems. LAPACK's solvers and Blendenpik basically perform the same as in the dense case. DGELSY is the slowest among the three. DGELS and DGELSD still overlap with each other, faster than DGELSY but slower than Blendenpik. We see that MATLAB's backslash handles sparse problems very well. On the 10 6 \u00d7 10 3 problem, backslash's running time reduces to 55 seconds, from 273 seconds on the dense counterpart. The overall performance of MATLAB's backslash is better than Blendenpik's. LSRN's curve is very flat. For small problems (m \u2264 10 5 ), LSRN is slow. When m > 10 5 , LSRN becomes the fastest solver among the six. LSRN takes only 23 seconds on the over-determined problem of size 10 6 \u00d7 10 3 . On large under-determined problems, LSRN still leads by a huge margin.\n\nLSRN makes no distinction between dense and sparse problems. The speedup on sparse problems is due to faster matrix-vector and matrix-matrix multiplications. Hence, although no test was performed, we expect a similar speedup on fast linear operators as well. Also note that, in the multi-threaded implementation of LSRN, we use a naive multi-threaded routine for sparse matrix-vector and matrix-matrix Table 6.2 Real-world problems and corresponding running times in seconds. DGELSD doesn't take advantage of sparsity. Though MATLAB's backslash (SuiteSparseQR) may not give the min-length solutions to rank-deficient or under-determined problems, we still report its running times. Blendenpik either doesn't apply to rank-deficient problems or runs out of memory (OOM). LSRN's running time is mainly determined by the problem size and the sparsity. multiplications, which is far from optimized and thus leaves room for improvement.\n\n6.6. Real-world problems. In this section, we report results on some realworld large data problems. The problems are summarized in Table 6.2, along with running times.\n\nlandmark and rail4284 are from the University of Florida Sparse Matrix Collection [6]. landmark originated from a rank-deficient LS problem. rail4284 has full rank and originated from a linear programming problem on Italian railways. Both matrices are very sparse and have structured patterns. MATLAB's backslash (SuiteS-parseQR) runs extremely fast on these two problems, though it doesn't guarantee to return the min-length solution. Blendenpik is not designed to handle the rankdeficient landmark, and it unfortunately runs out of memory (OOM) on rail4284. LSRN takes 17.55 seconds on landmark and 136.0 seconds on rail4284. DGELSD is slightly slower than LSRN on landmark and much slower on rail4284.\n\ntnimg is generated from the TinyImages collection [23], which provides 80 million color images of size 32 \u00d7 32. For each image, we first convert it to grayscale, compute its two-dimensional DCT, and then only keep the top 2% largest coefficients in magnitude. This gives a sparse matrix of size 1024 \u00d7 8e7 where each column has 20 or 21 nonzero elements. Note that tnimg doesn't have apparent structured pattern. Since the whole matrix is too big, we work on submatrices of different sizes. tnimg i is the submatrix consisting of the first 10 6 \u00d7 i columns of the whole matrix for i = 1, . . . , 80, where empty rows are removed. The running times of LSRN are approximately linear in n. Both DGELSD and MATLAB's backslash are very slow on the tnimg problems. Blendenpik either doesn't apply to the rank-deficient cases or runs OOM.\n\nWe see that, though both methods taking advantage of sparsity, MATLAB's backslash relies heavily on the sparsity pattern, and its performance is unpredictable until the sparsity pattern is analyzed, while LSRN doesn't rely on the sparsity pattern and always delivers predictable performance and, moreover, the min-length solution.\n\n6.7. Scalability and choice of iterative solvers on clusters. In this section, we move to the Amazon EC2 cluster. The goals are to demonstrate that (1) LSRN scales well on clusters, and (2) the CS method is preferred to LSQR on clusters with high communication cost. The test problems are submatrices of the tnimg matrix in the previous section: tnimg 4, tnimg 10, tnimg 20, and tnimg 40, solved with 4, 10, 20, and 40 cores respectively. Each process stores a submatrix of size 1024\u00d71e6. Table  6.3 shows the results, averaged over 5 runs. Ideally, from the complexity analysis (4.8), when we double n and double the number of cores, the increase in running time should be a constant if the cluster is homogeneous and has perfect load balancing (which we Table 6.3 Test problems on the Amazon EC2 cluster and corresponding running times in seconds. When we enlarge the problem scale by a factor of 10 and increase the number of cores accordingly, the running time only increases by a factor of 50%. It shows LSRN's good scalability. Though the CS method takes more iterations, it is faster than LSQR by saving communication cost. have observed is not true on Amazon EC2). For LSRN with CS, from tnimg 10 to tnimg 20 the running time increases 27.6 seconds, and from tnimg 20 to tnimg 40 the running time increases 34.7 seconds. We believe the difference between the time increases is caused by the heterogeneity of the cluster, because Amazon EC2 doesn't guarantee the connection speed among nodes. From tnimg 4 to tnimg 40, the problem scale is enlarged by a factor of 10 while the running time only increases by a factor of 50%. The result still demonstrates LSRN's good scalability. We also compare the performance of LSQR and CS as the iterative solvers in LSRN. For all problems LSQR converges in 84 iterations and CS converges in 106 iterations. However, LSQR is slower than CS. The communication cost saved by CS is significant on those tests. As a result, we recommend CS as the default LSRN iterative solver for cluster environments. Note that to reduce the communication cost on a cluster, we could also consider increasing \u03b3 to reduce the number of iterations.\n\n\nConclusion.\n\nWe developed LSRN, a parallel solver for strongly over-or underdetermined, and possibly rank-deficient, systems. LSRN uses random normal projection to compute a preconditioner matrix for an iterative solver such as LSQR and the Chebyshev semi-iterative (CS) method. The preconditioning process is embarrassingly parallel and automatically speeds up on sparse matrices and fast linear operators, and on rank-deficient data. We proved that the preconditioned system is consistent and extremely well-conditioned, and derived strong bounds on the number of iterations of LSQR or the CS method, and hence on the total running time. On large dense systems, LSRN is competitive with the best existing solvers, and it runs significantly faster than competing solvers on strongly over-or under-determined sparse systems. LSRN is easy to implement using threads or MPI, and it scales well in parallel environments.\n\nAcknowledgements. After completing the initial version of this manuscript, we learned of the LS algorithm of Coakley et al. [2]. We thank Mark Tygert for pointing us to this reference. We are also grateful to Lisandro Dalcin, the author of mpi4py, for his own version of the MPI Barrier function to prevent idle processes from interrupting the multi-threaded SVD process too frequently.\n\n\nB \u2020 = M (BM ) \u2020 if and only if range(M M T B T ) = range(B T ), which is equivalent to saying A \u2020 = (M T A) \u2020 M T if and only if range(M M T A) = range(A).\n\n5 :\n5Let M =\u0168\u03a3 \u22121 . 6: Compute the min-length solution to min x M T Ax \u2212 M T b 2 using an iterative method, denoted byx. 7: Returnx.\n\n4. 2 .\n2Theoretical properties. The use of random normal projection offers LSRN some nice theoretical properties. We start with consistency.Theorem 4.1. In Algorithm 1, we havex = A \u2020 b almost surely.\n\n4. 3 .\n3Running time complexity. In this section, we discuss the running time complexity of LSRN. Let's first calculate the computational cost of LSRN (Algorithm 1) in terms of floating-point operations (flops). Note that we need only\u03a3 and\u1e7c but not\u0168 or a full SVD of\u00c3 in step 4 of Algorithm 1. In step 6, we assume that the dominant cost per iteration is the cost of applying AN and (AN ) T . Then the total cost is given by sm \u00d7 flops(randn) for generating G + s \u00d7 flops(A T u) for computing\u00c3 + 2sn 2 + 11n 3 for computing\u03a3 and\u1e7c [10, p. 254] + N iter \u00d7 (flops(Av) + flops(A T u) + 4nr) for solving min y AN y \u2212 b 2 ,\n\nr\n\u2190 r \u2212 \u03b1Av 8: end for flops(A T u) are the flop counts for the respective matrix-vector products. If A is a dense matrix, then we have flops(Av) = flops(A T u) = 2mn. Hence, the total cost becomes flops(LSRN dense ) = sm flops(randn) + 2smn + 2sn 2 + 11n 3 + N iter \u00d7 (4mn + 4nr).\n\n\n(5.3) is an under-determined problem of size m \u00d7 (m + n). Hence, if m n, we have m m + n and we can use LSRN to compute the min-length solution\n\nFig. 6 . 1 .\n61Left: \u03ba + (A) vs. \u03ba(AN ) for different choices of r and s. A \u2208 R 10 4 \u00d710 3 is randomly generated with rank r. For each (r, s) pair, we take the largest value of \u03ba(AN ) in 10 independent runs for each \u03ba + (A) and connect them using a solid line. The estimate (1 + r/s)/(1 \u2212 r/s) is drawn in a dotted line for each (r, s) pair, if not overlapped with the corresponding solid line. Right: number of LSQR iterations vs. r/s. The number of LSQR iterations is merely a function of r/s, independent of the condition number of the original system.\n\nFig. 6 . 2 .\n62The overall running time of LSRN and the running time of each LSRN stage with different oversampling factor \u03b3 for a randomly generated problem of size 10 5 \u00d710 3 . For this particular problem, the optimal \u03b3 that minimizes the overall running time lies in[1.8, 2.2].\n\nFig. 6 . 3 .\n63Running times on m \u00d7 1000 dense over-determined problems with full rank (left) and on 1000 \u00d7 n dense under-determined problems with full rank (right). Note that DGELS and DGELSD almost overlap. When m > 3e4, we have Blendenpik > LSRN > DGELS/DGELSD > DGELSY > A\\b in terms of speed. On under-determined problems, LAPACK's performance decreases significantly compared with the over-determined cases. Blendenpik's performance decreases as well. LSRN doesn't change much.\n\nFig. 6 . 4 .\n64Running times on m \u00d7 1000 dense over-determined problems with rank 800 (left) and on 1000 \u00d7 n dense under-determined problems with rank 800 (right). LSRN takes advantage of rank deficiency. We have LSRN > DGSLS/DGELSD > DGELSY in terms of speed.\n\nFig. 6 . 5 .\n65Running times on m \u00d7 1000 sparse over-determined problems with full rank (left) and on 1000 \u00d7 n sparse under-determined problems with full rank (right). DGELS and DGELSD overlap with each other. LAPACK's solvers and Blendenpik perform almost the same as in the dense case. Matlab's backslash speeds up on sparse problems, and performs a little better than Blendenpik, but it is still slower than LSRN. LSRN leads by a huge margin on under-determined problems as well.\n\nTable 6 .1\n6LS solvers and their properties.solver \nmin-len solution to \ntaking advantage of \nunder-det? rank-def? sparse A operator A \nLAPACK's DGELSD \nyes \nyes \nno \nno \nMATLAB's backslash \nno \nno \nyes \nno \nBlendenpik \nyes \nno \nno \nno \nLSRN \nyes \nyes \nyes \nyes \n\nto \nhttp://www.cise.ufl.edu/research/sparse/SPQR/ 2 http://web.mit.edu/stardev/cluster/ 3 \"One EC2 Compute Unit provides the equivalent CPU capacity of a 1.0-1.2 GHz 2007 Opteron or 2007 Xeon processor.\" from http://aws.amazon.com/ec2/faqs/ 4 http://www.stanford.edu/group/SOL/software/lsrn.html\n\nBlendenpik: Supercharging LAPACK's leastsquares solver. H Avron, P Maymounkov, S Toledo, SIAM J. Sci. Comput. 32H. Avron, P. Maymounkov, and S. Toledo, Blendenpik: Supercharging LAPACK's least- squares solver, SIAM J. Sci. Comput., 32 (2010), pp. 1217-1236.\n\nA fast randomized algorithm for orthogonal projection. E S Coakley, V Rokhlin, M Tygert, SIAM J. Sci. Comput. 33E. S. Coakley, V. Rokhlin, and M. Tygert, A fast randomized algorithm for orthogonal projection, SIAM J. Sci. Comput., 33 (2011), pp. 849-868.\n\nLocal operator theory, random matrices and Banach spaces. K R Davidson, S J Szarek, Handbook of the Geometry of Banach Spaces. North Holland1K. R. Davidson and S. J. Szarek, Local operator theory, random matrices and Banach spaces, in Handbook of the Geometry of Banach Spaces, vol. 1, North Holland, 2001, pp. 317-366.\n\nDirect Methods for Sparse Linear Systems. T A Davis, SIAM, PhiladelphiaT. A. Davis, Direct Methods for Sparse Linear Systems, SIAM, Philadelphia, 2006.\n\nSuiteSparseQR: Multifrontal multithreaded rank-revealing sparse QR factorization. ACM Trans. Math. Softw. 91538Algorithm, Algorithm 915, SuiteSparseQR: Multifrontal multithreaded rank-revealing sparse QR factorization, ACM Trans. Math. Softw., 38 (2011).\n\nThe University of Florida sparse matrix collection. T A Davis, Y Hu, ACM Trans. Math. Softw. 38T. A. Davis and Y. Hu, The University of Florida sparse matrix collection, ACM Trans. Math. Softw., 38 (2011).\n\nSampling algorithms for 2 regression and applications. P Drineas, M W Mahoney, S Muthukrishnan, Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms. the 17th Annual ACM-SIAM Symposium on Discrete AlgorithmsACMP. Drineas, M. W. Mahoney, and S. Muthukrishnan, Sampling algorithms for 2 regression and applications, in Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms, ACM, 2006, pp. 1127-1136.\n\nFaster least squares approximation. P Drineas, M W Mahoney, S Muthukrishnan, T Sarl\u00f3s, Numer. Math. 117P. Drineas, M. W. Mahoney, S. Muthukrishnan, and T. Sarl\u00f3s, Faster least squares approximation, Numer. Math., 117 (2011), pp. 219-249.\n\nLSMR: An iterative algorithm for sparse least-squares problems. D. C.-L Fong, M Saunders, SIAM J. Sci. Comput. 332950D. C.-L. Fong and M. Saunders, LSMR: An iterative algorithm for sparse least-squares problems, SIAM J. Sci. Comput., 33 (2011), p. 2950.\n\n. G H Golub, C F Van Loan, Matrix Computations. Johns Hopkins Univ Pressthird ed.G. H. Golub and C. F. Van Loan, Matrix Computations, Johns Hopkins Univ Press, third ed., 1996.\n\nChebyshev semi-iterative methods, successive over-relaxation methods, and second-order Richardson iterative methods, parts I and II. G H Golub, R S Varga, Numer. Math. 3G. H. Golub and R. S. Varga, Chebyshev semi-iterative methods, successive over-relaxation methods, and second-order Richardson iterative methods, parts I and II, Numer. Math., 3 (1961), pp. 147-168.\n\nThe Chebyshev iteration revisited. M H Gutknecht, S Rollin, Parallel Comput. 28M. H. Gutknecht and S. Rollin, The Chebyshev iteration revisited, Parallel Comput., 28 (2002), pp. 263-283.\n\nFinding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. N Halko, P G Martinsson, J A Tropp, SIAM Review. 53N. Halko, P. G. Martinsson, and J. A. Tropp, Finding structure with randomness: Proba- bilistic algorithms for constructing approximate matrix decompositions, SIAM Review, 53 (2011), pp. 217-288.\n\nA storage-efficient algorithm for finding the regularized solution of a large, inconsistent system of equations. G T Herman, A Lent, H Hurwitz, IMA J. Appl. Math. 25G. T. Herman, A. Lent, and H. Hurwitz, A storage-efficient algorithm for finding the reg- ularized solution of a large, inconsistent system of equations, IMA J. Appl. Math., 25 (1980), pp. 361-366.\n\nMethods of conjugate gradients for solving linear systems. M R Hestenes, E Stiefel, J. Res. Nat. Bur. Stand. 49M. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving linear systems, J. Res. Nat. Bur. Stand., 49 (1952), pp. 409-436.\n\nD G Luenberger, Introduction to Linear and Nonlinear Programming. Addison-WesleyD. G. Luenberger, Introduction to Linear and Nonlinear Programming, Addison-Wesley, 1973.\n\nRandomized Algorithms for Matrices and Data, Foundations and Trends in Machine Learning. M W Mahoney, arXiv:1104.5557v2NOW PublishersBostonM. W. Mahoney, Randomized Algorithms for Matrices and Data, Foundations and Trends in Machine Learning, NOW Publishers, Boston, 2011. Also available at arXiv:1104.5557v2.\n\nThe ziggurat method for generating random variables. G Marsaglia, W W Tsang, J. Stat. Softw. 5G. Marsaglia and W. W. Tsang, The ziggurat method for generating random variables, J. Stat. Softw., 5 (2000), pp. 1-7.\n\nSolution of sparse indefinite systems of linear equations. C C Paige, M A Saunders, SIAM J. Numer. Anal. 12C. C. Paige and M. A. Saunders, Solution of sparse indefinite systems of linear equations, SIAM J. Numer. Anal., 12 (1975), pp. 617-629.\n\nLSQR: An algorithm for sparse linear equations and sparse least squares. ACM Trans. Math. Softw. 8, LSQR: An algorithm for sparse linear equations and sparse least squares, ACM Trans. Math. Softw., 8 (1982), pp. 43-71.\n\nA fast randomized algorithm for overdetermined linear leastsquares regression. V Rokhlin, M Tygert, Proc. Natl. Acad. Sci. USA, 105. Natl. Acad. Sci. USA, 105V. Rokhlin and M. Tygert, A fast randomized algorithm for overdetermined linear least- squares regression, Proc. Natl. Acad. Sci. USA, 105 (2008), pp. 13212-13217.\n\nImproved approximation algorithms for large matrices via random projections. T Sarl\u00f3s, Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science. the 47th Annual IEEE Symposium on Foundations of Computer ScienceIEEET. Sarl\u00f3s, Improved approximation algorithms for large matrices via random projections, in Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, IEEE, 2006, pp. 143-152.\n\n80 million tiny images: A large data set for nonparametric object and scene recognition. A Torralba, R Fergus, W T Freeman, IEEE Trans. Pattern Anal. Mach. Intell. 30A. Torralba, R. Fergus, and W. T. Freeman, 80 million tiny images: A large data set for nonparametric object and scene recognition, IEEE Trans. Pattern Anal. Mach. Intell., 30 (2008), pp. 1958-1970.\n", "annotations": {"author": "[{\"end\":97,\"start\":83},{\"end\":117,\"start\":98},{\"end\":139,\"start\":118}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":92},{\"end\":116,\"start\":108},{\"end\":138,\"start\":131}]", "author_first_name": "[{\"end\":91,\"start\":83},{\"end\":105,\"start\":98},{\"end\":107,\"start\":106},{\"end\":128,\"start\":121},{\"end\":130,\"start\":129}]", "author_affiliation": null, "title": "[{\"end\":80,\"start\":1},{\"end\":219,\"start\":140}]", "venue": null, "abstract": "[{\"end\":1970,\"start\":597}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2934,\"start\":2931},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2937,\"start\":2934},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2939,\"start\":2937},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2942,\"start\":2939},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2944,\"start\":2942},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3028,\"start\":3024},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3212,\"start\":3208},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4907,\"start\":4903},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4955,\"start\":4951},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5543,\"start\":5540},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8190,\"start\":8186},{\"end\":8460,\"start\":8455},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8677,\"start\":8673},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8702,\"start\":8698},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8713,\"start\":8709},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8909,\"start\":8905},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8922,\"start\":8919},{\"end\":9147,\"start\":9136},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9773,\"start\":9770},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10644,\"start\":10641},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10754,\"start\":10750},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11575,\"start\":11572},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12367,\"start\":12364},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24266,\"start\":24263},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25605,\"start\":25603},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26765,\"start\":26761},{\"end\":31604,\"start\":31599},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31919,\"start\":31915},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32715,\"start\":32712},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32868,\"start\":32865},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32870,\"start\":32868},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34114,\"start\":34110},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40920,\"start\":40917},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":41281,\"start\":41278},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":43560,\"start\":43557},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":44235,\"start\":44231},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":48567,\"start\":48564}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48984,\"start\":48827},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49118,\"start\":48985},{\"attributes\":{\"id\":\"fig_2\"},\"end\":49320,\"start\":49119},{\"attributes\":{\"id\":\"fig_3\"},\"end\":49939,\"start\":49321},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50222,\"start\":49940},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50368,\"start\":50223},{\"attributes\":{\"id\":\"fig_6\"},\"end\":50925,\"start\":50369},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51207,\"start\":50926},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51692,\"start\":51208},{\"attributes\":{\"id\":\"fig_9\"},\"end\":51954,\"start\":51693},{\"attributes\":{\"id\":\"fig_10\"},\"end\":52438,\"start\":51955},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":52707,\"start\":52439}]", "paragraph": "[{\"end\":3213,\"start\":1972},{\"end\":3624,\"start\":3215},{\"end\":3631,\"start\":3626},{\"end\":3798,\"start\":3633},{\"end\":4130,\"start\":3800},{\"end\":4437,\"start\":4191},{\"end\":5140,\"start\":4439},{\"end\":5713,\"start\":5142},{\"end\":6181,\"start\":5715},{\"end\":6453,\"start\":6208},{\"end\":6676,\"start\":6480},{\"end\":6933,\"start\":6779},{\"end\":7495,\"start\":6935},{\"end\":7586,\"start\":7497},{\"end\":8215,\"start\":7657},{\"end\":8947,\"start\":8217},{\"end\":9159,\"start\":8949},{\"end\":9693,\"start\":9236},{\"end\":10713,\"start\":9717},{\"end\":11525,\"start\":10715},{\"end\":12324,\"start\":11527},{\"end\":13084,\"start\":12326},{\"end\":13710,\"start\":13086},{\"end\":14689,\"start\":13712},{\"end\":15557,\"start\":14691},{\"end\":15845,\"start\":15603},{\"end\":15956,\"start\":15924},{\"end\":16474,\"start\":15982},{\"end\":16584,\"start\":16503},{\"end\":16680,\"start\":16610},{\"end\":16784,\"start\":16729},{\"end\":17220,\"start\":17030},{\"end\":17587,\"start\":17377},{\"end\":17981,\"start\":17948},{\"end\":18291,\"start\":18170},{\"end\":18588,\"start\":18463},{\"end\":18961,\"start\":18590},{\"end\":19711,\"start\":19598},{\"end\":20625,\"start\":19731},{\"end\":21234,\"start\":20627},{\"end\":21722,\"start\":21281},{\"end\":21765,\"start\":21724},{\"end\":22010,\"start\":21812},{\"end\":22215,\"start\":22101},{\"end\":22639,\"start\":22308},{\"end\":23429,\"start\":22696},{\"end\":23546,\"start\":23494},{\"end\":24471,\"start\":23703},{\"end\":24674,\"start\":24543},{\"end\":24738,\"start\":24676},{\"end\":24874,\"start\":24826},{\"end\":25050,\"start\":24913},{\"end\":25272,\"start\":25052},{\"end\":25631,\"start\":25326},{\"end\":26928,\"start\":25745},{\"end\":27175,\"start\":27002},{\"end\":27472,\"start\":27455},{\"end\":28269,\"start\":27474},{\"end\":28515,\"start\":28271},{\"end\":28879,\"start\":28584},{\"end\":29659,\"start\":28881},{\"end\":30156,\"start\":29754},{\"end\":30377,\"start\":30185},{\"end\":30659,\"start\":30423},{\"end\":31193,\"start\":30697},{\"end\":31499,\"start\":31250},{\"end\":31920,\"start\":31551},{\"end\":32525,\"start\":31940},{\"end\":33044,\"start\":32527},{\"end\":34763,\"start\":33046},{\"end\":37887,\"start\":34765},{\"end\":39461,\"start\":37889},{\"end\":40070,\"start\":39463},{\"end\":40950,\"start\":40072},{\"end\":41409,\"start\":40952},{\"end\":42371,\"start\":41411},{\"end\":43304,\"start\":42373},{\"end\":43473,\"start\":43306},{\"end\":44179,\"start\":43475},{\"end\":45012,\"start\":44181},{\"end\":45344,\"start\":45014},{\"end\":47518,\"start\":45346},{\"end\":48438,\"start\":47534},{\"end\":48826,\"start\":48440}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4190,\"start\":4131},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6778,\"start\":6677},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7656,\"start\":7587},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9235,\"start\":9160},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15923,\"start\":15846},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15981,\"start\":15957},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16502,\"start\":16475},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16609,\"start\":16585},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16728,\"start\":16681},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17029,\"start\":16785},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17376,\"start\":17221},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17947,\"start\":17588},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18169,\"start\":17982},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18462,\"start\":18292},{\"attributes\":{\"id\":\"formula_14\"},\"end\":19597,\"start\":18962},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21280,\"start\":21235},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21811,\"start\":21766},{\"attributes\":{\"id\":\"formula_17\"},\"end\":22100,\"start\":22011},{\"attributes\":{\"id\":\"formula_18\"},\"end\":22307,\"start\":22216},{\"attributes\":{\"id\":\"formula_19\"},\"end\":22695,\"start\":22640},{\"attributes\":{\"id\":\"formula_20\"},\"end\":23493,\"start\":23430},{\"attributes\":{\"id\":\"formula_21\"},\"end\":23702,\"start\":23547},{\"attributes\":{\"id\":\"formula_22\"},\"end\":24542,\"start\":24472},{\"attributes\":{\"id\":\"formula_23\"},\"end\":24825,\"start\":24739},{\"attributes\":{\"id\":\"formula_24\"},\"end\":24912,\"start\":24875},{\"attributes\":{\"id\":\"formula_25\"},\"end\":25325,\"start\":25273},{\"attributes\":{\"id\":\"formula_26\"},\"end\":25667,\"start\":25632},{\"attributes\":{\"id\":\"formula_27\"},\"end\":25744,\"start\":25667},{\"attributes\":{\"id\":\"formula_28\"},\"end\":27449,\"start\":27176},{\"attributes\":{\"id\":\"formula_29\"},\"end\":28583,\"start\":28516},{\"attributes\":{\"id\":\"formula_30\"},\"end\":29753,\"start\":29660},{\"attributes\":{\"id\":\"formula_31\"},\"end\":30422,\"start\":30378},{\"attributes\":{\"id\":\"formula_32\"},\"end\":30696,\"start\":30660},{\"attributes\":{\"id\":\"formula_33\"},\"end\":31249,\"start\":31194},{\"attributes\":{\"id\":\"formula_34\"},\"end\":31550,\"start\":31500},{\"attributes\":{\"id\":\"formula_35\"},\"end\":31939,\"start\":31921}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32949,\"start\":32942},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":42782,\"start\":42775},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":43444,\"start\":43437},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":45843,\"start\":45835},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":46109,\"start\":46102}]", "section_header": "[{\"attributes\":{\"n\":\"2.\"},\"end\":6206,\"start\":6184},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6478,\"start\":6456},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9715,\"start\":9696},{\"attributes\":{\"n\":\"3.\"},\"end\":15601,\"start\":15560},{\"attributes\":{\"n\":\"4.\"},\"end\":19729,\"start\":19714},{\"end\":27000,\"start\":26931},{\"end\":27453,\"start\":27451},{\"attributes\":{\"n\":\"5.\"},\"end\":30183,\"start\":30159},{\"attributes\":{\"n\":\"7.\"},\"end\":47532,\"start\":47521},{\"end\":48989,\"start\":48986},{\"end\":49126,\"start\":49120},{\"end\":49328,\"start\":49322},{\"end\":49942,\"start\":49941},{\"end\":50382,\"start\":50370},{\"end\":50939,\"start\":50927},{\"end\":51221,\"start\":51209},{\"end\":51706,\"start\":51694},{\"end\":51968,\"start\":51956},{\"end\":52450,\"start\":52440}]", "table": "[{\"end\":52707,\"start\":52484}]", "figure_caption": "[{\"end\":48984,\"start\":48829},{\"end\":49118,\"start\":48991},{\"end\":49320,\"start\":49128},{\"end\":49939,\"start\":49330},{\"end\":50222,\"start\":49943},{\"end\":50368,\"start\":50225},{\"end\":50925,\"start\":50385},{\"end\":51207,\"start\":50942},{\"end\":51692,\"start\":51224},{\"end\":51954,\"start\":51709},{\"end\":52438,\"start\":51971},{\"end\":52484,\"start\":52452}]", "figure_ref": "[{\"end\":35152,\"start\":35144},{\"end\":36025,\"start\":36017},{\"end\":36873,\"start\":36865},{\"end\":38598,\"start\":38590},{\"end\":39633,\"start\":39625},{\"end\":41560,\"start\":41552}]", "bib_author_first_name": "[{\"end\":53058,\"start\":53057},{\"end\":53067,\"start\":53066},{\"end\":53081,\"start\":53080},{\"end\":53316,\"start\":53315},{\"end\":53318,\"start\":53317},{\"end\":53329,\"start\":53328},{\"end\":53340,\"start\":53339},{\"end\":53575,\"start\":53574},{\"end\":53577,\"start\":53576},{\"end\":53589,\"start\":53588},{\"end\":53591,\"start\":53590},{\"end\":53880,\"start\":53879},{\"end\":53882,\"start\":53881},{\"end\":54299,\"start\":54298},{\"end\":54301,\"start\":54300},{\"end\":54310,\"start\":54309},{\"end\":54509,\"start\":54508},{\"end\":54520,\"start\":54519},{\"end\":54522,\"start\":54521},{\"end\":54533,\"start\":54532},{\"end\":54928,\"start\":54927},{\"end\":54939,\"start\":54938},{\"end\":54941,\"start\":54940},{\"end\":54952,\"start\":54951},{\"end\":54969,\"start\":54968},{\"end\":55201,\"start\":55194},{\"end\":55209,\"start\":55208},{\"end\":55388,\"start\":55387},{\"end\":55390,\"start\":55389},{\"end\":55399,\"start\":55398},{\"end\":55401,\"start\":55400},{\"end\":55697,\"start\":55696},{\"end\":55699,\"start\":55698},{\"end\":55708,\"start\":55707},{\"end\":55710,\"start\":55709},{\"end\":55968,\"start\":55967},{\"end\":55970,\"start\":55969},{\"end\":55983,\"start\":55982},{\"end\":56233,\"start\":56232},{\"end\":56242,\"start\":56241},{\"end\":56244,\"start\":56243},{\"end\":56258,\"start\":56257},{\"end\":56260,\"start\":56259},{\"end\":56594,\"start\":56593},{\"end\":56596,\"start\":56595},{\"end\":56606,\"start\":56605},{\"end\":56614,\"start\":56613},{\"end\":56904,\"start\":56903},{\"end\":56906,\"start\":56905},{\"end\":56918,\"start\":56917},{\"end\":57097,\"start\":57096},{\"end\":57099,\"start\":57098},{\"end\":57357,\"start\":57356},{\"end\":57359,\"start\":57358},{\"end\":57632,\"start\":57631},{\"end\":57645,\"start\":57644},{\"end\":57647,\"start\":57646},{\"end\":57852,\"start\":57851},{\"end\":57854,\"start\":57853},{\"end\":57863,\"start\":57862},{\"end\":57865,\"start\":57864},{\"end\":58337,\"start\":58336},{\"end\":58348,\"start\":58347},{\"end\":58658,\"start\":58657},{\"end\":59107,\"start\":59106},{\"end\":59119,\"start\":59118},{\"end\":59129,\"start\":59128},{\"end\":59131,\"start\":59130}]", "bib_author_last_name": "[{\"end\":53064,\"start\":53059},{\"end\":53078,\"start\":53068},{\"end\":53088,\"start\":53082},{\"end\":53326,\"start\":53319},{\"end\":53337,\"start\":53330},{\"end\":53347,\"start\":53341},{\"end\":53586,\"start\":53578},{\"end\":53598,\"start\":53592},{\"end\":53888,\"start\":53883},{\"end\":54307,\"start\":54302},{\"end\":54313,\"start\":54311},{\"end\":54517,\"start\":54510},{\"end\":54530,\"start\":54523},{\"end\":54547,\"start\":54534},{\"end\":54936,\"start\":54929},{\"end\":54949,\"start\":54942},{\"end\":54966,\"start\":54953},{\"end\":54976,\"start\":54970},{\"end\":55206,\"start\":55202},{\"end\":55218,\"start\":55210},{\"end\":55396,\"start\":55391},{\"end\":55410,\"start\":55402},{\"end\":55705,\"start\":55700},{\"end\":55716,\"start\":55711},{\"end\":55980,\"start\":55971},{\"end\":55990,\"start\":55984},{\"end\":56239,\"start\":56234},{\"end\":56255,\"start\":56245},{\"end\":56266,\"start\":56261},{\"end\":56603,\"start\":56597},{\"end\":56611,\"start\":56607},{\"end\":56622,\"start\":56615},{\"end\":56915,\"start\":56907},{\"end\":56926,\"start\":56919},{\"end\":57110,\"start\":57100},{\"end\":57367,\"start\":57360},{\"end\":57642,\"start\":57633},{\"end\":57653,\"start\":57648},{\"end\":57860,\"start\":57855},{\"end\":57874,\"start\":57866},{\"end\":58345,\"start\":58338},{\"end\":58355,\"start\":58349},{\"end\":58665,\"start\":58659},{\"end\":59116,\"start\":59108},{\"end\":59126,\"start\":59120},{\"end\":59139,\"start\":59132}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15880145},\"end\":53258,\"start\":53001},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17172436},\"end\":53514,\"start\":53260},{\"attributes\":{\"id\":\"b2\"},\"end\":53835,\"start\":53516},{\"attributes\":{\"id\":\"b3\"},\"end\":53988,\"start\":53837},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9886323},\"end\":54244,\"start\":53990},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":207191190},\"end\":54451,\"start\":54246},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2415336},\"end\":54889,\"start\":54453},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7730230},\"end\":55128,\"start\":54891},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9281514},\"end\":55383,\"start\":55130},{\"attributes\":{\"id\":\"b9\"},\"end\":55561,\"start\":55385},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":218521070},\"end\":55930,\"start\":55563},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16739214},\"end\":56118,\"start\":55932},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":88251},\"end\":56478,\"start\":56120},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":122033652},\"end\":56842,\"start\":56480},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2207234},\"end\":57094,\"start\":56844},{\"attributes\":{\"id\":\"b15\"},\"end\":57265,\"start\":57096},{\"attributes\":{\"doi\":\"arXiv:1104.5557v2\",\"id\":\"b16\"},\"end\":57576,\"start\":57267},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":51729074},\"end\":57790,\"start\":57578},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":122319074},\"end\":58035,\"start\":57792},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":21774},\"end\":58255,\"start\":58037},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8351640},\"end\":58578,\"start\":58257},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1299951},\"end\":59015,\"start\":58580},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7487588},\"end\":59381,\"start\":59017}]", "bib_title": "[{\"end\":53055,\"start\":53001},{\"end\":53313,\"start\":53260},{\"end\":53572,\"start\":53516},{\"end\":54070,\"start\":53990},{\"end\":54296,\"start\":54246},{\"end\":54506,\"start\":54453},{\"end\":54925,\"start\":54891},{\"end\":55192,\"start\":55130},{\"end\":55694,\"start\":55563},{\"end\":55965,\"start\":55932},{\"end\":56230,\"start\":56120},{\"end\":56591,\"start\":56480},{\"end\":56901,\"start\":56844},{\"end\":57629,\"start\":57578},{\"end\":57849,\"start\":57792},{\"end\":58108,\"start\":58037},{\"end\":58334,\"start\":58257},{\"end\":58655,\"start\":58580},{\"end\":59104,\"start\":59017}]", "bib_author": "[{\"end\":53066,\"start\":53057},{\"end\":53080,\"start\":53066},{\"end\":53090,\"start\":53080},{\"end\":53328,\"start\":53315},{\"end\":53339,\"start\":53328},{\"end\":53349,\"start\":53339},{\"end\":53588,\"start\":53574},{\"end\":53600,\"start\":53588},{\"end\":53890,\"start\":53879},{\"end\":54309,\"start\":54298},{\"end\":54315,\"start\":54309},{\"end\":54519,\"start\":54508},{\"end\":54532,\"start\":54519},{\"end\":54549,\"start\":54532},{\"end\":54938,\"start\":54927},{\"end\":54951,\"start\":54938},{\"end\":54968,\"start\":54951},{\"end\":54978,\"start\":54968},{\"end\":55208,\"start\":55194},{\"end\":55220,\"start\":55208},{\"end\":55398,\"start\":55387},{\"end\":55412,\"start\":55398},{\"end\":55707,\"start\":55696},{\"end\":55718,\"start\":55707},{\"end\":55982,\"start\":55967},{\"end\":55992,\"start\":55982},{\"end\":56241,\"start\":56232},{\"end\":56257,\"start\":56241},{\"end\":56268,\"start\":56257},{\"end\":56605,\"start\":56593},{\"end\":56613,\"start\":56605},{\"end\":56624,\"start\":56613},{\"end\":56917,\"start\":56903},{\"end\":56928,\"start\":56917},{\"end\":57112,\"start\":57096},{\"end\":57369,\"start\":57356},{\"end\":57644,\"start\":57631},{\"end\":57655,\"start\":57644},{\"end\":57862,\"start\":57851},{\"end\":57876,\"start\":57862},{\"end\":58347,\"start\":58336},{\"end\":58357,\"start\":58347},{\"end\":58667,\"start\":58657},{\"end\":59118,\"start\":59106},{\"end\":59128,\"start\":59118},{\"end\":59141,\"start\":59128}]", "bib_venue": "[{\"end\":53109,\"start\":53090},{\"end\":53368,\"start\":53349},{\"end\":53641,\"start\":53600},{\"end\":53877,\"start\":53837},{\"end\":54094,\"start\":54072},{\"end\":54337,\"start\":54315},{\"end\":54621,\"start\":54549},{\"end\":54989,\"start\":54978},{\"end\":55239,\"start\":55220},{\"end\":55431,\"start\":55412},{\"end\":55729,\"start\":55718},{\"end\":56007,\"start\":55992},{\"end\":56279,\"start\":56268},{\"end\":56641,\"start\":56624},{\"end\":56951,\"start\":56928},{\"end\":57160,\"start\":57112},{\"end\":57354,\"start\":57267},{\"end\":57669,\"start\":57655},{\"end\":57895,\"start\":57876},{\"end\":58132,\"start\":58110},{\"end\":58388,\"start\":58357},{\"end\":58747,\"start\":58667},{\"end\":59179,\"start\":59141},{\"end\":54680,\"start\":54623},{\"end\":58415,\"start\":58390},{\"end\":58814,\"start\":58749}]"}}}, "year": 2023, "month": 12, "day": 17}