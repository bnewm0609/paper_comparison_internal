{"id": 210173022, "updated": "2022-02-01 03:59:54.931", "metadata": {"title": "Nuclei Segmentation Using Mixed Points and Masks Selected From Uncertainty", "authors": "[{\"middle\":[],\"last\":\"Qu\",\"first\":\"Hui\"},{\"middle\":[],\"last\":\"Yi\",\"first\":\"Jingru\"},{\"middle\":[],\"last\":\"Huang\",\"first\":\"Qiaoying\"},{\"middle\":[],\"last\":\"Wu\",\"first\":\"Pengxiang\"},{\"middle\":[],\"last\":\"Metaxas\",\"first\":\"Dimitris\"}]", "venue": "2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)", "journal": "2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Weakly supervised learning has drawn much attention to mitigate the manual effort of annotating pixel-level labels for segmentation tasks. In nuclei segmentation, point annotation has been successfully used for training. However, points lack the shape information. Thus the segmentation of nuclei with nonuniform color is unsatisfactory. In this paper, we propose a framework of weakly supervised nuclei segmentation using mixed points and masks annotation. To save the extra annotation effort, we select typical nuclei to annotate masks from uncertainty map. Using Bayesian deep learning tools, we first train a model with points annotation to predict the uncertainty. Then we utilize the uncertainty map to select the representative hard nuclei for mask annotation automatically. The selected nuclear masks are combined with points to train a better segmentation model. Experimental results on two nuclei segmentation datasets prove the effectiveness of our method. The code is publicly available11https://github.com/huiqu18/WeaklySegMixedAnno.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3026704210", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/isbi/QuYHWM20", "doi": "10.1109/isbi45749.2020.9098474"}}, "content": {"source": {"pdf_hash": "ceb223bac7909b5ab0766aec8aba5504441ce8cf", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8997db4c0ec6630eb6a9a48e14a37ce422a8784f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ceb223bac7909b5ab0766aec8aba5504441ce8cf.txt", "contents": "\nNUCLEI SEGMENTATION USING MIXED POINTS AND MASKS SELECTED FROM UNCERTAINTY\n\n\nHui Qu \nDepartment of Computer Science\nRutgers University\n\n\nJingru Yi \nDepartment of Computer Science\nRutgers University\n\n\nQiaoying Huang \nDepartment of Computer Science\nRutgers University\n\n\nPengxiang Wu \nDepartment of Computer Science\nRutgers University\n\n\nDimitris Metaxas \nDepartment of Computer Science\nRutgers University\n\n\nNUCLEI SEGMENTATION USING MIXED POINTS AND MASKS SELECTED FROM UNCERTAINTY\nIndex Terms-Nuclei segmentationweakly supervised learninguncertaintydeep learning\nWeakly supervised learning has drawn much attention to mitigate the manual effort of annotating pixel-level labels for segmentation tasks. In nuclei segmentation, point annotation has been successfully used for training. However, points lack the shape information. Thus the segmentation of nuclei with nonuniform color is unsatisfactory. In this paper, we propose a framework of weakly supervised nuclei segmentation using mixed points and masks annotation. To save the extra annotation effort, we select typical nuclei to annotate masks from uncertainty map. Using Bayesian deep learning tools, we first train a model with points annotation to predict the uncertainty. Then we utilize the uncertainty map to select the representative hard nuclei for mask annotation automatically. The selected nuclear masks are combined with points to train a better segmentation model. Experimental results on two nuclei segmentation datasets prove the effectiveness of our method. The code is publicly available 1 .\n\nINTRODUCTION\n\nNuclei segmentation, which extracts the pixel-wise mask of each nucleus in the image, is an important yet challenging task in pathology image analysis. The variations in nuclear size, shape and color make it hard to obtain good segmentation performance with traditional algorithms. It is also difficult to separate crowded and touching nuclei. The fullysupervised learning of deep convolutional neural networks (CNNs) [1,2,3,4,5] have achieved great success in handling these issues. However, they require precise pixel-wise labels for training, which are annotated manually by pathologists. It is painstaking and time-consuming to obtain these labels because of the large number and small size of nuclei.\n\nOne way to alleviate the annotation burden is weakly supervised learning. It adopts weak labels as supervision and has drawn much attention in both natural and medical image segmentation. Papandreou et al. [6] develop an online Expectation-Maximization (EM) method for training CNNs from image-level tags. Lin et al. [7] use a graphical model 1 https://github.com/huiqu18/WeaklySegMixedAnno to propagate the information from scribbles annotation to the unmarked pixels and learned a CNN by the propagated labels. Bearman et al. [8] incorporate an objectiveness prior derived from points annotation for training. And bounding boxes are also utilized for weakly-supervised training [9]. Among the above four types of weak annotations, the information in the labels increase from image tags to bounding boxes, but the annotation effort also becomes more extensive. For nuclei segmentation, points annotation is the best trade-off between information and annotation effort. Qu et al. [10] generated two proxy labels from the labeled points and trained a model with the two types of labels. The overall performance is good, but it is not able to do well in nuclei with non-uniform color even with the dense conditional random field (CRF) loss.\n\nIn this paper we propose to add a few nuclear masks to the points to further reduce the gap between the weakly supervised method and fully-supervised one. Instead of randomly choosing nuclear masks, we select representative hard nuclei to better promote the performance with limited extra annotation effort. We make use of the Bayesian deep learning method [11] to predict the uncertainty in the segmentation results, and pick 5% nuclei with top uncertainty values to annotate the full masks. The mixed points and masks labels are used to train the model based on a similar framework in [10] without the dense CRF loss. Experimental results demonstrate that our method can achieve evident performance gain with the expense of a little annotation effort for the masks.\n\n\nMETHOD\n\nThe proposed framework ( Fig. 1) consists of two steps: (1) uncertainty prediction using Bayesian CNN, (2) model training using mixed points and selectedly annotated masks from uncertainty.\n\n\nUncertainty prediction\n\nThere are two major types of uncertainty: aleatoric uncertainty and epistemic uncertainty [11]. The former captures the noise in the input data, which is also called data uncertainty. The latter captures the ignorance about which model generated the data and is referred as model uncertainty [11]. We focus on the data uncertainty because we want to find representative nuclei for mask annotation.  The uncertainty prediction part ( Fig. 1(a)) is based on the architecture in [10]. Like [11], we change the CNN into a Bayesian CNN by placing a Gaussian distribution over the vector f W i before the last softmax layer for each pixel i:\nx i |W \u223c N (f W i , (\u03c3 W i ) 2 ),(1)\nwhere W is the parameter matrix of the network. (\u03c3 W i ) 2 is the Gaussian noise variance (a diagonal matrix with one element for each logit value) and treated as the data uncertainty. The corrupted vectorx i is squashed with the softmax function to obtain the probability vector p i for pixel i. The loss function is an expectation over the Gaussian distribution. Monte Carlo sampling is used to approximate the expected loss because we have no idea about the true distribution. Specifically, we draw samples from the logits of f W i , and rewrite Eqn. (1) a\u015d\nx i,t = f W i + \u03c3 W i t , t \u223c N (0, I),(2)\nwhere t \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , T } and T is the sampling number. The final output for pixel i is\ny i = 1 T t Softmax(x i,t ).\nThe original points label is not sufficient to supervise the training. Therefore, two types of pixel-wise proxy labels are derived using k-means clustering and Voronoi diagram partition algorithms as in [10], namely the cluster label and Voronoi label (see Fig. 1). In the labels, the pixels with red and green colors belong to background and nuclei, respectively. Pixels in black are ignored during training.\n\nThe loss function of the Bayesian model is the sum of cross entropy loss for each type of labels: where q i is the true label for pixel i, \u2126 is the set consisting of non-ignored pixels.\nL cluster/vor = \u2212 1 |\u2126| i\u2208\u2126 [q i log y i + (1 \u2212 q i ) log(1 \u2212 y i )] , L ce = L vor + L cluster ,(3)\n\nTraining with mixed points and selected masks 2.2.1. Nuclei selection for annotation\n\nWith the predicted uncertainty map (see Fig. 2(b)), the average uncertainty value within an area of radius 10 centered at each nuclear point is calculated and treated as the uncertainty of that nucleus. Since we model the data uncertainty, the nuclei with high uncertainty values have large noise in the Voronoi label or cluster label. They often have non-uniform colors or similar color as the background, for example, some nuclei in the top right area of Fig. 2(a). They are hard to be accurately segmented using only points label. Therefore we sort the uncertainty values of all nuclei and select the top 5% representative nuclei for mask annotation (see Fig. 2(c)).\n\n\nRevising labels with masks\n\nFor the selected nuclei, the masks are more accurate than their points annotation. Therefore, we integrate their masks into the cluster and Voronoi labels. In the cluster label, we copy the nuclei masks to the corresponding areas and add a small dilated area of 2-pixel width around each mask as background. In the Voronoi label, we replace the points with cor- responding masks and mark the other pixels in those Voronoi cells as background, as shown in Fig. 1. These newly introduced background pixels may be not always correct, but they are beneficial to the training combined with the masks.\n\n\nTraining with revised labels\n\nThe revised labels combining the information of mixed points and masks annotation are used to train a regular CNN (see Fig. 1(b)). The loss function is the same as step 1 in Eqn. 3. We don't train the model with the dense CRF loss as in [10] because the selected masks already contain important shape information, and it will take an effort to find suitable parameters for the CRF loss. The same metrics are used as in [10]. The pixel accuracy and F1-score are used to evaluate the pixel-level performance, and the object-level Dice score and aggregated Jaccard index (AJI) [1] are used to assess the instance-level accuracy.\n\n\nEXPERIMENTS\n\n\nDatasets and evaluation metrics\n\n\nImplementation details\n\nColor normalization is performed on all images. The image sizes in both datasets are too large, thus we extract 16 patches of size 250\u00d7250 uniformly from each training image with overlap, and randomly crop a 224 \u00d7 224 patch as input.\n\nOther data augmentations include random scale, horizontal flip, affine transformation, rotation, normalization with mean and standard deviation by channel. Adam optimizer is used to train both Bayesian and regular CNNs. The learning rate and batch size are 1e-4 and 8, respectively. In the uncertainty prediction step, the model is trained 100 epochs. The number of Monte Carlo sampling is set to T = 20. In the training with mixed labels step, the model is trained 150 epochs.\n\n\nResults and discussion\n\nThe quantitative and typical qualitative results on the Lung Cancer and Multi-Organ datasets are shown in Table 1, Table 2 and Fig. 3. We compare our method with the points only method [10], the state-of-the-art methods on each dataset [2,3,4], and the same network trained with full mask labels.\n\nCompared with the baseline method using points only annotation [10] (Points, Points w/ CRF in Table 1 and 2), our method using mixed annotation (Mixed (un) in the tables) achieves much better performance. The gain attributes to two aspects. One is that the masks introduce extra shape information about some nuclei. That's why the results using the masks of randomly selected nuclei (Mixed (rand)) also outperform the baseline. The other aspect is the uncertainty helps to pick the representative nuclei, resulting in better accuracy than random selection, especially on nuclei with non-uniform colors (see Fig. 3). Besides, our method doesn't rely on the CRF loss because the shape information of newly added nuclei can help training. It is more convenient compared to searching suitable parameters in the CRF loss.\n\nOur method narrows the gap between weakly supervised training and the fully supervised training (Full masks in both tables) using the same network and full masks. On the Lung Cancer dataset, the differences on pixel accuracy, F1, Dice and AJI are 1.2%, 4.6%, 3.2%, 2.8%, respectively. On the Multi-Organ dataset, the fully supervised model has low Dice and AJI because it cannot separate the crowded nuclei well.\n\nCompared to the state-of-the-art fully-supervised methods [2,3,4], our method obtains close overall performance, but still needs to be improved with regard to the object-level accuracy. The introduction of 5% masks is not enough to cover the shape information of all nuclei, leading to suboptimal solutions in extremely hard cases. For example, in the cases of Fig. 3, methods using weak supervision cannot accurately capture the shape of those nuclei without seeing sufficient masks of similar nuclei in the training set. It remains open on how to handle this issue effectively in the weakly supervised setting. (c) Points [10] (d) Mixed (rand) (e) Mixed (un) Fig. 3. Typical results in the LC dataset (top row) and MO dataset (bottom row). Distinct colors represent different nuclei. Mixed (rand) and Mixed (un) are the mixed annotation using masks selected randomly and from uncertainty, respectively.\n\n\nAnnotation time\n\nOn the Lung Cancer dataset, the average annotation time for full masks and points is 115 and 14 minutes [10]. Adding 5% masks introduces extra 6 minutes based on points annotation, which is not a big deal compared to full masks.\n\n\nCONCLUSION\n\nIn this paper, we propose a weakly supervised nuclei segmentation method using mixed points and masks annotation. Uncertainty map is estimated from a Bayesian CNN trained with points annotation. Then we choose 5% representative hard nuclei according to the uncertainty map to annotate the full masks. The masks are combined with the two types of labels derived from points annotation to supervise the training of a segmentation model. Our method reduces the gap between segmentation in the weakly supervised setting and fully supervised setting while requiring a little extra annotation effort.\n\nFig. 1 .\n1Overview of the proposed framework. (a) Uncertainty prediction. The uncertainty map is predicted from the Bayesian CNN trained with two types of proxy labels as in[10]. (b) CNN training using mixed points and selected masks from uncertainty. The annotated masks are combined with the Voronoi and Cluster labels to generate the revised labels for training.\n\nFig. 2 .\n2Uncertainty map and masks of selected nuclei.\n\nTable 1 .\n1Results on Lung Cancer dataset.Method \nPixel-level \nObject-level \n\nAcc \nF1 \nDice obj \nAJI \n\nQu et al. [3] \n-\n0.8860 0.8760 \n-\nFull masks \n0.9615 0.8771 0.8521 0.6979 \n\nPoints [10] \n0.9413 0.8028 0.7885 0.6328 \nPoints w/ CRF [10] 0.9433 0.8120 0.8002 0.6503 \nMixed (rand) \n0.9485 0.8283 0.8164 0.6645 \nMixed (un) \n0.9501 0.8366 0.8249 0.6783 \n\nTable 2. Results on Multi-Organ dataset. \n\nMethod \nPixel-level \nObject-level \n\nAcc \nF1 \nDice obj \nAJI \n\nDIST [2] \n-\n0.7623 \n-\n0.5598 \nCIA-Net [4] \n-\n-\n-\n0.6205 \nFull masks \n0.9194 0.8100 0.6763 0.3919 \n\nPoints [10] \n0.9052 0.7745 0.7231 0.5045 \nPoints w/ CRF [10] 0.9071 0.7776 0.7270 0.5097 \nMixed (rand) \n0.9111 0.7753 0.7280 0.5106 \nMixed (un) \n0.9114 0.7748 0.7323 0.5297 \n\n\nAcknowledgement. This work is funded by NSF 1763523, 1747778, 1733843, 1703883.\nA dataset and a technique for generalized nuclear segmentation for computational pathology. N Kumar, R Verma, S Sharma, S Bhargava, A Vahadane, A Sethi, IEEE Transactions on Medical imaging. 367N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Va- hadane, and A. Sethi, \"A dataset and a technique for generalized nuclear segmentation for computational pathology,\" IEEE Transactions on Medical imaging, vol. 36, no. 7, pp. 1550-1560, 2017.\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. P Naylor, M La\u00e9, F Reyal, T Walter, IEEE Transactions on Medical Imaging. 382P. Naylor, M. La\u00e9, F. Reyal, and T. Walter, \"Segmen- tation of nuclei in histopathology images by deep re- gression of the distance map,\" IEEE Transactions on Medical Imaging, vol. 38, no. 2, pp. 448-459, 2018.\n\nJoint segmentation and fine-grained classification of nuclei in histopathology images. H Qu, G M Riedlinger, P Wu, Q Huang, J Yi, S De, D Metaxas, ISBI. H. Qu, G.M. Riedlinger, P. Wu, Q. Huang, J. Yi, S. De, and D. Metaxas, \"Joint segmentation and fine-grained classification of nuclei in histopathology images,\" in ISBI, 2019, pp. 900-904.\n\nCia-net: Robust nuclei instance segmentation with contour-aware information aggregation. Y Zhou, O F Onder, Q Dou, E Tsougenis, H Chen, P Heng, IPMI. Y. Zhou, O. F. Onder, Q. Dou, E. Tsougenis, H. Chen, and P. Heng, \"Cia-net: Robust nuclei instance segmen- tation with contour-aware information aggregation,\" in IPMI, 2019, pp. 682-693.\n\nImproving nuclei/gland instance segmentation in histopathology images by full resolution neural network and spatial constrained loss. H Qu, Z Yan, G M Riedlinger, S De, D Metaxas, MICCAI. SpringerH. Qu, Z. Yan, G. M. Riedlinger, S. De, and D. Metaxas, \"Improving nuclei/gland instance segmen- tation in histopathology images by full resolution neu- ral network and spatial constrained loss,\" in MICCAI. Springer, 2019, pp. 378-386.\n\nWeakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation. G Papandreou, L Chen, K P Murphy, A L Yuille, CVPR. G. Papandreou, L. Chen, K. P. Murphy, and A. L. Yuille, \"Weakly-and semi-supervised learning of a deep convo- lutional network for semantic image segmentation,\" in CVPR, 2015, pp. 1742-1750.\n\nScribblesup: Scribble-supervised convolutional networks for semantic segmentation. D Lin, J Dai, J Jia, K He, J Sun, CVPR. D. Lin, J. Dai, J. Jia, K. He, and J. Sun, \"Scribblesup: Scribble-supervised convolutional networks for seman- tic segmentation,\" in CVPR, 2016, pp. 3159-3167.\n\nWhat's the point: Semantic segmentation with point supervision. A Bearman, O Russakovsky, V Ferrari, F Li, ECCV. A. Bearman, O. Russakovsky, V. Ferrari, and F. Li, \"What's the point: Semantic segmentation with point supervision,\" in ECCV, 2016, pp. 549-565.\n\nDeep learning based instance segmentation in 3d biomedical images using weak annotation. Z Zhao, L Yang, Ha Zheng, I H Guldner, S Zhang, D Z Chen, in MICCAI. Z. Zhao, L. Yang, Ha. Zheng, I. H. Guldner, S. Zhang, and D. Z. Chen, \"Deep learning based instance segmen- tation in 3d biomedical images using weak annotation,\" in MICCAI, 2018, pp. 352-360.\n\nWeakly supervised deep nuclei segmentation using points annotation in histopathology images. H Qu, P Wu, Q Huang, J Yi, G M Riedlinger, S De, D Metaxas, MIDLH. Qu, P. Wu, Q. Huang, J. Yi, G. M. Riedlinger, S. De, and D. Metaxas, \"Weakly supervised deep nuclei seg- mentation using points annotation in histopathology im- ages,\" in MIDL, 2019, pp. 390-400.\n\nWhat uncertainties do we need in bayesian deep learning for computer vision?. A Kendall, Y Gal, Advances in neural information processing systems. A. Kendall and Y. Gal, \"What uncertainties do we need in bayesian deep learning for computer vision?,\" in Ad- vances in neural information processing systems, 2017, pp. 5574-5584.\n", "annotations": {"author": "[{\"start\":\"78\",\"end\":\"137\"},{\"start\":\"138\",\"end\":\"200\"},{\"start\":\"201\",\"end\":\"268\"},{\"start\":\"269\",\"end\":\"334\"},{\"start\":\"335\",\"end\":\"404\"}]", "publisher": null, "author_last_name": "[{\"start\":\"82\",\"end\":\"84\"},{\"start\":\"145\",\"end\":\"147\"},{\"start\":\"210\",\"end\":\"215\"},{\"start\":\"279\",\"end\":\"281\"},{\"start\":\"344\",\"end\":\"351\"}]", "author_first_name": "[{\"start\":\"78\",\"end\":\"81\"},{\"start\":\"138\",\"end\":\"144\"},{\"start\":\"201\",\"end\":\"209\"},{\"start\":\"269\",\"end\":\"278\"},{\"start\":\"335\",\"end\":\"343\"}]", "author_affiliation": "[{\"start\":\"86\",\"end\":\"136\"},{\"start\":\"149\",\"end\":\"199\"},{\"start\":\"217\",\"end\":\"267\"},{\"start\":\"283\",\"end\":\"333\"},{\"start\":\"353\",\"end\":\"403\"}]", "title": "[{\"start\":\"1\",\"end\":\"75\"},{\"start\":\"405\",\"end\":\"479\"}]", "venue": null, "abstract": "[{\"start\":\"562\",\"end\":\"1564\"}]", "bib_ref": "[{\"start\":\"1998\",\"end\":\"2001\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2001\",\"end\":\"2003\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"2003\",\"end\":\"2005\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"2005\",\"end\":\"2007\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"2007\",\"end\":\"2009\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2493\",\"end\":\"2496\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"2604\",\"end\":\"2607\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"2815\",\"end\":\"2818\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"2967\",\"end\":\"2970\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"3267\",\"end\":\"3271\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"3884\",\"end\":\"3888\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"4114\",\"end\":\"4118\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4611\",\"end\":\"4615\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"4813\",\"end\":\"4817\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"4997\",\"end\":\"5001\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"5008\",\"end\":\"5012\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"6121\",\"end\":\"6125\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"8268\",\"end\":\"8272\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"8450\",\"end\":\"8454\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"8605\",\"end\":\"8608\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"9655\",\"end\":\"9659\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"9706\",\"end\":\"9709\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"9709\",\"end\":\"9711\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"9711\",\"end\":\"9713\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"9831\",\"end\":\"9835\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"11058\",\"end\":\"11061\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"11061\",\"end\":\"11063\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"11063\",\"end\":\"11065\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"11624\",\"end\":\"11628\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"12028\",\"end\":\"12032\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"12936\",\"end\":\"12940\",\"attributes\":{\"ref_id\":\"b9\"}}]", "figure": "[{\"start\":\"12762\",\"end\":\"13128\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"13129\",\"end\":\"13185\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"13186\",\"end\":\"13919\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1580\",\"end\":\"2285\"},{\"start\":\"2287\",\"end\":\"3525\"},{\"start\":\"3527\",\"end\":\"4294\"},{\"start\":\"4305\",\"end\":\"4494\"},{\"start\":\"4521\",\"end\":\"5156\"},{\"start\":\"5194\",\"end\":\"5754\"},{\"start\":\"5798\",\"end\":\"5888\"},{\"start\":\"5918\",\"end\":\"6327\"},{\"start\":\"6329\",\"end\":\"6514\"},{\"start\":\"6703\",\"end\":\"7372\"},{\"start\":\"7403\",\"end\":\"7998\"},{\"start\":\"8031\",\"end\":\"8656\"},{\"start\":\"8731\",\"end\":\"8964\"},{\"start\":\"8966\",\"end\":\"9443\"},{\"start\":\"9470\",\"end\":\"9766\"},{\"start\":\"9768\",\"end\":\"10584\"},{\"start\":\"10586\",\"end\":\"10998\"},{\"start\":\"11000\",\"end\":\"11904\"},{\"start\":\"11924\",\"end\":\"12152\"},{\"start\":\"12167\",\"end\":\"12761\"}]", "formula": "[{\"start\":\"5157\",\"end\":\"5193\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"5755\",\"end\":\"5797\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"5889\",\"end\":\"5917\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"6515\",\"end\":\"6615\",\"attributes\":{\"id\":\"formula_3\"}}]", "table_ref": "[{\"start\":\"9576\",\"end\":\"9583\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"9862\",\"end\":\"9869\",\"attributes\":{\"ref_id\":\"tab_1\"}}]", "section_header": "[{\"start\":\"1566\",\"end\":\"1578\",\"attributes\":{\"n\":\"1.\"}},{\"start\":\"4297\",\"end\":\"4303\",\"attributes\":{\"n\":\"2.\"}},{\"start\":\"4497\",\"end\":\"4519\",\"attributes\":{\"n\":\"2.1.\"}},{\"start\":\"6617\",\"end\":\"6701\",\"attributes\":{\"n\":\"2.2.\"}},{\"start\":\"7375\",\"end\":\"7401\",\"attributes\":{\"n\":\"2.2.2.\"}},{\"start\":\"8001\",\"end\":\"8029\",\"attributes\":{\"n\":\"2.2.3.\"}},{\"start\":\"8659\",\"end\":\"8670\",\"attributes\":{\"n\":\"3.\"}},{\"start\":\"8673\",\"end\":\"8704\",\"attributes\":{\"n\":\"3.1.\"}},{\"start\":\"8707\",\"end\":\"8729\",\"attributes\":{\"n\":\"3.2.\"}},{\"start\":\"9446\",\"end\":\"9468\",\"attributes\":{\"n\":\"3.3.\"}},{\"start\":\"11907\",\"end\":\"11922\",\"attributes\":{\"n\":\"3.4.\"}},{\"start\":\"12155\",\"end\":\"12165\",\"attributes\":{\"n\":\"4.\"}},{\"start\":\"12763\",\"end\":\"12771\"},{\"start\":\"13130\",\"end\":\"13138\"},{\"start\":\"13187\",\"end\":\"13196\"}]", "table": "[{\"start\":\"13229\",\"end\":\"13919\"}]", "figure_caption": "[{\"start\":\"12773\",\"end\":\"13128\"},{\"start\":\"13140\",\"end\":\"13185\"},{\"start\":\"13198\",\"end\":\"13229\"}]", "figure_ref": "[{\"start\":\"4330\",\"end\":\"4337\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"4954\",\"end\":\"4963\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"6175\",\"end\":\"6181\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"6743\",\"end\":\"6749\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"7160\",\"end\":\"7169\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"7361\",\"end\":\"7370\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"7858\",\"end\":\"7864\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"8150\",\"end\":\"8159\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"9597\",\"end\":\"9603\"},{\"start\":\"10375\",\"end\":\"10381\"},{\"start\":\"11361\",\"end\":\"11367\"},{\"start\":\"11661\",\"end\":\"11667\"}]", "bib_author_first_name": "[{\"start\":\"14092\",\"end\":\"14093\"},{\"start\":\"14101\",\"end\":\"14102\"},{\"start\":\"14110\",\"end\":\"14111\"},{\"start\":\"14120\",\"end\":\"14121\"},{\"start\":\"14132\",\"end\":\"14133\"},{\"start\":\"14144\",\"end\":\"14145\"},{\"start\":\"14525\",\"end\":\"14526\"},{\"start\":\"14535\",\"end\":\"14536\"},{\"start\":\"14542\",\"end\":\"14543\"},{\"start\":\"14551\",\"end\":\"14552\"},{\"start\":\"14901\",\"end\":\"14902\"},{\"start\":\"14907\",\"end\":\"14908\"},{\"start\":\"14909\",\"end\":\"14910\"},{\"start\":\"14923\",\"end\":\"14924\"},{\"start\":\"14929\",\"end\":\"14930\"},{\"start\":\"14938\",\"end\":\"14939\"},{\"start\":\"14944\",\"end\":\"14945\"},{\"start\":\"14950\",\"end\":\"14951\"},{\"start\":\"15245\",\"end\":\"15246\"},{\"start\":\"15253\",\"end\":\"15254\"},{\"start\":\"15255\",\"end\":\"15256\"},{\"start\":\"15264\",\"end\":\"15265\"},{\"start\":\"15271\",\"end\":\"15272\"},{\"start\":\"15284\",\"end\":\"15285\"},{\"start\":\"15292\",\"end\":\"15293\"},{\"start\":\"15628\",\"end\":\"15629\"},{\"start\":\"15634\",\"end\":\"15635\"},{\"start\":\"15641\",\"end\":\"15642\"},{\"start\":\"15643\",\"end\":\"15644\"},{\"start\":\"15657\",\"end\":\"15658\"},{\"start\":\"15663\",\"end\":\"15664\"},{\"start\":\"16028\",\"end\":\"16029\"},{\"start\":\"16042\",\"end\":\"16043\"},{\"start\":\"16050\",\"end\":\"16051\"},{\"start\":\"16052\",\"end\":\"16053\"},{\"start\":\"16062\",\"end\":\"16063\"},{\"start\":\"16064\",\"end\":\"16065\"},{\"start\":\"16355\",\"end\":\"16356\"},{\"start\":\"16362\",\"end\":\"16363\"},{\"start\":\"16369\",\"end\":\"16370\"},{\"start\":\"16376\",\"end\":\"16377\"},{\"start\":\"16382\",\"end\":\"16383\"},{\"start\":\"16620\",\"end\":\"16621\"},{\"start\":\"16631\",\"end\":\"16632\"},{\"start\":\"16646\",\"end\":\"16647\"},{\"start\":\"16657\",\"end\":\"16658\"},{\"start\":\"16904\",\"end\":\"16905\"},{\"start\":\"16912\",\"end\":\"16913\"},{\"start\":\"16920\",\"end\":\"16922\"},{\"start\":\"16930\",\"end\":\"16931\"},{\"start\":\"16932\",\"end\":\"16933\"},{\"start\":\"16943\",\"end\":\"16944\"},{\"start\":\"16952\",\"end\":\"16953\"},{\"start\":\"16954\",\"end\":\"16955\"},{\"start\":\"17260\",\"end\":\"17261\"},{\"start\":\"17266\",\"end\":\"17267\"},{\"start\":\"17272\",\"end\":\"17273\"},{\"start\":\"17281\",\"end\":\"17282\"},{\"start\":\"17287\",\"end\":\"17288\"},{\"start\":\"17289\",\"end\":\"17290\"},{\"start\":\"17303\",\"end\":\"17304\"},{\"start\":\"17309\",\"end\":\"17310\"},{\"start\":\"17602\",\"end\":\"17603\"},{\"start\":\"17613\",\"end\":\"17614\"}]", "bib_author_last_name": "[{\"start\":\"14094\",\"end\":\"14099\"},{\"start\":\"14103\",\"end\":\"14108\"},{\"start\":\"14112\",\"end\":\"14118\"},{\"start\":\"14122\",\"end\":\"14130\"},{\"start\":\"14134\",\"end\":\"14142\"},{\"start\":\"14146\",\"end\":\"14151\"},{\"start\":\"14527\",\"end\":\"14533\"},{\"start\":\"14537\",\"end\":\"14540\"},{\"start\":\"14544\",\"end\":\"14549\"},{\"start\":\"14553\",\"end\":\"14559\"},{\"start\":\"14903\",\"end\":\"14905\"},{\"start\":\"14911\",\"end\":\"14921\"},{\"start\":\"14925\",\"end\":\"14927\"},{\"start\":\"14931\",\"end\":\"14936\"},{\"start\":\"14940\",\"end\":\"14942\"},{\"start\":\"14946\",\"end\":\"14948\"},{\"start\":\"14952\",\"end\":\"14959\"},{\"start\":\"15247\",\"end\":\"15251\"},{\"start\":\"15257\",\"end\":\"15262\"},{\"start\":\"15266\",\"end\":\"15269\"},{\"start\":\"15273\",\"end\":\"15282\"},{\"start\":\"15286\",\"end\":\"15290\"},{\"start\":\"15294\",\"end\":\"15298\"},{\"start\":\"15630\",\"end\":\"15632\"},{\"start\":\"15636\",\"end\":\"15639\"},{\"start\":\"15645\",\"end\":\"15655\"},{\"start\":\"15659\",\"end\":\"15661\"},{\"start\":\"15665\",\"end\":\"15672\"},{\"start\":\"16030\",\"end\":\"16040\"},{\"start\":\"16044\",\"end\":\"16048\"},{\"start\":\"16054\",\"end\":\"16060\"},{\"start\":\"16066\",\"end\":\"16072\"},{\"start\":\"16357\",\"end\":\"16360\"},{\"start\":\"16364\",\"end\":\"16367\"},{\"start\":\"16371\",\"end\":\"16374\"},{\"start\":\"16378\",\"end\":\"16380\"},{\"start\":\"16384\",\"end\":\"16387\"},{\"start\":\"16622\",\"end\":\"16629\"},{\"start\":\"16633\",\"end\":\"16644\"},{\"start\":\"16648\",\"end\":\"16655\"},{\"start\":\"16659\",\"end\":\"16661\"},{\"start\":\"16906\",\"end\":\"16910\"},{\"start\":\"16914\",\"end\":\"16918\"},{\"start\":\"16923\",\"end\":\"16928\"},{\"start\":\"16934\",\"end\":\"16941\"},{\"start\":\"16945\",\"end\":\"16950\"},{\"start\":\"16956\",\"end\":\"16960\"},{\"start\":\"17262\",\"end\":\"17264\"},{\"start\":\"17268\",\"end\":\"17270\"},{\"start\":\"17274\",\"end\":\"17279\"},{\"start\":\"17283\",\"end\":\"17285\"},{\"start\":\"17291\",\"end\":\"17301\"},{\"start\":\"17305\",\"end\":\"17307\"},{\"start\":\"17311\",\"end\":\"17318\"},{\"start\":\"17604\",\"end\":\"17611\"},{\"start\":\"17615\",\"end\":\"17618\"}]", "bib_entry": "[{\"start\":\"14000\",\"end\":\"14435\",\"attributes\":{\"matched_paper_id\":\"5162860\",\"id\":\"b0\"}},{\"start\":\"14437\",\"end\":\"14812\",\"attributes\":{\"matched_paper_id\":\"59601271\",\"id\":\"b1\"}},{\"start\":\"14814\",\"end\":\"15154\",\"attributes\":{\"matched_paper_id\":\"85464685\",\"id\":\"b2\"}},{\"start\":\"15156\",\"end\":\"15492\",\"attributes\":{\"matched_paper_id\":\"76666202\",\"id\":\"b3\"}},{\"start\":\"15494\",\"end\":\"15925\",\"attributes\":{\"matched_paper_id\":\"204027903\",\"id\":\"b4\"}},{\"start\":\"15927\",\"end\":\"16270\",\"attributes\":{\"matched_paper_id\":\"8420864\",\"id\":\"b5\"}},{\"start\":\"16272\",\"end\":\"16554\",\"attributes\":{\"matched_paper_id\":\"3121011\",\"id\":\"b6\"}},{\"start\":\"16556\",\"end\":\"16813\",\"attributes\":{\"matched_paper_id\":\"1356654\",\"id\":\"b7\"}},{\"start\":\"16815\",\"end\":\"17165\",\"attributes\":{\"matched_paper_id\":\"49558518\",\"id\":\"b8\"}},{\"start\":\"17167\",\"end\":\"17522\",\"attributes\":{\"id\":\"b9\"}},{\"start\":\"17524\",\"end\":\"17850\",\"attributes\":{\"matched_paper_id\":\"71134\",\"id\":\"b10\"}}]", "bib_title": "[{\"start\":\"14000\",\"end\":\"14090\"},{\"start\":\"14437\",\"end\":\"14523\"},{\"start\":\"14814\",\"end\":\"14899\"},{\"start\":\"15156\",\"end\":\"15243\"},{\"start\":\"15494\",\"end\":\"15626\"},{\"start\":\"15927\",\"end\":\"16026\"},{\"start\":\"16272\",\"end\":\"16353\"},{\"start\":\"16556\",\"end\":\"16618\"},{\"start\":\"16815\",\"end\":\"16902\"},{\"start\":\"17524\",\"end\":\"17600\"}]", "bib_author": "[{\"start\":\"14092\",\"end\":\"14101\"},{\"start\":\"14101\",\"end\":\"14110\"},{\"start\":\"14110\",\"end\":\"14120\"},{\"start\":\"14120\",\"end\":\"14132\"},{\"start\":\"14132\",\"end\":\"14144\"},{\"start\":\"14144\",\"end\":\"14153\"},{\"start\":\"14525\",\"end\":\"14535\"},{\"start\":\"14535\",\"end\":\"14542\"},{\"start\":\"14542\",\"end\":\"14551\"},{\"start\":\"14551\",\"end\":\"14561\"},{\"start\":\"14901\",\"end\":\"14907\"},{\"start\":\"14907\",\"end\":\"14923\"},{\"start\":\"14923\",\"end\":\"14929\"},{\"start\":\"14929\",\"end\":\"14938\"},{\"start\":\"14938\",\"end\":\"14944\"},{\"start\":\"14944\",\"end\":\"14950\"},{\"start\":\"14950\",\"end\":\"14961\"},{\"start\":\"15245\",\"end\":\"15253\"},{\"start\":\"15253\",\"end\":\"15264\"},{\"start\":\"15264\",\"end\":\"15271\"},{\"start\":\"15271\",\"end\":\"15284\"},{\"start\":\"15284\",\"end\":\"15292\"},{\"start\":\"15292\",\"end\":\"15300\"},{\"start\":\"15628\",\"end\":\"15634\"},{\"start\":\"15634\",\"end\":\"15641\"},{\"start\":\"15641\",\"end\":\"15657\"},{\"start\":\"15657\",\"end\":\"15663\"},{\"start\":\"15663\",\"end\":\"15674\"},{\"start\":\"16028\",\"end\":\"16042\"},{\"start\":\"16042\",\"end\":\"16050\"},{\"start\":\"16050\",\"end\":\"16062\"},{\"start\":\"16062\",\"end\":\"16074\"},{\"start\":\"16355\",\"end\":\"16362\"},{\"start\":\"16362\",\"end\":\"16369\"},{\"start\":\"16369\",\"end\":\"16376\"},{\"start\":\"16376\",\"end\":\"16382\"},{\"start\":\"16382\",\"end\":\"16389\"},{\"start\":\"16620\",\"end\":\"16631\"},{\"start\":\"16631\",\"end\":\"16646\"},{\"start\":\"16646\",\"end\":\"16657\"},{\"start\":\"16657\",\"end\":\"16663\"},{\"start\":\"16904\",\"end\":\"16912\"},{\"start\":\"16912\",\"end\":\"16920\"},{\"start\":\"16920\",\"end\":\"16930\"},{\"start\":\"16930\",\"end\":\"16943\"},{\"start\":\"16943\",\"end\":\"16952\"},{\"start\":\"16952\",\"end\":\"16962\"},{\"start\":\"17260\",\"end\":\"17266\"},{\"start\":\"17266\",\"end\":\"17272\"},{\"start\":\"17272\",\"end\":\"17281\"},{\"start\":\"17281\",\"end\":\"17287\"},{\"start\":\"17287\",\"end\":\"17303\"},{\"start\":\"17303\",\"end\":\"17309\"},{\"start\":\"17309\",\"end\":\"17320\"},{\"start\":\"17602\",\"end\":\"17613\"},{\"start\":\"17613\",\"end\":\"17620\"}]", "bib_venue": "[{\"start\":\"14153\",\"end\":\"14189\"},{\"start\":\"14561\",\"end\":\"14597\"},{\"start\":\"14961\",\"end\":\"14965\"},{\"start\":\"15300\",\"end\":\"15304\"},{\"start\":\"15674\",\"end\":\"15680\"},{\"start\":\"16074\",\"end\":\"16078\"},{\"start\":\"16389\",\"end\":\"16393\"},{\"start\":\"16663\",\"end\":\"16667\"},{\"start\":\"16962\",\"end\":\"16971\"},{\"start\":\"17167\",\"end\":\"17258\"},{\"start\":\"17620\",\"end\":\"17669\"}]"}}}, "year": 2023, "month": 12, "day": 17}