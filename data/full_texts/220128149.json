{"id": 220128149, "updated": "2023-10-06 14:19:08.565", "metadata": {"title": "Deep Partition Aggregation: Provable Defense against General Poisoning Attacks", "authors": "[{\"first\":\"Alexander\",\"last\":\"Levine\",\"middle\":[]},{\"first\":\"Soheil\",\"last\":\"Feizi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 26}, "abstract": "Adversarial poisoning attacks distort training data in order to corrupt the test-time behavior of a classifier. A provable defense provides a certificate for each test sample, which is a lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification. We propose two provable defenses against poisoning attacks: (i) Deep Partition Aggregation (DPA), a certified defense against a general poisoning threat model, defined as the insertion or deletion of a bounded number of samples to the training set -- by implication, this threat model also includes arbitrary distortions to a bounded number of images and/or labels; and (ii) Semi-Supervised DPA (SS-DPA), a certified defense against label-flipping poisoning attacks. DPA is an ensemble method where base models are trained on partitions of the training set determined by a hash function. DPA is related to subset aggregation, a well-studied ensemble method in classical machine learning. DPA can also be viewed as an extension of randomized ablation (Levine&Feizi, 2020a), a certified defense against sparse evasion attacks, to the poisoning domain. Our label-flipping defense, SS-DPA, uses a semi-supervised learning algorithm as its base classifier model: we train each base classifier using the entire unlabeled training set in addition to the labels for a partition. SS-DPA outperforms the existing certified defense for label-flipping attacks (Rosenfeld et al., 2020). SS-DPA certifies>= 50% of test images against 675 label flips (vs.<200 label flips with the existing defense) on MNIST and 83 label flips on CIFAR-10. Against general poisoning attacks (no prior certified defense), DPA certifies>= 50% of test images against>500 poison image insertions on MNIST, and nine insertions on CIFAR-10. These results establish new state-of-the-art provable defenses against poison attacks.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2006.14768", "mag": "3037299416", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/0001F21", "doi": null}}, "content": {"source": {"pdf_hash": "abef3821cdab3819ce998f0abcbb10cc8acdc5c1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.14768v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3666ebcf9c3ece154a5397ca14b073eeca92d6b9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/abef3821cdab3819ce998f0abcbb10cc8acdc5c1.txt", "contents": "\nDeep Partition Aggregation: Provable Defense against General Poisoning Attacks\n\n\nAlexander Levine alevine0@cs.umd.edu \nUniversity of Maryland College Park\nUniversity of Maryland College Park\n20740, 20740MD, MD\n\nSoheil Feizi sfeizi@cs.umd.edu \nUniversity of Maryland College Park\nUniversity of Maryland College Park\n20740, 20740MD, MD\n\nDeep Partition Aggregation: Provable Defense against General Poisoning Attacks\n\nAdversarial poisoning attacks distort training data in order to corrupt the test-time behavior of a classifier. A provable defense provides a certificate for each test sample, which is a lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification. We propose two novel provable defenses against poisoning attacks: (i) Deep Partition Aggregation (DPA), a certified defense against a general poisoning threat model, defined as the insertion or deletion of a bounded number of samples to the training set -by implication, this threat model also includes arbitrary distortions to a bounded number of images and/or labels; and (ii) Semi-Supervised DPA (SS-DPA), a certified defense against label-flipping poisoning attacks. DPA is an ensemble method where base models are trained on partitions of the training set determined by a hash function. DPA is related to subset aggregation, a well-studied ensemble method in classical machine learning. DPA can also be regarded as an extension of randomized ablation (Levine and Feizi, 2020a), a smoothing-based certified defense against sparse evasion attacks, to the poisoning domain. Our defense against label-flipping poison attacks, SS-DPA, uses a semi-supervised learning algorithm as its base classifier model: each base classifier is trained using the entire unlabeled training set in addition to the labels for a partition. SS-DPA outperforms the existing certified defense for label-flipping attacks(Rosenfeld et al., 2020). SS-DPA can certify \u2265 50% of test images against 675 label flips (vs. < 200 label flips with the existing defense) on MNIST and 83 label flips on CIFAR-10. Against general poisoning attacks (no prior certified defenses), DPA can certify \u2265 50% of test images against over 500 poison image insertions on MNIST, and nine insertions on CIFAR-10. These results establish new state-of-the-art provable defenses against poison attacks.Preprint. Under review.\n\nIntroduction\n\nAdversarial poisoning attacks are an important vulnerability in machine learning systems. In these attacks, an adversary can manipulate the training data of a classifier, in order to change the classifications of specific inputs at test time. Several poisoning threat models have been studied in the literature, including threat models where the adversary may insert new poison samples (Chen et al., 2017), manipulate the training labels (Xiao et al., 2012;Rosenfeld et al., 2020), or manipulate the training sample values (Biggio et al., 2012;Shafahi et al., 2018). A certified defense against a poisoning attack provides a certificate for each test sample, which is a guaranteed lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification. In this work, we propose certified defenses against two types of poisoning attacks:\n\n\u2022 General poisoning attacks: In this threat model, the attacker can insert or remove a bounded number of samples from the training set. In particular, the attack magnitude \u03c1 is defined as the cardinality of the symmetric difference between the clean and poisoned training sets. This threat model also includes any distortion to an image and/or label in the training set -a distortion of a training image is simply the removal of the original image followed by the insertion of the distorted image. (Note that an image distortion or label flip therefore increases the symmetric difference attack magnitude by two.) \u2022 Label-flipping poisoning attacks: In this threat model, the adversary changes only the label for \u03c1 out of m training samples. Rosenfeld et al. (2020) has recently provided a certified defense for this threat model, which we improve upon.\n\nIn the last couple of years, certified defenses have been extensively studied for evasion attacks, where the adversary manipulates the test images, rather than the training data (e.g. Wong and Kolter (2018); Gowal et al. (2018); Lecuyer et al. (2019); ; Salman et al. (2019); Feizi (2020a, 2019); Cohen et al. (2019), etc.) In the evasion case, a certificate is a lower bound on the distance from the image to the classifier's decision boundary: this guarantees that the image's classification remains unchanged under adversarial distortions up to the certified magnitude. Rosenfeld et al. (2020) provides an analogous certificate for label-flipping poisoning attacks: for an input image x, the certificate of x is a lower bound on the number of labels in the training set that would have to change in order to change the classification of x. 1 Rosenfeld et al. (2020)'s method is an adaptation of a certified defense for sparse (L 0 ) evasion attacks proposed by Lee et al. (2019). In Lee et al. (2019)'s randomized smoothing technique, the final classification is the consensus of many classifications by a base classifier on noisy copies of the input image x: in each copy of x, each pixel is corrupted to a random value with fixed probability. This results in probabilistic certificates, with a failure rate that decreases as the number of noisy copies used in the classification increases.\n\nThe adapted method for label-flipping attacks proposed by Rosenfeld et al. (2020) is equivalent to randomly flipping each training label with fixed probability and taking a consensus result. If implemented directly, this would require one to train a large ensemble of classifiers on different noisy versions of the training data. However, instead of actually doing this, Rosenfeld et al. (2020) focuses only on linear classifiers and is therefore able to analytically calculate the expected result. This gives deterministic, rather than probabilistic, certificates. Further, because Rosenfeld et al. (2020) considers a threat model where only labels are modified, they are able to train an unsupervised nonlinear feature extractor on the (unlabeled) training data before applying their technique, in order to learn more complex features. There is no need to worry about the robustness of the feature extractor training, because the unlabeled data cannot be corrupted by the adversary in their considered threat model.\n\nHowever, the technique proposed by Lee et al. (2019) is not the current state-of-the-art certifiable defense against L 0 evasion attacks on large-scale image datasets (i.e. ImageNet). Levine and Feizi (2020a) propose a randomized ablation technique which, rather than randomizing the values of some pixels in each noisy copy of x, instead ablates some pixels, replacing them with a null value. Since it is possible for the base classifier to distinguish exactly which pixels originate from x, this results in more accurate base classifications and therefore substantially greater certified robustness. For example, on ImageNet, Lee et al. (2019) certifies the median test image against distortions of one pixel, while Levine and Feizi (2020a) certifies against distortions of 16 pixels. The key question is whether or not one can adapt the evasion-time defense proposed by Levine and Feizi (2020a) to the poisoning setup. In this paper, we provide an affirmative answer to this question and show that the resulting method significantly outperforms the current state-of-the-art certifiable defenses. As shown in Figure 1-a, our method for certified robustness against label-flipping attacks substantially outperforms Rosenfeld et al. (2020) for multiclass classification tasks. Furthermore, while our method is de-randomized (as Rosenfeld et al. (2020) is) and therefore yields deterministic certificates, our technique does not require that the classification model be linear, allowing deep networks to be used. Moreover, in Figure 1-b, we illustrate our certified accuracy against \"general\" poisoning attacks on MNIST; Rosenfeld et al. (2020) does not provide a provable defense for this threat model.\n\nIn what follows, we explain our methods: we develop a certifiable defense against general poisoning attacks called Deep Partition Aggregation (DPA) -we first partition the training set into k partitions, with the partition assignment for a training sample determined by a hash function of the sample. (b) Certified accuracy against general poisoning attacks on MNIST using our DPA defense. The attack size is the number of samples which the adversary may add or remove from the training set. Rosenfeld et al. (2020) does not provide a provable defense for this more general threat model.\n\nThe hash function can be any deterministic function that maps a training sample t to a partition assignment: the only requirement is that the hash value depends only on the value of the training sample t itself, so that neither poisoning other samples, nor changing the total number of samples, nor reordering the samples can change the partition that t is assigned to. We then train k base classifiers separately, one on each partition. At the test time, we evaluate each of the base classifiers on the test image x and return the plurality classification c as the final result. The key insight is that removing a training sample, or adding a new sample, will only change the contents of one partition, and therefore will only affect the classification of one of the k base classifiers. Let n 1 be the number of base classifiers that output the consensus class c and n 2 be the number of base classifiers that output the next-most-frequently returned class c . Let \u2206 := n 1 \u2212 n 2 . To change the plurality classification from c to c , the adversary must change the output of at least \u2206/2 base classifiers: this means inserting or removing at least \u2206/2 training samples. This immediately gives us a robustness certificate.\n\nOur proposed method is related to classical ensemble approaches in machine learning, namely bootstrap aggregation and subset aggregation (Breiman, 1996;Buja and Stuetzle, 2006;B\u00fchlmann, 2003;Zaman and Hirose, 2009). However, in these methods each base classifier in the ensemble is trained on an independently sampled collection of points from the training set: multiple classifiers in the ensemble may be trained on (and therefore poisoned by) the same sample point. The purpose of these methods has typically been to improve generalization. Bootstrap aggregation has been proposed as an empirical defense against poisoning attacks (Biggio et al., 2011) as well as for evasion attacks (Smutz and Stavrou, 2016). However, to our knowledge, these techniques have not yet been used to provide certified robustness. Our unique partition aggregation variant provides deterministic robustness certificates against poisoning attacks. See Appendix D for further discussion.\n\nNote that DPA provides a robustness guarantee against any poisoning perturbation; it provides robustness to insertions and deletions, and by implication also both label-flips and image distortions. However, if the adversary is restricted to flipping labels only (as in Rosenfeld et al. (2020)), we can achieve even larger certificates through a modified technique. In this setting, the unlabeled data is trustworthy: each base classifier in the ensemble can then make use of the entire training set without labels, but only has access to the labels in its own partition. Therefore, each base classifier can be trained as if the entire dataset is available as unlabeled data, but only a very small number of labels are available. This is precisely the problem statement of semi-supervised learning, a well-studied domain of machine learning (Verma et al., 2019;Luo et al., 2018;Laine and Aila, 2017;Kingma et al., 2014;Gidaris et al., 2018). We can then leverage these existing semi-supervised learning techniques directly to improve the accuracies of the base classifiers in DPA. Furthermore, we can ensure that a particular image is assigned to the same partition regardless of label, so that only one partition is affected by a label flip (rather than possibly two). The resulting algorithm, Semi-Supervised Deep Partition Aggregation (SS-DPA) yields substantially increased certified accuracy against label-flipping attacks, compared to DPA alone and compared to the current state-of-the-art.\n\nOn MNIST, SS-DPA substantially outperforms the existing state of the art (Rosenfeld et al., 2020) in defending against label-flip attacks: we are able to certify at least half of images in the test set against attacks to over 600 (1.0%) of the labels in the training set, while still maintaining over 93% accuracy (See Figure 1-a, and Table 1). In comparison, Rosenfeld et al. (2020)'s method achieves less than 60% clean accuracy on MNIST, and most test images cannot be certified with the correct class against attacks of even 200 label flips. We are also the first work to our knowledge to certify against general poisoning attacks, including insertions and deletions of new training images: in this domain, we can certify at least half of test images against attacks consisting of over 500 arbitrary training image insertions or deletions. On CIFAR-10, a substantially more difficult classification task which Rosenfeld et al. (2020) does not test on, we can certify at least half of test images against label-flipping attacks on over 80 labels using SS-DPA, and can certify at least half of test images against general poisoning attacks of up to nine insertions or deletions using DPA. 2 Proposed Methods\n\n\nNotation\n\nLet S be the space of all possible unlabeled samples (i.e., the set of all possible images). We assume that it is possible to sort elements of S in a deterministic, unambiguous way. In particular, we can sort images lexicographically by pixel values. We represent labels as integers, so that the set of all possible labeled samples is S L := {(x, c)|x \u2208 S, c \u2208 N}. A training set for a classifier is then represented as T \u2208 P(S L ), where P(S L ) is the power set of S L . For t \u2208 S L , we let sample(t) \u2208 S refer to the (unlabeled) sample, and label(t) \u2208 N refer to the label. For a set of samples T \u2208 P(S L ), we let samples(T ) \u2208 P(S) refer to the set of unique unlabeled samples which occur in T . A classifier model is defined as a deterministic function from both the training set and the sample to be classified to a label, i.e. f : P(S L ) \u00d7 S \u2192 N. We will use f (\u00b7) to represent a base classifier model (i.e., a neural network), and g(\u00b7) to refer to a robust classifier (using DPA or SS-DPA).\n\nA B represents the set symmetric difference between A and B:\nA B = (A \\ B) \u222a (B \\ A). The number of elements in A is |A|, [n]\nis the set of integers 1 through n, and z is the largest integer less than or equal to z. 1 represents the indicator function: 1 Prop = 1 if Prop is true; 1 Prop = 0 otherwise. For a set A of sortable elements, we define Sort(A) as the sorted list of elements. For a list L of unique elements, for l \u2208 L, we will define index(L, l) as the index of l in the list L.\n\n\nDPA\n\nThe Deep Partition Aggregation (DPA) algorithm requires a base classifier model f : P(S L )\u00d7S \u2192 N, a training set T \u2208 P(S L ), a deterministic hash function h : S L \u2192 N, and a hyperparameter k \u2208 N indicating the number of base classifiers which will be used in the ensemble.\n\nAt the training time, the algorithm first uses the hash function h to define partitions P 1 , ..., P k \u2286 T of the training set, as follows:\nP i := {t \u2208 T | h(t) \u2261 i (mod k)}.\n(1) The hash function h can be any deterministic function from S L to N: however, it is preferable that the partitions are roughly equal in size. Therefore we should choose an h which maps images to a domain of integers significantly larger than k, in a way such that h(.) (mod k) will be roughly uniform over [k]. In practice, we let h(t) be the sum of the pixel values in the image t.\n\nBase classifiers are then trained on each partition: we define trained base classifiers f i : S \u2192 N as:\nf i (x) := f (P i , x).\n(2) Finally, at the inference time, we evaluate the input on each base classification, and then count the number of classifiers which return each class:\nn c (x) := |{i \u2208 [k]|f i (x) = c}|.\n(3) This lets us define the classifier which returns the consensus output of the ensemble:\ng dpa (T, x) := arg max c n c (x).(4)\nWhen taking the argmax, we break ties deterministically by returning the smaller class index. The resulting robust classifier has the following guarantee: Theorem 1. For a fixed deterministic base classifier f , hash function h, ensemble size k, training set T , and input x, let:\nc := g dpa (T, x) \u03c1(x) := n c \u2212 max c =c (n c (x) + 1 c <c ) 2 .(5)\nThen, for any poisoned training set U , if |T U | \u2264\u03c1(x), then g dpa (U, x) = c.\n\nAll proofs are presented in Appendix A. Note that T and U are unordered sets: therefore, in addition to providing certified robustness against insertions or deletions of training data, the robust classifier g dpa is also invariant under re-ordering of the training data, provided that f has this invariance (which is implied, because f maps deterministically from a set; see Section 2.2.1 for practical considerations). As mentioned in Section 1, DPA is a deterministic variant of randomized ablation (Levine and Feizi, 2020a) adapted to the poisoning domain. Each base classifier ablates most of the training set, retaining only the samples in one partition. However, unlike in randomized ablation, the partitions are deterministic and use disjoint samples, rather than selecting them randomly and independently. In Appendix C, we argue that our derandomization has little effect on the certified accuracies, while allowing for exact certificates using finite samples. We also discuss how this work relates to Levine and Feizi (2020b), which proposes a de-randomized ablation technique for a restricted class of sparse evasion attacks (patch adversarial attacks).\n\n\nDPA Practical Implementation Details\n\nOne of the advantages of DPA is that we can use deep neural networks for the base classifier f . However, enforcing that the output of a deep neural network is a deterministic function of its training data, and specifically, its training data as an unordered set, requires some care. First, we must remove dependence on the order in which the training samples are read in. To do this, in each partition P i , we sort the training samples prior to training, taking advantage of the assumption that S is well-ordered (and therefore S L = S \u00d7 N is also well ordered). In the case of the image data, this is implemented as a lexical sort by pixel values, with the labels concatenated to the samples as an additional value. The training procedure for the network, which is based on standard stochastic gradient descent, must also be made deterministic: in our PyTorch (Paszke et al., 2019) implementation, this can be accomplished by deterministically setting a random seed at the start of training. As discussed in Appendix F, we find that it is best to use different random seeds during training for each partition. This reduces the correlation in output between base classifiers in the ensemble. Thus, in practice, we use the partition index as the random seed (i.e., we train base classifier f i using random seed i.)\n\n\nSS-DPA\n\nSemi-Supervised DPA (SS-DPA) is a defense against label-flip attacks. For this defense, the base classifier may be a semi-supervised learning algorithm: it can use the entire unlabeled training dataset, in addition to the labels for a partition. We will therefore define the base classifier to also accept an unlabelled dataset as input: f : P(S) \u00d7 P(S L ) \u00d7 S \u2192 N. Additionally, our method of partitioning the data is modified both to ensure that changing the label of a sample affects only one partition rather than possibly two, and to create a more equal distribution of samples between partitions.\n\nFirst, we will sort the unlabeled data samples(T ): T sorted := Sort(samples(T )).\n\n(6) For a sample t \u2208 T , note that index(T sorted , sample(t)) is invariant under any label-flipping attack to T , and also under permutation of the training data as they are read. We now partition the data based on sorted index:\nP i := {t \u2208 T | index(T sorted , sample(t)) \u2261 i (mod k)}.\n(7) Note that in this partitioning scheme, we no longer need to use a hash function h. Moreover, this scheme creates a more uniform distribution of samples between partitions, compared with the hashing scheme used in DPA. This can lead to improved certificates: see Appendix E. This sorting-based partitioning is possible because the unlabeled samples are \"clean\", so we can rely on their ordering, when sorted, to remain fixed. As in DPA, we train base classifiers on each partition, this time additionally using the entire unlabeled training set:\nf i (x) := f (samples(T ), P i , x).(8)\nThe inference procedure is the same as in the standard DPA:\nn c (x) := |{i \u2208 [k]|f i (x) = c}| g ssdpa (T, x) := arg max c n c (x)(9)\nThe SS-DPA algorithm provides the following robustness guarantee against label-flipping attacks. 2 Theorem 2. For a fixed deterministic semi-supervised base classifier f , ensemble size k, training set T (with no repeated samples), and input x, let: c := g ssdpa (T, x),\n\u03c1(x) := n c \u2212 max c =c (n c (x) + 1 c <c ) 2 .(10)\nFor a poisoned training set U obtained by changing the labels of at most\u03c1 samples in T , g ssdpa (U, x) = c.\n\n\nSemi-Supervised Learning Methods for SS-DPA\n\nIn the standard DPA algorithm, we are able to train each classifier in the ensemble using only a small fraction of the training data; this means that each classifier can be trained relatively quickly: as the number of classifiers increases, the time to train each classifier can decrease (see Table 1). However, in a naive implementation of SS-DPA, Equation 8 might suggest that training time will scale with k, because each semi-supervised base classifier requires to be trained on the entire training set. Indeed, with many popular and highly effective choices of semi-supervised classification algorithms, such as temporal ensembling ( As discussed in Section 2.2.1, we also sort the data prior to learning (including when learning unsupervised features), and set random seeds, in order to ensure determinism. \n\n\nResults\n\nIn this section, we present empirical results evaluating the performance of proposed methods, DPA and SS-DPA, against poison attacks on MNIST and CIFAR-10 datasets. As discussed in Section 2.3.1, we use the RotNet architecture (Gidaris et al., 2018) for the semi-supervised learning. Conveniently, the RotNet architecture is structured such that the feature extracting layers, combined with the final classification layers, together make up the Network-In-Network (NiN) architecture for the supervised classification (Lin et al., 2013). We use NiN for DPA's supervised training, and RotNet for SS-DPA's semi-supervised training. On CIFAR-10, we use training parameters, for both the DPA (NiN) and SS-DPA (RotNet), directly from Gidaris et al. (2018). 3 On MNIST, we use the same architectures and training parameters, with a slight modification: we eliminate horizontal flips in data augmentation, because, unlike in CIFAR-10, horizontal aligment is semantically meaningful for digits. Figures 2 and 3, and are summarized in Table 1. Our metric, Certified Accuracy as a function of attack magnitude (symmetric-difference or label-flips), refers to the fraction of samples which are both correctly classified and are certified as robust to attacks of that magnitude. Note that different poisoning perturbations, which poison different sets of training samples, may be required to poison each test sample; i.e. we assume the attacker can use the attack budget separately for each test sample. Table 1 also reports Median Certified Robustness, the attack magnitude to which at least 50% of the test set is provably robust.\n\n\nResults are presented in\n\nAs shown in Figure 1, our SS-DPA method substantially outperforms the existing certificate (Rosenfeld et al., 2020) on label-flipping attacks. With DPA, we are also able to certify at least half of MNIST images to attacks of over 500 poisoning insertions or deletions. On CIFAR-10, on which Rosenfeld et al. (2020) does not test, DPA can certify at least half of images to 9 poisoning insertions or deletions, and SS-DPA can certify to over 80 label flips. In summary, we provide a new state-ofthe-art certified defence against label flipping attacks using SS-DPA, while also providing the first certified defense against a much more general poisoning threat model.\n\nThe hyperparameter k controls the number of classifiers in the ensemble: because each sample is used in training exactly one classifier, the average number of samples used to train each classifier is inversely proportional to k. Therefore, we observe that the base classifier accuracy (and therefore also the final ensemble classifier accuracy) decreases as k is increased; see Table 1. However, because the certificates described in Theorems 1 and 2 depend directly on the gap in the number of classifiers in the ensemble which output the top and runner-up classes, larger numbers of classifiers are necessary  to achieve large certificates. In fact, using k classifiers, the largest certified robustness possible is k/2. Thus, we see in Figures 2 and 3 that larger values of k tend to produce larger robustness certificates. Therefore k controls a trade-off between robustness and accuracy. Rosenfeld et al. (2020) also reports robustness certificates against label-flipping attacks on binary MNIST classification, with classes 1 and 7. Rosenfeld et al. (2020) reports clean-accuracy of 94.5% and certified accuracies for attack magnitudes up to 2000 label flips (out of 13007), with best certified accuracy less than 70%. By contrast, using a specialized form of SS-DPA, we are able to achieve clean accuracy of 95.5%, with every correctly-classified image certifiably robust up to 5952 label flips (i.e. certified accuracy is also 95.5% at 5952 label flips.) This represents a substantial improvement. We present details of this experiment in Appendix B.\n\n\nConclusion\n\nIn this paper, we described a novel approach to provable defenses against poisoning attacks. Unlike previous techniques, our method both allows for exact, deterministic certificates and can be implemented using deep neural networks. These advantages allow us to outperform the current state-of-the-art on label-flip attacks, and to develop the first certified defense against a broadly defined class of general poisoning attacks.\n\n\nBroader Impact\n\nMany datasets for machine learning often originate from information gathered from the public, through crowdsourcing, web scraping, or analysing user behavior in web applications. Systems trained on such datasets can then be used in highly critical applications such as web content moderation or fraud detection. Thus, it is important to develop learning algorithms that are robust to malicious distortions of the training data, i.e. poisoning attacks. We believe this work is useful to these ends. Additionally, because our algorithm provides provable certificates, it can be useful in establishing trust in machine learning systems: even in a non-adversarial setting, a user may feel more confident knowing that the system's decision did not hinge on a small number of possibly-spurious training examples. In this way, the certificate can be viewed as an accessible, transparent, easy-to-interpret metric of confidence. We also note that this work presents a purely defensive method against poisoning attacks; i.e. we are not revealing any new vulnerabilities in machine learning systems by presenting our techniques.\n\nCharles Smutz and Angelos Stavrou.\n\nWhen a tree falls: Using diversity in ensemble classifiers to identify evasion in malware detectors.\n\nIn 23rd Annual Network and Distributed System Security Symposium, NDSS 2016, San Diego, California, USA, February 21-24, 2016. The Internet Society, 2016 URL http: //wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2017/09/ when-tree-falls-using-diversity-ensemble-classifiers-identify-evasion-malware-detectors. pdf.\n\nJacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks.\n\nIn Advances in neural information processing systems, pages 3517-3529, 2017. Proof. Recall the definition:\n\nT sorted := Sort(samples(T )).\n\n(18) Because samples(T ) = samples(U ), we have T sorted = U sorted . We can then define partitions and base classifiers for each training set (T and U ) as described in the main text:\nP T i := {t \u2208 T | index(T sorted , sample(t)) \u2261 i (mod k)} P U i := {t \u2208 U | index(T sorted , sample(t)) \u2261 i (mod k)} (19) f T i (x) := f (samples(T ), P T i , x) f U i (x) := f (samples(T ), P U i , x)(20)\nRecall that for any t \u2208 T , index(T sorted , sample(t)) is invariant under label-flipping attack to T . Then, for each i, the samples in P T i will be the same as the samples in P U i , possibly with some labels flipped. In particular, the functions f T i (\u00b7) and f U i (\u00b7) will be identical, unless the label of some sample with index(T sorted , sample(t)) \u2261 i (mod k) has been changed. If at most\u03c1(x) labels change, at most\u03c1(x) ensemble classifiers are affected: the rest of the proof proceeds similarly as that of Theorem 1.\n\n\nB Binary MNIST Experiments\n\nWe perform a specialized instance of SS-DPA on the binary '1' versus '7' MNIST classification task. Specifically, we set k = m, so that every partition receives only one label. We first use 2-means clustering on the unlabeled data, to compute two means. This allows for each base classifier to use a very simple \"semi-supervised learning algorithm\": if the test image and the one labeled training image provided to the base classifier belong to the same cluster, then the base classifier assigns the label of the training image to the test image. Otherwise, it assigns the opposite label to the test image. Formally:\n\u00b5 1 , \u00b5 2 := Cluster centroids of samples(T ) assignment(s) := 1, if \u00b5 1 \u2212 s 2 \u2264 \u00b5 2 \u2212 s 2 2, if \u00b5 1 \u2212 s 2 > \u00b5 2 \u2212 s 2 f i (x) = f (samples(T ), {t i }, x) = label(t i ), if assignment(x) = assignment(t i ) 1 \u2212 label(t i ), if assignment(x) = assignment(t i )\nNote that each base classifier behaves exactly identically, up to a transpose of the labels: so in practice, we simply count the training samples which associate each of the two cluster centroids with each of the two labels, and determine the number of label flips which would be required to change the consensus label assignments of the clusters. At the test time, each test image therefore needs to be processed only once. The amount of time required for inference is then simply the time needed to calculate the distance from the test sample to each of the two clusters. This also means that every image has the same robustness certificate. As stated in the main text, using this method, we are able to achieve clean accuracy of 95.5%, with every correctly-classified image certifiably robust up to 5952 label flips (i.e. certified accuracy is also 95.5% at 5952 label flips.) This means that the classifier is robust to adversarial label flips on 45.8% of the training data. Rosenfeld et al. (2020) also reports robustness certificates against label-flipping attacks on binary MNIST classification with classes 1 and 7. Rosenfeld et al. (2020) reports clean-accuracy of 94.5% and certified accuracies for attack magnitudes up to 2000 label flips (out of 13007: 15.4%), with the best certified accuracy less than 70%.\n\n\nC Relationship to Randomized Ablation\n\nAs mentioned in Section 1, (SS-)DPA is in some sense related to Randomized Ablation (Levine and Feizi, 2020a) (used in defense against sparse inference-time attacks) for training-time poisoning attacks. Randomized Ablation is a certified defense against L 0 (sparse) inference attacks, in which the final classification is a consensus among classifications of copies of the image. In each copy, a fixed number of pixels are randomly ablated (replaced with a null value). A direct application of Randomized Ablation to poisoning attacks would require each base classifier to be trained on a random subset of the training data, with each base classifier's training set chosen randomly and independently. Due to the randomized nature of this algorithm, estimation error would have to be considered in practice when applying Randomized Ablation using a finite number of base classifiers: this decreases the certificates that can be reported, while also introducing a failure probability to the certificates. By contrast, in our algorithms, the partitions are deterministic and use disjoint, rather than independent, samples. In this section, we argue that our derandomization has little effect on the certified accuracies compared to randomized ablation, even considering randomized ablation with no estimation error (i.e., with infinite base classifiers). In the poisoning case specifically, using additional base classifiers is expensive -because they must each be trained -so one would observe a large estimation error when using a realistic number of base classifiers. Therefore our derandomization can potentially improve the certificates which can be reported, while also allowing for exact certificates using a finite number of base classifiers.\n\nFor simplicity, consider the label-flipping case. In this case, the training set has a fixed size, m. Thus, Randomized Ablation bounds can be considered directly. A direct adaptation of Levine and Feizi (2020a) would, for each base classifier, choose s out of m samples to retain labels for, and would ablate the labels for the rest of the training data. Suppose an adversary has flipped r labels. For each base classifier, the probability that a flipped label is used in classification (and therefore that the base classifier is 'poisoned') is:\nPr(poisoned) RA = 1 \u2212 m\u2212r s m s(21)\nwhere \"RA\" stands for Randomized Ablation.\n\nIn this direct adaptation, one must then use a very large ensemble of randomized classifiers. The ensemble must be large enough that we can estimate with high confidence the probabilities (on the distribution of possible choices of training labels to retain) that the base classifier selects each class. If the gap between the highest and the next-highest class probabilities can be determined to be greater than 2 Pr(poisoned) RA , then the consensus classification cannot be changed by flipping r labels. This is because, at worst, every poisoned classifier could switch the highest-class classification to the runner-up class, reducing the gap by at most 2 Pr(poisoned).\n\nNote that this estimation relies on each base classifier using a subset of labels selected randomly and independently from the other base classifier. In contrast, our SS-DPA method selects each subset disjointly. If r labels are flipped, assuming that in the worst case each flipped label is in a different partition, using the union bound, the proportion of base classifiers which can be poisoned is\nPr(poisoned) SS-DPA \u2264 r k = rs m(22)\nwhere for simplicity we assume that k evenly divides the number of samples m, so s = m/k labels are kept by each partition. Again we need the gap in class probabilities to be at least 2 Pr(poisoned) SS-DPA to ensure robustness. While the use of the union bound might suggest that our deterministic scheme (Equation 22) might lead to a significantly looser bound than that of the probabilistic certificate To understand this, note that if the number of poisonings r is small compared to the number of partitions k, then even if the partitions are random and independent, the chance that any two poisonings occur in the same partition is quite small. In that case, the union bound in Equation 22 is actually quite close to an independence assumption. By accepting this small increase in the upper bound of the probability that each base classification is poisoned, our method provides all of the benefits of de-randomization, including allowing for exact robustness certificates using only a finite number of classifiers. Additionally, note that in the Randomized Ablation case, the empirical gap in estimated class probabilities must be somewhat larger than Pr(poisoned) RA in order to certify robustness with high confidence, due to estimation error: the gap required increases more as the number of base classifiers decreases. This is particularly important in the poisoning case, because training a large number of classifiers is substantially more expensive than performing a large number of evaluations, as in randomized smoothing for evasion attacks.\n\nWe also note that Levine and Feizi (2020b) also used a de-randomized scheme based on Randomized Ablation to certifiably defend against evasion patch attacks. However, in that work, the de-randomization does not involve a union bound over arbitrary partitions of the vulnerable inputs. Instead, in the case of patch attacks, the attack is geometrically constrained: the image is therefore divided into geometric regions (bands or blocks) such that the attacker will only overlap with a fixed number of these regions. Each base classifier then uses only a single region to make its classification. Levine and Feizi (2020b) do not apply this to derandomization via disjoint subsets/union bound to defend against poison attacks. Also, we note that we borrow from Levine and Feizi (2020b) the deterministic \"tie-breaking\" technique when evaluating the consensus class in Equation 4, which can increase our robustness certificate by up to one.\n\n\nD Relationship to Existing Ensemble Methods\n\nAs mentioned in Section 1, our proposed method is related to classical ensemble approaches in machine learning, namely bootstrap aggregation (\"bagging\") and subset aggregation (\"subagging\") (Breiman, 1996;Buja and Stuetzle, 2006;B\u00fchlmann, 2003;Zaman and Hirose, 2009). In these methods, each base classifier in the ensemble is trained on an independently sampled collection of points from the training set: this means that multiple classifiers in the ensemble may be trained on the same sample point. The purpose of these methods has typically been to improve generalization, and therefore to improve test set accuracy: bagging and subagging decrease the variance component of the classifier's error.\n\nIn subagging, each training set for a base classifier is an independently sampled subset of the training data: this is in fact an identical formulation to the \"direct Randomized Ablation\" approach discussed in Appendix C. However, in practice, the size of each training subset has typically been quite large: the bias error term increases with decreasing subsample sizes (Buja and Stuetzle, 2006). Thus, the optimal subsample size for maximum accuracy is large: B\u00fchlmann (2003) recommends using s = m/2 samples per classifier (\"half-subagging\"), with theoretical justification for optimal generalization. This would not be useful in Randomized Ablation-like certification, because any one poisoned element would affect half of the ensemble. Indeed, in our certifiably robust classifiers, we observe a trade-off between accuracy and certified robustness: our use of many very small partitions is clearly not optimal for the test-set accuracy (Table 1).\n\nIn bagging, the samples in each base classifier training set are chosen with replacement, so elements may be repeated in the training \"set\" for a single base classifier. Bagging has been proposed as an empirical defense against poisoning attacks (Biggio et al., 2011) as well as for evasion attacks (Smutz and Stavrou, 2016). However, to our knowledge, these techniques have not yet been used to provide certified robustness.\n\n\nE SS-DPA with Hashing\n\nIt is possible to use hashing, as in DPA, in order to partition data for SS-DPA: as long as the hash function h(t) does not use the sample label in assigning a class (as ours indeed does not), it will always assign an image to the same partition regardless of label-flipping, so only one partition will be affected by a label-flip. Therefore, the SS-DPA label-flipping certificate should still be correct. However, as explained in the main text, treating the unlabeled data as trustworthy allows us to partition the samples evenly among partitions using sorting. This is motivated by the classical understanding in machine learning (e.g. Amari et al. (1992)) that learning curves (the test error versus the number of samples that a classifier is trained on) tend to be convex-like. The test error of a base classifier is then approximately a convex function of that base classifier's partition size. Therefore, if the partition size is a random variable, by Jensen's inequality, the expected test error of the (random) partition size is greater than the test error of the mean partition size. Setting all base classifiers to use the mean number of samples should then maximize the average base classifier accuracy. To validate this reasoning, we tested SS-DPA with partitions determined by hashing (using the same partitions as we used in DPA), rather than the sorting method described in the main text. See Table 2 for results. As expected, the average base classifier accuracy decreased in all experiments when using the DPA hashing, compared to using the sorting method of SS-DPA. However, the effect was minimal in CIFAR-10 experiments: the main advantage of the sorting method was seen on MNIST. This is partly because we used more partitions, and hence fewer average samples per partition, in the MNIST  Table 2: Comparison of SS-DPA with hashing ('Hash' columns, described in Appendix E) to the SS-DPA algorithm with partitions determined by sorting ('Sort' columns, described in the main text). Note that partitioning via sorting consistently results in higher base classifier accuracies, and can increase (and never decreases) median certified robustness. These effects seem to be larger on MNIST than on CIFAR-10. experiments: fewer average samples per partition creates a greater variation in the number of samples per partition in the hashing method. However, CIFAR-10 with k = 1000 and MNIST with k = 1200 both average 50 samples per partition, but the base classifier accuracy difference still was much more significant on MNIST (2.11%) compared to CIFAR-10 (0.32%).\n\nOn the MNIST experiments, where the base classifier accuracy gap was observed, we also saw that the effect of hashing on the smoothed classifier was mainly to decrease the certified robustness, and that there was not a significant effect on the clean smoothed classifier accuracy. As discussed in Appendix F, this may imply that the outputs of the base classifiers using the sorting method are more correlated, in addition to being more accurate.\n\n\nF Effect of Random Seed Selection\n\nIn Section 2.2.1, we mention that we \"deterministically\" choose different random seeds for training each partition, rather than training every partition with the same random seed. To see a comparison between using distinct and the same random seed for each partition, see Table 3. Note that there is not a large, consistent effect across experiments on either the base classifier accuracy nor the the median certified robustness: however, across all 10 experiments, the distinct random seeds always resulted in higher smoothed classifier accuracies. This effect was particularly pronounced using SS-DPA: using distinct seeds increased smoothed accuracy by at least 1.5% on each value of k on MNIST, and by nearly 3% for k = 1000 on CIFAR-10 (where the base classifier accuracy difference was only 0.1%). This implies that shared random seeds make the base classifiers more correlated with each other: at the same level of average base classifier accuracy, it is more likely that a plurality of base classifiers will all misclassify the same sample (If the base classifiers were perfectly uncorrelated, we would see nearly 100% smoothed clean accuracy wherever the base classifier accuracy was over 50%. Also if they were perfectly correlated, the smoothed clean accuracy would equal the base classifier accuracy).\n\nIt is somewhat surprising that the base classifiers become correlated when using the same random seed, given that they are trained on entirely distinct data. However, two factors may be at play here. First, note that the random seed used in training controls the random cropping of training images: it is possible that, because the training sets of the base classifiers are so small, using the same cropping patterns in every classifier would create a systematic bias. Second, note that the effect is most pronounced using SS-DPA, where all base classifiers share their final network layers: this would tend to increase the baseline correlation between base classifiers (Indeed, for k = 3000 on MNIST, SS-DPA has similar clean accuracy to DPA, despite having a 10% higher base classifier accuracy). We can speculate that there may be some synergistic effect between these two causes of correlation: however, understanding these effects precisely is an opportunity for potential future research.\n\nFigure 1 :\n1(a). Comparison of certified accuracy to label-flipping poison attacks for our defense (SS-DPA algorithm) vs. Rosenfeld et al. (2020) on MNIST. Solid lines represent certified accuracy as a function of attack size; dashed lines show the clean accuracies of each model. Our algorithm produces substantially higher certified accuracies. Performance curves for Rosenfeld et al. (2020) are adapted from Figure 1 in that work. The parameter q is a hyperparameter of Rosenfeld et al. (2020)'s algorithm, and k is a hyperparameter of our algorithm: the number of base classifiers in an ensemble.\n\n\nWeber et al. (2020) have recently proposed a different randomized-smoothing based defense against poisoning attacks by directly applying Cohen et al. (2019)'s smoothing L 2 evasion defense to the poisoning domain. The proposed technique can only certify for clean-label attacks (where only the existing images in the dataset are modified, and not their labels), and the certificate guarantees robustness only to bounded L 2 distortions of the training data, where the L 2 norm of the distortion is calculated across all pixels in the entire training set. Due to well-known limitations of dimensional scaling for smoothing-based robustness certificates (Yang et al., 2020; Kumar et al., 2020; Blum et al., 2020), this yields certificates to only very small distortions of the training data. (For binary MNIST [13,007 images], the maximum reported L 2 certificate is 2 pixels.) Additionally, when using deep classifiers, Weber et al. (2020) proposes a randomized certificate, rather than a deterministic one, with a failure probability that decreases to zero only as the number of trained classifiers in an ensemble approaches infinity. Moreover, in Weber et al. (2020), unlike in our method, each classifier in the ensemble must be trained on a noisy version of the entire dataset. These issues hinder Weber et al. (2020)'s method to be an effective scheme for certified robustness against poisoning attacks.\n\n\nLaine and Aila, 2017),ICT (Verma et al., 2019), Teacher Graphs(Luo et al., 2018) and generative approaches(Kingma et al., 2014), the main training loop trains on both labeled and unlabeled samples, so we would see the total training time scale linearly with k. In order to avoid this, we instead choose a semi-supervised training method where the unlabeled samples are used only to learn semantic features of the data, before the labeled samples are introduced: this allows us to use the unlabeled samples only once, and to then share the learned feature representations when training each base classifier. In our experiments, we choose the RotNet model introduced byGidaris et al. (2018), which first trains a network to infer the angle of rotation on rotated forms of unlabeled images: an intermediate layer of this network is then used as features in the final classification.\n\nFigure 2 :\n2Certified Accuracy to poisoning attacks on MNIST, using (a) DPA to certify against general poisoning attacks, and (b) SS-DPA to certify against label-flipping attacks. Dashed lines show the clean accuracies of each model. The hyperparameter k is the number of base classifier models in the ensemble used to make final classifications.\n\nFigure 3 :\n3Certified Accuracy to poisoning attacks on CIFAR, using (a) DPA to certify against general poisoning attacks, and (b) SS-DPA to certify against label-flipping attacks. The hyperparameter k is the number of base classifier models in the ensemble used to make final classifications.\n\n(\nEquation 21), this is not the case in practice where rs << m. For example, in an MNIST-sized dataset (m = 60000), using s = 50 labels per base classifier, to certify for r = 200 label flips, we have Pr(poisoned) RA = 0.154, and Pr(poisoned) SS-DPA \u2264 0.167. The derandomization only sightly increases the required gap between the top two class probabilities.\n\n\nSummary statistics for DPA and SS-DPA algorithms on MNIST and CIFAR. Median Certified Robustness is the attack magnitude (symmetric difference for DPA, label flips for SS-DPA) at which certified accuracy is 50%. Training times are on a single GPU; note that many partitions can be trained in parallel. Note we observe some constant overhead time for training each classifier, so on MNIST, where the training time per image is small, k has little effect on the training time.Training Number of \n\nMedian \nBase \nTraining \nset \nPartitions \nCertified \nClean \nClassifier time per \nsize \nk \nRobustness Accuracy Accuracy Partition \n\nMNIST, DPA \n60000 \n1200 \n448 \n95.82% \n77.00% \n0.28 min \n3000 \n509 \n93.28% \n49.59% \n0.26 min \n\nMNIST, SS-DPA \n60000 \n1200 \n493 \n95.91% \n81.70% \n0.11 min \n3000 \n675 \n93.93% \n59.21% \n0.10 min \n50 \n9 \n70.29% \n56.25% \n1.42 min \nCIFAR, DPA \n50000 \n250 \n6 \n55.72% \n35.18% \n0.56 min \n1000 \nN/A \n44.30% \n23.25% \n0.36 min \n50 \n21 \n81.97% \n74.73% \n0.72 min \nCIFAR, SS-DPA \n50000 \n250 \n59 \n74.60% \n61.98% \n0.28 min \n1000 \n83 \n68.22% \n44.26% \n0.12 min \nTable 1: \n\n\nGreg Yang, Tony Duan, Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized smoothing of all shapes and sizes. arXiv preprint arXiv:2002.08118, 2020. Faisal Zaman and Hideo Hirose. Effect of subsampling rate on subbagging and related ensembles of stable classifiers. In Santanu Chaudhury, Sushmita Mitra, C. A. Murthy, P. S. Sastry, and Sankar K. Pal, editors, Pattern Recognition and Machine Intelligence, pages 44-49, Berlin, Heidelberg, 2009.Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation \nconsistency training for semi-supervised learning. In Proceedings of the 28th International Joint \nConference on Artificial Intelligence, pages 3635-3641. AAAI Press, 2019. \n\nMaurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against \nbackdoor attacks. arXiv preprint arXiv:2003.08904, 2020. \n\nEric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer \nadversarial polytope. In International Conference on Machine Learning, pages 5283-5292, 2018. \n\nHan Xiao, Huang Xiao, and Claudia Eckert. Adversarial label flips attack on support vector machines. \nIn Proceedings of the 20th European Conference on Artificial Intelligence, pages 870-875, 2012. \n\nSpringer Berlin Heidelberg. ISBN 978-3-642-11164-8. \n\n Steinhardt et al. (2017)  also refers to a \"certified defense\" for poisoning attacks. However, the definition of the certificate is substantially different in that work, which instead provides overall accuracy guarantees under the assumption that the training and test data are drawn from similar distributions, rather than providing guarantees for individual realized inputs.\nThe theorem as stated assumes that there are no repeated unlabeled samples (with different labels) in the training set T . This is a reasonable assumption, and in the label-flipping attack model, the attacker cannot cause this assumption to be broken. Without this assumption, the analysis is more complicated; see Appendix G.\nIn addition to the de-randomization changes mentioned in Section 2.2.1, we made one modification to the NiN 'baseline' for supervised learning: the baseline implementation inGidaris et al. (2018), even when trained on a small subset of the training data, uses normalization constants derived from the entire training set. This is a (minor) error inGidaris et al. (2018) that we correct by calculating normalization constants on each subset.\nSpecifically, to optimize for performance, we verify that there are no repeated sample values, and then sort T itself (rather than samples(T )) lexicographically by image pixel values: this is equivalent to sorting samples(T ) if no repeated images occur -which we have already verified -and avoids an unnecessary lookup procedure to find the sorted index of the unlabeled sample for each labeled sample.\nAcknowledgementsThis project was supported in part by NSF CAREER AWARD 1942230, HR 00111990077, HR001119S0026 and Simons Fellowship on \"Foundations of Deep Learning.\".Then, for any poisoned training set U , if |T U | \u2264\u03c1(x), we have: g dpa (U, x) = c.Proof. We define the partitions, trained classifiers, and counts for each training set (T and U ) as described in the main text:Note that here, we are using superscripts to explicitly distinguish between partitions (as well as base classifiers and counts) of the clean training set T and the poisoned dataset U (i.e, P T i is equivalent to P i in the main text). In Equation 15, as discussed in the main text, when taking the argmax, we break ties deterministically by returning the smaller class index.Note that P T i = P U i unless there is some t, with h(t) \u2261 i (mod k), in T U . Because the mapping from t to h(t) (mod k) is a deterministic function, the number of partitions i for which P T i = P U i is at most |T U |, which is at most\u03c1(x).is also at most\u03c1(x). Then:Let c := g dpa (T, x). Note that g dpa (U, x) = c iff:where the separate cases come from the deterministic selection of the smaller index in cases of ties in Equation 15: this can be condensed to \u2200c = c :This condition is true by the definition of\u03c1(x), so g dpa (U, x) = c.Theorem 2. For a fixed deterministic semi-supervised base classifier f , ensemble size k, training set T (with no repeated samples), and input x, let:For a poisoned training set U obtained by changing the labels of at most\u03c1 samples in T ,Table 3: Comparison of DPA and SS-DPA algorithms using the same random seed for each partition ('Same' columns, described in Appendix F) to the DPA and SS-DPA algorithms using the distinct random seeds for each partition ('Distinct' columns, described in the main text). Note that using distinct random seeds consistently results in higher smoothed classifier clean accuracies, often by large margins when using SS-DPA. While it might appear that there some tendency for the base classifiers to also be more accurate using distict seeds (in eight out of 10 experiments), the magnitude of this difference is often very small, and it is not a consistent effect.G SS-DPA with Repeated Unlabeled DataThe definition of a training set that we use, T \u2208 P(S L ), technically allows for repeated samples with differing labels: there could be a pair of distinct samples, t, t \u2208 T , such that sample(t) = sample(t ), but label(t) = label(t ). This creates difficulties with the definition of the label-flipping attack: for example, the attacker could flip the label of t to become the label of t : this would break the definition of T as a set. In most applications, this is not a circumstance that warrants practical consideration: indeed, none of the datasets used in our experiments have such instances (nor can label-flipping attacks create them), and therefore, for performance reasons, our implementation of SS-DPA does not handle these cases 4 . However, the SS-DPA algorithm as described in Section 2.3 can be implemented to handle such datasets, under a formalism of label-flipping tailored to represent this edge case.Specifically, we define the space of possible labeled data points as S L = S \u00d7 P(N): each labeled data point consists of a sample along with a set of associated labels. We then restrict our dataset T to be any subset of S L such that for all t, t \u2208 T , sample(t) = sample(t ). In other words, we do not allow repeated sample values in T as formally defined: if repeated samples exist, one can simply merge their sets of associated labels (in the formalism).Note that using this definition, the size of T will equal the size of samples(T ), and the samples will always remain the same: the adversary can only modify the label sets of samples \"in place\". SS-DPA will always assign an unlabeled sample, along with all of its associated labels, to the same partition, regardless of any label flipping. In practice, this is because, as described in Section 2.3, the partition assignment of a labeled sample depends only on its sample value, not its label: all labeled samples with the same sample value will be put in the same partition. Note that this is true even if the implementation represents two identical sample values with different labels as two separate samples: one does not actually have to implement labels as sets. Therefore, any changes to any labels associated with a sample will only change the output of one base classifier, so all such changes can together be considered a single label-flip in the context of the certificate. In the above formalism, the certificate represents the number of samples in T whose label sets have been \"flipped\": i.e., modified in any way.\nFour types of learning curves. Naotake Shun-Ichi Amari, Shigeru Fujita, Shinomoto, Neural Computation. 44Shun-ichi Amari, Naotake Fujita, and Shigeru Shinomoto. Four types of learning curves. Neural Computation, 4(4):605-618, 1992.\n\nBagging classifiers for fighting poisoning attacks in adversarial classification tasks. Battista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto, Fabio Roli, International workshop on multiple classifier systems. SpringerBattista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli. Bagging classifiers for fighting poisoning attacks in adversarial classification tasks. In International workshop on multiple classifier systems, pages 350-359. Springer, 2011.\n\nPoisoning attacks against support vector machines. Battista Biggio, Blaine Nelson, Pavel Laskov, Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML'12. the 29th International Coference on International Conference on Machine Learning, ICML'12Madison, WI, USA9781450312851Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector ma- chines. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML'12, page 1467-1474, Madison, WI, USA, 2012. Omnipress. ISBN 9781450312851.\n\nRandom smoothing might be unable to certify \u221e robustness for high-dimensional images. Avrim Blum, Travis Dick, Naren Manoj, Hongyang Zhang, arXiv:2002.03517arXiv preprintAvrim Blum, Travis Dick, Naren Manoj, and Hongyang Zhang. Random smoothing might be unable to certify \u221e robustness for high-dimensional images. arXiv preprint arXiv:2002.03517, 2020.\n\nBagging predictors. Machine learning. Leo Breiman, 24Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.\n\nBagging, subagging and bragging for improving some prediction algorithms. Peter Lukas B\u00fchlmann, Research report/Seminar f\u00fcr Statistik, Eidgen\u00f6ssische Technische Hochschule. Z\u00fcrich113Eidgen\u00f6ssische Technische Hochschule (ETH)Peter Lukas B\u00fchlmann. Bagging, subagging and bragging for improving some prediction algorithms. In Research report/Seminar f\u00fcr Statistik, Eidgen\u00f6ssische Technische Hochschule (ETH), volume 113. Seminar f\u00fcr Statistik, Eidgen\u00f6ssische Technische Hochschule (ETH), Z\u00fcrich, 2003.\n\nObservations on bagging. Andreas Buja, Werner Stuetzle, Statistica Sinica. Andreas Buja and Werner Stuetzle. Observations on bagging. Statistica Sinica, pages 323-351, 2006.\n\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, Dawn Song, arXiv:1712.05526Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprintXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n\nCertified adversarial robustness via randomized smoothing. Jeremy Cohen, Elan Rosenfeld, Zico Kolter, International Conference on Machine Learning. Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, pages 1310-1320, 2019.\n\nUnsupervised representation learning by predicting image rotations. Spyros Gidaris, Praveer Singh, Nikos Komodakis, International Conference on Learning Representations. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=S1v4N2l0-.\n\nOn the effectiveness of interval bound propagation for training verifiably robust models. Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, Pushmeet Kohli, arXiv:1810.12715arXiv preprintSven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.\n\nSemi-supervised learning with deep generative models. Shakir Durk P Kingma, Danilo Mohamed, Max Jimenez Rezende, Welling, Z. Ghahramani, M. Welling, C. Cortes, N. DDurk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Z. Ghahramani, M. Welling, C. Cortes, N. D.\n\nK Q Lawrence, Weinberger, Advances in Neural Information Processing Systems. Curran Associates, Inc27Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3581-3589. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/ 5352-semi-supervised-learning-with-deep-generative-models.pdf.\n\nAounon Kumar, Alexander Levine, Tom Goldstein, Soheil Feizi, arXiv:2002.03239Curse of dimensionality on randomized smoothing for certifiable robustness. arXiv preprintAounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensionality on randomized smoothing for certifiable robustness. arXiv preprint arXiv:2002.03239, 2020.\n\nTemporal ensembling for semi-supervised learning. Samuli Laine, Timo Aila, 5th International Conference on Learning Representations. Toulon, FranceConference Track ProceedingsSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In 5th Interna- tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. URL https://openreview.net/forum?id=BJ6oOfqge.\n\nCertified robustness to adversarial examples with differential privacy. M Lecuyer, V Atlidakis, R Geambasu, D Hsu, S Jana, https:/doi.ieeecomputersociety.org/10.1109/SP.2019.000442019 2019 IEEE Symposium on Security and Privacy (SP). Los Alamitos, CA, USAM. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. Certified robustness to adversarial examples with differential privacy. In 2019 2019 IEEE Symposium on Security and Privacy (SP), pages 726-742, Los Alamitos, CA, USA, may 2019. IEEE Computer Society. doi: 10.1109/SP. 2019.00044. URL https://doi.ieeecomputersociety.org/10.1109/SP.2019.00044.\n\nTight certificates of adversarial robustness for randomly smoothed classifiers. Guang-He Lee, Yang Yuan, Shiyu Chang, Tommi Jaakkola, Advances in Neural Information Processing Systems. Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi Jaakkola. Tight certificates of adversarial robustness for randomly smoothed classifiers. In Advances in Neural Information Processing Systems, pages 4911-4922, 2019.\n\nWasserstein smoothing: Certified robustness against wasserstein adversarial attacks. Alexander Levine, Soheil Feizi, arXiv:1910.10783arXiv preprintAlexander Levine and Soheil Feizi. Wasserstein smoothing: Certified robustness against wasserstein adversarial attacks. arXiv preprint arXiv:1910.10783, 2019.\n\nRobustness certificates for sparse adversarial attacks by randomized ablation. Association for the Advancement of. Alexander Levine, Soheil Feizi, Artificial Intelligence. 2020AAAIAlexander Levine and Soheil Feizi. Robustness certificates for sparse adversarial attacks by random- ized ablation. Association for the Advancement of Artificial Intelligence (AAAI), 2020a.\n\n(de) randomized smoothing for certifiable defense against patch attacks. Alexander Levine, Soheil Feizi, arXiv:2002.10733arXiv preprintAlexander Levine and Soheil Feizi. (de) randomized smoothing for certifiable defense against patch attacks. arXiv preprint arXiv:2002.10733, 2020b.\n\nBai Li, Changyou Chen, Wenlin Wang, Lawrence Carin, arXiv:1809.03113Second-order adversarial attack and certifiable robustness. arXiv preprintBai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack and certifiable robustness. arXiv preprint arXiv:1809.03113, 2018.\n\n. Min Lin, Qiang Chen, Shuicheng Yan, arXiv:1312.4400Network in network. arXiv preprintMin Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.\n\nSmooth neighbors on teacher graphs for semi-supervised learning. Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, Bo Zhang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc32Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch\u00e9- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.\n\nCertified robustness to label-flipping attacks via randomized smoothing. Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, J Zico Kolter, arXiv:2002.03018arXiv preprintElan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and J. Zico Kolter. Certified robustness to label-flipping attacks via randomized smoothing. arXiv preprint arXiv:2002.03018, 2020.\n\nGreg Hadi Salman, Jerry Yang, Pengchuan Li, Huan Zhang, Ilya Zhang, Sebastien Razenshteyn, Bubeck, arXiv:1906.04584Provably robust deep learning via adversarially trained smoothed classifiers. arXiv preprintHadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. arXiv preprint arXiv:1906.04584, 2019.\n\nPoison frogs! targeted clean-label poisoning attacks on neural networks. Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa- Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 6103-6113. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7849-poison-frogs-targeted-clean-label-poisoning-attacks-on-neural-networks. pdf.\n", "annotations": {"author": "[{\"end\":211,\"start\":82},{\"end\":335,\"start\":212}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":92},{\"end\":224,\"start\":219}]", "author_first_name": "[{\"end\":91,\"start\":82},{\"end\":218,\"start\":212}]", "author_affiliation": "[{\"end\":210,\"start\":120},{\"end\":334,\"start\":244}]", "title": "[{\"end\":79,\"start\":1},{\"end\":414,\"start\":336}]", "venue": null, "abstract": "[{\"end\":2407,\"start\":416}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2828,\"start\":2809},{\"end\":2880,\"start\":2861},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2903,\"start\":2880},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2967,\"start\":2946},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2988,\"start\":2967},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4084,\"start\":4061},{\"end\":4380,\"start\":4358},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4401,\"start\":4382},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4424,\"start\":4403},{\"end\":4469,\"start\":4450},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4490,\"start\":4471},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4770,\"start\":4747},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5042,\"start\":5019},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5155,\"start\":5138},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5177,\"start\":5160},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5651,\"start\":5628},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5964,\"start\":5941},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6176,\"start\":6153},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6641,\"start\":6624},{\"end\":6797,\"start\":6773},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7234,\"start\":7217},{\"end\":7331,\"start\":7307},{\"end\":7486,\"start\":7462},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7828,\"start\":7805},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7940,\"start\":7917},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8232,\"start\":8209},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8808,\"start\":8785},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10258,\"start\":10243},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10282,\"start\":10258},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10297,\"start\":10282},{\"end\":10319,\"start\":10297},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10760,\"start\":10739},{\"end\":10817,\"start\":10792},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11366,\"start\":11343},{\"end\":11934,\"start\":11914},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11951,\"start\":11934},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11972,\"start\":11951},{\"end\":11992,\"start\":11972},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12013,\"start\":11992},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12668,\"start\":12644},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12954,\"start\":12931},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13508,\"start\":13485},{\"end\":16058,\"start\":16055},{\"end\":17535,\"start\":17510},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18044,\"start\":18020},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19098,\"start\":19077},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22792,\"start\":22770},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23078,\"start\":23060},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23292,\"start\":23271},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24505,\"start\":24482},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25774,\"start\":25751},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25920,\"start\":25897},{\"end\":28212,\"start\":28152},{\"end\":28262,\"start\":28212},{\"end\":28290,\"start\":28262},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31533,\"start\":31510},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31678,\"start\":31655},{\"end\":32001,\"start\":31977},{\"end\":33853,\"start\":33829},{\"end\":35699,\"start\":35687},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36981,\"start\":36957},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":37559,\"start\":37535},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38129,\"start\":38114},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":38153,\"start\":38129},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38168,\"start\":38153},{\"end\":38190,\"start\":38168},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":39022,\"start\":38997},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39846,\"start\":39825},{\"end\":39903,\"start\":39878},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40687,\"start\":40668},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":47441,\"start\":47420},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":47500,\"start\":47482},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":48108,\"start\":48087},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":52605,\"start\":52584},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":52779,\"start\":52758}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46007,\"start\":45406},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47417,\"start\":46008},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48299,\"start\":47418},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48647,\"start\":48300},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48941,\"start\":48648},{\"attributes\":{\"id\":\"fig_5\"},\"end\":49302,\"start\":48942},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":50379,\"start\":49303},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":51704,\"start\":50380}]", "paragraph": "[{\"end\":3317,\"start\":2423},{\"end\":4172,\"start\":3319},{\"end\":5568,\"start\":4174},{\"end\":6587,\"start\":5570},{\"end\":8291,\"start\":6589},{\"end\":8880,\"start\":8293},{\"end\":10104,\"start\":8882},{\"end\":11072,\"start\":10106},{\"end\":12569,\"start\":11074},{\"end\":13780,\"start\":12571},{\"end\":14794,\"start\":13793},{\"end\":14856,\"start\":14796},{\"end\":15286,\"start\":14922},{\"end\":15568,\"start\":15294},{\"end\":15709,\"start\":15570},{\"end\":16131,\"start\":15745},{\"end\":16236,\"start\":16133},{\"end\":16413,\"start\":16261},{\"end\":16540,\"start\":16450},{\"end\":16859,\"start\":16579},{\"end\":17007,\"start\":16928},{\"end\":18173,\"start\":17009},{\"end\":19530,\"start\":18214},{\"end\":20143,\"start\":19541},{\"end\":20227,\"start\":20145},{\"end\":20458,\"start\":20229},{\"end\":21065,\"start\":20517},{\"end\":21165,\"start\":21106},{\"end\":21510,\"start\":21240},{\"end\":21670,\"start\":21562},{\"end\":22531,\"start\":21718},{\"end\":24162,\"start\":22543},{\"end\":24856,\"start\":24191},{\"end\":26416,\"start\":24858},{\"end\":26860,\"start\":26431},{\"end\":27997,\"start\":26879},{\"end\":28033,\"start\":27999},{\"end\":28135,\"start\":28035},{\"end\":28461,\"start\":28137},{\"end\":28562,\"start\":28463},{\"end\":28670,\"start\":28564},{\"end\":28702,\"start\":28672},{\"end\":28888,\"start\":28704},{\"end\":29623,\"start\":29096},{\"end\":30270,\"start\":29654},{\"end\":31851,\"start\":30531},{\"end\":33641,\"start\":31893},{\"end\":34188,\"start\":33643},{\"end\":34267,\"start\":34225},{\"end\":34942,\"start\":34269},{\"end\":35344,\"start\":34944},{\"end\":36937,\"start\":35382},{\"end\":37876,\"start\":36939},{\"end\":38624,\"start\":37924},{\"end\":39577,\"start\":38626},{\"end\":40004,\"start\":39579},{\"end\":42610,\"start\":40030},{\"end\":43058,\"start\":42612},{\"end\":44409,\"start\":43096},{\"end\":45405,\"start\":44411}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14921,\"start\":14857},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15744,\"start\":15710},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16260,\"start\":16237},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16449,\"start\":16414},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16578,\"start\":16541},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16927,\"start\":16860},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20516,\"start\":20459},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21105,\"start\":21066},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21239,\"start\":21166},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21561,\"start\":21511},{\"attributes\":{\"id\":\"formula_10\"},\"end\":29095,\"start\":28889},{\"attributes\":{\"id\":\"formula_11\"},\"end\":30530,\"start\":30271},{\"attributes\":{\"id\":\"formula_12\"},\"end\":34224,\"start\":34189},{\"attributes\":{\"id\":\"formula_13\"},\"end\":35381,\"start\":35345}]", "table_ref": "[{\"end\":12913,\"start\":12906},{\"end\":22018,\"start\":22011},{\"end\":23575,\"start\":23568},{\"end\":24041,\"start\":24034},{\"end\":25243,\"start\":25236},{\"end\":39575,\"start\":39567},{\"end\":41445,\"start\":41438},{\"end\":41847,\"start\":41840},{\"end\":43375,\"start\":43368}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2421,\"start\":2409},{\"attributes\":{\"n\":\"2.1\"},\"end\":13791,\"start\":13783},{\"attributes\":{\"n\":\"2.2\"},\"end\":15292,\"start\":15289},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":18212,\"start\":18176},{\"attributes\":{\"n\":\"2.3\"},\"end\":19539,\"start\":19533},{\"attributes\":{\"n\":\"2.3.1\"},\"end\":21716,\"start\":21673},{\"attributes\":{\"n\":\"3\"},\"end\":22541,\"start\":22534},{\"end\":24189,\"start\":24165},{\"attributes\":{\"n\":\"4\"},\"end\":26429,\"start\":26419},{\"end\":26877,\"start\":26863},{\"end\":29652,\"start\":29626},{\"end\":31891,\"start\":31854},{\"end\":37922,\"start\":37879},{\"end\":40028,\"start\":40007},{\"end\":43094,\"start\":43061},{\"end\":45417,\"start\":45407},{\"end\":48311,\"start\":48301},{\"end\":48659,\"start\":48649},{\"end\":48944,\"start\":48943}]", "table": "[{\"end\":50379,\"start\":49779},{\"end\":51704,\"start\":50840}]", "figure_caption": "[{\"end\":46007,\"start\":45419},{\"end\":47417,\"start\":46010},{\"end\":48299,\"start\":47420},{\"end\":48647,\"start\":48313},{\"end\":48941,\"start\":48661},{\"end\":49302,\"start\":48945},{\"end\":49779,\"start\":49305},{\"end\":50840,\"start\":50382}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7708,\"start\":7700},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8122,\"start\":8114},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12898,\"start\":12890},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23544,\"start\":23529},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24211,\"start\":24203},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25612,\"start\":25597}]", "bib_author_first_name": "[{\"end\":58028,\"start\":58021},{\"end\":58053,\"start\":58046},{\"end\":58319,\"start\":58311},{\"end\":58333,\"start\":58328},{\"end\":58349,\"start\":58342},{\"end\":58365,\"start\":58358},{\"end\":58381,\"start\":58376},{\"end\":58769,\"start\":58761},{\"end\":58784,\"start\":58778},{\"end\":58798,\"start\":58793},{\"end\":59406,\"start\":59401},{\"end\":59419,\"start\":59413},{\"end\":59431,\"start\":59426},{\"end\":59447,\"start\":59439},{\"end\":59710,\"start\":59707},{\"end\":60327,\"start\":60320},{\"end\":60340,\"start\":60334},{\"end\":60476,\"start\":60470},{\"end\":60488,\"start\":60483},{\"end\":60496,\"start\":60494},{\"end\":60509,\"start\":60501},{\"end\":60518,\"start\":60514},{\"end\":60865,\"start\":60859},{\"end\":60877,\"start\":60873},{\"end\":60893,\"start\":60889},{\"end\":61201,\"start\":61195},{\"end\":61218,\"start\":61211},{\"end\":61231,\"start\":61226},{\"end\":61622,\"start\":61618},{\"end\":61643,\"start\":61630},{\"end\":61661,\"start\":61655},{\"end\":61677,\"start\":61673},{\"end\":61692,\"start\":61685},{\"end\":61706,\"start\":61698},{\"end\":61722,\"start\":61715},{\"end\":61737,\"start\":61729},{\"end\":62096,\"start\":62090},{\"end\":62118,\"start\":62112},{\"end\":62131,\"start\":62128},{\"end\":62375,\"start\":62374},{\"end\":62377,\"start\":62376},{\"end\":62721,\"start\":62715},{\"end\":62738,\"start\":62729},{\"end\":62750,\"start\":62747},{\"end\":62768,\"start\":62762},{\"end\":63119,\"start\":63113},{\"end\":63131,\"start\":63127},{\"end\":63582,\"start\":63581},{\"end\":63593,\"start\":63592},{\"end\":63606,\"start\":63605},{\"end\":63618,\"start\":63617},{\"end\":63625,\"start\":63624},{\"end\":64206,\"start\":64198},{\"end\":64216,\"start\":64212},{\"end\":64228,\"start\":64223},{\"end\":64241,\"start\":64236},{\"end\":64613,\"start\":64604},{\"end\":64628,\"start\":64622},{\"end\":64950,\"start\":64941},{\"end\":64965,\"start\":64959},{\"end\":65279,\"start\":65270},{\"end\":65294,\"start\":65288},{\"end\":65484,\"start\":65481},{\"end\":65497,\"start\":65489},{\"end\":65510,\"start\":65504},{\"end\":65525,\"start\":65517},{\"end\":65784,\"start\":65781},{\"end\":65795,\"start\":65790},{\"end\":65811,\"start\":65802},{\"end\":66035,\"start\":66030},{\"end\":66044,\"start\":66041},{\"end\":66056,\"start\":66050},{\"end\":66065,\"start\":66061},{\"end\":66073,\"start\":66071},{\"end\":66432,\"start\":66428},{\"end\":66444,\"start\":66441},{\"end\":66461,\"start\":66452},{\"end\":66473,\"start\":66469},{\"end\":66486,\"start\":66481},{\"end\":66504,\"start\":66497},{\"end\":66519,\"start\":66513},{\"end\":66535,\"start\":66529},{\"end\":66548,\"start\":66541},{\"end\":66565,\"start\":66561},{\"end\":66579,\"start\":66574},{\"end\":66598,\"start\":66591},{\"end\":66611,\"start\":66605},{\"end\":66625,\"start\":66618},{\"end\":66640,\"start\":66634},{\"end\":66656,\"start\":66649},{\"end\":66671,\"start\":66665},{\"end\":66692,\"start\":66686},{\"end\":66704,\"start\":66702},{\"end\":66717,\"start\":66711},{\"end\":66730,\"start\":66723},{\"end\":67674,\"start\":67670},{\"end\":67690,\"start\":67686},{\"end\":67707,\"start\":67700},{\"end\":67720,\"start\":67719},{\"end\":67725,\"start\":67721},{\"end\":67950,\"start\":67946},{\"end\":67969,\"start\":67964},{\"end\":67985,\"start\":67976},{\"end\":67994,\"start\":67990},{\"end\":68006,\"start\":68002},{\"end\":68023,\"start\":68014},{\"end\":68450,\"start\":68447},{\"end\":68461,\"start\":68460},{\"end\":68467,\"start\":68462},{\"end\":68481,\"start\":68475},{\"end\":68498,\"start\":68490},{\"end\":68515,\"start\":68506},{\"end\":68529,\"start\":68524},{\"end\":68543,\"start\":68540}]", "bib_author_last_name": "[{\"end\":58044,\"start\":58029},{\"end\":58060,\"start\":58054},{\"end\":58071,\"start\":58062},{\"end\":58326,\"start\":58320},{\"end\":58340,\"start\":58334},{\"end\":58356,\"start\":58350},{\"end\":58374,\"start\":58366},{\"end\":58386,\"start\":58382},{\"end\":58776,\"start\":58770},{\"end\":58791,\"start\":58785},{\"end\":58805,\"start\":58799},{\"end\":59411,\"start\":59407},{\"end\":59424,\"start\":59420},{\"end\":59437,\"start\":59432},{\"end\":59453,\"start\":59448},{\"end\":59718,\"start\":59711},{\"end\":59889,\"start\":59869},{\"end\":60332,\"start\":60328},{\"end\":60349,\"start\":60341},{\"end\":60481,\"start\":60477},{\"end\":60492,\"start\":60489},{\"end\":60499,\"start\":60497},{\"end\":60512,\"start\":60510},{\"end\":60523,\"start\":60519},{\"end\":60871,\"start\":60866},{\"end\":60887,\"start\":60878},{\"end\":60900,\"start\":60894},{\"end\":61209,\"start\":61202},{\"end\":61224,\"start\":61219},{\"end\":61241,\"start\":61232},{\"end\":61628,\"start\":61623},{\"end\":61653,\"start\":61644},{\"end\":61671,\"start\":61662},{\"end\":61683,\"start\":61678},{\"end\":61696,\"start\":61693},{\"end\":61713,\"start\":61707},{\"end\":61727,\"start\":61723},{\"end\":61743,\"start\":61738},{\"end\":62110,\"start\":62097},{\"end\":62126,\"start\":62119},{\"end\":62147,\"start\":62132},{\"end\":62156,\"start\":62149},{\"end\":62386,\"start\":62378},{\"end\":62398,\"start\":62388},{\"end\":62727,\"start\":62722},{\"end\":62745,\"start\":62739},{\"end\":62760,\"start\":62751},{\"end\":62774,\"start\":62769},{\"end\":63125,\"start\":63120},{\"end\":63136,\"start\":63132},{\"end\":63590,\"start\":63583},{\"end\":63603,\"start\":63594},{\"end\":63615,\"start\":63607},{\"end\":63622,\"start\":63619},{\"end\":63630,\"start\":63626},{\"end\":64210,\"start\":64207},{\"end\":64221,\"start\":64217},{\"end\":64234,\"start\":64229},{\"end\":64250,\"start\":64242},{\"end\":64620,\"start\":64614},{\"end\":64634,\"start\":64629},{\"end\":64957,\"start\":64951},{\"end\":64971,\"start\":64966},{\"end\":65286,\"start\":65280},{\"end\":65300,\"start\":65295},{\"end\":65487,\"start\":65485},{\"end\":65502,\"start\":65498},{\"end\":65515,\"start\":65511},{\"end\":65531,\"start\":65526},{\"end\":65788,\"start\":65785},{\"end\":65800,\"start\":65796},{\"end\":65815,\"start\":65812},{\"end\":66039,\"start\":66036},{\"end\":66048,\"start\":66045},{\"end\":66059,\"start\":66057},{\"end\":66069,\"start\":66066},{\"end\":66079,\"start\":66074},{\"end\":66439,\"start\":66433},{\"end\":66450,\"start\":66445},{\"end\":66467,\"start\":66462},{\"end\":66479,\"start\":66474},{\"end\":66495,\"start\":66487},{\"end\":66511,\"start\":66505},{\"end\":66527,\"start\":66520},{\"end\":66539,\"start\":66536},{\"end\":66559,\"start\":66549},{\"end\":66572,\"start\":66566},{\"end\":66589,\"start\":66580},{\"end\":66603,\"start\":66599},{\"end\":66616,\"start\":66612},{\"end\":66632,\"start\":66626},{\"end\":66647,\"start\":66641},{\"end\":66663,\"start\":66657},{\"end\":66684,\"start\":66672},{\"end\":66700,\"start\":66693},{\"end\":66709,\"start\":66705},{\"end\":66721,\"start\":66718},{\"end\":66739,\"start\":66731},{\"end\":67684,\"start\":67675},{\"end\":67698,\"start\":67691},{\"end\":67717,\"start\":67708},{\"end\":67732,\"start\":67726},{\"end\":67962,\"start\":67951},{\"end\":67974,\"start\":67970},{\"end\":67988,\"start\":67986},{\"end\":68000,\"start\":67995},{\"end\":68012,\"start\":68007},{\"end\":68035,\"start\":68024},{\"end\":68043,\"start\":68037},{\"end\":68458,\"start\":68451},{\"end\":68473,\"start\":68468},{\"end\":68488,\"start\":68482},{\"end\":68504,\"start\":68499},{\"end\":68522,\"start\":68516},{\"end\":68538,\"start\":68530},{\"end\":68553,\"start\":68544}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17577797},\"end\":58221,\"start\":57990},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12680508},\"end\":58708,\"start\":58223},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9089716},\"end\":59313,\"start\":58710},{\"attributes\":{\"doi\":\"arXiv:2002.03517\",\"id\":\"b3\"},\"end\":59667,\"start\":59315},{\"attributes\":{\"id\":\"b4\"},\"end\":59793,\"start\":59669},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":53649777},\"end\":60293,\"start\":59795},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":11387983},\"end\":60468,\"start\":60295},{\"attributes\":{\"doi\":\"arXiv:1712.05526\",\"id\":\"b7\"},\"end\":60798,\"start\":60470},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":59842968},\"end\":61125,\"start\":60800},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4009713},\"end\":61526,\"start\":61127},{\"attributes\":{\"doi\":\"arXiv:1810.12715\",\"id\":\"b10\"},\"end\":62034,\"start\":61528},{\"attributes\":{\"id\":\"b11\"},\"end\":62372,\"start\":62036},{\"attributes\":{\"id\":\"b12\"},\"end\":62713,\"start\":62374},{\"attributes\":{\"doi\":\"arXiv:2002.03239\",\"id\":\"b13\"},\"end\":63061,\"start\":62715},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13123084},\"end\":63507,\"start\":63063},{\"attributes\":{\"doi\":\"https:/doi.ieeecomputersociety.org/10.1109/SP.2019.00044\",\"id\":\"b15\",\"matched_paper_id\":49431481},\"end\":64116,\"start\":63509},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":204904840},\"end\":64517,\"start\":64118},{\"attributes\":{\"doi\":\"arXiv:1910.10783\",\"id\":\"b17\"},\"end\":64824,\"start\":64519},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":208202381},\"end\":65195,\"start\":64826},{\"attributes\":{\"doi\":\"arXiv:2002.10733\",\"id\":\"b19\"},\"end\":65479,\"start\":65197},{\"attributes\":{\"doi\":\"arXiv:1809.03113\",\"id\":\"b20\"},\"end\":65777,\"start\":65481},{\"attributes\":{\"doi\":\"arXiv:1312.4400\",\"id\":\"b21\"},\"end\":65963,\"start\":65779},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4381935},\"end\":66356,\"start\":65965},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":202786778},\"end\":67595,\"start\":66358},{\"attributes\":{\"doi\":\"arXiv:2002.03018\",\"id\":\"b24\"},\"end\":67944,\"start\":67597},{\"attributes\":{\"doi\":\"arXiv:1906.04584\",\"id\":\"b25\"},\"end\":68372,\"start\":67946},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4626477},\"end\":69209,\"start\":68374}]", "bib_title": "[{\"end\":58019,\"start\":57990},{\"end\":58309,\"start\":58223},{\"end\":58759,\"start\":58710},{\"end\":59867,\"start\":59795},{\"end\":60318,\"start\":60295},{\"end\":60857,\"start\":60800},{\"end\":61193,\"start\":61127},{\"end\":63111,\"start\":63063},{\"end\":63579,\"start\":63509},{\"end\":64196,\"start\":64118},{\"end\":64939,\"start\":64826},{\"end\":66028,\"start\":65965},{\"end\":66426,\"start\":66358},{\"end\":68445,\"start\":68374}]", "bib_author": "[{\"end\":58046,\"start\":58021},{\"end\":58062,\"start\":58046},{\"end\":58073,\"start\":58062},{\"end\":58328,\"start\":58311},{\"end\":58342,\"start\":58328},{\"end\":58358,\"start\":58342},{\"end\":58376,\"start\":58358},{\"end\":58388,\"start\":58376},{\"end\":58778,\"start\":58761},{\"end\":58793,\"start\":58778},{\"end\":58807,\"start\":58793},{\"end\":59413,\"start\":59401},{\"end\":59426,\"start\":59413},{\"end\":59439,\"start\":59426},{\"end\":59455,\"start\":59439},{\"end\":59720,\"start\":59707},{\"end\":59891,\"start\":59869},{\"end\":60334,\"start\":60320},{\"end\":60351,\"start\":60334},{\"end\":60483,\"start\":60470},{\"end\":60494,\"start\":60483},{\"end\":60501,\"start\":60494},{\"end\":60514,\"start\":60501},{\"end\":60525,\"start\":60514},{\"end\":60873,\"start\":60859},{\"end\":60889,\"start\":60873},{\"end\":60902,\"start\":60889},{\"end\":61211,\"start\":61195},{\"end\":61226,\"start\":61211},{\"end\":61243,\"start\":61226},{\"end\":61630,\"start\":61618},{\"end\":61655,\"start\":61630},{\"end\":61673,\"start\":61655},{\"end\":61685,\"start\":61673},{\"end\":61698,\"start\":61685},{\"end\":61715,\"start\":61698},{\"end\":61729,\"start\":61715},{\"end\":61745,\"start\":61729},{\"end\":62112,\"start\":62090},{\"end\":62128,\"start\":62112},{\"end\":62149,\"start\":62128},{\"end\":62158,\"start\":62149},{\"end\":62388,\"start\":62374},{\"end\":62400,\"start\":62388},{\"end\":62729,\"start\":62715},{\"end\":62747,\"start\":62729},{\"end\":62762,\"start\":62747},{\"end\":62776,\"start\":62762},{\"end\":63127,\"start\":63113},{\"end\":63138,\"start\":63127},{\"end\":63592,\"start\":63581},{\"end\":63605,\"start\":63592},{\"end\":63617,\"start\":63605},{\"end\":63624,\"start\":63617},{\"end\":63632,\"start\":63624},{\"end\":64212,\"start\":64198},{\"end\":64223,\"start\":64212},{\"end\":64236,\"start\":64223},{\"end\":64252,\"start\":64236},{\"end\":64622,\"start\":64604},{\"end\":64636,\"start\":64622},{\"end\":64959,\"start\":64941},{\"end\":64973,\"start\":64959},{\"end\":65288,\"start\":65270},{\"end\":65302,\"start\":65288},{\"end\":65489,\"start\":65481},{\"end\":65504,\"start\":65489},{\"end\":65517,\"start\":65504},{\"end\":65533,\"start\":65517},{\"end\":65790,\"start\":65781},{\"end\":65802,\"start\":65790},{\"end\":65817,\"start\":65802},{\"end\":66041,\"start\":66030},{\"end\":66050,\"start\":66041},{\"end\":66061,\"start\":66050},{\"end\":66071,\"start\":66061},{\"end\":66081,\"start\":66071},{\"end\":66441,\"start\":66428},{\"end\":66452,\"start\":66441},{\"end\":66469,\"start\":66452},{\"end\":66481,\"start\":66469},{\"end\":66497,\"start\":66481},{\"end\":66513,\"start\":66497},{\"end\":66529,\"start\":66513},{\"end\":66541,\"start\":66529},{\"end\":66561,\"start\":66541},{\"end\":66574,\"start\":66561},{\"end\":66591,\"start\":66574},{\"end\":66605,\"start\":66591},{\"end\":66618,\"start\":66605},{\"end\":66634,\"start\":66618},{\"end\":66649,\"start\":66634},{\"end\":66665,\"start\":66649},{\"end\":66686,\"start\":66665},{\"end\":66702,\"start\":66686},{\"end\":66711,\"start\":66702},{\"end\":66723,\"start\":66711},{\"end\":66741,\"start\":66723},{\"end\":67686,\"start\":67670},{\"end\":67700,\"start\":67686},{\"end\":67719,\"start\":67700},{\"end\":67734,\"start\":67719},{\"end\":67964,\"start\":67946},{\"end\":67976,\"start\":67964},{\"end\":67990,\"start\":67976},{\"end\":68002,\"start\":67990},{\"end\":68014,\"start\":68002},{\"end\":68037,\"start\":68014},{\"end\":68045,\"start\":68037},{\"end\":68460,\"start\":68447},{\"end\":68475,\"start\":68460},{\"end\":68490,\"start\":68475},{\"end\":68506,\"start\":68490},{\"end\":68524,\"start\":68506},{\"end\":68540,\"start\":68524},{\"end\":68555,\"start\":68540}]", "bib_venue": "[{\"end\":59018,\"start\":58913},{\"end\":59974,\"start\":59968},{\"end\":63210,\"start\":63196},{\"end\":63764,\"start\":63743},{\"end\":58091,\"start\":58073},{\"end\":58441,\"start\":58388},{\"end\":58911,\"start\":58807},{\"end\":59399,\"start\":59315},{\"end\":59705,\"start\":59669},{\"end\":59966,\"start\":59891},{\"end\":60368,\"start\":60351},{\"end\":60612,\"start\":60541},{\"end\":60946,\"start\":60902},{\"end\":61295,\"start\":61243},{\"end\":61616,\"start\":61528},{\"end\":62088,\"start\":62036},{\"end\":62449,\"start\":62400},{\"end\":62866,\"start\":62792},{\"end\":63194,\"start\":63138},{\"end\":63741,\"start\":63688},{\"end\":64301,\"start\":64252},{\"end\":64602,\"start\":64519},{\"end\":64996,\"start\":64973},{\"end\":65268,\"start\":65197},{\"end\":65607,\"start\":65549},{\"end\":65850,\"start\":65832},{\"end\":66150,\"start\":66081},{\"end\":66790,\"start\":66741},{\"end\":67668,\"start\":67597},{\"end\":68137,\"start\":68061},{\"end\":68604,\"start\":68555}]"}}}, "year": 2023, "month": 12, "day": 17}