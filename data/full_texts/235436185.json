{"id": 235436185, "updated": "2023-10-06 01:47:31.993", "metadata": {"title": "BEiT: BERT Pre-Training of Image Transformers", "authors": "[{\"first\":\"Hangbo\",\"last\":\"Bao\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Songhao\",\"last\":\"Piao\",\"middle\":[]},{\"first\":\"Furu\",\"last\":\"Wei\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first\"tokenize\"the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.08254", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/Bao0PW22", "doi": null}}, "content": {"source": {"pdf_hash": "722ad6ac92286507437b31486f47987d6ece05c9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2106.08254v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "dfedb8522f629e961f2e92f466b38ff6ce1a8ffa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/722ad6ac92286507437b31486f47987d6ece05c9.txt", "contents": "\nBEIT: BERT Pre-Training of Image Transformers\n\n\nHangbo Bao \nHarbin Institute of Technology \u2021 Microsoft Research\n\n\nLi Dong \nHarbin Institute of Technology \u2021 Microsoft Research\n\n\nSonghao Piao \nHarbin Institute of Technology \u2021 Microsoft Research\n\n\nFuru Wei \nHarbin Institute of Technology \u2021 Microsoft Research\n\n\nBEIT: BERT Pre-Training of Image Transformers\n\nWe introduce a self-supervised vision representation model BEIT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT [DCLT19] developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16\u00d716 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEIT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.\n\nIntroduction\n\nTransformer [VSP + 17] has achieved promising performance in computer vision [DBK + 20, TCD + 20]. However, empirical studies show that vision Transformers require more training data than convolutional neural networks. In order to solve the data-hungry issue [LSB + 21], self-supervised pre-training is a promising solution to leverage large-scale image data. Several strands of methods have been explored for vision Transformers, such as contrastive learning [CXH21,XLY + 21], and self-distillation [CTM + 21].\n\nConcurrently, BERT [DCLT19] has achieved great success in natural language processing. Its masked language modeling task first randomly masks some proportion of tokens within a text, and then recovers the masked tokens based on the Transformer encoding results of the corrupted text. Motivated by BERT, we turn to the denoising auto-encoding idea to pretrain vision Transformers, which has not been well studied by the vision community. It is challenging to directly apply BERTstyle pre-training for image data. First of all, there is no pre-exist vocabulary for vision Transformer's input unit, i.e., image patches. So we cannot simply employ a softmax classifier to predict over all possible candidates for masked patches. In contrast, the language vocabulary, such as words and BPE [SHB16], is well-defined and eases auto-encoding prediction. A straightforward alternative is regarding the task as a regression problem, which predicts the raw pixels of masked patches. However, such pixel-level recovery task tends to waste modeling capability on pre-training shortrange dependencies and high-frequency details [RPG + 21]. Our goal is to overcome the above issues for pre-training of vision Transformers.\n\nIn this work, we introduce a self-supervised vision representation model BEIT, which stands for Bidirectional Encoder representation from Image Transformers. Inspired by BERT, we propose a pre-training task, namely, masked image modeling (MIM). As shown in Figure 1 Figure 1: Overview of BEIT pre-training. Before pre-training, we learn an \"image tokenizer\" via autoencoding-style reconstruction, where an image is tokenized into discrete visual tokens according to the learned vocabulary. During pre-training, each image has two views, i.e., image patches, and visual tokens. We randomly mask some proportion of image patches (gray patches in the figure) and replace them with a special mask embedding [M]. Then the patches are fed to a backbone vision Transformer. The pre-training task aims at predicting the visual tokens of the original image based on the encoding vectors of the corrupted image.\n\nviews for each images, i.e., image patches, and visual tokens. We split the image into a grid of patches that are the input representation of backbone Transformer. Moreover, we \"tokenize\" the image to discrete visual tokens, which is obtained by the latent codes of discrete VAE [RPG + 21]. During pre-training, we randomly mask some proportion of image patches, and feed the corrupted input to Transformer. The model learns to recover the visual tokens of the original image, instead of the raw pixels of masked patches.\n\nWe perform self-supervised learning and then fine-tune the pretrained BEIT on two downstream tasks, i.e., image classification, and semantic segmentation. Experimental results indicate that BEIT outperforms both from-scratch training and previous strong self-supervised models. Moreover, BEIT is complementary to supervised pre-training. Performance of BEIT can be further improved by intermediate fine-tuning with ImageNet labels. Ablation studies show that our proposed techniques are critical to the effectiveness of BERT-style pre-training for image data. Apart from performance, the improvements of convergence speed and stability of fine-tuning reduce training costs on end tasks. In addition, we demonstrate that self-supervised BEIT can learn reasonable semantic regions via pre-training, unleashing the rich supervision signals contained in images.\n\nOur contributions are summarized as follows:\n\n\u2022 We propose a masked image modeling task to pretrain vision Transformers in a self-supervised manner. We also provide a theoretical explanation from the perspective of variational autoencoder. \u2022 We pretrain BEIT and conduct extensive fine-tuning experiments on downstream tasks, such as image classification, and semantic segmentation. \u2022 We present that the self-attention mechanism of self-supervised BEIT learns to distinguish semantic regions and object boundaries, although without using any human annotation.\n\n\nMethods\n\nGiven an input image x, BEIT encodes it to contextualized vector representations. As shown in Figure 1, BEIT is pretrained by the masked image modeling (MIM) task in a self-supervised learning manner. MIM aims at recovering the masked image patches based on encoding vectors. For downstream tasks (such as image classification, and semantic segmentation), we append task layers upon pretrained BEIT and fine-tune the parameters on the specific datasets.\n\n\nImage Representations\n\nThe images have two views of representations in our method, namely, image patch, and visual tokens.\n\nThe two types serve as input and output representations during pre-training, respectively.\n\n\nImage Patch\n\nThe 2D image is split into a sequence of patches [DBK + 20], so that a standard Transformer can directly accept image data. Formally, we reshape the image x \u2208 R H\u00d7W \u00d7C into N = HW /P 2 patches x p \u2208 R N \u00d7(P 2 C) , where C is the number of channels, (H, W ) is the input image resolution, and (P, P ) is the resolution of each patch. The image patches {x p i } N i=1 are flattened into vectors and are linearly projected, which is similar to word embeddings in BERT [DCLT19]. Image patches preserve raw pixels and are used as input features in BEIT.\n\nIn our experiments, we split each 224 \u00d7 224 image into a 14 \u00d7 14 grid of image patches, where each patch is 16 \u00d7 16.\n\n\nVisual Token\n\nSimilar to natural language, we represent the image as a sequence of discrete tokens obtained by an \"image tokenizer\", instead of raw pixels. Specifically, we tokenize the image x \u2208 R H\u00d7W \u00d7C into z = [z 1 , . . . , z N ] \u2208 V h\u00d7w , where the vocabulary V = {1, . . . , |V|} contains discrete token indices.\n\nFollowing [RPG + 21], we use the image tokenizer learned by discrete variational autoencoder (dVAE). There are two modules during visual token learning, namely, tokenizer and decoder. The tokenizer q \u03c6 (z|x) maps image pixels x into discrete tokens z according to a visual codebook (i.e., vocabulary). The decoder p \u03c8 (x|z) learns to reconstruct the input image x based on the visual tokens z. The reconstruction objective can be written as E z\u223cq \u03c6 (z|x) [log p \u03c8 (x|z)]. Because the latent visual tokens are discrete, the model training is non-differentiable. Gumbel-softmax relaxation [JGP17,MMT17] is employed to train the model parameters. Moreover, a uniform prior is put on q \u03c6 during dVAE training. Refer to [RPG + 21] for more training details of the image tokenizer.\n\nWe tokenize each image to a 14 \u00d7 14 grid of visual tokens. Notice the number of visual tokens and the number of image patches for one image are the same. The vocabulary size is set to |V| = 8192. In our work, we directly use the publicly available 2 image tokenizer described in [RPG + 21]. We also compare it with a re-implemented tokenizer in Appendix C.\n\n\nBackbone Network: Image Transformer\n\nFollowing ViT [DBK + 20], we use the standard Transformer [VSP + 17] as the backbone network. So the results can be directly compared with previous work in terms of the network architecture.\n\nThe input of Transformer is a sequence of image patches {x p i } N i=1 . The patches are then linearly projected to obtain patch embeddings Ex p i , where E \u2208 R (P 2 C)\u00d7D . Moreover, we prepend a special token [S] to the input sequence. We also add standard learnable 1D position embeddings \n\n\nPre-Training BEIT: Masked Image Modeling\n\nWe propose a masked image modeling (MIM) task. We randomly mask some percentage of image patches, and then predict the visual tokens that are corresponding to the masked patches. Figure 1 shows the overview of our method. As presented in Section 2.1, given an input image x, we split it into N image patches ({x p i } N i=1 ), and tokenize it to N visual tokens ({z i } N i=1 ). We randomly mask approximately 40% image patches, where the masked positions are denoted as M \u2208 {1, . . . , N } 0.4N . Next we replace the masked patches with a learnable embedding e [M] \u2208 R D . The corrupted image patches\nx M = {x p i : i / \u2208 M} N i=1 {e [M] : i \u2208 M} N i=1\nare then fed into the L-layer Transformer as described in Section 2.2. The final hidden vectors {h L i } N i=1 are regarded as encoded representations of the input patches. For each masked position {h L i : i \u2208 M} N i=1 , we use a softmax classifier to predict the corresponding visual tokens p MIM (z |x M ) = softmax z (W c h L i +b c ), where x M is the corrupted image, W c \u2208 R |V|\u00d7D , and b c \u2208 R |V| . The pre-training objective is to maximize the log-likelihood of the correct visual tokens z i given the corrupted image:\nmax x\u2208D E M i\u2208M log p MIM (z i |x M )(1)\nwhere D is the training corpus, M represents randomly masked positions, and x M is the corrupted image that is masked according to M.\n\n\nAlgorithm 1 Blockwise Masking\nInput: N (= h \u00d7 w) image patches Output: Masked positions M M \u2190 {} repeat s \u2190 Rand(16, 0.4N \u2212 |M|) Block size r \u2190 Rand(0.3, 1 0.3 ) Aspect ratio of block a \u2190 \u221a s \u00b7 r; b \u2190 s/r t \u2190 Rand(0, h \u2212 a) ; l \u2190 Rand(0, w \u2212 b) M \u2190 M {(i, j) : i \u2208 [t, t + a), j \u2208 [l, l + b)} until |M| > 0.4N\nMasking ratio is 40% return M Rather than randomly choosing patches for the masked positions M, we employ blockwise masking in our work. As summarized in Algorithm 1, a block of image patches is masked each time. For each block, we set the minimum number of patches to 16. Then we randomly choose an aspect ratio for the masking block. We repeat the above two steps until obtaining enough masked patches, i.e., 0.4N , where N is the total number of image patches, and 0.4 is masking ratio.\n\nThe MIM task is greatly inspired by masked language modeling [DCLT19], which is one of the most successful pre-training objective in natural language processing. Moreover, blockwise (or n-gram) masking is also widely applied in BERT-like models [JCL + 20, BDW + 20, RSR + 20]. However, directly using pixel-level auto-encoding (i.e., recovering the pixels of masked patches) for vision pretraining pushes the model to focus on short-range dependencies and high-frequency details [RPG + 21]. BEIT overcomes the above issue by predicting discrete visual tokens, which summarizes the details to high-level abstractions. Ablation studies in Section 3.3 show that our proposed method significantly outperforms pixel-level auto-encoding.\n\n\nFrom the Perspective of Variational Autoencoder\n\nThe BEIT pre-training can be viewed as variational autoencoder [KW14] training. Let x denote the original image,x the masked image, and z the visual tokens. Considering the evidence lower bound (ELBO) of the log-likelihood p(x|x), i.e., recovering the original image from its corrupted version:\n(xi,xi)\u2208D log p(x i |x i ) \u2265 (xi,xi)\u2208D E zi\u223cq \u03c6 (z|xi) [log p \u03c8 (x i |z i )] Visual Token Reconstruction \u2212D KL [q \u03c6 (z|x i ), p \u03b8 (z|x i )](2)\nwhere (1) q \u03c6 (z|x) denotes the image tokenizer that obtains visual tokens; (2) p \u03c8 (x|z) decodes the original image given input visual tokens; (3) p \u03b8 (z|x) recovers the visual tokens based on the masked image, which is our MIM pre-training task.\n\nWe learn the model following a two-stage procedure similar to [vdOVK17,RvdOV19]. In the first stage, we obtain the image tokenizer as a discrete variational autoencoder [RPG + 21]. Specifically, the first stage minimizes the reconstruction loss \u2212E zi\u223cq \u03c6 (z|xi) [log p \u03c8 (x i |z i )] with an uniform prior as described in Equation (2). In the second stage, we learn the prior p \u03b8 while keeping q \u03c6 and p \u03c8 fixed. We simplify q \u03c6 (z|x i ) to a one-point distribution with the most likely visual token\u015d z i = arg max z q \u03c6 (z|x i ). Then Equation (2) can be rewritten as:\n(xi,xi)\u2208D E zi\u223cq \u03c6 (z|xi) [log p \u03c8 (x i |z i )]\nStage 1: Visual Token Reconstruction\n+ log p \u03b8 (\u1e91 i |x i ) Stage 2: Masked Image Modeling(3)\nwhere the second term is our BEIT pre-training objective.\n\n\nPre-Training Setup\n\nThe network architecture of BEIT follows that of ViT-Base [DBK + 20] for a fair comparison. We use a 12-layer Transformer with 768 hidden size, and 12 attention heads. The intermediate size of feed-forward networks is 3072. We employ the default 16 \u00d7 16 input patch size. We directly borrow the image tokenizer trained by [RPG + 21]. The vocabulary size of visual tokens is 8192.\n\nWe pretrain BEIT on the training set of ImageNet-1K [RDS + 15], which contains about 1.2M images. Our augmentation policy includes random resized cropping, horizontal flipping, color jittering [WXYL18]. Notice that we do not use the labels for self-supervised learning. We use the 224 \u00d7 224 resolution in our experiments. So the input is split to 14 \u00d7 14 image patches, and the same amount of visual tokens. We randomly mask at most 75 patches (i.e., roughly 40% of total image patches).\n\nThe pre-training runs for about 500k steps (i.e., 800 epochs) with 2k batch size. Adam [LH19] with \u03b2 1 = 0.9, \u03b2 2 = 0.999 is employed for optimization. The learning rate is set to 1.5e-3, with a warmup of 10 epochs, and cosine learning rate decay. The weight decay is 0.05. We employ stochastic depth [HSL + 16] with a 0.1 rate, and disable dropout. The 500k training steps take about five days using 16 Nvidia Telsa V100 32GB GPU cards.\n\nWe find that proper initialization is important to stabilize Transformer, especially for large-scale pretraining. We first randomly initialize all the parameters within a small range, such as [\u22120.02, 0.02].\n\nThen, for the l-th Transformer layer, we rescale the output matrices (i.e., the last linear projection within each sub-layer) of the self-attention module and the feed-forward network by 1 \u221a 2l .\n\n\nFine-Tuning BEIT on Downstream Vision Tasks\n\nAfter pre-training BEIT, we append a task layer upon the Transformer, and fine-tune the parameters on downstream tasks, like BERT. We take image classification and semantic segmentation as examples in our work. It is straightforward to leverage the pre-training-then-fine-tuning paradigm on other vision tasks with BEIT.\n\nImage classification. For image classification tasks, we directly employ a simple linear classifier as the task layer. Specifically, we use average pooling to aggregate the representations, and feed the global to a softmax classifier. The category probabilities are computed\nas softmax(avg({h L i } N i=1 W c )),\nwhere h L i is the final encoding vector of the i-th image patch, W c \u2208 R D\u00d7C is a parameter matrix, and C is the number of labels. We maximize the likelihood of labeled data by updating the parameters of BEIT and the softmax classifier.\n\nSemantic segmentation. For semantic segmentation, we follow the task layer used in SETR-PUP [ZLZ + 20]. To be specific, we use pretrained BEIT as a backbone encoder, and incorporate several deconvolution layers as decoder to produce segmentation. The model is also end-to-end fine-tuned similar to image classification.\n\nIntermediate fine-tuning. After self-supervised pre-training, we can further train BEIT on a datarich intermediate dataset (i.e., ImageNet-1K in our work), and then finetune the model on the target downstream tasks. Such intermediate fine-tuning is the common practice of BERT fine-tuning in NLP [PPL + 20]. We directly follow the method for BEIT.\n\n\nExperiments\n\nWe conduct full fine-tuning experiments on image classification and semantic segmentation. Moreover, we present various ablation studies for pre-training and analyze the representations learned by BEIT. We also report linear probes on ImageNet in Appendix D.\n\n\nImage Classification\n\nThe image classification task classifies input images to various categories. We evaluate BEIT on the ILSVRC-2012 ImageNet dataset [RDS + 15] with 1k classes and 1.3M images. We directly follow the most of hyperparameters of DeiT [TCD + 20] in our fine-tuning experiments for a fair comparison. We reduce fine-tuning epochs compared with training from scratch, as BEIT has been pre-trained. Accordingly, we use a larger learning rate with layer-wise decay. The detailed hyperparameters are summarized in Appendix H. Table 1 reports top-1 accuracy on image classification. We compare BEIT with vision Transformers trained by random initialization, supervised pre-training, and previous self-supervised learning methods. All the compared models are base-size, except iGPT has 1.36B parameters. Pre-training is conducted on ImageNet for the comparison purpose, except ViT-JFT300M is pretrained on Google's in-house 300M images.\n\nCompared with the models trained by random initialization, we find that pre-trained BEIT significantly improves performance on both datasets. BEIT improves the performance on ImageNet, which shows the effectiveness under the rich-resource setting.\n\nMoreover, we compare BEIT with previous state-of-the-art self-supervised methods for Transformer, such as DINO [CTM + 21], and MoCo v3 [CXH21]. Our proposed method outperforms previous models on ImageNet fine-tuning. Among them, iGPT-1.36B [CRC + 20] uses much more parameters (i.e., 1.36B vs 86M), and ViT-JFT300M [DBK + 20] is pretrained on larger corpus (i.e., 300M vs 1.3M), while others pretrain ViT-Base on ImageNet-1K. iGPT-1.36B and ViT-JFT300M are the most comparable methods, which also follows auto-encoding pre-training for vision Transformer. Specifically, iGPT uses clustered image tokens as both input and output for image GPT or image BERT. In contrast, we use image patches as input to preserve raw pixels, and employ discrete visual tokens as a prediction bottleneck. ViT-JFT300 predicts the mean, 3-bit color of each masked patch, rather than visual tokens learned by discrete VAE. We also pretrain the self-supervised tasks of BEIT and DINO in a multi-task learning manner, which is presented in Appendix E.\n\nIn addition, we evaluate our proposed method with intermediate fine-tuning. In other words, we first pretrain BEIT in a self-supervised manner, and then fine-tune the pretrained model on ImageNet with labeled data. The results show that BEIT is complementary to supervised pre-training, achieving additional gain after intermediate fine-tuning on ImageNet.\n\nFine-tuning to 384 \u00d7 384 resolution. After fine-tuning with resolution 224 \u00d7 224, we additionally fine-tune the model on 384\u00d7384 images by 10 more epochs. We follow the standard higher-resolution setting of DeiT [TCD + 20], except using fewer epochs. Notice that we keep patch size the same for both 224 \u00d7 224 and 384 \u00d7 384 images. So the input sequence length of Transformers becomes longer for higher resolutions. Table 1 shows that higher resolution improves the BEIT results by 1+ points on ImageNet. More importantly, BEIT 384 pretrained on ImageNet-1K even outperforms supervised pre-training ViT 384 that uses ImageNet-22K, when they use the same input resolution.\n\nScaling up to larger size. We further scale up BEIT to the large size (same as ViT-L). As shown in Table 1, ViT 384 -L is worse than ViT 384 on ImageNet, when training from scratch. The results verifies the data-hungry issue of vision Transformers. Supervised pre-training on ImageNet-22K partially relieves the issue, where ViT 384 -L finally outperforms ViT 384 by 1.2. In comparison, BEIT-L is better than BEIT by 2.0, and BEIT 384 -L outperforms BEIT 384 by 1.7. In other words, the benefits of scaling up BEIT from base to large are greater than supervised pre-training with ImageNet-22K. More importantly, comparing between BEIT 384 with ViT 384 that conducts supervised pre-training on ImageNet-22K, the improvements of BEIT become greater along with scaling the size from base (i.e., 0.6) to large (i.e., 1.1). The results suggest that BEIT tends to help more for extremely larger models (such as 1B, or 10B), especially when labeled data are insufficient 3 to conduct supervised pre-training 4 for such large models.\n\n\nModels\n\n\nModel Size\n\nResolution ImageNet  Table 1: Top-1 accuracy on ImageNet-1K. We evaluate base-(\"-B\") and large-size (\"-L\") models at resolutions 224 \u00d7 224 and 384 \u00d7 384. \u2020 : iGPT-1.36B contains 1.36 billion parameters, while others are base-size models. \u2021 : ViT 384 -B-JFT300M is pretrained with the \"masked patch prediction\" task on Google's in-house 300M images, while others use ImageNet. Top-1 Acc.\n\nDeiT (Training from scratch) BEiT (Fine-tuning)  Convergence curves. Figure 2 compares the convergence curves of the training-from-scratch and pre-training-then-fine-tuning paradigms. We find that fine-tuning BEIT not only achieves better performance, but also converging much faster than training DeiT from scratch. Moreover, fine-tuning BEIT can reach reasonable numbers within very few epochs.\n\n\nSemantic Segmentation\n\nSemantic segmentation aims to predict a corresponding class for each pixel of the input image. We evaluate BEIT on the ADE20K benchmark [ZZP + 19] with 25K images and 150 semantic categories. We report the metric of mean Intersection of Union (mIoU) averaged over all semantic categories. As presented in Section 2.6, we directly follow the task layer and the most of hyperparameters described in SETR-PUP [ZLZ + 20]. On ADE20K, we use Adam [LH19] as the optimizer. The learning rate is set to 1e-3 with layer-wise decay similar to image classification. We conduct fine-tuning for 160K steps. The batch size is 16. The detailed hyperparameters are described in Appendix I. Table 3, we compare BEIT with supervised pre-training that relies on labeled data of ImageNet. We find that our proposed method achieves better performance than supervised pretraining, although BEIT does not require manual annotations for pre-training. Moreover, we employ \n\n\nAs shown in\n\n\nAblation Studies\n\nWe conduct ablation studies to analyze the contributions of each component in BEIT. The models are evaluated on image classification (i.e., ImageNet) and semantic segmentation (i.e., ADE20K). We set the default pre-training steps to 300 epochs for the ablation studies, which is 37.5% of the total steps used in the previous experiments. Table 4 reports the results of various model variants. First, we ablate blockwise masking by randomly sample masked positions. We find that blockwise masking is beneficial on both tasks, especially on semantic segmentation. Second, we ablate the usage of visual tokens by predicting the raw pixels of masked patches, i.e., the pre-training task becomes a pixel regression problem to recover masked patches. Our proposed masked image modeling task significantly outperforms naive pixel-level auto-encoding. Compared with the results in Table 1, the ablation result is worse than training vision Transformer from scratch on two tasks. The results indicate that the prediction of visual tokens is the key ingredient of BEIT. Third, we ablate the usage of visual tokens and blockwise masking together. We find that blockwise masking is even more helpful for pixel-level auto-encoding, which relieves the suffering of short-distance dependency. Forth, recovering all the visual tokens harms performance on downstream tasks. Fifth, we compare BEIT with different training steps. Pre-training the model longer can further improve performance on downstream tasks.\n\n\nAnalysis of Self-Attention Map\n\nWe show that the self-attention mechanism in BEIT can separate objects, even though our pre-training does not rely on any manual annotation at all. Similar properties are also observed by [CTM + 21]. The probing images are taken from the MS COCO [LMB + 14] corpus to avoid appearing in the pre-training data.\n\nAs shown in Figure 2, we plot the self-attention map for different reference points within an image. The visualizations are produced by attention scores computed via query-key product in the last layer. For each reference point, we use the corresponding patch as query, and show which patch it attends to. After pre-training, BEIT learns to distinguish semantic regions using self-attention heads, without any task-specific supervision. The property partially indicates the reason why BEIT is able to help downstream tasks. Such knowledge acquired by BEIT potentially improves the generalization ability of fine-tuned models, especially on small-scale datasets.\n\n\nRelated Work\n\nSelf-supervised visual representation learning. Various methods have been introduced over the years to pretrain vision models in a self-supervised manner. Pioneering works design clever pretext tasks, such as predicting the patch orderings [NF16], colorization [ZIE16], and predicting rotation angles [KG18]. In addition, [TLL19] propose to mask some patches within an image, and classify whether the masked patches are real or fake for each masked position. The method is similar to the Figure 2: Self-attention map for different reference points. The self-attention mechanism in BEIT is able to separate objects, although self-supervised pre-training does not use manual annotations. masked version of Jigsaw pre-training [NF16]. The recent strand of research follows contrastive paradigm [WXYL18,OLV18,HFLM + 19,BHB19,HFW + 20,CKNH20,CFGH20]. The models typically regard various data augmentations as different views of an image, and then make the representations of positive pairs similar while pushing negative pairs away. In order to obtain enough informative negative samples in contrastive learning, the methods usually rely on large memory banks [WXYL18,HFW + 20] or large batch size [CKNH20]. BYOL [GSA + 20] and SimSiam [CH20] further eliminate the requirement of negative samples, using various techniques to avoid representation collapse. Another strand of methods use clustering to organize image examples [CBJD18, ARV20, CMM + 20, LZXH21].\n\nSelf-supervised vision Transformers. Pre-training vision Transformers has received significant attention recently due to the data-hungry issue. iGPT [CRC + 20] first creates a 9-bit color palette by k-means clustering RGB pixels, and then uses the clustered tokens to represent images. Next iGPT uses the tasks of BERT and GPT to pretrain Transformers. In comparison, our proposed method uses image patches as input without losing pixel-level information. Moreover, our visual tokens are obtained by discrete VAE instead of clustering. ViT [DBK + 20] conducts a preliminary exploration with the masked patch prediction task, which predicts the 3-bit mean color of the masked patches. [DBK + 20] also report that pixel-level auto-encoding performs worse, although it is the most straightforward translation of BERT from NLP to CV. Rather than using heuristically designed pre-training tasks, our proposed model leverages visual tokens learned by discrete VAE, which not only achieves better performance but also is better theoretically motivated. Apart from masked auto-encoding, other mainstream research works use contrastive learning [CXH21,XLY + 21], and self-distillation [CTM + 21]. In comparison, BEIT can achieve several times of improvement in terms of pre-training throughput (Appendix E), and memory consumption. The advantages make BEIT appealing to scale up vision Transformers.\n\n\nConclusion\n\nWe introduce a self-supervised pre-training framework for vision Transformers, achieving strong fine-tuning results on downstream tasks, such as image classification, and semantic segmentation. We show that the proposed method is critical to make BERT-like pre-training (i.e., auto-encoding with masked input) work well for image Transformers. We also present the intriguing property of automatically acquired knowledge about semantic regions, without using any human-annotated data. In the future, we would like to scale up BEIT pre-training in terms of data size and model size. Moreover, we will conduct multimodal pre-training in a more unified way, using the similar objectives and the shared architecture for texts and images.\n\n[DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pretraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171-4186. Association for Computational Linguistics, 2019.\n\n\nA Architecture Variants of Vision Transformer\n\nWe use the standard vision Transformer (ViT) in the experiments for fair comparisons. In addition, we find that LayerScale [TCS + 21] and relative position bias [BDW + 20,RSR + 20] improve ViTs on downstream tasks. We employ the same setting as in Section 3.3 for ablation studies, which pretrains base-size models for 300 epochs on ImageNet-1K.\n\nAs shown in Table 5, both LayerScale and relative position bias improve performance on ImageNet classification and ADE20K semantic segmentation. We denote the improved architecture as BEIT + and use it for the experiments in Appendix B. We empirically notice that vanilla Transformer is the most stable when scaling up the model to billions of parameters, so we do not use LayerScale for extra-large models.\n\n\nArchitecture ImageNet ADE20K\n\nViT ( Table 5: Ablation studies of architecture variants on image classification and semantic segmentation. For ADE20K, we use UperNet [XLZ + 18] as the task layer, and report mIoU scores of single-scale inference.\n\n\nB Comparison with Large-Scale Supervised Pre-Training\n\nWe compare with state-of-the-art supervised pre-training at scale. In addition to using ImageNet-1K for fair comparisons with previous work, we pretrain BEIT on ImageNet-22K to boost performance. We employ the architecture improvements (i.e., LayerScale, and relative position bias) as described in Appendix A, which is denoted as BEIT + in Table 6 and Table 7. We follow the same pre-training setup as in Section 2.5, except we pretrain 150 epochs on ImageNet-22K. After self-supervised pre-training, we conduct intermediate fine-tuning on ImageNet-22K for 90 epochs. Moreover, we use an in-house dataset that has about 70M labeled images as a drop-in replacement of ImageNet-22K.\n\n\nModels\n\nModel Size Self-Supervised Pre-Training, and Intermediate Fine-Tuning on In-House-70M BEIT-L + (ours) 307M 70M 89.3 89.5 Table 6: Top-1 accuracy on ImageNet-1K fine-tuning. We evaluate models at resolutions 384 2 and 512 2 . As shown in Table 7, we report the fine-tuning results on the ADE20K semantic segmentation benchmark. Following Swin [LLC + 21], we use the same task layer (i.e., UperNet) and evaluate the models at the resolution 640 \u00d7 640. The BEIT-L model obtains state-of-the-art performance on ADE20K.\n\n\nModels mIoU (%) Multi-Scale mIoU (%)\n\nSupervised Pre-Training on ImageNet-22K (using labeled data) Swin-B [LLC + 21] 50.0 51.7 Swin-L [LLC + 21] 52.1 53.5\n\nSelf-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-22K BEIT-B + (ours) 53.6 54.2 BEIT-L + (ours) 56.7 57.0\n\nSelf-Supervised Pre-Training, and Intermediate Fine-Tuning on In-House-70M BEIT-L + (ours) 57.9 58.4 Table 7: Performance comparison on the ADE20K semantic segmentation. We follow Swin-L [LLC + 21] to use UperNet [XLZ + 18] as the task layer and evaluate at resolution 640 \u00d7 640.\n\n\nC Ablation Studies of Image Tokenizer\n\nFor comparison, we re-train the image tokenizer on ImageNet-1K. The reimplementation is based on https://github.com/lucidrains/DALLE-pytorch. We use the same codebook size 8K as in DALL-E [RPG + 21]. Then we plug the tokenizer into our pre-training process. We follow the same experimental setup of ablation studies as in Section 3.3. Table 8 shows that our reimplemented tokenizer obtains comparable reconstruction loss and ImageNet fine-tuning performance compared with the off-the-shelf DALL-E tokenizer.\n\n\nImage Tokenizer\n\nReconstruction Error ImageNet \n\n\nD Linear Probes on ImageNet\n\nWe evaluate linear probes on ImageNet for various pretrained vision Transformers. We compare BEIT with two main strands of work, namely discriminative and generative self-supervised learning. The first one applies discriminative learning for pre-training, such as contrastive learning [CXH21], and self distillation [CTM + 21]. The above methods typically learn to aggregate the image-level features into a global vector, which is relatively suitable for linear probing. In contrast, the second strand of methods, such as iGPT [CRC + 20] and ours, usually do not pretrain such global feature aggregation, which tends to make linear probes difficult.\n\nFollowing iGPT [CRC + 20], we use average pooling to aggregate the hidden states of each image patches, and add the probing layer at the middle layer of Transformer instead of always at the final layer. Similarly, we find that the best layer lies in 9-th layer for BEIT-B, and 14-th layer for BEIT-L. To be specific, we use AdamW [LH19] to update the linear probe layer for 50 epochs. The learning rate is 4e-3 with cosine decay. The batch size is 1024. The weight decay is set to 1e-4. We follow data augmentation used in DINO [CTM + 21], which uses random resize crops and horizontal flips augmentation during training and evaluates on central crops.\n\n\nModels\n\nModel Size Accuracy  Table 9: Linear probing accuracy on ImageNet. \" * \" denotes that iGPT-XL uses concatenation of five layers for linear probing, while others use the features of single layer.\n\nAs shown in Table 9, we evaluate linear probes on ImageNet-1K for self-supervised learning. Overall, discriminative methods perform better than generative pre-training on linear probing. Linear probes keep the Transformer parameters fixed and only update the linear layer. So the pre-training of global aggregation of image-level features is beneficial to linear probing in DINO and MoCo v3, although full fine-tuning eliminates the gap. Moreover, the results indicate that increasing the model size from base (86M) to large (304M) significantly improves accuracy for our proposed method. In contrast, the gap between base-and large-size MoCo v3 is smaller. We also find that BEIT outperforms iGPT by a large margin even using much fewer parameters.\n\n\nE Multi-Task Pre-Training with DINO\n\nWe train the pre-training tasks of BEIT and DINO [CTM + 21] together in a multi-task manner. As shown in Table 10, augmenting masked image modeling with DINO improves semantic segmentation on ADE20K, and obtains comparable results on ImageNet classification. Moreover, BEIT is more efficient in terms of pre-training speed, as DINO has two copies of Transformer parameters for self-distillation and multi-crop augmentation [CMM + 20]. For the throughput comparisons between BEIT and BEIT+DINO, we set batch size to the same. Because BEIT is also more memory-efficient, we can use larger batch size to fully utilize GPU cards, which obtains greater speedup in practice than the reported numbers.\n\n\nModels\n\nImageNet ADE20K Table 10: We train the pre-training tasks of BEIT and DINO [CTM + 21] in the way of multi-task learning. We report the performance by fine-tuning on ImageNet-1K image classification and ADE20K semantic segmentation. For ADE20K, we use SETR-PUP [ZLZ + 20] as the task layer and report the mIoU score of single-scale inference. The pre-training throughput measures the speed, where larger numbers indicate faster pre-training.\n\n\nF Image Classification on CIFAR-100\n\nIn addition to ImageNet classification, we conduct fine-tuning experiments on the CIFAR-100 [KH09] benchmark with 100 classes and 60k images. The experimental setup is the same as in Section 3.1. Table 11 reports the top-1 accuracy on CIFAR-100. Notably, on the smaller CIFAR-100 dataset, ViT trained from scratch only reaches 48.5% accuracy [CXH21]. In comparison, BEIT achieves 90.1% with the help of pre-training. The results indicate that BEIT can greatly reduce the requirement of annotation efforts. BEIT also outperforms MoCo v3. Moreover, intermediate fine-tuning on ImageNet-1K further improves the results on CIFAR-100.\n\n\nModels\n\nCIFAR-100  \n\n\nE pos \u2208 R N \u00d7D to patch embeddings. The input vectors H 0 = [e [S] , Ex p i , . . . , Ex p N ] + E pos is fed into Transformer. The encoder contains L layers of Transformer blocks H l = Transformer(H l\u22121 ), where l = 1, . . . , L. The output vectors of the last layer H L = [h L [S] , h L 1 , . . . , h L N ] are used as the encoded representations for the image patches, where h L i is the vector of the i-th image patch.\n\n\n, MIM uses two123 234 456 567 \n\n987 876 765 543 \n\n112 223 334 445 \n\n211 322 433 544 \n\n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n+ \n\nBEIT Encoder \n\nBlockwise \nMasking \n\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 10 1 1 12 13 14 15 16 \n0 \n\nFlatten \n\nTokenizer \nDecoder \n\nPosition \nEmbedding \n\nPatch \nEmbedding \n\nOriginal \nImage \n\nImage \nPatches \n\nVisual Tokens \n\n2 \nL \n3 \nL \n6 \nL \n7 \nL \n14 \nL \n\nMasked Image Modeling Head \n\nReconstructed \nImage \n\nUnused During \nPre-Training \n\n234 456 \n876 765 \n322 \n\n[S] \n[M] [M] \n[M] [M] \n[M] \n\n\n\nTable 2 :\n2Convergence curves of training DeiT from scratch and fine-tuning BEIT on ImageNet-1K.Models \nADE20K \n\nSupervised Pre-Training on ImageNet \n45.3 \n\nDINO [CTM + 21] \n44.1 \nBEIT (ours) \n45.6 \nBEIT + Intermediate Fine-Tuning (ours) \n47.7 \n\n\n\nTable 3 :\n3Results of semantic segmentation on ADE20K. We use SETR-PUP [ZLZ + 20] as the task layer and report results of single-scale inference.\n\n\nAblation studies for BEIT pre-training on image classification and semantic segmentation.intermediate fine-tuning for BEIT on ImageNet, i.e., we first fine-tune pretrained BEIT on ImageNet, and then fine-tune the model on ADE20K. The results indicate that intermediate fine-tuning further improves BEIT on semantic segmentation.Models \n\nImageNet ADE20K \n\nBEIT (300 Epochs) \n82.86 \n44.65 \n\n\u2212 Blockwise masking \n82.77 \n42.93 \n\u2212 Visual tokens (i.e., recover masked pixels) \n81.04 \n41.38 \n\u2212 Visual tokens \u2212 Blockwise masking \n80.50 \n37.09 \n+ Recover 100% visual tokens \n82.59 \n40.93 \n\u2212 Masking + Recover 100% visual tokens \n81.67 \n36.73 \n\nPretrain longer (800 epochs) \n83.19 \n45.58 \nTable 4: \n\nTable 6\n6compares BEIT with previous state-of-the-art supervised pre-training [DBK + 20, ZKHB21] \non ImageNet fine-tuning. Rather than heavily relying on extremely large-size labeled data (such as \nGoogle's in-house JFT-300M and JFT-3B), we demonstrate that BEIT pre-training can catch up with \nonly ImageNet-22k (14M). Specifically, BEIT-L fine-tuned on ImageNet-22K achieves comparable \nperformance with ViT-L trained on Google JFT-3B. Moreover, BEIT-L obtains 89.5% top-1 accuracy \non ImageNet after intermediate fine-tuning on an in-house 70M dataset. The results indicate that \nBEIT pre-training greatly reduces the required labeling efforts and advances the new state of the art \nfor large-size vision Transformers. \n\n\n\n\nTop-1 accuracy on ImageNet-1K using different image tokenizers during pre-training. For image reconstruction, we report mean absolute error of normalized RGB values. The reimplemented image tokenizer is trained on ImageNet-1K without labels.DALL-E Tokenizer [RPG + 21] 0.0856 \n82.86 \nOur reimplementation \n0.0880 \n82.70 \nTable 8: \n\n\nPre-Training ThroughputDINO (400 Epochs) \n82.8 \n44.08 \n-\nBEIT (300 Epochs) \n82.9 \n44.65 \n4.2x \nBEIT + DINO (300 Epochs) \n82.9 \n46.85 \n1.0x \n\n\n\nTraining from scratch (i.e., random initialization) ViT 384[DBK + 20]   48.5* Supervised Pre-Training on ImageNet-1K (using labeled data) ViT 384 [DBK + 20]Self-Supervised Pre-Training on ImageNet-1K (without labeled data)Self-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-1K BEIT (ours) 91.8Table 11: Top-1 accuracy of image classification on CIFAR-100. The models are at resolution 224 \u00d7 224, except ViT 384 uses 384 \u00d7 384. The results, unless otherwise indicated, are all obtained by base-size models. *: result is taken from[CXH21].G Hyperparameters for Pre-Training87.1 \nDeiT [TCD + 20] \n90.8 \n\nDINO [CTM + 21] \n91.7 \nMoCo v3 [CXH21] \n87.1 \nBEIT (ours) \n90.1 \n\nHyperparameters \nBase Size Large Size \n\nLayers \n12 \n24 \nHidden size \n768 \n1024 \nFFN inner hidden size \n3072 \n4096 \nAttention heads \n12 \n16 \nAttention head size \n64 \nPatch size \n16 \u00d7 16 \n\nTraining epochs \n800 \nBatch size \n2048 \nAdam \n1e-8 \nAdam \u03b2 \n(0.9, 0.999) \nPeak learning rate \n1.5e-3 \nMinimal learning rate \n1e-5 \nLearning rate schedule \nCosine \nWarmup epochs \n10 \n\nGradient clipping \n3.0 \n1.0 \nDropout \n\nStoch. depth \n0.1 \nWeight decay \n0.05 \n\nData Augment \nRandomResizeAndCrop \nInput resolution \n224 \u00d7 224 \nColor jitter \n0.4 \n\n\n\nTable 12 :\n12Hyperparameters for pre-training BEIT on ImageNet-1K.H Hyperparameters for Image Classification Fine-TuningTable 13: Hyperparameters for fine-tuning BEIT on ImageNet-1K and CIFAR-100.I Hyperparameters for ADE20K Semantic Segmentation Fine-TuningPosition embedding interpolate bilinear Table 14: Hyperparameters for fine-tuning BEIT on ADE20K.Hyperparameters \nCIFAR-100 \nImageNet-1K \nBase Size \nBase Size Large Size \nPeak learning rate \n{2e-3, 3e-3, 4e-3, 5e-3} \nFine-tuning epochs \n150 \n100 \n50 \nBatch size \n512 \n1024 \n1024 \nWarmup epochs \n20 \n20 \n5 \nLayer-wise learning rate decay \n0.65 \n0.65 \n0.75 \nAdam \n1e-8 \nAdam \u03b2 \n(0.9, 0.999) \nMinimal learning rate \n1e-6 \nLearning rate schedule \nCosine \n\nRepeated Aug \n\n\n\nWeight decay \n0.3 \n0.05 \n0.05 \nLabel smoothing \u03b5 \n0.1 \nStoch. depth \n0.1 \nDropout \n\nGradient clipping \n\n\nErasing prob. \n\n0.25 \n0.25 \nInput resolution \n224 \u00d7 224 \nRand Augment \n9/0.5 \nMixup prob. \n0.8 \nCutmix prob. \n1.0 \nHyperparameters \nBase Size \n\nPeak learning rate \n1e-3 \nFine-tuning steps \n160K \nBatch size \n16 \nAdam \n1e-8 \nAdam \u03b2 \n(0.9, 0.999) \nLayer-wise learning rate decay \n0.65 \nMinimal learning rate \n0 \nLearning rate schedule \nLinear \nWarmup steps \n1500 \n\nDropout \n\nStoch. depth \n0.1 \nWeight decay \n0.05 \n\nInput resolution \n512 \u00d7 512 \n\nhttps://github.com/openai/DALL-E\n[ZKHB21] report that supervised pre-training of a 1.8B-size vision Transformer requires billions of labeled images.4  Appendix B shows that BEIT fine-tuned on ImageNet-22K (14M) can match the performance of supervised pre-training on Google's in-house JFT-3B[ZKHB21], while using 214x less labels. We also demonstrate that large-size BEIT fine-tuned on 70M labeled images can achieve 89.5% top-1 accuracy on ImageNet and 58.4% mIoU on ADE20K, creating new state-of-the-art results for large-size vision Transformers.\nAcknowledgement We would like to acknowledge Yue Cao, Han Hu, Hang Hua, Jingdong Wang, Zheng Zhang for the helpful discussions, and Yaru Hao for some analysis experiments using[HDWX20].\nSelf-labelling via simultaneous clustering and representation learning. Yuki M Asano, Christian Rupprecht, Andrea Vedaldi, International Conference on Learning Representations (ICLR). 2020Yuki M. Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simulta- neous clustering and representation learning. In International Conference on Learning Representations (ICLR), 2020.\n\nUniLMv2: Pseudomasked language models for unified language model pre-training. + 20] Hangbo, Li Bao, Furu Dong, Wenhui Wei, Nan Wang, Xiaodong Yang, Yu Liu, Jianfeng Wang, Songhao Gao, Ming Piao, Hsiao-Wuen Zhou, Hon, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020+ 20] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo- masked language models for unified language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, volume 119 of Proceedings of Machine Learning Research, pages 642-652. PMLR, 2020.\n\nLearning representations by maximizing mutual information across views. Philip Bachman, Devon Hjelm, William Buchwalter, Advances in Neural Information Processing Systems. Curran Associates, Inc32Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nDeep clustering for unsupervised learning of visual features. Mathilde Caron, Piotr Bojanowski, Armand Joulin, Matthijs Douze, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clus- tering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pages 132-149, 2018.\n\nImproved baselines with momentum contrastive learning. Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He, arXiv:2003.04297preprintXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. preprint arXiv:2003.04297, 2020.\n\nExploring simple siamese representation learning. Xinlei Chen, Kaiming He, arXiv:2011.10566preprintXinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566, 2020.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, arXiv:2002.05709preprintTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. preprint arXiv:2002.05709, 2020.\n\nUnsupervised learning of visual features by contrasting cluster assignments. Ishan Cmm + 20] Mathilde Caron, Julien Misra, Priya Mairal, Piotr Goyal, Armand Bojanowski, Joulin, Advances in Neural Information Processing Systems. Curran Associates, Inc33CMM + 20] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster as- signments. In Advances in Neural Information Processing Systems, volume 33, pages 9912-9924. Curran Associates, Inc., 2020.\n\nGenerative pretraining from pixels. Alec Crc + 20] Mark Chen, Rewon Radford, Jeffrey Child, Heewoo Wu, David Jun, Ilya Luan, Sutskever, PMLRProceedings of the 37th International Conference on Machine Learning. Hal Daum\u00e9 III and Aarti Singhthe 37th International Conference on Machine Learning119CRC + 20] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1691-1703. PMLR, 13-18 Jul 2020.\n\nEmerging properties in self-supervised vision transformers. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin, arXiv:2104.14294arXiv preprintCTM + 21[CTM + 21] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bo- janowski, and Armand Joulin. Emerging properties in self-supervised vision transform- ers. arXiv preprint arXiv:2104.14294, 2021.\n\nAn empirical study of training selfsupervised vision transformers. Xinlei Chen, Saining Xie, Kaiming He, abs/2104.02057ArXiv. Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self- supervised vision transformers. ArXiv, abs/2104.02057, 2021.\n\nSylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Dbk + 20 ; Alexey, Lucas Dosovitskiy, Alexander Beyer, Dirk Kolesnikov, Xiaohua Weissenborn, Thomas Zhai, Mostafa Unterthiner, Matthias Dehghani, Georg Minderer, Heigold, arXiv:2010.11929preprintDBK + 20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929, 2020.\n\nBootstrap your own latent: A new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, H Pierre, Elena Richemond, Carl Buchatskaya, Bernardo Doersch, Zhaohan Daniel Avila Pires, Mohammad Gheshlaghi Guo, Bilal Azar, Koray Piot, R\u00e9mi Kavukcuoglu, Michal Munos, Valko, GSA + 20. NeurIPSGSA + 20] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham- mad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020.\n\nSelf-attention attribution: Interpreting information interactions inside Transformer. Yaru Hao, Li Dong, Furu Wei, Ke Xu, arXiv:2004.11207arXiv preprintYaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside Transformer. arXiv preprint arXiv:2004.11207, 2020.\n\nLearning deep representations by mutual information estimation and maximization. Hflm + 19 ; R Devon, Alex Hjelm, Samuel Fedorov, Karan Lavoie-Marchildon, Phil Grewal, Adam Bachman, Yoshua Trischler, Bengio, International Conference on Learning Representations. HFLM + 19] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bach- man, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019.\n\nMomentum contrast for unsupervised visual representation learning. Haoqi + 20] Kaiming He, Yuxin Fan, Saining Wu, Ross Xie, Girshick, CVPR. + 20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\n\nDeep networks with stochastic depth. + 16] Gao, Yu Huang, Zhuang Sun, Daniel Liu, Kilian Q Sedra, Weinberger, Computer Vision -ECCV 2016. Bastian Leibe, Jiri Matas, Nicu Sebe, and Max WellingChamSpringer International Publishing+ 16] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision -ECCV 2016, pages 646-661, Cham, 2016. Springer International Publishing.\n\nSpanBERT: Improving pre-training by representing and predicting spans. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, Omer Levy, Transactions of the Association for Computational Linguistics. 8JCL + 20[JCL + 20] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64-77, 2020.\n\nCategorical reparameterization with gumbelsoftmax. Eric Jang, Shixiang Gu, Ben Poole, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel- softmax. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\nUnsupervised representation learning by predicting image rotations. Nikos Komodakis, Spyros Gidaris, International Conference on Learning Representations (ICLR). Nikos Komodakis and Spyros Gidaris. Unsupervised representation learning by pre- dicting image rotations. In International Conference on Learning Representations (ICLR), 2018.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, Department of Computer Science, University of TorontoMaster's thesisA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master's thesis, Department of Computer Science, University of Toronto, 2009.\n\nAuto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, 2nd International Conference on Learning Representations. Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, 2014.\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Interna- tional Conference on Learning Representations, 2019.\n\n+ 21] Ze, Yutong Liu, Yue Lin, Han Cao, Yixuan Hu, Zheng Wei, Stephen Zhang, Baining Lin, Guo, arXiv:2103.14030Swin Transformer: Hierarchical vision transformer using shifted windows. arXiv preprint+ 21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. Springer14+ 14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.\n\nEfficient training of visual transformers with small datasets. Lsb + 21] Yahui, Enver Liu, Wei Sangineto, Nicu Bi, Bruno Sebe, Marco De Lepri, Nadai, Thirty-Fifth Conference on Neural Information Processing Systems. LSB + 21] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, and Marco De Nadai. Efficient training of visual transformers with small datasets. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\n\nPrototypical contrastive learning of unsupervised representations. Junnan Li, Pan Zhou, Caiming Xiong, Steven Hoi, International Conference on Learning Representations. Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Prototypical contrastive learning of unsupervised representations. In International Conference on Learning Representations, 2021.\n\nThe Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. Chris J Maddison, Andriy Mnih, Yee Whye Teh, International Conference on Learning Representations. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. In International Conference on Learning Representations, 2017.\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. Mehdi Noroozi, Paolo Favaro, European conference on computer vision. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European conference on computer vision, pages 69-84.\n\n. Springer, Springer, 2016.\n\nRepresentation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. preprint arXiv:1807.03748, 2018.\n\nIntermediate-task transfer learning with pretrained language models: When and why does it work?. Jason + 20] Yada Pruksachatkun, Haokun Phang, Liu, Xiaoyi Phu Mon Htut, Richard Yuanzhe Zhang, Clara Pang, Katharina Vania, Samuel R Kann, Bowman, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics+ 20] Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, July 2020.\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, ; A Ramesh, Mikhail Pavlov, Gabriel Goh, abs/2102.12092ArXiv. Zero-shot text-to-image generation[RDS + 15] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. IJCV, 2015. [RPG + 21] A. Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad- ford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 2167RSR + 20RSR + 20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67, 2020.\n\nGenerating diverse high-fidelity images with VQ-VAE-2. Ali Razavi, Aaron Van Den Oord, Oriol Vinyals, Advances in Neural Information Processing Systems. Curran Associates, Inc32Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nNeural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics1Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics.\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, arXiv:2012.12877Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. TCD + 20. preprint[TCD + 20] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay- rolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. preprint arXiv:2012.12877, 2020.\n\nMatthieu Tcs + 21] Hugo Touvron, Cord, arXiv:2103.17239Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. arXiv preprintTCS + 21] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.\n\nSelfie: Self-supervised pretraining for image embedding. H Trieu, Minh-Thang Trinh, Quoc V Luong, Le, arXiv:1906.02940arXiv preprintTrieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selfie: Self-supervised pretraining for image embedding. arXiv preprint arXiv:1906.02940, 2019.\n\nNeural discrete representation learning. Aaron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. the 31st International Conference on Neural Information Processing Systems, NIPS'17Red Hook, NY, USACurran Associates IncAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete repre- sentation learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 6309-6318, Red Hook, NY, USA, 2017. Curran Associates Inc.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USAVSP + 17[VSP + 17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008, 2017.\n\nUnsupervised feature learning via non-parametric instance discrimination. Zhirong Wu, Yuanjun Xiong, X Stella, Dahua Yu, Lin, CVPR. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, 2018.\n\nSelf-supervised learning with swin transformers. Yutong + 21] Zhenda Xie, Zhuliang Lin, Zheng Yao, Qi Zhang, Yue Dai, Han Cao, Hu, arXiv:2105.04553arXiv preprint+ 21] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-supervised learning with swin transformers. arXiv preprint arXiv:2105.04553, 2021.\n\nUnified perceptual parsing for scene understanding. Yingcheng Xlz + 18] Tete Xiao, Bolei Liu, Yuning Zhou, Jian Jiang, Sun, ECCV. XLZ + 18] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018.\n\nColorful image colorization. Richard Zhang, Phillip Isola, Alexei A Efros, ECCV. Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\n\nScaling vision transformers. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer, arXiv:2106.04560arXiv preprintXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. arXiv preprint arXiv:2106.04560, 2021.\n\nRethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Jiachen Zlz + 20] Sixiao Zheng, Hengshuang Lu, Xiatian Zhao, Zekun Zhu, Yabiao Luo, Yanwei Wang, Jianfeng Fu, Tao Feng, Xiang, H S Philip, Li Torr, Zhang, abs/2012.15840CoRRZLZ + 20] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. CoRR, abs/2012.15840, 2020.\n\nSemantic understanding of scenes through the ADE20K dataset. Zzp + 19] Bolei, Hang Zhou, Xavier Zhao, Tete Puig, Sanja Xiao, Adela Fidler, Antonio Barriuso, Torralba, Int. J. Comput. Vis. 1273ZZP + 19] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ADE20K dataset. Int. J. Comput. Vis., 127(3):302-321, 2019.\n", "annotations": {"author": "[{\"end\":114,\"start\":49},{\"end\":177,\"start\":115},{\"end\":245,\"start\":178},{\"end\":309,\"start\":246},{\"end\":114,\"start\":49},{\"end\":177,\"start\":115},{\"end\":245,\"start\":178},{\"end\":309,\"start\":246}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":56},{\"end\":122,\"start\":118},{\"end\":190,\"start\":186},{\"end\":254,\"start\":251},{\"end\":59,\"start\":56},{\"end\":122,\"start\":118},{\"end\":190,\"start\":186},{\"end\":254,\"start\":251}]", "author_first_name": "[{\"end\":55,\"start\":49},{\"end\":117,\"start\":115},{\"end\":185,\"start\":178},{\"end\":250,\"start\":246},{\"end\":55,\"start\":49},{\"end\":117,\"start\":115},{\"end\":185,\"start\":178},{\"end\":250,\"start\":246}]", "author_affiliation": "[{\"end\":113,\"start\":61},{\"end\":176,\"start\":124},{\"end\":244,\"start\":192},{\"end\":308,\"start\":256},{\"end\":113,\"start\":61},{\"end\":176,\"start\":124},{\"end\":244,\"start\":192},{\"end\":308,\"start\":256}]", "title": "[{\"end\":46,\"start\":1},{\"end\":355,\"start\":310},{\"end\":46,\"start\":1},{\"end\":355,\"start\":310}]", "venue": null, "abstract": "[{\"end\":1343,\"start\":357},{\"end\":1343,\"start\":357}]", "bib_ref": "[{\"end\":1628,\"start\":1618},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1826,\"start\":1819},{\"end\":1835,\"start\":1826},{\"end\":1869,\"start\":1859},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2664,\"start\":2657},{\"end\":2996,\"start\":2986},{\"end\":3787,\"start\":3784},{\"end\":4273,\"start\":4263},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8208,\"start\":8201},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8214,\"start\":8208},{\"end\":8680,\"start\":8670},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12328,\"start\":12322},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13017,\"start\":13008},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13025,\"start\":13017},{\"end\":14069,\"start\":14059},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14319,\"start\":14311},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14700,\"start\":14694},{\"end\":14914,\"start\":14908},{\"end\":16998,\"start\":16988},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18654,\"start\":18647},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22878,\"start\":22872},{\"end\":25138,\"start\":25128},{\"end\":25196,\"start\":25186},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26174,\"start\":26168},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26196,\"start\":26189},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26235,\"start\":26229},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26257,\"start\":26250},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26658,\"start\":26652},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26727,\"start\":26719},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26733,\"start\":26727},{\"end\":26743,\"start\":26733},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26749,\"start\":26743},{\"end\":26758,\"start\":26749},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26765,\"start\":26758},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26772,\"start\":26765},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27091,\"start\":27083},{\"end\":27100,\"start\":27091},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27129,\"start\":27121},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27165,\"start\":27159},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28527,\"start\":28520},{\"end\":28536,\"start\":28527},{\"end\":30112,\"start\":30102},{\"end\":30121,\"start\":30112},{\"end\":32044,\"start\":32034},{\"end\":32325,\"start\":32315},{\"end\":32353,\"start\":32343},{\"end\":32690,\"start\":32680},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33695,\"start\":33688},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34390,\"start\":34384},{\"end\":36135,\"start\":36125},{\"end\":36492,\"start\":36482},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":36985,\"start\":36979},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37236,\"start\":37229},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":41329,\"start\":41322},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43310,\"start\":43302},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43568,\"start\":43560},{\"end\":1628,\"start\":1618},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1826,\"start\":1819},{\"end\":1835,\"start\":1826},{\"end\":1869,\"start\":1859},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2664,\"start\":2657},{\"end\":2996,\"start\":2986},{\"end\":3787,\"start\":3784},{\"end\":4273,\"start\":4263},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8208,\"start\":8201},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8214,\"start\":8208},{\"end\":8680,\"start\":8670},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12328,\"start\":12322},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13017,\"start\":13008},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13025,\"start\":13017},{\"end\":14069,\"start\":14059},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14319,\"start\":14311},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14700,\"start\":14694},{\"end\":14914,\"start\":14908},{\"end\":16998,\"start\":16988},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18654,\"start\":18647},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22878,\"start\":22872},{\"end\":25138,\"start\":25128},{\"end\":25196,\"start\":25186},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26174,\"start\":26168},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26196,\"start\":26189},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26235,\"start\":26229},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26257,\"start\":26250},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26658,\"start\":26652},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26727,\"start\":26719},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26733,\"start\":26727},{\"end\":26743,\"start\":26733},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26749,\"start\":26743},{\"end\":26758,\"start\":26749},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26765,\"start\":26758},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26772,\"start\":26765},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27091,\"start\":27083},{\"end\":27100,\"start\":27091},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27129,\"start\":27121},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27165,\"start\":27159},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28527,\"start\":28520},{\"end\":28536,\"start\":28527},{\"end\":30112,\"start\":30102},{\"end\":30121,\"start\":30112},{\"end\":32044,\"start\":32034},{\"end\":32325,\"start\":32315},{\"end\":32353,\"start\":32343},{\"end\":32690,\"start\":32680},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33695,\"start\":33688},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34390,\"start\":34384},{\"end\":36135,\"start\":36125},{\"end\":36492,\"start\":36482},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":36985,\"start\":36979},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37236,\"start\":37229},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":41329,\"start\":41322},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43310,\"start\":43302},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43568,\"start\":43560}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37963,\"start\":37539},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38483,\"start\":37964},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38731,\"start\":38484},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38878,\"start\":38732},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39569,\"start\":38879},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":40295,\"start\":39570},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":40628,\"start\":40296},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":40771,\"start\":40629},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":41993,\"start\":40772},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":43268,\"start\":41994},{\"attributes\":{\"id\":\"fig_0\"},\"end\":37963,\"start\":37539},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38483,\"start\":37964},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38731,\"start\":38484},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38878,\"start\":38732},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39569,\"start\":38879},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":40295,\"start\":39570},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":40628,\"start\":40296},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":40771,\"start\":40629},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":41993,\"start\":40772},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":43268,\"start\":41994}]", "paragraph": "[{\"end\":1870,\"start\":1359},{\"end\":3079,\"start\":1872},{\"end\":3982,\"start\":3081},{\"end\":4505,\"start\":3984},{\"end\":5364,\"start\":4507},{\"end\":5410,\"start\":5366},{\"end\":5926,\"start\":5412},{\"end\":6391,\"start\":5938},{\"end\":6516,\"start\":6417},{\"end\":6608,\"start\":6518},{\"end\":7172,\"start\":6624},{\"end\":7290,\"start\":7174},{\"end\":7612,\"start\":7307},{\"end\":8389,\"start\":7614},{\"end\":8747,\"start\":8391},{\"end\":8977,\"start\":8787},{\"end\":9270,\"start\":8979},{\"end\":9916,\"start\":9315},{\"end\":10497,\"start\":9969},{\"end\":10672,\"start\":10539},{\"end\":11474,\"start\":10985},{\"end\":12207,\"start\":11476},{\"end\":12553,\"start\":12259},{\"end\":12944,\"start\":12697},{\"end\":13515,\"start\":12946},{\"end\":13600,\"start\":13564},{\"end\":13714,\"start\":13657},{\"end\":14116,\"start\":13737},{\"end\":14605,\"start\":14118},{\"end\":15044,\"start\":14607},{\"end\":15252,\"start\":15046},{\"end\":15449,\"start\":15254},{\"end\":15817,\"start\":15497},{\"end\":16093,\"start\":15819},{\"end\":16369,\"start\":16132},{\"end\":16690,\"start\":16371},{\"end\":17039,\"start\":16692},{\"end\":17313,\"start\":17055},{\"end\":18261,\"start\":17338},{\"end\":18510,\"start\":18263},{\"end\":19539,\"start\":18512},{\"end\":19897,\"start\":19541},{\"end\":20570,\"start\":19899},{\"end\":21597,\"start\":20572},{\"end\":22007,\"start\":21621},{\"end\":22405,\"start\":22009},{\"end\":23377,\"start\":22431},{\"end\":24905,\"start\":23412},{\"end\":25248,\"start\":24940},{\"end\":25911,\"start\":25250},{\"end\":27382,\"start\":25928},{\"end\":28774,\"start\":27384},{\"end\":29521,\"start\":28789},{\"end\":29891,\"start\":29523},{\"end\":30286,\"start\":29941},{\"end\":30695,\"start\":30288},{\"end\":30942,\"start\":30728},{\"end\":31681,\"start\":31000},{\"end\":32206,\"start\":31692},{\"end\":32363,\"start\":32247},{\"end\":32491,\"start\":32365},{\"end\":32772,\"start\":32493},{\"end\":33321,\"start\":32814},{\"end\":33371,\"start\":33341},{\"end\":34052,\"start\":33403},{\"end\":34706,\"start\":34054},{\"end\":34911,\"start\":34717},{\"end\":35662,\"start\":34913},{\"end\":36396,\"start\":35702},{\"end\":36847,\"start\":36407},{\"end\":37516,\"start\":36887},{\"end\":37538,\"start\":37527},{\"end\":1870,\"start\":1359},{\"end\":3079,\"start\":1872},{\"end\":3982,\"start\":3081},{\"end\":4505,\"start\":3984},{\"end\":5364,\"start\":4507},{\"end\":5410,\"start\":5366},{\"end\":5926,\"start\":5412},{\"end\":6391,\"start\":5938},{\"end\":6516,\"start\":6417},{\"end\":6608,\"start\":6518},{\"end\":7172,\"start\":6624},{\"end\":7290,\"start\":7174},{\"end\":7612,\"start\":7307},{\"end\":8389,\"start\":7614},{\"end\":8747,\"start\":8391},{\"end\":8977,\"start\":8787},{\"end\":9270,\"start\":8979},{\"end\":9916,\"start\":9315},{\"end\":10497,\"start\":9969},{\"end\":10672,\"start\":10539},{\"end\":11474,\"start\":10985},{\"end\":12207,\"start\":11476},{\"end\":12553,\"start\":12259},{\"end\":12944,\"start\":12697},{\"end\":13515,\"start\":12946},{\"end\":13600,\"start\":13564},{\"end\":13714,\"start\":13657},{\"end\":14116,\"start\":13737},{\"end\":14605,\"start\":14118},{\"end\":15044,\"start\":14607},{\"end\":15252,\"start\":15046},{\"end\":15449,\"start\":15254},{\"end\":15817,\"start\":15497},{\"end\":16093,\"start\":15819},{\"end\":16369,\"start\":16132},{\"end\":16690,\"start\":16371},{\"end\":17039,\"start\":16692},{\"end\":17313,\"start\":17055},{\"end\":18261,\"start\":17338},{\"end\":18510,\"start\":18263},{\"end\":19539,\"start\":18512},{\"end\":19897,\"start\":19541},{\"end\":20570,\"start\":19899},{\"end\":21597,\"start\":20572},{\"end\":22007,\"start\":21621},{\"end\":22405,\"start\":22009},{\"end\":23377,\"start\":22431},{\"end\":24905,\"start\":23412},{\"end\":25248,\"start\":24940},{\"end\":25911,\"start\":25250},{\"end\":27382,\"start\":25928},{\"end\":28774,\"start\":27384},{\"end\":29521,\"start\":28789},{\"end\":29891,\"start\":29523},{\"end\":30286,\"start\":29941},{\"end\":30695,\"start\":30288},{\"end\":30942,\"start\":30728},{\"end\":31681,\"start\":31000},{\"end\":32206,\"start\":31692},{\"end\":32363,\"start\":32247},{\"end\":32491,\"start\":32365},{\"end\":32772,\"start\":32493},{\"end\":33321,\"start\":32814},{\"end\":33371,\"start\":33341},{\"end\":34052,\"start\":33403},{\"end\":34706,\"start\":34054},{\"end\":34911,\"start\":34717},{\"end\":35662,\"start\":34913},{\"end\":36396,\"start\":35702},{\"end\":36847,\"start\":36407},{\"end\":37516,\"start\":36887},{\"end\":37538,\"start\":37527}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9968,\"start\":9917},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10538,\"start\":10498},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10984,\"start\":10705},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12696,\"start\":12554},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13563,\"start\":13516},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13656,\"start\":13601},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16131,\"start\":16094},{\"attributes\":{\"id\":\"formula_0\"},\"end\":9968,\"start\":9917},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10538,\"start\":10498},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10984,\"start\":10705},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12696,\"start\":12554},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13563,\"start\":13516},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13656,\"start\":13601},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16131,\"start\":16094}]", "table_ref": "[{\"end\":17860,\"start\":17853},{\"end\":20322,\"start\":20315},{\"end\":20678,\"start\":20671},{\"end\":21649,\"start\":21642},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23111,\"start\":23104},{\"end\":23757,\"start\":23750},{\"end\":24292,\"start\":24285},{\"end\":30307,\"start\":30300},{\"end\":30741,\"start\":30734},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31360,\"start\":31341},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31820,\"start\":31813},{\"end\":31936,\"start\":31929},{\"end\":32601,\"start\":32594},{\"end\":33156,\"start\":33149},{\"end\":34745,\"start\":34738},{\"end\":34932,\"start\":34925},{\"end\":35815,\"start\":35807},{\"end\":36431,\"start\":36423},{\"end\":37091,\"start\":37083},{\"end\":17860,\"start\":17853},{\"end\":20322,\"start\":20315},{\"end\":20678,\"start\":20671},{\"end\":21649,\"start\":21642},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23111,\"start\":23104},{\"end\":23757,\"start\":23750},{\"end\":24292,\"start\":24285},{\"end\":30307,\"start\":30300},{\"end\":30741,\"start\":30734},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31360,\"start\":31341},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31820,\"start\":31813},{\"end\":31936,\"start\":31929},{\"end\":32601,\"start\":32594},{\"end\":33156,\"start\":33149},{\"end\":34745,\"start\":34738},{\"end\":34932,\"start\":34925},{\"end\":35815,\"start\":35807},{\"end\":36431,\"start\":36423},{\"end\":37091,\"start\":37083}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1357,\"start\":1345},{\"attributes\":{\"n\":\"2\"},\"end\":5936,\"start\":5929},{\"attributes\":{\"n\":\"2.1\"},\"end\":6415,\"start\":6394},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":6622,\"start\":6611},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":7305,\"start\":7293},{\"attributes\":{\"n\":\"2.2\"},\"end\":8785,\"start\":8750},{\"attributes\":{\"n\":\"2.3\"},\"end\":9313,\"start\":9273},{\"end\":10704,\"start\":10675},{\"attributes\":{\"n\":\"2.4\"},\"end\":12257,\"start\":12210},{\"attributes\":{\"n\":\"2.5\"},\"end\":13735,\"start\":13717},{\"attributes\":{\"n\":\"2.6\"},\"end\":15495,\"start\":15452},{\"attributes\":{\"n\":\"3\"},\"end\":17053,\"start\":17042},{\"attributes\":{\"n\":\"3.1\"},\"end\":17336,\"start\":17316},{\"end\":21606,\"start\":21600},{\"end\":21619,\"start\":21609},{\"attributes\":{\"n\":\"3.2\"},\"end\":22429,\"start\":22408},{\"end\":23391,\"start\":23380},{\"attributes\":{\"n\":\"3.3\"},\"end\":23410,\"start\":23394},{\"attributes\":{\"n\":\"3.4\"},\"end\":24938,\"start\":24908},{\"attributes\":{\"n\":\"4\"},\"end\":25926,\"start\":25914},{\"attributes\":{\"n\":\"5\"},\"end\":28787,\"start\":28777},{\"end\":29939,\"start\":29894},{\"end\":30726,\"start\":30698},{\"end\":30998,\"start\":30945},{\"end\":31690,\"start\":31684},{\"end\":32245,\"start\":32209},{\"end\":32812,\"start\":32775},{\"end\":33339,\"start\":33324},{\"end\":33401,\"start\":33374},{\"end\":34715,\"start\":34709},{\"end\":35700,\"start\":35665},{\"end\":36405,\"start\":36399},{\"end\":36885,\"start\":36850},{\"end\":37525,\"start\":37519},{\"end\":38494,\"start\":38485},{\"end\":38742,\"start\":38733},{\"end\":39578,\"start\":39571},{\"end\":42005,\"start\":41995},{\"attributes\":{\"n\":\"1\"},\"end\":1357,\"start\":1345},{\"attributes\":{\"n\":\"2\"},\"end\":5936,\"start\":5929},{\"attributes\":{\"n\":\"2.1\"},\"end\":6415,\"start\":6394},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":6622,\"start\":6611},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":7305,\"start\":7293},{\"attributes\":{\"n\":\"2.2\"},\"end\":8785,\"start\":8750},{\"attributes\":{\"n\":\"2.3\"},\"end\":9313,\"start\":9273},{\"end\":10704,\"start\":10675},{\"attributes\":{\"n\":\"2.4\"},\"end\":12257,\"start\":12210},{\"attributes\":{\"n\":\"2.5\"},\"end\":13735,\"start\":13717},{\"attributes\":{\"n\":\"2.6\"},\"end\":15495,\"start\":15452},{\"attributes\":{\"n\":\"3\"},\"end\":17053,\"start\":17042},{\"attributes\":{\"n\":\"3.1\"},\"end\":17336,\"start\":17316},{\"end\":21606,\"start\":21600},{\"end\":21619,\"start\":21609},{\"attributes\":{\"n\":\"3.2\"},\"end\":22429,\"start\":22408},{\"end\":23391,\"start\":23380},{\"attributes\":{\"n\":\"3.3\"},\"end\":23410,\"start\":23394},{\"attributes\":{\"n\":\"3.4\"},\"end\":24938,\"start\":24908},{\"attributes\":{\"n\":\"4\"},\"end\":25926,\"start\":25914},{\"attributes\":{\"n\":\"5\"},\"end\":28787,\"start\":28777},{\"end\":29939,\"start\":29894},{\"end\":30726,\"start\":30698},{\"end\":30998,\"start\":30945},{\"end\":31690,\"start\":31684},{\"end\":32245,\"start\":32209},{\"end\":32812,\"start\":32775},{\"end\":33339,\"start\":33324},{\"end\":33401,\"start\":33374},{\"end\":34715,\"start\":34709},{\"end\":35700,\"start\":35665},{\"end\":36405,\"start\":36399},{\"end\":36885,\"start\":36850},{\"end\":37525,\"start\":37519},{\"end\":38494,\"start\":38485},{\"end\":38742,\"start\":38733},{\"end\":39578,\"start\":39571},{\"end\":42005,\"start\":41995}]", "table": "[{\"end\":38483,\"start\":37980},{\"end\":38731,\"start\":38581},{\"end\":39569,\"start\":39209},{\"end\":40295,\"start\":39580},{\"end\":40628,\"start\":40539},{\"end\":40771,\"start\":40654},{\"end\":41993,\"start\":41364},{\"end\":43268,\"start\":42350},{\"end\":38483,\"start\":37980},{\"end\":38731,\"start\":38581},{\"end\":39569,\"start\":39209},{\"end\":40295,\"start\":39580},{\"end\":40628,\"start\":40539},{\"end\":40771,\"start\":40654},{\"end\":41993,\"start\":41364},{\"end\":43268,\"start\":42350}]", "figure_caption": "[{\"end\":37963,\"start\":37541},{\"end\":37980,\"start\":37966},{\"end\":38581,\"start\":38496},{\"end\":38878,\"start\":38744},{\"end\":39209,\"start\":38881},{\"end\":40539,\"start\":40298},{\"end\":40654,\"start\":40631},{\"end\":41364,\"start\":40774},{\"end\":42350,\"start\":42008},{\"end\":37963,\"start\":37541},{\"end\":37980,\"start\":37966},{\"end\":38581,\"start\":38496},{\"end\":38878,\"start\":38744},{\"end\":39209,\"start\":38881},{\"end\":40539,\"start\":40298},{\"end\":40654,\"start\":40631},{\"end\":41364,\"start\":40774},{\"end\":42350,\"start\":42008}]", "figure_ref": "[{\"end\":3346,\"start\":3338},{\"end\":6040,\"start\":6032},{\"end\":9502,\"start\":9494},{\"end\":22086,\"start\":22078},{\"end\":25270,\"start\":25262},{\"end\":3346,\"start\":3338},{\"end\":6040,\"start\":6032},{\"end\":9502,\"start\":9494},{\"end\":22086,\"start\":22078},{\"end\":25270,\"start\":25262}]", "bib_author_first_name": "[{\"end\":44081,\"start\":44077},{\"end\":44083,\"start\":44082},{\"end\":44100,\"start\":44091},{\"end\":44118,\"start\":44112},{\"end\":44489,\"start\":44487},{\"end\":44499,\"start\":44495},{\"end\":44512,\"start\":44506},{\"end\":44521,\"start\":44518},{\"end\":44536,\"start\":44528},{\"end\":44545,\"start\":44543},{\"end\":44559,\"start\":44551},{\"end\":44573,\"start\":44566},{\"end\":44583,\"start\":44579},{\"end\":44600,\"start\":44590},{\"end\":45212,\"start\":45206},{\"end\":45227,\"start\":45222},{\"end\":45242,\"start\":45235},{\"end\":45624,\"start\":45616},{\"end\":45637,\"start\":45632},{\"end\":45656,\"start\":45650},{\"end\":45673,\"start\":45665},{\"end\":46081,\"start\":46075},{\"end\":46093,\"start\":46088},{\"end\":46103,\"start\":46099},{\"end\":46121,\"start\":46114},{\"end\":46350,\"start\":46344},{\"end\":46364,\"start\":46357},{\"end\":46580,\"start\":46576},{\"end\":46592,\"start\":46587},{\"end\":46612,\"start\":46604},{\"end\":46630,\"start\":46622},{\"end\":46917,\"start\":46912},{\"end\":46950,\"start\":46944},{\"end\":46963,\"start\":46958},{\"end\":46977,\"start\":46972},{\"end\":46991,\"start\":46985},{\"end\":47424,\"start\":47420},{\"end\":47451,\"start\":47446},{\"end\":47468,\"start\":47461},{\"end\":47482,\"start\":47476},{\"end\":47492,\"start\":47487},{\"end\":47502,\"start\":47498},{\"end\":48096,\"start\":48088},{\"end\":48108,\"start\":48104},{\"end\":48123,\"start\":48118},{\"end\":48136,\"start\":48131},{\"end\":48150,\"start\":48144},{\"end\":48164,\"start\":48159},{\"end\":48183,\"start\":48177},{\"end\":48526,\"start\":48520},{\"end\":48540,\"start\":48533},{\"end\":48553,\"start\":48546},{\"end\":48842,\"start\":48837},{\"end\":48865,\"start\":48856},{\"end\":48877,\"start\":48873},{\"end\":48897,\"start\":48890},{\"end\":48917,\"start\":48911},{\"end\":48931,\"start\":48924},{\"end\":48953,\"start\":48945},{\"end\":48969,\"start\":48964},{\"end\":49399,\"start\":49387},{\"end\":49414,\"start\":49407},{\"end\":49429,\"start\":49422},{\"end\":49446,\"start\":49438},{\"end\":49456,\"start\":49455},{\"end\":49470,\"start\":49465},{\"end\":49486,\"start\":49482},{\"end\":49508,\"start\":49500},{\"end\":49525,\"start\":49518},{\"end\":49532,\"start\":49526},{\"end\":49554,\"start\":49546},{\"end\":49565,\"start\":49555},{\"end\":49576,\"start\":49571},{\"end\":49588,\"start\":49583},{\"end\":49599,\"start\":49595},{\"end\":49619,\"start\":49613},{\"end\":50093,\"start\":50089},{\"end\":50101,\"start\":50099},{\"end\":50112,\"start\":50108},{\"end\":50120,\"start\":50118},{\"end\":50427,\"start\":50423},{\"end\":50441,\"start\":50435},{\"end\":50456,\"start\":50451},{\"end\":50480,\"start\":50476},{\"end\":50493,\"start\":50489},{\"end\":50509,\"start\":50503},{\"end\":50931,\"start\":50926},{\"end\":50955,\"start\":50950},{\"end\":50968,\"start\":50961},{\"end\":50977,\"start\":50973},{\"end\":51203,\"start\":51201},{\"end\":51217,\"start\":51211},{\"end\":51229,\"start\":51223},{\"end\":51241,\"start\":51235},{\"end\":51243,\"start\":51242},{\"end\":51730,\"start\":51724},{\"end\":51743,\"start\":51738},{\"end\":51756,\"start\":51750},{\"end\":51768,\"start\":51762},{\"end\":51770,\"start\":51769},{\"end\":51781,\"start\":51777},{\"end\":51799,\"start\":51795},{\"end\":52181,\"start\":52177},{\"end\":52196,\"start\":52188},{\"end\":52204,\"start\":52201},{\"end\":52653,\"start\":52648},{\"end\":52671,\"start\":52665},{\"end\":52975,\"start\":52974},{\"end\":52989,\"start\":52988},{\"end\":53263,\"start\":53262},{\"end\":53277,\"start\":53274},{\"end\":53544,\"start\":53540},{\"end\":53562,\"start\":53557},{\"end\":53780,\"start\":53774},{\"end\":53789,\"start\":53786},{\"end\":53798,\"start\":53795},{\"end\":53810,\"start\":53804},{\"end\":53820,\"start\":53815},{\"end\":53833,\"start\":53826},{\"end\":53848,\"start\":53841},{\"end\":54224,\"start\":54216},{\"end\":54237,\"start\":54230},{\"end\":54250,\"start\":54245},{\"end\":54266,\"start\":54261},{\"end\":54279,\"start\":54273},{\"end\":54292,\"start\":54288},{\"end\":54307,\"start\":54302},{\"end\":54326,\"start\":54316},{\"end\":54719,\"start\":54714},{\"end\":54728,\"start\":54725},{\"end\":54744,\"start\":54740},{\"end\":54754,\"start\":54749},{\"end\":54769,\"start\":54761},{\"end\":55152,\"start\":55146},{\"end\":55160,\"start\":55157},{\"end\":55174,\"start\":55167},{\"end\":55188,\"start\":55182},{\"end\":55517,\"start\":55512},{\"end\":55519,\"start\":55518},{\"end\":55536,\"start\":55530},{\"end\":55551,\"start\":55543},{\"end\":55886,\"start\":55881},{\"end\":55901,\"start\":55896},{\"end\":56208,\"start\":56203},{\"end\":56228,\"start\":56223},{\"end\":56238,\"start\":56233},{\"end\":56517,\"start\":56512},{\"end\":56550,\"start\":56544},{\"end\":56569,\"start\":56563},{\"end\":56591,\"start\":56584},{\"end\":56599,\"start\":56592},{\"end\":56612,\"start\":56607},{\"end\":56628,\"start\":56619},{\"end\":56642,\"start\":56636},{\"end\":56644,\"start\":56643},{\"end\":57311,\"start\":57307},{\"end\":57328,\"start\":57325},{\"end\":57338,\"start\":57335},{\"end\":57351,\"start\":57343},{\"end\":57367,\"start\":57360},{\"end\":57382,\"start\":57378},{\"end\":57394,\"start\":57387},{\"end\":57408,\"start\":57402},{\"end\":57425,\"start\":57419},{\"end\":57441,\"start\":57434},{\"end\":57462,\"start\":57453},{\"end\":57464,\"start\":57463},{\"end\":57473,\"start\":57471},{\"end\":57484,\"start\":57483},{\"end\":57486,\"start\":57485},{\"end\":57502,\"start\":57495},{\"end\":57518,\"start\":57511},{\"end\":58111,\"start\":58106},{\"end\":58124,\"start\":58120},{\"end\":58138,\"start\":58134},{\"end\":58157,\"start\":58148},{\"end\":58169,\"start\":58163},{\"end\":58185,\"start\":58178},{\"end\":58199,\"start\":58194},{\"end\":58209,\"start\":58206},{\"end\":58219,\"start\":58214},{\"end\":58221,\"start\":58220},{\"end\":58583,\"start\":58580},{\"end\":58597,\"start\":58592},{\"end\":58617,\"start\":58612},{\"end\":58970,\"start\":58966},{\"end\":58986,\"start\":58981},{\"end\":59004,\"start\":58995},{\"end\":59629,\"start\":59625},{\"end\":59647,\"start\":59639},{\"end\":59662,\"start\":59654},{\"end\":59679,\"start\":59670},{\"end\":59997,\"start\":59989},{\"end\":60389,\"start\":60388},{\"end\":60407,\"start\":60397},{\"end\":60421,\"start\":60415},{\"end\":60654,\"start\":60649},{\"end\":60674,\"start\":60669},{\"end\":60689,\"start\":60684},{\"end\":61226,\"start\":61220},{\"end\":61240,\"start\":61236},{\"end\":61254,\"start\":61250},{\"end\":61268,\"start\":61263},{\"end\":61285,\"start\":61280},{\"end\":61298,\"start\":61293},{\"end\":61300,\"start\":61299},{\"end\":61314,\"start\":61308},{\"end\":61328,\"start\":61323},{\"end\":62163,\"start\":62156},{\"end\":62175,\"start\":62168},{\"end\":62184,\"start\":62183},{\"end\":62198,\"start\":62193},{\"end\":62414,\"start\":62408},{\"end\":62441,\"start\":62433},{\"end\":62452,\"start\":62447},{\"end\":62460,\"start\":62458},{\"end\":62471,\"start\":62468},{\"end\":62480,\"start\":62477},{\"end\":62756,\"start\":62747},{\"end\":62783,\"start\":62778},{\"end\":62795,\"start\":62789},{\"end\":62806,\"start\":62802},{\"end\":63005,\"start\":62998},{\"end\":63020,\"start\":63013},{\"end\":63034,\"start\":63028},{\"end\":63036,\"start\":63035},{\"end\":63181,\"start\":63174},{\"end\":63197,\"start\":63188},{\"end\":63214,\"start\":63210},{\"end\":63229,\"start\":63224},{\"end\":63502,\"start\":63495},{\"end\":63537,\"start\":63527},{\"end\":63549,\"start\":63542},{\"end\":63561,\"start\":63556},{\"end\":63573,\"start\":63567},{\"end\":63585,\"start\":63579},{\"end\":63600,\"start\":63592},{\"end\":63608,\"start\":63605},{\"end\":63623,\"start\":63622},{\"end\":63625,\"start\":63624},{\"end\":63636,\"start\":63634},{\"end\":64031,\"start\":64027},{\"end\":64044,\"start\":64038},{\"end\":64055,\"start\":64051},{\"end\":64067,\"start\":64062},{\"end\":64079,\"start\":64074},{\"end\":64095,\"start\":64088},{\"end\":44081,\"start\":44077},{\"end\":44083,\"start\":44082},{\"end\":44100,\"start\":44091},{\"end\":44118,\"start\":44112},{\"end\":44489,\"start\":44487},{\"end\":44499,\"start\":44495},{\"end\":44512,\"start\":44506},{\"end\":44521,\"start\":44518},{\"end\":44536,\"start\":44528},{\"end\":44545,\"start\":44543},{\"end\":44559,\"start\":44551},{\"end\":44573,\"start\":44566},{\"end\":44583,\"start\":44579},{\"end\":44600,\"start\":44590},{\"end\":45212,\"start\":45206},{\"end\":45227,\"start\":45222},{\"end\":45242,\"start\":45235},{\"end\":45624,\"start\":45616},{\"end\":45637,\"start\":45632},{\"end\":45656,\"start\":45650},{\"end\":45673,\"start\":45665},{\"end\":46081,\"start\":46075},{\"end\":46093,\"start\":46088},{\"end\":46103,\"start\":46099},{\"end\":46121,\"start\":46114},{\"end\":46350,\"start\":46344},{\"end\":46364,\"start\":46357},{\"end\":46580,\"start\":46576},{\"end\":46592,\"start\":46587},{\"end\":46612,\"start\":46604},{\"end\":46630,\"start\":46622},{\"end\":46917,\"start\":46912},{\"end\":46950,\"start\":46944},{\"end\":46963,\"start\":46958},{\"end\":46977,\"start\":46972},{\"end\":46991,\"start\":46985},{\"end\":47424,\"start\":47420},{\"end\":47451,\"start\":47446},{\"end\":47468,\"start\":47461},{\"end\":47482,\"start\":47476},{\"end\":47492,\"start\":47487},{\"end\":47502,\"start\":47498},{\"end\":48096,\"start\":48088},{\"end\":48108,\"start\":48104},{\"end\":48123,\"start\":48118},{\"end\":48136,\"start\":48131},{\"end\":48150,\"start\":48144},{\"end\":48164,\"start\":48159},{\"end\":48183,\"start\":48177},{\"end\":48526,\"start\":48520},{\"end\":48540,\"start\":48533},{\"end\":48553,\"start\":48546},{\"end\":48842,\"start\":48837},{\"end\":48865,\"start\":48856},{\"end\":48877,\"start\":48873},{\"end\":48897,\"start\":48890},{\"end\":48917,\"start\":48911},{\"end\":48931,\"start\":48924},{\"end\":48953,\"start\":48945},{\"end\":48969,\"start\":48964},{\"end\":49399,\"start\":49387},{\"end\":49414,\"start\":49407},{\"end\":49429,\"start\":49422},{\"end\":49446,\"start\":49438},{\"end\":49456,\"start\":49455},{\"end\":49470,\"start\":49465},{\"end\":49486,\"start\":49482},{\"end\":49508,\"start\":49500},{\"end\":49525,\"start\":49518},{\"end\":49532,\"start\":49526},{\"end\":49554,\"start\":49546},{\"end\":49565,\"start\":49555},{\"end\":49576,\"start\":49571},{\"end\":49588,\"start\":49583},{\"end\":49599,\"start\":49595},{\"end\":49619,\"start\":49613},{\"end\":50093,\"start\":50089},{\"end\":50101,\"start\":50099},{\"end\":50112,\"start\":50108},{\"end\":50120,\"start\":50118},{\"end\":50427,\"start\":50423},{\"end\":50441,\"start\":50435},{\"end\":50456,\"start\":50451},{\"end\":50480,\"start\":50476},{\"end\":50493,\"start\":50489},{\"end\":50509,\"start\":50503},{\"end\":50931,\"start\":50926},{\"end\":50955,\"start\":50950},{\"end\":50968,\"start\":50961},{\"end\":50977,\"start\":50973},{\"end\":51203,\"start\":51201},{\"end\":51217,\"start\":51211},{\"end\":51229,\"start\":51223},{\"end\":51241,\"start\":51235},{\"end\":51243,\"start\":51242},{\"end\":51730,\"start\":51724},{\"end\":51743,\"start\":51738},{\"end\":51756,\"start\":51750},{\"end\":51768,\"start\":51762},{\"end\":51770,\"start\":51769},{\"end\":51781,\"start\":51777},{\"end\":51799,\"start\":51795},{\"end\":52181,\"start\":52177},{\"end\":52196,\"start\":52188},{\"end\":52204,\"start\":52201},{\"end\":52653,\"start\":52648},{\"end\":52671,\"start\":52665},{\"end\":52975,\"start\":52974},{\"end\":52989,\"start\":52988},{\"end\":53263,\"start\":53262},{\"end\":53277,\"start\":53274},{\"end\":53544,\"start\":53540},{\"end\":53562,\"start\":53557},{\"end\":53780,\"start\":53774},{\"end\":53789,\"start\":53786},{\"end\":53798,\"start\":53795},{\"end\":53810,\"start\":53804},{\"end\":53820,\"start\":53815},{\"end\":53833,\"start\":53826},{\"end\":53848,\"start\":53841},{\"end\":54224,\"start\":54216},{\"end\":54237,\"start\":54230},{\"end\":54250,\"start\":54245},{\"end\":54266,\"start\":54261},{\"end\":54279,\"start\":54273},{\"end\":54292,\"start\":54288},{\"end\":54307,\"start\":54302},{\"end\":54326,\"start\":54316},{\"end\":54719,\"start\":54714},{\"end\":54728,\"start\":54725},{\"end\":54744,\"start\":54740},{\"end\":54754,\"start\":54749},{\"end\":54769,\"start\":54761},{\"end\":55152,\"start\":55146},{\"end\":55160,\"start\":55157},{\"end\":55174,\"start\":55167},{\"end\":55188,\"start\":55182},{\"end\":55517,\"start\":55512},{\"end\":55519,\"start\":55518},{\"end\":55536,\"start\":55530},{\"end\":55551,\"start\":55543},{\"end\":55886,\"start\":55881},{\"end\":55901,\"start\":55896},{\"end\":56208,\"start\":56203},{\"end\":56228,\"start\":56223},{\"end\":56238,\"start\":56233},{\"end\":56517,\"start\":56512},{\"end\":56550,\"start\":56544},{\"end\":56569,\"start\":56563},{\"end\":56591,\"start\":56584},{\"end\":56599,\"start\":56592},{\"end\":56612,\"start\":56607},{\"end\":56628,\"start\":56619},{\"end\":56642,\"start\":56636},{\"end\":56644,\"start\":56643},{\"end\":57311,\"start\":57307},{\"end\":57328,\"start\":57325},{\"end\":57338,\"start\":57335},{\"end\":57351,\"start\":57343},{\"end\":57367,\"start\":57360},{\"end\":57382,\"start\":57378},{\"end\":57394,\"start\":57387},{\"end\":57408,\"start\":57402},{\"end\":57425,\"start\":57419},{\"end\":57441,\"start\":57434},{\"end\":57462,\"start\":57453},{\"end\":57464,\"start\":57463},{\"end\":57473,\"start\":57471},{\"end\":57484,\"start\":57483},{\"end\":57486,\"start\":57485},{\"end\":57502,\"start\":57495},{\"end\":57518,\"start\":57511},{\"end\":58111,\"start\":58106},{\"end\":58124,\"start\":58120},{\"end\":58138,\"start\":58134},{\"end\":58157,\"start\":58148},{\"end\":58169,\"start\":58163},{\"end\":58185,\"start\":58178},{\"end\":58199,\"start\":58194},{\"end\":58209,\"start\":58206},{\"end\":58219,\"start\":58214},{\"end\":58221,\"start\":58220},{\"end\":58583,\"start\":58580},{\"end\":58597,\"start\":58592},{\"end\":58617,\"start\":58612},{\"end\":58970,\"start\":58966},{\"end\":58986,\"start\":58981},{\"end\":59004,\"start\":58995},{\"end\":59629,\"start\":59625},{\"end\":59647,\"start\":59639},{\"end\":59662,\"start\":59654},{\"end\":59679,\"start\":59670},{\"end\":59997,\"start\":59989},{\"end\":60389,\"start\":60388},{\"end\":60407,\"start\":60397},{\"end\":60421,\"start\":60415},{\"end\":60654,\"start\":60649},{\"end\":60674,\"start\":60669},{\"end\":60689,\"start\":60684},{\"end\":61226,\"start\":61220},{\"end\":61240,\"start\":61236},{\"end\":61254,\"start\":61250},{\"end\":61268,\"start\":61263},{\"end\":61285,\"start\":61280},{\"end\":61298,\"start\":61293},{\"end\":61300,\"start\":61299},{\"end\":61314,\"start\":61308},{\"end\":61328,\"start\":61323},{\"end\":62163,\"start\":62156},{\"end\":62175,\"start\":62168},{\"end\":62184,\"start\":62183},{\"end\":62198,\"start\":62193},{\"end\":62414,\"start\":62408},{\"end\":62441,\"start\":62433},{\"end\":62452,\"start\":62447},{\"end\":62460,\"start\":62458},{\"end\":62471,\"start\":62468},{\"end\":62480,\"start\":62477},{\"end\":62756,\"start\":62747},{\"end\":62783,\"start\":62778},{\"end\":62795,\"start\":62789},{\"end\":62806,\"start\":62802},{\"end\":63005,\"start\":62998},{\"end\":63020,\"start\":63013},{\"end\":63034,\"start\":63028},{\"end\":63036,\"start\":63035},{\"end\":63181,\"start\":63174},{\"end\":63197,\"start\":63188},{\"end\":63214,\"start\":63210},{\"end\":63229,\"start\":63224},{\"end\":63502,\"start\":63495},{\"end\":63537,\"start\":63527},{\"end\":63549,\"start\":63542},{\"end\":63561,\"start\":63556},{\"end\":63573,\"start\":63567},{\"end\":63585,\"start\":63579},{\"end\":63600,\"start\":63592},{\"end\":63608,\"start\":63605},{\"end\":63623,\"start\":63622},{\"end\":63625,\"start\":63624},{\"end\":63636,\"start\":63634},{\"end\":64031,\"start\":64027},{\"end\":64044,\"start\":64038},{\"end\":64055,\"start\":64051},{\"end\":64067,\"start\":64062},{\"end\":64079,\"start\":64074},{\"end\":64095,\"start\":64088}]", "bib_author_last_name": "[{\"end\":44089,\"start\":44084},{\"end\":44110,\"start\":44101},{\"end\":44126,\"start\":44119},{\"end\":44485,\"start\":44473},{\"end\":44493,\"start\":44490},{\"end\":44504,\"start\":44500},{\"end\":44516,\"start\":44513},{\"end\":44526,\"start\":44522},{\"end\":44541,\"start\":44537},{\"end\":44549,\"start\":44546},{\"end\":44564,\"start\":44560},{\"end\":44577,\"start\":44574},{\"end\":44588,\"start\":44584},{\"end\":44605,\"start\":44601},{\"end\":44610,\"start\":44607},{\"end\":45220,\"start\":45213},{\"end\":45233,\"start\":45228},{\"end\":45253,\"start\":45243},{\"end\":45630,\"start\":45625},{\"end\":45648,\"start\":45638},{\"end\":45663,\"start\":45657},{\"end\":45679,\"start\":45674},{\"end\":46086,\"start\":46082},{\"end\":46097,\"start\":46094},{\"end\":46112,\"start\":46104},{\"end\":46124,\"start\":46122},{\"end\":46355,\"start\":46351},{\"end\":46367,\"start\":46365},{\"end\":46585,\"start\":46581},{\"end\":46602,\"start\":46593},{\"end\":46620,\"start\":46613},{\"end\":46637,\"start\":46631},{\"end\":46942,\"start\":46918},{\"end\":46956,\"start\":46951},{\"end\":46970,\"start\":46964},{\"end\":46983,\"start\":46978},{\"end\":47002,\"start\":46992},{\"end\":47010,\"start\":47004},{\"end\":47444,\"start\":47425},{\"end\":47459,\"start\":47452},{\"end\":47474,\"start\":47469},{\"end\":47485,\"start\":47483},{\"end\":47496,\"start\":47493},{\"end\":47507,\"start\":47503},{\"end\":47518,\"start\":47509},{\"end\":48102,\"start\":48097},{\"end\":48116,\"start\":48109},{\"end\":48129,\"start\":48124},{\"end\":48142,\"start\":48137},{\"end\":48157,\"start\":48151},{\"end\":48175,\"start\":48165},{\"end\":48190,\"start\":48184},{\"end\":48531,\"start\":48527},{\"end\":48544,\"start\":48541},{\"end\":48556,\"start\":48554},{\"end\":48835,\"start\":48818},{\"end\":48854,\"start\":48843},{\"end\":48871,\"start\":48866},{\"end\":48888,\"start\":48878},{\"end\":48909,\"start\":48898},{\"end\":48922,\"start\":48918},{\"end\":48943,\"start\":48932},{\"end\":48962,\"start\":48954},{\"end\":48978,\"start\":48970},{\"end\":48987,\"start\":48980},{\"end\":49405,\"start\":49400},{\"end\":49420,\"start\":49415},{\"end\":49436,\"start\":49430},{\"end\":49453,\"start\":49447},{\"end\":49463,\"start\":49457},{\"end\":49480,\"start\":49471},{\"end\":49498,\"start\":49487},{\"end\":49516,\"start\":49509},{\"end\":49544,\"start\":49533},{\"end\":49569,\"start\":49566},{\"end\":49581,\"start\":49577},{\"end\":49593,\"start\":49589},{\"end\":49611,\"start\":49600},{\"end\":49625,\"start\":49620},{\"end\":49632,\"start\":49627},{\"end\":50097,\"start\":50094},{\"end\":50106,\"start\":50102},{\"end\":50116,\"start\":50113},{\"end\":50123,\"start\":50121},{\"end\":50421,\"start\":50402},{\"end\":50433,\"start\":50428},{\"end\":50449,\"start\":50442},{\"end\":50474,\"start\":50457},{\"end\":50487,\"start\":50481},{\"end\":50501,\"start\":50494},{\"end\":50519,\"start\":50510},{\"end\":50527,\"start\":50521},{\"end\":50948,\"start\":50932},{\"end\":50959,\"start\":50956},{\"end\":50971,\"start\":50969},{\"end\":50981,\"start\":50978},{\"end\":50991,\"start\":50983},{\"end\":51199,\"start\":51190},{\"end\":51209,\"start\":51204},{\"end\":51221,\"start\":51218},{\"end\":51233,\"start\":51230},{\"end\":51249,\"start\":51244},{\"end\":51261,\"start\":51251},{\"end\":51736,\"start\":51731},{\"end\":51748,\"start\":51744},{\"end\":51760,\"start\":51757},{\"end\":51775,\"start\":51771},{\"end\":51793,\"start\":51782},{\"end\":51804,\"start\":51800},{\"end\":52186,\"start\":52182},{\"end\":52199,\"start\":52197},{\"end\":52210,\"start\":52205},{\"end\":52663,\"start\":52654},{\"end\":52679,\"start\":52672},{\"end\":52986,\"start\":52976},{\"end\":52996,\"start\":52990},{\"end\":53272,\"start\":53264},{\"end\":53284,\"start\":53278},{\"end\":53293,\"start\":53286},{\"end\":53555,\"start\":53545},{\"end\":53569,\"start\":53563},{\"end\":53772,\"start\":53764},{\"end\":53784,\"start\":53781},{\"end\":53793,\"start\":53790},{\"end\":53802,\"start\":53799},{\"end\":53813,\"start\":53811},{\"end\":53824,\"start\":53821},{\"end\":53839,\"start\":53834},{\"end\":53852,\"start\":53849},{\"end\":53857,\"start\":53854},{\"end\":54228,\"start\":54225},{\"end\":54243,\"start\":54238},{\"end\":54259,\"start\":54251},{\"end\":54271,\"start\":54267},{\"end\":54286,\"start\":54280},{\"end\":54300,\"start\":54293},{\"end\":54314,\"start\":54308},{\"end\":54334,\"start\":54327},{\"end\":54712,\"start\":54697},{\"end\":54723,\"start\":54720},{\"end\":54738,\"start\":54729},{\"end\":54747,\"start\":54745},{\"end\":54759,\"start\":54755},{\"end\":54775,\"start\":54770},{\"end\":54782,\"start\":54777},{\"end\":55155,\"start\":55153},{\"end\":55165,\"start\":55161},{\"end\":55180,\"start\":55175},{\"end\":55192,\"start\":55189},{\"end\":55528,\"start\":55520},{\"end\":55541,\"start\":55537},{\"end\":55555,\"start\":55552},{\"end\":55894,\"start\":55887},{\"end\":55908,\"start\":55902},{\"end\":56124,\"start\":56116},{\"end\":56221,\"start\":56209},{\"end\":56231,\"start\":56229},{\"end\":56246,\"start\":56239},{\"end\":56542,\"start\":56518},{\"end\":56556,\"start\":56551},{\"end\":56561,\"start\":56558},{\"end\":56582,\"start\":56570},{\"end\":56605,\"start\":56600},{\"end\":56617,\"start\":56613},{\"end\":56634,\"start\":56629},{\"end\":56649,\"start\":56645},{\"end\":56657,\"start\":56651},{\"end\":57323,\"start\":57312},{\"end\":57333,\"start\":57329},{\"end\":57341,\"start\":57339},{\"end\":57358,\"start\":57352},{\"end\":57376,\"start\":57368},{\"end\":57385,\"start\":57383},{\"end\":57400,\"start\":57395},{\"end\":57417,\"start\":57409},{\"end\":57432,\"start\":57426},{\"end\":57451,\"start\":57442},{\"end\":57469,\"start\":57465},{\"end\":57481,\"start\":57474},{\"end\":57493,\"start\":57487},{\"end\":57509,\"start\":57503},{\"end\":57522,\"start\":57519},{\"end\":58118,\"start\":58112},{\"end\":58132,\"start\":58125},{\"end\":58146,\"start\":58139},{\"end\":58161,\"start\":58158},{\"end\":58176,\"start\":58170},{\"end\":58192,\"start\":58186},{\"end\":58204,\"start\":58200},{\"end\":58212,\"start\":58210},{\"end\":58225,\"start\":58222},{\"end\":58590,\"start\":58584},{\"end\":58610,\"start\":58598},{\"end\":58625,\"start\":58618},{\"end\":58979,\"start\":58971},{\"end\":58993,\"start\":58987},{\"end\":59010,\"start\":59005},{\"end\":59637,\"start\":59630},{\"end\":59652,\"start\":59648},{\"end\":59668,\"start\":59663},{\"end\":59685,\"start\":59680},{\"end\":60020,\"start\":59998},{\"end\":60026,\"start\":60022},{\"end\":60395,\"start\":60390},{\"end\":60413,\"start\":60408},{\"end\":60427,\"start\":60422},{\"end\":60431,\"start\":60429},{\"end\":60667,\"start\":60655},{\"end\":60682,\"start\":60675},{\"end\":60701,\"start\":60690},{\"end\":61234,\"start\":61227},{\"end\":61248,\"start\":61241},{\"end\":61261,\"start\":61255},{\"end\":61278,\"start\":61269},{\"end\":61291,\"start\":61286},{\"end\":61306,\"start\":61301},{\"end\":61321,\"start\":61315},{\"end\":61339,\"start\":61329},{\"end\":62166,\"start\":62164},{\"end\":62181,\"start\":62176},{\"end\":62191,\"start\":62185},{\"end\":62201,\"start\":62199},{\"end\":62206,\"start\":62203},{\"end\":62431,\"start\":62415},{\"end\":62445,\"start\":62442},{\"end\":62456,\"start\":62453},{\"end\":62466,\"start\":62461},{\"end\":62475,\"start\":62472},{\"end\":62484,\"start\":62481},{\"end\":62488,\"start\":62486},{\"end\":62776,\"start\":62757},{\"end\":62787,\"start\":62784},{\"end\":62800,\"start\":62796},{\"end\":62812,\"start\":62807},{\"end\":62817,\"start\":62814},{\"end\":63011,\"start\":63006},{\"end\":63026,\"start\":63021},{\"end\":63042,\"start\":63037},{\"end\":63186,\"start\":63182},{\"end\":63208,\"start\":63198},{\"end\":63222,\"start\":63215},{\"end\":63235,\"start\":63230},{\"end\":63525,\"start\":63503},{\"end\":63540,\"start\":63538},{\"end\":63554,\"start\":63550},{\"end\":63565,\"start\":63562},{\"end\":63577,\"start\":63574},{\"end\":63590,\"start\":63586},{\"end\":63603,\"start\":63601},{\"end\":63613,\"start\":63609},{\"end\":63620,\"start\":63615},{\"end\":63632,\"start\":63626},{\"end\":63641,\"start\":63637},{\"end\":63648,\"start\":63643},{\"end\":64025,\"start\":64010},{\"end\":64036,\"start\":64032},{\"end\":64049,\"start\":64045},{\"end\":64060,\"start\":64056},{\"end\":64072,\"start\":64068},{\"end\":64086,\"start\":64080},{\"end\":64104,\"start\":64096},{\"end\":64114,\"start\":64106},{\"end\":44089,\"start\":44084},{\"end\":44110,\"start\":44101},{\"end\":44126,\"start\":44119},{\"end\":44485,\"start\":44473},{\"end\":44493,\"start\":44490},{\"end\":44504,\"start\":44500},{\"end\":44516,\"start\":44513},{\"end\":44526,\"start\":44522},{\"end\":44541,\"start\":44537},{\"end\":44549,\"start\":44546},{\"end\":44564,\"start\":44560},{\"end\":44577,\"start\":44574},{\"end\":44588,\"start\":44584},{\"end\":44605,\"start\":44601},{\"end\":44610,\"start\":44607},{\"end\":45220,\"start\":45213},{\"end\":45233,\"start\":45228},{\"end\":45253,\"start\":45243},{\"end\":45630,\"start\":45625},{\"end\":45648,\"start\":45638},{\"end\":45663,\"start\":45657},{\"end\":45679,\"start\":45674},{\"end\":46086,\"start\":46082},{\"end\":46097,\"start\":46094},{\"end\":46112,\"start\":46104},{\"end\":46124,\"start\":46122},{\"end\":46355,\"start\":46351},{\"end\":46367,\"start\":46365},{\"end\":46585,\"start\":46581},{\"end\":46602,\"start\":46593},{\"end\":46620,\"start\":46613},{\"end\":46637,\"start\":46631},{\"end\":46942,\"start\":46918},{\"end\":46956,\"start\":46951},{\"end\":46970,\"start\":46964},{\"end\":46983,\"start\":46978},{\"end\":47002,\"start\":46992},{\"end\":47010,\"start\":47004},{\"end\":47444,\"start\":47425},{\"end\":47459,\"start\":47452},{\"end\":47474,\"start\":47469},{\"end\":47485,\"start\":47483},{\"end\":47496,\"start\":47493},{\"end\":47507,\"start\":47503},{\"end\":47518,\"start\":47509},{\"end\":48102,\"start\":48097},{\"end\":48116,\"start\":48109},{\"end\":48129,\"start\":48124},{\"end\":48142,\"start\":48137},{\"end\":48157,\"start\":48151},{\"end\":48175,\"start\":48165},{\"end\":48190,\"start\":48184},{\"end\":48531,\"start\":48527},{\"end\":48544,\"start\":48541},{\"end\":48556,\"start\":48554},{\"end\":48835,\"start\":48818},{\"end\":48854,\"start\":48843},{\"end\":48871,\"start\":48866},{\"end\":48888,\"start\":48878},{\"end\":48909,\"start\":48898},{\"end\":48922,\"start\":48918},{\"end\":48943,\"start\":48932},{\"end\":48962,\"start\":48954},{\"end\":48978,\"start\":48970},{\"end\":48987,\"start\":48980},{\"end\":49405,\"start\":49400},{\"end\":49420,\"start\":49415},{\"end\":49436,\"start\":49430},{\"end\":49453,\"start\":49447},{\"end\":49463,\"start\":49457},{\"end\":49480,\"start\":49471},{\"end\":49498,\"start\":49487},{\"end\":49516,\"start\":49509},{\"end\":49544,\"start\":49533},{\"end\":49569,\"start\":49566},{\"end\":49581,\"start\":49577},{\"end\":49593,\"start\":49589},{\"end\":49611,\"start\":49600},{\"end\":49625,\"start\":49620},{\"end\":49632,\"start\":49627},{\"end\":50097,\"start\":50094},{\"end\":50106,\"start\":50102},{\"end\":50116,\"start\":50113},{\"end\":50123,\"start\":50121},{\"end\":50421,\"start\":50402},{\"end\":50433,\"start\":50428},{\"end\":50449,\"start\":50442},{\"end\":50474,\"start\":50457},{\"end\":50487,\"start\":50481},{\"end\":50501,\"start\":50494},{\"end\":50519,\"start\":50510},{\"end\":50527,\"start\":50521},{\"end\":50948,\"start\":50932},{\"end\":50959,\"start\":50956},{\"end\":50971,\"start\":50969},{\"end\":50981,\"start\":50978},{\"end\":50991,\"start\":50983},{\"end\":51199,\"start\":51190},{\"end\":51209,\"start\":51204},{\"end\":51221,\"start\":51218},{\"end\":51233,\"start\":51230},{\"end\":51249,\"start\":51244},{\"end\":51261,\"start\":51251},{\"end\":51736,\"start\":51731},{\"end\":51748,\"start\":51744},{\"end\":51760,\"start\":51757},{\"end\":51775,\"start\":51771},{\"end\":51793,\"start\":51782},{\"end\":51804,\"start\":51800},{\"end\":52186,\"start\":52182},{\"end\":52199,\"start\":52197},{\"end\":52210,\"start\":52205},{\"end\":52663,\"start\":52654},{\"end\":52679,\"start\":52672},{\"end\":52986,\"start\":52976},{\"end\":52996,\"start\":52990},{\"end\":53272,\"start\":53264},{\"end\":53284,\"start\":53278},{\"end\":53293,\"start\":53286},{\"end\":53555,\"start\":53545},{\"end\":53569,\"start\":53563},{\"end\":53772,\"start\":53764},{\"end\":53784,\"start\":53781},{\"end\":53793,\"start\":53790},{\"end\":53802,\"start\":53799},{\"end\":53813,\"start\":53811},{\"end\":53824,\"start\":53821},{\"end\":53839,\"start\":53834},{\"end\":53852,\"start\":53849},{\"end\":53857,\"start\":53854},{\"end\":54228,\"start\":54225},{\"end\":54243,\"start\":54238},{\"end\":54259,\"start\":54251},{\"end\":54271,\"start\":54267},{\"end\":54286,\"start\":54280},{\"end\":54300,\"start\":54293},{\"end\":54314,\"start\":54308},{\"end\":54334,\"start\":54327},{\"end\":54712,\"start\":54697},{\"end\":54723,\"start\":54720},{\"end\":54738,\"start\":54729},{\"end\":54747,\"start\":54745},{\"end\":54759,\"start\":54755},{\"end\":54775,\"start\":54770},{\"end\":54782,\"start\":54777},{\"end\":55155,\"start\":55153},{\"end\":55165,\"start\":55161},{\"end\":55180,\"start\":55175},{\"end\":55192,\"start\":55189},{\"end\":55528,\"start\":55520},{\"end\":55541,\"start\":55537},{\"end\":55555,\"start\":55552},{\"end\":55894,\"start\":55887},{\"end\":55908,\"start\":55902},{\"end\":56124,\"start\":56116},{\"end\":56221,\"start\":56209},{\"end\":56231,\"start\":56229},{\"end\":56246,\"start\":56239},{\"end\":56542,\"start\":56518},{\"end\":56556,\"start\":56551},{\"end\":56561,\"start\":56558},{\"end\":56582,\"start\":56570},{\"end\":56605,\"start\":56600},{\"end\":56617,\"start\":56613},{\"end\":56634,\"start\":56629},{\"end\":56649,\"start\":56645},{\"end\":56657,\"start\":56651},{\"end\":57323,\"start\":57312},{\"end\":57333,\"start\":57329},{\"end\":57341,\"start\":57339},{\"end\":57358,\"start\":57352},{\"end\":57376,\"start\":57368},{\"end\":57385,\"start\":57383},{\"end\":57400,\"start\":57395},{\"end\":57417,\"start\":57409},{\"end\":57432,\"start\":57426},{\"end\":57451,\"start\":57442},{\"end\":57469,\"start\":57465},{\"end\":57481,\"start\":57474},{\"end\":57493,\"start\":57487},{\"end\":57509,\"start\":57503},{\"end\":57522,\"start\":57519},{\"end\":58118,\"start\":58112},{\"end\":58132,\"start\":58125},{\"end\":58146,\"start\":58139},{\"end\":58161,\"start\":58158},{\"end\":58176,\"start\":58170},{\"end\":58192,\"start\":58186},{\"end\":58204,\"start\":58200},{\"end\":58212,\"start\":58210},{\"end\":58225,\"start\":58222},{\"end\":58590,\"start\":58584},{\"end\":58610,\"start\":58598},{\"end\":58625,\"start\":58618},{\"end\":58979,\"start\":58971},{\"end\":58993,\"start\":58987},{\"end\":59010,\"start\":59005},{\"end\":59637,\"start\":59630},{\"end\":59652,\"start\":59648},{\"end\":59668,\"start\":59663},{\"end\":59685,\"start\":59680},{\"end\":60020,\"start\":59998},{\"end\":60026,\"start\":60022},{\"end\":60395,\"start\":60390},{\"end\":60413,\"start\":60408},{\"end\":60427,\"start\":60422},{\"end\":60431,\"start\":60429},{\"end\":60667,\"start\":60655},{\"end\":60682,\"start\":60675},{\"end\":60701,\"start\":60690},{\"end\":61234,\"start\":61227},{\"end\":61248,\"start\":61241},{\"end\":61261,\"start\":61255},{\"end\":61278,\"start\":61269},{\"end\":61291,\"start\":61286},{\"end\":61306,\"start\":61301},{\"end\":61321,\"start\":61315},{\"end\":61339,\"start\":61329},{\"end\":62166,\"start\":62164},{\"end\":62181,\"start\":62176},{\"end\":62191,\"start\":62185},{\"end\":62201,\"start\":62199},{\"end\":62206,\"start\":62203},{\"end\":62431,\"start\":62415},{\"end\":62445,\"start\":62442},{\"end\":62456,\"start\":62453},{\"end\":62466,\"start\":62461},{\"end\":62475,\"start\":62472},{\"end\":62484,\"start\":62481},{\"end\":62488,\"start\":62486},{\"end\":62776,\"start\":62757},{\"end\":62787,\"start\":62784},{\"end\":62800,\"start\":62796},{\"end\":62812,\"start\":62807},{\"end\":62817,\"start\":62814},{\"end\":63011,\"start\":63006},{\"end\":63026,\"start\":63021},{\"end\":63042,\"start\":63037},{\"end\":63186,\"start\":63182},{\"end\":63208,\"start\":63198},{\"end\":63222,\"start\":63215},{\"end\":63235,\"start\":63230},{\"end\":63525,\"start\":63503},{\"end\":63540,\"start\":63538},{\"end\":63554,\"start\":63550},{\"end\":63565,\"start\":63562},{\"end\":63577,\"start\":63574},{\"end\":63590,\"start\":63586},{\"end\":63603,\"start\":63601},{\"end\":63613,\"start\":63609},{\"end\":63620,\"start\":63615},{\"end\":63632,\"start\":63626},{\"end\":63641,\"start\":63637},{\"end\":63648,\"start\":63643},{\"end\":64025,\"start\":64010},{\"end\":64036,\"start\":64032},{\"end\":64049,\"start\":64045},{\"end\":64060,\"start\":64056},{\"end\":64072,\"start\":64068},{\"end\":64086,\"start\":64080},{\"end\":64104,\"start\":64096},{\"end\":64114,\"start\":64106}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207930156},\"end\":44392,\"start\":44005},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":211572655},\"end\":45132,\"start\":44394},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":173990164},\"end\":45552,\"start\":45134},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49865868},\"end\":46018,\"start\":45554},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b4\"},\"end\":46292,\"start\":46020},{\"attributes\":{\"doi\":\"arXiv:2011.10566\",\"id\":\"b5\"},\"end\":46503,\"start\":46294},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b6\"},\"end\":46833,\"start\":46505},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":219721240},\"end\":47382,\"start\":46835},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b8\",\"matched_paper_id\":219781060},\"end\":48026,\"start\":47384},{\"attributes\":{\"doi\":\"arXiv:2104.14294\",\"id\":\"b9\"},\"end\":48451,\"start\":48028},{\"attributes\":{\"doi\":\"abs/2104.02057\",\"id\":\"b10\",\"matched_paper_id\":233024948},\"end\":48718,\"start\":48453},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b11\"},\"end\":49314,\"start\":48720},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":219687798},\"end\":50001,\"start\":49316},{\"attributes\":{\"doi\":\"arXiv:2004.11207\",\"id\":\"b13\"},\"end\":50319,\"start\":50003},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52055130},\"end\":50857,\"start\":50321},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207930212},\"end\":51151,\"start\":50859},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6773885},\"end\":51651,\"start\":51153},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":198229624},\"end\":52124,\"start\":51653},{\"attributes\":{\"id\":\"b18\"},\"end\":52578,\"start\":52126},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4009713},\"end\":52917,\"start\":52580},{\"attributes\":{\"id\":\"b20\"},\"end\":53227,\"start\":52919},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":216078090},\"end\":53499,\"start\":53229},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":53592270},\"end\":53762,\"start\":53501},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b23\"},\"end\":54171,\"start\":53764},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14113767},\"end\":54632,\"start\":54173},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":244117288},\"end\":55077,\"start\":54634},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":218581596},\"end\":55429,\"start\":55079},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14307651},\"end\":55804,\"start\":55431},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":187547},\"end\":56112,\"start\":55806},{\"attributes\":{\"id\":\"b29\"},\"end\":56141,\"start\":56114},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b30\"},\"end\":56413,\"start\":56143},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":220045835},\"end\":57254,\"start\":56415},{\"attributes\":{\"doi\":\"abs/2102.12092\",\"id\":\"b32\",\"matched_paper_id\":2930547},\"end\":58021,\"start\":57256},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":204838007},\"end\":58523,\"start\":58023},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":173990382},\"end\":58903,\"start\":58525},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1114678},\"end\":59546,\"start\":58905},{\"attributes\":{\"doi\":\"arXiv:2012.12877\",\"id\":\"b36\",\"matched_paper_id\":229363322},\"end\":59987,\"start\":59548},{\"attributes\":{\"doi\":\"arXiv:2103.17239\",\"id\":\"b37\"},\"end\":60329,\"start\":59989},{\"attributes\":{\"doi\":\"arXiv:1906.02940\",\"id\":\"b38\"},\"end\":60606,\"start\":60331},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":20282961},\"end\":61191,\"start\":60608},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13756489},\"end\":62080,\"start\":61193},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":4591284},\"end\":62357,\"start\":62082},{\"attributes\":{\"doi\":\"arXiv:2105.04553\",\"id\":\"b42\"},\"end\":62693,\"start\":62359},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":50781105},\"end\":62967,\"start\":62695},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":50698},\"end\":63143,\"start\":62969},{\"attributes\":{\"doi\":\"arXiv:2106.04560\",\"id\":\"b45\"},\"end\":63401,\"start\":63145},{\"attributes\":{\"doi\":\"abs/2012.15840\",\"id\":\"b46\"},\"end\":63947,\"start\":63403},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":11371972},\"end\":64354,\"start\":63949},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207930156},\"end\":44392,\"start\":44005},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":211572655},\"end\":45132,\"start\":44394},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":173990164},\"end\":45552,\"start\":45134},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49865868},\"end\":46018,\"start\":45554},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b4\"},\"end\":46292,\"start\":46020},{\"attributes\":{\"doi\":\"arXiv:2011.10566\",\"id\":\"b5\"},\"end\":46503,\"start\":46294},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b6\"},\"end\":46833,\"start\":46505},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":219721240},\"end\":47382,\"start\":46835},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b8\",\"matched_paper_id\":219781060},\"end\":48026,\"start\":47384},{\"attributes\":{\"doi\":\"arXiv:2104.14294\",\"id\":\"b9\"},\"end\":48451,\"start\":48028},{\"attributes\":{\"doi\":\"abs/2104.02057\",\"id\":\"b10\",\"matched_paper_id\":233024948},\"end\":48718,\"start\":48453},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b11\"},\"end\":49314,\"start\":48720},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":219687798},\"end\":50001,\"start\":49316},{\"attributes\":{\"doi\":\"arXiv:2004.11207\",\"id\":\"b13\"},\"end\":50319,\"start\":50003},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52055130},\"end\":50857,\"start\":50321},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207930212},\"end\":51151,\"start\":50859},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6773885},\"end\":51651,\"start\":51153},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":198229624},\"end\":52124,\"start\":51653},{\"attributes\":{\"id\":\"b18\"},\"end\":52578,\"start\":52126},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4009713},\"end\":52917,\"start\":52580},{\"attributes\":{\"id\":\"b20\"},\"end\":53227,\"start\":52919},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":216078090},\"end\":53499,\"start\":53229},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":53592270},\"end\":53762,\"start\":53501},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b23\"},\"end\":54171,\"start\":53764},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14113767},\"end\":54632,\"start\":54173},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":244117288},\"end\":55077,\"start\":54634},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":218581596},\"end\":55429,\"start\":55079},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14307651},\"end\":55804,\"start\":55431},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":187547},\"end\":56112,\"start\":55806},{\"attributes\":{\"id\":\"b29\"},\"end\":56141,\"start\":56114},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b30\"},\"end\":56413,\"start\":56143},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":220045835},\"end\":57254,\"start\":56415},{\"attributes\":{\"doi\":\"abs/2102.12092\",\"id\":\"b32\",\"matched_paper_id\":2930547},\"end\":58021,\"start\":57256},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":204838007},\"end\":58523,\"start\":58023},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":173990382},\"end\":58903,\"start\":58525},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1114678},\"end\":59546,\"start\":58905},{\"attributes\":{\"doi\":\"arXiv:2012.12877\",\"id\":\"b36\",\"matched_paper_id\":229363322},\"end\":59987,\"start\":59548},{\"attributes\":{\"doi\":\"arXiv:2103.17239\",\"id\":\"b37\"},\"end\":60329,\"start\":59989},{\"attributes\":{\"doi\":\"arXiv:1906.02940\",\"id\":\"b38\"},\"end\":60606,\"start\":60331},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":20282961},\"end\":61191,\"start\":60608},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13756489},\"end\":62080,\"start\":61193},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":4591284},\"end\":62357,\"start\":62082},{\"attributes\":{\"doi\":\"arXiv:2105.04553\",\"id\":\"b42\"},\"end\":62693,\"start\":62359},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":50781105},\"end\":62967,\"start\":62695},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":50698},\"end\":63143,\"start\":62969},{\"attributes\":{\"doi\":\"arXiv:2106.04560\",\"id\":\"b45\"},\"end\":63401,\"start\":63145},{\"attributes\":{\"doi\":\"abs/2012.15840\",\"id\":\"b46\"},\"end\":63947,\"start\":63403},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":11371972},\"end\":64354,\"start\":63949}]", "bib_title": "[{\"end\":44075,\"start\":44005},{\"end\":44471,\"start\":44394},{\"end\":45204,\"start\":45134},{\"end\":45614,\"start\":45554},{\"end\":46910,\"start\":46835},{\"end\":47418,\"start\":47384},{\"end\":48518,\"start\":48453},{\"end\":49385,\"start\":49316},{\"end\":50400,\"start\":50321},{\"end\":50924,\"start\":50859},{\"end\":51188,\"start\":51153},{\"end\":51722,\"start\":51653},{\"end\":52175,\"start\":52126},{\"end\":52646,\"start\":52580},{\"end\":53260,\"start\":53229},{\"end\":53538,\"start\":53501},{\"end\":54214,\"start\":54173},{\"end\":54695,\"start\":54634},{\"end\":55144,\"start\":55079},{\"end\":55510,\"start\":55431},{\"end\":55879,\"start\":55806},{\"end\":56510,\"start\":56415},{\"end\":57305,\"start\":57256},{\"end\":58104,\"start\":58023},{\"end\":58578,\"start\":58525},{\"end\":58964,\"start\":58905},{\"end\":59623,\"start\":59548},{\"end\":60647,\"start\":60608},{\"end\":61218,\"start\":61193},{\"end\":62154,\"start\":62082},{\"end\":62745,\"start\":62695},{\"end\":62996,\"start\":62969},{\"end\":64008,\"start\":63949},{\"end\":44075,\"start\":44005},{\"end\":44471,\"start\":44394},{\"end\":45204,\"start\":45134},{\"end\":45614,\"start\":45554},{\"end\":46910,\"start\":46835},{\"end\":47418,\"start\":47384},{\"end\":48518,\"start\":48453},{\"end\":49385,\"start\":49316},{\"end\":50400,\"start\":50321},{\"end\":50924,\"start\":50859},{\"end\":51188,\"start\":51153},{\"end\":51722,\"start\":51653},{\"end\":52175,\"start\":52126},{\"end\":52646,\"start\":52580},{\"end\":53260,\"start\":53229},{\"end\":53538,\"start\":53501},{\"end\":54214,\"start\":54173},{\"end\":54695,\"start\":54634},{\"end\":55144,\"start\":55079},{\"end\":55510,\"start\":55431},{\"end\":55879,\"start\":55806},{\"end\":56510,\"start\":56415},{\"end\":57305,\"start\":57256},{\"end\":58104,\"start\":58023},{\"end\":58578,\"start\":58525},{\"end\":58964,\"start\":58905},{\"end\":59623,\"start\":59548},{\"end\":60647,\"start\":60608},{\"end\":61218,\"start\":61193},{\"end\":62154,\"start\":62082},{\"end\":62745,\"start\":62695},{\"end\":62996,\"start\":62969},{\"end\":64008,\"start\":63949}]", "bib_author": "[{\"end\":44091,\"start\":44077},{\"end\":44112,\"start\":44091},{\"end\":44128,\"start\":44112},{\"end\":44487,\"start\":44473},{\"end\":44495,\"start\":44487},{\"end\":44506,\"start\":44495},{\"end\":44518,\"start\":44506},{\"end\":44528,\"start\":44518},{\"end\":44543,\"start\":44528},{\"end\":44551,\"start\":44543},{\"end\":44566,\"start\":44551},{\"end\":44579,\"start\":44566},{\"end\":44590,\"start\":44579},{\"end\":44607,\"start\":44590},{\"end\":44612,\"start\":44607},{\"end\":45222,\"start\":45206},{\"end\":45235,\"start\":45222},{\"end\":45255,\"start\":45235},{\"end\":45632,\"start\":45616},{\"end\":45650,\"start\":45632},{\"end\":45665,\"start\":45650},{\"end\":45681,\"start\":45665},{\"end\":46088,\"start\":46075},{\"end\":46099,\"start\":46088},{\"end\":46114,\"start\":46099},{\"end\":46126,\"start\":46114},{\"end\":46357,\"start\":46344},{\"end\":46369,\"start\":46357},{\"end\":46587,\"start\":46576},{\"end\":46604,\"start\":46587},{\"end\":46622,\"start\":46604},{\"end\":46639,\"start\":46622},{\"end\":46944,\"start\":46912},{\"end\":46958,\"start\":46944},{\"end\":46972,\"start\":46958},{\"end\":46985,\"start\":46972},{\"end\":47004,\"start\":46985},{\"end\":47012,\"start\":47004},{\"end\":47446,\"start\":47420},{\"end\":47461,\"start\":47446},{\"end\":47476,\"start\":47461},{\"end\":47487,\"start\":47476},{\"end\":47498,\"start\":47487},{\"end\":47509,\"start\":47498},{\"end\":47520,\"start\":47509},{\"end\":48104,\"start\":48088},{\"end\":48118,\"start\":48104},{\"end\":48131,\"start\":48118},{\"end\":48144,\"start\":48131},{\"end\":48159,\"start\":48144},{\"end\":48177,\"start\":48159},{\"end\":48192,\"start\":48177},{\"end\":48533,\"start\":48520},{\"end\":48546,\"start\":48533},{\"end\":48558,\"start\":48546},{\"end\":48837,\"start\":48818},{\"end\":48856,\"start\":48837},{\"end\":48873,\"start\":48856},{\"end\":48890,\"start\":48873},{\"end\":48911,\"start\":48890},{\"end\":48924,\"start\":48911},{\"end\":48945,\"start\":48924},{\"end\":48964,\"start\":48945},{\"end\":48980,\"start\":48964},{\"end\":48989,\"start\":48980},{\"end\":49407,\"start\":49387},{\"end\":49422,\"start\":49407},{\"end\":49438,\"start\":49422},{\"end\":49455,\"start\":49438},{\"end\":49465,\"start\":49455},{\"end\":49482,\"start\":49465},{\"end\":49500,\"start\":49482},{\"end\":49518,\"start\":49500},{\"end\":49546,\"start\":49518},{\"end\":49571,\"start\":49546},{\"end\":49583,\"start\":49571},{\"end\":49595,\"start\":49583},{\"end\":49613,\"start\":49595},{\"end\":49627,\"start\":49613},{\"end\":49634,\"start\":49627},{\"end\":50099,\"start\":50089},{\"end\":50108,\"start\":50099},{\"end\":50118,\"start\":50108},{\"end\":50125,\"start\":50118},{\"end\":50423,\"start\":50402},{\"end\":50435,\"start\":50423},{\"end\":50451,\"start\":50435},{\"end\":50476,\"start\":50451},{\"end\":50489,\"start\":50476},{\"end\":50503,\"start\":50489},{\"end\":50521,\"start\":50503},{\"end\":50529,\"start\":50521},{\"end\":50950,\"start\":50926},{\"end\":50961,\"start\":50950},{\"end\":50973,\"start\":50961},{\"end\":50983,\"start\":50973},{\"end\":50993,\"start\":50983},{\"end\":51201,\"start\":51190},{\"end\":51211,\"start\":51201},{\"end\":51223,\"start\":51211},{\"end\":51235,\"start\":51223},{\"end\":51251,\"start\":51235},{\"end\":51263,\"start\":51251},{\"end\":51738,\"start\":51724},{\"end\":51750,\"start\":51738},{\"end\":51762,\"start\":51750},{\"end\":51777,\"start\":51762},{\"end\":51795,\"start\":51777},{\"end\":51806,\"start\":51795},{\"end\":52188,\"start\":52177},{\"end\":52201,\"start\":52188},{\"end\":52212,\"start\":52201},{\"end\":52665,\"start\":52648},{\"end\":52681,\"start\":52665},{\"end\":52988,\"start\":52974},{\"end\":52998,\"start\":52988},{\"end\":53274,\"start\":53262},{\"end\":53286,\"start\":53274},{\"end\":53295,\"start\":53286},{\"end\":53557,\"start\":53540},{\"end\":53571,\"start\":53557},{\"end\":53774,\"start\":53764},{\"end\":53786,\"start\":53774},{\"end\":53795,\"start\":53786},{\"end\":53804,\"start\":53795},{\"end\":53815,\"start\":53804},{\"end\":53826,\"start\":53815},{\"end\":53841,\"start\":53826},{\"end\":53854,\"start\":53841},{\"end\":53859,\"start\":53854},{\"end\":54230,\"start\":54216},{\"end\":54245,\"start\":54230},{\"end\":54261,\"start\":54245},{\"end\":54273,\"start\":54261},{\"end\":54288,\"start\":54273},{\"end\":54302,\"start\":54288},{\"end\":54316,\"start\":54302},{\"end\":54336,\"start\":54316},{\"end\":54714,\"start\":54697},{\"end\":54725,\"start\":54714},{\"end\":54740,\"start\":54725},{\"end\":54749,\"start\":54740},{\"end\":54761,\"start\":54749},{\"end\":54777,\"start\":54761},{\"end\":54784,\"start\":54777},{\"end\":55157,\"start\":55146},{\"end\":55167,\"start\":55157},{\"end\":55182,\"start\":55167},{\"end\":55194,\"start\":55182},{\"end\":55530,\"start\":55512},{\"end\":55543,\"start\":55530},{\"end\":55557,\"start\":55543},{\"end\":55896,\"start\":55881},{\"end\":55910,\"start\":55896},{\"end\":56126,\"start\":56116},{\"end\":56223,\"start\":56203},{\"end\":56233,\"start\":56223},{\"end\":56248,\"start\":56233},{\"end\":56544,\"start\":56512},{\"end\":56558,\"start\":56544},{\"end\":56563,\"start\":56558},{\"end\":56584,\"start\":56563},{\"end\":56607,\"start\":56584},{\"end\":56619,\"start\":56607},{\"end\":56636,\"start\":56619},{\"end\":56651,\"start\":56636},{\"end\":56659,\"start\":56651},{\"end\":57325,\"start\":57307},{\"end\":57335,\"start\":57325},{\"end\":57343,\"start\":57335},{\"end\":57360,\"start\":57343},{\"end\":57378,\"start\":57360},{\"end\":57387,\"start\":57378},{\"end\":57402,\"start\":57387},{\"end\":57419,\"start\":57402},{\"end\":57434,\"start\":57419},{\"end\":57453,\"start\":57434},{\"end\":57471,\"start\":57453},{\"end\":57483,\"start\":57471},{\"end\":57495,\"start\":57483},{\"end\":57511,\"start\":57495},{\"end\":57524,\"start\":57511},{\"end\":58120,\"start\":58106},{\"end\":58134,\"start\":58120},{\"end\":58148,\"start\":58134},{\"end\":58163,\"start\":58148},{\"end\":58178,\"start\":58163},{\"end\":58194,\"start\":58178},{\"end\":58206,\"start\":58194},{\"end\":58214,\"start\":58206},{\"end\":58227,\"start\":58214},{\"end\":58592,\"start\":58580},{\"end\":58612,\"start\":58592},{\"end\":58627,\"start\":58612},{\"end\":58981,\"start\":58966},{\"end\":58995,\"start\":58981},{\"end\":59012,\"start\":58995},{\"end\":59639,\"start\":59625},{\"end\":59654,\"start\":59639},{\"end\":59670,\"start\":59654},{\"end\":59687,\"start\":59670},{\"end\":60022,\"start\":59989},{\"end\":60028,\"start\":60022},{\"end\":60397,\"start\":60388},{\"end\":60415,\"start\":60397},{\"end\":60429,\"start\":60415},{\"end\":60433,\"start\":60429},{\"end\":60669,\"start\":60649},{\"end\":60684,\"start\":60669},{\"end\":60703,\"start\":60684},{\"end\":61236,\"start\":61220},{\"end\":61250,\"start\":61236},{\"end\":61263,\"start\":61250},{\"end\":61280,\"start\":61263},{\"end\":61293,\"start\":61280},{\"end\":61308,\"start\":61293},{\"end\":61323,\"start\":61308},{\"end\":61341,\"start\":61323},{\"end\":62168,\"start\":62156},{\"end\":62183,\"start\":62168},{\"end\":62193,\"start\":62183},{\"end\":62203,\"start\":62193},{\"end\":62208,\"start\":62203},{\"end\":62433,\"start\":62408},{\"end\":62447,\"start\":62433},{\"end\":62458,\"start\":62447},{\"end\":62468,\"start\":62458},{\"end\":62477,\"start\":62468},{\"end\":62486,\"start\":62477},{\"end\":62490,\"start\":62486},{\"end\":62778,\"start\":62747},{\"end\":62789,\"start\":62778},{\"end\":62802,\"start\":62789},{\"end\":62814,\"start\":62802},{\"end\":62819,\"start\":62814},{\"end\":63013,\"start\":62998},{\"end\":63028,\"start\":63013},{\"end\":63044,\"start\":63028},{\"end\":63188,\"start\":63174},{\"end\":63210,\"start\":63188},{\"end\":63224,\"start\":63210},{\"end\":63237,\"start\":63224},{\"end\":63527,\"start\":63495},{\"end\":63542,\"start\":63527},{\"end\":63556,\"start\":63542},{\"end\":63567,\"start\":63556},{\"end\":63579,\"start\":63567},{\"end\":63592,\"start\":63579},{\"end\":63605,\"start\":63592},{\"end\":63615,\"start\":63605},{\"end\":63622,\"start\":63615},{\"end\":63634,\"start\":63622},{\"end\":63643,\"start\":63634},{\"end\":63650,\"start\":63643},{\"end\":64027,\"start\":64010},{\"end\":64038,\"start\":64027},{\"end\":64051,\"start\":64038},{\"end\":64062,\"start\":64051},{\"end\":64074,\"start\":64062},{\"end\":64088,\"start\":64074},{\"end\":64106,\"start\":64088},{\"end\":64116,\"start\":64106},{\"end\":44091,\"start\":44077},{\"end\":44112,\"start\":44091},{\"end\":44128,\"start\":44112},{\"end\":44487,\"start\":44473},{\"end\":44495,\"start\":44487},{\"end\":44506,\"start\":44495},{\"end\":44518,\"start\":44506},{\"end\":44528,\"start\":44518},{\"end\":44543,\"start\":44528},{\"end\":44551,\"start\":44543},{\"end\":44566,\"start\":44551},{\"end\":44579,\"start\":44566},{\"end\":44590,\"start\":44579},{\"end\":44607,\"start\":44590},{\"end\":44612,\"start\":44607},{\"end\":45222,\"start\":45206},{\"end\":45235,\"start\":45222},{\"end\":45255,\"start\":45235},{\"end\":45632,\"start\":45616},{\"end\":45650,\"start\":45632},{\"end\":45665,\"start\":45650},{\"end\":45681,\"start\":45665},{\"end\":46088,\"start\":46075},{\"end\":46099,\"start\":46088},{\"end\":46114,\"start\":46099},{\"end\":46126,\"start\":46114},{\"end\":46357,\"start\":46344},{\"end\":46369,\"start\":46357},{\"end\":46587,\"start\":46576},{\"end\":46604,\"start\":46587},{\"end\":46622,\"start\":46604},{\"end\":46639,\"start\":46622},{\"end\":46944,\"start\":46912},{\"end\":46958,\"start\":46944},{\"end\":46972,\"start\":46958},{\"end\":46985,\"start\":46972},{\"end\":47004,\"start\":46985},{\"end\":47012,\"start\":47004},{\"end\":47446,\"start\":47420},{\"end\":47461,\"start\":47446},{\"end\":47476,\"start\":47461},{\"end\":47487,\"start\":47476},{\"end\":47498,\"start\":47487},{\"end\":47509,\"start\":47498},{\"end\":47520,\"start\":47509},{\"end\":48104,\"start\":48088},{\"end\":48118,\"start\":48104},{\"end\":48131,\"start\":48118},{\"end\":48144,\"start\":48131},{\"end\":48159,\"start\":48144},{\"end\":48177,\"start\":48159},{\"end\":48192,\"start\":48177},{\"end\":48533,\"start\":48520},{\"end\":48546,\"start\":48533},{\"end\":48558,\"start\":48546},{\"end\":48837,\"start\":48818},{\"end\":48856,\"start\":48837},{\"end\":48873,\"start\":48856},{\"end\":48890,\"start\":48873},{\"end\":48911,\"start\":48890},{\"end\":48924,\"start\":48911},{\"end\":48945,\"start\":48924},{\"end\":48964,\"start\":48945},{\"end\":48980,\"start\":48964},{\"end\":48989,\"start\":48980},{\"end\":49407,\"start\":49387},{\"end\":49422,\"start\":49407},{\"end\":49438,\"start\":49422},{\"end\":49455,\"start\":49438},{\"end\":49465,\"start\":49455},{\"end\":49482,\"start\":49465},{\"end\":49500,\"start\":49482},{\"end\":49518,\"start\":49500},{\"end\":49546,\"start\":49518},{\"end\":49571,\"start\":49546},{\"end\":49583,\"start\":49571},{\"end\":49595,\"start\":49583},{\"end\":49613,\"start\":49595},{\"end\":49627,\"start\":49613},{\"end\":49634,\"start\":49627},{\"end\":50099,\"start\":50089},{\"end\":50108,\"start\":50099},{\"end\":50118,\"start\":50108},{\"end\":50125,\"start\":50118},{\"end\":50423,\"start\":50402},{\"end\":50435,\"start\":50423},{\"end\":50451,\"start\":50435},{\"end\":50476,\"start\":50451},{\"end\":50489,\"start\":50476},{\"end\":50503,\"start\":50489},{\"end\":50521,\"start\":50503},{\"end\":50529,\"start\":50521},{\"end\":50950,\"start\":50926},{\"end\":50961,\"start\":50950},{\"end\":50973,\"start\":50961},{\"end\":50983,\"start\":50973},{\"end\":50993,\"start\":50983},{\"end\":51201,\"start\":51190},{\"end\":51211,\"start\":51201},{\"end\":51223,\"start\":51211},{\"end\":51235,\"start\":51223},{\"end\":51251,\"start\":51235},{\"end\":51263,\"start\":51251},{\"end\":51738,\"start\":51724},{\"end\":51750,\"start\":51738},{\"end\":51762,\"start\":51750},{\"end\":51777,\"start\":51762},{\"end\":51795,\"start\":51777},{\"end\":51806,\"start\":51795},{\"end\":52188,\"start\":52177},{\"end\":52201,\"start\":52188},{\"end\":52212,\"start\":52201},{\"end\":52665,\"start\":52648},{\"end\":52681,\"start\":52665},{\"end\":52988,\"start\":52974},{\"end\":52998,\"start\":52988},{\"end\":53274,\"start\":53262},{\"end\":53286,\"start\":53274},{\"end\":53295,\"start\":53286},{\"end\":53557,\"start\":53540},{\"end\":53571,\"start\":53557},{\"end\":53774,\"start\":53764},{\"end\":53786,\"start\":53774},{\"end\":53795,\"start\":53786},{\"end\":53804,\"start\":53795},{\"end\":53815,\"start\":53804},{\"end\":53826,\"start\":53815},{\"end\":53841,\"start\":53826},{\"end\":53854,\"start\":53841},{\"end\":53859,\"start\":53854},{\"end\":54230,\"start\":54216},{\"end\":54245,\"start\":54230},{\"end\":54261,\"start\":54245},{\"end\":54273,\"start\":54261},{\"end\":54288,\"start\":54273},{\"end\":54302,\"start\":54288},{\"end\":54316,\"start\":54302},{\"end\":54336,\"start\":54316},{\"end\":54714,\"start\":54697},{\"end\":54725,\"start\":54714},{\"end\":54740,\"start\":54725},{\"end\":54749,\"start\":54740},{\"end\":54761,\"start\":54749},{\"end\":54777,\"start\":54761},{\"end\":54784,\"start\":54777},{\"end\":55157,\"start\":55146},{\"end\":55167,\"start\":55157},{\"end\":55182,\"start\":55167},{\"end\":55194,\"start\":55182},{\"end\":55530,\"start\":55512},{\"end\":55543,\"start\":55530},{\"end\":55557,\"start\":55543},{\"end\":55896,\"start\":55881},{\"end\":55910,\"start\":55896},{\"end\":56126,\"start\":56116},{\"end\":56223,\"start\":56203},{\"end\":56233,\"start\":56223},{\"end\":56248,\"start\":56233},{\"end\":56544,\"start\":56512},{\"end\":56558,\"start\":56544},{\"end\":56563,\"start\":56558},{\"end\":56584,\"start\":56563},{\"end\":56607,\"start\":56584},{\"end\":56619,\"start\":56607},{\"end\":56636,\"start\":56619},{\"end\":56651,\"start\":56636},{\"end\":56659,\"start\":56651},{\"end\":57325,\"start\":57307},{\"end\":57335,\"start\":57325},{\"end\":57343,\"start\":57335},{\"end\":57360,\"start\":57343},{\"end\":57378,\"start\":57360},{\"end\":57387,\"start\":57378},{\"end\":57402,\"start\":57387},{\"end\":57419,\"start\":57402},{\"end\":57434,\"start\":57419},{\"end\":57453,\"start\":57434},{\"end\":57471,\"start\":57453},{\"end\":57483,\"start\":57471},{\"end\":57495,\"start\":57483},{\"end\":57511,\"start\":57495},{\"end\":57524,\"start\":57511},{\"end\":58120,\"start\":58106},{\"end\":58134,\"start\":58120},{\"end\":58148,\"start\":58134},{\"end\":58163,\"start\":58148},{\"end\":58178,\"start\":58163},{\"end\":58194,\"start\":58178},{\"end\":58206,\"start\":58194},{\"end\":58214,\"start\":58206},{\"end\":58227,\"start\":58214},{\"end\":58592,\"start\":58580},{\"end\":58612,\"start\":58592},{\"end\":58627,\"start\":58612},{\"end\":58981,\"start\":58966},{\"end\":58995,\"start\":58981},{\"end\":59012,\"start\":58995},{\"end\":59639,\"start\":59625},{\"end\":59654,\"start\":59639},{\"end\":59670,\"start\":59654},{\"end\":59687,\"start\":59670},{\"end\":60022,\"start\":59989},{\"end\":60028,\"start\":60022},{\"end\":60397,\"start\":60388},{\"end\":60415,\"start\":60397},{\"end\":60429,\"start\":60415},{\"end\":60433,\"start\":60429},{\"end\":60669,\"start\":60649},{\"end\":60684,\"start\":60669},{\"end\":60703,\"start\":60684},{\"end\":61236,\"start\":61220},{\"end\":61250,\"start\":61236},{\"end\":61263,\"start\":61250},{\"end\":61280,\"start\":61263},{\"end\":61293,\"start\":61280},{\"end\":61308,\"start\":61293},{\"end\":61323,\"start\":61308},{\"end\":61341,\"start\":61323},{\"end\":62168,\"start\":62156},{\"end\":62183,\"start\":62168},{\"end\":62193,\"start\":62183},{\"end\":62203,\"start\":62193},{\"end\":62208,\"start\":62203},{\"end\":62433,\"start\":62408},{\"end\":62447,\"start\":62433},{\"end\":62458,\"start\":62447},{\"end\":62468,\"start\":62458},{\"end\":62477,\"start\":62468},{\"end\":62486,\"start\":62477},{\"end\":62490,\"start\":62486},{\"end\":62778,\"start\":62747},{\"end\":62789,\"start\":62778},{\"end\":62802,\"start\":62789},{\"end\":62814,\"start\":62802},{\"end\":62819,\"start\":62814},{\"end\":63013,\"start\":62998},{\"end\":63028,\"start\":63013},{\"end\":63044,\"start\":63028},{\"end\":63188,\"start\":63174},{\"end\":63210,\"start\":63188},{\"end\":63224,\"start\":63210},{\"end\":63237,\"start\":63224},{\"end\":63527,\"start\":63495},{\"end\":63542,\"start\":63527},{\"end\":63556,\"start\":63542},{\"end\":63567,\"start\":63556},{\"end\":63579,\"start\":63567},{\"end\":63592,\"start\":63579},{\"end\":63605,\"start\":63592},{\"end\":63615,\"start\":63605},{\"end\":63622,\"start\":63615},{\"end\":63634,\"start\":63622},{\"end\":63643,\"start\":63634},{\"end\":63650,\"start\":63643},{\"end\":64027,\"start\":64010},{\"end\":64038,\"start\":64027},{\"end\":64051,\"start\":64038},{\"end\":64062,\"start\":64051},{\"end\":64074,\"start\":64062},{\"end\":64088,\"start\":64074},{\"end\":64106,\"start\":64088},{\"end\":64116,\"start\":64106}]", "bib_venue": "[{\"end\":44735,\"start\":44682},{\"end\":45796,\"start\":45747},{\"end\":47676,\"start\":47623},{\"end\":51348,\"start\":51344},{\"end\":52284,\"start\":52270},{\"end\":56820,\"start\":56748},{\"end\":59188,\"start\":59101},{\"end\":60903,\"start\":60803},{\"end\":61593,\"start\":61574},{\"end\":44735,\"start\":44682},{\"end\":45796,\"start\":45747},{\"end\":47676,\"start\":47623},{\"end\":51348,\"start\":51344},{\"end\":52284,\"start\":52270},{\"end\":56820,\"start\":56748},{\"end\":59188,\"start\":59101},{\"end\":60903,\"start\":60803},{\"end\":61593,\"start\":61574},{\"end\":44187,\"start\":44128},{\"end\":44680,\"start\":44612},{\"end\":45304,\"start\":45255},{\"end\":45745,\"start\":45681},{\"end\":46073,\"start\":46020},{\"end\":46342,\"start\":46294},{\"end\":46574,\"start\":46505},{\"end\":47061,\"start\":47012},{\"end\":47592,\"start\":47524},{\"end\":48086,\"start\":48028},{\"end\":48577,\"start\":48572},{\"end\":48816,\"start\":48720},{\"end\":49642,\"start\":49634},{\"end\":50087,\"start\":50003},{\"end\":50581,\"start\":50529},{\"end\":50997,\"start\":50993},{\"end\":51289,\"start\":51263},{\"end\":51867,\"start\":51806},{\"end\":52268,\"start\":52212},{\"end\":52740,\"start\":52681},{\"end\":52972,\"start\":52919},{\"end\":53351,\"start\":53295},{\"end\":53623,\"start\":53571},{\"end\":53946,\"start\":53875},{\"end\":54374,\"start\":54336},{\"end\":54848,\"start\":54784},{\"end\":55246,\"start\":55194},{\"end\":55609,\"start\":55557},{\"end\":55948,\"start\":55910},{\"end\":56201,\"start\":56143},{\"end\":56746,\"start\":56659},{\"end\":57543,\"start\":57538},{\"end\":58246,\"start\":58227},{\"end\":58676,\"start\":58627},{\"end\":59099,\"start\":59012},{\"end\":59742,\"start\":59703},{\"end\":60139,\"start\":60044},{\"end\":60386,\"start\":60331},{\"end\":60801,\"start\":60703},{\"end\":61453,\"start\":61341},{\"end\":62212,\"start\":62208},{\"end\":62406,\"start\":62359},{\"end\":62823,\"start\":62819},{\"end\":63048,\"start\":63044},{\"end\":63172,\"start\":63145},{\"end\":63493,\"start\":63403},{\"end\":64135,\"start\":64116},{\"end\":44187,\"start\":44128},{\"end\":44680,\"start\":44612},{\"end\":45304,\"start\":45255},{\"end\":45745,\"start\":45681},{\"end\":46073,\"start\":46020},{\"end\":46342,\"start\":46294},{\"end\":46574,\"start\":46505},{\"end\":47061,\"start\":47012},{\"end\":47592,\"start\":47524},{\"end\":48086,\"start\":48028},{\"end\":48577,\"start\":48572},{\"end\":48816,\"start\":48720},{\"end\":49642,\"start\":49634},{\"end\":50087,\"start\":50003},{\"end\":50581,\"start\":50529},{\"end\":50997,\"start\":50993},{\"end\":51289,\"start\":51263},{\"end\":51867,\"start\":51806},{\"end\":52268,\"start\":52212},{\"end\":52740,\"start\":52681},{\"end\":52972,\"start\":52919},{\"end\":53351,\"start\":53295},{\"end\":53623,\"start\":53571},{\"end\":53946,\"start\":53875},{\"end\":54374,\"start\":54336},{\"end\":54848,\"start\":54784},{\"end\":55246,\"start\":55194},{\"end\":55609,\"start\":55557},{\"end\":55948,\"start\":55910},{\"end\":56201,\"start\":56143},{\"end\":56746,\"start\":56659},{\"end\":57543,\"start\":57538},{\"end\":58246,\"start\":58227},{\"end\":58676,\"start\":58627},{\"end\":59099,\"start\":59012},{\"end\":59742,\"start\":59703},{\"end\":60139,\"start\":60044},{\"end\":60386,\"start\":60331},{\"end\":60801,\"start\":60703},{\"end\":61453,\"start\":61341},{\"end\":62212,\"start\":62208},{\"end\":62406,\"start\":62359},{\"end\":62823,\"start\":62819},{\"end\":63048,\"start\":63044},{\"end\":63172,\"start\":63145},{\"end\":63493,\"start\":63403},{\"end\":64135,\"start\":64116}]"}}}, "year": 2023, "month": 12, "day": 17}