{"id": 1688357, "updated": "2023-09-29 05:30:49.282", "metadata": {"title": "Modeling Context in Referring Expressions", "authors": "[{\"first\":\"Licheng\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Patrick\",\"last\":\"Poirson\",\"middle\":[]},{\"first\":\"Shan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Berg\",\"middle\":[\"C.\"]},{\"first\":\"Tamara\",\"last\":\"Berg\",\"middle\":[\"L.\"]}]", "venue": "ECCV", "journal": "69-85", "publication_date": {"year": 2016, "month": 7, "day": 31}, "abstract": "Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring expression generation and comprehension.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1608.00272", "mag": "2949107813", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/YuPYBB16", "doi": "10.1007/978-3-319-46475-6_5"}}, "content": {"source": {"pdf_hash": "2b69ccc5dc0cd2f73352fc866005a41043564a9c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1608.00272v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1608.00272", "status": "GREEN"}}, "grobid": {"id": "3eb56fb5d06e1aef8b95f444486cc59d37b94cef", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2b69ccc5dc0cd2f73352fc866005a41043564a9c.txt", "contents": "\nModeling Context in Referring Expressions\n\n\nLicheng Yu licheng@cs.unc.edu \nDepartment of Computer Science\nUniversity of North Carolina at Chapel Hill\n\n\nPatrick Poirson poirson@cs.unc.edu \nDepartment of Computer Science\nUniversity of North Carolina at Chapel Hill\n\n\nShan Yang \nDepartment of Computer Science\nUniversity of North Carolina at Chapel Hill\n\n\nAlexander C Berg aberg@cs.unc.edu \nDepartment of Computer Science\nUniversity of North Carolina at Chapel Hill\n\n\nTamara L Berg tlberg@cs.unc.edu \nDepartment of Computer Science\nUniversity of North Carolina at Chapel Hill\n\n\nModeling Context in Referring Expressions\nlanguagelanguage and visiongenerationreferring expres- sion generation\nHumans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images.In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets -RefCOCO, RefCOCO+, and RefCOCOg 1 , shows the advantages of our methods for both referring expression generation and comprehension.\n\nIntroduction\n\nIn this paper, we look at the dual-tasks of generating and comprehending natural language expressions referring to particular objects within an image. Referring to objects is a natural and common experience. For example, one often uses referring expressions in everyday speech to indicate a particular person or object to a co-observer, e.g., \"the man in the red hat\" or \"the book on the table\". Computational models to generate and comprehend such expressions would have applicability to human-computer interactions, especially for agents such as robots, interacting with humans in the physical world.\n\nSuccessful models will have to connect both recognition of visual attributes of objects and effective natural language generation to compose useful expressions for dialogue. A broader version of this latter goal was considered in 1975 by Paul Grice who introduced maxims describing cooperative conversation between people [11]. These maxims, called the Gricean Maxims, describe a set of rational principles for natural language dialogue interactions. The 4 maxims are: quality (try to be truthful), quantity (make your contribution as informative as you can, giving as much information as is needed but no more), relevance (be relevant and pertinent to the discussion), and manner (be as clear, brief, and orderly as possible, avoiding obscurity and ambiguity).  For the purpose of referring to objects in complex real world scenes these maxims suggest that a well formed expression should be informative, succinct, and unambiguous. The last point is especially necessary for referring to objects in the real world since we often find multiple objects of a particular category situated together in a scene. For example, consider the image in Fig. 1 which contains three giraffes. We should not refer to the target (outlined in green) as \"the spotted giraffe\" since all of the giraffes are spotted and this would create an ambiguous reference. More reasonably we should refer to the target as \"the giraffe with lowered head\" to differentiate this giraffe from the other two.\n\nThe task of referring expression generation (REG) has been studied since the 1970s [40,22,30,7], with most work focused on studying particular aspects of the problem in some relatively constrained datasets. Recent approaches have pushed this work toword more realistic scenarios. Kazemzadeh et al [19] introduced the first large-scale dataset of referring expressions for objects in real-world natural images, collected in a two-player game. This dataset was originally collected on top of the 20,000 image ImageCleft dataset, but has recently been extended to images from the MSCOCO collection. We make use of the RefCOCO and RefCOCO+ datasets in our work along with another recently collected referring expression dataset, released by Google, denoted in our paper as RefCOCOg [26].\n\nThe most relevant work to ours is Mao et al [26] which introduced the first deep learning approach to REG. In this model, the authors use a Convolutional Neural Network (CNN) [36] model pre-trained on ImageNet [34] to extract visual features from a bounding box around the target object and from the entire image. They use these features plus 5 features encoding the target object location and size as input to a Long Short-term Memory (LSTM) [10] network that generates expressions. Additionally, they apply the same model to the inverse problem of referring expression comprehension where the input is a natural language expression and the goal is to localize the referred object in the image.\n\nSimilar to these recent methods, we also take a deep learning approach to referring expression generation and comprehension. However, while they use a generic model for object context -CNN features for the entire image containing the target object -we take a more focused approach to encode object comparisons. These object comparisons are critical for producing an unambiguous referring expression since one must consider visual characteristics of similar objects during generation in order to select the most distinct aspects for description. This mimics the process that a human would use to compose a good referring expression for an object, e.g. look at the object, look at other relevant objects, and generate an expression that could be used by a co-observer to unambiguously pick out the target object.\n\nIn addition, for the referring expression generation task, we introduce a method to tie the language generation process together for all depicted objects of the same type. This helps generate a good set of expressions such that the expressions differentiate between objects but are also complementary. For example, we never want to generate the exact same expression for two objects in an image. Alternatively, if we call one object \"the red ball\" then we may desire the expression for the other object to follow the same generation pattern, i.e., \"the blue ball\". Our experimental evaluations show that these visual and linguistic comparisons improve performance over previous state of the art.\n\nIn the rest of our paper, we first describe related work (Sec 2). We then describe our improvements to models for referring expression generation and comprehension (Sec 3), describe 3 referring expression datasets (Sec 4), and perform experimental evaluations on several model variations (Sec 5). Finally we present our conclusions (Sec 6).\n\n\nRelated Work\n\nReferring expressions are closely related to the more general problem of modeling the connection between images and descriptive language. In recent years, this has been studied in the image captioning task [6,37,13,31,23]. There, the aim is to condition the generation of language on the visual information from an image. The wide range of aspects of an image that could be described, and the variety of words that could be chosen for a particular description complicate studying image captioning. Our study of referring expressions is partially motivated by focusing on description for a specific, and more easily evaluated, communication goal. Although our task is somewhat different, we borrow machinery from state of the art caption generation [3,39,27,5,18,21,41] using LSTM to generate captions based on CNN features computed on an input image. Three recent approaches for referring expression generation [26] and comprehension [14,33] also take a deep learning approach. However, we add visual object comparisons and tie together language generation for multiple objects.\n\nReferring expression generation has been studied for many years [40,22,30] in linguistics and natural language processing. These works were limited by data collection and insufficient computer vision algorithms. Together Amazon Mechanical Turk and CNNs have somewhat mitigated these limitations, allowing us to revisit these ideas on large-scale datasets. We still use such work to motivate the architecture of our pipeline. For instance, Mitchell and Jordan et al [30,16]  show the importance of using attributes, Funakoshi et al [8] show the importance of relative relations between objects in the same perceptual group, and Kelleher et al [20] show the importance of spatial relationships. These provide motivation for our modeling choices: when considering a referring expression for an object, the model takes into account the relative spatial location of other objects of the same type and visual comparisons to objects in the same perceptual group.\n\nThe REG datasets of the past were sometimes limited to using computer generated images [38], or relatively small collections of natural objects [29,28,7]. Recently, a large-scale referring expression dataset was collected by Kazemzadeh et al [19] featuring natural objects in the real world. Since then, another three REG datasets based on the object labels in MSCOCO have been collected [19,26]. The availability of large-scale referring expression datasets allows us to train deep learning models. Additionally, our analysis of these datasets motivates our incorporation of visual comparisons between same-type objects, and the need to tie together choices for referring expression generation between objects.\n\n\nModels\n\nWe implement several model variations for referring expression generation and comprehension. The first set of models are recent state of the art deep learning approaches from Mao et al [26]. We use these as our baselines (Sec 3.1). Next, we investigate incorporating better visual context features into the models (Sec 3.2). Finally, we explore methods to jointly produce an entire set of referring expressions for all depicted objects of the same category (Sec 3.3).\n\n\nBaselines\n\nFor comparison, we implement both the baseline and strong model of Mao et al [26]. Both models utilize a pre-trained CNN network to model the target object and its context within the image, and then use a LSTM for generation. In particular, object and context are modeled as features from a CNN trained to recognize 1,000 object categories [36] from ImageNet [34]. Specifically, the visual representation is composed of:\n\n-Target object representation, o i . The object is modeled as features extracted from the VGG-fc7 layer by forwarding its bounding box through the network. -Global context representation, g i . Context is modeled as features extracted from the VGG-fc7 layer for the entire image. -Location/size representation, l i , for the target object. Location and size are modeled as a 5-d vector encoding the x and y locations of the top left and bottom right corners of the target object bounding box, as well as the bounding box size with respect to the image, i.e.,\nl i = [ x tl W , y tl H , x br W , y br H , w\u00b7h W \u00b7H ].\nLanguage generation is handled by a long short-term memory network (LSTM) [10] where inputs are the above visual features and the network is trained to generate natural language referring expressions. In Mao et al's baseline [26], the model uses maximum likelihood training and outputs the most likely referring expression given the target object, context, and location/size features. In addition, they also propose a stronger model that uses maximum mutual information (MMI) training to consider whether a listener would interpret a referring expression unambiguously. They impose this by penalizing the model if a generated referring expression could also be generated by some other object within the image. We implement both their original model and MMI model in our experiments. We subsequently refer to these two models as Baseline and MMI, respectively.\n\n\nVisual Comparison\n\nPrevious works [2,30] have shown that objects in an image, of the same type as the target object, are most important for influencing what attributes people use to describe the target. One drawback of considering a general feature over the entire image to encode context (as in the baseline models) is that it may not specifically focus on visual comparisons to the most relevant objects -the other objects of the same object category within the image.\n\nIn this paper, we propose a more explicit encoding of the visual difference between objects of the same category within an image. This helps for generating referring expressions which best discriminate the target object from the surrounding objects. For example, in an image with three cars, two blue and one red, visual appearance comparisons could help generate \"the red car\" as an expression for the latter object.\n\nGiven the referred object and its surrounding objects, we compute two types of features for visual comparison. The first type encodes the similarities and differences in visual appearance between the target object and other objects of the same cateogry depicted in the image. Inspired by Sadeghi et al [35], we compute the difference in visual CNN features as our representation of relative appearance. Because there may be many surrounding objects of the same type in the image, and not every object will provide useful information about how to describe the target object, we need to first select which objects to compare and aggregate their visual differences. In Section 5, we experiment with selecting different subsets of comparison objects: objects of the same category, objects of different category, or all other depicted objects. For each selected comparison object, we compute the appearance difference as the subtraction of the target object and comparison object CNN representations. We experiment with three different strategies for computing an aggregate vector to represent the visual difference between the target object and the surrounding objects: minimum, maximum, and average over each feature dimension. In our experiments, pooling the average difference between the target object and surrounding objects seems to work best. Therefore, we use this pooling in all experiments.\n-Visual appearance difference representation, \u03b4v i = 1 n j =i oi\u2212oj oi\u2212oj ,\nwhere n is the number of objects chosen for comparisons and we use average pooling to aggregate the differences.\n\nThe second type of comparison feature encodes the relative location and size differences between the target object and surrounding objects of the same object category. People often use comparative size or location terms in referring expressions, e.g. \"the second giraffe from the left\" or \"the smaller monkey\" [38]. To address the dynamic number of nearby objects, we choose up to five comparison objects of the same category as the target object, sorted by distance to the target. When fewer than five objects of the same category are depicted, this 25-d vector (5-d x 5 surrounding objects) is padded with zeros.\n-Location difference representation, \u03b4l i , where each 5-d difference is computed as \u03b4l ij = [ [ x tl ]ij wi , [ y tl ]ij hi , [ x br ]ij wi , [ y br ]ij hi , wj\nhj wihi ]. In summary, our final visual representation for a target object is:\nr i = W m [o i , g i , l i , \u03b4v i , \u03b4l i ] + b m(1)\nwhere o i , g i , l i are the target object, global context, and location/size features from the baseline model, \u03b4v i and \u03b4l i encodes visual appearance difference and location difference. W m and b m project the concatenation of the five types of features to be the final representation.\n\n\nJoint Language Generation\n\nFor the referring expression generation task, rather than generating sentences for each object in an image separately [15][26], we consider tying the generation process together into a single task to jointly generate expressions for all objects of the same object category depicted in an image. This makes sense intuitively -when a person attempts to generate a referring expression for an object in an image they inherently compose that expression while keeping in mind expressions for the other objects in the picture. This can be observed in the fact that the expressions people generate for objects in an image tend to share similar patterns of expression. If you say \"the man on the left\" for one object then you tend to say \"the man on the right\" for the other object. We would like our algorithms to mimic these behaviors. Additionally, the algorithm should also be able to push generated expressions away from each other to create less ambiguous references.\n\nFor example, if we use the word \"red\" to describe one object, then we probably shouldn't use the same word to describe another object.\n\nTo model this joint generation process, we model generation using an LSTM model where in addition to the usual connections between time steps within an expression we also add connections between expressions for different objects. This architecture is illustrated in Fig 2. Specifically, we use LSTM to generate multiple referring expressions, {r i }, given depicted objects of the same type, {o j }.\nP (R|O) = i P (r i |o i , {o j =i }, {r j =i }), = i t P (w it |w it\u22121 , ..., w i1 , v i , {h jt,j =i })(2)\nwhere w it are words at time t, v i visual representations, and h jt is the hidden output of j-th object at time step t that encodes the visual and sentence information for the j-th object. As visual comparison, we aggregate the difference of hidden outputs to push away ambiguous information.\nh difi t = 1 n j =i hi t \u2212hj t hi t \u2212hj t .\nThere, n is the the number of other objects of the same type. The hidden difference is jointly embedded with the target object's hidden output, and forwarded to the softmax layer for predicting the word.\nP (w it |w it\u22121 , ..., w i1 , v i , {h jt,j =i }) = softmax(W h [h it , h difi t ] + b h )(3)\n\nData\n\nWe make use of 3 referring expression datasets in our work, all collected on top of the Microsoft COCO image collection [24]. One dataset, RefCOCOg [26] is collected in a non-interactive setting, while the other two datasets, RefCOCO and RefCOCO+, are collected interactively in a two-player game [19]. In the following, we describe each dataset and provide some analysis of their similarities and differences, and then discuss splits of the datasets used in our experiments .\n\n\nDatasets & Analysis\n\nImages for each dataset were selected to contain multiple objects of the same category (object categories depicted cover the 80 common objects from MSCOCO with ground-truth segmentation). These images provide useful cases for referring expression generation since the referrer needs to compose a referring expression that uniquely singles out one object from other relevant objects.\n\nRefCOCOg: This dataset was collected on Amazon Mechanical Turk in a non-interactive setting. One set of workers were asked to write natural language referring expressions for objects in MSCOCO images then another set of workers were asked to click on the indicated object given a referring expression. If the click overlapped with the correct object then the referring expression was considered valid and added to the dataset. If not, another referring expression was collected for the object. This dataset consists of 85,474 referring expressions for 54,822 objects in 26,711 images. Images were selected to contain between 2 and 4 objects of the same object category.\n\nRefCOCO & RefCOCO+: These datasets were collected using the Refer-itGame [19]. In this two-player game, the first player is shown an image with a segmented target object and asked to write a natural language expression referring to the target object. The second player is shown only the image and the referring expression and asked to click on the corresponding object. If the players do their job correctly, they receive points and swap roles. If not, they are presented with a new object and image for description. Images in these collections were selected to contain two or more objects of the same object category. In the RefCOCO dataset, no restrictions are placed on the type of language used in the referring expressions while in the RefCOCO+ dataset players are disallowed from using location words in their referring expressions by adding \"taboo\" words to the ReferItGame. This dataset was collected to obtain a referring expression dataset focsed on purely appearance based description, e.g., \"the man in the yellow polka-dotted shirt\" rather than \"the second man from the left\", which tend to be more interesting from a computer vision based perspective and are independent of viewer perspective. RefCOCO consists of 142,209 refer expressions for 50,000 objects in 19,994 images, and RefCOCO+ has 141,564 expressions for 49,856 objects in 19,992 images.\n\nDataset Comparisons: As shown in Fig. 1, the languages used in RefCOCO and RefCOCO+ datasets tend to be more concise and less flowery than the languages used in the RefCOCOg. RefCOCO expressions have an average length of 3.61 while RefCOCO+ have an average length of 3.53, and RefCOCOg contain an average of 8.43 words. This is most likely due to the differences in collection strategy. RefCOCO and RefCOCO+ were collected in a game scenario where players are trying to efficiently provide enough information to indicate the correct object to the other player. RefCOCOg was collected in independent rounds of Mechanical Turk without any interactive time constraints and therefore tend to provide more complex expressions, often entire sentences rather than phrases.\n\nIn addition, RefCOCO and RefCOCO+ do not limit the number of objects of the same type to 4 and thus contain some images with many objects of the same type. Both RefCOCO and RefCOCO+ contain an average of 3.9 sametype objects per image, while RefCOCOg contains an average of 1.63 sametype objects per image. The large number of same-type objects per image in RefCOCO and RefCOCO+ suggests that incorporating visual comparisons to same-type objecs will be useful.\n\nDataset Splits: There are two types of splits of the data into train/test sets: a per-object split and a people-vs-objects split.\n\nThe first type is per-object split. In this split, the dataset is divided by randomly partitioning objects into training and testing sets. This means that each object will only appear either in training or testing set, but that one object from an image may appear in the training set while another object from the same image may appear in the test set. We use this split for RefCOCOg since same division was used in the previous state-of-the-art approach [26].\n\nThe second type is people-vs-objects splits. One thing we observe from analyzing the datasets is that about half of the referred objects are people. Therefore, we create a split for RefCOCO and RefCOCO+ datasets that evaluates images containing multiple people (testA) vs images containing multiple instances of all other objects (testB). In this split all objects from an image will appear either in the training or testing sets, but not both. This split creates a more meaningfully separated division between training and testing, allowing us to evaluate the usefulness of context more fairly.\n\n\nExperiments\n\nWe first perform some experiments to analyze the use of context in referring expressions (Sec 5.1). Given these findings, we then perform experiments evaluating the usefulness of our proposed visual and language innovations on the comprehension (Sec 5.2) and generation tasks (Sec 5.3).\n\nIn experiments for the referring expression comprehension task, we use the same evaluation as Mao et al [26], namely we first predict the region referred by the given expression, then we compute the intersection over union (IOU) ratio between the true and predicted bounding box. If the IOU is larger than 0.5 we count it as a true positive. Otherwise, we count it as a false positive. We average this score over all images. For the referring expression generation task we use automatic evaluation metrics, BLEU, ROUGE, and METEOR developed for evaluating machine translation results, commonly used to evaluate language generation results [41,18,5,27,39,23]. We further perform human evaluations, and propose a new metric evaluating the duplicate rate of generated expressions. For both tasks, we compare our models with \"Baseline\" and \"MMI\" [26]. Specifically, we denote \"visdif\" as our visual comparison model, and \"tie\" as the LSTM tying model. We also perform an ablation study, evaluating the combinations.\n\n\nAnalysis Experiments\n\nContext Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works [26,14] make use of relatively weak contextual information, by only considering a single global image context for all objects. To verify this intuition, we implemented both the baseline and strong MMI models from Mao et al [26], and compare the results for referring expression comprehension task with and without global context on RefCOCO and Refcoco+ in Table 1. Surprisingly we find that the global context does not improve the performance of the model. In fact, adding context even decreases performance slightly. This may be due to the fact that the global context for each object in an image would be the same, introducing some ambiguity into the referring expression comprehension task. Given these findings, we implemented a simple modification to the global context, computing the same visual representation, but on a somewhat scaled window centered around the target object.\n\nWe found this to improve performance, suggesting room for improving the visual context feature. This motivate our development of a better context feature.\n\nVisual Comparison For our visual comparison model, there could be several choices regarding which objects from the image should be compared to the target object. We experiment with three sets of reference objects on RefCOCO and RefCOCO+ datasets: a) objects of the same-category in the image, b) objects of different-category in the image, and c) all objects appeared in the image. We use our \"visdif\" model for this experiment. The results are shown in Figure 3. It is clear to see the visual comparisons to the same-category objects are most useful for referring expression comprehension task. This is more like mimicing how human refer object -we tend to point out the difference between the target object with the other same-category objects within the same image. Fig. 3. Comprehension accuracies on RefCOCO and RefCOCO+ datasets. We compare the performance of \"visdif\" model without visual comparison, and visual comparison between different-category objects, between all objects, and between same-type objects.\n\n\nReferring Expression Comprehension\n\nWe evaluate performance on the referring expression comprehension task on Re-fCOCO, RefCOCO+ and RefCOCOg datasets. For RefCOCO and RefCOCO+, we evaluate on the two subsets of people (testA) and all other objects (testB). For RefCOCOg, we evaluate on the per-object split as previous work [26]. Since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper-parameters on RefCOCO. Table 2 shows the comprehension accuracies. We observe that our implementation of Mao et al [26] achieves comparable performance to the numbers reported in their paper. We also find that adding visual comparison features to the Baseline model improves performance across all datasets and splits. Similar improvements are also observed on top of the MMI model. In order to make a fully automatic referring system, we also train a Fast-RCNN [9] detector and build our system on top of the detections. We train Fast-RCNN on the validation portion only as the RefCOCO and RefCOCO+ are collected using MSCOCO training data. For RefCOCOg, we use the detection results provided by [26], which were trained uisng Multibox [4]. Results on shown in the bottom half of Table 2. Although all comprehension accuracies drop due to imperfect detections, the improvements of our models over Baseline and MMI are still observed. One weakness of our automatic system is that it highly depends on detection performance, especially for general objects (testB). However, considering our detector was trained on MSCOCO validation only, we believe such weakness may be alleviated with more training data and stronger detection techniques, e.g., [12] \n\n\nReferring Expression Generation\n\nFor the referring expression generation task, we evaluate the usefulness of our visual comparison features as well as our joint language generation model. These serve to tie the generation process together so that the model considers other objects of the same type both visually and linguistically during generation. On the visual side, comparisons are used to judge similarity of the target object to other objects of the same type in terms of appearance, size and location. On the language side, the joint LSTM model serves to both differentiate and mimic language patterns in the referring expressions for the entire set of depicted objects. Fig 5 shows some comparison between our model with other methods.\n\nOur full results are shown in Table 3. We find that incorporating our visual comparison features into the Baseline model improves generation quality (compare row \"Baseline\" to row \"visdif\"). It also improves the performance of MMI model (compare row \"MMI\" to row \"visdif+MMI\"). We also observe that tying the language generation together across all objects consistently improves the performance (compare the bottom three \"+tie\" rows with the above). Especially for method \"visdif+tie\", it achieves the highest score under almost every measurement. We do not perform language tying on RefCOCOg since here some objects from an image may appear in training while others may appear in testing. We observe in Table 3 that models incoporating \"+MMI\" are worse than without \"+MMI\" under the automatic scoring metrics. To verify whether these metrics really reflect performance, we performed human evaluations on the expression generation task. Three Turkers were asked to click on the referred object  Table 5. Fraction of images for which the algorithm generates the same referring expression for multiple objects. Smaller is better.\n\n\nRefCOCO\n\ngiven the image and the generated expression. If more than two clicked on the true target object, we consider this expression to be correct. Table 4 shows the human evaluation results, indicating that models with \"+MMI\" are consistently higher performance. We also find \"+tie\" methods perform the best, indicating that tying language together is able to produce less ambiguous referring expressions. Some referring expression generation examples using different methods are shown in Fig 5. Besides, we show more examples of tied generations using \"visdif+MMI+tie\" model in Fig 6. Finally, we introduce another evaluation metric which measures the fraction of images for which an algorithm produces the same generated referring expression for multiple objects within the image. Obviously, a good referring expression generator should never produce the same expressions for two objects within the same image. Thus we would like this number to be as small as possible. The evaluation results under such metric are shown in Table 5. We find \"+MMI\" produces smaller number of duplicated expressions on both RefCOCO and Re-fCOCO+, while \"+tie\" helps generating even more different expressions. Our combined model \"visdif+MMI+tie\" performs the best under this metric.\n\n\nConclusion\n\nIn this paper, we have developed a new model for incorporating detailed context into referring expression models. With this visual comparison based context we have improved performance over previous state of the art for referring expression generation and comprehension. In addition, for the referring expression generation task, we explore methods for joint generation over all relevant objects. Experiments verify that this joint generation improves results over previous attempts to reduce ambiguity during generation.   \n\n\nan)adult)giraffe)scratching)its) back)with)its)horn 2.)giraffe)hugging)another)giraffe\n\nFig. 1 .\n1Example referring expressions for the giraffe outlined in green from three referring expression datasets (described in Sec 4).\n\nFig. 4 .\n4Referring expression comprehension on RefCOCO and RefCOCO+ using \"visdif\" based on detections. The blue and red bounding boxes are correct and incorrect comprehension respectively, while the green boxes indicate the ground-truth regions.\n\nFig. 5 .\n5Referring expression generation on RefCOCO and RefCOCO+ by different methods.\n\nFig. 6 .\n6Joint referring expression generation using our full model of \"visdif+MMI+tie\".\n\n\n[25][32][17][1], etc. We show some automatic comprehension examples of RefCOCO, RefCOCO+ and RefCOCOg in Fig 4, where top three rows show correct comprehensions (object correctly localized) and bottom three rows show incorrect comprehensions (wrong object localized).RefCOCO \nRefCOCO+ \nRefCOCOg \nTest A \nTest B \nTest A Test B \nValidation \nBaseline[26] \n63.15% 64.21% 48.73% 42.13% \n55.16% \nvisdif \n67.57% 71.19% 52.44% 47.51% \n59.25% \nMMI[26] \n71.72% 71.09% 58.42% 51.23% \n62.14% \nvisdif+MMI \n73.98% 76.59% 59.17% 55.62% \n64.02% \n\nBaseline(det)[26] 58.32% 48.48% 46.86% 34.04% \n40.75% \nvisdif(det) \n62.50% 50.80% 50.10% 37.48% \n41.85% \nMMI(det)[26] \n64.90% 54.51% 54.03% 42.81% \n45.85% \nvisdif+MMI(det) 67.64% 55.16% 55.81% 43.43% \n46.86% \n\nTable 2. Referring Expression comprehension results on the RefCOCO, RefCOCO+, \nand RefCOCOg datasets. Rows of \"method(det)\" are the results of automatic system \nbuilt on Fast-RCNN [9] and Multibox [4] detections. \n\n\n\nTable 3. Referring Expression Generation Results: Bleu, Rouge, Meteor evaluations for RefCOCO, RefCOCO+ and RefCOCOg.Test A \nTest B \nBleu 1 \nBleu 2 \nRouge Meteor \nBleu 1 \nBleu 2 \nRouge Meteor \nBaseline [26] \n0.477 \n0.290 \n0.413 \n0.173 \n0.553 \n0.343 \n0.499 \n0.228 \nMMI [26] \n0.478 \n0.295 \n0.418 \n0.175 \n0.547 \n0.341 \n0.497 \n0.228 \nvisdif \n0.505 \n0.322 \n0.441 \n0.184 \n0.583 \n0.382 \n0.530 \n0.245 \nvisdif+MMI \n0.494 \n0.307 \n0.441 \n0.185 \n0.578 \n0.375 \n0.531 \n0.247 \nBaseline+tie \n0.490 \n0.308 \n0.431 \n0.181 \n0.561 \n0.352 \n0.505 \n0.234 \nvisdif+tie \n0.510 \n0.318 \n0.446 \n0.189 \n0.593 \n0.386 \n0.533 \n0.249 \nvisdif+MMI+tie 0.506 \n0.312 \n0.445 \n0.188 \n0.579 \n0.370 \n0.525 \n0.246 \nRefCOCO+ \nTest A \nTest B \nBleu 1 \nBleu 2 \nRouge Meteor \nBleu 1 \nBleu 2 \nRouge Meteor \nBaseline [26] \n0.391 \n0.218 \n0.356 \n0.140 \n0.331 \n0.174 \n0.322 \n0.135 \nMMI [26] \n0.370 \n0.203 \n0.346 \n0.136 \n0.324 \n0.167 \n0.320 \n0.133 \nvisdif \n0.407 \n0.235 \n0.363 \n0.145 \n0.339 \n0.177 \n0.325 \n0.145 \nvisdif+MMI \n0.386 \n0.221 \n0.360 \n0.142 \n0.327 \n0.172 \n0.325 \n0.135 \nBaseline+tie \n0.392 \n0.219 \n0.361 \n0.143 \n0.336 \n0.177 \n0.325 \n0.140 \nvisdif+tie \n0.409 \n0.232 \n0.372 \n0.150 \n0.340 \n0.178 \n0.328 \n0.143 \nvisdif+MMI+tie 0.393 \n0.220 \n0.360 \n0.142 \n0.327 \n0.175 \n0.321 \n0.137 \nRefCOCOg \nvalidation \nBleu 1 Bleu 2 Rouge Meteor \nBaseline [26] 0.437 0.273 0.363 0.149 \nMMI [26] \n0.428 0.263 0.354 0.144 \nvisdif \n0.442 0.277 0.370 0.151 \nvisdif+MMI 0.430 0.262 0.356 0.145 \n\n\n\nTable 4 .\n440% 76.14% 57.17% 47.92% visdif+MMI+tie 70.01% 76.31% 55.64% 48.04% Human Evaluations on referring expression generation.RefCOCO \nRefCOCO+ \nTest A \nTest B \nTest A Test B \nBaseline [26] \n62.42% 64.99% 49.18% 42.03% \nMMI \n65.76% 68.25% 49.84% 45.38% \nvisdif \n68.27% 74.92% 55.20% 43.65% \nvisdif+MMI \n70.25% 75.47% 53.56% 47.58% \nBaseline+tie \n64.51% 68.34% 52.06% 43.53% \nvisdif+tie \n71.RefCOCO \nRefCOCO+ \nTest A \nTest B \nTest A Test B \nBaseline [26] \n15.60% 16.40% 28.67% 46.27% \nMMI \n11.60% 11.73% 21.07% 26.40% \nvisdif \n9.20% \n8.80% \n19.60% 31.07% \nvisdif+MMI \n5.07% \n6.13% \n12.13% 16.00% \nBaseline+tie \n11.20% 14.93% 22.00% 32.13% \nvisdif+tie \n4.27% \n5.33% \n11.73% 16.27% \nvisdif+MMI+tie 6.53% \n4.53% 10.13% 13.33% \n\n\nDatasets and toolbox can be downloaded from https://github.com/lichengunc/refer\nAcknowledgements:We thank Junhua Mao, Dequan Wang and Varun K. Nagaraja for helpful discussions. This research is supported by NSF Grants #1444234, 1445409, 1405822, and Microsoft.\nS Bell, C L Zitnick, K Bala, R Girshick, Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. Bell, S., Zitnick, C.L., Bala, K., Girshick, R.: Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2015)\n\nWatching the eyes when talking about size: An investigation of message formulation and utterance planning. S Brown-Schmidt, M K Tanenhaus, Journal of Memory and Language. 544Brown-Schmidt, S., Tanenhaus, M.K.: Watching the eyes when talking about size: An investigation of message formulation and utterance planning. Journal of Mem- ory and Language 54(4), 592-609 (2006)\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, Anne Hendricks, L Guadarrama, S Rohrbach, M Venugopalan, S Saenko, K Darrell, T , CVPRDonahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: CVPR (2015)\n\nScalable object detection using deep neural networks. D Erhan, C Szegedy, A Toshev, D Anguelov, CVPRErhan, D., Szegedy, C., Toshev, A., Anguelov, D.: Scalable object detection using deep neural networks. In: CVPR (2014)\n\nH Fang, S Gupta, F Iandola, R K Srivastava, L Deng, P Doll\u00e1r, J Gao, X He, M Mitchell, J C Platt, From captions to visual concepts and back. CVPRFang, H., Gupta, S., Iandola, F., Srivastava, R.K., Deng, L., Doll\u00e1r, P., Gao, J., He, X., Mitchell, M., Platt, J.C., et al.: From captions to visual concepts and back. In: CVPR (2015)\n\nEvery picture tells a story: Generating sentences from images. A Farhadi, M Hejrati, M A Sadeghi, P Young, C Rashtchian, J Hockenmaier, D Forsyth, ECCVFarhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every picture tells a story: Generating sentences from images. In: ECCV (2010)\n\nLearning distributions over logical forms for referring expression generation. N Fitzgerald, Y Artzi, L S Zettlemoyer, EMNLP. pp. FitzGerald, N., Artzi, Y., Zettlemoyer, L.S.: Learning distributions over logical forms for referring expression generation. In: EMNLP. pp. 1914-1925 (2013)\n\nGenerating referring expressions using perceptual groups. K Funakoshi, S Watanabe, N Kuriyama, T Tokunaga, Natural Language Generation. SpringerFunakoshi, K., Watanabe, S., Kuriyama, N., Tokunaga, T.: Generating referring expressions using perceptual groups. In: Natural Language Generation, pp. 51-60. Springer (2004)\n\nFast r-cnn. R Girshick, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionGirshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1440-1448 (2015)\n\nK Greff, R K Srivastava, J Koutn\u00edk, B R Steunebrink, J Schmidhuber, arXiv:1503.04069Lstm: A search space odyssey. arXiv preprintGreff, K., Srivastava, R.K., Koutn\u00edk, J., Steunebrink, B.R., Schmidhuber, J.: Lstm: A search space odyssey. arXiv preprint arXiv:1503.04069 (2015)\n\nLogic and conversation. H P Grice, Syntax and Semantics. Cole, P., Morgan, J.L.San Diego, CAAcademic Press3Speech ActsGrice, H.P.: Logic and conversation. In: Cole, P., Morgan, J.L. (eds.) Syntax and Semantics: Vol. 3: Speech Acts, pp. 41-58. Academic Press, San Diego, CA (1975)\n\nK He, X Zhang, S Ren, J Sun, Deep residual learning for image recognition. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n\nFraming image description as a ranking task: Data, models and evaluation metrics. M Hodosh, P Young, J Hockenmaier, Journal of Artificial Intelligence Research. Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a rank- ing task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research (2013)\n\nNatural language object retrieval. R Hu, H Xu, M Rohrbach, J Feng, K Saenko, T Darrell, CVPRHu, R., Xu, H., Rohrbach, M., Feng, J., Saenko, K., Darrell, T.: Natural language object retrieval. In: CVPR (2016)\n\nJ Johnson, A Karpathy, L Fei-Fei, arXiv:1511.07571Densecap: Fully convolutional localization networks for dense captioning. arXiv preprintJohnson, J., Karpathy, A., Fei-Fei, L.: Densecap: Fully convolutional localization networks for dense captioning. arXiv preprint arXiv:1511.07571 (2015)\n\nLearning attribute selections for non-pronominal expressions. P Jordan, M Walker, ACLJordan, P., Walker, M.: Learning attribute selections for non-pronominal expres- sions. In: ACL (2000)\n\nDeepm: A deep part-based model for object detection and semantic part localization. Jun Zhu, X C Yuille, A , Jun Zhu, X.C., Yuille, A.: Deepm: A deep part-based model for object detection and semantic part localization (2015)\n\nDeep visual-semantic alignments for generating image descriptions. A Karpathy, L Fei-Fei, CVPRKarpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image descriptions. In: CVPR (2015)\n\nReferitgame: Referring to objects in photographs of natural scenes. S Kazemzadeh, V Ordonez, M Matten, T L Berg, EMNLP. pp. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.L.: Referitgame: Referring to objects in photographs of natural scenes. In: EMNLP. pp. 787-798 (2014)\n\nIncremental generation of spatial referring expressions in situated dialog. J D Kelleher, G J M Kruijff, ACLKelleher, J.D., Kruijff, G.J.M.: Incremental generation of spatial referring expres- sions in situated dialog. In: ACL (2006)\n\nUnifying visual-semantic embeddings with multimodal neural language models. R Kiros, R Salakhutdinov, R S Zemel, TACL. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings with multimodal neural language models. TACL (2015)\n\nComputational generation of referring expressions: A survey. E Krahmer, K Van Deemter, Computational Linguistics. 381Krahmer, E., Van Deemter, K.: Computational generation of referring expressions: A survey. Computational Linguistics 38(1), 173-218 (2012)\n\nBabytalk: Understanding and generating simple image descriptions. Pattern Analysis and Machine Intelligence. G Kulkarni, V Premraj, V Ordonez, S Dhar, S Li, Y Choi, A C Berg, T Berg, IEEE Transactions. Kulkarni, G., Premraj, V., Ordonez, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg, T.: Babytalk: Understanding and generating simple image descriptions. Pattern Analysis and Machine Intelligence, IEEE Transactions on (2013)\n\nMicrosoft coco: Common objects in context. T Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, ECCVLin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)\n\nW Liu, D Anguelov, D Erhan, C Szegedy, S Reed, Ssd: Single shot multibox detector. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.: Ssd: Single shot multibox detector (2015)\n\nGeneration and comprehension of unambiguous object descriptions. J Mao, J Huang, A Toshev, O Camburu, A Yuille, K Murphy, CVPRMao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A., Murphy, K.: Generation and comprehension of unambiguous object descriptions. In: CVPR (2016)\n\nJ Mao, W Xu, Y Yang, J Wang, Z Huang, A Yuille, Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR. Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., Yuille, A.: Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR (2015)\n\nNatural reference to objects in a visual domain. M Mitchell, K Van Deemter, E Reiter, Proceedings of the 6th international natural language generation conference. the 6th international natural language generation conferenceAssociation for Computational LinguisticsMitchell, M., van Deemter, K., Reiter, E.: Natural reference to objects in a vi- sual domain. In: Proceedings of the 6th international natural language generation conference. pp. 95-104. Association for Computational Linguistics (2010)\n\nM Mitchell, E Reiter, K Van Deemter, Typicality and object reference. Cognitive Science (CogSci. Mitchell, M., Reiter, E., van Deemter, K.: Typicality and object reference. Cogni- tive Science (CogSci) (2013)\n\nGenerating expressions that refer to visible objects. M Mitchell, K Van Deemter, E Reiter, HLT-NAACL. Mitchell, M., Van Deemter, K., Reiter, E.: Generating expressions that refer to visible objects. In: HLT-NAACL. pp. 1174-1184 (2013)\n\nIm2text: Describing images using 1 million captioned photographs. V Ordonez, G Kulkarni, T L Berg, Advances in Neural Information Processing Systems. Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million captioned photographs. In: Advances in Neural Information Processing Systems (2011)\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, NIPSRen, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object de- tection with region proposal networks. In: NIPS (2015)\n\nA Rohrbach, M Rohrbach, R Hu, T Darrell, B Schiele, arXiv:1511.03745Grounding of textual phrases in images by reconstruction. arXiv preprintRohrbach, A., Rohrbach, M., Hu, R., Darrell, T., Schiele, B.: Grounding of textual phrases in images by reconstruction. arXiv preprint arXiv:1511.03745 (2015)\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International Journal of Computer Vision. 1153Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog- nition challenge. International Journal of Computer Vision 115(3), 211-252 (2015)\n\nF Sadeghi, C L Zitnick, A Farhadi, Visalogy: Answering visual analogy questions. In: NIPS. Sadeghi, F., Zitnick, C.L., Farhadi, A.: Visalogy: Answering visual analogy ques- tions. In: NIPS (2015)\n\nK Simonyan, A Zisserman, arXiv:1409.1556Very deep convolutional networks for large-scale image recognition. arXiv preprintSimonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)\n\nGrounded compositional semantics for finding and describing images with sentences. R Socher, A Karpathy, Q V Le, C D Manning, A Y Ng, Transactions of the Association for Computational Linguistics. Socher, R., Karpathy, A., Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded compo- sitional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics (2014)\n\nThe use of spatial relations in referring expression generation. J Viethen, R Dale, Proceedings of the Fifth International Natural Language Generation Conference. the Fifth International Natural Language Generation ConferenceAssociation for Computational LinguisticsViethen, J., Dale, R.: The use of spatial relations in referring expression gener- ation. In: Proceedings of the Fifth International Natural Language Generation Conference. pp. 59-67. Association for Computational Linguistics (2008)\n\nShow and tell: A neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, CVPRVinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image caption generator. In: CVPR (2015)\n\nUnderstanding natural language. T Winograd, Cognitive psychology. 31Winograd, T.: Understanding natural language. Cognitive psychology 3(1), 1-191 (1972)\n\nShow, attend and tell: Neural image caption generation with visual attention. K Xu, J Ba, R Kiros, A Courville, R Salakhutdinov, R Zemel, Y Bengio, Xu, K., Ba, J., Kiros, R., Courville, A., Salakhutdinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. ICML (2015)\n", "annotations": {"author": "[{\"end\":152,\"start\":45},{\"end\":265,\"start\":153},{\"end\":353,\"start\":266},{\"end\":465,\"start\":354},{\"end\":575,\"start\":466}]", "publisher": null, "author_last_name": "[{\"end\":55,\"start\":53},{\"end\":168,\"start\":161},{\"end\":275,\"start\":271},{\"end\":370,\"start\":366},{\"end\":479,\"start\":475}]", "author_first_name": "[{\"end\":52,\"start\":45},{\"end\":160,\"start\":153},{\"end\":270,\"start\":266},{\"end\":363,\"start\":354},{\"end\":365,\"start\":364},{\"end\":472,\"start\":466},{\"end\":474,\"start\":473}]", "author_affiliation": "[{\"end\":151,\"start\":76},{\"end\":264,\"start\":189},{\"end\":352,\"start\":277},{\"end\":464,\"start\":389},{\"end\":574,\"start\":499}]", "title": "[{\"end\":42,\"start\":1},{\"end\":617,\"start\":576}]", "venue": null, "abstract": "[{\"end\":1429,\"start\":689}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2375,\"start\":2371},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3611,\"start\":3607},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3614,\"start\":3611},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3617,\"start\":3614},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3619,\"start\":3617},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3825,\"start\":3821},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4306,\"start\":4302},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4357,\"start\":4353},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4488,\"start\":4484},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4523,\"start\":4519},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4756,\"start\":4752},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7081,\"start\":7078},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7084,\"start\":7081},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7087,\"start\":7084},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7090,\"start\":7087},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7093,\"start\":7090},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7623,\"start\":7620},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7626,\"start\":7623},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7629,\"start\":7626},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7631,\"start\":7629},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7634,\"start\":7631},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7637,\"start\":7634},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7640,\"start\":7637},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7787,\"start\":7783},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7810,\"start\":7806},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7813,\"start\":7810},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8020,\"start\":8016},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8023,\"start\":8020},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8026,\"start\":8023},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8421,\"start\":8417},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8424,\"start\":8421},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8486,\"start\":8483},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8598,\"start\":8594},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9000,\"start\":8996},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9057,\"start\":9053},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9060,\"start\":9057},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9062,\"start\":9060},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9155,\"start\":9151},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9301,\"start\":9297},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9304,\"start\":9301},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9820,\"start\":9816},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10193,\"start\":10189},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10456,\"start\":10452},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10475,\"start\":10471},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11227,\"start\":11223},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11378,\"start\":11374},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12048,\"start\":12045},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12051,\"start\":12048},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13208,\"start\":13204},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14803,\"start\":14799},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15837,\"start\":15833},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18093,\"start\":18089},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18121,\"start\":18117},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18270,\"start\":18266},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19601,\"start\":19597},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22710,\"start\":22706},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23720,\"start\":23716},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24255,\"start\":24251},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24258,\"start\":24255},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24260,\"start\":24258},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24263,\"start\":24260},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24266,\"start\":24263},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24269,\"start\":24266},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24458,\"start\":24454},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24777,\"start\":24773},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24780,\"start\":24777},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25000,\"start\":24996},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27164,\"start\":27160},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27419,\"start\":27415},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27765,\"start\":27762},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28001,\"start\":27997},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28040,\"start\":28037},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28549,\"start\":28545}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32325,\"start\":32237},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32463,\"start\":32326},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32712,\"start\":32464},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32801,\"start\":32713},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32892,\"start\":32802},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33850,\"start\":32893},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35282,\"start\":33851},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36014,\"start\":35283}]", "paragraph": "[{\"end\":2047,\"start\":1445},{\"end\":3522,\"start\":2049},{\"end\":4307,\"start\":3524},{\"end\":5004,\"start\":4309},{\"end\":5816,\"start\":5006},{\"end\":6513,\"start\":5818},{\"end\":6855,\"start\":6515},{\"end\":7950,\"start\":6872},{\"end\":8907,\"start\":7952},{\"end\":9620,\"start\":8909},{\"end\":10098,\"start\":9631},{\"end\":10532,\"start\":10112},{\"end\":11092,\"start\":10534},{\"end\":12008,\"start\":11149},{\"end\":12481,\"start\":12030},{\"end\":12900,\"start\":12483},{\"end\":14298,\"start\":12902},{\"end\":14487,\"start\":14375},{\"end\":15103,\"start\":14489},{\"end\":15344,\"start\":15266},{\"end\":15685,\"start\":15397},{\"end\":16680,\"start\":15715},{\"end\":16816,\"start\":16682},{\"end\":17217,\"start\":16818},{\"end\":17619,\"start\":17326},{\"end\":17867,\"start\":17664},{\"end\":18445,\"start\":17969},{\"end\":18851,\"start\":18469},{\"end\":19522,\"start\":18853},{\"end\":20888,\"start\":19524},{\"end\":21655,\"start\":20890},{\"end\":22118,\"start\":21657},{\"end\":22249,\"start\":22120},{\"end\":22711,\"start\":22251},{\"end\":23308,\"start\":22713},{\"end\":23610,\"start\":23324},{\"end\":24623,\"start\":23612},{\"end\":25657,\"start\":24648},{\"end\":25813,\"start\":25659},{\"end\":26832,\"start\":25815},{\"end\":28550,\"start\":26871},{\"end\":29296,\"start\":28586},{\"end\":30425,\"start\":29298},{\"end\":31697,\"start\":30437},{\"end\":32236,\"start\":31712}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11148,\"start\":11093},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14374,\"start\":14299},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15265,\"start\":15104},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15396,\"start\":15345},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17325,\"start\":17218},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17663,\"start\":17620},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17961,\"start\":17868}]", "table_ref": "[{\"end\":25136,\"start\":25129},{\"end\":27330,\"start\":27323},{\"end\":28088,\"start\":28081},{\"end\":29335,\"start\":29328},{\"end\":30009,\"start\":30002},{\"end\":30300,\"start\":30293},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30585,\"start\":30578},{\"end\":31464,\"start\":31457}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1443,\"start\":1431},{\"attributes\":{\"n\":\"2\"},\"end\":6870,\"start\":6858},{\"attributes\":{\"n\":\"3\"},\"end\":9629,\"start\":9623},{\"attributes\":{\"n\":\"3.1\"},\"end\":10110,\"start\":10101},{\"attributes\":{\"n\":\"3.2\"},\"end\":12028,\"start\":12011},{\"attributes\":{\"n\":\"3.3\"},\"end\":15713,\"start\":15688},{\"attributes\":{\"n\":\"4\"},\"end\":17967,\"start\":17963},{\"attributes\":{\"n\":\"4.1\"},\"end\":18467,\"start\":18448},{\"attributes\":{\"n\":\"5\"},\"end\":23322,\"start\":23311},{\"attributes\":{\"n\":\"5.1\"},\"end\":24646,\"start\":24626},{\"attributes\":{\"n\":\"5.2\"},\"end\":26869,\"start\":26835},{\"attributes\":{\"n\":\"5.3\"},\"end\":28584,\"start\":28553},{\"end\":30435,\"start\":30428},{\"attributes\":{\"n\":\"6\"},\"end\":31710,\"start\":31700},{\"end\":32335,\"start\":32327},{\"end\":32473,\"start\":32465},{\"end\":32722,\"start\":32714},{\"end\":32811,\"start\":32803},{\"end\":35293,\"start\":35284}]", "table": "[{\"end\":33850,\"start\":33162},{\"end\":35282,\"start\":33970},{\"end\":36014,\"start\":35416}]", "figure_caption": "[{\"end\":32325,\"start\":32239},{\"end\":32463,\"start\":32337},{\"end\":32712,\"start\":32475},{\"end\":32801,\"start\":32724},{\"end\":32892,\"start\":32813},{\"end\":33162,\"start\":32895},{\"end\":33970,\"start\":33853},{\"end\":35416,\"start\":35295}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3197,\"start\":3191},{\"end\":17090,\"start\":17084},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20929,\"start\":20923},{\"end\":26277,\"start\":26269},{\"end\":26590,\"start\":26584},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29242,\"start\":29231},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30926,\"start\":30920},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31016,\"start\":31010}]", "bib_author_first_name": "[{\"end\":36277,\"start\":36276},{\"end\":36285,\"start\":36284},{\"end\":36287,\"start\":36286},{\"end\":36298,\"start\":36297},{\"end\":36306,\"start\":36305},{\"end\":36677,\"start\":36676},{\"end\":36694,\"start\":36693},{\"end\":36696,\"start\":36695},{\"end\":37026,\"start\":37025},{\"end\":37040,\"start\":37036},{\"end\":37053,\"start\":37052},{\"end\":37067,\"start\":37066},{\"end\":37079,\"start\":37078},{\"end\":37094,\"start\":37093},{\"end\":37104,\"start\":37103},{\"end\":37115,\"start\":37114},{\"end\":37382,\"start\":37381},{\"end\":37391,\"start\":37390},{\"end\":37402,\"start\":37401},{\"end\":37412,\"start\":37411},{\"end\":37549,\"start\":37548},{\"end\":37557,\"start\":37556},{\"end\":37566,\"start\":37565},{\"end\":37577,\"start\":37576},{\"end\":37579,\"start\":37578},{\"end\":37593,\"start\":37592},{\"end\":37601,\"start\":37600},{\"end\":37611,\"start\":37610},{\"end\":37618,\"start\":37617},{\"end\":37624,\"start\":37623},{\"end\":37636,\"start\":37635},{\"end\":37638,\"start\":37637},{\"end\":37943,\"start\":37942},{\"end\":37954,\"start\":37953},{\"end\":37965,\"start\":37964},{\"end\":37967,\"start\":37966},{\"end\":37978,\"start\":37977},{\"end\":37987,\"start\":37986},{\"end\":38001,\"start\":38000},{\"end\":38016,\"start\":38015},{\"end\":38288,\"start\":38287},{\"end\":38302,\"start\":38301},{\"end\":38311,\"start\":38310},{\"end\":38313,\"start\":38312},{\"end\":38555,\"start\":38554},{\"end\":38568,\"start\":38567},{\"end\":38580,\"start\":38579},{\"end\":38592,\"start\":38591},{\"end\":38829,\"start\":38828},{\"end\":39083,\"start\":39082},{\"end\":39092,\"start\":39091},{\"end\":39094,\"start\":39093},{\"end\":39108,\"start\":39107},{\"end\":39119,\"start\":39118},{\"end\":39121,\"start\":39120},{\"end\":39136,\"start\":39135},{\"end\":39383,\"start\":39382},{\"end\":39385,\"start\":39384},{\"end\":39640,\"start\":39639},{\"end\":39646,\"start\":39645},{\"end\":39655,\"start\":39654},{\"end\":39662,\"start\":39661},{\"end\":39887,\"start\":39886},{\"end\":39897,\"start\":39896},{\"end\":39906,\"start\":39905},{\"end\":40177,\"start\":40176},{\"end\":40183,\"start\":40182},{\"end\":40189,\"start\":40188},{\"end\":40201,\"start\":40200},{\"end\":40209,\"start\":40208},{\"end\":40219,\"start\":40218},{\"end\":40351,\"start\":40350},{\"end\":40362,\"start\":40361},{\"end\":40374,\"start\":40373},{\"end\":40705,\"start\":40704},{\"end\":40715,\"start\":40714},{\"end\":40918,\"start\":40915},{\"end\":40925,\"start\":40924},{\"end\":40927,\"start\":40926},{\"end\":40937,\"start\":40936},{\"end\":41126,\"start\":41125},{\"end\":41138,\"start\":41137},{\"end\":41332,\"start\":41331},{\"end\":41346,\"start\":41345},{\"end\":41357,\"start\":41356},{\"end\":41367,\"start\":41366},{\"end\":41369,\"start\":41368},{\"end\":41616,\"start\":41615},{\"end\":41618,\"start\":41617},{\"end\":41630,\"start\":41629},{\"end\":41634,\"start\":41631},{\"end\":41851,\"start\":41850},{\"end\":41860,\"start\":41859},{\"end\":41877,\"start\":41876},{\"end\":41879,\"start\":41878},{\"end\":42087,\"start\":42086},{\"end\":42098,\"start\":42097},{\"end\":42392,\"start\":42391},{\"end\":42404,\"start\":42403},{\"end\":42415,\"start\":42414},{\"end\":42426,\"start\":42425},{\"end\":42434,\"start\":42433},{\"end\":42440,\"start\":42439},{\"end\":42448,\"start\":42447},{\"end\":42450,\"start\":42449},{\"end\":42458,\"start\":42457},{\"end\":42756,\"start\":42755},{\"end\":42758,\"start\":42757},{\"end\":42765,\"start\":42764},{\"end\":42774,\"start\":42773},{\"end\":42786,\"start\":42785},{\"end\":42794,\"start\":42793},{\"end\":42804,\"start\":42803},{\"end\":42815,\"start\":42814},{\"end\":42825,\"start\":42824},{\"end\":42827,\"start\":42826},{\"end\":43000,\"start\":42999},{\"end\":43007,\"start\":43006},{\"end\":43019,\"start\":43018},{\"end\":43028,\"start\":43027},{\"end\":43039,\"start\":43038},{\"end\":43248,\"start\":43247},{\"end\":43255,\"start\":43254},{\"end\":43264,\"start\":43263},{\"end\":43274,\"start\":43273},{\"end\":43285,\"start\":43284},{\"end\":43295,\"start\":43294},{\"end\":43460,\"start\":43459},{\"end\":43467,\"start\":43466},{\"end\":43473,\"start\":43472},{\"end\":43481,\"start\":43480},{\"end\":43489,\"start\":43488},{\"end\":43498,\"start\":43497},{\"end\":43770,\"start\":43769},{\"end\":43782,\"start\":43781},{\"end\":43797,\"start\":43796},{\"end\":44222,\"start\":44221},{\"end\":44234,\"start\":44233},{\"end\":44244,\"start\":44243},{\"end\":44486,\"start\":44485},{\"end\":44498,\"start\":44497},{\"end\":44513,\"start\":44512},{\"end\":44734,\"start\":44733},{\"end\":44745,\"start\":44744},{\"end\":44757,\"start\":44756},{\"end\":44759,\"start\":44758},{\"end\":45065,\"start\":45064},{\"end\":45072,\"start\":45071},{\"end\":45078,\"start\":45077},{\"end\":45090,\"start\":45089},{\"end\":45240,\"start\":45239},{\"end\":45252,\"start\":45251},{\"end\":45264,\"start\":45263},{\"end\":45270,\"start\":45269},{\"end\":45281,\"start\":45280},{\"end\":45591,\"start\":45590},{\"end\":45606,\"start\":45605},{\"end\":45614,\"start\":45613},{\"end\":45620,\"start\":45619},{\"end\":45630,\"start\":45629},{\"end\":45642,\"start\":45641},{\"end\":45648,\"start\":45647},{\"end\":45657,\"start\":45656},{\"end\":45669,\"start\":45668},{\"end\":45679,\"start\":45678},{\"end\":45985,\"start\":45984},{\"end\":45996,\"start\":45995},{\"end\":45998,\"start\":45997},{\"end\":46009,\"start\":46008},{\"end\":46182,\"start\":46181},{\"end\":46194,\"start\":46193},{\"end\":46523,\"start\":46522},{\"end\":46533,\"start\":46532},{\"end\":46545,\"start\":46544},{\"end\":46547,\"start\":46546},{\"end\":46553,\"start\":46552},{\"end\":46555,\"start\":46554},{\"end\":46566,\"start\":46565},{\"end\":46568,\"start\":46567},{\"end\":46918,\"start\":46917},{\"end\":46929,\"start\":46928},{\"end\":47402,\"start\":47401},{\"end\":47413,\"start\":47412},{\"end\":47423,\"start\":47422},{\"end\":47433,\"start\":47432},{\"end\":47592,\"start\":47591},{\"end\":47793,\"start\":47792},{\"end\":47799,\"start\":47798},{\"end\":47805,\"start\":47804},{\"end\":47814,\"start\":47813},{\"end\":47827,\"start\":47826},{\"end\":47844,\"start\":47843},{\"end\":47853,\"start\":47852}]", "bib_author_last_name": "[{\"end\":36282,\"start\":36278},{\"end\":36295,\"start\":36288},{\"end\":36303,\"start\":36299},{\"end\":36315,\"start\":36307},{\"end\":36691,\"start\":36678},{\"end\":36706,\"start\":36697},{\"end\":37034,\"start\":37027},{\"end\":37050,\"start\":37041},{\"end\":37064,\"start\":37054},{\"end\":37076,\"start\":37068},{\"end\":37091,\"start\":37080},{\"end\":37101,\"start\":37095},{\"end\":37112,\"start\":37105},{\"end\":37388,\"start\":37383},{\"end\":37399,\"start\":37392},{\"end\":37409,\"start\":37403},{\"end\":37421,\"start\":37413},{\"end\":37554,\"start\":37550},{\"end\":37563,\"start\":37558},{\"end\":37574,\"start\":37567},{\"end\":37590,\"start\":37580},{\"end\":37598,\"start\":37594},{\"end\":37608,\"start\":37602},{\"end\":37615,\"start\":37612},{\"end\":37621,\"start\":37619},{\"end\":37633,\"start\":37625},{\"end\":37644,\"start\":37639},{\"end\":37951,\"start\":37944},{\"end\":37962,\"start\":37955},{\"end\":37975,\"start\":37968},{\"end\":37984,\"start\":37979},{\"end\":37998,\"start\":37988},{\"end\":38013,\"start\":38002},{\"end\":38024,\"start\":38017},{\"end\":38299,\"start\":38289},{\"end\":38308,\"start\":38303},{\"end\":38325,\"start\":38314},{\"end\":38565,\"start\":38556},{\"end\":38577,\"start\":38569},{\"end\":38589,\"start\":38581},{\"end\":38601,\"start\":38593},{\"end\":38838,\"start\":38830},{\"end\":39089,\"start\":39084},{\"end\":39105,\"start\":39095},{\"end\":39116,\"start\":39109},{\"end\":39133,\"start\":39122},{\"end\":39148,\"start\":39137},{\"end\":39391,\"start\":39386},{\"end\":39643,\"start\":39641},{\"end\":39652,\"start\":39647},{\"end\":39659,\"start\":39656},{\"end\":39666,\"start\":39663},{\"end\":39894,\"start\":39888},{\"end\":39903,\"start\":39898},{\"end\":39918,\"start\":39907},{\"end\":40180,\"start\":40178},{\"end\":40186,\"start\":40184},{\"end\":40198,\"start\":40190},{\"end\":40206,\"start\":40202},{\"end\":40216,\"start\":40210},{\"end\":40227,\"start\":40220},{\"end\":40359,\"start\":40352},{\"end\":40371,\"start\":40363},{\"end\":40382,\"start\":40375},{\"end\":40712,\"start\":40706},{\"end\":40722,\"start\":40716},{\"end\":40922,\"start\":40919},{\"end\":40934,\"start\":40928},{\"end\":41135,\"start\":41127},{\"end\":41146,\"start\":41139},{\"end\":41343,\"start\":41333},{\"end\":41354,\"start\":41347},{\"end\":41364,\"start\":41358},{\"end\":41374,\"start\":41370},{\"end\":41627,\"start\":41619},{\"end\":41642,\"start\":41635},{\"end\":41857,\"start\":41852},{\"end\":41874,\"start\":41861},{\"end\":41885,\"start\":41880},{\"end\":42095,\"start\":42088},{\"end\":42110,\"start\":42099},{\"end\":42401,\"start\":42393},{\"end\":42412,\"start\":42405},{\"end\":42423,\"start\":42416},{\"end\":42431,\"start\":42427},{\"end\":42437,\"start\":42435},{\"end\":42445,\"start\":42441},{\"end\":42455,\"start\":42451},{\"end\":42463,\"start\":42459},{\"end\":42762,\"start\":42759},{\"end\":42771,\"start\":42766},{\"end\":42783,\"start\":42775},{\"end\":42791,\"start\":42787},{\"end\":42801,\"start\":42795},{\"end\":42812,\"start\":42805},{\"end\":42822,\"start\":42816},{\"end\":42835,\"start\":42828},{\"end\":43004,\"start\":43001},{\"end\":43016,\"start\":43008},{\"end\":43025,\"start\":43020},{\"end\":43036,\"start\":43029},{\"end\":43044,\"start\":43040},{\"end\":43252,\"start\":43249},{\"end\":43261,\"start\":43256},{\"end\":43271,\"start\":43265},{\"end\":43282,\"start\":43275},{\"end\":43292,\"start\":43286},{\"end\":43302,\"start\":43296},{\"end\":43464,\"start\":43461},{\"end\":43470,\"start\":43468},{\"end\":43478,\"start\":43474},{\"end\":43486,\"start\":43482},{\"end\":43495,\"start\":43490},{\"end\":43505,\"start\":43499},{\"end\":43779,\"start\":43771},{\"end\":43794,\"start\":43783},{\"end\":43804,\"start\":43798},{\"end\":44231,\"start\":44223},{\"end\":44241,\"start\":44235},{\"end\":44256,\"start\":44245},{\"end\":44495,\"start\":44487},{\"end\":44510,\"start\":44499},{\"end\":44520,\"start\":44514},{\"end\":44742,\"start\":44735},{\"end\":44754,\"start\":44746},{\"end\":44764,\"start\":44760},{\"end\":45069,\"start\":45066},{\"end\":45075,\"start\":45073},{\"end\":45087,\"start\":45079},{\"end\":45094,\"start\":45091},{\"end\":45249,\"start\":45241},{\"end\":45261,\"start\":45253},{\"end\":45267,\"start\":45265},{\"end\":45278,\"start\":45271},{\"end\":45289,\"start\":45282},{\"end\":45603,\"start\":45592},{\"end\":45611,\"start\":45607},{\"end\":45617,\"start\":45615},{\"end\":45627,\"start\":45621},{\"end\":45639,\"start\":45631},{\"end\":45645,\"start\":45643},{\"end\":45654,\"start\":45649},{\"end\":45666,\"start\":45658},{\"end\":45676,\"start\":45670},{\"end\":45689,\"start\":45680},{\"end\":45993,\"start\":45986},{\"end\":46006,\"start\":45999},{\"end\":46017,\"start\":46010},{\"end\":46191,\"start\":46183},{\"end\":46204,\"start\":46195},{\"end\":46530,\"start\":46524},{\"end\":46542,\"start\":46534},{\"end\":46550,\"start\":46548},{\"end\":46563,\"start\":46556},{\"end\":46571,\"start\":46569},{\"end\":46926,\"start\":46919},{\"end\":46934,\"start\":46930},{\"end\":47410,\"start\":47403},{\"end\":47420,\"start\":47414},{\"end\":47430,\"start\":47424},{\"end\":47439,\"start\":47434},{\"end\":47601,\"start\":47593},{\"end\":47796,\"start\":47794},{\"end\":47802,\"start\":47800},{\"end\":47811,\"start\":47806},{\"end\":47824,\"start\":47815},{\"end\":47841,\"start\":47828},{\"end\":47850,\"start\":47845},{\"end\":47860,\"start\":47854}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36567,\"start\":36276},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18402440},\"end\":36940,\"start\":36569},{\"attributes\":{\"id\":\"b2\"},\"end\":37325,\"start\":36942},{\"attributes\":{\"id\":\"b3\"},\"end\":37546,\"start\":37327},{\"attributes\":{\"id\":\"b4\"},\"end\":37877,\"start\":37548},{\"attributes\":{\"id\":\"b5\"},\"end\":38206,\"start\":37879},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9682853},\"end\":38494,\"start\":38208},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":16889889},\"end\":38814,\"start\":38496},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206770307},\"end\":39080,\"start\":38816},{\"attributes\":{\"doi\":\"arXiv:1503.04069\",\"id\":\"b9\"},\"end\":39356,\"start\":39082},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":148132585},\"end\":39637,\"start\":39358},{\"attributes\":{\"id\":\"b11\"},\"end\":39802,\"start\":39639},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":928608},\"end\":40139,\"start\":39804},{\"attributes\":{\"id\":\"b13\"},\"end\":40348,\"start\":40141},{\"attributes\":{\"doi\":\"arXiv:1511.07571\",\"id\":\"b14\"},\"end\":40640,\"start\":40350},{\"attributes\":{\"id\":\"b15\"},\"end\":40829,\"start\":40642},{\"attributes\":{\"id\":\"b16\"},\"end\":41056,\"start\":40831},{\"attributes\":{\"id\":\"b17\"},\"end\":41261,\"start\":41058},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6308361},\"end\":41537,\"start\":41263},{\"attributes\":{\"id\":\"b19\"},\"end\":41772,\"start\":41539},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7732372},\"end\":42023,\"start\":41774},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7983519},\"end\":42280,\"start\":42025},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":53307035},\"end\":42710,\"start\":42282},{\"attributes\":{\"id\":\"b23\"},\"end\":42997,\"start\":42712},{\"attributes\":{\"id\":\"b24\"},\"end\":43180,\"start\":42999},{\"attributes\":{\"id\":\"b25\"},\"end\":43457,\"start\":43182},{\"attributes\":{\"id\":\"b26\"},\"end\":43718,\"start\":43459},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7198177},\"end\":44219,\"start\":43720},{\"attributes\":{\"id\":\"b28\"},\"end\":44429,\"start\":44221},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6034383},\"end\":44665,\"start\":44431},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14579301},\"end\":44982,\"start\":44667},{\"attributes\":{\"id\":\"b31\"},\"end\":45237,\"start\":44984},{\"attributes\":{\"doi\":\"arXiv:1511.03745\",\"id\":\"b32\"},\"end\":45537,\"start\":45239},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2930547},\"end\":45982,\"start\":45539},{\"attributes\":{\"id\":\"b34\"},\"end\":46179,\"start\":45984},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b35\"},\"end\":46437,\"start\":46181},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2317858},\"end\":46850,\"start\":46439},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":9460079},\"end\":47350,\"start\":46852},{\"attributes\":{\"id\":\"b38\"},\"end\":47557,\"start\":47352},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":56798209},\"end\":47712,\"start\":47559},{\"attributes\":{\"id\":\"b40\"},\"end\":48035,\"start\":47714}]", "bib_title": "[{\"end\":36674,\"start\":36569},{\"end\":38285,\"start\":38208},{\"end\":38552,\"start\":38496},{\"end\":38826,\"start\":38816},{\"end\":39380,\"start\":39358},{\"end\":39884,\"start\":39804},{\"end\":41329,\"start\":41263},{\"end\":41848,\"start\":41774},{\"end\":42084,\"start\":42025},{\"end\":42389,\"start\":42282},{\"end\":43767,\"start\":43720},{\"end\":44483,\"start\":44431},{\"end\":44731,\"start\":44667},{\"end\":45588,\"start\":45539},{\"end\":46520,\"start\":46439},{\"end\":46915,\"start\":46852},{\"end\":47589,\"start\":47559}]", "bib_author": "[{\"end\":36284,\"start\":36276},{\"end\":36297,\"start\":36284},{\"end\":36305,\"start\":36297},{\"end\":36317,\"start\":36305},{\"end\":36693,\"start\":36676},{\"end\":36708,\"start\":36693},{\"end\":37036,\"start\":37025},{\"end\":37052,\"start\":37036},{\"end\":37066,\"start\":37052},{\"end\":37078,\"start\":37066},{\"end\":37093,\"start\":37078},{\"end\":37103,\"start\":37093},{\"end\":37114,\"start\":37103},{\"end\":37118,\"start\":37114},{\"end\":37390,\"start\":37381},{\"end\":37401,\"start\":37390},{\"end\":37411,\"start\":37401},{\"end\":37423,\"start\":37411},{\"end\":37556,\"start\":37548},{\"end\":37565,\"start\":37556},{\"end\":37576,\"start\":37565},{\"end\":37592,\"start\":37576},{\"end\":37600,\"start\":37592},{\"end\":37610,\"start\":37600},{\"end\":37617,\"start\":37610},{\"end\":37623,\"start\":37617},{\"end\":37635,\"start\":37623},{\"end\":37646,\"start\":37635},{\"end\":37953,\"start\":37942},{\"end\":37964,\"start\":37953},{\"end\":37977,\"start\":37964},{\"end\":37986,\"start\":37977},{\"end\":38000,\"start\":37986},{\"end\":38015,\"start\":38000},{\"end\":38026,\"start\":38015},{\"end\":38301,\"start\":38287},{\"end\":38310,\"start\":38301},{\"end\":38327,\"start\":38310},{\"end\":38567,\"start\":38554},{\"end\":38579,\"start\":38567},{\"end\":38591,\"start\":38579},{\"end\":38603,\"start\":38591},{\"end\":38840,\"start\":38828},{\"end\":39091,\"start\":39082},{\"end\":39107,\"start\":39091},{\"end\":39118,\"start\":39107},{\"end\":39135,\"start\":39118},{\"end\":39150,\"start\":39135},{\"end\":39393,\"start\":39382},{\"end\":39645,\"start\":39639},{\"end\":39654,\"start\":39645},{\"end\":39661,\"start\":39654},{\"end\":39668,\"start\":39661},{\"end\":39896,\"start\":39886},{\"end\":39905,\"start\":39896},{\"end\":39920,\"start\":39905},{\"end\":40182,\"start\":40176},{\"end\":40188,\"start\":40182},{\"end\":40200,\"start\":40188},{\"end\":40208,\"start\":40200},{\"end\":40218,\"start\":40208},{\"end\":40229,\"start\":40218},{\"end\":40361,\"start\":40350},{\"end\":40373,\"start\":40361},{\"end\":40384,\"start\":40373},{\"end\":40714,\"start\":40704},{\"end\":40724,\"start\":40714},{\"end\":40924,\"start\":40915},{\"end\":40936,\"start\":40924},{\"end\":40940,\"start\":40936},{\"end\":41137,\"start\":41125},{\"end\":41148,\"start\":41137},{\"end\":41345,\"start\":41331},{\"end\":41356,\"start\":41345},{\"end\":41366,\"start\":41356},{\"end\":41376,\"start\":41366},{\"end\":41629,\"start\":41615},{\"end\":41644,\"start\":41629},{\"end\":41859,\"start\":41850},{\"end\":41876,\"start\":41859},{\"end\":41887,\"start\":41876},{\"end\":42097,\"start\":42086},{\"end\":42112,\"start\":42097},{\"end\":42403,\"start\":42391},{\"end\":42414,\"start\":42403},{\"end\":42425,\"start\":42414},{\"end\":42433,\"start\":42425},{\"end\":42439,\"start\":42433},{\"end\":42447,\"start\":42439},{\"end\":42457,\"start\":42447},{\"end\":42465,\"start\":42457},{\"end\":42764,\"start\":42755},{\"end\":42773,\"start\":42764},{\"end\":42785,\"start\":42773},{\"end\":42793,\"start\":42785},{\"end\":42803,\"start\":42793},{\"end\":42814,\"start\":42803},{\"end\":42824,\"start\":42814},{\"end\":42837,\"start\":42824},{\"end\":43006,\"start\":42999},{\"end\":43018,\"start\":43006},{\"end\":43027,\"start\":43018},{\"end\":43038,\"start\":43027},{\"end\":43046,\"start\":43038},{\"end\":43254,\"start\":43247},{\"end\":43263,\"start\":43254},{\"end\":43273,\"start\":43263},{\"end\":43284,\"start\":43273},{\"end\":43294,\"start\":43284},{\"end\":43304,\"start\":43294},{\"end\":43466,\"start\":43459},{\"end\":43472,\"start\":43466},{\"end\":43480,\"start\":43472},{\"end\":43488,\"start\":43480},{\"end\":43497,\"start\":43488},{\"end\":43507,\"start\":43497},{\"end\":43781,\"start\":43769},{\"end\":43796,\"start\":43781},{\"end\":43806,\"start\":43796},{\"end\":44233,\"start\":44221},{\"end\":44243,\"start\":44233},{\"end\":44258,\"start\":44243},{\"end\":44497,\"start\":44485},{\"end\":44512,\"start\":44497},{\"end\":44522,\"start\":44512},{\"end\":44744,\"start\":44733},{\"end\":44756,\"start\":44744},{\"end\":44766,\"start\":44756},{\"end\":45071,\"start\":45064},{\"end\":45077,\"start\":45071},{\"end\":45089,\"start\":45077},{\"end\":45096,\"start\":45089},{\"end\":45251,\"start\":45239},{\"end\":45263,\"start\":45251},{\"end\":45269,\"start\":45263},{\"end\":45280,\"start\":45269},{\"end\":45291,\"start\":45280},{\"end\":45605,\"start\":45590},{\"end\":45613,\"start\":45605},{\"end\":45619,\"start\":45613},{\"end\":45629,\"start\":45619},{\"end\":45641,\"start\":45629},{\"end\":45647,\"start\":45641},{\"end\":45656,\"start\":45647},{\"end\":45668,\"start\":45656},{\"end\":45678,\"start\":45668},{\"end\":45691,\"start\":45678},{\"end\":45995,\"start\":45984},{\"end\":46008,\"start\":45995},{\"end\":46019,\"start\":46008},{\"end\":46193,\"start\":46181},{\"end\":46206,\"start\":46193},{\"end\":46532,\"start\":46522},{\"end\":46544,\"start\":46532},{\"end\":46552,\"start\":46544},{\"end\":46565,\"start\":46552},{\"end\":46573,\"start\":46565},{\"end\":46928,\"start\":46917},{\"end\":46936,\"start\":46928},{\"end\":47412,\"start\":47401},{\"end\":47422,\"start\":47412},{\"end\":47432,\"start\":47422},{\"end\":47441,\"start\":47432},{\"end\":47603,\"start\":47591},{\"end\":47798,\"start\":47792},{\"end\":47804,\"start\":47798},{\"end\":47813,\"start\":47804},{\"end\":47826,\"start\":47813},{\"end\":47843,\"start\":47826},{\"end\":47852,\"start\":47843},{\"end\":47862,\"start\":47852}]", "bib_venue": "[{\"end\":36413,\"start\":36317},{\"end\":36738,\"start\":36708},{\"end\":37023,\"start\":36942},{\"end\":37379,\"start\":37327},{\"end\":37687,\"start\":37646},{\"end\":37940,\"start\":37879},{\"end\":38336,\"start\":38327},{\"end\":38630,\"start\":38603},{\"end\":38907,\"start\":38840},{\"end\":39194,\"start\":39166},{\"end\":39413,\"start\":39393},{\"end\":39712,\"start\":39668},{\"end\":39963,\"start\":39920},{\"end\":40174,\"start\":40141},{\"end\":40472,\"start\":40400},{\"end\":40702,\"start\":40642},{\"end\":40913,\"start\":40831},{\"end\":41123,\"start\":41058},{\"end\":41385,\"start\":41376},{\"end\":41613,\"start\":41539},{\"end\":41891,\"start\":41887},{\"end\":42137,\"start\":42112},{\"end\":42482,\"start\":42465},{\"end\":42753,\"start\":42712},{\"end\":43080,\"start\":43046},{\"end\":43245,\"start\":43182},{\"end\":43578,\"start\":43507},{\"end\":43881,\"start\":43806},{\"end\":44316,\"start\":44258},{\"end\":44531,\"start\":44522},{\"end\":44815,\"start\":44766},{\"end\":45062,\"start\":44984},{\"end\":45363,\"start\":45307},{\"end\":45731,\"start\":45691},{\"end\":46073,\"start\":46019},{\"end\":46287,\"start\":46221},{\"end\":46634,\"start\":46573},{\"end\":47013,\"start\":46936},{\"end\":47399,\"start\":47352},{\"end\":47623,\"start\":47603},{\"end\":47790,\"start\":47714},{\"end\":38961,\"start\":38909},{\"end\":39450,\"start\":39437},{\"end\":43943,\"start\":43883},{\"end\":47077,\"start\":47015}]"}}}, "year": 2023, "month": 12, "day": 17}