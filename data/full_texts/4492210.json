{"id": 4492210, "updated": "2023-09-27 17:28:56.462", "metadata": {"title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations", "authors": "[{\"first\":\"Ranjay\",\"last\":\"Krishna\",\"middle\":[]},{\"first\":\"Yuke\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Oliver\",\"last\":\"Groth\",\"middle\":[]},{\"first\":\"Justin\",\"last\":\"Johnson\",\"middle\":[]},{\"first\":\"Kenji\",\"last\":\"Hata\",\"middle\":[]},{\"first\":\"Joshua\",\"last\":\"Kravitz\",\"middle\":[]},{\"first\":\"Stephanie\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yannis\",\"last\":\"Kalantidis\",\"middle\":[]},{\"first\":\"Li-Jia\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Shamma\",\"middle\":[\"A.\"]},{\"first\":\"Michael\",\"last\":\"Bernstein\",\"middle\":[\"S.\"]},{\"first\":\"Fei-Fei\",\"last\":\"Li\",\"middle\":[]}]", "venue": "International Journal of Computer Vision", "journal": "International Journal of Computer Vision", "publication_date": {"year": 2016, "month": 2, "day": 23}, "abstract": "Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked\"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that\"the person is riding a horse-drawn carriage\". In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1602.07332", "mag": "2949474740", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/KrishnaZGJHKCKL16", "doi": "10.1007/s11263-016-0981-7"}}, "content": {"source": {"pdf_hash": "fdc2d05c9ee932fa19df3edb9922b4f0406538a4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1602.07332v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://link.springer.com/content/pdf/10.1007/s11263-016-0981-7.pdf", "status": "HYBRID"}}, "grobid": {"id": "950aeae186a4bb5ebd4b0faf6c1e39a5229c0726", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fdc2d05c9ee932fa19df3edb9922b4f0406538a4.txt", "contents": "\nVisual Genome Connecting Language and Vision Using Crowdsourced Dense Image Annotations\n\n\nRanjay Krishna ranjaykrishna@cs.stanford.edu \nYuke Zhu \nOliver Groth \nJustin Johnson \nKenji Hata \nJoshua Kravitz \nStephanie Chen \nYannis Kalantidis \n\u00b7 Li \n-Jia Li \n\u00b7 David \nA Shamma \nMichael S Bernstein \n\u00b7 Li \nFei-Fei \nRanjay Krishna \nYuke Zhu \nOliver Groth \nJustin Johnson \nKenji Hata \nJoshua Kravitz \nStephanie Chen \nLi-Jia Li \nDavid A Shamma \nMichael S Bernstein \nLi Fei-Fei \n\nStanford University\nStanfordCAUSA\n\n\nStanford University\nStanfordCAUSA\n\n\nDresden University of Technology\nDresdenGermany\n\n\nStanford University\nStanfordCAUSA\n\n\nStanford University\nStanfordCAUSA\n\n\nStanford University\nStanfordCAUSA\n\n\nStanford University\nStanfordCAUSA\n\n\nYannis Kalantidis Yahoo Inc\nSan FranciscoCAUSA\n\n\nSnapchat Inc\nLos AngelesCAUSA\n\n\nYahoo Inc\nSan FranciscoCAUSA\n\n\nStanford University\nStanfordCAUSA\n\n\nStanford University\nStanfordCAUSA\n\nVisual Genome Connecting Language and Vision Using Crowdsourced Dense Image Annotations\nReceived: date / Accepted: dateNoname manuscript No. (will be inserted by the editor)Computer Vision \u00b7 Dataset \u00b7 Image \u00b7 Scene Graph \u00b7 Question Answering \u00b7 Objects \u00b7 Attributes \u00b7 Relationships \u00b7 Knowledge \u00b7 Language \u00b7 Crowdsourcing\nDespite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage.\"In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.\n\nIntroduction\n\nA holy grail of computer vision is the complete understanding of visual scenes: a model that is able to name and detect objects, describe their attributes, and recognize their relationships and interactions. Understanding scenes would enable important applications such as image search, question answering, and robotic interactions. Much progress has been made in recent years towards this goal, including image classification (Deng et al., 2009, Perronnin et al., 2010, Simonyan and Zisserman, 2014, Krizhevsky et al., 2012: An overview of the data needed to move from perceptual awareness to cognitive understanding of images. We present a dataset of images densely annotated with numerous region descriptions, objects, attributes, and relationships. Region descriptions (e.g. \"girl feeding large elephant\" and \"a man taking a picture behind girl\") are shown (top). The objects (e.g. elephant), attributes (e.g. large) and relationships (e.g. feeding) are shown (bottom). Our dataset also contains image related question answer pairs (not shown). et al., 2014) and object detection (Everingham et al., 2010, Girshick et al., 2014, Sermanet et al., 2013, Girshick, 2015, Ren et al., 2015b). An important contributing factor is the availability of a large amount of data that drives the statistical models that underpin today's advances in computational visual understanding. While the progress is exciting, we are still far from reaching the goal of comprehensive scene understanding. As Figure 1 shows, existing models would be able to detect discreet objects in a photo but would not be able to explain their interactions or the relationships between them. Such explanations tend to be cognitive in nature, integrating perceptual information into conclusions about the relationships between objects in a scene (Bruner, 1990, Firestone andScholl, 2015). A cognitive understanding of our visual world thus requires that we complement computers' ability to detect objects with abilities to describe those objects (Isola et al., 2015) and understand their interactions within a scene (Sadeghi and Farhadi, 2011).\n\nThere is an increasing effort to put together the next generation of datasets to serve as training and benchmarking datasets for these deeper, cognitive scene understanding and reasoning tasks, the most notable being MS-COCO (Lin et al., 2014) and VQA (Antol et al., 2015). The MS-COCO dataset consists of 300K real-world photos collected from Flickr. For each image, there is pixel-level segmentation of 91 object classes (when present) and 5 independent, user-generated sentences describing the scene. VQA adds to this a set of 614K question-answer pairs related to the visual contents of each image (see more details in Section 3.1). With this information, MS-COCO and VQA provide a fertile training and testing ground for models aimed at tasks for accurate object detection, segmentation, and summary-level image captioning (Kiros et al., 2014, Mao et al., 2014, Karpathy and Fei-Fei, 2014, Vinyals et al., 2014 as well as basic QA (Ren et al., 2015a, Antol et al., 2015, Malinowski et al., 2015, Gao et al., 2015, Malinowski and Fritz, 2014. For example, a stateof-the-art model (Karpathy and Fei-Fei, 2014) provides a description of one MS-COCO image in Figure 1 as \"two men are standing next to an elephant.\" But what is missing is the further understanding of where each object is, what each person is doing, what the relationship between the person and elephant is, etc. Without such relationships, these models fail to differentiate this image from other images of people next to elephants.\n\nTo understand images thoroughly, we believe three key elements need to be added to existing datasets: a grounding of visual concepts to language (Kiros et al., 2014), a more complete set of descriptions and QAs for each image based on multiple image regions (Johnson et al., 2015), and a formalized representation of the components of an image (Hayes, 1978). In the spirit of mapping out this complete information of the visual world, we introduce the Visual Genome dataset. The first release of the Visual Genome dataset uses 108, 249 images from the intersection of the YFCC100M (Thomee et al., 2016) and MS-COCO (Lin et al., 2014). Section 5 provides a more detailed description of the dataset. We highlight below the motivation and contributions of the three key elements that set Visual Genome apart from existing datasets.\n\nThe Visual Genome dataset regards relationships and attributes as first-class citizens of the annotation space, in addition to the traditional focus on objects. Recognition of relationships and attributes is an important part of the complete understanding of the visual scene, and in many cases, these elements are key to the story of a scene (e.g., the difference between \"a dog chasing a man\" versus \"a man chasing a dog\"). The Visual Genome dataset is among the first to provide a detailed labeling of object interactions and attributes, grounding visual concepts to language 1 .\n\nAn image is often a rich scenery that cannot be fully described in one summarizing sentence. The scene in Figure 1 contains multiple \"stories\": \"a man taking a photo of elephants,\" \"a woman feeding an elephant,\" \"a river in the background of lush grounds,\" etc. Existing datasets such as Flickr 30K (Young et al., 2014) and MS-COCO (Lin et al., 2014) focus on high-level descriptions of an image 2 . Instead, for each image in the Visual Genome dataset, we collect more than 42 descriptions for different regions in the image, providing a much denser and complete set of descriptions of the scene. In addition, inspired by VQA (Antol et al.,1 The Lotus Hill Dataset (Yao et al., 2007) also provides a similar annotation of object relationships, see Sec 3.1.\n\n\n2015)\n\n, we also collect an average of 17 question-answer pairs based on the descriptions for each image. Regionbased question answers can be used to jointly develop NLP and vision models that can answer questions from either the description or the image, or both of them.\n\nWith a set of dense descriptions of an image and the explicit correspondences between visual pixels (i.e. bounding boxes of objects) and textual descriptors (i.e. relationships, attributes), the Visual Genome dataset is poised to be the first image dataset that is capable of providing a structured formalized representation of an image, in the form that is widely used in knowledge base representations in NLP (Zhou et al., 2007, GuoDong et al., 2005, Culotta and Sorensen, 2004, Socher et al., 2012. For example, in Figure 1, we can formally express the relationship holding between the woman and food as holding(woman, food)). Putting together all the objects and relations in a scene, we can represent each image as a scene graph (Johnson et al., 2015). The scene graph representation has been shown to improve semantic image retrieval (Johnson et al., 2015, Schuster et al., 2015 and image captioning (Farhadi et al., 2009, Chang et al., 2014, Gupta and Davis, 2008. Furthermore, all objects, attributes and relationships in each image in the Visual Genome dataset are canonicalized to its corresponding Word-Net (Miller, 1995) ID (called a synset ID). This mapping connects all images in Visual Genome and provides an effective way to consistently query the same concept (object, attribute, or relationship) in the dataset. It can also potentially help train models that can learn from contextual information from multiple images.\n\nIn this paper, we introduce the Visual Genome dataset with the aim of training and benchmarking the next generation of computer models for comprehensive scene understanding. The paper proceeds as follows: In Section 2, we provide a detailed description of each component of the dataset. Section 3 provides a literature review of related datasets as well as related recognition tasks. Section 4 discusses the crowdsourcing strategies we deployed in the ongoing effort of collecting this dataset. Section 5 is a collection of data analysis statistics, showcasing the key properties of the Visual Genome dataset. Last but not least, Section 6 provides a set of experimental results that use Visual Genome as a benchmark.\n\nFurther visualizations, API, and additional information on the Visual Genome dataset can be found online 3 .\n\n\nThe man is almost bald\n\nPark bench is made of gray weathered wood A man and a woman sit on a park bench along a river. Fig. 2: An example image from the Visual Genome dataset. We show 3 region descriptions and their corresponding region graphs. We also show the connected scene graph collected by combining all of the image's region graphs. The top region description is \"a man and a woman sit on a park bench along a river.\" It contains the objects: man, woman, bench and river. The relationships that connect these objects are: sits on(man, bench), in front of (man, river ), and sits on(woman, bench). : An example image from our dataset along with its scene graph representation. The scene graph contains objects (child, instructor, helmet, etc.) that are localized in the image as bounding boxes (not shown). These objects also have attributes: large, green, behind, etc. Finally, objects are connected to each other through relationships: wears(child, helmet), wears(instructor, jacket), etc. The Visual Genome dataset consists of seven main components: region descriptions, objects, attributes, relationships, region graphs, scene graphs, and questionanswer pairs. Figure 4 shows examples of each component for one image. To enable research on comprehensive understanding of images, we begin by collecting descriptions and question answers. These are raw texts without any restrictions on length or vocabulary. Next, we extract objects, attributes and relationships from our descriptions. Together, objects, attributes and relationships fabricate our scene graphs that represent a formal representation of an image. In this section, we break down Figure 4 and explain each of the seven components. In Section 4, we will describe in more detail how data from each component is collected through a crowdsourcing platform.\n\n\nMultiple regions and their descriptions\n\nIn a real-world image, one simple summary sentence is often insufficient to describe all the contents of and interactions in an image. Instead, one natural way to extend this might be a collection of descriptions based on different regions of a scene. In Visual Genome, we collect human-generated image region descriptions, with each region localized by a bounding box. In Figure 5, we show three examples of region descriptions. Regions are allowed to have a high degree of overlap with each other when the descriptions differ. For example, \"yellow fire hydrant\" and \"woman in shorts is standing behind the man\" have very little overlap, while \"man jumping over fire hydrant\" has a very high overlap with the other two regions. Our dataset contains on average a total of 42 region descriptions per image. Each description is a phrase ranging from 1 to 16 words in length describing that region.\n\n\nMultiple objects and their bounding boxes\n\nEach image in our dataset consists of an avarege of 21 objects, each delineated by a tight bounding box (Figure 6). Furthermore, each object is canonicalized to a synset ID in WordNet (Miller, 1995). For example, man and person would get mapped to man.n.03 (the generic use of the word to refer to any human being). Similarly, person gets mapped to person.n.01 (a human being). Afterwards, these two concepts can be joined to person.n.01 since this is a hypernym of man.n.03. This is an important standardization step to avoid multiple in an image, the Visual Genome dataset includes multiple human-generated image regions descriptions, with each region localized by a bounding box. Here, we show three regions descriptions: \"man jumping over a fire hydrant,\" \"yellow fire hydrant,\" and \"woman in shorts is standing beghind the man.\"\n\nnames for one object (e.g. man, person, human), and to connect information across images.\n\n\nA set of attributes\n\nEach image in Visual Genome has an average of 16 attributes. Objects can have zero or more attributes associated with them. Attributes can be color (yellow), states (standing), etc. (Figure 7). Just like we extract objects from region descriptions, we also extract the attributes attached to these objects. In Figure 7, from the phrase \"yellow fire hydrant,\" we extract the attribute yellow for the fire hydrant.\n\nAs with objects, we canonicalize all attributes to Word-Net (Miller, 1995); for example, yellow is mapped to yellow.s.01 (of the color intermediate between green and orange in the color spectrum; of something resembling the color of an egg yolk).\n\n\nA set of relationships\n\nRelationships connect two objects together. These relationships can be actions (jumping over), spatial (is behind), verbs (wear), prepositions (with), Fig. 6: From all of the region descriptions, we extract all objects mentioned. For example, from the region description \"man jumping over a fire hydrant,\" we extract man and fire hydrant. : Some descriptions also provide attributes for objects. For example, the region description \"yellow fire hydrant\" adds that the fire hydrant is yellow.\n\nHere we show two attributes: yellow and standing.\n\ncomparative (taller than), or prepositional phrases (drive on). For example, from the region description \"man jumping over fire hydrant,\" we extract the relationship jumping over between the objects man and fire hydrant (Figure 8). These relationships are directed from one object, called the subject, to another, called the object. In this case, the subject is the man, who is performing the relationship jumping over on the object fire hydrant. Each relationship is canonicalized to a WordNet (Miller, 1995) synset ID; i.e. jumping is canonicalized to jump.a.1 (move forward by leaps and bounds). On average, each image in our dataset contains 18 relationships.\n\n\nA set of region graphs\n\nCombining the objects, attributes, and relationships extracted from region descriptions, we create a directed graph representation for each of the 42 regions. Examples of region graphs are shown in Figure 4. Each region graph is a structured representation of a part of the image. The nodes in the graph represent objects, attributes, and relationships. Objects are linked to their respective attributes while relationships link one object to another. The links connecting two objects in Figure 4 point from the subject to the relationship and from the relationship to the other object.\n\n\nOne scene graph\n\nWhile region graphs are localized representations of an image, we also combine them into a single scene graph representing the entire image ( Figure 3). The scene graph is the union of all region graphs and contains all objects, attributes, and relationships from each region description. By doing so, we are able to combine multiple levels of scene information in a more coherent way. For example in Figure 4, the leftmost region description tells us that the \"fire hydrant is yellow,\" while the middle region description tells us that the \"man is jumping over the fire hydrant.\" Together, the two descriptions tell us that the \"man is jumping over a yellow fire hydrant.\"\n\n\nA set of question answer pairs\n\nWe have two types of QA pairs associated with each image in our dataset: freeform QAs, based on the entire image, and region-based QAs, based on selected regions of the image. We collect 6 different types of questions per image: what, where, how, when, who, and why. In Figure 4, \"Q. What is the woman standing next to?; A. Her belongings\" is a freeform QA. Each image has at least one question of each type listed above. Regionbased QAs are collected by prompting workers with region descriptions. For example, we use the region \"yellow fire hydrant\" to collect the region-based QA: \"Q. What color is the fire hydrant?; A. Yellow.\" Region based QAs allow us to independently study methods that use NLP and vision priors to answer questions.\n\n\nRelated Work\n\nWe discuss existing datasets that have been released and used by the vision community for classification and object detection. We also mention work that has improved object and attribute detection models. Then, we explore existing work that has utilized representations similar to our relationships between objects. In addition, we dive into literature related to cognitive tasks like image description, question answering, and knowledge representation.\n\n\nDatasets\n\nDatasets (Table 1) et al., 2008) introduced a dataset with multiple objects per category. They also provided a web interface that experts and novices could use to annotate additional images. This web interface enabled images to be labeled with polygons, helping create datasets for image segmentation. The Lotus Hill dataset (Yao et al., 2007) contains a hierarchical decomposition of objects (vehicles, man-made objects, animals, etc.) along with segmentations. Only a small part of this dataset is freely available. SUN (Xiao et al., 2010), just like LabelMe (Russell et al., 2008) and Lotus Hill (Yao et al., 2007), was curated for object detection. Pushing the size of datasets even further, 80 Million Tiny Images ) created a significantly larger dataset than its predecessors. It contains tiny (i.e. 32 \u00d7 32 pixels) images that were collected using Word-Net (Miller, 1995) synsets as queries. However, because the data in 80 Million Images were not human-verified, they contain numerous errors. YFCC100M (Thomee et al., 2016) is another large database of 100 million images that is still largely unexplored. It contains human generated and machine generated tags.\n\nPascal VOC (Everingham et al., 2010) pushed research from classification to object detection with a dataset containing 20 semantic categories in 11, 000 images. Imagenet (Deng et al., 2009) took WordNet synsets and crowdsourced a large dataset of 14 million images. They started the ILSVRC (Russakovsky et al., 2015) challenge for a variety of computer vision tasks. ILSVRC and PASCAL provide a test bench for object detection, image classification, object segmentation, person layout, and action classification. MS-COCO (Lin et al., 2014) recently released its dataset, with over 328, 000 images with sentence descriptions and segmentations of 91 object categories. The current largest dataset for QA, VQA (Antol et al., 2015), contains 204, 721 images annotated with one or more question answers. They collected a dataset of 614, 163 freeform questions with 6.1M ground truth answers and provided a baseline approach in answering questions using an image and a textual question as the input.\n\nVisual Genome aims to bridge the gap between all these datasets, collecting not just annotations for a large number of objects but also scene graphs, region descriptions, and question answer pairs for image regions. Unlike previous datasets, which were collected for a single task like image classification, the Visual Genome dataset was collected to be a general-purpose representation of the visual world, without bias toward a particular task. Our images contain an average of 21 objects, which is almost an order of magnitude more dense than any existing vision dataset. Similarly, we contain an average of 18 attributes and 18 relationships  Table 1: A comparison of existing datasets with Visual Genome. We show that Visual Genome has an order of magnitude more descriptions and question answers. It also has a more diverse set of object, attribute, and relationship classes. Additionally, Visual Genome contains a higher density of these annotations per image.\n\nper image. We also have an order of magnitude more unique objects, attributes, and relationships than any other dataset. Finally, we have 1.7 million question answer pairs, also larger than any other dataset for visual question answering.\n\n\nImage Descriptions\n\nOne of the core contributions of Visual Genome is its descriptions for multiple regions in an image. As such, we mention other image description datasets and models in this subsection. Most work related to describing images can be divided into two categories: retrieval of human-generated captions and generation of novel captions. Methods in the first category use similarity metrics between image features from predefined models to retrieve similar sentences (Ordonez et al., 2011, Hodosh et al., 2013. Other methods map both sentences and their images to a common vector space (Ordonez et al., 2011) or map them to a space of triples (Farhadi et al., 2010). Among those in the second category, a common theme has been to use recurrent neural networks to produce novel captions (Kiros et al., 2014, Mao et al., 2014, Karpathy and Fei-Fei, 2014, Vinyals et al., 2014. More recently, researchers have also used a visual attention model .\n\nOne drawback of these approaches is their attention to describing only the most salient aspect of the image. This problem is amplified by datasets like Flickr 30K (Young et al., 2014) and MS-COCO (Lin et al., 2014), whose sentence desriptions tend to focus, somewhat redundantly, on these salient parts. For example, \"an elephant is seen wandering around on a sunny day,\" \"a large elephant in a tall grass field,\" and \"a very large elephant standing alone in some brush\" are 3 descriptions from the MS-COCO dataset, and all of them focus on the salient elephant in the image and ignore the other regions in the image. Many real-world scenes are complex, with multiple objects and interactions that are best described using multiple descriptions (Karpathy andFei-Fei, 2014, Lebret et al., 2015). Our dataset pushes toward a complete understanding of an image by collecting a dataset in which we capture not just scene-level descriptions but also myriad of low-level descriptions, the \"grammar\" of the scene.\n\n\nObjects\n\nObject detection is a fundamental task in computer vision, with applications ranging from identification of faces in photo software to identification of other cars by self-driving cars on the road. It involves classifying an object into a semantic category and localizing the object in the image. Visual Genome uses objects as a core component on which each visual scene is built. Early datasets include the face detectio (Huang et al., 2008) and pedestrian datasets (Dollar et al., 2012). The PASCAL VOC and ILSVRC's detection dataset (Deng et al., 2009) pushed research in object detection. But the images in these datasets are iconic and do not capture the settings in which these objects usually co-occur. To remedy this problem, MS-COCO (Lin et al., 2014) annotated real-world scenes that capture object contexts. However, MS-COCO was unable to describe all the objects in its images, since they annotated only 91 object categories. In the real world, there are many more objects that the ones captured by existing datasets. Visual Genome aims at collecting annotations for all visual elements that occur in images, increasing the number of semantic categories to over 17,000.\n\n\nAttributes\n\nThe inclusion of attributes allows us to describe, compare, and more easily categorize objects. Even if we haven't seen an object before, attributes allow us to infer something about it; for example, \"yellow and brown spotted with long neck\" likely refers to a giraffe. Initial work in this area involved finding objects with similar features (Malisiewicz et al., 2008) using examplar SVMs. Next, textures were used to study objects (Varma and Zisserman, 2005), while other methods learned to predict colors (Ferrari and Zisserman, 2007). Finally, the study of attributes was explicitly demonstrated to lead to improvements in object classification (Farhadi et al., 2009). Attributes were defined to be paths (\"has legs\"), shapes (\"spherical\"), or materials (\"furry\") and could be used to classify new categories of objects. Attributes have also played a large role in improving fine-grained recognition (Goering et al., 2014) on fine-grained attribute datasets like CUB-2011 (Wah et al., 2011). In Visual Genome, we use a generalized formulation (Johnson et al., 2015), but we extend it such that attributes are not image-specific binaries but rather object-specific for each object in a real-world scene. We also extend the types of attributes to include size (\"small\"), pose (\"bent\"), state (\"transparent\"), emotion (\"happy\"), and many more.\n\n\nRelationships\n\nRelationship extraction has been a traditional problem in information extraction and in natural language processing. Syntactic features (Zhou et al., 2007, GuoDong et al., 2005, dependency tree methods (Culotta andSorensen, 2004, Bunescu andMooney, 2005), and deep neural networks (Socher et al., 2012, Zeng et al., 2014 have been employed to extract relationships between two entities in a sentence. However, in computer vision, very little work has gone into learning or predicting relationships. Instead, relationships have been implicitly used to improve other vision tasks. Relative layouts between objects have improved scene categorization (Izadinia et al., 2014), and 3D spatial geometry between objects has helped object detection (Choi et al., 2013). Comparative adjectives and prepositions between pairs of objects have been used to model visual relationships and improved object localization (Gupta and Davis, 2008).\n\nRelationships have already shown their utility in improving cognitive tasks. A meaning space of relationships has improved the mapping of images to sentences (Farhadi et al., 2010). Relationships in a structured representation with objects have been defined as a graph structure called a scene graph, where the nodes are objects with attributes and edges are relationships between objects. This representation can be used to generate indoor images from sentences and also to improve image search (Chang et al., 2014, Johnson et al., 2015. We use a similar scene graph representation of an image that generalizes across all these previous works (Johnson et al., 2015). Recently, relationships have come into focus again in the form of question answering about associations between objects (Sadeghi et al., 2015). These questions ask if a relationship, involving generally two objects, is true, e.g. \"do dogs eat ice cream?\". We believe that relationships will be necessary for higher-level cognitive tasks (Johnson et al., 2015, Lu et al., 2016, so we collect the largest corpus of them in an attempt to improve tasks by actually understanding relationships between objects.\n\n\nQuestion Answering\n\nVisual question answering (QA) has been recently proposed as a proxy task of evaluating a computer vision system's ability to understand an image beyond object recognition (Geman et al., 2015, Malinowski andFritz, 2014). Several visual QA benchmarks have been proposed in the last few months. The DAQUAR (Malinowski and Fritz, 2014) dataset was the first toy-sized QA benchmark built upon indoor scene RGB-D images of NYU Depth v2 (Nathan Silberman and Fergus, 2012). Most new datasets (Yu et al., 2015, Ren et al., 2015a, Antol et al., 2015, Gao et al., 2015 have collected QA pairs on MS-COCO images, either generated automatically by NLP tools (Ren et al., 2015a) or written by human workers (Yu et al., 2015, Antol et al., 2015, Gao et al., 2015.\n\nIn previous datasets, most questions concentrated on simple recognition-based questions about the salient objects, and answers were often extremely short. For instance, 90% of DAQUAR answers (Malinowski and Fritz, 2014) and 87% of VQA answers (Antol et al., 2015) consist of single-word object names, attributes, and quantities. This shortness limits their diversity and fails to capture the long-tail details of the images. Given the availability of new datasets, an array of visual QA models have been proposed to tackle QA tasks. The proposed models range from SVM classifiers (Antol et al., 2015) and probabilistic inference (Malinowski and Fritz, 2014) to recurrent neural networks (Gao et al., 2015, Malinowski et al., 2015, Ren et al., 2015a and convolutional networks . Visual Genome aims to capture the details of the images with diverse question types and long answers. These questions should cover a wide range of visual tasks from basic perception to complex reasoning. Our QA dataset of 1.7 million QAs is also larger than any currently existing dataset.\n\n\nKnowledge Representation\n\nA knowledge representation of the visual world is capable of tackling an array of vision tasks, from action recognition to general question answering. However, it is difficult to answer \"what is the minimal viable set of knowledge needed to understand about the physical world?\" (Hayes, 1978). It was later proposed that there be a certain plurality to concepts and their related axioms (Hayes, 1985). These efforts have grown to model physical processes (Forbus, 1984) or to model a series of actions as scripts (Schank and Abelson, 2013) for stories-both of which are not depicted in a single static image but which play roles in an image's story. More recently, NELL (Betteridge et al., 2009) learns probabilistic horn clauses by extracting information from the web. DeepQA (Ferrucci et al., 2010) proposes a probabilistic question answering architecture involving over 100 different techniques. Others have used Markov logic networks (Zhu et al., 2009, Niu et al., 2012 as their representation to perform statistical inference for knowledge base construction. Our work is most similar to that of those , Zhu et al., 2015, Sadeghi et al., 2015 who attempt to learn common-sense relationships from images. Visual Genome scene graphs can also be considered a dense knowledge representation for images. It is similar to the format used in knowledge bases in NLP.\n\n\nCrowdsourcing Strategies\n\nVisual Genome was collected and verified entirely by crowd workers from Amazon Mechanical Turk. In this section, we outline the pipeline employed in creating all the components of the dataset. Each component (region descriptions, objects, attributes, relationships, region graphs, scene graphs, questions and answers) involved multiple task stages. We mention the different strategies used to make our data accurate and to enforce diversity in each component. We also provide background information about the workers who helped make Visual Genome possible.\n\n\nCrowd Workers\n\nWe used Amazon Mechanical Turk (AMT) as our primary source of annotations. Overall, a total of over 33, 000 unique workers contributed to the dataset. The dataset was collected over the course of 6 months after 15 months of experimentation and iteration on the data representation. Approximately 800, 000 Human Intelligence Tasks (HITs) were launched on AMT, where each HIT involved creating descriptions, questions and answers, or region graphs. Each HIT was designed such that workers manage to earn anywhere between $6-$8 per hour if they work continuously, in line with ethical research standards on Mechanical Turk (Salehi et al., 2015). Visual Genome HITs achieved a 94.1% retention rate, meaning that 94.1% of workers who completed one of our tasks went ahead to do more.  Table 2: Geographic distribution of countries from where crowd workers contributed to Visual Genome.\n\nFigures 9 (a) and (b) outline the demographic distribution of our crowd workers. The majority of our workers were between the ages of 25 and 34 years old. Our youngest contributor was 18 years old and the oldest was 68 years old. We also had a near-balanced split of 54.15% male and 45.85% female workers.\n\n\nRegion Descriptions\n\nVisual Genome's main goal is to enable the study of cognitive computer vision tasks. The next step towards understanding images requires studying relationships between objects in scene graph representations of images. However, we observed that collecting scene graphs directly from an image leads to workers annotating easy, frequently-occurring relationships like wearing(man, shirt) instead of focusing on salient parts of the image. This is evident from previous datasets (Johnson et al., 2015, Lu et al., 2016 that contain a large number of such relationships. After experimentation, we observed that when asked to describe an image using natural language, crowd workers naturally start with the most salient part of the image and then move to describing other parts of the image one by one. Inspired by this finding, we focused our attention towards collecting a dataset of region descriptions that is diverse in content.\n\nWhen a new image is added to the crowdsourcing pipeline with no annotations, it is sent to a worker who is asked to draw three bounding boxes and write three descriptions for the region enclosed by each box. Next, the image is sent to another worker along with the previously written descriptions. Workers are explicitly encouraged to write descriptions that have not been written before. This process is repeated until we have collect 50 region descriptions for each image. To prevent workers from having to skim through a long list of previously written descriptions, we only show them the top seven most similar descriptions. We calculate these most similar descriptions using BLEU (Papineni et al., 2002) (n-gram) scores between pairs of sentences. We define the BLEU score between a description d i and a previous description d j to be:\nBLEU N (d i , d j ) = b(d i , d j ) exp( 1 N N n=1 log p n (d i , d j )) (1)\nwhere we enforce a brevity penalty using:\nb(d i , d j ) = 1 if len(d i ) > len(d j ) e 1\u2212 len(d j ) len(d i ) otherwise(2)\nand p n calculates the percentage of n-grams in d i that match n-grams in d j .\n\nWhen a worker writes a new description, we programmatically enforce that it has not been repeated by using BLEU score thresholds set to 0.7 to ensure that it is dissimilar to descriptions from both of the following two lists: 1. Image-specific descriptions. A list of all previously written descriptions for that image. 2. Global image descriptions. A list of the top 100 most common written descriptions of all images in the dataset. This prevents very common phrases like \"sky is blue\" from dominating the set of region descriptions.\n\nFinally, we ask workers to draw bounding boxes that satisfy one requirement: coverage. The bounding box must cover all objects mentioned in the description. Figure 10 shows an example of a good box that covers both the street as well the car mentioned in the description, as well as an example of a bad box.\n\n\nObjects\n\nOnce 50 region descriptions are collected for an image, we extract the visual objects from each description. Each description is sent to one crowd worker, who extracts all the objects from the description and grounds each object as a bounding box in the image. For example, from Figure 4, let's consider the description \"woman in shorts is standing behind the man.\" A worker would extract three objects: woman, shorts, and man. They would then draw a box around each of . the objects. We require each bounding box to be drawn to satisfy two requirements: coverage and quality. Coverage has the same definition as described above in Section 4.2, where we ask workers to make sure that the bounding box covers the object completely (Figure 11). Quality requires that each bounding box be as tight as possible around its object such that if the box's length or height were decreased by one pixel, it would no longer satisfy the coverage requirement. Since a one pixel error can be physically impossible for most workers, we relax the definition of quality to four pixels.\n\nMultiple descriptions for an image might refer to the same object, sometimes with different words. For example, a man in one description might be referred to as person in another description. We can thus use this crowdsourcing stage to build these co-reference chains. With each region description given to a worker to process, we include a list of previously extracted objects as suggestions. This allows a worker to choose a previously drawn box annotated as man instead of redrawing a new box for person.\n\nFinally, to increase the speed with which workers complete this task, we also use Stanford's dependency parser  to extract nouns automatically and send them to the workers as suggestions.\n\nWhile the parser manages to find most of the nouns, it sometimes misses compound nouns, so we avoided completely depending on this automated method. By combining the parser with crowdsourcing tasks, we were able to speed up our object extraction process without losing accuracy.\n\n\nAttributes, Relationships, and Region Graphs\n\nOnce all objects have been extracted from each region description, we can extract the attributes and relationships described in the region. We present each worker with a region description along with its extracted objects and ask them to add attributes to objects or to connect pairs of objects with relationships, based on the text of the description. From the description \"woman in shorts is standing behind the man\", workers will extract the attribute standing for the woman and the relationships in(woman, shorts) and behind (woman, man). Together, objects, attributes, and relationships form the region graph for a region description. Some descriptions like \"it is a sunny day\" do not contain any objects and therefore have no region graphs associated with them. Workers are asked to not generate any graphs for such descriptions. We create scene graphs by combining all the region graphs for an image by combining all the co-referenced objects from different region graphs.\n\n\nScene Graphs\n\nThe scene graph is the union of all region graphs extracted from region descriptions. We merge nodes from region graphs that correspond to the same object; for example, man and person in two different region graphs might refer to the same object in the image. We say that objects from different graphs refer to the same object if their bounding boxes have an overlap over union of 0.8. However, this heuristic might contain false positives. So, before merging two objects, we ask workers to confirm that a pair of objects with significant overlap are indeed the same object. For example, in Figure 12 (right), the fox might be extracted from two different region descriptions. These boxes are then combined together (Figure 12 (left)) when constructing the scene graph. Two region graphs are combined together by merging objects that are co-referenced by both the graphs.\n\n\nQuestions and Answers\n\nTo create question answer (QA) pairs, we ask the AMT workers to write pairs of questions and answers about . an image. To ensure quality, we instruct the workers to follow three rules: 1) start the questions with one of the \"seven Ws\" (who, what, where, when, why, how and which); 2) avoid ambiguous and speculative questions;\n\n3) be precise and unique, and relate the question to the image such that it is clearly answerable if and only if the image is shown.\n\nWe collected two separate types of QAs: freeform QAs and region-based QAs. In freeform QA, we ask a worker to look at an image and write eight QA pairs about it. To encourage diversity, we enforce that workers write at least three different Ws out of the seven in their eight pairs. In region-based QA, we ask the workers to write a pair based on a given region. We select the regions that have large areas (more than 5k pixels) and long phrases (more than 4 words). This enables us to collect around twenty region-based pairs at the same cost of the eight freeform QAs. In general, freeform QA tends to yield more diverse QA pairs that enrich the question distribution; region-based QA tends to produce more factual QA pairs at a lower cost.\n\n\nVerification\n\nAll Visual Genome data go through a verification stage as soon as they are annotated. This stage helps eliminate incorrectly labeled objects, attributes, and relationships. It also helps remove region descriptions and questions and answers that might be correct but are vague (\"This person seems to enjoy the sun.\"), subjective (\"room looks dirty\"), or opinionated (\"Being exposed to hot sun like this may cause cancer\").\n\nVerification is conducted using two separate strategies: majority voting (Snow et al., 2008) and rapid judgments . All components of the dataset except objects are verified using majority voting. Majority voting (Snow et al., 2008) involves three unique workers looking at each annotation and vot-ing on whether it is factually correct. An annotation is added to our dataset if at least two (a majority) out of the three workers verify that it is correct.\n\nWe only use rapid judgments to speed up the verification of the objects in our dataset. Meanwhile, rapid judgments  use an interface inspired by rapid serial visual processing that enable verification of objects with an order of magnitude increase in speed than majority voting.\n\n\nCanonicalization\n\nAll the descriptions and QAs that we collect are freeform worker-generated texts. They are not constrained by any limitations. For example, we do not force workers to refer to a man in the image as a man. We allow them to choose to refer to the man as person, boy, man, etc. This ambiguity makes it difficult to collect all instances of man from our dataset. In order to reduce the ambiguity in the concepts of our dataset and connect it to other resources used by the research community, we map all objects, attributes, relationships, and noun phrases in region descriptions and QAs to synsets in WordNet (Miller, 1995). In the example above, person, boy, and man would map to the synsets: person.n.01 (a human being), male child.n.01 (a youthful male person) and man.n.03 (the generic use of the word to refer to any human being) respectively. Thanks to the WordNet hierarchy it is now possible to fuse those three expressions of the same concept into person.n.01 (a human being) since this is the lowest common ancestor node of all aforementioned synsets.\n\nWe use the Stanford NLP tools  to extract the noun phrases from the region descriptions and QAs. Next, we map them to their most frequent matching synset in WordNet according to WordNet lexeme counts. We then refine this simple heuristic by hand-crafting mapping rules for the 30 most common failure cases. For example according to WordNet's lexeme counts the most common semantic for \"table\" is table.n.01 (a set of data arranged in rows and columns). However in our data it is more likely to see pieces of furniture and therefore bias the mapping towards table.n.02 (a piece of furniture having a smooth flat top that is usually supported by one or more vertical legs). The objects in our scene graphs are already noun phrases and are mapped to WordNet in the same way.\n\nWe normalize each attribute based on morphology (so called \"stemming\") and map them to the WordNet adjectives. We include 15 hand-crafted rules to address common failure cases, which typically occur when the concrete or spatial sense of the word seen in an image is not the most common overall sense. For example, the synset long.a.02 (of relatively great or greater than average spatial extension) is less common in WordNet than long.a.01 (indicating a relatively great or greater than average duration of time), even though instances of the word \"long\" in our images are much more likely to refer to that spatial sense.\n\nFor relationships, we ignore all prepositions as they are not recognized by WordNet. Since the meanings of verbs are highly dependent upon their morphology and syntactic placement (e.g. passive cases, prepositional phrases), we try to find WordNet synsets whose sentence frames match with the context of the relationship. Sentence frames in WordNet are formalized syntactic frames in which a certain sense of a word might appear; for example, play.v.01: participate in games or sport occurs in the sentence frames \"Somebody [play]s\" and \"Somebody [play]s something.\" For each verb-synset pair, we then consider the root hypernym of that synset to reduce potential noise from WordNet's fine-grained sense distinctions. The WordNet hierarchy for verbs is segmented and originates from over 100 root verbs. For example, draw.v.01: cause to move by pulling traces back to the root hypernym move.v.02: cause to move or shift into a new position, while draw.v.02: get or derive traces to the root get.v.01: come into the possession of something concrete or abstract. We also include 20 hand-mapped rules, again to correct for WordNet's lower representation of concrete or spatial senses.\n\nThese mappings are not perfect and still contain some ambiguity. Therefore, we send all our mappings along with the top four alternative synsets for each term to Amazon Mechanical Turk. We ask workers to verify that our mapping was accurate and change the mapping to an alternative one if it was a better fit. We present workers with the concept we want to canonicalize along with our proposed corresponding synset with 4 additional options. To prevent workers from always defaulting to the our proposed synset, we do not explicitly specify which one of the 5 synsets presented is our proposed synset. Section 5.8 provides experimental precision and recall scores for our canonicalization strategy. \n\n\nDataset Statistics and Analysis\n\nIn this section, we provide statistical insights and analysis for each component of Visual Genome. Specifically, we examine the distribution of images (Section 5.1) and the collected data for region descriptions (Section 5) and questions and answers (Section 5.7). We analyze region graphs and scene graphs together in one section (Section 5.6), but we also break up these graph structures into their three constituent parts-objects (Section 5.3), attributes (Section 5.4), and relationships (Section 5.5)-and study each part individually. Finally, we describe our canonicalization pipeline and results (Section 5.8).  \n\n\nImage Selection\n\nThe Visual Genome dataset consists of all 108, 249 images from the intersection of MS-COCO's (Lin et al., 2014) 328, 000 images and YFCC's (Thomee et al., 2016) 100 million images. These images are real-world, non-iconic images that were uploaded onto Flickr by users. The images range from as small as 72 pixels wide to as large as 1280 pixels wide, with an average width  of 500 pixels. We collected the WordNet synsets into which our 108, 249 images can be categorized using the same method as ImageNet (Deng et al., 2009). Visual Genome images cover 972 synsets. Figure 13 shows the top synsets to which our images belong. \"ski\" is the most common synset, with 2612 images; it is followed by \"ballplayer\" and \"racket,\" with all three synsets referring to images of people playing sports. Our dataset is somewhat biased towards images of people, as Figure 13 shows; however, they are quite diverse overall, as the top 25 synsets each have over 800 images, while the top 50 synsets each have over 500 examples.\n\n\nRegion Description Statistics\n\nOne of the primary components of Visual Genome is its region descriptions. Every image includes an aver- Fig. 17: The process used to convert a region description into a 300-dimensional vectorized representation.\n\nage of 42 regions with a bounding box and a descriptive phrase. Figure 14 shows an example image from our dataset with its 50 region descriptions. We display bounding boxes for only 6 out of the 50 descriptions in the figure to avoid clutter. These descriptions tend to be highly diverse and can focus on a single object, like in \"A bag,\" or on multiple objects, like in \"Man taking a photo of the elephants.\" They encompass the most salient parts of the image, as in \"An elephant taking food from a woman,\" while also capturing the background, as in \"Small buildings surrounded by trees.\" MS-COCO (Lin et al., 2014) dataset is good at generating variations on a single scene-level descriptor. Consider three sentences from MS-COCO dataset on a similar image: \"there is a person petting a very large elephant,\" \"a person touching an elephant in front of a wall,\" and \"a man in white shirt petting the cheek of an elephant.\" These three sentences are single scenelevel descriptions. In comparison, Visual Genome descriptions emphasize different regions in the image and thus are less semantically similar. To ensure diversity in the descriptions, we use BLEU score (Papineni et al., 2002) thresholds between new descriptions and all previously written descriptions. More information about crowdsourcing can be found in Section 4.\n\nRegion descriptions must be specific enough in an image to describe individual objects, like in the description \"A bag,\" but they must also be general enough to describe high-level concepts in an image, like \"An man being chased by a bear.\" Qualitatively, we note that regions that cover large portions of the image tend to be general descriptions of an image, while regions that cover only a small fraction of the image tend to be more specific. In Figure 15 (a), we show the distribution of regions over the width of the region normalized by the width of the image. We see that the majority of our regions tend to be around 10% to 15% of the image width. We also note that there are a large number of regions covering 100% of the image width. These regions usually include elements like \"sky,\" \"ocean,\" \"snow,\" \"mountains,\" etc. that cannot be bounded and thus span the entire image width. In Figure 15 (b), we show a similar distribution over the normalized height of the region. We see a similar overall pattern, as most of our regions tend to be very specific descriptions of about 10% to 15% of the image height. Unlike the distribution over width, however, we do not see a increase in the number of regions that span the entire height of the image, as there are no common visual equivalents that span images vertically. Out of all the descriptions gathered, only one or two of them tend to be global scene descriptions that are similar to MS-COCO (Lin et al., 2014).\n\nAfter examining the distribution of the size of the regions described, it is also valuable to look at the semantic information captured by these descriptions. In Figure 16, we show the distribution of the length (word count) of these region descriptions. The average word count for a description is 5 words, with a minimum of 1 word and a maximum of 12 words. In Figure 18 (a), we plot the most common phrases occurring in our region descriptions, with stop words removed. Common visual elements like \"green grass,\" \"tree [in] distance,\" and \"blue sky\" occur much more often than other, more nuanced elements like \"fresh strawberry.\" We also study descriptions with finer precision in Figure 18 (b), where we plot the most common words used in descriptions. Again, we eliminate stop words from our study. Colors like \"white\" and \"black\" are the most frequently used words to describe visual concepts; we conduct a similar study on other captioning datasets including MS-COCO (Lin et al., 2014) and Flickr 30K (Young et al., 2014) and find a similar distribution with colors occur-ring most frequently. Besides colors, we also see frequent occurrences of common objects like \"man,\" \"tree,\" and \"sign\" and of universal visual elements like \"sky.\"\n\nSemantic diversity. We also study the actual semantic contents of the descriptions. We use an unsupervised approach to analyze the semantics of these descriptions. Specifically, we use word2vec (Mikolov et al., 2013) to convert each word in a description to a 300-dimensional vector. Next, we remove stop words and average the remaining words to get a vector representation of the whole region description. This pipeline is outlined in Figure 17. We use hierarchical agglomerative clustering on vector representations of each region description and find 71 semantic and syntactic groupings or \"clusters.\" Figure 19 (a) shows four such example clusters. One cluster contains all descriptions related to tennis, like \"A man swings the racquet\" and \"White lines on the ground of the tennis court,\" while another cluster contains descriptions related to numbers, like \"Three dogs on the street\" and \"Two people inside the tent.\" To quantitatively measure the diversity of Visual Genome's region descriptions, we calculate the number of clusters represented in a single image's region descriptions. We show the distribution of the variety of descriptions for an image in Figure 19 (b). We find that on average, each image contains descriptions from 17 different clusters. The image with the least diverse descriptions contains descriptions from 4 clusters, while the image with the most diverse descriptions contains descriptions from 26 clusters.\n\nFinally, we also compare the descriptions in Visual Genome to the captions in MS-COCO. First we aggregate all Visual Genome and MS-COCO descriptions and remove all stop words. After removing stop words, the descriptions from both datasets are roughly the same length. We conduct a similar study, in which we vectorize the descriptions for each image and calculate each dataset's cluster diversity per image. We find that on average, 2 clusters are represented in the captions for each image in MS-COCO, with very few images in which 5 clusters are represented. Because each image in MS-COCO only contains 5 captions, it is not a fair comparison to compare the number of clusters represented in all the region descriptions in the Visual Genome dataset. We thus randomly sample 5 Visual Genome region descriptions per image and calculate the number of clusters in an image. We find that Visual Genome descriptions come from 4 or 5 clusters. We show our comparison results in Figure 19 (c). The difference between the semantic diversity between the two datasets is statistically significant (t = \u2212240, p < 0.01).  Fig. 18: (a) A plot of the most common visual concepts or phrases that occur in region descriptions. The most common phrases refer to universal visual concepts like \"blue sky,\" \"green grass,\" etc. (b) A plot of the most frequently used words in region descriptions. Colors occur the most frequently, followed by common objects like \"man\" and \"dog\" and universal visual concepts like \"sky.\"\n\n\nNumbers Cluster\n\nTwo people inside the tent.\n\n\nMany animals crossing the road.\n\nFive ducks almost in a row.\n\n\nThe number four.\n\nThree dogs on the street. Two towels hanging on racks.\n\n\nTennis Cluster\n\nWhite lines on the ground of the tennis court.\n\n\nA pair of tennis shoes.\n\nMetal fence securing the tennis court. Navy blue shorts on tennis player. The man swings the racquet. Tennis player preparing a backhand swing.\n\n\nOcean Cluster\n\nOcean is blue and calm.\n\n\nRows of waves in front of surfer.\n\nA group of men on a boat.\n\n\nSurfboard on the beach.\n\nWoman is surfing in the ocean. Foam on water's edge.\n\n\nTransportation Cluster\n\nLadder folded on fire truck. Dragon design on the motorcycle. Tall windshield on bike.\n\n\nFront wheels of the airplane.\n\nA bus rear view mirror. The front tire of the police car.  Objects in Visual Genome come from a variety of categories. As shown in Figure 22 (b), objects related to WordNet categories such as humans, animals, sports, and scenery are most common; this is consistent with the general bias in image subject matter in our dataset. Common objects like man, person, and woman occur especially frequently with occurrences of 24K, 17K, and 11K. Other objects that also occur in MS-COCO (Lin et al., 2014) are also well represented with around 5000 instances on average. Figure 22 (a) shows some examples of objects in images. Objects in Visual Genome span a diverse set of Wordnet categories like food, animals, and man-made structures.\n\nIt is important to look not only at what types of objects we have but also at the distribution of objects in images and regions. Figure 20 (a) shows, as expected, that we have between 0 and 2 objects in each region on average. It is possible for regions to contain no objects if their descriptions refer to no explicit objects in the image. For example, a region described as \"it is dark outside\" has no objects to extract. Regions with only one object generally have descriptions that focus on the attributes of a single object. On the other hand, regions with two or more objects generally have descriptions that contain both attributes of specific objects and relationships between pairs of objects.\n\nAs shown in Figure 20 (b), each image contains on average around 21 unique objects. Few images have a low number of objects, which we expect since images usually capture more than a few objects. Moreover, few images have an extremely high number of objects (e.g. over 40).   People are the most frequently occurring objects in our dataset, followed by common objects and visual elements like building, shirt, and sky.\n\n\nAttribute Statistics\n\nAttributes allow for detailed description and disambiguation of objects in our dataset. About 45% of objects in Visual Genome are annotated with at least one attribute; our dataset contains 1.6 million total attributes with 13, 041 unique attributes. Attributes include colors (green), sizes (tall), continuous action verbs (standing), materials (plastic), etc. Each attribute in our scene graphs belongs to one object, while each object can have multiple attributes. We denote attributes as attribute(object).\n\nOn average, each image in Visual Genome contains 21 attributes, as shown in Figure 23. Each region contains on average 1 attribute, though about 42% of regions contain no attribute at all; this is primarily because many regions are relationship-focused. Figure 24 (a) shows the distribution of the most common attributes in our dataset. Colors (e.g. white, green) are by far the most frequent attributes. Also common are sizes (e.g. large) and materials (e.g. wooden). Figure 24 (b) shows the distribution of attributes describing people (e.g. man, girls, and person). The most common attributes describing people are intransitive verbs describing their states of motion (e.g.standing and walking). Certain sports (skiing, surfboarding) are overrepresented due to a bias towards these sports in our images.\n\nAttribute Graphs. We also qualitatively analyze the attributes in our dataset by constructing co-occurrence graphs, in which nodes are unique attributes and edges connect those attributes that describe the same object. For example, if an image contained a \"large black dog\" (large(dog), black(dog)) and another image contained a \"large yellow cat\" (large(cat), yellow(cat)), its attributes would form an incomplete graph with edges (large, black) and (large, yellow). We create two such graphs: one for both the total set of attributes and a second where we consider only objects that refer to people. A subgraph of the 16 most frequently connected (co-occurring) personrelated attributes is shown in Figure 25 (a).\n\nCliques in these graphs represent groups of attributes in which at least one co-occurrence exists for each pair of attributes in that group. In the previous example, if a third image contained a \"black and yellow taxi\" (black(taxi), yellow(taxi)), the resulting third edge would create a clique between the attributes black, large, and yellow. When calculated across the entire Visual Genome dataset, these cliques provide insight into commonly perceived traits of different types of objects. Figure 25 (b) is a selected representation of three example cliques and their overlaps. From just a clique of attributes, we can predict what types of objects are usually referenced. In Figure 25 (b), we see that these cliques describe an animal (left), water body (top right), and human hair (bottom right).\n\nOther cliques (not shown) can also uniquely identify objects. In our set, one clique contains athletic, young, fit, skateboarding, focused, teenager, male, skinny, and happy, capturing some of the common traits of skateboarders in our set. Another such clique has shiny, small, metal, silver, rusty, parked, and empty, most likely describing a subset of cars. From these cliques, we can thus infer distinct objects and object types based solely on their attributes, potentially allowing for highly specific object identification based on selected characteristics. \n\n\nRelationship Statistics\n\nRelationships are the core components that link objects in our scene graphs. Relationships are directional, i.e. they involve two objects, one acting as the subject and one as the object of a predicate relationship. We denote all relationships in the form relationship(subject, object). For example, if a man is swinging a bat, we write swinging(man, bat). Relationships can be spatial (e.g. inside of), action (e.g. swinging), compositional (e.g. part of), etc. More complex relationships such as standing on, which includes both an action and a spatial aspect, are also represented. Relationships are extracted from region descriptions by crowd workers, similarly to attributes and objects. Visual Genome contains a total of 13, 894 unique relationships, with over 1.8 million total relationships. Figure 26 (a) shows the distribution of relationships per region description. On average, we have 1 relationship per region, with a maximum of 7. We also have some descriptions like \"an old, tall man,\" which have multiple attributes associated with the man but no relationships. Figure 26 (b) is a distribution of relationships per image object. Finally, Figure 26 (c) shows the distribution of relationships per image. Each image has an average of 19 relationships, with a minimum of 1 relationship and with ax maximum of over 60 relationships.\n\nTop relationship distributions. We display the most frequently occurring relationships in Figure 27 (a). on is the most common relationship in our dataset. This is primarily because of the flexibility of the word on, which can refer to spatial configuration (on top of), attachment (hanging on), etc. Other common relationships involve actions like holding and wearing and spatial configurations like behind, next to, and under. Figure 27 (b) shows a similar distribution but for relationships involving people. Here we notice more human-centric relationships or actions such as kissing, chatting with, and talking to. The two distributions follow a Zipf distribution.\n\nUnderstanding affordances. Relationships allow us to also understand the affordances of objects. We show this using a specific distribution of subjects and objects involved in the relationship riding in Figure 28. Figure 28 (a) shows the distribution for subjects while Figure 28 (b) shows a similar distribution for objects. Comparing the two distributions, we find clear patterns of people-like subject entities such as person, man, policeman, boy, and skateboarder that can ride other objects; the other distribution contains objects that afford riding, such as horse, bike, elephant, motorcycle, and skateboard. We can also learn specific common-sense knowledge, like that skateboarders only ride skateboards and only surfers ride waves or surfboards.\n\nRelated work comparison. It is also worth mentioning in this section some prior work on relationships. The concept of visual relationships has already been explored in Visual Phrases (Sadeghi and Farhadi, 2011), who introduced a dataset of 17 such relationships such as next to(person, bike) and riding(person, horse). However, their dataset is limited to just these 17 relationships. Similarly, the MS-COCO-a dataset (Ruggero Ronchi and Perona, 2015) introduced 140 actions that humans performed in MS-COCO's dataset (Lin et al., 2014). However, their dataset is limited to just actions, while our relationships are more general and numerous, with over 13K unique relationships. Finally, VisKE (Sadeghi et al., 2015) introduced 6500 relationships, but in a much smaller dataset of images than Visual Genome.   \n\n\nRegion and Scene Graph Statistics\n\nWe introduce in this paper the largest dataset of scene graphs to date. We use these graph representations of images as a deeper understanding of the visual world. In this section, we analyze the properties of these representations, both at the region level through region graphs and at the image level through scene graphs. We also briefly explore other datasets with scene graphs and provide aggregate statistics on our entire dataset.\n\nScene graphs by asking humans to write triples about an image (Johnson et al., 2015). However, unlike them, we collect graphs at a much more fine-grained level, the region graph. We obtained our graphs by asking workers to create them from the descriptions we collected from our regions. Therefore, we end up with multiple graphs for an image, one for every region description. Together, we can combine all the individual region graphs to aggregate a scene graph for an image. This scene graph is made up of all the individual region graphs. In our scene graph representation, we merge all the objects that referenced by multiple region graphs into one node in the scene graph. Each of our images has a distribution between 40 to 50 region graphs per image, with an average of 42. Each image has exactly one scene graph. Note that the number of region descriptions and the number of region graphs for an image are not the same. For example, consider the description \"it is a sunny day\". Such a description contains no objects, which are the building blocks of a region graph. Therefore, such descriptions have no region graphs associated with them.\n\nObjects, attributes, and relationships occur as a normal distribution in our data. Table 4 shows that in a region graph, there are an average of 0.43 objects, 0.41 attributes, and 0.45 relationships. Each scene graph and consequently each image has average of 21.26 objects, 16.21 attributes, and 18.67 relationships.\n\n\nQuestion Answering Statistics\n\nWe collected 1, 773, 258 question answering (QA) pairs on the Visual Genome images. Each pair consists of a question and its correct answer regarding the content of an image. On average, every image has 17 QA pairs. Rather than collecting unconstrained QA pairs as previous work has done (Antol et al., 2015, Gao et al., 2015, Malinowski and Fritz, 2014, each question in Visual Genome starts with one of the six Ws -what, where, when, who, why, and how. There are two major benefits to focusing on six types of questions. First, they offer a considerable coverage of question types, ranging from basic perceptual tasks (e.g. recognizing objects and scenes) to complex common sense reasoning (e.g. inferring motivations of people and causality of events). Second, these categories present a natural and consistent stratification of task difficulty, indicated by the baseline performance in Section 6.4. For instance, why questions that involve complex reasoning lead to the poorest performance (3.4% top-100 accuracy) of the six categories. This enables us to obtain a better understanding of the strengths and weaknesses of today's computer vision models, which sheds light on future directions in which to proceed.\n\nWe now analyze the diversity and quality of our questions and answers. Our goal is to construct a largescale visual question answering dataset that covers a diverse range of question types, from basic cognition tasks to complex reasoning tasks. We demonstrate the richness and diversity of our QA pairs by examining the distributions of questions and answers in Figure 29.\n\nQuestion type distributions. The questions naturally fall into the 6W categories via their interrogative words. Inside each of the categories, the second and following words categorize the questions with increasing granularity. Inspired by VQA (Antol et al., 2015), we show the distributions of the questions by their first three words in Figure 30. We can see that \"what\" is the most common of the six categories. A notable difference between our question distribution and VQA's is that we focus on ensuring that all 7 question categories are adequately represented, while in VQA, 32.37% of the questions are yes/no binary questions. As a result, a trivial model can achieve a reasonable performance by just predicting \"yes\" or \"no\" as answers. We encourage more difficult QA pairs by ruling out binary questions.\n\nQuestion and answer length distributions. We also analyze the question and answer lengths of each 6W category. Figure 31 shows the average question and answer lengths of each category. Overall, the average question and answer lengths are 5.7 and 1.8 words respectively. In contrast to the VQA dataset, where .88%, 8.38%, and 3.25% of the answers consist of one, two, or three words, our answers exhibit a long-tail distribution where 57.3%, 18.1%, and 15.7% of the answers have one, two, or three words respectively. We avoid verbosity by instructing the workers to write answers as concisely as possible. The coverage of long answers means that many answers contain a short description that contains more details than merely an object or an attribute. It shows the richness and complexity of our visual QA tasks beyond object-centric recognition tasks. We foresee that these long-tail questions can mo- Fig. 31: Question and answer lengths by question type. The bars show the average question and answer lengths of each question type. The whiskers show the standard deviations. The factual questions, such as \"what\" and \"how\" questions, usually come with short answers of a single object or a number. This is only because \"how\" questions are disproportionately counting questions that start with \"how many\". Questions from the \"where\" and \"why\" categories usually have phrases and sentences as answers.\n\ntivate future research in common-sense reasoning and high-level image understanding.\n\n\nCanonicalization Statistics\n\nIn order to reduce the ambiguity in the concepts of our dataset and connect it to other resources used by the research community, we canonicalize the semantic meanings of all objects, relationships, and attributes in Visual Genome. By \"canonicalization,\" we refer to word sense disambiguation (WSD) by mapping the components in our dataset to their respective synsets in the WordNet ontology (Miller, 1995). This mapping reduces the noise in the concepts contained in the dataset and also facilitates the linkage between Visual Genome and other data sources such as ImageNet (Deng et al., 2009), which is built on top of the WordNet ontology. Figure 32 shows an example image from the Visual Genome dataset with its components canonicalized. For example, horse is canonicalized as horse.n.01: solid-hoofed herbivorous quadruped domesticated since prehistoric times. Its attribute, clydesdale, is canonicalized as its breed clydesdale.n.01: heavy feathered-legged breed of draft horse originally from Scotland. We also show an example of a QA from We also only show a subset of the scene graph for this image to avoid cluttering the figure.\n\nwhich we extract the nouns shamrocks, symbol, and St. Patrick's day, all of which we canonicalize to WordNet as well.\n\nRelated work. Canonicalization, or WSD (Pal and Saha, 2015), has been used in numerous applications, including machine translation, information retrieval, and information extraction (Rothe andSch\u00fctze, 2015, Leacock et al., 1998). In English sentences, sentences like \"He scored a goal\" and \"It was his goal in life\" carry different meanings for the word \"goal.\" Understanding these differences is crucial for translating languages and for returning correct results for a query. Similarly, in Visual Genome, we ensure that all our components are canonicalized to understand how different objects are related to each other; for example, \"person\" is a hypernym of \"man\" and \"woman.\" Most past canonicalization models use precision, recall, and F1 score to evaluate on the Semeval dataset (Mihalcea et al., 2004). The current state-of-the-art performance on Semeval is an F1 score of 75.8% (Chen et al., 2014). Since our canonicalization setup is different from the Semeval benchmark (we have an open vocabulary and no annotated ground truth for evaluation), our canonicalization method is not directly comparable to these existing methods. We do however, achieve a similar precision and recall score on a held-out test set described below.\n\nRegion descriptions and QAs. We canonicalize all objects mentioned in all region descriptions and QA pairs.  Table 5: Precision, recall, and mapping accuracy percentages for object, attribute, and relationship canonicalization.\n\nBecause objects need to be extracted from the phrase text, we use Stanford NLP tools  to extract the noun phrases in each region description and QA, resulting in 99% recall of noun phrases from a subset of 200 region descriptions we manually annotated. After obtaining the noun phrases, we map each to its most frequent matching synset (according to Word-Net lexeme counts). This resulted in an overall mapping accuracy of 86% and a recall of 98.5%. The most common synsets extracted from region descriptions, QAs, and objects are shown in Figure 33.\n\nAttributes. We canonicalize attributes from the crowdextracted attributes present in our scene graphs. The \"attribute\" designation encompasses a wide range of grammatical parts of speech. Because part-of-speech taggers rely on high-level syntax information and thus fail on the disjoint elements of our scene graphs, we normalize each attribute based on morphology alone (so-called \"stemming\"). Then, as with objects, we map each attribute phrase to the most frequent matching WordNet synset. We include 15 hand-mapped rules to address common failure cases in which WordNet's frequency counts prefer abstract senses of words over the spatial senses present in visual data, e.g. \"short.a.01: limited in duration\" over short.a.02: lacking in length. For verification, we randomly sample 200 attributes, produce ground-truth mappings by hand, and compare them to the results of our algorithm. This resulted in a recall of 95.9% and a mapping accuracy of 83.5%. The most common attribute synsets are shown in Figure 34 (a).\n\nRelationships. As with attributes, we canonicalize the relationships isolated in our scene graphs. We exclude prepositions, which are not recognized in Word-Net, leaving a set primarily composed of verb relationships. Since the meanings of verbs are highly dependent upon their morphology and syntactic placement (e.g. passive cases, prepositional phrases), we map the structure of each relationship to the appropriate WordNet sentence frame and only consider those WordNet synsets with matching sentence frames.\n\nFor each verb-synset pair, we then consider the root hypernym of that synset to reduce potential noise from WordNet's fine-grained sense distinctions. We also include 20 hand-mapped rules, again to correct for WordNet's lower representation of concrete or spatial senses; for example, the concrete hold.v.02: have or hold in one's hand or grip is less frequent in WordNet than the abstract hold.v.01: cause to continue in a certain state. For verification, we again randomly sample 200 relationships and compare the results of our canonicalization against ground-truth mappings. This resulted in a recall of 88.5% and a mapping accuracy of 77.6%. While several datasets, such as VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), include semantic restrictions or frames to improve classification, there is no comprehensive method of mapping to those restrictions or frames. The most common relationship synsets are shown in Figure 34 (b).\n\n\nExperiments\n\nThus far, we have presented the Visual Genome dataset and analyzed its individual components. With such rich information provided, numerous perceptual and cognitive tasks can be tackled. In this section, we aim to provide baseline experimental results using components of Visual Genome that have not been extensively studied.\n\nObject detection is already a well-studied problem (Everingham et al., 2010, Girshick et al., 2014, Sermanet et al., 2013, Girshick, 2015, Ren et al., 2015b. Similarly, region graphs and scene graphs have been shown to improve semantic image retrieval (Johnson et al., 2015, Schuster et al., 2015. We therefore focus on the remaining components, i.e. attributes, relationships, region descriptions, and question answer pairs. In Section 6.1, we present results for two experiments on attribute prediction. In the first, we treat attributes independently from objects and train a classifier for each attribute, i.e. a classifier for red or a classifier for old, as in (Malisiewicz et al., 2008, Varma and Zisserman, 2005, Ferrari and Zisserman, 2007, Farhadi et al., 2009, Johnson et al., 2015. In the second experiment, we learn object and attribute classifiers jointly and predict object-attribute pairs (e.g. predicting that an apple is red), as in (Sadeghi and Farhadi, 2011).\n\nIn Section 6.2, we present two experiments on relationship prediction. In the first, we aim to predict the predicate between two objects, e.g. predicting the predicate kicking or wearing between two objects. This experiment is synonymous with existing work in action recognition (Gupta et al., 2009, Ramanathan et al., 2015. In another experiment, we study relationships by classifying jointly the objects and the predicate (e.g. predicting kicking(man, ball )); we show that this is a very difficult task due to the high variability in the appearance of a relationship (e.g. the ball might be on the ground or in mid-air above the man). These experiment are generalizations of tasks that study spatial relationships between objects and ones that jointly reason about the interaction of humans with objects (Yao andFei-Fei, 2010, Prest et al., 2012).\n\nIn Section 6.3 we present results for region captioning. This task is closely related to image captioning (Chen et al., 2015); however, results from the two are not directly comparable, as region descriptions are short, incomplete sentences. We train one of the top 16 state-of-the-art image caption generator (Karpathy and Fei-Fei, 2014) on (1) our dataset to generate region descriptions and on (2) Flickr30K (Young et al., 2014) to generate sentence descriptions. To compare results between the two training approaches, we use simple templates to convert region descriptions into complete sentences. For a more robust evaluation, we validate the descriptions we generate using human judgment.\n\nFinally, in Section 6.4, we experiment on visual question answering, i.e. given an image and a question, we attempt to provide an answer for the question. We report results on the retrieval of the correct answer from a list of existing answers.\n\n\nAttribute Prediction\n\nAttributes are becoming increasingly important in the field of computer vision, as they offer higher-level semantic cues for various problems and lead to a deeper understanding of images. We can express a wide variety of properties through attributes, such as form (sliced), function (decorative), sentiment (angry), and even intention (helping). Distinguishing between similar objects (Isola et al., 2015) leads to finer-grained classification, while describing a previously unseen class through attributes shared with known classes can enable \"zero-shot\" learning (Farhadi et al., 2009, Lampert et al., 2009. Visual Genome is the largest dataset of attributes, with 18 attributes per image for a total of 1.8 million attributes.\n\nSetup. For both experiments, we focus on the 100 most common attributes in our dataset. We only use objects that occur at least 100 times and are associated with one of the 100 attributes in at least one image. For both experiments, we follow a similar data pre-processing pipeline. First, we lowercase, lemmatize, and strip excess whitespace from all attributes. Since the number of examples per attribute class varies, we randomly sample 500 attributes from each category (if fewer than 500 are in the class, we take all of them).\n\nWe end up with around 50, 000 attribute instances and 43, 000 object-attribute pair instances in total. We use 80% of the images for training and 10% each for validation and testing. Because each image has about the same number of examples, this results in an approximately 80%-10%-10% split over the attributes themselves. The input data for this experiment is the cropped bounding box of the object associated with each attribute.\n\nWe train an attribute predictor by using features learned from a convolutional neural network. Specifi-\"playing\" (predicted \"grazing\") \"beautiful\" (predicted \"concrete\") \"metal\" (predicted \"closed\") \"dark\" (predicted \"dark\") \"parked\" (predicted \"parked\") \"white\" (predicted \"stuffed\") (a) \"green leaves\" (predicted \"white snow\") \"flying bird\" (predicted \"black jacket\") \"brown grass\" (predicted \"green grass\") \"red bus\" (predicted \"red bus\") \"skiing person\" (predicted \"skiing person\") \"white stripe\" (predicted \"black and white zebra\") (b) Fig. 35: (a) Example predictions from the attribute prediction experiment. Attributes in the first row are predicted correctly, those in the second row differ from the ground truth but still correctly classify an attribute in the image, and those in the third row are classified incorrectly. The model tends to associate objects with attributes (e.g. elephant with grazing). (b) Example predictions from the joint object-attribute prediction experiment.\n\ncally, we fine-tune a 16-layer VGG network (Simonyan and Zisserman, 2014) for both of these experiments using the 50, 000 attribute and 43, 000 object-attribute pair instances respectively. We modify the network so that the learning rate of the final fully-connected layer is 10 times that of the other layers, as this improves convergence time. We use a base learning rate of 0.001, which we scale by 0.1 every 200 iterations, and momentum and weight decays of 0.9 and 0.0005 respectively. We use the fine-tuned features from the network and train 100 individual SVMs (Hearst et al., 1998) to predict each attribute. We output multiple attributes for each bounding box input. For the second experiment, we also output the object class.\n\nResults. 17%. This implies that some attributes occur exclusively with a small number of objects. Additionally, by jointly learning attributes with objects, we increase the inter-class variance, making the classification process an easier task. Figure 35 (a) shows example predictions for the first attribute prediction experiment. In general, the model is good at associating objects with their most salient attributes, for example, animal with stuffed and elephant with grazing. However, there is some difference between the user-provided result and the correct ground truth, so the model incorrectly classifies some correct predictions. For example, the white stuffed animal is correct but evaluated as incorrect. Figure 35 (b) shows example predictions for the second experiment in which we also predict the object. While the results in the second row might be considered correct, to keep a consistent evaluation, we mark them as incorrect. For example, the predicted \"green grass\" might be considered subjectively correct even though it is annotated as \"brown grass\". For cases where the objects are not clearly visible but are abstract outlines, our model is unable to predict attributes or objects accurately. For example, it thinks that the \"flying bird\" is actually a \"black jacket\".  The attribute clique graphs in Section 5.4 clearly show that learning attributes can help us identify types of objects. This experiment strengthens that insight. We learn that studying attributes together with objects can improve attribute prediction.\n\n\nRelationship Prediction\n\nWhile objects are the core building blocks of an image, relationships put them in context. These relationships help distinguish between images that contain the same objects but have different holistic interpretations. For example, an image of \"a man riding a bike\" and \"a man falling off a bike\" both contain man and bike, but the relationship (riding vs. falling off) changes how we perceive both situations. Visual Genome is the largest known dataset of relationships, with a total of 1.8 million relationships and an average of 18 relationships per image.\n\nSetup. The setups of both experiments are similar to those of the experiments we performed on attributes. We again focus on the top 100 most frequent relationships. We lowercase, lemmatize, and strip excess whitespace from all relationships. We end up with around 34, 000 relationships and 27, 000 subject-relationshipobject triples for training, validation, and testing. The input data to the experiment is the image region containing the union of the bounding boxes of the subject and object (essentially, the bounding box containing the two object boxes). We fine-tune a 16-layer VGG network (Simonyan and Zisserman, 2014) with the same learning rates mentioned in Section 6.1.\n\nResults. Overall, we find that relationships are not visually distinct enough for our discriminative model to learn effectively. Table 7 shows results for both experiments. For relationship classification, we converge after around 800 iterations with 8.74% top-one accuracy and 29.69% top-five accuracy. Unlike attribute prediction, the accuracy results for relationships are much lower  because of the high intra-class variability of most relationships. For the second experiment jointly predicting the relationship and its two object classes, we converge after around 450 iterations with 25.83% top-one accuracy and 65.57% top-five accuracy. We notice that object classification aids relationship prediction. Some relationships occur with some objects and never others; for example, the relationship drive only occurs with the object person and never with any other objects (dog, chair, etc.). Figure 36 (a) shows example predictions for the relationship classification experiment. In general, the model associates object categories with certain relationships (e.g. animals with eating or drinking, bikes with riding, and kids with playing). Figure 36 (b), structured as in Figure 36 (a), shows example predictions for the joint prediction of relationships with its objects. The model is able to predict the salient features of the image (e.g. \"boat in water\") but fails to distinguish between different objects (e.g. boy vs. woman and car vs. bus in the bottom row).\n\n\nGenerating Region Descriptions\n\nGenerating sentence descriptions of images has gained popularity as a task in computer vision (Kiros et al., 2014, Mao et al., 2014, Karpathy and Fei-Fei, 2014, Vinyals et al., 2014; however, current state-of-the-art models fail to describe all the different events captured in an image and instead provide only a high-level summary of the image. In this section, we test how well state-of-the-art models can caption the details of images. For both experiments, we use the NeuralTalk model (Karpathy and Fei-Fei, 2014), since it not only provides state-of-the-art results but also is shown to be robust enough for predicting short descriptions. We train NeuralTalk on the Visual Genome dataset for region descriptions and on Flickr30K (Young et al., 2014) for full sentence descriptions. As a model trained on other datasets would generate complete sentences and would not be comparable (Chen et al., 2015) to our region descriptions, we convert all region descriptions generated by our model into complete sentences using predefined templates (Hou et al., 2002).\n\nDog \"carrying\" frisbee (predicted: \"laying on\") Boy \"playing\" soccer (predicted \"playing\") Sheep \"eating\" grass (predicted \"eating\")\n\nBike \"attached to\" rack (predicted \"riding\")\n\nBag \"inside\" rickshaw (predicted \"riding\") Shadow \"from\" zebra (predicted \"drinking\") (a)\n\n\"glass on table\" (predicted \"plate on table\") \"car on road\" (predicted \"bus on street\") \"train on tracks\" (predicted \"train on tracks\") \"leaf on tree\" (predicted \"building in background\") \"boat in water\" (predicted \"boat in water\") \"boy has hair\" (predicted \"woman wearing glasses\") Setup. For training, we begin by preprocessing region descriptions; we remove all non-alphanumeric characters and lowercase and strip excess whitespace from them. We have 4, 158, 841 region descriptions in total. We end up with 3, 150, 000 region descriptions for training -504, 420 each for validation and testing. Note that we ensure descriptions of regions from the same image are exclusively in the training, validation, or testing set. We feed the bounding boxes of the regions through the pretrained VGG 16-layer network (Simonyan and Zisserman, 2014) to get the 4096-dimensional feature vectors of each region. We then use the NeuralTalk (Karpathy and Fei-Fei, 2014) model to train a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to generate descriptions of regions. We use a learning rate of 0.001 trained with rmsprop (Dauphin et al., 2015). The model converges after four days. For testing, we crop the ground-truth region bounding boxes of images and extract their 4096-dimensional 16-layer VGG network (Simonyan and Zisserman, 2014) features. We then feed these vectors through the pretrained NeuralTalk model to get predictions for region descriptions.\n\nResults. Table 8 shows the results for the experiment. We calculate BLEU, CIDEr, and METEOR scores (Chen et al., 2015) between the generated descriptions and their ground-truth descriptions. In all cases, the model trained on VisualGenome performs better. Moreover, we asked crowd workers to evaluate whether a generated description was correct-we got 1.6% and 43.03% for models trained on Flickr30K and on Visual Genome, respectively. The large increase in accuracy when the model trained on our data is due to the specificity of our dataset. Our region descriptions are shorter and cover a smaller image area. In comparison, the Flickr30K data are generic descriptions of entire images with multiple events happening in different regions of the image. The model trained on our data is able to make predictions that are more likely to concentrate on the specific part of the image it is looking at, instead of generating a summary description. The objectively low accuracy in both cases illustrates that current models are unable to reason about complex images. Figure 37 shows examples of regions and their predicted descriptions. Since many examples have short \"a black motorcycle\" \"trees in background\" \"train is visible\" \"the umbrella is red\" \"black and white cow\" \"a kite in the sky\" descriptions, the predicted descriptions are also short as expected; however, this causes the model to fail to produce more descriptive phrases for regions with multiple objects or with distinctive objects (i.e. objects with many attributes). While we use templates to convert region descriptions into sentences, future work can explore smarter approaches to combine region descriptions and generate a paragraph connecting all the regions into one coherent description.\n\n\nQuestion Answering\n\nVisual Genome is currently the largest dataset of visual question answers with 1.7 million question and answer pairs. Each of our 108, 249 images contains an average of 17 question answer pairs. Answering questions requires a deeper understanding of an image than generic image captioning. Question answering can involve fine-grained recognition (e.g. \"What is the breed of the dog?\"), object detection (e.g. \"Where is the kite in the image?\"), activity recognition (e.g. \"What is this man doing?\"), knowledge base reasoning (e.g. \"Is this glass full?\"), and common-sense reasoning (e.g. \"What street will we be on if we turn right?\").\n\nBy leveraging the detailed annotations in the scene graphs in Visual Genome, we envision building smart  Table 9: Baseline QA performances (in accuracy).\n\nmodels that can answer a myriad of visual questions. While we encourage the construction of smart models, in this paper, we provide some baseline metrics to help others compare their models.\n\nSetup. We split the QA pairs into a training set (60%) and a test set (40%). We ensure that all images are exclusive to either the training set or the test set. We implement a simple baseline model that relies on answer frequency. The model counts the top k most frequent answers (similar to the ImageNet challenge (Russakovsky et al., 2015)) in the training set as the predictions for all the test questions, where k = 100, 500, and 1000. We let a model make k different predictions. We say the model is correct on a QA if one of the k predictions matches exactly with the groundtruth answer. We report the accuracy over all test questions. This evaluation method works well when the answers are short, especially for single-word answers. However, it causes problems when the answers are long phrases and sentences. Other evaluation methods require word ontologies (Malinowski and Fritz, 2014), multiple choices (Antol et al., 2015, Yu et al., 2015, or human judges (Gao et al., 2015).\n\nResults. Table 9 shows the performance of the openended visual question answering task. These baseline results imply the long-tail distribution of the answers. Long-tail distribution is common in existing QA datasets as well (Antol et al., 2015, Malinowski andFritz, 2014). The top 100, 500, and 1000 most frequent answers only cover 41.1%, 57.3%, and 64.1% of the correct answers. In comparison, the corresponding sets of frequent answers in VQA (Antol et al., 2015) cover 63%, 75%, and 80% of the test set answers. The \"where\" and \"why\" questions, which tend to involve spatial and common sense reasoning, tend to have more diverse answers and hence perform poorly, with performances of 0.096% and 0.024% top-100 respectively. The top 1000 frequent answers cover only 41.8% and 18.7% of the correct answers from these two question types respectively.  Table 8: Results for the region description generation experiment. Scores in the first row are for the region descriptions generated from the NeuralTalk model trained on Flickr8K, and those in the second row are for those generated by the model trained on Visual Genome data. BLEU, CIDEr, and METEOR scores all compare the predicted description to a ground truth in different ways.\n\n\nFuture Applications\n\nWe have analyzed the individual components of this dataset and presented experiments with baseline results for tasks such as attribute classification, relationship classification, description generation, and question answering. There are, however, more applications and experiments for which our dataset can be used. In this section, we note a few potential applications that our dataset can enable.\n\nDense image captioning. We have seen numerous image captioning papers (Kiros et al., 2014, Mao et al., 2014, Karpathy and Fei-Fei, 2014, Vinyals et al., 2014 that attempt to describe an entire image with a single caption. However, these captions do not exhaustively describe every part of the scene. An natural extension to this application, which the Visual Genome dataset enables, is the ability to create dense captioning models that describe parts of the scene.\n\nVisual question answering. While visual question answering has been studied as a standalone task (Yu et al., 2015, Ren et al., 2015a, Antol et al., 2015, Gao et al., 2015, we introduce a dataset that combines all of our question answers with descriptions and scene graphs. Future work can build supervised models that utilize various components of Visual Genome to tackle question answering.\n\nImage understanding. While we have seen a surge of image captioning (Kiros et al., 2014) and question answering (Antol et al., 2015) models, there has been little work on creating more comprehensive evaluation metrics to measure how well these models are performing. Such models are usually evaluated using BLEU, CIDEr, or METEOR and other similar metrics that do not effectively measure how well these models understand the image (Chen et al., 2015). The Visual Genome scene graphs can be used as a measurement for image understanding. Generated descriptions and answers can be matched against the ground truth scene graph of an image to evaluate its corresponding model.\n\nRelationship extraction. Relationship extraction has been extensively studied in information retrieval and natural language processing (Zhou et al., 2007, GuoDong et al., 2005, Culotta and Sorensen, 2004, Socher et al., 2012. Visual Genome is the first largescale visual relationship dataset. This dataset can be used to study the extraction of visual relationships (Sadeghi et al., 2015) from images, and its interactions between objects can also be used to study action recognition (Yao andFei-Fei, 2010, Ramanathan et al., 2015) and spatial orientation between objects (Gupta et al., 2009, Prest et al., 2012.\n\nSemantic image retrieval. Previous work has already shown that scene graphs can be used to improve semantic image search (Johnson et al., 2015, Schuster et al., 2015. Further methods can be explored using our region descriptions combined with region graphs. Attention-based search methods can also be explored where the area of interest specified by a query is also localized in the retrieved images.\n\n\nConclusion\n\nVisual Genome provides a multi-layered understanding of pictures. It allows for a multi-perspective study of an image, from pixel-level information like objects, to relationships that require further inference, and to even deeper cognitive tasks like question answering. It is a comprehensive dataset for training and benchmarking the next generation of computer vision models. With Visual Genome, we expect these models to develop a broader understanding of our visual world, complementing computers' capacities to detect objects with abilities to describe those objects and explain their interactions and relationships. Visual Genome is a large formalized knowledge representation for visual understanding and a more complete set of descriptions and question answers that grounds visual concepts to language.\n\nFig. 3\n3Fig. 3: An example image from our dataset along with its scene graph representation. The scene graph contains objects (child, instructor, helmet, etc.) that are localized in the image as bounding boxes (not shown). These objects also have attributes: large, green, behind, etc. Finally, objects are connected to each other through relationships: wears(child, helmet), wears(instructor, jacket), etc.\n\nFig. 5 :\n5To describe all the contents of and interactions\n\nFig. 7\n7Fig. 7: Some descriptions also provide attributes for objects. For example, the region description \"yellow fire hydrant\" adds that the fire hydrant is yellow. Here we show two attributes: yellow and standing.\n\nFig. 8 :\n8Our dataset also captures the relationships and interactions between objects in our images. In this example, we show the relationship jumping over between the objects man and fire hydrant.\n\nFig. 9 :\n9(a) Age and (b) gender distribution of Visual Genome's crowd workers.Fig. 10: Good (left) and bad (right) bounding boxes for the phrase \"a street with a red car parked on the side,\" judged on coverage.\n\nFig. 11 :\n11Good (left)  and bad (right) bounding boxes for the object fox, judged on both coverage as well as quality.\n\nFig. 12 :\n12Each object (fox) has only one bounding box referring to it (left). Multiple boxes drawn for the same object (right) are combined together if they have a minimum threshold of 0.9 intersection over union.\n\nFig. 13 :\n13A distribution of the top 25 image synsets in the Visual Genome dataset. A variety of synsets are well represented in the dataset, with the top 25 synsets having at least 800 example images each.\n\nFig. 14 :\n14(a) An example image from the dataset with its region descriptions. We only display localizations for 6 of the 42 descriptions to avoid clutter; all 50 descriptions do have corresponding bounding boxes. (b) All 42 region bounding boxes visualized on the image.\n\nFig. 15 :\n15(a) A distribution of the width of the bounding box of a region description normalized by the image width. (b) A distribution of the height of the bounding box of a region description normalized by the image height.\n\nFig. 16 :\n16A distribution of the number of words in a region description. The average number of words in a region description is 5, with shortest descriptions of 1 word and longest descriptions of 16 words.\n\nFig. 19 :\n19(a) Example illustration showing four clusters of region descriptions and their overall themes. Other clusters not shown due to limited space. (b) Distribution of images over number of clusters represented in each image's region descriptions. (c) We take Visual Genome with 5 random descriptions taken from each image and MS-COCO dataset with all 5 sentence descriptions per image and compare how many clusters are represented in the descriptions. We show that Visual Genome's descriptions are more varied for a given image, with an average of 4 clusters per image, while MS-COCO's images have an average of 3 clusters per image.\n\nFig. 20 :Fig. 21 :\n2021(a) Distribution of the number of objects per region. Most regions have between 0 and 2 objects. (b) Distribution of the number of objects per image. Most images contain between 15 and 20 objects.5.3 Object StatisticsIn comparison to related datasets, Visual Genome fares well in terms of object density and diversity. Visual Genome contains approximately 21 objects per image, exceeding ImageNet(Deng et al., 2009), PASCAL (Everingham et al., 2010), MS-COCO (Lin et al., 2014), and other datasets by large margins. As shown in Figure 21, there are more object categories represented in Visual Genome than in any other dataset. This comparison is especially pertinent with regards to Microsoft MS-COCO (Lin et al., 2014), which uses the same images as Visual Genome. The lower count of objects per category is a result of our higher number of categories. For a fairer comparison with ILSVRC 2014 Detection (Russakovsky et al., 2015), Visual Genome has about 2239 objects per category when only the top 200 categories are considered, which is comparable to ILSVRC's 2671.5 objects per category. For a fairer comparison with MS-COCO, Visual Genome has about 3768 objects per category when only the top 91 Comparison of object diversity between various datasets. Visual Genome far surpasses other datasets in terms of number of object categories.categories are considered. This is comparable to MS-COCO's(Lin et al., 2014) when we consider just the 108, 249 MS-COCO images in Visual Genome.\n\nFig. 22 :\n22(a) Examples of objects in Visual Genome. Each object is localized in its image with a tightly drawn bounding box. (b) Plot of the most frequently occurring objects in images.\n\nFig. 23 :\n23Distribution of the number of attributes (a) per image, (b) per region description, (c) per object.\n\nFig. 24 :Fig. 25 :\n2425(a) Distribution showing the most common attributes in the dataset. Colors (white, red) and materials (wooden, metal) are the most common. (b) Distribution showing the number of attributes describing people. State-of-motion verbs (standing, walking) are the most common, while certain sports (skiing, surfing) are also highly represented due to an image source bias in our image set. (a) Graph of the person-describing attributes with the most co-occurrences. Edge thickness represents the frequency of co-occurrence of the two nodes. (b) A subgraph showing the co-occurrences and intersections of three cliques, which appear to describe water (top right), hair (bottom right), and some type of animal (left). Edges between cliques have been removed for clarity.\n\nFig. 26 :\n26Distribution of relationships (a) per image region, (b) per image object, (c) per image.\n\nFig. 27 :\n27(a) A sample of the most frequent relationships in our dataset. In general, the most common relationships are spatial (on top of, on side of, etc.). (b) A sample of the most frequent relationships involving humans in our dataset. The relationships involving people tend to be more action oriented (walk, speak, run, etc.\n\nFig. 28 :\n28(a) Distribution of subjects for the relationship riding. (b) Distribution of objects for the relationship riding. Subjects comprise of people-like entities like person, man, policeman, boy, and skateboarder that can ride other objects. On the other hand, objects like horse, bike, elephant and motorcycle are entities that can afford riding.\n\nFig. 29 :\n29Example QA pairs in the Visual Genome dataset. Our QA pairs cover a spectrum of visual tasks from recognition to high-level reasoning.\n\nFig. 30 :\n30Distribution of question types by starting words. This figure shows the distribution of the questions by their first three words. The angles of the regions are proportional to the number of pairs from the corresponding categories. We can see that \"what\" questions are the largest category with nearly half of the QA pairs.\n\nFig. 33 :\n33Distribution of the 25 most common synsets mapped from (a) region descriptions and question answers and (b) objects.\n\nFig. 34 :\n34Distribution of the 25 most common synsets mapped from (a) attributes and (b) relationships.\n\nFig. 36 :\n36(a) Example predictions from the relationship prediction experiment. Relationships in the first row are predicted correctly, those in the second row differ from the ground truth but still correctly classify a relationship in the image, and those in the third row are classified incorrectly. The model learns to associate animals leaning towards the ground as eating or drinking and bikes with riding. (b) Example predictions from the relationship-objects prediction experiment. The figure is organized in the same way asFigure (a). The model is able to predict the salient features of the image but fails to distinguish between different objects (e.g. boy and woman and car and bus in the bottom row).\n\nFig. 37 :\n37Example predictions from the region description generation experiment. Regions in the first column (left) accurately describe the region, and those in the second column (right) are incorrect and unrelated to the corresponding region.\n\n\nFig. 4: A representation of the Visual Genome dataset. Each image contains region descriptions that describe a localized portion of the image. We collect two types of question answer pairs (QAs): freeform QAs and region-based QAs. Each region is converted to a region graph representation of objects, attributes, and pairwise relationships. Finally, each of these region graphs are combined to form a scene graph with all the objects grounded to the image.Questions \n\nRegion Descriptions \n\nRegion Graphs \n\nScene Graph \n\nLegend: \nobjects \nrelationships \nattributes \n\nfire hydrant \n\nyellow \n\nfire hydrant \n\nman \n\nwoman \n\nstanding \n\njumping over \n\nman \nshorts \n\nin \nis behind \n\nfire hydrant \n\nman \n\njumping over \n\nwoman \n\nstanding \n\nshorts \n\nin \n\nis behind \n\nyellow \n\nwoman in shorts is \nstanding behind \nthe man \n\nyellow fire hydrant \n\nQ. \nWhat is the woman standing next to? \n\nA. \nHer belongings. \n\nQ. \nWhat color is the fire hydrant? \n\nA. \nYellow. \n\nman jumping over \nfire hydrant \n\nRegion Based Question Answers \nFree Form Question Answers \n\nBest viewed in color \n2 Visual Genome Data Representation \n\n\n\nTable 2\n2outlines the percentage distribution of the locations of the workers. 93.02% of workers contributed from the United States.Country \nDistribution \n\nUnited States \n93.02% \nPhilippines \n1.29% \nKenya \n1.13% \nIndia \n0.94% \nRussia \n0.50% \nCanada \n0.47% \n(Others) \n2.65% \n\n\n\n\nThe elephant with a seat on top A woman with a purple dress. A pair of pink flip flops. A handle of bananas. Tree near the water A blue short. Small houses on the hillside A woman feeding an elephant A woman wearing a white shirt and shorts A man taking a pictureGirl feeding elephant \nMan taking picture \nHuts on a hillside \nA man taking a picture. \nFlip flops on the ground \nHillside with water below \nElephants interacting with people \nYoung girl in glasses with backpack \nElephant that could carry people \nAn elephant trunk taking two bananas. \nA bush next to a river. \nPeople watching elephants eating \nA woman wearing glasses. \nA bag \nGlasses on the hair. \nA man wearing an orange shirt \nAn elephant taking food from a woman \nA woman wearing a brown shirt \nA woman wearing purple clothes \nA man wearing blue flip flops \nMan taking a photo of the elephants \nBlue flip flop sandals \nThe girl's white and black handbag \nThe girl is feeding the elephant \nThe nearby river \nA woman wearing a brown t shirt \nElephant's trunk grabbing the food \nThe lady wearing a purple outfit \nA young Asian woman wearing glasses \nElephants trunk being touched by a hand \nA man taking a picture holding a camera \nElephant with carrier on it's back \nWoman with sunglasses on her head \nA body of water \nSmall buildings surrounded by trees \nWoman wearing a purple dress \nTwo people near elephants \nA man wearing a hat \nA woman wearing glasses \nLeaves on the ground \n\n\n\nTable 3 :\n3Comparison of Visual Genome objects and categories to related datasets.Street Light \nGlass \n\nBench \nPizza \n\nStop Light \nBird \n\nBuilding \nBear \n\nPlane \nTruck \n\n\n\n\n).Objects Attributes Relationships \n\nRegion Graph 0.43 \n0.41 \n0.45 \nScene Graph \n21.26 \n16.21 \n18.67 \n\n\n\nTable 4 :\n4The average number of objects, attributes, and relationships per region graph and per scene graph.\n\nTable 6\n6shows results for both experiments. For the first experiment on attribute prediction, we converge after around 700 iterations with 18.97% topone accuracy and 43.11% top-five accuracy. Thus, attributes (like objects) are visually distinguishable from each other. For the second experiment where we also predict the object class, we converge after around 400 iterations with 43.17% top-one accuracy and 71.97% top-five accuracy. Predicting objects jointly with attributes increases the top-one accuracy from 18.97% to 43.\n\n\nTop-1 Accuracy Top-5 AccuracyAttribute \n18.97% \n43.11% \nObject-Attribute \n43.17% \n71.97% \n\n\n\nTable 6 :\n6(First row) Results for the attribute prediction task where we only predict attributes for a given image crop. (Second row) Attribute-object prediction experiment where we predict both the attributes as well as the object from a given crop of the image.\n\n\nTop-1 Accuracy Top-5 AccuracyRelationship \n8.74% \n26.69% \nSub./Rel./Obj. \n25.83% \n65.57% \n\n\n\nTable 7 :\n7Results for relationship classification (first row) and joint classification (second row) experiments.\n\n\ntop-100 top-500 top-1000What \n0.420 0.602 \n0.672 \nWhere 0.096 0.324 \n0.418 \nWhen 0.714 0.809 \n0.834 \nWho \n0.355 0.493 \n0.605 \nWhy \n0.034 0.118 \n0.187 \nHow \n0.780 0.827 \n0.846 \n\nOverall 0.411 0.573 \n0.641 \n\n\n\n\nBLEU-1 BLEU-2 BLEU-3 BLEU-4 CIDEr METEOR HumanFlickr8K 0.09 \n0.01 \n0.002 0.0004 0.05 \n0.04 \n1.6% \nVG \n0.17 \n0.05 \n0.02 \n0.01 \n0.30 \n0.09 \n43.03% \n\n\nHowever, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects inRanjay Krishna Stanford University, Stanford, CA, USA E-mail: ranjaykrishna@cs.stanford.edu Yuke Zhu Stanford University, Stanford, CA, USA Oliver Groth Dresden University of Technology, Dresden, Germany Justin Johnson Stanford University, Stanford, CA, USA Kenji Hata Stanford University, Stanford, CA, USA\nLi-Jia Li Snapchat Inc., Los Angeles, CA, USA David A. Shamma Yahoo Inc., San Francisco, CA, USA Michael S. Bernstein Stanford University, Stanford, CA, USA\nRanjay Krishna et al.\nAcknowledgements We would like to start by thanking our sponsors: Stanford Computer Science Department, Yahoo Labs!, The Brown Institute for Media Innovation, Toyota and Adobe. Next, we specially thank Michael Stark, Yutian Li, Frederic Ren, Sherman Leung, Michelle Guo and Gavin Mai for their contributions. We thank Carsten Rother from the University of Dresden for facilitating Oliver Groth's involvement. We also thank all the thousands of crowd workers for their diligent contribution to Visual Genome. Finally, we thank all members of the Stanford Vision Lab and HCI Lab for their useful comments and discussions.\nAntol, arXiv:1505.00468Vqa: Visual question answering. arXiv preprintAntol et al., 2015. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. (2015). Vqa: Visual question answering. arXiv preprint arXiv:1505.00468.\n\nThe berkeley framenet project. Baker, Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics. the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational LinguisticsStroudsburg, PA, USAAssociation for Computational Linguistics1ACL '98Baker et al., 1998. Baker, C. F., Fillmore, C. J., and Lowe, J. B. (1998). The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Compu- tational Linguistics and 17th International Conference on Computational Linguistics -Volume 1, ACL '98, pages 86- 90, Stroudsburg, PA, USA. Association for Computational Linguistics.\n\nToward never ending language learning. Betteridge, AAAI Spring Symposium: Learning by Reading and Learning to Read. Betteridge et al., 2009. Betteridge, J., Carlson, A., Hong, S. A., Hruschka Jr, E. R., Law, E. L., Mitchell, T. M., and Wang, S. H. (2009). Toward never ending language learn- ing. In AAAI Spring Symposium: Learning by Reading and Learning to Read, pages 1-2.\n\nCulture and human development: A new look. Bruner, J Bruner, Human development. 336Bruner, 1990. Bruner, J. (1990). Culture and human devel- opment: A new look. Human development, 33(6):344-355.\n\nA shortest path dependency kernel for relation extraction. Mooney ; Bunescu, R C Bunescu, R J Mooney, Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. the conference on Human Language Technology and Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsBunescu and Mooney, 2005. Bunescu, R. C. and Mooney, R. J. (2005). A shortest path dependency kernel for re- lation extraction. In Proceedings of the conference on Hu- man Language Technology and Empirical Methods in Nat- ural Language Processing, pages 724-731. Association for Computational Linguistics.\n\nSemantic parsing for text to 3d scene generation. Chang , 17Chang et al., 2014. Chang, A. X., Savva, M., and Manning, C. D. (2014). Semantic parsing for text to 3d scene gener- ation. ACL 2014, page 17.\n\nMicrosoft coco captions: Data collection and evaluation server. Chen , arXiv:1504.00325arXiv preprintChen et al., 2015. Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. (2015). Mi- crosoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.\n\nA unified model for word sense representation and disambiguation. Chen , EMNLP. CiteseerChen et al., 2014. Chen, X., Liu, Z., and Sun, M. (2014). A unified model for word sense representation and disam- biguation. In EMNLP, pages 1025-1035. Citeseer.\n\nNeil: Extracting visual knowledge from web data. Chen , Computer Vision (ICCV), 2013 IEEE International Conference on. IEEEChen et al., 2013. Chen, X., Shrivastava, A., and Gupta, A. (2013). Neil: Extracting visual knowledge from web data. In Computer Vision (ICCV), 2013 IEEE International Con- ference on, pages 1409-1416. IEEE.\n\nUnderstanding indoor scenes using 3d geometric phrases. Choi, Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. the 42nd Annual Meeting on Association for Computational LinguisticsAssociation for Computational Linguistics423Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference onChoi et al., 2013. Choi, W., Chao, Y.-W., Pantofaru, C., and Savarese, S. (2013). Understanding indoor scenes using 3d geometric phrases. In Computer Vision and Pattern Recog- nition (CVPR), 2013 IEEE Conference on, pages 33-40. IEEE. Culotta and Sorensen, 2004. Culotta, A. and Sorensen, J. (2004). Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 423. Association for Computational Linguistics.\n\nDauphin, arXiv:1502.04390Rmsprop and equilibrated adaptive learning rates for non-convex optimization. arXiv preprintDauphin et al., 2015. Dauphin, Y. N., de Vries, H., Chung, J., and Bengio, Y. (2015). Rmsprop and equilibrated adap- tive learning rates for non-convex optimization. arXiv preprint arXiv:1502.04390.\n\nImagenet: A large-scale hierarchical image database. Deng, Computer Vision and Pattern Recognition. IEEECVPR 2009. IEEE Conference onDeng et al., 2009. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hi- erarchical image database. In Computer Vision and Pat- tern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248-255. IEEE.\n\nPedestrian detection: An evaluation of the state of the art. Pattern Analysis and Machine Intelligence. Dollar, IEEE Transactions on. 344Dollar et al., 2012. Dollar, P., Wojek, C., Schiele, B., and Perona, P. (2012). Pedestrian detection: An evaluation of the state of the art. Pattern Analysis and Machine Intelli- gence, IEEE Transactions on, 34(4):743-761.\n\nThe pascal visual object classes (voc) challenge. Everingham, International journal of computer vision. 882Everingham et al., 2010. Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. (2010). The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303-338.\n\nDescribing objects by their attributes. Farhadi, Computer Vision and Pattern Recognition. IEEECVPR 2009. IEEE Conference onFarhadi et al., 2009. Farhadi, A., Endres, I., Hoiem, D., and Forsyth, D. (2009). Describing objects by their attributes. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1778-1785. IEEE.\n\nEvery picture tells a story: Generating sentences from images. Farhadi, Computer Vision-ECCV 2010. SpringerFarhadi et al., 2010. Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., and Forsyth, D. (2010). Every picture tells a story: Generating sentences from images. In Computer Vision-ECCV 2010, pages 15-29. Springer.\n\nLearning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding. Fei-Fei, 106Fei-Fei et al., 2007. Fei-Fei, L., Fergus, R., and Perona, P. (2007). Learning generative visual models from few train- ing examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Under- standing, 106(1):59-70.\n\nLearning visual attributes. Zisserman ; Ferrari, V Ferrari, A Zisserman, Advances in Neural Information Processing Systems. Ferrari and Zisserman, 2007. Ferrari, V. and Zisserman, A. (2007). Learning visual attributes. In Advances in Neural Information Processing Systems, pages 433-440.\n\nBuilding watson: An overview of the deepqa project. Ferrucci, AI magazine. 313Ferrucci et al., 2010. Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A. A., Lally, A., Mur- dock, J. W., Nyberg, E., Prager, J., et al. (2010). Building watson: An overview of the deepqa project. AI magazine, 31(3):59-79.\n\nCognition does not affect perception: Evaluating the evidence for top-down effects. Firestone, Scholl, C Firestone, B J Scholl, Behavioral and brain sciences. Firestone and Scholl, 2015. Firestone, C. and Scholl, B. J. (2015). Cognition does not affect perception: Evaluating the evidence for top-down effects. Behavioral and brain sciences, pages 1-72.\n\nQualitative process theory. K D Forbus ; Forbus, Artificial intelligence. 241Forbus, 1984. Forbus, K. D. (1984). Qualitative process the- ory. Artificial intelligence, 24(1):85-168.\n\nAre you talking to a machine? dataset and methods for multilingual image question answering. Gao, arXiv:1505.05612arXiv preprintGao et al., 2015. Gao, H., Mao, J., Zhou, J., Huang, Z., Wang, L., and Xu, W. (2015). Are you talking to a ma- chine? dataset and methods for multilingual image question answering. arXiv preprint arXiv:1505.05612.\n\nVisual turing test for computer vision systems. Geman, Proceedings of the National Academy of Sciences. 11212Geman et al., 2015. Geman, D., Geman, S., Hallonquist, N., and Younes, L. (2015). Visual turing test for computer vision systems. Proceedings of the National Academy of Sciences, 112(12):3618-3623.\n\n. R Girshick ; Girshick, arXiv:1504.08083Fast r-cnn. arXiv preprintGirshick, 2015. Girshick, R. (2015). Fast r-cnn. arXiv preprint arXiv:1504.08083.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. Girshick, Computer Vision and Pattern Recognition (CVPR). IEEEGirshick et al., 2014. Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Con- ference on, pages 580-587. IEEE.\n\nNonparametric part transfer for fine-grained recognition. Goering, Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEEGoering et al., 2014. Goering, C., Rodner, E., Freytag, A., and Denzler, J. (2014). Nonparametric part transfer for fine-grained recognition. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2489-2496. IEEE.\n\nGriffin, Caltech-256 object category dataset. Griffin et al., 2007. Griffin, G., Holub, A., and Perona, P. (2007). Caltech-256 object category dataset.\n\nExploring various knowledge in relation extraction. Guodong, Proceedings of the 43rd annual meeting on association for computational linguistics. the 43rd annual meeting on association for computational linguisticsAssociation for Computational LinguisticsGuoDong et al., 2005. GuoDong, Z., Jian, S., Jie, Z., and Min, Z. (2005). Exploring various knowledge in relation extraction. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 427-434. Association for Computational Linguistics.\n\nBeyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers. Davis ; Gupta, A Gupta, L S Davis, Computer Vision-ECCV 2008. SpringerGupta and Davis, 2008. Gupta, A. and Davis, L. S. (2008). Beyond nouns: Exploiting prepositions and comparative ad- jectives for learning visual classifiers. In Computer Vision- ECCV 2008, pages 16-29. Springer.\n\nObserving human-object interactions: Using spatial and functional compatibility for recognition. Pattern Analysis and Machine Intelligence. Gupta, IEEE Transactions on. 3110Gupta et al., 2009. Gupta, A., Kembhavi, A., and Davis, L. S. (2009). Observing human-object interactions: Using spatial and functional compatibility for recognition. Pat- tern Analysis and Machine Intelligence, IEEE Transac- tions on, 31(10):1775-1789.\n\nThe naive physics manifesto. P J Hayes ; Hayes, Universit\u00e9 de Gen\u00e8veInstitut pour les\u00e9tudes s\u00e9mantiques et cognitivesHayes, 1978. Hayes, P. J. (1978). The naive physics man- ifesto. Institut pour les\u00e9tudes s\u00e9mantiques et cogni- tives/Universit\u00e9 de Gen\u00e8ve.\n\nThe second naive physics manifesto. P J Hayes ; Hayes, Theories of the Commonsense World. Hayes, 1985. Hayes, P. J. (1985). The second naive physics manifesto. Theories of the Commonsense World, pages 1- 36.\n\nSupport vector machines. Intelligent Systems and their Applications. Hearst, IEEE. 134Hearst et al., 1998. Hearst, M. A., Dumais, S. T., Osman, E., Platt, J., and Scholkopf, B. (1998). Support vector ma- chines. Intelligent Systems and their Applications, IEEE, 13(4):18-28.\n\nLong short-term memory. Schmidhuber ; Hochreiter, S Hochreiter, J Schmidhuber, Neural computation. 98Hochreiter and Schmidhuber, 1997. Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8):1735-1780.\n\nFraming image description as a ranking task: Data, models and evaluation metrics. Hodosh, J. Artif. Int. Res. 471Hodosh et al., 2013. Hodosh, M., Young, P., and Hocken- maier, J. (2013). Framing image description as a ranking task: Data, models and evaluation metrics. J. Artif. Int. Res., 47(1):853-899.\n\nA template-based approach toward acquisition of logical sentences. Hou, Intelligent Information Processing. SpringerHou et al., 2002. Hou, C.-S. J., Noy, N. F., and Musen, M. A. (2002). A template-based approach toward acquisition of logical sentences. In Intelligent Information Processing, pages 77-89. Springer.\n\nLabeled faces in the wild: A database forstudying face recognition in unconstrained environments. Huang, Workshop on Faces in'Real-Life'Images: Detection, Alignment, and Recognition. Huang et al., 2008. Huang, G. B., Mattar, M., Berg, T., and Learned-Miller, E. (2008). Labeled faces in the wild: A database forstudying face recognition in unconstrained en- vironments. In Workshop on Faces in'Real-Life'Images: Detection, Alignment, and Recognition.\n\nDiscovering states and transformations in image collections. Isola, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIsola et al., 2015. Isola, P., Lim, J. J., and Adelson, E. H. (2015). Discovering states and transformations in image col- lections. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 1383-1391.\n\nIncorporating scene context and object layout into appearance modeling. Izadinia, Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEEIzadinia et al., 2014. Izadinia, H., Sadeghi, F., and Farhadi, A. (2014). Incorporating scene context and object lay- out into appearance modeling. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 232-239. IEEE.\n\nImage retrieval using scene graphs. Johnson , IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Johnson et al., 2015. Johnson, J., Krishna, R., Stark, M., Li, L.-J., Shamma, D. A., Bernstein, M., and Fei-Fei, L. (2015). Image retrieval using scene graphs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nDeep visual-semantic alignments for generating image descriptions. Fei-Fei ; Karpathy, A Karpathy, L Fei-Fei, arXiv:1412.2306arXiv preprintKarpathy and Fei-Fei, 2014. Karpathy, A. and Fei-Fei, L. (2014). Deep visual-semantic alignments for generating im- age descriptions. arXiv preprint arXiv:1412.2306.\n\nMultimodal neural language models. Kiros, Proceedings of the 31st International Conference on Machine Learning (ICML-14). the 31st International Conference on Machine Learning (ICML-14)Kiros et al., 2014. Kiros, R., Salakhutdinov, R., and Zemel, R. (2014). Multimodal neural language models. In Pro- ceedings of the 31st International Conference on Machine Learning (ICML-14), pages 595-603.\n\nEmbracing error to enable rapid crowdsourcing. Krishna , CHI'16-SIGCHI Conference on Human Factors in Computing System. Krishna et al., 2016. Krishna, R., Hata, K., Chen, S., Kravitz, J., Shamma, D. A., Fei-Fei, L., and Bernstein, M. S. (2016). Embracing error to enable rapid crowdsourc- ing. In CHI'16-SIGCHI Conference on Human Factors in Computing System.\n\nImagenet classification with deep convolutional neural networks. Krizhevsky, Advances in neural information processing systems. Krizhevsky et al., 2012. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural in- formation processing systems, pages 1097-1105.\n\nLearning to detect unseen object classes by between-class attribute transfer. Lampert, Computer Vision and Pattern Recognition. IEEECVPR 2009. IEEE Conference onLampert et al., 2009. Lampert, C. H., Nickisch, H., and Harmeling, S. (2009). Learning to detect unseen object classes by between-class attribute transfer. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 951-958. IEEE.\n\nUsing corpus statistics and wordnet relations for sense identification. Leacock, Computational Linguistics. 241Leacock et al., 1998. Leacock, C., Miller, G. A., and Chodorow, M. (1998). Using corpus statistics and wordnet relations for sense identification. Computational Linguis- tics, 24(1):147-165.\n\nLebret, arXiv:1502.03671Phrase-based image captioning. arXiv preprintLebret et al., 2015. Lebret, R., Pinheiro, P. O., and Col- lobert, R. (2015). Phrase-based image captioning. arXiv preprint arXiv:1502.03671.\n\nMicrosoft coco: Common objects in context. Lin, Computer Vision-ECCV 2014. SpringerLin et al., 2014. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., and Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014, pages 740-755. Springer.\n\nLu, Visual relationship detection using language priors. Lu et al., 2016. Lu, C., Krishna, R., Bernstein, M. S., and Fei-Fei, L. (2016). Visual relationship detection using lan- guage priors.\n\nMa, arXiv:1506.00333Learning to answer questions from image using convolutional neural network. arXiv preprintMa et al., 2015. Ma, L., Lu, Z., and Li, H. (2015). Learning to answer questions from image using convolutional neural network. arXiv preprint arXiv:1506.00333.\n\nA multi-world approach to question answering about real-world scenes based on uncertain input. Fritz Malinowski, M Malinowski, M Fritz, Advances in Neural Information Processing Systems. Malinowski and Fritz, 2014. Malinowski, M. and Fritz, M. (2014). A multi-world approach to question answering about real-world scenes based on uncertain input. In Ad- vances in Neural Information Processing Systems, pages 1682-1690.\n\nMalinowski, arXiv:1505.01121Ask your neurons: A neural-based approach to answering questions about images. arXiv preprintMalinowski et al., 2015. Malinowski, M., Rohrbach, M., and Fritz, M. (2015). Ask your neurons: A neural-based ap- proach to answering questions about images. arXiv preprint arXiv:1505.01121.\n\nRecognition by association via learning perexemplar distances. Malisiewicz, Computer Vision and Pattern Recognition. IEEECVPR 2008. IEEE Conference onMalisiewicz et al., 2008. Malisiewicz, T., Efros, A., et al. (2008). Recognition by association via learning per- exemplar distances. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1-8. IEEE.\n\nThe Stanford CoreNLP natural language processing toolkit. Manning, Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 52nd Annual Meeting of the Association for Computational Linguistics: System DemonstrationsManning et al., 2014. Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. J., and McClosky, D. (2014). The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55-60.\n\nMao, arXiv:1410.1090Explain images with multimodal recurrent neural networks. arXiv preprintMao et al., 2014. Mao, J., Xu, W., Yang, Y., Wang, J., and Yuille, A. L. (2014). Explain images with multimodal re- current neural networks. arXiv preprint arXiv:1410.1090.\n\nThe senseval-3 english lexical sample task. Mihalcea, Association for Computational LinguisticsMihalcea et al., 2004. Mihalcea, R., Chklovski, T. A., and Kilgarriff, A. (2004). The senseval-3 english lexical sam- ple task. Association for Computational Linguistics.\n\nEfficient estimation of word representations in vector space. Mikolov, arXiv:1301.3781.MillerCommunications of the ACM. 3811Miller, G. AarXiv preprintWordnet: a lexical database for englishMikolov et al., 2013. Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word represen- tations in vector space. arXiv preprint arXiv:1301.3781. Miller, 1995. Miller, G. A. (1995). Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41.\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Fergus, Derek Nathan Silberman, P K Hoiem, R Fergus, ECCV. Nathan Silberman and Fergus, 2012. Nathan Silberman, Derek Hoiem, P. K. and Fergus, R. (2012). Indoor seg- mentation and support inference from rgbd images. In ECCV.\n\nElementary: Large-scale knowledge-base construction via machine learning and statistical inference. Niu, International Journal on Semantic Web and Information Systems (IJSWIS). 83Niu et al., 2012. Niu, F., Zhang, C., R\u00e9, C., and Shavlik, J. (2012). Elementary: Large-scale knowledge-base construc- tion via machine learning and statistical inference. Interna- tional Journal on Semantic Web and Information Systems (IJSWIS), 8(3):42-73.\n\nIm2text: Describing images using 1 million captioned photographs. Ordonez, Shawe-Taylor, J., Zemel, R.Ordonez et al., 2011. Ordonez, V., Kulkarni, G., and Berg, T. L. (2011). Im2text: Describing images using 1 million captioned photographs. In Shawe-Taylor, J., Zemel, R.,\n\nP Bartlett, F Pereira, K Weinberger, arXiv:1508.01346Advances in Neural Information Processing Systems. Pal, A. R. and Saha, D.Curran Associates, Inc. Pal and Saha24arXiv preprintWord sense disambiguation: a surveyBartlett, P., Pereira, F., and Weinberger, K., editors, Ad- vances in Neural Information Processing Systems 24, pages 1143-1151. Curran Associates, Inc. Pal and Saha, 2015. Pal, A. R. and Saha, D. (2015). Word sense disambiguation: a survey. arXiv preprint arXiv:1508.01346.\n\nBleu: a method for automatic evaluation of machine translation. Papineni, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsPapineni et al., 2002. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th an- nual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.\n\nThe sun attribute database: Beyond categories for deeper scene understanding. Patterson, International Journal of Computer Vision. 1081-2Patterson et al., 2014. Patterson, G., Xu, C., Su, H., and Hays, J. (2014). The sun attribute database: Beyond cate- gories for deeper scene understanding. International Jour- nal of Computer Vision, 108(1-2):59-81.\n\nImproving the fisher kernel for largescale image classification. Perronnin, Computer Vision-ECCV 2010. SpringerPerronnin et al., 2010. Perronnin, F., S\u00e1nchez, J., and Mensink, T. (2010). Improving the fisher kernel for large- scale image classification. In Computer Vision-ECCV 2010, pages 143-156. Springer.\n\nWeakly supervised learning of interactions between humans and objects. Pattern Analysis and Machine Intelligence. Prest, IEEE Transactions on. 343Prest et al., 2012. Prest, A., Schmid, C., and Ferrari, V. (2012). Weakly supervised learning of interactions between humans and objects. Pattern Analysis and Machine Intel- ligence, IEEE Transactions on, 34(3):601-614.\n\nLearning semantic relationships for better action retrieval in images. Ramanathan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRamanathan et al., 2015. Ramanathan, V., Li, C., Deng, J., Han, W., Li, Z., Gu, K., Song, Y., Bengio, S., Rossenberg, C., and Fei-Fei, L. (2015). Learning semantic relationships for better action retrieval in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 1100-1109.\n\nImage question answering: A visual semantic embedding model and a new dataset. Ren, arXiv:1505.02074arXiv:1506.01497Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprintRen et al., 2015a. Ren, M., Kiros, R., and Zemel, R. (2015a). Image question answering: A visual semantic embedding model and a new dataset. arXiv preprint arXiv:1505.02074. Ren et al., 2015b. Ren, S., He, K., Girshick, R., and Sun, J. (2015b). Faster r-cnn: Towards real-time object de- tection with region proposal networks. arXiv preprint arXiv:1506.01497.\n\nRothe, S Sch\u00fctze ; Rothe, H Sch\u00fctze, arXiv:1507.01127Autoextend: Extending word embeddings to embeddings for synsets and lexemes. arXiv preprintRothe and Sch\u00fctze, 2015. Rothe, S. and Sch\u00fctze, H. (2015). Autoextend: Extending word embeddings to embeddings for synsets and lexemes. arXiv preprint arXiv:1507.01127.\n\nDescribing Common Human Visual Actions in Images. Ruggero Ronchi, Perona, M Ronchi, P Perona, ArXiv e-printsRuggero Ronchi and Perona, 2015. Ruggero Ronchi, M. and Perona, P. (2015). Describing Common Human Visual Ac- tions in Images. ArXiv e-prints.\n\nImageNet Large Scale Visual Recognition Challenge. Russakovsky, International Journal of Computer Vision (IJCV). Russakovsky et al., 2015. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Chal- lenge. International Journal of Computer Vision (IJCV), pages 1-42.\n\nLabelme: a database and web-based tool for image annotation. Russell, International journal of computer vision. 771-3Russell et al., 2008. Russell, B. C., Torralba, A., Murphy, K. P., and Freeman, W. T. (2008). Labelme: a database and web-based tool for image annotation. International journal of computer vision, 77(1-3):157-173.\n\nViske: Visual knowledge extraction and question answering by visual verification of relation phrases. Sadeghi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSadeghi et al., 2015. Sadeghi, F., Divvala, S. K., and Farhadi, A. (2015). Viske: Visual knowledge extraction and question answering by visual verification of relation phrases. In Proceedings of the IEEE Conference on Computer Vi- sion and Pattern Recognition, pages 1456-1464.\n\nRecognition using visual phrases. Farhadi Sadeghi, M A Sadeghi, A Farhadi, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEESadeghi and Farhadi, 2011. Sadeghi, M. A. and Farhadi, A. (2011). Recognition using visual phrases. In Computer Vi- sion and Pattern Recognition (CVPR), 2011 IEEE Con- ference on, pages 1745-1752. IEEE.\n\nWe are dynamo: Overcoming stalling and friction in collective action for crowd workers. Salehi, Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. the 33rd Annual ACM Conference on Human Factors in Computing SystemsACMSalehi et al., 2015. Salehi, N., Irani, L. C., and Bernstein, M. S. (2015). We are dynamo: Overcoming stalling and fric- tion in collective action for crowd workers. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, pages 1621-1630. ACM.\n\nScripts, plans, goals, and understanding: An inquiry into human knowledge structures. Abelson ; Schank, R C Schank, R P Abelson, Psychology PressSchank and Abelson, 2013. Schank, R. C. and Abelson, R. P. (2013). Scripts, plans, goals, and understanding: An in- quiry into human knowledge structures. Psychology Press.\n\nVerbnet: A Broadcoverage, Comprehensive Verb Lexicon. K K Schuler ; Schuler, Philadelphia, PA, USA. AAI3179808PhD thesisSchuler, 2005. Schuler, K. K. (2005). Verbnet: A Broad- coverage, Comprehensive Verb Lexicon. PhD thesis, Philadelphia, PA, USA. AAI3179808.\n\nGenerating semantically precise scene graphs from textual descriptions for improved image retrieval. Schuster, Schuster et al., 2015. Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., and Manning, C. D. (2015). Generating seman- tically precise scene graphs from textual descriptions for improved image retrieval.\n\nCheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks. Sermanet, arXiv:1312.6229arXiv:1409.1556Proceedings of the conference on empirical methods in natural language processing. the conference on empirical methods in natural language processingAssociation for Computational LinguisticsarXiv preprintOverfeat: Integrated recognition, localization and detection using convolutional networksSermanet et al., 2013. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. (2013). Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229. Simonyan and Zisserman, 2014. Simonyan, K. and Zisser- man, A. (2014). Very deep convolutional networks for large- scale image recognition. arXiv preprint arXiv:1409.1556. Snow et al., 2008. Snow, R., O'Connor, B., Jurafsky, D., and Ng, A. Y. (2008). Cheap and fast-but is it good?: eval- uating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language processing, pages 254-263. Association for Computational Linguistics.\n\nSemantic compositionality through recursive matrix-vector spaces. Socher, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language LearningAssociation for Computational LinguisticsSocher et al., 2012. Socher, R., Huval, B., Manning, C. D., and Ng, A. Y. (2012). Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Language Learning, pages 1201-1211. Association for Computational Linguistics.\n\nSzegedy, arXiv:1409.4842Going deeper with convolutions. arXiv preprintSzegedy et al., 2014. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842.\n\nYfcc100m: The new data in multimedia research. Thomee, Commun. ACM. 592Thomee et al., 2016. Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J. (2016). Yfcc100m: The new data in multimedia research. Commun. ACM, 59(2):64-73.\n\n80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence. Torralba, IEEE Transactions on. 3011Torralba et al., 2008. Torralba, A., Fergus, R., and Freeman, W. T. (2008). 80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(11):1958-1970.\n\nA statistical approach to texture classification from single images. Zisserman ; Varma, M Varma, A Zisserman, International Journal of Computer Vision. 621-2Varma and Zisserman, 2005. Varma, M. and Zisserman, A. (2005). A statistical approach to texture classification from single images. International Journal of Computer Vision, 62(1-2):61-81.\n\nShow and tell: A neural image caption generator. Vinyals, arXiv:1411.4555arXiv preprintVinyals et al., 2014. Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2014). Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555.\n\nWah, The caltech-ucsd birds-200-2011 dataset. Wah et al., 2011. Wah, C., Branson, S., Welinder, P., Per- ona, P., and Belongie, S. (2011). The caltech-ucsd birds- 200-2011 dataset.\n\nSun database: Large-scale scene recognition from abbey to zoo. Xiao , Computer vision and pattern recognition (CVPR), 2010 IEEE conference on. IEEEXiao et al., 2010. Xiao, J., Hays, J., Ehinger, K., Oliva, A., Torralba, A., et al. (2010). Sun database: Large-scale scene recognition from abbey to zoo. In Computer vision and pat- tern recognition (CVPR), 2010 IEEE conference on, pages 3485-3492. IEEE.\n\nShow, attend and tell: Neural image caption generation with visual attention. CoRR, abs/1502.03044. Yao and Fei-Fei. Xu, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEEModeling mutual context of object and human pose in humanobject interaction activitiesXu et al., 2015. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A. C., Salakhutdinov, R., Zemel, R. S., and Bengio, Y. (2015). Show, attend and tell: Neural image caption gener- ation with visual attention. CoRR, abs/1502.03044. Yao and Fei-Fei, 2010. Yao, B. and Fei-Fei, L. (2010). Mod- eling mutual context of object and human pose in human- object interaction activities. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 17-24. IEEE.\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Yao, Energy Minimization Methods in Computer Vision and Pattern Recognition. 2Introduction to a large-scale general purpose ground truth database: methodology, annotation tool and benchmarksYao et al., 2007. Yao, B., Yang, X., and Zhu, S.-C. (2007). Introduction to a large-scale general purpose ground truth database: methodology, annotation tool and benchmarks. In Energy Minimization Methods in Computer Vision and Pattern Recognition, pages 169-183. Springer. Young et al., 2014. Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. (2014). From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67-78.\n\nYu , arXiv:1506.00278Visual Madlibs: Fill in the blank Image Generation and Question Answering. arXiv preprintYu et al., 2015. Yu, L., Park, E., Berg, A. C., and Berg, T. L. (2015). Visual Madlibs: Fill in the blank Image Generation and Question Answering. arXiv preprint arXiv:1506.00278.\n\nRelation classification via convolutional deep neural network. Zeng, Proceedings of COLING. COLINGZeng et al., 2014. Zeng, D., Liu, K., Lai, S., Zhou, G., and Zhao, J. (2014). Relation classification via convolutional deep neural network. In Proceedings of COLING, pages 2335-2344.\n\nTree kernel-based relation extraction with context-sensitive structured parse tree information. Zhou, EMNLP-CoNLL. 728Zhou et al., 2007. Zhou, G., Zhang, M., Ji, D. H., and Zhu, Q. (2007). Tree kernel-based relation extraction with context-sensitive structured parse tree information. EMNLP-CoNLL 2007, page 728.\n\nStatsnowball: a statistical approach to extracting entity relationships. Zhu, Proceedings of the 18th international conference on World wide web. the 18th international conference on World wide webACMZhu et al., 2009. Zhu, J., Nie, Z., Liu, X., Zhang, B., and Wen, J.-R. (2009). Statsnowball: a statistical approach to extracting entity relationships. In Proceedings of the 18th international conference on World wide web, pages 101- 110. ACM.\n\nReasoning about Object Affordances in a Knowledge Base Representation. Zhu, European Conference on Computer Vision. Zhu et al., 2014. Zhu, Y., Fathi, A., and Fei-Fei, L. (2014). Reasoning about Object Affordances in a Knowledge Base Representation. In European Conference on Computer Vi- sion.\n\nBuilding a Large-scale Multimodal Knowledge Base System for Answering Visual Queries. Zhu, arXivpreprintarXiv:1507.05670Zhu et al., 2015. Zhu, Y., Zhang, C., R\u00e9, C., and Fei-Fei, L. (2015). Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries. In arXiv preprint arXiv:1507.05670.\n\nBringing semantics into focus using visual abstraction. Zitnick, Parikh, C L Zitnick, D Parikh, Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. IEEEZitnick and Parikh, 2013. Zitnick, C. L. and Parikh, D. (2013). Bringing semantics into focus using visual ab- straction. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3009-3016. IEEE.\n", "annotations": {"author": "[{\"end\":136,\"start\":91},{\"end\":146,\"start\":137},{\"end\":160,\"start\":147},{\"end\":176,\"start\":161},{\"end\":188,\"start\":177},{\"end\":204,\"start\":189},{\"end\":220,\"start\":205},{\"end\":239,\"start\":221},{\"end\":245,\"start\":240},{\"end\":254,\"start\":246},{\"end\":263,\"start\":255},{\"end\":273,\"start\":264},{\"end\":294,\"start\":274},{\"end\":300,\"start\":295},{\"end\":309,\"start\":301},{\"end\":325,\"start\":310},{\"end\":335,\"start\":326},{\"end\":349,\"start\":336},{\"end\":365,\"start\":350},{\"end\":377,\"start\":366},{\"end\":393,\"start\":378},{\"end\":409,\"start\":394},{\"end\":420,\"start\":410},{\"end\":436,\"start\":421},{\"end\":457,\"start\":437},{\"end\":469,\"start\":458},{\"end\":505,\"start\":470},{\"end\":541,\"start\":506},{\"end\":591,\"start\":542},{\"end\":627,\"start\":592},{\"end\":663,\"start\":628},{\"end\":699,\"start\":664},{\"end\":735,\"start\":700},{\"end\":784,\"start\":736},{\"end\":816,\"start\":785},{\"end\":847,\"start\":817},{\"end\":883,\"start\":848},{\"end\":919,\"start\":884}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":98},{\"end\":145,\"start\":142},{\"end\":159,\"start\":154},{\"end\":175,\"start\":168},{\"end\":187,\"start\":183},{\"end\":203,\"start\":196},{\"end\":219,\"start\":215},{\"end\":238,\"start\":228},{\"end\":244,\"start\":242},{\"end\":253,\"start\":251},{\"end\":262,\"start\":257},{\"end\":272,\"start\":266},{\"end\":293,\"start\":284},{\"end\":299,\"start\":297},{\"end\":324,\"start\":317},{\"end\":334,\"start\":331},{\"end\":348,\"start\":343},{\"end\":364,\"start\":357},{\"end\":376,\"start\":372},{\"end\":392,\"start\":385},{\"end\":408,\"start\":404},{\"end\":419,\"start\":417},{\"end\":435,\"start\":429},{\"end\":456,\"start\":447},{\"end\":468,\"start\":461}]", "author_first_name": "[{\"end\":97,\"start\":91},{\"end\":141,\"start\":137},{\"end\":153,\"start\":147},{\"end\":167,\"start\":161},{\"end\":182,\"start\":177},{\"end\":195,\"start\":189},{\"end\":214,\"start\":205},{\"end\":227,\"start\":221},{\"end\":241,\"start\":240},{\"end\":250,\"start\":246},{\"end\":256,\"start\":255},{\"end\":265,\"start\":264},{\"end\":281,\"start\":274},{\"end\":283,\"start\":282},{\"end\":296,\"start\":295},{\"end\":308,\"start\":301},{\"end\":316,\"start\":310},{\"end\":330,\"start\":326},{\"end\":342,\"start\":336},{\"end\":356,\"start\":350},{\"end\":371,\"start\":366},{\"end\":384,\"start\":378},{\"end\":403,\"start\":394},{\"end\":416,\"start\":410},{\"end\":426,\"start\":421},{\"end\":428,\"start\":427},{\"end\":444,\"start\":437},{\"end\":446,\"start\":445},{\"end\":460,\"start\":458}]", "author_affiliation": "[{\"end\":504,\"start\":471},{\"end\":540,\"start\":507},{\"end\":590,\"start\":543},{\"end\":626,\"start\":593},{\"end\":662,\"start\":629},{\"end\":698,\"start\":665},{\"end\":734,\"start\":701},{\"end\":783,\"start\":737},{\"end\":815,\"start\":786},{\"end\":846,\"start\":818},{\"end\":882,\"start\":849},{\"end\":918,\"start\":885}]", "title": "[{\"end\":88,\"start\":1},{\"end\":1007,\"start\":920}]", "venue": null, "abstract": "[{\"end\":2446,\"start\":1240}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2907,\"start\":2889},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":2931,\"start\":2907},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":2961,\"start\":2931},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2986,\"start\":2961},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3570,\"start\":3546},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3593,\"start\":3570},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":3616,\"start\":3593},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3632,\"start\":3616},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":3651,\"start\":3632},{\"end\":4303,\"start\":4275},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4316,\"start\":4303},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4495,\"start\":4475},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":4572,\"start\":4545},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4818,\"start\":4800},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4847,\"start\":4827},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5422,\"start\":5403},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":5440,\"start\":5422},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5468,\"start\":5440},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":5490,\"start\":5468},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":5529,\"start\":5511},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5549,\"start\":5529},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5574,\"start\":5549},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5592,\"start\":5574},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5620,\"start\":5592},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5686,\"start\":5659},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6242,\"start\":6222},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6357,\"start\":6335},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6434,\"start\":6421},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":6679,\"start\":6658},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6710,\"start\":6692},{\"end\":7810,\"start\":7786},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7841,\"start\":7823},{\"end\":8132,\"start\":8118},{\"end\":8133,\"start\":8132},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":8175,\"start\":8157},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":8954,\"start\":8936},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8976,\"start\":8954},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9004,\"start\":8976},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":9025,\"start\":9004},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9281,\"start\":9259},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9386,\"start\":9365},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":9409,\"start\":9386},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9452,\"start\":9431},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9472,\"start\":9452},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9495,\"start\":9472},{\"end\":9657,\"start\":9634},{\"end\":11543,\"start\":11510},{\"end\":13718,\"start\":13708},{\"end\":13802,\"start\":13788},{\"end\":15040,\"start\":15017},{\"end\":16292,\"start\":16278},{\"end\":19043,\"start\":19030},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":19354,\"start\":19336},{\"end\":19552,\"start\":19529},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":19594,\"start\":19572},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":19628,\"start\":19610},{\"end\":19889,\"start\":19866},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":20042,\"start\":20021},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20218,\"start\":20193},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20370,\"start\":20352},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":20498,\"start\":20472},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20721,\"start\":20703},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20909,\"start\":20889},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22889,\"start\":22868},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22910,\"start\":22889},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":23009,\"start\":22987},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23066,\"start\":23044},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23206,\"start\":23187},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23224,\"start\":23206},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23252,\"start\":23224},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":23274,\"start\":23252},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":23529,\"start\":23509},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23560,\"start\":23542},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":24104,\"start\":24091},{\"end\":24139,\"start\":24104},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24806,\"start\":24786},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24852,\"start\":24831},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24919,\"start\":24900},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":25124,\"start\":25106},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25929,\"start\":25903},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":26020,\"start\":25993},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26097,\"start\":26068},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26231,\"start\":26209},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26486,\"start\":26464},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":26554,\"start\":26536},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26629,\"start\":26607},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":27076,\"start\":27058},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27098,\"start\":27076},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27136,\"start\":27124},{\"end\":27163,\"start\":27136},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27176,\"start\":27163},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":27223,\"start\":27203},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":27242,\"start\":27223},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27592,\"start\":27569},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27681,\"start\":27662},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27849,\"start\":27826},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28032,\"start\":28010},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28367,\"start\":28348},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28389,\"start\":28367},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28518,\"start\":28496},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":28662,\"start\":28640},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28878,\"start\":28857},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28895,\"start\":28878},{\"end\":29255,\"start\":29220},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":29267,\"start\":29255},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":29514,\"start\":29487},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":29550,\"start\":29534},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":29569,\"start\":29550},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29589,\"start\":29569},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29607,\"start\":29589},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":29714,\"start\":29695},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":29759,\"start\":29743},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29779,\"start\":29759},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29797,\"start\":29779},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":30019,\"start\":29991},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30063,\"start\":30043},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30400,\"start\":30380},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":30457,\"start\":30429},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30504,\"start\":30487},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":30529,\"start\":30504},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":30548,\"start\":30529},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31188,\"start\":31175},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31296,\"start\":31283},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31365,\"start\":31351},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":31435,\"start\":31409},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31591,\"start\":31566},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31696,\"start\":31673},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":31851,\"start\":31834},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31869,\"start\":31851},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":32020,\"start\":32002},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":32042,\"start\":32020},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":33502,\"start\":33481},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34568,\"start\":34547},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":34585,\"start\":34568},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":35708,\"start\":35685},{\"end\":37720,\"start\":37709},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":42703,\"start\":42684},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":42842,\"start\":42823},{\"end\":43987,\"start\":43973},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":48491,\"start\":48473},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":48540,\"start\":48519},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":48905,\"start\":48886},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":50256,\"start\":50238},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":50827,\"start\":50804},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":52442,\"start\":52424},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":53438,\"start\":53420},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":53907,\"start\":53885},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":57883,\"start\":57865},{\"end\":60042,\"start\":60029},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":65679,\"start\":65652},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":65920,\"start\":65896},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":66005,\"start\":65987},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":66186,\"start\":66164},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":66841,\"start\":66819},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":68565,\"start\":68546},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":68583,\"start\":68565},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":68611,\"start\":68583},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":70114,\"start\":70094},{\"end\":72593,\"start\":72579},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":72781,\"start\":72762},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":73639,\"start\":73629},{\"end\":73675,\"start\":73639},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":74255,\"start\":74232},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":77703,\"start\":77688},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":77737,\"start\":77717},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":78365,\"start\":78341},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":78388,\"start\":78365},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":78411,\"start\":78388},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":78427,\"start\":78411},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":78446,\"start\":78427},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":78563,\"start\":78542},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":78586,\"start\":78563},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":78982,\"start\":78957},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":79009,\"start\":78982},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":79038,\"start\":79009},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":79060,\"start\":79038},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":79082,\"start\":79060},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":79268,\"start\":79241},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":79569,\"start\":79550},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":79594,\"start\":79569},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":80086,\"start\":80078},{\"end\":80120,\"start\":80086},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":80248,\"start\":80229},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":80461,\"start\":80433},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":80554,\"start\":80534},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":81495,\"start\":81475},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":81676,\"start\":81655},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":81698,\"start\":81676},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":84375,\"start\":84354},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":88955,\"start\":88936},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":88973,\"start\":88955},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":89001,\"start\":88973},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":89023,\"start\":89001},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":89360,\"start\":89332},{\"end\":89597,\"start\":89567},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":89748,\"start\":89729},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":89904,\"start\":89886},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":91224,\"start\":91190},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":91337,\"start\":91315},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":91773,\"start\":91754},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":94762,\"start\":94736},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":95315,\"start\":95287},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":95353,\"start\":95334},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":95370,\"start\":95353},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":95406,\"start\":95388},{\"end\":95669,\"start\":95634},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":95681,\"start\":95669},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":95876,\"start\":95856},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":97158,\"start\":97139},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":97176,\"start\":97158},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":97204,\"start\":97176},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":97226,\"start\":97204},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":97649,\"start\":97633},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":97668,\"start\":97649},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":97688,\"start\":97668},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":97706,\"start\":97688},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":98017,\"start\":97997},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":98061,\"start\":98041},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":98379,\"start\":98360},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":98756,\"start\":98738},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":98778,\"start\":98756},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":98806,\"start\":98778},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":98827,\"start\":98806},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":98991,\"start\":98969},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":99095,\"start\":99087},{\"end\":99134,\"start\":99095},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":99194,\"start\":99175},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":99214,\"start\":99194},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":99359,\"start\":99338},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":99382,\"start\":99359},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":103884,\"start\":103865},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":104888,\"start\":104870}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":100851,\"start\":100443},{\"attributes\":{\"id\":\"fig_1\"},\"end\":100911,\"start\":100852},{\"attributes\":{\"id\":\"fig_2\"},\"end\":101129,\"start\":100912},{\"attributes\":{\"id\":\"fig_3\"},\"end\":101329,\"start\":101130},{\"attributes\":{\"id\":\"fig_5\"},\"end\":101542,\"start\":101330},{\"attributes\":{\"id\":\"fig_6\"},\"end\":101663,\"start\":101543},{\"attributes\":{\"id\":\"fig_7\"},\"end\":101880,\"start\":101664},{\"attributes\":{\"id\":\"fig_8\"},\"end\":102089,\"start\":101881},{\"attributes\":{\"id\":\"fig_9\"},\"end\":102363,\"start\":102090},{\"attributes\":{\"id\":\"fig_10\"},\"end\":102592,\"start\":102364},{\"attributes\":{\"id\":\"fig_11\"},\"end\":102801,\"start\":102593},{\"attributes\":{\"id\":\"fig_13\"},\"end\":103444,\"start\":102802},{\"attributes\":{\"id\":\"fig_14\"},\"end\":104956,\"start\":103445},{\"attributes\":{\"id\":\"fig_15\"},\"end\":105145,\"start\":104957},{\"attributes\":{\"id\":\"fig_16\"},\"end\":105258,\"start\":105146},{\"attributes\":{\"id\":\"fig_17\"},\"end\":106045,\"start\":105259},{\"attributes\":{\"id\":\"fig_18\"},\"end\":106147,\"start\":106046},{\"attributes\":{\"id\":\"fig_19\"},\"end\":106481,\"start\":106148},{\"attributes\":{\"id\":\"fig_20\"},\"end\":106837,\"start\":106482},{\"attributes\":{\"id\":\"fig_21\"},\"end\":106985,\"start\":106838},{\"attributes\":{\"id\":\"fig_22\"},\"end\":107321,\"start\":106986},{\"attributes\":{\"id\":\"fig_23\"},\"end\":107451,\"start\":107322},{\"attributes\":{\"id\":\"fig_24\"},\"end\":107557,\"start\":107452},{\"attributes\":{\"id\":\"fig_25\"},\"end\":108272,\"start\":107558},{\"attributes\":{\"id\":\"fig_26\"},\"end\":108519,\"start\":108273},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":109625,\"start\":108520},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":109902,\"start\":109626},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":111353,\"start\":109903},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":111525,\"start\":111354},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":111631,\"start\":111526},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":111742,\"start\":111632},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":112272,\"start\":111743},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":112366,\"start\":112273},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":112632,\"start\":112367},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":112726,\"start\":112633},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":112841,\"start\":112727},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":113050,\"start\":112842},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":113200,\"start\":113051}]", "paragraph": "[{\"end\":4573,\"start\":2462},{\"end\":6075,\"start\":4575},{\"end\":6905,\"start\":6077},{\"end\":7489,\"start\":6907},{\"end\":8248,\"start\":7491},{\"end\":8523,\"start\":8258},{\"end\":9961,\"start\":8525},{\"end\":10680,\"start\":9963},{\"end\":10790,\"start\":10682},{\"end\":12619,\"start\":10817},{\"end\":13558,\"start\":12663},{\"end\":14437,\"start\":13604},{\"end\":14528,\"start\":14439},{\"end\":14964,\"start\":14552},{\"end\":15212,\"start\":14966},{\"end\":15730,\"start\":15239},{\"end\":15781,\"start\":15732},{\"end\":16446,\"start\":15783},{\"end\":17059,\"start\":16473},{\"end\":17752,\"start\":17079},{\"end\":18528,\"start\":17787},{\"end\":18998,\"start\":18545},{\"end\":20180,\"start\":19011},{\"end\":21175,\"start\":20182},{\"end\":22144,\"start\":21177},{\"end\":22384,\"start\":22146},{\"end\":23344,\"start\":22407},{\"end\":24352,\"start\":23346},{\"end\":25545,\"start\":24364},{\"end\":26904,\"start\":25560},{\"end\":27850,\"start\":26922},{\"end\":29025,\"start\":27852},{\"end\":29798,\"start\":29048},{\"end\":30867,\"start\":29800},{\"end\":32258,\"start\":30896},{\"end\":32843,\"start\":32287},{\"end\":33741,\"start\":32861},{\"end\":34048,\"start\":33743},{\"end\":34998,\"start\":34072},{\"end\":35841,\"start\":35000},{\"end\":35960,\"start\":35919},{\"end\":36121,\"start\":36042},{\"end\":36658,\"start\":36123},{\"end\":36967,\"start\":36660},{\"end\":38047,\"start\":36979},{\"end\":38556,\"start\":38049},{\"end\":38745,\"start\":38558},{\"end\":39025,\"start\":38747},{\"end\":40053,\"start\":39074},{\"end\":40941,\"start\":40070},{\"end\":41293,\"start\":40967},{\"end\":41427,\"start\":41295},{\"end\":42171,\"start\":41429},{\"end\":42609,\"start\":42188},{\"end\":43066,\"start\":42611},{\"end\":43346,\"start\":43068},{\"end\":44425,\"start\":43367},{\"end\":45198,\"start\":44427},{\"end\":45821,\"start\":45200},{\"end\":47004,\"start\":45823},{\"end\":47705,\"start\":47006},{\"end\":48360,\"start\":47741},{\"end\":49392,\"start\":48380},{\"end\":49638,\"start\":49426},{\"end\":50968,\"start\":49640},{\"end\":52443,\"start\":50970},{\"end\":53689,\"start\":52445},{\"end\":55133,\"start\":53691},{\"end\":56635,\"start\":55135},{\"end\":56682,\"start\":56655},{\"end\":56745,\"start\":56718},{\"end\":56820,\"start\":56766},{\"end\":56885,\"start\":56839},{\"end\":57056,\"start\":56913},{\"end\":57097,\"start\":57074},{\"end\":57160,\"start\":57135},{\"end\":57240,\"start\":57188},{\"end\":57353,\"start\":57267},{\"end\":58115,\"start\":57387},{\"end\":58819,\"start\":58117},{\"end\":59238,\"start\":58821},{\"end\":59773,\"start\":59263},{\"end\":60581,\"start\":59775},{\"end\":61298,\"start\":60583},{\"end\":62101,\"start\":61300},{\"end\":62667,\"start\":62103},{\"end\":64040,\"start\":62695},{\"end\":64710,\"start\":64042},{\"end\":65467,\"start\":64712},{\"end\":66280,\"start\":65469},{\"end\":66755,\"start\":66318},{\"end\":67905,\"start\":66757},{\"end\":68224,\"start\":67907},{\"end\":69474,\"start\":68258},{\"end\":69848,\"start\":69476},{\"end\":70664,\"start\":69850},{\"end\":72069,\"start\":70666},{\"end\":72155,\"start\":72071},{\"end\":73326,\"start\":72187},{\"end\":73445,\"start\":73328},{\"end\":74683,\"start\":73447},{\"end\":74912,\"start\":74685},{\"end\":75464,\"start\":74914},{\"end\":76485,\"start\":75466},{\"end\":76999,\"start\":76487},{\"end\":77947,\"start\":77001},{\"end\":78288,\"start\":77963},{\"end\":79269,\"start\":78290},{\"end\":80121,\"start\":79271},{\"end\":80818,\"start\":80123},{\"end\":81064,\"start\":80820},{\"end\":81819,\"start\":81089},{\"end\":82353,\"start\":81821},{\"end\":82787,\"start\":82355},{\"end\":83783,\"start\":82789},{\"end\":84521,\"start\":83785},{\"end\":86068,\"start\":84523},{\"end\":86654,\"start\":86096},{\"end\":87336,\"start\":86656},{\"end\":88807,\"start\":87338},{\"end\":89905,\"start\":88842},{\"end\":90039,\"start\":89907},{\"end\":90085,\"start\":90041},{\"end\":90176,\"start\":90087},{\"end\":91653,\"start\":90178},{\"end\":93414,\"start\":91655},{\"end\":94072,\"start\":93437},{\"end\":94227,\"start\":94074},{\"end\":94419,\"start\":94229},{\"end\":95407,\"start\":94421},{\"end\":96644,\"start\":95409},{\"end\":97067,\"start\":96668},{\"end\":97534,\"start\":97069},{\"end\":97927,\"start\":97536},{\"end\":98601,\"start\":97929},{\"end\":99215,\"start\":98603},{\"end\":99617,\"start\":99217},{\"end\":100442,\"start\":99632}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":35918,\"start\":35842},{\"attributes\":{\"id\":\"formula_1\"},\"end\":36041,\"start\":35961}]", "table_ref": "[{\"end\":19029,\"start\":19020},{\"end\":21831,\"start\":21824},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":33648,\"start\":33641},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":67997,\"start\":67990},{\"end\":74801,\"start\":74794},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":87474,\"start\":87467},{\"end\":91671,\"start\":91664},{\"end\":94186,\"start\":94179},{\"end\":95425,\"start\":95418},{\"end\":96270,\"start\":96263}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2460,\"start\":2448},{\"end\":8256,\"start\":8251},{\"end\":10815,\"start\":10793},{\"attributes\":{\"n\":\"2.1\"},\"end\":12661,\"start\":12622},{\"attributes\":{\"n\":\"2.2\"},\"end\":13602,\"start\":13561},{\"attributes\":{\"n\":\"2.3\"},\"end\":14550,\"start\":14531},{\"attributes\":{\"n\":\"2.4\"},\"end\":15237,\"start\":15215},{\"attributes\":{\"n\":\"2.5\"},\"end\":16471,\"start\":16449},{\"attributes\":{\"n\":\"2.6\"},\"end\":17077,\"start\":17062},{\"attributes\":{\"n\":\"2.7\"},\"end\":17785,\"start\":17755},{\"attributes\":{\"n\":\"3\"},\"end\":18543,\"start\":18531},{\"attributes\":{\"n\":\"3.1\"},\"end\":19009,\"start\":19001},{\"attributes\":{\"n\":\"3.2\"},\"end\":22405,\"start\":22387},{\"attributes\":{\"n\":\"3.3\"},\"end\":24362,\"start\":24355},{\"attributes\":{\"n\":\"3.4\"},\"end\":25558,\"start\":25548},{\"attributes\":{\"n\":\"3.5\"},\"end\":26920,\"start\":26907},{\"attributes\":{\"n\":\"3.6\"},\"end\":29046,\"start\":29028},{\"attributes\":{\"n\":\"3.7\"},\"end\":30894,\"start\":30870},{\"attributes\":{\"n\":\"4\"},\"end\":32285,\"start\":32261},{\"attributes\":{\"n\":\"4.1\"},\"end\":32859,\"start\":32846},{\"attributes\":{\"n\":\"4.2\"},\"end\":34070,\"start\":34051},{\"attributes\":{\"n\":\"4.3\"},\"end\":36977,\"start\":36970},{\"attributes\":{\"n\":\"4.4\"},\"end\":39072,\"start\":39028},{\"attributes\":{\"n\":\"4.5\"},\"end\":40068,\"start\":40056},{\"attributes\":{\"n\":\"4.6\"},\"end\":40965,\"start\":40944},{\"attributes\":{\"n\":\"4.7\"},\"end\":42186,\"start\":42174},{\"attributes\":{\"n\":\"4.8\"},\"end\":43365,\"start\":43349},{\"attributes\":{\"n\":\"5\"},\"end\":47739,\"start\":47708},{\"attributes\":{\"n\":\"5.1\"},\"end\":48378,\"start\":48363},{\"attributes\":{\"n\":\"5.2\"},\"end\":49424,\"start\":49395},{\"end\":56653,\"start\":56638},{\"end\":56716,\"start\":56685},{\"end\":56764,\"start\":56748},{\"end\":56837,\"start\":56823},{\"end\":56911,\"start\":56888},{\"end\":57072,\"start\":57059},{\"end\":57133,\"start\":57100},{\"end\":57186,\"start\":57163},{\"end\":57265,\"start\":57243},{\"end\":57385,\"start\":57356},{\"attributes\":{\"n\":\"5.4\"},\"end\":59261,\"start\":59241},{\"attributes\":{\"n\":\"5.5\"},\"end\":62693,\"start\":62670},{\"attributes\":{\"n\":\"5.6\"},\"end\":66316,\"start\":66283},{\"attributes\":{\"n\":\"5.7\"},\"end\":68256,\"start\":68227},{\"attributes\":{\"n\":\"5.8\"},\"end\":72185,\"start\":72158},{\"attributes\":{\"n\":\"6\"},\"end\":77961,\"start\":77950},{\"attributes\":{\"n\":\"6.1\"},\"end\":81087,\"start\":81067},{\"attributes\":{\"n\":\"6.2\"},\"end\":86094,\"start\":86071},{\"attributes\":{\"n\":\"6.3\"},\"end\":88840,\"start\":88810},{\"attributes\":{\"n\":\"6.4\"},\"end\":93435,\"start\":93417},{\"attributes\":{\"n\":\"7\"},\"end\":96666,\"start\":96647},{\"attributes\":{\"n\":\"8\"},\"end\":99630,\"start\":99620},{\"end\":100450,\"start\":100444},{\"end\":100861,\"start\":100853},{\"end\":100919,\"start\":100913},{\"end\":101139,\"start\":101131},{\"end\":101339,\"start\":101331},{\"end\":101553,\"start\":101544},{\"end\":101674,\"start\":101665},{\"end\":101891,\"start\":101882},{\"end\":102100,\"start\":102091},{\"end\":102374,\"start\":102365},{\"end\":102603,\"start\":102594},{\"end\":102812,\"start\":102803},{\"end\":103464,\"start\":103446},{\"end\":104967,\"start\":104958},{\"end\":105156,\"start\":105147},{\"end\":105278,\"start\":105260},{\"end\":106056,\"start\":106047},{\"end\":106158,\"start\":106149},{\"end\":106492,\"start\":106483},{\"end\":106848,\"start\":106839},{\"end\":106996,\"start\":106987},{\"end\":107332,\"start\":107323},{\"end\":107462,\"start\":107453},{\"end\":107568,\"start\":107559},{\"end\":108283,\"start\":108274},{\"end\":109634,\"start\":109627},{\"end\":111364,\"start\":111355},{\"end\":111642,\"start\":111633},{\"end\":111751,\"start\":111744},{\"end\":112377,\"start\":112368},{\"end\":112737,\"start\":112728}]", "table": "[{\"end\":109625,\"start\":108978},{\"end\":109902,\"start\":109759},{\"end\":111353,\"start\":110168},{\"end\":111525,\"start\":111437},{\"end\":111631,\"start\":111530},{\"end\":112366,\"start\":112304},{\"end\":112726,\"start\":112664},{\"end\":113050,\"start\":112868},{\"end\":113200,\"start\":113099}]", "figure_caption": "[{\"end\":100851,\"start\":100452},{\"end\":100911,\"start\":100863},{\"end\":101129,\"start\":100921},{\"end\":101329,\"start\":101141},{\"end\":101542,\"start\":101341},{\"end\":101663,\"start\":101556},{\"end\":101880,\"start\":101677},{\"end\":102089,\"start\":101894},{\"end\":102363,\"start\":102103},{\"end\":102592,\"start\":102377},{\"end\":102801,\"start\":102606},{\"end\":103444,\"start\":102815},{\"end\":104956,\"start\":103469},{\"end\":105145,\"start\":104970},{\"end\":105258,\"start\":105159},{\"end\":106045,\"start\":105283},{\"end\":106147,\"start\":106059},{\"end\":106481,\"start\":106161},{\"end\":106837,\"start\":106495},{\"end\":106985,\"start\":106851},{\"end\":107321,\"start\":106999},{\"end\":107451,\"start\":107335},{\"end\":107557,\"start\":107465},{\"end\":108272,\"start\":107571},{\"end\":108519,\"start\":108286},{\"end\":108978,\"start\":108522},{\"end\":109759,\"start\":109636},{\"end\":110168,\"start\":109905},{\"end\":111437,\"start\":111366},{\"end\":111530,\"start\":111528},{\"end\":111742,\"start\":111644},{\"end\":112272,\"start\":111753},{\"end\":112304,\"start\":112275},{\"end\":112632,\"start\":112379},{\"end\":112664,\"start\":112635},{\"end\":112841,\"start\":112739},{\"end\":112868,\"start\":112844},{\"end\":113099,\"start\":113053}]", "figure_ref": "[{\"end\":3959,\"start\":3951},{\"end\":5743,\"start\":5735},{\"end\":7605,\"start\":7597},{\"end\":9051,\"start\":9043},{\"end\":10918,\"start\":10912},{\"end\":11973,\"start\":11965},{\"end\":12455,\"start\":12447},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13044,\"start\":13036},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14743,\"start\":14734},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14870,\"start\":14862},{\"end\":15396,\"start\":15390},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16012,\"start\":16003},{\"end\":16679,\"start\":16671},{\"end\":16969,\"start\":16961},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17229,\"start\":17221},{\"end\":17488,\"start\":17480},{\"end\":18065,\"start\":18057},{\"end\":36826,\"start\":36817},{\"end\":37266,\"start\":37258},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":40670,\"start\":40661},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":40796,\"start\":40786},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":48956,\"start\":48947},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49241,\"start\":49232},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":49538,\"start\":49531},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":49713,\"start\":49704},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":51429,\"start\":51420},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":51874,\"start\":51865},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":52616,\"start\":52607},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":52817,\"start\":52808},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":53139,\"start\":53130},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":54136,\"start\":54127},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":54305,\"start\":54296},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":54866,\"start\":54857},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":56117,\"start\":56108},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":56253,\"start\":56246},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":57527,\"start\":57518},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":57958,\"start\":57949},{\"end\":58255,\"start\":58246},{\"end\":58842,\"start\":58833},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59860,\"start\":59851},{\"end\":60257,\"start\":60244},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":61297,\"start\":61284},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":61806,\"start\":61793},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":61988,\"start\":61979},{\"attributes\":{\"ref_id\":\"fig_18\"},\"end\":63504,\"start\":63495},{\"attributes\":{\"ref_id\":\"fig_18\"},\"end\":63787,\"start\":63774},{\"attributes\":{\"ref_id\":\"fig_18\"},\"end\":63859,\"start\":63850},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":64145,\"start\":64132},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":64480,\"start\":64471},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":64924,\"start\":64915},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":64935,\"start\":64926},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":64991,\"start\":64982},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":69847,\"start\":69838},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":70198,\"start\":70189},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":70786,\"start\":70777},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":71577,\"start\":71570},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":72839,\"start\":72830},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":75463,\"start\":75454},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":76480,\"start\":76471},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":77942,\"start\":77933},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":83337,\"start\":83330},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":84777,\"start\":84768},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":85249,\"start\":85240},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":88243,\"start\":88234},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":88491,\"start\":88482},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":88523,\"start\":88514},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":92727,\"start\":92718}]", "bib_author_first_name": "[{\"end\":116019,\"start\":116018},{\"end\":116230,\"start\":116222},{\"end\":116241,\"start\":116240},{\"end\":116243,\"start\":116242},{\"end\":116254,\"start\":116253},{\"end\":116256,\"start\":116255},{\"end\":116877,\"start\":116872},{\"end\":117094,\"start\":117090},{\"end\":117412,\"start\":117408},{\"end\":117647,\"start\":117643},{\"end\":121319,\"start\":121308},{\"end\":121330,\"start\":121329},{\"end\":121341,\"start\":121340},{\"end\":122002,\"start\":122001},{\"end\":122015,\"start\":122014},{\"end\":122017,\"start\":122016},{\"end\":122282,\"start\":122281},{\"end\":122284,\"start\":122283},{\"end\":123090,\"start\":123089},{\"end\":124818,\"start\":124813},{\"end\":124820,\"start\":124819},{\"end\":124829,\"start\":124828},{\"end\":124838,\"start\":124837},{\"end\":124840,\"start\":124839},{\"end\":125554,\"start\":125553},{\"end\":125556,\"start\":125555},{\"end\":125818,\"start\":125817},{\"end\":125820,\"start\":125819},{\"end\":126303,\"start\":126290},{\"end\":126317,\"start\":126316},{\"end\":126331,\"start\":126330},{\"end\":128471,\"start\":128464},{\"end\":128848,\"start\":128839},{\"end\":128860,\"start\":128859},{\"end\":128872,\"start\":128871},{\"end\":129525,\"start\":129518},{\"end\":131992,\"start\":131987},{\"end\":132006,\"start\":132005},{\"end\":132020,\"start\":132019},{\"end\":134656,\"start\":134650},{\"end\":134681,\"start\":134676},{\"end\":134701,\"start\":134700},{\"end\":134703,\"start\":134702},{\"end\":134712,\"start\":134711},{\"end\":135607,\"start\":135606},{\"end\":135619,\"start\":135618},{\"end\":135630,\"start\":135629},{\"end\":138814,\"start\":138813},{\"end\":138833,\"start\":138832},{\"end\":139177,\"start\":139170},{\"end\":139195,\"start\":139194},{\"end\":139205,\"start\":139204},{\"end\":140688,\"start\":140681},{\"end\":140699,\"start\":140698},{\"end\":140701,\"start\":140700},{\"end\":140712,\"start\":140711},{\"end\":141627,\"start\":141618},{\"end\":141637,\"start\":141636},{\"end\":141639,\"start\":141638},{\"end\":141649,\"start\":141648},{\"end\":141651,\"start\":141650},{\"end\":141906,\"start\":141905},{\"end\":141908,\"start\":141907},{\"end\":145354,\"start\":145343},{\"end\":145363,\"start\":145362},{\"end\":145372,\"start\":145371},{\"end\":146120,\"start\":146116},{\"end\":148068,\"start\":148066},{\"end\":150081,\"start\":150080},{\"end\":150083,\"start\":150082},{\"end\":150094,\"start\":150093}]", "bib_author_last_name": "[{\"end\":114588,\"start\":114583},{\"end\":114870,\"start\":114865},{\"end\":115639,\"start\":115629},{\"end\":116016,\"start\":116010},{\"end\":116026,\"start\":116020},{\"end\":116238,\"start\":116231},{\"end\":116251,\"start\":116244},{\"end\":116263,\"start\":116257},{\"end\":117986,\"start\":117982},{\"end\":118754,\"start\":118747},{\"end\":119121,\"start\":119117},{\"end\":119561,\"start\":119555},{\"end\":119872,\"start\":119862},{\"end\":120182,\"start\":120175},{\"end\":120555,\"start\":120548},{\"end\":121017,\"start\":121010},{\"end\":121327,\"start\":121320},{\"end\":121338,\"start\":121331},{\"end\":121351,\"start\":121342},{\"end\":121629,\"start\":121621},{\"end\":121991,\"start\":121982},{\"end\":121999,\"start\":121993},{\"end\":122012,\"start\":122003},{\"end\":122024,\"start\":122018},{\"end\":122300,\"start\":122285},{\"end\":122532,\"start\":122529},{\"end\":122832,\"start\":122827},{\"end\":123110,\"start\":123091},{\"end\":123327,\"start\":123319},{\"end\":123713,\"start\":123706},{\"end\":124041,\"start\":124034},{\"end\":124246,\"start\":124239},{\"end\":124826,\"start\":124821},{\"end\":124835,\"start\":124830},{\"end\":124846,\"start\":124841},{\"end\":125241,\"start\":125236},{\"end\":125570,\"start\":125557},{\"end\":125834,\"start\":125821},{\"end\":126065,\"start\":126059},{\"end\":126314,\"start\":126304},{\"end\":126328,\"start\":126318},{\"end\":126343,\"start\":126332},{\"end\":126593,\"start\":126587},{\"end\":126881,\"start\":126878},{\"end\":127230,\"start\":127225},{\"end\":127645,\"start\":127640},{\"end\":128103,\"start\":128095},{\"end\":128857,\"start\":128849},{\"end\":128869,\"start\":128861},{\"end\":128880,\"start\":128873},{\"end\":129118,\"start\":129113},{\"end\":129907,\"start\":129897},{\"end\":130266,\"start\":130259},{\"end\":130680,\"start\":130673},{\"end\":130910,\"start\":130904},{\"end\":131162,\"start\":131159},{\"end\":131429,\"start\":131427},{\"end\":131622,\"start\":131620},{\"end\":132003,\"start\":131993},{\"end\":132017,\"start\":132007},{\"end\":132026,\"start\":132021},{\"end\":132323,\"start\":132313},{\"end\":132700,\"start\":132689},{\"end\":133074,\"start\":133067},{\"end\":133577,\"start\":133574},{\"end\":133892,\"start\":133884},{\"end\":134176,\"start\":134169},{\"end\":134666,\"start\":134657},{\"end\":134674,\"start\":134668},{\"end\":134698,\"start\":134682},{\"end\":134709,\"start\":134704},{\"end\":134719,\"start\":134713},{\"end\":134997,\"start\":134994},{\"end\":135405,\"start\":135398},{\"end\":135616,\"start\":135608},{\"end\":135627,\"start\":135620},{\"end\":135641,\"start\":135631},{\"end\":136168,\"start\":136160},{\"end\":136748,\"start\":136739},{\"end\":137089,\"start\":137080},{\"end\":137444,\"start\":137439},{\"end\":137773,\"start\":137763},{\"end\":138317,\"start\":138314},{\"end\":138811,\"start\":138806},{\"end\":138830,\"start\":138815},{\"end\":138841,\"start\":138834},{\"end\":139184,\"start\":139178},{\"end\":139192,\"start\":139186},{\"end\":139202,\"start\":139196},{\"end\":139212,\"start\":139206},{\"end\":139434,\"start\":139423},{\"end\":139852,\"start\":139845},{\"end\":140225,\"start\":140218},{\"end\":140696,\"start\":140689},{\"end\":140709,\"start\":140702},{\"end\":140720,\"start\":140713},{\"end\":141097,\"start\":141091},{\"end\":141634,\"start\":141628},{\"end\":141646,\"start\":141640},{\"end\":141659,\"start\":141652},{\"end\":141926,\"start\":141909},{\"end\":142222,\"start\":142214},{\"end\":142531,\"start\":142523},{\"end\":143649,\"start\":143643},{\"end\":144310,\"start\":144303},{\"end\":144635,\"start\":144629},{\"end\":144997,\"start\":144989},{\"end\":145360,\"start\":145355},{\"end\":145369,\"start\":145364},{\"end\":145382,\"start\":145373},{\"end\":145677,\"start\":145670},{\"end\":145874,\"start\":145871},{\"end\":146576,\"start\":146574},{\"end\":147334,\"start\":147331},{\"end\":148424,\"start\":148420},{\"end\":148740,\"start\":148736},{\"end\":149030,\"start\":149027},{\"end\":149473,\"start\":149470},{\"end\":149783,\"start\":149780},{\"end\":150070,\"start\":150063},{\"end\":150078,\"start\":150072},{\"end\":150091,\"start\":150084},{\"end\":150101,\"start\":150095}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":114832,\"start\":114583},{\"attributes\":{\"id\":\"b1\"},\"end\":115588,\"start\":114834},{\"attributes\":{\"id\":\"b2\"},\"end\":115965,\"start\":115590},{\"attributes\":{\"id\":\"b3\"},\"end\":116161,\"start\":115967},{\"attributes\":{\"id\":\"b4\"},\"end\":116820,\"start\":116163},{\"attributes\":{\"id\":\"b5\"},\"end\":117024,\"start\":116822},{\"attributes\":{\"id\":\"b6\"},\"end\":117340,\"start\":117026},{\"attributes\":{\"id\":\"b7\"},\"end\":117592,\"start\":117342},{\"attributes\":{\"id\":\"b8\"},\"end\":117924,\"start\":117594},{\"attributes\":{\"id\":\"b9\"},\"end\":118745,\"start\":117926},{\"attributes\":{\"id\":\"b10\"},\"end\":119062,\"start\":118747},{\"attributes\":{\"id\":\"b11\"},\"end\":119449,\"start\":119064},{\"attributes\":{\"id\":\"b12\"},\"end\":119810,\"start\":119451},{\"attributes\":{\"id\":\"b13\"},\"end\":120133,\"start\":119812},{\"attributes\":{\"id\":\"b14\"},\"end\":120483,\"start\":120135},{\"attributes\":{\"id\":\"b15\"},\"end\":120839,\"start\":120485},{\"attributes\":{\"id\":\"b16\"},\"end\":121278,\"start\":120841},{\"attributes\":{\"id\":\"b17\"},\"end\":121567,\"start\":121280},{\"attributes\":{\"id\":\"b18\"},\"end\":121896,\"start\":121569},{\"attributes\":{\"id\":\"b19\"},\"end\":122251,\"start\":121898},{\"attributes\":{\"id\":\"b20\"},\"end\":122434,\"start\":122253},{\"attributes\":{\"id\":\"b21\"},\"end\":122777,\"start\":122436},{\"attributes\":{\"id\":\"b22\"},\"end\":123085,\"start\":122779},{\"attributes\":{\"id\":\"b23\"},\"end\":123235,\"start\":123087},{\"attributes\":{\"id\":\"b24\"},\"end\":123646,\"start\":123237},{\"attributes\":{\"id\":\"b25\"},\"end\":124032,\"start\":123648},{\"attributes\":{\"id\":\"b26\"},\"end\":124185,\"start\":124034},{\"attributes\":{\"id\":\"b27\"},\"end\":124713,\"start\":124187},{\"attributes\":{\"id\":\"b28\"},\"end\":125094,\"start\":124715},{\"attributes\":{\"id\":\"b29\"},\"end\":125522,\"start\":125096},{\"attributes\":{\"id\":\"b30\"},\"end\":125779,\"start\":125524},{\"attributes\":{\"id\":\"b31\"},\"end\":125988,\"start\":125781},{\"attributes\":{\"id\":\"b32\"},\"end\":126264,\"start\":125990},{\"attributes\":{\"id\":\"b33\"},\"end\":126503,\"start\":126266},{\"attributes\":{\"id\":\"b34\"},\"end\":126809,\"start\":126505},{\"attributes\":{\"id\":\"b35\"},\"end\":127125,\"start\":126811},{\"attributes\":{\"id\":\"b36\"},\"end\":127577,\"start\":127127},{\"attributes\":{\"id\":\"b37\"},\"end\":128021,\"start\":127579},{\"attributes\":{\"id\":\"b38\"},\"end\":128426,\"start\":128023},{\"attributes\":{\"id\":\"b39\"},\"end\":128770,\"start\":128428},{\"attributes\":{\"id\":\"b40\"},\"end\":129076,\"start\":128772},{\"attributes\":{\"id\":\"b41\"},\"end\":129469,\"start\":129078},{\"attributes\":{\"id\":\"b42\"},\"end\":129830,\"start\":129471},{\"attributes\":{\"id\":\"b43\"},\"end\":130179,\"start\":129832},{\"attributes\":{\"id\":\"b44\"},\"end\":130599,\"start\":130181},{\"attributes\":{\"id\":\"b45\"},\"end\":130902,\"start\":130601},{\"attributes\":{\"id\":\"b46\"},\"end\":131114,\"start\":130904},{\"attributes\":{\"id\":\"b47\"},\"end\":131425,\"start\":131116},{\"attributes\":{\"id\":\"b48\"},\"end\":131618,\"start\":131427},{\"attributes\":{\"id\":\"b49\"},\"end\":131890,\"start\":131620},{\"attributes\":{\"id\":\"b50\"},\"end\":132311,\"start\":131892},{\"attributes\":{\"id\":\"b51\"},\"end\":132624,\"start\":132313},{\"attributes\":{\"id\":\"b52\"},\"end\":133007,\"start\":132626},{\"attributes\":{\"id\":\"b53\"},\"end\":133572,\"start\":133009},{\"attributes\":{\"id\":\"b54\"},\"end\":133838,\"start\":133574},{\"attributes\":{\"id\":\"b55\"},\"end\":134105,\"start\":133840},{\"attributes\":{\"id\":\"b56\"},\"end\":134588,\"start\":134107},{\"attributes\":{\"id\":\"b57\"},\"end\":134892,\"start\":134590},{\"attributes\":{\"id\":\"b58\"},\"end\":135330,\"start\":134894},{\"attributes\":{\"id\":\"b59\"},\"end\":135604,\"start\":135332},{\"attributes\":{\"id\":\"b60\"},\"end\":136094,\"start\":135606},{\"attributes\":{\"id\":\"b61\"},\"end\":136659,\"start\":136096},{\"attributes\":{\"id\":\"b62\"},\"end\":137013,\"start\":136661},{\"attributes\":{\"id\":\"b63\"},\"end\":137323,\"start\":137015},{\"attributes\":{\"id\":\"b64\"},\"end\":137690,\"start\":137325},{\"attributes\":{\"id\":\"b65\"},\"end\":138233,\"start\":137692},{\"attributes\":{\"id\":\"b66\"},\"end\":138804,\"start\":138235},{\"attributes\":{\"id\":\"b67\"},\"end\":139118,\"start\":138806},{\"attributes\":{\"id\":\"b68\"},\"end\":139370,\"start\":139120},{\"attributes\":{\"id\":\"b69\"},\"end\":139782,\"start\":139372},{\"attributes\":{\"id\":\"b70\"},\"end\":140114,\"start\":139784},{\"attributes\":{\"id\":\"b71\"},\"end\":140645,\"start\":140116},{\"attributes\":{\"id\":\"b72\"},\"end\":141001,\"start\":140647},{\"attributes\":{\"id\":\"b73\"},\"end\":141530,\"start\":141003},{\"attributes\":{\"id\":\"b74\"},\"end\":141849,\"start\":141532},{\"attributes\":{\"id\":\"b75\"},\"end\":142111,\"start\":141851},{\"attributes\":{\"id\":\"b76\"},\"end\":142427,\"start\":142113},{\"attributes\":{\"id\":\"b77\"},\"end\":143575,\"start\":142429},{\"attributes\":{\"id\":\"b78\"},\"end\":144301,\"start\":143577},{\"attributes\":{\"id\":\"b79\"},\"end\":144580,\"start\":144303},{\"attributes\":{\"id\":\"b80\"},\"end\":144855,\"start\":144582},{\"attributes\":{\"id\":\"b81\"},\"end\":145272,\"start\":144857},{\"attributes\":{\"id\":\"b82\"},\"end\":145619,\"start\":145274},{\"attributes\":{\"id\":\"b83\"},\"end\":145869,\"start\":145621},{\"attributes\":{\"id\":\"b84\"},\"end\":146051,\"start\":145871},{\"attributes\":{\"id\":\"b85\"},\"end\":146455,\"start\":146053},{\"attributes\":{\"id\":\"b86\"},\"end\":147211,\"start\":146457},{\"attributes\":{\"id\":\"b87\"},\"end\":148064,\"start\":147213},{\"attributes\":{\"id\":\"b88\"},\"end\":148355,\"start\":148066},{\"attributes\":{\"id\":\"b89\"},\"end\":148638,\"start\":148357},{\"attributes\":{\"id\":\"b90\"},\"end\":148952,\"start\":148640},{\"attributes\":{\"id\":\"b91\"},\"end\":149397,\"start\":148954},{\"attributes\":{\"id\":\"b92\"},\"end\":149692,\"start\":149399},{\"attributes\":{\"id\":\"b93\"},\"end\":150005,\"start\":149694},{\"attributes\":{\"id\":\"b94\"},\"end\":150400,\"start\":150007}]", "bib_title": "[{\"end\":114863,\"start\":114834},{\"end\":115627,\"start\":115590},{\"end\":116008,\"start\":115967},{\"end\":116220,\"start\":116163},{\"end\":117406,\"start\":117342},{\"end\":117641,\"start\":117594},{\"end\":117980,\"start\":117926},{\"end\":119115,\"start\":119064},{\"end\":119553,\"start\":119451},{\"end\":119860,\"start\":119812},{\"end\":120173,\"start\":120135},{\"end\":120546,\"start\":120485},{\"end\":121306,\"start\":121280},{\"end\":121619,\"start\":121569},{\"end\":121980,\"start\":121898},{\"end\":122279,\"start\":122253},{\"end\":122825,\"start\":122779},{\"end\":123317,\"start\":123237},{\"end\":123704,\"start\":123648},{\"end\":124237,\"start\":124187},{\"end\":124811,\"start\":124715},{\"end\":125234,\"start\":125096},{\"end\":125815,\"start\":125781},{\"end\":126057,\"start\":125990},{\"end\":126288,\"start\":126266},{\"end\":126585,\"start\":126505},{\"end\":126876,\"start\":126811},{\"end\":127223,\"start\":127127},{\"end\":127638,\"start\":127579},{\"end\":128093,\"start\":128023},{\"end\":128462,\"start\":128428},{\"end\":129111,\"start\":129078},{\"end\":129516,\"start\":129471},{\"end\":129895,\"start\":129832},{\"end\":130257,\"start\":130181},{\"end\":130671,\"start\":130601},{\"end\":131157,\"start\":131116},{\"end\":131985,\"start\":131892},{\"end\":132687,\"start\":132626},{\"end\":133065,\"start\":133009},{\"end\":134167,\"start\":134107},{\"end\":134648,\"start\":134590},{\"end\":134992,\"start\":134894},{\"end\":136158,\"start\":136096},{\"end\":136737,\"start\":136661},{\"end\":137078,\"start\":137015},{\"end\":137437,\"start\":137325},{\"end\":137761,\"start\":137692},{\"end\":138312,\"start\":138235},{\"end\":139421,\"start\":139372},{\"end\":139843,\"start\":139784},{\"end\":140216,\"start\":140116},{\"end\":140679,\"start\":140647},{\"end\":141089,\"start\":141003},{\"end\":142521,\"start\":142429},{\"end\":143641,\"start\":143577},{\"end\":144627,\"start\":144582},{\"end\":144987,\"start\":144857},{\"end\":145341,\"start\":145274},{\"end\":146114,\"start\":146053},{\"end\":146572,\"start\":146457},{\"end\":147329,\"start\":147213},{\"end\":148418,\"start\":148357},{\"end\":148734,\"start\":148640},{\"end\":149025,\"start\":148954},{\"end\":149468,\"start\":149399},{\"end\":150061,\"start\":150007}]", "bib_author": "[{\"end\":114590,\"start\":114583},{\"end\":114872,\"start\":114865},{\"end\":115641,\"start\":115629},{\"end\":116018,\"start\":116010},{\"end\":116028,\"start\":116018},{\"end\":116240,\"start\":116222},{\"end\":116253,\"start\":116240},{\"end\":116265,\"start\":116253},{\"end\":116880,\"start\":116872},{\"end\":117097,\"start\":117090},{\"end\":117415,\"start\":117408},{\"end\":117650,\"start\":117643},{\"end\":117988,\"start\":117982},{\"end\":118756,\"start\":118747},{\"end\":119123,\"start\":119117},{\"end\":119563,\"start\":119555},{\"end\":119874,\"start\":119862},{\"end\":120184,\"start\":120175},{\"end\":120557,\"start\":120548},{\"end\":121019,\"start\":121010},{\"end\":121329,\"start\":121308},{\"end\":121340,\"start\":121329},{\"end\":121353,\"start\":121340},{\"end\":121631,\"start\":121621},{\"end\":121993,\"start\":121982},{\"end\":122001,\"start\":121993},{\"end\":122014,\"start\":122001},{\"end\":122026,\"start\":122014},{\"end\":122302,\"start\":122281},{\"end\":122534,\"start\":122529},{\"end\":122834,\"start\":122827},{\"end\":123112,\"start\":123089},{\"end\":123329,\"start\":123319},{\"end\":123715,\"start\":123706},{\"end\":124043,\"start\":124034},{\"end\":124248,\"start\":124239},{\"end\":124828,\"start\":124813},{\"end\":124837,\"start\":124828},{\"end\":124848,\"start\":124837},{\"end\":125243,\"start\":125236},{\"end\":125572,\"start\":125553},{\"end\":125836,\"start\":125817},{\"end\":126067,\"start\":126059},{\"end\":126316,\"start\":126290},{\"end\":126330,\"start\":126316},{\"end\":126345,\"start\":126330},{\"end\":126595,\"start\":126587},{\"end\":126883,\"start\":126878},{\"end\":127232,\"start\":127225},{\"end\":127647,\"start\":127640},{\"end\":128105,\"start\":128095},{\"end\":128474,\"start\":128464},{\"end\":128859,\"start\":128839},{\"end\":128871,\"start\":128859},{\"end\":128882,\"start\":128871},{\"end\":129120,\"start\":129113},{\"end\":129528,\"start\":129518},{\"end\":129909,\"start\":129897},{\"end\":130268,\"start\":130259},{\"end\":130682,\"start\":130673},{\"end\":130912,\"start\":130904},{\"end\":131164,\"start\":131159},{\"end\":131431,\"start\":131427},{\"end\":131624,\"start\":131620},{\"end\":132005,\"start\":131987},{\"end\":132019,\"start\":132005},{\"end\":132028,\"start\":132019},{\"end\":132325,\"start\":132313},{\"end\":132702,\"start\":132689},{\"end\":133076,\"start\":133067},{\"end\":133579,\"start\":133574},{\"end\":133894,\"start\":133884},{\"end\":134178,\"start\":134169},{\"end\":134668,\"start\":134650},{\"end\":134676,\"start\":134668},{\"end\":134700,\"start\":134676},{\"end\":134711,\"start\":134700},{\"end\":134721,\"start\":134711},{\"end\":134999,\"start\":134994},{\"end\":135407,\"start\":135398},{\"end\":135618,\"start\":135606},{\"end\":135629,\"start\":135618},{\"end\":135643,\"start\":135629},{\"end\":136170,\"start\":136160},{\"end\":136750,\"start\":136739},{\"end\":137091,\"start\":137080},{\"end\":137446,\"start\":137439},{\"end\":137775,\"start\":137763},{\"end\":138319,\"start\":138314},{\"end\":138813,\"start\":138806},{\"end\":138832,\"start\":138813},{\"end\":138843,\"start\":138832},{\"end\":139186,\"start\":139170},{\"end\":139194,\"start\":139186},{\"end\":139204,\"start\":139194},{\"end\":139214,\"start\":139204},{\"end\":139436,\"start\":139423},{\"end\":139854,\"start\":139845},{\"end\":140227,\"start\":140218},{\"end\":140698,\"start\":140681},{\"end\":140711,\"start\":140698},{\"end\":140722,\"start\":140711},{\"end\":141099,\"start\":141091},{\"end\":141636,\"start\":141618},{\"end\":141648,\"start\":141636},{\"end\":141661,\"start\":141648},{\"end\":141928,\"start\":141905},{\"end\":142224,\"start\":142214},{\"end\":142533,\"start\":142523},{\"end\":143651,\"start\":143643},{\"end\":144312,\"start\":144303},{\"end\":144637,\"start\":144629},{\"end\":144999,\"start\":144989},{\"end\":145362,\"start\":145343},{\"end\":145371,\"start\":145362},{\"end\":145384,\"start\":145371},{\"end\":145679,\"start\":145670},{\"end\":145876,\"start\":145871},{\"end\":146123,\"start\":146116},{\"end\":146578,\"start\":146574},{\"end\":147336,\"start\":147331},{\"end\":148071,\"start\":148066},{\"end\":148426,\"start\":148420},{\"end\":148742,\"start\":148736},{\"end\":149032,\"start\":149027},{\"end\":149475,\"start\":149470},{\"end\":149785,\"start\":149780},{\"end\":150072,\"start\":150063},{\"end\":150080,\"start\":150072},{\"end\":150093,\"start\":150080},{\"end\":150103,\"start\":150093}]", "bib_venue": "[{\"end\":115179,\"start\":115024},{\"end\":116474,\"start\":116378},{\"end\":118141,\"start\":118073},{\"end\":124401,\"start\":124333},{\"end\":127788,\"start\":127726},{\"end\":129263,\"start\":129200},{\"end\":133275,\"start\":133184},{\"end\":136323,\"start\":136255},{\"end\":137916,\"start\":137854},{\"end\":140368,\"start\":140306},{\"end\":141252,\"start\":141184},{\"end\":142712,\"start\":142646},{\"end\":143910,\"start\":143789},{\"end\":148455,\"start\":148449},{\"end\":149151,\"start\":149100},{\"end\":114636,\"start\":114606},{\"end\":115022,\"start\":114872},{\"end\":115704,\"start\":115641},{\"end\":116045,\"start\":116028},{\"end\":116376,\"start\":116265},{\"end\":116870,\"start\":116822},{\"end\":117088,\"start\":117026},{\"end\":117420,\"start\":117415},{\"end\":117711,\"start\":117650},{\"end\":118071,\"start\":117988},{\"end\":118848,\"start\":118772},{\"end\":119162,\"start\":119123},{\"end\":119583,\"start\":119563},{\"end\":119914,\"start\":119874},{\"end\":120223,\"start\":120184},{\"end\":120582,\"start\":120557},{\"end\":121008,\"start\":120841},{\"end\":121402,\"start\":121353},{\"end\":121642,\"start\":121631},{\"end\":122055,\"start\":122026},{\"end\":122325,\"start\":122302},{\"end\":122527,\"start\":122436},{\"end\":122881,\"start\":122834},{\"end\":123375,\"start\":123329},{\"end\":123786,\"start\":123715},{\"end\":124078,\"start\":124043},{\"end\":124331,\"start\":124248},{\"end\":124873,\"start\":124848},{\"end\":125263,\"start\":125243},{\"end\":125551,\"start\":125524},{\"end\":125869,\"start\":125836},{\"end\":126071,\"start\":126067},{\"end\":126363,\"start\":126345},{\"end\":126613,\"start\":126595},{\"end\":126917,\"start\":126883},{\"end\":127308,\"start\":127232},{\"end\":127724,\"start\":127647},{\"end\":128176,\"start\":128105},{\"end\":128539,\"start\":128474},{\"end\":128837,\"start\":128772},{\"end\":129198,\"start\":129120},{\"end\":129589,\"start\":129528},{\"end\":129958,\"start\":129909},{\"end\":130307,\"start\":130268},{\"end\":130707,\"start\":130682},{\"end\":130957,\"start\":130928},{\"end\":131189,\"start\":131164},{\"end\":131482,\"start\":131431},{\"end\":131714,\"start\":131640},{\"end\":132077,\"start\":132028},{\"end\":132418,\"start\":132341},{\"end\":132741,\"start\":132702},{\"end\":133182,\"start\":133076},{\"end\":133650,\"start\":133594},{\"end\":133882,\"start\":133840},{\"end\":134225,\"start\":134200},{\"end\":134725,\"start\":134721},{\"end\":135069,\"start\":134999},{\"end\":135396,\"start\":135332},{\"end\":135708,\"start\":135659},{\"end\":136253,\"start\":136170},{\"end\":136790,\"start\":136750},{\"end\":137116,\"start\":137091},{\"end\":137466,\"start\":137446},{\"end\":137852,\"start\":137775},{\"end\":138429,\"start\":138351},{\"end\":138934,\"start\":138859},{\"end\":139168,\"start\":139120},{\"end\":139483,\"start\":139436},{\"end\":139894,\"start\":139854},{\"end\":140304,\"start\":140227},{\"end\":140793,\"start\":140722},{\"end\":141182,\"start\":141099},{\"end\":141616,\"start\":141532},{\"end\":141903,\"start\":141851},{\"end\":142212,\"start\":142113},{\"end\":142644,\"start\":142563},{\"end\":143787,\"start\":143651},{\"end\":144357,\"start\":144327},{\"end\":144648,\"start\":144637},{\"end\":145019,\"start\":144999},{\"end\":145424,\"start\":145384},{\"end\":145668,\"start\":145621},{\"end\":145915,\"start\":145876},{\"end\":146194,\"start\":146123},{\"end\":146649,\"start\":146578},{\"end\":147406,\"start\":147336},{\"end\":148160,\"start\":148087},{\"end\":148447,\"start\":148426},{\"end\":148753,\"start\":148742},{\"end\":149098,\"start\":149032},{\"end\":149513,\"start\":149475},{\"end\":149778,\"start\":149694},{\"end\":150174,\"start\":150103}]"}}}, "year": 2023, "month": 12, "day": 17}