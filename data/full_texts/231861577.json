{"id": 231861577, "updated": "2023-10-06 06:05:02.354", "metadata": {"title": "Decontextualization: Making Sentences Stand-Alone", "authors": "[{\"first\":\"Eunsol\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"Jennimaria\",\"last\":\"Palomaki\",\"middle\":[]},{\"first\":\"Matthew\",\"last\":\"Lamm\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Kwiatkowski\",\"middle\":[]},{\"first\":\"Dipanjan\",\"last\":\"Das\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Collins\",\"middle\":[]}]", "venue": "Transactions of the Association for Computational Linguistics", "journal": "Transactions of the Association for Computational Linguistics", "publication_date": {"year": 2021, "month": 2, "day": 9}, "abstract": "Models for question answering, dialogue agents, and summarization often interpret the meaning of a sentence in a rich context and use that meaning in a new context. Taking excerpts of text can be problematic, as key pieces may not be explicit in a local window. We isolate and define the problem of sentence decontextualization: taking a sentence together with its context and rewriting it to be interpretable out of context, while preserving its meaning. We describe an annotation procedure, collect data on the Wikipedia corpus, and use the data to train models to automatically decontextualize sentences. We present preliminary studies that show the value of sentence decontextualization in a user facing task, and as preprocessing for systems that perform document understanding. We argue that decontextualization is an important subtask in many downstream applications, and that the definitions and resources provided can benefit tasks that operate on sentences that occur in a richer context.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2102.05169", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tacl/ChoiPLKDC21", "doi": "10.1162/tacl_a_00377"}}, "content": {"source": {"pdf_hash": "972a74968d2522908b06c5bd1e26266194c5a9ee", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2102.05169v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00377/1924234/tacl_a_00377.pdf", "status": "GOLD"}}, "grobid": {"id": "98ecbb5596a95198f3cdc0a12f1a76b78cc25b63", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/972a74968d2522908b06c5bd1e26266194c5a9ee.txt", "contents": "\nDecontextualization: Making Sentences Stand-Alone\n\n\nEunsol Choi eunsol@cs.utexas.edu \nDepartment of Computer Science\nThe University of Texas at Austin\n\n\nJennimaria Palomaki jpalomaki@google.com \nGoogle Research\n\n\nMatthew Lamm mrlamm@google.com \nGoogle Research\n\n\nTom Kwiatkowski \nGoogle Research\n\n\nDipanjan Das dipanjand@google.com \nGoogle Research\n\n\nMichael Collins mjcollins@google.com \nGoogle Research\n\n\nDecontextualization: Making Sentences Stand-Alone\n\nModels for question answering, dialogue agents, and summarization often interpret the meaning of a sentence in a rich context and use that meaning in a new context. Taking excerpts of text can be problematic, as key pieces may not be explicit in a local window. We isolate and define the problem of sentence decontextualization: taking a sentence together with its context and rewriting it to be interpretable out of context, while preserving its meaning. We describe an annotation procedure, collect data on the Wikipedia corpus, and use the data to train models to automatically decontextualize sentences. We present preliminary studies that show the value of sentence decontextualization in a user facing task, and as preprocessing for systems that perform document understanding. We argue that decontextualization is an important subtask in many downstream applications, and that the definitions and resources provided can benefit tasks that operate on sentences that occur in a richer context.\n\nIntroduction\n\nMany applications of natural language processing need to be able to interpret, or present, text independently from the rich context in which it occurs. For example, summarization systems extract salient information from documents and present it in a reduced context. Many systems also segment documents prior to interpretation of retrieval for computational efficiency. In all of these cases, we would like the context-reduction step to be meaning preserving but, to date, there has been no independent method of ensuring this.\n\nIn this paper we isolate and define the problem of sentence decontextualization: taking a sentence * Work done at Google.\n\nPage Title: Croatia at the FIFA World Cup Paragraph: Croatia national football team have appeared in the FIFA World Cup on five occasions (in 1998, 2002, 2006, 2014 and 2018) since gaining independence in 1991. Before that, from 1930 to 1990 Croatia was part of Yugoslavia. Their best result thus far was reaching the 2018 final, where they lost 4-2 to France. Decontextualized Sentence: The Croatia national football team's best result thus far in the FIFA World Cup was reaching the 2018 final, where they lost 4-2 to France. together with its context and rewriting it to be interpretable out of context if feasible, while preserving its meaning. 1 Having defined the problem, we operationalize this definition into a high quality annotation procedure; use the resulting data to train models to automatically decontextualize sentences; and present preliminary results that show the value of automatic decontextualization in a user facing task, and as preprocessing for systems that perform document understanding. We argue that decontextualization is an important sub-task in many downstream applications, and we believe this work can benefit tasks that operate on sentences that occur in a wider context.\n\nOne contribution of this work is to release a dataset of decontextualized sentences that can be used as training and evaluation data, together with the evaluation script: on publication of this paper the data will be available at https://github. com/google-research/language/ tree/master/language/decontext. Figure 1 shows an example decontextualization. In this example we have a coreference resolution step (their \u2192 The Croatia national football team's) and a bridging step (insertion of the prepositional phrase \"in the FIFA World Cup\" to modify \"Croa-tia's best result thus far\"). Decontextualization involves various linguistic phenomena, including coreference resolution, global scoping, and bridging anaphora (Clark, 1975). We present a linguistically motivated definition of decontextualiation in Section 2 and show that this definition can be reliably applied by crowdworkers in Section 3.\n\nWe generate a corpus of decontextualized sentences corresponding to original sentences drawn from the English Wikipedia. We show that a high proportion of these original sentences can be decontextualized using a relatively simple set of rewrite operations, and we use the data to define a new automatic decontextualization task in which a computer system needs to create a decontextualized sentence from an original sentence presented in paragraph context. We discuss the implications of choosing Wikipedia as a domain in Section 3.4.\n\nWe present two methods for automatic decontextualization based on state-of-the-art coreference (Joshi et al., 2020) and generation (Raffel et al., 2019a) models. We evaluate the output of these models with automatic measures (derived from Xu et al. (2016)), as well as through human evaluation. Both automatic and human evaluations show that the largest sequence-to-sequence model produces high quality decontextualizations in the majority of cases, although it still lags human performance in the thoroughness and accuracy of these decontextualization edits.\n\nFinally, we present two demonstrations of the utility of decontextualization. The first is a user study giving evidence that decontextualized sentences can be valuable when presented to users as answers in a question-answering task-raters judge that they balance conciseness with informativeness. In the second one, we use decontextualization as a preprocessing component for generating a retrieval corpus for open domain question answering. Decontextualizing the sentences to be indexed by retrieval system enables more efficient answer string retrieval for information seeking queries. These demonstrations are presented as preliminary results, and we argue that decontextualization is an important sub-task for a wide range of NLP applications.\n\n\nLinguistic Background\n\nWe start with the following definition:\n\nDefinition 1 (Decontextualization) Given a sentence-context pair (s, c), a sentence s is a valid decontextualization of s if: (1) the sentence s is interpretable in the empty context; and (2) the truth-conditional meaning of s in the empty context is the same as the truth-conditional meaning of s in context c.\n\nA context c is a sequence of sentences preceding s, and the empty context is the empty sequence.\n\nWe have been careful here to use the more specific term \"truth conditional meaning\" rather than \"meaning\". Here we follow the distinction in semantics/pragmatics between truth conditional meaning and implicature, and deliberately exclude implicatures (which can also be considered part of the meaning of an utterance) from our definition. There is a rich history of work in semantics and pragmatics on truth-conditional meaning and implicatures, going back to Grice (1975). Our concept of \"truth conditional meaning\" is very close to \"explicature\" as used in Relevance Theory (Sperber and Wilson, 1986). Consider this description of explicature from Birner (2012) (pages 96-97, our own emphasis added):\n\n\"The explicature in an utterance is the result of enriching the semantic content with the sorts of pragmatic information necessary to provide us with a truth-evaluable proposition. This includes calculating the referents for pronouns, working out the intended interpretation for deictic phrases like here and later ..., disambiguating lexically and structurally ambiguous words and phrases, making any \"bridging\" inferences necessary for reference resolution ... and so on.\" We will see in the next section that our annotation task follows this definition quite closely.\n\nAs an example consider the following exchange:\n\nSusan: Has the Croatia national football team ever won the FIFA World Cup? Jon: Their best result thus far was reaching the 2018 final, where they lost 4-2 to France.\n\nHere the truth conditional meaning of Jon's reply is equivalent to \"Croatia's best result thus far in the FIFA World Cup was reaching the 2018 final, where they lost 4-2 to France.\", whereas the implicature would be \"the Croatia national football team has never won the FIFA World Cup\" (which answers Susan's question). In our definition the decontextualized sentence s should preserve the truth-conditional meaning, but is not required to preserve the implicature(s) of the sentence. 2 Remark (extra-linguistic context): In addition to its document context, a given sentence s and its counterpart s also come with a temporal, cultural, and geographic context -i.e. where and when they are being written or read and by whom. 3 We assume that these aspects of context are preserved during decontextualization. The effect of this is that elements of s which derive their meaning from outside of the document context will receive equivalent interpretation in s , and hence do not require decontextualization. For example, the expression \"thus far\" in Figure 1 is interpreted relative to the time of utterance, not relative to what has been previously said in the Wikipedia article, and hence it appears in both the original and decontextualized sentences.\n\n\nTask Definition\n\nAn annotator is provided with an entire document d with a target sentence within the document, represented as a start and end index s st , s end . First, the annotator decides whether the target sentence can be decontextualized or not, labeling it as FEASIBLE or INFEASIBLE. If the example is marked as FEASIBLE, the annotator decontextualizes the sentence, producing y, a new sentence that satisfies the conditions in definition 1.\n\n\nFeasibility\n\nSentences in FEASIBLE include sentences that do not require any modification to be decontextualized (e.g., \"\u00c9milie du Ch\u00e2telet proposed the hypothesis of the conservation of total energy, as distinct from momentum.\"), and sentences that require edits to stand alone.\n\nIn the decontextualization step, we instructed annotators to make only minor modifications, which includes copying and pasting a few phrases from the document to the target sentence and deleting phrases from the target sentence. When it is too challenging to decontextualize, it is classified into the INFEASIBLE category. Often, sentences in this category are a part of a narrative Page Title: Postage stamps and postal history of India Paragraph: ... In the opinion of Geoffrey Clarke , the reformed system was to be maintained \" for the benefit of the people of India and not for the purpose of swelling the revenue.\" The Commissioners voted to abolish the earlier practice of conveying official letters free of postage (\"franking\"). The new system was recommended by the Governor -General , Lord Dalhousie , and adopted by the East India Company 's Court of Directors.\n\nPage Title: Thermodynamic temperature Paragraph: ... To completely melt ice at 0 C into water at 0 C, one must add roughly 80 times the thermal energy as is required to increase the temperature of the same mass of liquid water by one degree Celsius. The metals' ratios are even greater, typically in the range of 400 to 1200 times. \n\n\nEdit Types and Linguistic Phenomena\n\nWhen an example is classified as FEASIBLE, the annotator makes edits to decontextualize the sentence. Table 1 shows the different edit types. They fall into four broad categories: NAME COMPLETION, PRONOUN / NP SWAP correspond to replacement of a referring expression that is unclear out of context with a referring expression that is unambiguous out of context. For example replacing the pronoun \"She\" with \"Cynthia Nixon\", the definite NP \"the copper statue\" with \"The Statue of Liberty\", or the abbreviated name \"Meg\" with \"Megan \"Meg\" Griffin\".\n\nDM REMOVAL involves removal of discourse markers (DMs) such as \"therefore\".\n\nBRIDGING, GLOBAL SCOPING involve addition of a phrase (typically a prepositional phrase) that modifies either a particular noun phrase (\"bridging\") or the entire sentence (\"global scoping\"). For example adding \"in the Ultimate Fighting Championship\" as a modifier to \"all fights\", or adding \"at the 2018 Cannes Film Festival\" at the end of the sentence. The additional phrase essentially spells out a modifier that is implied by the context.\n\nADDITION inserts background information that significantly improves readability: in many cases, this involves adding an appositive or premodifier \n\n\nADDITION\n\nAddition of background information that is not necessary but helps readability significantly\n\nCharles Darwin +, an English naturalist and biologist, was among the first to suggest that physiological changes caused by an emotion had a direct impact on , rather than being just the consequence of that emotion.\n\n10 to a named entity to add useful background information about that entity. Unlike other edits described above, edits in this category are optional. For example, replacing \"The Eagles\" with \"The American rock band The Eagles.\"\n\n\nVariability\n\nWe note that for a given sentence frequently there will be more than one possible decontextualization. While this inherent subjectivity makes task challenging to crowdsource and evaluate, we argue this is important feature, as shown in recent literature (Aroyo and Welty, 2015;Pavlick and Kwiatkowski, 2019;, and propose to collect multiple references per example. Table 2 shows examples where there can be multiple different correct decontextualizations.\n\nIn the first example, while the semantics of the edits are roughly equivalent, i.e., the annotators agreed on what noun phrases have to be disambiguated and information has to be added, they differ in how to rewrite the sentence. In the second example, we see disagreement on what information should be added to the sentence. We do not make any explicit assumptions about what is known and salient to the reader, and instructed them to use their best judgement to rewrite such that the new sentence is fluent, unambiguous and clear when posed alone. In the last example, annotators disagree on the feasibility. While the sentence is a part of a bigger narrative, two annotators judged it could be edited to alone, by adding a global scoping modifier, \"In Greek mythology.\"\n\n\nScope of Current Task Formulation\n\nOur data comes from the English portion of the Wikipedia corpus. We sampled sentences as follows. We first pick a (question, Wikipedia, short answer) triple from the Natural Questions  uniformly at random from the questions that have a short answer. We include the sentence containing the short answer as one example; as a second example we choose a sentence at random from the Wikipedia page. After sampling we exclude (1) sentences under a \"Plot\" category as they are often infeasible to decontextualize;\n\n(2) any sentence that is the first sentence of the page; and (3) any sentence from a paragraph containing only a single sentence. We designed this data selection process to ensure that a large proportion of examples (90%) could be decontextualized using simple edits de-Page title / Section title: We Don't Talk Anymore (Charlie Puth song) / Music video Paragraph: The music video premiered on August 2 , 2016 , on BuzzFeed and was directed by Phil Pinto . It shows Puth and Mirella Cardoso as his love interest . . . . Decontextualization 1: -It , +We Don't Talk Anymore music video shows -Puth , +Charlie Puth and Mirella Cardoso as his love interest. Decontextualization 2: -It , +The \"We Don't Talk Anymore\"(Charlie Puth song) music video shows Puth and Mirella Cardoso as his love interest. Page title: Gemini (Constellation) Paragraph: In Greek mythology, Gemini was associated with the myth of Castor and Pollux, the children of Leda and Argonauts both. Pollux was the son of Zeus, who seduced Leda, while Castor was the son of Tyndareus, king of Sparta and Leda's husband. Castor and Pollux were also mythologically associated with St. Elmo's fire in their role as the protectors of sailors. When Castor died, because he was mortal, Pollux begged his father Zeus to give Castor immortality, and he did, by uniting them together in the heavens. Decontextualization 1: INFEASIBLE Decontextualization 2: +In Greek mythology, when Castor died, because he was mortal, Pollux begged his father Zeus to give Castor immortality, and he did, by uniting them together in the heavens. Before settling on Wikipedia, we conducted an initial pilot study which revealed that encyclopedic text is substantially easier to decontextualize compared to newswire or literary text. In the latter genres, the context required for the comprehension of any given sentence appears to be much more complex in structure. Similarly, it is difficult to posit decontextualization for sentences that appear on social media platforms, because they are situated within complex and highly specific social contexts. In contrast, being written for a general audience, Wikipedia makes limited assumptions about its reader.\n\nWithin Wikipedia, we similarly found that articles on popular historical or cultural entities and events were easier to decontextualize by crowdworkers compared to articles from technical domains, such as ones on medical or mathematical concepts. Comprehension of such articles requires a considerable body of background knowledge or information from preceding paragraphs. Articles in our dataset cover topics that require little background knowledge to comprehend.\n\nWe focus on decontextualization of sentences, where the space of edits is restricted, to make the task easier to quality control and annotate. However alternate formulations, such as decontextual-ization on paragraphs could also be studied. One could even also consider allowing wider range of edits, such as multi-sentence outputs and edits beyond copy-and-pasting, such as paraphrasing and re-ordering. We anticipate exploring such alternative formulations would help to extend the scope of decontextualization to the more challenging domains previously mentioned.\n\nWe stress however that in spite of our restriction to single sentences in Wikipedia, the decontextualization task is nevertheless valuable: Wikipedia (and other encyclopedic sources) contain a wealth of factual information, and a high proportion (over 60%; see table 3) of sentences both require decontextualization and can be decontextualized under our definitions (only 30% of sentences are interpretable out of context without any edits).\n\n\nData Collection\n\nAnnotation Interface The annotator is presented a sentence in the context of an entire Wikipedia page. In the first step the annotator judges whether the example is FEASIBLE or INFEASIBLE. If the example is marked as FEASIBLE, the annotator can use delete, add, or swap operations within a user interface to produce a decontextualized string.  Train  11290  695  156  60  31  9  Dev  1945  695  162  67  21  12  Test  1945  711  160  68  20  12   Expert  100  658  163  63  26  12   Table 3: Data statistics. par. len refers to paragraph length in bytes, and sent. len refers to sentence length in bytes. All lengths are in bytes. The development and test set is five-way annotated, and the expert data is four-way annotated.\n\nData Statistics We collected one reference for each example in the training data, and five references for each example in the evaluation data.\n\nAnnotators are native speakers of English located in the United States, and on average, they took 4 minutes to annotate a single example.\n\nIn total, 28 annotators annotated the examples, with 11 annotators annotating more than 1K examples each. Table 3 represents some overall data statistics. Decontextualization is possible for the majority of examples, with the INFEASIBLE category covering roughly 10% of the data. We note a slight discrepancy between train and evaluation dataset distribution, potentially due to a change in the annotation interface. A small subset of data is annotated by the authors to be compared with the crowd-sourced data (last row in the table).\n\nAnnotation Quality We quantify the annotation agreement on the category classification. The Fleiss' kappa on category classification is 0.51 among expert annotators, and is 0.30 among the crowd annotators (binary agreement is at 85%). We observed more variability in crowdworkers as annotators' background is more diverse, and some annotators have a loose concept of \"stand alone\" and consistently attempted decontextualization.\n\nWe also measured agreement among the individual edits. For each of the edit operations (as defined in Section 3.2), we compare the output sentence after the single edit and to a set of output sentences, each after a single edit by other annotators. About 32.5% of edits were covered.\n\nBecause of the inherent annotation variability, four of the authors manually evaluated 100 crowd-sourced annotations from the evaluation data based on two measures: (1) whether the sentence is sufficiently and correctly decontextualized, and (2) whether the sentence is grammati-cally correct and fluent. Overall, 88% of annotations were valid in both, 89% on the content and 88% on form.\n\n\nAutomatic Decontextualization\n\n\nModels\n\nWe present two models for decontextualization: a coreference resolution model and a sequenceto-sequence generation model. For both models, the input is a concatenation of the title of the Wikipedia document, the section titles, and the paragraph containing the target sentence. During the annotation pilots, we found the document title is crucial for decontextualization, while section headers were frequently necessary or missing. We denote the title of the Wikipedia page as the sequence of tokens t, section titles of the paragraph as the sequence of tokens t s and the n sentences in the paragraph where the target sentence is coming from as x 1 . . . x n , where each x i is a sequence of tokens, and x t is the target sentence (1 \u2264 t \u2264 n). The model considers the concatenation of a subset of the document,\n[CLS]t[S]t s [S]x 1 \u00b7 \u00b7 \u00b7 x t\u22121 [S]x t [S]x t+1 \u00b7 \u00b7 \u00b7 x n [S]\nwhere [S] is a separator token. This representation differs from the setting of annotators, where they were given the full document context. As an approximation, we include article and section titles in the inputs, as these often contain salient contextual elements. We did experiment with giving more context, i.e., adding the first paragraph of the article as an additional input, but did not observe a performance improvement. On the initial pilot, annotators marked that 10-20% of examples required access to the full document.\n\nThe Coreference Model As many decontextualization edits can be recovered by a coreference resolution module, we adapt the output from the state-of-the-art coreference resolution system of (Joshi et al., 2020), trained on the CoNLL dataset (Pradhan et al., 2012), as a decontextualization system. We used the publicly available pre-trained checkpoint of SpanBERT-Large with the original hyper parameters. 4 We run this model on the input sequence, and map the coreference cluster predictions to modify the sentence as follows. We only consider clusters with a mention in the target sentence. For each such cluster, we find its first mention inside the target sentence, and find another mention in the same cluster that was presented earlier in the input and is longer than the current mention. If such a mention is found, we replace the current entity mention string with the earliest such mention string (e.g., \"She\" is replaced with \"Taylor Swift\"). On average, 36.5% of examples were modified through this process.\n\nThe Seq2Seq Generation Model is based on the recent T5 model (Raffel et al., 2019b). We show two variations of the model, BASE and 11B, which mainly differ in the model capacity. We fine-tune the model on our crowd- We limit the input/output to 512/128 tokens for both variants, and fine-tuned from pre-trained checkpoints 5 with a batch size of 100 examples until the validation loss stopped decreasing, after about 32K for the larger and 500K steps for the smaller model.\n\n\nEvaluation\n\n\nFeasibility Detection\n\nWe first evaluate the accuracy of models in making the feasible vs. infeasible decision. To do this we compute the binary agreements with all human references and average them to get an accuracy.\n\nResults For the feasible vs. infeasible classification task, baseline that always predicts FEA-SIBLE will have 88% accuracy. The larger variant of T5, T5-11B, achieves 89% accuracy, outperforming human agreement (85% accuracy), affirming the strong performance of pre-trained language models on classification tasks (Devlin et al., 2018). This model predicts the INFEASIBLE category infrequently for the larger variant (5% of examples), while humans classify an example as IN-FEASIBLE for 12% of examples. We observe the smaller variant, T5-Base, is less accurate, overpredicting the INFEASIBLE category (for 20% of examples), getting 77% accuracy. The coreference model cannot decide the decontextualization feasibility, as an untrained baseline.\n\n\nDecontextualized Sentence Generation\n\nSetup For development / test examples, we have five human annotations per example. We only consider examples marked by three or more annotators (out of five) as FEASIBLE for decontextualized sentence generation. For each of these examples, we discard annotations which mark the example as INFEASIBLE. For automatic evaluation and comparison, we need a human output, which will be compared to model outputs, and a set of reference annotations which will be considered as correct, gold annotations. The single human output provides a reference point for evaluation measures to which the automatic output can be compared.\n\nWe observed comparing a longer decontextualized sentence to shorter decontextualized sentences often erroneously results in low scores automatic metrics (e.g., In the last example of Table 2, adding extra information will be erroneously punished). Thus, instead of randomly selecting one annotation to be used as the representative human output, we sort the annotations by the length of the output sentence (raw bytes), and take the annotation with median length 6 as a human output and take the remaining annotations as a set of reference annotations. From manual inspection of the data the median-length output appeared often to be optimal in terms of balancing length versus accuracy of the decontextualization. Metric For each model prediction and human output, we report:\n\n\u2022 Length Increase, the average value of (len(decontext)-len(original)) / len(original).\n\n\u2022 % edited, the proportion of examples that were modified for decontextualization (as opposed to being left unchanged).\n\n\u2022 Sentence match, a binary score computed between the output and a set of references, indicating whether the output matches any of the references after normalization (stripping away articles and punctuation and lowercasing). We report two numbers, a score on all examples, and a score on examples where all references edited the sentence.  \u2022 SARI (system output against references and against the input sentence) metric (Xu et al., 2016). To compute this, for each reference, we calculate a set of add edits, corresponding to which unigrams are seen in the reference but not in the original sentence. Conversely, we can calculate the set of delete edits, corresponding to unigrams that are in the original sentence but not in the reference. We calculate precision/recall/F1-measure on add and delete edits. We look at unigrams only, and use fractional counts for the words in the references (i.e., a word appearing in one of r references will be counted as 1/r). We compute micro average across examples, i.e., globally by counting the total true positives, false negatives and false positives, as many examples do not require any edits. 7\n\nWhile the sentence match score is the easiest to interpret, it punishes longer outputs, making comparisons across systems producing outputs of different lengths challenging, and it overly rewards conservative strategies that simply copy across the original sentence. Thus, we use the SARI metric as our main evaluation metric. SARI can be thought of as a precision/recall measure on topics (unigrams) that should be added or deleted. Tables 4 and 5 show development and test performance. A successful 7 Similar to BLEU in machine translation, SARI is a useful measure for comparing different systems; however, due to the relatively large space of possible decontextualizations it will not be possible to achieve anything close to 100% F1 on SARI measures, and thus the absolute score is harder to interpret. A SARI score of for example 50% should not be interpreted as indicating a system with 50% accuracy.   Table 6: Preference between T5 output and human annotation. Columns represents the judgement of the expert A, rows that of the expert B. We see high agreement between two expert annotators, despite one expert annotator (column annotator) is ambivalent more frequently.\n\n\nAutomatic Evaluation\n\ndecontextualization system would result in high sentence match, adequate changed ratio (experts edited about 79% of examples), and length change ratio (the experts' ratio is 1.19), as well as high SARI addition and deletion scores. As a sanity check, we report REPEAT, which outputs the original sentence. This alone results in high sentence match score, around 40%, meaning that on this number of examples, at least one of the annotators deemed the sentence can stand alone without any edits.\n\nThe coreference system has an exact match of about 13% of examples that require edits, without any task-specific fine-tuning. Its SARI add scores shows high precision and low recall, and its deletion scores are low as it cannot delete discourse markers. The Seq2seq generation model achieves high scores across all measures. The bigger variant is substantially better, editing more than its smaller variant without losing precision. We observe the larger variants outperform the average human on sentence match measure, but not in SARI measures. The T5 model modifies fewer examples than the annotator, and edits involve fewer tokens, benefiting it on the sentence match measure. However, the model is more likely to miss required edits, as shown in low recall for the SARI add and deletion measures. We discuss this further in the following human evaluation section.\n\n\nHuman Evaluation\n\nWe sampled 100 examples in the evaluation set, where at least two annotators and our best model made decontextualization edits. We randomized the order or presentation of the T5 and human outputs so as to not bias the annotation. On this set, we (two of the authors) conducted a manual evaluation. Given two decontextualized sentences, one from the best model and another randomly selected from a set of annotations with decontextualization edits, we evaluated each on two dimensions: (a) is it fluent and grammatically correct? (b) is it sufficiently and correctly decontextualized? Lastly, we chose the preference between two outputs (A, B, or either).\n\nExpert annotators marked as 'sufficient' those items for which all possible referential ambiguities had been resolved. Given the subjective nature of the task, some 'insufficient' decontextualizations by the expert annotator could be valid for the another annotator with a different world knowledge. We report averaged binary scores from two experts. The model output scored 88.0% on fluency, and 67.5% on correct decontextualization, while the human reference output scored 84.5% on fluency and 78.5% on correct decontextualization. Both annotators found T5 to be slightly more fluent, while humans are more thorough and accurate in decontextualizating. Table 6 shows the preferences of two annotators. Both preferred human output, and their preferences exhibit high agreement (matching on 37 out of forty examples when both had preferences).\n\nWe briefly characterize common error patterns for annotators and the T5 model. Similar error patterns emerge between the annotations and model outputs. Both occasionally fail to identify generics that need to be replaced with referring NPs, phrases that require bridging, and temporal contexts that should be provided. Additionally, we noticed that the T5 model heavily relies on the title cues, and sometimes fail to clarify ambiguous entities that are not the main entity of the page. We noticed very few examples where T5 hallucinates factually incorrect contents.\n\n\nTwo Applications\n\nWe present two demonstrations of the utility of decontextualization. First, we argue that the decontextualized sentences can be valuable in themselves in question answering, and show that they can be useful as a preprocessing step.  Table 7: User study results. Dec. refers to decontextualized sentence answer, Ori. means original sentence answer, Par. means paragraph answer. We present raw counts of preferences and the log odds of preferring option A and its 95% confidence interval.\n\n\nDecontextualized Answer As Is\n\nWe showcase a use case of decontextualized sentences as providing a succinct yet informative answer to open domain factoid questions . We design a user study where people compare a decontextualizedsentence answer with an original-sentence answer and a paragraph answer to the same query. 8\n\nSetup Given a question and two presentations of the same answer, raters were tasked with marking their preference between the two answer presentations (option A, option B, or either). The actual short span answer in the sentence is always highlighted (similar to seen in Table 8) (See Figure 3 for a screenshot).\n\nWe conduct three comparison studies on the same set of 150 questions: (a) decontextualized sentence vs. original sentence, (b) original sentence vs. original paragraph, (c) decontextualized sentence vs. original paragraph. For each example in each study, we collected 10 user ratings. The questions are randomly chosen from a set of questions which have a short answer, and such that the sentence containing the short answer is categorized as FEASIBLE by the annotators and edits were necessary to decontextualize. We use crowd-sourced annotations of decontextualized sentences. Figure 3 shows the screenshot of the user study interface. Table 7 shows the results of the user study. We observe that decontextualized sentence answers are preferred to both the original sentence answers and the original paragraph answers. We also note that the users preferred sentence answer compared to paragraph answer in general.  : Each dot represents how frequently the decontextualized answer is preferred for a single question / rater to the original sentence for a single question (top plot) and single rater (bottom plot). Questions (top plot) and raters (bottom plot) are sorted by its preference towards the decontextualized answer. The red line is where both are equally preferred, and above the line represents question / rater where decontextualized answers were preferred. While the decontextualized answer is preferred overall, we see a large variability.\n\n\nResult\n\nWe further investigated the statistical significance of the preferences reported in Table 7. We noticed a quite large amount of question and rater variability-some raters consistently preferred a sentence answer, valuing conciseness, while some raters behaved in the other direction. Similarly, for some questions, all raters preferred a sentence answer. Figure 4 visualizes such variability based on the questions and raters.\n\nTo control for the correlations induced by the rater and question groups, we fit a generalized linear mixed model (GLMM) using the brm R package (B\u00fcrkner, 2017). For this analysis, we excluded data points where users did not show a preference (selected either). We used the formula: p \u223c 1 + (1|r) + (1|q) where p is whether a rater chose one option over the other; r is the rater id; and q is the question id. This formula specifies a regression of the log-odds of the rater preference while allowing for random effects in the raters (r) and questions (q). The last column of Table 7 shows the fixed effect coefficients and their confidence intervals. The intercept represents the strength of preference towards option A. We found a statistically significant preference for decontextualized sentences over both original sentences and the paragraphs (p-value was smaller than 0.05 for both studies).\n\nExamples We qualitatively investigated which examples benefit from decontextualization, and in which examples raters prefer paragraph answers. Table 8 shows questions together with two answer presentations, along with the predicted fixed effect question coefficient towards decontextualized answer in study (b) and towards the sentence answer in study (c). In the first row, the added information from the decontextualization is not relevant to the question, thus we observe preference against decontextualization. In the second and third row, the decontextualized sentence answer is preferred as it provides enough evidence to answer the query,\n\n\nQuery\n\nDecontextualized answer Paragraph answer (sentence answer highlighted) Decont. Ori.\n\nwhen was the rising of the moon written  while the original sentence answer does not.\n\n\nDecontextualizing System Inputs\n\nHaving shown the benefits of decontextualization in a user facing task, we now investigate the use of decontextualizaton as a preprocessing step. Specifically, we construct a passage retrieval corpus for open domain question answering (Chen et al., 2017) with decontextualized sentences. Experiment shows that decontextualized sentences ensure completeness of the passages while minimizing their length (thus computational cost).\n\nBackground Open domain question answering typically consists of pair a passage retrieval (Liu and Croft, 2002) and transformer-based answer extractor (reading comprehension model) based on the retrieved passages (Guu et al., 2020;Karpukhin et al., 2020;Izacard and Grave, 2020). And the computational cost is dominated by the cost of co-encoding the query with the retrieved passages (typically paragraphs or overlapping 100 word windows).\n\nSetup We create a corpus using the 7k documents (233k paragraphs, 868k sentences) from the documents associated with the questions in the NQ-open development set (Lee et al., 2019). We consider a retrieved passage to be correct if it con-tains one of the answer strings 9 and investigate the number of questions for which we can retrieve a correct passage for a fixed computational cost. Under this measure, we compare paragraphs, windows of 100 words, sentences, and decontextualized sentences as a set of retrieval passages. These segmentation approaches generate different number of passages for the same article (paragraph and a window of 100 words segmentation make fewer passages compared to sentences-level segmentation). To generate decontextualized sentences process all paragraphs with T5-11B model, which are trained on all annotated data (including development and test set). For about 40% of sentences, the model classified the sentence as infeasible to decontextualize or unnecessary to make any edits, we use the original sentence. On the other 60% model tended to add more information. For example, for a sentence \"Bush was widely seen as a 'pragmatic caretaker' president who lacked a unified and compelling long-term theme in his efforts.\", the decontextualized sentence will be \"George H.W. Bush was widely seen as a 'pragmatic caretaker' president of the United States who lacked a unified and compelling longterm theme in his efforts.\" and a paragraph would be entire paragraph containing this sentence, and 100-word window will be a chunk without using a sentence boundary as a segmentation. For all, we prepend the document title to the passage, following the literature and use the TFIDF as a retriever model.\n\nMetric Let q i be a question; let A i = [a 0 i . . . a n i ] be the set of valid answers; let C i = [c 1 i . . . c k i ] be a ranked list of evidence passages; and let H(A i , C i ) be the index of the top ranked context that contains one of the valid answers (or k + 1 if there is no such context). We first define the cost of encoding a single question and passage, c(q i , c m i ) = (|q i | + 1 + |c m i |) 2 . This captures the fact that the Transformer's computation cost scales quadratically with the length of the encoded text (question + separator + evidence passage).\nO(q i , A i , C i ) = H(A i ,C i ) m=1 c(q i , c m i ).\nGiven the per example cost defined above, we define the recall of the retrieval system at computational cost budget t to be:\n1 N N i=0 1 [O(q j , a j , C j ) < t](1)\nwhere N is the total number of examples and 1 is an indicator function. We use this as an evaluation measure instead of mean reciprocal rank or recall at N, to compare across different retrieval passage length.\n\nResults Figure 5 plots the recall of each retrieval corpus at different computational cost budget t on the whole NQ-open evaluation set. The graph shows that sentence level segmentation is more cost-effective than paragraph or 100 word level segmentation, and using decontextualized sentences is more cost effective than using the original sentences. Decontextualized sentences near the performance of commonly used 100 word windows with 1/10th the cost.\n\nThis result exemplifies the way in which decontextualization can be used to ensure that the input to natural language understanding system is concise yet complete. We think this way of using decontextualization as a preprocessing could also aid tasks such as summarization. \n\n\nRelated Work\n\nPrior literature in summarization studied how article context affects the understanding of sentences within an article. It has been observed that disambiguating entity mentions and correctly resolving anaphora is crucial for automatic summarization (Otterbacher et al., 2002;Steinberger et al., 2007) and for evaluation of summarization systems (Pitler et al., 2010). Li et al. (2016) identified information missing from a sentence could be identified in the article context in newswire text 60% of the time. This is considerably less frequent than for the encyclopedic text studied here, but nevertheless hints that decontextualization for newswire text could be feasible. It remains unclear whether information accessible in newswire contexts can be readily incorprated into sentences using controlled edits of the type we employ.\n\nSuccessful decontextualization models must resolve entity and event coreferences (Humphreys et al., 1997) as well as other forms of anaphora (R\u00f6siger et al., 2018). These are necessary but insufficient for decontextualization however, which also involves discourse marker removal, acronym expansion, and fluent and grammatical sentence generation.\n\nThe term decontextualization was introduced in a recent table-to-text generation dataset (Parikh et al., 2020) where a sentence from a Wikipedia document was decontextualized such that it can be interpretable when presented with a table alone. They cover only the sentences that are relevant to the table, and adapt it to the table context. In a recent image captioning dataset (Sharma et al., 2018), sentences are re-written such that information that cannot be inferred from the image is removed. For example, entity names are replaced with generics (e.g.\n\n-Tom Cruz , +A man is waiting.\").\n\n\nConclusion\n\nWe define decontextualization, the task of rewriting a sentence from a document to be interpretable in an empty context, while preserving its meaning. We build a crowdsourced dataset and a model for decontextualization, and demonstrate how decontextualization can be used in a user-facing task and as a sub-component of an application system. We believe that decontextualization will also be helpful in a wide range of other applications. For example, in multi-document summarization (Fabbri et al., 2019), co-referring entities and events must be resolved across different documents and removing ambiguous references may help; extractive summarization (Cheng and Lapata, 2016) could benefit from the type of pre-processing that we presented for opendomain QA; anaphora resolution is crucial for both summarization and machine translation (Susanne et al., 1992); and decontextualizing sentences may help in recovering explicit mentions of entities and relations which can help information extraction (Narasimhan et al., 2016). The current formulation focuses on the English encyclopedic corpus and rewriting for an empty context, and future work can explore different domains of text as well as mapping to a different context.\n\nFigure 1 :\n1An example decontextualization. The sentence to decontextualize is highlighted in grey.\n\nFigure 2 :\n2Decontextualization examples falling into the INFEASIBLE category. The sentence to be decontextualized is highlighted in grey.story, or rely heavily on the preceding few sentences. SeeFigure 2for examples.\n\n\nPage title: The American Baking Competition Paragraph: CBS placed casting calls for participants on November 14, 2012 . Auditions were held between December 1 and December 15, 2012. The competition took place at the Gibbs Gardens in Ball Ground , Georgia in March 2013. Decontextualization 1: The -competition , +American Baking Competition took place at the Gibbs Gardens in Ball Ground , Georgia in March 2013. Decontextualization 2: The -competition , +American Baking Competition, a reality competition television series, took place at the Gibbs Gardens in Ball Ground , Georgia in March 2013 .\n\n\nsourced training set, by setting the target sequence to be [CAT][SEP]y, where [CAT] \u2208 {UNNECESSARY, FEASIBLE, INFEASIBLE} and y is a decontextualized sentence when [CAT] = FEASIBLE and the original sentence when [CAT] \u2208 {UNNECESSARY, INFEASIBLE}. UN-NECESSARY are examples where the original sentence without any edit can stand alone.\n\nFigure 3 :\n3A screenshot of the instruction and an example instance comparing the original sentence answer and the paragraph answer shown to annotators for the user study.\n\nFigure 4\n4Figure 4: Each dot represents how frequently the decontextualized answer is preferred for a single question / rater to the original sentence for a single question (top plot) and single rater (bottom plot). Questions (top plot) and raters (bottom plot) are sorted by its preference towards the decontextualized answer. The red line is where both are equally preferred, and above the line represents question / rater where decontextualized answers were preferred. While the decontextualized answer is preferred overall, we see a large variability.\n\nFigure 5 :\n5Retrieval recall plotted against computational cost budget (Eqn 1) for different methods of document segmentation.\n\n\nFrance to the people of the United States, was designed by French sculptor Fr\u00e9d\u00e9ric Auguste Bartholdi and built by Gustave Eiffel.Edit Type \n\nDescription \nExample \n% \n\nPRONOUN/NP SWAP Replacement of a definite \npronoun / noun phrase \nwith another referring ex-\npression \n\n-The copper statue , +The Statue of Liberty , a gift from \nthe people of 40.5 \n\nNAME COMPLETION Expansion of acronyms or \npartial names \n\n-Meg , +Megan \"Meg\" Griffin made her first appearance \non television when Family Guy debuted on Fox on January \n31, 1999, with the episode \"Death Has a Shadow\". \n\n11.5 \n\nDM REMOVAL \nRemoval of discourse \nmarkers that can be only \nunderstood in context \n\n-For instance, Alaska could be regarded as the highest \nstate because Denali, at 20,310 feet, is the highest point in \nthe US. \n\n3.5 \n\nBRIDGING \nAddition of a modifier \n(typically a PP) to a noun \nphrase \n\nIn all fights +in the Ultimate Fighting Championship , \neach round can be no longer than five minutes. \n\n13 \n\nGLOBAL SCOPING \nAddition of a phrase (typ-\nically a PP) that modifies \nthe entire sentence \n\nThe \nJapanese \nfilm \nShoplifters, \ndirected \nby \nHirokazu \nKore-eda, \nwon \nthe \nPalme \nd'Or \n+at the 2018 Cannes Film Festival. \n\n\n\nTable 1 :\n1The list of possible edits in decontextualization. The last column represents how frequently the phenomena occurs in the data, from manual analysis on 200 examples, including examples that belongs to INFEASIBLE categories and examples that does not require any edits. The bag notation removes x and adds y ( -x , +y ) at its position.\n\nTable 2 :\n2Examples showing the diversity of valid decontextualization edits. scribed in Section 3.2.\n\nTable 4 :\n4Development set performance. Len inc. is the \naverage percentage increase in length from decontextu-\nalization. % edited is the proportion of examples that \nhave at least one edit. match-all shows percentage of \noutputs that have at least one match in the human refer-\nences; match-edited shows the match value calculated \non cases where all references include at least one edit. \n\n\n\nTable 5 :\n5Test set results. See table 4 caption for a key.T5 either Annotator Sum \n\nT5 \n13 \n12 \n2 \n27 \neither \n7 \n22 \n4 \n33 \nAnnotator \n1 \n15 \n24 \n40 \n\nSum \n21 \n49 \n30 100 \n\n\n\n\nEngland did not enter the competition until 1950. . . Their best ever performance is winning the Cup in the 1966 , whilst they also finished in fourth place in 1990, and in 2018. Other than that, the team have reached the quarter -finals on nine occasions, the latest of which were at the 2002 and the 2006 .The Rising of the Moon, Irish \nballad recounting a battle be-\ntween the United Irishmen and the \nBritish Army has been in circula-\ntion since circa 1865 . \n\nThe ballad has been in circulation since circa \n1865 . The earliest verifiable date found in pub-\nlication is 1867 . \n\n-2.09 -1.53 \n\nwhat is the most \nviewed video \non youtube in \n24 hours \n\nThe most viewed music video \nwithin 24 hours of its release is \nTaylor Swift 's Look What You \nMade Me Do . \n\nThis list of most viewed online videos in the first \n24 hours contains the top 30 online videos that \nreceived the most views within 24 hours of re-\nlease across the world. This list excludes movie \ntrailers , which are featured on the list of most \nviewed online trailers in the first 24 hours. The \nmost viewed music video in this time period is \nTaylor Swift 's Look What You Made Me Do . \n\n1.06 \n-0.48 \n\nwhen was last \ntime england \ngot to quarter \nfinals in world \ncup \n\nThe England national football \nteam have reached the quarter -fi-\nnals on nine occasions , the latest \nof which were at the 2002 and the \n2006 . \n\n1.40 \n0.70 \n\n\n\nTable 8 :\n8Examples from the user study. The last column represent coefficients for preferring original sentence over the original paragraph, and the fourth column presents coefficients for decontextualized sentence over the paragraph. Positive values means preference towards the sentence-length answer over the paragraph-length answer.\nMore precisely the truth-conditional meaning or explicature(Sperber and Wilson, 1986); see section 2 for discussion.\nWe have not necessarily given up on recovering implicatures: the decontextualized sentence will likely be a valuable intermediate step in deriving the implicatures of an utterance.3  Research on text simplification(Xu et al., 2015;Bingel et al., 2018) also shows how target output depends on the expected audience.\nhttps://github.com/facebookresearch/SpanBERT/\nhttps://github.com/google-research/text-to-text-transfertransformer\nWhen there are four of references, we take the second shortest sentence.\nUnderstanding how to present answers to users is a complex problem with many desiderata, e.g., preserving the original content, crediting the source, interaction with the user interface, which we are not covering comprehensively.\nWe adopt the answer match heuristics fromLee et al. (2019).\nAcknowledgmentsWe would like to thank members of Google AI, especially Jacob Eisenstein, Kenton Lee, Santiago Ontanon, Ankur Parikh, Daniel Andor, Chris Alberti, Slav Petrov for helpful discussions and comments. Lastly, we would like to thank our annotators.\nTruth is a lie: Crowd truth and the seven myths of human annotation. Lora Aroyo, Chris Welty, 36AI MagazineLora Aroyo and Chris Welty. 2015. Truth is a lie: Crowd truth and the seven myths of human an- notation. AI Magazine, 36:15-24.\n\nLexi: A tool for adaptive, personalized text simplification. Joachim Bingel, Gustavo Paetzold, Anders S\u00f8gaard, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational LinguisticsJoachim Bingel, Gustavo Paetzold, and Anders S\u00f8gaard. 2018. Lexi: A tool for adaptive, per- sonalized text simplification. In Proceedings of the 27th International Conference on Compu- tational Linguistics, pages 245-258, Santa Fe, New Mexico, USA. Association for Computa- tional Linguistics.\n\nBetty J Birner, Introduction to Pragmatics. Wiley Publishing1st editionBetty J. Birner. 2012. Introduction to Pragmatics, 1st edition. Wiley Publishing.\n\nbrms: An R package for Bayesian multilevel models using Stan. Paul-Christian B\u00fcrkner, 10.18637/jss.v080.i01Journal of Statistical Software. 801Paul-Christian B\u00fcrkner. 2017. brms: An R pack- age for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1):1-28.\n\nReading wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, Proceedings of the Annual Meeting of the Association for Computation Linguistics (ACL). the Annual Meeting of the Association for Computation Linguistics (ACL)Danqi Chen, Adam Fisch, Jason Weston, and An- toine Bordes. 2017. Reading wikipedia to an- swer open-domain questions. Proceedings of the Annual Meeting of the Association for Com- putation Linguistics (ACL).\n\nNeural summarization by extracting sentences and words. Jianpeng Cheng, Mirella Lapata, 10.18653/v1/P16-1046Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational LinguisticsJianpeng Cheng and Mirella Lapata. 2016. Neu- ral summarization by extracting sentences and words. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Lin- guistics, pages 484-494, Berlin, Germany. As- sociation for Computational Linguistics.\n\nBridging. H Herbert, Clark, Theoretical issues in natural language processing. Herbert H Clark. 1975. Bridging. In Theoretical issues in natural language processing.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding.\n\nMulti-news: a large-scale multi-document summarization dataset and abstractive hierarchical model. Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir R Radev, Proceedings of the Annual Meeting of the Association for Computation Linguistics (ACL). the Annual Meeting of the Association for Computation Linguistics (ACL)Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. 2019. Multi-news: a large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the Annual Meeting of the Asso- ciation for Computation Linguistics (ACL).\n\nLogic and conversation. H , Paul Grice, Speech Acts. Peter Cole and Jerry L. MorganNew YorkAcademic Press3H. Paul Grice. 1975. Logic and conversation. In Peter Cole and Jerry L. Morgan, editors, Speech Acts, volume 3 of Syntax and Semantics, pages 41-58. Academic Press, New York.\n\nRealm: Retrieval-augmented language model pretraining. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre- training.\n\nEvent coreference for information extraction. K Humphreys, R Gaizauskas, Saliha Azzam, K. Humphreys, R. Gaizauskas, and Saliha Azzam. 1997. Event coreference for information ex- traction.\n\nLeveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, Gautier Izacard and Edouard Grave. 2020. Lever- aging passage retrieval with generative models for open domain question answering.\n\nSpanbert: Improving pre-training by representing and predicting spans. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, Omer Levy, Transactions of the Association for Computational Linguistics. 8Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by represent- ing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64-77.\n\nDense passage retrieval for Open-Domain question answering. V Karpukhin, Oguz, Min, S Wu, Edunov, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingEMNLPV Karpukhin, B Oguz, S Min, L Wu, S Edunov, and others. 2020. Dense passage retrieval for Open-Domain question answering. Proceed- ings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics. Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav PetrovKenton LeeTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association of Computa- tional Linguistics.\n\nLatent retrieval for weakly supervised open domain question answering. Kenton Lee, Ming-Wei Chang, Kristina Toutanova, 1906.00300arXiv preprintKenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. arXiv preprint 1906.00300.\n\nImproving the annotation of sentence specificity. Jessy Junyi, Li, O&apos; Bridget, Y Daniel, W Wu, A Zhao, Nenkova, LREC. Junyi Jessy Li, Bridget O'Daniel, Y. Wu, W. Zhao, and A. Nenkova. 2016. Improving the annota- tion of sentence specificity. In LREC.\n\nPassage retrieval based on language models. X Liu, W Croft, CIKM '02. X. Liu and W. Croft. 2002. Passage retrieval based on language models. In CIKM '02.\n\nImproving information extraction by acquiring external evidence with reinforcement learning. Karthik Narasimhan, Adam Yala, Regina Barzilay, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingEMNLPKarthik Narasimhan, Adam Yala, and Regina Barzilay. 2016. Improving information extrac- tion by acquiring external evidence with rein- forcement learning. In Proceedings of the Con- ference on Empirical Methods in Natural Lan- guage Processing (EMNLP).\n\nRevisions that improve cohesion in multi-document summaries: a preliminary study. Jahna Otterbacher, R Dragomir, Airong Radev, Luo, Proceedings of the Annual Meeting of the Association for Computation Linguistics (ACL). the Annual Meeting of the Association for Computation Linguistics (ACL)Jahna Otterbacher, Dragomir R. Radev, and Airong Luo. 2002. Revisions that improve co- hesion in multi-document summaries: a prelim- inary study. In Proceedings of the Annual Meet- ing of the Association for Computation Linguis- tics (ACL).\n\nTotto: A controlled table-to-text generation dataset. P Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Diyi Dhingra, Dipanjan Yang, Das, abs/2004.14373Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingAnkur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. Pro- ceedings of the Conference on Empirical Meth- ods in Natural Language Processing (EMNLP), abs/2004.14373.\n\nInherent disagreements in human textual inferences. Ellie Pavlick, Tom Kwiatkowski, Transactions of the Association for Computational Linguistics. 7Ellie Pavlick and Tom Kwiatkowski. 2019. Inher- ent disagreements in human textual inferences. Transactions of the Association for Computa- tional Linguistics, 7:677-694.\n\nAutomatic evaluation of linguistic quality in multi-document summarization. Emily Pitler, Annie Louis, Ani Nenkova, Proceedings of the Annual Meeting of the Association for Computation Linguistics (ACL). the Annual Meeting of the Association for Computation Linguistics (ACL)Emily Pitler, Annie Louis, and Ani Nenkova. 2010. Automatic evaluation of linguistic qual- ity in multi-document summarization. In Pro- ceedings of the Annual Meeting of the Associa- tion for Computation Linguistics (ACL).\n\nConll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. Alessandro Sameer Pradhan, Nianwen Moschitti, Olga Xue, Yuchen Uryupina, Zhang, EMNLP-CoNLL Shared Task. Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll-2012 shared task: Modeling multilin- gual unrestricted coreference in ontonotes. In EMNLP-CoNLL Shared Task.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv preprint 1910.10683Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019a. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint 1910.10683.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, abs/1910.10683ArXiv. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019b. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683.\n\nBridging resolution: Task definition, corpus resources and rule-based experiments. Ina R\u00f6siger, Arndt Riester, Jonas Kuhn, Proceedings of the International Conference on Computational Linguistics (COLING). the International Conference on Computational Linguistics (COLING)Ina R\u00f6siger, Arndt Riester, and Jonas Kuhn. 2018. Bridging resolution: Task definition, corpus re- sources and rule-based experiments. In Pro- ceedings of the International Conference on Computational Linguistics (COLING).\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the Annual Meeting of the Association for Computation Linguistics (ACL). the Annual Meeting of the Association for Computation Linguistics (ACL)Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceed- ings of the Annual Meeting of the Association for Computation Linguistics (ACL).\n\nRelevance: Communication and Cognition. Dan Sperber, Deirdre Wilson, Harvard University PressUSADan Sperber and Deirdre Wilson. 1986. Rele- vance: Communication and Cognition. Har- vard University Press, USA.\n\nTwo uses of anaphora resolution in summarization. Josef Steinberger, Massimo Poesio, A Mijail, Karel Kabadjov, Jeek, 10.1016/j.ipm.2007.01.010Inf. Process. Manage. 436Josef Steinberger, Massimo Poesio, Mijail A. Kabadjov, and Karel Jeek. 2007. Two uses of anaphora resolution in summarization. Inf. Pro- cess. Manage., 43(6):1663-1680.\n\nChrista Hauenschild, and Carla Umbach. 1992. Anaphora resolution in machine translation. Preusz Susanne, Birte Schmitz, Preusz Susanne, Birte Schmitz, Christa Hauen- schild, and Carla Umbach. 1992. Anaphora res- olution in machine translation.\n\nWei Xu, Chris Callison-Burch, Courtney Napoles, Problems in current text simplification research: New data can help. Transactions of the Association for Computational Linguistics. 3Wei Xu, Chris Callison-Burch, and Courtney Napoles. 2015. Problems in current text simpli- fication research: New data can help. Transac- tions of the Association for Computational Lin- guistics, 3:283-297.\n\nOptimizing statistical machine translation for text simplification. Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, Chris Callison-Burch, Transactions of the Association for Computational Linguistics. 4Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. 2016. Opti- mizing statistical machine translation for text simplification. Transactions of the Association for Computational Linguistics, 4:401-415.\n", "annotations": {"author": "[{\"end\":153,\"start\":53},{\"end\":213,\"start\":154},{\"end\":263,\"start\":214},{\"end\":298,\"start\":264},{\"end\":351,\"start\":299},{\"end\":407,\"start\":352}]", "publisher": null, "author_last_name": "[{\"end\":64,\"start\":60},{\"end\":173,\"start\":165},{\"end\":226,\"start\":222},{\"end\":279,\"start\":268},{\"end\":311,\"start\":308},{\"end\":367,\"start\":360}]", "author_first_name": "[{\"end\":59,\"start\":53},{\"end\":164,\"start\":154},{\"end\":221,\"start\":214},{\"end\":267,\"start\":264},{\"end\":307,\"start\":299},{\"end\":359,\"start\":352}]", "author_affiliation": "[{\"end\":152,\"start\":87},{\"end\":212,\"start\":196},{\"end\":262,\"start\":246},{\"end\":297,\"start\":281},{\"end\":350,\"start\":334},{\"end\":406,\"start\":390}]", "title": "[{\"end\":50,\"start\":1},{\"end\":457,\"start\":408}]", "venue": null, "abstract": "[{\"end\":1457,\"start\":459}]", "bib_ref": "[{\"end\":2299,\"start\":2263},{\"end\":2775,\"start\":2774},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4063,\"start\":4050},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4885,\"start\":4865},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4923,\"start\":4901},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5025,\"start\":5009},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7028,\"start\":7016},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7158,\"start\":7132},{\"end\":8534,\"start\":8533},{\"end\":8774,\"start\":8773},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13340,\"start\":13317},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13370,\"start\":13340},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22837,\"start\":22817},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22890,\"start\":22868},{\"end\":23034,\"start\":23033},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23730,\"start\":23708},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24693,\"start\":24672},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27189,\"start\":27172},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35746,\"start\":35731},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":37601,\"start\":37582},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37888,\"start\":37867},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":38008,\"start\":37990},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38031,\"start\":38008},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38055,\"start\":38031},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38399,\"start\":38381},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":41987,\"start\":41961},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":42012,\"start\":41987},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":42078,\"start\":42057},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":42096,\"start\":42080},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":42651,\"start\":42627},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":42709,\"start\":42687},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":43005,\"start\":42984},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":43294,\"start\":43273},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":44007,\"start\":43986},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":44179,\"start\":44155},{\"end\":44363,\"start\":44341},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":44527,\"start\":44502},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":50902,\"start\":50876},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":51165,\"start\":51148},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":51185,\"start\":51165},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":51724,\"start\":51707}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44829,\"start\":44729},{\"attributes\":{\"id\":\"fig_1\"},\"end\":45048,\"start\":44830},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45649,\"start\":45049},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45986,\"start\":45650},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46159,\"start\":45987},{\"attributes\":{\"id\":\"fig_6\"},\"end\":46716,\"start\":46160},{\"attributes\":{\"id\":\"fig_7\"},\"end\":46844,\"start\":46717},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":48050,\"start\":46845},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48397,\"start\":48051},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48500,\"start\":48398},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48895,\"start\":48501},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":49072,\"start\":48896},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":50477,\"start\":49073},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50816,\"start\":50478}]", "paragraph": "[{\"end\":2000,\"start\":1473},{\"end\":2123,\"start\":2002},{\"end\":3332,\"start\":2125},{\"end\":4232,\"start\":3334},{\"end\":4768,\"start\":4234},{\"end\":5329,\"start\":4770},{\"end\":6078,\"start\":5331},{\"end\":6143,\"start\":6104},{\"end\":6456,\"start\":6145},{\"end\":6554,\"start\":6458},{\"end\":7258,\"start\":6556},{\"end\":7830,\"start\":7260},{\"end\":7878,\"start\":7832},{\"end\":8046,\"start\":7880},{\"end\":9300,\"start\":8048},{\"end\":9752,\"start\":9320},{\"end\":10034,\"start\":9768},{\"end\":10908,\"start\":10036},{\"end\":11242,\"start\":10910},{\"end\":11829,\"start\":11282},{\"end\":11906,\"start\":11831},{\"end\":12349,\"start\":11908},{\"end\":12497,\"start\":12351},{\"end\":12602,\"start\":12510},{\"end\":12818,\"start\":12604},{\"end\":13047,\"start\":12820},{\"end\":13518,\"start\":13063},{\"end\":14292,\"start\":13520},{\"end\":14836,\"start\":14330},{\"end\":17030,\"start\":14838},{\"end\":17497,\"start\":17032},{\"end\":18065,\"start\":17499},{\"end\":18508,\"start\":18067},{\"end\":19253,\"start\":18528},{\"end\":19397,\"start\":19255},{\"end\":19536,\"start\":19399},{\"end\":20073,\"start\":19538},{\"end\":20503,\"start\":20075},{\"end\":20788,\"start\":20505},{\"end\":21178,\"start\":20790},{\"end\":22033,\"start\":21221},{\"end\":22627,\"start\":22096},{\"end\":23645,\"start\":22629},{\"end\":24120,\"start\":23647},{\"end\":24354,\"start\":24159},{\"end\":25103,\"start\":24356},{\"end\":25762,\"start\":25144},{\"end\":26540,\"start\":25764},{\"end\":26629,\"start\":26542},{\"end\":26750,\"start\":26631},{\"end\":27891,\"start\":26752},{\"end\":29071,\"start\":27893},{\"end\":29589,\"start\":29096},{\"end\":30458,\"start\":29591},{\"end\":31133,\"start\":30479},{\"end\":31978,\"start\":31135},{\"end\":32547,\"start\":31980},{\"end\":33054,\"start\":32568},{\"end\":33377,\"start\":33088},{\"end\":33691,\"start\":33379},{\"end\":35147,\"start\":33693},{\"end\":35584,\"start\":35158},{\"end\":36484,\"start\":35586},{\"end\":37131,\"start\":36486},{\"end\":37224,\"start\":37141},{\"end\":37311,\"start\":37226},{\"end\":37776,\"start\":37347},{\"end\":38217,\"start\":37778},{\"end\":39952,\"start\":38219},{\"end\":40530,\"start\":39954},{\"end\":40711,\"start\":40587},{\"end\":40963,\"start\":40753},{\"end\":41419,\"start\":40965},{\"end\":41695,\"start\":41421},{\"end\":42544,\"start\":41712},{\"end\":42893,\"start\":42546},{\"end\":43452,\"start\":42895},{\"end\":43487,\"start\":43454},{\"end\":44728,\"start\":43502}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":22095,\"start\":22034},{\"attributes\":{\"id\":\"formula_1\"},\"end\":40586,\"start\":40531},{\"attributes\":{\"id\":\"formula_2\"},\"end\":40752,\"start\":40712}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11391,\"start\":11384},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13435,\"start\":13428},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19018,\"start\":18872},{\"end\":19651,\"start\":19644},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28341,\"start\":28327},{\"end\":28810,\"start\":28803},{\"end\":31797,\"start\":31790},{\"end\":32808,\"start\":32801},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":33657,\"start\":33650},{\"end\":34338,\"start\":34331},{\"end\":35249,\"start\":35242},{\"end\":36169,\"start\":36162},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":36636,\"start\":36629}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1471,\"start\":1459},{\"attributes\":{\"n\":\"2\"},\"end\":6102,\"start\":6081},{\"attributes\":{\"n\":\"3\"},\"end\":9318,\"start\":9303},{\"attributes\":{\"n\":\"3.1\"},\"end\":9766,\"start\":9755},{\"attributes\":{\"n\":\"3.2\"},\"end\":11280,\"start\":11245},{\"attributes\":{\"n\":\"7\"},\"end\":12508,\"start\":12500},{\"attributes\":{\"n\":\"3.3\"},\"end\":13061,\"start\":13050},{\"attributes\":{\"n\":\"3.4\"},\"end\":14328,\"start\":14295},{\"attributes\":{\"n\":\"4\"},\"end\":18526,\"start\":18511},{\"attributes\":{\"n\":\"5\"},\"end\":21210,\"start\":21181},{\"attributes\":{\"n\":\"5.1\"},\"end\":21219,\"start\":21213},{\"attributes\":{\"n\":\"5.2\"},\"end\":24133,\"start\":24123},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":24157,\"start\":24136},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":25142,\"start\":25106},{\"end\":29094,\"start\":29074},{\"end\":30477,\"start\":30461},{\"attributes\":{\"n\":\"6\"},\"end\":32566,\"start\":32550},{\"attributes\":{\"n\":\"6.1\"},\"end\":33086,\"start\":33057},{\"end\":35156,\"start\":35150},{\"end\":37139,\"start\":37134},{\"attributes\":{\"n\":\"6.2\"},\"end\":37345,\"start\":37314},{\"attributes\":{\"n\":\"7\"},\"end\":41710,\"start\":41698},{\"attributes\":{\"n\":\"8\"},\"end\":43500,\"start\":43490},{\"end\":44740,\"start\":44730},{\"end\":44841,\"start\":44831},{\"end\":45998,\"start\":45988},{\"end\":46169,\"start\":46161},{\"end\":46728,\"start\":46718},{\"end\":48061,\"start\":48052},{\"end\":48408,\"start\":48399},{\"end\":48511,\"start\":48502},{\"end\":48906,\"start\":48897},{\"end\":50488,\"start\":50479}]", "table": "[{\"end\":48050,\"start\":46977},{\"end\":48895,\"start\":48513},{\"end\":49072,\"start\":48956},{\"end\":50477,\"start\":49383}]", "figure_caption": "[{\"end\":44829,\"start\":44742},{\"end\":45048,\"start\":44843},{\"end\":45649,\"start\":45051},{\"end\":45986,\"start\":45652},{\"end\":46159,\"start\":46000},{\"end\":46716,\"start\":46171},{\"end\":46844,\"start\":46730},{\"end\":46977,\"start\":46847},{\"end\":48397,\"start\":48063},{\"end\":48500,\"start\":48410},{\"end\":48956,\"start\":48908},{\"end\":49383,\"start\":49075},{\"end\":50816,\"start\":50490}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3650,\"start\":3642},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9104,\"start\":9096},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33672,\"start\":33664},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34280,\"start\":34272},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35521,\"start\":35513},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":40981,\"start\":40973}]", "bib_author_first_name": "[{\"end\":52058,\"start\":52054},{\"end\":52071,\"start\":52066},{\"end\":52289,\"start\":52282},{\"end\":52305,\"start\":52298},{\"end\":52322,\"start\":52316},{\"end\":52839,\"start\":52834},{\"end\":52841,\"start\":52840},{\"end\":53064,\"start\":53050},{\"end\":53327,\"start\":53322},{\"end\":53338,\"start\":53334},{\"end\":53351,\"start\":53346},{\"end\":53367,\"start\":53360},{\"end\":53809,\"start\":53801},{\"end\":53824,\"start\":53817},{\"end\":54354,\"start\":54353},{\"end\":54597,\"start\":54592},{\"end\":54614,\"start\":54606},{\"end\":54628,\"start\":54622},{\"end\":54642,\"start\":54634},{\"end\":54917,\"start\":54908},{\"end\":54919,\"start\":54918},{\"end\":54933,\"start\":54928},{\"end\":54945,\"start\":54938},{\"end\":54955,\"start\":54951},{\"end\":54968,\"start\":54960},{\"end\":54970,\"start\":54969},{\"end\":55437,\"start\":55436},{\"end\":55444,\"start\":55440},{\"end\":55755,\"start\":55749},{\"end\":55767,\"start\":55761},{\"end\":55777,\"start\":55773},{\"end\":55792,\"start\":55784},{\"end\":55810,\"start\":55802},{\"end\":56002,\"start\":56001},{\"end\":56015,\"start\":56014},{\"end\":56034,\"start\":56028},{\"end\":56239,\"start\":56232},{\"end\":56256,\"start\":56249},{\"end\":56473,\"start\":56467},{\"end\":56486,\"start\":56481},{\"end\":56499,\"start\":56493},{\"end\":56511,\"start\":56505},{\"end\":56513,\"start\":56512},{\"end\":56524,\"start\":56520},{\"end\":56542,\"start\":56538},{\"end\":56937,\"start\":56936},{\"end\":57323,\"start\":57320},{\"end\":57347,\"start\":57337},{\"end\":57364,\"start\":57358},{\"end\":57382,\"start\":57375},{\"end\":57397,\"start\":57392},{\"end\":57411,\"start\":57406},{\"end\":57429,\"start\":57421},{\"end\":57444,\"start\":57439},{\"end\":57464,\"start\":57457},{\"end\":57478,\"start\":57473},{\"end\":58226,\"start\":58220},{\"end\":58240,\"start\":58232},{\"end\":58256,\"start\":58248},{\"end\":58504,\"start\":58499},{\"end\":58523,\"start\":58516},{\"end\":58534,\"start\":58533},{\"end\":58544,\"start\":58543},{\"end\":58550,\"start\":58549},{\"end\":58751,\"start\":58750},{\"end\":58758,\"start\":58757},{\"end\":58961,\"start\":58954},{\"end\":58978,\"start\":58974},{\"end\":58991,\"start\":58985},{\"end\":59497,\"start\":59492},{\"end\":59512,\"start\":59511},{\"end\":59529,\"start\":59523},{\"end\":59998,\"start\":59997},{\"end\":60012,\"start\":60006},{\"end\":60030,\"start\":60021},{\"end\":60043,\"start\":60037},{\"end\":60060,\"start\":60054},{\"end\":60074,\"start\":60070},{\"end\":60092,\"start\":60084},{\"end\":60607,\"start\":60602},{\"end\":60620,\"start\":60617},{\"end\":60951,\"start\":60946},{\"end\":60965,\"start\":60960},{\"end\":60976,\"start\":60973},{\"end\":61464,\"start\":61454},{\"end\":61488,\"start\":61481},{\"end\":61504,\"start\":61500},{\"end\":61516,\"start\":61510},{\"end\":61853,\"start\":61848},{\"end\":61866,\"start\":61862},{\"end\":61880,\"start\":61876},{\"end\":61899,\"start\":61890},{\"end\":61911,\"start\":61905},{\"end\":61927,\"start\":61920},{\"end\":61941,\"start\":61936},{\"end\":61951,\"start\":61948},{\"end\":61963,\"start\":61956},{\"end\":62325,\"start\":62320},{\"end\":62338,\"start\":62334},{\"end\":62352,\"start\":62348},{\"end\":62371,\"start\":62362},{\"end\":62383,\"start\":62377},{\"end\":62399,\"start\":62392},{\"end\":62413,\"start\":62408},{\"end\":62423,\"start\":62420},{\"end\":62433,\"start\":62428},{\"end\":62435,\"start\":62434},{\"end\":62788,\"start\":62785},{\"end\":62803,\"start\":62798},{\"end\":62818,\"start\":62813},{\"end\":63303,\"start\":63297},{\"end\":63315,\"start\":63312},{\"end\":63331,\"start\":63322},{\"end\":63345,\"start\":63341},{\"end\":63818,\"start\":63815},{\"end\":63835,\"start\":63828},{\"end\":64040,\"start\":64035},{\"end\":64061,\"start\":64054},{\"end\":64071,\"start\":64070},{\"end\":64085,\"start\":64080},{\"end\":64417,\"start\":64411},{\"end\":64432,\"start\":64427},{\"end\":64570,\"start\":64567},{\"end\":64580,\"start\":64575},{\"end\":64605,\"start\":64597},{\"end\":65027,\"start\":65024},{\"end\":65040,\"start\":65032},{\"end\":65055,\"start\":65050},{\"end\":65071,\"start\":65065},{\"end\":65083,\"start\":65078}]", "bib_author_last_name": "[{\"end\":52064,\"start\":52059},{\"end\":52077,\"start\":52072},{\"end\":52296,\"start\":52290},{\"end\":52314,\"start\":52306},{\"end\":52330,\"start\":52323},{\"end\":52848,\"start\":52842},{\"end\":53072,\"start\":53065},{\"end\":53332,\"start\":53328},{\"end\":53344,\"start\":53339},{\"end\":53358,\"start\":53352},{\"end\":53374,\"start\":53368},{\"end\":53815,\"start\":53810},{\"end\":53831,\"start\":53825},{\"end\":54362,\"start\":54355},{\"end\":54369,\"start\":54364},{\"end\":54604,\"start\":54598},{\"end\":54620,\"start\":54615},{\"end\":54632,\"start\":54629},{\"end\":54652,\"start\":54643},{\"end\":54926,\"start\":54920},{\"end\":54936,\"start\":54934},{\"end\":54949,\"start\":54946},{\"end\":54958,\"start\":54956},{\"end\":54976,\"start\":54971},{\"end\":55450,\"start\":55445},{\"end\":55759,\"start\":55756},{\"end\":55771,\"start\":55768},{\"end\":55782,\"start\":55778},{\"end\":55800,\"start\":55793},{\"end\":55816,\"start\":55811},{\"end\":56012,\"start\":56003},{\"end\":56026,\"start\":56016},{\"end\":56040,\"start\":56035},{\"end\":56247,\"start\":56240},{\"end\":56262,\"start\":56257},{\"end\":56479,\"start\":56474},{\"end\":56491,\"start\":56487},{\"end\":56503,\"start\":56500},{\"end\":56518,\"start\":56514},{\"end\":56536,\"start\":56525},{\"end\":56547,\"start\":56543},{\"end\":56923,\"start\":56912},{\"end\":56929,\"start\":56925},{\"end\":56934,\"start\":56931},{\"end\":56940,\"start\":56938},{\"end\":56948,\"start\":56942},{\"end\":57335,\"start\":57324},{\"end\":57356,\"start\":57348},{\"end\":57373,\"start\":57365},{\"end\":57390,\"start\":57383},{\"end\":57404,\"start\":57398},{\"end\":57419,\"start\":57412},{\"end\":57437,\"start\":57430},{\"end\":57455,\"start\":57445},{\"end\":57471,\"start\":57465},{\"end\":57485,\"start\":57479},{\"end\":58230,\"start\":58227},{\"end\":58246,\"start\":58241},{\"end\":58266,\"start\":58257},{\"end\":58510,\"start\":58505},{\"end\":58514,\"start\":58512},{\"end\":58531,\"start\":58524},{\"end\":58541,\"start\":58535},{\"end\":58547,\"start\":58545},{\"end\":58555,\"start\":58551},{\"end\":58564,\"start\":58557},{\"end\":58755,\"start\":58752},{\"end\":58764,\"start\":58759},{\"end\":58972,\"start\":58962},{\"end\":58983,\"start\":58979},{\"end\":59000,\"start\":58992},{\"end\":59509,\"start\":59498},{\"end\":59521,\"start\":59513},{\"end\":59535,\"start\":59530},{\"end\":59540,\"start\":59537},{\"end\":60004,\"start\":59999},{\"end\":60019,\"start\":60013},{\"end\":60035,\"start\":60031},{\"end\":60052,\"start\":60044},{\"end\":60068,\"start\":60061},{\"end\":60082,\"start\":60075},{\"end\":60097,\"start\":60093},{\"end\":60102,\"start\":60099},{\"end\":60615,\"start\":60608},{\"end\":60632,\"start\":60621},{\"end\":60958,\"start\":60952},{\"end\":60971,\"start\":60966},{\"end\":60984,\"start\":60977},{\"end\":61479,\"start\":61465},{\"end\":61498,\"start\":61489},{\"end\":61508,\"start\":61505},{\"end\":61525,\"start\":61517},{\"end\":61532,\"start\":61527},{\"end\":61860,\"start\":61854},{\"end\":61874,\"start\":61867},{\"end\":61888,\"start\":61881},{\"end\":61903,\"start\":61900},{\"end\":61918,\"start\":61912},{\"end\":61934,\"start\":61928},{\"end\":61946,\"start\":61942},{\"end\":61954,\"start\":61952},{\"end\":61967,\"start\":61964},{\"end\":62332,\"start\":62326},{\"end\":62346,\"start\":62339},{\"end\":62360,\"start\":62353},{\"end\":62375,\"start\":62372},{\"end\":62390,\"start\":62384},{\"end\":62406,\"start\":62400},{\"end\":62418,\"start\":62414},{\"end\":62426,\"start\":62424},{\"end\":62439,\"start\":62436},{\"end\":62796,\"start\":62789},{\"end\":62811,\"start\":62804},{\"end\":62823,\"start\":62819},{\"end\":63310,\"start\":63304},{\"end\":63320,\"start\":63316},{\"end\":63339,\"start\":63332},{\"end\":63353,\"start\":63346},{\"end\":63826,\"start\":63819},{\"end\":63842,\"start\":63836},{\"end\":64052,\"start\":64041},{\"end\":64068,\"start\":64062},{\"end\":64078,\"start\":64072},{\"end\":64094,\"start\":64086},{\"end\":64100,\"start\":64096},{\"end\":64425,\"start\":64418},{\"end\":64440,\"start\":64433},{\"end\":64573,\"start\":64571},{\"end\":64595,\"start\":64581},{\"end\":64613,\"start\":64606},{\"end\":65030,\"start\":65028},{\"end\":65048,\"start\":65041},{\"end\":65063,\"start\":65056},{\"end\":65076,\"start\":65072},{\"end\":65098,\"start\":65084}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":52219,\"start\":51985},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52013214},\"end\":52832,\"start\":52221},{\"attributes\":{\"id\":\"b2\"},\"end\":52986,\"start\":52834},{\"attributes\":{\"doi\":\"10.18637/jss.v080.i01\",\"id\":\"b3\",\"matched_paper_id\":73586718},\"end\":53269,\"start\":52988},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3618568},\"end\":53743,\"start\":53271},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1046\",\"id\":\"b5\",\"matched_paper_id\":1499080},\"end\":54341,\"start\":53745},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":216804551},\"end\":54508,\"start\":54343},{\"attributes\":{\"id\":\"b7\"},\"end\":54807,\"start\":54510},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":174799390},\"end\":55410,\"start\":54809},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":148132585},\"end\":55692,\"start\":55412},{\"attributes\":{\"id\":\"b10\"},\"end\":55953,\"start\":55694},{\"attributes\":{\"id\":\"b11\"},\"end\":56142,\"start\":55955},{\"attributes\":{\"id\":\"b12\"},\"end\":56394,\"start\":56144},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":198229624},\"end\":56850,\"start\":56396},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":215737187},\"end\":57318,\"start\":56852},{\"attributes\":{\"id\":\"b15\"},\"end\":58147,\"start\":57320},{\"attributes\":{\"doi\":\"1906.00300\",\"id\":\"b16\"},\"end\":58447,\"start\":58149},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5301683},\"end\":58704,\"start\":58449},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10714285},\"end\":58859,\"start\":58706},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":12203802},\"end\":59408,\"start\":58861},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":18400586},\"end\":59941,\"start\":59410},{\"attributes\":{\"doi\":\"abs/2004.14373\",\"id\":\"b21\",\"matched_paper_id\":216641852},\"end\":60548,\"start\":59943},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207933446},\"end\":60868,\"start\":60550},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12251117},\"end\":61367,\"start\":60870},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":41479182},\"end\":61763,\"start\":61369},{\"attributes\":{\"id\":\"b25\"},\"end\":62235,\"start\":61765},{\"attributes\":{\"doi\":\"abs/1910.10683\",\"id\":\"b26\",\"matched_paper_id\":204838007},\"end\":62700,\"start\":62237},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52009015},\"end\":63196,\"start\":62702},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":51876975},\"end\":63773,\"start\":63198},{\"attributes\":{\"id\":\"b29\"},\"end\":63983,\"start\":63775},{\"attributes\":{\"doi\":\"10.1016/j.ipm.2007.01.010\",\"id\":\"b30\",\"matched_paper_id\":9497619},\"end\":64320,\"start\":63985},{\"attributes\":{\"id\":\"b31\"},\"end\":64565,\"start\":64322},{\"attributes\":{\"id\":\"b32\"},\"end\":64954,\"start\":64567},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2177849},\"end\":65393,\"start\":64956}]", "bib_title": "[{\"end\":52280,\"start\":52221},{\"end\":53048,\"start\":52988},{\"end\":53320,\"start\":53271},{\"end\":53799,\"start\":53745},{\"end\":54351,\"start\":54343},{\"end\":54906,\"start\":54809},{\"end\":55434,\"start\":55412},{\"end\":56465,\"start\":56396},{\"end\":56910,\"start\":56852},{\"end\":58497,\"start\":58449},{\"end\":58748,\"start\":58706},{\"end\":58952,\"start\":58861},{\"end\":59490,\"start\":59410},{\"end\":59995,\"start\":59943},{\"end\":60600,\"start\":60550},{\"end\":60944,\"start\":60870},{\"end\":61452,\"start\":61369},{\"end\":62318,\"start\":62237},{\"end\":62783,\"start\":62702},{\"end\":63295,\"start\":63198},{\"end\":64033,\"start\":63985},{\"end\":65022,\"start\":64956}]", "bib_author": "[{\"end\":52066,\"start\":52054},{\"end\":52079,\"start\":52066},{\"end\":52298,\"start\":52282},{\"end\":52316,\"start\":52298},{\"end\":52332,\"start\":52316},{\"end\":52850,\"start\":52834},{\"end\":53074,\"start\":53050},{\"end\":53334,\"start\":53322},{\"end\":53346,\"start\":53334},{\"end\":53360,\"start\":53346},{\"end\":53376,\"start\":53360},{\"end\":53817,\"start\":53801},{\"end\":53833,\"start\":53817},{\"end\":54364,\"start\":54353},{\"end\":54371,\"start\":54364},{\"end\":54606,\"start\":54592},{\"end\":54622,\"start\":54606},{\"end\":54634,\"start\":54622},{\"end\":54654,\"start\":54634},{\"end\":54928,\"start\":54908},{\"end\":54938,\"start\":54928},{\"end\":54951,\"start\":54938},{\"end\":54960,\"start\":54951},{\"end\":54978,\"start\":54960},{\"end\":55440,\"start\":55436},{\"end\":55452,\"start\":55440},{\"end\":55761,\"start\":55749},{\"end\":55773,\"start\":55761},{\"end\":55784,\"start\":55773},{\"end\":55802,\"start\":55784},{\"end\":55818,\"start\":55802},{\"end\":56014,\"start\":56001},{\"end\":56028,\"start\":56014},{\"end\":56042,\"start\":56028},{\"end\":56249,\"start\":56232},{\"end\":56264,\"start\":56249},{\"end\":56481,\"start\":56467},{\"end\":56493,\"start\":56481},{\"end\":56505,\"start\":56493},{\"end\":56520,\"start\":56505},{\"end\":56538,\"start\":56520},{\"end\":56549,\"start\":56538},{\"end\":56925,\"start\":56912},{\"end\":56931,\"start\":56925},{\"end\":56936,\"start\":56931},{\"end\":56942,\"start\":56936},{\"end\":56950,\"start\":56942},{\"end\":57337,\"start\":57320},{\"end\":57358,\"start\":57337},{\"end\":57375,\"start\":57358},{\"end\":57392,\"start\":57375},{\"end\":57406,\"start\":57392},{\"end\":57421,\"start\":57406},{\"end\":57439,\"start\":57421},{\"end\":57457,\"start\":57439},{\"end\":57473,\"start\":57457},{\"end\":57487,\"start\":57473},{\"end\":58232,\"start\":58220},{\"end\":58248,\"start\":58232},{\"end\":58268,\"start\":58248},{\"end\":58512,\"start\":58499},{\"end\":58516,\"start\":58512},{\"end\":58533,\"start\":58516},{\"end\":58543,\"start\":58533},{\"end\":58549,\"start\":58543},{\"end\":58557,\"start\":58549},{\"end\":58566,\"start\":58557},{\"end\":58757,\"start\":58750},{\"end\":58766,\"start\":58757},{\"end\":58974,\"start\":58954},{\"end\":58985,\"start\":58974},{\"end\":59002,\"start\":58985},{\"end\":59511,\"start\":59492},{\"end\":59523,\"start\":59511},{\"end\":59537,\"start\":59523},{\"end\":59542,\"start\":59537},{\"end\":60006,\"start\":59997},{\"end\":60021,\"start\":60006},{\"end\":60037,\"start\":60021},{\"end\":60054,\"start\":60037},{\"end\":60070,\"start\":60054},{\"end\":60084,\"start\":60070},{\"end\":60099,\"start\":60084},{\"end\":60104,\"start\":60099},{\"end\":60617,\"start\":60602},{\"end\":60634,\"start\":60617},{\"end\":60960,\"start\":60946},{\"end\":60973,\"start\":60960},{\"end\":60986,\"start\":60973},{\"end\":61481,\"start\":61454},{\"end\":61500,\"start\":61481},{\"end\":61510,\"start\":61500},{\"end\":61527,\"start\":61510},{\"end\":61534,\"start\":61527},{\"end\":61862,\"start\":61848},{\"end\":61876,\"start\":61862},{\"end\":61890,\"start\":61876},{\"end\":61905,\"start\":61890},{\"end\":61920,\"start\":61905},{\"end\":61936,\"start\":61920},{\"end\":61948,\"start\":61936},{\"end\":61956,\"start\":61948},{\"end\":61969,\"start\":61956},{\"end\":62334,\"start\":62320},{\"end\":62348,\"start\":62334},{\"end\":62362,\"start\":62348},{\"end\":62377,\"start\":62362},{\"end\":62392,\"start\":62377},{\"end\":62408,\"start\":62392},{\"end\":62420,\"start\":62408},{\"end\":62428,\"start\":62420},{\"end\":62441,\"start\":62428},{\"end\":62798,\"start\":62785},{\"end\":62813,\"start\":62798},{\"end\":62825,\"start\":62813},{\"end\":63312,\"start\":63297},{\"end\":63322,\"start\":63312},{\"end\":63341,\"start\":63322},{\"end\":63355,\"start\":63341},{\"end\":63828,\"start\":63815},{\"end\":63844,\"start\":63828},{\"end\":64054,\"start\":64035},{\"end\":64070,\"start\":64054},{\"end\":64080,\"start\":64070},{\"end\":64096,\"start\":64080},{\"end\":64102,\"start\":64096},{\"end\":64427,\"start\":64411},{\"end\":64442,\"start\":64427},{\"end\":64575,\"start\":64567},{\"end\":64597,\"start\":64575},{\"end\":64615,\"start\":64597},{\"end\":65032,\"start\":65024},{\"end\":65050,\"start\":65032},{\"end\":65065,\"start\":65050},{\"end\":65078,\"start\":65065},{\"end\":65100,\"start\":65078}]", "bib_venue": "[{\"end\":52052,\"start\":51985},{\"end\":52409,\"start\":52332},{\"end\":52876,\"start\":52850},{\"end\":53126,\"start\":53095},{\"end\":53462,\"start\":53376},{\"end\":53940,\"start\":53853},{\"end\":54420,\"start\":54371},{\"end\":54590,\"start\":54510},{\"end\":55064,\"start\":54978},{\"end\":55463,\"start\":55452},{\"end\":55747,\"start\":55694},{\"end\":55999,\"start\":55955},{\"end\":56230,\"start\":56144},{\"end\":56610,\"start\":56549},{\"end\":57031,\"start\":56950},{\"end\":57611,\"start\":57487},{\"end\":58218,\"start\":58149},{\"end\":58570,\"start\":58566},{\"end\":58774,\"start\":58766},{\"end\":59083,\"start\":59002},{\"end\":59628,\"start\":59542},{\"end\":60199,\"start\":60118},{\"end\":60695,\"start\":60634},{\"end\":61072,\"start\":60986},{\"end\":61557,\"start\":61534},{\"end\":61846,\"start\":61765},{\"end\":62460,\"start\":62455},{\"end\":62906,\"start\":62825},{\"end\":63441,\"start\":63355},{\"end\":63813,\"start\":63775},{\"end\":64147,\"start\":64127},{\"end\":64409,\"start\":64322},{\"end\":64745,\"start\":64615},{\"end\":65161,\"start\":65100},{\"end\":52498,\"start\":52411},{\"end\":53535,\"start\":53464},{\"end\":54029,\"start\":53942},{\"end\":55137,\"start\":55066},{\"end\":55503,\"start\":55495},{\"end\":57099,\"start\":57033},{\"end\":57728,\"start\":57718},{\"end\":59151,\"start\":59085},{\"end\":59701,\"start\":59630},{\"end\":60267,\"start\":60201},{\"end\":61145,\"start\":61074},{\"end\":62974,\"start\":62908},{\"end\":63514,\"start\":63443}]"}}}, "year": 2023, "month": 12, "day": 17}