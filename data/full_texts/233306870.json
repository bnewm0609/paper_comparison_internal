{"id": 233306870, "updated": "2023-10-06 04:47:29.676", "metadata": {"title": "Gradient Matching for Domain Generalization", "authors": "[{\"first\":\"Yuge\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Jeffrey\",\"last\":\"Seely\",\"middle\":[]},{\"first\":\"Philip\",\"last\":\"Torr\",\"middle\":[\"H.S.\"]},{\"first\":\"N.\",\"last\":\"Siddharth\",\"middle\":[]},{\"first\":\"Awni\",\"last\":\"Hannun\",\"middle\":[]},{\"first\":\"Nicolas\",\"last\":\"Usunier\",\"middle\":[]},{\"first\":\"Gabriel\",\"last\":\"Synnaeve\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 4, "day": 20}, "abstract": "Machine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an inter-domain gradient matching objective that targets domain generalization by maximizing the inner product between gradients from different domains. Since direct optimization of the gradient inner product can be computationally prohibitive -- requires computation of second-order derivatives -- we derive a simpler first-order algorithm named Fish that approximates its optimization. We demonstrate the efficacy of Fish on 6 datasets from the Wilds benchmark, which captures distribution shift across a diverse range of modalities. Our method produces competitive results on these datasets and surpasses all baselines on 4 of them. We perform experiments on both the Wilds benchmark, which captures distribution shift in the real world, as well as datasets in DomainBed benchmark that focuses more on synthetic-to-real transfer. Our method produces competitive results on both benchmarks, demonstrating its effectiveness across a wide range of domain generalization tasks.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2104.09937", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/ShiSTNHUS22", "doi": null}}, "content": {"source": {"pdf_hash": "b7a4be5a703f251b2d341d30ccec5e201881981b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.09937v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6c1dfffa01c2159e18c25cb81a314849d8845ccb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b7a4be5a703f251b2d341d30ccec5e201881981b.txt", "contents": "\nGradient Matching for Domain Generalization\n\n\nYuge Shi yshi@robots.ox.ac.uk \nFacebook Reality Labs\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity of Oxford\nUniversity of Oxford\nThe University of Edinburgh\nThe Alan Turing Institute\n\n\nJeffrey Seely jseely@fb.com \nFacebook Reality Labs\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity of Oxford\nUniversity of Oxford\nThe University of Edinburgh\nThe Alan Turing Institute\n\n\nPhilip H S Torr philip.torr@eng.ox.ac.uk \nFacebook Reality Labs\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity of Oxford\nUniversity of Oxford\nThe University of Edinburgh\nThe Alan Turing Institute\n\n\nN Siddharth n.siddharth@ed.ac.uk \nFacebook Reality Labs\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity of Oxford\nUniversity of Oxford\nThe University of Edinburgh\nThe Alan Turing Institute\n\n\nAwni Hannun \nFacebook Reality Labs\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity of Oxford\nUniversity of Oxford\nThe University of Edinburgh\nThe Alan Turing Institute\n\n\nNicolas Usunier usunier@fb.com \nFacebook Reality Labs\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity of Oxford\nUniversity of Oxford\nThe University of Edinburgh\nThe Alan Turing Institute\n\n\nGabriel Synnaeve \nFacebook Reality Labs\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nUniversity of Oxford\nUniversity of Oxford\nThe University of Edinburgh\nThe Alan Turing Institute\n\n\nGradient Matching for Domain Generalization\nCode is available at https://github.com/YugeTen/fish.\nMachine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an inter-domain gradient matching objective that targets domain generalization by maximizing the inner product between gradients from different domains. Since direct optimization of the gradient inner product can be computationally prohibitive -it requires computation of second-order derivatives --we derive a simpler first-order algorithm named Fish that approximates its optimization. We perform experiments on the WILDS benchmark, which captures distribution shift in the real world, as well as the DOMAINBED benchmark that focuses more on syntheticto-real transfer. Our method produces competitive results on both benchmarks, demonstrating its effectiveness across a wide range of domain generalization tasks.\n\nIntroduction\n\nThe goal of domain generalization is to train models that performs well on unseen, out-of-distribution data, which is crucial in practice for model deployment in the wild. This seemingly difficult task is made possible by the presence of multiple distributions/domains at train time. As we have seen in past work (Arjovsky et al., 2019, Gulrajani and Lopez-Paz, 2020, Ganin et al., 2016, a key aspect of domain generalization is to learn from features that remain invariant across multiple domains, while ignoring those that are spuriously correlated to label information (as defined in Torralba and Efros (2011), Stock and Cisse (2017)). Consider, for example, a model that is built to distinguish between cows and camels using photos collected in nature under different climates. Since CNNs are known to have a bias towards texture (Geirhos et al., 2018, Brendel andBethge, 2019), if we simply try to minimize the average loss across different domains, the classifier is prone to spuriously correlate \"cow\" with grass and \"camels\" with desert, and predict the species using only the background. Such a classifier can be rendered useless when the animals are placed indoors or in a zoo. However, if the model could recognize that while the landscapes change with climate, the biological characteristics of the animals (e.g. humps, neck lengths) remain invariant and use those features to determine the species, we have a much better chance at generalizing to unseen domains.\n\nSimilar intuitions have already motivated several approaches that consider learning \"invariances\" accross domains as the main challenge of domain generalization. Most of these work focuses on learning invariant features, for instance domain adversarial neural networks (Ganin et al., 2016), CORAL (Sun and Saenko, 2016) and MMD for domain generalization (Li et al., 2018b). Different from previous approaches, invariant risk minimization (Arjovsky et al., 2019) proposes to learn intermediate features such that we have invariant predictor (when optimal) across different domains.\n\nIn this paper, we propose an inter-domain gradient matching (IDGM) objective. While our method also follows the invariance assumption, we are interested in learning a model with invariant gradient direction for different domains. Our IDGM objective augments the loss with an auxiliary term that maximizes the gradient inner product between domains, which encourages the alignment between the domain-specific gradients. By simultaneously minimizing the loss and matching the gradients, IDGM encourages the optimization paths to be the same for all domains, favouring invariant predictions. See Figure 1 for a visualization: given 2 domains, each containing one invariant feature (orange cross) and one spurious feature (yellow and red cross). While empirical risk minimisation (ERM) minimizes the average loss between these domains at the cost of learning spurious features only, IDGM maximizes the gradient inner product and is therefore able to focus on the invariant feature. Note that this plot is generated from an example, which we will describe in more details in Section 3.2.\n\nWhile the IDGM objective achieves the desirable learning dynamic in theory, naive optimization of the objective by gradient descent is computationally costly due to the second-order derivatives. Leveraging the theoretical analysis of Reptile, a meta-learning algorithm (Nichol et al., 2018), we propose to approximate the gradients of IDGM using a simple first-order algorithm, which we name Fish. Fish is simple to implement, computationally effective and as we show in our experiments, functionally similar to direct optimization of IDGM.\n\nOur contribution is a simple but effective training algorithm for domain generalization, which exhibits state-of-the-art performance on 13 datasets from recent domain generalization benchmark WILDS (Koh et al., 2020) and DOMAINBED. The strong performance of our method on a wide variety of datasets demonstrates that it is broadly applicable in different applications and subgenres of domain generalization problems. We also perform experiments to verify that our algorithm Fish does improve the normalized inter-domain gradient inner product, while this inner product decreases throughout training for ERM baseline.\n\n\nRelated Work\n\nDomain Generalization In domain generalization, the training data is sampled from one or many source domains, while the test data is sampled from a new target domain. In contrast to domain adaptation, the learner does not have access to any data from the target domain (labeled or unlabeled) during train time (Quionero-Candela et al., 2009). In this paper we are interested in the scenario where multiple source domains are available, and the domain where the data comes from is known. Further, Koh et al. (2020) defines the datasets where train and test has disjoint domains \"domain generalization\", and those where domains overlap between splits (but typically have different distributions) \"subpopulation shift\". Following e.g., Gulrajani and Lopez-Paz (2020), in this paper we use domain generalization in a broader sense that encompasses the two categories.\n\nWe will now discuss the three main families of approaches to domain generalization:\n\n1. Distributional Robustness (DRO): DRO approaches minimize the worst-case loss over a set of data distributions constructed from the training domains. Rojas-Carulla et al. (2015) proposed DRO to address covariate shift (Gretton et al., 2009a,b), where P (Y |X) remains constant across domains but P (X) changes. Later work also studied subpopulation shift, where the train and test distributions are mixtures of the same domains, but the mixture weights change between train and test ; 2. Domain-invariant representation learning: This family of approaches to domain generalization aims at learning high-level features that make domains statistically indistinguishable. Prediction is then based on these features only. The principle is motivated by a generalization error bound for unsupervised domain adaptation (Ben-David et al., 2010, Ganin et al., 2016, but the approach readily applies to domain generalization Lopez-Paz, 2020, Koh et al., 2020). Algorithms include penalising the domain-predictive power of the model (Ganin et al., 2016, Huang et al., 2020, matching mean and variance of feature distributions across domains (Sun and Saenko, 2016), learning useful representations by solving Jigsaw puzzles (Carlucci et al., 2019) or using the maximum mean discrepancy (Gretton et al., 2006) to match the feature distributions (Li et al., 2018b). Similar to our approach, Koyama and Yamaguchi (2021) proposes IGA, which also adopts a gradient-alignment approach for domain generalization. The key difference between IGA and our IDGM objective is that IGA learns invariant features by minimizing the variance of inter-domain gradients. Notably, IGA is completely identical to ERM when ERM is the optimal solution on every training domain, since the variances of the gradients will be zero. While they achieve the best performance on the training set, both IGA and ERM could completely fail when generalizing to unseen domains (see Section 3.2 for such an example). Our method, on the contrary, biases towards non-ERM solutions as long as the gradients are aligned, and is therefore able to avoid this issue. Additionally, in Lopez-Paz and Ranzato (2017) we also see the application of gradient-alignment, however in this case it is applied under the continual learning setting to determine whether a gradient update will increase the loss of the previous tasks.\n\n3. Invariant Risk Minimization (IRM): IRM is proposed by Arjovsky et al. (2019), which learns an intermediate representation such that the optimal classifiers (on top of this representation) of all domains are the same. The motivation is to exploit invariant causal effects between domains while reducing the effect of domain-specific spurious correlations.\n\nApart from these algorithms that are tailored for domain generalization, a well-studied baseline in this area is ERM, which simply minimizes the average loss over training domains. Using vanilla ERM is theoretically unfounded (Hashimoto et al., 2018, Blodgett et al., 2016, Tatman, 2017 since ERM is guaranteed to work only when train and test distributions match. Nonetheless, recent benchmarks suggest that ERM obtains strong performance in practice, in many case surpassing domain generalization algorithms Lopez-Paz, 2020, Koh et al., 2020). Our goal is to fill this gap, using an algorithm significantly simpler than previous approaches.\n\nConnections to meta-learning There are close connections between meta-learning (Thrun and Pratt, 1998) Andrychowicz et al. (2016). The key idea is to backpropagate through gradient descent itself to learn representations that can be easily adapted to unseen tasks. Our algorithmic solution is inspired by Reptile, a first-order approximation to MAML. However, our method has a fundamentally different goal, which is to exploit input-output correspondences that are invariant across domains. In contrast, meta-learning algorithms such as Reptile extract knowledge (e.g., input representations) that are useful to different tasks, but nothing has to be invariant across all tasks.\n\nWhile our motivation is the exploitation of invariant features, similarly to IRM, our inter-domain gradient matching principle is an alternative to specifying hard constraints on the desired invariants across tasks. The resulting algorithm is both simple and efficient as it is a combination of standard gradient computations and parameter updates.\n\n\nMethodology\n\n\nGoals\n\nConsider a training dataset D tr consisting of S domains D tr = {D 1 , \u00b7 \u00b7 \u00b7 , D S }, where each domain s is characterized by a dataset D s := {(x s i , y s i )} ns i=1 containing data drawn i.i.d. from some probability distribution. Also consider a test dataset D te consisting of T domains D te = {D S+1 , \u00b7 \u00b7 \u00b7 , D S+T }, where D tr \u2229 D te = \u2205. The goal of domain generalization is to train a model with weights \u03b8 that generalizes well on the test dataset D te such that:\narg min \u03b8 E D\u223cDte E (x,y)\u223cD [l((x, y); \u03b8)] ,(1)\nwhere l((x, y); \u03b8) is the loss of model \u03b8 evaluated on (x, y).\n\nA naive approach is to emply ERM, which simply minimizes the average loss on D tr , ignoring the discrepancy between train and test domains:\nL erm (D tr ; \u03b8) = E D\u223cDtr E (x,y)\u223cD [l((x, y); \u03b8)] .(2)\nThe ERM objective clearly does not exploit the input-output invariance across different domains in D tr and could perform arbitrarily poorly on test data. We demonstrate this with a simple linear example as described in the next section. \n\n\nThe pitfall of ERM: a linear example\n\nConsider a binary classification setup where data (x, y) \u2208 B 4 \u00d7 B, and a data instance is denoted x = [f 1 , f 2 , f 3 , f 4 ], y. Training data spans two domains {D 1 , D 2 }, and test data one D 3 . The goal is to learn a linear model W x + b = y, W \u2208 R 4 , b \u2208 R on the train data, such that the error on test data is minimized. The setup and dataset of this example is illustrated in Figure 2.\n\nAs we can see in Figure 2, f 1 is the invariant feature in this dataset, since the correlation between f 1 and y is stable across different domains. The relationships between y and f 2 , f 3 and f 4 changes for D 1 , D 2 , D 3 , making them the spurious features. Importantly, if we consider one domain only, the spurious feature is a more accurate indicator of the label than the invariant feature. For instance, using f 2 to predict y can give 97% accuracy on D 1 , while using f 1 only achieves 93% accuracy. However, predicting with f 2 on D 2 and D 3 can at most reach 57% accuracy, while f 1 's accuracy remains 93% regardless of the domain. The performance of ERM on this simple example is shown in Table 1 (first row). From the trained parameters W and b, we see that the model places most of its weights on spurious features f 2 and f 3 . While this achieves the highest train accuracy (97%), the model cannot generalize to unseen domains and performs poorly on test accuracy (57%).\n\n\nInter-domain Gradient Matching (IDGM)\n\nTo mitigate the problem with ERM, we need an objective that learns from features that are invariant across domains. Let us consider the case where the train dataset consists of S = 2 domains D tr = {D 1 , D 2 }. Given model \u03b8 and loss function l, the expected gradients for data in the two domains is expressed as\nG 1 = E D1 \u2202l((x, y); \u03b8) \u2202\u03b8 , G 2 = E D2 \u2202l((x, y); \u03b8) \u2202\u03b8 .(3)\nThe direction, and by extension, inner product of these gradients are of particular importance to our goal of learning invariant features. If G 1 and G 2 point in a similar direction, i.e. G 1 \u00b7 G 2 > 0, taking a gradient step along G 1 or G 2 improves the model's performance on both domains, indicating that the features learned by either gradient step are invariant across {D 1 , D 2 }. This invariance cannot be guaranteed if G 1 and G 2 are pointing in opposite directions, i.e. G 1 \u00b7 G 2 \u2264 0.\n\nTo exploit this observation, we propose to maximize the gradient inner product (GIP) to align the gradient direction across domains. The intended effect is to find weights such that the input-output correspondence is as close as possible across domains. We name our objective inter-domain gradient matching (IDGM), and it is formed by subtracting the inner product of gradients between domains G from the original ERM objective. For the general case where S \u2265 2, we can write\nL idgm = L erm (D tr ; \u03b8) \u2212 \u03b3 2 S(S \u2212 1) i =j i,j\u2208S G i \u00b7 G j GIP, denote as G ,(4)\nwhere \u03b3 is the scaling term for G. Note that GIP can be computed in linear time as G = || i G i || 2 \u2212 i ||G i || 2 (ignoring the constant factor). We can also compute the stochastic estimates of Equation (4) by replacing out the expectations over the entire dataset by minibatches.\n\nWe test this objective on our simple linear dataset, and report results in the second row of Table 1. Note that to avoid exploding gradient we use the normalized GIP during training. The model has lower training accuracy compared to ERM (93%), however its accuracy remains the same on the test set, much higher than ERM. The trained weights W reveal that the model assigns the largest weight to the invariant feature f 1 , which is desirable. The visualization in Figure 1 also confirms that by maximizing the gradient inner product, IDGM is able to focus on the feature that is common between domains, yielding better generalization performance than ERM.\n\n\nOptimizing IDGM with Fish\n\nThe proposed IDGM objective, although effective, requires computing the second-order derivative of the model's parameters due to the gradient inner product term, which can be computationally prohibitive. To mitigate this, we propose a first-order algorithm named Fish 2 that approximates the optimization of IDGM with inner-loop updates. In Algorithm 1 we present Fish. As a comparison, we also present direct optimization of IDGM using SGD in Algorithm 2.\nAlgorithm 1 Fish. 1: for iterations = 1, 2, \u00b7 \u00b7 \u00b7 do 2: \u03b8 \u2190 \u03b8 3: for D i \u2208 permute({D 1 , D 2 , \u00b7 \u00b7 \u00b7 , D S }) do 4: Sample batch d i \u223c D i 5: g i = E di \u2202l((x, y); \u03b8) \u2202 \u03b8 / /Grad wrt \u03b8 6: Update \u03b8 \u2190 \u03b8 \u2212 \u03b1 g i 7:\nend for 8:\n\n\n9:\n\nUpdate \u03b8 \u2190 \u03b8 + ( \u03b8 \u2212 \u03b8) 10: end for Algorithm 2 Direct optimization of IDGM.\n\n1: for iterations = 1, 2, \u00b7 \u00b7 \u00b7 do 2:\n\u03b8 \u2190 \u03b8 3: for D i \u2208 permute({D 1 , D 2 , \u00b7 \u00b7 \u00b7 , D S }) do 4: Sample batch d i \u223c D i 5: g i = E di \u2202l((x, y); \u03b8) \u2202\u03b8 / /Grad wrt \u03b8 6: 7: end for 8:\u1e21 = 1 S S s=1 g s , g = GIP (batch) 2 S(S \u2212 1) i =j i,j\u2208S g i \u00b7 g j 9:\nUpdate \u03b8 \u2190 \u03b8 \u2212 (\u1e21 \u2212 \u03b3(\u2202 g/\u2202\u03b8)) 10: end for Fish performs S inner-loop (l3-l7) update steps with learning rate \u03b1 on a clone of the original model \u03b8, and each update uses a minibatch d i from the domain selected in step i. Subsequently, \u03b8 is updated by a weighted difference between the cloned model and the original model ( \u03b8 \u2212 \u03b8).\n\nTo see why Fish is an approximation to directly optimizing IDGM, we can perform Taylor-series expansion on its update in l8, Algorithm 1. Doing so reveals two leading terms: 1)\u1e21: averaged gradients over inner-loop's minibatches (effectively the ERM gradient); 2) \u2202 g/\u2202\u03b8: gradient of the minibatch version of GIP. Observing l8 of Algorithm 2, we see that\u1e21 and g are actually the two gradient components used in direct optimization of IDGM. Therefore, Fish implicitly optimizes IDGM by construction (up to a constant factor), avoiding the computation of second-order derivative \u2202 g/\u2202\u03b8. We present this more formally for the full gradient G in Theorem 3.1.\n\nTheorem 3.1 Given twice-differentiable model with parameters \u03b8 and objective l. Let us define the following:\nG f = E[(\u03b8 \u2212 \u03b8)] \u2212 \u03b1S \u00b7\u1e20, Fish update -\u03b1S\u00b7ERM grad G g = \u2212\u2202 G/\u2202\u03b8, grad of max \u03b8 ( G) where\u1e20 = 1 S S s=1 G s and is the full gradient of ERM. Then we have lim \u03b1\u21920 G f \u00b7 G g G f \u00b7 G g = 1.\nNote that the expectation in G f is over the sampling of domains and minibatches. Theorem 3.1 indicates that when \u03b1 is sufficiently small, if we remove the scaled ERM gradient component\u1e20 from Fish's update, we are left with a term G f that is in similar direction to the gradient of maximizing the GIP term in IDGM, which was originally second-order. Note that this approximation comes at the cost of losing direct control over the GIP scaling \u03b3 -we therefore also derived a smoothed version of Fish that recovers this scaling term, however we find that changing the value of \u03b3 does not make much difference empirically. See Appendix B for more details.\n\nThe proof to Theorem 3.1 can be found in Appendix A. We follow the analysis from Nichol et al. (2018), which proposes Reptile for model-agnostic meta-learning (MAML), where the relationship between inner-loop update and maximization of gradient inner product was first highlighted. Nichol et al. (2018) found the GIP term in their algorithm to be over minibatches from the same domain, which promoted within-task generalization; in Fish we construct inner-loop using minibatches over different domains -it therefore instead encourages across-domain generalization. We compare the two algorithms in further details in Appendix A.1.\n\nWe also train Fish on our simple linear dataset, with results in Table 1, and see it performs similarly to IDGM -the model assigns the most weight to the invariant feature f 1 , and achieves 93% accuracy on both train and test dataset. Dataset We propose a simple shape-color dataset CDSPRITES-N based on the DSPRITES dataset (Matthey et al., 2017), which contains a collection of white 2D sprites of different shapes, scales, rotations and positions. CDSPRITES-N contains N domains. The goal is to classify the shape of the sprites, and there is a shape-color deterministic matching that is specific per domain. This way we have shape as the invariant feature and color as the spurious feature. See Figure 3 for an illustration.\n\nTo construct the train split of CDSPRITES-N, we take a subset of DSPRITES that contains only 2 shapes (square and oval). We make N replicas of this subset and assign 2 (a) Train (b) Test Figure 4: Performance on CDSPRITES-N, with N \u2208 [5, 50] colors to each, with every color corresponding to one shape (e.g. yellow block in Figure 3a, pink \u2192 squares, purple \u2192 oval). For the test split, we create another replica of the DSPRITES-N subset, and randomly assign one of the 2N colors in the training set to each shape in the test set.\n\nWe design this dataset with CNN's texture bias in mind (Geirhos et al., 2018, Brendel andBethge, 2019). If the value of N is small enough, the model can simply memorize the N colors that correspond to each shape, and make predictions solely based on colors, resulting in poor performance on the test set where color and shape are no longer correlated. Compared to other simple domain generalization datasets such as Digits-5 and Office-31, our dataset allows for precise control over the features that remains stable across domains and the features that change as domains change; we can also change the number of domains N easily, making it possible to examine the effect N has on the performance for domain generalization.\n\n\nResults\n\nWe train the same model using three different objectives including Fish, dicrect optimization of IDGM and ERM on this dataset with number of domains N ranging from 5 to 50. Again, for direct optimization of IDGM, we use the normalized gradient inner product to avoid exploding gradient.\n\nWe plot the average train, test accuracy for each objective over 5 runs against the number of domains N in Figure 4. We can see that the train accuracy is always 100% for all methods regardless of N (Figure 4a), while the test performance varies: Figure 4b shows that direct optimization of IDGM (red) and Fish (blue) obtain the best performances, with the test accruacy rising to over 90% when N \u2265 10 and near 100% when N \u2265 20. The predictions of ERM (yellow), on the other hand, remain nearly random on the test set up until N = 20, and reach 95% accuracy only for N \u2265 40.\n\nThis experiment confirms the following: 1) the proposed IDGM objective have much stronger domain generalization capabilities compared to ERM; 2) Fish is an effective approximation of IDGM, with similar performance to its direct optimization. We also plot the gradient inner product progression of Fish vs. ERM during training in Figure 9a, showing clearly that Fish does improve the gradient inner product across domain while ERM does not; 3) we also observe during training that Fish is about 10 times faster than directly optimizing IDGM, demonstrating its computational efficiency.\n\n\nWILDS\n\nDatasets We evaluate our model on the WILDS benchmark (Koh et al., 2020), which contains multiple datasets that capture real-world distribution shifts across a diverse range of modalities. We report experimental results on 6 challenging datasets in WILDS, and find Fish to outperform all baselines on most tasks. A summary on the WILDS datasets can be found in Table 2. For hyperparameters including learning rate, batch size, choice of optimizer and model architecutre, we follow the exact configuration as reported in the WILDS benchmark. Importantly, we also use the same model selection strategy used in WILDS to ensure a fair comparison. See details in Appendix D.  Results See a summary of results in Table 3, where we use the metrics recommended in WILDS for each dataset. Again, following practices in WILDS, all results are reported over 3 random seed runs, apart from CAMELYON17 which is reported over 10 random seed runs. We included additional results as well as a in-depth discussion on each dataset in Appendix C. We make the following observations:\n\n1. Strong performance across datasets: Considering results on all 6 datasets, Fish is the best performing algorithm on WILDS. It outperforms all baseline on 4 datasets and achieves similar level of performance to the best method on the other 2 (AMAZON and IWILDCAM). Fish's strong performance on different types of data and architectures such as RESNET (He et al., 2016), DENSENET (Huang et al., 2017) and BERT (Devlin et al., 2018) demonstrated it's capability to generalize to a diverse variety of tasks; 2. Strong performance on different domain generalization tasks: We make special note the CIVILCOMMENTS dataset captures subpopulation shift problems, where the domains in test are a subpopulation of the domains in train, while all other WILDS datasets depicts pure domain generalization problems, where the domains in train and test are disjointed. As a result, the baseline models for CIVILCOMMENTS selected by the WILDS benchmark are different from the methods used in all other datasets, and are tailored to avoiding systematic failure on data from minority subpopulations. We see that Fish works well in this setting too without any changes or special sampling strategies (used for baselines on CIVILCOMMENTS, see more in Table 10), demonstrating it's capability to perform in different domain generalization scenarios;\n\n3. Failure mode of domain generalization algorithms: We noticed that on IWILDCAM and AMA-ZON, ERM is the best algorithm, outperforming all domain generalization algorithms except for Fish on AMAZON. We believe that these domain generalization algorithms failed due to the large number of domains in these two datasets -324 for IWILDCAM and 7,676 for AMAZON. This is a common drawback of current domain generalization literature and is a direction worth exploring.\n\n\nDOMAINBED\n\nDatasets While WILDS is a challenging benchmark capturing realistic distribution shift, to test our model under the synthetic-to-real transfer setting and provide more comparisons to SOTA methods, we also performed experiments on the DOMAINBED benchmark (Gulrajani and Lopez-Paz, 2020).  (Yan et al., 2020), MLDG (Li et al., 2018a), Coral (Sun and Saenko, 2016), MMD (Li et al., 2018b), DANN (Ganin et al., 2016) and CDANN (Li et al., 2018).\n\nResults Following recommendations in DOMAINBED, we report results using training domain as validation set for model selection. See results in Table 4, reported over 5 random trials. Averaging the performance over all 7 datasets, Fish ranks second out of 10 domain generalization methods. It performs only marginally worse than Coral (0.1%), and is one of the three methods that performs better than ERM. This showcases Fish's effectiveness on domain generalization datasets with stronger focus to synthetic-to-real transfer, which again demonstrates its versatility and robustness on different domain generalization tasks.\n\n\nAnalysis\n\nTracking gradient inner product In Figure 5, we demonstrate the progression of inter-domain gradient inner products during training using different objectives. We train both Fish (blue) and ERM (yellow) untill convergence while tracking the normalized gradient inner products between minibatches from different domains used in each inner-loop. To ensure a fair comparison, we use the exact same sequence of data for Fish and ERM (see Appendix H for more details). From Figure 5, it is clear that during training, the normalized gradient inner product of Fish increases, while that for ERM stays at the same value. The observations here shows that Fish is indeed effective in increasing/maintaining the level of inter-domain gradient inner product.\n\nWe conduct the same gradient inner product tracking experiments for the WILDS datasets we studied as well to shed some lights on its efficiency -see Appendix G for results.\n\n\nRandom Grouping\n\nWe conducted experiments where data are grouped randomly instead of by domain for the inner-loop update. By doing so, we are still maximizing the inner product between minibatches, however it no longer holds that each minibatch contain data from one domain only. We therefore expect the results to be slightly worse than Fish, and the bigger the domain gap is, the more advantage Fish has against the random grouping strategy. We show the results for random grouping (Fish, RG) in Table 5.  As expected, the random grouping strategy performs worse than Fish on all datasets. This is the most prominent on CDSPRITES with 10 domains (N=10), where Fish achieves 100% test accuracy and both random grouping and ERM predicts randomly on the test split. The experiment demonstrated that the effectiveness of our algorithm largely benefited from the domain grouping strategy, and that maximising the gradient inner product between random batches of data does not achieve the same domain generalization performance.\n\nAblation studies on hyper-parameters We provide ablation studies on the learning rate of Fish's inner-loop \u03b1, meta step , and number of inner loop steps N in Appendix F. We also study the effect of fine-tuning on pretrained models at different stage of convergence in Appendix E.\n\n\nConclusion\n\nIn this paper we presented inter-domain gradient matching (IDGM) for domain generalization. To avoid costly second-order computations, we approximated IDGM with a simple first-order algorithm, Fish. We demonstrated our algorithm's capability to learn from invariant features (as well as ERM's failure to do so) using simple datasets such as CDSPRITES-N and the linear example. We then evaluated the model's performance on WILDS and DOMAINBED, demonstrating that Fish performs well on different subgenres of domain generalization, and surpasses baseline performance on a diverse range of vision and language tasks using different architectures such as DenseNet, ResNet-50 and BERT. Our experiments can be replicated with 1500 GPU hours on NVIDIA V100.\n\nDespite its strong performance, similar to previous work on domain generalization, when the number of domains is large Fish struggles to outperform ERM. We are currently investigating approaches by which Fish can be made to scale to datasets with orders of magnitude more domains and expect to report on this improvement in our future work. \n\n\nA Taylor Expansion of Reptile and Fish Inner-loop Update\n\nIn this section we provide proof to Theorem 3.1. We reproduce and adapt the proof from Nichol et al. (2018) in the context of Fish, for completeness.\n\nWe demonstrate that when the inner-loop learning rate \u03b1 is small, the direction of G f aligns with that of G g , where\nG f = E (\u03b8 \u2212\u03b8) \u2212 \u03b1S \u00b7\u1e20,(5)G g = \u2212\u2202 G/\u2202\u03b8,(6)\nExpanding G g G g is the gradient of maximizing the gradient inner product (GIP).\nG g = \u2212 2 S(S \u2212 1) i =j i,j\u2208S \u2202 \u2202\u03b8 G i \u00b7 G j(7)\nExpanding G f To write out G f , we need to derive the gradient update of Fish, \u03b8 \u2212\u03b8. Let us first define some notations.\n\nFor each inner-loop with S steps of gradient updates, we assume a loss functions l as well as a sequence of inputs\n{d i } S i=1 , where d i := {x b , y b } B b=1\ndenotes a minibatch at step i randomly drawn from one of the available domains in {D 1 , \u00b7 \u00b7 \u00b7 , D S }. For reasons that will become clearer later, take extra note that the subscript i here denotes the index of step, rather than the index of domain. We also define the following:\ng i = E di \u2202l((x, y); \u03b8 i ) \u2202\u03b8 i (gradient at step i, wrt \u03b8 i ) (8) \u03b8 i+1 = \u03b8 i \u2212 \u03b1 g i (sequence of parameters) (9) g i = E di \u2202l((x, y); \u03b8 1 ) \u2202\u03b8 1 (gradient at step i, wrt \u03b8 1 )(10)\nH i = E di \u2202 2 l((x, y); \u03b8 1 ) \u2202\u03b8 2 1 (Hessian at initial point)\n\nIn the following analysis we omit the expectation E di and input (x, y) to l and instead denote the loss at step i as l i . Performing second-order Taylor approximation to g i yields:\ng i = l i (\u03b8 i ) (12) = l i (\u03b8 1 ) + l i (\u03b8 1 )(\u03b8 i \u2212 \u03b8 1 ) + O( \u03b8 i \u2212 \u03b8 1 2 ) =O(\u03b1 2 ) (13) = g i + H i (\u03b8 i \u2212 \u03b8 1 ) + O(\u03b1 2 ) (14) = g i \u2212 \u03b1H i i\u22121 j=1 g j + O(\u03b1 2 ).(15)\nApplying first-order Taylor approximation to g j gives us\ng j = g j + O(\u03b1),(16)\nplugging this back to Equation (15) yields:\ng i = g i \u2212 \u03b1H i i\u22121 j=1 g j + O(\u03b1 2 ).(17)\nFor simplicity reason, let us consider performing two steps in inner-loop updates, i.e. S = 2. We can then write the gradient of Fish \u03b8 \u2212\u03b8 as\n\u03b8 \u2212\u03b8 = \u03b1( g 1 + g 2 ) (18) = \u03b1(g 1 + g 2 1 ) \u2212 \u03b1 2 H 2 g 1 2 +O(\u03b1 3 ).(19)\nFurthermore, taking the expectation of \u03b8 \u2212\u03b8 under minibatch sampling gives us\n1 = E 1,2 [g 1 + g 2 ] = G 1 + G 2 2 = E 1,2 [H 2 g 1 ] = E 1,2 [H 1 g 2 ] (interchanging indices) = 1 2 E 1,2 [H 2 g 1 + H 1 g 2 ]\n(averaging last two eqs)\n= 1 2 E 1,2 \u2202(g 1 \u00b7 g 2 ) \u2202\u03b8 1 = 1 2 \u00b7 \u2202(G 1 \u00b7 G 2 ) \u2202\u03b8 1\nNote that the only reason we can interchange the indices in 2 is because the subscripts represent steps in the inner loop rather than index of domains. Plugging 1 , 2 in Equation (19) yields:\nE[\u03b8 \u2212\u03b8] = \u03b1(G 1 + G 2 ) + \u03b1 2 2 \u00b7 \u2202(G 1 \u00b7 G 2 ) \u2202\u03b8 1 + O(\u03b1 3 )(20)\nWe can also expand this to the general case where S \u2265 2:\nE[\u03b8 \u2212\u03b8] = \u03b1 S s=1 G s \u2212 \u03b1 2 S(S \u2212 1) i =j i,j\u2208S \u2202(G i \u00b7 G j ) \u2202\u03b8 1 + O(\u03b1 3 ).(21)\nThe second term in Equation (5) is\u1e20, which is the full gradient of ERM defined as follow:\nG = 1 S S s=1 G s .(22)\nPlugging Equation (21) and Equation (22) to Equation (5) yields\nG f = E[\u03b8 \u2212\u03b8] \u2212 \u03b1S\u1e20 (23) = \u2212 \u03b1 2 S(S \u2212 1) i =j i,j\u2208S \u2202 \u2202\u03b8 1 G i \u00b7 G j(24)\nComparing Equation (7) to Equation (24), we have:\nlim \u03b1\u21920 G f \u00b7 G g G f G g = 1.\n\nA.1 Fish and Reptile: Differences and Connections\n\nAs we introduced, our algorithm Fish is inspired by Reptile, a MAML algorithm.\n\nEven though meta learning and domain generalization both study N -way, K-shot problems, there are some distinct differences that set them apart. The most prominent one is that in meta learning, some examples in the test dataset will be made available at test time (K > 0), while in domain generalization no example in the test dataset is seen by the model (K = 0); another important difference is that while domain generalization aims to train models that perform well on an unseen distribution of the same task, meta-learning assumes multiple tasks and requires the model to quickly learn an unseen task using only K examples.\n\nDue to these differences, it does not make sense in general to use MAML framework in domain generalization. As it turns out however, the idea of aligning gradients to improve generalization is relevant to both methods -The fundamental difference here that MAML algorithms such as Reptile aligns the gradients between batches from the same task Nichol et al. (2018), while Fish aligns those between batches from different tasks.\n\nTo see how this is ahiceved, let us have a look at the algorithmic comparison between Fish (blue) and Reptile (green) in Algorithm 3. As we can see, the key difference between the algorithm of Fish Algorithm 3 Black fonts denote steps used in both algorithms, colored fonts are steps unique to Fish or Reptile. 1: for i = 1, 2, \u00b7 \u00b7 \u00b7 do 2:\u03b8 \u2190 \u03b8 3:\n\nSample task D t \u223c {D 1 , \u00b7 \u00b7 \u00b7 , D T } 4:\n\nfor s \u2208 {1, \u00b7 \u00b7 \u00b7 , S} or D t \u2208 {D 1 , \u00b7 \u00b7 \u00b7 , D T } do Update \u03b8 \u2190 \u03b8 + (\u03b8 \u2212 \u03b8) 10: end for and Reptile is that Reptile performs its inner-loop using minibatches from the same task, while Fish uses minibatches from different tasks (l4-8). Based on the analysis in Nichol et al. (2018) (which we reproduce in Appendix A), this is why Reptile maximizes the within-task gradient inner products and Fish maximizes the across-task gradient inner products.\n\nA natural question to ask here is -how does this affect their empirical performance? In Figure 6, we show the train and test performance of Fish (blue) and Reptile (green) on CDSPRITES-N. We can see that despite the algorithmic similarity between Fish and Reptile, the two methods behave very differently on this domain generalization task: while Fish's test accuracy goes to 100% at N = 10, Reptile's test performance is always 50% regardless of N . Moreover, we observe a dip in Reptile's training performance early on, with the accuracy plateaus at 56% when N > 20. Reptile's poor performance on this dataset is to be expected since its inner-loop is designed to encourage within-domain generalization, which is not helpful for learning what's invariant across domains.\n\n\nB SmoothFish: a more general algorithm B.1 Derivation\n\nWe conclude in Appendix A that a component of Fish's update G f = E[\u03b8 \u2212\u03b8] \u2212 \u03b1S \u00b7\u1e20 is in the same direction as the gradient of GIP, G g . It is therefore possible to have explicit control over the scaling of the GIP component in Fish, similar to the original IDGM objective, by writing the following:\nG sm = \u03b1S \u00b7\u1e20 + \u03b3 E[\u03b8 \u2212\u03b8] \u2212 \u03b1S \u00b7\u1e20 .(25)\nBy introducing the scaling term \u03b3, we have better control on how much the objective focus on inner product vs average gradient. Note that \u03b3 = 1 recovers the original Fish gradient, and when \u03b3 = 0 the gradient G sm is equivalent to ERM's gradient with learning rate \u03b1S. We name the resulting algorithm SmoothFish. See Algorithm 4. for D i \u2208 permute({D 1 , D 2 , \u00b7 \u00b7 \u00b7 , D S }) do 4:\n\nSample batch d i \u223c D i 5: y); \u03b8) \u2202\u03b8 / /Grad wrt \u03b8 6:\ng i = E di \u2202l((x,g i = E di \u2202l((x, y); \u03b8) \u2202 \u03b8 / /Grad wrt \u03b8 7:\nUpdate \u03b8 \u2190 \u03b8 \u2212 \u03b1 g i 8:\n\nend for\n9:\u1e21 = 1 S S s=1 g i , g sm = \u03b1S\u1e21 + \u03b3 ( \u03b8 \u2212 \u03b8) \u2212 \u03b1S\u1e21 10:\nUpdate \u03b8 \u2190 \u03b8 + g sm 11: end for \n\n\nB.2 Results\n\nWe run experiments on the 6 datasets in WILDS using SmoothFish, with \u03b3 ranging in [0.1, 0.2, 0.5, 0.8]. We also include results for \u03b3 = 0 (equivalent to ERM) and \u03b3 = 1 (equivalent to Fish). See results in Figure 7. The other hyperparameters including \u03b1, meta steps, used here are the same as the ones used in our main experiments section.\n\n\nC Discussions and Results on WILDS\n\nWe provide a more detailed summary of each dataset in Table 6. Some entries in # Domains are omitted because the domains in each split overlap. Note that in this paper we report the results on WILDS v1 -the benchmark has been updated since with slightly different dataset splits. We are currently working on updating our results to v2 of WILDS.   \n\n\nC.1 POVERTYMAP-WILDS\n\n\nTask: Asset index prediction (real-valued). Domains: 23 countries\n\nThe task is to predict the real-valued asset wealth index of an area, given its satellite imagery. Since the number of domains considered here is large (23 countries), instead of looping over all S domains in each inner-loop, we sample N << S domains in each iteration and perform inner-loop updates using minibatches from these domains only to speed up computation. For this dataset we choose N = 5 by hyper-parameter search.\n\nEvalutaion: Pearson Correlation (r). Following the practice in WILDS benchmark, we compare the results by computing Pearson correlation (r) between the predicted and ground-truth asset index over 3 random seed runs.\n\nResults: We train the model using a ResNet-18 (He et al., 2016) backbone. See Table 7.\n\nWe see that Fish obtains the highest test performance, with the same validation performance as the best baseline. The performance is more stable between validation and test, and the standard deviation is smaller than for the baselines. We also report the results of ERM models trained in our environment as \"ERM (ours)\", which shows similar performance to the canonical results reported in the WILDS benchmark itself (\"ERM\").\n\n\nC.2 CAMELYON17-WILDS\n\nTask: Tumor detection (2 classes). Domains: 5 hospitals\n\nThe CAMELYON17-WILDS dataset contains 450,000 lymph-node scans from 5 hospitals. Due to the size of the dataset, instead of training with Fish from scratch, we pre-train the model with ERM using the recommended hyper-parameters in Koh et al. (2020), and fine-tune with Fish. For this dataset, we find that Fish performs the best when starting from a pretrained model that has not yet converged, achieving much higher accuracy than the ERM model. we provide an ablation study on this in Appendix E.\n\nEvaluation: Average accuracy. We evaluate the average accuracy of this binary classification task. Following Koh et al. (2020), we show the mean and standard deviation of results over 10 random seeds runs. The number of random seeds required here is greater than other WILDS datasets due to the large variance observed in results. Note that these random seeds are not only applied during the fine-tuning stage, but also to the pretrained models to ensure a fair comparison.\n\nResults: Following the practice in WILDS, we adopt DenseNet-121's (Huang et al., 2017) architecture for models trained on this dataset. See results in Table 8.\n\nThe results show that Fish significantly outperforms all baselines -its test accuracy surpasses the best performing baseline by 6%. Also note that for all other baselines, there is a large gap between validation and test accuracy (11% \u223c 27%). This is because WILDS chose the hospital that is the most difficult to generalize to as the test split to make the task more challenging. Surprisingly, as we can observe in Table 8, the discrepancy between test and validation accuracy of Fish is quite small (3%).\n\nThe fact that it is able to achieve a similar level of accuracy on the worst-performing domain further demonstrates that Fish does not rely on domain-specific information, and instead makes predictions using the invariant features across domains.  Similar to CAMELYON17-WILDS, since the number of domains is large, we sample N = 5 domains for each inner-loop. To speed up computation, we also use a pretrained ERM model and fine-tune with Fish; different from Appendix C.2, we find the best-performing models are acquired when using converged pretrained models (see details in Appendix E).\n\nEvaluation: Average & worst-region accuracies. Following WILDS, the average accuracy evaluates the model's ability to generalize over years, and the worst-region accuracy measures the model's performance across regions under a time shift. We report results using 3 random seeds.\n\nResults: Following Koh et al. (2020), we use a DenseNet-121 pretrained on ImageNet for this dataset. Results in Table 9 show that Fish has the highest worst-region accuracy on both test and validation sets. It ranks second in terms of average accuracy, right after ERM. Again, Fish's performance is notably stable with the smallest standard deviation across all metrics compared to baselines.\n\n\nC.4 CIVILCOMMENTS-WILDS\n\nTask: Toxicity detection in online comments (2 classes). Domains: 8 demographic identities.\n\nThe CIVILCOMMENTS-WILDS contains 450,000 comments collected from online articles, each annotated for toxicity and the mentioning of demographic identities. Again, we use ERM pre-trained model to speed up computation, and sample N = 5 domains for each inner-loop.\n\nEvaluation: Worst-group accuracy. To study the bias of annotating comments that mentions demographic groups as toxic, the WILDS benchmark proposes to evaluate the model's performance by doing the following: 1) Further separate each of the 8 demographic identities into 2 groups by toxicity -for example, separate black into black, toxic and black, not toxic; 2) measure the accuracies of these 8 \u00d7 2 = 16 groups and use the lowest accuracy as the final evaluation of the model. This metric is equivalent to computing the sensitivity and specificity of the classifier on each demographic identity, and reporting the worse of the two metrics over all domains. Good performance on the group with the worst accuracy implies that the model does not tend to use demographic identity as an indicator of toxic comments.\n\nAgain, following Koh et al. (2020) we report results of 3 random seed runs.\n\n\nResults:\n\nWe compare results to the baselines used in the WILDS benchmark over 3 random seed runs in Table 10. All models are trained using BERT (Devlin et al., 2018).\n\nThe results show that Fish outperforms the best baseline by 4% and 7% on the test and validation set's worst-group accuracy respectively, and is competitive in terms of average accuracy with ERM (within standard deviation). The strong performance of Fish on worst-group accuracy suggests that the model relies the least on demographic identity as an indicator of toxic comments compared to other baselines. ERM, on the other hand, has the highest average accuracy and the lowest worst-group accuracy. This indicates that it achieves good average performance by leveraging the spurious correlation between toxic comments and the mention of certain demographic groups.\n\nNote that different from all other datasets in WILDS that focus on pure domain generalization (i.e, no overlap between domains in train and test splits), CIVILCOMMENTS-WILDS is a subpopulation shift problem, where the domains in test are a subpopulation of the domains in train. As a result, the baseline models used in WILDS for this dataset are different from the methods used in all other datasets, and are tailored to avoiding systematic failure on data from minority subpopulations. Fish works well in this setting too without any changes or special sampling strategies (such as * and + in Table 10). This further demonstrates the good performance of our algorithm on different domain generalization scenarios.\n\n\nC.5 IWILDCAM-WILDS\n\nTask: Animal species (186 classes). Domains: 324 camera locations.\n\nThe dataset consists of over 200,000 photos of animal in the wild, using stationary cameras across 324 locations. Classifying animal species from these heat or motion-activated photos is especially challenging: methods can easily rely on the background information of photos from the same camera setup. Fish models are pretrained with ERM till convergence, and for each inner loop we sample from N = 10 domains.\n\nEvaluation: Macro F1 score. Across the 186 class labels, we report average accuracy and both weighted and macro F1 scores (F1-w and F1-m, respectively, in Table 11). We run 3 random seeds for each model.\n\nResults: All models reported in Table 8 are trained using a ResNet-50. We find Fish to outperform baselines on both test accuracy and weighted F1, with a 1% improvement on both metrics over the best performing model (ERM). However, this comes at the cost of lower macro F1 score, where Fish performs 1% worse than ERM models that we trained and 3% than the ERM reported in WILDS. This suggests that Fish is less good at classifying rarer species, however the overall accuracy on the test dataset is improved.\n\nAlthough Fish did not outperform the ERM baseline on the primary evaluation metric proposed in Koh et al. (2020), we found the improvement of Fish in both accuracy and weighted F1 to be robust across a range of hyperparameters. See more details on this in Appendix D.\n\n\nC.6 AMAZON-WILDS\n\nTask: Sentiment analysis (5 classes). Domains: 7,676 Amazon reviewers.\n\nThe dataset contains 1.4 million customer reviews on Amazon from 7,676 customers, and the task is to predict the score (1-5 stars) given the review. Similarly, we pretrained the model with ERM till convergence, and due to the large number of domains (S = 5008 in train) we sample N = 5 reviewers for each inner loop.\n\nEvaluation: 10th percentile accuracy. Reporting the accuracy of the 10th percentile reviewer helps us assess whether the model performance is consistent across different reviewers. The results in Table 12 are reported over 3 random seeds.\n\nResults: The model is trained using BERT (Devlin et al., 2018) backbone. While Fish has lower average accuracy compared to ERM, its 10th percentile accuracy matches that of ERM, outperforming all other baselines.  \n\n\nD Hyperparameters\n\nIn Table 13 we list the hyperparameters we used to train ERM. The same hyperparameters were used for producing ERM baseline results and as pretrained models for Fish. In val. metric we report the metric on validation set that is used for model selection, and in cut-off we specify when to stop training when using ERM to generate pretrained models. In Table 14 we list out the hyperparameters we used to train Fish. Note that we train Fish using the same model, batch size, val metric and optimizer as ERM -these are not listed in Table 14 to avoid repetitions. Weight decay is always set as 0. \n\n\nE Ablation Studies on Pre-trained Models\n\nIn this section we perform ablation study on the convergence of pretrained ERM models. We study the performance of Fish with the following three configurations of pretrained ERM models:  We see that CIVILCOMMENTS sustain good performance using pretrained models at different convergence levels. FMOW and IWILDCAM on the other hand seems to have strong preference towards converged model, and the results worsen as the amount of data seen during training goes down. CAMELYON17 achieves the best performance when only 10% of data is seen, and the test accuracy decreases while training with models with higher level of convergence.\n\n\nF Ablation Studies on hyperparameters \u03b1 and\n\nWe study the effect of changing Fish's inner loop learning rate \u03b1 and outer loop learning rate . To make the comparisons more meaningful, we keep \u03b1 \u00b7 constant while changing their respective values. See results in Figure 8.\n\nMeta steps N For most of the datasets we studied (all apart from CAMELYON17 where T = 3) we sample a N -sized subset of all T domains available for training (see Table 14 for T of each dataset). Here we study when N = 5, 10, 20. In general altering these hyperparameters don't have a huge impact on the model's performance, however it does seem thet when N = 20 the performance on some datasets (POVERTY and CIVIL-COMMENTS) degrade slightly. In Figure 9, we demonstrate the progression of inter-domain gradient inner products during training using different objectives. We train both Fish (blue) and ERM (yellow) untill convergence while recording the normalized gradient inner products (i.e. cosine similarity) between minibatches from different domains used in each inner-loop. The gradient inner products are computed both before (dotted) and after (solid) the model's update. To ensure a fair comparison, we use the exact same sequence of data for Fish and ERM (see Appendix H for more details).\n\n\nG Tracking gradient inner product\n\nInevitably, the gradient inner product trends differently for each dataset since the data, types of domain splits and the choice of architecture are very different. In fact, the plot for CDSPRITES-N and POVERTY are significantly different from others, with a dip in gradient inner product at the beginning of training -this is because these are the two datasets that we train from scratch. On all other datasets, the gradient inner products are recorded when fine-tuning with Fish.\n\nDespite their differences, there are some important commonalities between these plots: if we compare the pre-update (dotted) to post-update (solid) curves, we can see that ERM updates often result in the decrease of gradient inner product, while for Fish it can either increase significantly (Figure 9c and Figure 9e) or at least stay at the same level ( Figure 9a, Figure 9b, Figure 9d and Figure 9f). As a result of this, we can see that the post-update gradient inner product of Fish is always greater than ERM at convergence.\n\nThe observations here shows that Fish is effective in increasing/maintaining the level of inter-domain gradient inner product and sheds some lights on its efficiency on the datasets we studied.\n\n\nH Algorithm for tracking gradient inner product\n\nTo make sure that the gradients we record for ERM and Fish are comparable, we use the same sequence of S-minibatches to train both algorithms. See Algorithm 6 for details.\n\nFigure 1 :\n1Isometric projection of training with ERM (blue) vs. our IDGM objective (dark blue), using data fromFigure 2.\n\nFigure 2 :\n2All domains contain 3 types of inputs x 1 , x 2 and x 3 , each depicted in one column. 1 st col.: x 1 = [0, 0, 0, 0], y = 0, makes up for 50% of each dataset; 2 nd col.: x 2 changes for each domain, y = 1 always. 40% of each dataset; 3 rd col.: x 3 = [1, 0, 0, 0], 30% of y = 1 and 70% of y = 0. 10% of each dataset.\n\nFigure 3 :\n3CDSPRITES-N train and test splits. Each 3x3 grid in train (e.g. yellow block) represents one domain.\n\n\nDOMAINBED is a testbed for domain generalization that implements consistent experimental protocols across SOTA methods to ensure fair comparison. It contains 7 popular domain generalization datasets, including Colored MNIST(Arjovsky et al., 2019), RotatedMNIST (Ghifary et al., 2015),VLCS (Fang et al., 2013), PACS(Li et al., 2017), OfficeHome(Venkateswara et al., 2017), Terra Incognita(Beery et al., 2018) and DomainNet(Peng et al., 2019), and offers comparison to a variety of SOTA domain generalization methods, including IRM(Arjovsky et al., 2019), Group DRO, Mixup\n\nFigure 5 :\n5Gradient inner product values during the training for CDSPRITES-N (N=15).\n\nFigure 6 :\n6 with N \u2208 [5, 50]    \n\nFigure 7 :\n7Results on WILDS using SmoothFish with \u03b3 ranging from 0 to 1.\n\nFigure 8 :\n8Ablation studies on \u03b1 and . Note that \u03b1 \u00d7 remains constant in all experiments, and the midpoint of each plot is the hyperparameter we chose to use to report our experiment results.\n\nFigure 9 :\n9Gradient inner product values during the training for CDSPRITES-N (N=15) and 5 different WILDS datasets.\n\n\nand (multi-source) domain adaptation. In fact, there are a few works in domain generalization that are inspired by the meta-learning principles, such as Li et al. (2018a), Balaji et al. (2018), Li et al. (2019), Dou et al. (2019). Meta-learning aims at reducing the sample complexity of new, unseen tasks. A popular school of thinking in meta-learning is model agnostic meta-learning (MAML), first proposed in Finn et al. (2017),\n\nTable 1 :\n1Performance comparison on the linear dataset.Method train acc. test acc. \nW \nb \n\nERM \n97% \n57% [2.8, 3.3, 3.3, 0.0] \u22122.7 \nIDGM \n93% \n93% [0.4, 0.2, 0.2, 0.0] \u22120.4 \nFish \n93% \n93% [0.4, 0.2, 0.2, 0.0] \u22120.4 \n\n\n\nTable 2 :\n2A summary on WILDS datasets. See more details on the dataset in Appendix C.Dataset \nDomains \nMetric \nDisjoint Architecture # Train examples \n\nPOVERTY \n23 countries \nPearson (r) \n\nResnet-18 \n10,000 \nCAMELYON17 \n5 hospitals \nAvg. acc. \n\nDenseNet-121 \n302,436 \nFMOW \n16 years x 5 regions \nAvg. & worst group acc. \n\nDenseNet-121 \n76,863 \nCIVILCOMMENTS 8 demographic groups worst group acc. \n\nBERT \n269,038 \n\nIWILDCAM \n\n324 locations \nMacro F1 \n\nResNet-50 \n142,202 \nAMAZON \n7,676 reviewers \n10th percentile acc. \n\nBERT \n1,000,124 \n\n\n\nTable 4 :\n4Test accuracy (%) on DOMAINBED benchmark.ERM \nIRM \nGroupDRO \nMixup \nMLDG \nCoral \nMMD \nDANN \nCDANN Fish (ours) \n\nCMNIST \n52.0 (\u00b10.1) 51.8 (\u00b10.1) 52.0 (\u00b10.1) 51.9 (\u00b10.1) 51.6 (\u00b10.1) 51.7 (\u00b10.1) 51.8 (\u00b10.1) 51.5 (\u00b10.3) 51.9 (\u00b10.1) 51.6 (\u00b10.1) \nRMNIST \n98.0 (\u00b10.0) 97.9 (\u00b10.0) 98.1 (\u00b10.0) 98.1 (\u00b10.0) 98.0 (\u00b10.0) 98.1 (\u00b10.1) 98.1 (\u00b10.0) 97.9 (\u00b10.1) 98.0 (\u00b10.0) 98.0 (\u00b10.0) \nVLCS \n77.4 (\u00b10.3) 78.1 (\u00b10.0) 77.2 (\u00b10.6) 77.7 (\u00b10.4) 77.1 (\u00b10.4) 77.7 (\u00b10.5) 76.7 (\u00b10.9) 78.7 (\u00b10.3) 78.2 (\u00b10.4) 77.8 (\u00b10.3) \nPACS \n85.7 (\u00b10.5) 84.4 (\u00b11.1) 84.1 (\u00b10.4) 84.3 (\u00b10.5) 84.8 (\u00b10.6) 86.0 (\u00b10.2) 85.0 (\u00b10.2) 84.6 (\u00b11.1) 82.8 (\u00b11.5) 85.5 (\u00b10.3) \nOfficeHome 67.5 (\u00b10.5) 66.6 (\u00b11.0) 66.9 (\u00b10.3) 69.0 (\u00b10.1) 68.2 (\u00b10.1) 68.6 (\u00b10.4) 67.7 (\u00b10.1) 65.4 (\u00b10.6) 65.6 (\u00b10.5) 68.6 (\u00b10.4) \nTerraInc \n47.2 (\u00b10.4) 47.9 (\u00b10.7) 47.0 (\u00b10.3) 48.9 (\u00b10.8) 46.1 (\u00b10.8) 46.4 (\u00b10.8) 49.3 (\u00b11.4) 48.7 (\u00b10.5) 47.6 (\u00b10.8) 45.1 (\u00b11.3) \nDomainNet 41.2 (\u00b10.2) 35.7 (\u00b11.9) 33.7 (\u00b10.2) 39.6 (\u00b10.1) 41.8 (\u00b10.4) 41.8 (\u00b10.2) 39.4 (\u00b10.8) 38.4 (\u00b10.0) 38.9 (\u00b10.1) 42.7 (\u00b10.2) \n\nAverage \n67.0 \n66.0 \n65.5 \n67.1 \n66.8 \n67.2 \n66.8 \n66.4 \n66.1 \n67.1 \n\n\n\nTable 3 :\n3Results on WILDS benchmark.POVERTYMAP \nCAMELYON17 \nFMOW \nCIVILCOMMENTS \n\nIWILDCAM \n\nAMAZON \n\nPearson r \nAvg. acc. (%) \nWorst acc. (%) \nWorst acc. (%) \nMacro F1 \n10-th per. acc. (%) \n\nFish \n0.80 (\u00b11e-2) \n74.7 (\u00b17e-2) \n34.6 (\u00b10.00) \n72.8 (\u00b10.0) \n22.0 (\u00b10.0) \n53.3 (\u00b10.0) \nIRM \n0.78 (\u00b13e-2) \n64.2 (\u00b18.1) \n33.5 (\u00b11.35) \n66.3 (\u00b12.1) \n15.1 (\u00b14.9) \n52.4 (\u00b10.8) \nCoral \n0.77 (\u00b15e-2) \n59.5 (\u00b17.7) \n31.0 (\u00b10.35) \n65.6 (\u00b11.3) \n32.8(\u00b10.1) \n52.9 (\u00b10.8) \nReweighted \n-\n-\n-\n66.2 (\u00b11.2) \n-\n52.4 (\u00b10.8) \nGroupDRO \n0.78 (\u00b15e-2) \n68.4 (\u00b17.3) \n31.4 (\u00b12.10) \n69.1 (\u00b11.8) \n23.9 (\u00b12.1) \n53.5 (\u00b10.0) \nERM \n0.78 (\u00b13e-2) \n70.3 (\u00b16.4) \n32.8 (\u00b10.45) \n56.0 (\u00b13.6) \n31.0 (\u00b11.3) \n53.8 (\u00b10.8) \nERM (ours) \n0.77 (\u00b15e-2) \n70.5 (\u00b112.1) \n30.9 (\u00b11.53) \n58.1 (\u00b11.7) \n25.1 (\u00b10.2) \n53.3 (\u00b10.8) \n\n\n\nTable 5 :\n5Ablation study on random grouping: test accuracy on different datasets.CDSPRITES(N=10) \n\nFMOW \nVLCS \nPACS \nOfficeHome \n\n\n\n\nC. Fang, Y. Xu, and D. N. Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In 2013 IEEE International Conference on Computer Vision, pages 1657-1664, 2013. C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML'17 Proceedings of the 34th International Conference on Machine Learning -Volume 70, pages 1126-1135, 2017.Q. Dou, D. C. de Castro, K. Kamnitsas, and B. Glocker. Domain generalization via model-agnostic \nlearning of semantic features. In Advances in Neural Information Processing Systems, volume 32, \npages 6450-6461, 2019. \n\nY. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and \nV. Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning \nresearch, 17(1):2096-2030, 2016. \n\nR. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel. Imagenet-trained \ncnns are biased towards texture; increasing shape bias improves accuracy and robustness. In \nInternational Conference on Learning Representations (ICLR), 2018. \n\nM. Ghifary, W. B. Kleijn, M. Zhang, and D. Balduzzi. Domain generalization for object recognition \nwith multi-task autoencoders. In 2015 IEEE International Conference on Computer Vision (ICCV), \npages 2551-2559, 2015. \n\nA. Gretton, K. Borgwardt, M. Rasch, B. Sch\u00f6lkopf, and A. Smola. A kernel method for the two-\nsample-problem. Advances in neural information processing systems, 19:513-520, 2006. \n\nA. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Sch\u00f6lkopf. Covariate shift \nand local learning by distribution matching, pages 131-160. MIT Press, Cambridge, MA, USA, \n2009a. \n\n\nTable 6 :\n6Details of the 6 WILDS datasets we experimented on.Dataset \nDomains (# domains) \nData (x) \nTarget (y) \n# Examples \n# Domains \n\ntrain \nval \ntest \ntrain \nval \ntest \n\nFMOW \nTime (16), Regions (5) \nSatellite images \nLand use (62 classes) \n76,863 \n19,915 22,108 \n11, -3, -\n2, -\nPOVERTY \nCountries (23), Urban/rural (2) Satellite images \nAsset (real valued) \n10,000 \n4,000 \n4,000 \n13, -5, -\n5, -\nCAMELYON17 \nHospitals (5) \nTissue slides \nTumor (2 classes) \n302,436 34,904 85,054 \n3 \n1 \n1 \nCIVILCOMMENTS Demographics (8) \nOnline comments Toxicity (2 classes) \n269,038 45,180 133,782 \n-\n-\n-\n\nIWILDCAM2020 \n\nTrap locations (324) \nPhotos \nAnimal species (186 classes) 142,202 20,784 38,943 \n245 \n32 \n47 \nAMAZON \nReviewers (7,676) \nProduct reviews Star rating (5 classes) \n1,000,124 100,050 100,050 \n5,008 1,334 1,334 \n\n\n\nTable 7 :\n7Results on POVERTYMAP-WILDS.Method \nVal. Pearson r Test Pearson r \n\nFish \n0.81 (\u00b16e-3) \n0.81 (\u00b19e-3) \nIRM \n0.81 (\u00b14e-2) \n0.78 (\u00b13e-2) \nERM \n0.80 (\u00b13e-2) \n0.78 (\u00b13e-2) \nERM (ours) 0.80 (\u00b13e-2) \n0.77 (\u00b15e-2) \nCoral \n0.80 (\u00b14e-2) \n0.77 (\u00b15e-2) \n\n\n\nTable 8 :\n8Results on CAMELYON17-WILDS.Method \nVal. Accuracy (%) Test Accuracy (%) \n\nFish \n82.5 (\u00b11.2) \n79.5 (\u00b16.0) \nERM \n84.3 (\u00b12.1) \n73.3 (\u00b19.9) \nERM (ours) \n84.1 (\u00b12.4) \n70.5 (\u00b112.1) \nIRM \n86.2 (\u00b12.1) \n60.9 (\u00b115.3) \nCoral \n86.3 (\u00b12.2) \n59.2 (\u00b115.1) \n\n\n\nTable 9 :\n9Results on FMOW-WILDS.Method \nVal. Accuracy (%) \nTest Accuracy (%) \n\nAverage \nWorst \nAverage \nWorst \n\nFish \n57.3 (\u00b10.01) 49.5 (\u00b10.44) \n51.8 (\u00b10.12) 34.3 (\u00b10.61) \nERM \n59.7 (\u00b10.14) 48.2 (\u00b12.05) \n53.1 (\u00b10.25) 31.7 (\u00b11.01) \nERM (ours) 59.9 (\u00b10.22) 47.1 (\u00b11.21) \n52.9 (\u00b10.18) 30.9 (\u00b11.53) \nIRM \n57.2 (\u00b10.01) 47.4 (\u00b12.36) \n50.9 (\u00b10.32) 31.0 (\u00b11.15) \nCoral \n56.7 (\u00b10.06) 46.8 (\u00b11.18) \n50.5 (\u00b10.30) 30.5 (\u00b10.70) \n\n\n\nTable 10 :\n10Results on CIVILCOMMENTS-WILDS.Method \nVal. Accuracy (%) \nTest Accuracy (%) \n\nAverage \nWorst \nAverage \nWorst \n\nFish \n91.8 (\u00b10.2) 75.3 (\u00b10.3) \n91.4 (\u00b10.3) 74.2 (\u00b10.5) \nGroup DRO 89.6 (\u00b10.3) 68.7 (\u00b11.0) \n89.4 (\u00b10.3) 70.4 (\u00b12.1) \nReweighted 89.1 (\u00b10.3) 67.9 (\u00b11.2) \n88.9 (\u00b10.3) 67.3 (\u00b10.1) \nERM \n92.3 (\u00b10.6) 53.6 (\u00b10.7) \n92.2 (\u00b10.6) 58.0 (\u00b11.2) \nERM (ours) 92.1 (\u00b10.5) 54.1 (\u00b10.4) \n92.5 (\u00b10.3) 58.1 (\u00b11.7) \n\n\n\nTable 11 :\n11Results on IWILDCAM-WILDS.Method \nValidation \nTest \n\nAcc. (%) Weighted F1 Macro F1 \nAcc. (%) Weighted F1 Macro F1 \n\nFish \n58.0 (\u00b10.2) 56.5 (\u00b10.3) 25.8 (\u00b10.5) \n63.2 (\u00b10.7) 64.0 (\u00b10.5) 24.2 (\u00b10.9) \nCoral \n-\n-\n-\n62.5 (\u00b11.7) \n-\n26.3 (\u00b11.4) \nERM \n-\n-\n-\n62.9 (\u00b10.5) \n-\n27.8 (\u00b11.3) \nERM (ours) 55.8 (\u00b10.2) 54.6 (\u00b10.5) \n27.7 (\u00b11) \n63.0 (\u00b10.6) 62.4 (\u00b10.2) 25.1 (\u00b10.2) \n\n\nTable 12 :\n12Results on AMAZON-WILDS.Method \nVal. Accuracy (%) \nTest Accuracy (%) \n\nAverage 10-th per. \nAverage 10-th per. \n\nFish \n73.8 (\u00b10.1) 57.3 (\u00b10.0) \n72.9 (\u00b10.2) 56.0 (\u00b10.0) \nERM \n74.3 (\u00b10.0) 57.3 (\u00b10.0) \n73.5 (\u00b10.1) 56.0 (\u00b10.0) \nERM (ours) 74.0 (\u00b10.0) 57.3 (\u00b10.0) \n73.3 (\u00b10.1) 56.0 (\u00b10.0) \nIRM \n73.6 (\u00b10.2) 56.4 (\u00b10.8) \n73.3 (\u00b10.2) 55.1 (\u00b10.8) \nReweighted 69.6 (\u00b10.0) 53.3 (\u00b10.0) \n69.2 (\u00b10.1) 52.4 (\u00b10.8) \n\n\n\nTable 13 :\n13Hyperparameters for ERM. We follow the hyperparameters used in WILDS benchmark. Note that we did not use a pretrained model for POVERTY, therefore its cut-off condition is not reported.Dataset \nModel \nLearning rate Batch size Weight decay Optimizer \nVal. metric \nCut-off \n\nCAMELYON17 \nDensenet-121 \n1e-3 \n32 \n0 \nSGD \nacc. avg. \niter 500 \nCIVILCOMMENTS \nBERT \n1e-5 \n16 \n0.01 \nAdam \nacc. wg. \nBest val. metric \nFMOW \nDensenet-121 \n1e-4 \n64 \n0 \nAdam \nacc. avg. \nBest val. metric \n\nIWILDCAM \n\nResnet-50 \n1e-4 \n16 \n0 \nAdam \nF1-macro (all) \nBest val. metric \nPOVERTY \nResnet-18 \n1e-3 \n64 \n0 \nAdam \nPearson (r) \n-\nAMAZON \nBERT \n2e-6 \n8 \n0.01 \nAdam \n10th percentile acc. \n-\n\n\n\nTable 14 :\n14Hyperparameters for Fish.Dataset \nGroup by \n\u03b1 \n# domains Meta steps \n\nCAMELYON17 \nHospitals \n1e-3 0.01 \n3 \n3 \nCIVILCOMMENTS Demographics \u00d7 toxicity 1e-5 0.05 \n16 \n5 \nFMOW \ntime \u00d7 regions \n1e-4 0.01 \n80 \n5 \n\nIWILDCAM \n\nTrap locations \n1e-4 0.01 \n324 \n10 \nPOVERTY \nCountries \n1e-3 0.1 \n23 \n5 \nAMAZON \nReviewers \n2e-6 0.01 \n7,676 \n5 \n\n\n\nTable 15 :\n15Ablation study on pretrained ERM models.Test Avg Acc Test Avg Acc Test Macro F1Test Worst AccModel \nFMOW \nCAMELYON17 IWILDCAM CIVILCOMMENTS \n\n10% data \n21.7 (\u00b12.5) \n79.1 (\u00b112.3) \n13.7 (\u00b10.5) \n71.8 (\u00b11.3) \n50% data \n31.0 (\u00b10.8) \n64.6 (\u00b112.3) \n19.0 (\u00b10.06) \n74.2 (\u00b10.5) \nConverged 32.7 (\u00b11.2) \n63.5 (\u00b18.2) \n23.7 (\u00b10.9) \n73.8 (\u00b11.8) \n\n\n\nTable 16 :\n16Ablation study on meta steps N .N \nFMOW \nPOVERTY \n\nIWILDCAM \n\nCIVILCOMMENTS \n\nTest Avg Acc Test Pearson r Test Macro F1 \nTest Worst Acc \n\n5 \n33.0 (\u00b11.6) \n80.3 (\u00b11.7) \n23.7 (\u00b10.9) \n74.3 (\u00b11.5) \n10 32.7 (\u00b11.2) \n80.0 (\u00b10.8) \n23.7 (\u00b10.5) \n73.4 (\u00b11.0) \n20 33.3 (\u00b12.1) \n77.7 (\u00b12.1) \n23.7 (\u00b10.9) \n72.6 (\u00b12.3) \n\n\nFollowing the convention of naming this style of algorithms after classes of vertebrates (animals with backbones).\nAlgorithm 6 Algorithm of collecting gradient inner product\u1e21 for Fish and ERM both before and after updates. See GIP in Algorithm 5.1:Initialize Fish \u03b8 f \u2190 \u03b8, ERM \u03b8 e \u2190 \u03b8 2: for i = 1, 2, \u00b7 \u00b7 \u00b7 do 3:/ /Get all minibatches 4:for D n \u2208 {D 1 , D 2 , \u00b7 \u00b7 \u00b7 , D N } do 5:Sample batch d n \u223c D n 6: end for 7: / /GradInnerProd after update 26:\u1e21 F a = GIP({d 1 , d 2 , \u00b7 \u00b7 \u00b7 , d N }, \u03b8 f ) 27:\u1e21 Ea = GIP({d 1 , d 2 , \u00b7 \u00b7 \u00b7 , d N }, \u03b8 e ) 28: end for 29: Return\u1e21 F b ,\u1e21 F a ,\u1e21 Eb ,\u1e21 Ea Algorithm 5 Function GIP 1: function GIP({d 1 , d 2 , \u00b7 \u00b7 \u00b7 , d N }, \u03b8): 2: for d n \u2208 {d 1 , d 2 , \u00b7 \u00b7 \u00b7 , d N } do3:g n = \u2202l(d n ; \u03b8)/\u2202\u03b8 4: end for 5:\u1e21 = 1 S(S\u22121) i =j i,j\u2208S g i \u00b7 g j 6: return\u1e21\nLearning to learn by gradient descent by gradient descent. M Andrychowicz, M Denil, S Gomez, M W Hoffman, D Pfau, T Schaul, B Shillingford, N. De Freitas, arXiv:1606.04474arXiv preprintM. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and N. De Freitas. Learning to learn by gradient descent by gradient descent. arXiv preprint arXiv:1606.04474, 2016.\n\nM Arjovsky, L Bottou, I Gulrajani, D Lopez-Paz, arXiv:1907.02893Invariant risk minimization. arXiv preprintM. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.\n\nMetareg: towards domain generalization using meta-regularization. Y Balaji, S Sankaranarayanan, R Chellappa, NIPS'18 Proceedings of the 32nd International Conference on Neural Information Processing Systems. 31Y. Balaji, S. Sankaranarayanan, and R. Chellappa. Metareg: towards domain generalization using meta-regularization. In NIPS'18 Proceedings of the 32nd International Conference on Neural Information Processing Systems, volume 31, pages 1006-1016, 2018.\n\nRecognition in terra incognita. S Beery, G V Horn, P Perona, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)S. Beery, G. V. Horn, and P. Perona. Recognition in terra incognita. In Proceedings of the European Conference on Computer Vision (ECCV), pages 472-489, 2018.\n\nA theory of learning from different domains. S Ben-David, J Blitzer, K Crammer, A Kulesza, F Pereira, J W Vaughan, Machine learning. 791S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different domains. Machine learning, 79(1):151-175, 2010.\n\nDemographic dialectal variation in social media: A case study of african-american english. S L Blodgett, L Green, B T O&apos;connor, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingS. L. Blodgett, L. Green, and B. T. O'Connor. Demographic dialectal variation in social media: A case study of african-american english. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1119-1130, 2016.\n\nApproximating cnns with bag-of-local-features models works surprisingly well on imagenet. W Brendel, M Bethge, International Conference on Learning Representations (ICLR). W. Brendel and M. Bethge. Approximating cnns with bag-of-local-features models works surprisingly well on imagenet. In International Conference on Learning Representations (ICLR), 2019.\n\nDomain generalization by solving jigsaw puzzles. F M Carlucci, A D&apos;innocente, S Bucci, B Caputo, T Tommasi, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). F. M. Carlucci, A. D'Innocente, S. Bucci, B. Caputo, and T. Tommasi. Domain generalization by solving jigsaw puzzles. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2229-2238, 2019.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K N Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1J. Devlin, M.-W. Chang, K. Lee, and K. N. Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2018.\n\nCovariate shift by kernel mean matching. A Gretton, A Smola, J Huang, M Schmittfull, K Borgwardt, B Sch\u00f6lkopf, Dataset shift in machine learning. 345A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Sch\u00f6lkopf. Covariate shift by kernel mean matching. Dataset shift in machine learning, 3(4):5, 2009b.\n\nI Gulrajani, D Lopez-Paz, arXiv:2007.01434search of lost domain generalization. arXiv preprintI. Gulrajani and D. Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.\n\nFairness without demographics in repeated loss minimization. T B Hashimoto, M Srivastava, H Namkoong, P Liang, International Conference on Machine Learning. T. B. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang. Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning, pages 1929-1938, 2018.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016.\n\nDoes distributionally robust supervised learning give robust classifiers. W Hu, G Niu, I Sato, M Sugiyama, International Conference on Machine Learning. W. Hu, G. Niu, I. Sato, and M. Sugiyama. Does distributionally robust supervised learning give robust classifiers. In International Conference on Machine Learning, pages 2029-2037, 2018.\n\nDoes distributionally robust supervised learning give robust classifiers. W Hu, G Niu, I Sato, M Sugiyama, International Conference on Machine Learning. PMLRW. Hu, G. Niu, I. Sato, and M. Sugiyama. Does distributionally robust supervised learning give robust classifiers? In International Conference on Machine Learning, pages 2029-2037. PMLR, 2018.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261-2269, 2017.\n\nSelf-challenging improves cross-domain generalization. Z Huang, H Wang, E P Xing, D Huang, European Conference on Computer Vision. Z. Huang, H. Wang, E. P. Xing, and D. Huang. Self-challenging improves cross-domain generalization. In European Conference on Computer Vision, pages 124-140, 2020.\n\nP W Koh, S Sagawa, H Marklund, S M Xie, M Zhang, A Balsubramani, W Hu, M Yasunaga, R L Phillips, S Beery, J Leskovec, A Kundaje, E Pierson, S Levine, C Finn, P Liang, arXiv:2012.07421Wilds: A benchmark of in-the-wild distribution shifts. arXiv preprintP. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, S. Beery, J. Leskovec, A. Kundaje, E. Pierson, S. Levine, C. Finn, and P. Liang. Wilds: A benchmark of in-the-wild distribution shifts. arXiv preprint arXiv:2012.07421, 2020.\n\nOut-of-distribution generalization with maximal invariant predictor. M Koyama, S Yamaguchi, In arXiv e-printsM. Koyama and S. Yamaguchi. Out-of-distribution generalization with maximal invariant predictor. In arXiv e-prints, 2021.\n\nDeeper, broader and artier domain generalization. D Li, Y Yang, Y.-Z Song, T M Hospedales, 2017 IEEE International Conference on Computer Vision (ICCV). D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain generalization. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 5543-5551, 2017.\n\nLearning to generalize: Meta-learning for domain generalization. D Li, Y Yang, Y.-Z Song, T Hospedales, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32D. Li, Y. Yang, Y.-Z. Song, and T. Hospedales. Learning to generalize: Meta-learning for domain generalization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018a.\n\nEpisodic training for domain generalization. D Li, J Zhang, Y Yang, C Liu, Y.-Z Song, T Hospedales, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). D. Li, J. Zhang, Y. Yang, C. Liu, Y.-Z. Song, and T. Hospedales. Episodic training for domain generalization. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1446-1455, 2019.\n\nDomain generalization with adversarial feature learning. H Li, S J Pan, S Wang, A C Kot, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionH. Li, S. J. Pan, S. Wang, and A. C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5400-5409, 2018b.\n\nDeep domain generalization via conditional invariant adversarial networks. Y Li, X Tian, M Gong, Y Liu, T Liu, K Zhang, D Tao, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Y. Li, X. Tian, M. Gong, Y. Liu, T. Liu, K. Zhang, and D. Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 647-663, 2018.\n\nGradient episodic memory for continual learning. D Lopez-Paz, M Ranzato, Advances in Neural Information Processing Systems. 30D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, volume 30, pages 6467-6476, 2017.\n\nL Matthey, I Higgins, D Hassabis, A Lerchner, dsprites: Disentanglement testing sprites dataset. L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.\n\nOn first-order meta-learning algorithms. A Nichol, J Achiam, J Schulman, arXiv: LearningA. Nichol, J. Achiam, and J. Schulman. On first-order meta-learning algorithms. arXiv: Learning, 2018.\n\nMoment matching for multi-source domain adaptation. X Peng, Q Bai, X Xia, Z Huang, K Saenko, B Wang, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source domain adaptation. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1406-1415, 2019.\n\nDataset shift in machine learning. J Quionero-Candela, M Sugiyama, A Schwaighofer, N D Lawrence, The MIT PressJ. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in machine learning. The MIT Press, 2009.\n\nA causal perspective on domain adaptation. M Rojas-Carulla, B Sch\u00f6lkopf, R Turner, J Peters, stat. 19M. Rojas-Carulla, B. Sch\u00f6lkopf, R. Turner, and J. Peters. A causal perspective on domain adaptation. stat, 1050:19, 2015.\n\nDistributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. S Sagawa, P W Koh, T B Hashimoto, P Liang, arXiv:1911.08731arXiv preprintS. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\n\nDistributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. S Sagawa, P W Koh, T B Hashimoto, P Liang, arXiv:1911.08731arXiv preprintS. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\n\nP Stock, M Cisse, arXiv:1711.11443Convnets and imagenet beyond accuracy: Explanations, bias detection, adversarial examples and model criticism. arXiv preprintP. Stock and M. Cisse. Convnets and imagenet beyond accuracy: Explanations, bias detection, adversarial examples and model criticism. arXiv preprint arXiv:1711.11443, 2017.\n\nDeep coral: Correlation alignment for deep domain adaptation. B Sun, K Saenko, European conference on computer vision. SpringerB. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443-450. Springer, 2016.\n\nGender and dialect bias in youtube's automatic captions. R Tatman, Proceedings of the First ACL Workshop on Ethics in Natural Language Processing. the First ACL Workshop on Ethics in Natural Language ProcessingR. Tatman. Gender and dialect bias in youtube's automatic captions. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pages 53-59, 2017.\n\nLearning to Learn. S Thrun, L Pratt, Kluwer Academic PublishersUSAISBN 0792380479S. Thrun and L. Pratt, editors. Learning to Learn. Kluwer Academic Publishers, USA, 1998. ISBN 0792380479.\n\nUnbiased look at dataset bias. A Torralba, A A Efros, 10.1109/CVPR.2011.5995347Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR '11. the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR '11USAA. Torralba and A. A. Efros. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR '11, page 1521-1528, USA, 2011. IEEE Computer Society. ISBN 9781457703942. doi: 10.1109/CVPR.2011.5995347. URL https://doi.org/10.1109/CVPR.2011.5995347.\n\nDeep hashing network for unsupervised domain adaptation. H Venkateswara, J Eusebio, S Chakraborty, S Panchanathan, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan. Deep hashing network for unsupervised domain adaptation. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5385-5394, 2017.\n\nLearning robust global representations by penalizing local predictive power. H Wang, S Ge, Z C Lipton, E P Xing, Advances in Neural Information Processing Systems. 32H. Wang, S. Ge, Z. C. Lipton, and E. P. Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, volume 32, pages 10506-10518, 2019.\n\nImprove unsupervised domain adaptation with mixup training. S Yan, H Song, N Li, L Zou, L Ren, arXiv:2001.00677arXiv preprintS. Yan, H. Song, N. Li, L. Zou, and L. Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020.\n\nModel is trained on 10% of the data. epoch 1Model is trained on 10% of the data (epoch 1);\n\nModel is trained on 50% of the data. epoch 12. Model is trained on 50% of the data (epoch 1);\n\nModel at convergence. Model at convergence.\n\nBy comparing the results between these three settings, we demonstrate how the level of convergence affects the Fish's training performance. See results in Table 15. Note that POVERTY is excluded here because the dataset is small enough that we are able to train Fish from scratchBy comparing the results between these three settings, we demonstrate how the level of convergence affects the Fish's training performance. See results in Table 15. Note that POVERTY is excluded here because the dataset is small enough that we are able to train Fish from scratch.\n", "annotations": {"author": "[{\"end\":260,\"start\":47},{\"end\":472,\"start\":261},{\"end\":697,\"start\":473},{\"end\":914,\"start\":698},{\"end\":1110,\"start\":915},{\"end\":1325,\"start\":1111},{\"end\":1526,\"start\":1326}]", "publisher": null, "author_last_name": "[{\"end\":55,\"start\":52},{\"end\":274,\"start\":269},{\"end\":488,\"start\":484},{\"end\":709,\"start\":700},{\"end\":926,\"start\":920},{\"end\":1126,\"start\":1119},{\"end\":1342,\"start\":1334}]", "author_first_name": "[{\"end\":51,\"start\":47},{\"end\":268,\"start\":261},{\"end\":479,\"start\":473},{\"end\":483,\"start\":480},{\"end\":699,\"start\":698},{\"end\":919,\"start\":915},{\"end\":1118,\"start\":1111},{\"end\":1333,\"start\":1326}]", "author_affiliation": "[{\"end\":259,\"start\":78},{\"end\":471,\"start\":290},{\"end\":696,\"start\":515},{\"end\":913,\"start\":732},{\"end\":1109,\"start\":928},{\"end\":1324,\"start\":1143},{\"end\":1525,\"start\":1344}]", "title": "[{\"end\":44,\"start\":1},{\"end\":1570,\"start\":1527}]", "venue": null, "abstract": "[{\"end\":2578,\"start\":1625}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2929,\"start\":2907},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2960,\"start\":2929},{\"end\":2980,\"start\":2960},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3206,\"start\":3181},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3230,\"start\":3208},{\"end\":3462,\"start\":3428},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3475,\"start\":3462},{\"end\":4360,\"start\":4340},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4390,\"start\":4368},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4443,\"start\":4425},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4532,\"start\":4509},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6027,\"start\":6006},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6495,\"start\":6477},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7253,\"start\":7222},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7425,\"start\":7408},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7675,\"start\":7645},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8041,\"start\":8014},{\"end\":8107,\"start\":8082},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8699,\"start\":8676},{\"end\":8719,\"start\":8699},{\"end\":8813,\"start\":8779},{\"end\":8905,\"start\":8886},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8925,\"start\":8905},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9016,\"start\":8994},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9099,\"start\":9076},{\"end\":9160,\"start\":9138},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9214,\"start\":9196},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9268,\"start\":9241},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10310,\"start\":10288},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10839,\"start\":10816},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10862,\"start\":10839},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10876,\"start\":10862},{\"end\":11134,\"start\":11100},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11336,\"start\":11313},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11363,\"start\":11337},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20247,\"start\":20227},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20448,\"start\":20428},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21126,\"start\":21104},{\"end\":22130,\"start\":22096},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22143,\"start\":22130},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24306,\"start\":24288},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25669,\"start\":25652},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25700,\"start\":25680},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25730,\"start\":25710},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27393,\"start\":27362},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27414,\"start\":27396},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27439,\"start\":27421},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27469,\"start\":27447},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27493,\"start\":27475},{\"end\":27520,\"start\":27495},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27548,\"start\":27531},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31691,\"start\":31671},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":35735,\"start\":35715},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":36475,\"start\":36455},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":39973,\"start\":39956},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40753,\"start\":40736},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":41130,\"start\":41113},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41565,\"start\":41545},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":43055,\"start\":43038},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":44643,\"start\":44626},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":44853,\"start\":44832},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":47570,\"start\":47553},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":48438,\"start\":48417},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":53437,\"start\":53414},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":53522,\"start\":53505},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":53561,\"start\":53534},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":53598,\"start\":53578},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":53631,\"start\":53612},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":53743,\"start\":53720}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52744,\"start\":52622},{\"attributes\":{\"id\":\"fig_1\"},\"end\":53074,\"start\":52745},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53188,\"start\":53075},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53761,\"start\":53189},{\"attributes\":{\"id\":\"fig_4\"},\"end\":53848,\"start\":53762},{\"attributes\":{\"id\":\"fig_7\"},\"end\":53883,\"start\":53849},{\"attributes\":{\"id\":\"fig_8\"},\"end\":53958,\"start\":53884},{\"attributes\":{\"id\":\"fig_9\"},\"end\":54152,\"start\":53959},{\"attributes\":{\"id\":\"fig_10\"},\"end\":54270,\"start\":54153},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54702,\"start\":54271},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54922,\"start\":54703},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":55462,\"start\":54923},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56564,\"start\":55463},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":57333,\"start\":56565},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":57466,\"start\":57334},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":59204,\"start\":57467},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":60026,\"start\":59205},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":60282,\"start\":60027},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":60538,\"start\":60283},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":60958,\"start\":60539},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":61378,\"start\":60959},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":61753,\"start\":61379},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":62169,\"start\":61754},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":62851,\"start\":62170},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":63198,\"start\":62852},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":63545,\"start\":63199},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":63864,\"start\":63546}]", "paragraph": "[{\"end\":4069,\"start\":2594},{\"end\":4651,\"start\":4071},{\"end\":5735,\"start\":4653},{\"end\":6277,\"start\":5737},{\"end\":6895,\"start\":6279},{\"end\":7775,\"start\":6912},{\"end\":7860,\"start\":7777},{\"end\":10229,\"start\":7862},{\"end\":10588,\"start\":10231},{\"end\":11232,\"start\":10590},{\"end\":11912,\"start\":11234},{\"end\":12262,\"start\":11914},{\"end\":12760,\"start\":12286},{\"end\":12871,\"start\":12809},{\"end\":13013,\"start\":12873},{\"end\":13309,\"start\":13071},{\"end\":13748,\"start\":13350},{\"end\":14741,\"start\":13750},{\"end\":15096,\"start\":14783},{\"end\":15658,\"start\":15160},{\"end\":16135,\"start\":15660},{\"end\":16502,\"start\":16220},{\"end\":17159,\"start\":16504},{\"end\":17645,\"start\":17189},{\"end\":17869,\"start\":17859},{\"end\":17952,\"start\":17876},{\"end\":17991,\"start\":17954},{\"end\":18538,\"start\":18208},{\"end\":19193,\"start\":18540},{\"end\":19303,\"start\":19195},{\"end\":20144,\"start\":19491},{\"end\":20776,\"start\":20146},{\"end\":21507,\"start\":20778},{\"end\":22039,\"start\":21509},{\"end\":22764,\"start\":22041},{\"end\":23062,\"start\":22776},{\"end\":23638,\"start\":23064},{\"end\":24224,\"start\":23640},{\"end\":25297,\"start\":24234},{\"end\":26629,\"start\":25299},{\"end\":27094,\"start\":26631},{\"end\":27549,\"start\":27108},{\"end\":28173,\"start\":27551},{\"end\":28933,\"start\":28186},{\"end\":29107,\"start\":28935},{\"end\":30134,\"start\":29127},{\"end\":30415,\"start\":30136},{\"end\":31180,\"start\":30430},{\"end\":31523,\"start\":31182},{\"end\":31733,\"start\":31584},{\"end\":31853,\"start\":31735},{\"end\":31979,\"start\":31898},{\"end\":32149,\"start\":32028},{\"end\":32265,\"start\":32151},{\"end\":32592,\"start\":32313},{\"end\":32842,\"start\":32778},{\"end\":33027,\"start\":32844},{\"end\":33258,\"start\":33201},{\"end\":33324,\"start\":33281},{\"end\":33510,\"start\":33369},{\"end\":33663,\"start\":33586},{\"end\":33820,\"start\":33796},{\"end\":34070,\"start\":33879},{\"end\":34194,\"start\":34138},{\"end\":34366,\"start\":34277},{\"end\":34454,\"start\":34391},{\"end\":34578,\"start\":34529},{\"end\":34740,\"start\":34662},{\"end\":35369,\"start\":34742},{\"end\":35798,\"start\":35371},{\"end\":36147,\"start\":35800},{\"end\":36190,\"start\":36149},{\"end\":36641,\"start\":36192},{\"end\":37415,\"start\":36643},{\"end\":37772,\"start\":37473},{\"end\":38193,\"start\":37812},{\"end\":38247,\"start\":38195},{\"end\":38334,\"start\":38311},{\"end\":38343,\"start\":38336},{\"end\":38432,\"start\":38400},{\"end\":38786,\"start\":38448},{\"end\":39172,\"start\":38825},{\"end\":39691,\"start\":39265},{\"end\":39908,\"start\":39693},{\"end\":39996,\"start\":39910},{\"end\":40423,\"start\":39998},{\"end\":40503,\"start\":40448},{\"end\":41002,\"start\":40505},{\"end\":41477,\"start\":41004},{\"end\":41638,\"start\":41479},{\"end\":42146,\"start\":41640},{\"end\":42737,\"start\":42148},{\"end\":43017,\"start\":42739},{\"end\":43411,\"start\":43019},{\"end\":43530,\"start\":43439},{\"end\":43794,\"start\":43532},{\"end\":44607,\"start\":43796},{\"end\":44684,\"start\":44609},{\"end\":44854,\"start\":44697},{\"end\":45522,\"start\":44856},{\"end\":46239,\"start\":45524},{\"end\":46328,\"start\":46262},{\"end\":46741,\"start\":46330},{\"end\":46946,\"start\":46743},{\"end\":47456,\"start\":46948},{\"end\":47725,\"start\":47458},{\"end\":47816,\"start\":47746},{\"end\":48134,\"start\":47818},{\"end\":48374,\"start\":48136},{\"end\":48590,\"start\":48376},{\"end\":49207,\"start\":48612},{\"end\":49881,\"start\":49252},{\"end\":50152,\"start\":49929},{\"end\":51153,\"start\":50154},{\"end\":51672,\"start\":51191},{\"end\":52203,\"start\":51674},{\"end\":52398,\"start\":52205},{\"end\":52621,\"start\":52450}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12808,\"start\":12761},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13070,\"start\":13014},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15159,\"start\":15097},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16219,\"start\":16136},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17858,\"start\":17646},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18207,\"start\":17992},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19490,\"start\":19304},{\"attributes\":{\"id\":\"formula_7\"},\"end\":31880,\"start\":31854},{\"attributes\":{\"id\":\"formula_8\"},\"end\":31897,\"start\":31880},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32027,\"start\":31980},{\"attributes\":{\"id\":\"formula_10\"},\"end\":32312,\"start\":32266},{\"attributes\":{\"id\":\"formula_11\"},\"end\":32777,\"start\":32593},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33200,\"start\":33028},{\"attributes\":{\"id\":\"formula_14\"},\"end\":33280,\"start\":33259},{\"attributes\":{\"id\":\"formula_15\"},\"end\":33368,\"start\":33325},{\"attributes\":{\"id\":\"formula_16\"},\"end\":33585,\"start\":33511},{\"attributes\":{\"id\":\"formula_17\"},\"end\":33795,\"start\":33664},{\"attributes\":{\"id\":\"formula_18\"},\"end\":33878,\"start\":33821},{\"attributes\":{\"id\":\"formula_19\"},\"end\":34137,\"start\":34071},{\"attributes\":{\"id\":\"formula_20\"},\"end\":34276,\"start\":34195},{\"attributes\":{\"id\":\"formula_21\"},\"end\":34390,\"start\":34367},{\"attributes\":{\"id\":\"formula_22\"},\"end\":34528,\"start\":34455},{\"attributes\":{\"id\":\"formula_23\"},\"end\":34609,\"start\":34579},{\"attributes\":{\"id\":\"formula_24\"},\"end\":37811,\"start\":37773},{\"attributes\":{\"id\":\"formula_25\"},\"end\":38265,\"start\":38248},{\"attributes\":{\"id\":\"formula_26\"},\"end\":38310,\"start\":38265},{\"attributes\":{\"id\":\"formula_27\"},\"end\":38399,\"start\":38344}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14463,\"start\":14456},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16604,\"start\":16597},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20850,\"start\":20843},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24602,\"start\":24595},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24948,\"start\":24941},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26540,\"start\":26532},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27700,\"start\":27693},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29615,\"start\":29608},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":38886,\"start\":38879},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":39995,\"start\":39988},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":41637,\"start\":41630},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":42063,\"start\":42056},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":43138,\"start\":43131},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44796,\"start\":44788},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":46127,\"start\":46119},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":46906,\"start\":46898},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":46987,\"start\":46980},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":48340,\"start\":48332},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":48623,\"start\":48615},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":48972,\"start\":48964},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49151,\"start\":49143},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":50324,\"start\":50316}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2592,\"start\":2580},{\"attributes\":{\"n\":\"2\"},\"end\":6910,\"start\":6898},{\"attributes\":{\"n\":\"3\"},\"end\":12276,\"start\":12265},{\"attributes\":{\"n\":\"3.1\"},\"end\":12284,\"start\":12279},{\"attributes\":{\"n\":\"3.2\"},\"end\":13348,\"start\":13312},{\"attributes\":{\"n\":\"3.3\"},\"end\":14781,\"start\":14744},{\"attributes\":{\"n\":\"3.4\"},\"end\":17187,\"start\":17162},{\"end\":17874,\"start\":17872},{\"end\":22774,\"start\":22767},{\"attributes\":{\"n\":\"4.2\"},\"end\":24232,\"start\":24227},{\"attributes\":{\"n\":\"4.3\"},\"end\":27106,\"start\":27097},{\"attributes\":{\"n\":\"4.4\"},\"end\":28184,\"start\":28176},{\"end\":29125,\"start\":29110},{\"attributes\":{\"n\":\"5\"},\"end\":30428,\"start\":30418},{\"end\":31582,\"start\":31526},{\"end\":34660,\"start\":34611},{\"end\":37471,\"start\":37418},{\"end\":38446,\"start\":38435},{\"end\":38823,\"start\":38789},{\"end\":39195,\"start\":39175},{\"end\":39263,\"start\":39198},{\"end\":40446,\"start\":40426},{\"end\":43437,\"start\":43414},{\"end\":44695,\"start\":44687},{\"end\":46260,\"start\":46242},{\"end\":47744,\"start\":47728},{\"end\":48610,\"start\":48593},{\"end\":49250,\"start\":49210},{\"end\":49927,\"start\":49884},{\"end\":51189,\"start\":51156},{\"end\":52448,\"start\":52401},{\"end\":52633,\"start\":52623},{\"end\":52756,\"start\":52746},{\"end\":53086,\"start\":53076},{\"end\":53773,\"start\":53763},{\"end\":53860,\"start\":53850},{\"end\":53895,\"start\":53885},{\"end\":53970,\"start\":53960},{\"end\":54164,\"start\":54154},{\"end\":54713,\"start\":54704},{\"end\":54933,\"start\":54924},{\"end\":55473,\"start\":55464},{\"end\":56575,\"start\":56566},{\"end\":57344,\"start\":57335},{\"end\":59215,\"start\":59206},{\"end\":60037,\"start\":60028},{\"end\":60293,\"start\":60284},{\"end\":60549,\"start\":60540},{\"end\":60970,\"start\":60960},{\"end\":61390,\"start\":61380},{\"end\":61765,\"start\":61755},{\"end\":62181,\"start\":62171},{\"end\":62863,\"start\":62853},{\"end\":63210,\"start\":63200},{\"end\":63557,\"start\":63547}]", "table": "[{\"end\":54922,\"start\":54760},{\"end\":55462,\"start\":55010},{\"end\":56564,\"start\":55516},{\"end\":57333,\"start\":56604},{\"end\":57466,\"start\":57417},{\"end\":59204,\"start\":57903},{\"end\":60026,\"start\":59268},{\"end\":60282,\"start\":60067},{\"end\":60538,\"start\":60323},{\"end\":60958,\"start\":60573},{\"end\":61378,\"start\":61004},{\"end\":61753,\"start\":61419},{\"end\":62169,\"start\":61792},{\"end\":62851,\"start\":62369},{\"end\":63198,\"start\":62891},{\"end\":63545,\"start\":63306},{\"end\":63864,\"start\":63592}]", "figure_caption": "[{\"end\":52744,\"start\":52635},{\"end\":53074,\"start\":52758},{\"end\":53188,\"start\":53088},{\"end\":53761,\"start\":53191},{\"end\":53848,\"start\":53775},{\"end\":53883,\"start\":53862},{\"end\":53958,\"start\":53897},{\"end\":54152,\"start\":53972},{\"end\":54270,\"start\":54166},{\"end\":54702,\"start\":54273},{\"end\":54760,\"start\":54715},{\"end\":55010,\"start\":54935},{\"end\":55516,\"start\":55475},{\"end\":56604,\"start\":56577},{\"end\":57417,\"start\":57346},{\"end\":57903,\"start\":57469},{\"end\":59268,\"start\":59217},{\"end\":60067,\"start\":60039},{\"end\":60323,\"start\":60295},{\"end\":60573,\"start\":60551},{\"end\":61004,\"start\":60973},{\"end\":61419,\"start\":61393},{\"end\":61792,\"start\":61768},{\"end\":62369,\"start\":62184},{\"end\":62891,\"start\":62866},{\"end\":63306,\"start\":63213},{\"end\":63592,\"start\":63560}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5254,\"start\":5246},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13747,\"start\":13739},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13775,\"start\":13767},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16976,\"start\":16968},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21486,\"start\":21478},{\"end\":21704,\"start\":21696},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21842,\"start\":21833},{\"end\":23179,\"start\":23171},{\"end\":23273,\"start\":23263},{\"end\":23320,\"start\":23311},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":23978,\"start\":23969},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28229,\"start\":28221},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28663,\"start\":28655},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":36739,\"start\":36731},{\"end\":38223,\"start\":38221},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":38661,\"start\":38653},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":50151,\"start\":50143},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":50607,\"start\":50599},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":51976,\"start\":51966},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":51990,\"start\":51981},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":52049,\"start\":52029},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":52060,\"start\":52051},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":52074,\"start\":52065}]", "bib_author_first_name": "[{\"end\":64712,\"start\":64711},{\"end\":64728,\"start\":64727},{\"end\":64737,\"start\":64736},{\"end\":64746,\"start\":64745},{\"end\":64748,\"start\":64747},{\"end\":64759,\"start\":64758},{\"end\":64767,\"start\":64766},{\"end\":64777,\"start\":64776},{\"end\":64797,\"start\":64792},{\"end\":65045,\"start\":65044},{\"end\":65057,\"start\":65056},{\"end\":65067,\"start\":65066},{\"end\":65080,\"start\":65079},{\"end\":65343,\"start\":65342},{\"end\":65353,\"start\":65352},{\"end\":65373,\"start\":65372},{\"end\":65772,\"start\":65771},{\"end\":65781,\"start\":65780},{\"end\":65783,\"start\":65782},{\"end\":65791,\"start\":65790},{\"end\":66121,\"start\":66120},{\"end\":66134,\"start\":66133},{\"end\":66145,\"start\":66144},{\"end\":66156,\"start\":66155},{\"end\":66167,\"start\":66166},{\"end\":66178,\"start\":66177},{\"end\":66180,\"start\":66179},{\"end\":66469,\"start\":66468},{\"end\":66471,\"start\":66470},{\"end\":66483,\"start\":66482},{\"end\":66492,\"start\":66491},{\"end\":66494,\"start\":66493},{\"end\":67012,\"start\":67011},{\"end\":67023,\"start\":67022},{\"end\":67330,\"start\":67329},{\"end\":67332,\"start\":67331},{\"end\":67344,\"start\":67343},{\"end\":67364,\"start\":67363},{\"end\":67373,\"start\":67372},{\"end\":67383,\"start\":67382},{\"end\":67773,\"start\":67772},{\"end\":67786,\"start\":67782},{\"end\":67795,\"start\":67794},{\"end\":67802,\"start\":67801},{\"end\":67804,\"start\":67803},{\"end\":68472,\"start\":68471},{\"end\":68483,\"start\":68482},{\"end\":68492,\"start\":68491},{\"end\":68501,\"start\":68500},{\"end\":68516,\"start\":68515},{\"end\":68529,\"start\":68528},{\"end\":68752,\"start\":68751},{\"end\":68765,\"start\":68764},{\"end\":69019,\"start\":69018},{\"end\":69021,\"start\":69020},{\"end\":69034,\"start\":69033},{\"end\":69048,\"start\":69047},{\"end\":69060,\"start\":69059},{\"end\":69354,\"start\":69353},{\"end\":69360,\"start\":69359},{\"end\":69369,\"start\":69368},{\"end\":69376,\"start\":69375},{\"end\":69709,\"start\":69708},{\"end\":69715,\"start\":69714},{\"end\":69722,\"start\":69721},{\"end\":69730,\"start\":69729},{\"end\":70050,\"start\":70049},{\"end\":70056,\"start\":70055},{\"end\":70063,\"start\":70062},{\"end\":70071,\"start\":70070},{\"end\":70369,\"start\":70368},{\"end\":70378,\"start\":70377},{\"end\":70385,\"start\":70384},{\"end\":70403,\"start\":70402},{\"end\":70405,\"start\":70404},{\"end\":70746,\"start\":70745},{\"end\":70755,\"start\":70754},{\"end\":70763,\"start\":70762},{\"end\":70765,\"start\":70764},{\"end\":70773,\"start\":70772},{\"end\":70987,\"start\":70986},{\"end\":70989,\"start\":70988},{\"end\":70996,\"start\":70995},{\"end\":71006,\"start\":71005},{\"end\":71018,\"start\":71017},{\"end\":71020,\"start\":71019},{\"end\":71027,\"start\":71026},{\"end\":71036,\"start\":71035},{\"end\":71052,\"start\":71051},{\"end\":71058,\"start\":71057},{\"end\":71070,\"start\":71069},{\"end\":71072,\"start\":71071},{\"end\":71084,\"start\":71083},{\"end\":71093,\"start\":71092},{\"end\":71105,\"start\":71104},{\"end\":71116,\"start\":71115},{\"end\":71127,\"start\":71126},{\"end\":71137,\"start\":71136},{\"end\":71145,\"start\":71144},{\"end\":71593,\"start\":71592},{\"end\":71603,\"start\":71602},{\"end\":71806,\"start\":71805},{\"end\":71812,\"start\":71811},{\"end\":71823,\"start\":71819},{\"end\":71831,\"start\":71830},{\"end\":71833,\"start\":71832},{\"end\":72163,\"start\":72162},{\"end\":72169,\"start\":72168},{\"end\":72180,\"start\":72176},{\"end\":72188,\"start\":72187},{\"end\":72555,\"start\":72554},{\"end\":72561,\"start\":72560},{\"end\":72570,\"start\":72569},{\"end\":72578,\"start\":72577},{\"end\":72588,\"start\":72584},{\"end\":72596,\"start\":72595},{\"end\":72936,\"start\":72935},{\"end\":72942,\"start\":72941},{\"end\":72944,\"start\":72943},{\"end\":72951,\"start\":72950},{\"end\":72959,\"start\":72958},{\"end\":72961,\"start\":72960},{\"end\":73390,\"start\":73389},{\"end\":73396,\"start\":73395},{\"end\":73404,\"start\":73403},{\"end\":73412,\"start\":73411},{\"end\":73419,\"start\":73418},{\"end\":73426,\"start\":73425},{\"end\":73435,\"start\":73434},{\"end\":73835,\"start\":73834},{\"end\":73848,\"start\":73847},{\"end\":74079,\"start\":74078},{\"end\":74090,\"start\":74089},{\"end\":74101,\"start\":74100},{\"end\":74113,\"start\":74112},{\"end\":74376,\"start\":74375},{\"end\":74386,\"start\":74385},{\"end\":74396,\"start\":74395},{\"end\":74579,\"start\":74578},{\"end\":74587,\"start\":74586},{\"end\":74594,\"start\":74593},{\"end\":74601,\"start\":74600},{\"end\":74610,\"start\":74609},{\"end\":74620,\"start\":74619},{\"end\":74933,\"start\":74932},{\"end\":74953,\"start\":74952},{\"end\":74965,\"start\":74964},{\"end\":74981,\"start\":74980},{\"end\":74983,\"start\":74982},{\"end\":75179,\"start\":75178},{\"end\":75196,\"start\":75195},{\"end\":75209,\"start\":75208},{\"end\":75219,\"start\":75218},{\"end\":75485,\"start\":75484},{\"end\":75495,\"start\":75494},{\"end\":75497,\"start\":75496},{\"end\":75504,\"start\":75503},{\"end\":75506,\"start\":75505},{\"end\":75519,\"start\":75518},{\"end\":75901,\"start\":75900},{\"end\":75911,\"start\":75910},{\"end\":75913,\"start\":75912},{\"end\":75920,\"start\":75919},{\"end\":75922,\"start\":75921},{\"end\":75935,\"start\":75934},{\"end\":76192,\"start\":76191},{\"end\":76201,\"start\":76200},{\"end\":76587,\"start\":76586},{\"end\":76594,\"start\":76593},{\"end\":76868,\"start\":76867},{\"end\":77211,\"start\":77210},{\"end\":77220,\"start\":77219},{\"end\":77412,\"start\":77411},{\"end\":77424,\"start\":77423},{\"end\":77426,\"start\":77425},{\"end\":77998,\"start\":77997},{\"end\":78014,\"start\":78013},{\"end\":78025,\"start\":78024},{\"end\":78040,\"start\":78039},{\"end\":78427,\"start\":78426},{\"end\":78435,\"start\":78434},{\"end\":78441,\"start\":78440},{\"end\":78443,\"start\":78442},{\"end\":78453,\"start\":78452},{\"end\":78455,\"start\":78454},{\"end\":78790,\"start\":78789},{\"end\":78797,\"start\":78796},{\"end\":78805,\"start\":78804},{\"end\":78811,\"start\":78810},{\"end\":78818,\"start\":78817}]", "bib_author_last_name": "[{\"end\":64725,\"start\":64713},{\"end\":64734,\"start\":64729},{\"end\":64743,\"start\":64738},{\"end\":64756,\"start\":64749},{\"end\":64764,\"start\":64760},{\"end\":64774,\"start\":64768},{\"end\":64790,\"start\":64778},{\"end\":64805,\"start\":64798},{\"end\":65054,\"start\":65046},{\"end\":65064,\"start\":65058},{\"end\":65077,\"start\":65068},{\"end\":65090,\"start\":65081},{\"end\":65350,\"start\":65344},{\"end\":65370,\"start\":65354},{\"end\":65383,\"start\":65374},{\"end\":65778,\"start\":65773},{\"end\":65788,\"start\":65784},{\"end\":65798,\"start\":65792},{\"end\":66131,\"start\":66122},{\"end\":66142,\"start\":66135},{\"end\":66153,\"start\":66146},{\"end\":66164,\"start\":66157},{\"end\":66175,\"start\":66168},{\"end\":66188,\"start\":66181},{\"end\":66480,\"start\":66472},{\"end\":66489,\"start\":66484},{\"end\":66508,\"start\":66495},{\"end\":67020,\"start\":67013},{\"end\":67030,\"start\":67024},{\"end\":67341,\"start\":67333},{\"end\":67361,\"start\":67345},{\"end\":67370,\"start\":67365},{\"end\":67380,\"start\":67374},{\"end\":67391,\"start\":67384},{\"end\":67780,\"start\":67774},{\"end\":67792,\"start\":67787},{\"end\":67799,\"start\":67796},{\"end\":67814,\"start\":67805},{\"end\":68480,\"start\":68473},{\"end\":68489,\"start\":68484},{\"end\":68498,\"start\":68493},{\"end\":68513,\"start\":68502},{\"end\":68526,\"start\":68517},{\"end\":68539,\"start\":68530},{\"end\":68762,\"start\":68753},{\"end\":68775,\"start\":68766},{\"end\":69031,\"start\":69022},{\"end\":69045,\"start\":69035},{\"end\":69057,\"start\":69049},{\"end\":69066,\"start\":69061},{\"end\":69357,\"start\":69355},{\"end\":69366,\"start\":69361},{\"end\":69373,\"start\":69370},{\"end\":69380,\"start\":69377},{\"end\":69712,\"start\":69710},{\"end\":69719,\"start\":69716},{\"end\":69727,\"start\":69723},{\"end\":69739,\"start\":69731},{\"end\":70053,\"start\":70051},{\"end\":70060,\"start\":70057},{\"end\":70068,\"start\":70064},{\"end\":70080,\"start\":70072},{\"end\":70375,\"start\":70370},{\"end\":70382,\"start\":70379},{\"end\":70400,\"start\":70386},{\"end\":70416,\"start\":70406},{\"end\":70752,\"start\":70747},{\"end\":70760,\"start\":70756},{\"end\":70770,\"start\":70766},{\"end\":70779,\"start\":70774},{\"end\":70993,\"start\":70990},{\"end\":71003,\"start\":70997},{\"end\":71015,\"start\":71007},{\"end\":71024,\"start\":71021},{\"end\":71033,\"start\":71028},{\"end\":71049,\"start\":71037},{\"end\":71055,\"start\":71053},{\"end\":71067,\"start\":71059},{\"end\":71081,\"start\":71073},{\"end\":71090,\"start\":71085},{\"end\":71102,\"start\":71094},{\"end\":71113,\"start\":71106},{\"end\":71124,\"start\":71117},{\"end\":71134,\"start\":71128},{\"end\":71142,\"start\":71138},{\"end\":71151,\"start\":71146},{\"end\":71600,\"start\":71594},{\"end\":71613,\"start\":71604},{\"end\":71809,\"start\":71807},{\"end\":71817,\"start\":71813},{\"end\":71828,\"start\":71824},{\"end\":71844,\"start\":71834},{\"end\":72166,\"start\":72164},{\"end\":72174,\"start\":72170},{\"end\":72185,\"start\":72181},{\"end\":72199,\"start\":72189},{\"end\":72558,\"start\":72556},{\"end\":72567,\"start\":72562},{\"end\":72575,\"start\":72571},{\"end\":72582,\"start\":72579},{\"end\":72593,\"start\":72589},{\"end\":72607,\"start\":72597},{\"end\":72939,\"start\":72937},{\"end\":72948,\"start\":72945},{\"end\":72956,\"start\":72952},{\"end\":72965,\"start\":72962},{\"end\":73393,\"start\":73391},{\"end\":73401,\"start\":73397},{\"end\":73409,\"start\":73405},{\"end\":73416,\"start\":73413},{\"end\":73423,\"start\":73420},{\"end\":73432,\"start\":73427},{\"end\":73439,\"start\":73436},{\"end\":73845,\"start\":73836},{\"end\":73856,\"start\":73849},{\"end\":74087,\"start\":74080},{\"end\":74098,\"start\":74091},{\"end\":74110,\"start\":74102},{\"end\":74122,\"start\":74114},{\"end\":74383,\"start\":74377},{\"end\":74393,\"start\":74387},{\"end\":74405,\"start\":74397},{\"end\":74584,\"start\":74580},{\"end\":74591,\"start\":74588},{\"end\":74598,\"start\":74595},{\"end\":74607,\"start\":74602},{\"end\":74617,\"start\":74611},{\"end\":74625,\"start\":74621},{\"end\":74950,\"start\":74934},{\"end\":74962,\"start\":74954},{\"end\":74978,\"start\":74966},{\"end\":74992,\"start\":74984},{\"end\":75193,\"start\":75180},{\"end\":75206,\"start\":75197},{\"end\":75216,\"start\":75210},{\"end\":75226,\"start\":75220},{\"end\":75492,\"start\":75486},{\"end\":75501,\"start\":75498},{\"end\":75516,\"start\":75507},{\"end\":75525,\"start\":75520},{\"end\":75908,\"start\":75902},{\"end\":75917,\"start\":75914},{\"end\":75932,\"start\":75923},{\"end\":75941,\"start\":75936},{\"end\":76198,\"start\":76193},{\"end\":76207,\"start\":76202},{\"end\":76591,\"start\":76588},{\"end\":76601,\"start\":76595},{\"end\":76875,\"start\":76869},{\"end\":77217,\"start\":77212},{\"end\":77226,\"start\":77221},{\"end\":77421,\"start\":77413},{\"end\":77432,\"start\":77427},{\"end\":78011,\"start\":77999},{\"end\":78022,\"start\":78015},{\"end\":78037,\"start\":78026},{\"end\":78053,\"start\":78041},{\"end\":78432,\"start\":78428},{\"end\":78438,\"start\":78436},{\"end\":78450,\"start\":78444},{\"end\":78460,\"start\":78456},{\"end\":78794,\"start\":78791},{\"end\":78802,\"start\":78798},{\"end\":78808,\"start\":78806},{\"end\":78815,\"start\":78812},{\"end\":78822,\"start\":78819}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1606.04474\",\"id\":\"b0\"},\"end\":65042,\"start\":64652},{\"attributes\":{\"doi\":\"arXiv:1907.02893\",\"id\":\"b1\"},\"end\":65274,\"start\":65044},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":53979606},\"end\":65737,\"start\":65276},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49744838},\"end\":66073,\"start\":65739},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8577357},\"end\":66375,\"start\":66075},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1066490},\"end\":66919,\"start\":66377},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":68222714},\"end\":67278,\"start\":66921},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":81978372},\"end\":67688,\"start\":67280},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52967399},\"end\":68428,\"start\":67690},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":108301245},\"end\":68749,\"start\":68430},{\"attributes\":{\"doi\":\"arXiv:2007.01434\",\"id\":\"b10\"},\"end\":68955,\"start\":68751},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49343170},\"end\":69305,\"start\":68957},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":69632,\"start\":69307},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":19819905},\"end\":69973,\"start\":69634},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":19819905},\"end\":70324,\"start\":69975},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9433631},\"end\":70688,\"start\":70326},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":220363892},\"end\":70984,\"start\":70690},{\"attributes\":{\"doi\":\"arXiv:2012.07421\",\"id\":\"b17\"},\"end\":71521,\"start\":70986},{\"attributes\":{\"id\":\"b18\"},\"end\":71753,\"start\":71523},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6037691},\"end\":72095,\"start\":71755},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1883787},\"end\":72507,\"start\":72097},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":59553457},\"end\":72876,\"start\":72509},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52833113},\"end\":73312,\"start\":72878},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52956008},\"end\":73783,\"start\":73314},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":37308416},\"end\":74076,\"start\":73785},{\"attributes\":{\"id\":\"b25\"},\"end\":74332,\"start\":74078},{\"attributes\":{\"doi\":\"arXiv: Learning\",\"id\":\"b26\"},\"end\":74524,\"start\":74334},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":54458071},\"end\":74895,\"start\":74526},{\"attributes\":{\"id\":\"b28\"},\"end\":75133,\"start\":74897},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":67220582},\"end\":75357,\"start\":75135},{\"attributes\":{\"doi\":\"arXiv:1911.08731\",\"id\":\"b30\"},\"end\":75773,\"start\":75359},{\"attributes\":{\"doi\":\"arXiv:1911.08731\",\"id\":\"b31\"},\"end\":76189,\"start\":75775},{\"attributes\":{\"doi\":\"arXiv:1711.11443\",\"id\":\"b32\"},\"end\":76522,\"start\":76191},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12453047},\"end\":76808,\"start\":76524},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13997424},\"end\":77189,\"start\":76810},{\"attributes\":{\"id\":\"b35\"},\"end\":77378,\"start\":77191},{\"attributes\":{\"doi\":\"10.1109/CVPR.2011.5995347\",\"id\":\"b36\",\"matched_paper_id\":2777306},\"end\":77938,\"start\":77380},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2928248},\"end\":78347,\"start\":77940},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":173188134},\"end\":78727,\"start\":78349},{\"attributes\":{\"doi\":\"arXiv:2001.00677\",\"id\":\"b39\"},\"end\":78996,\"start\":78729},{\"attributes\":{\"id\":\"b40\"},\"end\":79088,\"start\":78998},{\"attributes\":{\"id\":\"b41\"},\"end\":79183,\"start\":79090},{\"attributes\":{\"id\":\"b42\"},\"end\":79228,\"start\":79185},{\"attributes\":{\"id\":\"b43\"},\"end\":79789,\"start\":79230}]", "bib_title": "[{\"end\":65340,\"start\":65276},{\"end\":65769,\"start\":65739},{\"end\":66118,\"start\":66075},{\"end\":66466,\"start\":66377},{\"end\":67009,\"start\":66921},{\"end\":67327,\"start\":67280},{\"end\":67770,\"start\":67690},{\"end\":68469,\"start\":68430},{\"end\":69016,\"start\":68957},{\"end\":69351,\"start\":69307},{\"end\":69706,\"start\":69634},{\"end\":70047,\"start\":69975},{\"end\":70366,\"start\":70326},{\"end\":70743,\"start\":70690},{\"end\":71803,\"start\":71755},{\"end\":72160,\"start\":72097},{\"end\":72552,\"start\":72509},{\"end\":72933,\"start\":72878},{\"end\":73387,\"start\":73314},{\"end\":73832,\"start\":73785},{\"end\":74576,\"start\":74526},{\"end\":75176,\"start\":75135},{\"end\":76584,\"start\":76524},{\"end\":76865,\"start\":76810},{\"end\":77409,\"start\":77380},{\"end\":77995,\"start\":77940},{\"end\":78424,\"start\":78349}]", "bib_author": "[{\"end\":64727,\"start\":64711},{\"end\":64736,\"start\":64727},{\"end\":64745,\"start\":64736},{\"end\":64758,\"start\":64745},{\"end\":64766,\"start\":64758},{\"end\":64776,\"start\":64766},{\"end\":64792,\"start\":64776},{\"end\":64807,\"start\":64792},{\"end\":65056,\"start\":65044},{\"end\":65066,\"start\":65056},{\"end\":65079,\"start\":65066},{\"end\":65092,\"start\":65079},{\"end\":65352,\"start\":65342},{\"end\":65372,\"start\":65352},{\"end\":65385,\"start\":65372},{\"end\":65780,\"start\":65771},{\"end\":65790,\"start\":65780},{\"end\":65800,\"start\":65790},{\"end\":66133,\"start\":66120},{\"end\":66144,\"start\":66133},{\"end\":66155,\"start\":66144},{\"end\":66166,\"start\":66155},{\"end\":66177,\"start\":66166},{\"end\":66190,\"start\":66177},{\"end\":66482,\"start\":66468},{\"end\":66491,\"start\":66482},{\"end\":66510,\"start\":66491},{\"end\":67022,\"start\":67011},{\"end\":67032,\"start\":67022},{\"end\":67343,\"start\":67329},{\"end\":67363,\"start\":67343},{\"end\":67372,\"start\":67363},{\"end\":67382,\"start\":67372},{\"end\":67393,\"start\":67382},{\"end\":67782,\"start\":67772},{\"end\":67794,\"start\":67782},{\"end\":67801,\"start\":67794},{\"end\":67816,\"start\":67801},{\"end\":68482,\"start\":68471},{\"end\":68491,\"start\":68482},{\"end\":68500,\"start\":68491},{\"end\":68515,\"start\":68500},{\"end\":68528,\"start\":68515},{\"end\":68541,\"start\":68528},{\"end\":68764,\"start\":68751},{\"end\":68777,\"start\":68764},{\"end\":69033,\"start\":69018},{\"end\":69047,\"start\":69033},{\"end\":69059,\"start\":69047},{\"end\":69068,\"start\":69059},{\"end\":69359,\"start\":69353},{\"end\":69368,\"start\":69359},{\"end\":69375,\"start\":69368},{\"end\":69382,\"start\":69375},{\"end\":69714,\"start\":69708},{\"end\":69721,\"start\":69714},{\"end\":69729,\"start\":69721},{\"end\":69741,\"start\":69729},{\"end\":70055,\"start\":70049},{\"end\":70062,\"start\":70055},{\"end\":70070,\"start\":70062},{\"end\":70082,\"start\":70070},{\"end\":70377,\"start\":70368},{\"end\":70384,\"start\":70377},{\"end\":70402,\"start\":70384},{\"end\":70418,\"start\":70402},{\"end\":70754,\"start\":70745},{\"end\":70762,\"start\":70754},{\"end\":70772,\"start\":70762},{\"end\":70781,\"start\":70772},{\"end\":70995,\"start\":70986},{\"end\":71005,\"start\":70995},{\"end\":71017,\"start\":71005},{\"end\":71026,\"start\":71017},{\"end\":71035,\"start\":71026},{\"end\":71051,\"start\":71035},{\"end\":71057,\"start\":71051},{\"end\":71069,\"start\":71057},{\"end\":71083,\"start\":71069},{\"end\":71092,\"start\":71083},{\"end\":71104,\"start\":71092},{\"end\":71115,\"start\":71104},{\"end\":71126,\"start\":71115},{\"end\":71136,\"start\":71126},{\"end\":71144,\"start\":71136},{\"end\":71153,\"start\":71144},{\"end\":71602,\"start\":71592},{\"end\":71615,\"start\":71602},{\"end\":71811,\"start\":71805},{\"end\":71819,\"start\":71811},{\"end\":71830,\"start\":71819},{\"end\":71846,\"start\":71830},{\"end\":72168,\"start\":72162},{\"end\":72176,\"start\":72168},{\"end\":72187,\"start\":72176},{\"end\":72201,\"start\":72187},{\"end\":72560,\"start\":72554},{\"end\":72569,\"start\":72560},{\"end\":72577,\"start\":72569},{\"end\":72584,\"start\":72577},{\"end\":72595,\"start\":72584},{\"end\":72609,\"start\":72595},{\"end\":72941,\"start\":72935},{\"end\":72950,\"start\":72941},{\"end\":72958,\"start\":72950},{\"end\":72967,\"start\":72958},{\"end\":73395,\"start\":73389},{\"end\":73403,\"start\":73395},{\"end\":73411,\"start\":73403},{\"end\":73418,\"start\":73411},{\"end\":73425,\"start\":73418},{\"end\":73434,\"start\":73425},{\"end\":73441,\"start\":73434},{\"end\":73847,\"start\":73834},{\"end\":73858,\"start\":73847},{\"end\":74089,\"start\":74078},{\"end\":74100,\"start\":74089},{\"end\":74112,\"start\":74100},{\"end\":74124,\"start\":74112},{\"end\":74385,\"start\":74375},{\"end\":74395,\"start\":74385},{\"end\":74407,\"start\":74395},{\"end\":74586,\"start\":74578},{\"end\":74593,\"start\":74586},{\"end\":74600,\"start\":74593},{\"end\":74609,\"start\":74600},{\"end\":74619,\"start\":74609},{\"end\":74627,\"start\":74619},{\"end\":74952,\"start\":74932},{\"end\":74964,\"start\":74952},{\"end\":74980,\"start\":74964},{\"end\":74994,\"start\":74980},{\"end\":75195,\"start\":75178},{\"end\":75208,\"start\":75195},{\"end\":75218,\"start\":75208},{\"end\":75228,\"start\":75218},{\"end\":75494,\"start\":75484},{\"end\":75503,\"start\":75494},{\"end\":75518,\"start\":75503},{\"end\":75527,\"start\":75518},{\"end\":75910,\"start\":75900},{\"end\":75919,\"start\":75910},{\"end\":75934,\"start\":75919},{\"end\":75943,\"start\":75934},{\"end\":76200,\"start\":76191},{\"end\":76209,\"start\":76200},{\"end\":76593,\"start\":76586},{\"end\":76603,\"start\":76593},{\"end\":76877,\"start\":76867},{\"end\":77219,\"start\":77210},{\"end\":77228,\"start\":77219},{\"end\":77423,\"start\":77411},{\"end\":77434,\"start\":77423},{\"end\":78013,\"start\":77997},{\"end\":78024,\"start\":78013},{\"end\":78039,\"start\":78024},{\"end\":78055,\"start\":78039},{\"end\":78434,\"start\":78426},{\"end\":78440,\"start\":78434},{\"end\":78452,\"start\":78440},{\"end\":78462,\"start\":78452},{\"end\":78796,\"start\":78789},{\"end\":78804,\"start\":78796},{\"end\":78810,\"start\":78804},{\"end\":78817,\"start\":78810},{\"end\":78824,\"start\":78817}]", "bib_venue": "[{\"end\":65915,\"start\":65866},{\"end\":66669,\"start\":66598},{\"end\":68087,\"start\":67960},{\"end\":72310,\"start\":72264},{\"end\":73108,\"start\":73046},{\"end\":73556,\"start\":73507},{\"end\":77020,\"start\":76957},{\"end\":77633,\"start\":77553},{\"end\":64709,\"start\":64652},{\"end\":65135,\"start\":65108},{\"end\":65482,\"start\":65385},{\"end\":65864,\"start\":65800},{\"end\":66206,\"start\":66190},{\"end\":66596,\"start\":66510},{\"end\":67091,\"start\":67032},{\"end\":67467,\"start\":67393},{\"end\":67958,\"start\":67816},{\"end\":68574,\"start\":68541},{\"end\":68829,\"start\":68793},{\"end\":69112,\"start\":69068},{\"end\":69452,\"start\":69382},{\"end\":69785,\"start\":69741},{\"end\":70126,\"start\":70082},{\"end\":70488,\"start\":70418},{\"end\":70819,\"start\":70781},{\"end\":71222,\"start\":71169},{\"end\":71590,\"start\":71523},{\"end\":71906,\"start\":71846},{\"end\":72262,\"start\":72201},{\"end\":72673,\"start\":72609},{\"end\":73044,\"start\":72967},{\"end\":73505,\"start\":73441},{\"end\":73907,\"start\":73858},{\"end\":74173,\"start\":74124},{\"end\":74373,\"start\":74334},{\"end\":74691,\"start\":74627},{\"end\":74930,\"start\":74897},{\"end\":75232,\"start\":75228},{\"end\":75482,\"start\":75359},{\"end\":75898,\"start\":75775},{\"end\":76334,\"start\":76225},{\"end\":76641,\"start\":76603},{\"end\":76955,\"start\":76877},{\"end\":77208,\"start\":77191},{\"end\":77551,\"start\":77459},{\"end\":78125,\"start\":78055},{\"end\":78511,\"start\":78462},{\"end\":78787,\"start\":78729},{\"end\":79033,\"start\":78998},{\"end\":79125,\"start\":79090},{\"end\":79205,\"start\":79185},{\"end\":79368,\"start\":79230}]"}}}, "year": 2023, "month": 12, "day": 17}