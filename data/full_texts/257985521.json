{"id": 257985521, "updated": "2023-12-01 15:01:23.195", "metadata": {"title": "Neural Fields Meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes", "authors": "[{\"first\":\"Zian\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Tianchang\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Shengyu\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Jacob\",\"last\":\"Munkberg\",\"middle\":[]},{\"first\":\"Jon\",\"last\":\"Hasselgren\",\"middle\":[]},{\"first\":\"Zan\",\"last\":\"Gojcic\",\"middle\":[]},{\"first\":\"Wenzheng\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Sanja\",\"last\":\"Fidler\",\"middle\":[]}]", "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2023, "month": 6, "day": 1}, "abstract": "Reconstruction and intrinsic decomposition of scenes from captured imagery would enable many applications such as relighting and virtual object insertion. Recent NeRF based methods achieve impressive fidelity of 3D reconstruction, but bake the lighting and shadows into the radiance field, while mesh-based methods that facilitate intrinsic decomposition through differentiable rendering have not yet scaled to the complexity and scale of outdoor scenes. We present a novel inverse rendering framework for large urban scenes capable of jointly reconstructing the scene geometry, spatially-varying materials, and HDR lighting from a set of posed RGB images with optional depth. Specifically, we use a neural field to account for the primary rays, and use an explicit mesh (reconstructed from the underlying neural field) for modeling secondary rays that produce higher-order lighting effects such as cast shadows. By faithfully disentangling complex geometry and materials from lighting effects, our method enables photorealistic relighting with specular and shadow effects on several outdoor datasets. Moreover, it supports physics-based scene manipulations such as virtual object insertion with ray-traced shadow casting.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/WangSGHMHGCF23", "doi": "10.1109/cvpr52729.2023.00809"}}, "content": {"source": {"pdf_hash": "0f912dc39d7a013471a35a559f9c44647e44f658", "pdf_src": "IEEE", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.03266v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "3f4733690eac2496a7275e2e4ec0aab1bdba1f8c", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/0f912dc39d7a013471a35a559f9c44647e44f658.txt", "contents": "\nNeural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes\n\n\nZian Wang \nNVIDIA\n\n\nUniversity of Toronto\n\n\nVector Institute\n\n\nTianchang Shen \nNVIDIA\n\n\nUniversity of Toronto\n\n\nVector Institute\n\n\nJun Gao \nNVIDIA\n\n\nUniversity of Toronto\n\n\nVector Institute\n\n\nShengyu Huang \nNVIDIA\n\n\nETH Z\u00fcrich\n\n\nJacob Munkberg \nNVIDIA\n\n\nJon Hasselgren \nNVIDIA\n\n\nZan Gojcic \nNVIDIA\n\n\nWenzheng Chen \nNVIDIA\n\n\nUniversity of Toronto\n\n\nVector Institute\n\n\nSanja Fidler \nNVIDIA\n\n\nUniversity of Toronto\n\n\nVector Institute\n\n\nNeural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes\n10.1109/CVPR52729.2023.00809\nFigure 1. We present FEGR, an approach for reconstructing scene geometry and recovering intrinsic properties of the scene from posed camera images. Our approach works both for single and multi-illumination captured data. FEGR enables various downstream applications such as VR and AR where users may want to control the lighting of the environment and insert desired 3D objects into the scene.AbstractReconstruction and intrinsic decomposition of scenes from captured imagery would enable many applications such as relighting and virtual object insertion. Recent NeRF based methods achieve impressive fidelity of 3D reconstruction, but bake the lighting and shadows into the radiance field, while mesh-based methods that facilitate intrinsic decomposition through differentiable rendering have not yet scaled to the complexity and scale of outdoor scenes. We present a novel inverse rendering framework for large urban scenes capable of jointly reconstructing the scene geometry, spatially-varying materials, and HDR lighting from a set of posed RGB images with optional depth. Specifically, we use a neural field to account for the primary rays, and use an explicit mesh (reconstructed from the underlying neural field) for modeling secondary rays that produce higher-order lighting effects such as cast shadows. By faithfully disentangling complex geometry and materials from lighting effects, our method enables photorealistic relighting with specular and shadow effects on several outdoor datasets. Moreover, it supports physics-based scene manipulations such as virtual object insertion with ray-traced shadow casting.\n\nIntroduction\n\nReconstructing high fidelity 3D scenes from captured imagery is an important utility of scaleable 3D content creation. However, for the reconstructed environments to serve as \"digital twins\" for downstream applications such as augmented reality and gaming, we require that these environments are compatible with modern graphics pipeline and can be rendered with user-specified lighting. This means that we not only need to reconstruct 3D geometry and texture but also recover the intrinsic properties of the scene such as material properties and lighting information. This is an ill-posed, challenging problem oftentimes referred to as inverse rendering [1].\n\nNeural radiance fields (NeRFs) [34] have recently emerged as a powerful neural reconstruction approach that enables photo-realistic novel-view synthesis. NeRFs can be reconstructed from a set of posed camera images in a matter of minutes [14,35,43] and have been shown to scale to room-level scenes and beyond [45,48,55], making them an attractive representation for augmented/virtual reality and generation of digital twins. However, in NeRF, the intrinsic properties of the scene are not separated from the effect of incident light. As a result, novel views can only be synthesised under fixed lighting conditions present in the input images, i.e. a NeRF cannot be relighted [42].\n\nWhile NeRF can be extended into a full inverse rendering formulation [3], this requires computing the volume rendering integral when tracing multiple ray bounces. This quickly becomes intractable due to the underlying volumetric representation. Specifically, in order to estimate the secondary rays, the volumetric density field of NeRF would have to be queried along the path from each surface point to all the light sources, scaling with O(nm) per point, where n denotes the number of samples along each ray and m is the number of light sources or Monte Carlo (MC) samples in the case of global illumination. To restrict the incurred computational cost, prior works have mostly focused on the single object setting and often assume a single (known) illumination source [42]. Additionally, they forgo the volumetric rendering of secondary rays and instead approximate the direct/indirect lighting through a visibility MLP [42,64].\n\nIn contrast to NeRF, the explicit mesh-based representation allows for very efficient rendering. With a known mesh topology, the estimation of both primary and secondary rays is carried out using ray-mesh intersection (O(m)) queries that can be efficiently computed using highly-optimized libraries such as OptiX [39]. However, inverse rendering methods based on explicit mesh representations either assume a fixed mesh topology [13], or recover the surface mesh via an SDF defined on a volumetric grid [36] and are thus bounded by the grid resolution. Insofar, these methods have been shown to produce high-quality results only for the smaller, object-centric scenes.\n\nIn this work, we combine the advantages of the neural field (NeRF) and explicit (mesh) representations and propose FEGR 1 , a new hybrid-rendering pipeline for inverse rendering of large urban scenes. Specifically, we represent the intrinsic properties of the scene using a neural field and estimate the primary rays (G-buffer) with volumetric rendering. To model the secondary rays that produce higher-order lighting effects such as specular highlights and cast shadows, we convert the neural field to an explicit representation and preform physics-based rendering. The underlying neural field enables us to represent high-resolution details, while ray tracing secondary rays using the explicit mesh reduces the computational complexity. The proposed hybridrendering is fully differentiable an can be embedded into an optimization scheme that allows us to estimate 3D spatiallyvarying material properties, geometry, and HDR lighting of the scene from a set of posed camera images 2 . By modeling the HDR properties of the scene, our representation is also well suited for AR applications such as virtual object inser-tion that require spatially-varying lighting to cast shadows in a physically correct way.\n\nWe summarize our contributions as follows: \u2022 We propose a novel neural field representation that decomposes scene into geometry, spatially varying materials, and HDR lighting.\n\n\u2022 To achieve efficient ray-tracing within a neural scene representation, we introduce a hybrid renderer that renders primary rays through volumetric rendering, and models the secondary rays using physics-based rendering. This enables high-quality inverse rendering of large urban scenes.\n\n\u2022 We model the HDR lighting and material properties of the scene, making our representation well suited for downstream applications such as relighting and virtual object insertion with cast shadows.\n\nFEGR significantly outperforms state-of-the-art in terms of novel-view synthesis under varying lighting conditions on the NeRF-OSR dataset [40]. We also show qualitative results on a single-illumination capture of an urban environment, collected by an autonomous vehicle. Moreover, we show the intrinsic rendering results, and showcase virtual object insertion as an application. Finally, we conduct a user study, in which the results of our method are significantly preferred to those of the baselines.\n\n\nRelated Work\n\nInverse Rendering is a fundamental task in computer vision. The seminal work by Barrow and Tenenbaum [1] aimed to understand the intrinsic scene properties including reflectance, lighting, and geometry from captured imagery. Considering the ill-posed nature [24] of this challenging task, early works resided to tackle the subtask known as intrinsic image decomposition, that aims to decompose an image into diffuse albedo and shading. These methods are mostly optimization-based and rely on hand-crafted priors [10,17,24,65]. In the deep learning era, learning-based methods [2,9,22,27,28,30,41,[52][53][54]57] replaced the classic optimisation pipeline and learn the intrinsic decomposition in a data-driven manner, but typically require ground truth supervision. However, acquiring ground truth intrinsic decomposition in the real world is extremely challenging. Learning-based methods thus often train on synthetic datasets [27,28,30,41,53], and may suffer from a domain gap between synthetic and real captures. In addition, these methods are limited to 2.5D prediction, i.e. 2D intrinsic images and a normal map, thus are unable to reconstruct the full 3D scene. Recent advances in differentiable rendering [38] and neural volume rendering [34] revive the optimization paradigm by enabling direct optimization of the 3D scene representation [4-8, 18, 23, 36, 60, 61, 63]. However, these works mostly focus on a single object setting and ignore higher-order lighting effects such as cast shadows.  Figure 2. Overview of FEGR. Given a set of posed camera images, FEGR estimates the geometry, spatially varying materials, and HDR lighting of the underlying scene. We model the intrinsic properties of the scene using a neural intrinsic field and use an HDR Sky Dome to represent the lighting. Our Hybrid Deferred Renderer models the primary rays with volumetric rendering of the neural field, while the secondary rays are ray-traced using an explicit mesh reconstructed from the SD field. By modeling the HDR properties of the scene FEGR can support several scene manipulations including novel-view synthesis, scene relighting, and AR.\n\nNeural Scene Representation for inverse rendering mostly falls into two categories: explicit textured mesh [12,13,18,36,37,59] and neural fields [4,6,60,61,63]. Explicit mesh representations [18,36] are compatible with graphics pipeline and naturally benefit from classic graphics techniques. These methods show impressive performance under single-object setting but suffers from bounded resolution when scaling up to a larger scene extent. With the impressive image synthesis quality demonstrated by neural fields [34], recent works on inverse rendering also adopt neural fields as representation for scene intrinsic properties [4,6,50,56,60,61,63]. Despite the impressive results for primary ray appearance, it remains an open challenge for neural fields to represent higher order lighting effects such as cast shadows via ray-tracing. To reduce the complexity of ray-tracing in neural fields, prior works explore using MLP to encode visibility field [26,42] or Spherical Gaussian visibility [64], but typically limited to object-level or low-frequency effects. The closest setup to our work is NeRF-OSR [40] that works on outdoor scene-level inverse rendering. It uses a network to represent shadows and relys on multiple illumination to disentangle shadows from albedo, but usually cannot recover sharp shadow boundaries. Related to our work are also methods that factorize the appearance changes through latent codes [31,33]. These methods can modify scene appearance by interpolation of the latent codes, but do not offer explicit control of lighting conditions.\n\nLighting Estimation is a subtask of inverse rendering which aims to understand the lighting distribution across the scene, typically with the goal of photorealistic virtual object insertion. Existing work on lighting estimation is usually learning-based, adopting feed-forward neural networks given the input of a single image [16,19,20,25,29,46,52,66]. For outdoor scenes, prior work investigates network designs to predict lighting representations such as a HDR sky model [19,20,58], spatially-varying environment map [46,66] and a lighting volume [52]. The key challenge for outdoor lighting estimation is to correctly estimate the peak direction and intensity of the sky, which is usually the location of the sun. This is a challenging ill-posed task where a single image input may be insufficient to produce accurate results. Recent optimization-based inverse rendering works jointly optimize lighting from multi-view images [6,18,36,61], however their primary purpose of lighting is to serve the joint optimization framework for recovering material properties. Lighting representations are usually point light [42] and low frequency spherical lobes [6,40,61], which are not suited for AR applications. In our work, we investigate optimization-based lighting estimation to directly optimize HDR lighting from visual cues in the input imagery, such as shadows. Our neural lighting representation is used as the light source for inserting virtual objects.\n\n\nMethod\n\nGiven a set of posed camera images\n{I i , c i } NRGB i=1 , where I \u2208 R h\u00d7w\u00d73\nis an image and c \u2208 SE(3) is its corresponding camera pose, we aim to estimate the geometry, spatially varying materials, and HDR lighting of the underlying scene. We represent the intrinsic scene properties using a neural field (Sec. 3.1) and render the views with a differentiable hybrid renderer (Sec. 3.2). To estimate the parameters of the neural field, we minimize the reconstruction error on the observed views and employ several regularization terms to constrain the highly ill-posed nature of the problem (Sec. 3.3). Implementation details are provided in the Appendix.\n\nNote that our method addresses both single-and multiillumination intrinsic decomposition. Existing literature [40] considers the multi-illumination setting that efficiently constraints the solution space of intrinsic properties and thus leads to a more faithful decomposition, while our formulation is general, and we demonstrate its effectiveness even when in the case of a single illumination capture. In the following, we keep the writing general, and address the distinction where required.\n\n\nNeural Intrinsic Scene Representation\n\nNeural intrinsic field We represent the intrinsic properties of the scene as a neural field F \u03c6 : x \u2192 (s, n, k d , k s ) that maps each 3D location x \u2208 R 3 to its Signed Distance (SD) value s \u2208 R, normal vector n \u2208 R 3 , base color k d \u2208 R 3 , and materials k s \u2208 R 2 . Here, we use k s to denote the roughness and metallic parameters of the physics-based (PBR) material model from Disney [11]. In practice we represent the neural field F \u03c6 with three neural networks\ns = f SDF (x; \u03b8 SDF ), n = f norm. (x; \u03b8 norm. ), and (k d , k s ) = f mat. (x; \u03b8 mat.\n) which are all Multi-Layer Perceptrons (MLPs) with a multi-resolution hash positional encoding [35].\n\nHDR sky dome In urban scenes, the main source of light is the sky. We therefore model the lighting as an HDR environment map located at infinity, which we represent as a neural network e = f env. (d; \u03b8 env. ), that maps the direction vector d \u2208 R 2 to the HDR light intensity value e \u2208 R 3 . Specifically, f env. is again an MLP with hash positional encoding. The HDR representation of the environment map allows to perform scene manipulations such as relighting and virtual object insertion with ray-traced shadow casting.\n\nSingle vs multi-illumination setting As the intrinsic properties of the scene do not change with the illumination, we use a single neural field representation of the underlying scene, and use M HDR sky maps to represent M different illumination conditions present in the captured imagery.\n\n\nHybrid Deferred Rendering\n\nWe now describe how the estimated intrinsic properties and lighting parameters are utilized in the proposed hybrid deferred rendering pipeline. We start from the non-emissive rendering equation [21]:\nL o (x, \u03c9 o ) = \u03a9 f r (x, \u03c9 o , \u03c9 i )L i (x, \u03c9 i ) |n \u00b7 \u03c9 i | d\u03c9 i ,(1)\nwhere the outgoing radiance L o at the surface point x and direction \u03c9 o is computed as the integral of the surface BRDF f r (x, \u03c9 o , \u03c9 i ) multiplied by the incoming light L i (x, \u03c9 i ) and cosine term |\u03c9 i \u00b7 n|, over the hemisphere \u03a9. In all experiments we assume the simplified Disney BRDF model. Albeit an accurate model, the rendering equation does not admit an analytical solution and is therefore commonly solved using MC methods. However, due to the volumetric nature of our scene representation, sampling enough rays to estimate the integral quickly becomes intractable, even when relying on importance sampling. To alleviate the cost of evaluating the rendering equation, while keeping the highresolution of the volumetric neural field, we propose a novel hybrid deferred rendering pipeline. Specifically, we first use the neural field to perform volumetric rendering of primary rays into a G-buffer that includes the surface normal, base color, and material parameters for each pixel. We then extract the mesh from the underlying SD field, and perform the shading pass in which we compute illumination by integrating over the hemisphere at the shading point using MC ray tracing. This allows us to synthesize high quality shading effects, including specular highlights and shadows.\n\nNeural G-buffer rendering To perform volume rendering of the G-buffer G \u2208 R h\u00d7w\u00d78 , which contains a normal map N \u2208 R h\u00d7w\u00d73 , a base color map K d \u2208 R h\u00d7w\u00d73 , a material map M \u2208 R h\u00d7w\u00d72 and a depth map D \u2208 R h\u00d7w , we follow the standard NeRF volumetric rendering equation [62]. For example, consider base color k d and let r = o + td denote the camera ray with origin o and direction d. The alphacomposited diffuse albedo map K d along the ray can then be estimated as\nK d (r) = t f tn T (t)\u03c1(r(t))k d (r(t))dt,(2)\nwhere T (t) = exp (\u2212 t tn \u03c1(r(s))ds) denotes the accumulated transmittance, and t n , t f are the near and far bound respectively. Following [51] the opaque density \u03c1(t) can be recovered from the underlying SD field as:\n\u03c1(r(t)) = max( \u2212 d\u03a6\u03ba dt (f SDF (r(t))) \u03a6 \u03ba (f SDF (r(t))) , 0)(3)\nwhere \u03a6 \u03ba (x) = Sigmoid(\u03bax) and \u03ba is a learnable parameter [51]. The surface normals and material buffer of G are rendered analogously. We render the depth buffer D \u2208 R h\u00d7w as radial distance:\nD(r) = t f tn T (t)\u03c1(r(t))tdt,(4)\nShading pass Given the G-buffer, we can now perform the shading pass. To this end, we first extract an explicit mesh S of the scene from the optimized SD field using marching cubes [32]. We then estimate Eq. (1) based on the rendered G-buffer, Specifically, for each pixel in the Gbuffer, we query its intrinsic parameters (surface normal, base color, and material) and use the depth value to compute its corresponding 3D surface point x. We then perform MC sampling of the secondary rays from the surface point x.\n\nWhile previous work assume a simplified case where all the rays reach the light source [13,36,61], the extracted mesh S enables us to determine the visibility v of each secondary ray with OptiX [39], a highly-optimized library for ray-mesh intersection queries. Here, the v is defined as:\nv i (x, \u03c9 i , S) = 0 if \u03c9 i is blocked by S 1 otherwise(5)\nThe visibility of each ray is incorporated into the estimation of the incoming light as\nL i (x, \u03c9 i ) = v i (x, \u03c9 i , S)f env. (\u03c9 i ; \u03b8 env.\n). Explicit modeling of the visibility in combination with the physically based BRDF enables us to compute higher-order lighting effects such as cast shadows.\n\nIn practice, we trace 512 secondary rays by importance sampling the BSDF and the HDR environment map. Following [18], we combine samples of the two sampling strategies using multiple importance sampling [49]. Using the highly optimized library OptiX, ray-tracing of the secondary rays is carried out in real-time. Once our representation is optimized, we can export the environment map E \u2208 R he\u00d7we\u00d73 (evaluating f env. once per each texel of E), allowing us to perform importance sampling using E without additional evaluations of f env. . During optimization, when the SD field is continuously updated, we reconstruct a new explicit mesh every 20 iterations. Empirically, this offers a good compromise between the rendering quality and efficiency.\n\n\nOptimizing the Neural Scene Representation\n\nGiven a set of posed images captured under unknown illumination condition and, when available, LiDAR point clouds, we optimize the neural scene representation end-toend by minimizing the loss:\nL =L render + \u03bb depth L depth + \u03bb rad. L rad. + \u03bb norm. L norm. + \u03bb shade L shade + \u03bb reg. L reg. ,(6)\nwhere L render , L depth are the reconstruction loss on the observed pixel and LiDAR rays and L rad. , L norm. , and L shade are used to regularize the geometry, normal field, and lighting, respectively. We additionally employ several regularization terms L reg. to constrain the ill-posed nature of the problem. \u03bb * are the weights used to balance the contribution of the individual terms. More details are discussed in the Appendix.\n\nRendering loss As the main supervision signal, we use the L1 reconstruction loss between input images and correspond-ing views rendered using the proposed hybrid renderer:\nL render = 1 |R| r\u2208R |C render (r) \u2212 C gt (r)|,(7)\nwhere C render (r) denotes the rendered RGB value for the camera ray r, C gt (r) is the ground truth RGB value of the corresponding ray, and R denotes the set of camera rays in a single batch. As our representation is fully differentiable, the gradients of L render are propagated to all intrinsic properties in the neural field, as well as to the HDR sky map.\n\nGeometry supervision To regularize the underlying SD field to learn reasonable geometry, we introduce an auxiliary radiance field C rad. = f rad. (x, d; \u03b8 rad. ) that maps each 3D location x along direction d to its emitted color and define the loss as\nL rad. = 1 |R| r\u2208R |C rad. (r) \u2212 C gt (r)|,(8)\nwhere C rad. (r) is the RGB color obtained through volumetric rendering 3 of f rad. along the ray r,\u0108(r) is the corresponding ground truth RGB, and R denotes the set of camera rays in a single batch. Note that the radiance field f rad. is only used to provide an auxiliary supervision of the geometry and is discarded after the optimization converges.\n\nFor driving data where additional LiDAR measurements are available, we use L1 loss on the range value\nL depth = 1 |R d | r\u2208Rd |D(r) \u2212 D gt (r)|.(9)\nNormal regularization While the normal vector\u00f1 x at the point x could be directly estimated from the SD field as\u00f1 x = \u2212 \u2207xfSDF ||\u2207xfSDF|| , we empirically observe that such formulation results in smooth normal vectors that cannot represent highfrequency geometry details. Instead, we estimate the normal vectors n x through volumetric rendering (see Sec. 3.2) and use\u00f1 x only as a regularizer in form of an angular loss\nL norm. = 1 |R| r\u2208R cos \u22121 (|\u00f1 x \u00b7 n x |),(10)\nwhere |\u00b7| denotes the dot product. Normal vectors obtained through volumetric rendering are capable of capturing highfrequency details while also respecting the low frequency.\n\nShading regularization Inverse rendering under unknown illumination is a highly ill-posed problem. Without adequate regularization, optimization-based methods tend to bake shadows into diffuse albedo, rather than explaining them as a combination of geometry and environment map [18]. In urban scenes, shadows are often cast on areas with a single dominant albedo, resulting in shadow boundaries that can be used as visual cues for intrinsic decomposition. In addition, these regions are often in the same semantic class, e.g., road, sidewalks, and buildings. Based on this observation, we introduce a set of auxiliary learnable parameters -one albedo per semantic class, and encourage its re-rendering to be consistent with the groundtruth image:\nL shade = 1 B B b=1 1 |R b | r\u2208R b |C b diffuse (r) \u2212\u0108(r)|,(11)\nwhere B is the number of semantic classes, k b sem \u2208 R 3 is the b-th learnable semantic albedo, R b is the set of camera rays that belong to the b-th semantic class, C b diffuse (r) = k b sem s diffuse is the rendered color, and s diffuse is the diffuse shading in deferred rendering. Intuitively, the shading regularization term encourages the optimization to explain the cast shadows by adapting the environment map, due to the limited capacity of per-semantic class albedo. The semantic segmentation are computed with an off-the-shelf semantic segmentation network [47].\n\nOptimization scheme Since our hybrid renderer relies on the explicit mesh extracted from the SD field, similar to NeR-Factor [63], we first initialise the geometry by optimizing with only radiance, then optimize with other scene intrinsics using all losses. More details are in the Appendix.\n\n\nExperiments\n\nWe use three urban outdoor datasets to evaluate FEGR and to justify our design choices. We start by describing the datasets and the evaluation setting used in our experiments (Sec. 4.1). We then provide a quantitative and qualitative evaluation of inverse rendering of large urban scenes under multi-illumination setting (Sec. 4.2). Additionally, we evaluate our method on a very challenging scenario of autonomous driving scenes captured under a single illumination. Finally, we showcase that FEGR can support downstream tasks such as virtual object insertion with raytraced shadow casting (Sec. 4.3).\n\n\nDatasets and evaluation setting\n\nNeRF-OSR dataset [40] contains in total eight outdoor scenes captured using a DSLR camera in 110 recording sessions across all scenes. Each session also contains an environment map estimated from the images acquired using a 360 \u2022 camera. In our evaluation, we follow the setting proposed in [40]. Specifically, we use three scenes for quantitative evaluation and use 13/12/11 sessions respectively to optimize the parameters of our neural scene representation. We then use environment maps from five other recording sessions to relight each scene and measure average PSNR and MSE between the rendered and ground-truth images. To remove dynamic objects, sky and vegetation pixels we again follow [40] and use the segmentation masks predicted by an off-the-shelf semantic segmentation network [47].\n\nDriving dataset includes two scenes captured by autonomous vehicles (AV) in an urban environment. The first scene is from the Waymo Open Dataset (WOD) [44], and has a 20-second clip acquired by five pinhole cameras and one 64-beam LiDAR sensor at 10 Hz. We use all five camera views for our experiments. The second set of scenes is also from a high-quality AV dataset (dubbed RoadData) acquired in-house. It is captured using eight high resolution (3848x2168 pixel) cameras with calibrated distortion and one 128-beam LiDAR. We only use images from the front-facing 120 FoV camera. For both scenes, we additionally rely on LiDAR point clouds for depth supervision. The Driving dataset is challenging as it records large street environments with complex geometry, lighting, and occlusion, and typically with a fast camera motion. It also only records a scene in a single drive thus providing only single illumination capture. However, urban environments are of high interest to digitize so as to serve as content to a variety of downstream applications such as gaming and AV simulation.\n\nBaselines We select different baselines for each of the tasks. In the relighting benchmark on NeRF-OSR dataset, we compare our method to NeRF-OSR [40]. For the challenging inverse rendering problem on the Driving dataset, we compare to Nvdiffrecmc [18]. Finally, we perform a user-study and compare FEGR to Hold-Geoffroy et al. [19] and Wang et al. [52] on the task of virtual object insertion.\n\n\nEvaluation of Inverse Rendering\n\nOutdoor scene relighting Tab. 1 shows the quantitative evaluation of the relighting performance on the NeRF-OSR dataset. FEGR significantly outperforms the baseline across all three scenes in terms of both PSNR and MSE. In Fig. 3 we additionally show qualitative results obtained by relighting the scenes using two different environment maps The normal vectors estimated by NeRF-OSR contain high-frequency noises, which result in artifacts when relighting the scene 8375 Figure 3. Qualitative results of scene relighting on NeRF-OSR [40] dataset. Our method reconstructs clean diffuse albedo and enables high-quality relighting with photo-realistic cast shadow. with strong directional light. On the other hand, FEGR succeeds in faithfully decomposing the geometry and material from lighting and yields visually much more pleasing results with sharp cast shadows and high-quality details.\n\nAblation study We ablate our design choices by comparing FEGR to three simplified versions: (i) Ours (mesh only) denotes a version where we transfer the intrinsic properties from the neural field to the vertices of the reconstructed mesh and compute both primary and secondary rays from the mesh representation, (ii) in Ours (w/o shadows) we disregard secondary rays and render only the primary rays from the neural field, and (iii) Ours (w/o exposure) where we do not perform per color channel exposure compensation (see Appendix for more details). Tab. 1 show that the proposed combination of the high resolution neural field with the explicit mesh is crucial to high-quality results. Physically based ray-tracing of shadows and exposure compensation further boost our performance and result in gains of up to 1.5 dB PSNR.\n\nDriving dataset Driving dataset is challenging in several aspects: (i) The scenes are large (up to 200m \u00d7 200m in horizontal plane) with complex geometry and spatiallyvarying material; (ii) Environment illumination is unknown and could contain high intensity from the sun; (iii) Images are captured by a fast-moving vehicle (\u2248 10 m/s on WOD dataset) resulting in motion blur and HDR artifacts. Even so, our method still achieves superior intrinsic scene decomposition on both scenes, leading to photo-realistic view-synthesis results (see Fig. 4). Compared to Nvdiffrecmc 4 [18], we reconstruct cleaner base color, more accurate geometry, and % Ours is preferred vs Hold-Geoffroy et al. [19] 86.2 % vs Wang et al. [52] 68.9 % Table 2. User study results of object insertion quality. Users consistently prefer ours over results from baseline methods.\n\nhigher resolution environment maps. It is worth noting that Nvdiffrecmc is designed for \"outside-looking-in\" setups with 360-view coverage and does not directly work on Driving dataset without modifications. In Fig. 5 we additionally show the qualitative scene relighting results under the challenging illumination settings.\n\n\nApplication to virtual object insertion\n\nQualitative comparison Fig. 6 shows qualitative results of object insertion on Driving dataset. FEGR is capable of faithfully representing the location of the sun, resulting in cast shadows that agree with the surroundings and yield a photo-realistic insertion.\n\nUser study To quantitatively evaluate the object insertion results of FEGR against other baselines, we conduct a user study using Amazon Mechanical Turk. In particular, we show the participants two augmented images with the same car inserted by our method and by the baseline in random order, and ask them to evaluate which one is more photorealistic based on: (i) the quality of cast shadows, and (ii) the quality of reflections. For each baseline comparison, we invite 9 users to judge 29 examples and use the majority vote for the preference for each example. The results of the user study are presented in Tab. 2. A significant majority of the participants agree that FEGR yields more realistic results than all baselines, indicating a more accurate lighting estimation of our method.\n\n\nConclusion\n\nWe introduced FEGR, a novel hybrid rendering pipeline for inverse rendering of large urban scenes. FEGR combines high-resolution of the neural fields with the efficiency of explicit mesh representations and is capable of extracting the scene geometry, spatially varying materials, and HDR lighting from a set of posed camera images. The formulation of FEGR is flexible and it supports both single and multi-illumination data. We demonstrated that FEGR consistently outperforms SoTA methods across various challenging datasets. Finally, we have demonstrated that FEGR can seamlessly support various scene manipulations including relighting and virtual object insertion (AR).\n\nLimitations While FEGR makes an important step forward in neural rendering of large urban scenes, it naturally also has limitations. Inverse rendering is a highly-ill posed problem in which the solution spaces has to be constrained, especially when operating on single illumination data. We currently rely on manually designed priors to define regularization terms. In the future we would like to explore ways of learning these priors from the abundance of available data. Similar to most methods based on neural fields, FEGR is currently limited to static scenes. A promising extension in the future could incorporate advances in dynamic NeRFs [15] to mitigate this problem.\n\nFigure 4 .\n4Qualitative results of intrinsic scene decomposition on the Driving dataset. Our method successfully separates shadows from diffuse albedo (see mark), and reconstructs a high intensity, small area in the environment map (see mark).\n\nFigure 5 .Figure 6 .\n56Qualitative results of scene relighting on RoadData. We show 3 relighting results for each scene.Input imageHold-Geoffroy et al.[19] Wang et al.[52] Ours Qualitative comparison of virtual object insertion. Our method faithfully reconstructs the environment map and produces photo-realistic cast shadows with sharp boundaries.\n\n\nPSNR \u2191 MSE \u2193 PSNR \u2191 MSE \u2193 PSNR \u2191 MSE \u2193Site 1 \n\nSite 2 \nSite 3 \nNeRF-OSR [40] \n19.34 \n0.012 \n16.35 \n0.027 \n15.66 \n0.029 \nOurs \n21.53 \n0.007 \n17.00 \n0.023 \n17.57 \n0.018 \nOurs (mesh only) \n18.94 \n0.013 \n16.50 \n0.025 \n16.86 \n0.021 \nOurs (w/o shadow) \n20.62 \n0.009 \n16.17 \n0.028 \n16.15 \n0.024 \nOurs (w/o exposure) 20.70 \n0.009 \n16.70 \n0.025 \n16.09 \n0.025 \n\nTable 1. Outdoor scene relighting results on NeRF-OSR dataset. \n\n\nAbbreviation FEGR is derived from neural Fields meet Explicit Geometric Representations and is pronounced as \"figure\".2 We can also integrate depth information, if available, to further constrain the solution space.\nf rad. only encodes the radiance. The SD field used to perform volumetric rendering is shared with our neural scene representation.\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nIn driving scenario where inputs is a restricted set of views in an \"insidelooking-out\" manner, Nvdiffrecmc relies heavily on depth supervision to recover the geometry. This leads to artifacts on surfaces that are not observed by LiDAR or have incorrect depth signal (e.g. windows). The erroneous geometry hurts the estimation of intrinsic properties.\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\n\nRecovering intrinsic scene characteristics. Harry Barrow, Tenenbaum, E Hanson, Riseman, Comput. Vis. Syst. 22Harry Barrow, J Tenenbaum, A Hanson, and E Riseman. Recovering intrinsic scene characteristics. Comput. Vis. Syst, 2:3-26, 1978. 1, 2\n\nIntrinsic images in the wild. Sean Bell, Kavita Bala, Noah Snavely, ACM Transactions on Graphics (TOG). 334159Sean Bell, Kavita Bala, and Noah Snavely. Intrinsic images in the wild. ACM Transactions on Graphics (TOG), 33(4):159, 2014. 2\n\nSai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi, arXiv:2008.03824Neural reflectance fields for appearance acquisition. arXiv preprintSai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Neural re- flectance fields for appearance acquisition. arXiv preprint arXiv:2008.03824, 2020. 2\n\nDeep reflectance volumes: Relightable reconstructions from multi-view photometric images. Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi, ECCV. Springer23Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Deep reflectance volumes: Relightable reconstructions from multi-view photometric images. In ECCV, pages 294-311. Springer, 2020. 2, 3\n\nFactorized and controllable neural re-rendering of outdoor scene for photo extrapolation. Boming Zhao, Bangbang Yang, Zhenyang Li, Zuoyue Li, Guofeng Zhang, Jiashu Zhao, Dawei Yin, Zhaopeng Cui, Hujun Bao, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022Boming Zhao and Bangbang Yang, Zhenyang Li, Zuoyue Li, Guofeng Zhang, Jiashu Zhao, Dawei Yin, Zhaopeng Cui, and Hujun Bao. Factorized and controllable neural re-rendering of outdoor scene for photo extrapolation. In Proceedings of the 30th ACM International Conference on Multimedia, 2022. 2\n\nNerd: Neural reflectance decomposition from image collections. Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Barron, Ce Liu, Hendrik P A Lensch, ICCV. 23Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar- ron, Ce Liu, and Hendrik P.A. Lensch. Nerd: Neural re- flectance decomposition from image collections. In ICCV, 2021. 2, 3\n\nSAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections. Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T Barron, Hendrik P A Lensch, Varun Jampani, Advances in Neural Information Processing Systems (NeurIPS). 2022Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T. Barron, Hendrik P.A. Lensch, and Varun Jampani. SAMURAI: Shape And Material from Uncon- strained Real-world Arbitrary Image collections. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 2\n\nNeural-pil: Neural pre-integrated lighting for reflectance decomposition. Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T Barron, Hendrik P A Lensch, Advances in Neural Information Processing Systems (NeurIPS). 2021Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T. Barron, and Hendrik P.A. Lensch. Neural-pil: Neural pre-integrated lighting for reflectance decomposi- tion. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 2\n\nTwo-shot spatially-varying brdf and shape estimation. Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P A Lensch, Jan Kautz, CVPR. 2020Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A. Lensch, and Jan Kautz. Two-shot spatially-varying brdf and shape estimation. In CVPR, 2020. 2\n\nUserassisted intrinsic images. Adrien Bousseau, Sylvain Paris, Fr\u00e9do Durand, In ACM Transactions on Graphics. 282130ACMAdrien Bousseau, Sylvain Paris, and Fr\u00e9do Durand. User- assisted intrinsic images. In ACM Transactions on Graphics (TOG), volume 28, page 130. ACM, 2009. 2\n\nPhysically-based shading at disney. Brent Burley, 4Brent Burley. Physically-based shading at disney. 2012. 4\n\nLearning to predict 3d objects with an interpolation-based differentiable renderer. Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaako Lehtinen, Alec Jacobson, Sanja Fidler, NeurIPS. Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaako Lehtinen, Alec Jacobson, and Sanja Fidler. Learning to pre- dict 3d objects with an interpolation-based differentiable ren- derer. In NeurIPS, 2019. 3\n\nDIB-R++: Learning to predict lighting and material with a hybrid differentiable renderer. Wenzheng Chen, Joey Litalien, Jun Gao, Zian Wang, Clement Fuji Tsang, Sameh Khalis, NeurIPS. 25Or Litany, and Sanja FidlerWenzheng Chen, Joey Litalien, Jun Gao, Zian Wang, Clement Fuji Tsang, Sameh Khalis, Or Litany, and Sanja Fidler. DIB-R++: Learning to predict lighting and material with a hybrid differentiable renderer. In NeurIPS, 2021. 2, 3, 5\n\nPlenoxels: Radiance fields without neural networks. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa, CVPR. 2022Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022. 1\n\nMonocular dynamic view synthesis: A reality check. Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, Angjoo Kanazawa, NeurIPS. Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: A reality check. In NeurIPS, 2022. 8\n\nFast spatially-varying indoor lighting estimation. Kalyan Mathieu Garon, Sunil Sunkavalli, Nathan Hadap, Jean-Fran\u00e7ois Carr, Lalonde, CVPR. Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, and Jean-Fran\u00e7ois Lalonde. Fast spatially-varying indoor lighting estimation. In CVPR, pages 6908-6917, 2019. 3\n\nGround truth dataset and baseline evaluations for intrinsic image algorithms. Roger Grosse, Micah K Johnson, William T Edward H Adelson, Freeman, ICCV. Roger Grosse, Micah K Johnson, Edward H Adelson, and William T Freeman. Ground truth dataset and baseline evalu- ations for intrinsic image algorithms. In ICCV, pages 2335- 2342. IEEE, 2009. 2\n\nShape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising. Jon Hasselgren, Nikolai Hofmann, Jacob Munkberg, arXiv:2206.0338067Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising. arXiv:2206.03380, 2022. 2, 3, 5, 6, 7\n\nDeep sky modeling for single image outdoor lighting estimation. Yannick Hold-Geoffroy, Akshaya Athawale, Jean-Fran\u00e7ois Lalonde, CVPR. 6Yannick Hold-Geoffroy, Akshaya Athawale, and Jean- Fran\u00e7ois Lalonde. Deep sky modeling for single image out- door lighting estimation. In CVPR, pages 6927-6935, 2019. 3, 6, 8\n\nDeep outdoor illumination estimation. Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto, Jean-Fran\u00e7ois Lalonde, CVPR. Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto, and Jean-Fran\u00e7ois Lalonde. Deep outdoor illumination estimation. In CVPR, pages 7312-7321, 2017. 3\n\nThe rendering equation. T James, Kajiya, Proceedings of the 13th annual conference on Computer graphics and interactive techniques. the 13th annual conference on Computer graphics and interactive techniquesJames T Kajiya. The rendering equation. In Proceedings of the 13th annual conference on Computer graphics and interactive techniques, pages 143-150, 1986. 4\n\nShading annotations in the wild. Balazs Kovacs, Sean Bell, Noah Snavely, Kavita Bala, CVPR. Balazs Kovacs, Sean Bell, Noah Snavely, and Kavita Bala. Shading annotations in the wild. In CVPR, pages 6998-7007, 2017. 2\n\nNeROIC: Neural object capture and rendering from online image collections. Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, Sergey Tulyakov, abs/2201.02533Computing Research. 20222Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and Sergey Tulyakov. NeROIC: Neural object capture and rendering from online image collections. Computing Research Repository (CoRR), abs/2201.02533, 2022. 2\n\nLightness and retinex theory. H Edwin, John J Land, Mccann, Josa. 611Edwin H Land and John J McCann. Lightness and retinex theory. Josa, 61(1):1-11, 1971. 2\n\nDeeplight: Learning illumination for unconstrained mobile mixed reality. Chloe Legendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, Paul Debevec, CVPR. Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, and Paul Debevec. Deeplight: Learning illumination for unconstrained mobile mixed reality. In CVPR, pages 5918-5928, 2019. 3\n\nNeulighting: Neural lighting for free viewpoint outdoor scene relighting with unconstrained photo collections. Quewei Li, Jie Guo, Yang Fei, Feichao Li, Yanwen Guo, SIGGRAPH Asia 2022 Conference Papers, SA 2022. Soon Ki Jung, Jehee Lee, and Adam W. BargteilDaegu, Republic of KoreaACM13Quewei Li, Jie Guo, Yang Fei, Feichao Li, and Yanwen Guo. Neulighting: Neural lighting for free viewpoint outdoor scene relighting with unconstrained photo collections. In Soon Ki Jung, Jehee Lee, and Adam W. Bargteil, editors, SIGGRAPH Asia 2022 Conference Papers, SA 2022, Daegu, Republic of Korea, December 6-9, 2022, pages 13:1-13:9. ACM, 2022. 3\n\nInverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single image. Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, Manmohan Chandraker, CVPR. 2020Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single image. In CVPR, pages 2475-2484, 2020. 2\n\nCgintrinsics: Better intrinsic image decomposition through physically-based rendering. Zhengqi Li, Noah Snavely, ECCV. Zhengqi Li and Noah Snavely. Cgintrinsics: Better intrinsic image decomposition through physically-based rendering. In ECCV, pages 371-387, 2018. 2\n\nLearning to reconstruct shape and spatially-varying reflectance from a single image. Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan Sunkavalli, Manmohan Chandraker, ACM Transactions on Graphics (TOG). 376Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Learning to recon- struct shape and spatially-varying reflectance from a single image. ACM Transactions on Graphics (TOG), 37(6):1-11, 2018. 3\n\nOpenrooms: An end-to-end open framework for photorealistic indoor scene datasets. Zhengqin Li, Ting-Wei Yu, Shen Sang, Sarah Wang, Sai Bi, Zexiang Xu, Hong-Xing Yu, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Ravi Ramamoorthi, arXiv:2007.12868arXiv preprintZhengqin Li, Ting-Wei Yu, Shen Sang, Sarah Wang, Sai Bi, Zexiang Xu, Hong-Xing Yu, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Ravi Ramamoorthi, et al. Openrooms: An end-to-end open framework for photorealistic indoor scene datasets. arXiv preprint arXiv:2007.12868, 2020. 2\n\nLearning to factorize and relight a city. Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A Efros, Noah Snavely, ECCV. 2020Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros, and Noah Snavely. Learning to factorize and relight a city. In ECCV, 2020. 3\n\nMarching cubes: A high resolution 3d surface construction algorithm. William E Lorensen, Harvey E Cline, SIGGRAPH Comput. Graph. 214William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. SIGGRAPH Comput. Graph., 21(4):163-169, aug 1987. 4\n\nNeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. Ricardo Martin-Brualla, Noha Radwan, S M Mehdi, Jonathan T Sajjadi, Alexey Barron, Daniel Dosovitskiy, Duckworth, arXiv, 2020. 3Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Saj- jadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In arXiv, 2020. 3\n\nBen Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, arXiv:2003.08934Nerf: Representing scenes as neural radiance fields for view synthesis. 13arXiv preprintBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthe- sis. arXiv preprint arXiv:2003.08934, 2020. 1, 2, 3\n\nInstant neural graphics primitives with a multiresolution hash encoding. Thomas M\u00fcller, Alex Evans, Christoph Schied, Alexander Keller, 102:1-102:15ACM Transactions on Graphics (TOG). 4144Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant neural graphics primitives with a mul- tiresolution hash encoding. ACM Transactions on Graphics (TOG), 41(4):102:1-102:15, July 2022. 1, 4\n\nJacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Mueller, Sanja Fidler, arXiv:2111.12503Extracting Triangular 3D Models, Materials, and Lighting From Images. 25Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Mueller, and Sanja Fidler. Extracting Triangular 3D Models, Materials, and Lighting From Images. arXiv:2111.12503, 2021. 2, 3, 5\n\nMaterial and lighting reconstruction for complex indoor scenes with texture-space differentiable rendering. Merlin Nimier-David, Zhao Dong, Jakob Wenzel, Anton Kaplanyan, 2021. 3Merlin Nimier-David, Zhao Dong, Wenzel Jakob, and Anton Kaplanyan. Material and lighting reconstruction for com- plex indoor scenes with texture-space differentiable rendering. 2021. 3\n\nMitsuba 2: A retargetable forward and inverse renderer. Merlin Nimier-David, Delio Vicini, Tizian Zeltner, Wenzel Jakob, ACM Transactions on Graphics (TOG). 386Merlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wen- zel Jakob. Mitsuba 2: A retargetable forward and inverse renderer. ACM Transactions on Graphics (TOG), 38(6), Dec. 2019. 2\n\nOptix: A general purpose ray tracing engine. G Steven, James Parker, Andreas Bigler, Heiko Dietrich, Jared Friedrich, David Hoberock, David Luebke, Morgan Mcallister, Keith Mcguire, Austin Morley, Martin Robison, Stich, ACM Trans. Graph. 2945Steven G. Parker, James Bigler, Andreas Dietrich, Heiko Friedrich, Jared Hoberock, David Luebke, David McAllister, Morgan McGuire, Keith Morley, Austin Robison, and Martin Stich. Optix: A general purpose ray tracing engine. ACM Trans. Graph., 29(4), jul 2010. 2, 5\n\nNerf for outdoor scene relighting. Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie Liu, Vladislav Golyanik, Christian Theobalt, ECCV. 67Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie Liu, Vladislav Golyanik, and Christian Theobalt. Nerf for outdoor scene relighting. In ECCV, 2022. 2, 3, 4, 6, 7\n\nNeural inverse rendering of an indoor scene from a single image. Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W Jacobs, Jan Kautz, ICCV. Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, and Jan Kautz. Neural inverse rendering of an indoor scene from a single image. In ICCV, 2019. 2\n\nNerv: Neural reflectance and visibility fields for relighting and view synthesis. P Pratul, Boyang Srinivasan, Xiuming Deng, Matthew Zhang, Ben Tancik, Jonathan T Mildenhall, Barron, CVPR. 23Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. Nerv: Neu- ral reflectance and visibility fields for relighting and view synthesis. In CVPR, 2021. 2, 3\n\nDirect voxel grid optimization: Super-fast convergence for radiance fields reconstruction. Cheng Sun, Min Sun, Hwann-Tzong Chen, CVPR. 2022Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In CVPR, 2022. 1\n\nScalability in perception for autonomous driving: Waymo open dataset. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, CVPR. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, 2020. 6\n\n. Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul Srinivasan, Jonathan T Barron, Henrik Kretzschmar Block-Nerf, Scalable large scene neural view synthesis. arXiv, 2022. 1Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad- han, Ben Mildenhall, Pratul Srinivasan, Jonathan T. Barron, and Henrik Kretzschmar. Block-NeRF: Scalable large scene neural view synthesis. arXiv, 2022. 1\n\nEstimating spatially-varying lighting in urban scenes with disentangled representation. Jiajun Tang, Yongjie Zhu, Haoyu Wang, Jun-Hoong Chan, Si Li, Boxin Shi, ECCV. 2022Jiajun Tang, Yongjie Zhu, Haoyu Wang, Jun-Hoong Chan, Si Li, and Boxin Shi. Estimating spatially-varying lighting in urban scenes with disentangled representation. In ECCV, 2022. 3\n\nHierarchical multi-scale attention for semantic segmentation. Andrew Tao, Karan Sapra, Bryan Catanzaro, arXiv:2005.10821arXiv preprintAndrew Tao, Karan Sapra, and Bryan Catanzaro. Hierarchi- cal multi-scale attention for semantic segmentation. arXiv preprint arXiv:2005.10821, 2020. 6\n\nMega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. Haithem Turki, Deva Ramanan, Mahadev Satyanarayanan, CVPR. Haithem Turki, Deva Ramanan, and Mahadev Satya- narayanan. Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In CVPR, pages 12922-12931, June 2022. 1\n\nOptimally combining sampling techniques for monte carlo rendering. Eric Veach, Leonidas J Guibas, Proceedings of the 22nd annual conference on Computer graphics and interactive techniques. the 22nd annual conference on Computer graphics and interactive techniquesEric Veach and Leonidas J Guibas. Optimally combining sam- pling techniques for monte carlo rendering. In Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, pages 419-428, 1995. 5\n\nDor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, Pratul P Srinivasan, Ref-NeRF: Structured view-dependent appearance for neural radiance fields. CVPR. 2022Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: Structured view-dependent appearance for neural radiance fields. CVPR, 2022. 3\n\nNeus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang, NeurIPS. 4Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021. 4\n\nNeural light field estimation for street scenes with differentiable virtual object insertion. Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz, Sanja Fidler, ECCV. 6Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz, and Sanja Fidler. Neural light field estimation for street scenes with differentiable virtual object insertion. In ECCV, 2022. 2, 3, 6, 8\n\nLearning indoor inverse rendering with 3d spatially-varying lighting. Zian Wang, Jonah Philion, Sanja Fidler, Jan Kautz, ICCV. Zian Wang, Jonah Philion, Sanja Fidler, and Jan Kautz. Learn- ing indoor inverse rendering with 3d spatially-varying light- ing. In ICCV, 2021. 2\n\nDe-rendering 3d objects in the wild. Felix Wimbauer, Shangzhe Wu, Christian Rupprecht, CVPR. 2022Felix Wimbauer, Shangzhe Wu, and Christian Rupprecht. De-rendering 3d objects in the wild. In CVPR, 2022. 2\n\nBungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, Dahua Lin, ECCV. 2022Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. In ECCV, 2022. 1\n\nNeilf: Neural incident light field for physically-based material estimation. Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David Mckinnon, Yanghai Tsin, Long Quan, European Conference on Computer Vision (ECCV). 2022Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Neilf: Neu- ral incident light field for physically-based material estima- tion. In European Conference on Computer Vision (ECCV), 2022. 3\n\nInverserendernet: Learning single image inverse rendering. Ye Yu, A P William, Smith, CVPR. Ye Yu and William AP Smith. Inverserendernet: Learning single image inverse rendering. In CVPR, 2019. 2\n\nAll-weather deep outdoor lighting estimation. Jinsong Zhang, Kalyan Sunkavalli, Yannick Hold-Geoffroy, Sunil Hadap, Jonathan Eisenman, Jean-Fran\u00e7ois Lalonde, CVPR. Jinsong Zhang, Kalyan Sunkavalli, Yannick Hold-Geoffroy, Sunil Hadap, Jonathan Eisenman, and Jean-Fran\u00e7ois Lalonde. All-weather deep outdoor lighting estimation. In CVPR, pages 10158-10166, 2019. 3\n\nNeRS: Neural reflectance surfaces for sparseview 3d reconstruction in the wild. Jason Y Zhang, Gengshan Yang, Shubham Tulsiani, Deva Ramanan, Conference on Neural Information Processing Systems. Jason Y. Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. NeRS: Neural reflectance surfaces for sparse- view 3d reconstruction in the wild. In Conference on Neural Information Processing Systems, 2021. 3\n\nIron: Inverse rendering by optimizing neural sdfs and materials from photometric images. Kai Zhang, Fujun Luan, Zhengqi Li, Noah Snavely, CVPR, 2022. 23Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images. In CVPR, 2022. 2, 3\n\nPhySG: Inverse rendering with spherical gaussians for physics-based material editing and relighting. Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, Noah Snavely, CVPR, 2021. 25Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse rendering with spherical gaus- sians for physics-based material editing and relighting. In CVPR, 2021. 2, 3, 5\n\nKai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun, 2020. 4Nerf++: Analyzing and improving neural radiance fields. Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields, 2020. 4\n\nNerfactor: Neural factorization of shape and reflectance under an unknown illumination. Xiuming Zhang, P Pratul, Boyang Srinivasan, Paul Deng, Debevec, T William, Jonathan T Freeman, Barron, ACM Transactions on Graphics (TOG). 4066Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul De- bevec, William T Freeman, and Jonathan T Barron. Nerfactor: Neural factorization of shape and reflectance under an un- known illumination. ACM Transactions on Graphics (TOG), 40(6):1-18, 2021. 2, 3, 6\n\nModeling indirect illumination for inverse rendering. Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, Xiaowei Zhou, CVPR, 2022. 23Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for inverse rendering. In CVPR, 2022. 2, 3\n\nA closed-form solution to retinex with nonlocal texture constraints. Qi Zhao, Ping Tan, Qiang Dai, Li Shen, Enhua Wu, Stephen Lin, 34Qi Zhao, Ping Tan, Qiang Dai, Li Shen, Enhua Wu, and Stephen Lin. A closed-form solution to retinex with nonlocal texture constraints. 34(7):1437-1444, 2012. 2\n\nSpatiallyvarying outdoor lighting estimation from intrinsics. Yongjie Zhu, Yinda Zhang, Si Li, Boxin Shi, CVPR. Yongjie Zhu, Yinda Zhang, Si Li, and Boxin Shi. Spatially- varying outdoor lighting estimation from intrinsics. In CVPR, 2021. 3\n", "annotations": {"author": "[{\"end\":157,\"start\":95},{\"end\":225,\"start\":158},{\"end\":286,\"start\":226},{\"end\":323,\"start\":287},{\"end\":348,\"start\":324},{\"end\":373,\"start\":349},{\"end\":394,\"start\":374},{\"end\":461,\"start\":395},{\"end\":527,\"start\":462}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":100},{\"end\":172,\"start\":168},{\"end\":233,\"start\":230},{\"end\":300,\"start\":295},{\"end\":338,\"start\":330},{\"end\":363,\"start\":353},{\"end\":384,\"start\":378},{\"end\":408,\"start\":404},{\"end\":474,\"start\":468}]", "author_first_name": "[{\"end\":99,\"start\":95},{\"end\":167,\"start\":158},{\"end\":229,\"start\":226},{\"end\":294,\"start\":287},{\"end\":329,\"start\":324},{\"end\":352,\"start\":349},{\"end\":377,\"start\":374},{\"end\":403,\"start\":395},{\"end\":467,\"start\":462}]", "author_affiliation": "[{\"end\":113,\"start\":106},{\"end\":137,\"start\":115},{\"end\":156,\"start\":139},{\"end\":181,\"start\":174},{\"end\":205,\"start\":183},{\"end\":224,\"start\":207},{\"end\":242,\"start\":235},{\"end\":266,\"start\":244},{\"end\":285,\"start\":268},{\"end\":309,\"start\":302},{\"end\":322,\"start\":311},{\"end\":347,\"start\":340},{\"end\":372,\"start\":365},{\"end\":393,\"start\":386},{\"end\":417,\"start\":410},{\"end\":441,\"start\":419},{\"end\":460,\"start\":443},{\"end\":483,\"start\":476},{\"end\":507,\"start\":485},{\"end\":526,\"start\":509}]", "title": "[{\"end\":92,\"start\":1},{\"end\":619,\"start\":528}]", "venue": null, "abstract": "[{\"end\":2272,\"start\":649}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2945,\"start\":2942},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2983,\"start\":2979},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3190,\"start\":3186},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3193,\"start\":3190},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3196,\"start\":3193},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3262,\"start\":3258},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3265,\"start\":3262},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3268,\"start\":3265},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3629,\"start\":3625},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3704,\"start\":3701},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4407,\"start\":4403},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4559,\"start\":4555},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":4562,\"start\":4559},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4882,\"start\":4878},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4998,\"start\":4994},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5072,\"start\":5068},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7253,\"start\":7249},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7734,\"start\":7731},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7892,\"start\":7888},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8146,\"start\":8142},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8149,\"start\":8146},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8152,\"start\":8149},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8155,\"start\":8152},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8209,\"start\":8206},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8211,\"start\":8209},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8214,\"start\":8211},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8217,\"start\":8214},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8220,\"start\":8217},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8223,\"start\":8220},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8226,\"start\":8223},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8230,\"start\":8226},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8234,\"start\":8230},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8238,\"start\":8234},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8241,\"start\":8238},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8562,\"start\":8558},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8565,\"start\":8562},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8568,\"start\":8565},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8571,\"start\":8568},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8574,\"start\":8571},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8846,\"start\":8842},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8879,\"start\":8875},{\"end\":9005,\"start\":8976},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9880,\"start\":9876},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9883,\"start\":9880},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9886,\"start\":9883},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9889,\"start\":9886},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9892,\"start\":9889},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9895,\"start\":9892},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9917,\"start\":9914},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9919,\"start\":9917},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9922,\"start\":9919},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9925,\"start\":9922},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9928,\"start\":9925},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9964,\"start\":9960},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9967,\"start\":9964},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10288,\"start\":10284},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10401,\"start\":10398},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10403,\"start\":10401},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10406,\"start\":10403},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10409,\"start\":10406},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10412,\"start\":10409},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10415,\"start\":10412},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":10418,\"start\":10415},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10726,\"start\":10722},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10729,\"start\":10726},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10767,\"start\":10763},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10879,\"start\":10875},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11195,\"start\":11191},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11198,\"start\":11195},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11670,\"start\":11666},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11673,\"start\":11670},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11676,\"start\":11673},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11679,\"start\":11676},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11682,\"start\":11679},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11685,\"start\":11682},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11688,\"start\":11685},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":11691,\"start\":11688},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11817,\"start\":11813},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11820,\"start\":11817},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11823,\"start\":11820},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11863,\"start\":11859},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":11866,\"start\":11863},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11893,\"start\":11889},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12272,\"start\":12269},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12275,\"start\":12272},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12278,\"start\":12275},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12281,\"start\":12278},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12459,\"start\":12455},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12497,\"start\":12494},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12500,\"start\":12497},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12503,\"start\":12500},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13579,\"start\":13575},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14394,\"start\":14390},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14656,\"start\":14652},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15700,\"start\":15696},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":17345,\"start\":17341},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":17729,\"start\":17725},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":17933,\"start\":17929},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18282,\"start\":18278},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18704,\"start\":18700},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18707,\"start\":18704},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":18710,\"start\":18707},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18811,\"start\":18807},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19378,\"start\":19374},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19469,\"start\":19465},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23101,\"start\":23097},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24202,\"start\":24198},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":24334,\"start\":24330},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25171,\"start\":25167},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25445,\"start\":25441},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25849,\"start\":25845},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":25945,\"start\":25941},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26103,\"start\":26099},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27185,\"start\":27181},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27287,\"start\":27283},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27367,\"start\":27363},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27388,\"start\":27384},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28002,\"start\":27998},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29759,\"start\":29755},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29872,\"start\":29868},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29899,\"start\":29895},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32790,\"start\":32786},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33218,\"start\":33214},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":33234,\"start\":33230},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33951,\"start\":33950}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":33061,\"start\":32817},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33411,\"start\":33062},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33831,\"start\":33412}]", "paragraph": "[{\"end\":2946,\"start\":2288},{\"end\":3630,\"start\":2948},{\"end\":4563,\"start\":3632},{\"end\":5233,\"start\":4565},{\"end\":6442,\"start\":5235},{\"end\":6619,\"start\":6444},{\"end\":6908,\"start\":6621},{\"end\":7108,\"start\":6910},{\"end\":7613,\"start\":7110},{\"end\":9767,\"start\":7630},{\"end\":11337,\"start\":9769},{\"end\":12797,\"start\":11339},{\"end\":12842,\"start\":12808},{\"end\":13463,\"start\":12885},{\"end\":13959,\"start\":13465},{\"end\":14468,\"start\":14001},{\"end\":14657,\"start\":14556},{\"end\":15182,\"start\":14659},{\"end\":15472,\"start\":15184},{\"end\":15701,\"start\":15502},{\"end\":17067,\"start\":15774},{\"end\":17537,\"start\":17069},{\"end\":17803,\"start\":17584},{\"end\":18062,\"start\":17870},{\"end\":18611,\"start\":18097},{\"end\":18901,\"start\":18613},{\"end\":19048,\"start\":18961},{\"end\":19260,\"start\":19102},{\"end\":20010,\"start\":19262},{\"end\":20249,\"start\":20057},{\"end\":20787,\"start\":20353},{\"end\":20960,\"start\":20789},{\"end\":21372,\"start\":21012},{\"end\":21626,\"start\":21374},{\"end\":22025,\"start\":21674},{\"end\":22128,\"start\":22027},{\"end\":22594,\"start\":22175},{\"end\":22817,\"start\":22642},{\"end\":23565,\"start\":22819},{\"end\":24203,\"start\":23630},{\"end\":24496,\"start\":24205},{\"end\":25114,\"start\":24512},{\"end\":25946,\"start\":25150},{\"end\":27033,\"start\":25948},{\"end\":27429,\"start\":27035},{\"end\":28353,\"start\":27465},{\"end\":29179,\"start\":28355},{\"end\":30030,\"start\":29181},{\"end\":30356,\"start\":30032},{\"end\":30661,\"start\":30400},{\"end\":31451,\"start\":30663},{\"end\":32139,\"start\":31466},{\"end\":32816,\"start\":32141}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12884,\"start\":12843},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14555,\"start\":14469},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15773,\"start\":15702},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17583,\"start\":17538},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17869,\"start\":17804},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18096,\"start\":18063},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18960,\"start\":18902},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19101,\"start\":19049},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20352,\"start\":20250},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21011,\"start\":20961},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21673,\"start\":21627},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22174,\"start\":22129},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22641,\"start\":22595},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23629,\"start\":23566}]", "table_ref": "[{\"end\":29914,\"start\":29907}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2286,\"start\":2274},{\"attributes\":{\"n\":\"2.\"},\"end\":7628,\"start\":7616},{\"attributes\":{\"n\":\"3.\"},\"end\":12806,\"start\":12800},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13999,\"start\":13962},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15500,\"start\":15475},{\"attributes\":{\"n\":\"3.3.\"},\"end\":20055,\"start\":20013},{\"attributes\":{\"n\":\"4.\"},\"end\":24510,\"start\":24499},{\"attributes\":{\"n\":\"4.1.\"},\"end\":25148,\"start\":25117},{\"attributes\":{\"n\":\"4.2.\"},\"end\":27463,\"start\":27432},{\"attributes\":{\"n\":\"4.3.\"},\"end\":30398,\"start\":30359},{\"attributes\":{\"n\":\"5.\"},\"end\":31464,\"start\":31454},{\"end\":32828,\"start\":32818},{\"end\":33083,\"start\":33063}]", "table": "[{\"end\":33831,\"start\":33452}]", "figure_caption": "[{\"end\":33061,\"start\":32830},{\"end\":33411,\"start\":33086},{\"end\":33452,\"start\":33414}]", "figure_ref": "[{\"end\":9140,\"start\":9132},{\"end\":27694,\"start\":27688},{\"end\":27944,\"start\":27936},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29726,\"start\":29720},{\"end\":30249,\"start\":30243},{\"end\":30429,\"start\":30423}]", "bib_author_first_name": "[{\"end\":34806,\"start\":34801},{\"end\":34827,\"start\":34826},{\"end\":35035,\"start\":35031},{\"end\":35048,\"start\":35042},{\"end\":35059,\"start\":35055},{\"end\":35242,\"start\":35239},{\"end\":35254,\"start\":35247},{\"end\":35265,\"start\":35259},{\"end\":35281,\"start\":35278},{\"end\":35300,\"start\":35294},{\"end\":35318,\"start\":35313},{\"end\":35333,\"start\":35326},{\"end\":35354,\"start\":35349},{\"end\":35369,\"start\":35365},{\"end\":35806,\"start\":35803},{\"end\":35818,\"start\":35811},{\"end\":35829,\"start\":35823},{\"end\":35847,\"start\":35842},{\"end\":35862,\"start\":35855},{\"end\":35883,\"start\":35878},{\"end\":35898,\"start\":35894},{\"end\":36273,\"start\":36267},{\"end\":36288,\"start\":36280},{\"end\":36303,\"start\":36295},{\"end\":36314,\"start\":36308},{\"end\":36326,\"start\":36319},{\"end\":36340,\"start\":36334},{\"end\":36352,\"start\":36347},{\"end\":36366,\"start\":36358},{\"end\":36377,\"start\":36372},{\"end\":36866,\"start\":36862},{\"end\":36880,\"start\":36873},{\"end\":36893,\"start\":36888},{\"end\":36911,\"start\":36903},{\"end\":36913,\"start\":36912},{\"end\":36924,\"start\":36922},{\"end\":36937,\"start\":36930},{\"end\":36941,\"start\":36938},{\"end\":37231,\"start\":37227},{\"end\":37245,\"start\":37238},{\"end\":37266,\"start\":37258},{\"end\":37280,\"start\":37272},{\"end\":37291,\"start\":37285},{\"end\":37305,\"start\":37297},{\"end\":37307,\"start\":37306},{\"end\":37323,\"start\":37316},{\"end\":37327,\"start\":37324},{\"end\":37341,\"start\":37336},{\"end\":37786,\"start\":37782},{\"end\":37798,\"start\":37793},{\"end\":37815,\"start\":37808},{\"end\":37825,\"start\":37823},{\"end\":37839,\"start\":37831},{\"end\":37841,\"start\":37840},{\"end\":37857,\"start\":37850},{\"end\":37861,\"start\":37858},{\"end\":38236,\"start\":38232},{\"end\":38248,\"start\":38243},{\"end\":38264,\"start\":38258},{\"end\":38277,\"start\":38270},{\"end\":38281,\"start\":38278},{\"end\":38293,\"start\":38290},{\"end\":38494,\"start\":38488},{\"end\":38512,\"start\":38505},{\"end\":38525,\"start\":38520},{\"end\":38774,\"start\":38769},{\"end\":38935,\"start\":38927},{\"end\":38946,\"start\":38942},{\"end\":38956,\"start\":38953},{\"end\":38968,\"start\":38962},{\"end\":38981,\"start\":38976},{\"end\":38996,\"start\":38992},{\"end\":39012,\"start\":39007},{\"end\":39335,\"start\":39327},{\"end\":39346,\"start\":39342},{\"end\":39360,\"start\":39357},{\"end\":39370,\"start\":39366},{\"end\":39384,\"start\":39377},{\"end\":39389,\"start\":39385},{\"end\":39402,\"start\":39397},{\"end\":39735,\"start\":39731},{\"end\":39756,\"start\":39752},{\"end\":39768,\"start\":39761},{\"end\":39784,\"start\":39777},{\"end\":39799,\"start\":39791},{\"end\":39813,\"start\":39807},{\"end\":40056,\"start\":40052},{\"end\":40069,\"start\":40062},{\"end\":40081,\"start\":40074},{\"end\":40097,\"start\":40092},{\"end\":40113,\"start\":40107},{\"end\":40338,\"start\":40332},{\"end\":40359,\"start\":40354},{\"end\":40378,\"start\":40372},{\"end\":40399,\"start\":40386},{\"end\":40677,\"start\":40672},{\"end\":40691,\"start\":40686},{\"end\":40693,\"start\":40692},{\"end\":40712,\"start\":40703},{\"end\":41039,\"start\":41036},{\"end\":41059,\"start\":41052},{\"end\":41074,\"start\":41069},{\"end\":41362,\"start\":41355},{\"end\":41385,\"start\":41378},{\"end\":41409,\"start\":41396},{\"end\":41647,\"start\":41640},{\"end\":41669,\"start\":41663},{\"end\":41687,\"start\":41682},{\"end\":41703,\"start\":41695},{\"end\":41729,\"start\":41716},{\"end\":41946,\"start\":41945},{\"end\":42324,\"start\":42318},{\"end\":42337,\"start\":42333},{\"end\":42348,\"start\":42344},{\"end\":42364,\"start\":42358},{\"end\":42585,\"start\":42577},{\"end\":42597,\"start\":42593},{\"end\":42616,\"start\":42609},{\"end\":42627,\"start\":42623},{\"end\":42640,\"start\":42635},{\"end\":42659,\"start\":42653},{\"end\":42975,\"start\":42974},{\"end\":42989,\"start\":42983},{\"end\":43180,\"start\":43175},{\"end\":43199,\"start\":43191},{\"end\":43210,\"start\":43204},{\"end\":43222,\"start\":43218},{\"end\":43237,\"start\":43230},{\"end\":43253,\"start\":43250},{\"end\":43265,\"start\":43261},{\"end\":43610,\"start\":43604},{\"end\":43618,\"start\":43615},{\"end\":43628,\"start\":43624},{\"end\":43641,\"start\":43634},{\"end\":43652,\"start\":43646},{\"end\":44250,\"start\":44242},{\"end\":44263,\"start\":44255},{\"end\":44277,\"start\":44273},{\"end\":44297,\"start\":44291},{\"end\":44318,\"start\":44310},{\"end\":44674,\"start\":44667},{\"end\":44683,\"start\":44679},{\"end\":44941,\"start\":44933},{\"end\":44953,\"start\":44946},{\"end\":44962,\"start\":44958},{\"end\":44982,\"start\":44976},{\"end\":45003,\"start\":44995},{\"end\":45376,\"start\":45368},{\"end\":45389,\"start\":45381},{\"end\":45398,\"start\":45394},{\"end\":45410,\"start\":45405},{\"end\":45420,\"start\":45417},{\"end\":45432,\"start\":45425},{\"end\":45446,\"start\":45437},{\"end\":45457,\"start\":45451},{\"end\":45475,\"start\":45470},{\"end\":45487,\"start\":45483},{\"end\":45843,\"start\":45837},{\"end\":45854,\"start\":45849},{\"end\":45871,\"start\":45864},{\"end\":45884,\"start\":45878},{\"end\":45886,\"start\":45885},{\"end\":45898,\"start\":45894},{\"end\":46130,\"start\":46123},{\"end\":46132,\"start\":46131},{\"end\":46149,\"start\":46143},{\"end\":46151,\"start\":46150},{\"end\":46434,\"start\":46427},{\"end\":46455,\"start\":46451},{\"end\":46465,\"start\":46464},{\"end\":46467,\"start\":46466},{\"end\":46483,\"start\":46475},{\"end\":46485,\"start\":46484},{\"end\":46501,\"start\":46495},{\"end\":46516,\"start\":46510},{\"end\":46777,\"start\":46774},{\"end\":46791,\"start\":46790},{\"end\":46807,\"start\":46800},{\"end\":46828,\"start\":46820},{\"end\":46830,\"start\":46829},{\"end\":46843,\"start\":46839},{\"end\":46855,\"start\":46852},{\"end\":47280,\"start\":47274},{\"end\":47293,\"start\":47289},{\"end\":47310,\"start\":47301},{\"end\":47328,\"start\":47319},{\"end\":47611,\"start\":47606},{\"end\":47625,\"start\":47622},{\"end\":47647,\"start\":47638},{\"end\":47657,\"start\":47654},{\"end\":47671,\"start\":47663},{\"end\":47682,\"start\":47678},{\"end\":47696,\"start\":47690},{\"end\":47711,\"start\":47706},{\"end\":48143,\"start\":48137},{\"end\":48162,\"start\":48158},{\"end\":48174,\"start\":48169},{\"end\":48188,\"start\":48183},{\"end\":48455,\"start\":48449},{\"end\":48475,\"start\":48470},{\"end\":48490,\"start\":48484},{\"end\":48506,\"start\":48500},{\"end\":48783,\"start\":48782},{\"end\":48797,\"start\":48792},{\"end\":48813,\"start\":48806},{\"end\":48827,\"start\":48822},{\"end\":48843,\"start\":48838},{\"end\":48860,\"start\":48855},{\"end\":48876,\"start\":48871},{\"end\":48891,\"start\":48885},{\"end\":48909,\"start\":48904},{\"end\":48925,\"start\":48919},{\"end\":48940,\"start\":48934},{\"end\":49286,\"start\":49280},{\"end\":49302,\"start\":49295},{\"end\":49320,\"start\":49313},{\"end\":49335,\"start\":49328},{\"end\":49350,\"start\":49341},{\"end\":49370,\"start\":49361},{\"end\":49633,\"start\":49624},{\"end\":49650,\"start\":49644},{\"end\":49661,\"start\":49655},{\"end\":49673,\"start\":49667},{\"end\":49684,\"start\":49679},{\"end\":49686,\"start\":49685},{\"end\":49698,\"start\":49695},{\"end\":49965,\"start\":49964},{\"end\":49980,\"start\":49974},{\"end\":50000,\"start\":49993},{\"end\":50014,\"start\":50007},{\"end\":50025,\"start\":50022},{\"end\":50042,\"start\":50034},{\"end\":50044,\"start\":50043},{\"end\":50380,\"start\":50375},{\"end\":50389,\"start\":50386},{\"end\":50406,\"start\":50395},{\"end\":50647,\"start\":50644},{\"end\":50659,\"start\":50653},{\"end\":50679,\"start\":50673},{\"end\":50699,\"start\":50691},{\"end\":50717,\"start\":50709},{\"end\":50731,\"start\":50727},{\"end\":50743,\"start\":50738},{\"end\":50752,\"start\":50749},{\"end\":50765,\"start\":50759},{\"end\":50780,\"start\":50772},{\"end\":51042,\"start\":51035},{\"end\":51058,\"start\":51051},{\"end\":51074,\"start\":51067},{\"end\":51086,\"start\":51080},{\"end\":51099,\"start\":51096},{\"end\":51118,\"start\":51112},{\"end\":51139,\"start\":51131},{\"end\":51141,\"start\":51140},{\"end\":51156,\"start\":51150},{\"end\":51168,\"start\":51157},{\"end\":51547,\"start\":51541},{\"end\":51561,\"start\":51554},{\"end\":51572,\"start\":51567},{\"end\":51588,\"start\":51579},{\"end\":51597,\"start\":51595},{\"end\":51607,\"start\":51602},{\"end\":51873,\"start\":51867},{\"end\":51884,\"start\":51879},{\"end\":51897,\"start\":51892},{\"end\":52178,\"start\":52171},{\"end\":52190,\"start\":52186},{\"end\":52207,\"start\":52200},{\"end\":52482,\"start\":52478},{\"end\":52498,\"start\":52490},{\"end\":52500,\"start\":52499},{\"end\":52898,\"start\":52895},{\"end\":52912,\"start\":52907},{\"end\":52924,\"start\":52921},{\"end\":52941,\"start\":52937},{\"end\":52959,\"start\":52951},{\"end\":52961,\"start\":52960},{\"end\":52976,\"start\":52970},{\"end\":52978,\"start\":52977},{\"end\":53363,\"start\":53359},{\"end\":53377,\"start\":53370},{\"end\":53387,\"start\":53383},{\"end\":53402,\"start\":53393},{\"end\":53417,\"start\":53413},{\"end\":53433,\"start\":53426},{\"end\":53742,\"start\":53738},{\"end\":53757,\"start\":53749},{\"end\":53769,\"start\":53764},{\"end\":53780,\"start\":53777},{\"end\":53793,\"start\":53788},{\"end\":54072,\"start\":54068},{\"end\":54084,\"start\":54079},{\"end\":54099,\"start\":54094},{\"end\":54111,\"start\":54108},{\"end\":54314,\"start\":54309},{\"end\":54333,\"start\":54325},{\"end\":54347,\"start\":54338},{\"end\":54571,\"start\":54565},{\"end\":54588,\"start\":54581},{\"end\":54600,\"start\":54593},{\"end\":54613,\"start\":54606},{\"end\":54624,\"start\":54620},{\"end\":54639,\"start\":54630},{\"end\":54652,\"start\":54650},{\"end\":54663,\"start\":54658},{\"end\":54972,\"start\":54969},{\"end\":54986,\"start\":54978},{\"end\":55000,\"start\":54994},{\"end\":55012,\"start\":55006},{\"end\":55021,\"start\":55017},{\"end\":55033,\"start\":55028},{\"end\":55051,\"start\":55044},{\"end\":55062,\"start\":55058},{\"end\":55425,\"start\":55423},{\"end\":55431,\"start\":55430},{\"end\":55433,\"start\":55432},{\"end\":55614,\"start\":55607},{\"end\":55628,\"start\":55622},{\"end\":55648,\"start\":55641},{\"end\":55669,\"start\":55664},{\"end\":55685,\"start\":55677},{\"end\":55709,\"start\":55696},{\"end\":56009,\"start\":56004},{\"end\":56011,\"start\":56010},{\"end\":56027,\"start\":56019},{\"end\":56041,\"start\":56034},{\"end\":56056,\"start\":56052},{\"end\":56425,\"start\":56422},{\"end\":56438,\"start\":56433},{\"end\":56452,\"start\":56445},{\"end\":56461,\"start\":56457},{\"end\":56752,\"start\":56749},{\"end\":56765,\"start\":56760},{\"end\":56780,\"start\":56772},{\"end\":56793,\"start\":56787},{\"end\":56804,\"start\":56800},{\"end\":57027,\"start\":57024},{\"end\":57041,\"start\":57035},{\"end\":57055,\"start\":57051},{\"end\":57072,\"start\":57065},{\"end\":57365,\"start\":57358},{\"end\":57374,\"start\":57373},{\"end\":57389,\"start\":57383},{\"end\":57406,\"start\":57402},{\"end\":57423,\"start\":57422},{\"end\":57441,\"start\":57433},{\"end\":57443,\"start\":57442},{\"end\":57823,\"start\":57815},{\"end\":57838,\"start\":57831},{\"end\":57850,\"start\":57844},{\"end\":57859,\"start\":57855},{\"end\":57871,\"start\":57864},{\"end\":57884,\"start\":57877},{\"end\":58131,\"start\":58129},{\"end\":58142,\"start\":58138},{\"end\":58153,\"start\":58148},{\"end\":58161,\"start\":58159},{\"end\":58173,\"start\":58168},{\"end\":58185,\"start\":58178},{\"end\":58423,\"start\":58416},{\"end\":58434,\"start\":58429},{\"end\":58444,\"start\":58442},{\"end\":58454,\"start\":58449}]", "bib_author_last_name": "[{\"end\":34813,\"start\":34807},{\"end\":34824,\"start\":34815},{\"end\":34834,\"start\":34828},{\"end\":34843,\"start\":34836},{\"end\":35040,\"start\":35036},{\"end\":35053,\"start\":35049},{\"end\":35067,\"start\":35060},{\"end\":35245,\"start\":35243},{\"end\":35257,\"start\":35255},{\"end\":35276,\"start\":35266},{\"end\":35292,\"start\":35282},{\"end\":35311,\"start\":35301},{\"end\":35324,\"start\":35319},{\"end\":35347,\"start\":35334},{\"end\":35363,\"start\":35355},{\"end\":35381,\"start\":35370},{\"end\":35809,\"start\":35807},{\"end\":35821,\"start\":35819},{\"end\":35840,\"start\":35830},{\"end\":35853,\"start\":35848},{\"end\":35876,\"start\":35863},{\"end\":35892,\"start\":35884},{\"end\":35910,\"start\":35899},{\"end\":36278,\"start\":36274},{\"end\":36293,\"start\":36289},{\"end\":36306,\"start\":36304},{\"end\":36317,\"start\":36315},{\"end\":36332,\"start\":36327},{\"end\":36345,\"start\":36341},{\"end\":36356,\"start\":36353},{\"end\":36370,\"start\":36367},{\"end\":36381,\"start\":36378},{\"end\":36871,\"start\":36867},{\"end\":36886,\"start\":36881},{\"end\":36901,\"start\":36894},{\"end\":36920,\"start\":36914},{\"end\":36928,\"start\":36925},{\"end\":36948,\"start\":36942},{\"end\":37236,\"start\":37232},{\"end\":37256,\"start\":37246},{\"end\":37270,\"start\":37267},{\"end\":37283,\"start\":37281},{\"end\":37295,\"start\":37292},{\"end\":37314,\"start\":37308},{\"end\":37334,\"start\":37328},{\"end\":37349,\"start\":37342},{\"end\":37791,\"start\":37787},{\"end\":37806,\"start\":37799},{\"end\":37821,\"start\":37816},{\"end\":37829,\"start\":37826},{\"end\":37848,\"start\":37842},{\"end\":37868,\"start\":37862},{\"end\":38241,\"start\":38237},{\"end\":38256,\"start\":38249},{\"end\":38268,\"start\":38265},{\"end\":38288,\"start\":38282},{\"end\":38299,\"start\":38294},{\"end\":38503,\"start\":38495},{\"end\":38518,\"start\":38513},{\"end\":38532,\"start\":38526},{\"end\":38781,\"start\":38775},{\"end\":38940,\"start\":38936},{\"end\":38951,\"start\":38947},{\"end\":38960,\"start\":38957},{\"end\":38974,\"start\":38969},{\"end\":38990,\"start\":38982},{\"end\":39005,\"start\":38997},{\"end\":39019,\"start\":39013},{\"end\":39340,\"start\":39336},{\"end\":39355,\"start\":39347},{\"end\":39364,\"start\":39361},{\"end\":39375,\"start\":39371},{\"end\":39395,\"start\":39390},{\"end\":39409,\"start\":39403},{\"end\":39750,\"start\":39736},{\"end\":39759,\"start\":39757},{\"end\":39775,\"start\":39769},{\"end\":39789,\"start\":39785},{\"end\":39805,\"start\":39800},{\"end\":39822,\"start\":39814},{\"end\":40060,\"start\":40057},{\"end\":40072,\"start\":40070},{\"end\":40090,\"start\":40082},{\"end\":40105,\"start\":40098},{\"end\":40122,\"start\":40114},{\"end\":40352,\"start\":40339},{\"end\":40370,\"start\":40360},{\"end\":40384,\"start\":40379},{\"end\":40404,\"start\":40400},{\"end\":40413,\"start\":40406},{\"end\":40684,\"start\":40678},{\"end\":40701,\"start\":40694},{\"end\":40729,\"start\":40713},{\"end\":40738,\"start\":40731},{\"end\":41050,\"start\":41040},{\"end\":41067,\"start\":41060},{\"end\":41083,\"start\":41075},{\"end\":41376,\"start\":41363},{\"end\":41394,\"start\":41386},{\"end\":41417,\"start\":41410},{\"end\":41661,\"start\":41648},{\"end\":41680,\"start\":41670},{\"end\":41693,\"start\":41688},{\"end\":41714,\"start\":41704},{\"end\":41737,\"start\":41730},{\"end\":41952,\"start\":41947},{\"end\":41960,\"start\":41954},{\"end\":42331,\"start\":42325},{\"end\":42342,\"start\":42338},{\"end\":42356,\"start\":42349},{\"end\":42369,\"start\":42365},{\"end\":42591,\"start\":42586},{\"end\":42607,\"start\":42598},{\"end\":42621,\"start\":42617},{\"end\":42633,\"start\":42628},{\"end\":42651,\"start\":42641},{\"end\":42668,\"start\":42660},{\"end\":42981,\"start\":42976},{\"end\":42994,\"start\":42990},{\"end\":43002,\"start\":42996},{\"end\":43189,\"start\":43181},{\"end\":43202,\"start\":43200},{\"end\":43216,\"start\":43211},{\"end\":43228,\"start\":43223},{\"end\":43248,\"start\":43238},{\"end\":43259,\"start\":43254},{\"end\":43273,\"start\":43266},{\"end\":43613,\"start\":43611},{\"end\":43622,\"start\":43619},{\"end\":43632,\"start\":43629},{\"end\":43644,\"start\":43642},{\"end\":43656,\"start\":43653},{\"end\":44253,\"start\":44251},{\"end\":44271,\"start\":44264},{\"end\":44289,\"start\":44278},{\"end\":44308,\"start\":44298},{\"end\":44329,\"start\":44319},{\"end\":44677,\"start\":44675},{\"end\":44691,\"start\":44684},{\"end\":44944,\"start\":44942},{\"end\":44956,\"start\":44954},{\"end\":44974,\"start\":44963},{\"end\":44993,\"start\":44983},{\"end\":45014,\"start\":45004},{\"end\":45379,\"start\":45377},{\"end\":45392,\"start\":45390},{\"end\":45403,\"start\":45399},{\"end\":45415,\"start\":45411},{\"end\":45423,\"start\":45421},{\"end\":45435,\"start\":45433},{\"end\":45449,\"start\":45447},{\"end\":45468,\"start\":45458},{\"end\":45481,\"start\":45476},{\"end\":45499,\"start\":45488},{\"end\":45847,\"start\":45844},{\"end\":45862,\"start\":45855},{\"end\":45876,\"start\":45872},{\"end\":45892,\"start\":45887},{\"end\":45906,\"start\":45899},{\"end\":46141,\"start\":46133},{\"end\":46157,\"start\":46152},{\"end\":46449,\"start\":46435},{\"end\":46462,\"start\":46456},{\"end\":46473,\"start\":46468},{\"end\":46493,\"start\":46486},{\"end\":46508,\"start\":46502},{\"end\":46528,\"start\":46517},{\"end\":46539,\"start\":46530},{\"end\":46788,\"start\":46778},{\"end\":46798,\"start\":46792},{\"end\":46818,\"start\":46808},{\"end\":46837,\"start\":46831},{\"end\":46850,\"start\":46844},{\"end\":46867,\"start\":46856},{\"end\":46871,\"start\":46869},{\"end\":47287,\"start\":47281},{\"end\":47299,\"start\":47294},{\"end\":47317,\"start\":47311},{\"end\":47335,\"start\":47329},{\"end\":47620,\"start\":47612},{\"end\":47636,\"start\":47626},{\"end\":47652,\"start\":47648},{\"end\":47661,\"start\":47658},{\"end\":47676,\"start\":47672},{\"end\":47688,\"start\":47683},{\"end\":47704,\"start\":47697},{\"end\":47718,\"start\":47712},{\"end\":48156,\"start\":48144},{\"end\":48167,\"start\":48163},{\"end\":48181,\"start\":48175},{\"end\":48198,\"start\":48189},{\"end\":48468,\"start\":48456},{\"end\":48482,\"start\":48476},{\"end\":48498,\"start\":48491},{\"end\":48512,\"start\":48507},{\"end\":48790,\"start\":48784},{\"end\":48804,\"start\":48798},{\"end\":48820,\"start\":48814},{\"end\":48836,\"start\":48828},{\"end\":48853,\"start\":48844},{\"end\":48869,\"start\":48861},{\"end\":48883,\"start\":48877},{\"end\":48902,\"start\":48892},{\"end\":48917,\"start\":48910},{\"end\":48932,\"start\":48926},{\"end\":48948,\"start\":48941},{\"end\":48955,\"start\":48950},{\"end\":49293,\"start\":49287},{\"end\":49311,\"start\":49303},{\"end\":49326,\"start\":49321},{\"end\":49339,\"start\":49336},{\"end\":49359,\"start\":49351},{\"end\":49379,\"start\":49371},{\"end\":49642,\"start\":49634},{\"end\":49653,\"start\":49651},{\"end\":49665,\"start\":49662},{\"end\":49677,\"start\":49674},{\"end\":49693,\"start\":49687},{\"end\":49704,\"start\":49699},{\"end\":49972,\"start\":49966},{\"end\":49991,\"start\":49981},{\"end\":50005,\"start\":50001},{\"end\":50020,\"start\":50015},{\"end\":50032,\"start\":50026},{\"end\":50055,\"start\":50045},{\"end\":50063,\"start\":50057},{\"end\":50384,\"start\":50381},{\"end\":50393,\"start\":50390},{\"end\":50411,\"start\":50407},{\"end\":50651,\"start\":50648},{\"end\":50671,\"start\":50660},{\"end\":50689,\"start\":50680},{\"end\":50707,\"start\":50700},{\"end\":50725,\"start\":50718},{\"end\":50736,\"start\":50732},{\"end\":50747,\"start\":50744},{\"end\":50757,\"start\":50753},{\"end\":50770,\"start\":50766},{\"end\":50786,\"start\":50781},{\"end\":51049,\"start\":51043},{\"end\":51065,\"start\":51059},{\"end\":51078,\"start\":51075},{\"end\":51094,\"start\":51087},{\"end\":51110,\"start\":51100},{\"end\":51129,\"start\":51119},{\"end\":51148,\"start\":51142},{\"end\":51179,\"start\":51169},{\"end\":51552,\"start\":51548},{\"end\":51565,\"start\":51562},{\"end\":51577,\"start\":51573},{\"end\":51593,\"start\":51589},{\"end\":51600,\"start\":51598},{\"end\":51611,\"start\":51608},{\"end\":51877,\"start\":51874},{\"end\":51890,\"start\":51885},{\"end\":51907,\"start\":51898},{\"end\":52184,\"start\":52179},{\"end\":52198,\"start\":52191},{\"end\":52222,\"start\":52208},{\"end\":52488,\"start\":52483},{\"end\":52507,\"start\":52501},{\"end\":52905,\"start\":52899},{\"end\":52919,\"start\":52913},{\"end\":52935,\"start\":52925},{\"end\":52949,\"start\":52942},{\"end\":52968,\"start\":52962},{\"end\":52989,\"start\":52979},{\"end\":53368,\"start\":53364},{\"end\":53381,\"start\":53378},{\"end\":53391,\"start\":53388},{\"end\":53411,\"start\":53403},{\"end\":53424,\"start\":53418},{\"end\":53438,\"start\":53434},{\"end\":53747,\"start\":53743},{\"end\":53762,\"start\":53758},{\"end\":53775,\"start\":53770},{\"end\":53786,\"start\":53781},{\"end\":53800,\"start\":53794},{\"end\":54077,\"start\":54073},{\"end\":54092,\"start\":54085},{\"end\":54106,\"start\":54100},{\"end\":54117,\"start\":54112},{\"end\":54323,\"start\":54315},{\"end\":54336,\"start\":54334},{\"end\":54357,\"start\":54348},{\"end\":54579,\"start\":54572},{\"end\":54591,\"start\":54589},{\"end\":54604,\"start\":54601},{\"end\":54618,\"start\":54614},{\"end\":54628,\"start\":54625},{\"end\":54648,\"start\":54640},{\"end\":54656,\"start\":54653},{\"end\":54667,\"start\":54664},{\"end\":54976,\"start\":54973},{\"end\":54992,\"start\":54987},{\"end\":55004,\"start\":55001},{\"end\":55015,\"start\":55013},{\"end\":55026,\"start\":55022},{\"end\":55042,\"start\":55034},{\"end\":55056,\"start\":55052},{\"end\":55067,\"start\":55063},{\"end\":55428,\"start\":55426},{\"end\":55441,\"start\":55434},{\"end\":55448,\"start\":55443},{\"end\":55620,\"start\":55615},{\"end\":55639,\"start\":55629},{\"end\":55662,\"start\":55649},{\"end\":55675,\"start\":55670},{\"end\":55694,\"start\":55686},{\"end\":55717,\"start\":55710},{\"end\":56017,\"start\":56012},{\"end\":56032,\"start\":56028},{\"end\":56050,\"start\":56042},{\"end\":56064,\"start\":56057},{\"end\":56431,\"start\":56426},{\"end\":56443,\"start\":56439},{\"end\":56455,\"start\":56453},{\"end\":56469,\"start\":56462},{\"end\":56758,\"start\":56753},{\"end\":56770,\"start\":56766},{\"end\":56785,\"start\":56781},{\"end\":56798,\"start\":56794},{\"end\":56812,\"start\":56805},{\"end\":57033,\"start\":57028},{\"end\":57049,\"start\":57042},{\"end\":57063,\"start\":57056},{\"end\":57079,\"start\":57073},{\"end\":57371,\"start\":57366},{\"end\":57381,\"start\":57375},{\"end\":57400,\"start\":57390},{\"end\":57411,\"start\":57407},{\"end\":57420,\"start\":57413},{\"end\":57431,\"start\":57424},{\"end\":57451,\"start\":57444},{\"end\":57459,\"start\":57453},{\"end\":57829,\"start\":57824},{\"end\":57842,\"start\":57839},{\"end\":57853,\"start\":57851},{\"end\":57862,\"start\":57860},{\"end\":57875,\"start\":57872},{\"end\":57889,\"start\":57885},{\"end\":58136,\"start\":58132},{\"end\":58146,\"start\":58143},{\"end\":58157,\"start\":58154},{\"end\":58166,\"start\":58162},{\"end\":58176,\"start\":58174},{\"end\":58189,\"start\":58186},{\"end\":58427,\"start\":58424},{\"end\":58440,\"start\":58435},{\"end\":58447,\"start\":58445},{\"end\":58458,\"start\":58455}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":34999,\"start\":34757},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":495068},\"end\":35237,\"start\":35001},{\"attributes\":{\"doi\":\"arXiv:2008.03824\",\"id\":\"b2\"},\"end\":35711,\"start\":35239},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":220647311},\"end\":36175,\"start\":35713},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":250526662},\"end\":36797,\"start\":36177},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":227351877},\"end\":37138,\"start\":36799},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":249209647},\"end\":37706,\"start\":37140},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":239998218},\"end\":38176,\"start\":37708},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":214743454},\"end\":38455,\"start\":38178},{\"attributes\":{\"id\":\"b9\"},\"end\":38731,\"start\":38457},{\"attributes\":{\"id\":\"b10\"},\"end\":38841,\"start\":38733},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":199442423},\"end\":39235,\"start\":38843},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":240353816},\"end\":39677,\"start\":39237},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":245006364},\"end\":39999,\"start\":39679},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":253098551},\"end\":40279,\"start\":40001},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":182952549},\"end\":40592,\"start\":40281},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10985549},\"end\":40938,\"start\":40594},{\"attributes\":{\"doi\":\"arXiv:2206.03380\",\"id\":\"b17\"},\"end\":41289,\"start\":40940},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":150374022},\"end\":41600,\"start\":41291},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4788055},\"end\":41919,\"start\":41602},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9226468},\"end\":42283,\"start\":41921},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2934104},\"end\":42500,\"start\":42285},{\"attributes\":{\"doi\":\"abs/2201.02533\",\"id\":\"b22\"},\"end\":42942,\"start\":42502},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14430259},\"end\":43100,\"start\":42944},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":91184158},\"end\":43491,\"start\":43102},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":254070603},\"end\":44129,\"start\":43493},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":147704018},\"end\":44578,\"start\":44131},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52095643},\"end\":44846,\"start\":44580},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52841588},\"end\":45284,\"start\":44848},{\"attributes\":{\"doi\":\"arXiv:2007.12868\",\"id\":\"b29\"},\"end\":45793,\"start\":45286},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221006004},\"end\":46052,\"start\":45795},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15545924},\"end\":46347,\"start\":46054},{\"attributes\":{\"doi\":\"arXiv, 2020. 3\",\"id\":\"b32\"},\"end\":46772,\"start\":46349},{\"attributes\":{\"doi\":\"arXiv:2003.08934\",\"id\":\"b33\"},\"end\":47199,\"start\":46774},{\"attributes\":{\"doi\":\"102:1-102:15\",\"id\":\"b34\",\"matched_paper_id\":246016186},\"end\":47604,\"start\":47201},{\"attributes\":{\"doi\":\"arXiv:2111.12503\",\"id\":\"b35\"},\"end\":48027,\"start\":47606},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b36\"},\"end\":48391,\"start\":48029},{\"attributes\":{\"id\":\"b37\"},\"end\":48735,\"start\":48393},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5947188},\"end\":49243,\"start\":48737},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":250921347},\"end\":49557,\"start\":49245},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":57759333},\"end\":49880,\"start\":49559},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":227348246},\"end\":50282,\"start\":49882},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":244477646},\"end\":50572,\"start\":50284},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":209140225},\"end\":51031,\"start\":50574},{\"attributes\":{\"id\":\"b44\"},\"end\":51451,\"start\":51033},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":253523781},\"end\":51803,\"start\":51453},{\"attributes\":{\"doi\":\"arXiv:2005.10821\",\"id\":\"b46\"},\"end\":52089,\"start\":51805},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":245334780},\"end\":52409,\"start\":52091},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":207194026},\"end\":52893,\"start\":52411},{\"attributes\":{\"id\":\"b49\"},\"end\":53266,\"start\":52895},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":235490453},\"end\":53642,\"start\":53268},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":251710519},\"end\":53996,\"start\":53644},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":237492104},\"end\":54270,\"start\":53998},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":245827946},\"end\":54476,\"start\":54272},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":251040899},\"end\":54890,\"start\":54478},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":247447583},\"end\":55362,\"start\":54892},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":54000152},\"end\":55559,\"start\":55364},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":186206662},\"end\":55922,\"start\":55561},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":238856848},\"end\":56331,\"start\":55924},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":247958019},\"end\":56646,\"start\":56333},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":232478349},\"end\":57022,\"start\":56648},{\"attributes\":{\"doi\":\"2020. 4\",\"id\":\"b61\"},\"end\":57268,\"start\":57024},{\"attributes\":{\"id\":\"b62\"},\"end\":57759,\"start\":57270},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":248177748},\"end\":58058,\"start\":57761},{\"attributes\":{\"id\":\"b64\"},\"end\":58352,\"start\":58060},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":233204283},\"end\":58594,\"start\":58354}]", "bib_title": "[{\"end\":34799,\"start\":34757},{\"end\":35029,\"start\":35001},{\"end\":35801,\"start\":35713},{\"end\":36265,\"start\":36177},{\"end\":36860,\"start\":36799},{\"end\":37225,\"start\":37140},{\"end\":37780,\"start\":37708},{\"end\":38230,\"start\":38178},{\"end\":38486,\"start\":38457},{\"end\":38925,\"start\":38843},{\"end\":39325,\"start\":39237},{\"end\":39729,\"start\":39679},{\"end\":40050,\"start\":40001},{\"end\":40330,\"start\":40281},{\"end\":40670,\"start\":40594},{\"end\":41353,\"start\":41291},{\"end\":41638,\"start\":41602},{\"end\":41943,\"start\":41921},{\"end\":42316,\"start\":42285},{\"end\":42575,\"start\":42502},{\"end\":42972,\"start\":42944},{\"end\":43173,\"start\":43102},{\"end\":43602,\"start\":43493},{\"end\":44240,\"start\":44131},{\"end\":44665,\"start\":44580},{\"end\":44931,\"start\":44848},{\"end\":45835,\"start\":45795},{\"end\":46121,\"start\":46054},{\"end\":47272,\"start\":47201},{\"end\":48447,\"start\":48393},{\"end\":48780,\"start\":48737},{\"end\":49278,\"start\":49245},{\"end\":49622,\"start\":49559},{\"end\":49962,\"start\":49882},{\"end\":50373,\"start\":50284},{\"end\":50642,\"start\":50574},{\"end\":51539,\"start\":51453},{\"end\":52169,\"start\":52091},{\"end\":52476,\"start\":52411},{\"end\":53357,\"start\":53268},{\"end\":53736,\"start\":53644},{\"end\":54066,\"start\":53998},{\"end\":54307,\"start\":54272},{\"end\":54563,\"start\":54478},{\"end\":54967,\"start\":54892},{\"end\":55421,\"start\":55364},{\"end\":55605,\"start\":55561},{\"end\":56002,\"start\":55924},{\"end\":56420,\"start\":56333},{\"end\":56747,\"start\":56648},{\"end\":57356,\"start\":57270},{\"end\":57813,\"start\":57761},{\"end\":58414,\"start\":58354}]", "bib_author": "[{\"end\":34815,\"start\":34801},{\"end\":34826,\"start\":34815},{\"end\":34836,\"start\":34826},{\"end\":34845,\"start\":34836},{\"end\":35042,\"start\":35031},{\"end\":35055,\"start\":35042},{\"end\":35069,\"start\":35055},{\"end\":35247,\"start\":35239},{\"end\":35259,\"start\":35247},{\"end\":35278,\"start\":35259},{\"end\":35294,\"start\":35278},{\"end\":35313,\"start\":35294},{\"end\":35326,\"start\":35313},{\"end\":35349,\"start\":35326},{\"end\":35365,\"start\":35349},{\"end\":35383,\"start\":35365},{\"end\":35811,\"start\":35803},{\"end\":35823,\"start\":35811},{\"end\":35842,\"start\":35823},{\"end\":35855,\"start\":35842},{\"end\":35878,\"start\":35855},{\"end\":35894,\"start\":35878},{\"end\":35912,\"start\":35894},{\"end\":36280,\"start\":36267},{\"end\":36295,\"start\":36280},{\"end\":36308,\"start\":36295},{\"end\":36319,\"start\":36308},{\"end\":36334,\"start\":36319},{\"end\":36347,\"start\":36334},{\"end\":36358,\"start\":36347},{\"end\":36372,\"start\":36358},{\"end\":36383,\"start\":36372},{\"end\":36873,\"start\":36862},{\"end\":36888,\"start\":36873},{\"end\":36903,\"start\":36888},{\"end\":36922,\"start\":36903},{\"end\":36930,\"start\":36922},{\"end\":36950,\"start\":36930},{\"end\":37238,\"start\":37227},{\"end\":37258,\"start\":37238},{\"end\":37272,\"start\":37258},{\"end\":37285,\"start\":37272},{\"end\":37297,\"start\":37285},{\"end\":37316,\"start\":37297},{\"end\":37336,\"start\":37316},{\"end\":37351,\"start\":37336},{\"end\":37793,\"start\":37782},{\"end\":37808,\"start\":37793},{\"end\":37823,\"start\":37808},{\"end\":37831,\"start\":37823},{\"end\":37850,\"start\":37831},{\"end\":37870,\"start\":37850},{\"end\":38243,\"start\":38232},{\"end\":38258,\"start\":38243},{\"end\":38270,\"start\":38258},{\"end\":38290,\"start\":38270},{\"end\":38301,\"start\":38290},{\"end\":38505,\"start\":38488},{\"end\":38520,\"start\":38505},{\"end\":38534,\"start\":38520},{\"end\":38783,\"start\":38769},{\"end\":38942,\"start\":38927},{\"end\":38953,\"start\":38942},{\"end\":38962,\"start\":38953},{\"end\":38976,\"start\":38962},{\"end\":38992,\"start\":38976},{\"end\":39007,\"start\":38992},{\"end\":39021,\"start\":39007},{\"end\":39342,\"start\":39327},{\"end\":39357,\"start\":39342},{\"end\":39366,\"start\":39357},{\"end\":39377,\"start\":39366},{\"end\":39397,\"start\":39377},{\"end\":39411,\"start\":39397},{\"end\":39752,\"start\":39731},{\"end\":39761,\"start\":39752},{\"end\":39777,\"start\":39761},{\"end\":39791,\"start\":39777},{\"end\":39807,\"start\":39791},{\"end\":39824,\"start\":39807},{\"end\":40062,\"start\":40052},{\"end\":40074,\"start\":40062},{\"end\":40092,\"start\":40074},{\"end\":40107,\"start\":40092},{\"end\":40124,\"start\":40107},{\"end\":40354,\"start\":40332},{\"end\":40372,\"start\":40354},{\"end\":40386,\"start\":40372},{\"end\":40406,\"start\":40386},{\"end\":40415,\"start\":40406},{\"end\":40686,\"start\":40672},{\"end\":40703,\"start\":40686},{\"end\":40731,\"start\":40703},{\"end\":40740,\"start\":40731},{\"end\":41052,\"start\":41036},{\"end\":41069,\"start\":41052},{\"end\":41085,\"start\":41069},{\"end\":41378,\"start\":41355},{\"end\":41396,\"start\":41378},{\"end\":41419,\"start\":41396},{\"end\":41663,\"start\":41640},{\"end\":41682,\"start\":41663},{\"end\":41695,\"start\":41682},{\"end\":41716,\"start\":41695},{\"end\":41739,\"start\":41716},{\"end\":41954,\"start\":41945},{\"end\":41962,\"start\":41954},{\"end\":42333,\"start\":42318},{\"end\":42344,\"start\":42333},{\"end\":42358,\"start\":42344},{\"end\":42371,\"start\":42358},{\"end\":42593,\"start\":42577},{\"end\":42609,\"start\":42593},{\"end\":42623,\"start\":42609},{\"end\":42635,\"start\":42623},{\"end\":42653,\"start\":42635},{\"end\":42670,\"start\":42653},{\"end\":42983,\"start\":42974},{\"end\":42996,\"start\":42983},{\"end\":43004,\"start\":42996},{\"end\":43191,\"start\":43175},{\"end\":43204,\"start\":43191},{\"end\":43218,\"start\":43204},{\"end\":43230,\"start\":43218},{\"end\":43250,\"start\":43230},{\"end\":43261,\"start\":43250},{\"end\":43275,\"start\":43261},{\"end\":43615,\"start\":43604},{\"end\":43624,\"start\":43615},{\"end\":43634,\"start\":43624},{\"end\":43646,\"start\":43634},{\"end\":43658,\"start\":43646},{\"end\":44255,\"start\":44242},{\"end\":44273,\"start\":44255},{\"end\":44291,\"start\":44273},{\"end\":44310,\"start\":44291},{\"end\":44331,\"start\":44310},{\"end\":44679,\"start\":44667},{\"end\":44693,\"start\":44679},{\"end\":44946,\"start\":44933},{\"end\":44958,\"start\":44946},{\"end\":44976,\"start\":44958},{\"end\":44995,\"start\":44976},{\"end\":45016,\"start\":44995},{\"end\":45381,\"start\":45368},{\"end\":45394,\"start\":45381},{\"end\":45405,\"start\":45394},{\"end\":45417,\"start\":45405},{\"end\":45425,\"start\":45417},{\"end\":45437,\"start\":45425},{\"end\":45451,\"start\":45437},{\"end\":45470,\"start\":45451},{\"end\":45483,\"start\":45470},{\"end\":45501,\"start\":45483},{\"end\":45849,\"start\":45837},{\"end\":45864,\"start\":45849},{\"end\":45878,\"start\":45864},{\"end\":45894,\"start\":45878},{\"end\":45908,\"start\":45894},{\"end\":46143,\"start\":46123},{\"end\":46159,\"start\":46143},{\"end\":46451,\"start\":46427},{\"end\":46464,\"start\":46451},{\"end\":46475,\"start\":46464},{\"end\":46495,\"start\":46475},{\"end\":46510,\"start\":46495},{\"end\":46530,\"start\":46510},{\"end\":46541,\"start\":46530},{\"end\":46790,\"start\":46774},{\"end\":46800,\"start\":46790},{\"end\":46820,\"start\":46800},{\"end\":46839,\"start\":46820},{\"end\":46852,\"start\":46839},{\"end\":46869,\"start\":46852},{\"end\":46873,\"start\":46869},{\"end\":47289,\"start\":47274},{\"end\":47301,\"start\":47289},{\"end\":47319,\"start\":47301},{\"end\":47337,\"start\":47319},{\"end\":47622,\"start\":47606},{\"end\":47638,\"start\":47622},{\"end\":47654,\"start\":47638},{\"end\":47663,\"start\":47654},{\"end\":47678,\"start\":47663},{\"end\":47690,\"start\":47678},{\"end\":47706,\"start\":47690},{\"end\":47720,\"start\":47706},{\"end\":48158,\"start\":48137},{\"end\":48169,\"start\":48158},{\"end\":48183,\"start\":48169},{\"end\":48200,\"start\":48183},{\"end\":48470,\"start\":48449},{\"end\":48484,\"start\":48470},{\"end\":48500,\"start\":48484},{\"end\":48514,\"start\":48500},{\"end\":48792,\"start\":48782},{\"end\":48806,\"start\":48792},{\"end\":48822,\"start\":48806},{\"end\":48838,\"start\":48822},{\"end\":48855,\"start\":48838},{\"end\":48871,\"start\":48855},{\"end\":48885,\"start\":48871},{\"end\":48904,\"start\":48885},{\"end\":48919,\"start\":48904},{\"end\":48934,\"start\":48919},{\"end\":48950,\"start\":48934},{\"end\":48957,\"start\":48950},{\"end\":49295,\"start\":49280},{\"end\":49313,\"start\":49295},{\"end\":49328,\"start\":49313},{\"end\":49341,\"start\":49328},{\"end\":49361,\"start\":49341},{\"end\":49381,\"start\":49361},{\"end\":49644,\"start\":49624},{\"end\":49655,\"start\":49644},{\"end\":49667,\"start\":49655},{\"end\":49679,\"start\":49667},{\"end\":49695,\"start\":49679},{\"end\":49706,\"start\":49695},{\"end\":49974,\"start\":49964},{\"end\":49993,\"start\":49974},{\"end\":50007,\"start\":49993},{\"end\":50022,\"start\":50007},{\"end\":50034,\"start\":50022},{\"end\":50057,\"start\":50034},{\"end\":50065,\"start\":50057},{\"end\":50386,\"start\":50375},{\"end\":50395,\"start\":50386},{\"end\":50413,\"start\":50395},{\"end\":50653,\"start\":50644},{\"end\":50673,\"start\":50653},{\"end\":50691,\"start\":50673},{\"end\":50709,\"start\":50691},{\"end\":50727,\"start\":50709},{\"end\":50738,\"start\":50727},{\"end\":50749,\"start\":50738},{\"end\":50759,\"start\":50749},{\"end\":50772,\"start\":50759},{\"end\":50788,\"start\":50772},{\"end\":51051,\"start\":51035},{\"end\":51067,\"start\":51051},{\"end\":51080,\"start\":51067},{\"end\":51096,\"start\":51080},{\"end\":51112,\"start\":51096},{\"end\":51131,\"start\":51112},{\"end\":51150,\"start\":51131},{\"end\":51181,\"start\":51150},{\"end\":51554,\"start\":51541},{\"end\":51567,\"start\":51554},{\"end\":51579,\"start\":51567},{\"end\":51595,\"start\":51579},{\"end\":51602,\"start\":51595},{\"end\":51613,\"start\":51602},{\"end\":51879,\"start\":51867},{\"end\":51892,\"start\":51879},{\"end\":51909,\"start\":51892},{\"end\":52186,\"start\":52171},{\"end\":52200,\"start\":52186},{\"end\":52224,\"start\":52200},{\"end\":52490,\"start\":52478},{\"end\":52509,\"start\":52490},{\"end\":52907,\"start\":52895},{\"end\":52921,\"start\":52907},{\"end\":52937,\"start\":52921},{\"end\":52951,\"start\":52937},{\"end\":52970,\"start\":52951},{\"end\":52991,\"start\":52970},{\"end\":53370,\"start\":53359},{\"end\":53383,\"start\":53370},{\"end\":53393,\"start\":53383},{\"end\":53413,\"start\":53393},{\"end\":53426,\"start\":53413},{\"end\":53440,\"start\":53426},{\"end\":53749,\"start\":53738},{\"end\":53764,\"start\":53749},{\"end\":53777,\"start\":53764},{\"end\":53788,\"start\":53777},{\"end\":53802,\"start\":53788},{\"end\":54079,\"start\":54068},{\"end\":54094,\"start\":54079},{\"end\":54108,\"start\":54094},{\"end\":54119,\"start\":54108},{\"end\":54325,\"start\":54309},{\"end\":54338,\"start\":54325},{\"end\":54359,\"start\":54338},{\"end\":54581,\"start\":54565},{\"end\":54593,\"start\":54581},{\"end\":54606,\"start\":54593},{\"end\":54620,\"start\":54606},{\"end\":54630,\"start\":54620},{\"end\":54650,\"start\":54630},{\"end\":54658,\"start\":54650},{\"end\":54669,\"start\":54658},{\"end\":54978,\"start\":54969},{\"end\":54994,\"start\":54978},{\"end\":55006,\"start\":54994},{\"end\":55017,\"start\":55006},{\"end\":55028,\"start\":55017},{\"end\":55044,\"start\":55028},{\"end\":55058,\"start\":55044},{\"end\":55069,\"start\":55058},{\"end\":55430,\"start\":55423},{\"end\":55443,\"start\":55430},{\"end\":55450,\"start\":55443},{\"end\":55622,\"start\":55607},{\"end\":55641,\"start\":55622},{\"end\":55664,\"start\":55641},{\"end\":55677,\"start\":55664},{\"end\":55696,\"start\":55677},{\"end\":55719,\"start\":55696},{\"end\":56019,\"start\":56004},{\"end\":56034,\"start\":56019},{\"end\":56052,\"start\":56034},{\"end\":56066,\"start\":56052},{\"end\":56433,\"start\":56422},{\"end\":56445,\"start\":56433},{\"end\":56457,\"start\":56445},{\"end\":56471,\"start\":56457},{\"end\":56760,\"start\":56749},{\"end\":56772,\"start\":56760},{\"end\":56787,\"start\":56772},{\"end\":56800,\"start\":56787},{\"end\":56814,\"start\":56800},{\"end\":57035,\"start\":57024},{\"end\":57051,\"start\":57035},{\"end\":57065,\"start\":57051},{\"end\":57081,\"start\":57065},{\"end\":57373,\"start\":57358},{\"end\":57383,\"start\":57373},{\"end\":57402,\"start\":57383},{\"end\":57413,\"start\":57402},{\"end\":57422,\"start\":57413},{\"end\":57433,\"start\":57422},{\"end\":57453,\"start\":57433},{\"end\":57461,\"start\":57453},{\"end\":57831,\"start\":57815},{\"end\":57844,\"start\":57831},{\"end\":57855,\"start\":57844},{\"end\":57864,\"start\":57855},{\"end\":57877,\"start\":57864},{\"end\":57891,\"start\":57877},{\"end\":58138,\"start\":58129},{\"end\":58148,\"start\":58138},{\"end\":58159,\"start\":58148},{\"end\":58168,\"start\":58159},{\"end\":58178,\"start\":58168},{\"end\":58191,\"start\":58178},{\"end\":58429,\"start\":58416},{\"end\":58442,\"start\":58429},{\"end\":58449,\"start\":58442},{\"end\":58460,\"start\":58449}]", "bib_venue": "[{\"end\":36502,\"start\":36451},{\"end\":42127,\"start\":42053},{\"end\":43774,\"start\":43750},{\"end\":52674,\"start\":52600},{\"end\":34862,\"start\":34845},{\"end\":35103,\"start\":35069},{\"end\":35451,\"start\":35399},{\"end\":35916,\"start\":35912},{\"end\":36449,\"start\":36383},{\"end\":36954,\"start\":36950},{\"end\":37410,\"start\":37351},{\"end\":37929,\"start\":37870},{\"end\":38305,\"start\":38301},{\"end\":38565,\"start\":38534},{\"end\":38767,\"start\":38733},{\"end\":39028,\"start\":39021},{\"end\":39418,\"start\":39411},{\"end\":39828,\"start\":39824},{\"end\":40131,\"start\":40124},{\"end\":40419,\"start\":40415},{\"end\":40744,\"start\":40740},{\"end\":41034,\"start\":40940},{\"end\":41423,\"start\":41419},{\"end\":41743,\"start\":41739},{\"end\":42051,\"start\":41962},{\"end\":42375,\"start\":42371},{\"end\":42702,\"start\":42684},{\"end\":43008,\"start\":43004},{\"end\":43279,\"start\":43275},{\"end\":43703,\"start\":43658},{\"end\":44335,\"start\":44331},{\"end\":44697,\"start\":44693},{\"end\":45050,\"start\":45016},{\"end\":45366,\"start\":45286},{\"end\":45912,\"start\":45908},{\"end\":46181,\"start\":46159},{\"end\":46425,\"start\":46349},{\"end\":46959,\"start\":46889},{\"end\":47383,\"start\":47349},{\"end\":47804,\"start\":47736},{\"end\":48135,\"start\":48029},{\"end\":48548,\"start\":48514},{\"end\":48973,\"start\":48957},{\"end\":49385,\"start\":49381},{\"end\":49710,\"start\":49706},{\"end\":50069,\"start\":50065},{\"end\":50417,\"start\":50413},{\"end\":50792,\"start\":50788},{\"end\":51617,\"start\":51613},{\"end\":51865,\"start\":51805},{\"end\":52228,\"start\":52224},{\"end\":52598,\"start\":52509},{\"end\":53070,\"start\":52991},{\"end\":53447,\"start\":53440},{\"end\":53806,\"start\":53802},{\"end\":54123,\"start\":54119},{\"end\":54363,\"start\":54359},{\"end\":54673,\"start\":54669},{\"end\":55114,\"start\":55069},{\"end\":55454,\"start\":55450},{\"end\":55723,\"start\":55719},{\"end\":56117,\"start\":56066},{\"end\":56481,\"start\":56471},{\"end\":56824,\"start\":56814},{\"end\":57142,\"start\":57088},{\"end\":57495,\"start\":57461},{\"end\":57901,\"start\":57891},{\"end\":58127,\"start\":58060},{\"end\":58464,\"start\":58460}]"}}}, "year": 2023, "month": 12, "day": 17}