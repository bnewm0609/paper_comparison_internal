{"id": 54489888, "updated": "2023-10-09 06:11:34.353", "metadata": {"title": "Deep Online Video Stabilization With Multi-Grid Warping Transformation Learning", "authors": "[{\"first\":\"Miao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Guo-Ye\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Jin-Kun\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Song-Hai\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ariel\",\"last\":\"Shamir\",\"middle\":[]},{\"first\":\"Shao-Ping\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Shi-Min\",\"last\":\"Hu\",\"middle\":[]}]", "venue": "IEEE Transactions on Image Processing", "journal": "IEEE Transactions on Image Processing", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Video stabilization techniques are essential for most hand-held captured videos due to high-frequency shakes. Several 2D-, 2.5D-, and 3D-based stabilization techniques have been presented previously, but to the best of our knowledge, no solutions based on deep neural networks had been proposed to date. The main reason for this omission is shortage in training data as well as the challenge of modeling the problem using neural networks. In this paper, we present a video stabilization technique using a convolutional neural network. Previous works usually propose an off-line algorithm that smoothes a holistic camera path based on feature matching. Instead, we focus on low-latency, real-time camera path smoothing that does not explicitly represent the camera path and does not use future frames. Our neural network model, called StabNet, learns a set of mesh-grid transformations progressively for each input frame from the previous set of stabilized camera frames and creates stable corresponding latent camera paths implicitly. To train the network, we collect a dataset of synchronized steady and unsteady video pairs via a specially designed hand-held hardware. Experimental results show that our proposed online method performs comparatively to the traditional off-line video stabilization methods without using future frames while running about 10 times faster. More importantly, our proposed StabNet is able to handle low-quality videos, such as night-scene videos, watermarked videos, blurry videos, and noisy videos, where the existing methods fail in feature extraction or matching.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2902346020", "acl": null, "pubmed": "30507533", "pubmedcentral": null, "dblp": "journals/tip/WangYLZSLH19", "doi": "10.1109/tip.2018.2884280"}}, "content": {"source": {"pdf_hash": "c96faae7bb03d3222c57d56c9383c0cfdb6590fa", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5d0531bf4075848b657964c24702c8eaab5727ff", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/c96faae7bb03d3222c57d56c9383c0cfdb6590fa.txt", "contents": "\nDeep Online Video Stabilization With Multi-Grid Warping Transformation Learning\nMAY 2019 2283\n\nMember, IEEEMiao Wang \nMember, IEEEGuo-Ye Yang \nMember, IEEEJin-Kun Lin \nMember, IEEESong-Hai Zhang \nMember, IEEEAriel Shamir \nMember, IEEEShao-Ping Lu \nSenior Member, IEEEShi-Min Hu \nSong-Hai Zhang \nM Wang \nG.-Y Yang \nJ.-K Lin \nS.-H Zhang \nS.-P Lu \n\nis with\nBeihang University\n100083BeijingChina\n\n\nTsinghua University\n100084BeijingChina\n\n\nNankai University\n300071TianjinChina. S.-M\n\n\nHu is with Tsinghua University\n100084BeijingChina\n\n\nBeihang University\n100084BeijingChina\n\nDeep Online Video Stabilization With Multi-Grid Warping Transformation Learning\n\nIEEE TRANSACTIONS ON IMAGE PROCESSING\n285MAY 2019 228310.1109/TIP.2018.2884280Manuscript received March 14, 2018; revised August 30, 2018 and October 21, 2018; accepted November 19, 2018. Date of publication November 30, 2018; date of current version January 30, 2019.The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Chia-Kai Liang. (Corresponding author: A. Shamir is with the Interdisciplinary Center Herzliya, Herzliya 46150, Israel.Index Terms-Video stabilization, video processing\nVideo stabilization techniques are essential for most hand-held captured videos due to high-frequency shakes. Several 2D-, 2.5D-, and 3D-based stabilization techniques have been presented previously, but to the best of our knowledge, no solutions based on deep neural networks had been proposed to date. The main reason for this omission is shortage in training data as well as the challenge of modeling the problem using neural networks. In this paper, we present a video stabilization technique using a convolutional neural network. Previous works usually propose an off-line algorithm that smoothes a holistic camera path based on feature matching. Instead, we focus on low-latency, real-time camera path smoothing that does not explicitly represent the camera path and does not use future frames. Our neural network model, called StabNet, learns a set of mesh-grid transformations progressively for each input frame from the previous set of stabilized camera frames and creates stable corresponding latent camera paths implicitly. To train the network, we collect a dataset of synchronized steady and unsteady video pairs via a specially designed hand-held hardware. Experimental results show that our proposed online method performs comparatively to the traditional off-line video stabilization methods without using future frames while running about 10 times faster. More importantly, our proposed StabNet is able to handle low-quality videos, such as night-scene videos, watermarked videos, blurry videos, and noisy videos, where the existing methods fail in feature extraction or matching.\n Fig. 1\n. Deep online video stabilization. We propose StabNet, a neural network that learns to predict transformations for each incoming unsteady frame, given the history of steady frames. Applying the predicted transformations to the original unsteady frame generates the stabilized output frame. The stabilized frames then act as historical frames for stabilizing the following unsteady frames.\n\n\nI. INTRODUCTION\n\nV IDEOS captured by hand-held camera are often not easy to watch due to shaky content. Several digital video stabilization techniques have been proposed in the past decade to improve the visual quality of hand-held videos, by removing high-frequency camera movements [1]- [5]. The majority of the proposed methods deal with this problem using a global view, by first estimating and then smoothing the camera path using offline computation. The very few online stabilization methods follow a 'capture\u2192 compute\u2192display' procedure for each incoming video frame in real time with low latency. Due to the real-time requirement in such methods, the camera motion is estimated by an Affine transformation, homography or using meshflow. In this paper, we focus on the online stabilization problem. Different from existing approaches, that must explicitly model the camera path to smooth it, we use a learning-based approach to directly compute a target steady transformation, with guidance from historical stabilized frames (see Figure 1).\n\nIn recent years, we have witnessed how convolutional neural networks (CNNs) changed Computer Vision and Computer Graphics fields. Methods that are based on CNNs perform more accurately and more efficiently. For example, several traditional video processing topics such as video stylization [6] and video deblurring [7] are re-addressed using CNNs. To our knowledge, there are no CNN-based methods published for digital video stabilization, although it is an important topic in video processing. We observed two main obstacles that prevent a CNN-based stabilization solution. First, the lack of training data: pairs of steady and unsteady synchronized videos with an identical capturing route and content are required for training a CNN model. While this is not necessary for traditional methods, it is essential for a learning-based stabilization approach. Second, the challenge of correct problem definition: traditional stabilization methods compute and smooth a camera path, which cannot be easily adapted to a CNN-based solution. A somewhat different problem definition is required.\n\nBased on these observations, we propose to solve the corresponding issues by creating a practical data set for training a neural network, and modifying the formulation of the problem by defining a progressive online stabilization algorithm. First, to collect training data, we captured synchronized handheld steady/unsteady video pairs using a special hardware. We remodeled a hand-held stabilizer with two cameras, where only one camera is stabilized by the stabilizer while the other camera is fixed to the stabilizer grip, moving consistently with the hand motions. Second, in our modified formulation of the stabilization problem, instead of estimating and smoothing a virtual camera path, we learn transformations for spatially distributed regular mesh grids from each unsteady frame progressively along the time-line, and generate a steady output video in an online fashion.\n\nWe present StabNet, a CNN model to stabilize frames with light-weighted feed-forward operations through the network. The learning process is driven by the information of historically stabilized frames with the supervised ground-truth steady frame. Figure 1 shows the overview of our deep video stabilization. The proposed deep stabilization method performs comparably well on test videos collected from existing works. The main merit of our algorithm is the ability to run in real-time at 35.5 FPS with minumum latency (1 frame) on a NVIDIA GTX 1080Ti graphic card, being about 10\u00d7 faster than offline methods. More importantly, our method is superior to existing methods with the ability to handle low-quality videos, such as night-scene videos, watermarked videos, blurry videos and noisy videos, where existing featurematching based methods may totally fail. To our knowledge, the proposed StabNet is a pioneer in using convolutional network for digital video stabilization.\n\nWe also built the DeepStab dataset consisting of pairs of synchronized steady/unsteady videos. We have released the dataset and believe that it will benefit the community for future research on stabilization using data-driven methods.\n\n\nII. RELATED WORK\n\nOur work is closely related to digital video stabilization approaches and deep learning video manipulation.\n\n\nA. Digital Video Stabilization\n\nExisting offline stabilization techniques estimate the camera trajectory from 2D, 2.5D or 3D perspective and then synthesize a new smooth camera trajectory to remove the undesirable high-frequency motion. 2D video stabilization methods estimate (bundled) homography or affine transformations between consecutive frames and smooth these transformations temporally. In pioneer works, low-pass filters were applied to smooth parameters of models [1], [8]. An L 1 -norm optimizationbased method was proposed by Grundman et al. [3] with a path synthesis consisting of simple cinematography motions. Later, a bundled camera path based model was proposed by Liu et al. [5], estimating and smoothing multiple local camera paths. Zhang et al. [9] proposed to optimize geodesics on the Lie group embedded in transformation space to stabilize video. Liang et al. [10] analyzed the rolling shutter effect via global motion estimation and velocity estimation, and corrected the distortion via local motion refinement and scanline realignment. 3D-based video stabilization approaches reconstruct the 3D scene [11] from video, then estimate and smooth the 3D camera trajectory. Content-preserve warping [2] was proposed as the first 3D stabilization method. Later, subspace video stabilization [4] was proposed with long tracked features smoothed using subspace constraints. Goldstein and Fattal [12] proposed to enhance the length of feature tracks with epipolar transfer. Generally speaking, 2D stabilization methods perform efficiently and robustly, and 3D-based methods are able to generate visually better results.\n\nReal-time online stabilization is specifically desired for live stream applications. Solutions combining the gyroscope hardware and image contents were applied on mobile phones [13]. Liu et al. [14] proposed an online stabilization method which only use historical camera path to compute warping functions for incoming frames. Inspired by their idea, we present a deep online stabilization approach which performs stabilization given a few historical stabilized frames. The novelty of our approach is that we avoid explicitly estimating and smoothing camera path, instead, we use a CNN model to directly predict warping functions.\n\n\nB. CNNs for Video Applications\n\nIn recent years, CNNs have made huge improvements in computer vision tasks such as image recognition [15], [16] and generation [17], [18]. When feeding multiple successive frames from videos, CNNs can predict optical flow [19], camera motion [20], or semantics [21]. There are several works which use CNNs to directly produce video contents, such as scene dynamic generation [22], frame interpolation [23] and deblurring [7], [24]. Because predicting a long video sequence is still a challenging problem, all of the above works used only two or very few successive frames as training samples. The proposed StabNet also considers a temporal neighborhood at each time. The stabilization problem cannot be solved using a generation-based model because of the severe vibration of the input video content. To generate visually pleasing result, our StabNet learns the warping transformations instead of generating pixel values.\n\n\nIII. TRAINING DATASET\n\nGenerating training data is one of the key challenges for digital video stabilization, where ground truth data cannot be easily collected/labeled. To train StabNet, two synchronized video sequences of the same scene are required: one sequence captures a steady camera movement, while the other is unstable. One possible way to generate such data is to render a  virtual scene with two camera path configurations: smooth and jumpy. However, CNN models trained using rendered virtual scene may not generalize well due to the domain gap between training synthetic video and testing real videos captured by hand-held camera. To generate authentic data, we designed a specialized hardware with two portable GoPro Hero 4 Black cameras and a hand-held stabilizer, 1 where the cameras lay horizontally next to each other with small disparity (Figure 3). When capturing videos, the two cameras shoot synchronously, with only one camera stabilized, while the other moves consistently with the hand/body motion of the holder. We turned off the auto-focus and auto-exposure functions of the cameras and used the synchronous remote control for synchronization.\n\nTraining videos are obtained by holding the designed hardware while taking videos in a first-person point of view. We present the DeepStab dataset, containing pairs of synchronized videos with diverse camera movements. The dataset includes indoor scenes with large parallax, and common outdoor scenes with buildings, vegetation, crowd, etc. Camera motions include forward movement, pan movement, spin 1 https://www.youtube.com/watch?v=8vu7IDuDD64 movement and complex movements including combinations of the above, at various speed. We remove the fish-eye distortion of the videos in post-processing. We trim parts with large lighting difference between the cameras pair, and videos with nonoverlapping field of views of the cameras by aligning the frame content and cropping a new rectangular view for each camera.\n\nIn total, we collected 60 pairs of synchronized videos whose length is within 30 seconds, at 30 FPS. The videos are split into 44 training pairs, 8 validation pairs and 8 testing pairs. Figure 2 shows representative sampled frames from the dataset. The recorded video pairs are augmented to provide more training samples by horizontally flipping the frames, reversing the video sequences and combining both flipping and reversing.\n\n\nIV. THE STABNET\n\nOverview: We propose to stabilize the video without using future frames, relying on how the hand-held stabilizer works during capturing paired steady and unsteady videos. We convert the online stabilization problem to a supervised learning problem of conditional transformation regression without explicitly computing a camera path. Our goal is to learn to warp the input video from an unstable camera to a virtually stable camera horizontally next to the unstable camera with a small parallax, as in the training data.\n\nThe inputs to StabNet are an incoming unsteady frame I t and six conditional historical steady frames sampled from approximately one second\nS t = \u0130 t \u221232 ,\u0130 t \u221216 ,\u0130 t \u22128 , I t \u22124 ,\u0130 t \u22122 ,\u0130 t \u22121 for time-stamp t.\nThe sampling of historical frames are denser near the incoming frame and sparser far from the incoming frame. Inspired by [5] which uses a bundled camera model for stabilization, we propose to regress a transformation f\ni, j t for the (i, j )-th regularly divided mesh- grid g i, j t , where a 4 \u00d7 4 mesh G t = {g i, j t |1 \u2264 i, j \u2264 4} are spatially distributed on frame I t . The output of our model is consequentially a set of transformations F t = { f i, j t |1 \u2264 i, j \u2264 4} for frame I t .\nThe steady frame is then created by applying Fig. 4. Network Architecture. StabNet is a two-branch Siamese network with shared parameters in each branch. It consists of an Encoder and a Multi-Grid Regressor. The Encoder is an adapted ResNet-50 backbone model, which encodes input concatenated frames into a 1 \u00d7 1 \u00d7 2048 feature vector. The Multi-Grid Regressor consists of a sequence of FC layers where the last fc reg layer regresses grid vertex positions with dimension (h + 1) \u00d7 (w + 1) \u00d7 2, and h, w are grid numbers along x-axis and y-axis respectively. During training, samples I t , s t and I t\u22121 , s t\u22121 of two successive incoming frames with corresponding historical frames fed to the network. The transformations F t and F t\u22121 are then predicted. The network is trained with stability loss, shape-preserving loss and temporal loss.\nI t = F t * I t , where * is the warping operator. We use the desired vertices {(x i t ,\u0177 j t ), (x i+1 t ,\u0177 j t ), (x i ,\u0177 j +1 t ), (x i+1 ,\u0177 j +1 )} of the deformed mesh grid\u011d i, j t to represent each transfor- mation f i, j t .\nOur network can regress the mesh grid vertex transformation representation, and can drive the warping of image content located inside the grid. The learning process is supervised by our ground-truth steady frames I t . When training StabNet, the conditional inputs S t are the groundtruth steady frames\nI t \u221232 , I t \u221216 , I t \u22128 , I t \u22124 , I t \u22122 , I t \u22121 , while when testing, S t are the historical stabilized frames I t \u221232 , I t \u221216 , I t \u22128 , I t \u22124 , I t \u22122 , I t \u22121 .\n\nA. Network Architecture\n\nOur StabNet is a Siamese network [25] that has two branches sharing the network parameters. We use a Siamese architecture to preserve temporal consistency of successive transformed frames I t \u22121 = F t \u22121 * I t \u22121 and I t = F t * I t . Each branch of StabNet is a two-stage network consisting of a backbone encoder, that extracts high-level features from the inputs and a multi-grid transformation regressor, that predicts the stabilization transformations from the extracted feature map. Figure 4 shows the architecture of StabNet. The inputs are seven concatenated grayscale frames, each with dimension W \u00d7 H \u00d7 1, consisting of six conditional steady frames S t and one unsteady frame I t . Frames are sent to an encoder to extract features. This encoder adapts ResNet-50 [16] as the backbone feature extractor, using the conv 1 as the input channel, modified to meet our inputs, and removing all layers after average pooling. The extracted feature map from the encoder is of dimension 1 \u00d7 1 \u00d7 2048. Next, we use a sequence of FC layers with output feature dimensions 2048, 1024, 512, (h + 1) \u00d7 (w + 1) \u00d7 2, where w = 4 and h = 4 are grid sizes along x-axis and y-axis respectively. The output dimension corresponds to the total number of grid vertex points.\n\n\nB. Stabilization Loss Functions\n\nStabNet training process is driven by three types of loss functions: stability loss, shape-preserving loss and temporal smoothness loss. The comprehensive loss function is based on neighboring input frames I t and I t \u22121 , and is defined as:\nL = i\u2208{t,t \u22121} L stab (F i , I i ) + L shape (F i , G i ) +L temp (F t , F t \u22121 , I t , I t \u22121 ), (1)\nwhere L stab is the stability loss, L shape is the shape-preserving loss and L temp is the temporal loss.\n\n1) Stability Loss: The stability loss drives the warped unsteady frames to the ground-truth steady frames using cues of pixel alignment and feature point alignment. It is defined as: (2) where L pixel is the pixel alignment term, L feature is the feature alignment term, and \u03b1 1 = 50.0, \u03b1 2 = 1.0 are constant weights.\nL stab (F t , I t ) = \u03b1 1 L pixel (F t , I t ) + \u03b1 2 L feature (F t , I t ),\nThe pixel alignment term L pixel measures how the transformed frame I t = F t * I t aligns with the ground-truth steady frame I t , using mean squared error (MSE):\nL pixel (F t , I t ) = 1 D ||I t \u2212 F t * I t || 2 2 ,(3)\nwhere D is the spatial dimension of frame. The transformation F t * I t operates in the image domain. To make the warping function differentiable, we used spatial transformer layer [26]. L pixel loss will be small if the transformed frame I t aligns well with the ground-truth frame I t . However, during training I t can not converge well to I t . During early training stages, unsteady and steady frames are not aligned and the loss term is less correlated. For better convergence during training, we further introduce a feature alignment loss.\n\nThe feature alignment term L feature is computed as the average alignment error of matched feature points after transforming the unsteady frame I t using the predicted transformation F t :\nL feature (F t , I t ) = 1 m m i=1 || p i t \u2212 F t * p i t || 2 2 .(4)where P t = {p i t , p i t | i \u2208 {1\n, \u00b7 \u00b7 \u00b7 , m}} are the m pairs of matched feature points between each steady/unsteady frame pair, and p i t and p i t are the i -th matched feature points from unsteady frame I t and ground-truth steady frame I t respectively.\n\nTo compute the feature loss, all pairs P t are computed in a pre-processing stage between steady and unsteady frame pairs. We extract SURF features [27] from both I t and I t , then calculate the matching between them by dividing the frames into 2 \u00d7 2 sub-images, and using a RANSAC algorithm [28] to fit a Homography in each corresponding sub-image. We match features in 2 \u00d7 2 sub-images instead of 4 \u00d7 4 as in [5], because of the large camera pose and content variation between the steady and unsteady cameras. Please note that the feature extraction and feature matching processes are only performed for training the network and not needed during online stabilization.\n\n2) Shape-Preserving Loss: Because our model regresses mesh vertex positions of the stabilized video, it is important to preserve the shapes of grids to avoid distortion artifact and to encourage neighboring grids to transform consistently. Our shape-preserving loss thus consists of an intra-grid distortion term L intra and an inter-grid consistency term L inter .\n\nInspired by [2], we introduce an intra-grid loss L intra to encourage the triangle of neighboring deformed vertices {v t ,v 0 t ,v 1 t } \u2282 f t * g t to follow a similarity transformation:\nL intra (F t , G t ) = 1 N v t ||v t \u2212v 1 t \u2212 s R v 01 t || 2 2 , R = 0 1 \u22121 0 ,(5)\nwhere v 01\nt =v 0 t \u2212v 1 t ,v 0 t andv 1 t are neighboring vertices and s = ||v t \u2212 v 1 t ||/||v 0 t \u2212 v 1 t ||, {v t , v 0 t , v 1 t } \u2282 g t\nis the ratio of original grid side lengths, N is the total amount of triangular vertices.\n\nTo encourage the neighboring grids to transform consistently, we introduce an inter-grid loss L inter . For each vertex v t and its neighboring vertices v 0 t , v 1 t along an edge of two original neighboring grids, the two vectors v t =v 1 t \u2212v t and v 0 t =v t \u2212v 0 t formed by deformed vertices are encouraged to be identical:\nL inter (F t , G t ) = 1 M v 0 t ,v t ,v 1 t ||v 1 t \u2212v t \u2212 (v t \u2212v 0 t )|| 2 2 ,(6)\nwhere v 0 t ,v t ,v 1 t are three successive deformed grid vertices belonging to F t * G t , along an original mesh edge, M is the total amount of successive vertex tuples of the mesh. Figure 5 shows an illustration of the loss terms.\n\nThe shape-preserving loss is then defined as the combination of the above two terms: (7) with the weights set as \u03b3 1 = 1.0, \u03b3 2 = 20.0.\nL shape (F t , G t ) = \u03b3 1 L intra (F t , G t ) + \u03b3 2 L inter (F t , G t ),\n3) Temporal Loss: Simply applying the transformations separately to every video frame can create wobble artifacts in the video. Therefore, we incorporate a temporal loss term to enforce temporal coherency between adjacent frames using the Siamese network architecture. Each time two successive samples I t , s t and I t \u22121 , s t \u22121 are fed into StabNet, two successive transformations F t and F t \u22121 are predicted. The temporal loss is defined as the mean square error between the successive output frames:\nL temp (F t , F t \u22121 , I t , I t \u22121 ) = \u03bb 1 D ||F t * I t \u2212 w(F t \u22121 * I t \u22121 )|| 2 2 ,(8)\nwhere D is the spatial dimension of frame, w(\u00b7) is a function that warps the steady frame at t \u2212 1 to the steady frame t according to pre-computed optical flow, \u03bb = 10.0 is a constant. In our experiments we use TV-L1 algorithm [29] to compute the optical flow, but alternative methods for optical flow calculation can also be used.\n\n\nC. Implementation Details\n\nTo train StabNet, we resize the videos to a spatial dimension of W = 512 and H = 288 for efficiency. Pre-trained ResNet-50 model on ImageNet [15] without the Conv 1 layer is loaded, and is fine-tuned during the training process. We use minibatch size of 8 and ADAM [30] for optimization with \u03b2 1 = 0.9, \u03b2 2 = 0.999. Initial learning rate is set to 2e-5, and multiplied by 0.1 every 30, 000 iterations. The training is initialized to learn identity transformations for 300 iterations before introducing aforementioned losses. The training process is terminated when reaching 90, 000 iterations. The whole training process takes about 20 hours on an NVIDIA GTX 1080 Ti graphics card.\n\nIn training process, we feed two successive samples to the two branches (with shared network parameters) of Stab-Net so that temporal coherency is aware during learning. However, during testing, the network is used to stabilize a single frame at a time; temporal consistency is automatically preserved. Further, the stabilization processing is self-driven for a test video as follows: we start by duplicating the first frame and regard the duplicated frames as S 1 . After stabilizing frame I t , historical stabilized frames I t \u221231 , I t \u221215 , I t \u22127 , I t \u22123 , I t \u22121 , I t are regarded as S t +1 for stabilizing the next frame I t +1 . This process is repeated through the time-line.\n\nThe stabilization results inevitably have meaningless frame borders introduced by the warping function. As StabNet uses stabilized frames as the inputs for future frames, we need to make StabNet robust to such borders. During training, we add some black borders produced by Homography perturbation around the Identity transformation to the groundtruth historical frames. The Homography perturbances are  \n\n\nV. EXPERIMENTAL RESULTS\n\nWe train the StabNet model on the DeepStab dataset, and test it on various video sources. Testing videos are from our DeepStab testing set, previous dataset [5] and mobile phone cameras. On average, testing runs at 35.5 FPS on a graphics card, which meets the requirement of real-time online stabilization with 1 frame latency.\n\nWe use quantitative evaluation metrics, computed following [14] to evaluate stabilization methods. The three metrics are cropping ratio, distortion and stability.\n\nCropping Ratio: This metric measures the area of the remaining content after stabilization. Larger cropping ratio with less cropping is favored. Per frame cropping ratio is computed as the scale component of the global Homography H t estimated from input frame I t to output frame I t . Ratio values of video frames are averaged to generate the cropping ratio value of the whole video.\n\nDistortion Value: Distortion value evaluates the distortion degree introduced by stabilization. Per frame distortion value is computed by the ratio of the two largest eigenvalues of the affine part of the Homography H t . The minimum value which represents the worst distortion is chosen as the distortion value for the whole video.\n\nStability Score: Stability score measures how stable a video is. Following [14], we use frequency-domain analysis of camera paths to estimate the stability score. Spatially distributed camera paths are computed as vertex profiles for 4 \u00d7 4 mesh grid vertices between successive frames. The vertex profiles are then presented as 1D temporal signals for frequency domain analysis. We take each of their lowest frequencies components over full frequencies (DC component is excluded) as the stability score [14]. Averaging from all profiles gives the final score.\n\n\nA. Ablation Study\n\nIn order to evaluate the effectiveness of our proposed framework, we experiment with other possible network architectures, loss functions and alternative input solutions. We conduct an ablation study on public stabilization dataset from [5] which consists of several video categories according to scene type and camera motion, including Regular, Quick Rotation, Quick Zooming, Large Parallax, Running and Crowd. 1) Network Architectures: Because our network uses multigrid regressor (denoted as MGR) to learn transformations for each input frame, here we evaluate how the proposed MGR performs against the single-regressor one (denoted as SGR) and the alternative MGR variations with various grid divisions. We implement the variations using the same backbone ResNet-50 encoder, and similar regressor architectures with the output channels (h + 1) \u00d7 (w + 1) \u00d7 2 adapted to mesh division choices. In SGR, four frame vertex positions are regressed. In MGR variations, 2 \u00d7 2 and 8 \u00d7 8 mesh vertex positions are regressed, with architectures denoted as MGR-2 and MGR-8 respectively. As a result, the stability level of results from SGR and MGR-2 are inferior to our MGR, as they regress coarser grids; at the same time MGR-2 and SGR's cropping ratio values are generally higher than our method. This is because in a method with a better stability, a more flexible warping must be performed, with larger warping borders. We also observe that although the mesh division of MGR-8 is finer than our model, regressing transformations in such granularity ended in failure.\n\n2) Loss Functions: We test the proposed StabNet with some of the loss terms turned off to validate the loss function setup. We observed that without consistency loss, the network training will not converge, and other alternative results are worse than the proposed one. Fig. 6. Compare with publicly available videos from [5] in terms of three metrics.\n\n\n3) Input Variations:\n\nWe experiment with different stacked input frame sequences during training the model. The variations are: 1) training one frame supervised by neighboring historical frames as I t \u22125 , I t \u22124 , \u00b7 \u00b7 \u00b7 , I t \u22121 , I ; 2) training one frame supervised by uniformly distributed historical frames as\nI t \u221231 , I t \u221225 , I t \u221219 , I t \u221213 , I t \u22127 , I t \u22121 , I ; 3)\ntraining current frame and a few future frames with historical guidance as\nI t \u221232 , I t \u221216 , . . . , I t \u22122 , I t \u22121 , I, I t +1 , I t +2 , . . . , I t +16 , I t +32 .\nCorresponding performances are reported in Table I, as a conclusion, results from alternative inputs are inferior to the proposed one in terms of stability. We also experiment with the back-bone of ResNet-101, however the improvement is not apparent, with running time increased.\n\n\nB. Comparison With Publicly Available Results\n\nWe compare with [2]- [4], [12], and [14] using six publicly available videos in terms of the objective metrics, based on results provided by corresponding authors. Comparing with offline stabilizations is slightly unfair for our method because future-frames information is not available for our online stabilization method in real-time. As a result, the stability score of our method is slightly lower, occasionally with probable visual artifacts of unnatural cross-frame wobbling and distortion. This is mainly because our online method only uses historical frames without holistic knowledge of the full camera path. Nevertheless, our method performs in real time while being visually comparable to all existing methods. Comparison details are shown in Figure 6, for videos that we were not able to find the result, we leave it blank.\n\n\nC. Comparison With the State-of-the-Art Software\n\nWe further compare our method with commercial offline stabilization software Adobe Premiere CS6 on dataset [5]. As far as we know, Adobe Premiere stabilizer is developed based on subspace stabilization [4]. We choose the default parameters for Adobe Premiere (smoothness: 50%, 'Smooth Motion' and 'Subspace Warp') to produce results. Figure 7 shows an visualization of feature trajectories before and after  stabilization. Further evaluation on the test dataset is reported in Figure 8. Please note that the online stabilization problem is inherently harder than offline stabilization, because only historical frames are available in online stabilization, without the global sense of the camera path. Hence, the quantitative performance statistics for online stabilization methods would be inferior to offline ones. However the average running time performance of our method is superior to all existing methods. The running time performance is given in Table II.\n\n\nD. Stabilizing Low-Quality Videos\n\nOne promising feature of StabNet is its robustness to lowquality videos caused by noise, motion blur, etc. When dealing with such videos, traditional methods could fail because of  either feature extraction failures from one frame or featuremismatches between frames. We demonstrate the superiority of StabNet to traditional feature-based methods via four types of low-quality videos captured by mobile phones: night-scene videos, watermarked videos, blurry videos and noisy videos, whose representative frames are shown in Figure 9.\n\n1) Night-Scene Videos: We test the proposed network on night-scene videos. Such videos are typically blurry and contain severe noise, where wobble distortions can appear from traditional feature matching-based stabilization methods.\n\n2) Watermarked Videos: Watermarks such as logos or repetitive patterns can be overlaid on video frames. We synthesize watermarked videos using repetitive patterns and overlay the patterns at the same spatial positions across video frames. Such repetitive watermark patterns can disturb feature matching process from the original video content, resulting in false feature matchings from existing stabilization methods.\n\n3) Blurry Videos: Motion blur can appear in shaky frames and cause uncomfortable viewing experience. Such motion blur makes it difficult to extract and match features using existing stabilization methods. 4) Noisy Videos: Videos could be noisy if capturing is effected by poor illumination, high temperature, etc. Gaussian noise could be introduced from multiple noise sources. In our experiments, we synthesize noisy videos by adding Gaussian noise to each video frame, and apply stabilization algorithms to the videos. The match of features would fail using existing stabilization methods.\n\nIn our experimental results, on low quality video cases, StabNet performs robustly, while traditional stabilization methods such as subspace stabilization [4] fail to generate stable results. Please refer to the supplementary video for visual comparison.\n\n\nE. User Study\n\nTo visually compare our method with Adobe Premiere stabilizer, we further conduct a user study with 20 participants aged from 18 to 32. We provide 18 videos from [5], 3 from each aforementioned category; and 12 low-quality videos, 3 from each aforementioned type. In each testing case, we simultaneously show the original input video, our result, and the result from Adobe Premiere stabilizer to the subjects. The two stabilization results are displayed horizontally in random order. Every participant is asked to pick the more stable result from the results of our method and Adobe Premiere stabilizer, or mark them \"indistinguishable\", while disregarding differences in aspect ratio, or sharpness. We show the average percentage of user preference for each category in Figure 10. It can be concluded that for low-quality videos, our method performs much better, and for videos from Quick Zooming, Quick Rotation, Running categories, our results are comparable with those from offline approach. For other categories that were harder to process without future frames, our results are slightly worse, which coincides with our aforementioned discussion.\n\n\nVI. LIMITATION AND CONCLUSION\n\nStabNet has limitations. First, controlling cropping ratio is not supported by our network, which may generate warping borders in the stabilized video. However, like some existing offline video stabilization methods, with an automatic processing of warping border trimming off after holistic path stabilization, the final rendered videos do not contain borders. In our case, the stabilization (with warping borders) and cropping bounding box position are computed and updated progressively in an online stage. The warping borders can be further trimmed off with an automatic post-processing cropping stage, based on the computed bounding box. Nevertheless, one possible way to control cropping ratio is to train a network conditioned with a required specific cropping ratio, which we regard as a future work. Second, in scenes with drastic motion or with extreme near-range foreground objects, our method may fail, this is because our model learns to warp the unstable camera to a virtual stable camera with parallax. We note that these scenarios are also challenging for previous methods [3]- [5], [14]. Third, our solution is purely based on software. Fused video stabilization with additional gyro signals using CNNs [13] is an interesting future research direction.\n\nTo summarize, we have presented StabNet, a convolutional network for digital online video stabilization. Unlike traditional methods which calculate estimated camera paths, StabNet learns warping transformations of multi-grids for each unsteady frame, using only historical stabilized frames as condition. It runs in real time by fast feed-forward operations. We also present the DeepStab dataset-a dataset consisting of pairs of synchronized steady/unsteady videos for training. This dataset was created using a practical method to generate training videos with synchronized steady/unsteady frames, which could benefit future deep stabilization methods. To our knowledge, StabNet is the first CNN model for video stabilization. We have demonstrated the power of StabNet for handling typical types of hand-held videos and its advantage in stabilizing low-quality videos. We believe CNN-based methods are a promising direction for digital video stabilization. Guo-Ye Yang is currently pursuing the bachelor's\n\nFig. 2 .\n2Exemplar frames of DeepStab dataset. The dataset includes pairs of synchronously captured videos. Each pair consists of an unsteady video and a stabilized video, with the same content. Camera motions include forward movement, pan movement, spin movement and complex movements including combinations of the above, at various speed.\n\nFig. 3 .\n3Hardware and training data capturing process.\n\nFig. 5 .\n5Shape-preserving loss terms. (a) illustrates the intra-grid distortion term associating three triangular grid vertices. (b) shows the inter-grid consistency term on three consecutive mesh vertices along an edge.\n\n\u23a6\n, where the image axis is normalized to [\u22121, 1]. For testing, we crop and trim the borders in post-processing. We plan to release source code and pre-trained StabNet model.\n\nFig. 7 .\n7Visualization of feature trajectories from an unsteady video and the corresponding stabilized videos. Left: two stabilized frames by StabNet, with feature trajectories (green) and the feature trajectories from the original video (red) highlighted. Right: visualization of the average horizontal feature offsets between neighboring frames along the time-line, from the original unsteady video, Adobe Stabilizer and our StabNet.\n\nFig. 8 .\n8Quantitative comparison with Adobe Premiere CS6 stabilizer on six categories of hand-held videos.\n\nFig. 9 .\n9Representative frames of low-quality videos. Please refer to the supplementary video for stabilization result comparison.\n\nFig. 10 .\n10User study result by comparing our method with Adobe Premiere Stabilizer.\n\nMiao\nWang received the Ph.D. degree from Tsinghua University in 2016. From 2013 to 2014, he visited the Visual Computing Group, Cardiff University, as a Joint Ph.D. Student. From 2016 to 2018, he was a Post-Doctoral Researcher with Tsinghua University. He is currently an Assistant Professor with the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University. His research interests lie in computer graphics with a particular focus on interactive image and video editing and video in virtual reality.\n\nTABLE I\nIABLATION STUDYOF  ARCHITECTURE, LOSS FUNCTION AND INPUT VARIATIONS. EACH ROW (EXCEPT THE FIRST COLUMN) FROM LEFT TO RIGHT SHOWS THE STABILIZATION STATISTICS IN SIX SUB-SET OF VIDEOS FROM [5]: Regular, Quick Rotation, Quick Zooming, Parallax, Running AND Crowd, IN THREE METRICS: Cropping Ratio (C), Distortion Value (D) AND Stability Score (S). THE FIRST TWO ROWS OF STATISTICS SHOW PERFORMANCES OF ALTERNATIVE ARCHITECTURES, THE NEXT FIVE ROWS COMPARE THE RESULTS WITHOUT Feature, Pixel, Temporal, Distortion AND Consistency LOSS FUNCTIONS, THEN THE RESULTS OF THREE INPUT VARIATIONS ARE SHOWN. THE LAST ROW SHOWS THE PERFORMANCE OF THE PROPOSED NETWORK. SYMBOL \"-\" MEANS THE CORRESPONDING NETWORK DOES NOT CONVERGE randomly sampled between H min =\n\nTABLE II RUNNING\nIITIME COMPARISON. FPS STATISTICS ARE GIVEN IN THE SECOND COLUMN. THIRD COLUMN SHOWS WHETHER FUTURE FRAMES ARE REQUIRED FOR STABILIZATION\nACKNOWLEDGMENTThe authors would like to thank all the reviewers.\nFull-frame video stabilization with motion inpainting. Y Matsushita, E Ofek, W Ge, X Tang, H.-Y Shum, IEEE Trans. Pattern Anal. Mach. Intell. 287Y. Matsushita, E. Ofek, W. Ge, X. Tang, and H.-Y. Shum, \"Full-frame video stabilization with motion inpainting,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 7, pp. 1150-1163, Jul. 2006.\n\nContent-preserving warps for 3D video stabilization. F Liu, M Gleicher, H Jin, A Agarwala, ACM Trans. Graph. 283F. Liu, M. Gleicher, H. Jin, and A. Agarwala, \"Content-preserving warps for 3D video stabilization,\" ACM Trans. Graph., vol. 28, no. 3, pp. 44:1-44:9, Aug. 2009.\n\nAuto-directed video stabilization with robust L1 optimal camera paths. M Grundmann, V Kwatra, I Essa, Proc. IEEE CVPR. IEEE CVPRM. Grundmann, V. Kwatra, and I. Essa, \"Auto-directed video stabi- lization with robust L1 optimal camera paths,\" in Proc. IEEE CVPR, Jun. 2011, pp. 225-232.\n\nSubspace video stabilization. F Liu, M Gleicher, J Wang, H Jin, A Agarwala, ACM Trans. Graph. 301F. Liu, M. Gleicher, J. Wang, H. Jin, and A. Agarwala, \"Subspace video stabilization,\" ACM Trans. Graph., vol. 30, no. 1, pp. 4:1-4:10, Feb. 2011.\n\nBundled camera paths for video stabilization. S Liu, L Yuan, P Tan, J Sun, ACM Trans. Graph. 324S. Liu, L. Yuan, P. Tan, and J. Sun, \"Bundled camera paths for video stabilization,\" ACM Trans. Graph., vol. 32, no. 4, pp. 78:1-78:10, Jul. 2013.\n\nReal-time neural style transfer for videos. H Huang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)H. Huang et al., \"Real-time neural style transfer for videos,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 7044-7052.\n\nDeep video deblurring. S Su, M Delbracio, J Wang, G Sapiro, W Heidrich, O Wang, S. Su, M. Delbracio, J. Wang, G. Sapiro, W. Heidrich, and O. Wang. (2016). \"Deep video deblurring.\" [Online]. Available: https://arxiv.org/abs/1611.08387\n\nA robust real-time video stabilization algorithm. H.-C Chang, S.-H Lai, K.-R Lu, J. Vis. Commun. Image Represent. 173H.-C. Chang, S.-H. Lai, and K.-R. Lu, \"A robust real-time video stabilization algorithm,\" J. Vis. Commun. Image Represent., vol. 17, no. 3, pp. 659-673, 2006.\n\nGeodesic video stabilization in transformation space. L Zhang, X.-Q Chen, X.-Y Kong, H Huang, IEEE Trans. Image Process. 265L. Zhang, X.-Q. Chen, X.-Y. Kong, and H. Huang, \"Geodesic video stabilization in transformation space,\" IEEE Trans. Image Process., vol. 26, no. 5, pp. 2219-2229, May 2017.\n\nAnalysis and compensation of rolling shutter effect. C.-K Liang, L.-W Chang, H H Chen, IEEE Trans. Image Process. 178C.-K. Liang, L.-W. Chang, and H. H. Chen, \"Analysis and compensation of rolling shutter effect,\" IEEE Trans. Image Process., vol. 17, no. 8, pp. 1323-1330, Aug. 2008.\n\nPhoto tourism: Exploring photo collections in 3D. N Snavely, S M Seitz, R Szeliski, ACM Trans. Graph. 253N. Snavely, S. M. Seitz, and R. Szeliski, \"Photo tourism: Explor- ing photo collections in 3D,\" ACM Trans. Graph., vol. 25, no. 3, pp. 835-846, 2006.\n\nVideo stabilization using epipolar geometry. A Goldstein, R , ACM Trans. Graph. 315A. Goldstein and R. Fattal, \"Video stabilization using epipolar geome- try,\" ACM Trans. Graph., vol. 31, no. 5, pp. 126:1-126:10, Sep. 2012.\n\nFused video stabilization on the pixel 2 and pixel 2 xl. C.-K Liang, Google Res. Blog. Rep. 11C.-K. Liang, \"Fused video stabilization on the pixel 2 and pixel 2 xl,\" Google Res. Blog, Mountain View, CA, USA, Tech. Rep. 11, 2017.\n\nMeshFlow: Minimum latency online video stabilization. S Liu, P Tan, L Yuan, J Sun, B Zeng, Proc. IEEE Eur. Conf. Comput. Vis. (ECCV). IEEE Eur. Conf. Comput. Vis. (ECCV)S. Liu, P. Tan, L. Yuan, J. Sun, and B. Zeng, \"MeshFlow: Minimum latency online video stabilization,\" in Proc. IEEE Eur. Conf. Comput. Vis. (ECCV), 2016, pp. 800-815.\n\nImageNet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"ImageNet: A large-scale hierarchical image database,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2009, pp. 248-255.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recog- nit. (CVPR), Jun. 2016, pp. 770-778.\n\nImage-to-image translation with conditional adversarial networks. P Isola, J.-Y Zhu, T Zhou, A A Efros, P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. (2016). \"Image-to-image translation with conditional adversarial networks.\" [Online]. Available: https://arxiv.org/abs/1611.07004\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \"Unpaired image-to-image translation using cycle-consistent adversarial networks,\" in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 2242-2251.\n\nFlowNet: Learning optical flow with convolutional networks. A Dosovitskiy, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)A. Dosovitskiy et al., \"FlowNet: Learning optical flow with convo- lutional networks,\" in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 2758-2766.\n\nRobust camera pose estimation by viewpoint classification using deep learning. Y Nakajima, H Saito, Comput. Vis. Media. 32Y. Nakajima and H. Saito, \"Robust camera pose estimation by viewpoint classification using deep learning,\" Comput. Vis. Media, vol. 3, no. 2, pp. 189-198, 2017.\n\nTwo-stream convolutional networks for action recognition in videos. K Simonyan, A Zisserman, Proc. 27th Int. Conf. Neural Inf. Process. Syst. (NIPS). 27th Int. Conf. Neural Inf. ess. Syst. (NIPS)1K. Simonyan and A. Zisserman, \"Two-stream convolutional networks for action recognition in videos,\" in Proc. 27th Int. Conf. Neural Inf. Process. Syst. (NIPS), vol. 1, 2014, pp. 568-576.\n\nVisual dynamics: Probabilistic future frame synthesis via cross convolutional networks. T Xue, J Wu, K L Bouman, W T Freeman, Proc. NIPS. NIPST. Xue, J. Wu, K. L. Bouman, and W. T. Freeman, \"Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks,\" in Proc. NIPS, 2016, pp. 91-99.\n\nVideo frame synthesis using deep voxel flow. Z Liu, R A Yeh, X Tang, Y Liu, A Agarwala, Z. Liu, R. A. Yeh, X. Tang, Y. Liu, and A. Agarwala. (2017). \"Video frame synthesis using deep voxel flow.\" [Online]. Available: https://arxiv.org/abs/1702.02463\n\nOnline video deblurring via dynamic temporal blending network. T H Kim, K M Lee, B Sch\u00f6lkopf, M Hirsch, T. H. Kim, K. M. Lee, and B. Sch\u00f6lkopf, and M. Hirsch. (2017). \"Online video deblurring via dynamic temporal blending network.\" [Online].\n\nLearning to compare image patches via convolutional neural networks. S Zagoruyko, N Komodakis, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)S. Zagoruyko and N. Komodakis, \"Learning to compare image patches via convolutional neural networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 4353-4361.\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, K Kavukcuoglu, Proc. NIPS. NIPSM. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu, \"Spa- tial transformer networks,\" in Proc. NIPS, 2015, pp. 2017-2025.\n\nSURF: Speeded up robust features. H Bay, T Tuytelaars, L Van Gool, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisH. Bay, T. Tuytelaars, and L. Van Gool, \"SURF: Speeded up robust features,\" in Proc. Eur. Conf. Comput. Vis., 2006, pp. 404-417.\n\nRandom sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. M A Fischler, R C Bolles, Commun. ACM. 246M. A. Fischler and R. C. Bolles, \"Random sample consensus: A para- digm for model fitting with applications to image analysis and automated cartography,\" Commun. ACM, vol. 24, no. 6, pp. 381-395, 1981.\n\nTVL1-optical flow estimation. J S P\u00e9rez, E Meinhardt-Llopis, G Facciolo, Image Process. On Line. 3J. S. P\u00e9rez, E. Meinhardt-Llopis, and G. Facciolo, \"TVL1-optical flow estimation,\" Image Process. On Line, vol. 3, pp. 137-150, 2013.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, D. P. Kingma and J. Ba. (2014). \"Adam: A method for stochastic optimization.\" [Online]. Available: https://arxiv.org/abs/1412.6980\n", "annotations": {"author": "[{\"end\":118,\"start\":96},{\"end\":143,\"start\":119},{\"end\":168,\"start\":144},{\"end\":196,\"start\":169},{\"end\":222,\"start\":197},{\"end\":248,\"start\":223},{\"end\":279,\"start\":249},{\"end\":295,\"start\":280},{\"end\":303,\"start\":296},{\"end\":314,\"start\":304},{\"end\":324,\"start\":315},{\"end\":336,\"start\":325},{\"end\":345,\"start\":337},{\"end\":393,\"start\":346},{\"end\":434,\"start\":394},{\"end\":479,\"start\":435},{\"end\":531,\"start\":480},{\"end\":571,\"start\":532},{\"end\":118,\"start\":96},{\"end\":143,\"start\":119},{\"end\":168,\"start\":144},{\"end\":196,\"start\":169},{\"end\":222,\"start\":197},{\"end\":248,\"start\":223},{\"end\":279,\"start\":249},{\"end\":295,\"start\":280},{\"end\":303,\"start\":296},{\"end\":314,\"start\":304},{\"end\":324,\"start\":315},{\"end\":336,\"start\":325},{\"end\":345,\"start\":337},{\"end\":393,\"start\":346},{\"end\":434,\"start\":394},{\"end\":479,\"start\":435},{\"end\":531,\"start\":480},{\"end\":571,\"start\":532}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":113},{\"end\":142,\"start\":138},{\"end\":167,\"start\":164},{\"end\":195,\"start\":190},{\"end\":221,\"start\":215},{\"end\":247,\"start\":245},{\"end\":278,\"start\":276},{\"end\":294,\"start\":289},{\"end\":302,\"start\":298},{\"end\":313,\"start\":309},{\"end\":323,\"start\":320},{\"end\":335,\"start\":330},{\"end\":344,\"start\":342},{\"end\":117,\"start\":113},{\"end\":142,\"start\":138},{\"end\":167,\"start\":164},{\"end\":195,\"start\":190},{\"end\":221,\"start\":215},{\"end\":247,\"start\":245},{\"end\":278,\"start\":276},{\"end\":294,\"start\":289},{\"end\":302,\"start\":298},{\"end\":313,\"start\":309},{\"end\":323,\"start\":320},{\"end\":335,\"start\":330},{\"end\":344,\"start\":342}]", "author_first_name": "[{\"end\":112,\"start\":108},{\"end\":137,\"start\":131},{\"end\":163,\"start\":156},{\"end\":189,\"start\":181},{\"end\":214,\"start\":209},{\"end\":244,\"start\":235},{\"end\":275,\"start\":268},{\"end\":288,\"start\":280},{\"end\":297,\"start\":296},{\"end\":308,\"start\":304},{\"end\":319,\"start\":315},{\"end\":329,\"start\":325},{\"end\":341,\"start\":337},{\"end\":112,\"start\":108},{\"end\":137,\"start\":131},{\"end\":163,\"start\":156},{\"end\":189,\"start\":181},{\"end\":214,\"start\":209},{\"end\":244,\"start\":235},{\"end\":275,\"start\":268},{\"end\":288,\"start\":280},{\"end\":297,\"start\":296},{\"end\":308,\"start\":304},{\"end\":319,\"start\":315},{\"end\":329,\"start\":325},{\"end\":341,\"start\":337}]", "author_affiliation": "[{\"end\":392,\"start\":347},{\"end\":433,\"start\":395},{\"end\":478,\"start\":436},{\"end\":530,\"start\":481},{\"end\":570,\"start\":533},{\"end\":392,\"start\":347},{\"end\":433,\"start\":395},{\"end\":478,\"start\":436},{\"end\":530,\"start\":481},{\"end\":570,\"start\":533}]", "title": "[{\"end\":80,\"start\":1},{\"end\":651,\"start\":572},{\"end\":80,\"start\":1},{\"end\":651,\"start\":572}]", "venue": "[{\"end\":690,\"start\":653},{\"end\":690,\"start\":653}]", "abstract": "[{\"end\":2792,\"start\":1195},{\"end\":2792,\"start\":1195}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3479,\"start\":3476},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3484,\"start\":3481},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4535,\"start\":4532},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4560,\"start\":4557},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8034,\"start\":8031},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8039,\"start\":8036},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8114,\"start\":8111},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8253,\"start\":8250},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8325,\"start\":8322},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8444,\"start\":8440},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8687,\"start\":8683},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8779,\"start\":8776},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8870,\"start\":8867},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8973,\"start\":8969},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9375,\"start\":9371},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9392,\"start\":9388},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9964,\"start\":9960},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9970,\"start\":9966},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9990,\"start\":9986},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9996,\"start\":9992},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10085,\"start\":10081},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10105,\"start\":10101},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10124,\"start\":10120},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10238,\"start\":10234},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10264,\"start\":10260},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10283,\"start\":10280},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10289,\"start\":10285},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14082,\"start\":14079},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16063,\"start\":16059},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16803,\"start\":16799},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17958,\"start\":17955},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18574,\"start\":18570},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19610,\"start\":19606},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19755,\"start\":19751},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19873,\"start\":19870},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20513,\"start\":20510},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21742,\"start\":21739},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22695,\"start\":22691},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22970,\"start\":22966},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23094,\"start\":23090},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24789,\"start\":24786},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25021,\"start\":25017},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25922,\"start\":25918},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26350,\"start\":26346},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26664,\"start\":26661},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28313,\"start\":28310},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29241,\"start\":29238},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29246,\"start\":29243},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29252,\"start\":29248},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29262,\"start\":29258},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30220,\"start\":30217},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30315,\"start\":30312},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33049,\"start\":33046},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33328,\"start\":33325},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35440,\"start\":35437},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35445,\"start\":35442},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35451,\"start\":35447},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35572,\"start\":35568},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3479,\"start\":3476},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3484,\"start\":3481},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4535,\"start\":4532},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4560,\"start\":4557},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8034,\"start\":8031},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8039,\"start\":8036},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8114,\"start\":8111},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8253,\"start\":8250},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8325,\"start\":8322},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8444,\"start\":8440},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8687,\"start\":8683},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8779,\"start\":8776},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8870,\"start\":8867},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8973,\"start\":8969},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9375,\"start\":9371},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9392,\"start\":9388},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9964,\"start\":9960},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9970,\"start\":9966},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9990,\"start\":9986},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9996,\"start\":9992},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10085,\"start\":10081},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10105,\"start\":10101},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10124,\"start\":10120},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10238,\"start\":10234},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10264,\"start\":10260},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10283,\"start\":10280},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10289,\"start\":10285},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14082,\"start\":14079},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16063,\"start\":16059},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16803,\"start\":16799},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17958,\"start\":17955},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18574,\"start\":18570},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19610,\"start\":19606},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19755,\"start\":19751},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19873,\"start\":19870},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20513,\"start\":20510},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21742,\"start\":21739},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22695,\"start\":22691},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22970,\"start\":22966},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23094,\"start\":23090},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24789,\"start\":24786},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25021,\"start\":25017},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25922,\"start\":25918},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26350,\"start\":26346},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26664,\"start\":26661},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28313,\"start\":28310},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29241,\"start\":29238},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29246,\"start\":29243},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29252,\"start\":29248},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29262,\"start\":29258},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30220,\"start\":30217},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30315,\"start\":30312},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33049,\"start\":33046},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33328,\"start\":33325},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35440,\"start\":35437},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35445,\"start\":35442},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35451,\"start\":35447},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35572,\"start\":35568}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36967,\"start\":36626},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37024,\"start\":36968},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37247,\"start\":37025},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37423,\"start\":37248},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37861,\"start\":37424},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37970,\"start\":37862},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38103,\"start\":37971},{\"attributes\":{\"id\":\"fig_7\"},\"end\":38190,\"start\":38104},{\"attributes\":{\"id\":\"fig_8\"},\"end\":38717,\"start\":38191},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39477,\"start\":38718},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39633,\"start\":39478},{\"attributes\":{\"id\":\"fig_0\"},\"end\":36967,\"start\":36626},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37024,\"start\":36968},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37247,\"start\":37025},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37423,\"start\":37248},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37861,\"start\":37424},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37970,\"start\":37862},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38103,\"start\":37971},{\"attributes\":{\"id\":\"fig_7\"},\"end\":38190,\"start\":38104},{\"attributes\":{\"id\":\"fig_8\"},\"end\":38717,\"start\":38191},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39477,\"start\":38718},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39633,\"start\":39478}]", "paragraph": "[{\"end\":3189,\"start\":2801},{\"end\":4240,\"start\":3209},{\"end\":5328,\"start\":4242},{\"end\":6210,\"start\":5330},{\"end\":7189,\"start\":6212},{\"end\":7425,\"start\":7191},{\"end\":7553,\"start\":7446},{\"end\":9192,\"start\":7588},{\"end\":9824,\"start\":9194},{\"end\":10780,\"start\":9859},{\"end\":11953,\"start\":10806},{\"end\":12770,\"start\":11955},{\"end\":13202,\"start\":12772},{\"end\":13741,\"start\":13222},{\"end\":13882,\"start\":13743},{\"end\":14176,\"start\":13957},{\"end\":15291,\"start\":14450},{\"end\":15826,\"start\":15524},{\"end\":17285,\"start\":16026},{\"end\":17562,\"start\":17321},{\"end\":17770,\"start\":17665},{\"end\":18090,\"start\":17772},{\"end\":18331,\"start\":18168},{\"end\":18935,\"start\":18389},{\"end\":19125,\"start\":18937},{\"end\":19456,\"start\":19231},{\"end\":20129,\"start\":19458},{\"end\":20496,\"start\":20131},{\"end\":20685,\"start\":20498},{\"end\":20780,\"start\":20770},{\"end\":21001,\"start\":20912},{\"end\":21332,\"start\":21003},{\"end\":21652,\"start\":21418},{\"end\":21789,\"start\":21654},{\"end\":22372,\"start\":21866},{\"end\":22795,\"start\":22464},{\"end\":23506,\"start\":22825},{\"end\":24195,\"start\":23508},{\"end\":24601,\"start\":24197},{\"end\":24956,\"start\":24629},{\"end\":25120,\"start\":24958},{\"end\":25507,\"start\":25122},{\"end\":25841,\"start\":25509},{\"end\":26402,\"start\":25843},{\"end\":27986,\"start\":26424},{\"end\":28340,\"start\":27988},{\"end\":28657,\"start\":28365},{\"end\":28797,\"start\":28723},{\"end\":29172,\"start\":28893},{\"end\":30057,\"start\":29222},{\"end\":31072,\"start\":30110},{\"end\":31643,\"start\":31110},{\"end\":31877,\"start\":31645},{\"end\":32296,\"start\":31879},{\"end\":32889,\"start\":32298},{\"end\":33145,\"start\":32891},{\"end\":34314,\"start\":33163},{\"end\":35617,\"start\":34348},{\"end\":36625,\"start\":35619},{\"end\":3189,\"start\":2801},{\"end\":4240,\"start\":3209},{\"end\":5328,\"start\":4242},{\"end\":6210,\"start\":5330},{\"end\":7189,\"start\":6212},{\"end\":7425,\"start\":7191},{\"end\":7553,\"start\":7446},{\"end\":9192,\"start\":7588},{\"end\":9824,\"start\":9194},{\"end\":10780,\"start\":9859},{\"end\":11953,\"start\":10806},{\"end\":12770,\"start\":11955},{\"end\":13202,\"start\":12772},{\"end\":13741,\"start\":13222},{\"end\":13882,\"start\":13743},{\"end\":14176,\"start\":13957},{\"end\":15291,\"start\":14450},{\"end\":15826,\"start\":15524},{\"end\":17285,\"start\":16026},{\"end\":17562,\"start\":17321},{\"end\":17770,\"start\":17665},{\"end\":18090,\"start\":17772},{\"end\":18331,\"start\":18168},{\"end\":18935,\"start\":18389},{\"end\":19125,\"start\":18937},{\"end\":19456,\"start\":19231},{\"end\":20129,\"start\":19458},{\"end\":20496,\"start\":20131},{\"end\":20685,\"start\":20498},{\"end\":20780,\"start\":20770},{\"end\":21001,\"start\":20912},{\"end\":21332,\"start\":21003},{\"end\":21652,\"start\":21418},{\"end\":21789,\"start\":21654},{\"end\":22372,\"start\":21866},{\"end\":22795,\"start\":22464},{\"end\":23506,\"start\":22825},{\"end\":24195,\"start\":23508},{\"end\":24601,\"start\":24197},{\"end\":24956,\"start\":24629},{\"end\":25120,\"start\":24958},{\"end\":25507,\"start\":25122},{\"end\":25841,\"start\":25509},{\"end\":26402,\"start\":25843},{\"end\":27986,\"start\":26424},{\"end\":28340,\"start\":27988},{\"end\":28657,\"start\":28365},{\"end\":28797,\"start\":28723},{\"end\":29172,\"start\":28893},{\"end\":30057,\"start\":29222},{\"end\":31072,\"start\":30110},{\"end\":31643,\"start\":31110},{\"end\":31877,\"start\":31645},{\"end\":32296,\"start\":31879},{\"end\":32889,\"start\":32298},{\"end\":33145,\"start\":32891},{\"end\":34314,\"start\":33163},{\"end\":35617,\"start\":34348},{\"end\":36625,\"start\":35619}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13956,\"start\":13883},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14449,\"start\":14177},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15523,\"start\":15292},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15999,\"start\":15827},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17664,\"start\":17563},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18167,\"start\":18091},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18388,\"start\":18332},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19195,\"start\":19126},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19230,\"start\":19195},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20769,\"start\":20686},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20911,\"start\":20781},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21417,\"start\":21333},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21865,\"start\":21790},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22463,\"start\":22373},{\"attributes\":{\"id\":\"formula_14\"},\"end\":28722,\"start\":28658},{\"attributes\":{\"id\":\"formula_15\"},\"end\":28892,\"start\":28798},{\"attributes\":{\"id\":\"formula_0\"},\"end\":13956,\"start\":13883},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14449,\"start\":14177},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15523,\"start\":15292},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15999,\"start\":15827},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17664,\"start\":17563},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18167,\"start\":18091},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18388,\"start\":18332},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19195,\"start\":19126},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19230,\"start\":19195},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20769,\"start\":20686},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20911,\"start\":20781},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21417,\"start\":21333},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21865,\"start\":21790},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22463,\"start\":22373},{\"attributes\":{\"id\":\"formula_14\"},\"end\":28722,\"start\":28658},{\"attributes\":{\"id\":\"formula_15\"},\"end\":28892,\"start\":28798}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28943,\"start\":28936},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31071,\"start\":31063},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28943,\"start\":28936},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31071,\"start\":31063}]", "section_header": "[{\"end\":3207,\"start\":3192},{\"end\":7444,\"start\":7428},{\"end\":7586,\"start\":7556},{\"end\":9857,\"start\":9827},{\"end\":10804,\"start\":10783},{\"end\":13220,\"start\":13205},{\"end\":16024,\"start\":16001},{\"end\":17319,\"start\":17288},{\"end\":22823,\"start\":22798},{\"end\":24627,\"start\":24604},{\"end\":26422,\"start\":26405},{\"end\":28363,\"start\":28343},{\"end\":29220,\"start\":29175},{\"end\":30108,\"start\":30060},{\"end\":31108,\"start\":31075},{\"end\":33161,\"start\":33148},{\"end\":34346,\"start\":34317},{\"end\":36635,\"start\":36627},{\"end\":36977,\"start\":36969},{\"end\":37034,\"start\":37026},{\"end\":37250,\"start\":37249},{\"end\":37433,\"start\":37425},{\"end\":37871,\"start\":37863},{\"end\":37980,\"start\":37972},{\"end\":38114,\"start\":38105},{\"end\":38196,\"start\":38192},{\"end\":38726,\"start\":38719},{\"end\":39495,\"start\":39479},{\"end\":3207,\"start\":3192},{\"end\":7444,\"start\":7428},{\"end\":7586,\"start\":7556},{\"end\":9857,\"start\":9827},{\"end\":10804,\"start\":10783},{\"end\":13220,\"start\":13205},{\"end\":16024,\"start\":16001},{\"end\":17319,\"start\":17288},{\"end\":22823,\"start\":22798},{\"end\":24627,\"start\":24604},{\"end\":26422,\"start\":26405},{\"end\":28363,\"start\":28343},{\"end\":29220,\"start\":29175},{\"end\":30108,\"start\":30060},{\"end\":31108,\"start\":31075},{\"end\":33161,\"start\":33148},{\"end\":34346,\"start\":34317},{\"end\":36635,\"start\":36627},{\"end\":36977,\"start\":36969},{\"end\":37034,\"start\":37026},{\"end\":37250,\"start\":37249},{\"end\":37433,\"start\":37425},{\"end\":37871,\"start\":37863},{\"end\":37980,\"start\":37972},{\"end\":38114,\"start\":38105},{\"end\":38196,\"start\":38192},{\"end\":38726,\"start\":38719},{\"end\":39495,\"start\":39479}]", "table": null, "figure_caption": "[{\"end\":36967,\"start\":36637},{\"end\":37024,\"start\":36979},{\"end\":37247,\"start\":37036},{\"end\":37423,\"start\":37251},{\"end\":37861,\"start\":37435},{\"end\":37970,\"start\":37873},{\"end\":38103,\"start\":37982},{\"end\":38190,\"start\":38117},{\"end\":38717,\"start\":38197},{\"end\":39477,\"start\":38728},{\"end\":39633,\"start\":39498},{\"end\":36967,\"start\":36637},{\"end\":37024,\"start\":36979},{\"end\":37247,\"start\":37036},{\"end\":37423,\"start\":37251},{\"end\":37861,\"start\":37435},{\"end\":37970,\"start\":37873},{\"end\":38103,\"start\":37982},{\"end\":38190,\"start\":38117},{\"end\":38717,\"start\":38197},{\"end\":39477,\"start\":38728},{\"end\":39633,\"start\":39498}]", "figure_ref": "[{\"end\":2800,\"start\":2794},{\"end\":4238,\"start\":4230},{\"end\":6468,\"start\":6460},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11649,\"start\":11640},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12966,\"start\":12958},{\"end\":14501,\"start\":14495},{\"end\":16522,\"start\":16514},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21611,\"start\":21603},{\"end\":28264,\"start\":28258},{\"end\":29984,\"start\":29976},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30452,\"start\":30444},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30595,\"start\":30587},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31642,\"start\":31634},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33943,\"start\":33934},{\"end\":2800,\"start\":2794},{\"end\":4238,\"start\":4230},{\"end\":6468,\"start\":6460},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11649,\"start\":11640},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12966,\"start\":12958},{\"end\":14501,\"start\":14495},{\"end\":16522,\"start\":16514},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21611,\"start\":21603},{\"end\":28264,\"start\":28258},{\"end\":29984,\"start\":29976},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30452,\"start\":30444},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30595,\"start\":30587},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31642,\"start\":31634},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33943,\"start\":33934}]", "bib_author_first_name": "[{\"end\":39755,\"start\":39754},{\"end\":39769,\"start\":39768},{\"end\":39777,\"start\":39776},{\"end\":39783,\"start\":39782},{\"end\":39794,\"start\":39790},{\"end\":40095,\"start\":40094},{\"end\":40102,\"start\":40101},{\"end\":40114,\"start\":40113},{\"end\":40121,\"start\":40120},{\"end\":40388,\"start\":40387},{\"end\":40401,\"start\":40400},{\"end\":40411,\"start\":40410},{\"end\":40633,\"start\":40632},{\"end\":40640,\"start\":40639},{\"end\":40652,\"start\":40651},{\"end\":40660,\"start\":40659},{\"end\":40667,\"start\":40666},{\"end\":40894,\"start\":40893},{\"end\":40901,\"start\":40900},{\"end\":40909,\"start\":40908},{\"end\":40916,\"start\":40915},{\"end\":41136,\"start\":41135},{\"end\":41421,\"start\":41420},{\"end\":41427,\"start\":41426},{\"end\":41440,\"start\":41439},{\"end\":41448,\"start\":41447},{\"end\":41458,\"start\":41457},{\"end\":41470,\"start\":41469},{\"end\":41686,\"start\":41682},{\"end\":41698,\"start\":41694},{\"end\":41708,\"start\":41704},{\"end\":41964,\"start\":41963},{\"end\":41976,\"start\":41972},{\"end\":41987,\"start\":41983},{\"end\":41995,\"start\":41994},{\"end\":42264,\"start\":42260},{\"end\":42276,\"start\":42272},{\"end\":42285,\"start\":42284},{\"end\":42287,\"start\":42286},{\"end\":42543,\"start\":42542},{\"end\":42554,\"start\":42553},{\"end\":42556,\"start\":42555},{\"end\":42565,\"start\":42564},{\"end\":42794,\"start\":42793},{\"end\":42807,\"start\":42806},{\"end\":43034,\"start\":43030},{\"end\":43258,\"start\":43257},{\"end\":43265,\"start\":43264},{\"end\":43272,\"start\":43271},{\"end\":43280,\"start\":43279},{\"end\":43287,\"start\":43286},{\"end\":43594,\"start\":43593},{\"end\":43602,\"start\":43601},{\"end\":43610,\"start\":43609},{\"end\":43623,\"start\":43619},{\"end\":43629,\"start\":43628},{\"end\":43635,\"start\":43634},{\"end\":43997,\"start\":43996},{\"end\":44003,\"start\":44002},{\"end\":44012,\"start\":44011},{\"end\":44019,\"start\":44018},{\"end\":44367,\"start\":44366},{\"end\":44379,\"start\":44375},{\"end\":44386,\"start\":44385},{\"end\":44394,\"start\":44393},{\"end\":44396,\"start\":44395},{\"end\":44667,\"start\":44663},{\"end\":44674,\"start\":44673},{\"end\":44682,\"start\":44681},{\"end\":44691,\"start\":44690},{\"end\":44693,\"start\":44692},{\"end\":45043,\"start\":45042},{\"end\":45375,\"start\":45374},{\"end\":45387,\"start\":45386},{\"end\":45648,\"start\":45647},{\"end\":45660,\"start\":45659},{\"end\":46052,\"start\":46051},{\"end\":46059,\"start\":46058},{\"end\":46065,\"start\":46064},{\"end\":46067,\"start\":46066},{\"end\":46077,\"start\":46076},{\"end\":46079,\"start\":46078},{\"end\":46322,\"start\":46321},{\"end\":46329,\"start\":46328},{\"end\":46331,\"start\":46330},{\"end\":46338,\"start\":46337},{\"end\":46346,\"start\":46345},{\"end\":46353,\"start\":46352},{\"end\":46591,\"start\":46590},{\"end\":46593,\"start\":46592},{\"end\":46600,\"start\":46599},{\"end\":46602,\"start\":46601},{\"end\":46609,\"start\":46608},{\"end\":46622,\"start\":46621},{\"end\":46840,\"start\":46839},{\"end\":46853,\"start\":46852},{\"end\":47188,\"start\":47187},{\"end\":47201,\"start\":47200},{\"end\":47213,\"start\":47212},{\"end\":47226,\"start\":47225},{\"end\":47423,\"start\":47422},{\"end\":47430,\"start\":47429},{\"end\":47444,\"start\":47443},{\"end\":47755,\"start\":47754},{\"end\":47757,\"start\":47756},{\"end\":47769,\"start\":47768},{\"end\":47771,\"start\":47770},{\"end\":48030,\"start\":48029},{\"end\":48032,\"start\":48031},{\"end\":48041,\"start\":48040},{\"end\":48061,\"start\":48060},{\"end\":48277,\"start\":48276},{\"end\":48279,\"start\":48278},{\"end\":48289,\"start\":48288},{\"end\":39755,\"start\":39754},{\"end\":39769,\"start\":39768},{\"end\":39777,\"start\":39776},{\"end\":39783,\"start\":39782},{\"end\":39794,\"start\":39790},{\"end\":40095,\"start\":40094},{\"end\":40102,\"start\":40101},{\"end\":40114,\"start\":40113},{\"end\":40121,\"start\":40120},{\"end\":40388,\"start\":40387},{\"end\":40401,\"start\":40400},{\"end\":40411,\"start\":40410},{\"end\":40633,\"start\":40632},{\"end\":40640,\"start\":40639},{\"end\":40652,\"start\":40651},{\"end\":40660,\"start\":40659},{\"end\":40667,\"start\":40666},{\"end\":40894,\"start\":40893},{\"end\":40901,\"start\":40900},{\"end\":40909,\"start\":40908},{\"end\":40916,\"start\":40915},{\"end\":41136,\"start\":41135},{\"end\":41421,\"start\":41420},{\"end\":41427,\"start\":41426},{\"end\":41440,\"start\":41439},{\"end\":41448,\"start\":41447},{\"end\":41458,\"start\":41457},{\"end\":41470,\"start\":41469},{\"end\":41686,\"start\":41682},{\"end\":41698,\"start\":41694},{\"end\":41708,\"start\":41704},{\"end\":41964,\"start\":41963},{\"end\":41976,\"start\":41972},{\"end\":41987,\"start\":41983},{\"end\":41995,\"start\":41994},{\"end\":42264,\"start\":42260},{\"end\":42276,\"start\":42272},{\"end\":42285,\"start\":42284},{\"end\":42287,\"start\":42286},{\"end\":42543,\"start\":42542},{\"end\":42554,\"start\":42553},{\"end\":42556,\"start\":42555},{\"end\":42565,\"start\":42564},{\"end\":42794,\"start\":42793},{\"end\":42807,\"start\":42806},{\"end\":43034,\"start\":43030},{\"end\":43258,\"start\":43257},{\"end\":43265,\"start\":43264},{\"end\":43272,\"start\":43271},{\"end\":43280,\"start\":43279},{\"end\":43287,\"start\":43286},{\"end\":43594,\"start\":43593},{\"end\":43602,\"start\":43601},{\"end\":43610,\"start\":43609},{\"end\":43623,\"start\":43619},{\"end\":43629,\"start\":43628},{\"end\":43635,\"start\":43634},{\"end\":43997,\"start\":43996},{\"end\":44003,\"start\":44002},{\"end\":44012,\"start\":44011},{\"end\":44019,\"start\":44018},{\"end\":44367,\"start\":44366},{\"end\":44379,\"start\":44375},{\"end\":44386,\"start\":44385},{\"end\":44394,\"start\":44393},{\"end\":44396,\"start\":44395},{\"end\":44667,\"start\":44663},{\"end\":44674,\"start\":44673},{\"end\":44682,\"start\":44681},{\"end\":44691,\"start\":44690},{\"end\":44693,\"start\":44692},{\"end\":45043,\"start\":45042},{\"end\":45375,\"start\":45374},{\"end\":45387,\"start\":45386},{\"end\":45648,\"start\":45647},{\"end\":45660,\"start\":45659},{\"end\":46052,\"start\":46051},{\"end\":46059,\"start\":46058},{\"end\":46065,\"start\":46064},{\"end\":46067,\"start\":46066},{\"end\":46077,\"start\":46076},{\"end\":46079,\"start\":46078},{\"end\":46322,\"start\":46321},{\"end\":46329,\"start\":46328},{\"end\":46331,\"start\":46330},{\"end\":46338,\"start\":46337},{\"end\":46346,\"start\":46345},{\"end\":46353,\"start\":46352},{\"end\":46591,\"start\":46590},{\"end\":46593,\"start\":46592},{\"end\":46600,\"start\":46599},{\"end\":46602,\"start\":46601},{\"end\":46609,\"start\":46608},{\"end\":46622,\"start\":46621},{\"end\":46840,\"start\":46839},{\"end\":46853,\"start\":46852},{\"end\":47188,\"start\":47187},{\"end\":47201,\"start\":47200},{\"end\":47213,\"start\":47212},{\"end\":47226,\"start\":47225},{\"end\":47423,\"start\":47422},{\"end\":47430,\"start\":47429},{\"end\":47444,\"start\":47443},{\"end\":47755,\"start\":47754},{\"end\":47757,\"start\":47756},{\"end\":47769,\"start\":47768},{\"end\":47771,\"start\":47770},{\"end\":48030,\"start\":48029},{\"end\":48032,\"start\":48031},{\"end\":48041,\"start\":48040},{\"end\":48061,\"start\":48060},{\"end\":48277,\"start\":48276},{\"end\":48279,\"start\":48278},{\"end\":48289,\"start\":48288}]", "bib_author_last_name": "[{\"end\":39766,\"start\":39756},{\"end\":39774,\"start\":39770},{\"end\":39780,\"start\":39778},{\"end\":39788,\"start\":39784},{\"end\":39799,\"start\":39795},{\"end\":40099,\"start\":40096},{\"end\":40111,\"start\":40103},{\"end\":40118,\"start\":40115},{\"end\":40130,\"start\":40122},{\"end\":40398,\"start\":40389},{\"end\":40408,\"start\":40402},{\"end\":40416,\"start\":40412},{\"end\":40637,\"start\":40634},{\"end\":40649,\"start\":40641},{\"end\":40657,\"start\":40653},{\"end\":40664,\"start\":40661},{\"end\":40676,\"start\":40668},{\"end\":40898,\"start\":40895},{\"end\":40906,\"start\":40902},{\"end\":40913,\"start\":40910},{\"end\":40920,\"start\":40917},{\"end\":41142,\"start\":41137},{\"end\":41424,\"start\":41422},{\"end\":41437,\"start\":41428},{\"end\":41445,\"start\":41441},{\"end\":41455,\"start\":41449},{\"end\":41467,\"start\":41459},{\"end\":41475,\"start\":41471},{\"end\":41692,\"start\":41687},{\"end\":41702,\"start\":41699},{\"end\":41711,\"start\":41709},{\"end\":41970,\"start\":41965},{\"end\":41981,\"start\":41977},{\"end\":41992,\"start\":41988},{\"end\":42001,\"start\":41996},{\"end\":42270,\"start\":42265},{\"end\":42282,\"start\":42277},{\"end\":42292,\"start\":42288},{\"end\":42551,\"start\":42544},{\"end\":42562,\"start\":42557},{\"end\":42574,\"start\":42566},{\"end\":42804,\"start\":42795},{\"end\":43040,\"start\":43035},{\"end\":43262,\"start\":43259},{\"end\":43269,\"start\":43266},{\"end\":43277,\"start\":43273},{\"end\":43284,\"start\":43281},{\"end\":43292,\"start\":43288},{\"end\":43599,\"start\":43595},{\"end\":43607,\"start\":43603},{\"end\":43617,\"start\":43611},{\"end\":43626,\"start\":43624},{\"end\":43632,\"start\":43630},{\"end\":43643,\"start\":43636},{\"end\":44000,\"start\":43998},{\"end\":44009,\"start\":44004},{\"end\":44016,\"start\":44013},{\"end\":44023,\"start\":44020},{\"end\":44373,\"start\":44368},{\"end\":44383,\"start\":44380},{\"end\":44391,\"start\":44387},{\"end\":44402,\"start\":44397},{\"end\":44671,\"start\":44668},{\"end\":44679,\"start\":44675},{\"end\":44688,\"start\":44683},{\"end\":44699,\"start\":44694},{\"end\":45055,\"start\":45044},{\"end\":45384,\"start\":45376},{\"end\":45393,\"start\":45388},{\"end\":45657,\"start\":45649},{\"end\":45670,\"start\":45661},{\"end\":46056,\"start\":46053},{\"end\":46062,\"start\":46060},{\"end\":46074,\"start\":46068},{\"end\":46087,\"start\":46080},{\"end\":46326,\"start\":46323},{\"end\":46335,\"start\":46332},{\"end\":46343,\"start\":46339},{\"end\":46350,\"start\":46347},{\"end\":46362,\"start\":46354},{\"end\":46597,\"start\":46594},{\"end\":46606,\"start\":46603},{\"end\":46619,\"start\":46610},{\"end\":46629,\"start\":46623},{\"end\":46850,\"start\":46841},{\"end\":46863,\"start\":46854},{\"end\":47198,\"start\":47189},{\"end\":47210,\"start\":47202},{\"end\":47223,\"start\":47214},{\"end\":47238,\"start\":47227},{\"end\":47427,\"start\":47424},{\"end\":47441,\"start\":47431},{\"end\":47453,\"start\":47445},{\"end\":47766,\"start\":47758},{\"end\":47778,\"start\":47772},{\"end\":48038,\"start\":48033},{\"end\":48058,\"start\":48042},{\"end\":48070,\"start\":48062},{\"end\":48286,\"start\":48280},{\"end\":48292,\"start\":48290},{\"end\":39766,\"start\":39756},{\"end\":39774,\"start\":39770},{\"end\":39780,\"start\":39778},{\"end\":39788,\"start\":39784},{\"end\":39799,\"start\":39795},{\"end\":40099,\"start\":40096},{\"end\":40111,\"start\":40103},{\"end\":40118,\"start\":40115},{\"end\":40130,\"start\":40122},{\"end\":40398,\"start\":40389},{\"end\":40408,\"start\":40402},{\"end\":40416,\"start\":40412},{\"end\":40637,\"start\":40634},{\"end\":40649,\"start\":40641},{\"end\":40657,\"start\":40653},{\"end\":40664,\"start\":40661},{\"end\":40676,\"start\":40668},{\"end\":40898,\"start\":40895},{\"end\":40906,\"start\":40902},{\"end\":40913,\"start\":40910},{\"end\":40920,\"start\":40917},{\"end\":41142,\"start\":41137},{\"end\":41424,\"start\":41422},{\"end\":41437,\"start\":41428},{\"end\":41445,\"start\":41441},{\"end\":41455,\"start\":41449},{\"end\":41467,\"start\":41459},{\"end\":41475,\"start\":41471},{\"end\":41692,\"start\":41687},{\"end\":41702,\"start\":41699},{\"end\":41711,\"start\":41709},{\"end\":41970,\"start\":41965},{\"end\":41981,\"start\":41977},{\"end\":41992,\"start\":41988},{\"end\":42001,\"start\":41996},{\"end\":42270,\"start\":42265},{\"end\":42282,\"start\":42277},{\"end\":42292,\"start\":42288},{\"end\":42551,\"start\":42544},{\"end\":42562,\"start\":42557},{\"end\":42574,\"start\":42566},{\"end\":42804,\"start\":42795},{\"end\":43040,\"start\":43035},{\"end\":43262,\"start\":43259},{\"end\":43269,\"start\":43266},{\"end\":43277,\"start\":43273},{\"end\":43284,\"start\":43281},{\"end\":43292,\"start\":43288},{\"end\":43599,\"start\":43595},{\"end\":43607,\"start\":43603},{\"end\":43617,\"start\":43611},{\"end\":43626,\"start\":43624},{\"end\":43632,\"start\":43630},{\"end\":43643,\"start\":43636},{\"end\":44000,\"start\":43998},{\"end\":44009,\"start\":44004},{\"end\":44016,\"start\":44013},{\"end\":44023,\"start\":44020},{\"end\":44373,\"start\":44368},{\"end\":44383,\"start\":44380},{\"end\":44391,\"start\":44387},{\"end\":44402,\"start\":44397},{\"end\":44671,\"start\":44668},{\"end\":44679,\"start\":44675},{\"end\":44688,\"start\":44683},{\"end\":44699,\"start\":44694},{\"end\":45055,\"start\":45044},{\"end\":45384,\"start\":45376},{\"end\":45393,\"start\":45388},{\"end\":45657,\"start\":45649},{\"end\":45670,\"start\":45661},{\"end\":46056,\"start\":46053},{\"end\":46062,\"start\":46060},{\"end\":46074,\"start\":46068},{\"end\":46087,\"start\":46080},{\"end\":46326,\"start\":46323},{\"end\":46335,\"start\":46332},{\"end\":46343,\"start\":46339},{\"end\":46350,\"start\":46347},{\"end\":46362,\"start\":46354},{\"end\":46597,\"start\":46594},{\"end\":46606,\"start\":46603},{\"end\":46619,\"start\":46610},{\"end\":46629,\"start\":46623},{\"end\":46850,\"start\":46841},{\"end\":46863,\"start\":46854},{\"end\":47198,\"start\":47189},{\"end\":47210,\"start\":47202},{\"end\":47223,\"start\":47214},{\"end\":47238,\"start\":47227},{\"end\":47427,\"start\":47424},{\"end\":47441,\"start\":47431},{\"end\":47453,\"start\":47445},{\"end\":47766,\"start\":47758},{\"end\":47778,\"start\":47772},{\"end\":48038,\"start\":48033},{\"end\":48058,\"start\":48042},{\"end\":48070,\"start\":48062},{\"end\":48286,\"start\":48280},{\"end\":48292,\"start\":48290}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17293101},\"end\":40039,\"start\":39699},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8264664},\"end\":40314,\"start\":40041},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17707171},\"end\":40600,\"start\":40316},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1305354},\"end\":40845,\"start\":40602},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207202654},\"end\":41089,\"start\":40847},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":28280219},\"end\":41395,\"start\":41091},{\"attributes\":{\"id\":\"b6\"},\"end\":41630,\"start\":41397},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8265897},\"end\":41907,\"start\":41632},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18015285},\"end\":42205,\"start\":41909},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14194210},\"end\":42490,\"start\":42207},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13385757},\"end\":42746,\"start\":42492},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":17754191},\"end\":42971,\"start\":42748},{\"attributes\":{\"id\":\"b12\"},\"end\":43201,\"start\":42973},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":34482582},\"end\":43538,\"start\":43203},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":57246310},\"end\":43948,\"start\":43540},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":44298,\"start\":43950},{\"attributes\":{\"id\":\"b16\"},\"end\":44580,\"start\":44300},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":195944196},\"end\":44980,\"start\":44582},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":12552176},\"end\":45293,\"start\":44982},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1335416},\"end\":45577,\"start\":45295},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11797475},\"end\":45961,\"start\":45579},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12712095},\"end\":46274,\"start\":45963},{\"attributes\":{\"id\":\"b22\"},\"end\":46525,\"start\":46276},{\"attributes\":{\"id\":\"b23\"},\"end\":46768,\"start\":46527},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":215827033},\"end\":47155,\"start\":46770},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6099034},\"end\":47386,\"start\":47157},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":161878},\"end\":47635,\"start\":47388},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":972888},\"end\":47997,\"start\":47637},{\"attributes\":{\"id\":\"b28\"},\"end\":48230,\"start\":47999},{\"attributes\":{\"id\":\"b29\"},\"end\":48424,\"start\":48232},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17293101},\"end\":40039,\"start\":39699},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8264664},\"end\":40314,\"start\":40041},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17707171},\"end\":40600,\"start\":40316},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1305354},\"end\":40845,\"start\":40602},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207202654},\"end\":41089,\"start\":40847},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":28280219},\"end\":41395,\"start\":41091},{\"attributes\":{\"id\":\"b6\"},\"end\":41630,\"start\":41397},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8265897},\"end\":41907,\"start\":41632},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18015285},\"end\":42205,\"start\":41909},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":260429222},\"end\":42490,\"start\":42207},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13385757},\"end\":42746,\"start\":42492},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":17754191},\"end\":42971,\"start\":42748},{\"attributes\":{\"id\":\"b12\"},\"end\":43201,\"start\":42973},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":34482582},\"end\":43538,\"start\":43203},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":57246310},\"end\":43948,\"start\":43540},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":44298,\"start\":43950},{\"attributes\":{\"id\":\"b16\"},\"end\":44580,\"start\":44300},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":195944196},\"end\":44980,\"start\":44582},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":12552176},\"end\":45293,\"start\":44982},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1335416},\"end\":45577,\"start\":45295},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11797475},\"end\":45961,\"start\":45579},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12712095},\"end\":46274,\"start\":45963},{\"attributes\":{\"id\":\"b22\"},\"end\":46525,\"start\":46276},{\"attributes\":{\"id\":\"b23\"},\"end\":46768,\"start\":46527},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":215827033},\"end\":47155,\"start\":46770},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6099034},\"end\":47386,\"start\":47157},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":161878},\"end\":47635,\"start\":47388},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":972888},\"end\":47997,\"start\":47637},{\"attributes\":{\"id\":\"b28\"},\"end\":48230,\"start\":47999},{\"attributes\":{\"id\":\"b29\"},\"end\":48424,\"start\":48232}]", "bib_title": "[{\"end\":39752,\"start\":39699},{\"end\":40092,\"start\":40041},{\"end\":40385,\"start\":40316},{\"end\":40630,\"start\":40602},{\"end\":40891,\"start\":40847},{\"end\":41133,\"start\":41091},{\"end\":41680,\"start\":41632},{\"end\":41961,\"start\":41909},{\"end\":42258,\"start\":42207},{\"end\":42540,\"start\":42492},{\"end\":42791,\"start\":42748},{\"end\":43028,\"start\":42973},{\"end\":43255,\"start\":43203},{\"end\":43591,\"start\":43540},{\"end\":43994,\"start\":43950},{\"end\":44661,\"start\":44582},{\"end\":45040,\"start\":44982},{\"end\":45372,\"start\":45295},{\"end\":45645,\"start\":45579},{\"end\":46049,\"start\":45963},{\"end\":46837,\"start\":46770},{\"end\":47185,\"start\":47157},{\"end\":47420,\"start\":47388},{\"end\":47752,\"start\":47637},{\"end\":48027,\"start\":47999},{\"end\":39752,\"start\":39699},{\"end\":40092,\"start\":40041},{\"end\":40385,\"start\":40316},{\"end\":40630,\"start\":40602},{\"end\":40891,\"start\":40847},{\"end\":41133,\"start\":41091},{\"end\":41680,\"start\":41632},{\"end\":41961,\"start\":41909},{\"end\":42258,\"start\":42207},{\"end\":42540,\"start\":42492},{\"end\":42791,\"start\":42748},{\"end\":43028,\"start\":42973},{\"end\":43255,\"start\":43203},{\"end\":43591,\"start\":43540},{\"end\":43994,\"start\":43950},{\"end\":44661,\"start\":44582},{\"end\":45040,\"start\":44982},{\"end\":45372,\"start\":45295},{\"end\":45645,\"start\":45579},{\"end\":46049,\"start\":45963},{\"end\":46837,\"start\":46770},{\"end\":47185,\"start\":47157},{\"end\":47420,\"start\":47388},{\"end\":47752,\"start\":47637},{\"end\":48027,\"start\":47999}]", "bib_author": "[{\"end\":39768,\"start\":39754},{\"end\":39776,\"start\":39768},{\"end\":39782,\"start\":39776},{\"end\":39790,\"start\":39782},{\"end\":39801,\"start\":39790},{\"end\":40101,\"start\":40094},{\"end\":40113,\"start\":40101},{\"end\":40120,\"start\":40113},{\"end\":40132,\"start\":40120},{\"end\":40400,\"start\":40387},{\"end\":40410,\"start\":40400},{\"end\":40418,\"start\":40410},{\"end\":40639,\"start\":40632},{\"end\":40651,\"start\":40639},{\"end\":40659,\"start\":40651},{\"end\":40666,\"start\":40659},{\"end\":40678,\"start\":40666},{\"end\":40900,\"start\":40893},{\"end\":40908,\"start\":40900},{\"end\":40915,\"start\":40908},{\"end\":40922,\"start\":40915},{\"end\":41144,\"start\":41135},{\"end\":41426,\"start\":41420},{\"end\":41439,\"start\":41426},{\"end\":41447,\"start\":41439},{\"end\":41457,\"start\":41447},{\"end\":41469,\"start\":41457},{\"end\":41477,\"start\":41469},{\"end\":41694,\"start\":41682},{\"end\":41704,\"start\":41694},{\"end\":41713,\"start\":41704},{\"end\":41972,\"start\":41963},{\"end\":41983,\"start\":41972},{\"end\":41994,\"start\":41983},{\"end\":42003,\"start\":41994},{\"end\":42272,\"start\":42260},{\"end\":42284,\"start\":42272},{\"end\":42294,\"start\":42284},{\"end\":42553,\"start\":42542},{\"end\":42564,\"start\":42553},{\"end\":42576,\"start\":42564},{\"end\":42806,\"start\":42793},{\"end\":42810,\"start\":42806},{\"end\":43042,\"start\":43030},{\"end\":43264,\"start\":43257},{\"end\":43271,\"start\":43264},{\"end\":43279,\"start\":43271},{\"end\":43286,\"start\":43279},{\"end\":43294,\"start\":43286},{\"end\":43601,\"start\":43593},{\"end\":43609,\"start\":43601},{\"end\":43619,\"start\":43609},{\"end\":43628,\"start\":43619},{\"end\":43634,\"start\":43628},{\"end\":43645,\"start\":43634},{\"end\":44002,\"start\":43996},{\"end\":44011,\"start\":44002},{\"end\":44018,\"start\":44011},{\"end\":44025,\"start\":44018},{\"end\":44375,\"start\":44366},{\"end\":44385,\"start\":44375},{\"end\":44393,\"start\":44385},{\"end\":44404,\"start\":44393},{\"end\":44673,\"start\":44663},{\"end\":44681,\"start\":44673},{\"end\":44690,\"start\":44681},{\"end\":44701,\"start\":44690},{\"end\":45057,\"start\":45042},{\"end\":45386,\"start\":45374},{\"end\":45395,\"start\":45386},{\"end\":45659,\"start\":45647},{\"end\":45672,\"start\":45659},{\"end\":46058,\"start\":46051},{\"end\":46064,\"start\":46058},{\"end\":46076,\"start\":46064},{\"end\":46089,\"start\":46076},{\"end\":46328,\"start\":46321},{\"end\":46337,\"start\":46328},{\"end\":46345,\"start\":46337},{\"end\":46352,\"start\":46345},{\"end\":46364,\"start\":46352},{\"end\":46599,\"start\":46590},{\"end\":46608,\"start\":46599},{\"end\":46621,\"start\":46608},{\"end\":46631,\"start\":46621},{\"end\":46852,\"start\":46839},{\"end\":46865,\"start\":46852},{\"end\":47200,\"start\":47187},{\"end\":47212,\"start\":47200},{\"end\":47225,\"start\":47212},{\"end\":47240,\"start\":47225},{\"end\":47429,\"start\":47422},{\"end\":47443,\"start\":47429},{\"end\":47455,\"start\":47443},{\"end\":47768,\"start\":47754},{\"end\":47780,\"start\":47768},{\"end\":48040,\"start\":48029},{\"end\":48060,\"start\":48040},{\"end\":48072,\"start\":48060},{\"end\":48288,\"start\":48276},{\"end\":48294,\"start\":48288},{\"end\":39768,\"start\":39754},{\"end\":39776,\"start\":39768},{\"end\":39782,\"start\":39776},{\"end\":39790,\"start\":39782},{\"end\":39801,\"start\":39790},{\"end\":40101,\"start\":40094},{\"end\":40113,\"start\":40101},{\"end\":40120,\"start\":40113},{\"end\":40132,\"start\":40120},{\"end\":40400,\"start\":40387},{\"end\":40410,\"start\":40400},{\"end\":40418,\"start\":40410},{\"end\":40639,\"start\":40632},{\"end\":40651,\"start\":40639},{\"end\":40659,\"start\":40651},{\"end\":40666,\"start\":40659},{\"end\":40678,\"start\":40666},{\"end\":40900,\"start\":40893},{\"end\":40908,\"start\":40900},{\"end\":40915,\"start\":40908},{\"end\":40922,\"start\":40915},{\"end\":41144,\"start\":41135},{\"end\":41426,\"start\":41420},{\"end\":41439,\"start\":41426},{\"end\":41447,\"start\":41439},{\"end\":41457,\"start\":41447},{\"end\":41469,\"start\":41457},{\"end\":41477,\"start\":41469},{\"end\":41694,\"start\":41682},{\"end\":41704,\"start\":41694},{\"end\":41713,\"start\":41704},{\"end\":41972,\"start\":41963},{\"end\":41983,\"start\":41972},{\"end\":41994,\"start\":41983},{\"end\":42003,\"start\":41994},{\"end\":42272,\"start\":42260},{\"end\":42284,\"start\":42272},{\"end\":42294,\"start\":42284},{\"end\":42553,\"start\":42542},{\"end\":42564,\"start\":42553},{\"end\":42576,\"start\":42564},{\"end\":42806,\"start\":42793},{\"end\":42810,\"start\":42806},{\"end\":43042,\"start\":43030},{\"end\":43264,\"start\":43257},{\"end\":43271,\"start\":43264},{\"end\":43279,\"start\":43271},{\"end\":43286,\"start\":43279},{\"end\":43294,\"start\":43286},{\"end\":43601,\"start\":43593},{\"end\":43609,\"start\":43601},{\"end\":43619,\"start\":43609},{\"end\":43628,\"start\":43619},{\"end\":43634,\"start\":43628},{\"end\":43645,\"start\":43634},{\"end\":44002,\"start\":43996},{\"end\":44011,\"start\":44002},{\"end\":44018,\"start\":44011},{\"end\":44025,\"start\":44018},{\"end\":44375,\"start\":44366},{\"end\":44385,\"start\":44375},{\"end\":44393,\"start\":44385},{\"end\":44404,\"start\":44393},{\"end\":44673,\"start\":44663},{\"end\":44681,\"start\":44673},{\"end\":44690,\"start\":44681},{\"end\":44701,\"start\":44690},{\"end\":45057,\"start\":45042},{\"end\":45386,\"start\":45374},{\"end\":45395,\"start\":45386},{\"end\":45659,\"start\":45647},{\"end\":45672,\"start\":45659},{\"end\":46058,\"start\":46051},{\"end\":46064,\"start\":46058},{\"end\":46076,\"start\":46064},{\"end\":46089,\"start\":46076},{\"end\":46328,\"start\":46321},{\"end\":46337,\"start\":46328},{\"end\":46345,\"start\":46337},{\"end\":46352,\"start\":46345},{\"end\":46364,\"start\":46352},{\"end\":46599,\"start\":46590},{\"end\":46608,\"start\":46599},{\"end\":46621,\"start\":46608},{\"end\":46631,\"start\":46621},{\"end\":46852,\"start\":46839},{\"end\":46865,\"start\":46852},{\"end\":47200,\"start\":47187},{\"end\":47212,\"start\":47200},{\"end\":47225,\"start\":47212},{\"end\":47240,\"start\":47225},{\"end\":47429,\"start\":47422},{\"end\":47443,\"start\":47429},{\"end\":47455,\"start\":47443},{\"end\":47768,\"start\":47754},{\"end\":47780,\"start\":47768},{\"end\":48040,\"start\":48029},{\"end\":48060,\"start\":48040},{\"end\":48072,\"start\":48060},{\"end\":48288,\"start\":48276},{\"end\":48294,\"start\":48288}]", "bib_venue": "[{\"end\":39839,\"start\":39801},{\"end\":40148,\"start\":40132},{\"end\":40433,\"start\":40418},{\"end\":40694,\"start\":40678},{\"end\":40938,\"start\":40922},{\"end\":41198,\"start\":41144},{\"end\":41418,\"start\":41397},{\"end\":41744,\"start\":41713},{\"end\":42028,\"start\":42003},{\"end\":42319,\"start\":42294},{\"end\":42592,\"start\":42576},{\"end\":42826,\"start\":42810},{\"end\":43058,\"start\":43042},{\"end\":43335,\"start\":43294},{\"end\":43699,\"start\":43645},{\"end\":44079,\"start\":44025},{\"end\":44364,\"start\":44300},{\"end\":44742,\"start\":44701},{\"end\":45098,\"start\":45057},{\"end\":45413,\"start\":45395},{\"end\":45727,\"start\":45672},{\"end\":46099,\"start\":46089},{\"end\":46319,\"start\":46276},{\"end\":46588,\"start\":46527},{\"end\":46919,\"start\":46865},{\"end\":47250,\"start\":47240},{\"end\":47483,\"start\":47455},{\"end\":47791,\"start\":47780},{\"end\":48094,\"start\":48072},{\"end\":48274,\"start\":48232},{\"end\":39839,\"start\":39801},{\"end\":40148,\"start\":40132},{\"end\":40433,\"start\":40418},{\"end\":40694,\"start\":40678},{\"end\":40938,\"start\":40922},{\"end\":41198,\"start\":41144},{\"end\":41418,\"start\":41397},{\"end\":41744,\"start\":41713},{\"end\":42028,\"start\":42003},{\"end\":42319,\"start\":42294},{\"end\":42592,\"start\":42576},{\"end\":42826,\"start\":42810},{\"end\":43058,\"start\":43042},{\"end\":43335,\"start\":43294},{\"end\":43699,\"start\":43645},{\"end\":44079,\"start\":44025},{\"end\":44364,\"start\":44300},{\"end\":44742,\"start\":44701},{\"end\":45098,\"start\":45057},{\"end\":45413,\"start\":45395},{\"end\":45727,\"start\":45672},{\"end\":46099,\"start\":46089},{\"end\":46319,\"start\":46276},{\"end\":46588,\"start\":46527},{\"end\":46919,\"start\":46865},{\"end\":47250,\"start\":47240},{\"end\":47483,\"start\":47455},{\"end\":47791,\"start\":47780},{\"end\":48094,\"start\":48072},{\"end\":48274,\"start\":48232},{\"end\":40444,\"start\":40435},{\"end\":41248,\"start\":41200},{\"end\":43372,\"start\":43337},{\"end\":43749,\"start\":43701},{\"end\":44129,\"start\":44081},{\"end\":44779,\"start\":44744},{\"end\":45135,\"start\":45100},{\"end\":45774,\"start\":45729},{\"end\":46105,\"start\":46101},{\"end\":46969,\"start\":46921},{\"end\":47256,\"start\":47252},{\"end\":47507,\"start\":47485},{\"end\":40444,\"start\":40435},{\"end\":41248,\"start\":41200},{\"end\":43372,\"start\":43337},{\"end\":43749,\"start\":43701},{\"end\":44129,\"start\":44081},{\"end\":44779,\"start\":44744},{\"end\":45135,\"start\":45100},{\"end\":45774,\"start\":45729},{\"end\":46105,\"start\":46101},{\"end\":46969,\"start\":46921},{\"end\":47256,\"start\":47252},{\"end\":47507,\"start\":47485}]"}}}, "year": 2023, "month": 12, "day": 17}