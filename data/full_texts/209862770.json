{"id": 209862770, "updated": "2023-11-26 14:51:47.576", "metadata": {"title": "A block-based generative model for attributed network embedding", "authors": "[{\"first\":\"Xueyan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Wenzhuo\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Katarzyna\",\"last\":\"Musial\",\"middle\":[]},{\"first\":\"Wanli\",\"last\":\"Zuo\",\"middle\":[]},{\"first\":\"Hongxu\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Hongzhi\",\"last\":\"Yin\",\"middle\":[]}]", "venue": "World Wide Web", "journal": "World Wide Web", "publication_date": {"year": 2021, "month": 7, "day": 15}, "abstract": "Attributed network embedding has attracted plenty of interest in recent years. It aims to learn task-independent, low-dimensional, and continuous vectors for nodes preserving both topology and attribute information. Most of the existing methods, such as random-walk based methods and GCNs, mainly focus on the local information, i.e., the attributes of the neighbours. Thus, they have been well studied for assortative networks (i.e., networks with communities) but ignored disassortative networks (i.e., networks with multipartite, hubs, and hybrid structures), which are common in the real world. To model both assortative and disassortative networks, we propose a block-based generative model for attributed network embedding from a probability perspective. Specifically, the nodes are assigned to several blocks wherein the nodes in the same block share the similar linkage patterns. These patterns can define assortative networks containing communities or disassortative networks with the multipartite, hub, or any hybrid structures. To preserve the attribute information, we assume that each node has a hidden embedding related to its assigned block. We use a neural network to characterize the nonlinearity between node embeddings and node attributes. We perform extensive experiments on real-world and synthetic attributed networks. The results show that our proposed method consistently outperforms state-of-the-art embedding methods for both clustering and classification tasks, especially on disassortative networks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3183749514", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/www/LiuYSMZCY21", "doi": "10.1007/s11280-021-00918-y"}}, "content": {"source": {"pdf_hash": "f67c422833d96acc3a3b35b7e409a4679947b34c", "pdf_src": "SpringerNature", "pdf_uri": "[\"https://arxiv.org/pdf/2001.01383v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4cd8af908105185ce5e5bfa26f41f3560c408f38", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/f67c422833d96acc3a3b35b7e409a4679947b34c.txt", "contents": "\nA block-based generative model for attributed network embedding\n\n\nXueyan Liu \nBo Yang \nWenzhuo Song \nKatarzyna Musial \n\u00b7 Wanli Zuo \nHongxu Chen \nHongzhi Yin \nA block-based generative model for attributed network embedding\n10.1007/s11280-021-00918-yReceived: 6 June 2020 / Revised: 21 June 2021 / Accepted: 23 June 2021 /Attributed networks \u00b7 Representation learning \u00b7 Disassortative networks \u00b7 Generative model\nAttributed network embedding has attracted plenty of interest in recent years. It aims to learn task-independent, low-dimensional, and continuous vectors for nodes preserving both topology and attribute information. Most of the existing methods, such as random-walk based methods and GCNs, mainly focus on the local information, i.e., the attributes of the neighbours. Thus, they have been well studied for assortative networks (i.e., networks with communities) but ignored disassortative networks (i.e., networks with multipartite, hubs, and hybrid structures), which are common in the real world. To model both assortative and disassortative networks, we propose a block-based generative model for attributed network embedding from a probability perspective. Specifically, the nodes are assigned to several blocks wherein the nodes in the same block share the similar linkage patterns. These patterns can define assortative networks containing communities or disassortative networks with the multipartite, hub, or any hybrid structures. To preserve the attribute information, we assume that each node has a hidden embedding related to its assigned block. We use a neural network to characterize the nonlinearity between node embeddings and node attributes. We perform extensive experiments on real-world and synthetic attributed networks. The results show that our proposed method consistently outperforms state-of-the-art embedding methods for both clustering and classification tasks, especially on disassortative networks.\n\nrelationships between the nodes, attributed networks provide much richer and heterogeneous information about the systems due to the fact that, during the modelling process, they include node features [24,49]. For example, in social networks, the attributes provide individuals' gender, nationality, location, and interests. In the protein-protein interaction networks, a protein is defined by the amino acid types, the protein structures (\u03b1-helices, \u03b2-sheet or turns), etc. A paper consists of a title, keywords, authors, and venue in the academic citation networks. Thus, studying attributed networks is particularly important for real-world networks and their applications.\n\nRecently, attributed network embedding or representation learning (RL) has become a research hotspot. RL methods aim to map nodes to low-dimension and continuous embeddings, while preserving both the topological properties and the attribute information of the attributed networks. Compared with traditional methods for specific network analysis tasks, including node clustering [6,7,12,43], node classification [45], link prediction [2,48], temporal paths discovery [58], and outlier and change point detection [39] [23], RL methods are task-independent. Therefore, the learned embeddings can be used by off-the-shelf methods to perform the downstream network analysis tasks.\n\nIn general, the representations of nodes in the same cluster (or block) 1 are similar so that traditional cluster/classification methods can use them for network analysis. Learning representations for assortative networks and disassortative networks are both critical because these two types of networks are common in the real world. Both assortative and disassortative networks are defined by the kinds of groups they contained. In [35], Newman and Clauset described the assortative structure as a group of nodes, in which the links are denser in the same group than between different groups. In contrast, the links are sparser intragroups than inter-groups for disassortative structures. Given this, we refer to the networks that contain only assortative structures as assortative networks. In other words, if a network is assortative, it satisfies the following condition: the nodes are connected densely in the same group and sparsely between the different groups. Similarly, we define the disassortative networks as networks containing at least a disassortative structure, i.e., at least a block containing nodes that connect sparsely within the block and densely between the blocks. For example, the academic citation networks are assortative networks since the papers are more likely to cite other papers in the same field. A food web is a disassortative network because a predator links densely to the preys and rarely connects to other predators. According to the above definition, a node's representation is highly correlated to its proximal node for assortative networks. However, for disassortative networks, the representations of two nodes in the same cluster also should be similar even if they are far away from each other in a geodesic sense.\n\nHowever, most of the existing RL methods, such as random-walk based methods [10,20,32,37,57] and graph neural networks (GNNs) based methods [17,26,27,33,36,47], are designed for assortative networks, ignoring the disassortative networks. For example, random-walk based methods [10,20,32,37,57] learn the structural similarity between node pairs using the concept of neighbours defined by k\u2212step random walk before learning embeddings. Intuitively, using random walk approaches, \"close\" nodes are more likely to co-occur in node sequences. Thus, the representation of a node is more similar to that of nodes in short distance than long distance [42]. Most of the GNN-based methods learn node embeddings by aggregating only the information from nodes in close proximity [38]. In this way, the embeddings of nodes depend greatly on their neighbour's messages. Besides, many matrix factorization-based methods [19,54,55] learned attributed network representation by decomposing the matrices that are constructed based on the network topology and node features. However, they cannot depict the relations between the different properties of the networks in a nonlinear way. More details about these methods are discussed in Related Work (Section 2).\n\nAlthough many efforts were devoted to addressing this problem, to develop a unified representation learning method for both assortative and disassortative attributed networks is still a challenging problem. For example, Gao et al. [13] proposed BiNE (Bipartite Network Embedding) to learn node embeddings for bipartite networks, in which the nodes connect sparsely to each other in the same cluster/type but densely between different clusters/types. They first constructed two new networks for each kind of nodes and then performed random walk on the original and newfound networks, respectively. BiNE requires prior knowledge about types of networks and labels of the nodes before designing an appropriate RL model. However, in real-world networks, those are unknown and expensive to be collected. It is essential to design a general attributed network embedding method for both assortative and disassortative networks, especially when their types are unknown.\n\nAnother approach that attempts to deal with both assortative and disassortative networks is the stochastic block model (SBM) [18]. SBM is commonly used to characterize networks with complicated structural patterns, including communities, multipartite, hubs, and hybrid structures. Recently, various extensions of SBM are presented for different tasks, including structural pattern detection [1], link prediction [16], signed networks analysis [22,53], and dynamic networks evolution [56]. However, these SBMs only consider the network topology, so they are not suitable for attributed networks. Additionally, the obtained embeddings of nodes by SBM are not general features but defined as assignment relationships between nodes and blocks, which will limit its application.\n\nIn light of the above problems, we propose a novel attributed network generative model (ANGM) for both assortative and disassortative networks and its learning algorithm inspired by the stochastic block model and neural networks. Specifically, we use the concept of \"block\" and \"block-block\" link probability matrix as model parameters to describe the generative process of the topology of networks with diverse structural patterns. It is worth noting that the block-block link probability matrix defines the types of networks in terms of the structural patterns. For example, if the diagonal entries of the matrix are higher than the off-diagonal entries, it depicts assortative networks, otherwise, it describes disassortative networks. This matrix will be optimized if we fit the model to real-world networks. Thus, ANGM is able to model both assortative and disassortative networks without prior knowledge about the types of networks. We introduce neural networks to integrate node attributes into our model. We assume the embeddings of nodes in the same block are similar, and then use a neural network to characterize the nonlinearity between the node embeddings and node attributes. Different from SBM, we use two latent variables to model node assignment and node embedding, respectively. Thus, the embeddings in our model can be used for different downstream tasks. Finally, we unify the generative process of nodes' links and attributes to a probabilistic graph model.\n\nThe main contributions of this paper are as follows:\n\n(1) We propose a block-based attributed network generative model (ANGM) for attributed network embedding. Besides assortative networks, ANGM can deal with disassortative networks, which are almost ignored by existing methods.\n\n(2) We propose a variational learning method for estimating the parameters and the latent variables of ANGM by maximizing the ELBO, which can use a simple distribution to approximate the intractable distribution. (3) We conduct extensive validations and comparisons on different downstream tasks, including node clustering, classification, and visualization, using both assortative and disassortative attributed networks. The results show that ANGM outperforms many state-of-the-art algorithms, especially for dealing with disassortative networks.\n\nThe rest of the paper is organised as follows. In Section 2, we review and discuss the state-of-the-art methods for attributed network embedding. In Section 3, we introduce the notations used in the paper and the definition of attributed network embedding. In Sections 4 and 5, we present the attributed network generative model and its learning method. In Section 6, we test our method on both synthetic and real-world datasets for different network analysis tasks, including node clustering, node classification, and visualization of representation. Finally, we conclude and summarize our proposed model and method in Section 7.\n\n\nRelated work\n\nIn recent years, attributed network embedding or representation learning (RL) has become a prominent research area that focuses on learning low-dimension, continuous, and task-dependent node embeddings/representations. Unlike the graph embedding methods designed for pure networks [4,5,15,21,40], the attributed network embedding methods can preserve both the topology and the attribute information. In this section, we review and discuss the state-of-the-art attributed network embedding methods. According to their principles, we divide them into three folds: matrix factorization based methods, random-walk based methods, and graph neural network based methods.\n\n\nMatrix factorization based methods\n\nMatrix factorization (MF) based methods construct matrices based on the network properties, such as the network topology and the node attributes, and then factorize them to obtain the node representations. In 2015, Yang et al. first proved that the DeepWalk algorithm [40] is equal to factorizing a matrix built based on the walking probability. Inspired that, they introduced text information into MF to use both structural and text information for network embedding. Accelerated attributed network embedding (AANE) [19] transformed the node attributes into the similarity matrix and then decomposed the matrix by cooperating the edge-based penalty to learn node representations efficiently. Binarized attributed network embedding (BANE) [54] constructed a Weisfeiler-Lehman proximity matrix to aggregate structural and attributed information and then formulated a factorization learning function for the proximity matrix to learn binary node embeddings faster. To solve the problem that networks are sparse in the real world, Yang et al. [55] considered the node text groups and then used the consistent relationships between the text clustering and node representations to learn the network embeddings under the nonnegative matrix factorization framework. In order to detect communities from attributed networks, Li et al. [31] first constructed a community structure embedding matrix considering the definition of the community in terms of the link density, and then they factorized both the embedding matrix and the attributed matrix to learn node embeddings about the community membership matrix. However, the above MF based methods use the liner functions to learn network representations. Thus, they could not characterize the relations between different properties of the networks nonlinearly.\n\n\nRandom-walk based methods\n\nRandom-walk based methods for attributed networks are mainly extended from DeepWalk [40] and Node2Vec [15] to learn the node embeddings. For example, Pan et al. [37] used the random-walk to model the structural information and then adopted Paragraph2Vec to describe the relations among the nodes, the attributes, and the labels. Feat-Walk [20] learnt node sequences by performing random-walk on the node-node network and distributed feature-walk on the node-attribute network, and then fed node sequences to the scalable word embedding algorithms to learn node embedding. Deep attributed network embedding (DANE) [10], attributed social network embedding (ASNE) [32], and attributed network representation learning (ANRL) [57] first learnt the structural proximity through executing random-walk or calculating the k\u2212order neighbours and then combined Word2Vec and deep neural networks together to encode structural and attributed proximity to the embeddings nonlinearly. As we know, if two nodes are closer, they are more likely to co-occur in the node sequences after executing random walk on the network. The co-occurrence of nodes makes their embeddings similar after we feed node sequences to the word2vec method. Thus, we can deduce that the embeddings of nodes are similar if they are densely connected. Therefore, these methods are only suitable for assortative networks but not applicable to disassortative networks.\n\n\nGraph neural networks\n\nUnlike random-walk based methods, graph neural network (GNN) based methods for attributed network embedding are inductive. It means that we can learn the node embeddings for newly coming nodes without retraining the models. Among all GNNs, graph convolutional network (GCN) [27] is the most popular one. Hamilton et. al. concluded the GCN and its variations to message passing algorithms. They adopted various aggregators to learn the node embeddings by aggregating the local attribute information [17]. Graph attention network (GAT) [28,47] introduced attention mechanism to describe the impact of valuable information on node embeddings. Graph wavelet neural network (GWNN) used graph wavelet as a set of bases and regarded wavelet transform as the convolution operator [50]. Graph U-Nets [11] extended pooling operations to attributed network embedding. Gama et al. [9] defined graph scattering transformation using diffusion wavelets to obtain stable network representation. These methods are discriminative, which means they usually require prior knowledge about labels of nodes to predefine the objective functions elaborately, which profoundly influence their performance. However, gaining proper prior information is expensive. To solve this problem, many researchers proposed generative GNN methods, which generate new samples according to probability theory, and then they regarded the gap between real and generative samples as the objective function. For example, variational graph auto-encoder (VGAE) [26] considered a two-layer GCN as an encoder to learn the node embeddings, then calculated the link probability between two nodes according to the inner product of their embeddings, finally decoded the network topology according to the link probability. Adversarial regularized variational graph auto-encoder (ARVGA) [36] incorporated the adversarial model to the VGAE for robust representation learning. Based on the decoding process in terms of inner product, VGAE and ARVGA assume that the more similar the embeddings of two nodes are, the more likely they are connected, which are consistent with assortative networks. From the perspective of the generative model, these two methods can only generate the topological structure of networks but not the node attributes. The graph attention auto-encoder (GATE) [44] used an attention machine to encode the node attributes to node representations and reversed the encoding process to generate the node attributes to solve this problem. Also, it utilized the links to minimize the difference between the two linked nodes' embeddings. Deep generative latent feature relational model (DGLFRM) [33], like VGAE and ARVGA, also used GCN as an encoder to obtain node embeddings, but then it used latent feature relational model and neural networks as decoders to generate links and attributes, respectively. DGLFRM defined node embedding by nodes' membership and the strength of the membership, which limits the application scopes of the embeddings. Besides, Graph2Gauss (G2G) [3] first obtained each node embedding's distribution. It derived an unsupervised loss function, which satisfied that the shortest path length between two nodes is smaller, their distributions are more similar. All the above GNN methods learn embedding of a node by aggregating attribute information of its neighbours, which means that the embeddings of two nodes in the long distance are irrelevant. Therefore, they can only perform well on assortative networks but not on disassortative networks.\n\n\nProblem statement\n\nIn this section, we first summarize the main notations used in this paper and formally define the problem of attributed network embedding.\n\nLet G = (V, E, X X X) denote an attributed network with n nodes, and each node has Mdimension attributes. V and E are the sets of nodes and edges, respectively. X X X \u2208 {0, 1} n\u00d7M or X X X \u2208 R n\u00d7M denotes the binary or continuous attribute matrix, and each row x x x i refers to the attributes of node i. A A A \u2208 {0, 1} n\u00d7n is the adjacency matrix of G, where a ij = 1 denotes node i links to node j , otherwise a ij = 0. Table 1 shows the main notations for describing the attributed network and the proposed model in this paper.\n\nWe define the problem of attributed network embedding as follows: Given an attributed network G, we aim to learn attributed networks' embedding Z Z Z \u2208 R n\u00d7D , where z z z i \u2208 R 1\u00d7D is a low-dimensional vector representation of node i, and D is the dimension of the embedding for each node.\n\n\nThe attributed network generative model\n\nIn this section, we propose an attributed network generative model ( Fig. 1) for both assortative and disassortative attributed network embedding.\n\nIn this work, we introduce a concept of \"block\" to our embedding method, and we can use \"block\" to model the hidden patterns for both attributes and topology of attributed networks. In the standard stochastic blockmodel, a block is a subset of similar nodes in terms of connections in a given network [52]. In our paper, we add node attributes to the concept of \"block\", assuming that the nodes' attributes in the same block are similar. For example, we can group the papers from the same field into a block in the citation networks. The papers in the same block are more likely to cite each other and seldom mention the papers from the other fields. Their attributes, such as keywords, venues, and titles, are also more similar in the same areas (blocks) than those from different fields. Specifically, we make four assumptions for our model: (a) a node belongs to one of K blocks; (b) embeddings of Table 1 Table of notations   Notations Definitions\nG = (V, E, X X X) Attributed network n Number of nodes in G M Dimension of attributes in G A A A \u2208 {0, 1} n\u00d7n Adjacency matrix of G X X X \u2208 {0, 1} n\u00d7M or X X X \u2208 R n\u00d7M Binary or continuous attribute matrix of G K Number of blocks in G D Dimension of node embeddings Z Z Z \u2208 R n\u00d7D Node embedding matrix c i \u2208 {1, 2, ..., K} Block of node i \u03c9 \u03c9 \u03c9 \u2208 [0, 1] 1\u00d7K Node assignment probability vector \u2208 [0, 1] 1\u00d7K\nBlock-block link probability matrix  Figure 2. Our model assume that any node i belongs to one of three blocks (k, l or q). When it comes to linkage patterns, using the nodes in block k as an example, all the nodes in this block link to each other with similar link density and also link to other nodes in blocks l or q with another similar link density. As shown in Figure 2a, the nodes are linked densely within the same block and sparsely with the different blocks. This network can be regarded as an assortative network with communities. Similarly, the network in Figure 2b is a disassortative network with multipartite structures, that is, the nodes are linked sparsely within the same block and densely with the different blocks. The network in Figure 2c is also a disassortative network containing a community (Block k) and two multipartite structures (Blocks q and l). Under these assumptions, the node embeddings depend more on the nodes who have the similar structural patterns with them than their neighbors. In this way, we can learn the node embeddings for both the assortative and the disassortative networks. Mathematically, we define an attributed network generative model (ANGM) as a 4-tuple:\n\u03bc \u03bc \u03bc k \u2208 R 1\u00d7D Mean of node embeddings in block k \u03c3 \u03c3 \u03c3 k \u2208 R 1\u00d7D Standard deviation of node embeddings in block k \u03c5 \u03c5 \u03c5 i \u2208 [0, 1] 1\u00d7M or \u03c5 \u03c5 \u03c5 i \u2208 R 1\u00d7M Probability of node i have the attributes if X X X \u2208 {0, 1} n\u00d7M or mean of the attributes of node i if X X X \u2208 R n\u00d7M \u03bb \u03bb \u03bb i Standard deviation of attributes of node i if X X X \u2208 R n\u00d7M i, j indexAN GM = (\u03c9 \u03c9 \u03c9, , \u03bc \u03bc \u03bc, \u03c3 \u03c3 \u03c3 ).( 1 )\nThe K-dimensional vector \u03c9 \u03c9 \u03c9 refers to the node assignment probability, wherein \u03c9 k denotes how likely it is that a node belongs to block k, and it satisfies the criterion K k=1 \u03c9 k = 1. is a K \u00d7 K matrix, where \u03c0 kl denotes the probability that two nodes in blocks k and l will be connected. \u03bc \u03bc \u03bc and \u03c3 \u03c3 \u03c3 are two K \u00d7 D matrices. \u03bc \u03bc \u03bc k and \u03c3 \u03c3 \u03c3 k denote the mean and the standard deviation of the embeddings of the nodes in block k, respectively.\n\nGiven an attributed network, we can deduce two latent variables: membership vector c c c = (c 1 , c 1 , ..., c n ) and embedding matrix Z Z Z \u2208 R n\u00d7D , wherein c i \u2208 {1, 2, ..., K} denotes that node i belongs to block c i , and vector z z z i \u2208 R 1\u00d7D denotes the embedding of the node i. Figure 1 shows the probabilistic graphical model of ANGM. In this Figure, A A A and X X X are observed data; Z Z Z and c c c are latent variables; , \u03c9 \u03c9 \u03c9, \u03bc \u03bc \u03bc, \u03c3 \u03c3 \u03c3 are model parameters.\n\nBased on ANGM and our assumptions, the generation process of an attributed network is designed as follows:\n\n1. For each node i:\n\n(a) Assign node i to one of K blocks according to a multinomial distribution: c i \u223c mul(\u03c9 \u03c9 \u03c9); (b) Generate the embedding of node i according to its membership and a Gaussian distribution: z z z i \u223c N (\u03bc \u03bc \u03bc c i , \u03c3 \u03c3 \u03c3 2 c i I); (c) Generate the attributes of node i:\n\u2022 if x x x i is binary, i.e., x x x i \u2208 {0, 1} 1\u00d7M , x x x i is generated according to a Bernoulli distribution: x x x i \u223c Ber(\u03c5 \u03c5 \u03c5 i ), where \u03c5 \u03c5 \u03c5 i = f (z z z i ; \u03b8), \u03c5 \u03c5 \u03c5 i \u2208 [0, 1] 1\u00d7M . \u2022 if x x x i is continuous, i.e., x x x i \u2208 R 1\u00d7M , x x x i is generated according to a Gaussian distribution: x x x i \u223c N (\u03c5 \u03c5 \u03c5 i , \u03bb \u03bb \u03bb 2 i I), where [\u03c5 \u03c5 \u03c5 i , log \u03bb \u03bb \u03bb 2 i ] = f (z z z i ; \u03b8), and \u03c5 \u03c5 \u03c5 i \u2208 R 1\u00d7M , \u03bb \u03bb \u03bb i \u2208 R 1\u00d7M .\nf (z z z i ; \u03b8) denotes the neural networks parameterized by \u03b8, the input of f is z z z i , and the output is the parameters of the Bernoulli distribution or the Gaussian distribution for the generation of node attributes. f models the nonlinearity between node embeddings and node attributes.\n\n2. For each node pair (i, j ):\n\n\u2022 Generate the link between node i and node j according to their memberships and a Bernoulli distribution: a ij \u223c Ber(\u03c0 c i ,c j ).\n\nAccording to the probabilistic graphical model shown in Figure 1 and the generation process, the likelihood of the complete-data (if the node attributes are binary) is written as (see Appendix for more details):\np(X X X, A A A, Z Z Z, c c c| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc) = p(A A A|c c c, )p(X X X|Z Z Z)p(Z Z Z|c c c, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc)p(c c c|\u03c9 \u03c9 \u03c9) = ij \u03c0 a ij c i c j (1 \u2212 \u03c0 c i c j ) 1\u2212a ij \u00d7 im \u03c5 x im im (1 \u2212 \u03c5 im ) (1\u2212x im ) \u00d7 id 1 \u221a 2\u03c0\u03c3 c i d e \u2212 (z id \u2212\u03bc c i d ) 2 2\u03c3 2 c i d \u00d7 i \u03c9 c i .( 2 )\nThe proposed generative model for attributed networks has two advantages. (1) It can generate networks with different structural patterns by setting different . For example, we can generate networks with communities by setting \u03c0 kk > \u03c0 kl for k = l and multipartite structures if \u03c0 kk < \u03c0 kl . (2) It defines the similarity of the node embeddings from the perspective of \"block\" instead of \"neighbours\". Thus, it considers global information of networks.\n\n\nThe learning method\n\nIn this section, we will introduce the learning algorithm for ANGM by fitting the model to a given attributed network and maximizing the likelihood of the data.\n\nBased on (2), the log-likelihood of the observed data is log p(A A A, X X X| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc) = log Z Z Z c c c p(X X X, A A A, Z Z Z, c c c| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc)dZ Z Z.\n\nOur goal is to maximize log p(A A A, X X X| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc) for finding the optimal model for a given attributed network. However, it is intractable to calculate (3) directly. Thus, we introduce a decomposable variational distribution q(Z Z Z, c c c|X X X), which is approximated to the intractable posterior distribution p(Z Z Z, c c c|A A A, X X X), and then we use Jensen's inequality to gain the lower bound of (3). Alternatively, we will maximize the log-likelihood's lower bound as shown in (4). \n\nAccording to the mean-field theory, we know q(Z Z Z, c c c|X X X) can be factorized as\n\n\nq(Z Z Z, c c c|X X X) = q(Z Z Z|X X X)q(c c c).\n\nWe use neural networks g parameterized by \u03c6 to calculate q(Z Z Z|X X X). The input is the node attribute X X X, and the outputs are the parameters of the Gaussian distribution for the embeddings of nodes. For each node i,\n[\u03bc \u03bc \u03bc i , log\u03c3 \u03c3 \u03c3 2 i ] = g(x x x i ; \u03c6), q(z z z i |x x x i ) = N (\u03bc \u03bc \u03bc i ,\u03c3 \u03c3 \u03c3 2 i I),(5)\nwhere\u03bc \u03bc \u03bc i ,\u03c3 \u03c3 \u03c3 2 i \u2208 R 1\u00d7D . Then we assume that q(c c c i ) = mul(\u03c4 i1 , \u03c4 i2 , ..., \u03c4 iK )\n\nwhere \u03c4 ik denotes the probability of node i belonging to block k. Thus, we can obtain L(A A A, X X X) according to (2), (4), (5), and (6) as follows:\nL(A A A, X X X) = ij kl \u03c4 ik \u03c4 jl [a ij log \u03c0 kl + (1 \u2212 a ij ) log(1 \u2212 \u03c0 kl )] + 1 L L l=1 n i=1 M m=1 [x im log \u03c5 (l) im + (1 \u2212 x im ) log(1 \u2212 \u03c5 (l) im )] \u2212 1 2 n i=1 K k=1 D d=1 \u03c4 ik (log \u03c3 2 kd +\u03c3 2 id \u03c3 2 kd + (\u03bc id \u2212 \u03bc kd ) 2 \u03c3 2 kd ) + n i=1 K k=1 \u03c4 ik log \u03c9 k \u03c4 ik + 1 2 n i=1 M d=1 (1 + log\u03c3 2 id ).( 7 )\nL is the sampling frequency for Z Z Z. Note, we assume X X X \u2208 {0, 1} n\u00d7M here. It is easy to extend to X X X \u2208 R n\u00d7M by using Gaussian distribution. To minimize the \u2212L(A A A, X X X), we will use the coordinate descent to optimize \u03c4 \u03c4 \u03c4 , , \u03c9 \u03c9 \u03c9, \u03bc \u03bc \u03bc and \u03c3 \u03c3 \u03c3 , and then use Adam to optimize the parameters of neural networks, i.e., \u03b8 and \u03c6.\n\nIn (7),\u03bc \u03bc \u03bc i and\u03c3 \u03c3 \u03c3 2 i are computed by (5). \u03c5 \u03c5 \u03c5 (l) i can be calculated by \u03c5 \u03c5 \u03c5 (l) i = f (z z z (l) i ; \u03b8), and z i z i z i (l) is sampled by (5). Using reparameterized trick [25], z z z N (0, 1), and \u2022 denotes Hadamard product. We derived the update formulas of \u03c4 \u03c4 \u03c4 , , \u03c9 \u03c9 \u03c9, \u03bc \u03bc \u03bc and \u03c3 \u03c3 \u03c3 as follows by making the derivative of \u2212L(A A A, X X X) with respect to them equal to zero (see Appendix for more details), respectively.\n(l) i =\u03bc \u03bc \u03bc i +\u03c3 \u03c3 \u03c3 i \u2022 (l) , where (l) \u223c\u03c4 ik \u221d exp( j l \u03c4 jl [a ij log \u03c0 kl + (1 \u2212 a ij ) log(1 \u2212 \u03c0 kl )] \u2212 1 2 D d (log \u03c3 2 kd +\u03c3 2 id \u03c3 2 kd + (\u03bc id \u2212 \u03bc kd ) 2 \u03c3 2 kd ) + log \u03c9 k ).( 8 )\u03c9 k = 1 n i \u03c4 ik ,(9)\n\u03c0 kl = ij \u03c4 ik \u03c4 jl a ij ij \u03c4 ik \u03c4 jl ,\n\u03bc kd = n i \u03c4 ik\u03bcid n i \u03c4 ik ,(10)\nand\n\u03c3 kd = n i \u03c4 ik (\u03c3 id + (\u03bc id \u2212 \u03bc kd ) 2 ) n i \u03c4 ik .(12)\nFinally, we summarize the learning algorithms in Algorithm 1. According to Algorithm 1, we analyse the time complexity of the process. In each iteration, it takes O(K 2 n 2 +KDn), O(K 2 n 2 ), and O(Kn) to update \u03c4 \u03c4 \u03c4 , , and \u03c9 \u03c9 \u03c9. Calculating \u03bc \u03bc \u03bc and \u03c3 \u03c3 \u03c3 takes O(KDn). Thus, the total time complexity is O(K 2 n 2 + KDn + W ) per iteration, where W is the scale of parameters of the neural networks.\n\n\nExperiments\n\nIn this section, we first introduce the state-of-the-art approaches that are compared with ANGM method proposed in this study. Then, we test our method on node clustering and node classification tasks on both assortative and disassortative real-world networks. Finally, we visualize and cluster the learned embeddings on the synthetic networks, which are generated by ANGM, to show the performance of the methods. The code of ANGM is available at https://github.com/liuxyjlu/ANGM.\n\n\nBaselines\n\nSince our method is unsupervised, we compare our method with several unsupervised network representation learning methods, which fall into four categories: classical pure network embedding methods (Node2Vec [15], NOBE [21]), matrix factorization based method (BANE [54]), random walk based methods (ASNE [32], and ANRL [57]) and graph neural networks based methods (VGAE [26], ARVGE [36], G2G [3], and GATE [44]). They are different types of state-of-the-art methods for attributed network embedding. The details of these methods are as follows. .\n\n\u2022 NOBE [21] is a spectral embedding method based on the non-backtracking strategy to exploits nonlinear structure of graphs. \u2022 Node2Vec [15] is a random-walk based method for learning node embeddings using only network topology. \u2022 BANE is a [54] is a matrix factorization model. It constructs Weisfeiler-Lehman proximity matrix that aggregated structural and attributed information and then factorizes the matrix to learn the binarized embeddings. \u2022 ASNE [32] first learns the structure embeddings by performing Node2Vec, then feeds the structure embeddings and attributes to the deep neural networks to determine the final embeddings. \u2022 ANRL [57] is a neighbour-enhancement auto-encoder. It uses the random-walk to learn structural proximity and then adopts the attribute-aware skip-gram model to merge the topology and the attributes information. \u2022 VGAE [26] is a variational graph auto-encoder method. The network topology and attributes are mapped to vectors by GCN, and then the vectors are decoded into the networks using the inner product of embeddings. \u2022 ARVGE [36] adds the adversarial model to VGAE to learn robust embeddings. \u2022 G2G [3] transforms the node attributes to the Gaussian distribution of the node embeddings, and leverages the personalized ranking to constraint the similarity between two node's embeddings. \u2022 GATE [44] uses an attention mechanism to encode the node attributes to node representations and reversed the encoding process to generate the node attributes. Then, it utilizes the links to minimize the difference between the two linked nodes' embeddings.\n\n\nNode clustering and node classification on real-world networks\n\nIn this section, we test our method for node clustering and node classification tasks on the real-world networks.\n\n\nReal-World Networks\n\nWe use eight real-world networks to test our proposed method as shown in Table 2, where n, m, K, and D are the numbers of nodes, edges, blocks, and attributes in the networks respectively, and Type denotes the types of the networks. Cornell, Texas, Washington, Wisconsin (Corn., Texa., Wash., and Wisc., for short) are hypertext datasets from four universities [8]. The nodes denote web pages. The connections for short) is an academic citation network [34]. The nodes are academic papers. The edges represent citation relations. The labels denote the research areas of the papers, and attributes are words in the papers. Actor is a cooperation network [38]. The nodes refer to actors. An edge between two nodes means that they co-occur in the same Wikipedia web pages. The labels refer to categories of the actors in Wikipedia. The attributes are some words in the actors' Wikipedia web pages. BlogCatalog (Blog. for short) and Flickr are social networks [30]. In BlogCatalog, the nodes denote bloggers, and a link between two nodes means that one blogger follows the other one. The attributes represent the keywords in the blogs of the bloggers. Moreover, the labels are the interests of the bloggers. In Flickr, the nodes are the users, and edges are friendships between the users, the attributes denote the users' interests, and the labels refer to the groups that the users joined. First, we analyse the structural patterns contained in real-world networks and infer the types of the networks. According to the definition of the structural patterns [52], we show the block-block link probability matrices and block models of two selected networks (i.e., Cornell and Citeseer) in Figure 3.\n\nBased on the ground truth and edges of the networks, the elements in the block matrices are calculated by \u03c0 kl = e kl s kl , where e kl denotes the number of links between block k and l in the real-world network, and s kl represents the number of links between block k and l in a fully-linked network with the same ground truth and the same nodes as the realworld network. For communities, generally speaking, node i is connected to node j with higher probability if they belong to the same blocks. For multipartite structures, two nodes in different blocks are more likely to connect with each other. From Figure 3a and b, Cornell is a disassortative network containing a community (block 2) and three multipartite structures (blocks 1-3, blocks 3-5, and blocks 2-4). From Figure 3c and d, the structural patterns in Citeseer are all identified as communities, which means Citeseer is an assortative network. Thus, the structural patterns in Cornell are more complicated than those in Citeseer. Using the same approach, we can infer that Texas, Washington, Wisconsin, and Actor are also disassortative networks. BlogCatalog and Flickr are assortative networks.\n\n\nExperiments settings\n\nThe experiments are run in two steps: 1) Learning step: We use each embedding method to learn node representations, i.e., low-dimension and task-independent vectors or embeddings from the topology and attributes information of a network. 2) Evaluation step: We evaluate In the learning step, we use grid search to obtain the hyperparameters of our method and the details are as follows. For all of the real-world networks, each neural network contains two layers, the dimension of the node embddings is 20, and the optimizer is Adam. For Cornell, Texas, Washington, and Wisconsin, each layer consists of 32 hidden unites, the learning rate is 0.001, the number of iterations is 600. For Citeseer, each layer includes 32 hidden unites, the learning rate is 0.005, the number of iterations is 1000. For Actor, BlogCatalog, and Flickr, each layer consists of 128 hidden unites and the learning rate is 0.01, the number of iterations is 2000.\n\nIn the evaluation step, we feed the learned node embeddings to the Gaussian mixture model (GMM) for node clustering. Then, we choose the normalized mutual information (NMI) [29] and accuracy (AC) [51] to evaluate the quality of the node embeddings by their results on GMM. For node classification, we first fix the ratio of test set to 20% and increase the ratio of training set from 10% to 80% by a step of 10%. Then, we train an SVM using the labels and the learned embedding of the nodes in the training set. Next, we use the trained SVM to predict the labels of nodes in the testing set. Finally, we choose Macro-F1 and Micro-F1 [41] to evaluate the quality of the learned node embeddings through the performances of the SVM. For each setting, we randomly sample nodes ten times and show the average Macro-F1 and Micro-F1.\n\nFor all baselines, we use the implementation released by the original authors and retain the hyperparameter settings in their implementation except for the dimension of the node embeddings, which is set to 20 to be same as our method for fairness. We use NOBE-GU and ANRL-WAN for NOBE and ANRL, which perform best among their variants. Table 3 and Figure 4 show the results of our method and the compared algorithms for node clustering and node classification tasks. Specially, \"Improvement\" means the ratio of the improvement of ANGM over the best performed baseline method in Table 3. For example, the \"Improvement\" of NMI on Cornell dataset can be calculated by 29.09%\u221212.98%\n\n\nExperimental Results\n\n\n12.98%\n\n= 124.11%, where 29.09% and 12.98% are the NMIs of our method (ANGM) and the best performing baseline method (ANRL), respectively.\n\nAmong the eight networks, ANGM outperforms all baselines on six and five networks under the NMI and AC metrics, respectively, as shown in Table 3. ANGM improves the NMI score by more than 15% on Cornell dataset compared ANRL. In the cases when  considering AC, ANGM increases by more than 10% compared with BANE on Wisconsin dataset and the ratio of the improvement is 29.31%. On Citeseer with communities, ANGM is in the second place under AC, and its AC score is only 0.4% less than that of G2G, which is the best performing baseline method. On another two assortative networks, i.e., BlogCatalog and Flickr, ANGM outperforms the baselines. This because the community in Citeseer is more obvious than that in BlogCatalog and Flickr. Therefore, the baselines designed for assortative networks are superior to ANGM on Citeseer but perform worse on the other two. On Actor, all algorithms perform poorly because there is little difference in the link probability between the different blocks. For the datasets for which ANGM performs best, especially on disassortative networks, the improvement over the second best performing method is significant as in majority of cases we notice more than 50% and 5% improvement ratio with respect to NMI and AC, respectively. From Figure 4 we can see that Macro-F1 and Micro-F1 maintain steady growth for all algorithms on small-scale networks (Cornell, Texas, Washington and Wisconsin). On Citeseer, Actor, BlogCatalog, and Flickr, the Macro-F1 and Micro-F1 are stable with the increasing of training ratio, because the small ratios of nodes are enough to train a SVM on these four networks. Our method (ANGM) outperforms the baselines on most of the networks, especially on Cornell and Flickr dataset, with more than 60% Macro-F1 and Micro-F1 in each training ratio on Flickr, which are at least 10% more than those of the best performing baseline method (ASNE), respectively. From Table 3 and Figure 4, we can conclude that ANGM performs better on disassortative networks with complicated structural patterns (Cornell, Taxes, Washington, Wisconsin, and Actor datasets), which is the main goal of our research. ANGM results are also very good and although, not the best, are comparable with other state-of-the-art algorithms on Citesser (an assortative network). Lower ANGM's performance on assortative networks than disassortative ones, when compared with other methods, is down to the fact that our proposed method uses to fit networks with different structures and other baseline embedding methods are designed for assortative networks.\n\n\nVisualization of representations on synthetic networks\n\nIn this section, we will test and visualize the performance of ANGM and the baselines on different kinds of synthetic networks, including assortative networks with communities and disassortative networks with multipartite structures, hubs, and hybrid structures. We first show how to generate synthetic networks step by step, and then we run the algorithms on these networks and show their results.\n\n\nGeneration model for synthetic networks\n\nTo test and visualize the performance of our method on networks with different structures, we generate four types of attributed networks, which are networks with communities, multipartite structures, hubs, and hybrid structures, respectively. We denote the model for generating networks as (n, K, \u03c9 \u03c9 \u03c9, , \u03c5 \u03c5 \u03c5), which can be regarded as a simplified version of our ANGM omitting the neural networks or an extension of standard SBM adding node attributes. n and K are the numbers of nodes and blocks, respectively; \u03c9 \u03c9 \u03c9, , \u03c5 \u03c5 \u03c5 have the same meaning as they are in Section 4. The generation process are as follows: (a) Assign each node i to one of K blocks according to a multinomial distribution: c i \u223c mul(\u03c9 \u03c9 \u03c9); (b) Generate node attributes x x x i according to a Bernoulli distribution:\n\nx x x i \u223c Ber(\u03c5 \u03c5 \u03c5 i ); (c) For each pair of nodes (i, j ), determine if there is a link between them according to a Bernoulli distribution: a ij \u223c Ber(\u03c0 c i c j ).\n\nThen, we give some details for setting the parameters to generate different types of networks. (a) To generate node attributes, we assume that the nodes in the same block have similar attributes by setting \u03c5 \u03c5 \u03c5 as follows. If node i belongs to block k, we assume that the elements of the n \u00d7 (K \u00d7 h)-dimension matrix \u03c5 are set as \u03c5 id = p a 1 if d \u2208 {(k \u2212 1) \u00d7 h + 1, (k \u2212 1) \u00d7 h + 2, ..., k \u00d7 h}, otherwise \u03c5 id = p a 2 . (b) For structural topology, we generate four types of networks as follows. We denote the indices of blocks as k, l \u2208 {1, 2, ..., K}. For networks with communities, we set \u03c0 kl = p s 1 if k = l, otherwise \u03c0 kl = p s 2 . For networks with multipartite structures, we set \u03c0 kl = p s 2 if k = l, otherwise \u03c0 kl = p s 1 . For networks with hubs, we set \u03c0 kl = p s 1 if k = l or k = K or l = K, otherwise \u03c0 kl = p s 2 . For networks with hybrid structures containing k 1 communities and k 2 multipartite networks (k 1 + k 2 = K), we set as followings:\n= \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 p s 1 p s 2 \u00b7 . . .\n\u00b7 p s 2 p s 2 p s 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 p s 2 p s 1  Figure 5 shows the adjacency and attribute matrices of the generated networks.\np s 2 \u00b7 . . . \u00b7 p s 1 p s 2 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6\nWe test the proposed method and the baselines on these generated attributed networks and show the results in Section 6.3.2.\n\n\nExperimental results\n\nWe first perform the proposed methods and the baselines on four types of networks and then map the embeddings into 2-dimension space by applying t-SNE [46] and then visualize them as shown in Figures 6, 7, 8 and 9. For t-SNE, the perplexity is set to 10, the number of iterations is 1000. Besides, we use GMM to cluster the nodes. Table 4 shows the clustering NMI and AC of the ten methods on four types of synthetic networks.\n\nFrom Table 4 and Figures 6-9, we can conclude several observations. (1) ANGM finds all blocks on four types of networks, and both NMI and AC of ANGM achieve 100%. Because    the parameter in ANGM is capable of characterizing networks with various structural patterns.\n\n(2) NOBE and Node2Vec perform worse than others on most networks, especially on networks with multipartite structures, because they only use the structural topology information but not the attribute information. It indicates that the additional node attributes can help the network representation methods to learn node embeddings with higher quality.\n\n(3) Among the four types of networks, most state-of-the-art attributed network embedding algorithms, like BANE, VGAE, ARVGE, and G2G, perform worst on the network with multipartite structures and perform best on that with communities. Since they assume that the attributes propagate based on the links, they are suitable in the case of linked nodes sharing similar embeddings. However, the nodes in different blocks are more likely to connect to each other in networks with multipartite structures. In summary, ANGM outperforms most of the baselines on these four types of synthetic networks, especially on disassortative networks. It indicates that ANGM can deal with both assortative networks and disassortative networks.\n\n\nConclusion\n\nIn this paper, we propose a novel block-based generative model for attributed network representation learning. Accordingly, we introduce \"block\" concept to attributed network embedding methods. The connection patterns related to blocks can define assortative networks with communities as well as disassortative networks with multipartite structures, hubs, or any hybrid of them. Then, we use neural networks to depict the nonlinearity between the node embeddings and the node attributes. The topology information and the attribute information are combined by assuming that the nodes in the same blocks share similar embeddings and similar linkage patterns. Finally, the variational inference is introduced for learning the parameters of the proposed model. Experiments show that our proposed model consistently outperforms state-of-the-art methods on both real-word and synthetic attributed networks with various structural patterns.\n\nSo far, the proposed method's time complexity is square to the network scale. In future work, we will try to use a Poisson distribution [14] to generate the links between the nodes to reduce the time complexity. Under the Poisson distribution, the time complexity will be relative to the number of edges, which is linear to the network scale in most real-world networks.\n\n\nDerivation of likelihood of complete-data\n\nAccording to the generative process of attributed networks, the joint probability or the likelihood of complete-data is p(X X X, A A A, Z Z Z, c c c| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc) = p(A A A|c c c, )p(X X X|Z Z Z)p(Z Z Z|c c c, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc)p(C C C|\u03c9 \u03c9 \u03c9) (13) and each factor is defined as follows.\n\nFirst, we know that the node assignment follows a multinomial distribution. The probability of node i belongs to block k is \u03c9 k and the assignment for each node is independent. Thus, the probability of assigning all nodes, i.e., obtaining vector c c c =< c 1 , c 2 , ...c n >, is\np(c c c|\u03c9 \u03c9 \u03c9) = i \u03c9 c i .(14)\nThen, the embedding of node i follows a Gaussian distribution with mean \u03bc \u03bc \u03bc c i and standard derivation \u03c3 \u03c3 \u03c3 c i if we know that node i belongs to block c i . Thus, we have \n\nAs for the probability of generating node attributes, if X X X \u2208 {0, 1} n\u00d7M , it follows a Bernoulli distribution, i.e., the probability of node i having m-th attribute is \u03c5 im . Thus,\np(X X X|Z Z Z) = im \u03c5 x im im (1 \u2212 \u03c5 im ) 1\u2212x im ,(16)\nSimilarly, if X X X \u2208 R n\u00d7M , we can obtain\np(X X X|Z Z Z) = im 1 \u221a 2\u03c0\u03bb im e \u2212 (x im \u2212\u03c5 im ) 2 2\u03bb 2 im .(17)\nFinally, generating links between each pair of nodes follows a Bernoulli distribution and the generation process of each pair of nodes is independent. The probability of node i connecting to node j is \u03c0 c i c j if the node assignment is known. Thus, the probability of generating links is\np(A A A|c c c, ) = ij \u03c0 a ij c i c j (1 \u2212 \u03c0 c i c j ) 1\u2212a ij .(18)\nUsing a network with binary attributes as an example, we substitute (14)- (16) and (18) \n\n\nDerivation of update rules of the parameters\n\nFirst, the items related to \u03c4 \u03c4 \u03c4 on (7) \n+ (\u03bc id \u2212 \u03bc kd ) 2 \u03c3 2 kd ) + log \u03c9 k ).(24)\nThen, we optimize \u03c0 kl :\nL [\u03c0 kl ] = ij \u03c4 ik \u03c4 jl [A ij log \u03c0 kl + (1 \u2212 A ij ) log(1 \u2212 \u03c0 kl )]. Set \u2202L [\u03c0 kl ]\n\u2202\u03c0 kl = 0, we obtain \u03c0 kl = ij \u03c4 ik \u03c4 jl A ij ij \u03c4 ik \u03c4 jl .\n\nNext, the items related to \u03c9 k are\nL [\u03c9 k ] = i \u03b3 ik log \u03c9 k .(23)\nSince K k=1 = 1, we take the derivative of L [\u03c9 k ] + \u03b2( k \u03c9 k \u2212 1) of \u03c9 k , and make the derivative to zero. Then, we can obtain the update formula for \u03c9 k as follows:\n\u03c9 k = 1 n i \u03b3 ik .(23)\nIn the same way, we can obtain the items related to \u03bc kd and \u03c3 kd as follows: \nL [\u03bc kd ] = \u2212 1 2 n i \u03b3 ik (\u03bc id \u2212 \u03bc kd ) 2 \u03c3 2 kd ,(23)\nWe set \n\nand \u03c3 kd = n i \u03c4 ik (\u03c3 id + (\u03bc id \u2212 \u03bc kd ) 2 ) n i \u03c4 ik , (23) respectively.\n\n\nof nodes k, l index of blocks m index of attribute for each node d index of embedding for each node nodes in the same block are similar; (c) node embeddings and node attributes are related nonlinearly; (d) nodes in the same blocks share similar linkage patterns. For example, there is a network containing 12 nodes as shown in\n\nFigure 1 Figure 2\n12The Examples for assortative and disassortative networks. a An assortaitve network with communities; b and c Disassortaitve networks with multipartite structures and hybrid structures, respectively\n\n\nlog p(A A A, X X X| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc) = logZ Z Z c c c q(Z Z Z, c c c|X X X) p(X X X, A A A, Z Z Z, c c c| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc) q(Z Z Z, c c c|X X X) dZ Z Z \u2265 Z Z Z c c c q(Z Z Z, c c c|X X X) log p(X X X, A A A, Z Z Z, c c c| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc) q(Z Z Z, c c c|X X X) dZ Z Z= E q(Z Z Z,c c c|X X X) log p(X X X, A A A, Z Z Z, c c c| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc) q(Z Z Z, c c c|X X X) = L(A A A, X X X)\n\nFigure 3\n3The block matrices and block models of Cornell and Citeseer. The color in (a) and (c) denotes the link density of nodes in each blocks. The circles in (b) and (d) denote the blocks and the arrows denote the higher link probabilities the quality of the learned embeddings in machine learning-based network analysis tasks, including node clustering and node classification.\n\nFigure 4\n4Micro-F1 and Macro-F1 of the methods on the node classification for eight real-world networks\n\nFigure 5\n5The adjacency and attribute matrices of attributed networks. a-d are the adjacency matrices of networks with 4 types of structural patterns: a communities; b multipartite structures; c hubs; and ds hybrid structures. e is the attribute matrix of one of the networks Here, we set n = 128, K = 4, k 1 = k 2 = 2, \u03c9 \u03c9 \u03c9 = ( 1 4 , 1 4 , 1 4 , 1 4 ), h = 50, p s 1 = p a 1 = 0.4, and p s 2 = p a 2 = 0.1.\n\nFigure 6\n6Visualization of representation learned by algorithms on attributed networks with communities\n\nFigure 7 Figure 8\n78Visualization of representation learned by algorithms on attributed networks with multipartite structures Visualization of representation learned by algorithms on attributed networks with hubs\n\nFigure 9\n9Visualization of representation learned by algorithms attributed networks with hybrid structures\n\n\nto(13), we obtainp(X X X, A A A, Z Z Z, c c c| , \u03c9 \u03c9 \u03c9, \u03c3 \u03c3 \u03c3 , \u03bc \u03bc \u03bc) c j (1 \u2212 \u03c0 c i c j ) 1\u2212a ij \u00d7 im \u03c5 x im im (1 \u2212 \u03c5 im ) (1\u2212x im ) \u00d7 id 1 \u221a 2\u03c0\u03c3 c i d e \u2212 (z id \u2212\u03bc c i d )\n\n\u2202L\n[\u03bc kd ] \u2202\u03bc kd = 0 and \u2202L [\u03c3 kd ] \u2202\u03c3 kd = 0, then we derive the update rules for \u03bc kd and \u03c3 kd are \u03bc kd =\n\nTable 2 Statistic\n2features of \neight real-world networks \nNetwork \nn \nm \nK \nD \nType \n\nCorn. \n195 \n304 \n5 \n1,703 \ndisassortative \n\nTexa. \n187 \n328 \n5 \n1,703 \ndisassortative \n\nWash. \n230 \n446 \n5 \n1,703 \ndisassortative \n\nWisc. \n265 \n530 \n5 \n1,703 \ndisassortative \n\nCite. \n3,312 \n4,715 \n6 \n3,703 \nassortative \n\nActor \n7,600 \n33,544 \n4 \n931 \ndisassortative \n\nBlog. \n5,196 \n17,143 \n6 \n8,189 \nassortative \n\nFlickr \n7,575 \n239,738 \n9 \n12,047 \nassortative \n\nindicate hyperlinks. Node labels are types of web pages, including student, staff, faculty, \ncourse, and research project. The attributes refer to the words in web pages. Citeseer (Cite. \n\n\nTable 3\n3NMI (%) and AC (%) of the methods and improvement ratio (%) on node clustering for eight real-world networksMetric \nMethod \nCorn. \nTexa. \nWash. \nWisc. \nCite. \nActor \nBlog. \nFlickr \n\nNMI \nNOBE \n3.29 \n4.87 \n2.97 \n6.46 \n7.69 \n0.12 \n1.79 \n1.72 \n\nNode2Vec \n7.60 \n5.57 \n3.67 \n2.52 \n17.03 \n0.09 \n20.26 \n17.94 \n\nBANE \n12.57 \n17.67 \n16.82 \n20.00 \n9.24 \n0.44 \n3.38 \n1.83 \n\nASNE \n8.57 \n16.23 \n22.77 \n19.10 \n15.72 \n4.04 \n5.45 \n7.60 \n\nANRL \n12.98 \n15.28 \n16.56 \n10.46 \n35.43 \n1.00 \n4.31 \n5.70 \n\nVGAE \n7.31 \n5.53 \n10.33 \n8.23 \n17.80 \n0.62 \n11.57 \n15.40 \n\nARVGE \n10.80 \n12.64 \n10.80 \n8.18 \n19.71 \n1.69 \n23.83 \n12.89 \n\nG2G \n9.25 \n5.00 \n4.89 \n9.39 \n33.54 \n0.90 \n14.45 \n5.45 \n\nGATE \n8.10 \n10.69 \n10.94 \n8.73 \n35.09 \n0.49 \n22.12 \n10.74 \n\nANGM \n29.09 \n28.64 \n34.73 \n40.10 \n29.04 \n2.45 \n23.87 \n21.49 \n\nImprovement \n124.11 \n62.08 \n52.53 \n100.50 \n-18.03 \n-39.36 \n0.17 \n19.79 \n\nAC \nNOBE \n33.85 \n50.81 \n37.33 \n38.93 \n27.14 \n25.82 \n17.86 \n11.80 \n\nNode2Vec \n41.54 \n32.62 \n47.39 \n40.75 \n41.49 \n24.05 \n36.86 \n32.75 \n\nBANE \n38.05 \n45.56 \n42.91 \n43.62 \n33.34 \n23.96 \n22.92 \n15.76 \n\nASNE \n40.51 \n39.04 \n43.91 \n43.77 \n42.91 \n28.92 \n26.35 \n20.86 \n\nANRL \n36.92 \n42.62 \n45.43 \n38.57 \n51.76 \n23.33 \n27.12 \n17.54 \n\nVGAE \n34.62 \n34.06 \n39.17 \n30.30 \n37.00 \n22.62 \n31.32 \n30.07 \n\nARVGE \n31.90 \n42.73 \n37.87 \n35.25 \n42.65 \n22.79 \n40.57 \n24.30 \n\nG2G \n35.39 \n35.83 \n32.61 \n32.45 \n55.34 \n23.97 \n33.74 \n20.07 \n\nGATE \n37.95 \n48.13 \n41.30 \n34.72 \n51.54 \n21.67 \n38.11 \n22.24 \n\nANGM \n44.21 \n50.59 \n54.35 \n56.60 \n54.98 \n25.14 \n43.79 \n33.74 \n\nImprovement \n6.43 \n-0.43 \n14.69 \n29.31 \n-0.65 \n-13.07 \n7.94 \n3.02 \n\n\nTable 4\n4NMI (%) and ACC (%) of the methods on node clustering for four synthetic networksMetrics \nMethod \nCommunity \nMultipartite \nHub \nHybrid \n\nNMI \nNOBE \n97.48 \n1.80 \n28.35 \n39.31 \nNode2Vec \n90.78 \n7.07 \n51.64 \n57.22 \nBANE \n94.96 \n16.48 \n52.97 \n50.22 \nASNE \n94.98 \n86.54 \n60.38 \n97.48 \nANRL \n100 \n95.75 \n94.98 \n100 \nVGAE \n100 \n81.54 \n100 \n91.82 \nARVGE \n100 \n57.49 \n92.49 \n78.01 \nG2G \n92.48 \n1.53 \n92.47 \n61.24 \nGATE \n100 \n97.48 \n75.83 \n71.38 \nANGM (ours) \n100 \n100 \n100 \n100 \n\nAC \nNOBE \n99.22 \n30.47 \n41.41 \n39.31 \nNode2Vec \n98.88 \n36.72 \n64.06 \n68.75 \nBANE \n98.44 \n46.09 \n71.88 \n50.03 \nASNE \n97.66 \n89.85 \n72.66 \n99.22 \nANRL \n100 \n98.44 \n98.44 \n100 \nVGAE \n100 \n93.75 \n100 \n96.88 \nARVGE \n100 \n78.91 \n97.66 \n89.85 \nG2G \n97.66 \n30.47 \n97.66 \n71.09 \nGATE \n100 \n99.22 \n89.84 \n83.59 \n\nANGM (ours) \n100 \n100 \n100 \n100 \n\n\nare :\nareL [\u03c4 ik ] = j l \u03c4 ik \u03c4 jl [a ij log \u03c0 kl + (1 \u2212 a ij ) log(1 \u2212 \u03c0 kl )]\u2202\u03c4 ik = 0, then we can update \u03c4 ik by(20) \n\n\u2212 \n1 \n2 \n\nD \n\nd=1 \n\n\u03c4 ik (log \u03c3 2 \nkd +\u03c3 \n\n2 \nid \n\n\u03c3 2 \n\nkd \n\n+ \n(\u03bc id \u2212 \u03bc kd ) 2 \n\u03c3 2 \n\nkd \n\n) \n(21) \n\n+\u03c4 ik log \n\u03c9 k \n\u03c4 ik \n. \n(22) \n\nSet \n\n\u2202L [\u03c4 ik ] \n\n\u03c4 ik \u221d exp ( \n\nj \nl \n\n\u03c4 jl [a ij log \u03c0 kl + (1 \u2212 a ij ) log(1 \u2212 \u03c0 kl )] \n(23) \n\n\u2212 \n1 \n2 \n\nD \n\nd \n\n(log \u03c3 2 \nkd +\u03c3 \n\n2 \nid \n\n\u03c3 2 \n\nkd \n\n\nIn this paper, cluster, group, and block are interchangeable.\nSchool of Information Technology and Electrical Engineering, The University of Queensland, Brisbane, QLD, 4072, Australia\nCompeting interestsThe authors declare that they have no conflict of interest.AppendixIn this section, we give some details for the derivation of likelihood of complete-data and the update rules of the parameters of our model.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Affiliations\nCommunity detection and stochastic block models: recent developments. E Abbe, J. Mach. Learn. Res. 181Abbe, E.: Community detection and stochastic block models: recent developments. J. Mach. Learn. Res. 18(1), 6446-6531 (2017)\n\nWho to Follow and Why: Link Prediction with Explanations. N Barbieri, F Bonchi, G Manco, KDD. Barbieri, N., Bonchi, F., Manco, G.: Who to Follow and Why: Link Prediction with Explanations. In: KDD, pp. 1266-1275 (2014)\n\nDeep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking. A Bojchevski, S G\u00fcnnemann, International Conference on Learning Representations. Bojchevski, A., G\u00fcnnemann, S.: Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learn- ing via Ranking. In: International Conference on Learning Representations (2018)\n\nExploiting Centrality Information with Graph Convolutions for Network Representation Learning. H Chen, H Yin, T Chen, Q V H Nguyen, W C Peng, X Li, 2019 IEEE 35Th International Conference on Data Engineering (ICDE). IEEEChen, H., Yin, H., Chen, T., Nguyen, Q.V.H., Peng, W.C., Li, X.: Exploiting Centrality Information with Graph Convolutions for Network Representation Learning. In: 2019 IEEE 35Th International Conference on Data Engineering (ICDE), pp. 590-601. IEEE (2019)\n\nSocial boosted recommendation with folded bipartite network embedding. H Chen, H Yin, T Chen, W Wang, X Li, X Hu, IEEE Transactions on Knowledge and Data Engineering. Chen, H., Yin, H., Chen, T., Wang, W., Li, X., Hu, X.: Social boosted recommendation with folded bipartite network embedding. IEEE Transactions on Knowledge and Data Engineering (2020)\n\nContextual Community Search over Large Social Networks. L Chen, C Liu, K Liao, J Li, R Zhou, 2019 IEEE 35Th International Conference on Data Engineering (ICDE). IEEEChen, L., Liu, C., Liao, K., Li, J., Zhou, R.: Contextual Community Search over Large Social Networks. In: 2019 IEEE 35Th International Conference on Data Engineering (ICDE), pp. 88-99. IEEE (2019)\n\nFinding Attribute Diversified Communities in Complex Networks. A A Chowdhary, C Liu, L Chen, R Zhou, Y Yang, International Conference on Database Systems for Advanced Applications. SpringerChowdhary, A.A., Liu, C., Chen, L., Zhou, R., Yang, Y.: Finding Attribute Diversified Communities in Complex Networks. In: International Conference on Database Systems for Advanced Applications, pp. 19-35. Springer (2020)\n\nLearning to Extract Symbolic Knowledge from the World Wide Web. M Craveny, D Dipasquoy, D Freitagy, A Mccallumzy, T Mitchelly, K Nigamy, S Slatteryy, AAAICraveny, M., DiPasquoy, D., Freitagy, D., McCallumzy, A., Mitchelly, T., Nigamy, K., an Slatteryy, S.: Learning to Extract Symbolic Knowledge from the World Wide Web. In: AAAI, pp. 509-516 (1998)\n\nDiffusion Scattering Transforms on Graphs. F Gama, A Ribeiro, J Bruna, International Conference on Learning Representations. Gama, F., Ribeiro, A., Bruna, J.: Diffusion Scattering Transforms on Graphs. In: International Confer- ence on Learning Representations (2019)\n\nDeep Attributed Network Embedding. H Gao, H Huang, IJCAI. Gao, H., Huang, H.: Deep Attributed Network Embedding. In: IJCAI, pp. 3364-3370 (2018)\n\nGraph u-nets. H Gao, S Ji, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningGao, H., Ji, S.: Graph u-nets. In: Proceedings of the 36th International Conference on Machine Learning (2019)\n\nOn Community Outliers and Their Efficient Detection in Information Networks. J Gao, F Liang, W Fan, C Wang, Y Sun, J Han, KDD. ACMGao, J., Liang, F., Fan, W., Wang, C., Sun, Y., Han, J.: On Community Outliers and Their Efficient Detection in Information Networks. In: KDD, pp. 813-822. ACM (2010)\n\nBine: Bipartite Network Embedding. M Gao, L Chen, X He, A Zhou, SIGIR. Gao, M., Chen, L., He, X., Zhou, A.: Bine: Bipartite Network Embedding. In: SIGIR, pp. 715-724 (2018)\n\nScalable Recommendation with Hierarchical Poisson Factorization. P Gopalan, J M Hofman, D M Blei, UAI. Gopalan, P., Hofman, J.M., Blei, D.M.: Scalable Recommendation with Hierarchical Poisson Factoriza- tion. In: UAI, pp. 326-335 (2015)\n\nA Grover, J Leskovec, Node2vec: Scalable Feature Learning for Networks. KDDGrover, A., Leskovec, J.: Node2vec: Scalable Feature Learning for Networks. In: KDD (2016)\n\nMissing and spurious interactions and the reconstruction of complex networks. R Guimer\u00e0, M Sales-Pardo, PNAS. 10652Guimer\u00e0, R., Sales-Pardo, M.: Missing and spurious interactions and the reconstruction of complex networks. PNAS 106(52), 22073-22078 (2009)\n\nInductive Representation Learning on Large Graphs. W Hamilton, Z Ying, J Leskovec, Advances in Neural Information Processing Systems. Hamilton, W., Ying, Z., Leskovec, J.: Inductive Representation Learning on Large Graphs. In: Advances in Neural Information Processing Systems, pp. 1024-1034 (2017)\n\nStochastic blockmodels: First steps. P W Holland, K B Laskey, S Leinhardt, Soc. Netw. 52Holland, P.W., Laskey, K.B., Leinhardt, S.: Stochastic blockmodels: First steps. Soc. Netw. 5(2), 109- 137 (1983)\n\nAccelerated attributed network embedding. X Huang, J Li, X Hu, Proceedings of the 2017 SIAM international conference on data mining. the 2017 SIAM international conference on data miningHuang, X., Li, J., Hu, X.: Accelerated attributed network embedding. In: Proceedings of the 2017 SIAM international conference on data mining, pp. 633-641 (2017)\n\nLarge-Scale Heterogeneous Feature Embedding. X Huang, Q Song, F Yang, X Hu, AAAIHuang, X., Song, Q., Yang, F., Hu, X.: Large-Scale Heterogeneous Feature Embedding. In: AAAI (2019)\n\nOn spectral graph embedding: a non-backtracking perspective and graph approximation. F Jiang, L He, Y Zheng, E Zhu, J Xu, P S Yu, Proceedings of the 2018 SIAM International Conference on Data Mining. the 2018 SIAM International Conference on Data MiningSIAMJiang, F., He, L., Zheng, Y., Zhu, E., Xu, J., Yu, P.S.: On spectral graph embedding: a non-backtracking perspective and graph approximation. In: Proceedings of the 2018 SIAM International Conference on Data Mining, pp. 324-332. SIAM (2018)\n\nStochastic block model and exploratory analysis in signed networks. J Q Jiang, Phys. Rev. E. 91662805Jiang, J.Q.: Stochastic block model and exploratory analysis in signed networks. Phys. Rev. E 91(6), 062805 (2015)\n\nChange point detection in social networks-critical review with experiments. L Kendrick, K Musial, B Gabrys, Comput. Sci. Rev. 29Kendrick, L., Musial, K., Gabrys, B.: Change point detection in social networks-critical review with experiments. Comput. Sci. Rev. 29, 1-13 (2018)\n\nSet-based unified approach for summarization of a multi-attributed graph. K U Khan, W Nawaz, Y K Lee, World Wide Web. 203Khan, K.U., Nawaz, W., Lee, Y.K.: Set-based unified approach for summarization of a multi-attributed graph. World Wide Web 20(3), 543-570 (2017)\n\nAuto-encoding variational bayes. D P Kingma, M Welling, ICLRKingma, D.P., Welling, M.: Auto-encoding variational bayes. ICLR (2014)\n\nVariational graph auto-encoders. T N Kipf, M Welling, NIPS Workshop on Bayesian Deep Learning. Kipf, T.N., Welling, M.: Variational graph auto-encoders. NIPS Workshop on Bayesian Deep Learning (2016)\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. ICLR (2017)\n\nUnderstanding Attention and Generalization in Graph Neural Networks. B Knyazev, G W Taylor, M Amer, Advances in Neural Information Processing Systems. Knyazev, B., Taylor, G.W., Amer, M.: Understanding Attention and Generalization in Graph Neural Networks. In: Advances in Neural Information Processing Systems, pp. 4202-4212 (2019)\n\nUsing Diversity in Cluster Ensembles. L I Kuncheva, S T Hadjitodorov, IEEE International Conference on Systems, Man and Cybernetics. 2Kuncheva, L.I., Hadjitodorov, S.T.: Using Diversity in Cluster Ensembles. In: IEEE International Conference on Systems, Man and Cybernetics, vol. 2, pp. 1214-1219 (2004)\n\nUnsupervised streaming feature selection in social media. J Li, X Hu, J Tang, H Liu, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. the 24th ACM International on Conference on Information and Knowledge ManagementLi, J., Hu, X., Tang, J., Liu, H.: Unsupervised streaming feature selection in social media. In: Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pp. 1041- 1050 (2015)\n\nCommunity detection in attributed graphs: an embedding approach. Y Li, C Sha, X Huang, Y Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32Li, Y., Sha, C., Huang, X., Zhang, Y.: Community detection in attributed graphs: an embedding approach. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32 (2018)\n\nAttributed social network embedding. L Liao, X He, H Zhang, T S Chua, IEEE TKDE. 3012Liao, L., He, X., Zhang, H., Chua, T.S.: Attributed social network embedding. IEEE TKDE 30(12), 2257-2270 (2018)\n\nN Mehta, L C Duke, P Rai, Stochastic Blockmodels Meet Graph Neural Networks. In: ICML. Mehta, N., Duke, L.C., Rai, P.: Stochastic Blockmodels Meet Graph Neural Networks. In: ICML, pp. 4466-4474 (2019)\n\nQuery-Driven Active Surveying for Collective Classification. G Namata, B London, L Getoor, B Huang, U Edu, 10Th International Workshop on Mining and Learning with Graphs. 8Namata, G., London, B., Getoor, L., Huang, B., EDU, U.: Query-Driven Active Surveying for Collective Classification. In: 10Th International Workshop on Mining and Learning with Graphs, vol. 8 (2012)\n\nStructure and inference in annotated networks. M E Newman, A Clauset, Nat. Commun. 71Newman, M.E., Clauset, A.: Structure and inference in annotated networks. Nat. Commun. 7(1), 1-11 (2016)\n\nAdversarially Regularized Graph Autoencoder for Graph Embedding. S Pan, R Hu, G Long, J Jiang, L Yao, C Zhang, IJCAI. Pan, S., Hu, R., Long, G., Jiang, J., Yao, L., Zhang, C.: Adversarially Regularized Graph Autoencoder for Graph Embedding. In: IJCAI, pp. 2609-2615 (2018)\n\nTri-Party Deep Network Representation. S Pan, J Wu, X Zhu, C Zhang, Y Wang, IJCAI. Pan, S., Wu, J., Zhu, X., Zhang, C., Wang, Y.: Tri-Party Deep Network Representation. In: IJCAI, pp. 1895-1901 (2016)\n\nGeom-Gcn: Geometric Graph Convolutional Networks. H Pei, B Wei, K C C Chang, Y Lei, B Yang, ICLRPei, H., Wei, B., Chang, K.C.C., Lei, Y., Yang, B.: Geom-Gcn: Geometric Graph Convolutional Networks. In: ICLR (2020)\n\nFocused Clustering and Outlier Detection in Large Attributed Graphs. B Perozzi, L Akoglu, P Iglesias S\u00e1nchez, E M\u00fcller, KDD. ACMPerozzi, B., Akoglu, L., Iglesias S\u00e1nchez, P., M\u00fcller, E.: Focused Clustering and Outlier Detection in Large Attributed Graphs. In: KDD, pp. 1346-1355. ACM (2014)\n\nDeepwalk: Online Learning of Social Representations. B Perozzi, R Al-Rfou, S Skiena, KDDPerozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk: Online Learning of Social Representations. In: KDD, pp. 701-710 (2014)\n\nF-measure optimisation in multi-label classifiers. I Pillai, G Fumera, F Roli, Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012). the 21st International Conference on Pattern Recognition (ICPR2012)Pillai, I., Fumera, G., Roli, F.: F-measure optimisation in multi-label classifiers. In: Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012), pp. 2424-2427 (2012)\n\nL F Ribeiro, P H Saverese, D R Figueiredo, Struc2vec: Learning Node Representations from Structural Identity. In: KDD. Ribeiro, L.F., Saverese, P.H., Figueiredo, D.R.: Struc2vec: Learning Node Representations from Structural Identity. In: KDD, pp. 385-394 (2017)\n\nEfficient Community Detection in Large Networks Using Content and Links. Y Ruan, D Fuhry, S Parthasarathy, ACMRuan, Y., Fuhry, D., Parthasarathy, S.: Efficient Community Detection in Large Networks Using Content and Links. In: WWW, pp. 1089-1098. ACM (2013)\n\nA Salehi, H Davulcu, arXiv:1905.10715Graph attention auto-encoders. Salehi, A., Davulcu, H.: Graph attention auto-encoders. arXiv:1905.10715 (2019)\n\nSocial-Based Classification of Multiple Interactions in Dynamic Attributed Networks. T H Silva, A H Laender, P O V De Melo, 2018 IEEE International Conference on Big Data (Big Data). Silva, T.H., Laender, A.H., de Melo, P.O.V.: Social-Based Classification of Multiple Interactions in Dynamic Attributed Networks. In: 2018 IEEE International Conference on Big Data (Big Data), pp. 4063-4072 (2018)\n\nAccelerating t-sne using tree-based algorithms. L Van Der Maaten, J. Mach. Learn. Res. 151Van Der Maaten, L.: Accelerating t-sne using tree-based algorithms. J. Mach. Learn. Res. 15(1), 3221- 3245 (2014)\n\nP Veli\u010dkovi\u0107, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, Graph attention networks. ICLR. Veli\u010dkovi\u0107, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y.: Graph attention networks. ICLR (2018)\n\nHow to predict social relationships-physics-inspired approach to link prediction. A Wahid-Ul-Ashraf, M Budka, K Musial, Physica A: Stat. Mech. Appl. 523Wahid-Ul-Ashraf, A., Budka, M., Musial, K.: How to predict social relationships-physics-inspired approach to link prediction. Physica A: Stat. Mech. Appl. 523, 1110-1129 (2019)\n\nA survey of typical attributed graph queries. Y Wang, Y Li, J Fan, C Ye, M Chai, World Wide Web. 241Wang, Y., Li, Y., Fan, J., Ye, C., Chai, M.: A survey of typical attributed graph queries. World Wide Web 24(1), 297-346 (2021)\n\nGraph Wavelet Neural Network. B Xu, H Shen, Q Cao, Y Qiu, X Cheng, International Conference on Learning Representations. Xu, B., Shen, H., Cao, Q., Qiu, Y., Cheng, X.: Graph Wavelet Neural Network. In: International Conference on Learning Representations (2018)\n\nDocument Clustering Based on Non-Negative Matrix Factorization. W Xu, X Liu, Y Gong, SIGIR. Xu, W., Liu, X., Gong, Y.: Document Clustering Based on Non-Negative Matrix Factorization. In: SIGIR, pp. 267-273 (2003)\n\nCharacterizing and extracting multiplex patterns in complex networks. B Yang, J Liu, D Liu, IEEE Trans. Syst. Man Cybern. Part b Cybern. Publ. IEEE Syst. Man Cybern. Soc. 422469Yang, B., Liu, J., Liu, D.: Characterizing and extracting multiplex patterns in complex networks. IEEE Trans. Syst. Man Cybern. Part b Cybern. Publ. IEEE Syst. Man Cybern. Soc. 42(2), 469 (2012)\n\nStochastic blockmodeling and variational bayes learning for signed network analysis. B Yang, X Liu, Y Li, X Zhao, IEEE TKDE. 299Yang, B., Liu, X., Li, Y., Zhao, X.: Stochastic blockmodeling and variational bayes learning for signed network analysis. IEEE TKDE 29(9), 2026-2039 (2017)\n\nBinarized Attributed Network Embedding. H Yang, S Pan, P Zhang, L Chen, D Lian, C Zhang, 2018 IEEE International Conference on Data Mining (ICDM). IEEEYang, H., Pan, S., Zhang, P., Chen, L., Lian, D., Zhang, C.: Binarized Attributed Network Embedding. In: 2018 IEEE International Conference on Data Mining (ICDM), pp. 1476-1481. IEEE (2018)\n\nEnhanced Network Embedding with Text Information. S Yang, B Yang, 2018 24Th International Conference on Pattern Recognition (ICPR). IEEEYang, S., Yang, B.: Enhanced Network Embedding with Text Information. In: 2018 24Th International Conference on Pattern Recognition (ICPR), pp. 326-331. IEEE (2018)\n\nDetecting communities and their evolutions in dynamic social networks-a bayesian approach. T Yang, Y Chi, S Zhu, Y Gong, R Jin, Mach. Learn. 822Yang, T., Chi, Y., Zhu, S., Gong, Y., Jin, R.: Detecting communities and their evolutions in dynamic social networks-a bayesian approach. Mach. Learn. 82(2), 157-189 (2011)\n\nZ Zhang, H Yang, J Bu, S Zhou, P Yu, J Zhang, M Ester, C Wang, Anrl: Attributed Network Representation Learning via Deep Neural Networks. In: IJCAI. Zhang, Z., Yang, H., Bu, J., Zhou, S., Yu, P., Zhang, J., Ester, M., Wang, C.: Anrl: Attributed Network Representation Learning via Deep Neural Networks. In: IJCAI, pp. 3155-3161 (2018)\n\nTemporal paths discovery with multiple constraints in attributed dynamic graphs. A Zhao, G Liu, B Zheng, Y Zhao, K Zheng, World Wide Web. 231Zhao, A., Liu, G., Zheng, B., Zhao, Y., Zheng, K.: Temporal paths discovery with multiple constraints in attributed dynamic graphs. World Wide Web 23(1), 313-336 (2020)\n", "annotations": {"author": "[{\"end\":78,\"start\":67},{\"end\":87,\"start\":79},{\"end\":101,\"start\":88},{\"end\":119,\"start\":102},{\"end\":132,\"start\":120},{\"end\":145,\"start\":133},{\"end\":158,\"start\":146}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":74},{\"end\":86,\"start\":82},{\"end\":100,\"start\":96},{\"end\":118,\"start\":112},{\"end\":131,\"start\":128},{\"end\":144,\"start\":140},{\"end\":157,\"start\":154}]", "author_first_name": "[{\"end\":73,\"start\":67},{\"end\":81,\"start\":79},{\"end\":95,\"start\":88},{\"end\":111,\"start\":102},{\"end\":121,\"start\":120},{\"end\":127,\"start\":122},{\"end\":139,\"start\":133},{\"end\":153,\"start\":146}]", "author_affiliation": null, "title": "[{\"end\":64,\"start\":1},{\"end\":222,\"start\":159}]", "venue": null, "abstract": "[{\"end\":1939,\"start\":412}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2145,\"start\":2141},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2148,\"start\":2145},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2999,\"start\":2996},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3001,\"start\":2999},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3004,\"start\":3001},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3007,\"start\":3004},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3033,\"start\":3029},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3054,\"start\":3051},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3057,\"start\":3054},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3088,\"start\":3084},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3133,\"start\":3129},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3138,\"start\":3134},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3368,\"start\":3367},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3732,\"start\":3728},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5136,\"start\":5132},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5139,\"start\":5136},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5142,\"start\":5139},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5145,\"start\":5142},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5148,\"start\":5145},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5200,\"start\":5196},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5203,\"start\":5200},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5206,\"start\":5203},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5209,\"start\":5206},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5212,\"start\":5209},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5215,\"start\":5212},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5337,\"start\":5333},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5340,\"start\":5337},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5343,\"start\":5340},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5346,\"start\":5343},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5349,\"start\":5346},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5704,\"start\":5700},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5828,\"start\":5824},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5966,\"start\":5962},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":5969,\"start\":5966},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":5972,\"start\":5969},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6536,\"start\":6532},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7393,\"start\":7389},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7658,\"start\":7655},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7680,\"start\":7676},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7711,\"start\":7707},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7714,\"start\":7711},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7751,\"start\":7747},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11280,\"start\":11277},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11282,\"start\":11280},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11285,\"start\":11282},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11288,\"start\":11285},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11291,\"start\":11288},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11971,\"start\":11967},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12220,\"start\":12216},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12442,\"start\":12438},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12743,\"start\":12739},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13029,\"start\":13025},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13619,\"start\":13615},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13637,\"start\":13633},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13696,\"start\":13692},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13874,\"start\":13870},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14148,\"start\":14144},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14197,\"start\":14193},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":14257,\"start\":14253},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15259,\"start\":15255},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15483,\"start\":15479},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15519,\"start\":15515},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15522,\"start\":15519},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15757,\"start\":15753},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15776,\"start\":15772},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15853,\"start\":15850},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16499,\"start\":16495},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16817,\"start\":16813},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17312,\"start\":17308},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17640,\"start\":17636},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18019,\"start\":18016},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19995,\"start\":19991},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27377,\"start\":27374},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27387,\"start\":27384},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28116,\"start\":28113},{\"end\":28127,\"start\":28124},{\"end\":28177,\"start\":28174},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28223,\"start\":28220},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28257,\"start\":28253},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29988,\"start\":29984},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29999,\"start\":29995},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30046,\"start\":30042},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30085,\"start\":30081},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30100,\"start\":30096},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30152,\"start\":30148},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30164,\"start\":30160},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30173,\"start\":30170},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30188,\"start\":30184},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30337,\"start\":30333},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30466,\"start\":30462},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30571,\"start\":30567},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30785,\"start\":30781},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30973,\"start\":30969},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31186,\"start\":31182},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31399,\"start\":31395},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31472,\"start\":31469},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31667,\"start\":31663},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32481,\"start\":32478},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32574,\"start\":32570},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32774,\"start\":32770},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33077,\"start\":33073},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":33675,\"start\":33671},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36115,\"start\":36111},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":36138,\"start\":36134},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36575,\"start\":36571},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43154,\"start\":43150},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":45861,\"start\":45857},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46391,\"start\":46387},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":47704,\"start\":47700},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":48489,\"start\":48485},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":50798,\"start\":50794}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48832,\"start\":48504},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49051,\"start\":48833},{\"attributes\":{\"id\":\"fig_2\"},\"end\":49464,\"start\":49052},{\"attributes\":{\"id\":\"fig_3\"},\"end\":49847,\"start\":49465},{\"attributes\":{\"id\":\"fig_4\"},\"end\":49952,\"start\":49848},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50362,\"start\":49953},{\"attributes\":{\"id\":\"fig_6\"},\"end\":50467,\"start\":50363},{\"attributes\":{\"id\":\"fig_7\"},\"end\":50681,\"start\":50468},{\"attributes\":{\"id\":\"fig_8\"},\"end\":50789,\"start\":50682},{\"attributes\":{\"id\":\"fig_10\"},\"end\":50967,\"start\":50790},{\"attributes\":{\"id\":\"fig_12\"},\"end\":51076,\"start\":50968},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":51716,\"start\":51077},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":53286,\"start\":51717},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":54104,\"start\":53287},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":54519,\"start\":54105}]", "paragraph": "[{\"end\":2616,\"start\":1941},{\"end\":3293,\"start\":2618},{\"end\":5054,\"start\":3295},{\"end\":6299,\"start\":5056},{\"end\":7262,\"start\":6301},{\"end\":8037,\"start\":7264},{\"end\":9517,\"start\":8039},{\"end\":9571,\"start\":9519},{\"end\":9798,\"start\":9573},{\"end\":10347,\"start\":9800},{\"end\":10979,\"start\":10349},{\"end\":11660,\"start\":10996},{\"end\":13501,\"start\":11699},{\"end\":14955,\"start\":13531},{\"end\":18514,\"start\":14981},{\"end\":18674,\"start\":18536},{\"end\":19206,\"start\":18676},{\"end\":19498,\"start\":19208},{\"end\":19688,\"start\":19542},{\"end\":20641,\"start\":19690},{\"end\":22257,\"start\":21048},{\"end\":23102,\"start\":22648},{\"end\":23582,\"start\":23104},{\"end\":23690,\"start\":23584},{\"end\":23711,\"start\":23692},{\"end\":23982,\"start\":23713},{\"end\":24710,\"start\":24417},{\"end\":24742,\"start\":24712},{\"end\":24875,\"start\":24744},{\"end\":25088,\"start\":24877},{\"end\":25824,\"start\":25370},{\"end\":26008,\"start\":25848},{\"end\":26189,\"start\":26010},{\"end\":26701,\"start\":26191},{\"end\":26789,\"start\":26703},{\"end\":27062,\"start\":26841},{\"end\":27256,\"start\":27159},{\"end\":27408,\"start\":27258},{\"end\":28067,\"start\":27722},{\"end\":28511,\"start\":28069},{\"end\":28764,\"start\":28725},{\"end\":28802,\"start\":28799},{\"end\":29267,\"start\":28861},{\"end\":29763,\"start\":29283},{\"end\":30324,\"start\":29777},{\"end\":31913,\"start\":30326},{\"end\":32093,\"start\":31980},{\"end\":33810,\"start\":32117},{\"end\":34973,\"start\":33812},{\"end\":35936,\"start\":34998},{\"end\":36764,\"start\":35938},{\"end\":37444,\"start\":36766},{\"end\":37608,\"start\":37478},{\"end\":40188,\"start\":37610},{\"end\":40645,\"start\":40247},{\"end\":41483,\"start\":40689},{\"end\":41650,\"start\":41485},{\"end\":42622,\"start\":41652},{\"end\":42798,\"start\":42669},{\"end\":42974,\"start\":42851},{\"end\":43425,\"start\":42999},{\"end\":43694,\"start\":43427},{\"end\":44046,\"start\":43696},{\"end\":44771,\"start\":44048},{\"end\":45719,\"start\":44786},{\"end\":46091,\"start\":45721},{\"end\":46430,\"start\":46137},{\"end\":46711,\"start\":46432},{\"end\":46919,\"start\":46743},{\"end\":47105,\"start\":46921},{\"end\":47204,\"start\":47161},{\"end\":47558,\"start\":47270},{\"end\":47714,\"start\":47626},{\"end\":47804,\"start\":47763},{\"end\":47874,\"start\":47850},{\"end\":48021,\"start\":47961},{\"end\":48057,\"start\":48023},{\"end\":48258,\"start\":48090},{\"end\":48360,\"start\":48282},{\"end\":48425,\"start\":48418},{\"end\":48503,\"start\":48427}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":21047,\"start\":20642},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22609,\"start\":22258},{\"attributes\":{\"id\":\"formula_2\"},\"end\":22647,\"start\":22609},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24416,\"start\":23983},{\"attributes\":{\"id\":\"formula_4\"},\"end\":25369,\"start\":25089},{\"attributes\":{\"id\":\"formula_7\"},\"end\":27158,\"start\":27063},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27721,\"start\":27409},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28555,\"start\":28512},{\"attributes\":{\"id\":\"formula_11\"},\"end\":28703,\"start\":28555},{\"attributes\":{\"id\":\"formula_12\"},\"end\":28724,\"start\":28703},{\"attributes\":{\"id\":\"formula_13\"},\"end\":28798,\"start\":28765},{\"attributes\":{\"id\":\"formula_15\"},\"end\":28860,\"start\":28803},{\"attributes\":{\"id\":\"formula_16\"},\"end\":42668,\"start\":42623},{\"attributes\":{\"id\":\"formula_17\"},\"end\":42850,\"start\":42799},{\"attributes\":{\"id\":\"formula_18\"},\"end\":46742,\"start\":46712},{\"attributes\":{\"id\":\"formula_20\"},\"end\":47160,\"start\":47106},{\"attributes\":{\"id\":\"formula_21\"},\"end\":47269,\"start\":47205},{\"attributes\":{\"id\":\"formula_22\"},\"end\":47625,\"start\":47559},{\"attributes\":{\"id\":\"formula_24\"},\"end\":47849,\"start\":47805},{\"attributes\":{\"id\":\"formula_25\"},\"end\":47960,\"start\":47875},{\"attributes\":{\"id\":\"formula_27\"},\"end\":48089,\"start\":48058},{\"attributes\":{\"id\":\"formula_28\"},\"end\":48281,\"start\":48259},{\"attributes\":{\"id\":\"formula_29\"},\"end\":48417,\"start\":48361}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":10096,\"start\":10013},{\"end\":19105,\"start\":19098},{\"end\":20629,\"start\":20591},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32197,\"start\":32190},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37109,\"start\":37102},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37351,\"start\":37344},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37755,\"start\":37748},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39538,\"start\":39531},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":43337,\"start\":43330},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":43439,\"start\":43432}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":10994,\"start\":10982},{\"attributes\":{\"n\":\"2.1\"},\"end\":11697,\"start\":11663},{\"attributes\":{\"n\":\"2.2\"},\"end\":13529,\"start\":13504},{\"attributes\":{\"n\":\"2.3\"},\"end\":14979,\"start\":14958},{\"attributes\":{\"n\":\"3\"},\"end\":18534,\"start\":18517},{\"attributes\":{\"n\":\"4\"},\"end\":19540,\"start\":19501},{\"attributes\":{\"n\":\"5\"},\"end\":25846,\"start\":25827},{\"end\":26839,\"start\":26792},{\"attributes\":{\"n\":\"6\"},\"end\":29281,\"start\":29270},{\"attributes\":{\"n\":\"6.1\"},\"end\":29775,\"start\":29766},{\"attributes\":{\"n\":\"6.2\"},\"end\":31978,\"start\":31916},{\"attributes\":{\"n\":\"6.2.1\"},\"end\":32115,\"start\":32096},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":34996,\"start\":34976},{\"attributes\":{\"n\":\"6.2.3\"},\"end\":37467,\"start\":37447},{\"end\":37476,\"start\":37470},{\"attributes\":{\"n\":\"6.3\"},\"end\":40245,\"start\":40191},{\"attributes\":{\"n\":\"6.3.1\"},\"end\":40687,\"start\":40648},{\"attributes\":{\"n\":\"6.3.2\"},\"end\":42997,\"start\":42977},{\"attributes\":{\"n\":\"7\"},\"end\":44784,\"start\":44774},{\"end\":46135,\"start\":46094},{\"end\":47761,\"start\":47717},{\"end\":48851,\"start\":48834},{\"end\":49474,\"start\":49466},{\"end\":49857,\"start\":49849},{\"end\":49962,\"start\":49954},{\"end\":50372,\"start\":50364},{\"end\":50486,\"start\":50469},{\"end\":50691,\"start\":50683},{\"end\":50971,\"start\":50969},{\"end\":51095,\"start\":51078},{\"end\":51725,\"start\":51718},{\"end\":53295,\"start\":53288},{\"end\":54111,\"start\":54106}]", "table": "[{\"end\":51716,\"start\":51097},{\"end\":53286,\"start\":51835},{\"end\":54104,\"start\":53378},{\"end\":54519,\"start\":54222}]", "figure_caption": "[{\"end\":48832,\"start\":48506},{\"end\":49051,\"start\":48854},{\"end\":49464,\"start\":49054},{\"end\":49847,\"start\":49476},{\"end\":49952,\"start\":49859},{\"end\":50362,\"start\":49964},{\"end\":50467,\"start\":50374},{\"end\":50681,\"start\":50489},{\"end\":50789,\"start\":50693},{\"end\":50967,\"start\":50792},{\"end\":51076,\"start\":50972},{\"end\":51835,\"start\":51727},{\"end\":53378,\"start\":53297},{\"end\":54222,\"start\":54115}]", "figure_ref": "[{\"end\":19618,\"start\":19611},{\"end\":21093,\"start\":21085},{\"end\":21424,\"start\":21415},{\"end\":21625,\"start\":21616},{\"end\":21808,\"start\":21799},{\"end\":23400,\"start\":23392},{\"end\":23465,\"start\":23458},{\"end\":24941,\"start\":24933},{\"end\":28273,\"start\":28265},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33809,\"start\":33801},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34428,\"start\":34419},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34595,\"start\":34586},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":37122,\"start\":37114},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":38886,\"start\":38878},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39551,\"start\":39543},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":42728,\"start\":42720},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":43212,\"start\":43191},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":43455,\"start\":43444}]", "bib_author_first_name": "[{\"end\":55149,\"start\":55148},{\"end\":55365,\"start\":55364},{\"end\":55377,\"start\":55376},{\"end\":55387,\"start\":55386},{\"end\":55607,\"start\":55606},{\"end\":55621,\"start\":55620},{\"end\":55961,\"start\":55960},{\"end\":55969,\"start\":55968},{\"end\":55976,\"start\":55975},{\"end\":55984,\"start\":55983},{\"end\":55988,\"start\":55985},{\"end\":55998,\"start\":55997},{\"end\":56000,\"start\":55999},{\"end\":56008,\"start\":56007},{\"end\":56415,\"start\":56414},{\"end\":56423,\"start\":56422},{\"end\":56430,\"start\":56429},{\"end\":56438,\"start\":56437},{\"end\":56446,\"start\":56445},{\"end\":56452,\"start\":56451},{\"end\":56753,\"start\":56752},{\"end\":56761,\"start\":56760},{\"end\":56768,\"start\":56767},{\"end\":56776,\"start\":56775},{\"end\":56782,\"start\":56781},{\"end\":57124,\"start\":57123},{\"end\":57126,\"start\":57125},{\"end\":57139,\"start\":57138},{\"end\":57146,\"start\":57145},{\"end\":57154,\"start\":57153},{\"end\":57162,\"start\":57161},{\"end\":57537,\"start\":57536},{\"end\":57548,\"start\":57547},{\"end\":57561,\"start\":57560},{\"end\":57573,\"start\":57572},{\"end\":57587,\"start\":57586},{\"end\":57600,\"start\":57599},{\"end\":57610,\"start\":57609},{\"end\":57867,\"start\":57866},{\"end\":57875,\"start\":57874},{\"end\":57886,\"start\":57885},{\"end\":58128,\"start\":58127},{\"end\":58135,\"start\":58134},{\"end\":58253,\"start\":58252},{\"end\":58260,\"start\":58259},{\"end\":58578,\"start\":58577},{\"end\":58585,\"start\":58584},{\"end\":58594,\"start\":58593},{\"end\":58601,\"start\":58600},{\"end\":58609,\"start\":58608},{\"end\":58616,\"start\":58615},{\"end\":58834,\"start\":58833},{\"end\":58841,\"start\":58840},{\"end\":58849,\"start\":58848},{\"end\":58855,\"start\":58854},{\"end\":59038,\"start\":59037},{\"end\":59049,\"start\":59048},{\"end\":59051,\"start\":59050},{\"end\":59061,\"start\":59060},{\"end\":59063,\"start\":59062},{\"end\":59211,\"start\":59210},{\"end\":59221,\"start\":59220},{\"end\":59456,\"start\":59455},{\"end\":59467,\"start\":59466},{\"end\":59686,\"start\":59685},{\"end\":59698,\"start\":59697},{\"end\":59706,\"start\":59705},{\"end\":59972,\"start\":59971},{\"end\":59974,\"start\":59973},{\"end\":59985,\"start\":59984},{\"end\":59987,\"start\":59986},{\"end\":59997,\"start\":59996},{\"end\":60180,\"start\":60179},{\"end\":60189,\"start\":60188},{\"end\":60195,\"start\":60194},{\"end\":60532,\"start\":60531},{\"end\":60541,\"start\":60540},{\"end\":60549,\"start\":60548},{\"end\":60557,\"start\":60556},{\"end\":60753,\"start\":60752},{\"end\":60762,\"start\":60761},{\"end\":60768,\"start\":60767},{\"end\":60777,\"start\":60776},{\"end\":60784,\"start\":60783},{\"end\":60790,\"start\":60789},{\"end\":60792,\"start\":60791},{\"end\":61235,\"start\":61234},{\"end\":61237,\"start\":61236},{\"end\":61460,\"start\":61459},{\"end\":61472,\"start\":61471},{\"end\":61482,\"start\":61481},{\"end\":61735,\"start\":61734},{\"end\":61737,\"start\":61736},{\"end\":61745,\"start\":61744},{\"end\":61754,\"start\":61753},{\"end\":61756,\"start\":61755},{\"end\":61961,\"start\":61960},{\"end\":61963,\"start\":61962},{\"end\":61973,\"start\":61972},{\"end\":62094,\"start\":62093},{\"end\":62096,\"start\":62095},{\"end\":62104,\"start\":62103},{\"end\":62328,\"start\":62327},{\"end\":62330,\"start\":62329},{\"end\":62338,\"start\":62337},{\"end\":62522,\"start\":62521},{\"end\":62533,\"start\":62532},{\"end\":62535,\"start\":62534},{\"end\":62545,\"start\":62544},{\"end\":62825,\"start\":62824},{\"end\":62827,\"start\":62826},{\"end\":62839,\"start\":62838},{\"end\":62841,\"start\":62840},{\"end\":63150,\"start\":63149},{\"end\":63156,\"start\":63155},{\"end\":63162,\"start\":63161},{\"end\":63170,\"start\":63169},{\"end\":63636,\"start\":63635},{\"end\":63642,\"start\":63641},{\"end\":63649,\"start\":63648},{\"end\":63658,\"start\":63657},{\"end\":64002,\"start\":64001},{\"end\":64010,\"start\":64009},{\"end\":64016,\"start\":64015},{\"end\":64025,\"start\":64024},{\"end\":64027,\"start\":64026},{\"end\":64164,\"start\":64163},{\"end\":64173,\"start\":64172},{\"end\":64175,\"start\":64174},{\"end\":64183,\"start\":64182},{\"end\":64427,\"start\":64426},{\"end\":64437,\"start\":64436},{\"end\":64447,\"start\":64446},{\"end\":64457,\"start\":64456},{\"end\":64466,\"start\":64465},{\"end\":64785,\"start\":64784},{\"end\":64787,\"start\":64786},{\"end\":64797,\"start\":64796},{\"end\":64994,\"start\":64993},{\"end\":65001,\"start\":65000},{\"end\":65007,\"start\":65006},{\"end\":65015,\"start\":65014},{\"end\":65024,\"start\":65023},{\"end\":65031,\"start\":65030},{\"end\":65242,\"start\":65241},{\"end\":65249,\"start\":65248},{\"end\":65255,\"start\":65254},{\"end\":65262,\"start\":65261},{\"end\":65271,\"start\":65270},{\"end\":65455,\"start\":65454},{\"end\":65462,\"start\":65461},{\"end\":65469,\"start\":65468},{\"end\":65473,\"start\":65470},{\"end\":65482,\"start\":65481},{\"end\":65489,\"start\":65488},{\"end\":65689,\"start\":65688},{\"end\":65700,\"start\":65699},{\"end\":65710,\"start\":65709},{\"end\":65730,\"start\":65729},{\"end\":65965,\"start\":65964},{\"end\":65976,\"start\":65975},{\"end\":65987,\"start\":65986},{\"end\":66171,\"start\":66170},{\"end\":66181,\"start\":66180},{\"end\":66191,\"start\":66190},{\"end\":66545,\"start\":66544},{\"end\":66547,\"start\":66546},{\"end\":66558,\"start\":66557},{\"end\":66560,\"start\":66559},{\"end\":66572,\"start\":66571},{\"end\":66574,\"start\":66573},{\"end\":66882,\"start\":66881},{\"end\":66890,\"start\":66889},{\"end\":66899,\"start\":66898},{\"end\":67068,\"start\":67067},{\"end\":67078,\"start\":67077},{\"end\":67302,\"start\":67301},{\"end\":67304,\"start\":67303},{\"end\":67313,\"start\":67312},{\"end\":67315,\"start\":67314},{\"end\":67326,\"start\":67325},{\"end\":67330,\"start\":67327},{\"end\":67663,\"start\":67662},{\"end\":67820,\"start\":67819},{\"end\":67834,\"start\":67833},{\"end\":67846,\"start\":67845},{\"end\":67858,\"start\":67857},{\"end\":67868,\"start\":67867},{\"end\":67875,\"start\":67874},{\"end\":68115,\"start\":68114},{\"end\":68134,\"start\":68133},{\"end\":68143,\"start\":68142},{\"end\":68409,\"start\":68408},{\"end\":68417,\"start\":68416},{\"end\":68423,\"start\":68422},{\"end\":68430,\"start\":68429},{\"end\":68436,\"start\":68435},{\"end\":68622,\"start\":68621},{\"end\":68628,\"start\":68627},{\"end\":68636,\"start\":68635},{\"end\":68643,\"start\":68642},{\"end\":68650,\"start\":68649},{\"end\":68919,\"start\":68918},{\"end\":68925,\"start\":68924},{\"end\":68932,\"start\":68931},{\"end\":69139,\"start\":69138},{\"end\":69147,\"start\":69146},{\"end\":69154,\"start\":69153},{\"end\":69527,\"start\":69526},{\"end\":69535,\"start\":69534},{\"end\":69542,\"start\":69541},{\"end\":69548,\"start\":69547},{\"end\":69767,\"start\":69766},{\"end\":69775,\"start\":69774},{\"end\":69782,\"start\":69781},{\"end\":69791,\"start\":69790},{\"end\":69799,\"start\":69798},{\"end\":69807,\"start\":69806},{\"end\":70119,\"start\":70118},{\"end\":70127,\"start\":70126},{\"end\":70462,\"start\":70461},{\"end\":70470,\"start\":70469},{\"end\":70477,\"start\":70476},{\"end\":70484,\"start\":70483},{\"end\":70492,\"start\":70491},{\"end\":70689,\"start\":70688},{\"end\":70698,\"start\":70697},{\"end\":70706,\"start\":70705},{\"end\":70712,\"start\":70711},{\"end\":70720,\"start\":70719},{\"end\":70726,\"start\":70725},{\"end\":70735,\"start\":70734},{\"end\":70744,\"start\":70743},{\"end\":71106,\"start\":71105},{\"end\":71114,\"start\":71113},{\"end\":71121,\"start\":71120},{\"end\":71130,\"start\":71129},{\"end\":71138,\"start\":71137}]", "bib_author_last_name": "[{\"end\":55154,\"start\":55150},{\"end\":55374,\"start\":55366},{\"end\":55384,\"start\":55378},{\"end\":55393,\"start\":55388},{\"end\":55618,\"start\":55608},{\"end\":55631,\"start\":55622},{\"end\":55966,\"start\":55962},{\"end\":55973,\"start\":55970},{\"end\":55981,\"start\":55977},{\"end\":55995,\"start\":55989},{\"end\":56005,\"start\":56001},{\"end\":56011,\"start\":56009},{\"end\":56420,\"start\":56416},{\"end\":56427,\"start\":56424},{\"end\":56435,\"start\":56431},{\"end\":56443,\"start\":56439},{\"end\":56449,\"start\":56447},{\"end\":56455,\"start\":56453},{\"end\":56758,\"start\":56754},{\"end\":56765,\"start\":56762},{\"end\":56773,\"start\":56769},{\"end\":56779,\"start\":56777},{\"end\":56787,\"start\":56783},{\"end\":57136,\"start\":57127},{\"end\":57143,\"start\":57140},{\"end\":57151,\"start\":57147},{\"end\":57159,\"start\":57155},{\"end\":57167,\"start\":57163},{\"end\":57545,\"start\":57538},{\"end\":57558,\"start\":57549},{\"end\":57570,\"start\":57562},{\"end\":57584,\"start\":57574},{\"end\":57597,\"start\":57588},{\"end\":57607,\"start\":57601},{\"end\":57620,\"start\":57611},{\"end\":57872,\"start\":57868},{\"end\":57883,\"start\":57876},{\"end\":57892,\"start\":57887},{\"end\":58132,\"start\":58129},{\"end\":58141,\"start\":58136},{\"end\":58257,\"start\":58254},{\"end\":58263,\"start\":58261},{\"end\":58582,\"start\":58579},{\"end\":58591,\"start\":58586},{\"end\":58598,\"start\":58595},{\"end\":58606,\"start\":58602},{\"end\":58613,\"start\":58610},{\"end\":58620,\"start\":58617},{\"end\":58838,\"start\":58835},{\"end\":58846,\"start\":58842},{\"end\":58852,\"start\":58850},{\"end\":58860,\"start\":58856},{\"end\":59046,\"start\":59039},{\"end\":59058,\"start\":59052},{\"end\":59068,\"start\":59064},{\"end\":59218,\"start\":59212},{\"end\":59230,\"start\":59222},{\"end\":59464,\"start\":59457},{\"end\":59479,\"start\":59468},{\"end\":59695,\"start\":59687},{\"end\":59703,\"start\":59699},{\"end\":59715,\"start\":59707},{\"end\":59982,\"start\":59975},{\"end\":59994,\"start\":59988},{\"end\":60007,\"start\":59998},{\"end\":60186,\"start\":60181},{\"end\":60192,\"start\":60190},{\"end\":60198,\"start\":60196},{\"end\":60538,\"start\":60533},{\"end\":60546,\"start\":60542},{\"end\":60554,\"start\":60550},{\"end\":60560,\"start\":60558},{\"end\":60759,\"start\":60754},{\"end\":60765,\"start\":60763},{\"end\":60774,\"start\":60769},{\"end\":60781,\"start\":60778},{\"end\":60787,\"start\":60785},{\"end\":60795,\"start\":60793},{\"end\":61243,\"start\":61238},{\"end\":61469,\"start\":61461},{\"end\":61479,\"start\":61473},{\"end\":61489,\"start\":61483},{\"end\":61742,\"start\":61738},{\"end\":61751,\"start\":61746},{\"end\":61760,\"start\":61757},{\"end\":61970,\"start\":61964},{\"end\":61981,\"start\":61974},{\"end\":62101,\"start\":62097},{\"end\":62112,\"start\":62105},{\"end\":62335,\"start\":62331},{\"end\":62346,\"start\":62339},{\"end\":62530,\"start\":62523},{\"end\":62542,\"start\":62536},{\"end\":62550,\"start\":62546},{\"end\":62836,\"start\":62828},{\"end\":62854,\"start\":62842},{\"end\":63153,\"start\":63151},{\"end\":63159,\"start\":63157},{\"end\":63167,\"start\":63163},{\"end\":63174,\"start\":63171},{\"end\":63639,\"start\":63637},{\"end\":63646,\"start\":63643},{\"end\":63655,\"start\":63650},{\"end\":63664,\"start\":63659},{\"end\":64007,\"start\":64003},{\"end\":64013,\"start\":64011},{\"end\":64022,\"start\":64017},{\"end\":64032,\"start\":64028},{\"end\":64170,\"start\":64165},{\"end\":64180,\"start\":64176},{\"end\":64187,\"start\":64184},{\"end\":64434,\"start\":64428},{\"end\":64444,\"start\":64438},{\"end\":64454,\"start\":64448},{\"end\":64463,\"start\":64458},{\"end\":64470,\"start\":64467},{\"end\":64794,\"start\":64788},{\"end\":64805,\"start\":64798},{\"end\":64998,\"start\":64995},{\"end\":65004,\"start\":65002},{\"end\":65012,\"start\":65008},{\"end\":65021,\"start\":65016},{\"end\":65028,\"start\":65025},{\"end\":65037,\"start\":65032},{\"end\":65246,\"start\":65243},{\"end\":65252,\"start\":65250},{\"end\":65259,\"start\":65256},{\"end\":65268,\"start\":65263},{\"end\":65276,\"start\":65272},{\"end\":65459,\"start\":65456},{\"end\":65466,\"start\":65463},{\"end\":65479,\"start\":65474},{\"end\":65486,\"start\":65483},{\"end\":65494,\"start\":65490},{\"end\":65697,\"start\":65690},{\"end\":65707,\"start\":65701},{\"end\":65727,\"start\":65711},{\"end\":65737,\"start\":65731},{\"end\":65973,\"start\":65966},{\"end\":65984,\"start\":65977},{\"end\":65994,\"start\":65988},{\"end\":66178,\"start\":66172},{\"end\":66188,\"start\":66182},{\"end\":66196,\"start\":66192},{\"end\":66555,\"start\":66548},{\"end\":66569,\"start\":66561},{\"end\":66585,\"start\":66575},{\"end\":66887,\"start\":66883},{\"end\":66896,\"start\":66891},{\"end\":66913,\"start\":66900},{\"end\":67075,\"start\":67069},{\"end\":67086,\"start\":67079},{\"end\":67310,\"start\":67305},{\"end\":67323,\"start\":67316},{\"end\":67338,\"start\":67331},{\"end\":67678,\"start\":67664},{\"end\":67831,\"start\":67821},{\"end\":67843,\"start\":67835},{\"end\":67855,\"start\":67847},{\"end\":67865,\"start\":67859},{\"end\":67872,\"start\":67869},{\"end\":67882,\"start\":67876},{\"end\":68131,\"start\":68116},{\"end\":68140,\"start\":68135},{\"end\":68150,\"start\":68144},{\"end\":68414,\"start\":68410},{\"end\":68420,\"start\":68418},{\"end\":68427,\"start\":68424},{\"end\":68433,\"start\":68431},{\"end\":68441,\"start\":68437},{\"end\":68625,\"start\":68623},{\"end\":68633,\"start\":68629},{\"end\":68640,\"start\":68637},{\"end\":68647,\"start\":68644},{\"end\":68656,\"start\":68651},{\"end\":68922,\"start\":68920},{\"end\":68929,\"start\":68926},{\"end\":68937,\"start\":68933},{\"end\":69144,\"start\":69140},{\"end\":69151,\"start\":69148},{\"end\":69158,\"start\":69155},{\"end\":69532,\"start\":69528},{\"end\":69539,\"start\":69536},{\"end\":69545,\"start\":69543},{\"end\":69553,\"start\":69549},{\"end\":69772,\"start\":69768},{\"end\":69779,\"start\":69776},{\"end\":69788,\"start\":69783},{\"end\":69796,\"start\":69792},{\"end\":69804,\"start\":69800},{\"end\":69813,\"start\":69808},{\"end\":70124,\"start\":70120},{\"end\":70132,\"start\":70128},{\"end\":70467,\"start\":70463},{\"end\":70474,\"start\":70471},{\"end\":70481,\"start\":70478},{\"end\":70489,\"start\":70485},{\"end\":70496,\"start\":70493},{\"end\":70695,\"start\":70690},{\"end\":70703,\"start\":70699},{\"end\":70709,\"start\":70707},{\"end\":70717,\"start\":70713},{\"end\":70723,\"start\":70721},{\"end\":70732,\"start\":70727},{\"end\":70741,\"start\":70736},{\"end\":70749,\"start\":70745},{\"end\":71111,\"start\":71107},{\"end\":71118,\"start\":71115},{\"end\":71127,\"start\":71122},{\"end\":71135,\"start\":71131},{\"end\":71144,\"start\":71139}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9134861},\"end\":55304,\"start\":55078},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15625952},\"end\":55524,\"start\":55306},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4630420},\"end\":55863,\"start\":55526},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":174818771},\"end\":56341,\"start\":55865},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":216438471},\"end\":56694,\"start\":56343},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":174819616},\"end\":57058,\"start\":56696},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":221839771},\"end\":57470,\"start\":57060},{\"attributes\":{\"id\":\"b7\"},\"end\":57821,\"start\":57472},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":49416020},\"end\":58090,\"start\":57823},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":51610030},\"end\":58236,\"start\":58092},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":153311899},\"end\":58498,\"start\":58238},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":840252},\"end\":58796,\"start\":58500},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49645902},\"end\":58970,\"start\":58798},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1723648},\"end\":59208,\"start\":58972},{\"attributes\":{\"id\":\"b14\"},\"end\":59375,\"start\":59210},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":47399},\"end\":59632,\"start\":59377},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4755450},\"end\":59932,\"start\":59634},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":34098453},\"end\":60135,\"start\":59934},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":19908368},\"end\":60484,\"start\":60137},{\"attributes\":{\"id\":\"b19\"},\"end\":60665,\"start\":60486},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10466935},\"end\":61164,\"start\":60667},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":393162},\"end\":61381,\"start\":61166},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52883753},\"end\":61658,\"start\":61383},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":24703597},\"end\":61925,\"start\":61660},{\"attributes\":{\"id\":\"b24\"},\"end\":62058,\"start\":61927},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14249137},\"end\":62259,\"start\":62060},{\"attributes\":{\"id\":\"b26\"},\"end\":62450,\"start\":62261},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":195069083},\"end\":62784,\"start\":62452},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":263869323},\"end\":63089,\"start\":62786},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":11520899},\"end\":63568,\"start\":63091},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":22049936},\"end\":63962,\"start\":63570},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206744635},\"end\":64161,\"start\":63964},{\"attributes\":{\"id\":\"b32\"},\"end\":64363,\"start\":64163},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15384610},\"end\":64735,\"start\":64365},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2178841},\"end\":64926,\"start\":64737},{\"attributes\":{\"id\":\"b35\"},\"end\":65200,\"start\":64928},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":943660},\"end\":65402,\"start\":65202},{\"attributes\":{\"id\":\"b37\"},\"end\":65617,\"start\":65404},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5849893},\"end\":65909,\"start\":65619},{\"attributes\":{\"id\":\"b39\"},\"end\":66117,\"start\":65911},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":421281},\"end\":66542,\"start\":66119},{\"attributes\":{\"id\":\"b41\"},\"end\":66806,\"start\":66544},{\"attributes\":{\"id\":\"b42\"},\"end\":67065,\"start\":66808},{\"attributes\":{\"doi\":\"arXiv:1905.10715\",\"id\":\"b43\"},\"end\":67214,\"start\":67067},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":59232387},\"end\":67612,\"start\":67216},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":14618636},\"end\":67817,\"start\":67614},{\"attributes\":{\"id\":\"b46\"},\"end\":68030,\"start\":67819},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":145971821},\"end\":68360,\"start\":68032},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":228906587},\"end\":68589,\"start\":68362},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":59279266},\"end\":68852,\"start\":68591},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":2237682},\"end\":69066,\"start\":68854},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":20166198},\"end\":69439,\"start\":69068},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":21046929},\"end\":69724,\"start\":69441},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":59525833},\"end\":70066,\"start\":69726},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":54436622},\"end\":70368,\"start\":70068},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":14057513},\"end\":70686,\"start\":70370},{\"attributes\":{\"id\":\"b56\"},\"end\":71022,\"start\":70688},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":75139679},\"end\":71333,\"start\":71024}]", "bib_title": "[{\"end\":55146,\"start\":55078},{\"end\":55362,\"start\":55306},{\"end\":55604,\"start\":55526},{\"end\":55958,\"start\":55865},{\"end\":56412,\"start\":56343},{\"end\":56750,\"start\":56696},{\"end\":57121,\"start\":57060},{\"end\":57864,\"start\":57823},{\"end\":58125,\"start\":58092},{\"end\":58250,\"start\":58238},{\"end\":58575,\"start\":58500},{\"end\":58831,\"start\":58798},{\"end\":59035,\"start\":58972},{\"end\":59453,\"start\":59377},{\"end\":59683,\"start\":59634},{\"end\":59969,\"start\":59934},{\"end\":60177,\"start\":60137},{\"end\":60750,\"start\":60667},{\"end\":61232,\"start\":61166},{\"end\":61457,\"start\":61383},{\"end\":61732,\"start\":61660},{\"end\":62091,\"start\":62060},{\"end\":62519,\"start\":62452},{\"end\":62822,\"start\":62786},{\"end\":63147,\"start\":63091},{\"end\":63633,\"start\":63570},{\"end\":63999,\"start\":63964},{\"end\":64424,\"start\":64365},{\"end\":64782,\"start\":64737},{\"end\":64991,\"start\":64928},{\"end\":65239,\"start\":65202},{\"end\":65686,\"start\":65619},{\"end\":66168,\"start\":66119},{\"end\":67299,\"start\":67216},{\"end\":67660,\"start\":67614},{\"end\":68112,\"start\":68032},{\"end\":68406,\"start\":68362},{\"end\":68619,\"start\":68591},{\"end\":68916,\"start\":68854},{\"end\":69136,\"start\":69068},{\"end\":69524,\"start\":69441},{\"end\":69764,\"start\":69726},{\"end\":70116,\"start\":70068},{\"end\":70459,\"start\":70370},{\"end\":71103,\"start\":71024}]", "bib_author": "[{\"end\":55156,\"start\":55148},{\"end\":55376,\"start\":55364},{\"end\":55386,\"start\":55376},{\"end\":55395,\"start\":55386},{\"end\":55620,\"start\":55606},{\"end\":55633,\"start\":55620},{\"end\":55968,\"start\":55960},{\"end\":55975,\"start\":55968},{\"end\":55983,\"start\":55975},{\"end\":55997,\"start\":55983},{\"end\":56007,\"start\":55997},{\"end\":56013,\"start\":56007},{\"end\":56422,\"start\":56414},{\"end\":56429,\"start\":56422},{\"end\":56437,\"start\":56429},{\"end\":56445,\"start\":56437},{\"end\":56451,\"start\":56445},{\"end\":56457,\"start\":56451},{\"end\":56760,\"start\":56752},{\"end\":56767,\"start\":56760},{\"end\":56775,\"start\":56767},{\"end\":56781,\"start\":56775},{\"end\":56789,\"start\":56781},{\"end\":57138,\"start\":57123},{\"end\":57145,\"start\":57138},{\"end\":57153,\"start\":57145},{\"end\":57161,\"start\":57153},{\"end\":57169,\"start\":57161},{\"end\":57547,\"start\":57536},{\"end\":57560,\"start\":57547},{\"end\":57572,\"start\":57560},{\"end\":57586,\"start\":57572},{\"end\":57599,\"start\":57586},{\"end\":57609,\"start\":57599},{\"end\":57622,\"start\":57609},{\"end\":57874,\"start\":57866},{\"end\":57885,\"start\":57874},{\"end\":57894,\"start\":57885},{\"end\":58134,\"start\":58127},{\"end\":58143,\"start\":58134},{\"end\":58259,\"start\":58252},{\"end\":58265,\"start\":58259},{\"end\":58584,\"start\":58577},{\"end\":58593,\"start\":58584},{\"end\":58600,\"start\":58593},{\"end\":58608,\"start\":58600},{\"end\":58615,\"start\":58608},{\"end\":58622,\"start\":58615},{\"end\":58840,\"start\":58833},{\"end\":58848,\"start\":58840},{\"end\":58854,\"start\":58848},{\"end\":58862,\"start\":58854},{\"end\":59048,\"start\":59037},{\"end\":59060,\"start\":59048},{\"end\":59070,\"start\":59060},{\"end\":59220,\"start\":59210},{\"end\":59232,\"start\":59220},{\"end\":59466,\"start\":59455},{\"end\":59481,\"start\":59466},{\"end\":59697,\"start\":59685},{\"end\":59705,\"start\":59697},{\"end\":59717,\"start\":59705},{\"end\":59984,\"start\":59971},{\"end\":59996,\"start\":59984},{\"end\":60009,\"start\":59996},{\"end\":60188,\"start\":60179},{\"end\":60194,\"start\":60188},{\"end\":60200,\"start\":60194},{\"end\":60540,\"start\":60531},{\"end\":60548,\"start\":60540},{\"end\":60556,\"start\":60548},{\"end\":60562,\"start\":60556},{\"end\":60761,\"start\":60752},{\"end\":60767,\"start\":60761},{\"end\":60776,\"start\":60767},{\"end\":60783,\"start\":60776},{\"end\":60789,\"start\":60783},{\"end\":60797,\"start\":60789},{\"end\":61245,\"start\":61234},{\"end\":61471,\"start\":61459},{\"end\":61481,\"start\":61471},{\"end\":61491,\"start\":61481},{\"end\":61744,\"start\":61734},{\"end\":61753,\"start\":61744},{\"end\":61762,\"start\":61753},{\"end\":61972,\"start\":61960},{\"end\":61983,\"start\":61972},{\"end\":62103,\"start\":62093},{\"end\":62114,\"start\":62103},{\"end\":62337,\"start\":62327},{\"end\":62348,\"start\":62337},{\"end\":62532,\"start\":62521},{\"end\":62544,\"start\":62532},{\"end\":62552,\"start\":62544},{\"end\":62838,\"start\":62824},{\"end\":62856,\"start\":62838},{\"end\":63155,\"start\":63149},{\"end\":63161,\"start\":63155},{\"end\":63169,\"start\":63161},{\"end\":63176,\"start\":63169},{\"end\":63641,\"start\":63635},{\"end\":63648,\"start\":63641},{\"end\":63657,\"start\":63648},{\"end\":63666,\"start\":63657},{\"end\":64009,\"start\":64001},{\"end\":64015,\"start\":64009},{\"end\":64024,\"start\":64015},{\"end\":64034,\"start\":64024},{\"end\":64172,\"start\":64163},{\"end\":64182,\"start\":64172},{\"end\":64189,\"start\":64182},{\"end\":64436,\"start\":64426},{\"end\":64446,\"start\":64436},{\"end\":64456,\"start\":64446},{\"end\":64465,\"start\":64456},{\"end\":64472,\"start\":64465},{\"end\":64796,\"start\":64784},{\"end\":64807,\"start\":64796},{\"end\":65000,\"start\":64993},{\"end\":65006,\"start\":65000},{\"end\":65014,\"start\":65006},{\"end\":65023,\"start\":65014},{\"end\":65030,\"start\":65023},{\"end\":65039,\"start\":65030},{\"end\":65248,\"start\":65241},{\"end\":65254,\"start\":65248},{\"end\":65261,\"start\":65254},{\"end\":65270,\"start\":65261},{\"end\":65278,\"start\":65270},{\"end\":65461,\"start\":65454},{\"end\":65468,\"start\":65461},{\"end\":65481,\"start\":65468},{\"end\":65488,\"start\":65481},{\"end\":65496,\"start\":65488},{\"end\":65699,\"start\":65688},{\"end\":65709,\"start\":65699},{\"end\":65729,\"start\":65709},{\"end\":65739,\"start\":65729},{\"end\":65975,\"start\":65964},{\"end\":65986,\"start\":65975},{\"end\":65996,\"start\":65986},{\"end\":66180,\"start\":66170},{\"end\":66190,\"start\":66180},{\"end\":66198,\"start\":66190},{\"end\":66557,\"start\":66544},{\"end\":66571,\"start\":66557},{\"end\":66587,\"start\":66571},{\"end\":66889,\"start\":66881},{\"end\":66898,\"start\":66889},{\"end\":66915,\"start\":66898},{\"end\":67077,\"start\":67067},{\"end\":67088,\"start\":67077},{\"end\":67312,\"start\":67301},{\"end\":67325,\"start\":67312},{\"end\":67340,\"start\":67325},{\"end\":67680,\"start\":67662},{\"end\":67833,\"start\":67819},{\"end\":67845,\"start\":67833},{\"end\":67857,\"start\":67845},{\"end\":67867,\"start\":67857},{\"end\":67874,\"start\":67867},{\"end\":67884,\"start\":67874},{\"end\":68133,\"start\":68114},{\"end\":68142,\"start\":68133},{\"end\":68152,\"start\":68142},{\"end\":68416,\"start\":68408},{\"end\":68422,\"start\":68416},{\"end\":68429,\"start\":68422},{\"end\":68435,\"start\":68429},{\"end\":68443,\"start\":68435},{\"end\":68627,\"start\":68621},{\"end\":68635,\"start\":68627},{\"end\":68642,\"start\":68635},{\"end\":68649,\"start\":68642},{\"end\":68658,\"start\":68649},{\"end\":68924,\"start\":68918},{\"end\":68931,\"start\":68924},{\"end\":68939,\"start\":68931},{\"end\":69146,\"start\":69138},{\"end\":69153,\"start\":69146},{\"end\":69160,\"start\":69153},{\"end\":69534,\"start\":69526},{\"end\":69541,\"start\":69534},{\"end\":69547,\"start\":69541},{\"end\":69555,\"start\":69547},{\"end\":69774,\"start\":69766},{\"end\":69781,\"start\":69774},{\"end\":69790,\"start\":69781},{\"end\":69798,\"start\":69790},{\"end\":69806,\"start\":69798},{\"end\":69815,\"start\":69806},{\"end\":70126,\"start\":70118},{\"end\":70134,\"start\":70126},{\"end\":70469,\"start\":70461},{\"end\":70476,\"start\":70469},{\"end\":70483,\"start\":70476},{\"end\":70491,\"start\":70483},{\"end\":70498,\"start\":70491},{\"end\":70697,\"start\":70688},{\"end\":70705,\"start\":70697},{\"end\":70711,\"start\":70705},{\"end\":70719,\"start\":70711},{\"end\":70725,\"start\":70719},{\"end\":70734,\"start\":70725},{\"end\":70743,\"start\":70734},{\"end\":70751,\"start\":70743},{\"end\":71113,\"start\":71105},{\"end\":71120,\"start\":71113},{\"end\":71129,\"start\":71120},{\"end\":71137,\"start\":71129},{\"end\":71146,\"start\":71137}]", "bib_venue": "[{\"end\":58388,\"start\":58335},{\"end\":60323,\"start\":60270},{\"end\":60920,\"start\":60867},{\"end\":63353,\"start\":63273},{\"end\":63775,\"start\":63729},{\"end\":66349,\"start\":66282},{\"end\":55175,\"start\":55156},{\"end\":55398,\"start\":55395},{\"end\":55685,\"start\":55633},{\"end\":56079,\"start\":56013},{\"end\":56508,\"start\":56457},{\"end\":56855,\"start\":56789},{\"end\":57239,\"start\":57169},{\"end\":57534,\"start\":57472},{\"end\":57946,\"start\":57894},{\"end\":58148,\"start\":58143},{\"end\":58333,\"start\":58265},{\"end\":58625,\"start\":58622},{\"end\":58867,\"start\":58862},{\"end\":59073,\"start\":59070},{\"end\":59280,\"start\":59232},{\"end\":59485,\"start\":59481},{\"end\":59766,\"start\":59717},{\"end\":60018,\"start\":60009},{\"end\":60268,\"start\":60200},{\"end\":60529,\"start\":60486},{\"end\":60865,\"start\":60797},{\"end\":61257,\"start\":61245},{\"end\":61507,\"start\":61491},{\"end\":61776,\"start\":61762},{\"end\":61958,\"start\":61927},{\"end\":62153,\"start\":62114},{\"end\":62325,\"start\":62261},{\"end\":62601,\"start\":62552},{\"end\":62917,\"start\":62856},{\"end\":63271,\"start\":63176},{\"end\":63727,\"start\":63666},{\"end\":64043,\"start\":64034},{\"end\":64248,\"start\":64189},{\"end\":64534,\"start\":64472},{\"end\":64818,\"start\":64807},{\"end\":65044,\"start\":65039},{\"end\":65283,\"start\":65278},{\"end\":65452,\"start\":65404},{\"end\":65742,\"start\":65739},{\"end\":65962,\"start\":65911},{\"end\":66280,\"start\":66198},{\"end\":66661,\"start\":66587},{\"end\":66879,\"start\":66808},{\"end\":67133,\"start\":67104},{\"end\":67397,\"start\":67340},{\"end\":67699,\"start\":67680},{\"end\":67914,\"start\":67884},{\"end\":68179,\"start\":68152},{\"end\":68457,\"start\":68443},{\"end\":68710,\"start\":68658},{\"end\":68944,\"start\":68939},{\"end\":69237,\"start\":69160},{\"end\":69564,\"start\":69555},{\"end\":69871,\"start\":69815},{\"end\":70198,\"start\":70134},{\"end\":70509,\"start\":70498},{\"end\":70835,\"start\":70751},{\"end\":71160,\"start\":71146}]"}}}, "year": 2023, "month": 12, "day": 17}