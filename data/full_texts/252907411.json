{"id": 252907411, "updated": "2023-10-05 09:52:34.138", "metadata": {"title": "SQA3D: Situated Question Answering in 3D Scenes", "authors": "[{\"first\":\"Xiaojian\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Silong\",\"last\":\"Yong\",\"middle\":[]},{\"first\":\"Zilong\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Qing\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yitao\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Song-Chun\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Siyuan\",\"last\":\"Huang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capability.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2210.07474", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/MaYZ0LZH23", "doi": "10.48550/arxiv.2210.07474"}}, "content": {"source": {"pdf_hash": "c1a4fb211cf995b1af1247a152fcc145594ec13b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.07474v5.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9235ad617a2caceaa072a0bcf6c90ba6f2fefecd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c1a4fb211cf995b1af1247a152fcc145594ec13b.txt", "contents": "\nPublished as a conference paper at ICLR 2023 SQA3D: SITUATED QUESTION ANSWERING IN 3D SCENES\n\n\nXiaojian Ma xiaojian.ma@ucla.edu \nUCLA\n\n\nSilong Yong \nBeijing Institute for General Artificial Intelligence (BIGAI)\n\n\nTsinghua University\n\n\nZilong Zheng \nBeijing Institute for General Artificial Intelligence (BIGAI)\n\n\nQing Li \nBeijing Institute for General Artificial Intelligence (BIGAI)\n\n\nYitao Liang yitaol@pku.edu.cn \nBeijing Institute for General Artificial Intelligence (BIGAI)\n\n\nPeking University\n\n\nSong-Chun Zhu \nBeijing Institute for General Artificial Intelligence (BIGAI)\n\n\nUCLA\n\n\nTsinghua University\n\n\nPeking University\n\n\nSiyuan Huang syhuang@bigai.ai \nBeijing Institute for General Artificial Intelligence (BIGAI)\n\n\nPublished as a conference paper at ICLR 2023 SQA3D: SITUATED QUESTION ANSWERING IN 3D SCENES\n\nDescriptionSitting at the edge of the bed and facing the couch. Question q : Can I go straight to the coffee table in front of me? Scene context : 3D scan, egocentric video, birdeye view (BEV) picture, etc. Answer : No Location (optional): t t+1ABSTRACT We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capabilities. Code and data are released at sqa3d.github.io.\n\n. Given scene context S (e.g., 3D scan, egocentric video, bird-eye view picture), SQA3D requires an agent to first comprehend and localize its situation (position, orientation, etc.) in the 3D scene from a textual description s txt , then answer a question q under that situation. Note that understanding the situation and imagining the corresponding egocentric view correctly is necessary to accomplish our task. We provide more example questions in Figure 2.\n\n\nINTRODUCTION\n\nIn recent years, the endeavor of building intelligent embodied agents has delivered fruitful achievements. Robots now can navigate (Anderson et al., 2018) and manipulate objects (Liang et al., 2019;Savva et al., 2019;Shridhar et al., 2022;Ahn et al., 2022) following natural language commands Figure 2: Examples from SQA3D. We provide some example questions and the corresponding situations (s txt and ) and 3D scenes. The categories listed here do not mean to be exhaustive and a question could fall into multiple categories. The green boxes indicate relevant objects in situation description s txt while red boxes are for the questions q. : One or dialogues. Albeit these promising advances, their actual performances in real-world embodied environments could still fall short of human expectations, especially in generalization to different situations (scenes and locations) and tasks that require substantial, knowledge-intensive reasoning.\n\nTo diagnose the fundamental capability of realistic embodied agents, we investigate the problem of embodied scene understanding, where the agent needs to understand its situation and the surroundings in the environment from a dynamic egocentric view, then perceive, reason, and act accordingly, to accomplish complex tasks.\n\nWhat is at the core of embodied scene understanding? Drawing inspirations from situated cognition (Greeno, 1998;Anderson et al., 2000), a seminal theory of embodiment, we anticipate it to be two-fold:\n\n\u2022 Situation understanding. The ability to imagine what the agent will see from arbitrary situations (position, orientations, etc.) in a 3D scene and understand the surroundings anchored to the situation, therefore generalize to novel positions or scenes; \u2022 Situated reasoning. The ability to acquire knowledge about the environment based on the agents' current situation and reason with the knowledge, therefore further facilitates accomplishing complex action planning tasks.\n\nTo step towards embodied scene understanding, we introduce SQA3D, a new task that reconciles the best of both parties, situation understanding, and situated reasoning, into embodied 3D scene understanding. Figure 1 sketches our task: given a 3D scene context (e.g., 3D scan, ego-centric video, or bird-eye view (BEV) picture), the agent in the 3D scene needs to first comprehend and localize its situation (position, orientation, etc.) from a textual description, then answer a question that requires substantial situated reasoning from that perspective. We crowd-sourced the situation descriptions from Amazon MTurk (AMT), where participants are instructed to select diverse locations and orientations in 3D scenes. To systematically examine the agent's ability in situated reasoning, we collect questions that cover a wide spectrum of knowledge, ranging from spatial relations to navigation, common sense reasoning, and multi-hop reasoning. In total, SQA3D comprises 20.4k descriptions of 6.8k unique situations collected from 650 ScanNet scenes and 33.4k questions about these situations. Examples of SQA3D can be found Figure 2.\n\nOur task closely connects to the recent efforts on 3D language grounding Hong et al., 2021b;Achlioptas et al., 2020;Wang et al., 2022;Azuma et al., 2022). However, most of these avenues assume observations of a 3D scene are made from some third-person perspectives rather than an embodied, egocentric view, and they primarily inspect spatial understanding, while SQA3D examines scene understanding with a wide range of knowledge, and the problems have to be solved using an (imagined) first-person view. Embodied QA (Das et al., 2018;Wijmans et al., 2019a) draws very similar motivation as SQA3D, but our task adopts a simplified protocol (QA only) while still preserving the function of benchmarking embodied scene understanding, therefore allowing more complex, knowledge-intensive questions and a much larger scale of data collection. Comparisons with relevant tasks and benchmarks are listed in Table 1.\n\nBenchmarking existing baselines: In our experiments, we examine state-of-the-art multi-modal reasoning models, including ScanQA from Azuma et al. (2022) that leverages 3D scan data, Clip- In general, we consider semantic grounding, language-driven navigation, and question-answering in photorealistic 3D scenes. In the first row, situated indicates whether the benchmark task is supposed to be completed by a \"situated\" agent with its egocentric perspective. navigation, common sense, and multi-hop reasoning show whether the task requires a certain capability or knowledge level of 3D understanding. * Rather than observing a complete 3D scan of the scene, the learner needs to navigate in a simulator to perceive the 3D scene incrementally. ScanNet  seg. scan n/a 800 rooms 1.5k\n\nScanRefer  det. scan human 800 rooms 52k ReferIt3D (Achlioptas et al., 2020) det. scan human 707 rooms 41k\n\nScanQA (Azuma et al., 2022) q.a. scan template 800 rooms 41k 3D-QA (Ye et al., 2021) q.a. scan human 806 rooms 5.8k CLEVR3D (Yan et al., 2021) q BERT (Lei et al., 2021) and MCAN (Yu et al., 2019) that exploits egocentric videos and BEV pictures. However, the results unveil that both models still largely fall behind human performances by a large margin (47.2% of the best model vs. 90.06% of amateur human testers). To understand the failure modes, we conduct experiments on settings that could alleviate the challenges brought by situation understanding. The improvement of these models confirms that the current models are indeed struggling with situation understanding, which is pivotal for embodied scene understanding. Finally, we explore whether powerful Large Language Models (LLMs) like GPT-3 (Brown et al., 2020) and Unified QA (Khashabi et al., 2020) could tackle our tasks by converting the multi-modal SQA3D problems into single-modal surrogates using scene captioning. However, our results read that these models can still be bottlenecked by the lack of spatial understanding and accurate captions.\n\nOur contributions can be summarized as follow:\n\n\u2022 We introduce SQA3D, a new benchmark for embodied scene understanding, aiming at reconciling the challenging capabilities of situation understanding and situated reasoning and facilitating the development of intelligent embodied agents. \u2022 We meticulously curate the SQA3D to include diverse situations and interesting questions. These questions probe a wide spectrum of knowledge and reasoning abilities of embodied agents, ranging from spatial relation comprehension to navigation, common sense reasoning, and multi-hop reasoning. \u2022 We perform extensive analysis on the state-of-the-art multi-modal reasoning models. However, experimental results indicate that these avenues are still struggling on SQA3D. Our hypothesis suggests the crucial role of proper 3D representations and the demand for better situation understanding in embodied scene understanding.\n\n\nTHE SQA3D DATASET\n\nA problem instance in SQA3D can be formulated as a triplet S, s, q , where S denotes the scene context, e.g., 3D scan, egocentric video, bird-eye view (BEV) picture, etc.; s = s txt , s pos , s rot denotes a situation, where the textual situation description s txt (e.g., \"Sitting at the edge of the bed and facing the couch\" in Figure 1) depicts the position s pos and orientation s rot of an agent in the scene; Note that the agent is assumed to be first rotated according to s rot at the origin of the scene coordinate and then translated to s pos ; q denotes a question. The task is to retrieve the correct answer from the answer set a = {a 1 , . . . , a N }, while optionally predicting the ground truth location s pos , s rot from the text. The additional prediction of location could help alleviate the challenges brought by situation understanding. The following subsections will detail how to collect and curate the data and then build the benchmark.\n\n2.1 DATA FORMATION The 3D indoor scenes are selected from the ScanNet  dataset. We notice that some scenes could be too crowded/sparse, or overall tiny, making situations and questions collection infeasible. Therefore, we first manually categorize these scenes based on the richness of objects/layouts and the space volume. We end up retaining 650 scenes after dropping those that failed to meet\n\n\nI. Situation Identification\n\nParticipants are asked to pick and write description .\n\n\nII. Question Preparation\n\nParticipants are asked to write question q given the situation depicted in both and .\n\n\nIII. Answer Collection & Human Study\n\nMore participants are asked to answer question q given the situation depicted only in . txt s txt s txt s pos s rot s Figure 3: Data collection pipeline of SQA3D. Since our dataset comprises multiple types of annotations (situations and their descriptions, questions, answers, etc.), we found it more manageable to break down a single annotation task into three sub-tasks: i) Situation Identification; ii) Question Preparation; iii) Answer Collection & Human Study, where the participants recruited on AMT only need to focus on a relatively simple sub-task at a time. the requirement. We then develop an interactive web-based user interface (UI) to collect the data. Details of UI design can be found in appendix. All the participants are recruited on AMT.\n\nCompared to counterparts, the annotation load of a single SQA3D problem instance could be significantly heavier as participants need to explore the scene, pick a situation, make descriptions, and ask a few questions. All these steps also require dense interaction with the 3D scene. To ensure good quality, we introduce a multi-stage collection pipeline, which breaks down the load into more manageable sub-tasks. Figure 3 delineates this process:\n\nI. Situation Identification. We ask the workers to pick 5 situations by changing the location s pos , s rot of a virtual avatar in a ScanNet scene S. The workers are then instructed to write descriptions s txt that can uniquely depict these situations in the scene. We also use examples and bonuses to encourage more natural sentences and the use of human activities (e.g., \"I'm waiting for my lunch to be heated in front of the microwave\"). All the collected situations are later manually curated to ensure diversity and the least ambiguity. If necessary, we would augment the data with more situations to cover different areas of the scene.\n\nII. Question Preparation. We collect a set of questions w.r.t. each pair of the 3D scene S, and the situation description s txt (the virtual avatar is also rendered at s pos , s rot ). To help prepare questions that require substantial situated reasoning, we tutor the workers before granting them access to our tasks. They are instructed to follow the rules and learn from good examples. We also remove & penalize the responses that do not depend on the current situation, e.g. \"How many chairs are there in the room?\".\n\n\nIII. Answer Collection & Human Study.\n\nIn addition to the answers collected alongside the questions, we send out the questions to more workers and record their responses. These workers are provided with the same interface as in stage II except showing in the scene to ensure consistency between question and answer collection. There is also mandatory scene familiarization in all three steps before the main job starts and we find it extremely helpful especially for more crowded scenes. More details can be found in appendix.\n\n\nCURATION, DATA STATISTICS, AND METRICS\n\nCuration. Our multi-stage collection ends up with around 21k descriptions of 6.8k unique situations and 35k questions. Although the aforementioned prompt did yield many high-quality annotations, some of them are still subject to curation. We first apply a basic grammar check to clean up the language glitches. Then we follow the practices in VQAv2 (Goyal et al., 2017) and OK-VQA (Marino et al., 2019) to further eliminate low-effort descriptions and questions. Specifically, we eliminate & rewrite template-alike descriptions (e.g., repeating the same sentence patterns) and questions that are too simple or do not require looking at the scene. We also notice the similar answer bias reported in Marino et al. (2019) where some types of questions might bias toward certain answers. Therefore, we remove questions to ensure a more uniform answer distribution. A comparison of answer distribution before and after the balancing can be found in appendix. As a result, our final dataset comprises 20.4k descriptions and 33.4k diverse and challenging questions. Figure 2 demonstrates some example questions in SQA3D.\n\nStatistics. Compared to most counterparts with template-based text generation, SQA3D is crowdsourced on AMT and therefore enjoys more naturalness and better diversity. To the best of our   knowledge, SQA3D is the largest dataset of grounded 3D scene understanding with the humanannotated question-answering pairs (a comparison to the counterparts can be found in Table 1).  Figure 4, and Figure 5 illustrate the basic statistics of our dataset, including the word cloud of situation descriptions and question distribution based on their prefixes. It can be seen that descriptions overall meet our expectations as human activities like \"sitting\" and \"facing\" are among the most common words. Our questions are also more diverse and balanced than our counterparts, where those starting with \"What\" make up more than half of the questions and result in biased questions (Azuma et al., 2022). More statistics like distributions over answers and length of the text can be found in appendix.\n\nDataset splits and evaluation metric. We follow the practice of ScanNet and split SQA3D into train, val, and test sets. Since we cannot access the semantic annotations in ScanNet test set, we instead divide the ScanNet validation scenes into two subsets and use them as our val and test sets, respectively. The statistics of these splits can be found in Table 2. Following the protocol in VQAv2 (Goyal et al., 2017), we provide a set of 706 \"top-K\" answer candidates by excluding answers that only appear very few times. Subsequently, we adopt the \"exact match\" as our evaluation metric, i.e., the accuracy of answer classification in the test set. No further metric is included as we find it sufficient enough to measure the differences among baseline models with \"exact match\".\n\n\nMODELS FOR SQA3D\n\nGenerally speaking, SQA3D can be characterized as a multi-modal reasoning problem. Inspired by the recent advances in transformer-based (Vaswani et al., 2017) vision-language models (Lu et al., 2019;Li et al., 2020;Alayrac et al., 2022), we investigate how could these methods approach our task. Specifically, we study a recent transformer-based question-answering system: ScanQA (Azuma et al., 2022), which maps 3D scans and questions into answers. We make a few adaptations to ensure its compatibility with the protocol in SQA3D. To further improve this model, we consider including some auxiliary tasks during training (Ma et al., 2022). For other types of 3D scene context, e.g. egocentric video clips and BEV pictures, we employ the corresponding state-ofthe-art models. Finally, we explore the potential of recently-introduced LLMs like GPT-3 (Brown et al., 2020) and Unified QA (Khashabi et al., 2020) on solving SQA3D in a zero-shot fashion. An overview of these models can be found in Figure 6.\n\n3D model. We use the term 3D model to refer a modified version of the ScanQA model (Azuma et al., 2022), depicted in the blue box of Figure 6. It includes a VoteNet )-based 3D perception module that extracts object-centric features, LSTM-based language encoders for processing both questions q and situation description s txt , and some cross-attention transformer blocks (Vaswani et al., 2017). The object-centric feature tokens attend to the language tokens of s txt and q successively. Finally, these features will be fused and mapped to predict the answer. Optionally, we can add one head to predict the location s pos , s rot of the agent. Since the VoteNet  Figure 6: Potential models for SQA3D. We split the considered models into three groups: 3D model, video / image model, and zero-shot model. The 3D model is modified from the ScanQA model (Azuma et al., 2022) and maps 3D scan input to the answer. While the video / image models are effectively borrowed from canonical video QA and VQA tasks but we augment them with the additional situation input. The zero-shot model explores the potential of large pre-trained LLMs on our tasks. But they have to work with an additional 3D caption model that converts the 3D scene into text.\n\nmodule is trained from scratch, we also employ an object detection objective (not shown in the figure).\n\nAuxiliary task. As we mentioned before, situation understanding plays a crucial role in accomplishing SQA3D tasks. To encourage a better understanding of the specified situation, we introduce two auxiliary tasks: the model is required to make predictions about the s pos and s rot of the situation. We use mean-square-error (MSE) loss for these tasks. The overall loss for our problem therefore becomes L = L ans + \u03b1L pos + \u03b2L rot , where L ans , L pos , and L rot depicts the losses of the main and auxiliary tasks, \u03b1 and \u03b2 are balancing weights.\n\nVideo and Image-based model. The orange box in the middle of Figure 6 demonstrates the models for video and image-based input. SQA3D largely resembles a video question answering or visual question answering problem when choosing to represent the 3D scene context S as egocentric video clips or BEV pictures. However, SQA3D also requires the model to take both question q and the newly added situation description s txt as input. We, therefore, follow the practice in the task of context-based QA (Rajpurkar et al., 2018) and prepend s txt to the question as a context. For the model, we use the state-of-the-art video QA system ClipBERT (Lei et al., 2021) and VQA system MCAN (Yu et al., 2019). We adopt most of their default hyper-parameters and the details can be found in appendix.\n\nZero-shot model. We explore to which extent the powerful LLMs like GPT-3 (Brown et al., 2020) and Unified QA (Khashabi et al., 2020) could tackle our tasks. Following prior practices that apply GPT-3 to VQA (Changpinyo et al., 2022;Gao et al., 2022), we propose to convert the 3D scene into text using an emerging technique called 3D captioning . We provide the caption, s txt , and q as part of the prompt and ask these models to complete the answer. For GPT-3, we further found providing few-shot examples in the prompt helpful with much better results. Minor postprocessing is also needed to ensure answer quality. We provide more details on prompt engineering in the appendix.\n\n\nEXPERIMENTS\n\n\nSETUP\n\nWe benchmark the models introduced in Section 3 to evaluate their performances on SQA3D. As mentioned before, we examine three types of scene context S: 3D scan (point cloud), egocentric video, and BEV picture. Both the 3D scan and egocentric video for each scene are provided by ScanNet . However, we down-sample the video to allow more efficient computation per the requirement of the ClipBERT model (Lei et al., 2021). The BEV pictures are rendered by placing a top-down camera on top of the scan of each 3D scene. We also conduct additional experiments that investigate factors that could contribute to the results, e.g., situation and auxiliary tasks. In our early experiments, we found that the 3D model overall performs better than the video or image-based models. Therefore we only conduct these additional experiments with the variants of our 3D model due to the limit of computational resources. We use the official implementation of ScanQA, ClipBERT, and MCAN and include our modifications for SQA3D. For the  Table 3: Quantitative results on the SQA3D benchmark. Results are presented in accuracy (%) on different types of questions. In the \"Format\" column: V = 3D visual input S; S = situation description s txt ; Q = question q; A = answer a; L = location s pos , s rot . In ScanQA, aux. task indicates the use of both Lpos and Lrot as additional losses. We use the Large variant as Unified QA (Khashabi et al., 2020) as it works better.\n\nzero-shot models, we extract 3D scene captions from two sources: ScanRefer  and ReferIt3D (Achlioptas et al., 2020). Considering the limit on the length of the input prompt, these 3D captions are also down-sampled. The Unified QA model weights are obtained from its Huggingface official repo. All the models are tuned using the validation set and we only report results on the test set. More details on model implementation can be found in appendix.\n\n\nQUANTITATIVE RESULTS\n\nWe provide the quantitative results of the considered models (detailed in Section 3) on our SQA3D benchmark in Table 3. The findings are summarized below:\n\nQuestion types. In Table 3, we demonstrate accuracy on six types of questions based on their prefixes. Most models tend to perform better on the \"Is\" and \"Can\" questions while delivering worse results on \"What\" questions, likely due to a smaller number of answer candidates -most questions with binary answers start with \"Is\" and \"Can\", offering a better chance for the random guess. Moreover, we observe the hugest gap between the blind test (model w/o 3D scene context input) and our best model on the \"What\" and \"Which\" categories, suggesting the need for more visual information for these two types of questions. This also partially echoes the finding reported in Lei et al. (2018).\n\nSituation understanding and reasoning. At the heart of SQA3D benchmark is the requirement of situation understanding and reasoning. As we mentioned in Section 2.1, the model will be more vulnerable to wrong answer predictions if ignoring the situation that the question depends on (e.g. \"What is in front of me\" could have completely different answers under different situations). In Table 3, removing situation description s txt from the input leads to worse results, while adding the auxiliary situation prediction tasks boosts the overall performance, especially on the challenging \"What\" questions. The only exception is \"How\" questions, where a majority of them are about counting. We hypothesize that most objects in each ScanNet scene only have a relatively small number of instances, and the number could also correlate to the object category. Therefore, guessing/memorization based on the question only could offer better results than models with the situation as input if the situation understanding & reasoning are still not perfect yet. Additionally, we also provide an inspection of the relation between situation understanding and QA using attention visualization in Section 4.3.\n\nRepresentations of 3D scenes. Indeed, SQA3D does not limit the input to be 3D scan only, as we also offer options of egocentric videos and BEV pictures. Compared to models with the 3D scan as input, the tested models with other 3D representations (i.e., MCAN and ClipBERT) deliver much worse results, implying that the 3D scan so far could still be a better representation for the 3D scene when the reasoning models are probed with questions that require a holistic understanding of the scene. On the other hand, MCAN and ClipBERT are general-purpose QA systems, while ScanQA is designed for 3D-language reasoning tasks. The generalist-specialty trade-off could also partially account for the gap. Finally, the poor results of BEV and egocentric videos based models compared to the blind test could also be due to the additional \"vision-bias\" when the visual input is  Figure 7: Qualitative results. We show the predicted answer and bbox with highest attention for the variants of ScanQA (Azuma et al., 2022) models. We anticipate the bbox to indicate the object that situation description s txt or question q refers to. We observe that better situation understanding (via comprehension on s txt or auxiliary tasks) could result in more reasonable attention over objects, which positively correlates to more robust answer prediction.\n\nprovided (Antol et al., 2015). Note that the vision-bias can be mitigated with better visual representations (Wen et al., 2021), implying that ScanQA, which seems to suffer less from the vision-bias than the counterparts using BEV and egocentric videos, is fueled by better visual representations in terms of combating the dataset bias.\n\nZero-shot vs. training from scratch. The success of pre-trained LLMs like GPT-3 on myriads of challenging reasoning tasks a) suggests that these models could possibly also understand embodied 3D scenes with language-only input (Landau & Jackendoff, 1993). However, SQA3D imposes a grand challenge to these models. The powerful Unified QA (Large variant) and GPT-3 both fail to deliver reasonable results on our tasks. Further, we hypothesize the bottleneck could also be on the 3D captions, as the results verify the consistent impact on model performances brought by a different source of captions (ScanRefer\u2192ReferIt3D). However, we still believe these models have great potential. For example, one zero-shot model (GPT-3 + ScanRefer) do pretty well on the challenging \"What\" questions (39.67%), even better than the best ScanQA variant.\n\nHuman vs. machine. Finally, all the machine learning models largely fall behind amateur human participants (47.2% of ScanQA + aux. task vs. 90.06%). Notably, we only offer a limited number of examples for the testers before sending them the SQA3D problems. Our participants promptly master how to interact with the 3D scene, understand the situation from the textual description, and answer the challenging questions. The human performance also shows no significant bias for different question types.\n\n\nQUALITATIVE RESULTS\n\nFinally, we offer some qualitative results of the variants of our 3D model in Figure 7. We primarily focus on visualizing both the answer predictions and the transformer attention over the object-centric feature tokens (bounding boxes) generated by the VoteNet  backbone. We highlight the most-attended bounding box among all the predictions by the transformer-based model, in the hope of a better understanding of how these models perceive the 3D scene to comprehend the situations and answer the questions. In Figure 7, the correct predictions are always associated with attention over relevant objects in the situation description s txt and questions. Moreover, in case there are multiple instances of the same object category, it is also crucial to identify the correct instance.\n\nFor example, only ScanQA + aux. task makes the correct prediction for the first question and also attends to the right chair behind , while ScanQA focuses on a wrong instance. These results confirm our findings in Section 4.2 about the critical role of situation understanding. We also provide some failure modes in appendix. Figure 1, the agent could optionally predict the current location based off the situation description s txt and the current 3D scene context S. We therefore provide some additional metrics to help evaluate these predictions. Specifically, the agent needs to predict both the current position s pos in 3D coordinate x, y, z (unit is meter) and orientation in quaternion x, y, z, w . Then these predictions will be evaluated separately using the following metrics:\n\n\nADDITIONAL TASK FOR LOCALIZATION As illustrated in\n\n\u2022 Acc@0.5m: If the predicted position is within 0.5 meter range to the ground truth position, the prediction will be counted as correct. We then report #correctly predicted ground truth #all ground truth .\n\n\u2022 Acc@1.0m: Similar to Acc@0.5m but the range limit is 1.0 meter instead.\n\n\u2022 Acc@15\u00b0: If the prediction orientation is within a 15\u00b0range to the ground truth orientation, the prediction will be counted as correct.\n\n\u2022 Acc@30\u00b0: Similar to Acc@15\u00b0but the range limit is 30\u00b0instead.\n\nNote that, for position prediction, we only consider the predicted x, y and for orientation prediction, only the rotation along z-axis counts. We report the result of random prediction below as an reference.\n\nAcc@0.5m Acc@1.0m Acc@15\u00b0Acc@30\u00b0R andom 14.60 34.21 22.39 42.28 Table 4: Random predictions evaluated on the localization task.\n\n\nRELATED WORK\n\nEmbodied AI. The study of embodied AI (Brooks, 1990) emerges from the hypothesis of \"ongoing physical interaction with the environment as the primary source of constraint on the design of intelligent systems\". To this end, researchers have proposed a myriad of AI tasks to investigate whether intelligence will emerge by acting in virtual or photo-realistic environments. Notable tasks including robotic navigation ( . These tasks are made more challenging as instructions or natural-dialogues are further employed as conditions. Sophisticated models have also been developed to tackle these challenges. Earlier endeavors usually comprise multi-modal fusion (Tenenbaum & Freeman, 1996;Perez et al., 2018) and are trained from scratch Fried et al., 2018;Wang et al., 2019), while recent efforts would employ pre-trained models (Pashevich et al., 2021;Hong et al., 2021a;Suglia et al., 2021). However, the agents still suffer from poor generalization to novel and more complex testing tasks (Shridhar et al., 2020a) compared to results on training tasks. More detailed inspection has still yet to be conducted and it also motivates our SQA3D dataset, which investigates one crucial capability that the current embodied agents might need to improve: embodied scene understanding.\n\nGrounded 3D understanding. Visual grounding has been viewed as a key to connecting human knowledge, which is presumably encoded in our language, to the visual world, so as enable the intelligent agent to better understand and act in the real environment. It is natural to extend this ability to 3D data as it offers more immersive representations of the world. Earlier work has examined word-level grounding with detection and segmentation tasks on 3D data (Gupta et al., 2013;Song & Xiao, 2014;. Recent research starts to cover sentence-level grounding with complex semantics Achlioptas et al., 2020;. More recently, new benchmarks introduce complex visual reasoning to 3D data (Azuma et al., 2022;Ye et al., 2021;Yan et al., 2021). However, these tasks mostly assume a passive, third-person's perspective, while our SQA3D requires problem-solving with an egocentric viewpoint. This introduces both challenges and chances for tasks that need a first-person's view, e.g. embodied AI.\n\nMulti-modal question answering. Building generalist question answering (QA) systems has long been a goal for AI. Along with the progress in multi-modal machine learning, VQA (Antol et al., 2015;Zhu et al., 2016) pioneers the efforts of facilitating the development of more human-like, multi-modal QA systems. It has been extended with more types of knowledge, e.g. common sense (Zellers et al., 2019) and factual knowledge (Marino et al., 2019). Recent research has also introduced QA tasks on video (Lei et al., 2018;Jia et al., 2020;Grunde-McLaughlin et al., 2021;Wu et al., 2021;Datta et al., 2022), and 3D data (Ye et al., 2021;Azuma et al., 2022;Yan et al., 2021). We propose the SQA3D benchmark also in hope of facilitating multi-modal QA systems with the ability of embodied scene understanding. Notably, models for SQA3D could choose their input from a 3D scan, egocentric video, or BEV picture, which makes our dataset compatible with a wide spectrum of existing QA systems.\n\n\nCONCLUSION\n\nWe've introduced SQA3D, a benchmark that investigates the capability of embodied scene understanding by combining the best of situation understanding and situated reasoning. We carefully curate our dataset to include diverse situations and interesting questions while preserving the relatively large scale (20.4k situation descriptions and 33.4k questions). Our questions probe a wide spectrum of knowledge and reasoning abilities of embodied agents, notably navigation, common sense, and multi-hop reasoning. We examine many state-of-the-art multi-modal reasoning systems but the gap between the best ML model and human performances so far is still significant. Our findings suggest the crucial role of proper 3D representations and better situation understanding. With SQA3D, we hope of fostering research efforts in developing better embodied scene understanding methods and ultimately facilitate the emergence of more intelligent embodied agents.\n\nA DATA COLLECTION\n\n\nA.1 DATA COLLECTION WEB UI\n\nWe present the Web UI of our data collection in Figure 8 (Stage I), Figure 9 (Stage II) and Figure 11 (Stage III) respectively. We developed our UI based on . These UIs share some common components: a 3D scene viewer, where the user can drag, rotate, and zoom in/out the scene; clickable objects/tags, where users might click on either the object mesh directly or the tag on the sidebar to highlight it in the scene; and an instruction set that guide the user through the task. Users may also switch between a full scene or object mesh only to focus on the tasks. The users are also required to submit multiple responses with the same scene.\n\nNotably, we create detailed tutorials for each stage (not shown in the UI) with examples and animated demonstrations. We found tutorials and instruction sets with clear criteria on rejection and bonus(e.g. Figure 10) helpful with high-quality data. Finally, all the testers need to pass a test before the qualification for our task is granted.\n\nA.2 DATA POST-PROCESSING There are two major data post-processing steps in SQA3D: cleaning and balancing. For cleaning, we primarily focus on grammatical correction. We adopt both rule-based cleaning and an ML-based tool called GECToR (Omelianchuk et al., 2020) in our grammatical correction pipeline. We adjust the correction threshold based on human judgment over the corrected data samples.\n\nIn the balancing step, our goal is to reduce the question-answer bias in the dataset. Therefore we follow the practice in Antol et al. (2015); Marino et al. (2019) and re-sample the questions based on their prefixes and answer type, in hope of a more balanced answer distribution. We provide answer distribution before and after balancing in Section B.1.\n\n\nA.3 MORE MTURK DETAILS\n\nWe provide the detailed MTurk job settings below:\n\nRegion. We enable access to our tasks in the following countries/regions: US, DE, GB, AU, CA, SG, NZ, NO, SE, FI, DK, IE Approval rate & Number of approved jobs. The testers are required to have at least a 95% approval rate and have completed more than 1000 tasks. However, we relax this requirement to a 90% approval rate for Stage III as it is simpler than the other annotation tasks.\n\nReward. The participants will be rewarded $0.5 for each task in Stage I and II, and $0.2 for the QA tasks in Stage III, with a possibility of a bonus depending on the overall quality. We actively monitor the response quality and send bonuses/rejections daily. Note that we collected 5 responses for each task in all three stages.\n\nTask lifetime. We set the lifetime as 10 days for tasks in Stage I and 20 days for those in Stage II and III. However, we found most of the tasks can be completed in less than 7 days.\n\n\nB DATASET DETAILS\n\n\nB.1 MORE STATISTICS\n\nWe provide the histogram of the answer distribution before & after balancing in Figure 12 and Figure 13, respectively. It can be seen that we manage to ensure there is no single answer that dominates any type of question (categorized by their prefixes). However, we do acknowledge that prefix-based balancing might still not be sufficient since models could also learn to use the n-grams pattern. A more effective avenue is collecting more questions with less-frequent answers, which we leave as future work. In Figure 14a and Figure 14b, we show the histogram of the length of situation description s txt and question q. Overall most of the descriptions and questions are middle-length sentences (10-20 words).\n\n\nB.2 DETAILS ON EGOCENTRIC VIDEO AND BEV IMAGE\n\nFor egocentric videos, we uniformly downsample the frames of the original ScanNet  video by using the first frame of every 20 frames. Afterward, we resize all the frames to 224 \u00d7 224 to create the video used for training ClipBERT (Lei et al., 2021). Blender is used for rendering all BEV images. We compute the radius of the bounding sphere of the scene and put the camera at the top of the scene with a distance of 7 times the radius to the center of the bounding sphere. Images of size 1920 \u00d7 1080 are rendered for clarity while the input to the MCAN (Yu et al., 2019) model is the resized version of the images to 224 \u00d7 224.\n\nC MODEL DETAILS\n\n\nC.1 INPUT PIPELINE\n\nWe follow the input pipeline in ScanQA (Azuma et al., 2022) without further modification. As for MCAN, we only transform the images to fit the ImageNet-pretrained encoder. In ClipBERT, we randomly sample 8 clips with each clip consisting of 2 frames of the video to feed into the model as the scene representation. Note that each frame is resized to 1000 \u00d7 1000 following the practice of original ClipBERT (Lei et al., 2021).  \n\n\nC.2 HYPER-PARAMETERS\n\nWe provide the hyper-parameters of the considered models in Table 5.\n\n\nC.3 ADDITIONAL DETAILS ON ZERO-SHOT MODELS\n\nWe uniformly sample 30 sentences from our 3D caption sources for both models. When testing with the Unified QA Large model, we employ a simple greedy sampling method and the following prompt:  , where {s txt } and {q} are replaced by the situation description and question. For GPT-3, we use the text-davinci-002 variant and the following prompt:\n\nContext: There is a book on the desk. A laptop with a green cover is to the left of the book. Q: I'm working by the desk. What is on the desk beside the book? A: laptop Context:{s txt } Q: {q} A:\n\n, where we use a 1-shot example to demonstrate the format of our task. Interestingly, we found only GPT-3 would benefit from few-shot examples.\n\n\nC.4 ADDITIONAL DETAILS ON SCANQA/MCAN/CLIPBERT\n\nScanQA (Azuma et al., 2022). We slightly modify the original ScanQA code base (from https://github.com/ATR-DBI/ScanQA) to make it fit our task better. The original reference branch is discarded and the supervision signal for the language classification branch is changed to make use of it as a regression branch. More specific details can be found below.\n\n\u2022 The original data loader only outputs the question as a whole (meaning that the situation is concatenated before the question), while our version split the two sentences. \u2022 The original model takes language as 1 input, while we feed situation and question separately into the model. \u2022 The original model uses 1 self-attention block and 1 cross-attention block for the fusion of language and visual features, while our version uses 2 self-attention blocks and 2 cross-attention blocks to treat situations and questions separately. \u2022 The original model uses additive operation to fuse language & visual features, while our version uses concatenation for fusion. \u2022 To conduct the ablation experiment of blind test, we simply discard the output feature of VoteNet and only feed the situation feature and question feature into the QA head. \u2022 To conduct the ablation experiment of w/o s txt , we replace situation with several unk tokens to make a fair comparison. \u2022 To add an auxiliary task into training, we change the supervision of the language classification head from Cross Entropy to MSE Loss to make it a regression head.\n\nMCAN (Yu et al., 2019). We use the code base from RelVIT (Ma et al., 2022) (https://github.com/NVlabs/RelViT) since its implementation of MCAN could take raw images as input while the original one cannot. The default training setting is kept except for learning rate decay. We cancel it to make a fair comparison with the other baselines. We concatenate the situation before the question to make them as a whole and use this new sentence as the question that MCAN requires.\n\nClipBERT (Lei et al., 2021).\n\nWe use the official repository of ClipBERT (https://github.com/jayleicn/ClipBERT) and follow the instruction to transform our data into the format ClipBERT takes. The configuration file for MSR-VTT QA (Xu et al., 2016) is used for generality as we find all the configuration files to be almost identical. The evaluated question types are changed since our focus is different from MSR-VTT. We turn off mixed precision training as we observe instability when using it. We concatenate the situation before the question to make them as a whole and use this new sentence as the question that ClipBERT requires.\n\n\nD ADDITIONAL EMPIRICAL RESULTS\n\nWe provide additional qualitative results and failure modes in Figure 15 and Figure 16. \n\n\nE LIMITATION AND POTENTIAL IMPACT\n\nLimitations. One major limitation of SQA3D is the selection of 3D scenes. Since our dataset is collected on mostly indoor ScanNet scenes about household environments, it cannot cover outdoor scenes and other types of scenes, ex. warehouse. This could limit the application to autonomous driving and warehouse robots, which are likely deployed to the scene types that do not present in SQA3D. Moreover, all the scenes in ScanNet are static, i.e. the agent cannot interact with the object, making the exploration in SQA3D limited to hovering over the 3D scenes. However, many embodied tasks also require non-trivial interaction with articulated objects, e.g. drawers. Therefore, the embodied scene understanding capability examined by SQA3D can also be limited to non-interactive scenarios, i.e. situation understanding and situated reasoning.\n\nSocietal impact. SQA3D offers two sets of annotations: situations s txt , s pos , s rot and QA q, a . The situation annotations themselves could enable many exciting applications including building a real-world household assistant robot -one of its core capabilities is connecting natural language instructions/descriptions to the situations in the scene, e.g. locations. Moreover, non-trivial commonsense reasoning is also required in this process. Our annotations with accurate description-location pairs and the requirement of commonsense knowledge in text understanding could support these needs. The QA tasks also examine a wide spectrum of capabilities of embodied agents in household domains, making it a great benchmark for testing these household assistant robots. Finally, we will also release all the annotation interfaces and meta information, inviting everyone from either academia or industry to develop a customized version of QA datasets upon SQA3D and its infrastructure, which might help with the development of the 3D-language-related research and products.\n\ntxt : I am sitting on the armchair in front of the window. : What is above the armchair that is far away in front of me?\n\ntxt : I am facing an ottoman with a couch to my right within reach and an armchair to my left.\n\n: What color is the armchair to my left?\n\ntxt : I am facing the table and there   is a coffee table and a foosball table  to Figure 16: Failure mode. Models are likely to predict the wrong answers when they do not attend to relevant objects.\n\nFigure 1 :\n1Task illustration of Situated Question Answering in 3D Scenes (SQA3D)\n\n\ntxt : Working by the desk and the window is on my right. : How many chairs will I pass by to open the window from other side of the desk? : Three txt : Just looking for some food in the fridge. : Which direction should I go to heat my lunch? : Right txt : Playing computer games and the window is on my right. : How many monitors are there on the desk that the chair on my left is facing?\n\nFigure 4 :\n4Word cloud of s txt in SQA3D.\n\n\nDas et al., 2018;Anderson et al., 2018;Savva et al., 2019;Chen et al., 2019;Wijmans et al., 2019b;Qi et al., 2020;Deitke et al., 2022) and vision-based manipulation(Kolve et al., 2017;Puig et al., 2018;Xie et al., 2019;Shridhar et al., 2020a;b;\n\nFigure 8 :\n8Dataset collection Web UI for Stage I.\n\nFigure 9 :\n9Dataset collection Web UI for Stage II.\n\nFigure 10 :\n10Additional instruction set to the AMT participants in Stage II.\n\nFigure 11 :Figure 12 :Figure 13 :\n111213Dataset collection Web UI for Stage III. Answer distribution (organized by question prefixes) before balancing.{s txt } Q: {q} A: Answer distribution (organized by question prefixes) after balancing. of situation description s txt length.\n\n\nof question q length.\n\nFigure 15 :\n15Additional qualitative results.\n\nTable 1 :\n1An overview of the different benchmark datasets covering grounded 3D scene understanding.\n\nTable 2 :\n2SQA3D dataset statistics. \nFigure 5: Question distribution in SQA3D \n\n\n\nTable 2 ,\n2\n\n\nScanQA + aux. task 3D scan VSQ\u2192AL 33.48 66.10 42.37 69.53 43.02 46.40 47.20S \nFormat \ntest set \nAvg. \nWhat Is \nHow Can Which Others \n\nBlind test \n-\nSQ\u2192A 26.75 63.34 43.44 69.53 37.89 43.41 43.65 \n\nScanQA (w/o s txt ) \n3D scan \nVQ\u2192A 28.58 65.03 47.31 66.27 43.87 42.88 45.27 \nScanQA \n3D scan \nVSQ\u2192A 31.64 63.80 46.02 69.53 43.87 45.34 46.58 \nMCAN \nBEV \nVSQ\u2192A 28.86 59.66 44.09 68.34 40.74 40.46 43.42 \nClipBERT \nEgo. video VSQ\u2192A 30.24 60.12 38.71 63.31 42.45 42.71 43.31 \n\nUnified QA Large \nScanRefer VSQ\u2192A 33.01 50.43 31.91 56.51 45.17 41.11 41.00 \nUnified QA Large \nReferIt3D VSQ\u2192A 27.58 47.99 34.05 59.47 40.91 39.77 38.71 \nGPT-3 \nScanRefer VSQ\u2192A 39.67 45.99 40.47 45.56 36.08 38.42 41.00 \nGPT-3 \nReferIt3D VSQ\u2192A 28.90 46.42 28.05 40.24 30.11 36.07 34.57 \n\nHuman (amateur) \n3D scan \nVSQ\u2192A 88.53 93.84 88.44 95.27 87.22 88.57 90.06 \n\n\n\n\ntxt : I am entering the room.: What is the color of the backpack on my 10 o'clock chair?txt : I am picking up my backpack \n\nwith a chair to my left within reach. \n: How many chairs are behind me? \n\ntxt : I am sitting on the rightmost \n\nside of my couch, and there is an end \ntable to my right. \n: The coffee table that is furthest \nfrom me is surrounded by what? \n\n: One \u274c \n: Couch \u2705 \n: Black \u274c \n\nScanQA + aux. task \n\nScanQA \n\nScanQA (w/o txt \n\n) \n\n: Unknown \u274c \n: One \u274c \n: Black \u274c \n\n: Two \u2705 \n: Couch \u2705 \n: Red \u2705 \n\n: Front \u274c \n\n: Wall \u2705 \n\n: Wall \u2705 \n\ntxt : I am working at the counter \n\nwith cabinets over my head and the \nwall on my left is within reach. \n: If I walked backwards, what \nwould I hit behind me? \n\n\n\n\nmy left. : Which way should I go to sit on the couch? txt : I am facing an end table and there is a couch on my left within reach. : How many chairs does the table on my left have?: Picture \u274c \n: Red \u274c \n: Forward \u274c \n\nScanQA + aux. task \n\nScanQA \n\nScanQA (w/o txt \n\n) \n\n: Black \u274c \n: Light \u274c \n: Left \u274c \n\n: TV \u274c \n: Brown \u274c \n: Left \u274c \n\n: Four \u274c \n\n: Four \u274c \n\n: Four \u274c \n\nGT answer \n\n: Bulletin board \u2705 \n: White \u2705 \n: Right \u2705 \n: Zero \u2705 \n\n\nACKNOWLEDGEMENTThe authors would like to thank Dave Zhenyu Chen for his insightful ScanRefer project and help on data collection, Wenjuan Han for discussions on data collection and model design. This project is supported by National Key R&D Program of China (2021ZD0150200).Parameter ValueScanQA(\nReferit3D: Neural listeners for fine-grained 3D object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas Guibas, European Conference on Computer Vision (ECCV). 79Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3D: Neural listeners for fine-grained 3D object identification in real-world scenes. In Eu- ropean Conference on Computer Vision (ECCV), pp. 422-440, 2020. 2, 3, 7, 9\n\nDo as i can, not as i say: Grounding language in robotic affordances. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, arXiv:2204.01691arXiv preprintMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 1\n\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katie Mensch, Malcolm Millican, Reynolds, arXiv:2204.14198arXiv preprintJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. 5\n\nPerspectives on learning, thinking, and activity. James G John R Anderson, Lynne M Greeno, Herbert A Reder, Simon, Educational Researcher. 294John R Anderson, James G Greeno, Lynne M Reder, and Herbert A Simon. Perspectives on learning, thinking, and activity. Educational Researcher, 29(4):11-13, 2000. 2\n\nVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Conference on Computer Vision and Pattern Recognition (CVPR). 19Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Conference on Computer Vi- sion and Pattern Recognition (CVPR), pp. 3674-3683, 2018. 1, 3, 9\n\nVqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, International Conference on Computer Vision (ICCV). 1016Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit- nick, and Devi Parikh. Vqa: Visual question answering. In International Conference on Computer Vision (ICCV), pp. 2425-2433, 2015. 8, 10, 16\n\nScanQA: 3D Question Answering for Spatial Scene Understanding. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe, Conference on Computer Vision and Pattern Recognition (CVPR). 1021Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. ScanQA: 3D Question Answering for Spatial Scene Understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19129-19139, 2022. 2, 3, 5, 6, 8, 9, 10, 17, 21\n\nElephants don't play chess. A Rodney, Brooks, Robotics and autonomous systems. 61-2Rodney A Brooks. Elephants don't play chess. Robotics and autonomous systems, 6(1-2):3-15, 1990. 9\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems (NeurIPS). 336Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:1877-1901, 2020. 3, 5, 6\n\nAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, arXiv:1709.06158Matterport3D: Learning from rgb-d data in indoor environments. arXiv preprintAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. 9\n\nAll You May Need for VQA are Image Captions. Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi Chen, Nan Ding, Radu Soricut, arXiv:2205.01883arXiv preprintSoravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi Chen, Nan Ding, and Radu Soricut. All You May Need for VQA are Image Captions. arXiv preprint arXiv:2205.01883, 2022. 6\n\nScanrefer: 3D object localization in rgb-d scans using natural language. Dave Zhenyu Chen, X Angel, Matthias Chang, Nie\u00dfner, European Conference on Computer Vision (ECCV). 16Dave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. Scanrefer: 3D object localization in rgb-d scans using natural language. In European Conference on Computer Vision (ECCV), pp. 202-221, 2020. 2, 3, 7, 9, 16\n\nTouchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi, Conference on Computer Vision and Pattern Recognition (CVPR). Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natu- ral language navigation and spatial reasoning in visual street environments. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12538-12547, 2019. 9\n\nContext-aware dense captioning in rgb-d scans. Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, Chang, Conference on Computer Vision and Pattern Recognition (CVPR). 29Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, and Angel X Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3193-3203, 2021. 2, 6, 9\n\nScannet: Richly-annotated 3D reconstructions of indoor scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Nie\u00dfner, Conference on Computer Vision and Pattern Recognition (CVPR). 917Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3D reconstructions of indoor scenes. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5828-5839, 2017. 2, 3, 6, 9, 17\n\nEmbodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Conference on Computer Vision and Pattern Recognition (CVPR). 29Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Em- bodied question answering. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-10, 2018. 2, 9\n\nEpisodic Memory Question Answering. Samyak Datta, Sameer Dharur, Vincent Cartillier, Ruta Desai, Mukul Khanna, Dhruv Batra, Devi Parikh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSamyak Datta, Sameer Dharur, Vincent Cartillier, Ruta Desai, Mukul Khanna, Dhruv Batra, and Devi Parikh. Episodic Memory Question Answering. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pp. 19119-19128, 2022. 10\n\nMatt Deitke, Eli Vanderbilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, arXiv:2206.06994Large-Scale Embodied AI Using Procedural Generation. arXiv preprintMatt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, et al. ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. arXiv preprint arXiv:2206.06994, 2022. 9\n\nSpeaker-follower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Advances in Neural Information Processing Systems (NeurIPS). 31Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. Advances in Neural Information Processing Systems (NeurIPS), 31, 2018. 9\n\nTransform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering. Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti, Ying Nian Wu, Prem Natarajan, Conference on Computer Vision and Pattern Recognition (CVPR). Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti, Ying Nian Wu, and Prem Natarajan. Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5067-5077, 2022. 6\n\nMaking the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Conference on Computer Vision and Pattern Recognition (CVPR). 45Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6904-6913, 2017. 4, 5\n\nThe situativity of knowing, learning, and research. G James, Greeno, American psychologist. 531James G Greeno. The situativity of knowing, learning, and research. American psychologist, 53(1): 5, 1998. 2\n\nAGQA: A benchmark for compositional spatio-temporal reasoning. Madeleine Grunde-Mclaughlin, Ranjay Krishna, Maneesh Agrawala, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMadeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. AGQA: A benchmark for compositional spatio-temporal reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11287-11297, 2021. 10\n\nPerceptual organization and recognition of indoor scenes from RGB-D images. Saurabh Gupta, Pablo Arbelaez, Jitendra Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSaurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Perceptual organization and recognition of indoor scenes from RGB-D images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 564-571, 2013. 9\n\nVln bert: A recurrent vision-and-language bert for navigation. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould, Conference on Computer Vision and Pattern Recognition (CVPR). Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. Vln bert: A recurrent vision-and-language bert for navigation. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1643-1653, 2021a. 9\n\nVlgrammar: Grounded grammar induction of vision and language. Yining Hong, Qing Li, Song-Chun Zhu, Siyuan Huang, 2021b. 2International Conference on Computer Vision (ICCV). Yining Hong, Qing Li, Song-Chun Zhu, and Siyuan Huang. Vlgrammar: Grounded grammar in- duction of vision and language. In International Conference on Computer Vision (ICCV), 2021b. 2\n\nLemma: A multi-view dataset for learning multi-agent multi-task activities. Baoxiong Jia, Yixin Chen, Siyuan Huang, Yixin Zhu, Song-Chun Zhu, European Conference on Computer Vision (ECCV). Baoxiong Jia, Yixin Chen, Siyuan Huang, Yixin Zhu, and Song-chun Zhu. Lemma: A multi-view dataset for learning multi-agent multi-task activities. In European Conference on Computer Vision (ECCV), 2020. 10\n\nEgoTaskQA: Understanding Human Tasks in Egocentric Videos. Baoxiong Jia, Ting Lei, Song-Chun, Siyuan Zhu, Huang, The 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks. Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. EgoTaskQA: Understanding Human Tasks in Egocentric Videos. In The 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks, 2022. 10\n\nUnifiedqa: Crossing format boundaries with a single qa system. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, arXiv:2005.0070067arXiv preprintDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020. 3, 5, 6, 7\n\nAi2-thor: An interactive 3D environment for visual ai. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXiv:1712.05474arXiv preprintEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3D environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. 9\n\nWhat\" and \"where\" in spatial language and spatial cognition. Barbara Landau, Ray Jackendoff, Behavioral and brain sciences. 162Barbara Landau and Ray Jackendoff. \"What\" and \"where\" in spatial language and spatial cognition. Behavioral and brain sciences, 16(2):217-238, 1993. 8\n\nJie Lei, Licheng Yu, Mohit Bansal, Tamara L Berg, arXiv:1809.01696Tvqa: Localized, compositional video question answering. 710arXiv preprintJie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696, 2018. 7, 10\n\nLess is more: Clipbert for video-and-language learning via sparse sampling. Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, Jingjing Liu, Conference on Computer Vision and Pattern Recognition (CVPR). 1721Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7331-7341, 2021. 3, 6, 17, 21\n\nObject-semantics aligned pre-training for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, European Conference on Computer Vision (ECCV). SpringerXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision (ECCV), pp. 121-137. Springer, 2020. 5\n\nPointnetgpd: Detecting grasp configurations from point sets. Hongzhuo Liang, Xiaojian Ma, Shuang Li, Michael G\u00f6rner, Song Tang, Bin Fang, Fuchun Sun, Jianwei Zhang, International Conference on Robotics and Automation (ICRA). IEEEHongzhuo Liang, Xiaojian Ma, Shuang Li, Michael G\u00f6rner, Song Tang, Bin Fang, Fuchun Sun, and Jianwei Zhang. Pointnetgpd: Detecting grasp configurations from point sets. In International Conference on Robotics and Automation (ICRA), pp. 3629-3635. IEEE, 2019. 1\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Advances in Neural Information Processing Systems (NeurIPS). 32Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolin- guistic representations for vision-and-language tasks. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. 5\n\nLearn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, arXiv:2209.09513arXiv preprintPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. arXiv preprint arXiv:2209.09513, 2022a.\n\nDynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, arXiv:2209.14610arXiv preprintPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022b.\n\nRelvit: Concept-guided vision transformer for visual relational reasoning. Xiaojian Ma, Weili Nie, Zhiding Yu, Huaizu Jiang, Chaowei Xiao, Yuke Zhu, Song-Chun, Anima Zhu, Anandkumar, arXiv:2204.11167521arXiv preprintXiaojian Ma, Weili Nie, Zhiding Yu, Huaizu Jiang, Chaowei Xiao, Yuke Zhu, Song-Chun Zhu, and Anima Anandkumar. Relvit: Concept-guided vision transformer for visual relational reasoning. arXiv preprint arXiv:2204.11167, 2022. 5, 21\n\nOk-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Conference on Computer Vision and Pattern Recognition (CVPR). 416Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3195-3204, 2019. 4, 10, 16\n\nGECToR -grammatical error correction: Tag, not rewrite. Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem Chernodub, Oleksandr Skurzhanskyi, Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications. the Fifteenth Workshop on Innovative Use of NLP for Building Educational ApplicationsSeattle, WA, USA\u00e2 \u2020' OnlineAssociation for Computational LinguisticsKostiantyn Omelianchuk, Vitaliy Atrasevych, Artem Chernodub, and Oleksandr Skurzhanskyi. GECToR -grammatical error correction: Tag, not rewrite. In Proceedings of the Fifteenth Work- shop on Innovative Use of NLP for Building Educational Applications, pp. 163-170, Seattle, WA, USA\u00e2 \u2020' Online, July 2020. Association for Computational Linguistics. URL https: //www.aclweb.org/anthology/2020.bea-1.16. 16\n\nEpisodic transformer for vision-andlanguage navigation. Alexander Pashevich, Cordelia Schmid, Chen Sun, Conference on Computer Vision and Pattern Recognition (CVPR). Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and- language navigation. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15942-15952, 2021. 9\n\nFilm: Visual reasoning with a general conditioning layer. Ethan Perez, Florian Strub, Harm De, Vincent Vries, Aaron Dumoulin, Courville, AAAI Conference on Artificial Intelligence (AAAI). 32Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI Conference on Artificial Intelligence (AAAI), volume 32, 2018. 9\n\nVirtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Conference on Computer Vision and Pattern Recognition (CVPR). Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8494-8502, 2018. 9\n\nDeep hough voting for 3D object detection in point clouds. Or Charles R Qi, Kaiming Litany, Leonidas J He, Guibas, Conference on Computer Vision and Pattern Recognition (CVPR). 5Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3D object detection in point clouds. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9277-9286, 2019. 5, 8\n\nReverie: Remote embodied visual referring expression in real indoor environments. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton Van Den, Hengel, Conference on Computer Vision and Pattern Recognition (CVPR). Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environ- ments. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9982-9991, 2020. 9\n\nPranav Rajpurkar, Robin Jia, Percy Liang, arXiv:1806.03822Know what you don't know: Unanswerable questions for SQuAD. arXiv preprintPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for SQuAD. arXiv preprint arXiv:1806.03822, 2018. 6\n\nHabitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, International Conference on Computer Vision (ICCV). 19Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In International Conference on Computer Vision (ICCV), pp. 9339-9347, 2019. 1, 9\n\nAlfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Conference on Computer Vision and Pattern Recognition (CVPR). Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10740-10749, 2020a. 9\n\nAlfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.03768arXiv preprintMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020b. 9\n\nCliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, PMLRConference on Robot Learning (CoRL). 19Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning (CoRL), pp. 894-906. PMLR, 2022. 1, 9\n\nSliding shapes for 3D object detection in depth images. Shuran Song, Jianxiong Xiao, European conference on computer vision. SpringerShuran Song and Jianxiong Xiao. Sliding shapes for 3D object detection in depth images. In European conference on computer vision, pp. 634-651. Springer, 2014. 9\n\nEmbodied bert: A transformer model for embodied, language-guided visual task completion. Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme, arXiv:2108.04927arXiv preprintAlessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme. Embodied bert: A transformer model for embodied, language-guided visual task completion. arXiv preprint arXiv:2108.04927, 2021. 9\n\nSeparating style and content. Joshua Tenenbaum, William Freeman, Advances in Neural Information Processing Systems (NeurIPS). 9Joshua Tenenbaum and William Freeman. Separating style and content. Advances in Neural Infor- mation Processing Systems (NeurIPS), 9, 1996. 9\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems (NeurIPS). 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Informa- tion Processing Systems (NeurIPS), 30, 2017. 5\n\nLook before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang Wang, European Conference on Computer Vision (ECCV). Xin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. Look before you leap: Bridg- ing model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. In European Conference on Computer Vision (ECCV), pp. 37-53, 2018. 9\n\nReinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang, Conference on Computer Vision and Pattern Recognition (CVPR). Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised im- itation learning for vision-language navigation. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6629-6638, 2019. 9\n\nHUMANISE: Language-conditioned Human Motion Generation in 3D Scenes. Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang, Advances in Neural Information Processing Systems (NeurIPS). 2022Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes. In Advances in Neural Informa- tion Processing Systems (NeurIPS), 2022. 2\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. arXiv preprintJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo- gatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a. 8\n\nChain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b. 8\n\nDebiased Visual Question Answering from Feature and Sample Perspectives. Zhiquan Wen, Guanghui Xu, Mingkui Tan, Qingyao Wu, Qi Wu, 34Zhiquan Wen, Guanghui Xu, Mingkui Tan, Qingyao Wu, and Qi Wu. Debiased Visual Question Answering from Feature and Sample Perspectives. 34:3784-3796, 2021. 8\n\nEmbodied question answering in photorealistic environments with point cloud perception. Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra, Conference on Computer Vision and Pattern Recognition (CVPR). 23Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, and Dhruv Batra. Embodied question answering in photorealistic envi- ronments with point cloud perception. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6659-6668, 2019a. 2, 3\n\nDd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra, arXiv:1911.00357arXiv preprintErik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. arXiv preprint arXiv:1911.00357, 2019b. 9\n\nSTAR: A benchmark for situated reasoning in real-world videos. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, Chuang Gan, 2021. 10Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Round 2Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. STAR: A benchmark for situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 10\n\nVrgym: A virtual testbed for physical and interactive ai. Xu Xie, Hangxin Liu, Zhenliang Zhang, Yuxing Qiu, Feng Gao, Siyuan Qi, Yixin Zhu, Song-Chun Zhu, Proceedings of the ACM Turing Celebration Conference-China. the ACM Turing Celebration Conference-ChinaXu Xie, Hangxin Liu, Zhenliang Zhang, Yuxing Qiu, Feng Gao, Siyuan Qi, Yixin Zhu, and Song- Chun Zhu. Vrgym: A virtual testbed for physical and interactive ai. In Proceedings of the ACM Turing Celebration Conference-China, pp. 1-6, 2019. 9\n\nMsr-vtt: A large video description dataset for bridging video and language. Jun Xu, Tao Mei, Ting Yao, Yong Rui, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition21Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5288-5296, 2016. 21\n\nXu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Zhen Li, Shuguang Cui, arXiv:2112.11691CLEVR3D: Compositional Language and Elementary Visual Reasoning for Question Answering in 3D Real-World Scenes. 310arXiv preprintXu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Zhen Li, and Shuguang Cui. CLEVR3D: Compositional Language and Elementary Visual Reasoning for Question Answer- ing in 3D Real-World Scenes. arXiv preprint arXiv:2112.11691, 2021. 3, 10\n\n. Dongdong Shuquan Ye, Songfang Chen, Jing Han, Liao, arXiv:2112.083593103D Question Answering. arXiv preprintShuquan Ye, Dongdong Chen, Songfang Han, and Jing Liao. 3D Question Answering. arXiv preprint arXiv:2112.08359, 2021. 3, 10\n\nDeep modular co-attention networks for visual question answering. Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian, Conference on Computer Vision and Pattern Recognition (CVPR). 1721Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6281-6290, 2019. 3, 6, 17, 21\n\nFrom recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, Conference on Computer Vision and Pattern Recognition (CVPR). Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6720-6731, 2019. 10\n\nVisual7w: Grounded question answering in images. Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei, Conference on Computer Vision and Pattern Recognition (CVPR). Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question an- swering in images. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4995-5004, 2016. 10\n", "annotations": {"author": "[{\"end\":136,\"start\":96},{\"end\":235,\"start\":137},{\"end\":313,\"start\":236},{\"end\":386,\"start\":314},{\"end\":501,\"start\":387},{\"end\":629,\"start\":502},{\"end\":724,\"start\":630}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":105},{\"end\":148,\"start\":144},{\"end\":248,\"start\":243},{\"end\":321,\"start\":319},{\"end\":398,\"start\":393},{\"end\":515,\"start\":512},{\"end\":642,\"start\":637}]", "author_first_name": "[{\"end\":104,\"start\":96},{\"end\":143,\"start\":137},{\"end\":242,\"start\":236},{\"end\":318,\"start\":314},{\"end\":392,\"start\":387},{\"end\":511,\"start\":502},{\"end\":636,\"start\":630}]", "author_affiliation": "[{\"end\":135,\"start\":130},{\"end\":212,\"start\":150},{\"end\":234,\"start\":214},{\"end\":312,\"start\":250},{\"end\":385,\"start\":323},{\"end\":480,\"start\":418},{\"end\":500,\"start\":482},{\"end\":579,\"start\":517},{\"end\":586,\"start\":581},{\"end\":608,\"start\":588},{\"end\":628,\"start\":610},{\"end\":723,\"start\":661}]", "title": "[{\"end\":93,\"start\":1},{\"end\":817,\"start\":725}]", "venue": null, "abstract": "[{\"end\":2286,\"start\":819}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2919,\"start\":2896},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2963,\"start\":2943},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2982,\"start\":2963},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3004,\"start\":2982},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3021,\"start\":3004},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4148,\"start\":4134},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4170,\"start\":4148},{\"end\":5942,\"start\":5923},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5966,\"start\":5942},{\"end\":5984,\"start\":5966},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6003,\"start\":5984},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6384,\"start\":6366},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":6405,\"start\":6384},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6911,\"start\":6892},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7617,\"start\":7592},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7676,\"start\":7656},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":7733,\"start\":7716},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7791,\"start\":7773},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7817,\"start\":7799},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7844,\"start\":7827},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8471,\"start\":8451},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8510,\"start\":8487},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14602,\"start\":14582},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14635,\"start\":14614},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14951,\"start\":14931},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16235,\"start\":16215},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16750,\"start\":16730},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":17293,\"start\":17271},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17334,\"start\":17317},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17350,\"start\":17334},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17371,\"start\":17350},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17535,\"start\":17515},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17774,\"start\":17757},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18004,\"start\":17984},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18043,\"start\":18020},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18243,\"start\":18223},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18534,\"start\":18512},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19011,\"start\":18991},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20555,\"start\":20531},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20690,\"start\":20672},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":20728,\"start\":20711},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20914,\"start\":20894},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20953,\"start\":20930},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21053,\"start\":21028},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21070,\"start\":21053},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21945,\"start\":21927},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22956,\"start\":22933},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23093,\"start\":23068},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24293,\"start\":24276},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26499,\"start\":26479},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26855,\"start\":26835},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":26953,\"start\":26935},{\"end\":27288,\"start\":27286},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27418,\"start\":27391},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":31680,\"start\":31653},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31699,\"start\":31680},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31748,\"start\":31729},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31766,\"start\":31748},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31845,\"start\":31821},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31864,\"start\":31845},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":31884,\"start\":31864},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":32008,\"start\":31984},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32750,\"start\":32730},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":32768,\"start\":32750},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32874,\"start\":32850},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32972,\"start\":32952},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":32988,\"start\":32972},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":33005,\"start\":32988},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33452,\"start\":33432},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":33469,\"start\":33452},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":33658,\"start\":33636},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33702,\"start\":33681},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33776,\"start\":33758},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33793,\"start\":33776},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33824,\"start\":33793},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":33840,\"start\":33824},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33859,\"start\":33840},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":33890,\"start\":33873},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33909,\"start\":33890},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":33926,\"start\":33909},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36505,\"start\":36479},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36780,\"start\":36761},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36802,\"start\":36782},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39026,\"start\":39008},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":39348,\"start\":39331},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":39504,\"start\":39484},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39869,\"start\":39851},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40778,\"start\":40758},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":42256,\"start\":42239},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":42736,\"start\":42718},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":42957,\"start\":42940},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":46422,\"start\":46405},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":46444,\"start\":46422},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":46463,\"start\":46444},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46481,\"start\":46463},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":46503,\"start\":46481},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":46519,\"start\":46503},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46539,\"start\":46519},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":46589,\"start\":46569},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":46607,\"start\":46589},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":46624,\"start\":46607},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":46647,\"start\":46624},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":46649,\"start\":46647}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45968,\"start\":45886},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46359,\"start\":45969},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46402,\"start\":46360},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46649,\"start\":46403},{\"attributes\":{\"id\":\"fig_4\"},\"end\":46701,\"start\":46650},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46754,\"start\":46702},{\"attributes\":{\"id\":\"fig_6\"},\"end\":46833,\"start\":46755},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47113,\"start\":46834},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47137,\"start\":47114},{\"attributes\":{\"id\":\"fig_9\"},\"end\":47184,\"start\":47138},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47286,\"start\":47185},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":47369,\"start\":47287},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47382,\"start\":47370},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":48220,\"start\":47383},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":48932,\"start\":48221},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":49364,\"start\":48933}]", "paragraph": "[{\"end\":2748,\"start\":2288},{\"end\":3709,\"start\":2765},{\"end\":4034,\"start\":3711},{\"end\":4236,\"start\":4036},{\"end\":4714,\"start\":4238},{\"end\":5848,\"start\":4716},{\"end\":6757,\"start\":5850},{\"end\":7539,\"start\":6759},{\"end\":7647,\"start\":7541},{\"end\":8761,\"start\":7649},{\"end\":8809,\"start\":8763},{\"end\":9671,\"start\":8811},{\"end\":10652,\"start\":9693},{\"end\":11049,\"start\":10654},{\"end\":11135,\"start\":11081},{\"end\":11249,\"start\":11164},{\"end\":12046,\"start\":11290},{\"end\":12495,\"start\":12048},{\"end\":13139,\"start\":12497},{\"end\":13661,\"start\":13141},{\"end\":14190,\"start\":13703},{\"end\":15346,\"start\":14233},{\"end\":16333,\"start\":15348},{\"end\":17114,\"start\":16335},{\"end\":18138,\"start\":17135},{\"end\":19379,\"start\":18140},{\"end\":19484,\"start\":19381},{\"end\":20033,\"start\":19486},{\"end\":20819,\"start\":20035},{\"end\":21501,\"start\":20821},{\"end\":22976,\"start\":21525},{\"end\":23427,\"start\":22978},{\"end\":23606,\"start\":23452},{\"end\":24294,\"start\":23608},{\"end\":25489,\"start\":24296},{\"end\":26824,\"start\":25491},{\"end\":27162,\"start\":26826},{\"end\":28002,\"start\":27164},{\"end\":28504,\"start\":28004},{\"end\":29311,\"start\":28528},{\"end\":30101,\"start\":29313},{\"end\":30361,\"start\":30156},{\"end\":30436,\"start\":30363},{\"end\":30575,\"start\":30438},{\"end\":30640,\"start\":30577},{\"end\":30849,\"start\":30642},{\"end\":30978,\"start\":30851},{\"end\":32271,\"start\":30995},{\"end\":33256,\"start\":32273},{\"end\":34241,\"start\":33258},{\"end\":35206,\"start\":34256},{\"end\":35225,\"start\":35208},{\"end\":35897,\"start\":35256},{\"end\":36242,\"start\":35899},{\"end\":36637,\"start\":36244},{\"end\":36993,\"start\":36639},{\"end\":37069,\"start\":37020},{\"end\":37457,\"start\":37071},{\"end\":37788,\"start\":37459},{\"end\":37973,\"start\":37790},{\"end\":38728,\"start\":38017},{\"end\":39405,\"start\":38778},{\"end\":39422,\"start\":39407},{\"end\":39872,\"start\":39445},{\"end\":39965,\"start\":39897},{\"end\":40358,\"start\":40012},{\"end\":40555,\"start\":40360},{\"end\":40700,\"start\":40557},{\"end\":41105,\"start\":40751},{\"end\":42232,\"start\":41107},{\"end\":42707,\"start\":42234},{\"end\":42737,\"start\":42709},{\"end\":43344,\"start\":42739},{\"end\":43467,\"start\":43379},{\"end\":44346,\"start\":43505},{\"end\":45424,\"start\":44348},{\"end\":45546,\"start\":45426},{\"end\":45642,\"start\":45548},{\"end\":45684,\"start\":45644},{\"end\":45885,\"start\":45686}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6756,\"start\":6749},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15718,\"start\":15711},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16696,\"start\":16689},{\"end\":22553,\"start\":22546},{\"end\":23570,\"start\":23563},{\"end\":23634,\"start\":23627},{\"end\":30922,\"start\":30915},{\"end\":39964,\"start\":39957},{\"end\":45768,\"start\":45690}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2763,\"start\":2751},{\"attributes\":{\"n\":\"2\"},\"end\":9691,\"start\":9674},{\"end\":11079,\"start\":11052},{\"end\":11162,\"start\":11138},{\"end\":11288,\"start\":11252},{\"end\":13701,\"start\":13664},{\"attributes\":{\"n\":\"2.2\"},\"end\":14231,\"start\":14193},{\"attributes\":{\"n\":\"3\"},\"end\":17133,\"start\":17117},{\"attributes\":{\"n\":\"4\"},\"end\":21515,\"start\":21504},{\"attributes\":{\"n\":\"4.1\"},\"end\":21523,\"start\":21518},{\"attributes\":{\"n\":\"4.2\"},\"end\":23450,\"start\":23430},{\"attributes\":{\"n\":\"4.3\"},\"end\":28526,\"start\":28507},{\"attributes\":{\"n\":\"4.4\"},\"end\":30154,\"start\":30104},{\"attributes\":{\"n\":\"5\"},\"end\":30993,\"start\":30981},{\"attributes\":{\"n\":\"6\"},\"end\":34254,\"start\":34244},{\"end\":35254,\"start\":35228},{\"end\":37018,\"start\":36996},{\"end\":37993,\"start\":37976},{\"end\":38015,\"start\":37996},{\"end\":38776,\"start\":38731},{\"end\":39443,\"start\":39425},{\"end\":39895,\"start\":39875},{\"end\":40010,\"start\":39968},{\"end\":40749,\"start\":40703},{\"end\":43377,\"start\":43347},{\"end\":43503,\"start\":43470},{\"end\":45897,\"start\":45887},{\"end\":46371,\"start\":46361},{\"end\":46661,\"start\":46651},{\"end\":46713,\"start\":46703},{\"end\":46767,\"start\":46756},{\"end\":46868,\"start\":46835},{\"end\":47150,\"start\":47139},{\"end\":47195,\"start\":47186},{\"end\":47297,\"start\":47288},{\"end\":47380,\"start\":47371}]", "table": "[{\"end\":47369,\"start\":47299},{\"end\":48220,\"start\":47460},{\"end\":48932,\"start\":48311},{\"end\":49364,\"start\":49115}]", "figure_caption": "[{\"end\":45968,\"start\":45899},{\"end\":46359,\"start\":45971},{\"end\":46402,\"start\":46373},{\"end\":46649,\"start\":46405},{\"end\":46701,\"start\":46663},{\"end\":46754,\"start\":46715},{\"end\":46833,\"start\":46770},{\"end\":47113,\"start\":46875},{\"end\":47137,\"start\":47116},{\"end\":47184,\"start\":47153},{\"end\":47286,\"start\":47197},{\"end\":47460,\"start\":47385},{\"end\":48311,\"start\":48223},{\"end\":49115,\"start\":48935}]", "figure_ref": "[{\"end\":2747,\"start\":2739},{\"end\":3066,\"start\":3058},{\"end\":5847,\"start\":5839},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10030,\"start\":10022},{\"end\":11416,\"start\":11408},{\"end\":12470,\"start\":12462},{\"end\":15300,\"start\":15292},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15730,\"start\":15722},{\"end\":15744,\"start\":15736},{\"end\":18137,\"start\":18129},{\"end\":18281,\"start\":18273},{\"end\":18812,\"start\":18804},{\"end\":20104,\"start\":20096},{\"end\":26368,\"start\":26360},{\"end\":28614,\"start\":28606},{\"end\":29048,\"start\":29040},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29647,\"start\":29639},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35312,\"start\":35304},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35332,\"start\":35324},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35357,\"start\":35348},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36114,\"start\":36105},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38106,\"start\":38097},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38120,\"start\":38111},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38539,\"start\":38529},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38554,\"start\":38544},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43451,\"start\":43442},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43465,\"start\":43456},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45778,\"start\":45769}]", "bib_author_first_name": "[{\"end\":49759,\"start\":49754},{\"end\":49777,\"start\":49772},{\"end\":49794,\"start\":49791},{\"end\":49807,\"start\":49800},{\"end\":49827,\"start\":49819},{\"end\":50223,\"start\":50216},{\"end\":50236,\"start\":50229},{\"end\":50249,\"start\":50245},{\"end\":50263,\"start\":50257},{\"end\":50278,\"start\":50274},{\"end\":50292,\"start\":50287},{\"end\":50307,\"start\":50300},{\"end\":50323,\"start\":50314},{\"end\":50345,\"start\":50340},{\"end\":50359,\"start\":50355},{\"end\":50730,\"start\":50726},{\"end\":50761,\"start\":50754},{\"end\":50778,\"start\":50771},{\"end\":50788,\"start\":50784},{\"end\":50800,\"start\":50796},{\"end\":50812,\"start\":50807},{\"end\":50827,\"start\":50821},{\"end\":50839,\"start\":50834},{\"end\":50855,\"start\":50848},{\"end\":51217,\"start\":51212},{\"end\":51219,\"start\":51218},{\"end\":51242,\"start\":51237},{\"end\":51244,\"start\":51243},{\"end\":51262,\"start\":51253},{\"end\":51583,\"start\":51578},{\"end\":51596,\"start\":51594},{\"end\":51607,\"start\":51601},{\"end\":51619,\"start\":51615},{\"end\":51631,\"start\":51627},{\"end\":51645,\"start\":51641},{\"end\":51661,\"start\":51658},{\"end\":51675,\"start\":51668},{\"end\":51688,\"start\":51683},{\"end\":52148,\"start\":52139},{\"end\":52165,\"start\":52156},{\"end\":52181,\"start\":52175},{\"end\":52194,\"start\":52186},{\"end\":52210,\"start\":52205},{\"end\":52226,\"start\":52218},{\"end\":52240,\"start\":52236},{\"end\":52611,\"start\":52605},{\"end\":52624,\"start\":52619},{\"end\":52642,\"start\":52636},{\"end\":52658,\"start\":52651},{\"end\":53013,\"start\":53012},{\"end\":53209,\"start\":53206},{\"end\":53225,\"start\":53217},{\"end\":53236,\"start\":53232},{\"end\":53251,\"start\":53244},{\"end\":53266,\"start\":53261},{\"end\":53268,\"start\":53267},{\"end\":53285,\"start\":53277},{\"end\":53302,\"start\":53296},{\"end\":53322,\"start\":53316},{\"end\":53336,\"start\":53330},{\"end\":53351,\"start\":53345},{\"end\":53719,\"start\":53714},{\"end\":53733,\"start\":53727},{\"end\":53745,\"start\":53739},{\"end\":53764,\"start\":53758},{\"end\":53781,\"start\":53773},{\"end\":53799,\"start\":53792},{\"end\":53813,\"start\":53807},{\"end\":53824,\"start\":53820},{\"end\":53836,\"start\":53831},{\"end\":54228,\"start\":54221},{\"end\":54246,\"start\":54241},{\"end\":54263,\"start\":54259},{\"end\":54276,\"start\":54274},{\"end\":54286,\"start\":54283},{\"end\":54297,\"start\":54293},{\"end\":54598,\"start\":54587},{\"end\":54606,\"start\":54605},{\"end\":54622,\"start\":54614},{\"end\":54999,\"start\":54993},{\"end\":55011,\"start\":55006},{\"end\":55026,\"start\":55018},{\"end\":55038,\"start\":55034},{\"end\":55052,\"start\":55048},{\"end\":55431,\"start\":55425},{\"end\":55441,\"start\":55438},{\"end\":55459,\"start\":55451},{\"end\":55824,\"start\":55818},{\"end\":55831,\"start\":55830},{\"end\":55846,\"start\":55839},{\"end\":55860,\"start\":55854},{\"end\":55874,\"start\":55868},{\"end\":55891,\"start\":55883},{\"end\":56278,\"start\":56270},{\"end\":56290,\"start\":56284},{\"end\":56305,\"start\":56298},{\"end\":56322,\"start\":56316},{\"end\":56332,\"start\":56328},{\"end\":56346,\"start\":56341},{\"end\":56666,\"start\":56660},{\"end\":56680,\"start\":56674},{\"end\":56696,\"start\":56689},{\"end\":56713,\"start\":56709},{\"end\":56726,\"start\":56721},{\"end\":56740,\"start\":56735},{\"end\":56752,\"start\":56748},{\"end\":57170,\"start\":57166},{\"end\":57182,\"start\":57179},{\"end\":57201,\"start\":57195},{\"end\":57216,\"start\":57212},{\"end\":57229,\"start\":57224},{\"end\":57245,\"start\":57240},{\"end\":57260,\"start\":57254},{\"end\":57270,\"start\":57266},{\"end\":57281,\"start\":57278},{\"end\":57300,\"start\":57291},{\"end\":57717,\"start\":57711},{\"end\":57733,\"start\":57725},{\"end\":57744,\"start\":57738},{\"end\":57756,\"start\":57752},{\"end\":57772,\"start\":57767},{\"end\":57796,\"start\":57782},{\"end\":57812,\"start\":57806},{\"end\":57835,\"start\":57831},{\"end\":57847,\"start\":57844},{\"end\":57861,\"start\":57855},{\"end\":58335,\"start\":58331},{\"end\":58345,\"start\":58341},{\"end\":58358,\"start\":58352},{\"end\":58377,\"start\":58368},{\"end\":58391,\"start\":58387},{\"end\":58396,\"start\":58392},{\"end\":58405,\"start\":58401},{\"end\":58861,\"start\":58857},{\"end\":58874,\"start\":58869},{\"end\":58888,\"start\":58881},{\"end\":58908,\"start\":58903},{\"end\":58920,\"start\":58916},{\"end\":59314,\"start\":59313},{\"end\":59538,\"start\":59529},{\"end\":59564,\"start\":59558},{\"end\":59581,\"start\":59574},{\"end\":60067,\"start\":60060},{\"end\":60080,\"start\":60075},{\"end\":60099,\"start\":60091},{\"end\":60548,\"start\":60542},{\"end\":60557,\"start\":60555},{\"end\":60569,\"start\":60562},{\"end\":60582,\"start\":60574},{\"end\":60607,\"start\":60600},{\"end\":60975,\"start\":60969},{\"end\":60986,\"start\":60982},{\"end\":61000,\"start\":60991},{\"end\":61012,\"start\":61006},{\"end\":61348,\"start\":61340},{\"end\":61359,\"start\":61354},{\"end\":61372,\"start\":61366},{\"end\":61385,\"start\":61380},{\"end\":61400,\"start\":61391},{\"end\":61726,\"start\":61718},{\"end\":61736,\"start\":61732},{\"end\":61759,\"start\":61753},{\"end\":62190,\"start\":62184},{\"end\":62206,\"start\":62201},{\"end\":62218,\"start\":62212},{\"end\":62231,\"start\":62225},{\"end\":62249,\"start\":62243},{\"end\":62264,\"start\":62259},{\"end\":62280,\"start\":62272},{\"end\":62611,\"start\":62607},{\"end\":62626,\"start\":62619},{\"end\":62643,\"start\":62637},{\"end\":62652,\"start\":62649},{\"end\":62669,\"start\":62665},{\"end\":62683,\"start\":62677},{\"end\":62700,\"start\":62694},{\"end\":62713,\"start\":62709},{\"end\":62726,\"start\":62719},{\"end\":62737,\"start\":62734},{\"end\":63086,\"start\":63079},{\"end\":63098,\"start\":63095},{\"end\":63300,\"start\":63297},{\"end\":63313,\"start\":63306},{\"end\":63323,\"start\":63318},{\"end\":63338,\"start\":63332},{\"end\":63340,\"start\":63339},{\"end\":63673,\"start\":63670},{\"end\":63685,\"start\":63679},{\"end\":63696,\"start\":63690},{\"end\":63706,\"start\":63703},{\"end\":63718,\"start\":63712},{\"end\":63720,\"start\":63719},{\"end\":63732,\"start\":63727},{\"end\":63749,\"start\":63741},{\"end\":64157,\"start\":64151},{\"end\":64164,\"start\":64162},{\"end\":64178,\"start\":64170},{\"end\":64192,\"start\":64183},{\"end\":64207,\"start\":64200},{\"end\":64215,\"start\":64212},{\"end\":64229,\"start\":64223},{\"end\":64243,\"start\":64236},{\"end\":64250,\"start\":64248},{\"end\":64261,\"start\":64257},{\"end\":64668,\"start\":64660},{\"end\":64684,\"start\":64676},{\"end\":64695,\"start\":64689},{\"end\":64707,\"start\":64700},{\"end\":64720,\"start\":64716},{\"end\":64730,\"start\":64727},{\"end\":64743,\"start\":64737},{\"end\":64756,\"start\":64749},{\"end\":65194,\"start\":65188},{\"end\":65204,\"start\":65199},{\"end\":65216,\"start\":65212},{\"end\":65231,\"start\":65225},{\"end\":65620,\"start\":65617},{\"end\":65632,\"start\":65625},{\"end\":65645,\"start\":65641},{\"end\":65656,\"start\":65651},{\"end\":65669,\"start\":65662},{\"end\":65686,\"start\":65677},{\"end\":65698,\"start\":65692},{\"end\":65713,\"start\":65708},{\"end\":65727,\"start\":65721},{\"end\":66111,\"start\":66108},{\"end\":66121,\"start\":66116},{\"end\":66134,\"start\":66127},{\"end\":66146,\"start\":66142},{\"end\":66151,\"start\":66147},{\"end\":66165,\"start\":66156},{\"end\":66177,\"start\":66171},{\"end\":66195,\"start\":66190},{\"end\":66209,\"start\":66203},{\"end\":66574,\"start\":66566},{\"end\":66584,\"start\":66579},{\"end\":66597,\"start\":66590},{\"end\":66608,\"start\":66602},{\"end\":66623,\"start\":66616},{\"end\":66634,\"start\":66630},{\"end\":66656,\"start\":66651},{\"end\":67022,\"start\":67015},{\"end\":67039,\"start\":67031},{\"end\":67054,\"start\":67051},{\"end\":67071,\"start\":67064},{\"end\":67457,\"start\":67447},{\"end\":67478,\"start\":67471},{\"end\":67496,\"start\":67491},{\"end\":67517,\"start\":67508},{\"end\":68257,\"start\":68248},{\"end\":68277,\"start\":68269},{\"end\":68290,\"start\":68286},{\"end\":68622,\"start\":68617},{\"end\":68637,\"start\":68630},{\"end\":68661,\"start\":68654},{\"end\":68674,\"start\":68669},{\"end\":69028,\"start\":69022},{\"end\":69040,\"start\":69035},{\"end\":69050,\"start\":69045},{\"end\":69064,\"start\":69058},{\"end\":69075,\"start\":69069},{\"end\":69087,\"start\":69082},{\"end\":69103,\"start\":69096},{\"end\":69481,\"start\":69479},{\"end\":69503,\"start\":69496},{\"end\":69520,\"start\":69512},{\"end\":69522,\"start\":69521},{\"end\":69898,\"start\":69891},{\"end\":69905,\"start\":69903},{\"end\":69915,\"start\":69910},{\"end\":69929,\"start\":69926},{\"end\":69943,\"start\":69936},{\"end\":69948,\"start\":69944},{\"end\":69962,\"start\":69955},{\"end\":69974,\"start\":69969},{\"end\":70337,\"start\":70331},{\"end\":70354,\"start\":70349},{\"end\":70365,\"start\":70360},{\"end\":70664,\"start\":70657},{\"end\":70680,\"start\":70672},{\"end\":70698,\"start\":70689},{\"end\":70714,\"start\":70710},{\"end\":70725,\"start\":70721},{\"end\":70742,\"start\":70735},{\"end\":70755,\"start\":70749},{\"end\":70767,\"start\":70764},{\"end\":70780,\"start\":70773},{\"end\":70797,\"start\":70789},{\"end\":71226,\"start\":71221},{\"end\":71242,\"start\":71237},{\"end\":71259,\"start\":71253},{\"end\":71275,\"start\":71268},{\"end\":71288,\"start\":71282},{\"end\":71301,\"start\":71294},{\"end\":71316,\"start\":71312},{\"end\":71336,\"start\":71330},{\"end\":71781,\"start\":71776},{\"end\":71798,\"start\":71792},{\"end\":71819,\"start\":71805},{\"end\":71833,\"start\":71826},{\"end\":71844,\"start\":71840},{\"end\":71863,\"start\":71856},{\"end\":72193,\"start\":72188},{\"end\":72209,\"start\":72204},{\"end\":72226,\"start\":72220},{\"end\":72515,\"start\":72509},{\"end\":72531,\"start\":72522},{\"end\":72848,\"start\":72838},{\"end\":72863,\"start\":72857},{\"end\":72874,\"start\":72869},{\"end\":72891,\"start\":72885},{\"end\":72907,\"start\":72901},{\"end\":73199,\"start\":73193},{\"end\":73218,\"start\":73211},{\"end\":73466,\"start\":73460},{\"end\":73480,\"start\":73476},{\"end\":73494,\"start\":73490},{\"end\":73508,\"start\":73503},{\"end\":73525,\"start\":73520},{\"end\":73538,\"start\":73533},{\"end\":73540,\"start\":73539},{\"end\":73554,\"start\":73548},{\"end\":73568,\"start\":73563},{\"end\":74006,\"start\":74003},{\"end\":74019,\"start\":74013},{\"end\":74034,\"start\":74027},{\"end\":74048,\"start\":74041},{\"end\":74053,\"start\":74049},{\"end\":74477,\"start\":74474},{\"end\":74491,\"start\":74484},{\"end\":74503,\"start\":74499},{\"end\":74525,\"start\":74517},{\"end\":74538,\"start\":74531},{\"end\":74554,\"start\":74545},{\"end\":74568,\"start\":74561},{\"end\":74573,\"start\":74569},{\"end\":74583,\"start\":74580},{\"end\":75040,\"start\":75037},{\"end\":75052,\"start\":75047},{\"end\":75065,\"start\":75059},{\"end\":75076,\"start\":75071},{\"end\":75085,\"start\":75082},{\"end\":75099,\"start\":75093},{\"end\":75395,\"start\":75390},{\"end\":75403,\"start\":75401},{\"end\":75414,\"start\":75409},{\"end\":75431,\"start\":75426},{\"end\":75446,\"start\":75440},{\"end\":75462,\"start\":75453},{\"end\":75477,\"start\":75473},{\"end\":75495,\"start\":75488},{\"end\":75508,\"start\":75503},{\"end\":75521,\"start\":75515},{\"end\":75920,\"start\":75915},{\"end\":75932,\"start\":75926},{\"end\":75943,\"start\":75939},{\"end\":75963,\"start\":75956},{\"end\":75973,\"start\":75971},{\"end\":75983,\"start\":75979},{\"end\":75993,\"start\":75988},{\"end\":76313,\"start\":76306},{\"end\":76327,\"start\":76319},{\"end\":76339,\"start\":76332},{\"end\":76352,\"start\":76345},{\"end\":76359,\"start\":76357},{\"end\":76616,\"start\":76612},{\"end\":76632,\"start\":76626},{\"end\":76649,\"start\":76640},{\"end\":76669,\"start\":76661},{\"end\":76682,\"start\":76675},{\"end\":76699,\"start\":76693},{\"end\":76710,\"start\":76705},{\"end\":76721,\"start\":76717},{\"end\":76735,\"start\":76730},{\"end\":77205,\"start\":77201},{\"end\":77223,\"start\":77215},{\"end\":77235,\"start\":77232},{\"end\":77250,\"start\":77244},{\"end\":77261,\"start\":77256},{\"end\":77272,\"start\":77268},{\"end\":77288,\"start\":77281},{\"end\":77301,\"start\":77296},{\"end\":77635,\"start\":77633},{\"end\":77647,\"start\":77640},{\"end\":77660,\"start\":77652},{\"end\":77673,\"start\":77667},{\"end\":77675,\"start\":77674},{\"end\":77693,\"start\":77687},{\"end\":78122,\"start\":78120},{\"end\":78135,\"start\":78128},{\"end\":78150,\"start\":78141},{\"end\":78164,\"start\":78158},{\"end\":78174,\"start\":78170},{\"end\":78186,\"start\":78180},{\"end\":78196,\"start\":78191},{\"end\":78211,\"start\":78202},{\"end\":78640,\"start\":78637},{\"end\":78648,\"start\":78645},{\"end\":78658,\"start\":78654},{\"end\":78668,\"start\":78664},{\"end\":79043,\"start\":79041},{\"end\":79055,\"start\":79049},{\"end\":79067,\"start\":79062},{\"end\":79080,\"start\":79072},{\"end\":79090,\"start\":79087},{\"end\":79100,\"start\":79096},{\"end\":79113,\"start\":79105},{\"end\":79516,\"start\":79508},{\"end\":79537,\"start\":79529},{\"end\":79548,\"start\":79544},{\"end\":79811,\"start\":79807},{\"end\":79819,\"start\":79816},{\"end\":79829,\"start\":79824},{\"end\":79842,\"start\":79835},{\"end\":79850,\"start\":79848},{\"end\":80209,\"start\":80204},{\"end\":80226,\"start\":80219},{\"end\":80236,\"start\":80233},{\"end\":80251,\"start\":80246},{\"end\":80582,\"start\":80578},{\"end\":80594,\"start\":80588},{\"end\":80609,\"start\":80602},{\"end\":80623,\"start\":80621}]", "bib_author_last_name": "[{\"end\":49770,\"start\":49760},{\"end\":49789,\"start\":49778},{\"end\":49798,\"start\":49795},{\"end\":49817,\"start\":49808},{\"end\":49834,\"start\":49828},{\"end\":50227,\"start\":50224},{\"end\":50243,\"start\":50237},{\"end\":50255,\"start\":50250},{\"end\":50272,\"start\":50264},{\"end\":50285,\"start\":50279},{\"end\":50298,\"start\":50293},{\"end\":50312,\"start\":50308},{\"end\":50338,\"start\":50324},{\"end\":50353,\"start\":50346},{\"end\":50366,\"start\":50360},{\"end\":50752,\"start\":50731},{\"end\":50769,\"start\":50762},{\"end\":50782,\"start\":50779},{\"end\":50794,\"start\":50789},{\"end\":50805,\"start\":50801},{\"end\":50819,\"start\":50813},{\"end\":50832,\"start\":50828},{\"end\":50846,\"start\":50840},{\"end\":50864,\"start\":50856},{\"end\":50874,\"start\":50866},{\"end\":51235,\"start\":51220},{\"end\":51251,\"start\":51245},{\"end\":51268,\"start\":51263},{\"end\":51275,\"start\":51270},{\"end\":51592,\"start\":51584},{\"end\":51599,\"start\":51597},{\"end\":51613,\"start\":51608},{\"end\":51625,\"start\":51620},{\"end\":51639,\"start\":51632},{\"end\":51656,\"start\":51646},{\"end\":51666,\"start\":51662},{\"end\":51681,\"start\":51676},{\"end\":51696,\"start\":51689},{\"end\":51704,\"start\":51698},{\"end\":52154,\"start\":52149},{\"end\":52173,\"start\":52166},{\"end\":52184,\"start\":52182},{\"end\":52203,\"start\":52195},{\"end\":52216,\"start\":52211},{\"end\":52234,\"start\":52227},{\"end\":52247,\"start\":52241},{\"end\":52617,\"start\":52612},{\"end\":52634,\"start\":52625},{\"end\":52649,\"start\":52643},{\"end\":52667,\"start\":52659},{\"end\":53020,\"start\":53014},{\"end\":53028,\"start\":53022},{\"end\":53215,\"start\":53210},{\"end\":53230,\"start\":53226},{\"end\":53242,\"start\":53237},{\"end\":53259,\"start\":53252},{\"end\":53275,\"start\":53269},{\"end\":53294,\"start\":53286},{\"end\":53314,\"start\":53303},{\"end\":53328,\"start\":53323},{\"end\":53343,\"start\":53337},{\"end\":53358,\"start\":53352},{\"end\":53725,\"start\":53720},{\"end\":53737,\"start\":53734},{\"end\":53756,\"start\":53746},{\"end\":53771,\"start\":53765},{\"end\":53790,\"start\":53782},{\"end\":53805,\"start\":53800},{\"end\":53818,\"start\":53814},{\"end\":53829,\"start\":53825},{\"end\":53842,\"start\":53837},{\"end\":54239,\"start\":54229},{\"end\":54257,\"start\":54247},{\"end\":54272,\"start\":54264},{\"end\":54281,\"start\":54277},{\"end\":54291,\"start\":54287},{\"end\":54305,\"start\":54298},{\"end\":54603,\"start\":54599},{\"end\":54612,\"start\":54607},{\"end\":54628,\"start\":54623},{\"end\":54637,\"start\":54630},{\"end\":55004,\"start\":55000},{\"end\":55016,\"start\":55012},{\"end\":55032,\"start\":55027},{\"end\":55046,\"start\":55039},{\"end\":55058,\"start\":55053},{\"end\":55436,\"start\":55432},{\"end\":55449,\"start\":55442},{\"end\":55467,\"start\":55460},{\"end\":55474,\"start\":55469},{\"end\":55828,\"start\":55825},{\"end\":55837,\"start\":55832},{\"end\":55852,\"start\":55847},{\"end\":55866,\"start\":55861},{\"end\":55881,\"start\":55875},{\"end\":55902,\"start\":55892},{\"end\":55911,\"start\":55904},{\"end\":56282,\"start\":56279},{\"end\":56296,\"start\":56291},{\"end\":56314,\"start\":56306},{\"end\":56326,\"start\":56323},{\"end\":56339,\"start\":56333},{\"end\":56352,\"start\":56347},{\"end\":56672,\"start\":56667},{\"end\":56687,\"start\":56681},{\"end\":56707,\"start\":56697},{\"end\":56719,\"start\":56714},{\"end\":56733,\"start\":56727},{\"end\":56746,\"start\":56741},{\"end\":56759,\"start\":56753},{\"end\":57177,\"start\":57171},{\"end\":57193,\"start\":57183},{\"end\":57210,\"start\":57202},{\"end\":57222,\"start\":57217},{\"end\":57238,\"start\":57230},{\"end\":57252,\"start\":57246},{\"end\":57264,\"start\":57261},{\"end\":57276,\"start\":57271},{\"end\":57289,\"start\":57282},{\"end\":57309,\"start\":57301},{\"end\":57723,\"start\":57718},{\"end\":57736,\"start\":57734},{\"end\":57750,\"start\":57745},{\"end\":57765,\"start\":57757},{\"end\":57780,\"start\":57773},{\"end\":57804,\"start\":57797},{\"end\":57829,\"start\":57813},{\"end\":57842,\"start\":57836},{\"end\":57853,\"start\":57848},{\"end\":57869,\"start\":57862},{\"end\":58339,\"start\":58336},{\"end\":58350,\"start\":58346},{\"end\":58366,\"start\":58359},{\"end\":58385,\"start\":58378},{\"end\":58399,\"start\":58397},{\"end\":58415,\"start\":58406},{\"end\":58867,\"start\":58862},{\"end\":58879,\"start\":58875},{\"end\":58901,\"start\":58889},{\"end\":58914,\"start\":58909},{\"end\":58927,\"start\":58921},{\"end\":59320,\"start\":59315},{\"end\":59328,\"start\":59322},{\"end\":59556,\"start\":59539},{\"end\":59572,\"start\":59565},{\"end\":59590,\"start\":59582},{\"end\":60073,\"start\":60068},{\"end\":60089,\"start\":60081},{\"end\":60105,\"start\":60100},{\"end\":60553,\"start\":60549},{\"end\":60560,\"start\":60558},{\"end\":60572,\"start\":60570},{\"end\":60598,\"start\":60583},{\"end\":60613,\"start\":60608},{\"end\":60980,\"start\":60976},{\"end\":60989,\"start\":60987},{\"end\":61004,\"start\":61001},{\"end\":61018,\"start\":61013},{\"end\":61352,\"start\":61349},{\"end\":61364,\"start\":61360},{\"end\":61378,\"start\":61373},{\"end\":61389,\"start\":61386},{\"end\":61404,\"start\":61401},{\"end\":61730,\"start\":61727},{\"end\":61740,\"start\":61737},{\"end\":61751,\"start\":61742},{\"end\":61763,\"start\":61760},{\"end\":61770,\"start\":61765},{\"end\":62199,\"start\":62191},{\"end\":62210,\"start\":62207},{\"end\":62223,\"start\":62219},{\"end\":62241,\"start\":62232},{\"end\":62257,\"start\":62250},{\"end\":62270,\"start\":62265},{\"end\":62291,\"start\":62281},{\"end\":62617,\"start\":62612},{\"end\":62635,\"start\":62627},{\"end\":62647,\"start\":62644},{\"end\":62663,\"start\":62653},{\"end\":62675,\"start\":62670},{\"end\":62692,\"start\":62684},{\"end\":62707,\"start\":62701},{\"end\":62717,\"start\":62714},{\"end\":62732,\"start\":62727},{\"end\":62745,\"start\":62738},{\"end\":63093,\"start\":63087},{\"end\":63109,\"start\":63099},{\"end\":63304,\"start\":63301},{\"end\":63316,\"start\":63314},{\"end\":63330,\"start\":63324},{\"end\":63345,\"start\":63341},{\"end\":63677,\"start\":63674},{\"end\":63688,\"start\":63686},{\"end\":63701,\"start\":63697},{\"end\":63710,\"start\":63707},{\"end\":63725,\"start\":63721},{\"end\":63739,\"start\":63733},{\"end\":63753,\"start\":63750},{\"end\":64160,\"start\":64158},{\"end\":64168,\"start\":64165},{\"end\":64181,\"start\":64179},{\"end\":64198,\"start\":64193},{\"end\":64210,\"start\":64208},{\"end\":64221,\"start\":64216},{\"end\":64234,\"start\":64230},{\"end\":64246,\"start\":64244},{\"end\":64255,\"start\":64251},{\"end\":64265,\"start\":64262},{\"end\":64674,\"start\":64669},{\"end\":64687,\"start\":64685},{\"end\":64698,\"start\":64696},{\"end\":64714,\"start\":64708},{\"end\":64725,\"start\":64721},{\"end\":64735,\"start\":64731},{\"end\":64747,\"start\":64744},{\"end\":64762,\"start\":64757},{\"end\":65197,\"start\":65195},{\"end\":65210,\"start\":65205},{\"end\":65223,\"start\":65217},{\"end\":65235,\"start\":65232},{\"end\":65623,\"start\":65621},{\"end\":65639,\"start\":65633},{\"end\":65649,\"start\":65646},{\"end\":65660,\"start\":65657},{\"end\":65675,\"start\":65670},{\"end\":65690,\"start\":65687},{\"end\":65706,\"start\":65699},{\"end\":65719,\"start\":65714},{\"end\":65734,\"start\":65728},{\"end\":66114,\"start\":66112},{\"end\":66125,\"start\":66122},{\"end\":66140,\"start\":66135},{\"end\":66154,\"start\":66152},{\"end\":66169,\"start\":66166},{\"end\":66188,\"start\":66178},{\"end\":66201,\"start\":66196},{\"end\":66216,\"start\":66210},{\"end\":66577,\"start\":66575},{\"end\":66588,\"start\":66585},{\"end\":66600,\"start\":66598},{\"end\":66614,\"start\":66609},{\"end\":66628,\"start\":66624},{\"end\":66638,\"start\":66635},{\"end\":66649,\"start\":66640},{\"end\":66660,\"start\":66657},{\"end\":66672,\"start\":66662},{\"end\":67029,\"start\":67023},{\"end\":67049,\"start\":67040},{\"end\":67062,\"start\":67055},{\"end\":67080,\"start\":67072},{\"end\":67469,\"start\":67458},{\"end\":67489,\"start\":67479},{\"end\":67506,\"start\":67497},{\"end\":67530,\"start\":67518},{\"end\":68267,\"start\":68258},{\"end\":68284,\"start\":68278},{\"end\":68294,\"start\":68291},{\"end\":68628,\"start\":68623},{\"end\":68643,\"start\":68638},{\"end\":68652,\"start\":68645},{\"end\":68667,\"start\":68662},{\"end\":68683,\"start\":68675},{\"end\":68694,\"start\":68685},{\"end\":69033,\"start\":69029},{\"end\":69043,\"start\":69041},{\"end\":69056,\"start\":69051},{\"end\":69067,\"start\":69065},{\"end\":69080,\"start\":69076},{\"end\":69094,\"start\":69088},{\"end\":69112,\"start\":69104},{\"end\":69494,\"start\":69482},{\"end\":69510,\"start\":69504},{\"end\":69525,\"start\":69523},{\"end\":69533,\"start\":69527},{\"end\":69901,\"start\":69899},{\"end\":69908,\"start\":69906},{\"end\":69924,\"start\":69916},{\"end\":69934,\"start\":69930},{\"end\":69953,\"start\":69949},{\"end\":69967,\"start\":69963},{\"end\":69982,\"start\":69975},{\"end\":69990,\"start\":69984},{\"end\":70347,\"start\":70338},{\"end\":70358,\"start\":70355},{\"end\":70371,\"start\":70366},{\"end\":70670,\"start\":70665},{\"end\":70687,\"start\":70681},{\"end\":70708,\"start\":70699},{\"end\":70719,\"start\":70715},{\"end\":70733,\"start\":70726},{\"end\":70747,\"start\":70743},{\"end\":70762,\"start\":70756},{\"end\":70771,\"start\":70768},{\"end\":70787,\"start\":70781},{\"end\":70803,\"start\":70798},{\"end\":71235,\"start\":71227},{\"end\":71251,\"start\":71243},{\"end\":71266,\"start\":71260},{\"end\":71280,\"start\":71276},{\"end\":71292,\"start\":71289},{\"end\":71310,\"start\":71302},{\"end\":71328,\"start\":71317},{\"end\":71340,\"start\":71337},{\"end\":71790,\"start\":71782},{\"end\":71803,\"start\":71799},{\"end\":71824,\"start\":71820},{\"end\":71838,\"start\":71834},{\"end\":71854,\"start\":71845},{\"end\":71874,\"start\":71864},{\"end\":72202,\"start\":72194},{\"end\":72218,\"start\":72210},{\"end\":72230,\"start\":72227},{\"end\":72520,\"start\":72516},{\"end\":72536,\"start\":72532},{\"end\":72855,\"start\":72849},{\"end\":72867,\"start\":72864},{\"end\":72883,\"start\":72875},{\"end\":72899,\"start\":72892},{\"end\":72916,\"start\":72908},{\"end\":73209,\"start\":73200},{\"end\":73226,\"start\":73219},{\"end\":73474,\"start\":73467},{\"end\":73488,\"start\":73481},{\"end\":73501,\"start\":73495},{\"end\":73518,\"start\":73509},{\"end\":73531,\"start\":73526},{\"end\":73546,\"start\":73541},{\"end\":73561,\"start\":73555},{\"end\":73579,\"start\":73569},{\"end\":74011,\"start\":74007},{\"end\":74025,\"start\":74020},{\"end\":74039,\"start\":74035},{\"end\":74058,\"start\":74054},{\"end\":74482,\"start\":74478},{\"end\":74497,\"start\":74492},{\"end\":74515,\"start\":74504},{\"end\":74529,\"start\":74526},{\"end\":74543,\"start\":74539},{\"end\":74559,\"start\":74555},{\"end\":74578,\"start\":74574},{\"end\":74589,\"start\":74584},{\"end\":75045,\"start\":75041},{\"end\":75057,\"start\":75053},{\"end\":75069,\"start\":75066},{\"end\":75080,\"start\":75077},{\"end\":75091,\"start\":75086},{\"end\":75105,\"start\":75100},{\"end\":75399,\"start\":75396},{\"end\":75407,\"start\":75404},{\"end\":75424,\"start\":75415},{\"end\":75438,\"start\":75432},{\"end\":75451,\"start\":75447},{\"end\":75471,\"start\":75463},{\"end\":75486,\"start\":75478},{\"end\":75501,\"start\":75496},{\"end\":75513,\"start\":75509},{\"end\":75529,\"start\":75522},{\"end\":75924,\"start\":75921},{\"end\":75937,\"start\":75933},{\"end\":75954,\"start\":75944},{\"end\":75969,\"start\":75964},{\"end\":75977,\"start\":75974},{\"end\":75986,\"start\":75984},{\"end\":75998,\"start\":75994},{\"end\":76317,\"start\":76314},{\"end\":76330,\"start\":76328},{\"end\":76343,\"start\":76340},{\"end\":76355,\"start\":76353},{\"end\":76362,\"start\":76360},{\"end\":76624,\"start\":76617},{\"end\":76638,\"start\":76633},{\"end\":76659,\"start\":76650},{\"end\":76673,\"start\":76670},{\"end\":76691,\"start\":76683},{\"end\":76703,\"start\":76700},{\"end\":76715,\"start\":76711},{\"end\":76728,\"start\":76722},{\"end\":76741,\"start\":76736},{\"end\":77213,\"start\":77206},{\"end\":77230,\"start\":77224},{\"end\":77242,\"start\":77236},{\"end\":77254,\"start\":77251},{\"end\":77266,\"start\":77262},{\"end\":77279,\"start\":77273},{\"end\":77294,\"start\":77289},{\"end\":77307,\"start\":77302},{\"end\":77638,\"start\":77636},{\"end\":77650,\"start\":77648},{\"end\":77665,\"start\":77661},{\"end\":77685,\"start\":77676},{\"end\":77697,\"start\":77694},{\"end\":78126,\"start\":78123},{\"end\":78139,\"start\":78136},{\"end\":78156,\"start\":78151},{\"end\":78168,\"start\":78165},{\"end\":78178,\"start\":78175},{\"end\":78189,\"start\":78187},{\"end\":78200,\"start\":78197},{\"end\":78215,\"start\":78212},{\"end\":78643,\"start\":78641},{\"end\":78652,\"start\":78649},{\"end\":78662,\"start\":78659},{\"end\":78672,\"start\":78669},{\"end\":79047,\"start\":79044},{\"end\":79060,\"start\":79056},{\"end\":79070,\"start\":79068},{\"end\":79085,\"start\":79081},{\"end\":79094,\"start\":79091},{\"end\":79103,\"start\":79101},{\"end\":79117,\"start\":79114},{\"end\":79527,\"start\":79517},{\"end\":79542,\"start\":79538},{\"end\":79552,\"start\":79549},{\"end\":79558,\"start\":79554},{\"end\":79814,\"start\":79812},{\"end\":79822,\"start\":79820},{\"end\":79833,\"start\":79830},{\"end\":79846,\"start\":79843},{\"end\":79855,\"start\":79851},{\"end\":80217,\"start\":80210},{\"end\":80231,\"start\":80227},{\"end\":80244,\"start\":80237},{\"end\":80256,\"start\":80252},{\"end\":80586,\"start\":80583},{\"end\":80600,\"start\":80595},{\"end\":80619,\"start\":80610},{\"end\":80631,\"start\":80624}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":221378802},\"end\":50144,\"start\":49662},{\"attributes\":{\"doi\":\"arXiv:2204.01691\",\"id\":\"b1\"},\"end\":50667,\"start\":50146},{\"attributes\":{\"doi\":\"arXiv:2204.14198\",\"id\":\"b2\"},\"end\":51160,\"start\":50669},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":145255457},\"end\":51467,\"start\":51162},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4673790},\"end\":52105,\"start\":51469},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3180429},\"end\":52540,\"start\":52107},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":245334889},\"end\":52982,\"start\":52542},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7454683},\"end\":53165,\"start\":52984},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":218971783},\"end\":53712,\"start\":53167},{\"attributes\":{\"doi\":\"arXiv:1709.06158\",\"id\":\"b9\"},\"end\":54174,\"start\":53714},{\"attributes\":{\"doi\":\"arXiv:2205.01883\",\"id\":\"b10\"},\"end\":54512,\"start\":54176},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":209414687},\"end\":54899,\"start\":54514},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":54078068},\"end\":55376,\"start\":54901},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":227305513},\"end\":55753,\"start\":55378},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7684883},\"end\":56239,\"start\":55755},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":35985986},\"end\":56622,\"start\":56241},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":248506192},\"end\":57164,\"start\":56624},{\"attributes\":{\"doi\":\"arXiv:2206.06994\",\"id\":\"b17\"},\"end\":57649,\"start\":57166},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":46979001},\"end\":58230,\"start\":57651},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":249455925},\"end\":58755,\"start\":58232},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8081284},\"end\":59259,\"start\":58757},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13332429},\"end\":59464,\"start\":59261},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232417303},\"end\":59982,\"start\":59466},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12061055},\"end\":60477,\"start\":59984},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":227228335},\"end\":60905,\"start\":60479},{\"attributes\":{\"doi\":\"2021b. 2\",\"id\":\"b25\",\"matched_paper_id\":232335373},\"end\":61262,\"start\":60907},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":220634784},\"end\":61657,\"start\":61264},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":252781179},\"end\":62119,\"start\":61659},{\"attributes\":{\"doi\":\"arXiv:2005.00700\",\"id\":\"b28\"},\"end\":62550,\"start\":62121},{\"attributes\":{\"doi\":\"arXiv:1712.05474\",\"id\":\"b29\"},\"end\":63016,\"start\":62552},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":144503525},\"end\":63295,\"start\":63018},{\"attributes\":{\"doi\":\"arXiv:1809.01696\",\"id\":\"b31\"},\"end\":63592,\"start\":63297},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":231880022},\"end\":64084,\"start\":63594},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":215754208},\"end\":64597,\"start\":64086},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52283515},\"end\":65088,\"start\":64599},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":199453025},\"end\":65525,\"start\":65090},{\"attributes\":{\"doi\":\"arXiv:2209.09513\",\"id\":\"b36\"},\"end\":66018,\"start\":65527},{\"attributes\":{\"doi\":\"arXiv:2209.14610\",\"id\":\"b37\"},\"end\":66489,\"start\":66020},{\"attributes\":{\"doi\":\"arXiv:2204.11167\",\"id\":\"b38\"},\"end\":66937,\"start\":66491},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":173991173},\"end\":67389,\"start\":66939},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":218889746},\"end\":68190,\"start\":67391},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":234482879},\"end\":68557,\"start\":68192},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":19119291},\"end\":68961,\"start\":68559},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":49317780},\"end\":69418,\"start\":68963},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":127956465},\"end\":69807,\"start\":69420},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":214264259},\"end\":70329,\"start\":69809},{\"attributes\":{\"doi\":\"arXiv:1806.03822\",\"id\":\"b46\"},\"end\":70609,\"start\":70331},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":91184540},\"end\":71140,\"start\":70611},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":208617407},\"end\":71698,\"start\":71142},{\"attributes\":{\"doi\":\"arXiv:2010.03768\",\"id\":\"b49\"},\"end\":72127,\"start\":71700},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b50\",\"matched_paper_id\":237396838},\"end\":72451,\"start\":72129},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":206685792},\"end\":72747,\"start\":72453},{\"attributes\":{\"doi\":\"arXiv:2108.04927\",\"id\":\"b52\"},\"end\":73161,\"start\":72749},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2209131},\"end\":73431,\"start\":73163},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":13756489},\"end\":73870,\"start\":73433},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":3993448},\"end\":74369,\"start\":73872},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":53735892},\"end\":74966,\"start\":74371},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":252967858},\"end\":75388,\"start\":74968},{\"attributes\":{\"doi\":\"arXiv:2206.07682\",\"id\":\"b58\"},\"end\":75842,\"start\":75390},{\"attributes\":{\"doi\":\"arXiv:2201.11903\",\"id\":\"b59\"},\"end\":76231,\"start\":75844},{\"attributes\":{\"id\":\"b60\"},\"end\":76522,\"start\":76233},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":102352810},\"end\":77123,\"start\":76524},{\"attributes\":{\"doi\":\"arXiv:1911.00357\",\"id\":\"b62\"},\"end\":77568,\"start\":77125},{\"attributes\":{\"doi\":\"2021. 10\",\"id\":\"b63\",\"matched_paper_id\":244907062},\"end\":78060,\"start\":77570},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":102485920},\"end\":78559,\"start\":78062},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":206594535},\"end\":79039,\"start\":78561},{\"attributes\":{\"doi\":\"arXiv:2112.11691\",\"id\":\"b66\"},\"end\":79504,\"start\":79041},{\"attributes\":{\"doi\":\"arXiv:2112.08359\",\"id\":\"b67\"},\"end\":79739,\"start\":79506},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":195657908},\"end\":80141,\"start\":79741},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":53734356},\"end\":80527,\"start\":80143},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":5714907},\"end\":80893,\"start\":80529}]", "bib_title": "[{\"end\":49752,\"start\":49662},{\"end\":51210,\"start\":51162},{\"end\":51576,\"start\":51469},{\"end\":52137,\"start\":52107},{\"end\":52603,\"start\":52542},{\"end\":53010,\"start\":52984},{\"end\":53204,\"start\":53167},{\"end\":54585,\"start\":54514},{\"end\":54991,\"start\":54901},{\"end\":55423,\"start\":55378},{\"end\":55816,\"start\":55755},{\"end\":56268,\"start\":56241},{\"end\":56658,\"start\":56624},{\"end\":57709,\"start\":57651},{\"end\":58329,\"start\":58232},{\"end\":58855,\"start\":58757},{\"end\":59311,\"start\":59261},{\"end\":59527,\"start\":59466},{\"end\":60058,\"start\":59984},{\"end\":60540,\"start\":60479},{\"end\":60967,\"start\":60907},{\"end\":61338,\"start\":61264},{\"end\":61716,\"start\":61659},{\"end\":63077,\"start\":63018},{\"end\":63668,\"start\":63594},{\"end\":64149,\"start\":64086},{\"end\":64658,\"start\":64599},{\"end\":65186,\"start\":65090},{\"end\":67013,\"start\":66939},{\"end\":67445,\"start\":67391},{\"end\":68246,\"start\":68192},{\"end\":68615,\"start\":68559},{\"end\":69020,\"start\":68963},{\"end\":69477,\"start\":69420},{\"end\":69889,\"start\":69809},{\"end\":70655,\"start\":70611},{\"end\":71219,\"start\":71142},{\"end\":72186,\"start\":72129},{\"end\":72507,\"start\":72453},{\"end\":73191,\"start\":73163},{\"end\":73458,\"start\":73433},{\"end\":74001,\"start\":73872},{\"end\":74472,\"start\":74371},{\"end\":75035,\"start\":74968},{\"end\":76610,\"start\":76524},{\"end\":77631,\"start\":77570},{\"end\":78118,\"start\":78062},{\"end\":78635,\"start\":78561},{\"end\":79805,\"start\":79741},{\"end\":80202,\"start\":80143},{\"end\":80576,\"start\":80529}]", "bib_author": "[{\"end\":49772,\"start\":49754},{\"end\":49791,\"start\":49772},{\"end\":49800,\"start\":49791},{\"end\":49819,\"start\":49800},{\"end\":49836,\"start\":49819},{\"end\":50229,\"start\":50216},{\"end\":50245,\"start\":50229},{\"end\":50257,\"start\":50245},{\"end\":50274,\"start\":50257},{\"end\":50287,\"start\":50274},{\"end\":50300,\"start\":50287},{\"end\":50314,\"start\":50300},{\"end\":50340,\"start\":50314},{\"end\":50355,\"start\":50340},{\"end\":50368,\"start\":50355},{\"end\":50754,\"start\":50726},{\"end\":50771,\"start\":50754},{\"end\":50784,\"start\":50771},{\"end\":50796,\"start\":50784},{\"end\":50807,\"start\":50796},{\"end\":50821,\"start\":50807},{\"end\":50834,\"start\":50821},{\"end\":50848,\"start\":50834},{\"end\":50866,\"start\":50848},{\"end\":50876,\"start\":50866},{\"end\":51237,\"start\":51212},{\"end\":51253,\"start\":51237},{\"end\":51270,\"start\":51253},{\"end\":51277,\"start\":51270},{\"end\":51594,\"start\":51578},{\"end\":51601,\"start\":51594},{\"end\":51615,\"start\":51601},{\"end\":51627,\"start\":51615},{\"end\":51641,\"start\":51627},{\"end\":51658,\"start\":51641},{\"end\":51668,\"start\":51658},{\"end\":51683,\"start\":51668},{\"end\":51698,\"start\":51683},{\"end\":51706,\"start\":51698},{\"end\":52156,\"start\":52139},{\"end\":52175,\"start\":52156},{\"end\":52186,\"start\":52175},{\"end\":52205,\"start\":52186},{\"end\":52218,\"start\":52205},{\"end\":52236,\"start\":52218},{\"end\":52249,\"start\":52236},{\"end\":52619,\"start\":52605},{\"end\":52636,\"start\":52619},{\"end\":52651,\"start\":52636},{\"end\":52669,\"start\":52651},{\"end\":53022,\"start\":53012},{\"end\":53030,\"start\":53022},{\"end\":53217,\"start\":53206},{\"end\":53232,\"start\":53217},{\"end\":53244,\"start\":53232},{\"end\":53261,\"start\":53244},{\"end\":53277,\"start\":53261},{\"end\":53296,\"start\":53277},{\"end\":53316,\"start\":53296},{\"end\":53330,\"start\":53316},{\"end\":53345,\"start\":53330},{\"end\":53360,\"start\":53345},{\"end\":53727,\"start\":53714},{\"end\":53739,\"start\":53727},{\"end\":53758,\"start\":53739},{\"end\":53773,\"start\":53758},{\"end\":53792,\"start\":53773},{\"end\":53807,\"start\":53792},{\"end\":53820,\"start\":53807},{\"end\":53831,\"start\":53820},{\"end\":53844,\"start\":53831},{\"end\":54241,\"start\":54221},{\"end\":54259,\"start\":54241},{\"end\":54274,\"start\":54259},{\"end\":54283,\"start\":54274},{\"end\":54293,\"start\":54283},{\"end\":54307,\"start\":54293},{\"end\":54605,\"start\":54587},{\"end\":54614,\"start\":54605},{\"end\":54630,\"start\":54614},{\"end\":54639,\"start\":54630},{\"end\":55006,\"start\":54993},{\"end\":55018,\"start\":55006},{\"end\":55034,\"start\":55018},{\"end\":55048,\"start\":55034},{\"end\":55060,\"start\":55048},{\"end\":55438,\"start\":55425},{\"end\":55451,\"start\":55438},{\"end\":55469,\"start\":55451},{\"end\":55476,\"start\":55469},{\"end\":55830,\"start\":55818},{\"end\":55839,\"start\":55830},{\"end\":55854,\"start\":55839},{\"end\":55868,\"start\":55854},{\"end\":55883,\"start\":55868},{\"end\":55904,\"start\":55883},{\"end\":55913,\"start\":55904},{\"end\":56284,\"start\":56270},{\"end\":56298,\"start\":56284},{\"end\":56316,\"start\":56298},{\"end\":56328,\"start\":56316},{\"end\":56341,\"start\":56328},{\"end\":56354,\"start\":56341},{\"end\":56674,\"start\":56660},{\"end\":56689,\"start\":56674},{\"end\":56709,\"start\":56689},{\"end\":56721,\"start\":56709},{\"end\":56735,\"start\":56721},{\"end\":56748,\"start\":56735},{\"end\":56761,\"start\":56748},{\"end\":57179,\"start\":57166},{\"end\":57195,\"start\":57179},{\"end\":57212,\"start\":57195},{\"end\":57224,\"start\":57212},{\"end\":57240,\"start\":57224},{\"end\":57254,\"start\":57240},{\"end\":57266,\"start\":57254},{\"end\":57278,\"start\":57266},{\"end\":57291,\"start\":57278},{\"end\":57311,\"start\":57291},{\"end\":57725,\"start\":57711},{\"end\":57738,\"start\":57725},{\"end\":57752,\"start\":57738},{\"end\":57767,\"start\":57752},{\"end\":57782,\"start\":57767},{\"end\":57806,\"start\":57782},{\"end\":57831,\"start\":57806},{\"end\":57844,\"start\":57831},{\"end\":57855,\"start\":57844},{\"end\":57871,\"start\":57855},{\"end\":58341,\"start\":58331},{\"end\":58352,\"start\":58341},{\"end\":58368,\"start\":58352},{\"end\":58387,\"start\":58368},{\"end\":58401,\"start\":58387},{\"end\":58417,\"start\":58401},{\"end\":58869,\"start\":58857},{\"end\":58881,\"start\":58869},{\"end\":58903,\"start\":58881},{\"end\":58916,\"start\":58903},{\"end\":58929,\"start\":58916},{\"end\":59322,\"start\":59313},{\"end\":59330,\"start\":59322},{\"end\":59558,\"start\":59529},{\"end\":59574,\"start\":59558},{\"end\":59592,\"start\":59574},{\"end\":60075,\"start\":60060},{\"end\":60091,\"start\":60075},{\"end\":60107,\"start\":60091},{\"end\":60555,\"start\":60542},{\"end\":60562,\"start\":60555},{\"end\":60574,\"start\":60562},{\"end\":60600,\"start\":60574},{\"end\":60615,\"start\":60600},{\"end\":60982,\"start\":60969},{\"end\":60991,\"start\":60982},{\"end\":61006,\"start\":60991},{\"end\":61020,\"start\":61006},{\"end\":61354,\"start\":61340},{\"end\":61366,\"start\":61354},{\"end\":61380,\"start\":61366},{\"end\":61391,\"start\":61380},{\"end\":61406,\"start\":61391},{\"end\":61732,\"start\":61718},{\"end\":61742,\"start\":61732},{\"end\":61753,\"start\":61742},{\"end\":61765,\"start\":61753},{\"end\":61772,\"start\":61765},{\"end\":62201,\"start\":62184},{\"end\":62212,\"start\":62201},{\"end\":62225,\"start\":62212},{\"end\":62243,\"start\":62225},{\"end\":62259,\"start\":62243},{\"end\":62272,\"start\":62259},{\"end\":62293,\"start\":62272},{\"end\":62619,\"start\":62607},{\"end\":62637,\"start\":62619},{\"end\":62649,\"start\":62637},{\"end\":62665,\"start\":62649},{\"end\":62677,\"start\":62665},{\"end\":62694,\"start\":62677},{\"end\":62709,\"start\":62694},{\"end\":62719,\"start\":62709},{\"end\":62734,\"start\":62719},{\"end\":62747,\"start\":62734},{\"end\":63095,\"start\":63079},{\"end\":63111,\"start\":63095},{\"end\":63306,\"start\":63297},{\"end\":63318,\"start\":63306},{\"end\":63332,\"start\":63318},{\"end\":63347,\"start\":63332},{\"end\":63679,\"start\":63670},{\"end\":63690,\"start\":63679},{\"end\":63703,\"start\":63690},{\"end\":63712,\"start\":63703},{\"end\":63727,\"start\":63712},{\"end\":63741,\"start\":63727},{\"end\":63755,\"start\":63741},{\"end\":64162,\"start\":64151},{\"end\":64170,\"start\":64162},{\"end\":64183,\"start\":64170},{\"end\":64200,\"start\":64183},{\"end\":64212,\"start\":64200},{\"end\":64223,\"start\":64212},{\"end\":64236,\"start\":64223},{\"end\":64248,\"start\":64236},{\"end\":64257,\"start\":64248},{\"end\":64267,\"start\":64257},{\"end\":64676,\"start\":64660},{\"end\":64689,\"start\":64676},{\"end\":64700,\"start\":64689},{\"end\":64716,\"start\":64700},{\"end\":64727,\"start\":64716},{\"end\":64737,\"start\":64727},{\"end\":64749,\"start\":64737},{\"end\":64764,\"start\":64749},{\"end\":65199,\"start\":65188},{\"end\":65212,\"start\":65199},{\"end\":65225,\"start\":65212},{\"end\":65237,\"start\":65225},{\"end\":65625,\"start\":65617},{\"end\":65641,\"start\":65625},{\"end\":65651,\"start\":65641},{\"end\":65662,\"start\":65651},{\"end\":65677,\"start\":65662},{\"end\":65692,\"start\":65677},{\"end\":65708,\"start\":65692},{\"end\":65721,\"start\":65708},{\"end\":65736,\"start\":65721},{\"end\":66116,\"start\":66108},{\"end\":66127,\"start\":66116},{\"end\":66142,\"start\":66127},{\"end\":66156,\"start\":66142},{\"end\":66171,\"start\":66156},{\"end\":66190,\"start\":66171},{\"end\":66203,\"start\":66190},{\"end\":66218,\"start\":66203},{\"end\":66579,\"start\":66566},{\"end\":66590,\"start\":66579},{\"end\":66602,\"start\":66590},{\"end\":66616,\"start\":66602},{\"end\":66630,\"start\":66616},{\"end\":66640,\"start\":66630},{\"end\":66651,\"start\":66640},{\"end\":66662,\"start\":66651},{\"end\":66674,\"start\":66662},{\"end\":67031,\"start\":67015},{\"end\":67051,\"start\":67031},{\"end\":67064,\"start\":67051},{\"end\":67082,\"start\":67064},{\"end\":67471,\"start\":67447},{\"end\":67491,\"start\":67471},{\"end\":67508,\"start\":67491},{\"end\":67532,\"start\":67508},{\"end\":68269,\"start\":68248},{\"end\":68286,\"start\":68269},{\"end\":68296,\"start\":68286},{\"end\":68630,\"start\":68617},{\"end\":68645,\"start\":68630},{\"end\":68654,\"start\":68645},{\"end\":68669,\"start\":68654},{\"end\":68685,\"start\":68669},{\"end\":68696,\"start\":68685},{\"end\":69035,\"start\":69022},{\"end\":69045,\"start\":69035},{\"end\":69058,\"start\":69045},{\"end\":69069,\"start\":69058},{\"end\":69082,\"start\":69069},{\"end\":69096,\"start\":69082},{\"end\":69114,\"start\":69096},{\"end\":69496,\"start\":69479},{\"end\":69512,\"start\":69496},{\"end\":69527,\"start\":69512},{\"end\":69535,\"start\":69527},{\"end\":69903,\"start\":69891},{\"end\":69910,\"start\":69903},{\"end\":69926,\"start\":69910},{\"end\":69936,\"start\":69926},{\"end\":69955,\"start\":69936},{\"end\":69969,\"start\":69955},{\"end\":69984,\"start\":69969},{\"end\":69992,\"start\":69984},{\"end\":70349,\"start\":70331},{\"end\":70360,\"start\":70349},{\"end\":70373,\"start\":70360},{\"end\":70672,\"start\":70657},{\"end\":70689,\"start\":70672},{\"end\":70710,\"start\":70689},{\"end\":70721,\"start\":70710},{\"end\":70735,\"start\":70721},{\"end\":70749,\"start\":70735},{\"end\":70764,\"start\":70749},{\"end\":70773,\"start\":70764},{\"end\":70789,\"start\":70773},{\"end\":70805,\"start\":70789},{\"end\":71237,\"start\":71221},{\"end\":71253,\"start\":71237},{\"end\":71268,\"start\":71253},{\"end\":71282,\"start\":71268},{\"end\":71294,\"start\":71282},{\"end\":71312,\"start\":71294},{\"end\":71330,\"start\":71312},{\"end\":71342,\"start\":71330},{\"end\":71792,\"start\":71776},{\"end\":71805,\"start\":71792},{\"end\":71826,\"start\":71805},{\"end\":71840,\"start\":71826},{\"end\":71856,\"start\":71840},{\"end\":71876,\"start\":71856},{\"end\":72204,\"start\":72188},{\"end\":72220,\"start\":72204},{\"end\":72232,\"start\":72220},{\"end\":72522,\"start\":72509},{\"end\":72538,\"start\":72522},{\"end\":72857,\"start\":72838},{\"end\":72869,\"start\":72857},{\"end\":72885,\"start\":72869},{\"end\":72901,\"start\":72885},{\"end\":72918,\"start\":72901},{\"end\":73211,\"start\":73193},{\"end\":73228,\"start\":73211},{\"end\":73476,\"start\":73460},{\"end\":73490,\"start\":73476},{\"end\":73503,\"start\":73490},{\"end\":73520,\"start\":73503},{\"end\":73533,\"start\":73520},{\"end\":73548,\"start\":73533},{\"end\":73563,\"start\":73548},{\"end\":73581,\"start\":73563},{\"end\":74013,\"start\":74003},{\"end\":74027,\"start\":74013},{\"end\":74041,\"start\":74027},{\"end\":74060,\"start\":74041},{\"end\":74484,\"start\":74474},{\"end\":74499,\"start\":74484},{\"end\":74517,\"start\":74499},{\"end\":74531,\"start\":74517},{\"end\":74545,\"start\":74531},{\"end\":74561,\"start\":74545},{\"end\":74580,\"start\":74561},{\"end\":74591,\"start\":74580},{\"end\":75047,\"start\":75037},{\"end\":75059,\"start\":75047},{\"end\":75071,\"start\":75059},{\"end\":75082,\"start\":75071},{\"end\":75093,\"start\":75082},{\"end\":75107,\"start\":75093},{\"end\":75401,\"start\":75390},{\"end\":75409,\"start\":75401},{\"end\":75426,\"start\":75409},{\"end\":75440,\"start\":75426},{\"end\":75453,\"start\":75440},{\"end\":75473,\"start\":75453},{\"end\":75488,\"start\":75473},{\"end\":75503,\"start\":75488},{\"end\":75515,\"start\":75503},{\"end\":75531,\"start\":75515},{\"end\":75926,\"start\":75915},{\"end\":75939,\"start\":75926},{\"end\":75956,\"start\":75939},{\"end\":75971,\"start\":75956},{\"end\":75979,\"start\":75971},{\"end\":75988,\"start\":75979},{\"end\":76000,\"start\":75988},{\"end\":76319,\"start\":76306},{\"end\":76332,\"start\":76319},{\"end\":76345,\"start\":76332},{\"end\":76357,\"start\":76345},{\"end\":76364,\"start\":76357},{\"end\":76626,\"start\":76612},{\"end\":76640,\"start\":76626},{\"end\":76661,\"start\":76640},{\"end\":76675,\"start\":76661},{\"end\":76693,\"start\":76675},{\"end\":76705,\"start\":76693},{\"end\":76717,\"start\":76705},{\"end\":76730,\"start\":76717},{\"end\":76743,\"start\":76730},{\"end\":77215,\"start\":77201},{\"end\":77232,\"start\":77215},{\"end\":77244,\"start\":77232},{\"end\":77256,\"start\":77244},{\"end\":77268,\"start\":77256},{\"end\":77281,\"start\":77268},{\"end\":77296,\"start\":77281},{\"end\":77309,\"start\":77296},{\"end\":77640,\"start\":77633},{\"end\":77652,\"start\":77640},{\"end\":77667,\"start\":77652},{\"end\":77687,\"start\":77667},{\"end\":77699,\"start\":77687},{\"end\":78128,\"start\":78120},{\"end\":78141,\"start\":78128},{\"end\":78158,\"start\":78141},{\"end\":78170,\"start\":78158},{\"end\":78180,\"start\":78170},{\"end\":78191,\"start\":78180},{\"end\":78202,\"start\":78191},{\"end\":78217,\"start\":78202},{\"end\":78645,\"start\":78637},{\"end\":78654,\"start\":78645},{\"end\":78664,\"start\":78654},{\"end\":78674,\"start\":78664},{\"end\":79049,\"start\":79041},{\"end\":79062,\"start\":79049},{\"end\":79072,\"start\":79062},{\"end\":79087,\"start\":79072},{\"end\":79096,\"start\":79087},{\"end\":79105,\"start\":79096},{\"end\":79119,\"start\":79105},{\"end\":79529,\"start\":79508},{\"end\":79544,\"start\":79529},{\"end\":79554,\"start\":79544},{\"end\":79560,\"start\":79554},{\"end\":79816,\"start\":79807},{\"end\":79824,\"start\":79816},{\"end\":79835,\"start\":79824},{\"end\":79848,\"start\":79835},{\"end\":79857,\"start\":79848},{\"end\":80219,\"start\":80204},{\"end\":80233,\"start\":80219},{\"end\":80246,\"start\":80233},{\"end\":80258,\"start\":80246},{\"end\":80588,\"start\":80578},{\"end\":80602,\"start\":80588},{\"end\":80621,\"start\":80602},{\"end\":80633,\"start\":80621}]", "bib_venue": "[{\"end\":49881,\"start\":49836},{\"end\":50214,\"start\":50146},{\"end\":50724,\"start\":50669},{\"end\":51299,\"start\":51277},{\"end\":51766,\"start\":51706},{\"end\":52299,\"start\":52249},{\"end\":52729,\"start\":52669},{\"end\":53061,\"start\":53030},{\"end\":53419,\"start\":53360},{\"end\":53921,\"start\":53860},{\"end\":54219,\"start\":54176},{\"end\":54684,\"start\":54639},{\"end\":55120,\"start\":55060},{\"end\":55536,\"start\":55476},{\"end\":55973,\"start\":55913},{\"end\":56414,\"start\":56354},{\"end\":56842,\"start\":56761},{\"end\":57378,\"start\":57327},{\"end\":57930,\"start\":57871},{\"end\":58477,\"start\":58417},{\"end\":58989,\"start\":58929},{\"end\":59351,\"start\":59330},{\"end\":59673,\"start\":59592},{\"end\":60184,\"start\":60107},{\"end\":60675,\"start\":60615},{\"end\":61078,\"start\":61028},{\"end\":61451,\"start\":61406},{\"end\":61880,\"start\":61772},{\"end\":62182,\"start\":62121},{\"end\":62605,\"start\":62552},{\"end\":63140,\"start\":63111},{\"end\":63418,\"start\":63363},{\"end\":63815,\"start\":63755},{\"end\":64312,\"start\":64267},{\"end\":64822,\"start\":64764},{\"end\":65296,\"start\":65237},{\"end\":65615,\"start\":65527},{\"end\":66106,\"start\":66020},{\"end\":66564,\"start\":66491},{\"end\":67142,\"start\":67082},{\"end\":67632,\"start\":67532},{\"end\":68356,\"start\":68296},{\"end\":68745,\"start\":68696},{\"end\":69174,\"start\":69114},{\"end\":69595,\"start\":69535},{\"end\":70052,\"start\":69992},{\"end\":70447,\"start\":70389},{\"end\":70855,\"start\":70805},{\"end\":71402,\"start\":71342},{\"end\":71774,\"start\":71700},{\"end\":72271,\"start\":72236},{\"end\":72576,\"start\":72538},{\"end\":72836,\"start\":72749},{\"end\":73287,\"start\":73228},{\"end\":73640,\"start\":73581},{\"end\":74105,\"start\":74060},{\"end\":74651,\"start\":74591},{\"end\":75166,\"start\":75107},{\"end\":75590,\"start\":75547},{\"end\":75913,\"start\":75844},{\"end\":76304,\"start\":76233},{\"end\":76803,\"start\":76743},{\"end\":77199,\"start\":77125},{\"end\":77801,\"start\":77707},{\"end\":78275,\"start\":78217},{\"end\":78751,\"start\":78674},{\"end\":79245,\"start\":79135},{\"end\":79917,\"start\":79857},{\"end\":80318,\"start\":80258},{\"end\":80693,\"start\":80633},{\"end\":56910,\"start\":56844},{\"end\":59741,\"start\":59675},{\"end\":60248,\"start\":60186},{\"end\":67746,\"start\":67634},{\"end\":78320,\"start\":78277},{\"end\":78815,\"start\":78753}]"}}}, "year": 2023, "month": 12, "day": 17}