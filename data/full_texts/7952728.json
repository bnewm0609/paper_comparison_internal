{"id": 7952728, "updated": "2023-09-30 17:57:01.034", "metadata": {"title": "Large-Scale Domain Adaptation via Teacher-Student Learning", "authors": "[{\"first\":\"Jinyu\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Seltzer\",\"middle\":[\"L.\"]},{\"first\":\"Xi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Rui\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Yifan\",\"last\":\"Gong\",\"middle\":[]}]", "venue": "INTERSPEECH", "journal": "2386-2390", "publication_date": {"year": 2017, "month": 8, "day": 17}, "abstract": "High accuracy speech recognition requires a large amount of transcribed data for supervised training. In the absence of such data, domain adaptation of a well-trained acoustic model can be performed, but even here, high accuracy usually requires significant labeled data from the target domain. In this work, we propose an approach to domain adaptation that does not require transcriptions but instead uses a corpus of unlabeled parallel data, consisting of pairs of samples from the source domain of the well-trained model and the desired target domain. To perform adaptation, we employ teacher/student (T/S) learning, in which the posterior probabilities generated by the source-domain model can be used in lieu of labels to train the target-domain model. We evaluate the proposed approach in two scenarios, adapting a clean acoustic model to noisy speech and adapting an adults speech acoustic model to children speech. Significant improvements in accuracy are obtained, with reductions in word error rate of up to 44% over the original source model without the need for transcribed data in the target domain. Moreover, we show that increasing the amount of unlabeled data results in additional model robustness, which is particularly beneficial when using simulated training data in the target-domain.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1708.05466", "mag": "2962894366", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1708-05466", "doi": "10.21437/interspeech.2017-519"}}, "content": {"source": {"pdf_hash": "195d13a69e736024360878521124c3016ae9ef1f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1708.05466v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1708.05466", "status": "GREEN"}}, "grobid": {"id": "3fe482df74fec0574f6ee512e7e10a05cd882a4a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/195d13a69e736024360878521124c3016ae9ef1f.txt", "contents": "\nLarge-Scale Domain Adaptation via Teacher-Student Learning\n\n\nJinyu Li \nMicrosoft AI and Research\nOne Microsoft Way98052RedmondWA\n\nMichael L Seltzer \nMicrosoft AI and Research\nOne Microsoft Way98052RedmondWA\n\nXi Wang \nMicrosoft AI and Research\nOne Microsoft Way98052RedmondWA\n\nRui Zhao \nMicrosoft AI and Research\nOne Microsoft Way98052RedmondWA\n\nYifan Gong ygong@microsoft.com \nMicrosoft AI and Research\nOne Microsoft Way98052RedmondWA\n\nLarge-Scale Domain Adaptation via Teacher-Student Learning\nIndex Terms: teacher-student learning, parallel unlabeled data\nHigh accuracy speech recognition requires a large amount of transcribed data for supervised training. In the absence of such data, domain adaptation of a well-trained acoustic model can be performed, but even here, high accuracy usually requires significant labeled data from the target domain. In this work, we propose an approach to domain adaptation that does not require transcriptions but instead uses a corpus of unlabeled parallel data, consisting of pairs of samples from the source domain of the well-trained model and the desired target domain. To perform adaptation, we employ teacher/student (T/S) learning, in which the posterior probabilities generated by the source-domain model can be used in lieu of labels to train the target-domain model. We evaluate the proposed approach in two scenarios, adapting a clean acoustic model to noisy speech and adapting an adults' speech acoustic model to children's speech. Significant improvements in accuracy are obtained, with reductions in word error rate of up to 44% over the original source model without the need for transcribed data in the target domain. Moreover, we show that increasing the amount of unlabeled data results in additional model robustness, which is particularly beneficial when using simulated training data in the target-domain.\n\nIntroduction\n\nThe success of deep neural networks [1] [2][3] [4][5] relies on the availability of a large amount of transcribed data to train millions of model parameters. However, deep models still suffer reduced performance when exposed to test data from a new domain. Because it is typically very time-consuming or expensive to transcribe large amounts of data for a new domain, domain-adaptation approaches have been proposed to bootstrap the training of a new system from an existing welltrained model [6] [7][8] [9]. These supervised methods still require transcribed data from the new domain and thus their effectiveness is limited by the amount of transcribed data available in the new domain. Although unsupervised adaptation methods can be used by generating labels from a decoder, the performance gap between supervised and unsupervised adaptation is large [7].\n\nIn this work, we propose an approach to domain adaptation that does not require transcriptions but instead uses a corpus of unlabeled parallel data, consisting of pairs of samples from the source domain of the well-trained source model and the target domain. There are many important scenarios in which collecting a virtually unlimited amount of parallel data is relatively simple. For example, to collect noisy or reverberant data from a particular set of environments, speech can be captured simultaneously using a close-talking microphone and a microphone located at a distance from the user. Such a collection effort can also be simulated by acoustically replaying a pre-existing corpus of high signal-tonoise ratio speech files in the target environment or by digitally simulating the target environment offline [10] [11].\n\nTo perform adaptation without the use of transcriptions, we propose to use teacher/student (T/S) learning. In T/S learning, the data from the source domain are processed by the source-domain model (teacher) to generate the corresponding posterior probabilities or soft labels. These posterior probabilities are used in lieu of the usual hard labels derived from the transcriptions to train the target (student) model with the parallel data from the target domain. With this approach, the network can be trained on a potentially enormous amount of training data and the challenge of adapting a large-scale system shifts from transcribing thousands of hours of audio to the potentially much simpler and lower-cost task of designing a scheme to generate the appropriate parallel data.\n\nThe proposed approach is closely related to other approaches for adaptation or retraining that employ knowledge distillation [12]. In these approaches, the soft labels generated by a teacher model are used as a regularization term to train a student model with conventional hard labels. Knowledge distillation was used to train a system on the Aurora 2 digit recognition task [13], using the clean and noisy training sets [14]. In [15] it was shown that for the multi-channel CHiME-4 task [16], soft labels could be derived using enhanced features generated by a beamformer then processed through a network trained with conventional multi-style training [17]. However, it is unclear whether this approach is superior to simply using the enhanced features for the recognition at test time as well. Knowledge distillation was also used to adapt an acoustic model to new dialects using a small adaptation corpus [18]. In all cases, the soft labels provided by the teacher network regularized the conventional training of the student network using hard labels derived from transcriptions. Thus, the use of additional unlabeled training data was not possible.\n\nIn contrast, the proposed approach forgoes the need for hard labels from the data in the new domain entirely and relies solely on the soft labels provided by the parallel corpus and well-trained source model. This allows the use of a significantly larger set of adaptation data which adds robustness to the resulting model. In this work, for example, the unlabeled training data represents an order of magnitude more acoustic data than was used to create the well-trained source model. We evaluate the proposed approach in two scenarios, adapting a clean acoustic model to noisy speech and adapting an adults' speech acoustic model to children's speech. We show that the resulting noisy speech model can obtain performance significantly better than multi-condition training with far better robustness to unseen noise conditions. Significant reduction in word error rate (WER) is obtained on children's speech when no children's speech is present in the original source model.\n\n\nT/S learning for domain adaptation\n\nIn this section, we present T/S learning as a general framework for domain adaptation using unlabeled data. We propose to directly minimize the Kullback-Leibler (KL) divergence between the output distribution of the student network and the teacher network by leveraging large amounts of unlabeled parallel data as shown in Figure 1. We denote the posterior distribution of the teacher and student networks as ( | ) and ( | ), respectively. and are the source and target inputs to the teacher and student networks, respectively. The KL divergence between these two distributions is\n\u2211 \u2211 ( | , ) ( ( | , ) ( | , ) ),(1)\nwhere s indicates senone, i is the senone index and f is the frame index. This formulation takes both the source data and the target data , differing from the original T/S formulation in [19] Figure 1: The flow chart of teacher-student learning using parallel data for domain adaptation To learn a student network that approximates the given teacher network, only the parameters of the student network needs to be optimized. Minimizing the above KL divergence is equivalent to minimizing\n\u2212 \u2211 \u2211 ( | , ) ( | , )(2)\nbecause ( | , ) ( | , ) has no impact to the student network parameter optimization. The training steps of a student network guided by a teacher network which is welltrained with source-domain transcribed data are: 1. Clone the student network from the teacher network. 2. Use parallel unlabeled source data and target data to train the student network with the following steps. a. For each mini-batch, do forward propagation of teacher network using and student network using to calculate ( | , ) and ( | , ). b. Calculate the error signal of Eq. (2), and then do back propagation for the student network. c. Repeat Step 2.a & 2.b until convergence.\n\nThe advantage of the proposed method is that training of the student network doesn't need any transcription as long as we have parallel data because its supervision signal ( | , ) is obtained by passing the source data through the teacher network. Thus, the student network trained optimizing Eq. (2) can exploit unlimited parallel training data. Increased training data provides better coverage of the acoustic space, such that the student network in the target domain behaves very similarly to the well-trained teacher network in the source domain.\n\n\nParallel corpus generation\n\nT/S learning for domain adaptation relies on the availability of a parallel corpus of source and target data, which consists of unlabeled real or the simulated training pairs from the source and target domains, respectively. In this study, we explore two domain-adaptation scenarios: 1) adapting from clean to noisy environments; 2) adapting from adults to children speech.\n\nIt is quite straightforward to collect a parallel corpus of clean and noisy speech. Real paired examples can be obtained by replaying clean speech in a noisy environment. Simulated examples can be obtained by digitally mixing the clean speech with noise. Then, Eq. (2) can be re-written as\n\u2212 \u2211 \u2211 ( | , ) ( | , )\n. Obtaining a parallel corpus of adult and child speech is more challenging. It is very hard to synchronize natural speech from the adults and children so that the resulting samples are synchronized. We opt for a voice transformation approach to simulate children's speech, using formant-based frequency warping. We adopt bilinear frequency warping on the adults speech spectrum and reconstruct the signal with higher pitch. The bilinear transform [20][21] produces the frequency transformation as = + 2 arctan [ \u2212 sin( ) 1+ cos( ) ], where denotes the frequency and the parameter decides the warping factor. For general voice conversion, there is typically a mapping from source to target speech which can be used to calculate the value from vowel segments. In this work, we simply select a warping factor of 0.1 which moves the format frequencies higher. Thus, we are not performing voice conversion per se since there is not a specific voice target. In this scenario, Eq. (2) can be re-written as\n\u2212 \u2211 \u2211 ( | , ) ( | \u210e , ).\n\nExperimental evaluation\n\nThe proposed methods are evaluated using several tasks. The baseline acoustic model for all experiments is a 4-layer LSTM-RNN [22] with 5976 senones trained with the crossentropy criterion. LSTM-RNNs have been shown to be superior than the feed-forward DNNs [22] [23]. Each LSTM layer has 1024 hidden units and the output size of each LSTM layer is reduced to 512 using a linear projection layer. There is no frame stacking, and the output HMM state label is delayed by 5 frames as in [22]. The input feature is the 80-dimension log-filter-bank feature. The transcribed data used to train the baseline acoustic model comes from 375 hours of US-English Cortana audio.\n\n\nNoisy Cortana task\n\nIn this series of experiments, we investigate adaptation of a clean acoustic model to noisy speech [24] [25]. We consider the original Cortana data in the source domain as the clean source data. While this data is not noise-free, Table 2 shows that about 80% of the data is at SNRs higher than 20 dB.   Table 1 shows the WER for different systems. The original evaluation is with the original Cortana test data, and the noisy evaluation is with the noise-added Cortana data. The baseline LSTM obtained 15.62% WER in the original source condition and increases to 18.80% in the noisy target condition. We then add noise to the baseline 375h of training data to train a standard multi-condition LSTM model. This model improves the WER in noisy condition to 17.34% WER but degrades the WER in original condition to 16.58%. We next evaluate the proposed T/S framework using the parallel corpus of 375 hours without using the transcription, with the original and noisy data as the inputs to the teacher and student networks, respectively. In this case, the student network obtains a WER of 16.66%, significantly outperforming the standard multicondition model. If we increase the size of the parallel training data to 3400 hours the WER further improves to 16.11%, showing the advantage of exploiting a large amount of unlabeled data. The 16.11% WER obtained in the target domain is very close to the 15.62% WER obtained by the source (teacher) model on the original Cortana task. This indicates that the T/S learning is effective in that the behavior of the student network in the target domain is approaching that of the teacher network in the source domain. Note that we did not see any improvement using knowledge distillation with the hard labels derived from transcriptions. This is consistent with the findings reported in [15] [27].\n\nAt first glance, it is surprising that the student models trained from 375h and 3400h parallel data outperform the teacher model on the original source condition. To understand this behavior, we broke down the WERs of the original test set into different SNR levels by running an automatic SNR detector on every utterance. The resulting WERs are shown in Table 2, for the baseline source model and the adapted T/S model. Note that few utterances failed to generate SNR results, and hence the weighted average WER in Table 2 is slightly different from the WER in Table 1. As the table  indicates, some of the original Cortana utterances are already noisy. The student model clearly wins for SNR levels below 35dB most likely because the simulated noisy utterances for the parallel training have [5,20]dB SNRs and the soft-target learning may extrapolate well beyond that SNR range. It is still worth investigating why the student model even wins for SNRs larger than 35dB although the gap is small.\n\n\nCHiME-3 task\n\nWe next investigated how the models learned in Section 3.1 behave in a highly-mismatched test environment. The mismatched task we choose is CHiME-3 [28], which contains the Wall Street Journal (WSJ) utterances recorded in real noisy environments. The single-channel far-field noisy speech (the 5th microphone channel) is used for evaluation. WSJ 5K word 3-gram language model is used for decoding. The CHiME-3 test set and the Cortana parallel training data are mismatched in terms of task, speaking style, microphone, environment etc. Also, the noisy speech in the Cortana parallel data is simulated while the CHiME-3 test set is real speech. Table 3 compares WERs from different models. The baseline model trained on 375h of Cortana data obtains a WER of 23.16%. Because of the significant mismatch between Cortana and CHiME-3, both the model trained with 375h of noisy transcribed data and the student model trained with parallel 375h original-noisy data fail to improve performance. However, the student model trained with parallel 3400h of parallel original/noisy Cortana data improves the WER to 19.89%, a 14% relative WER reduction. This gain results from significantly increasing the amount of parallel training data which helps the student model cover much more of the acoustic space.\n\nAlthough the essence of T/S learning is using very large amount of unlabeled data so that the student's behavior in the target domain can approach the teacher's behavior in the source domain, we also want to evaluate the performance of T/S learning when only limited parallel data is available. To that end, we used the parallel data from CHiME-3 training set to adapt the baseline 375h Cortana model. Now, the source data comes from the clean CHiME-3 data, while the noisy target data comes from different sources by combining the real and simulated data from one or more microphones as shown in Table 4. The numbers of real and simulated utterances in each channel are around 2k and 7k, respectively. The T/S learning using either the 2k parallel clean-real channel 5 or the 7k clean-simulated channel 5 utterances can reduce the WER from 23.16% to just under 16%. The results show that using the real data is most effective, but if the real data is unavailable more simulated data can be used. By combining both the real and simulated channel 5 data as the input to the student network, T/S learning can further reduce the WER to 13.77%. Then, with more data from the other microphones, T/S learning can get further improvement. The final student model which was trained with the real and simulated data from all channels get the 12.99% WER, about 44% relative WER reduction over the original source model. This improvement is much larger than what can be obtained from the traditional feature mapping [29][30] and mask learning [31][32] methods. In [33], we proposed advanced models to improve the feature mapping and mask learning methods, but can only obtain 25% relative WER reduction, far below the improvement obtained from the T/S learning in this work.  \n\n\nChildren's speech\n\nIn this section, we explore the T/S learning method for children's speech recognition which is important to home entertainment applications [34] [35]. We first run a DNN gender classifier to determine the percentage of the male adults, female adults, and children in the 375h transcribed data as: 70.5%, 25.3%, and 4.2%, respectively. We remove both children and female adults' data from the training set, as some female adults' data and children are acoustically similar. We then train a baseline LSTM model from only the adult male data, with the same structure as the baseline LSTM model in Section 3.1. Table 5 gives the model evaluation results on children's utterances, recorded from boys and girls. This adult male LSTM model preforms very poorly, with 39.05% and 34.16% WER, on the girls and boys test sets, respectively. Then, we use the bilinear transform described in Section 2.2 to transform adult speech into simulated children's speech. The quality of the transformation seems to be an issue as the DNN gender classifier only labels 6.8% of the transformed utterances as children's speech. Thus, we only use those 6.8% of the transformed utterances as the target data, with the corresponding adult utterances as the source data. After T/S learning, the student model significantly improves the WER of girls' speech to 25.03%, and moderately improves the WER of boys' speech to 32.32%. The student model gets further improvement by extending the training set to the 3400h training set and selecting the portion of data that DNN gender classifier labels as children's speech. With this additional parallel data, the student model achieves a WER of 21.19% and 31.89% on the girls' and boys' test sets, respectively.\n\nFinally, we evaluate whether the LSTM trained with all data (adults and children) can still benefit from T/S learning. Table 6 shows this baseline LSTM model has 18.38% and 22.98% WER on the girls and boys test sets, respectively. Therefore, this LSTM model is much better in handling children's speech. Using the gender DNN trusted transformed utterances from 375h data, the student model cannot improve anymore. But using the gender DNN trusted transformed utterances from 3400h data can improve the girls' speech to 16.65% WER, but still significantly degrades the WER of boys' speech.  Both Tables 5 & 6 show the advantage of using a large amount of data. More parallel data means that the student model can explore more of the acoustic space, resulting in good adaptation performance. In Table 5, the target domain data (children's speech) is not observed in the source domain (adult male speech). Therefore, it is very easy to observe a gain. However, in Table 6, the target domain data is already well modeled by the source model, and hence it is more challenging to get improvement with simple voice conversion approaches. We have listened to the transformed utterances and found that it is relatively easy to obtain girls' speech via voice transformation but harder to create accurate examples of boys' speech. Therefore, the student models perform poorly when evaluated on boys' speech. We expect further improvements with a better voice transformation process.\n\n\nConclusions\n\nIn this study, we explore the large-scale domain adaptation using the T/S learning framework. To learn a deep network in a target domain without labeled data, we minimize the KL divergence of the output distribution between the source domain model with source data and the target domain model with target data. Different from the distillation framework which needs transcribed data, the T/S learning method relies on parallel unlabeled data which is easier to obtain. By increasing the size of the unlabeled parallel training data, the behavior of student network in the target domain is very close to that of teacher network in the source domain.\n\nEvaluated with the noisy Cortana task, the T/S learning student model can achieve a 16.11% WER, very close to the 15.62% WER obtained by the source model on the original Cortana task. On the CHiME-3 task, the student model gets up to 44% relative WER reduction over the source model. On the children's speech recognition task, the student model improves the WER of girls' speech significantly, but it is very challenging to improve the WER of boys' speech due to the limitations of the voice conversion method we employed. All experiments demonstrated that increasing the amount of unlabeled data results in additional model robustness, which is particularly beneficial when using simulated data in the targetdomain.\n\n\nwhich takes the same data for teacher and student networks.... \n... \n... \n... \n... \n... \n\nSource Domain Data \n\nText \n\n... \n... \n\nForward propagation to \ncalculate posterior \n\nCalculate error signal \n\nBack propagation to update \nnetwork parameters \n\nTeacher \nnetwork \n\nStudent \nnetwork \n\n... \n... \n... \n... \n... \n... Text \n\nTarget Domain Data \n\n\n\nTable 2 :\n2WER (%) breakdown for original Cortana condition \nSNR \n<5 db \n[5, 20]db [20,35]db \n>35db \nWord Count \n155 \n15568 \n31652 \n25015 \nBaseline \n23.87 \n15.71 \n15.46 \n14.7 \nT/S 3400h \n20.65 \n15.31 \n14.9 \n14.44 \n\n\nTable 3 :\n3WERs (%) on Chime 3 test sets using Cortana data.The columns have the same meaning as inTable 1.Train Teacher \n(transcribed) \n\nTrain Student \n(unlabeled pair) \n\nWER \n\noriginal 375h \nNone \n23.16 \nnoisy 375h \nNone \n24.51 \noriginal 375h \noriginal-noisy 375h \n23.67 \noriginal 375h \noriginal-noisy 3400h \n19.89 \n\n\n\nTable 4 :\n4WERs(%) on Chime 3 test sets using Chime 3 data. \nThe source data is the clean data. The target data comes from \ndifferent noisy sources. \nThe noisy target data in the pair comes from \n\nWER \n\nReal \nchannel 5 \n\nSimulated \nchannel 5 \n\nOther \nreal \nchannels \n\nSimulated \nother \nchannels \nY \nN \nN \nN \n15.88 \nN \nY \nN \nN \n15.73 \nY \nY \nN \nN \n13.77 \nY \nY \nY \nY \n12.99 \n\n\n\nTable 5 :\n5WERs (%) of models initiated from Male LSTM on children's speech tasks.Model \ngirls \nboys \nAdult male data from 375h transcribed \n39.05 34.16 \nTarget data: transformed children from \n375h unlabeled \n25.03 32.32 \nTarget data: transformed children from \n3400h unlabeled \n\n21.19 31.89 \n\n\n\nTable 6 :\n6WERs (%) of models initiated from LSTM trained with male, female, children data on children's speech tasks.Model \ngirls \nboys \nAll data (male, female, children) from \n375h transcribed \n18.38 22.98 \nTarget data: transformed children from \n375h unlabeled \n18.86 30.24 \nTarget data: transformed children from \n3400h unlabeled \n\n16.65 29.20 \n\n\n\nMaking deep belief networks effective for large vocabulary continuous speech recognition. T N Sainath, B Kingsbury, B Ramabhadran, P Fousek, P Novak, A Mohamed, Proc. Workshop on Automatic Speech Recognition and Understanding. Workshop on Automatic Speech Recognition and UnderstandingT. N. Sainath, B. Kingsbury, B. Ramabhadran, P. Fousek, P. Novak, and A. Mohamed, \"Making deep belief networks effective for large vocabulary continuous speech recognition,\" in Proc. Workshop on Automatic Speech Recognition and Understanding, pp. 30-35, 2011.\n\nContext-dependent pre-trained deep neural networks for large-vocabulary speech recognition. G E Dahl, D Yu, L Deng, A Acero, IEEE Trans. on Audio, Speech and Language Processing. 201G. E. Dahl, D. Yu, L. Deng, and A. Acero, \"Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition,\" IEEE Trans. on Audio, Speech and Language Processing, vol. 20, no. 1, pp. 30-42, 2012.\n\nApplication of pretrained deep neural networks to large vocabulary speech recognition. N Jaitly, P Nguyen, V Vanhoucke, Proc. Interspeech. InterspeechN. Jaitly, P. Nguyen, and V. Vanhoucke, \"Application of pretrained deep neural networks to large vocabulary speech recognition\", in Proc. Interspeech, 2012.\n\nDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. G Hinton, L Deng, D Yu, G Dahl, A Mohamed, N Jaitly, A Senior, V Vanhoucke, P Nguyen, T Sainath, B Kingsbury, IEEE Signal Processing Magazine. 296G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, \"Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\" IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97, 2012.\n\nRecent advances in deep learning for speech research at Microsoft. L Deng, J Li, J. -T Huang, Proc. ICASSP. ICASSPL. Deng, J. Li, J. -T. Huang et al. \"Recent advances in deep learning for speech research at Microsoft,\" in Proc. ICASSP, 2013.\n\nFeature engineering in context-dependent deep neural networks for conversational speech transcription. F Seide, G Li, X Chen, D Yu, Proc. ASRU. ASRUF. Seide, G. Li, X. Chen, and D. Yu, \"Feature engineering in context-dependent deep neural networks for conversational speech transcription,\" In Proc. ASRU, pp. 24-29, 2011.\n\nSpeaker adaptation of context dependent deep neural networks. H Liao, Proc. ICASSP. ICASSPH. Liao, \"Speaker adaptation of context dependent deep neural networks,\" in Proc. ICASSP, 2013.\n\nKL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition. D Yu, K Yao, H Su, G Li, F Seide, Proc. ICASSP. ICASSPD. Yu, K. Yao, H. Su, G. Li, and F. Seide, \"KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition.\" In Proc. ICASSP, pages 7893- 7897, 2013.\n\nSingular value decomposition based low-footprint speaker adaptation and personalization for deep neural network. J Xue, J Li, D Yu, M Seltzer, Y Gong, Proc. ICASSP. ICASSPJ. Xue, J. Li, D. Yu, M. Seltzer, and Y. Gong, \"Singular value decomposition based low-footprint speaker adaptation and personalization for deep neural network,\" In Proc. ICASSP, pp. 6359-6363, 2014.\n\nA study on data augmentation of reverberant speech for robust speech recognition. T Ko, V Peddinti, D Povey, M Seltzer, S Khudanpur, Proc. ICASSP. ICASSPT. Ko, V. Peddinti, D. Povey, M. Seltzer and S. Khudanpur, \"A study on data augmentation of reverberant speech for robust speech recognition,\" in Proc. ICASSP, 2017.\n\nFast and accurate recurrent neural network acoustic models for speech recognition. H Sak, A Senior, K Rao, F Beaufays, Proc. Interspeech. InterspeechH. Sak, A. Senior, K. Rao, and F. Beaufays, \"Fast and accurate recurrent neural network acoustic models for speech recognition,\" in Proc. Interspeech, 2015.\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, arXiv:1503.02531G. Hinton, O. Vinyals, and J. Dean, \"Distilling the knowledge in a neural network,\" arXiv:1503.02531, 2015.\n\nThe Aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions. H G Hirsch, D Pearce, Proc. ISCA ITRW ASR. ISCA ITRW ASRH. G. Hirsch and D. Pearce, \"The Aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions,\" Proc. ISCA ITRW ASR, 2000.\n\nRobust Speech Recognition using Generalized Distillation Framework. K Markov, T Matsui, Proc. Interspeech. InterspeechK. Markov, and T. Matsui, \"Robust Speech Recognition using Generalized Distillation Framework,\" in Proc. Interspeech, pp.2364-2368, 2016.\n\nStudentteacher network learning with enhanced features. S Watanabe, T Hori, J Le Roux, J Hershey, Proc. ICASSP. ICASSPS. Watanabe, T. Hori, J. Le Roux, and J. Hershey, \"Student- teacher network learning with enhanced features\", in Proc. ICASSP, 2017.\n\nAn analysis of environment, microphone and data simulation mismatches in robust speech recognition. E Vincent, S Watanabe, A Nugraha, J Barker, R Marxer, Computer Speech & Language. E. Vincent, S. Watanabe, A. Nugraha, J. Barker, and R. Marxer, \"An analysis of environment, microphone and data simulation mismatches in robust speech recognition,\" Computer Speech & Language, 2016.\n\nMulti-style training for robust isolated-word speech recognition. R Lippmann, E Martin, D Paul, Proc. ICASSP. ICASSPR. Lippmann, E. Martin, and D. Paul, \"Multi-style training for robust isolated-word speech recognition,\" In Proc. ICASSP, pp. 705-708, 1987.\n\nDomain adaptation of DNN acoustic models using knowledge distillation. T Asami, R Masumura, Y Yamaguchi, H Masataki, Y Aono, Proc. ICASSP. ICASSPT. Asami, R. Masumura, Y. Yamaguchi, H. Masataki, Y. Aono, \"Domain adaptation of DNN acoustic models using knowledge distillation,\" in Proc. ICASSP, 2017.\n\nLearning smallsize DNN with output-distribution-based criteria. J Li, R Zhao, J.-T Huang, Yifan Gong, InterspeechJ. Li, R. Zhao, J.-T. Huang and Yifan Gong, \"Learning small- size DNN with output-distribution-based criteria,\" in Interspeech, 2014.\n\nRobust speech recognition by normalization of the acoustic space. A Acero, R , M Stern, Proc. ICASSP. ICASSPA. Acero, R., M., Stern, \"Robust speech recognition by normalization of the acoustic space,\" in Proc. ICASSP, pp. 893- 896, 1991.\n\nA frame mapping based HMM approach to cross-lingual voice transform. Y Qian, J Xu, F K Soong, Proc. ICASSP. ICASSPY. Qian, J. Xu, F. K. Soong, \"A frame mapping based HMM approach to cross-lingual voice transform,\" in Proc. ICASSP, pp. 5120-5123, 2011.\n\nLong short-term memory recurrent neural network architectures for large scale acoustic modeling. H Sak, A Senior, F Beaufays, Proc. Interspeech. InterspeechH. Sak, A. Senior, F. Beaufays, \"Long short-term memory recurrent neural network architectures for large scale acoustic modeling,\" in Proc. Interspeech, 2014.\n\nSimplifying long short-term memory acoustic models for fast training and decoding. Y Miao, J Li, Y Wang, S X Zhang, Y Gong, Proc. ICASSP. ICASSPY. Miao, J. Li, Y. Wang, S.X. Zhang, and Y Gong, \"Simplifying long short-term memory acoustic models for fast training and decoding,\" in Proc. ICASSP, 2016.\n\nAn Overview of Noise-Robust Automatic Speech Recognition. J Li, L Deng, Y Gong, R Haeb-Umbach, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 22J. Li, L. Deng, Y. Gong and R. Haeb-Umbach, \"An Overview of Noise-Robust Automatic Speech Recognition,\" in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745-777, 2014.\n\nRobust Automatic Speech Recognition: A Bridge to Practical Applications. J Li, L Deng, R Haeb-Umbach, Y Gong, Morgan Kaufmann PressJ. Li, L. Deng, R. Haeb-Umbach, and Y. Gong, Robust Automatic Speech Recognition: A Bridge to Practical Applications, Morgan Kaufmann Press, 2015. [26] ETSI noise: https://docbox.etsi.org/stq/Open/EG%20202%20396- 1%20Background%20noise%20database/Binaural_Signals\n\nKnowledge distillation for smallfootprint highway networks. L Lu, M Guo, S Renals, arXiv:1608.00892L. Lu, M. Guo, and S. Renals, \"Knowledge distillation for small- footprint highway networks,\" arXiv:1608.00892, 2016.\n\nThe third chime speech separation and recognition challenge: Dataset, task and baselines. J Barker, R Marxer, E Vincent, S Watanabe, Proc. ASRU. ASRUJ. Barker, R. Marxer, E. Vincent, and S. Watanabe, \"The third chime speech separation and recognition challenge: Dataset, task and baselines,\" in Proc. ASRU, pp. 504-511, 2015.\n\nRecurrent neural networks for noise reduction in robust ASR. A L Maas, Q V Le, T M O&apos;neil, O Vinyals, P Nguyen, A Y Ng, Proc. Interspeech. InterspeechA. L. Maas, Q. V. Le, T. M. O'Neil, O. Vinyals, P. Nguyen, and A. Y. Ng, \"Recurrent neural networks for noise reduction in robust ASR,\" in Proc. Interspeech, 2012, pp. 22-25.\n\nSingle-channel speech separation with memory-enhanced recurrent neural networks. F Weninger, F Eyben, B Schuller, Proc. ICASSP. ICASSPF. Weninger, F. Eyben, and B. Schuller, \"Single-channel speech separation with memory-enhanced recurrent neural networks,\" in Proc. ICASSP, 2014.\n\nIdeal ratio mask estimation using deep neural networks. A Narayanan, D L Wang, Proc. ICASSP. ICASSPA. Narayanan and D.L.Wang, \"Ideal ratio mask estimation using deep neural networks,\" in Proc. ICASSP, 2013, pp. 7092-7096.\n\nOn training targets for supervised speech separation. Y Wang, A Narayanan, D Wang, IEEE/ACM Trans. on Audio, Speech and Language Processing. 2212Y.Wang, A. Narayanan, and D.Wang, \"On training targets for supervised speech separation,\" IEEE/ACM Trans. on Audio, Speech and Language Processing, vol. 22, no. 12, pp. 1849- 1858, 2014.\n\nImproving mask learning based speech enhancement system with restoration layers and residual connection. Z Chen, Y Huang, J Li, Y Gong, Proc. Interspeech. InterspeechZ. Chen, Y. Huang, J. Li, and Y. Gong, \"Improving mask learning based speech enhancement system with restoration layers and residual connection,\" in Proc. Interspeech, 2017.\n\nLarge vocabulary automatic speech recognition for children. H Liao, Proc. Interspeech. InterspeechH. Liao, et al. \"Large vocabulary automatic speech recognition for children,\" in Proc. Interspeech. 2015.\n\nImproving children's speech recognition through out-of-domain data augmentation. J Fainberg, P Bell, M Lincoln, S Renals, Proc. Interspeech. InterspeechJ. Fainberg, P. Bell, M. Lincoln, and S. Renals, \"Improving children's speech recognition through out-of-domain data augmentation,\" in Proc. Interspeech, pp.1598-1602, 2016.\n", "annotations": {"author": "[{\"end\":130,\"start\":62},{\"end\":208,\"start\":131},{\"end\":276,\"start\":209},{\"end\":345,\"start\":277},{\"end\":436,\"start\":346}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":68},{\"end\":148,\"start\":141},{\"end\":216,\"start\":212},{\"end\":285,\"start\":281},{\"end\":356,\"start\":352}]", "author_first_name": "[{\"end\":67,\"start\":62},{\"end\":138,\"start\":131},{\"end\":140,\"start\":139},{\"end\":211,\"start\":209},{\"end\":280,\"start\":277},{\"end\":351,\"start\":346}]", "author_affiliation": "[{\"end\":129,\"start\":72},{\"end\":207,\"start\":150},{\"end\":275,\"start\":218},{\"end\":344,\"start\":287},{\"end\":435,\"start\":378}]", "title": "[{\"end\":59,\"start\":1},{\"end\":495,\"start\":437}]", "venue": null, "abstract": "[{\"end\":1867,\"start\":559}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1922,\"start\":1919},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1926,\"start\":1923},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1933,\"start\":1930},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2379,\"start\":2376},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2383,\"start\":2380},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2390,\"start\":2387},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2740,\"start\":2737},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3564,\"start\":3560},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3569,\"start\":3565},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4484,\"start\":4480},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4735,\"start\":4731},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4781,\"start\":4777},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4790,\"start\":4786},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4848,\"start\":4844},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5013,\"start\":5009},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5268,\"start\":5264},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7333,\"start\":7329},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10027,\"start\":10023},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10756,\"start\":10752},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10888,\"start\":10884},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10893,\"start\":10889},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11115,\"start\":11111},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11418,\"start\":11414},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11423,\"start\":11419},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13144,\"start\":13140},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13149,\"start\":13145},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13949,\"start\":13946},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13952,\"start\":13949},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14318,\"start\":14314},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16970,\"start\":16966},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16997,\"start\":16993},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17018,\"start\":17014},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17392,\"start\":17388},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17397,\"start\":17393},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19497,\"start\":19495}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":22175,\"start\":21829},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22391,\"start\":22176},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":22712,\"start\":22392},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":23087,\"start\":22713},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":23384,\"start\":23088},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":23736,\"start\":23385}]", "paragraph": "[{\"end\":2741,\"start\":1883},{\"end\":3570,\"start\":2743},{\"end\":4353,\"start\":3572},{\"end\":5509,\"start\":4355},{\"end\":6486,\"start\":5511},{\"end\":7105,\"start\":6525},{\"end\":7629,\"start\":7142},{\"end\":8305,\"start\":7655},{\"end\":8857,\"start\":8307},{\"end\":9261,\"start\":8888},{\"end\":9552,\"start\":9263},{\"end\":10574,\"start\":9575},{\"end\":11292,\"start\":10626},{\"end\":13150,\"start\":11315},{\"end\":14149,\"start\":13152},{\"end\":15459,\"start\":14166},{\"end\":17226,\"start\":15461},{\"end\":18974,\"start\":17248},{\"end\":20447,\"start\":18976},{\"end\":21110,\"start\":20463},{\"end\":21828,\"start\":21112}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7141,\"start\":7106},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7654,\"start\":7630},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9574,\"start\":9553},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10599,\"start\":10575}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":11552,\"start\":11545},{\"end\":11625,\"start\":11618},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13514,\"start\":13507},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13675,\"start\":13668},{\"end\":13799,\"start\":13714},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":14817,\"start\":14810},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16065,\"start\":16058},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17862,\"start\":17855},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":19102,\"start\":19095},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19583,\"start\":19571},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19776,\"start\":19769},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":19944,\"start\":19937}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1881,\"start\":1869},{\"attributes\":{\"n\":\"2.\"},\"end\":6523,\"start\":6489},{\"attributes\":{\"n\":\"3.\"},\"end\":8886,\"start\":8860},{\"attributes\":{\"n\":\"4.\"},\"end\":10624,\"start\":10601},{\"attributes\":{\"n\":\"4.1\"},\"end\":11313,\"start\":11295},{\"attributes\":{\"n\":\"4.2\"},\"end\":14164,\"start\":14152},{\"attributes\":{\"n\":\"4.3\"},\"end\":17246,\"start\":17229},{\"attributes\":{\"n\":\"5.\"},\"end\":20461,\"start\":20450},{\"end\":22186,\"start\":22177},{\"end\":22402,\"start\":22393},{\"end\":22723,\"start\":22714},{\"end\":23098,\"start\":23089},{\"end\":23395,\"start\":23386}]", "table": "[{\"end\":22175,\"start\":21890},{\"end\":22391,\"start\":22188},{\"end\":22712,\"start\":22500},{\"end\":23087,\"start\":22729},{\"end\":23384,\"start\":23171},{\"end\":23736,\"start\":23504}]", "figure_caption": "[{\"end\":21890,\"start\":21831},{\"end\":22500,\"start\":22404},{\"end\":22729,\"start\":22725},{\"end\":23171,\"start\":23100},{\"end\":23504,\"start\":23397}]", "figure_ref": "[{\"end\":6856,\"start\":6848},{\"end\":7342,\"start\":7334}]", "bib_author_first_name": "[{\"end\":23829,\"start\":23828},{\"end\":23831,\"start\":23830},{\"end\":23842,\"start\":23841},{\"end\":23855,\"start\":23854},{\"end\":23870,\"start\":23869},{\"end\":23880,\"start\":23879},{\"end\":23889,\"start\":23888},{\"end\":24377,\"start\":24376},{\"end\":24379,\"start\":24378},{\"end\":24387,\"start\":24386},{\"end\":24393,\"start\":24392},{\"end\":24401,\"start\":24400},{\"end\":24778,\"start\":24777},{\"end\":24788,\"start\":24787},{\"end\":24798,\"start\":24797},{\"end\":25107,\"start\":25106},{\"end\":25117,\"start\":25116},{\"end\":25125,\"start\":25124},{\"end\":25131,\"start\":25130},{\"end\":25139,\"start\":25138},{\"end\":25150,\"start\":25149},{\"end\":25160,\"start\":25159},{\"end\":25170,\"start\":25169},{\"end\":25183,\"start\":25182},{\"end\":25193,\"start\":25192},{\"end\":25204,\"start\":25203},{\"end\":25622,\"start\":25621},{\"end\":25630,\"start\":25629},{\"end\":25640,\"start\":25635},{\"end\":25901,\"start\":25900},{\"end\":25910,\"start\":25909},{\"end\":25916,\"start\":25915},{\"end\":25924,\"start\":25923},{\"end\":26183,\"start\":26182},{\"end\":26415,\"start\":26414},{\"end\":26421,\"start\":26420},{\"end\":26428,\"start\":26427},{\"end\":26434,\"start\":26433},{\"end\":26440,\"start\":26439},{\"end\":26776,\"start\":26775},{\"end\":26783,\"start\":26782},{\"end\":26789,\"start\":26788},{\"end\":26795,\"start\":26794},{\"end\":26806,\"start\":26805},{\"end\":27117,\"start\":27116},{\"end\":27123,\"start\":27122},{\"end\":27135,\"start\":27134},{\"end\":27144,\"start\":27143},{\"end\":27155,\"start\":27154},{\"end\":27438,\"start\":27437},{\"end\":27445,\"start\":27444},{\"end\":27455,\"start\":27454},{\"end\":27462,\"start\":27461},{\"end\":27708,\"start\":27707},{\"end\":27718,\"start\":27717},{\"end\":27729,\"start\":27728},{\"end\":27981,\"start\":27980},{\"end\":27983,\"start\":27982},{\"end\":27993,\"start\":27992},{\"end\":28282,\"start\":28281},{\"end\":28292,\"start\":28291},{\"end\":28527,\"start\":28526},{\"end\":28539,\"start\":28538},{\"end\":28547,\"start\":28546},{\"end\":28550,\"start\":28548},{\"end\":28558,\"start\":28557},{\"end\":28823,\"start\":28822},{\"end\":28834,\"start\":28833},{\"end\":28846,\"start\":28845},{\"end\":28857,\"start\":28856},{\"end\":28867,\"start\":28866},{\"end\":29171,\"start\":29170},{\"end\":29183,\"start\":29182},{\"end\":29193,\"start\":29192},{\"end\":29434,\"start\":29433},{\"end\":29443,\"start\":29442},{\"end\":29455,\"start\":29454},{\"end\":29468,\"start\":29467},{\"end\":29480,\"start\":29479},{\"end\":29728,\"start\":29727},{\"end\":29734,\"start\":29733},{\"end\":29745,\"start\":29741},{\"end\":29758,\"start\":29753},{\"end\":29978,\"start\":29977},{\"end\":29987,\"start\":29986},{\"end\":29991,\"start\":29990},{\"end\":30220,\"start\":30219},{\"end\":30228,\"start\":30227},{\"end\":30234,\"start\":30233},{\"end\":30236,\"start\":30235},{\"end\":30501,\"start\":30500},{\"end\":30508,\"start\":30507},{\"end\":30518,\"start\":30517},{\"end\":30803,\"start\":30802},{\"end\":30811,\"start\":30810},{\"end\":30817,\"start\":30816},{\"end\":30825,\"start\":30824},{\"end\":30827,\"start\":30826},{\"end\":30836,\"start\":30835},{\"end\":31080,\"start\":31079},{\"end\":31086,\"start\":31085},{\"end\":31094,\"start\":31093},{\"end\":31102,\"start\":31101},{\"end\":31465,\"start\":31464},{\"end\":31471,\"start\":31470},{\"end\":31479,\"start\":31478},{\"end\":31494,\"start\":31493},{\"end\":31848,\"start\":31847},{\"end\":31854,\"start\":31853},{\"end\":31861,\"start\":31860},{\"end\":32096,\"start\":32095},{\"end\":32106,\"start\":32105},{\"end\":32116,\"start\":32115},{\"end\":32127,\"start\":32126},{\"end\":32394,\"start\":32393},{\"end\":32396,\"start\":32395},{\"end\":32404,\"start\":32403},{\"end\":32406,\"start\":32405},{\"end\":32412,\"start\":32411},{\"end\":32414,\"start\":32413},{\"end\":32429,\"start\":32428},{\"end\":32440,\"start\":32439},{\"end\":32450,\"start\":32449},{\"end\":32452,\"start\":32451},{\"end\":32745,\"start\":32744},{\"end\":32757,\"start\":32756},{\"end\":32766,\"start\":32765},{\"end\":33001,\"start\":33000},{\"end\":33014,\"start\":33013},{\"end\":33016,\"start\":33015},{\"end\":33222,\"start\":33221},{\"end\":33230,\"start\":33229},{\"end\":33243,\"start\":33242},{\"end\":33606,\"start\":33605},{\"end\":33614,\"start\":33613},{\"end\":33623,\"start\":33622},{\"end\":33629,\"start\":33628},{\"end\":33902,\"start\":33901},{\"end\":34128,\"start\":34127},{\"end\":34140,\"start\":34139},{\"end\":34148,\"start\":34147},{\"end\":34159,\"start\":34158}]", "bib_author_last_name": "[{\"end\":23839,\"start\":23832},{\"end\":23852,\"start\":23843},{\"end\":23867,\"start\":23856},{\"end\":23877,\"start\":23871},{\"end\":23886,\"start\":23881},{\"end\":23897,\"start\":23890},{\"end\":24384,\"start\":24380},{\"end\":24390,\"start\":24388},{\"end\":24398,\"start\":24394},{\"end\":24407,\"start\":24402},{\"end\":24785,\"start\":24779},{\"end\":24795,\"start\":24789},{\"end\":24808,\"start\":24799},{\"end\":25114,\"start\":25108},{\"end\":25122,\"start\":25118},{\"end\":25128,\"start\":25126},{\"end\":25136,\"start\":25132},{\"end\":25147,\"start\":25140},{\"end\":25157,\"start\":25151},{\"end\":25167,\"start\":25161},{\"end\":25180,\"start\":25171},{\"end\":25190,\"start\":25184},{\"end\":25201,\"start\":25194},{\"end\":25214,\"start\":25205},{\"end\":25627,\"start\":25623},{\"end\":25633,\"start\":25631},{\"end\":25646,\"start\":25641},{\"end\":25907,\"start\":25902},{\"end\":25913,\"start\":25911},{\"end\":25921,\"start\":25917},{\"end\":25927,\"start\":25925},{\"end\":26188,\"start\":26184},{\"end\":26418,\"start\":26416},{\"end\":26425,\"start\":26422},{\"end\":26431,\"start\":26429},{\"end\":26437,\"start\":26435},{\"end\":26446,\"start\":26441},{\"end\":26780,\"start\":26777},{\"end\":26786,\"start\":26784},{\"end\":26792,\"start\":26790},{\"end\":26803,\"start\":26796},{\"end\":26811,\"start\":26807},{\"end\":27120,\"start\":27118},{\"end\":27132,\"start\":27124},{\"end\":27141,\"start\":27136},{\"end\":27152,\"start\":27145},{\"end\":27165,\"start\":27156},{\"end\":27442,\"start\":27439},{\"end\":27452,\"start\":27446},{\"end\":27459,\"start\":27456},{\"end\":27471,\"start\":27463},{\"end\":27715,\"start\":27709},{\"end\":27726,\"start\":27719},{\"end\":27734,\"start\":27730},{\"end\":27990,\"start\":27984},{\"end\":28000,\"start\":27994},{\"end\":28289,\"start\":28283},{\"end\":28299,\"start\":28293},{\"end\":28536,\"start\":28528},{\"end\":28544,\"start\":28540},{\"end\":28555,\"start\":28551},{\"end\":28566,\"start\":28559},{\"end\":28831,\"start\":28824},{\"end\":28843,\"start\":28835},{\"end\":28854,\"start\":28847},{\"end\":28864,\"start\":28858},{\"end\":28874,\"start\":28868},{\"end\":29180,\"start\":29172},{\"end\":29190,\"start\":29184},{\"end\":29198,\"start\":29194},{\"end\":29440,\"start\":29435},{\"end\":29452,\"start\":29444},{\"end\":29465,\"start\":29456},{\"end\":29477,\"start\":29469},{\"end\":29485,\"start\":29481},{\"end\":29731,\"start\":29729},{\"end\":29739,\"start\":29735},{\"end\":29751,\"start\":29746},{\"end\":29763,\"start\":29759},{\"end\":29984,\"start\":29979},{\"end\":29997,\"start\":29992},{\"end\":30225,\"start\":30221},{\"end\":30231,\"start\":30229},{\"end\":30242,\"start\":30237},{\"end\":30505,\"start\":30502},{\"end\":30515,\"start\":30509},{\"end\":30527,\"start\":30519},{\"end\":30808,\"start\":30804},{\"end\":30814,\"start\":30812},{\"end\":30822,\"start\":30818},{\"end\":30833,\"start\":30828},{\"end\":30841,\"start\":30837},{\"end\":31083,\"start\":31081},{\"end\":31091,\"start\":31087},{\"end\":31099,\"start\":31095},{\"end\":31114,\"start\":31103},{\"end\":31468,\"start\":31466},{\"end\":31476,\"start\":31472},{\"end\":31491,\"start\":31480},{\"end\":31499,\"start\":31495},{\"end\":31851,\"start\":31849},{\"end\":31858,\"start\":31855},{\"end\":31868,\"start\":31862},{\"end\":32103,\"start\":32097},{\"end\":32113,\"start\":32107},{\"end\":32124,\"start\":32117},{\"end\":32136,\"start\":32128},{\"end\":32401,\"start\":32397},{\"end\":32409,\"start\":32407},{\"end\":32426,\"start\":32415},{\"end\":32437,\"start\":32430},{\"end\":32447,\"start\":32441},{\"end\":32455,\"start\":32453},{\"end\":32754,\"start\":32746},{\"end\":32763,\"start\":32758},{\"end\":32775,\"start\":32767},{\"end\":33011,\"start\":33002},{\"end\":33021,\"start\":33017},{\"end\":33227,\"start\":33223},{\"end\":33240,\"start\":33231},{\"end\":33248,\"start\":33244},{\"end\":33611,\"start\":33607},{\"end\":33620,\"start\":33615},{\"end\":33626,\"start\":33624},{\"end\":33634,\"start\":33630},{\"end\":33907,\"start\":33903},{\"end\":34137,\"start\":34129},{\"end\":34145,\"start\":34141},{\"end\":34156,\"start\":34149},{\"end\":34166,\"start\":34160}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":387331},\"end\":24282,\"start\":23738},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14862572},\"end\":24688,\"start\":24284},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13521651},\"end\":24996,\"start\":24690},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206485943},\"end\":25552,\"start\":24998},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13412186},\"end\":25795,\"start\":25554},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9933050},\"end\":26118,\"start\":25797},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14657792},\"end\":26305,\"start\":26120},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":16753181},\"end\":26660,\"start\":26307},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12971356},\"end\":27032,\"start\":26662},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":23138179},\"end\":27352,\"start\":27034},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":11298362},\"end\":27659,\"start\":27354},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b11\"},\"end\":27859,\"start\":27661},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":204529},\"end\":28211,\"start\":27861},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11940370},\"end\":28468,\"start\":28213},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9154797},\"end\":28720,\"start\":28470},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":34466505},\"end\":29102,\"start\":28722},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":61524006},\"end\":29360,\"start\":29104},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206742843},\"end\":29661,\"start\":29362},{\"attributes\":{\"id\":\"b18\"},\"end\":29909,\"start\":29663},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":11362220},\"end\":30148,\"start\":29911},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16708114},\"end\":30401,\"start\":30150},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6263878},\"end\":30717,\"start\":30403},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14215400},\"end\":31019,\"start\":30719},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14557362},\"end\":31389,\"start\":31021},{\"attributes\":{\"id\":\"b24\"},\"end\":31785,\"start\":31391},{\"attributes\":{\"doi\":\"arXiv:1608.00892\",\"id\":\"b25\"},\"end\":32003,\"start\":31787},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13638584},\"end\":32330,\"start\":32005},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":171208},\"end\":32661,\"start\":32332},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":15661489},\"end\":32942,\"start\":32663},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4288794},\"end\":33165,\"start\":32944},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":12431609},\"end\":33498,\"start\":33167},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1461399},\"end\":33839,\"start\":33500},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":260103},\"end\":34044,\"start\":33841},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8488261},\"end\":34371,\"start\":34046}]", "bib_title": "[{\"end\":23826,\"start\":23738},{\"end\":24374,\"start\":24284},{\"end\":24775,\"start\":24690},{\"end\":25104,\"start\":24998},{\"end\":25619,\"start\":25554},{\"end\":25898,\"start\":25797},{\"end\":26180,\"start\":26120},{\"end\":26412,\"start\":26307},{\"end\":26773,\"start\":26662},{\"end\":27114,\"start\":27034},{\"end\":27435,\"start\":27354},{\"end\":27978,\"start\":27861},{\"end\":28279,\"start\":28213},{\"end\":28524,\"start\":28470},{\"end\":28820,\"start\":28722},{\"end\":29168,\"start\":29104},{\"end\":29431,\"start\":29362},{\"end\":29975,\"start\":29911},{\"end\":30217,\"start\":30150},{\"end\":30498,\"start\":30403},{\"end\":30800,\"start\":30719},{\"end\":31077,\"start\":31021},{\"end\":32093,\"start\":32005},{\"end\":32391,\"start\":32332},{\"end\":32742,\"start\":32663},{\"end\":32998,\"start\":32944},{\"end\":33219,\"start\":33167},{\"end\":33603,\"start\":33500},{\"end\":33899,\"start\":33841},{\"end\":34125,\"start\":34046}]", "bib_author": "[{\"end\":23841,\"start\":23828},{\"end\":23854,\"start\":23841},{\"end\":23869,\"start\":23854},{\"end\":23879,\"start\":23869},{\"end\":23888,\"start\":23879},{\"end\":23899,\"start\":23888},{\"end\":24386,\"start\":24376},{\"end\":24392,\"start\":24386},{\"end\":24400,\"start\":24392},{\"end\":24409,\"start\":24400},{\"end\":24787,\"start\":24777},{\"end\":24797,\"start\":24787},{\"end\":24810,\"start\":24797},{\"end\":25116,\"start\":25106},{\"end\":25124,\"start\":25116},{\"end\":25130,\"start\":25124},{\"end\":25138,\"start\":25130},{\"end\":25149,\"start\":25138},{\"end\":25159,\"start\":25149},{\"end\":25169,\"start\":25159},{\"end\":25182,\"start\":25169},{\"end\":25192,\"start\":25182},{\"end\":25203,\"start\":25192},{\"end\":25216,\"start\":25203},{\"end\":25629,\"start\":25621},{\"end\":25635,\"start\":25629},{\"end\":25648,\"start\":25635},{\"end\":25909,\"start\":25900},{\"end\":25915,\"start\":25909},{\"end\":25923,\"start\":25915},{\"end\":25929,\"start\":25923},{\"end\":26190,\"start\":26182},{\"end\":26420,\"start\":26414},{\"end\":26427,\"start\":26420},{\"end\":26433,\"start\":26427},{\"end\":26439,\"start\":26433},{\"end\":26448,\"start\":26439},{\"end\":26782,\"start\":26775},{\"end\":26788,\"start\":26782},{\"end\":26794,\"start\":26788},{\"end\":26805,\"start\":26794},{\"end\":26813,\"start\":26805},{\"end\":27122,\"start\":27116},{\"end\":27134,\"start\":27122},{\"end\":27143,\"start\":27134},{\"end\":27154,\"start\":27143},{\"end\":27167,\"start\":27154},{\"end\":27444,\"start\":27437},{\"end\":27454,\"start\":27444},{\"end\":27461,\"start\":27454},{\"end\":27473,\"start\":27461},{\"end\":27717,\"start\":27707},{\"end\":27728,\"start\":27717},{\"end\":27736,\"start\":27728},{\"end\":27992,\"start\":27980},{\"end\":28002,\"start\":27992},{\"end\":28291,\"start\":28281},{\"end\":28301,\"start\":28291},{\"end\":28538,\"start\":28526},{\"end\":28546,\"start\":28538},{\"end\":28557,\"start\":28546},{\"end\":28568,\"start\":28557},{\"end\":28833,\"start\":28822},{\"end\":28845,\"start\":28833},{\"end\":28856,\"start\":28845},{\"end\":28866,\"start\":28856},{\"end\":28876,\"start\":28866},{\"end\":29182,\"start\":29170},{\"end\":29192,\"start\":29182},{\"end\":29200,\"start\":29192},{\"end\":29442,\"start\":29433},{\"end\":29454,\"start\":29442},{\"end\":29467,\"start\":29454},{\"end\":29479,\"start\":29467},{\"end\":29487,\"start\":29479},{\"end\":29733,\"start\":29727},{\"end\":29741,\"start\":29733},{\"end\":29753,\"start\":29741},{\"end\":29765,\"start\":29753},{\"end\":29986,\"start\":29977},{\"end\":29990,\"start\":29986},{\"end\":29999,\"start\":29990},{\"end\":30227,\"start\":30219},{\"end\":30233,\"start\":30227},{\"end\":30244,\"start\":30233},{\"end\":30507,\"start\":30500},{\"end\":30517,\"start\":30507},{\"end\":30529,\"start\":30517},{\"end\":30810,\"start\":30802},{\"end\":30816,\"start\":30810},{\"end\":30824,\"start\":30816},{\"end\":30835,\"start\":30824},{\"end\":30843,\"start\":30835},{\"end\":31085,\"start\":31079},{\"end\":31093,\"start\":31085},{\"end\":31101,\"start\":31093},{\"end\":31116,\"start\":31101},{\"end\":31470,\"start\":31464},{\"end\":31478,\"start\":31470},{\"end\":31493,\"start\":31478},{\"end\":31501,\"start\":31493},{\"end\":31853,\"start\":31847},{\"end\":31860,\"start\":31853},{\"end\":31870,\"start\":31860},{\"end\":32105,\"start\":32095},{\"end\":32115,\"start\":32105},{\"end\":32126,\"start\":32115},{\"end\":32138,\"start\":32126},{\"end\":32403,\"start\":32393},{\"end\":32411,\"start\":32403},{\"end\":32428,\"start\":32411},{\"end\":32439,\"start\":32428},{\"end\":32449,\"start\":32439},{\"end\":32457,\"start\":32449},{\"end\":32756,\"start\":32744},{\"end\":32765,\"start\":32756},{\"end\":32777,\"start\":32765},{\"end\":33013,\"start\":33000},{\"end\":33023,\"start\":33013},{\"end\":33229,\"start\":33221},{\"end\":33242,\"start\":33229},{\"end\":33250,\"start\":33242},{\"end\":33613,\"start\":33605},{\"end\":33622,\"start\":33613},{\"end\":33628,\"start\":33622},{\"end\":33636,\"start\":33628},{\"end\":33909,\"start\":33901},{\"end\":34139,\"start\":34127},{\"end\":34147,\"start\":34139},{\"end\":34158,\"start\":34147},{\"end\":34168,\"start\":34158}]", "bib_venue": "[{\"end\":24023,\"start\":23965},{\"end\":24840,\"start\":24829},{\"end\":25668,\"start\":25662},{\"end\":25945,\"start\":25941},{\"end\":26210,\"start\":26204},{\"end\":26468,\"start\":26462},{\"end\":26833,\"start\":26827},{\"end\":27187,\"start\":27181},{\"end\":27503,\"start\":27492},{\"end\":28036,\"start\":28023},{\"end\":28331,\"start\":28320},{\"end\":28588,\"start\":28582},{\"end\":29220,\"start\":29214},{\"end\":29507,\"start\":29501},{\"end\":30019,\"start\":30013},{\"end\":30264,\"start\":30258},{\"end\":30559,\"start\":30548},{\"end\":30863,\"start\":30857},{\"end\":32154,\"start\":32150},{\"end\":32487,\"start\":32476},{\"end\":32797,\"start\":32791},{\"end\":33043,\"start\":33037},{\"end\":33666,\"start\":33655},{\"end\":33939,\"start\":33928},{\"end\":34198,\"start\":34187},{\"end\":23963,\"start\":23899},{\"end\":24461,\"start\":24409},{\"end\":24827,\"start\":24810},{\"end\":25247,\"start\":25216},{\"end\":25660,\"start\":25648},{\"end\":25939,\"start\":25929},{\"end\":26202,\"start\":26190},{\"end\":26460,\"start\":26448},{\"end\":26825,\"start\":26813},{\"end\":27179,\"start\":27167},{\"end\":27490,\"start\":27473},{\"end\":27705,\"start\":27661},{\"end\":28021,\"start\":28002},{\"end\":28318,\"start\":28301},{\"end\":28580,\"start\":28568},{\"end\":28902,\"start\":28876},{\"end\":29212,\"start\":29200},{\"end\":29499,\"start\":29487},{\"end\":29725,\"start\":29663},{\"end\":30011,\"start\":29999},{\"end\":30256,\"start\":30244},{\"end\":30546,\"start\":30529},{\"end\":30855,\"start\":30843},{\"end\":31179,\"start\":31116},{\"end\":31462,\"start\":31391},{\"end\":31845,\"start\":31787},{\"end\":32148,\"start\":32138},{\"end\":32474,\"start\":32457},{\"end\":32789,\"start\":32777},{\"end\":33035,\"start\":33023},{\"end\":33306,\"start\":33250},{\"end\":33653,\"start\":33636},{\"end\":33926,\"start\":33909},{\"end\":34185,\"start\":34168}]"}}}, "year": 2023, "month": 12, "day": 17}