{"id": 216641636, "updated": "2023-10-06 16:40:56.81", "metadata": {"title": "TextGAIL: Generative Adversarial Imitation Learning for Text Generation", "authors": "[{\"first\":\"Qingyang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Zhou\",\"last\":\"Yu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 4, "day": 7}, "abstract": "Exposure bias is one of the most challenging problems in text generation with maximum likelihood estimation (MLE) based methods. Previous studies try to tackle this problem by applying Generative Adversarial Networks. However, lacking sufficient language priors makes those methods perform worse than MLE-based methods. To address this problem, we propose TextGAIL, a generative adversarial imitation learning framework with large pretrained language models RoBERTa and GPT-2. We also leverage proximal policy optimization, and efficiently utilize human demonstrations to stabilize the generator's training. We evaluate TextGAIL on a story ending generation task on the ROCStories dataset. We introduce the Perception score to evaluate our model. Experiments show that with the help of the large pretrained language models, our proposed method can generate highly diverse and appropriate story endings. We release the code and generated outputs.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2004.13796", "mag": "3023581195", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/Wu0Y21", "doi": "10.1609/aaai.v35i16.17656"}}, "content": {"source": {"pdf_hash": "2ff04294838863ec2d2087cbaa36bac48bf0e555", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2004.13796v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "01bf021f67a7978a95d1bc09c2d90ca87128ce0a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2ff04294838863ec2d2087cbaa36bac48bf0e555.txt", "contents": "\nTextGAIL: Generative Adversarial Imitation Learning for Text Generation\n\n\nQingyang Wu \nUniversity of California\nDavis\n\nLei Li \nUniversity of California\nDavis\n\nZhou Yu \nUniversity of California\nDavis\n\nTextGAIL: Generative Adversarial Imitation Learning for Text Generation\n\nExposure bias is one of the most challenging problems in text generation with maximum likelihood estimation (MLE) based methods. Previous studies try to tackle this problem by applying Generative Adversarial Networks. However, lacking sufficient language priors makes those methods perform worse than MLE-based methods. To address this problem, we propose TextGAIL, a generative adversarial imitation learning framework with large pretrained language models RoBERTa and GPT-2. We also leverage proximal policy optimization, and efficiently utilize human demonstrations to stabilize the generator's training. We evaluate TextGAIL on a story ending generation task on the ROCStories dataset. We introduce the Perception score to evaluate our model. Experiments show that with the help of the large pretrained language models, our proposed method can generate highly diverse and appropriate story endings. We release the code and generated outputs.\n\nIntroduction\n\nText generation is gaining popularity in research recently. The most widely used approach for neural text generation is to maximize the joint probability of the target text sequence (Bengio et al., 2000), which is also referred as Maximum Likelihood Estimation (MLE) . However, MLE suffers from the exposure bias problem caused by the traininginference discrepancy. During training, the model is trained on the ground truth, but during inference, the model needs to autoregressively predict the next word conditioned to its own previously generated words. This discrepancy leads to dull and repetitive sentences (Welleck et al., 2019;Wiseman and Rush, 2016;Serban et al., 2017). As a consequence, solving the exposure bias problem becomes important to improve the text generation quality.\n\nOne direction of reducing exposure bias is to utilize Generative Adversarial Networks (GAN). The main idea is to alternately train a discriminator to distinguish real samples from generated samples, and train the generator to improve its generated samples against the discriminator. Along this direction, there have been many studies that achieve certain progress. Nevertheless, there are increasing criticisms towards text GANs. Caccia et al. (2019) claims that GAN generated text is substantially worse than the ones generated by MLE. In the mean time, the recent large generative pretrained language models have greatly improved the quality of MLE generations (Radford and Sutskever, 2018;Radford et al., 2019).\n\nWe want to explore the possibility of applying GAN to the pretrained language models to solve the exposure bias problem while preserving stable results. Thus, we propose TextGAIL, a generative adversarial training framework that exploits the large-scale pretrained language models RoBERTa (Liu et al., 2019) and GPT-2 (Radford et al., 2019). However, traditional text GAN methods suffer from high variance of gradients. To reduce variance, we leverage the recent advancements in reinforcement learning (RL): generative imitation learning (GAIL) (Ho and Ermon, 2016), proximal policy optimization (PPO) (Schulman et al., 2017), and efficient use human demonstrations (Paine et al., 2019) to help optimize the generator.\n\nTo validate our model, we apply TextGAIL on one difficult language generation task -story ending generation, which is based on the ROCStories Corpora (Mostafazadeh et al., 2016). Story ending generation is a more open-ended task compared to some other language generation tasks, such as machine translation (Sutskever et al., 2014) or abstractive text summarization (Nallapati et al., 2016). It cannot use automatic evaluation metrics that are based on n-gram overlap such as BLEU (Papineni et al., 2002), because one story can have various diverse endings. To remedy this problem, we introduce Perception score to assess the text quality. It approximately measures the similarity between the real and generated samples' distributions.\n\nOur contributions of the paper are as follows: (1) Compared to the previous text GAN approaches, our method exploits the large-scale pretrained language models.\n\n(2) Further, we leverage generative adversarial imitation learning (GAIL), proximal policy optimization (PPO), and other techniques to stabilize the training of the generator. (3) Experimental results show that our method can generate highly diverse results with better logic and commonsense compared to MLE, suggesting the importance of large pretrained language models in improving the adversarial training performance.\n\n\nRelated Work\n\nExposure bias can lead to problems such as neural text degeneration by generating generic and repetitive generations. Even after the emergence of large-scale pretrained language model GPT-2, this problem is still prevalent. Many works tried to use Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) to solve the exposure bias problem caused by MLE (Ranzato et al., 2016;Welleck et al., 2019).\n\nSeqGAN (Yu et al., 2017) is the very first paper which adopts the adversarial training idea at text generation. As text sequence is discrete, SeqGAN applies REINFORCE (Williams, 1992), which is a policy gradient algorithm, to train the generator with a reward defined by the discriminator's prediction on the generated sample. The alternative is to apply Gumbel Softmax to relax discrete distribution to have a continuous approximation which enables the gradient back-propagation from the discriminator to the generator (Kusner and Hern\u00e1ndez-Lobato, 2016). However, these two methods have very high variance, and are highly unstable in training. We tried to implement them on GPT-2, but failed to reach convergence.\n\nThere are many other text GANs (Ke et al., 2019;Che et al., 2017;Guo et al., 2018;Nie et al., 2019) that tried to solve the stability problem. The main difference is that we leverage the large pretrained language models for generative adversarial training. In specific, we use RoBERTa as the discriminator, and GPT-2 as the generator. We show the importance of large-scale pretraining to help improve the quality of text generations.\n\nBesides, we utilize the recent Reinforcement Learning (RL) achievements, specifically Imitation Learning (IL) to combine the benefits of reinforcement learning and supervised learning. Behavior Cloning (BC) (Rahmatizadeh et al., 2018) is a simple IL method which is equivalent to MLE. However, BC is not generalizable to unseen data, due to its compounding error (Ross and Bagnell, 2010). Inverse Reinforcement Learning (IRL) (Ziebart et al., 2008) inversely learns the reward function on human demonstrations to solve this problem, but in practice it is computationally expensive. Ho and Ermon (2016) proposed generative adversarial imitation learning (GAIL) to improve IRL. We apply GAIL to text generation, and use proximal policy optimization (PPO) (Schulman et al., 2017), and efficient use of human demonstrations (Paine et al., 2019) to further stabilize the training. Figure 1: Overall architecture of TextGAIL. The green arrow represents the samples from human demonstrations, while the yellow arrow represents the samples from the generator. p is the ratio between human demonstrations and generated samples during training.\n\n\nMethods\n\nIn this section, we first introduce the main generative adversarial imitation learning framework. Then we explain the details of the paired discriminator and text generator, which are augmented with the large pretrained language models -RoBERTa and GPT-2. We further combine efficient use of human demonstrations to stabilize the training. In the end, we summarize the entire training process. The overall architecture is shown in Figure 1.\n\n\nGenerative Adversarial Imitation Learning\n\nThe training framework is mostly based on generative adversarial imitation learning (GAIL). The framework consists of a generator p \u03b8 and a discriminator D \u03c6 , which are parameterized with \u03b8 and \u03c6, respectively. The goal of the generator is to output sequences similar to human demonstrations. Meanwhile, the discriminator needs to distinguish the real from the generated sequences, and provide a single sparse reward for each generated sequence.\n\nHere, we only consider the conditional generation setting. We have a context x and want to generate the corresponding target sequence y. y can either be given from ground truth in the dataset as real data or sampled from the generator p \u03b8 as fake data. GAIL finds a saddle point where the generator and discriminator satisfy the following expression:\nmin p \u03b8 max D \u03c6 E y\u223cp * [D \u03c6 (x, y)] + E y\u223cp \u03b8 [1 \u2212 D \u03c6 (x, y)]\n(1) where p * represents the real data distribution.\n\n\nPaired Discriminator\n\nThe discriminator aims to distinguish between the real and generated samples. Traditional discriminator utilizes logistic loss (sigmoid), but it saturates quickly after the model learns the difference between the real and the generated samples. Moreover, logistic loss causes higher variance as it is hard for the model to calibrate its prediction based on a single estimate.\n\nA better alternative to perform prediction is to utilize softmax cross-entropy instead of logistic loss. To calculate softmax cross-entropy, for each context x, we need one paired real target y * and a generated target y as the inputs. The objective is to assign higher probabilities to the real target. Then we train the discriminator with cross-entropy loss:\nL D (\u03c6) = \u2212 E y\u223cp \u03b8 ,y * \u223cp * [log D \u03c6 (x, y * )] (2)\nD(x, y * ) is the probability prediction after softmax for the real sample y * . The paired discriminator can calibrate the predictions close to the real samples, which avoids overconfidence in logistic output. The probability of fake sequence D(x, y) will be used as reward to optimize the generator. Zellers et al. (2019) shows that the paired discriminator yields higher accuracy in fake news detection. When applied in the adversarial setting, the model can also be viewed as an importance sampling version of GAN that reduces variance compared to the traditional GANs (Lin, 2017).\n\n\nText Generator\n\nFor the generator, we begin by defining the probability of a text sequence as the joint probability of all the tokens:\np \u03b8 (y 1:T |x) = T t=0 p \u03b8 (y t |y <t , x)(3)\nwhere y 1:T is a text sequence. T is the sequence length, and w t is the word at the time step t. We sample from this distribution with Monte Carlo search to acquire the generated sequences. Previous GAN methods have used policy gradient to directly optimize the generator with generated outputs which maximizes the expected reward below:\nE[R(x, y)] = E y\u223cp \u03b8 [1 \u2212 D \u03c6 (x, y)](4)\nDirectly optimizing this objective suffers from high variance of gradients, because the D \u03c6 is not stationary.\n\nAs a solution to reduce high variance, the original GAIL employs trust region policy optimization (TRPO) (Schulman et al., 2015), as it is crucial to ensure that p \u03b8 i+1 does not move too far away from p \u03b8 i . However, TRPO needs to compute natural gradient which is computationally expensive. As an alternative, we replace it with a more recent and stable method, proximal policy optimization (PPO) (Schulman et al., 2017). Compared to TRPO, PPO is easier to implement and generalize. PPO has better sample complexity in practice as well.\n\nPPO applies importance sampling by the likelihood ratio between the current and old policy for y \u223c p \u03b8 old (\u00b7|x):\nr(\u03b8) = p \u03b8 (y | x) p \u03b8 old (y | x)(5)\nThen it maximizes the expected reward by optimizing the following surrogate:\nL policy (\u03b8) = min r(\u03b8)\u00c2 y clip (r(\u03b8), 1 \u2212 , 1 + )\u00c2 y(6)\nwhere\u00c2 y is the advantage term that controls the update. This surrogate serves the same purpose as TRPO to have a trust region constraint on the gradient update. It will prevent the generator from moving too far away from the pretrained language model.\n\nIn addition, we add the KL divergence between the generator and another pretrained language model in replacement of the traditional maximum entropy regularizer. The final generator objective can be written as the following:\nL G (\u03b8) = E y\u223cp \u03b8 (\u00b7|x) [L policy (\u03b8) + \u03b2 KL(q| p \u03b8 )] (7)\nq is another pretrained language model. The benefit for this replacement is to add entropy bonus to stimulate more exploration of the generator. Also it limits the generator distribution from changing too much from the pretrained language model q.\n\n\nEfficient Use of Human Demonstrations\n\nAnother problem is that GAIL alone has difficulty in solving complex exploration tasks (Paine et al., 2019). Especially in text generation, it faces a series of challenges, such as sparse rewards, latent knowledge, and highly diverse context. Fortunately, the recent work of DeepMind's Recurrent Replay Distributed DQN from Demonstrations (R2D3) (Paine et al., 2019) introduced another imitation learning method that efficiently uses human demonstrations to solve hard exploration problems in RL. We apply the this method to the TextGAIL framework.\n\nDuring the training of the generator, we introduce a hyper-parameter p to control the ratio between the sampling from human demonstrations and generated sequences. Moreover, we force the advantage value for human demonstrations to always be a constant across the training.\nA y = constant y \u2208 human demos, R otherwise,(8)\nR is the reward after normalization. By fixing the human demonstrations' advantage to be a constant, it stabilizes the training to update the generator distribution to be close to the real data distribution.\n\n\nTraining\n\nFinally, we present the algorithm of TextGAIL. First, the generator is trained with part of the training set using MLE as warm-up. We also include a replay buffer to store temporary generated outputs and human demonstrations. After the replay buffer is full, the discriminator would assign rewards to all the context-target pairs in the replay buffer. Next, we normalize all the rewards in the buffer to haveR. We use a constant reward for all human demonstrations. We update the generator p \u03b8 with PPO using the replay buffer. In the meantime, we update the discriminator D \u03c6 with the real and generated pairs. We repeat the process until the model over-fits. The summary of the training process is illustrated in Algorithm 1. Update the discriminator \u03c6 with Eq. 2 10:\n\nUpdate the generator \u03b8 using the PPO with Eq. 7 and Eq. 8 11:\n\nClear Buffer B 12: end for 4 Perception Score Traditional n-gram matching metrics such as BLEU and ROUGE are not sufficient to evaluate openended text generation tasks. More and more studies rely on using human evaluators to determine the final quality of generated text. However, fully using human evaluation instead of automatic evaluation is expensive and inconvenient in practice.\n\nWe introduce Perception score, which is a simple automatic evaluation metric for text generation built on the top of fake text detector. We report the accuracy of detecting fake text as the Perception score. The assumption of this approach is that the pretrained language models such as BERT have acquired plenty of prior knowledge during pretraining. We can use them to detect not only the statistical difference, but also the semantic errors of the generated text. If it has low accuracy in detecting generated text, it means that the real and generated text have very similar distribution. Also, to reduce variance, we only train the discriminator with one epoch on a fixed dataset, where the generated samples are acquired from greedy sampling However, the drawback for this method is that it relies on the researchers themselves to train this fake text detector Its result can be manipulated easily and misleading. It is better to use it as a selfrating rather than a metric for comparison. However, while Perception score is still a preliminary version of the metric and has limitations, we believe it merits further exploration to fill in the blank of text evaluation metrics.\n\n\nExperiments\n\nWe first introduce the dataset, and the automatic evaluation metrics we used to evaluate TextGAIL. Then we describe the implementation details.\n\n\nDataset\n\nWe evaluate TextGAIL on the ROCStories dataset (Mostafazadeh et al., 2016). The task is to generate an open-ended ending for a four-sentence story. Different from unconditional or other more constrained text generation tasks, remembering the training set will not lead to good performance in this task, as every story is unique. In ROCStories, the training set contains 98,162 five-sentence stories. The fifth sentence in the story is considered to be the ending of the story. Both the validation set and the test set have 1,871 samples. Unlike the training set, these stories have another wrong ending constructed adversarially by human. These adversarial endings are fluent and relevant but contain commonsense mistakes. We show an example in Figure 2, where the blue box is the context, the green one is the right ending, and the yellow one is wrong ending. We also demonstrate if the yellow is generated by the generator, how the discriminator would assign the reward.\n\n\nAutomatic Evaluation Metrics\n\nBecause multiple story ending can be appropriate under the same story context, n-gram-based reference matching metrics such as BLEU and ROUGE cannot reflect the generation quality. Therefore, rather than using the traditional automatic evaluation metrics, we decide to evaluate both the overall quality and diversity of the generation using three metrics: Perception Score, sentence level repetition and unique n-gram.\n\nPerception score (Perception): We use a large pretrained language model RoBERTa-base as a classifier to distinguish between the real and generated samples. We train the classifier for one epoch on the validation set, and use greedy sampling to sample outputs from the generator. Then we deploy the classifier to the test set, and report the accuracy as the final score. The resulting score approximately reflects the similarity of the real and generated distributions.\n\nSentence-level repetition (Sent-REP 2grams): We use sentence-level repeated 2-grams to evaluate the repetition in the corpus, which usually accounts for the dull and generic sentences. We first collect all the 2-grams that appear more than once in the training set. Then we count the number of occurrences of those 2-grams in the generated story endings. Sent-REP 2-grams can capture the sentence-level repetition, as those repetitive and generic sentences patterns often occur more than once in the corpus. Finally, we report the result normalized with the total number of the 2-grams in all the story endings.\n\nUnique n-grams (Unique n-grams): We also follow Xu et al. (2018) to use the number of unique n-grams to evaluate language diversity. We report the number of unique 1-gram and unique 2-grams.\n\n\nImplementation Details\n\nTextGAIL takes the advantage of large pretrained language models. The generator uses the GPT-2 base (117M parameters) model, while the discriminator uses the RoBERTa-base (125M parameters) model. One can use any other pretrained language model for the generator and discriminator. We choose these two models because they share the same vocabulary, which saves us the effort from extra tokenization. We warm up the generator with 20% of data in the training set. We apply nucleus sampling    ing training, the temperature is set to 1.0, as we find that a lower temperature makes the the discriminator easy to distinguish real samples from generated samples. During evaluation, we set temperature for all models to 0.8. We train about 10 epochs, and we use Perception score to estimate over-fitting. The p ratio is set to 0.2. The constant reward for human demonstrations is set to 2.0.\n\n\nResults and Analysis\n\nWe first show the automatic evaluation, and then the human evaluation results. In both results, TextGAIL performs better compared against MLE. Finally, we conduct case study and error analysis to interpret the results from TextGAIL.\n\n\nAutomatic Evaluation Results\n\nWe want to test if TextGAIL is better than simply fine-tuning GPT-2 with MLE in the story ending generation task. As mentioned previously, n-gram match metrics scores such as BLEU are not applicable. We focus on reporting the Perception score as an approximate quality metric, and the three diversity metrics: Sent-Rep 2-grams, Unique 1-gram, and Unique 2-grams. For Perception score, we run the experiments five times, and for other metrics, three times to report the mean and variance. The results are shown in Table 1. TextGAIL performs well in terms of diversity, which is reflected by the three diversity metrics. Our results are considered even more diverse than the ground truth written by human. This suggests that TextGAIL has utilized the pretrained language models to alleviate the exposure bias problem. However, we still need other metrics to ensure the overall quality such as logic is not affected. We use Perception Score to approximately measure the similarity between the real and generated samples distributions. TextGAIL achieves lower score than MLE, meaning that the TextGAIL generated outputs are more similar to the human written ground truth. Theoretically, human written story endings would have Perception Score 0.5, which is the lower bound.\n\nThe overall automatic evaluation results suggest that TextGAIL generations are more diverse and similar to the human written ground truth.\n\nFor the ablation study, we replace the pretrained discriminator RoBERTa-base with another randomly initialized discriminator. Though the adversarial training process is still stable, we observe severe performance drop which is also reflected by the automatic evaluation scores. This observation matches some previous criticisms that GAN generated text is worse than MLE. Through our experiments, we suspect that this is possibly due to the fact that without pretraining, the discriminator does not have sufficient prior knowledge to distinguish the real and generated samples from the generator, and therefore does not provide helpful reward signal to the generator. Different from the previous text GANs, our method exploits the large pretrained language models and achieves much better performance than MLE. \n\n\nHuman Evaluation Results\n\nAs automatic evaluation metrics are limited, we conduct human evaluation here. We randomly select 40 samples from the test set. Each sample contains one story body and two endings. We ask Amazon Mechanical Turkers to vote which ending fits the story body better. To guarantee Turkers voting quality, we also mix 10 human-written right endings and wrong endings pairs from the validation set, where the answers are obvious for this task. If the workers answer any one of those pairs wrong, we will exclude their results. In the end, Example 1 Example 2\n\n\nContext:\n\nI wanted to buy a video game console. I asked my parents, and they came up with an idea. They said if I did my chores, I would be given money to save. I did my chores without being asked every week for a whole summer.\n\nBen went to the DMV to get his License. The instructor gave Ben a passing grade at the end. Excited, Ben calls up his father to tell him the good news. Ben father never picked up, he died in a car accident that day.\n\nGround Truth: My parents gave me enough money to buy the console.\n\nBen was devastated.\n\n\nMLE:\n\n(1) Now I have the video game console I asked for.\n\n(2) It was an awesome idea.\n\n(3) The next week, I had to buy a new gaming console!\n\n(1) Ben was happy to learn his lessons about being smart.\n\n(2) Ben's father is now very sad, and he has a job to do.\n\n(3) Ben was happy that his dad was alive.\n\nTextGAIL:\n\n(1) I bought a PlayStation 4 to play with my parents.\n\n(2) I was so happy when my parents gave me a Wii U.\n\n(3) When I got my console, I played my favorite video games.\n\n(1) Ben regrets going to the DMV.\n\n(2) Ben mourns the loss of his father but also the passing of a great man.\n\n(3) It seems like too much to bear. we collect 10 unique and valid votes for each test sample, which results in 400 votes in total. As shown in Table 2, TextGAIL significantly outperforms GPT-2 fine-tuned with MLE, by having 158% more votes. This proves TextGAIL can not only generate more diverse story endings, but also achieves better overall quality judged by humans. It also suggests that TextGAIL achieves better commonsense reasoning than MLE. In the Case Study Section, we show what leads to the huge improvement over MLE. We also compare the results of TextGAIL with ground truth ending that is written by humans. Nevertheless, even though TextGAIL improves the text generation quality compared to MLE, human written endings are still better (52.20%). In the error analysis section, we demonstrate the main discrepancy that contributes to our model's imperfection.\n\n\nCase Study\n\nTo better understand the difference between MLE and TextGAIL outputs, we carefully analyze the generated results. Here, we choose two example stories and generate their endings by randomly sample three times with nucleus sampling and to reduce observation bias. The two example stories are shown in Table 3. These two examples showcase the two main types of errors from MLE.\n\nOne major problem of MLE's output is being generic and repetitive, which is also referred as neural text degeneration (Welleck et al., 2019). In Example 1, MLE generated outputs lack of details and are universal. The generated output (2) appears in other story contexts. Meanwhile, TextGAIL can generate detailed named entities such as \"PlayStation 4\" and \"Wii U\", and have better logic than MLE. Surprisingly, those named entities never appeared in the training data. They probably come from the large-scale corpus in pretraining GPT-2 and RoBERTa.\n\nAnother problem of MLE is the lack of commonsense. The Example 2 reflects this problem. All three MLE generated outputs do not make sense to humans. They either are against the factual information in the context, or do not follow the progression of the story. In contrast, the three TextGAIL generated samples appear to be much more logical. This difference suggests that the discriminator can distinguish the real and generated samples by using commonsense. Such commonsense later helps to improve the generated endings' quality. In Section 7, we specifically analyze the discriminator's capability of common sense reasoning after adversarial training.\n\n\nError Analysis\n\nHowever, we need to be aware of TextGAIL's limitations, as it is not perfect in terms of following commonsense. We can also observe its limitation from human evaluation results when comparing against the ground truth. We have conducted error analysis of those failed results from human\n\n\nContext:\n\nAri spends $20 a day on pickles. He decides to make his own to save money. He puts the pickles in brine. Ari waits 2 weeks for his pickles to get sour.\n\n\nRight Ending:\n\nAri opens the jar to find perfect pickles. Generated Ending: When his pickles are rotten, he dumps it. evaluation. In Table 4, an example of generated endings that contain commonsense error is shown. For human, we know that pickles' taste is sour, but it does not mean it is rotten. However, the machine cannot correctly understand the difference between \"sour\" and \"rotten\", but probably based on its learned prior knowledge, it makes the association and generate the wrong ending. \"Sour\" sometimes does have association with \"rotten\", but here \"sour\" is a property of \"pickles\". This result shows that commonsense reasoning is still a very hard task in text generation. We will examine more direct approaches such as using knowledge graph to tackle this problem in the future.\n\n\nUnsupervised Commonsense Reasoning\n\nIn order to verify our hypothesis that the discriminator learns the difference between the real and generated samples by using commonsense reasoning, we evaluate our learned discriminator on the ROCStories' correct ending prediction task. As mentioned, the test set contains two candidate endings: one is right and the other is wrong. The difference is that the wrong ending contains commonsense error. For this reason, this dataset is often used for commonsense reasoning. We use the learned discriminator to classify which one is the right one. Ideally, the discriminator has captured the commonsense errors to distinguish real samples from generated samples, and therefore it should tell the difference on a separate data. We compare our discriminator with two baselines. One baseline, GPT-2+MLE, uses a language model to assign a probability to each ending to see which one has a higher probability (Trinh and Le, 2018). Here, we use GPT-2 fine-tuned with MLE as the language model. The other baseline, RoBERTa+Fine-tune, is to train RoBERTa as a text classifier on the validation set with 1,872 pairs of right and wrong endings, and then predict on the test set. The results are shown in Table 5: As we expected, our adversarially trained dis-  criminator can accomplish story ending prediction without any supervision to a certain degree. Our model achieves 79.1% accuracy which is much better than the other unsupervised baseline GPT-2+MLE (69.6%). However, our model still is not as good as supervised method such as RoBERTa+Fine-tune (92.8%). We also test the discriminator with randomly initialized weights instead of RoBERTa, and find that without the pretrained language model, it is impossible to accomplish unsupervised commonsense reasoning (51.2%). The results are consistent with our hypothesis that the discriminator uses commonsense from the pretrained RoBERTa to distinguish the real and generated samples.\n\n\nConclusion\n\nDespite the success on image generation, GANs have not yet achieved considerable progress on text generation. One of the difficulties is the instability of applying GANs on discrete data. Also, we observe that another reason may be the lack of sufficient language priors for the discriminator.\n\nTo tackle those problems, we propose a novel adversarial training framework -TextGAIL, which leverages the large pretrained language models. We further utilize generative adversarial imitation learning, proximal policy optimization, and efficient use of human demonstrations to solve the instability problem of generator's training. Also, we introduce a new automatic evaluation metric Perception score which uses the RoBERTa as a fake text classifier to assess the quality of generated sentences. Experiment shows that TextGAIL can generate not only more diverse, but more logical and reasonable text. This paper extends the exploration of adversarial training on text generation. We plan to apply TextGAN to other text generation tasks such dialogue generation and text summarization in the future.\n\nFigure 2 :\n2An example of story ending generation in TextGAIL. D is the discriminator. G is the generator. for deterministic outputs.\n\n\nin decoding to avoid low probability words being sampled. Dur-Models \n\nPerplexity\u2193 \nPerception\u2193 \nSent-Rep 2-grams \u2193 Unique 1-gram \u2191 Unique 2-grams \u2191 \n\nGPT-2 + MLE \n11.61 \u00b1 0.25 0.94 \u00b1 0.0067 \n0.68 \u00b1 0.21 \n2,847 \u00b1 119 \n10,085 \u00b1 131 \n\nTextGAIL \n16.20 \u00b1 0.94 0.77 \u00b1 0.0095 \n0.58 \u00b1 0.24 \n3,420 \u00b1 147 \n11,354 \u00b1 92.8 \nw/ random init discriminator** 14.92 \u00b1 0.53 0.95 \u00b1 0.0093 \n0.71 \u00b1 0.21 \n2,402 \u00b1 97.5 \n8,476 \u00b1 47.5 \n\nHuman \n-\n0.50* \n0.60 \u00b1 0.23 \n3,157 \n10,109 \n\n\n\nTable 1 :\n1Automatic evaluation results. *Perception Score for human written endings is theoretically 0.5. **The discriminator is initialized with random weights instead of RoBERTa-base.\n\nTable 2 :\n2Results of Human Evaluation, showing preferences (%) between different story endings. GT means Ground Truth endings written by human.Model \nWin \nLose \nTie \n\nMLE vs. GT \n21.20% 72.60% \n6.20 % \nTextGAIL vs. GT \n41.00% 52.20% \n6.80 % \nTextGAIL vs. MLE 60.52% 23.44% 16.04 % \n\n\n\nTable 3 :\n3Examples of generated results from MLE and TextGAIL. We sample three samples to reduce bias.\n\nTable 4 :\n4An example of generated endings that contain commonsense error.\n\n\nTextGAIL discriminator with random weights \u00d7 51.2 \u00b1 0.85Models \nSupervised? Accuracy(%) \n\nRoBERTa+Fine-tune w/ extra data \n92.8 \u00b1 0.28 \n\nGPT-2 + MLE* \n\u00d7 \n69.6 \u00b1 0.35 \n\nTextGAIL discriminator \n\u00d7 \n79.1 \u00b1 0.76 \n\n\nTable 5 :\n5Results of unsupervised commonsense classification. *We use the language model's sentence joint probability to do classification.\n\nA neural probabilistic language model. Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS). Denver, CO, USAMIT PressYoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. In Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Sys- tems (NIPS) 2000, Denver, CO, USA, pages 932- 938. MIT Press.\n\nLanguage gans falling short. Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, Laurent Charlin, abs/1811.02549CoRRMassimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2018. Language gans falling short. CoRR, abs/1811.02549.\n\nMaximum-likelihood augmented discrete generative adversarial networks. Yanran Tong Che, Ruixiang Li, R Devon Zhang, Wenjie Hjelm, Li, abs/1702.07983CoRRTong Che, Yanran Li, Ruixiang Zhang, R. Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. 2017. Maximum-likelihood augmented discrete generative adversarial networks. CoRR, abs/1702.07983.\n\n. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, Yoshua Bengio, abs/1406.2661Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Gen- erative adversarial networks. CoRR, abs/1406.2661.\n\nLong text generation via adversarial training with leaked information. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)Louisiana, USAAAAI PressJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Long text generation via adversarial training with leaked information. In Pro- ceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innova- tive Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Ad- vances in Artificial Intelligence (EAAI-18), New Or- leans, Louisiana, USA, February 2-7, 2018, pages 5141-5148. AAAI Press.\n\nGenerative adversarial imitation learning. Jonathan Ho, Stefano Ermon, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016. Barcelona, SpainJonathan Ho and Stefano Ermon. 2016. Generative ad- versarial imitation learning. In Advances in Neu- ral Information Processing Systems 29: Annual Conference on Neural Information Processing Sys- tems 2016, December 5-10, 2016, Barcelona, Spain, pages 4565-4573.\n\nThe curious case of neural text degeneration. Ari Holtzman, Jan Buys, Maxwell Forbes, Yejin Choi, abs/1904.09751CoRRAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degener- ation. CoRR, abs/1904.09751.\n\nARAML: A stable adversarial training framework for text generation. Pei Ke, Fei Huang, Minlie Huang, Xiaoyan Zhu, abs/1908.07195CoRRPei Ke, Fei Huang, Minlie Huang, and Xiaoyan Zhu. 2019. ARAML: A stable adversarial training frame- work for text generation. CoRR, abs/1908.07195.\n\nGANS for sequences of discrete elements with the gumbel-softmax distribution. J Matt, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato Kusner, abs/1611.04051CoRRMatt J. Kusner and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. 2016. GANS for sequences of discrete ele- ments with the gumbel-softmax distribution. CoRR, abs/1611.04051.\n\nAdversarial ranking for language generation. Kevin Lin, Dianqi Li, Xiaodong He, Ming-Ting Sun, Zhengyou Zhang, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USAKevin Lin, Dianqi Li, Xiaodong He, Ming-Ting Sun, and Zhengyou Zhang. 2017. Adversarial ranking for language generation. In Advances in Neural Infor- mation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4- 9 December 2017, Long Beach, CA, USA, pages 3155-3165.\n\n. Min Lin, Softmax GAN. Min Lin. 2017. Softmax GAN.\n\n. Corr, abs/1704.06191CoRR, abs/1704.06191.\n\nRoberta: A robustly optimized BERT pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, abs/1907.11692CoRRYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining ap- proach. CoRR, abs/1907.11692.\n\nA corpus and evaluation framework for deeper understanding of commonsense stories. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James F Allen, abs/1604.01696CoRRNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vander- wende, Pushmeet Kohli, and James F. Allen. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. CoRR, abs/1604.01696.\n\nAbstractive text summarization using sequence-tosequence rnns and beyond. Ramesh Nallapati, Bowen Zhou, C\u00edcero Nogueira, Santos, Bing Aglar G\u00fcl\u00e7ehre, Xiang, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. the 20th SIGNLL Conference on Computational Natural Language LearningBerlin, GermanyACLRamesh Nallapati, Bowen Zhou, C\u00edcero Nogueira dos Santos, \u00c7 aglar G\u00fcl\u00e7ehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to- sequence rnns and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pages 280-290. ACL.\n\nRelgan: Relational generative adversarial networks for text generation. Weili Nie, Nina Narodytska, Ankit Patel, 7th International Conference on Learning Representations. LA, USAOpenReview.netWeili Nie, Nina Narodytska, and Ankit Patel. 2019. Relgan: Relational generative adversarial networks for text generation. In 7th International Conference on Learning Representations, ICLR 2019, New Or- leans, LA, USA, May 6-9, 2019. OpenReview.net.\n\nMaking efficient use of demonstrations to solve hard exploration problems. Tom Le Paine, Bobak Aglar G\u00fcl\u00e7ehre, Misha Shahriari, Matthew D Denil, Hubert Hoffman, Richard Soyer, Steven Tanburn, Neil C Kapturowski, Duncan Rabinowitz, Gabriel Williams, Ziyu Barth-Maron, Wang, abs/1909.01387CoRRNando de Freitas, and Worlds TeamTom Le Paine, \u00c7 aglar G\u00fcl\u00e7ehre, Bobak Shahriari, Misha Denil, Matthew D. Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil C. Ra- binowitz, Duncan Williams, Gabriel Barth-Maron, Ziyu Wang, Nando de Freitas, and Worlds Team. 2019. Making efficient use of demonstrations to solve hard exploration problems. CoRR, abs/1909.01387.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, PA, USAACLKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.\n\nImproving language understanding by generative pre-training. Alec Radford, Ilya Sutskever, Alec Radford and Ilya Sutskever. 2018. Improving lan- guage understanding by generative pre-training.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\nVision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau B\u00f6l\u00f6ni, Sergey Levine, 2018 IEEE International Conference on Robotics and Automation. Brisbane, AustraliaIEEERouhollah Rahmatizadeh, Pooya Abolghasemi, Ladis- lau B\u00f6l\u00f6ni, and Sergey Levine. 2018. Vision-based multi-task manipulation for inexpensive robots us- ing end-to-end learning from demonstration. In 2018 IEEE International Conference on Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pages 3758-3765. IEEE.\n\nSequence level training with recurrent neural networks. Aurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, 4th International Conference on Learning Representations. San Juan, Puerto RicoConference Track ProceedingsMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level train- ing with recurrent neural networks. In 4th Inter- national Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.\n\nEfficient reductions for imitation learning. St\u00e9phane Ross, Drew Bagnell, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort. the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna ResortSardinia, ItalyJMLR.org9St\u00e9phane Ross and Drew Bagnell. 2010. Efficient re- ductions for imitation learning. In Proceedings of the Thirteenth International Conference on Artifi- cial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceedings, pages 661-668. JMLR.org.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, Philipp Moritz, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015. the 32nd International Conference on Machine Learning, ICML 2015Lille, FranceJMLR.orgJMLR Workshop and Conference ProceedingsJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learn- ing, ICML 2015, Lille, France, 6-11 July 2015, vol- ume 37 of JMLR Workshop and Conference Pro- ceedings, pages 1889-1897. JMLR.org.\n\n. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, abs/1707.06347John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. CoRR, abs/1707.06347.\n\nA hierarchical latent variable encoder-decoder model for generating dialogues. Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C Courville, Yoshua Bengio, Singh and MarkovitchIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C. Courville, and Yoshua Bengio. 2017. A hierarchical latent variable encoder-decoder model for generating di- alogues. In (Singh and Markovitch, 2017), pages 3295-3301.\n\nP Satinder, Shaul Singh, Markovitch, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. the Thirty-First AAAI Conference on Artificial IntelligenceSan Francisco, California, USAAAAI PressSatinder P. Singh and Shaul Markovitch, editors. 2017. Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Fran- cisco, California, USA. AAAI Press.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, V Quoc, Le, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Montreal, Quebec, CanadaIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Sys- tems 27: Annual Conference on Neural Informa- tion Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3104-3112.\n\nA simple method for commonsense reasoning. H Trieu, Quoc V Trinh, Le, abs/1806.02847CoRRTrieu H. Trinh and Quoc V. Le. 2018. A sim- ple method for commonsense reasoning. CoRR, abs/1806.02847.\n\nNeural text generation with unlikelihood training. Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, abs/1908.04319CoRRSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di- nan, Kyunghyun Cho, and Jason Weston. 2019. Neu- ral text generation with unlikelihood training. CoRR, abs/1908.04319.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. Ronald J Williams, Machine Learning. 8Ronald J. Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine Learning, 8:229-256.\n\nSequence-to-sequence learning as beam-search optimization. Sam Wiseman, Alexander M Rush, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, Texas, USAThe Association for Computational LinguisticsSam Wiseman and Alexander M. Rush. 2016. Sequence-to-sequence learning as beam-search opti- mization. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 1296-1306. The Association for Com- putational Linguistics.\n\nDP-GAN: diversity-promoting generative adversarial network for generating informative and diversified text. Jingjing Xu, Xu Sun, Xuancheng Ren, Junyang Lin, Bingzhen Wei, Wei Li, abs/1802.01345CoRRJingjing Xu, Xu Sun, Xuancheng Ren, Junyang Lin, Bingzhen Wei, and Wei Li. 2018. DP- GAN: diversity-promoting generative adversarial network for generating informative and diversified text. CoRR, abs/1802.01345.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu, Singh and MarkovitchLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In (Singh and Markovitch, 2017), pages 2852-2858.\n\nDefending against neural fake news. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, abs/1905.12616CoRRRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. CoRR, abs/1905.12616.\n\nMaximum entropy inverse reinforcement learning. Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI. the Twenty-Third AAAI Conference on Artificial Intelligence, AAAIChicago, Illinois, USAAAAI PressBrian D. Ziebart, Andrew L. Maas, J. Andrew Bag- nell, and Anind K. Dey. 2008. Maximum entropy inverse reinforcement learning. In Proceedings of the Twenty-Third AAAI Conference on Artificial In- telligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 1433-1438. AAAI Press.\n", "annotations": {"author": "[{\"end\":119,\"start\":75},{\"end\":159,\"start\":120},{\"end\":200,\"start\":160}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":84},{\"end\":126,\"start\":124},{\"end\":167,\"start\":165}]", "author_first_name": "[{\"end\":83,\"start\":75},{\"end\":123,\"start\":120},{\"end\":164,\"start\":160}]", "author_affiliation": "[{\"end\":118,\"start\":88},{\"end\":158,\"start\":128},{\"end\":199,\"start\":169}]", "title": "[{\"end\":72,\"start\":1},{\"end\":272,\"start\":201}]", "venue": null, "abstract": "[{\"end\":1219,\"start\":274}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1438,\"start\":1417},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1869,\"start\":1847},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1892,\"start\":1869},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1912,\"start\":1892},{\"end\":2475,\"start\":2455},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2717,\"start\":2688},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2738,\"start\":2717},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3048,\"start\":3030},{\"end\":3081,\"start\":3053},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3306,\"start\":3286},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3366,\"start\":3343},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3427,\"start\":3407},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3638,\"start\":3611},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3792,\"start\":3768},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3851,\"start\":3827},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3965,\"start\":3942},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5109,\"start\":5084},{\"end\":5181,\"start\":5155},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5202,\"start\":5181},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5229,\"start\":5212},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5388,\"start\":5372},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5970,\"start\":5953},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5987,\"start\":5970},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6004,\"start\":5987},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6020,\"start\":6004},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6590,\"start\":6564},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6744,\"start\":6720},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6805,\"start\":6783},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6958,\"start\":6939},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7133,\"start\":7110},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7197,\"start\":7177},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10044,\"start\":10023},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10305,\"start\":10294},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11110,\"start\":11087},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11405,\"start\":11382},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12742,\"start\":12722},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16374,\"start\":16347},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18872,\"start\":18856},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25537,\"start\":25515}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31075,\"start\":30941},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31536,\"start\":31076},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31724,\"start\":31537},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32010,\"start\":31725},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32115,\"start\":32011},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32191,\"start\":32116},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32402,\"start\":32192},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32544,\"start\":32403}]", "paragraph": "[{\"end\":2023,\"start\":1235},{\"end\":2739,\"start\":2025},{\"end\":3459,\"start\":2741},{\"end\":4196,\"start\":3461},{\"end\":4358,\"start\":4198},{\"end\":4781,\"start\":4360},{\"end\":5203,\"start\":4798},{\"end\":5920,\"start\":5205},{\"end\":6355,\"start\":5922},{\"end\":7491,\"start\":6357},{\"end\":7943,\"start\":7503},{\"end\":8435,\"start\":7989},{\"end\":8787,\"start\":8437},{\"end\":8904,\"start\":8852},{\"end\":9304,\"start\":8929},{\"end\":9666,\"start\":9306},{\"end\":10306,\"start\":9721},{\"end\":10443,\"start\":10325},{\"end\":10828,\"start\":10490},{\"end\":10980,\"start\":10870},{\"end\":11521,\"start\":10982},{\"end\":11636,\"start\":11523},{\"end\":11751,\"start\":11675},{\"end\":12061,\"start\":11809},{\"end\":12286,\"start\":12063},{\"end\":12593,\"start\":12346},{\"end\":13183,\"start\":12635},{\"end\":13457,\"start\":13185},{\"end\":13713,\"start\":13506},{\"end\":14495,\"start\":13726},{\"end\":14558,\"start\":14497},{\"end\":14944,\"start\":14560},{\"end\":16129,\"start\":14946},{\"end\":16288,\"start\":16145},{\"end\":17272,\"start\":16300},{\"end\":17723,\"start\":17305},{\"end\":18193,\"start\":17725},{\"end\":18806,\"start\":18195},{\"end\":18998,\"start\":18808},{\"end\":19909,\"start\":19025},{\"end\":20166,\"start\":19934},{\"end\":21468,\"start\":20199},{\"end\":21608,\"start\":21470},{\"end\":22420,\"start\":21610},{\"end\":23000,\"start\":22449},{\"end\":23230,\"start\":23013},{\"end\":23447,\"start\":23232},{\"end\":23514,\"start\":23449},{\"end\":23535,\"start\":23516},{\"end\":23594,\"start\":23544},{\"end\":23623,\"start\":23596},{\"end\":23678,\"start\":23625},{\"end\":23737,\"start\":23680},{\"end\":23796,\"start\":23739},{\"end\":23839,\"start\":23798},{\"end\":23850,\"start\":23841},{\"end\":23905,\"start\":23852},{\"end\":23958,\"start\":23907},{\"end\":24020,\"start\":23960},{\"end\":24055,\"start\":24022},{\"end\":24131,\"start\":24057},{\"end\":25006,\"start\":24133},{\"end\":25395,\"start\":25021},{\"end\":25946,\"start\":25397},{\"end\":26601,\"start\":25948},{\"end\":26905,\"start\":26620},{\"end\":27069,\"start\":26918},{\"end\":27865,\"start\":27087},{\"end\":29830,\"start\":27904},{\"end\":30138,\"start\":29845},{\"end\":30940,\"start\":30140}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8851,\"start\":8788},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9720,\"start\":9667},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10489,\"start\":10444},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10869,\"start\":10829},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11674,\"start\":11637},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11808,\"start\":11752},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12345,\"start\":12287},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13505,\"start\":13458}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20719,\"start\":20712},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24284,\"start\":24277},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25327,\"start\":25320},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27212,\"start\":27205},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":29104,\"start\":29097}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1233,\"start\":1221},{\"attributes\":{\"n\":\"2\"},\"end\":4796,\"start\":4784},{\"attributes\":{\"n\":\"3\"},\"end\":7501,\"start\":7494},{\"attributes\":{\"n\":\"3.1\"},\"end\":7987,\"start\":7946},{\"attributes\":{\"n\":\"3.2\"},\"end\":8927,\"start\":8907},{\"attributes\":{\"n\":\"3.3\"},\"end\":10323,\"start\":10309},{\"attributes\":{\"n\":\"3.4\"},\"end\":12633,\"start\":12596},{\"attributes\":{\"n\":\"3.5\"},\"end\":13724,\"start\":13716},{\"attributes\":{\"n\":\"5\"},\"end\":16143,\"start\":16132},{\"attributes\":{\"n\":\"5.1\"},\"end\":16298,\"start\":16291},{\"attributes\":{\"n\":\"5.2\"},\"end\":17303,\"start\":17275},{\"attributes\":{\"n\":\"5.3\"},\"end\":19023,\"start\":19001},{\"attributes\":{\"n\":\"6\"},\"end\":19932,\"start\":19912},{\"attributes\":{\"n\":\"6.1\"},\"end\":20197,\"start\":20169},{\"attributes\":{\"n\":\"6.2\"},\"end\":22447,\"start\":22423},{\"end\":23011,\"start\":23003},{\"end\":23542,\"start\":23538},{\"attributes\":{\"n\":\"6.3\"},\"end\":25019,\"start\":25009},{\"attributes\":{\"n\":\"6.4\"},\"end\":26618,\"start\":26604},{\"end\":26916,\"start\":26908},{\"end\":27085,\"start\":27072},{\"attributes\":{\"n\":\"7\"},\"end\":27902,\"start\":27868},{\"attributes\":{\"n\":\"8\"},\"end\":29843,\"start\":29833},{\"end\":30952,\"start\":30942},{\"end\":31547,\"start\":31538},{\"end\":31735,\"start\":31726},{\"end\":32021,\"start\":32012},{\"end\":32126,\"start\":32117},{\"end\":32413,\"start\":32404}]", "table": "[{\"end\":31536,\"start\":31140},{\"end\":32010,\"start\":31870},{\"end\":32402,\"start\":32250}]", "figure_caption": "[{\"end\":31075,\"start\":30954},{\"end\":31140,\"start\":31078},{\"end\":31724,\"start\":31549},{\"end\":31870,\"start\":31737},{\"end\":32115,\"start\":32023},{\"end\":32191,\"start\":32128},{\"end\":32250,\"start\":32194},{\"end\":32544,\"start\":32415}]", "figure_ref": "[{\"end\":7241,\"start\":7233},{\"end\":7942,\"start\":7934},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17053,\"start\":17045}]", "bib_author_first_name": "[{\"end\":32591,\"start\":32585},{\"end\":32606,\"start\":32600},{\"end\":32623,\"start\":32617},{\"end\":33069,\"start\":33062},{\"end\":33083,\"start\":33078},{\"end\":33099,\"start\":33092},{\"end\":33111,\"start\":33107},{\"end\":33130,\"start\":33124},{\"end\":33146,\"start\":33139},{\"end\":33407,\"start\":33401},{\"end\":33426,\"start\":33418},{\"end\":33432,\"start\":33431},{\"end\":33438,\"start\":33433},{\"end\":33452,\"start\":33446},{\"end\":33684,\"start\":33681},{\"end\":33686,\"start\":33685},{\"end\":33703,\"start\":33699},{\"end\":33724,\"start\":33719},{\"end\":33736,\"start\":33732},{\"end\":33746,\"start\":33741},{\"end\":33768,\"start\":33761},{\"end\":33781,\"start\":33776},{\"end\":33783,\"start\":33782},{\"end\":33801,\"start\":33795},{\"end\":34099,\"start\":34092},{\"end\":34109,\"start\":34105},{\"end\":34117,\"start\":34114},{\"end\":34129,\"start\":34123},{\"end\":34141,\"start\":34137},{\"end\":34149,\"start\":34146},{\"end\":35193,\"start\":35185},{\"end\":35205,\"start\":35198},{\"end\":35662,\"start\":35659},{\"end\":35676,\"start\":35673},{\"end\":35690,\"start\":35683},{\"end\":35704,\"start\":35699},{\"end\":35933,\"start\":35930},{\"end\":35941,\"start\":35938},{\"end\":35955,\"start\":35949},{\"end\":35970,\"start\":35963},{\"end\":36222,\"start\":36221},{\"end\":36257,\"start\":36229},{\"end\":36492,\"start\":36487},{\"end\":36504,\"start\":36498},{\"end\":36517,\"start\":36509},{\"end\":36531,\"start\":36522},{\"end\":36545,\"start\":36537},{\"end\":36995,\"start\":36992},{\"end\":37151,\"start\":37145},{\"end\":37161,\"start\":37157},{\"end\":37172,\"start\":37167},{\"end\":37187,\"start\":37180},{\"end\":37198,\"start\":37192},{\"end\":37211,\"start\":37206},{\"end\":37222,\"start\":37218},{\"end\":37233,\"start\":37229},{\"end\":37245,\"start\":37241},{\"end\":37266,\"start\":37259},{\"end\":37610,\"start\":37604},{\"end\":37634,\"start\":37625},{\"end\":37653,\"start\":37645},{\"end\":37662,\"start\":37658},{\"end\":37676,\"start\":37671},{\"end\":37688,\"start\":37684},{\"end\":37710,\"start\":37702},{\"end\":37723,\"start\":37718},{\"end\":37725,\"start\":37724},{\"end\":38079,\"start\":38073},{\"end\":38096,\"start\":38091},{\"end\":38109,\"start\":38103},{\"end\":38132,\"start\":38128},{\"end\":38739,\"start\":38734},{\"end\":38749,\"start\":38745},{\"end\":38767,\"start\":38762},{\"end\":39199,\"start\":39194},{\"end\":39221,\"start\":39216},{\"end\":39240,\"start\":39233},{\"end\":39242,\"start\":39241},{\"end\":39256,\"start\":39250},{\"end\":39273,\"start\":39266},{\"end\":39287,\"start\":39281},{\"end\":39301,\"start\":39297},{\"end\":39303,\"start\":39302},{\"end\":39323,\"start\":39317},{\"end\":39343,\"start\":39336},{\"end\":39358,\"start\":39354},{\"end\":39843,\"start\":39836},{\"end\":39859,\"start\":39854},{\"end\":39872,\"start\":39868},{\"end\":39887,\"start\":39879},{\"end\":40432,\"start\":40428},{\"end\":40446,\"start\":40442},{\"end\":40618,\"start\":40614},{\"end\":40632,\"start\":40628},{\"end\":40642,\"start\":40637},{\"end\":40655,\"start\":40650},{\"end\":40667,\"start\":40662},{\"end\":40680,\"start\":40676},{\"end\":40949,\"start\":40940},{\"end\":40969,\"start\":40964},{\"end\":40991,\"start\":40983},{\"end\":41006,\"start\":41000},{\"end\":41498,\"start\":41491},{\"end\":41510,\"start\":41505},{\"end\":41527,\"start\":41520},{\"end\":41544,\"start\":41536},{\"end\":41999,\"start\":41991},{\"end\":42010,\"start\":42006},{\"end\":42641,\"start\":42637},{\"end\":42658,\"start\":42652},{\"end\":42673,\"start\":42667},{\"end\":42689,\"start\":42682},{\"end\":42691,\"start\":42690},{\"end\":42707,\"start\":42700},{\"end\":43256,\"start\":43252},{\"end\":43272,\"start\":43267},{\"end\":43289,\"start\":43281},{\"end\":43304,\"start\":43300},{\"end\":43318,\"start\":43314},{\"end\":43575,\"start\":43569},{\"end\":43599,\"start\":43589},{\"end\":43613,\"start\":43609},{\"end\":43627,\"start\":43620},{\"end\":43643,\"start\":43637},{\"end\":43657,\"start\":43652},{\"end\":43659,\"start\":43658},{\"end\":43677,\"start\":43671},{\"end\":43967,\"start\":43966},{\"end\":43983,\"start\":43978},{\"end\":44432,\"start\":44428},{\"end\":44449,\"start\":44444},{\"end\":44460,\"start\":44459},{\"end\":44948,\"start\":44947},{\"end\":44960,\"start\":44956},{\"end\":44962,\"start\":44961},{\"end\":45152,\"start\":45148},{\"end\":45166,\"start\":45162},{\"end\":45183,\"start\":45176},{\"end\":45197,\"start\":45192},{\"end\":45214,\"start\":45205},{\"end\":45225,\"start\":45220},{\"end\":45522,\"start\":45516},{\"end\":45524,\"start\":45523},{\"end\":45766,\"start\":45763},{\"end\":45785,\"start\":45776},{\"end\":45787,\"start\":45786},{\"end\":46444,\"start\":46436},{\"end\":46451,\"start\":46449},{\"end\":46466,\"start\":46457},{\"end\":46479,\"start\":46472},{\"end\":46493,\"start\":46485},{\"end\":46502,\"start\":46499},{\"end\":46811,\"start\":46805},{\"end\":46822,\"start\":46816},{\"end\":46833,\"start\":46830},{\"end\":46844,\"start\":46840},{\"end\":47082,\"start\":47077},{\"end\":47095,\"start\":47092},{\"end\":47112,\"start\":47106},{\"end\":47129,\"start\":47122},{\"end\":47139,\"start\":47136},{\"end\":47158,\"start\":47149},{\"end\":47173,\"start\":47168},{\"end\":47423,\"start\":47418},{\"end\":47425,\"start\":47424},{\"end\":47441,\"start\":47435},{\"end\":47443,\"start\":47442},{\"end\":47451,\"start\":47450},{\"end\":47458,\"start\":47452},{\"end\":47473,\"start\":47468},{\"end\":47475,\"start\":47474}]", "bib_author_last_name": "[{\"end\":32598,\"start\":32592},{\"end\":32615,\"start\":32607},{\"end\":32631,\"start\":32624},{\"end\":33076,\"start\":33070},{\"end\":33090,\"start\":33084},{\"end\":33105,\"start\":33100},{\"end\":33122,\"start\":33112},{\"end\":33137,\"start\":33131},{\"end\":33154,\"start\":33147},{\"end\":33416,\"start\":33408},{\"end\":33429,\"start\":33427},{\"end\":33444,\"start\":33439},{\"end\":33458,\"start\":33453},{\"end\":33462,\"start\":33460},{\"end\":33697,\"start\":33687},{\"end\":33717,\"start\":33704},{\"end\":33730,\"start\":33725},{\"end\":33739,\"start\":33737},{\"end\":33759,\"start\":33747},{\"end\":33774,\"start\":33769},{\"end\":33793,\"start\":33784},{\"end\":33808,\"start\":33802},{\"end\":34103,\"start\":34100},{\"end\":34112,\"start\":34110},{\"end\":34121,\"start\":34118},{\"end\":34135,\"start\":34130},{\"end\":34144,\"start\":34142},{\"end\":34154,\"start\":34150},{\"end\":35196,\"start\":35194},{\"end\":35211,\"start\":35206},{\"end\":35671,\"start\":35663},{\"end\":35681,\"start\":35677},{\"end\":35697,\"start\":35691},{\"end\":35709,\"start\":35705},{\"end\":35936,\"start\":35934},{\"end\":35947,\"start\":35942},{\"end\":35961,\"start\":35956},{\"end\":35974,\"start\":35971},{\"end\":36227,\"start\":36223},{\"end\":36264,\"start\":36258},{\"end\":36496,\"start\":36493},{\"end\":36507,\"start\":36505},{\"end\":36520,\"start\":36518},{\"end\":36535,\"start\":36532},{\"end\":36551,\"start\":36546},{\"end\":36999,\"start\":36996},{\"end\":37049,\"start\":37045},{\"end\":37155,\"start\":37152},{\"end\":37165,\"start\":37162},{\"end\":37178,\"start\":37173},{\"end\":37190,\"start\":37188},{\"end\":37204,\"start\":37199},{\"end\":37216,\"start\":37212},{\"end\":37227,\"start\":37223},{\"end\":37239,\"start\":37234},{\"end\":37257,\"start\":37246},{\"end\":37275,\"start\":37267},{\"end\":37623,\"start\":37611},{\"end\":37643,\"start\":37635},{\"end\":37656,\"start\":37654},{\"end\":37669,\"start\":37663},{\"end\":37682,\"start\":37677},{\"end\":37700,\"start\":37689},{\"end\":37716,\"start\":37711},{\"end\":37731,\"start\":37726},{\"end\":38089,\"start\":38080},{\"end\":38101,\"start\":38097},{\"end\":38118,\"start\":38110},{\"end\":38126,\"start\":38120},{\"end\":38147,\"start\":38133},{\"end\":38154,\"start\":38149},{\"end\":38743,\"start\":38740},{\"end\":38760,\"start\":38750},{\"end\":38773,\"start\":38768},{\"end\":39192,\"start\":39180},{\"end\":39214,\"start\":39200},{\"end\":39231,\"start\":39222},{\"end\":39248,\"start\":39243},{\"end\":39264,\"start\":39257},{\"end\":39279,\"start\":39274},{\"end\":39295,\"start\":39288},{\"end\":39315,\"start\":39304},{\"end\":39334,\"start\":39324},{\"end\":39352,\"start\":39344},{\"end\":39370,\"start\":39359},{\"end\":39376,\"start\":39372},{\"end\":39852,\"start\":39844},{\"end\":39866,\"start\":39860},{\"end\":39877,\"start\":39873},{\"end\":39891,\"start\":39888},{\"end\":40440,\"start\":40433},{\"end\":40456,\"start\":40447},{\"end\":40626,\"start\":40619},{\"end\":40635,\"start\":40633},{\"end\":40648,\"start\":40643},{\"end\":40660,\"start\":40656},{\"end\":40674,\"start\":40668},{\"end\":40690,\"start\":40681},{\"end\":40962,\"start\":40950},{\"end\":40981,\"start\":40970},{\"end\":40998,\"start\":40992},{\"end\":41013,\"start\":41007},{\"end\":41503,\"start\":41499},{\"end\":41518,\"start\":41511},{\"end\":41534,\"start\":41528},{\"end\":41549,\"start\":41545},{\"end\":41558,\"start\":41551},{\"end\":42004,\"start\":42000},{\"end\":42018,\"start\":42011},{\"end\":42650,\"start\":42642},{\"end\":42665,\"start\":42659},{\"end\":42680,\"start\":42674},{\"end\":42698,\"start\":42692},{\"end\":42714,\"start\":42708},{\"end\":43265,\"start\":43257},{\"end\":43279,\"start\":43273},{\"end\":43298,\"start\":43290},{\"end\":43312,\"start\":43305},{\"end\":43325,\"start\":43319},{\"end\":43587,\"start\":43576},{\"end\":43607,\"start\":43600},{\"end\":43618,\"start\":43614},{\"end\":43635,\"start\":43628},{\"end\":43650,\"start\":43644},{\"end\":43669,\"start\":43660},{\"end\":43684,\"start\":43678},{\"end\":43976,\"start\":43968},{\"end\":43989,\"start\":43984},{\"end\":44001,\"start\":43991},{\"end\":44442,\"start\":44433},{\"end\":44457,\"start\":44450},{\"end\":44465,\"start\":44461},{\"end\":44469,\"start\":44467},{\"end\":44954,\"start\":44949},{\"end\":44968,\"start\":44963},{\"end\":44972,\"start\":44970},{\"end\":45160,\"start\":45153},{\"end\":45174,\"start\":45167},{\"end\":45190,\"start\":45184},{\"end\":45203,\"start\":45198},{\"end\":45218,\"start\":45215},{\"end\":45232,\"start\":45226},{\"end\":45533,\"start\":45525},{\"end\":45774,\"start\":45767},{\"end\":45792,\"start\":45788},{\"end\":46447,\"start\":46445},{\"end\":46455,\"start\":46452},{\"end\":46470,\"start\":46467},{\"end\":46483,\"start\":46480},{\"end\":46497,\"start\":46494},{\"end\":46505,\"start\":46503},{\"end\":46814,\"start\":46812},{\"end\":46828,\"start\":46823},{\"end\":46838,\"start\":46834},{\"end\":46847,\"start\":46845},{\"end\":47090,\"start\":47083},{\"end\":47104,\"start\":47096},{\"end\":47120,\"start\":47113},{\"end\":47134,\"start\":47130},{\"end\":47147,\"start\":47140},{\"end\":47166,\"start\":47159},{\"end\":47178,\"start\":47174},{\"end\":47433,\"start\":47426},{\"end\":47448,\"start\":47444},{\"end\":47466,\"start\":47459},{\"end\":47479,\"start\":47476}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":221275765},\"end\":33031,\"start\":32546},{\"attributes\":{\"doi\":\"abs/1811.02549\",\"id\":\"b1\"},\"end\":33328,\"start\":33033},{\"attributes\":{\"doi\":\"abs/1702.07983\",\"id\":\"b2\"},\"end\":33677,\"start\":33330},{\"attributes\":{\"doi\":\"abs/1406.2661\",\"id\":\"b3\"},\"end\":34019,\"start\":33679},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3389583},\"end\":35140,\"start\":34021},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16153365},\"end\":35611,\"start\":35142},{\"attributes\":{\"doi\":\"abs/1904.09751\",\"id\":\"b6\"},\"end\":35860,\"start\":35613},{\"attributes\":{\"doi\":\"abs/1908.07195\",\"id\":\"b7\"},\"end\":36141,\"start\":35862},{\"attributes\":{\"doi\":\"abs/1611.04051\",\"id\":\"b8\"},\"end\":36440,\"start\":36143},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4857922},\"end\":36988,\"start\":36442},{\"attributes\":{\"id\":\"b10\"},\"end\":37041,\"start\":36990},{\"attributes\":{\"doi\":\"abs/1704.06191\",\"id\":\"b11\"},\"end\":37086,\"start\":37043},{\"attributes\":{\"doi\":\"abs/1907.11692\",\"id\":\"b12\"},\"end\":37519,\"start\":37088},{\"attributes\":{\"doi\":\"abs/1604.01696\",\"id\":\"b13\"},\"end\":37997,\"start\":37521},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8928715},\"end\":38660,\"start\":37999},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":68160504},\"end\":39103,\"start\":38662},{\"attributes\":{\"doi\":\"abs/1909.01387\",\"id\":\"b16\"},\"end\":39770,\"start\":39105},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11080756},\"end\":40365,\"start\":39772},{\"attributes\":{\"id\":\"b18\"},\"end\":40559,\"start\":40367},{\"attributes\":{\"id\":\"b19\"},\"end\":40832,\"start\":40561},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5043670},\"end\":41433,\"start\":40834},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7147309},\"end\":41944,\"start\":41435},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8498625},\"end\":42601,\"start\":41946},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16046818},\"end\":43248,\"start\":42603},{\"attributes\":{\"doi\":\"abs/1707.06347\",\"id\":\"b24\"},\"end\":43488,\"start\":43250},{\"attributes\":{\"id\":\"b25\"},\"end\":43964,\"start\":43490},{\"attributes\":{\"id\":\"b26\"},\"end\":44374,\"start\":43966},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7961699},\"end\":44902,\"start\":44376},{\"attributes\":{\"doi\":\"abs/1806.02847\",\"id\":\"b28\"},\"end\":45095,\"start\":44904},{\"attributes\":{\"doi\":\"abs/1908.04319\",\"id\":\"b29\"},\"end\":45424,\"start\":45097},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2332513},\"end\":45702,\"start\":45426},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2783746},\"end\":46326,\"start\":45704},{\"attributes\":{\"doi\":\"abs/1802.01345\",\"id\":\"b32\"},\"end\":46736,\"start\":46328},{\"attributes\":{\"id\":\"b33\"},\"end\":47039,\"start\":46738},{\"attributes\":{\"doi\":\"abs/1905.12616\",\"id\":\"b34\"},\"end\":47368,\"start\":47041},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":336219},\"end\":47949,\"start\":47370}]", "bib_title": "[{\"end\":32583,\"start\":32546},{\"end\":34090,\"start\":34021},{\"end\":35183,\"start\":35142},{\"end\":36485,\"start\":36442},{\"end\":38071,\"start\":37999},{\"end\":38732,\"start\":38662},{\"end\":39834,\"start\":39772},{\"end\":40938,\"start\":40834},{\"end\":41489,\"start\":41435},{\"end\":41989,\"start\":41946},{\"end\":42635,\"start\":42603},{\"end\":44426,\"start\":44376},{\"end\":45514,\"start\":45426},{\"end\":45761,\"start\":45704},{\"end\":47416,\"start\":47370}]", "bib_author": "[{\"end\":32600,\"start\":32585},{\"end\":32617,\"start\":32600},{\"end\":32633,\"start\":32617},{\"end\":33078,\"start\":33062},{\"end\":33092,\"start\":33078},{\"end\":33107,\"start\":33092},{\"end\":33124,\"start\":33107},{\"end\":33139,\"start\":33124},{\"end\":33156,\"start\":33139},{\"end\":33418,\"start\":33401},{\"end\":33431,\"start\":33418},{\"end\":33446,\"start\":33431},{\"end\":33460,\"start\":33446},{\"end\":33464,\"start\":33460},{\"end\":33699,\"start\":33681},{\"end\":33719,\"start\":33699},{\"end\":33732,\"start\":33719},{\"end\":33741,\"start\":33732},{\"end\":33761,\"start\":33741},{\"end\":33776,\"start\":33761},{\"end\":33795,\"start\":33776},{\"end\":33810,\"start\":33795},{\"end\":34105,\"start\":34092},{\"end\":34114,\"start\":34105},{\"end\":34123,\"start\":34114},{\"end\":34137,\"start\":34123},{\"end\":34146,\"start\":34137},{\"end\":34156,\"start\":34146},{\"end\":35198,\"start\":35185},{\"end\":35213,\"start\":35198},{\"end\":35673,\"start\":35659},{\"end\":35683,\"start\":35673},{\"end\":35699,\"start\":35683},{\"end\":35711,\"start\":35699},{\"end\":35938,\"start\":35930},{\"end\":35949,\"start\":35938},{\"end\":35963,\"start\":35949},{\"end\":35976,\"start\":35963},{\"end\":36229,\"start\":36221},{\"end\":36266,\"start\":36229},{\"end\":36498,\"start\":36487},{\"end\":36509,\"start\":36498},{\"end\":36522,\"start\":36509},{\"end\":36537,\"start\":36522},{\"end\":36553,\"start\":36537},{\"end\":37001,\"start\":36992},{\"end\":37051,\"start\":37045},{\"end\":37157,\"start\":37145},{\"end\":37167,\"start\":37157},{\"end\":37180,\"start\":37167},{\"end\":37192,\"start\":37180},{\"end\":37206,\"start\":37192},{\"end\":37218,\"start\":37206},{\"end\":37229,\"start\":37218},{\"end\":37241,\"start\":37229},{\"end\":37259,\"start\":37241},{\"end\":37277,\"start\":37259},{\"end\":37625,\"start\":37604},{\"end\":37645,\"start\":37625},{\"end\":37658,\"start\":37645},{\"end\":37671,\"start\":37658},{\"end\":37684,\"start\":37671},{\"end\":37702,\"start\":37684},{\"end\":37718,\"start\":37702},{\"end\":37733,\"start\":37718},{\"end\":38091,\"start\":38073},{\"end\":38103,\"start\":38091},{\"end\":38120,\"start\":38103},{\"end\":38128,\"start\":38120},{\"end\":38149,\"start\":38128},{\"end\":38156,\"start\":38149},{\"end\":38745,\"start\":38734},{\"end\":38762,\"start\":38745},{\"end\":38775,\"start\":38762},{\"end\":39194,\"start\":39180},{\"end\":39216,\"start\":39194},{\"end\":39233,\"start\":39216},{\"end\":39250,\"start\":39233},{\"end\":39266,\"start\":39250},{\"end\":39281,\"start\":39266},{\"end\":39297,\"start\":39281},{\"end\":39317,\"start\":39297},{\"end\":39336,\"start\":39317},{\"end\":39354,\"start\":39336},{\"end\":39372,\"start\":39354},{\"end\":39378,\"start\":39372},{\"end\":39854,\"start\":39836},{\"end\":39868,\"start\":39854},{\"end\":39879,\"start\":39868},{\"end\":39893,\"start\":39879},{\"end\":40442,\"start\":40428},{\"end\":40458,\"start\":40442},{\"end\":40628,\"start\":40614},{\"end\":40637,\"start\":40628},{\"end\":40650,\"start\":40637},{\"end\":40662,\"start\":40650},{\"end\":40676,\"start\":40662},{\"end\":40692,\"start\":40676},{\"end\":40964,\"start\":40940},{\"end\":40983,\"start\":40964},{\"end\":41000,\"start\":40983},{\"end\":41015,\"start\":41000},{\"end\":41505,\"start\":41491},{\"end\":41520,\"start\":41505},{\"end\":41536,\"start\":41520},{\"end\":41551,\"start\":41536},{\"end\":41560,\"start\":41551},{\"end\":42006,\"start\":41991},{\"end\":42020,\"start\":42006},{\"end\":42652,\"start\":42637},{\"end\":42667,\"start\":42652},{\"end\":42682,\"start\":42667},{\"end\":42700,\"start\":42682},{\"end\":42716,\"start\":42700},{\"end\":43267,\"start\":43252},{\"end\":43281,\"start\":43267},{\"end\":43300,\"start\":43281},{\"end\":43314,\"start\":43300},{\"end\":43327,\"start\":43314},{\"end\":43589,\"start\":43569},{\"end\":43609,\"start\":43589},{\"end\":43620,\"start\":43609},{\"end\":43637,\"start\":43620},{\"end\":43652,\"start\":43637},{\"end\":43671,\"start\":43652},{\"end\":43686,\"start\":43671},{\"end\":43978,\"start\":43966},{\"end\":43991,\"start\":43978},{\"end\":44003,\"start\":43991},{\"end\":44444,\"start\":44428},{\"end\":44459,\"start\":44444},{\"end\":44467,\"start\":44459},{\"end\":44471,\"start\":44467},{\"end\":44956,\"start\":44947},{\"end\":44970,\"start\":44956},{\"end\":44974,\"start\":44970},{\"end\":45162,\"start\":45148},{\"end\":45176,\"start\":45162},{\"end\":45192,\"start\":45176},{\"end\":45205,\"start\":45192},{\"end\":45220,\"start\":45205},{\"end\":45234,\"start\":45220},{\"end\":45535,\"start\":45516},{\"end\":45776,\"start\":45763},{\"end\":45794,\"start\":45776},{\"end\":46449,\"start\":46436},{\"end\":46457,\"start\":46449},{\"end\":46472,\"start\":46457},{\"end\":46485,\"start\":46472},{\"end\":46499,\"start\":46485},{\"end\":46507,\"start\":46499},{\"end\":46816,\"start\":46805},{\"end\":46830,\"start\":46816},{\"end\":46840,\"start\":46830},{\"end\":46849,\"start\":46840},{\"end\":47092,\"start\":47077},{\"end\":47106,\"start\":47092},{\"end\":47122,\"start\":47106},{\"end\":47136,\"start\":47122},{\"end\":47149,\"start\":47136},{\"end\":47168,\"start\":47149},{\"end\":47180,\"start\":47168},{\"end\":47435,\"start\":47418},{\"end\":47450,\"start\":47435},{\"end\":47468,\"start\":47450},{\"end\":47481,\"start\":47468}]", "bib_venue": "[{\"end\":32743,\"start\":32633},{\"end\":33060,\"start\":33033},{\"end\":33399,\"start\":33330},{\"end\":34402,\"start\":34156},{\"end\":35330,\"start\":35213},{\"end\":35657,\"start\":35613},{\"end\":35928,\"start\":35862},{\"end\":36219,\"start\":36143},{\"end\":36665,\"start\":36553},{\"end\":37012,\"start\":37001},{\"end\":37143,\"start\":37088},{\"end\":37602,\"start\":37521},{\"end\":38240,\"start\":38156},{\"end\":38831,\"start\":38775},{\"end\":39178,\"start\":39105},{\"end\":39980,\"start\":39893},{\"end\":40426,\"start\":40367},{\"end\":40612,\"start\":40561},{\"end\":41076,\"start\":41015},{\"end\":41616,\"start\":41560},{\"end\":42150,\"start\":42020},{\"end\":42795,\"start\":42716},{\"end\":43567,\"start\":43490},{\"end\":44077,\"start\":44003},{\"end\":44583,\"start\":44471},{\"end\":44945,\"start\":44904},{\"end\":45146,\"start\":45097},{\"end\":45551,\"start\":45535},{\"end\":45880,\"start\":45794},{\"end\":46434,\"start\":46328},{\"end\":46803,\"start\":46738},{\"end\":47075,\"start\":47041},{\"end\":47561,\"start\":47481},{\"end\":32760,\"start\":32745},{\"end\":34649,\"start\":34404},{\"end\":35348,\"start\":35332},{\"end\":36686,\"start\":36667},{\"end\":38326,\"start\":38242},{\"end\":38840,\"start\":38833},{\"end\":40075,\"start\":39982},{\"end\":41097,\"start\":41078},{\"end\":41639,\"start\":41618},{\"end\":42282,\"start\":42152},{\"end\":42874,\"start\":42797},{\"end\":44168,\"start\":44079},{\"end\":44609,\"start\":44585},{\"end\":45971,\"start\":45882},{\"end\":47650,\"start\":47563}]"}}}, "year": 2023, "month": 12, "day": 17}