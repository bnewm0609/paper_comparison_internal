{"id": 558372, "updated": "2023-09-28 12:09:26.711", "metadata": {"title": "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares", "authors": "[{\"first\":\"Mert\",\"last\":\"Pilanci\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Wainwright\",\"middle\":[\"J.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2014, "month": null, "day": null}, "abstract": "We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including $\\ell_1$-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1411.0347", "mag": "2963002486", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/PilanciW14a", "doi": null}}, "content": {"source": {"pdf_hash": "1a7a17c4f97c68d68fbeefee1751d349b83eb14a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1411.0347v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a0abdf003ef3df384c31c1d69700bc39d23c78e5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1a7a17c4f97c68d68fbeefee1751d349b83eb14a.txt", "contents": "\nIterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares\nNovember 4, 2014\n\nMert Pilanci \nDepartment of Electrical Engineering and Computer Science\n\n\nMartin J Wainwright wainwrig@berkeley.edu \nDepartment of Electrical Engineering and Computer Science\n\n\nDepartment of Statistics\n\n\n\nUniversity of California\nBerkeley\n\nIterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares\nNovember 4, 2014\nWe study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including 1 -regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.\n\nIntroduction\n\nOver the past decade, the explosion of data volume and complexity has led to a surge of interest in fast procedures for approximate forms of matrix multiplication, low-rank approximation, and convex optimization. One interesting class of problems that arise frequently in data analysis and scientific computing are constrained least-squares problems. More specifically, given a data vector y \u2208 R n , a data matrix A \u2208 R n\u00d7d and a convex constraint set C, a constrained least-squares problem can be written as follows \n\nThe simplest case is the unconstrained form (C = R d ), but this class also includes other interesting constrained programs, including those based 1 -norm balls, nuclear norm balls, interval constraints [\u22121, 1] d and other types of regularizers designed to enforce structure in the solution.\n\nRandomized sketches are a well-established way of obtaining an approximate solutions to a variety of problems, and there is a long line of work on their uses (e.g., see the books and papers [38,8,25,16,21], as well as references therein). In application to problem (1), sketching methods involving using a random matrix S \u2208 R m\u00d7n to project the data matrix A and/or data vector y to a lower dimensional space (m n), and then solving the approximated least-squares problem. There are many choices of random sketching matrices; see Section 2.1 for discussion of a few possibilities. Given some choice of random sketching matrix S, the most well-studied form of sketched least-squares is based on solving the problem\nx : = arg min x\u2208C 1 2n SAx \u2212 Sy 2 2 ,(2)\nin which the data matrix-vector pair (A, y) are approximated by their sketched versions (SA, Sy). Note that the sketched program is an m-dimensional least-squares problem, involving the new data matrix SA \u2208 R m\u00d7d . Thus, in the regime n d, this approach can lead to substantial computational savings as long as the projection dimension m can be chosen substantially less than n. A number of authors (e.g., [8,16,25,31]) have investigated the properties of this sketched solution (2), and accordingly, we refer to to it as the classical least-squares sketch.\n\nThere are various ways in which the quality of the approximate solution x can be assessed. One standard way is in terms of the minimizing value of the quadratic cost function f defining the original problem (1), which we refer to as cost approximation. In terms of f -cost, the approximate solution x is said to be \u03b5-optimal if\nf (x LS ) \u2264 f ( x) \u2264 (1 + \u03b5) 2 f (x LS ).(3)\nFor example, in the case of unconstrained least-squares (C = R d ) with n > d, it is known that with Gaussian random sketches, a sketch size m 1 \u03b5 2 d suffices to guarantee that x is \u03b5-optimal with high probability (for instance, see the papers by Sarlos [35] and Mahoney [25], as well as references therein). Similar guarantees can be established for sketches based on sampling according to the statistical leverage scores [15,14]. Sketching can also be applied to problems with constraints: Boutsidis and Drineas [8] prove analogous results for the case of non-negative least-squares considering the sketch in (2), whereas our own past work [31] provides sufficient conditions for \u03b5-accurate cost approximation of least-squares problems over arbitrary convex sets based also on the form in (2).\n\nIt should be noted, however, that other notions of \"approximation goodness\" are possible. In many applications, it is the least-squares minimizer x LS itself-as opposed to the cost value f (x LS )-that is of primary interest. In such settings, a more suitable measure of approximation quality would be the 2 -norm x \u2212 x LS 2 , or the prediction (semi)-norm\n\nx \u2212 x LS A : =\n1 \u221a n A( x \u2212 x LS ) 2 .(4)\nWe refer to these measures as solution approximation. Now of course, a cost approximation bound (3) can be used to derive guarantees on the solution approximation error. However, it is natural to wonder whether or not, for a reasonable sketch size, the resulting guarantees are \"good\". For instance, using arguments from Drineas et al. [16], for the problem of unconstrained least-squares, it can be shown that the same conditions ensuring a \u03b5-accurate cost approximation also ensure that\nx \u2212 x LS A \u2264 \u03b5 f (x LS ).(5)\nGiven lower bounds on the singular values of the data matrix A, this bound also yields control of the 2 -error. In certain ways, the bound (5) is quite satisfactory: given our normalized definition (1) of the least-squares cost f , the quantity f (x LS ) remains an order one quantity as the sample size n grows, and the multiplicative factor \u03b5 can be reduced by increasing the sketch dimension m. But how small should \u03b5 be chosen? In many applications of least-squares, each element of the response vector y \u2208 R n corresponds to an observation, and so as the sample size n increases, we expect that x LS provides a more accurate approximation to some underlying population quantity, say x * \u2208 R d . As an illustrative example, in the special case of unconstrained least-squares, the accuracy of the least-squares solution x LS as an estimate of x * scales as\nx LS \u2212 x * A \u03c3 2 d n .\nConsequently, in order for our sketched solution to have an accuracy of the same order as the least-square estimate, we must set \u03b5 2 \u03c3 2 d n . Combined with our earlier bound on the projection dimension, this calculation suggests that a projection dimension of the order m d \u03b5 2 n \u03c3 2 is required. This scaling is undesirable in the regime n d, where the whole point of sketching is to have the sketch dimension m much lower than n. Now the alert reader will have observed that the preceding argument was only rough and heuristic. However, the first result of this paper (Theorem 1) provides a rigorous confirmation of the conclusion: whenever m n, the classical least-squares sketch (2) is sub-optimal as a method for solution approximation. Figure 1 provides an empirical demonstration of the poor behavior of the classical least-squares sketch for an unconstrained problem.\n\nThis sub-optimality holds not only for unconstrained least-squares but also more generally for a broad class of constrained problems. Actually, Theorem 1 is a more general claim: any estimator based only on the pair (SA, Sy)-an infinite family of methods including the standard sketching algorithm as a particular case-is sub-optimal relative to the original least-squares estimator in the regime m n. We are thus led to a natural question: can this sub-optimality be avoided by a different type of sketch that is nonetheless computationally efficient? Motivated by this question, our second main result (Theorem 2) is to propose an alternative method-known as the iterative Hessian sketch-and prove that it yields optimal approximations to the least-squares solution using a projection size that scales with the intrinsic dimension of the underlying problem, along with a logarithmic number of iterations. The main idea underlying iterative Hessian sketch is to obtain multiple sketches of the data (S 1 A, ..., S N A) and iteratively refine the solution where N can be chosen logarithmic in n.\n\nThe remainder of this paper is organized as follows. In Section 2, we begin by introducing some background on classes of random sketching matrices, before turning to the statement of our lower bound (Theorem 1) on the classical least-squares sketch (2). We then introduce the  \n\u2212 x * 2 2 . (b) Prediction error x \u2212 x * 2 A = 1 n A( x \u2212 x * ) 2 2 .\nEach point corresponds to the mean taken over 300 trials with standard errors shown above and below in crosses.\n\nHessian sketch, and show that an iterative version of it can be used to compute \u03b5-accurate solution approximations using log(1/\u03b5)-steps (Theorem 2). In Section 3, we illustrate the consequences of this general theorem for various specific classes of least-squares problems, and we conclude with a discussion in Section 4. The majority of our proofs are deferred to the appendices.\n\nNotation: For the convenience of the reader, we summarize some standard notation used in this paper. For sequences {a t } \u221e t=0 and {b t } \u221e t=0 , we use the notation a t b t to mean that there is a constant (independent of t) such that a t \u2264 C b t for all t. Equivalently, we write b t a t . We write a t b t if a t b t and b t a t .\n\n\nMain results\n\nIn this section, we begin with background on different classes of randomized sketches, including those based on random matrices with sub-Gaussian entries, as well as those based on randomized orthonormal systems and random sampling. In Section 2.2, we prove a general lower bound on the solution approximation accuracy of any method that attempts to approximate the least-squares problem based on observing only the pair (SA, Sy). This negative result motivates the investigation of alternative sketching methods, and we begin this investigation by introducing the Hessian sketch in Section 2.3. It serves as the basic building block of the iterative Hessian sketch (IHS), which can be used to construct an iterative method that is optimal up to logarithmic factors.\n\n\nDifferent types of randomized sketches\n\nVarious types of randomized sketches are possible, and we describe a few of them here. Given a sketching matrix S, we use {s i } m i=1 to denote the collection of its n-dimensional rows. We restrict our attention to sketch matrices that are zero-mean, and that are normalized so that\nE[S T S/m] = I n .\nSub-Gaussian sketches: The most classical sketch is based on a random matrix S \u2208 R m\u00d7n with i.i.d. standard Gaussian entries. A straightforward generalization is a random sketch with i.i.d. sub-Gaussian rows. In particular, a zero-mean random vector s \u2208 R n is 1-sub-Gaussian if for any u \u2208 R n , we have\nP[ s, u \u2265 \u03b5 u 2 \u2264 e \u2212\u03b5 2 /2 for all \u03b5 \u2265 0.(6)\nFor instance, a vector with i.i.d. N (0, 1) entries is 1-sub-Gaussian, as is a vector with i.i.d. Rademacher entries (uniformly distributed over {\u22121, +1}). Suppose that we generate a random matrix S \u2208 R m\u00d7n with i.i.d. rows that are zero-mean, 1-sub-Gaussian, and with cov(s) = I n ; we refer to any such matrix as a sub-Gaussian sketch. As will be clear, such sketches are the most straightforward to control from the probabilistic point of view. However, from a computational perspective, a disadvantage of sub-Gaussian sketches is that they require matrix-vector multiplications with unstructured random matrices. In particular, given an data matrix A \u2208 R n\u00d7d , computing its sketched version SA requires O(mnd) basic operations in general (using classical matrix multiplication).\n\nSketches based on randomized orthonormal systems (ROS): The second type of randomized sketch we consider is randomized orthonormal system (ROS), for which matrix multiplication can be performed much more efficiently.\n\nIn order to define a ROS sketch, we first let H \u2208 R n\u00d7n be an orthonormal matrix with entries H ij \u2208 [\u2212 1 \u221a n , 1 \u221a n ]. Standard classes of such matrices are the Hadamard or Fourier bases, for which matrix-vector multiplication can be performed in O(n log n) time via the fast Hadamard or Fourier transforms, respectively. Based on any such matrix, a sketching matrix S \u2208 R m\u00d7n from a ROS ensemble is obtained by sampling i.i.d. rows of the form\ns T = \u221a ne T j HD with probability 1/n for j = 1, . . . , n,\nwhere the random vector e j \u2208 R n is chosen uniformly at random from the set of all n canonical basis vectors, and D = diag(\u03bd) is a diagonal matrix of i.i.d. Rademacher variables \u03bd \u2208 {\u22121, +1} n . Given a fast routine for matrix-vector multiplication, the sketched data (SA, Sy) can be formed in O(n d log m) time (for instance, see the paper [1]).\n\nSketches based on random row sampling: Given a probability distribution {p j } n j=1 over [n] = {1, . . . , n}, another choice of sketch is to randomly sample the rows of the extended data matrix A y a total of m times with replacement from the given probability distribution. Thus, the rows of S are independent and take on the values s T = e j \u221a p j with probability p j for j = 1, . . . , n where e j \u2208 R n is the j th canonical basis vector. Different choices of the weights {p j } n j=1 are possible, including those based on the leverage values of A-i.e., p j \u221d u j 2 for j = 1, . . . , n, where U \u2208 R n\u00d7d is the matrix of left singular vectors of A [15]. In our analysis of lower bounds to follow, we assume that the weights are \u03b1-balanced, meaning that max j=1,...,n p j \u2264 \u03b1 n\n\nfor some constant \u03b1 independent of n.\n\nIn the following section, we present a lower bound that applies to all the three kinds of sketching matrices described above.\n\n\nSub-optimality of classical least-squares sketch\n\nWe begin by proving a lower bound on any estimator that is a function of the pair (SA, Sy). In order to do so, we consider an ensemble of least-squares problems, namely those generated by a noisy observation model of the form\ny = Ax * + w, where w \u223c N (0, \u03c3 2 I n ),(8)\nthe data matrix A \u2208 R n\u00d7d is fixed, and the unknown vector x * belongs to some compact subset C 0 \u2286 C. In this case, the constrained least-squares estimate x LS from equation (1) corresponds to a constrained form of maximum-likelihood for estimating the unknown regression vector x * . In Appendix D, we provide a general upper bound on the error E[ x LS \u2212 x * 2 A ] in the least-squares solution as an estimate of x * . This result provides a baseline against which to measure the performance of a sketching method: in particular, our goal is to characterize the minimal projection dimension m required in order to return an estimate x with an error guarantee\nx \u2212 x LS A \u2248 x LS \u2212 x * A .\nThe result to follow shows that unless m \u2265 n, then any method based on observing only the pair (SA, Sy) necessarily has a substantially larger error than the least-squares estimate. In particular, our result applies to an arbitrary measureable function (SA, Sy) \u2192 x \u2020 , which we refer to as an estimator.\n\nMore precisely, our lower bound applies to any random matrix S \u2208 R m\u00d7n for which\n|||E S T (SS T ) \u22121 S ||| op \u2264 \u03b7 m n ,(9)\nwhere \u03b7 is a constant independent of n and m, and |||A||| op denotes the 2 -operator norm (maximum eigenvalue for a symmetric matrix). In Appendix A.1, we show that these conditions hold for various standard choices, including most of those discussed in the previous section. Our lower bound also involves the complexity of the set C 0 , which we measure in terms of its metric entropy. In particular, for a given semi-norm \u00b7 and tolerance \u03b4 > 0, the \u03b4-packing number M \u03b4 of the set C 0 is the largest number of vectors {x j } M j=1 \u2282 C 0 such that x j \u2212x k > \u03b4 for all distinct pairs j = k.\n\nWith this set-up, we have the following result:\n\nTheorem 1 (Sub-optimality). For any random sketching matrix S \u2208 R m\u00d7n satisfying condition (9), any estimator (SA, Sy) \u2192 x \u2020 has MSE lower bounded as\nsup x * \u2208C 0 E S,w x \u2020 \u2212 x * 2 A \u2265 \u03c3 2 128 \u03b7 log( 1 2 M 1/2 ) min{m, n}(10)\nwhere M 1/2 is the 1/2-packing number of C 0 in the semi-norm \u00b7 A .\n\nThe proof, given in Appendix A, is based on a reduction from statistical minimax theory combined with information-theoretic bounds. The lower bound is best understood by considering some concrete examples:\n\nExample 1 (Sub-optimality for ordinary least-squares). We begin with the simplest casenamely, in which C = R d . With this choice and for any data matrix A with rank(A) = d, it is straightforward to show that the least-squares solution x LS has its prediction mean-squared error at most\nE x LS \u2212 x * 2 A \u03c3 2 d n . (11a)\nOn the other hand, with the choice C 0 = B 2 (1), we can construct a 1/2-packing with M = 2 d elements, so that Theorem 1 implies that any estimator x \u2020 based on (SA, Sy) has its prediction MSE lower bounded as\nE S,w x \u2212 x * 2 A \u03c3 2 d min{m, n} . (11b)\nConsequently, the sketch dimension m must grow proportionally to n in order for the sketched solution to have a mean-squared error comparable to the original least-squares estimate. This is highly undesirable for least-squares problems in which n d, since it should be possible to sketch down to a dimension proportional to rank(A) = d. Thus, Theorem 1 this reveals a surprising gap between the classical least-squares sketch (2) and the accuracy of the original least-squares estimate.\n\nIn contrast, the sketching method of this paper, known as iterative Hessian sketching (IHS), matches the optimal mean-squared error using a sketch of size d + log(n) in each round, and a total of log(n) rounds; see Corollary 2 for a precise statement. The red curves in Figure 1 show that the mean-squared errors ( x \u2212 x * 2 2 in panel (a), and x \u2212 x * 2 A in panel (b)) of the IHS method using this sketch dimension closely track the associated errors of the full least-squares solution (blue curves). Consistent with our previous discussion, both curves drop off at the n \u22121 rate.\n\nSince the IHS method with log(n) rounds uses a total of T = log(n) d+log(n)} sketches, a fair comparison is to implement the classical method with T sketches in total. The black curves show the MSE of the resulting sketch: as predicted by our theory, these curves are relatively flat as a function of sample size n. Indeed, in this particular case, the lower bound (10)\nE S,w x \u2212 x * 2 A \u03c3 2 d m \u03c3 2 log 2 (n) ,\nshowing we can expect (at best) an inverse logarithmic drop-off. \u2666\n\nThis sub-optimality can be extended to other forms of constrained least-squares estimates as well, such as those involving sparsity constraints.\n\nExample 2 (Sub-optimality for sparse linear models). We now consider the sparse variant of the linear regression problem, which involves the 0 -\"ball\"\nB 0 (s) : = x \u2208 R d | d j=1 I[x j = 0] \u2264 s},\ncorresponding to the set of all vectors with at most s non-zero entries. Fixing some radius R \u2265 \u221a s, consider a vector x * \u2208 C 0 : = B 0 (s) \u2229 { x 1 = R}, and suppose that we make noisy observations of the form y = Ax * + w.\n\nGiven this set-up, one way in which to estimate x * is by by computing the least-squares estimate x LS constrained 1 to the 1 -ball C = {x \u2208 R n | x 1 \u2264 R}. This estimator is a form of the Lasso [37]: as shown in Appendix D.2, when the design matrix A satisfies the restricted isometry property (see [10] for a definition), then it has MSE at most\nE x LS \u2212 x * 2 A \u03c3 2 s log ed s n . (12a)\nOn the other hand, the 1 2 -packing number M of the set C 0 can be lower bounded as log M s log ed s ; see Appendix D.2 for the details of this calculation. Consequently, in application to this particular problem, Theorem 1 implies that any estimator x \u2020 based on the pair (SA, Sy) has mean-squared error lower bounded as\nE w,S x \u2020 \u2212 x * 2 A \u03c3 2 s log ed s min{m, n} . (12b)\nAgain, we see that the projection dimension m must be of the order of n in order to match the mean-squared error of the constrained least-squares estimate x LS up to constant factors. By contrast, in this special case, the sketching method developed in this paper matches the error x LS \u2212 x * 2 using a sketch dimension that scales only as s log ed s + log(n); see Corollary 3 for the details of a more general result. \u2666 Example 3 (Sub-optimality for low-rank matrix estimation). In the problem of multivariate regression, the goal is to estimate a matrix X * \u2208 R d 1 \u00d7d 2 model based on observations of the form\nY = AX * + W,(13)\nwhere Y \u2208 R n\u00d7d 1 is a matrix of observed responses, A \u2208 R n\u00d7d 1 is a data matrix, and W \u2208 R n\u00d7d 2 is a matrix of noise variables. One interpretation of this model is as a collection of d 2 regression problems, each involving a d 1 -dimensional regression vector, namely a particular column of X * . In many applications, among them reduced rank regression, multitask learning and recommender systems (e.g., [36,43,27,9]), it is reasonable to model the matrix X * as having a low-rank. Note a rank constraint on matrix X be written as an 0 -\"norm\" constraint on its singular values: in particular, we have rank(X) \u2264 r if and only if\nmin{d 1 ,d 2 } j=1 I[\u03b3 j (X) > 0] \u2264 r,\nwhere \u03b3 j (X) denotes the j th singular value of X. This observation motivates a standard relaxation of the rank constraint using the nuclear norm |||X||| nuc : =\nmin{d 1 ,d 2 } j=1\n\u03b3 j (X). Accordingly, let us consider the constrained least-squares problem\nX LS = arg min X\u2208R d 1 \u00d7d 2 1 2 |||Y \u2212 AX||| 2 fro such that |||X||| nuc \u2264 R,(14)\nwhere ||| \u00b7 ||| fro denotes the Frobenius norm on matrices, or equivalently the Euclidean norm on its vectorized version. Let C 0 denote the set of matrices with rank r < 1 2 min{d 1 , d 2 }, and Frobenius norm at most one. In this case, we show in Appendix D that the constrained least-squares solution X LS satisfies the bound\nE X LS \u2212 X * 2 A \u03c3 2 r (d 1 + d 2 ) n . (15a)\nOn the other hand, the 1 2 -packing number of the set C 0 is lower bounded as log M r d 1 + d 2 , so that Theorem 1 implies that any estimator X \u2020 based on the pair (SA, SY ) has MSE lower bounded as\nE w,S X \u2020 \u2212 X * 2 A \u03c3 2 r d 1 + d 2 min{m, n} . (15b)\nAs with the previous examples, we see the sub-optimality of the sketched approach in the regime m < n. In contrast, for this class of problems, our sketching method matches the error X LS \u2212 X * A using a sketch dimension that scales only as {r(d 1 + d 2 ) + log(n)} log(n). See Corollary 4 for further details. \u2666\n\n\nIntroducing the Hessian sketch\n\nAs will be revealed during the proof of Theorem 1, the sub-optimality is in part due to sketching the response vector-i.e., observing Sy instead of y. It is thus natural to consider instead methods that sketch only the data matrix A, as opposed to both the data matrix and data vector y. In abstract terms, such methods are based on observing the pair SA, A T y \u2208 R m\u00d7d \u00d7 R d . One such approach is what we refer to as the Hessian sketchnamely, the sketched least-squares problem\n\nx : = arg min\nx\u2208C 1 2 SAx 2 2 \u2212 A T y, x g S (x) .(16)\nAs with the classical least-squares sketch (2), the quadratic form is defined by the matrix SA \u2208 R m\u00d7d , which leads to computational savings. Although the Hessian sketch on its own does not provide an optimal approximation to the least-squares solution, it serves as the building block for an iterative method that can obtain an \u03b5-accurate solution approximation in log(1/\u03b5) iterations.\n\nIn controlling the error with respect to the least-squares solution x LS the set of possible descent directions {x \u2212 x LS | x \u2208 C} plays an important role. In particular, we define the transformed tangent cone\nK LS = v \u2208 R d | v = t A(x \u2212 x LS ) for some t \u2265 0 and x \u2208 C .(17)\nNote that the error vector v : = A( x\u2212x LS ) of interest belongs to this cone. Our approximation bound is a function of the quantities\nZ 1 (S) : = inf v\u2208K LS \u2229S n\u22121 1 m Sv 2 2 and (18a) Z 2 (S) : = sup v\u2208K LS \u2229S n\u22121 u, ( S T S m \u2212 I n ) v ,(18b)\nwhere u is a fixed unit-norm vector. These variables played an important role in our previous analysis [31] of the classical sketch (2). The following bound applies in a deterministic fashion to any sketching matrix.\n\nProposition 1 (Bounds on Hessian sketch). For any convex set C and any sketching matrix S \u2208 R m\u00d7n , the Hessian sketch solution x satisfies the bound\nx \u2212 x LS A \u2264 Z 2 Z 1 x LS A .(19)\nFor random sketching matrices, Proposition 1 can be combined with probabilistic analysis to obtain high probability error bounds. For a given tolerance parameter \u03c1 \u2208 (0, 1 2 ], consider the \"good event\"\nE(\u03c1) : = Z 1 \u2265 1 \u2212 \u03c1, and Z 2 \u2264 \u03c1 2 . (20a)\nConditioned on this event, Proposition 1 implies that\nx \u2212 x LS A \u2264 \u03c1 2 (1 \u2212 \u03c1) x LS A \u2264 \u03c1 x LS A ,(20b)\nwhere the final inequality holds for all \u03c1 \u2208 (0, 1/2]. Thus, for a given family of random sketch matrices, we need to choose the projection dimension m so as to ensure the event E\u03c1 holds for some \u03c1. For future reference, let us state some known results for the cases of sub-Gaussian and ROS sketching matrices. We use (c 0 , c 1 , c 2 ) to refer to numerical constants, and we let D = dim(C) denote the dimension of the space C. In particular, we have D = d for vector-valued estimation, and D = d 1 d 2 for matrix problems.\n\nOur bounds involve the \"size\" of the cone K LS previously defined (17), as measured in terms of its Gaussian width\nW(K LS ) : = E g sup v\u2208K LS \u2229B 2 (1) | g, v | ,(21)\nwhere g \u223c N (0, I n ) is a standard Gaussian vector. With this notation, we have the following:\n\nLemma 1 (Sufficient conditions on sketch dimension [31]).\n\n(a) For sub-Gaussian sketch matrices, given a sketch size m > c 0\n\u03c1 2 W 2 (K LS ), we have P E(\u03c1)] \u2265 1 \u2212 c 1 e \u2212c 2 m\u03b4 2 . (22a)\n(b) For randomized orthogonal system (ROS) sketches over the class of self-bounding cones, given a sketch size m > c 0 log 4 (D)\n\u03c1 2 W 2 (K LS ), we have P E(\u03c1)] \u2265 1 \u2212 c 1 e \u2212c 2 m\u03c1 2 log 4 (D) . (22b)\nThe class of self-bounding cones is described more precisely in Lemma 8 of our earlier paper [31]. It includes among other special cases the cones generated by unconstrained leastsquares (Example 1), 1 -constrained least squares (Example 2), and least squares with nuclear norm constraints (Example 3). For these cones, given a sketch size m > c 0 log 4 (D)\n\u03c1 2 W 2 (K LS ),\nthe Hessian sketch applied with ROS matrices is guaranteed to return an estimate x such that x \u2212 x LS A \u2264 \u03c1 x LS A with high probability. This bound is an analogue of our earlier bound (5) for the classical sketch with f (x LS ) replaced by x LS A . For this reason, we see that the Hessian sketch alone suffers from the same deficiency as the classical sketch: namely, it will require a sketch size m n in order to mimic the O(n \u22121 ) accuracy of the least-squares solution.\n\n\nIterative Hessian sketch\n\nDespite the deficiency of the Hessian sketch itself, it serves as the building block for an novel scheme-known as the iterative Hessian sketch-that can be used to match the accuracy of the least-squares solution using a reasonable sketch dimension. Let begin by describing the underlying intuition. As summarized by the bound (20b), conditioned on the good event E(\u03c1), the Hessian sketch returns an estimate with error within a \u03c1-factor of x LS A , where x LS is the solution to the original unsketched problem. As show by Lemma 1, as long as the projection dimension m is sufficiently large, we can ensure that E(\u03c1) holds for some \u03c1 \u2208 (0, 1/2) with high probability. Accordingly, given the current iterate x t , suppose that we can construct a new least-squares problem for which the optimal solution is x LS \u2212 x t . Applying the Hessian sketch to this problem will then produce a new iterate x t+1 whose distance to x LS has been reduced by a factor of \u03c1. Repeating this procedure N times will reduce the initial approximation error by a factor \u03c1 N .\n\nWith this intuition in place, we now turn a precise formulation of the iterative Hessian sketch. Consider the optimization problem\nu = arg min u\u2208C\u2212x t 1 2 Au 2 2 \u2212 A T (y \u2212 Ax t ), u ,(23)\nwhere x t is the iterate at step t. By construction, the optimum to this problem is given by u = x LS \u2212 x t . We then apply to Hessian sketch to this optimization problem (23) in order to obtain an approximation x t+1 = x t + u to the original least-squares solution x LS that is more accurate than x t by a factor \u03c1 \u2208 (0, 1/2). Recursing this procedure yields a sequence of iterates whose error decays geometrically in \u03c1.\n\nFormally, the iterative Hessian sketch algorithm takes the following form:\n\nIterative Hessian sketch (IHS): Given an iteration number N \u2265 1:\n\n(1) Initialize at x 0 = 0.\n\n(2) For iterations t = 0, 1, 2, . . . , N \u2212 1, generate an independent sketch matrix S t+1 \u2208 R m\u00d7n , and perform the update\nx t+1 = arg min x\u2208C 1 2m S t+1 A(x \u2212 x t ) 2 2 \u2212 A T (y \u2212 Ax t ), x .(24)\n(3) Return the estimate x = x N .\n\nThe following theorem summarizes the key properties of this algorithm. It involves the sequence {Z 1 (S t ), Z 2 (S t )} N t=1 , where the quantities Z 1 and Z 2 were previously defined in equations (18a) and (18b). In addition, as a generalization of the event (20a), we define the sequence of \"good\" events\nE t (\u03c1) : = Z 1 (S t ) \u2265 1 \u2212 \u03c1, and Z 2 (S t ) \u2264 \u03c1 2 for t = 1, . . . , N .(25)\nWith this notation, we have the following guarantee:\n\nTheorem 2 (Guarantees for iterative Hessian sketch). The final solution x = x N satisfies the bound\nx \u2212 x LS A \u2264 N t=1 Z 2 (S t ) Z 1 (S t ) x LS A .(26a)\nConsequently, conditioned on the event \u2229 N t=1 E t (\u03c1) for some \u03c1 \u2208 (0, 1/2), we have\nx \u2212 x LS A \u2264 \u03c1 N x LS A .(26b)\nNote that for any \u03c1 \u2208 (0, 1/2), then event E t (\u03c1) implies that Z 2 (S t ) Z 1 (S t ) \u2264 \u03c1, so that the bound (26b) is an immediate consequence of the product bound (26a). Lemma 1 can be combined with the union bound in order to ensure that the compound event \u2229 N t=1 E t (\u03c1) holds with high probability over a sequence of N iterates, as long as the sketch size is lower bounded as m \u2265 c 0 \u03c1 2 W 2 (K LS ) log 4 (D) + log N . Based on the bound (26b), we then expect to observe geometric convergence of the iterates.\n\nIn order to test this prediction, we implemented the IHS algorithm using Gaussian sketch matrices, and applied it to an unconstrained least-squares problem based on a data matrix with dimensions (d, n) = (200, 6000) and noise variance \u03c3 2 = 1. As shown in Appendix D.2, the Gaussian width of K LS is proportional to d, so that Lemma 1 shows that it suffices to choose a projection dimension m \u03b3d for a sufficiently large constant \u03b3. Panel (a) of Figure 2 illustrates the resulting convergence rate of the IHS algorithm, measured in terms of the error x t \u2212 x LS A , for different values \u03b3 \u2208 {4, 6, 8}. As predicted by Theorem 2, the convergence rate is geometric (linear on the log scale shown), with the rate increasing as the parameter \u03b3 is increased. Assuming that the sketch dimension has been chosen to ensure geometric convergence, Theorem 2 allows us to specify, for a given target accuracy \u03b5 \u2208 (0, 1), the number of iterations required.\n\nCorollary 1. Fix some \u03c1 \u2208 (0, 1/2), and choose a sketch dimension m > c 0 log 4 (D)\n\u03c1 2 W 2 (K LS ).\nIf we apply the IHS algorithm for N (\u03c1, \u03b5) : = 1+ log(1/\u03b5) log(1/\u03c1) steps, then the output x = x N satisfies the bound\nx \u2212 x LS A x LS A \u2264 \u03b5(27)\nwith probability at least\n1 \u2212 c 1 N (\u03c1, \u03b5)e \u2212c 2 m\u03c1 2 log 4 (D) .\nThis corollary is an immediate consequence of Theorem 2 combined with Lemma 1, and it holds for both ROS and sub-Gaussian sketches. (In the latter case, the additional log(D) terms may be omitted.) Combined with bounds on the width function W(K LS ), it leads to a number of concrete consequences for different statistical models, as we illustrate in the following section. One way to understand the improvement of the IHS algorithm over the classical sketch is as follows. Fix some error tolerance \u03b5 \u2208 (0, 1). Disregarding logarithmic factors, our previous results [31] on the classical sketch then imply that a sketch size m \u03b5 \u22122 W 2 (K LS ) is sufficient to produce a \u03b5-accurate solution approximation. In contrast, Corollary 1 guarantees that a sketch size m log(1/\u03b5) W 2 (K LS ) is sufficient. Thus, the benefit is the reduction from \u03b5 \u22122 to log(1/\u03b5) scaling of the required sketch size.\n\nIt is worth noting that in the absence of constraints, the least-squares problem reduces to solving a linear system, so that alternative approaches are available. For instance, one can use a randomized sketch to obtain a preconditioner, which can then be used within the conjugate gradient method. As shown in past work [34,4], two-step methods of this type can lead to same reduction of \u03b5 \u22122 dependence to log(1/\u03b5). However, a method of this type is very specific to unconstrained least-squares, whereas the procedure described in this paper is generally applicable to least-squares over any compact, convex constraint set.\n\n\nComputational and space complexity\n\nLet us now make a few comments about the computational and space complexity of implementing the IHS algorithm using ROS sketches (e.g., such as those based on the fast Hadamard transform). For a given sketch size m, at iteration t, the IHS algorithm requires O(nd log(m)) basic operation for computing the data sketch S t+1 A and also O(nd) operations to compute A T (y \u2212 Ax t ). Consequently, if we run the algorithm for N iterations, then the overall complexity is O (nd log(m) + C(m, d)) N , where C(m, d) is the complexity of solving the m \u00d7 d dimensional problem in the update (24). The total space used scales as O(md).\n\nIf we want to obtain estimates with accuracy \u03b5, then we need to perform N log(1/\u03b5) iterations in total. Moreover, for ROS sketches, we need to choose m W 2 (K LS ) log 4 (d). Consequently, it only remains to bound the Gaussian width W in order to specify complexities that depend only on the pair (n, d).\n\nUnconstrained least-squares: For an unconstrained problem with n > d, the Gaussian width can be bounded as W 2 (K LS ) d, and the complexity of the solving the sub-problem (24) can be bounded as d 3 . Thus, the overall complexity of computing an \u03b5-accurate solution scales as O(nd log(d) + d 3 ) log(1/\u03b5), and the space required is O(d 2 ).\n\nSparse least-squares: As will be shown in Section 3.2, in certain cases, the cone K LS can have substantially lower complexity than the unconstrained case. For instance, if the solution is sparse, say with s non-zero entries and the least-squares program involves an 1 -constraint, then we have W 2 (K LS ) s log d. Using a standard interior point method to solve the sketched problem, the total complexity for obtaining an \u03b5-accurate solution is upper bounded by O((nd log(s)+s 2 d log 2 (d)) log(1/\u03b5)), along with an O(sd log(d)) space complexity. The sparsity s is not known a priori, however we note that there exists efficient bounds on s which can be computed in O(nd) time (see e.g. [17]). Another approach is to impose some conditions on the design matrix A which will guarantee support recovery, i.e., number of nonzero entries of x * equals s, (e.g., see [40]).\n\n\nConsequences for concrete models\n\nIn this section, we derive some consequences of Corollary 1 for particular classes of leastsquares problems. Our goal is to provide empirical confirmation of the sharpness of our theoretical predictions, namely the minimal sketch dimension required in order to match the accuracy of the original least-squares solution.\n\n\nUnconstrained least squares\n\nWe begin with the simplest case, namely the unconstrained least-squares problem (C = R d ). For a given pair (n, d) with n > d, we generated a random ensemble of least-square problems according to the following procedure:\n\n\u2022 first, generate a random data matrix A \u2208 R n\u00d7d with i.i.d. N (0, 1) entries\n\n\u2022 second, choose a regression vector x * uniformly at random from the sphere S d\u22121\n\n\u2022 third, form the response vector y = Ax * + w, where w \u223c N (0, \u03c3 2 I n ) is observation noise with \u03c3 = 1.\n\nAs discussed following Lemma 1, for this class of problems, taking a sketch dimension m d \u03c1 2\n\nguarantees \u03c1-contractivity of the IHS iterates with high probability. Consequently, we can obtain a \u03b5-accurate approximation to the original least-squares solution by running roughly log(1/\u03b5)/ log(1/\u03c1) iterations. Now how should the tolerance \u03b5 be chosen? Recall that the underlying reason for solving the least-squares problem is to approximate x * . Given this goal, it is natural to measure the approximation quality in terms of x t \u2212 x * A . Panel (b) of Figure 2 shows the convergence of the iterates to x * . As would be expected, this measure of error levels off at the ordinary least-squares error\nx LS \u2212 x * 2 A \u03c3 2 d n \u2248 0.10.\nConsequently, it is reasonable to set the tolerance parameter proportional to \u03c3 2 d n , and then perform roughly 1 + log(1/\u03b5) log(1/\u03c1) steps. The following corollary summarizes the properties of the resulting procedure:\n\nCorollary 2. For some given \u03c1 \u2208 (0, 1/2), suppose that we run the IHS algorithm for\nN = 1 + log \u221a n x LS A \u03c3 log(1/\u03c1)\niterations using m = c 0 \u03c1 2 d projections per round. Then the output x satisfies the bounds\nx \u2212 x LS A \u2264 \u03c3 2 d n , and x N \u2212 x * A \u2264 \u03c3 2 d n + x LS \u2212 x * A(28)\nwith probability greater than 1 \u2212 c 1 N e \u2212c 2 m\u03c1 2 log 4 (d) .\n\nIn order to confirm the predicted bound (28) on the error x \u2212 x LS A , we performed a second experiment. Fixing n = 100d, we generated T = 20 random least squares problems from the ensemble described above with dimension d ranging over {32, 64, 128, 256, 512}. By our previous choices, the least-squares estimate should have error x LS \u2212 x * 2 \u2248 \u03c3 2 d n = 0.1 with high probability, independently of the dimension d. This predicted behavior is confirmed by the blue bars in Figure 3; the bar height corresponds to the average over T = 20 trials, with the standard errors also marked. On these same problem instances, we also ran the IHS algorithm using m = 6d samples per iteration, and for a total of\nN = 1 + log n d log 2 = 4 iterations. Since x LS \u2212x * A \u03c3 2 d\nn \u2248 0.10, Corollary 2 implies that with high probability, the sketched solution x = x N satisfies the error bound\nx \u2212 x * 2 \u2264 c 0 \u03c3 2 d n\nfor some constant c 0 > 0. This prediction is confirmed by the green bars in Figure 3, showing that x \u2212 x * A \u2248 0.11 across all dimensions. Finally, the red bars show the results of running the classical sketch with a sketch dimension of (6 \u00d7 4)d = 24d sketches, corresponding to the total number of sketches used by the IHS algorithm. Note that the error is roughly twice as large.\n\n\nSparse least-squares\n\nWe now turn to a study of an 1 -constrained form of least-squares, referred to as the Lasso or relaxed basis pursuit [11,37]. In particular, consider the convex program\n\nx LS = arg min\nx 1 \u2264R 1 2 y \u2212 Ax 2 2 ,(29)\nwhere R > 0 is a user-defined radius. This estimator is well-suited to the problem of sparse linear regression, based on the observation model y = Ax * + w, where x * has at most s nonzero entries, and A \u2208 R n\u00d7d has i.i.d. N (0, 1) entries. For the purposes of this illustration, we assume 2 that the radius is chosen such that R = x * 1 . Under these conditions, the proof of Corollary 3 shows that a sketch size m \u2265 \u03b3 s log ed s suffices to guarantee geometric convergence of the IHS updates. Panel (a) of Figure 4 illustrates the accuracy of this prediction, showing the resulting convergence rate of the the IHS algorithm, measured in terms of the error x t \u2212 x LS A , for different values \u03b3 \u2208 {2, 5, 25}. As predicted by Theorem 2, the convergence rate is geometric (linear on the log scale shown), with the rate increasing as the parameter \u03b3 is increased.  As long as n s log ed s , it also follows as a corollary of Proposition 2 that\nx LS \u2212 x * 2 A \u03c3 2 s log ed s n .(30)\nwith high probability. This bound suggests an appropriate choice for the tolerance parameter \u03b5 in Theorem 2, and leads us to the following guarantee. In order to verify the predicted bound (31) on the error x\u2212x LS A , we performed a second experiment. Fixing n = 100s log ed s . we generated T = 20 random least squares problems (as described above) with the regression dimension ranging as d \u2208 {32, 64, 128, 256}, and  . This predicted behavior is confirmed by the blue bars in Figure 5; the bar height corresponds to the average over T = 20 trials, with the standard errors also marked.\n\nOn these same problem instances, we also ran the IHS algorithm using N = 4 iterations with a sketch size m = 4s log ed s . Together with our earlier calculation of x LS \u2212 x * A , Corollary 2 implies that with high probability, the sketched solution x = x N satisfies the error bound\nx \u2212 x * A \u2264 c 0 \u03c3 2 s log ed s n for some constant c 0 \u2208 (1, 2]\n. This prediction is confirmed by the green bars in Figure 5,\n\nshowing that x \u2212 x * A 0.11 across all dimensions. Finally, the green bars in Figure 5 show the error based on using the naive sketch estimate with a total of M = N m random projections in total; as with the case of ordinary least-squares, the resulting error is roughly twice as large. We also note that a similar bound also applies to problems where a parameter constrained to unit simplex is estimated, e.g., in portfolio analysis and density estimation [26,30].  Simulations of the IHS algorithm for 1 -constrained least-squares. In these experiments, we generated random sparse least-squares problem of dimensions d \u2208 {16, 32, 64, 128, 256} and sparsity s = 2 \u221a d , on all occasions with a fixed sample size n = 100s log ed s . The initial Lasso solution has error x LS \u2212x * 2 \u2248 0.10, as shown by the blue bars. We then ran the IHS algorithm for N = 4 iterations with a sketch size m = 4s log ed s . These sketched solutions show an error x \u2212 x * A \u2248 0.11 independently of dimension, consistent with the predictions of Corollary 3. Red bars show the error in the naive sketch estimate, using a sketch of size M = N m = 16s log ed s , equal to the total number of random projections used by the IHS algorithm. The resulting error is roughly twice as large.\n\n\nMatrix estimation with nuclear norm constraints\n\nWe now turn to the study of nuclear-norm constrained form of least-squares matrix regression. This class of problems has proven useful in many different application areas, among them matrix completion, collaborative filtering, multi-task learning and control theory (e.g., [18,42,5,33,28]). In particular, let us consider the convex program\nX LS = arg min X\u2208R d 1 \u00d7d 2 1 2 |||Y \u2212 AX||| 2 fro such that |||X||| nuc \u2264 R,(32)\nwhere R > 0 is a user-defined radius as a regularization parameter.\n\n\nSimulated data\n\nRecall the linear observation model previously introduced in Example 3: we observe the pair (Y, A) linked according to the linear Y = AX * + W , where the unknown matrix X * \u2208 R d 1 \u00d7d 2 is an unknown matrix of rank r. The matrix W is observation noise, formed with i.i.d. N (0, \u03c3 2 ) entries. This model is a special case of the more general class of matrix regression problems [28]. As shown in Appendix D.2, if we solve the nuclear-norm constrained problem with R = |||X * ||| nuc , then it produces a solution such that\nE |||X LS \u2212 X * ||| 2 fro ] \u03c3 2 r (d 1 +d 2 ) n .\nThe following corollary characterizes the sketch dimension and iteration number required for the IHS algorithm to match this scaling up to a constant factor.  d 1 d 2 ) , the output X N satisfies the bound\nX N \u2212 X * A \u2264 \u03c3 2 r d 1 + d 2 n + X LS \u2212 X * A .(33)\nWe have also performed simulations for low-rank matrix estimation, and observed that the IHS algorithm exhibits convergence behavior qualitatively similar to that shown in Figures 3  and 5. Similarly, panel (a) of Figure 7 compares the performance of the IHS and classical methods for sketching the optimal solution over a range of row sizes n. As with the unconstrained least-squares results from Figure 1, the classical sketch is very poor compared to the original solution whereas the IHS algorithm exhibits near optimal performance.\n\n\nApplication to multi-task learning\n\nTo conclude, let us illustrate the use of the IHS algorithm in speeding up the training of a classifier for facial expressions. In particular, suppose that our goal is to separate a collection of facial images into different groups, corresponding either to distinct individuals or to different facial expressions. One approach would be to learn a different linear classifier (a \u2192 a, x ) for each separate task, but since the classification problems are so closely related, the optimal classifiers are likely to share structure. One way of capturing this shared structure is by concatenating all the different linear classifiers into a matrix, and then estimating this matrix in conjunction with a nuclear norm penalty [2,3]. In more detail, we performed a simulation study using the The Japanese Female Facial Expression (JAFFE) database [24]. It consists of N = 213 images of 7 facial expressions (6 basic facial expressions + 1 neutral) posed by 10 different Japanese female models; see Figure 6 for a few example images. We performed an approximately 80 : 20 split of the data set into n train = 170 training and n test = 43 test images respectively. Then we consider classifying each facial expression and each female model as a separate task which gives a total of d task = 17 tasks. For each task j = 1, . . . , d task , we construct a linear classifier of the form a \u2192 sign( a, x j ), where a \u2208 R d denotes the vectorized image features given by Local Phase Quantization [29]. In our implementation, we fixed the number of features d = 32. Given this set-up, we train the classifiers in a joint manner, by optimizing simultaneously over the matrix X \u2208 R d\u00d7d task with the classifier vector x j \u2208 R d as its j th column. The image data is loaded into the matrix A \u2208 R n train \u00d7d , with image feature vector a i \u2208 R d in column i for i = 1, . . . , n train . Finally, the matrix Y \u2208 {\u22121, +1} n train \u00d7d task encodes class labels for the different classification problems. These instantiations of the pair (Y, X) give us an optimization problem of the form (32), and we solve it over a range of regularization radii R.\n\nMore specifically, in order to verify the classification accuracy of the classifier obtained by IHT algorithm, we solved the original convex program, the classical sketch based on ROS sketches of dimension m = 100, and also the corresponding IHS algorithm using ROS sketches of size 20 in each of 5 iterations. In this way, both the classical and IHS procedures use the same total number of sketches, making for a fair comparison. We repeated each of these three procedures for all choices of the radius R \u2208 {1, 2, 3, . . . , 12}, and then applied the resulting classifiers to classify images in the test dataset. For each of the three procedures, we calculated the classification error rate, defined as the total number of mis-classified images divided by n test \u00d7 d task . Panel (b) of Figure 7 plots the resulting classification errors versus the regularization parameter. The error bars correspond to one standard deviation calculated over the randomness in generating sketching matrices. The plots show that the IHS algorithm yields classifiers with performance close to that given by the original solution over a range of regularizer parameters, and is superior to the classification sketch. The error bars also show that the IHS algorithm has less variability in its outputs than the classical sketch.\n\n\nDiscussion\n\nIn this paper, we focused on the problem of solution approximation (as opposed to cost approximation) for a broad class of constrained least-squares problem. We began by showing that the classical sketching methods are sub-optimal, from an information-theoretic point of view, for the purposes of solution approximation. We then proposed a novel iterative scheme, known as the iterative Hessian sketch, for deriving \u03b5-accurate solution approximations. We proved a general theorem on the properties of this algorithm, showing that the sketch dimension per iteration need grow only proportionally to the statistical dimension of the optimal solution, as measured by the Gaussian width of the tangent cone at the optimum. By taking log(1/\u03b5) iterations, the IHS algorithm is guaranteed to return an \u03b5-accurate solution approximation with exponentially high probability.\n\nIn addition to these theoretical results, we also provided empirical evaluations that reveal the sub-optimality of the classical sketch, and show that the IHS algorithm produces nearoptimal estimators. Finally, we applied our methods to a problem of facial expression using a multi-task learning model applied to the JAFFE face database. We showed that IHS algorithm applied to a nuclear-norm constrained program produces classifiers with considerably better classification accuracy compared to the naive sketch. There are many directions for further research, but we only list here some of them. The idea behind iterative sketching can also be applied to problems beyond minimizing a leastsquares objective function subject to convex constraints. An important class of such problems are p -norm forms of regression, based on the convex program\nmin x\u2208R d Ax \u2212 y p p for some p \u2208 [1, \u221e].\nThe case of 1 -regression (p = 1) is an important special case, known as robust regression; it is especially effective for data sets containing outliers [20]. Recent work [12] has proposed to find faster solutions of the 1 -regression problem using the classical sketch (i.e., based on (SA, Sy)) but with sketching matrices based on Cauchy random vectors. Based on the results of the current paper, our iterative technique might be useful in obtaining sharper bounds for solution approximation in this setting as well.\n\naddition, MP was supported by a Microsoft Research Fellowship.\n\n\nA Proof of lower bounds\n\nThis appendix is devoted to the verification of condition (9) for different model classes, followed by the proof of Theorem 1.\n\n\nA.1 Verification of condition (9)\n\nWe verify the condition for three different types of sketches.\n\nGaussian sketches: First, let S \u2208 R m\u00d7n be a random matrix with i.i.d. Gaussian entries. We use the singular value decomposition to write S = U \u039bV T where both U and V are orthonormal matrices of left and right singular vectors. By rotation invariance, the columns {v i } m i=1 are uniformly distributed over the sphere S n\u22121 . Consquently, we have\nE S S T SS T ) \u22121 S = E m i=1 v i v T i = m n I n ,(34)\nshowing that condition (9) holds with \u03b7 = 1.\n\n\nROS sketches:\n\nIn this case, we have S = \u221a nP HD, where P \u2208 R m\u00d7n is a random picking matrix (with each row being a standard basis vector). We then have SS T = nI m and also E P [P T P ] = m n I n , so that showing that the condition holds with \u03b7 = 1.\n\nWeighted row sampling: Finally, suppose that we sample m rows independently using a distribution {p j } n j=1 on the rows of the data matrix that is \u03b1-balanced (7). Letting R \u2286 {1, 2, . . . , n} be the subset of rows that are sampled, and let N j be the number of times each row is sampled. We then have\nE S T SS T ) \u22121 S = j\u2208R E[e j e T j ] = D,\nwhere D \u2208 R n\u00d7n is a diagonal matrix with entries D jj = P[j \u2208 R]. Since the trials are independent, the j th row is sampled at least once in m trials with probability q j = 1\u2212(1\u2212p j ) m , and hence\nE S S T SS T ) \u22121 S = diag {1 \u2212 (1 \u2212 p i ) m } m i=1 1 \u2212 (1 \u2212 p \u221e ) m I n mp \u221e ,\nwhere p \u221e = max j\u2208[n] p j . Consequently, as long as the row weights are \u03b1-balanced (7) so that p \u221e \u2264 \u03b1 n , we have |||E S S T SS T ) \u22121 S ||| op \u2264 \u03b1 m n showing that condition (9) holds with \u03b7 = \u03b1, as claimed.\n\n\nA.2 Proof of Theorem 1\n\nLet {z j } M j=1 be a 1/2-packing of C 0 in the semi-norm \u00b7 A and for a fixed \u03b4 \u2208 (0, 1/4), define x j = 4\u03b4z j . We thus obtain a collection of vectors in C 0 such that\n2\u03b4 \u2264 1 \u221a n A(x j \u2212 x k ) 2 \u2264 8\u03b4 for all j = k.\nLetting J be a random index uniformly distributed over {1, . . . , M }, suppose that conditionally on J = j, we observe the sketched observation vector Sy = SAx j + Sw, as well as the sketched matrix SA. Conditioned on J = j, the random vector Sy follows a N (SAx j , \u03c3 2 SS T ) distribution, denoted by P x j . We let Y denote the resulting mixture variable, with distribution 1 M M j=1 P x j . Consider the multiway testing problem of determining the index J based on observing Y . With this set-up, a standard reduction in statistical minimax (e.g., [7,41]) implies that, for any estimator x \u2020 , the worst-case mean-squared error is lower bounded as\nsup x * \u2208C E S,w x \u2020 \u2212 x * 2 A \u2265 \u03b4 2 inf \u03c8 P[\u03c8(Y ) = J],(35)\nwhere the infimum ranges over all testing functions \u03c8. Consequently, it suffices to show that the testing error is lower bounded by 1/2. In order to do so, we first apply Fano's inequality [13] conditionally on the sketching matrix S to see that\nP[\u03c8(Y ) = J] = E S P[\u03c8(Y ) = J | S] \u2265 1 \u2212 E S I S (Y ; J) + log 2 log M ,(36)\nwhere I S (Y ; J) denotes the mutual information between Y and J with S fixed. Our next step is to upper bound the expectation E S [I(Y ; J)].\n\nLetting D(P x j P x k ) denote the Kullback-Leibler divergence between the distributions P x j and P x k , the convexity of Kullback-Leibler divergence implies that\nI S (Y ; J) = 1 M M j=1 D(P x j 1 M M k=1 P x k ) \u2264 1 M 2 M j,k=1 D(P x j P x k ).\nComputing the KL divergence for Gaussian vectors yields\nI S (Y ; J) \u2264 1 M 2 M j,k=1 1 2\u03c3 2 (x j \u2212 x k ) T A T S T (SS T ) \u22121 S A(x j \u2212 x k ).\nThus, using condition (9), we have\nE S [I(Y ; J)] \u2264 1 M 2 M j,k=1 m \u03b7 2 n\u03c3 2 A(x j \u2212 x k ) 2 2 \u2264 32 m \u03b7 \u03c3 2 \u03b4 2 ,\nwhere the final inequality uses the fact that x j \u2212 x k A \u2264 8\u03b4 for all pairs. Combined with our previous bounds (35) and (36), we find that \nsup x * \u2208C E x \u2212 x * 2 2 \u2265 \u03b4 2 1 \u2212\n\nB Proof of Proposition 1\n\nSince x and x LS are optimal and feasible, respectively, for the Hessian sketch program (16), we have\nA T S T SA x \u2212 y , x LS \u2212 x \u2265 0 (37a)\nSimilarly, since x LS and x are optimal and feasible, respectively, for the original least squares program\nA T (Ax LS \u2212 y), x \u2212 x LS \u2265 0. (37b)\nAdding these two inequalities and performing some algebra yields the basic inequality\n1 m SA\u2206 2 2 \u2264 (Ax LS ) T I n \u2212 S T S m A\u2206 .(38)\nSince Ax LS is independent of the sketching matrix and A\u2206 \u2208 K LS , we have\n1 m SA\u2206 2 2 \u2265 Z 1 A\u2206 2 2 , and (Ax LS ) T I n \u2212 S T S A\u2206 \u2264 Z 2 Ax LS 2 A\u2206 2 ,\nusing the definitions (18a) and (18b) of the random variables Z 1 and Z 2 respectively. Combining the pieces yields the claim.\n\n\nC Proof of Theorem 2\n\nIt suffices to show that, for each iteration t = 0, 1, 2, . . ., we have\nx t+1 \u2212 x LS A \u2264 Z 2 (S t+1 ) Z 1 (S t+1 ) x t \u2212 x LS A .(39)\nThe claimed bounds (26a) and (26b) then follow by applying the bound (39) successively to iterates 1 through N . For simplicity in notation, we abbreviate S t+1 to S and x t+1 to x. Define the error vector \u2206 = x \u2212 x LS . With some simple algebra, the optimization problem (24) that underlies the update t + 1 can be re-written as\nx = arg min x\u2208C 1 2m SAx 2 2 \u2212 A T y, x ,\nwhere y : = y + I \u2212 S T S m Ax t . Since x and x LS are optimal and feasible respectively, the usual first-order optimality conditions imply that\nA T S T S m Ax \u2212 A T y, x LS \u2212 x \u2265 0.\nAs before, since x LS is optimal for the original program, we have\nA T (Ax LS \u2212 y + I \u2212 S T S m Ax t ), x \u2212 x LS \u2265 0.\nAdding together these two inequalities and introducing the shorthand \u2206 = x \u2212 x LS yields\n1 m SA\u2206 2 2 \u2264 (A(x LS \u2212 x t ) T I \u2212 S T S m A\u2206(40)\nNote that the vector A(x LS \u2212 x t ) is independent of the randomness in the sketch matrix S t+1 . Moreover, the vector A\u2206 belongs to the cone K, so that by the definition of Z 2 (S t+1 ), we have\n(A(x LS \u2212 x t ) T I \u2212 S T S m A\u2206 \u2264 A(x LS \u2212 x t ) 2 A\u2206 2 Z 2 (S t+1 ).(41a)\nSimilarly, note the lower bound\n1 m SA\u2206 2 2 \u2265 A\u2206 2 2 Z 1 (S t+1 ).(41b)\nCombining the two bounds (41a) and (41b) with the earlier bound (40) yields the claim (39).\n\n\nD Maximum likelihood estimator and examples\n\nIn this section, we a general upper bound on the error of the constrained least-squares estimate. We then use it (and other results) to work through the calculations underlying Examples 1 through 3 from Section 2.2.\n\n\nD.1 Upper bound on MLE\n\nThe accuracy of x LS as an estimate of x * depends on the \"size\" of the star-shaped set\nK(x * ) = v \u2208 R d | v = t \u221a n\nA(x \u2212 x * ) for some t \u2208 [0, 1] and x \u2208 C .\n\nWhen the vector x * is clear from context, we use the shorthand notation K * for this set. By taking a union over all possible x * \u2208 C 0 , we obtain the set K : =\n\nx * \u2208C 0 K(x * ), which plays an important role in our bounds. The complexity of these sets can be measured of their localized Gaussian widths. For any radius \u03b5 > 0 and set \u0398 \u2286 R n , the Gaussian width of the set \u0398 \u2229 B 2 (\u03b5) is given by\nW \u03b5 (\u0398) : = E g sup \u03b8\u2208\u0398 \u03b8 2 \u2264\u03b5 | w, \u03b8 | ,(43a)\nwhere g \u223c N (0, I n\u00d7n ) is a standard Gaussian vector. Whenever the set \u0398 is star-shaped, then it can be shown that, for any \u03c3 > 0 and positive integer , the inequality\nW \u03b5 (\u0398) \u03b5 \u221a \u2264 \u03b5 \u03c3 (43b)\nhas a smallest positive solution, which we denote by \u03b5 (\u0398; \u03c3). We refer the reader to Bartlett et al. [6] for further discussion of such localized complexity measures and their properties.\n\nThe following result bounds the mean-squared error associated with the constrained leastsquares estimate:\n\nusing the lower RIP property (i). By the upper RIP property, for any pair of vectors \u2206, \u2206 with 0 -norms at most 4s, we have\n\n\nD.3 Proof of Proposition 2\n\nThroughout this proof, we adopt the shorthand \u03b5 n = \u03b5 n (K * ). Our strategy is to prove the following more general claim: for any t \u2265 \u03b5 n , we have\nP S,w x LS \u2212 x * 2 A \u2265 16t\u03b5 n \u2264 c 1 e \u2212c 2 nt\u03b5n \u03c3 2 .(45)\nA simple integration argument applied to this tail bound implies the claimed bound (44) on the expected mean-squared error. Since x * and x LS are feasible and optimal, respectively, for the optimization problem (1), we have the basic inequality\n1 2n y \u2212 Ax LS 2 2 \u2264 1 2n y \u2212 Ax * 2 = 1 2n w 2 2 .\nIntroducing the shorthand \u2206 = x LS \u2212 x * and re-arranging terms yields\n1 2 \u2206 2 A = 1 2n A\u2206 2 2 \u2264 \u03c3 n n i=1 g, A\u2206 ,(46)\nwhere g \u223c N (0, I n ) is a standard normal vector. For a given u \u2265 \u03b5 n , define the \"bad\" event B(u) : = \u2203 z \u2208 C \u2212 x * with z A \u2265 u, and | \u03c3 n n i=1 g i (Az) i | \u2265 2u z A The following lemma controls the probability of this event:\n\nLemma 2. For all u \u2265 \u03b5 n , we have P[B(u)] \u2264 e \u2212 nu 2 2\u03c3 2 .\n\nReturning to prove this lemma momentarily, let us prove the bound (45). For any t \u2265 \u03b5 n , we can apply Lemma 2 with u = \u221a t\u03b5 n to find that\nP[B c ( \u221a t\u03b5 n )] \u2265 1 \u2212 e \u2212 nt\u03b5n 2\u03c3 2 .\nIf \u2206 A < \u221a t \u03b5 n , then the claim is immediate. Otherwise, we have \u2206 A \u2265 \u221a t \u03b5 n . Since \u2206 \u2208 C \u2212 x * , we may condition on B c ( \u221a t\u03b5 n ) so as to obtain the bound\n\u03c3 n n i=1 g i (A\u2206) i \u2264 2 \u2206 A \u221a t\u03b5 n .\nCombined with the basic inequality (46), we see that\n1 2 \u2206 2 A \u2264 2 \u2206 A \u221a t\u03b5 n , or equivalently \u2206 2 A \u2264 16t\u03b5 n ,\na bound that holds with probability greater than 1 \u2212 e \u2212 nt\u03b5n 2\u03c3 2 as claimed.\n\nIt remains to prove Lemma 2. Our proof involves the auxiliary random variable V n (u) : = sup\nz\u2208star(C\u2212x * ) z A \u2264u | \u03c3 n n i=1 g i (Az) i |,\nInclusion of events: We first claim that B(u) \u2286 {V n (u) \u2265 2u 2 }. Indeed, if B(u) occurs, then there exists some z \u2208 C \u2212 x * with z A \u2265 u and\n| \u03c3 n n i=1 g i (Az) i | \u2265 2u z A .(47)\nDefine the rescaled vector z = u z A z. Since z \u2208 C\u2212x * and u z A \u2264 1, the vector z \u2208 star(C\u2212x * ). Moreover, by construction, we have z A = u. When the inequality (47) holds, the vector z thus satisfies | \u03c3 n n i=1 g i (A z) i | \u2265 2u 2 , which certifies that V n (u) \u2265 2u 2 , as claimed.\n\nControlling the tail probability: The final step is to control the probabability of the event {V n (u) \u2265 2u 2 }. Viewed as a function of the standard Gaussian vector (g 1 , . . . , g n ), it is easy to see that V n (u) is Lipschitz with constant L = \u03c3u \u221a n . Consequently, by concentration of measure for Lipschitz Gaussian functions, we have\nP V n (u) \u2265 E[V n (u)] + u 2 \u2264 e \u2212 nu 2 2\u03c3 2 .(48)\nIn order to complete the proof, it suffices to show that E[V n (u)] \u2264 u 2 . By definition, we have E[V n (u)] = \u03c3 \u221a n W u (K * ). Since K * is a star-shaped set, the function v \u2192 W v (K * )/v is non-increasing [6]. Since u \u2265 \u03b5 n , we have\n\u03c3 W u (K * ) u \u2264 \u03c3 W \u03b5n (K * ) \u03b5 n \u2264 \u03b5 n .\nwhere the final step follows from the definition of \u03b5 n . Putting together the pieces, we conclude that E[V n (u)] \u2264 \u03b5 n u \u2264 u 2 as claimed.\n\nx\nLS : = arg min x\u2208C f (x) where f (x) : = 1 2n Ax \u2212 y 2 2 .\n\nFigure 1 .\n1Plots of mean-squared error versus the row dimension n \u2208 {100, 200, 400, . . . , 25600} for unconstrained least-squares in dimension d = 10. The blue curves correspond to the error x LS \u2212 x * of the unsketched least-squares estimate. Red curves correspond to the IHS method applied for N = 1 + log(n) rounds using a sketch size m = 7d. Black curves correspond to the naive sketch applied using M = N m projections in total, corresponding to the same number used in all iterations of the IHS algorithm. (a) Error x\n\nFigure 2 .\n2Simulations of the IHS algorithm for an unconstrained least-squares problem with noise variance \u03c3 2 = 1, and of dimensions (d, n) = (200, 6000). Simulations based on sketch sizes m = \u03b3d, for a parameter \u03b3 > 0 to be set. (a) Plots of the log error x t \u2212 x LS A versus the iteration number t. Three different curves for \u03b3 \u2208 {4, 6, 8}. Consistent with the theory, the convergence is geometric, with the rate increasing as the sampling factor \u03b3 is increased. (b) Plots of the log error x t \u2212 x * A versus the iteration number t. Three different curves for \u03b3 \u2208 {4, 6, 8}. As expected, all three curves flatten out at the level of the least-squares error x LS \u2212 x * A = 0.20 \u2248 \u03c3 2 d/n.\n\nFigure 3 .\n3Simulations of the IHS algorithm for unconstrained least-squares. In these experiments, we generated random least-squares problem of dimensions d \u2208 {16, 32, 64, 128, 256}, on all occasions with a fixed sample size n = 100d. The initial least-squares solution has error x LS \u2212 x * A \u2248 0.10, as shown by the blue bars. We then ran the IHS algorithm for N = 4 iterations with a sketch size m = 6d. As shown by the green bars, these sketched solutions show an error x \u2212 x * A \u2248 0.11 independently of dimension, consistent with the predictions of Corollary 2. Finally, the red bars show the error in the classical sketch, based on a sketch size M = N m = 24d, corresponding to the total number of projections used in the iterative algorithm. This error is roughly twice as large.\n\nCorollary 3 .\n3For the stated random ensemble of sparse linear regression problems, suppose that we run the IHS algorithm for N d) , the output x satisfies the boundsx \u2212 x LS A \u2264 \u03c3 2 s log ed s n and x N \u2212 x * A \u2264 \u03c3 2 s log ed s n + x LS \u2212 x * A .(31)\n\nFigure 4 .\n4Simulations of the IHS algorithm for a sparse least-squares problem with noise variance \u03c3 2 = 1, and of dimensions (d, n, s) = (256, 8872, 32). Simulations based on sketch sizes m = \u03b3s log d, for a parameter \u03b3 > 0 to be set. (a) Plots of the log error x t \u2212 x LS 2 versus the iteration number t. Three different curves for \u03b3 \u2208 {2, 5, 25}. Consistent with the theory, the convergence is geometric, with the rate increasing as the sampling factor \u03b3 is increased. (b) Plots of the log error x t \u2212 x * 2 versus the iteration number t. Three different curves for \u03b3 \u2208 {2, 5, 25}. As expected, all three curves flatten out at the level of the least-squares error x LS \u2212 x * 2 = 0.10 \u2248 s log(ed/s) n . sparsity s = 2 \u221a d . Based on these choices, the least-squares estimate should have error x LS \u2212 x * A \u2248 \u03c3 2 s log ed s n = 0.1 with high probability, independently of the pair (s, d)\n\nFigure 5 .\n5Figure 5. Simulations of the IHS algorithm for 1 -constrained least-squares. In these experiments, we generated random sparse least-squares problem of dimensions d \u2208 {16, 32, 64, 128, 256} and sparsity s = 2 \u221a d , on all occasions with a fixed sample size n = 100s log ed s . The initial Lasso solution has error x LS \u2212x * 2 \u2248 0.10, as shown by the blue bars. We then ran the IHS algorithm for N = 4 iterations with a sketch size m = 4s log ed s . These sketched solutions show an error x \u2212 x * A \u2248 0.11 independently of dimension, consistent with the predictions of Corollary 3. Red bars show the error in the naive sketch estimate, using a sketch of size M = N m = 16s log ed s , equal to the total number of random projections used by the IHS algorithm. The resulting error is roughly twice as large.\n\nCorollary 4 (\n4IHS for nuclear-norm constrained least squares). Suppose that we run the IHS algorithm for N m = c 0 \u03c1 2 r d 1 + d 2 projections per round. Then with probability greater than 1 \u2212 c\n\nFigure 6 .\n6Japanese Female Facial Expression (JAFFE) Database: The JAFFE database consists of 213 images of 7 different emotional facial expressions (6 basic facial expressions + 1 neutral) posed by 10 Japanese female models.\n\nFigure 7 .\n7Simulations of the IHS algorithm for nuclear-norm constrained problems. The blue curves correspond to the solution of the original (unsketched problem), wheras red curves correspond to the IHS method applied for N = 1 + log(n) rounds using a sketch size of m. Black curves correpsond to the naive sketch applied using M = N m projections in total, corresponding to the same number used in all iterations of the IHS algorithm. (a) Mean-squared error versus the row dimension n \u2208 [10, 100] for recovering a 20 \u00d7 20 matrix of rank r2, using a sketch dimension m = 60. Note how the accuracy of the IHS algorithm tracks the error of the unsketched solution over a wide range of n, whereas the classical sketch has essentially constant error. (b) Classification error rate versus regularization parameter R \u2208 {1, . . . , 12}, with error bars corresponding to one standard deviation over the test set. Sketching algorithms were applied to the JAFFE face expression using a sketch dimension of M = 100 for the classical sketch, and N = 5 iterations with m = 20 sketches per iteration for the IHS algorithm.\n\nE\nS [S T (SS T ) \u22121 S] = E D,P [DH T P T P HD] = E D [DH T (\nThis set-up is slightly unrealistic, since the estimator is assumed to know the radius R = x * 1. In practice, one solves the least-squares problem with a Lagrangian constraint, but the underlying arguments are basically the same.\nIn practice, this unrealistic assumption of exactly knowing x * 1 is avoided by instead considering the 1-penalized form of least-squares, but we focus on the constrained case to keep this illustration as simple as possible.\nAcknowledgementsBoth authors were partially supported by Office of Naval Research MURI grant N00014-11-1-0688, and National Science Foundation Grants CIF-31712-23800 and DMS-1107000. InProposition 2. For any set C containing x * , the constrained least-squares estimate (1) has mean-squared error upper bounded asWe provide the proof of this claim in Section D.3.D.2 Detailed calculations for illustrative examplesIn this appendix, we collect together the details of calculations used in our illustrative examples from Section 2.2. In all cases, we make use tof the convenient shorthand A = A/ \u221a n.D.2.1 Unconstrained least squares: Example 1By definition of the Gaussian width, we haveD.2.2 Sparse vectors: Example 2The RIP property of order 8s implies thatfor all vectors with \u2206 0 \u2264 8s, a fact which we use throughout the proof. By definition of the Gaussian width, we haveSince x * \u2208 B 0 (s), it can be shown (e.g., see the proof of Corollary 3 in the paper[31]) that for any vector | g, A\u2206 | ,\nApproximate nearest neighbors and the fast johnsonlindenstrauss transform. N Ailon, B Chazelle, Proceedings of the thirty-eighth annual ACM symposium on Theory of computing. the thirty-eighth annual ACM symposium on Theory of computingACMN. Ailon and B. Chazelle. Approximate nearest neighbors and the fast johnson- lindenstrauss transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pages 557-563. ACM, 2006.\n\nUncovering shared structures in multiclass classification. Y Amit, M Fink, N Srebro, S Ullman, Proceedings of the 24th International Conference on Machine Learning, ICML '07. the 24th International Conference on Machine Learning, ICML '07New York, NY, USAACMY. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classification. In Proceedings of the 24th International Conference on Machine Learning, ICML '07, pages 17-24, New York, NY, USA, 2007. ACM.\n\nConvex multi-task feature learning. A Argyriou, T Evgeniou, M Pontil, Machine Learning. 73A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243-272, 2008.\n\nBlendenpik: Supercharging lapack's leastsquares solver. H Avron, P Maymounkov, S Toledo, SIAM Journal on Scientific Computing. 323H. Avron, P. Maymounkov, and S. Toledo. Blendenpik: Supercharging lapack's least- squares solver. SIAM Journal on Scientific Computing, 32(3):1217-1236, 2010.\n\nConsistency of trace norm minimization. F Bach, Journal of Machine Learning Research. 9F. Bach. Consistency of trace norm minimization. Journal of Machine Learning Research, 9:1019-1048, June 2008.\n\nLocal Rademacher complexities. P L Bartlett, O Bousquet, S Mendelson, Annals of Statistics. 334P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of Statistics, 33(4):1497-1537, 2005.\n\nEstimating a density under order restrictions: Non-asymptotic minimax risk. L Birg\u00e9, Annals of Statistics. 153L. Birg\u00e9. Estimating a density under order restrictions: Non-asymptotic minimax risk. Annals of Statistics, 15(3):995-1012, March 1987.\n\nRandom projections for the nonnegative least-squares problem. C Boutsidis, P Drineas, Linear Algebra and its Applications. 4315-7C. Boutsidis and P. Drineas. Random projections for the nonnegative least-squares prob- lem. Linear Algebra and its Applications, 431(5-7):760-771, 2009.\n\nOptimal selection of reduced rank estimators of high-dimensional matrices. F Bunea, Y She, M Wegkamp, Annals of Statistics. 392F. Bunea, Y. She, and M. Wegkamp. Optimal selection of reduced rank estimators of high-dimensional matrices. Annals of Statistics, 39(2):1282-1309, 2011.\n\nDecoding by linear programming. E J Candes, T Tao, IEEE Trans. Info Theory. 5112E. J. Candes and T. Tao. Decoding by linear programming. IEEE Trans. Info Theory, 51(12):4203-4215, December 2005.\n\nAtomic decomposition by basis pursuit. S Chen, D L Donoho, M A Saunders, SIAM J. Sci. Computing. 201S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM J. Sci. Computing, 20(1):33-61, 1998.\n\nThe fast cauchy transform and faster robust linear regression. K L Clarkson, P Drineas, M Magdon-Ismail, M W Majoney, X Meng, D P Woodruff, Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms. the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete AlgorithmsSIAMK. L. Clarkson, P. Drineas, M. Magdon-Ismail, M. W. Majoney, X. Meng, and D. P. Woodruff. The fast cauchy transform and faster robust linear regression. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 466-477. SIAM, 2013.\n\nElements of Information Theory. T M Cover, J A Thomas, John Wiley and SonsNew YorkT.M. Cover and J.A. Thomas. Elements of Information Theory. John Wiley and Sons, New York, 1991.\n\nFast approximation of matrix coherence and statistical leverage. P Drineas, M Magdon-Ismail, M W Mahoney, D P Woodruff, Journal of Machine Learning Research. 131P. Drineas, M. Magdon-Ismail, M. W. Mahoney, and D. P. Woodruff. Fast approximation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13(1):3475-3506, 2012.\n\nEffective resistances, statistical leverage, and applications to linear equation solving. P Drineas, M W Mahoney, arXiv:1005.3097arXiv preprintP. Drineas and M. W. Mahoney. Effective resistances, statistical leverage, and applica- tions to linear equation solving. arXiv preprint arXiv:1005.3097, 2010.\n\nFaster least squares approximation. P Drineas, M W Mahoney, S Muthukrishnan, T Sarlos, Numer. Math. 1172P. Drineas, M. W. Mahoney, S. Muthukrishnan, and T. Sarlos. Faster least squares approximation. Numer. Math, 117(2):219-249, 2011.\n\nSafe feature elimination for the lasso. Laurent El Ghaoui, Vivian Viallon, Tarek Rabbani, SubmittedLaurent El Ghaoui, Vivian Viallon, and Tarek Rabbani. Safe feature elimination for the lasso. Submitted, April, 2011.\n\nMatrix Rank Minimization with Applications. M Fazel, StanfordPhD thesisM. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford, 2002. Available online: http://faculty.washington.edu/mfazel/thesis-final.pdf.\n\nGaussian averages of interpolated bodies and applications to approximate reconstruction. Y Gordon, A E Litvak, S Mendelson, A Pajor, Journal of Approximation Theory. 149Y. Gordon, A. E. Litvak, S. Mendelson, and A. Pajor. Gaussian averages of interpo- lated bodies and applications to approximate reconstruction. Journal of Approximation Theory, 149:59-73, 2007.\n\nRobust regression: Asymptotics, conjectures and Monte Carlo. P Huber, Annals of Statistics. 1P. Huber. Robust regression: Asymptotics, conjectures and Monte Carlo. Annals of Statistics, 1:799-821, 2001.\n\nSparser Johnson-Lindenstrauss transforms. D M Kane, J Nelson, Journal of the ACM. 611D. M. Kane and J. Nelson. Sparser Johnson-Lindenstrauss transforms. Journal of the ACM, 61(1), 2014.\n\nM Ledoux, M Talagrand, Probability in Banach Spaces: Isoperimetry and Processes. New York, NYSpringer-VerlagM. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer-Verlag, New York, NY, 1991.\n\nHigh-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. P Loh, M J Wainwright, Annals of Statistics. 403P. Loh and M. J. Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Annals of Statistics, 40(3):1637-1664, Septem- ber 2012.\n\nCoding facial expressions with gabor wavelets. M J Lyons, S Akamatsu, M Kamachi, J Gyoba, Proc. Int'l Conf. Automatic Face and Gesture Recognition. Int'l Conf. Automatic Face and Gesture RecognitionM.J. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba. Coding facial expressions with gabor wavelets. In Proc. Int'l Conf. Automatic Face and Gesture Recognition, pages 200-205, 1998.\n\nRandomized algorithms for matrices and data. Foundations and Trends in Machine Learning in Machine Learning. M W Mahoney, 3M. W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends in Machine Learning in Machine Learning, 3(2), 2011.\n\nPortfolio Selection. H M Markowitz, WileyNew YorkH. M. Markowitz. Portfolio Selection. Wiley, New York, 1959.\n\nEstimation of (near) low-rank matrices with noise and high-dimensional scaling. S Negahban, M J Wainwright, Annals of Statistics. 392S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. Annals of Statistics, 39(2):1069-1097, 2011.\n\nRestricted strong convexity and (weighted) matrix completion: Optimal bounds with noise. S Negahban, M J Wainwright, Journal of Machine Learning Research. 13S. Negahban and M. J. Wainwright. Restricted strong convexity and (weighted) matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 13:1665- 1697, May 2012.\n\nBlur insensitive texture classification using local phase quantization. V Ojansivu, J Heikkil, Proc. Image and Signal Processing. Image and Signal essingV. Ojansivu and J. Heikkil. Blur insensitive texture classification using local phase quantization. In Proc. Image and Signal Processing (ICISP 2008), pages 236-243, 2008.\n\nRecovery of sparse probability measures via convex programming. M Pilanci, L El Ghaoui, V Chandrasekaran, Advances in Neural Information Processing Systems. M. Pilanci, L. El Ghaoui, and V. Chandrasekaran. Recovery of sparse probability mea- sures via convex programming. In Advances in Neural Information Processing Systems, pages 2420-2428, 2012.\n\nRandomized sketches of convex programs with sharp guarantees. M Pilanci, M J Wainwright, arXiv:1404.7203UC BerkeleyTechnical reportFull length version at. Presented in part at ISITM. Pilanci and M. J. Wainwright. Randomized sketches of convex programs with sharp guarantees. Technical report, UC Berkeley, 2014. Full length version at arXiv:1404.7203; Presented in part at ISIT 2014.\n\nMinimax rates of estimation for high-dimensional linear regression over q -balls. G Raskutti, M J Wainwright, B Yu, IEEE Trans. Information Theory. 5710G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear regression over q -balls. IEEE Trans. Information Theory, 57(10):6976-6994, October 2011.\n\nGuaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. B Recht, M Fazel, P Parrilo, SIAM Review. 523B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471-501, 2010.\n\nA fast randomized algorithm for overdetermined linear leastsquares regression. V Rokhlin, M Tygert, Proceedings of the National Academy of Sciences. 10536V. Rokhlin and M. Tygert. A fast randomized algorithm for overdetermined linear least- squares regression. Proceedings of the National Academy of Sciences, 105(36):13212- 13217, 2008.\n\nImproved approximation algorithms for large matrices via random projections. T Sarlos, Foundations of Computer Science, 2006. FOCS'06. 47th Annual IEEE Symposium on. IEEET. Sarlos. Improved approximation algorithms for large matrices via random projections. In Foundations of Computer Science, 2006. FOCS'06. 47th Annual IEEE Symposium on, pages 143-152. IEEE, 2006.\n\nGeneralization error bounds for collaborative prediction with low-rank matrices. N Srebro, N Alon, T S Jaakkola, Neural Information Processing Systems (NIPS). Vancouver, CanadaN. Srebro, N. Alon, and T. S. Jaakkola. Generalization error bounds for collaborative prediction with low-rank matrices. In Neural Information Processing Systems (NIPS), Vancouver, Canada, December 2005.\n\nRegression shrinkage and selection via the Lasso. R Tibshirani, Journal of the Royal Statistical Society, Series B. 581R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267-288, 1996.\n\nThe Random Projection Method. Discrete Mathematics and Theoretical Computer Science. S Vempala, American Mathematical SocietyProvidence, RIS. Vempala. The Random Projection Method. Discrete Mathematics and Theoretical Computer Science. American Mathematical Society, Providence, RI, 2004.\n\nIntroduction to the non-asymptotic analysis of random matrices. Compressed Sensing: Theory and Applications. R Vershynin, R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Com- pressed Sensing: Theory and Applications, 2012.\n\nSharp thresholds for high-dimensional and noisy sparsity recovery using 1 -constrained quadratic programming (Lasso). M J Wainwright, IEEE Trans. Information Theory. 55M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using 1 -constrained quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183-2202, May 2009.\n\n. B Yu, Fano Assouad, Le Cam, Festschrift for Lucien Le CamSpringer-VerlagBerlinB. Yu. Assouad, Fano and Le Cam. In Festschrift for Lucien Le Cam, pages 423-435. Springer-Verlag, Berlin, 1997.\n\nDimension reduction and coefficient estimation in multivariate linear regression. M Yuan, A Ekici, Z Lu, R Monteiro, Journal Of The Royal Statistical Society Series B. 693M. Yuan, A. Ekici, Z. Lu, and R. Monteiro. Dimension reduction and coefficient estima- tion in multivariate linear regression. Journal Of The Royal Statistical Society Series B, 69(3):329-346, 2007.\n\nModel selection and estimation in regression with grouped variables. M Yuan, Y Lin, Journal of the Royal Statistical Society B. 16849M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society B, 1(68):49, 2006.\n", "annotations": {"author": "[{\"end\":189,\"start\":116},{\"end\":319,\"start\":190},{\"end\":355,\"start\":320}]", "publisher": null, "author_last_name": "[{\"end\":128,\"start\":121},{\"end\":209,\"start\":199}]", "author_first_name": "[{\"end\":120,\"start\":116},{\"end\":196,\"start\":190},{\"end\":198,\"start\":197}]", "author_affiliation": "[{\"end\":188,\"start\":130},{\"end\":291,\"start\":233},{\"end\":318,\"start\":293},{\"end\":354,\"start\":321}]", "title": "[{\"end\":97,\"start\":1},{\"end\":452,\"start\":356}]", "venue": null, "abstract": "[{\"end\":1772,\"start\":470}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2794,\"start\":2790},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2796,\"start\":2794},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2799,\"start\":2796},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2802,\"start\":2799},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2805,\"start\":2802},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3764,\"start\":3761},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3767,\"start\":3764},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3770,\"start\":3767},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3773,\"start\":3770},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3837,\"start\":3834},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4124,\"start\":4121},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4546,\"start\":4542},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4563,\"start\":4559},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4715,\"start\":4711},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4718,\"start\":4715},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4805,\"start\":4802},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4902,\"start\":4899},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4934,\"start\":4930},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5082,\"start\":5079},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5584,\"start\":5581},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5825,\"start\":5821},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9113,\"start\":9110},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13374,\"start\":13371},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14038,\"start\":14034},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19805,\"start\":19801},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19910,\"start\":19906},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21414,\"start\":21410},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21417,\"start\":21414},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21420,\"start\":21417},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21422,\"start\":21420},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24545,\"start\":24541},{\"end\":25013,\"start\":25010},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25787,\"start\":25783},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26036,\"start\":26032},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26468,\"start\":26464},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28667,\"start\":28663},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32379,\"start\":32375},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33027,\"start\":33023},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33029,\"start\":33027},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33952,\"start\":33948},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34475,\"start\":34471},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35335,\"start\":35331},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":35510,\"start\":35506},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39121,\"start\":39117},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":39124,\"start\":39121},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":41654,\"start\":41650},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41657,\"start\":41654},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":42782,\"start\":42778},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":42785,\"start\":42782},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":42787,\"start\":42785},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":42790,\"start\":42787},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":42793,\"start\":42790},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43397,\"start\":43393},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":45143,\"start\":45140},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45145,\"start\":45143},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":45264,\"start\":45260},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":45904,\"start\":45900},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":49780,\"start\":49776},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":49798,\"start\":49794},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":52802,\"start\":52799},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":52805,\"start\":52802},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":53153,\"start\":53149},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":54048,\"start\":54044},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":57440,\"start\":57437},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":60501,\"start\":60498}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":60772,\"start\":60711},{\"attributes\":{\"id\":\"fig_2\"},\"end\":61299,\"start\":60773},{\"attributes\":{\"id\":\"fig_3\"},\"end\":61992,\"start\":61300},{\"attributes\":{\"id\":\"fig_5\"},\"end\":62780,\"start\":61993},{\"attributes\":{\"id\":\"fig_6\"},\"end\":63033,\"start\":62781},{\"attributes\":{\"id\":\"fig_8\"},\"end\":63924,\"start\":63034},{\"attributes\":{\"id\":\"fig_10\"},\"end\":64741,\"start\":63925},{\"attributes\":{\"id\":\"fig_11\"},\"end\":64938,\"start\":64742},{\"attributes\":{\"id\":\"fig_12\"},\"end\":65166,\"start\":64939},{\"attributes\":{\"id\":\"fig_13\"},\"end\":66278,\"start\":65167},{\"attributes\":{\"id\":\"fig_14\"},\"end\":66340,\"start\":66279}]", "paragraph": "[{\"end\":2305,\"start\":1788},{\"end\":2598,\"start\":2307},{\"end\":3313,\"start\":2600},{\"end\":3912,\"start\":3355},{\"end\":4241,\"start\":3914},{\"end\":5083,\"start\":4287},{\"end\":5441,\"start\":5085},{\"end\":5457,\"start\":5443},{\"end\":5973,\"start\":5485},{\"end\":6862,\"start\":6003},{\"end\":7762,\"start\":6886},{\"end\":8859,\"start\":7764},{\"end\":9138,\"start\":8861},{\"end\":9320,\"start\":9209},{\"end\":9702,\"start\":9322},{\"end\":10038,\"start\":9704},{\"end\":10821,\"start\":10055},{\"end\":11147,\"start\":10864},{\"end\":11471,\"start\":11167},{\"end\":12301,\"start\":11518},{\"end\":12519,\"start\":12303},{\"end\":12967,\"start\":12521},{\"end\":13376,\"start\":13029},{\"end\":14162,\"start\":13378},{\"end\":14201,\"start\":14164},{\"end\":14328,\"start\":14203},{\"end\":14606,\"start\":14381},{\"end\":15311,\"start\":14651},{\"end\":15644,\"start\":15340},{\"end\":15726,\"start\":15646},{\"end\":16360,\"start\":15769},{\"end\":16409,\"start\":16362},{\"end\":16560,\"start\":16411},{\"end\":16704,\"start\":16637},{\"end\":16911,\"start\":16706},{\"end\":17199,\"start\":16913},{\"end\":17443,\"start\":17233},{\"end\":17972,\"start\":17486},{\"end\":18556,\"start\":17974},{\"end\":18927,\"start\":18558},{\"end\":19036,\"start\":18970},{\"end\":19182,\"start\":19038},{\"end\":19334,\"start\":19184},{\"end\":19604,\"start\":19380},{\"end\":19953,\"start\":19606},{\"end\":20317,\"start\":19996},{\"end\":20983,\"start\":20371},{\"end\":21634,\"start\":21002},{\"end\":21836,\"start\":21674},{\"end\":21931,\"start\":21856},{\"end\":22342,\"start\":22014},{\"end\":22588,\"start\":22389},{\"end\":22955,\"start\":22643},{\"end\":23469,\"start\":22990},{\"end\":23484,\"start\":23471},{\"end\":23913,\"start\":23526},{\"end\":24124,\"start\":23915},{\"end\":24326,\"start\":24192},{\"end\":24654,\"start\":24438},{\"end\":24805,\"start\":24656},{\"end\":25042,\"start\":24840},{\"end\":25140,\"start\":25087},{\"end\":25715,\"start\":25191},{\"end\":25831,\"start\":25717},{\"end\":25979,\"start\":25884},{\"end\":26038,\"start\":25981},{\"end\":26105,\"start\":26040},{\"end\":26297,\"start\":26169},{\"end\":26728,\"start\":26371},{\"end\":27220,\"start\":26746},{\"end\":28301,\"start\":27249},{\"end\":28433,\"start\":28303},{\"end\":28914,\"start\":28492},{\"end\":28990,\"start\":28916},{\"end\":29056,\"start\":28992},{\"end\":29084,\"start\":29058},{\"end\":29209,\"start\":29086},{\"end\":29317,\"start\":29284},{\"end\":29627,\"start\":29319},{\"end\":29760,\"start\":29708},{\"end\":29861,\"start\":29762},{\"end\":30002,\"start\":29917},{\"end\":30549,\"start\":30034},{\"end\":31495,\"start\":30551},{\"end\":31580,\"start\":31497},{\"end\":31716,\"start\":31598},{\"end\":31768,\"start\":31743},{\"end\":32701,\"start\":31809},{\"end\":33327,\"start\":32703},{\"end\":33991,\"start\":33366},{\"end\":34297,\"start\":33993},{\"end\":34639,\"start\":34299},{\"end\":35512,\"start\":34641},{\"end\":35868,\"start\":35549},{\"end\":36121,\"start\":35900},{\"end\":36200,\"start\":36123},{\"end\":36284,\"start\":36202},{\"end\":36392,\"start\":36286},{\"end\":36487,\"start\":36394},{\"end\":37094,\"start\":36489},{\"end\":37345,\"start\":37126},{\"end\":37430,\"start\":37347},{\"end\":37557,\"start\":37465},{\"end\":37689,\"start\":37626},{\"end\":38392,\"start\":37691},{\"end\":38568,\"start\":38455},{\"end\":38975,\"start\":38593},{\"end\":39168,\"start\":39000},{\"end\":39184,\"start\":39170},{\"end\":40154,\"start\":39213},{\"end\":40781,\"start\":40193},{\"end\":41065,\"start\":40783},{\"end\":41191,\"start\":41130},{\"end\":42453,\"start\":41193},{\"end\":42845,\"start\":42505},{\"end\":42995,\"start\":42928},{\"end\":43537,\"start\":43014},{\"end\":43793,\"start\":43588},{\"end\":44383,\"start\":43847},{\"end\":46544,\"start\":44422},{\"end\":47854,\"start\":46546},{\"end\":48734,\"start\":47869},{\"end\":49580,\"start\":48736},{\"end\":50141,\"start\":49623},{\"end\":50205,\"start\":50143},{\"end\":50359,\"start\":50233},{\"end\":50459,\"start\":50397},{\"end\":50809,\"start\":50461},{\"end\":50910,\"start\":50866},{\"end\":51164,\"start\":50928},{\"end\":51469,\"start\":51166},{\"end\":51711,\"start\":51513},{\"end\":52003,\"start\":51793},{\"end\":52198,\"start\":52030},{\"end\":52898,\"start\":52246},{\"end\":53205,\"start\":52960},{\"end\":53426,\"start\":53284},{\"end\":53592,\"start\":53428},{\"end\":53731,\"start\":53676},{\"end\":53852,\"start\":53818},{\"end\":54072,\"start\":53932},{\"end\":54236,\"start\":54135},{\"end\":54381,\"start\":54275},{\"end\":54504,\"start\":54419},{\"end\":54627,\"start\":54553},{\"end\":54832,\"start\":54706},{\"end\":54929,\"start\":54857},{\"end\":55321,\"start\":54992},{\"end\":55509,\"start\":55364},{\"end\":55614,\"start\":55548},{\"end\":55754,\"start\":55666},{\"end\":56001,\"start\":55806},{\"end\":56109,\"start\":56078},{\"end\":56241,\"start\":56150},{\"end\":56504,\"start\":56289},{\"end\":56618,\"start\":56531},{\"end\":56692,\"start\":56649},{\"end\":56856,\"start\":56694},{\"end\":57094,\"start\":56858},{\"end\":57310,\"start\":57142},{\"end\":57523,\"start\":57335},{\"end\":57630,\"start\":57525},{\"end\":57755,\"start\":57632},{\"end\":57934,\"start\":57786},{\"end\":58238,\"start\":57993},{\"end\":58361,\"start\":58291},{\"end\":58640,\"start\":58410},{\"end\":58702,\"start\":58642},{\"end\":58843,\"start\":58704},{\"end\":59047,\"start\":58884},{\"end\":59138,\"start\":59086},{\"end\":59277,\"start\":59199},{\"end\":59372,\"start\":59279},{\"end\":59563,\"start\":59421},{\"end\":59892,\"start\":59604},{\"end\":60236,\"start\":59894},{\"end\":60526,\"start\":60288},{\"end\":60710,\"start\":60570}]", "formula": "[{\"attributes\":{\"id\":\"formula_1\"},\"end\":3354,\"start\":3314},{\"attributes\":{\"id\":\"formula_2\"},\"end\":4286,\"start\":4242},{\"attributes\":{\"id\":\"formula_3\"},\"end\":5484,\"start\":5458},{\"attributes\":{\"id\":\"formula_4\"},\"end\":6002,\"start\":5974},{\"attributes\":{\"id\":\"formula_5\"},\"end\":6885,\"start\":6863},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9208,\"start\":9139},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11166,\"start\":11148},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11517,\"start\":11472},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13028,\"start\":12968},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14650,\"start\":14607},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15339,\"start\":15312},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15768,\"start\":15727},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16636,\"start\":16561},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17232,\"start\":17200},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17485,\"start\":17444},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18969,\"start\":18928},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19379,\"start\":19335},{\"attributes\":{\"id\":\"formula_19\"},\"end\":19995,\"start\":19954},{\"attributes\":{\"id\":\"formula_20\"},\"end\":20370,\"start\":20318},{\"attributes\":{\"id\":\"formula_21\"},\"end\":21001,\"start\":20984},{\"attributes\":{\"id\":\"formula_22\"},\"end\":21673,\"start\":21635},{\"attributes\":{\"id\":\"formula_23\"},\"end\":21855,\"start\":21837},{\"attributes\":{\"id\":\"formula_24\"},\"end\":22013,\"start\":21932},{\"attributes\":{\"id\":\"formula_25\"},\"end\":22388,\"start\":22343},{\"attributes\":{\"id\":\"formula_26\"},\"end\":22642,\"start\":22589},{\"attributes\":{\"id\":\"formula_27\"},\"end\":23525,\"start\":23485},{\"attributes\":{\"id\":\"formula_28\"},\"end\":24191,\"start\":24125},{\"attributes\":{\"id\":\"formula_29\"},\"end\":24437,\"start\":24327},{\"attributes\":{\"id\":\"formula_30\"},\"end\":24839,\"start\":24806},{\"attributes\":{\"id\":\"formula_31\"},\"end\":25086,\"start\":25043},{\"attributes\":{\"id\":\"formula_32\"},\"end\":25190,\"start\":25141},{\"attributes\":{\"id\":\"formula_33\"},\"end\":25883,\"start\":25832},{\"attributes\":{\"id\":\"formula_34\"},\"end\":26168,\"start\":26106},{\"attributes\":{\"id\":\"formula_35\"},\"end\":26370,\"start\":26298},{\"attributes\":{\"id\":\"formula_36\"},\"end\":26745,\"start\":26729},{\"attributes\":{\"id\":\"formula_37\"},\"end\":28491,\"start\":28434},{\"attributes\":{\"id\":\"formula_38\"},\"end\":29283,\"start\":29210},{\"attributes\":{\"id\":\"formula_39\"},\"end\":29707,\"start\":29628},{\"attributes\":{\"id\":\"formula_40\"},\"end\":29916,\"start\":29862},{\"attributes\":{\"id\":\"formula_41\"},\"end\":30033,\"start\":30003},{\"attributes\":{\"id\":\"formula_42\"},\"end\":31597,\"start\":31581},{\"attributes\":{\"id\":\"formula_43\"},\"end\":31742,\"start\":31717},{\"attributes\":{\"id\":\"formula_44\"},\"end\":31808,\"start\":31769},{\"attributes\":{\"id\":\"formula_45\"},\"end\":37125,\"start\":37095},{\"attributes\":{\"id\":\"formula_46\"},\"end\":37464,\"start\":37431},{\"attributes\":{\"id\":\"formula_47\"},\"end\":37625,\"start\":37558},{\"attributes\":{\"id\":\"formula_48\"},\"end\":38454,\"start\":38393},{\"attributes\":{\"id\":\"formula_49\"},\"end\":38592,\"start\":38569},{\"attributes\":{\"id\":\"formula_50\"},\"end\":39212,\"start\":39185},{\"attributes\":{\"id\":\"formula_51\"},\"end\":40192,\"start\":40155},{\"attributes\":{\"id\":\"formula_52\"},\"end\":41129,\"start\":41066},{\"attributes\":{\"id\":\"formula_53\"},\"end\":42927,\"start\":42846},{\"attributes\":{\"id\":\"formula_54\"},\"end\":43587,\"start\":43538},{\"attributes\":{\"id\":\"formula_55\"},\"end\":43846,\"start\":43794},{\"attributes\":{\"id\":\"formula_56\"},\"end\":49622,\"start\":49581},{\"attributes\":{\"id\":\"formula_57\"},\"end\":50865,\"start\":50810},{\"attributes\":{\"id\":\"formula_58\"},\"end\":51512,\"start\":51470},{\"attributes\":{\"id\":\"formula_59\"},\"end\":51792,\"start\":51712},{\"attributes\":{\"id\":\"formula_60\"},\"end\":52245,\"start\":52199},{\"attributes\":{\"id\":\"formula_61\"},\"end\":52959,\"start\":52899},{\"attributes\":{\"id\":\"formula_62\"},\"end\":53283,\"start\":53206},{\"attributes\":{\"id\":\"formula_63\"},\"end\":53675,\"start\":53593},{\"attributes\":{\"id\":\"formula_64\"},\"end\":53817,\"start\":53732},{\"attributes\":{\"id\":\"formula_65\"},\"end\":53931,\"start\":53853},{\"attributes\":{\"id\":\"formula_66\"},\"end\":54107,\"start\":54073},{\"attributes\":{\"id\":\"formula_67\"},\"end\":54274,\"start\":54237},{\"attributes\":{\"id\":\"formula_68\"},\"end\":54418,\"start\":54382},{\"attributes\":{\"id\":\"formula_69\"},\"end\":54552,\"start\":54505},{\"attributes\":{\"id\":\"formula_70\"},\"end\":54705,\"start\":54628},{\"attributes\":{\"id\":\"formula_71\"},\"end\":54991,\"start\":54930},{\"attributes\":{\"id\":\"formula_72\"},\"end\":55363,\"start\":55322},{\"attributes\":{\"id\":\"formula_73\"},\"end\":55547,\"start\":55510},{\"attributes\":{\"id\":\"formula_74\"},\"end\":55665,\"start\":55615},{\"attributes\":{\"id\":\"formula_75\"},\"end\":55805,\"start\":55755},{\"attributes\":{\"id\":\"formula_76\"},\"end\":56077,\"start\":56002},{\"attributes\":{\"id\":\"formula_77\"},\"end\":56149,\"start\":56110},{\"attributes\":{\"id\":\"formula_78\"},\"end\":56648,\"start\":56619},{\"attributes\":{\"id\":\"formula_80\"},\"end\":57141,\"start\":57095},{\"attributes\":{\"id\":\"formula_81\"},\"end\":57334,\"start\":57311},{\"attributes\":{\"id\":\"formula_82\"},\"end\":57992,\"start\":57935},{\"attributes\":{\"id\":\"formula_83\"},\"end\":58290,\"start\":58239},{\"attributes\":{\"id\":\"formula_84\"},\"end\":58409,\"start\":58362},{\"attributes\":{\"id\":\"formula_85\"},\"end\":58883,\"start\":58844},{\"attributes\":{\"id\":\"formula_86\"},\"end\":59085,\"start\":59048},{\"attributes\":{\"id\":\"formula_87\"},\"end\":59198,\"start\":59139},{\"attributes\":{\"id\":\"formula_88\"},\"end\":59420,\"start\":59373},{\"attributes\":{\"id\":\"formula_89\"},\"end\":59603,\"start\":59564},{\"attributes\":{\"id\":\"formula_90\"},\"end\":60287,\"start\":60237},{\"attributes\":{\"id\":\"formula_91\"},\"end\":60569,\"start\":60527}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1786,\"start\":1774},{\"attributes\":{\"n\":\"2\"},\"end\":10053,\"start\":10041},{\"attributes\":{\"n\":\"2.1\"},\"end\":10862,\"start\":10824},{\"attributes\":{\"n\":\"2.2\"},\"end\":14379,\"start\":14331},{\"attributes\":{\"n\":\"2.3\"},\"end\":22988,\"start\":22958},{\"attributes\":{\"n\":\"2.4\"},\"end\":27247,\"start\":27223},{\"attributes\":{\"n\":\"2.5\"},\"end\":33364,\"start\":33330},{\"attributes\":{\"n\":\"3\"},\"end\":35547,\"start\":35515},{\"attributes\":{\"n\":\"3.1\"},\"end\":35898,\"start\":35871},{\"attributes\":{\"n\":\"3.2\"},\"end\":38998,\"start\":38978},{\"attributes\":{\"n\":\"3.3\"},\"end\":42503,\"start\":42456},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":43012,\"start\":42998},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":44420,\"start\":44386},{\"attributes\":{\"n\":\"4\"},\"end\":47867,\"start\":47857},{\"end\":50231,\"start\":50208},{\"end\":50395,\"start\":50362},{\"end\":50926,\"start\":50913},{\"end\":52028,\"start\":52006},{\"end\":54133,\"start\":54109},{\"end\":54855,\"start\":54835},{\"end\":56287,\"start\":56244},{\"end\":56529,\"start\":56507},{\"end\":57784,\"start\":57758},{\"end\":60713,\"start\":60712},{\"end\":60784,\"start\":60774},{\"end\":61311,\"start\":61301},{\"end\":62004,\"start\":61994},{\"end\":62795,\"start\":62782},{\"end\":63045,\"start\":63035},{\"end\":63936,\"start\":63926},{\"end\":64756,\"start\":64743},{\"end\":64950,\"start\":64940},{\"end\":65178,\"start\":65168},{\"end\":66281,\"start\":66280}]", "table": null, "figure_caption": "[{\"end\":60772,\"start\":60714},{\"end\":61299,\"start\":60786},{\"end\":61992,\"start\":61313},{\"end\":62780,\"start\":62006},{\"end\":63033,\"start\":62797},{\"end\":63924,\"start\":63047},{\"end\":64741,\"start\":63938},{\"end\":64938,\"start\":64758},{\"end\":65166,\"start\":64952},{\"end\":66278,\"start\":65180},{\"end\":66340,\"start\":66282}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7637,\"start\":7629},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18252,\"start\":18244},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31005,\"start\":30997},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36956,\"start\":36948},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38173,\"start\":38165},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38678,\"start\":38670},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":39729,\"start\":39721},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":40680,\"start\":40672},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":41190,\"start\":41182},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":41279,\"start\":41271},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":43756,\"start\":43747},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":44035,\"start\":44019},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":44069,\"start\":44061},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":44253,\"start\":44245},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":45419,\"start\":45411},{\"end\":45812,\"start\":45794},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":47342,\"start\":47334}]", "bib_author_first_name": "[{\"end\":67871,\"start\":67870},{\"end\":67880,\"start\":67879},{\"end\":68304,\"start\":68303},{\"end\":68312,\"start\":68311},{\"end\":68320,\"start\":68319},{\"end\":68330,\"start\":68329},{\"end\":68769,\"start\":68768},{\"end\":68781,\"start\":68780},{\"end\":68793,\"start\":68792},{\"end\":68996,\"start\":68995},{\"end\":69005,\"start\":69004},{\"end\":69019,\"start\":69018},{\"end\":69270,\"start\":69269},{\"end\":69460,\"start\":69459},{\"end\":69462,\"start\":69461},{\"end\":69474,\"start\":69473},{\"end\":69486,\"start\":69485},{\"end\":69724,\"start\":69723},{\"end\":69957,\"start\":69956},{\"end\":69970,\"start\":69969},{\"end\":70254,\"start\":70253},{\"end\":70263,\"start\":70262},{\"end\":70270,\"start\":70269},{\"end\":70493,\"start\":70492},{\"end\":70495,\"start\":70494},{\"end\":70505,\"start\":70504},{\"end\":70696,\"start\":70695},{\"end\":70704,\"start\":70703},{\"end\":70706,\"start\":70705},{\"end\":70716,\"start\":70715},{\"end\":70718,\"start\":70717},{\"end\":70946,\"start\":70945},{\"end\":70948,\"start\":70947},{\"end\":70960,\"start\":70959},{\"end\":70971,\"start\":70970},{\"end\":70988,\"start\":70987},{\"end\":70990,\"start\":70989},{\"end\":71001,\"start\":71000},{\"end\":71009,\"start\":71008},{\"end\":71011,\"start\":71010},{\"end\":71475,\"start\":71474},{\"end\":71477,\"start\":71476},{\"end\":71486,\"start\":71485},{\"end\":71488,\"start\":71487},{\"end\":71688,\"start\":71687},{\"end\":71699,\"start\":71698},{\"end\":71716,\"start\":71715},{\"end\":71718,\"start\":71717},{\"end\":71729,\"start\":71728},{\"end\":71731,\"start\":71730},{\"end\":72066,\"start\":72065},{\"end\":72077,\"start\":72076},{\"end\":72079,\"start\":72078},{\"end\":72316,\"start\":72315},{\"end\":72327,\"start\":72326},{\"end\":72329,\"start\":72328},{\"end\":72340,\"start\":72339},{\"end\":72357,\"start\":72356},{\"end\":72562,\"start\":72555},{\"end\":72565,\"start\":72563},{\"end\":72580,\"start\":72574},{\"end\":72595,\"start\":72590},{\"end\":72778,\"start\":72777},{\"end\":73050,\"start\":73049},{\"end\":73060,\"start\":73059},{\"end\":73062,\"start\":73061},{\"end\":73072,\"start\":73071},{\"end\":73085,\"start\":73084},{\"end\":73386,\"start\":73385},{\"end\":73571,\"start\":73570},{\"end\":73573,\"start\":73572},{\"end\":73581,\"start\":73580},{\"end\":73716,\"start\":73715},{\"end\":73726,\"start\":73725},{\"end\":74045,\"start\":74044},{\"end\":74052,\"start\":74051},{\"end\":74054,\"start\":74053},{\"end\":74324,\"start\":74323},{\"end\":74326,\"start\":74325},{\"end\":74335,\"start\":74334},{\"end\":74347,\"start\":74346},{\"end\":74358,\"start\":74357},{\"end\":74765,\"start\":74764},{\"end\":74767,\"start\":74766},{\"end\":74937,\"start\":74936},{\"end\":74939,\"start\":74938},{\"end\":75107,\"start\":75106},{\"end\":75119,\"start\":75118},{\"end\":75121,\"start\":75120},{\"end\":75409,\"start\":75408},{\"end\":75421,\"start\":75420},{\"end\":75423,\"start\":75422},{\"end\":75736,\"start\":75735},{\"end\":75748,\"start\":75747},{\"end\":76054,\"start\":76053},{\"end\":76065,\"start\":76064},{\"end\":76068,\"start\":76066},{\"end\":76078,\"start\":76077},{\"end\":76402,\"start\":76401},{\"end\":76413,\"start\":76412},{\"end\":76415,\"start\":76414},{\"end\":76807,\"start\":76806},{\"end\":76819,\"start\":76818},{\"end\":76821,\"start\":76820},{\"end\":76835,\"start\":76834},{\"end\":77158,\"start\":77157},{\"end\":77167,\"start\":77166},{\"end\":77176,\"start\":77175},{\"end\":77445,\"start\":77444},{\"end\":77456,\"start\":77455},{\"end\":77782,\"start\":77781},{\"end\":78154,\"start\":78153},{\"end\":78164,\"start\":78163},{\"end\":78172,\"start\":78171},{\"end\":78174,\"start\":78173},{\"end\":78504,\"start\":78503},{\"end\":78797,\"start\":78796},{\"end\":79111,\"start\":79110},{\"end\":79374,\"start\":79373},{\"end\":79376,\"start\":79375},{\"end\":79619,\"start\":79618},{\"end\":79628,\"start\":79624},{\"end\":79640,\"start\":79638},{\"end\":79893,\"start\":79892},{\"end\":79901,\"start\":79900},{\"end\":79910,\"start\":79909},{\"end\":79916,\"start\":79915},{\"end\":80251,\"start\":80250},{\"end\":80259,\"start\":80258}]", "bib_author_last_name": "[{\"end\":67877,\"start\":67872},{\"end\":67889,\"start\":67881},{\"end\":68309,\"start\":68305},{\"end\":68317,\"start\":68313},{\"end\":68327,\"start\":68321},{\"end\":68337,\"start\":68331},{\"end\":68778,\"start\":68770},{\"end\":68790,\"start\":68782},{\"end\":68800,\"start\":68794},{\"end\":69002,\"start\":68997},{\"end\":69016,\"start\":69006},{\"end\":69026,\"start\":69020},{\"end\":69275,\"start\":69271},{\"end\":69471,\"start\":69463},{\"end\":69483,\"start\":69475},{\"end\":69496,\"start\":69487},{\"end\":69730,\"start\":69725},{\"end\":69967,\"start\":69958},{\"end\":69978,\"start\":69971},{\"end\":70260,\"start\":70255},{\"end\":70267,\"start\":70264},{\"end\":70278,\"start\":70271},{\"end\":70502,\"start\":70496},{\"end\":70509,\"start\":70506},{\"end\":70701,\"start\":70697},{\"end\":70713,\"start\":70707},{\"end\":70727,\"start\":70719},{\"end\":70957,\"start\":70949},{\"end\":70968,\"start\":70961},{\"end\":70985,\"start\":70972},{\"end\":70998,\"start\":70991},{\"end\":71006,\"start\":71002},{\"end\":71020,\"start\":71012},{\"end\":71483,\"start\":71478},{\"end\":71495,\"start\":71489},{\"end\":71696,\"start\":71689},{\"end\":71713,\"start\":71700},{\"end\":71726,\"start\":71719},{\"end\":71740,\"start\":71732},{\"end\":72074,\"start\":72067},{\"end\":72087,\"start\":72080},{\"end\":72324,\"start\":72317},{\"end\":72337,\"start\":72330},{\"end\":72354,\"start\":72341},{\"end\":72364,\"start\":72358},{\"end\":72572,\"start\":72566},{\"end\":72588,\"start\":72581},{\"end\":72603,\"start\":72596},{\"end\":72784,\"start\":72779},{\"end\":73057,\"start\":73051},{\"end\":73069,\"start\":73063},{\"end\":73082,\"start\":73073},{\"end\":73091,\"start\":73086},{\"end\":73392,\"start\":73387},{\"end\":73578,\"start\":73574},{\"end\":73588,\"start\":73582},{\"end\":73723,\"start\":73717},{\"end\":73736,\"start\":73727},{\"end\":74049,\"start\":74046},{\"end\":74065,\"start\":74055},{\"end\":74332,\"start\":74327},{\"end\":74344,\"start\":74336},{\"end\":74355,\"start\":74348},{\"end\":74364,\"start\":74359},{\"end\":74775,\"start\":74768},{\"end\":74949,\"start\":74940},{\"end\":75116,\"start\":75108},{\"end\":75132,\"start\":75122},{\"end\":75418,\"start\":75410},{\"end\":75434,\"start\":75424},{\"end\":75745,\"start\":75737},{\"end\":75756,\"start\":75749},{\"end\":76062,\"start\":76055},{\"end\":76075,\"start\":76069},{\"end\":76093,\"start\":76079},{\"end\":76410,\"start\":76403},{\"end\":76426,\"start\":76416},{\"end\":76816,\"start\":76808},{\"end\":76832,\"start\":76822},{\"end\":76838,\"start\":76836},{\"end\":77164,\"start\":77159},{\"end\":77173,\"start\":77168},{\"end\":77184,\"start\":77177},{\"end\":77453,\"start\":77446},{\"end\":77463,\"start\":77457},{\"end\":77789,\"start\":77783},{\"end\":78161,\"start\":78155},{\"end\":78169,\"start\":78165},{\"end\":78183,\"start\":78175},{\"end\":78515,\"start\":78505},{\"end\":78805,\"start\":78798},{\"end\":79121,\"start\":79112},{\"end\":79387,\"start\":79377},{\"end\":79622,\"start\":79620},{\"end\":79636,\"start\":79629},{\"end\":79644,\"start\":79641},{\"end\":79898,\"start\":79894},{\"end\":79907,\"start\":79902},{\"end\":79913,\"start\":79911},{\"end\":79925,\"start\":79917},{\"end\":80256,\"start\":80252},{\"end\":80263,\"start\":80260}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":490517},\"end\":68242,\"start\":67795},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15645633},\"end\":68730,\"start\":68244},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6617228},\"end\":68937,\"start\":68732},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15880145},\"end\":69227,\"start\":68939},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3262509},\"end\":69426,\"start\":69229},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2259087},\"end\":69645,\"start\":69428},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":121326060},\"end\":69892,\"start\":69647},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6826978},\"end\":70176,\"start\":69894},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":88512064},\"end\":70458,\"start\":70178},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12605120},\"end\":70654,\"start\":70460},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2429822},\"end\":70880,\"start\":70656},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8450763},\"end\":71440,\"start\":70882},{\"attributes\":{\"id\":\"b12\"},\"end\":71620,\"start\":71442},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":129692},\"end\":71973,\"start\":71622},{\"attributes\":{\"doi\":\"arXiv:1005.3097\",\"id\":\"b14\"},\"end\":72277,\"start\":71975},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7730230},\"end\":72513,\"start\":72279},{\"attributes\":{\"id\":\"b16\"},\"end\":72731,\"start\":72515},{\"attributes\":{\"id\":\"b17\"},\"end\":72958,\"start\":72733},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1766603},\"end\":73322,\"start\":72960},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":123395408},\"end\":73526,\"start\":73324},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7821848},\"end\":73713,\"start\":73528},{\"attributes\":{\"id\":\"b21\"},\"end\":73945,\"start\":73715},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4530725},\"end\":74274,\"start\":73947},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1586662},\"end\":74653,\"start\":74276},{\"attributes\":{\"id\":\"b24\"},\"end\":74913,\"start\":74655},{\"attributes\":{\"id\":\"b25\"},\"end\":75024,\"start\":74915},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1004801},\"end\":75317,\"start\":75026},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2418897},\"end\":75661,\"start\":75319},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7242326},\"end\":75987,\"start\":75663},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16128730},\"end\":76337,\"start\":75989},{\"attributes\":{\"doi\":\"arXiv:1404.7203\",\"id\":\"b30\"},\"end\":76722,\"start\":76339},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2942626},\"end\":77063,\"start\":76724},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8249100},\"end\":77363,\"start\":77065},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8351640},\"end\":77702,\"start\":77365},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1299951},\"end\":78070,\"start\":77704},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":520370},\"end\":78451,\"start\":78072},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":16162039},\"end\":78709,\"start\":78453},{\"attributes\":{\"id\":\"b37\"},\"end\":78999,\"start\":78711},{\"attributes\":{\"id\":\"b38\"},\"end\":79253,\"start\":79001},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11046646},\"end\":79614,\"start\":79255},{\"attributes\":{\"id\":\"b40\"},\"end\":79808,\"start\":79616},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":123079500},\"end\":80179,\"start\":79810},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":6162124},\"end\":80462,\"start\":80181}]", "bib_title": "[{\"end\":67868,\"start\":67795},{\"end\":68301,\"start\":68244},{\"end\":68766,\"start\":68732},{\"end\":68993,\"start\":68939},{\"end\":69267,\"start\":69229},{\"end\":69457,\"start\":69428},{\"end\":69721,\"start\":69647},{\"end\":69954,\"start\":69894},{\"end\":70251,\"start\":70178},{\"end\":70490,\"start\":70460},{\"end\":70693,\"start\":70656},{\"end\":70943,\"start\":70882},{\"end\":71685,\"start\":71622},{\"end\":72313,\"start\":72279},{\"end\":73047,\"start\":72960},{\"end\":73383,\"start\":73324},{\"end\":73568,\"start\":73528},{\"end\":74042,\"start\":73947},{\"end\":74321,\"start\":74276},{\"end\":75104,\"start\":75026},{\"end\":75406,\"start\":75319},{\"end\":75733,\"start\":75663},{\"end\":76051,\"start\":75989},{\"end\":76804,\"start\":76724},{\"end\":77155,\"start\":77065},{\"end\":77442,\"start\":77365},{\"end\":77779,\"start\":77704},{\"end\":78151,\"start\":78072},{\"end\":78501,\"start\":78453},{\"end\":79371,\"start\":79255},{\"end\":79890,\"start\":79810},{\"end\":80248,\"start\":80181}]", "bib_author": "[{\"end\":67879,\"start\":67870},{\"end\":67891,\"start\":67879},{\"end\":68311,\"start\":68303},{\"end\":68319,\"start\":68311},{\"end\":68329,\"start\":68319},{\"end\":68339,\"start\":68329},{\"end\":68780,\"start\":68768},{\"end\":68792,\"start\":68780},{\"end\":68802,\"start\":68792},{\"end\":69004,\"start\":68995},{\"end\":69018,\"start\":69004},{\"end\":69028,\"start\":69018},{\"end\":69277,\"start\":69269},{\"end\":69473,\"start\":69459},{\"end\":69485,\"start\":69473},{\"end\":69498,\"start\":69485},{\"end\":69732,\"start\":69723},{\"end\":69969,\"start\":69956},{\"end\":69980,\"start\":69969},{\"end\":70262,\"start\":70253},{\"end\":70269,\"start\":70262},{\"end\":70280,\"start\":70269},{\"end\":70504,\"start\":70492},{\"end\":70511,\"start\":70504},{\"end\":70703,\"start\":70695},{\"end\":70715,\"start\":70703},{\"end\":70729,\"start\":70715},{\"end\":70959,\"start\":70945},{\"end\":70970,\"start\":70959},{\"end\":70987,\"start\":70970},{\"end\":71000,\"start\":70987},{\"end\":71008,\"start\":71000},{\"end\":71022,\"start\":71008},{\"end\":71485,\"start\":71474},{\"end\":71497,\"start\":71485},{\"end\":71698,\"start\":71687},{\"end\":71715,\"start\":71698},{\"end\":71728,\"start\":71715},{\"end\":71742,\"start\":71728},{\"end\":72076,\"start\":72065},{\"end\":72089,\"start\":72076},{\"end\":72326,\"start\":72315},{\"end\":72339,\"start\":72326},{\"end\":72356,\"start\":72339},{\"end\":72366,\"start\":72356},{\"end\":72574,\"start\":72555},{\"end\":72590,\"start\":72574},{\"end\":72605,\"start\":72590},{\"end\":72786,\"start\":72777},{\"end\":73059,\"start\":73049},{\"end\":73071,\"start\":73059},{\"end\":73084,\"start\":73071},{\"end\":73093,\"start\":73084},{\"end\":73394,\"start\":73385},{\"end\":73580,\"start\":73570},{\"end\":73590,\"start\":73580},{\"end\":73725,\"start\":73715},{\"end\":73738,\"start\":73725},{\"end\":74051,\"start\":74044},{\"end\":74067,\"start\":74051},{\"end\":74334,\"start\":74323},{\"end\":74346,\"start\":74334},{\"end\":74357,\"start\":74346},{\"end\":74366,\"start\":74357},{\"end\":74777,\"start\":74764},{\"end\":74951,\"start\":74936},{\"end\":75118,\"start\":75106},{\"end\":75134,\"start\":75118},{\"end\":75420,\"start\":75408},{\"end\":75436,\"start\":75420},{\"end\":75747,\"start\":75735},{\"end\":75758,\"start\":75747},{\"end\":76064,\"start\":76053},{\"end\":76077,\"start\":76064},{\"end\":76095,\"start\":76077},{\"end\":76412,\"start\":76401},{\"end\":76428,\"start\":76412},{\"end\":76818,\"start\":76806},{\"end\":76834,\"start\":76818},{\"end\":76840,\"start\":76834},{\"end\":77166,\"start\":77157},{\"end\":77175,\"start\":77166},{\"end\":77186,\"start\":77175},{\"end\":77455,\"start\":77444},{\"end\":77465,\"start\":77455},{\"end\":77791,\"start\":77781},{\"end\":78163,\"start\":78153},{\"end\":78171,\"start\":78163},{\"end\":78185,\"start\":78171},{\"end\":78517,\"start\":78503},{\"end\":78807,\"start\":78796},{\"end\":79123,\"start\":79110},{\"end\":79389,\"start\":79373},{\"end\":79624,\"start\":79618},{\"end\":79638,\"start\":79624},{\"end\":79646,\"start\":79638},{\"end\":79900,\"start\":79892},{\"end\":79909,\"start\":79900},{\"end\":79915,\"start\":79909},{\"end\":79927,\"start\":79915},{\"end\":80258,\"start\":80250},{\"end\":80265,\"start\":80258}]", "bib_venue": "[{\"end\":67967,\"start\":67891},{\"end\":68417,\"start\":68339},{\"end\":68818,\"start\":68802},{\"end\":69064,\"start\":69028},{\"end\":69313,\"start\":69277},{\"end\":69518,\"start\":69498},{\"end\":69752,\"start\":69732},{\"end\":70015,\"start\":69980},{\"end\":70300,\"start\":70280},{\"end\":70534,\"start\":70511},{\"end\":70751,\"start\":70729},{\"end\":71103,\"start\":71022},{\"end\":71472,\"start\":71442},{\"end\":71778,\"start\":71742},{\"end\":72063,\"start\":71975},{\"end\":72377,\"start\":72366},{\"end\":72553,\"start\":72515},{\"end\":72775,\"start\":72733},{\"end\":73124,\"start\":73093},{\"end\":73414,\"start\":73394},{\"end\":73608,\"start\":73590},{\"end\":73794,\"start\":73738},{\"end\":74087,\"start\":74067},{\"end\":74422,\"start\":74366},{\"end\":74762,\"start\":74655},{\"end\":74934,\"start\":74915},{\"end\":75154,\"start\":75134},{\"end\":75472,\"start\":75436},{\"end\":75791,\"start\":75758},{\"end\":76144,\"start\":76095},{\"end\":76399,\"start\":76339},{\"end\":76870,\"start\":76840},{\"end\":77197,\"start\":77186},{\"end\":77512,\"start\":77465},{\"end\":77868,\"start\":77791},{\"end\":78229,\"start\":78185},{\"end\":78567,\"start\":78517},{\"end\":78794,\"start\":78711},{\"end\":79108,\"start\":79001},{\"end\":79419,\"start\":79389},{\"end\":79976,\"start\":79927},{\"end\":80307,\"start\":80265},{\"end\":68030,\"start\":67969},{\"end\":68499,\"start\":68419},{\"end\":71171,\"start\":71105},{\"end\":73808,\"start\":73796},{\"end\":74474,\"start\":74424},{\"end\":75816,\"start\":75793},{\"end\":78248,\"start\":78231}]"}}}, "year": 2023, "month": 12, "day": 17}