{"id": 233448337, "updated": "2023-04-05 13:49:07.455", "metadata": {"title": "An Attention-Based Deep Learning Approach for Sleep Stage Classification With Single-Channel EEG", "authors": "[{\"first\":\"Emadeldeen\",\"last\":\"Eldele\",\"middle\":[]},{\"first\":\"Zhenghua\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Chengyu\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Min\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Chee-Keong\",\"last\":\"Kwoh\",\"middle\":[]},{\"first\":\"Xiaoli\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Cuntai\",\"last\":\"Guan\",\"middle\":[]}]", "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering", "journal": "IEEE Transactions on Neural Systems and Rehabilitation Engineering", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Automatic sleep stage mymargin classification is of great importance to measure sleep quality. In this paper, we propose a novel attention-based deep learning architecture called AttnSleep to classify sleep stages using single channel EEG signals. This architecture starts with the feature extraction module based on multi-resolution convolutional neural network (MRCNN) and adaptive feature recalibration (AFR). The MRCNN can extract low and high frequency features and the AFR is able to improve the quality of the extracted features by modeling the inter-dependencies between the features. The second module is the temporal context encoder (TCE) that leverages a multi-head attention mechanism to capture the temporal dependencies among the extracted features. Particularly, the multi-head attention deploys causal convolutions to model the temporal relations in the input features. We evaluate the performance of our proposed AttnSleep model using three public datasets. The results show that our AttnSleep outperforms state-of-the-art techniques in terms of different evaluation metrics. Our source codes, experimental data, and supplementary materials are available at https://github.com/emadeldeen24/AttnSleep.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "33909566", "pubmedcentral": null, "dblp": null, "doi": "10.1109/tnsre.2021.3076234"}}, "content": {"source": {"pdf_hash": "47ffe00dba4ae2381b69008242de94afe2030a51", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "c15368f8aa63c75e2f495afa30b663e95efd76cb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/47ffe00dba4ae2381b69008242de94afe2030a51.txt", "contents": "\nAn Attention-Based Deep Learning Approach for Sleep Stage Classification With Single-Channel EEG\n\n\nEmadeldeen Eldele \nSenior Member, IEEEZhenghua Chen \nSenior Member, IEEEChengyu Liu \nSenior Member, IEEEMin Wu \nChee-Keong Kwoh \nSenior Member, IEEEXiaoli Li \nFellow, IEEECuntai Guan \nAn Attention-Based Deep Learning Approach for Sleep Stage Classification With Single-Channel EEG\n\nIEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING\n29202110.1109/TNSRE.2021.3076234809\nAutomatic sleep stage classification is of great importance to measure sleep quality. In this paper, we propose a novel attention-based deep learning architecture called AttnSleep to classify sleep stages using single channel EEG signals. This architecture starts with the feature extraction module based on multi-resolution convolutional neural network (MRCNN) and adaptive feature recalibration (AFR). The MRCNN can extract low and high frequency features and the AFR is able to improve the quality of the extracted features by modeling the inter-dependencies between the features. The second module is the temporal context encoder (TCE) that leverages a multi-head attention mechanism to capture the temporal dependencies among the extracted features. Particularly, the multi-head attention deploys causal convolutions to model the temporal relations in the input features. We evaluate the performance of our proposed AttnSleep model using three public datasets. The results show that our AttnSleep outperforms state-of-the-art techniques in terms of different evaluation metrics. Our source codes, experimental data, and supplementary materials are available at https://github.com/emadeldeen24/AttnSleep. Index Terms-Sleep stage classification, multi-resolution convolutional neural network, adaptive feature recalibration, temporal context encoder, multi-head attention.\n\napnea [2]. In particular, sleep stages (e.g., light sleep and deep sleep) are important for immune system, memory, metabolism, etc. [3]- [5]. Therefore, it is highly desired to measure sleep quality through sleep monitoring and sleep stage classification.\n\nSleep specialists usually determine the sleep stages based on the polysomnography (PSG), which consists of electroencephalogram (EEG), electrooculogram (EOG), anelectromyogram (EMG) and electrocardiogram (ECG) [6]. Single-channel EEG has recently become attractive for sleep monitoring due to its ease-of-use. In particular, PSG or single-channel EEG recordings are usually divided into 30-second segments and each segment is manually checked by sleep specialists and then classified into one of the six stages, i.e., wake (W), rapid eye movement (REM) and four non-REM stages (N1, N2, N3 and N4) [7]. This manual process is very exhaustive, tedious, and time-consuming. As such, automatic sleep stage classification systems are required to assist sleep specialists.\n\nMany studies have adopted conventional machine learning methods to classify EEG signals into corresponding sleep stages. These methods usually consist of two steps, namely, manual feature extraction and sleep stage classification. First, they design and extract various features from time and frequency domains. Feature selection algorithms are often applied to further select the most discriminative features. Second, the selected features are then fed into conventional machine learning models for sleep stage classification, such as Naive Bayes [8], support vector machines (SVM) [9], [10], random forest (RF) [7], [11], or even ensemble learning based classifiers [12]. However, these methods require domain knowledge to extract the best representative features.\n\nRecently, deep learning has been employed in different areas and shown its superiority over conventional machine learning models without the need of domain knowledge. This motivates researchers to exploit deep learning techniques for automatic sleep stages classification. Several studies have designed convolutional neural networks (CNNs) [13]- [18] for this task. For example, successive convolution and pooling layers with fully-connected layers were used to perform this classification task in [13]. In [14], the authors used 12 convolution layers together with 2 fully connected layers. Additionally in [16], the authors used 2D convolution along with MaxPooling layers to classify the raw data from three channels (i.e., EEG, EOG, and EMG). In [15], the authors designed a relatively deep CNN architecture to show that a better performance can be achieved by relying on network depth. In [17], authors converted each raw signal into a log-power spectra and used a CNN to perform a joint classification and prediction task for identifying sleep stages. Generally, the CNN models above have achieved good performance for sleep stage classification. However, most of them are not able to effectively model the temporal dependencies among the EEG data.\n\nRecurrent Neural Networks (RNNs) were proposed to capture temporal dependencies in time-series EEG data. For example, [19] used a cascaded RNN architecture to classify sleep stages. Some researchers combined CNN with RNN by using CNN for features extraction and RNN for modelling the time dependencies [20]- [23]. For example, [20] used a CNN architecture to extract features from the raw data, and then exploited the long short-term memory (LSTM) to learn transition rules through sleep stages. Similarly, in [23], the authors also used a successive blocks of CNN followed by LSTM to classify raw EEG signals. In addition, [24] used an LSTM based encoder-decoder with an attention mechanism after the encoder to find out the most relevant parts in input sequences. However, RNNs also have limitations due to their recurrent nature, i.e., they usually have high model complexity and it is thus difficult to train them in parallel. Some works used attention mechanism completely instead of using RNN. For example, [25] segmented the EEG epochs and used self-attention to learn both intra-epoch features, and inter-epoch temporal features.\n\nAside from selecting classification models, sleep staging also needs to address the data imbalance problem, since humans spend different time periods in each stage. Oversampling is a common strategy to address this issue. For example, the studies in [13], [17], [20] replicated the minority classes for model training. The authors in [24] applied Synthetic Minority Over-sampling TEchnique (SMOTE) [26] for oversampling to balance the data. However, oversampling techniques expand the training data and thus increase the training time.\n\nTo address the above issues, we propose a novel architecture called AttnSleep for automatic sleep stages classification. First, we propose a novel feature extraction module based on multi-resolution CNN (MRCNN) and adaptive feature recalibration (AFR). The MRCNN extracts features corresponding to low and high frequencies from different frequency bands, and the AFR models the features inter-dependencies to enhance the feature learning. Second, we propose a novel temporal context encoder (TCE) that deploys a multi-head attention with causal convolutions to efficiently capture the temporal dependencies in the extracted features. We also design a class-aware loss function to effectively address the data imbalance issue without additional computations. We perform extensive experiments on three public datasets and experimental results demonstrate that our AttnSleep model outperform the state-of-the-arts for sleep stage classification.\n\nOverall, the main contributions of our proposed model can be summarized as follows.\n\n1) We propose a novel feature extraction technique, i.e., a multi-resolution CNN module, to extract features corresponding to low and high frequencies from different frequency bands, and an adaptive feature recalibration to learn the features interdependencies and enhance the representation capability of the extracted features. 2) We propose a novel temporal context encoder that deploys a multi-head self-attention with causal convolutions to efficiently capture the temporal dependencies in the extracted features. 3) We design a class-aware loss function to efficiently handle the class imbalance without introducing additional computations. 4) These novel components are supported by extensive experiments over three public datasets. The results demonstrate that our proposed model outperforms stateof-the-arts in sleep stage classification. The rest of the paper is organized as follows: Section II illustrates the details of the proposed model. In Section III, we introduce the datasets, evaluation metrics, experimental setup and the baseline methods. We then present the comparison results against the baseline methods and the ablation study of our AttnSleep model, as well as a sensitivity analysis of the choice of the number of heads in the MHA. Finally, the conclusion of the paper is presented in Section IV.\n\n\nII. PROPOSED METHOD\n\nIn this section, we introduce our proposed AttnSleep model for sleep stage classification from single-channel EEG data. Fig. 1 illustrates the overall framework of our AttnSleep model. It consists of three main blocks, namely, 1) feature extraction, 2) temporal context encoder and 3) classification.\n\n\nA. Overview of AttnSleep Model\n\nFirst, the MRCNN with two-branch CNN architectures is exploited to extract the features from a 30-second EEG signal. In particular, it extracts high-frequency features by the small kernel convolutions and low-frequency features by the wide kernel convolutions. Following MRCNN, we propose an AFR module to model the inter-dependencies among the features extracted by MRCNN. Moreover, AFR can adaptively select and highlight the most important features, which helps to enhance the classification performance. Second, we develop a TCE module to capture the long-term dependencies in the input features. The core component of TCE is the multi-head attention supported by causal convolutions. Third, the classification decision is done by a fully connected layer with a softmax activation function. We also leverage a class-aware cost-sensitive loss function to handle the data imbalance issue. In the following subsections, we will introduce each block in details. Fig. 2 shows the MRCNN and AFR modules for feature extraction from raw single-channel EEG signals.\n\n\nB. Feature Extraction\n\n1) Multi-Resolution CNN: To extract different types of features, we develop a multi-resolution CNN architecture as shown in Fig. 2. We implement two branches of convolutional layers with different kernel sizes, where the choice of the kernel sizes is related to the sampling rate of the EEG signals and aims to explore different frequency bands. This is inspired by some previous works that used multiple CNN kernel sizes to extract different frequency features (i.e., low-and highfrequencies) such as [27], [28]. Additionally, different sleep stages are characterized by different frequency ranges [7], and thus, it is becoming important to address different frequency bands to improve the extracted features. Therefore, we use different kernel sizes to capture different ranges of timesteps, and hence address features from different sleep-related frequency bands. To further explain this, we consider a dataset with a sampling rate of 100 Hz (100 timesteps are sampled in one second) to justify the selection of the kernel sizes of the two branches. First, the wide kernel (with a kernel of 400) captures timesteps with 4-second windows, and thus captures a whole cycle of sinusoidal signal down to \u223c0.25 Hz (T = 1/F). This range corresponds to delta band. Second, for the smaller kernel (with a kernel of 50), each convolution window captures 50 samples (0.5 second), thus it will be able to capture a whole cycle of sinusoidal signal down to \u223c2 Hz, which means that data corresponds to alpha and theta bands.\n\nOn the other hand, such a combination of features is important for the non-stationary characteristic of EEG signals that requires exploring different kinds of features. As shown in Fig. 2, each branch consists of three convolutional layers and two max-pooling layers, where each convolution layer includes a batch normalization layer [29] and uses a Gaussian Error Linear Unit (GELU) as the activation function. In particular, Conv1D (64, 50, 6) in Fig. 2 refers to using 1D convolution layer with 64 filters, a kernel size of 50 and a stride of 6. Similarly, MaxPooling (8, 2) refers to a maxpooling layer with a kernel size of 8 and a stride of 2. To reduce overfitting, we also apply dropout after the first maxpooling in both branches and after the concatenation of both branches as shown in Fig. 2.\n\n2) Adaptive Feature Recalibration (AFR): AFR aims to recalibrate the features learned by MRCNN for improving its performance. In particular, the AFR models the inter-dependencies between the features and adaptively selects the most discriminative features through a residual squeeze and excitation (residual SE) block [30]. The SE block helps the lower layers of the network to exploit more contextual information outside its local receptive field by a context aware mechanism. In residual SE block, we implement two convolutions Conv1D (30,1,1) with both the kernel and stride size as 1 and ReLU as the activation function. Given a feature map I \u2208 R L\u00d7d learned by MRCNN, we apply two convolution\noperations to I such that F = Conv2( Conv1(I ) ), where F = {F 1 , . . . , F N } \u2208 R N\u00d7d , N is the total number of features, d is the length of F i (1 \u2264 i \u2264 N)\n, and Conv1 and Conv2 are the two convolution operations in AFR module.\n\nNext, the global spatial information is squeezed by using adaptive average pooling that shrinks F \u2208 R N\u00d7d to s = {s 1 , . . . , s N }, where s i is the average of the d data points in F i \u2208 R d , 1 \u2264 i \u2264 N. Two fully connected (FC) layers are then applied to make use of the aggregated information. In particular, the first layer is followed by a ReLU activation function to perform dimensionality reduction, and the second layer is followed by a smoothing sigmoid activation function to perform dimensionality increasing as shown in Equation 1. where \u03c3 and \u03b4 refer to sigmoid and ReLU activation functions respectively, and W 1 and W 2 represent the two FC layers in AFR. Then, the feature map F is scaled by e as follows:\ne = \u03c3 (W 2 (\u03b4(W 1 (s)))) \u2208 R N\u00d7d ,(1)O = F \u2297 e \u2208 R N\u00d7d ,(2)\nwhere \u2297 refers to the point-wise multiplication between F and e. We also add a shortcut connection to combine the original input I with the enhanced selected features learned from the residual SE block. The final output of the AFR module is:\nX = I + O \u2208 R N\u00d7d .(3)\nNote that we use GELU activation function in the MRCNN module as it allows some negative weights of the input to pass through. These negative weights might be important for the following AFR module, leading to different decisions. Compared to ReLU, GELU should perform better, as ReLU suppresses all the negative weights to zeros, and thus the AFR module will not be able to make use of them. However, we use ReLU in the AFR module itself as ReLU aims to avoid exploding/vanishing gradient besides making the computations faster and easier to converge [31]. On the other hand, GELU can be a better choice over some other activation functions that also pass negative values, such as Leaky-ReLU and PReLU. The reason is that these activation functions allow strong negative activations to generate undesirable impact on the sum of activations feeding the next layers, which generates undesirable effects. Differently, GELU shows more control to bound the effect of these negative activations. These conclusions are supported with experiments in Table S.1 in the supplementary materials.\n\n\nC. Temporal Context Encoder (TCE)\n\nThe TCE layer aims to capture temporal dependencies in the input features. As shown in Fig. 1, TCE layer consists of a multi-head attention (MHA) layer, a normalization layer and two FC layers. Moreover, TCE stacks two identical structures to generate the final features. As attention mechanism is a key part in the TCE module, we first introduce the self-attention mechanism, and then we introduce each component in the TCE layer. 1) Self-Attention: We use self-attention to quantify the interdependence within input features, at which higher weights are assigned to the regions of interest according to each position in the input, while lower weights are assigned for less interesting regions. In particular, given an input Z = {z 1 , . . . , z N } \u2208 R N\u00d7d where N is the total number of features, and d is the length of x i , 1 \u2264 i \u2264 N, this input is transformed into another space using a transforming function \u03c6(\u00b7). In our AttnSleep model, \u03c6(\u00b7) is a causal convolution function.\n\nNext, we calculate a score \u03b1 i j that indicates the weight at which i -th position is attending to j -th position, as follows:\n\u03b1 i j = exp s i j d k=1 exp (s ik ) ,(4)s i j = \u03c6(z i )\u03c6(z j ) .(5)\nEach attention output element a i is computed as weighted sum of the transformed input elements:\na i = d j =1 \u03b1 i j \u03c6(x j ).(6)\nThe output of the attention layer is A = (a 0 , a 1 , . . . , a d ) \u2208 R N\u00d7d .\n\n2) Multi-Head Attention (MHA): MHA is inspired by the Transformer model [32], which shows great success in machine translation applications due to its ability to learn long range relationships in sentences [33]. MHA improves the self-attention in two main aspects. First, it expands the model's capability to focus on different positions, as the encoding of each head knows about the encodings of the other heads as well. This improves the model ability to learn temporal dependencies. Second, splitting the input features into different partitions increases the representation subspaces. Therefore, the generated attention weights for each subspace are more representative to the importance of each partition, and concatenating these representations produces better overall representation, which enhances the classification accuracy. In our AttnSleep model, MHA leverages the causal convolutions to encode the positional information of input features and capture their temporal relations. The causal convolutions have an advantage of fast and parallel processing, which significantly reduces the model training time compared with RNNs. Next we illustrate how MHA works in our AttnSleep model.\n\nThe output of the AFR module, denoted as X = {x 1 , . . . , x N } \u2208 R N\u00d7d , serves as the input of MHA as shown in Fig. 3. Here, N is the total number of features, and d is the length of x i , 1 \u2264 i \u2264 N. More specifically, MHA takes three duplicates of X as inputs. First, the causal convolutions generate X from X, i.e., X = \u03c6(X). Second, we pass the three matrices of X to calculate the attention in Equation 7 according to [32].\nAT T ( X, X , X ) = So f tmax( X \u00b7 X T \u221a d ) \u00b7 X ,(7)\nwhere (\u00b7) is the multiplication operation. We further expand the attention over H heads for each of the three matrices. In particular, each matrix X is split into H subspaces, i.e.,\nX = {X 1 , \u00b7 \u00b7 \u00b7 , X H }, X h \u2208 R N\u00d7 d H , 1 \u2264 h \u2264 H .\nIn each subspace h, we calculate the attention A h similarly in Equation 8.\nA h = AT T ( X h , X h , X h ) \u2208 R N\u00d7 d H .(8)\nFinally, all the H representations are concatenated together to produce the final output as follows:\nM H A( X, X , X) = Concat (A 1 , \u00b7 \u00b7 \u00b7 , A H ) \u2208 R N\u00d7d . (9)\n3) Add and Normalize Layer: The TCE has two Add & Normalize layers, which add the output of the previous layer to the input of that layer through a residual connection, and then normalize the sum. This operation can be expressed as Layer Norm(x + SubLayer (x)), where Layer Norm refers to applying layer normalization [34], SubLayer refer to either the MHA or the two FC layers as shown in Fig. 1, and x is the input of the SubLayer . Using the residual connections helps the model to utilize the lower-layer features by propagating them to the higher layers if they are useful. Additionally, the normalization operation helps to speed up the training process.\n\n\n4) Feed-Forward Layer:\n\nThe outputs of the MHA layer are fed into a feed-forward neural network, which is a combination of two FC layers. This layer employs ReLU activation function to break the non-linearity in the model and consider the interactions among latent dimensions. This operation can be modeled as F out = W 4 ( \u03b4(W 3 (x)) ), where W 3 and W 4 refer to the the two FC layers in the TCE module, as shown in Fig. 1.\n\n\nD. Class-Aware Loss Function and Optimization\n\nBasically, we can apply the standard multi-class cross-entropy in Equation 10 as the loss function for our model.\nL = \u2212 1 M K k=1 M i=1 y k i log( y i k ),(10)\nwhere y k i is the actual label for i -th sample and y i k is the predicted probability of i -th sample for the class k, M is the total number of samples and K is the number of classes. Note that various sleep datasets are imbalanced, i.e., the amount of data for each class varies a lot. The loss function in Equation 10 equally penalizes the miss-classification of all the classes, and thus the trained model may be biased towards the majority classes.\n\nWe propose a class-aware loss function to address the above issue, which uses a weighted cross-entropy loss as follows:\nL = \u2212 1 M K k=1 M i=1 w k y k i log( y i k ),(11)w k = \u03bc k \u00b7 max(1, log(\u03bc k M/M k )),(12)\nwhere w k represents the weight assigned to the class k, \u03bc k is a tunable parameter, and M k is the number of samples in class k. The choice of the class weight w k relies on two factors i.e. the number of samples of this class (controlled by M/M k ), and the distinctness of this class (controlled by \u03bc k ). With analyzing the public sleep data, we can reach out to two conclusions. First, class N2 has a large number of samples, while classes N1 and N3 have much fewer number of samples (i.e., N1 and N3 are minority classes). Second, we observe that signals of N3 have significantly higher magnitude than other classes as shown in Fig. 4. Hence, the model can easily make correct predictions for N3 samples. Meanwhile, N1 samples are not distinguishable from those in classes N2 and REM as shown in Fig. 4. Therefore, we assign the highest \u03bc k to N1, the lowest to N3 and assign similar values to the other three classes W, N2 and REM as follows.\n\u03bc k = \u23a7 \u23aa \u23a8 \u23aa \u23a9 a/K k = N3 b/K k = W, N2, R E M c/K k = N1\nwhere a, b, c are hyperparameters that change for each dataset. To fulfill the above recommendation, we chose a < b < c. Note that we use \u03bc k to scale down the M/M k value so we keep the values of \u03bc k less than 1 by dividing them by K . As such, the values of a, b, c are better to be kept less than K to scale down the weights. Additional experiments on the effect of the different variants of a, b, and c values are provided in Section S.II the supplementary materials.\n\nFinally, we use Adam [35] as the optimizer to minimize our class-aware loss in Equation 11 and learn model parameters.\n\n\nIII. EXPERIMENTAL RESULTS\n\nIn this section, we first introduce the experimental setup. Then, we demonstrate the evaluation results of our proposed AttnSleep.\n\n\nA. Datasets and Evaluation Metrics\n\nIn our experiments, we used three public datasets, namely, Sleep-EDF-20, Sleep-EDF-78 and Sleep Heart Health Study (SHHS) as shown in Table I. For each dataset, we used a single EEG channel for various models in our experiments.\n\nSleep-EDF-20 and Sleep-EDF-78 were obtained from the PhysioBank [36]. Sleep-EDF-20 contains data files for 20 subjects, while Sleep-EDF-78 is an expanded version with 78 subjects. The participants were involved in two studies. The first is Sleep Cassette (SC* files), which studies age effects on sleep and it was conducted on healthy participants aged from 25 to 101 years. The second is Sleep Telemetry (ST* files), which addressed the temazepam effects on sleep in 22 Caucasian males and females without having any other medication. For these two datasets, each PSG file contains two EEG channels (Fpz-Cz, Pz-Oz) with a sampling rate of 100 Hz, one EOG channel and one chin EMG channel. Following previous studies [13], [15], [17], [18], [37], we adopted the data from Sleep Cassette study and used the single Fpz-Cz channel as the input for various models in our experiments.\n\nSHHS [38], [39] is a multi-center cohort study of the cardiovascular and other consequences of sleep-disordered breathing. The subjects suffer from various diseases including lung diseases, cardiovascular diseases and coronary diseases. To minimize the impact of these diseases, we followed the study in [40] to select subjects, who are considered to have a regular sleep (e.g., Apnea Hypopnea Index or AHI less than 5). Eventually, 329 out of 6,441 subjects were selected for our experiments. Notably, we selected the C4-A1 channel with a sampling rate of 125 Hz. Further details about the datasets can be found in Section S.III in our supplementary materials. For the three datasets, we applied the following preprocessing steps. First, we excluded any UNKNOWN stages that don't belong to any of the sleep stages. Second, we merged stages N3 and N4 into one stage (N3) according AASM standard. Third, we include only 30 minutes of wake periods before and after the sleep periods to add more focus on the sleep stages [20]. We adopted four metrics to evaluate the performance of various models for sleep stage classification, namely, the accuracy (ACC), macro-averaged F1-score (MF1), Cohen Kappa (\u03ba) [41], and the macro-averaged G-mean (MGm). Both MF1 and MGm are common metrics to evaluate the performance of the models on imbalanced datasets [42]. Given the True Positives (T P i ), False Positives (F P i ), True Negatives (T N i ) and False Negatives (F N i ) for the i -th class, the overall accuracy ACC, MF1 and MGm are defined as follows.   \nACC = K i=1 T P i M ,(13)M F1 = 1 K K i=1 2 \u00d7 Precision i \u00d7 Recall i Precision i + Recall i ,(14)MGm = 1 K K i=1 Speci f icity i \u00d7 Recall i ,(15)\nwhere\nPrecision i = T P i T P i +F P i , Recall i = T P i T P i +F N i and Speci f icity i = T N i T N i +F P i .\nM is the total number of samples and K is the number of classes or sleep stages.\n\nWe also used per-class precision (PR), per-class recall (RE), per-class F1-score (F1), and per-class G-mean (GM) to evaluate each our model. They are calculated as in binary classification by considering one class as the positive class and the other four classes as the negative class. Tables II, III  Notably, stage N1 achieves the lowest performance with the F1 less than 50%, where it is often misclassified to classes W, REM and N2. In counterpart, class N3 achieves the best performance for Sleep-EDF-20 and SHHS datasets, but it decreases for Sleep-EDF-78 as it is the minority class on this dataset. Most of the misclassifications in the different datasets are with class N2 as it is the majority class.\n\n\nB. Scoring Performance of AttnSleep\n\n\nC. Baselines and Experimental Setup\n\nIn our experiments, we compared our model with five baselines, namely, DeepSleepNet [20], SleepEEGNet [24], ResnetLSTM [43], MultitaskCNN [17] and SeqSleepNet [37]. Brief descriptions for each baseline are as follows.\n\n\u2022 DeepSleepNet [20] exploits a custom CNN architecture followed by an LSTM with a residual connection for sleep stage classification. \u2022 SleepEEGNet [24] employs the same CNN architecture as DeepSleepNet [20] followed by an encoder-decoder with attention mechanism. \u2022 ResnetLSTM [43] implements a ResNet architecture for feature extraction, followed by an LSTM to classify EEG signals into different sleep stages. \u2022 MultitaskCNN [17] starts by converting the raw EEG signals into power spectrum images, and then applies a joint classification and prediction technique using a multi-task CNN architecture for identifying sleep stages. \u2022 SeqSleepNet [37] also converts the raw EEG signal into power spectrum images and then uses a hierarchical RNN structure to classify multiple epochs at once. In particular, we used the published codes for DeepSleepNet [20], SleepEEGNet [24], MultitaskCNN [17] and SeqSleepNet [37], and re-implemented ResnetLSTM [43]. To evaluate the performance of various models, we adopted a subject-wise 20-fold cross-validation by dividing the subjects in each dataset into 20 groups. For example, subject-wise 20-fold cross-validation on Sleep-EDF-20 dataset with 20 subjects is thus leave-one-subject-out (LOSO) cross-validation. For each round, we selected one group of subjects as testing data and the remaining 19 groups as training data. Eventually we combined the predicted sleep stages for the testing samples from all the 20 rounds to calculate various performance metrics. We chose the neural network hyperparameters based on the performance across these folds. In addition, we noticed that AttnSleep performance stabilizes before reaching 100 epochs, so we trained all the models for 100 epochs in each round to fairly compare their average training time. Fig. 5 shows the performance graph of our model showing both the loss and accuracy during AttnSleep training. Our model shows a stable performance during training, and we notice that it converges quickly. Additionally, it can be seen that the validation loss stabilizes even with the continual decrement in the training loss, which reflects the robustness of our model against overfitting. We built our model using PyTorch 1.4 and trained it on a Tesla K40 GPU. We applied a batch size of 128, and the Adam optimizer with the learning rate starting with 1e-3 then reducing to 1e-4 after 10 epochs. The weight decay of Adam was set to 1e-3, the betas (b1, b2) were used as (0.9, 0.999) respectively, the epsilon value was 1e-08 and the amsgrad was set to true. All the convolutional layers were initialized using a Gaussian distribution with a mean of 0 and a variance of 0.02. For the TCE module, we used 5 heads in MHA and the dimension of each feature d was 80 for Sleep-EDF dataset, and 100 for SHHS dataset because of its higher sampling rate, and hence its longer signal length. For the two fully connected layers, the input dimension was d and the output dimension was set to 120, and vice versa for the second fully connected layer. A detailed description can also be found in Table S \n\n\nD. Comparison With State-of-the-Art Approaches\n\nWe evaluated the performance of our AttnSleep model against various state-of-the-art approaches. We compared their performance in terms of overall accuracy, macro F1-score, cohen kappa, macro G-mean and the average training time on three datasets. Table V shows the comparison among DeepSleepNet [20], SleepEEGNet [24], ResnetLSTM [43], MultitaskCNN [17] and our AttnSleep. We observe that our AttnSleep achieves better classification performance than the other four approaches, due to its powerful feature extraction module as well as the TCE with attention mechanism. In particular, our AttnSleep achieves better MF1 and MGm on Sleep-EDF-78 and SHHS, indicating that the designed cost-sensitive loss function is helpful to handle imbalanced data. In addition, we can observe that our AttnSleep achieves lower performance for class N1 than [20], [24]. As shown in Fig. 4, W, REM and N1 have similar features in our framework. Therefore, our AttnSleep tends to misclassify N1 as other classes including W and REM, which is also demonstrated in the confusion matrices in Tables II, III and IV. Note that all the five methods in Table V Table VI, our AttnSleep outperforms SeqSleepNet in terms of all the four metrics (ACC, MF1, \u03ba and MGm). By comparing Tables V and VI, we also observe that using more epochs as input includes more temporal relations and helps our AttnSleep model to achieve better performance.\n\nIn addition, the training time of our method is much less than other methods as shown in Tables V and VI. First, DeepSleepNet [20], SleepEEGNet [24] and SeqSleepNet [37] all exploit LSTMs which slow down the training due to the recurrent processing in LSTM. Second, MultitaskCNN [17] and SeqSleepNet [37] require additional computation to pre-train a DNN-based filter bank before training the main model. Differently, our AttnSleep model captures the temporal dependency among EEG data using TCE instead of LSTM, and can thus benefit from parallel computation to achieve the reduced training complexity. \n\n\nE. Ablation Study\n\nNote that our AttnSleep consists of MRCNN, AFR and TCE modules together with the class-aware loss function. To analyze the effectiveness of each module in our AttnSleep, we present an ablation study conducted on Sleep-EDF-20 dataset as shown in Fig. 6. Specifically, we derive five model variants as follows and the first four variants do not use the class-aware loss function. with the class-aware loss function. We can draw the following three conclusions based on the ablation study as shown in Fig. 6. First, AFR can enhance the classification performance, which demonstrates the necessity of modeling the feature inter-dependencies. This is further demonstrated by comparing the third and fourth variants (i.e., MRCNN+TCE vs. MRCNN+AFR+TCE). Second, by comparing MRCNN and MRCNN+TCE (similarly MRCNN+AFR vs. MRCNN+AFR+TCE), we conclude that capturing the temporal dependencies with TCE is important for sleep stage classification. Moreover, TCE is even more important than AFR as MRCNN+TCE outperforms MRCNN+AFR. Third, AttnSleep achieves significantly better MF1 and MGm than other four variants, indicating that the proposed class-aware cost-sensitive loss function can effectively address the data imbalance issue without any added computation overhead. We also conduct the ablation study for Sleep-EDF-78 and SHHS dataset, which can be found in \n\n\nF. Sensitivity Analysis for the Number of Heads in MHA\n\nAs MHA is one key component of our model, it is important to study how the number of heads affects the model performance. In particular, we fix the other parameters and test different number of heads in MHA. Note that the number of heads should be dividable by the length of features d. As d is 80 for Sleep-EDF-20 dataset, we run our model using 1, 2, 4, 5, 8 and 10 heads. Fig. 7 shows the model performance on Sleep-EDF-20 dataset in terms of accuracy and MF1 score. Overall, the model performance is quite stable when we use different number of heads. With increasing the number of heads from 1, 2 to 4 and 5, we can observe a slight improvement on the performance, since using more heads allows the model to find more meaningful features and feature interactions. Meanwhile, when the number of heads further increases (H equals to 8 and 10), i.e., the length of features in each head becomes smaller, which leads to a slight performance decrease. In our experiments, we eventually set H as 5 on Sleep-EDF-20 dataset. For other two datasets, we also set H as 5, and the detailed sensitivity analysis can be found in Fig. S.5 and S.6 in our supplementary materials.\n\n\nIV. CONCLUSION\n\nWe proposed a novel architecture for sleep stage classification from single channel raw EEG signals called AttnSleep. The AttnSleep relies on extracting the features from EEG signals using two modules: the multi-resolution convolutional neural network (MRCNN) and the adaptive feature recalibration (AFR). These two modules are followed by the temporal context encoder (TCE) module, which captures the temporal dependencies among the extracted features by using a multi-head attention (MHA) mechanism. We also proposed a class-aware cost-sensitive loss function to handle the issue of data imbalance. The experimental results on three public datasets demonstrated that our model outperforms state-of-the-art methods under various evaluation matrices. Besides, an ablation study was performed to show the effectiveness of each module in the proposed method. Finally, we conducted a sensitivity analysis to demonstrate the impact of the number of heads in MHA. The results indicated that our method is quite stable with different number of heads. For future directions, we will consider transfer learning and domain adaptation techniques, which adapt the model trained on labeled dataset to classify the unlabeled sleep data in other datasets.\n\nFig. 1 .\n1Overall framework of the proposed AttnSleep model for automatic sleep stage classification.\n\nFig. 2 .\n2The MRCNN and AFR modules for feature extraction. Each convolution block is followed by a Batch Normalization.\n\nFig. 3 .\n3Structure of proposed multi-head attention.\n\nFig. 4 .\n4The amplitude of the extracted features differs among 5 classes. This snapshot is from Sleep-EDF-20 dataset[36].\n\n\nand IV show the confusion matrices of the proposed model applied on the Fpz-Cz channel in bothTABLE V COMPARISON AMONG ATTNSLEEP AND STATE-OF-THE-ART MODELS. THE BEST VALUES ON EACH DATASET ARE HIGHLIGHTED IN BOLDSleep-EDF datasets and on C4-A1 channel in SHHS dataset. The confusion matrix is calculated by adding up all the scoring values of the testing data through the 20 folds. Each row represents the number of samples classified by experts, while each column represents the number of epochs predicted by our model. The tables also show the per-class precision, recall, F1 score and G-mean value for each class.\n\nFig. 5 .\n5Training and testing accuracy and loss comparison for a random fold (i.e. fold 10 on subject 16) in Sleep-EDF-20 dataset.\n\nFig. 6 .\n6Ablation study conducted on Sleep-EDF-20 dataset.\n\n\nFig. S.3 and S.4 in our supplementary materials.\n\nTABLE I DETAILS\nIOF THREE DATASETS USED IN OUR EXPERIMENTS (EACH SAMPLE IS A 30-SECOND EPOCH)\n\nTABLE III CONFUSION\nIIIMATRIX OF PROPOSED MODEL APPLIED ON FPZ-CZ CHANNEL FROM EDF-78 DATASET\n\nTABLE IV CONFUSION\nIVMATRIX OF PROPOSED MODEL APPLIED ON C4-A1 CHANNEL FROM SHHS DATASET\n\n\n.3 in our supplementary materials. Our source codes and supplementary materials are publicly available at https://github.com/emadeldeen24/AttnSleep.\n\n\nuse a single epoch (i.e., 30-second EEG signal) as the model input. Differently, SeqSleepNet [37] takes 3 epochs as input and then predicts the label for the middle epoch. For fair comparison, we compare our AttnSleep with SeqSleepNet in Table VI by using 3 epochs as input. As shown in\n\n1 )\n1MRCNN: MRCNN module only. 2) MRCNN+AFR: MRCNN and AFR without TCE. 3) MRCNN+TCE: MRCNN and TCE without AFR. 4) MRCNN+AFR+TCE: training MRCNN, AFR and TCE together, without the class-aware loss function. 5) AttnSleep: training MRCNN, AFR and TCE together,\n\nTABLE VI COMPARISON\nVIOF THE PERFORMANCE OF ATTNSLEEP AGAINST SEQSLEEPNET WITH 3 EPOCHS AS INPUT Fig. 7. The performance of our AttnSleep model on Sleep-EDF-20 dataset by using different number of heads in MHA.\n\nSleep: A health imperative. F S Luyster, P J Strollo, P C Zee, J K Walsh, Sleep. 356F. S. Luyster, P. J. Strollo, P. C. Zee, and J. K. Walsh, \"Sleep: A health imperative,\" Sleep, vol. 35, no. 6, pp. 727-734, Jun. 2012.\n\nPartial sleep deprivation attenuates the positive affective system: Effects across multiple measurement modalities. P H Finan, Sleep. 401P. H. Finan et al., \"Partial sleep deprivation attenuates the positive affec- tive system: Effects across multiple measurement modalities,\" Sleep, vol. 40, no. 1, pp. 1-9, Jan. 2017.\n\nThe relationship between memory systems and sleep stages. G Rauchs, B Desgranges, J Foret, F Eustache, J. Sleep Res. 142G. Rauchs, B. Desgranges, J. Foret, and F. Eustache, \"The relationship between memory systems and sleep stages,\" J. Sleep Res., vol. 14, no. 2, pp. 123-140, 2005.\n\nSleep and metabolism: An overview. S Sharma, M Kavuru, Int. J. Endocrinol. 2010S. Sharma and M. Kavuru, \"Sleep and metabolism: An overview,\" Int. J. Endocrinol., vol. 2010, pp. 1-12, Oct. 2010.\n\nRelationship between blood pressure, sleep Kcomplexes, and muscle sympathetic nerve activity in humans. J Tank, Integrative Comparative Physiol. 2851Amer. J. Physiol.-RegulatoryJ. Tank et al., \"Relationship between blood pressure, sleep K- complexes, and muscle sympathetic nerve activity in humans,\" Amer. J. Physiol.-Regulatory, Integrative Comparative Physiol., vol. 285, no. 1, pp. R208-R214, Jul. 2003.\n\nAn overview of polysomnography. A S Keenan, Handbook of Clinical Neurophysiology. Amsterdam, The NetherlandsElsevier6A. S. Keenan, \"An overview of polysomnography,\" in Handbook of Clinical Neurophysiology, vol. 6. Amsterdam, The Netherlands: Elsevier, 2005, ch. 3, pp. 33-50.\n\nA novel multi-class EEG-based sleep stage classification system. P Memar, F Faradji, IEEE Trans. Neural Syst. Rehabil. Eng. 261P. Memar and F. Faradji, \"A novel multi-class EEG-based sleep stage classification system,\" IEEE Trans. Neural Syst. Rehabil. Eng., vol. 26, no. 1, pp. 84-95, Jan. 2018.\n\nA novel, fast and efficient single-sensor automatic sleep-stage classification based on complementary cross-frequency coupling estimates. S I Dimitriadis, C Salis, D Linden, Clin. Neurophysiol. 1294S. I. Dimitriadis, C. Salis, and D. Linden, \"A novel, fast and efficient single-sensor automatic sleep-stage classification based on complemen- tary cross-frequency coupling estimates,\" Clin. Neurophysiol., vol. 129, no. 4, pp. 815-828, Apr. 2018.\n\nAnalysis and classification of sleep stages based on difference visibility graphs from a single-channel EEG signal. G Zhu, Y Li, P Wen, IEEE J. Biomed. Health Informat. 186G. Zhu, Y. Li, and P. Wen, \"Analysis and classification of sleep stages based on difference visibility graphs from a single-channel EEG signal,\" IEEE J. Biomed. Health Informat., vol. 18, no. 6, pp. 1813-1821, Nov. 2014.\n\nA new automatic sleep staging system based on statistical behavior of local extrema using single channel EEG signal. S Seifpour, H Niknazar, M Mikaeili, A M Nasrabadi, Expert Syst. Appl. 104S. Seifpour, H. Niknazar, M. Mikaeili, and A. M. Nasrabadi, \"A new automatic sleep staging system based on statistical behavior of local extrema using single channel EEG signal,\" Expert Syst. Appl., vol. 104, pp. 277-293, Aug. 2018.\n\nHyCLASSS: A hybrid classifier for automatic sleep stage scoring. X Li, L Cui, S Tao, J Chen, X Zhang, G.-Q Zhang, IEEE J. Biomed. Health Informat. 222X. Li, L. Cui, S. Tao, J. Chen, X. Zhang, and G.-Q. Zhang, \"HyCLASSS: A hybrid classifier for automatic sleep stage scoring,\" IEEE J. Biomed. Health Informat., vol. 22, no. 2, pp. 375-385, Mar. 2018.\n\nComputer-aided sleep staging using complete ensemble empirical mode decomposition with adaptive noise and bootstrap aggregating. A R Hassan, M I H Bhuiyan, Biomed. Signal Process. Control. 24A. R. Hassan and M. I. H. Bhuiyan, \"Computer-aided sleep staging using complete ensemble empirical mode decomposition with adaptive noise and bootstrap aggregating,\" Biomed. Signal Process. Control, vol. 24, pp. 1-10, Feb. 2016.\n\nAutomatic sleep stage scoring with single-channel EEG using convolutional neural networks. O Tsinalis, P M Matthews, Y Guo, S Zafeiriou, arXiv:1610.01683O. Tsinalis, P. M. Matthews, Y. Guo, and S. Zafeiriou, \"Auto- matic sleep stage scoring with single-channel EEG using convolu- tional neural networks,\" 2016, arXiv:1610.01683. [Online]. Available: http://arxiv.org/abs/1610.01683\n\nA convolutional neural network for sleep stage scoring from raw singlechannel EEG. A Sors, S Bonnet, S Mirek, L Vercueil, J.-F Payen, Biomed. Signal Process. Control. 42A. Sors, S. Bonnet, S. Mirek, L. Vercueil, and J.-F. Payen, \"A con- volutional neural network for sleep stage scoring from raw single- channel EEG,\" Biomed. Signal Process. Control, vol. 42, pp. 107-114, Apr. 2018.\n\nDeep learning for automated feature discovery and classification of sleep stages. M Sokolovsky, F Guerrero, S Paisarnsrisomsuk, C Ruiz, S A Alvarez, IEEE/ACM Trans. Comput. Biol. Bioinf. 176M. Sokolovsky, F. Guerrero, S. Paisarnsrisomsuk, C. Ruiz, and S. A. Alvarez, \"Deep learning for automated feature discovery and classification of sleep stages,\" IEEE/ACM Trans. Comput. Biol. Bioinf., vol. 17, no. 6, pp. 1835-1845, Nov. 2020.\n\nA deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series. S Chambon, M N Galtier, P J Arnal, G Wainrib, A Gramfort, IEEE Trans. Neural Syst. Rehabil. Eng. 264S. Chambon, M. N. Galtier, P. J. Arnal, G. Wainrib, and A. Gramfort, \"A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series,\" IEEE Trans. Neural Syst. Rehabil. Eng., vol. 26, no. 4, pp. 758-769, Apr. 2018.\n\nJoint classification and prediction CNN framework for automatic sleep stage classification. H Phan, F Andreotti, N Cooray, O Y Ch\u00e9n, M. De Vos, IEEE Trans. Biomed. Eng. 665H. Phan, F. Andreotti, N. Cooray, O. Y. Ch\u00e9n, and M. De Vos, \"Joint classification and prediction CNN framework for automatic sleep stage classification,\" IEEE Trans. Biomed. Eng., vol. 66, no. 5, pp. 1285-1296, May 2019.\n\nEnd-to-end sleep staging using convolutional neural network in raw single-channel EEG. F Li, Biomed. Signal Process. Control. 63Art. no. 102203F. Li et al., \"End-to-end sleep staging using convolutional neural network in raw single-channel EEG,\" Biomed. Signal Process. Control, vol. 63, Jan. 2021, Art. no. 102203.\n\nCascaded LSTM recurrent neural network for automated sleep stage classification using single-channel EEG signals. N Michielli, U R Acharya, F Molinari, Comput. Biol. Med. 106N. Michielli, U. R. Acharya, and F. Molinari, \"Cascaded LSTM recurrent neural network for automated sleep stage classification using single-channel EEG signals,\" Comput. Biol. Med., vol. 106, pp. 71-81, Mar. 2019.\n\nDeepSleepNet: A model for automatic sleep stage scoring based on raw single-channel EEG. A Supratak, H Dong, C Wu, Y Guo, IEEE Trans. Neural Syst. Rehabil. Eng. 2511A. Supratak, H. Dong, C. Wu, and Y. Guo, \"DeepSleepNet: A model for automatic sleep stage scoring based on raw single-channel EEG,\" IEEE Trans. Neural Syst. Rehabil. Eng., vol. 25, no. 11, pp. 1998-2008, Nov. 2017.\n\nAn attention based CNN-LSTM approach for sleep-wake detection with heterogeneous sensors. Z Chen, M Wu, W Cui, C Liu, X Li, 10.1109/JBHI.2020.3006145IEEE J. Biomed. Health Informat. early accessZ. Chen, M. Wu, W. Cui, C. Liu, and X. Li, \"An attention based CNN- LSTM approach for sleep-wake detection with heterogeneous sensors,\" IEEE J. Biomed. Health Informat., early access, Jun. 30. 2020, doi: 10.1109/JBHI.2020.3006145.\n\nA novel ensemble deep learning approach for sleepwake detection using heart rate variability and acceleration. Z Chen, 10.1109/TETCI.2020.2996943IEEE Trans. Emerg. Topics Comput. Intell., early access. Z. Chen et al., \"A novel ensemble deep learning approach for sleep- wake detection using heart rate variability and acceleration,\" IEEE Trans. Emerg. Topics Comput. Intell., early access, Jun. 8, 2020, doi: 10.1109/TETCI.2020.2996943.\n\nAutomatic human sleep stage scoring using deep neural networks. A Malafeev, Frontiers Neurosci. 12781A. Malafeev et al., \"Automatic human sleep stage scoring using deep neural networks,\" Frontiers Neurosci., vol. 12, p. 781, Nov. 2018.\n\nSleepEEGNet: Automated sleep stage scoring with sequence to sequence deep learning approach. S Mousavi, F Afghah, U R Acharya, PLoS ONE. 145Art. no. e0216456S. Mousavi, F. Afghah, and U. R. Acharya, \"SleepEEGNet: Automated sleep stage scoring with sequence to sequence deep learning approach,\" PLoS ONE, vol. 14, no. 5, May 2019, Art. no. e0216456.\n\nConvolution-and attention-based neural network for automated sleep stage classification. T Zhu, W Luo, F Yu, Int. J. Environ. Res. Public Health. 17114152T. Zhu, W. Luo, and F. Yu, \"Convolution-and attention-based neural network for automated sleep stage classification,\" Int. J. Environ. Res. Public Health, vol. 17, no. 11, p. 4152, Jun. 2020.\n\nSMOTE: Synthetic minority over-sampling technique. N V Chawla, K W Bowyer, L O Hall, W P Kegelmeyer, J. Artif. Intell. Res. 16N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, \"SMOTE: Synthetic minority over-sampling technique,\" J. Artif. Intell. Res., vol. 16, pp. 321-357, Jun. 2002.\n\nAn improved deep convolutional neural network with multi-scale information for bearing fault diagnosis. W Huang, J Cheng, Y Yang, G Guo, Neurocomputing. 359W. Huang, J. Cheng, Y. Yang, and G. Guo, \"An improved deep convolutional neural network with multi-scale information for bearing fault diagnosis,\" Neurocomputing, vol. 359, pp. 77-92, Sep. 2019.\n\nVery deep convolutional neural networks for raw waveforms. W Dai, C Dai, S Qu, J Li, S Das, Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP). IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)W. Dai, C. Dai, S. Qu, J. Li, and S. Das, \"Very deep convolutional neural networks for raw waveforms,\" in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Mar. 2017, pp. 421-425.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, Proc. 32nd Int. Conf. Mach. Learn. 32nd Int. Conf. Mach. LearnS. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" in Proc. 32nd Int. Conf. Mach. Learn., 2015, pp. 448-456.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)J. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2018, pp. 7132-7141.\n\nEmpirical evaluation of rectified activations in convolutional network. B Xu, N Wang, T Chen, M Li, arXiv:1505.00853B. Xu, N. Wang, T. Chen, and M. Li, \"Empirical evaluation of rec- tified activations in convolutional network,\" 2015, arXiv:1505.00853. [Online]. Available: http://arxiv.org/abs/1505.00853\n\nAttention is all you need. A Vaswani, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystA. Vaswani et al., \"Attention is all you need,\" in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 5998-6008.\n\nBert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, N K Toutanova, Proc. Assoc. Comput. Linguistics. Assoc. Comput. LinguisticsJ. Devlin, M.-W. Chang, K. Lee, and N. K. Toutanova, \"Bert: Pre- training of deep bidirectional transformers for language understanding,\" in Proc. Assoc. Comput. Linguistics, 2019, pp. 4171-4186.\n\nLayer normalization. J L Ba, J R Kiros, G E Hinton, arXiv:1607.06450J. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" 2016, arXiv:1607.06450. [Online]. Available: http://arxiv.org/abs/1607.06450\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, Proc. Int. Conf. Learn. Represent. Int. Conf. Learn. RepresentD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" in Proc. Int. Conf. Learn. Represent., 2015, pp. 1-15.\n\nA L Goldberger, PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. 101A. L. Goldberger et al., \"PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic sig- nals,\" Circulation, vol. 101, no. 23, pp. e215-e220, Jun. 2000.\n\nSeqSleepNet: End-to-end hierarchical recurrent neural network for sequence-to-sequence automatic sleep staging. H Phan, F Andreotti, N Cooray, O Y Chen, M. De Vos, IEEE Trans. Neural Syst. Rehabil. Eng. 273H. Phan, F. Andreotti, N. Cooray, O. Y. Chen, and M. De Vos, \"SeqSleepNet: End-to-end hierarchical recurrent neural network for sequence-to-sequence automatic sleep staging,\" IEEE Trans. Neural Syst. Rehabil. Eng., vol. 27, no. 3, pp. 400-410, Mar. 2019.\n\nThe national sleep research resource: Towards a sleep data commons. G.-Q Zhang, J. Amer. Med. Inform. Assoc. 2510G.-Q. Zhang et al., \"The national sleep research resource: Towards a sleep data commons,\" J. Amer. Med. Inform. Assoc., vol. 25, no. 10, pp. 1351-1358, Oct. 2018.\n\nThe sleep heart health study: Design, rationale, and methods. S F Quan, Sleep. 2012S. F. Quan et al., \"The sleep heart health study: Design, rationale, and methods,\" Sleep, vol. 20, no. 12, pp. 1077-1085, 1997.\n\nCardiorespiratory sleep stage detection using conditional random fields. P Fonseca, N Teuling, X Long, R M Aarts, IEEE J. Biomed. Health Informat. 214P. Fonseca, N. den Teuling, X. Long, and R. M. Aarts, \"Cardiorespiratory sleep stage detection using conditional random fields,\" IEEE J. Biomed. Health Informat., vol. 21, no. 4, pp. 956-966, Jul. 2017.\n\nA coefficient of agreement for nominal scales. J Cohen, Educ. Psychol. Meas. 201J. Cohen, \"A coefficient of agreement for nominal scales,\" Educ. Psychol. Meas., vol. 20, no. 1, pp. 37-46, Apr. 1960.\n\nSVMs modeling for highly imbalanced classification. Y Tang, Y.-Q Zhang, N V Chawla, S Krasser, IEEE Trans. Syst. Man, Cybern. B, Cybern. 391Y. Tang, Y.-Q. Zhang, N. V. Chawla, and S. Krasser, \"SVMs modeling for highly imbalanced classification,\" IEEE Trans. Syst. Man, Cybern. B, Cybern., vol. 39, no. 1, pp. 281-288, Feb. 2009.\n\nDeep convolutional network method for automatic sleep stage classification based on neurophysiological signals. Y Sun, B Wang, J Jin, X Wang, Proc. 11th Int. Congr. Image Signal Process. 11th Int. Congr. Image Signal essY. Sun, B. Wang, J. Jin, and X. Wang, \"Deep convolutional network method for automatic sleep stage classification based on neurophysio- logical signals,\" in Proc. 11th Int. Congr. Image Signal Process., Biomed. Eng. Informat. (CISP-BMEI), Oct. 2018, pp. 1-5.\n", "annotations": {"author": "[{\"end\":118,\"start\":100},{\"end\":152,\"start\":119},{\"end\":184,\"start\":153},{\"end\":211,\"start\":185},{\"end\":228,\"start\":212},{\"end\":258,\"start\":229},{\"end\":283,\"start\":259}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":111},{\"end\":183,\"start\":180},{\"end\":227,\"start\":223},{\"end\":282,\"start\":278}]", "author_first_name": "[{\"end\":110,\"start\":100},{\"end\":146,\"start\":138},{\"end\":151,\"start\":147},{\"end\":179,\"start\":172},{\"end\":207,\"start\":204},{\"end\":210,\"start\":208},{\"end\":222,\"start\":212},{\"end\":254,\"start\":248},{\"end\":257,\"start\":255},{\"end\":277,\"start\":271}]", "author_affiliation": null, "title": "[{\"end\":97,\"start\":1},{\"end\":380,\"start\":284}]", "venue": "[{\"end\":448,\"start\":382}]", "abstract": "[{\"end\":1860,\"start\":485}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1871,\"start\":1868},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1997,\"start\":1994},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2002,\"start\":1999},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2332,\"start\":2329},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2719,\"start\":2716},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3438,\"start\":3435},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3473,\"start\":3470},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3479,\"start\":3475},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3503,\"start\":3500},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3509,\"start\":3505},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3559,\"start\":3555},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3999,\"start\":3995},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4005,\"start\":4001},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4157,\"start\":4153},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4166,\"start\":4162},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4267,\"start\":4263},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4409,\"start\":4405},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4553,\"start\":4549},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5033,\"start\":5029},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5217,\"start\":5213},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5223,\"start\":5219},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5242,\"start\":5238},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5425,\"start\":5421},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5539,\"start\":5535},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5928,\"start\":5924},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6304,\"start\":6300},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6310,\"start\":6306},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6316,\"start\":6312},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6388,\"start\":6384},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6452,\"start\":6448},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10890,\"start\":10886},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10896,\"start\":10892},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10986,\"start\":10983},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12237,\"start\":12233},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13026,\"start\":13022},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15241,\"start\":15237},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17270,\"start\":17266},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17404,\"start\":17400},{\"end\":18801,\"start\":18791},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18819,\"start\":18815},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19186,\"start\":19185},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19719,\"start\":19715},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22868,\"start\":22864},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23458,\"start\":23454},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24111,\"start\":24107},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24117,\"start\":24113},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24123,\"start\":24119},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24129,\"start\":24125},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24135,\"start\":24131},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24280,\"start\":24276},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24286,\"start\":24282},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24579,\"start\":24575},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25294,\"start\":25290},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25477,\"start\":25473},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25621,\"start\":25617},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27041,\"start\":27037},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27059,\"start\":27055},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27076,\"start\":27072},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27095,\"start\":27091},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27116,\"start\":27112},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27191,\"start\":27187},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27324,\"start\":27320},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27379,\"start\":27375},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27454,\"start\":27450},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27604,\"start\":27600},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27823,\"start\":27819},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28028,\"start\":28024},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28046,\"start\":28042},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28065,\"start\":28061},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28122,\"start\":28118},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30604,\"start\":30600},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30622,\"start\":30618},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30639,\"start\":30635},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30658,\"start\":30654},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31149,\"start\":31145},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31155,\"start\":31151},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31846,\"start\":31842},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31864,\"start\":31860},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31885,\"start\":31881},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31999,\"start\":31995},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32020,\"start\":32016},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":36586,\"start\":36582}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36286,\"start\":36184},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36408,\"start\":36287},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36463,\"start\":36409},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36587,\"start\":36464},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37207,\"start\":36588},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37340,\"start\":37208},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37401,\"start\":37341},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37452,\"start\":37402},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37547,\"start\":37453},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37642,\"start\":37548},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37732,\"start\":37643},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37883,\"start\":37733},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38172,\"start\":37884},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38433,\"start\":38173},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":38645,\"start\":38434}]", "paragraph": "[{\"end\":2117,\"start\":1862},{\"end\":2885,\"start\":2119},{\"end\":3653,\"start\":2887},{\"end\":4909,\"start\":3655},{\"end\":6048,\"start\":4911},{\"end\":6585,\"start\":6050},{\"end\":7529,\"start\":6587},{\"end\":7614,\"start\":7531},{\"end\":8939,\"start\":7616},{\"end\":9263,\"start\":8963},{\"end\":10358,\"start\":9298},{\"end\":11897,\"start\":10384},{\"end\":12702,\"start\":11899},{\"end\":13401,\"start\":12704},{\"end\":13634,\"start\":13563},{\"end\":14359,\"start\":13636},{\"end\":14661,\"start\":14420},{\"end\":15769,\"start\":14685},{\"end\":16790,\"start\":15807},{\"end\":16918,\"start\":16792},{\"end\":17083,\"start\":16987},{\"end\":17192,\"start\":17115},{\"end\":18387,\"start\":17194},{\"end\":18820,\"start\":18389},{\"end\":19056,\"start\":18875},{\"end\":19187,\"start\":19112},{\"end\":19335,\"start\":19235},{\"end\":20057,\"start\":19397},{\"end\":20485,\"start\":20084},{\"end\":20648,\"start\":20535},{\"end\":21149,\"start\":20695},{\"end\":21270,\"start\":21151},{\"end\":22310,\"start\":21361},{\"end\":22841,\"start\":22370},{\"end\":22961,\"start\":22843},{\"end\":23121,\"start\":22991},{\"end\":23388,\"start\":23160},{\"end\":24269,\"start\":23390},{\"end\":25822,\"start\":24271},{\"end\":25974,\"start\":25969},{\"end\":26163,\"start\":26083},{\"end\":26875,\"start\":26165},{\"end\":27170,\"start\":26953},{\"end\":30253,\"start\":27172},{\"end\":31714,\"start\":30304},{\"end\":32320,\"start\":31716},{\"end\":33696,\"start\":32342},{\"end\":34923,\"start\":33755},{\"end\":36183,\"start\":34942}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13562,\"start\":13402},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14397,\"start\":14360},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14419,\"start\":14397},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14684,\"start\":14662},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16959,\"start\":16919},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16986,\"start\":16959},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17114,\"start\":17084},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18874,\"start\":18821},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19111,\"start\":19057},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19234,\"start\":19188},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19396,\"start\":19336},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20694,\"start\":20649},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21320,\"start\":21271},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21360,\"start\":21320},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22369,\"start\":22311},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25848,\"start\":25823},{\"attributes\":{\"id\":\"formula_16\"},\"end\":25920,\"start\":25848},{\"attributes\":{\"id\":\"formula_17\"},\"end\":25968,\"start\":25920},{\"attributes\":{\"id\":\"formula_18\"},\"end\":26082,\"start\":25975}]", "table_ref": "[{\"end\":15735,\"start\":15728},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23301,\"start\":23294},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26465,\"start\":26451},{\"end\":30252,\"start\":30245},{\"end\":30559,\"start\":30552},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31396,\"start\":31374},{\"end\":31438,\"start\":31431},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31447,\"start\":31439},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31820,\"start\":31805}]", "section_header": "[{\"end\":8961,\"start\":8942},{\"end\":9296,\"start\":9266},{\"end\":10382,\"start\":10361},{\"end\":15805,\"start\":15772},{\"end\":20082,\"start\":20060},{\"end\":20533,\"start\":20488},{\"end\":22989,\"start\":22964},{\"end\":23158,\"start\":23124},{\"end\":26913,\"start\":26878},{\"end\":26951,\"start\":26916},{\"end\":30302,\"start\":30256},{\"end\":32340,\"start\":32323},{\"end\":33753,\"start\":33699},{\"end\":34940,\"start\":34926},{\"end\":36193,\"start\":36185},{\"end\":36296,\"start\":36288},{\"end\":36418,\"start\":36410},{\"end\":36473,\"start\":36465},{\"end\":37217,\"start\":37209},{\"end\":37350,\"start\":37342},{\"end\":37469,\"start\":37454},{\"end\":37568,\"start\":37549},{\"end\":37662,\"start\":37644},{\"end\":38177,\"start\":38174},{\"end\":38454,\"start\":38435}]", "table": null, "figure_caption": "[{\"end\":36286,\"start\":36195},{\"end\":36408,\"start\":36298},{\"end\":36463,\"start\":36420},{\"end\":36587,\"start\":36475},{\"end\":37207,\"start\":36590},{\"end\":37340,\"start\":37219},{\"end\":37401,\"start\":37352},{\"end\":37452,\"start\":37404},{\"end\":37547,\"start\":37471},{\"end\":37642,\"start\":37572},{\"end\":37732,\"start\":37665},{\"end\":37883,\"start\":37735},{\"end\":38172,\"start\":37886},{\"end\":38433,\"start\":38179},{\"end\":38645,\"start\":38457}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9089,\"start\":9083},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10266,\"start\":10260},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10514,\"start\":10508},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12086,\"start\":12080},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12354,\"start\":12348},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12701,\"start\":12695},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15900,\"start\":15894},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18510,\"start\":18504},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19793,\"start\":19787},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20484,\"start\":20478},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22001,\"start\":21995},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22169,\"start\":22163},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28967,\"start\":28961},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31175,\"start\":31169},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32593,\"start\":32587},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32846,\"start\":32840},{\"end\":34136,\"start\":34130},{\"end\":34881,\"start\":34875}]", "bib_author_first_name": "[{\"end\":38676,\"start\":38675},{\"end\":38678,\"start\":38677},{\"end\":38689,\"start\":38688},{\"end\":38691,\"start\":38690},{\"end\":38702,\"start\":38701},{\"end\":38704,\"start\":38703},{\"end\":38711,\"start\":38710},{\"end\":38713,\"start\":38712},{\"end\":38984,\"start\":38983},{\"end\":38986,\"start\":38985},{\"end\":39247,\"start\":39246},{\"end\":39257,\"start\":39256},{\"end\":39271,\"start\":39270},{\"end\":39280,\"start\":39279},{\"end\":39508,\"start\":39507},{\"end\":39518,\"start\":39517},{\"end\":39772,\"start\":39771},{\"end\":40109,\"start\":40108},{\"end\":40111,\"start\":40110},{\"end\":40419,\"start\":40418},{\"end\":40428,\"start\":40427},{\"end\":40790,\"start\":40789},{\"end\":40792,\"start\":40791},{\"end\":40807,\"start\":40806},{\"end\":40816,\"start\":40815},{\"end\":41215,\"start\":41214},{\"end\":41222,\"start\":41221},{\"end\":41228,\"start\":41227},{\"end\":41610,\"start\":41609},{\"end\":41622,\"start\":41621},{\"end\":41634,\"start\":41633},{\"end\":41646,\"start\":41645},{\"end\":41648,\"start\":41647},{\"end\":41982,\"start\":41981},{\"end\":41988,\"start\":41987},{\"end\":41995,\"start\":41994},{\"end\":42002,\"start\":42001},{\"end\":42010,\"start\":42009},{\"end\":42022,\"start\":42018},{\"end\":42397,\"start\":42396},{\"end\":42399,\"start\":42398},{\"end\":42409,\"start\":42408},{\"end\":42413,\"start\":42410},{\"end\":42780,\"start\":42779},{\"end\":42792,\"start\":42791},{\"end\":42794,\"start\":42793},{\"end\":42806,\"start\":42805},{\"end\":42813,\"start\":42812},{\"end\":43155,\"start\":43154},{\"end\":43163,\"start\":43162},{\"end\":43173,\"start\":43172},{\"end\":43182,\"start\":43181},{\"end\":43197,\"start\":43193},{\"end\":43539,\"start\":43538},{\"end\":43553,\"start\":43552},{\"end\":43565,\"start\":43564},{\"end\":43585,\"start\":43584},{\"end\":43593,\"start\":43592},{\"end\":43595,\"start\":43594},{\"end\":44006,\"start\":44005},{\"end\":44017,\"start\":44016},{\"end\":44019,\"start\":44018},{\"end\":44030,\"start\":44029},{\"end\":44032,\"start\":44031},{\"end\":44041,\"start\":44040},{\"end\":44052,\"start\":44051},{\"end\":44466,\"start\":44465},{\"end\":44474,\"start\":44473},{\"end\":44487,\"start\":44486},{\"end\":44497,\"start\":44496},{\"end\":44499,\"start\":44498},{\"end\":44511,\"start\":44506},{\"end\":44856,\"start\":44855},{\"end\":45200,\"start\":45199},{\"end\":45213,\"start\":45212},{\"end\":45215,\"start\":45214},{\"end\":45226,\"start\":45225},{\"end\":45564,\"start\":45563},{\"end\":45576,\"start\":45575},{\"end\":45584,\"start\":45583},{\"end\":45590,\"start\":45589},{\"end\":45946,\"start\":45945},{\"end\":45954,\"start\":45953},{\"end\":45960,\"start\":45959},{\"end\":45967,\"start\":45966},{\"end\":45974,\"start\":45973},{\"end\":46393,\"start\":46392},{\"end\":46784,\"start\":46783},{\"end\":47050,\"start\":47049},{\"end\":47061,\"start\":47060},{\"end\":47071,\"start\":47070},{\"end\":47073,\"start\":47072},{\"end\":47396,\"start\":47395},{\"end\":47403,\"start\":47402},{\"end\":47410,\"start\":47409},{\"end\":47705,\"start\":47704},{\"end\":47707,\"start\":47706},{\"end\":47717,\"start\":47716},{\"end\":47719,\"start\":47718},{\"end\":47729,\"start\":47728},{\"end\":47731,\"start\":47730},{\"end\":47739,\"start\":47738},{\"end\":47741,\"start\":47740},{\"end\":48057,\"start\":48056},{\"end\":48066,\"start\":48065},{\"end\":48075,\"start\":48074},{\"end\":48083,\"start\":48082},{\"end\":48364,\"start\":48363},{\"end\":48371,\"start\":48370},{\"end\":48378,\"start\":48377},{\"end\":48384,\"start\":48383},{\"end\":48390,\"start\":48389},{\"end\":48802,\"start\":48801},{\"end\":48811,\"start\":48810},{\"end\":49097,\"start\":49096},{\"end\":49103,\"start\":49102},{\"end\":49111,\"start\":49110},{\"end\":49455,\"start\":49454},{\"end\":49461,\"start\":49460},{\"end\":49469,\"start\":49468},{\"end\":49477,\"start\":49476},{\"end\":49716,\"start\":49715},{\"end\":49984,\"start\":49983},{\"end\":49997,\"start\":49993},{\"end\":50006,\"start\":50005},{\"end\":50013,\"start\":50012},{\"end\":50015,\"start\":50014},{\"end\":50306,\"start\":50305},{\"end\":50308,\"start\":50307},{\"end\":50314,\"start\":50313},{\"end\":50316,\"start\":50315},{\"end\":50325,\"start\":50324},{\"end\":50327,\"start\":50326},{\"end\":50539,\"start\":50538},{\"end\":50541,\"start\":50540},{\"end\":50551,\"start\":50550},{\"end\":50745,\"start\":50744},{\"end\":50747,\"start\":50746},{\"end\":51189,\"start\":51188},{\"end\":51197,\"start\":51196},{\"end\":51210,\"start\":51209},{\"end\":51220,\"start\":51219},{\"end\":51222,\"start\":51221},{\"end\":51234,\"start\":51229},{\"end\":51610,\"start\":51606},{\"end\":51878,\"start\":51877},{\"end\":51880,\"start\":51879},{\"end\":52101,\"start\":52100},{\"end\":52112,\"start\":52111},{\"end\":52123,\"start\":52122},{\"end\":52131,\"start\":52130},{\"end\":52133,\"start\":52132},{\"end\":52429,\"start\":52428},{\"end\":52634,\"start\":52633},{\"end\":52645,\"start\":52641},{\"end\":52654,\"start\":52653},{\"end\":52656,\"start\":52655},{\"end\":52666,\"start\":52665},{\"end\":53024,\"start\":53023},{\"end\":53031,\"start\":53030},{\"end\":53039,\"start\":53038},{\"end\":53046,\"start\":53045}]", "bib_author_last_name": "[{\"end\":38686,\"start\":38679},{\"end\":38699,\"start\":38692},{\"end\":38708,\"start\":38705},{\"end\":38719,\"start\":38714},{\"end\":38992,\"start\":38987},{\"end\":39254,\"start\":39248},{\"end\":39268,\"start\":39258},{\"end\":39277,\"start\":39272},{\"end\":39289,\"start\":39281},{\"end\":39515,\"start\":39509},{\"end\":39525,\"start\":39519},{\"end\":39777,\"start\":39773},{\"end\":40118,\"start\":40112},{\"end\":40425,\"start\":40420},{\"end\":40436,\"start\":40429},{\"end\":40804,\"start\":40793},{\"end\":40813,\"start\":40808},{\"end\":40823,\"start\":40817},{\"end\":41219,\"start\":41216},{\"end\":41225,\"start\":41223},{\"end\":41232,\"start\":41229},{\"end\":41619,\"start\":41611},{\"end\":41631,\"start\":41623},{\"end\":41643,\"start\":41635},{\"end\":41658,\"start\":41649},{\"end\":41985,\"start\":41983},{\"end\":41992,\"start\":41989},{\"end\":41999,\"start\":41996},{\"end\":42007,\"start\":42003},{\"end\":42016,\"start\":42011},{\"end\":42028,\"start\":42023},{\"end\":42406,\"start\":42400},{\"end\":42421,\"start\":42414},{\"end\":42789,\"start\":42781},{\"end\":42803,\"start\":42795},{\"end\":42810,\"start\":42807},{\"end\":42823,\"start\":42814},{\"end\":43160,\"start\":43156},{\"end\":43170,\"start\":43164},{\"end\":43179,\"start\":43174},{\"end\":43191,\"start\":43183},{\"end\":43203,\"start\":43198},{\"end\":43550,\"start\":43540},{\"end\":43562,\"start\":43554},{\"end\":43582,\"start\":43566},{\"end\":43590,\"start\":43586},{\"end\":43603,\"start\":43596},{\"end\":44014,\"start\":44007},{\"end\":44027,\"start\":44020},{\"end\":44038,\"start\":44033},{\"end\":44049,\"start\":44042},{\"end\":44061,\"start\":44053},{\"end\":44471,\"start\":44467},{\"end\":44484,\"start\":44475},{\"end\":44494,\"start\":44488},{\"end\":44504,\"start\":44500},{\"end\":44515,\"start\":44512},{\"end\":44859,\"start\":44857},{\"end\":45210,\"start\":45201},{\"end\":45223,\"start\":45216},{\"end\":45235,\"start\":45227},{\"end\":45573,\"start\":45565},{\"end\":45581,\"start\":45577},{\"end\":45587,\"start\":45585},{\"end\":45594,\"start\":45591},{\"end\":45951,\"start\":45947},{\"end\":45957,\"start\":45955},{\"end\":45964,\"start\":45961},{\"end\":45971,\"start\":45968},{\"end\":45977,\"start\":45975},{\"end\":46398,\"start\":46394},{\"end\":46793,\"start\":46785},{\"end\":47058,\"start\":47051},{\"end\":47068,\"start\":47062},{\"end\":47081,\"start\":47074},{\"end\":47400,\"start\":47397},{\"end\":47407,\"start\":47404},{\"end\":47413,\"start\":47411},{\"end\":47714,\"start\":47708},{\"end\":47726,\"start\":47720},{\"end\":47736,\"start\":47732},{\"end\":47752,\"start\":47742},{\"end\":48063,\"start\":48058},{\"end\":48072,\"start\":48067},{\"end\":48080,\"start\":48076},{\"end\":48087,\"start\":48084},{\"end\":48368,\"start\":48365},{\"end\":48375,\"start\":48372},{\"end\":48381,\"start\":48379},{\"end\":48387,\"start\":48385},{\"end\":48394,\"start\":48391},{\"end\":48808,\"start\":48803},{\"end\":48819,\"start\":48812},{\"end\":49100,\"start\":49098},{\"end\":49108,\"start\":49104},{\"end\":49115,\"start\":49112},{\"end\":49458,\"start\":49456},{\"end\":49466,\"start\":49462},{\"end\":49474,\"start\":49470},{\"end\":49480,\"start\":49478},{\"end\":49724,\"start\":49717},{\"end\":49991,\"start\":49985},{\"end\":50003,\"start\":49998},{\"end\":50010,\"start\":50007},{\"end\":50025,\"start\":50016},{\"end\":50311,\"start\":50309},{\"end\":50322,\"start\":50317},{\"end\":50334,\"start\":50328},{\"end\":50548,\"start\":50542},{\"end\":50554,\"start\":50552},{\"end\":50758,\"start\":50748},{\"end\":51194,\"start\":51190},{\"end\":51207,\"start\":51198},{\"end\":51217,\"start\":51211},{\"end\":51227,\"start\":51223},{\"end\":51238,\"start\":51235},{\"end\":51616,\"start\":51611},{\"end\":51885,\"start\":51881},{\"end\":52109,\"start\":52102},{\"end\":52120,\"start\":52113},{\"end\":52128,\"start\":52124},{\"end\":52139,\"start\":52134},{\"end\":52435,\"start\":52430},{\"end\":52639,\"start\":52635},{\"end\":52651,\"start\":52646},{\"end\":52663,\"start\":52657},{\"end\":52674,\"start\":52667},{\"end\":53028,\"start\":53025},{\"end\":53036,\"start\":53032},{\"end\":53043,\"start\":53040},{\"end\":53051,\"start\":53047}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14183150},\"end\":38865,\"start\":38647},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3715244},\"end\":39186,\"start\":38867},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":40327401},\"end\":39470,\"start\":39188},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18661981},\"end\":39665,\"start\":39472},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":620378},\"end\":40074,\"start\":39667},{\"attributes\":{\"id\":\"b5\"},\"end\":40351,\"start\":40076},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10892355},\"end\":40649,\"start\":40353},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3523993},\"end\":41096,\"start\":40651},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18289786},\"end\":41490,\"start\":41098},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":19215643},\"end\":41914,\"start\":41492},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3790207},\"end\":42265,\"start\":41916},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":19297463},\"end\":42686,\"start\":42267},{\"attributes\":{\"doi\":\"arXiv:1610.01683\",\"id\":\"b12\"},\"end\":43069,\"start\":42688},{\"attributes\":{\"id\":\"b13\"},\"end\":43454,\"start\":43071},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":135441418},\"end\":43887,\"start\":43456},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4787295},\"end\":44371,\"start\":43889},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":21690736},\"end\":44766,\"start\":44373},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":224980581},\"end\":45083,\"start\":44768},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":59304356},\"end\":45472,\"start\":45085},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206761818},\"end\":45853,\"start\":45474},{\"attributes\":{\"doi\":\"10.1109/JBHI.2020.3006145\",\"id\":\"b20\",\"matched_paper_id\":220977959},\"end\":46279,\"start\":45855},{\"attributes\":{\"doi\":\"10.1109/TETCI.2020.2996943\",\"id\":\"b21\",\"matched_paper_id\":226753588},\"end\":46717,\"start\":46281},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":53217607},\"end\":46954,\"start\":46719},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":70349928},\"end\":47304,\"start\":46956},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":219621375},\"end\":47651,\"start\":47306},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1554582},\"end\":47950,\"start\":47653},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":191206643},\"end\":48302,\"start\":47952},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":751749},\"end\":48705,\"start\":48304},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5808102},\"end\":49061,\"start\":48707},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":140309863},\"end\":49380,\"start\":49063},{\"attributes\":{\"doi\":\"arXiv:1505.00853\",\"id\":\"b30\"},\"end\":49686,\"start\":49382},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13756489},\"end\":49900,\"start\":49688},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":52967399},\"end\":50282,\"start\":49902},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b33\"},\"end\":50492,\"start\":50284},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6628106},\"end\":50742,\"start\":50494},{\"attributes\":{\"id\":\"b35\"},\"end\":51074,\"start\":50744},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52894597},\"end\":51536,\"start\":51076},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":44108477},\"end\":51813,\"start\":51538},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":30853087},\"end\":52025,\"start\":51815},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3384186},\"end\":52379,\"start\":52027},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":15926286},\"end\":52579,\"start\":52381},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14272714},\"end\":52909,\"start\":52581},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":59601722},\"end\":53389,\"start\":52911}]", "bib_title": "[{\"end\":38673,\"start\":38647},{\"end\":38981,\"start\":38867},{\"end\":39244,\"start\":39188},{\"end\":39505,\"start\":39472},{\"end\":39769,\"start\":39667},{\"end\":40106,\"start\":40076},{\"end\":40416,\"start\":40353},{\"end\":40787,\"start\":40651},{\"end\":41212,\"start\":41098},{\"end\":41607,\"start\":41492},{\"end\":41979,\"start\":41916},{\"end\":42394,\"start\":42267},{\"end\":43152,\"start\":43071},{\"end\":43536,\"start\":43456},{\"end\":44003,\"start\":43889},{\"end\":44463,\"start\":44373},{\"end\":44853,\"start\":44768},{\"end\":45197,\"start\":45085},{\"end\":45561,\"start\":45474},{\"end\":45943,\"start\":45855},{\"end\":46390,\"start\":46281},{\"end\":46781,\"start\":46719},{\"end\":47047,\"start\":46956},{\"end\":47393,\"start\":47306},{\"end\":47702,\"start\":47653},{\"end\":48054,\"start\":47952},{\"end\":48361,\"start\":48304},{\"end\":48799,\"start\":48707},{\"end\":49094,\"start\":49063},{\"end\":49713,\"start\":49688},{\"end\":49981,\"start\":49902},{\"end\":50536,\"start\":50494},{\"end\":51186,\"start\":51076},{\"end\":51604,\"start\":51538},{\"end\":51875,\"start\":51815},{\"end\":52098,\"start\":52027},{\"end\":52426,\"start\":52381},{\"end\":52631,\"start\":52581},{\"end\":53021,\"start\":52911}]", "bib_author": "[{\"end\":38688,\"start\":38675},{\"end\":38701,\"start\":38688},{\"end\":38710,\"start\":38701},{\"end\":38721,\"start\":38710},{\"end\":38994,\"start\":38983},{\"end\":39256,\"start\":39246},{\"end\":39270,\"start\":39256},{\"end\":39279,\"start\":39270},{\"end\":39291,\"start\":39279},{\"end\":39517,\"start\":39507},{\"end\":39527,\"start\":39517},{\"end\":39779,\"start\":39771},{\"end\":40120,\"start\":40108},{\"end\":40427,\"start\":40418},{\"end\":40438,\"start\":40427},{\"end\":40806,\"start\":40789},{\"end\":40815,\"start\":40806},{\"end\":40825,\"start\":40815},{\"end\":41221,\"start\":41214},{\"end\":41227,\"start\":41221},{\"end\":41234,\"start\":41227},{\"end\":41621,\"start\":41609},{\"end\":41633,\"start\":41621},{\"end\":41645,\"start\":41633},{\"end\":41660,\"start\":41645},{\"end\":41987,\"start\":41981},{\"end\":41994,\"start\":41987},{\"end\":42001,\"start\":41994},{\"end\":42009,\"start\":42001},{\"end\":42018,\"start\":42009},{\"end\":42030,\"start\":42018},{\"end\":42408,\"start\":42396},{\"end\":42423,\"start\":42408},{\"end\":42791,\"start\":42779},{\"end\":42805,\"start\":42791},{\"end\":42812,\"start\":42805},{\"end\":42825,\"start\":42812},{\"end\":43162,\"start\":43154},{\"end\":43172,\"start\":43162},{\"end\":43181,\"start\":43172},{\"end\":43193,\"start\":43181},{\"end\":43205,\"start\":43193},{\"end\":43552,\"start\":43538},{\"end\":43564,\"start\":43552},{\"end\":43584,\"start\":43564},{\"end\":43592,\"start\":43584},{\"end\":43605,\"start\":43592},{\"end\":44016,\"start\":44005},{\"end\":44029,\"start\":44016},{\"end\":44040,\"start\":44029},{\"end\":44051,\"start\":44040},{\"end\":44063,\"start\":44051},{\"end\":44473,\"start\":44465},{\"end\":44486,\"start\":44473},{\"end\":44496,\"start\":44486},{\"end\":44506,\"start\":44496},{\"end\":44517,\"start\":44506},{\"end\":44861,\"start\":44855},{\"end\":45212,\"start\":45199},{\"end\":45225,\"start\":45212},{\"end\":45237,\"start\":45225},{\"end\":45575,\"start\":45563},{\"end\":45583,\"start\":45575},{\"end\":45589,\"start\":45583},{\"end\":45596,\"start\":45589},{\"end\":45953,\"start\":45945},{\"end\":45959,\"start\":45953},{\"end\":45966,\"start\":45959},{\"end\":45973,\"start\":45966},{\"end\":45979,\"start\":45973},{\"end\":46400,\"start\":46392},{\"end\":46795,\"start\":46783},{\"end\":47060,\"start\":47049},{\"end\":47070,\"start\":47060},{\"end\":47083,\"start\":47070},{\"end\":47402,\"start\":47395},{\"end\":47409,\"start\":47402},{\"end\":47415,\"start\":47409},{\"end\":47716,\"start\":47704},{\"end\":47728,\"start\":47716},{\"end\":47738,\"start\":47728},{\"end\":47754,\"start\":47738},{\"end\":48065,\"start\":48056},{\"end\":48074,\"start\":48065},{\"end\":48082,\"start\":48074},{\"end\":48089,\"start\":48082},{\"end\":48370,\"start\":48363},{\"end\":48377,\"start\":48370},{\"end\":48383,\"start\":48377},{\"end\":48389,\"start\":48383},{\"end\":48396,\"start\":48389},{\"end\":48810,\"start\":48801},{\"end\":48821,\"start\":48810},{\"end\":49102,\"start\":49096},{\"end\":49110,\"start\":49102},{\"end\":49117,\"start\":49110},{\"end\":49460,\"start\":49454},{\"end\":49468,\"start\":49460},{\"end\":49476,\"start\":49468},{\"end\":49482,\"start\":49476},{\"end\":49726,\"start\":49715},{\"end\":49993,\"start\":49983},{\"end\":50005,\"start\":49993},{\"end\":50012,\"start\":50005},{\"end\":50027,\"start\":50012},{\"end\":50313,\"start\":50305},{\"end\":50324,\"start\":50313},{\"end\":50336,\"start\":50324},{\"end\":50550,\"start\":50538},{\"end\":50556,\"start\":50550},{\"end\":50760,\"start\":50744},{\"end\":51196,\"start\":51188},{\"end\":51209,\"start\":51196},{\"end\":51219,\"start\":51209},{\"end\":51229,\"start\":51219},{\"end\":51240,\"start\":51229},{\"end\":51618,\"start\":51606},{\"end\":51887,\"start\":51877},{\"end\":52111,\"start\":52100},{\"end\":52122,\"start\":52111},{\"end\":52130,\"start\":52122},{\"end\":52141,\"start\":52130},{\"end\":52437,\"start\":52428},{\"end\":52641,\"start\":52633},{\"end\":52653,\"start\":52641},{\"end\":52665,\"start\":52653},{\"end\":52676,\"start\":52665},{\"end\":53030,\"start\":53023},{\"end\":53038,\"start\":53030},{\"end\":53045,\"start\":53038},{\"end\":53053,\"start\":53045}]", "bib_venue": "[{\"end\":38726,\"start\":38721},{\"end\":38999,\"start\":38994},{\"end\":39303,\"start\":39291},{\"end\":39545,\"start\":39527},{\"end\":39810,\"start\":39779},{\"end\":40156,\"start\":40120},{\"end\":40475,\"start\":40438},{\"end\":40843,\"start\":40825},{\"end\":41265,\"start\":41234},{\"end\":41677,\"start\":41660},{\"end\":42061,\"start\":42030},{\"end\":42454,\"start\":42423},{\"end\":42777,\"start\":42688},{\"end\":43236,\"start\":43205},{\"end\":43641,\"start\":43605},{\"end\":44100,\"start\":44063},{\"end\":44540,\"start\":44517},{\"end\":44892,\"start\":44861},{\"end\":45254,\"start\":45237},{\"end\":45633,\"start\":45596},{\"end\":46035,\"start\":46004},{\"end\":46481,\"start\":46426},{\"end\":46813,\"start\":46795},{\"end\":47091,\"start\":47083},{\"end\":47450,\"start\":47415},{\"end\":47775,\"start\":47754},{\"end\":48103,\"start\":48089},{\"end\":48458,\"start\":48396},{\"end\":48854,\"start\":48821},{\"end\":49175,\"start\":49117},{\"end\":49452,\"start\":49382},{\"end\":49762,\"start\":49726},{\"end\":50059,\"start\":50027},{\"end\":50303,\"start\":50284},{\"end\":50589,\"start\":50556},{\"end\":50871,\"start\":50760},{\"end\":51277,\"start\":51240},{\"end\":51645,\"start\":51618},{\"end\":51892,\"start\":51887},{\"end\":52172,\"start\":52141},{\"end\":52456,\"start\":52437},{\"end\":52716,\"start\":52676},{\"end\":53096,\"start\":53053},{\"end\":40184,\"start\":40158},{\"end\":48512,\"start\":48460},{\"end\":48883,\"start\":48856},{\"end\":49229,\"start\":49177},{\"end\":49790,\"start\":49764},{\"end\":50087,\"start\":50061},{\"end\":50618,\"start\":50591},{\"end\":53131,\"start\":53098}]"}}}, "year": 2023, "month": 12, "day": 17}