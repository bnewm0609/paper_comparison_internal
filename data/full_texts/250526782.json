{"id": 250526782, "updated": "2023-10-05 12:31:40.922", "metadata": {"title": "ReAct: Temporal Action Detection with Relational Queries", "authors": "[{\"first\":\"Dingfeng\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Yujie\",\"last\":\"Zhong\",\"middle\":[]},{\"first\":\"Qiong\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Jing\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lin\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Jia\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Dacheng\",\"last\":\"Tao\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This work aims at advancing temporal action detection (TAD) using an encoder-decoder framework with action queries, similar to DETR, which has shown great success in object detection. However, the framework suffers from several problems if directly applied to TAD: the insufficient exploration of inter-query relation in the decoder, the inadequate classification training due to a limited number of training samples, and the unreliable classification scores at inference. To this end, we first propose a relational attention mechanism in the decoder, which guides the attention among queries based on their relations. Moreover, we propose two losses to facilitate and stabilize the training of action classification. Lastly, we propose to predict the localization quality of each action query at inference in order to distinguish high-quality queries. The proposed method, named ReAct, achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Besides, extensive ablation studies are conducted to verify the effectiveness of each proposed component. The code is available at https://github.com/sssste/React.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.07097", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ShiZCZ00T22", "doi": "10.48550/arxiv.2207.07097"}}, "content": {"source": {"pdf_hash": "3ddafae9cf565edb458b485ca27f7f49254ba95b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2207.07097v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "aaa4f29ab61f20300d5b8329dd568dc7eb6a6ed8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3ddafae9cf565edb458b485ca27f7f49254ba95b.txt", "contents": "\nReAct: Temporal Action Detection with Relational Queries\n\n\nDingfeng Shi \nSchool of Computer Science and Engineering\nState Key Laboratory of Virtual Reality Technology and Systems\nBeihang University\n\n\nYujie Zhong \nMeituan Inc 3 JD Explore Academy\n\n\nQiong Cao \n\u2020 \nJing Zhang \nThe University of Sydney\n\n\nLin Ma \nMeituan Inc 3 JD Explore Academy\n\n\nJia Li \nSchool of Computer Science and Engineering\nState Key Laboratory of Virtual Reality Technology and Systems\nBeihang University\n\n\nDacheng Tao \nThe University of Sydney\n\n\nReAct: Temporal Action Detection with Relational Queries\n\nThis work aims at advancing temporal action detection (TAD) using an encoder-decoder framework with action queries, similar to DETR, which has shown great success in object detection. However, the framework suffers from several problems if directly applied to TAD: the insufficient exploration of inter-query relation in the decoder, the inadequate classification training due to a limited number of training samples, and the unreliable classification scores at inference. To this end, we first propose a relational attention mechanism in the decoder, which guides the attention among queries based on their relations. Moreover, we propose two losses to facilitate and stabilize the training of action classification. Lastly, we propose to predict the localization quality of each action query at inference in order to distinguish high-quality queries. The proposed method, named ReAct, achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Besides, extensive ablation studies are conducted to verify the effectiveness of each proposed component.\n\nIntroduction\n\nTemporal action detection (TAD) has been actively studied because of the deep learning era. Inspired by the advance of one-stage object detectors [22,32,10], many recent works focus on one-stage action detectors [18], which show excellent performance while having a relatively simple structure. On the other hand, DETR [4], which tackles object detection in a Transformer encoder-decoder framework, attracted considerable attention. In this work, we propose a novel one-stage action detector ReAct that is based on such a learning paradigm. Inspired by DETR, ReAct models action instances as a set of learnable action queries. These action queries are fed into the decoder as inputs, and they iteratively attend to the output features of the encoder as well as update their predictions. The action classification and localization are then predicted by two simple feedforward neural nets.\n\nHowever, the DETR-like methods suffer from several problems when applied to TAD task. First, the inter-query relations are not fully explored by the selfattention in the decoder, which is performed densely over all the queries. Second, DETR-like methods may suffer from the inadequate training of action classification since the number of positive training samples for the classifier is relatively small compared to anchor-based/free methods. Moreover, when multiple queries fire for the same action instance at inference, queries with higher classification scores may not necessarily have better temporal localization. In the following, we elaborate on these problems and introduce the proposed methods to alleviate them in three aspects: attention mechanism, training losses, and inference.\n\nThe decoder in DETR-like methods applies the self-attention over the action queries to capture their relations, which can not fully explore the complex relations among queries. In this work, we denote the action queries that are responsible for localizing different action instances of similar or same action classes as distinct-similar queries, and those detecting different action classes as distinct-dissimilar queries. For the queries that fire for the same action instance, we regard them as duplicate queries. In this work, we propose a novel attention mechanism, named Relational Attention with IoU Decay (RAID), to explicitly handle these three types of query relations in the decoder. As Fig. 1 shows, RAID focuses on the communication among distinct-similar queries (since they are expected to provide more informative signals) and blocks the attention between distinct-dissimilar and duplicate queries. Furthermore, the proposed IoU decay encourages the duplicate queries to be slightly different from each other to enable a more diverse prediction.\n\nAnother problem is that a DETR-like approach may have a relatively low classification accuracy due to inadequate classification training. This is because the positive training samples for the classification of DETR-like methods are much fewer than those of the anchor-free methods. Namely, for DETR-like methods, the number of positives per input clip is only the same as the ground truth actions because of the bipartite-matching-based label assignment. To address this problem, we propose two training losses, codenamed Action Classification Enhancement (ACE) losses, to facilitate the classification learning. The first loss ACE-enc is applied to the input features of the encoder and is designed to reduce the intra-class variance and inter-class similarity of action instances. This loss explicitly improves the discriminability of video features regarding acting classes, thus benefiting the classification. Meanwhile, a ACE-dec loss is proposed as the classification loss in the decoder, which considers both the predicted segments and the ground-truth segments for action classification. It increases the training samples and generates a stable learning signal for the classifier.\n\nLastly, the action queries are redundant by design compared to the actual action instances. At inference, it is a common situation where multiple actions queries fire for the same action instance. Hence, it is important to focus on precise action localization queries. Nonetheless, the classification score is deficient in measuring the temporal localization quality. As a result, we propose a Segment Quality to predict the localization quality of each action query at inference, such that the more high-quality queries can be distinguished.\n\nTo summarize, we make the following contributions in this work:\n\n-We approach temporal action detection using a DETR-like framework and identify three limitations of such method when directly applied to TAD. -We propose the relational attention with IoU decay, the action classification enhancement losses, and the segment quality prediction, which alleviate the identified problems from the perspectives of attention mechanism, training losses, and network inference, respectively. -Experiments on two action detection benchmarks demonstrate the superiority of ReAct: it achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Extensive ablation studies are conducted to verify the effectiveness of each component.\n\n\nRelated Work\n\nTemporal action detection. Temporal action detection (TAD) aims to detect all the start and end timestamps and the corresponding action types based on the video stream information. The existing methods can be roughly divided into two categories: two-stage methods and one-stage methods. Two-stage methods [11,28,38,43,19,21,12,17] split the detection task into two subtasks: proposal generation and proposal classification. Concretely, some methods [21,17,19] generate the proposals by predicting the probability of the start point and endpoint of the action and then selecting the proposal segments according to prediction score. In addition, PGCN [43] considers the relationship between proposals, then refines and classifies the proposals by Graph Convolutional Network. These twostage methods can perform better by combining proposal generation networks and proposal classification networks. However, they can not be trained in an endto-end manner and are computationally inefficient. To solve the above problems, some one-stage methods [20,6,18,26,37] are proposed. Some works [6,18,40] try to adapt to the high variance of the action duration by constructing a temporal feature pyramid, while Liu et al. [23] propose to dynamically sample temporal features by learnable parameters. These one-stage methods reduce the complexity of the models, which are more computationally friendly. In this work, we mainly follow the one-stage fashion and the deformable convolution design [9,47,23] to build a efficient action detector, which will be detailed in the Section 3.\n\nAttention-based model. Attention-based models [33] have achieved great success in machine translation and been extended to the field of computer vision [25,1,24,39,44,8] in recent years. The attention module computes a soft weight dynamically for a set of points at runtime. Concretely, DETR [4] proposes a Transformer-based image detection paradigm. It learns decoder input features shared by all input videos and detects a fixed number of outputs. Deformable DETR [47] improves DETR by reducing the number of pairs to be computed in the attention module with learnable spatial offsets. Liu et al. [23] propose an end-to-end framework for TAD based on Deformable DETR. This type of training paradigm is highly efficient and fast in prediction. However, there is still a performance gap between these methods and the latest methods in TAD [23,43]. Our work is built on DETR-like workflows. In contrast to the above work, our approach suppresses the flow of invalid information by constricting a computational subset for the attention module, which improves performance effectively.\n\nContrastive learning. Contrastive learning [7] is a method that has been widely used in unsupervised learning. NCE [13] mines data features by distinguishing between data and noise. Info-NCE [27] is proposed to extract representations from high-dimensional data with a probabilistic contrastive loss. Lin et al. [18] leverage contrastive learning to help network identify action boundaries. Inspired by these works, we use contrastive learning to extract a global common representation of action categories and enlarge the feature distance between action segments and noise segments.\n\n\nMethod\n\nProblem definition. This work focuses on the problem of temporal action detection (TAD). Specifically, given a set of untrimmed videos\nD = {V i } n i=1 . A set of {X i , Y i } can be extracted from each video V i , where X i = {x t } T t=1\ncorresponds to the image (and optical flow) features of T snippets and Y i = {m k , d k , c k } Ki k=1 is K i segment labels for the video V i with the action segment midpoint time m k , the action duration d k and the corresponding action category c k . Temporal action detection aims at predicting all segments Y i based on the input feature X i . Method overview. Motivated by DETR [4], we approach the problem of TAD by an encoder-decoder framework based on the transformer network. As Fig. 3 shows, the overall architecture of ReAct contains three parts: a video feature extractor, an action encoder, and an action decoder. First, video clip features are extracted from each RGB frame by using the widely-used 3D-CNN (e.g., TSN [35] or I3D [5]). The optical flow features are also extracted using TVL1 optical flow algorithm [42]. Following that, a 1-D conv layer is used to modify the feature dimension of the clip features. The output features are then passed to the action encoder, which is a L E -layer transformer network. The encoded clip features serve as one of the inputs to the action decoder. The decoder is a L D -layer transformer, and it differs from the encoder in two aspects. It has action queries (which are learnable embeddings) as inputs, and the queries attend the encoder outputs in each layer of the decoder, known as Cross-attention. Essentially, ReAct maps action instances as a set of action queries. The action queries are transformed by the decoder into output embeddings which are used for both action classification and temporal localization by separate feed-forward neural nets. The details of the encoder structure are provided in the appendix. At training, following previous works [4,47,23], the Hungarian Algorithm [16] is applied to assign labels to the action queries. The edge weight is defined by the summation of the segment IoU, the probability of classification, and the L1 norm between two coordinates. Based on the matching, ReAct applies several losses to the action queries, including the action classification loss and temporal segment regression loss.\n\nLimitations of DETR-like methods for TAD. DETR-like methods may suffer from several problems when applied to TAD task. First, the decoder performs the self-attention densely over all the queries, which causes the inter-query relations not to be sufficiently explored. Second, compared with anchor-based/free methods, DETR-like methods may have issues in deficit training of action classification attributed to relatively smaller number of positive training samples for the classifier. Third, queries with higher classification scores may not be reliable due to multiple queries firing for the same action instance at inference.\n\nIn this work, we mitigate these problems in three aspects: (1) We propose the Relational Attention with IoU Decay which allows each action query to attend to others in the decoder based on their relations; (2) We design two Action Classification Enhancement losses to enhance the action classification learning at the encoder and decoder, respectively; (3) We introduce a Segment Quality to predict the localization quality of each action query at inference to compensate the deficiency of classification score at inference. We elaborate on these three aspects in the following. \n\n\nRelational Attention with IoU Decay\n\nTo better explore the inter-query relation in the decoder, we present the Relational Attention with IoU Decay (RAID) which replaces the self-attention in the transformer decoder. Below, we describe the proposed method in detail.\n\nRelational attention. As a recap, we define three types of queries with respect to an action query q i , which are differentiated by their relations to q i . Distinctsimilar queries are the queries that try to detect different action instances but of similar (or same) action class to q i . Distinct-dissimilar queries are those which try to detect different action instances and of dissimilar action class to q i . Duplicate queries are the queries that try to detect the same action instance as q i . Intuitively, we anticipate that attending to distinct-dissimilar queries does not provide informative signals to q i , since they focus on different action classes, and the relation between action classes may not be a reliable cue for detecting actions. On the contrary, attending to distinct-similar queries can benefit the query q by gathering some background information and cues around q i . For example, some actions may occur multiple times in a clip, and attending to each other can increase the confidence of the detection. Moreover, duplicate queries only repeat the prediction as q i , so they bring no extra information and should be ignored in the attention for q i .\n\nTo find the distinct-similar queries for a query q i , we consider two properties, namely, high context similarity and low temporal overlap. To measure context similarity, we compute a similarity matrix A \u2208 R Lq\u00d7Lq (L q is the number of queries) based on the query features, where each element represents the cosine similarity of two queries. Then the query-pair set E sim is constructed by\nE sim = {(i, j)|A[i, j] \u2212 \u03b3 > 0},(1)\nwhere \u03b3 \u2208 [\u22121, 1] is a preset similarity threshold. To identify the queries having low temporal overlap with q, a natural strategy is using the Interaction of Union (IoU) in the time domain, which measures the overlap between two temporal segments. Therefore, we compute a pair-wise IoU matrix B \u2208 R Lq\u00d7Lq for the reference segments and construct a query-pair set E IoU as follows:\nE IoU = {(i, j)|B[i, j] \u2212 \u03c4 < 0},(2)\nwhere i and j denote the i-th and j-th queries respectively, and \u03c4 \u2208 [0, 1] is a preset IoU threshold. As shown in Fig. 3, this simple strategy removes the segments which have large temporal overlap. We can then define the distinctsimilar query-pair set E by combining E sim , E IoU and the query itself E s . The definition is given as follow:\nE = (E IoU \\ E sim ) \u222a E s .(3)\nFor a query q i and its distinct query-pair set E i , the key and value features can be written as\nK i = concatenate({k j |(i, j) \u2208 E i }) and V i = concatenate({v j |(i, j) \u2208 E i }).\nThen, the query features q i are updated by\nq \u2032 i = a i V T i ,(4)\nwhere the attention weight a i is\na i = Sof tmax K (q i K T i ).(5)\nNote that by considering both the context similarity and temporal overlap, the proposed relational attention successfully preserves the communication between q i and useful queries while blocking that between uninformative ones.\n\nIoU decay. Apart from relational attention, we introduce a further improvement by handling duplicate queries. Namely, we propose a regularization, termed as IoU decay, which is added to the network optimization. It is given as\n\u03c9 d = 1 2 Lq i=1 Lq j=1\nIoU (s i , s j ).\n\nDuring the detector training, it penalizes the IoU between queries, such that duplicate queries can be diversified and different from each other, which can increase the probability of obtaining a more precise localization for the target action instance.\n\n\nAction Classification Enhancement\n\nTo combat the issue of the inadequate learning of classification when applying the DETR-like methods to TAD, we propose two Action Classification Enhancement (ACE) losses to boost the classification performance. ACE-enc loss. We aim to enhance the features with respect to action classification in the phase of encoder by enlarging the similarity of inter-class action instances and reducing the variance between intra-class action instances. We posit that explicitly increasing the discriminability of the features on the action detection dataset in an early stage can also benefit the final action classification. Specifically, we optimize the input features of the encoder using contrastive loss.\n\nThe positive and negative action instance pairs are constructed as follows. For a given ground-truth action segment s g and its category c g in a video v i , we choose its positive instances by sampling the action segments of the same category c g from either the same or different videos. As for its negative instances, we choose them from two different sources: (1) segments of action categories different from c g and (2) segments that are completely inside the ground-truth segment, but their IoU is less than a specific threshold \u03be.\n\nFor a given segment s, we denote x \u2208 R T \u00d7D \u2032 and x \u2208 R T \u00d7D as the pretrained video feature and feature further projected by a fully-connected layer l (i.e. x = l(x)), respectively. Then, the segment feature after temporal RoI pooling [38] can be denoted as f = RoI( x, s) \u2208 R D . With the above definitions, the loss L ACE\u2212enc is given by\nL ACE\u2212enc = \u2212 log exp(f T f p ) j\u2208D exp(f T f j ) ,(7)\nwhere f p is a positive segment of f and D is the index of k random negative instances as well as a positive instance. ACE-dec loss. Anchor-based/free methods treat all (or multiple) the temporal locations within the ground truth action segment as positives (i.e., belonging to an action class rather than backgrounds) for training the action classifiers, whereas DETR-like methods have much fewer positives due to the bipartite matching at label assignment. We, therefore, propose the ACE-dec loss to train the action classifiers.\n\nAs Fig. 3 (right) shows, in the training phase, an additional positive training sample is fed to the action classifiers for each query segment (i.e., the green one) matched with a ground-truth action instance. The additional positive is obtained by feeding the ground-truth segment (i.e., the yellow one) as a normal query segment to the cross-attention layer. The details of the cross-attention layer are described in the supplementary material.\n\nConcretely, every decoder layer is attached a ACE-dec loss which is given by\nL ACE\u2212dec = L q f oc + 1 y\u0338 =\u2205 [L gt f oc ],(8)\nwhere L q f oc and L gt f oc is the sigmoid Focal Loss [22] for the query and ground-truth classification loss respectively. Note that, only the queries that are matched to ground-truth segments will contribute the ground-truth classification loss.\n\n\nSegment Quality Prediction\n\nTo remedy the problem that classification score is unreliable for selecting the best query among a set of duplicate queries, we propose a Segment Quality to predict the localization quality of each action query at inference for distinguishing highquality queries. The proposed segment quality prediction considers both the midpoint of the segment as well as its temporal coverage on the action instance.\n\nConcretely, given a predicted segment s q and its query feature f q , we define (\u03b6 1 , \u03b6 2 ) = \u03d5(f q ), where \u03d5 is a single fully-connected layer and \u03b6 1 , \u03b6 2 \u2208 [0, 1]. Then, a final quality value \u03b6 is defined by \u03b6 = \u03b6 1 \u00b7 \u03b6 2 . Segment Quality is supervised by a two-dimensional vector composing of the offset of the predicted midpoint and its ground truth for localizing the midpoint precisely, and the IoU between the predicted segment and its closest ground-truth segment for accurate temporal localization and coverage. The overall loss is given by\nL \u03b6 = \u03d5(f q ) \u2212 (exp(\u2212 1 l gt |m q \u2212 m gt |), IoU (s q , s gt )) 1 ,(9)\nwhere m q is the midpoint of the predicted segment, and m gt , l gt are the midpoint and length of the ground-truth segment, respectively. At inference, \u03b6 is multiplied with the classification score of the segment.\n\n\nTraining Losses\n\nAt training, based on the label assignment by the Hungarian Algorithm, ReAct is trained by the total loss as follow:\nL = L ACE\u2212enc + L ACE\u2212dec + L \u03b6 + L reg .(10)\nHere, L reg is the commonly used regression loss for TAD which regresses the midpoint and the duration of the detected segments using the summation of L1 distance and the generalized IoU distance [29] for the matched pair. We define each objective as follows:\nL reg = 1 N cgt\u0338 =\u2205 j\u2208Lq 1 c (j) gt \u0338 =\u2205 [\u03b3 1 L (j) L1 + \u03b3 2 L (j) gIoU ], L (j) L1 = |m (j) gt \u2212 m (j) | + |d (j) gt \u2212 d (j) |, L (j) gIoU = 1 \u2212 gIoU (s (j) gt , s (j) ),(11)\nwhere s (j) = (m (j) , d (j) ) is j-th detected segment represented by midpoint and the duration. c gt . In addition, we follow the segment refinement fashion [47,23] to predict detections in each decoder layer, each of which will be updated by summing with the upper layer segment and re-normalizing it. In this way, each layer provides auxiliary classification loss L \u2032 cls and regression loss L \u2032 reg , which further helps the network training.\n\n\nExperiment\n\nWe conduct experiments on two challenging datasets: THUMOS14 [14] and ActivityNet-1.3 [3].\n\n\nImplementation Details\n\nArchitecture details. Optimization parameters and inference. We train the ReAct with AdamW optimizer with a batch size of 16. The learning rate is set to 2\u00d710 \u22124 and 1\u00d710 \u22124 for THUMOS14 and ActivityNet-1.3 respectively. ReAct is trained for 15 epochs on THUMOS14 and 35 epochs on ActivityNet-1.3. At inference, the classification head output is activated by sigmoid. Then all the predictions will be processed with Soft-NMS [2] to remove the redundant and low-quality segments.\n\n\nMain Results\n\nOn THUMOS14 (see Tab. 1), our ReAct achieves superior performance and suppresses the state-of-the-art one-stage and two-stage methods in mAP at different thresholds. In particular, ReAct achieves 55.0% in the average mAP, which outperforms TadTR by a large margin, namely about the 9.4% absolute improvement. Besides, we compare the computational performance during testing. We adopt Floating-point operations per second (FLOPs) per clip following the previous works. [23,48]. We can see that our model has FLOPS of 0.68G, which is 0.06G lower than TadTr and much lower than all the other methods. Note that the FLOPS we report in the table does not include the computation of video feature extraction with backbone. For methods like AFSD, which fine-tunes the backbone and does feature extraction during testing, we ignore the computation of feature extraction and only report the FLOPs afterward. On ActivityNet-1.3, our method achieves comparable results to the stateof-the-art (See Tab. 2). The ReAct outperforms the other DETR-based methods while enjoying a low computational cost (e.g., 0.38G). The Actioness and Anchor-based methods tend to have higher performance compared with the DETR-based methods. One possible reason is that the DETR-based methods take learnable query embedding as input, which is video-agnostic and only keeps statistical information. For a dataset with a large variance in action time, a query feature has to take both long and short action into account (See appendix for more details) and is prone to conflicts.\n\n\nAblation Study\n\nIn this section, we conduct the ablation studies on the THUMOS14 dataset.\n\nMain components. We demonstrate the effectiveness of three proposed components in ReAct: RAID, ACE, and Segment Quality. From Tab. 3 (row 2 and row 5), we can see that compared with the plain deformable decoder layer, our RAID brings about a 3.7% absolute improvement in the average mAP, proving the effectiveness of the module by introducing the relational attention based on the defined distinct-similar, distinct-dissimilar and duplicated queries. Besides, from rows 4 and 5 of the Table, we see our ACE improves the average mAP performance by 2.9%, which shows its effectiveness by designing new losses to enhance classification learning. Finally, from rows 3 and 5, the proposed Segment Quality achieves 2.8% improvements in average mAP, which effectively estimates the predicted segments' quality at inference.\n\n\nAnalysis of RAID.\n\nWe study the effect of two hyperparameters \u03b3 and \u03c4 in Section 3.1 for thresholding the similarity scores and IoU values when constructing the distinct similar and dissimilar query sets. First, we set \u03c4 = 1 and plot the average mAP when varying \u03b3. From Fig. 5(a) we see that as \u03b3 increases, the mAP exhibits an increase followed by a decrease, with a peak at \u03c4 = 0.2. Besides, we observe that the detection performance shows greater volatility as \u03c4 decreases further (i.e., \u03c4 < \u22120.1). Intuitively, smaller \u03c4 leads to more irrelevant query pairs communicating, thus introducing greater uncertainty. Next, we study the effect of the choice of \u03c4 by fixing \u03b3 = 0.2. From Fig. 5   similar trend with the figure for similarity as \u03c4 changes, and the optimal value is obtained at 0.5. Notice that the smaller \u03c4 is, the more queries will be excluded, and when \u03c4 = 0, only those that do not overlap will be retained. Intuitively, partially overlapped queries tend to be in the vicinity of the target query, which helps to perceive the information near the boundary. A visualized example of the queries is presented in Fig. 4.3 to illustrate the work of RAID.\n\nAnalysis of ACE. We analyze the effect of ACE-enc loss in the following aspects: the construction of contrastive pairs, where to apply ACE-enc loss and training losses. First, we study how contrastive pairs affect performance. In particular, to form the positive segment pairs, we randomly choose segments of the same category from either the same video or different videos, denoted by S1 and S2, respectively. As for negative pairs, there are two ways: segment pairs belonging to different action classes (denoted by N1), and segment pairs that one completely includes the other, but their IoU is less than a threshold (denoted by N2), as described in 3.2. Tab. 4 presents the results using different combinations of positive and negative pairs. In Tab. 4, we see that N2 play a Secondly, we study the effect of where to apply ACE-enc loss. We mainly consider two positions: before the transformer encoder and after it. We train a single fully connected layer for the former to enhance the video features. For the latter, we use the encoder output. The experimental results show that a single fully connected layer is much better than a complex transformer encoder. Intuitively, after encoder processing, the features on each frame already contain local temporal information, therefore, the pooled segment features can not represent the action precisely, leading to inaccurate convergence.\n\nFinally, to go deeper into the ACE-dec loss, we conducted three experiments: query classification loss only, ground-truth classification loss only, and the complete ACE-dec loss. For the case of the ground-truth classification loss only, we still predict and match the ground-truth segment with the input query feature, which provides the matched query position and reference ground-truth segment. However, we only update the network with ground-truth classification loss L gt f oc . From the Tab. 4, neither L q f oc nor L gt f oc can perform well, but when we combine them together, the result are significantly better (e.g., 53.4 versus 51.2).\n\n\nConclusion\n\nIn this work, we consider the task of temporal action detection and propose a novel one-stage action detector ReAct based on a DETR-like learning framework. Three limitations of such a method when directly applied to TAD are identified. We propose the relational attention with IoU decay, the action classification enhancement losses, and the segment quality prediction and handle those issues from three aspects: attention mechanism, training losses, and network inference, respectively. ReAct achieves the state-of-the-art performance with much lower computational costs than previous methods on THUMOS14. Extensive ablation studies are also conducted to demonstrate the effectiveness of each proposed component. In the future, we plan to include the video feature extractor in the action detection training to improve the performance further.\n\n\nA Encoder in Detail\n\nTo be self-contained, we provide the detailed structure of the encoder. As Fig.  1 shows, for the input video feature F \u2208 R T \u00d7D , a local offset position and attention weight will be predicted with two fully-connected layers, respectively. \n\n\nB Decoder in Detail\n\nTo help understand our method better, we introduce the decoder in detail. There are two attention modules in the decoder: the proposed relational attention module and a cross-attention module.\n\nIn the following, we elaborate on the deformable cross-attention module. As Fig. 2 showed, reference segment, offset position, and attention weights are predicted by three fully-connect layers, based on which the network samples sparse features to update the query feature at each decoder layer. There are two main differences in the deformable attention module between the encoder and decoder. First, the inputs and outputs are different. The input of the crossattention in the decoder is the queries, while the input of the encoder is video features. The second difference is the reference segment. In the encoder, temporal offsets for each frame are sampled only around that frame. Whereas for the crossattention module, an additional reference segment length is predicted for each query feature, and the offsets are normalizes such that the sampled frames are always in the segment.\n\n\nC Architecture and Training Detail\n\nFor THUMOS14, following [38], we use the TSN network [35] pre-trained on Kinetics [15] to extract features, which are then down-sampled every five frames. Each video feature is cropped in sequence with a window size 256, and two adjacent windows will have 192 overlapped features with a stride rate of 0.25. In the training phase, ground-truth cut by windows over 75% duration will be kept, and all empty windows without any ground-truth are removed. Finally, all ground-truth coordinates are re-normalized to the window coordinate system. we set L q = 40, L E = 2, L D = 4 for the number of queries, encoder layer and decoder layer, respectively. Each deformable attention module will sample 4 temporal offsets for computing the attention. The hidden layer dimension of the feedforward network is set to 1024, and the other hidden feature dimension in the intermediate of the network is all set to 256. The pair-wise IoU threshold \u03c4 and feature similarity threshold \u03b3 in ACE module are set to 0.5 and 0.2, respectively. For ActivityNet, the pre-trained TSN network by Xiong et al. [36] is adopted to extract features. Then each video feature downsamples every 16 frames, and the resultant feature will be rescaled to 100 snippets using linear interpolation. We only do video-level detection instead of window-level. We set the L q = 60, L E = 3, L D = 4. We sample 4 temporal offsets for the deformable module. The dimension of hidden features is set to 256, and we set the pair-wise IoU threshold \u03c4 and feature similarity threshold \u03b3 to 0.9 and -0.2, respectively. Following previous works [38,43,46,40], we combined the Untrimmed-Net videolevel classification results [34] with our classification score.\n\n\nD Visualization of the Classification Loss\n\nTo further demonstrate the effect of ACE-dec loss, we compute the classification loss for the Activitynet-1.3 test set. As Fig. 3 shows, compared to the Focal Loss, the ACE-dec loss improves not only the convergence speed but also the accuracy.  Fig. 3. Visualization of the test classification loss. We record the testing loss with or without ACE-dec loss during training\n\nFig. 1 .\n1The relation of queries. We choose the green one as the reference query , and the queries in a different relation to it are labeled with different colors. Only the Distinctsimilar pair (Blue ones) will be kept for attention computation.\n\nFig. 2 .\n2Illustration of the proposed framework. The video feature is extracted by a pretrained backbone, followed by a fully-connected layer to project the feature, and is additionally supervised by the AEC-Enc loss. After enhancement by the Transformer encoder, the features are fed into the decoder and attended by Lq action queries in the decoder. The classification head is trained with the proposed ACE-Dec loss.\n\nFig. 3 .\n3Illustration of our decoder. Left: plain deformable decoder. Each query performs the attention operation with all other query features and sample segment features from the encoder output. Right: decoder of ReAct. Each query only attends to specific queries based on the inter-query relation. Besides, the ground-truth segment provides an additional loss to further supervise the classification head. Note that for clarity, the LayerNorm, FFN, and residual connection are not shown in the figure (see appendix for detailed network structure).\n\n\nis a set of the ground-truth segments that s j is matched and N cgt\u0338 =\u2205 is the number of segments in c\n\n\nFor THUMOS14, we set L q = 40, L E = 2, L D = 4 for the number of queries, encoder layer and decoder layer, respectively. Each deformable attention module samples 4 temporal offsets for computing the attention. The hidden layer dimension of the feedforward network is set to 1024, and the other hidden feature dimension in the intermediate of the network is all set to 256. The pair-wise IoU threshold \u03c4 and feature similarity threshold \u03b3 in ACE module are set to 0.2 and 0.2, respectively. For ActivityNet-1.3, we set L q = 60, L E = 3, L D = 4, \u03c4 = 0.9, \u03b3 = \u22120.2. We sample 4 temporal offsets for the deformable module. For more implementation details including feature extraction and training details, please refer to the supplementary material.\n\nFig. 5 .\n5(a) is visualization of the choice of the hyperparameter \u03b3, with \u03c4 = 1; (b) is visualization of the choice of the hyperparameter \u03c4 , with \u03b3 = 0.2\n\nFig. 1 .\n1For each time step, feature are then sampled according to the K offsets with linear interpolation. The sampled features are weighted by the attention weights and summed up to produce the updated frame feature for the corresponding time step. Illustration of the encoder.\n\nFig. 2 .\n2Illustration of the Deformable Cross Attention module.\n\nTable 1 .\n1Comparison with the state-of-the-art methods on THUMOS14 dataset. We report the mean Average Precision (mAP) in different thresholds and the floating-point operations (FLOPs, G).Type \nMethod \n0.3 \n0.4 \n0.5 \n0.6 \n0.7 Avg. FLOPs \n\nTwo-stage \n\nBSN[21] \n53.5 45.0 36.9 28.4 20.0 36.8 \n3.4 \nBMN[19] \n56.0 47.4 38.8 29.7 20.5 38.5 171.0 \nG-TAD[38] \n54.5 47.6 40.3 30.8 23.4 39.3 639.8 \nTAL[6] \n53.2 48.5 42.8 33.8 20.8 39.8 \n-\nTCANet[28] \n60.6 53.2 44.6 36.8 26.7 44.3 \n-\nCSA+BMN[30] 64.4 58.0 49.2 38.2 27.8 47.5 \n-\nP-GCN[43] \n63.6 57.8 49.1 \n-\n-\n-\n4.4 \nRTD-Net[31] 68.3 62.3 51.9 38.8 23.7 49.0 \n-\nVSGN[45] \n66.7 60.4 52.4 41.0 30.4 50.2 \n-\nContextLoc[48] 68.3 63.8 54.3 41.8 26.2 50.9 \n3.1 \n\nOne-stage \n\nSSAD[20] \n43.0 35.0 24.6 \n-\n-\n-\n-\nSSN[41] \n51.9 41.0 29.9 \n-\n-\n-\n-\nA2Net[40] \n58.6 54.1 45.5 32.5 17.2 41.6 \n30.4 \nAFSD[18] \n67.3 62.4 55.5 43.7 31.1 52.0 \n5.1 \nTadTr[23] \n62.4 57.4 49.2 37.8 26.3 46.6 \n0.75 \nReAct \n69.2 65.0 57.1 47.8 35.6 55.0 0.68 \n\n\n\nTable 2 .\n2Comparison with the state-of-the-art methods on ActivityNet-1.3 dataset.Type \nMethod \n0.5 0.75 0.95 Avg. FLOPs(G) \n\nActioness \n\nBSN[21] \n46.5 30.0 8.0 28.2 \n-\nSSN[41] \n43.2 28.7 5.6 28.3 \n-\nBMN[19] \n50.1 34.8 8.3 33.9 \n45.6 \nG-TAD[38] 50.4 34.6 9.0 34.1 \n45.7 \nBU-TAL[46] 43.5 33.9 9.2 34.3 \n-\nVSGN[45] 52.3 35.2 8.3 34.7 \n-\n\nAnchor-based \n\nTAL[6] \n38.2 18.3 1.3 20.2 \n-\nPGCN[43] \n48.3 33.2 3.3 31.1 \n5.0 \nTCANet[28] 52.3 36.7 6.9 35.5 \n-\nAFSD[18] \n52.4 35.2 6.5 34.3 \n15.3 \n\nDETR-based \n\nRTD-Net[31] 47.2 30.7 8.6 30.8 \n-\nTadTr[23] \n49.1 32.6 8.5 32.3 \n0.38 \nReAct \n49.6 33.0 8.6 32.6 \n0.38 \n\nTable 3. Ablation study on three main components. \n\nMethod RAID ACE SQ 0.3 \n0.4 \n0.5 \n0.6 \n0.7 Avg. \n\nOur \nBase \n\n66.6 59.2 49.7 38.0 25.0 47.7 \n\u221a \n\u221a \n66.6 61.5 53.7 43.4 31.2 51.3 \n\u221a \n\u221a \n67.0 62.6 54.4 44.0 32.2 52.1 \n\u221a \n\u221a \n69.1 63.3 54.2 43.5 31.0 52.2 \n\u221a \n\u221a \n\u221a \n69.2 65.0 57.1 47.8 35.6 55.0 \n\n\n\nTable 4 .\n4Comparison of different settings of ACE module.Module \nSetting \n0.3 \n0.4 \n0.5 \n0.6 \n0.7 Avg. \n\nACE-enc \n\nNo Contrastive \n68.1 63.4 55.0 46.0 32.8 53.1 \n{S1,S2} + {N1} \n68.3 63.4 55.4 46.2 33.9 53.4 \n{S1,S2} + {N2} \n69.7 64.6 55.7 45.6 33.8 53.9 \n{S1,S2} + {N1,N2} \n69.7 64.5 56.6 45.9 34.7 54.3 \n{S1} + {N1,N2} \n69.1 64.4 56.3 46.2 34.6 54.1 \nBefore Transformer Enc. 69.7 64.3 56.1 46.4 34.2 54.1 \nAfter Transformer Enc. 66.4 61.2 53.3 43.4 32.0 51.2 \n\nACE-dec \n\nL q \nf oc Only \n67.5 62.6 53.9 43.3 33.2 52.1 \nL gt \nf oc Only \n66.1 61.1 53.6 44.2 30.9 51.2 \nL q \nf oc + L gt \n\nf oc \n\n68.3 63.4 55.4 46.2 33.9 53.4 \n\nmore important role in training than N1 (e.g., average mAP 53.9 versus 53.4), \nand merging them can gain further promotion (i.e., 54.3). \n\n\nVivit: A video vision transformer. A Arnab, M Dehghani, G Heigold, C Sun, M Lu\u010di\u0107, C Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionArnab, A., Dehghani, M., Heigold, G., Sun, C., Lu\u010di\u0107, M., Schmid, C.: Vivit: A video vision transformer. In: Proceedings of the IEEE/CVF International Confer- ence on Computer Vision. pp. 6836-6846 (2021)\n\nSoft-nms-improving object detection with one line of code. N Bodla, B Singh, R Chellappa, L S Davis, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionBodla, N., Singh, B., Chellappa, R., Davis, L.S.: Soft-nms-improving object de- tection with one line of code. In: Proceedings of the IEEE international conference on computer vision. pp. 5561-5569 (2017)\n\nActivitynet: A large-scale video benchmark for human activity understanding. F Caba Heilbron, V Escorcia, B Ghanem, J Carlos Niebles, Proceedings of the ieee conference on computer vision and pattern recognition. the ieee conference on computer vision and pattern recognitionCaba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A large-scale video benchmark for human activity understanding. In: Proceedings of the ieee conference on computer vision and pattern recognition. pp. 961-970 (2015)\n\nEndto-end object detection with transformers. N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, S Zagoruyko, European conference on computer vision. SpringerCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End- to-end object detection with transformers. In: European conference on computer vision. pp. 213-229. Springer (2020)\n\nQuo vadis, action recognition? a new model and the kinetics dataset. J Carreira, A Zisserman, proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCarreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the kinetics dataset. In: proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6299-6308 (2017)\n\nRethinking the faster r-cnn architecture for temporal action localization. Y W Chao, S Vijayanarasimhan, B Seybold, D A Ross, J Deng, R Sukthankar, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross, D.A., Deng, J., Sukthankar, R.: Rethinking the faster r-cnn architecture for temporal action localization. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1130-1139 (2018)\n\nA simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, International conference on machine learning. PMLRChen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con- trastive learning of visual representations. In: International conference on machine learning. pp. 1597-1607. PMLR (2020)\n\nDearkd: Dataefficient early knowledge distillation for vision transformers. X Chen, Q Cao, Y Zhong, J Zhang, S Gao, D Tao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionChen, X., Cao, Q., Zhong, Y., Zhang, J., Gao, S., Tao, D.: Dearkd: Data- efficient early knowledge distillation for vision transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12052- 12062 (2022)\n\nDeformable convolutional networks. J Dai, H Qi, Y Xiong, Y Li, G Zhang, H Hu, Y Wei, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionDai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolu- tional networks. In: Proceedings of the IEEE international conference on computer vision. pp. 764-773 (2017)\n\nTood: Task-aligned onestage object detection. C Feng, Y Zhong, Y Gao, M R Scott, W Huang, arXiv:2108.07755arXiv preprintFeng, C., Zhong, Y., Gao, Y., Scott, M.R., Huang, W.: Tood: Task-aligned one- stage object detection. arXiv preprint arXiv:2108.07755 (2021)\n\nCtap: Complementary temporal action proposal generation. J Gao, K Chen, R Nevatia, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Gao, J., Chen, K., Nevatia, R.: Ctap: Complementary temporal action pro- posal generation. In: Proceedings of the European conference on computer vision (ECCV). pp. 68-83 (2018)\n\nTurn tap: Temporal unit regression network for temporal action proposals. J Gao, Z Yang, K Chen, C Sun, R Nevatia, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionGao, J., Yang, Z., Chen, K., Sun, C., Nevatia, R.: Turn tap: Temporal unit regres- sion network for temporal action proposals. In: Proceedings of the IEEE interna- tional conference on computer vision. pp. 3628-3636 (2017)\n\nNoise-contrastive estimation: A new estimation principle for unnormalized statistical models. M Gutmann, A Hyv\u00e4rinen, Proceedings of the thirteenth international conference on artificial intelligence and statistics. the thirteenth international conference on artificial intelligence and statisticsJMLR Workshop and Conference ProceedingsGutmann, M., Hyv\u00e4rinen, A.: Noise-contrastive estimation: A new estimation prin- ciple for unnormalized statistical models. In: Proceedings of the thirteenth inter- national conference on artificial intelligence and statistics. pp. 297-304. JMLR Workshop and Conference Proceedings (2010)\n\nY G Jiang, J Liu, A Roshan Zamir, G Toderici, I Laptev, M Shah, R Sukthankar, THUMOS challenge: Action recognition with a large number of classes. Jiang, Y.G., Liu, J., Roshan Zamir, A., Toderici, G., Laptev, I., Shah, M., Suk- thankar, R.: THUMOS challenge: Action recognition with a large number of classes. http://crcv.ucf.edu/THUMOS14/ (2014)\n\nW Kay, J Carreira, K Simonyan, B Zhang, C Hillier, S Vijayanarasimhan, F Viola, T Green, T Back, P Natsev, arXiv:1705.06950The kinetics human action video dataset. arXiv preprintKay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017)\n\nThe hungarian method for the assignment problem. H W Kuhn, Naval research logistics quarterly. 21-2Kuhn, H.W.: The hungarian method for the assignment problem. Naval research logistics quarterly 2(1-2), 83-97 (1955)\n\nFast learning of temporal action proposal via dense boundary generator. C Lin, J Li, Y Wang, Y Tai, D Luo, Z Cui, C Wang, J Li, F Huang, R Ji, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Lin, C., Li, J., Wang, Y., Tai, Y., Luo, D., Cui, Z., Wang, C., Li, J., Huang, F., Ji, R.: Fast learning of temporal action proposal via dense boundary generator. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 11499-11506 (2020)\n\nLearning salient boundary feature for anchor-free temporal action localization. C Lin, C Xu, D Luo, Y Wang, Y Tai, C Wang, J Li, F Huang, Y Fu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLin, C., Xu, C., Luo, D., Wang, Y., Tai, Y., Wang, C., Li, J., Huang, F., Fu, Y.: Learning salient boundary feature for anchor-free temporal action localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3320-3329 (2021)\n\nBmn: Boundary-matching network for temporal action proposal generation. T Lin, X Liu, X Li, E Ding, S Wen, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLin, T., Liu, X., Li, X., Ding, E., Wen, S.: Bmn: Boundary-matching network for temporal action proposal generation. In: Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision. pp. 3889-3898 (2019)\n\nSingle shot temporal action detection. T Lin, X Zhao, Z Shou, Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on MultimediaLin, T., Zhao, X., Shou, Z.: Single shot temporal action detection. In: Proceedings of the 25th ACM international conference on Multimedia. pp. 988-996 (2017)\n\nBsn: Boundary sensitive network for temporal action proposal generation. T Lin, X Zhao, H Su, C Wang, M Yang, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.: Bsn: Boundary sensitive network for temporal action proposal generation. In: Proceedings of the European conference on computer vision (ECCV). pp. 3-19 (2018)\n\nFocal loss for dense object detection. T Y Lin, P Goyal, R Girshick, K He, P Doll\u00e1r, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2980-2988 (2017)\n\nEnd-to-end temporal action detection with transformer. X Liu, Q Wang, Y Hu, X Tang, S Bai, X Bai, arXiv:2106.10271arXiv preprintLiu, X., Wang, Q., Hu, Y., Tang, X., Bai, S., Bai, X.: End-to-end temporal action detection with transformer. arXiv preprint arXiv:2106.10271 (2021)\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10012-10022 (2021)\n\nZ Liu, J Ning, Y Cao, Y Wei, Z Zhang, S Lin, H Hu, arXiv:2106.13230Video swin transformer. arXiv preprintLiu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin trans- former. arXiv preprint arXiv:2106.13230 (2021)\n\nGaussian temporal awareness networks for action localization. F Long, T Yao, Z Qiu, X Tian, J Luo, T Mei, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLong, F., Yao, T., Qiu, Z., Tian, X., Luo, J., Mei, T.: Gaussian temporal awareness networks for action localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 344-353 (2019)\n\nA Van Den Oord, Y Li, O Vinyals, Representation learning with contrastive predictive coding. arXiv e-prints pp. 1807Van den Oord, A., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv e-prints pp. arXiv-1807 (2018)\n\nTemporal context aggregation network for temporal action proposal refinement. Z Qing, H Su, W Gan, D Wang, W Wu, X Wang, Y Qiao, J Yan, C Gao, N Sang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionQing, Z., Su, H., Gan, W., Wang, D., Wu, W., Wang, X., Qiao, Y., Yan, J., Gao, C., Sang, N.: Temporal context aggregation network for temporal action proposal refinement. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 485-494 (2021)\n\nGeneralized intersection over union: A metric and a loss for bounding box regression. H Rezatofighi, N Tsoi, J Gwak, A Sadeghian, I Reid, S Savarese, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionRezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S.: General- ized intersection over union: A metric and a loss for bounding box regression. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recog- nition. pp. 658-666 (2019)\n\nClass semanticsbased attention for action detection. D Sridhar, N Quader, S Muralidharan, Y Li, P Dai, J Lu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionSridhar, D., Quader, N., Muralidharan, S., Li, Y., Dai, P., Lu, J.: Class semantics- based attention for action detection. In: Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision. pp. 13739-13748 (2021)\n\nRelaxed transformer decoders for direct action proposal generation. J Tan, J Tang, L Wang, G Wu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionTan, J., Tang, J., Wang, L., Wu, G.: Relaxed transformer decoders for direct action proposal generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13526-13535 (2021)\n\nFcos: Fully convolutional one-stage object detection. Z Tian, C Shen, H Chen, T He, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionTian, Z., Shen, C., Chen, H., He, T.: Fcos: Fully convolutional one-stage object detection. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 9627-9636 (2019)\n\nAttention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 30Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information pro- cessing systems 30 (2017)\n\nUntrimmednets for weakly supervised action recognition and detection. L Wang, Y Xiong, D Lin, L Van Gool, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. the IEEE conference on Computer Vision and Pattern RecognitionWang, L., Xiong, Y., Lin, D., Van Gool, L.: Untrimmednets for weakly super- vised action recognition and detection. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. pp. 4325-4334 (2017)\n\nTemporal segment networks for action recognition in videos. L Wang, Y Xiong, Z Wang, Y Qiao, D Lin, X Tang, L Van Gool, IEEE transactions. 4111Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal segment networks for action recognition in videos. IEEE transactions on pattern analysis and machine intelligence 41(11), 2740-2755 (2018)\n\nY Xiong, L Wang, Z Wang, B Zhang, H Song, W Li, D Lin, Y Qiao, L Van Gool, X Tang, arXiv:1608.00797Cuhk & ethz & siat submission to activitynet challenge 2016. arXiv preprintXiong, Y., Wang, L., Wang, Z., Zhang, B., Song, H., Li, W., Lin, D., Qiao, Y., Van Gool, L., Tang, X.: Cuhk & ethz & siat submission to activitynet challenge 2016. arXiv preprint arXiv:1608.00797 (2016)\n\nR-c3d: Region convolutional 3d network for temporal activity detection. H Xu, A Das, K Saenko, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionXu, H., Das, A., Saenko, K.: R-c3d: Region convolutional 3d network for tem- poral activity detection. In: Proceedings of the IEEE international conference on computer vision. pp. 5783-5792 (2017)\n\nG-tad: Sub-graph localization for temporal action detection. M Xu, C Zhao, D S Rojas, A Thabet, B Ghanem, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXu, M., Zhao, C., Rojas, D.S., Thabet, A., Ghanem, B.: G-tad: Sub-graph localiza- tion for temporal action detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10156-10165 (2020)\n\nVitae: Vision transformer advanced by exploring intrinsic inductive bias. Y Xu, Q Zhang, J Zhang, D Tao, Advances in Neural Information Processing Systems. 34Xu, Y., Zhang, Q., Zhang, J., Tao, D.: Vitae: Vision transformer advanced by exploring intrinsic inductive bias. Advances in Neural Information Processing Sys- tems 34 (2021)\n\nRevisiting anchor mechanisms for temporal action localization. L Yang, H Peng, D Zhang, J Fu, J Han, IEEE Transactions on Image Processing. 29Yang, L., Peng, H., Zhang, D., Fu, J., Han, J.: Revisiting anchor mechanisms for temporal action localization. IEEE Transactions on Image Processing 29, 8535- 8548 (2020)\n\nTemporal structure mining for weakly supervised action detection. T Yu, Z Ren, Y Li, E Yan, N Xu, J Yuan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionYu, T., Ren, Z., Li, Y., Yan, E., Xu, N., Yuan, J.: Temporal structure mining for weakly supervised action detection. In: Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision. pp. 5522-5531 (2019)\n\nA duality based approach for realtime tv-l 1 optical flow. C Zach, T Pock, H Bischof, Joint pattern recognition symposium. SpringerZach, C., Pock, T., Bischof, H.: A duality based approach for realtime tv-l 1 optical flow. In: Joint pattern recognition symposium. pp. 214-223. Springer (2007)\n\nGraph convolutional networks for temporal action localization. R Zeng, W Huang, M Tan, Y Rong, P Zhao, J Huang, C Gan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZeng, R., Huang, W., Tan, M., Rong, Y., Zhao, P., Huang, J., Gan, C.: Graph convolutional networks for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7094-7103 (2019)\n\nQ Zhang, Y Xu, J Zhang, D Tao, arXiv:2202.10108Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond. arXiv preprintZhang, Q., Xu, Y., Zhang, J., Tao, D.: Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond. arXiv preprint arXiv:2202.10108 (2022)\n\nVideo self-stitching graph network for temporal action localization. C Zhao, A K Thabet, B Ghanem, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhao, C., Thabet, A.K., Ghanem, B.: Video self-stitching graph network for tempo- ral action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13658-13667 (2021)\n\nBottom-up temporal action localization with mutual regularization. P Zhao, L Xie, C Ju, Y Zhang, Y Wang, Q Tian, European Conference on Computer Vision. SpringerZhao, P., Xie, L., Ju, C., Zhang, Y., Wang, Y., Tian, Q.: Bottom-up temporal action localization with mutual regularization. In: European Conference on Computer Vision. pp. 539-555. Springer (2020)\n\nDeformable detr: Deformable transformers for end-to-end object detection. X Zhu, W Su, L Lu, B Li, X Wang, J Dai, arXiv:2010.04159arXiv preprintZhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020)\n\nEnriching local and global contexts for temporal action localization. Z Zhu, W Tang, L Wang, N Zheng, G Hua, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhu, Z., Tang, W., Wang, L., Zheng, N., Hua, G.: Enriching local and global contexts for temporal action localization. In: Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision. pp. 13516-13525 (2021)\n", "annotations": {"author": "[{\"end\":200,\"start\":60},{\"end\":248,\"start\":201},{\"end\":259,\"start\":249},{\"end\":262,\"start\":260},{\"end\":301,\"start\":263},{\"end\":344,\"start\":302},{\"end\":479,\"start\":345},{\"end\":519,\"start\":480}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":69},{\"end\":212,\"start\":207},{\"end\":258,\"start\":255},{\"end\":273,\"start\":268},{\"end\":308,\"start\":306},{\"end\":351,\"start\":349},{\"end\":491,\"start\":488}]", "author_first_name": "[{\"end\":68,\"start\":60},{\"end\":206,\"start\":201},{\"end\":254,\"start\":249},{\"end\":261,\"start\":260},{\"end\":267,\"start\":263},{\"end\":305,\"start\":302},{\"end\":348,\"start\":345},{\"end\":487,\"start\":480}]", "author_affiliation": "[{\"end\":199,\"start\":74},{\"end\":247,\"start\":214},{\"end\":300,\"start\":275},{\"end\":343,\"start\":310},{\"end\":478,\"start\":353},{\"end\":518,\"start\":493}]", "title": "[{\"end\":57,\"start\":1},{\"end\":576,\"start\":520}]", "venue": null, "abstract": "[{\"end\":1684,\"start\":578}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1850,\"start\":1846},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1853,\"start\":1850},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1856,\"start\":1853},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1916,\"start\":1912},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2022,\"start\":2019},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7278,\"start\":7274},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7281,\"start\":7278},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7284,\"start\":7281},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7287,\"start\":7284},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7290,\"start\":7287},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7293,\"start\":7290},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7296,\"start\":7293},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7299,\"start\":7296},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7422,\"start\":7418},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7425,\"start\":7422},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7428,\"start\":7425},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7622,\"start\":7618},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8014,\"start\":8010},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8016,\"start\":8014},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8019,\"start\":8016},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8022,\"start\":8019},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8025,\"start\":8022},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8054,\"start\":8051},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8057,\"start\":8054},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8060,\"start\":8057},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8183,\"start\":8179},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8453,\"start\":8450},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8456,\"start\":8453},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8459,\"start\":8456},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8590,\"start\":8586},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8696,\"start\":8692},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8698,\"start\":8696},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8701,\"start\":8698},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8704,\"start\":8701},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8707,\"start\":8704},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8709,\"start\":8707},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8835,\"start\":8832},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9010,\"start\":9006},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9143,\"start\":9139},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9383,\"start\":9379},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9386,\"start\":9383},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9669,\"start\":9666},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9742,\"start\":9738},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9818,\"start\":9814},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9939,\"start\":9935},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10845,\"start\":10842},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11194,\"start\":11190},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11205,\"start\":11202},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11291,\"start\":11287},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12180,\"start\":12177},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12183,\"start\":12180},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12186,\"start\":12183},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12216,\"start\":12212},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19039,\"start\":19035},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20360,\"start\":20356},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22209,\"start\":22205},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22608,\"start\":22604},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22611,\"start\":22608},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22972,\"start\":22968},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22996,\"start\":22993},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23452,\"start\":23449},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23991,\"start\":23987},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23994,\"start\":23991},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31478,\"start\":31474},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31507,\"start\":31503},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31536,\"start\":31532},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32536,\"start\":32532},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33046,\"start\":33042},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33049,\"start\":33046},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":33052,\"start\":33049},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33055,\"start\":33052},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33125,\"start\":33121}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33823,\"start\":33576},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34244,\"start\":33824},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34797,\"start\":34245},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34902,\"start\":34798},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35653,\"start\":34903},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35810,\"start\":35654},{\"attributes\":{\"id\":\"fig_7\"},\"end\":36092,\"start\":35811},{\"attributes\":{\"id\":\"fig_8\"},\"end\":36158,\"start\":36093},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37125,\"start\":36159},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38029,\"start\":37126},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38796,\"start\":38030}]", "paragraph": "[{\"end\":2587,\"start\":1700},{\"end\":3381,\"start\":2589},{\"end\":4443,\"start\":3383},{\"end\":5633,\"start\":4445},{\"end\":6177,\"start\":5635},{\"end\":6242,\"start\":6179},{\"end\":6952,\"start\":6244},{\"end\":8538,\"start\":6969},{\"end\":9621,\"start\":8540},{\"end\":10206,\"start\":9623},{\"end\":10351,\"start\":10217},{\"end\":12561,\"start\":10457},{\"end\":13190,\"start\":12563},{\"end\":13771,\"start\":13192},{\"end\":14039,\"start\":13811},{\"end\":15223,\"start\":14041},{\"end\":15615,\"start\":15225},{\"end\":16034,\"start\":15653},{\"end\":16416,\"start\":16072},{\"end\":16547,\"start\":16449},{\"end\":16676,\"start\":16633},{\"end\":16733,\"start\":16700},{\"end\":16996,\"start\":16768},{\"end\":17224,\"start\":16998},{\"end\":17266,\"start\":17249},{\"end\":17521,\"start\":17268},{\"end\":18258,\"start\":17559},{\"end\":18797,\"start\":18260},{\"end\":19139,\"start\":18799},{\"end\":19726,\"start\":19195},{\"end\":20174,\"start\":19728},{\"end\":20252,\"start\":20176},{\"end\":20549,\"start\":20301},{\"end\":20983,\"start\":20580},{\"end\":21539,\"start\":20985},{\"end\":21826,\"start\":21612},{\"end\":21962,\"start\":21846},{\"end\":22268,\"start\":22009},{\"end\":22892,\"start\":22445},{\"end\":22997,\"start\":22907},{\"end\":23502,\"start\":23024},{\"end\":25063,\"start\":23519},{\"end\":25155,\"start\":25082},{\"end\":25973,\"start\":25157},{\"end\":27142,\"start\":25995},{\"end\":28534,\"start\":27144},{\"end\":29182,\"start\":28536},{\"end\":30042,\"start\":29197},{\"end\":30307,\"start\":30066},{\"end\":30523,\"start\":30331},{\"end\":31411,\"start\":30525},{\"end\":33156,\"start\":31450},{\"end\":33575,\"start\":33203}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10456,\"start\":10352},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15652,\"start\":15616},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16071,\"start\":16035},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16448,\"start\":16417},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16632,\"start\":16548},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16699,\"start\":16677},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16767,\"start\":16734},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17248,\"start\":17225},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19194,\"start\":19140},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20300,\"start\":20253},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21611,\"start\":21540},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22008,\"start\":21963},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22444,\"start\":22269}]", "table_ref": "[{\"end\":25648,\"start\":25642}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1698,\"start\":1686},{\"attributes\":{\"n\":\"2\"},\"end\":6967,\"start\":6955},{\"attributes\":{\"n\":\"3\"},\"end\":10215,\"start\":10209},{\"attributes\":{\"n\":\"3.1\"},\"end\":13809,\"start\":13774},{\"attributes\":{\"n\":\"3.2\"},\"end\":17557,\"start\":17524},{\"attributes\":{\"n\":\"3.3\"},\"end\":20578,\"start\":20552},{\"attributes\":{\"n\":\"3.4\"},\"end\":21844,\"start\":21829},{\"attributes\":{\"n\":\"4\"},\"end\":22905,\"start\":22895},{\"attributes\":{\"n\":\"4.1\"},\"end\":23022,\"start\":23000},{\"attributes\":{\"n\":\"4.2\"},\"end\":23517,\"start\":23505},{\"attributes\":{\"n\":\"4.3\"},\"end\":25080,\"start\":25066},{\"end\":25993,\"start\":25976},{\"attributes\":{\"n\":\"5\"},\"end\":29195,\"start\":29185},{\"end\":30064,\"start\":30045},{\"end\":30329,\"start\":30310},{\"end\":31448,\"start\":31414},{\"end\":33201,\"start\":33159},{\"end\":33585,\"start\":33577},{\"end\":33833,\"start\":33825},{\"end\":34254,\"start\":34246},{\"end\":35663,\"start\":35655},{\"end\":35820,\"start\":35812},{\"end\":36102,\"start\":36094},{\"end\":36169,\"start\":36160},{\"end\":37136,\"start\":37127},{\"end\":38040,\"start\":38031}]", "table": "[{\"end\":37125,\"start\":36349},{\"end\":38029,\"start\":37210},{\"end\":38796,\"start\":38089}]", "figure_caption": "[{\"end\":33823,\"start\":33587},{\"end\":34244,\"start\":33835},{\"end\":34797,\"start\":34256},{\"end\":34902,\"start\":34800},{\"end\":35653,\"start\":34905},{\"end\":35810,\"start\":35665},{\"end\":36092,\"start\":35822},{\"end\":36158,\"start\":36104},{\"end\":36349,\"start\":36171},{\"end\":37210,\"start\":37138},{\"end\":38089,\"start\":38042}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4086,\"start\":4080},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10953,\"start\":10947},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16193,\"start\":16187},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19745,\"start\":19731},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26256,\"start\":26247},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26667,\"start\":26661},{\"end\":27108,\"start\":27102},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30148,\"start\":30141},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30607,\"start\":30601},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33332,\"start\":33326},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33455,\"start\":33449}]", "bib_author_first_name": "[{\"end\":38834,\"start\":38833},{\"end\":38843,\"start\":38842},{\"end\":38855,\"start\":38854},{\"end\":38866,\"start\":38865},{\"end\":38873,\"start\":38872},{\"end\":38882,\"start\":38881},{\"end\":39286,\"start\":39285},{\"end\":39295,\"start\":39294},{\"end\":39304,\"start\":39303},{\"end\":39317,\"start\":39316},{\"end\":39319,\"start\":39318},{\"end\":39732,\"start\":39731},{\"end\":39749,\"start\":39748},{\"end\":39761,\"start\":39760},{\"end\":39771,\"start\":39770},{\"end\":40221,\"start\":40220},{\"end\":40231,\"start\":40230},{\"end\":40240,\"start\":40239},{\"end\":40252,\"start\":40251},{\"end\":40263,\"start\":40262},{\"end\":40275,\"start\":40274},{\"end\":40606,\"start\":40605},{\"end\":40618,\"start\":40617},{\"end\":41050,\"start\":41049},{\"end\":41052,\"start\":41051},{\"end\":41060,\"start\":41059},{\"end\":41080,\"start\":41079},{\"end\":41091,\"start\":41090},{\"end\":41093,\"start\":41092},{\"end\":41101,\"start\":41100},{\"end\":41109,\"start\":41108},{\"end\":41600,\"start\":41599},{\"end\":41608,\"start\":41607},{\"end\":41621,\"start\":41620},{\"end\":41632,\"start\":41631},{\"end\":41969,\"start\":41968},{\"end\":41977,\"start\":41976},{\"end\":41984,\"start\":41983},{\"end\":41993,\"start\":41992},{\"end\":42002,\"start\":42001},{\"end\":42009,\"start\":42008},{\"end\":42449,\"start\":42448},{\"end\":42456,\"start\":42455},{\"end\":42462,\"start\":42461},{\"end\":42471,\"start\":42470},{\"end\":42477,\"start\":42476},{\"end\":42486,\"start\":42485},{\"end\":42492,\"start\":42491},{\"end\":42860,\"start\":42859},{\"end\":42868,\"start\":42867},{\"end\":42877,\"start\":42876},{\"end\":42884,\"start\":42883},{\"end\":42886,\"start\":42885},{\"end\":42895,\"start\":42894},{\"end\":43133,\"start\":43132},{\"end\":43140,\"start\":43139},{\"end\":43148,\"start\":43147},{\"end\":43527,\"start\":43526},{\"end\":43534,\"start\":43533},{\"end\":43542,\"start\":43541},{\"end\":43550,\"start\":43549},{\"end\":43557,\"start\":43556},{\"end\":44007,\"start\":44006},{\"end\":44018,\"start\":44017},{\"end\":44540,\"start\":44539},{\"end\":44542,\"start\":44541},{\"end\":44551,\"start\":44550},{\"end\":44558,\"start\":44557},{\"end\":44574,\"start\":44573},{\"end\":44586,\"start\":44585},{\"end\":44596,\"start\":44595},{\"end\":44604,\"start\":44603},{\"end\":44888,\"start\":44887},{\"end\":44895,\"start\":44894},{\"end\":44907,\"start\":44906},{\"end\":44919,\"start\":44918},{\"end\":44928,\"start\":44927},{\"end\":44939,\"start\":44938},{\"end\":44959,\"start\":44958},{\"end\":44968,\"start\":44967},{\"end\":44977,\"start\":44976},{\"end\":44985,\"start\":44984},{\"end\":45331,\"start\":45330},{\"end\":45333,\"start\":45332},{\"end\":45571,\"start\":45570},{\"end\":45578,\"start\":45577},{\"end\":45584,\"start\":45583},{\"end\":45592,\"start\":45591},{\"end\":45599,\"start\":45598},{\"end\":45606,\"start\":45605},{\"end\":45613,\"start\":45612},{\"end\":45621,\"start\":45620},{\"end\":45627,\"start\":45626},{\"end\":45636,\"start\":45635},{\"end\":46096,\"start\":46095},{\"end\":46103,\"start\":46102},{\"end\":46109,\"start\":46108},{\"end\":46116,\"start\":46115},{\"end\":46124,\"start\":46123},{\"end\":46131,\"start\":46130},{\"end\":46139,\"start\":46138},{\"end\":46145,\"start\":46144},{\"end\":46154,\"start\":46153},{\"end\":46652,\"start\":46651},{\"end\":46659,\"start\":46658},{\"end\":46666,\"start\":46665},{\"end\":46672,\"start\":46671},{\"end\":46680,\"start\":46679},{\"end\":47073,\"start\":47072},{\"end\":47080,\"start\":47079},{\"end\":47088,\"start\":47087},{\"end\":47448,\"start\":47447},{\"end\":47455,\"start\":47454},{\"end\":47463,\"start\":47462},{\"end\":47469,\"start\":47468},{\"end\":47477,\"start\":47476},{\"end\":47846,\"start\":47845},{\"end\":47848,\"start\":47847},{\"end\":47855,\"start\":47854},{\"end\":47864,\"start\":47863},{\"end\":47876,\"start\":47875},{\"end\":47882,\"start\":47881},{\"end\":48258,\"start\":48257},{\"end\":48265,\"start\":48264},{\"end\":48273,\"start\":48272},{\"end\":48279,\"start\":48278},{\"end\":48287,\"start\":48286},{\"end\":48294,\"start\":48293},{\"end\":48554,\"start\":48553},{\"end\":48561,\"start\":48560},{\"end\":48568,\"start\":48567},{\"end\":48575,\"start\":48574},{\"end\":48581,\"start\":48580},{\"end\":48588,\"start\":48587},{\"end\":48597,\"start\":48596},{\"end\":48604,\"start\":48603},{\"end\":48987,\"start\":48986},{\"end\":48994,\"start\":48993},{\"end\":49002,\"start\":49001},{\"end\":49009,\"start\":49008},{\"end\":49016,\"start\":49015},{\"end\":49025,\"start\":49024},{\"end\":49032,\"start\":49031},{\"end\":49285,\"start\":49284},{\"end\":49293,\"start\":49292},{\"end\":49300,\"start\":49299},{\"end\":49307,\"start\":49306},{\"end\":49315,\"start\":49314},{\"end\":49322,\"start\":49321},{\"end\":49703,\"start\":49702},{\"end\":49719,\"start\":49718},{\"end\":49725,\"start\":49724},{\"end\":50034,\"start\":50033},{\"end\":50042,\"start\":50041},{\"end\":50048,\"start\":50047},{\"end\":50055,\"start\":50054},{\"end\":50063,\"start\":50062},{\"end\":50069,\"start\":50068},{\"end\":50077,\"start\":50076},{\"end\":50085,\"start\":50084},{\"end\":50092,\"start\":50091},{\"end\":50099,\"start\":50098},{\"end\":50620,\"start\":50619},{\"end\":50635,\"start\":50634},{\"end\":50643,\"start\":50642},{\"end\":50651,\"start\":50650},{\"end\":50664,\"start\":50663},{\"end\":50672,\"start\":50671},{\"end\":51159,\"start\":51158},{\"end\":51170,\"start\":51169},{\"end\":51180,\"start\":51179},{\"end\":51196,\"start\":51195},{\"end\":51202,\"start\":51201},{\"end\":51209,\"start\":51208},{\"end\":51638,\"start\":51637},{\"end\":51645,\"start\":51644},{\"end\":51653,\"start\":51652},{\"end\":51661,\"start\":51660},{\"end\":52056,\"start\":52055},{\"end\":52064,\"start\":52063},{\"end\":52072,\"start\":52071},{\"end\":52080,\"start\":52079},{\"end\":52472,\"start\":52471},{\"end\":52483,\"start\":52482},{\"end\":52494,\"start\":52493},{\"end\":52504,\"start\":52503},{\"end\":52517,\"start\":52516},{\"end\":52526,\"start\":52525},{\"end\":52528,\"start\":52527},{\"end\":52537,\"start\":52536},{\"end\":52547,\"start\":52546},{\"end\":52828,\"start\":52827},{\"end\":52836,\"start\":52835},{\"end\":52845,\"start\":52844},{\"end\":52852,\"start\":52851},{\"end\":53286,\"start\":53285},{\"end\":53294,\"start\":53293},{\"end\":53303,\"start\":53302},{\"end\":53311,\"start\":53310},{\"end\":53319,\"start\":53318},{\"end\":53326,\"start\":53325},{\"end\":53334,\"start\":53333},{\"end\":53592,\"start\":53591},{\"end\":53601,\"start\":53600},{\"end\":53609,\"start\":53608},{\"end\":53617,\"start\":53616},{\"end\":53626,\"start\":53625},{\"end\":53634,\"start\":53633},{\"end\":53640,\"start\":53639},{\"end\":53647,\"start\":53646},{\"end\":53655,\"start\":53654},{\"end\":53667,\"start\":53666},{\"end\":54042,\"start\":54041},{\"end\":54048,\"start\":54047},{\"end\":54055,\"start\":54054},{\"end\":54445,\"start\":54444},{\"end\":54451,\"start\":54450},{\"end\":54459,\"start\":54458},{\"end\":54461,\"start\":54460},{\"end\":54470,\"start\":54469},{\"end\":54480,\"start\":54479},{\"end\":54942,\"start\":54941},{\"end\":54948,\"start\":54947},{\"end\":54957,\"start\":54956},{\"end\":54966,\"start\":54965},{\"end\":55265,\"start\":55264},{\"end\":55273,\"start\":55272},{\"end\":55281,\"start\":55280},{\"end\":55290,\"start\":55289},{\"end\":55296,\"start\":55295},{\"end\":55582,\"start\":55581},{\"end\":55588,\"start\":55587},{\"end\":55595,\"start\":55594},{\"end\":55601,\"start\":55600},{\"end\":55608,\"start\":55607},{\"end\":55614,\"start\":55613},{\"end\":56029,\"start\":56028},{\"end\":56037,\"start\":56036},{\"end\":56045,\"start\":56044},{\"end\":56327,\"start\":56326},{\"end\":56335,\"start\":56334},{\"end\":56344,\"start\":56343},{\"end\":56351,\"start\":56350},{\"end\":56359,\"start\":56358},{\"end\":56367,\"start\":56366},{\"end\":56376,\"start\":56375},{\"end\":56744,\"start\":56743},{\"end\":56753,\"start\":56752},{\"end\":56759,\"start\":56758},{\"end\":56768,\"start\":56767},{\"end\":57151,\"start\":57150},{\"end\":57159,\"start\":57158},{\"end\":57161,\"start\":57160},{\"end\":57171,\"start\":57170},{\"end\":57585,\"start\":57584},{\"end\":57593,\"start\":57592},{\"end\":57600,\"start\":57599},{\"end\":57606,\"start\":57605},{\"end\":57615,\"start\":57614},{\"end\":57623,\"start\":57622},{\"end\":57952,\"start\":57951},{\"end\":57959,\"start\":57958},{\"end\":57965,\"start\":57964},{\"end\":57971,\"start\":57970},{\"end\":57977,\"start\":57976},{\"end\":57985,\"start\":57984},{\"end\":58258,\"start\":58257},{\"end\":58265,\"start\":58264},{\"end\":58273,\"start\":58272},{\"end\":58281,\"start\":58280},{\"end\":58290,\"start\":58289}]", "bib_author_last_name": "[{\"end\":38840,\"start\":38835},{\"end\":38852,\"start\":38844},{\"end\":38863,\"start\":38856},{\"end\":38870,\"start\":38867},{\"end\":38879,\"start\":38874},{\"end\":38889,\"start\":38883},{\"end\":39292,\"start\":39287},{\"end\":39301,\"start\":39296},{\"end\":39314,\"start\":39305},{\"end\":39325,\"start\":39320},{\"end\":39746,\"start\":39733},{\"end\":39758,\"start\":39750},{\"end\":39768,\"start\":39762},{\"end\":39786,\"start\":39772},{\"end\":40228,\"start\":40222},{\"end\":40237,\"start\":40232},{\"end\":40249,\"start\":40241},{\"end\":40260,\"start\":40253},{\"end\":40272,\"start\":40264},{\"end\":40285,\"start\":40276},{\"end\":40615,\"start\":40607},{\"end\":40628,\"start\":40619},{\"end\":41057,\"start\":41053},{\"end\":41077,\"start\":41061},{\"end\":41088,\"start\":41081},{\"end\":41098,\"start\":41094},{\"end\":41106,\"start\":41102},{\"end\":41120,\"start\":41110},{\"end\":41605,\"start\":41601},{\"end\":41618,\"start\":41609},{\"end\":41629,\"start\":41622},{\"end\":41639,\"start\":41633},{\"end\":41974,\"start\":41970},{\"end\":41981,\"start\":41978},{\"end\":41990,\"start\":41985},{\"end\":41999,\"start\":41994},{\"end\":42006,\"start\":42003},{\"end\":42013,\"start\":42010},{\"end\":42453,\"start\":42450},{\"end\":42459,\"start\":42457},{\"end\":42468,\"start\":42463},{\"end\":42474,\"start\":42472},{\"end\":42483,\"start\":42478},{\"end\":42489,\"start\":42487},{\"end\":42496,\"start\":42493},{\"end\":42865,\"start\":42861},{\"end\":42874,\"start\":42869},{\"end\":42881,\"start\":42878},{\"end\":42892,\"start\":42887},{\"end\":42901,\"start\":42896},{\"end\":43137,\"start\":43134},{\"end\":43145,\"start\":43141},{\"end\":43156,\"start\":43149},{\"end\":43531,\"start\":43528},{\"end\":43539,\"start\":43535},{\"end\":43547,\"start\":43543},{\"end\":43554,\"start\":43551},{\"end\":43565,\"start\":43558},{\"end\":44015,\"start\":44008},{\"end\":44028,\"start\":44019},{\"end\":44548,\"start\":44543},{\"end\":44555,\"start\":44552},{\"end\":44571,\"start\":44559},{\"end\":44583,\"start\":44575},{\"end\":44593,\"start\":44587},{\"end\":44601,\"start\":44597},{\"end\":44615,\"start\":44605},{\"end\":44892,\"start\":44889},{\"end\":44904,\"start\":44896},{\"end\":44916,\"start\":44908},{\"end\":44925,\"start\":44920},{\"end\":44936,\"start\":44929},{\"end\":44956,\"start\":44940},{\"end\":44965,\"start\":44960},{\"end\":44974,\"start\":44969},{\"end\":44982,\"start\":44978},{\"end\":44992,\"start\":44986},{\"end\":45338,\"start\":45334},{\"end\":45575,\"start\":45572},{\"end\":45581,\"start\":45579},{\"end\":45589,\"start\":45585},{\"end\":45596,\"start\":45593},{\"end\":45603,\"start\":45600},{\"end\":45610,\"start\":45607},{\"end\":45618,\"start\":45614},{\"end\":45624,\"start\":45622},{\"end\":45633,\"start\":45628},{\"end\":45639,\"start\":45637},{\"end\":46100,\"start\":46097},{\"end\":46106,\"start\":46104},{\"end\":46113,\"start\":46110},{\"end\":46121,\"start\":46117},{\"end\":46128,\"start\":46125},{\"end\":46136,\"start\":46132},{\"end\":46142,\"start\":46140},{\"end\":46151,\"start\":46146},{\"end\":46157,\"start\":46155},{\"end\":46656,\"start\":46653},{\"end\":46663,\"start\":46660},{\"end\":46669,\"start\":46667},{\"end\":46677,\"start\":46673},{\"end\":46684,\"start\":46681},{\"end\":47077,\"start\":47074},{\"end\":47085,\"start\":47081},{\"end\":47093,\"start\":47089},{\"end\":47452,\"start\":47449},{\"end\":47460,\"start\":47456},{\"end\":47466,\"start\":47464},{\"end\":47474,\"start\":47470},{\"end\":47482,\"start\":47478},{\"end\":47852,\"start\":47849},{\"end\":47861,\"start\":47856},{\"end\":47873,\"start\":47865},{\"end\":47879,\"start\":47877},{\"end\":47889,\"start\":47883},{\"end\":48262,\"start\":48259},{\"end\":48270,\"start\":48266},{\"end\":48276,\"start\":48274},{\"end\":48284,\"start\":48280},{\"end\":48291,\"start\":48288},{\"end\":48298,\"start\":48295},{\"end\":48558,\"start\":48555},{\"end\":48565,\"start\":48562},{\"end\":48572,\"start\":48569},{\"end\":48578,\"start\":48576},{\"end\":48585,\"start\":48582},{\"end\":48594,\"start\":48589},{\"end\":48601,\"start\":48598},{\"end\":48608,\"start\":48605},{\"end\":48991,\"start\":48988},{\"end\":48999,\"start\":48995},{\"end\":49006,\"start\":49003},{\"end\":49013,\"start\":49010},{\"end\":49022,\"start\":49017},{\"end\":49029,\"start\":49026},{\"end\":49035,\"start\":49033},{\"end\":49290,\"start\":49286},{\"end\":49297,\"start\":49294},{\"end\":49304,\"start\":49301},{\"end\":49312,\"start\":49308},{\"end\":49319,\"start\":49316},{\"end\":49326,\"start\":49323},{\"end\":49716,\"start\":49704},{\"end\":49722,\"start\":49720},{\"end\":49733,\"start\":49726},{\"end\":50039,\"start\":50035},{\"end\":50045,\"start\":50043},{\"end\":50052,\"start\":50049},{\"end\":50060,\"start\":50056},{\"end\":50066,\"start\":50064},{\"end\":50074,\"start\":50070},{\"end\":50082,\"start\":50078},{\"end\":50089,\"start\":50086},{\"end\":50096,\"start\":50093},{\"end\":50104,\"start\":50100},{\"end\":50632,\"start\":50621},{\"end\":50640,\"start\":50636},{\"end\":50648,\"start\":50644},{\"end\":50661,\"start\":50652},{\"end\":50669,\"start\":50665},{\"end\":50681,\"start\":50673},{\"end\":51167,\"start\":51160},{\"end\":51177,\"start\":51171},{\"end\":51193,\"start\":51181},{\"end\":51199,\"start\":51197},{\"end\":51206,\"start\":51203},{\"end\":51212,\"start\":51210},{\"end\":51642,\"start\":51639},{\"end\":51650,\"start\":51646},{\"end\":51658,\"start\":51654},{\"end\":51664,\"start\":51662},{\"end\":52061,\"start\":52057},{\"end\":52069,\"start\":52065},{\"end\":52077,\"start\":52073},{\"end\":52083,\"start\":52081},{\"end\":52480,\"start\":52473},{\"end\":52491,\"start\":52484},{\"end\":52501,\"start\":52495},{\"end\":52514,\"start\":52505},{\"end\":52523,\"start\":52518},{\"end\":52534,\"start\":52529},{\"end\":52544,\"start\":52538},{\"end\":52558,\"start\":52548},{\"end\":52833,\"start\":52829},{\"end\":52842,\"start\":52837},{\"end\":52849,\"start\":52846},{\"end\":52861,\"start\":52853},{\"end\":53291,\"start\":53287},{\"end\":53300,\"start\":53295},{\"end\":53308,\"start\":53304},{\"end\":53316,\"start\":53312},{\"end\":53323,\"start\":53320},{\"end\":53331,\"start\":53327},{\"end\":53343,\"start\":53335},{\"end\":53598,\"start\":53593},{\"end\":53606,\"start\":53602},{\"end\":53614,\"start\":53610},{\"end\":53623,\"start\":53618},{\"end\":53631,\"start\":53627},{\"end\":53637,\"start\":53635},{\"end\":53644,\"start\":53641},{\"end\":53652,\"start\":53648},{\"end\":53664,\"start\":53656},{\"end\":53672,\"start\":53668},{\"end\":54045,\"start\":54043},{\"end\":54052,\"start\":54049},{\"end\":54062,\"start\":54056},{\"end\":54448,\"start\":54446},{\"end\":54456,\"start\":54452},{\"end\":54467,\"start\":54462},{\"end\":54477,\"start\":54471},{\"end\":54487,\"start\":54481},{\"end\":54945,\"start\":54943},{\"end\":54954,\"start\":54949},{\"end\":54963,\"start\":54958},{\"end\":54970,\"start\":54967},{\"end\":55270,\"start\":55266},{\"end\":55278,\"start\":55274},{\"end\":55287,\"start\":55282},{\"end\":55293,\"start\":55291},{\"end\":55300,\"start\":55297},{\"end\":55585,\"start\":55583},{\"end\":55592,\"start\":55589},{\"end\":55598,\"start\":55596},{\"end\":55605,\"start\":55602},{\"end\":55611,\"start\":55609},{\"end\":55619,\"start\":55615},{\"end\":56034,\"start\":56030},{\"end\":56042,\"start\":56038},{\"end\":56053,\"start\":56046},{\"end\":56332,\"start\":56328},{\"end\":56341,\"start\":56336},{\"end\":56348,\"start\":56345},{\"end\":56356,\"start\":56352},{\"end\":56364,\"start\":56360},{\"end\":56373,\"start\":56368},{\"end\":56380,\"start\":56377},{\"end\":56750,\"start\":56745},{\"end\":56756,\"start\":56754},{\"end\":56765,\"start\":56760},{\"end\":56772,\"start\":56769},{\"end\":57156,\"start\":57152},{\"end\":57168,\"start\":57162},{\"end\":57178,\"start\":57172},{\"end\":57590,\"start\":57586},{\"end\":57597,\"start\":57594},{\"end\":57603,\"start\":57601},{\"end\":57612,\"start\":57607},{\"end\":57620,\"start\":57616},{\"end\":57628,\"start\":57624},{\"end\":57956,\"start\":57953},{\"end\":57962,\"start\":57960},{\"end\":57968,\"start\":57966},{\"end\":57974,\"start\":57972},{\"end\":57982,\"start\":57978},{\"end\":57989,\"start\":57986},{\"end\":58262,\"start\":58259},{\"end\":58270,\"start\":58266},{\"end\":58278,\"start\":58274},{\"end\":58287,\"start\":58282},{\"end\":58294,\"start\":58291}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":232417054},\"end\":39224,\"start\":38798},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15155826},\"end\":39652,\"start\":39226},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1710722},\"end\":40172,\"start\":39654},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218889832},\"end\":40534,\"start\":40174},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206596127},\"end\":40972,\"start\":40536},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5011503},\"end\":41526,\"start\":40974},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211096730},\"end\":41890,\"start\":41528},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":248406101},\"end\":42411,\"start\":41892},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4028864},\"end\":42811,\"start\":42413},{\"attributes\":{\"doi\":\"arXiv:2108.07755\",\"id\":\"b9\"},\"end\":43073,\"start\":42813},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49742219},\"end\":43450,\"start\":43075},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3060010},\"end\":43910,\"start\":43452},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15816723},\"end\":44537,\"start\":43912},{\"attributes\":{\"id\":\"b13\"},\"end\":44885,\"start\":44539},{\"attributes\":{\"doi\":\"arXiv:1705.06950\",\"id\":\"b14\"},\"end\":45279,\"start\":44887},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9426884},\"end\":45496,\"start\":45281},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207852689},\"end\":46013,\"start\":45498},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":232335667},\"end\":46577,\"start\":46015},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":198179957},\"end\":47031,\"start\":46579},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":93002},\"end\":47372,\"start\":47033},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":47009464},\"end\":47804,\"start\":47374},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":47252984},\"end\":48200,\"start\":47806},{\"attributes\":{\"doi\":\"arXiv:2106.10271\",\"id\":\"b22\"},\"end\":48478,\"start\":48202},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":232352874},\"end\":48984,\"start\":48480},{\"attributes\":{\"doi\":\"arXiv:2106.13230\",\"id\":\"b24\"},\"end\":49220,\"start\":48986},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":196183677},\"end\":49700,\"start\":49222},{\"attributes\":{\"id\":\"b26\"},\"end\":49953,\"start\":49702},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":232335357},\"end\":50531,\"start\":49955},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":67855581},\"end\":51103,\"start\":50533},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":237421037},\"end\":51567,\"start\":51105},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":231786590},\"end\":51999,\"start\":51569},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":91184137},\"end\":52391,\"start\":52001},{\"attributes\":{\"id\":\"b32\"},\"end\":52755,\"start\":52393},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2398945},\"end\":53223,\"start\":52757},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":541442},\"end\":53589,\"start\":53225},{\"attributes\":{\"doi\":\"arXiv:1608.00797\",\"id\":\"b35\"},\"end\":53967,\"start\":53591},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10140667},\"end\":54381,\"start\":53969},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":208291175},\"end\":54865,\"start\":54383},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":235358331},\"end\":55199,\"start\":54867},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":221202136},\"end\":55513,\"start\":55201},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":201705136},\"end\":55967,\"start\":55515},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":15250191},\"end\":56261,\"start\":55969},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":202538533},\"end\":56741,\"start\":56263},{\"attributes\":{\"doi\":\"arXiv:2202.10108\",\"id\":\"b43\"},\"end\":57079,\"start\":56743},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":227227770},\"end\":57515,\"start\":57081},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":220546492},\"end\":57875,\"start\":57517},{\"attributes\":{\"doi\":\"arXiv:2010.04159\",\"id\":\"b46\"},\"end\":58185,\"start\":57877},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":236447522},\"end\":58645,\"start\":58187}]", "bib_title": "[{\"end\":38831,\"start\":38798},{\"end\":39283,\"start\":39226},{\"end\":39729,\"start\":39654},{\"end\":40218,\"start\":40174},{\"end\":40603,\"start\":40536},{\"end\":41047,\"start\":40974},{\"end\":41597,\"start\":41528},{\"end\":41966,\"start\":41892},{\"end\":42446,\"start\":42413},{\"end\":43130,\"start\":43075},{\"end\":43524,\"start\":43452},{\"end\":44004,\"start\":43912},{\"end\":45328,\"start\":45281},{\"end\":45568,\"start\":45498},{\"end\":46093,\"start\":46015},{\"end\":46649,\"start\":46579},{\"end\":47070,\"start\":47033},{\"end\":47445,\"start\":47374},{\"end\":47843,\"start\":47806},{\"end\":48551,\"start\":48480},{\"end\":49282,\"start\":49222},{\"end\":50031,\"start\":49955},{\"end\":50617,\"start\":50533},{\"end\":51156,\"start\":51105},{\"end\":51635,\"start\":51569},{\"end\":52053,\"start\":52001},{\"end\":52825,\"start\":52757},{\"end\":53283,\"start\":53225},{\"end\":54039,\"start\":53969},{\"end\":54442,\"start\":54383},{\"end\":54939,\"start\":54867},{\"end\":55262,\"start\":55201},{\"end\":55579,\"start\":55515},{\"end\":56026,\"start\":55969},{\"end\":56324,\"start\":56263},{\"end\":57148,\"start\":57081},{\"end\":57582,\"start\":57517},{\"end\":58255,\"start\":58187}]", "bib_author": "[{\"end\":38842,\"start\":38833},{\"end\":38854,\"start\":38842},{\"end\":38865,\"start\":38854},{\"end\":38872,\"start\":38865},{\"end\":38881,\"start\":38872},{\"end\":38891,\"start\":38881},{\"end\":39294,\"start\":39285},{\"end\":39303,\"start\":39294},{\"end\":39316,\"start\":39303},{\"end\":39327,\"start\":39316},{\"end\":39748,\"start\":39731},{\"end\":39760,\"start\":39748},{\"end\":39770,\"start\":39760},{\"end\":39788,\"start\":39770},{\"end\":40230,\"start\":40220},{\"end\":40239,\"start\":40230},{\"end\":40251,\"start\":40239},{\"end\":40262,\"start\":40251},{\"end\":40274,\"start\":40262},{\"end\":40287,\"start\":40274},{\"end\":40617,\"start\":40605},{\"end\":40630,\"start\":40617},{\"end\":41059,\"start\":41049},{\"end\":41079,\"start\":41059},{\"end\":41090,\"start\":41079},{\"end\":41100,\"start\":41090},{\"end\":41108,\"start\":41100},{\"end\":41122,\"start\":41108},{\"end\":41607,\"start\":41599},{\"end\":41620,\"start\":41607},{\"end\":41631,\"start\":41620},{\"end\":41641,\"start\":41631},{\"end\":41976,\"start\":41968},{\"end\":41983,\"start\":41976},{\"end\":41992,\"start\":41983},{\"end\":42001,\"start\":41992},{\"end\":42008,\"start\":42001},{\"end\":42015,\"start\":42008},{\"end\":42455,\"start\":42448},{\"end\":42461,\"start\":42455},{\"end\":42470,\"start\":42461},{\"end\":42476,\"start\":42470},{\"end\":42485,\"start\":42476},{\"end\":42491,\"start\":42485},{\"end\":42498,\"start\":42491},{\"end\":42867,\"start\":42859},{\"end\":42876,\"start\":42867},{\"end\":42883,\"start\":42876},{\"end\":42894,\"start\":42883},{\"end\":42903,\"start\":42894},{\"end\":43139,\"start\":43132},{\"end\":43147,\"start\":43139},{\"end\":43158,\"start\":43147},{\"end\":43533,\"start\":43526},{\"end\":43541,\"start\":43533},{\"end\":43549,\"start\":43541},{\"end\":43556,\"start\":43549},{\"end\":43567,\"start\":43556},{\"end\":44017,\"start\":44006},{\"end\":44030,\"start\":44017},{\"end\":44550,\"start\":44539},{\"end\":44557,\"start\":44550},{\"end\":44573,\"start\":44557},{\"end\":44585,\"start\":44573},{\"end\":44595,\"start\":44585},{\"end\":44603,\"start\":44595},{\"end\":44617,\"start\":44603},{\"end\":44894,\"start\":44887},{\"end\":44906,\"start\":44894},{\"end\":44918,\"start\":44906},{\"end\":44927,\"start\":44918},{\"end\":44938,\"start\":44927},{\"end\":44958,\"start\":44938},{\"end\":44967,\"start\":44958},{\"end\":44976,\"start\":44967},{\"end\":44984,\"start\":44976},{\"end\":44994,\"start\":44984},{\"end\":45340,\"start\":45330},{\"end\":45577,\"start\":45570},{\"end\":45583,\"start\":45577},{\"end\":45591,\"start\":45583},{\"end\":45598,\"start\":45591},{\"end\":45605,\"start\":45598},{\"end\":45612,\"start\":45605},{\"end\":45620,\"start\":45612},{\"end\":45626,\"start\":45620},{\"end\":45635,\"start\":45626},{\"end\":45641,\"start\":45635},{\"end\":46102,\"start\":46095},{\"end\":46108,\"start\":46102},{\"end\":46115,\"start\":46108},{\"end\":46123,\"start\":46115},{\"end\":46130,\"start\":46123},{\"end\":46138,\"start\":46130},{\"end\":46144,\"start\":46138},{\"end\":46153,\"start\":46144},{\"end\":46159,\"start\":46153},{\"end\":46658,\"start\":46651},{\"end\":46665,\"start\":46658},{\"end\":46671,\"start\":46665},{\"end\":46679,\"start\":46671},{\"end\":46686,\"start\":46679},{\"end\":47079,\"start\":47072},{\"end\":47087,\"start\":47079},{\"end\":47095,\"start\":47087},{\"end\":47454,\"start\":47447},{\"end\":47462,\"start\":47454},{\"end\":47468,\"start\":47462},{\"end\":47476,\"start\":47468},{\"end\":47484,\"start\":47476},{\"end\":47854,\"start\":47845},{\"end\":47863,\"start\":47854},{\"end\":47875,\"start\":47863},{\"end\":47881,\"start\":47875},{\"end\":47891,\"start\":47881},{\"end\":48264,\"start\":48257},{\"end\":48272,\"start\":48264},{\"end\":48278,\"start\":48272},{\"end\":48286,\"start\":48278},{\"end\":48293,\"start\":48286},{\"end\":48300,\"start\":48293},{\"end\":48560,\"start\":48553},{\"end\":48567,\"start\":48560},{\"end\":48574,\"start\":48567},{\"end\":48580,\"start\":48574},{\"end\":48587,\"start\":48580},{\"end\":48596,\"start\":48587},{\"end\":48603,\"start\":48596},{\"end\":48610,\"start\":48603},{\"end\":48993,\"start\":48986},{\"end\":49001,\"start\":48993},{\"end\":49008,\"start\":49001},{\"end\":49015,\"start\":49008},{\"end\":49024,\"start\":49015},{\"end\":49031,\"start\":49024},{\"end\":49037,\"start\":49031},{\"end\":49292,\"start\":49284},{\"end\":49299,\"start\":49292},{\"end\":49306,\"start\":49299},{\"end\":49314,\"start\":49306},{\"end\":49321,\"start\":49314},{\"end\":49328,\"start\":49321},{\"end\":49718,\"start\":49702},{\"end\":49724,\"start\":49718},{\"end\":49735,\"start\":49724},{\"end\":50041,\"start\":50033},{\"end\":50047,\"start\":50041},{\"end\":50054,\"start\":50047},{\"end\":50062,\"start\":50054},{\"end\":50068,\"start\":50062},{\"end\":50076,\"start\":50068},{\"end\":50084,\"start\":50076},{\"end\":50091,\"start\":50084},{\"end\":50098,\"start\":50091},{\"end\":50106,\"start\":50098},{\"end\":50634,\"start\":50619},{\"end\":50642,\"start\":50634},{\"end\":50650,\"start\":50642},{\"end\":50663,\"start\":50650},{\"end\":50671,\"start\":50663},{\"end\":50683,\"start\":50671},{\"end\":51169,\"start\":51158},{\"end\":51179,\"start\":51169},{\"end\":51195,\"start\":51179},{\"end\":51201,\"start\":51195},{\"end\":51208,\"start\":51201},{\"end\":51214,\"start\":51208},{\"end\":51644,\"start\":51637},{\"end\":51652,\"start\":51644},{\"end\":51660,\"start\":51652},{\"end\":51666,\"start\":51660},{\"end\":52063,\"start\":52055},{\"end\":52071,\"start\":52063},{\"end\":52079,\"start\":52071},{\"end\":52085,\"start\":52079},{\"end\":52482,\"start\":52471},{\"end\":52493,\"start\":52482},{\"end\":52503,\"start\":52493},{\"end\":52516,\"start\":52503},{\"end\":52525,\"start\":52516},{\"end\":52536,\"start\":52525},{\"end\":52546,\"start\":52536},{\"end\":52560,\"start\":52546},{\"end\":52835,\"start\":52827},{\"end\":52844,\"start\":52835},{\"end\":52851,\"start\":52844},{\"end\":52863,\"start\":52851},{\"end\":53293,\"start\":53285},{\"end\":53302,\"start\":53293},{\"end\":53310,\"start\":53302},{\"end\":53318,\"start\":53310},{\"end\":53325,\"start\":53318},{\"end\":53333,\"start\":53325},{\"end\":53345,\"start\":53333},{\"end\":53600,\"start\":53591},{\"end\":53608,\"start\":53600},{\"end\":53616,\"start\":53608},{\"end\":53625,\"start\":53616},{\"end\":53633,\"start\":53625},{\"end\":53639,\"start\":53633},{\"end\":53646,\"start\":53639},{\"end\":53654,\"start\":53646},{\"end\":53666,\"start\":53654},{\"end\":53674,\"start\":53666},{\"end\":54047,\"start\":54041},{\"end\":54054,\"start\":54047},{\"end\":54064,\"start\":54054},{\"end\":54450,\"start\":54444},{\"end\":54458,\"start\":54450},{\"end\":54469,\"start\":54458},{\"end\":54479,\"start\":54469},{\"end\":54489,\"start\":54479},{\"end\":54947,\"start\":54941},{\"end\":54956,\"start\":54947},{\"end\":54965,\"start\":54956},{\"end\":54972,\"start\":54965},{\"end\":55272,\"start\":55264},{\"end\":55280,\"start\":55272},{\"end\":55289,\"start\":55280},{\"end\":55295,\"start\":55289},{\"end\":55302,\"start\":55295},{\"end\":55587,\"start\":55581},{\"end\":55594,\"start\":55587},{\"end\":55600,\"start\":55594},{\"end\":55607,\"start\":55600},{\"end\":55613,\"start\":55607},{\"end\":55621,\"start\":55613},{\"end\":56036,\"start\":56028},{\"end\":56044,\"start\":56036},{\"end\":56055,\"start\":56044},{\"end\":56334,\"start\":56326},{\"end\":56343,\"start\":56334},{\"end\":56350,\"start\":56343},{\"end\":56358,\"start\":56350},{\"end\":56366,\"start\":56358},{\"end\":56375,\"start\":56366},{\"end\":56382,\"start\":56375},{\"end\":56752,\"start\":56743},{\"end\":56758,\"start\":56752},{\"end\":56767,\"start\":56758},{\"end\":56774,\"start\":56767},{\"end\":57158,\"start\":57150},{\"end\":57170,\"start\":57158},{\"end\":57180,\"start\":57170},{\"end\":57592,\"start\":57584},{\"end\":57599,\"start\":57592},{\"end\":57605,\"start\":57599},{\"end\":57614,\"start\":57605},{\"end\":57622,\"start\":57614},{\"end\":57630,\"start\":57622},{\"end\":57958,\"start\":57951},{\"end\":57964,\"start\":57958},{\"end\":57970,\"start\":57964},{\"end\":57976,\"start\":57970},{\"end\":57984,\"start\":57976},{\"end\":57991,\"start\":57984},{\"end\":58264,\"start\":58257},{\"end\":58272,\"start\":58264},{\"end\":58280,\"start\":58272},{\"end\":58289,\"start\":58280},{\"end\":58296,\"start\":58289}]", "bib_venue": "[{\"end\":38962,\"start\":38891},{\"end\":39394,\"start\":39327},{\"end\":39865,\"start\":39788},{\"end\":40325,\"start\":40287},{\"end\":40707,\"start\":40630},{\"end\":41199,\"start\":41122},{\"end\":41685,\"start\":41641},{\"end\":42096,\"start\":42015},{\"end\":42565,\"start\":42498},{\"end\":42857,\"start\":42813},{\"end\":43222,\"start\":43158},{\"end\":43634,\"start\":43567},{\"end\":44126,\"start\":44030},{\"end\":44684,\"start\":44617},{\"end\":45049,\"start\":45010},{\"end\":45374,\"start\":45340},{\"end\":45702,\"start\":45641},{\"end\":46240,\"start\":46159},{\"end\":46757,\"start\":46686},{\"end\":47161,\"start\":47095},{\"end\":47548,\"start\":47484},{\"end\":47958,\"start\":47891},{\"end\":48255,\"start\":48202},{\"end\":48681,\"start\":48610},{\"end\":49075,\"start\":49053},{\"end\":49409,\"start\":49328},{\"end\":49812,\"start\":49735},{\"end\":50187,\"start\":50106},{\"end\":50764,\"start\":50683},{\"end\":51285,\"start\":51214},{\"end\":51737,\"start\":51666},{\"end\":52152,\"start\":52085},{\"end\":52469,\"start\":52393},{\"end\":52940,\"start\":52863},{\"end\":53362,\"start\":53345},{\"end\":53749,\"start\":53690},{\"end\":54131,\"start\":54064},{\"end\":54570,\"start\":54489},{\"end\":55021,\"start\":54972},{\"end\":55339,\"start\":55302},{\"end\":55692,\"start\":55621},{\"end\":56090,\"start\":56055},{\"end\":56453,\"start\":56382},{\"end\":56887,\"start\":56790},{\"end\":57251,\"start\":57180},{\"end\":57668,\"start\":57630},{\"end\":57949,\"start\":57877},{\"end\":58367,\"start\":58296},{\"end\":39020,\"start\":38964},{\"end\":39448,\"start\":39396},{\"end\":39929,\"start\":39867},{\"end\":40771,\"start\":40709},{\"end\":41263,\"start\":41201},{\"end\":42164,\"start\":42098},{\"end\":42619,\"start\":42567},{\"end\":43273,\"start\":43224},{\"end\":43688,\"start\":43636},{\"end\":44209,\"start\":44128},{\"end\":45750,\"start\":45704},{\"end\":46308,\"start\":46242},{\"end\":46815,\"start\":46759},{\"end\":47214,\"start\":47163},{\"end\":47599,\"start\":47550},{\"end\":48012,\"start\":47960},{\"end\":48739,\"start\":48683},{\"end\":49477,\"start\":49411},{\"end\":50255,\"start\":50189},{\"end\":50832,\"start\":50766},{\"end\":51343,\"start\":51287},{\"end\":51795,\"start\":51739},{\"end\":52206,\"start\":52154},{\"end\":53004,\"start\":52942},{\"end\":54185,\"start\":54133},{\"end\":54638,\"start\":54572},{\"end\":55750,\"start\":55694},{\"end\":56511,\"start\":56455},{\"end\":57309,\"start\":57253},{\"end\":58425,\"start\":58369}]"}}}, "year": 2023, "month": 12, "day": 17}