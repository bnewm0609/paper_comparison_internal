{"id": 221266650, "updated": "2023-10-06 12:18:23.075", "metadata": {"title": "A Unified Taylor Framework for Revisiting Attribution Methods", "authors": "[{\"first\":\"Huiqi\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Na\",\"last\":\"Zou\",\"middle\":[]},{\"first\":\"Mengnan\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Weifu\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Guocan\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Xia\",\"last\":\"Hu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 8, "day": 21}, "abstract": "Attribution methods have been developed to understand the decision making process of machine learning models, especially deep neural networks, by assigning importance scores to individual features. Existing attribution methods often built upon empirical intuitions and heuristics. There still lacks a unified framework that can provide deeper understandings of their rationales, theoretical fidelity, and limitations. To bridge the gap, we present a Taylor attribution framework to theoretically characterize the fidelity of explanations. The key idea is to decompose model behaviors into first-order, high-order independent, and high-order interactive terms, which makes clearer attribution of high-order effects and complex feature interactions. Three desired properties are proposed for Taylor attributions, i.e., low model approximation error, accurate assignment of independent and interactive effects. Moreover, several popular attribution methods are mathematically reformulated under the unified Taylor attribution framework. Our theoretical investigations indicate that these attribution methods implicitly reflect high-order terms involving complex feature interdependencies. Among these methods, Integrated Gradient is the only one satisfying the proposed three desired properties. New attribution methods are proposed based on Integrated Gradient by utilizing the Taylor framework. Experimental results show that the proposed method outperforms the existing ones in model interpretations.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "2008.09695", "mag": "3080190064", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/DengZDCFH21", "doi": "10.1609/aaai.v35i13.17365"}}, "content": {"source": {"pdf_hash": "40ea10d2112d19a3da61d0f1b21f87cb18156049", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.09695v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "dfaab566f3acf037470e661db19abc28e218a007", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/40ea10d2112d19a3da61d0f1b21f87cb18156049.txt", "contents": "\nA Unified Taylor Framework for Revisiting Attribution Methods\n\n\nHuiqi Deng \nSchool of Mathematics\nSun Yat-Sen University\nChina\n\nNa Zou \nDepartment of Engineering Technology and Industrial Distribution\nTexas A&M University\nUSA\n\nMengnan Du dumengnan@tamu.edu \nDepartment of Computer Science and Engineering\nTexas A&M University\nUSA\n\nWeifu Chen \nSchool of Mathematics\nSun Yat-Sen University\nChina\n\nGuocan Feng \nSchool of Mathematics\nSun Yat-Sen University\nChina\n\nXia Hu xiahu@tamu.edu \nDepartment of Computer Science and Engineering\nTexas A&M University\nUSA\n\nA Unified Taylor Framework for Revisiting Attribution Methods\n\nAttribution methods have been developed to understand the decision making process of machine learning models, especially deep neural networks, by assigning importance scores to individual features. Existing attribution methods often built upon empirical intuitions and heuristics. There still lacks a unified framework that can provide deeper understandings of their rationales, theoretical fidelity, and limitations. To bridge the gap, we present a Taylor attribution framework to theoretically characterize the fidelity of explanations. The key idea is to decompose model behaviors into first-order, high-order independent, and high-order interactive terms, which makes clearer attribution of high-order effects and complex feature interactions. Three desired properties are proposed for Taylor attributions, i.e., low model approximation error, accurate assignment of independent and interactive effects. Moreover, several popular attribution methods are mathematically reformulated under the unified Taylor attribution framework. Our theoretical investigations indicate that these attribution methods, which are widely considered as first-order terms of model behavior, implicitly reflect high-order terms involving complex feature interdependencies. Among these methods, Integrated Gradient is the only one satisfying the proposed three desired properties. New attribution methods are proposed based on Integrated Gradient by utilizing the Taylor framework. Experimental results show that the proposed method outperforms the existing ones in model interpretations. * This work is conducted during her visit at Texas A&M University.Preprint. Under review.\n\nIntroduction\n\nAttribution methods have become an effective computational tool in understanding the behavior of machine learning models, especially Deep Neural Networks (DNNs) [1]. It uncovers how DNNs make a prediction by calculating the contribution score of each input feature to the final prediction. For example, in image classification, the attribution methods aim to infer the contribution of each pixel to the prediction for a pre-trained model. Recently several attribution methods [2][3][4][5][6][7][8] have been proposed to create saliency maps with visually pleasing results. However, the designs of attribution methods are based on different heuristics without deep theoretical understanding. Although related work have indicated some kinds of formulation unification [9,10], there still lacks further investigation on underlying rationales, fidelity, and limitations. In other words, the following interesting questions are rarely explored and need theoretical investigation: i) What model behaviors do these attribution methods actually reveal (rationale); ii) How much can decision making process be reflected (fidelity); iii) What are their limitations.\n\nAnswering those questions is difficult mainly due to the following two challenges. The first challenge (C1) is that an objective and formal definition of the fidelity for a decision making process is missing, especially for DNN models due to their black-box nature. The existing definitions of fidelity [11][12][13][14] are subjective. For instance, object localization performance [11], a popular evaluation metric, evaluates the consistence between explanations and human cognition, while the human cognition may have a divergence with the underlying model behavior. The second challenge (C2) is lacking a unified theoretical tool to connect attribution methods and model behaviors, so as to reveal the underlying rationales and corresponding limitations of these attribution methods. The existing attribution methods are mostly based on heuristics, and there is little knowledge about the model behaviors they convey.\n\nTo address the aforementioned two challenges, we propose a theoretical Taylor attribution framework based on Taylor expansion, to characterize the fidelity of the interpretation and assess existing attribution methods. To address C1, we give a theoretical definition of the fidelity in two steps. First, DNNs are difficult to be explained directly due to large amounts of function composition. Instead of explaining DNNs directly, we explain a sufficient approximation function of DNNs, which is much more understandable to humans (e.g., polynomial function family) [15]. We adopt Taylor polynomial expansion function as it has a theoretical guarantee on approximation error. The fidelity of the explanation in DNNs is equivalent to the fidelity of the explanation in Taylor approximation function, when the approximation error is sufficiently small. Second, to explain the Taylor approximation function, Taylor attribution framework decomposes the function into first-order, context-independent high-order, context-aware high-order model behaviors. Three desired properties are introduced for a faithful attribution method of DNNs. The framework reveals that the feature importance is not in an isolated fashion and contributions of feature interaction should not be neglected.\n\nTo address C2, we investigate the relationship between the proposed Taylor attribution framework and existing attribution methods. It is computationally infeasible to directly analyze the attribution methods by using the Taylor attribution framework, due to the high complexity to compute partial derivatives. Instead, these attribution methods could be reformulated into a unified Taylor attribution framework. The Taylor reformulations uncover theoretical insights of rationales, measure fidelity, and summarize limitations of these attribution methods in a systematic and sufficient way. Therefore, new attribution methods are developed to improve interpretation performance.\n\nIn summary, this paper includes the following major contributions:\n\n\u2022 The explanation fidelity of DNNs is generally defined via Taylor expansion function, which is a sufficient approximation with theoretical guarantee on approximation error.\n\n\u2022 A Taylor attribution framework is proposed to evaluate the explanation fidelity of Taylor expansion function. Three desired properties are derived for a faithful explanation for DNNs.\n\n\u2022 Several existing attribution methods are reformulated into the unified Taylor attribution framework for a systematic and theoretical investigation on their rationale, fidelity, and limitations. New attribution methods are proposed to improve interpretation performance.\n\n\nTaylor Attribution Framework\n\nIn this section, we propose a Taylor attribution framework to theoretically interpret and understand how input features contribute to the decision making process in DNNs. Given a pre-trained DNN model f and a sample x = [x 1 , x 2 , . . . , x n ] \u2208 R n , attribution methods aim to characterize the contribution of each feature x i to its prediction f (x). It is not tractable to analyze DNN models directly due to multi-layers of function compositions. Our basic idea is to address the attribution problem by using a much more interpretable approximation function of f (x). We adopt Taylor polynomial expansion function, denoted as g(x), to approximate DNNs f (x), due to its theoretical guarantee on the approximation error. The fidelity of the attribution of f (x) is equivalent to the fidelity of the attribution of g(x), when the approximation error of g(x) is sufficiently small. Next, Figure 1: An overview of Taylor attribution framework. Given a second-order Taylor expansion function g(x) as an example. g(x) is composed of first-order independent, second-order independent, and second-order interactive terms. The first-order and second-order independent terms of x i are clearly assigned to x i (property 2), as shown in solid line. The second-order interactive term between x i and x j should be and only be assigned to x i and x j (property 3), as shown in dash line.\n\nwe will elaborate how to address the attribution problem of g(x). An overview of Taylor attribution framework is shown in Figure 1.\n\nThe first-order Taylor expansion of f at x is,\nf (x) \u2212 f (x) = f x \u2206 + = i f xi \u2206 i + .(1)\nwherex represents a selected baseline point, and\n\u2206 def =x \u2212 x, \u2206 i def =x i \u2212 x i .\nis the approximation error between f (x) and the approximation function g(x) at pointx. We ignore the constant term f (x) if not mentioned thereafter, as the baseline pointx of attribution methods always satisfies f (x) = 0. In addition, we omit a negative sign of g(x), which would not affect the attribution.\n\nFirst-order Taylor attribution In first-order Taylor expansion, the linear approximation function g(x) = i f xi \u2206 i is additive across features and can be easily interpreted. It is obvious that f xi \u2206 i quantifies the contribution of i-th feature x i to the prediction. a i is denoted as the attribution score of i-th feature, i.e.,\na i = f xi \u2206 i .(2)\nThe second-order Taylor expansion of f at x is,\nf (x) \u2212 f (x) = f x \u2206 first-order term F + 1 2 \u2206 T H x \u2206 second-order term S + ,(3)\nwhere H x is the second-order partial derivative matrix (Hessian matrix) of f at a given point x.\n\nThe second-order Taylor expansion, with the second-order term S = 1 2 \u2206 T H x \u2206, has a smaller approximation error than the first-order one.\n\nSecond-order Taylor attribution The second-order approximation g(x) = f x \u2206 + 1 2 \u2206 T H x \u2206 is indistinct in determining features contribution compared with first-order expansion due to Hessian matrix H x . To make it more interpretable, the Hessian matrix H x is decomposed into two matrices: a second-order independent matrix H ind x and an interactive matrix H int\nx def = H x \u2212 H ind x , where H ind x is a diagonal matrix composed of the diagonal elements in H x . H ind x\ndescribes the secondorder isolated effect of features, and H int x represents the interactive effect among different features. Hence, the second-order expansion could be rewritten as the sum of first order term F , second-order independent term S ind , and second-order interactive term S int (S = S ind + S int ):\nf (x) \u2212 f (x) = f x \u2206 first-order term F + 1 2 \u2206 T H ind x \u2206 second-order independent term S ind + 1 2 \u2206 T H int x \u2206 second-order interactive term S int + . (4)\nHere, the second-order independent term can be decomposed as\nS ind = i S ind i , where S ind i = 1 2 f xixi \u2206 2 i represents effect of x 2 i .\nSince F and S ind are additive across features, the contribution for feature x i from F and S ind can be clearly identified as\nf xi \u2206 i + 1 2 f xixi \u2206 2 i . The second-order interactive term can be formulated as S int = ij S int ij , where S int ij = f xixj \u2206 i \u2206 j denotes interactive effect of x i x j .\nIt has been debated frequently in performance measurement [16] regarding how to assign interactive effect S int ij to feature x i and x j respectively. This is even more challenging in neural network scenarios with high-complexity.\n\nWe propose to handle the interaction effect by following an intuition behind: the interaction effect of certain features should be and only be assigned to the contribution of corresponding features. For example, the interactive effect f xixj \u2206 i \u2206 j should be assigned to the contribution of feature x i and x j . Hence, we define the second-order Taylor attribution as\na i = f xi \u2206 i + 1 2 f xixi \u2206 2 i + a int i = f xi \u2206 i + 1 2 f xixi \u2206 2 i + j =i w ij f xixj \u2206 i \u2206 j ,(5)\nwhere\na int i = j =i w ij f xixj \u2206 i \u2206 j is the allocated contribution of x i from the interaction effect S int i,. = j =i f xixj \u2206 i \u2206 j associated with x i .\nOne way to determine w ij is to equally split the interactive effects, i.e., w ij = 1 2 . Higher-order Taylor attribution All high-order expansion terms are denoted as T . Similar to the second-order case, T is decomposed into independent term T ind and interactive term T int :\nf (x) \u2212 f (x) = f x \u2206 + T high-order term = f x \u2206 + T ind independent term + T int interactive term . (6) F i , T ind i\nare defined as the allocated contributions of x i from first-order and high-order independent terms respectively. T int A is defined as the high-order interactive term among features in a subset A. The attribution of high-order Taylor expansion follows the similar rule as in the second-order case, i.e., interactive term T int A is only assigned to the features in the subset A. The fidelity of the attribution of f (x) depends on two factors: i) approximation error of g(x), ii) the fidelity of the attribution of g(x). Hence, integrating the proposed Taylor attribution framework, a faithful attribution method should satisfy the following three properties:\n\nProperty 1 A Taylor attribution method has low model approximation error, if for \u2200x, the approximation error is sufficiently small.\n\nProperty 2 A Taylor attribution method has accurate assignment of independent term, if for any feature x i , its first-order term and high-order independent terms are accurately assigned to the contribution of x i , instead of other features.\n\nProperty 3 A Taylor attribution method has accurate assignment of interactive term, if for any interactive term with features in a set A, it is only assigned to the features in A.\n\nTaylor attribution framework theoretically defines the fidelity of interpretation to the model and can be used to assess the existing attribution methods (Noted that although [17][18][19] also mentioned Taylor expansion function, they mainly focus on first-order and second-order expansion around neighborhood) . However, it is generally computationally infeasible in large-scale real-world applications. For example, when interpreting a prediction of an image with n pixels, k-order Taylor attribution would take O(n k ) to compute high-order partial derivatives. To tackle the computational challenge, we investigate the theoretical relationship between the proposed Taylor attribution framework and the existing attribution methods, and find that these attribution methods can be unified into the Taylor attribution framework via reformulations.\n\n\nUnified Reformulation of Existing Attribution Methods\n\nIn this section, we mathematically reformulate several attribution methods into the Taylor attribution framework. We mainly focus on the following attribution methods: i) Gradient * Input [5], ii) Perturbation-1 [20], iii) Perturbation-patch [8], iv) DeepLift (Rescale) [4] and -LRP [2], v) Integrated Gradient [7]. Deconvnet [8] and Guided BP [21] are beyond the scope of discussion and not included. It has been theoretically proved [22] that these two methods are essentially constructing (partial) recovery to the input, which is unrelated to decision making. Empirical evidence in [23] also demonstrated that these two methods are not sensitive to the randomization of network parameters and target labels. Grad-CAM [14] is not included since it does not directly explain the behavior of convolutional layer. It explains CNNs by interpreting the fully-connected layers and utilizing the location information of the top convolutional layer. Next, we will elaborate on the Taylor reformulations of these attribution methods.\n\nI) Gradient * Input Gradient * Input was firstly proposed in [5] to generate saliency maps. The attribution is computed by multiplying the partial derivatives (of the output to the input) with the input. That is,\na i = f xi (x) * x i .\nProposition 1 Gradient*Input is a first-order Taylor attribution approximation of DNN model f . Here, the baseline point is set to 0 \u2208 R n . That is,\na i = F i .(7)\nGradient * Input satisfies Proposition 1. However, the linear approximation function can not reflect the highly nonlinear functions in DNNs and fails to satisfy Property 1.\n\nII) Perturbation-1 Perturbation-1 [20] attributes an input feature by calculating how much the prediction f (x) changes according to the perturbed feature x i . Specifically, x i is perturbed by a constant v, and a perturbed input is denoted as x , with\nx i = v, x j = x j (j = i).\nThe corresponding new output f (x ) is obtained by a forward pass. The difference between two outputs is considered as the attribution of feature\nx i , i.e., a i = f (x) \u2212 f (x ).\nProposition 2 Perturbation-1 is a context-independent high-order attribution approximation of DNN model f , i.e., the attribution is the sum of first-order term and high-order independent term. The attribution of x i is reformulated as (See Appendix A for proof):\na i = F i + T ind i .(8)\nCompared to Gradient * Input, perturbation-1 could characterize the high-order independent effects. However, it fails to incorporate the complex interactions among features (such as pixels), which captures critical information for prediction. \n\n\nIII) Perturbation-patch\n\n\nProposition 3\n\nThe attribution of perturbation-patch is the sum of the first-order term, the high-order independent term of all features in the patch and high-order interactions among features in the same patch. The attribution of x i \u2208 p j is reformulated as (See Appendix A for proof):\na i = F p j + T ind p j + T int p j = xi\u2208p j F i + xi\u2208p j T ind i + A\u2282p j T int A(9)\nIt is worth noting that Proposition 3 holds for a random subset of x. Perturbation-patch reflects the overall contribution of the features within the patch, which includes independent and interactive effects. However, it assigns the same contribution score to all features in the patch, which fails to provide fine-grained explanations. Moreover, the interactions among different patches are neglected. [2] compute relevance scores by using a recursive relevance propagation in a layerwise manner. In DeepLift (Rescale rule), x\n\n\nIV) DeepLift and -LRP DeepLift [4] and -LRP\n\ni , x (l+1) j represent the value of neuron i at l-th layer and neuron j at (l + 1)-th layer respectively, which satisfy x\n(l+1) j = \u03c3( i w ji x (l) i + b j ).\nHere, w ji is the weight parameter that connects x  \n\n\nMethod\n\nTaylor reformulation Properties\nGradient*Input f xi \u2206 i None Perturbation-1 f xi \u2206 i + 1 2 f xixi \u2206 2 i 2 Perturbation-patch i\u2208p f xi \u2206 i + i\u2208p 1 2 f xixi \u2206 2 i + i\u2208p j\u2208p f xixj \u2206 i \u2206 j 2 DeepLift(layerwise) f xi \u2206 i + w( i 1 2 f xixi \u2206 2 i + i j f xixj \u2206 i \u2206 j ) 1\nIntegratedGradients\nf xi \u2206 i + 1 2 f xixi \u2206 2 i + 1 2 j =i f xixj \u2206 i \u2206 j 1,2,3\nis the additive bias, and\nz ji = w ji x (l) i is the weighted impact of x (l) i to x (l+1) j\n. \u03c3 is a non-linear activation function. DeepLift aims to propagate the output difference between input x and a selected baselinex, wherez\n(l) ji = w jix (l)\ni is the corresponding weighted impact of baseline. DeepLift calculates the relevance score of x\n(l+1) j at x (l) i , denoted as a (l)\nij , as follows:\na (l) ij = z (l) ji \u2212z (l) ji i z (l) ji \u2212 i z (l) ji a (l+1) j Where a (l+1) j = k a (l+1) jk\nrepresents the total relevance score of neuron x \nij = F i + z (l) ji \u2212 z (l) ji i z (l) ji \u2212 i z (l) ji T(10)\nWhere T = i T ind i + A\u2282x T int A is the overall high-order expansion of the output difference of j-th neuron at (l + 1) layer.\n\nDeepLift follows the Summation-to-Delta property, i.e., the summation of attributions across all features is the change of output. It's considered that DeepLift satisfies Property 1. However, the reformulation in Proposition 4 indicates that it does not satisfy Property 2 and Property 3: accurate assignment of independent term and interactive term. It fails to distinguish the relative importance of features from high-order effect T .\n\n-LRP is similar to DeepLift, and shares the same limitation on distinguishing attributions of features from high-order term T (The details on reformulation derivation of -LRP are shown in Appendix A). [7] integrates the gradient over the straightline path from baselinex to input x. The points on the path are represented as\n\n\nV) Integrated Gradient Given a baseline pointx, Integrated Gradient\nx =x + \u03b1(x \u2212x), \u03b1 \u2208 [0, 1]. The attribution of feature x i in input x is: a i = (x i \u2212x i ) 1 0 \u2202f (x + \u03b1(x \u2212x)) \u2202x i d\u03b1 \u2248 (x i \u2212x i ) 1 m m k=1 \u2202f (x + k m (x \u2212x)) \u2202x i .\nwhere m is the number of steps in the Riemman approximation of the integral.\n\nProposition 5 Integrated Gradient is a context-aware high-order attribution approximation of DNN f . The attribution of x i is the sum of first-order term F i , high-order independent term T ind i , and an interactive term a int i . The interactive term a int i is the allocated contribution to x i from overall interactive effect T int i,. between x i and other features (See Appendix A for proof).\na i = F i + T ind i + a int i(11)\nwhere i a i = \u2206f . Specifically, Integrated Gradient assigns the contribution of x i from the interactive effect T int i,. , according to the degree of polynomial term. It allcolates ki k proportion of the high-order interactive term \u2206 k1 1 . . . \u2206 ki i . . . \u2206 kn n to the feature x i ( i k i = k). For example, considering the second-order case,\na i = f xi \u2206 i + 1 2 f xixi \u2206 2 i + 1 2 j =i f xixj \u2206 i \u2206 j .\nAccording to Proposition 5, Integrated Gradient method, which is widely considered as a first-order attribution, not only describes the first order and context-independent high-order attribution of each feature x i but also comprehensively incorporates the interaction effect between x i and other features. This theoretical insight uncovers why Integrated Gradient method can well discriminate features from the input image. It satisfies the proposed desired Properties 1, 2 and 3: low model approximation error, accurate assignment of independent term, and accurate assignment of interactive term, while all other aforementioned attribution methods fail to do so. Table 1 summarizes the second-order Taylor reformulations of these existing attribution methods and whether the proposed three desired properties are satisfied.\n\n\nImprovements on Integrated Gradient\n\nAccording to the reformulations, Integrated Gradient is the only method satisfying three desired properties among these attribution methods. However, the reformulation of Integrated Gradient also indicates that attribution is closely related to input change \u2206, which is determined by the chosen baseline. This leads to one major defect that Integrated Gradient method is sensitive to the baseline but not to model parameters and target labels [23].\n\nWe consider three strategies to overcome the weakness. First, we rescale the attribution a i with respect to \u2206 i , which could alleviate the correlation to some extent. Specifically, a i = ai \u2206i = 1 0 \u2202f (x+\u03b1\u2206) \u2202xi d\u03b1, denoted by IG1. According to Proposition 5, the Integrated Gradient attribution a i represents the influence on prediction difference \u2206f caused by the change of feature \u2206 i . Hence a i actually measures the sensitivity of output with respect to the feature x i , i.e., the expected change in the output as each feature x i changes by one unit.\n\nSecond, we set limit on the magnitude of \u2206 to further relieve the correlation. The rescaling relieves the sensitivity to \u2206 in some normal cases. It fails to resolve the issue when \u2206 has large variation across different features (pixels). For example, given a black image as baseline, \u2206 i for pixel in white color is close to 255, while \u2206 j for pixel in black color is almost 0. Hence the rescaled attributon a j for pixels in black color is larger than a i for pixels in white color due to the selected baseline. To avoid such cases, we assume \u2206 follows a Gaussian distribution with 0 mean and a constrained standard derivation \u03c3. That is, a i = ai \u2206i , where \u2206 i \u223c N (0, \u03c3 2 ), denoted by IG2. Third, to further guarantee robustness of the attribution, we select multiple baselines instead of a single one. IG3 is defined as\na i = 1 J j a j i \u2206 j i , where \u2206 j i \u223c N (0, \u03c3 2 )\nand J is the number of baselines. IG3, an average attribution across multiple baselines, reduces the possibility that an attribution is dominated by a special baseline.\n\n\nExperiments\n\nIn this section, we conduct experiments to evaluate the effectiveness of the proposed IG1, IG2, and IG3. The interpretations are evaluated on ILSVRC2014 dataset [24] under VGG19 architectures [25], compared with Gradient and Integrated Gradient (More comparisons to the state-of-the-art attribution methods could be found in Appendix B). In Integrated Gradient and IG1, the black image is selected as baseline. In IG2 and IG3, the standard deviation is set as 0.25 * 255. In IG3, 20 baselines are randomly generated from Gaussian distribution.\n\nThe performance is compared both qualitatively and quantitatively. First, the visualization comparisons via saliency maps are shown in Figure 2 for Gradient, Integrated Gradient, and three proposed methods. In general, the saliency map based on Gradient is visually noisy and includes some irrelevant regions to the prediction, while IG3 accurately localizes the objects of interest. The generated saliency maps by IG3, evenly distributed with less noises, are sharper, and clearly display the shapes and boarders of the objects. Specifically, the corgi object in the first image has a large image contrast   ratio (i.e., white legs, black body and mixed face). When interpreting the classification decision of the corgi, Integrated Gradient assigns significantly higher contributions to white areas than black areas, even though it identifies right location of the corgi. This is due to \u2206 i corresponding to white pixels is large by selecting a black image baseline, which demonstrates the limitation of Integrated Gradient. Similar observation could also be found when interpreting the cardoon image. The proposed IG1 and IG2 alleviate this phenomenon by mitigating the sensitivity to the baseline image. IG3 further enhances the robustness of the saliency map by averaging across several baselines. More visualization examples could be found in Appendix B.\n\nSecond, quantitative measurements are compared via localization performance, i.e., how well attribution methods locate objects of interest. A common approach is to compare the saliency maps from the attribution scores with bounding box annotations. Assume the bounding box contains n pixels. We select top n pixels according to ranked attribution scores and count the number of pixels m inside the bounding box. The ratio m n is used as the metric of localization accuracy [12]. \u03b1 represents the percentage of pixels in a bounding box. We consider two scenarios: bounding boxes covering less than 33% (\u03b1 \u2264 33%) and 66% (\u03b1 \u2264 66% ) of the input image respectively. We run the experiments over 4k images for different \u03b1 and the results of localization accuracy are shown in Table 2. Integrated Gradient has a superior localization performance than Gradient for both \u03b1. This observation is consistent with our theoretical investigation. The reformulation of Integrated Gradient involves complex feature interactions so that it describes the model behaviors more adequately than Gradient. The proposed improvements IG1, IG2 significantly outperform Integrated Gradient by over two percentages. IG3 performs the best due to improved attribution robustness by averaging attributions across multiple baselines.\n\n\nConclusion and Future Work\n\nIn this work, we aim to theoretically understand and assess several existing attribution methods in a unified framework, which builds a systematic metric to evaluate and compare the attribution methods designed based on heuristic or empirical intuition. It also provides a direction to improve the existing attribution methods and develop novel attribution methods. We utilize Taylor expansion function to approximate complex DNNs and propose a Taylor attribution framework to define faithfulness of interpretation via three desired properties. Several attribution methods are reformulated into the unified framework to investigate their rationales, fidelity, and limitations. New methods based on existing attribution methods are proposed to improve interpretation performance, which have been demonstrated by qualitative and quantitative experiments. In the future work, we will explore more about the effect of complex feature interactions and how it could be applied to provide better interpretations, as complex feature interaction is a core characteristic in DNNs.\n\n\nBroader Impact\n\nThis work aims to build a unified framework to theoretically understand and assess attribution methods, which will immediately lead to advances in pursuing interpretability of deep learning. It will have an immediate impact on the interpretable machine learning systems, including formalizing the model interpretability, providing a systematic definition and evaluation metric of the interpretation fidelity, revisiting and filtering existing attribution methods, and prompting better attribution methods.\n\nFurthermore, it will have strong impact on improving the usability of deep learning in important applications, such as autonomous driving, cybersecurity and AI healthcare. Also it will help improve the overall value of the deep learning based systems, prompt a more transparent, fair and accountable platform for emerging and future data/information management systems, and understand how the interpretations could be utilized to further advance machine learning applications. The outcome of this work will play an integral part in educating and training students with fundamental AI concepts and algorithms, which will be integrated as a education module targeting students in cybersecurity and data science in author's institute.\n\nHuman subjects are not involved in this work, and we have strictly followed institutional and national regulations to avoid violating any ethical issues in conducting this research.\n\n\nSimilar to Perturbation-1, Perturbation-patch [8] attributes features in a patch level. An image is splitted into m patches as x = [p 1 , . . . , p m ], where a patch p j is a subset of x. Using the same intuition in Perturbation-1, features in the patch p j are perturbed by a constant vector v. The altered input and the corresponding output are denoted as x = [p 1 , . . . , p j\u22121 , v, p j+1 , . . . , p m ] and f (x ). The attribution of feature x i in the patch p j is the difference between output f (x) and f (x ) as a i = f (x) \u2212 f (x ).\n\n.\nHere we mainly focus on investigating the fidelity of the layer-wise rule.Proposition 4 The relevance score a (l) ij of DeepLift is reformulated as (See Appendix A for proof), a (l)\n\nFigure 2 :\n2Visualization saliency map comparison.\n\nTable 1 :\n1An Example of Second-order Taylor Reformulations\n\nTable 2 :\n2Bounding box localization accuracy.\n\nTechniques for interpretable machine learning. M Du, N Liu, X Hu, Communications of the ACM. 631Du, M., N. Liu, X. Hu. Techniques for interpretable machine learning. Communications of the ACM, 63(1):68-77, 2019.\n\nOn pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. S Bach, A Binder, G Montavon, PloS one. 107Bach, S., A. Binder, G. Montavon, et al. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7), 2015.\n\nExplaining nonlinear classification decisions with deep taylor decomposition. G Montavon, S Lapuschkin, A Binder, Pattern Recognition. 65Montavon, G., S. Lapuschkin, A. Binder, et al. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognition, 65:211-222, 2017.\n\nLearning important features through propagating activation differences. A Shrikumar, P Greenside, A Kundaje, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Shrikumar, A., P. Greenside, A. Kundaje. Learning important features through propagat- ing activation differences. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3145-3153. JMLR. org, 2017.\n\nNot just a black box: Learning important features through propagating activation differences. A Shrikumar, P Greenside, A Shcherbina, arXiv:1605.01713arXiv preprintShrikumar, A., P. Greenside, A. Shcherbina, et al. Not just a black box: Learning important features through propagating activation differences. arXiv preprint arXiv:1605.01713, 2016.\n\nSmoothgrad: removing noise by adding noise. D Smilkov, N Thorat, B Kim, International Conference on Learning Representations Workshop. Smilkov, D., N. Thorat, B. Kim, et al. Smoothgrad: removing noise by adding noise. In International Conference on Learning Representations Workshop. 2017.\n\nAxiomatic attribution for deep networks. M Sundararajan, A Taly, Q Yan, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Sundararajan, M., A. Taly, Q. Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3319-3328. JMLR. org, 2017.\n\nVisualizing and understanding convolutional networks. M D Zeiler, R Fergus, European conference on computer vision. SpringerZeiler, M. D., R. Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818-833. Springer, 2014.\n\nA unified approach to interpreting model predictions. S M Lundberg, S.-I Lee, Advances in neural information processing systems. Lundberg, S. M., S.-I. Lee. A unified approach to interpreting model predictions. In Advances in neural information processing systems, pages 4765-4774. 2017.\n\nTowards better understanding of gradientbased attribution methods for deep neural networks. E C Marco Ancona, M G Cengiz \u00d6ztireli, International Conference on Learning Representations. Marco Ancona, E. C., M. G. Cengiz \u00d6ztireli. Towards better understanding of gradient- based attribution methods for deep neural networks. In International Conference on Learning Representations. 2015.\n\nInterpretable explanations of black boxes by meaningful perturbation. R C Fong, A Vedaldi, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionFong, R. C., A. Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision, pages 3429-3437. 2017.\n\nRestricting the flow: Information bottlenecks for attribution. Karl Schulz, L S , T L Federico Tombari, International Conference on Learning Representations. Karl Schulz, L. S., T. L. Federico Tombari. Restricting the flow: Information bottlenecks for attribution. In International Conference on Learning Representations. 2020.\n\nEvaluating the visualization of what a deep neural network has learned. W Samek, A Binder, G Montavon, IEEE transactions on neural networks and learning systems. 28Samek, W., A. Binder, G. Montavon, et al. Evaluating the visualization of what a deep neural network has learned. IEEE transactions on neural networks and learning systems, 28(11):2660- 2673, 2016.\n\nGrad-cam: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, M Cogswell, A Das, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionSelvaraju, R. R., M. Cogswell, A. Das, et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618-626. 2017.\n\nwhy should i trust you?\" explaining the predictions of any classifier. M T Ribeiro, S Singh, C Guestrin, Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on knowledge discovery and data miningRibeiro, M. T., S. Singh, C. Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135-1144. 2016.\n\nQuantitative models for performance measurement system. P Suwignjo, U S Bititci, A S Carrie, International journal of production economics. 641-3Suwignjo, P., U. S. Bititci, A. S. Carrie. Quantitative models for performance measurement system. International journal of production economics, 64(1-3):231-241, 2000.\n\nMethods for interpreting and understanding deep neural networks. G Montavon, W Samek, K.-R M\u00fcller, Digital Signal Processing. 73Montavon, G., W. Samek, K.-R. M\u00fcller. Methods for interpreting and understanding deep neural networks. Digital Signal Processing, 73:1-15, 2018.\n\nUnderstanding impacts of high-order loss approximations and features in deep learning interpretation. E W Sahil Singla, S F Shi Feng, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning70Sahil Singla, E. W., S. F. Shi Feng. Understanding impacts of high-order loss approximations and features in deep learning interpretation. In Proceedings of the 36th International Conference on Machine Learning-Volume 70. JMLR. org, 2019.\n\nDeliberative explanations: visualizing network insecurities. P Wang, N Nvasconcelos, Advances in Neural Information Processing Systems. Wang, P., N. Nvasconcelos. Deliberative explanations: visualizing network insecurities. In Advances in Neural Information Processing Systems, pages 1372-1383. 2019.\n\nVisualizing deep neural network decisions: Prediction difference analysis. L M Zintgraf, T S Cohen, T Adel, arXiv:1702.04595arXiv preprintZintgraf, L. M., T. S. Cohen, T. Adel, et al. Visualizing deep neural network decisions: Prediction difference analysis. arXiv preprint arXiv:1702.04595, 2017.\n\nStriving for simplicity: The all convolutional net. J T Springenberg, A Dosovitskiy, T Brox, arXiv:1412.6806arXiv preprintSpringenberg, J. T., A. Dosovitskiy, T. Brox, et al. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.\n\nA theoretical explanation for perplexing behaviors of backpropagation-based visualizations. W Nie, Y Zhang, A Patel, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine Learning70Nie, W., Y. Zhang, A. Patel. A theoretical explanation for perplexing behaviors of backpropagation-based visualizations. In Proceedings of the 35th International Conference on Machine Learning-Volume 70, pages 3809-3818. JMLR. org, 2018.\n\nSanity checks for saliency maps. J Adebayo, J Gilmer, M Muelly, Advances in Neural Information Processing Systems. Adebayo, J., J. Gilmer, M. Muelly, et al. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, pages 9505-9515. 2018.\n\nImageNet Large Scale Visual Recognition Challenge. O Russakovsky, J Deng, H Su, International Journal of Computer Vision (IJCV). 1153Russakovsky, O., J. Deng, H. Su, et al. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations. Simonyan, K., A. Zisserman. Very deep convolutional networks for large-scale image recogni- tion. In International Conference on Learning Representations. 2015.\n", "annotations": {"author": "[{\"end\":128,\"start\":65},{\"end\":227,\"start\":129},{\"end\":331,\"start\":228},{\"end\":395,\"start\":332},{\"end\":460,\"start\":396},{\"end\":556,\"start\":461}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":71},{\"end\":135,\"start\":132},{\"end\":238,\"start\":236},{\"end\":342,\"start\":338},{\"end\":407,\"start\":403},{\"end\":467,\"start\":465}]", "author_first_name": "[{\"end\":70,\"start\":65},{\"end\":131,\"start\":129},{\"end\":235,\"start\":228},{\"end\":337,\"start\":332},{\"end\":402,\"start\":396},{\"end\":464,\"start\":461}]", "author_affiliation": "[{\"end\":127,\"start\":77},{\"end\":226,\"start\":137},{\"end\":330,\"start\":259},{\"end\":394,\"start\":344},{\"end\":459,\"start\":409},{\"end\":555,\"start\":484}]", "title": "[{\"end\":62,\"start\":1},{\"end\":618,\"start\":557}]", "venue": null, "abstract": "[{\"end\":2279,\"start\":620}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2459,\"start\":2456},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2774,\"start\":2771},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2777,\"start\":2774},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2780,\"start\":2777},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2783,\"start\":2780},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2786,\"start\":2783},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2789,\"start\":2786},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2792,\"start\":2789},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3064,\"start\":3061},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3067,\"start\":3064},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3759,\"start\":3755},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3763,\"start\":3759},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3767,\"start\":3763},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3771,\"start\":3767},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3838,\"start\":3834},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4944,\"start\":4940},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11262,\"start\":11258},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13867,\"start\":13863},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13871,\"start\":13867},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13875,\"start\":13871},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14785,\"start\":14782},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14810,\"start\":14806},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14839,\"start\":14836},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14867,\"start\":14864},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14880,\"start\":14877},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14908,\"start\":14905},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14923,\"start\":14920},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14942,\"start\":14938},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15033,\"start\":15029},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15184,\"start\":15180},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15319,\"start\":15315},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15687,\"start\":15684},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16236,\"start\":16232},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18000,\"start\":17997},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20119,\"start\":20116},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22717,\"start\":22713},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24511,\"start\":24507},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24542,\"start\":24538},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26729,\"start\":26725}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30643,\"start\":30096},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30828,\"start\":30644},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30880,\"start\":30829},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30941,\"start\":30881},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30989,\"start\":30942}]", "paragraph": "[{\"end\":3450,\"start\":2295},{\"end\":4372,\"start\":3452},{\"end\":5652,\"start\":4374},{\"end\":6332,\"start\":5654},{\"end\":6400,\"start\":6334},{\"end\":6575,\"start\":6402},{\"end\":6762,\"start\":6577},{\"end\":7035,\"start\":6764},{\"end\":8449,\"start\":7068},{\"end\":8582,\"start\":8451},{\"end\":8630,\"start\":8584},{\"end\":8723,\"start\":8675},{\"end\":9069,\"start\":8759},{\"end\":9403,\"start\":9071},{\"end\":9471,\"start\":9424},{\"end\":9653,\"start\":9556},{\"end\":9795,\"start\":9655},{\"end\":10164,\"start\":9797},{\"end\":10589,\"start\":10275},{\"end\":10811,\"start\":10751},{\"end\":11020,\"start\":10894},{\"end\":11431,\"start\":11200},{\"end\":11802,\"start\":11433},{\"end\":11914,\"start\":11909},{\"end\":12347,\"start\":12069},{\"end\":13128,\"start\":12468},{\"end\":13261,\"start\":13130},{\"end\":13505,\"start\":13263},{\"end\":13686,\"start\":13507},{\"end\":14536,\"start\":13688},{\"end\":15621,\"start\":14594},{\"end\":15835,\"start\":15623},{\"end\":16008,\"start\":15859},{\"end\":16196,\"start\":16024},{\"end\":16451,\"start\":16198},{\"end\":16625,\"start\":16480},{\"end\":16923,\"start\":16660},{\"end\":17192,\"start\":16949},{\"end\":17508,\"start\":17236},{\"end\":18121,\"start\":17594},{\"end\":18291,\"start\":18169},{\"end\":18381,\"start\":18329},{\"end\":18423,\"start\":18392},{\"end\":18677,\"start\":18658},{\"end\":18763,\"start\":18738},{\"end\":18969,\"start\":18831},{\"end\":19085,\"start\":18989},{\"end\":19140,\"start\":19124},{\"end\":19285,\"start\":19236},{\"end\":19474,\"start\":19347},{\"end\":19913,\"start\":19476},{\"end\":20239,\"start\":19915},{\"end\":20558,\"start\":20482},{\"end\":20959,\"start\":20560},{\"end\":21341,\"start\":20994},{\"end\":22230,\"start\":21404},{\"end\":22718,\"start\":22270},{\"end\":23282,\"start\":22720},{\"end\":24109,\"start\":23284},{\"end\":24330,\"start\":24162},{\"end\":24889,\"start\":24346},{\"end\":26250,\"start\":24891},{\"end\":27554,\"start\":26252},{\"end\":28655,\"start\":27585},{\"end\":29179,\"start\":28674},{\"end\":29912,\"start\":29181},{\"end\":30095,\"start\":29914}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8674,\"start\":8631},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8758,\"start\":8724},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9423,\"start\":9404},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9555,\"start\":9472},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10274,\"start\":10165},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10750,\"start\":10590},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10893,\"start\":10812},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11199,\"start\":11021},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11908,\"start\":11803},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12068,\"start\":11915},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12467,\"start\":12348},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15858,\"start\":15836},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16023,\"start\":16009},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16479,\"start\":16452},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16659,\"start\":16626},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16948,\"start\":16924},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17593,\"start\":17509},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18328,\"start\":18292},{\"attributes\":{\"id\":\"formula_19\"},\"end\":18657,\"start\":18424},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18737,\"start\":18678},{\"attributes\":{\"id\":\"formula_21\"},\"end\":18830,\"start\":18764},{\"attributes\":{\"id\":\"formula_22\"},\"end\":18988,\"start\":18970},{\"attributes\":{\"id\":\"formula_23\"},\"end\":19123,\"start\":19086},{\"attributes\":{\"id\":\"formula_24\"},\"end\":19235,\"start\":19141},{\"attributes\":{\"id\":\"formula_25\"},\"end\":19346,\"start\":19286},{\"attributes\":{\"id\":\"formula_26\"},\"end\":20481,\"start\":20310},{\"attributes\":{\"id\":\"formula_27\"},\"end\":20993,\"start\":20960},{\"attributes\":{\"id\":\"formula_28\"},\"end\":21403,\"start\":21342},{\"attributes\":{\"id\":\"formula_29\"},\"end\":24161,\"start\":24110}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22077,\"start\":22070},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27030,\"start\":27023}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2293,\"start\":2281},{\"attributes\":{\"n\":\"2\"},\"end\":7066,\"start\":7038},{\"attributes\":{\"n\":\"3\"},\"end\":14592,\"start\":14539},{\"end\":17218,\"start\":17195},{\"end\":17234,\"start\":17221},{\"end\":18167,\"start\":18124},{\"end\":18390,\"start\":18384},{\"end\":20309,\"start\":20242},{\"attributes\":{\"n\":\"4\"},\"end\":22268,\"start\":22233},{\"attributes\":{\"n\":\"5\"},\"end\":24344,\"start\":24333},{\"attributes\":{\"n\":\"6\"},\"end\":27583,\"start\":27557},{\"attributes\":{\"n\":\"7\"},\"end\":28672,\"start\":28658},{\"end\":30646,\"start\":30645},{\"end\":30840,\"start\":30830},{\"end\":30891,\"start\":30882},{\"end\":30952,\"start\":30943}]", "table": null, "figure_caption": "[{\"end\":30643,\"start\":30098},{\"end\":30828,\"start\":30647},{\"end\":30880,\"start\":30842},{\"end\":30941,\"start\":30893},{\"end\":30989,\"start\":30954}]", "figure_ref": "[{\"end\":7968,\"start\":7960},{\"end\":8581,\"start\":8573},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25034,\"start\":25026}]", "bib_author_first_name": "[{\"end\":31039,\"start\":31038},{\"end\":31045,\"start\":31044},{\"end\":31052,\"start\":31051},{\"end\":31305,\"start\":31304},{\"end\":31313,\"start\":31312},{\"end\":31323,\"start\":31322},{\"end\":31591,\"start\":31590},{\"end\":31603,\"start\":31602},{\"end\":31617,\"start\":31616},{\"end\":31887,\"start\":31886},{\"end\":31900,\"start\":31899},{\"end\":31913,\"start\":31912},{\"end\":32376,\"start\":32375},{\"end\":32389,\"start\":32388},{\"end\":32402,\"start\":32401},{\"end\":32675,\"start\":32674},{\"end\":32686,\"start\":32685},{\"end\":32696,\"start\":32695},{\"end\":32963,\"start\":32962},{\"end\":32979,\"start\":32978},{\"end\":32987,\"start\":32986},{\"end\":33367,\"start\":33366},{\"end\":33369,\"start\":33368},{\"end\":33379,\"start\":33378},{\"end\":33646,\"start\":33645},{\"end\":33648,\"start\":33647},{\"end\":33663,\"start\":33659},{\"end\":33973,\"start\":33972},{\"end\":33975,\"start\":33974},{\"end\":33991,\"start\":33990},{\"end\":33993,\"start\":33992},{\"end\":34338,\"start\":34337},{\"end\":34340,\"start\":34339},{\"end\":34348,\"start\":34347},{\"end\":34737,\"start\":34733},{\"end\":34747,\"start\":34746},{\"end\":34749,\"start\":34748},{\"end\":34753,\"start\":34752},{\"end\":34755,\"start\":34754},{\"end\":35072,\"start\":35071},{\"end\":35081,\"start\":35080},{\"end\":35091,\"start\":35090},{\"end\":35445,\"start\":35444},{\"end\":35447,\"start\":35446},{\"end\":35460,\"start\":35459},{\"end\":35472,\"start\":35471},{\"end\":35893,\"start\":35892},{\"end\":35895,\"start\":35894},{\"end\":35906,\"start\":35905},{\"end\":35915,\"start\":35914},{\"end\":36405,\"start\":36404},{\"end\":36417,\"start\":36416},{\"end\":36419,\"start\":36418},{\"end\":36430,\"start\":36429},{\"end\":36432,\"start\":36431},{\"end\":36729,\"start\":36728},{\"end\":36741,\"start\":36740},{\"end\":36753,\"start\":36749},{\"end\":37040,\"start\":37039},{\"end\":37042,\"start\":37041},{\"end\":37058,\"start\":37057},{\"end\":37060,\"start\":37059},{\"end\":37498,\"start\":37497},{\"end\":37506,\"start\":37505},{\"end\":37814,\"start\":37813},{\"end\":37816,\"start\":37815},{\"end\":37828,\"start\":37827},{\"end\":37830,\"start\":37829},{\"end\":37839,\"start\":37838},{\"end\":38090,\"start\":38089},{\"end\":38092,\"start\":38091},{\"end\":38108,\"start\":38107},{\"end\":38123,\"start\":38122},{\"end\":38396,\"start\":38395},{\"end\":38403,\"start\":38402},{\"end\":38412,\"start\":38411},{\"end\":38818,\"start\":38817},{\"end\":38829,\"start\":38828},{\"end\":38839,\"start\":38838},{\"end\":39104,\"start\":39103},{\"end\":39119,\"start\":39118},{\"end\":39127,\"start\":39126},{\"end\":39417,\"start\":39416},{\"end\":39429,\"start\":39428}]", "bib_author_last_name": "[{\"end\":31042,\"start\":31040},{\"end\":31049,\"start\":31046},{\"end\":31055,\"start\":31053},{\"end\":31310,\"start\":31306},{\"end\":31320,\"start\":31314},{\"end\":31332,\"start\":31324},{\"end\":31600,\"start\":31592},{\"end\":31614,\"start\":31604},{\"end\":31624,\"start\":31618},{\"end\":31897,\"start\":31888},{\"end\":31910,\"start\":31901},{\"end\":31921,\"start\":31914},{\"end\":32386,\"start\":32377},{\"end\":32399,\"start\":32390},{\"end\":32413,\"start\":32403},{\"end\":32683,\"start\":32676},{\"end\":32693,\"start\":32687},{\"end\":32700,\"start\":32697},{\"end\":32976,\"start\":32964},{\"end\":32984,\"start\":32980},{\"end\":32991,\"start\":32988},{\"end\":33376,\"start\":33370},{\"end\":33386,\"start\":33380},{\"end\":33657,\"start\":33649},{\"end\":33667,\"start\":33664},{\"end\":33988,\"start\":33976},{\"end\":34009,\"start\":33994},{\"end\":34345,\"start\":34341},{\"end\":34356,\"start\":34349},{\"end\":34744,\"start\":34738},{\"end\":34772,\"start\":34756},{\"end\":35078,\"start\":35073},{\"end\":35088,\"start\":35082},{\"end\":35100,\"start\":35092},{\"end\":35457,\"start\":35448},{\"end\":35469,\"start\":35461},{\"end\":35476,\"start\":35473},{\"end\":35903,\"start\":35896},{\"end\":35912,\"start\":35907},{\"end\":35924,\"start\":35916},{\"end\":36414,\"start\":36406},{\"end\":36427,\"start\":36420},{\"end\":36439,\"start\":36433},{\"end\":36738,\"start\":36730},{\"end\":36747,\"start\":36742},{\"end\":36760,\"start\":36754},{\"end\":37055,\"start\":37043},{\"end\":37069,\"start\":37061},{\"end\":37503,\"start\":37499},{\"end\":37519,\"start\":37507},{\"end\":37825,\"start\":37817},{\"end\":37836,\"start\":37831},{\"end\":37844,\"start\":37840},{\"end\":38105,\"start\":38093},{\"end\":38120,\"start\":38109},{\"end\":38128,\"start\":38124},{\"end\":38400,\"start\":38397},{\"end\":38409,\"start\":38404},{\"end\":38418,\"start\":38413},{\"end\":38826,\"start\":38819},{\"end\":38836,\"start\":38830},{\"end\":38846,\"start\":38840},{\"end\":39116,\"start\":39105},{\"end\":39124,\"start\":39120},{\"end\":39130,\"start\":39128},{\"end\":39426,\"start\":39418},{\"end\":39439,\"start\":39430}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":51893222},\"end\":31202,\"start\":30991},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9327892},\"end\":31510,\"start\":31204},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5731985},\"end\":31812,\"start\":31512},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3385018},\"end\":32279,\"start\":31814},{\"attributes\":{\"doi\":\"arXiv:1605.01713\",\"id\":\"b4\"},\"end\":32628,\"start\":32281},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11695878},\"end\":32919,\"start\":32630},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":16747630},\"end\":33310,\"start\":32921},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3960646},\"end\":33589,\"start\":33312},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":21889700},\"end\":33878,\"start\":33591},{\"attributes\":{\"id\":\"b9\"},\"end\":34265,\"start\":33880},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1633753},\"end\":34668,\"start\":34267},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":209532057},\"end\":34997,\"start\":34670},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7689122},\"end\":35360,\"start\":34999},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15019293},\"end\":35819,\"start\":35362},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13029170},\"end\":36346,\"start\":35821},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":17925621},\"end\":36661,\"start\":36348},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207170725},\"end\":36935,\"start\":36663},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":59553259},\"end\":37434,\"start\":36937},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":202776418},\"end\":37736,\"start\":37436},{\"attributes\":{\"doi\":\"arXiv:1702.04595\",\"id\":\"b19\"},\"end\":38035,\"start\":37738},{\"attributes\":{\"doi\":\"arXiv:1412.6806\",\"id\":\"b20\"},\"end\":38301,\"start\":38037},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":29164433},\"end\":38782,\"start\":38303},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52938797},\"end\":39050,\"start\":38784},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2930547},\"end\":39346,\"start\":39052},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14124313},\"end\":39655,\"start\":39348}]", "bib_title": "[{\"end\":31036,\"start\":30991},{\"end\":31302,\"start\":31204},{\"end\":31588,\"start\":31512},{\"end\":31884,\"start\":31814},{\"end\":32672,\"start\":32630},{\"end\":32960,\"start\":32921},{\"end\":33364,\"start\":33312},{\"end\":33643,\"start\":33591},{\"end\":33970,\"start\":33880},{\"end\":34335,\"start\":34267},{\"end\":34731,\"start\":34670},{\"end\":35069,\"start\":34999},{\"end\":35442,\"start\":35362},{\"end\":35890,\"start\":35821},{\"end\":36402,\"start\":36348},{\"end\":36726,\"start\":36663},{\"end\":37037,\"start\":36937},{\"end\":37495,\"start\":37436},{\"end\":38393,\"start\":38303},{\"end\":38815,\"start\":38784},{\"end\":39101,\"start\":39052},{\"end\":39414,\"start\":39348}]", "bib_author": "[{\"end\":31044,\"start\":31038},{\"end\":31051,\"start\":31044},{\"end\":31057,\"start\":31051},{\"end\":31312,\"start\":31304},{\"end\":31322,\"start\":31312},{\"end\":31334,\"start\":31322},{\"end\":31602,\"start\":31590},{\"end\":31616,\"start\":31602},{\"end\":31626,\"start\":31616},{\"end\":31899,\"start\":31886},{\"end\":31912,\"start\":31899},{\"end\":31923,\"start\":31912},{\"end\":32388,\"start\":32375},{\"end\":32401,\"start\":32388},{\"end\":32415,\"start\":32401},{\"end\":32685,\"start\":32674},{\"end\":32695,\"start\":32685},{\"end\":32702,\"start\":32695},{\"end\":32978,\"start\":32962},{\"end\":32986,\"start\":32978},{\"end\":32993,\"start\":32986},{\"end\":33378,\"start\":33366},{\"end\":33388,\"start\":33378},{\"end\":33659,\"start\":33645},{\"end\":33669,\"start\":33659},{\"end\":33990,\"start\":33972},{\"end\":34011,\"start\":33990},{\"end\":34347,\"start\":34337},{\"end\":34358,\"start\":34347},{\"end\":34746,\"start\":34733},{\"end\":34752,\"start\":34746},{\"end\":34774,\"start\":34752},{\"end\":35080,\"start\":35071},{\"end\":35090,\"start\":35080},{\"end\":35102,\"start\":35090},{\"end\":35459,\"start\":35444},{\"end\":35471,\"start\":35459},{\"end\":35478,\"start\":35471},{\"end\":35905,\"start\":35892},{\"end\":35914,\"start\":35905},{\"end\":35926,\"start\":35914},{\"end\":36416,\"start\":36404},{\"end\":36429,\"start\":36416},{\"end\":36441,\"start\":36429},{\"end\":36740,\"start\":36728},{\"end\":36749,\"start\":36740},{\"end\":36762,\"start\":36749},{\"end\":37057,\"start\":37039},{\"end\":37071,\"start\":37057},{\"end\":37505,\"start\":37497},{\"end\":37521,\"start\":37505},{\"end\":37827,\"start\":37813},{\"end\":37838,\"start\":37827},{\"end\":37846,\"start\":37838},{\"end\":38107,\"start\":38089},{\"end\":38122,\"start\":38107},{\"end\":38130,\"start\":38122},{\"end\":38402,\"start\":38395},{\"end\":38411,\"start\":38402},{\"end\":38420,\"start\":38411},{\"end\":38828,\"start\":38817},{\"end\":38838,\"start\":38828},{\"end\":38848,\"start\":38838},{\"end\":39118,\"start\":39103},{\"end\":39126,\"start\":39118},{\"end\":39132,\"start\":39126},{\"end\":39428,\"start\":39416},{\"end\":39441,\"start\":39428}]", "bib_venue": "[{\"end\":32046,\"start\":31993},{\"end\":33116,\"start\":33063},{\"end\":34479,\"start\":34427},{\"end\":35599,\"start\":35547},{\"end\":36109,\"start\":36026},{\"end\":37194,\"start\":37141},{\"end\":38543,\"start\":38490},{\"end\":31082,\"start\":31057},{\"end\":31342,\"start\":31334},{\"end\":31645,\"start\":31626},{\"end\":31991,\"start\":31923},{\"end\":32373,\"start\":32281},{\"end\":32763,\"start\":32702},{\"end\":33061,\"start\":32993},{\"end\":33426,\"start\":33388},{\"end\":33718,\"start\":33669},{\"end\":34063,\"start\":34011},{\"end\":34425,\"start\":34358},{\"end\":34826,\"start\":34774},{\"end\":35159,\"start\":35102},{\"end\":35545,\"start\":35478},{\"end\":36024,\"start\":35926},{\"end\":36486,\"start\":36441},{\"end\":36787,\"start\":36762},{\"end\":37139,\"start\":37071},{\"end\":37570,\"start\":37521},{\"end\":37811,\"start\":37738},{\"end\":38087,\"start\":38037},{\"end\":38488,\"start\":38420},{\"end\":38897,\"start\":38848},{\"end\":39179,\"start\":39132},{\"end\":39493,\"start\":39441}]"}}}, "year": 2023, "month": 12, "day": 17}