{"id": 232417418, "updated": "2023-10-06 05:13:53.427", "metadata": {"title": "Data-Uncertainty Guided Multi-Phase Learning for Semi-Supervised Object Detection", "authors": "[{\"first\":\"Zhenyu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yali\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Ye\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Lu\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Shengjin\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 3, "day": 29}, "abstract": "In this paper, we delve into semi-supervised object detection where unlabeled images are leveraged to break through the upper bound of fully-supervised object detection models. Previous semi-supervised methods based on pseudo labels are severely degenerated by noise and prone to overfit to noisy labels, thus are deficient in learning different unlabeled knowledge well. To address this issue, we propose a data-uncertainty guided multi-phase learning method for semi-supervised object detection. We comprehensively consider divergent types of unlabeled images according to their difficulty levels, utilize them in different phases and ensemble models from different phases together to generate ultimate results. Image uncertainty guided easy data selection and region uncertainty guided RoI Re-weighting are involved in multi-phase learning and enable the detector to concentrate on more certain knowledge. Through extensive experiments on PASCAL VOC and MS COCO, we demonstrate that our method behaves extraordinarily compared to baseline approaches and outperforms them by a large margin, more than 3% on VOC and 2% on COCO.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.16368", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Wang0GFW21", "doi": "10.1109/cvpr46437.2021.00454"}}, "content": {"source": {"pdf_hash": "76c5df8cef7c758c656d4afb08ab0b5e50eb51c1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.16368v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2103.16368", "status": "GREEN"}}, "grobid": {"id": "6ad212bd61e8427e8ea5a85ff5eea43e0bca2ba9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/76c5df8cef7c758c656d4afb08ab0b5e50eb51c1.txt", "contents": "\nData-Uncertainty Guided Multi-Phase Learning for Semi-Supervised Object Detection\n\n\nZhenyu Wang \nDepartment of Electronic Engineering\nTsinghua University\n\n\nYali Li \nDepartment of Electronic Engineering\nTsinghua University\n\n\nYe Guo \nDepartment of Electronic Engineering\nTsinghua University\n\n\nLu Fang \nDepartment of Electronic Engineering\nTsinghua University\n\n\nShengjin Wang \nDepartment of Electronic Engineering\nTsinghua University\n\n\nData-Uncertainty Guided Multi-Phase Learning for Semi-Supervised Object Detection\n\nIn this paper, we delve into semi-supervised object detection where unlabeled images are leveraged to break through the upper bound of fully-supervised object detection. Previous semi-supervised methods based on pseudo labels are severely degenerated by noise and prone to overfit to noisy labels, thus are deficient in learning different unlabeled knowledge well. To address this issue, we propose a datauncertainty guided multi-phase learning method for semisupervised object detection. We comprehensively consider divergent types of unlabeled images according to their difficulty levels, utilize them in different phases, and ensemble models from different phases together to generate ultimate results. Image uncertainty guided easy data selection and region uncertainty guided RoI Re-weighting are involved in multi-phase learning and enable the detector to concentrate on more certain knowledge. Through extensive experiments on PASCAL VOC and MS COCO, we demonstrate that our method behaves extraordinarily compared to baseline approaches and outperforms them by a large margin, more than 3% on VOC and 2% on COCO.\n\nIntroduction\n\nWith the success of Convolutional Neural Networks (CNNs) [19,22], object detection methods have been promoted rapidly in recent years. Plenty of object detection models [12,11,36,34,27] achieve superior performance on benchmark datasets [10,26]. However, these models are heavily dependent on a large amount of fully-supervised data with complete category and bounding box annotations, which are labor-intensive to collect [20].\n\nTo address the preceding problems, semi-supervised object detection (SSOD) [37] receives much attention recently. It exploits a large amount of unlabeled data to boost the performance of fully-supervised object detection, especially when only limited labeled data are available. Currently, many SSOD methods [33] are established on pseudo labels * Corresponding author (a) (b) Figure 1: Frameworks for previous pseudo-label based one-phase training (a) and our multi-phase method (b). Image-level uncertainty based selection and region-level uncertainty based re-weighting guide our multi-phase learning for handling noise in pseudo labels. [23] and adopt the one-phase learning scheme in Fig. 1a. With a pre-trained fully-supervised model on labeled images, pseudo annotations of unlabeled images are obtained. These pseudo labels are treated as groundtruth of unlabeled data and are integrated with annotations of labeled data to train the SSOD model.\n\nDespite that the one-phase learning is somewhat effective for SSOD, it is insufficient for knowledge excavation to exploit the unlabeled data only once. The reason lies in the fact that noise is inherently attached to pseudo annotations. Deep learning based models have the potential to fit any training annotation, even the incorrect ones. When pseudo annotations are noisy with some false information, detection models are also able to \"learn\" to fit them. This fitting ability to incorrect annotations surpasses the representative learning for correct ones. We call this phenomenon as label noise overfitting problem, which is also corroborated in previous studies [1,50,28]. As a result, SSOD models with one-phase learning tend to fit the difficult data with more noise, while neglecting the easy data with high confidence.\n\nThe negative impacts of label noise overfitting are mainly two-fold. On the one hand, at image level, difficult images with much noise preponderate during training, making the detector inflexible to employ unlabeled data with different difficulty levels. On the other hand, at region level, regions overlapped heavily with existing objects but lacking pseudo annotations incorporate much more noisy information and dominate the training, contributing extensive incorrect gradient messages to SSOD training.\n\nTo tackle this, we describe the noisy labeled data with uncertainty and propose a data-uncertainty guided multiphase learning for SSOD. At image level, we introduce the uncertainty to guide the image selection for different phases of training. In practice, we perform SSOD training on easy unlabeled images with low uncertainty first, then continue with difficult unlabeled images with high uncertainty. During this process, we collect more than one model which experts in images with different difficulty levels separately. They cooperatively specialize all unlabeled images so we aggregate them together to complement each other for inference. At region level, we measure the uncertainty of background regions based on their similarity and overlaps with each other. We further conduct RoI re-weighting by the guidance of region uncertainty degrees and involve it in the multi-phase training. This RoI re-weighting strategy reduces weights for uncertain regions and forces detection models to pay more attention to certain regions.\n\nOur main contributions can be summarized as follows:\n\n\u2022 We propose an uncertainty guided multi-phase SSOD learning method. With image-uncertainty based selection, we alleviate attention imbalance on different difficulty levels of data and are capable of fitting all unlabeled images well.\n\n\u2022 We introduce a region-uncertainty based RoI reweighting strategy to guide multi-phase learning and assist the detector in focusing on more certain regions.\n\n\u2022 On the PASCAL VOC and MS COCO dataset, our method reaches 78.6% and 42.3%, which exceeds the state-of-the-art by 2.4%, 2.2%, respectively.\n\n\nRelated Work\n\nObject detection is one of the most important tasks in computer vision. It aims to detect objects from an image, predict correct classification categories, and assign accurate bounding boxes. It is generally divided into twostage detection methods and one-stage methods. Two-stage detectors [12,11,36,8,13] usually produce region pro-posals then perform classification and regression on these proposals, while one-stage detectors [34,27,35] generate bounding boxes prediction and region classification directly. Fully-supervised object detection (FSOD) develops rapidly recently [25,38,5,51] and achieves outstanding results in benchmark datasets [10,26]. However, FSOD requires instance-level annotated datasets that are expensive to obtain. Weakly-supervised object detection (WSOD) [4,41,9,52,43] is thus studied as it only needs image-level annotations. However, WSOD fails to achieve a satisfying result compared to fully-supervised object detection methods, which stimulates the demand for SSOD.\n\nSemi-supervised learning trains models with both labeled and unlabeled data. Many works for semi-supervised learning appears in recent years, such as consistency regularization based methods [21,29,42,30,24], self-training [49,6,32], label propagation [53,2], data augmentation [47,3], or entropy regularization [18]. Although semisupervised learning develops rapidly, it usually targets at classification or semantic segmentation problems rather than detection. Object detection is in nature much more difficult so off-the-shell semi-supervised methods can hardly be directly applied to object detection.\n\nSemi-supervised object detection (SSOD) aims to train a detector with both instance-level annotated data and unlabeled data. For example, [15] utilizes unlabeled data to expand the number of categories that the detector recognizes, and [54] studies the effect of self-training and pre-training. Different from them, we mainly utilize unlabeled images to boost the performance of FSOD. Current SSOD methods are generally divided into two groups. The first is based on pseudo labels [45,44,33]. They usually perform SSOD training only once and are susceptiable to the label noise overfitting problem. The second is to borrow ideas from semi-supervised classification methods and use consistency regularization [16,17]. But they are more suitable for onestage detectors and behave poorly in two-stage models. Our approach is established on pseudo labels.\n\n\nMethod\n\n\nLabel Noise Overfitting Problem\n\nFor pseudo label based SSOD, pseudo labels are regarded as groundtruth annotations of unlabeled images for training the detector. Label noise inherently exists in these pseudo labels and brings about the uncertainty for SSOD training. We observe that deep learning detection models are susceptible to overfit to noisy labels, which damages the SSOD training. For further verification, we implement selftraining SSOD twice with VOC 2007 trainval as labeled set and VOC 2012 trainval as unlabeled set. The performance such as mAP versus training phases is plotted in Fig. 2a.\n\nFrom the empirical analysis we can see that the mAP on VOC07 test improves after the first SSOD phase because of the utility of extra unlabeled data, but it declines on VOC12 trainval. As a consequence, the quality of generated pseudo labels descends, causing worse test mAP in the second phase. This demonstrates that during SSOD training, the generalization ability of the model on testing data is strengthened while its ability on training data is weakened. This phenomenon derives from noisy pseudo labels. During SSOD training, noisy pseudo labels deliver uncertain supervised signals to the detectors, which attracts excessive attention. SSOD models attempt to follow this incorrect information, thus fitting to noisy labels and amplifying the noise. We refer to this as the label noise overfitting problem.\n\nAt image level, for an unlabeled image with pseudo labels, it consists of some correct knowledge that can improve the performance and some incorrect noise that hurts the training. If the correct knowledge is more, this image will benefit the model. We call this kind of image an easy one and vice versa. We propose that the recall of pseudo labels reflects the correct information it contains and 1precision is a metric of noise. With this metric, we evaluate previous models on easy or difficult images from VOC07 test separately. From Fig. 2b, we observe that although the SSOD model's prediction on the test set is more precise, it deteriorates on easy images. Higher mAP on test set mainly comes from difficult images, suggesting that the model over-focuses on difficult images with more noise and ignores easy images, a consequence of the label noise overfitting problem. Therefore, this model is deficient in utilizing unlabeled images adequately.\n\nAt region level, even for an easy image defined above where pseudo annotations are relatively clean, some objects within it may still lack pseudo annotations. In this circumstance, some positive regions are labeled as background category during training. The detector's classification module reveals that these regions are similar to some existing ob-Algorithm 1 The overall procedure for multi-phase SSOD learning.\n\n\nRequire:\n\nThe number of training phases, N Training:\n\nTrain a FSOD model with all labeled data. Set the initial easy data fraction: k = 1/N for i = 1; i <= N ; i + + do 1. Predict on unlabeled data with all current models. 2. Take the intersection for all current pseudo labels. 3. Select top k easy images from unlabeled images. 4. Train a SSOD model with labeled and easy unlabeled data. 5. k = k + 1/N end for Testing:\n\nEnsemble testing results from all models to generate ultimate results.\n\njects but they are not highly overlapped with any positive instances. This contradiction leads to the uncertainty of these regions. These noisy regions usually hold a large loss value thus are dominant over other regions during training, which negatively affects the performance.\n\n\nMulti-Phase Learning\n\nCurrent SSOD methods usually utilize unlabeled images once. Because of the label noise overfitting problem, difficult images with more noise are attached with higher importance and easy images are relatively discounted. One-phase learning generating a single SSOD model is hard to alleviate this problem. Because no matter how advanced the initial supervised detector is, pseudo labels of difficult images always accommodate more noise and dominate during the training. We thus leverage more than one model for easy or difficult data separately. Specifically, we choose easy unlabeled images to conduct SSOD first. In this training process, most of the data are relatively easy so the model will fit high-confident easy data well. Difficult images are added to train other subsequent detection models, and those models progressively concentrate on difficult images. Finally, we obtain a series of models that excel in different difficulty levels of images, and all of them together can fit all unlabeled data. During inference, we consider all models to fully exploit different abilities of divergent models. We use weighted boxes fusion [39] to ensemble detection results from all models. In this way, different types of information are utilized comprehensively.\n\nFor beginning models, they are trained with easy images thus are less affected by the label noise overfitting problem. With stronger generalization ability and milder overfitting to noisy labels, their predictions on the unlabeled training set are more convincing. Their generated pseudo labels can be provided for later training without performance reduction appearing in previous self-training methods. The above training ultimately forms a multi-phase procedure.\n\nFor a particular training phase, it is more appropriate to synthesize all models from previous phases for creating new pseudo labels, rather than just depend on the latest model. We consider the intersection of pseudo labels from all previous models. Intersection operation improves the precision and further reduces the uncertainty of pseudo labels. After the intersection, all models reach a consensus on each pseudo annotation, resulting in higher self-confidence and certainty. The overall process is in Algorithm 1.\n\n\nUncertainty Guided Training\n\n\nImage Uncertainty Guided Selection\n\nTo proceed with our method, we need to select easy images from the unlabeled dataset. Since annotations of unlabeled images are inaccessible, the above recall/precision metric for discriminating whether an image is easy is unavailable. We need an alternative metric that should guarantee that the detection model is more certain about easy images than difficult ones. Now that we have detected objects (i.e. pseudo labels)\n{(bb mn , s mn )} M m=1 of Image I m ,\nwhere bb mn denotes the bounding box and s mn is the corresponding confidence score, s mn is a reasonable and simple representation of how certain the model is about the specific object. The average of all bounding boxes' scores inside an image measures the certainty degree of all annotations. This corresponds with the image's difficulty, just as follows:\ns m = M m=1 s mn /M(1)\nThe above formulation provides a description of image uncertainty. It selects images that detectors are more certain about and enables that detection models are certain about each annotation in pseudo labels. Images with a small s m are regarded as difficult ones and are filtered out in the first several phases. This selection strategy based on image uncertainty guides the detector on more certain images to mitigate the label noise overfitting problem.\n\n\nRegion Uncertainty Guided RoI Re-weighting\n\nWith the above training framework, we exclude difficult images in the initial phases and integrate pseudo labels from different models by intersection. SSOD is hence able to avoid uncertain images with more noise, especially in the initial phases. But the detector is still distracted by noisy regions with missing annotation problems. To alleviate this, we introduce our uncertainty based re-weighting strategy. Details are shown in Fig. 4. Concretely, we illustrate region-level uncertainty and re-weight for RoIs with their uncertainty guided during training. The strategy discovers uncertain regions and reduces their gradients by down-weighting to facilitate more accurate and certain regions standing out.\n\nAccording to [46], background RoIs which are hardly overlapped with all positive instances are more likely to be miss-annotated. Based on it, we adopt Intersection over Union (IoU) as one of the metrics for uncertainty measurement. Overlap based weights are computed as follows:\nw i = a + (1 \u2212 a)e \u2212be \u2212c\u00b7IoU i(2)\nwhere a, b, c are pre-defined parameters, IoU i is the maximum IoU between the negative RoI i and all positive RoIs. w i is the reduced weight for RoI i . If a region is attributed with high similarity with some current objects but is not pseudo-labeled, we can claim that it is uncertain. Besides of IoU, the similarity between dif- ferent RoIs should be a necessary description of uncertainty. We apply the cosine distance to calculate the similarity between different RoIs. Let f i denote the feature of RoI i . The distance (i.e. dissimilarity) d ij between RoI i and RoI j is:\nd ij = |f T i f j | f i f j (3)\nWe notice that there are many small RoIs inside a large RoI after RPN. These small RoIs are usually highly similar to the large one, but they are not uncertain because their categories should be background, not other positive ones. Intersection over foreground (IoF) can quantify this phenomenon. We combine both cosine distance and IoF for ultimate similarity based uncertainty description:\nD i = 1 \u2212 max j d ij (1 \u2212 IoF ij )(4)\nThe ultimate uncertainty based weights are as follows:\nw i = (a + (1 \u2212 a)e \u2212be \u2212c 1 \u00b7IoU i )e \u2212be \u2212c 2 D i(5)\nwhere a, b, c 1 , c 2 are pre-defined parameters. Equation 5 is composed of two items, stemming from overlap based uncertainty and similarity based uncertainty separately. The product offers a balance between two different uncertainty. This formulation is based on the Gompertz function, a special form of the logistic function, where b is a large value. If the region is uncertain (IoU i and D i are small), its weight is close to 0, and its uncertain gradient is diminished to prevent from back-propagating through the network. As the uncertainty degree decreases, the weight increases rapidly to 1 for normal training.\n\nBut noisy knowledge still impairs the above process since raw RoI features are supervised with noisy labels in SSOD models. To further avoid noisy missing annotation labels, we embed the feature for similarity with a fully-connected layer. The purpose is to decrease distances among the same objects and increase distances among different objects. Considering that a bounding box is highly similar to boxes inside it, we adopt IoF as a supervised signal for similarity learning:\ny ij = I(IoF ij > t) or I(IoF ji > t)(6)\nwhere I(\u00b7) is the indicator function, t is a pre-defined thresh. We set t to 0.7 in all experiments. Ultimately, the loss function for learning similarity is defined as\nL sim = y ij (1 \u2212 d ij ) 2 + (1 \u2212 y ij )d 2 ij(7)\n\nExperiments\n\nWe evaluate the proposed method for SSOD on PASCAL VOC [10] and MSCOCO [26]. For VOC, we use VOC 2007 trainval (5,011 images) as the labeled data and VOC 2012 trainval (11,540 images) as the unlabeled data, then evaluate on VOC 2007 test (4,952 images). For COCO, we refer to a 35k subset of COCO 2014 validation set as co-35, the 80k training set as co-80 and the union of them as co-115. 120k unlabeled images from COCO 2017 is called as co-120. We use two settings for semi-supervised training: 1) co-35 as labeled set, co-80 as unlabeled set; 2) co-115 as labeled set, co-120 as unlabeled set, then report the model performance on COCO 2014 minival set (5,000 images). All experiments are implemented with PyTorch [31] and MMDetection [7]. Pseudo labels are obtained from the model's predictions post-processed by a per-category threshold as in [33]. Unless otherwise specified, we use ResNet50 [14] based Faster RCNN [36] for two-phase training. All other experimental settings are the same as MMDetection.\n\n\nComparison with Existing Methods\n\nPASCAL VOC. We perform the comparative study for SSOD based on the two-stage detector Faster RCNN and one-stage detector SSD [27] on VOC dataset. The results are presented in Tab. 1. For Faster RCNN, we note that our method significantly outperforms the previous onephase SSOD baseline model, which improves the mAP of the fully-supervised model (FS) on VOC07 from 74.8% to 75.6%. For comparison, our data uncertainty-based multiphase learning achieves the mAP of 78.6% and increases the mAP by 3% compared to the baseline. Compared to DD [33] which produces more accurate pseudo labels, the mAP is increased by 2.6% even with relatively lowquality pseudo labels. The experimental results show that our method is quite efficacious for SSOD. The less than 3% gap between our method (78.6%) and the upper bound obtained by fully-supervised learning on VOC0712 (81.2%) demonstrates the strong ability of our method to learn the knowledge within unlabeled data.  For SSD [27], our uncertainty-based multi-phase learning achieves the mAP of 74.5%, 2.7% higher than the baseline of one-phase learning, Compared to consistency-based semi-supervised learning (CSD) [16] and interpolationbased semi-supervised learning (ISD) [17], the detection mAP is improved by 2.2% and 1.2% respectively. However, these two methods behave poorly when combined with twostage detectors. In contrast, our method works consistently well for both two-stage and one-stage detectors.\n\nMS COCO. We conduct the comparative experiments with Faster RCNN as the base detector on COCO dataset. The metrics to measure detection accuracy such as AP , AP 50 , AP 75 are presented in Tab. 2. Note that our reproduced DD on co-35/80 performs a little better than the original paper. For co-35/80 split, the proposed multi-phase learning based on ResNet50 backbone achieves the AP of 34.8% for SSOD, outperforming DD by 1.7%. Moreover, we combine our method with DD, achieving the AP of 35.2%. Compared to DD, the achieved AP is increased by 2.1%, which is extremely prominent for COCO dataset, especially in semi-supervised settings. For co-115/120 split, our method also achieves consistently better than DD method and the state-of-the-art method on coco-115/120proposal learning (PL) [40]. Even without using ensemble to excavate knowledge of different difficulty levels, our final model still outperforms DD by 1% and PL by 0.5%. With ResNet101, a much more powerful feature extractor, the overall AP is further enhanced to 42.3%. The results validate the efficiency of our proposed multi-phase learning guided by data uncertainty and its strong ability to transcend the upper bound of traditional fully-supervised methods.\n\n\nAblation Study\n\nWe perform ablation study on PASCAL VOC to analyze the impacts of 1) multi-phase learning (two-phase concretely), 2) RoI Re-weighting strategy that enables the model to focus on certain regions, 3) model ensembling during inference. The results are in Tab. 3.\n\nFor Faster RCNN, two-phase training is 0.5% higher than one-phase training. This improvement reveals that our method alleviates the label noise overfitting problem that causes performance deterioration. RoI re-weighting further produces a 1.5% mAP gain, which confirms the ability of re-weighting to force the detector into more certain regions and reduce the missing annotation noise effect. Results ensembling finally boosts the mAP with 1.2% improvement. According to previous studies [48], model ensembling performs poorly when ensembled models are similar to each other. The significant promotion brought by ensembling in our method thereby indicates that models from different phases master in images with different difficulty levels.\n\nFor SSD, since RoI re-weighting cannot be applied to one-stage detectors, the mAP gain is a little lower. It is also noticeable that ensembling contributes more to the SSD detector, resulting in 2.2% mAP improvement. This is because one-stage detectors are usually uncompetitive in performance compared to two-stage detectors and could hardly excavate sufficient information. Models possess more random elements thus benefit more from ensembling.\n\n\u2022 RoI Re-weighting Analysis. We further study the effect of RoI re-weighting. We compare our method with baseline  and overlap based soft sampling [46]. The results are listed in Tab. 4. For a raw Faster RCNN, mAP increases from 74.8% to 75.9% in the first phase but almost remains unchanged in the second phase. This is because Faster RCNN is easily misguided by noisy labels. For the first phase, unlabeled images are easy and pseudo labels are relatively clean. In such a situation, Faster RCNN can obtain a satisfying result. But in the second phase, difficult noisy unlabeled images participate in training so the performance is seriously affected. Soft sampling mitigates the missing annotation problem to some extent. But it regards region-level uncertainty metric just as a function of overlaps, which is insufficient. Take images in Fig. 5 for example. For the first one, the horse in the right is not labeled and for the second one, the tiny plane in the middle is missed. The common is that almost all background RoIs share little overlaps with positive instances and are down-weighted to the same value with just overlap based metric. For our method based on both similarity and overlaps metric, as shown in the right column in Fig. 5, uncertain regions with missing annotation problems are successfully detected (blue regions) and their weights are further reduced. As a result, uncertain gradient information is depressed and our model is able to focus on more certain regions. Then the final model is more robust, especially in the second phase.\n\n\u2022 Model Divergence Analysis. We evaluate the models from multiple phases on easy or difficult images from the test set like section 3.1. From Tab. 5, we observe that the model from the first phase performs best for easy images, which indicates that the model trained with just easy unlabeled data fixates on certain knowledge. The model from the second phase has the best generalization ability since it learns with the largest amount of data. But it performs worse on easy data due to the label noise overfitting problem that makes the model attach great importance to difficult data and overlook easy data. This experiment verifies that models from different phases are experienced on data of different difficulty levels. Since these two models target at easy and difficult images separately, they are complementary to each other and ensembling them brings about a large improvement. We also notice that these two models are already able to fit all certain information thus adding the FS model does not boost performance on easy images. For difficult uncertain features, the FS model is still able to complement a little and enhance the final mAP.\n\n\nDiscussion\n\nIn previous experiments, we conduct two-phase SSOD training. In this section, we evaluate our method with more overall experiments to discuss the effect of learning process.\n\n\nTwo-Phase Learning\n\nAccording to Algorithm 1, our semi-supervised learning increases the number of unlabeled images evenly in different phases. In this section, we perform two-phase SSOD with a variant amount of easy images for the first phase, then continue with all unlabeled images for the second phase. The results are plotted in Fig. 6. A positive correlation between the rate of easy images and the first phase mAP is observed when easy images are not too many, because more available unlabeled data contribute to stronger generalization ability. The performance attains the highest mAP when easy images are about 50%, almost 76.8% mAP. Then, the performance declines steadily when the per- Figure 6: Two-phase SSOD with different amount of easy data, mAP reaches the peak when the ratio is 50%.\n\ncentage of easy images continues to increase, because noise within pseudo labels increases and label noise overfitting problem becomes severer as more difficult unlabeled data are accessible. The ability of the first phase model directly influences the quality of pseudo labels for the second phase. Ultimately, the mAP is the highest when easy images are about a half of total unlabeled images. Above results suggest that in SSOD settings, the volume of unlabeled data for training needs to be considered carefully. More data do not necessarily lead to better performance for SSOD.\n\nWhy results are the best when easy images are about 50%? We evaluate pseudo labels from VOC and COCO with recall/precision metric presented in section 3.1 and find that the fraction of easy images is close to 50% in both VOC and COCO. Even after data distillation [33], easy data still occupy approximately 50%. Easy images are those which contain more correct information than noise and contribute positively to SSOD training, so we can expect the best performance if all easy data participate in the first phase. For SSOD settings, the labeled dataset and unlabeled dataset share the same distribution and their quantity proportion is moderate. The labeled dataset is a little smaller, but is enough to train a nice supervised detection model. Images are also naturally collected. As a result, for an unlabeled image that the model has not ever seen before, the possibility that it is easy should be close to 50%. We thus assert that without any prior knowledge about the dataset, 50% is a good estimation of easy image proportion and an appropriate value for implementation.\n\n\nMore Phases\n\nWe extend our method to more phases. Unlabeled data are evenly divided for different phases since datasets are naturally distributed. The results are in Fig. 7. For VOC dataset, the performance improves slightly from two-phase 78.6% to three-phase 78.9%, while four-phase learning does not produce better mAP. We believe two-phase learning that generates two semi-supervised models is already able to describe most unlabeled information adequatelyone for easy certain information and the other for difficult uncertain information. Since information has been fully encoded, we do not need more models, i.e., more phases. For three-phase learning, the one more model may make up a little missing information. But when the phases continue to raise, existing models are already sufficient for all information and extra models cannot offer more information. For COCO dataset, although images are more complicated, two-phase is also sufficient and more phases do not introduce significant improvement. Therefore, we assert that two-phase learning is a good choice in practice.\n\n\nConclusion\n\nIn this paper, we propose a novel data uncertainty guided multi-phase learning method for semi-supervised object detection. The multi-phase training method enables the model to fully utilize all information and uncertainty descriptions guide the training process to make the detector concentrate on certain knowledge. We demonstrate the extraordinary ability of our method to excavate unlabeled knowledge and achieve state-of-the-art performance. Semi-supervised object detection is a challenging problem and we will further explore how to utilize unlabeled data more efficiently.\n\nFigure 2 :\n2Illustration of the label noise overfitting problem for detection models. The data sequence is: supervised model -semi-supervised model from the first phasesemi-supervised model from the second phase.\n\nFigure 3 :\n3The diagram of uncertainty guided multi-phase learning. Multi-phase self-training is designed for unlabeled images flow in SSOD. Image uncertainty estimation and region uncertainty estimation guide the multi-phase SSOD learning.\n\nFigure 4 :\n4The pipeline of uncertainty based re-weighting. Similarity, IoUs and IoFs among different objects are calculated for weights, and IoFs are treated as groundtruth labels for similarity learning.\n\nFigure 5 :\n5Illustrative examples for RoI Re-weighting. The left column is pseudo labels with missing annotations, and the right column is the heatmap for region uncertainty after RR. Blue regions are more uncertain for the detector. Before RR, all regions have the same weight of 1.0, while after RR, uncertain regions are assigned with lower weights.\n\nFigure 7 :\n7Multiple phases semi-supervised learning on VOC07 test and COCO minival.\n\nTable 1 :\n1Semi-supervised Detection Results on PASCAL VOC 2007 test vs. current SSOD methods and fullysupervised results trained on VOC07 or VOC0712. (L: labeled data, Un: unlabeled data.)Model \nBackbone \nMethod \nL \nUn \nmAP \n\nFaster \nRCNN \nResNet50 \n\nFS \nVOC07 \n-\n74.8 \nBaseline \nVOC07 \nVOC12 75.6 \nDD [33] \nVOC07 \nVOC12 76.0 \nours \nVOC07 \nVOC12 78.6 \nFS \nVOC0712 \n-\n81.2 \n\nSSD300 \nVGG16 \n\nFS \nVOC07 \n-\n70.2 \nBaseline \nVOC07 \nVOC12 71.8 \nCSD [16] \nVOC07 \nVOC12 72.3 \nISD [17] \nVOC07 \nVOC12 73.3 \nours \nVOC07 \nVOC12 74.5 \nFS \nVOC0712 \n-\n77.2 \n\n\n\nTable 2 :\n2Semi-supervised detection Results on COCO minival vs. current SSOD and FSOD results. \u2020 denotes that the performance is obtained by the final model after the multi-phase learning without ensemble.Backbone \nMethod \nL \nUn \nAP AP 50 AP 75 \n\nResNet50 \n\nFS \nco-35 \n-\n31.3 52.0 \n33.0 \nDD \nco-35 \nco-80 33.1 53.3 \n35.4 \nours \nco-35 \nco-80 34.8 55.1 \n37.2 \nours + DD \nco-35 \nco-80 35.2 55.7 \n37.6 \nFS \nco-115 \n-\n37.4 58.1 \n40.4 \nDD \nco-115 co-120 37.9 60.1 \n40.8 \nPL [40] \nco-115 co-120 38.4 59.7 \n41.7 \nours \nco-115 co-120 40.1 60.4 \n43.7 \nours  \u2020 + DD co-115 co-120 38.9 59.4 \n42.3 \nours + DD co-115 co-120 40.3 61.0 \n43.9 \n\nResNet101 \n\nFS \nco-115 \n-\n39.4 60.1 \n43.1 \nDD \nco-115 co-120 40.1 62.1 \n43.5 \nours \nco-115 co-120 42.2 62.5 \n46.1 \nours  \u2020 + DD co-115 co-120 41.2 61.5 \n44.9 \nours + DD co-115 co-120 42.3 62.7 \n46.3 \n\n\n\nTable 3 :\n3Ablation Study on PASCAL VOC 2007 test. (RR: RoI Re-weighting)Model \nL \nUn \nTwo-\nPhase \nRR Ensemble mAP \n\nFaster \nRCNN \n\nVOC07 \n-\n74.8 \nVOC07 VOC12 \n75.6 \nVOC07 VOC12 \n76.1 \nVOC07 VOC12 \n77.4 \nVOC07 VOC12 \n78.6 \n\nSSD300 \n\nVOC07 \n-\n70.2 \nVOC07 VOC12 \n71.8 \nVOC07 VOC12 \n72.3 \nVOC07 VOC12 \n74.5 \n\n\n\nTable 4 :\n4Effect of RoI Re-weighting on SSOD compared with Baseline and Soft Sampling. 0 \u223c 2 is the ensemble result of model from phase 0 (FS model) to phase 2.Phase \nBaseline Soft Sampling RoI Re-weighting \n0 (FS Model) \n74.8 \n74.8 \n74.8 \n1 \n75.9 \n76.2 \n76.6 \n2 \n76.1 \n76.6 \n77.4 \n0 \u223c 2 \n77.8 \n78.1 \n78.6 \n\n\n\nTable 5 :\n5Detection results from different models.0 \u223c 2 \n\n\nA closer look at memorization in deep networks. Devansh Arpit, K Stanislaw, Nicolas Jastrzebski, David Ballas, Emmanuel Krueger, Bengio, S Maxinder, Tegan Kanwal, Asja Maharaj, Fischer, C Aaron, Yoshua Courville, Bengio, ICML. Devansh Arpit, Stanislaw K Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron C Courville, Yoshua Bengio, et al. A closer look at memorization in deep net- works. In ICML, 2017. 2\n\nlabel propagation and quadratic criterion. Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux, 11Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. la- bel propagation and quadratic criterion.(2006). 11. 2\n\nMixmatch: A holistic approach to semi-supervised learning. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin A Raffel, Advances in Neural Information Processing Systems. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Ad- vances in Neural Information Processing Systems, pages 5049-5059, 2019. 2\n\nWeakly supervised deep detection networks. Hakan Bilen, Andrea Vedaldi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHakan Bilen and Andrea Vedaldi. Weakly supervised deep detection networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2846- 2854, 2016. 2\n\nCascade r-cnn: High quality object detection and instance segmentation. Zhaowei Cai, Nuno Vasconcelos, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High quality object detection and instance segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, page 1-1, 2019. 2\n\nSemi-supervised learning (chapelle. Olivier Chapelle, Bernhard Scholkopf, Alexander Zien, IEEE Transactions on Neural Networks. o. et al.203book reviewsOlivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Net- works, 20(3):542-542, 2009. 2\n\nKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, arXiv:1906.07155Open mmlab detection toolbox and benchmark. arXiv preprintKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection tool- box and benchmark. arXiv preprint arXiv:1906.07155, 2019. 5\n\nR-fcn: Object detection via region-based fully convolutional networks. Jifeng Dai, Yi Li, Kaiming He, Jian Sun, Advances in neural information processing systems. Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks. In Advances in neural information processing systems, pages 379-387, 2016. 2\n\nWeakly supervised cascaded convolutional networks. Ali Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash, Luc Van Gool, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionAli Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash, and Luc Van Gool. Weakly supervised cascaded convo- lutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 914-922, 2017. 2\n\nThe pascal visual object classes (voc) challenge. Mark Everingham, Luc Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, International journal of computer vision. 882Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303-338, 2010. 1, 2, 5\n\nFast r-cnn. Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision1Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter- national conference on computer vision, pages 1440-1448, 2015. 1, 2\n\nRich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition1Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 580-587, 2014. 1, 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017. 2\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 5\n\nLsda: Large scale detection through adaptation. Judy Hoffman, Sergio Guadarrama, Eric S Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, Kate Saenko, Advances in Neural Information Processing Systems. Judy Hoffman, Sergio Guadarrama, Eric S Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, and Kate Saenko. Lsda: Large scale detection through adaptation. In Advances in Neural Information Processing Systems, pages 3536-3544, 2014. 2\n\nConsistency-based semi-supervised learning for object detection. Jisoo Jeong, Seungeui Lee, Jeesoo Kim, Nojun Kwak, Advances in neural information processing systems. 26Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-based semi-supervised learning for object de- tection. In Advances in neural information processing sys- tems, pages 10759-10768, 2019. 2, 6\n\nInterpolation-based semisupervised learning for object detection. Jisoo Jeong, Vikas Verma, Minsung Hyun, Juho Kannala, Nojun Kwak, arXiv:2006.0215826arXiv preprintJisoo Jeong, Vikas Verma, Minsung Hyun, Juho Kan- nala, and Nojun Kwak. Interpolation-based semi- supervised learning for object detection. arXiv preprint arXiv:2006.02158, 2020. 2, 6\n\nUniversal semi-supervised semantic segmentation. Tarun Kalluri, Girish Varma, Manmohan Chandraker, C V Jawahar, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionTarun Kalluri, Girish Varma, Manmohan Chandraker, and CV Jawahar. Universal semi-supervised semantic segmenta- tion. In Proceedings of the IEEE International Conference on Computer Vision, pages 5259-5270, 2019. 2\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In Advances in neural information processing sys- tems, pages 1097-1105, 2012. 1\n\nThe open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, arXiv:1811.00982arXiv preprintAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui- jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, et al. The open im- ages dataset v4: Unified image classification, object detec- tion, and visual relationship detection at scale. arXiv preprint arXiv:1811.00982, 2018. 1\n\nTemporal ensembling for semisupervised learning. Samuli Laine, Timo Aila, arXiv:1610.02242arXiv preprintSamuli Laine and Timo Aila. Temporal ensembling for semi- supervised learning. arXiv preprint arXiv:1610.02242, 2016. 2\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. the IEEE86Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recog- nition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 1\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Dong-Hyun Lee, Workshop on challenges in representation learning, ICML. 3Dong-Hyun Lee. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, 2013. 1\n\nTransformation-consistent selfensembling model for semisupervised medical image segmentation. Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, Lei Xing, Pheng-Ann Heng, IEEE Transactions on Neural Networks and Learning Systems. 2Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, Lei Xing, and Pheng-Ann Heng. Transformation-consistent self- ensembling model for semisupervised medical image seg- mentation. IEEE Transactions on Neural Networks and Learning Systems, 2020. 2\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, 2017. 2\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. Springer15Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 1, 2, 5\n\nSsd: Single shot multibox detector. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C Berg, European conference on computer vision. Springer6Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European con- ference on computer vision, pages 21-37. Springer, 2016. 1, 2, 5, 6\n\nDimensionality-driven learning with noisy labels. Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah Erfani, Shutao Xia, Sudanthi Wijewickrema, James Bailey, International Conference on Machine Learning. Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah Erfani, Shutao Xia, Sudanthi Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In International Conference on Machine Learning, pages 3355-3364, 2018. 2\n\nVirtual adversarial training: a regularization method for supervised and semi-supervised learning. Takeru Miyato, Masanori Shin-Ichi Maeda, Shin Koyama, Ishii, IEEE transactions on pattern analysis and machine intelligence. 41Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979-1993, 2018. 2\n\nSemisupervised semantic segmentation with cross-consistency training. Yassine Ouali, C\u00e9line Hudelot, Myriam Tami, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYassine Ouali, C\u00e9line Hudelot, and Myriam Tami. Semi- supervised semantic segmentation with cross-consistency training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12674- 12684, 2020. 2\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in neural information processing systems, pages 8026-8037, 2019. 5\n\nDeep co-training for semi-supervised image segmentation. Jizong Peng, Guillermo Estrada, Marco Pedersoli, Christian Desrosiers, Pattern Recognition. 2107269Jizong Peng, Guillermo Estrada, Marco Pedersoli, and Chris- tian Desrosiers. Deep co-training for semi-supervised image segmentation. Pattern Recognition, page 107269, 2020. 2\n\nData distillation: Towards omnisupervised learning. Ilija Radosavovic, Piotr Doll\u00e1r, Ross Girshick, Georgia Gkioxari, Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition6Ilija Radosavovic, Piotr Doll\u00e1r, Ross Girshick, Georgia Gkioxari, and Kaiming He. Data distillation: Towards omni- supervised learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4119- 4128, 2018. 1, 2, 5, 6, 8\n\nYou only look once: Unified, real-time object detection. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition1Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object de- tection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779-788, 2016. 1, 2\n\nYolov3: An incremental improvement. Joseph Redmon, Ali Farhadi, Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018. 2\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in neural information processing systems. 15Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information pro- cessing systems, pages 91-99, 2015. 1, 2, 5\n\nSemi-supervised self-training of object detection models. Chuck Rosenberg, Henry Hebert, Schneiderman, Chuck Rosenberg, Martial Hebert, and Henry Schneider- man. Semi-supervised self-training of object detection mod- els. 2005. 1\n\nTraining region-based object detectors with online hard example mining. Abhinav Shrivastava, Abhinav Gupta, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionAbhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard ex- ample mining. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 761-769, 2016. 2\n\nWeighted boxes fusion: ensembling boxes for object detection models. Roman Solovyev, Weimin Wang, arXiv:1910.13302arXiv preprintRoman Solovyev and Weimin Wang. Weighted boxes fu- sion: ensembling boxes for object detection models. arXiv preprint arXiv:1910.13302, 2019. 3\n\nProposal learning for semi-supervised object detection. Peng Tang, Chetan Ramaiah, Ran Xu, Caiming Xiong, arXiv:2001.05086arXiv preprintPeng Tang, Chetan Ramaiah, Ran Xu, and Caiming Xiong. Proposal learning for semi-supervised object detection. arXiv preprint arXiv:2001.05086, 2020. 6\n\nMultiple instance detection network with online instance classifier refinement. Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPeng Tang, Xinggang Wang, Xiang Bai, and Wenyu Liu. Multiple instance detection network with online instance classifier refinement. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2843- 2851, 2017. 2\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Antti Tarvainen, Harri Valpola, Advances in neural information processing systems. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pages 1195-1204, 2017. 2\n\nMin-entropy latent model for weakly supervised object detection. Fang Wan, Pengxu Wei, Jianbin Jiao, Zhenjun Han, Qixiang Ye, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionFang Wan, Pengxu Wei, Jianbin Jiao, Zhenjun Han, and Qix- iang Ye. Min-entropy latent model for weakly supervised object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1297- 1306, 2018. 2\n\nCost-effective object detection: Active sample mining with switchable selection criteria. Keze Wang, Liang Lin, Xiaopeng Yan, Ziliang Chen, Dongyu Zhang, Lei Zhang, IEEE transactions on neural networks and learning systems. 30Keze Wang, Liang Lin, Xiaopeng Yan, Ziliang Chen, Dongyu Zhang, and Lei Zhang. Cost-effective object de- tection: Active sample mining with switchable selection cri- teria. IEEE transactions on neural networks and learning systems, 30(3):834-850, 2018. 2\n\nTowards human-machine cooperation: Selfsupervised sample mining for object detection. Keze Wang, Xiaopeng Yan, Dongyu Zhang, Lei Zhang, Liang Lin, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKeze Wang, Xiaopeng Yan, Dongyu Zhang, Lei Zhang, and Liang Lin. Towards human-machine cooperation: Self- supervised sample mining for object detection. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1605-1613, 2018. 2\n\nZhe Wu, Navaneeth Bodla, Bharat Singh, Mahyar Najibi, Rama Chellappa, Larry S Davis, arXiv:1806.06986Soft sampling for robust object detection. 47arXiv preprintZhe Wu, Navaneeth Bodla, Bharat Singh, Mahyar Najibi, Rama Chellappa, and Larry S Davis. Soft sampling for robust object detection. arXiv preprint arXiv:1806.06986, 2018. 4, 7\n\nUnsupervised data augmentation for consistency training. Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V Le, arXiv:1904.12848arXiv preprintQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for con- sistency training. arXiv preprint arXiv:1904.12848, 2019. 2\n\nMultimodel ensemble with rich spatial information for object detection. Jie Xu, Wei Wang, Hanyuan Wang, Jinhong Guo, Pattern Recognition. 996107098Jie Xu, Wei Wang, Hanyuan Wang, and Jinhong Guo. Multi- model ensemble with rich spatial information for object de- tection. Pattern Recognition, 99:107098, 2020. 6\n\nUnsupervised word sense disambiguation rivaling supervised methods. David Yarowsky, 33rd annual meeting of the association for computational linguistics. David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd annual meeting of the association for computational linguistics, pages 189-196, 1995. 2\n\nUnderstanding deep learning requires rethinking generalization. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, arXiv:1611.03530arXiv preprintChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learn- ing requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. 2\n\nDynamic R-CNN: Towards high quality object detection via dynamic training. Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, Xilin Chen, arXiv:2004.06002arXiv preprintHongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, and Xilin Chen. Dynamic R-CNN: Towards high qual- ity object detection via dynamic training. arXiv preprint arXiv:2004.06002, 2020. 2\n\nZigzag learning for weakly supervised object detection. Xiaopeng Zhang, Jiashi Feng, Hongkai Xiong, Qi Tian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXiaopeng Zhang, Jiashi Feng, Hongkai Xiong, and Qi Tian. Zigzag learning for weakly supervised object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4262-4270, 2018. 2\n\nLearning from labeled and unlabeled data with label propagation. Xiaojin Zhu, Zoubin Ghahramani, Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. 2002. 2\n\nRethinking pre-training and self-training. Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, D Ekin, Quoc V Cubuk, Le, arXiv:2006.06882arXiv preprintBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training and self-training. arXiv preprint arXiv:2006.06882, 2020. 2\n", "annotations": {"author": "[{\"end\":156,\"start\":85},{\"end\":224,\"start\":157},{\"end\":291,\"start\":225},{\"end\":359,\"start\":292},{\"end\":433,\"start\":360}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":92},{\"end\":164,\"start\":162},{\"end\":231,\"start\":228},{\"end\":299,\"start\":295},{\"end\":373,\"start\":369}]", "author_first_name": "[{\"end\":91,\"start\":85},{\"end\":161,\"start\":157},{\"end\":227,\"start\":225},{\"end\":294,\"start\":292},{\"end\":368,\"start\":360}]", "author_affiliation": "[{\"end\":155,\"start\":98},{\"end\":223,\"start\":166},{\"end\":290,\"start\":233},{\"end\":358,\"start\":301},{\"end\":432,\"start\":375}]", "title": "[{\"end\":82,\"start\":1},{\"end\":515,\"start\":434}]", "venue": null, "abstract": "[{\"end\":1637,\"start\":517}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1714,\"start\":1710},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1717,\"start\":1714},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1826,\"start\":1822},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1829,\"start\":1826},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1832,\"start\":1829},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1835,\"start\":1832},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1838,\"start\":1835},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1894,\"start\":1890},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1897,\"start\":1894},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2080,\"start\":2076},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2162,\"start\":2158},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2395,\"start\":2391},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2728,\"start\":2724},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3709,\"start\":3706},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3712,\"start\":3709},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3715,\"start\":3712},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6311,\"start\":6307},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6314,\"start\":6311},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6317,\"start\":6314},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6319,\"start\":6317},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6322,\"start\":6319},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6450,\"start\":6446},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6453,\"start\":6450},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6456,\"start\":6453},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6599,\"start\":6595},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6602,\"start\":6599},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6604,\"start\":6602},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6607,\"start\":6604},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6667,\"start\":6663},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6670,\"start\":6667},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6804,\"start\":6801},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6807,\"start\":6804},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6809,\"start\":6807},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6812,\"start\":6809},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6815,\"start\":6812},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7214,\"start\":7210},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7217,\"start\":7214},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7220,\"start\":7217},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7223,\"start\":7220},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7226,\"start\":7223},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7246,\"start\":7242},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7248,\"start\":7246},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7251,\"start\":7248},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7275,\"start\":7271},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7277,\"start\":7275},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7301,\"start\":7297},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7303,\"start\":7301},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7335,\"start\":7331},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7768,\"start\":7764},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7866,\"start\":7862},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8111,\"start\":8107},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8114,\"start\":8111},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8117,\"start\":8114},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8338,\"start\":8334},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8341,\"start\":8338},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11616,\"start\":11615},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13226,\"start\":13222},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":16481,\"start\":16477},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19367,\"start\":19363},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19383,\"start\":19379},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20030,\"start\":20026},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20050,\"start\":20047},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20161,\"start\":20157},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20211,\"start\":20207},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20234,\"start\":20230},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20485,\"start\":20481},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20899,\"start\":20895},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21327,\"start\":21323},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21517,\"start\":21513},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21576,\"start\":21572},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22606,\"start\":22602},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23814,\"start\":23810},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24663,\"start\":24659},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29069,\"start\":29065}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31773,\"start\":31560},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32015,\"start\":31774},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32222,\"start\":32016},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32576,\"start\":32223},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32662,\"start\":32577},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33208,\"start\":32663},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34040,\"start\":33209},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34348,\"start\":34041},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34659,\"start\":34349},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34719,\"start\":34660}]", "paragraph": "[{\"end\":2081,\"start\":1653},{\"end\":3036,\"start\":2083},{\"end\":3866,\"start\":3038},{\"end\":4374,\"start\":3868},{\"end\":5408,\"start\":4376},{\"end\":5462,\"start\":5410},{\"end\":5698,\"start\":5464},{\"end\":5857,\"start\":5700},{\"end\":5999,\"start\":5859},{\"end\":7017,\"start\":6016},{\"end\":7624,\"start\":7019},{\"end\":8477,\"start\":7626},{\"end\":9095,\"start\":8522},{\"end\":9910,\"start\":9097},{\"end\":10865,\"start\":9912},{\"end\":11282,\"start\":10867},{\"end\":11337,\"start\":11295},{\"end\":11706,\"start\":11339},{\"end\":11778,\"start\":11708},{\"end\":12059,\"start\":11780},{\"end\":13347,\"start\":12084},{\"end\":13814,\"start\":13349},{\"end\":14336,\"start\":13816},{\"end\":14827,\"start\":14405},{\"end\":15224,\"start\":14867},{\"end\":15704,\"start\":15248},{\"end\":16462,\"start\":15751},{\"end\":16742,\"start\":16464},{\"end\":17359,\"start\":16778},{\"end\":17783,\"start\":17392},{\"end\":17876,\"start\":17822},{\"end\":18553,\"start\":17932},{\"end\":19033,\"start\":18555},{\"end\":19243,\"start\":19075},{\"end\":20319,\"start\":19308},{\"end\":21810,\"start\":20356},{\"end\":23042,\"start\":21812},{\"end\":23320,\"start\":23061},{\"end\":24062,\"start\":23322},{\"end\":24510,\"start\":24064},{\"end\":26072,\"start\":24512},{\"end\":27223,\"start\":26074},{\"end\":27411,\"start\":27238},{\"end\":28215,\"start\":27434},{\"end\":28799,\"start\":28217},{\"end\":29878,\"start\":28801},{\"end\":30964,\"start\":29894},{\"end\":31559,\"start\":30979}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14866,\"start\":14828},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15247,\"start\":15225},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16777,\"start\":16743},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17391,\"start\":17360},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17821,\"start\":17784},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17931,\"start\":17877},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19074,\"start\":19034},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19293,\"start\":19244}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1651,\"start\":1639},{\"attributes\":{\"n\":\"2.\"},\"end\":6014,\"start\":6002},{\"attributes\":{\"n\":\"3.\"},\"end\":8486,\"start\":8480},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8520,\"start\":8489},{\"end\":11293,\"start\":11285},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12082,\"start\":12062},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14366,\"start\":14339},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":14403,\"start\":14369},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":15749,\"start\":15707},{\"attributes\":{\"n\":\"4.\"},\"end\":19306,\"start\":19295},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20354,\"start\":20322},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23059,\"start\":23045},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27236,\"start\":27226},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":27432,\"start\":27414},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":29892,\"start\":29881},{\"attributes\":{\"n\":\"5.\"},\"end\":30977,\"start\":30967},{\"end\":31571,\"start\":31561},{\"end\":31785,\"start\":31775},{\"end\":32027,\"start\":32017},{\"end\":32234,\"start\":32224},{\"end\":32588,\"start\":32578},{\"end\":32673,\"start\":32664},{\"end\":33219,\"start\":33210},{\"end\":34051,\"start\":34042},{\"end\":34359,\"start\":34350},{\"end\":34670,\"start\":34661}]", "table": "[{\"end\":33208,\"start\":32853},{\"end\":34040,\"start\":33416},{\"end\":34348,\"start\":34115},{\"end\":34659,\"start\":34511},{\"end\":34719,\"start\":34712}]", "figure_caption": "[{\"end\":31773,\"start\":31573},{\"end\":32015,\"start\":31787},{\"end\":32222,\"start\":32029},{\"end\":32576,\"start\":32236},{\"end\":32662,\"start\":32590},{\"end\":32853,\"start\":32675},{\"end\":33416,\"start\":33221},{\"end\":34115,\"start\":34053},{\"end\":34511,\"start\":34361},{\"end\":34712,\"start\":34672}]", "figure_ref": "[{\"end\":2468,\"start\":2460},{\"end\":2779,\"start\":2772},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9094,\"start\":9087},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10456,\"start\":10449},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16191,\"start\":16185},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25360,\"start\":25354},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25758,\"start\":25752},{\"end\":27754,\"start\":27748},{\"end\":28119,\"start\":28111},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30053,\"start\":30047}]", "bib_author_first_name": "[{\"end\":34776,\"start\":34769},{\"end\":34785,\"start\":34784},{\"end\":34804,\"start\":34797},{\"end\":34823,\"start\":34818},{\"end\":34840,\"start\":34832},{\"end\":34859,\"start\":34858},{\"end\":34875,\"start\":34870},{\"end\":34888,\"start\":34884},{\"end\":34908,\"start\":34907},{\"end\":34922,\"start\":34916},{\"end\":35242,\"start\":35236},{\"end\":35258,\"start\":35251},{\"end\":35277,\"start\":35270},{\"end\":35280,\"start\":35278},{\"end\":35467,\"start\":35462},{\"end\":35487,\"start\":35479},{\"end\":35500,\"start\":35497},{\"end\":35520,\"start\":35513},{\"end\":35537,\"start\":35531},{\"end\":35551,\"start\":35546},{\"end\":35553,\"start\":35552},{\"end\":35906,\"start\":35901},{\"end\":35920,\"start\":35914},{\"end\":36334,\"start\":36327},{\"end\":36344,\"start\":36340},{\"end\":36655,\"start\":36648},{\"end\":36674,\"start\":36666},{\"end\":36695,\"start\":36686},{\"end\":36963,\"start\":36960},{\"end\":36975,\"start\":36970},{\"end\":36991,\"start\":36982},{\"end\":37004,\"start\":36998},{\"end\":37012,\"start\":37010},{\"end\":37028,\"start\":37020},{\"end\":37040,\"start\":37033},{\"end\":37052,\"start\":37046},{\"end\":37064,\"start\":37059},{\"end\":37076,\"start\":37070},{\"end\":37461,\"start\":37455},{\"end\":37469,\"start\":37467},{\"end\":37481,\"start\":37474},{\"end\":37490,\"start\":37486},{\"end\":37795,\"start\":37792},{\"end\":37807,\"start\":37802},{\"end\":37819,\"start\":37816},{\"end\":37835,\"start\":37830},{\"end\":37851,\"start\":37848},{\"end\":38290,\"start\":38286},{\"end\":38306,\"start\":38303},{\"end\":38318,\"start\":38317},{\"end\":38320,\"start\":38319},{\"end\":38338,\"start\":38334},{\"end\":38355,\"start\":38349},{\"end\":38645,\"start\":38641},{\"end\":38994,\"start\":38990},{\"end\":39009,\"start\":39005},{\"end\":39025,\"start\":39019},{\"end\":39043,\"start\":39035},{\"end\":39503,\"start\":39496},{\"end\":39515,\"start\":39508},{\"end\":39875,\"start\":39868},{\"end\":39887,\"start\":39880},{\"end\":39903,\"start\":39895},{\"end\":39913,\"start\":39909},{\"end\":40321,\"start\":40317},{\"end\":40337,\"start\":40331},{\"end\":40354,\"start\":40350},{\"end\":40356,\"start\":40355},{\"end\":40372,\"start\":40364},{\"end\":40381,\"start\":40377},{\"end\":40395,\"start\":40391},{\"end\":40412,\"start\":40406},{\"end\":40426,\"start\":40422},{\"end\":40806,\"start\":40801},{\"end\":40822,\"start\":40814},{\"end\":40834,\"start\":40828},{\"end\":40845,\"start\":40840},{\"end\":41185,\"start\":41180},{\"end\":41198,\"start\":41193},{\"end\":41213,\"start\":41206},{\"end\":41224,\"start\":41220},{\"end\":41239,\"start\":41234},{\"end\":41517,\"start\":41512},{\"end\":41533,\"start\":41527},{\"end\":41549,\"start\":41541},{\"end\":41563,\"start\":41562},{\"end\":41565,\"start\":41564},{\"end\":41980,\"start\":41976},{\"end\":41997,\"start\":41993},{\"end\":42017,\"start\":42009},{\"end\":42019,\"start\":42018},{\"end\":42409,\"start\":42404},{\"end\":42428,\"start\":42422},{\"end\":42438,\"start\":42434},{\"end\":42454,\"start\":42448},{\"end\":42469,\"start\":42465},{\"end\":42483,\"start\":42478},{\"end\":42502,\"start\":42496},{\"end\":42517,\"start\":42511},{\"end\":42531,\"start\":42525},{\"end\":42544,\"start\":42541},{\"end\":42962,\"start\":42956},{\"end\":42974,\"start\":42970},{\"end\":43193,\"start\":43189},{\"end\":43205,\"start\":43201},{\"end\":43220,\"start\":43214},{\"end\":43236,\"start\":43229},{\"end\":43559,\"start\":43550},{\"end\":43922,\"start\":43914},{\"end\":43933,\"start\":43927},{\"end\":43941,\"start\":43938},{\"end\":43956,\"start\":43948},{\"end\":43964,\"start\":43961},{\"end\":43980,\"start\":43971},{\"end\":44367,\"start\":44359},{\"end\":44378,\"start\":44373},{\"end\":44390,\"start\":44386},{\"end\":44767,\"start\":44759},{\"end\":44780,\"start\":44773},{\"end\":44793,\"start\":44788},{\"end\":44809,\"start\":44804},{\"end\":44822,\"start\":44816},{\"end\":44835,\"start\":44831},{\"end\":44850,\"start\":44845},{\"end\":44869,\"start\":44859},{\"end\":45218,\"start\":45215},{\"end\":45232,\"start\":45224},{\"end\":45250,\"start\":45243},{\"end\":45267,\"start\":45258},{\"end\":45282,\"start\":45277},{\"end\":45299,\"start\":45289},{\"end\":45315,\"start\":45304},{\"end\":45661,\"start\":45654},{\"end\":45671,\"start\":45666},{\"end\":45685,\"start\":45678},{\"end\":45687,\"start\":45686},{\"end\":45699,\"start\":45695},{\"end\":45711,\"start\":45706},{\"end\":45726,\"start\":45720},{\"end\":45740,\"start\":45732},{\"end\":45760,\"start\":45755},{\"end\":46164,\"start\":46158},{\"end\":46181,\"start\":46173},{\"end\":46203,\"start\":46199},{\"end\":46616,\"start\":46609},{\"end\":46630,\"start\":46624},{\"end\":46646,\"start\":46640},{\"end\":47111,\"start\":47107},{\"end\":47123,\"start\":47120},{\"end\":47140,\"start\":47131},{\"end\":47152,\"start\":47148},{\"end\":47165,\"start\":47160},{\"end\":47183,\"start\":47176},{\"end\":47198,\"start\":47192},{\"end\":47214,\"start\":47208},{\"end\":47227,\"start\":47220},{\"end\":47244,\"start\":47240},{\"end\":47670,\"start\":47664},{\"end\":47686,\"start\":47677},{\"end\":47701,\"start\":47696},{\"end\":47722,\"start\":47713},{\"end\":47997,\"start\":47992},{\"end\":48016,\"start\":48011},{\"end\":48029,\"start\":48025},{\"end\":48047,\"start\":48040},{\"end\":48065,\"start\":48058},{\"end\":48532,\"start\":48526},{\"end\":48548,\"start\":48541},{\"end\":48562,\"start\":48558},{\"end\":48576,\"start\":48573},{\"end\":49002,\"start\":48996},{\"end\":49014,\"start\":49011},{\"end\":49187,\"start\":49180},{\"end\":49206,\"start\":49202},{\"end\":49215,\"start\":49211},{\"end\":49566,\"start\":49561},{\"end\":49583,\"start\":49578},{\"end\":49813,\"start\":49806},{\"end\":49834,\"start\":49827},{\"end\":49846,\"start\":49842},{\"end\":50307,\"start\":50302},{\"end\":50324,\"start\":50318},{\"end\":50566,\"start\":50562},{\"end\":50579,\"start\":50573},{\"end\":50592,\"start\":50589},{\"end\":50604,\"start\":50597},{\"end\":50878,\"start\":50874},{\"end\":50893,\"start\":50885},{\"end\":50905,\"start\":50900},{\"end\":50916,\"start\":50911},{\"end\":51430,\"start\":51425},{\"end\":51447,\"start\":51442},{\"end\":51813,\"start\":51809},{\"end\":51825,\"start\":51819},{\"end\":51838,\"start\":51831},{\"end\":51852,\"start\":51845},{\"end\":51865,\"start\":51858},{\"end\":52346,\"start\":52342},{\"end\":52358,\"start\":52353},{\"end\":52372,\"start\":52364},{\"end\":52385,\"start\":52378},{\"end\":52398,\"start\":52392},{\"end\":52409,\"start\":52406},{\"end\":52824,\"start\":52820},{\"end\":52839,\"start\":52831},{\"end\":52851,\"start\":52845},{\"end\":52862,\"start\":52859},{\"end\":52875,\"start\":52870},{\"end\":53288,\"start\":53285},{\"end\":53302,\"start\":53293},{\"end\":53316,\"start\":53310},{\"end\":53330,\"start\":53324},{\"end\":53343,\"start\":53339},{\"end\":53362,\"start\":53355},{\"end\":53684,\"start\":53679},{\"end\":53696,\"start\":53690},{\"end\":53708,\"start\":53702},{\"end\":53725,\"start\":53715},{\"end\":53739,\"start\":53733},{\"end\":54019,\"start\":54016},{\"end\":54027,\"start\":54024},{\"end\":54041,\"start\":54034},{\"end\":54055,\"start\":54048},{\"end\":54330,\"start\":54325},{\"end\":54663,\"start\":54656},{\"end\":54675,\"start\":54671},{\"end\":54690,\"start\":54684},{\"end\":54706,\"start\":54698},{\"end\":54719,\"start\":54714},{\"end\":55026,\"start\":55019},{\"end\":55038,\"start\":55034},{\"end\":55054,\"start\":55046},{\"end\":55065,\"start\":55059},{\"end\":55077,\"start\":55072},{\"end\":55366,\"start\":55358},{\"end\":55380,\"start\":55374},{\"end\":55394,\"start\":55387},{\"end\":55404,\"start\":55402},{\"end\":55845,\"start\":55838},{\"end\":55857,\"start\":55851},{\"end\":56028,\"start\":56022},{\"end\":56041,\"start\":56035},{\"end\":56058,\"start\":56050},{\"end\":56067,\"start\":56064},{\"end\":56080,\"start\":56073},{\"end\":56087,\"start\":56086},{\"end\":56100,\"start\":56094}]", "bib_author_last_name": "[{\"end\":34782,\"start\":34777},{\"end\":34795,\"start\":34786},{\"end\":34816,\"start\":34805},{\"end\":34830,\"start\":34824},{\"end\":34848,\"start\":34841},{\"end\":34856,\"start\":34850},{\"end\":34868,\"start\":34860},{\"end\":34882,\"start\":34876},{\"end\":34896,\"start\":34889},{\"end\":34905,\"start\":34898},{\"end\":34914,\"start\":34909},{\"end\":34932,\"start\":34923},{\"end\":34940,\"start\":34934},{\"end\":35249,\"start\":35243},{\"end\":35268,\"start\":35259},{\"end\":35285,\"start\":35281},{\"end\":35477,\"start\":35468},{\"end\":35495,\"start\":35488},{\"end\":35511,\"start\":35501},{\"end\":35529,\"start\":35521},{\"end\":35544,\"start\":35538},{\"end\":35560,\"start\":35554},{\"end\":35912,\"start\":35907},{\"end\":35928,\"start\":35921},{\"end\":36338,\"start\":36335},{\"end\":36356,\"start\":36345},{\"end\":36664,\"start\":36656},{\"end\":36684,\"start\":36675},{\"end\":36700,\"start\":36696},{\"end\":36968,\"start\":36964},{\"end\":36980,\"start\":36976},{\"end\":36996,\"start\":36992},{\"end\":37008,\"start\":37005},{\"end\":37018,\"start\":37013},{\"end\":37031,\"start\":37029},{\"end\":37044,\"start\":37041},{\"end\":37057,\"start\":37053},{\"end\":37068,\"start\":37065},{\"end\":37079,\"start\":37077},{\"end\":37465,\"start\":37462},{\"end\":37472,\"start\":37470},{\"end\":37484,\"start\":37482},{\"end\":37494,\"start\":37491},{\"end\":37800,\"start\":37796},{\"end\":37814,\"start\":37808},{\"end\":37828,\"start\":37820},{\"end\":37846,\"start\":37836},{\"end\":37860,\"start\":37852},{\"end\":38301,\"start\":38291},{\"end\":38315,\"start\":38307},{\"end\":38332,\"start\":38321},{\"end\":38347,\"start\":38339},{\"end\":38360,\"start\":38356},{\"end\":38371,\"start\":38362},{\"end\":38654,\"start\":38646},{\"end\":39003,\"start\":38995},{\"end\":39017,\"start\":39010},{\"end\":39033,\"start\":39026},{\"end\":39049,\"start\":39044},{\"end\":39506,\"start\":39504},{\"end\":39524,\"start\":39516},{\"end\":39878,\"start\":39876},{\"end\":39893,\"start\":39888},{\"end\":39907,\"start\":39904},{\"end\":39917,\"start\":39914},{\"end\":40329,\"start\":40322},{\"end\":40348,\"start\":40338},{\"end\":40362,\"start\":40357},{\"end\":40375,\"start\":40373},{\"end\":40389,\"start\":40382},{\"end\":40404,\"start\":40396},{\"end\":40420,\"start\":40413},{\"end\":40433,\"start\":40427},{\"end\":40812,\"start\":40807},{\"end\":40826,\"start\":40823},{\"end\":40838,\"start\":40835},{\"end\":40850,\"start\":40846},{\"end\":41191,\"start\":41186},{\"end\":41204,\"start\":41199},{\"end\":41218,\"start\":41214},{\"end\":41232,\"start\":41225},{\"end\":41244,\"start\":41240},{\"end\":41525,\"start\":41518},{\"end\":41539,\"start\":41534},{\"end\":41560,\"start\":41550},{\"end\":41573,\"start\":41566},{\"end\":41991,\"start\":41981},{\"end\":42007,\"start\":41998},{\"end\":42026,\"start\":42020},{\"end\":42420,\"start\":42410},{\"end\":42432,\"start\":42429},{\"end\":42446,\"start\":42439},{\"end\":42463,\"start\":42455},{\"end\":42476,\"start\":42470},{\"end\":42494,\"start\":42484},{\"end\":42509,\"start\":42503},{\"end\":42523,\"start\":42518},{\"end\":42539,\"start\":42532},{\"end\":42551,\"start\":42545},{\"end\":42968,\"start\":42963},{\"end\":42979,\"start\":42975},{\"end\":43199,\"start\":43194},{\"end\":43212,\"start\":43206},{\"end\":43227,\"start\":43221},{\"end\":43244,\"start\":43237},{\"end\":43563,\"start\":43560},{\"end\":43925,\"start\":43923},{\"end\":43936,\"start\":43934},{\"end\":43946,\"start\":43942},{\"end\":43959,\"start\":43957},{\"end\":43969,\"start\":43965},{\"end\":43985,\"start\":43981},{\"end\":44371,\"start\":44368},{\"end\":44384,\"start\":44379},{\"end\":44399,\"start\":44391},{\"end\":44771,\"start\":44768},{\"end\":44786,\"start\":44781},{\"end\":44802,\"start\":44794},{\"end\":44814,\"start\":44810},{\"end\":44829,\"start\":44823},{\"end\":44843,\"start\":44836},{\"end\":44857,\"start\":44851},{\"end\":44877,\"start\":44870},{\"end\":45222,\"start\":45219},{\"end\":45241,\"start\":45233},{\"end\":45256,\"start\":45251},{\"end\":45275,\"start\":45268},{\"end\":45287,\"start\":45283},{\"end\":45302,\"start\":45300},{\"end\":45320,\"start\":45316},{\"end\":45664,\"start\":45662},{\"end\":45676,\"start\":45672},{\"end\":45693,\"start\":45688},{\"end\":45704,\"start\":45700},{\"end\":45718,\"start\":45712},{\"end\":45730,\"start\":45727},{\"end\":45753,\"start\":45741},{\"end\":45767,\"start\":45761},{\"end\":46171,\"start\":46165},{\"end\":46197,\"start\":46182},{\"end\":46210,\"start\":46204},{\"end\":46217,\"start\":46212},{\"end\":46622,\"start\":46617},{\"end\":46638,\"start\":46631},{\"end\":46651,\"start\":46647},{\"end\":47118,\"start\":47112},{\"end\":47129,\"start\":47124},{\"end\":47146,\"start\":47141},{\"end\":47158,\"start\":47153},{\"end\":47174,\"start\":47166},{\"end\":47190,\"start\":47184},{\"end\":47206,\"start\":47199},{\"end\":47218,\"start\":47215},{\"end\":47238,\"start\":47228},{\"end\":47251,\"start\":47245},{\"end\":47675,\"start\":47671},{\"end\":47694,\"start\":47687},{\"end\":47711,\"start\":47702},{\"end\":47733,\"start\":47723},{\"end\":48009,\"start\":47998},{\"end\":48023,\"start\":48017},{\"end\":48038,\"start\":48030},{\"end\":48056,\"start\":48048},{\"end\":48068,\"start\":48066},{\"end\":48539,\"start\":48533},{\"end\":48556,\"start\":48549},{\"end\":48571,\"start\":48563},{\"end\":48584,\"start\":48577},{\"end\":49009,\"start\":49003},{\"end\":49022,\"start\":49015},{\"end\":49200,\"start\":49188},{\"end\":49209,\"start\":49207},{\"end\":49224,\"start\":49216},{\"end\":49229,\"start\":49226},{\"end\":49576,\"start\":49567},{\"end\":49590,\"start\":49584},{\"end\":49604,\"start\":49592},{\"end\":49825,\"start\":49814},{\"end\":49840,\"start\":49835},{\"end\":49855,\"start\":49847},{\"end\":50316,\"start\":50308},{\"end\":50329,\"start\":50325},{\"end\":50571,\"start\":50567},{\"end\":50587,\"start\":50580},{\"end\":50595,\"start\":50593},{\"end\":50610,\"start\":50605},{\"end\":50883,\"start\":50879},{\"end\":50898,\"start\":50894},{\"end\":50909,\"start\":50906},{\"end\":50920,\"start\":50917},{\"end\":51440,\"start\":51431},{\"end\":51455,\"start\":51448},{\"end\":51817,\"start\":51814},{\"end\":51829,\"start\":51826},{\"end\":51843,\"start\":51839},{\"end\":51856,\"start\":51853},{\"end\":51868,\"start\":51866},{\"end\":52351,\"start\":52347},{\"end\":52362,\"start\":52359},{\"end\":52376,\"start\":52373},{\"end\":52390,\"start\":52386},{\"end\":52404,\"start\":52399},{\"end\":52415,\"start\":52410},{\"end\":52829,\"start\":52825},{\"end\":52843,\"start\":52840},{\"end\":52857,\"start\":52852},{\"end\":52868,\"start\":52863},{\"end\":52879,\"start\":52876},{\"end\":53291,\"start\":53289},{\"end\":53308,\"start\":53303},{\"end\":53322,\"start\":53317},{\"end\":53337,\"start\":53331},{\"end\":53353,\"start\":53344},{\"end\":53368,\"start\":53363},{\"end\":53688,\"start\":53685},{\"end\":53700,\"start\":53697},{\"end\":53713,\"start\":53709},{\"end\":53731,\"start\":53726},{\"end\":53742,\"start\":53740},{\"end\":54022,\"start\":54020},{\"end\":54032,\"start\":54028},{\"end\":54046,\"start\":54042},{\"end\":54059,\"start\":54056},{\"end\":54339,\"start\":54331},{\"end\":54669,\"start\":54664},{\"end\":54682,\"start\":54676},{\"end\":54696,\"start\":54691},{\"end\":54712,\"start\":54707},{\"end\":54727,\"start\":54720},{\"end\":55032,\"start\":55027},{\"end\":55044,\"start\":55039},{\"end\":55057,\"start\":55055},{\"end\":55070,\"start\":55066},{\"end\":55082,\"start\":55078},{\"end\":55372,\"start\":55367},{\"end\":55385,\"start\":55381},{\"end\":55400,\"start\":55395},{\"end\":55409,\"start\":55405},{\"end\":55849,\"start\":55846},{\"end\":55868,\"start\":55858},{\"end\":56033,\"start\":56029},{\"end\":56048,\"start\":56042},{\"end\":56062,\"start\":56059},{\"end\":56071,\"start\":56068},{\"end\":56084,\"start\":56081},{\"end\":56092,\"start\":56088},{\"end\":56106,\"start\":56101},{\"end\":56110,\"start\":56108}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11455421},\"end\":35191,\"start\":34721},{\"attributes\":{\"id\":\"b1\"},\"end\":35401,\"start\":35193},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":146808485},\"end\":35856,\"start\":35403},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9173222},\"end\":36253,\"start\":35858},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":195345409},\"end\":36610,\"start\":36255},{\"attributes\":{\"id\":\"b5\"},\"end\":36958,\"start\":36612},{\"attributes\":{\"doi\":\"arXiv:1906.07155\",\"id\":\"b6\"},\"end\":37382,\"start\":36960},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7428689},\"end\":37739,\"start\":37384},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12221270},\"end\":38234,\"start\":37741},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4246903},\"end\":38627,\"start\":38236},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206770307},\"end\":38906,\"start\":38629},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":215827080},\"end\":39449,\"start\":38908},{\"attributes\":{\"id\":\"b12\"},\"end\":39820,\"start\":39451},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206594692},\"end\":40267,\"start\":39822},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14260314},\"end\":40734,\"start\":40269},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202782547},\"end\":41112,\"start\":40736},{\"attributes\":{\"doi\":\"arXiv:2006.02158\",\"id\":\"b16\"},\"end\":41461,\"start\":41114},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53774686},\"end\":41909,\"start\":41463},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":195908774},\"end\":42282,\"start\":41911},{\"attributes\":{\"doi\":\"arXiv:1811.00982\",\"id\":\"b19\"},\"end\":42905,\"start\":42284},{\"attributes\":{\"doi\":\"arXiv:1610.02242\",\"id\":\"b20\"},\"end\":43130,\"start\":42907},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14542261},\"end\":43451,\"start\":43132},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18507866},\"end\":43818,\"start\":43453},{\"attributes\":{\"id\":\"b23\"},\"end\":44288,\"start\":43820},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206771220},\"end\":44714,\"start\":44290},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14113767},\"end\":45177,\"start\":44716},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2141740},\"end\":45602,\"start\":45179},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":46980528},\"end\":46057,\"start\":45604},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":17504174},\"end\":46537,\"start\":46059},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":214605688},\"end\":47035,\"start\":46539},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202786778},\"end\":47605,\"start\":47037},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":85528481},\"end\":47938,\"start\":47607},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7350432},\"end\":48467,\"start\":47940},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206594738},\"end\":48958,\"start\":48469},{\"attributes\":{\"id\":\"b34\"},\"end\":49098,\"start\":48960},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10328909},\"end\":49501,\"start\":49100},{\"attributes\":{\"id\":\"b36\"},\"end\":49732,\"start\":49503},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2843566},\"end\":50231,\"start\":49734},{\"attributes\":{\"doi\":\"arXiv:1910.13302\",\"id\":\"b38\"},\"end\":50504,\"start\":50233},{\"attributes\":{\"doi\":\"arXiv:2001.05086\",\"id\":\"b39\"},\"end\":50792,\"start\":50506},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11035584},\"end\":51302,\"start\":50794},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2759724},\"end\":51742,\"start\":51304},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":52846693},\"end\":52250,\"start\":51744},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":49556437},\"end\":52732,\"start\":52252},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4351960},\"end\":53283,\"start\":52734},{\"attributes\":{\"doi\":\"arXiv:1806.06986\",\"id\":\"b45\"},\"end\":53620,\"start\":53285},{\"attributes\":{\"doi\":\"arXiv:1904.12848\",\"id\":\"b46\"},\"end\":53942,\"start\":53622},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":208609904},\"end\":54255,\"start\":53944},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1487550},\"end\":54590,\"start\":54257},{\"attributes\":{\"doi\":\"arXiv:1611.03530\",\"id\":\"b49\"},\"end\":54942,\"start\":54592},{\"attributes\":{\"doi\":\"arXiv:2004.06002\",\"id\":\"b50\"},\"end\":55300,\"start\":54944},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":13745084},\"end\":55771,\"start\":55302},{\"attributes\":{\"id\":\"b52\"},\"end\":55977,\"start\":55773},{\"attributes\":{\"doi\":\"arXiv:2006.06882\",\"id\":\"b53\"},\"end\":56318,\"start\":55979}]", "bib_title": "[{\"end\":34767,\"start\":34721},{\"end\":35460,\"start\":35403},{\"end\":35899,\"start\":35858},{\"end\":36325,\"start\":36255},{\"end\":36646,\"start\":36612},{\"end\":37453,\"start\":37384},{\"end\":37790,\"start\":37741},{\"end\":38284,\"start\":38236},{\"end\":38639,\"start\":38629},{\"end\":38988,\"start\":38908},{\"end\":39494,\"start\":39451},{\"end\":39866,\"start\":39822},{\"end\":40315,\"start\":40269},{\"end\":40799,\"start\":40736},{\"end\":41510,\"start\":41463},{\"end\":41974,\"start\":41911},{\"end\":43187,\"start\":43132},{\"end\":43548,\"start\":43453},{\"end\":43912,\"start\":43820},{\"end\":44357,\"start\":44290},{\"end\":44757,\"start\":44716},{\"end\":45213,\"start\":45179},{\"end\":45652,\"start\":45604},{\"end\":46156,\"start\":46059},{\"end\":46607,\"start\":46539},{\"end\":47105,\"start\":47037},{\"end\":47662,\"start\":47607},{\"end\":47990,\"start\":47940},{\"end\":48524,\"start\":48469},{\"end\":49178,\"start\":49100},{\"end\":49804,\"start\":49734},{\"end\":50872,\"start\":50794},{\"end\":51423,\"start\":51304},{\"end\":51807,\"start\":51744},{\"end\":52340,\"start\":52252},{\"end\":52818,\"start\":52734},{\"end\":54014,\"start\":53944},{\"end\":54323,\"start\":54257},{\"end\":55356,\"start\":55302}]", "bib_author": "[{\"end\":34784,\"start\":34769},{\"end\":34797,\"start\":34784},{\"end\":34818,\"start\":34797},{\"end\":34832,\"start\":34818},{\"end\":34850,\"start\":34832},{\"end\":34858,\"start\":34850},{\"end\":34870,\"start\":34858},{\"end\":34884,\"start\":34870},{\"end\":34898,\"start\":34884},{\"end\":34907,\"start\":34898},{\"end\":34916,\"start\":34907},{\"end\":34934,\"start\":34916},{\"end\":34942,\"start\":34934},{\"end\":35251,\"start\":35236},{\"end\":35270,\"start\":35251},{\"end\":35287,\"start\":35270},{\"end\":35479,\"start\":35462},{\"end\":35497,\"start\":35479},{\"end\":35513,\"start\":35497},{\"end\":35531,\"start\":35513},{\"end\":35546,\"start\":35531},{\"end\":35562,\"start\":35546},{\"end\":35914,\"start\":35901},{\"end\":35930,\"start\":35914},{\"end\":36340,\"start\":36327},{\"end\":36358,\"start\":36340},{\"end\":36666,\"start\":36648},{\"end\":36686,\"start\":36666},{\"end\":36702,\"start\":36686},{\"end\":36970,\"start\":36960},{\"end\":36982,\"start\":36970},{\"end\":36998,\"start\":36982},{\"end\":37010,\"start\":36998},{\"end\":37020,\"start\":37010},{\"end\":37033,\"start\":37020},{\"end\":37046,\"start\":37033},{\"end\":37059,\"start\":37046},{\"end\":37070,\"start\":37059},{\"end\":37081,\"start\":37070},{\"end\":37467,\"start\":37455},{\"end\":37474,\"start\":37467},{\"end\":37486,\"start\":37474},{\"end\":37496,\"start\":37486},{\"end\":37802,\"start\":37792},{\"end\":37816,\"start\":37802},{\"end\":37830,\"start\":37816},{\"end\":37848,\"start\":37830},{\"end\":37862,\"start\":37848},{\"end\":38303,\"start\":38286},{\"end\":38317,\"start\":38303},{\"end\":38334,\"start\":38317},{\"end\":38349,\"start\":38334},{\"end\":38362,\"start\":38349},{\"end\":38373,\"start\":38362},{\"end\":38656,\"start\":38641},{\"end\":39005,\"start\":38990},{\"end\":39019,\"start\":39005},{\"end\":39035,\"start\":39019},{\"end\":39051,\"start\":39035},{\"end\":39508,\"start\":39496},{\"end\":39526,\"start\":39508},{\"end\":39880,\"start\":39868},{\"end\":39895,\"start\":39880},{\"end\":39909,\"start\":39895},{\"end\":39919,\"start\":39909},{\"end\":40331,\"start\":40317},{\"end\":40350,\"start\":40331},{\"end\":40364,\"start\":40350},{\"end\":40377,\"start\":40364},{\"end\":40391,\"start\":40377},{\"end\":40406,\"start\":40391},{\"end\":40422,\"start\":40406},{\"end\":40435,\"start\":40422},{\"end\":40814,\"start\":40801},{\"end\":40828,\"start\":40814},{\"end\":40840,\"start\":40828},{\"end\":40852,\"start\":40840},{\"end\":41193,\"start\":41180},{\"end\":41206,\"start\":41193},{\"end\":41220,\"start\":41206},{\"end\":41234,\"start\":41220},{\"end\":41246,\"start\":41234},{\"end\":41527,\"start\":41512},{\"end\":41541,\"start\":41527},{\"end\":41562,\"start\":41541},{\"end\":41575,\"start\":41562},{\"end\":41993,\"start\":41976},{\"end\":42009,\"start\":41993},{\"end\":42028,\"start\":42009},{\"end\":42422,\"start\":42404},{\"end\":42434,\"start\":42422},{\"end\":42448,\"start\":42434},{\"end\":42465,\"start\":42448},{\"end\":42478,\"start\":42465},{\"end\":42496,\"start\":42478},{\"end\":42511,\"start\":42496},{\"end\":42525,\"start\":42511},{\"end\":42541,\"start\":42525},{\"end\":42553,\"start\":42541},{\"end\":42970,\"start\":42956},{\"end\":42981,\"start\":42970},{\"end\":43201,\"start\":43189},{\"end\":43214,\"start\":43201},{\"end\":43229,\"start\":43214},{\"end\":43246,\"start\":43229},{\"end\":43565,\"start\":43550},{\"end\":43927,\"start\":43914},{\"end\":43938,\"start\":43927},{\"end\":43948,\"start\":43938},{\"end\":43961,\"start\":43948},{\"end\":43971,\"start\":43961},{\"end\":43987,\"start\":43971},{\"end\":44373,\"start\":44359},{\"end\":44386,\"start\":44373},{\"end\":44401,\"start\":44386},{\"end\":44773,\"start\":44759},{\"end\":44788,\"start\":44773},{\"end\":44804,\"start\":44788},{\"end\":44816,\"start\":44804},{\"end\":44831,\"start\":44816},{\"end\":44845,\"start\":44831},{\"end\":44859,\"start\":44845},{\"end\":44879,\"start\":44859},{\"end\":45224,\"start\":45215},{\"end\":45243,\"start\":45224},{\"end\":45258,\"start\":45243},{\"end\":45277,\"start\":45258},{\"end\":45289,\"start\":45277},{\"end\":45304,\"start\":45289},{\"end\":45322,\"start\":45304},{\"end\":45666,\"start\":45654},{\"end\":45678,\"start\":45666},{\"end\":45695,\"start\":45678},{\"end\":45706,\"start\":45695},{\"end\":45720,\"start\":45706},{\"end\":45732,\"start\":45720},{\"end\":45755,\"start\":45732},{\"end\":45769,\"start\":45755},{\"end\":46173,\"start\":46158},{\"end\":46199,\"start\":46173},{\"end\":46212,\"start\":46199},{\"end\":46219,\"start\":46212},{\"end\":46624,\"start\":46609},{\"end\":46640,\"start\":46624},{\"end\":46653,\"start\":46640},{\"end\":47120,\"start\":47107},{\"end\":47131,\"start\":47120},{\"end\":47148,\"start\":47131},{\"end\":47160,\"start\":47148},{\"end\":47176,\"start\":47160},{\"end\":47192,\"start\":47176},{\"end\":47208,\"start\":47192},{\"end\":47220,\"start\":47208},{\"end\":47240,\"start\":47220},{\"end\":47253,\"start\":47240},{\"end\":47677,\"start\":47664},{\"end\":47696,\"start\":47677},{\"end\":47713,\"start\":47696},{\"end\":47735,\"start\":47713},{\"end\":48011,\"start\":47992},{\"end\":48025,\"start\":48011},{\"end\":48040,\"start\":48025},{\"end\":48058,\"start\":48040},{\"end\":48070,\"start\":48058},{\"end\":48541,\"start\":48526},{\"end\":48558,\"start\":48541},{\"end\":48573,\"start\":48558},{\"end\":48586,\"start\":48573},{\"end\":49011,\"start\":48996},{\"end\":49024,\"start\":49011},{\"end\":49202,\"start\":49180},{\"end\":49211,\"start\":49202},{\"end\":49226,\"start\":49211},{\"end\":49231,\"start\":49226},{\"end\":49578,\"start\":49561},{\"end\":49592,\"start\":49578},{\"end\":49606,\"start\":49592},{\"end\":49827,\"start\":49806},{\"end\":49842,\"start\":49827},{\"end\":49857,\"start\":49842},{\"end\":50318,\"start\":50302},{\"end\":50331,\"start\":50318},{\"end\":50573,\"start\":50562},{\"end\":50589,\"start\":50573},{\"end\":50597,\"start\":50589},{\"end\":50612,\"start\":50597},{\"end\":50885,\"start\":50874},{\"end\":50900,\"start\":50885},{\"end\":50911,\"start\":50900},{\"end\":50922,\"start\":50911},{\"end\":51442,\"start\":51425},{\"end\":51457,\"start\":51442},{\"end\":51819,\"start\":51809},{\"end\":51831,\"start\":51819},{\"end\":51845,\"start\":51831},{\"end\":51858,\"start\":51845},{\"end\":51870,\"start\":51858},{\"end\":52353,\"start\":52342},{\"end\":52364,\"start\":52353},{\"end\":52378,\"start\":52364},{\"end\":52392,\"start\":52378},{\"end\":52406,\"start\":52392},{\"end\":52417,\"start\":52406},{\"end\":52831,\"start\":52820},{\"end\":52845,\"start\":52831},{\"end\":52859,\"start\":52845},{\"end\":52870,\"start\":52859},{\"end\":52881,\"start\":52870},{\"end\":53293,\"start\":53285},{\"end\":53310,\"start\":53293},{\"end\":53324,\"start\":53310},{\"end\":53339,\"start\":53324},{\"end\":53355,\"start\":53339},{\"end\":53370,\"start\":53355},{\"end\":53690,\"start\":53679},{\"end\":53702,\"start\":53690},{\"end\":53715,\"start\":53702},{\"end\":53733,\"start\":53715},{\"end\":53744,\"start\":53733},{\"end\":54024,\"start\":54016},{\"end\":54034,\"start\":54024},{\"end\":54048,\"start\":54034},{\"end\":54061,\"start\":54048},{\"end\":54341,\"start\":54325},{\"end\":54671,\"start\":54656},{\"end\":54684,\"start\":54671},{\"end\":54698,\"start\":54684},{\"end\":54714,\"start\":54698},{\"end\":54729,\"start\":54714},{\"end\":55034,\"start\":55019},{\"end\":55046,\"start\":55034},{\"end\":55059,\"start\":55046},{\"end\":55072,\"start\":55059},{\"end\":55084,\"start\":55072},{\"end\":55374,\"start\":55358},{\"end\":55387,\"start\":55374},{\"end\":55402,\"start\":55387},{\"end\":55411,\"start\":55402},{\"end\":55851,\"start\":55838},{\"end\":55870,\"start\":55851},{\"end\":56035,\"start\":56022},{\"end\":56050,\"start\":56035},{\"end\":56064,\"start\":56050},{\"end\":56073,\"start\":56064},{\"end\":56086,\"start\":56073},{\"end\":56094,\"start\":56086},{\"end\":56108,\"start\":56094},{\"end\":56112,\"start\":56108}]", "bib_venue": "[{\"end\":34946,\"start\":34942},{\"end\":35234,\"start\":35193},{\"end\":35611,\"start\":35562},{\"end\":36007,\"start\":35930},{\"end\":36420,\"start\":36358},{\"end\":36738,\"start\":36702},{\"end\":37139,\"start\":37097},{\"end\":37545,\"start\":37496},{\"end\":37939,\"start\":37862},{\"end\":38413,\"start\":38373},{\"end\":38723,\"start\":38656},{\"end\":39128,\"start\":39051},{\"end\":39593,\"start\":39526},{\"end\":39996,\"start\":39919},{\"end\":40484,\"start\":40435},{\"end\":40901,\"start\":40852},{\"end\":41178,\"start\":41114},{\"end\":41642,\"start\":41575},{\"end\":42077,\"start\":42028},{\"end\":42402,\"start\":42284},{\"end\":42954,\"start\":42907},{\"end\":43269,\"start\":43246},{\"end\":43620,\"start\":43565},{\"end\":44044,\"start\":43987},{\"end\":44468,\"start\":44401},{\"end\":44917,\"start\":44879},{\"end\":45360,\"start\":45322},{\"end\":45813,\"start\":45769},{\"end\":46281,\"start\":46219},{\"end\":46734,\"start\":46653},{\"end\":47302,\"start\":47253},{\"end\":47754,\"start\":47735},{\"end\":48147,\"start\":48070},{\"end\":48663,\"start\":48586},{\"end\":48994,\"start\":48960},{\"end\":49280,\"start\":49231},{\"end\":49559,\"start\":49503},{\"end\":49934,\"start\":49857},{\"end\":50300,\"start\":50233},{\"end\":50560,\"start\":50506},{\"end\":50999,\"start\":50922},{\"end\":51506,\"start\":51457},{\"end\":51947,\"start\":51870},{\"end\":52474,\"start\":52417},{\"end\":52958,\"start\":52881},{\"end\":53427,\"start\":53386},{\"end\":53677,\"start\":53622},{\"end\":54080,\"start\":54061},{\"end\":54409,\"start\":54341},{\"end\":54654,\"start\":54592},{\"end\":55017,\"start\":54944},{\"end\":55488,\"start\":55411},{\"end\":55836,\"start\":55773},{\"end\":56020,\"start\":55979},{\"end\":36071,\"start\":36009},{\"end\":38003,\"start\":37941},{\"end\":38777,\"start\":38725},{\"end\":39192,\"start\":39130},{\"end\":39647,\"start\":39595},{\"end\":40060,\"start\":39998},{\"end\":41696,\"start\":41644},{\"end\":43279,\"start\":43271},{\"end\":44522,\"start\":44470},{\"end\":46802,\"start\":46736},{\"end\":48211,\"start\":48149},{\"end\":48727,\"start\":48665},{\"end\":49998,\"start\":49936},{\"end\":51063,\"start\":51001},{\"end\":52011,\"start\":51949},{\"end\":53022,\"start\":52960},{\"end\":55552,\"start\":55490}]"}}}, "year": 2023, "month": 12, "day": 17}