{"id": 227232796, "updated": "2022-01-17 16:06:49.273", "metadata": {"title": "Multi-Agent Embodied Question Answering in Interactive Environments", "authors": "[{\"middle\":[],\"last\":\"Tan\",\"first\":\"Sinan\"},{\"middle\":[],\"last\":\"Xiang\",\"first\":\"Weilai\"},{\"middle\":[],\"last\":\"Liu\",\"first\":\"Huaping\"},{\"middle\":[],\"last\":\"Guo\",\"first\":\"Di\"},{\"middle\":[],\"last\":\"Sun\",\"first\":\"Fuchun\"}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We investigate a new AI task \u2014 Multi-Agent Interactive Question Answering \u2014 where several agents explore the scene jointly in interactive environments to answer a question. To cooperate efficiently and answer accurately, agents must be well-organized to have balanced work division and share knowledge about the objects involved. We address this new problem in two stages: Multi-Agent 3D Reconstruction in Interactive Environments and Question Answering. Our proposed framework features multi-layer structural and semantic memories shared by all agents, as well as a question answering model built upon a 3D-CNN network to encode the scene memories. During the reconstruction, agents simultaneously explore and scan the scene with a clear division of work, organized by next viewpoints planning. We evaluate our framework on the IQuADv1 dataset and outperform the IQA baseline in a single-agent scenario. In multi-agent scenarios, our framework shows favorable speedups while remaining high accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3108503467", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/TanXLGS20", "doi": "10.1007/978-3-030-58601-0_39"}}, "content": {"source": {"pdf_hash": "1750864f01e9059ae69c02c27abe41f31024fc64", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d78bc45cfcd685326c3c42dba37e9fdbab075836", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1750864f01e9059ae69c02c27abe41f31024fc64.txt", "contents": "\nMulti-Agent Embodied Question Answering in Interactive Environments\n\n\nSinan Tan \nDepartment of Computer Science and Technology\nTsinghua University\n100084BeijingChina\n\nBeijing National Research Center for Information Science and Technology\n100084BeijingChina\n\nWeilai Xiang \nShenyuan Honors College\nBeihang University\n100191BeijingChina\n\nHuaping Liu \nDepartment of Computer Science and Technology\nTsinghua University\n100084BeijingChina\n\nBeijing National Research Center for Information Science and Technology\n100084BeijingChina\n\nDi Guo \nDepartment of Computer Science and Technology\nTsinghua University\n100084BeijingChina\n\nBeijing National Research Center for Information Science and Technology\n100084BeijingChina\n\nFuchun Sun \nDepartment of Computer Science and Technology\nTsinghua University\n100084BeijingChina\n\nBeijing National Research Center for Information Science and Technology\n100084BeijingChina\n\nMulti-Agent Embodied Question Answering in Interactive Environments\n3D ReconstructionEmbodied VisionQuestion Answering\nWe investigate a new AI task -Multi-Agent Interactive Question Answering -where several agents explore the scene jointly in interactive environments to answer a question. To cooperate efficiently and answer accurately, agents must be well-organized to have balanced work division and share knowledge about the objects involved. We address this new problem in two stages: Multi-Agent 3D Reconstruction in Interactive Environments and Question Answering. Our proposed framework features multi-layer structural and semantic memories shared by all agents, as well as a question answering model built upon a 3D-CNN network to encode the scene memories. During the reconstruction, agents simultaneously explore and scan the scene with a clear division of work, organized by next viewpoints planning. We evaluate our framework on the IQuADv1 dataset and outperform the IQA baseline in a single-agent scenario. In multi-agent scenarios, our framework shows favorable speedups while remaining high accuracy.\n\nIntroduction\n\nFor decades, one of our best wishes has been to develop robots that can assist humans with the ability to understand the scene, to interact with environments, and to communicate with humans. For instance, a domestic robot might be asked: How many apples are in the house? To answer it, the agent must explore the house, open fridges & cabinets for possibly hidden apples, check the occurrence of apples, and answer the question by natural language.\n\nThis sort of problem refers to Embodied Question Answering (EQA) [4] : Being asked What color is the car?, an agent navigates to the car and observes it before it answers the question. Since the car may be out of sight initially, the agent must have common sense about possible locations of the car and a way to get there. However, point-to-point navigation is not enough -what if we want the agent to search for a missing fork which may be anywhere in the kitchen?\n\nTo be more practical, Interactive Question Answering (IQA) [7] takes both interactive actions (e.g., open a cabinet) and more generic questions (e.g., existence and counting) into consideration. To answer Is there a fork in the kitchen?, the agent must have comprehensive cognition to the kitchen, without missing any place where the target may exist, including interactive objects like containers. However, this process could be time-costing.\n\nParallelism has always been a fundamental but effective idea. Since several agents can search for an object simultaneously, the question will soon be answered if multiple robots can explore collaboratively. Therefore, we introduce Multi-Agent Interactive Question Answering, which presents additional challenges to AI systems. First, the multi-agent system must be well-organized to avoid duplicate work and unbalanced work. Second, the multi-agent QA system must integrate information from all agents and answer the question accurately without a repeat or a miss. Third, the multi-agent system should achieve as high speedup as possible while keeping the high accuracy.\n\nVery few studies have looked into multi-agent embodied question answering tasks. However, active 3D reconstruction [5] [24] is not a novel problem. Here we propose a two-stage framework for Multi-Agent IQA, which firstly executes a multi-agent (embodied) 3D reconstruction to construct 3D global structural and semantic memories and secondly encodes the scene via 3D memories to answer the question. To support interactive objects, we propose a multi-layer data structure as an extension to traditional voxel-based reconstructions.\n\nWe train and evaluate our proposed two-stage framework on the IQuADv1 IQA dataset [7] in both single-agent and multi-agent scenarios and observe promising results of highly effective and efficient in both cases.\n\nContributions. In summary, our main contributions include:\n\n-Problem. We introduce the Multi-Agent IQA, the task of organizing collaborative Interactive Question Answering for several agents. -Method. We propose a two-stage framework for Multi-Agent IQA, a method to efficiently construct 3D global memories via multi-agent 3D reconstruction and to answer the question by encoding the scene memories with 3D-CNN. -Results. Our 3D-memory-based framework surpasses the original IQA method in both answering accuracy and episode length, with a single agent on the IQuADv1 dataset. With 2, 3, and 4 agents, we show consistent high-level parallelism and affordable speedups in average episode length.\n\n\nRelated Work\n\n\nQuestion Answering in Embodied Environments\n\nVisual Question Answering. VQA requires the agent to observe the given visual contents (i.e., images [1] or videos [13] [23]) and reason out the answer combining the multi-modal inputs. Common architectures for images VQA involve RNNs to encode questions, CNNs to encode images and fully connected layers to fuse language and visual features [15]. Our approach to Question Answering uses similar encoding and modality fusion strategies but uses a 3D-CNN to encode the scene with semantic memories instead of 2D-CNNs for images. Embodied Question Answering. EQA [4] requires active perception of the environment instead of answering with images passively received. Similar to Visual Semantic Navigation [22], EQA requires the agent to navigate from the current location to the target specified by its semantic category. Some recent studies use deep Reinforcement Learning (RL) to generate navigational actions directly from visual observations [4] [25]. However, for our problems which require holistic scene searching, point-to-point navigation is not enough. Interactive Question Answering. IQA is an extension of EQA with actionable environments and requires the agent to discover underlying objects. The IQuADv1 dataset [7] consists of question types including existence, counting and spatial relationship. Therefore, it requires holistic scene understanding to cover all occurrences of the object instead of direct navigation to a single target. The IQA baseline maintains a 2D spatial memory to encode semantic representation at each location. However, the top-down memory may fail to complex concepts like \"containing\". In our work, a 3D semantic memory is constructed to provide more precise records.\n\n\nMulti-Agent Systems\n\nMulti-agent systems offer obvious advantages over single-agent ones including parallelism, robustness, scalability, and fitness for geographic distribution [18]. For IQA tasks, expecting a robot to visit every corner where the apple may occur is unreal, but it will be possible to have several robots to answer the question quickly with parallelism when objects are scattered throughout the house. Multi-agent reinforcement learning is a popular topic related. Some studies involve the communication of local knowledge between agents [6][16] [19]. However, designing networks and protocols for communication becomes complicated for complex tasks when the number of agents increases. Meanwhile, many traditional multi-agent systems rely on optimization-based methods such as optimal mass transport [5], which can exploit collaborations of any number of agents for the 3D reconstruction task. In this paper, we adopt the optimization-based idea and formulate the multi-agent 3D reconstruction as a Set Cover Problem.\n\n\n3D Computer Vision\n\n3D Reconstruction. With RGB-D data available, 3D reconstruction becomes fundamental to 3D machine learning tasks. KinectFusion [12] is a typical realtime 3D reconstruction framework with TSDF [3] fusion. These volumetric-based methods result in voxel-wise data representing the structure of the target, denoted as \"Structural Memory\" in our work. Active 3D Reconstruction. In recent years we witnessed the development of active reconstruction by robots. Quite a few studies focus on proposing a measurement (e.g., the score of uncertainty or variance) field in the 3D space and selecting Next Best Views as targets for each time step [5] [24]. Inspired by these works, we evaluate voxel coverage from each view to select the next viewpoints with a set cover algorithm and assign them to agents by clustering. 3D Semantic Segmentation. With 3D datasets, 3D deep learning has made impressive progress. 3D-SIS [10] is one of those 3D instance segmentation frameworks which proposes 3D-RPN networks. Since we use 3D-CNN in question answering stage, here we use Mask R-CNN [9] to perform 2D instance segmentation and back-project the 2D semantic map to 3D voxels as Semantic Memory.\n\n\nEnvironments and Datasets\n\nThere are several environments for embodied agents widely used such as AI2-THOR [14], Habitat [17] (a platform supporting Matterport3D [2] and Gibson [21]) and House3D [20]. However, only AI2-THOR explicitly supports multiple agents as well as interactions with objects. Therefore, we adopt the AI2-THOR interactive environment for our embodied AIs. AI2-THOR is a photorealistic simulation environment consists of a variety of objects. We use the IQuADv1 dataset developed on AI2-THOR to evaluate our method with questions including counting, existence and spatial relationships. In our work, the simulator settings are slightly different from IQA [7]. We perform the OpenObject/CloseObject actions by specifying the object ID and allow agents to get the IDs from the simulator in already reconstructed areas to set free from the trouble of linking each object in 3D voxels to the object ID. Overview of the framework. The navigational actions for agents are planned step by step, according to the partially reconstructed memories. The agents execute these actions and update 3D memories along their routes. This procedure is repeated until the termination model decided there is enough data to answer the question or the whole scene is scanned. Then all agents are stopped, and the QA model encodes 3D memories and the question to predict the answer.\n\n\nOverview of the Proposed Framework\n\nOur framework features enriched structural and semantic memories built along with 3D Reconstruction. Afterward, the QA model gives the answer based on memories constructed. Therefore, our framework consists of these two parts:\n\n-Multi-Agent 3D Reconstruction in Interactive Environments: Our agents scan and reconstruct the interactive scene via voxel-based reconstruction, resulting in a global multi-layer structural memory. To divide labor for multiple agents and avoid duplicate work, we introduce a scalable optimization-based planner to select next-step viewpoints for each agent.\n\nThey are assigned to agents and agents execute actions to navigate towards these viewpoints. During this procedure, global semantic memory is being constructed as well for semantic-related questions, by back-projecting 2D instance segmentation results to the 3D volume. Meanwhile agents open every openable object they meet and a new exclusive layer in both memories is created to record the object's inside structure and contents. After the data in memories is sufficient to answer the question, the reconstruction stops.  Traditional voxel-based reconstruction does not support interactive scenes, because voxels occupied by openable objects may be in different states when they are open and closed (denoted as \"dynamic\" voxels in our paper). However, we have to record both situations, otherwise, we will miss apples in cabinets/fridges. To address this issue, we develop an extended data structure for 3D reconstruction, introducing the concept of \"layer\".\n\nFor an interactive scene with M interactive objects, we use M + 1 layers to store its structure, where each layer is a W \u00d7 L \u00d7 H array. Layer 0 represents the \"background\", i.e., the voxels when all openable objects are closed. Layer 1 to Layer M record M interactive objects to be open, in the order in which they are discovered during 3D reconstruction. In AI2Thor environments, all instances of certain categories (including \"Fridge\", \"Cabinet\" and \"Microwave Oven\") are guaranteed to be interactive (openable). Thus, when agents discover an object in those categories, a new layer is added.\n\nThis data structure is applied to both Structural Memory for 3D reconstruction and Semantic Memory for semantic(instance) segmentation in the scene.\n\n\nStructural Memory and Semantic Memory\n\nEach voxel in the multi-layer volume has multiple information stored, and one of the most important is its scan status. Here we call it the Structural Memory,  which monitors and records whether a voxel is scanned. This information is crucial to plan actions for the agents. To be specific, we assign a status to each voxel in the multi-layer volume, which is one of these following statuses:\n\n-UNKNOWN: The initial status of all voxels, representing that the voxel has not been scanned yet. -EMPTY: Indicating that in all scans involving this voxel, no object is found.\n\n-CONCRETE: Indicating that in at least one scan, a concrete object occupies this voxel.\n\nHence, the complete structure of the scene is modeled via this voxel-based memory. However, the Structural Memory itself only records the geometrical structure, which is not enough for downstream tasks (i.e. Question Answering in our case). Therefore a piece of extra information is recorded, named Semantic Memory.\n\nFor each scan of the 3D scene, an instance segmentation model will be applied to the observed RGB image. These semantic labels acquired is written into the Semantic Memory by back projecting labels to all \"CONCRETE\" voxels. The Semantic Memory provides visual information of the 3D scene, therefore it is used for question answering in our QA model.\n\n\nScanning Boundaries and Scanning Tasks\n\nScanning boundaries are voxels at the border of the scanned part and the unscanned part of a layer in the 3D scene. Denoting status(l, v) to be the status of a voxel with coordinate v = (x, y, z) on layer l, the actions of agents are planned according to these two kinds of scanning boundaries:\n\n-Scene Scanning Boundaries (B S ), the border between scanned and unscanned parts in Layer 0.\nB S = \uf8f1 \uf8f2 \uf8f3 v v \u2208 V, status(0, v) = UNKNOWN, CONCRETE \u2208 status(0, Adj(v)) \uf8fc \uf8fd \uf8fe(1)\n-Interactive Scanning Boundaries (B I ), the border between scanned and unscanned voxels in an object's \"dynamic\" part. For example, when a cabinet is open, the border between the scanned part inside the cabinet and the unscanned part of the cabinet is considered Interactive Scanning Boundary.\nB I = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 v v \u2208 V, status(i, v) = UNKNOWN, \u2203v a \u2208 Adj(v), status(i, v a ) = status(0, v a ), status(i, v a ) = CONCRETE \uf8fc \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fe(2)\nOne task for our agents is to cover voxels on scanning boundaries which represent the unfinished parts of reconstruction. Furthermore, agents must visit the unopened interactive objects (e.g. cabinets that have never been opened so far) because these unopened objects have no scanning boundaries yet. Therefore a new kind of task is created to ensure they will be opened at least once, and the scanning tasks are:\n\n-Voxels on Scanning Boundaries (B S \u222aB I ): They need to be observed on later scans to complete the memories. -Unopened interactive objects (T U ): These objects must be opened at least once to create new layers in memories and examine their inside.\n\nTherefore scanning tasks can be formulated as a set of voxels to be scanned:\nT = B S \u222a B I \u222a T U(3)\n\nViewpoint-Voxel Coverage Matrix\n\nThe target of the planning algorithm for scene reconstruction is to move the agent. Yet in our work we do not plan every single specific move, instead, we choose the target of a series of moves. To be convenient, we denote the discretized observed part of the scene as V , which contains all possible viewpoints. A viewpoint is the combination of position and rotation of the camera. Then for every viewpoint, we compute the visible voxels from it according to the reconstruction of the scene. Then we construct a matrix about whether a voxel in scanning tasks T can be seen from a viewpoint in V . This is denoted as C, a |V | \u00d7 |T | matrix, the Viewpoint-Voxel Coverage Matrix.\n\n\nTermination Condition\n\nWith the definition of scanning tasks, it's obvious that, when there are no remaining tasks for interactive scene reconstruction, the reconstruction process can be terminated. Therefore, the termination condition can be formulated as:\nT = \u2205(4)\nBesides, for some questions, the scan can be terminated before the environment is completely scanned. We propose a special learning-based Termination Model in Section 5 to achieve the early stopping.\n\n\nMulti-Agent 3D Reconstruction\n\nAfter introducing the data structures, tasks, and termination conditions, we propose the multi-agent reconstruction algorithm that repeats the routes planning procedure over and over again until either termination condition is satisfied.\n\nIn each iteration, we investigate the voxels to be scanned next and assign them to agents. They are introduced as Scanning Tasks and can be retrieved from the semi-finished Structural Memory. Afterward, we evaluate the visibility of those voxels from each possible viewpoint. To avoid duplicate work and maximize the efficiency, we expect agents to cover as more Scanning Tasks voxels as possible while having little intersection, so we convert it into a Set Cover Problem and solve it by a greedy algorithm.\n\nTo regroup the selected viewpoints into N groups (suppose that we have N agents), we use the K-means algorithm to execute a spatial clustering. Then we assign a cluster to each agent by solving a Balanced Assignment Problem to minimize the total route length from current locations to their target viewpoints. We plan the route for each agent with a TSP solver. Agents execute actions to navigate along the route and update the Structural Memory and Semantic Memory. After an agent reaches a viewpoint or when an agent gets stuck due to wrong route planning, we clear the routes and repeat the procedure to re-plan the moves.\n\nA 2D map is maintained according to the reconstructed 3D scene, determining which location the agent can pass through. This 2D map is used for route planning. At each time step, the 2D map is updated, and all newly discovered 3D objects will be created a new layer for, and added to the multi-layer 3D data structure. \n\n\nQuestion Answering with 3D-CNN and LSTM\n\n\n3D-CNN Scene Encoder and Question Encoder\n\nThe question answering model generates the answer according to the semantic volume and the given question. Here the question, denoted as Q, is encoded with an LSTM, getting the question feature f Q . For CNN, we first need to process all the observations we get in the process of 3D reconstruction.\n\nImagine a voxel v = (x, y, z) in the 3D volume with shape W \u00d7 L \u00d7 H, where W, L, H is the size of each dimension, respectively. Then this voxel v must have been observed several times in the multi-layer Semantic Memory with shape M \u00d7 W \u00d7 L \u00d7 H, not only from the multi-layer scans but also within each layer. We denote the total number of observations to v in Semantic Memory layers with class label c as N (v, c), and each observation has a confidence score of s i (v, c). We build a tensor V with shape C \u00d7 W \u00d7 L \u00d7 H to integrate all information about voxel v by averaging all these observations, i.e.:\nV(c, x, y, z) = 1 N (v, c) ( N (v,c) i=1 s i (v, c))(5)\nWe encode the semantic map with a 3D-CNN network similar to ResNet-18, yet replacing all 2D Convolution layers with 3D Convolution layers, yielding the scene feature vector f S . Since the 3D volume can be huge, we use submanifold sparse convolutions [8] instead of traditional convolutions to process those sparse data.\n\n\nQuestion Answering Model\n\nHere we concatenate the scene feature vector f S and the language feature vector f Q and use a simple multi-layer perceptron (MLP), to get the joint representation h of the scene and the question.\nh = MLP([f S ; f Q ]) (6)\nFinally, a fully connected layer is applied to produce the probability distribution of the final answer.\np(ans) = Softmax(W a h + b a )(7)\n\nTermination Model\n\nSimilarly, we apply another fully connect layer to predict the probability for agents to stop:\np(stop) = Softmax(W s h + b s )(8)\n\nTraining the QA Model and the Termination Model\n\nTraining the QA Model and the Termination Model is not a trivial task. Among thousands of voxels, sometimes there could be only less than ten voxels related to a given question. With such sparsity of interested voxels, end-to-end training of the QA network does not work in our experiments. Below are the steps we go through to train the QA model. Pretraining the Instance Segmentation Model We use Mask R-CNN for instance segmentation. The Mask R-CNN is trained on more than 10k images sampled from the 3D scenes in the training set, with annotations automatically generated from the output of the simulators. The pretrained model achieves 56.7% mAP on our validation set.\n\nPreparing training data For each question in the IQuADv1 dataset, we perform scene reconstruction with the proposed interactive reconstruction algorithm to generate semantic memory for 3D scenes in the IQuADv1 dataset. Since the termination model may decide to early stop the navigation, the intermediate reconstruction results are also saved for the QA model. This provides data for pretraining the 3D-CNN. Here we use the ground truth segmentation provided by the AI2Thor simulator.\n\nPretraining the 3D-CNN and the LSTM End-to-end training of the 3D-CNN & LSTM QA model from scratch is very hard to converge. Therefore, we split these two networks. For 3D-CNN network, we add three auxiliary branches, corresponding to three different kinds of questions in the IQuADv1 dataset. These branches respectively predict whether an object of a category exists, the number of objects of that category, and all containers holding that category of objects. We design loss functions for these three branches and pretrain the 3D-CNN alone. For the LSTM for language understanding, we pretrain it in the way similar to IQA, which is, using fully-connected layers on f Q to predict the question type and all involved object categories corresponding to the type.\n\nTraining the models The weights of pretrained networks are transferred to the 3D-CNN-LSTM QA Network. The whole QA network is then trained with the answers as supervision. The Termination Model is trained in a similar way, with the supervision being no further exploration is needed for a given semantic memory and question (e.g. for \"existence\" problems, the reconstruction process can stop immediately when the object we are interested in is already found).\n\n6 Experimental Results\n\n\nSingle-Agent IQA\n\nTo investigate the performance of our 3D-memory-based QA framework, we perform experiments with single-agent set-ups and compare it with the original IQA model proposed in [7]. Results are shown in Table 1. Accuracy and episode length for three question types in IQuADv1 dataset are reported separately. When ground truth (GT) semantic segmentation is available, our proposed framework not only outperforms the baseline method with GT detections, but also achieves higher accuracy than humans, showing the potential advantages of our model. Still, it takes more actions than humans to answer a question, indicating that its efficiency can be improved.\n\nWhen replacing GT segmentation with results predicted by Mask R-CNN, we notice obvious performance drops. It indicates that the bottleneck of our method is the accuracy of semantic segmentation. With more advanced methods such as multi-view based image segmentation, our method may perform better.\n\nHowever, even with predicted segmentation, the overall performance of our method still outperforms the IQA baseline with GT detection (better in Counting and Containing, worse in Existence), showing that the rest part of our model is robust enough to tolerate imperfect segmentation. Note that we use GT depth images to perform the 3D Reconstruction, but the IQA model noted as \"GT Detection\" uses GT depth as well. Since our method doesn't require very high reconstruction precision, noisy depth sensor is unlikely to cause severe problems. However, when it comes to predicted depth data, registration between different predicted depth frames, or MVS-based techniques like [11] would be required. We test the proposed framework on multi-agent set-ups. The results are shown in Table 2. There are no significant differences in accuracy for different numbers of agents, which indicates that our constructed semantic memory is sufficient and stable. Despite the similar accuracy, it takes much fewer steps for each agent to finish the task when more agents are available.\n\n\nMulti-Agent IQA on IQuADv1 Dataset\n\nTo examine the parallelism in our framework, we calculate the speedup in length with 2, 3, and 4 agents. The length is the maximum number of actions taken among all agents. When the task assignment is unbalanced, some agents may be in heavy load while other agents are idle. Therefore using the maximum number of actions as the metric can more accurately measure the speedup in terms of time consumption. As the number of agents increases, the speedup also becomes higher. However, the percentage of the actual speedup compared to the ideal speedup drops from 87% to 68%, when the number of agents increases from 2 to 4. These results show that our clustering and task assignment based multi-agent schedule algorithm is scalable, but still has room for improvement.\n\n\nQualitative Examples\n\nTo illustrate how our agents navigates in Embodied Environments, we select a question within the IQuADv1 dataset -How many eggs are there in the room?. The question is answered correctly with 1, 2, 3 and 4 agents set-ups. All agents are spawn at the same location as defined in the IQuADv1 dataset. We record the track of each agent and visualize it in the following figure.\n\nAs is shown in Fig. 5, the searching area of agents is relatively scattered. For example, with three agents (colored in red, yellow and blue), they act in the bottom part, the top-left part and the right part of the room respectively, indicating that the proposed task assignment algorithm is effective in allocating the scanning tasks to each agent. With more agents joining the reconstruct process, their searching area has more overlaps at the left and bottom part of the scene. That's reasonable because more interactive objects (mainly cabinets) exist in this region, and agents are required to head to this region when other parts have been fully reconstructed.\n\n\nConclusion\n\nIn this work, we introduce a new task of Multi-Agent Interactive Question Answering. We propose a novel two-stage framework to solve this problem, where firstly Multi-Agent 3D Reconstruction is performed to build a semantic memory, and then a 3D-CNN based QA model is used to generate the answer. Experiments show that our framework achieves high accuracy with single-agent set-up, and it is scalable to extend to multi-agent scenarios. Additionally, with GT semantic segmentation our proposed framework surpasses human performance, indicating that accurate 3D Semantic Segmentation is the bottleneck in our method.\n\nFig. 1 :\n1A demonstration of the Multi-Agent Interactive Question Answering task. Three agents search the room simultaneously with a clear division of work, enabling them to answer the question Do we have any apples? more efficiently.\n\nFig. 2 :\n2Fig. 2: Overview of the framework. The navigational actions for agents are planned step by step, according to the partially reconstructed memories. The agents execute these actions and update 3D memories along their routes. This procedure is repeated until the termination model decided there is enough data to answer the question or the whole scene is scanned. Then all agents are stopped, and the QA model encodes 3D memories and the question to predict the answer.\n\n\n-Question Answering with 3D-CNN and LSTM: A 3D-CNN network is used to encode the semantic memory and an LSTM network is used to encode questions. Then we concatenate the semantic feature and the language feature and predict the final answer by an MLP.\n\nFig. 3 :\n3The proposed multi-layer data structure from different angles. The \"background\" layer is demonstrated in white, recording the scene with all objects in default. The \"dynamic\" voxels in layers for interactive objects are shown in color. Large colored cubes represent CONCRETE voxels, while smaller ones are EMPTY voxels inside.\n\nFig. 4 :\n4(a) Demonstration of a fully reconstructed Structural Memory in Layer 0. Large white cubes represent CONCRETE voxels, while small yellow cubes represent EMPTY voxels. The vast parts outside the volume are UNKNOWN. (b) Demonstration of the corresponding Semantic Memory. Each color represents one of the 20 semantic categories in the IQuADv1 dataset. White cubes represent background voxels or voxels of other unspecified semantic categories.\n\nFig. 5 :\n5A qualitative example with the question and some rendered image of the scene on the left side. The track of each agent, with single-agent set-up and multi-agent set-up with 2, 3 and 4 agents are shown on the right.\n\nAlgorithm 1 :\n1Multi-Agent 3D Reconstruction Result: SceneMap Initialize SceneMap; while T = \u2205 \u2227 \u00acTerminationModel(SceneMap) do Generate B S , B I , and T U ; Generate T = B S \u222a B I \u222a T U and Count all viewpoints V ; Evaluate the coverage of T from each viewpoint to form matrix C; Choose a subset v \u2286 V by running Set Cover Problem solver on C;Regroup v into N clusters: v = \u222a N j=1 v j by K-means Algorithm, and assign v j to agent A i by Hungarian Algorithm; Plan route for agent A i to travel a series of viewpoints until An agent reaches a selected viewpoint \u2228 One gets blocked ; endv j = {v j,1 , v j,2 , ..., v j,nj } with TSP solver; \nrepeat \nExecute actions along the planned routes; \nUpdate the SceneMap for each step; \nAdd new layers to for newly discovered interactive objects on \nSceneMap; \nUpdate the 2D Map for navigation according to SceneMap; \n\n\nTable 1 :\n1Experiment results with single-agent set-up IQA (Pred. Detection) uses predicted depth, while others use GT depthExistence \nCounting \nContaining \nModel \nAccuracy Length Accuracy Length Accuracy Length \nIQA (GT Detection) [7] \n86.56% 679.70 35.31% 604.79 70.94% 311.03 \nIQA (Pred. Detection) [7] 68.47% 318.33 30.43% 926.11 58.67% 516.23 \nHuman [7] \n90.00% 58.40 80.00% 81.90 90.00% 43.00 \nOurs (GT Segmentation) \n98.75% 166.31 88.28 % 237.40 91.88% 195.89 \nOurs (Pred. Segmentation) 79.53% 159.85 45.62% 220.95 77.50% 204.87 \n\n\n\nTable 2 :\n2Multi-agent experiments with N agents Acc. Length Acc. Length Acc. Length Acc. Length Ideal Actual % of Ideal 1 79.53% 159.85 45.62% 220.95 77.50% 204.87 67.55% 195.22 1.0 1.00 100% 2 78.59% 93.99 46.56% 127.57 77.50% 116.69 67.55% 112.75 2.0 1.73 87% 3 78.28% 69.24 45.47% 90.33 77.03% 86.36 66.93% 81.98 3.0 2.38 79% 4 78.44% 60.41 43.91% 79.63 76.25% 75.46 66.20% 71.83 4.0 2.72 68%Existence \nCounting \nContaining \nOverall \nOverall Speedup \nN \nAcknowledgementsThis work was supported in part by the National Natural Science Foundation of China under Grants U1613212 and 61703284.\nVqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C Lawrence Zitnick, D Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh, D.: Vqa: Visual question answering. In: Proceedings of the IEEE international conference on computer vision. pp. 2425-2433 (2015)\n\nA Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, arXiv:1709.06158Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprintChang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matterport3d: Learning from rgb-d data in indoor envi- ronments. arXiv preprint arXiv:1709.06158 (2017)\n\nA volumetric method for building complex models from range images. B Curless, M Levoy, Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques. the 23rd Annual Conference on Computer Graphics and Interactive TechniquesCurless, B., Levoy, M.: A volumetric method for building complex models from range images. In: Proceedings of the 23rd Annual Conference on Computer Graph- ics and Interactive Techniques. pp. 303-312 (1996)\n\nEmbodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsDas, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied ques- tion answering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. pp. 2054-2063 (2018)\n\nMultirobot collaborative dense scene reconstruction. S Dong, K Xu, Q Zhou, A Tagliasacchi, S Xin, M Nie\u00dfner, B Chen, ACM Transactions on Graphics (TOG). 384Dong, S., Xu, K., Zhou, Q., Tagliasacchi, A., Xin, S., Nie\u00dfner, M., Chen, B.: Multi- robot collaborative dense scene reconstruction. ACM Transactions on Graphics (TOG) 38(4), 1-16 (2019)\n\nLearning to communicate with deep multi-agent reinforcement learning. J Foerster, I A Assael, N De Freitas, S Whiteson, Advances in neural information processing systems. Foerster, J., Assael, I.A., De Freitas, N., Whiteson, S.: Learning to communicate with deep multi-agent reinforcement learning. In: Advances in neural information processing systems. pp. 2137-2145 (2016)\n\nIqa: Visual question answering in interactive environments. D Gordon, A Kembhavi, M Rastegari, J Redmon, D Fox, A Farhadi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionGordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., Farhadi, A.: Iqa: Visual question answering in interactive environments. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4089-4098 (2018)\n\nB Graham, L Van Der Maaten, arXiv:1706.01307Submanifold sparse convolutional networks. arXiv preprintGraham, B., van der Maaten, L.: Submanifold sparse convolutional networks. arXiv preprint arXiv:1706.01307 (2017)\n\nMask r-cnn. K He, G Gkioxari, P Dollar, R Girshick, The IEEE International Conference on Computer Vision (ICCV). He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: The IEEE Interna- tional Conference on Computer Vision (ICCV) (Oct 2017)\n\n3d-sis: 3d semantic instance segmentation of rgbd scans. J Hou, A Dai, M Niessner, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Hou, J., Dai, A., Niessner, M.: 3d-sis: 3d semantic instance segmentation of rgb- d scans. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)\n\nDeepmvs: Learning multi-view stereopsis. P H Huang, K Matzen, J Kopf, N Ahuja, J B Huang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHuang, P.H., Matzen, K., Kopf, J., Ahuja, N., Huang, J.B.: Deepmvs: Learning multi-view stereopsis. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2821-2830 (2018)\n\nKinectfusion: Real-time 3d reconstruction and interaction using a moving depth camera. S Izadi, D Kim, O Hilliges, D Molyneaux, R Newcombe, P Kohli, J Shotton, S Hodges, D Freeman, A Davison, Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology. the 24th Annual ACM Symposium on User Interface Software and TechnologyIzadi, S., Kim, D., Hilliges, O., Molyneaux, D., Newcombe, R., Kohli, P., Shot- ton, J., Hodges, S., Freeman, D., Davison, A., et al.: Kinectfusion: Real-time 3d reconstruction and interaction using a moving depth camera. In: Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology. pp. 559-568 (2011)\n\nTgif-qa: Toward spatio-temporal reasoning in visual question answering. Y Jang, Y Song, Y Yu, Y Kim, G Kim, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jang, Y., Song, Y., Yu, Y., Kim, Y., Kim, G.: Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (July 2017)\n\nAi2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, D Gordon, Y Zhu, A Gupta, A Farhadi, arXiv:1712.05474arXiv preprintKolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Gordon, D., Zhu, Y., Gupta, A., Farhadi, A.: Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474 (2017)\n\nAsk your neurons: A neural-based approach to answering questions about images. M Malinowski, M Rohrbach, M Fritz, The IEEE International Conference on Computer Vision (ICCV. Malinowski, M., Rohrbach, M., Fritz, M.: Ask your neurons: A neural-based ap- proach to answering questions about images. In: The IEEE International Confer- ence on Computer Vision (ICCV) (December 2015)\n\nMulti-agent image classification via reinforcement learning. H K Mousavi, M Nazari, M Tak\u00e1\u010d, N Motee, arXiv:1905.04835arXiv preprintMousavi, H.K., Nazari, M., Tak\u00e1\u010d, M., Motee, N.: Multi-agent image classification via reinforcement learning. arXiv preprint arXiv:1905.04835 (2019)\n\nHabitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionSavva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., et al.: Habitat: A platform for embodied ai research. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 9339-9347 (2019)\n\nMultiagent systems: A survey from a machine learning perspective. P Stone, M Veloso, Autonomous Robots. 83Stone, P., Veloso, M.: Multiagent systems: A survey from a machine learning per- spective. Autonomous Robots 8(3), 345-383 (2000)\n\nLearning multiagent communication with backpropagation. S Sukhbaatar, R Fergus, Advances in neural information processing systems. Sukhbaatar, S., Fergus, R., et al.: Learning multiagent communication with back- propagation. In: Advances in neural information processing systems. pp. 2244-2252 (2016)\n\nBuilding generalizable agents with a realistic and rich 3d environment. Y Wu, Y Wu, G Gkioxari, Y Tian, arXiv:1801.02209arXiv preprintWu, Y., Wu, Y., Gkioxari, G., Tian, Y.: Building generalizable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209 (2018)\n\nF Xia, C Li, K Chen, W B Shen, R Mart\u0131n-Mart\u0131n, N Hirose, A R Zamir, L F F S Savarese, Gibson env v2: Embodied simulation environments for interactive navigation. Xia, F., Li, C., Chen, K., Shen, W.B., Mart\u0131n-Mart\u0131n, R., Hirose, N., Zamir, A.R., Savarese, L.F.F.S.: Gibson env v2: Embodied simulation environments for interac- tive navigation (2019)\n\nVisual semantic navigation using scene priors. W Yang, X Wang, A Farhadi, A Gupta, R Mottaghi, arXiv:1810.06543arXiv preprintYang, W., Wang, X., Farhadi, A., Gupta, A., Mottaghi, R.: Visual semantic navi- gation using scene priors. arXiv preprint arXiv:1810.06543 (2018)\n\nVideo question answering via hierarchical spatio-temporal attention networks. Z Zhao, Q Yang, D Cai, X He, Y Zhuang, Z Zhao, Q Yang, D Cai, X He, Y Zhuang, IJCAI. pp. Zhao, Z., Yang, Q., Cai, D., He, X., Zhuang, Y., Zhao, Z., Yang, Q., Cai, D., He, X., Zhuang, Y.: Video question answering via hierarchical spatio-temporal attention networks. In: IJCAI. pp. 3518-3524 (2017)\n\nActive scene understanding via online semantic reconstruction. L Zheng, C Zhu, J Zhang, H Zhao, H Huang, M Niessner, K Xu, Computer Graphics Forum. 38Wiley Online LibraryZheng, L., Zhu, C., Zhang, J., Zhao, H., Huang, H., Niessner, M., Xu, K.: Active scene understanding via online semantic reconstruction. In: Computer Graphics Forum. vol. 38, pp. 103-114. Wiley Online Library (2019)\n\nTarget-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, 2017 IEEE international conference on robotics and automation (ICRA). IEEEZhu, Y., Mottaghi, R., Kolve, E., Lim, J.J., Gupta, A., Fei-Fei, L., Farhadi, A.: Target-driven visual navigation in indoor scenes using deep reinforcement learning. In: 2017 IEEE international conference on robotics and automation (ICRA). pp. 3357-3364. IEEE (2017)\n", "annotations": {"author": "[{\"start\":\"71\",\"end\":\"259\"},{\"start\":\"260\",\"end\":\"336\"},{\"start\":\"337\",\"end\":\"527\"},{\"start\":\"528\",\"end\":\"713\"},{\"start\":\"714\",\"end\":\"903\"}]", "publisher": null, "author_last_name": "[{\"start\":\"77\",\"end\":\"80\"},{\"start\":\"267\",\"end\":\"272\"},{\"start\":\"345\",\"end\":\"348\"},{\"start\":\"531\",\"end\":\"534\"},{\"start\":\"721\",\"end\":\"724\"}]", "author_first_name": "[{\"start\":\"71\",\"end\":\"76\"},{\"start\":\"260\",\"end\":\"266\"},{\"start\":\"337\",\"end\":\"344\"},{\"start\":\"528\",\"end\":\"530\"},{\"start\":\"714\",\"end\":\"720\"}]", "author_affiliation": "[{\"start\":\"82\",\"end\":\"166\"},{\"start\":\"168\",\"end\":\"258\"},{\"start\":\"274\",\"end\":\"335\"},{\"start\":\"350\",\"end\":\"434\"},{\"start\":\"436\",\"end\":\"526\"},{\"start\":\"536\",\"end\":\"620\"},{\"start\":\"622\",\"end\":\"712\"},{\"start\":\"726\",\"end\":\"810\"},{\"start\":\"812\",\"end\":\"902\"}]", "title": "[{\"start\":\"1\",\"end\":\"68\"},{\"start\":\"904\",\"end\":\"971\"}]", "venue": null, "abstract": "[{\"start\":\"1023\",\"end\":\"2021\"}]", "bib_ref": "[{\"start\":\"2552\",\"end\":\"2555\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3013\",\"end\":\"3016\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"4186\",\"end\":\"4189\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"4190\",\"end\":\"4194\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"4686\",\"end\":\"4689\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"5676\",\"end\":\"5679\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"5690\",\"end\":\"5694\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"5695\",\"end\":\"5699\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"5917\",\"end\":\"5921\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"6136\",\"end\":\"6139\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"6277\",\"end\":\"6281\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"6518\",\"end\":\"6521\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"6522\",\"end\":\"6526\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"6798\",\"end\":\"6801\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"7462\",\"end\":\"7466\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"7840\",\"end\":\"7843\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"7848\",\"end\":\"7852\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"8103\",\"end\":\"8106\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"8470\",\"end\":\"8474\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"8535\",\"end\":\"8538\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"8977\",\"end\":\"8980\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"8981\",\"end\":\"8985\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"9250\",\"end\":\"9254\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"9411\",\"end\":\"9414\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"9630\",\"end\":\"9634\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"9644\",\"end\":\"9648\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"9685\",\"end\":\"9688\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"9700\",\"end\":\"9704\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"9718\",\"end\":\"9722\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"10198\",\"end\":\"10201\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"20553\",\"end\":\"20556\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"23815\",\"end\":\"23818\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"25269\",\"end\":\"25273\",\"attributes\":{\"ref_id\":\"b10\"}}]", "figure": "[{\"start\":\"28167\",\"end\":\"28402\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"28403\",\"end\":\"28881\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"28882\",\"end\":\"29135\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"29136\",\"end\":\"29473\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"29474\",\"end\":\"29926\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"29927\",\"end\":\"30152\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"30153\",\"end\":\"31015\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"31016\",\"end\":\"31555\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"31556\",\"end\":\"32014\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2037\",\"end\":\"2485\"},{\"start\":\"2487\",\"end\":\"2952\"},{\"start\":\"2954\",\"end\":\"3397\"},{\"start\":\"3399\",\"end\":\"4069\"},{\"start\":\"4071\",\"end\":\"4602\"},{\"start\":\"4604\",\"end\":\"4815\"},{\"start\":\"4817\",\"end\":\"4875\"},{\"start\":\"4877\",\"end\":\"5512\"},{\"start\":\"5575\",\"end\":\"7282\"},{\"start\":\"7306\",\"end\":\"8320\"},{\"start\":\"8343\",\"end\":\"9520\"},{\"start\":\"9550\",\"end\":\"10901\"},{\"start\":\"10940\",\"end\":\"11166\"},{\"start\":\"11168\",\"end\":\"11526\"},{\"start\":\"11528\",\"end\":\"12488\"},{\"start\":\"12490\",\"end\":\"13084\"},{\"start\":\"13086\",\"end\":\"13234\"},{\"start\":\"13276\",\"end\":\"13668\"},{\"start\":\"13670\",\"end\":\"13846\"},{\"start\":\"13848\",\"end\":\"13935\"},{\"start\":\"13937\",\"end\":\"14252\"},{\"start\":\"14254\",\"end\":\"14603\"},{\"start\":\"14646\",\"end\":\"14940\"},{\"start\":\"14942\",\"end\":\"15035\"},{\"start\":\"15119\",\"end\":\"15413\"},{\"start\":\"15577\",\"end\":\"15990\"},{\"start\":\"15992\",\"end\":\"16241\"},{\"start\":\"16243\",\"end\":\"16319\"},{\"start\":\"16377\",\"end\":\"17056\"},{\"start\":\"17082\",\"end\":\"17316\"},{\"start\":\"17326\",\"end\":\"17525\"},{\"start\":\"17559\",\"end\":\"17796\"},{\"start\":\"17798\",\"end\":\"18306\"},{\"start\":\"18308\",\"end\":\"18933\"},{\"start\":\"18935\",\"end\":\"19253\"},{\"start\":\"19341\",\"end\":\"19639\"},{\"start\":\"19641\",\"end\":\"20245\"},{\"start\":\"20302\",\"end\":\"20622\"},{\"start\":\"20651\",\"end\":\"20847\"},{\"start\":\"20874\",\"end\":\"20978\"},{\"start\":\"21033\",\"end\":\"21127\"},{\"start\":\"21213\",\"end\":\"21886\"},{\"start\":\"21888\",\"end\":\"22372\"},{\"start\":\"22374\",\"end\":\"23137\"},{\"start\":\"23139\",\"end\":\"23598\"},{\"start\":\"23600\",\"end\":\"23622\"},{\"start\":\"23643\",\"end\":\"24294\"},{\"start\":\"24296\",\"end\":\"24593\"},{\"start\":\"24595\",\"end\":\"25664\"},{\"start\":\"25703\",\"end\":\"26468\"},{\"start\":\"26493\",\"end\":\"26867\"},{\"start\":\"26869\",\"end\":\"27536\"},{\"start\":\"27551\",\"end\":\"28166\"}]", "formula": "[{\"start\":\"15036\",\"end\":\"15118\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"15414\",\"end\":\"15576\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"16320\",\"end\":\"16342\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"17317\",\"end\":\"17325\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"20246\",\"end\":\"20301\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"20848\",\"end\":\"20873\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"20979\",\"end\":\"21012\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"21128\",\"end\":\"21162\",\"attributes\":{\"id\":\"formula_7\"}}]", "table_ref": "[{\"start\":\"23841\",\"end\":\"23848\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"25373\",\"end\":\"25380\",\"attributes\":{\"ref_id\":\"tab_2\"}}]", "section_header": "[{\"start\":\"2023\",\"end\":\"2035\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"5515\",\"end\":\"5527\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"5530\",\"end\":\"5573\",\"attributes\":{\"n\":\"2.1\"}},{\"start\":\"7285\",\"end\":\"7304\",\"attributes\":{\"n\":\"2.2\"}},{\"start\":\"8323\",\"end\":\"8341\",\"attributes\":{\"n\":\"2.3\"}},{\"start\":\"9523\",\"end\":\"9548\",\"attributes\":{\"n\":\"2.4\"}},{\"start\":\"10904\",\"end\":\"10938\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"13237\",\"end\":\"13274\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"14606\",\"end\":\"14644\",\"attributes\":{\"n\":\"4.3\"}},{\"start\":\"16344\",\"end\":\"16375\",\"attributes\":{\"n\":\"4.4\"}},{\"start\":\"17059\",\"end\":\"17080\",\"attributes\":{\"n\":\"4.5\"}},{\"start\":\"17528\",\"end\":\"17557\",\"attributes\":{\"n\":\"4.6\"}},{\"start\":\"19256\",\"end\":\"19295\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"19298\",\"end\":\"19339\",\"attributes\":{\"n\":\"5.1\"}},{\"start\":\"20625\",\"end\":\"20649\",\"attributes\":{\"n\":\"5.2\"}},{\"start\":\"21014\",\"end\":\"21031\",\"attributes\":{\"n\":\"5.3\"}},{\"start\":\"21164\",\"end\":\"21211\",\"attributes\":{\"n\":\"5.4\"}},{\"start\":\"23625\",\"end\":\"23641\",\"attributes\":{\"n\":\"6.1\"}},{\"start\":\"25667\",\"end\":\"25701\",\"attributes\":{\"n\":\"6.2\"}},{\"start\":\"26471\",\"end\":\"26491\",\"attributes\":{\"n\":\"6.3\"}},{\"start\":\"27539\",\"end\":\"27549\",\"attributes\":{\"n\":\"7\"}},{\"start\":\"28168\",\"end\":\"28176\"},{\"start\":\"28404\",\"end\":\"28412\"},{\"start\":\"29137\",\"end\":\"29145\"},{\"start\":\"29475\",\"end\":\"29483\"},{\"start\":\"29928\",\"end\":\"29936\"},{\"start\":\"30154\",\"end\":\"30167\"},{\"start\":\"31017\",\"end\":\"31026\"},{\"start\":\"31557\",\"end\":\"31566\"}]", "table": "[{\"start\":\"30742\",\"end\":\"31015\"},{\"start\":\"31141\",\"end\":\"31555\"},{\"start\":\"31953\",\"end\":\"32014\"}]", "figure_caption": "[{\"start\":\"28178\",\"end\":\"28402\"},{\"start\":\"28414\",\"end\":\"28881\"},{\"start\":\"28884\",\"end\":\"29135\"},{\"start\":\"29147\",\"end\":\"29473\"},{\"start\":\"29485\",\"end\":\"29926\"},{\"start\":\"29938\",\"end\":\"30152\"},{\"start\":\"30169\",\"end\":\"30742\"},{\"start\":\"31028\",\"end\":\"31141\"},{\"start\":\"31568\",\"end\":\"31953\"}]", "figure_ref": "[{\"start\":\"26884\",\"end\":\"26890\",\"attributes\":{\"ref_id\":\"fig_6\"}}]", "bib_author_first_name": "[{\"start\":\"32183\",\"end\":\"32184\"},{\"start\":\"32192\",\"end\":\"32193\"},{\"start\":\"32203\",\"end\":\"32204\"},{\"start\":\"32209\",\"end\":\"32210\"},{\"start\":\"32221\",\"end\":\"32222\"},{\"start\":\"32230\",\"end\":\"32231\"},{\"start\":\"32250\",\"end\":\"32251\"},{\"start\":\"32599\",\"end\":\"32600\"},{\"start\":\"32608\",\"end\":\"32609\"},{\"start\":\"32615\",\"end\":\"32616\"},{\"start\":\"32629\",\"end\":\"32630\"},{\"start\":\"32639\",\"end\":\"32640\"},{\"start\":\"32651\",\"end\":\"32652\"},{\"start\":\"32660\",\"end\":\"32661\"},{\"start\":\"32668\",\"end\":\"32669\"},{\"start\":\"32676\",\"end\":\"32677\"},{\"start\":\"33054\",\"end\":\"33055\"},{\"start\":\"33065\",\"end\":\"33066\"},{\"start\":\"33476\",\"end\":\"33477\"},{\"start\":\"33483\",\"end\":\"33484\"},{\"start\":\"33492\",\"end\":\"33493\"},{\"start\":\"33504\",\"end\":\"33505\"},{\"start\":\"33511\",\"end\":\"33512\"},{\"start\":\"33521\",\"end\":\"33522\"},{\"start\":\"33956\",\"end\":\"33957\"},{\"start\":\"33964\",\"end\":\"33965\"},{\"start\":\"33970\",\"end\":\"33971\"},{\"start\":\"33978\",\"end\":\"33979\"},{\"start\":\"33994\",\"end\":\"33995\"},{\"start\":\"34001\",\"end\":\"34002\"},{\"start\":\"34012\",\"end\":\"34013\"},{\"start\":\"34317\",\"end\":\"34318\"},{\"start\":\"34329\",\"end\":\"34330\"},{\"start\":\"34331\",\"end\":\"34332\"},{\"start\":\"34341\",\"end\":\"34342\"},{\"start\":\"34355\",\"end\":\"34356\"},{\"start\":\"34683\",\"end\":\"34684\"},{\"start\":\"34693\",\"end\":\"34694\"},{\"start\":\"34705\",\"end\":\"34706\"},{\"start\":\"34718\",\"end\":\"34719\"},{\"start\":\"34728\",\"end\":\"34729\"},{\"start\":\"34735\",\"end\":\"34736\"},{\"start\":\"35127\",\"end\":\"35128\"},{\"start\":\"35137\",\"end\":\"35138\"},{\"start\":\"35355\",\"end\":\"35356\"},{\"start\":\"35361\",\"end\":\"35362\"},{\"start\":\"35373\",\"end\":\"35374\"},{\"start\":\"35383\",\"end\":\"35384\"},{\"start\":\"35651\",\"end\":\"35652\"},{\"start\":\"35658\",\"end\":\"35659\"},{\"start\":\"35665\",\"end\":\"35666\"},{\"start\":\"35967\",\"end\":\"35968\"},{\"start\":\"35969\",\"end\":\"35970\"},{\"start\":\"35978\",\"end\":\"35979\"},{\"start\":\"35988\",\"end\":\"35989\"},{\"start\":\"35996\",\"end\":\"35997\"},{\"start\":\"36005\",\"end\":\"36006\"},{\"start\":\"36007\",\"end\":\"36008\"},{\"start\":\"36449\",\"end\":\"36450\"},{\"start\":\"36458\",\"end\":\"36459\"},{\"start\":\"36465\",\"end\":\"36466\"},{\"start\":\"36477\",\"end\":\"36478\"},{\"start\":\"36490\",\"end\":\"36491\"},{\"start\":\"36502\",\"end\":\"36503\"},{\"start\":\"36511\",\"end\":\"36512\"},{\"start\":\"36522\",\"end\":\"36523\"},{\"start\":\"36532\",\"end\":\"36533\"},{\"start\":\"36543\",\"end\":\"36544\"},{\"start\":\"37119\",\"end\":\"37120\"},{\"start\":\"37127\",\"end\":\"37128\"},{\"start\":\"37135\",\"end\":\"37136\"},{\"start\":\"37141\",\"end\":\"37142\"},{\"start\":\"37148\",\"end\":\"37149\"},{\"start\":\"37486\",\"end\":\"37487\"},{\"start\":\"37495\",\"end\":\"37496\"},{\"start\":\"37507\",\"end\":\"37508\"},{\"start\":\"37514\",\"end\":\"37515\"},{\"start\":\"37528\",\"end\":\"37529\"},{\"start\":\"37537\",\"end\":\"37538\"},{\"start\":\"37549\",\"end\":\"37550\"},{\"start\":\"37559\",\"end\":\"37560\"},{\"start\":\"37566\",\"end\":\"37567\"},{\"start\":\"37575\",\"end\":\"37576\"},{\"start\":\"37910\",\"end\":\"37911\"},{\"start\":\"37924\",\"end\":\"37925\"},{\"start\":\"37936\",\"end\":\"37937\"},{\"start\":\"38271\",\"end\":\"38272\"},{\"start\":\"38273\",\"end\":\"38274\"},{\"start\":\"38284\",\"end\":\"38285\"},{\"start\":\"38294\",\"end\":\"38295\"},{\"start\":\"38303\",\"end\":\"38304\"},{\"start\":\"38538\",\"end\":\"38539\"},{\"start\":\"38547\",\"end\":\"38548\"},{\"start\":\"38557\",\"end\":\"38558\"},{\"start\":\"38570\",\"end\":\"38571\"},{\"start\":\"38578\",\"end\":\"38579\"},{\"start\":\"38589\",\"end\":\"38590\"},{\"start\":\"38597\",\"end\":\"38598\"},{\"start\":\"38607\",\"end\":\"38608\"},{\"start\":\"38614\",\"end\":\"38615\"},{\"start\":\"38624\",\"end\":\"38625\"},{\"start\":\"39084\",\"end\":\"39085\"},{\"start\":\"39093\",\"end\":\"39094\"},{\"start\":\"39311\",\"end\":\"39312\"},{\"start\":\"39325\",\"end\":\"39326\"},{\"start\":\"39629\",\"end\":\"39630\"},{\"start\":\"39635\",\"end\":\"39636\"},{\"start\":\"39641\",\"end\":\"39642\"},{\"start\":\"39653\",\"end\":\"39654\"},{\"start\":\"39843\",\"end\":\"39844\"},{\"start\":\"39850\",\"end\":\"39851\"},{\"start\":\"39856\",\"end\":\"39857\"},{\"start\":\"39864\",\"end\":\"39865\"},{\"start\":\"39866\",\"end\":\"39867\"},{\"start\":\"39874\",\"end\":\"39875\"},{\"start\":\"39891\",\"end\":\"39892\"},{\"start\":\"39901\",\"end\":\"39902\"},{\"start\":\"39903\",\"end\":\"39904\"},{\"start\":\"39912\",\"end\":\"39913\"},{\"start\":\"39914\",\"end\":\"39919\"},{\"start\":\"40241\",\"end\":\"40242\"},{\"start\":\"40249\",\"end\":\"40250\"},{\"start\":\"40257\",\"end\":\"40258\"},{\"start\":\"40268\",\"end\":\"40269\"},{\"start\":\"40277\",\"end\":\"40278\"},{\"start\":\"40544\",\"end\":\"40545\"},{\"start\":\"40552\",\"end\":\"40553\"},{\"start\":\"40560\",\"end\":\"40561\"},{\"start\":\"40567\",\"end\":\"40568\"},{\"start\":\"40573\",\"end\":\"40574\"},{\"start\":\"40583\",\"end\":\"40584\"},{\"start\":\"40591\",\"end\":\"40592\"},{\"start\":\"40599\",\"end\":\"40600\"},{\"start\":\"40606\",\"end\":\"40607\"},{\"start\":\"40612\",\"end\":\"40613\"},{\"start\":\"40905\",\"end\":\"40906\"},{\"start\":\"40914\",\"end\":\"40915\"},{\"start\":\"40921\",\"end\":\"40922\"},{\"start\":\"40930\",\"end\":\"40931\"},{\"start\":\"40938\",\"end\":\"40939\"},{\"start\":\"40947\",\"end\":\"40948\"},{\"start\":\"40959\",\"end\":\"40960\"},{\"start\":\"41313\",\"end\":\"41314\"},{\"start\":\"41320\",\"end\":\"41321\"},{\"start\":\"41332\",\"end\":\"41333\"},{\"start\":\"41341\",\"end\":\"41342\"},{\"start\":\"41343\",\"end\":\"41344\"},{\"start\":\"41350\",\"end\":\"41351\"},{\"start\":\"41359\",\"end\":\"41360\"},{\"start\":\"41370\",\"end\":\"41371\"}]", "bib_author_last_name": "[{\"start\":\"32185\",\"end\":\"32190\"},{\"start\":\"32194\",\"end\":\"32201\"},{\"start\":\"32205\",\"end\":\"32207\"},{\"start\":\"32211\",\"end\":\"32219\"},{\"start\":\"32223\",\"end\":\"32228\"},{\"start\":\"32232\",\"end\":\"32248\"},{\"start\":\"32252\",\"end\":\"32258\"},{\"start\":\"32601\",\"end\":\"32606\"},{\"start\":\"32610\",\"end\":\"32613\"},{\"start\":\"32617\",\"end\":\"32627\"},{\"start\":\"32631\",\"end\":\"32637\"},{\"start\":\"32641\",\"end\":\"32649\"},{\"start\":\"32653\",\"end\":\"32658\"},{\"start\":\"32662\",\"end\":\"32666\"},{\"start\":\"32670\",\"end\":\"32674\"},{\"start\":\"32678\",\"end\":\"32683\"},{\"start\":\"33056\",\"end\":\"33063\"},{\"start\":\"33067\",\"end\":\"33072\"},{\"start\":\"33478\",\"end\":\"33481\"},{\"start\":\"33485\",\"end\":\"33490\"},{\"start\":\"33494\",\"end\":\"33502\"},{\"start\":\"33506\",\"end\":\"33509\"},{\"start\":\"33513\",\"end\":\"33519\"},{\"start\":\"33523\",\"end\":\"33528\"},{\"start\":\"33958\",\"end\":\"33962\"},{\"start\":\"33966\",\"end\":\"33968\"},{\"start\":\"33972\",\"end\":\"33976\"},{\"start\":\"33980\",\"end\":\"33992\"},{\"start\":\"33996\",\"end\":\"33999\"},{\"start\":\"34003\",\"end\":\"34010\"},{\"start\":\"34014\",\"end\":\"34018\"},{\"start\":\"34319\",\"end\":\"34327\"},{\"start\":\"34333\",\"end\":\"34339\"},{\"start\":\"34343\",\"end\":\"34353\"},{\"start\":\"34357\",\"end\":\"34365\"},{\"start\":\"34685\",\"end\":\"34691\"},{\"start\":\"34695\",\"end\":\"34703\"},{\"start\":\"34707\",\"end\":\"34716\"},{\"start\":\"34720\",\"end\":\"34726\"},{\"start\":\"34730\",\"end\":\"34733\"},{\"start\":\"34737\",\"end\":\"34744\"},{\"start\":\"35129\",\"end\":\"35135\"},{\"start\":\"35139\",\"end\":\"35153\"},{\"start\":\"35357\",\"end\":\"35359\"},{\"start\":\"35363\",\"end\":\"35371\"},{\"start\":\"35375\",\"end\":\"35381\"},{\"start\":\"35385\",\"end\":\"35393\"},{\"start\":\"35653\",\"end\":\"35656\"},{\"start\":\"35660\",\"end\":\"35663\"},{\"start\":\"35667\",\"end\":\"35675\"},{\"start\":\"35971\",\"end\":\"35976\"},{\"start\":\"35980\",\"end\":\"35986\"},{\"start\":\"35990\",\"end\":\"35994\"},{\"start\":\"35998\",\"end\":\"36003\"},{\"start\":\"36009\",\"end\":\"36014\"},{\"start\":\"36451\",\"end\":\"36456\"},{\"start\":\"36460\",\"end\":\"36463\"},{\"start\":\"36467\",\"end\":\"36475\"},{\"start\":\"36479\",\"end\":\"36488\"},{\"start\":\"36492\",\"end\":\"36500\"},{\"start\":\"36504\",\"end\":\"36509\"},{\"start\":\"36513\",\"end\":\"36520\"},{\"start\":\"36524\",\"end\":\"36530\"},{\"start\":\"36534\",\"end\":\"36541\"},{\"start\":\"36545\",\"end\":\"36552\"},{\"start\":\"37121\",\"end\":\"37125\"},{\"start\":\"37129\",\"end\":\"37133\"},{\"start\":\"37137\",\"end\":\"37139\"},{\"start\":\"37143\",\"end\":\"37146\"},{\"start\":\"37150\",\"end\":\"37153\"},{\"start\":\"37488\",\"end\":\"37493\"},{\"start\":\"37497\",\"end\":\"37505\"},{\"start\":\"37509\",\"end\":\"37512\"},{\"start\":\"37516\",\"end\":\"37526\"},{\"start\":\"37530\",\"end\":\"37535\"},{\"start\":\"37539\",\"end\":\"37547\"},{\"start\":\"37551\",\"end\":\"37557\"},{\"start\":\"37561\",\"end\":\"37564\"},{\"start\":\"37568\",\"end\":\"37573\"},{\"start\":\"37577\",\"end\":\"37584\"},{\"start\":\"37912\",\"end\":\"37922\"},{\"start\":\"37926\",\"end\":\"37934\"},{\"start\":\"37938\",\"end\":\"37943\"},{\"start\":\"38275\",\"end\":\"38282\"},{\"start\":\"38286\",\"end\":\"38292\"},{\"start\":\"38296\",\"end\":\"38301\"},{\"start\":\"38305\",\"end\":\"38310\"},{\"start\":\"38540\",\"end\":\"38545\"},{\"start\":\"38549\",\"end\":\"38555\"},{\"start\":\"38559\",\"end\":\"38568\"},{\"start\":\"38572\",\"end\":\"38576\"},{\"start\":\"38580\",\"end\":\"38587\"},{\"start\":\"38591\",\"end\":\"38595\"},{\"start\":\"38599\",\"end\":\"38605\"},{\"start\":\"38609\",\"end\":\"38612\"},{\"start\":\"38616\",\"end\":\"38622\"},{\"start\":\"38626\",\"end\":\"38631\"},{\"start\":\"39086\",\"end\":\"39091\"},{\"start\":\"39095\",\"end\":\"39101\"},{\"start\":\"39313\",\"end\":\"39323\"},{\"start\":\"39327\",\"end\":\"39333\"},{\"start\":\"39631\",\"end\":\"39633\"},{\"start\":\"39637\",\"end\":\"39639\"},{\"start\":\"39643\",\"end\":\"39651\"},{\"start\":\"39655\",\"end\":\"39659\"},{\"start\":\"39845\",\"end\":\"39848\"},{\"start\":\"39852\",\"end\":\"39854\"},{\"start\":\"39858\",\"end\":\"39862\"},{\"start\":\"39868\",\"end\":\"39872\"},{\"start\":\"39876\",\"end\":\"39889\"},{\"start\":\"39893\",\"end\":\"39899\"},{\"start\":\"39905\",\"end\":\"39910\"},{\"start\":\"39920\",\"end\":\"39928\"},{\"start\":\"40243\",\"end\":\"40247\"},{\"start\":\"40251\",\"end\":\"40255\"},{\"start\":\"40259\",\"end\":\"40266\"},{\"start\":\"40270\",\"end\":\"40275\"},{\"start\":\"40279\",\"end\":\"40287\"},{\"start\":\"40546\",\"end\":\"40550\"},{\"start\":\"40554\",\"end\":\"40558\"},{\"start\":\"40562\",\"end\":\"40565\"},{\"start\":\"40569\",\"end\":\"40571\"},{\"start\":\"40575\",\"end\":\"40581\"},{\"start\":\"40585\",\"end\":\"40589\"},{\"start\":\"40593\",\"end\":\"40597\"},{\"start\":\"40601\",\"end\":\"40604\"},{\"start\":\"40608\",\"end\":\"40610\"},{\"start\":\"40614\",\"end\":\"40620\"},{\"start\":\"40907\",\"end\":\"40912\"},{\"start\":\"40916\",\"end\":\"40919\"},{\"start\":\"40923\",\"end\":\"40928\"},{\"start\":\"40932\",\"end\":\"40936\"},{\"start\":\"40940\",\"end\":\"40945\"},{\"start\":\"40949\",\"end\":\"40957\"},{\"start\":\"40961\",\"end\":\"40963\"},{\"start\":\"41315\",\"end\":\"41318\"},{\"start\":\"41322\",\"end\":\"41330\"},{\"start\":\"41334\",\"end\":\"41339\"},{\"start\":\"41345\",\"end\":\"41348\"},{\"start\":\"41352\",\"end\":\"41357\"},{\"start\":\"41361\",\"end\":\"41368\"},{\"start\":\"41372\",\"end\":\"41379\"}]", "bib_entry": "[{\"start\":\"32151\",\"end\":\"32597\",\"attributes\":{\"matched_paper_id\":\"3180429\",\"id\":\"b0\"}},{\"start\":\"32599\",\"end\":\"32985\",\"attributes\":{\"id\":\"b1\",\"doi\":\"arXiv:1709.06158\"}},{\"start\":\"32987\",\"end\":\"33445\",\"attributes\":{\"matched_paper_id\":\"12358833\",\"id\":\"b2\"}},{\"start\":\"33447\",\"end\":\"33901\",\"attributes\":{\"matched_paper_id\":\"35985986\",\"id\":\"b3\"}},{\"start\":\"33903\",\"end\":\"34245\",\"attributes\":{\"matched_paper_id\":\"196834926\",\"id\":\"b4\"}},{\"start\":\"34247\",\"end\":\"34621\",\"attributes\":{\"matched_paper_id\":\"53391180\",\"id\":\"b5\"}},{\"start\":\"34623\",\"end\":\"35125\",\"attributes\":{\"matched_paper_id\":\"4670339\",\"id\":\"b6\"}},{\"start\":\"35127\",\"end\":\"35341\",\"attributes\":{\"id\":\"b7\",\"doi\":\"arXiv:1706.01307\"}},{\"start\":\"35343\",\"end\":\"35592\",\"attributes\":{\"matched_paper_id\":\"54465873\",\"id\":\"b8\"}},{\"start\":\"35594\",\"end\":\"35924\",\"attributes\":{\"matched_paper_id\":\"56171922\",\"id\":\"b9\"}},{\"start\":\"35926\",\"end\":\"36360\",\"attributes\":{\"matched_paper_id\":\"4559916\",\"id\":\"b10\"}},{\"start\":\"36362\",\"end\":\"37045\",\"attributes\":{\"matched_paper_id\":\"3345516\",\"id\":\"b11\"}},{\"start\":\"37047\",\"end\":\"37429\",\"attributes\":{\"matched_paper_id\":\"3030826\",\"id\":\"b12\"}},{\"start\":\"37431\",\"end\":\"37829\",\"attributes\":{\"id\":\"b13\",\"doi\":\"arXiv:1712.05474\"}},{\"start\":\"37831\",\"end\":\"38208\",\"attributes\":{\"matched_paper_id\":\"738850\",\"id\":\"b14\"}},{\"start\":\"38210\",\"end\":\"38490\",\"attributes\":{\"id\":\"b15\",\"doi\":\"arXiv:1905.04835\"}},{\"start\":\"38492\",\"end\":\"39016\",\"attributes\":{\"matched_paper_id\":\"91184540\",\"id\":\"b16\"}},{\"start\":\"39018\",\"end\":\"39253\",\"attributes\":{\"matched_paper_id\":\"945193\",\"id\":\"b17\"}},{\"start\":\"39255\",\"end\":\"39555\",\"attributes\":{\"matched_paper_id\":\"6925519\",\"id\":\"b18\"}},{\"start\":\"39557\",\"end\":\"39841\",\"attributes\":{\"id\":\"b19\",\"doi\":\"arXiv:1801.02209\"}},{\"start\":\"39843\",\"end\":\"40192\",\"attributes\":{\"id\":\"b20\"}},{\"start\":\"40194\",\"end\":\"40464\",\"attributes\":{\"id\":\"b21\",\"doi\":\"arXiv:1810.06543\"}},{\"start\":\"40466\",\"end\":\"40840\",\"attributes\":{\"matched_paper_id\":\"325460\",\"id\":\"b22\"}},{\"start\":\"40842\",\"end\":\"41227\",\"attributes\":{\"matched_paper_id\":\"190000813\",\"id\":\"b23\"}},{\"start\":\"41229\",\"end\":\"41721\",\"attributes\":{\"matched_paper_id\":\"2305273\",\"id\":\"b24\"}}]", "bib_title": "[{\"start\":\"32151\",\"end\":\"32181\"},{\"start\":\"32987\",\"end\":\"33052\"},{\"start\":\"33447\",\"end\":\"33474\"},{\"start\":\"33903\",\"end\":\"33954\"},{\"start\":\"34247\",\"end\":\"34315\"},{\"start\":\"34623\",\"end\":\"34681\"},{\"start\":\"35343\",\"end\":\"35353\"},{\"start\":\"35594\",\"end\":\"35649\"},{\"start\":\"35926\",\"end\":\"35965\"},{\"start\":\"36362\",\"end\":\"36447\"},{\"start\":\"37047\",\"end\":\"37117\"},{\"start\":\"37831\",\"end\":\"37908\"},{\"start\":\"38492\",\"end\":\"38536\"},{\"start\":\"39018\",\"end\":\"39082\"},{\"start\":\"39255\",\"end\":\"39309\"},{\"start\":\"40466\",\"end\":\"40542\"},{\"start\":\"40842\",\"end\":\"40903\"},{\"start\":\"41229\",\"end\":\"41311\"}]", "bib_author": "[{\"start\":\"32183\",\"end\":\"32192\"},{\"start\":\"32192\",\"end\":\"32203\"},{\"start\":\"32203\",\"end\":\"32209\"},{\"start\":\"32209\",\"end\":\"32221\"},{\"start\":\"32221\",\"end\":\"32230\"},{\"start\":\"32230\",\"end\":\"32250\"},{\"start\":\"32250\",\"end\":\"32260\"},{\"start\":\"32599\",\"end\":\"32608\"},{\"start\":\"32608\",\"end\":\"32615\"},{\"start\":\"32615\",\"end\":\"32629\"},{\"start\":\"32629\",\"end\":\"32639\"},{\"start\":\"32639\",\"end\":\"32651\"},{\"start\":\"32651\",\"end\":\"32660\"},{\"start\":\"32660\",\"end\":\"32668\"},{\"start\":\"32668\",\"end\":\"32676\"},{\"start\":\"32676\",\"end\":\"32685\"},{\"start\":\"33054\",\"end\":\"33065\"},{\"start\":\"33065\",\"end\":\"33074\"},{\"start\":\"33476\",\"end\":\"33483\"},{\"start\":\"33483\",\"end\":\"33492\"},{\"start\":\"33492\",\"end\":\"33504\"},{\"start\":\"33504\",\"end\":\"33511\"},{\"start\":\"33511\",\"end\":\"33521\"},{\"start\":\"33521\",\"end\":\"33530\"},{\"start\":\"33956\",\"end\":\"33964\"},{\"start\":\"33964\",\"end\":\"33970\"},{\"start\":\"33970\",\"end\":\"33978\"},{\"start\":\"33978\",\"end\":\"33994\"},{\"start\":\"33994\",\"end\":\"34001\"},{\"start\":\"34001\",\"end\":\"34012\"},{\"start\":\"34012\",\"end\":\"34020\"},{\"start\":\"34317\",\"end\":\"34329\"},{\"start\":\"34329\",\"end\":\"34341\"},{\"start\":\"34341\",\"end\":\"34355\"},{\"start\":\"34355\",\"end\":\"34367\"},{\"start\":\"34683\",\"end\":\"34693\"},{\"start\":\"34693\",\"end\":\"34705\"},{\"start\":\"34705\",\"end\":\"34718\"},{\"start\":\"34718\",\"end\":\"34728\"},{\"start\":\"34728\",\"end\":\"34735\"},{\"start\":\"34735\",\"end\":\"34746\"},{\"start\":\"35127\",\"end\":\"35137\"},{\"start\":\"35137\",\"end\":\"35155\"},{\"start\":\"35355\",\"end\":\"35361\"},{\"start\":\"35361\",\"end\":\"35373\"},{\"start\":\"35373\",\"end\":\"35383\"},{\"start\":\"35383\",\"end\":\"35395\"},{\"start\":\"35651\",\"end\":\"35658\"},{\"start\":\"35658\",\"end\":\"35665\"},{\"start\":\"35665\",\"end\":\"35677\"},{\"start\":\"35967\",\"end\":\"35978\"},{\"start\":\"35978\",\"end\":\"35988\"},{\"start\":\"35988\",\"end\":\"35996\"},{\"start\":\"35996\",\"end\":\"36005\"},{\"start\":\"36005\",\"end\":\"36016\"},{\"start\":\"36449\",\"end\":\"36458\"},{\"start\":\"36458\",\"end\":\"36465\"},{\"start\":\"36465\",\"end\":\"36477\"},{\"start\":\"36477\",\"end\":\"36490\"},{\"start\":\"36490\",\"end\":\"36502\"},{\"start\":\"36502\",\"end\":\"36511\"},{\"start\":\"36511\",\"end\":\"36522\"},{\"start\":\"36522\",\"end\":\"36532\"},{\"start\":\"36532\",\"end\":\"36543\"},{\"start\":\"36543\",\"end\":\"36554\"},{\"start\":\"37119\",\"end\":\"37127\"},{\"start\":\"37127\",\"end\":\"37135\"},{\"start\":\"37135\",\"end\":\"37141\"},{\"start\":\"37141\",\"end\":\"37148\"},{\"start\":\"37148\",\"end\":\"37155\"},{\"start\":\"37486\",\"end\":\"37495\"},{\"start\":\"37495\",\"end\":\"37507\"},{\"start\":\"37507\",\"end\":\"37514\"},{\"start\":\"37514\",\"end\":\"37528\"},{\"start\":\"37528\",\"end\":\"37537\"},{\"start\":\"37537\",\"end\":\"37549\"},{\"start\":\"37549\",\"end\":\"37559\"},{\"start\":\"37559\",\"end\":\"37566\"},{\"start\":\"37566\",\"end\":\"37575\"},{\"start\":\"37575\",\"end\":\"37586\"},{\"start\":\"37910\",\"end\":\"37924\"},{\"start\":\"37924\",\"end\":\"37936\"},{\"start\":\"37936\",\"end\":\"37945\"},{\"start\":\"38271\",\"end\":\"38284\"},{\"start\":\"38284\",\"end\":\"38294\"},{\"start\":\"38294\",\"end\":\"38303\"},{\"start\":\"38303\",\"end\":\"38312\"},{\"start\":\"38538\",\"end\":\"38547\"},{\"start\":\"38547\",\"end\":\"38557\"},{\"start\":\"38557\",\"end\":\"38570\"},{\"start\":\"38570\",\"end\":\"38578\"},{\"start\":\"38578\",\"end\":\"38589\"},{\"start\":\"38589\",\"end\":\"38597\"},{\"start\":\"38597\",\"end\":\"38607\"},{\"start\":\"38607\",\"end\":\"38614\"},{\"start\":\"38614\",\"end\":\"38624\"},{\"start\":\"38624\",\"end\":\"38633\"},{\"start\":\"39084\",\"end\":\"39093\"},{\"start\":\"39093\",\"end\":\"39103\"},{\"start\":\"39311\",\"end\":\"39325\"},{\"start\":\"39325\",\"end\":\"39335\"},{\"start\":\"39629\",\"end\":\"39635\"},{\"start\":\"39635\",\"end\":\"39641\"},{\"start\":\"39641\",\"end\":\"39653\"},{\"start\":\"39653\",\"end\":\"39661\"},{\"start\":\"39843\",\"end\":\"39850\"},{\"start\":\"39850\",\"end\":\"39856\"},{\"start\":\"39856\",\"end\":\"39864\"},{\"start\":\"39864\",\"end\":\"39874\"},{\"start\":\"39874\",\"end\":\"39891\"},{\"start\":\"39891\",\"end\":\"39901\"},{\"start\":\"39901\",\"end\":\"39912\"},{\"start\":\"39912\",\"end\":\"39930\"},{\"start\":\"40241\",\"end\":\"40249\"},{\"start\":\"40249\",\"end\":\"40257\"},{\"start\":\"40257\",\"end\":\"40268\"},{\"start\":\"40268\",\"end\":\"40277\"},{\"start\":\"40277\",\"end\":\"40289\"},{\"start\":\"40544\",\"end\":\"40552\"},{\"start\":\"40552\",\"end\":\"40560\"},{\"start\":\"40560\",\"end\":\"40567\"},{\"start\":\"40567\",\"end\":\"40573\"},{\"start\":\"40573\",\"end\":\"40583\"},{\"start\":\"40583\",\"end\":\"40591\"},{\"start\":\"40591\",\"end\":\"40599\"},{\"start\":\"40599\",\"end\":\"40606\"},{\"start\":\"40606\",\"end\":\"40612\"},{\"start\":\"40612\",\"end\":\"40622\"},{\"start\":\"40905\",\"end\":\"40914\"},{\"start\":\"40914\",\"end\":\"40921\"},{\"start\":\"40921\",\"end\":\"40930\"},{\"start\":\"40930\",\"end\":\"40938\"},{\"start\":\"40938\",\"end\":\"40947\"},{\"start\":\"40947\",\"end\":\"40959\"},{\"start\":\"40959\",\"end\":\"40965\"},{\"start\":\"41313\",\"end\":\"41320\"},{\"start\":\"41320\",\"end\":\"41332\"},{\"start\":\"41332\",\"end\":\"41341\"},{\"start\":\"41341\",\"end\":\"41350\"},{\"start\":\"41350\",\"end\":\"41359\"},{\"start\":\"41359\",\"end\":\"41370\"},{\"start\":\"41370\",\"end\":\"41381\"}]", "bib_venue": "[{\"start\":\"32260\",\"end\":\"32327\"},{\"start\":\"32701\",\"end\":\"32762\"},{\"start\":\"33074\",\"end\":\"33163\"},{\"start\":\"33530\",\"end\":\"33617\"},{\"start\":\"34020\",\"end\":\"34054\"},{\"start\":\"34367\",\"end\":\"34416\"},{\"start\":\"34746\",\"end\":\"34823\"},{\"start\":\"35171\",\"end\":\"35212\"},{\"start\":\"35395\",\"end\":\"35454\"},{\"start\":\"35677\",\"end\":\"35746\"},{\"start\":\"36016\",\"end\":\"36093\"},{\"start\":\"36554\",\"end\":\"36640\"},{\"start\":\"37155\",\"end\":\"37224\"},{\"start\":\"37431\",\"end\":\"37484\"},{\"start\":\"37945\",\"end\":\"38003\"},{\"start\":\"38210\",\"end\":\"38269\"},{\"start\":\"38633\",\"end\":\"38700\"},{\"start\":\"39103\",\"end\":\"39120\"},{\"start\":\"39335\",\"end\":\"39384\"},{\"start\":\"39557\",\"end\":\"39627\"},{\"start\":\"39930\",\"end\":\"40004\"},{\"start\":\"40194\",\"end\":\"40239\"},{\"start\":\"40622\",\"end\":\"40631\"},{\"start\":\"40965\",\"end\":\"40988\"},{\"start\":\"41381\",\"end\":\"41449\"},{\"start\":\"32329\",\"end\":\"32381\"},{\"start\":\"33165\",\"end\":\"33239\"},{\"start\":\"33619\",\"end\":\"33691\"},{\"start\":\"34825\",\"end\":\"34887\"},{\"start\":\"36095\",\"end\":\"36157\"},{\"start\":\"36642\",\"end\":\"36713\"},{\"start\":\"38702\",\"end\":\"38754\"}]"}}}, "year": 2023, "month": 12, "day": 17}