{"id": 52239205, "updated": "2023-11-13 14:45:27.139", "metadata": {"title": "Believe it or not: Designing a Human-AI Partnership for Mixed-Initiative Fact-Checking", "authors": "[{\"first\":\"An\",\"last\":\"Nguyen\",\"middle\":[\"T.\"]},{\"first\":\"Aditya\",\"last\":\"Kharosekar\",\"middle\":[]},{\"first\":\"Saumyaa\",\"last\":\"Krishnan\",\"middle\":[]},{\"first\":\"Siddhesh\",\"last\":\"Krishnan\",\"middle\":[]},{\"first\":\"Elizabeth\",\"last\":\"Tate\",\"middle\":[]},{\"first\":\"Byron\",\"last\":\"Wallace\",\"middle\":[\"C.\"]},{\"first\":\"Matthew\",\"last\":\"Lease\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Fact-checking, the task of assessing the veracity of claims, is an important, timely, and challenging problem. While many automated fact-checking systems have been recently proposed, the human side of the partnership has been largely neglected: how might people understand, interact with, and establish trust with an AI fact-checking system? Does such a system actually help people better assess the factuality of claims? In this paper, we present the design and evaluation of a mixed-initiative approach to fact-checking, blending human knowledge and experience with the efficiency and scalability of automated information retrieval and ML. In a user study in which participants used our system to aid their own assessment of claims, our results suggest that individuals tend to trust the system: participant accuracy assessing claims improved when exposed to correct model predictions. However, this trust perhaps goes too far: when the model was wrong, exposure to its predictions often degraded human accuracy. Participants given the option to interact with these incorrect predictions were often able improve their own performance. This suggests that transparent models are key to facilitating effective human interaction with fallible AI models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2895993994", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/uist/NguyenKKKTWL18", "doi": "10.1145/3242587.3242666"}}, "content": {"source": {"pdf_hash": "c622f760d05eeb45f5a28ec2cc1e2c8af95cc2e3", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7b1dafb971180d82b4fc2decd18031cc24316e0d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c622f760d05eeb45f5a28ec2cc1e2c8af95cc2e3.txt", "contents": "\nBelieve it or not: Designing a Human-AI Partnership for Mixed-Initiative Fact-Checking\n\n\nAn T Nguyen \nUniversity of Texas at Austin\n\n\nAditya Kharosekar \nUniversity of Texas at Austin\n\n\nSaumyaa Krishnan \nUniversity of Texas at Austin\n\n\nSiddhesh Krishnan \nElizabeth Tate \nUniversity of Texas at Austin\n\n\nByron C Wallace \nUniversity of Texas at Austin\n\n\nNortheastern University\n\n\nMatthew Lease \nUniversity of Texas at Austin\n\n\nBelieve it or not: Designing a Human-AI Partnership for Mixed-Initiative Fact-Checking\n10.1145/3242587.3242666Author Keywords AIMixed-initiativeFact-checkingInformation Literacy\nFact-checking, the task of assessing the veracity of claims, is an important, timely, and challenging problem. While many automated fact-checking systems have been recently proposed, the human side of the partnership has been largely neglected: how might people understand, interact with, and establish trust with an AI fact-checking system? Does such a system actually help people better assess the factuality of claims? In this paper, we present the design and evaluation of a mixed-initiative approach to fact-checking, blending human knowledge and experience with the efficiency and scalability of automated information retrieval and ML. In a user study in which participants used our system to aid their own assessment of claims, our results suggest that individuals tend to trust the system: participant accuracy assessing claims improved when exposed to correct model predictions. However, this trust perhaps goes too far: when the model was wrong, exposure to its predictions often degraded human accuracy. Participants given the option to interact with these incorrect predictions were often able improve their own performance. This suggests that transparent models are key to facilitating effective human interaction with fallible AI models.\n\nINTRODUCTION\n\nIn designating October 2009 as National Information Literacy 1 Awareness Month, former U.S. President Barack Obama drew national attention to a key 21st century information challenge:\n\nThough we may know how to find the information we need, we must also know how to evaluate it. Over the past decade, we have seen a crisis of authenticity emerge. We now live in a world where anyone can publish an opinion or perspective, whether true or not, and have that opinion amplified within the information marketplace.\n\nHistorically, we have relied upon information literacy education in our schools and libraries to teach our citizenry key 1 https://en.wikipedia.org/wiki/Information_literacy Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\nUIST '18, October 14-17, 2018, Berlin, Germany critical reading skills, the importance of consulting multiple, independent sources, and to investigate the potential for underlying bias in whatever we read. For this reason, information literacy has been advocated as \"a distinct skill set and a necessary key to one's social and economic well-being in an increasingly complex information society.\" [26] However, the internet era presents new challenges to consumers of information. As information generation has accelerated, making sense of it has become increasingly difficult. While breaking down traditional barriers to authorship has democratized information exchange and dissemination, the massive growth of information production by less-established sources has created significant new challenges for readers in accurately interpreting and assessing the veracity of this barrage of content [47,10]. The deluge of new online articles and sources means that is practically difficult for individuals to consistently manually cross-check sources. The rise of misinformation -unwitting or deliberate -has made it harder still for readers to tell fact from fiction.\n\nIn response, researchers in machine learning (ML) and natural language processing (NLP) have developed a variety of innovative new models that automatically fact-check claims [34,31,40]. However, these works have largely viewed fact-checking as a standard ML task in which the aim first and foremost is to achieve high predictive accuracy. While improving predictive accuracy is a laudable goal, we believe that consideration of the human element is equally important if such models are to be useful in practice.\n\nWe argue that fact-checking models must provide three key properties for practical use: (i) model transparency, (ii) support for integrating user knowledge; and (iii) quantification and communication of model uncertainty. Regarding (i), high predictive accuracy alone is insufficient; someone skeptical of online information is likely to be equally skeptical of any fact-checking tool. Indeed, many people distrust popular factchecking services [4]. Consequently, a system must be transparent (and auditable) in how it arrived at its prediction so that a user can understand and trust the model. Concerning (ii), claim assessments will invariably rely at least partially on world-views (priors) pertaining to the perceived a priori credibility of claims and sources; a fact-checking system should enable explicit, individual specification of these, in turn providing a framework for users to easily inject their own views and knowledge into the system and realizing an integrated prediction that incorporates these. Finally, addressing (iii), system predictions ought to be presented as relative statements with respect to incomplete model specification and knowledge, rather than definitive judgments: an ML model should communicate its confidence in its predictions while accounting for potential sources of errors, empowering users to conduct their own in-depth inspection and reasoning.\n\nIn this work we present a mixed-initiative [19] approach that realizes the three properties above. We position automatic fact-checking as an assistive technology to augment human decision making. To intuitively complement and reinforce the information literacy skills that users bring to the partnership, we have designed the system to follow the same key steps advocated by information literacy education in order to estimate claim veracity: 1) find relevant articles (textual evidence); 2) assess each article's reliability and its relative support for the claim in question; and 3) assess claim validity based on this body of textual evidence. Moreover, by making the model's reasoning process transparent to the user and interactive, our interface has further potential to help teach and structure the user's own information literacy skills regarding the logical process to follow for assessing claim validity.\n\nOur mixed-initiative approach to fact-checking blends human knowledge and experience with the efficiency and scalability of automated information retrieval and AI. Given a claim in natural language, the system first automatically finds and retrieves relevant articles from a variety of sources. It then infers the degree to which each article supports or refutes the claim, as well as the reputation of each source. Finally, the system aggregates this body of evidence to predict the veracity of the claim. Regarding user interaction, the (automatically inferred) source reputation and stance of each retrieved article can be changed via simple sliders to reflect user beliefs and/or to correct erroneousness model estimates. This, in turn, instantly updates the system's overall veracity prediction.\n\nTo evaluate our approach, we conduct three randomized experiments, designed to measure the participants' performance in predicting the veracity of given claims. The first compares users who perform the task with and without seeing ML predictions. Our results suggest that users tend to trust the model, even when it is wrong. The second compares a static interface to an interactive one in which users can fix or override model predictions. We found that users are generally able to do so, although this is less helpful when the model makes correct predictions. The last experiment compares a gamified task design to a non-gamified one, but we found no significant differences in performance and participation.\n\nContributions. We provide: (1) a novel mix-initiative approach to fact-checking, combining human and machine intelligence; and (2) a user study of our approach, revealing the practical promise and challenges of this human-AI partnership. To foster future work by others, we share our anonymized data, source code for significance testing, and an interactive demo 2 .\n\n\nRELATED WORK\n\n\nInformation Credibility\n\nSomeone skeptical of online information is likely to be equally skeptical of any fact-checking website or software. For exam-2 https://github.com/thanhan/uist18 ple, many people are reported to distrust popular fact-checking services [4]. Just as we consider information credibility factors [16] in assessing a news article or website, therefore, we must also consider such factors in designing a website or web application to support fact-checking.\n\nSome established best practices include: websites should be clearly organized and navigable, professional looking, well written, updated, and functional [16,18]. More specific to this domain, sites and tools should be explicit about any potential sources of bias, e.g., by providing an \"about\" page to provide context, indicating any paid sponsors, discussing or posting an ethical code, and admitting when a mistake has been made. In relation to asking users to perform a potentially complicated task (e.g., consulting various uncertain evidence to fact-check a questionable claim) which heightens user uncertainty, a clean and usable website is even more important to further reduce cognitive load [43]. We have sought to adhere to the above best practices in designing our prototype web application.\n\n\nHuman Fact-checking and Information Literacy\n\nWebsites such as Snopes and PolitiFact have become increasingly important in providing expert fact-checking of popular claims. However, the reliance on human labor, particularly experts, does not scale to lets users check arbitrary claims.\n\nCrowdsourcing-based fact-checking sites, such as TruthSetter 3 , now also provide more scalable, peer-based assessment. Our design is inspired in part by recent research on crowdsourced fact-checking [41,42] which suggests that . . . the best remedy for propaganda and misinformation intended to manipulate public opinion is helping readers engage in critical thinking and evidence-based reasoning. . . [which] can have benefits well beyond identifying specific instances of \"fake news\" -it can teach users the critical thinking skills needed to detect and evaluate misinformation and fake news . . .\n\nIn that work [41,42], users can post claims, and other users can then share related sources, stances, and claims. We similarly structure the claim evaluation process through evidence collection and assessment. Whereas they develop a social, volunteer crowdsourced solution, we propose mixed-initiative approach between a single user and a machine learning system. Future work might usefully further integrate information literacy education with fact-checking of claims.\n\nAnother intriguing approach to teaching information literacy turns the process on its head, engaging people in gamified generation of fake news stories. By learning how to write fake news, participants were subsequently found to be less likely to believe or be persuaded by actual fake news articles [37].\n\n\nAI-based Fact-checking\n\nMany recent studies have explored the potential of AI for automated fact-checking [50,49,13,34,32,45,24]. These studies have primarily focused on model variants and techniques that increase the predictive performance of models (e.g., accuracy in predicting veracity). Hybrid work combining ML with crowdsourcing [24,32] has similarly focused on predictive accuracy, without considering information literacy educational objectives, or exposure consequences, for the crowd [41,48]. While some models do generate explanations for their predictions [34,32], it remains unclear how users might interpret and interact with these predictions. Our work in this area is distinguished by our human-centered approach: our mixedinitiative design emphasizes the human-AI partnership, and our evaluation measures how use of such predictive systems impacts human inference in assessing claim veracity.\n\nRecently, [22] present a visual analytic system for users to detect social media accounts that distribute misinformation. This is complementary to our goal of detecting false claims. Furthermore, although their system is inspired by a prediction model prediction [49], the users can not directly interact with that model to correct and override when its prediction is wrong.\n\n\nDesigning Human-AI Interfaces\n\nAs AI has been embedded within an increasing number of human facing applications, there has been a concomitant growth in interest in designing interfaces for humans to interpret and interact with these systems and predictions [1,8]. In response, researchers have proposed a variety of novel interfaces for interacting with machine learning models, but these tend to require significant expertise on behalf of users [2,25]. Others have developed interaction techniques tailored to specific tasks, such as image segmentation [11], image search [15], text classification [28], topic models [44], code search [36], and others. The fact-checking task is especially challenging, as the the system needs to present convincing evidence for its predictions, assuming users are (understandably) skeptical.\n\nBeyond making ML more interpretable for people, there is also increasing appreciation for the greater potential capabilities that may be possible with hybrid AI-human collaboration. Jordan and Mitchel [21] recently opined about the kind of strengths each side of such a partnership, noting the potential to harness ML's ability to extract subtle statistical patterns from large datasets in concert with human skills in pulling these into plausible narratives informed by diverse perspectives. Such partnerships are now materializing even in creative endeavors, such as AI-human co-design of fashion [23,46], creative writing [6,35], and music composition [14].\n\nTo build such effective partnerships, Horvitz's suggestions for mixed-initiative design [19] remain relevant today, e.g.:\n\nWe can enhance the value of automation by giving agents the ability to gracefully degrade the precision of service to match current uncertainty. . . We should design agents with the assumption that users may often wish to complete or refine an analysis provided by an agent.\n\nOur system openly conveys its fallibility by showing the confidence of each prediction to the user. By clearly communicating rather than hiding this uncertainty, the system discourages users having over-confidence in the model or making poor decisions based on such a misunderstanding. Instead, the system presents the evidence it has in favor of its disposition, and leaves it to the user to consider this evidence in the context of their own prior knowledge and experience. We further provide an interaction mechanism by which users can inject their own beliefs into the system to refine its predictions.\n\nWithout users having a mental model of how the system combines evidence to predict claim veracity (i.e., model transparency), such interaction would not be possible. The importance of model transparency has been similarly reported in other studies, such as Kulesza et al. [27]'s report of a case study in which users responded positively to greater model intelligibility; as users learned more about the system through interaction, they became more satisfied with system output. As such, the study demonstrated that users valued going beyond \"black box\" ML and were willing and able to learn more about a system in order to use it more effectively.\n\nAnother area in which human-AI partnerships are being explored is interactive machine learning (IML) [1]. For example, recent work has investigated design and evaluation of IML systems with non-expert users [44]. Allowing users to alter system inputs and observe how outputs change in response is one technique for realizing model explainability [1]. Visual analytics [39] models similarly facilitate human decision making via model interaction. Following this principle, our model also offers fast incremental updates to predictions, enabling lay users to easily alter inputs via sliders and see immediate model updates of estimated claim veracity.\n\nUnfortunately, adoption of best practices for usability in ML system design is not yet as widespread as one might hope [1]:\n\n. . . machine-learning systems also often inherently violate many existing interface design principles. For example, research has shown that traditional interfaces that support understandability . . . and actionability . . . are generally more usable than interfaces that do not . . . Many machine-learning systems violate both principles: they are inherently difficult for users to understand fully and they largely limit the control given to the end user.\n\nThis motivates greater collaboration between HCI and AI researchers, with both fields and their research products standing to benefit. HCI researchers have similarly endorsed the value for HCI to better understand and engage with AI [9,17].\n\n\nUSER EXPERIENCE\n\nThis section describes the user experience we seek to cultivate with our mixed-initiative design. The main goal for our user interface is to realize transparency, in that users can understand how the system makes its predictions and thereby know when (and when not) to trust them. Below we discuss how we present our model outputs to users, including its final disposition regarding claim veracities and intermediate estimates of each article stance and source reliability.\n\nUser interactions with our system proceed as follows. The user first enters a claim (or selects an example claim, e.g., \"Saudi Arabia has a new law that can force women to cover up their tempting eyes\"). A list of articles relevant to this claim is then retrieved (along with the source of each article, i.e., the website where the article was published). Based on these articles, a prediction is made and presented to the user regarding the correctness of the claim; for example the model may be 80% confident that the claim is true. Figure 1 shows a screenshot of the main results page for the example claim above. We display the claim and the system prediction regarding its correctness at the top. The interface emphasizes the textual evidence and reasoning underlying the overall claim estimate. In particular, we present a table of retrieved articles relevant to the claim, including the sources, headlines, and two predictions: the reputation of each source and the stance of each article headline. Each prediction is shown as a slider: reputation ranges between {low, unknown, high}, while stance ranges from {deny, neutral, support}.\n\nA key feature of our interface is that users can change (override) the reputations and stances (inferred by the model) by moving the sliders. They can then observe how the prediction regarding overall claim veracity is affected. For example, for a headline associated with an article published by a reputable source, changing its stance from support to deny will tend to increase the chance that the claim is false. This interactive feature is beneficial in three ways. 1) It increases transparency: users can see how model predictions about each relevant article contribute toward the overall veracity prediction for the claim. 2) Users can access a more personalized prediction, for example by lowering the reputations of sources they believe are not credible. 3) They can correct the system's incorrect predictions, e.g., by changing predicted stances.\n\nTo further aid transparency, each headline is linked to the original article, allowing users to easily browse each article to see the textual evidence and assess the model's stance prediction.\n\nIn addition, we generate a page for each source, and each occurrence of the source on a claim page is linked back to the source's generated page. When a user sees, for a given claim, that a relevant article comes from a particular source with a given prediction reputation, the user can consult the generated source page to learn more about the source and its predicted reputation. The source page lists all articles from that source in our training data, the claim associated with each article, the journalist-annotated article stance, and the journalist annotation for the claim's veracity. As before, users may click on each article to verify the stance label for themselves. By consulting the source page and seeing the various information about it, the user can thereby better understand the model's predicted reputation for the source: the more false claims supported by a source, the lower its reputation.\n\n\nPREDICTION MODELS\n\nIn this section, we describe the predictive models underlying the above user experience. Our system's automated predictions are based on two machine learning classifiers: one for article stance, and one for claim veracity [34,32]. Both classifiers are trained on the Emergent dataset [13], which contains 300 claims with 2595 relevant articles (on average there are 8.65 articles per claim). Each claim is labeled as false, true, or unknown. Article headlines are labeled by journalists as being either supporting, neutral, or refuting the claim. There is a case of Ebola in Kansas City False 3\n\nThe police officer leading the Charlie Hebdo investigation committed suicide False 4\n\nA 5-year-old boy was invoiced for missing a birthday party True 5\n\nThe Indian government fired an employee who hadn't been to work in 24 years True The stance classifier accepts the claim and an article headline to predict the stance of the headline with respect to the claim. The veracity classifier operates over the outputs of the stance classifier for all of the relevant articles (and corresponding sources), yielding a prediction concerning the veracity of the claim. This veracity classifier explicitly models the reputation of each source. Sources that support true claims and deny false claims (in the training dataset) are given higher weights (i.e., more trusted). Both classifiers (stance and veracity) have an average accuracy of approximately 70%.\n\nFor inference and learning, we first train the stance classifier, then use its outputs to train the veracity classifier. For the stance classifier, we use the same text features as in [13], including bag-of-words (common n-grams), dependency parse, and paraphrase alignment (word embeddings are not used due to the reported negative impact). Each logistic regression classifer is implemented using Scikit-learn [33] with L1 regularization, Liblinear solver [12] and default parameters. While prior work has used joint training of the two classifiers in a graphical model framework [34,32], we favor simplicity and speed to facilitate real-time user interaction.\n\nThe underlying model architecture of predicting article stances and using these predictions to estimate veracity is designed to improve transparency. While others have considered alternative architectures, for example deep neural networks [50], these can achieve good predictive performance but are less transparent, and so less amenable to interaction and supporting decision making. In contrast, we prioritize transparency over raw predictive performance. In particular, we rely on linear models in which individual terms have well-defined semantics, and we adopt a Bayesian view so that users may express subjective beliefs as priors imposed oer these variables. We operationalize this via a graphical user interface design.\n\n\nUSER STUDY\n\nWe conduct three experiments with participants from Amazon Mechanical Turk (MTurk). We required participants to have completed 1000 approved tasks with at least 95% approval rate. Participants were allowed to partake in only one of our experiments. The task takes less than 10 minutes on average and we paid $1.25, for roughly $7.50 per hour. While we did not collect participant demographics, because we post our task in small batches throughout the day, our participant demographics likely follow the general MTurk demographics reported in prior work [7,38,20]: mostly from the US or India, balanced gender, and younger than the working population.\n\nIn all experiments, participants predict the correctness of five randomly selected claims ( Table 1) from the Emergent [13] test set. Note that our machine learning models have not seen any of these claims or the associated relevant articles.\n\nAn additional (sixth) claim served as an attention check (AC) [30]: a headline instructing participants to select a particular answer (e.g., \"If you read this headline please select neutral\"). While this AC was designed to filter out participants who did not pay attention to the task, we observed that many participants failing this check appeared to have honestly completed the task (e.g. many have accurate answers and helpful comments). We thus decided not to filter out any participants.\n\n\nExperiment 1\n\nThis experiment tests whether system predictions help humans predict claim veracity more accurately.\n\nProcedure: Participants are randomly assigned to one of two groups, Control and System. In both groups, users are first shown a screenshot of our results page (similar to Figure 1), but without the claim veracity prediction. In group Control, users are shown only the sources and headlines of relevant articles. In group System, they are also shown the source reputations and predicted article stances inferred by our system. The task for both groups is to evaluate the claim correctness, using a a Likert scale: Definitely false, Probably false, Neutral, Probably true, and Definitely true. After making this assessment, participants in both groups are shown the model's prediction concerning claim correctness and given the option to change their assessment. After completing this exercise for all claims, participants complete a short survey.\n\n\nResults:\n\nWe collected results for 113 participants (58 assigned to Control, 55 to System). We measure error by calculating the distance from the participants' responses to the correct answers. For example, for a (definitely) false claim, an assessment of Probably false corresponds to an error of 1, and Definitely true corresponds to an error of 4.\n\nIn Figure 2, we plot average errors over participants for each of the five claims, with the standard deviation displayed to characterize variance. On the left (sub- figure a), we show the error before the participants see the system's prediction. Firstly, we observe mixed differences across the two groups and five claims. Participants in group System were seen to show higher average prediction error on claims 1 and 2 and lower error on claim 4, while claims 3 and 5 show only small differences between the two groups.  Secondly, we observe variable accuracy of the automatic stance classifier. On claims 1 and 2, the stance classifier predicts the wrong answers for 80% and 90% of the relevant articles. Its error is 30% on claim 3 and 0% on claims 4 and 5. While average stance classifier accuracy is over 70%, this accuracy distributes unevenly across claims, with very low accuracy on two out of our five randomly selected claims.\n\nFor example, in inspecting the stance classifier on claim 1, we found a probable reason for its weak performance on this claim. This is an incorrect statement about Tiger Woods, a claim that is denied by many of the relevant articles. However, the stance classifier incorrectly predicts these articles' stances as being supportive. In the training set there are many articles about Tiger Woods that support a different (unrelated) claim. The classifier has incorrectly learned that the bi-gram 'Tiger Woods' indicates that the article is supportive. While a more sophisticated classifier might avoid this particular error, in general we should expect our AI systems to be imperfect.\n\nThirdly, we note the pattern of human error appears to roughly follow the system's stance classifier errors. It seems that when the stance classifier is wrong, participants are often misled by it, but when it is correct, it improves their predictions. This may suggest participants were overly trusting of our AI. While this is seemingly at odds with prior findings of users not trusting popular fact-checking services [4], it may stem from differences in participant demographics, participant incentives, or other factors of experimental design. For example, users in our study need to make predictions, instead of just saying whether they trust the fact-checking results. We return to this issue later in discussing study limitations.\n\nIn Figure 2 (sub-figure b), we also plot the change in human error vs. claim after the participants saw our claim correctness prediction (a positive change means that human error increases). We observe that there are larger changes for errors made by participants in group Control: those who have not seen our stance predictions change their answers more than those who have. These response changes increase the error for some claims (e.g. claim 3), and decrease it for others (e.g. claim 4). This roughly corresponds to the errors by the veracity classifier, showing again that system predictions can both help users (when correct) or lead users to errors that reflect model fallibility or biases implicit in training data.\n\nTo quantify our results, we fit two Generalized Linear Models (GLMs): one for the data before participants see our system's correctness prediction, one after. We modeled human error as an ordinal response predicted by claim and participant as random effects, and the number of correct/wrong stance predictions by our system as fixed effects. Specifically, we use the clmm function of the R package ordinal [5] with the formula:\nHuman.Error \u223c 1+CSP+WSP+(1|Claim)+(1|Participant)\nwhere 'CSP' is the number of correct stance predictions that the human participant sees. It is 0 for all group Control participants who did not see any stance predictions. For group System, it is equal to the number of correct stance predictions for the claim. For example in claim 1, where the stance classifier is correct for 2 out of 10 articles, 'CSP' is 2 and 'WSP' Session 4: Crowds and Human-AI Partnership UIST 2018, October 14-17, 2018, Berlin, Germany is 8 (WSP is similarly defined as the number of wrong stance predictions the participant sees). Also in the formula, the notation '(1|Claim)' means that Claim is a random effect and an intercept is estimated for each claim. For the data before seeing the correctness predictions, the results for the fixed effects are: Coefficient Estimate SE p-value (two-tailed) CSP -0.053 0.029 0.064 WSP 0.076 0.031 0.014 these suggest that seeing correct stance predictions (CSP) decreases human error while seeing wrong predictions (WSP) increases human error by a larger amount. Although the pvalue for CSP is slightly larger than the 0.05 significant level, we consider that a solid evidence (the p-value is two-tail and includes the unlikely possibility that seeing correct stance predictions increases human error). After seeing the correctness prediction, the results are:\n\nCoefficient Estimate SE p-value (two-tailed) CSP -0.016 0.029 0.523 WSP 0.063 0.031 0.040\n\nWe can observe that seeing correct stances is now not as helpful because the participants can see the correctness prediction: in claim 4 and 5, many participants are able to lower their errors (Figure 2b). But seeing wrong stances is still harmful because these wrong stances cause the correctness classifier to make predictions with high errors (claim 1 and 2).\n\n\nExperiment 2\n\nThis experiments assesses whether participants are able to interact with the system to inject their own knowledge, fix model predictions, and improve their own predictions.\n\nProcedure: Whereas participants in Experiment 1 were shown only static screenshots, participants in Experiment 2 use our interactive interface. Participants were randomly assigned to two groups, Control and Slider. In the interface for group Control, all predictions (reputations, stances, and claim correctness) are fixed. However, those in the latter group (Slider) could change the (initially inferred) reputations and predicted stances, using the sliders, and observe how the prediction regarding overall claim correctness changes in response.\n\nTo encourage more attentive responses, we designed the task in this experiment as a simple game in which participants predict the correctness of a given claim (false or true) and indicate their confidence (0%, 5%, ..., 100%) in this response. Participants win points for correct answers, and lose points for incorrect answers. The number of points won or lost is proportional to their stated confidence. Participants may win up to 20 points on a given question; these are scaled linearly with the given confidence. For instance, a correct answer associated with a 75% confidence wins 20x75%=15 points (Figure 3). After a participant submits their prediction for a claim, we reveal the correct answer and the number of points that he or she has won or lost (for example \"the correct answer is false, you have won 15 points\"). The participant can then move to the next claim. After finishing the task for our selected claims, the participants have the option to continue working on other claims in the datasets, but this is not required. We hypothesized that participants who could see the consequence of their predictions in winning or losing game points may be more engaged and therefore make better predictions.\n\nResults: We collected results for 109 participants (51 assigned to Control, 58 to Slider). In Figure 4, we show the distribution of the participants' points as a boxplot, for each claim and condition. For claims 1-3, where the system is less accurate, participants in the Slider group (i.e., those able to use the slider-change feature) earn more points on average (assess veracity more accurately and/or confidently). For claims 4 and 5, where the system prediction is already accurate, boxplots reveal the Slider group participants have lower first quartiles, although the medians are still roughly the same. This suggests that some group-Slider participants are negatively impacted by the slider interface.\n\nWithin the 58 group-Slider participants, the sliders are used by 79% of the participants on claim 1, 59% on claim 2, and roughly 45% on claims 3, 4, 5. This decrease in use may be due to the variable error of the automatic stance classifier, or may reflect a familiarization effect as participants become accustomed to using the sliders.\n\nWe find no evidence of any difference in points between those who used the sliders and those who did not (p-value > 0.5 for all tests). This seems contradictory for claim 2, where we observe that group-Slider performs better than group-Control by a reasonable amount. If there is no difference between those who use the sliders and those did not, why was group Slider better for that claim? One possible explanation is that in claim 1, the Slider-group participants have learned to question the model (and not to blindly trust its predictions) so that they made better predictions on claim 2 (compared to group-Control), even for those who did not use sliders on claim 2. Although we have informed the participants (in both groups Control and Slider) in our task instruction that the model is only 70% correct, being able to experience the uncertainty in the model's predictions may help participants see the true quality of the model (by using the sliders and seeing how the predictions change).\n\nOur GLM model for this experiment simply use the group assignment as the fixed effect:\nPoints \u223c 1 + Group + (1|Claim) + (1|Participant)\nwhere 'Points' is treated as a continuous response. Using the function lmer in the package lme4 [3]  We also fit the same model after removing the outliers (the diamonds in Figure 4) and find roughly the same estimate, with the p-value drops slightly to 0.122. The results suggests that participants in group Slider have higher points, although this is not significant at the 0.05 level. We believe that this relatively weak statistical signal is due to several reasons. First, there are large variances in human performance across claims and participants. Second, as discussed above, the sliders are not helpful (and sometimes even harmful) when the model is already correct. Third, the sliders were not used by as many participants as we expected. Finally, although the sliders are intended to be intuitive, they may require familiarization and a learning curve, and ultimately be found clearer to some users than others.\n\n\nExperiment 3\n\nThis experiment investigates the degree to which 'gamified' aspects of Experiment 2 impact participant performance.\n\nProcedure: We compare performance of two groups (to which participants were again randomly assigned). Both groups assess claim correctness and indicate confidence in this assessment. In the first group (Control) participants perform the task without the game (and so are not shown a number of 'points' they currently have or will get). In the second group (Game) participants complete the 'gamified' variant of the task.\n\nResults: Across 106 participants (56 assigned to condition E, 50 to F), we find no significant differences between two groups in the number of points or number of extra tasks done (p-value > 0.5 for both). Consequently, we find that the game design did not impact participant performance (wrt. predictive performance or engagement) as hypothesized.\n\n\nSurvey Responses\n\nIn our post-task surveys across experiments, participants were generally positive about our tool, for example:\n\n\"I thought it was really cool! I'd enjoy playing with this more if it wasn't during my work time.\"\n\nSome participants expressed concern about the amount of information, the slider-change feature, and the correctness 'true' judgment (by the Emergent journalists).\n\n\"there's a lot of data in this hit but not enough money to make it worth exploring\" \"Do not give me the option to tweak the deny, neutral, support rating as it led to some confusion regarding the task, however I was able to understand it once very quickly with practice.\" \"It was very hard to understand. It seems on one task, I was 100% sure it was true and I was told it was false, I even followed links to verify the sources.\"\n\nThis first participant appears to be overwhelmed with the complexity of the fact-checking work, which is understandable. The second is not receptive to our slider-change feature, but also acknowledges that it became useful after some practice. The last participant seems to refer to claim 3, which no articles deny, but which the Emergent's journalists deemed false due to the reported event being exaggerated. This shows that fact-checking can be very difficult, and that many subtleties are lost by dichotomizing claims as either true or false.\n\n\nDISCUSSION\n\nWe have presented a prototype system that enables users to interact with ML predictions for the challenging task of factchecking (assessing claim validity). We designed the interface In some variants we allowed users to optionally override these predictions with their own beliefs or inferences. While our findings show that this human-AI interaction can be effective, they also suggest that caution should be exercised. Fallible ML models may make seriously wrong predictions (due to spurious correlations gleaned from potentially imperfect training data) that can in turn mislead users in some cases.\n\nLimitations. This study presents what we believe to be intriguing results, but we note several important limitations. Firstly, our results in Experiment 2 are not significant at the 0.05 level, as we discuss, and should be interpreted with caution. Secondly, we have relied on MTurk participants, and different participant demographics or incentives may influence findings. For example, international MTurk workers may not be most representative of American news consumers or the most familiar and interested in American news. Thirdly, alternative plausible interpretations of the results exist. For instance, since MTurk workers are paid per task (rather than hourly), some workers may echo model predictions not due to trust but rather simple expediency of work. We acknowledge that possibility, though inspection of the data suggests such behavior only forms a small minority of what we observe.\n\nIt is also important to recognize that our work has the potential for negative impact as well. When the model makes errors, people may not recognize them, or could be even more confused by the introduction of AI modeling into an already confusing landscape of questionable sources and facts. While sliders support human-AI joint reasoning and allow users to correct modeling errors, they also create a new opportunity for self-constructed echo chambers, where correct model outputs can be manipulated at the whim of user bias. Were such \"corrections\" shared, one can easily imagine an adversarial setting where groups with competing interests seek to manipulate fact checking tools alongside their existing processes. Future work could consider designs to help users further recognize system limitations, interpret predictions with more caution, and explore the limits of their own knowledge and biases. For example, an interface could state the model's assumptions more clearly and ask users to confirm their understanding before they can see the predictions. Exploration of adversarial settings is also important to enable effective collaboration.\n\nOur experiments include only claims for which we have elected to trust a \"reference\" veracity designation (by the Emergent journalists), and each has here been associated with a fixed set of relevant articles. As highlighted by the recent growth of work on algorithmic bias, our system learns only from the data it is given, with its accuracy and bias ultimately determined by that of the underlying data. An interesting direction for future work is to design for users to check their own claims, using relevant articles they find, and interact with other users' predictions.\n\nIn light of deeply divided views in political discourse and an increasingly ill-defined notion of 'truth', an assistive tool such as the one we present offers intriguing potential for brokering a more rational process in formulating individual beliefs and structuring debate among disagreeing parties [41]. If we can agree on the basic information literacy process to follow in assessing claims, and if we provide a structured process by which differing viewpoints can be precisely articulated and injected as prior knowledge into an automated system's reasoning process, perhaps we can create a space for more reasoned discourse. Instead of simply debating claims, two people with disagreeing views might sit down together and employ such a tool as a technological mediator. By alternatively injecting one another's viewpoints and beliefs as prior knowledge into the tool's reasoning process, we might come to more clearly understand the key evidence on which our beliefs disagree, and in so doing, gain additional insights into both our own beliefs as well of those who disagree with us.\n\n\n\u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-5948-1/18/10. . . $15.00 DOI: https://doi.org/10.1145/3242587.3242666\n\nFigure 1 :\n1Top: the main results page, which includes the claim, its predicted correctness, and a table of relevant articles, their sources and inferred reputations and stances. Bottom left: sources link to pages that allow users to see the articles in our training set, which enables them to see why it has a particular predicted reputation. Bottom right: each headline links to the original article.\n\n\n) BEFORE seeing the system's correctness prediction.(b) AFTER seeing the system's correctness prediction.\n\nFigure 2 :\n2Experiment 1. Control group participants see a list of relevant articles with sources, while those in the System group see the same list along with predicted stances and reputations. Left: the errors of the stance classifier and of the participants before they see the overall veracity prediction. Right: the errors of the claim veracity classifier and the change in the participants' errors after being shown these predictions (a positive change means that human error increases).\n\nFigure 3 :\n3Top: In our gamified task interface, participants enter their prediction on the correctness of the claim and their confidence on the prediction. They win or lose points based on whether their prediction matches the correct answer. The number of points is 20 x confidence (20x75%=15). Bottom: after submitting, participants will see the correct answer.\n\nFigure 4 :\n4Experiment 2: boxplots of points earned for each claim in the two groups. In group Control participants cannot change the predicted stances and reputations; in group Slider they can. The diamonds indicate detected outliers (more than 1.5 Inter-Quartile-Range from the first or third quartile).\n\nTable 1 :\n1Five claims that we randomly selected for our user study from the test set of the Emergent dataset (excluding the attention check claim). Each claim is linked to the original Emergent's webpage showing the relevant articles and their stances.\n\n\nSession 4: Crowds and Human-AI Partnership UIST 2018, October 14-17, 2018, Berlin, Germany so that users can know where the predictions are coming from.\nhttps://truthsetter.com Session 4: Crowds and Human-AI Partnership UIST 2018, October 14-17, 2018, Berlin, Germany\nACKNOWLEDGMENTSWe thank Brent Biglin, Shravan Ravi, ChiaHui Liu, and Jyothi Vinjumur for their valuable contributions. The anonymous reviewers also provided terrific feedback that greatly improved this work. Last but not least, we thank the many talented Mechanical Turk workers for their participation in our study.\nPower to the people: The role of humans in interactive machine learning. Saleema Amershi, Maya Cakmak, William Bradley Knox, Todd Kulesza, AI Magazine. 35Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014. Power to the people: The role of humans in interactive machine learning. AI Magazine 35, 4 (2014), 105-120.\n\nModeltracker: Redesigning performance analysis tools for machine learning. Saleema Amershi, Max Chickering, M Steven, Bongshin Drucker, Patrice Lee, Jina Simard, Suh, Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. the 33rd Annual ACM Conference on Human Factors in Computing SystemsACMSaleema Amershi, Max Chickering, Steven M Drucker, Bongshin Lee, Patrice Simard, and Jina Suh. 2015. Modeltracker: Redesigning performance analysis tools for machine learning. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 337-346.\n\nSteven Walker, and others. 2014. lme4: Linear mixed-effects models using Eigen and S4. R package version. Douglas Bates, Martin Maechler, Ben Bolker, 1Douglas Bates, Martin Maechler, Ben Bolker, Steven Walker, and others. 2014. lme4: Linear mixed-effects models using Eigen and S4. R package version 1, 7 (2014), 1-23.\n\nTrust and Distrust in Online Fact-checking Services. Petter Bae Brandtzaeg, Asbj\u00f8rn F\u00f8lstad, Commun. ACM. 60Petter Bae Brandtzaeg and Asbj\u00f8rn F\u00f8lstad. 2017. Trust and Distrust in Online Fact-checking Services. Commun. ACM 60, 9 (Aug. 2017), 65-71.\n\nR Package \"ordinal. Rune Haubo Bojesen Christensen, Rune Haubo Bojesen Christensen. 2018. R Package \"ordinal\". (2018).\n\nCreative Writing with a Machine in the Loop: Case Studies on Slogans and Stories. Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, Noah A Smith, 23rd International Conference on Intelligent User Interfaces. ACMElizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A Smith. 2018. Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories. In 23rd International Conference on Intelligent User Interfaces. ACM, 329-340.\n\nDemographics and dynamics of mechanical turk workers. Djellel Difallah, Elena Filatova, Panos Ipeirotis, Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. the Eleventh ACM International Conference on Web Search and Data MiningACMDjellel Difallah, Elena Filatova, and Panos Ipeirotis. 2018. Demographics and dynamics of mechanical turk workers. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 135-143.\n\nFinale Doshi, - Velez, Been Kim, Towards A Rigorous Science of Interpretable Machine Learning. Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. arXiv (2017). https://arxiv.org/abs/1702.08608\n\nSession 4: Crowds and Human-AI Partnership UIST 2018. Berlin, GermanySession 4: Crowds and Human-AI Partnership UIST 2018, October 14-17, 2018, Berlin, Germany\n\nUX Design Innovation: Challenges for Working with Machine Learning As a Design Material. Graham Dove, Kim Halskov, Jodi Forlizzi, John Zimmerman, Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. the 2017 CHI Conference on Human Factors in Computing SystemsGraham Dove, Kim Halskov, Jodi Forlizzi, and John Zimmerman. 2017. UX Design Innovation: Challenges for Working with Machine Learning As a Design Material. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. 278-288.\n\nHighlighting disputed claims on the web. Rob Ennals, Beth Trushkowsky, John Mark Agosta, Proceedings of the 19th international conference on World wide web. the 19th international conference on World wide webACMRob Ennals, Beth Trushkowsky, and John Mark Agosta. 2010. Highlighting disputed claims on the web. In Proceedings of the 19th international conference on World wide web. ACM, 341-350.\n\nInteractive machine learning. Jerry , Alan Fails, Dan R OlsenJr, Proceedings of the 8th international conference on Intelligent user interfaces. the 8th international conference on Intelligent user interfacesACMJerry Alan Fails and Dan R Olsen Jr. 2003. Interactive machine learning. In Proceedings of the 8th international conference on Intelligent user interfaces. ACM, 39-45.\n\nLIBLINEAR: A library for large linear classification. Kai-Wei Rong-En Fan, Cho-Jui Chang, Xiang-Rui Hsieh, Chih-Jen Wang, Lin, Journal of machine learning research. 9Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of machine learning research 9, Aug (2008), 1871-1874.\n\nEmergent: a novel data-set for stance classification. William Ferreira, Andreas Vlachos, North American Chapter of the Association for Computational Linguistics. ACL. William Ferreira and Andreas Vlachos. 2016. Emergent: a novel data-set for stance classification. In North American Chapter of the Association for Computational Linguistics. ACL.\n\nReal-time Human Interaction with Supervised Learning Algorithms for Music Composition and Performance. Rebecca Anne Fiebrink, Princeton, NJ, USA. Advisor(s) Cook, Perry R. AAI3445567Ph.D. DissertationRebecca Anne Fiebrink. 2011. Real-time Human Interaction with Supervised Learning Algorithms for Music Composition and Performance. Ph.D. Dissertation. Princeton, NJ, USA. Advisor(s) Cook, Perry R. AAI3445567.\n\nCueFlik: interactive concept learning in image search. James Fogarty, Desney Tan, Ashish Kapoor, Simon Winder, Proceedings of the sigchi conference on human factors in computing systems. the sigchi conference on human factors in computing systemsACMJames Fogarty, Desney Tan, Ashish Kapoor, and Simon Winder. 2008. CueFlik: interactive concept learning in image search. In Proceedings of the sigchi conference on human factors in computing systems. ACM, 29-38.\n\nWhat makes Web sites credible?: a report on a large quantitative study. Jonathan Bj Fogg, Othman Marshall, Alex Laraki, Chris Osipovich, Nicholas Varma, Jyoti Fang, Akshay Paul, John Rangnekar, Shon, Proceedings of the SIGCHI conference on Human factors in computing systems. the SIGCHI conference on Human factors in computing systemsACMPreeti Swani, and othersBJ Fogg, Jonathan Marshall, Othman Laraki, Alex Osipovich, Chris Varma, Nicholas Fang, Jyoti Paul, Akshay Rangnekar, John Shon, Preeti Swani, and others. 2001. What makes Web sites credible?: a report on a large quantitative study. In Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 61-68.\n\nHuman-centered machine learning. Marco Gillies, Rebecca Fiebrink, Atau Tanaka, J\u00e9r\u00e9mie Garcia, Frederic Bevilacqua, Alexis Heloir, Fabrizio Nunnari, Wendy Mackay, Saleema Amershi, Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems. the 2016 CHI Conference Extended Abstracts on Human Factors in Computing SystemsACMBongshin Lee, and othersMarco Gillies, Rebecca Fiebrink, Atau Tanaka, J\u00e9r\u00e9mie Garcia, Frederic Bevilacqua, Alexis Heloir, Fabrizio Nunnari, Wendy Mackay, Saleema Amershi, Bongshin Lee, and others. 2016. Human-centered machine learning. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems. ACM, 3558-3565.\n\nAurora Harley, Trustworthiness in Web Design: 4 Credibility Factors. Utg. av Nielsen Norman group. Aurora Harley. 2016. Trustworthiness in Web Design: 4 Credibility Factors. Utg. av Nielsen Norman group. url: https://www. nngroup. com/articles/trustworthy-design (2016).\n\nPrinciples of mixed-initiative user interfaces. Eric Horvitz, Proceedings of the SIGCHI conference on Human Factors in Computing Systems. the SIGCHI conference on Human Factors in Computing SystemsACMEric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems. ACM, 159-166.\n\nDemographics of mechanical turk. Panagiotis G Ipeirotis, No. CEDER-10-01NYU Working PaperPanagiotis G Ipeirotis. 2010. Demographics of mechanical turk. (2010). NYU Working Paper No. CEDER-10-01. https://ssrn.com/abstract=1585030.\n\nMachine learning: Trends, perspectives, and prospects. I Michael, Tom M Jordan, Mitchell, Science. 349Michael I Jordan and Tom M Mitchell. 2015. Machine learning: Trends, perspectives, and prospects. Science 349, 6245 (2015), 255-260.\n\nCan You Verifi This? Studying Uncertainty and Decision-Making About Misinformation using Visual Analytics. Alireza Karduni, Ryan Wesslen, Sashank Santhanam, Isaac Cho, Svitlana Volkova, Dustin Arendt, Samira Shaikh, Wenwen Dou, 12th International AAAI Conference on Web and Social Media. Alireza Karduni, Ryan Wesslen, Sashank Santhanam, Isaac Cho, Svitlana Volkova, Dustin Arendt, Samira Shaikh, and Wenwen Dou. 2018. Can You Verifi This? Studying Uncertainty and Decision-Making About Misinformation using Visual Analytics. In 12th International AAAI Conference on Web and Social Media (ICWSM 2018).\n\nDeepWear: A Case Study of Collaborative Design Between Human and Artificial Intelligence. Natsumi Kato, Hiroyuki Osone, Daitetsu Sato, Naoya Muramatsu, Yoichi Ochiai, 10.1145/3173225.3173302Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction (TEI '18). the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction (TEI '18)New York, NY, USAACMNatsumi Kato, Hiroyuki Osone, Daitetsu Sato, Naoya Muramatsu, and Yoichi Ochiai. 2018. DeepWear: A Case Study of Collaborative Design Between Human and Artificial Intelligence. In Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction (TEI '18). ACM, New York, NY, USA, 529-536. DOI: http://dx.doi.org/10.1145/3173225.3173302\n\nLeveraging the Crowd to Detect and Reduce the Spread of Fake News and Misinformation. J Kim, B Tabibian, A Oh, B Schoelkopf, M Gomez-Rodriguez, WSDM '18: Proceedings of the 11th ACM International Conference on Web Search and Data Mining. J. Kim, B. Tabibian, A. Oh, B. Schoelkopf, and M. Gomez-Rodriguez. 2018. Leveraging the Crowd to Detect and Reduce the Spread of Fake News and Misinformation. In WSDM '18: Proceedings of the 11th ACM International Conference on Web Search and Data Mining.\n\nInteracting with predictions: Visual inspection of black-box machine learning models. Josua Krause, Adam Perer, Kenney Ng, Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. the 2016 CHI Conference on Human Factors in Computing SystemsACMJosua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with predictions: Visual inspection of black-box machine learning models. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 5686-5697.\n\nInformation Skills for an Information Society: A Review of Research. An ERIC Information Analysis Product. Carol Collier Kuhlthau, ERICCarol Collier Kuhlthau. 1987. Information Skills for an Information Society: A Review of Research. An ERIC Information Analysis Product. ERIC.\n\nTell me more?: the effects of mental model soundness on personalizing an intelligent agent. Todd Kulesza, Simone Stumpf, Margaret Burnett, Irwin Kwan, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsACMTodd Kulesza, Simone Stumpf, Margaret Burnett, and Irwin Kwan. 2012. Tell me more?: the effects of mental model soundness on personalizing an intelligent agent. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1-10.\n\nWhy-oriented end-user debugging of naive Bayes text classification. Todd Kulesza, Simone Stumpf, Weng-Keen Wong, Margaret M Burnett, Stephen Perona, Andrew Ko, Ian Oberst, ACM Transactions on Interactive Intelligent Systems (TiiS). 12Todd Kulesza, Simone Stumpf, Weng-Keen Wong, Margaret M Burnett, Stephen Perona, Andrew Ko, and Ian Oberst. 2011. Why-oriented end-user debugging of naive Bayes text classification. ACM Transactions on Interactive Intelligent Systems (TiiS) 1, 1 (2011), 2.\n\nlmerTest package: tests in linear mixed effects models. Alexandra Kuznetsova, B Per, Rune Haubo Bojesen Brockhoff, Christensen, Journal of Statistical Software. 8213Alexandra Kuznetsova, Per B Brockhoff, and Rune Haubo Bojesen Christensen. 2017. lmerTest package: tests in linear mixed effects models. Journal of Statistical Software 82, 13 (2017).\n\nExperiences surveying the crowd: Reflections on methods, participation, and reliability. C Catherine, Marshall, M Frank, Shipman, Proceedings of the 5th Annual ACM Web Science Conference. the 5th Annual ACM Web Science ConferenceACMCatherine C Marshall and Frank M Shipman. 2013. Experiences surveying the crowd: Reflections on methods, participation, and reliability. In Proceedings of the 5th Annual ACM Web Science Conference. ACM, 234-243.\n\nLanguage-Aware Truth Assessment of Fact Candidates. Ndapandula Nakashole, M Tom, Mitchell, Ndapandula Nakashole and Tom M Mitchell. 2014. Language-Aware Truth Assessment of Fact Candidates..\n\n. Acl In, In ACL (1). 1009-1019.\n\nSession 4: Crowds and Human-AI Partnership UIST 2018. Berlin, GermanySession 4: Crowds and Human-AI Partnership UIST 2018, October 14-17, 2018, Berlin, Germany\n\nAn Interpretable Joint Graphical Model for Fact-Checking from Crowds. An T Nguyen, Aditya Kharosekar, Matthew Lease, Byron C Wallace, AAAI. An T. Nguyen, Aditya Kharosekar, Matthew Lease, and Byron C. Wallace. 2018. An Interpretable Joint Graphical Model for Fact-Checking from Crowds. In AAAI.\n\nScikit-learn: Machine learning in Python. Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Journal of machine learning research. 12Vincent Dubourg, and othersFabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, and others. 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research 12, Oct (2011), 2825-2830.\n\nWhere the truth lies: Explaining the credibility of emerging claims on the web and social media. Kashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6tgen, Gerhard Weikum, Proceedings of the 26th Intl. the 26th IntlKashyap Popat, Subhabrata Mukherjee, Jannik Str\u00f6tgen, and Gerhard Weikum. 2017. Where the truth lies: Explaining the credibility of emerging claims on the web and social media. In Proceedings of the 26th Intl. Conference on World Wide Web. 1003-1012.\n\nAutomated Assistance for Creative Writing with an RNN Language Model. Melissa Roemmele, Andrew S Gordon, Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion. the 23rd International Conference on Intelligent User Interfaces CompanionACM21Melissa Roemmele and Andrew S Gordon. 2018. Automated Assistance for Creative Writing with an RNN Language Model. In Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion. ACM, 21.\n\nCodeMend: Assisting Interactive Programming with Bimodal Embedding. Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, Eytan Adar, Proceedings of the 29th Annual Symposium on User Interface Software and Technology. the 29th Annual Symposium on User Interface Software and TechnologyACMXin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, and Eytan Adar. 2016. CodeMend: Assisting Interactive Programming with Bimodal Embedding. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 247-258.\n\nThe fake news game: actively inoculating against the risk of misinformation. Jon Roozenbeek, Sander Van Der Linden, Journal of Risk Research. Jon Roozenbeek and Sander van der Linden. 2018. The fake news game: actively inoculating against the risk of misinformation. Journal of Risk Research (2018), 1-11.\n\nWho are the crowdworkers?: shifting demographics in mechanical turk. Joel Ross, Lilly Irani, M Silberman, Andrew Zaldivar, Bill Tomlinson, CHI'10 extended abstracts on Human factors in computing systems. ACMJoel Ross, Lilly Irani, M Silberman, Andrew Zaldivar, and Bill Tomlinson. 2010. Who are the crowdworkers?: shifting demographics in mechanical turk. In CHI'10 extended abstracts on Human factors in computing systems. ACM, 2863-2872.\n\nHuman-centered machine learning through interactive visualization. Dominik Sacha, Michael Sedlmair, Leishi Zhang, John Aldo Lee, Daniel Weiskopf, Stephen North, Daniel Keim, European Symposium on Artificial Neural Networks. ESANNDominik Sacha, Michael Sedlmair, Leishi Zhang, John Aldo Lee, Daniel Weiskopf, Stephen North, and Daniel Keim. 2016. Human-centered machine learning through interactive visualization. European Symposium on Artificial Neural Networks (ESANN).\n\nClaimEval: Integrated and Flexible Framework for Claim Evaluation Using Credibility of Sources. Mehdi Samadi, Partha Talukdar, Manuela Veloso, Manuel Blum, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. the Thirtieth AAAI Conference on Artificial IntelligenceMehdi Samadi, Partha Talukdar, Manuela Veloso, and Manuel Blum. 2016. ClaimEval: Integrated and Flexible Framework for Claim Evaluation Using Credibility of Sources. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. 222-228.\n\nCrowdsourcing the verification of fake news and alternative facts. J Ricky, Sethi, Proceedings of the 28th ACM Conference on Hypertext and Social Media. the 28th ACM Conference on Hypertext and Social MediaACMRicky J Sethi. 2017a. Crowdsourcing the verification of fake news and alternative facts. In Proceedings of the 28th ACM Conference on Hypertext and Social Media. ACM, 315-316.\n\nHow citizen investigators can collaborate on crowdsourced fact-checking. Ricky J Sethi, The Conversation. Ricky J. Sethi. 2017b. How citizen investigators can collaborate on crowdsourced fact-checking. (2017). The Conversation, November 5th.\n\nTrust in Web GIS: the role of the trustee attributes in the design of trustworthy Web GIS applications. Artemis Skarlatidou, Muki Haklay, Tao Cheng, Intl. Journal of Geographical Info. Science. 25Artemis Skarlatidou, Muki Haklay, and Tao Cheng. 2011. Trust in Web GIS: the role of the trustee attributes in the design of trustworthy Web GIS applications. Intl. Journal of Geographical Info. Science 25, 12 (2011), 1913-1930.\n\nClosing the Loop: User-Centered Design and Evaluation of a Human-in-the-Loop Topic Modeling System. Alison Smith, Varun Kumar, Jordan Boyd-Graber, Kevin Seppi, Leah Findlater, 23rd International Conference on Intelligent User Interfaces. ACMAlison Smith, Varun Kumar, Jordan Boyd-Graber, Kevin Seppi, and Leah Findlater. 2018. Closing the Loop: User-Centered Design and Evaluation of a Human-in-the-Loop Topic Modeling System. In 23rd International Conference on Intelligent User Interfaces. ACM, 293-304.\n\nFake News Detection in Social Networks via Crowd Signals. Sebastian Tschiatschek, Adish Singla, Manuel Gomez Rodriguez, Arpit Merchant, Andreas Krause, Companion Proceedings of The Web Conference. 517-524. Alternate Track on Journalism, Misinformation, and Fact-checking. Sebastian Tschiatschek, Adish Singla, Manuel Gomez Rodriguez, Arpit Merchant, and Andreas Krause. 2018. Fake News Detection in Social Networks via Crowd Signals. In Companion Proceedings of The Web Conference. 517-524. Alternate Track on Journalism, Misinformation, and Fact-checking.\n\nThe Elements of Fashion Style. Kristen Vaccaro, Sunaya Shivakumar, Ziqiao Ding, Karrie Karahalios, Ranjitha Kumar, Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16. the 29th Annual Symposium on User Interface Software and Technology (UIST '16Kristen Vaccaro, Sunaya Shivakumar, Ziqiao Ding, Karrie Karahalios, and Ranjitha Kumar. 2016. The Elements of Fashion Style. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). 777-785.\n\nWeb 2.0 meets information fluency: Evaluating blogs. Joyce Valenza, Joyce Valenza. 2010. Web 2.0 meets information fluency: Evaluating blogs. http://21cif.com/rkitp/assessment/ v1n5/valenza1.5_blogeval.html. (2010).\n\nThe Rise of Guardians: Fact-checking URL Recommendation to Combat Fake News. Nguyen Vo, Kyumin Lee, The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. Nguyen Vo and Kyumin Lee. 2018. The Rise of Guardians: Fact-checking URL Recommendation to Combat Fake News. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 275-284.\n\nSeparating facts from fiction: Linguistic models to classify suspicious and trusted news posts on twitter. Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, Nathan Hodas, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics2Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, and Nathan Hodas. 2017. Separating facts from fiction: Linguistic models to classify suspicious and trusted news posts on twitter. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Vol. 2. 647-653.\n\nLiar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection. William Yang, Wang , Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics2William Yang Wang. 2017. \" Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Vol. 2. 422-426.\n\nSession 4: Crowds and Human-AI Partnership UIST 2018. Berlin, GermanySession 4: Crowds and Human-AI Partnership UIST 2018, October 14-17, 2018, Berlin, Germany\n", "annotations": {"author": "[{\"end\":134,\"start\":90},{\"end\":185,\"start\":135},{\"end\":235,\"start\":186},{\"end\":254,\"start\":236},{\"end\":302,\"start\":255},{\"end\":377,\"start\":303},{\"end\":424,\"start\":378}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":95},{\"end\":152,\"start\":142},{\"end\":202,\"start\":194},{\"end\":253,\"start\":245},{\"end\":269,\"start\":265},{\"end\":318,\"start\":311},{\"end\":391,\"start\":386}]", "author_first_name": "[{\"end\":92,\"start\":90},{\"end\":94,\"start\":93},{\"end\":141,\"start\":135},{\"end\":193,\"start\":186},{\"end\":244,\"start\":236},{\"end\":264,\"start\":255},{\"end\":308,\"start\":303},{\"end\":310,\"start\":309},{\"end\":385,\"start\":378}]", "author_affiliation": "[{\"end\":133,\"start\":103},{\"end\":184,\"start\":154},{\"end\":234,\"start\":204},{\"end\":301,\"start\":271},{\"end\":350,\"start\":320},{\"end\":376,\"start\":352},{\"end\":423,\"start\":393}]", "title": "[{\"end\":87,\"start\":1},{\"end\":511,\"start\":425}]", "venue": null, "abstract": "[{\"end\":1854,\"start\":603}]", "bib_ref": "[{\"end\":3172,\"start\":3148},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3544,\"start\":3540},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4042,\"start\":4038},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4045,\"start\":4042},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4488,\"start\":4484},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4491,\"start\":4488},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4494,\"start\":4491},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5271,\"start\":5268},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6262,\"start\":6258},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9291,\"start\":9288},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9349,\"start\":9345},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9662,\"start\":9658},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9665,\"start\":9662},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10209,\"start\":10205},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10801,\"start\":10797},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10804,\"start\":10801},{\"end\":11007,\"start\":11000},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11216,\"start\":11212},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11219,\"start\":11216},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11974,\"start\":11970},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12088,\"start\":12084},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12091,\"start\":12088},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12094,\"start\":12091},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12097,\"start\":12094},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12100,\"start\":12097},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12103,\"start\":12100},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12106,\"start\":12103},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12318,\"start\":12314},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12321,\"start\":12318},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12477,\"start\":12473},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12480,\"start\":12477},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12551,\"start\":12547},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12554,\"start\":12551},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12904,\"start\":12900},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13157,\"start\":13153},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13527,\"start\":13524},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13529,\"start\":13527},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13716,\"start\":13713},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13719,\"start\":13716},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13825,\"start\":13821},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13844,\"start\":13840},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13870,\"start\":13866},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13889,\"start\":13885},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13907,\"start\":13903},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14300,\"start\":14296},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14698,\"start\":14694},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14701,\"start\":14698},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14723,\"start\":14720},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14726,\"start\":14723},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14754,\"start\":14750},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14849,\"start\":14845},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16040,\"start\":16036},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16518,\"start\":16515},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16625,\"start\":16621},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16763,\"start\":16760},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16786,\"start\":16782},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17187,\"start\":17184},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17885,\"start\":17882},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17888,\"start\":17885},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21739,\"start\":21735},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21742,\"start\":21739},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21801,\"start\":21797},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23146,\"start\":23142},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23373,\"start\":23369},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23419,\"start\":23415},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23543,\"start\":23539},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23546,\"start\":23543},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23864,\"start\":23860},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24919,\"start\":24916},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24922,\"start\":24919},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24925,\"start\":24922},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25138,\"start\":25134},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25325,\"start\":25321},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29115,\"start\":29112},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30566,\"start\":30563},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":36655,\"start\":36652},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":43309,\"start\":43305}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44263,\"start\":44094},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44667,\"start\":44264},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44775,\"start\":44668},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45270,\"start\":44776},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45635,\"start\":45271},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45942,\"start\":45636},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46197,\"start\":45943},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":46352,\"start\":46198}]", "paragraph": "[{\"end\":2053,\"start\":1870},{\"end\":2380,\"start\":2055},{\"end\":3141,\"start\":2382},{\"end\":4307,\"start\":3143},{\"end\":4821,\"start\":4309},{\"end\":6213,\"start\":4823},{\"end\":7129,\"start\":6215},{\"end\":7931,\"start\":7131},{\"end\":8643,\"start\":7933},{\"end\":9011,\"start\":8645},{\"end\":9503,\"start\":9054},{\"end\":10307,\"start\":9505},{\"end\":10595,\"start\":10356},{\"end\":11197,\"start\":10597},{\"end\":11668,\"start\":11199},{\"end\":11975,\"start\":11670},{\"end\":12888,\"start\":12002},{\"end\":13264,\"start\":12890},{\"end\":14093,\"start\":13298},{\"end\":14755,\"start\":14095},{\"end\":14878,\"start\":14757},{\"end\":15154,\"start\":14880},{\"end\":15762,\"start\":15156},{\"end\":16412,\"start\":15764},{\"end\":17063,\"start\":16414},{\"end\":17188,\"start\":17065},{\"end\":17647,\"start\":17190},{\"end\":17889,\"start\":17649},{\"end\":18382,\"start\":17909},{\"end\":19526,\"start\":18384},{\"end\":20383,\"start\":19528},{\"end\":20577,\"start\":20385},{\"end\":21491,\"start\":20579},{\"end\":22107,\"start\":21513},{\"end\":22193,\"start\":22109},{\"end\":22260,\"start\":22195},{\"end\":22956,\"start\":22262},{\"end\":23619,\"start\":22958},{\"end\":24348,\"start\":23621},{\"end\":25013,\"start\":24363},{\"end\":25257,\"start\":25015},{\"end\":25751,\"start\":25259},{\"end\":25868,\"start\":25768},{\"end\":26715,\"start\":25870},{\"end\":27068,\"start\":26728},{\"end\":28007,\"start\":27070},{\"end\":28691,\"start\":28009},{\"end\":29429,\"start\":28693},{\"end\":30155,\"start\":29431},{\"end\":30584,\"start\":30157},{\"end\":31963,\"start\":30635},{\"end\":32054,\"start\":31965},{\"end\":32418,\"start\":32056},{\"end\":32607,\"start\":32435},{\"end\":33156,\"start\":32609},{\"end\":34370,\"start\":33158},{\"end\":35081,\"start\":34372},{\"end\":35420,\"start\":35083},{\"end\":36418,\"start\":35422},{\"end\":36506,\"start\":36420},{\"end\":37479,\"start\":36556},{\"end\":37611,\"start\":37496},{\"end\":38033,\"start\":37613},{\"end\":38383,\"start\":38035},{\"end\":38514,\"start\":38404},{\"end\":38614,\"start\":38516},{\"end\":38778,\"start\":38616},{\"end\":39209,\"start\":38780},{\"end\":39757,\"start\":39211},{\"end\":40374,\"start\":39772},{\"end\":41274,\"start\":40376},{\"end\":42425,\"start\":41276},{\"end\":43002,\"start\":42427},{\"end\":44093,\"start\":43004}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":30634,\"start\":30585},{\"attributes\":{\"id\":\"formula_1\"},\"end\":36555,\"start\":36507}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25114,\"start\":25107}]", "section_header": "[{\"end\":1868,\"start\":1856},{\"end\":9026,\"start\":9014},{\"end\":9052,\"start\":9029},{\"end\":10354,\"start\":10310},{\"end\":12000,\"start\":11978},{\"end\":13296,\"start\":13267},{\"end\":17907,\"start\":17892},{\"end\":21511,\"start\":21494},{\"end\":24361,\"start\":24351},{\"end\":25766,\"start\":25754},{\"end\":26726,\"start\":26718},{\"end\":32433,\"start\":32421},{\"end\":37494,\"start\":37482},{\"end\":38402,\"start\":38386},{\"end\":39770,\"start\":39760},{\"end\":44275,\"start\":44265},{\"end\":44787,\"start\":44777},{\"end\":45282,\"start\":45272},{\"end\":45647,\"start\":45637},{\"end\":45953,\"start\":45944}]", "table": null, "figure_caption": "[{\"end\":44263,\"start\":44096},{\"end\":44667,\"start\":44277},{\"end\":44775,\"start\":44670},{\"end\":45270,\"start\":44789},{\"end\":45635,\"start\":45284},{\"end\":45942,\"start\":45649},{\"end\":46197,\"start\":45955},{\"end\":46352,\"start\":46200}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18927,\"start\":18919},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26049,\"start\":26041},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27081,\"start\":27073},{\"end\":27244,\"start\":27235},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29457,\"start\":29434},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32259,\"start\":32249},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33769,\"start\":33759},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34474,\"start\":34466},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36737,\"start\":36729}]", "bib_author_first_name": "[{\"end\":46865,\"start\":46858},{\"end\":46879,\"start\":46875},{\"end\":46895,\"start\":46888},{\"end\":46903,\"start\":46896},{\"end\":46914,\"start\":46910},{\"end\":47206,\"start\":47199},{\"end\":47219,\"start\":47216},{\"end\":47233,\"start\":47232},{\"end\":47250,\"start\":47242},{\"end\":47267,\"start\":47260},{\"end\":47277,\"start\":47273},{\"end\":47839,\"start\":47832},{\"end\":47853,\"start\":47847},{\"end\":47867,\"start\":47864},{\"end\":48105,\"start\":48099},{\"end\":48129,\"start\":48122},{\"end\":48319,\"start\":48315},{\"end\":48506,\"start\":48497},{\"end\":48518,\"start\":48514},{\"end\":48526,\"start\":48519},{\"end\":48540,\"start\":48533},{\"end\":48554,\"start\":48546},{\"end\":48565,\"start\":48559},{\"end\":48947,\"start\":48940},{\"end\":48963,\"start\":48958},{\"end\":48979,\"start\":48974},{\"end\":49380,\"start\":49374},{\"end\":49389,\"start\":49388},{\"end\":49401,\"start\":49397},{\"end\":49874,\"start\":49868},{\"end\":49884,\"start\":49881},{\"end\":49898,\"start\":49894},{\"end\":49913,\"start\":49909},{\"end\":50355,\"start\":50352},{\"end\":50368,\"start\":50364},{\"end\":50386,\"start\":50382},{\"end\":50391,\"start\":50387},{\"end\":50742,\"start\":50737},{\"end\":50749,\"start\":50745},{\"end\":51148,\"start\":51141},{\"end\":51169,\"start\":51162},{\"end\":51186,\"start\":51177},{\"end\":51202,\"start\":51194},{\"end\":51515,\"start\":51508},{\"end\":51533,\"start\":51526},{\"end\":51911,\"start\":51904},{\"end\":51916,\"start\":51912},{\"end\":52272,\"start\":52267},{\"end\":52288,\"start\":52282},{\"end\":52300,\"start\":52294},{\"end\":52314,\"start\":52309},{\"end\":52754,\"start\":52746},{\"end\":52770,\"start\":52764},{\"end\":52785,\"start\":52781},{\"end\":52799,\"start\":52794},{\"end\":52819,\"start\":52811},{\"end\":52832,\"start\":52827},{\"end\":52845,\"start\":52839},{\"end\":52856,\"start\":52852},{\"end\":53398,\"start\":53393},{\"end\":53415,\"start\":53408},{\"end\":53430,\"start\":53426},{\"end\":53446,\"start\":53439},{\"end\":53463,\"start\":53455},{\"end\":53482,\"start\":53476},{\"end\":53499,\"start\":53491},{\"end\":53514,\"start\":53509},{\"end\":53530,\"start\":53523},{\"end\":54079,\"start\":54073},{\"end\":54397,\"start\":54393},{\"end\":54994,\"start\":54993},{\"end\":55009,\"start\":55004},{\"end\":55288,\"start\":55281},{\"end\":55302,\"start\":55298},{\"end\":55319,\"start\":55312},{\"end\":55336,\"start\":55331},{\"end\":55350,\"start\":55342},{\"end\":55366,\"start\":55360},{\"end\":55381,\"start\":55375},{\"end\":55396,\"start\":55390},{\"end\":55874,\"start\":55867},{\"end\":55889,\"start\":55881},{\"end\":55905,\"start\":55897},{\"end\":55917,\"start\":55912},{\"end\":55935,\"start\":55929},{\"end\":56651,\"start\":56650},{\"end\":56658,\"start\":56657},{\"end\":56670,\"start\":56669},{\"end\":56676,\"start\":56675},{\"end\":56690,\"start\":56689},{\"end\":57150,\"start\":57145},{\"end\":57163,\"start\":57159},{\"end\":57177,\"start\":57171},{\"end\":57667,\"start\":57662},{\"end\":57930,\"start\":57926},{\"end\":57946,\"start\":57940},{\"end\":57963,\"start\":57955},{\"end\":57978,\"start\":57973},{\"end\":58447,\"start\":58443},{\"end\":58463,\"start\":58457},{\"end\":58481,\"start\":58472},{\"end\":58496,\"start\":58488},{\"end\":58498,\"start\":58497},{\"end\":58515,\"start\":58508},{\"end\":58530,\"start\":58524},{\"end\":58538,\"start\":58535},{\"end\":58932,\"start\":58923},{\"end\":58946,\"start\":58945},{\"end\":58970,\"start\":58952},{\"end\":59307,\"start\":59306},{\"end\":59330,\"start\":59329},{\"end\":59724,\"start\":59714},{\"end\":59737,\"start\":59736},{\"end\":59859,\"start\":59856},{\"end\":60121,\"start\":60119},{\"end\":60123,\"start\":60122},{\"end\":60138,\"start\":60132},{\"end\":60158,\"start\":60151},{\"end\":60171,\"start\":60166},{\"end\":60173,\"start\":60172},{\"end\":60393,\"start\":60387},{\"end\":60409,\"start\":60405},{\"end\":60430,\"start\":60421},{\"end\":60448,\"start\":60441},{\"end\":60465,\"start\":60457},{\"end\":60482,\"start\":60475},{\"end\":60498,\"start\":60491},{\"end\":60513,\"start\":60508},{\"end\":60531,\"start\":60528},{\"end\":61004,\"start\":60997},{\"end\":61022,\"start\":61012},{\"end\":61040,\"start\":61034},{\"end\":61058,\"start\":61051},{\"end\":61439,\"start\":61432},{\"end\":61926,\"start\":61923},{\"end\":61939,\"start\":61933},{\"end\":61952,\"start\":61945},{\"end\":61963,\"start\":61959},{\"end\":61980,\"start\":61975},{\"end\":62465,\"start\":62462},{\"end\":62765,\"start\":62761},{\"end\":62777,\"start\":62772},{\"end\":62786,\"start\":62785},{\"end\":62804,\"start\":62798},{\"end\":62819,\"start\":62815},{\"end\":63207,\"start\":63200},{\"end\":63222,\"start\":63215},{\"end\":63239,\"start\":63233},{\"end\":63251,\"start\":63247},{\"end\":63256,\"start\":63252},{\"end\":63268,\"start\":63262},{\"end\":63286,\"start\":63279},{\"end\":63300,\"start\":63294},{\"end\":63706,\"start\":63701},{\"end\":63721,\"start\":63715},{\"end\":63739,\"start\":63732},{\"end\":63754,\"start\":63748},{\"end\":64210,\"start\":64209},{\"end\":64606,\"start\":64601},{\"end\":64608,\"start\":64607},{\"end\":64882,\"start\":64875},{\"end\":64900,\"start\":64896},{\"end\":64912,\"start\":64909},{\"end\":65303,\"start\":65297},{\"end\":65316,\"start\":65311},{\"end\":65330,\"start\":65324},{\"end\":65349,\"start\":65344},{\"end\":65361,\"start\":65357},{\"end\":65771,\"start\":65762},{\"end\":65791,\"start\":65786},{\"end\":65806,\"start\":65800},{\"end\":65812,\"start\":65807},{\"end\":65829,\"start\":65824},{\"end\":65847,\"start\":65840},{\"end\":66300,\"start\":66293},{\"end\":66316,\"start\":66310},{\"end\":66335,\"start\":66329},{\"end\":66348,\"start\":66342},{\"end\":66369,\"start\":66361},{\"end\":66839,\"start\":66834},{\"end\":67081,\"start\":67075},{\"end\":67092,\"start\":67086},{\"end\":67527,\"start\":67519},{\"end\":67541,\"start\":67537},{\"end\":67554,\"start\":67551},{\"end\":67558,\"start\":67555},{\"end\":67571,\"start\":67565},{\"end\":68136,\"start\":68129},{\"end\":68147,\"start\":68143}]", "bib_author_last_name": "[{\"end\":46873,\"start\":46866},{\"end\":46886,\"start\":46880},{\"end\":46908,\"start\":46904},{\"end\":46922,\"start\":46915},{\"end\":47214,\"start\":47207},{\"end\":47230,\"start\":47220},{\"end\":47240,\"start\":47234},{\"end\":47258,\"start\":47251},{\"end\":47271,\"start\":47268},{\"end\":47284,\"start\":47278},{\"end\":47289,\"start\":47286},{\"end\":47845,\"start\":47840},{\"end\":47862,\"start\":47854},{\"end\":47874,\"start\":47868},{\"end\":48120,\"start\":48106},{\"end\":48137,\"start\":48130},{\"end\":48345,\"start\":48320},{\"end\":48512,\"start\":48507},{\"end\":48531,\"start\":48527},{\"end\":48544,\"start\":48541},{\"end\":48557,\"start\":48555},{\"end\":48571,\"start\":48566},{\"end\":48956,\"start\":48948},{\"end\":48972,\"start\":48964},{\"end\":48989,\"start\":48980},{\"end\":49386,\"start\":49381},{\"end\":49395,\"start\":49390},{\"end\":49405,\"start\":49402},{\"end\":49879,\"start\":49875},{\"end\":49892,\"start\":49885},{\"end\":49907,\"start\":49899},{\"end\":49923,\"start\":49914},{\"end\":50362,\"start\":50356},{\"end\":50380,\"start\":50369},{\"end\":50398,\"start\":50392},{\"end\":50755,\"start\":50750},{\"end\":50768,\"start\":50757},{\"end\":51160,\"start\":51149},{\"end\":51175,\"start\":51170},{\"end\":51192,\"start\":51187},{\"end\":51207,\"start\":51203},{\"end\":51212,\"start\":51209},{\"end\":51524,\"start\":51516},{\"end\":51541,\"start\":51534},{\"end\":51925,\"start\":51917},{\"end\":52280,\"start\":52273},{\"end\":52292,\"start\":52289},{\"end\":52307,\"start\":52301},{\"end\":52321,\"start\":52315},{\"end\":52762,\"start\":52755},{\"end\":52779,\"start\":52771},{\"end\":52792,\"start\":52786},{\"end\":52809,\"start\":52800},{\"end\":52825,\"start\":52820},{\"end\":52837,\"start\":52833},{\"end\":52850,\"start\":52846},{\"end\":52866,\"start\":52857},{\"end\":52872,\"start\":52868},{\"end\":53406,\"start\":53399},{\"end\":53424,\"start\":53416},{\"end\":53437,\"start\":53431},{\"end\":53453,\"start\":53447},{\"end\":53474,\"start\":53464},{\"end\":53489,\"start\":53483},{\"end\":53507,\"start\":53500},{\"end\":53521,\"start\":53515},{\"end\":53538,\"start\":53531},{\"end\":54086,\"start\":54080},{\"end\":54405,\"start\":54398},{\"end\":54762,\"start\":54740},{\"end\":55002,\"start\":54995},{\"end\":55016,\"start\":55010},{\"end\":55026,\"start\":55018},{\"end\":55296,\"start\":55289},{\"end\":55310,\"start\":55303},{\"end\":55329,\"start\":55320},{\"end\":55340,\"start\":55337},{\"end\":55358,\"start\":55351},{\"end\":55373,\"start\":55367},{\"end\":55388,\"start\":55382},{\"end\":55400,\"start\":55397},{\"end\":55879,\"start\":55875},{\"end\":55895,\"start\":55890},{\"end\":55910,\"start\":55906},{\"end\":55927,\"start\":55918},{\"end\":55942,\"start\":55936},{\"end\":56655,\"start\":56652},{\"end\":56667,\"start\":56659},{\"end\":56673,\"start\":56671},{\"end\":56687,\"start\":56677},{\"end\":56706,\"start\":56691},{\"end\":57157,\"start\":57151},{\"end\":57169,\"start\":57164},{\"end\":57180,\"start\":57178},{\"end\":57684,\"start\":57668},{\"end\":57938,\"start\":57931},{\"end\":57953,\"start\":57947},{\"end\":57971,\"start\":57964},{\"end\":57983,\"start\":57979},{\"end\":58455,\"start\":58448},{\"end\":58470,\"start\":58464},{\"end\":58486,\"start\":58482},{\"end\":58506,\"start\":58499},{\"end\":58522,\"start\":58516},{\"end\":58533,\"start\":58531},{\"end\":58545,\"start\":58539},{\"end\":58943,\"start\":58933},{\"end\":58950,\"start\":58947},{\"end\":58980,\"start\":58971},{\"end\":58993,\"start\":58982},{\"end\":59317,\"start\":59308},{\"end\":59327,\"start\":59319},{\"end\":59336,\"start\":59331},{\"end\":59345,\"start\":59338},{\"end\":59734,\"start\":59725},{\"end\":59741,\"start\":59738},{\"end\":59751,\"start\":59743},{\"end\":59862,\"start\":59860},{\"end\":60130,\"start\":60124},{\"end\":60149,\"start\":60139},{\"end\":60164,\"start\":60159},{\"end\":60181,\"start\":60174},{\"end\":60403,\"start\":60394},{\"end\":60419,\"start\":60410},{\"end\":60439,\"start\":60431},{\"end\":60455,\"start\":60449},{\"end\":60473,\"start\":60466},{\"end\":60489,\"start\":60483},{\"end\":60506,\"start\":60499},{\"end\":60526,\"start\":60514},{\"end\":60537,\"start\":60532},{\"end\":61010,\"start\":61005},{\"end\":61032,\"start\":61023},{\"end\":61049,\"start\":61041},{\"end\":61065,\"start\":61059},{\"end\":61448,\"start\":61440},{\"end\":61465,\"start\":61450},{\"end\":61931,\"start\":61927},{\"end\":61943,\"start\":61940},{\"end\":61957,\"start\":61953},{\"end\":61973,\"start\":61964},{\"end\":61985,\"start\":61981},{\"end\":62476,\"start\":62466},{\"end\":62499,\"start\":62478},{\"end\":62770,\"start\":62766},{\"end\":62783,\"start\":62778},{\"end\":62796,\"start\":62787},{\"end\":62813,\"start\":62805},{\"end\":62829,\"start\":62820},{\"end\":63213,\"start\":63208},{\"end\":63231,\"start\":63223},{\"end\":63245,\"start\":63240},{\"end\":63260,\"start\":63257},{\"end\":63277,\"start\":63269},{\"end\":63292,\"start\":63287},{\"end\":63305,\"start\":63301},{\"end\":63713,\"start\":63707},{\"end\":63730,\"start\":63722},{\"end\":63746,\"start\":63740},{\"end\":63759,\"start\":63755},{\"end\":64216,\"start\":64211},{\"end\":64223,\"start\":64218},{\"end\":64614,\"start\":64609},{\"end\":64894,\"start\":64883},{\"end\":64907,\"start\":64901},{\"end\":64918,\"start\":64913},{\"end\":65309,\"start\":65304},{\"end\":65322,\"start\":65317},{\"end\":65342,\"start\":65331},{\"end\":65355,\"start\":65350},{\"end\":65371,\"start\":65362},{\"end\":65784,\"start\":65772},{\"end\":65798,\"start\":65792},{\"end\":65822,\"start\":65813},{\"end\":65838,\"start\":65830},{\"end\":65854,\"start\":65848},{\"end\":66308,\"start\":66301},{\"end\":66327,\"start\":66317},{\"end\":66340,\"start\":66336},{\"end\":66359,\"start\":66349},{\"end\":66375,\"start\":66370},{\"end\":66847,\"start\":66840},{\"end\":67084,\"start\":67082},{\"end\":67096,\"start\":67093},{\"end\":67535,\"start\":67528},{\"end\":67549,\"start\":67542},{\"end\":67563,\"start\":67559},{\"end\":67577,\"start\":67572},{\"end\":68141,\"start\":68137}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":127197},\"end\":47122,\"start\":46785},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15716615},\"end\":47724,\"start\":47124},{\"attributes\":{\"id\":\"b2\"},\"end\":48044,\"start\":47726},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":35718913},\"end\":48293,\"start\":48046},{\"attributes\":{\"id\":\"b4\"},\"end\":48413,\"start\":48295},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3817960},\"end\":48884,\"start\":48415},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":22339115},\"end\":49372,\"start\":48886},{\"attributes\":{\"id\":\"b7\"},\"end\":49616,\"start\":49374},{\"attributes\":{\"id\":\"b8\"},\"end\":49777,\"start\":49618},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9872719},\"end\":50309,\"start\":49779},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10024967},\"end\":50705,\"start\":50311},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11430990},\"end\":51085,\"start\":50707},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3116168},\"end\":51452,\"start\":51087},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1434196},\"end\":51799,\"start\":51454},{\"attributes\":{\"id\":\"b14\"},\"end\":52210,\"start\":51801},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3331657},\"end\":52672,\"start\":52212},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207603242},\"end\":53358,\"start\":52674},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8870436},\"end\":54071,\"start\":53360},{\"attributes\":{\"id\":\"b18\"},\"end\":54343,\"start\":54073},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8943607},\"end\":54705,\"start\":54345},{\"attributes\":{\"doi\":\"No. CEDER-10-01\",\"id\":\"b20\"},\"end\":54936,\"start\":54707},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":677218},\"end\":55172,\"start\":54938},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":49269642},\"end\":55775,\"start\":55174},{\"attributes\":{\"doi\":\"10.1145/3173225.3173302\",\"id\":\"b23\",\"matched_paper_id\":3842274},\"end\":56562,\"start\":55777},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":229228},\"end\":57057,\"start\":56564},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14162250},\"end\":57553,\"start\":57059},{\"attributes\":{\"id\":\"b26\"},\"end\":57832,\"start\":57555},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15512333},\"end\":58373,\"start\":57834},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7537204},\"end\":58865,\"start\":58375},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":55130898},\"end\":59215,\"start\":58867},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6496659},\"end\":59660,\"start\":59217},{\"attributes\":{\"id\":\"b31\"},\"end\":59852,\"start\":59662},{\"attributes\":{\"id\":\"b32\"},\"end\":59886,\"start\":59854},{\"attributes\":{\"id\":\"b33\"},\"end\":60047,\"start\":59888},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11709778},\"end\":60343,\"start\":60049},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10659969},\"end\":60898,\"start\":60345},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4837028},\"end\":61360,\"start\":60900},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3710232},\"end\":61853,\"start\":61362},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14475079},\"end\":62383,\"start\":61855},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":149129860},\"end\":62690,\"start\":62385},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11386257},\"end\":63131,\"start\":62692},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":62048618},\"end\":63603,\"start\":63133},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":10381531},\"end\":64140,\"start\":63605},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":144167},\"end\":64526,\"start\":64142},{\"attributes\":{\"id\":\"b44\"},\"end\":64769,\"start\":64528},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":5905120},\"end\":65195,\"start\":64771},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":3268227},\"end\":65702,\"start\":65197},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3680951},\"end\":66260,\"start\":65704},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3333879},\"end\":66779,\"start\":66262},{\"attributes\":{\"id\":\"b49\"},\"end\":66996,\"start\":66781},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":49318525},\"end\":67410,\"start\":66998},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":29259081},\"end\":68051,\"start\":67412},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":10326133},\"end\":68548,\"start\":68053},{\"attributes\":{\"id\":\"b53\"},\"end\":68709,\"start\":68550}]", "bib_title": "[{\"end\":46856,\"start\":46785},{\"end\":47197,\"start\":47124},{\"end\":48097,\"start\":48046},{\"end\":48495,\"start\":48415},{\"end\":48938,\"start\":48886},{\"end\":49866,\"start\":49779},{\"end\":50350,\"start\":50311},{\"end\":50735,\"start\":50707},{\"end\":51139,\"start\":51087},{\"end\":51506,\"start\":51454},{\"end\":52265,\"start\":52212},{\"end\":52744,\"start\":52674},{\"end\":53391,\"start\":53360},{\"end\":54391,\"start\":54345},{\"end\":54991,\"start\":54938},{\"end\":55279,\"start\":55174},{\"end\":55865,\"start\":55777},{\"end\":56648,\"start\":56564},{\"end\":57143,\"start\":57059},{\"end\":57924,\"start\":57834},{\"end\":58441,\"start\":58375},{\"end\":58921,\"start\":58867},{\"end\":59304,\"start\":59217},{\"end\":60117,\"start\":60049},{\"end\":60385,\"start\":60345},{\"end\":60995,\"start\":60900},{\"end\":61430,\"start\":61362},{\"end\":61921,\"start\":61855},{\"end\":62460,\"start\":62385},{\"end\":62759,\"start\":62692},{\"end\":63198,\"start\":63133},{\"end\":63699,\"start\":63605},{\"end\":64207,\"start\":64142},{\"end\":64599,\"start\":64528},{\"end\":64873,\"start\":64771},{\"end\":65295,\"start\":65197},{\"end\":65760,\"start\":65704},{\"end\":66291,\"start\":66262},{\"end\":67073,\"start\":66998},{\"end\":67517,\"start\":67412},{\"end\":68127,\"start\":68053}]", "bib_author": "[{\"end\":46875,\"start\":46858},{\"end\":46888,\"start\":46875},{\"end\":46910,\"start\":46888},{\"end\":46924,\"start\":46910},{\"end\":47216,\"start\":47199},{\"end\":47232,\"start\":47216},{\"end\":47242,\"start\":47232},{\"end\":47260,\"start\":47242},{\"end\":47273,\"start\":47260},{\"end\":47286,\"start\":47273},{\"end\":47291,\"start\":47286},{\"end\":47847,\"start\":47832},{\"end\":47864,\"start\":47847},{\"end\":47876,\"start\":47864},{\"end\":48122,\"start\":48099},{\"end\":48139,\"start\":48122},{\"end\":48347,\"start\":48315},{\"end\":48514,\"start\":48497},{\"end\":48533,\"start\":48514},{\"end\":48546,\"start\":48533},{\"end\":48559,\"start\":48546},{\"end\":48573,\"start\":48559},{\"end\":48958,\"start\":48940},{\"end\":48974,\"start\":48958},{\"end\":48991,\"start\":48974},{\"end\":49388,\"start\":49374},{\"end\":49397,\"start\":49388},{\"end\":49407,\"start\":49397},{\"end\":49881,\"start\":49868},{\"end\":49894,\"start\":49881},{\"end\":49909,\"start\":49894},{\"end\":49925,\"start\":49909},{\"end\":50364,\"start\":50352},{\"end\":50382,\"start\":50364},{\"end\":50400,\"start\":50382},{\"end\":50745,\"start\":50737},{\"end\":50757,\"start\":50745},{\"end\":50772,\"start\":50757},{\"end\":51162,\"start\":51141},{\"end\":51177,\"start\":51162},{\"end\":51194,\"start\":51177},{\"end\":51209,\"start\":51194},{\"end\":51214,\"start\":51209},{\"end\":51526,\"start\":51508},{\"end\":51543,\"start\":51526},{\"end\":51927,\"start\":51904},{\"end\":52282,\"start\":52267},{\"end\":52294,\"start\":52282},{\"end\":52309,\"start\":52294},{\"end\":52323,\"start\":52309},{\"end\":52764,\"start\":52746},{\"end\":52781,\"start\":52764},{\"end\":52794,\"start\":52781},{\"end\":52811,\"start\":52794},{\"end\":52827,\"start\":52811},{\"end\":52839,\"start\":52827},{\"end\":52852,\"start\":52839},{\"end\":52868,\"start\":52852},{\"end\":52874,\"start\":52868},{\"end\":53408,\"start\":53393},{\"end\":53426,\"start\":53408},{\"end\":53439,\"start\":53426},{\"end\":53455,\"start\":53439},{\"end\":53476,\"start\":53455},{\"end\":53491,\"start\":53476},{\"end\":53509,\"start\":53491},{\"end\":53523,\"start\":53509},{\"end\":53540,\"start\":53523},{\"end\":54088,\"start\":54073},{\"end\":54407,\"start\":54393},{\"end\":54764,\"start\":54740},{\"end\":55004,\"start\":54993},{\"end\":55018,\"start\":55004},{\"end\":55028,\"start\":55018},{\"end\":55298,\"start\":55281},{\"end\":55312,\"start\":55298},{\"end\":55331,\"start\":55312},{\"end\":55342,\"start\":55331},{\"end\":55360,\"start\":55342},{\"end\":55375,\"start\":55360},{\"end\":55390,\"start\":55375},{\"end\":55402,\"start\":55390},{\"end\":55881,\"start\":55867},{\"end\":55897,\"start\":55881},{\"end\":55912,\"start\":55897},{\"end\":55929,\"start\":55912},{\"end\":55944,\"start\":55929},{\"end\":56657,\"start\":56650},{\"end\":56669,\"start\":56657},{\"end\":56675,\"start\":56669},{\"end\":56689,\"start\":56675},{\"end\":56708,\"start\":56689},{\"end\":57159,\"start\":57145},{\"end\":57171,\"start\":57159},{\"end\":57182,\"start\":57171},{\"end\":57686,\"start\":57662},{\"end\":57940,\"start\":57926},{\"end\":57955,\"start\":57940},{\"end\":57973,\"start\":57955},{\"end\":57985,\"start\":57973},{\"end\":58457,\"start\":58443},{\"end\":58472,\"start\":58457},{\"end\":58488,\"start\":58472},{\"end\":58508,\"start\":58488},{\"end\":58524,\"start\":58508},{\"end\":58535,\"start\":58524},{\"end\":58547,\"start\":58535},{\"end\":58945,\"start\":58923},{\"end\":58952,\"start\":58945},{\"end\":58982,\"start\":58952},{\"end\":58995,\"start\":58982},{\"end\":59319,\"start\":59306},{\"end\":59329,\"start\":59319},{\"end\":59338,\"start\":59329},{\"end\":59347,\"start\":59338},{\"end\":59736,\"start\":59714},{\"end\":59743,\"start\":59736},{\"end\":59753,\"start\":59743},{\"end\":59864,\"start\":59856},{\"end\":60132,\"start\":60119},{\"end\":60151,\"start\":60132},{\"end\":60166,\"start\":60151},{\"end\":60183,\"start\":60166},{\"end\":60405,\"start\":60387},{\"end\":60421,\"start\":60405},{\"end\":60441,\"start\":60421},{\"end\":60457,\"start\":60441},{\"end\":60475,\"start\":60457},{\"end\":60491,\"start\":60475},{\"end\":60508,\"start\":60491},{\"end\":60528,\"start\":60508},{\"end\":60539,\"start\":60528},{\"end\":61012,\"start\":60997},{\"end\":61034,\"start\":61012},{\"end\":61051,\"start\":61034},{\"end\":61067,\"start\":61051},{\"end\":61450,\"start\":61432},{\"end\":61467,\"start\":61450},{\"end\":61933,\"start\":61923},{\"end\":61945,\"start\":61933},{\"end\":61959,\"start\":61945},{\"end\":61975,\"start\":61959},{\"end\":61987,\"start\":61975},{\"end\":62478,\"start\":62462},{\"end\":62501,\"start\":62478},{\"end\":62772,\"start\":62761},{\"end\":62785,\"start\":62772},{\"end\":62798,\"start\":62785},{\"end\":62815,\"start\":62798},{\"end\":62831,\"start\":62815},{\"end\":63215,\"start\":63200},{\"end\":63233,\"start\":63215},{\"end\":63247,\"start\":63233},{\"end\":63262,\"start\":63247},{\"end\":63279,\"start\":63262},{\"end\":63294,\"start\":63279},{\"end\":63307,\"start\":63294},{\"end\":63715,\"start\":63701},{\"end\":63732,\"start\":63715},{\"end\":63748,\"start\":63732},{\"end\":63761,\"start\":63748},{\"end\":64218,\"start\":64209},{\"end\":64225,\"start\":64218},{\"end\":64616,\"start\":64601},{\"end\":64896,\"start\":64875},{\"end\":64909,\"start\":64896},{\"end\":64920,\"start\":64909},{\"end\":65311,\"start\":65297},{\"end\":65324,\"start\":65311},{\"end\":65344,\"start\":65324},{\"end\":65357,\"start\":65344},{\"end\":65373,\"start\":65357},{\"end\":65786,\"start\":65762},{\"end\":65800,\"start\":65786},{\"end\":65824,\"start\":65800},{\"end\":65840,\"start\":65824},{\"end\":65856,\"start\":65840},{\"end\":66310,\"start\":66293},{\"end\":66329,\"start\":66310},{\"end\":66342,\"start\":66329},{\"end\":66361,\"start\":66342},{\"end\":66377,\"start\":66361},{\"end\":66849,\"start\":66834},{\"end\":67086,\"start\":67075},{\"end\":67098,\"start\":67086},{\"end\":67537,\"start\":67519},{\"end\":67551,\"start\":67537},{\"end\":67565,\"start\":67551},{\"end\":67579,\"start\":67565},{\"end\":68143,\"start\":68129},{\"end\":68150,\"start\":68143}]", "bib_venue": "[{\"end\":46935,\"start\":46924},{\"end\":47374,\"start\":47291},{\"end\":47830,\"start\":47726},{\"end\":48150,\"start\":48139},{\"end\":48313,\"start\":48295},{\"end\":48633,\"start\":48573},{\"end\":49077,\"start\":48991},{\"end\":49467,\"start\":49407},{\"end\":49670,\"start\":49618},{\"end\":50001,\"start\":49925},{\"end\":50466,\"start\":50400},{\"end\":50850,\"start\":50772},{\"end\":51250,\"start\":51214},{\"end\":51619,\"start\":51543},{\"end\":51902,\"start\":51801},{\"end\":52397,\"start\":52323},{\"end\":52948,\"start\":52874},{\"end\":53635,\"start\":53540},{\"end\":54170,\"start\":54088},{\"end\":54481,\"start\":54407},{\"end\":54738,\"start\":54707},{\"end\":55035,\"start\":55028},{\"end\":55460,\"start\":55402},{\"end\":56076,\"start\":55967},{\"end\":56800,\"start\":56708},{\"end\":57258,\"start\":57182},{\"end\":57660,\"start\":57555},{\"end\":58059,\"start\":57985},{\"end\":58605,\"start\":58547},{\"end\":59026,\"start\":58995},{\"end\":59403,\"start\":59347},{\"end\":59712,\"start\":59662},{\"end\":59940,\"start\":59888},{\"end\":60187,\"start\":60183},{\"end\":60575,\"start\":60539},{\"end\":61095,\"start\":61067},{\"end\":61556,\"start\":61467},{\"end\":62069,\"start\":61987},{\"end\":62525,\"start\":62501},{\"end\":62894,\"start\":62831},{\"end\":63355,\"start\":63307},{\"end\":63832,\"start\":63761},{\"end\":64293,\"start\":64225},{\"end\":64632,\"start\":64616},{\"end\":64963,\"start\":64920},{\"end\":65433,\"start\":65373},{\"end\":65974,\"start\":65856},{\"end\":66469,\"start\":66377},{\"end\":66832,\"start\":66781},{\"end\":67192,\"start\":67098},{\"end\":67666,\"start\":67579},{\"end\":68237,\"start\":68150},{\"end\":68602,\"start\":68550},{\"end\":47444,\"start\":47376},{\"end\":49150,\"start\":49079},{\"end\":49687,\"start\":49672},{\"end\":50064,\"start\":50003},{\"end\":50519,\"start\":50468},{\"end\":50915,\"start\":50852},{\"end\":52458,\"start\":52399},{\"end\":53009,\"start\":52950},{\"end\":53717,\"start\":53637},{\"end\":54542,\"start\":54483},{\"end\":56189,\"start\":56078},{\"end\":57321,\"start\":57260},{\"end\":58120,\"start\":58061},{\"end\":59446,\"start\":59405},{\"end\":59957,\"start\":59942},{\"end\":61110,\"start\":61097},{\"end\":61632,\"start\":61558},{\"end\":62138,\"start\":62071},{\"end\":63890,\"start\":63834},{\"end\":64348,\"start\":64295},{\"end\":66548,\"start\":66471},{\"end\":67740,\"start\":67668},{\"end\":68311,\"start\":68239},{\"end\":68619,\"start\":68604}]"}}}, "year": 2023, "month": 12, "day": 17}