{"id": 12122749, "updated": "2023-07-19 15:06:27.649", "metadata": {"title": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction", "authors": "[{\"first\":\"Christopher\",\"last\":\"Bryant\",\"middle\":[]},{\"first\":\"Mariano\",\"last\":\"Felice\",\"middle\":[]},{\"first\":\"Ted\",\"last\":\"Briscoe\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as \u201cGood\u201d or \u201cAcceptable\u201d in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2741494657", "acl": "P17-1074", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/BryantFB17", "doi": "10.18653/v1/p17-1074"}}, "content": {"source": {"pdf_hash": "de156418a83b763532e305daae9e09a785c4f960", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/P17-1074.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/P17-1074.pdf", "status": "HYBRID"}}, "grobid": {"id": "3432ebd8a7f46744684318c172857f1434a8b604", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/de156418a83b763532e305daae9e09a785c4f960.txt", "contents": "\nAutomatic Annotation and Evaluation of Error Types for Grammatical Error Correction\nJuly 30 -August 4, 2017. July 30 -August 4, 2017\n\nChristopher Bryant \nALTA Institute Computer Laboratory University of Cambridge Cambridge\nUK\n\nMariano Felice \nALTA Institute Computer Laboratory University of Cambridge Cambridge\nUK\n\nTed Briscoe \nALTA Institute Computer Laboratory University of Cambridge Cambridge\nUK\n\nAutomatic Annotation and Evaluation of Error Types for Grammatical Error Correction\n\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics\nthe 55th Annual Meeting of the Association for Computational LinguisticsVancouver, Canada; Vancouver, CanadaJuly 30 -August 4, 2017. July 30 -August 4, 201710.18653/v1/P17-1074\nUntil now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rulebased framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as \"Good\" or \"Acceptable\" in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.\n\nIntroduction\n\nGrammatical Error Correction (GEC) systems are often only evaluated in terms of overall performance because system hypotheses are not annotated. This can be misleading however, and a system that performs poorly overall may in fact outperform others at specific error types. This is significant because a robust specialised system is actually more desirable than a mediocre general system. Without an error type analysis however, this information is completely unknown.\n\nThe main aim of this paper is hence to rectify this situation and provide a method by which parallel error correction data can be automatically annotated with error type information. This not only facilitates error type evaluation, but can also be used to provide detailed error type feedback to non-native learners. Given that different corpora are also annotated according to different standards, we also attempted to standardise existing datasets under a common error type framework.\n\nOur approach consists of two main steps. First, we automatically extract the edits between parallel original and corrected sentences by means of a linguistically-enhanced alignment algorithm (Felice et al., 2016) and second, we classify them according to a new, rule-based framework that relies solely on dataset-agnostic information such as lemma and part-of-speech. We demonstrate the value of our approach, which we call the ERRor ANnotation Toolkit (ERRANT) 1 , by carrying out a detailed error type analysis of each system in the CoNLL-2014 shared task on grammatical error correction (Ng et al., 2014).\n\nIt is worth mentioning that despite an increased interest in GEC evaluation in recent years (Dahlmeier and Ng, 2012;Felice and Briscoe, 2015;Bryant and Ng, 2015;Napoles et al., 2015;Grundkiewicz et al., 2015;Sakaguchi et al., 2016), ERRANT is the only toolkit currently capable of producing error types scores.\n\n\nEdit Extraction\n\nThe first stage of automatic annotation is edit extraction. Specifically, given an original and corrected sentence pair, we need to determine the start and end boundaries of any edits. This is fundamentally an alignment problem:\n\nWe took a guide tour on center city . We took a guided tour of the city center .\n\nTable 1: A sample alignment between an original and corrected sentence (Felice et al., 2016).\n\nThe first attempt at automatic edit extraction was made by Swanson and Yamangil (2012), who simply used the Levenshtein distance to align parallel original and corrected sentences. As the Levenshtein distance only aligns individual tokens however, they also merged all adjacent nonmatches in an effort to capture multi-token edits. Xue and Hwa (2014) subsequently improved on Swanson and Yamangil's work by training a maximum entropy classifier to predict whether edits should be merged or not.\n\nMost recently, Felice et al. (2016) proposed a new method of edit extraction using a linguistically-enhanced alignment algorithm supported by a set of merging rules. More specifically, they incorporated various linguistic information, such as part-of-speech and lemma, into the cost function of the Damerau-Levenshtein 2 algorithm to make it more likely that tokens with similar linguistic properties aligned. This approach ultimately proved most effective at approximating human edits in several datasets (80-85% F 1 ), and so we use it in the present study.\n\n\nAutomatic Error Typing\n\nHaving extracted the edits, the next step is to assign them error types. While Swanson and Yamangil (2012) did this by means of maximum entropy classifiers, one disadvantage of this approach is that such classifiers are biased towards their particular training corpora. For example, a classifier trained on the First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011) is unlikely to perform as well on the National University of Singapore Corpus of Learner English (NU-CLE) (Dahlmeier and Ng, 2012) or vice versa, because both corpora have been annotated according to different standards (cf. Xue and Hwa (2014)). Instead, a dataset-agnostic error type classifier is much more desirable.\n\n\nA Rule-Based Error Type Framework\n\nTo solve this problem, we took inspiration from Swanson and Yamangil's (2012) observation that most error types are based on part-of-speech (POS) categories, and wrote a rule to classify an edit based only on its automatic POS tags. We then added another rule to similarly differentiate between Missing, Unnecessary and Replace-ment errors depending on whether tokens were inserted, deleted or substituted. Finally, we extended our approach to classify errors that are not well-characterised by POS, such as Spelling or Word Order, and ultimately assigned all error types based solely on automatically-obtained, objective properties of the data.\n\nIn total, we wrote roughly 50 rules. While many of them are very straightforward, significant attention was paid to discriminating between different kinds of verb errors. For example, despite all having the same correction, the following sentences contain different types of common learner errors:\n\n(a) He IS asleep now.\n\n[ While the final three rules could certainly be reordered, we informally found the above sequence performed best during development. It is also worth mentioning that this is a somewhat simplified example and that there are additional rules to discriminate between auxiliary verbs, main verbs and multi verb expressions. Nevertheless, the above case exemplifies our approach, and a more complete description of all rules is provided with the software.  \n\n\nA Dataset-Agnostic Classifier\n\nOne of the key strengths of a rule-based approach is that by being dependent only on automatic mark-up information, our classifier is entirely dataset independent and does not require labelled training data. This is in contrast with machine learning approaches which not only learn dataset specific biases, but also presuppose the existence of sufficient quantities of training data. A second significant advantage of our approach is that it is also always possible to determine precisely why an edit was assigned a particular error category. In contrast, human and machine learning classification decisions are often much less transparent.\n\nFinally, by being fully deterministic, our approach bypasses bias effects altogether and should hence be more consistent.\n\n\nAutomatic Markup\n\nThe prerequisites for our rule-based classifier are that each token in both the original and corrected sentence is POS tagged, lemmatized, stemmed and dependency parsed. We use spaCy 3 v1.7.3 for all but the stemming, which is performed by the Lancaster Stemmer in NLTK. 4 Since fine-grained POS tags are often too detailed for the purposes of error evaluation, we also map spaCy's Penn Treebank style tags to the coarser set of Universal Dependency tags. 5 We use the latest Hunspell GB-large word list 6 to help classify non-word errors. The marked-up tokens in an edit span are then input to the classifier and an error type is returned.\n\n\nError Categories\n\nThe complete list of 25 error types in our new framework is shown in Table 2. Note that most of them can be prefixed with 'M:', 'R:' or 'U:', depending on whether they describe a Missing, Replacement, or Unnecessary edit, to enable evaluation at different levels of granularity (see Appendix A for all valid combinations). This means we can choose to evaluate, for example, only replacement errors (anything prefixed by 'R:'), only noun errors (anything suffixed with 'NOUN') or only replacement noun errors ('R:NOUN'). This flexibility allows us to make more detailed observations about different aspects of system performance.\n\nOne caveat concerning error scheme design is that it is always possible to add new categories for increasingly detailed error types; for instance, we currently label [could \u2192 should] a tense error, when it might otherwise be considered a modal error. The reason we do not call it a modal error, however, is because it would then become less clear how to handle other cases such as [can \u2192 should] and [has eaten \u2192 should eat], which might be considered a more complex combination of modal and tense error. As it is impractical to create new categories and rules to differentiate between such narrow distinctions however, our final framework aims to be a compromise between informativeness and practicality.\n\n\nClassifier Evaluation\n\nAs our new error scheme is based solely on automatically obtained properties of the data, there are no gold standard labels against which to evaluate classifier performance. For this reason, we instead carried out a small-scale manual evaluation, where we simply asked 5 GEC researchers to rate the appropriateness of the predicted error types for 200 randomly chosen edits in context (100 from FCE-test and 100 from CoNLL-2014) as \"Good\", \"Acceptable\" or \"Bad\". \"Good' meant the chosen type was the most appropriate for the given edit, \"Acceptable\" meant the chosen type was appropriate, but probably not optimum, while \"Bad\" meant the chosen type was not appropriate for the edit. Raters were warned that the edit boundaries had been determined automatically and hence might be unusual, but that they should focus on the appropriateness of the error type regardless of whether they agreed with the boundary or not.\n\nIt is worth stating that the main purpose of this evaluation was not to evaluate the specific strengths and weaknesses of the classifier, but rather ascertain how well humans believed the predicted error types characterised each edit. GEC is known to be a highly subjective task (Bryant and Table 3: The percent distribution for how each expert rated the appropriateness of the predicted error types. E.g. Rater 3 considered 83% of all predicted types to be \"Good\".\n\nNg, 2015) and so we were more interested in overall judgements than specific disagreements. The results from this evaluation are shown in Table 3. Significantly, all 5 raters considered at least 95% of the predicted error types to be either \"Good\" or \"Acceptable\", despite the degree of noise introduced by automatic edit extraction. Furthermore, whenever raters judged an edit as \"Bad\", this could usually be traced back to a POS or parse error; e.g. [ring \u2192 rings] might be considered a NOUN:NUM or VERB:SVA error depending on whether the POS tagger considered both sides of the edit nouns or verbs. Interannotator agreement was also good at 0.724 \u03ba f ree (Randolph, 2005).\n\nIn contrast, although incomparable on account of the different metric and error scheme, the best results using machine learning were between 50-70% F 1 (Felice et al., 2016). Ultimately however, we believe the high scores awarded by the raters validates the efficacy of our rule-based approach.\n\n\nError Type Scoring\n\nHaving described how to automatically annotate parallel sentences with ERRANT, we now also have a method to annotate system hypotheses; this is the first step towards an error type evaluation. Since no scorer is currently capable of calculating error type performance however (Dahlmeier and Ng, 2012;Felice and Briscoe, 2015;Napoles et al., 2015), we instead built our own.\n\nFortunately, one benefit of explicitly annotating system hypotheses is that it makes evaluation much more straightforward. In particular, for each sentence, we only need to compare the edits in the hypothesis against the edits in each respective reference and measure the overlap. Any edit with the same span and correction in both files is hence a true positive (TP), while unmatched edits in the hypothesis and references are false positives (FP) and false negatives (FN) respectively. These results can then be grouped by error type for the purposes of error type evaluation.\n\nFinally, it is worth noting that this scorer is much simpler than other scorers in GEC which typically incorporate edit extraction or alignment directly into their algorithms. Our approach, on the other hand, treats edit extraction and evaluation as separate tasks.\n\n\nGold Reference vs. Auto Reference\n\nBefore evaluating an automatically annotated hypothesis against its reference, we must also address another mismatch: namely that hypothesis edits must be extracted and classified automatically, while reference edits are typically extracted and classified manually using a different framework. Since evaluation is now reduced to a straightforward comparison between two files however, it is especially important that the hypothesis and references are both processed in the same way. For instance, a hypothesis edit [have eating \u2192 has eaten] will not match the reference edits [have \u2192 has] and [eating \u2192 eaten] because the former is one edit while the latter is two edits, even though they equate to the same thing.\n\nTo solve this problem, we can reprocess the references in the same way as the hypotheses. In other words, we can apply ERRANT to the references such that each reference edit is subject to the same automatic extraction and classification criteria as each hypothesis edit. While it may seem unorthodox to discard gold reference information in favour of automatic reference information, this is necessary to minimise the difference between hypothesis and reference edits and also standardise error type annotations.\n\nTo show that automatic references are feasible alternatives to gold references, we evaluated each team in the CoNLL-2014 shared task using both types of reference with the M 2 scorer (Dahlmeier and Ng, 2012), the de facto standard of GEC evaluation, and our own scorer. Table 4 hence shows that there is little difference between the overall scores for each team, and we formally validated this hypothesis for precision, recall and F 0.5 by means of bootstrap significance testing (Efron and Tibshirani, 1993). Ultimately, we found no statistically significant difference  between automatic and gold references (1,000 iterations, p > .05) which leads us to conclude that our automatic references are qualitatively as good as human references.\n\n\nComparison with the M 2 Scorer\n\nDespite using the same metric, Table 4 also shows that the M 2 scorer tends to produce slightly higher F 0.5 scores than our own. This initially led us to believe that our scorer was underestimating performance, but we subsequently found that instead the M 2 scorer tends to overestimate performance (cf. Felice and Briscoe (2015) and Napoles et al. (2015)).\n\nIn particular, given a choice between matching [have eating \u2192 has eaten] from Annotator 1 or [have \u2192 has] and [eating \u2192 eaten] from Annotator 2, the M 2 scorer will always choose Annotator 2 because two true positives (TP) are worth more than one. Similarly, whenever the scorer encounters two false positives (FP) within a certain distance of each other, 7 it merges them and treats them as one false positive; e.g. [is a cat \u2192 are a cats] is selected over [is \u2192 are] and [cat \u2192 cats] even though these edits are best handled separately. In other words, the M 2 scorer exploits its dynamic edit boundary prediction to artificially maximise true positives and minimise false positives and hence produce slightly inflated scores.  A dash indicates the team's system did not attempt to correct the given error type (TP+FP = 0).\n\n\nCoNLL-2014 Shared Task Analysis\n\nTo demonstrate the value of ERRANT, we applied it to the data produced in the CoNLL-2014 shared task (Ng et al., 2014). Specifically, we automatically annotated all the system hypotheses and official reference files. 8 Although ERRANT can be applied to any dataset of parallel sentences, we chose to evaluate on CoNLL-2014 because it represents the largest collection of publicly available GEC system output. For more information about the systems in CoNLL-2014, we refer the reader to the shared task paper.\n\n\nEdit Operation\n\nIn our first category experiment, we simply investigated the performance of each system in terms of Missing, Replacement and Unnecessary edits. The results are shown in Table 5 with additional information in Appendix B, Table 10. The most surprising result is that five teams (AMU, IPN, PKU, RAC, UFC) failed to correct any unnecessary token errors at all. This is noteworthy because unnecessary token errors account for roughly 25% of all errors in the CoNLL-2014 test data and so failing to address them significantly limits a system's maximum performance. While the reason for this is clear in some cases, e.g. UFC's rule-based system was never designed to tackle unnecessary tokens (Gupta, 2014), it is less clear in others, e.g. there is no obvious reason why AMU's SMT system failed to learn when 8 http://www.comp.nus.edu.sg/ \u223c nlp/conll14st.html to delete tokens (Junczys-Dowmunt and Grundkiewicz, 2014). AMU's result is especially remarkable given that their system still came 3rd overall despite this limitation.\n\nIn contrast, CUUI's classifier approach (Rozovskaya et al., 2014) was the most successful at correcting not only unnecessary token errors, but also replacement token errors, while CAMB's hybrid MT approach (Felice et al., 2014) significantly outperformed all others in terms of missing token errors. It would hence make sense to combine these two approaches, and indeed recent research has shown this improves overall performance (Rozovskaya and Roth, 2016). Table 6 shows precision, recall and F 0.5 for each of the error types in our proposed framework for each team in CoNLL-2014. As some error types are more common than others, we also provide the TP, FP and FN counts used to make this table in Appendix B, Table 11.\n\n\nGeneral Error Types\n\nOverall, CAMB was the most successful team in terms of error types, achieving the highest Fscore in 10 (out of 24) error categories, followed by AMU, who scored highest in 6 categories. All but 3 teams (IITB, IPN and POST) achieved the best score in at least 1 category, which suggests that different approaches to GEC complement different error types. Only CAMB attempted to correct at least 1 error from every category.\n\nOther interesting observations we can make from this  Table 6: Precision, recall and F 0.5 for each team and error type. A dash indicates the team's system did not attempt to correct the given error type (TP+FP = 0). The highest F-score for each type is highlighted.  Table 7: Detailed breakdown of Determiner errors for two teams.\n\n\u2022 Despite the prevalence of spell checkers nowadays, many teams did not seem to employ them; this would have been an easy way to boost overall performance.\n\n\u2022 Although several teams built specialised classifiers for DET and PREP errors, CAMB's hybrid MT approach still outperformed them. This might be because the classifiers were trained using a different error type framework however.\n\n\u2022 CUUI's classifiers significantly outperformed all other approaches at ORTH and VERB:FORM errors. This suggests classifiers are well-suited to these error types.\n\n\u2022 Although UFC's rule-based approach was the best at VERB:SVA errors, CUUI's classifier was not very far behind.\n\n\u2022 Only AMU managed to correct any CONJ errors.\n\n\u2022 Content word errors (i.e. ADJ, ADV, NOUN and VERB) were unsurprisingly very difficult for all teams.\n\n\nDetailed Error Types\n\nIn addition to analysing general error types, the modular design of our framework also allows us to evaluate error type performance at an even greater level of detail. For example, Table 7 shows the breakdown of Determiner errors for two teams using different approaches in terms of edit operation. Note that this is a representative example of detailed error type performance, as an analysis of all error type combinations for all teams would take up too much space.  While CAMB's hybrid MT approach achieved a higher score than CUUI's classifier overall, our more detailed evaluation reveals that CUUI actually outperformed CAMB at Replacement Determiner errors. We also learn that CAMB scored twice as highly on M:DET and U:DET than it did on R:DET and that CUUI's significantly higher U:DET recall was offset by a lower precision. Ultimately, this shows that even though one approach might be better than another overall, different approaches may still have complementary strengths.\n\n\nMulti Token Errors\n\nAnother benefit of explicitly annotating all hypothesis edits is that edit spans become fixed; this means we can evaluate system performance in terms of edit size. Table 8 hence shows the overall performance for each team at correcting multitoken edits, where a multi-token edit is an edit that has at least two tokens on either side. In the CoNLL-2014 test set, there are roughly 220 such edits (about 10% of all edits).\n\nIn general, teams did not do well at multi-token edits. In fact only three teams achieved scores greater than 10% F 0.5 and all of them used MT (AMU, CAMB, UMC). This is significant because recent work has suggested that the main goal of GEC should be to produce fluent-sounding, rather than just grammatical sentences, even though this often requires complex multi-token edits (Sakaguchi et al., 2016). If no system is particularly adept at correcting multi-token errors however, robust fluency correction will likely require more sophisticated methods than are currently available. \n\n\nDetection vs. Correction\n\nAnother important aspect of GEC that is seldom reported in the literature is that of error detection; i.e. the extent to which a system can identify erroneous tokens in text. This can be calculated by comparing the edit overlap between the hypothesis and reference files regardless of the proposed correction in a manner similar to Recognition evaluation in the HOO shared tasks for GEC (Dale and Kilgarriff, 2011). Figure 1 hence shows how each team's score for detection differed in relation to their score for correction. While CAMB scored highest for detection overall, it is interesting to note that CUUI ultimately performed slightly better than CAMB at correction. This suggests CUUI was more successful at correcting the errors they detected than CAMB. In contrast, IPN and PKU are notable for detecting significantly more errors than they were able to correct. Nevertheless, a system's ability to detect errors, even if it is unable to correct them, is still likely to be valuable information to a learner (Rei and Yannakoudakis, 2016).\n\nFinally, although we do not do so here, our scorer is also capable of providing a detailed error type breakdown for detection.\n\n\nConclusion\n\nIn this paper, we described ERRANT, a grammatical ERRor ANnotation Toolkit designed to au-tomatically annotate parallel error correction data with explicit edit spans and error type information. ERRANT can be used to not only facilitate a detailed error type evaluation in GEC, but also to standardise existing error correction corpora and reduce annotator workload. We release ERRANT with this paper.\n\nOur approach makes use of previous work to align sentences based on linguistic intuition and then introduces a new rule-based framework to classify edits. This framework is entirely dataset independent, and relies only on automatically obtained information such as POS tags and lemmas. A small-scale evaluation of our classifier found that each rater considered >95% of the predicted error types as either \"Good\" (85%) or \"Acceptable\" (10%).\n\nWe demonstrated the value of ERRANT by carrying out a detailed evaluation of system error type performance for all teams in the CoNLL-2014 shared task on Grammatical Error Correction. We found that different systems had different strengths and weaknesses which we hope researchers can exploit to further improve general performance.    ADJ   TP  2  5  0  0  0  0  2  0  1  0  0  0  FP  39  50  0  3  2  3  1  4  7  8  0  21  FN  28  31  30  23  23  25  26  33  27  20  20  26   ADJ:FORM   TP  5  6  3  2  0  3  2  1  2  0  0  3  FP  4  2  0  0  1  6  0  1  23  0  0  0  FN  3  4  6  3  5  5  5  6  3  5  5  2   ADV   TP  1  9  0  0  0  0  0  0  0  1  0  5  FP  14  69  1  1  1  2  1  0  4  20  0   These results were used to make Table 6.\n\nFigure 1 :\n1The difference between detection and correction scores for each team overall.\n\n\nHe are asleep now. [are \u2192 is]: SVA To handle these cases, we hence wrote the following ordered rules: 1. Are the lower case forms of both sides of theIS \u2192 is]: orthography \n\n(b) He iss asleep now. \n[iss \u2192 is]: spelling \n\n(c) He has asleep now. [has \u2192 is]: verb \n\n(d) He being asleep now. [being \u2192 is]: form \n\n(e) He was asleep now. [was \u2192 is]: tense \n\n(f) edit the same? (a) \n\n2. Is the original token a real word? (b) \n\n3. Do both sides of the edit have the same \nlemma? (c) \n\n4. Is one side of the edit a gerund (VBG) or par-\nticiple (VBN)? (d) \n\n5. Is one side of the edit in the past tense \n(VBD)? (e) \n\n6. Is one side of the edit in the 3rd person \npresent tense (VBZ)? (f) \n\n\n\n\nVERB:TENSE Verb TenseIncludes inflectional and periphrastic tense, modal verbs and passivization. eats \u2192 ate, eats \u2192 has eaten, eats \u2192 can eat, eats \u2192 was eaten WO Word Order only can \u2192 can onlyCode \n\nMeaning \nDescription / Example \nADJ \nAdjective \nbig \u2192 wide \n\nADJ:FORM \nAdjective Form \nComparative or superlative adjective errors. \ngoodest \u2192 best, bigger \u2192 biggest, more easy \u2192 easier \nADV \nAdverb \nspeedily \u2192 quickly \nCONJ \nConjunction \nand \u2192 but \nCONTR \nContraction \nn't \u2192 not \nDET \nDeterminer \nthe \u2192 a \n\nMORPH \nMorphology \nTokens have the same lemma but nothing else in common. \nquick (adj) \u2192 quickly (adv) \nNOUN \nNoun \nperson \u2192 people \n\nNOUN:INFL \nNoun Inflection \nCount-mass noun errors. \ninformations \u2192 information \nNOUN:NUM \nNoun Number \ncat \u2192 cats \nNOUN:POSS \nNoun Possessive \nfriends \u2192 friend's \n\nORTH \nOrthography \nCase and/or whitespace errors. \nBestfriend \u2192 best friend \n\nOTHER \nOther \nErrors that do not fall into any other category (e.g. paraphrasing). \nat his best \u2192 well, job \u2192 professional \nPART \nParticle \n(look) in \u2192 (look) at \nPREP \nPreposition \nof \u2192 at \nPRON \nPronoun \nours \u2192 ourselves \nPUNCT \nPunctuation \n! \u2192 . \nSPELL \nSpelling \ngenectic \u2192 genetic, color \u2192 colour \nUNK \nUnknown \nThe annotator detected an error but was unable to correct it. \nVERB \nVerb \nambulate \u2192 walk \n\nVERB:FORM \nVerb Form \nInfinitives (with or without \"to\"), gerunds (-ing) and participles. \nto eat \u2192 eating, dancing \u2192 danced \n\nVERB:INFL \nVerb Inflection \nMisapplication of tense morphology. \ngetted \u2192 got, fliped \u2192 flipped \nVERB:SVA \nSubject-Verb Agreement (He) have \u2192 (He) has \n\n\n\nTable 2 :\n2The list of 25 main error categories in our new framework with examples and explanations.\n\nTable 4 :\n4Overall scores for each team in CoNLL-\n2014 using gold and auto references with both the \nM 2 scorer and our simpler edit comparison ap-\nproach. All scores are in terms of F 0.5 . \n\n\n\nTable 5 :\n5Precision, recall and F 0.5 for Missing, Unnecessary, and Replacement errors for each team.\n\ntable include :\nincludeAMU CAMB CUUI \n\nIITB \nIPN NTHU \nPKU POST \nRAC SJTU \nUFC UMC \n\nADJ \n\nP \n4.88 \n9.09 \n-\n0.00 \n0.00 \n0.00 \n66.67 \n0.00 \n12.50 \n0.00 \n-\n0.00 \nR \n6.67 \n13.89 \n-\n0.00 \n0.00 \n0.00 \n7.14 \n0.00 \n3.57 \n0.00 \n-\n0.00 \nF 0.5 \n5.15 \n9.77 \n-\n0.00 \n0.00 \n0.00 \n25.00 \n0.00 \n8.33 \n0.00 \n-\n0.00 \n\nADJ:FORM \n\nP \n55.56 \n75.00 100.00 100.00 \n0.00 \n33.33 100.00 \n50.00 \n8.00 \n-\n-100.00 \nR \n62.50 \n60.00 \n33.33 \n40.00 \n0.00 \n37.50 \n28.57 \n14.29 \n40.00 \n-\n-\n60.00 \nF 0.5 \n56.82 \n71.43 \n71.43 \n76.92 \n0.00 \n34.09 \n66.67 \n33.33 \n9.52 \n-\n-\n88.24 \n\nADV \n\nP \n6.67 \n11.54 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n-\n0.00 \n4.76 \n-\n8.77 \nR \n2.94 \n20.45 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n-\n0.00 \n3.03 \n-\n12.50 \nF 0.5 \n5.32 \n12.64 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n-\n0.00 \n4.27 \n-\n9.33 \n\nCONJ \n\nP \n6.25 \n0.00 \n-\n-\n0.00 \n0.00 \n-\n-\n-\n0.00 \n-\n0.00 \nR \n7.69 \n0.00 \n-\n-\n0.00 \n0.00 \n-\n-\n-\n0.00 \n-\n0.00 \nF 0.5 \n6.49 \n0.00 \n-\n-\n0.00 \n0.00 \n-\n-\n-\n0.00 \n-\n0.00 \n\nCONTR \n\nP \n29.17 \n40.00 \n46.15 \n-\n0.00 \n-\n-\n33.33 \n0.00 \n66.67 \n-\n28.57 \nR \n100.00 \n33.33 \n85.71 \n-\n0.00 \n-\n-\n57.14 \n0.00 \n40.00 \n-\n33.33 \nF 0.5 \n33.98 \n38.46 \n50.85 \n-\n0.00 \n-\n-\n36.36 \n0.00 \n58.82 \n-\n29.41 \n\nDET \n\nP \n33.33 \n36.16 \n30.92 \n21.43 \n0.00 \n36.03 \n29.35 \n26.09 \n0.00 \n43.88 \n-\n36.21 \nR \n14.09 \n43.03 \n51.91 \n0.92 \n0.00 \n28.46 \n7.85 \n49.41 \n0.00 \n12.54 \n-\n23.66 \nF 0.5 \n26.18 \n37.35 \n33.64 \n3.92 \n0.00 \n34.21 \n18.96 \n28.81 \n0.00 \n29.25 \n-\n32.74 \n\nMORPH \n\nP \n55.56 \n59.15 \n55.88 \n28.57 \n1.16 \n27.87 \n20.80 \n27.78 \n32.69 100.00 \n40.00 \n43.75 \nR \n48.91 \n47.73 \n20.88 \n5.41 \n1.39 \n21.52 \n30.59 \n12.50 \n21.25 \n2.74 \n5.00 \n15.91 \nF 0.5 \n54.09 \n56.45 \n41.85 \n15.38 \n1.20 \n26.32 \n22.22 \n22.32 \n29.51 \n12.35 \n16.67 \n32.41 \n\nNOUN \n\nP \n20.90 \n25.27 \n0.00 \n28.57 \n4.35 \n0.00 \n0.00 \n10.00 \n10.53 \n0.00 \n-\n27.78 \nR \n12.39 \n19.49 \n0.00 \n2.20 \n2.17 \n0.00 \n0.00 \n1.92 \n1.92 \n0.00 \n-\n9.90 \nF 0.5 \n18.37 \n23.86 \n0.00 \n8.40 \n3.62 \n0.00 \n0.00 \n5.43 \n5.56 \n0.00 \n-\n20.41 \n\nNOUN:INFL \n\nP \n60.00 \n60.00 \n50.00 \n-\n25.00 100.00 \n62.50 \n66.67 \n66.67 \n0.00 \n-\n-\nR \n85.71 \n66.67 \n71.43 \n-\n16.67 \n33.33 \n62.50 \n57.14 \n66.67 \n0.00 \n-\n-\nF 0.5 \n63.83 \n61.22 \n53.19 \n-\n22.73 \n71.43 \n62.50 \n64.52 \n66.67 \n0.00 \n-\n-\n\nNOUN:NUM \n\nP \n49.42 \n44.20 \n44.06 \n41.18 \n14.38 \n44.05 \n29.39 \n31.05 \n29.00 \n54.29 \n-\n44.29 \nR \n56.14 \n53.74 \n59.49 \n3.87 \n11.28 \n47.62 \n42.54 \n56.20 \n36.45 \n10.27 \n-\n16.94 \nF 0.5 \n50.63 \n45.83 \n46.47 \n14.06 \n13.63 \n44.72 \n31.33 \n34.10 \n30.23 \n29.23 \n-\n33.48 \n\nNOUN:POSS \n\nP \n20.00 \n66.67 \n-\n-\n-\n-\n14.29 \n0.00 \n0.00 \n25.00 \n-\n50.00 \nR \n14.29 \n10.53 \n-\n-\n-\n-\n5.26 \n0.00 \n0.00 \n4.55 \n-\n5.00 \nF 0.5 \n18.52 \n32.26 \n-\n-\n-\n-\n10.64 \n0.00 \n0.00 \n13.16 \n-\n17.86 \n\nORTH \n\nP \n60.00 \n66.67 \n73.81 \n-\n3.45 \n0.00 \n28.57 \n49.32 \n16.57 \n-\n-\n50.00 \nR \n11.11 \n40.00 \n59.62 \n-\n4.55 \n0.00 \n6.90 \n64.29 \n49.12 \n-\n-\n17.24 \nF 0.5 \n31.91 \n58.82 \n70.45 \n-\n3.62 \n0.00 \n17.54 \n51.72 \n19.10 \n-\n-\n36.23 \n\nOTHER \n\nP \n20.34 \n23.60 \n10.34 \n0.00 \n2.33 \n1.37 \n14.29 \n10.00 \n0.00 \n0.00 \n-\n11.58 \nR \n6.92 \n10.03 \n0.83 \n0.00 \n0.31 \n0.58 \n0.58 \n1.13 \n0.00 \n0.00 \n-\n3.15 \nF 0.5 \n14.65 \n18.57 \n3.14 \n0.00 \n1.01 \n1.07 \n2.49 \n3.90 \n0.00 \n0.00 \n-\n7.54 \n\nPART \n\nP \n71.43 \n33.33 \n25.00 \n-\n-\n16.67 \n-\n-\n-\n50.00 \n-\n20.00 \nR \n20.83 \n15.38 \n4.76 \n-\n-\n21.74 \n-\n-\n-\n9.52 \n-\n11.11 \nF 0.5 \n48.08 \n27.03 \n13.51 \n-\n-\n17.48 \n-\n-\n-\n27.03 \n-\n17.24 \n\nPREP \n\nP \n47.56 \n41.44 \n33.33 \n75.00 \n0.00 \n10.71 \n-\n21.74 \n0.00 \n36.59 \n-\n20.53 \nR \n16.05 \n35.66 \n13.49 \n1.44 \n0.00 \n12.35 \n-\n2.17 \n0.00 \n7.18 \n-\n13.36 \nF 0.5 \n34.15 \n40.14 \n25.76 \n6.70 \n0.00 \n11.01 \n-\n7.76 \n0.00 \n20.11 \n-\n18.54 \n\nPRON \n\nP \n41.18 \n20.37 \n0.00 \n0.00 \n11.11 \n50.00 100.00 \n27.27 \n5.00 \n0.00 \n-\n22.92 \nR \n9.72 \n13.41 \n0.00 \n0.00 \n1.69 \n2.82 \n1.54 \n4.62 \n1.52 \n0.00 \n-\n13.92 \nF 0.5 \n25.00 \n18.46 \n0.00 \n0.00 \n5.26 \n11.49 \n7.25 \n13.76 \n3.42 \n0.00 \n-\n20.30 \n\nPUNCT \n\nP \n25.00 \n60.47 \n37.21 100.00 \n0.00 \n44.83 \n-\n27.27 \n0.00 \n5.00 \n-\n43.02 \nR \n3.52 \n15.48 \n10.60 \n1.85 \n0.00 \n8.97 \n-\n6.34 \n0.00 \n0.96 \n-\n23.13 \nF 0.5 \n11.26 \n38.24 \n24.77 \n8.62 \n0.00 \n24.90 \n-\n16.42 \n0.00 \n2.72 \n-\n36.71 \n\nSPELL \n\nP \n76.92 \n77.55 \n0.00 \n0.00 \n25.00 \n0.00 \n44.17 \n68.63 \n73.98 \n-\n-100.00 \nR \n63.83 \n41.76 \n0.00 \n0.00 \n4.23 \n0.00 \n71.29 \n71.43 \n85.85 \n-\n-\n1.37 \nF 0.5 \n73.89 \n66.20 \n0.00 \n0.00 \n12.61 \n0.00 \n47.81 \n69.17 \n76.09 \n-\n-\n6.49 \n\nVERB \n\nP \n18.84 \n15.12 \n-\n0.00 \n7.69 \n0.00 \n14.29 \n0.00 \n0.00 \n0.00 \n-\n16.33 \nR \n8.23 \n8.33 \n-\n0.00 \n0.74 \n0.00 \n0.70 \n0.00 \n0.00 \n0.00 \n-\n5.37 \nF 0.5 \n14.98 \n13.00 \n-\n0.00 \n2.66 \n0.00 \n2.94 \n0.00 \n0.00 \n0.00 \n-\n11.59 \n\nVERB:FORM \n\nP \n34.92 \n36.36 \n68.75 \n0.00 \n8.77 \n35.11 \n30.77 \n25.00 \n34.41 \n28.57 \n-\n31.11 \nR \n23.40 \n25.00 \n24.18 \n0.00 \n5.75 \n35.11 \n35.56 \n3.45 \n32.65 \n4.65 \n-\n16.09 \nF 0.5 \n31.79 \n33.33 \n50.23 \n0.00 \n7.94 \n35.11 \n31.62 \n11.11 \n34.04 \n14.08 \n-\n26.22 \n\nVERB:INFL \n\nP \n100.00 100.00 \n-\n-100.00 100.00 \n50.00 100.00 100.00 \n-\n0.00 \n-\nR \n100.00 100.00 \n-\n-\n50.00 \n50.00 \n50.00 \n50.00 100.00 \n-\n0.00 \n-\nF 0.5 100.00 100.00 \n-\n-\n83.33 \n83.33 \n50.00 \n83.33 100.00 \n-\n0.00 \n-\n\nVERB:SVA \n\nP \n49.09 \n44.05 \n54.80 \n50.00 \n24.56 \n50.56 \n56.25 \n32.69 \n35.56 \n59.09 \n81.58 \n60.00 \nR \n27.55 \n32.74 \n71.85 \n1.12 \n14.58 \n67.16 \n18.75 \n17.35 \n31.07 \n13.83 \n29.25 \n15.00 \nF 0.5 \n42.45 \n41.20 \n57.53 \n5.15 \n21.60 \n53.19 \n40.18 \n27.78 \n34.56 \n35.71 \n60.08 \n37.50 \n\nVERB:TENSE \n\nP \n20.55 \n26.27 \n70.00 \n66.67 \n3.70 \n31.25 \n9.38 \n20.00 \n22.78 \n14.81 100.00 \n31.25 \nR \n8.72 \n17.51 \n4.12 \n1.25 \n0.61 \n2.98 \n3.66 \n2.31 \n20.57 \n2.45 \n0.63 \n12.05 \nF 0.5 \n16.16 \n23.88 \n16.67 \n5.81 \n1.84 \n10.78 \n7.14 \n7.91 \n22.30 \n7.38 \n3.05 \n23.70 \n\nWO \n\nP \n-\n38.89 \n0.00 \n66.67 \n-\n-\n-\n0.00 \n0.00 \n-\n-\n41.18 \nR \n-\n33.33 \n0.00 \n14.29 \n-\n-\n-\n0.00 \n0.00 \n-\n-\n35.00 \nF 0.5 \n-\n37.63 \n0.00 \n38.46 \n-\n-\n-\n0.00 \n0.00 \n-\n-\n39.77 \n\n\n\nTable 8 :\n8Each team's performance at correcting \nmulti-token edits; i.e. there are at least two tokens \non one side of the edit. \n\n\n\n\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A new dataset and method for automatically grading ESOL texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Portland, Oregon, USA, pages 180-189. http://www.aclweb.org/anthology/P11-1019.\n\nTable 9 :\n9There are 55 total possible error types. This table shows all of them except UNK, which indicates an uncorrected error. A dash indicates an impossible combination. B TP, FP and FN counts for various CoNLL-2014 resultsAMU \nCAMB \nCUUI \nIITB \nType TP FP \nFN \nTP FP \nFN \nTP FP \nFN \nTP FP \nFN \nMissing \n58 \n74 \n347 131 154 \n310 \n77 215 \n347 \n2 \n11 \n336 \nReplacement 428 722 1162 477 794 1219 381 449 1277 \n20 \n47 1320 \nUnnecessary \n0 \n0 \n412 125 365 \n330 158 304 \n316 \n6 \n7 \n385 \n\nIPN \nNTHU \nPKU \nPOST \nType TP FP \nFN \nTP FP \nFN \nTP FP \nFN \nTP FP \nFN \nMissing \n1 \n34 \n339 \n46 \n88 \n358 \n16 \n32 \n350 \n52 115 \n344 \nReplacement \n53 484 1319 299 784 1262 279 663 1243 312 629 1302 \nUnnecessary \n0 \n2 \n389 \n65 122 \n342 \n0 \n1 \n397 155 434 \n317 \n\nRAC \nSJTU \nUFC \nUMC \nType TP FP \nFN \nTP FP \nFN \nTP FP \nFN \nTP FP \nFN \nMissing \n1 \n65 \n368 \n15 \n9 \n323 \n0 \n0 \n339 \n99 148 \n321 \nReplacement 325 780 1236 \n47 \n46 1325 \n36 \n14 1326 143 269 1331 \nUnnecessary \n0 \n5 \n407 \n45 210 \n351 \n0 \n0 \n381 \n74 365 \n357 \n\n\n\nTable 10 :\n10True Positive, False Positive and False Negative counts for each team in terms of Missing, Replacement and Unnecessary edits. The total number of edits may vary for each system, as this depends on the individual references that are chosen during evaluation. These results were used to makeTable 5.AMU CAMB CUUI IITB IPN NTHU PKU POST RAC SJTU UFC UMC\n\nTable 11 :\n11True Positive, False Positive and False Negative counts for each error type for each team.\nhttps://github.com/chrisjbryant/errant\nDamerau-Levenshtein is an extension of Levenshtein that also handles transpositions; e.g. AB\u2192BA\nhttps://spacy.io/ 4 http://www.nltk.org/ 5 http://universaldependencies.org/tagset-conversion/ en-penn-uposf.html 6 https://sourceforge.net/projects/wordlist/files/speller/ 2017.01.22/\nThe distance is controlled by the max unchanged words parameter which is set to 2 by default.\n\nHow far are we from fully automatic high quality grammatical error correction?. Christopher Bryant, Hwee Tou Ng, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaLong Papers). Association for Computational LinguisticsChristopher Bryant and Hwee Tou Ng. 2015. How far are we from fully automatic high quality gram- matical error correction? In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers). Association for Computa- tional Linguistics, Beijing, China, pages 697-707. http://www.aclweb.org/anthology/P15-1068.\n\nBetter evaluation for grammatical error correction. Daniel Dahlmeier, Hwee Tou Ng, Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMontr\u00e9al, CanadaAssociation for Computational LinguisticsDaniel Dahlmeier and Hwee Tou Ng. 2012. Bet- ter evaluation for grammatical error correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Montr\u00e9al, Canada, pages 568-572. http://www.aclweb.org/anthology/N12-1067.\n\nHelping Our Own: The HOO 2011 pilot shared task. Robert Dale, Adam Kilgarriff, Proceedings of the 13th European Workshop on Natural Language Generation. the 13th European Workshop on Natural Language GenerationStroudsburg, PA, USAAssociation for Computational LinguisticsRobert Dale and Adam Kilgarriff. 2011. Help- ing Our Own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation. Association for Computational Linguistics, Strouds- burg, PA, USA, ENLG '11, pages 242-249.\n\nAn Introduction to the Bootstrap. Bradley Efron, Robert J Tibshirani, Chapman & HallNew YorkBradley Efron and Robert J. Tibshirani. 1993. An In- troduction to the Bootstrap. Chapman & Hall, New York.\n\nTowards a standard evaluation method for grammatical error detection and correction. Mariano Felice, Ted Briscoe, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics. the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational LinguisticsDenver, ColoradoMariano Felice and Ted Briscoe. 2015. Towards a stan- dard evaluation method for grammatical error de- tection and correction. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics, Denver, Colorado, pages 578- 587. http://www.aclweb.org/anthology/N15-1060.\n\nAutomatic extraction of learner errors in ESL sentences using linguistically enhanced alignments. Mariano Felice, Christopher Bryant, Ted Briscoe, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee. COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing CommitteeOsaka, JapanMariano Felice, Christopher Bryant, and Ted Briscoe. 2016. Automatic extraction of learner errors in ESL sentences using linguistically enhanced align- ments. In Proceedings of COLING 2016, the 26th International Conference on Computational Lin- guistics: Technical Papers. The COLING 2016 Or- ganizing Committee, Osaka, Japan, pages 825-835. http://aclweb.org/anthology/C16-1079.\n\nGrammatical error correction using hybrid systems and type filtering. Mariano Felice, Zheng Yuan, \u00d8istein E Andersen, Helen Yannakoudakis, Ekaterina Kochmar, Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics. the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational LinguisticsBaltimore, MarylandMariano Felice, Zheng Yuan, \u00d8istein E. Andersen, He- len Yannakoudakis, and Ekaterina Kochmar. 2014. Grammatical error correction using hybrid systems and type filtering. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computa- tional Linguistics, Baltimore, Maryland, pages 15- 24. http://www.aclweb.org/anthology/W14-1702.\n\nHuman evaluation of grammatical error correction systems. Roman Grundkiewicz, Marcin Junczys-Dowmunt, Edward Gillian, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational LinguisticsLisbon, PortugalRoman Grundkiewicz, Marcin Junczys-Dowmunt, and Edward Gillian. 2015. Human evaluation of gram- matical error correction systems. In Proceedings of the 2015 Conference on Empirical Methods in Natu- ral Language Processing. Association for Computa- tional Linguistics, Lisbon, Portugal, pages 461-470. http://aclweb.org/anthology/D15-1052.\n\nGrammatical error detection using tagger disagreement. Anubhav Gupta, Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics. the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational LinguisticsBaltimore, MarylandAnubhav Gupta. 2014. Grammatical error detec- tion using tagger disagreement. In Proceedings of the Eighteenth Conference on Computa- tional Natural Language Learning: Shared Task. Association for Computational Lin- guistics, Baltimore, Maryland, pages 49-52. http://www.aclweb.org/anthology/W14-1706.\n\nThe AMU system in the CoNLL-2014 shared task: Grammatical error correction by dataintensive and feature-rich statistical machine translation. Marcin Junczys, - Dowmunt, Roman Grundkiewicz, Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. the Eighteenth Conference on Computational Natural Language Learning: Shared TaskBaltimore, MarylandAssociation for Computational LinguisticsMarcin Junczys-Dowmunt and Roman Grundkiewicz. 2014. The AMU system in the CoNLL-2014 shared task: Grammatical error correction by data- intensive and feature-rich statistical machine trans- lation. In Proceedings of the Eighteenth Confer- ence on Computational Natural Language Learn- ing: Shared Task. Association for Computational Linguistics, Baltimore, Maryland, pages 25-33. http://www.aclweb.org/anthology/W14-1703.\n\nGround truth for grammatical error correction metrics. Courtney Napoles, Keisuke Sakaguchi, Matt Post, Joel Tetreault, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaShort Papers). Association for Computational LinguisticsCourtney Napoles, Keisuke Sakaguchi, Matt Post, and Joel Tetreault. 2015. Ground truth for grammati- cal error correction metrics. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol- ume 2: Short Papers). Association for Computa- tional Linguistics, Beijing, China, pages 588-593. http://www.aclweb.org/anthology/P15-2097.\n\n. Hwee Tou Ng, Mei Siew, Ted Wu, Christian Briscoe, Hadiwinoto, Raymond Hendy Susanto, andHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Chris- tian Hadiwinoto, Raymond Hendy Susanto, and\n\nThe CoNLL-2014 shared task on grammatical error correction. Christopher Bryant, Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. ACL. the Eighteenth Conference on Computational Natural Language Learning: Shared Task. ACLBaltimore, Maryland, USAChristopher Bryant. 2014. The CoNLL-2014 shared task on grammatical error correction. In Pro- ceedings of the Eighteenth Conference on Com- putational Natural Language Learning: Shared Task. ACL, Baltimore, Maryland, USA, pages 1-14. http://aclweb.org/anthology/W/W14/W14- 1701.pdf.\n\nFree-marginal multirater kappa: An alternative to Fleiss' fixedmarginal multirater kappa. J Justus, Randolph, Joensuu University Learning and Instruction Symposium. Justus J. Randolph. 2005. Free-marginal mul- tirater kappa: An alternative to Fleiss' fixed- marginal multirater kappa. Joensuu Uni- versity Learning and Instruction Symposium http://files.eric.ed.gov/fulltext/ED490661.pdf.\n\nCompositional sequence labeling models for error detection in learner writing. Marek Rei, Helen Yannakoudakis, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany1Long Papers). Association for Computational LinguisticsMarek Rei and Helen Yannakoudakis. 2016. Com- positional sequence labeling models for error detection in learner writing. In Proceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin- guistics, Berlin, Germany, pages 1181-1191. http://www.aclweb.org/anthology/P16-1112.\n\nThe Illinois-Columbia system in the CoNLL-2014 shared task. Alla Rozovskaya, Kai-Wei Chang, Mark Sammons, Dan Roth, Nizar Habash, Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. the Eighteenth Conference on Computational Natural Language Learning: Shared TaskBaltimore, MarylandAssociation for Computational LinguisticsAlla Rozovskaya, Kai-Wei Chang, Mark Sammons, Dan Roth, and Nizar Habash. 2014. The Illinois- Columbia system in the CoNLL-2014 shared task. In Proceedings of the Eighteenth Confer- ence on Computational Natural Language Learn- ing: Shared Task. Association for Computational Linguistics, Baltimore, Maryland, pages 34-42. http://www.aclweb.org/anthology/W14-1704.\n\nGrammatical error correction: Machine translation and classifiers. Alla Rozovskaya, Dan Roth, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational LinguisticsAlla Rozovskaya and Dan Roth. 2016. Gram- matical error correction: Machine translation and classifiers. In Proceedings of the 54th An- nual Meeting of the Association for Computa- tional Linguistics. Association for Computational Linguistics, Berlin, Germany, pages 2205-2215. http://aclweb.org/anthology/P16-1208.\n\nReassessing the goals of grammatical error correction: Fluency instead of grammaticality. Keisuke Sakaguchi, Courtney Napoles, Matt Post, Joel Tetreault, Transactions of the Association for Computational Linguistics. 4Keisuke Sakaguchi, Courtney Napoles, Matt Post, and Joel Tetreault. 2016. Reassessing the goals of grammatical error correction: Fluency instead of grammaticality. Transactions of the Asso- ciation for Computational Linguistics 4:169-182. https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/ article/view/800.\n\nCorrection detection and error type selection as an ESL educational aid. Ben Swanson, Elif Yamangil, Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics. the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational LinguisticsMontr\u00e9al, CanadaBen Swanson and Elif Yamangil. 2012. Correction detection and error type selection as an ESL ed- ucational aid. In Proceedings of the 2012 Con- ference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics, Montr\u00e9al, Canada, pages 357- 361. http://www.aclweb.org/anthology/N12-1037.\n\nImproved correction detection in revised ESL sentences. Huichao Xue, Rebecca Hwa, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational LinguisticsBaltimore, MarylandAssociation for Computational Linguistics2Short Papers)Huichao Xue and Rebecca Hwa. 2014. Improved correction detection in revised ESL sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Baltimore, Maryland, pages 599-604. http://www.aclweb.org/anthology/P14-2098.\n", "annotations": {"author": "[{\"end\":227,\"start\":135},{\"end\":316,\"start\":228},{\"end\":402,\"start\":317}]", "publisher": null, "author_last_name": "[{\"end\":153,\"start\":147},{\"end\":242,\"start\":236},{\"end\":328,\"start\":321}]", "author_first_name": "[{\"end\":146,\"start\":135},{\"end\":235,\"start\":228},{\"end\":320,\"start\":317}]", "author_affiliation": "[{\"end\":226,\"start\":155},{\"end\":315,\"start\":244},{\"end\":401,\"start\":330}]", "title": "[{\"end\":84,\"start\":1},{\"end\":486,\"start\":403}]", "venue": "[{\"end\":575,\"start\":488}]", "abstract": "[{\"end\":1566,\"start\":753}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2752,\"start\":2731},{\"end\":3147,\"start\":3130},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3266,\"start\":3242},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3291,\"start\":3266},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3311,\"start\":3291},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3332,\"start\":3311},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3358,\"start\":3332},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3381,\"start\":3358},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3884,\"start\":3863},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3973,\"start\":3946},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4237,\"start\":4219},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4418,\"start\":4398},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5075,\"start\":5048},{\"end\":5350,\"start\":5322},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5481,\"start\":5457},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5594,\"start\":5576},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5785,\"start\":5756},{\"end\":8405,\"start\":8404},{\"end\":11178,\"start\":11167},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12029,\"start\":12013},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12205,\"start\":12184},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12649,\"start\":12625},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12674,\"start\":12649},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12695,\"start\":12674},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15044,\"start\":15020},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15346,\"start\":15318},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15944,\"start\":15919},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15970,\"start\":15949},{\"end\":16953,\"start\":16936},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18061,\"start\":18048},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18272,\"start\":18233},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18613,\"start\":18592},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18843,\"start\":18816},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22562,\"start\":22539},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23188,\"start\":23161},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23818,\"start\":23789}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25637,\"start\":25547},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26321,\"start\":25638},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27901,\"start\":26322},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28003,\"start\":27902},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28198,\"start\":28004},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":28302,\"start\":28199},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":33845,\"start\":28303},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":33979,\"start\":33846},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":34345,\"start\":33980},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":35346,\"start\":34346},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":35711,\"start\":35347},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":35816,\"start\":35712}]", "paragraph": "[{\"end\":2050,\"start\":1582},{\"end\":2538,\"start\":2052},{\"end\":3148,\"start\":2540},{\"end\":3460,\"start\":3150},{\"end\":3708,\"start\":3480},{\"end\":3790,\"start\":3710},{\"end\":3885,\"start\":3792},{\"end\":4381,\"start\":3887},{\"end\":4942,\"start\":4383},{\"end\":5670,\"start\":4969},{\"end\":6353,\"start\":5708},{\"end\":6652,\"start\":6355},{\"end\":6675,\"start\":6654},{\"end\":7130,\"start\":6677},{\"end\":7804,\"start\":7164},{\"end\":7927,\"start\":7806},{\"end\":8588,\"start\":7948},{\"end\":9237,\"start\":8609},{\"end\":9944,\"start\":9239},{\"end\":10886,\"start\":9970},{\"end\":11353,\"start\":10888},{\"end\":12030,\"start\":11355},{\"end\":12326,\"start\":12032},{\"end\":12722,\"start\":12349},{\"end\":13302,\"start\":12724},{\"end\":13569,\"start\":13304},{\"end\":14321,\"start\":13607},{\"end\":14835,\"start\":14323},{\"end\":15579,\"start\":14837},{\"end\":15972,\"start\":15614},{\"end\":16799,\"start\":15974},{\"end\":17343,\"start\":16835},{\"end\":18384,\"start\":17362},{\"end\":19108,\"start\":18386},{\"end\":19553,\"start\":19132},{\"end\":19886,\"start\":19555},{\"end\":20043,\"start\":19888},{\"end\":20274,\"start\":20045},{\"end\":20438,\"start\":20276},{\"end\":20552,\"start\":20440},{\"end\":20600,\"start\":20554},{\"end\":20704,\"start\":20602},{\"end\":21715,\"start\":20729},{\"end\":22159,\"start\":21738},{\"end\":22745,\"start\":22161},{\"end\":23819,\"start\":22774},{\"end\":23947,\"start\":23821},{\"end\":24363,\"start\":23962},{\"end\":24806,\"start\":24365},{\"end\":25546,\"start\":24808}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":8685,\"start\":8678},{\"end\":11186,\"start\":11179},{\"end\":11500,\"start\":11493},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":15114,\"start\":15107},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":15652,\"start\":15645},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":17538,\"start\":17531},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":17590,\"start\":17582},{\"end\":18852,\"start\":18845},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":19107,\"start\":19099},{\"end\":19616,\"start\":19609},{\"end\":19830,\"start\":19823},{\"end\":20917,\"start\":20910},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":21909,\"start\":21902},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25503,\"start\":25144},{\"end\":25545,\"start\":25538}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1580,\"start\":1568},{\"attributes\":{\"n\":\"2\"},\"end\":3478,\"start\":3463},{\"attributes\":{\"n\":\"3\"},\"end\":4967,\"start\":4945},{\"attributes\":{\"n\":\"3.1\"},\"end\":5706,\"start\":5673},{\"attributes\":{\"n\":\"3.2\"},\"end\":7162,\"start\":7133},{\"attributes\":{\"n\":\"3.3\"},\"end\":7946,\"start\":7930},{\"attributes\":{\"n\":\"3.4\"},\"end\":8607,\"start\":8591},{\"attributes\":{\"n\":\"3.5\"},\"end\":9968,\"start\":9947},{\"attributes\":{\"n\":\"4\"},\"end\":12347,\"start\":12329},{\"attributes\":{\"n\":\"4.1\"},\"end\":13605,\"start\":13572},{\"attributes\":{\"n\":\"4.2\"},\"end\":15612,\"start\":15582},{\"attributes\":{\"n\":\"5\"},\"end\":16833,\"start\":16802},{\"attributes\":{\"n\":\"5.1\"},\"end\":17360,\"start\":17346},{\"attributes\":{\"n\":\"5.2\"},\"end\":19130,\"start\":19111},{\"attributes\":{\"n\":\"5.3\"},\"end\":20727,\"start\":20707},{\"attributes\":{\"n\":\"5.4\"},\"end\":21736,\"start\":21718},{\"attributes\":{\"n\":\"5.5\"},\"end\":22772,\"start\":22748},{\"attributes\":{\"n\":\"6\"},\"end\":23960,\"start\":23950},{\"end\":25558,\"start\":25548},{\"end\":27912,\"start\":27903},{\"end\":28014,\"start\":28005},{\"end\":28209,\"start\":28200},{\"end\":28319,\"start\":28304},{\"end\":33856,\"start\":33847},{\"end\":34356,\"start\":34347},{\"end\":35358,\"start\":35348},{\"end\":35723,\"start\":35713}]", "table": "[{\"end\":26321,\"start\":25790},{\"end\":27901,\"start\":26518},{\"end\":28198,\"start\":28016},{\"end\":33845,\"start\":28327},{\"end\":33979,\"start\":33858},{\"end\":35346,\"start\":34575}]", "figure_caption": "[{\"end\":25637,\"start\":25560},{\"end\":25790,\"start\":25640},{\"end\":26518,\"start\":26324},{\"end\":28003,\"start\":27914},{\"end\":28302,\"start\":28211},{\"end\":34345,\"start\":33982},{\"end\":34575,\"start\":34358},{\"end\":35711,\"start\":35361},{\"end\":35816,\"start\":35726}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23198,\"start\":23190}]", "bib_author_first_name": "[{\"end\":36323,\"start\":36312},{\"end\":36340,\"start\":36332},{\"end\":37217,\"start\":37211},{\"end\":37237,\"start\":37229},{\"end\":37985,\"start\":37979},{\"end\":37996,\"start\":37992},{\"end\":38502,\"start\":38495},{\"end\":38516,\"start\":38510},{\"end\":38518,\"start\":38517},{\"end\":38754,\"start\":38747},{\"end\":38766,\"start\":38763},{\"end\":39650,\"start\":39643},{\"end\":39670,\"start\":39659},{\"end\":39682,\"start\":39679},{\"end\":40442,\"start\":40435},{\"end\":40456,\"start\":40451},{\"end\":40470,\"start\":40463},{\"end\":40472,\"start\":40471},{\"end\":40488,\"start\":40483},{\"end\":40513,\"start\":40504},{\"end\":41265,\"start\":41260},{\"end\":41286,\"start\":41280},{\"end\":41310,\"start\":41304},{\"end\":41983,\"start\":41976},{\"end\":42726,\"start\":42720},{\"end\":42737,\"start\":42736},{\"end\":42752,\"start\":42747},{\"end\":43493,\"start\":43485},{\"end\":43510,\"start\":43503},{\"end\":43526,\"start\":43522},{\"end\":43537,\"start\":43533},{\"end\":44391,\"start\":44388},{\"end\":44401,\"start\":44398},{\"end\":44415,\"start\":44406},{\"end\":44625,\"start\":44614},{\"end\":45222,\"start\":45221},{\"end\":45605,\"start\":45600},{\"end\":45616,\"start\":45611},{\"end\":46290,\"start\":46286},{\"end\":46310,\"start\":46303},{\"end\":46322,\"start\":46318},{\"end\":46335,\"start\":46332},{\"end\":46347,\"start\":46342},{\"end\":47032,\"start\":47028},{\"end\":47048,\"start\":47045},{\"end\":47686,\"start\":47679},{\"end\":47706,\"start\":47698},{\"end\":47720,\"start\":47716},{\"end\":47731,\"start\":47727},{\"end\":48194,\"start\":48191},{\"end\":48208,\"start\":48204},{\"end\":49040,\"start\":49033},{\"end\":49053,\"start\":49046}]", "bib_author_last_name": "[{\"end\":36330,\"start\":36324},{\"end\":36343,\"start\":36341},{\"end\":37227,\"start\":37218},{\"end\":37240,\"start\":37238},{\"end\":37990,\"start\":37986},{\"end\":38007,\"start\":37997},{\"end\":38508,\"start\":38503},{\"end\":38529,\"start\":38519},{\"end\":38761,\"start\":38755},{\"end\":38774,\"start\":38767},{\"end\":39657,\"start\":39651},{\"end\":39677,\"start\":39671},{\"end\":39690,\"start\":39683},{\"end\":40449,\"start\":40443},{\"end\":40461,\"start\":40457},{\"end\":40481,\"start\":40473},{\"end\":40502,\"start\":40489},{\"end\":40521,\"start\":40514},{\"end\":41278,\"start\":41266},{\"end\":41302,\"start\":41287},{\"end\":41318,\"start\":41311},{\"end\":41989,\"start\":41984},{\"end\":42734,\"start\":42727},{\"end\":42745,\"start\":42738},{\"end\":42765,\"start\":42753},{\"end\":43501,\"start\":43494},{\"end\":43520,\"start\":43511},{\"end\":43531,\"start\":43527},{\"end\":43547,\"start\":43538},{\"end\":44386,\"start\":44375},{\"end\":44396,\"start\":44392},{\"end\":44404,\"start\":44402},{\"end\":44423,\"start\":44416},{\"end\":44435,\"start\":44425},{\"end\":44632,\"start\":44626},{\"end\":45229,\"start\":45223},{\"end\":45239,\"start\":45231},{\"end\":45609,\"start\":45606},{\"end\":45630,\"start\":45617},{\"end\":46301,\"start\":46291},{\"end\":46316,\"start\":46311},{\"end\":46330,\"start\":46323},{\"end\":46340,\"start\":46336},{\"end\":46354,\"start\":46348},{\"end\":47043,\"start\":47033},{\"end\":47053,\"start\":47049},{\"end\":47696,\"start\":47687},{\"end\":47714,\"start\":47707},{\"end\":47725,\"start\":47721},{\"end\":47741,\"start\":47732},{\"end\":48202,\"start\":48195},{\"end\":48217,\"start\":48209},{\"end\":49044,\"start\":49041},{\"end\":49057,\"start\":49054}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15484870},\"end\":37157,\"start\":36232},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9613043},\"end\":37928,\"start\":37159},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":18357549},\"end\":38459,\"start\":37930},{\"attributes\":{\"id\":\"b3\"},\"end\":38660,\"start\":38461},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2723528},\"end\":39543,\"start\":38662},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7832335},\"end\":40363,\"start\":39545},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":16548363},\"end\":41200,\"start\":40365},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14441283},\"end\":41919,\"start\":41202},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18514528},\"end\":42576,\"start\":41921},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18318073},\"end\":43428,\"start\":42578},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5092868},\"end\":44371,\"start\":43430},{\"attributes\":{\"id\":\"b11\"},\"end\":44552,\"start\":44373},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":219306476},\"end\":45129,\"start\":44554},{\"attributes\":{\"id\":\"b13\"},\"end\":45519,\"start\":45131},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1521197},\"end\":46224,\"start\":45521},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1547333},\"end\":46959,\"start\":46226},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":18563136},\"end\":47587,\"start\":46961},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11746938},\"end\":48116,\"start\":47589},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1946677},\"end\":48975,\"start\":48118},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14049282},\"end\":49622,\"start\":48977}]", "bib_title": "[{\"end\":36310,\"start\":36232},{\"end\":37209,\"start\":37159},{\"end\":37977,\"start\":37930},{\"end\":38745,\"start\":38662},{\"end\":39641,\"start\":39545},{\"end\":40433,\"start\":40365},{\"end\":41258,\"start\":41202},{\"end\":41974,\"start\":41921},{\"end\":42718,\"start\":42578},{\"end\":43483,\"start\":43430},{\"end\":44612,\"start\":44554},{\"end\":45219,\"start\":45131},{\"end\":45598,\"start\":45521},{\"end\":46284,\"start\":46226},{\"end\":47026,\"start\":46961},{\"end\":47677,\"start\":47589},{\"end\":48189,\"start\":48118},{\"end\":49031,\"start\":48977}]", "bib_author": "[{\"end\":36332,\"start\":36312},{\"end\":36345,\"start\":36332},{\"end\":37229,\"start\":37211},{\"end\":37242,\"start\":37229},{\"end\":37992,\"start\":37979},{\"end\":38009,\"start\":37992},{\"end\":38510,\"start\":38495},{\"end\":38531,\"start\":38510},{\"end\":38763,\"start\":38747},{\"end\":38776,\"start\":38763},{\"end\":39659,\"start\":39643},{\"end\":39679,\"start\":39659},{\"end\":39692,\"start\":39679},{\"end\":40451,\"start\":40435},{\"end\":40463,\"start\":40451},{\"end\":40483,\"start\":40463},{\"end\":40504,\"start\":40483},{\"end\":40523,\"start\":40504},{\"end\":41280,\"start\":41260},{\"end\":41304,\"start\":41280},{\"end\":41320,\"start\":41304},{\"end\":41991,\"start\":41976},{\"end\":42736,\"start\":42720},{\"end\":42747,\"start\":42736},{\"end\":42767,\"start\":42747},{\"end\":43503,\"start\":43485},{\"end\":43522,\"start\":43503},{\"end\":43533,\"start\":43522},{\"end\":43549,\"start\":43533},{\"end\":44388,\"start\":44375},{\"end\":44398,\"start\":44388},{\"end\":44406,\"start\":44398},{\"end\":44425,\"start\":44406},{\"end\":44437,\"start\":44425},{\"end\":44634,\"start\":44614},{\"end\":45231,\"start\":45221},{\"end\":45241,\"start\":45231},{\"end\":45611,\"start\":45600},{\"end\":45632,\"start\":45611},{\"end\":46303,\"start\":46286},{\"end\":46318,\"start\":46303},{\"end\":46332,\"start\":46318},{\"end\":46342,\"start\":46332},{\"end\":46356,\"start\":46342},{\"end\":47045,\"start\":47028},{\"end\":47055,\"start\":47045},{\"end\":47698,\"start\":47679},{\"end\":47716,\"start\":47698},{\"end\":47727,\"start\":47716},{\"end\":47743,\"start\":47727},{\"end\":48204,\"start\":48191},{\"end\":48219,\"start\":48204},{\"end\":49046,\"start\":49033},{\"end\":49059,\"start\":49046}]", "bib_venue": "[{\"end\":36506,\"start\":36345},{\"end\":37384,\"start\":37242},{\"end\":38081,\"start\":38009},{\"end\":38493,\"start\":38461},{\"end\":38961,\"start\":38776},{\"end\":39838,\"start\":39692},{\"end\":40662,\"start\":40523},{\"end\":41449,\"start\":41320},{\"end\":42130,\"start\":41991},{\"end\":42863,\"start\":42767},{\"end\":43710,\"start\":43549},{\"end\":44735,\"start\":44634},{\"end\":45294,\"start\":45241},{\"end\":45719,\"start\":45632},{\"end\":46452,\"start\":46356},{\"end\":47142,\"start\":47055},{\"end\":47804,\"start\":47743},{\"end\":48404,\"start\":48219},{\"end\":49146,\"start\":49059},{\"end\":36668,\"start\":36508},{\"end\":37529,\"start\":37386},{\"end\":38160,\"start\":38083},{\"end\":39149,\"start\":38963},{\"end\":39983,\"start\":39840},{\"end\":40807,\"start\":40664},{\"end\":41581,\"start\":41451},{\"end\":42275,\"start\":42132},{\"end\":42965,\"start\":42865},{\"end\":43872,\"start\":43712},{\"end\":44847,\"start\":44737},{\"end\":45808,\"start\":45721},{\"end\":46554,\"start\":46454},{\"end\":47231,\"start\":47144},{\"end\":48592,\"start\":48406},{\"end\":49239,\"start\":49148}]"}}}, "year": 2023, "month": 12, "day": 17}