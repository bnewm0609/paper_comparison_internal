{"id": 260506635, "updated": "2023-08-05 17:31:13.666", "metadata": {"title": "Meta-Calibration: Meta-Learning of Model Calibration Using Differentiable Expected Calibration Error", "authors": "[{\"first\":\"Ondrej\",\"last\":\"Bohdal\",\"middle\":[]},{\"first\":\"Yongxin\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Timothy\",\"last\":\"Hospedales\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Calibration of neural networks is a topical problem that is becoming increasingly important for real-world use of neural networks. The problem is especially noticeable when using modern neural networks, for which there is significant difference between the model confidence and the confidence it should have. Various strategies have been successfully proposed, yet there is more space for improvements. We propose a novel approach that introduces a differentiable metric for expected calibration error and successfully uses it as an objective for meta-learning, achieving competitive results with state-of-the-art approaches. Our approach presents a new direction of using metalearning to directly optimize model calibration, which we believe will inspire further work in this promising and new direction.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2106-09613", "doi": null}}, "content": {"source": {"pdf_hash": "0d8ef92a738925bb15c9796dd0e792fe473a975a", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e7979d0002bb0f606322a3aaaf831af64dc7ef56", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0d8ef92a738925bb15c9796dd0e792fe473a975a.txt", "contents": "\nMeta-Calibration: Meta-Learning of Model Calibration Using Differentiable Expected Calibration Error\n\n\nOndrej Bohdal \nYongxin Yang \nTimothy Hospedales \nMeta-Calibration: Meta-Learning of Model Calibration Using Differentiable Expected Calibration Error\n\nCalibration of neural networks is a topical problem that is becoming increasingly important for real-world use of neural networks. The problem is especially noticeable when using modern neural networks, for which there is significant difference between the model confidence and the confidence it should have. Various strategies have been successfully proposed, yet there is more space for improvements. We propose a novel approach that introduces a differentiable metric for expected calibration error and successfully uses it as an objective for meta-learning, achieving competitive results with state-of-the-art approaches. Our approach presents a new direction of using metalearning to directly optimize model calibration, which we believe will inspire further work in this promising and new direction.\n\nIntroduction\n\nWhen deploying neural networks to real-world applications, it is crucial that models have accurate estimates of their belief confidence. If a model is over-confident about its predictions, we cannot rely on it. Models that are accurately confident about their predictions can be described as wellcalibrated. However, modern neural networks are known to be badly calibrated (Guo et al., 2017), which has motivated research in the area of calibration.\n\nSeveral calibration approaches have been proposed so far (Guo et al., 2017;Mukhoti et al., 2020;Kumar et al., 2018), but model calibration remains an active area of research where further important advances can be made. Guo et al. (2017) is a foundational work that discovers modern neural networks are typically miscalibrated. Guo et al. (2017) Preliminary work. Under review by the ICML 2021 Workshop on Uncertainty and Robustness in Deep Learning. Copyright 2021 by the author(s). study a variety of potential solutions and find simple posttraining rescaling of the logits -temperature scaling -works relatively well. Kumar et al. (2018) propose a kernel-based measure of calibration that they use as regularization during training of neural networks. Mukhoti et al. (2020) show focal loss -a relatively simple weighted alternative to crossentropy -can be used to train well-calibrated neural networks. The classic Brier score (Brier, 1950), which is the squared error between the softmax vector with probabilities and the ground-truth one-hot encoding, has also been show to work well. Similarly, label smoothing  has also been shown to improve model calibration.\n\nCalibration is typically measured using expected calibration error (ECE). Prior work seeks indirect ways to optimize ECE, since it is not differentiable. We take a more direct approach by deriving a differentiable approximation to ECE (DECE), which can be optimized directly. Since ECE is based on measuring accuracy of predictions binned by confidence, this requires both a differentiable approximation to 0/1 accuracy and to the binning operator. Further, since calibration is highly influenced by model hyperparameters such as regularizers, we introduce a novel meta-learning framework for tuning hyperparameters to optimize calibration without altering the underlying model training strategy.\n\nWe show we can successfully use DECE-driven metalearning to obtain well-calibrated and high-accuracy models. In particular, we meta-learn unit-wise L2 regularization on the classifier layer and demonstrate competitive results on CIFAR-10 and CIFAR-100 benchmarks (Krizhevsky, 2009). To summarize our contributions: 1) We introduce a novel differentiable approximation to calibration error. 2) We analyse the proposed metric in detail and show the approximation closely matches the original non-differentiable version. 3) We demonstrate proof-of-concept results that show the measure can be successfully used as part of meta-learning pipeline to optimize hyperparameters for calibration.\n\n\nMethods\n\n\nPreliminaries\n\nWe first discuss the definition of expected calibration error (ECE) (Naeini et al., 2015), before we derive a differentiable approximation to it. ECE measures the expected difference (in absolute value) between the accuracies and the confidences of the model on examples that belong to different confidence intervals. ECE is defined as\nECE = M m=1 |B m | n |acc (B m ) \u2212 conf (B m )| ,\nwhere accuracy and confidence for bin B m are\nacc (B m ) = 1 |B m | i\u2208Bm 1 (\u0177 i = y i ) conf (B m ) = 1 |B m | i\u2208Bmp i .\nThere are M interval bins each of size 1/M and n samples. Confidencep i is the probability of the top prediction as given by the model for example i. We group the confidences into their corresponding bins, with bin B m covering interval ( m\u22121 M , m M ]. The predicted class of example i is\u0177 i , while y i is the true class of example i and 1 is an indicator function.\n\nECE metric is not differentiable because assigning examples into bins is not differentiable and also accuracy is not differentiable due to the indicator function. We propose approximations to both binning and accuracy and derive a new metric called differentiable ECE (DECE).\n\n\nDifferentiable ECE\n\nThere are three main components in ECE: accuracy, confidence and bins. Only confidence is differentiable.\n\nDifferentiable Accuracy: In order to obtain a differentiable approximation to accuracy, we consider approaches that allow us to find a differentiable way to calculate the rank of a given class. Two approaches stand out: differentiable ranking (Blondel et al., 2020) and an all-pairs approach (Qin et al., 2010). While both allow us to approximate the rank in a differentiable way, differentiable ranking can only be done on CPU which would introduce a potential bottleneck for modern applications. All-pairs approach has asymptotic complexity of O(n 2 ) for n classes, while differentiable ranking is O(n log n). However, if the number of classes is not in thousands or millions, differentiable ranking would be slower because of not using GPUs. We use the all-pairs approach to estimate the rank of a given class.\n\nAll-pairs (Qin et al., 2010) calculates a rank of class i as\n[R (\u00b7)] i = 1 + j =i 1 \u03c6 i < \u03c6 j , where \u03c6 are the logits.\nWe can obtain soft ranks by replacing the indicator function with a sigmoid scaled with some temperature value \u03c4 a to obtain reliable estimates of the rank of the top predicted class. Once the rank [R(\u00b7)] l for true class l is calculated, we can estimate the accuracy as acc = max(0, [R (\u00b7)] l \u2212L+1), where L is the total number of classes.\n\nSoft Binning: Our approach is similar to Yang et al. (2018). We take confidencep i for example x i and pass it through one-layer neural network softmax((wp i + b)/\u03c4 b ) parameterized with different values of w and b as explained in Yang et al. (2018), with temperature \u03c4 b to control the binning. This leads to M different probabilities, saying how likely it is thatp i belongs to the specific bin B m\u22081..M . We will denote these probabilities as o m (x i ) = p(B m |p i ).\n\nPutting these parts together, we define DECE using a minibatch of n examples as:\nDECE = M m=1 n i=1 o m (x i ) n |acc (B m ) \u2212 conf (B m )| , acc (B m ) = 1 n i=1 o m (x i ) n i=1 o m (x i )1 (\u0177 i = y i ) , conf (B m ) = 1 n i=1 o m (x i ) n i=1 o m (x i )p i .\n\nMeta-Learning\n\nDifferentiable ECE is only one component -it provides an objective to optimize. But we still must decide how to utilize it. One option could be to directly use it as an extra objective in combination with standard cross-entropy. However, given that calibration is largely influenced by choice of hyperparameters, and to avoid disturbing the standard optimization by multi-task learning with new losses, we explore the novel approach of using DECE as an objective for hyperparameter meta-learning in an outer loop while retaining conventional cross-entropy-driven learning for model training.\n\nA key part of meta-learning is to select suitable metaknowledge (hyperparameters) that we will optimize to achieve the given goal (Hospedales et al., 2021). In our case meta-knowledge will be the L2 regularization coefficients. In particular, we learn L2 regularization coefficients of each weight in the classifier layer. This is similar to the approach of Balaji et al. (2018), who used a similar representation for improving domain generalization. We adopt online meta-learning approach (Luketina et al., 2016) where we alternate base model and meta-knowledge (coefficient) updates. This is an efficient strategy as we do not need to backpropagate through many inner-loop steps or retrain the model from scratch for each update of meta-knowledge.\n\nWe formulate our approach as a bilevel optimization problem. The goal is to find unit-wise L2 regularization coefficients \u03c9 for the classifier layer \u03c6 so that training with them optimizes validation DECE (\u03b8 is the feature extractor):\n\u03c9 * = arg min \u03c9 L val DECE (\u03c6 * \u2022 \u03b8 * (\u03c9)), \u03c6 * , \u03b8 * (\u03c9) = arg min \u03c6,\u03b8 L train CE (\u03c6 \u2022 \u03b8) + \u03c9 \u03c6 2 .\nWhen simulating training during the inner loop, we only update the classifier and keep the feature extractor frozen for efficiency (Balaji et al., 2018). Base model training is done separately using a full model update and more advanced optimizer. We give overview of our meta-learning algorithm in Algorithm 1. The inner loop that trains the main model (line 8) is regularized using the learnable L2 regularization, while the outer loop (line 10) that trains the meta-knowledge (learnable L2 regularization) does not directly use it for evaluating DECE. We backpropagate through one step of update of the main model.\n\nAlgorithm 1 Meta-Calibration 1: Input: \u03b1, \u03b2: inner and outer-loop learning rates 2: Output: trained feature extractor \u03b8, classifier \u03c6 and regularization \u03c9 3: \u03c9 \u223c p(\u03c9) 4: \u03c6, \u03b8 \u223c p(\u03c6), p(\u03b8) 5: while \u03c9 not converged do \nCalculate L i = L CE (f \u03c6\u2022\u03b8 (x t ) , y t ) + \u03c9 \u03c6 2 8: Update \u03b8, \u03c6 \u2190 \u03b8, \u03c6 \u2212 \u03b1\u2207 \u03b8,\u03c6 L i 9: Calculate L o = L DECE (f \u03c6\u2022\u03b8 (x v ) , y v ) 10:\nUpdate \u03c9 \u2190 \u03c9 \u2212 \u03b2\u2207 \u03c9 L o 11: end while 3. Experiments\n\n\nExperiment Settings\n\nWe use ResNet-18 (He et al., 2015) as a model and CIFAR-10 and CIFAR-100 as the benchmarks. We use the implementation from Mukhoti et al. (2020) and use the same hyperparameters. We train the models for 350 epochs, with a multi-step scheduler that decreases the initial learning rate of 0.1 by a factor of 10 after 150 and 250 epochs. The model is trained with SGD with momentum of 0.9, weight decay of 0.0005 and minibatch size of 128. 90% of the original training set is used for training and 10% for validation. In the case of meta-learning, we create a further separate metavalidation set that is of size 10% of the original training data, so we directly train with 80% of the original training data.\n\nFor DECE, we use M = 15 bins and scaling parameters \u03c4 a = 100, \u03c4 b = 0.01. Learnable unit-wise L2 regularization for the classifier layer is optimized using Adam (Kingma and Ba, 2015) optimizer with learning rate of 0.001. The meta-learnable parameters are initialized at 0.0 and their number is 512 \u00d7 C + C, where C is the number of classes.\n\nWe compare our approach with the following: 1) crossentropy, 2) Brier score (Brier, 1950), 3) Weighted MMCE  with \u03bb = 2, 4) Focal loss  with \u03b3 = 3, 5) Adaptive (sample dependent) focal loss (FLSD)  with \u03b3 = 3, 6) Label smoothing (LS) with smothing factor of 0.05. We also explore temperature scaling using the validation set.\n\n\nResults\n\nThe results in Table 1 show means and standard deviations of 3 repetitions of each experiment on CIFAR-10 and CIFAR-100. While temperature scaling (TS) benefits all models, the key column is plain ECE since TS is not always possible, e.g. in online or few-shot learning without a validation set (Kim and Yun, 2020).\n\nThe results confirm our DECE-driven meta-learning obtains strong calibration results while maintaining comparable accuracy to the competitors. The slightly worse test error is due to training with a marginally smaller dataset, but this is an acceptable cost when calibration is the priority. We also study adaptive (Nixon et al., 2019) and classwise (Kull et al., 2019) ECE calibration metrics in Table 3 and 4 in the Appendix. Note that while Brier score, Focal loss, FLSD and LS modify the base model's loss function objective, our Meta-Calibration corresponds exactly to the vanilla crossentropy baseline, but where L2 regularization is tuned by our DECE-driven hyperparameter meta-learning rather than chosen ad-hoc. While we have explored calibration via simple L2 regularization, diverse hyperparameters of the base model can be trained by meta-learning (Hospedales et al., 2021), and future work exploring DECE's application to different meta-learning targets is likely to improve the results further.\n\n\nFurther Analysis\n\n\nDECE vs ECE:\n\nTo assess the quality of our differentiable approximation to ECE, we evaluate DECE and ECE correlation. We trained the same ResNet-18 backbone on both CIFAR-10 and CIFAR-100 benchmarks for 350 epochs, recording DECE and ECE values at various points. The results in Figure 1 show both Spearman and Pearson correlation. In both cases they are close to 1, and become even closer to 1 as training continues. This shows that DECE is an accurate differentiable approximation of ECE.\n\nLearned Regularizers: We also analyse the learned classifier regularization coefficients. We can see in Figure 2 that the values are distributed around 0 (their value at initialization), but their spread indicates that we have successfully meta-learned useful values for the calibration task.\n\nAblation Study: Finally, we perform an ablation study on the design of our approach. The results in Table 2 show various alternative designs in terms of the use of DECE, CE and the alternative calibration metric MMCE  in either conventional or meta-loss roles. From the results we can conclude that: (1) Meta-learning with Table 1. Test errors (%), test ECE (%) and test ECE (%) after applying temperature scaling (TS) optimized with validation set. Mean and standard deviation across 3 repetitions. Optimal temperature is in brackets and is averaged over the 3 repetitions.\n\n\nDataset\n\nLoss Error ECE ECE + TS CIFAR-10 Cross-entropy 4.99 \u00b1 0.14 4.23 \u00b1 0.15 1.16 \u00b1 0.10 (2.30) Brier score (Brier, 1950) 5.27 \u00b1 0.21 1.23 \u00b1 0.03 1.23 \u00b1 0.03 (1.00) MMCE  5.21 \u00b1 0.17 4.41 \u00b1 0.14 1.27 \u00b1 0.10 (2.27) Focal loss  5.17 \u00b1 0.09 2.17 \u00b1 0.03 1.22 \u00b1 0.12 (0.90) FLSD  5.28 \u00b1 0.13 2.26 \u00b1 0.03 1.63 \u00b1 0.18 (0.87) LS-0.05  4.94 \u00b1 0.13 3.63 \u00b1 0.06 1.31 \u00b1 0.08 (0.90) Meta-Calibration ( (Brier, 1950) 23.50 \u00b1 0.17 5.19 \u00b1 0.18 4.21 \u00b1 0.23 (0.90) MMCE  23.64 \u00b1 0.25 8.21 \u00b1 0.20 6. 25 \u00b1 0.18 (1.20) Focal loss  23.02 \u00b1 0.08 2.77 \u00b1 0.23 2.77 \u00b1 0.23 (1.00) FLSD  23.06 \u00b1 0.11 2.50 \u00b1 0.11 2.84 \u00b1 0.48 (0.97) LS-0.05     \n\n\nDiscussion and Conclusion\n\nThis work is the first step in using meta-learning to directly optimize model calibration, and there are many ways how it could be expanded in the future. One direction is to target different types of meta-knowledge -we have used unitwise L2 regularization for the classifier, and there are many other options that could work better. Second direction is to consider different ways of meta-learning, for example implicit meta-learning (Lorraine et al., 2020) could provide a better way to optimize the meta-objective. Third direction is to extend the differentiable metric itself.\n\nWe have demonstrated proof-of-concept approach that shows meta-learning can be used to optimize model calibration via hyperparameter tuning. The results show the approach is competitive with the existing approaches for model calibration, and it is likely different types of metaknowledge and improved meta-optimizers will improve the calibration even further.\n\nWe provide a PyTorch implementation of our approach at https://github.com/ondrejbohdal/ meta-calibration.\n\n\nA. Additional Results\n\nTables 3 and 4 show that optimizing for ECE leads to strong improvements on adaptive ECE (Nixon et al., 2019), although less clear improvements on classwise ECE (Kull et al., 2019). The reason is that adaptive ECE is relatively close to the ECE formulation, while classwise ECE is further away due to the focus on all classes rather than only the top one. \n\n\nof training x t , y t and validation x v , y v examples 7:\n\nFigure 2 .\n2Histograms of the learned unit-wise L2 classifier regularization values. CIFAR-100 has 10 times as many values as CIFAR-10 because of 10 times more classes.\n\n\nECE over vanilla training (M2 vs M0). (2) MMCE does not work well as a meta-objective (M3 vs M0). (3) DECE provides calibration benefit as a secondary loss in conventional learning, but at greater detriment to test error (M1 vs M0). (4) Our DECE-driven meta-objective (M4) is the best overall.22.35 \u00b1 0.27 6.87 \u00b1 0.29 4.37 \u00b1 0.45 (0.90) \nMeta-Calibration (our) \n25.27 \u00b1 0.16 2.96 \u00b1 0.38 3.61 \u00b1 0.54 (0.97) \n\n0 \n50 \n100 150 200 250 300 \n\nEpoch \n\n0.980 \n\n0.985 \n\n0.990 \n\n0.995 \n\n1.000 \n\nCorrelation(ECE, DECE) \n\nCIFAR-10 \n\n0 \n50 \n100 150 200 250 300 \n\nEpoch \n\n0.88 \n\n0.90 \n\n0.92 \n\n0.94 \n\n0.96 \n\n0.98 \n\n1.00 \n\nCIFAR-100 \n\nSpearman \nPearson \n\nFigure 1. Correlation between DECE and ECE for ResNet-18 \nCIFAR-10 and CIFAR-100 is very close to 1. \n\nDECE meta-objective is beneficial for improving calibration \n(M4 vs M0), while CE meta-objective does not generally \nimprove \n\nTable 2 .\n2Ablation study on the design of our approach. CIFAR-10 at the top and CIFAR-100 below it.Model \nMeta-Loss \nLoss \nError \nECE \n\nM0: Vanilla CE \n-\nCE \n4.99 \u00b1 0.14 \n4.23 \u00b1 0.15 \nM1: Multi-task \n-\nCE + DECE 10.24 \u00b1 0.21 \n3.80 \u00b1 0.03 \nM2: Meta-CE \nCE \nCE \n5.93 \u00b1 0.09 \n3.96 \u00b1 0.18 \nM3: Meta-MMCE \nMMCE \nCE \n5.95 \u00b1 0.29 \n9.84 \u00b1 2.62 \nM4: Meta-DECE \nDECE \nCE \n5.69 \u00b1 0.15 \n1.33 \u00b1 0.18 \n\nM0: Vanilla CE \n-\nCE \n22.85 \u00b1 0.17 \n8.79 \u00b1 0.59 \nM1: Multi-task \n-\nCE + DECE 29.49 \u00b1 0.17 \n4.40 \u00b1 0.39 \nM2: Meta-CE \nCE \nCE \n26.47 \u00b1 0.60 \n9.17 \u00b1 3.89 \nM3: Meta-MMCE \nMMCE \nCE \n26.38 \u00b1 0.67 26.41 \u00b1 1.33 \nM4: Meta-DECE \nDECE \nCE \n25.27 \u00b1 0.16 \n2.96 \u00b1 0.38 \n\n\n\nTable 3 .\n3Test errors(%)  and various calibration metrics evaluated on the test set (all in %) -ECE, Adaptive ECE (AECE) and Classwise ECE (CECE). Mean and standard deviation across 3 repetitions.Dataset \nLoss \nError \nECE \nAECE \nCECE \n\nSchool of Informatics, The University of Edinburgh, Edinburgh, United Kingdom. Correspondence to: Ondrej Bohdal <ondrej.bohdal@ed.ac.uk>.\nAcknowledgementsThis work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh.\nMetaReg: towards domain generalization using metaregularization. Y Balaji, S Sankaranarayanan, R Chellappa, NeurIPS. Balaji, Y., Sankaranarayanan, S., and Chellappa, R. (2018). MetaReg: towards domain generalization using meta- regularization. In NeurIPS.\n\nFast differentiable sorting and ranking. M Blondel, O Teboul, Q Berthet, J Djolonga, ICML. Blondel, M., Teboul, O., Berthet, Q., and Djolonga, J. (2020). Fast differentiable sorting and ranking. In ICML.\n\nVerification of forecasts expressed in terms of probability. G W Brier, Monthly weather review. Brier, G. W. (1950). Verification of forecasts expressed in terms of probability. In Monthly weather review.\n\nOn calibration of modern neural networks. C Guo, G Pleiss, Y Sun, K Q Weinberger, ICML. Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. (2017). On calibration of modern neural networks. In ICML.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep residual learning for image recognition. In CVPR.\n\nMeta-learning in neural networks: a survey. T M Hospedales, A Antoniou, P Micaelli, A J Storkey, IEEE Transactions on Pattern Analysis and Machine Intelligence. Hospedales, T. M., Antoniou, A., Micaelli, P., and Storkey, A. J. (2021). Meta-learning in neural networks: a sur- vey. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP:1-1.\n\nTask calibration for distributional uncertainty in few-shot classification. S Kim, S.-Y Yun, OpenReview. Kim, S. and Yun, S.-Y. (2020). Task calibration for distribu- tional uncertainty in few-shot classification. In OpenRe- view.\n\nAdam: a method for stochastic optimization. D P Kingma, J Ba, ICLR. Kingma, D. P. and Ba, J. (2015). Adam: a method for stochastic optimization. In ICLR.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, Technical reportKrizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report.\n\nBeyond temperature scaling: obtaining well-calibrated multiclass probabilities with Dirichlet calibration. M Kull, M Perello-Nieto, M K\u00e4ngsepp, T S Filho, H Song, P Flach, NeurIPS. Kull, M., Perello-Nieto, M., K\u00e4ngsepp, M., Filho, T. S., Song, H., and Flach, P. (2019). Beyond temperature scaling: obtaining well-calibrated multiclass probabilities with Dirichlet calibration. In NeurIPS.\n\nTrainable calibration measures for neural networks from kernel mean embeddings. A Kumar, S Sarawagi, U Jain, ICML. Kumar, A., Sarawagi, S., and Jain, U. (2018). Trainable calibration measures for neural networks from kernel mean embeddings. In ICML.\n\nFocal loss for dense object detection. T.-Y Lin, P Goyal, R Girshick, K He, P Doll\u00e1r, ICCV. Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll\u00e1r, P. (2017). Focal loss for dense object detection. In ICCV.\n\nOptimizing millions of hyperparameters by implicit differentiation. J Lorraine, P Vicol, D Duvenaud, AISTATS. Lorraine, J., Vicol, P., and Duvenaud, D. (2020). Optimizing millions of hyperparameters by implicit differentiation. In AISTATS.\n\nScalable gradient-based tuning of continuous regularization hyperparameters. J Luketina, M Berglund, A Klaus Greff, T Raiko, ICML. Luketina, J., Berglund, M., Klaus Greff, A., and Raiko, T. (2016). Scalable gradient-based tuning of continuous regularization hyperparameters. In ICML.\n\nCalibrating deep neural networks using focal loss. J Mukhoti, V Kulharia, A Sanyal, S Golodetz, P H S Torr, P K Dokania, NeurIPS. Mukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P. H. S., and Dokania, P. K. (2020). Calibrating deep neural networks using focal loss. In NeurIPS.\n\nWhen does label smoothing help? In NeurIPS. R M\u00fcller, S Kornblith, G Hinton, M\u00fcller, R., Kornblith, S., and Hinton, G. (2019). When does label smoothing help? In NeurIPS.\n\nObtaining well calibrated probabilities using bayesian binning. P Naeini, G F Cooper, M Hauskrecht, AAAI. Naeini, P., Cooper, G. F., and Hauskrecht, M. (2015). Ob- taining well calibrated probabilities using bayesian bin- ning. In AAAI.\n\nMeasuring calibration in deep learning. J Nixon, M W Dusenberry, L Zhang, G Jerfel, Tran , D , CVPR Workshop. Nixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., and Tran, D. (2019). Measuring calibration in deep learning. In CVPR Workshop.\n\nA general approximation framework for direct optimization of information retrieval measures. T Qin, T.-Y Liu, Li , H , Information retrieval. 134Qin, T., Liu, T.-Y., and Li, H. (2010). A general approxi- mation framework for direct optimization of information retrieval measures. Information retrieval, 13(4):375-397.\n\nDeep neural decision trees. Y Yang, I G Morillo, T M Hospedales, ICML Workshop on Human Interpretability in Machine Learning. Yang, Y., Morillo, I. G., and Hospedales, T. M. (2018). Deep neural decision trees. In ICML Workshop on Human Interpretability in Machine Learning.\n\nBrier score (Brier, 1950) 5. 27 \u00b1 0.21 1.23 \u00b1 0.03 2.00 \u00b1 0.25 0.46 \u00b1 0.02Brier score (Brier, 1950) 5.27 \u00b1 0.21 1.23 \u00b1 0.03 2.00 \u00b1 0.25 0.46 \u00b1 0.02\n\n. Mmce (kumar, MMCE (Kumar et al., 2018)\n\n. Lin, 17 \u00b1 0.09 2.17 \u00b1 0.03 2.16 \u00b1 0.13 0.52 \u00b1 0.02Focal loss (Lin et al., 2017) 5.17 \u00b1 0.09 2.17 \u00b1 0.03 2.16 \u00b1 0.13 0.52 \u00b1 0.02\n\n. Flsd (mukhoti, 2020) 5.28 \u00b1 0.13 2.26 \u00b1 0.03 2.20 \u00b1 0.02 0.54 \u00b1 0.02FLSD (Mukhoti et al., 2020) 5.28 \u00b1 0.13 2.26 \u00b1 0.03 2.20 \u00b1 0.02 0.54 \u00b1 0.02\n\n. M\u00fcller, LS-0.05LS-0.05 (M\u00fcller et al., 2019)\n\n. Mmce (kumar, 23MMCE (Kumar et al., 2018) 23\n\nFocal loss. Lin, 23Focal loss (Lin et al., 2017) 23\n\n. Flsd (mukhoti, 23FLSD (Mukhoti et al., 2020) 23\n\n. M\u00fcller, LS-0.0522LS-0.05 (M\u00fcller et al., 2019) 22\n\nMeta-Calibration (our). Meta-Calibration (our)\n\nTest errors (%) and ECE, AECE and CECE calibration metrics evaluated on the test set after applying temperature scaling. Table 4.. all inTable 4. Test errors (%) and ECE, AECE and CECE calibration metrics evaluated on the test set after applying temperature scaling (all in\n\nBrier score (Brier, 1950) 5. 27 \u00b1 0.21 1.23 \u00b1 0.03 2.00 \u00b1 0.25 0.46 \u00b1 0.02Brier score (Brier, 1950) 5.27 \u00b1 0.21 1.23 \u00b1 0.03 2.00 \u00b1 0.25 0.46 \u00b1 0.02\n\n. Mmce (kumar, 21 \u00b1 0.17 1.27 \u00b1 0.10 2.06 \u00b1 0.04 0.49 \u00b1 0.015MMCE (Kumar et al., 2018) 5.21 \u00b1 0.17 1.27 \u00b1 0.10 2.06 \u00b1 0.04 0.49 \u00b1 0.01\n\nFocal loss. Lin, Focal loss (Lin et al., 2017)\n\n. Flsd (mukhoti, FLSD (Mukhoti et al., 2020)\n\n. M\u00fcller, LS-0.05LS-0.05 (M\u00fcller et al., 2019)\n\n. Mmce (kumar, 23MMCE (Kumar et al., 2018) 23\n\nFocal loss. Lin, 23Focal loss (Lin et al., 2017) 23\n\n. Flsd (mukhoti, 23FLSD (Mukhoti et al., 2020) 23\n\n. M\u00fcller, LS-0.0522LS-0.05 (M\u00fcller et al., 2019) 22\n\nMeta-Calibration (our). Meta-Calibration (our)\n", "annotations": {"author": "[{\"end\":118,\"start\":104},{\"end\":132,\"start\":119},{\"end\":152,\"start\":133},{\"end\":118,\"start\":104},{\"end\":132,\"start\":119},{\"end\":152,\"start\":133}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":111},{\"end\":131,\"start\":127},{\"end\":151,\"start\":141},{\"end\":117,\"start\":111},{\"end\":131,\"start\":127},{\"end\":151,\"start\":141}]", "author_first_name": "[{\"end\":110,\"start\":104},{\"end\":126,\"start\":119},{\"end\":140,\"start\":133},{\"end\":110,\"start\":104},{\"end\":126,\"start\":119},{\"end\":140,\"start\":133}]", "author_affiliation": null, "title": "[{\"end\":101,\"start\":1},{\"end\":253,\"start\":153},{\"end\":101,\"start\":1},{\"end\":253,\"start\":153}]", "venue": null, "abstract": "[{\"end\":1060,\"start\":255},{\"end\":1060,\"start\":255}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1467,\"start\":1449},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1602,\"start\":1584},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1623,\"start\":1602},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1642,\"start\":1623},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1764,\"start\":1747},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1872,\"start\":1855},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2167,\"start\":2148},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2303,\"start\":2282},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2470,\"start\":2457},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3675,\"start\":3657},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4197,\"start\":4176},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5654,\"start\":5632},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5699,\"start\":5681},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6233,\"start\":6215},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6726,\"start\":6708},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6917,\"start\":6899},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8168,\"start\":8143},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8391,\"start\":8371},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8526,\"start\":8503},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9251,\"start\":9230},{\"end\":9908,\"start\":9906},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10183,\"start\":10166},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10293,\"start\":10272},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11038,\"start\":11017},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11288,\"start\":11275},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11850,\"start\":11831},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12188,\"start\":12168},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12222,\"start\":12203},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12738,\"start\":12713},{\"end\":14016,\"start\":14013},{\"end\":14030,\"start\":14027},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14370,\"start\":14357},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14651,\"start\":14638},{\"end\":14746,\"start\":14730},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15352,\"start\":15329},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16077,\"start\":16057},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16148,\"start\":16129},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1467,\"start\":1449},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1602,\"start\":1584},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1623,\"start\":1602},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1642,\"start\":1623},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1764,\"start\":1747},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1872,\"start\":1855},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2167,\"start\":2148},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2303,\"start\":2282},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2470,\"start\":2457},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3675,\"start\":3657},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4197,\"start\":4176},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5654,\"start\":5632},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5699,\"start\":5681},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6233,\"start\":6215},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6726,\"start\":6708},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6917,\"start\":6899},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8168,\"start\":8143},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8391,\"start\":8371},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8526,\"start\":8503},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9251,\"start\":9230},{\"end\":9908,\"start\":9906},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10183,\"start\":10166},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10293,\"start\":10272},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11038,\"start\":11017},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11288,\"start\":11275},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11850,\"start\":11831},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12188,\"start\":12168},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12222,\"start\":12203},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12738,\"start\":12713},{\"end\":14016,\"start\":14013},{\"end\":14030,\"start\":14027},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14370,\"start\":14357},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14651,\"start\":14638},{\"end\":14746,\"start\":14730},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15352,\"start\":15329},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16077,\"start\":16057},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16148,\"start\":16129}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16385,\"start\":16325},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16555,\"start\":16386},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":17425,\"start\":16556},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":18074,\"start\":17426},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":18312,\"start\":18075},{\"attributes\":{\"id\":\"fig_0\"},\"end\":16385,\"start\":16325},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16555,\"start\":16386},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":17425,\"start\":16556},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":18074,\"start\":17426},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":18312,\"start\":18075}]", "paragraph": "[{\"end\":1525,\"start\":1076},{\"end\":2694,\"start\":1527},{\"end\":3392,\"start\":2696},{\"end\":4080,\"start\":3394},{\"end\":4443,\"start\":4108},{\"end\":4539,\"start\":4494},{\"end\":4982,\"start\":4615},{\"end\":5259,\"start\":4984},{\"end\":5387,\"start\":5282},{\"end\":6203,\"start\":5389},{\"end\":6265,\"start\":6205},{\"end\":6665,\"start\":6325},{\"end\":7140,\"start\":6667},{\"end\":7222,\"start\":7142},{\"end\":8011,\"start\":7420},{\"end\":8762,\"start\":8013},{\"end\":8997,\"start\":8764},{\"end\":9716,\"start\":9099},{\"end\":9934,\"start\":9718},{\"end\":10125,\"start\":10073},{\"end\":10853,\"start\":10149},{\"end\":11197,\"start\":10855},{\"end\":11524,\"start\":11199},{\"end\":11851,\"start\":11536},{\"end\":12861,\"start\":11853},{\"end\":13373,\"start\":12897},{\"end\":13667,\"start\":13375},{\"end\":14243,\"start\":13669},{\"end\":14865,\"start\":14255},{\"end\":15474,\"start\":14895},{\"end\":15835,\"start\":15476},{\"end\":15942,\"start\":15837},{\"end\":16324,\"start\":15968},{\"end\":1525,\"start\":1076},{\"end\":2694,\"start\":1527},{\"end\":3392,\"start\":2696},{\"end\":4080,\"start\":3394},{\"end\":4443,\"start\":4108},{\"end\":4539,\"start\":4494},{\"end\":4982,\"start\":4615},{\"end\":5259,\"start\":4984},{\"end\":5387,\"start\":5282},{\"end\":6203,\"start\":5389},{\"end\":6265,\"start\":6205},{\"end\":6665,\"start\":6325},{\"end\":7140,\"start\":6667},{\"end\":7222,\"start\":7142},{\"end\":8011,\"start\":7420},{\"end\":8762,\"start\":8013},{\"end\":8997,\"start\":8764},{\"end\":9716,\"start\":9099},{\"end\":9934,\"start\":9718},{\"end\":10125,\"start\":10073},{\"end\":10853,\"start\":10149},{\"end\":11197,\"start\":10855},{\"end\":11524,\"start\":11199},{\"end\":11851,\"start\":11536},{\"end\":12861,\"start\":11853},{\"end\":13373,\"start\":12897},{\"end\":13667,\"start\":13375},{\"end\":14243,\"start\":13669},{\"end\":14865,\"start\":14255},{\"end\":15474,\"start\":14895},{\"end\":15835,\"start\":15476},{\"end\":15942,\"start\":15837},{\"end\":16324,\"start\":15968}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4493,\"start\":4444},{\"attributes\":{\"id\":\"formula_1\"},\"end\":4614,\"start\":4540},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6324,\"start\":6266},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7403,\"start\":7223},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9098,\"start\":8998},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10072,\"start\":9935},{\"attributes\":{\"id\":\"formula_0\"},\"end\":4493,\"start\":4444},{\"attributes\":{\"id\":\"formula_1\"},\"end\":4614,\"start\":4540},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6324,\"start\":6266},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7403,\"start\":7223},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9098,\"start\":8998},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10072,\"start\":9935}]", "table_ref": "[{\"end\":11558,\"start\":11551},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12257,\"start\":12250},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13776,\"start\":13769},{\"end\":13999,\"start\":13992},{\"end\":11558,\"start\":11551},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12257,\"start\":12250},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13776,\"start\":13769},{\"end\":13999,\"start\":13992}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1074,\"start\":1062},{\"attributes\":{\"n\":\"2.\"},\"end\":4090,\"start\":4083},{\"attributes\":{\"n\":\"2.1.\"},\"end\":4106,\"start\":4093},{\"attributes\":{\"n\":\"2.2.\"},\"end\":5280,\"start\":5262},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7418,\"start\":7405},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10147,\"start\":10128},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11534,\"start\":11527},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12880,\"start\":12864},{\"end\":12895,\"start\":12883},{\"end\":14253,\"start\":14246},{\"attributes\":{\"n\":\"4.\"},\"end\":14893,\"start\":14868},{\"end\":15966,\"start\":15945},{\"end\":16397,\"start\":16387},{\"end\":17436,\"start\":17427},{\"end\":18085,\"start\":18076},{\"attributes\":{\"n\":\"1.\"},\"end\":1074,\"start\":1062},{\"attributes\":{\"n\":\"2.\"},\"end\":4090,\"start\":4083},{\"attributes\":{\"n\":\"2.1.\"},\"end\":4106,\"start\":4093},{\"attributes\":{\"n\":\"2.2.\"},\"end\":5280,\"start\":5262},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7418,\"start\":7405},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10147,\"start\":10128},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11534,\"start\":11527},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12880,\"start\":12864},{\"end\":12895,\"start\":12883},{\"end\":14253,\"start\":14246},{\"attributes\":{\"n\":\"4.\"},\"end\":14893,\"start\":14868},{\"end\":15966,\"start\":15945},{\"end\":16397,\"start\":16387},{\"end\":17436,\"start\":17427},{\"end\":18085,\"start\":18076}]", "table": "[{\"end\":17425,\"start\":16851},{\"end\":18074,\"start\":17527},{\"end\":18312,\"start\":18273},{\"end\":17425,\"start\":16851},{\"end\":18074,\"start\":17527},{\"end\":18312,\"start\":18273}]", "figure_caption": "[{\"end\":16385,\"start\":16327},{\"end\":16555,\"start\":16399},{\"end\":16851,\"start\":16558},{\"end\":17527,\"start\":17438},{\"end\":18273,\"start\":18087},{\"end\":16385,\"start\":16327},{\"end\":16555,\"start\":16399},{\"end\":16851,\"start\":16558},{\"end\":17527,\"start\":17438},{\"end\":18273,\"start\":18087}]", "figure_ref": "[{\"end\":13170,\"start\":13162},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13487,\"start\":13479},{\"end\":13170,\"start\":13162},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13487,\"start\":13479}]", "bib_author_first_name": "[{\"end\":18746,\"start\":18745},{\"end\":18756,\"start\":18755},{\"end\":18776,\"start\":18775},{\"end\":18979,\"start\":18978},{\"end\":18990,\"start\":18989},{\"end\":19000,\"start\":18999},{\"end\":19011,\"start\":19010},{\"end\":19204,\"start\":19203},{\"end\":19206,\"start\":19205},{\"end\":19391,\"start\":19390},{\"end\":19398,\"start\":19397},{\"end\":19408,\"start\":19407},{\"end\":19415,\"start\":19414},{\"end\":19417,\"start\":19416},{\"end\":19595,\"start\":19594},{\"end\":19601,\"start\":19600},{\"end\":19610,\"start\":19609},{\"end\":19617,\"start\":19616},{\"end\":19778,\"start\":19777},{\"end\":19780,\"start\":19779},{\"end\":19794,\"start\":19793},{\"end\":19806,\"start\":19805},{\"end\":19818,\"start\":19817},{\"end\":19820,\"start\":19819},{\"end\":20164,\"start\":20163},{\"end\":20174,\"start\":20170},{\"end\":20364,\"start\":20363},{\"end\":20366,\"start\":20365},{\"end\":20376,\"start\":20375},{\"end\":20530,\"start\":20529},{\"end\":20764,\"start\":20763},{\"end\":20772,\"start\":20771},{\"end\":20789,\"start\":20788},{\"end\":20801,\"start\":20800},{\"end\":20803,\"start\":20802},{\"end\":20812,\"start\":20811},{\"end\":20820,\"start\":20819},{\"end\":21127,\"start\":21126},{\"end\":21136,\"start\":21135},{\"end\":21148,\"start\":21147},{\"end\":21340,\"start\":21336},{\"end\":21347,\"start\":21346},{\"end\":21356,\"start\":21355},{\"end\":21368,\"start\":21367},{\"end\":21374,\"start\":21373},{\"end\":21575,\"start\":21574},{\"end\":21587,\"start\":21586},{\"end\":21596,\"start\":21595},{\"end\":21825,\"start\":21824},{\"end\":21837,\"start\":21836},{\"end\":21849,\"start\":21848},{\"end\":21864,\"start\":21863},{\"end\":22084,\"start\":22083},{\"end\":22095,\"start\":22094},{\"end\":22107,\"start\":22106},{\"end\":22117,\"start\":22116},{\"end\":22129,\"start\":22128},{\"end\":22133,\"start\":22130},{\"end\":22141,\"start\":22140},{\"end\":22143,\"start\":22142},{\"end\":22367,\"start\":22366},{\"end\":22377,\"start\":22376},{\"end\":22390,\"start\":22389},{\"end\":22559,\"start\":22558},{\"end\":22569,\"start\":22568},{\"end\":22571,\"start\":22570},{\"end\":22581,\"start\":22580},{\"end\":22773,\"start\":22772},{\"end\":22782,\"start\":22781},{\"end\":22784,\"start\":22783},{\"end\":22798,\"start\":22797},{\"end\":22807,\"start\":22806},{\"end\":22820,\"start\":22816},{\"end\":22824,\"start\":22823},{\"end\":23069,\"start\":23068},{\"end\":23079,\"start\":23075},{\"end\":23087,\"start\":23085},{\"end\":23091,\"start\":23090},{\"end\":23323,\"start\":23322},{\"end\":23331,\"start\":23330},{\"end\":23333,\"start\":23332},{\"end\":23344,\"start\":23343},{\"end\":23346,\"start\":23345},{\"end\":18746,\"start\":18745},{\"end\":18756,\"start\":18755},{\"end\":18776,\"start\":18775},{\"end\":18979,\"start\":18978},{\"end\":18990,\"start\":18989},{\"end\":19000,\"start\":18999},{\"end\":19011,\"start\":19010},{\"end\":19204,\"start\":19203},{\"end\":19206,\"start\":19205},{\"end\":19391,\"start\":19390},{\"end\":19398,\"start\":19397},{\"end\":19408,\"start\":19407},{\"end\":19415,\"start\":19414},{\"end\":19417,\"start\":19416},{\"end\":19595,\"start\":19594},{\"end\":19601,\"start\":19600},{\"end\":19610,\"start\":19609},{\"end\":19617,\"start\":19616},{\"end\":19778,\"start\":19777},{\"end\":19780,\"start\":19779},{\"end\":19794,\"start\":19793},{\"end\":19806,\"start\":19805},{\"end\":19818,\"start\":19817},{\"end\":19820,\"start\":19819},{\"end\":20164,\"start\":20163},{\"end\":20174,\"start\":20170},{\"end\":20364,\"start\":20363},{\"end\":20366,\"start\":20365},{\"end\":20376,\"start\":20375},{\"end\":20530,\"start\":20529},{\"end\":20764,\"start\":20763},{\"end\":20772,\"start\":20771},{\"end\":20789,\"start\":20788},{\"end\":20801,\"start\":20800},{\"end\":20803,\"start\":20802},{\"end\":20812,\"start\":20811},{\"end\":20820,\"start\":20819},{\"end\":21127,\"start\":21126},{\"end\":21136,\"start\":21135},{\"end\":21148,\"start\":21147},{\"end\":21340,\"start\":21336},{\"end\":21347,\"start\":21346},{\"end\":21356,\"start\":21355},{\"end\":21368,\"start\":21367},{\"end\":21374,\"start\":21373},{\"end\":21575,\"start\":21574},{\"end\":21587,\"start\":21586},{\"end\":21596,\"start\":21595},{\"end\":21825,\"start\":21824},{\"end\":21837,\"start\":21836},{\"end\":21849,\"start\":21848},{\"end\":21864,\"start\":21863},{\"end\":22084,\"start\":22083},{\"end\":22095,\"start\":22094},{\"end\":22107,\"start\":22106},{\"end\":22117,\"start\":22116},{\"end\":22129,\"start\":22128},{\"end\":22133,\"start\":22130},{\"end\":22141,\"start\":22140},{\"end\":22143,\"start\":22142},{\"end\":22367,\"start\":22366},{\"end\":22377,\"start\":22376},{\"end\":22390,\"start\":22389},{\"end\":22559,\"start\":22558},{\"end\":22569,\"start\":22568},{\"end\":22571,\"start\":22570},{\"end\":22581,\"start\":22580},{\"end\":22773,\"start\":22772},{\"end\":22782,\"start\":22781},{\"end\":22784,\"start\":22783},{\"end\":22798,\"start\":22797},{\"end\":22807,\"start\":22806},{\"end\":22820,\"start\":22816},{\"end\":22824,\"start\":22823},{\"end\":23069,\"start\":23068},{\"end\":23079,\"start\":23075},{\"end\":23087,\"start\":23085},{\"end\":23091,\"start\":23090},{\"end\":23323,\"start\":23322},{\"end\":23331,\"start\":23330},{\"end\":23333,\"start\":23332},{\"end\":23344,\"start\":23343},{\"end\":23346,\"start\":23345}]", "bib_author_last_name": "[{\"end\":18753,\"start\":18747},{\"end\":18773,\"start\":18757},{\"end\":18786,\"start\":18777},{\"end\":18987,\"start\":18980},{\"end\":18997,\"start\":18991},{\"end\":19008,\"start\":19001},{\"end\":19020,\"start\":19012},{\"end\":19212,\"start\":19207},{\"end\":19395,\"start\":19392},{\"end\":19405,\"start\":19399},{\"end\":19412,\"start\":19409},{\"end\":19428,\"start\":19418},{\"end\":19598,\"start\":19596},{\"end\":19607,\"start\":19602},{\"end\":19614,\"start\":19611},{\"end\":19621,\"start\":19618},{\"end\":19791,\"start\":19781},{\"end\":19803,\"start\":19795},{\"end\":19815,\"start\":19807},{\"end\":19828,\"start\":19821},{\"end\":20168,\"start\":20165},{\"end\":20178,\"start\":20175},{\"end\":20373,\"start\":20367},{\"end\":20379,\"start\":20377},{\"end\":20541,\"start\":20531},{\"end\":20769,\"start\":20765},{\"end\":20786,\"start\":20773},{\"end\":20798,\"start\":20790},{\"end\":20809,\"start\":20804},{\"end\":20817,\"start\":20813},{\"end\":20826,\"start\":20821},{\"end\":21133,\"start\":21128},{\"end\":21145,\"start\":21137},{\"end\":21153,\"start\":21149},{\"end\":21344,\"start\":21341},{\"end\":21353,\"start\":21348},{\"end\":21365,\"start\":21357},{\"end\":21371,\"start\":21369},{\"end\":21381,\"start\":21375},{\"end\":21584,\"start\":21576},{\"end\":21593,\"start\":21588},{\"end\":21605,\"start\":21597},{\"end\":21834,\"start\":21826},{\"end\":21846,\"start\":21838},{\"end\":21861,\"start\":21850},{\"end\":21870,\"start\":21865},{\"end\":22092,\"start\":22085},{\"end\":22104,\"start\":22096},{\"end\":22114,\"start\":22108},{\"end\":22126,\"start\":22118},{\"end\":22138,\"start\":22134},{\"end\":22151,\"start\":22144},{\"end\":22374,\"start\":22368},{\"end\":22387,\"start\":22378},{\"end\":22397,\"start\":22391},{\"end\":22566,\"start\":22560},{\"end\":22578,\"start\":22572},{\"end\":22592,\"start\":22582},{\"end\":22779,\"start\":22774},{\"end\":22795,\"start\":22785},{\"end\":22804,\"start\":22799},{\"end\":22814,\"start\":22808},{\"end\":23073,\"start\":23070},{\"end\":23083,\"start\":23080},{\"end\":23328,\"start\":23324},{\"end\":23341,\"start\":23334},{\"end\":23357,\"start\":23347},{\"end\":23731,\"start\":23720},{\"end\":23765,\"start\":23762},{\"end\":23906,\"start\":23893},{\"end\":24046,\"start\":24040},{\"end\":24099,\"start\":24088},{\"end\":24148,\"start\":24145},{\"end\":24201,\"start\":24188},{\"end\":24245,\"start\":24239},{\"end\":24775,\"start\":24764},{\"end\":24913,\"start\":24910},{\"end\":24961,\"start\":24948},{\"end\":25000,\"start\":24994},{\"end\":25053,\"start\":25042},{\"end\":25102,\"start\":25099},{\"end\":25155,\"start\":25142},{\"end\":25199,\"start\":25193},{\"end\":18753,\"start\":18747},{\"end\":18773,\"start\":18757},{\"end\":18786,\"start\":18777},{\"end\":18987,\"start\":18980},{\"end\":18997,\"start\":18991},{\"end\":19008,\"start\":19001},{\"end\":19020,\"start\":19012},{\"end\":19212,\"start\":19207},{\"end\":19395,\"start\":19392},{\"end\":19405,\"start\":19399},{\"end\":19412,\"start\":19409},{\"end\":19428,\"start\":19418},{\"end\":19598,\"start\":19596},{\"end\":19607,\"start\":19602},{\"end\":19614,\"start\":19611},{\"end\":19621,\"start\":19618},{\"end\":19791,\"start\":19781},{\"end\":19803,\"start\":19795},{\"end\":19815,\"start\":19807},{\"end\":19828,\"start\":19821},{\"end\":20168,\"start\":20165},{\"end\":20178,\"start\":20175},{\"end\":20373,\"start\":20367},{\"end\":20379,\"start\":20377},{\"end\":20541,\"start\":20531},{\"end\":20769,\"start\":20765},{\"end\":20786,\"start\":20773},{\"end\":20798,\"start\":20790},{\"end\":20809,\"start\":20804},{\"end\":20817,\"start\":20813},{\"end\":20826,\"start\":20821},{\"end\":21133,\"start\":21128},{\"end\":21145,\"start\":21137},{\"end\":21153,\"start\":21149},{\"end\":21344,\"start\":21341},{\"end\":21353,\"start\":21348},{\"end\":21365,\"start\":21357},{\"end\":21371,\"start\":21369},{\"end\":21381,\"start\":21375},{\"end\":21584,\"start\":21576},{\"end\":21593,\"start\":21588},{\"end\":21605,\"start\":21597},{\"end\":21834,\"start\":21826},{\"end\":21846,\"start\":21838},{\"end\":21861,\"start\":21850},{\"end\":21870,\"start\":21865},{\"end\":22092,\"start\":22085},{\"end\":22104,\"start\":22096},{\"end\":22114,\"start\":22108},{\"end\":22126,\"start\":22118},{\"end\":22138,\"start\":22134},{\"end\":22151,\"start\":22144},{\"end\":22374,\"start\":22368},{\"end\":22387,\"start\":22378},{\"end\":22397,\"start\":22391},{\"end\":22566,\"start\":22560},{\"end\":22578,\"start\":22572},{\"end\":22592,\"start\":22582},{\"end\":22779,\"start\":22774},{\"end\":22795,\"start\":22785},{\"end\":22804,\"start\":22799},{\"end\":22814,\"start\":22808},{\"end\":23073,\"start\":23070},{\"end\":23083,\"start\":23080},{\"end\":23328,\"start\":23324},{\"end\":23341,\"start\":23334},{\"end\":23357,\"start\":23347},{\"end\":23731,\"start\":23720},{\"end\":23765,\"start\":23762},{\"end\":23906,\"start\":23893},{\"end\":24046,\"start\":24040},{\"end\":24099,\"start\":24088},{\"end\":24148,\"start\":24145},{\"end\":24201,\"start\":24188},{\"end\":24245,\"start\":24239},{\"end\":24775,\"start\":24764},{\"end\":24913,\"start\":24910},{\"end\":24961,\"start\":24948},{\"end\":25000,\"start\":24994},{\"end\":25053,\"start\":25042},{\"end\":25102,\"start\":25099},{\"end\":25155,\"start\":25142},{\"end\":25199,\"start\":25193}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53979606},\"end\":18935,\"start\":18680},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":211204737},\"end\":19140,\"start\":18937},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":122906757},\"end\":19346,\"start\":19142},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":28671436},\"end\":19546,\"start\":19348},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206594692},\"end\":19731,\"start\":19548},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":215744839},\"end\":20085,\"start\":19733},{\"attributes\":{\"id\":\"b6\"},\"end\":20317,\"start\":20087},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6628106},\"end\":20472,\"start\":20319},{\"attributes\":{\"id\":\"b8\"},\"end\":20654,\"start\":20474},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202773833},\"end\":21044,\"start\":20656},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49314079},\"end\":21295,\"start\":21046},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":47252984},\"end\":21504,\"start\":21297},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207847535},\"end\":21745,\"start\":21506},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6281930},\"end\":22030,\"start\":21747},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":211252346},\"end\":22320,\"start\":22032},{\"attributes\":{\"id\":\"b15\"},\"end\":22492,\"start\":22322},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6292807},\"end\":22730,\"start\":22494},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":102486060},\"end\":22973,\"start\":22732},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3018043},\"end\":23292,\"start\":22975},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49317359},\"end\":23567,\"start\":23294},{\"attributes\":{\"doi\":\"27 \u00b1 0.21 1.23 \u00b1 0.03 2.00 \u00b1 0.25 0.46 \u00b1 0.02\",\"id\":\"b20\"},\"end\":23716,\"start\":23569},{\"attributes\":{\"id\":\"b21\"},\"end\":23758,\"start\":23718},{\"attributes\":{\"doi\":\"17 \u00b1 0.09 2.17 \u00b1 0.03 2.16 \u00b1 0.13 0.52 \u00b1 0.02\",\"id\":\"b22\"},\"end\":23889,\"start\":23760},{\"attributes\":{\"doi\":\"2020) 5.28 \u00b1 0.13 2.26 \u00b1 0.03 2.20 \u00b1 0.02 0.54 \u00b1 0.02\",\"id\":\"b23\"},\"end\":24036,\"start\":23891},{\"attributes\":{\"doi\":\"LS-0.05\",\"id\":\"b24\"},\"end\":24084,\"start\":24038},{\"attributes\":{\"id\":\"b25\"},\"end\":24131,\"start\":24086},{\"attributes\":{\"id\":\"b26\"},\"end\":24184,\"start\":24133},{\"attributes\":{\"id\":\"b27\"},\"end\":24235,\"start\":24186},{\"attributes\":{\"doi\":\"LS-0.05\",\"id\":\"b28\"},\"end\":24288,\"start\":24237},{\"attributes\":{\"id\":\"b29\"},\"end\":24336,\"start\":24290},{\"attributes\":{\"id\":\"b30\"},\"end\":24611,\"start\":24338},{\"attributes\":{\"doi\":\"27 \u00b1 0.21 1.23 \u00b1 0.03 2.00 \u00b1 0.25 0.46 \u00b1 0.02\",\"id\":\"b31\"},\"end\":24760,\"start\":24613},{\"attributes\":{\"doi\":\"21 \u00b1 0.17 1.27 \u00b1 0.10 2.06 \u00b1 0.04 0.49 \u00b1 0.01\",\"id\":\"b32\"},\"end\":24896,\"start\":24762},{\"attributes\":{\"id\":\"b33\"},\"end\":24944,\"start\":24898},{\"attributes\":{\"id\":\"b34\"},\"end\":24990,\"start\":24946},{\"attributes\":{\"doi\":\"LS-0.05\",\"id\":\"b35\"},\"end\":25038,\"start\":24992},{\"attributes\":{\"id\":\"b36\"},\"end\":25085,\"start\":25040},{\"attributes\":{\"id\":\"b37\"},\"end\":25138,\"start\":25087},{\"attributes\":{\"id\":\"b38\"},\"end\":25189,\"start\":25140},{\"attributes\":{\"doi\":\"LS-0.05\",\"id\":\"b39\"},\"end\":25242,\"start\":25191},{\"attributes\":{\"id\":\"b40\"},\"end\":25290,\"start\":25244},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53979606},\"end\":18935,\"start\":18680},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":211204737},\"end\":19140,\"start\":18937},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":122906757},\"end\":19346,\"start\":19142},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":28671436},\"end\":19546,\"start\":19348},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206594692},\"end\":19731,\"start\":19548},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":215744839},\"end\":20085,\"start\":19733},{\"attributes\":{\"id\":\"b6\"},\"end\":20317,\"start\":20087},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6628106},\"end\":20472,\"start\":20319},{\"attributes\":{\"id\":\"b8\"},\"end\":20654,\"start\":20474},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202773833},\"end\":21044,\"start\":20656},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49314079},\"end\":21295,\"start\":21046},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":47252984},\"end\":21504,\"start\":21297},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207847535},\"end\":21745,\"start\":21506},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6281930},\"end\":22030,\"start\":21747},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":211252346},\"end\":22320,\"start\":22032},{\"attributes\":{\"id\":\"b15\"},\"end\":22492,\"start\":22322},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6292807},\"end\":22730,\"start\":22494},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":102486060},\"end\":22973,\"start\":22732},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3018043},\"end\":23292,\"start\":22975},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49317359},\"end\":23567,\"start\":23294},{\"attributes\":{\"doi\":\"27 \u00b1 0.21 1.23 \u00b1 0.03 2.00 \u00b1 0.25 0.46 \u00b1 0.02\",\"id\":\"b20\"},\"end\":23716,\"start\":23569},{\"attributes\":{\"id\":\"b21\"},\"end\":23758,\"start\":23718},{\"attributes\":{\"doi\":\"17 \u00b1 0.09 2.17 \u00b1 0.03 2.16 \u00b1 0.13 0.52 \u00b1 0.02\",\"id\":\"b22\"},\"end\":23889,\"start\":23760},{\"attributes\":{\"doi\":\"2020) 5.28 \u00b1 0.13 2.26 \u00b1 0.03 2.20 \u00b1 0.02 0.54 \u00b1 0.02\",\"id\":\"b23\"},\"end\":24036,\"start\":23891},{\"attributes\":{\"doi\":\"LS-0.05\",\"id\":\"b24\"},\"end\":24084,\"start\":24038},{\"attributes\":{\"id\":\"b25\"},\"end\":24131,\"start\":24086},{\"attributes\":{\"id\":\"b26\"},\"end\":24184,\"start\":24133},{\"attributes\":{\"id\":\"b27\"},\"end\":24235,\"start\":24186},{\"attributes\":{\"doi\":\"LS-0.05\",\"id\":\"b28\"},\"end\":24288,\"start\":24237},{\"attributes\":{\"id\":\"b29\"},\"end\":24336,\"start\":24290},{\"attributes\":{\"id\":\"b30\"},\"end\":24611,\"start\":24338},{\"attributes\":{\"doi\":\"27 \u00b1 0.21 1.23 \u00b1 0.03 2.00 \u00b1 0.25 0.46 \u00b1 0.02\",\"id\":\"b31\"},\"end\":24760,\"start\":24613},{\"attributes\":{\"doi\":\"21 \u00b1 0.17 1.27 \u00b1 0.10 2.06 \u00b1 0.04 0.49 \u00b1 0.01\",\"id\":\"b32\"},\"end\":24896,\"start\":24762},{\"attributes\":{\"id\":\"b33\"},\"end\":24944,\"start\":24898},{\"attributes\":{\"id\":\"b34\"},\"end\":24990,\"start\":24946},{\"attributes\":{\"doi\":\"LS-0.05\",\"id\":\"b35\"},\"end\":25038,\"start\":24992},{\"attributes\":{\"id\":\"b36\"},\"end\":25085,\"start\":25040},{\"attributes\":{\"id\":\"b37\"},\"end\":25138,\"start\":25087},{\"attributes\":{\"id\":\"b38\"},\"end\":25189,\"start\":25140},{\"attributes\":{\"doi\":\"LS-0.05\",\"id\":\"b39\"},\"end\":25242,\"start\":25191},{\"attributes\":{\"id\":\"b40\"},\"end\":25290,\"start\":25244}]", "bib_title": "[{\"end\":18743,\"start\":18680},{\"end\":18976,\"start\":18937},{\"end\":19201,\"start\":19142},{\"end\":19388,\"start\":19348},{\"end\":19592,\"start\":19548},{\"end\":19775,\"start\":19733},{\"end\":20161,\"start\":20087},{\"end\":20361,\"start\":20319},{\"end\":20761,\"start\":20656},{\"end\":21124,\"start\":21046},{\"end\":21334,\"start\":21297},{\"end\":21572,\"start\":21506},{\"end\":21822,\"start\":21747},{\"end\":22081,\"start\":22032},{\"end\":22556,\"start\":22494},{\"end\":22770,\"start\":22732},{\"end\":23066,\"start\":22975},{\"end\":23320,\"start\":23294},{\"end\":18743,\"start\":18680},{\"end\":18976,\"start\":18937},{\"end\":19201,\"start\":19142},{\"end\":19388,\"start\":19348},{\"end\":19592,\"start\":19548},{\"end\":19775,\"start\":19733},{\"end\":20161,\"start\":20087},{\"end\":20361,\"start\":20319},{\"end\":20761,\"start\":20656},{\"end\":21124,\"start\":21046},{\"end\":21334,\"start\":21297},{\"end\":21572,\"start\":21506},{\"end\":21822,\"start\":21747},{\"end\":22081,\"start\":22032},{\"end\":22556,\"start\":22494},{\"end\":22770,\"start\":22732},{\"end\":23066,\"start\":22975},{\"end\":23320,\"start\":23294}]", "bib_author": "[{\"end\":18755,\"start\":18745},{\"end\":18775,\"start\":18755},{\"end\":18788,\"start\":18775},{\"end\":18989,\"start\":18978},{\"end\":18999,\"start\":18989},{\"end\":19010,\"start\":18999},{\"end\":19022,\"start\":19010},{\"end\":19214,\"start\":19203},{\"end\":19397,\"start\":19390},{\"end\":19407,\"start\":19397},{\"end\":19414,\"start\":19407},{\"end\":19430,\"start\":19414},{\"end\":19600,\"start\":19594},{\"end\":19609,\"start\":19600},{\"end\":19616,\"start\":19609},{\"end\":19623,\"start\":19616},{\"end\":19793,\"start\":19777},{\"end\":19805,\"start\":19793},{\"end\":19817,\"start\":19805},{\"end\":19830,\"start\":19817},{\"end\":20170,\"start\":20163},{\"end\":20180,\"start\":20170},{\"end\":20375,\"start\":20363},{\"end\":20381,\"start\":20375},{\"end\":20543,\"start\":20529},{\"end\":20771,\"start\":20763},{\"end\":20788,\"start\":20771},{\"end\":20800,\"start\":20788},{\"end\":20811,\"start\":20800},{\"end\":20819,\"start\":20811},{\"end\":20828,\"start\":20819},{\"end\":21135,\"start\":21126},{\"end\":21147,\"start\":21135},{\"end\":21155,\"start\":21147},{\"end\":21346,\"start\":21336},{\"end\":21355,\"start\":21346},{\"end\":21367,\"start\":21355},{\"end\":21373,\"start\":21367},{\"end\":21383,\"start\":21373},{\"end\":21586,\"start\":21574},{\"end\":21595,\"start\":21586},{\"end\":21607,\"start\":21595},{\"end\":21836,\"start\":21824},{\"end\":21848,\"start\":21836},{\"end\":21863,\"start\":21848},{\"end\":21872,\"start\":21863},{\"end\":22094,\"start\":22083},{\"end\":22106,\"start\":22094},{\"end\":22116,\"start\":22106},{\"end\":22128,\"start\":22116},{\"end\":22140,\"start\":22128},{\"end\":22153,\"start\":22140},{\"end\":22376,\"start\":22366},{\"end\":22389,\"start\":22376},{\"end\":22399,\"start\":22389},{\"end\":22568,\"start\":22558},{\"end\":22580,\"start\":22568},{\"end\":22594,\"start\":22580},{\"end\":22781,\"start\":22772},{\"end\":22797,\"start\":22781},{\"end\":22806,\"start\":22797},{\"end\":22816,\"start\":22806},{\"end\":22823,\"start\":22816},{\"end\":22827,\"start\":22823},{\"end\":23075,\"start\":23068},{\"end\":23085,\"start\":23075},{\"end\":23090,\"start\":23085},{\"end\":23094,\"start\":23090},{\"end\":23330,\"start\":23322},{\"end\":23343,\"start\":23330},{\"end\":23359,\"start\":23343},{\"end\":23733,\"start\":23720},{\"end\":23767,\"start\":23762},{\"end\":23908,\"start\":23893},{\"end\":24048,\"start\":24040},{\"end\":24101,\"start\":24088},{\"end\":24150,\"start\":24145},{\"end\":24203,\"start\":24188},{\"end\":24247,\"start\":24239},{\"end\":24777,\"start\":24764},{\"end\":24915,\"start\":24910},{\"end\":24963,\"start\":24948},{\"end\":25002,\"start\":24994},{\"end\":25055,\"start\":25042},{\"end\":25104,\"start\":25099},{\"end\":25157,\"start\":25142},{\"end\":25201,\"start\":25193},{\"end\":18755,\"start\":18745},{\"end\":18775,\"start\":18755},{\"end\":18788,\"start\":18775},{\"end\":18989,\"start\":18978},{\"end\":18999,\"start\":18989},{\"end\":19010,\"start\":18999},{\"end\":19022,\"start\":19010},{\"end\":19214,\"start\":19203},{\"end\":19397,\"start\":19390},{\"end\":19407,\"start\":19397},{\"end\":19414,\"start\":19407},{\"end\":19430,\"start\":19414},{\"end\":19600,\"start\":19594},{\"end\":19609,\"start\":19600},{\"end\":19616,\"start\":19609},{\"end\":19623,\"start\":19616},{\"end\":19793,\"start\":19777},{\"end\":19805,\"start\":19793},{\"end\":19817,\"start\":19805},{\"end\":19830,\"start\":19817},{\"end\":20170,\"start\":20163},{\"end\":20180,\"start\":20170},{\"end\":20375,\"start\":20363},{\"end\":20381,\"start\":20375},{\"end\":20543,\"start\":20529},{\"end\":20771,\"start\":20763},{\"end\":20788,\"start\":20771},{\"end\":20800,\"start\":20788},{\"end\":20811,\"start\":20800},{\"end\":20819,\"start\":20811},{\"end\":20828,\"start\":20819},{\"end\":21135,\"start\":21126},{\"end\":21147,\"start\":21135},{\"end\":21155,\"start\":21147},{\"end\":21346,\"start\":21336},{\"end\":21355,\"start\":21346},{\"end\":21367,\"start\":21355},{\"end\":21373,\"start\":21367},{\"end\":21383,\"start\":21373},{\"end\":21586,\"start\":21574},{\"end\":21595,\"start\":21586},{\"end\":21607,\"start\":21595},{\"end\":21836,\"start\":21824},{\"end\":21848,\"start\":21836},{\"end\":21863,\"start\":21848},{\"end\":21872,\"start\":21863},{\"end\":22094,\"start\":22083},{\"end\":22106,\"start\":22094},{\"end\":22116,\"start\":22106},{\"end\":22128,\"start\":22116},{\"end\":22140,\"start\":22128},{\"end\":22153,\"start\":22140},{\"end\":22376,\"start\":22366},{\"end\":22389,\"start\":22376},{\"end\":22399,\"start\":22389},{\"end\":22568,\"start\":22558},{\"end\":22580,\"start\":22568},{\"end\":22594,\"start\":22580},{\"end\":22781,\"start\":22772},{\"end\":22797,\"start\":22781},{\"end\":22806,\"start\":22797},{\"end\":22816,\"start\":22806},{\"end\":22823,\"start\":22816},{\"end\":22827,\"start\":22823},{\"end\":23075,\"start\":23068},{\"end\":23085,\"start\":23075},{\"end\":23090,\"start\":23085},{\"end\":23094,\"start\":23090},{\"end\":23330,\"start\":23322},{\"end\":23343,\"start\":23330},{\"end\":23359,\"start\":23343},{\"end\":23733,\"start\":23720},{\"end\":23767,\"start\":23762},{\"end\":23908,\"start\":23893},{\"end\":24048,\"start\":24040},{\"end\":24101,\"start\":24088},{\"end\":24150,\"start\":24145},{\"end\":24203,\"start\":24188},{\"end\":24247,\"start\":24239},{\"end\":24777,\"start\":24764},{\"end\":24915,\"start\":24910},{\"end\":24963,\"start\":24948},{\"end\":25002,\"start\":24994},{\"end\":25055,\"start\":25042},{\"end\":25104,\"start\":25099},{\"end\":25157,\"start\":25142},{\"end\":25201,\"start\":25193}]", "bib_venue": "[{\"end\":18795,\"start\":18788},{\"end\":19026,\"start\":19022},{\"end\":19236,\"start\":19214},{\"end\":19434,\"start\":19430},{\"end\":19627,\"start\":19623},{\"end\":19892,\"start\":19830},{\"end\":20190,\"start\":20180},{\"end\":20385,\"start\":20381},{\"end\":20527,\"start\":20474},{\"end\":20835,\"start\":20828},{\"end\":21159,\"start\":21155},{\"end\":21387,\"start\":21383},{\"end\":21614,\"start\":21607},{\"end\":21876,\"start\":21872},{\"end\":22160,\"start\":22153},{\"end\":22364,\"start\":22322},{\"end\":22598,\"start\":22594},{\"end\":22840,\"start\":22827},{\"end\":23115,\"start\":23094},{\"end\":23418,\"start\":23359},{\"end\":23596,\"start\":23569},{\"end\":24143,\"start\":24133},{\"end\":24312,\"start\":24290},{\"end\":24457,\"start\":24338},{\"end\":24640,\"start\":24613},{\"end\":24908,\"start\":24898},{\"end\":25097,\"start\":25087},{\"end\":25266,\"start\":25244},{\"end\":18795,\"start\":18788},{\"end\":19026,\"start\":19022},{\"end\":19236,\"start\":19214},{\"end\":19434,\"start\":19430},{\"end\":19627,\"start\":19623},{\"end\":19892,\"start\":19830},{\"end\":20190,\"start\":20180},{\"end\":20385,\"start\":20381},{\"end\":20527,\"start\":20474},{\"end\":20835,\"start\":20828},{\"end\":21159,\"start\":21155},{\"end\":21387,\"start\":21383},{\"end\":21614,\"start\":21607},{\"end\":21876,\"start\":21872},{\"end\":22160,\"start\":22153},{\"end\":22364,\"start\":22322},{\"end\":22598,\"start\":22594},{\"end\":22840,\"start\":22827},{\"end\":23115,\"start\":23094},{\"end\":23418,\"start\":23359},{\"end\":23596,\"start\":23569},{\"end\":24143,\"start\":24133},{\"end\":24312,\"start\":24290},{\"end\":24457,\"start\":24338},{\"end\":24640,\"start\":24613},{\"end\":24908,\"start\":24898},{\"end\":25097,\"start\":25087},{\"end\":25266,\"start\":25244}]"}}}, "year": 2023, "month": 12, "day": 17}