{"id": 259089172, "updated": "2023-10-05 00:01:21.473", "metadata": {"title": "Stack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models", "authors": "[{\"first\":\"Manisha\",\"last\":\"Mukherjee\",\"middle\":[]},{\"first\":\"Vincent\",\"last\":\"Hellendoorn\",\"middle\":[\"J.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large pre-trained neural language models have brought immense progress to both NLP and software engineering. Models in OpenAI's GPT series now dwarf Google's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide range of NLP applications. These models are trained on massive corpora of heterogeneous data from web crawls, which enables them to learn general language patterns and semantic relationships. However, the largest models are both expensive to train and deploy and are often closed-source, so we lack access to their data and design decisions. We argue that this trend towards large, general-purpose models should be complemented with single-purpose, more modestly sized pre-trained models. In this work, we take StackOverflow (SO) as a domain example in which large volumes of rich aligned code and text data is available. We adopt standard practices for pre-training large language models, including using a very large context size (2,048 tokens), batch size (0.5M tokens) and training set (27B tokens), coupled with a powerful toolkit (Megatron-LM), to train two models: SOBertBase, with 109M parameters, and SOBertLarge with 762M parameters, at a budget of just $187 and $800 each. We compare the performance of our models with both the previous SOTA model trained on SO data exclusively as well general-purpose BERT models and OpenAI's ChatGPT on four SO-specific downstream tasks - question quality prediction, closed question prediction, named entity recognition and obsoletion prediction (a new task we introduce). Not only do our models consistently outperform all baselines, the smaller model is often sufficient for strong results. Both models are released to the public. These results demonstrate that pre-training both extensively and properly on in-domain data can yield a powerful and affordable alternative to leveraging closed-source general-purpose models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.03268", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-03268", "doi": "10.48550/arxiv.2306.03268"}}, "content": {"source": {"pdf_hash": "0ce17ce506657527d0e3ef3679c8be369c785d2b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.03268v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ec35be1e81bb4f777d6127aa74136a6ad7dc0d82", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0ce17ce506657527d0e3ef3679c8be369c785d2b.txt", "contents": "\nStack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models\n\n\nManisha Mukherjee \nCarnegie Mellon University Pittsburgh\nPennsylvaniaUSA\n\nVincent J Hellendoorn \nCarnegie Mellon University Pittsburgh\nPennsylvaniaUSA\n\nStack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models\nLarge Language ModelsPre-trained ModelsDiscriminative mod- elsStack Overflow\nLarge pre-trained neural language models have brought immense progress to both natural language processing and software engineering. Models in OpenAI's GPT series now dwarf Google's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide range of NLP applications. These models are trained on massive corpora of heterogeneous data from web crawls, which enables them to learn general language patterns and semantic relationships. However, the largest models are both expensive to train and deploy and are often closed-source, so we lack access to their data and design decisions. We argue that this trend towards large, general-purpose models should be complemented with single-purpose, more modestly sized pre-trained models. In this work, we take StackOverflow (SO) as a domain example in which large volumes of rich aligned code and text data is available. We adopt standard practices for pre-training large language models, including using a very large context size (2,048 tokens), batch size (0.5M tokens) and training set (27B tokens), coupled with a powerful toolkit (Megatron-LM), to train two models: SOBertBase, with 109 million parameters, and SOBertLarge with 762 million parameters, at a budget of just $187 and $800 each. We compare the performance of our models with both the previous SOTA model trained on StackOverflow data exclusively (BERTOverflow, which relied on an approach tailored to natural language processing) as well general-purpose BERT models (BERTBase and BERTLarge) and OpenAI's ChatGPT on four StackOverflow-specific downstream tasks -question quality prediction, closed question prediction, named entity recognition and obsoletion prediction (a new task we introduce). Not only do our models consistently outperform all baselines, the smaller model is often sufficient for strong results. Both models are released to the public. 1 These results demonstrate that pre-training both extensively and properly on in-domain data can yield a powerful and affordable alternative to leveraging closed-source general-purpose models.\n\nINTRODUCTION\n\nNeural Language Models (LMs) have brought about a significant transformation in the domains of Natural Language Processing (NLP) and Software Engineering (SE). In NLP, LMs are utilized for tasks including text classification [36], sentiment analysis [35], and machine translation [46]. In SE, LMs treat code as a form of language and are used for tasks such as code completion [28], bug 1  localization [14], and code summarization [41]. Language models can be trained from scratch for a given task as long as they are provided with enough data, typically measured in millions of tokens. In recent years, however, it has been found more effective to pretrain models on large amounts of data to learn general patterns from text or code. Following this pre-training, smaller models such as BERT [15] (with fewer than one billion parameters) are typically fine-tuned on downstream tasks while larger ones such as GPT-3 [11] (with many billions of parameters) can be prompted with a natural language question and some examples.\n\nFollowing the observation that language model performance scales predictably and steeply with compute budget [22], recent trends strongly favor larger models: models like GPT-3, with 175B parameters [11], and PALM [13] with 540B parameters are trained with hundreds of billions of tokens using a fairly simple, text completion training signal. Commonly referred to as Large Language Models (LLMs) , these models gain remarkable capabilities on a host of language tasks when prompted with just a problem description and some examples. Figure 1 shows the relative scale in terms of both parameters and training data of various BERT models, including ours (in bold) compared to GPT-3 (note the log-scale on both axes). 2 Yet the ease with which LLMs can be used for new tasks comes at a cost: LLMs are typically massive, requiring millions of dollars of compute to train [20], making them nearly impossible to fine-tune on academic budgets. Most LLMs are also closed-source, so we have no access to their learned weights nor insight into their training data. Finally, as we will show in this work, their generalpurpose training signal leaves them performing below-par on tasks where sufficient fine-tuning data is available.\n\nThe SE community has responded to the trend towards larger language models with code-specific LMs. For example, CodeBERT [16] (110M parameters) is based on BERT, a so-called encoder-only model trained to fill in masked-out tokens in an input. CodeT5 [43] (comes in 60M, 220M and 770 parameters) is an encoder-decoder model that encode inputs (mainly, entire functions) plus a task description and decodes (essentially, converts) these into outputs, such as labels, repaired functions. PolyCoder [44] (2.7B parameters), Codex [12] (12B parameters, not open-source) and CodeGen [31] (16B parameters) are all decoder-only models trained to generate code in a left-to-right fashion. In this work, we add to this collection of models by training several BERT-style models on StackOverflow (SO). SO has the unique appeal of providing high-quality text (often, explanations) alongside code, making it a key resource for programmers. Generic (non-SO pre-trained) BERT models are frequently and successfully fine-tuned for downstream tasks involving SO data [9,10,24]. Several papers even propose to pre-train such models on SO data, including BERTOverflow [37], seBERT [40] and PTM4Tag [19], often finding that such models are even more powerful when fine-tuned for downstream tasks. However, these pre-training efforts tend to be based in NLP conventions, such as treating each sentence in a post as a separate input to the model. As such, they overlook key properties of StackOverflow posts and large language modeling best practices more generally, like the significant benefits of using very large inputs -BertOverflow limits inputs to 512 tokens, just like CodeT5; we use 2,048 in line with GPT-3 and corpus statistics shown in Figure 4. Unlike seBert [40], we preserve the code within the text, as they contain information that is valuable to the overall comprehension of the content. Furthermore, we train models with both posts and their associated comments, which often add key information about the answer's quality over time.\n\nWith this input representation, we train two BERT models on 27 billion tokens of SO data following best-practice guidelines for training large language models: SOBertBase, with 109M parameters, and SOBertLarge, with 762M parameters. While nontrivial in size, these models can be trained in the cloud at a budget of just $187 and $800 respectively.\n\nWe fine-tune both SOBert models as well as an array of baselines including two generic BERT models and BertOverflow, as well as ChatGPT queried via few-shot prompting, on four different tasks that measure StackOverflow question & answer understanding: question quality prediction [7], closed question prediction [1], named entity recognition [37] and obsoletion prediction. The latter is a new task that we introduce focused on detecting whether answers contain outdated code, a common phenomenon on SO that can cause vulnerability and compatibility issues for those adopting the code without scrutiny [17,30,32]. We constructed this dataset by manually annotating about 1,000 SO answers. SOBert consistently, and often widely, outperforms all other compared models on all four tasks, with even the smaller model, SOBertBase frequently yielding state-of-the-art performance. These results highlight the significant promise of pre-training sub-billion parameter language models on in-domain data when such data is rich in detail and abundant. We release our models for public use.  In summary we make the following contributions:\n\n\u2022 We introduce two models, SOBertBase (109M parameters) and SOBertLarge (762M parameters), that were trained on StackOverflow answer and comment text using the Megatron Toolkit. The training process used multiple GPUs and cost a modest budget of $187 and $800 for SOBertBase and SOBertLarge, respectively. \u2022 We extensively evaluate both SOBert models on four different tasks, which include question quality prediction, closed question prediction, named entity recognition, and obsoletion prediction. \u2022 Our evaluation involves comparing the performance of our models to two variants of BERT (BERTBase and BERTLarge), BERTOverflow, and ChatGPT (for three tasks). We demonstrate that SOBert outperforms all other models on all four downstream tasks, and sometimes even SOBertBase outperforms the other compared models in terms of performance.\n\n\nBACKGROUND\n\nTo provide context and prevent ambiguity for the features discussed later, we begin by defining some of the terms we will be using in this paper. We then delve into the background work that has been done in this field.  as an example, this image demonstrates the structure of a question and answer on SO, featuring a question title, question body, answer body, and comments. We also demonstrate which features we use for each of the following four downstream tasks.\n\n\nHow StackOverflow Works\n\nStackOverflow (SO) is a social Q&A platform. SO posts consist of several components, each of which serves a distinct purpose. The primary components of a post are the title, question body, answer body and comments. The title is the headline of the post, and it is meant to provide a brief summary of what the post is about. The question body is the main content of the post, where the user explains the problem they are facing or the question they want to ask. The answer body is where other users can provide their responses to the question asked in the post. Comments are additional remarks that users can add to a post. They can be used to ask for clarification, provide feedback, or suggest improvements to the post. Other components of a post include tags, which are keywords that describe the topic of the post, and votes, which allow users to express their opinion on the quality of the post. Figure 3 provides a visual representation of the various components that make up Stack Overflow (SO) posts. It namely illustrates the different elements, such as the title, question body, answer body and comments that users can interact with when creating or viewing a post on the platform. Users who ask questions are encouraged to both post text describing the problem and include other material such as their (minimal) incorrect code, screenshots, URLs etc. Questions should include between one and five 'tags', which identify features like the programming language, library used, or domain of the problem and serve to bring the question to the attention of experienced developers in the corresponding domains. Once a question is posted, developers may respond to with answers, one of which the question-asker may eventually mark as the accepted answer. It is not uncommon for multiple answers to be posted, and upvoted, for a single question; each usually differs in some aspect such as the library (version) used, assumptions about the underlying system & context, or degree of optimization. 3 Users have the ability to downvote answers or questions as well, to help reduce the visibility of poor quality content. It is not necessary for the accepted answer to have the most upvotes. Along with questions and answers, the website also features a commenting system that allows users to provide feedback or additional information on a particular question or answer. Comments on Stack Overflow can be used for a variety of purposes, such as clarifying a question, suggesting an alternative solution, or pointing out potential issues with an existing answer. They can also be used to ask for further information or to provide additional context that may be helpful in finding a solution to a particular problem.\n\n\nPre-trained Language Models\n\nLanguage models generate text in a given context. Auto-regressive, or left-to-right, language models do so by repeatedly predicting the next token in an input conditioned on all previous tokens, while masked-language models predict a random subset (typically some 15%) of \"masked\" tokens in an entire input. Both can be pre-trained on very large datasets of text by converting each document into a series of training examples. Auto-regressive, or decoder-only, models tend to be useful for downstream tasks where new text is a desired output, as in ChatGPT, while masked-language model training assumes that the entire input document (minus zero or more masked-out tokens) is available while training which makes it easier to \"embed\" inputs in a single representation that may then be used for learning any given downstream task.\n\nBERT (Bidirectional Encoder Representations from Transformers) is a type of pre-trained masked language model developed by Google introduced in [15] that can be fine-tuned for various natural language processing tasks. It utilizes the Transformer architecture [38], which employs a self-attention mechanism to learn a contextual representation of text. It is trained using both the masked language modeling and next sentence prediction objectives on BooksCorpus (800M words) and Wikipedia (2,500M words). Of these two tasks, predicting masked words has proven particularly important as it teaches the model to represent each word in the context of its surrounding words, which helps it learn meaningful representations of the entire document as well. This makes BERTlike models versatile and effective in various downstream NLP tasks such as text classification [18,36] and named entity recognition [26,29].\n\nTwo well-trained BERT models were made available by Google. BERTBase-uncased [15] is the smaller model, which has 12 layers of transformer blocks with an internal dimension of 768 for a total of 110 million parameters. The \"uncased\" tag means that its input text has been converted to lowercase and all punctuation has been removed, which simplifies learning from a heterogeneous body of text. A second model, BERTLarge [15], has 24 1,024-dimensional transformer layers for a total of 340 million parameters. We match our own Base model, SOBertBase, to the former, while adopting a model with a slightly larger hidden dimension (1,536) than BERT-Large but the same number of layers for SOBertLarge, yielding 762M parameters. We pre-train our models with the masked language modeling objective.\n\n\nModeling StackOverflow data\n\nStackOverflow (SO) frequently releases 'data dumps' of publicly available content (questions, answers, comments, etc.). This, combined with its natural alignment of informative text and code, has made it a popular target for SE research. Numerous aspects of software engineering in have been studied on SO by researchers. For instance, one study scrutinized the interactions among developers on the platform [42], while another analyzed the themes and patterns in discussions on SO [8]. Working with StackOverflow data is not without challenges. For one, the quality and structure of answers varies widely from post to post, with many answers being incomplete, containing errors, or lacking context. Another issue is the sheer volume of data on the platform, making it difficult to effectively navigate and identify high-quality task-relevant posts for specific study goals. To mitigate this, studies often focus on a narrow slice of the data, such as investigating the specific inquiries made by software engineers about Android testing on Stack Overflow [39].\n\nAt the intersection of language models and SO data, there has been a lot of work in the space of fine-tuning BERT for various tasks such as emotion recognition [10], sentiment analysis [9], automated summarization [24] etc. that involve SO data. Several models have been pre-trained on SO data specifically. He et al. [19] build a tag recommendation framework PTM4TAG for Stack Overflow posts with different BERT-based models. The model seBERT [40] is a 340 million parameter model trained on 62.8GB of textual data (they remove all code from the posts) from answer, posts and comments from SO along with more data from GitHub and Jira using the BERTLarge architecture. They use an cased-insensitive 30,522 token WordPiece vocabulary to tokenize their data and restrict inputs to a maximum length of 128 tokens -significantly shorter than most SO posts ( Figure 4). Tabassum et al. [37] released a BERT model pre-trained on StackOverflow data. Their model, BERTOverflow, was pre-trained on 152 million sentences from SO using a case-sensitive 64,000 token WordPiece vocabulary. Training on individual sentences was a common practice in training BERT-like models, but recent work has found that language models perform substantially better when considering a document-level context [22], albeit at a substantial increase in compute cost. Our findings echo that observation. Tabassum et al. focus on the downstream task of Named Entity Recognition (NER, essentially tagging each word with its syntactic role), on which their model out-performs existing models such as BERTLarge as well as GloVe and ELMo models trained on the same SO data.\n\nCompared to these efforts, we pre-train SOBert with 19 GB data presented as just 15 million samples where each sample contains an entire post and all its corresponding comments. We also include all code in each answer so that our model is bimodal in nature. Moreover, we use a SentencePiece tokenizer trained with Byte-Pair Encoding [25], which has the benefit over WordPiece of never labeling tokens as \"unknown\". Additionally, SOBert is trained with a a maximum sequence length of 2048 based on the empirical length distribution of StackOverflow posts (see Figure 4) and a relatively large batch size of 0.5M tokens (following Kaplan et al. [22]). Our model performs substantially better than both BertOverflow and BERT models trained on general corpora of text on all downstream tasks (see Section 5).\n\n\nAPPROACH\n\nStackOverflow (SO) contains code in various languages with rich natural language information about the code. In this section we describe how we design SOBert, a BERT model trained on bimodal data from 15 million StackOverflow questions and answers.\n\n\nData and Preprocessing\n\nWe downloaded the SO data dump published on 7 March 2022 [2] which includes posts from 2008-2022. The Stack Overflow data dump is a collection of structured data that includes all information publicly available on the website. The data is made available for download periodically and includes information such as user profiles, questions, answers, comments, tags, and votes. The data is provided in XML format and is compressed into a set of files that can be downloaded and processed using a variety of tools and programming languages. Each file corresponds to a specific type of data, and contains all the relevant information for that type. We loaded these files into SQL database tables and specifically worked with the 'Posts' and 'Comments' tables. We filter the posts to extract only those that meet the criteria of having a minimum of one upvote and at least one accompanying comment. Although the second criterion is not strictly necessary for capturing higher-quality posts, it allows us to prioritize learning from posts with community input. We then used this filter and extracted both answer posts along with all their associated comments. This yielded 15 million answers and 39.5 million comments (median 2, mean 2.68 comments per post).\n\n\nData decisions\n\nDevlin et al. [15] and Tabassum et al. [37] tokenize the datasets for BERT and BERTOverflow respectively using a WordPiece vocabulary with 30,522 tokens. We preprocess the data for SOBert using a 50,000 token Byte-Pair Encoded (BPE) vocabulary, trained using SentencePiece [25]. BPE sub-tokenizes words by greedily finding the largest tokens in its vocabulary that collectively reconstruct the word. While common words such as \"the\" tend to appear as a single token in the vocabulary, rare words may be broken up into smaller pieces. Since BPE uses a large vocabulary of single characters as its base vocabulary, it avoids treating almost any token as \"unknown\" or out-of-vocabulary, in contrast to WordPiece, which tends to improve results on downstream tasks [23]. We trained our tokenizer using a randomly selected subset of 10% of the entire dataset (ca. 2GB) due to memory limitations. The input text is first cleaned. We make sure to keep blocks of code, demarcated with <code> </code> tags, in the input data to preserve the complete structure of each post. This ensures that any code snippets or programming language syntax are not altered or misrepresented during processing or analysis. After that the data is tokenized using the SentencePiece tokenizer. A reserved token, <RS>, is placed between the answer and each of the comments to signal to the model where each comment begins. This tokenized representation is then passed through a stack of Transformer layers, described next.\n\n\nModel Design\n\nFor the BERT base model the maximum length of the input sequence for the model is 512 tokens and each token is represented with a 768 dimensional vector. The choice of 2048 as the maximum length of the input sequence for SOBert was informed by an analysis of the text lengths of combined posts + comments that we use as inputs to our approach, shown in Figure 4. This shows that rather few posts are no more than 512 tokens long, which has conventionally been used as the maximum input length [16,43]. A slight majority of the posts contain up to 1024 tokens but many more posts span between 1024-2048 tokens. As the cost of encoding long inputs grows quadratically in attention-based models, we choose to set the maximum length of inputs in our study to 2048 to balance training cost and fidelity to the task domain.\n\nBatch size refers to the number of training examples that are processed together in a single iteration during training. Larger batch sizes tend to lead to better language model performance [22] by providing more stable gradients for each model update, but they also require more memory and computing resources. We use gradient accumulation to simulate large batch sizes on our limited hardware, allowing us to process the training data in batches of 0.5 million tokens for both models. DeepMind proposed a series of \"scaling laws\" that can be used to determine how to best balance a given compute budget between training larger models and providing models more data [21]. Their conclusion was that models should be trained with ca. 20 times more tokens than they have parameters. While models trained this way won't have converged (training longer would continue to yield better performance), the empirical observation is that this minimizes loss for a given compute budget. For instance, if one had the budget to train a 1 billion parameter model with 50B tokens, it would instead be better to train a significantly larger (e.g., 1.5B parameter) model with 30B tokens. The latter would achieve a lower loss in the same training time. This \"rule\" thus provides a useful lower bound for the training budget that should be allocated to a given model. We determined that, with our compute budget (see Section 3.4), the largest model that we would be able to train to this \"optimum\" within ca. two weeks time contained ca. 760M parameters. We therefore aimed to train this model with at least 15B tokens. Our actual training resources enabled us to provide nearly twice as many tokens as this minimum, which naturally improved its performance ( Figure 5). 4 We train our models using the Megatron-LM toolkit [34]. This toolkit was introduced for efficient parallel model training of LLMs with up to 8.3B parameters. It provides powerful support for multi-GPU training with optimized kernels for many components. It also incorporates many architectural best-practices. For instance, Shoeybi et al. [34] showed that rearranging the order of the layer normalization and the residual connections is critical to enabling the scaling of the BERT-style models beyond 336M parameters. We use the same architecture configuration for SOBert.\n\n\nTraining details\n\nThe training was performed on four NVIDIA Quadro RTX 8000 GPUs, each with 48GB of memory, hosted in an on-premise server. To calculate the cost of training our models for those without access to this hardware, we base our estimates on quotes for NVIDIA's A6000 GPU series, as these are currently widely available via cloud computing environments and have the same memory capacity as our GPUs. Nevertheless, A6000 GPUs outperform the Quadro RTX 8000 by ca. 80% [6]. We thus take the GPU hours required to train each of our models and multiply them by the standard rate of an A6000 in the cloud, which is approximately $1 per hour [5], and by the ratio 1/1.8 to compensate for the performance disparity. Training the SOBertLarge model required ca. 15 days on four GPUs for a total of 1,440 GPU hours, which is equivalent to $800. The SOBertBase model required just 3.5 days (336 GPU hours) to train, which amounts to $187. Figure 5 shows the comparison between the training and validation loss curves of SOBertBase and SOBertLarge. Both SOBertBase and SOBertLarge were trained for 50,000 steps at which point they had seen around 27 billion tokens.\n\n\nEVALUATION\n\nIn this section we define the metrics we use to evaluate and compare the models and describe each of the four StackOverflow (SO) related downstream tasks, most of which come from prior research or Kaggle competitions. The four tasks we consider are question quality prediction [7], closed question prediction [1], named entity recognition (NER) [37] and a novel task we formulate called obsoletion detection. We try to cover different types of downstream tasks with varying amounts of data available for fine-tuning. The above tasks, described in detail below, include a multi-class classification problem with about 60k samples, a multi-class classification problem with 140k samples, a token-level tagging task with 15k samples and a binary class classification with just 942 samples.\n\nOur evaluation of the models involves a comparative analysis with two general-purpose BERT models, namely BERTBase [15] and BERTLarge [15]. In addition, we also assess the performance of the models against a domain-specific model called BERTOverflow [37] as well as with ChatGPT [4], although we do not fine-tune the latter. In all cases, each model is trained with the same hyperparameters, typically involving a batch size of 32 samples and a learning rate of 1e-5. Small variations from task to task, mainly in the input length, are noted below. We note that, although our model is capable of handling longer sequences, we set the maximum sequence length to be the same as that of the baseline models for all our experiments.\n\n\nMetrics\n\nTwo of our tasks (NER and Obsoletion) have imbalanced datasets. An imbalanced dataset is one in which the distribution of classes is not uniform, meaning that one class may be over-represented or under-represented in comparison to the other classes. If the model always predicts \"not-obsolete\" or \"O\" the recall will be high, while a model that nearly never predicts \"obsolete\" or any of the other NER tags except when highly certain will achieve high precision. Metrics such as accuracy also tend to overvalue the performance on the majority class in imbalanced datasets. Class-weighted accuracy adjusts the contribution of each class to the overall accuracy score based on the inverse of the class frequency, giving more weight to the underrepresented classes. This offers a better indicator of a model's true performance. The formula for weighted accuracy is:\n= \u2211\ufe01 =1 *\nWhere is the number of instances belonging to class , is the total number of instances in the dataset, is the total number of classes, and is the accuracy of the model on class . Similarly, weighted recall is a performance metric used in classification tasks to measure the ability of a model to correctly identify all positive samples, taking into account the class imbalance. Recall is a measure of how well the model can identify positive samples from the total number of positive samples in the dataset. The formula for weighted recall incorporates class-weights as follows:\n= =1 * =1 *\nWhere is the number of classes, is the weight assigned to each class , TruePositives is the number of correctly predicted positive samples for class , and ActualPositives is the total number of actual positive samples for class . The class weights are typically selected according to their inverse frequency, as in the weighted accuracy formula.\n\nNeither nor fully captures a model's performance. 1 \u2212 takes both and into account and is their harmonic mean. Its class-weighted formula is:\n1 = =1 * 1 * =1 *\nIn which 1 is the F1 score for class .\n\n\nTask 1: Named entity recognition (NER)\n\nTabassum et al. [37] release a dataset for the extraction of softwarerelated named entities from StackOverflow. It spans roughly 10 years (from September 2008 to March 2018) of question-answer threads in which each token in each sentence is manually annotated with close to 50 types of entities. The StackOverflow NER corpus has 9,352 train, 2,942 development and 3,115 test sentences. We finetuned all the models with a batch size of 16, maximum sequence length of 128 (based on the maximum length of the samples in the dataset) and learning rate of 1e-4. 5 We use a weighted cross entropy loss function as the dataset is imbalanced with many more 'O' (no label) tags than others. We were able to replicate the results of [37] within a 5%-10% margin for all reported metrics with this configuration on both their dev and test sets. The results of this task are discussed in the next section.\n\n\nTask 2: Quality Tag prediction\n\nWe use another Kaggle competition dataset for predicting question quality on SO [7] for this downstream task. This dataset features 60,000 SO questions from 2016-2020 classified into three categories based on their quality:\n\n\u2022 HQ: High-quality posts without a single edit.\n\n\u2022 LQ_EDIT: Low-quality posts with a negative score, and multiple community edits. However, they still remain open after those changes. \u2022 LQ_CLOSE: Low-quality posts that were closed by the community without a single edit.\n\nWe use a batch size of 32, learning rate of 1e-5 and set the maximum sequence length as 512 for this task. The results of this task are discussed in the next section.\n\n\nTask 3: Closed question prediction\n\nWe choose a Kaggle competition dataset for predicting closed questions on SO [1] for this downstream task. The provided dataset contains 140,272 samples. The competition motivates this task by saying that SO is a widely-used platform that provides programmers with high-quality answers to their programming questions on a daily basis. At the time this competition was published, SO received over 6,000 questions every weekday and an automatic content moderation would help maintain post quality. One aspect of moderation involves closing questions that are deemed inappropriate for the platform. At the time this competition was published, around 6% of all new questions ended up being closed for various reasons. The purpose of the competition is to analyze said quality and predict when questions should be closed for effective moderation. They excluded the 'exact duplicate' closed reason from the dataset as it depends on previous questions. The competition requires the development of a classifier that can predict whether a question will be closed or not, based on the submitted question and the reason for closure. The categories of classification are 'open', 'not a real question', 'off topic', 'not constructive' and 'too localized'.\n\nThe competition provides a plethora of features from user data, however for simplicity we use the question title and question body for the classification task at hand. We use a batch size of 32, learning rate of 1e-5 and set the maximum sequence length as 512 for this task. The results of this task are discussed in the next section.\n\n\nTask 4: Obsolete post detection (New)\n\nGiven the speed at which technology advances, answers on Stack-Overflow often become outdated. Outdated answers are any of those that propose solutions that are no longer state-of-the-art. They may offer solutions that are less performant or precise than what is now available, use now-deprecated APIs and libraries (be obsolete), and in some cases propose solutions that create security concerns. For instance, there have been more than 25 versions of TensorFlow [3] since its release in 2015, including a major change to its entire compute structure (version 2). This library is also a highly popular subject of questions. Many of those are now years old and involve classes, methods, and parameters that have since been renamed. More generally, over 24 million answers contain code snippets, which software engineers often hope to reuse. When such One might hope that such \"outdatedness\" is easily discovered by e.g. looking at the date posted, or comments pointing out such issues. However, date is not always a good indicator of obsolence (one type of outdatedness); Zhang et al. [45] found that more than half of obsolete answers were already obsolete at the time of posting. Indeed, 'year of posting' only poorly explained outdatedness in our analysis as well. Comments are a useful resource: many posts have comments to indicate obsoletion. However, commenting or editing answers is time-consuming, so many posts with obsolete content go unmarked. Downvoting obsolete answers is ineffective, as posts that were once correct and popular require many downvotes to yield a negative score. SO's maintainers are aware of this problem and have recently proposed explicitly marking outdated answers as such as shown in Figure 6. However, there are only 9 such answers marked as obsolete by SO at the time this paper was written. We thus propose to fine-tune SOBert to be able to predict these obsolete answers based on a manually curated dataset, which we describe next.\n\n\nRaw Data Collection.\n\nWe curate an initial dataset of obsolete answer candidates based on three heuristics described below, which are selected to provide the model with a wide range of patterns to learn from. In all cases, we focused only on answers with at least some community support, filtering out answers which had less than 1 upvote to reduce noise from potentially bad solutions.\n\nComments With Keywords. Users often leave comments on a post or answer to indicate obsoletion. We selected answer threads on the basis of the selection criteria from [45]. Comments were chosen which had one of the four keywords: deprecated, outdated, obsolete or out of date, while the corresponding question posts did not have these keywords in them. We found 85,586 such samples of comments.\n\nAnswers Edited After Comment. Often, though far from universally, when comments point out issues with an existing answer, that answer is edited to resolve that concern. Yet it is also not uncommon for answers to be edited even without such prompting, sometimes for cosmetic reasons, often soon after posting. Other edits are more substantial and often dramatically revise and extend an answer. To prioritize discovering the latter kind, we chose answers that were updated after a comment was posted and the Levenshtein distance between the code before and after edits was at least 100, indicating a significant change to the answer. We also filter out answers which have been included while considering the comments with keywords to prevent these answers from being repeated in our dataset. We find 388,809 samples of answers edited substantially following a comment.\n\nAnswers Added Late. Often, questions continue to draw attention for years after posting from developers facing similar issues, but the originally accepted answer ceases to be relevant. When this happens, the simple solution would be to simply edit the existing answer, but for various reasons 6 this is not always the path taken. Rather, we found many cases where a new answer was posted long after the original/accepted one, which usually references the previous answers in some way. These may serve various purposes, such as showing how a new feature can be used for a better solution than what was previously posted. We included such answers that were posted 1.5 years or more after the initial question was posted, where the answer had more than one upvote and referred to another answer on the same question thread. The check for reference to another answer is heuristic and keyword based, where we look for the keywords 's answers, answer by, accepted answer and other answer. We found 19,371 such samples.\n\n\nData Annotation.\n\nAfter applying our heuristic selection criteria described above we obtain some 493K samples from the total database. We manually annotate 1000 random samples out of these, sampling equally from the three categories. The goal of the annotation process is to produce samples consisting solely of the text in an answer or comment on the answer along with a classification of obsolete or otherwise. Answers or comments that either include or refer to obsolete code in any way (e.g., comments pointing out obsolete code in the answer or answers edited to point out they contain outdated solutions) are classified as obsolete, regardless of whether newer techniques or alternative solutions are also provided in the same. This way, models trained on this dataset can be used to highlight posts that should be marked as obsolete using SO's new system based on existing community input.\n\nThe annotation protocol was developed by having two professional programmers annotate a pilot set of answers independently. After completion of the pilot annotation, the differences and difficult-to-annotate cases were discussed and details of such cases were documented. This protocol was improved iteratively by both professionals on other pilot sample sets until consensus was reached about the protocol. The data annotation process relied on the protocol and a set of 10 annotated samples that served as guidelines.\n\nThe pilot annotation sets were then discarded and the same samples were re-annotated using the finalized protocol. The interannotator agreement of the annotations using the finalized protocol has a Cohen's kappa of 0.63, which indicates that the agreement level is substantial [27].\n\nThis yielded many non-obsolete examples -while many comments and new answers contain the keywords we relied on, and many answers are heavily edited over time, these often signified spurious concerns, so the task for the models is nontrivial. We finetuned all the models with a batch size of 32, maximum sequence length of 512 (this being the largest sequence length supported by the baseline models) using a learning rate of 1e-4. We use a weighted cross entropy loss function as the dataset is imbalanced with many more non-obsolete samples than obsolete ones.\n\n\nRESULTS AND DISCUSSION\n\nWe fine-tune the SOBert models and the three baseline BERT models on the four downstream tasks described in Section 4. Figure 7 shows the F1-scores attained by the different models juxtaposed with their respective sizes (note the log-scaling and broken x-axis). In the NER task, we utilized the test set provided by the authors and observed a significant improvement of +12.9 points in the F1score when comparing SOBertBase to BERTOverflow. We found that SOBertLarge had a greater improvement of +19 in F1-score over BERTLarge, with SOBertBase coming in close behind with an increase of +18 in F1-score over BERTLarge. Both SOBert models had an increase of about +25 over BERTBase. SOBertLarge also had higher accuracy, precision, and recall values of 0.77, 0.89, and 0.77, respectively, compared to BERTLarge's scores of 0.56, 0.83, and 0.56. Additionally, when compared to the values reported in the paper for SoftNER, which incorporates a context-independent code token classifier with corpus-level features on top of BERTOverflow to further increase performance, we found that both SOBert models had an improvement of ca. +2 in F1-score over SoftNER.\n\nIn the second task, quality tag prediction, we found the SOBert-Large model to be particularly powerful, yielding a +19 point F1-score improvement compared to BERTOverflow. Our smaller, SOBertBase also improved over BERTOverflow's F1 score by +14 points and by +15 points over BERTLarge.\n\nIn the third task, which involves closed question prediction, we observed an increase of +18 in F1-score between SOBertBase and BERTOverflow, and an increase of +16 between SOBertLarge and BERTOverflow. This marks the first task where our Large model is slightly outperformed by the smaller one -we discuss this phenomenon below. The F1-score for closed question prediction was relatively low for all the models in this task, with the highest F1score achieved by SOBertBase at 0.61. While low, these scores are similar to the results reported in recent studies on the same task [33]. The dataset used in this task contains several features, such as CreationDate, Tags, Reputation, Title, BodyMarkdown, etc. Roy and Singh [33] have explored the use of feature selection techniques and chose to use BodyMarkdown. We used both the Title and Body-Markdown.\n\nIn the fourth task which involves obsoletion detection, we see large gains on the F1-score between SOBert and all the other baselines models. The F1 score of BERTOverflow is significantly lower than that of all other baselines. In 5.2 we delve into potential causes for this behavior and explore possible explanations.  (1) NER, (2) quality tag prediction, (3) closed question prediction, and (4) obsolete prediction. The models evaluated include BERT (base and large), SOBert (base and large), BERTOverflow. We include SoftNER for the NER task. We also include ChatGPT@0 and ChatGPT@3, which consist of zero-shot and 3-shot prompting ChatGPT respectively, for closed question prediction, quality tag prediction and obsoletion detection on 250 random samples from the test set. SOBert outperforms the other models on all four tasks.\n\n\nRelation to BERT\n\nIn several experiments, the generic BERT models outperform BERT-Overflow, despite the latter using a very similar architecture and training on StackOverflow data only. In two cases, quality tag and closed question prediction, they even came close to rivaling one of our SOBert models. Their strong performance can likely be attributed by a combination of two causes. Compared to BertOverflow, these BERT models trained on significantly more data (16GB vs. 11GB), which would have helped them learn the patterns in language more effectively. Secondly, the two downstream datasets where they perform best are relatively large, containing 60K and 140K samples respectively. Fine-tuning on a larger dataset allows these models to better overcome the distribution-shift compared to their pre-training setup.\n\n\nRelation to BERTOverflow\n\nFor the final task of obsoletion detection, we find the F1-score increased by a large margin between both SOBert models and BERTOverflow. Although BERTOverflow converges the fastest during fine-tuning between all 5 models compared for this task, we see that it struggles with this classification task. There could be various factors contributing to unsatisfactory performance, which may include some design decisions taken during pre-training the model such as a small context size and the structure of training data. The SO data BERTOverflow was pre-trained on processed individual sentences in isolation which has lead to a loss of contextual information from the original SO corpus. It is also possible that BERTOverflow was inadequately pre-trained, thus rendering it unable to generalize well to newer data and tasks.\n\n\nRelation to model size\n\nIn this study, we noticed that both SOBert sizes perform better than both BERT sizes in all of the four tasks we examined. However, we did not find a consistent trend of the larger models (of either kind) outperforming their smaller counterparts. For example, in closed question prediction and obsoletion detection, SOBertBase outperformed SOBertLarge in terms of F1-score. Similarly, BERTLarge did not perform as well as BERTBase in quality tag prediction. Out of the four tasks, the two large models both performed better in exactly half. While this runs counter to the wisdom that larger language models are better [22], we note that this notion typically applies to models that are not fine-tuned but rather prompted (queried). Finetuning adds the noteworthy constraint that larger models represent their inputs in more hidden dimensions, which means that the task-specific prediction layer(s) also require more parameters. For instance, predicting a 1-dimensional label from a 768-dimensional hidden state (for the Base models) requires training half as many new parameters as transforming the 1,536-dimensional states from the Large models. Tuning more parameters also increase the threat of overfitting as models quickly learn to memorize their training inputs on small datasets. We also speculate that SOBertLarge and BERTLarge are (significantly) underfitting for their size. While both models were trained with more than 20 times as many tokens as parameters (see Section 3.3), they only slightly exceeded that budget, which itself tends to yield models that are far from convergence. Figure 5 underscores this: the validation loss gap between the Base and Large models in our analysis was still widening at the time that we stopped training. In future work, we intend to train these models for substantially more steps to further improve their quality.\n\n\nRelation to ChatGPT\n\nFinally, we compared the results from SOBertBase and SOBertLarge with ChatGPT on 3 downstream tasks -closed question prediction, question quality prediction and obsoletion. While it would be great to be able to test on more samples, due to limits of testing ChatGPT, we mainly evaluate ChatGPT on a small part of the testing set for each of these tasks. Specifically, we chose 250 random samples from the test data (using test+validation for obsoletion detection, the smallest dataset). We use the GPT3.5 API to get predictions from ChatGPT. We get predictions from both SOBertBase and SOBert-Large on the same 250 samples. We compare this with responses from ChatGPT in 2 settings -with only an instruction and no examples (0-shot prompting) and with three examples drawn from the training data (3-shot prompting). The results for each of the tasks are shown in Table 1. These results indicate that although ChatGPT can solve many NLP problems quite well, it doesn't fare too well on tasks in this domain.\n\nA point to consider is that our downstream tasks are essentially classification tasks, and ChatGPT is a decoder-only pre-trained language model as opposed to SOBert which is an encoder-only model. BERT-like models are built upon a bidirectional Masked Language Modeling (MLM) objective that emphasizes the encoding of contextual information. Decoder-only models instead learn to derive representations that are primarily useful for text generation, which prioritizes knowledge from recent tokens. This, combined with its lack of domain-specific fine-tuning (in exchange for much ChatGPT on these tasks. We started this work by arguing that, while many tasks generally benefit from general purpose LLMs, they do not necessarily guarantee cutting-edge performance for tasks where rich domain-specific data is available. The superior results of even our smallest model, SOBertBase, over both ChatGPT and several general-purpose BERT models on our downstream tasks shows that model size is not the only important factor for performance. Our substantial improvement over BertOverflow further highlights that it is crucial to carefully evaluate the requirements, constraints and data available for a given problem and use an appropriate training design to address the specific challenges at hand.\n\n\nCONCLUSION\n\nWe present SOBertBase and SOBertLarge, 109M and 762M parameter models respectively trained exclusively on StackOverflow data. We discuss a number of design decisions inspired by both the training of large language models and the nature of StackOverflow posts that enable us to train our models to a high quality. Among others, we create large samples consisting of entire StackOverflow posts and all their associated comments; we train with large batch sizes using a toolkit that enables efficient parallel training, and train our models extensively. With these configurations, the models we trained could be reproduced with cloud computing resources at a budget of just $187 and $800. We then fine-tuned both our models and an array of BERT baselines on four different downstream tasks: question quality prediction, closed question prediction, named entity recognition and a novel task, obsoletion detection. We compare all our models, as well as ChatGPT (on all but one task) and find that our models consistently achieve top performance in all downstream tasks, with the smaller model regularly performing best. Overall, we underscore the importance of consider the specific needs of a task and the available data when choosing an appropriate model training approach, and provide our guidelines for doing so effectively. Our results highlight that there is ample room for smaller, domain-specific models to outperform general-purpose LLMs at a low budget.\n\nFigure 1 :\n1https://figshare.com/sModel and dataset sizes of state-of-the-art LLMs.\n\nFigure 2 :\n2Framework outlining the key steps of our analysis\n\nFigure 3 :\n3With this post (ID: 14569223)\n\nFigure 4 :\n4Histogram showing length buckets of post+comments samples\n\nFigure 5 :\n5Training and Validation Loss comparison of SOB-Base and SOBLarge.\n\nFigure 6 :\n6Answer 4601538 marked as obsolete on StackOverflow snippets are obsolete, that reuse may severely reduce the quality of their code.\n\nFigure 7 :\n7Comparison of F1-score performance of various models on 4 down-stream tasks:\n\n\nPhase 1: Creation of corpusPhase 2: Train tokenizer and SOBertPhase 3: Fine-tune SOBert on downstream tasksSOTorrent \nGetting all answers \nwith more than 1 vote \nand all their comments \n\nCreate answer and \ncomment records \n\nTrain SentencePiece \ntokenizer \n\nTrain SOBert on \ntokenized corpus \n\nFine-tune SOBert \non various tasks \n\nClosed Question \nPrediction \n\nQuality Tag \nPrediction \n\nNamed Entity \nRecognition \n\nObsoletion \ndetection \n\n\n\nTable 1 :\n1Comparison of task accuracy for SOBertBase, SOBert-Large, ChatGPT@0 and ChatGPT@3.Model \nQuality \nPrediction \n\nClosed \nPrediction \n\nObsoletion \nDetection \n\nChatGPT@0 0.39 \n0.16 \n0.56 \n\nChatGPT@3 0.44 \n0.17 \n0.52 \n\nSOBertBase 0.9 \n0.65 \n0.90 \n\nSOBertLarge 0.89 \n0.64 \n0.88 \n\nmore general capabilities) could explain the lower performance for \n\nWhere dataset size is only provided in tokens, we assume an average token size of 5 bytes. While GPT-3 is likely significantly smaller than state-of-the-art models in the GPT series, OpenAI has not released details on any more recent models.\narXiv'23, 2023, USA Mukherjee, et al.\nIndeed, popular questions frequently have at least one \"meta-answer\" benchmarking and comparing the solutions proposed in the other answers.\nWe follow the conventional GPT-2 architecture sizes here. The typical \"next-size\" model would contain 1.4B parameters, which we estimate we could only train with 10\nHyper-parameters are particularly different here because this task involves one label per token, so a batch of 16 samples has far more labels than in the other, sequence-level prediction tasks.\nSome related to SO's \"reputation\" system, which prevents all-but highly experienced users from editing others' answers.\narXiv'23, 2023, USA   \n\nPredict Closed Questions on Stack Overflow. 2013. Predict Closed Questions on Stack Overflow. https://www.kaggle.com/ competitions/predict-closed-questions-on-stack-overflow\n\n2023. Introducing NVIDIA RTX\u2122 A6000 GPU Instances On Lambda Cloud. 2021. StackExchange. https://stackexchange.com/sites?view=list#users [3] 2021. tensorflow/docs. https://github.com/tensorflow/docs/tree/r1.0/site/en/api_ docs [4] 2023. ChatGPT. https://chat.openai.com/chat [5] 2023. Introducing NVIDIA RTX\u2122 A6000 GPU Instances On Lambda Cloud. https://lambdalabs.com/blog/introducing-nvidia-rtx-a6000-gpu- instances-on-lambda-cloud\n\n2023. Nvidia RTX A6000 48GB Review Roundup. 2023. Nvidia RTX A6000 48GB Review Roundup. https://www.pugetsystems.com/ labs/articles/nvidia-rtx-a6000-48gb-review-roundup-2063/\n\nMulti-View Approach to Suggest Moderation Actions in Community Question Answering Sites. Jafar Issa Annamoradnejad, Mohammadamin Habibi, Fazli, 10.1016/j.ins.2022.03.085Information Sciences. 600Issa Annamoradnejad, Jafar Habibi, and Mohammadamin Fazli. 2022. Multi-View Approach to Suggest Moderation Actions in Community Question Answering Sites. Information Sciences 600 (2022), 144-154. https://doi.org/10.1016/j.ins.2022. 03.085\n\nWhat are developers talking about? an analysis of topics and trends in stack overflow. Anton Barua, W Stephen, Ahmed E Thomas, Hassan, Empirical Software Engineering. 19Anton Barua, Stephen W Thomas, and Ahmed E Hassan. 2014. What are devel- opers talking about? an analysis of topics and trends in stack overflow. Empirical Software Engineering 19 (2014), 619-654.\n\nAchieving reliable sentiment analysis in the software engineering domain using bert. Eeshita Biswas, Mehmet Efruz Karabulut, Lori Pollock, K Vijay-Shanker, 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEEEeshita Biswas, Mehmet Efruz Karabulut, Lori Pollock, and K Vijay-Shanker. 2020. Achieving reliable sentiment analysis in the software engineering domain using bert. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 162-173.\n\nEmotion Recognition on StackOverflow Posts Using BERT. Donald Bleyl, Elham Khorasani Buxton, 2022 IEEE International Conference on Big Data (Big Data). IEEEDonald Bleyl and Elham Khorasani Buxton. 2022. Emotion Recognition on StackOverflow Posts Using BERT. In 2022 IEEE International Conference on Big Data (Big Data). IEEE, 5881-5885.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.\n\n. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374arXiv preprintet al. 2021. Evaluating large language models trained on codeMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se- bastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n\nFast changeset-based bug localization with BERT. Agnieszka Ciborowska, Kostadin Damevski, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software EngineeringAgnieszka Ciborowska and Kostadin Damevski. 2022. Fast changeset-based bug localization with BERT. In Proceedings of the 44th International Conference on Software Engineering. 946-957.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, arXiv:2002.08155Codebert: A pre-trained model for programming and natural languages. arXiv preprintZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020).\n\nStack overflow considered harmful? the impact of copy&paste on android application security. Felix Fischer, Konstantin B\u00f6ttinger, Huang Xiao, Christian Stransky, 2017 IEEE Symposium on Security and Privacy (SP). IEEEYasemin Acar, Michael Backes, and Sascha FahlFelix Fischer, Konstantin B\u00f6ttinger, Huang Xiao, Christian Stransky, Yasemin Acar, Michael Backes, and Sascha Fahl. 2017. Stack overflow considered harmful? the impact of copy&paste on android application security. In 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 121-136.\n\nComparing BERT against traditional machine learning text classification. Santiago Gonz\u00e1lez, - Carvajal, Eduardo C Garrido-Merch\u00e1n, arXiv:2005.13012arXiv preprintSantiago Gonz\u00e1lez-Carvajal and Eduardo C Garrido-Merch\u00e1n. 2020. Comparing BERT against traditional machine learning text classification. arXiv preprint arXiv:2005.13012 (2020).\n\nPTM4Tag: sharpening tag recommendation of stack overflow posts with pre-trained models. Junda He, Bowen Xu, Zhou Yang, Donggyun Han, Chengran Yang, David Lo, Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. the 30th IEEE/ACM International Conference on Program ComprehensionJunda He, Bowen Xu, Zhou Yang, DongGyun Han, Chengran Yang, and David Lo. 2022. PTM4Tag: sharpening tag recommendation of stack overflow posts with pre-trained models. In Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. 1-11.\n\nThe growing cost of deep learning for source code. J Vincent, Anand Ashok Hellendoorn, Sawant, Commun. ACM. 65Vincent J Hellendoorn and Anand Ashok Sawant. 2021. The growing cost of deep learning for source code. Commun. ACM 65, 1 (2021), 31-33.\n\n. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556arXiv preprintet al. 2022. Training compute-optimal large language modelsJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 (2022).\n\nJared Kaplan, Sam Mccandlish, Tom Henighan, B Tom, Benjamin Brown, Rewon Chess, Scott Child, Alec Gray, Jeffrey Radford, Wu, arXiv:2001.08361and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprintJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n\nBig code!= big vocabulary: Open-vocabulary models for source code. Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, Andrea Janes, Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. the ACM/IEEE 42nd International Conference on Software EngineeringRafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and Andrea Janes. 2020. Big code!= big vocabulary: Open-vocabulary models for source code. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 1073-1085.\n\nAutomated Summarization of Stack Overflow Posts. Bonan Kou, Y Di, M Chen, T Zhang, Bonan Kou, Y DI, M CHEN, and T ZHANG. 2023. Automated Summarization of Stack Overflow Posts. ICSE.\n\nSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, arXiv:1808.06226arXiv preprintTaku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 (2018).\n\nBERT for named entity recognition in contemporary and historical German. Kai Labusch, Preu\u00dfischer Kulturbesitz, Clemens Neudecker, David Zellh\u00f6fer, Proceedings of the 15th conference on natural language processing. the 15th conference on natural language processingErlangen, GermanyKai Labusch, Preu\u00dfischer Kulturbesitz, Clemens Neudecker, and David Zellh\u00f6fer. 2019. BERT for named entity recognition in contemporary and historical German. In Proceedings of the 15th conference on natural language processing, Erlangen, Germany. 8-11.\n\nAn application of hierarchical kappatype statistics in the assessment of majority agreement among multiple observers. Richard Landis, Gary G Koch, Biometrics. J Richard Landis and Gary G Koch. 1977. An application of hierarchical kappa- type statistics in the assessment of majority agreement among multiple observers. Biometrics (1977), 363-374.\n\nMulti-task learning based pretrained language model for code completion. Fang Liu, Ge Li, Yunfei Zhao, Zhi Jin, Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering. the 35th IEEE/ACM International Conference on Automated Software EngineeringFang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task learning based pre- trained language model for code completion. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering. 473-485.\n\nShuheng Liu, Alan Ritter, arXiv:2212.09747Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023. arXiv preprintShuheng Liu and Alan Ritter. 2022. Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023? arXiv preprint arXiv:2212.09747 (2022).\n\nSecure coding practices in java: Challenges and vulnerabilities. Na Meng, Stefan Nagy, Danfeng Yao, Wenjie Zhuang, Gustavo Arango Argoty, Proceedings of the 40th International Conference on Software Engineering. the 40th International Conference on Software EngineeringNa Meng, Stefan Nagy, Danfeng Yao, Wenjie Zhuang, and Gustavo Arango Argoty. 2018. Secure coding practices in java: Challenges and vulnerabilities. In Proceedings of the 40th International Conference on Software Engineering. 372-383.\n\nCodegen: An open large language model for code with multi-turn program synthesis. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, arXiv:2203.13474arXiv preprintErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022).\n\nToxic code snippets on stack overflow. Chaiyong Ragkhitwetsagul, Jens Krinke, Matheus Paixao, Giuseppe Bianco, Rocco Oliveto, IEEE Transactions on Software Engineering. Chaiyong Ragkhitwetsagul, Jens Krinke, Matheus Paixao, Giuseppe Bianco, and Rocco Oliveto. 2019. Toxic code snippets on stack overflow. IEEE Transactions on Software Engineering (2019).\n\nPredicting closed questions on community question answering sites using convolutional neural network. Pradeep Kumar Roy, Jyoti Prakash Singh, Neural Computing and Applications. 32Pradeep Kumar Roy and Jyoti Prakash Singh. 2020. Predicting closed questions on community question answering sites using convolutional neural network. Neural Computing and Applications 32, 14 (2020), 10555-10572.\n\nMegatron-lm: Training multi-billion parameter language models using model parallelism. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick Legresley, Jared Casper, Bryan Catanzaro, arXiv:1909.08053arXiv preprintMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).\n\nUtilizing BERT intermediate layers for aspect based sentiment analysis and natural language inference. Youwei Song, Jiahai Wang, Zhiwei Liang, Zhiyue Liu, Tao Jiang, arXiv:2002.04815arXiv preprintYouwei Song, Jiahai Wang, Zhiwei Liang, Zhiyue Liu, and Tao Jiang. 2020. Utiliz- ing BERT intermediate layers for aspect based sentiment analysis and natural language inference. arXiv preprint arXiv:2002.04815 (2020).\n\nHow to fine-tune bert for text classification. Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang, Chinese Computational Linguistics: 18th China National Conference. Kunming, ChinaSpringer18Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to fine-tune bert for text classification?. In Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18-20, 2019, Proceedings 18. Springer, 194-206.\n\nCode and named entity recognition in stackoverflow. Jeniya Tabassum, Mounica Maddela, Wei Xu, Alan Ritter, arXiv:2005.01634arXiv preprintJeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. 2020. Code and named entity recognition in stackoverflow. arXiv preprint arXiv:2005.01634 (2020).\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998-6008.\n\nWhat are software engineers asking about android testing on stack overflow. K Isabel, Villanes, Josias Silvia M Ascate, Arilo Claudio Dias-Neto Gomes, Proceedings of the XXXI Brazilian Symposium on Software Engineering. the XXXI Brazilian Symposium on Software EngineeringIsabel K Villanes, Silvia M Ascate, Josias Gomes, and Arilo Claudio Dias-Neto. 2017. What are software engineers asking about android testing on stack over- flow?. In Proceedings of the XXXI Brazilian Symposium on Software Engineering. 104-113.\n\nOn the validity of pre-trained transformers for natural language processing in the software engineering domain. Alexander Julian Von Der Mosel, Steffen Trautsch, Herbold, arXiv:2109.04738arXiv preprintJulian von der Mosel, Alexander Trautsch, and Steffen Herbold. 2021. On the va- lidity of pre-trained transformers for natural language processing in the software engineering domain. arXiv preprint arXiv:2109.04738 (2021).\n\nFret: Functional reinforced transformer with bert for code summarization. Ruyun Wang, Hanwen Zhang, Guoliang Lu, Lei Lyu, Chen Lyu, IEEE Access. 8Ruyun Wang, Hanwen Zhang, Guoliang Lu, Lei Lyu, and Chen Lyu. 2020. Fret: Functional reinforced transformer with bert for code summarization. IEEE Access 8 (2020), 135591-135604.\n\nAn empirical study on developer interactions in stackoverflow. Shaowei Wang, David Lo, Lingxiao Jiang, Proceedings of the 28th annual ACM symposium on applied computing. the 28th annual ACM symposium on applied computingShaowei Wang, David Lo, and Lingxiao Jiang. 2013. An empirical study on developer interactions in stackoverflow. In Proceedings of the 28th annual ACM symposium on applied computing. 1019-1024.\n\nCodet5: Identifieraware unified pre-trained encoder-decoder models for code understanding and generation. Yue Wang, Weishi Wang, Shafiq Joty, C H Steven, Hoi, arXiv:2109.00859arXiv preprintYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier- aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).\n\nA systematic evaluation of large language models of code. F Frank, Uri Xu, Graham Alon, Vincent Josua Neubig, Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine ProgrammingFrank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. 1-10.\n\nAn empirical study of obsolete answers on Stack Overflow. Haoxiang Zhang, Shaowei Wang, Tse-Hsun Peter Chen, Ying Zou, Ahmed E Hassan, IEEE Transactions on Software Engineering. Haoxiang Zhang, Shaowei Wang, Tse-Hsun Peter Chen, Ying Zou, and Ahmed E Hassan. 2019. An empirical study of obsolete answers on Stack Overflow. IEEE Transactions on Software Engineering (2019).\n\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, Tie-Yan Liu, arXiv:2002.06823Incorporating bert into neural machine translation. arXiv preprintJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020. Incorporating bert into neural machine translation. arXiv preprint arXiv:2002.06823 (2020).\n", "annotations": {"author": "[{\"end\":181,\"start\":108},{\"end\":259,\"start\":182}]", "publisher": null, "author_last_name": "[{\"end\":125,\"start\":116},{\"end\":203,\"start\":192}]", "author_first_name": "[{\"end\":115,\"start\":108},{\"end\":189,\"start\":182},{\"end\":191,\"start\":190}]", "author_affiliation": "[{\"end\":180,\"start\":127},{\"end\":258,\"start\":205}]", "title": "[{\"end\":105,\"start\":1},{\"end\":364,\"start\":260}]", "venue": null, "abstract": "[{\"end\":2513,\"start\":442}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2758,\"start\":2754},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2783,\"start\":2779},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2813,\"start\":2809},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2910,\"start\":2906},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2917,\"start\":2916},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2936,\"start\":2932},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2965,\"start\":2961},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3326,\"start\":3322},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3449,\"start\":3445},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3667,\"start\":3663},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3757,\"start\":3753},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3772,\"start\":3768},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4271,\"start\":4270},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4426,\"start\":4422},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4902,\"start\":4898},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5031,\"start\":5027},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5276,\"start\":5272},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5306,\"start\":5302},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5357,\"start\":5353},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5829,\"start\":5826},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5832,\"start\":5829},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5835,\"start\":5832},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5929,\"start\":5925},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5942,\"start\":5938},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5959,\"start\":5955},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6530,\"start\":6526},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7439,\"start\":7436},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7471,\"start\":7468},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7502,\"start\":7498},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7762,\"start\":7758},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7765,\"start\":7762},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7768,\"start\":7765},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13356,\"start\":13352},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13472,\"start\":13468},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14074,\"start\":14070},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14077,\"start\":14074},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14111,\"start\":14107},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14114,\"start\":14111},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14198,\"start\":14194},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14541,\"start\":14537},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15354,\"start\":15350},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15427,\"start\":15424},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16002,\"start\":15998},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16169,\"start\":16165},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16193,\"start\":16190},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16223,\"start\":16219},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16327,\"start\":16323},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16453,\"start\":16449},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16891,\"start\":16887},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17290,\"start\":17286},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17981,\"start\":17977},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18291,\"start\":18287},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18796,\"start\":18793},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20024,\"start\":20020},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20049,\"start\":20045},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20283,\"start\":20279},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20771,\"start\":20767},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22012,\"start\":22008},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22015,\"start\":22012},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22527,\"start\":22523},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23004,\"start\":23000},{\"end\":24087,\"start\":24086},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24142,\"start\":24138},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24431,\"start\":24427},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25145,\"start\":25142},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26123,\"start\":26120},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26155,\"start\":26152},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26192,\"start\":26188},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26750,\"start\":26746},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26769,\"start\":26765},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26885,\"start\":26881},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29442,\"start\":29438},{\"end\":29980,\"start\":29979},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30149,\"start\":30145},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30432,\"start\":30429},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31131,\"start\":31128},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":33760,\"start\":33756},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35203,\"start\":35199},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":39012,\"start\":39008},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41630,\"start\":41626},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41773,\"start\":41769},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":45057,\"start\":45053}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50178,\"start\":50094},{\"attributes\":{\"id\":\"fig_1\"},\"end\":50241,\"start\":50179},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50284,\"start\":50242},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50355,\"start\":50285},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50434,\"start\":50356},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50579,\"start\":50435},{\"attributes\":{\"id\":\"fig_7\"},\"end\":50669,\"start\":50580},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":51110,\"start\":50670},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":51465,\"start\":51111}]", "paragraph": "[{\"end\":3552,\"start\":2529},{\"end\":4775,\"start\":3554},{\"end\":6805,\"start\":4777},{\"end\":7154,\"start\":6807},{\"end\":8284,\"start\":7156},{\"end\":9125,\"start\":8286},{\"end\":9605,\"start\":9140},{\"end\":12345,\"start\":9633},{\"end\":13206,\"start\":12377},{\"end\":14115,\"start\":13208},{\"end\":14910,\"start\":14117},{\"end\":16003,\"start\":14942},{\"end\":17642,\"start\":16005},{\"end\":18448,\"start\":17644},{\"end\":18709,\"start\":18461},{\"end\":19987,\"start\":18736},{\"end\":21498,\"start\":20006},{\"end\":22332,\"start\":21515},{\"end\":24661,\"start\":22334},{\"end\":25828,\"start\":24682},{\"end\":26629,\"start\":25843},{\"end\":27359,\"start\":26631},{\"end\":28233,\"start\":27371},{\"end\":28822,\"start\":28244},{\"end\":29180,\"start\":28835},{\"end\":29322,\"start\":29182},{\"end\":29379,\"start\":29341},{\"end\":30314,\"start\":29422},{\"end\":30572,\"start\":30349},{\"end\":30621,\"start\":30574},{\"end\":30844,\"start\":30623},{\"end\":31012,\"start\":30846},{\"end\":32293,\"start\":31051},{\"end\":32629,\"start\":32295},{\"end\":34642,\"start\":32671},{\"end\":35031,\"start\":34667},{\"end\":35426,\"start\":35033},{\"end\":36295,\"start\":35428},{\"end\":37309,\"start\":36297},{\"end\":38208,\"start\":37330},{\"end\":38729,\"start\":38210},{\"end\":39013,\"start\":38731},{\"end\":39576,\"start\":39015},{\"end\":40757,\"start\":39603},{\"end\":41046,\"start\":40759},{\"end\":41900,\"start\":41048},{\"end\":42734,\"start\":41902},{\"end\":43557,\"start\":42755},{\"end\":44408,\"start\":43586},{\"end\":46298,\"start\":44435},{\"end\":47328,\"start\":46322},{\"end\":48620,\"start\":47330},{\"end\":50093,\"start\":48635}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":28243,\"start\":28234},{\"attributes\":{\"id\":\"formula_1\"},\"end\":28834,\"start\":28823},{\"attributes\":{\"id\":\"formula_2\"},\"end\":29340,\"start\":29323}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":47192,\"start\":47185}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2527,\"start\":2515},{\"attributes\":{\"n\":\"2\"},\"end\":9138,\"start\":9128},{\"attributes\":{\"n\":\"2.1\"},\"end\":9631,\"start\":9608},{\"attributes\":{\"n\":\"2.2\"},\"end\":12375,\"start\":12348},{\"attributes\":{\"n\":\"2.3\"},\"end\":14940,\"start\":14913},{\"attributes\":{\"n\":\"3\"},\"end\":18459,\"start\":18451},{\"attributes\":{\"n\":\"3.1\"},\"end\":18734,\"start\":18712},{\"attributes\":{\"n\":\"3.2\"},\"end\":20004,\"start\":19990},{\"attributes\":{\"n\":\"3.3\"},\"end\":21513,\"start\":21501},{\"attributes\":{\"n\":\"3.4\"},\"end\":24680,\"start\":24664},{\"attributes\":{\"n\":\"4\"},\"end\":25841,\"start\":25831},{\"attributes\":{\"n\":\"4.1\"},\"end\":27369,\"start\":27362},{\"attributes\":{\"n\":\"4.2\"},\"end\":29420,\"start\":29382},{\"attributes\":{\"n\":\"4.3\"},\"end\":30347,\"start\":30317},{\"attributes\":{\"n\":\"4.4\"},\"end\":31049,\"start\":31015},{\"attributes\":{\"n\":\"4.5\"},\"end\":32669,\"start\":32632},{\"attributes\":{\"n\":\"4.5.1\"},\"end\":34665,\"start\":34645},{\"attributes\":{\"n\":\"4.5.2\"},\"end\":37328,\"start\":37312},{\"attributes\":{\"n\":\"5\"},\"end\":39601,\"start\":39579},{\"attributes\":{\"n\":\"5.1\"},\"end\":42753,\"start\":42737},{\"attributes\":{\"n\":\"5.2\"},\"end\":43584,\"start\":43560},{\"attributes\":{\"n\":\"5.3\"},\"end\":44433,\"start\":44411},{\"attributes\":{\"n\":\"5.4\"},\"end\":46320,\"start\":46301},{\"attributes\":{\"n\":\"6\"},\"end\":48633,\"start\":48623},{\"end\":50105,\"start\":50095},{\"end\":50190,\"start\":50180},{\"end\":50253,\"start\":50243},{\"end\":50296,\"start\":50286},{\"end\":50367,\"start\":50357},{\"end\":50446,\"start\":50436},{\"end\":50591,\"start\":50581},{\"end\":51121,\"start\":51112}]", "table": "[{\"end\":51110,\"start\":50779},{\"end\":51465,\"start\":51205}]", "figure_caption": "[{\"end\":50178,\"start\":50107},{\"end\":50241,\"start\":50192},{\"end\":50284,\"start\":50255},{\"end\":50355,\"start\":50298},{\"end\":50434,\"start\":50369},{\"end\":50579,\"start\":50448},{\"end\":50669,\"start\":50593},{\"end\":50779,\"start\":50672},{\"end\":51205,\"start\":51123}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4096,\"start\":4088},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":6510,\"start\":6502},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10541,\"start\":10533},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16868,\"start\":16860},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18211,\"start\":18203},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21876,\"start\":21868},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24083,\"start\":24075},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25611,\"start\":25603},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34399,\"start\":34391},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39730,\"start\":39722},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":46038,\"start\":46030}]", "bib_author_first_name": "[{\"end\":53269,\"start\":53264},{\"end\":53303,\"start\":53291},{\"end\":53701,\"start\":53696},{\"end\":53710,\"start\":53709},{\"end\":53725,\"start\":53720},{\"end\":53727,\"start\":53726},{\"end\":54068,\"start\":54061},{\"end\":54083,\"start\":54077},{\"end\":54089,\"start\":54084},{\"end\":54105,\"start\":54101},{\"end\":54116,\"start\":54115},{\"end\":54546,\"start\":54540},{\"end\":54569,\"start\":54554},{\"end\":54865,\"start\":54862},{\"end\":54881,\"start\":54873},{\"end\":54892,\"start\":54888},{\"end\":54907,\"start\":54900},{\"end\":54922,\"start\":54917},{\"end\":54924,\"start\":54923},{\"end\":54941,\"start\":54933},{\"end\":54958,\"start\":54952},{\"end\":54978,\"start\":54972},{\"end\":54992,\"start\":54986},{\"end\":55007,\"start\":55001},{\"end\":55354,\"start\":55350},{\"end\":55366,\"start\":55361},{\"end\":55381,\"start\":55375},{\"end\":55393,\"start\":55387},{\"end\":55408,\"start\":55400},{\"end\":55439,\"start\":55434},{\"end\":55453,\"start\":55448},{\"end\":55467,\"start\":55463},{\"end\":55483,\"start\":55475},{\"end\":55496,\"start\":55492},{\"end\":55868,\"start\":55859},{\"end\":55886,\"start\":55880},{\"end\":55900,\"start\":55895},{\"end\":55916,\"start\":55909},{\"end\":55930,\"start\":55924},{\"end\":55943,\"start\":55939},{\"end\":55957,\"start\":55953},{\"end\":55984,\"start\":55977},{\"end\":56001,\"start\":55992},{\"end\":56418,\"start\":56409},{\"end\":56439,\"start\":56431},{\"end\":56854,\"start\":56849},{\"end\":56871,\"start\":56863},{\"end\":56885,\"start\":56879},{\"end\":56899,\"start\":56891},{\"end\":57144,\"start\":57136},{\"end\":57155,\"start\":57151},{\"end\":57165,\"start\":57161},{\"end\":57175,\"start\":57172},{\"end\":57191,\"start\":57182},{\"end\":57202,\"start\":57198},{\"end\":57215,\"start\":57209},{\"end\":57226,\"start\":57222},{\"end\":57236,\"start\":57232},{\"end\":57247,\"start\":57242},{\"end\":57694,\"start\":57689},{\"end\":57714,\"start\":57704},{\"end\":57731,\"start\":57726},{\"end\":57747,\"start\":57738},{\"end\":58222,\"start\":58214},{\"end\":58234,\"start\":58233},{\"end\":58252,\"start\":58245},{\"end\":58254,\"start\":58253},{\"end\":58573,\"start\":58568},{\"end\":58583,\"start\":58578},{\"end\":58592,\"start\":58588},{\"end\":58607,\"start\":58599},{\"end\":58621,\"start\":58613},{\"end\":58633,\"start\":58628},{\"end\":59103,\"start\":59102},{\"end\":59124,\"start\":59113},{\"end\":59306,\"start\":59300},{\"end\":59326,\"start\":59317},{\"end\":59343,\"start\":59337},{\"end\":59357,\"start\":59352},{\"end\":59377,\"start\":59371},{\"end\":59388,\"start\":59383},{\"end\":59406,\"start\":59401},{\"end\":59419,\"start\":59415},{\"end\":59424,\"start\":59420},{\"end\":59440,\"start\":59432},{\"end\":59457,\"start\":59452},{\"end\":59839,\"start\":59834},{\"end\":59851,\"start\":59848},{\"end\":59867,\"start\":59864},{\"end\":59879,\"start\":59878},{\"end\":59893,\"start\":59885},{\"end\":59906,\"start\":59901},{\"end\":59919,\"start\":59914},{\"end\":59931,\"start\":59927},{\"end\":59945,\"start\":59938},{\"end\":60365,\"start\":60351},{\"end\":60383,\"start\":60379},{\"end\":60397,\"start\":60391},{\"end\":60413,\"start\":60406},{\"end\":60428,\"start\":60422},{\"end\":60899,\"start\":60894},{\"end\":60906,\"start\":60905},{\"end\":60912,\"start\":60911},{\"end\":60920,\"start\":60919},{\"end\":61143,\"start\":61139},{\"end\":61154,\"start\":61150},{\"end\":61462,\"start\":61459},{\"end\":61483,\"start\":61472},{\"end\":61505,\"start\":61498},{\"end\":61522,\"start\":61517},{\"end\":62047,\"start\":62040},{\"end\":62062,\"start\":62056},{\"end\":62347,\"start\":62343},{\"end\":62355,\"start\":62353},{\"end\":62366,\"start\":62360},{\"end\":62376,\"start\":62373},{\"end\":62788,\"start\":62781},{\"end\":62798,\"start\":62794},{\"end\":63100,\"start\":63098},{\"end\":63113,\"start\":63107},{\"end\":63127,\"start\":63120},{\"end\":63139,\"start\":63133},{\"end\":63155,\"start\":63148},{\"end\":63162,\"start\":63156},{\"end\":63623,\"start\":63619},{\"end\":63635,\"start\":63633},{\"end\":63649,\"start\":63642},{\"end\":63663,\"start\":63659},{\"end\":63672,\"start\":63668},{\"end\":63685,\"start\":63679},{\"end\":63698,\"start\":63692},{\"end\":63716,\"start\":63709},{\"end\":64039,\"start\":64031},{\"end\":64061,\"start\":64057},{\"end\":64077,\"start\":64070},{\"end\":64094,\"start\":64086},{\"end\":64108,\"start\":64103},{\"end\":64457,\"start\":64450},{\"end\":64474,\"start\":64469},{\"end\":64482,\"start\":64475},{\"end\":64836,\"start\":64828},{\"end\":64853,\"start\":64846},{\"end\":64867,\"start\":64863},{\"end\":64881,\"start\":64874},{\"end\":64898,\"start\":64893},{\"end\":64912,\"start\":64907},{\"end\":65297,\"start\":65291},{\"end\":65310,\"start\":65304},{\"end\":65323,\"start\":65317},{\"end\":65337,\"start\":65331},{\"end\":65346,\"start\":65343},{\"end\":65653,\"start\":65650},{\"end\":65665,\"start\":65659},{\"end\":65675,\"start\":65671},{\"end\":65688,\"start\":65680},{\"end\":66102,\"start\":66096},{\"end\":66120,\"start\":66113},{\"end\":66133,\"start\":66130},{\"end\":66142,\"start\":66138},{\"end\":66372,\"start\":66366},{\"end\":66386,\"start\":66382},{\"end\":66400,\"start\":66396},{\"end\":66414,\"start\":66409},{\"end\":66431,\"start\":66426},{\"end\":66444,\"start\":66439},{\"end\":66446,\"start\":66445},{\"end\":66460,\"start\":66454},{\"end\":66474,\"start\":66469},{\"end\":66839,\"start\":66838},{\"end\":66864,\"start\":66858},{\"end\":66905,\"start\":66882},{\"end\":67401,\"start\":67392},{\"end\":67431,\"start\":67424},{\"end\":67784,\"start\":67779},{\"end\":67797,\"start\":67791},{\"end\":67813,\"start\":67805},{\"end\":67821,\"start\":67818},{\"end\":67831,\"start\":67827},{\"end\":68101,\"start\":68094},{\"end\":68113,\"start\":68108},{\"end\":68126,\"start\":68118},{\"end\":68555,\"start\":68552},{\"end\":68568,\"start\":68562},{\"end\":68581,\"start\":68575},{\"end\":68589,\"start\":68588},{\"end\":68591,\"start\":68590},{\"end\":68904,\"start\":68903},{\"end\":68915,\"start\":68912},{\"end\":68926,\"start\":68920},{\"end\":68946,\"start\":68933},{\"end\":69408,\"start\":69400},{\"end\":69423,\"start\":69416},{\"end\":69444,\"start\":69430},{\"end\":69455,\"start\":69451},{\"end\":69466,\"start\":69461},{\"end\":69468,\"start\":69467},{\"end\":69722,\"start\":69716},{\"end\":69734,\"start\":69728},{\"end\":69745,\"start\":69740},{\"end\":69752,\"start\":69750},{\"end\":69760,\"start\":69757},{\"end\":69773,\"start\":69766},{\"end\":69788,\"start\":69780},{\"end\":69800,\"start\":69793}]", "bib_author_last_name": "[{\"end\":53289,\"start\":53270},{\"end\":53310,\"start\":53304},{\"end\":53317,\"start\":53312},{\"end\":53707,\"start\":53702},{\"end\":53718,\"start\":53711},{\"end\":53734,\"start\":53728},{\"end\":53742,\"start\":53736},{\"end\":54075,\"start\":54069},{\"end\":54099,\"start\":54090},{\"end\":54113,\"start\":54106},{\"end\":54130,\"start\":54117},{\"end\":54552,\"start\":54547},{\"end\":54576,\"start\":54570},{\"end\":54871,\"start\":54866},{\"end\":54886,\"start\":54882},{\"end\":54898,\"start\":54893},{\"end\":54915,\"start\":54908},{\"end\":54931,\"start\":54925},{\"end\":54950,\"start\":54942},{\"end\":54970,\"start\":54959},{\"end\":54984,\"start\":54979},{\"end\":54999,\"start\":54993},{\"end\":55014,\"start\":55008},{\"end\":55359,\"start\":55355},{\"end\":55373,\"start\":55367},{\"end\":55385,\"start\":55382},{\"end\":55398,\"start\":55394},{\"end\":55432,\"start\":55409},{\"end\":55446,\"start\":55440},{\"end\":55461,\"start\":55454},{\"end\":55473,\"start\":55468},{\"end\":55490,\"start\":55484},{\"end\":55505,\"start\":55497},{\"end\":55878,\"start\":55869},{\"end\":55893,\"start\":55887},{\"end\":55907,\"start\":55901},{\"end\":55922,\"start\":55917},{\"end\":55937,\"start\":55931},{\"end\":55951,\"start\":55944},{\"end\":55964,\"start\":55958},{\"end\":55975,\"start\":55966},{\"end\":55990,\"start\":55985},{\"end\":56008,\"start\":56002},{\"end\":56018,\"start\":56010},{\"end\":56429,\"start\":56419},{\"end\":56448,\"start\":56440},{\"end\":56861,\"start\":56855},{\"end\":56877,\"start\":56872},{\"end\":56889,\"start\":56886},{\"end\":56909,\"start\":56900},{\"end\":57149,\"start\":57145},{\"end\":57159,\"start\":57156},{\"end\":57170,\"start\":57166},{\"end\":57180,\"start\":57176},{\"end\":57196,\"start\":57192},{\"end\":57207,\"start\":57203},{\"end\":57220,\"start\":57216},{\"end\":57230,\"start\":57227},{\"end\":57240,\"start\":57237},{\"end\":57253,\"start\":57248},{\"end\":57702,\"start\":57695},{\"end\":57724,\"start\":57715},{\"end\":57736,\"start\":57732},{\"end\":57756,\"start\":57748},{\"end\":58231,\"start\":58223},{\"end\":58243,\"start\":58235},{\"end\":58270,\"start\":58255},{\"end\":58576,\"start\":58574},{\"end\":58586,\"start\":58584},{\"end\":58597,\"start\":58593},{\"end\":58611,\"start\":58608},{\"end\":58626,\"start\":58622},{\"end\":58636,\"start\":58634},{\"end\":59111,\"start\":59104},{\"end\":59136,\"start\":59125},{\"end\":59144,\"start\":59138},{\"end\":59315,\"start\":59307},{\"end\":59335,\"start\":59327},{\"end\":59350,\"start\":59344},{\"end\":59369,\"start\":59358},{\"end\":59381,\"start\":59378},{\"end\":59399,\"start\":59389},{\"end\":59413,\"start\":59407},{\"end\":59430,\"start\":59425},{\"end\":59450,\"start\":59441},{\"end\":59463,\"start\":59458},{\"end\":59470,\"start\":59465},{\"end\":59846,\"start\":59840},{\"end\":59862,\"start\":59852},{\"end\":59876,\"start\":59868},{\"end\":59883,\"start\":59880},{\"end\":59899,\"start\":59894},{\"end\":59912,\"start\":59907},{\"end\":59925,\"start\":59920},{\"end\":59936,\"start\":59932},{\"end\":59953,\"start\":59946},{\"end\":59957,\"start\":59955},{\"end\":60377,\"start\":60366},{\"end\":60389,\"start\":60384},{\"end\":60404,\"start\":60398},{\"end\":60420,\"start\":60414},{\"end\":60434,\"start\":60429},{\"end\":60903,\"start\":60900},{\"end\":60909,\"start\":60907},{\"end\":60917,\"start\":60913},{\"end\":60926,\"start\":60921},{\"end\":61148,\"start\":61144},{\"end\":61165,\"start\":61155},{\"end\":61470,\"start\":61463},{\"end\":61496,\"start\":61484},{\"end\":61515,\"start\":61506},{\"end\":61532,\"start\":61523},{\"end\":62054,\"start\":62048},{\"end\":62067,\"start\":62063},{\"end\":62351,\"start\":62348},{\"end\":62358,\"start\":62356},{\"end\":62371,\"start\":62367},{\"end\":62380,\"start\":62377},{\"end\":62792,\"start\":62789},{\"end\":62805,\"start\":62799},{\"end\":63105,\"start\":63101},{\"end\":63118,\"start\":63114},{\"end\":63131,\"start\":63128},{\"end\":63146,\"start\":63140},{\"end\":63169,\"start\":63163},{\"end\":63631,\"start\":63624},{\"end\":63640,\"start\":63636},{\"end\":63657,\"start\":63650},{\"end\":63666,\"start\":63664},{\"end\":63677,\"start\":63673},{\"end\":63690,\"start\":63686},{\"end\":63707,\"start\":63699},{\"end\":63722,\"start\":63717},{\"end\":64055,\"start\":64040},{\"end\":64068,\"start\":64062},{\"end\":64084,\"start\":64078},{\"end\":64101,\"start\":64095},{\"end\":64116,\"start\":64109},{\"end\":64467,\"start\":64458},{\"end\":64488,\"start\":64483},{\"end\":64844,\"start\":64837},{\"end\":64861,\"start\":64854},{\"end\":64872,\"start\":64868},{\"end\":64891,\"start\":64882},{\"end\":64905,\"start\":64899},{\"end\":64922,\"start\":64913},{\"end\":65302,\"start\":65298},{\"end\":65315,\"start\":65311},{\"end\":65329,\"start\":65324},{\"end\":65341,\"start\":65338},{\"end\":65352,\"start\":65347},{\"end\":65657,\"start\":65654},{\"end\":65669,\"start\":65666},{\"end\":65678,\"start\":65676},{\"end\":65694,\"start\":65689},{\"end\":66111,\"start\":66103},{\"end\":66128,\"start\":66121},{\"end\":66136,\"start\":66134},{\"end\":66149,\"start\":66143},{\"end\":66380,\"start\":66373},{\"end\":66394,\"start\":66387},{\"end\":66407,\"start\":66401},{\"end\":66424,\"start\":66415},{\"end\":66437,\"start\":66432},{\"end\":66452,\"start\":66447},{\"end\":66467,\"start\":66461},{\"end\":66485,\"start\":66475},{\"end\":66846,\"start\":66840},{\"end\":66856,\"start\":66848},{\"end\":66880,\"start\":66865},{\"end\":66911,\"start\":66906},{\"end\":67422,\"start\":67402},{\"end\":67440,\"start\":67432},{\"end\":67449,\"start\":67442},{\"end\":67789,\"start\":67785},{\"end\":67803,\"start\":67798},{\"end\":67816,\"start\":67814},{\"end\":67825,\"start\":67822},{\"end\":67835,\"start\":67832},{\"end\":68106,\"start\":68102},{\"end\":68116,\"start\":68114},{\"end\":68132,\"start\":68127},{\"end\":68560,\"start\":68556},{\"end\":68573,\"start\":68569},{\"end\":68586,\"start\":68582},{\"end\":68598,\"start\":68592},{\"end\":68603,\"start\":68600},{\"end\":68910,\"start\":68905},{\"end\":68918,\"start\":68916},{\"end\":68931,\"start\":68927},{\"end\":68953,\"start\":68947},{\"end\":68966,\"start\":68955},{\"end\":69414,\"start\":69409},{\"end\":69428,\"start\":69424},{\"end\":69449,\"start\":69445},{\"end\":69459,\"start\":69456},{\"end\":69475,\"start\":69469},{\"end\":69726,\"start\":69723},{\"end\":69738,\"start\":69735},{\"end\":69748,\"start\":69746},{\"end\":69755,\"start\":69753},{\"end\":69764,\"start\":69761},{\"end\":69778,\"start\":69774},{\"end\":69791,\"start\":69789},{\"end\":69804,\"start\":69801}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":52563,\"start\":52390},{\"attributes\":{\"id\":\"b1\"},\"end\":52997,\"start\":52565},{\"attributes\":{\"id\":\"b2\"},\"end\":53173,\"start\":52999},{\"attributes\":{\"doi\":\"10.1016/j.ins.2022.03.085\",\"id\":\"b3\",\"matched_paper_id\":247967045},\"end\":53607,\"start\":53175},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13252554},\"end\":53974,\"start\":53609},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":226265659},\"end\":54483,\"start\":53976},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":256313216},\"end\":54821,\"start\":54485},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":218971783},\"end\":55346,\"start\":54823},{\"attributes\":{\"doi\":\"arXiv:2107.03374\",\"id\":\"b8\"},\"end\":55857,\"start\":55348},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b9\"},\"end\":56358,\"start\":55859},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":245537466},\"end\":56765,\"start\":56360},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b11\"},\"end\":57134,\"start\":56767},{\"attributes\":{\"doi\":\"arXiv:2002.08155\",\"id\":\"b12\"},\"end\":57594,\"start\":57136},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":25003307},\"end\":58139,\"start\":57596},{\"attributes\":{\"doi\":\"arXiv:2005.13012\",\"id\":\"b14\"},\"end\":58478,\"start\":58141},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":247594801},\"end\":59049,\"start\":58480},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":245274891},\"end\":59296,\"start\":59051},{\"attributes\":{\"doi\":\"arXiv:2203.15556\",\"id\":\"b17\"},\"end\":59832,\"start\":59298},{\"attributes\":{\"doi\":\"arXiv:2001.08361\",\"id\":\"b18\"},\"end\":60282,\"start\":59834},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":211161525},\"end\":60843,\"start\":60284},{\"attributes\":{\"id\":\"b20\"},\"end\":61026,\"start\":60845},{\"attributes\":{\"doi\":\"arXiv:1808.06226\",\"id\":\"b21\"},\"end\":61384,\"start\":61028},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":208192606},\"end\":61920,\"start\":61386},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":40673292},\"end\":62268,\"start\":61922},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":229703606},\"end\":62779,\"start\":62270},{\"attributes\":{\"doi\":\"arXiv:2212.09747\",\"id\":\"b25\"},\"end\":63031,\"start\":62781},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3480894},\"end\":63535,\"start\":63033},{\"attributes\":{\"doi\":\"arXiv:2203.13474\",\"id\":\"b27\"},\"end\":63990,\"start\":63537},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":49318394},\"end\":64346,\"start\":63992},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":207988657},\"end\":64739,\"start\":64348},{\"attributes\":{\"doi\":\"arXiv:1909.08053\",\"id\":\"b30\"},\"end\":65186,\"start\":64741},{\"attributes\":{\"doi\":\"arXiv:2002.04815\",\"id\":\"b31\"},\"end\":65601,\"start\":65188},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":153312532},\"end\":66042,\"start\":65603},{\"attributes\":{\"doi\":\"arXiv:2005.01634\",\"id\":\"b33\"},\"end\":66337,\"start\":66044},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13756489},\"end\":66760,\"start\":66339},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10555085},\"end\":67278,\"start\":66762},{\"attributes\":{\"doi\":\"arXiv:2109.04738\",\"id\":\"b36\"},\"end\":67703,\"start\":67280},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":220939000},\"end\":68029,\"start\":67705},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14123058},\"end\":68444,\"start\":68031},{\"attributes\":{\"doi\":\"arXiv:2109.00859\",\"id\":\"b39\"},\"end\":68843,\"start\":68446},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":247158549},\"end\":69340,\"start\":68845},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":88524386},\"end\":69714,\"start\":69342},{\"attributes\":{\"doi\":\"arXiv:2002.06823\",\"id\":\"b42\"},\"end\":70079,\"start\":69716}]", "bib_title": "[{\"end\":53262,\"start\":53175},{\"end\":53694,\"start\":53609},{\"end\":54059,\"start\":53976},{\"end\":54538,\"start\":54485},{\"end\":54860,\"start\":54823},{\"end\":56407,\"start\":56360},{\"end\":57687,\"start\":57596},{\"end\":58566,\"start\":58480},{\"end\":59100,\"start\":59051},{\"end\":60349,\"start\":60284},{\"end\":61457,\"start\":61386},{\"end\":62038,\"start\":61922},{\"end\":62341,\"start\":62270},{\"end\":63096,\"start\":63033},{\"end\":64029,\"start\":63992},{\"end\":64448,\"start\":64348},{\"end\":65648,\"start\":65603},{\"end\":66364,\"start\":66339},{\"end\":66836,\"start\":66762},{\"end\":67777,\"start\":67705},{\"end\":68092,\"start\":68031},{\"end\":68901,\"start\":68845},{\"end\":69398,\"start\":69342}]", "bib_author": "[{\"end\":53291,\"start\":53264},{\"end\":53312,\"start\":53291},{\"end\":53319,\"start\":53312},{\"end\":53709,\"start\":53696},{\"end\":53720,\"start\":53709},{\"end\":53736,\"start\":53720},{\"end\":53744,\"start\":53736},{\"end\":54077,\"start\":54061},{\"end\":54101,\"start\":54077},{\"end\":54115,\"start\":54101},{\"end\":54132,\"start\":54115},{\"end\":54554,\"start\":54540},{\"end\":54578,\"start\":54554},{\"end\":54873,\"start\":54862},{\"end\":54888,\"start\":54873},{\"end\":54900,\"start\":54888},{\"end\":54917,\"start\":54900},{\"end\":54933,\"start\":54917},{\"end\":54952,\"start\":54933},{\"end\":54972,\"start\":54952},{\"end\":54986,\"start\":54972},{\"end\":55001,\"start\":54986},{\"end\":55016,\"start\":55001},{\"end\":55361,\"start\":55350},{\"end\":55375,\"start\":55361},{\"end\":55387,\"start\":55375},{\"end\":55400,\"start\":55387},{\"end\":55434,\"start\":55400},{\"end\":55448,\"start\":55434},{\"end\":55463,\"start\":55448},{\"end\":55475,\"start\":55463},{\"end\":55492,\"start\":55475},{\"end\":55507,\"start\":55492},{\"end\":55880,\"start\":55859},{\"end\":55895,\"start\":55880},{\"end\":55909,\"start\":55895},{\"end\":55924,\"start\":55909},{\"end\":55939,\"start\":55924},{\"end\":55953,\"start\":55939},{\"end\":55966,\"start\":55953},{\"end\":55977,\"start\":55966},{\"end\":55992,\"start\":55977},{\"end\":56010,\"start\":55992},{\"end\":56020,\"start\":56010},{\"end\":56431,\"start\":56409},{\"end\":56450,\"start\":56431},{\"end\":56863,\"start\":56849},{\"end\":56879,\"start\":56863},{\"end\":56891,\"start\":56879},{\"end\":56911,\"start\":56891},{\"end\":57151,\"start\":57136},{\"end\":57161,\"start\":57151},{\"end\":57172,\"start\":57161},{\"end\":57182,\"start\":57172},{\"end\":57198,\"start\":57182},{\"end\":57209,\"start\":57198},{\"end\":57222,\"start\":57209},{\"end\":57232,\"start\":57222},{\"end\":57242,\"start\":57232},{\"end\":57255,\"start\":57242},{\"end\":57704,\"start\":57689},{\"end\":57726,\"start\":57704},{\"end\":57738,\"start\":57726},{\"end\":57758,\"start\":57738},{\"end\":58233,\"start\":58214},{\"end\":58245,\"start\":58233},{\"end\":58272,\"start\":58245},{\"end\":58578,\"start\":58568},{\"end\":58588,\"start\":58578},{\"end\":58599,\"start\":58588},{\"end\":58613,\"start\":58599},{\"end\":58628,\"start\":58613},{\"end\":58638,\"start\":58628},{\"end\":59113,\"start\":59102},{\"end\":59138,\"start\":59113},{\"end\":59146,\"start\":59138},{\"end\":59317,\"start\":59300},{\"end\":59337,\"start\":59317},{\"end\":59352,\"start\":59337},{\"end\":59371,\"start\":59352},{\"end\":59383,\"start\":59371},{\"end\":59401,\"start\":59383},{\"end\":59415,\"start\":59401},{\"end\":59432,\"start\":59415},{\"end\":59452,\"start\":59432},{\"end\":59465,\"start\":59452},{\"end\":59472,\"start\":59465},{\"end\":59848,\"start\":59834},{\"end\":59864,\"start\":59848},{\"end\":59878,\"start\":59864},{\"end\":59885,\"start\":59878},{\"end\":59901,\"start\":59885},{\"end\":59914,\"start\":59901},{\"end\":59927,\"start\":59914},{\"end\":59938,\"start\":59927},{\"end\":59955,\"start\":59938},{\"end\":59959,\"start\":59955},{\"end\":60379,\"start\":60351},{\"end\":60391,\"start\":60379},{\"end\":60406,\"start\":60391},{\"end\":60422,\"start\":60406},{\"end\":60436,\"start\":60422},{\"end\":60905,\"start\":60894},{\"end\":60911,\"start\":60905},{\"end\":60919,\"start\":60911},{\"end\":60928,\"start\":60919},{\"end\":61150,\"start\":61139},{\"end\":61167,\"start\":61150},{\"end\":61472,\"start\":61459},{\"end\":61498,\"start\":61472},{\"end\":61517,\"start\":61498},{\"end\":61534,\"start\":61517},{\"end\":62056,\"start\":62040},{\"end\":62069,\"start\":62056},{\"end\":62353,\"start\":62343},{\"end\":62360,\"start\":62353},{\"end\":62373,\"start\":62360},{\"end\":62382,\"start\":62373},{\"end\":62794,\"start\":62781},{\"end\":62807,\"start\":62794},{\"end\":63107,\"start\":63098},{\"end\":63120,\"start\":63107},{\"end\":63133,\"start\":63120},{\"end\":63148,\"start\":63133},{\"end\":63171,\"start\":63148},{\"end\":63633,\"start\":63619},{\"end\":63642,\"start\":63633},{\"end\":63659,\"start\":63642},{\"end\":63668,\"start\":63659},{\"end\":63679,\"start\":63668},{\"end\":63692,\"start\":63679},{\"end\":63709,\"start\":63692},{\"end\":63724,\"start\":63709},{\"end\":64057,\"start\":64031},{\"end\":64070,\"start\":64057},{\"end\":64086,\"start\":64070},{\"end\":64103,\"start\":64086},{\"end\":64118,\"start\":64103},{\"end\":64469,\"start\":64450},{\"end\":64490,\"start\":64469},{\"end\":64846,\"start\":64828},{\"end\":64863,\"start\":64846},{\"end\":64874,\"start\":64863},{\"end\":64893,\"start\":64874},{\"end\":64907,\"start\":64893},{\"end\":64924,\"start\":64907},{\"end\":65304,\"start\":65291},{\"end\":65317,\"start\":65304},{\"end\":65331,\"start\":65317},{\"end\":65343,\"start\":65331},{\"end\":65354,\"start\":65343},{\"end\":65659,\"start\":65650},{\"end\":65671,\"start\":65659},{\"end\":65680,\"start\":65671},{\"end\":65696,\"start\":65680},{\"end\":66113,\"start\":66096},{\"end\":66130,\"start\":66113},{\"end\":66138,\"start\":66130},{\"end\":66151,\"start\":66138},{\"end\":66382,\"start\":66366},{\"end\":66396,\"start\":66382},{\"end\":66409,\"start\":66396},{\"end\":66426,\"start\":66409},{\"end\":66439,\"start\":66426},{\"end\":66454,\"start\":66439},{\"end\":66469,\"start\":66454},{\"end\":66487,\"start\":66469},{\"end\":66848,\"start\":66838},{\"end\":66858,\"start\":66848},{\"end\":66882,\"start\":66858},{\"end\":66913,\"start\":66882},{\"end\":67424,\"start\":67392},{\"end\":67442,\"start\":67424},{\"end\":67451,\"start\":67442},{\"end\":67791,\"start\":67779},{\"end\":67805,\"start\":67791},{\"end\":67818,\"start\":67805},{\"end\":67827,\"start\":67818},{\"end\":67837,\"start\":67827},{\"end\":68108,\"start\":68094},{\"end\":68118,\"start\":68108},{\"end\":68134,\"start\":68118},{\"end\":68562,\"start\":68552},{\"end\":68575,\"start\":68562},{\"end\":68588,\"start\":68575},{\"end\":68600,\"start\":68588},{\"end\":68605,\"start\":68600},{\"end\":68912,\"start\":68903},{\"end\":68920,\"start\":68912},{\"end\":68933,\"start\":68920},{\"end\":68955,\"start\":68933},{\"end\":68968,\"start\":68955},{\"end\":69416,\"start\":69400},{\"end\":69430,\"start\":69416},{\"end\":69451,\"start\":69430},{\"end\":69461,\"start\":69451},{\"end\":69477,\"start\":69461},{\"end\":69728,\"start\":69716},{\"end\":69740,\"start\":69728},{\"end\":69750,\"start\":69740},{\"end\":69757,\"start\":69750},{\"end\":69766,\"start\":69757},{\"end\":69780,\"start\":69766},{\"end\":69793,\"start\":69780},{\"end\":69806,\"start\":69793}]", "bib_venue": "[{\"end\":56581,\"start\":56524},{\"end\":58789,\"start\":58722},{\"end\":60585,\"start\":60519},{\"end\":61668,\"start\":61601},{\"end\":62551,\"start\":62475},{\"end\":63302,\"start\":63245},{\"end\":65777,\"start\":65763},{\"end\":67034,\"start\":66982},{\"end\":68251,\"start\":68201},{\"end\":69117,\"start\":69051},{\"end\":52432,\"start\":52390},{\"end\":52630,\"start\":52565},{\"end\":53041,\"start\":52999},{\"end\":53364,\"start\":53344},{\"end\":53774,\"start\":53744},{\"end\":54212,\"start\":54132},{\"end\":54635,\"start\":54578},{\"end\":55065,\"start\":55016},{\"end\":56081,\"start\":56036},{\"end\":56522,\"start\":56450},{\"end\":56847,\"start\":56767},{\"end\":57338,\"start\":57271},{\"end\":57806,\"start\":57758},{\"end\":58212,\"start\":58141},{\"end\":58720,\"start\":58638},{\"end\":59157,\"start\":59146},{\"end\":60038,\"start\":59975},{\"end\":60517,\"start\":60436},{\"end\":60892,\"start\":60845},{\"end\":61137,\"start\":61028},{\"end\":61599,\"start\":61534},{\"end\":62079,\"start\":62069},{\"end\":62473,\"start\":62382},{\"end\":62881,\"start\":62823},{\"end\":63243,\"start\":63171},{\"end\":63617,\"start\":63537},{\"end\":64159,\"start\":64118},{\"end\":64523,\"start\":64490},{\"end\":64826,\"start\":64741},{\"end\":65289,\"start\":65188},{\"end\":65761,\"start\":65696},{\"end\":66094,\"start\":66044},{\"end\":66536,\"start\":66487},{\"end\":66980,\"start\":66913},{\"end\":67390,\"start\":67280},{\"end\":67848,\"start\":67837},{\"end\":68199,\"start\":68134},{\"end\":68550,\"start\":68446},{\"end\":69049,\"start\":68968},{\"end\":69518,\"start\":69477},{\"end\":69872,\"start\":69822}]"}}}, "year": 2023, "month": 12, "day": 17}