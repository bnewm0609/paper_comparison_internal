{"id": 221819358, "updated": "2023-10-06 10:59:56.138", "metadata": {"title": "3D-FUTURE: 3D Furniture shape with TextURE", "authors": "[{\"first\":\"Huan\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Rongfei\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Lin\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Mingming\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Binqiang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Steve\",\"last\":\"Maybank\",\"middle\":[]},{\"first\":\"Dacheng\",\"last\":\"Tao\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 9, "day": 21}, "abstract": "The 3D CAD shapes in current 3D benchmarks are mostly collected from online model repositories. Thus, they typically have insufficient geometric details and less informative textures, making them less attractive for comprehensive and subtle research in areas such as high-quality 3D mesh and texture recovery. This paper presents 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated and large-scale repository of 3D furniture shapes in the household scenario. At the time of this technical report, 3D-FUTURE contains 20,240 clean and realistic synthetic images of 5,000 different rooms. There are 9,992 unique detailed 3D instances of furniture with high-resolution textures. Experienced designers developed the room scenes, and the 3D CAD shapes in the scene are used for industrial production. Given the well-organized 3D-FUTURE, we provide baseline experiments on several widely studied tasks, such as joint 2D instance segmentation and 3D object pose estimation, image-based 3D shape retrieval, 3D object reconstruction from a single image, and texture recovery for 3D shapes, to facilitate related future researches on our database.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2009.09633", "mag": "3087067522", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ijcv/FuJGGZMT21", "doi": "10.1007/s11263-021-01534-z"}}, "content": {"source": {"pdf_hash": "bf68b7d2566a2bb4f3af582ea9c9e4a15ee20c1b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.09633v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://link.springer.com/content/pdf/10.1007/s11263-021-01534-z.pdf", "status": "BRONZE"}}, "grobid": {"id": "cbb1545c289b19b11fc0d749ffc3a41d3e711ffe", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bf68b7d2566a2bb4f3af582ea9c9e4a15ee20c1b.txt", "contents": "\n3D-FUTURE: 3D Furniture shape with TextURE\n\n\nHuan Fu \n\u00b7 Rongfei \nJia \u00b7 Lin \nGao \nMingming Gong mingming.gong@unimelb.edu.au \nBinqiang Zhao binqiang.zhao@alibaba-inc.com \nSteve Maybank sjmaybank@dcs.bbk.ac.uk \nDacheng Tao dacheng.tao@sydney.edu.au \nHuan Fu \nRongfei Jia rongfei.jrf@alibaba-inc.com \nLin Gao \nMingming Gong \nBinqiang Zhao \nSteve Maybank \nDacheng Tao \n\nTaoXi Technology Department\nTaoXi Technology Department, Alibaba Group\nInstitute of Computing Technology\nTaoXi Technology Department, Alibaba Group\nDepartment of Computer Science and Information Systems, Birkbeck College\nAlibaba Group\nChinese Academy of Sciences\nThe University of Melbourne\nVIC\nUniversity of London\nCN, CN, CN, AU, CNUK\n\n\nUBTECH Sydney AI Centre\nThe University of Sydney\nNSW\nAU\n\n3D-FUTURE: 3D Furniture shape with TextURE\nReceived: date / Accepted: dateNoname manuscript No. (will be inserted by the editor)3D-FUTURE \u00b7 Furniture Shapes \u00b7 Textures \u00b7 Interior Designs \u00b7 Synthetic Images\nThe 3D CAD shapes in current 3D benchmarks are mostly collected from online model repositories. Thus, they typically have insufficient geometric details and less informative textures, making them less attractive for comprehensive and subtle research in areas such as high-quality 3D mesh and texture recovery. This paper presents 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated and large-scale repository of 3D furniture shapes in the household scenario. At the time of this technical report, 3D-FUTURE contains 20,240 clean and realistic synthetic images of 5,000 different rooms. There are 9,992 unique detailed 3D instances of furniture with high-resolution textures. Experienced designers developed the room scenes, and the 3D CAD shapes in the scene are used for industrial production. Given the well-organized 3D-FUTURE, we provide baseline experiments on several widely studied tasks, such as joint 2D instance segmentation and 3D object pose estimation, image-based 3D shape retrieval, 3D object reconstruction from a single image, and texture recovery for 3D shapes, to facilitate related future researches on our database.\n\nIntroduction\n\nThe rapid progress of modern machine learning methods, such as deep neural models, has led to various impressive breakthroughs towards 2D computer vision and natural language processing (NLP). One key to facilitating the advancement of these approaches is the availability of large-scale labeled benchmarks. Mirroring this pattern, the computer graphics and 3D vision communities have put tremendous efforts in establishing 3D datasets over the past years, expecting to enable and innovate the avenues of future research (Chang et al., 2015;Wu et al., 2015;Xiao et al., 2013;Song et al., 2015;Xiao et al., 2016;Sun et al., 2018a;Xiang et al., 2014Xiang et al., , 2016Silberman et al., 2012;Dai et al., 2017;Hua et al., 2016). For example, the largest 3D repositories, like ShapeNet (Chang et al., 2015) and ModelNet (Wu et al., 2015), collected massive 3D shapes from online repositories and organized them under the WordNet taxonomy. Relying on the repositories, several works, such as Pascal 3D+ (Xiang et al., 2014), ObjectNet3D (Xiang et al., 2016), Pix3D (Sun et al., 2018a), and Stanford Cars (Krause et al., 2013), further provided images and shapes associations or alignments with fine-grained pose annotations. Other works like NYU Depth Dataset (Silberman et al., 2012), SUN RGB-D , ScanNet , SceneNN (Hua et al., 2016), and Matterport3D  introduced RGB-D scans of real-world indoor environments with many estimated and manually verified annotations. Considering that there are rich 3D benchmarks, why do we need one more?\n\nIn contrast to the 2D counterparts (Krizhevsky et al., 2012;Lin et al., 2014;Geiger et al., 2012), we realize that there is still a big gap between 3D academic research and industrial productions. For instance, the 3D CAD models in existing datasets mainly come from public online repositories like Trimble 3D Warehouse 1 and Yobi3D 2 . These 3D shapes typically have fewer geometry details and uninformative textures or even no textures. Specific to shapes in the household scenario, most of them are outdated and dull furniture deprecated by modern professional designers. Therefore, the current 3D shapes are inadequate for comprehensive and subtle research in areas such as industry closely related fine-grained 3D shape understanding and texture recovery. Besides, existing benchmarks only provide pseudo image or shape alignments, and the estimated camera pose annotations. Namely, the benchmark designers manually choose a roughly matched 3D CAD model from available 3D shape benchmarks according to the object in the image. Thus, annotators may largely ignore some local shape details, which prevents the progress of fundamental data-driven studies such as high-quality 3D reconstruction from real-world images and high-accuracy image-based 3D shape retrieval. Last but not least, there is no well-organized benchmark that offers realistic synthetic indoor images with both instance-level semantic annotations and the involved 3D shapes with textures.\n\nMotivated by the observations, we present 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated, large-scale repository of 3D furniture shapes specific to the household scenario as shown in Figure 1. At this time, 3D-FUTURE provides 20,240 realistic indoor images and the associated 9,992 unique 3D furniture models with rich geometry details and informative textures. We render these images via one of the most advanced industrial 3D rendering engines based on 5,000 exquisite room scenes developed by experienced designers. The 3D furniture shapes are used for modern industrial productions and have fine-grained geometry and texture related attributes such as category, style, theme, and material. Further, 3D-FUTURE offers instance segmentation annotation and the rendering information, including six degrees of freedom (6DoF) pose and camera field of view (FoV). Apart from these highlight features, another compelling part of 3D-FUTURE is that it enables many fundamental studies and new research opportunities such as furnishing composition, texture recovery, and other interior understanding subjects.\n\nIt is, however, nontrivial to collect thousands of aesthetic interior designs. To the best of our knowledge, it takes a designer several days to complete a house's interior design. Thus, we considered two main research questions when establishing 3D-FUTURE: 1) can we develop a framework that allows creators to design delicate rooms efficiently? 2) can we automatically create some aesthetic designs based on the professional layout information? To investigate the former question, we build a furnishing suit composition (FSC) platform 3 . The system recurrently recommends visually matched furniture by considering instance aesthetics and compatibility during the design progress. For the latter one, we reuse the expert layouts, generate multiple furnishing suit candidates with some rules and the FSC approach, render the scene, and manually select visually appealing ones. These AI-created designs will also be reviewed by designers to ensure good quality.\n\nThe remainder of this paper is organized as follows. First, we briefly review the public 3D benchmarks and discuss their imperfections. Second, we present the data acquisition process and the FSC pipeline. Third, we introduce the properties and statistics of 3D-FUTURE. Finally, we conduct various experiments leveraging on the properties. These experiments can serve as baselines for subsequent research on 3D-FUTURE.\n\n\nRelated Work\n\nLots of 3D benchmarks have been established and made publicly available over the past decades (Chang et al., 2015;Wu et al., 2015;Xiao et al., 2013;Song et al., 2015;Xiao et al., 2016;Sun et al., 2018a;Xiang et al., 2014Xiang et al., , 2016Silberman et al., 2012;Dai et al., 2017;Hua et al., 2016;Choi et al., 2016;Shilane et al., 2004). These datasets can be mainly divided into two groups, including 3D models and RGB-D scenes. We will briefly review some representative 3D benchmarks in the following.\n\n\nBenchmarks\n\nShapes Texture Categories Shape Source Scene Images Instances Alignments PrincetonSB (Shilane et al., 2004) 6,670 \u00d7 161 Online \u00d7 \u00d7 \u00d7 ShapeNetCore (Chang et al., 2015) 51,300 \u221a * 55 Online \u00d7 \u00d7 \u00d7 ShapeNetSem (Chang et al., 2015) 12,000 \u221a * 270 Online \u00d7 \u00d7 \u00d7 ModelNet (Wu et al., 2015) 151,128 \u00d7 660 Online \u00d7 \u00d7 \u00d7 ObjectScans (Choi et al., 2016) \u223c1,900 \u00d7 44 Scans \u00d7 \u00d7 \u00d7 IKEA (Lim et al., 2013) 219 \u00d7 11 Industry 759 -pseudo PASCAL3D+ (Xiang et al., 2014) 79 \u00d7 12 ShapeNet \u00d7 30,899 raw ObjectNet3D+ (Xiang et al., 2016) (Sun et al., 2018a) 395 \u221a * 5 ShapeNet \u00d7 10,069 pseudo Standford Cars (Krause et al., 2013) 134 \u221a * 1 ShapeNet \u00d7 16,185 pseudo Comp Cars (Yang et al., 2015) 98 \u221a * 1 ShapeNet \u00d7 5,696 pseudo ScanNet  296 \u221a * 17 ShapeNet 1513 scans \u223c9,600 pseudo InteriorNet (Li et al., 2018) (Zheng et al., 2019)  only part of shapes comes with textures. \u223c: about. \u2020: synthetic images. \"Raw\" and \"pseudo\" mean that the 3D shapes are usually not the exact the ones corresponding to the 2D objects. Note that, our 3D-FUTURE is specific to household scenario, and all the 3D shapes are industrial used furniture shapes. See Figure 4, Figure 5, and Figure 7 for more details of our highlight features.\nN/A \u00d7 N/A N/A 20M \u2020 \u00d7 \u00d7 Structured3DN/A \u00d7 N/A N/A 20M \u2020 \u00d7 \u00d7 3D-FUTURE (\n\n3D Models\n\nOne of the large and exhaustively studied 3D shape repositories is ShapeNet (Chang et al., 2015). It collected millions of raw 3D CAD models from public online repositories such as Warehouse3D and Yobi3D. By re-organizing the datasets, the subsets ShapeNetCore and ShapeNetSem have been made available, including 51,300 and 12,000 models. ShapeNet assigned rich semantic annotations to part of the shapes, such as synsets in the WordNet taxonomy, functional patterns, parts, keypoints, and categories. 3D shape repositories like ModelNet (Wu et al., 2015) and Princeton Shape Benchmark (Shilane et al., 2004) also share similar content as ShapeNet. Several other works like (Choi et al., 2016) and ScanObjectNN (Uy et al., 2019) create the datasets of 3D scans of real objects based on state of the art (SOTA) RGB-D reconstruction approaches. These benchmarks have largely driven the fundamental 3D studies, including 3D representation, 3D shape recognition, 3D object reconstruction, and part segmentation. However, since the 3D shapes are collected online, many may lack geometry details and have dreamlike or no textures.\n\nRelying on these large-scale 3D shape databases, the community also builds benchmarks with image and shape associations to facilitate the research of 3D object understanding from images. For example, PASCAL3D+ (Xiang et al., 2014) and ObjectNet3D (Xiang et al., 2016) aligned objects in the 2D images with the 3D shapes and provided raw 3D pose annotation. Further, Pix3D (Sun et al., 2018a) contributed more accurate 2D-3D alignment for 395 3D shapes of nine object categories. Unluckily, these pseudo alignments may largely ignore some local shape details. Moreover, the expensive labor efforts make it difficult to build a large-scale benchmark with precise pixel-level 2D-3D alignment.\n\n\nRGB-D Scenes\n\nIn recent years, the community has put significant efforts into building RGB-D datasets to expand researches on 3D scene understanding. For example, NYU Depth V2 (Silberman et al., 2012) captured 464 short Kinect RGB-D sequences from 464 different indoor scenes, where 1,449 images are with dense per-pixel labeling, including depth, surface normal, and semantic labels. SUN RGB-D  followed the pattern by annotating 10,335 RGB-D frames, and offered 3D bounding boxes. To capture the full 3D extent of indoor environments, SUN3D (Xiao et al., 2013) obtained 415 long sequences in 254 unique spaces with comprehensive views. Further, Dai et al. established ScanNet (Dai et al., 2017), an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with estimated 3D camera poses, surface reconstructions, semantic segmentation, and a broad set of CAD model alignments. Later, a more extensive dataset Matterport3D  was made publicly available, contributing to panoramic HDR color images with 3D scene annotations. Different from these RGB-D real-world databases, we focus on experienced exquisite interior designs used in industrial productions. The works most closely related to ours are InteriorNet (Li et al., 2018) and Structured3D (Zheng et al., 2019), which also offer photorealistic images by rendering professional house designs. However, there are two significant differences. First of all, we provide furniture shapes with textures in the scenes. The 6DoF pose and camera FoV are shared in 3D-FUTURE. Second, 3D-FUTURE additionally expects to foster studies of exquisite interior design understanding. Thus, for each room, the camera viewpoints are suggested by designers, so that the captured images contain the whole design idea.\n\n\nData Acquisition Process\n\nIn this section, we introduce the pipeline of our dataset construction procedure. We mainly address the two issues, i.e., designing efficiency and aesthetic design creation, as stated in Sec. 1.\n\n\nLarge-scale Interior Database\n\nWe construct a 3D pool containing a large amount of industrial 3D computer-aided design (CAD) furnishing and interior finish models. We associate each shape  (Devlin et al., 2018). We construct two tasks here, including mask prediction and compatibility scoring, as explained in Sec. 3.2.1. There is only one visual embedding network (VEN) which is shared in both the two tasks. The deep visual embedding (\"orange\") for a specific item is captured by the trained VEN.\n\nwith multiple textures and materials, resulting in enlarged shape repositories. The models are richly annotated with diverse attributes, including theme color, style, material, brand, real-world size, and category in the WordNet taxonomy. There are 500 fine-grained categories in five levels of the taxonomy. High-resolution 2D rendering for each textured model is also available in the database. Based on these objects, hundreds of experienced designers have created \u223c60,000 decorative houses for different scenarios in several years, where \u223c30,000 homes have been evaluated as excellent or brilliant designs. A design sample is shown in Figure 1. The large-scale interior data is offered by Alibaba Topping Homestyler 4 . We set up a project based on the large-scale interior data to build 3D-FUTURE.\n\n\nFurnishing Suit Composition (FSC)\n\nOne of the main challenges in establishing 3D-FUTURE is how to collect many exquisite interior designs in an acceptable project cycle. To address this issue, we develop a high-performing furnishing suit composition framework. We mainly borrow the concepts of attribute-based interpretable compatibility methods (Yang et al., 2019;Wang et al., 2018b;Chen et al., 2019a) in fashion outfit compatibility learning. An overview of this framework is presented in Figure 2.\n\nOur training set for FSC has the form as\n{X 1 , X 2 , ..., X N }, and X i = {x i 1 , x i 2 , ..., x i mi }.\nHere, N is the total number of experienced house designs. m i is the number of items in house X i , and x i j is a specific furnishing item contained in house X i . Note that the elements in X i are in order, which means x i j is a former furnishing item selected by designers followed by x i j+1 .\n\n\nDeep Visual Embedding\n\nAs aforementioned, we have rich attribute annotations for each furnishing item. These interpretable attributes show significance in understanding the item's content and are thus beneficial to FSC learning. However, we can not expect a limited number of attributes to represent an item comprehensively. Therefore, we propose a Deep Furnishing Suit Model (DFSM), which consists of a visual embedding network (VEN ) and two transformer encoders (TransEnc1 and TransEnc2 ), to learn representative deep visual embedding leveraging on the large-scale excellent house designs. DFSM is driven by a margin ranking loss with hard sample mining and a variant of classification loss.\n\nIn specific, given a furnishing suit X i , we randomly capture a subset\nX i j\u223ck = {x i j , x i j+1 , ..., x i k }, where 1 \u2264 j \u2264 k < m i . Our goal is to predict x i k+1 given X i j\u223ck .\nAccording to the category label of X i j\u223ck , we randomly choose three negative examples from the furnishing pool to construct a candidate set C = {x i k+1 , z 0\nx i k+1 , z 1 x i k+1 , z 2 x i k+1\n} in an online manner. We also ensure that the negative examples z 0 / z 1 / z 2 have the same style / color / material as x i k+1 . We feed both X i j\u223ck and the candidate images into VEN to extract visual features. In our paper, we take the CNN part of MobileNetV2 followed by a projection layer as VEN, and pre-train it via the unsupervised learning strategy stated in (Wu et al., 2018).\n\nAfter obtaining the image features, we construct two tasks, i.e., mask prediction and compatibility scoring, based on the impressive transformer architecture in NLP (Devlin et al., 2018;Vaswani et al., 2017). For the former one, we have a sequence of feature vectors\nF i ={VEN (X i j\u223ck ), [Mask ]} with dimension d, where [Mask ]\ndenotes a particular mask embedding. The task is to predict the masked item given the previous ones. We thus feed F into TransEnc1 to capture the enhanced feature F i , and optimize the model via the following loss:\nL mp = \u2212 1 N N i=1 log(P(x i k+1 | F i ; \u0398, \u03a6)),(1)P(x i k+1 | F i ; \u0398, \u03a6)) = exp(f i mask f T x i k+1 ) c exp(f i mask f T c ) ,(2)\nwhere \u0398 and \u03a6 are the learnable parameters of VEN and TransEnc1, respectively; c \u2208 C is a candidate; f T x is the transpose of f x ; f x denotes the visual embedding of item x, i.e., f x = VEN (x); andf i mask \u2208 F i represents the feature vector of the [Mask] token from TransEnc1.\n\nFor the second task, we take the candidate suits as inputs and directly learn their compatibility scores.\nLet F (X i j\u223ck ,c) = {[Start], VEN (X i j\u223ck ), f c } be the visual feature vectors of a candidate suit O (X i j\u223ck ,c) , where [Start]\nis a particular start token embedding. To estimate the compatibility score of a suit, we need to first capture an embedding that can represent it. We thus employ TransEnc2 to acquire F (X i j\u223ck ,c) , and use the feature vector of the [Start] token as the representation of suit O (X i j\u223ck ,c) (denoted as r (X i j\u223ck ,c) ). Further, we utilize two fully connected layers and a sigmoid function to secure a score (s (X i j\u223ck ,c) ), which is the measure of the quality of the suit. For conventional presentation, s (X i j\u223ck ,c) is abbreviated as s(x i k+1 ) hereafter. Since the ground truth compatibility scores are not available, we minimize a margin ranking loss with a simple hard sample mining policy. The objective is expressed as:  where \u03b1 is set to 0.1 in our experiments. The trained visual embedding network (VEN) is used to extract the visual feature for each furniture item.\nL cs = \u2212 1 N N i=1 max(0, \u2212s(x i k+1 ) + s(z x i k+1 ) + \u03b1), (3) s(z x i k+1 ) = max(s(z 0 x i k+1 ), s(z 1 x i k+1 ), s(z 2 x i k+1 )),(4)\n\nDecision Tree Based FSC\n\nThe main goal here is to infer attribute-based matching patterns, i.e., attribute crosses, for FSC. Considering both interpretability and scalability, we utilize GBDT (Friedman, 2001) to automatically construct attribute crosses as shown in Figure 6. We will not introduce the details of GBDT here, but only present some facts in training the decision trees.\n\nWe employ six attributes to represent a specific item, including theme color, style, material, real-world size, the second-level category, and visual information (the learned visual embedding). Here, we denote the learned visual embedding as an attribute. For the discrete attributes (style, material, and the second-level category), we directly convert them to one-hot vectors. For theme color and real-world size, we first adopt k-Means Clustering (Kanungo et al., 2002) to discretize real values and then transform them into one-hot vectors. By further considering the visual embedding, we can represent each item as a feature vector. We assign a label (positive or negative) to each specific furnishing suit to train the decision trees. Both the positive and negative suits are constructed similarly in Sec. 3.2.1.\n\n\nRe-training via Hard Sample Mining\n\nThe negative furnishing suits construction strategy may return some naive negative samples, due to the large-scale furnishing pool, causing some inaccuracies in both the deep embedding networks and the decision trees. We fine-tune the visual embedding network and re-train the decision tree model via a straightforward hard sample mining strategy to address the issue. Specifically, given X i j\u223ck , we can have the TopK recommendations using the trained DFSM. We then randomly select negative samples from the TopK pool. After the re-training stage, we fix VEN's parameters and establish an automatically re-training system to update the decision tree model daily using continuously enlarged online designs.\n\n\nTopping Homestyler Design Platform\n\nOur DFSM is integrated into the online Topping Homestyler Design Platform 5 to improve the house design efficiency. There are also other highlight features that can facilitate the design procedure, such as the large-scale shape pool, image-based furniture retrieval, 2D display, 3D Roaming, various professional design templates, texture and item replacement, and online rendering.\n\n\nCreate Aesthetic Interior Design\n\nWe have collected 5,000 exquisite interior room designs in the 3D-FUTURE project. We do not plan to provide several synthetic images in different viewpoints for each Fig. 8 The statistics of the attribute annotations of the 9,992 shapes in 3D-FUTURE. Furniture shapes with the attributes such as \"Modern\", \"Japanese\", \"Smooth Net\", \"Texture Mark\", \"Rough Cloth\", and \"Wood\" may be more welcomed by designers when designing the rooms. Besides, except for some special cases, each attribute category has at least 90 shapes.\n\nroom. Instead, we expect to deliver more superlative designs to bring more research possibilities. Thus, we take these experienced designs as templates and create several aesthetic interior designs for each template.\n\nFor example, given a template room with professional design information, we first replace the interior finishing according to the materials, room style, and other descriptions. Second, we choose a furniture seed (e.g., bed) based on the interior finishing information. Third, we iteratively perform recommendations based on our DFSM and other rules to generate a furnishing list. Finally, we put the items contained in the furnishing list into their corresponding positions. In the third step, we also learn one-to-one visual compatibility models (e.g, bed-nightstand and sofa-coffee table) as additional rules to improve the robustness.\n\nWith the pipeline, we can automatically create many interior designs as shown in Figure 3. To ensure the quality, we render an image for each design and manually select 15,240 visually appealing designs. Our experienced designers further review these designs to assure good quality.\n\nThe 3D designs are rendered by one of the most advanced computer-generated imagery rendering software applications, V-Ray 6 . To ensure reality, we enable as many functions as possible supported by V-Ray.\n\n\nProperties of 3D-FUTURE\n\nIn this section, we summarize the properties of our 3D-FUTURE database. Compared to previous 3D benchmarks, 3D-FUTURE has some prominent properties that can bring more possibilities for future 3D research.\n\n\nPhoto-realistic Synthetic Images\n\n3D-FUTURE offers 20,240 photo-realistic synthetic images corresponding to 20,240 interior designs. As  aforementioned, we have 5,000 experienced designs and 15,240 automatically created aesthetic designs. We render one image for each design. Previous datasets, such as Structured3D (Zheng et al., 2019) and InteriorNet (Li et al., 2018), also provide realistic indoor images and scene parsing annotations. However, they put cameras in random positions and capture redundant images for each house. These images were not manually verified, thus suffer from unexpected viewpoints.\n\nIn contrast, 3D-FUTURE focuses more on inspiring the understanding of exquisite interior designs. Thus, the camera positions are suggested by professional designers to obtain the best viewpoint for each room. Besides, 3D-FUTURE provides instance semantic labels of 34 categories and ten supper-categories. Moreover, the images contained in 3D-FUTURE are visually more appealing and realistic compared to previous ones. Fig. 11 The confusion matrices obtained by MVCNN  and PointNet++ (Qi et al., 2017b) for 3D Object Recognition on 3D-FUTURE. Fig. 12 Histograms of the instance segmentation AP and rotation estimation AOS of the 34 categories on the test set. The closer AOS is to AP, the better the rotation estimation.\n\n\n2D-3D Alignments\n\nPrevious benchmarks only provide pseudo 2D-3D alignment annotations (Xiang et al., 2016;Sun et al., 2018a;Dai et al., 2017;Krause et al., 2013;Xiang et al., 2014). Namely, they manually choose a roughly matched 3D CAD model from public 3D shape benchmarks according to the object contained in an image. Annotators thus may largely ignore some local shape details. As a result, these benchmarks offer a small number of matched 3D shape and 2D image pairs. Besides, previous benchmarks with alignment annotations do not come with scene images. In contrast, 3D-FUTURE provides precious 2D-3D alignments and 3D pose annotations. It contains 9,992 unique 3D shapes and 20,240 scene images. By cropping instances from the scene images, we can further secure 37,441 image and shape pairs with slight occlusions, as reported in Table 3. Some samples are presented in Figure 4.  \n\n\nHigh-quality Shapes with Informative Textures\n\nThe 3D shapes contained in previous large-scale shape repositories (Chang et al., 2015;Shilane et al., 2004;Wu et al., 2015) are mainly collected from online Fig. 14 An illustration of the baseline method of cross-domain image-based 3D shape retrieval. We use instance-level non-parametric softmax loss (Wu et al., 2018) so that the network can capture shape similarity among furniture instances.  Table 3 The train and test sets for the subject of cross-domain image-based 3D shape retrieval. Object labeled as \"NO\" means the occluded ratio for the object is less than 10%. In our setting, the final retrieval pool consists of the CAD models from both the test set and the train set.\n\nrepositories. These 3D CAD models usually contain few geometry details and low informative textures. Luckily, 3D-FUTURE provides high-quality 3D furniture shapes with rich details in various styles, including European furniture, which often contains intricate carvings. All the shapes come with informative textures and have been used for modern industrial productions. We show some samples in Figure 5. We believe these features can potentially facilitate innovative research on high-quality 3D shape understanding and generation.\n\nIn Figure 10, we compare the proportion of different number of vertices and faces over ShapeNetCore (Chang et al., 2015), ModelNet40 (Wu et al., 2015) and our dataset. While other datasets have some extremely low-resolution shapes, 3D shapes in 3D-FUTURE show uniform distributions in both vertices and faces.\n\n\nFine-Grained Attributes\n\nPrevious 3D benchmark provides functional attribute annotations in WordNet taxonomy for 3D shapes (Chang et al., 2015). However, these attributes are not well organized and do not have corresponding textures. In contrast, for each textured shape in 3D-FUTURE, we provide four types of attributes verified by professional designers. We have 34 shape categories, 8 super-categories, 19 styles, 15 materials, and 16 themes. These attributes have been demonstrated valuable for interior designs and content understanding by industrial productions. We present the statistics of these attributes in Figure 9 and Figure 8. These figures imply the preferences of experienced modern designers when designing the rooms.\n\n\nBaseline Experiments\n\nIn the section, we conduct several baseline experiments by leveraging the properties of 3D-FUTURE, including shape recognition, joint 2D instance segmentation and 3D pose estimation, image-based shape retrieval, 3D object reconstruction, and texture synthesis. We split our 3D shapes into a training set with 6,699 models, and a test set with 3,293 models. The scene images are divided according to the training and test splits of 3D shapes. There are 14,761 images for training and 5,479 images for test. We will briefly present the experimental details for each task and report the scores.\n\n\nFine-grained 3D Object Recognition\n\nOver the past years, most 3D object recognition methods extend deep convolutional neural networks (DCNNs) to modeling 3D data. Because 3D CNNs are too memory intensive (Ji et al., 2012), some researchers prefer to either develop special deep learning operations on point clouds and mesh surfaces (Qi et al., 2017a,b;Hanocka et al., 2019;Feng et al., 2019), or project 3D shapes to several 2D images and then apply 2D convolutional networks .  However, it is nontrivial to extend the projection-based methods to high-resolution 3D scene understanding.\n\nMoreover, point and mesh-based approaches suffer from computation bottlenecks and are thus limited to sparse point clouds and a small number of surfaces. In contrast to ShapeNet (Chang et al., 2015) and ModelNet (Wu et al., 2015), 3D-FUTURE enables the study of fine-grained 3D furniture recognition, which requires the networks to capture more local and global geometric details. Here we consider the well-known MVCNN  and PointNet++ (Qi et al., 2017b) as the baselines. In specific, we train a 12-view MVCNN with ResNet50 as the backbone. For PointNet++, we sample 1024 points for each shape instance and adopt the multi-scale grouping (MSG) strategy (Qi et al., 2017b) and normal vectors to secure the best performance. We train the networks using 6,699 shapes and evaluate the trained models via the remaining 3293 shapes. The classification accuracy for each category is presented in Table 2 and Figure 11. While these methods can reach 90% accuracy on ModelNet40 and ShapenetCore, they do not perform well (69.2% \u223c 69.9%) on 3D-FUTURE, due to the presence of fine-grained furniture categories. This observation would motivate researchers to exploit more efficient 3D representation learning approaches for deeper 3D shape analysis.\n\n\nImage-based 3D Shape Retrieval\n\nCross-domain image-based 3D shape retrieval (IBSR) is to identify the CAD models of the objects contained in query images. The primary issue in IBSR is the large appearance gaps between 3D shapes and 2D images. To tackle this challenge, early works made efforts to map cross-domain representations into a  Table 5 Quantitative results of the Cascade-Mask R-CNN baseline for joint instance segmentation and 3D pose estimation. AOS and AVP: Higher is better. RMSE: Lower is better.\n\nunified constrained embedding space via adaptation techniques such as weight-sharing constraints, metric learning, and distance matching Aubry et al., 2014;Lee et al., 2018;Massa et al., 2016;Tasse and Dodgson, 2016;Girdhar et al., 2016). Recent works (Sun et al., 2018a;Huang et al., 2018;Wu et al., 2017;Bansal et al., 2016;Bachman, 1978;Grabner et al., 2018Grabner et al., , 2019 predict 2.5D sketches from images, such as surface normal, depth, and location field, to bridge the gaps between 3D and 2D domains. However, the performance of state-of-the-art IBSR methods show a large gap than its 2D counterpart, i.e., content-based image retrieval. This is because there are no large-scale benchmarks that offer large amounts of precious 2D-3D alignment annotations.\n\nIn this experiment, we train the baseline using 31,444 image-shape pairs and evaluate the retrieval algorithm via the other 5,994 image-shape pairs. Then we crop the furniture instances with occlusion levels of \"NO\", \"Slight\" and \"Standard\" from the scene images to produce the image-shape pairs. The statistics of the train and test sets are presented in Table. 3. We develop a DCNN based metric learning network to study the cross-domain shape similarities, as shown in 14. Specifically, we first project the selected 3D shapes  Table 6 Numerical comparison of our several baselines for single image 3D reconstruction on our 3D-FUTURE dataset.\n\nMetrics are IoU (%), CD (\u00d710 \u22123 , computed on 2,048 points) and F-score (thresholds is 1%, the reconstruction volume side length defined in (Tatarchenko et al., 2019)). IoU and F-score: Higher is better. CD: Lower is better.\n\ninto 2D planes using the toolbox 7 to bridge the 3D and 2D gaps. Given a query image and its corresponding 3D shape, we randomly sample a negative 3D shape from the 3D pool to construct a triplet. We then feed the triples (2D images) into a ResNet-34 feature extractor and adopt a margin ranking loss to push the query image close to its corresponding 3D shape.We utilize a category classification loss and an instance classification loss (Wu et al., 2018) such that the network can capture shape similarity among furniture instances. We take TopK Recall (TopK@R) and Top5 average F-score (mean F-score) as our metrics. The latter is used to measure the retrieval sequences. The retrieval results for each category are reports in Table. 4. We also show some qualitative retrieval sequences in Figure 15. We can see that while the captured Top1@R for a large portion of categories is less than 30.0%, the retrieval 7 https://github.com/3D-FRONT-FUTURE sequences seem to be visually acceptable. Besides, there is a remarkable gap between Top1@R (23.4%) and Top3@R (40.6%). The observations demonstrate that our large 3D pool contains many furniture with similar shape characteristics, which would provide potential opportunities for fine-grained shape retrieval studies.\n\n\nJointly 2D Instance Segmentation and 3D Pose Estimation\n\nImage-based 6DoF pose estimation is a fundamental 3D vision task that can benefit many intelligent applications such as autonomous driving, augmented reality, and robotic manipulation. Typical methods 6DoF pose estimation first build point-wise correspondences between 3D models and 2D images, followed by the Perspective-n-Point (PnP) algorithm to compute pose parameters (Collet et al., 2011;Fig. 18 Sample reconstruction results on our 3D-FUTURE benchmark. The SOTA methods cannot model the local geometric details. Rothganger et al., 2006). These approaches perform well for objects with rich textures but are not robust to featureless or occluded cases. Recent works thus employ RGB-D sensors and deep learning to improve keypoints detection or directly predict 6DoF pose from images (Kehl et al., 2016;Brachmann et al., 2014;Bo et al., 2014;Hinterstoisser et al., 2012;Xiang et al., 2017;Peng et al., 2019;Song et al., 2020;Tekin et al., 2018;Rad and Lepetit, 2017;Park et al., 2020). Nevertheless, the main issues such as occlusion and clutter, scalability to multiple objects, and symmetries have not been well addressed. Instance segmentation is the task of detecting and delineating each distinct object of interest appearing in an image. Current instance segmentation methods can be roughly categorized into two paradigms: segmentation-based methods and detection-based methods. The former category of approaches group the predicted category labels via techniques such as clustering (Dhanachandra et al., 2015), metric learning (Fathi et al., 2017), and watershed algorithms (Najman and Schmitt, 1994), to form instance segmentation results. The latter predicts the mask for region instances detected by SOTA object detectors. Methods such as Mask R-CNN series Huang et al., 2019;Cai and Vasconcelos, 2019) have achieved impressive performance for daily objects.\n\nIn this experiment, we learn to predict instance segmentation in 2D images and estimate their 6DoF poses in a unified framework. In contrast to the well-studied benchmarks such as ObjectNet3D (Xiang et al., 2016), PASCAL3D+ (Xiang et al., 2014), and Pix3D (Sun et al., 2018a), 3D-FUTURE encourages estimating pose parameters for multiple objects with occlusions in diverse indoor scenes. We provide 3D pose annotations for 100K+ objects in the scene images. The objects are further divided into five occlusion levels, including \"NO\", \"Slight\", \"Standard\", \"Heavy\", and \"N/A\". Here, an object labeled as \"N/A\" means that its corresponding 3D shape is not available, or a part of the object is out of the camera view. We train our model on the 14,761 training images and test it on the remaining 5,479 test images.  We modify Cascade Mask-RCNN (Cai and Vasconcelos, 2019;He et al., 2017) as our baseline. The network architecture is shown in Figure. 13. Specifically, we take ResNeXt-101 (Xie et al., 2017) with the setting of 64-4d (group number: 64, width of group: 4) as the backbone, and adopt FPN (Lin et al.,Fig. 19 An illustration of our BicycleGAN++ baseline. The input are rendered images from 3D shapes. E: Texture Encoder. G: Generator. 2017) to extract the dense features. Then, we utilize a three-stage cascade architecture to perform bounding box regression and object classification. Finally, we add two branches that consist of several fully connected layers to predict the instance masks and their 6DoF poses simultaneously. We cast rotation estimation as a viewpoint classification problem. In detail, we convert the rotation matrices to Euler angles and divide the 360-degree azimuth, 180-degree elevation, and 360-degree in-plane rotation into 18 bins, 9 bins, and 18 bins, respectively. For translation estimation, we use L1 smooth loss to regress the translation parameters directly.\n\nFor 2D instance segmentation, we report Average Precision (AP) and Average Recall (AR) over different IoU thresholds following . For 3D pose estimation, we take both Average Viewpoint Precision (AVP) in PASCAL3D+ (Xiang et al., 2014) and Average Orientation Similarity (AOS) in KITTI (Geiger et al., 2012) to measure the rotation predictions as (Xiang et al., 2016), and employ Root Mean Square Error (RMSE) to evaluate the translation predictions. In specific, we define the difference between an estimated rotation matrix R and its ground truth R gt as \u2207(R, R gt ) = 1 \u221a 2 log(R T R gt ) F . In AVP, a correct estimation should satisfy \u2207(R, R gt ) < \u03c0 6 . The cosine similarity between rotations in AOS is computed as cos(\u2207(R, R gt )).\n\nWe present the instance segmentation and pose estimation results in Table 5. Here, the metrics for camera poses are with respect to AP and AR, where the IoU thresholds range from 0.5 to 0.95. For instance segmentation, our baseline captures a mean AP of 0.55 on 3D-FUTURE. The score is at a similar level to those reported on the MSCOCO leaderboard achieved by recent SOTA methods. For 3D pose estimation, our baseline yields a mean AVP of 43%. Besides, as analyzed in (Xiang et al., 2016), AP is an upper bound of AOS. This means the closer AOS is to AP, the more accurate the rotation estimation is. By showing the gaps between AOS and AP in Figure 12, we can see that the estimated rotation (0.43) can be further improved. From the observations, we conclude that most objects' 3D poses are not well modeled in our challenging setting. This suggests that researchers may need to carefully study 3D pose estimation with different levels of occlusions based on 3D-FUTURE. Some qualitative results are shown in Figure 16 and Figure 17 to further justify our conclusions.\n\n\nSingle-View 3D Object Reconstruction\n\nInferring 3D structure from a single image has been an active research area for a long time. In the supervised setting, traditional methods investigated shape from shading (Durou et al., 2008;Zhang et al., 1999) and defoce (Favaro and Soatto, 2005) to reason the visible parts of objects. Leveraging on large-scale shape repositories, various works examined deep architectures to produce shapes in 3D volume , point cloud (Fan et al., 2017), and mesh surface (Groueix et al., 2018) directly. Recently, several SOTA methods recovered 3D meshes from initializations using shape deformation based on deep networks (Wang et al., 2018a). In the unsupervised setting, 3D recovery has been recast as a 2D image reconstruction progress of unobserved views with differentiable rendering Chen et al., 2019b).\n\nIn this paper, we examine several SOTA reconstruction algorithms as the baselines, including  ONet , Pixel2Mesh (Wang et al., 2018a), and DISN (Xu et al., 2019). We report the widely studied Intersection over Union (IoU), Chamfer Distance (CD), and F-score to evaluate these approaches on 3D-FUTURE. We refer (Xu et al., 2019) for the definitions of these metrics. We randomly render 24 different view images each model for training and a random view image for testing. The resolution of each image is 256 \u00d7 256. As shown in Table 6 and Figure 18, Pixel2Mesh is more robust in general 3D object reconstruction. However, all the SOTA methods cannot recover good-quality shapes when the 3D shapes contain many geometric details.\n\n\nTexture Synthesis For 3D Shapes\n\nUnlike geometry reconstruction, texture reconstruction of 3D objects has received less attention from the community. Previous works studied the subject by learning colored 3D reconstruction on voxels or point clouds (Sun et al., 2018b;Tulsiani et al., 2017) based on view synthesis and multi-view geometry. While voxel representations are limited to the low resolutions, point representations are sparse and thus ignore geometric details. Recent approaches alternatively learned a 2D texture atlas (UV mapping) for 3D meshes to map a point on the shape manifold to a pixel in the texture atlas. These methods mainly take advantage of differentiable rendering to recast the problem as an unobserved view synthesis problem (Raj et al., 2019;Oechsle et al., 2019).\n\nExisting 3D repositories contain less dreamlike or uninformative textures and cannot support high-quality texture recovery studies. In contrast, 3D-FUTURE provides furniture shapes with informative textures, which are widely used in industrial productions. We examine two baselines for texture synthesis, i.e., Texture Fileds  and a novel BicycleGAN++ method. Here, BicycleGAN++ extends BicycleGAN (Zhu et al., 2017) for texture synthesis. An illustration of the network is shown in Figure 19. In specific, we incorporate a texture encoder such that the learned model can perform controllable texture synthesis. Importantly, by enlarging the weights of the reconstruction losses and introducing a texture consistency loss, we find that the produced multi-view textured images will show preferable consistency in overlap regions.\n\nWe conduct experiments on four super-categories, including Sofa, Bed, Chair, and Table. The details of our train and test splits are reported in Table 7. We randomly render 32 views of images for each shape to enlarge the training set. For each baseline, we first train them on the whole train set and then perform category-specific fine-tuning. Following , we use structure similarity image metric (SSIM) (Wang et al., 2004), L1, Frechet Inception Distance (FID) (Heusel et al., 2017), and Feat1 as our metrics to evaluate the quality of the synthetic texture. Here, L1 is the L1 distance between the ground-truth view rendering and the produced textured image under the same viewpoint. Feat1 is a global perceptual measure operated on the Inception-net (Szegedy et al., 2015) feature space using the L1 distance. As shown in Table 8, while BicycleGAN++ earns higher scores on FID and Feat1, Texture Fields performs better in terms of SSIM and L1, indicating that BicycleGAN++ produces more realistic images with higher quality and Texture Fields focuses more on structured texture details. We also give some qualitative results in Figure 20 and Figure 21. We can see that BicylcGAN++ can only learn the main color information while largely ignores the semantic parts of objects. Texture Fields can partially preserve the structured texture details but produces dreamlike textures. These observations demonstrate that achieving visually appealing texture recovery for 3D meshes is still very challenging, especially for the industrial 3D shapes with informative texture details.\n\n\nConclusion\n\nIn this paper, we have built the large-scale 3D-FUTURE benchmark specific to the household scenario with rich 3D and 2D annotations. 3D-FUTURE contains 20,240 realistic synthetic images and 9,992 high-quality 3D CAD furniture shapes. The exciting features include but are not limited to the exhausting interior designs by experienced designers, photo-realistic renderings, 2D-3D alignments, and most significantly the industrial 3D furniture shapes with informative textures. We conduct several experiments to show the remarkable properties of 3D-FUTURE. The experiments can serve as baselines for future research using our database. We hope that 3D-FUTURE can facilitate innovative research on high-quality 3D shape understanding and generation, bring new research opportunities for 3D vision, and build a bridge between academic study and 3D industrial applications.\n\nFig. 1\n13D-FUTURE. Top: Exquisite interior designs obtained from Alibaba Topping Homestyler design platform. Bottom: An overview of the properties of 3D-FUTURE. All the interior designs are developed or reviewed by experienced designers to ensure their quality. The photo-realistic synthetic scenes are rendered by the advanced rendering engine V-ray. The statistics of 3D-FUTURE are presented in Sec. 4.\n\nFig. 2\n2DFSM. An illustration of the deep furnishing suit model (DFSM) for deep visual embedding in Sec. 3.2.1. The development of the framework borrows the concepts from Bert\n\nFig. 3\n3Realistic Renderings of Aesthetic Interior Designs. Left: experienced design templates. Right: created aesthetic interior designs. These AI generated designs are reviewed by designers. Zoom in for better view.\n\nFig. 4\n42D-3D Alignments. We provide precise 6DoF pose annotations for most of furniture shapes involved in each scene. zoom in for better view.\n\nFig. 5\n5Samples of the high-quality 3D shapes and their informative textures in 3D-FUTURE.\n\nFig. 6\n6An illustration of decision tree based FSC presented in Sec. 3.2.2. Orange: The visual embedding for each item is obtained from the trained DFSM inFigure 2. Blue: The attribute embedding obtained from the attribute labels for each furniture item.\n\nFig. 7\n7Top: samples of photo-realistic synthetic images and their corresponding instance-level annotations from 3D-FUTURE. Bottom: natural images from the widely studied large-scale scene parsing benchmark ADE20K. Zoom in for better view.\n\nFig. 9\n9The shape number of the 34 categories in 3D-FUTURE. These categories are verified and used by experienced designers in their daily works. The figure also implies the frequency of furniture selected by designers to design the room scenes. There are only 7 dressing chairs because designers commonly choose other chairs as the replacements of dressing chairs when designing a room. For example, Classic Chinese Chair and Chaise Longue Sofa only appear in some special designs.\n\nFig. 10\n10The percentile plot of the number of vertices and faces over ShapeNetCore(Chang et al., 2015), ModelNet40(Wu et al., 2015) and 3D-FUTURE. While other datasets have some extremely low-resolution shapes, 3D shapes in 3D-FUTURE show uniformed distributions on both vertices and faces.\n\nFig. 13\n13An illustration of the network for joint instance segmentation and pose estimation. B: region proposals. C: object recognition. H: network head. M: mask prediction. R&T: pose estimation. Seg: the network in the instance segmentation branch. Pose: the network in the pose estimation branch.\n\nFig. 15\n15The retrieval sequences for several query images. 3D-FUTURE contains fine-grained shapes for each furniture category.\n\nFig\nFig. 16 Instance segmentation results. The images are captured under suggested viewpoint (by designer) for design exhibition. Zoom in for better view.\n\nFig. 17\n17The pose estimation results. Zoom in for better view.\n\nFig. 20\n20The multi-view texture synthesis results. Top: Texture Fields. Bottom: Our BicycleGAN++ based on BicycleGAN(Zhu et al., 2017).\n\nFig. 21\n21A quantitative comparison between Texture Fields and BicycleGAN++ for conditional texture synthesis.\n\n\n. 16 Instance segmentation results. The images are captured under suggested viewpoint (by designer) for design exhibition. Zoom in for better view.Category \nTop1@R Top3@R F-score \nChildren Cabinet \n26.9 \n53.4 \n29.7 \nNightstand \n37.4 \n64.1 \n32.6 \nBookcase \n8.7 \n8.7 \n23.9 \nWardrobe \n21.7 \n43.3 \n42.2 \nCoffee Table \n27.7 \n57.1 \n31.3 \nCorner/Side Table \n32.0 \n49.8 \n32.4 \nWine Cabinet \n4.9 \n7.3 \n22.1 \nTV Stand \n16.4 \n27.5 \n37.1 \nDrawer Chest \n15.0 \n31.8 \n44.6 \nShelf \n19.4 \n27.3 \n35.6 \nRound End Table \n20.0 \n21.1 \n26.1 \nDouble/Queen/King Bed \n23.8 \n48.7 \n78.8 \nBunk Bed \n13.0 \n26.1 \n50.0 \nBed Frame \n26.0 \n47.1 \n52.6 \nSingle Bed \n16.5 \n34.7 \n63.2 \nKids Bed \n18.2 \n45.5 \n65.5 \nDining Chair \n16.1 \n38.4 \n50.5 \nLounge/Office Chair \n33.7 \n64.7 \n56.4 \nClassic Chinese Chair \n20.0 \n80.0 \n49.8 \nBarstool \n52.8 \n58.3 \n22.1 \nDressing Table \n53.9 \n65.4 \n31.8 \nDining Table \n13.9 \n21.5 \n26.0 \nDesk \n13.2 \n27.9 \n18.6 \nThree-Seat Sofa \n5.6 \n10.5 \n59.8 \nArmchair \n45.6 \n68.7 \n56.9 \nLoveseat Sofa \n6.7 \n17.5 \n56.1 \nLazy Sofa \n19.4 \n48.4 \n51.7 \nChaise Longue Sofa \n16.7 \n16.7 \n31.4 \nStool \n31.6 \n63.3 \n48.9 \nPendant Lamp \n29.4 \n56.4 \n48.8 \nCeiling Lamp \n31.4 \n59.1 \n37.6 \nmean \n23.4 \n40.6 \n47.1 \n\nTable 4 Numerical retrieval results on 3D-FUTURE for \ncategory level. We train a single model and perform retrieval \nin the full 3D-FUTURE pool. F-score here represents Top5 \naverage F-score. \n\n\n\nTable 7\n7The statistics of the training and test sets for the subject of texture synthesis for 3D shapes.\n\n\nQuantitative Evaluation using the FID, SSIM, L1, and Feat1 metrics. FID, L1, Feat1 : lower is better. SSIM: higher is better.Category \nTexture Field \nBicyleGAN++ \nFID \nSSIM \nL1 \nFeat1 \nFID \nSSIM \nL1 \nFeat1 \nSofa \n22.01 0.959 \n0.013 0.168 10.01 0.951 0.019 0.146 \nBed \n37.22 0.924 \n0.024 0.190 18.06 0.916 0.030 0.172 \nChair \n15.36 0.951 \n0.017 0.131 10.65 0.941 0.022 0.120 \nTable \n29.45 0.964 \n0.011 0.149 21.78 0.958 0.016 0.137 \nmean \n26.01 0.952 0.016 0.160 15.12 0.942 0.022 0.144 \n\nTable 8 \nhttps://3dwarehouse.sketchup.com 2 https://yobi3d.com\nhttps://3d.shejijia.com/\nhttps://www.tangping.com/\nhttp://3d.shejijia.com\nhttps://www.chaosgroup.com/\n\nSeeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. M Aubry, D Maturana, A A Efros, B C Russell, J Sivic, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionAubry M, Maturana D, Efros AA, Russell BC, Sivic J (2014) Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 3762-3769\n\nData processing system utilizing data field descriptors for processing data files. C W Bachman, US Patent. 4300Bachman CW (1978) Data processing system utilizing data field descriptors for processing data files. US Patent 4,068,300\n\nMarr revisited: 2d-3d alignment via surface normal prediction. A Bansal, B Russell, A Gupta, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionBansal A, Russell B, Gupta A (2016) Marr revisited: 2d-3d alignment via surface normal prediction. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 5965-5974\n\nLearning hierarchical sparse features for rgb-(d) object recognition. L Bo, X Ren, D Fox, The International Journal of Robotics Research. 334Bo L, Ren X, Fox D (2014) Learning hierarchical sparse features for rgb-(d) object recognition. The International Journal of Robotics Research 33(4):581-599\n\nLearning 6d object pose estimation using 3d object coordinates. E Brachmann, A Krull, F Michel, S Gumhold, J Shotton, C Rother, European conference on computer vision. SpringerBrachmann E, Krull A, Michel F, Gumhold S, Shotton J, Rother C (2014) Learning 6d object pose estimation using 3d object coordinates. In: European conference on computer vision, Springer, pp 536-551\n\nCascade r-cnn: high quality object detection and instance segmentation. Z Cai, N ; Vasconcelos, A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, arXiv:170906158IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv preprintMatterport3d: Learning from rgb-d data in indoor environmentsCai Z, Vasconcelos N (2019) Cascade r-cnn: high quality object detection and instance segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence Chang A, Dai A, Funkhouser T, Halber M, Niessner M, Savva M, Song S, Zeng A, Zhang Y (2017) Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:170906158\n\nA X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, arXiv:151203012Shapenet: An information-rich 3d model repository. arXiv preprintChang AX, Funkhouser T, Guibas L, Hanrahan P, Huang Q, Li Z, Savarese S, Savva M, Song S, Su H, et al. (2015) Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:151203012\n\nPog: Personalized outfit generation for fashion recommendation at alibaba ifashion. W Chen, P Huang, J Xu, X Guo, C Guo, F Sun, C Li, A Pfadler, H Zhao, B Zhao, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningChen W, Huang P, Xu J, Guo X, Guo C, Sun F, Li C, Pfadler A, Zhao H, Zhao B (2019a) Pog: Personalized outfit generation for fashion recommendation at alibaba ifashion. In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp 2662-2670\n\nLearning to predict 3d objects with an interpolation-based differentiable renderer. W Chen, H Ling, J Gao, E Smith, J Lehtinen, A Jacobson, S Fidler, Advances in Neural Information Processing Systems. Chen W, Ling H, Gao J, Smith E, Lehtinen J, Jacobson A, Fidler S (2019b) Learning to predict 3d objects with an interpolation-based differentiable renderer. In: Advances in Neural Information Processing Systems, pp 9609-9619\n\nS Choi, Q Y Zhou, S Miller, V Koltun, arXiv:160202481A large dataset of object scans. arXiv preprintChoi S, Zhou QY, Miller S, Koltun V (2016) A large dataset of object scans. arXiv preprint arXiv:160202481\n\n3D-R2N2: A unified approach for single and multi-view 3d object reconstruction. C B Choy, D Xu, J Gwak, K Chen, S Savarese, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Choy CB, Xu D, Gwak J, Chen K, Savarese S (2016) 3D-R2N2: A unified approach for single and multi-view 3d object reconstruction. In: Proceedings of the European Conference on Computer Vision (ECCV)\n\nThe moped framework: Object recognition and pose estimation for manipulation. A Collet, M Martinez, S S Srinivasa, The international journal of robotics research. 3010Collet A, Martinez M, Srinivasa SS (2011) The moped framework: Object recognition and pose estimation for manipulation. The international journal of robotics research 30(10):1284-1306\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nie\u00dfner, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDai A, Chang AX, Savva M, Halber M, Funkhouser T, Nie\u00dfner M (2017) Scannet: Richly-annotated 3d reconstructions of indoor scenes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 5828-5839\n\nJ Devlin, M W Chang, K Lee, K Toutanova, arXiv:181004805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:181004805\n\nImage segmentation using k-means clustering algorithm and subtractive clustering algorithm. N Dhanachandra, K Manglem, Y J Chanu, Procedia Computer Science. 54Dhanachandra N, Manglem K, Chanu YJ (2015) Image segmentation using k-means clustering algorithm and subtractive clustering algorithm. Procedia Computer Science 54:764-771\n\nNumerical methods for shape-from-shading: A new survey with benchmarks. J D Durou, M Falcone, M Sagona, Computer Vision and Image Understanding. 1091Durou JD, Falcone M, Sagona M (2008) Numerical methods for shape-from-shading: A new survey with benchmarks. Computer Vision and Image Understanding 109(1):22-43\n\nA point set generation network for 3d object reconstruction from a single image. H Fan, H Su, L J Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionFan H, Su H, Guibas LJ (2017) A point set generation network for 3d object reconstruction from a single image. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 605-613\n\nSemantic instance segmentation via deep metric learning. A Fathi, Z Wojna, V Rathod, P Wang, H O Song, S Guadarrama, K P Murphy, arXiv:170310277arXiv preprintFathi A, Wojna Z, Rathod V, Wang P, Song HO, Guadarrama S, Murphy KP (2017) Semantic instance segmentation via deep metric learning. arXiv preprint arXiv:170310277\n\nA geometric approach to shape from defocus. P Favaro, S Soatto, IEEE Transactions on Pattern Analysis and Machine Intelligence. 273Favaro P, Soatto S (2005) A geometric approach to shape from defocus. IEEE Transactions on Pattern Analysis and Machine Intelligence 27(3):406-417\n\nMeshnet: Mesh neural network for 3d shape representation. Y Feng, Y Feng, H You, X Zhao, Y Gao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Feng Y, Feng Y, You H, Zhao X, Gao Y (2019) Meshnet: Mesh neural network for 3d shape representation. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol 33, pp 8279-8286\n\nGreedy function approximation: a gradient boosting machine. J H Friedman, Annals of statistics. Friedman JH (2001) Greedy function approximation: a gradient boosting machine. Annals of statistics pp 1189-1232\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEEGeiger A, Lenz P, Urtasun R (2012) Are we ready for autonomous driving? the kitti vision benchmark suite. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition, IEEE, pp 3354-3361\n\nLearning a predictable and generative vector representation for objects. R Girdhar, D F Fouhey, M Rodriguez, A Gupta, European Conference on Computer Vision. SpringerGirdhar R, Fouhey DF, Rodriguez M, Gupta A (2016) Learning a predictable and generative vector representation for objects. In: European Conference on Computer Vision, Springer, pp 484-499\n\n) 3d pose estimation and 3d model retrieval for objects in the wild. A Grabner, P M Roth, V Lepetit, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionGrabner A, Roth PM, Lepetit V (2018) 3d pose estimation and 3d model retrieval for objects in the wild. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 3022-3031\n\nLocation field descriptors: Single image 3d model retrieval in the wild. A Grabner, P M Roth, V Lepetit, 2019 International Conference on 3D Vision (3DV). IEEEGrabner A, Roth PM, Lepetit V (2019) Location field descriptors: Single image 3d model retrieval in the wild. In: 2019 International Conference on 3D Vision (3DV), IEEE, pp 583-593\n\nAtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation. T Groueix, M Fisher, V G Kim, B Russell, Aubry M , Proceedings IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionCVPRGroueix T, Fisher M, Kim VG, Russell B, Aubry M (2018) AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation. In: Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)\n\n. R Hanocka, A Hertz, N Fish, R Giryes, S Fleishman, D Cohen-Or, ACM Transactions on Graphics (TOG). 384Hanocka R, Hertz A, Fish N, Giryes R, Fleishman S, Cohen-Or D (2019) Meshcnn: a network with an edge. ACM Transactions on Graphics (TOG) 38(4):1-12\n\nK He, G Gkioxari, P Doll\u00e1r, R Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionMask r-cnnHe K, Gkioxari G, Doll\u00e1r P, Girshick R (2017) Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision, pp 2961-2969\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. M Heusel, H Ramsauer, T Unterthiner, B Nessler, S Hochreiter, Advances in neural information processing systems. Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S (2017) Gans trained by a two time-scale update rule converge to a local nash equilibrium. In: Advances in neural information processing systems, pp 6626-6637\n\nModel based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. S Hinterstoisser, V Lepetit, S Ilic, S Holzer, G Bradski, K Konolige, N Navab, Asian conference on computer vision. SpringerHinterstoisser S, Lepetit V, Ilic S, Holzer S, Bradski G, Konolige K, Navab N (2012) Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. In: Asian conference on computer vision, Springer, pp 548-562\n\nScenenn: A scene meshes dataset with annotations. B S Hua, Q H Pham, D T Nguyen, M K Tran, L F Yu, S K Yeung, 2016 Fourth International Conference on 3D Vision (3DV). IEEEHua BS, Pham QH, Nguyen DT, Tran MK, Yu LF, Yeung SK (2016) Scenenn: A scene meshes dataset with annotations. In: 2016 Fourth International Conference on 3D Vision (3DV), IEEE, pp 92-101\n\nHolistic 3d scene parsing and reconstruction from a single rgb image. S Huang, S Qi, Y Zhu, Y Xiao, Y Xu, S C Zhu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Huang S, Qi S, Zhu Y, Xiao Y, Xu Y, Zhu SC (2018) Holistic 3d scene parsing and reconstruction from a single rgb image. In: Proceedings of the European Conference on Computer Vision (ECCV), pp 187-203\n\nMask scoring r-cnn. Z Huang, L Huang, Y Gong, C Huang, X Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHuang Z, Huang L, Gong Y, Huang C, Wang X (2019) Mask scoring r-cnn. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 6409-6418\n\n2012) 3d convolutional neural networks for human action recognition. S Ji, W Xu, M Yang, K Yu, 35Ji S, Xu W, Yang M, Yu K (2012) 3d convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence 35(1):221-231\n\nAn efficient k-means clustering algorithm: Analysis and implementation. T Kanungo, D M Mount, N S Netanyahu, C D Piatko, R Silverman, A Y Wu, IEEE transactions on pattern analysis and machine intelligence. 247Kanungo T, Mount DM, Netanyahu NS, Piatko CD, Silverman R, Wu AY (2002) An efficient k-means clustering algorithm: Analysis and implementation. IEEE transactions on pattern analysis and machine intelligence 24(7):881-892\n\nDeep learning of local rgb-d patches for 3d object detection and 6d pose estimation. W Kehl, F Milletari, F Tombari, S Ilic, N Navab, European conference on computer vision. SpringerKehl W, Milletari F, Tombari F, Ilic S, Navab N (2016) Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation. In: European conference on computer vision, Springer, pp 205-220\n\nImagenet classification with deep convolutional neural networks. J Krause, M Stark, J Deng, L Fei-Fei, 4th International IEEE Workshop on 3D Representation and Recognition. Sydney, Australia Krizhevsky A, Sutskever I, Hinton GEAdvances in neural information processing systemsKrause J, Stark M, Deng J, Fei-Fei L (2013) 3d object representations for fine-grained categorization. In: 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems, pp 1097-1105\n\nCross-domain image-based 3d shape retrieval by view sequence learning. T Lee, Y L Lin, H Chiang, M W Chiu, W Hsu, P Huang, 2018 International Conference on 3D Vision (3DV). IEEELee T, Lin YL, Chiang H, Chiu MW, Hsu W, Huang P (2018) Cross-domain image-based 3d shape retrieval by view sequence learning. In: 2018 International Conference on 3D Vision (3DV), IEEE, pp 258-266\n\nInteriornet: Mega-scale multi-sensor photo-realistic indoor scenes dataset. W Li, S Saeedi, J Mccormac, R Clark, D Tzoumanikas, Q Ye, Y Huang, R Tang, S Leutenegger, arXiv:180900716arXiv preprintLi W, Saeedi S, McCormac J, Clark R, Tzoumanikas D, Ye Q, Huang Y, Tang R, Leutenegger S (2018) Interiornet: Mega-scale multi-sensor photo-realistic indoor scenes dataset. arXiv preprint arXiv:180900716\n\nJoint embeddings of shapes and images via cnn image purification. Y Li, H Su, C R Qi, N Fish, D Cohen-Or, L J Guibas, ACM transactions on graphics (TOG). 346234Li Y, Su H, Qi CR, Fish N, Cohen-Or D, Guibas LJ (2015) Joint embeddings of shapes and images via cnn image purification. ACM transactions on graphics (TOG) 34(6):234\n\nParsing ikea objects: Fine pose estimation. J J Lim, H Pirsiavash, A Torralba, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLim JJ, Pirsiavash H, Torralba A (2013) Parsing ikea objects: Fine pose estimation. In: Proceedings of the IEEE International Conference on Computer Vision, pp 2992-2999\n\nMicrosoft coco: Common objects in context. T Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European conference on computer vision. SpringerLin TY, Maire M, Belongie S, Hays J, Perona P, Ramanan D, Doll\u00e1r P, Zitnick CL (2014) Microsoft coco: Common objects in context. In: European conference on computer vision, Springer, pp 740-755\n\nFeature pyramid networks for object detection. T Y Lin, P Doll\u00e1r, R Girshick, K He, B Hariharan, S Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionLin TY, Doll\u00e1r P, Girshick R, He K, Hariharan B, Belongie S (2017) Feature pyramid networks for object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 2117-2125\n\nSoft rasterizer: A differentiable renderer for image-based 3d reasoning. S Liu, T Li, W Chen, H Li, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLiu S, Li T, Chen W, Li H (2019) Soft rasterizer: A differentiable renderer for image-based 3d reasoning. In: Proceedings of the IEEE International Conference on Computer Vision, pp 7708-7717\n\nDeep exemplar 2d-3d detection by adapting from real to rendered views. F Massa, B C Russell, Aubry M , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMassa F, Russell BC, Aubry M (2016) Deep exemplar 2d-3d detection by adapting from real to rendered views. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 6024-6033\n\nOccupancy Networks: Learning 3D reconstruction in function space. L Mescheder, M Oechsle, M Niemeyer, S Nowozin, A Geiger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMescheder L, Oechsle M, Niemeyer M, Nowozin S, Geiger A (2019) Occupancy Networks: Learning 3D reconstruction in function space. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 4460-4470\n\nWatershed of a continuous function. L Najman, M Schmitt, Signal Processing. 381Najman L, Schmitt M (1994) Watershed of a continuous function. Signal Processing 38(1):99-112\n\nTexture fields: Learning texture representations in function space. M Oechsle, L Mescheder, M Niemeyer, T Strauss, A Geiger, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionOechsle M, Mescheder L, Niemeyer M, Strauss T, Geiger A (2019) Texture fields: Learning texture representations in function space. In: Proceedings of the IEEE International Conference on Computer Vision, pp 4531-4540\n\nLatentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation. K Park, A Mousavian, Y Xiang, D Fox, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPark K, Mousavian A, Xiang Y, Fox D (2020) Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 10710-10719\n\nPvnet: Pixel-wise voting network for 6dof pose estimation. S Peng, Y Liu, Q Huang, X Zhou, H Bao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPeng S, Liu Y, Huang Q, Zhou X, Bao H (2019) Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 4561-4570\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. C R Qi, H Su, K Mo, L J Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionQi CR, Su H, Mo K, Guibas LJ (2017a) Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 652-660\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, Advances in neural information processing systems. Qi CR, Yi L, Su H, Guibas LJ (2017b) Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in neural information processing systems, pp 5099-5108\n\nBb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. M Rad, V Lepetit, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionRad M, Lepetit V (2017) Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. In: Proceedings of the IEEE International Conference on Computer Vision, pp 3828-3836\n\nLearning to generate textures on 3d meshes. A Raj, C Ham, C Barnes, V Kim, J Lu, J Hays, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsRaj A, Ham C, Barnes C, Kim V, Lu J, Hays J (2019) Learning to generate textures on 3d meshes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp 32-38\n\n3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints. F Rothganger, S Lazebnik, C Schmid, J Ponce, International journal of computer vision. 663Rothganger F, Lazebnik S, Schmid C, Ponce J (2006) 3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints. International journal of computer vision 66(3):231-259\n\nThe princeton shape benchmark. P Shilane, P Min, M Kazhdan, T Funkhouser, Proceedings Shape Modeling Applications. Shape Modeling ApplicationsIEEEShilane P, Min P, Kazhdan M, Funkhouser T (2004) The princeton shape benchmark. In: Proceedings Shape Modeling Applications, 2004., IEEE, pp 167-178\n\nIndoor segmentation and support inference from rgbd images. N Silberman, D Hoiem, P Kohli, R Fergus, European conference on computer vision. SpringerSilberman N, Hoiem D, Kohli P, Fergus R (2012) Indoor segmentation and support inference from rgbd images. In: European conference on computer vision, Springer, pp 746-760\n\nHybridpose: 6d object pose estimation under hybrid representations. C Song, J Song, Q Huang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSong C, Song J, Huang Q (2020) Hybridpose: 6d object pose estimation under hybrid representations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 431-440\n\nSun rgb-d: A rgb-d scene understanding benchmark suite. S Song, S P Lichtenberg, J Xiao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSong S, Lichtenberg SP, Xiao J (2015) Sun rgb-d: A rgb-d scene understanding benchmark suite. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 567-576\n\nMulti-view convolutional neural networks for 3d shape recognition. H Su, S Maji, E Kalogerakis, E Learned-Miller, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionSu H, Maji S, Kalogerakis E, Learned-Miller E (2015) Multi-view convolutional neural networks for 3d shape recognition. In: Proceedings of the IEEE international conference on computer vision, pp 945-953\n\nPix3d: Dataset and methods for single-image 3d shape modeling. X Sun, J Wu, X Zhang, Z Zhang, C Zhang, T Xue, J B Tenenbaum, W T Freeman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSun X, Wu J, Zhang X, Zhang Z, Zhang C, Xue T, Tenenbaum JB, Freeman WT (2018a) Pix3d: Dataset and methods for single-image 3d shape modeling. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 2974-2983\n\nY Sun, Z Liu, Y Wang, S E Sarma, arXiv:180406375Colorful 3d reconstruction from a single image. 2arXiv preprintSun Y, Liu Z, Wang Y, Sarma SE (2018b) Im2avatar: Colorful 3d reconstruction from a single image. arXiv preprint arXiv:180406375\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSzegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A (2015) Going deeper with convolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 1-9\n\nShape2vec: semantic-based descriptors for 3d shapes, sketches and images. F P Tasse, N Dodgson, ACM Transactions on Graphics (TOG). 356208Tasse FP, Dodgson N (2016) Shape2vec: semantic-based descriptors for 3d shapes, sketches and images. ACM Transactions on Graphics (TOG) 35(6):208\n\nWhat do single-view 3d reconstruction networks learn?. M Tatarchenko, S R Richter, R Ranftl, Z Li, V Koltun, T Brox, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTatarchenko M, Richter SR, Ranftl R, Li Z, Koltun V, Brox T (2019) What do single-view 3d reconstruction networks learn? In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 3405-3414\n\nReal-time seamless single shot 6d object pose prediction. B Tekin, S N Sinha, P Fua, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTekin B, Sinha SN, Fua P (2018) Real-time seamless single shot 6d object pose prediction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 292-301\n\nMulti-view supervision for single-view reconstruction via differentiable ray consistency. S Tulsiani, T Zhou, A A Efros, J Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionTulsiani S, Zhou T, Efros AA, Malik J (2017) Multi-view supervision for single-view reconstruction via differentiable ray consistency. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 2626-2634\n\nRevisiting point cloud classification: A new benchmark dataset and classification model on real-world data. M A Uy, Q H Pham, B S Hua, T Nguyen, S K Yeung, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionUy MA, Pham QH, Hua BS, Nguyen T, Yeung SK (2019) Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In: Proceedings of the IEEE International Conference on Computer Vision, pp 1588-1597\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in neural information processing systems. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Attention is all you need. In: Advances in neural information processing systems, pp 5998-6008\n\nPixel2Mesh: Generating 3D mesh models from single RGB images. N Wang, Y Zhang, Z Li, Y Fu, W Liu, Y G Jiang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Wang N, Zhang Y, Li Z, Fu Y, Liu W, Jiang YG (2018a) Pixel2Mesh: Generating 3D mesh models from single RGB images. In: Proceedings of the European Conference on Computer Vision (ECCV), pp 52-67\n\nTem: Tree-enhanced embedding model for explainable recommendation. X Wang, X He, F Feng, L Nie, T S Chua, Proceedings of the 2018 World Wide Web Conference. the 2018 World Wide Web ConferenceWang X, He X, Feng F, Nie L, Chua TS (2018b) Tem: Tree-enhanced embedding model for explainable recommendation. In: Proceedings of the 2018 World Wide Web Conference, pp 1543-1552\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE transactions on image processing. 134Wang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13(4):600-612\n\nMarrnet: 3d shape reconstruction via 2.5 d sketches. J Wu, Y Wang, T Xue, X Sun, B Freeman, J Tenenbaum, Advances in neural information processing systems. Wu J, Wang Y, Xue T, Sun X, Freeman B, Tenenbaum J (2017) Marrnet: 3d shape reconstruction via 2.5 d sketches. In: Advances in neural information processing systems, pp 540-550\n\n3d shapenets: A deep representation for volumetric shapes. Z Wu, S Song, A Khosla, F Yu, L Zhang, X Tang, J Xiao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionWu Z, Song S, Khosla A, Yu F, Zhang L, Tang X, Xiao J (2015) 3d shapenets: A deep representation for volumetric shapes. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 1912-1920\n\nUnsupervised feature learning via non-parametric instance discrimination. Z Wu, Y Xiong, S X Yu, D Lin, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWu Z, Xiong Y, Yu SX, Lin D (2018) Unsupervised feature learning via non-parametric instance discrimination. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 3733-3742\n\nBeyond pascal: A benchmark for 3d object detection in the wild. Y Xiang, R Mottaghi, S Savarese, IEEE winter conference on applications of computer vision. IEEEXiang Y, Mottaghi R, Savarese S (2014) Beyond pascal: A benchmark for 3d object detection in the wild. In: IEEE winter conference on applications of computer vision, IEEE, pp 75-82\n\nObjectnet3d: A large scale database for 3d object recognition. Y Xiang, W Kim, W Chen, Ji J Choy, C Su, H Mottaghi, R Guibas, L Savarese, S , European Conference on Computer Vision. SpringerXiang Y, Kim W, Chen W, Ji J, Choy C, Su H, Mottaghi R, Guibas L, Savarese S (2016) Objectnet3d: A large scale database for 3d object recognition. In: European Conference on Computer Vision, Springer, pp 160-176\n\nPosecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Y Xiang, T Schmidt, V Narayanan, D Fox, arXiv:171100199arXiv preprintXiang Y, Schmidt T, Narayanan V, Fox D (2017) Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. arXiv preprint arXiv:171100199\n\nSun3d: A database of big spaces reconstructed using sfm and object labels. J Xiao, A Owens, A Torralba, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionXiao J, Owens A, Torralba A (2013) Sun3d: A database of big spaces reconstructed using sfm and object labels. In: Proceedings of the IEEE International Conference on Computer Vision, pp 1625-1632\n\nSun database: Exploring a large collection of scene categories. J Xiao, K A Ehinger, J Hays, A Torralba, A Oliva, International Journal of Computer Vision. 1191Xiao J, Ehinger KA, Hays J, Torralba A, Oliva A (2016) Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision 119(1):3-22\n\nAggregated residual transformations for deep neural networks. S Xie, R Girshick, P Doll\u00e1r, Z Tu, K He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionXie S, Girshick R, Doll\u00e1r P, Tu Z, He K (2017) Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 1492-1500\n\nDISN: Deep implicit surface network for high-quality single-view 3D reconstruction. Q Xu, W Wang, D Ceylan, R Mech, U Neumann, Advances in Neural Information Processing Systems. Xu Q, Wang W, Ceylan D, Mech R, Neumann U (2019) DISN: Deep implicit surface network for high-quality single-view 3D reconstruction. In: Advances in Neural Information Processing Systems, pp 492-502\n\nA large-scale car dataset for fine-grained categorization and verification. L Yang, P Luo, Change Loy, C Tang, X , Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYang L, Luo P, Change Loy C, Tang X (2015) A large-scale car dataset for fine-grained categorization and verification. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 3973-3981\n\nInterpretable fashion matching with rich attributes. X Yang, X He, X Wang, Y Ma, F Feng, M Wang, T S Chua, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 42nd International ACM SIGIR Conference on Research and Development in Information RetrievalYang X, He X, Wang X, Ma Y, Feng F, Wang M, Chua TS (2019) Interpretable fashion matching with rich attributes. In: Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp 775-784\n\nShape-from-shading: a survey. R Zhang, P S Tsai, J E Cryer, M Shah, 21Zhang R, Tsai PS, Cryer JE, Shah M (1999) Shape-from-shading: a survey. IEEE transactions on pattern analysis and machine intelligence 21(8):690-706\n\nJ Zheng, J Zhang, J Li, R Tang, S Gao, Z Zhou, arXiv:190800222Structured3d: A large photo-realistic dataset for structured 3d modeling. arXiv preprintZheng J, Zhang J, Li J, Tang R, Gao S, Zhou Z (2019) Structured3d: A large photo-realistic dataset for structured 3d modeling. arXiv preprint arXiv:190800222\n\nScene parsing through ade20k dataset. B Zhou, H Zhao, X Puig, S Fidler, A Barriuso, A Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhou B, Zhao H, Puig X, Fidler S, Barriuso A, Torralba A (2017) Scene parsing through ade20k dataset. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 633-641\n\nToward multimodal image-to-image translation. J Y Zhu, R Zhang, D Pathak, T Darrell, A A Efros, O Wang, E Shechtman, Advances in neural information processing systems. Zhu JY, Zhang R, Pathak D, Darrell T, Efros AA, Wang O, Shechtman E (2017) Toward multimodal image-to-image translation. In: Advances in neural information processing systems, pp 465-476\n", "annotations": {"author": "[{\"end\":54,\"start\":46},{\"end\":65,\"start\":55},{\"end\":76,\"start\":66},{\"end\":81,\"start\":77},{\"end\":125,\"start\":82},{\"end\":170,\"start\":126},{\"end\":209,\"start\":171},{\"end\":248,\"start\":210},{\"end\":257,\"start\":249},{\"end\":298,\"start\":258},{\"end\":307,\"start\":299},{\"end\":322,\"start\":308},{\"end\":337,\"start\":323},{\"end\":352,\"start\":338},{\"end\":365,\"start\":353},{\"end\":704,\"start\":366},{\"end\":762,\"start\":705}]", "publisher": null, "author_last_name": "[{\"end\":53,\"start\":51},{\"end\":64,\"start\":57},{\"end\":75,\"start\":72},{\"end\":95,\"start\":91},{\"end\":139,\"start\":135},{\"end\":184,\"start\":177},{\"end\":221,\"start\":218},{\"end\":256,\"start\":254},{\"end\":269,\"start\":266},{\"end\":306,\"start\":303},{\"end\":321,\"start\":317},{\"end\":336,\"start\":332},{\"end\":351,\"start\":344},{\"end\":364,\"start\":361}]", "author_first_name": "[{\"end\":50,\"start\":46},{\"end\":56,\"start\":55},{\"end\":69,\"start\":66},{\"end\":71,\"start\":70},{\"end\":80,\"start\":77},{\"end\":90,\"start\":82},{\"end\":134,\"start\":126},{\"end\":176,\"start\":171},{\"end\":217,\"start\":210},{\"end\":253,\"start\":249},{\"end\":265,\"start\":258},{\"end\":302,\"start\":299},{\"end\":316,\"start\":308},{\"end\":331,\"start\":323},{\"end\":343,\"start\":338},{\"end\":360,\"start\":353}]", "author_affiliation": "[{\"end\":703,\"start\":367},{\"end\":761,\"start\":706}]", "title": "[{\"end\":43,\"start\":1},{\"end\":805,\"start\":763}]", "venue": null, "abstract": "[{\"end\":2117,\"start\":969}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2674,\"start\":2654},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":2690,\"start\":2674},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":2708,\"start\":2690},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2726,\"start\":2708},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":2744,\"start\":2726},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2762,\"start\":2744},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":2780,\"start\":2762},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":2800,\"start\":2780},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2823,\"start\":2800},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2840,\"start\":2823},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2857,\"start\":2840},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2935,\"start\":2915},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":2966,\"start\":2949},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":3151,\"start\":3131},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":3185,\"start\":3165},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":3212,\"start\":3193},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3253,\"start\":3232},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3412,\"start\":3388},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3462,\"start\":3444},{\"end\":3727,\"start\":3702},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3744,\"start\":3727},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3764,\"start\":3744},{\"end\":6004,\"start\":5999},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7761,\"start\":7741},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":7777,\"start\":7761},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":7795,\"start\":7777},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7813,\"start\":7795},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":7831,\"start\":7813},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7849,\"start\":7831},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":7867,\"start\":7849},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":7887,\"start\":7867},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7910,\"start\":7887},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7927,\"start\":7910},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7944,\"start\":7927},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7962,\"start\":7944},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7983,\"start\":7962},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8273,\"start\":8251},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8332,\"start\":8312},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8392,\"start\":8372},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":8447,\"start\":8430},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8506,\"start\":8487},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8554,\"start\":8536},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":8615,\"start\":8595},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":8679,\"start\":8659},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":8699,\"start\":8680},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8771,\"start\":8750},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":8836,\"start\":8817},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8953,\"start\":8936},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":8974,\"start\":8954},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9540,\"start\":9520},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":9999,\"start\":9982},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10052,\"start\":10030},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10137,\"start\":10118},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":10172,\"start\":10155},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":10800,\"start\":10780},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":10837,\"start\":10817},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10961,\"start\":10942},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11462,\"start\":11438},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":11824,\"start\":11805},{\"end\":11958,\"start\":11909},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12500,\"start\":12483},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":12538,\"start\":12518},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13459,\"start\":13438},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":14919,\"start\":14900},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":14938,\"start\":14919},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14957,\"start\":14938},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":16934,\"start\":16917},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17123,\"start\":17102},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":17144,\"start\":17123},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19372,\"start\":19356},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20021,\"start\":19999},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":24010,\"start\":23990},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24044,\"start\":24027},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24789,\"start\":24771},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":25116,\"start\":25096},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":25134,\"start\":25116},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25151,\"start\":25134},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25171,\"start\":25151},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":25190,\"start\":25171},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26035,\"start\":26015},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":26056,\"start\":26035},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":26072,\"start\":26056},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":26268,\"start\":26251},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27287,\"start\":27267},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":27317,\"start\":27300},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27622,\"start\":27602},{\"end\":29053,\"start\":29036},{\"end\":29184,\"start\":29164},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29205,\"start\":29184},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29223,\"start\":29205},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29618,\"start\":29598},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":29649,\"start\":29632},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29873,\"start\":29855},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":30091,\"start\":30073},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31329,\"start\":31310},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31346,\"start\":31329},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":31365,\"start\":31346},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":31389,\"start\":31365},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31410,\"start\":31389},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":31444,\"start\":31425},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31463,\"start\":31444},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":31479,\"start\":31463},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31499,\"start\":31479},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31513,\"start\":31499},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31533,\"start\":31513},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31555,\"start\":31533},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":32757,\"start\":32731},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":33273,\"start\":33256},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34539,\"start\":34518},{\"end\":34546,\"start\":34539},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":34688,\"start\":34664},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34953,\"start\":34934},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34976,\"start\":34953},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34992,\"start\":34976},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35020,\"start\":34992},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":35039,\"start\":35020},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":35057,\"start\":35039},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":35075,\"start\":35057},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":35094,\"start\":35075},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":35116,\"start\":35094},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":35134,\"start\":35116},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":35666,\"start\":35639},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35704,\"start\":35684},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35757,\"start\":35731},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35936,\"start\":35917},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35962,\"start\":35936},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":36232,\"start\":36212},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":36264,\"start\":36244},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":36295,\"start\":36276},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36889,\"start\":36862},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36905,\"start\":36889},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":37024,\"start\":37006},{\"end\":37132,\"start\":37120},{\"end\":37139,\"start\":37132},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":38158,\"start\":38138},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38230,\"start\":38209},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":38290,\"start\":38270},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":39153,\"start\":39133},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":39966,\"start\":39946},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":39985,\"start\":39966},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":40022,\"start\":39997},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40214,\"start\":40196},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40255,\"start\":40233},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":40405,\"start\":40385},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40571,\"start\":40552},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":40706,\"start\":40686},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":40734,\"start\":40717},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":40900,\"start\":40883},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":41571,\"start\":41552},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":41593,\"start\":41571},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":42075,\"start\":42057},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":42096,\"start\":42075},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":42515,\"start\":42497},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":43354,\"start\":43335},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":43414,\"start\":43393},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":43706,\"start\":43684},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":47517,\"start\":47497},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":47546,\"start\":47529},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":48493,\"start\":48475}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45797,\"start\":45392},{\"attributes\":{\"id\":\"fig_1\"},\"end\":45974,\"start\":45798},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46193,\"start\":45975},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46339,\"start\":46194},{\"attributes\":{\"id\":\"fig_4\"},\"end\":46431,\"start\":46340},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46687,\"start\":46432},{\"attributes\":{\"id\":\"fig_6\"},\"end\":46928,\"start\":46688},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47412,\"start\":46929},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47705,\"start\":47413},{\"attributes\":{\"id\":\"fig_9\"},\"end\":48006,\"start\":47706},{\"attributes\":{\"id\":\"fig_10\"},\"end\":48135,\"start\":48007},{\"attributes\":{\"id\":\"fig_11\"},\"end\":48291,\"start\":48136},{\"attributes\":{\"id\":\"fig_12\"},\"end\":48356,\"start\":48292},{\"attributes\":{\"id\":\"fig_13\"},\"end\":48494,\"start\":48357},{\"attributes\":{\"id\":\"fig_14\"},\"end\":48606,\"start\":48495},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49983,\"start\":48607},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":50090,\"start\":49984},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50589,\"start\":50091}]", "paragraph": "[{\"end\":3665,\"start\":2133},{\"end\":5126,\"start\":3667},{\"end\":6247,\"start\":5128},{\"end\":7210,\"start\":6249},{\"end\":7630,\"start\":7212},{\"end\":8151,\"start\":7647},{\"end\":9359,\"start\":8166},{\"end\":10568,\"start\":9444},{\"end\":11259,\"start\":10570},{\"end\":13023,\"start\":11276},{\"end\":13246,\"start\":13052},{\"end\":13747,\"start\":13280},{\"end\":14551,\"start\":13749},{\"end\":15055,\"start\":14589},{\"end\":15097,\"start\":15057},{\"end\":15463,\"start\":15165},{\"end\":16161,\"start\":15489},{\"end\":16234,\"start\":16163},{\"end\":16509,\"start\":16349},{\"end\":16935,\"start\":16546},{\"end\":17203,\"start\":16937},{\"end\":17482,\"start\":17267},{\"end\":17897,\"start\":17616},{\"end\":18004,\"start\":17899},{\"end\":19022,\"start\":18139},{\"end\":19547,\"start\":19189},{\"end\":20367,\"start\":19549},{\"end\":21113,\"start\":20406},{\"end\":21533,\"start\":21152},{\"end\":22091,\"start\":21570},{\"end\":22309,\"start\":22093},{\"end\":22948,\"start\":22311},{\"end\":23232,\"start\":22950},{\"end\":23438,\"start\":23234},{\"end\":23671,\"start\":23466},{\"end\":24285,\"start\":23708},{\"end\":25007,\"start\":24287},{\"end\":25898,\"start\":25028},{\"end\":26632,\"start\":25948},{\"end\":27165,\"start\":26634},{\"end\":27476,\"start\":27167},{\"end\":28213,\"start\":27504},{\"end\":28829,\"start\":28238},{\"end\":29418,\"start\":28868},{\"end\":30657,\"start\":29420},{\"end\":31171,\"start\":30692},{\"end\":31942,\"start\":31173},{\"end\":32589,\"start\":31944},{\"end\":32815,\"start\":32591},{\"end\":34085,\"start\":32817},{\"end\":36018,\"start\":34145},{\"end\":37923,\"start\":36020},{\"end\":38662,\"start\":37925},{\"end\":39733,\"start\":38664},{\"end\":40572,\"start\":39774},{\"end\":41300,\"start\":40574},{\"end\":42097,\"start\":41336},{\"end\":42927,\"start\":42099},{\"end\":44508,\"start\":42929},{\"end\":45391,\"start\":44523}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9396,\"start\":9360},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9431,\"start\":9396},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15164,\"start\":15098},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16348,\"start\":16235},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16545,\"start\":16510},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17266,\"start\":17204},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17534,\"start\":17483},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17615,\"start\":17534},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18138,\"start\":18005},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19162,\"start\":19023}]", "table_ref": "[{\"end\":25855,\"start\":25848},{\"end\":26353,\"start\":26346},{\"end\":30316,\"start\":30309},{\"end\":31005,\"start\":30998},{\"end\":32306,\"start\":32300},{\"end\":32482,\"start\":32475},{\"end\":33553,\"start\":33547},{\"end\":38739,\"start\":38732},{\"end\":41106,\"start\":41099},{\"end\":43016,\"start\":43010},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":43081,\"start\":43074},{\"end\":43763,\"start\":43756}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2131,\"start\":2119},{\"attributes\":{\"n\":\"2\"},\"end\":7645,\"start\":7633},{\"end\":8164,\"start\":8154},{\"attributes\":{\"n\":\"2.1\"},\"end\":9442,\"start\":9433},{\"attributes\":{\"n\":\"2.2\"},\"end\":11274,\"start\":11262},{\"attributes\":{\"n\":\"3\"},\"end\":13050,\"start\":13026},{\"attributes\":{\"n\":\"3.1\"},\"end\":13278,\"start\":13249},{\"attributes\":{\"n\":\"3.2\"},\"end\":14587,\"start\":14554},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":15487,\"start\":15466},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":19187,\"start\":19164},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":20404,\"start\":20370},{\"attributes\":{\"n\":\"3.3\"},\"end\":21150,\"start\":21116},{\"attributes\":{\"n\":\"3.4\"},\"end\":21568,\"start\":21536},{\"attributes\":{\"n\":\"4\"},\"end\":23464,\"start\":23441},{\"attributes\":{\"n\":\"4.1\"},\"end\":23706,\"start\":23674},{\"attributes\":{\"n\":\"4.2\"},\"end\":25026,\"start\":25010},{\"attributes\":{\"n\":\"4.3\"},\"end\":25946,\"start\":25901},{\"attributes\":{\"n\":\"4.4\"},\"end\":27502,\"start\":27479},{\"attributes\":{\"n\":\"5\"},\"end\":28236,\"start\":28216},{\"attributes\":{\"n\":\"5.1\"},\"end\":28866,\"start\":28832},{\"attributes\":{\"n\":\"5.2\"},\"end\":30690,\"start\":30660},{\"attributes\":{\"n\":\"5.3\"},\"end\":34143,\"start\":34088},{\"attributes\":{\"n\":\"5.4\"},\"end\":39772,\"start\":39736},{\"attributes\":{\"n\":\"5.5\"},\"end\":41334,\"start\":41303},{\"attributes\":{\"n\":\"6\"},\"end\":44521,\"start\":44511},{\"end\":45399,\"start\":45393},{\"end\":45805,\"start\":45799},{\"end\":45982,\"start\":45976},{\"end\":46201,\"start\":46195},{\"end\":46347,\"start\":46341},{\"end\":46439,\"start\":46433},{\"end\":46695,\"start\":46689},{\"end\":46936,\"start\":46930},{\"end\":47421,\"start\":47414},{\"end\":47714,\"start\":47707},{\"end\":48015,\"start\":48008},{\"end\":48140,\"start\":48137},{\"end\":48300,\"start\":48293},{\"end\":48365,\"start\":48358},{\"end\":48503,\"start\":48496},{\"end\":49992,\"start\":49985}]", "table": "[{\"end\":49983,\"start\":48756},{\"end\":50589,\"start\":50218}]", "figure_caption": "[{\"end\":45797,\"start\":45401},{\"end\":45974,\"start\":45807},{\"end\":46193,\"start\":45984},{\"end\":46339,\"start\":46203},{\"end\":46431,\"start\":46349},{\"end\":46687,\"start\":46441},{\"end\":46928,\"start\":46697},{\"end\":47412,\"start\":46938},{\"end\":47705,\"start\":47424},{\"end\":48006,\"start\":47717},{\"end\":48135,\"start\":48018},{\"end\":48291,\"start\":48141},{\"end\":48356,\"start\":48303},{\"end\":48494,\"start\":48368},{\"end\":48606,\"start\":48506},{\"end\":48756,\"start\":48609},{\"end\":50090,\"start\":49994},{\"end\":50218,\"start\":50093}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5336,\"start\":5328},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9291,\"start\":9283},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":9301,\"start\":9293},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":9315,\"start\":9307},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14396,\"start\":14388},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15054,\"start\":15046},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19438,\"start\":19430},{\"end\":21742,\"start\":21736},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23039,\"start\":23031},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24713,\"start\":24706},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24837,\"start\":24830},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25895,\"start\":25887},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26113,\"start\":26106},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27036,\"start\":27028},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27179,\"start\":27170},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28105,\"start\":28097},{\"end\":28118,\"start\":28110},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30330,\"start\":30321},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33619,\"start\":33610},{\"end\":36967,\"start\":36960},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39317,\"start\":39308},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39683,\"start\":39674},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39697,\"start\":39688},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41120,\"start\":41111},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42591,\"start\":42582},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44071,\"start\":44062},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44085,\"start\":44076}]", "bib_author_first_name": "[{\"end\":50839,\"start\":50838},{\"end\":50848,\"start\":50847},{\"end\":50860,\"start\":50859},{\"end\":50862,\"start\":50861},{\"end\":50871,\"start\":50870},{\"end\":50873,\"start\":50872},{\"end\":50884,\"start\":50883},{\"end\":51363,\"start\":51362},{\"end\":51365,\"start\":51364},{\"end\":51576,\"start\":51575},{\"end\":51586,\"start\":51585},{\"end\":51597,\"start\":51596},{\"end\":52013,\"start\":52012},{\"end\":52019,\"start\":52018},{\"end\":52026,\"start\":52025},{\"end\":52306,\"start\":52305},{\"end\":52319,\"start\":52318},{\"end\":52328,\"start\":52327},{\"end\":52338,\"start\":52337},{\"end\":52349,\"start\":52348},{\"end\":52360,\"start\":52359},{\"end\":52690,\"start\":52689},{\"end\":52699,\"start\":52696},{\"end\":52714,\"start\":52713},{\"end\":52723,\"start\":52722},{\"end\":52730,\"start\":52729},{\"end\":52744,\"start\":52743},{\"end\":52754,\"start\":52753},{\"end\":52766,\"start\":52765},{\"end\":52775,\"start\":52774},{\"end\":52783,\"start\":52782},{\"end\":52791,\"start\":52790},{\"end\":53304,\"start\":53303},{\"end\":53306,\"start\":53305},{\"end\":53315,\"start\":53314},{\"end\":53329,\"start\":53328},{\"end\":53339,\"start\":53338},{\"end\":53351,\"start\":53350},{\"end\":53360,\"start\":53359},{\"end\":53366,\"start\":53365},{\"end\":53378,\"start\":53377},{\"end\":53387,\"start\":53386},{\"end\":53395,\"start\":53394},{\"end\":53758,\"start\":53757},{\"end\":53766,\"start\":53765},{\"end\":53775,\"start\":53774},{\"end\":53781,\"start\":53780},{\"end\":53788,\"start\":53787},{\"end\":53795,\"start\":53794},{\"end\":53802,\"start\":53801},{\"end\":53808,\"start\":53807},{\"end\":53819,\"start\":53818},{\"end\":53827,\"start\":53826},{\"end\":54382,\"start\":54381},{\"end\":54390,\"start\":54389},{\"end\":54398,\"start\":54397},{\"end\":54405,\"start\":54404},{\"end\":54414,\"start\":54413},{\"end\":54426,\"start\":54425},{\"end\":54438,\"start\":54437},{\"end\":54725,\"start\":54724},{\"end\":54733,\"start\":54732},{\"end\":54735,\"start\":54734},{\"end\":54743,\"start\":54742},{\"end\":54753,\"start\":54752},{\"end\":55013,\"start\":55012},{\"end\":55015,\"start\":55014},{\"end\":55023,\"start\":55022},{\"end\":55029,\"start\":55028},{\"end\":55037,\"start\":55036},{\"end\":55045,\"start\":55044},{\"end\":55449,\"start\":55448},{\"end\":55459,\"start\":55458},{\"end\":55471,\"start\":55470},{\"end\":55473,\"start\":55472},{\"end\":55786,\"start\":55785},{\"end\":55793,\"start\":55792},{\"end\":55795,\"start\":55794},{\"end\":55804,\"start\":55803},{\"end\":55813,\"start\":55812},{\"end\":55823,\"start\":55822},{\"end\":55837,\"start\":55836},{\"end\":56216,\"start\":56215},{\"end\":56226,\"start\":56225},{\"end\":56228,\"start\":56227},{\"end\":56237,\"start\":56236},{\"end\":56244,\"start\":56243},{\"end\":56620,\"start\":56619},{\"end\":56636,\"start\":56635},{\"end\":56647,\"start\":56646},{\"end\":56649,\"start\":56648},{\"end\":56932,\"start\":56931},{\"end\":56934,\"start\":56933},{\"end\":56943,\"start\":56942},{\"end\":56954,\"start\":56953},{\"end\":57253,\"start\":57252},{\"end\":57260,\"start\":57259},{\"end\":57266,\"start\":57265},{\"end\":57268,\"start\":57267},{\"end\":57682,\"start\":57681},{\"end\":57691,\"start\":57690},{\"end\":57700,\"start\":57699},{\"end\":57710,\"start\":57709},{\"end\":57718,\"start\":57717},{\"end\":57720,\"start\":57719},{\"end\":57728,\"start\":57727},{\"end\":57742,\"start\":57741},{\"end\":57744,\"start\":57743},{\"end\":57992,\"start\":57991},{\"end\":58002,\"start\":58001},{\"end\":58285,\"start\":58284},{\"end\":58293,\"start\":58292},{\"end\":58301,\"start\":58300},{\"end\":58308,\"start\":58307},{\"end\":58316,\"start\":58315},{\"end\":58685,\"start\":58684},{\"end\":58687,\"start\":58686},{\"end\":58906,\"start\":58905},{\"end\":58916,\"start\":58915},{\"end\":58924,\"start\":58923},{\"end\":59272,\"start\":59271},{\"end\":59283,\"start\":59282},{\"end\":59285,\"start\":59284},{\"end\":59295,\"start\":59294},{\"end\":59308,\"start\":59307},{\"end\":59623,\"start\":59622},{\"end\":59634,\"start\":59633},{\"end\":59636,\"start\":59635},{\"end\":59644,\"start\":59643},{\"end\":60070,\"start\":60069},{\"end\":60081,\"start\":60080},{\"end\":60083,\"start\":60082},{\"end\":60091,\"start\":60090},{\"end\":60407,\"start\":60406},{\"end\":60418,\"start\":60417},{\"end\":60428,\"start\":60427},{\"end\":60430,\"start\":60429},{\"end\":60437,\"start\":60436},{\"end\":60452,\"start\":60447},{\"end\":60454,\"start\":60453},{\"end\":60786,\"start\":60785},{\"end\":60797,\"start\":60796},{\"end\":60806,\"start\":60805},{\"end\":60814,\"start\":60813},{\"end\":60824,\"start\":60823},{\"end\":60837,\"start\":60836},{\"end\":61037,\"start\":61036},{\"end\":61043,\"start\":61042},{\"end\":61055,\"start\":61054},{\"end\":61065,\"start\":61064},{\"end\":61436,\"start\":61435},{\"end\":61446,\"start\":61445},{\"end\":61458,\"start\":61457},{\"end\":61473,\"start\":61472},{\"end\":61484,\"start\":61483},{\"end\":61877,\"start\":61876},{\"end\":61895,\"start\":61894},{\"end\":61906,\"start\":61905},{\"end\":61914,\"start\":61913},{\"end\":61924,\"start\":61923},{\"end\":61935,\"start\":61934},{\"end\":61947,\"start\":61946},{\"end\":62307,\"start\":62306},{\"end\":62309,\"start\":62308},{\"end\":62316,\"start\":62315},{\"end\":62318,\"start\":62317},{\"end\":62326,\"start\":62325},{\"end\":62328,\"start\":62327},{\"end\":62338,\"start\":62337},{\"end\":62340,\"start\":62339},{\"end\":62348,\"start\":62347},{\"end\":62350,\"start\":62349},{\"end\":62356,\"start\":62355},{\"end\":62358,\"start\":62357},{\"end\":62686,\"start\":62685},{\"end\":62695,\"start\":62694},{\"end\":62701,\"start\":62700},{\"end\":62708,\"start\":62707},{\"end\":62716,\"start\":62715},{\"end\":62722,\"start\":62721},{\"end\":62724,\"start\":62723},{\"end\":63068,\"start\":63067},{\"end\":63077,\"start\":63076},{\"end\":63086,\"start\":63085},{\"end\":63094,\"start\":63093},{\"end\":63103,\"start\":63102},{\"end\":63487,\"start\":63486},{\"end\":63493,\"start\":63492},{\"end\":63499,\"start\":63498},{\"end\":63507,\"start\":63506},{\"end\":63760,\"start\":63759},{\"end\":63771,\"start\":63770},{\"end\":63773,\"start\":63772},{\"end\":63782,\"start\":63781},{\"end\":63784,\"start\":63783},{\"end\":63797,\"start\":63796},{\"end\":63799,\"start\":63798},{\"end\":63809,\"start\":63808},{\"end\":63822,\"start\":63821},{\"end\":63824,\"start\":63823},{\"end\":64204,\"start\":64203},{\"end\":64212,\"start\":64211},{\"end\":64225,\"start\":64224},{\"end\":64236,\"start\":64235},{\"end\":64244,\"start\":64243},{\"end\":64572,\"start\":64571},{\"end\":64582,\"start\":64581},{\"end\":64591,\"start\":64590},{\"end\":64599,\"start\":64598},{\"end\":65237,\"start\":65236},{\"end\":65244,\"start\":65243},{\"end\":65246,\"start\":65245},{\"end\":65253,\"start\":65252},{\"end\":65263,\"start\":65262},{\"end\":65265,\"start\":65264},{\"end\":65273,\"start\":65272},{\"end\":65280,\"start\":65279},{\"end\":65618,\"start\":65617},{\"end\":65624,\"start\":65623},{\"end\":65634,\"start\":65633},{\"end\":65646,\"start\":65645},{\"end\":65655,\"start\":65654},{\"end\":65670,\"start\":65669},{\"end\":65676,\"start\":65675},{\"end\":65685,\"start\":65684},{\"end\":65693,\"start\":65692},{\"end\":66007,\"start\":66006},{\"end\":66013,\"start\":66012},{\"end\":66019,\"start\":66018},{\"end\":66021,\"start\":66020},{\"end\":66027,\"start\":66026},{\"end\":66035,\"start\":66034},{\"end\":66047,\"start\":66046},{\"end\":66049,\"start\":66048},{\"end\":66313,\"start\":66312},{\"end\":66315,\"start\":66314},{\"end\":66322,\"start\":66321},{\"end\":66336,\"start\":66335},{\"end\":66683,\"start\":66682},{\"end\":66685,\"start\":66684},{\"end\":66692,\"start\":66691},{\"end\":66701,\"start\":66700},{\"end\":66713,\"start\":66712},{\"end\":66721,\"start\":66720},{\"end\":66731,\"start\":66730},{\"end\":66742,\"start\":66741},{\"end\":66752,\"start\":66751},{\"end\":66754,\"start\":66753},{\"end\":67055,\"start\":67054},{\"end\":67057,\"start\":67056},{\"end\":67064,\"start\":67063},{\"end\":67074,\"start\":67073},{\"end\":67086,\"start\":67085},{\"end\":67092,\"start\":67091},{\"end\":67105,\"start\":67104},{\"end\":67542,\"start\":67541},{\"end\":67549,\"start\":67548},{\"end\":67555,\"start\":67554},{\"end\":67563,\"start\":67562},{\"end\":67954,\"start\":67953},{\"end\":67963,\"start\":67962},{\"end\":67965,\"start\":67964},{\"end\":67980,\"start\":67975},{\"end\":67982,\"start\":67981},{\"end\":68397,\"start\":68396},{\"end\":68410,\"start\":68409},{\"end\":68421,\"start\":68420},{\"end\":68433,\"start\":68432},{\"end\":68444,\"start\":68443},{\"end\":68857,\"start\":68856},{\"end\":68867,\"start\":68866},{\"end\":69063,\"start\":69062},{\"end\":69074,\"start\":69073},{\"end\":69087,\"start\":69086},{\"end\":69099,\"start\":69098},{\"end\":69110,\"start\":69109},{\"end\":69563,\"start\":69562},{\"end\":69571,\"start\":69570},{\"end\":69584,\"start\":69583},{\"end\":69593,\"start\":69592},{\"end\":70058,\"start\":70057},{\"end\":70066,\"start\":70065},{\"end\":70073,\"start\":70072},{\"end\":70082,\"start\":70081},{\"end\":70090,\"start\":70089},{\"end\":70517,\"start\":70516},{\"end\":70519,\"start\":70518},{\"end\":70525,\"start\":70524},{\"end\":70531,\"start\":70530},{\"end\":70537,\"start\":70536},{\"end\":70539,\"start\":70538},{\"end\":70980,\"start\":70979},{\"end\":70982,\"start\":70981},{\"end\":70988,\"start\":70987},{\"end\":70994,\"start\":70993},{\"end\":71000,\"start\":70999},{\"end\":71002,\"start\":71001},{\"end\":71383,\"start\":71382},{\"end\":71390,\"start\":71389},{\"end\":71811,\"start\":71810},{\"end\":71818,\"start\":71817},{\"end\":71825,\"start\":71824},{\"end\":71835,\"start\":71834},{\"end\":71842,\"start\":71841},{\"end\":71848,\"start\":71847},{\"end\":72333,\"start\":72332},{\"end\":72347,\"start\":72346},{\"end\":72359,\"start\":72358},{\"end\":72369,\"start\":72368},{\"end\":72679,\"start\":72678},{\"end\":72690,\"start\":72689},{\"end\":72697,\"start\":72696},{\"end\":72708,\"start\":72707},{\"end\":73004,\"start\":73003},{\"end\":73017,\"start\":73016},{\"end\":73026,\"start\":73025},{\"end\":73035,\"start\":73034},{\"end\":73334,\"start\":73333},{\"end\":73342,\"start\":73341},{\"end\":73350,\"start\":73349},{\"end\":73762,\"start\":73761},{\"end\":73770,\"start\":73769},{\"end\":73772,\"start\":73771},{\"end\":73787,\"start\":73786},{\"end\":74192,\"start\":74191},{\"end\":74198,\"start\":74197},{\"end\":74206,\"start\":74205},{\"end\":74221,\"start\":74220},{\"end\":74628,\"start\":74627},{\"end\":74635,\"start\":74634},{\"end\":74641,\"start\":74640},{\"end\":74650,\"start\":74649},{\"end\":74659,\"start\":74658},{\"end\":74668,\"start\":74667},{\"end\":74675,\"start\":74674},{\"end\":74677,\"start\":74676},{\"end\":74690,\"start\":74689},{\"end\":74692,\"start\":74691},{\"end\":75084,\"start\":75083},{\"end\":75091,\"start\":75090},{\"end\":75098,\"start\":75097},{\"end\":75106,\"start\":75105},{\"end\":75108,\"start\":75107},{\"end\":75357,\"start\":75356},{\"end\":75368,\"start\":75367},{\"end\":75375,\"start\":75374},{\"end\":75382,\"start\":75381},{\"end\":75394,\"start\":75393},{\"end\":75402,\"start\":75401},{\"end\":75414,\"start\":75413},{\"end\":75423,\"start\":75422},{\"end\":75436,\"start\":75435},{\"end\":75887,\"start\":75886},{\"end\":75889,\"start\":75888},{\"end\":75898,\"start\":75897},{\"end\":76153,\"start\":76152},{\"end\":76168,\"start\":76167},{\"end\":76170,\"start\":76169},{\"end\":76181,\"start\":76180},{\"end\":76191,\"start\":76190},{\"end\":76197,\"start\":76196},{\"end\":76207,\"start\":76206},{\"end\":76632,\"start\":76631},{\"end\":76641,\"start\":76640},{\"end\":76643,\"start\":76642},{\"end\":76652,\"start\":76651},{\"end\":77075,\"start\":77074},{\"end\":77087,\"start\":77086},{\"end\":77095,\"start\":77094},{\"end\":77097,\"start\":77096},{\"end\":77106,\"start\":77105},{\"end\":77596,\"start\":77595},{\"end\":77598,\"start\":77597},{\"end\":77604,\"start\":77603},{\"end\":77606,\"start\":77605},{\"end\":77614,\"start\":77613},{\"end\":77616,\"start\":77615},{\"end\":77623,\"start\":77622},{\"end\":77633,\"start\":77632},{\"end\":77635,\"start\":77634},{\"end\":78037,\"start\":78036},{\"end\":78048,\"start\":78047},{\"end\":78059,\"start\":78058},{\"end\":78069,\"start\":78068},{\"end\":78082,\"start\":78081},{\"end\":78091,\"start\":78090},{\"end\":78093,\"start\":78092},{\"end\":78102,\"start\":78101},{\"end\":78112,\"start\":78111},{\"end\":78429,\"start\":78428},{\"end\":78437,\"start\":78436},{\"end\":78446,\"start\":78445},{\"end\":78452,\"start\":78451},{\"end\":78458,\"start\":78457},{\"end\":78465,\"start\":78464},{\"end\":78467,\"start\":78466},{\"end\":78853,\"start\":78852},{\"end\":78861,\"start\":78860},{\"end\":78867,\"start\":78866},{\"end\":78875,\"start\":78874},{\"end\":78882,\"start\":78881},{\"end\":78884,\"start\":78883},{\"end\":79232,\"start\":79231},{\"end\":79240,\"start\":79239},{\"end\":79242,\"start\":79241},{\"end\":79251,\"start\":79250},{\"end\":79253,\"start\":79252},{\"end\":79263,\"start\":79262},{\"end\":79265,\"start\":79264},{\"end\":79551,\"start\":79550},{\"end\":79557,\"start\":79556},{\"end\":79565,\"start\":79564},{\"end\":79572,\"start\":79571},{\"end\":79579,\"start\":79578},{\"end\":79590,\"start\":79589},{\"end\":79891,\"start\":79890},{\"end\":79897,\"start\":79896},{\"end\":79905,\"start\":79904},{\"end\":79915,\"start\":79914},{\"end\":79921,\"start\":79920},{\"end\":79930,\"start\":79929},{\"end\":79938,\"start\":79937},{\"end\":80378,\"start\":80377},{\"end\":80384,\"start\":80383},{\"end\":80393,\"start\":80392},{\"end\":80395,\"start\":80394},{\"end\":80401,\"start\":80400},{\"end\":80819,\"start\":80818},{\"end\":80828,\"start\":80827},{\"end\":80840,\"start\":80839},{\"end\":81160,\"start\":81159},{\"end\":81169,\"start\":81168},{\"end\":81176,\"start\":81175},{\"end\":81185,\"start\":81183},{\"end\":81187,\"start\":81186},{\"end\":81195,\"start\":81194},{\"end\":81201,\"start\":81200},{\"end\":81213,\"start\":81212},{\"end\":81223,\"start\":81222},{\"end\":81235,\"start\":81234},{\"end\":81591,\"start\":81590},{\"end\":81600,\"start\":81599},{\"end\":81611,\"start\":81610},{\"end\":81624,\"start\":81623},{\"end\":81904,\"start\":81903},{\"end\":81912,\"start\":81911},{\"end\":81921,\"start\":81920},{\"end\":82315,\"start\":82314},{\"end\":82323,\"start\":82322},{\"end\":82325,\"start\":82324},{\"end\":82336,\"start\":82335},{\"end\":82344,\"start\":82343},{\"end\":82356,\"start\":82355},{\"end\":82646,\"start\":82645},{\"end\":82653,\"start\":82652},{\"end\":82665,\"start\":82664},{\"end\":82675,\"start\":82674},{\"end\":82681,\"start\":82680},{\"end\":83118,\"start\":83117},{\"end\":83124,\"start\":83123},{\"end\":83132,\"start\":83131},{\"end\":83142,\"start\":83141},{\"end\":83150,\"start\":83149},{\"end\":83488,\"start\":83487},{\"end\":83496,\"start\":83495},{\"end\":83508,\"start\":83502},{\"end\":83515,\"start\":83514},{\"end\":83523,\"start\":83522},{\"end\":83937,\"start\":83936},{\"end\":83945,\"start\":83944},{\"end\":83951,\"start\":83950},{\"end\":83959,\"start\":83958},{\"end\":83965,\"start\":83964},{\"end\":83973,\"start\":83972},{\"end\":83981,\"start\":83980},{\"end\":83983,\"start\":83982},{\"end\":84471,\"start\":84470},{\"end\":84480,\"start\":84479},{\"end\":84482,\"start\":84481},{\"end\":84490,\"start\":84489},{\"end\":84492,\"start\":84491},{\"end\":84501,\"start\":84500},{\"end\":84661,\"start\":84660},{\"end\":84670,\"start\":84669},{\"end\":84679,\"start\":84678},{\"end\":84685,\"start\":84684},{\"end\":84693,\"start\":84692},{\"end\":84700,\"start\":84699},{\"end\":85008,\"start\":85007},{\"end\":85016,\"start\":85015},{\"end\":85024,\"start\":85023},{\"end\":85032,\"start\":85031},{\"end\":85042,\"start\":85041},{\"end\":85054,\"start\":85053},{\"end\":85450,\"start\":85449},{\"end\":85452,\"start\":85451},{\"end\":85459,\"start\":85458},{\"end\":85468,\"start\":85467},{\"end\":85478,\"start\":85477},{\"end\":85489,\"start\":85488},{\"end\":85491,\"start\":85490},{\"end\":85500,\"start\":85499},{\"end\":85508,\"start\":85507}]", "bib_author_last_name": "[{\"end\":50845,\"start\":50840},{\"end\":50857,\"start\":50849},{\"end\":50868,\"start\":50863},{\"end\":50881,\"start\":50874},{\"end\":50890,\"start\":50885},{\"end\":51373,\"start\":51366},{\"end\":51583,\"start\":51577},{\"end\":51594,\"start\":51587},{\"end\":51603,\"start\":51598},{\"end\":52016,\"start\":52014},{\"end\":52023,\"start\":52020},{\"end\":52030,\"start\":52027},{\"end\":52316,\"start\":52307},{\"end\":52325,\"start\":52320},{\"end\":52335,\"start\":52329},{\"end\":52346,\"start\":52339},{\"end\":52357,\"start\":52350},{\"end\":52367,\"start\":52361},{\"end\":52694,\"start\":52691},{\"end\":52711,\"start\":52700},{\"end\":52720,\"start\":52715},{\"end\":52727,\"start\":52724},{\"end\":52741,\"start\":52731},{\"end\":52751,\"start\":52745},{\"end\":52763,\"start\":52755},{\"end\":52772,\"start\":52767},{\"end\":52780,\"start\":52776},{\"end\":52788,\"start\":52784},{\"end\":52797,\"start\":52792},{\"end\":53312,\"start\":53307},{\"end\":53326,\"start\":53316},{\"end\":53336,\"start\":53330},{\"end\":53348,\"start\":53340},{\"end\":53357,\"start\":53352},{\"end\":53363,\"start\":53361},{\"end\":53375,\"start\":53367},{\"end\":53384,\"start\":53379},{\"end\":53392,\"start\":53388},{\"end\":53398,\"start\":53396},{\"end\":53763,\"start\":53759},{\"end\":53772,\"start\":53767},{\"end\":53778,\"start\":53776},{\"end\":53785,\"start\":53782},{\"end\":53792,\"start\":53789},{\"end\":53799,\"start\":53796},{\"end\":53805,\"start\":53803},{\"end\":53816,\"start\":53809},{\"end\":53824,\"start\":53820},{\"end\":53832,\"start\":53828},{\"end\":54387,\"start\":54383},{\"end\":54395,\"start\":54391},{\"end\":54402,\"start\":54399},{\"end\":54411,\"start\":54406},{\"end\":54423,\"start\":54415},{\"end\":54435,\"start\":54427},{\"end\":54445,\"start\":54439},{\"end\":54730,\"start\":54726},{\"end\":54740,\"start\":54736},{\"end\":54750,\"start\":54744},{\"end\":54760,\"start\":54754},{\"end\":55020,\"start\":55016},{\"end\":55026,\"start\":55024},{\"end\":55034,\"start\":55030},{\"end\":55042,\"start\":55038},{\"end\":55054,\"start\":55046},{\"end\":55456,\"start\":55450},{\"end\":55468,\"start\":55460},{\"end\":55483,\"start\":55474},{\"end\":55790,\"start\":55787},{\"end\":55801,\"start\":55796},{\"end\":55810,\"start\":55805},{\"end\":55820,\"start\":55814},{\"end\":55834,\"start\":55824},{\"end\":55845,\"start\":55838},{\"end\":56223,\"start\":56217},{\"end\":56234,\"start\":56229},{\"end\":56241,\"start\":56238},{\"end\":56254,\"start\":56245},{\"end\":56633,\"start\":56621},{\"end\":56644,\"start\":56637},{\"end\":56655,\"start\":56650},{\"end\":56940,\"start\":56935},{\"end\":56951,\"start\":56944},{\"end\":56961,\"start\":56955},{\"end\":57257,\"start\":57254},{\"end\":57263,\"start\":57261},{\"end\":57275,\"start\":57269},{\"end\":57688,\"start\":57683},{\"end\":57697,\"start\":57692},{\"end\":57707,\"start\":57701},{\"end\":57715,\"start\":57711},{\"end\":57725,\"start\":57721},{\"end\":57739,\"start\":57729},{\"end\":57751,\"start\":57745},{\"end\":57999,\"start\":57993},{\"end\":58009,\"start\":58003},{\"end\":58290,\"start\":58286},{\"end\":58298,\"start\":58294},{\"end\":58305,\"start\":58302},{\"end\":58313,\"start\":58309},{\"end\":58320,\"start\":58317},{\"end\":58696,\"start\":58688},{\"end\":58913,\"start\":58907},{\"end\":58921,\"start\":58917},{\"end\":58932,\"start\":58925},{\"end\":59280,\"start\":59273},{\"end\":59292,\"start\":59286},{\"end\":59305,\"start\":59296},{\"end\":59314,\"start\":59309},{\"end\":59631,\"start\":59624},{\"end\":59641,\"start\":59637},{\"end\":59652,\"start\":59645},{\"end\":60078,\"start\":60071},{\"end\":60088,\"start\":60084},{\"end\":60099,\"start\":60092},{\"end\":60415,\"start\":60408},{\"end\":60425,\"start\":60419},{\"end\":60434,\"start\":60431},{\"end\":60445,\"start\":60438},{\"end\":60794,\"start\":60787},{\"end\":60803,\"start\":60798},{\"end\":60811,\"start\":60807},{\"end\":60821,\"start\":60815},{\"end\":60834,\"start\":60825},{\"end\":60846,\"start\":60838},{\"end\":61040,\"start\":61038},{\"end\":61052,\"start\":61044},{\"end\":61062,\"start\":61056},{\"end\":61074,\"start\":61066},{\"end\":61443,\"start\":61437},{\"end\":61455,\"start\":61447},{\"end\":61470,\"start\":61459},{\"end\":61481,\"start\":61474},{\"end\":61495,\"start\":61485},{\"end\":61892,\"start\":61878},{\"end\":61903,\"start\":61896},{\"end\":61911,\"start\":61907},{\"end\":61921,\"start\":61915},{\"end\":61932,\"start\":61925},{\"end\":61944,\"start\":61936},{\"end\":61953,\"start\":61948},{\"end\":62313,\"start\":62310},{\"end\":62323,\"start\":62319},{\"end\":62335,\"start\":62329},{\"end\":62345,\"start\":62341},{\"end\":62353,\"start\":62351},{\"end\":62364,\"start\":62359},{\"end\":62692,\"start\":62687},{\"end\":62698,\"start\":62696},{\"end\":62705,\"start\":62702},{\"end\":62713,\"start\":62709},{\"end\":62719,\"start\":62717},{\"end\":62728,\"start\":62725},{\"end\":63074,\"start\":63069},{\"end\":63083,\"start\":63078},{\"end\":63091,\"start\":63087},{\"end\":63100,\"start\":63095},{\"end\":63108,\"start\":63104},{\"end\":63490,\"start\":63488},{\"end\":63496,\"start\":63494},{\"end\":63504,\"start\":63500},{\"end\":63510,\"start\":63508},{\"end\":63768,\"start\":63761},{\"end\":63779,\"start\":63774},{\"end\":63794,\"start\":63785},{\"end\":63806,\"start\":63800},{\"end\":63819,\"start\":63810},{\"end\":63827,\"start\":63825},{\"end\":64209,\"start\":64205},{\"end\":64222,\"start\":64213},{\"end\":64233,\"start\":64226},{\"end\":64241,\"start\":64237},{\"end\":64250,\"start\":64245},{\"end\":64579,\"start\":64573},{\"end\":64588,\"start\":64583},{\"end\":64596,\"start\":64592},{\"end\":64607,\"start\":64600},{\"end\":65241,\"start\":65238},{\"end\":65250,\"start\":65247},{\"end\":65260,\"start\":65254},{\"end\":65270,\"start\":65266},{\"end\":65277,\"start\":65274},{\"end\":65286,\"start\":65281},{\"end\":65621,\"start\":65619},{\"end\":65631,\"start\":65625},{\"end\":65643,\"start\":65635},{\"end\":65652,\"start\":65647},{\"end\":65667,\"start\":65656},{\"end\":65673,\"start\":65671},{\"end\":65682,\"start\":65677},{\"end\":65690,\"start\":65686},{\"end\":65705,\"start\":65694},{\"end\":66010,\"start\":66008},{\"end\":66016,\"start\":66014},{\"end\":66024,\"start\":66022},{\"end\":66032,\"start\":66028},{\"end\":66044,\"start\":66036},{\"end\":66056,\"start\":66050},{\"end\":66319,\"start\":66316},{\"end\":66333,\"start\":66323},{\"end\":66345,\"start\":66337},{\"end\":66689,\"start\":66686},{\"end\":66698,\"start\":66693},{\"end\":66710,\"start\":66702},{\"end\":66718,\"start\":66714},{\"end\":66728,\"start\":66722},{\"end\":66739,\"start\":66732},{\"end\":66749,\"start\":66743},{\"end\":66762,\"start\":66755},{\"end\":67061,\"start\":67058},{\"end\":67071,\"start\":67065},{\"end\":67083,\"start\":67075},{\"end\":67089,\"start\":67087},{\"end\":67102,\"start\":67093},{\"end\":67114,\"start\":67106},{\"end\":67546,\"start\":67543},{\"end\":67552,\"start\":67550},{\"end\":67560,\"start\":67556},{\"end\":67566,\"start\":67564},{\"end\":67960,\"start\":67955},{\"end\":67973,\"start\":67966},{\"end\":68407,\"start\":68398},{\"end\":68418,\"start\":68411},{\"end\":68430,\"start\":68422},{\"end\":68441,\"start\":68434},{\"end\":68451,\"start\":68445},{\"end\":68864,\"start\":68858},{\"end\":68875,\"start\":68868},{\"end\":69071,\"start\":69064},{\"end\":69084,\"start\":69075},{\"end\":69096,\"start\":69088},{\"end\":69107,\"start\":69100},{\"end\":69117,\"start\":69111},{\"end\":69568,\"start\":69564},{\"end\":69581,\"start\":69572},{\"end\":69590,\"start\":69585},{\"end\":69597,\"start\":69594},{\"end\":70063,\"start\":70059},{\"end\":70070,\"start\":70067},{\"end\":70079,\"start\":70074},{\"end\":70087,\"start\":70083},{\"end\":70094,\"start\":70091},{\"end\":70522,\"start\":70520},{\"end\":70528,\"start\":70526},{\"end\":70534,\"start\":70532},{\"end\":70546,\"start\":70540},{\"end\":70985,\"start\":70983},{\"end\":70991,\"start\":70989},{\"end\":70997,\"start\":70995},{\"end\":71009,\"start\":71003},{\"end\":71387,\"start\":71384},{\"end\":71398,\"start\":71391},{\"end\":71815,\"start\":71812},{\"end\":71822,\"start\":71819},{\"end\":71832,\"start\":71826},{\"end\":71839,\"start\":71836},{\"end\":71845,\"start\":71843},{\"end\":71853,\"start\":71849},{\"end\":72344,\"start\":72334},{\"end\":72356,\"start\":72348},{\"end\":72366,\"start\":72360},{\"end\":72375,\"start\":72370},{\"end\":72687,\"start\":72680},{\"end\":72694,\"start\":72691},{\"end\":72705,\"start\":72698},{\"end\":72719,\"start\":72709},{\"end\":73014,\"start\":73005},{\"end\":73023,\"start\":73018},{\"end\":73032,\"start\":73027},{\"end\":73042,\"start\":73036},{\"end\":73339,\"start\":73335},{\"end\":73347,\"start\":73343},{\"end\":73356,\"start\":73351},{\"end\":73767,\"start\":73763},{\"end\":73784,\"start\":73773},{\"end\":73792,\"start\":73788},{\"end\":74195,\"start\":74193},{\"end\":74203,\"start\":74199},{\"end\":74218,\"start\":74207},{\"end\":74236,\"start\":74222},{\"end\":74632,\"start\":74629},{\"end\":74638,\"start\":74636},{\"end\":74647,\"start\":74642},{\"end\":74656,\"start\":74651},{\"end\":74665,\"start\":74660},{\"end\":74672,\"start\":74669},{\"end\":74687,\"start\":74678},{\"end\":74700,\"start\":74693},{\"end\":75088,\"start\":75085},{\"end\":75095,\"start\":75092},{\"end\":75103,\"start\":75099},{\"end\":75114,\"start\":75109},{\"end\":75365,\"start\":75358},{\"end\":75372,\"start\":75369},{\"end\":75379,\"start\":75376},{\"end\":75391,\"start\":75383},{\"end\":75399,\"start\":75395},{\"end\":75411,\"start\":75403},{\"end\":75420,\"start\":75415},{\"end\":75433,\"start\":75424},{\"end\":75447,\"start\":75437},{\"end\":75895,\"start\":75890},{\"end\":75906,\"start\":75899},{\"end\":76165,\"start\":76154},{\"end\":76178,\"start\":76171},{\"end\":76188,\"start\":76182},{\"end\":76194,\"start\":76192},{\"end\":76204,\"start\":76198},{\"end\":76212,\"start\":76208},{\"end\":76638,\"start\":76633},{\"end\":76649,\"start\":76644},{\"end\":76656,\"start\":76653},{\"end\":77084,\"start\":77076},{\"end\":77092,\"start\":77088},{\"end\":77103,\"start\":77098},{\"end\":77112,\"start\":77107},{\"end\":77601,\"start\":77599},{\"end\":77611,\"start\":77607},{\"end\":77620,\"start\":77617},{\"end\":77630,\"start\":77624},{\"end\":77641,\"start\":77636},{\"end\":78045,\"start\":78038},{\"end\":78056,\"start\":78049},{\"end\":78066,\"start\":78060},{\"end\":78079,\"start\":78070},{\"end\":78088,\"start\":78083},{\"end\":78099,\"start\":78094},{\"end\":78109,\"start\":78103},{\"end\":78123,\"start\":78113},{\"end\":78434,\"start\":78430},{\"end\":78443,\"start\":78438},{\"end\":78449,\"start\":78447},{\"end\":78455,\"start\":78453},{\"end\":78462,\"start\":78459},{\"end\":78473,\"start\":78468},{\"end\":78858,\"start\":78854},{\"end\":78864,\"start\":78862},{\"end\":78872,\"start\":78868},{\"end\":78879,\"start\":78876},{\"end\":78889,\"start\":78885},{\"end\":79237,\"start\":79233},{\"end\":79248,\"start\":79243},{\"end\":79260,\"start\":79254},{\"end\":79276,\"start\":79266},{\"end\":79554,\"start\":79552},{\"end\":79562,\"start\":79558},{\"end\":79569,\"start\":79566},{\"end\":79576,\"start\":79573},{\"end\":79587,\"start\":79580},{\"end\":79600,\"start\":79591},{\"end\":79894,\"start\":79892},{\"end\":79902,\"start\":79898},{\"end\":79912,\"start\":79906},{\"end\":79918,\"start\":79916},{\"end\":79927,\"start\":79922},{\"end\":79935,\"start\":79931},{\"end\":79943,\"start\":79939},{\"end\":80381,\"start\":80379},{\"end\":80390,\"start\":80385},{\"end\":80398,\"start\":80396},{\"end\":80405,\"start\":80402},{\"end\":80825,\"start\":80820},{\"end\":80837,\"start\":80829},{\"end\":80849,\"start\":80841},{\"end\":81166,\"start\":81161},{\"end\":81173,\"start\":81170},{\"end\":81181,\"start\":81177},{\"end\":81192,\"start\":81188},{\"end\":81198,\"start\":81196},{\"end\":81210,\"start\":81202},{\"end\":81220,\"start\":81214},{\"end\":81232,\"start\":81224},{\"end\":81597,\"start\":81592},{\"end\":81608,\"start\":81601},{\"end\":81621,\"start\":81612},{\"end\":81628,\"start\":81625},{\"end\":81909,\"start\":81905},{\"end\":81918,\"start\":81913},{\"end\":81930,\"start\":81922},{\"end\":82320,\"start\":82316},{\"end\":82333,\"start\":82326},{\"end\":82341,\"start\":82337},{\"end\":82353,\"start\":82345},{\"end\":82362,\"start\":82357},{\"end\":82650,\"start\":82647},{\"end\":82662,\"start\":82654},{\"end\":82672,\"start\":82666},{\"end\":82678,\"start\":82676},{\"end\":82684,\"start\":82682},{\"end\":83121,\"start\":83119},{\"end\":83129,\"start\":83125},{\"end\":83139,\"start\":83133},{\"end\":83147,\"start\":83143},{\"end\":83158,\"start\":83151},{\"end\":83493,\"start\":83489},{\"end\":83500,\"start\":83497},{\"end\":83512,\"start\":83509},{\"end\":83520,\"start\":83516},{\"end\":83942,\"start\":83938},{\"end\":83948,\"start\":83946},{\"end\":83956,\"start\":83952},{\"end\":83962,\"start\":83960},{\"end\":83970,\"start\":83966},{\"end\":83978,\"start\":83974},{\"end\":83988,\"start\":83984},{\"end\":84477,\"start\":84472},{\"end\":84487,\"start\":84483},{\"end\":84498,\"start\":84493},{\"end\":84506,\"start\":84502},{\"end\":84667,\"start\":84662},{\"end\":84676,\"start\":84671},{\"end\":84682,\"start\":84680},{\"end\":84690,\"start\":84686},{\"end\":84697,\"start\":84694},{\"end\":84705,\"start\":84701},{\"end\":85013,\"start\":85009},{\"end\":85021,\"start\":85017},{\"end\":85029,\"start\":85025},{\"end\":85039,\"start\":85033},{\"end\":85051,\"start\":85043},{\"end\":85063,\"start\":85055},{\"end\":85456,\"start\":85453},{\"end\":85465,\"start\":85460},{\"end\":85475,\"start\":85469},{\"end\":85486,\"start\":85479},{\"end\":85497,\"start\":85492},{\"end\":85505,\"start\":85501},{\"end\":85518,\"start\":85509}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13989517},\"end\":51277,\"start\":50747},{\"attributes\":{\"id\":\"b1\"},\"end\":51510,\"start\":51279},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3345396},\"end\":51940,\"start\":51512},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":28660094},\"end\":52239,\"start\":51942},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16080844},\"end\":52615,\"start\":52241},{\"attributes\":{\"doi\":\"arXiv:170906158\",\"id\":\"b5\",\"matched_paper_id\":195345409},\"end\":53301,\"start\":52617},{\"attributes\":{\"doi\":\"arXiv:151203012\",\"id\":\"b6\"},\"end\":53671,\"start\":53303},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":146120595},\"end\":54295,\"start\":53673},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":199442423},\"end\":54722,\"start\":54297},{\"attributes\":{\"doi\":\"arXiv:160202481\",\"id\":\"b9\"},\"end\":54930,\"start\":54724},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6325059},\"end\":55368,\"start\":54932},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10831118},\"end\":55720,\"start\":55370},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7684883},\"end\":56213,\"start\":55722},{\"attributes\":{\"doi\":\"arXiv:181004805\",\"id\":\"b13\"},\"end\":56525,\"start\":56215},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":61066084},\"end\":56857,\"start\":56527},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15006503},\"end\":57169,\"start\":56859},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6746759},\"end\":57622,\"start\":57171},{\"attributes\":{\"doi\":\"arXiv:170310277\",\"id\":\"b17\"},\"end\":57945,\"start\":57624},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":107084},\"end\":58224,\"start\":57947},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53856890},\"end\":58622,\"start\":58226},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":39450643},\"end\":58832,\"start\":58624},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6724907},\"end\":59196,\"start\":58834},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6598262},\"end\":59551,\"start\":59198},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4495230},\"end\":59994,\"start\":59553},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":199501870},\"end\":60335,\"start\":59996},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":208028203},\"end\":60781,\"start\":60337},{\"attributes\":{\"id\":\"b26\"},\"end\":61034,\"start\":60783},{\"attributes\":{\"id\":\"b27\"},\"end\":61350,\"start\":61036},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":326772},\"end\":61766,\"start\":61352},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":42441056},\"end\":62254,\"start\":61768},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11245438},\"end\":62613,\"start\":62256},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":51937125},\"end\":63045,\"start\":62615},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":67856179},\"end\":63415,\"start\":63047},{\"attributes\":{\"id\":\"b33\"},\"end\":63685,\"start\":63417},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12003435},\"end\":64116,\"start\":63687},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2123767},\"end\":64504,\"start\":64118},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":195908774},\"end\":65163,\"start\":64506},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52987155},\"end\":65539,\"start\":65165},{\"attributes\":{\"doi\":\"arXiv:180900716\",\"id\":\"b38\"},\"end\":65938,\"start\":65541},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":4311592},\"end\":66266,\"start\":65940},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":152322},\"end\":66637,\"start\":66268},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14113767},\"end\":67005,\"start\":66639},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":10716717},\"end\":67466,\"start\":67007},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":102484000},\"end\":67880,\"start\":67468},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":215826927},\"end\":68328,\"start\":67882},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":54465161},\"end\":68818,\"start\":68330},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":37255712},\"end\":68992,\"start\":68820},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":158046789},\"end\":69456,\"start\":68994},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":208527729},\"end\":69996,\"start\":69458},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":57189382},\"end\":70436,\"start\":69998},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":5115938},\"end\":70897,\"start\":70438},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":1745976},\"end\":71246,\"start\":70899},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4392433},\"end\":71764,\"start\":71248},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":198910141},\"end\":72212,\"start\":71766},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":1330784},\"end\":72645,\"start\":72214},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":7156990},\"end\":72941,\"start\":72647},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":545361},\"end\":73263,\"start\":72943},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":210023370},\"end\":73703,\"start\":73265},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":6242669},\"end\":74122,\"start\":73705},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":2407217},\"end\":74562,\"start\":74124},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":4794860},\"end\":75081,\"start\":74564},{\"attributes\":{\"doi\":\"arXiv:180406375\",\"id\":\"b61\"},\"end\":75322,\"start\":75083},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":206592484},\"end\":75810,\"start\":75324},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":1232587},\"end\":76095,\"start\":75812},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":115147778},\"end\":76571,\"start\":76097},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":4929149},\"end\":76982,\"start\":76573},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":73431591},\"end\":77485,\"start\":76984},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":199552106},\"end\":78007,\"start\":77487},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":13756489},\"end\":78364,\"start\":78009},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":4633214},\"end\":78783,\"start\":78366},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":4538305},\"end\":79155,\"start\":78785},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":207761262},\"end\":79495,\"start\":79157},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":3491267},\"end\":79829,\"start\":79497},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":206592833},\"end\":80301,\"start\":79831},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":4591284},\"end\":80752,\"start\":80303},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":11266650},\"end\":81094,\"start\":80754},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":10917359},\"end\":81497,\"start\":81096},{\"attributes\":{\"doi\":\"arXiv:171100199\",\"id\":\"b77\"},\"end\":81826,\"start\":81499},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":6033252},\"end\":82248,\"start\":81828},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":10224573},\"end\":82581,\"start\":82250},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":8485068},\"end\":83031,\"start\":82583},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":166228177},\"end\":83409,\"start\":83033},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":2510693},\"end\":83881,\"start\":83411},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":197928478},\"end\":84438,\"start\":83883},{\"attributes\":{\"id\":\"b84\"},\"end\":84658,\"start\":84440},{\"attributes\":{\"doi\":\"arXiv:190800222\",\"id\":\"b85\"},\"end\":84967,\"start\":84660},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":5636055},\"end\":85401,\"start\":84969},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":19046372},\"end\":85757,\"start\":85403}]", "bib_title": "[{\"end\":50836,\"start\":50747},{\"end\":51360,\"start\":51279},{\"end\":51573,\"start\":51512},{\"end\":52010,\"start\":51942},{\"end\":52303,\"start\":52241},{\"end\":52687,\"start\":52617},{\"end\":53755,\"start\":53673},{\"end\":54379,\"start\":54297},{\"end\":55010,\"start\":54932},{\"end\":55446,\"start\":55370},{\"end\":55783,\"start\":55722},{\"end\":56617,\"start\":56527},{\"end\":56929,\"start\":56859},{\"end\":57250,\"start\":57171},{\"end\":57989,\"start\":57947},{\"end\":58282,\"start\":58226},{\"end\":58682,\"start\":58624},{\"end\":58903,\"start\":58834},{\"end\":59269,\"start\":59198},{\"end\":59620,\"start\":59553},{\"end\":60067,\"start\":59996},{\"end\":60404,\"start\":60337},{\"end\":61433,\"start\":61352},{\"end\":61874,\"start\":61768},{\"end\":62304,\"start\":62256},{\"end\":62683,\"start\":62615},{\"end\":63065,\"start\":63047},{\"end\":63757,\"start\":63687},{\"end\":64201,\"start\":64118},{\"end\":64569,\"start\":64506},{\"end\":65234,\"start\":65165},{\"end\":66004,\"start\":65940},{\"end\":66310,\"start\":66268},{\"end\":66680,\"start\":66639},{\"end\":67052,\"start\":67007},{\"end\":67539,\"start\":67468},{\"end\":67951,\"start\":67882},{\"end\":68394,\"start\":68330},{\"end\":68854,\"start\":68820},{\"end\":69060,\"start\":68994},{\"end\":69560,\"start\":69458},{\"end\":70055,\"start\":69998},{\"end\":70514,\"start\":70438},{\"end\":70977,\"start\":70899},{\"end\":71380,\"start\":71248},{\"end\":71808,\"start\":71766},{\"end\":72330,\"start\":72214},{\"end\":72676,\"start\":72647},{\"end\":73001,\"start\":72943},{\"end\":73331,\"start\":73265},{\"end\":73759,\"start\":73705},{\"end\":74189,\"start\":74124},{\"end\":74625,\"start\":74564},{\"end\":75354,\"start\":75324},{\"end\":75884,\"start\":75812},{\"end\":76150,\"start\":76097},{\"end\":76629,\"start\":76573},{\"end\":77072,\"start\":76984},{\"end\":77593,\"start\":77487},{\"end\":78034,\"start\":78009},{\"end\":78426,\"start\":78366},{\"end\":78850,\"start\":78785},{\"end\":79229,\"start\":79157},{\"end\":79548,\"start\":79497},{\"end\":79888,\"start\":79831},{\"end\":80375,\"start\":80303},{\"end\":80816,\"start\":80754},{\"end\":81157,\"start\":81096},{\"end\":81901,\"start\":81828},{\"end\":82312,\"start\":82250},{\"end\":82643,\"start\":82583},{\"end\":83115,\"start\":83033},{\"end\":83485,\"start\":83411},{\"end\":83934,\"start\":83883},{\"end\":85005,\"start\":84969},{\"end\":85447,\"start\":85403}]", "bib_author": "[{\"end\":50847,\"start\":50838},{\"end\":50859,\"start\":50847},{\"end\":50870,\"start\":50859},{\"end\":50883,\"start\":50870},{\"end\":50892,\"start\":50883},{\"end\":51375,\"start\":51362},{\"end\":51585,\"start\":51575},{\"end\":51596,\"start\":51585},{\"end\":51605,\"start\":51596},{\"end\":52018,\"start\":52012},{\"end\":52025,\"start\":52018},{\"end\":52032,\"start\":52025},{\"end\":52318,\"start\":52305},{\"end\":52327,\"start\":52318},{\"end\":52337,\"start\":52327},{\"end\":52348,\"start\":52337},{\"end\":52359,\"start\":52348},{\"end\":52369,\"start\":52359},{\"end\":52696,\"start\":52689},{\"end\":52713,\"start\":52696},{\"end\":52722,\"start\":52713},{\"end\":52729,\"start\":52722},{\"end\":52743,\"start\":52729},{\"end\":52753,\"start\":52743},{\"end\":52765,\"start\":52753},{\"end\":52774,\"start\":52765},{\"end\":52782,\"start\":52774},{\"end\":52790,\"start\":52782},{\"end\":52799,\"start\":52790},{\"end\":53314,\"start\":53303},{\"end\":53328,\"start\":53314},{\"end\":53338,\"start\":53328},{\"end\":53350,\"start\":53338},{\"end\":53359,\"start\":53350},{\"end\":53365,\"start\":53359},{\"end\":53377,\"start\":53365},{\"end\":53386,\"start\":53377},{\"end\":53394,\"start\":53386},{\"end\":53400,\"start\":53394},{\"end\":53765,\"start\":53757},{\"end\":53774,\"start\":53765},{\"end\":53780,\"start\":53774},{\"end\":53787,\"start\":53780},{\"end\":53794,\"start\":53787},{\"end\":53801,\"start\":53794},{\"end\":53807,\"start\":53801},{\"end\":53818,\"start\":53807},{\"end\":53826,\"start\":53818},{\"end\":53834,\"start\":53826},{\"end\":54389,\"start\":54381},{\"end\":54397,\"start\":54389},{\"end\":54404,\"start\":54397},{\"end\":54413,\"start\":54404},{\"end\":54425,\"start\":54413},{\"end\":54437,\"start\":54425},{\"end\":54447,\"start\":54437},{\"end\":54732,\"start\":54724},{\"end\":54742,\"start\":54732},{\"end\":54752,\"start\":54742},{\"end\":54762,\"start\":54752},{\"end\":55022,\"start\":55012},{\"end\":55028,\"start\":55022},{\"end\":55036,\"start\":55028},{\"end\":55044,\"start\":55036},{\"end\":55056,\"start\":55044},{\"end\":55458,\"start\":55448},{\"end\":55470,\"start\":55458},{\"end\":55485,\"start\":55470},{\"end\":55792,\"start\":55785},{\"end\":55803,\"start\":55792},{\"end\":55812,\"start\":55803},{\"end\":55822,\"start\":55812},{\"end\":55836,\"start\":55822},{\"end\":55847,\"start\":55836},{\"end\":56225,\"start\":56215},{\"end\":56236,\"start\":56225},{\"end\":56243,\"start\":56236},{\"end\":56256,\"start\":56243},{\"end\":56635,\"start\":56619},{\"end\":56646,\"start\":56635},{\"end\":56657,\"start\":56646},{\"end\":56942,\"start\":56931},{\"end\":56953,\"start\":56942},{\"end\":56963,\"start\":56953},{\"end\":57259,\"start\":57252},{\"end\":57265,\"start\":57259},{\"end\":57277,\"start\":57265},{\"end\":57690,\"start\":57681},{\"end\":57699,\"start\":57690},{\"end\":57709,\"start\":57699},{\"end\":57717,\"start\":57709},{\"end\":57727,\"start\":57717},{\"end\":57741,\"start\":57727},{\"end\":57753,\"start\":57741},{\"end\":58001,\"start\":57991},{\"end\":58011,\"start\":58001},{\"end\":58292,\"start\":58284},{\"end\":58300,\"start\":58292},{\"end\":58307,\"start\":58300},{\"end\":58315,\"start\":58307},{\"end\":58322,\"start\":58315},{\"end\":58698,\"start\":58684},{\"end\":58915,\"start\":58905},{\"end\":58923,\"start\":58915},{\"end\":58934,\"start\":58923},{\"end\":59282,\"start\":59271},{\"end\":59294,\"start\":59282},{\"end\":59307,\"start\":59294},{\"end\":59316,\"start\":59307},{\"end\":59633,\"start\":59622},{\"end\":59643,\"start\":59633},{\"end\":59654,\"start\":59643},{\"end\":60080,\"start\":60069},{\"end\":60090,\"start\":60080},{\"end\":60101,\"start\":60090},{\"end\":60417,\"start\":60406},{\"end\":60427,\"start\":60417},{\"end\":60436,\"start\":60427},{\"end\":60447,\"start\":60436},{\"end\":60457,\"start\":60447},{\"end\":60796,\"start\":60785},{\"end\":60805,\"start\":60796},{\"end\":60813,\"start\":60805},{\"end\":60823,\"start\":60813},{\"end\":60836,\"start\":60823},{\"end\":60848,\"start\":60836},{\"end\":61042,\"start\":61036},{\"end\":61054,\"start\":61042},{\"end\":61064,\"start\":61054},{\"end\":61076,\"start\":61064},{\"end\":61445,\"start\":61435},{\"end\":61457,\"start\":61445},{\"end\":61472,\"start\":61457},{\"end\":61483,\"start\":61472},{\"end\":61497,\"start\":61483},{\"end\":61894,\"start\":61876},{\"end\":61905,\"start\":61894},{\"end\":61913,\"start\":61905},{\"end\":61923,\"start\":61913},{\"end\":61934,\"start\":61923},{\"end\":61946,\"start\":61934},{\"end\":61955,\"start\":61946},{\"end\":62315,\"start\":62306},{\"end\":62325,\"start\":62315},{\"end\":62337,\"start\":62325},{\"end\":62347,\"start\":62337},{\"end\":62355,\"start\":62347},{\"end\":62366,\"start\":62355},{\"end\":62694,\"start\":62685},{\"end\":62700,\"start\":62694},{\"end\":62707,\"start\":62700},{\"end\":62715,\"start\":62707},{\"end\":62721,\"start\":62715},{\"end\":62730,\"start\":62721},{\"end\":63076,\"start\":63067},{\"end\":63085,\"start\":63076},{\"end\":63093,\"start\":63085},{\"end\":63102,\"start\":63093},{\"end\":63110,\"start\":63102},{\"end\":63492,\"start\":63486},{\"end\":63498,\"start\":63492},{\"end\":63506,\"start\":63498},{\"end\":63512,\"start\":63506},{\"end\":63770,\"start\":63759},{\"end\":63781,\"start\":63770},{\"end\":63796,\"start\":63781},{\"end\":63808,\"start\":63796},{\"end\":63821,\"start\":63808},{\"end\":63829,\"start\":63821},{\"end\":64211,\"start\":64203},{\"end\":64224,\"start\":64211},{\"end\":64235,\"start\":64224},{\"end\":64243,\"start\":64235},{\"end\":64252,\"start\":64243},{\"end\":64581,\"start\":64571},{\"end\":64590,\"start\":64581},{\"end\":64598,\"start\":64590},{\"end\":64609,\"start\":64598},{\"end\":65243,\"start\":65236},{\"end\":65252,\"start\":65243},{\"end\":65262,\"start\":65252},{\"end\":65272,\"start\":65262},{\"end\":65279,\"start\":65272},{\"end\":65288,\"start\":65279},{\"end\":65623,\"start\":65617},{\"end\":65633,\"start\":65623},{\"end\":65645,\"start\":65633},{\"end\":65654,\"start\":65645},{\"end\":65669,\"start\":65654},{\"end\":65675,\"start\":65669},{\"end\":65684,\"start\":65675},{\"end\":65692,\"start\":65684},{\"end\":65707,\"start\":65692},{\"end\":66012,\"start\":66006},{\"end\":66018,\"start\":66012},{\"end\":66026,\"start\":66018},{\"end\":66034,\"start\":66026},{\"end\":66046,\"start\":66034},{\"end\":66058,\"start\":66046},{\"end\":66321,\"start\":66312},{\"end\":66335,\"start\":66321},{\"end\":66347,\"start\":66335},{\"end\":66691,\"start\":66682},{\"end\":66700,\"start\":66691},{\"end\":66712,\"start\":66700},{\"end\":66720,\"start\":66712},{\"end\":66730,\"start\":66720},{\"end\":66741,\"start\":66730},{\"end\":66751,\"start\":66741},{\"end\":66764,\"start\":66751},{\"end\":67063,\"start\":67054},{\"end\":67073,\"start\":67063},{\"end\":67085,\"start\":67073},{\"end\":67091,\"start\":67085},{\"end\":67104,\"start\":67091},{\"end\":67116,\"start\":67104},{\"end\":67548,\"start\":67541},{\"end\":67554,\"start\":67548},{\"end\":67562,\"start\":67554},{\"end\":67568,\"start\":67562},{\"end\":67962,\"start\":67953},{\"end\":67975,\"start\":67962},{\"end\":67985,\"start\":67975},{\"end\":68409,\"start\":68396},{\"end\":68420,\"start\":68409},{\"end\":68432,\"start\":68420},{\"end\":68443,\"start\":68432},{\"end\":68453,\"start\":68443},{\"end\":68866,\"start\":68856},{\"end\":68877,\"start\":68866},{\"end\":69073,\"start\":69062},{\"end\":69086,\"start\":69073},{\"end\":69098,\"start\":69086},{\"end\":69109,\"start\":69098},{\"end\":69119,\"start\":69109},{\"end\":69570,\"start\":69562},{\"end\":69583,\"start\":69570},{\"end\":69592,\"start\":69583},{\"end\":69599,\"start\":69592},{\"end\":70065,\"start\":70057},{\"end\":70072,\"start\":70065},{\"end\":70081,\"start\":70072},{\"end\":70089,\"start\":70081},{\"end\":70096,\"start\":70089},{\"end\":70524,\"start\":70516},{\"end\":70530,\"start\":70524},{\"end\":70536,\"start\":70530},{\"end\":70548,\"start\":70536},{\"end\":70987,\"start\":70979},{\"end\":70993,\"start\":70987},{\"end\":70999,\"start\":70993},{\"end\":71011,\"start\":70999},{\"end\":71389,\"start\":71382},{\"end\":71400,\"start\":71389},{\"end\":71817,\"start\":71810},{\"end\":71824,\"start\":71817},{\"end\":71834,\"start\":71824},{\"end\":71841,\"start\":71834},{\"end\":71847,\"start\":71841},{\"end\":71855,\"start\":71847},{\"end\":72346,\"start\":72332},{\"end\":72358,\"start\":72346},{\"end\":72368,\"start\":72358},{\"end\":72377,\"start\":72368},{\"end\":72689,\"start\":72678},{\"end\":72696,\"start\":72689},{\"end\":72707,\"start\":72696},{\"end\":72721,\"start\":72707},{\"end\":73016,\"start\":73003},{\"end\":73025,\"start\":73016},{\"end\":73034,\"start\":73025},{\"end\":73044,\"start\":73034},{\"end\":73341,\"start\":73333},{\"end\":73349,\"start\":73341},{\"end\":73358,\"start\":73349},{\"end\":73769,\"start\":73761},{\"end\":73786,\"start\":73769},{\"end\":73794,\"start\":73786},{\"end\":74197,\"start\":74191},{\"end\":74205,\"start\":74197},{\"end\":74220,\"start\":74205},{\"end\":74238,\"start\":74220},{\"end\":74634,\"start\":74627},{\"end\":74640,\"start\":74634},{\"end\":74649,\"start\":74640},{\"end\":74658,\"start\":74649},{\"end\":74667,\"start\":74658},{\"end\":74674,\"start\":74667},{\"end\":74689,\"start\":74674},{\"end\":74702,\"start\":74689},{\"end\":75090,\"start\":75083},{\"end\":75097,\"start\":75090},{\"end\":75105,\"start\":75097},{\"end\":75116,\"start\":75105},{\"end\":75367,\"start\":75356},{\"end\":75374,\"start\":75367},{\"end\":75381,\"start\":75374},{\"end\":75393,\"start\":75381},{\"end\":75401,\"start\":75393},{\"end\":75413,\"start\":75401},{\"end\":75422,\"start\":75413},{\"end\":75435,\"start\":75422},{\"end\":75449,\"start\":75435},{\"end\":75897,\"start\":75886},{\"end\":75908,\"start\":75897},{\"end\":76167,\"start\":76152},{\"end\":76180,\"start\":76167},{\"end\":76190,\"start\":76180},{\"end\":76196,\"start\":76190},{\"end\":76206,\"start\":76196},{\"end\":76214,\"start\":76206},{\"end\":76640,\"start\":76631},{\"end\":76651,\"start\":76640},{\"end\":76658,\"start\":76651},{\"end\":77086,\"start\":77074},{\"end\":77094,\"start\":77086},{\"end\":77105,\"start\":77094},{\"end\":77114,\"start\":77105},{\"end\":77603,\"start\":77595},{\"end\":77613,\"start\":77603},{\"end\":77622,\"start\":77613},{\"end\":77632,\"start\":77622},{\"end\":77643,\"start\":77632},{\"end\":78047,\"start\":78036},{\"end\":78058,\"start\":78047},{\"end\":78068,\"start\":78058},{\"end\":78081,\"start\":78068},{\"end\":78090,\"start\":78081},{\"end\":78101,\"start\":78090},{\"end\":78111,\"start\":78101},{\"end\":78125,\"start\":78111},{\"end\":78436,\"start\":78428},{\"end\":78445,\"start\":78436},{\"end\":78451,\"start\":78445},{\"end\":78457,\"start\":78451},{\"end\":78464,\"start\":78457},{\"end\":78475,\"start\":78464},{\"end\":78860,\"start\":78852},{\"end\":78866,\"start\":78860},{\"end\":78874,\"start\":78866},{\"end\":78881,\"start\":78874},{\"end\":78891,\"start\":78881},{\"end\":79239,\"start\":79231},{\"end\":79250,\"start\":79239},{\"end\":79262,\"start\":79250},{\"end\":79278,\"start\":79262},{\"end\":79556,\"start\":79550},{\"end\":79564,\"start\":79556},{\"end\":79571,\"start\":79564},{\"end\":79578,\"start\":79571},{\"end\":79589,\"start\":79578},{\"end\":79602,\"start\":79589},{\"end\":79896,\"start\":79890},{\"end\":79904,\"start\":79896},{\"end\":79914,\"start\":79904},{\"end\":79920,\"start\":79914},{\"end\":79929,\"start\":79920},{\"end\":79937,\"start\":79929},{\"end\":79945,\"start\":79937},{\"end\":80383,\"start\":80377},{\"end\":80392,\"start\":80383},{\"end\":80400,\"start\":80392},{\"end\":80407,\"start\":80400},{\"end\":80827,\"start\":80818},{\"end\":80839,\"start\":80827},{\"end\":80851,\"start\":80839},{\"end\":81168,\"start\":81159},{\"end\":81175,\"start\":81168},{\"end\":81183,\"start\":81175},{\"end\":81194,\"start\":81183},{\"end\":81200,\"start\":81194},{\"end\":81212,\"start\":81200},{\"end\":81222,\"start\":81212},{\"end\":81234,\"start\":81222},{\"end\":81238,\"start\":81234},{\"end\":81599,\"start\":81590},{\"end\":81610,\"start\":81599},{\"end\":81623,\"start\":81610},{\"end\":81630,\"start\":81623},{\"end\":81911,\"start\":81903},{\"end\":81920,\"start\":81911},{\"end\":81932,\"start\":81920},{\"end\":82322,\"start\":82314},{\"end\":82335,\"start\":82322},{\"end\":82343,\"start\":82335},{\"end\":82355,\"start\":82343},{\"end\":82364,\"start\":82355},{\"end\":82652,\"start\":82645},{\"end\":82664,\"start\":82652},{\"end\":82674,\"start\":82664},{\"end\":82680,\"start\":82674},{\"end\":82686,\"start\":82680},{\"end\":83123,\"start\":83117},{\"end\":83131,\"start\":83123},{\"end\":83141,\"start\":83131},{\"end\":83149,\"start\":83141},{\"end\":83160,\"start\":83149},{\"end\":83495,\"start\":83487},{\"end\":83502,\"start\":83495},{\"end\":83514,\"start\":83502},{\"end\":83522,\"start\":83514},{\"end\":83526,\"start\":83522},{\"end\":83944,\"start\":83936},{\"end\":83950,\"start\":83944},{\"end\":83958,\"start\":83950},{\"end\":83964,\"start\":83958},{\"end\":83972,\"start\":83964},{\"end\":83980,\"start\":83972},{\"end\":83990,\"start\":83980},{\"end\":84479,\"start\":84470},{\"end\":84489,\"start\":84479},{\"end\":84500,\"start\":84489},{\"end\":84508,\"start\":84500},{\"end\":84669,\"start\":84660},{\"end\":84678,\"start\":84669},{\"end\":84684,\"start\":84678},{\"end\":84692,\"start\":84684},{\"end\":84699,\"start\":84692},{\"end\":84707,\"start\":84699},{\"end\":85015,\"start\":85007},{\"end\":85023,\"start\":85015},{\"end\":85031,\"start\":85023},{\"end\":85041,\"start\":85031},{\"end\":85053,\"start\":85041},{\"end\":85065,\"start\":85053},{\"end\":85458,\"start\":85449},{\"end\":85467,\"start\":85458},{\"end\":85477,\"start\":85467},{\"end\":85488,\"start\":85477},{\"end\":85499,\"start\":85488},{\"end\":85507,\"start\":85499},{\"end\":85520,\"start\":85507}]", "bib_venue": "[{\"end\":51033,\"start\":50971},{\"end\":51746,\"start\":51684},{\"end\":54013,\"start\":53932},{\"end\":55171,\"start\":55122},{\"end\":55988,\"start\":55926},{\"end\":57418,\"start\":57356},{\"end\":58431,\"start\":58385},{\"end\":59795,\"start\":59733},{\"end\":60577,\"start\":60524},{\"end\":61197,\"start\":61145},{\"end\":62845,\"start\":62796},{\"end\":63251,\"start\":63189},{\"end\":64733,\"start\":64679},{\"end\":66468,\"start\":66416},{\"end\":67257,\"start\":67195},{\"end\":67689,\"start\":67637},{\"end\":68126,\"start\":68064},{\"end\":68594,\"start\":68532},{\"end\":69240,\"start\":69188},{\"end\":69748,\"start\":69682},{\"end\":70237,\"start\":70175},{\"end\":70689,\"start\":70627},{\"end\":71521,\"start\":71469},{\"end\":72016,\"start\":71944},{\"end\":72789,\"start\":72762},{\"end\":73507,\"start\":73441},{\"end\":73935,\"start\":73873},{\"end\":74359,\"start\":74307},{\"end\":74843,\"start\":74781},{\"end\":75590,\"start\":75528},{\"end\":76355,\"start\":76293},{\"end\":76799,\"start\":76737},{\"end\":77255,\"start\":77193},{\"end\":77764,\"start\":77712},{\"end\":78590,\"start\":78541},{\"end\":78976,\"start\":78942},{\"end\":80086,\"start\":80024},{\"end\":80548,\"start\":80486},{\"end\":82053,\"start\":82001},{\"end\":82827,\"start\":82765},{\"end\":83667,\"start\":83605},{\"end\":84199,\"start\":84103},{\"end\":85206,\"start\":85144},{\"end\":50969,\"start\":50892},{\"end\":51384,\"start\":51375},{\"end\":51682,\"start\":51605},{\"end\":52078,\"start\":52032},{\"end\":52407,\"start\":52369},{\"end\":52876,\"start\":52814},{\"end\":53464,\"start\":53415},{\"end\":53930,\"start\":53834},{\"end\":54496,\"start\":54447},{\"end\":54808,\"start\":54777},{\"end\":55120,\"start\":55056},{\"end\":55531,\"start\":55485},{\"end\":55924,\"start\":55847},{\"end\":56351,\"start\":56271},{\"end\":56682,\"start\":56657},{\"end\":57002,\"start\":56963},{\"end\":57354,\"start\":57277},{\"end\":57679,\"start\":57624},{\"end\":58073,\"start\":58011},{\"end\":58383,\"start\":58322},{\"end\":58718,\"start\":58698},{\"end\":58997,\"start\":58934},{\"end\":59354,\"start\":59316},{\"end\":59731,\"start\":59654},{\"end\":60149,\"start\":60101},{\"end\":60522,\"start\":60457},{\"end\":60882,\"start\":60848},{\"end\":61143,\"start\":61076},{\"end\":61546,\"start\":61497},{\"end\":61990,\"start\":61955},{\"end\":62421,\"start\":62366},{\"end\":62794,\"start\":62730},{\"end\":63187,\"start\":63110},{\"end\":63484,\"start\":63417},{\"end\":63891,\"start\":63829},{\"end\":64290,\"start\":64252},{\"end\":64677,\"start\":64609},{\"end\":65336,\"start\":65288},{\"end\":65615,\"start\":65541},{\"end\":66092,\"start\":66058},{\"end\":66414,\"start\":66347},{\"end\":66802,\"start\":66764},{\"end\":67193,\"start\":67116},{\"end\":67635,\"start\":67568},{\"end\":68062,\"start\":67985},{\"end\":68530,\"start\":68453},{\"end\":68894,\"start\":68877},{\"end\":69186,\"start\":69119},{\"end\":69680,\"start\":69599},{\"end\":70173,\"start\":70096},{\"end\":70625,\"start\":70548},{\"end\":71060,\"start\":71011},{\"end\":71467,\"start\":71400},{\"end\":71942,\"start\":71855},{\"end\":72417,\"start\":72377},{\"end\":72760,\"start\":72721},{\"end\":73082,\"start\":73044},{\"end\":73439,\"start\":73358},{\"end\":73871,\"start\":73794},{\"end\":74305,\"start\":74238},{\"end\":74779,\"start\":74702},{\"end\":75177,\"start\":75131},{\"end\":75526,\"start\":75449},{\"end\":75942,\"start\":75908},{\"end\":76291,\"start\":76214},{\"end\":76735,\"start\":76658},{\"end\":77191,\"start\":77114},{\"end\":77710,\"start\":77643},{\"end\":78174,\"start\":78125},{\"end\":78539,\"start\":78475},{\"end\":78940,\"start\":78891},{\"end\":79315,\"start\":79278},{\"end\":79651,\"start\":79602},{\"end\":80022,\"start\":79945},{\"end\":80484,\"start\":80407},{\"end\":80908,\"start\":80851},{\"end\":81276,\"start\":81238},{\"end\":81588,\"start\":81499},{\"end\":81999,\"start\":81932},{\"end\":82404,\"start\":82364},{\"end\":82763,\"start\":82686},{\"end\":83209,\"start\":83160},{\"end\":83603,\"start\":83526},{\"end\":84101,\"start\":83990},{\"end\":84468,\"start\":84440},{\"end\":84794,\"start\":84722},{\"end\":85142,\"start\":85065},{\"end\":85569,\"start\":85520}]"}}}, "year": 2023, "month": 12, "day": 17}