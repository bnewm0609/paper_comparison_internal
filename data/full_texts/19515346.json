{"id": 19515346, "updated": "2023-09-28 20:57:57.579", "metadata": {"title": "Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty", "authors": "[{\"first\":\"Apratim\",\"last\":\"Bhattacharyya\",\"middle\":[]},{\"first\":\"Mario\",\"last\":\"Fritz\",\"middle\":[]},{\"first\":\"Bernt\",\"last\":\"Schiele\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 11, "day": 24}, "abstract": "Progress towards advanced systems for assisted and autonomous driving is leveraging recent advances in recognition and segmentation methods. Yet, we are still facing challenges in bringing reliable driving to inner cities, as those are composed of highly dynamic scenes observed from a moving platform at considerable speeds. Anticipation becomes a key element in order to react timely and prevent accidents. In this paper we argue that it is necessary to predict at least 1 second and we thus propose a new model that jointly predicts ego motion and people trajectories over such large time horizons. We pay particular attention to modeling the uncertainty of our estimates arising from the non-deterministic nature of natural traffic scenes. Our experimental results show that it is indeed possible to predict people trajectories at the desired time horizons and that our uncertainty estimates are informative of the prediction error. We also show that both sequence modeling of trajectories as well as our novel method of long term odometry prediction are essential for best performance.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1711.09026", "mag": "2950014596", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/BhattacharyyaFS18", "doi": "10.1109/cvpr.2018.00441"}}, "content": {"source": {"pdf_hash": "5b99d066d1a97cfe68da9e2ce2c2b1ffe4a9b8fe", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1711.09026v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1711.09026", "status": "GREEN"}}, "grobid": {"id": "67eecc46f4e4821ad9b7617ff145fc572882762f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5b99d066d1a97cfe68da9e2ce2c2b1ffe4a9b8fe.txt", "contents": "\nLong-Term On-Board Prediction of People in Traffic Scenes under Uncertainty\n\n\nApratim Bhattacharyya \nMax Planck Institute for Informatics\nSaarland Informatics Campus\nSaarbr\u00fcckenGermany\n\nMario Fritz mfritz@mpi-inf.mpg.de \nMax Planck Institute for Informatics\nSaarland Informatics Campus\nSaarbr\u00fcckenGermany\n\nBernt Schiele schiele@mpi-inf.mpg.de \nMax Planck Institute for Informatics\nSaarland Informatics Campus\nSaarbr\u00fcckenGermany\n\nLong-Term On-Board Prediction of People in Traffic Scenes under Uncertainty\nLast Observation: t Prediction: t + 5 Prediction: t + 10 Prediction: t + 15 Probability Figure 1: Our predictive distribution upto t + 15 frames. The heat map encodes the probability of a certain pixel belonging to the person. The variance of the distribution encodes the uncertainty. Row 1: Low uncertainty. Row 2: High uncertainty.\nProgress towards advanced systems for assisted and autonomous driving is leveraging recent advances in recognition and segmentation methods. Yet, we are still facing challenges in bringing reliable driving to inner cities, as those are composed of highly dynamic scenes observed from a moving platform at considerable speeds. Anticipation becomes a key element in order to react timely and prevent accidents. In this paper we argue that it is necessary to predict at least 1 second and we thus propose a new model that jointly predicts ego motion and people trajectories over such large time horizons. We pay particular attention to modeling the uncertainty of our estimates arising from the non-deterministic nature of natural traffic scenes. Our experimental results show that it is indeed possible to predict people trajectories at the desired time horizons and that our uncertainty estimates are informative of the prediction error. We also show that both sequence modeling of trajectories as well as our novel method of long term odometry prediction are essential for best performance.\n\nIntroduction\n\nWhile methods for automatic scene understanding have progressed rapidly over the past years, it is just one key ingredient for assisted and autonomous driving. Human capabilities go beyond inference of scene structure and en-compass a broader type of scene understanding that also lends itself to anticipating the future.\n\nAnticipation is key in preventing collisions by predicting future movements of dynamic agents e.g. people and cars in inner cities. It is also the key to operating at practical safety distances. Without anticipation, domain knowledge and experience, drivers would have to maintain an equally large safety distance to all objects, which is clearly impractical in dense and cluttered inner city traffic. Additionally, anticipation enables decision making, e.g. passing cars and pedestrians while respecting the safety of all participants. Even at conservative and careful driving speeds of 25miles/hour (\u223c 40km/hour) in residential areas, the distance traveled in 1 second corresponds roughly to the breaking distance. Anticipation of traffic scenes on a time horizons of at least 1 second would therefore enable safe driving at such speeds.\n\nWe propose the first approach to predict people (pedestrians including cyclists) trajectories from on-board cameras over such long-time horizons with uncertainty estimates. Due to the particular importance for safety, we are focusing on the people class. While pedestrian trajectory prediction has been approached in prior work, we propose the first approach for on-board prediction. As predictions are made with respect to the moving vehicle, we formulate a novel two stream model for long-term person bounding box prediction and vehicle ego motion (odometry). In contrast to prior work, we model both aleatoric (observation) uncertainty and epistemic (model) uncertainty [4] in order to arrive at an estimate of the overall uncertainty.\n\nOur contributions in detail are: 1. First approach to long-term prediction of pedestrian bounding box sequences from a mobile platform; 2. Novel sequence to sequence model which provides a theoretically grounded approach to quantify uncertainty associated with each prediction; 3. Detailed experimental evaluation of alternative architectures illustrating the importance and effectiveness of using a two-stream architecture; 4. Analysis of dependencies between uncertainty estimates and actual prediction error leading to an empirical error bound.\n\n\nRelated work\n\nHuman Trajectory Prediction. Recent works such as [11,22] focus on the task of pedestrian trajectory prediction in 3D space. Initial trajectories and obstacle occupancy maps are obtained by dense stereo matching, assuming a linear road model of fixed width. However, 3D coordinates and obstacle maps obtained from stereo matching can be very noisy especially in unknown environments. Moreover, evaluation is on sequences with linear or no vehicle egomotion. Our method does not depend upon unreliable 3D coordinates and needs no assumptions about scene geometry and vehicle ego-motion. Another class of models such as [9,28,23,1] consider the problem of pedestrian trajectory prediction in a social context by modelling human-human interactions. The state of the art model [1] proposes to estimate the trajectories of each person in the scene by an instance of a \"Social\" LSTM. The instances of the Social LSTM can communicate with a special pooling layer. This enables the modelling of interactions and joint estimation of trajectories of all pedestrians in the scene. In [26] the joint estimation of robot and human trajectories are considered in a social context. However, in case of on-board prediction vehicle ego-motion dominates social aspects. Moreover, most methods are trained/tested on static camera datasets which are hand annotated with minimum observation noise. Apart from these, the class of models such as [10,13,18,31,29] aim at discovering motion patterns of humans and vehicles. Such methods cannot be used for trajectory prediction and do not consider vehicle ego-motion. Modeling Uncertainty in Deep Learning. Popular deep learning architectures do not model uncertainty. They assume uniform constant observation noise (aleatoric uncertainty). Heteroscedastic regression methods [20,15] estimate aleatoric uncertanity by predicting the parameters of a assumed observation noise distribution (also in [1]). Bayesian neural networks [17,19] offer a probabilistic view of deep learning and provide model (epistemic) uncertainty estimates. However, inference of model posterior in such networks is difficult. Variational Inference is a popular method. Gal et. al. in [6] showed that dropout training in deep neural networks approximates Bayesian inference in deep Gaussian processes. Extending these results it was shown in [5] that dropout training can be cast as approximate Bernoulli variational inference in Bayesian neural networks. These results were extended to RNNs in [7]. The developed Bayesian RNNs showed superior performance to standard RNNs with dropout in various tasks. More recently, [12] presents a Bayesian deep learning framework jointly estimating aleatoric uncertainty together with epistemic uncertainty. The resulting framework gives new state-of-the-art results on segmentation and depth regression benchmarks. Assisted and Autonomous driving. One of the earliest works on vehicle ego-motion (odometry) prediction or popularly, autonomous driving, was ALVINN by [21]. This work showed the possibility of directly predicting steering angles from visual input. This system used a simple fullyconnected network. More recently, [2] uses a convolutional neural network for this task and achieves a autonomy of 90% using a relatively small training set. However, the focus is on highway driving. [27] proposes a FCN-LSTM that predicts the next vehicle odometry based on the visual input captured by an on-board camera and previous odometry of the vehicle. Here, a diverse crowed sourced dataset is used. However, these methods predict vehicle odometry (e.g. steering angle) only for the next time-step. In contrast, we focus on inner-city driving and predict multiple time-steps into the future. [24] proposes a driving simulator that predicts the future in form of frames but suffers from blurriness problems in the long-term important details get lost. In [16] future segmentation masks are predicted, but only mid-term (upto 0.5sec) future is predicted and there is no pedestrian specific evaluation. We predict the future in terms of bounding box coordinates which remain well defined by design in the long-term.\n\n\nOn-board Pedestrian Prediction under Uncertainty\n\nIn order to anticipate motion of people in real-world traffic scenes from on-board cameras, we propose a novel approach that conditions the prediction of motion (subsection 3.1) of people on predicted odometry (subsection 3.4). Moreover, our approach models both aleatoric and epistemic uncertainty. Our model (see Figure 2) consists of two specialized streams for prediction of pedestrian motion and odometry. The odometry specialist stream predicts the most likely future vehicle odometry sequence. The bounding box specialist stream consists of a novel Bayesian RNN encoderdecoder architecture to predict odomerty conditioned distributions over pedestrian trajectories and to capture epistemic and aleatoric uncertainty. Bayesian probability theory provides us with a theoretically grounded approach to dealing with both types of uncertainties (subsection 3.2).\n\nWe start by describing the bounding box prediction stream of our model and introduce our novel Bayesian RNN encoder-decoder which provides theoretically grounded un- \n\n\nPrediction of Pedestrian Trajectories\n\nA bounding box corresponding to the i th pedestrian observed on-board a vehicle at time step t can be described by the top-left and bottom-right pixel coordinates: b t i = {(x tl , y tl ), (x br , y br )}. We want to predict the distribution of future bounding box sequences B f (where |B p | = m) of the pedestrian. We condition our predictions on the past bounding box sequence B p , the past odometry sequence O p and the corresponding future odometry sequence O f of the vehicle. The future odometry sequence O f is predicted conditioned on the past odometry sequence O p and on-board visual observation. Odometry sequences consists of the speed s t and steering angle d t of the vehicle, that is, o t = (s t , d t ).\np(B f = [b t+1 i , ..., b t+n i ] | B p , O p , O f ) B p = [b t\u2212m i , ..., b t i ], O p = [o t\u2212m , ..., o t ], O f = [o t+1 , ..., o t+n ]\nThe variance of the predictive distribution p(B f |B p' ) provides a measure of the associated uncertainty.\n\nWe will describe a basic sequence to sequence RNN first and then extend it to predict distributions and provide uncertainty estimates. Our sequence to sequence RNN ( Figure 2) consists of two embedding layers, an encoder RNN and a decoder RNN. The input sequence consists of the concatenated past bounding box and odometry sequences B p , O p . The input embedding layer embeds the inputs sequence x t into the representationx t . This embedded sequence is read by the encoder RNN (RNN enc ) which produces a summary vector v bbox . This summary vector is concatenated with predicted odometry O f and this summary sequence is embedded using the second embedding layer. This embedded summary sequencev (containing information about past pedestrian motion, past and future vehicle odometry) is used by the decoder RNN (RNN dec ) for prediction.\n\nIn the following, we extend this model to predict distributions and estimate uncertainty.\n\n\nBayesian Modelling of Uncertainty\n\nWe phrase our novel RNN encoder-decoder model in a Bayesian framework [12]. We capture epistemic (model) uncertainty by learning a distribution models p(f |X, Y ) likely to have generated our data {X, Y }. Here, models are RNN encoder-decoders with varying parameters. We infer the posterior distribution of RNN encoder-decoders p(f |X, Y ) , given the prior belief of the distribution of RNN encoder-decoders p(f ). The predictive probability over the future sequence B f given the past sequence B p is obtained by marginalizing over the posterior distribution of RNN encoder-decoders,\np(B f |B p , O p , O f ,X, Y ) = p(B f |B f , , O p , O f , f )p(f |X, Y )df.(1)\nHowever, the integral in (1) is intractable. But, we can approximate it in two steps [5,7,12]. First, we assume that our RNN encoder-decoder models can be described by a finite set of variables \u03c9. Thus, we constrain the set of possible RNN encoder-decoders to ones that can be described with \u03c9. Now, (1) can be equivalently written as,\np(B f |B p , O p , O f ,X, Y ) = p(B f |B p , O p , O f , \u03c9)p(\u03c9|X, Y )d\u03c9(2)\nSecond, we assume an approximating variational distribution q(\u03c9) which allows efficient sampling,\np(B f |B p , O p , O f ) = p(B f |B p , O p , O f , \u03c9)q(\u03c9)d\u03c9 (3)\nWe choose the set of weight matrices {W 1 , .., W L } \u2208 W of our RNN enocder-decoder as the set of variables \u03c9. Then we define an approximating Bernoulli variational distribution q(\u03c9) over the columns w c k of the weight matrices W k \u2208 W,\nq(W k ) = M k \u00b7 diag([z i,j ] C k j=1 ) z i,j = Bernoulli(p i ), i = 1, ..., L, j = 1, ..., K i\u22121 .(4)\nwhere, M k are the variational parameters. This distribution allows for efficient sampling during training and testing which we discuss in the following subsection.\n\nFor an accurate approximation, we minimize the KL divergence between q(\u03c9) and the true posterior p(\u03c9|X, Y ) as the training step. It can be shown that, (as in [6,5]),\nKL(q(\u03c9) || p(\u03c9|X, Y )) \u221d KL(q(\u03c9) || p(\u03c9)) \u2212 t q(\u03c9) log p(b t+n t |b t+n\u22121 t , B p , O p , O f , \u03c9)d\u03c9.(5)\nThe first part corresponds to the distance to the prior model distribution and the second to the data fit. During training and prediction, we use Monte-Carlo integration to approximate the integrals (3) and (5) (more details about (5) in the Appendix and the exact objective in subsection 3.5). Aleatoric uncertainty can be captured along with epistemic uncertainty, by assuming a distribution of observation noise and estimating the sufficient statistics of the distribution.\n\nHere, we assume it to be a 4-d Gaussian at each time-step,\nN (b t+n i , \u03a3 t i ), where, \u03a3 = diag (\u03c3 t+n x ) i , (\u03c3 t+n y ) i , (\u03c3 t+n x ) i , (\u03c3 t+n y )\ni in x and y directions in pixel space at time-step t + n. The predictive distribution of models parametrized by \u03c9,\np(B f |B p , , O p , O f , \u03c9) is Gaussian at every time-step.\nUncertainty is the variance of our predictive distribution (3) and can be obtained through moment matching [6,12]. If we have T samples of future pedestrian bounding box sequencesB f , the total uncertainty at time-step t is,\n1 T T i=1 (b t i ) b t i \u2212 1 T T i=1 (b t i ) T i=1b t i + 1 T T i=1 (\u03c3 t i ) x + T i=1 (\u03c3 t i ) y .(6)\nThe first part of the sum correspond to the epistemic uncertainty u e i and the second part corresponds to the aleatoric uncertainty u a i . We average the uncertainty across time-steps to arrive at the complete uncertainty estimate. Next, we describe how we sample from the Bernoulli distribution of RNN encoder-decoder weight matrices and the final sampling from the predictive distribution p(\nB f |B p , O p , O f ).\n\nBayesian RNN Encoder-Decoder\n\nThe RNN encoder-decoder model of subsection 3.1 contains four weight matrices. In detail, the two embedding layers contains two weight matrices W emi , W ems . The other two weight matrics belong to the encoder and decoder RNNs. We use an LSTM formulation as RNNs. Following [8] the weight matrices of an LSTM can be concatenated into a matrix W and the LSTM can be formulated as in, \uf8eb\n\uf8ec \uf8ec \uf8ed i f \u00f4 c \uf8f6 \uf8f7 \uf8f7 \uf8f8 = \uf8eb \uf8ec \uf8ec \uf8ed sigm sigm sigm tanh \uf8f6 \uf8f7 \uf8f7 \uf8f8 x t h t\u22121 \u00b7 W c t = f c t\u22121 + i \u0109 , h t = o tanh(c t )(7)\nwhere i is the input gate, f is the forget gate, o is the output gate, c t is the cell state,\u0109 is the candidate cell state and h t is the hidden state.\n\nWe define the Bernoulli variational distribution q(\u03c9) (as in (4)) over the union of all the weight matrices of our model,\n\u03c9 = {W emi , W ems , W enc , W dec } .(8)\nwhere, W enc , W dec are the weight matrices of our RNN encoder and decoder. Sampling from q(W emi ), q(W ems ) can be done efficiently by sampling random Bernoulli masks z emi , z ems and applying these masks after the linear transformations. In case of the input embedding,\nx t = (x t \u00b7 W emi ) z emi(9)\nSimilarly, it was shown in [7] sampling weight matrices of a LSTM (here, q(W enc ), q(W dec )) can be efficiently performed by sampling random Bernoulli masks z x , z h and applying them at each time-step, while the LSTM encoder and decoder are unrolled, \uf8eb\n\uf8ec \uf8ec \uf8ed i f \u00f4 c \uf8f6 \uf8f7 \uf8f7 \uf8f8 = \uf8eb \uf8ec \uf8ec \uf8ed sigm sigm sigm tanh \uf8f6 \uf8f7 \uf8f7 \uf8f8 x t z x h t\u22121 z h \u00b7 W(10)\nSampling\nfrom our predictive distribution p(B f |B p , O f , O p )\nis done by first sampling weights matrices of our Bayesian RNN encoder-decoder. Then the parameters of the Gaussian observation noise distribution at each time-step is predicted. For this, we use the hidden state sequence h t dec of the RNN dec and an additional linear transformation,\nh t+n dec = RNN dec (h t+n\u22121 dec , v bbox ; z x , z h ) b t+n i , (\u03c3 i t+n ) x , (\u03c3 t+n i ) y = W bbox * h t+n dec + bias bbox .\nWe then draw a sample from the predicted Gaussian distribution.\n\nNext, we describe the second stream of our two-stream model -our model for long-term odometry prediction.\n\n\nPrediction of Odometry\n\nWe use a similar RNN encoder-decoder architecture used for bounding box prediction, but without the embedding layers. We do not place a distribution over the weights of the RNN encoder-decoder. We condition the predicted sequence O f on the past odometry sequence O p and last visual observation on-board the vehicle. The past odometry O p is input to an encoder RNN which produces a summary vector v odo . The past odometry of the vehicle O p gives a strong cue about the future velocity especially in the short term (\u223c100ms). We use the same LSTM formulation described previously as the RNN encoder; with the final hidden state h t as the summary. The last visual observation can help in the longer term prediction of odometry; e.g. visual cues about bends in the road, obstacles etc. Similar to [27,2] we employ a convolutional neural network (CNN-encoder) to embed the visual information provided by the currently observed frame; a visual summary vector v vis . Next we describe our CNN-encoder architecture. CNN-encoder. Our CNN-encoder should extract visual features to improve longer-term (multi-step versus single-step in [27,2]) prediction. Therefore, we use a more complex CNN compared to [2] and during training we learn the parameters from scratch, unlike [27] which uses a pre-trained VGG network. Our CNN-encoder has 10 convolutional layers with ReLU non-linearities. We use a fixed, small filter size of 3x3 pixels. We use max-pooling after every two layers. After max-pooling we double the number of convolutional filters; we use {32,64,128,256,512} convolutional filters. The convolutional layers are followed by three fully connected layers with 1024, 256 and 128 neurons and ReLU non-linearities. The output of the last fully connected layer is the visual summary v vis .\n\nThe odometry and visual summary vectors are concatenated v = {v odo , v vis } and read by the RNN decoder (RNN dec ). We use the same LSTM formulation described previously as the RNN-decoder. As before, the hidden state of the LSTM decoder is used for predicting the future odometry sequence through a linear transformation.\nh t+n dec = RNN dec (h t+n\u22121 dec , {v odo , v vis }) o t+n i = W odo * h t+n dec + bias odo .\nWe next describe our training and inference processes.\n\n\nTraining and Inference\n\nTraining. The two streams are trained separately. As the odometry prediction stream predicts point estimates, it is trained first by minimizing the MSE over the training set.\n\nThe Bayesian bounding-box prediction stream is trained by estimating (Monte-Carlo) and minimizing the KL divergence of its approximate weight distribution q(\u03c9) (5). More specifically, 1. We sample a mini-batch of size T of exam-  (9) and (10). 3. For each example, the predicted meansB f and variances\u03c3 of the heteroscedastic models parameterized by \u03c9 are inferred. 4. The KL divergence (5) can be equivalently minimized by (similar to [6,12]) the following loss,\n1 4 n N N i=1 n j=1 b t+j i \u2212 b t+j i 2 2 (\u03a3 t i ) \u22122 + \u03bb W 2 + log\u03c3 2 i\nwhere, | B f |= n and N pedestrians. The left part is the equivalent of the negative log likelihood term in (5). The middle part is weight regularization parameterized by \u03bb, equivalent to the KL term in (5). The right part is additional regularization as in [12], to ensure finite predicted variance. The ADAM optimizer [14] is used during training. For training sequences longer than |B p | + |B f | (|O p + O f | respectively) we use a sliding window to convert to multiple sequences. Moreover, as the sequences in the training set are of varying lengths, we use a curriculum learning (CL) approach. We fix the length of the conditioning sequence |B p |, |O p | and train for increasing longer time horizons |B f |, |O f | (initializing the model parameters with those for shorter horizons). This allows us to train on a larger part of the Cityscapes training set (also on sequences shorter than |B p | + |B f | of the final model) and leads to faster convergence. Inference. Given B p and O p (and the visual observation), the odometry prediction stream is first used to predict O f . We sample from the predictive distribution (3) by, 1. Sampling T samples of the weight matrices {W emi , W ems , W enc , W dec } of the Bayesian bounding box prediction stream from the (learned) approximate distribution q(\u03c9), by sampling Bernoulli masks as in (9) and (10), 2. The RNN dec is unrolled to obtain a sample B f ,\u03c3 x ,\u03c3 y from each of the T predicted Gaussian distributions. The associated uncertainty is obtained using the T samples (6).\n\n\nExperiments\n\nWe evaluate our model on real-world on-board street scene data and show predictions over a 1 second time horizon along with the associated uncertainty. Dataset and Evaluation Metric. We evaluate on the Cityscapes dataset [3] which contains 2975 training, 500 validation and 1525 test video sequences of length 1.8 seconds (30 frames). The video resolution is 2048\u00d71024 pixels. The sequences were recorded on-board a vehicle in inner cities. Each sequence has associated odometry information. Pedestrian tracks were automatically extracted using the tracking by detection method of [25]. Detections were obtained using the Faster R-CNN based method of [30]   driving systems where detections/tracks are obtained with a state-of-the-art detector/tracker and we have to deal with noise introduced by the detector and on rare occasions detector false positives and tracker failures. We use as evaluation metric MSE in pixels (of the mean of the predictive distribution) and the negative log-likelihood L. The L metric measures the probability assigned to the true sequence by our predictive distribution. We report these metrics averaged across all time-steps and plots per time-step. We use a dropout rate of 0.35, \u03bb = 10 \u22124 (tuned on validation set) and use 50 Monte-Carlo samples across all Bayesian models.\n\nEvaluation of Bounding Box Prediction. We independently evaluate the first Bayesian LSTM stream of our two stream model, without conditioning it on predicted odometry. We predict 15 time-steps into the future and report the results in Table 1. We compare its performance with, 1. A linear Kalman filter baseline. 2. A homoscedastic LSTM encoder-decoder model (LSTM). 3. A heteroscedastic LSTM encoder-decoder (LSTM-Aleatoric). Finally, as an Oracle case, we compare against a Bayesian version in which the LSTM encoder can see the past odometry and the LSTM decoder can see the true future odometry at every time-step. We also vary the length of the conditioning sequence |B p | (training/test sets constant across varying |B p |). In Table 1, we see that the homoscedastic LSTM model (2nd row) outperforms the linear Kalman filter (1st row). This shows that many bounding box sequences have a complex motion and therefore cannot be modelled by a Kalman filter. We see that the heteroscedastic LSTM (LSTM-Aleatoric, 3rd row) outperforms the homoscedastic LSTM (2nd row) with respect to the L metric. This means that the heteroscedastic LSTM learns to capture uncertainty and assigns higher probability to the true bounding box sequence. However, when epistemic uncertainty is not modelled, aleatoric uncertainty tried to compensate (as in [12]) and this leads to poorer MSE.  introduced regularization (dropout and weight). Furthermore, we see that increasing the length of the conditioning sequence improves model performance. However, the performance gain saturates at |B p | = 8. Henceforth, we will report results using |B p | = {4, 8} in the following. Finally, the odometry oracle case outperforms our Bayesian LSTM by a large margin. This shows that knowledge of vehicle odometry is crucial for good performance.\n\nComparison with Social LSTM [1]. We compare our Bayesian LSTM model with the vanilla LSTM 1 model of [1] (with 128 neurons) that predicts trajectories independently in Table 2. Both models are trained to predict sequences of bounding box centers (length 15, given 8). Our Bayesian LSTM model performs better as it is more robust to mistakes during recursive prediction. The model of [1] observes true past pedestrian coordinates during training. However, during prediction it observes its own predictions causing errors to be propagated though multiple steps of prediction. Furthermore, we compare both methods to the centers obtained from the predictions of our Bayesian LSTM (second row of Table 1 Table 4: Evaluation of our Bayesian two stream model (Figure 2).  increases the training data with non-zero steering angles by a factor of two. We use MSE between the predicted future vehicle velocity and steering angles as evaluation metric. The velocity is in meters per second and angle in degrees. We include as baselines: 1. A constant steering predictor that predicts the last observed odometry. 2. A linear Kalman filter. 3. Our LSTM encoder-decoder without visual observation (v = {v odo }). The third baseline is an ablation study. We observe no significant performance difference between |O p | = {4} and |O p | = {8}. We evaluate 15 timesteps into the future and report the results in Table 3. We observe that the constant angle predictor performs significantly worse compared to the other baselines. This shows that the Cityscapes test set includes a significant number of non-trivial sequences with complex vehicle trajectories. We observe that the Kalman filter is able to quite accurately predict the vehicle speed. This is because in most vehicles are travelling with constant speed or accelerating/decelerating smoothly. However, the performance of the linear Kalman filter is worse compared to the LSTM models with respect to steering angle. This means that many sequences have non-linear vehicle trajectories. The superior performance of our model compared to the RNN baseline without visual observations, especially in the long-term shows that our CNN encoder extracts information useful for long-term prediction.\n\nWe also show visual examples in the Appendix. Evaluation of our Two-Stream model. We perform an ablation study of our two-stream model ( Figure 2) and compare with a single-stream Bayesian LSTM encoder-decoder model where the encoder observes the concatenated past bounding box and velocity sequence {B p , O p } and the decoder predicts the future bounding box sequence B f . This model does not see predicted future odometry. We evaluate the models and report the results in Table 4 and plot the MSE per time-step Table 5. The results show that jointly predicting odometry with pedestrian bounding boxes (3rd row) significantly improves performance (2nd row). The predicted odometry helps our two-stream model recover a significant fraction of the performance of the Oracle case in Table 1 row 5. The limiting factor here is that the odometry is difficult to predict in certain situations e.g. at T-intersections. Apart from cases with inaccurate odometry prediction, the residual error of our two-stream (and the Oracle case) on a large part is due to the noise of the pedestrian detector and tracker failures. We show qualitative examples in Figure 4. Row 1 shows point estimates under linear vehicle ego-motion and Rows 2, 3 non-linear vehicle ego-motion. Our two-stream model (mean of predictive distribution) outperforms other methods in the second case. Rows 4-5 shows the predictive distributions of the two-stream model under linear vehicle and pedestrian motion. The distribution is symmetric and has high aleatoric uncertainty which captures detection noise and possible pedestrian motion. Row 6 shows a case of a skewed distribution with high epistemic uncertainty which captures uncertainty in vehicle motion. Quality of our Uncertainty Metric. We evaluate our uncertainty metric in Figure 3. The first two plots show the aleatoric and epistemic uncertainty to the squared error of the mean of the predictive distribution of our two-stream model. We use log-log plots for better visualization as most Last Observation: t Prediction: t + 5 Prediction: t + 10 Prediction: t + 15  (Table 1 row 1), Yellow: One-stream model (Table 1 row 4), Green: Two-stream model (mean of predictive distribution, Table 4 row 3). Rows 4-6: Predictive distributions of our two-stream model as heat maps. (Link to video results in the Appendix).\n\nsequences have low error (note, log(530) \u2248 6.22 the MSE of our two stream model, Table 4). We see that the epistemic and aleatoric uncertainties are correlates well with the squared error. This means that for sequences where the mean of our predictive distribution is far from the true future sequence, our predictive distribution has a high variance (and vice versa). Therefore, for sequences with multiple likely futures, where the mean estimate would have high error, our model learns to predict diverse futures. In the third plot of Figure 3, we plot the maximum log squared error (of the mean of the predictive distribution) observed at a certain predicted uncertainty level (sum of aleatoric and epistemic) in the test test. In the fourth plot, we plot the uncertainty with the maximum observed squared error at time-steps t + {5, 10, 15}. In both cases, uncertainty and observed maximum error is well correlated. This shows that, the predicted uncertainty upper bounds the error of the mean of the predictive distribution. Therefore, the predicted uncertainty helps us express trust in predictions and has the potential to serve as a basis for better decision making.\n\n\nConclusion\n\nWe highlight the importance of anticipation for practical and safe driving in inner cities. We contribute to this important research direction the first model for long term prediction of pedestrians from on-board observations. We show predictions over a time horizon of 1 second. Predictions of our model are enriched by theoretically grounded uncertainty estimates. Key to our success is a Bayesian approach and long term prediction of odometry. We evaluate and compare several different architecture choices and arrive at a novel two-stream Bayesian LSTM encoder-decoder.\n\nAs derived in [6,5], in Bayesian Regression, the KL divergence between a approximate variational posterior q(\u03c9) and the true posterior p(\u03c9|X, Y ) distribution of models likely to have generated our data is given by,\nKL(q(\u03c9) || p(\u03c9|X, Y )) \u221d KL(q(\u03c9) || p(\u03c9)) \u2212 q(\u03c9) log p(Y |X, \u03c9)d\u03c9.(A1)\nIn our case, as we train our model to predict future bounding box sequences given the past bounding box sequence, past and future vehiche odometry, we have\nX = {B p , O f , O p } and Y = {B f }.\nTherefore, the KL divergence is given by,\nKL(q(\u03c9) || p(\u03c9|X, Y )) \u221d KL(q(\u03c9) || p(\u03c9)) \u2212 q(\u03c9) log p(B f |B p , O f , O p , \u03c9)d\u03c9.(A2)\nAs the bounding box at time t + n in B f is predicted conditioned on the bounding box at time t + n \u2212 1 and the past bounding box sequence, past and future vehiche odometry, by our Bayesian RNN Encoder-Decoder, the KL divergence is given by,\nKL(q(\u03c9) || p(\u03c9|X, Y )) \u221d KL(q(\u03c9) || p(\u03c9)) \u2212 t q(\u03c9) log p(b t+n t |b t+n\u22121 t , B p , O p , O f , \u03c9)d\u03c9.(A3)\nDuring training (as mentioned in subsection 3.5 of the main paper), we use Monte-Carlo integration to estimate the integral in (5) (using N samples),\nKL(q(\u03c9) || p(\u03c9|X, Y )) \u221d KL(q(\u03c9) || p(\u03c9)) \u2212 1 N t N i=0 log p(b t+n t |b t+n\u22121 t , B p , O p , O f ,\u03c9 i ), \u03c9 i \u223c q(\u03c9).(A4)\nThe probability term p(b t+n\nt |b t+n\u22121 t , B p , O p , O f ,\u03c9 i ) takes the form e \u2212 b t+j i \u2212b t+j i 2 2 (\u03a3 t i ) \u22122\n. Therefore, replacing the log probability term with the exponential squared error term and introducing additional regularization as mentioned in subsection 3.5 of the main paper leads to the training objective used,\n1 4 n N N i=1 n j=1 b t+j i \u2212 b t+j i 2 2 (\u03a3 t i ) \u22122 + \u03bb W 2 + log\u03c3 2 i\n\nB. Additional Details of Two Stream Model\n\nHere, we include details of each layer of our Two Stream Model. We refer to fully connected layers as Dense and Size refers to the number of neurons in the layer. Bayesian Bounding Box Prediction Stream. We provide the details of the Bayesian Bounding Box prediction stream in Table 6.\n\n\nLayer\n\nType Size Activation Input Output   Table 8 C. Database Statistics Figure 5: Length of recovered pedestrian tracks in Cityscapes.\nIn 1 Input B past EMB 1 In 2 Input O past EMB 1 EMB 1 Dense 64 ReLU {In 1 , In 2 } LSTM enc1 LSTM enc1 LSTM 128 tanh EMB 1 EMB 2 EMB 2 Dense 64 ReLU LSTM enc1 ,\u00d4 f LSTM dec1 LSTM dec1 LSTM 128 tanh EMB 2 Out 1 Out 1 Dense 4 LSTM decBf\nIn Figure 5 we plot the number of pedestrian tracks of Layer Type Filters Size Activation Input Output  Table 9: Evaluation with varying size of LSTM (|B p | = 8).\nIn 4 Input C 1 C 1 Conv 32 3\u00d73 ReLU In 2 C 2 C 2 Conv 32 3\u00d73 ReLU C 1 P 1 P 1 MaxPool 2\u00d72 C 2 C 3 C 3 Conv 64 3\u00d73 ReLU P 1 C 4 C 4 Conv 64 3\u00d73 ReLU C 4 P 2 P 2 MaxPool 2\u00d72 C 4 C 5 C 5 Conv 128 3\u00d73 ReLU P 2 C 6 C 6 Conv 128 3\u00d73 ReLU C 5 P 3 P 3 MaxPool 2\u00d72 C 6 C 7 C 7 Conv 256 3\u00d73 ReLU P 3 C 8 C 8 Conv 256 3\u00d73 ReLU C 7 C 8 P 4 MaxPool 2\u00d72 C 8 C 9 C 9 Conv 512 3\u00d73 ReLU P 4 C 10 C 10 Conv 512 3\u00d73 ReLU C 9 P 5 P 5 MaxPool 2\u00d72 C 10 FC 1 FC 1 Dense 1024 ReLU P 5 FC 2 FC 2 Dense 256 ReLU FC 1 FC 3 FC 3 Dense 128 tanh FC 2 LSTM dec2\nIn the main paper, we evaluate all models constant LSTM vector size of 128. Here, we report results for the (unconditioned) one stream homoscedastic LSTM encoder-decoder model and the one stream Bayesian LSTM encoder-decoder model using a vector size of 512 In Table 9. We see that the homoscedastic version with 512 neurons performs worse than the version with 128 neurons. This is because the larger LSTM over-fits to the bounding box estimation noise in dataset. However, the Bayesian versions have comparable performance, due to dropout which prevents overfitting.\n\n\nE. Visualization of Odometry Prediction\n\nVisual examples of odometry prediction in Figure 6. Please refer to section 4 for more details.  Here, we compare our Bayesian Two-stream model (Figure 2, of main paper) to, 1. A homoscedastic Two-stream LSTM encoder-decoder model (LSTM). 2. A heteroscedastic Two-stream LSTM encoder-decoder (LSTM-Aleatoric). Note that, both models have the same odometry prediction stream as our Bayesian Two-stream LSTM model (LSTM-Bayesian). The results mirror the evaluation of only the bounding box prediction stream. We see that the heteroscedastic LSTM (LSTM-Aleatoric, 2nd row) outperforms the homoscedastic LSTM (2nd row) with respect to the L metric. This means that the heteroscedastic Two-stream LSTM learns to capture uncertainty and assigns higher probability to the true bounding box sequence. However, when epistemic uncertainty is not modelled, aleatoric uncertainty tried to compensate and this leads to poorer MSE. Finally, our Bayesian Two-stream LSTM (3rd row) outperforms all other methods.\n\n\nF. Additional Evaluation of our Two-stream Model\n\n\nG. Additional Analysis of the Quality of our Uncertainty Metric\n\nWe compare the quality of the uncertainty metric obtained with our Two-stream LSTM-Bayesian model (Figure 3, of main paper) to that of the Two-stream LSTM-Aleatoric (the heteroscedastic Two-stream LSTM encoder-decoder in the previous section, which models only aleatoric uncertainty). In plot 1 of Figure 7 the aleatoric uncertainty to the log squared error of the mean of the predictive distribution of the Two-stream LSTM-Aleatoric model is shown. We see that the distribution is more spread-out with more outliers compared to our Two-stream LSTM-Bayesian model (plot 1, Figure 3, of main paper). In plot 2 of Figure 7 the maximum log squared error (of the mean of the predictive distribution) observed at a certain predicted uncertainty in the test test is shown for both our Two-stream Bayesian model and Twostream LSTM-Aleatoric. We see that the correlation is poor compared to our Two-stream LSTM-Bayesian model (also in plot 3, Figure 3, of main paper). In particular, the maximum  observed log squared error rises very sharply. Therefore, for a robust error bound it is essential to model both epistemic and aleatoric uncertainty.\n\n\nH. Additional Video Results\n\nWe include video results of prediction in video.mp4 (click here). We include examples of both point estimates and predictive distributions. We include point estimates for comparison against the Kalman Filter and One-stream baselines. The examples show accurate prediction by our Two-stream model over 15 time-steps into the future.\n\nFigure 2 :\n2Two stream architecture for prediction of future pedestrian bounding boxes. certainty estimates.\n\nFigure 3 :\n3Quality of our uncertainty metric: plots 1 and 2 -uncertainty versus squared error, plots 3 and 4 -uncertainty versus maximum observed squared error.\n\nFigure 4 :\n4Rows 1-3: Point estimates. Blue: Ground-truth, Red: Kalman Filter\n\nFigure 6 :\n6Odometry prediction: We show predicted odometry for 15 time-steps as points (bottom to top) over-layed on the last visual observation. The distance and angle between subsequent points is the predicted (proportional) speed and steering angle. Color codes: Blue: Ground-truth, Red: Kalman Filter, Yellow: Our LSTM without visual input, Green: Our LSTM with visual input.\n\nFigure 7 :\n7Plot 1 -uncertainty versus squared error, plot 3uncertainty versus maximum observed squared error.\n\n\n(statistics in the Appendix). This mimics real world autonomous/assistedMSE \n\nL \n\n|B p | \n|B p | \nMethod \nOdometry \n4 \n6 \n8 \n4 \n6 \n8 \n\nKalman Filter \nNone \n1938 1289 1098 x \nx \nx \nLSTM \nNone \n692 663 650 8.11 7.99 7.77 \nLSTM-Aleatoric \nNone \n772 758 750 5.92 5.81 5.54 \nLSTM-Bayesian \nNone \n647 624 618 4.31 4.26 4.13 \n\nLSTM-Bayesian Ground-truth 374 358 343 3.94 3.93 3.88 \n\nTable 1: Bounding box prediction error with varying |B p |. \n\nMethod \nMSE L \n\nSocial LSTM [1] \n1514 5.63 \nLSTM-Bayesian \n695 3.97 \nLSTM-Bayesian (centers) 648 \nx \n\n\n\nTable 2 :\n2Bounding box center prediction error.\n\nTable 3 :\n3Odometry prediction error (MSE), |O p | = {8}.\n\nTable 5 :\n5MSE per time-step of models in \nTable 1 row 1, 4, 5 and Table 4 row 3. \n\n\n\nTable 6 :\n6Details of the Bounding Box Prediction Stream. Note that, the weights of all the layers are sampled from the approximate posterior q(\u03c9). Prediction Stream. We provide the details of the odometry prediction stream inTable 7. We then provide details of the CNN encoder.Odometry Layer \nType \nSize Activation \nInput \nOutput \n\nIn 3 \nInput \nO past \nLSTM enc2 \n\nLSTM enc2 LSTM 128 \ntanh \nIn 3 \nLSTM dec2 \n\nLSTM dec2 LSTM 128 \ntanh \n{LSTM enc1 , FC 3 } \nOut 1 \nOut 2 \nDense \n2 \nLSTM dec2\u00d4f \n\n\n\nTable 7 :\n7Details of the Odometry Prediction Stream. Details of the CNN encoder (with output FC 3 ) follows in\n\nTable 8 :\n8Details of the CNN encoder used to condition the output of the Odometry prediction stream. Conv stands for 2D convolution, MaxPool stands for 2D max pooling and UpSample stands for 2D upsampling operations. lengths from 6 to 30. The track length distribution is consistent across training and test sets. We observe that there are many long tracks which stretch over the entire length (30) of the sequence.D. Evaluation with Varying Size of LSTMMethod \nLSTM size Odometry MSE \nL \n\nLSTM \n128 \nNone \n650 7.77 \nLSTM \n512 \nNone \n705 8.15 \nLSTM-Bayesian \n128 \nNone \n618 4.13 \nLSTM-Bayesian \n512 \nNone \n619 4.16 \n\n\n\nTable 10 :\n10Evaluation of Two-stream models (|B p |, |O p | = 8).\nThe version with social pooling did not converge on our dataset.\nAppendix A. Additional Details of Training Objective\nSocial lstm: Human trajectory prediction in crowded spaces. A Alahi, K Goel, V Ramanathan, A Robicquet, L Fei-Fei, S Savarese, CVPR. 26A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese. Social lstm: Human trajectory prediction in crowded spaces. In CVPR, 2016. 2, 6\n\nEnd to end learning for self-driving cars. M Bojarski, D Testa, D Dworakowski, B Firner, B Flepp, P Goyal, L D Jackel, M Monfort, U Muller, J Zhang, arXiv:1604.0731625arXiv preprintM. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016. 2, 5\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, CVPR. M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 5\n\nAleatory or epistemic? does it matter? Structural Safety. A Der Kiureghian, O Ditlevsen, 31A. Der Kiureghian and O. Ditlevsen. Aleatory or epistemic? does it matter? Structural Safety, 31(2):105-112, 2009. 1\n\nBayesian convolutional neural networks with Bernoulli approximate variational inference. Y Gal, Z Ghahramani, ICLR workshop track. 10Y. Gal and Z. Ghahramani. Bayesian convolutional neural networks with Bernoulli approximate variational inference. In ICLR workshop track, 2016. 2, 3, 4, 10\n\nDropout as a bayesian approximation: Representing model uncertainty in deep learning. Y Gal, Z Ghahramani, ICML. 510Y. Gal and Z. Ghahramani. Dropout as a bayesian approxi- mation: Representing model uncertainty in deep learning. In ICML, 2016. 2, 4, 5, 10\n\nA theoretically grounded application of dropout in recurrent neural networks. Y Gal, Z Ghahramani, Advances in neural information processing systems. 24Y. Gal and Z. Ghahramani. A theoretically grounded appli- cation of dropout in recurrent neural networks. In Advances in neural information processing systems, pages 1019-1027, 2016. 2, 3, 4\n\nSpeech recognition with deep recurrent neural networks. A Graves, A Mohamed, G Hinton, In ICASSP. 4A. Graves, A.-r. Mohamed, and G. Hinton. Speech recogni- tion with deep recurrent neural networks. In ICASSP, 2013. 4\n\nSocial force model for pedestrian dynamics. D Helbing, P Molnar, Physical review E. 5154282D. Helbing and P. Molnar. Social force model for pedestrian dynamics. Physical review E, 51(5):4282, 1995. 2\n\nSemanticbased surveillance video retrieval. W Hu, D Xie, Z Fu, W Zeng, S Maybank, IEEE Transactions on image processing. 164W. Hu, D. Xie, Z. Fu, W. Zeng, and S. Maybank. Semantic- based surveillance video retrieval. IEEE Transactions on image processing, 16(4):1168-1181, 2007. 2\n\nWill the pedestrian cross? probabilistic path prediction based on learned motion features. C G Keller, C Hermes, D M Gavrila, Joint Pattern Recognition Symposium. SpringerC. G. Keller, C. Hermes, and D. M. Gavrila. Will the pedes- trian cross? probabilistic path prediction based on learned motion features. In Joint Pattern Recognition Symposium, pages 386-395. Springer, 2011. 2\n\nWhat uncertainties do we need in bayesian deep learning for computer vision?. A Kendall, Y Gal, arXiv:1703.0497756arXiv preprintA. Kendall and Y. Gal. What uncertainties do we need in bayesian deep learning for computer vision? arXiv preprint arXiv:1703.04977, 2017. 2, 3, 4, 5, 6\n\nGaussian process regression flow for analysis of motion trajectories. K Kim, D Lee, I Essa, ICCV. K. Kim, D. Lee, and I. Essa. Gaussian process regression flow for analysis of motion trajectories. In ICCV, 2011. 2\n\nAdam: A method for stochastic optimization. ICLR. D Kingma, J Ba, D. Kingma and J. Ba. Adam: A method for stochastic opti- mization. ICLR, 2015. 5\n\nHeteroscedastic gaussian process regression. Q V Le, A J Smola, S Canu, ICML. Q. V. Le, A. J. Smola, and S. Canu. Heteroscedastic gaussian process regression. In ICML, 2005. 2\n\nPredicting deeper into the future of semantic segmentation. P Luc, N Neverova, C Couprie, J Verbeek, Y Lecun, ICCV 2017-International Conference on Computer Vision. 10P. Luc, N. Neverova, C. Couprie, J. Verbeek, and Y. LeCun. Predicting deeper into the future of semantic segmentation. In ICCV 2017-International Conference on Computer Vision, page 10, 2017. 2\n\nA practical bayesian framework for backpropagation networks. D J Mackay, Neural computation. 43D. J. MacKay. A practical bayesian framework for backprop- agation networks. Neural computation, 4(3):448-472, 1992. 2\n\nTrajectory learning for activity understanding: Unsupervised, multilevel, and long-term adaptive approach. B T Morris, M M Trivedi, IEEE transactions on pattern analysis and machine intelligence. 33B. T. Morris and M. M. Trivedi. Trajectory learning for activ- ity understanding: Unsupervised, multilevel, and long-term adaptive approach. IEEE transactions on pattern analysis and machine intelligence, 33(11):2287-2301, 2011. 2\n\nBayesian learning for neural networks. R M Neal, Springer Science & Business Media118R. M. Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012. 2\n\nEstimating the mean and variance of the target probability distribution. D A Nix, A S Weigend, Neural Networks. 1D. A. Nix and A. S. Weigend. Estimating the mean and variance of the target probability distribution. In Neural Networks, volume 1, pages 55-60, 1994. 2\n\nAlvinn, an autonomous land vehicle in a neural network. D A Pomerleau, Carnegie Mellon University, Computer Science DepartmentTechnical reportD. A. Pomerleau. Alvinn, an autonomous land vehicle in a neural network. Technical report, Carnegie Mellon University, Computer Science Department, 1989. 2\n\nGoal-directed pedestrian prediction. E Rehder, H Kloeden, ICCV Workshops. E. Rehder and H. Kloeden. Goal-directed pedestrian predic- tion. In ICCV Workshops, 2015. 2\n\nLearning social etiquette: Human trajectory understanding in crowded scenes. A Robicquet, A Sadeghian, A Alahi, S Savarese, ECCV. A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese. Learning social etiquette: Human trajectory understanding in crowded scenes. In ECCV, 2016. 2\n\nLearning a driving simulator. E Santana, G Hotz, arXiv:1608.01230arXiv preprintE. Santana and G. Hotz. Learning a driving simulator. arXiv preprint arXiv:1608.01230, 2016. 2\n\nMultiperson tracking by multicut and deep matching. S Tang, B Andres, M Andriluka, B Schiele, ECCV. S. Tang, B. Andres, M. Andriluka, and B. Schiele. Multi- person tracking by multicut and deep matching. In ECCV, 2016. 6\n\nRobot navigation in dense human crowds: the case for cooperation. P Trautman, J Ma, R M Murray, A Krause, ICRA. P. Trautman, J. Ma, R. M. Murray, and A. Krause. Robot navigation in dense human crowds: the case for cooperation. In ICRA, 2013. 2\n\nEnd-to-end learning of driving models from large-scale video datasets. H Xu, Y Gao, F Yu, T Darrell, 25H. Xu, Y. Gao, F. Yu, and T. Darrell. End-to-end learning of driving models from large-scale video datasets. CVPR, 2017. 2, 5\n\nWho are you with and where are you going? In CVPR. K Yamaguchi, A C Berg, L E Ortiz, T L Berg, K. Yamaguchi, A. C. Berg, L. E. Ortiz, and T. L. Berg. Who are you with and where are you going? In CVPR, 2011. 2\n\nUnderstanding highlevel semantics by modeling traffic patterns. H Zhang, A Geiger, R Urtasun, CVPR. H. Zhang, A. Geiger, and R. Urtasun. Understanding high- level semantics by modeling traffic patterns. In CVPR, 2013. 2\n\nCitypersons: A diverse dataset for pedestrian detection. S Zhang, R Benenson, B Schiele, S. Zhang, R. Benenson, and B. Schiele. Citypersons: A diverse dataset for pedestrian detection. CVPR, 2017. 6\n\nRandom field topic model for semantic region analysis in crowded scenes from tracklets. B Zhou, X Wang, X Tang, CVPR. B. Zhou, X. Wang, and X. Tang. Random field topic model for semantic region analysis in crowded scenes from tracklets. In CVPR, 2011. 2\n", "annotations": {"author": "[{\"end\":186,\"start\":79},{\"end\":306,\"start\":187},{\"end\":429,\"start\":307}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":87},{\"end\":198,\"start\":193},{\"end\":320,\"start\":313}]", "author_first_name": "[{\"end\":86,\"start\":79},{\"end\":192,\"start\":187},{\"end\":312,\"start\":307}]", "author_affiliation": "[{\"end\":185,\"start\":102},{\"end\":305,\"start\":222},{\"end\":428,\"start\":345}]", "title": "[{\"end\":76,\"start\":1},{\"end\":505,\"start\":430}]", "venue": null, "abstract": "[{\"end\":1930,\"start\":840}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3786,\"start\":3783},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4468,\"start\":4464},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4471,\"start\":4468},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5035,\"start\":5032},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5038,\"start\":5035},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5041,\"start\":5038},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5043,\"start\":5041},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5190,\"start\":5187},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5491,\"start\":5487},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5841,\"start\":5837},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5844,\"start\":5841},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5847,\"start\":5844},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5850,\"start\":5847},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5853,\"start\":5850},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6219,\"start\":6215},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6222,\"start\":6219},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6339,\"start\":6336},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6371,\"start\":6367},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6374,\"start\":6371},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6602,\"start\":6599},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6759,\"start\":6756},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6912,\"start\":6909},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7037,\"start\":7033},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7423,\"start\":7419},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7584,\"start\":7581},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7751,\"start\":7747},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8151,\"start\":8147},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8313,\"start\":8309},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11710,\"start\":11706},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12392,\"start\":12389},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12394,\"start\":12392},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12397,\"start\":12394},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13549,\"start\":13546},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13551,\"start\":13549},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14578,\"start\":14575},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14581,\"start\":14578},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15527,\"start\":15524},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16406,\"start\":16403},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18200,\"start\":18196},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18202,\"start\":18200},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18532,\"start\":18528},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18534,\"start\":18532},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18600,\"start\":18597},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18670,\"start\":18666},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20305,\"start\":20302},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20308,\"start\":20305},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20514,\"start\":20511},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20609,\"start\":20606},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20665,\"start\":20661},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20727,\"start\":20723},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22181,\"start\":22178},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22542,\"start\":22538},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22612,\"start\":22608},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24608,\"start\":24604},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25117,\"start\":25114},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25190,\"start\":25187},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25472,\"start\":25469},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31442,\"start\":31439},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31444,\"start\":31442}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37804,\"start\":37695},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37967,\"start\":37805},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38046,\"start\":37968},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38428,\"start\":38047},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38540,\"start\":38429},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39083,\"start\":38541},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39133,\"start\":39084},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39192,\"start\":39134},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":39278,\"start\":39193},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":39775,\"start\":39279},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":39888,\"start\":39776},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":40508,\"start\":39889},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":40576,\"start\":40509}]", "paragraph": "[{\"end\":2267,\"start\":1946},{\"end\":3108,\"start\":2269},{\"end\":3848,\"start\":3110},{\"end\":4397,\"start\":3850},{\"end\":8567,\"start\":4414},{\"end\":9484,\"start\":8620},{\"end\":9652,\"start\":9486},{\"end\":10415,\"start\":9694},{\"end\":10663,\"start\":10556},{\"end\":11507,\"start\":10665},{\"end\":11598,\"start\":11509},{\"end\":12222,\"start\":11636},{\"end\":12639,\"start\":12304},{\"end\":12813,\"start\":12716},{\"end\":13117,\"start\":12879},{\"end\":13385,\"start\":13221},{\"end\":13553,\"start\":13387},{\"end\":14135,\"start\":13659},{\"end\":14195,\"start\":14137},{\"end\":14405,\"start\":14290},{\"end\":14693,\"start\":14468},{\"end\":15193,\"start\":14798},{\"end\":15634,\"start\":15249},{\"end\":15904,\"start\":15753},{\"end\":16027,\"start\":15906},{\"end\":16345,\"start\":16070},{\"end\":16632,\"start\":16376},{\"end\":16727,\"start\":16719},{\"end\":17071,\"start\":16786},{\"end\":17264,\"start\":17201},{\"end\":17371,\"start\":17266},{\"end\":19188,\"start\":17398},{\"end\":19514,\"start\":19190},{\"end\":19663,\"start\":19609},{\"end\":19864,\"start\":19690},{\"end\":20329,\"start\":19866},{\"end\":21941,\"start\":20403},{\"end\":23263,\"start\":21957},{\"end\":25084,\"start\":23265},{\"end\":27319,\"start\":25086},{\"end\":29659,\"start\":27321},{\"end\":30835,\"start\":29661},{\"end\":31423,\"start\":30850},{\"end\":31640,\"start\":31425},{\"end\":31867,\"start\":31712},{\"end\":31948,\"start\":31907},{\"end\":32278,\"start\":32037},{\"end\":32534,\"start\":32385},{\"end\":32686,\"start\":32658},{\"end\":32993,\"start\":32777},{\"end\":33396,\"start\":33111},{\"end\":33535,\"start\":33406},{\"end\":33934,\"start\":33771},{\"end\":35034,\"start\":34466},{\"end\":36074,\"start\":35078},{\"end\":37331,\"start\":36193},{\"end\":37694,\"start\":37363}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10555,\"start\":10416},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12303,\"start\":12223},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12715,\"start\":12640},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12878,\"start\":12814},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13220,\"start\":13118},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13658,\"start\":13554},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14289,\"start\":14196},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14467,\"start\":14406},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14797,\"start\":14694},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15217,\"start\":15194},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15752,\"start\":15635},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16069,\"start\":16028},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16375,\"start\":16346},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16718,\"start\":16633},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16785,\"start\":16728},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17200,\"start\":17072},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19608,\"start\":19515},{\"attributes\":{\"id\":\"formula_17\"},\"end\":20402,\"start\":20330},{\"attributes\":{\"id\":\"formula_18\"},\"end\":31711,\"start\":31641},{\"attributes\":{\"id\":\"formula_19\"},\"end\":31906,\"start\":31868},{\"attributes\":{\"id\":\"formula_20\"},\"end\":32036,\"start\":31949},{\"attributes\":{\"id\":\"formula_21\"},\"end\":32384,\"start\":32279},{\"attributes\":{\"id\":\"formula_22\"},\"end\":32657,\"start\":32535},{\"attributes\":{\"id\":\"formula_23\"},\"end\":32776,\"start\":32687},{\"attributes\":{\"id\":\"formula_24\"},\"end\":33066,\"start\":32994},{\"attributes\":{\"id\":\"formula_25\"},\"end\":33770,\"start\":33536},{\"attributes\":{\"id\":\"formula_26\"},\"end\":34465,\"start\":33935}]", "table_ref": "[{\"end\":23507,\"start\":23500},{\"end\":24007,\"start\":24000},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25261,\"start\":25254},{\"end\":25785,\"start\":25778},{\"end\":25793,\"start\":25786},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26489,\"start\":26482},{\"end\":27805,\"start\":27798},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27844,\"start\":27837},{\"end\":28112,\"start\":28105},{\"end\":29428,\"start\":29413},{\"end\":29470,\"start\":29455},{\"end\":29537,\"start\":29530},{\"end\":29749,\"start\":29742},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":33395,\"start\":33388},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":33449,\"start\":33442},{\"end\":33882,\"start\":33875},{\"end\":34734,\"start\":34727}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1944,\"start\":1932},{\"attributes\":{\"n\":\"2.\"},\"end\":4412,\"start\":4400},{\"attributes\":{\"n\":\"3.\"},\"end\":8618,\"start\":8570},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9692,\"start\":9655},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11634,\"start\":11601},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15247,\"start\":15219},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17396,\"start\":17374},{\"attributes\":{\"n\":\"3.5.\"},\"end\":19688,\"start\":19666},{\"attributes\":{\"n\":\"4.\"},\"end\":21955,\"start\":21944},{\"attributes\":{\"n\":\"5.\"},\"end\":30848,\"start\":30838},{\"end\":33109,\"start\":33068},{\"end\":33404,\"start\":33399},{\"end\":35076,\"start\":35037},{\"end\":36125,\"start\":36077},{\"end\":36191,\"start\":36128},{\"end\":37361,\"start\":37334},{\"end\":37706,\"start\":37696},{\"end\":37816,\"start\":37806},{\"end\":37979,\"start\":37969},{\"end\":38058,\"start\":38048},{\"end\":38440,\"start\":38430},{\"end\":39094,\"start\":39085},{\"end\":39144,\"start\":39135},{\"end\":39203,\"start\":39194},{\"end\":39289,\"start\":39280},{\"end\":39786,\"start\":39777},{\"end\":39899,\"start\":39890},{\"end\":40520,\"start\":40510}]", "table": "[{\"end\":39083,\"start\":38615},{\"end\":39278,\"start\":39205},{\"end\":39775,\"start\":39558},{\"end\":40508,\"start\":40345}]", "figure_caption": "[{\"end\":37804,\"start\":37708},{\"end\":37967,\"start\":37818},{\"end\":38046,\"start\":37981},{\"end\":38428,\"start\":38060},{\"end\":38540,\"start\":38442},{\"end\":38615,\"start\":38543},{\"end\":39133,\"start\":39096},{\"end\":39192,\"start\":39146},{\"end\":39558,\"start\":39291},{\"end\":39888,\"start\":39788},{\"end\":40345,\"start\":39901},{\"end\":40576,\"start\":40523}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8943,\"start\":8935},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10839,\"start\":10831},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25848,\"start\":25839},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27466,\"start\":27458},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28475,\"start\":28467},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29126,\"start\":29118},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30206,\"start\":30198},{\"end\":33481,\"start\":33473},{\"end\":33782,\"start\":33774},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":35128,\"start\":35120},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35231,\"start\":35222},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36300,\"start\":36291},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":36499,\"start\":36491},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36774,\"start\":36766},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":36813,\"start\":36805},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37136,\"start\":37128}]", "bib_author_first_name": "[{\"end\":40756,\"start\":40755},{\"end\":40765,\"start\":40764},{\"end\":40773,\"start\":40772},{\"end\":40787,\"start\":40786},{\"end\":40800,\"start\":40799},{\"end\":40811,\"start\":40810},{\"end\":41032,\"start\":41031},{\"end\":41044,\"start\":41043},{\"end\":41053,\"start\":41052},{\"end\":41068,\"start\":41067},{\"end\":41078,\"start\":41077},{\"end\":41087,\"start\":41086},{\"end\":41096,\"start\":41095},{\"end\":41098,\"start\":41097},{\"end\":41108,\"start\":41107},{\"end\":41119,\"start\":41118},{\"end\":41129,\"start\":41128},{\"end\":41449,\"start\":41448},{\"end\":41459,\"start\":41458},{\"end\":41468,\"start\":41467},{\"end\":41477,\"start\":41476},{\"end\":41488,\"start\":41487},{\"end\":41501,\"start\":41500},{\"end\":41513,\"start\":41512},{\"end\":41523,\"start\":41522},{\"end\":41531,\"start\":41530},{\"end\":41793,\"start\":41792},{\"end\":41811,\"start\":41810},{\"end\":42033,\"start\":42032},{\"end\":42040,\"start\":42039},{\"end\":42321,\"start\":42320},{\"end\":42328,\"start\":42327},{\"end\":42571,\"start\":42570},{\"end\":42578,\"start\":42577},{\"end\":42893,\"start\":42892},{\"end\":42903,\"start\":42902},{\"end\":42914,\"start\":42913},{\"end\":43099,\"start\":43098},{\"end\":43110,\"start\":43109},{\"end\":43300,\"start\":43299},{\"end\":43306,\"start\":43305},{\"end\":43313,\"start\":43312},{\"end\":43319,\"start\":43318},{\"end\":43327,\"start\":43326},{\"end\":43629,\"start\":43628},{\"end\":43631,\"start\":43630},{\"end\":43641,\"start\":43640},{\"end\":43651,\"start\":43650},{\"end\":43653,\"start\":43652},{\"end\":43998,\"start\":43997},{\"end\":44009,\"start\":44008},{\"end\":44272,\"start\":44271},{\"end\":44279,\"start\":44278},{\"end\":44286,\"start\":44285},{\"end\":44467,\"start\":44466},{\"end\":44477,\"start\":44476},{\"end\":44610,\"start\":44609},{\"end\":44612,\"start\":44611},{\"end\":44618,\"start\":44617},{\"end\":44620,\"start\":44619},{\"end\":44629,\"start\":44628},{\"end\":44802,\"start\":44801},{\"end\":44809,\"start\":44808},{\"end\":44821,\"start\":44820},{\"end\":44832,\"start\":44831},{\"end\":44843,\"start\":44842},{\"end\":45165,\"start\":45164},{\"end\":45167,\"start\":45166},{\"end\":45426,\"start\":45425},{\"end\":45428,\"start\":45427},{\"end\":45438,\"start\":45437},{\"end\":45440,\"start\":45439},{\"end\":45788,\"start\":45787},{\"end\":45790,\"start\":45789},{\"end\":46014,\"start\":46013},{\"end\":46016,\"start\":46015},{\"end\":46023,\"start\":46022},{\"end\":46025,\"start\":46024},{\"end\":46264,\"start\":46263},{\"end\":46266,\"start\":46265},{\"end\":46544,\"start\":46543},{\"end\":46554,\"start\":46553},{\"end\":46751,\"start\":46750},{\"end\":46764,\"start\":46763},{\"end\":46777,\"start\":46776},{\"end\":46786,\"start\":46785},{\"end\":46984,\"start\":46983},{\"end\":46995,\"start\":46994},{\"end\":47181,\"start\":47180},{\"end\":47189,\"start\":47188},{\"end\":47199,\"start\":47198},{\"end\":47212,\"start\":47211},{\"end\":47417,\"start\":47416},{\"end\":47429,\"start\":47428},{\"end\":47435,\"start\":47434},{\"end\":47437,\"start\":47436},{\"end\":47447,\"start\":47446},{\"end\":47667,\"start\":47666},{\"end\":47673,\"start\":47672},{\"end\":47680,\"start\":47679},{\"end\":47686,\"start\":47685},{\"end\":47877,\"start\":47876},{\"end\":47890,\"start\":47889},{\"end\":47892,\"start\":47891},{\"end\":47900,\"start\":47899},{\"end\":47902,\"start\":47901},{\"end\":47911,\"start\":47910},{\"end\":47913,\"start\":47912},{\"end\":48100,\"start\":48099},{\"end\":48109,\"start\":48108},{\"end\":48119,\"start\":48118},{\"end\":48314,\"start\":48313},{\"end\":48323,\"start\":48322},{\"end\":48335,\"start\":48334},{\"end\":48545,\"start\":48544},{\"end\":48553,\"start\":48552},{\"end\":48561,\"start\":48560}]", "bib_author_last_name": "[{\"end\":40762,\"start\":40757},{\"end\":40770,\"start\":40766},{\"end\":40784,\"start\":40774},{\"end\":40797,\"start\":40788},{\"end\":40808,\"start\":40801},{\"end\":40820,\"start\":40812},{\"end\":41041,\"start\":41033},{\"end\":41050,\"start\":41045},{\"end\":41065,\"start\":41054},{\"end\":41075,\"start\":41069},{\"end\":41084,\"start\":41079},{\"end\":41093,\"start\":41088},{\"end\":41105,\"start\":41099},{\"end\":41116,\"start\":41109},{\"end\":41126,\"start\":41120},{\"end\":41135,\"start\":41130},{\"end\":41456,\"start\":41450},{\"end\":41465,\"start\":41460},{\"end\":41474,\"start\":41469},{\"end\":41485,\"start\":41478},{\"end\":41498,\"start\":41489},{\"end\":41510,\"start\":41502},{\"end\":41520,\"start\":41514},{\"end\":41528,\"start\":41524},{\"end\":41539,\"start\":41532},{\"end\":41808,\"start\":41794},{\"end\":41821,\"start\":41812},{\"end\":42037,\"start\":42034},{\"end\":42051,\"start\":42041},{\"end\":42325,\"start\":42322},{\"end\":42339,\"start\":42329},{\"end\":42575,\"start\":42572},{\"end\":42589,\"start\":42579},{\"end\":42900,\"start\":42894},{\"end\":42911,\"start\":42904},{\"end\":42921,\"start\":42915},{\"end\":43107,\"start\":43100},{\"end\":43117,\"start\":43111},{\"end\":43303,\"start\":43301},{\"end\":43310,\"start\":43307},{\"end\":43316,\"start\":43314},{\"end\":43324,\"start\":43320},{\"end\":43335,\"start\":43328},{\"end\":43638,\"start\":43632},{\"end\":43648,\"start\":43642},{\"end\":43661,\"start\":43654},{\"end\":44006,\"start\":43999},{\"end\":44013,\"start\":44010},{\"end\":44276,\"start\":44273},{\"end\":44283,\"start\":44280},{\"end\":44291,\"start\":44287},{\"end\":44474,\"start\":44468},{\"end\":44480,\"start\":44478},{\"end\":44615,\"start\":44613},{\"end\":44626,\"start\":44621},{\"end\":44634,\"start\":44630},{\"end\":44806,\"start\":44803},{\"end\":44818,\"start\":44810},{\"end\":44829,\"start\":44822},{\"end\":44840,\"start\":44833},{\"end\":44849,\"start\":44844},{\"end\":45174,\"start\":45168},{\"end\":45435,\"start\":45429},{\"end\":45448,\"start\":45441},{\"end\":45795,\"start\":45791},{\"end\":46020,\"start\":46017},{\"end\":46033,\"start\":46026},{\"end\":46276,\"start\":46267},{\"end\":46551,\"start\":46545},{\"end\":46562,\"start\":46555},{\"end\":46761,\"start\":46752},{\"end\":46774,\"start\":46765},{\"end\":46783,\"start\":46778},{\"end\":46795,\"start\":46787},{\"end\":46992,\"start\":46985},{\"end\":47000,\"start\":46996},{\"end\":47186,\"start\":47182},{\"end\":47196,\"start\":47190},{\"end\":47209,\"start\":47200},{\"end\":47220,\"start\":47213},{\"end\":47426,\"start\":47418},{\"end\":47432,\"start\":47430},{\"end\":47444,\"start\":47438},{\"end\":47454,\"start\":47448},{\"end\":47670,\"start\":47668},{\"end\":47677,\"start\":47674},{\"end\":47683,\"start\":47681},{\"end\":47694,\"start\":47687},{\"end\":47887,\"start\":47878},{\"end\":47897,\"start\":47893},{\"end\":47908,\"start\":47903},{\"end\":47918,\"start\":47914},{\"end\":48106,\"start\":48101},{\"end\":48116,\"start\":48110},{\"end\":48127,\"start\":48120},{\"end\":48320,\"start\":48315},{\"end\":48332,\"start\":48324},{\"end\":48343,\"start\":48336},{\"end\":48550,\"start\":48546},{\"end\":48558,\"start\":48554},{\"end\":48566,\"start\":48562}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9854676},\"end\":40986,\"start\":40695},{\"attributes\":{\"doi\":\"arXiv:1604.07316\",\"id\":\"b1\"},\"end\":41383,\"start\":40988},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":502946},\"end\":41732,\"start\":41385},{\"attributes\":{\"id\":\"b3\"},\"end\":41941,\"start\":41734},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2682206},\"end\":42232,\"start\":41943},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":160705},\"end\":42490,\"start\":42234},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15953218},\"end\":42834,\"start\":42492},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206741496},\"end\":43052,\"start\":42836},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":29333691},\"end\":43253,\"start\":43054},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11926924},\"end\":43535,\"start\":43255},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6660826},\"end\":43917,\"start\":43537},{\"attributes\":{\"doi\":\"arXiv:1703.04977\",\"id\":\"b11\"},\"end\":44199,\"start\":43919},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13932903},\"end\":44414,\"start\":44201},{\"attributes\":{\"id\":\"b13\"},\"end\":44562,\"start\":44416},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2891780},\"end\":44739,\"start\":44564},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3073351},\"end\":45101,\"start\":44741},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16543854},\"end\":45316,\"start\":45103},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9469305},\"end\":45746,\"start\":45318},{\"attributes\":{\"id\":\"b18\"},\"end\":45938,\"start\":45748},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":117583961},\"end\":46205,\"start\":45940},{\"attributes\":{\"id\":\"b20\"},\"end\":46504,\"start\":46207},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10081656},\"end\":46671,\"start\":46506},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3150075},\"end\":46951,\"start\":46673},{\"attributes\":{\"doi\":\"arXiv:1608.01230\",\"id\":\"b23\"},\"end\":47126,\"start\":46953},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":338287},\"end\":47348,\"start\":47128},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12700980},\"end\":47593,\"start\":47350},{\"attributes\":{\"id\":\"b26\"},\"end\":47823,\"start\":47595},{\"attributes\":{\"id\":\"b27\"},\"end\":48033,\"start\":47825},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2111429},\"end\":48254,\"start\":48035},{\"attributes\":{\"id\":\"b29\"},\"end\":48454,\"start\":48256},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4231589},\"end\":48709,\"start\":48456}]", "bib_title": "[{\"end\":40753,\"start\":40695},{\"end\":41446,\"start\":41385},{\"end\":42030,\"start\":41943},{\"end\":42318,\"start\":42234},{\"end\":42568,\"start\":42492},{\"end\":42890,\"start\":42836},{\"end\":43096,\"start\":43054},{\"end\":43297,\"start\":43255},{\"end\":43626,\"start\":43537},{\"end\":44269,\"start\":44201},{\"end\":44607,\"start\":44564},{\"end\":44799,\"start\":44741},{\"end\":45162,\"start\":45103},{\"end\":45423,\"start\":45318},{\"end\":46011,\"start\":45940},{\"end\":46541,\"start\":46506},{\"end\":46748,\"start\":46673},{\"end\":47178,\"start\":47128},{\"end\":47414,\"start\":47350},{\"end\":48097,\"start\":48035},{\"end\":48542,\"start\":48456}]", "bib_author": "[{\"end\":40764,\"start\":40755},{\"end\":40772,\"start\":40764},{\"end\":40786,\"start\":40772},{\"end\":40799,\"start\":40786},{\"end\":40810,\"start\":40799},{\"end\":40822,\"start\":40810},{\"end\":41043,\"start\":41031},{\"end\":41052,\"start\":41043},{\"end\":41067,\"start\":41052},{\"end\":41077,\"start\":41067},{\"end\":41086,\"start\":41077},{\"end\":41095,\"start\":41086},{\"end\":41107,\"start\":41095},{\"end\":41118,\"start\":41107},{\"end\":41128,\"start\":41118},{\"end\":41137,\"start\":41128},{\"end\":41458,\"start\":41448},{\"end\":41467,\"start\":41458},{\"end\":41476,\"start\":41467},{\"end\":41487,\"start\":41476},{\"end\":41500,\"start\":41487},{\"end\":41512,\"start\":41500},{\"end\":41522,\"start\":41512},{\"end\":41530,\"start\":41522},{\"end\":41541,\"start\":41530},{\"end\":41810,\"start\":41792},{\"end\":41823,\"start\":41810},{\"end\":42039,\"start\":42032},{\"end\":42053,\"start\":42039},{\"end\":42327,\"start\":42320},{\"end\":42341,\"start\":42327},{\"end\":42577,\"start\":42570},{\"end\":42591,\"start\":42577},{\"end\":42902,\"start\":42892},{\"end\":42913,\"start\":42902},{\"end\":42923,\"start\":42913},{\"end\":43109,\"start\":43098},{\"end\":43119,\"start\":43109},{\"end\":43305,\"start\":43299},{\"end\":43312,\"start\":43305},{\"end\":43318,\"start\":43312},{\"end\":43326,\"start\":43318},{\"end\":43337,\"start\":43326},{\"end\":43640,\"start\":43628},{\"end\":43650,\"start\":43640},{\"end\":43663,\"start\":43650},{\"end\":44008,\"start\":43997},{\"end\":44015,\"start\":44008},{\"end\":44278,\"start\":44271},{\"end\":44285,\"start\":44278},{\"end\":44293,\"start\":44285},{\"end\":44476,\"start\":44466},{\"end\":44482,\"start\":44476},{\"end\":44617,\"start\":44609},{\"end\":44628,\"start\":44617},{\"end\":44636,\"start\":44628},{\"end\":44808,\"start\":44801},{\"end\":44820,\"start\":44808},{\"end\":44831,\"start\":44820},{\"end\":44842,\"start\":44831},{\"end\":44851,\"start\":44842},{\"end\":45176,\"start\":45164},{\"end\":45437,\"start\":45425},{\"end\":45450,\"start\":45437},{\"end\":45797,\"start\":45787},{\"end\":46022,\"start\":46013},{\"end\":46035,\"start\":46022},{\"end\":46278,\"start\":46263},{\"end\":46553,\"start\":46543},{\"end\":46564,\"start\":46553},{\"end\":46763,\"start\":46750},{\"end\":46776,\"start\":46763},{\"end\":46785,\"start\":46776},{\"end\":46797,\"start\":46785},{\"end\":46994,\"start\":46983},{\"end\":47002,\"start\":46994},{\"end\":47188,\"start\":47180},{\"end\":47198,\"start\":47188},{\"end\":47211,\"start\":47198},{\"end\":47222,\"start\":47211},{\"end\":47428,\"start\":47416},{\"end\":47434,\"start\":47428},{\"end\":47446,\"start\":47434},{\"end\":47456,\"start\":47446},{\"end\":47672,\"start\":47666},{\"end\":47679,\"start\":47672},{\"end\":47685,\"start\":47679},{\"end\":47696,\"start\":47685},{\"end\":47889,\"start\":47876},{\"end\":47899,\"start\":47889},{\"end\":47910,\"start\":47899},{\"end\":47920,\"start\":47910},{\"end\":48108,\"start\":48099},{\"end\":48118,\"start\":48108},{\"end\":48129,\"start\":48118},{\"end\":48322,\"start\":48313},{\"end\":48334,\"start\":48322},{\"end\":48345,\"start\":48334},{\"end\":48552,\"start\":48544},{\"end\":48560,\"start\":48552},{\"end\":48568,\"start\":48560}]", "bib_venue": "[{\"end\":40826,\"start\":40822},{\"end\":41029,\"start\":40988},{\"end\":41545,\"start\":41541},{\"end\":41790,\"start\":41734},{\"end\":42072,\"start\":42053},{\"end\":42345,\"start\":42341},{\"end\":42640,\"start\":42591},{\"end\":42932,\"start\":42923},{\"end\":43136,\"start\":43119},{\"end\":43374,\"start\":43337},{\"end\":43698,\"start\":43663},{\"end\":43995,\"start\":43919},{\"end\":44297,\"start\":44293},{\"end\":44464,\"start\":44416},{\"end\":44640,\"start\":44636},{\"end\":44904,\"start\":44851},{\"end\":45194,\"start\":45176},{\"end\":45512,\"start\":45450},{\"end\":45785,\"start\":45748},{\"end\":46050,\"start\":46035},{\"end\":46261,\"start\":46207},{\"end\":46578,\"start\":46564},{\"end\":46801,\"start\":46797},{\"end\":46981,\"start\":46953},{\"end\":47226,\"start\":47222},{\"end\":47460,\"start\":47456},{\"end\":47664,\"start\":47595},{\"end\":47874,\"start\":47825},{\"end\":48133,\"start\":48129},{\"end\":48311,\"start\":48256},{\"end\":48572,\"start\":48568}]"}}}, "year": 2023, "month": 12, "day": 17}