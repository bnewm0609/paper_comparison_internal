{"id": 235813699, "updated": "2022-09-30 03:20:26.99", "metadata": {"title": "Joint Resource Allocation for Efficient Federated Learning in Internet of Things Supported by Edge Computing", "authors": "[{\"first\":\"Jianyang\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Junshuai\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Tian\",\"middle\":[]},{\"first\":\"Wanli\",\"last\":\"Ni\",\"middle\":[]},{\"first\":\"Gaofeng\",\"last\":\"Nie\",\"middle\":[]},{\"first\":\"Yingying\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "2021 IEEE International Conference on Communications Workshops (ICC Workshops)", "journal": "2021 IEEE International Conference on Communications Workshops (ICC Workshops)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Federated learning (FL) and edge computing are both important technologies to support the future Internet of Things (IoT). Despite that the network supported by edge computing has great potential to promote FL, it is more challenging to achieve efficient FL due to more complex resource coupling in it. Focus on this problem, we formulate a problem which minimizes the weighted sum of system cost and learning cost by jointly optimizing bandwidth, computation frequency, transmission power allocation and subcarrier assignment. In order to solve this mixed-integer non-linear problem, we first decouple the bandwidth allocation subproblem from the original problem and obtain a closed-form solution. Further considering the remaining joint optimization problem of computation frequency, transmission power and subcarrier, an iterative algorithm with polynomial time complexity is designed. In an iteration, the latency and computation frequency optimization subproblem and transmission power and subcarrier optimization subproblem are solved using the proposed algorithms in turn. The iterative algorithm is repeated until convergence. Finally, to verify the performance of the algorithm, we compare the proposed algorithm with five baselines. Numerical results show the significant performance gain and the robustness of the proposed algorithm.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icc/RenS0NNW21", "doi": "10.1109/iccworkshops50388.2021.9473734"}}, "content": {"source": {"pdf_hash": "c24a227a1cdfef623b044106267e236a7ebb9e7f", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "55d3f4de7b6f3d0204ac116e1bca48a0738aa776", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c24a227a1cdfef623b044106267e236a7ebb9e7f.txt", "contents": "\nJoint Resource Allocation for Efficient Federated Learning in Internet of Things Supported by Edge Computing\n\n\nJianyang Ren renjianyang@bupt.edu.cn \nJunshuai Sun \nSchool of Computer Science and Technology\nXidian University\nXi'anChina\n\nChina Mobile Research Institute\nBeijingChina\n\nHui Tian \nWanli Ni \nGaofeng Nie \nYingying Wang \nChina Mobile Research Institute\nBeijingChina\n\n\nState Key Laboratory of Networking and Switching Technology\nBeijing University of Posts and Telecommunications\n100876BeijingChina\n\nJoint Resource Allocation for Efficient Federated Learning in Internet of Things Supported by Edge Computing\n10.1109/ICCWorkshops50388.2021.9473734\nFederated learning (FL) and edge computing are both important technologies to support the future Internet of Things (IoT). Despite that the network supported by edge computing has great potential to promote FL, it is more challenging to achieve efficient FL due to more complex resource coupling in it. Focus on this problem, we formulate a problem which minimizes the weighted sum of system cost and learning cost by jointly optimizing bandwidth, computation frequency, transmission power allocation and subcarrier assignment. In order to solve this mixed-integer non-linear problem, we first decouple the bandwidth allocation subproblem from the original problem and obtain a closed-form solution. Further considering the remaining joint optimization problem of computation frequency, transmission power and subcarrier, an iterative algorithm with polynomial time complexity is designed. In an iteration, the latency and computation frequency optimization subproblem and transmission power and subcarrier optimization subproblem are solved using the proposed algorithms in turn. The iterative algorithm is repeated until convergence. Finally, to verify the performance of the algorithm, we compare the proposed algorithm with five baselines. Numerical results show the significant performance gain and the robustness of the proposed algorithm.978-1-7281-9441-7/21/$31.00 \u00a92021 IEEE\n\nI. INTRODUCTION\n\nThe past few years have witnessed the evolution from a smartphone-centric network to the Internet of Things (IoT) [1], which enables numerous resource-limited devices to be connected with each other via the Internet [2]. At the same time, artificial intelligence has gradually become a key force to promote the society into the intelligent era [3]. In fact, Machine Learning (ML) can help process the huge amounts of data created by IoT devices and use its technologies and ideas to deliver the desired results [4]. However, traditional ML framework mainly focus on centralized data processing which poses great challenges to data privacy protection and causes network congestion. For many emerging application scenarios in the future, these problems should not be ignored [5]. Therefore, a decentralized ML paradigm called federated learning (FL) is widely regarded as an attractive approach [6], [7]. By uploading only local model parameters for parameter aggregation, it avoids the problems of privacy leakage and network transmission pressure caused by raw data transmission. Unfortunately, FL also has its own challenges. First, the performance of FL is closely related to resource allocation. Second, a collaborative FL model for different participants must consider the validity of the model parameters received from them [8].\n\nDue to the limited resources for computation, storage and communication of most IoT devices [9] and the fast task offloading offered by Mobile Edge Computing (MEC), MEC becomes a key technology to support latency-critical mobile and IoT applications [10]. In fact, it is also very promising to introduce edge servers into IoT to promote federated learning. First, the long local training delays of resource-constrained sensors can be mitigated, since newly introduced edge servers can collect sensor data quickly and complete local model training using powerful computing capability. Second, the lifetime of the sensors can also be extended due to the reduction in energy consumption of local model training. Last but not least, with the higher power budgets of SBSs, the impact of transmission errors on FL can also be reduced. However, to take full use of these advantages and achieve efficient FL, careful allocation of resources is still necessary.\n\nRecently, federated learning in edge computing supported networks is becoming more and more widely studied [8], [11]- [13]. To realize green federated learning, Yang et. al [11] minimized the total energy consumption of all edge devices with latency constraints. To solve the formulated problem, an initial feasible solution search scheme and an iterative algorithm are designed. The hierarchical federated learning frameworks are considered in [8], [12], [13]. To minimize the loss function under the constraint of resource budget, Wang et. al [12] proposed a control algorithm which can adaptively adjust the frequency of global aggregation. Further adding the latency of federated learning into the optimization objective, a novel system model, namely, DFL was devised in [8] for the cognitive Internet of Things (C-IoT). An integer linear programming problem was constructed by the authors to minimize the global FL cost. Based on problem decomposition and variable relaxation, the optimization problem was transformed into two low-complexity convex optimization subproblems and solved iteratively. While both the importance of latency and energy consumption was considered in [13]. Specifically, Luo et. al [13] proposed a communication resource optimization scheme minimizing the energy consumption and latency  within one global iteration under the proposed hierarchical federated learning framework (HFEL). The above studies cover a variety of network scenarios. However, none of these studies jointly considered the latency, energy consumption and learning performance in federated learning processes. In addition, their algorithms may not work effectively for IoT scenarios with sensors with limited resources due to the long local training latency of sensors. The main contributions of this paper are summarized as follows: 1) considering federated learning scenario in IoT supported by edge computing, we formulate a mixed integer nonlinear programming problem (MINLP) to minimize the weighted sum of system cost and learning cost; 2) we simplify the original NP-hard problem by problem decomposition and design an effective algorithm for each subproblem; 3) numerical results show that the proposed algorithm can achieve better performance than benchmark schemes.\n\n\nII. SYSTEM MODEL AND PROBLEM FORMULATION\n\nAs illustrated in Fig. 1, we consider a network that consists of one macro base station (MBS), J small base stations (SBS) co-located with MEC servers and K wireless sensors. The sets of SBSs and sensors are denoted by J = {1, 2, . . . , J} and K = {1, 2, . . . , K}, respectively. The set of senseors served by SBS j is indexed by S j that satisfies S i \u2229 S j = \u2205, (i = j) and \u222a j S j = K. Besides, all channels considered in this paper are regarded as Rayleigh fading channels.\n\nEach SBS can be considered to be deployed within an isolated enterprise or organization, so the sensors served by the same SBS do not have the problem of privacy leakage when transmitting data through the wireless link. To complete one model aggregation, the FL procedure in our network contains three steps: 1) Data Collection: SBSs receive data uploaded by sensors within their range. 2) Local Computation: MEC servers use the received data to train their local models. 3) Aggregation and Broadcast: Each SBS uploads its trained local model to MBS for model aggregation, then the aggregated model is broadcasted in the downlink.\n\n\nA. Data Collection\n\nEach wireless sensor is connected to its SBS via frequency domain multiple access (FDMA). Thus, the achievable transmission rate of sensor k served by SBS j can be formulated by\nR k = B s k log 2 1 + p max h s k \u03c3 2 1 ,(1)\nwhere B s k is the bandwidth allocated to sensor k, p max is the maximum transmission power which is supposed same for all sensors, h s k is the channel gain from sensor k to SBS j, and \u03c3 2 1 is the noise power of the channel. Considering the bandwidth constraint of SBS j, we can get: k\u2208Sj B s k \u2264 B j , where B j is its total bandwidth. Let t k denote the latency for sensor k to transmit its loacl data set D s k with a total data amount of D s k to SBS j. Thus t k can be characteried by\nt k = D s k R k .(2)\nConsidering a synchronous federated learning scenario, where all the sensors in our network start transmitting data at the same time, the time taken by SBS j to receive all data completely is\nt r j = max k\u2208Sj (t k ) .(3)\nFrom the perspective of network operators, the energy consumption of sensors for data uploading is not considered in this paper.\n\n\nB. Local Computation\n\nAs soon as receiving all data samples, the MEC server begins training the local model. The total data amount collected by SBS j, denoted by D j , can be expressed as\nD j = k\u2208Sj D s k .(4)\nLet \u03b5 (cycles/bit) represent the number of CPU cycles required for computing one bit data. Thus, the time and energy consumed by MEC server j in the process of training model can be respectively denoted by\nt cmp j = \u03b5D j f j ,(5)e cmp j = \u03ba\u03b5D j (f j ) 2 ,(6)\nwhere f j is the CPU frequency of MEC server j and \u03ba denotes the effective switched capacitance which is a positive constant and only depends on the structure of the chip.\n\n\nC. Aggregation and Broadcast\n\nAfter training, each SBS will send the learned local model to MBS for aggregation with the models of other SBSs. For SBSs communicating with MBS, the total bandwidth of MBS, B m , is divided equally into J orthogonal subcarriers and each SBS uses one subcarrier of bandwidth B = B m /J. To avoid interference, each subcarrier is also selected for use by only one SBS. The subcarrier allocation state is indicated by C j,n \u2208 {0, 1}. Specifically, if the n-th subcarrier is allocated to SBS j, C j,n = 1, otherwise C j,n = 0. Therefore, the above description of the subcarrier allocation scheme can be expressed as follows:\nJ j=1 C j,n = 1, \u2200n; (7) J n=1 C j,n = 1, \u2200j.(8)\nLet p j,n denote the uplink transmission power of SBS j on the n-th subcarrier. Note that each SBS uses the non-zero power for transmission only on the selected subcarrier, so the transmission power of SBS j can be simplified as p j . With p j , the data rate of SBS j for uploading local model can be expressed as\nR up j = J n=1 C j,n Blog 2 1 + p j h j,n \u03c3 2 2 ,(9)\nwhere h j,n is the uplink channel gain between SBS j and MBS on the n-th subcarrier, \u03c3 2 2 is the corresponding noise power. In the federated learning scenario, the models that need to be transmitted from participants are of similar size, denoted by D uniformly in this paper. Accordingly, model transmission time t up j and corresponding energy consumption e up j can be respectively represented as\nt up j = D R up j , e up j = p j t up j .(10)\nThen the latency t total j experienced by SBS j in completing one learning process from data collection to parameter uploading is as follow\nt total j = t r j + t cmp j + t up j .(11)\nDue to fast matrix operations and high bandwidth for model broadcasting, the latency of model aggregation at MBS and downlink broadcasting is neglected. It is also important to note that the random channel variations will cause packet errors which have significant degradation effect on the global FL model accuracy [8]. Therefore, it is beneficial to aggregate the only models that have no transmission errors. To study the effect of transmission errors on learning performance, the packet error rate is first given by [14] Error\nj = J n=1 C j,n 1 \u2212 exp \u2212m\u03c3 2 2 p j h j,n ,(12)\nwhere m represents the waterfall threshold. Further, let F (W) denote the global loss function of the FL algorithm. We assume it satisfies:\n1)F (W) is uniformly Lipschitz continuous with respect to W; 2)F (W) is strongly convex; 3)F (W) is twice-continuously differentiable; 4)with variables \u03be 1 , \u03be 2 \u2265 0, F (W) satisfies \u2207f (W t , x jn , y jn ) 2 \u2212 \u03be 2 \u2207F (W t ) 2 \u2264 \u03be 1 .\nThus, with reference to [15], our cost function C learn which counts for the effect of packet error rate on the FL performance is shown as\nC learn = j\u2208J D j Error j .(13)\nTo sum up, considering the straggler's dilemma issue of synchronous FL, the time taken to complete one model aggregation can be formulated as\nt one = max j\u2208J t total j .(14)\nAnd the corresponding total energy consumption, denoted by e one , is calculated as\ne one = j\u2208J e cmp j + e up j .(15)\n\nD. Problem Formulation\n\nTo achieve efficient FL, the latency, energy consumption and learning cost are all areas of focus. Therefore, this paper will comprehensively consider these three indicators. To simplify the notation, we define the system cost and total cost as\nC system = \u03b1t one + (1 \u2212 \u03b1) e one ,(16)C total = \u03c1C system + (1 \u2212 \u03c1) C learn ,(17)\nwhere \u03b1 \u2208 [0, 1] and \u03c1 \u2208 [0, 1] are system parameters, denote the trade-off between delay and energy consumption and the trade-off between system cost and learning cost, respectively. To minimize the total cost C total , the optimization problem is formulated as\nmin B,F,P,C C total (18a) s.t. k\u2208Sj B s k \u2264 B j , \u2200j,(18b)B s k \u2265 0, \u2200k, (18c) 0 \u2264 p j \u2264 p max j , \u2200j, (18d) J j=1 C j,n = 1, \u2200n, (18e) J n=1 C j,n = 1, \u2200j,(18f)C j,n \u2208 {0, 1}, \u2200j, n, (18g) 0 \u2264 f j \u2264 f max j , \u2200j,(18h)\nwhere B = [B s 1 , \u00b7 \u00b7 \u00b7 , B s K ] T , F = [f 1 , \u00b7 \u00b7 \u00b7 , f J ] T , P = [p 1 , \u00b7 \u00b7 \u00b7 , p J ] T , C = [C 1,1 , \u00b7 \u00b7 \u00b7 , C 1,n , \u00b7 \u00b7 \u00b7 , C J,J ] T . p max \n\n\nIII. PROBLEM SOLUTION\n\nTo solve optimization problems with multivariable coupling, problem decomposition is an effective method. Through analyzing the influence of each optimization variable and the coupling relationship between them, the bandwidth optimization subproblem is first decomposed from problem (18) and a closed-form solution is obtained. Then, for the remaining joint optimization subproblem, an iterative algorithm is proposed.\n\nIn an iteration, the latency and computation frequency optimization subproblem and transmission power and subcarrier optimization subproblem are solved effectively in turn.\n\n\nA. Sensor Bandwidth Optimization\n\nThrough analysis, we can find that the allocation of bandwidth to sensors will only affect the data reception time t r j (\u2200j). In fact, in order to shorten the time completing one model aggregation, any SBS tends to spend the shortest time receiving data. In addition, bandwidth allocation schemes of different SBSs are independent, so the bandwidth optimization subproblem for SBS j can be formulated as\nmin Bj t r j (19a) s.t. k\u2208Sj B s k \u2264 B j ,(19b)B s k \u2265 0, \u2200k \u2208 S j ,(19c)\nwhere B j = {B s k }, \u2200k \u2208 S j . Theorem 1: To minimize t r j , the optimal sensor bandwidth allocation for SBS j should satisfies\n\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 B s k * = Bj D s k /log 2 1+ p max h s k \u03c3 2 1 i\u2208S j D s i /log 2 1+ p max h s i \u03c3 2 1 , \u2200k \u2208 S j , a * j = i\u2208S j D s i /log 2 1+ p max h s i \u03c3 2 1\nBj .\n\n(20)\n\nProof: The proof is omitted due to the limited space. where a * j is a positive constant representing the optimal data reception time of SBS j.\n\nBy substituting t r j * = a * j into problem (18) and further introducing the additional variable T , the optimization problem can be simplified as:\nmin F,P,C,T \u03c1\u03b1T + \u03c1 (1 \u2212 \u03b1) e one + (1 \u2212 \u03c1)C learn (21a) s.t. (18d), (18e), (18f ), (18g), (18h), (21b) a * j + t cmp j + t up j \u2264 T, \u2200j.(21c)\nThe optimization problem is still a MINLP problem. To solve it, we will design an iterative algorithm, in which we first optimize (F, T) with fixed (P, C), then (P, C) is solved based on the obtained (F, T).\n\n\nB. Latency and Computation Frequency Optimization\n\nWhen the subcarrier allocation scheme C and the corresponding transmission power P are given, the problem (21) becomes\nmin F,T \u03c1\u03b1T + \u03c1 (1 \u2212 \u03b1) j\u2208J \u03ba\u03b5D j (f j ) 2 (22a) s.t. T \u2212 \u03b5D j f j \u2265 a * j + t up j , \u2200j,(22b)(18h).(22c)\nIt can be seen from (22a) that it is most effective for all servers to use the minimum computation frequency. From (22b), the minimum computation frequency of server j can be obtained\nf * j = \u03b5D j T \u2212 a * j \u2212 t up j .(23)\nSubstituting (23) into (22a), the problem is rewritten as\nmin T \u03c1\u03b1T + \u03c1 (1 \u2212 \u03b1) \u03ba\u03b5 3 j\u2208J (D j ) 3 T \u2212 a * j \u2212 t up j 2 (24a) s.t. T \u2265 T min def = max j\u2208J a * j + t up j + \u03b5D j f max j , (24b)\nwhich is a convex optimization problem of T. Let g(T ) denote the first-order derivative of formula (24a), expressed as\ng(T ) = \u03c1\u03b1 \u2212 2\u03c1 (1 \u2212 \u03b1) \u03ba\u03b5 3 j\u2208J (D j ) 3 T \u2212 a * j \u2212 t up j 3 . (25)\nTherefore, the optimal value of T at the feasible interval [T min , +\u221e) can be given by\nT * = T min , g T min \u2265 0 T , else(26)\nwhereT is the solution of the equation g(T ) = 0 which can be solved by binary search. Substituting the solution T * into formula (23), f * j (\u2200j) can be obtained.\n\n\nC. Transmission Power and Subcarrier Optimization\n\nWith fixed (F, T), problem (21) can be simplified to\nmin P,C \u03c1 (1 \u2212 \u03b1) j\u2208J p j D n C j,n Blog 2 (1 + p j h j,n /\u03c3 2 2 ) + (1 \u2212 \u03c1) j\u2208J (D j Error j ) (27a) s.t. (18d), (18e), (18f ), (18g), (21c). (27b)\nTo solve it, we first assume that the subcarrier allocation scheme has been given. Let the function m(j) denotes the mapping function from SBS j to its assigned subcarrier. Therefore, the expression of power optimization problem is as follow:\nmin P \u03c1 (1 \u2212 \u03b1) j\u2208J p j D Blog 2 1 + p j h j,m(j) /\u03c3 2 2 + (1 \u2212 \u03c1) j\u2208J D j 1 \u2212 exp \u2212m\u03c3 2 2 p j h j,m(j) (28a) s.t. D Blog 2 1 + pj h j,m(j) \u03c3 2 2 \u2264 T \u2212 a * j \u2212 t cmp j , \u2200j, (28b) (18d).(28c)\nSince in problem (28), the optimal transmission power of any two SBSs is not coupled, it can be decomposed into\nD B ( T \u2212a * j \u2212\u03b5D j /f j ) \u2212 1 .(30)\nThe problem (29) is an univariate non-convex optimization problem which can be solve by successive convex approximation (SCA) algorithm. To simplify notation, denote formula (29a) as h(p j ). In the n-th iteration of the SCA algorithm, we take the functionh(p j , p n\u22121 j ) as the approximation of the function h(p j ). The functionh(p j , p n\u22121 j ) is defined as h j,m(j) . The SCA-based power optimization algorithm can be referred to Algorithm 4 in Page 20 of [16]. Now it is time to determine the subcarrier allocation scheme. With SCA-based power optimization algorithm, each SBS can solve the optimal power under each subcarrier. The optimal power of SBS j on the n-th subcarrier can be denoted as p * j,n . By substituting p * j,n into function h (p j ), we can obtain the minimum cost h p * j,n achieved by SBS j when selecting this subcarrier. So the problem (27) can now be converted to  (33b)\nh p j , p n\u22121 j = h p n\u22121 j + h p n\u22121 j p j \u2212 p n\u22121 j + \u03c4 j 2 p j \u2212 p n\u22121 j 2 ,(31)\nBecause of the non-negative nature of h p * j,n (\u2200j, n), it is easy to find that the problem (33) is an assignment problem and can be solved by the Hungarian algorithm. To sum up, the complete algorithm flow for problem (18) is summarized in Algorithm 1. Calculate T and F according to equations (26) and (23) with fixed C and P ; 5: Each SBS applies SCA-based power optimization algorithm to compute p * j,n under each subcarrier; 6: Calculate h p * j,n , \u2200j, n;\n\n\n7:\n\nApply the Hungarian algorithm to solve the current optimal subcarrier allocation scheme, then update C; 8: Find the corresponding optimal transmission power P according to C; 9: until the convergence condition is satisfied; 10: Output B * ,C * ,P * ,T * ,F * . \n\n\nParameter\n\nValue \nParameter Value p max 23dBm p max j 37dBm B j 10MHz f max j 5GHz D s k 3Mbit D 100Kbit \u03ba 2 \u00d7 10 \u2212\n\nIV. NUMERICAL RESULTS\n\nIn this section, numerical simulations are conducted to study the convergence of the proposed algorithm and its performance. For our simulations, we consider a circular area with a radius of 500m. One MBS with total bandwidth of 3.125MHz is located in the center of the area and 10 SBSs are uniformly distributed in the range of 200 to 500 meters from the MBS. Each SBS occupies a circular area with a radius of 50 meters. There are 10 to 20 sensors uniformly distributed within a range of 5 to 50 meters from each SBS. Other more detailed simulation parameters are shown in the TABLE I. For comparison, the following schemes are considered: 1) Equal bandwidth allocation: Each SBS distributes its bandwidth equally among all sensors in its range; 2) Learning cost guaranteed: Only the learning cost is considered as optimization objective; 3) Greedy subcarrier allocation algorithm: Channels that can maximize the sum of channel gains on the channel response matrix are selected directly with Hungarian algorithm; 4) System cost guaranteed: \u03c1 is set close to 1 when doing optimization; 5)Time-biased: All SBSs always adopt the maximum frequencies and maximum transmission power. Step 0  Fig. 2 shows the convergence of algorithms. It is clear that all six algorithms can converge to a stable value within 10 steps. Since the Greedy subcarrier allocation algorithm uses the determined subcarrier allocation scheme and all SBSs adopt fixed frequency and power in Time-biased algorithm, the convergence rate of these two algorithms is faster. In addition, it is obvious that the proposed algorithm can converge quickly to the lowest total cost among all six algorithms. Fig. 3 depicts the impact of the SBS maximum transmission power on total cost. Specifically, with the increase of the maximum transmission power of SBS, the total cost achieved by proposed algorithm, Equal bandwidth allocation algorithm and System cost guaranteed algorithm decreases at first and then stays approximately the same. That is because when the maximum transmission power of SBS is low, the lowest total cost that can be achieved is limited by it. As the constraint of the maximum power is relaxed, the power value searched is no longer affected by the maximum transmission power, so the corresponding total cost hardly changes. For Learning cost guaranteed algorithm and Time-biased algorithm, they always take the maximum power as optimal transmission power. With the increase of SBS maximum power, latency and learning cost are improved, but at the same time, energy consumption is rising. So these curves first decrease then start to increase. Finally, for the Greedy subcarrier allocation algorithm, the increase of the maximum transmission power of SBS can make up for the loss of latency and learning performance caused by greedy subcarrier selection, so the cost decreases. Fig. 4 shows the total cost as a function of the amount of data uploaded by each sensor. It can be seen from the figure that with the increase of sensor data amount, the total cost value obtained by each algorithm approximately presents a linear upward trend. The reason is that the latency, energy consumption and learning cost in total cost will all increase with the increase of the amount of data uploaded by each sensor. It is worth mentioning that the proposed algorithm always achieves the lowest cost value and is less affected by the amount of data uploaded by each sensor than baselines, since it jointly optimizes bandwidth, computation frequency, transmission power and subcarrier allocation.\n\nV. CONCLUSION In this paper, we investigate the federated learning scenario in edge computing supported IoT. Considering the three steps of federated learning, denoted by data collection, local computation and aggregation and broadcast, we formulate a MINLP problem that minimizes the total cost. The original optimization problem is decomposed into several subproblems and solved effectively with designed algorithms. Finally, numerical results show that the proposed algorithm can quickly converge to a stable solution and achieve better performance under various parameter settings.\n\n\nThis paper is funded by Beijing Univ. of Posts and Telecommun.-China Mobile Research Institute Joint Innovation Center.\n\nFig. 1 .\n1System model of MEC supported federated learning.\n\n\nthe maximum transmission power and local computation capacity of SBS j. (18b) and (18c) are bandwidth constraints. (18d) and (18h) are power constraint and computation frequency constraint, respectively. While (18e) to (18g) are the conditions to be satisfied by the feasible subcarrier allocation scheme. The problem (18) is a mixed integer nonlinear programing (MINLP) which is NP-hard.\n\n\nwhere \u03c4 j is any positive constant. h (p n\u22121 j ) is the function value of the first-order derivative of h(p j ) at point p n\n\nRemark 1 :\n1(Complexity): The algorithm shown in Algorithm 1 consists of the bandwidth allocation algorithm in the\n\n\nwith the time complexity O (K) and an iterative algorithm that runs repeatedly between the third and ninth line. The time complexity of iterative algorithm for one iteration is O J 3 + J 2 (1 + L SCA ) + J (1 + log 2 (1/\u03b4)) , where \u03b4 and L SCA denote the solution accuracy of binary search algorithm and total number of iterations within the SCA-based power optimization algorithm respectively.\n\nTABLE I SIMULATION\nIPARAMETERS\n\n\nFig. 2. Total cost vs.Step.Fig. 3. Total cost vs. SBS max. transmission power.Fig. 4. Total cost vs. Sensor data amount..5 \n\n1 \n\n1.5 \n\n2 \n\n2.5 \n\n3 \n\nTotal cost \n\nProposed algorithm \nEqual bandwidth \nLearning cost guaranteed \nGreedy subcarrier allocation \nSystem cost guaranteed \nTime-biased algorithm \n\n28 \n30 \n32 \n34 \n36 \n38 \n40 \n\nSBS maximum transmission power (dBmW) \n\n0.6 \n\n0.8 \n\n1 \n\n1.2 \n\n1.4 \n\n1.6 \n\n1.8 \n\n2 \n\n2.2 \n\n2.4 \n\nTotal cost \n\nProposed algorithm \nEqual bandwidth allocation \nLearning cost guaranteed \nGreedy subcarrier allocation \nSystem cost guaranteed \nTime-biased algorithm \n\n1 \n1.5 \n2 \n2.5 \n3 \n3.5 \n4 \n4.5 \n5 \n\nThe amount of data uploaded by the sensor(Mbit) \n\n0 \n\n0.5 \n\n1 \n\n1.5 \n\n2 \n\n2.5 \n\nTotal cost \n\nProposed algorithm \nEqual bandwidth \nLearning cost guaranteed \nGreedy subcarrier allocation \nSystem cost guaranteed \nTime-biased algorithm \n\n\nh j,m(j)\nJ unrelated optimization subproblems. Take SBS j as an example, the optimization subproblem iswhere p min j in (29b) is transformed from constraint (28b), expressed as p min j = \u03c3 2\nArtificial neural networks-based machine learning for wireless networks: A tutorial. M Chen, IEEE Commun. Surv. Tut. 214M. Chen et al., \"Artificial neural networks-based machine learning for wireless networks: A tutorial,\" IEEE Commun. Surv. Tut., vol. 21, no. 4, pp. 3039-3071, July 2019.\n\nThe Internet of Things: A survey. L Atzori, Comput. Netw. 5415L. Atzori et al., \"The Internet of Things: A survey,\" Comput. Netw., vol. 54, no. 15, pp. 2787-2805, Oct. 2010.\n\nEdgeFed: Optimized federated learning based on edge computing. Y Ye, IEEE Access. 8Y. Ye et al., \"EdgeFed: Optimized federated learning based on edge computing,\" IEEE Access, vol. 8, pp. 209 191-209 198, Nov. 2020.\n\nSurvey on applications of Internet of Things using machine learning. N Majumdar, Proc. Int. Conf. Cloud Comput. Int. Conf. Cloud ComputConfluence, Uttar Pradesh, IndiaN. Majumdar et al., \"Survey on applications of Internet of Things using machine learning,\" in Proc. Int. Conf. Cloud Comput., Data Sci. Eng., Confluence, Uttar Pradesh, India, Jan. 2019.\n\nFederated learning for 6G: Applications, challenges, and opportunities. Z Yang, Z. Yang et al., \"Federated learning for 6G: Applications, challenges, and opportunities,\" Jan. 2021. [Online]. Available: https://arxiv.org/abs/ 2101.01338\n\nFederated optimization: Distributed machine learning for on-device intelligence. J Kone\u010dn\u1ef3, J. Kone\u010dn\u1ef3 et al., \"Federated optimization: Distributed machine learning for on-device intelligence,\" Oct. 2016. [Online]. Available: https://arxiv. org/abs/1610.02527\n\nCommunication efficient federated learning. M Chen, PNAS2021M. Chen et al., \"Communication efficient federated learning,\" in PNAS, accepted, 2021.\n\nResource optimized federated learning-enabled Cognitive Internet of Things for smart industries. L U Khan, IEEE Access. 8L. U. Khan et al., \"Resource optimized federated learning-enabled Cognitive Internet of Things for smart industries,\" IEEE Access, vol. 8, pp. 168 854-168 864, Sep. 2020.\n\nToward edge intelligence: Multiaccess edge computing for 5G and Internet of Things. Y Liu, IEEE Internet Things J. 78Y. Liu et al., \"Toward edge intelligence: Multiaccess edge computing for 5G and Internet of Things,\" IEEE Internet Things J., vol. 7, no. 8, pp. 6722-6747, Aug. 2020.\n\nIn-Edge AI: Intelligentizing mobile edge computing, caching and communication by federated learning. X Wang, IEEE Netw. 335X. Wang et al., \"In-Edge AI: Intelligentizing mobile edge computing, caching and communication by federated learning,\" IEEE Netw., vol. 33, no. 5, pp. 156-165, Sept.-Oct. 2019.\n\nEnergy efficient federated learning over wireless communication networks. Z Yang, IEEE Trans. Wirel. Commun. 203Z. Yang et al., \"Energy efficient federated learning over wireless communication networks,\" IEEE Trans. Wirel. Commun., vol. 20, no. 3, pp. 1935-1949, Nov. 2020.\n\nAdaptive federated learning in resource constrained edge computing systems. S Wang, IEEE J. Sel. Areas Commun. 376S. Wang et al., \"Adaptive federated learning in resource constrained edge computing systems,\" IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1205-1221, Mar. 2019.\n\nHFEL: Joint edge association and resource allocation for cost-efficient hierarchical federated edge learning. S Luo, IEEE Trans. Wirel. Commun. 1910S. Luo et al., \"HFEL: Joint edge association and resource allocation for cost-efficient hierarchical federated edge learning,\" IEEE Trans. Wirel. Commun., vol. 19, no. 10, pp. 6535-6548, Oct. 2020.\n\nA general upper bound to evaluate packet error rate over quasi-static fading channels. Y Xi, IEEE Trans. Wirel. Commun. 105Y. Xi et al., \"A general upper bound to evaluate packet error rate over quasi-static fading channels,\" IEEE Trans. Wirel. Commun., vol. 10, no. 5, pp. 1373-1377, May 2011.\n\nA joint learning and communications framework for federated learning over wireless networks. M Chen, IEEE Trans. Wirel. Commun. 201M. Chen et al., \"A joint learning and communications framework for federated learning over wireless networks,\" IEEE Trans. Wirel. Commun., vol. 20, no. 1, pp. 269-283, Jan. 2021.\n\nSuccessive convex approximation: Analysis and applications. M Razaviyayn, ThesesDissertations and ThesesGradworks. M. Razaviyayn, \"Successive convex approximation: Analysis and appli- cations,\" ThesesDissertations and ThesesGradworks, Gradworks, May 2014.\n", "annotations": {"author": "[{\"end\":149,\"start\":112},{\"end\":281,\"start\":150},{\"end\":291,\"start\":282},{\"end\":301,\"start\":292},{\"end\":314,\"start\":302},{\"end\":375,\"start\":315},{\"end\":507,\"start\":376}]", "publisher": null, "author_last_name": "[{\"end\":124,\"start\":121},{\"end\":162,\"start\":159},{\"end\":290,\"start\":286},{\"end\":300,\"start\":298},{\"end\":313,\"start\":310},{\"end\":328,\"start\":324}]", "author_first_name": "[{\"end\":120,\"start\":112},{\"end\":158,\"start\":150},{\"end\":285,\"start\":282},{\"end\":297,\"start\":292},{\"end\":309,\"start\":302},{\"end\":323,\"start\":315}]", "author_affiliation": "[{\"end\":234,\"start\":164},{\"end\":280,\"start\":236},{\"end\":374,\"start\":330},{\"end\":506,\"start\":377}]", "title": "[{\"end\":109,\"start\":1},{\"end\":616,\"start\":508}]", "venue": null, "abstract": "[{\"end\":2039,\"start\":656}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2175,\"start\":2172},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2277,\"start\":2274},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2405,\"start\":2402},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2572,\"start\":2569},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2834,\"start\":2831},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2954,\"start\":2951},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2959,\"start\":2956},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3390,\"start\":3387},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3488,\"start\":3485},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3647,\"start\":3643},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4457,\"start\":4454},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4463,\"start\":4459},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4469,\"start\":4465},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4524,\"start\":4520},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4795,\"start\":4792},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4801,\"start\":4797},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4807,\"start\":4803},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4896,\"start\":4892},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5125,\"start\":5122},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5532,\"start\":5528},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5563,\"start\":5559},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11550,\"start\":11547},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11755,\"start\":11751},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12213,\"start\":12209},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18208,\"start\":18204},{\"end\":19062,\"start\":19060},{\"end\":19163,\"start\":19161},{\"end\":19305,\"start\":19303}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23878,\"start\":23757},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23939,\"start\":23879},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24330,\"start\":23940},{\"attributes\":{\"id\":\"fig_3\"},\"end\":24457,\"start\":24331},{\"attributes\":{\"id\":\"fig_6\"},\"end\":24573,\"start\":24458},{\"attributes\":{\"id\":\"fig_7\"},\"end\":24970,\"start\":24574},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":25002,\"start\":24971},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":25868,\"start\":25003}]", "paragraph": "[{\"end\":3391,\"start\":2058},{\"end\":4345,\"start\":3393},{\"end\":6623,\"start\":4347},{\"end\":7147,\"start\":6668},{\"end\":7779,\"start\":7149},{\"end\":7979,\"start\":7802},{\"end\":8516,\"start\":8025},{\"end\":8729,\"start\":8538},{\"end\":8887,\"start\":8759},{\"end\":9077,\"start\":8912},{\"end\":9305,\"start\":9100},{\"end\":9530,\"start\":9359},{\"end\":10184,\"start\":9563},{\"end\":10548,\"start\":10234},{\"end\":11001,\"start\":10602},{\"end\":11187,\"start\":11048},{\"end\":11761,\"start\":11231},{\"end\":11949,\"start\":11810},{\"end\":12323,\"start\":12185},{\"end\":12497,\"start\":12356},{\"end\":12613,\"start\":12530},{\"end\":12918,\"start\":12674},{\"end\":13264,\"start\":13002},{\"end\":13636,\"start\":13484},{\"end\":14080,\"start\":13662},{\"end\":14254,\"start\":14082},{\"end\":14695,\"start\":14291},{\"end\":14900,\"start\":14770},{\"end\":15075,\"start\":15071},{\"end\":15081,\"start\":15077},{\"end\":15226,\"start\":15083},{\"end\":15376,\"start\":15228},{\"end\":15727,\"start\":15520},{\"end\":15899,\"start\":15781},{\"end\":16189,\"start\":16006},{\"end\":16285,\"start\":16228},{\"end\":16539,\"start\":16420},{\"end\":16697,\"start\":16610},{\"end\":16900,\"start\":16737},{\"end\":17006,\"start\":16954},{\"end\":17398,\"start\":17156},{\"end\":17702,\"start\":17591},{\"end\":18644,\"start\":17741},{\"end\":19192,\"start\":18729},{\"end\":19460,\"start\":19199},{\"end\":19480,\"start\":19474},{\"end\":23169,\"start\":19603},{\"end\":23756,\"start\":23171}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8024,\"start\":7980},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8537,\"start\":8517},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8758,\"start\":8730},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9099,\"start\":9078},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9329,\"start\":9306},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9358,\"start\":9329},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10233,\"start\":10185},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10601,\"start\":10549},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11047,\"start\":11002},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11230,\"start\":11188},{\"attributes\":{\"id\":\"formula_10\"},\"end\":11809,\"start\":11762},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12184,\"start\":11950},{\"attributes\":{\"id\":\"formula_12\"},\"end\":12355,\"start\":12324},{\"attributes\":{\"id\":\"formula_13\"},\"end\":12529,\"start\":12498},{\"attributes\":{\"id\":\"formula_14\"},\"end\":12648,\"start\":12614},{\"attributes\":{\"id\":\"formula_15\"},\"end\":12958,\"start\":12919},{\"attributes\":{\"id\":\"formula_16\"},\"end\":13001,\"start\":12958},{\"attributes\":{\"id\":\"formula_17\"},\"end\":13323,\"start\":13265},{\"attributes\":{\"id\":\"formula_18\"},\"end\":13426,\"start\":13323},{\"attributes\":{\"id\":\"formula_19\"},\"end\":13483,\"start\":13426},{\"attributes\":{\"id\":\"formula_20\"},\"end\":14743,\"start\":14696},{\"attributes\":{\"id\":\"formula_21\"},\"end\":14769,\"start\":14743},{\"attributes\":{\"id\":\"formula_22\"},\"end\":15070,\"start\":14901},{\"attributes\":{\"id\":\"formula_23\"},\"end\":15519,\"start\":15377},{\"attributes\":{\"id\":\"formula_24\"},\"end\":15994,\"start\":15900},{\"attributes\":{\"id\":\"formula_25\"},\"end\":16005,\"start\":15994},{\"attributes\":{\"id\":\"formula_26\"},\"end\":16227,\"start\":16190},{\"attributes\":{\"id\":\"formula_27\"},\"end\":16419,\"start\":16286},{\"attributes\":{\"id\":\"formula_28\"},\"end\":16609,\"start\":16540},{\"attributes\":{\"id\":\"formula_29\"},\"end\":16736,\"start\":16698},{\"attributes\":{\"id\":\"formula_30\"},\"end\":17155,\"start\":17007},{\"attributes\":{\"id\":\"formula_31\"},\"end\":17590,\"start\":17399},{\"attributes\":{\"id\":\"formula_32\"},\"end\":17740,\"start\":17703},{\"attributes\":{\"id\":\"formula_33\"},\"end\":18728,\"start\":18645},{\"attributes\":{\"id\":\"formula_34\"},\"end\":19578,\"start\":19481}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20190,\"start\":20178}]", "section_header": "[{\"end\":2056,\"start\":2041},{\"end\":6666,\"start\":6626},{\"end\":7800,\"start\":7782},{\"end\":8910,\"start\":8890},{\"end\":9561,\"start\":9533},{\"end\":12672,\"start\":12650},{\"end\":13660,\"start\":13639},{\"end\":14289,\"start\":14257},{\"end\":15779,\"start\":15730},{\"end\":16952,\"start\":16903},{\"end\":19197,\"start\":19195},{\"end\":19472,\"start\":19463},{\"end\":19601,\"start\":19580},{\"end\":23888,\"start\":23880},{\"end\":24469,\"start\":24459},{\"end\":24990,\"start\":24972}]", "table": "[{\"end\":25868,\"start\":25125}]", "figure_caption": "[{\"end\":23878,\"start\":23759},{\"end\":23939,\"start\":23890},{\"end\":24330,\"start\":23942},{\"end\":24457,\"start\":24333},{\"end\":24573,\"start\":24471},{\"end\":24970,\"start\":24576},{\"end\":25002,\"start\":24992},{\"end\":25125,\"start\":25005}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6692,\"start\":6686},{\"end\":20797,\"start\":20791},{\"end\":21277,\"start\":21271},{\"end\":22471,\"start\":22465}]", "bib_author_first_name": "[{\"end\":26146,\"start\":26145},{\"end\":26386,\"start\":26385},{\"end\":26590,\"start\":26589},{\"end\":26812,\"start\":26811},{\"end\":27170,\"start\":27169},{\"end\":27416,\"start\":27415},{\"end\":27640,\"start\":27639},{\"end\":27841,\"start\":27840},{\"end\":27843,\"start\":27842},{\"end\":28121,\"start\":28120},{\"end\":28423,\"start\":28422},{\"end\":28697,\"start\":28696},{\"end\":28974,\"start\":28973},{\"end\":29287,\"start\":29286},{\"end\":29611,\"start\":29610},{\"end\":29913,\"start\":29912},{\"end\":30191,\"start\":30190}]", "bib_author_last_name": "[{\"end\":26151,\"start\":26147},{\"end\":26393,\"start\":26387},{\"end\":26593,\"start\":26591},{\"end\":26821,\"start\":26813},{\"end\":27175,\"start\":27171},{\"end\":27424,\"start\":27417},{\"end\":27645,\"start\":27641},{\"end\":27848,\"start\":27844},{\"end\":28125,\"start\":28122},{\"end\":28428,\"start\":28424},{\"end\":28702,\"start\":28698},{\"end\":28979,\"start\":28975},{\"end\":29291,\"start\":29288},{\"end\":29614,\"start\":29612},{\"end\":29918,\"start\":29914},{\"end\":30202,\"start\":30192}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":195767238},\"end\":26349,\"start\":26060},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1862633},\"end\":26524,\"start\":26351},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":227232561},\"end\":26740,\"start\":26526},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":199057589},\"end\":27095,\"start\":26742},{\"attributes\":{\"id\":\"b4\"},\"end\":27332,\"start\":27097},{\"attributes\":{\"id\":\"b5\"},\"end\":27593,\"start\":27334},{\"attributes\":{\"id\":\"b6\"},\"end\":27741,\"start\":27595},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":221913816},\"end\":28034,\"start\":27743},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":221120871},\"end\":28319,\"start\":28036},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52343892},\"end\":28620,\"start\":28321},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207880723},\"end\":28895,\"start\":28622},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":51921962},\"end\":29174,\"start\":28897},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":211505940},\"end\":29521,\"start\":29176},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":21799760},\"end\":29817,\"start\":29523},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":202660949},\"end\":30128,\"start\":29819},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":59834031},\"end\":30385,\"start\":30130}]", "bib_title": "[{\"end\":26143,\"start\":26060},{\"end\":26383,\"start\":26351},{\"end\":26587,\"start\":26526},{\"end\":26809,\"start\":26742},{\"end\":27838,\"start\":27743},{\"end\":28118,\"start\":28036},{\"end\":28420,\"start\":28321},{\"end\":28694,\"start\":28622},{\"end\":28971,\"start\":28897},{\"end\":29284,\"start\":29176},{\"end\":29608,\"start\":29523},{\"end\":29910,\"start\":29819},{\"end\":30188,\"start\":30130}]", "bib_author": "[{\"end\":26153,\"start\":26145},{\"end\":26395,\"start\":26385},{\"end\":26595,\"start\":26589},{\"end\":26823,\"start\":26811},{\"end\":27177,\"start\":27169},{\"end\":27426,\"start\":27415},{\"end\":27647,\"start\":27639},{\"end\":27850,\"start\":27840},{\"end\":28127,\"start\":28120},{\"end\":28430,\"start\":28422},{\"end\":28704,\"start\":28696},{\"end\":28981,\"start\":28973},{\"end\":29293,\"start\":29286},{\"end\":29616,\"start\":29610},{\"end\":29920,\"start\":29912},{\"end\":30204,\"start\":30190}]", "bib_venue": "[{\"end\":26175,\"start\":26153},{\"end\":26407,\"start\":26395},{\"end\":26606,\"start\":26595},{\"end\":26852,\"start\":26823},{\"end\":27167,\"start\":27097},{\"end\":27413,\"start\":27334},{\"end\":27637,\"start\":27595},{\"end\":27861,\"start\":27850},{\"end\":28149,\"start\":28127},{\"end\":28439,\"start\":28430},{\"end\":28729,\"start\":28704},{\"end\":29006,\"start\":28981},{\"end\":29318,\"start\":29293},{\"end\":29641,\"start\":29616},{\"end\":29945,\"start\":29920},{\"end\":30243,\"start\":30204},{\"end\":26909,\"start\":26854}]"}}}, "year": 2023, "month": 12, "day": 17}