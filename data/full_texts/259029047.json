{"id": 259029047, "updated": "2023-06-09 13:17:34.857", "metadata": {"title": "par-gem5: Parallelizing gem5's Atomic Mode", "authors": "[{\"first\":\"Niko\",\"last\":\"Zurstra\u00dfen\",\"middle\":[]},{\"first\":\"Jos\u00e9\",\"last\":\"Cubero-Cascante\",\"middle\":[]},{\"first\":\"Jan\",\"last\":\"Joseph\",\"middle\":[\"Moritz\"]},{\"first\":\"Li\",\"last\":\"Yichao\",\"middle\":[]},{\"first\":\"Xie\",\"last\":\"Xinghua\",\"middle\":[]},{\"first\":\"Rainer\",\"last\":\"Leupers\",\"middle\":[]}]", "venue": "2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "journal": "2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "publication_date": {"year": 2023, "month": 4, "day": 1}, "abstract": "While the complexity of MPSoCs continues to grow exponentially, their often sequential simulations could only benefit from a linear performance gain since the end of Dennard scaling. As a result, each new generation of MPSoCs requires ever longer simulation times. In this paper, we propose a solution to this problem: par-gem5-the first universally parallelized version of the Full System Simulator (FSS) gem5. It exploits the host system's multi-threading capabilities using a modified conservative, quantum-based Parallel Discrete Event Simulation (PDES). Compared to other parallel approaches, par-gem5 uses relaxed causality constraints, allowing temporal errors to occur. Yet, we show that the system's functionality is retained, and the inaccuracy of simulation statistics, such as simulation time or cache miss rate, can be kept within a single-digit percentage. Furthermore, we extend par-gem5 by a temporal error estimation that assesses the accuracy of a simulation without a sequential reference simulation. Our experiments reached speedups of 24.7\u00d7 when simulating a 128-core ARM-based MPSoC on a 128-core host system.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/date/ZurstrassenCJYXL23", "doi": "10.23919/date56975.2023.10137178"}}, "content": {"source": {"pdf_hash": "00b31d26da4891e701032c627eaf5038be470fcb", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "20da38a40648018818bba9bce6c2644729e23120", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/00b31d26da4891e701032c627eaf5038be470fcb.txt", "contents": "\npar-gem5: Parallelizing gem5's Atomic Mode\n\n\nNiko Zurstra\u00dfen \nInstitute for Communication Technologies and Embedded Systems \u2020 Huawei Technologies\nRWTH Aachen University\n\n\nJos\u00e9 Cubero-Cascante \nInstitute for Communication Technologies and Embedded Systems \u2020 Huawei Technologies\nRWTH Aachen University\n\n\nJan Moritz \nInstitute for Communication Technologies and Embedded Systems \u2020 Huawei Technologies\nRWTH Aachen University\n\n\nJoseph \nInstitute for Communication Technologies and Embedded Systems \u2020 Huawei Technologies\nRWTH Aachen University\n\n\nLi Yichao \nInstitute for Communication Technologies and Embedded Systems \u2020 Huawei Technologies\nRWTH Aachen University\n\n\nXie Xinghua \nInstitute for Communication Technologies and Embedded Systems \u2020 Huawei Technologies\nRWTH Aachen University\n\n\nRainer Leupers \nInstitute for Communication Technologies and Embedded Systems \u2020 Huawei Technologies\nRWTH Aachen University\n\n\npar-gem5: Parallelizing gem5's Atomic Mode\nIndex Terms-Parallel Discrete Event Simulationgem5Full- System Simulation\nWhile the complexity of MPSoCs continues to grow exponentially, their often sequential simulations could only benefit from a linear performance gain since the end of Dennard scaling. As a result, each new generation of MPSoCs requires ever longer simulation times. In this paper, we propose a solution to this problem: par-gem5-the first universally parallelized version of the Full System Simulator (FSS) gem5. It exploits the host system's multi-threading capabilities using a modified conservative, quantum-based Parallel Discrete Event Simulation (PDES). Compared to other parallel approaches, par-gem5 uses relaxed causality constraints, allowing temporal errors to occur. Yet, we show that the system's functionality is retained, and the inaccuracy of simulation statistics, such as simulation time or cache miss rate, can be kept within a single-digit percentage. Furthermore, we extend par-gem5 by a temporal error estimation that assesses the accuracy of a simulation without a sequential reference simulation. Our experiments reached speedups of 24.7\u00d7 when simulating a 128-core ARM-based MPSoC on a 128-core host system.Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.C. Timing Mode vs. Atomic ModeDepending on accuracy and performance constraints, gem5 can be operated either in the so-called timing mode or atomic mode. Similar concepts can be found in other frameworks, for example, SystemC, where the modes are referred to as approximately timed and loosely timed, respectively. ! Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\n\nI. INTRODUCTION\n\nThe general trend in today's CPU architectures is increased core count and system complexity. This complicates the design process, creating a need for a plethora of design tools to understand the impact of architectural decisions. Full System Simulators (FSSs) [5], [19], [22] are the backbone for computer architects, as they allow evaluating architectural optimization in the CPU's pipeline, the cache system, and the core interconnects, modeling a real operating system that executes real workloads. Key Performance Indicators (KPIs), such as power consumption or compute performance, can be determined even before physical prototypes are available.\n\nA common problem of FSSs is their inability to simulate massively parallel and complex systems, as their simulation kernels usually only run on a single thread. The most prominent example is gem5 [5]-the de facto standard open-source FSS for ARM-based systems. It does not support multi-threaded simulation, even though the simulated system can have over 100 cores. More than 1 MIPS accumulated is hardly achievable even on the most modern computers, and the execution of representative benchmarks takes days if not weeks. For example, executing the popular SPEC2017 integer benchmark suite natively on a modern host computer requires approximately 10 minutes. Simulating the same workload in gem5 modeling a 64-core system requires more than two years. This issue is only expected to get worse as the gap between the single-core performance and the transistor count as a representative for the system size increased in the last decades (see Fig. 1) and is likely to continue so. If computer architects do not transition to parallel FSS, the big freeze of simulation will be inevitable. To tackle this growing gap in simulation tools, multiple solutions have been proposed to increase the simulation speed. For example, trace-driven simulations like gem5's ElasticTraces [12], system emulation with KVM [18], or the Simpoint method [17]. Although considerable speedups can be achieved with these methods, this speedup is always based on an abstraction of partial aspects of an FSS leading to significant inaccuracies. Therefore, these methods do not replace but only approximate FSSs. The only solution to close the gap between limits from stagnating single-core performance in FSSs and the growing complexity of systems is to execute the FSS in parallel. For parallel simulations, the most crucial challenge arises from the synchronization of events between different threads. It is essential to find an efficient solution that does not break the FSS's functional correctness, but also ensures high performance with a limited number of synchronizations.\n\nIn this work, we solve this issue by proposing a parallel execution kernel for gem5. We use a modified synchronous, conservative approach in the Parallel Discrete Event Simulation (PDES) kernel with relaxed causality constraints. The simulation time is divided into slices, called quanta, in which event queues, including cores and caches, run independently in parallel. Threads only synchronize their events along with the barriers of the quanta at fixed times during the simulation. Compared to other works, our approach accepts errors in causality to increase simulation performance. In other words, we trade a speedup from parallelism with simulation accuracy. As the most important finding of this work, we show that this approach retains the target system's functional correctness, and the errors in simulation statistics are small.\n\n\nII. BACKGROUND\n\n\nA. gem5's Discrete Event Simulation\n\nThe gist of gem5 is a Discrete Event Simulation (DES) engine. Objects in gem5, such as the CPU model, can schedule events at a certain simulation time, or remove them if the event has not yet been processed. A scheduled event is initially located in an event queue, which is processed step by step and in chronological order by the simulation engine. When the event is processed, a function assigned to the event executed, fulfilling implementation-specific tasks. For example, the tickEvent of gem5's atomic CPU executes the next instruction. In most cases, events also schedule or deschedule other events, resulting in a dependency graph, as depicted in Fig. 2 a. The dependency graph G = (V, E) of a DES can be described by the following mathematical model:\nV \u2286 {(v, t, q) | v \u2208 Events, t \u2208 Z + , q \u2208 EventQ} (1) E \u2286 {((v 0 , t 0 , q 0 ), (v 1 , t 1 , q 1 ))|(v i , t i , q i ) \u2208 V \u2227 t 0 \u2264 t 1 } (2)\nThe graph comprises a set of scheduled events/vertices V , whereby each event v has an associated timestamp t and an event queue q. In the default sequential gem5 simulation, a single event queue handles all events. Schedule or deschedule dependencies refer to the edges E. Since the system is causal, edges can only advance in time and never reach back to past events. Due to dynamic dependencies, the exact shape of the graph can only be determined to a limited extent during the simulation. When all events have been processed or the simulation stop event is reached, the simulation terminates.\n\n\nB. Parallel Simulation in FSSs\n\nFundamental work on PDES was first published by Chandy et al. [7] in 1979. The work shows how independent processes are simulated in a distributed manner without focusing on any specific use case. One of the most difficult challenges of implementing any Parallel Discrete Event Simulator (PDES) is the synchronization of the individual threads. Depending on how synchronization is achieved, simulators can be either classified as synchronous or asynchronous [20].\n\nIn asynchronous simulations, as used in [22], [23], the individual threads communicate their local time with each other. Each thread keeps track of the times of all other threads, to determine when it is safe to execute a given event. This, however, poses the risk of situations where no event is deemed safe. Consequently, asynchronous simulations require a sophisticated deadlock prevention/recovery [11].\n\nA synchronous simulator uses global synchronization events to advance the simulation in a lockstep fashion. In its simplest form, a synchronous simulation only parallelizes the execution of all events scheduled for the same simulation timestamp, also called delta cycle. Such an approach is used in [8], [9], [20], Within a delta cycle, a simulator can exploit parallelism, as there is no partial order for the given events. However, the restriction to delta cycles limits the potential performance gain [23]. To achieve greater speedups, distance-based approaches, as used in [3], [16], only synchronize every t q\u2206 . This significantly increases the exploitable parallelism, but poses the risk of causality errors as there is no guarantee of a chronological execution order for the events. To prevent these errors, most PDES implementations fall back to either conservative or optimistic methods [4].\n\nWith optimistic methods, causality errors are not prevented in the first place. Instead, the simulation detects and corrects them. The correction mechanism can be implemented by rolling back the system to a known correct state and reiterating the errorcausing time slot with tighter synchronization. Examples of the optimistic approach can be found in [6], [13], [14].\n\nIn contrast to optimistic approaches, conservative methods prevent causality errors in the first place. This can be achieved by using design-specific timing information, which is either derived from the models or set manually. The multiple simulation threads synchronize every t q\u2206 , whereby a lookahead of t la is provided for each thread. Causality can be retained if t la \u2265 t q\u2206 . Conservative approaches are used in [16], [23].\n\nAlthough gem5 is one of the most popular FSSs, almost all the aforementioned publications refer to the parallelization of SystemC [8]- [10], [14], [20], [23] or other frameworks. The only parallel extension of gem5 to date is dist-gem5 [16], a work resulting from the consolidation of pd-gem5 and multi-gem5. It can be classified as a synchronous, conservative PDES. The focus of dist-gem5 is on simulating distributed systems connected via a Network Interface Controller (NIC). Each simulated node runs on its own gem5 instance and synchronizes with the other nodes at simulation time intervals of t q\u2206 , also called the quantum. Causality errors are prevented by equating t q\u2206 with the simulated latency of the NIC t N IC . This lookahead perfectly retains the causality, but limits the applicability of dist-gem5 to the simulation of distributed systems connected via NICs.\n\nAccording to [5], parallelizing gem5 is one of the community's proclaimed future goals. But 10 years after the paper's publication, parallel gem5 did not progress beyond a few lines of code primarily intended to support the Kernel Virtual Mode (KVM) simulation mode. To the best of our knowledge, a generalized parallel gem5 is not existent. Based on this fundament, we implemented par-gem5; a parallelized version of gem5 for the atomic mode, as described in the following sections. The timing mode allows for accurate modeling of complex communication channels, where exchanging data between two modules comprises multiple phases. In the simplest case, there is only a request phase and response phase (see Fig. 3). Between requesting and receiving data, a simulation time of t \u2206 passes and there is also a possibility of rejecting requests/responses by returning false instead of true. In contrast to the timing mode, the atomic mode implements the same functionality in one phase. With only one phase, dynamic effects like outstanding requests or bus contention cannot be modeled, reducing the accuracy of the simulation. However, the lower number of function calls is beneficial for the simulation performance and code complexity. Furthermore, each transaction is finished in zero simulation time, which aids the design of a thread-safe memory system as shown in Section III-C. Note that a latency t \u2206 is returned by the function call (see Fig. 3). This latency is not part of the request/receive process, and it is up to the requester to annotate it correctly in its following processing.\n\n\nIII. PARALLELIZING GEM5\n\n\nA. Implementation of the PDES\n\nThe principle of the par-gem5 PDES is to increase the number of events queues and simulation threads to N , and to execute them in parallel instead of one simulation thread and one event queue. Each simulation thread has its own local time, which is synchronized globally every t q\u2206 seconds. Hence, our implementation can be classified as a synchronous simulation.\n\nAs mentioned in Section II, a synchronous approach faces challenges in retaining causality. Usually, conservative or optimistic algorithms are used to circumvent that, but neither of these can be considered for our implementation. Optimistic algorithms would require an intrusive checkpoint system, which must be included from the beginning of the software project. With more than 500 kilo Software Lines Of Code (kSLOC), gem5 is already too far advanced in development. For conservative algorithms, the event latency between the individual threads must be known. This results in a rigid simulation structure with clearly defined thread boundaries, as for example seen in dist-gem5 [16]. Since our system is supposed to be flexible, this approach is not an option either.\n\nContrary to conservative and optimistic approaches, our chosen method allows causality errors. However, with sophisticated event queue partitioning, their occurrence can be reduced to a minimum, achieving a high speedup with low inaccuracy. With our approach, causality errors can occur when an event (de)schedules another event outside the event queue within a quantum (see Fig. 2 b):\n((v 0 , t 0 , q 0 ), (v 1 , t 1 , q 1 )) \u2208 E \u2227 q 0 = q 1 (3) i \u00b7 t q\u2206 < t 0 , t 1 < (i + 1) \u00b7 t q\u2206(4)\nFor example, it is possible that event queue q 0 is at the beginning of the quantum (i \u00b7 t q\u2206 ), while event queue q 1 is already at the end of the quantum ((i + 1) \u00b7 t q\u2206 ) waiting for synchronization. If an event in q 1 is now scheduled with t 1 < (i + 1) \u00b7 t q\u2206 , this event will no longer be executed, because the simulation time has already exceeded the point t 1 . This scenario is depicted in Fig. 2 where Event 3 initially attempts to cross-schedule an Event 4 into the event queue q 0 within the same quantum.\n\nOne of the most important findings of our work is that in the context of FSS the \"if\" has a greater impact on the simulation than the \"when\" for most cross-scheduled events. A timer interrupt, for instance, being issued a few microseconds too late affects the simulation accuracy only slightly, while a non-issued timer interrupt might break the functionality of the whole system. Consequently, we implemented a mechanism that postpones causality breaking events to the next quantum border, as shown in Fig. 2 \nb.\nEven though this mechanism deeply intervenes in the simulation, it retains the system's functionality. Most benchmarks and the boot sequence can be executed correctly (see Section IV). Our approach unlocks parallel simulation for gem5, showing significant speedups closing the simulation gap, while retaining the functional correctness and being flexible enough to model any kind of system.\n\n\nB. Assignment of Objects to Event Queues\n\nIn gem5, the individual simulated hardware modules are referred to as simulation objects. The model for a CPU core, a memory controller, or an attached UART device are examples of simulation objects. A system is built by instantiating objects hierarchically, replicating the structure of the target hardware platform. Since simulation objects always refer to an event queue, finding an assignment that minimizes causality errors is one of the key challenges. We empirically observed that exploiting the inherent concurrency of multi-core architectures yields the best results. To achieve this, we map simulation objects to a specific event queue according to the following rules: 1) Each CPU subsystem has a dedicated event queue.\n\n2) All other objects are assigned to the default q 0 . The event queue assignment occurs during the initialization phase. One simulation thread is created for each event queue. Consequently, the total number of simulation threads is the ! ! Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. number of cores plus one. Fig. 4 shows a simplified block diagram of a 2-core platform and the resulting event queue assignment. The block Real View Platform Devices comprises all peripheral devices, such as the generic interrupt controller, system timers, real-time clock, and IO interfaces. It can be seen that all objects of the CPU subsystem (core, TLBs, and private L1 caches) are assigned to the same queue. This assignment allows each CPU core to advance in the simulation using a local time within the quantum borders. Memory accesses that can be satisfied by the private L1 caches are handled by a single simulation thread. All other modules, including the shared L2 cache, DDR memory controller, and peripherals, are put into a separate event queue.\n\n\nC. Thread-Safe Memory System\n\nSo far, this work has focused on the theoretical aspects of PDES. However, a significant part of the work on par-gem5 was spent on the practical problem of thread safety. Since gem5's software stack was designed for sequential execution, function calls or data accesses between objects assigned to different event queues pose the risk of race conditions. In our chosen assignment of event queues, this particularly affects the communication from L1 caches to the crossbar (see Fig. 4). We performed a flow analysis of transactions, paying special attention to those moving between two or more event queue domains. Based on this, we extended the gem5 port class with a new locking functionality. Ports, which gem5 uses to transfer transactions between modules, now additionally the include the functions lock(), unlock() and unlock_peer(), lock_peer(). These can be used to unlock/lock the own or the peer's recursive mutex, respectively.\n\nAs presented in Listing 1 the new port class is used to guarantee that only one cache at a time is accessing the crossbar. Before a cache starts to process a CPU's request, it has to contend its port mutex (line 2). Once the mutex is held, it is determined whether the request requires access to the crossbar (line 3). Access is required for any transaction that cannot be served locally and requires communication to other caches or memories. For instance, read misses, write misses, cache maintenance operations, or load-linked/store-conditionals. If access is required, the cache mutex is released to prevent deadlocks from incoming snoops, and the crossbar mutex is contended using the port.lock_peer() function (lines 4-5). Next, the default gem5 cache mechanisms are executed (line 7), which might include calls to the crossbar. Since the crossbar communicates with other caches via snoop request, an additional lock (lines [17][18][19] ensures that the access does not lead to race conditions. Note that a cache snoop can lead to writebacks that need access to the crossbar in return. To avoid deadlocking by multiple crossbar accesses, the ports use recursive mutexes, allowing multiple accesses by the same thread. Finally, any held mutex is released (lines [8][9][10][11][12].\n\nTo mitigate deadlocks with multi-hop interconnects, all crossbars share the same static mutex. This approach still leaves room for optimization since the locality of the data in downstream caches (L2, L3, etc.) is not exploited optimally. Leveraging this potential requires extensive changes to the memory system that are left for future work.\n\n\nD. Temporal Error Estimation\n\nOne of the biggest challenges of the proposed approach is determining a quantum that yields an acceptable compromise between accuracy and speedup. From a speedup perspective, the largest possible quantum should be selected, since this reduces the number of synchronization points and enables better load balancing. For high accuracy, however, the quantum should be as small as possible due to the lower probability of temporal errors. In addition, determining the influence of the quantum in the first place is not trivial. While the speedup can be roughly estimated from the required wall-clock time, the impact on accuracy requires a sequential simulation as a reference point. Executing a sequential simulation annihilates the speed-up benefits of par-gem5.\n\nFor this reason, we have extended par-gem5 with a novel temporal error estimation. At the end of each parallel simulation, an analytical worst-case estimation of the temporal error is output as a simulation statistic. This estimation helps the user to assess the accuracy of the simulation without the need for a reference simulation.\n\nThe idea of our error estimation is to accumulate timing errors of postponed events. For this purpose, par-gem5 records for each quantum i, which postponed event had the largest shift t i,max pp in time caused by the postpone-mechanism (see Fig. 2 b). It is assumed the postponed event was on the program's critical path, prolonging the simulation by t i,max pp . By accumulating the prolongation times t i,max pp over all quanta, the following formula describes a worst-case temporal error estimation:\n\nWith Q being the number of simulated quanta and t sim,meas being the measured simulation time. We validate the formula by precisely detecting inaccurate simulations as shown in the next section. ! ! Fig. 5. Accumulated MIPS when executing bare-metal bubble sort with par-gem5 using various quantum sizes.\n\n\nIV. RESULTS AND DISCUSSION\n\nTo evaluate the accuracy and performance of par-gem5, we ran a set of benchmarks using the multicore system configuration shown in Table I. The assignment of simulation objects to event queues was done following the strategy explained in Section III-B. As a host system, we used an AMD Ryzen 3990x (64 cores, 128 threads) with 128GiB of 3200MHz DDR4-DRAM running Ubuntu Linux 20.04.\n\n\nA. Speedup and Accuracy of Bare-Metal Bubble Sort\n\nTo perform an initial examination of our parallel simulation approach, we created a simple multi-core bare-metal bubble sort benchmark. It comprises N threads, with the number of available CPU cores N , whereby each thread independently sorts its own array of integers. We designed this benchmark to attain a near best-case simulation throughput, meaning that thread synchronizations and accesses to shared memory are reduced to a minimum.\n\nThis simple program was run multiple times, changing the number of simulated cores and the simulation quantum. The throughput in MIPS achieved in this experiment is shown in Fig. 5. As it can be seen, the accumulated throughput grows with the number of simulated cores in parallel mode, while it stagnates in the regular single-threaded simulation. A maximum throughput of 22.1 MIPS is attained with the 128-cores configuration, representing a speedup factor of 24.7\u00d7. The achievable speedup increases with the size of the quantum and saturates at a certain point between 10\u00b5s and 1000\u00b5s. This observation is consistent with other publications [15], [16] and can be attributed to the lower number of synchronizations and better load balancing.\n\nDue to the possible causality errors of our approach, the accuracy of the simulation must be evaluated as well. Fig. 6 shows a comparison of the simulated time for the individual settings. It can be seen that the inaccuracy grows with increasing quantum and number of cores. Also shown in Fig. 6 are the results of the temporal error estimation. A large discrepancy between measured value and estimated value t sim,est indicates a larger inaccuracy of the simulation. Any greater inaccuracy is precisely detected by the temporal estimation (see 64 cores with 1000\u00b5s quantum).\n\n\nB. Speedup and Accuracy of NAS Parallel Benchmark\n\nTo evaluate par-gem5 with industry-standard multi-core software workloads, we ran the NAS Parallel Benchmarks Fig. 6. Accuracy of simulated seconds when executing bare-metal bubble sort with par-gem5 using various quantum sizes.\n\n(NPB) [1] on Ubuntu Linux 14.04. Specifically, we ran the original eight benchmarks specified in NPB 1 (IS, EP, CG, MG, FT, BT, SP, LU). We used the OpenMP implementation from NPB version 3.4.2 and the Wide (W) data size configuration. While the different test programs of the NPB are all designed for multi-core platforms, they do not scale homogeneously, as they vary greatly in terms of memory access patterns, type of operation, and frequency of synchronization points. A detailed analysis of the scalability of each benchmark is presented in [21].\n\nFor each of the benchmarks, we conduct the same experiment as described in the previous section, incrementing the number of simulated cores. In this case, we fix the quantum to 1\u00b5s due to the length of the simulations. With the default single-thread mode, a throughput between 0.57 and 0.96 MIPS is recorded, while in parallel mode, this value reaches up to 7.37 MIPS. We compute the speedup factor for each test and present the results in Fig. 7. A maximum speedup of 12.13\u00d7 is observed for the EP benchmark in the 64-core configuration. In addition to possible constraints from the host platform hardware, the potential speedup of our parallelization approach is also affected by the simulated application. Software workloads with a high parallelization potential and a low core-to-core communication achieve the best throughput in our simulator. This is the case of EP and BT, which show significant speedups when running both natively [21] and on par-gem5. Nevertheless, it can be concluded that all benchmarks in all configurations benefited from parallelization, consistently achieving speedups greater than 1. Each NPB test program prints a report, including several values, such as the total execution time and the measured performance of the system. It can be regarded as a macro statistic that sums up all performance influencing micro parameters, like cache hit rate or memory bandwidth. The deviation of the benchmark report between sequential and parallel simulation is always below 1.53%.\n\nBesides the aforementioned macro statistics, other statistics, e.g., on the performance of the storage system, may also be of  interest. Table II compares the L1 cache miss rates for regular gem5 and par-gem5 when simulating a 4-core system. Here, the inaccuracy of the parallel simulation never exceeds 5.7%. Also other parameters, such as the main memory bandwidth or L2-cache hit rate, exhibit accuracies of > 99%. The accuracy of the temporal error estimation is exemplarily shown for the IS benchmark in Fig. 8. Here, larger inaccuracies could reliably be detected (see 4 and 16 cores for 100\u00b5s quantum) as well.\n\nIn addition to pure performance measurement, the NPB suite checks the results for correctness. There were no irregularities in any of the benchmarks, which underpins the functional correctness of par-gem5.\n\n\nC. Other Benchmarks\n\nIn addition to the aforementioned benchmarks, experiments with other benchmarks like PARSEC, SPEC2017, and STREAM were conducted. Again, a similar picture emerges: arithmetic-intensive applications achieve high speedups (18\u00d7, PARSEC blackscholes @ 128 cores), while benchmarks with a high load on shared resources, such as STREAM, show low to no speedups.\n\n\nV. CONCLUSION & OUTLOOK\n\nIn this paper, we showed how gem5 can be parallelized using an synchronous, conservative PDES. Our work achieves a 24.7\u00d7 speedup for a best-case synthetic benchmark, while retaining a > 99% accuracy for most simulation statistics. For the widely used NAS Parallel Benchmark collection, a maximum speedup of 12\u00d7 has been observed when simulating a 64-core ARM MPSoC on a 64-core/128-threads host machine.\n\nIn our implementation, events will be postponed up to a quantum length in the worst case if they impair causality. However, the functionality of the ARM MPSoC and the OS are not affected for relevant quanta. Yet an influence on the accuracy of various simulation parameters can be observed.\n\nFinding a quantum that allows high accuracy with high speedup is one of the main challenges. While a large quantum allows high speedups, a small quantum yields better  accuracy in terms of simulation statistics. To find an optimal quantum without time-consuming reference simulations, we developed a temporal error estimation, which reliably detects inaccurate simulations in executed benchmarks. One of our future goals is to extend our work to gem5's timing mode. Contrary to the atomic, the timing mode uses multiple events to model a transaction, complicating the process of making gem5-internal models thread-safe. Nevertheless, the same PDES as presented here can be used for the timing mode.\n\nFig. 1 .\n1Development of highest single-threaded PassMark results [2] and transistors per CPU. From 2012 (Intel Xeon E3-1290 v2) to 2021 (Apple M1), the single-threaded score increased by 73%, while the number of transistors increased by 1042%.\n\nFig. 2 .\n2Example of scheduling events in gem5.\n\nFig. 3 .\n3Functional call diagram of gem5's timing mode and atomic mode.\n\nFig. 4 .\n4Mapping of objects to event queues for a simplified dual core system.\n\nFig. 7 .\n7Speedup attained with par-gem5 for each NPB test program using a 1\u00b5s quantum. The black baseline represents a speedup of 1.\n\nFig. 8 .\n8Accuracy of simulated seconds when executing NPB IS with par-gem5 using various quantum size.\n\n\nListing 1. Simplified C++ code of the thread-safe cache and crossbar model.1 BaseCache::recvAtomic(...) { \n2 \nport.lock(); \n3 \nif (xBarNeeded()) { \n4 \nport.unlock(); // Prevent deadlocks from snoops. \n5 \nport.lock_peer(); // Take ownership of XBar. \n6 \n} \n7 \n... // Untouched gem5 cache code. \n8 \nif (xBarNeeded()) { \n9 \nport.unlock_peer(); \n10 \n} else { \n11 \nport.unlock(); \n12 \n} \n13 } \n14 \n15 CoherentXBar::forwardAtomic(...) { \n16 \n... \n17 \np.lock_peer(); // Locks snoop target caches. \n18 \nTick latency = p->sendAtomicSnoop(pkt); \n19 \np.unlock_peer(); \n20 \n... \n21 } \n\n\n\nTABLE I CONFIGURATION\nIOF THE TARGET SYSTEM.Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.CPU \nARM64, AtomicSimpleCPU @ 2GHz \n# cores \n{1, 2, 4, 8, 16, 32, 64, 128} \nCaches \n64kiB L1-D, 32kiB L1-I, 2MiB L2 shared \nMain Memory \nDDR3 RAM @ 1600MHz \nPeriph. Sub-system \nReal View Virtual Express V1 \nOS \n{Bare-metal, Ubuntu 14.04} \n\n! \n\n! \n\n\n\nTABLE II 4\nII-CORE 1\u00b5S QUANTUM SIMULATION OF NPB IS.dcache miss rate \nicache miss rate \nCPU \n1\u00b5s \nreg \nerror \n1\u00b5s \nreg \nerror \n0 \n0.040828 \n0.042419 \n3.8% \n0.000506 \n0.000497 \n1.7% \n1 \n0.040065 \n0.040325 \n0.6% \n0.001457 \n0.001445 \n0.8% \n2 \n0.040093 \n0.042527 \n5.7% \n0.000288 \n0.000278 \n3.6% \n3 \n0.040960 \n0.040621 \n0.8% \n0.000460 \n0.000446 \n3.1% \n\n# of sim.cores \n\n\ne rel,t = t sim,meas t sim,meas \u2212 Q i=0 t i,max pp \u22121 = t sim,meas t sim,est \u22121 (5)\n\n. NAS Parallel Benchmarks. \"NAS Parallel Benchmarks,\" https://www.nas.nasa.gov/software/npb. html, accessed: 2023-01-23.\n\nPassmark Single Thread Performance. \"Passmark Single Thread Performance,\" https://www.cpubenchmark.net/ singleThread.html, accessed: 2023-01-23.\n\npd-gem5: Simulation Infrastructure for Parallel/ Distributed Computer Systems. M Alian, IEEE Comput. Archit. Letters. M. Alian et al., \"pd-gem5: Simulation Infrastructure for Parallel/ Distributed Computer Systems,\" IEEE Comput. Archit. Letters, 2016.\n\nLanguage support for parallel discrete-event simulations. R Bagrodia, Proceedings of Winter Simulation Conference. Winter Simulation ConferenceR. Bagrodia, \"Language support for parallel discrete-event simulations,\" in Proceedings of Winter Simulation Conference, 1994.\n\nThe Gem5 Simulator. N Binkert, SIGARCH Comput. Archit. News. N. Binkert et al., \"The Gem5 Simulator,\" SIGARCH Comput. Archit. News, 2011.\n\nStandard-compliant Parallel SystemC simulation of Loosely-Timed Transaction Level Models. G Busnot, ASP-DAC. G. Busnot et al., \"Standard-compliant Parallel SystemC simulation of Loosely-Timed Transaction Level Models,\" in ASP-DAC, 2020.\n\nDistributed Simulation: A Case Study in Design and Verification of Distributed Programs. K Chandy, IEEE Transactions on Software Engineering. K. Chandy et al., \"Distributed Simulation: A Case Study in Design and Verification of Distributed Programs,\" IEEE Transactions on Software Engineering, 1979.\n\nA Conservative Approach to SystemC Parallelization. B Chopard, Computational Science -ICCS. B. Chopard et al., \"A Conservative Approach to SystemC Parallelization,\" in Computational Science -ICCS, 2006.\n\nSimParallel: A high performance parallel SystemC simulator using hierarchical multi-threading. M.-K Chung, ISCAS. M.-K. Chung et al., \"SimParallel: A high performance parallel SystemC simulator using hierarchical multi-threading,\" in ISCAS, 2014.\n\nParallel discrete event simulation of Transaction Level Models. R D\u00f6mer, ASP-DAC. R. D\u00f6mer et al., \"Parallel discrete event simulation of Transaction Level Models,\" in ASP-DAC, 2012.\n\nParallel Discrete Event Simulation. R Fujimoto, Commun. ACM. R. Fujimoto, \"Parallel Discrete Event Simulation,\" Commun. ACM, 1990.\n\nExploring system performance using elastic traces: Fast, accurate and portable. R Jagtap, SAMOSR. Jagtap et al., \"Exploring system performance using elastic traces: Fast, accurate and portable,\" in SAMOS, 2016.\n\nVirtual Time. D R Jefferson, ACM Trans. Program. Lang. Syst. D. R. Jefferson, \"Virtual Time,\" ACM Trans. Program. Lang. Syst., 1985.\n\nSpeculative Temporal Decoupling Using fork(),\" in DATE. M Jung, M. Jung et al., \"Speculative Temporal Decoupling Using fork(),\" in DATE, 2019.\n\nOptimizing Temporal Decoupling using Event Relevance. L J\u00fcnger, ASP-DAC. L. J\u00fcnger et al., \"Optimizing Temporal Decoupling using Event Relevance,\" in ASP-DAC, 2021.\n\ndist-gem5: Distributed simulation of computer clusters. A Mohammad, IEEE ISPASS. A. Mohammad et al., \"dist-gem5: Distributed simulation of computer clusters,\" in IEEE ISPASS, 2017.\n\nUsing SimPoint for Accurate and Efficient Simulation. E Perelman, SIGMETRICS Perform. 31E. Perelman et al., \"Using SimPoint for Accurate and Efficient Simulation,\" SIGMETRICS Perform. Eval. Rev., vol. 31, no. 1, 2003.\n\nKVM: The Linux virtual machine monitor. A Qumranet, Proceedings Linux Symposium. 15A. Qumranet et al., \"KVM: The Linux virtual machine monitor,\" Proceedings Linux Symposium, vol. 15, 01 2007.\n\nZSim: Fast and Accurate Microarchitectural Simulation of Thousand-Core Systems. D Sanchez, SIGARCH Comput. Archit. News. D. Sanchez et al., \"ZSim: Fast and Accurate Microarchitectural Simulation of Thousand-Core Systems,\" SIGARCH Comput. Archit. News, 2013.\n\nparSC: Synchronous parallel SystemC simulation on multi-core host architectures. C Schumacher, CODES+ISSS. C. Schumacher et al., \"parSC: Synchronous parallel SystemC simulation on multi-core host architectures,\" in CODES+ISSS, 2010.\n\nPerformance characterization of the NAS Parallel Benchmarks in OpenCL. S Seo, IISWC. S. Seo et al., \"Performance characterization of the NAS Parallel Benchmarks in OpenCL,\" in IISWC, 2011.\n\nManifold: A parallel simulation framework for multicore systems. J Wang, IEEE ISPASS. J. Wang et al., \"Manifold: A parallel simulation framework for multicore systems,\" in IEEE ISPASS, 2014.\n\nSystemC-link: Parallel SystemC simulation using time-decoupled segments. J H Weinstock, DATEJ. H. Weinstock et al., \"SystemC-link: Parallel SystemC simulation using time-decoupled segments,\" in DATE, 2016.\n", "annotations": {"author": "[{\"end\":171,\"start\":46},{\"end\":302,\"start\":172},{\"end\":423,\"start\":303},{\"end\":540,\"start\":424},{\"end\":660,\"start\":541},{\"end\":782,\"start\":661},{\"end\":907,\"start\":783}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":51},{\"end\":192,\"start\":177},{\"end\":313,\"start\":307},{\"end\":550,\"start\":544},{\"end\":672,\"start\":665},{\"end\":797,\"start\":790}]", "author_first_name": "[{\"end\":50,\"start\":46},{\"end\":176,\"start\":172},{\"end\":306,\"start\":303},{\"end\":430,\"start\":424},{\"end\":543,\"start\":541},{\"end\":664,\"start\":661},{\"end\":789,\"start\":783}]", "author_affiliation": "[{\"end\":170,\"start\":63},{\"end\":301,\"start\":194},{\"end\":422,\"start\":315},{\"end\":539,\"start\":432},{\"end\":659,\"start\":552},{\"end\":781,\"start\":674},{\"end\":906,\"start\":799}]", "title": "[{\"end\":43,\"start\":1},{\"end\":950,\"start\":908}]", "venue": null, "abstract": "[{\"end\":2695,\"start\":1025}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2978,\"start\":2975},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2984,\"start\":2980},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2990,\"start\":2986},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3567,\"start\":3564},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4643,\"start\":4639},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4675,\"start\":4671},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4704,\"start\":4700},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7919,\"start\":7916},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8316,\"start\":8312},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8363,\"start\":8359},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8369,\"start\":8365},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8725,\"start\":8721},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9030,\"start\":9027},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9035,\"start\":9032},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9041,\"start\":9037},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9236,\"start\":9232},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9308,\"start\":9305},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9314,\"start\":9310},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9628,\"start\":9625},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9986,\"start\":9983},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9992,\"start\":9988},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9998,\"start\":9994},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10425,\"start\":10421},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10431,\"start\":10427},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10567,\"start\":10564},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10573,\"start\":10569},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10579,\"start\":10575},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10585,\"start\":10581},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10591,\"start\":10587},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10674,\"start\":10670},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11328,\"start\":11325},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14018,\"start\":14014},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19812,\"start\":19808},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19816,\"start\":19812},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19820,\"start\":19816},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20148,\"start\":20145},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20151,\"start\":20148},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20155,\"start\":20151},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20159,\"start\":20155},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20163,\"start\":20159},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24004,\"start\":24000},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24010,\"start\":24006},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24969,\"start\":24966},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25511,\"start\":25507},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26457,\"start\":26453}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29890,\"start\":29645},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29939,\"start\":29891},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30013,\"start\":29940},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30094,\"start\":30014},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30229,\"start\":30095},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30334,\"start\":30230},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30911,\"start\":30335},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31316,\"start\":30912},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31683,\"start\":31317}]", "paragraph": "[{\"end\":3366,\"start\":2714},{\"end\":5422,\"start\":3368},{\"end\":6262,\"start\":5424},{\"end\":7079,\"start\":6319},{\"end\":7819,\"start\":7222},{\"end\":8317,\"start\":7854},{\"end\":8726,\"start\":8319},{\"end\":9629,\"start\":8728},{\"end\":9999,\"start\":9631},{\"end\":10432,\"start\":10001},{\"end\":11310,\"start\":10434},{\"end\":12906,\"start\":11312},{\"end\":13330,\"start\":12966},{\"end\":14103,\"start\":13332},{\"end\":14490,\"start\":14105},{\"end\":15111,\"start\":14593},{\"end\":15623,\"start\":15113},{\"end\":16017,\"start\":15627},{\"end\":16792,\"start\":16062},{\"end\":17906,\"start\":16794},{\"end\":18876,\"start\":17939},{\"end\":20164,\"start\":18878},{\"end\":20509,\"start\":20166},{\"end\":21302,\"start\":20542},{\"end\":21638,\"start\":21304},{\"end\":22142,\"start\":21640},{\"end\":22448,\"start\":22144},{\"end\":22861,\"start\":22479},{\"end\":23354,\"start\":22915},{\"end\":24099,\"start\":23356},{\"end\":24676,\"start\":24101},{\"end\":24958,\"start\":24730},{\"end\":25512,\"start\":24960},{\"end\":27016,\"start\":25514},{\"end\":27635,\"start\":27018},{\"end\":27842,\"start\":27637},{\"end\":28221,\"start\":27866},{\"end\":28652,\"start\":28249},{\"end\":28944,\"start\":28654},{\"end\":29644,\"start\":28946}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7221,\"start\":7080},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14592,\"start\":14491},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15626,\"start\":15624}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22617,\"start\":22610},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27163,\"start\":27155}]", "section_header": "[{\"end\":2712,\"start\":2697},{\"end\":6279,\"start\":6265},{\"end\":6317,\"start\":6282},{\"end\":7852,\"start\":7822},{\"end\":12932,\"start\":12909},{\"end\":12964,\"start\":12935},{\"end\":16060,\"start\":16020},{\"end\":17937,\"start\":17909},{\"end\":20540,\"start\":20512},{\"end\":22477,\"start\":22451},{\"end\":22913,\"start\":22864},{\"end\":24728,\"start\":24679},{\"end\":27864,\"start\":27845},{\"end\":28247,\"start\":28224},{\"end\":29654,\"start\":29646},{\"end\":29900,\"start\":29892},{\"end\":29949,\"start\":29941},{\"end\":30023,\"start\":30015},{\"end\":30104,\"start\":30096},{\"end\":30239,\"start\":30231},{\"end\":30934,\"start\":30913},{\"end\":31328,\"start\":31318}]", "table": "[{\"end\":30911,\"start\":30412},{\"end\":31316,\"start\":31068},{\"end\":31683,\"start\":31370}]", "figure_caption": "[{\"end\":29890,\"start\":29656},{\"end\":29939,\"start\":29902},{\"end\":30013,\"start\":29951},{\"end\":30094,\"start\":30025},{\"end\":30229,\"start\":30106},{\"end\":30334,\"start\":30241},{\"end\":30412,\"start\":30337},{\"end\":31068,\"start\":30936},{\"end\":31370,\"start\":31331}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4316,\"start\":4310},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6981,\"start\":6975},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12027,\"start\":12021},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12763,\"start\":12757},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14489,\"start\":14480},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14999,\"start\":14993},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15622,\"start\":15616},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17179,\"start\":17173},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18422,\"start\":18416},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21890,\"start\":21881},{\"end\":22349,\"start\":22343},{\"end\":23536,\"start\":23530},{\"end\":24219,\"start\":24213},{\"end\":24396,\"start\":24390},{\"end\":24846,\"start\":24840},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25960,\"start\":25954},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27533,\"start\":27527}]", "bib_author_first_name": "[{\"end\":32117,\"start\":32116},{\"end\":32349,\"start\":32348},{\"end\":32582,\"start\":32581},{\"end\":32791,\"start\":32790},{\"end\":33028,\"start\":33027},{\"end\":33292,\"start\":33291},{\"end\":33542,\"start\":33538},{\"end\":33756,\"start\":33755},{\"end\":33912,\"start\":33911},{\"end\":34088,\"start\":34087},{\"end\":34234,\"start\":34233},{\"end\":34236,\"start\":34235},{\"end\":34410,\"start\":34409},{\"end\":34552,\"start\":34551},{\"end\":34720,\"start\":34719},{\"end\":34900,\"start\":34899},{\"end\":35105,\"start\":35104},{\"end\":35338,\"start\":35337},{\"end\":35598,\"start\":35597},{\"end\":35822,\"start\":35821},{\"end\":36006,\"start\":36005},{\"end\":36206,\"start\":36205},{\"end\":36208,\"start\":36207}]", "bib_author_last_name": "[{\"end\":32123,\"start\":32118},{\"end\":32358,\"start\":32350},{\"end\":32590,\"start\":32583},{\"end\":32798,\"start\":32792},{\"end\":33035,\"start\":33029},{\"end\":33300,\"start\":33293},{\"end\":33548,\"start\":33543},{\"end\":33762,\"start\":33757},{\"end\":33921,\"start\":33913},{\"end\":34095,\"start\":34089},{\"end\":34246,\"start\":34237},{\"end\":34415,\"start\":34411},{\"end\":34559,\"start\":34553},{\"end\":34729,\"start\":34721},{\"end\":34909,\"start\":34901},{\"end\":35114,\"start\":35106},{\"end\":35346,\"start\":35339},{\"end\":35609,\"start\":35599},{\"end\":35826,\"start\":35823},{\"end\":36011,\"start\":36007},{\"end\":36218,\"start\":36209}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":31889,\"start\":31769},{\"attributes\":{\"id\":\"b1\"},\"end\":32035,\"start\":31891},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206421644},\"end\":32288,\"start\":32037},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":814875},\"end\":32559,\"start\":32290},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":195349294},\"end\":32698,\"start\":32561},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":211031485},\"end\":32936,\"start\":32700},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2306509},\"end\":33237,\"start\":32938},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5097779},\"end\":33441,\"start\":33239},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":20882376},\"end\":33689,\"start\":33443},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4919139},\"end\":33873,\"start\":33691},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8596649},\"end\":34005,\"start\":33875},{\"attributes\":{\"id\":\"b11\"},\"end\":34217,\"start\":34007},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2654080},\"end\":34351,\"start\":34219},{\"attributes\":{\"id\":\"b13\"},\"end\":34495,\"start\":34353},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":231730651},\"end\":34661,\"start\":34497},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6334537},\"end\":34843,\"start\":34663},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":15718411},\"end\":35062,\"start\":34845},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2408450},\"end\":35255,\"start\":35064},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1827524},\"end\":35514,\"start\":35257},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13912376},\"end\":35748,\"start\":35516},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11169586},\"end\":35938,\"start\":35750},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1205174},\"end\":36130,\"start\":35940},{\"attributes\":{\"id\":\"b22\"},\"end\":36337,\"start\":36132}]", "bib_title": "[{\"end\":32114,\"start\":32037},{\"end\":32346,\"start\":32290},{\"end\":32579,\"start\":32561},{\"end\":32788,\"start\":32700},{\"end\":33025,\"start\":32938},{\"end\":33289,\"start\":33239},{\"end\":33536,\"start\":33443},{\"end\":33753,\"start\":33691},{\"end\":33909,\"start\":33875},{\"end\":34231,\"start\":34219},{\"end\":34549,\"start\":34497},{\"end\":34717,\"start\":34663},{\"end\":34897,\"start\":34845},{\"end\":35102,\"start\":35064},{\"end\":35335,\"start\":35257},{\"end\":35595,\"start\":35516},{\"end\":35819,\"start\":35750},{\"end\":36003,\"start\":35940}]", "bib_author": "[{\"end\":32125,\"start\":32116},{\"end\":32360,\"start\":32348},{\"end\":32592,\"start\":32581},{\"end\":32800,\"start\":32790},{\"end\":33037,\"start\":33027},{\"end\":33302,\"start\":33291},{\"end\":33550,\"start\":33538},{\"end\":33764,\"start\":33755},{\"end\":33923,\"start\":33911},{\"end\":34097,\"start\":34087},{\"end\":34248,\"start\":34233},{\"end\":34417,\"start\":34409},{\"end\":34561,\"start\":34551},{\"end\":34731,\"start\":34719},{\"end\":34911,\"start\":34899},{\"end\":35116,\"start\":35104},{\"end\":35348,\"start\":35337},{\"end\":35611,\"start\":35597},{\"end\":35828,\"start\":35821},{\"end\":36013,\"start\":36005},{\"end\":36220,\"start\":36205}]", "bib_venue": "[{\"end\":32433,\"start\":32405},{\"end\":31794,\"start\":31771},{\"end\":31925,\"start\":31891},{\"end\":32153,\"start\":32125},{\"end\":32403,\"start\":32360},{\"end\":32620,\"start\":32592},{\"end\":32807,\"start\":32800},{\"end\":33078,\"start\":33037},{\"end\":33329,\"start\":33302},{\"end\":33555,\"start\":33550},{\"end\":33771,\"start\":33764},{\"end\":33934,\"start\":33923},{\"end\":34085,\"start\":34007},{\"end\":34278,\"start\":34248},{\"end\":34407,\"start\":34353},{\"end\":34568,\"start\":34561},{\"end\":34742,\"start\":34731},{\"end\":34929,\"start\":34911},{\"end\":35143,\"start\":35116},{\"end\":35376,\"start\":35348},{\"end\":35621,\"start\":35611},{\"end\":35833,\"start\":35828},{\"end\":36024,\"start\":36013},{\"end\":36203,\"start\":36132}]"}}}, "year": 2023, "month": 12, "day": 17}