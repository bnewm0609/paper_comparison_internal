{"id": 3337929, "updated": "2023-11-10 22:07:40.016", "metadata": {"title": "Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos", "authors": "[{\"first\":\"Serena\",\"last\":\"Yeung\",\"middle\":[]},{\"first\":\"Olga\",\"last\":\"Russakovsky\",\"middle\":[]},{\"first\":\"Ning\",\"last\":\"Jin\",\"middle\":[]},{\"first\":\"Mykhaylo\",\"last\":\"Andriluka\",\"middle\":[]},{\"first\":\"Greg\",\"last\":\"Mori\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Fei-Fei\",\"middle\":[]}]", "venue": "International Journal of Computer Vision", "journal": "International Journal of Computer Vision", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Every moment counts in action recognition. A comprehensive understanding of human activity in video requires labeling every frame according to the actions occurring, placing multiple labels densely over a video sequence. To study this problem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new dataset of dense labels over unconstrained internet videos. Modeling multiple, dense labels benefits from temporal relations within and across classes. We define a novel variant of long short-term memory deep networks for modeling these temporal relations via multiple input and output connections. We show that this model improves action labeling accuracy and further enables deeper understanding tasks ranging from structured retrieval to action prediction.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2952835694", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ijcv/YeungRJAMF18", "doi": "10.1007/s11263-017-1013-y"}}, "content": {"source": {"pdf_hash": "eff28b5510ce47b31d75a21896967ea6d1ccc17d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1507.05738v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1507.05738", "status": "GREEN"}}, "grobid": {"id": "884e1a5f756bb5864b3f1881b8475046e4c69580", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/eff28b5510ce47b31d75a21896967ea6d1ccc17d.txt", "contents": "\nEvery Moment Counts: Dense Detailed Labeling of Actions in Complex Videos\n\n\nSerena Yeung \nOlga Russakovsky \n\u00b7 Ning \nJin \u00b7 Mykhaylo Andriluka \nGreg Mori \n\u00b7 Li \nFei-Fei \nEvery Moment Counts: Dense Detailed Labeling of Actions in Complex Videos\nReceived: date / Accepted: dateNoname manuscript No. (will be inserted by the editor)\nEvery moment counts in action recognition. A comprehensive understanding of human activity in video requires labeling every frame according to the actions occurring, placing multiple labels densely over a video sequence. To study this problem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new dataset of dense labels over unconstrained internet videos. Modeling multiple, dense labels benefits from temporal relations within and across classes. We define a novel variant of long short-term memory (LSTM) deep networks for modeling these temporal relations via multiple input and output connections. We show that this model improves action labeling accuracy and further enables deeper understanding tasks ranging from structured retrieval to ac-\n Fig. 1\n: In most internet videos there are multiple simultaneous human actions. Here, we show a concrete example from a basketball video to illustrate our target problem of dense detailed multi-label action understanding. potential labeled actions ( Figure 1). However, most work on human action recognition in video focuses on recognizing discrete instances or single actions at a time: for example, which sport [10] or which single cooking activity [29] is taking place. We argue this setup is fundamentally limiting. First, a single description is often insufficient to fully describe a person's activity. Second, operating in a singleaction regime largely ignores the intuition that actions are intricately connected. A person that is running and then jumping is likely to be simultaneously doing a sport such as basketball or long jump; a nurse that is taking a patient's blood pressure and looking worried is likely to call a doctor as her next action. In this work, we go beyond the standard onelabel paradigm to dense, detailed, multilabel understanding of human actions in videos.\n\nThere are two key steps on the path to tackling detailed multilabel human action understanding: (1) finding the right dataset and (2) developing an appropriate model. In this paper we present work in both dimensions.\n\n\narXiv:1507.05738v3 [cs.CV] 9 Jun 2017\n\nThe desiderata for a video dataset include the following: video clips need to be long enough to capture multiple consecutive actions, multiple simultaneous actions need to be annotated, and labeling must be dense with thorough coverage of action extents. Video annotation is very timeconsuming and expensive, and to the best of our knowledge no such dataset currently exists. UCF101 [37], HMDB51 [14], and Sports1M [10] are common challenging action recognition datasets. However, each video is associated with nonlocalized labels (Sports1M), and the videos in UCF101 and HMDB51 are further temporally clipped around the action. MPII Cooking [29] and Breakfast [13] datasets contain long untrimmed video sequences with multiple sequential actions but still only one label per frame; further, they are restricted to closed-world kitchen environments. THUMOS [9] contains long untrimmed videos but most videos (85%) only contain a single action class.\n\nTo overcome these problems, we introduce a new action detection dataset called MultiTHUMOS, significantly extending the annotations on 413 videos (30 hours) of THU-MOS action detection dataset. First, MultiTHUMOS allows for an in-depth study of simultaneous human action in video: it extends THUMOS from 20 action classes with 0.3 labels per frame to 65 classes and 1.5 labels per frame. Second, MultiTHUMOS allows for a thorough study of the temporal interaction between consecutive actions: the average number of distinct action categories in a video is 10.5 (compared to 1.1 in THUMOS). Going further, MultiTHUMOS lends itself to studying intricate relationships between action labels: the 45 new annotated classes include relationships such as hierarchical (e.g., more general Throw or Pol-eVault and more specific BasketballShot or PoleVaultPlant-Pole) and fine-grained (e.g., Guard versus Block or Dribble versus Pass in basketball). Figure 1 shows an example of our dense multilabel annotation.\n\nReasoning about multiple, dense labels on video requires models capable of incorporating temporal dependencies. A large set of techniques exist for modeling temporal structure, such as hidden Markov models (HMMs), dynamic time warping, and their variants. Recent action recognition literature has used recurrent neural networks known as Long Short Term Memory (LSTM) for action recognition in videos [4]. We introduce MultiLSTM, a new LSTM-based model targeting dense, multilabel action analysis. Taking advantage of the fact that more than 45% of frames in Multi-THUMOS have 2 or more labels, the model can learn dependencies between actions in nearby frames and between actions in the same frame, which allows it to subsequently perform dense multilabel temporal action detection on unseen videos.\n\nIn summary, our contributions are: 1. We introduce MultiTHUMOS, a new large-scale dataset of dense, multilabel action annotations in temporally untrimmed videos, and 2. We introduce MultiLSTM, a new recurrent model based on an LSTM that features temporally-extended input and output connections.\n\nOur experiments demonstrate improved performance of Mul-tiLSTM relative to a plain LSTM baseline on our dense, multilabel action detection benchmark.\n\n\nRelated Work\n\nVisual analysis of human activity has a long history in computer vision research. Thorough surveys of the literature include Poppe [27] and Weinland et al. [48]. Here we review recent work relevant to dense labeling of videos.\n\n\nDatasets\n\nResearch focus is closely intertwined with dataset creation and availability. The KTH [33] and Weizmann [2] datasets were catalysts for a body of work. This era focused on recognizing individual human actions, based on datasets consisting of an individual human imaged against a generally stationary background. In subsequent years, the attention of the community moved towards more challenging tasks. Benchmarks based on surveillance video were developed for crowded scenes, such as the TRECVID Surveillance Event Detection [25]. Interactions between humans or humans and objects [32,23] have been studied. Another line of work has shifted toward analyzing \"unconstrained\" internet video. Datasets in this line present challenges in the level of background clutter present in the videos. The Hollywood (HOHA) [18], HMDB [14], UCF 101 [37], ActivityNet [5], and THUMOS [9] datasets exemplify this trend. Task direction has also moved toward a retrieval setting, finding a (small) set of videos from a large background collection, including datasets such as TRECVID MED [25] and Sports 1M [10].\n\nWhile the push toward unconstrained internet video is positive in terms of the difficulty of this task, it has moved focus away from human action toward identifying scene context. Discriminating diving versus gymnastics largely involves determining the scene of the event. The MPII Cooking dataset [29] and Breakfast dataset [13] refocus efforts toward human action within restricted action domains ( Table 1). The Mul-tiTHUMOS dataset we propose shares commonalities with this line, but emphasizes generality of video, multiple labels per frame, and a broad set of general to specific actions.\n\n\nDetection\n\nUntrimmed Open-world Multilabel UCF101 [37] --yes -HMDB51 [14] --yes -Sports1M [10] yes yes -Cooking [29] yes yes --Breakfast [13] yes yes --THUMOS [9] yes yes yes -MultiTHUMOS yes yes yes yes \n\n\nTemporal models for video\n\nConstructing models of the temporal evolution of actions has deep roots in the literature. Early work includes Yamato et al. [50], using hidden Markov models (HMMs) for latent action state spaces. Lv and Nevatia [16] represented actions as a sequence of synthetic 2D human poses rendered from different view points. Constraints on transitions between key poses are represented using a state diagram called an \"Action Net\" which is constructed based on the order of key poses of an action. Shi et al. [28] proposes a semi-Markov model to segment a sequence temporally and label segments with an action class. Tang et al. [39] extend HMMs to model the duration of each hidden state in addition to the transition parameters of hidden states. Temporal feature aggregation is another common strategy for handling video data. Pooling models include aggregating over space and time, early and late fusion strategies, and temporal localization [42,19,24].\n\nDiscriminative models include those based on latent SVMs over key poses and action grammars [22,44,26]. A recent set of papers has deployed deep models using long shortterm memory (LSTM) models [7] for video analysis [4,20,38,51]. These papers have shown promising results applying LSTMs for tasks including video classification and sentence generation. In contrast, we develop a novel LSTM that performs spatial input aggregation and output modeling for dense labeling output.\n\n\nAction detection\n\nBeyond assigning a single label to a whole video, the task of action detection localizes this action within the video sequence. An example of canonical work in this vein is Ke et al. [11]. More recent work extended latent SVMs to spatiotemporal action detection and localization [40,15]. Rohrbach et al. [30] detect cooking actions using hand-centric features accounting for human pose variation. Ni et al. [21] similarly utilize hand-centric features on the MPII Cooking dataset, but focus on multiple levels of action granularity. Gkioxari and Malik [6] train SVMs for actions on top of deep learned features, and further link them in time for spatio-temporal action detection. In contrast, we address the task of dense multilabel action detection.\n\n\nAttention-based models\n\nSeminal work on computational spatial attention models for images was done by Itti et al. [8]. Recent action analysis work utilizing attention includes Shapovalova et al. [34] who use eye-gaze data to drive action detection and localization. Xu et al. [49] use visual attention to assist in caption generation. Yao et al. [51] develop an LSTM for video caption generation with soft temporal attention. Our method builds on these directions, using an attention-based input temporal context for dense action labeling.\n\n\nThe MultiTHUMOS Dataset\n\nResearch on detailed, multilabel action understanding requires a dataset of untrimmed, densely labeled videos. However, we are not aware of any existing dataset that fits these requirements. THUMOS [9] is untrimmed but contains on average only a single distinct action labeled per video. MPII Cooking [29] and Breakfast [13] datasets have labels of sequential actions, but contain only a single label per frame and are further captured in closed-world settings of a single or small set of kitchens (Table 1).  To address the limitations of previous datasets, we introduce a new dataset called MultiTHUMOS 1 . MultiTHU-MOS contains dense, multilabel, frame-level action annotations ( Figure 2) for 30 hours across 400 videos in the THU-MOS '14 action detection dataset (referred to hereafter as THUMOS) . In particular, all videos in the \"Validation Data\" and \"Test Data\" sets were labeled. THUMOS training data consists of 3 sets of videos: temporally clipped \"Training Data\", temporally untrimmed \"Validation Data\" with temporal annotations, and temporally untrimmed \"Background Data\" with no temporal annotations. Test data consists of temporally untrimmed \"Test Data\" with temporal annotations. We annotated all video sets originally including temporal annotations, i.e. \"Validation Data\" and \"Test Data\".\n\nAnnotations were collected in collaboration with Datatang 2 , a commercial data annotation service. Workers were provided with the name of an action, a brief (up to 1 sentence) description, and 2 annotation examples, and asked to annotate the start and end frame of the action in the videos. An action was annotated if it occurred anywhere in the frame. A single worker was used to annotate each video since the workers are employees of the company, and a second worker verified each annotation as part of Datatang's quality control process after annotation.\n\nIn total, we collected 32, 325 annotations of 45 action classes, bringing the total number of annotations from 6, 365 over 20 classes in THUMOS to 38, 690 over 65 classes in MultiTHUMOS. The classes were selected to have a diversity of length, to include hierarchical, hierarchical within a sport, and fine-grained categories, and to include both sport specific and non-sport specific categories. The action classes are described in more detail below. Importantly, it is not just the scale of the dataset that has increased. The density of annotations increased from 0.3 to 1.5 labels per frame on average and from 1.1 to 10.5 action classes per video. The availability of such densely labeled videos allows research on interaction between actions that was previously impossible with more sparsely labeled datasets. The maximum number of actions per frame increased from 2 in THUMOS to 9 MultiTHUMOS, and the maximum number of actions per video increased from 3 in THUMOS to 25 in MultiTHU-MOS. Figure 3 shows the full distribution of annotation density.\n\nUsing these dense multilabel video annotations, we are able to learn and visualize the relationships between actions. The co-occurrence hierarchy of object classes in images based on mutual information of object annotations was learned by Choi et al. [3]; we adapt their method to per-frame action annotations in video. Figure 4 shows the resulting action hierarchy. Classes such as squat and body contract frequently co-occur; in contrast, classes such as run and billiards rarely occur together in the same frame.\n\nMultiTHUMOS is a very challenging dataset for four key reasons.\n\n1. Long tail data distribution. First, MultiTHUMOS has a long tail distribution in the amount of annotated data per action class. This requires action detection algorithms to effectively utilize both small and large amounts of annotated data. Concretely, MultiTHUMOS has between 27 seconds to 5 hours of annotated video per action class (with the rarest actions being volleyball bump, a pat, volleyball serve, high five and basketball block, and the most common actions being stand, walk, run, sit and talk to the camera). In contrast, THUMOS is more uniformly annotated: the dataset ranges from the rarest action baseball pitch with 3.7 minutes annotated to the most common action pole vault with 1 hour of annotated video. Figure 5 shows the full distribution. 2. Length of actions. The second challenge is that Multi-THUMOS has much shorter actions compared to THU-  Instances of action classes in THUMOS last between 1.5 seconds on average for clicket bowling to 14.7 seconds on average for billiards. In contrast, MultiTHUMOS has seven action classes whose instances last less than a second on average: two-handed catch, planting the pole in pole vaulting, basketball shot, one-handed catch, basketball block, high five and throw. Shorter actions are more difficult to detect since there is very little visual signal in the positive frames. There are instances of actions throw, body contract and squat that last only 2 frames (or 66 milliseconds) in MultiTHUMOS! Accurately localizing such actions encourages strong contextual modeling and multi-action reasoning. 3. Fine-grained actions. The third challenge of MultiTHU-MOS is the many fine-grained action categories with low visual inter-class variation, including hierarchical (e.g. throw vs. baseball pitch), hierarchical within a sport (e.g. pole vault vs. the act of planting the pole when pole vaulting), and fine-grained (e.g. basketball dunk, shot, dribble, guard, block, and pass). It also contains both sport-specific actions (such as different basketball or volleyball moves), as well as general actions that can occur in multiple sports (e.g. pump fist, or one-handed catch). This requires the development of general action detection approaches that are able to accurate model a diverse set of visual appearances. 4. High intra-class variation. The final MultiTHUMOS challenge is the high intra-class variation as shown in Figure 6. The same action looks visually very different across multiple frames. For example, a hug can be shown from many different viewpoints, ranging from extreme close-up shots to zoomed-out scene shots, and may be between two people or a larger group. This encourages the development of models that are insensitive to particular camera viewpoint and instead accurately focus on the semantic information within a video.\n\nWith the MultiTHUMOS dataset providing new challenges for action detection, we now continue on to describing our proposed approach for addressing these challenges and making effective use of the dense multilabel annotation.\n\n\nTechnical Approach\n\nActions in videos exhibit rich patterns, both within a single frame due to action label relations and also across frames as they evolve in time. The desire to elegantly incorporate these cues with state-of-the-art appearance-based models has led to recent works [4,20,38] that study combinations of Convolutional Neural Networks (CNN) modeling frame-level spatial appearance and Recurrent Neural Networks (RNN) modeling the temporal dynamics. However, the density of the action labels in our dataset expands the opportunities for more complex modeling at the temporal level. While in principle even a simple instantiation of an ordinary RNN has the capacity to capture arbitrary temporal patterns, it is not necessarily the best model to use in practice. Indeed, our pro-Action #30/65: Hug Action #46/65: BasketballDribble Fig. 6: Our MultiTHUMOS dataset is very challenging due to high intra-class variation.\n\nposed MultiLSTM model extends the recurrent models described in previous work, and our experiments demonstrate its effectiveness.\n\n\nLSTM\n\nThe specific type of Recurrent architecture that is commonly chosen in previous work is the Long Short-Term Memory (LSTM), which owing to its appealing functional properties has brought success in a wide range of sequence-based tasks such as speech recognition, machine translation and very recently, video activity classification. Let x be an input sequence (x 1 , ..., x T ) and y be an output sequence (y 1 , ..., y T ). An LSTM then maps x to y through a series of intermediate representations:\ni t = \u03c3(W xi x t + W hi h t\u22121 + b i ) (1) f t = \u03c3(W xf x t + W hf h t\u22121 + b f ) (2) o t = \u03c3(W xo x t + W ho h t\u22121 + b o ) (3) g t = tanh(W xc x t + W hc h t\u22121 + b c ) (4) c t = f t c t\u22121 + i t g t (5) h t = o t tanh(c t )(6)y t = W hy h t + b y(7)\nHere c is the \"internal memory\" of the LSTM, and the gates i, f , o control the degree to which the memory accumulates new input g, attenuates its memory, or influences the hidden layer output h, respectively. Intuitively, the LSTM has the capacity to read and write to its internal memory, and hence maintain and process information over time. Compared to standard RNNs, the LSTM networks mitigate the \"vanishing gradients\" problem because except for the forget gate, the cell memory is influenced only by additive interactions that can communicate the gradient signal over longer time durations. The architecture is parametrized by the learnable weight matrices W and biases b , and we refer the reader to [7,4] for further details. However, an inherent flaw of the plain LSTM architecture is that it is forced to make a definite and final prediction at some time step based on what frame it happens to see at that time step, and its previous context vector.\n\n\nMultiLSTM\n\nOur core insight is that providing the model with more freedom in both reading its input and writing its output reduces the burden placed on the hidden layer representation. Concretely, the MultiLSTM expands the temporal receptive field of both input and output connections of an LSTM. These  allow the model to directly refine its predictions in retrospect after seeing more frames, and additionally provide direct pathways for referencing previously-seen frames without forcing the model to maintain and communicate this information through its recurrent connections.\n\n\nMultilabel Loss\n\nIn our specific application setting, the input vectors x t correspond to the 4096-dimensional fc-7 features of the VGG 16layer Convolutional Network which was first pretrained on ImageNet and then fine-tuned on our dataset on an individual frame level. We interpret the vectors y t as the unnormalized log probability of each action class. Since each frame of a video can be labeled with multiple classes, instead of using the conventional softmax loss we sum independent logistic regression losses per class:\nL(y|x) = t,c z tc log(\u03c3(y tc )) + (1 \u2212 z tc ) log(1 \u2212 \u03c3(y tc ))\nwhere y tc is the score for class c at time t, and z tc is the binary ground truth label for class c at time t.\n\n\nMultiple Inputs with Temporal Attention\n\nIn a standard LSTM network, all contextual information is summarized in the hidden state vector. Therefore, the net-work relies on the memory vector to contain all relevant information about past inputs, without any ability to explicitly revisit past inputs. This is particularly challenging in the context of more complex tasks such as dense, multilabel action detection.\n\nTo provide the LSTM with a more direct way of accessing recent inputs, we expand the temporal dimension of the input to be a fixed-length window of frames previous to the current time step (Figure 7(a)). This allows the LSTM to spend its modeling capacity on more complex and longerterm interactions instead of maintaining summary of the recent frames in case it may be useful for the next few frames. Furthermore, we incorporate a soft-attention weighting mechanism that has recently been proposed in the context of machine translation [1].\n\nConcretely, given a video V = {v 1 , . . . v T }, the input x i to the LSTM at time step i is now no longer the representation of a single frame v t , but a weighted combination x i = t \u03b1 it v t where t ranges over a fixed-size window of frames previous to i, and \u03b1 it is the contribution of frame v t to input x i as computed by the soft attention model. To compute the attention coefficients \u03b1 it , we use a model similar to Bahdanau et al. [1]. The precise formulation that worked best in our experiments is:\n\u03b1 it \u221d exp(w T ae [tanh(W ha h i\u22121 ) tanh(W va v t )])(8)\nHere is element-wise multiplication, {w ae , W ha , W va } are learned weights , and \u03b1 t is normalized using the softmax function with the interpretation that \u03b1 t expresses the relative amount of attention assigned to each frame in the input window. Intuitively, the first term tanh(W ha h i\u22121 ) allows the network to look for certain features in the input, while the second term tanh(W va v t ) allows each input to broadcast the presence/absence of these features. Therefore, the multiplicative interaction followed by the weighted sum with w ae has the effect of quantifying the agreement between what is present in the input and what the network is looking for. Note that the standard LSTM formulation is a special case of this model where all attention is focused on the last input window frame.\n\n\nMultiple Outputs\n\nAnalogous to providing explicit access to a window of frames at the input, we allow the LSTM to contribute to predictions in a window of frames at the output (Figure 7(b)). Intuitively, this mechanism lets the network refine its predictions in retrospect, after having seen more frames of the input. This feature is related to improvements that can be achieved by use of bi-directional recurrent networks. However, unlike bidirectional models our formulation can be used in an online setting where it delivers immediate predictions that become refined with a short time lag. 3 Given the multiple outputs, we consolidate the predicted labels for all classes c at time t with a weighted average y t = i \u03b2 it p it where p it are the predictions at the ith time step for the tth frame, and \u03b2 it weights the contribution. \u03b2 it can be learned although in our experiments we use 1 N for simplicity to average the predictions. The standard LSTM is a special case, where \u03b2 is an indicator function at the current time step. In our experiments we use the same temporal windows at the input and output. Similar to the inputs, we experimented with soft attention over the output predictions but did not observe noticeable improvements. This may be due to increased fragility when the attention is close to the output without intermediate network layers to add robustness, and we leave further study of this to future work.\n\n\nSingle Offset Output\n\nWe experimented with offset predictions to quantify how informative frames at time t are towards predicting labels at some given offset in time. In these experiments, the network is trained with shifted labels y t+s , where s is a given offset (Figure 7(c)). In our dense label setting, this type of model additionally enables applications such as action prediction in unconstrained internet video (c.f. [12]). For example, if the input is a frame depicting a person cocking his arm to throw, the model could predict future actions such as Catch or Hit.\n\n\nExperiments\n\nWe begin by describing our experimental setup in Section 5.1. We then empirically demonstrate the effectiveness of our model on the challenging tasks of action detection (Section 5.2) and action prediction (Section 5.3).\n\n\nSetup\n\n\nDataset\n\nWe evaluate our MultiLSTM model for dense, multilabel action detection on the MultiTHUMOS dataset. We use the same train and test splits as THUMOS (see Sec. 3 for details) but ignore the background training videos. Clipped training videos (the \"Training Data\" set in THUMOS) act as weak supervision since they are only labeled with the THUMOSsubset of MultiTHUMOS classes.\n\n\nImplementation Details\n\nOur single-frame baseline uses the 16-layer VGG CNN model [36], which achieves near state of the art performance on ILSVRC [31]. The model was pre-trained on ImageNet and all layers fine-tuned on MultiTHUMOS using a binary crossentropy loss per-class. The input to our LSTM models is the final 4096-dimensional, frame-level fc7 representation.\n\nWe use 512 hidden units in the LSTM, and 50 units in the attention component of MultiLSTM that is used to compute attention coefficients over a window of 15 frames. We train the model with an exact forward pass, passing LSTM hidden and cell activations from one mini-batch to the next. However we use approximate backpropagation through time where we only backpropagate errors for the duration of a single mini-batch. Our mini-batches consist of 32 input frames (approx. 3.2 seconds), and we use RMSProp [41] to modulate the per-parameter learning rate during optimization.\n\n\nPerformance Measure\n\nWe evaluate our models using Average Precision (AP) measured on our frame-level labels. The focus of our work is dense labeling, hence this is the measure we analyze to evaluate the performance of our model. We report AP values for individual action classes as well as mean Average Precision (mAP), the average of these values across the action categories.\n\nTo verify that our baseline models are strong, we can obtain discrete detection instances using standard heuristic post-processing. Concretely, for each class we threshold the frame-level confidences at \u03bb (\u03bb = 0.1 obtained by crossvalidation) to get binary predictions and then accumulate consecutive positive frames into detections. For each class C, let \u00b5(C) and \u03c3(C) be the mean and standard deviation respectively of frame lengths on the training set. The score of a detection for class C of length L with frame probabilities p 1 . . . p L is then computed as\nscore(C, p 1 . . . p L ) = ( L i p i ) \u00d7 exp( \u2212\u03b1(L \u2212 \u00b5(C)) 2 \u03c3(C) 2 )(9)\nwhere the hyperparameter \u03b1 = 0.01 is obtained by crossvalidation. Using this post-processing, our single-frame CNN model achieves 32.4 detection mAP with overlap threshold 0.1 on the THUMOS subset of MultiTHUMOS. Since state of the art performance on THUMOS reports 36.6 detection mAP including audio features, this confirms that our singleframe CNN is a reasonable baseline. Hereafter, we compare our models without this post-processing to achieve a comparison of the models' dense labeling representational ability.  Table 2: Per-frame mean Average Precision (mAP) of the MultiLSTM model compared to baselines. Two-stream CNN is computed with single-frame flow. LSTM is implemented in the spirit of [4] (details in Section 4.2). We show the relative contributions of adding first the input connections with averaging (LSTM + i), then the attention (LSTM + i + a) as in Figure 7(a), and finally the output connections to create our proposed MultiLSTM model (LSTM + i + a + o) as in Figure 7(b).   \n\n\nAction Detection\n\nWe first evaluate our models on the challenging task of dense per-frame action labeling on MultiTHUMOS. The MultiL-STM model achieves consistent improvements in mean average precision (mAP) compared to baselines. A model trained on Improved Dense Trajectories features [46] (using a linear SVM trained on top of a temporally pooled and quantized dictionary of pre-computed IDT features, provided by THUMOS'14) performs relatively poorly with 13.3 mAP. This highlights the difficulty of the dataset and the challenge of working with generic hand-crafted features that are not learned for these specific fine-grained actions. Additional variants of IDT could be used to improve performance. For example, Fisher Vector encoding of raw IDT features is commonly used to boost performance. However, these methods can be computationally expensive and are limited due to their reliance on underlying hand-crafted features and lack of opportunity for joint training. Hence, we use neural network-based models for the rest of our experiments.\n\nA single-frame CNN fine-tuned on MultiTHUMOS attains 25.4% mAP. We trained a base LSTM network in the spirit of [4] but modified for multilabel action labeling. Specifically, the LSTM is trained using a multilabel loss function and tied hidden context across 32 frame segments, as described in Section 4.2. This base LSTM boosts mAP to 28.1%. Our full MultiLSTM model handily outperforms both baselines with 29.7% mAP.  It is interesting to note from the two plots that compared with the CNN, the LSTM closes the gap with MultiLSTM on classes such as Frisbee Catch, Pole Vault, and Basetkball Guard, which are strongly associated with temporal context (e.g. a throw proceeds a frisbee catch, and a person usually stands at the track for some time before beginning a pole vault). This shows the benefit of stronger temporal modeling, which MultiLSTM continues to improve on the majority of classes.\n\nFigures 9 analyzes per-frame mAP as the number of attention units (at both input and output) in the MultiLSTM model is varied. We observe that increasing the number of attention units improves performance up to a point (75 units), as would be expected, and then decreases past that as the number of parameters becomes too large. In practice, we use 50 units in our experiments. Figure 10 visualizes some results of MultiLSTM compared to a baseline CNN. For ease of visualization, we binarize outputs by thresholding rather than showing the perframe probabilistic action labels our model produces. The CNN often produces short disjoint detections whereas Mul-tiLSTM effectively makes use of temporal and co-occurrence context to produce more consistent detections.\n\nThe multilabel nature of our model and dataset allows us to go beyond simple action labeling and tackle higherlevel tasks such as retrieval of video segments containing sequences of actions ( Figure 11) and co-occurring actions ( Figure 12). By learning accurate co-occurrence and temporal relationships, the model is able to retrieve video fragments with detailed action descriptions such as Pass and then Shot or frames with simultaneous actions such as Sit and Talk.\n\n\nAction Prediction\n\nDense multilabel action labeling in unconstrained internet videos is a challenging problem to tackle in and of itself. In this section we go one step further and aim to make predictions about what is likely to happen next or what happened previously in the video. By utilizing the MultiLSTM model with offset (Figure 7(c)) we are able to use the learned temporal relationships between actions to make inferences about actions likely occurring in past or future frames.\n\nWe evaluate the performance of this model as a function of temporal offset magnitude and report results in Figure 13. MultiLSTM prediction mAP is shown in red. The plot on the left quantifies the prediction ability of the model within a 4 second (+/-2 second) window, provided an input window of context spanning the previous 1.5 seconds. The model is able to \"see the future\" -while predicting actions 0.5 seconds in the past is easiest (mAP \u2248 30%), reasonable prediction performance (mAP \u2248 20 \u2212 25%) is possible 1-2 seconds into the future. The plot on the right shows the prediction ability of the model using an input context centered around the current frame, instead of spanning only the past. The model is able to provide stronger predictions at past times compared to future times, giving quantitative insight into the contribution of the hidden state vector to providing past context.\n\nIt is also interesting to compare MultiLSTM prediction to a model using the ground-truth label distribution (shown in gray). Specifically, this model makes action predictions using the most frequent label for a given temporal offset from the training set, per-class, and weighted by the MultiL-STM prediction probabilities of actions in the current frame. The label distribution-based model has relatively high performance in the future direction as opposed to the past, and at farther offsets from the current frame. This indicates that stronger priors can be learned in these temporal regions (e.g. frisbee throw should be followed by frisbee catch, and 2 seconds after a dive is typically background (no action)), and MultiLSTM does learn them to some extent. On the other hand, the label distribution-based model has poor performance immediately before the current frame, indicating that there is greater variability in this temporal region, e.g. clapping may be preceded by many different types of sport scoring actions, though a longer offset in the past may be more likely background. In this temporal region, MultiL-STM shows significantly stronger performance than using priors, indicating the benefit of its temporal modeling in this context. Figure 14 shows qualitative examples of predictions at frames 1 second in the future from the current time. The model is able to correctly infer that a Fall is likely to happen after a Jump, and a BasketballShot soon after a Dribble.\n\n\nConclusion\n\nIn conclusion, this paper presents progress in two aspects of human action understanding. First, we emphasize a broader definition of the task, reasoning about dense, multiple labels per frame of video. We have introduced a new dataset MultiTHUMOS, containing a substantial set of labeled data that we will release to spur research in this direction of ac-tion recognition. Second, we develop a novel LSTM-based model incorporating soft attention input-output temporal context for dense action labeling. We show that utilizing this model on our dataset leads to improved accuracy of action labeling and permits detailed understanding of human action.   : Action detection mAP when the MultiLSTM model predicts the action for a past (offset < 0) or future (offset > 0) frame rather than for the current frame (offset = 0). The input window of the MultiLSTM model is shown in gray. Thus, the left plot is of a model trained with input from the past, and the right plot is of a model trained with the input window centered around the current frame. mAP of the Mul-tiLSTM model is shown in red, and mAP of a model using ground-truth label distribution is shown in gray.  \n\nFig. 2 :\n2Our MultiTHUMOS dataset contains multiple action annotations per frame.\n\nFig. 3 :\n3Left. MultiTHUMOS has significantly more labels per frame than THUMOS[9] (1.5 in MultiTHUMOS versus 0.3 in THUMOS). Right. Additionally, MultiTHUMOS contains up to 25 action labels per video compared to \u2264 3 labels in THUMOS.\n\nFig. 4 :\n4We use the method of[3] to learn the relationships between the 65 MultiTHUMOS classes based on per-frame annotations. Blue (red) means positive (negative) correlation. The 20 original THUMOS classes are in green.\n\nFig. 5 :\n5MultiTHUMOS has a wider range of number of perclass frames and instances (contiguous sequences of a label) annotated than THUMOS. Some action classes like Stand or Run have up to 3.5K instances (up to 18K seconds, or 5.0 hours); others like VolleyballSet or Hug have only 15 and 46 instances (27 and 50 secs) respectively. MOS. For each action class, we compute the average length of an action instance of that class. Instance of action classes in THUMOS are on average 4.8 second long compared to only 3.3 seconds long in MultiTHUMOS.\n\nFig. 7 :\n7Components of our MultiLSTM model.\n\nFig. 8 :\n8Per-class Average Precision of the MultiLSTM model compared to (a) a single-frame CNN model [36]; and (b) an LSTM on MultiTHUMOS. MultiLSTM outperforms the single-frame CNN on 56 out of 65 action classes, and the LSTM on 50 out of 65 action classes.\n\nFig. 9 :\n9Number of attention units vs. per-frame mAP of the MultiTHUMOS model. Performance increases as the number of units is increased, but decreases past 75 units. We use 50 units in our experiments.\n\nFigure 8\n8compares per-class results of the CNN vs. Mul-tiLSTM, and the base LSTM vs. MultiLSTM. MultiTHU-MOS outperforms the CNN on 56 our of 65 action classes, and the LSTM on 50 out of 65 action classes. A sampling of action classes is labeled.\n\nFig. 10 :\n10Example timeline of multilabel action detections from our MultiLSTM model compared to a CNN. (best in color)\n\nFig. 11 :Fig. 12 :\n1112Examples of retrieved sequential actions (correct in green, mistakes in red). Results are shown in pairs: first action frame on the left, second action frame on the right. Examples of retrieved frames with co-occurring actions (correct in green, mistakes in red). The model is able to distinguish between subtly different scenarios.\n\nFig. 13\n13Fig. 13: Action detection mAP when the MultiLSTM model predicts the action for a past (offset < 0) or future (offset > 0) frame rather than for the current frame (offset = 0). The input window of the MultiLSTM model is shown in gray. Thus, the left plot is of a model trained with input from the past, and the right plot is of a model trained with the input window centered around the current frame. mAP of the Mul-tiLSTM model is shown in red, and mAP of a model using ground-truth label distribution is shown in gray.\n\nFig. 14 :\n14Examples of predicted actions. For each pair of actions, the first one (left) is the label of the current frame and the second one (right) is the predicted label 1 second into the future. Correct predictions are shown in green, and failure cases are shown in red.\n\nTable 1 :\n1Our MultiTHUMOS dataset overcomes many limitations of previous datasets.2.2 Deep learning for video \n\nIn common with object recognition, hand-crafted features \nfor video analysis are giving way to deep convolutional fea-\nture learning strategies. The best hand-crafted features, the \ndense trajectories of Wang et al. [45], achieve excellent re-\nsults on benchmark action recognition datasets. However, \nrecent work has shown superior results by learning video \nfeatures (often combined with dense trajectories). Simonyan \nand Zisserman [35] present a two-stream convolutional ar-\nchitecture utilizing both image and optical flow data as in-\nput sources. Zha et al. [52] examine aggregation strategies \nfor combining deep learned image-based features for each \nframe, obtaining impressive results on TRECVID MED re-\ntrieval. Karpathy et al. [10] and Tran et al. [43] learn spatio-\ntemporal filters in a deep network and apply them to a vari-\nety of human action understanding tasks. Mansimov et al. [17] \nconsider methods for incorporating ImageNet training data \nto assist in initializing model parameters for learning spatio-\ntemporal features. Wang et al. [47] study temporal pooling \nstrategies, specifically focused on classification in variable-\nlength input videos. \n\n\n\nTable 2\n2additionally demonstrates that each component of our model (input connections, input attention and output connections) is important for accurate action labeling.\n\n\nPass, then ShotPass, then ShotJump, then Fall \nJump, then Fall \n\nThrow, then OneHandedCatch \nThrow, then TwoHandedCatch \n\nClean, then Jerk \nPitch, then OneHandedCatch \n\n\n\n\nJump \u2192 Fall Jump \u2192 Fall Dribble \u2192 Shot Dribble \u2192 Shot DiscusWindUp \u2192 Release DiscusWindUp \u2192 Release VolleyballServe \u2192 VolleyballSpiking VolleyballServe \u2192 VolleyballSpiking Dribble \u2192 Shot Jump \u2192 Fall\ntion prediction.1 IntroductionHumans are great at multi-tasking: they can be walking while talking on the phone while holding a cup of coffee. Further, human action is continual, and every minute is filled withS. Yeung Stanford University, Stanford, CA, USA E-mail: serena@cs.stanford.edu O. Russakovsky Carnegie Mellon University, Pittsburgh, PA, USA Stanford University, Stanford, CA, USA\nThe dataset is available for download at http://ai. stanford.edu/\u02dcsyyeung/everymoment.html.2 http://factory.datatang.com/en/\nA similar behavior can be obtained with a bi-directional model by truncating the hidden state information from future time frames to zero, but this artificially distorts the test-time behavior of the model's outputs, while our model always operates in the regime it was trained with.\nAcknowledgmentsWe would like to thank Andrej Karpathy and Amir Zamir for helpful comments and discussion.\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.0473D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. CoRR arXiv:1409.0473, 2014.\n\nActions as space-time shapes. M Blank, L Gorelick, E Shechtman, M Irani, R Basri, The Tenth IEEE International Conference on Computer Vision (ICCV'05). M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri. Ac- tions as space-time shapes. In The Tenth IEEE International Con- ference on Computer Vision (ICCV'05), 2005.\n\nExploiting hierarchical context on a large database of object categories. M J Choi, J J Lim, A Torralba, A S Willsky, CVPR. M. J. Choi, J. J. Lim, A. Torralba, and A. S. Willsky. Exploiting hierarchical context on a large database of object categories. In CVPR, 2010.\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, L A Hendricks, S Guadarrama, M Rohrbach, S Venugopalan, K Saenko, T Darrell, arXiv:1411.4389J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recur- rent convolutional networks for visual recognition and description. CoRR arXiv:1411.4389, 2014.\n\nActivitynet: A large-scale video benchmark for human activity understanding. B G Fabian Caba Heilbron, Victor Escorcia, J C Niebles, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionB. G. Fabian Caba Heilbron, Victor Escorcia and J. C. Niebles. Activitynet: A large-scale video benchmark for human activity un- derstanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961-970, 2015.\n\nFinding action tubes. G Gkioxari, J Malik, arXiv:1411.6031G. Gkioxari and J. Malik. Finding action tubes. CoRR arXiv:1411.6031, 2014.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber. Long short-term memory. Neu- ral Computation, 9(8):1735-1780, 1997.\n\nA model of saliency-based visual attention for rapid scene analysis. L Itti, C Koch, E Niebur, PAMI20L. Itti, C. Koch, and E. Niebur. A model of saliency-based vi- sual attention for rapid scene analysis. PAMI, 20(11):1254-1259, 1998.\n\nY.-G Jiang, J Liu, A Zamir, G Toderici, I Laptev, M Shah, R Sukthankar, THUMOS challenge: Action recognition with a large number of classes. Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev, M. Shah, and R. Sukthankar. THUMOS challenge: Action recog- nition with a large number of classes. http://crcv.ucf. edu/THUMOS14/, 2014.\n\nLarge-scale video classification with convolutional neural networks. A Karpathy, G Toderici, S Shetty, T Leung, R Sukthankar, L Fei-Fei, CVPR. A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. In CVPR, 2014.\n\nEvent detection in crowded videos. Y Ke, R Sukthankar, M Hebert, ICCV. Y. Ke, R. Sukthankar, and M. Hebert. Event detection in crowded videos. In ICCV, 2007.\n\nActivity forecasting. K M Kitani, B Ziebart, J D Bagnell, M Hebert, ECCV. K. M. Kitani, B. Ziebart, J. D. Bagnell, and M. Hebert. Activity forecasting. In ECCV, 2012.\n\nThe language of actions: Recovering the syntax and semantics of goal-directed human activities. H Kuehne, A Arslan, T Serre, CVPR. H. Kuehne, A. Arslan, and T. Serre. The language of actions: Recovering the syntax and semantics of goal-directed human ac- tivities. In CVPR, 2014.\n\nHmdb: a large video database for human motion recognition. H Kuehne, H Jhuang, E Garrote, T Poggio, T Serre, ICCV. H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: a large video database for human motion recognition. In ICCV, 2011.\n\nDiscriminative figure-centric models for joint action localization and recognition. T Lan, Y Wang, G Mori, ICCV. T. Lan, Y. Wang, and G. Mori. Discriminative figure-centric mod- els for joint action localization and recognition. In ICCV, 2011.\n\nSingle view human action recognition using key pose matching and viterbi path searching. F J Lv, R Nevatia, CVPR. F. J. Lv and R. Nevatia. Single view human action recognition using key pose matching and viterbi path searching. In CVPR, 2007.\n\nInitialization strategies of spatio-temporal convolutional neural networks. E Mansimov, N Srivastava, R Salakhutdinov, arXiv:1503.07274CoRRE. Mansimov, N. Srivastava, and R. Salakhutdinov. Initializa- tion strategies of spatio-temporal convolutional neural networks. CoRR arXiv:1503.07274, 2015.\n\nActions in context. M Marsza\u0142ek, I Laptev, C Schmid, IEEE Conference on Computer Vision & Pattern Recognition. M. Marsza\u0142ek, I. Laptev, and C. Schmid. Actions in context. In IEEE Conference on Computer Vision & Pattern Recognition, 2009.\n\nEvaluating multimedia features and fusion for example-based event detection. Machine Vision and Applications. G K Myers, R Nallapati, J Van Hout, S Pancoast, R Nevatia, C Sun, A Habibian, D C Koelma, K E Van De Sande, A W Smeulders, 25G. K. Myers, R. Nallapati, J. van Hout, S. Pancoast, R. Nevatia, C. Sun, A. Habibian, D. C. Koelma, K. E. van de Sande, A. W. Smeulders, et al. Evaluating multimedia features and fusion for example-based event detection. Machine Vision and Applications, 25(1):17-32, 2014.\n\nBeyond short snippets: Deep networks for video classification. J Y Ng, M Hausknecht, S Vijayanarasimhan, O Vinyals, R Monga, G Toderici, arXiv:1503.08909J. Y.-H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici. Beyond short snippets: Deep networks for video classification. CoRR arXiv:1503.08909, 2015.\n\nMultiple granularity analysis for fine-grained action detection. B Ni, V R Paramathayalan, P Moulin, CVPR. B. Ni, V. R. Paramathayalan, and P. Moulin. Multiple granularity analysis for fine-grained action detection. In CVPR, 2014.\n\nModeling temporal structure of decomposable motion segments for activity classification. J C Niebles, C.-W Chen, L Fei-Fei, ECCV. J. C. Niebles, C.-W. Chen, and L. Fei-Fei. Modeling temporal structure of decomposable motion segments for activity classifica- tion. In ECCV, 2010.\n\nA large-scale benchmark dataset for event recognition in surveillance video. S Oh, A Hoogs, A Perera, N Cuntoor, C.-C Chen, J T Lee, S Mukherjee, J K Aggarwal, H Lee, L Davis, E Swears, X Wang, Q Ji, K Reddy, M Shah, C Vondrick, H Pirsiavash, D Ramanan, J Yuen, A Torralba, B Song, A Fong, A Roy-Chowdhury, M Desai, IEEE Conference on Computer Vision and Pattern Recognition. S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee, S. Mukherjee, J. K. Aggarwal, H. Lee, L. Davis, E. Swears, X. Wang, Q. Ji, K. Reddy, M. Shah, C. Vondrick, H. Pirsiavash, D. Ramanan, J. Yuen, A. Torralba, B. Song, A. Fong, A. Roy- Chowdhury, and M. Desai. A large-scale benchmark dataset for event recognition in surveillance video. In IEEE Conference on Computer Vision and Pattern Recognition, 2011.\n\nMultimedia event detection with multimodal feature fusion and temporal concept localization. Machine vision and applications. S Oh, S Mccloskey, I Kim, A Vahdat, K J Cannons, H Hajimirsadeghi, G Mori, A A Perera, M Pandey, J J Corso, 25S. Oh, S. Mccloskey, I. Kim, A. Vahdat, K. J. Cannons, H. Ha- jimirsadeghi, G. Mori, A. A. Perera, M. Pandey, and J. J. Corso. Multimedia event detection with multimodal feature fusion and temporal concept localization. Machine vision and applications, 25(1):49-69, 2014.\n\nTrecvid 2011 -an overview of the goals, tasks, data, evaluation mechansims and metrics. P Over, G Awad, M Michel, J Fiscus, W Kraaij, A F Smeaton, G Quenot, Proceedings of TRECVID 2011. TRECVID 2011P. Over, G. Awad, M. Michel, J. Fiscus, W. Kraaij, A. F. Smeaton, and G. Quenot. Trecvid 2011 -an overview of the goals, tasks, data, evaluation mechansims and metrics. In Proceedings of TRECVID 2011, 2011.\n\nParsing videos of actions with segmental grammars. H Pirsiavash, D Ramanan, CVPR. H. Pirsiavash and D. Ramanan. Parsing videos of actions with segmental grammars. In CVPR, 2014.\n\nA survey on vision-based human action recognition. R Poppe, IVC. 28R. Poppe. A survey on vision-based human action recognition. IVC, 28:976-990, 2010.\n\nHuman action segmentation and recognition using discriminative semi-markov models. L W A S Shi, Li Cheng, International Journal of Computer Vision. 93L. W. A. S. Qinfeng Shi, Li Cheng. Human action segmentation and recognition using discriminative semi-markov models. Inter- national Journal of Computer Vision, 93, May 2011.\n\nA database for fine grained activity detection of cooking activities. M Rohrbach, S Amin, M Andriluka, B Schiele, CVPR. M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A database for fine grained activity detection of cooking activities. In CVPR, 2012.\n\nRecognizing fine-grained and composite activities using hand-centric features and script data. M Rohrbach, A Rohrbach, M Regneri, S Amin, M Andriluka, M Pinkal, B Schiele, arXiv:1502.06648M. Rohrbach, A. Rohrbach, M. Regneri, S. Amin, M. Andriluka, M. Pinkal, and B. Schiele. Recognizing fine-grained and com- posite activities using hand-centric features and script data. CoRR arXiv:1502.06648, 2015.\n\nImageNet Large Scale Visual Recognition Challenge. O Russakovsky, International Journal of Computer Vision. O. Russakovsky et al. ImageNet Large Scale Visual Recogni- tion Challenge. International Journal of Computer Vision (IJCV), 2015.\n\nSpatio-temporal relationship match: Video structure comparison for recognition of complex human activities. M S Ryoo, J K Aggarwal, IEEE International Conference on Computer Vision (ICCV). M. S. Ryoo and J. K. Aggarwal. Spatio-temporal relationship match: Video structure comparison for recognition of complex hu- man activities. In IEEE International Conference on Computer Vision (ICCV), 2009.\n\nRecognizing human actions: A local svm approach. C Schuldt, I Laptev, B Caputo, ICPR. C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: A local svm approach. In ICPR, 2004.\n\nAction is in the eye of the beholder: Eye-gaze driven model for spatio-temporal action localization. N Shapovalova, M Raptis, L Sigal, G Mori, NIPS. N. Shapovalova, M. Raptis, L. Sigal, and G. Mori. Action is in the eye of the beholder: Eye-gaze driven model for spatio-temporal action localization. In NIPS, 2013.\n\nTwo-stream convolutional networks for action recognition in videos. K Simonyan, A Zisserman, NIPS. K. Simonyan and A. Zisserman. Two-stream convolutional net- works for action recognition in videos. In NIPS, 2014.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, abs/1409.1556CoRRK. Simonyan and A. Zisserman. Very deep convolutional net- works for large-scale image recognition. CoRR, abs/1409.1556, 2014.\n\nUcf101: A dataset of 101 human actions classes from videos in the wild. K Soomro, A R Zamir, M Shah, arXiv:1212.0402K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. Corr arXiv:1212.0402, 2012.\n\nUnsupervised learning of video representations using lstms. N Srivastava, E Mansimov, R Salakhutdinov, arXiv:1502.04681N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsuper- vised learning of video representations using lstms. CoRR arXiv:1502.04681, 2015.\n\nLearning latent temporal structure for complex event detection. K Tang, L Fei-Fei, D Koller, CVPR. K. Tang, L. Fei-Fei, and D. Koller. Learning latent temporal struc- ture for complex event detection. In CVPR, 2012.\n\nSpatiotemporal deformable part models for action detection. Y Tian, R Sukthankar, M Shah, CVPR. Y. Tian, R. Sukthankar, and M. Shah. Spatiotemporal deformable part models for action detection. In CVPR, 2013.\n\nLecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. T Tieleman, G E Hinton, T. Tieleman and G. E. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude., 2012.\n\nE-lamp: integration of innovative ideas for multimedia event detection. Machine Vision and Applications. W Tong, Y Yang, L Jiang, S.-I Yu, Z Lan, Z Ma, W Sze, E Younessian, A G Hauptmann, 25W. Tong, Y. Yang, L. Jiang, S.-I. Yu, Z. Lan, Z. Ma, W. Sze, E. Younessian, and A. G. Hauptmann. E-lamp: integration of in- novative ideas for multimedia event detection. Machine Vision and Applications, 25(1):5-15, 2014.\n\nC3d: Generic features for video analysis. D Tran, L Bourdev, R Fergus, L Torresani, M Paluri, arXiv:1412.0767D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. C3d: Generic features for video analysis. CoRR arXiv:1412.0767, 2015.\n\nA discriminative key pose sequence model for recognizing human interactions. A Vahdat, B Gao, M Ranjbar, G Mori, VS. A. Vahdat, B. Gao, M. Ranjbar, and G. Mori. A discriminative key pose sequence model for recognizing human interactions. In VS, 2011.\n\nAction recognition by dense trajectories. H Wang, A Kl\u00e4ser, C Schmid, C.-L Liu, CVPR. H. Wang, A. Kl\u00e4ser, C.Schmid, and C.-L. Liu. Action recognition by dense trajectories. In CVPR, 2011.\n\nAction recognition with improved trajectories. H Wang, C Schmid, IEEE International Conference on Computer Vision. Sydney, AustraliaH. Wang and C. Schmid. Action recognition with improved tra- jectories. In IEEE International Conference on Computer Vision, Sydney, Australia, 2013.\n\nTemporal pyramid pooling based convolutional neural networks for action recognition. P Wang, Y Cao, C Shen, L Liu, H T Shen, arXiv:1503.01224P. Wang, Y. Cao, C. Shen, L. Liu, and H. T. Shen. Temporal pyra- mid pooling based convolutional neural networks for action recog- nition. CoRR arXiv:1503.01224, 2015.\n\nA survey of vision-based methods for action representation, segmentation and recognition. D Weinland, R Ronfard, E Boyer, In CVIU. 1152241D. Weinland, R. Ronfard, and E. Boyer. A survey of vision-based methods for action representation, segmentation and recognition. In CVIU 115(2), pp. 224,241, 2010.\n\nShow, attend and tell: Neural image caption generation with visual attention. K Xu, arXiv:1502.03044K. Xu et al. Show, attend and tell: Neural image caption generation with visual attention. CoRR arXiv:1502.03044, 2015.\n\nRecognizing human action in time-sequential images using hidden markov model. J Yamato, J Ohya, K Ishii, CVPR. J. Yamato, J. Ohya, and K. Ishii. Recognizing human action in time-sequential images using hidden markov model. In CVPR, 1992.\n\nVideo description generation incorporating spatio-temporal features and a soft-attention mechanism. L Yao, A Torabi, K Cho, N Ballas, C Pal, H Larochelle, A Courville, arXiv:1502.08029L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville. Video description generation incorporating spatio-temporal features and a soft-attention mechanism. CoRR arXiv:1502.08029, 2015.\n\nExploiting image-trained cnn architectures for unconstrained video classification. S Zha, F Luisier, W Andrews, N Srivastava, R Salakhutdinov, arXiv:1503.04144S. Zha, F. Luisier, W. Andrews, N. Srivastava, and R. Salakhutdi- nov. Exploiting image-trained cnn architectures for unconstrained video classification. CoRR arXiv:1503.04144, 2015.\n", "annotations": {"author": "[{\"end\":90,\"start\":77},{\"end\":108,\"start\":91},{\"end\":116,\"start\":109},{\"end\":142,\"start\":117},{\"end\":153,\"start\":143},{\"end\":159,\"start\":154},{\"end\":168,\"start\":160}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":84},{\"end\":107,\"start\":96},{\"end\":115,\"start\":111},{\"end\":141,\"start\":123},{\"end\":152,\"start\":148},{\"end\":158,\"start\":156}]", "author_first_name": "[{\"end\":83,\"start\":77},{\"end\":95,\"start\":91},{\"end\":110,\"start\":109},{\"end\":120,\"start\":117},{\"end\":122,\"start\":121},{\"end\":147,\"start\":143},{\"end\":155,\"start\":154},{\"end\":167,\"start\":160}]", "author_affiliation": null, "title": "[{\"end\":74,\"start\":1},{\"end\":242,\"start\":169}]", "venue": null, "abstract": "[{\"end\":1094,\"start\":329}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1513,\"start\":1509},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1551,\"start\":1547},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2832,\"start\":2828},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2845,\"start\":2841},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2864,\"start\":2860},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3091,\"start\":3087},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3110,\"start\":3106},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3305,\"start\":3302},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4802,\"start\":4799},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5798,\"start\":5794},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5823,\"start\":5819},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5992,\"start\":5988},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6009,\"start\":6006},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6431,\"start\":6427},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6487,\"start\":6483},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6490,\"start\":6487},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6716,\"start\":6712},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6727,\"start\":6723},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6741,\"start\":6737},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6758,\"start\":6755},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6774,\"start\":6771},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6975,\"start\":6971},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6994,\"start\":6990},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7299,\"start\":7295},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7326,\"start\":7322},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7648,\"start\":7644},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7667,\"start\":7663},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7688,\"start\":7684},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7710,\"start\":7706},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7735,\"start\":7731},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7756,\"start\":7753},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7957,\"start\":7953},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8044,\"start\":8040},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8332,\"start\":8328},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8452,\"start\":8448},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8768,\"start\":8764},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8771,\"start\":8768},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8774,\"start\":8771},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8873,\"start\":8869},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8876,\"start\":8873},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8879,\"start\":8876},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8974,\"start\":8971},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8997,\"start\":8994},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9000,\"start\":8997},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9003,\"start\":9000},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9006,\"start\":9003},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9462,\"start\":9458},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9558,\"start\":9554},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9561,\"start\":9558},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9583,\"start\":9579},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9686,\"start\":9682},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9830,\"start\":9827},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10145,\"start\":10142},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10227,\"start\":10223},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10308,\"start\":10304},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10378,\"start\":10374},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10796,\"start\":10793},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10900,\"start\":10896},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10919,\"start\":10915},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13775,\"start\":13772},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17430,\"start\":17427},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17433,\"start\":17430},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17436,\"start\":17433},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19672,\"start\":19669},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19674,\"start\":19672},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22167,\"start\":22164},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22616,\"start\":22613},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24137,\"start\":24136},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25404,\"start\":25400},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26266,\"start\":26262},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26331,\"start\":26327},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27057,\"start\":27053},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28845,\"start\":28842},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":29433,\"start\":29429},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30309,\"start\":30306},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36549,\"start\":36546},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":36736,\"start\":36733}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36465,\"start\":36383},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36701,\"start\":36466},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36925,\"start\":36702},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37472,\"start\":36926},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37518,\"start\":37473},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37779,\"start\":37519},{\"attributes\":{\"id\":\"fig_8\"},\"end\":37984,\"start\":37780},{\"attributes\":{\"id\":\"fig_9\"},\"end\":38233,\"start\":37985},{\"attributes\":{\"id\":\"fig_10\"},\"end\":38355,\"start\":38234},{\"attributes\":{\"id\":\"fig_11\"},\"end\":38712,\"start\":38356},{\"attributes\":{\"id\":\"fig_12\"},\"end\":39243,\"start\":38713},{\"attributes\":{\"id\":\"fig_13\"},\"end\":39520,\"start\":39244},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40808,\"start\":39521},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40980,\"start\":40809},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41152,\"start\":40981},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41353,\"start\":41153}]", "paragraph": "[{\"end\":2185,\"start\":1103},{\"end\":2403,\"start\":2187},{\"end\":3394,\"start\":2445},{\"end\":4397,\"start\":3396},{\"end\":5198,\"start\":4399},{\"end\":5495,\"start\":5200},{\"end\":5646,\"start\":5497},{\"end\":5889,\"start\":5663},{\"end\":6995,\"start\":5902},{\"end\":7591,\"start\":6997},{\"end\":7798,\"start\":7605},{\"end\":8775,\"start\":7828},{\"end\":9254,\"start\":8777},{\"end\":10025,\"start\":9275},{\"end\":10567,\"start\":10052},{\"end\":11903,\"start\":10595},{\"end\":12463,\"start\":11905},{\"end\":13519,\"start\":12465},{\"end\":14036,\"start\":13521},{\"end\":14101,\"start\":14038},{\"end\":16917,\"start\":14103},{\"end\":17142,\"start\":16919},{\"end\":18074,\"start\":17165},{\"end\":18205,\"start\":18076},{\"end\":18712,\"start\":18214},{\"end\":19921,\"start\":18961},{\"end\":20504,\"start\":19935},{\"end\":21033,\"start\":20524},{\"end\":21209,\"start\":21098},{\"end\":21625,\"start\":21253},{\"end\":22168,\"start\":21627},{\"end\":22681,\"start\":22170},{\"end\":23540,\"start\":22740},{\"end\":24971,\"start\":23561},{\"end\":25549,\"start\":24996},{\"end\":25785,\"start\":25565},{\"end\":26177,\"start\":25805},{\"end\":26547,\"start\":26204},{\"end\":27122,\"start\":26549},{\"end\":27502,\"start\":27146},{\"end\":28067,\"start\":27504},{\"end\":29139,\"start\":28141},{\"end\":30192,\"start\":29160},{\"end\":31091,\"start\":30194},{\"end\":31856,\"start\":31093},{\"end\":32327,\"start\":31858},{\"end\":32817,\"start\":32349},{\"end\":33712,\"start\":32819},{\"end\":35200,\"start\":33714},{\"end\":36382,\"start\":35215}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18937,\"start\":18713},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18960,\"start\":18937},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21097,\"start\":21034},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22739,\"start\":22682},{\"attributes\":{\"id\":\"formula_4\"},\"end\":28140,\"start\":28068}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7405,\"start\":7398},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11102,\"start\":11093},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28667,\"start\":28660}]", "section_header": "[{\"end\":2443,\"start\":2406},{\"attributes\":{\"n\":\"2\"},\"end\":5661,\"start\":5649},{\"attributes\":{\"n\":\"2.1\"},\"end\":5900,\"start\":5892},{\"end\":7603,\"start\":7594},{\"attributes\":{\"n\":\"2.3\"},\"end\":7826,\"start\":7801},{\"attributes\":{\"n\":\"2.4\"},\"end\":9273,\"start\":9257},{\"attributes\":{\"n\":\"2.5\"},\"end\":10050,\"start\":10028},{\"attributes\":{\"n\":\"3\"},\"end\":10593,\"start\":10570},{\"attributes\":{\"n\":\"4\"},\"end\":17163,\"start\":17145},{\"attributes\":{\"n\":\"4.1\"},\"end\":18212,\"start\":18208},{\"attributes\":{\"n\":\"4.2\"},\"end\":19933,\"start\":19924},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":20522,\"start\":20507},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":21251,\"start\":21212},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":23559,\"start\":23543},{\"attributes\":{\"n\":\"4.2.4\"},\"end\":24994,\"start\":24974},{\"attributes\":{\"n\":\"5\"},\"end\":25563,\"start\":25552},{\"attributes\":{\"n\":\"5.1\"},\"end\":25793,\"start\":25788},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":25803,\"start\":25796},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":26202,\"start\":26180},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":27144,\"start\":27125},{\"attributes\":{\"n\":\"5.2\"},\"end\":29158,\"start\":29142},{\"attributes\":{\"n\":\"5.3\"},\"end\":32347,\"start\":32330},{\"attributes\":{\"n\":\"6\"},\"end\":35213,\"start\":35203},{\"end\":36392,\"start\":36384},{\"end\":36475,\"start\":36467},{\"end\":36711,\"start\":36703},{\"end\":36935,\"start\":36927},{\"end\":37482,\"start\":37474},{\"end\":37528,\"start\":37520},{\"end\":37789,\"start\":37781},{\"end\":37994,\"start\":37986},{\"end\":38244,\"start\":38235},{\"end\":38375,\"start\":38357},{\"end\":38721,\"start\":38714},{\"end\":39254,\"start\":39245},{\"end\":39531,\"start\":39522},{\"end\":40817,\"start\":40810}]", "table": "[{\"end\":40808,\"start\":39605},{\"end\":41152,\"start\":41013}]", "figure_caption": "[{\"end\":36465,\"start\":36394},{\"end\":36701,\"start\":36477},{\"end\":36925,\"start\":36713},{\"end\":37472,\"start\":36937},{\"end\":37518,\"start\":37484},{\"end\":37779,\"start\":37530},{\"end\":37984,\"start\":37791},{\"end\":38233,\"start\":37996},{\"end\":38355,\"start\":38247},{\"end\":38712,\"start\":38380},{\"end\":39243,\"start\":38724},{\"end\":39520,\"start\":39257},{\"end\":39605,\"start\":39533},{\"end\":40980,\"start\":40819},{\"end\":41013,\"start\":40983},{\"end\":41353,\"start\":41155}]", "figure_ref": "[{\"end\":1102,\"start\":1096},{\"end\":1354,\"start\":1346},{\"end\":4344,\"start\":4336},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11286,\"start\":11278},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13468,\"start\":13460},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13849,\"start\":13841},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14836,\"start\":14828},{\"end\":16503,\"start\":16495},{\"end\":17994,\"start\":17988},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21828,\"start\":21816},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23731,\"start\":23719},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25252,\"start\":25240},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29020,\"start\":29012},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29132,\"start\":29124},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":31480,\"start\":31471},{\"end\":32059,\"start\":32050},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32097,\"start\":32088},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32667,\"start\":32658},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32935,\"start\":32926},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":34976,\"start\":34967}]", "bib_author_first_name": "[{\"end\":42332,\"start\":42331},{\"end\":42344,\"start\":42343},{\"end\":42351,\"start\":42350},{\"end\":42542,\"start\":42541},{\"end\":42551,\"start\":42550},{\"end\":42563,\"start\":42562},{\"end\":42576,\"start\":42575},{\"end\":42585,\"start\":42584},{\"end\":42913,\"start\":42912},{\"end\":42915,\"start\":42914},{\"end\":42923,\"start\":42922},{\"end\":42925,\"start\":42924},{\"end\":42932,\"start\":42931},{\"end\":42944,\"start\":42943},{\"end\":42946,\"start\":42945},{\"end\":43191,\"start\":43190},{\"end\":43202,\"start\":43201},{\"end\":43204,\"start\":43203},{\"end\":43217,\"start\":43216},{\"end\":43231,\"start\":43230},{\"end\":43243,\"start\":43242},{\"end\":43258,\"start\":43257},{\"end\":43268,\"start\":43267},{\"end\":43585,\"start\":43584},{\"end\":43587,\"start\":43586},{\"end\":43616,\"start\":43610},{\"end\":43628,\"start\":43627},{\"end\":43630,\"start\":43629},{\"end\":44050,\"start\":44049},{\"end\":44062,\"start\":44061},{\"end\":44187,\"start\":44186},{\"end\":44201,\"start\":44200},{\"end\":44410,\"start\":44409},{\"end\":44418,\"start\":44417},{\"end\":44426,\"start\":44425},{\"end\":44580,\"start\":44576},{\"end\":44589,\"start\":44588},{\"end\":44596,\"start\":44595},{\"end\":44605,\"start\":44604},{\"end\":44617,\"start\":44616},{\"end\":44627,\"start\":44626},{\"end\":44635,\"start\":44634},{\"end\":44987,\"start\":44986},{\"end\":44999,\"start\":44998},{\"end\":45011,\"start\":45010},{\"end\":45021,\"start\":45020},{\"end\":45030,\"start\":45029},{\"end\":45044,\"start\":45043},{\"end\":45259,\"start\":45258},{\"end\":45265,\"start\":45264},{\"end\":45279,\"start\":45278},{\"end\":45405,\"start\":45404},{\"end\":45407,\"start\":45406},{\"end\":45417,\"start\":45416},{\"end\":45428,\"start\":45427},{\"end\":45430,\"start\":45429},{\"end\":45441,\"start\":45440},{\"end\":45647,\"start\":45646},{\"end\":45657,\"start\":45656},{\"end\":45667,\"start\":45666},{\"end\":45891,\"start\":45890},{\"end\":45901,\"start\":45900},{\"end\":45911,\"start\":45910},{\"end\":45922,\"start\":45921},{\"end\":45932,\"start\":45931},{\"end\":46165,\"start\":46164},{\"end\":46172,\"start\":46171},{\"end\":46180,\"start\":46179},{\"end\":46415,\"start\":46414},{\"end\":46417,\"start\":46416},{\"end\":46423,\"start\":46422},{\"end\":46646,\"start\":46645},{\"end\":46658,\"start\":46657},{\"end\":46672,\"start\":46671},{\"end\":46887,\"start\":46886},{\"end\":46900,\"start\":46899},{\"end\":46910,\"start\":46909},{\"end\":47216,\"start\":47215},{\"end\":47218,\"start\":47217},{\"end\":47227,\"start\":47226},{\"end\":47240,\"start\":47239},{\"end\":47252,\"start\":47251},{\"end\":47264,\"start\":47263},{\"end\":47275,\"start\":47274},{\"end\":47282,\"start\":47281},{\"end\":47294,\"start\":47293},{\"end\":47296,\"start\":47295},{\"end\":47306,\"start\":47305},{\"end\":47308,\"start\":47307},{\"end\":47324,\"start\":47323},{\"end\":47326,\"start\":47325},{\"end\":47678,\"start\":47677},{\"end\":47680,\"start\":47679},{\"end\":47686,\"start\":47685},{\"end\":47700,\"start\":47699},{\"end\":47720,\"start\":47719},{\"end\":47731,\"start\":47730},{\"end\":47740,\"start\":47739},{\"end\":48014,\"start\":48013},{\"end\":48020,\"start\":48019},{\"end\":48022,\"start\":48021},{\"end\":48040,\"start\":48039},{\"end\":48270,\"start\":48269},{\"end\":48272,\"start\":48271},{\"end\":48286,\"start\":48282},{\"end\":48294,\"start\":48293},{\"end\":48538,\"start\":48537},{\"end\":48544,\"start\":48543},{\"end\":48553,\"start\":48552},{\"end\":48563,\"start\":48562},{\"end\":48577,\"start\":48573},{\"end\":48585,\"start\":48584},{\"end\":48587,\"start\":48586},{\"end\":48594,\"start\":48593},{\"end\":48607,\"start\":48606},{\"end\":48609,\"start\":48608},{\"end\":48621,\"start\":48620},{\"end\":48628,\"start\":48627},{\"end\":48637,\"start\":48636},{\"end\":48647,\"start\":48646},{\"end\":48655,\"start\":48654},{\"end\":48661,\"start\":48660},{\"end\":48670,\"start\":48669},{\"end\":48678,\"start\":48677},{\"end\":48690,\"start\":48689},{\"end\":48704,\"start\":48703},{\"end\":48715,\"start\":48714},{\"end\":48723,\"start\":48722},{\"end\":48735,\"start\":48734},{\"end\":48743,\"start\":48742},{\"end\":48751,\"start\":48750},{\"end\":48768,\"start\":48767},{\"end\":49380,\"start\":49379},{\"end\":49386,\"start\":49385},{\"end\":49399,\"start\":49398},{\"end\":49406,\"start\":49405},{\"end\":49416,\"start\":49415},{\"end\":49418,\"start\":49417},{\"end\":49429,\"start\":49428},{\"end\":49447,\"start\":49446},{\"end\":49455,\"start\":49454},{\"end\":49457,\"start\":49456},{\"end\":49467,\"start\":49466},{\"end\":49477,\"start\":49476},{\"end\":49479,\"start\":49478},{\"end\":49851,\"start\":49850},{\"end\":49859,\"start\":49858},{\"end\":49867,\"start\":49866},{\"end\":49877,\"start\":49876},{\"end\":49887,\"start\":49886},{\"end\":49897,\"start\":49896},{\"end\":49899,\"start\":49898},{\"end\":49910,\"start\":49909},{\"end\":50220,\"start\":50219},{\"end\":50234,\"start\":50233},{\"end\":50399,\"start\":50398},{\"end\":50583,\"start\":50582},{\"end\":50589,\"start\":50584},{\"end\":50597,\"start\":50595},{\"end\":50897,\"start\":50896},{\"end\":50909,\"start\":50908},{\"end\":50917,\"start\":50916},{\"end\":50930,\"start\":50929},{\"end\":51180,\"start\":51179},{\"end\":51192,\"start\":51191},{\"end\":51204,\"start\":51203},{\"end\":51215,\"start\":51214},{\"end\":51223,\"start\":51222},{\"end\":51236,\"start\":51235},{\"end\":51246,\"start\":51245},{\"end\":51539,\"start\":51538},{\"end\":51835,\"start\":51834},{\"end\":51837,\"start\":51836},{\"end\":51845,\"start\":51844},{\"end\":51847,\"start\":51846},{\"end\":52173,\"start\":52172},{\"end\":52184,\"start\":52183},{\"end\":52194,\"start\":52193},{\"end\":52414,\"start\":52413},{\"end\":52429,\"start\":52428},{\"end\":52439,\"start\":52438},{\"end\":52448,\"start\":52447},{\"end\":52697,\"start\":52696},{\"end\":52709,\"start\":52708},{\"end\":52912,\"start\":52911},{\"end\":52924,\"start\":52923},{\"end\":53154,\"start\":53153},{\"end\":53164,\"start\":53163},{\"end\":53166,\"start\":53165},{\"end\":53175,\"start\":53174},{\"end\":53396,\"start\":53395},{\"end\":53410,\"start\":53409},{\"end\":53422,\"start\":53421},{\"end\":53661,\"start\":53660},{\"end\":53669,\"start\":53668},{\"end\":53680,\"start\":53679},{\"end\":53874,\"start\":53873},{\"end\":53882,\"start\":53881},{\"end\":53896,\"start\":53895},{\"end\":54110,\"start\":54109},{\"end\":54122,\"start\":54121},{\"end\":54124,\"start\":54123},{\"end\":54364,\"start\":54363},{\"end\":54372,\"start\":54371},{\"end\":54380,\"start\":54379},{\"end\":54392,\"start\":54388},{\"end\":54398,\"start\":54397},{\"end\":54405,\"start\":54404},{\"end\":54411,\"start\":54410},{\"end\":54418,\"start\":54417},{\"end\":54432,\"start\":54431},{\"end\":54434,\"start\":54433},{\"end\":54714,\"start\":54713},{\"end\":54722,\"start\":54721},{\"end\":54733,\"start\":54732},{\"end\":54743,\"start\":54742},{\"end\":54756,\"start\":54755},{\"end\":54990,\"start\":54989},{\"end\":55000,\"start\":54999},{\"end\":55007,\"start\":55006},{\"end\":55018,\"start\":55017},{\"end\":55207,\"start\":55206},{\"end\":55215,\"start\":55214},{\"end\":55225,\"start\":55224},{\"end\":55238,\"start\":55234},{\"end\":55401,\"start\":55400},{\"end\":55409,\"start\":55408},{\"end\":55722,\"start\":55721},{\"end\":55730,\"start\":55729},{\"end\":55737,\"start\":55736},{\"end\":55745,\"start\":55744},{\"end\":55752,\"start\":55751},{\"end\":55754,\"start\":55753},{\"end\":56037,\"start\":56036},{\"end\":56049,\"start\":56048},{\"end\":56060,\"start\":56059},{\"end\":56328,\"start\":56327},{\"end\":56549,\"start\":56548},{\"end\":56559,\"start\":56558},{\"end\":56567,\"start\":56566},{\"end\":56810,\"start\":56809},{\"end\":56817,\"start\":56816},{\"end\":56827,\"start\":56826},{\"end\":56834,\"start\":56833},{\"end\":56844,\"start\":56843},{\"end\":56851,\"start\":56850},{\"end\":56865,\"start\":56864},{\"end\":57186,\"start\":57185},{\"end\":57193,\"start\":57192},{\"end\":57204,\"start\":57203},{\"end\":57215,\"start\":57214},{\"end\":57229,\"start\":57228}]", "bib_author_last_name": "[{\"end\":42341,\"start\":42333},{\"end\":42348,\"start\":42345},{\"end\":42358,\"start\":42352},{\"end\":42548,\"start\":42543},{\"end\":42560,\"start\":42552},{\"end\":42573,\"start\":42564},{\"end\":42582,\"start\":42577},{\"end\":42591,\"start\":42586},{\"end\":42920,\"start\":42916},{\"end\":42929,\"start\":42926},{\"end\":42941,\"start\":42933},{\"end\":42954,\"start\":42947},{\"end\":43199,\"start\":43192},{\"end\":43214,\"start\":43205},{\"end\":43228,\"start\":43218},{\"end\":43240,\"start\":43232},{\"end\":43255,\"start\":43244},{\"end\":43265,\"start\":43259},{\"end\":43276,\"start\":43269},{\"end\":43608,\"start\":43588},{\"end\":43625,\"start\":43617},{\"end\":43638,\"start\":43631},{\"end\":44059,\"start\":44051},{\"end\":44068,\"start\":44063},{\"end\":44198,\"start\":44188},{\"end\":44213,\"start\":44202},{\"end\":44415,\"start\":44411},{\"end\":44423,\"start\":44419},{\"end\":44433,\"start\":44427},{\"end\":44586,\"start\":44581},{\"end\":44593,\"start\":44590},{\"end\":44602,\"start\":44597},{\"end\":44614,\"start\":44606},{\"end\":44624,\"start\":44618},{\"end\":44632,\"start\":44628},{\"end\":44646,\"start\":44636},{\"end\":44996,\"start\":44988},{\"end\":45008,\"start\":45000},{\"end\":45018,\"start\":45012},{\"end\":45027,\"start\":45022},{\"end\":45041,\"start\":45031},{\"end\":45052,\"start\":45045},{\"end\":45262,\"start\":45260},{\"end\":45276,\"start\":45266},{\"end\":45286,\"start\":45280},{\"end\":45414,\"start\":45408},{\"end\":45425,\"start\":45418},{\"end\":45438,\"start\":45431},{\"end\":45448,\"start\":45442},{\"end\":45654,\"start\":45648},{\"end\":45664,\"start\":45658},{\"end\":45673,\"start\":45668},{\"end\":45898,\"start\":45892},{\"end\":45908,\"start\":45902},{\"end\":45919,\"start\":45912},{\"end\":45929,\"start\":45923},{\"end\":45938,\"start\":45933},{\"end\":46169,\"start\":46166},{\"end\":46177,\"start\":46173},{\"end\":46185,\"start\":46181},{\"end\":46420,\"start\":46418},{\"end\":46431,\"start\":46424},{\"end\":46655,\"start\":46647},{\"end\":46669,\"start\":46659},{\"end\":46686,\"start\":46673},{\"end\":46897,\"start\":46888},{\"end\":46907,\"start\":46901},{\"end\":46917,\"start\":46911},{\"end\":47224,\"start\":47219},{\"end\":47237,\"start\":47228},{\"end\":47249,\"start\":47241},{\"end\":47261,\"start\":47253},{\"end\":47272,\"start\":47265},{\"end\":47279,\"start\":47276},{\"end\":47291,\"start\":47283},{\"end\":47303,\"start\":47297},{\"end\":47321,\"start\":47309},{\"end\":47336,\"start\":47327},{\"end\":47683,\"start\":47681},{\"end\":47697,\"start\":47687},{\"end\":47717,\"start\":47701},{\"end\":47728,\"start\":47721},{\"end\":47737,\"start\":47732},{\"end\":47749,\"start\":47741},{\"end\":48017,\"start\":48015},{\"end\":48037,\"start\":48023},{\"end\":48047,\"start\":48041},{\"end\":48280,\"start\":48273},{\"end\":48291,\"start\":48287},{\"end\":48302,\"start\":48295},{\"end\":48541,\"start\":48539},{\"end\":48550,\"start\":48545},{\"end\":48560,\"start\":48554},{\"end\":48571,\"start\":48564},{\"end\":48582,\"start\":48578},{\"end\":48591,\"start\":48588},{\"end\":48604,\"start\":48595},{\"end\":48618,\"start\":48610},{\"end\":48625,\"start\":48622},{\"end\":48634,\"start\":48629},{\"end\":48644,\"start\":48638},{\"end\":48652,\"start\":48648},{\"end\":48658,\"start\":48656},{\"end\":48667,\"start\":48662},{\"end\":48675,\"start\":48671},{\"end\":48687,\"start\":48679},{\"end\":48701,\"start\":48691},{\"end\":48712,\"start\":48705},{\"end\":48720,\"start\":48716},{\"end\":48732,\"start\":48724},{\"end\":48740,\"start\":48736},{\"end\":48748,\"start\":48744},{\"end\":48765,\"start\":48752},{\"end\":48774,\"start\":48769},{\"end\":49383,\"start\":49381},{\"end\":49396,\"start\":49387},{\"end\":49403,\"start\":49400},{\"end\":49413,\"start\":49407},{\"end\":49426,\"start\":49419},{\"end\":49444,\"start\":49430},{\"end\":49452,\"start\":49448},{\"end\":49464,\"start\":49458},{\"end\":49474,\"start\":49468},{\"end\":49485,\"start\":49480},{\"end\":49856,\"start\":49852},{\"end\":49864,\"start\":49860},{\"end\":49874,\"start\":49868},{\"end\":49884,\"start\":49878},{\"end\":49894,\"start\":49888},{\"end\":49907,\"start\":49900},{\"end\":49917,\"start\":49911},{\"end\":50231,\"start\":50221},{\"end\":50242,\"start\":50235},{\"end\":50405,\"start\":50400},{\"end\":50593,\"start\":50590},{\"end\":50603,\"start\":50598},{\"end\":50906,\"start\":50898},{\"end\":50914,\"start\":50910},{\"end\":50927,\"start\":50918},{\"end\":50938,\"start\":50931},{\"end\":51189,\"start\":51181},{\"end\":51201,\"start\":51193},{\"end\":51212,\"start\":51205},{\"end\":51220,\"start\":51216},{\"end\":51233,\"start\":51224},{\"end\":51243,\"start\":51237},{\"end\":51254,\"start\":51247},{\"end\":51551,\"start\":51540},{\"end\":51842,\"start\":51838},{\"end\":51856,\"start\":51848},{\"end\":52181,\"start\":52174},{\"end\":52191,\"start\":52185},{\"end\":52201,\"start\":52195},{\"end\":52426,\"start\":52415},{\"end\":52436,\"start\":52430},{\"end\":52445,\"start\":52440},{\"end\":52453,\"start\":52449},{\"end\":52706,\"start\":52698},{\"end\":52719,\"start\":52710},{\"end\":52921,\"start\":52913},{\"end\":52934,\"start\":52925},{\"end\":53161,\"start\":53155},{\"end\":53172,\"start\":53167},{\"end\":53180,\"start\":53176},{\"end\":53407,\"start\":53397},{\"end\":53419,\"start\":53411},{\"end\":53436,\"start\":53423},{\"end\":53666,\"start\":53662},{\"end\":53677,\"start\":53670},{\"end\":53687,\"start\":53681},{\"end\":53879,\"start\":53875},{\"end\":53893,\"start\":53883},{\"end\":53901,\"start\":53897},{\"end\":54119,\"start\":54111},{\"end\":54131,\"start\":54125},{\"end\":54369,\"start\":54365},{\"end\":54377,\"start\":54373},{\"end\":54386,\"start\":54381},{\"end\":54395,\"start\":54393},{\"end\":54402,\"start\":54399},{\"end\":54408,\"start\":54406},{\"end\":54415,\"start\":54412},{\"end\":54429,\"start\":54419},{\"end\":54444,\"start\":54435},{\"end\":54719,\"start\":54715},{\"end\":54730,\"start\":54723},{\"end\":54740,\"start\":54734},{\"end\":54753,\"start\":54744},{\"end\":54763,\"start\":54757},{\"end\":54997,\"start\":54991},{\"end\":55004,\"start\":55001},{\"end\":55015,\"start\":55008},{\"end\":55023,\"start\":55019},{\"end\":55212,\"start\":55208},{\"end\":55222,\"start\":55216},{\"end\":55232,\"start\":55226},{\"end\":55242,\"start\":55239},{\"end\":55406,\"start\":55402},{\"end\":55416,\"start\":55410},{\"end\":55727,\"start\":55723},{\"end\":55734,\"start\":55731},{\"end\":55742,\"start\":55738},{\"end\":55749,\"start\":55746},{\"end\":55759,\"start\":55755},{\"end\":56046,\"start\":56038},{\"end\":56057,\"start\":56050},{\"end\":56066,\"start\":56061},{\"end\":56331,\"start\":56329},{\"end\":56556,\"start\":56550},{\"end\":56564,\"start\":56560},{\"end\":56573,\"start\":56568},{\"end\":56814,\"start\":56811},{\"end\":56824,\"start\":56818},{\"end\":56831,\"start\":56828},{\"end\":56841,\"start\":56835},{\"end\":56848,\"start\":56845},{\"end\":56862,\"start\":56852},{\"end\":56875,\"start\":56866},{\"end\":57190,\"start\":57187},{\"end\":57201,\"start\":57194},{\"end\":57212,\"start\":57205},{\"end\":57226,\"start\":57216},{\"end\":57243,\"start\":57230}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b0\"},\"end\":42509,\"start\":42260},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":175905},\"end\":42836,\"start\":42511},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8847270},\"end\":43105,\"start\":42838},{\"attributes\":{\"doi\":\"arXiv:1411.4389\",\"id\":\"b3\"},\"end\":43505,\"start\":43107},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1710722},\"end\":44025,\"start\":43507},{\"attributes\":{\"doi\":\"arXiv:1411.6031\",\"id\":\"b5\"},\"end\":44160,\"start\":44027},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1915014},\"end\":44338,\"start\":44162},{\"attributes\":{\"id\":\"b7\"},\"end\":44574,\"start\":44340},{\"attributes\":{\"id\":\"b8\"},\"end\":44915,\"start\":44576},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206592218},\"end\":45221,\"start\":44917},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":17144146},\"end\":45380,\"start\":45223},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8979634},\"end\":45548,\"start\":45382},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9621856},\"end\":45829,\"start\":45550},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206769852},\"end\":46078,\"start\":45831},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":78173},\"end\":46323,\"start\":46080},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7757210},\"end\":46567,\"start\":46325},{\"attributes\":{\"doi\":\"arXiv:1503.07274\",\"id\":\"b16\"},\"end\":46864,\"start\":46569},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3155054},\"end\":47103,\"start\":46866},{\"attributes\":{\"id\":\"b18\"},\"end\":47612,\"start\":47105},{\"attributes\":{\"doi\":\"arXiv:1503.08909\",\"id\":\"b19\"},\"end\":47946,\"start\":47614},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13571114},\"end\":48178,\"start\":47948},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14779543},\"end\":48458,\"start\":48180},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1826131},\"end\":49251,\"start\":48460},{\"attributes\":{\"id\":\"b23\"},\"end\":49760,\"start\":49253},{\"attributes\":{\"id\":\"b24\"},\"end\":50166,\"start\":49762},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1887226},\"end\":50345,\"start\":50168},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":77451},\"end\":50497,\"start\":50347},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9054863},\"end\":50824,\"start\":50499},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":9349950},\"end\":51082,\"start\":50826},{\"attributes\":{\"doi\":\"arXiv:1502.06648\",\"id\":\"b29\"},\"end\":51485,\"start\":51084},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2930547},\"end\":51724,\"start\":51487},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":9333559},\"end\":52121,\"start\":51726},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8777811},\"end\":52310,\"start\":52123},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":5302242},\"end\":52626,\"start\":52312},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11797475},\"end\":52841,\"start\":52628},{\"attributes\":{\"doi\":\"abs/1409.1556\",\"id\":\"b35\"},\"end\":53079,\"start\":52843},{\"attributes\":{\"doi\":\"arXiv:1212.0402\",\"id\":\"b36\"},\"end\":53333,\"start\":53081},{\"attributes\":{\"doi\":\"arXiv:1502.04681\",\"id\":\"b37\"},\"end\":53594,\"start\":53335},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":11359025},\"end\":53811,\"start\":53596},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":622833},\"end\":54020,\"start\":53813},{\"attributes\":{\"id\":\"b40\"},\"end\":54256,\"start\":54022},{\"attributes\":{\"id\":\"b41\"},\"end\":54669,\"start\":54258},{\"attributes\":{\"doi\":\"arXiv:1412.0767\",\"id\":\"b42\"},\"end\":54910,\"start\":54671},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":16494320},\"end\":55162,\"start\":54912},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":13537104},\"end\":55351,\"start\":55164},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":753512},\"end\":55634,\"start\":55353},{\"attributes\":{\"doi\":\"arXiv:1503.01224\",\"id\":\"b46\"},\"end\":55944,\"start\":55636},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3467933},\"end\":56247,\"start\":55946},{\"attributes\":{\"doi\":\"arXiv:1502.03044\",\"id\":\"b48\"},\"end\":56468,\"start\":56249},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":28489640},\"end\":56707,\"start\":56470},{\"attributes\":{\"doi\":\"arXiv:1502.08029\",\"id\":\"b50\"},\"end\":57100,\"start\":56709},{\"attributes\":{\"doi\":\"arXiv:1503.04144\",\"id\":\"b51\"},\"end\":57443,\"start\":57102}]", "bib_title": "[{\"end\":42539,\"start\":42511},{\"end\":42910,\"start\":42838},{\"end\":43582,\"start\":43507},{\"end\":44184,\"start\":44162},{\"end\":44984,\"start\":44917},{\"end\":45256,\"start\":45223},{\"end\":45402,\"start\":45382},{\"end\":45644,\"start\":45550},{\"end\":45888,\"start\":45831},{\"end\":46162,\"start\":46080},{\"end\":46412,\"start\":46325},{\"end\":46884,\"start\":46866},{\"end\":48011,\"start\":47948},{\"end\":48267,\"start\":48180},{\"end\":48535,\"start\":48460},{\"end\":49848,\"start\":49762},{\"end\":50217,\"start\":50168},{\"end\":50396,\"start\":50347},{\"end\":50580,\"start\":50499},{\"end\":50894,\"start\":50826},{\"end\":51536,\"start\":51487},{\"end\":51832,\"start\":51726},{\"end\":52170,\"start\":52123},{\"end\":52411,\"start\":52312},{\"end\":52694,\"start\":52628},{\"end\":53658,\"start\":53596},{\"end\":53871,\"start\":53813},{\"end\":54987,\"start\":54912},{\"end\":55204,\"start\":55164},{\"end\":55398,\"start\":55353},{\"end\":56034,\"start\":55946},{\"end\":56546,\"start\":56470}]", "bib_author": "[{\"end\":42343,\"start\":42331},{\"end\":42350,\"start\":42343},{\"end\":42360,\"start\":42350},{\"end\":42550,\"start\":42541},{\"end\":42562,\"start\":42550},{\"end\":42575,\"start\":42562},{\"end\":42584,\"start\":42575},{\"end\":42593,\"start\":42584},{\"end\":42922,\"start\":42912},{\"end\":42931,\"start\":42922},{\"end\":42943,\"start\":42931},{\"end\":42956,\"start\":42943},{\"end\":43201,\"start\":43190},{\"end\":43216,\"start\":43201},{\"end\":43230,\"start\":43216},{\"end\":43242,\"start\":43230},{\"end\":43257,\"start\":43242},{\"end\":43267,\"start\":43257},{\"end\":43278,\"start\":43267},{\"end\":43610,\"start\":43584},{\"end\":43627,\"start\":43610},{\"end\":43640,\"start\":43627},{\"end\":44061,\"start\":44049},{\"end\":44070,\"start\":44061},{\"end\":44200,\"start\":44186},{\"end\":44215,\"start\":44200},{\"end\":44417,\"start\":44409},{\"end\":44425,\"start\":44417},{\"end\":44435,\"start\":44425},{\"end\":44588,\"start\":44576},{\"end\":44595,\"start\":44588},{\"end\":44604,\"start\":44595},{\"end\":44616,\"start\":44604},{\"end\":44626,\"start\":44616},{\"end\":44634,\"start\":44626},{\"end\":44648,\"start\":44634},{\"end\":44998,\"start\":44986},{\"end\":45010,\"start\":44998},{\"end\":45020,\"start\":45010},{\"end\":45029,\"start\":45020},{\"end\":45043,\"start\":45029},{\"end\":45054,\"start\":45043},{\"end\":45264,\"start\":45258},{\"end\":45278,\"start\":45264},{\"end\":45288,\"start\":45278},{\"end\":45416,\"start\":45404},{\"end\":45427,\"start\":45416},{\"end\":45440,\"start\":45427},{\"end\":45450,\"start\":45440},{\"end\":45656,\"start\":45646},{\"end\":45666,\"start\":45656},{\"end\":45675,\"start\":45666},{\"end\":45900,\"start\":45890},{\"end\":45910,\"start\":45900},{\"end\":45921,\"start\":45910},{\"end\":45931,\"start\":45921},{\"end\":45940,\"start\":45931},{\"end\":46171,\"start\":46164},{\"end\":46179,\"start\":46171},{\"end\":46187,\"start\":46179},{\"end\":46422,\"start\":46414},{\"end\":46433,\"start\":46422},{\"end\":46657,\"start\":46645},{\"end\":46671,\"start\":46657},{\"end\":46688,\"start\":46671},{\"end\":46899,\"start\":46886},{\"end\":46909,\"start\":46899},{\"end\":46919,\"start\":46909},{\"end\":47226,\"start\":47215},{\"end\":47239,\"start\":47226},{\"end\":47251,\"start\":47239},{\"end\":47263,\"start\":47251},{\"end\":47274,\"start\":47263},{\"end\":47281,\"start\":47274},{\"end\":47293,\"start\":47281},{\"end\":47305,\"start\":47293},{\"end\":47323,\"start\":47305},{\"end\":47338,\"start\":47323},{\"end\":47685,\"start\":47677},{\"end\":47699,\"start\":47685},{\"end\":47719,\"start\":47699},{\"end\":47730,\"start\":47719},{\"end\":47739,\"start\":47730},{\"end\":47751,\"start\":47739},{\"end\":48019,\"start\":48013},{\"end\":48039,\"start\":48019},{\"end\":48049,\"start\":48039},{\"end\":48282,\"start\":48269},{\"end\":48293,\"start\":48282},{\"end\":48304,\"start\":48293},{\"end\":48543,\"start\":48537},{\"end\":48552,\"start\":48543},{\"end\":48562,\"start\":48552},{\"end\":48573,\"start\":48562},{\"end\":48584,\"start\":48573},{\"end\":48593,\"start\":48584},{\"end\":48606,\"start\":48593},{\"end\":48620,\"start\":48606},{\"end\":48627,\"start\":48620},{\"end\":48636,\"start\":48627},{\"end\":48646,\"start\":48636},{\"end\":48654,\"start\":48646},{\"end\":48660,\"start\":48654},{\"end\":48669,\"start\":48660},{\"end\":48677,\"start\":48669},{\"end\":48689,\"start\":48677},{\"end\":48703,\"start\":48689},{\"end\":48714,\"start\":48703},{\"end\":48722,\"start\":48714},{\"end\":48734,\"start\":48722},{\"end\":48742,\"start\":48734},{\"end\":48750,\"start\":48742},{\"end\":48767,\"start\":48750},{\"end\":48776,\"start\":48767},{\"end\":49385,\"start\":49379},{\"end\":49398,\"start\":49385},{\"end\":49405,\"start\":49398},{\"end\":49415,\"start\":49405},{\"end\":49428,\"start\":49415},{\"end\":49446,\"start\":49428},{\"end\":49454,\"start\":49446},{\"end\":49466,\"start\":49454},{\"end\":49476,\"start\":49466},{\"end\":49487,\"start\":49476},{\"end\":49858,\"start\":49850},{\"end\":49866,\"start\":49858},{\"end\":49876,\"start\":49866},{\"end\":49886,\"start\":49876},{\"end\":49896,\"start\":49886},{\"end\":49909,\"start\":49896},{\"end\":49919,\"start\":49909},{\"end\":50233,\"start\":50219},{\"end\":50244,\"start\":50233},{\"end\":50407,\"start\":50398},{\"end\":50595,\"start\":50582},{\"end\":50605,\"start\":50595},{\"end\":50908,\"start\":50896},{\"end\":50916,\"start\":50908},{\"end\":50929,\"start\":50916},{\"end\":50940,\"start\":50929},{\"end\":51191,\"start\":51179},{\"end\":51203,\"start\":51191},{\"end\":51214,\"start\":51203},{\"end\":51222,\"start\":51214},{\"end\":51235,\"start\":51222},{\"end\":51245,\"start\":51235},{\"end\":51256,\"start\":51245},{\"end\":51553,\"start\":51538},{\"end\":51844,\"start\":51834},{\"end\":51858,\"start\":51844},{\"end\":52183,\"start\":52172},{\"end\":52193,\"start\":52183},{\"end\":52203,\"start\":52193},{\"end\":52428,\"start\":52413},{\"end\":52438,\"start\":52428},{\"end\":52447,\"start\":52438},{\"end\":52455,\"start\":52447},{\"end\":52708,\"start\":52696},{\"end\":52721,\"start\":52708},{\"end\":52923,\"start\":52911},{\"end\":52936,\"start\":52923},{\"end\":53163,\"start\":53153},{\"end\":53174,\"start\":53163},{\"end\":53182,\"start\":53174},{\"end\":53409,\"start\":53395},{\"end\":53421,\"start\":53409},{\"end\":53438,\"start\":53421},{\"end\":53668,\"start\":53660},{\"end\":53679,\"start\":53668},{\"end\":53689,\"start\":53679},{\"end\":53881,\"start\":53873},{\"end\":53895,\"start\":53881},{\"end\":53903,\"start\":53895},{\"end\":54121,\"start\":54109},{\"end\":54133,\"start\":54121},{\"end\":54371,\"start\":54363},{\"end\":54379,\"start\":54371},{\"end\":54388,\"start\":54379},{\"end\":54397,\"start\":54388},{\"end\":54404,\"start\":54397},{\"end\":54410,\"start\":54404},{\"end\":54417,\"start\":54410},{\"end\":54431,\"start\":54417},{\"end\":54446,\"start\":54431},{\"end\":54721,\"start\":54713},{\"end\":54732,\"start\":54721},{\"end\":54742,\"start\":54732},{\"end\":54755,\"start\":54742},{\"end\":54765,\"start\":54755},{\"end\":54999,\"start\":54989},{\"end\":55006,\"start\":54999},{\"end\":55017,\"start\":55006},{\"end\":55025,\"start\":55017},{\"end\":55214,\"start\":55206},{\"end\":55224,\"start\":55214},{\"end\":55234,\"start\":55224},{\"end\":55244,\"start\":55234},{\"end\":55408,\"start\":55400},{\"end\":55418,\"start\":55408},{\"end\":55729,\"start\":55721},{\"end\":55736,\"start\":55729},{\"end\":55744,\"start\":55736},{\"end\":55751,\"start\":55744},{\"end\":55761,\"start\":55751},{\"end\":56048,\"start\":56036},{\"end\":56059,\"start\":56048},{\"end\":56068,\"start\":56059},{\"end\":56333,\"start\":56327},{\"end\":56558,\"start\":56548},{\"end\":56566,\"start\":56558},{\"end\":56575,\"start\":56566},{\"end\":56816,\"start\":56809},{\"end\":56826,\"start\":56816},{\"end\":56833,\"start\":56826},{\"end\":56843,\"start\":56833},{\"end\":56850,\"start\":56843},{\"end\":56864,\"start\":56850},{\"end\":56877,\"start\":56864},{\"end\":57192,\"start\":57185},{\"end\":57203,\"start\":57192},{\"end\":57214,\"start\":57203},{\"end\":57228,\"start\":57214},{\"end\":57245,\"start\":57228}]", "bib_venue": "[{\"end\":42329,\"start\":42260},{\"end\":42661,\"start\":42593},{\"end\":42960,\"start\":42956},{\"end\":43188,\"start\":43107},{\"end\":43717,\"start\":43640},{\"end\":44047,\"start\":44027},{\"end\":44233,\"start\":44215},{\"end\":44407,\"start\":44340},{\"end\":44715,\"start\":44648},{\"end\":45058,\"start\":45054},{\"end\":45292,\"start\":45288},{\"end\":45454,\"start\":45450},{\"end\":45679,\"start\":45675},{\"end\":45944,\"start\":45940},{\"end\":46191,\"start\":46187},{\"end\":46437,\"start\":46433},{\"end\":46643,\"start\":46569},{\"end\":46975,\"start\":46919},{\"end\":47213,\"start\":47105},{\"end\":47675,\"start\":47614},{\"end\":48053,\"start\":48049},{\"end\":48308,\"start\":48304},{\"end\":48834,\"start\":48776},{\"end\":49377,\"start\":49253},{\"end\":49946,\"start\":49919},{\"end\":50248,\"start\":50244},{\"end\":50410,\"start\":50407},{\"end\":50645,\"start\":50605},{\"end\":50944,\"start\":50940},{\"end\":51177,\"start\":51084},{\"end\":51593,\"start\":51553},{\"end\":51913,\"start\":51858},{\"end\":52207,\"start\":52203},{\"end\":52459,\"start\":52455},{\"end\":52725,\"start\":52721},{\"end\":52909,\"start\":52843},{\"end\":53151,\"start\":53081},{\"end\":53393,\"start\":53335},{\"end\":53693,\"start\":53689},{\"end\":53907,\"start\":53903},{\"end\":54107,\"start\":54022},{\"end\":54361,\"start\":54258},{\"end\":54711,\"start\":54671},{\"end\":55027,\"start\":55025},{\"end\":55248,\"start\":55244},{\"end\":55466,\"start\":55418},{\"end\":55719,\"start\":55636},{\"end\":56075,\"start\":56068},{\"end\":56325,\"start\":56249},{\"end\":56579,\"start\":56575},{\"end\":56807,\"start\":56709},{\"end\":57183,\"start\":57102},{\"end\":43781,\"start\":43719},{\"end\":49960,\"start\":49948},{\"end\":55485,\"start\":55468}]"}}}, "year": 2023, "month": 12, "day": 17}