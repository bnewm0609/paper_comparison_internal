{"id": 218674010, "updated": "2023-11-11 03:31:44.39", "metadata": {"title": "Direction-Aware Spatial Context Features for Shadow Detection", "authors": "[{\"first\":\"Xiaowei\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Chi-Wing\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Jing\",\"last\":\"Qin\",\"middle\":[]},{\"first\":\"Pheng-Ann\",\"last\":\"Heng\",\"middle\":[]}]", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "journal": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Shadow detection is a fundamental and challenging task, since it requires an understanding of global image semantics and there are various backgrounds around shadows. This paper presents a novel network for shadow detection by analyzing image context in a direction-aware manner. To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN. By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting shadows. This design is developed into the DSC module and embedded in a CNN to learn DSC features at different levels. Moreover, a weighted cross entropy loss is designed to make the training more effective. We employ two common shadow detection benchmark datasets and perform various experiments to evaluate our network. Experimental results show that our network outperforms state-of-the-art methods and achieves 97% accuracy and 38% reduction on balance error rate.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3102699694", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Hu0F0H18", "doi": "10.1109/cvpr.2018.00778"}}, "content": {"source": {"pdf_hash": "d73a7ee2e2ded527c2319f0865def613fb0b1b14", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1712.04142v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1712.04142", "status": "GREEN"}}, "grobid": {"id": "8e91cc82288d5c08e08591d80dc636d0ecbb3054", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/d73a7ee2e2ded527c2319f0865def613fb0b1b14.txt", "contents": "\nDirection-aware Spatial Context Features for Shadow Detection\n16 May 2018\n\nXiaowei Hu \nDepartment of Computer Science and Engineering\nThe Chinese University of Hong Kong\n\n\nLei Zhu \nCentre for Smart Health\nSchool of Nursing\nThe Hong Kong Polytechnic University\n\n\nChi-Wing Fu \nDepartment of Computer Science and Engineering\nThe Chinese University of Hong Kong\n\n\nShenzhen Institutes of Advanced Technology\nGuangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology\nChinese Academy of Sciences\nChina\n\nJing Qin \nCentre for Smart Health\nSchool of Nursing\nThe Hong Kong Polytechnic University\n\n\nPheng-Ann Heng \nDepartment of Computer Science and Engineering\nThe Chinese University of Hong Kong\n\n\nShenzhen Institutes of Advanced Technology\nGuangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology\nChinese Academy of Sciences\nChina\n\nDirection-aware Spatial Context Features for Shadow Detection\n16 May 2018F8D16CAA316DD01C544B860631D1B4F8arXiv:1712.04142v2[cs.CV]\nShadow detection is a fundamental and challenging task, since it requires an understanding of global image semantics and there are various backgrounds around shadows.This paper presents a novel network for shadow detection by analyzing image context in a direction-aware manner.To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN.By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting shadows.This design is developed into the DSC module and embedded in a CNN to learn DSC features at different levels.Moreover, a weighted cross entropy loss is designed to make the training more effective.We employ two common shadow detection benchmark datasets and perform various experiments to evaluate our network.Experimental results show that our network outperforms state-of-the-art methods and achieves 97% accuracy and 38% reduction on balance error rate.\n\nIntroduction\n\nShadow is a monocular cue in human vision for depth and geometry perception.Knowing where the shadow is, on the one hand, allows us to acquire the lighting direction [14] and scene geometry [19,10], as well as the camera location and parameters [9].However, the presence of shadow, on the other hand, could deteriorate the performance of many fundamental computer vision tasks, such as object detection and tracking [2,17].Hence, shadow detection has long been a fundamental problem in computer vision.\n\nExisting methods detect shadows by developing physical models of color and illumination [4,3], or by using datadriven approaches based on hand-crafted features [7,15,30] or learned features [11,24,18].While state-of-the-art meth- ods have already achieved accuracy of 87% to 90% on two benchmark datasets [24,30], they may misrecognize black objects as shadows and miss unobvious shadows.These situations are revealed by the balance error rate (BER), which equally considers shadow and non-shadow regions; see Section 4 for quantitative comparison results.\n\nShadow detection requires an understanding of global image semantics, as shown very recently in [18].To improve the understanding, we propose to analyze the image (or spatial) context in a direction-aware manner.Taking region A in Figure 1 as an example, comparing it with regions B and C, region B would give a stronger indication that A is a shadow as compared to region C. Hence, spatial contexts along different directions would give different amount of contributions in suggesting the presence of shadows.\n\nTo take directional variance into account when reasoning the spatial contexts, we first design a network module called the direction-aware spatial context (DSC) module, or DSC module for short, by adopting a spatial recurrent neural network (RNN) to aggregate spatial contexts in four principal directions, and formulating the direction-aware attention mechanism in the RNN to learn attention weights for each direction.Then, we embed multiple copies of this DSC module in a convolutional neural network to learn DSC features in different layers (scales).After that, we combine the DSC features with convolutional features to predict a score map for each layer, and fuse the score maps into the final shadow detection map.The whole network is trained in an end-to-end manner with a weighted cross entropy loss.We summarize the major contributions of this work below:\n\n\u2022 First, we design a novel attention mechanism in a spatial RNN and construct the DSC module to learn spatial contexts in a direction-aware manner.\n\n\u2022 Second, we present a new shadow detection network that adopts multiple DSC modules to learn directionaware spatial contexts in different layers.A weighted cross entropy loss is designed to balance the detection accuracy in shadow and non-shadow regions.This network has potential for use in other applications such as saliency detection and semantic segmentation.\n\n\u2022 Third, we evaluate our network on two benchmark sets and compare it with several state-of-the-art methods on shadow detection, saliency detection, and semantic image segmentation.Results show that our network outperforms previous methods with over 97% accuracy and 38% reduction on the balance error rate.\n\n\nRelated Work\n\nIn this section, we focus on discussing works on singleimage shadow detection rather than trying to be exhaustive.\n\nTraditionally, single-image shadow detection is done by exploiting physical models of illumination and color [4,3,23].This approach, however, tends to produce satisfactory results only for wide dynamic range images [15,18].Another approach learns shadow properties using handcrafted features based on annotated shadow images.It first describes image regions by feature descriptors and then classifies the regions into shadow and non-shadow regions.Features like color [15,5,25], texture [30,5,25], edge [15,30,7] and T-junction [15] are commonly used for shadow detection followed by classifiers like decision tree [15,30] and SVM [5,7,25].However, since handcrafted features have limited capability in describing shadows, this approach often fails for complex cases.\n\nConvolutional neural network (CNN) is recently demonstrated to be a very powerful tool to learn features for detecting shadows, with results clearly outperforming previous approaches.Khan et al. [11] used multiple CNNs to learn features in super pixels and along object boundaries, and fed the output features to a conditional random field to locate shadows.Shen et al. [20] presented a deep structured shadow edge detector and employed structured labels to improve the local consistency of the predicted shadow map.Vicente et al. [24] trained stacked-CNN using a large data set with noisy annotations.They minimized the sum of squared leave-one-out errors for image clusters to recover the annotations, and trained two CNNs to detect shadows.\n\nVery recently, Hosseinzadeh et al. [6] detected shadows using a patch-level CNN and a shadow prior map generated from hand-crafted features, while Nguyen et al. [18] developed scGAN with a sensitivity parameter to adjust weights in the loss functions.Although the shadow detection accuracy keeps improving on the benchmark datasets [30,24], existing methods may still misrecognize black objects as shadows and miss unobvious shadows in the testing images.The most recent work by Nguyen et al. [18] emphasized the importance of reasoning global semantics for shadow detection.Compared to this work, we suggest to consider the directional variance when analyzing the spatial context.Results show that our method can further outperform [18] in terms of both the accuracy and the BER value.4) to learn direction-aware spatial context features.Our network takes the whole image as input and outputs the shadow detection map in an end-to-end manner.First, it begins by using a convolutional neural network (CNN) to extract a set of hierarchical feature maps, which encode fine details and semantic information in different scales over the CNN layers.Second, for each layer, we employ a DSC module to harvest spatial contexts in a direction-aware manner and produce DSC features.Third, we concatenate the DSC features with corresponding convolutional features, and upsample the concatenated feature map to the size of the input image.Moreover, we further combine the upsampled feature maps into the multi-level integrated features (MLIF) with a convolution layer (via a 1 \u00d7 1 kernel), and apply the deep supervision mechanism [27] to impose the supervision signals to each layer as well as to the MLIF and predict a score map at each layer.Lastly, we fuse all the predicted score maps into the final shadow map output.\n\n\nMethodology\n\nIn the following subsections, we first elaborate how the DSC module generates DSC features, and then introduce the training and testing strategies in the shadow detection network.\n\n\nDirection-aware Spatial Context\n\nFigure 4 shows our DSC module architecture, which takes feature maps as input and outputs DSC features.In this subsection, we first describe the concept of spatial context features and the spatial RNN model (Section 3.1.1),and then elaborate how we formulate the direction-aware attention mechanism in a spatial RNN to learn attention weights and generate DSC features (Section 3.1.2).\n\n\nSpatial Context Features\n\nRecurrent neural network (RNN) [16] is an effective model to process 1D sequential data via an array of input nodes (to receive data), an array of hidden nodes (to update internal states based on past and present data), and an array of output nodes (to output data).There are three kinds of data translations in an RNN: from input nodes to hidden nodes, from hidden nodes to output nodes, and between adjacent hidden nodes.By iteratively performing the data translations, the data received at input nodes can propagate across the hidden nodes, and eventually produce results at the output nodes.\n\nFor processing image data with 2D spatial context, RNN has been extended to build the spatial RNN model [1]; see the schematic illustration in Figure 3. Taking a 2D feature map from a CNN as input, the spatial RNN model first uses a 1\u00d71 convolution to perform an input-to-hidden data trans-lation.Then, it applies four independent data translations to aggregate local spatial context along each principal direction (left, right, up, and down), and fuses the results into an intermediate feature map; see Figure 3(b).Lastly, the whole process is repeated to further propagate the aggregated spatial context in each principal direction, and then to generate the overall spatial context; see Figure 3(c).\n\nComparing with Figure 3(c), each pixel in Figure 3(a) knows only its local spatial context, while each pixel in Figure 3(b) further knows the spatial context along the four principal directions after the first round of data translations.Hence, by having two rounds of data translations, each pixel can obtain necessary global spatial context for learning features and solving the problem that the network is intended for.\n\nTo perform data translations in a spatial RNN, we follow the IRNN model [1], since it is fast, easy to train, and has a good performance for long-range data dependencies [1].Denote h i,j as the feature at pixel (i, j), we perform one round of data translations to the right (similarly for the other directions) by repeating the following operation n times:\nh i,j = max( \u03b1 right h i,j\u22121 + h i,j , 0 ) , (1)\nwhere n is the width of the feature map and \u03b1 right is the weight parameter in the recurrent translation layer for the right direction.Note that \u03b1 right , as well as weights for the other directions, are initialized to be an identity matrix and are learned by the training process automatically.We compute directionaware spatial context by adopting a spatial RNN to aggregate spatial contexts in four principal directions with two rounds of recurrent translations, and formulate the attention mechanism to generate maps of attention weights to combine context features for different directions.We use the same set of weights in both rounds of recurrent translations.Best viewed in color.\n\n\nDirection-aware Spatial Context Features\n\nTo learn spatial context in a direction-aware manner, we formulate the direction-aware attention mechanism in a spatial RNN to learn attention weights and generate directionaware spatial context (DSC) features.\n\nDirection-aware attention mechanism.The purpose of the direction-aware attention mechanism is to enable the spatial RNN to selectively leverage the spatial context aggregated along different directions by means of learning.See the top-left blocks in the DSC module shown in Figure 4. First, we employ two successive convolutional layers (with 3\u00d73 kernels) followed by the ReLU [13] non-linear operation, and then the third convolutional layer (with 1\u00d71 kernels) to generate W.Then, we split W into four maps of attention weights denoted as W left , W down , W right , and W up .Mathematically, if we denote the above operators as f att and the input feature maps as X, we have\nW = f att ( X ; \u03b8 ) ,(2)\nwhere \u03b8 denotes the parameters to be learned by f att , and f att is also known as the attention estimator network.See again the DSC module shown in Figure 4.The four maps of weights are multiplied with the spatial context features (from the recurrent data translations) along different directions in an element-wise manner.Therefore, after we train the network with the shadow dataset, the network can learn \u03b8 for producing suitable attention weights to selectively leverage the spatial context in the spatial RNN.\n\nCompleting the DSC module.Next, we further provide additional details about the DSC module.As shown in Figure 4, after we multiply the spatial context features with the attention weights, we concatenate the results and use a 1\u00d71 convolution to simulate a hidden-to-hidden data translation and reduce the feature dimensions to a quarter of the dimension size.Then, we perform the second round of recurrent translations and use the same set of attention weights to select spatial context.We empricially find that the network delivers higher performance, if we share the attention weights rather than using two separate sets of weights.Note that these attention weights are learnt based on the deep features extracted from the input images, and they may vary from images to images.Lastly, we utilize a 1 \u00d7 1 convolution followed by the ReLU [13] non-linear operation on the concatenated feature maps to simulate the hidden-to-output translation and produce the output DSC features.\n\n\nTraining and Testing Strategies\n\nOur network is built upon the VGG network [22], where we apply a DSC module to each layer, except for the first layer, which involves a large memory footprint.\n\nLoss Function.In natural images, shadows usually occupy smaller areas than non-shadow regions.Hence, if the loss function simply aims for overall accuracy, it will incline to match the non-shadow regions, which have far more pixels.Therefore, we use a weighted cross-entropy loss to optimize the whole network in the training process.\n\nIn detail, assume that the ground truth value of a pixel is y (where y=1, if it is in shadow, and y=0, otherwise) and the prediction label of the pixel is p (where p \u2208 [0, 1]).The weighted cross entropy loss L equals L 1 + L 2 :\nL 1 = \u2212( N n N p + N n )y log(p)\u2212( N p N p + N n )(1\u2212y) log(1\u2212p) ,(3) andL 2 = \u2212(1 \u2212 T P N p )y log(p) \u2212 (1 \u2212 T N N n )(1 \u2212 y) log(1 \u2212 p) ,(4)\nwhere T P and T N are the number of true positives and true negatives, and N p and N n are the number of shadow and non-shadow pixels, respectively, so N p +N n is the total number of pixels.In practice, L 1 helps balance the detection of shadows non-shadows; if the area of shadows is less than that of the non-shadow region, we will penalize misclassified shadow pixels more than misclassified nonshadow pixels.On the other hand, L 2 helps the network focus on learning the class (shadow or non-shadow) that is difficult to be classified [21].This can be achieved, since the weight in loss function for shadow (or non-shadow) class is large when the number of correctly-classified shadow (or non-shadow) pixels is small, and vice versa.\n\nWe use the above loss function for each layer the shadow network presented in 2. Hence, the overall loss function L overall is a summation of the individual loss on all the predicted score maps:\nL overall = i w i L i + w m L m + w f L f ,(5)\nwhere w i and L i denote the weight and loss of the i-th layer (level) in the overall network, respectively; w m and L m are the weight and loss of the MLIF layer; and w f and L f are the weight and loss of the fusion layer, which is the last layer in the overall network to produce the final shadow detection result; see Figure 2. Note that all the weights w i , w m and w f are empirically set to be 1.\n\nTraining parameters.To accelerate the training process while reducing over-fitting, we initialize parameters in the feature extraction layers (see the frontal part of the network in Figure 2) by the well-trained VGG network [22] and the parameters in other layers by random noise.Stochastic gradient descent is used to optimize the whole network with a momentum value of 0.9 and a weight decay of 5\u00d710 \u22124 .We set the learning rate as 10 \u22128 and terminate the learning process after 12k iterations.Moreover, we horizontally flip images for data argumentation.Note that we build the model on Caffe [8] with a mini-batch size of 1.\n\nInference.In the testing process, our network produces one score map for each layer, including the MLIF layer and the fusion layer, with a supervision signal added to each layer.After that, we compute the mean of the score maps over the MLIF layer and the fusion layer to produce the final prediction map.Lastly, we apply the fully connected conditional field [12] to improve the detection result by considering the spatial coherence between neighborhood pixels.Evaluation metrics.We employ two commonly-used metrics to quantitatively evaluate the shadow detection performance.The first one is the accuracy metric:\n\n\nExperimental Results\naccuracy = T P + T N N p + N n ,(6)\nwhere T P , T N , N p and N n are true positives, true negatives, number of shadow pixels, and number of non-shadow pixels, respectively, as defined in Section 3.2.Since N p is usually much smaller than N n in natural images, we employ the second metric called the balance error rate (BER) to obtain a more balanced evaluation by equally considering the shadow and non-shadow regions:\nBER = (1 \u2212 1 2 ( T P N p + T N N n )) \u00d7 100 .(7)\nNote that unlike the accuracy metric, for BER, the lower its value, the better the detection result is.\n\n\nComparison with the State-of-the-art Shadow Detection Methods\n\nWe compare our method with four recent shadow detection methods: scGAN [18], stacked-CNN [24], patched-CNN [6] and Unary-Pairwise [5].Among them, the first three are deep-learning-based methods, while the last one is based on hand-crafted features.For a fair comparison, the shadow detection results of other methods are obtained either directly from results provided by the authors, or by generating them using implementations provided by the authors with recommended parameter setting.input image ground truth DSC (ours) scGAN [18] stkd'-CNN [24] patd'-CNN [6] SRM [26] Amulet [28] PSPNet [29] Figure 5: Visual comparison of shadow maps produced by our method and other methods (4th-9th columns) against ground truths shown in 2nd column.Note that stkd'-CNN and patd'-CNN stand for stacked-CNN and patched-CNN, respectively.\n\nTable 1: Comparing our method (DSC) with state-of-thearts methods for shadow detection (scGAN [18], stacked-CNN [24], patched-CNN [6] and Unary-Pairwise [5]), for saliency detection (SRM [26] and Amulet [28]), and for semantic segmentation (PSPNet [29]).\n\n[24] UCF [30] method accuracy BER accuracy BER DSC (ours) 0.97 5.59 0.95 8.10 scGAN [18] 0.90 9.10 0.87 11.50 stacked-CNN [24] 0.88 11.00 0.85 13.00 patched-CNN [6] 0.88 11.56 --Unary-Pairwise [5] 0.86 25.03 --SRM [26] 0.96 7.25 0.94 9.81 Amulet [28] 0.93 15.13 0.92 15.17 PSPNet [29] 0.95 8.57 0.93 11.75\n\nTable 1 reports the comparison results, where we can see that our method outperforms the others in terms of both accuracy and BER for both benchmark datasets.Note that our shadow detection network is trained using the SBU training set [24], but it still outperforms the others also for the UCF dataset, thus demonstrating its generalization ability.\n\nWe further provide visual comparison results in Figures 5 and 6, which show various challenging cases, e.g., a light shadow next to a dark shadow, shadows around complex backgrounds, and black objects around shadows.Without understanding the global image semantics, it is hard to locate these shadows, and non-shadow regions would be easily misrecognized as shadows.From the results, we can see that our method can effectively locate shadows and avoid false positives as compared to the others; for black objects misrecognized as shadows by other methods, our method could still recognize them as non-shadows.\n\n\nComparison with Saliency Detection and Semantic Segmentation Methods\n\nIn general, deep networks designed for saliency detection and semantic image segmentation may also be used for shadow detection by training the networks using datasets of annotated shadows.Hence, we conduct another experiment by using two recent deep models for saliency detection, i.e., SRM [26] and Amulet [28], and a recent deep model for semantic image segmentation, i.e., PSPNet [29].\n\nFor a fair comparison, we re-train their models on the SBU training set using implementations provided by the authors, and adjust the training parameters to obtain the best shadow detection results.The last three rows in Table 1 report the comparison results in terms of the accuracy and BER metrics.Although these methods achieve good re-input image ground truth DSC (ours) scGAN [18] stkd'-CNN [24] patd'-CNN [6] SRM [26] Amulet [28] PSPNet [29] Figure 6: More visual comparison results (continue from Figure 5).\n\nTable 2: Component analysis.We train three networks using the SBU training set and test them using the SBU testing set [24]: \"basic\" denotes the architecture shown in Figure 4 but without all DSC modules; \"basic+context\" denotes the \"basic\" network with spatial context but not direction-aware spatial context; and \"DSC\" is the overall network in sults for both metrics, our method still outperforms them for both benchmark datasets.Please also refer to the last three columns in Figures 5 and 6 for visual comparison results.\n\n\nEvaluation on the DCS Module\n\nComponent analysis.We perform an experiment to evaluate the effectiveness of the DSC module design.Here, we use the SBU dataset and consider two baseline networks.\n\nThe first baseline (denoted as \"basic\") is a network con- structed by removing all the DSC modules from the overall network shown in Figure 2. The second baseline (denoted as \"basic+context\") considers spatial context but ignores the direction-aware attention weights.Compared with the first baseline, this network has all the DSC modules, but it removes the direction-aware attention mechanism inside the DSC modules, i.e., removing the computation of W and directly concatenating the context features without multiplying them with the attention weights; see Figure 4.This is equivalent to setting all the attention weights W to be one.Table 2 reports the comparison results, showing that our basic network with multi-scale features and the weighed cross entropy loss function can produce good results.Moreover, by considering spatial context and DSC features can lead to further obvious improvement.See also Figure 7 for visual comparison results.\n\nDSC architecture analysis.When we design the network structure in the DSC module, we encounter two questions: (i) how many rounds of recurrent translations we should employ in the spatial RNN; and (ii) whether to share the attention weights, or to use separate attention weights in different rounds of recurrent translations in the spatial RNN.We modify our network for these two parameters and produce the quantitative comparison results shown in Table 3. From the results, we can see that having two rounds of recurrent translations and sharing the attention weights in both rounds produce the best detection result.We believe that when there is only one round of recurrent translations, the global context information cannot well propagate over the spatial domain, so there is insufficient information exchange for learning the shadows.On the other hand, when we have three rounds of recurrent translations or separate copies of attention weights, we will end up having too many parameters in the network, making it hard to be trained.Our method can still detect these shadows fairly well, but it fails in some extremely complex scenes: (a) a scene with many small shadows (see 1 st row in Figure 9), where the features in deep layers lose the detail information and features in shallow layers lack the semantic information for the shadow context; (b) a scene with a large black region (see 2 nd row in Figure 9), where there are insufficient surrounding context to indicate whether it is a shadow or simply a black object; and (c) a scene with soft shadows (see 3 rd row in Figure 9), where the difference between the soft shadow regions and the non-shadow regions is small.The code, trained model, and more shadow detection results on the datasets are publicly available at https://xw-hu.github.io/.\n\n\nMore Shadow Detection Results\n\n\nConclusion\n\nThis paper presents a novel network for single-image shadow detection by harvesting direction-aware spatial context.Our key idea is to analyze multi-level spatial context in a direction-aware manner by formulating a direction-aware attention mechanism in a spatial RNN.In our mechanism, the network can automatically learn the attention weights for leveraging and composing the spatial context in different directions in the spatial RNN.In this way, we can produce direction-aware spatial context (DSC) features and formulate the DSC module for the task.Further, we adopt multiple DSC modules in a multi-layer convolutional neural network to predict score maps in different scales, and design a weighted cross entropy loss function to make effective the training process.In the end, we test our network on two benchmark datasets, compare it with various state-of-theart methods, and show the superiority of our network over the others in terms of the accuracy and BER metrics.\n\nIn future, we plan to explore the potential of our network for other applications such as saliency detection and semantic segmentation, and further enhance its capability for detecting time-varying shadows in videos.\n\nFigure 1 :\n1\nFigure 1: In this example image, region B would give a stronger indication that A is a shadow compared to region C.This motivates us to analyze the global image context in a direction-aware manner for shadow detection.\n\n\nFigure 2\n2\nFigure2presents the workflow of the overall shadow detection network that employs the DSC module (see to learn direction-aware spatial context features.Our network takes the whole image as input and outputs the shadow detection map in an end-to-end manner.First, it begins by using a convolutional neural network (CNN) to extract a set of hierarchical feature maps, which encode fine details and semantic information in different scales over the CNN layers.Second, for each layer, we employ a DSC module to harvest spatial contexts in a direction-aware manner and produce DSC features.Third, we concatenate the DSC features with corresponding convolutional features, and upsample the concatenated feature map to the size of the input image.Moreover, we further combine the upsampled feature maps into the multi-level integrated features (MLIF) with a convolution layer (via a 1 \u00d7 1 kernel), and apply the deep supervision mechanism[27] to impose the supervision signals to each layer as well as to the MLIF and predict a score map at each layer.Lastly, we fuse all the predicted score maps into the final shadow map output.In the following subsections, we first elaborate how the DSC module generates DSC features, and then introduce the training and testing strategies in the shadow detection network.\n\n\nFigure 2 :Figure 3 :\n23\nFigure2: The schematic illustration of the overall shadow detection network: (i) we extract features in different scales over the CNN layers from the input image; (ii) we embed a DSC module (see Figure4) to generate direction-aware spatial context (DSC) features for each layer; (iii) we concatenate the DSC features with convolutional features at each layer and upsample the concatenated feature maps to the size of the input image; (iv) we combine the upsampled feature maps into the multi-level integrated features (MLIF), and predict a score map based on the features for each layer by a deep supervision mechanism[27]; and (v) lastly, we fuse the resulting score maps to produce the final shadow detection result.\n\n\nFigure 4 :\n4\nFigure4: The schematic illustration of the direction-aware spatial context module (DSC module).We compute directionaware spatial context by adopting a spatial RNN to aggregate spatial contexts in four principal directions with two rounds of recurrent translations, and formulate the attention mechanism to generate maps of attention weights to combine context features for different directions.We use the same set of weights in both rounds of recurrent translations.Best viewed in color.\n\n\nFigure 7 :\n7\nFigure 7: Visual comparison results of component analysis.\n\n\nFigure 8 :Figure 9 :\n89\nFigure 8: More results produced from our method.\n\n\nFigure 8\n8\nFigure 8 shows more shadow detection results: (a) light and dark shadows locate next to each other; (b) small\n\n\n\n\n[30]benchmark datasets are employed in this work.The first one is the SBU Shadow Dataset[24], which is the largest publicly available annotated shadow dataset with 4089 training images and 638 testing images.It includes a wide variety of scenes, e.g., urban, beach, mountain, roads, parks, snow, animals, vehicles, and houses, and covers various types of pictures, e.g., aerial, landscape, close range, and selfies.The second benchmark dataset we employed is the UCF Shadow Dataset[30].It includes 145 training images and 76 testing images, and covers outdoor scenes with various backgrounds.We train our shadow detection network using the SBU training set.\n4.1. Datasets and Evaluation MetricsBenchmark datasets.\n\nTable 3 :\n3number of rounds shared W? BER1-5.852Yes5.593Yes5.852No6.02\nDSC architecture analysis.By varying the parameters in the DSC architecture (see 2nd and 3rd columns below), we can have produce a slightly different overall network and explore their performance (see last column).\n\nAcknowledgmentsThe work is supported by the National Basic Program of China, 973 Program (Project no.2015CB351706), the Research Grants Council of the Hong Kong Special Administrative Region (Project no.CUHK 14225616), the Shenzhen Science and Technology Program (No. JCYJ20170413162617606), the CUHK strategic recruitment fund, and the Innovation and Technology Fund of Hong Kong (Project no.ITS/304/16).We thank reviewers for their valuable comments, Michael S. Brown for his discussion, and Minh Hoai Nguyen for sharing their results.Xiaowei Hu is funded by the Hong Kong Ph.D. Fellowship.\nInsideoutside net: Detecting objects in context with skip pooling and recurrent neural networks. S Bell, C L Zitnick, K Bala, R Girshick, CVPR. 2016\n\nDetecting moving objects, ghosts, and shadows in video streams. R Cucchiara, C Grana, M Piccardi, A Prati, IEEE Transactions on Pattern Analysis and Machine Intelligence. 25102003\n\nEntropy minimization for shadow removal. G D Finlayson, M S Drew, C Lu, International Journal of Computer Vision. 8512009\n\nOn the removal of shadows from images. G D Finlayson, S D Hordley, C Lu, M S Drew, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2812006\n\nSingle-image shadow detection and removal using paired regions. R Guo, Q Dai, D Hoiem, CVPR. 2033-2040, 2011. 2, 5, 6\n\nFast shadow detection from a single image using a patched convolutional neural network. S Hosseinzadeh, M Shakeri, H Zhang, arXiv:1709.092832017. 2, 5, 6, 7arXiv preprint\n\nWhat characterizes a shadow boundary under the sun and sky. X Huang, G Hua, J Tumblin, L Williams, ICCV. 20111\n\nCaffe: Convolutional architecture for fast feature embedding. Y Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaACM2014\n\nEstimating geo-temporal location of stationary cameras using shadow trajectories. I N Junejo, H Foroosh, ECCV. 2008\n\nRendering synthetic objects into legacy photographs. K Karsch, V Hedau, D Forsyth, D Hoiem, ACM Trans. on Graphics (SIGGRAPH Asia). 3062011\n\nAutomatic feature learning for robust shadow detection. S H Khan, M Bennamoun, F Sohel, R Togneri, CVPR. 20141\n\nEfficient inference in fully connected CRFs with Gaussian edge potentials. P Kr\u00e4henb\u00fchl, V Koltun, NIPS. 2011\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. 2012\n\nEstimating natural illumination from a single outdoor image. J.-F Lalonde, A A Efros, S G Narasimhan, ICCV. 2009\n\nDetecting ground shadows in outdoor consumer photographs. J.-F Lalonde, A A Efros, S G Narasimhan, ECCV. 20101\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 52175532015\n\nPhysical models for moving shadow and object detection in video. S Nadimi, B Bhanu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2682004\n\nShadow detection with conditional generative adversarial networks. V Nguyen, T F Y Vicente, M Zhao, M Hoai, D Samaras, ICCV. 2017. 1, 2, 5, 6, 7\n\nAttached shadow coding: Estimating surface normals from shadows under unknown reflectance and lighting conditions. T Okabe, I Sato, Y Sato, ICCV. 2009\n\nShadow optimization from structured deep edge detection. L Shen, T Wee Chua, K Leman, CVPR. 2015\n\nTraining regionbased object detectors with online hard example mining. A Shrivastava, A Gupta, R Girshick, CVPR. 2016\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556201445arXiv preprint\n\nNew spectrum ratio properties and features for shadow detection. Pattern Recognition. J Tian, X Qi, L Qu, Y Tang, 201651\n\nLarge-scale training of shadow detectors with noisilyannotated shadow examples. T F Y Vicente, L Hou, C.-P Yu, M Hoai, D Samaras, ECCV. 2016. 1, 2, 5, 6, 7\n\nLeaveone-out kernel optimization for shadow detection. Y Vicente, F Tomas, M Hoai, D Samaras, ICCV. 2015\n\nA stagewise refinement model for detecting salient objects in images. T Wang, A Borji, L Zhang, P Zhang, H Lu, ICCV. 201767\n\nHolistically-nested edge detection. S Xie, Z Tu, ICCV. 201523\n\nAmulet: Aggregating multi-level convolutional features for salient object detection. P Zhang, D Wang, H Lu, H Wang, X Ruan, ICCV. 201767\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, CVPR. 201767\n\nLearning to recognize shadows in monochromatic natural images. J Zhu, K G Samuel, S Z Masood, M F Tappen, CVPR. 2010. 1, 2, 5, 6\n", "annotations": {"author": "[{\"end\":172,\"start\":76},{\"end\":262,\"start\":173},{\"end\":524,\"start\":263},{\"end\":615,\"start\":525},{\"end\":880,\"start\":616}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":84},{\"end\":180,\"start\":177},{\"end\":274,\"start\":272},{\"end\":533,\"start\":530},{\"end\":630,\"start\":626}]", "author_first_name": "[{\"end\":83,\"start\":76},{\"end\":176,\"start\":173},{\"end\":271,\"start\":263},{\"end\":529,\"start\":525},{\"end\":625,\"start\":616}]", "author_affiliation": "[{\"end\":171,\"start\":88},{\"end\":261,\"start\":182},{\"end\":359,\"start\":276},{\"end\":523,\"start\":361},{\"end\":614,\"start\":535},{\"end\":715,\"start\":632},{\"end\":879,\"start\":717}]", "title": "[{\"end\":62,\"start\":1},{\"end\":942,\"start\":881}]", "venue": null, "abstract": "[{\"end\":2071,\"start\":1012}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2257,\"start\":2253},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2281,\"start\":2277},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2284,\"start\":2281},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2335,\"start\":2332},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2506,\"start\":2503},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2509,\"start\":2506},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2682,\"start\":2679},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2684,\"start\":2682},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2754,\"start\":2751},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2757,\"start\":2754},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2760,\"start\":2757},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2785,\"start\":2781},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2788,\"start\":2785},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2791,\"start\":2788},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2900,\"start\":2896},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2903,\"start\":2900},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3249,\"start\":3245},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5597,\"start\":5594},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5599,\"start\":5597},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5602,\"start\":5599},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5704,\"start\":5700},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5707,\"start\":5704},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5957,\"start\":5953},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5959,\"start\":5957},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5962,\"start\":5959},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5976,\"start\":5972},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5978,\"start\":5976},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5981,\"start\":5978},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5992,\"start\":5988},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5995,\"start\":5992},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5997,\"start\":5995},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6017,\"start\":6013},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6104,\"start\":6100},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6107,\"start\":6104},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6119,\"start\":6116},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6121,\"start\":6119},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6124,\"start\":6121},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6453,\"start\":6449},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6628,\"start\":6624},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6789,\"start\":6785},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7037,\"start\":7034},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7164,\"start\":7160},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7335,\"start\":7331},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7338,\"start\":7335},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7496,\"start\":7492},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7736,\"start\":7732},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8622,\"start\":8618},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9490,\"start\":9486},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10159,\"start\":10156},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11253,\"start\":11250},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11351,\"start\":11348},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12909,\"start\":12905},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14589,\"start\":14585},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14807,\"start\":14803},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16174,\"start\":16170},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17246,\"start\":17242},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17616,\"start\":17613},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18011,\"start\":18007},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18999,\"start\":18995},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19017,\"start\":19013},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19034,\"start\":19031},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19057,\"start\":19054},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19457,\"start\":19453},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19472,\"start\":19468},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19486,\"start\":19483},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19495,\"start\":19491},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19507,\"start\":19503},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19519,\"start\":19515},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19850,\"start\":19846},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19868,\"start\":19864},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19885,\"start\":19882},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19908,\"start\":19905},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19943,\"start\":19939},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19959,\"start\":19955},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20004,\"start\":20000},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20096,\"start\":20092},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20134,\"start\":20130},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20172,\"start\":20169},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20204,\"start\":20201},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20226,\"start\":20222},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20258,\"start\":20254},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20292,\"start\":20288},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20554,\"start\":20550},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21644,\"start\":21640},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21660,\"start\":21656},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21736,\"start\":21732},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22124,\"start\":22120},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22139,\"start\":22135},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22153,\"start\":22150},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22162,\"start\":22158},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22174,\"start\":22170},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22186,\"start\":22182},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22378,\"start\":22374},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28157,\"start\":28153},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29173,\"start\":29169}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27209,\"start\":26976},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28525,\"start\":27210},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29270,\"start\":28526},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29773,\"start\":29271},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29847,\"start\":29774},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29922,\"start\":29848},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30045,\"start\":29923},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30762,\"start\":30046},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31050,\"start\":30763}]", "paragraph": "[{\"end\":2589,\"start\":2087},{\"end\":3147,\"start\":2591},{\"end\":3659,\"start\":3149},{\"end\":4527,\"start\":3661},{\"end\":4676,\"start\":4529},{\"end\":5043,\"start\":4678},{\"end\":5352,\"start\":5045},{\"end\":5483,\"start\":5369},{\"end\":6252,\"start\":5485},{\"end\":6997,\"start\":6254},{\"end\":8810,\"start\":6999},{\"end\":9005,\"start\":8826},{\"end\":9426,\"start\":9041},{\"end\":10050,\"start\":9455},{\"end\":10753,\"start\":10052},{\"end\":11176,\"start\":10755},{\"end\":11534,\"start\":11178},{\"end\":12271,\"start\":11584},{\"end\":12526,\"start\":12316},{\"end\":13204,\"start\":12528},{\"end\":13745,\"start\":13230},{\"end\":14725,\"start\":13747},{\"end\":14920,\"start\":14761},{\"end\":15256,\"start\":14922},{\"end\":15486,\"start\":15258},{\"end\":16368,\"start\":15630},{\"end\":16564,\"start\":16370},{\"end\":17016,\"start\":16612},{\"end\":17645,\"start\":17018},{\"end\":18261,\"start\":17647},{\"end\":18705,\"start\":18321},{\"end\":18858,\"start\":18755},{\"end\":19750,\"start\":18924},{\"end\":20006,\"start\":19752},{\"end\":20313,\"start\":20008},{\"end\":20664,\"start\":20315},{\"end\":21275,\"start\":20666},{\"end\":21737,\"start\":21348},{\"end\":22253,\"start\":21739},{\"end\":22781,\"start\":22255},{\"end\":22977,\"start\":22814},{\"end\":23928,\"start\":22979},{\"end\":25734,\"start\":23930},{\"end\":26757,\"start\":25781},{\"end\":26975,\"start\":26759},{\"end\":27208,\"start\":26990},{\"end\":28524,\"start\":27222},{\"end\":29269,\"start\":28551},{\"end\":29772,\"start\":29285},{\"end\":29846,\"start\":29788},{\"end\":29921,\"start\":29873},{\"end\":30044,\"start\":29935},{\"end\":30706,\"start\":30049},{\"end\":31049,\"start\":30835}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11582,\"start\":11535},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11583,\"start\":11582},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13229,\"start\":13205},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15560,\"start\":15487},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15629,\"start\":15560},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16611,\"start\":16565},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18320,\"start\":18285},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18754,\"start\":18706}]", "table_ref": "[{\"end\":19759,\"start\":19758},{\"end\":20322,\"start\":20321},{\"end\":21967,\"start\":21966},{\"end\":22262,\"start\":22261},{\"end\":23623,\"start\":23622}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2085,\"start\":2073},{\"attributes\":{\"n\":\"2.\"},\"end\":5367,\"start\":5355},{\"attributes\":{\"n\":\"3.\"},\"end\":8824,\"start\":8813},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9039,\"start\":9008},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":9453,\"start\":9429},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":12314,\"start\":12274},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14759,\"start\":14728},{\"attributes\":{\"n\":\"4.\"},\"end\":18284,\"start\":18264},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18922,\"start\":18861},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21346,\"start\":21278},{\"attributes\":{\"n\":\"4.4.\"},\"end\":22812,\"start\":22784},{\"attributes\":{\"n\":\"4.5.\"},\"end\":25766,\"start\":25737},{\"attributes\":{\"n\":\"5.\"},\"end\":25779,\"start\":25769},{\"end\":26987,\"start\":26977},{\"end\":27219,\"start\":27211},{\"end\":28547,\"start\":28527},{\"end\":29282,\"start\":29272},{\"end\":29785,\"start\":29775},{\"end\":29869,\"start\":29849},{\"end\":29932,\"start\":29924},{\"end\":30773,\"start\":30764}]", "table": "[{\"end\":30762,\"start\":30707},{\"end\":30834,\"start\":30775}]", "figure_caption": "[{\"end\":27209,\"start\":26989},{\"end\":28525,\"start\":27221},{\"end\":29270,\"start\":28550},{\"end\":29773,\"start\":29284},{\"end\":29847,\"start\":29787},{\"end\":29922,\"start\":29872},{\"end\":30045,\"start\":29934},{\"end\":30707,\"start\":30048}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3388,\"start\":3387},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":7786,\"start\":7785},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9049,\"start\":9048},{\"end\":10203,\"start\":10202},{\"end\":10567,\"start\":10563},{\"end\":10752,\"start\":10748},{\"end\":10778,\"start\":10777},{\"end\":10805,\"start\":10804},{\"end\":10875,\"start\":10874},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12810,\"start\":12809},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13387,\"start\":13386},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13858,\"start\":13857},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16942,\"start\":16941},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17208,\"start\":17207},{\"end\":19528,\"start\":19527},{\"end\":22195,\"start\":22194},{\"end\":22251,\"start\":22250},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22430,\"start\":22429},{\"end\":22750,\"start\":22743},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23120,\"start\":23119},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23547,\"start\":23546},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23897,\"start\":23896},{\"end\":25131,\"start\":25130},{\"end\":25344,\"start\":25343},{\"end\":25516,\"start\":25515}]", "bib_author_first_name": "[{\"end\":31742,\"start\":31741},{\"end\":31750,\"start\":31749},{\"end\":31752,\"start\":31751},{\"end\":31763,\"start\":31762},{\"end\":31771,\"start\":31770},{\"end\":31859,\"start\":31858},{\"end\":31872,\"start\":31871},{\"end\":31881,\"start\":31880},{\"end\":31893,\"start\":31892},{\"end\":32017,\"start\":32016},{\"end\":32019,\"start\":32018},{\"end\":32032,\"start\":32031},{\"end\":32034,\"start\":32033},{\"end\":32042,\"start\":32041},{\"end\":32138,\"start\":32137},{\"end\":32140,\"start\":32139},{\"end\":32153,\"start\":32152},{\"end\":32155,\"start\":32154},{\"end\":32166,\"start\":32165},{\"end\":32172,\"start\":32171},{\"end\":32174,\"start\":32173},{\"end\":32319,\"start\":32318},{\"end\":32326,\"start\":32325},{\"end\":32333,\"start\":32332},{\"end\":32462,\"start\":32461},{\"end\":32478,\"start\":32477},{\"end\":32489,\"start\":32488},{\"end\":32606,\"start\":32605},{\"end\":32615,\"start\":32614},{\"end\":32622,\"start\":32621},{\"end\":32633,\"start\":32632},{\"end\":32720,\"start\":32719},{\"end\":32727,\"start\":32726},{\"end\":32740,\"start\":32739},{\"end\":32751,\"start\":32750},{\"end\":32762,\"start\":32761},{\"end\":32770,\"start\":32769},{\"end\":32782,\"start\":32781},{\"end\":32796,\"start\":32795},{\"end\":33017,\"start\":33016},{\"end\":33019,\"start\":33018},{\"end\":33029,\"start\":33028},{\"end\":33105,\"start\":33104},{\"end\":33115,\"start\":33114},{\"end\":33124,\"start\":33123},{\"end\":33135,\"start\":33134},{\"end\":33249,\"start\":33248},{\"end\":33251,\"start\":33250},{\"end\":33259,\"start\":33258},{\"end\":33272,\"start\":33271},{\"end\":33281,\"start\":33280},{\"end\":33380,\"start\":33379},{\"end\":33394,\"start\":33393},{\"end\":33481,\"start\":33480},{\"end\":33495,\"start\":33494},{\"end\":33508,\"start\":33507},{\"end\":33510,\"start\":33509},{\"end\":33596,\"start\":33592},{\"end\":33607,\"start\":33606},{\"end\":33609,\"start\":33608},{\"end\":33618,\"start\":33617},{\"end\":33620,\"start\":33619},{\"end\":33707,\"start\":33703},{\"end\":33718,\"start\":33717},{\"end\":33720,\"start\":33719},{\"end\":33729,\"start\":33728},{\"end\":33731,\"start\":33730},{\"end\":33773,\"start\":33772},{\"end\":33782,\"start\":33781},{\"end\":33792,\"start\":33791},{\"end\":33888,\"start\":33887},{\"end\":33898,\"start\":33897},{\"end\":34047,\"start\":34046},{\"end\":34057,\"start\":34056},{\"end\":34061,\"start\":34058},{\"end\":34072,\"start\":34071},{\"end\":34080,\"start\":34079},{\"end\":34088,\"start\":34087},{\"end\":34241,\"start\":34240},{\"end\":34250,\"start\":34249},{\"end\":34258,\"start\":34257},{\"end\":34335,\"start\":34334},{\"end\":34343,\"start\":34342},{\"end\":34347,\"start\":34344},{\"end\":34355,\"start\":34354},{\"end\":34447,\"start\":34446},{\"end\":34462,\"start\":34461},{\"end\":34471,\"start\":34470},{\"end\":34563,\"start\":34562},{\"end\":34575,\"start\":34574},{\"end\":34711,\"start\":34710},{\"end\":34719,\"start\":34718},{\"end\":34725,\"start\":34724},{\"end\":34731,\"start\":34730},{\"end\":34827,\"start\":34826},{\"end\":34831,\"start\":34828},{\"end\":34842,\"start\":34841},{\"end\":34852,\"start\":34848},{\"end\":34858,\"start\":34857},{\"end\":34866,\"start\":34865},{\"end\":34959,\"start\":34958},{\"end\":34970,\"start\":34969},{\"end\":34979,\"start\":34978},{\"end\":34987,\"start\":34986},{\"end\":35080,\"start\":35079},{\"end\":35088,\"start\":35087},{\"end\":35097,\"start\":35096},{\"end\":35106,\"start\":35105},{\"end\":35115,\"start\":35114},{\"end\":35171,\"start\":35170},{\"end\":35178,\"start\":35177},{\"end\":35283,\"start\":35282},{\"end\":35292,\"start\":35291},{\"end\":35300,\"start\":35299},{\"end\":35306,\"start\":35305},{\"end\":35314,\"start\":35313},{\"end\":35367,\"start\":35366},{\"end\":35375,\"start\":35374},{\"end\":35382,\"start\":35381},{\"end\":35388,\"start\":35387},{\"end\":35396,\"start\":35395},{\"end\":35480,\"start\":35479},{\"end\":35487,\"start\":35486},{\"end\":35489,\"start\":35488},{\"end\":35499,\"start\":35498},{\"end\":35501,\"start\":35500},{\"end\":35511,\"start\":35510},{\"end\":35513,\"start\":35512}]", "bib_author_last_name": "[{\"end\":31747,\"start\":31743},{\"end\":31760,\"start\":31753},{\"end\":31768,\"start\":31764},{\"end\":31780,\"start\":31772},{\"end\":31869,\"start\":31860},{\"end\":31878,\"start\":31873},{\"end\":31890,\"start\":31882},{\"end\":31899,\"start\":31894},{\"end\":32029,\"start\":32020},{\"end\":32039,\"start\":32035},{\"end\":32045,\"start\":32043},{\"end\":32150,\"start\":32141},{\"end\":32163,\"start\":32156},{\"end\":32169,\"start\":32167},{\"end\":32179,\"start\":32175},{\"end\":32323,\"start\":32320},{\"end\":32330,\"start\":32327},{\"end\":32339,\"start\":32334},{\"end\":32475,\"start\":32463},{\"end\":32486,\"start\":32479},{\"end\":32495,\"start\":32490},{\"end\":32612,\"start\":32607},{\"end\":32619,\"start\":32616},{\"end\":32630,\"start\":32623},{\"end\":32642,\"start\":32634},{\"end\":32724,\"start\":32721},{\"end\":32737,\"start\":32728},{\"end\":32748,\"start\":32741},{\"end\":32759,\"start\":32752},{\"end\":32767,\"start\":32763},{\"end\":32779,\"start\":32771},{\"end\":32793,\"start\":32783},{\"end\":32804,\"start\":32797},{\"end\":33026,\"start\":33020},{\"end\":33037,\"start\":33030},{\"end\":33112,\"start\":33106},{\"end\":33121,\"start\":33116},{\"end\":33132,\"start\":33125},{\"end\":33141,\"start\":33136},{\"end\":33256,\"start\":33252},{\"end\":33269,\"start\":33260},{\"end\":33278,\"start\":33273},{\"end\":33289,\"start\":33282},{\"end\":33391,\"start\":33381},{\"end\":33401,\"start\":33395},{\"end\":33492,\"start\":33482},{\"end\":33505,\"start\":33496},{\"end\":33517,\"start\":33511},{\"end\":33604,\"start\":33597},{\"end\":33615,\"start\":33610},{\"end\":33631,\"start\":33621},{\"end\":33715,\"start\":33708},{\"end\":33726,\"start\":33721},{\"end\":33742,\"start\":33732},{\"end\":33779,\"start\":33774},{\"end\":33789,\"start\":33783},{\"end\":33799,\"start\":33793},{\"end\":33895,\"start\":33889},{\"end\":33904,\"start\":33899},{\"end\":34054,\"start\":34048},{\"end\":34069,\"start\":34062},{\"end\":34077,\"start\":34073},{\"end\":34085,\"start\":34081},{\"end\":34096,\"start\":34089},{\"end\":34247,\"start\":34242},{\"end\":34255,\"start\":34251},{\"end\":34263,\"start\":34259},{\"end\":34340,\"start\":34336},{\"end\":34352,\"start\":34348},{\"end\":34361,\"start\":34356},{\"end\":34459,\"start\":34448},{\"end\":34468,\"start\":34463},{\"end\":34480,\"start\":34472},{\"end\":34572,\"start\":34564},{\"end\":34585,\"start\":34576},{\"end\":34716,\"start\":34712},{\"end\":34722,\"start\":34720},{\"end\":34728,\"start\":34726},{\"end\":34736,\"start\":34732},{\"end\":34839,\"start\":34832},{\"end\":34846,\"start\":34843},{\"end\":34855,\"start\":34853},{\"end\":34863,\"start\":34859},{\"end\":34874,\"start\":34867},{\"end\":34967,\"start\":34960},{\"end\":34976,\"start\":34971},{\"end\":34984,\"start\":34980},{\"end\":34995,\"start\":34988},{\"end\":35085,\"start\":35081},{\"end\":35094,\"start\":35089},{\"end\":35103,\"start\":35098},{\"end\":35112,\"start\":35107},{\"end\":35118,\"start\":35116},{\"end\":35175,\"start\":35172},{\"end\":35181,\"start\":35179},{\"end\":35289,\"start\":35284},{\"end\":35297,\"start\":35293},{\"end\":35303,\"start\":35301},{\"end\":35311,\"start\":35307},{\"end\":35319,\"start\":35315},{\"end\":35372,\"start\":35368},{\"end\":35379,\"start\":35376},{\"end\":35385,\"start\":35383},{\"end\":35393,\"start\":35389},{\"end\":35400,\"start\":35397},{\"end\":35484,\"start\":35481},{\"end\":35496,\"start\":35490},{\"end\":35508,\"start\":35502},{\"end\":35520,\"start\":35514}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":31792,\"start\":31644},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2259236},\"end\":31973,\"start\":31794},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3353774},\"end\":32096,\"start\":31975},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1800511},\"end\":32252,\"start\":32098},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6137921},\"end\":32371,\"start\":32254},{\"attributes\":{\"doi\":\"arXiv:1709.09283\",\"id\":\"b5\"},\"end\":32543,\"start\":32373},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14882677},\"end\":32655,\"start\":32545},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1799558},\"end\":32932,\"start\":32657},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":16453537},\"end\":33049,\"start\":32934},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12447228},\"end\":33190,\"start\":33051},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":659803},\"end\":33302,\"start\":33192},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":5574079},\"end\":33413,\"start\":33304},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":195908774},\"end\":33529,\"start\":33415},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15842367},\"end\":33643,\"start\":33531},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":10124817},\"end\":33755,\"start\":33645},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1779661},\"end\":33820,\"start\":33757},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1486522},\"end\":33977,\"start\":33822},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":25120102},\"end\":34123,\"start\":33979},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9652770},\"end\":34275,\"start\":34125},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2642044},\"end\":34373,\"start\":34277},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2843566},\"end\":34492,\"start\":34375},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b21\"},\"end\":34622,\"start\":34494},{\"attributes\":{\"id\":\"b22\"},\"end\":34744,\"start\":34624},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17623309},\"end\":34901,\"start\":34746},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":35488672},\"end\":35007,\"start\":34903},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":28544003},\"end\":35132,\"start\":35009},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6423078},\"end\":35195,\"start\":35134},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":10212545},\"end\":35333,\"start\":35197},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5299559},\"end\":35414,\"start\":35335},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15765187},\"end\":35544,\"start\":35416}]", "bib_title": "[{\"end\":31739,\"start\":31644},{\"end\":31856,\"start\":31794},{\"end\":32014,\"start\":31975},{\"end\":32135,\"start\":32098},{\"end\":32316,\"start\":32254},{\"end\":32603,\"start\":32545},{\"end\":32717,\"start\":32657},{\"end\":33014,\"start\":32934},{\"end\":33102,\"start\":33051},{\"end\":33246,\"start\":33192},{\"end\":33377,\"start\":33304},{\"end\":33478,\"start\":33415},{\"end\":33590,\"start\":33531},{\"end\":33701,\"start\":33645},{\"end\":33770,\"start\":33757},{\"end\":33885,\"start\":33822},{\"end\":34044,\"start\":33979},{\"end\":34238,\"start\":34125},{\"end\":34332,\"start\":34277},{\"end\":34444,\"start\":34375},{\"end\":34824,\"start\":34746},{\"end\":34956,\"start\":34903},{\"end\":35077,\"start\":35009},{\"end\":35168,\"start\":35134},{\"end\":35280,\"start\":35197},{\"end\":35364,\"start\":35335},{\"end\":35477,\"start\":35416}]", "bib_author": "[{\"end\":31749,\"start\":31741},{\"end\":31762,\"start\":31749},{\"end\":31770,\"start\":31762},{\"end\":31782,\"start\":31770},{\"end\":31871,\"start\":31858},{\"end\":31880,\"start\":31871},{\"end\":31892,\"start\":31880},{\"end\":31901,\"start\":31892},{\"end\":32031,\"start\":32016},{\"end\":32041,\"start\":32031},{\"end\":32047,\"start\":32041},{\"end\":32152,\"start\":32137},{\"end\":32165,\"start\":32152},{\"end\":32171,\"start\":32165},{\"end\":32181,\"start\":32171},{\"end\":32325,\"start\":32318},{\"end\":32332,\"start\":32325},{\"end\":32341,\"start\":32332},{\"end\":32477,\"start\":32461},{\"end\":32488,\"start\":32477},{\"end\":32497,\"start\":32488},{\"end\":32614,\"start\":32605},{\"end\":32621,\"start\":32614},{\"end\":32632,\"start\":32621},{\"end\":32644,\"start\":32632},{\"end\":32726,\"start\":32719},{\"end\":32739,\"start\":32726},{\"end\":32750,\"start\":32739},{\"end\":32761,\"start\":32750},{\"end\":32769,\"start\":32761},{\"end\":32781,\"start\":32769},{\"end\":32795,\"start\":32781},{\"end\":32806,\"start\":32795},{\"end\":33028,\"start\":33016},{\"end\":33039,\"start\":33028},{\"end\":33114,\"start\":33104},{\"end\":33123,\"start\":33114},{\"end\":33134,\"start\":33123},{\"end\":33143,\"start\":33134},{\"end\":33258,\"start\":33248},{\"end\":33271,\"start\":33258},{\"end\":33280,\"start\":33271},{\"end\":33291,\"start\":33280},{\"end\":33393,\"start\":33379},{\"end\":33403,\"start\":33393},{\"end\":33494,\"start\":33480},{\"end\":33507,\"start\":33494},{\"end\":33519,\"start\":33507},{\"end\":33606,\"start\":33592},{\"end\":33617,\"start\":33606},{\"end\":33633,\"start\":33617},{\"end\":33717,\"start\":33703},{\"end\":33728,\"start\":33717},{\"end\":33744,\"start\":33728},{\"end\":33781,\"start\":33772},{\"end\":33791,\"start\":33781},{\"end\":33801,\"start\":33791},{\"end\":33897,\"start\":33887},{\"end\":33906,\"start\":33897},{\"end\":34056,\"start\":34046},{\"end\":34071,\"start\":34056},{\"end\":34079,\"start\":34071},{\"end\":34087,\"start\":34079},{\"end\":34098,\"start\":34087},{\"end\":34249,\"start\":34240},{\"end\":34257,\"start\":34249},{\"end\":34265,\"start\":34257},{\"end\":34342,\"start\":34334},{\"end\":34354,\"start\":34342},{\"end\":34363,\"start\":34354},{\"end\":34461,\"start\":34446},{\"end\":34470,\"start\":34461},{\"end\":34482,\"start\":34470},{\"end\":34574,\"start\":34562},{\"end\":34587,\"start\":34574},{\"end\":34718,\"start\":34710},{\"end\":34724,\"start\":34718},{\"end\":34730,\"start\":34724},{\"end\":34738,\"start\":34730},{\"end\":34841,\"start\":34826},{\"end\":34848,\"start\":34841},{\"end\":34857,\"start\":34848},{\"end\":34865,\"start\":34857},{\"end\":34876,\"start\":34865},{\"end\":34969,\"start\":34958},{\"end\":34978,\"start\":34969},{\"end\":34986,\"start\":34978},{\"end\":34997,\"start\":34986},{\"end\":35087,\"start\":35079},{\"end\":35096,\"start\":35087},{\"end\":35105,\"start\":35096},{\"end\":35114,\"start\":35105},{\"end\":35120,\"start\":35114},{\"end\":35177,\"start\":35170},{\"end\":35183,\"start\":35177},{\"end\":35291,\"start\":35282},{\"end\":35299,\"start\":35291},{\"end\":35305,\"start\":35299},{\"end\":35313,\"start\":35305},{\"end\":35321,\"start\":35313},{\"end\":35374,\"start\":35366},{\"end\":35381,\"start\":35374},{\"end\":35387,\"start\":35381},{\"end\":35395,\"start\":35387},{\"end\":35402,\"start\":35395},{\"end\":35486,\"start\":35479},{\"end\":35498,\"start\":35486},{\"end\":35510,\"start\":35498},{\"end\":35522,\"start\":35510}]", "bib_venue": "[{\"end\":31786,\"start\":31782},{\"end\":31963,\"start\":31901},{\"end\":32087,\"start\":32047},{\"end\":32243,\"start\":32181},{\"end\":32345,\"start\":32341},{\"end\":32459,\"start\":32373},{\"end\":32648,\"start\":32644},{\"end\":32872,\"start\":32806},{\"end\":33043,\"start\":33039},{\"end\":33181,\"start\":33143},{\"end\":33295,\"start\":33291},{\"end\":33407,\"start\":33403},{\"end\":33523,\"start\":33519},{\"end\":33637,\"start\":33633},{\"end\":33748,\"start\":33744},{\"end\":33807,\"start\":33801},{\"end\":33968,\"start\":33906},{\"end\":34102,\"start\":34098},{\"end\":34269,\"start\":34265},{\"end\":34367,\"start\":34363},{\"end\":34486,\"start\":34482},{\"end\":34560,\"start\":34494},{\"end\":34708,\"start\":34624},{\"end\":34880,\"start\":34876},{\"end\":35001,\"start\":34997},{\"end\":35124,\"start\":35120},{\"end\":35187,\"start\":35183},{\"end\":35325,\"start\":35321},{\"end\":35406,\"start\":35402},{\"end\":35526,\"start\":35522},{\"end\":32925,\"start\":32874}]"}}}, "year": 2023, "month": 12, "day": 17}