{"id": 254044221, "updated": "2023-11-08 15:10:17.744", "metadata": {"title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers", "authors": "[{\"first\":\"Yuqi\",\"last\":\"Nie\",\"middle\":[]},{\"first\":\"Nam\",\"last\":\"Nguyen\",\"middle\":[\"H.\"]},{\"first\":\"Phanwadee\",\"last\":\"Sinthong\",\"middle\":[]},{\"first\":\"Jayant\",\"last\":\"Kalagnanam\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2211.14730", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/NieNSK23", "doi": "10.48550/arxiv.2211.14730"}}, "content": {"source": {"pdf_hash": "dad15404d372a23b4b3bf9a63b3124693df3c85e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.14730v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ae3eec02bc0c46eff3885b11b244ce9249166a32", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/dad15404d372a23b4b3bf9a63b3124693df3c85e.txt", "contents": "\nA TIME SERIES IS WORTH 64 WORDS: LONG-TERM FORECASTING WITH TRANSFORMERS\n\n\nYuqi Nie ynie@princeton.edu \nPrinceton University\n\n\nNam H Nguyen nnguyen@us.ibm.com \nIBM Research\n\n\nPhanwadee Sinthong gift.sinthong@ibm.com \nIBM Research\n\n\nJayant Kalagnanam jayant@us.ibm.com \nIBM Research\n\n\nA TIME SERIES IS WORTH 64 WORDS: LONG-TERM FORECASTING WITH TRANSFORMERS\nPublished as a conference paper at ICLR 2023\nWe propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pretraining tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. * Equal contribution.\n\nINTRODUCTION\n\nForecasting is one of the most important tasks in time series analysis. With the rapid growth of deep learning models, the number of research works has increased significantly on this topic (Bryan & Stefan, 2021;Torres et al., 2021;Lara-Ben\u00edtez et al., 2021). Deep models have shown excellent performance not only on forecasting tasks, but also on representation learning where abstract representation can be extracted and transferred to various downstream tasks such as classification and anomaly detection to attain state-of-the-art performance.\n\nAmong deep learning models, Transformer has achieved great success on various application fields such as natural language processing (NLP) (Kalyan et al., 2021), computer vision (CV) (Khan et al., 2021), speech (Karita et al., 2019), and more recently time series (Wen et al., 2022), benefiting from its attention mechanism which can automatically learn the connections between elements in a sequence, thus becomes ideal for sequential modeling tasks. Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021), andFEDformer (Zhou et al., 2022) are among the best variants of the Transformer model successfully applying to time series data. Unfortunately, regardless of the complicated design of Transformer-based models, it is shown in the recent paper (Zeng et al., 2022) that a very simple linear model can outperform all of the previous models on a variety of common benchmarks and it challenges the usefulness of Transformer for time series forecasting. In this paper, we attempt to answer this question by proposing a channel-independence patch time series Transformer (PatchTST) model that contains two key designs:\n\n\u2022 Patching. Time series forecasting aims to understand the correlation between data in each different time steps. However, a single time step does not have semantic meaning like a word in a sentence, thus extracting local semantic information is essential in analyzing their connections. Most of the previous works only use point-wise input tokens, or just a handcrafted information The best result is in bold and the second best is underlined. Down-sampled means sampling every 4 step and adding the last value. All the results are from supervised training except the best result which uses self-supervised learning.\n\nfrom series. In contrast, we enhance the locality and capture comprehensive semantic information that is not available in point-level by aggregating time steps into subseries-level patches.\n\n\u2022 Channel-independence. A multivariate time series is a multi-channel signal, and each Transformer input token can be represented by data from either a single channel or multiple channels. Depending on the design of input tokens, different variants of the Transformer architecture have been proposed. Channel-mixing refers to the latter case where the input token takes the vector of all time series features and projects it to the embedding space to mix information. On the other hand, channel-independence means that each input token only contains information from a single channel. This was proven to work well with CNN (Zheng et al., 2014) and linear models (Zeng et al., 2022), but hasn't been applied to Transformer-based models yet.\n\nWe offer a snapshot of our key results in Table 1 by doing a case study on Traffic dataset, which consists of 862 time series. Our model has several advantages:\n\n1. Reduction on time and space complexity: The original Transformer has O(N 2 ) complexity on both time and space, where N is the number of input tokens. Without pre-processing, N will have the same value as input sequence length L, which becomes a primary bottleneck of computation time and memory in practice. By applying patching, we can reduce N by a factor of the stride: N \u2248 L/S, thus reducing the complexity quadratically. Table 1 illustrates the usefulness of patching. By setting patch length P = 16 and stride S = 8 with L = 336, the training time is significantly reduced as much as 22 time on large datasets.\n\n2. Capability of learning from longer look-back window: Table 1 shows that by increasing lookback window L from 96 to 336, MSE can be reduced from 0.518 to 0.397. However, simply extending L comes at the cost of larger memory and computational usage. Since time series often carries heavily temporal redundant information, some previous work tried to ignore parts of data points by using downsampling or a carefully designed sparse connection of attention (Li et al., 2019) and the model still yields sufficient information to forecast well. We study the case when L = 380 and the time series is sampled every 4 steps with the last point added to sequence, resulting in the number of input tokens being N = 96. The model achieves better MSE score (0.447) than using the data sequence containing the most recent 96 time steps (0.518), indicating that longer look-back window conveys more important information even with the same number of input tokens. This leads us to think of a question: is there a way to avoid throwing values while maintaining a long look-back window? Patching is a good answer to it. It can group local time steps that may contain similar values while at the same time enables the model to reduce the input token length for computational benefit. As evident in Table 1, MSE score is further reduced from 0.397 to 0.367 with patching when L = 336.\n\n3. Capability of representation learning: With the emergence of powerful self-supervised learning techniques, sophisticated models with multiple non-linear layers of abstraction are required to capture abstract representation of the data. Simple models like linear ones (Zeng et al., 2022) may not be preferred for that task due to its limited expressibility. With our PatchTST model, we not only confirm that Transformer is actually effective for time series forecasting, but also demonstrate the representation capability that can further enhance the forecasting performance. Our PatchTST has achieved the best MSE (0.349) in Table 1.\n\nWe introduce our approach in more detail and conduct extensive experiments in the following sections to conclusively prove our claims. We not only demonstrate the model effectiveness with supervised forecasting results and ablation studies, but also achieves SOTA self-supervised representation learning and transfer learning performance.\n\n\nRELATED WORK\n\nPatch in Transformer-based Models. Transformer (Vaswani et al., 2017) has demonstrated a significant potential on different data modalities. Among all applications, patching is an essential part when local semantic information is important. In NLP, BERT (Devlin et al., 2018) considers subword-based tokenization (Schuster & Nakajima, 2012) instead of performing character-based tokenization. In CV, Vision Transformer (ViT) (Dosovitskiy et al., 2021) is a milestone work that splits an image into 16\u00d716 patches before feeding into the Transformer model. The following influential works such as BEiT (Bao et al., 2022) and masked autoencoders (He et al., 2021) are all using patches as input. Similarly, in speech researchers are using convolutions to extract information in sub-sequence levels from raw audio input (Baevski et al., 2020;Hsu et al., 2021). Most of these models focus on designing novel mechanisms to reduce the complexity of original attention mechanism, thus achieving better performance on forecasting, especially when the prediction length is long. However, most of the models use point-wise attention, which ignores the importance of patches. LogTrans (Li et al., 2019) avoids a point-wise dot product between the key and query, but its value is still based on a single time step. Autoformer (Wu et al., 2021) uses autocorrelation to get patch level connections, but it is a handcrafted design which doesn't include all the semantic information within a patch. Triformer (Cirstea et al., 2022) proposes patch attention, but the purpose is to reduce complexity by using a pseudo timestamp as the query within a patch, thus it neither treats a patch as a input unit, nor reveals the semantic importance behind it.\n\nTime Series Representation Learning. Besides supervised learning, self-supervised learning is also an important research topic since it has shown the potential to learn useful representations for downstream tasks. There are many non-Transformer-based models proposed in recent years to learn representations in time series (Franceschi et al., 2019;Tonekaboni et al., 2021;Yang & Hong, 2022;Yue et al., 2022). Meanwhile, Transformer is known to be an ideal candidate towards foundation models (Bommasani et al., 2021) and learning universal representations. However, although people have made attempts on Transformer-based models like time series Transformer (TST) (Zerveas et al., 2021) and TS-TCC (Eldele et al., 2021), the potential is still not fully realized yet.\n\n\nPROPOSED METHOD\n\n\nMODEL STRUCTURE\n\nWe consider the following problem: given a collection of multivariate time series samples with lookback window L : (x 1 , ..., x L ) where each x t at time step t is a vector of dimension M , we would like to forecast T future values (x L+1 , ..., x L+T ). Our PatchTST is illustrated in Figure 1 where the model makes use of the vanilla Transformer encoder as its core architecture.\n\nForward Process. We denote a i-th univariate series of length L starting at time index 1 as x (i) \n1:L = (x (i) 1 , ..., x (i) L ) where i = 1, ..., M . The input (x 1 , ..., x L ) is split to M univariate series x (i) \u2208 R 1\u00d7L ,\n\nChannelindependence\n\nTransformer Backbone  Each channel univariate series is passed through instance normalization operator and segmented into patches. These patches are used as Transformer input tokens. (c) Masked self-supervised representation learning with PatchTST where patches are randomly selected and set to zero. The model will reconstruct the masked patches. our channel-independence setting. Then the Transformer backbone will provide prediction result\u015d\nConcatenate (%) \u2208 \u211d '\u00d7# , = 1, \u2026 , + (%) \u2208 \u211d '\u00d7( , = 1, \u2026 , + \u2208 \u211d !\u00d7( (\") \u2208 \u211d $\u00d7&x (i) = (x (i) L+1 , ...,x (i) L+T ) \u2208 R 1\u00d7T accordingly .\nPatching. Each input univariate time series x (i) is first divided into patches which can be either overlapped or non-overlapped. Denote the patch length as P and the stride -the non overlapping region between two consecutive patches as S, then the patching process will generate the a sequence of patches x L \u2208 R to the end of the original sequence before patching. With the use of patches, the number of input tokens can reduce from L to approximately L/S. This implies the memory usage and computational complexity of the attention map are quadratically decreased by a factor of S. Thus constrained on the training time and GPU memory, patch design can allow the model to see the longer historical sequence, which can significantly improve the forecasting performance, as demonstrated in Table 1.\n\nTransformer Encoder. We use a vanilla Transformer encoder that maps the observed signals to the latent representations. The patches are mapped to the Transformer latent space of dimension D via a trainable linear projection W p \u2208 R D\u00d7P , and a learnable additive position encoding W pos \u2208 R D\u00d7N is applied to monitor the temporal order of patches:\nx (i) d = W p x (i) p + W pos , where x (i) d \u2208 R D\u00d7Ndenote\nthe input that will be fed into Transformer encoder in Figure 1. Then each head h = 1, ..., H in multi-head attention will transform them into query matrices Q\n(i) h = (x (i) d ) T W Q h , key matrices K (i) h = (x (i) d ) T W K h and value matrices V (i) h = (x (i) d ) T W V h , where W Q h , W K h \u2208 R D\u00d7d k and W V h \u2208 R D\u00d7D .\nAfter that a scaled production is used for getting attention output O\n(i) h \u2208 R D\u00d7N : (O (i) h ) T = Attention(Q (i) h , K (i) h , V (i) h ) = Softmax( Q (i) h K (i) h T \u221a d k )V (i) h\nThe multi-head attention block also includes BatchNorm 1 layers and a feed forward network with residual connections as shown in Figure 1. Afterwards it generates the representation denoted as z (i) \u2208 R D\u00d7N . Finally a flatten layer with linear head is used to obtain the prediction resultx\n(i) = (x (i) L+1 , ...,x (i) L+T ) \u2208 R 1\u00d7T\n. Loss Function. We choose to use the MSE loss to measure the discrepancy between the prediction and the ground truth. The loss in each channel is gathered and averaged over M time series to get the overall objective loss:\nL = E x 1 M M i=1 x (i) L+1:L+T \u2212 x (i) L+1:L+T 2 2 .\nInstance Normalization. This technique has recently been proposed to help mitigating the distribution shift effect between the training and testing data (Ulyanov et al., 2016;Kim et al., 2022). It simply normalizes each time series instance x (i) with zero mean and unit standard deviation. In essence, we normalize each x (i) before patching and the mean and deviation are added back to the output prediction.\n\n\nREPRESENTATION LEARNING\n\nSelf-supervised representation learning has become a popular approach to extract high level abstract representation from unlabelled data. In this section, we apply PatchTST to obtain useful representation of the multivariate time series. We will show that the learnt representation can be effectively transferred to forecasting tasks. Among popular methods to learn representation via self-supervise pre-training, masked autoencoder has been applied successfully to NLP (Devlin et al., 2018) and CV (He et al., 2021) domains. This technique is conceptually simple: a portion of input sequence is intentionally removed at random and the model is trained to recover the missing contents.\n\nMasked encoder has been recently employed in time series and delivered notable performance on classification and regression tasks (Zerveas et al., 2021). The authors proposed to apply the multivariate time series to Transformer, where each input token is a vector x i consisting of time series values at time step i-th. Masking is placed randomly within each time series and across different series. However, there are two potential issues with this setting: First, masking is applied at the level of single time steps. The masked values at the current time step can be easily inferred by interpolating with the immediate proceeding or succeeding time values without high level understanding of the entire sequence, which deviates from our goal of learning important abstract representation of the whole signal. Zerveas et al. (2021) proposed complex randomization strategies to resolve the problem in which groups of time series with different sizes are randomly masked.\n\nSecond, the design of the output layer for forecasting task can be troublesome. Given the representation vectors z t \u2208 R D corresponding to all L time steps, mapping these vectors to the output containing M variables each with prediction horizon T via a linear map requires a parameter matrix W of dimension (L \u00b7 D) \u00d7 (M \u00b7 T ). This matrix can be particularly oversized if either one or all of these four values are large. This may cause overfitting when the number of downstream training samples is scarce.\n\nOur proposed PatchTST can naturally overcome the aforementioned issues. As shown in Figure 1, we use the same Transformer encoder as the supervised settings. The prediction head is removed and a D \u00d7 P linear layer is attached. As opposed to supervised model where patches can be overlapped, we divide each input sequence into regular non-overlapping patches. It is for convenience to ensure observed patches do not contain information of the masked ones. We then select a subset of the patch indices uniformly at random and mask the patches according to these selected indices with zero values. The model is trained with MSE loss to reconstruct the masked patches.\n\nWe emphasize that each time series will have its own latent representation that are cross-learned via a shared weight mechanism. This design can allow the pre-training data to contain different number of time series than the downstream data, which may not be feasible by other approaches.\n\n\nEXPERIMENTS\n\n\nLONG-TERM TIME SERIES FORECASTING\n\nDatasets. We evaluate the performance of our proposed PatchTST on 8 popular datasets, including Weather, Traffic, Electricity, ILI and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). These datasets have been extensively utilized for benchmarking and publicly available on (Wu et al., 2021). The statistics of those datasets are summarized in Table 2. We would like to highlight several large datasets: Weather, Traffic, and Electricity. They have many more number of time series, thus the results would be more stable and less susceptible to overfitting than other smaller datasets.\n\n\nDatasets\n\nWeather  Model Variants. We propose two versions of PatchTST in Table 3. PatchTST/64 implies the number of input patches is 64, which uses the look-back window L = 512. PatchTST/42 means the number of input patches is 42, which has the default look-back window L = 336. Both of them use patch length P = 16 and stride S = 8. Thus, we could use PatchTST/42 as a fair comparison to DLinear and other Transformer-based models, and PatchTST/64 to explore even better results on larger datasets. More experimental details are provided in Appendix A.1.\n\nResults. Table 3 shows the multivariate long-term forecasting results. Overall, our model outperform all baseline methods. Quantitatively, compared with the best results that Transformer-based models can offer, PatchTST/64 achieves an overall 21.0% reduction on MSE and 16.7% reduction on MAE, while PatchTST/42 attains an overall 20.2% reduction on MSE and 16.4% reduction on MAE. Compared with the DLinear model, PatchTST can still outperform it in general, especially on large datasets (Weather, Traffic, Electricity) and ILI dataset. We also experiment with univariate datasets where the results are provided in Appendix A.3.\n\n\nREPRESENTATION LEARNING\n\nIn this section, we conduct experiments with masked self-supervised learning where we set the patches to be non-overlapped. Otherwise stated, across all representation learning experiments the input sequence length is chosen to be 512 and patch size is set to 12, which results in 42 patches. We consider high masking ratio where 40% of the patches are masked with zero values. We first apply self-supervised pre-training on the datasets mentioned in Section 4.1 for 100 epochs. Once the pre-trained model on each dataset is available, we perform supervised training to evaluate the learned representation with two options: (a) linear probing and (b) end-to-end fine-tuning. With (a), we only train the model head for 20 epochs while freezing the rest of the network; With (b), we apply linear probing for 10 epochs to update the model head and then end-to-end fine-tuning the entire network for 20 epochs. It was proven that a two-step strategy with linear probing followed by fine-tuning can outperform only doing fine-tuning directly (Kumar et al., 2022). We select a few representative results on below, and a full benchmark can be found in Appendix A.5.       Comparison with Supervised Methods. Table 4 compares the performance of PatchTST (with fine-tuning, linear probing, and supervising from scratch) with other supervised method. As shown in the table, on large datasets our pre-training procedure contributes a clear improvement compared to supervised training from scratch. By just fine-tuning the model head (linear probing), the forecasting performance is already comparable with training the entire network from scratch and better than DLinear. The best results are observed with end-to-end fine-tuning. Self-supervised PatchTST significantly outperforms other Transformer-based models on all the datasets.\n\nTransfer Learning. We test the capability of transfering the pre-trained model to downstream tasks. In particular, we pre-train the model on Electricity dataset and fine-tune on other datasets. We observe from Table 5 that overall the fine-tuning MSE is lightly worse than pre-training and finetuning on the same dataset, which is reasonable. The fine-tuning performance is also worse than supervised training in some cases. However, the forecasting performance is still better than other     (Eldele et al., 2021) which are among the state-of-the-art contrastive learning representation methods for time series 2 . We test the forecasting performance on ETTh1 dataset, where we only apply linear probing after the learned representation is obtained (only fine-tune the last linear layer) to make the comparison fair. Results from Table 6 strongly indicates the superior performance of PatchTST, both from pre-training on its own ETTh1 data (self-supervised columns) or pre-training on Traffic (transferred columns).\n\n\nABLATION STUDY\n\nPatching and Channel-independence. We study the effects of patching and channel-independence in Table 7. We include FEDformer as the SOTA benchmark for Transformer-based model. By comparing results with and without the design of patching / channel-independence accordingly, one can observe that both of them are important factors in improving the forecasting performance. The motivation of patching is natural; furthermore this technique improves the running time and memory consumption as shown in Table 1 due to shorter Transformer sequence input. Channel-independence, on the other hand, may not be as intuitive as patching is in terms of the technical advantages. Therefore, we provide an in-depth analysis on the key factors that make channel-independence more preferable in Appendix A.7. More ablation study results are available in Appendix A.4.\n\nVarying Look-back Window. In principle, a longer look-back window increases the receptive field, which will potentially improves the forecasting performance. However, as argued in (Zeng et al., 2022), this phenomenon hasn't been observed in most of the Transformer-based models. We also demonstrate in Figure 2 that in most cases, these Transformer-based baselines have not benefited from longer look-back window L, which indicates their ineffectiveness in capturing temporal information. In contrast, our PatchTST consistently reduces the MSE scores as the receptive field increases, which confirms our model's capability to learn from longer look-back window.  \n\n\nCONCLUSION AND FUTURE WORK\n\nThis paper proposes an effective design of Transformer-based models for time series forecasting tasks by introducing two key components: patching and channel-independent structure. Compared to the previous works, it could capture local semantic information and benefit from longer look-back windows. We not only show that our model outperforms other baselines in supervised learning, but also prove its promising capability in self-supervised representation learning and transfer learning.\n\nOur model exhibits the potential to be the based model for future work of Transformer-based forecasting and be a building block for time series foundation models. Patching is simple but proven to be an effective operator that can be transferred easily to other models. Channel-independence, on the other hand, can be further exploited to incorporate the correlation between different channels. It would be an important future step to model the cross-channel dependencies properly. There is an additional Exchange-rate 8 dataset mentioned in the original paper, which is the daily exchange-rate of eight different countries. However, financial datasets generally have different properties compared to time series datasets in other fields, for example the predictability. It is well known that if a market is efficient, the best prediction for x t will be just x t\u22121 (Fama, 1970). Rossi (2013) argues that the toughest benchmark for exchange-rate forecasting is a random walk without drift. Also, Zeng et al. (2022) shows that by simply repeating the last value in the look-back window, the MSE loss on exchange-rate dataset can outperform or be comparable to the best results. Therefore, we are prudent in containing it into our benchmark.\n\n\nA.1.2 DETAILS OF BASELINE SETTINGS\n\nThe default look-back windows for different baseline models could be different. For Transformerbased models, the default look-back window is L = 96; and for DLinear, the default look-back window is L = 336. The reason of this difference is that Transformer-based baselines are easy to overfit when look-back window is long while DLinear tend to underfit. However, this can possibly lead to an under-estimation of the baselines. To address this issue, we re-run FEDformer, Autoformer and Informer by ourselves for six different look-back window L \u2208 {24, 48, 96, 192, 336, 720}. And for each forecasting task (aka each different prediction length on each dataset), we choose the best one from those six results. Thus it could be a strong baseline.\n\nThe ILI dataset is much smaller than the other datasets, so a different set of parameters is applied (the default look-back windows of Transformer-based models and DLinear are L = 36 and L = 104 respectively; we run FEDformer, Autoformer and Informer for six different look-back window L \u2208 {24, 36, 48, 60, 104, 144} and choose the best results).\n\n\nA.1.3 BASELINES FROM TRADITIONAL MODELS\n\nTime series has been an ancient field of study, with many traditional models developed, for example the famous ARIMA model (Box & Jenkins, 1970). With the bloom of deep learning community, many new models were proposed for sequence modeling and time series forecasting before Transformer appears, such as LSTM (Hochreiter & Schmidhuber, 1997), TCN (Bai et al., 2018) and DeepAR (Salinas et al., 2020). However, they are demonstrated to be not as effective as Transformerbased models in long-term forecasting tasks (Zhou et al., 2021;Wu et al., 2021), thus we don't include them in our baselines. Although PatchTST processes channels in parallel which has to make multiple copies of the Transformer's weights, the computation can be implemented efficiently and does not require any special operator. The batch of samples of x \u2208 R M \u00d7L with size B \u00d7 M \u00d7 L is passed through the patching operator to generate a 4D tensor of size B \u00d7 M \u00d7 P \u00d7 N which represents a batch of x (i) p \u2208 R P \u00d7N in M series. By reshaping the tensor to form a 3D one of size (B \u00b7 M ) \u00d7 P \u00d7 N , this batch can be consumed by any standard Transformer implementation.\n\nWe further argue that our proposed PatchTST contains additional benefits: The components in Transformer backbone module shown in Figure 1 can differ across different input series, for instance the embedding layers and head layers. Note that if the embedding layers are designed differently for each group of time series, the reshaping step will be applied after embedding. Besides, the number of variables in the multivariate time series during the training may not need to match the number of series for testing. This is especially beneficial for self-supervised pre-training where the pre-training data can have different number of variables from the fine-tuning data.\n\n\nA.2 VISUALIZATION\n\nWe visualize the long-term forecasting results of supervised PatchTST/42 and other baselines in Figure 3. Here, we predict 192 steps ahead on Weather and Eletricity datasets and 60 steps ahead on ILI dataset. PatchTST provides the best forecasting both in terms of scale and bias. A.3 UNIVARIATE FORECASTING Table 8 summaries the results of univariate forecasting on ETT datasets. There is a target feature \"oil temperature\" within those datasets, which is the univariate series that we are trying to forecast. We cite the baseline results from (Zeng et al., 2022).\n\n\nA.4 MORE RESULTS ON ABLATION STUDY\n\n\nA.4.1 VARYING PATCH LENGTH\n\nThis experiment studies the effect of patch lengths to the forecasting performance. We fix the lookback window to be 336 and vary the patch lengths P = [4,8,16,24,32,40]. The stride length is set  the same as patch length, meaning no overlapping between patches. The model is trained to predict 96 steps. One observation from Figure 4 is that MSE scores don't vary significantly with different choices of P , which indicate the robustness of our model against the patch length hyperparameter. Overall, PatchTST benefits from increased patch length, not only in forecasting performance but also in the computational reduction. The ideal patch length may depend on the dataset, but P between {8, 16} seems to be general good numbers.  2,4,8,12,16,24,32,40] where the lookback window is 336 and the prediction length is 96.\n\n\nA.4.2 VARYING LOOK-BACK WINDOW\n\nHere we provide a full benchmark of quantitative results in Table 9 for varying look-back window in supervised PatchTST/42 regarding Figure 2 in the main text. Generally speaking, our model gains performance improvement with increasing look-back window, which show the effectiveness of our model in learning information from longer receptive field.\n\n\nA.4.3 PATCHING AND CHANNEL-INDEPENDENCE\n\nImplementation Details. For ablation study on patching and channel independence in section 4.3, we run different variants of PatchTST:\n\n\u2022 Both patching and channel-independence are included in model (P+CI): this is the full PatchTST model that we have proposed in paper.\n\n\u2022 Only channel-independence (CI): we simply set both patch length P and stride S to be 1 to avoid patching and only keep channel-independence.\n\n\u2022 Only patching (P): referring to the implementation in A.  Note that the default number of maximum epochs for Electricity and Traffic datasets are reduced from 100 to 20 for ablation experiments due to the huge space and time complexity of the original time series Transformer and channel independent model (no patching).\n\nFull Benchmark of Ablation Study. The full benchmark is shown in Table 10, which is a completed version of Table 7 in the main text. We can observe that patching together with channelindependence achieves the best results from the table, especially on larger datasets (Weather, Traffic, and Electricity) where the models are less susceptible to overfitting and thus the results would be more convincing. As one can see the improvement is robust on both patching and channelindependence. It is interesting to see that the improvement on ILI dataset is significant as well.\n\n\nA.4.4 INSTANCE NORMALIZATION\n\nNormalization is a technology used in many time series model to improve the forecasting performance (Kim et al., 2022;Chen et al., 2022;Zeng et al., 2022). In this experiment, we perform analysis on the effect of the instance normalization in our model. We train two models PatchTST/64 and PatchTST/42 with and without using instance normalization and observe the evaluated scores. As indicated in Table 11, instance normalization improves the forecasting performance slightly on two models. However, even without instance normalization operator, PatchTST still outperforms other Transformer methods on most of the datasets. This is to confirm that the improvement mainly comes from patching and channel independence designs.\n\nA.5 MORE RESULTS ON SELF-SUPERVISED REPRESENTATION LEARNING\n\n\nA.5.1 FULL BENCHMARK OF MULTIVARIATE FORECASTING\n\nIn this section we provide a full benchmark of multivariate forecasting results with self-supervised PatchTST in Table 12, which is an extended version of Table 4.     Table 11: Multivariate long-term forecasting results of supervised PatchTST with instance normalization (+in) or without instance normalization (-in). The best results are in bold and the second best are underlined. Although the models perform slightly better with instance normalization, compared to other Transformer models, the proposed approach achieve significantly better forecasting on most of the datasets even without instance normalization.\n\n\nA.5.2 FULL BENCHMARK OF TRANSFER LEARNING\n\nIn this section we provide Table 13 which contains the results of pre-training on Electricity dataset then transferred to other 6 datasets. Except Traffic data, the number of time series employed in the pre-training is much larger than the number of series during fine-tuning. It is a full version with respect to Table 5 in the main text and more cogently proves the capability to do transfer learning using our PatchTST model. We also validate the self-supervised PatchTST model on different runs. We pre-train the model once and fine-tune the model 5 times with different random batch selections. The mean and standard derivation across different runs are also provided in Table 14. We also observe that the variance is insignificant especially on large datasets while higher variance can be seen on smaller datasets.   Intuitively, channel-mixing models should outperform the channel-independent ones since they have more flexibility to explore the cross-channel information, while with channel-independence the correlation is indirectly learnt via weight sharing. However, this is contrast to what we have observed. In Section A.7.1 we will provide an in-depth analysis on why channel-independence has better forecasting performance than channel mixing, and in Section A.7.2 we show that channel-independence is a general technique that can be used not only for PatchTST but also for the other models.\n\n\nA.7.1 CHANNEL-INDEPENDENCE VS CHANNEL-MIXING\n\nWe find 3 key factors that makes channel-independent models more preferable:\n\n\u2022 Adaptability: Since each time series is passed through the Transformer separately, it generates its own attention maps. That means different series can learn different attention patterns for their prediction, as shown in Figure 6. In contrast, with the channel mixing approach, all the series share the same attention patterns, which may be harmful if the underlying multivariate time series carries series of different behaviors. Figure 6 reveals an interesting observation that the prediction of unrelated time series relies on different attention patterns while similar series can produce similar maps (e.g. series 11, 25, and 81 contain similar patterns while they are different from others). We suspect this adaptability is one of the main reasons why PatchTST performs much better forecasting than Informer and other channel-mixing models.\n\n\u2022 Channel-mixing models may need more training data to match the performance of the channelindependent ones. The flexibility of learning cross-channel correlations could be a doubleedged sword, because it may need much more data to learn the information from different channels and different time steps jointly and appropriately, while channel-independent models only focus on learning information along the time axis. We examine this assumption by experiments where we train the models with varying training data size and and the result is shown on left panel of Figure 7. It is clear that channel-independent models converges faster against the size of training data. As what we have observed in the figure and Table 2, the size of those widely used time series datasets may not be large enough for channel-mixing models to obtain similar performances in supervised learning.\n\n\u2022 Channel-independent models are less likely to overfit data during training. We record the MSE loss on test data and plot on the right panel of Figure 7. Channel-mixing models show overfitting after a few initial epochs, while channel-independent models continue optimizing the loss with more training epochs. The best trained models are determined by validation data, which are approximately the lowest points in the test loss curves. It is clear that the forecasting performance of channel-independent models are better.\n\nFurthermore, we would like to comment on a few additional technical advantages of channelindependence:\n\n(1) Possibility of learning spatial correlations across series: Although we haven't focused on this research in our paper, the channel-independence design can be naturally extended to learn cross-channel relationships by using methods like graph neural networks (Cao et al., 2020;.\n\n(2) Multi-task learning where different loss types can be imposed on different time series where the same underlying Transformer model is shared.\n\n(3) More robust to noise: If noise is dominant in one or several series, this noise will be projected to other series in the embedding space if we mix channels. Channel independence can mitigate this problem by only retaining the noise in these noisy channels. We can further alleviate the noise by introducing smaller weights to the objective losses that associate with noisy channels.\n\n\nA.7.2 PERFORMANCE OF CHANNEL-INDEPENDENCE ON OTHER MODELS\n\nTo show that channel-independence is a general technique that can be applied to the other models, we apply it to Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021), andFEDformer (Zhou et al., 2022). The results are shown in Table 15. The channel-independent technique can improve the forecasting performance of those models generally. Although they are still not able to outperform PatchTST which is based on vanilla attention mechanism, we believe that more performance boost and computational reduction can be obtained with more advanced attention designs incorporating the channel-independence architecture.  Here, train size denotes the fraction of the training data that is used to learn the model from scratch. Channelindependence contributes to a quicker convergence as more training data is available. Right Panel:\n\nTest loss vs epochs. Here, we use full train data and plot the first 20 epochs. Channel-mixing model quickly overfits the data.\n\nFigure 1 :\n1PatchTST architecture. (a) Multivariate time series data is divided into different channels. They share the same Transformer backbone, but the forward processes are independent. (b)\n\n\nR P \u00d7N where N is the number of patches, N = (L\u2212P ) S + 2. Here, we pad S repeated numbers of the last value x (i)\n\nTable 7 :\n7Ablation study of patching and channel-independence in PatchTST. 4 cases are included: (a) both patching and channel-independence are included in model (P+CI); (b) only channelindependence (CI); (c) only patching (P); (d) neither of them is included (Original TST model). PatchTST means supervised PatchTST/42. '-' in table means the model runs out of GPU memory (NVIDIA A40 48GB) even with batch size 1. The best results are in bold.\n\nFigure 2 :\n2Forecasting performance (MSE) with varying look-back windows on 3 large datasets: Electricity, Traffic, and Weather. The look-back windows are selected to be L = 24, 48, 96, 192, 336, 720, and the prediction horizons are T = 96, 720. We use supervised PatchTST/42 and other open-source Transformer-based baselines for this experiment.\n\n\n, PatchTST contains 3 encoder layers with head number H = 16 and dimension of latent space D = 128. The feed forward network in Transformer encoder block consists of 2 linear layers with GELU (Hendrycks & Gimpel, 2016) activation function: one projecting the hidden representation D = 128 to a new dimension F = 256, and another layer that project it back to D = 128. For very small datasets (ILI, ETTh1, ETTh2), a reduced size of parameters is used (H = 4, D = 16 and F = 128) to mitigate the possible overfitting. Dropout with probability 0.2 is applied in the encoders for all experiments. The code will be publicly available. A.1.5 IMPLEMENTATION DETAILS\n\nFigure 3 :\n3Visualization of 192-step forecasting on {Weather, Traffic} datasets with the look-back window L = 336 and 60-step forecasting on ILI dataset with L = 104. PatchTST (in red) can capture the trend and closest to the ground truth (in blue).\n\nFigure 4 :\n4MSE scores with varying patch lengths P = [\n\nFigure 5 :\n5MSE scores with varying model parameters. Each bar indicates the MSE score of a parameter combination. The combinations (L, D) = (3, 128), (3, 256), (4, 128), (4, 256), (5, 128), (5, 256) are orderly labeled from 1 to 6 in the figure. The model is run with supervised PatchTST/42 to forecast 96 steps. For Traffic and Electricity datasets, we reduce the maximum number of epochs to 50 to save computational time. A.7 CHANNEL-INDEPENDENCE ANALYSIS\n\nFigure 6 : 23 Figure 7 :\n6237Attention maps and the forecasting of a few time series from Electricity dataset run with supervised PatchTST/64. Attention map is calculated by averaging the attention matrices over all the heads and across all the layers. For each time series, we show the attention map and the prediction in orange. The blue curves are the actual data. The curves before the back lines are the actual input data. Channel-independence design allow each series to learn its own attention map for forecasting in which the pattern can be more similar for more correlated series and different otherwise. Channel-independence vs channel-mixing on Weather dataset. The base model is PatchTST/42, and the prediction length is 96. We plot the mean values and error bars with 5 different random seeds:{2019, 2020, 2021, 2022, 2023}. Left Panel: Test loss vs train size.\n\n\nTable 1: A case study of multivariate time series forecasting on Traffic dataset. The prediction horizon is 96. Results with different look-back window L and number of input tokens N are reported.Models \nL \nN \npatch \nmethod \nMSE \n\nChannel-independent \nPatchTST \n\n96 \n96 \n0.518 \n380 96 \ndown-sampled 0.447 \n336 336 \n0.397 \n336 42 \n0.367 \n336 42 \nself-supervised 0.349 \n\nChannel-mixing FEDFormer 336 336 \n0.597 \nDLinear \n336 336 \n0.410 \n\nRunning time (s) with L = 336 \nDataset \nw. patch w.o. patch Gain \n\nTraffic \n464 \n10040 \nx 22 \nElectricity \n300 \n5730 \nx 19 \nWeather \n156 \n680 \nx 4 \n\n\n\n\nTransformer-based Long-term Time Series Forecasting. There is a large body of work that tries to apply Transformer models to forecast long-term time series in recent years. We here summarize some of them. LogTrans(Li et al., 2019) uses convolutional self-attention layers with LogSparse design to capture local information and reduce the space complexity. Informer (Zhou et al., 2021) proposes a ProbSparse self-attention with distilling techniques to extract the most important keys efficiently. Autoformer (Wu et al., 2021) borrows the ideas of decomposition and auto-correlation from traditional time series analysis methods. FEDformer (Zhou et al., 2022) uses Fourier enhanced structure to get a linear complexity. Pyraformer (Liu et al., 2022) applies pyramidal attention module with inter-scale and intra-scale connections which also get a linear complexity.\n\n\nwhere each of them is fed independently into the Transformer backbone according toInput Univariate Series \n\nInstance Norm + Patching \n\n\u2208 \u211d !\u00d7# \n\n\n\nTable 2 :\n2Statistics of popular datasets for benchmark. All of the models are following the same experimental setup with prediction length T \u2208 {24, 36, 48, 60} for ILI dataset and T \u2208 {96, 192, 336, 720} for other datasets as in the original papers. We collect baseline results from Zeng et al.(2022)with the default look-back window L = 96 for Transformer-based models, and L = 336 for DLinear. But in order to avoid under-estimating the baselines, we also run FEDformer, Autoformer and Informer for six different look-back window L \u2208{24, 48, 96, 192, 336, 720}, and always choose the best results to create strong baselines. More details about the baselines could be found in Appendix A.1.2. We calculate the MSE and MAE of multivariate time series forecasting as metrics.Baselines and Experimental Settings. We choose the SOTA Transformer-based models, includ-\ning FEDformer (Zhou et al., 2022), Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021), \nPyraformer (Liu et al., 2022), LogTrans (Li et al., 2019), and a recent non-Transformer-based model \nDLinear (Zeng et al., 2022) as our baselines. \n\n\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEModels \nPatchTST/64 \nPatchTST/42 \nDLinear \nFEDformer \nAutoformer \nInformer \nPyraformer \nLogTrans \nMetric \nMSE MAE \n\nTable 3 :\n3Multivariate long-term forecasting results with supervised PatchTST. We use prediction lengths T \u2208 {24, 36, 48, 60} for ILI dataset and T \u2208 {96, 192, 336, 720} for the others. The best results are in bold and the second best are underlined.Models \nPatchTST \nDLinear \nFEDformer \nAutoformer \nInformer \nFine-tuning \nLin. Prob. \nSup. \nMetric \nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE \nWeather \n96 \n\nTable 4 :\n4Multivariate long-term forecasting results with self-supervised PatchTST. We use prediction lengths T \u2208 {96, 192, 336, 720}. The best results are in bold and the second best are underlined.\n\n\nMAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE Weather 96Models \nPatchTST \nDLinear \nFEDformer \nAutoformer \nInformer \nFine-tuning \nLin. Prob. \nSup. \nMetric \nMSE \n\nTable 5 :\n5Transfer learning task: PatchTST is pre-trained on Electricity dataset and the model is transferred to other datasets. The best results are in bold and the second best are underlined.Models \nIMP. \nPatchTST \nBTSF \nTS2Vec \nTNC \nTS-TCC \nTransferred \nSelf-supervised \nMetrics \nMSE \nMSE MAE MSE \nMAE \nMSE MAE MSE MAE MSE MAE MSE MAE \n\nETTh1 \n\n24 42.3% 0.312 0.362 0.322 0.369 \n0.541 0.519 0.599 0.534 0.632 0.596 0.653 0.610 \n48 44.7% 0.339 0.378 0.354 0.385 \n0.613 0.524 0.629 0.555 0.705 0.688 0.720 0.693 \n168 34.5% 0.424 0.437 0.419 0.424 \n0.640 0.532 0.755 0.636 1.097 0.993 1.129 1.044 \n336 48.5% 0.472 0.472 0.445 0.446 \n0.864 0.689 0.907 0.717 1.454 0.919 1.492 1.076 \n720 48.8% 0.508 0.507 0.487 0.478 \n0.993 0.712 1.048 0.790 1.604 1.118 1.603 1.206 \n\n\n\nTable 6 :\n6Representation learning methods comparison. Column name transferred implies pretraining PatchTST on Traffic dataset and transferring the representation to ETTh1, while selfsupervised implies both pre-training and linear probing on ETTh1. The best and second best results are in bold and underlined. IMP. denotes the improvement on best results of PatchTST compared to that of baselines, which is in the range of 34.5% to 48.8% on various prediction lengths. models. Note that as opposed to supervised PatchTST where the entire model is trained for each prediction horizon, here we only retrain the linear head or the entire model for much fewer epochs, which results in significant computational time reduction.Comparison with Other Self-supervised Methods. We compare our self-supervised model with \nBTSF (Yang & Hong, 2022), TS2Vec (Yue et al., 2022), TNC (Tonekaboni et al., 2021), and TS-\nTCC \n\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross B. Girshick. Masked autoencoders are scalable vision learners. CoRR, abs/2111.06377, 2021. URL https://arxiv. org/abs/2111.06377. Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM Computing Surveys (CSUR), 2021. Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=cGDAkQo1C0p. Pedro Lara-Ben\u00edtez, Manuel Carranza-Garc\u00eda, and Jos\u00e9 C Riquelme. An experimental review on deep learning architectures for time series forecasting. International Journal of Neural Systems, Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 6775a0635c302542da2c32aa19d86be0-Paper.pdf. Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for time series with temporal neighborhood coding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=8qDwejCuCN. Jos\u00e9 F Torres, Dalil Hadjout, Abderrazak Sebaa, Francisco Mart\u00ednez-\u00c1lvarez, and Alicia Troncoso. Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8980-8987, 2022. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? arXiv preprint arXiv:2205.13504, 2022. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, volume 35, pp. 11106-11115, 2021.Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint \narXiv:1606.08415, 2016. \n\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 1997. \n\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, \nand Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked \nprediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, \n29:3451-3460, 2021. \n\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha. Ammus: A sur-\nvey of transformer-based pretrained models in natural language processing. arXiv preprint \narXiv:2108.05542, 2021. \n\nShigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, \nMasao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al. A compar-\native study on transformer vs rnn in speech applications. In IEEE Automatic Speech Recognition \nand Understanding Workshop (ASRU), pp. 449-456. IEEE, 2019. \n\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-\ntuning can distort pretrained features and underperform out-of-distribution. In International Con-\nference on Learning Representations, 2022. URL https://openreview.net/forum? \nid=UYneFzXSJWh. \n\n31(03):2130001, 2021. \n\nShizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. \nPyraformer: Low-complexity pyramidal attention for long-range time series modeling and fore-\ncasting. In International Conference on Learning Representations, 2022. \n\nBarbara Rossi. Exchange rate predictability. Journal of Economic Literature, 51(4):1063-1119, De-\ncember 2013. doi: 10.1257/jel.51.4.1063. URL https://www.aeaweb.org/articles? \nid=10.1257/jel.51.4.1063. \n\nDavid Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic fore-\ncasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181-\n1191, 2020. \n\nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In IEEE International \nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5149-5152, 2012. doi: \n10.1109/ICASSP.2012.6289079. \nDeep learning for time series forecasting: a survey. Big Data, 9(1):3-21, 2021. \n\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-\ngredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. \n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, volume 30, 2017. URL https://proceedings.neurips. \ncc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. \n\nQingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. \nTransformers in time series: A survey. arXiv preprint arXiv:2202.07125, 2022. \n\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transform-\ners with Auto-Correlation for long-term series forecasting. In Advances in Neural Information \nProcessing Systems, 2021. \n\nLing Yang and Shenda Hong. Unsupervised time-series representation learning with iterative bilin-\near temporal-spectral fusion. In ICML, 2022. \n\nGeorge Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eick-\nhoff. A transformer-based framework for multivariate time series representation learning. In \nProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. \n2114-2124, 2021. \n\nYi Zheng, Qi Liu, Enhong Chen, Yong Ge, and J Leon Zhao. Time series classification using multi-\nchannels deep convolutional neural networks. In International conference on web-age information \nmanagement, pp. 298-310. Springer, 2014. \n\nTian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency \nenhanced decomposed transformer for long-term series forecasting. In Proc. 39th International \nConference on Machine Learning, 2022. \nA APPENDIX \n\nA.1 EXPERIMENTAL DETAILS \n\nA.1.1 DATASETS \n\nWe use 8 popular multivariate datasets provided in (Wu et al., 2021) for forecasting and representa-\ntion learning. Weather 3 dataset collects 21 meteorological indicators in Germany, such as humidity \nand air temperature. Traffic 4 dataset records the road occupancy rates from different sensors on San \nFrancisco freeways. Electricity 5 is a dataset that describes 321 customers' hourly electricity con-\nsumption. ILI 6 dataset collects the number of patients and influenza-like illness ratio in a weekly \nfrequency. ETT 7 (Electricity Transformer Temperature) datasets are collected from two different \nelectric transformers labeled with 1 and 2, and each of them contains 2 different resolutions (15 \nminutes and 1 hour) denoted with m and h. Thus, in total we have 4 ETT datasets: ETTm1, ETTm2, \nETTh1, and ETTh2. \n\n\n\nTable 8 :\n8Univariate long-term forecasting results with supervised PatchTST. ETT datasets are used \nwith prediction lengths T \u2208 {96, 192, 336, 720}. The best results are in bold. \n\n\n\nTable 9 :\n9Multivariate long-term forecasting results with varying look-back window L in supervised PatchTST/42.\u2022 Neither patching nor channel-independence is included (Original), which is just the original \nTST model Zerveas et al. (2021). \n\n\n\nTable 10 :\n10Ablation study of patching (P) and channel-independence (CI) in PatchTST/42. A full \nbenchmark regarding Table 7. The best results are in bold. '-' in table means the model runs out of \nGPU memory (NVIDIA A40 48GB) even with batch size 1. \n\n\n\nA.6 ROBUSTNESS ANALYSIS A.6.1 RESULTS WITH DIFFERENT RANDOM SEEDS The results reported in the main text and appendix above are run with the fixed random seed 2021. To examine the robustness of our results, we train the supervised PatchTST model with 5 different random seeds: 2019, 2020, 2021, 2022, 2023 and calculate the MSE and MAE scores with each selected seed. The mean and standard derivation of the results are reported inTable 14. It is clear that the variances are considerably small which indicates the robustness against choice of random seeds of our model.\n\nTable 13 :\n13Transfer learning task: PatchTST is pre-trained on Electricity dataset and the model is transferred to other datasets. A full benchmark regardingTable 13. The best results are in bold.A.6.2 RESULTS WITH DIFFERENT MODEL PARAMETERSTo see whether PatchTST is sensitive to the choice of Transformer settings, we perform another experiments with varying model parameters. We vary the number of Transformer layers L = {3, 4, 5} and select the model dimension D = {128, 256} while the inner-layer of the feed forward network is F = 2D. In total, there are 6 different sets of model hyper-parameters to examine.Figure 5shows the MSE scores of these combinations on different datasets. Except ILI dataset reveals high variance with different hyper-parameter settings, other datasets are robust to the choice of model hyper-parameters.\n\nTable 14 :\n14Multivariate long-term forecasting results with different random seeds in supervised and self-supervised PatchTST/42. The best results are in bold.1 2 3 4 5 6 \nParameter Combination \n\n0.354 \n0.362 \n0.37 \n0.378 \n0.386 \n\nMSE \n\netth1 \n\n1 2 3 4 5 6 \nParameter Combination \n\n0.27 \n\n0.28 \n\n0.29 \n\n0.3 \netth2 \n\n1 2 3 4 5 6 \nParameter Combination \n\n0.135 \n0.14 \n0.145 \n0.15 \nweather \n\n1 2 3 4 5 6 \nParameter Combination \n\n2.0 \n2.1 \n2.2 \n2.3 \nillness \n\n1 2 3 4 5 6 \nParameter Combination \n\n0.27 \n\n0.28 \n\n0.29 \n\nMSE \n\nettm1 \n\n1 2 3 4 5 6 \nParameter Combination \n\n0.15 \n0.155 \n0.16 \n0.165 \n0.17 \nettm2 \n\n1 2 3 4 5 6 \nParameter Combination \n\n0.37 \n\n0.38 \n\ntraffic \n\n1 2 3 4 5 6 \nParameter Combination \n\n0.115 \n0.12 \n0.125 \n0.13 \nelectricity \n\n\n Zerveas et al. (2021)  has shown that BatchNorm outperforms LayerNorm in time series Transformer.\nWe cite results of TS2Vec from (Yue et al., 2022) and {BTSF,TNC,TS-TCC} from (Yang & Hong, 2022).\nhttps://www.bgc-jena.mpg.de/wetter/ 4 https://pems.dot.ca.gov/ 5 https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 6 https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html 7 https://github.com/zhouhaoyi/ETDataset 8 https://github.com/laiguokun/multivariate-time-series-data\n\nAbdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Alexei Baevski, Yuhao Zhou, Advances in Neural Information Processing Systems. 33Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A frame- work for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33:12449-12460, 2020.\n\nAn empirical evaluation of generic convolutional and recurrent networks for sequence modeling. Shaojie Bai, Zico Kolter, Vladlen Koltun, arXiv:1803.01271arXiv preprintShaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\n\nBEiT: BERT pre-training of image transformers. Hangbo Bao, Li Dong, Songhao Piao, Furu Wei, International Conference on Learning Representations. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image trans- formers. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=p-BhZSz59o4.\n\nRishi Bommasani, A Drew, Ehsan Hudson, Russ Adeli, Simran Altman, Arora, Sydney Von Arx, S Michael, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. arXiv preprintRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu- nities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n\nTime series analysis, forecasting and control. G E P Box, Gwilym M Jenkins, G. E. P. Box and Gwilym M. Jenkins. Time series analysis, forecasting and control. 1970.\n\nTime-series forecasting with deep learning: a survey. Lim Bryan, Zohren Stefan, 10.1098/rsta.2020.0209Phil. Trans. R. Soc. A. Lim Bryan and Zohren Stefan. Time-series forecasting with deep learning: a survey. Phil. Trans. R. Soc. A, 2021. doi: 10.1098/rsta.2020.0209.\n\nSpectral temporal graph neural network for multivariate timeseries forecasting. Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, Advances in neural information processing systems. 33Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bix- iong Xu, Jing Bai, Jie Tong, et al. Spectral temporal graph neural network for multivariate time- series forecasting. Advances in neural information processing systems, 33:17766-17778, 2020.\n\nLearning to rotate: Quaternion transformer for complicated periodical time series forecasting. Weiqi Chen, Wenwei Wang, Bingqing Peng, Qingsong Wen, Tian Zhou, Liang Sun, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data MiningWeiqi Chen, Wenwei Wang, Bingqing Peng, Qingsong Wen, Tian Zhou, and Liang Sun. Learning to rotate: Quaternion transformer for complicated periodical time series forecasting. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 146-156, 2022.\n\nZ-gcnets: time zigzags at graph convolutional networks for time series forecasting. Yuzhou Chen, Ignacio Segovia, Yulia R Gel, International Conference on Machine Learning. PMLRYuzhou Chen, Ignacio Segovia, and Yulia R Gel. Z-gcnets: time zigzags at graph convolutional networks for time series forecasting. In International Conference on Machine Learning, pp. 1684-1694. PMLR, 2021.\n\nTriformer: Triangular, variable-specific attentions for long sequence multivariate time series forecasting. Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, Shirui Pan, 10.24963/ijcai.2022/277Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, and Shirui Pan. Triformer: Triangular, variable-specific attentions for long sequence multivariate time series forecasting. In Proceedings of the Thirty-First International Joint Conference on Artificial In- telligence, IJCAI-22, pp. 1994-2001, 7 2022. doi: 10.24963/ijcai.2022/277. URL https: //doi.org/10.24963/ijcai.2022/277.\n\nBERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, abs/1810.04805CoRRJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805.\n\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, International Conference on Learning Representations. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko- reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni- tion at scale. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy.\n\nTime-series representation learning via temporal and contextual contrasting. Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Xiaoli Chee Keong Kwoh, Cuntai Li, Guan, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 2352-2359, 2021.\n\nEfficient capital markets: A review of theory and empirical work. F Eugene, Fama, The journal of Finance. 252Eugene F Fama. Efficient capital markets: A review of theory and empirical work. The journal of Finance, 25(2):383-417, 1970.\n\nUnsupervised scalable representation learning for multivariate time series. Jean-Yves Franceschi, Aymeric Dieuleveut, Martin Jaggi, Advances in Neural Information Processing Systems. 32Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. In Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 53c6de78244e9f528eb3e1cda69699bb-Paper.pdf.\n\n. 96 0.766 0.419 0.671 0.381 0.477 0.305 0.401 0.267 0.367 0.251 0.365 0.251 192 0.725 0.398 0.616 0.356 0.471 0.299 0.406 0.268 0.385 0.259 0.382 0.258 336 0.752 0.410 0.635 0.364 0.485 0.305 0.421 0.277 0.398 0.265 0.398 0.267 720 0.786 0.427 0.673 0.383 0.518 0.325 0.452 0.297 0.434 0.287 0.436 0.289Traffic. Traffic 96 0.766 0.419 0.671 0.381 0.477 0.305 0.401 0.267 0.367 0.251 0.365 0.251 192 0.725 0.398 0.616 0.356 0.471 0.299 0.406 0.268 0.385 0.259 0.382 0.258 336 0.752 0.410 0.635 0.364 0.485 0.305 0.421 0.277 0.398 0.265 0.398 0.267 720 0.786 0.427 0.673 0.383 0.518 0.325 0.452 0.297 0.434 0.287 0.436 0.289\n\nModels PatchTST DLinear FEDformer Autoformer Informer Fine-tuning Lin. Mae Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae, Prob. Sup. Metric MSE. Models PatchTST DLinear FEDformer Autoformer Informer Fine-tuning Lin. Prob. Sup. Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n\nMultivariate long-term forecasting results with self-supervised PatchTST. An full benchmark regarding Table 4. The best results are in bold. Models PatchTST DLinear FEDformer Autoformer Informer Fine-tuning Lin. Mae Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae Mse Mae, Prob. Sup. Metric MSE. 12TableTable 12: Multivariate long-term forecasting results with self-supervised PatchTST. An full bench- mark regarding Table 4. The best results are in bold. Models PatchTST DLinear FEDformer Autoformer Informer Fine-tuning Lin. Prob. Sup. Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n\n. 96 0.3669\u00b10.0006 0.2504\u00b10.0007 0.3528\u00b10.0022 0.2443\u00b10.0016 192 0.3858\u00b10.0004 0.2586\u00b10.0004 0.3729\u00b10.0013 0.2531\u00b10.0009 336 0.3994\u00b10.0010 0.2672\u00b10.0016 0.3846\u00b10.0020 0.2588\u00b10.0011 720 0.4383\u00b10.0097 0.2913\u00b10.0104 0.4241\u00b10.0007 0.2816\u00b10.0010Traffic. Traffic 96 0.3669\u00b10.0006 0.2504\u00b10.0007 0.3528\u00b10.0022 0.2443\u00b10.0016 192 0.3858\u00b10.0004 0.2586\u00b10.0004 0.3729\u00b10.0013 0.2531\u00b10.0009 336 0.3994\u00b10.0010 0.2672\u00b10.0016 0.3846\u00b10.0020 0.2588\u00b10.0011 720 0.4383\u00b10.0097 0.2913\u00b10.0104 0.4241\u00b10.0007 0.2816\u00b10.0010\n\nBaselines without CI are cited from Zeng et al. (2022). The better results between CI and non-CI versions are in bold. PatchTST/42 is placed on the left for easy reference to other CI-based models. '-' denotes running out of GPU memory even with batch size 1, or exceeding the maximum running time. Table. 15Channel-independence for other models. 12 hoursTable 15: Channel-independence for other models. CI denote channel-independence. Baselines without CI are cited from Zeng et al. (2022). The better results between CI and non-CI versions are in bold. PatchTST/42 is placed on the left for easy reference to other CI-based models. '-' denotes running out of GPU memory even with batch size 1, or exceeding the maximum running time (12 hours).\n", "annotations": {"author": "[{\"end\":127,\"start\":76},{\"end\":175,\"start\":128},{\"end\":232,\"start\":176},{\"end\":284,\"start\":233}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":81},{\"end\":140,\"start\":134},{\"end\":194,\"start\":186},{\"end\":250,\"start\":240}]", "author_first_name": "[{\"end\":80,\"start\":76},{\"end\":131,\"start\":128},{\"end\":133,\"start\":132},{\"end\":185,\"start\":176},{\"end\":239,\"start\":233}]", "author_affiliation": "[{\"end\":126,\"start\":105},{\"end\":174,\"start\":161},{\"end\":231,\"start\":218},{\"end\":283,\"start\":270}]", "title": "[{\"end\":73,\"start\":1},{\"end\":357,\"start\":285}]", "venue": null, "abstract": "[{\"end\":1600,\"start\":403}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1828,\"start\":1806},{\"end\":1848,\"start\":1828},{\"end\":1874,\"start\":1848},{\"end\":2325,\"start\":2304},{\"end\":2367,\"start\":2348},{\"end\":2397,\"start\":2376},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2447,\"start\":2429},{\"end\":2645,\"start\":2626},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2680,\"start\":2658},{\"end\":2709,\"start\":2680},{\"end\":2937,\"start\":2919},{\"end\":4742,\"start\":4718},{\"end\":4780,\"start\":4761},{\"end\":6097,\"start\":6080},{\"end\":7283,\"start\":7264},{\"end\":8056,\"start\":8034},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8262,\"start\":8241},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8438,\"start\":8412},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8605,\"start\":8587},{\"end\":8647,\"start\":8630},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8825,\"start\":8803},{\"end\":8842,\"start\":8825},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9501,\"start\":9479},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10069,\"start\":10044},{\"end\":10093,\"start\":10069},{\"end\":10111,\"start\":10093},{\"end\":10128,\"start\":10111},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10237,\"start\":10213},{\"end\":10407,\"start\":10385},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10440,\"start\":10419},{\"end\":14257,\"start\":14235},{\"end\":14274,\"start\":14257},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15011,\"start\":14990},{\"end\":15036,\"start\":15016},{\"end\":15359,\"start\":15337},{\"end\":16040,\"start\":16019},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17981,\"start\":17964},{\"end\":20549,\"start\":20529},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21830,\"start\":21809},{\"end\":23404,\"start\":23385},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25267,\"start\":25255},{\"end\":25403,\"start\":25379},{\"end\":26242,\"start\":26215},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26948,\"start\":26927},{\"end\":27146,\"start\":27114},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27170,\"start\":27152},{\"end\":27204,\"start\":27175},{\"end\":27337,\"start\":27318},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":27353,\"start\":27337},{\"end\":29422,\"start\":29419},{\"end\":29424,\"start\":29422},{\"end\":29427,\"start\":29424},{\"end\":29430,\"start\":29427},{\"end\":29433,\"start\":29430},{\"end\":29436,\"start\":29433},{\"end\":30002,\"start\":30000},{\"end\":30004,\"start\":30002},{\"end\":30006,\"start\":30004},{\"end\":30009,\"start\":30006},{\"end\":30012,\"start\":30009},{\"end\":30015,\"start\":30012},{\"end\":30018,\"start\":30015},{\"end\":30021,\"start\":30018},{\"end\":31976,\"start\":31958},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31994,\"start\":31976},{\"end\":32012,\"start\":31994},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37531,\"start\":37513},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38305,\"start\":38283},{\"end\":38334,\"start\":38305}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39283,\"start\":39089},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39400,\"start\":39284},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39847,\"start\":39401},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40195,\"start\":39848},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40856,\"start\":40196},{\"attributes\":{\"id\":\"fig_9\"},\"end\":41108,\"start\":40857},{\"attributes\":{\"id\":\"fig_10\"},\"end\":41165,\"start\":41109},{\"attributes\":{\"id\":\"fig_13\"},\"end\":41625,\"start\":41166},{\"attributes\":{\"id\":\"fig_14\"},\"end\":42501,\"start\":41626},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43089,\"start\":42502},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43956,\"start\":43090},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44104,\"start\":43957},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":45214,\"start\":44105},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45386,\"start\":45215},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45807,\"start\":45387},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":46009,\"start\":45808},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":46177,\"start\":46010},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":46947,\"start\":46178},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":47857,\"start\":46948},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":55071,\"start\":47858},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":55255,\"start\":55072},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":55500,\"start\":55256},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":55755,\"start\":55501},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":56327,\"start\":55756},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":57167,\"start\":56328},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":57913,\"start\":57168}]", "paragraph": "[{\"end\":2163,\"start\":1616},{\"end\":3287,\"start\":2165},{\"end\":3906,\"start\":3289},{\"end\":4097,\"start\":3908},{\"end\":4838,\"start\":4099},{\"end\":5000,\"start\":4840},{\"end\":5622,\"start\":5002},{\"end\":6992,\"start\":5624},{\"end\":7630,\"start\":6994},{\"end\":7970,\"start\":7632},{\"end\":9719,\"start\":7987},{\"end\":10488,\"start\":9721},{\"end\":10909,\"start\":10526},{\"end\":11009,\"start\":10911},{\"end\":11605,\"start\":11162},{\"end\":12545,\"start\":11746},{\"end\":12894,\"start\":12547},{\"end\":13114,\"start\":12955},{\"end\":13355,\"start\":13286},{\"end\":13761,\"start\":13471},{\"end\":14027,\"start\":13805},{\"end\":14492,\"start\":14082},{\"end\":15205,\"start\":14520},{\"end\":16178,\"start\":15207},{\"end\":16687,\"start\":16180},{\"end\":17353,\"start\":16689},{\"end\":17643,\"start\":17355},{\"end\":18274,\"start\":17695},{\"end\":18833,\"start\":18287},{\"end\":19464,\"start\":18835},{\"end\":21314,\"start\":19492},{\"end\":22332,\"start\":21316},{\"end\":23203,\"start\":22351},{\"end\":23868,\"start\":23205},{\"end\":24388,\"start\":23899},{\"end\":25628,\"start\":24390},{\"end\":26412,\"start\":25667},{\"end\":26760,\"start\":26414},{\"end\":27940,\"start\":26804},{\"end\":28612,\"start\":27942},{\"end\":29199,\"start\":28634},{\"end\":30087,\"start\":29267},{\"end\":30470,\"start\":30122},{\"end\":30648,\"start\":30514},{\"end\":30784,\"start\":30650},{\"end\":30928,\"start\":30786},{\"end\":31252,\"start\":30930},{\"end\":31825,\"start\":31254},{\"end\":32583,\"start\":31858},{\"end\":32644,\"start\":32585},{\"end\":33315,\"start\":32697},{\"end\":34767,\"start\":33361},{\"end\":34892,\"start\":34816},{\"end\":35741,\"start\":34894},{\"end\":36620,\"start\":35743},{\"end\":37145,\"start\":36622},{\"end\":37249,\"start\":37147},{\"end\":37532,\"start\":37251},{\"end\":37679,\"start\":37534},{\"end\":38067,\"start\":37681},{\"end\":38959,\"start\":38129},{\"end\":39088,\"start\":38961}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11139,\"start\":11010},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11687,\"start\":11606},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11745,\"start\":11687},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12954,\"start\":12895},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13285,\"start\":13115},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13470,\"start\":13356},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13804,\"start\":13762},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14081,\"start\":14028}]", "table_ref": "[{\"end\":4889,\"start\":4882},{\"end\":5439,\"start\":5432},{\"end\":5687,\"start\":5680},{\"end\":6914,\"start\":6907},{\"end\":7629,\"start\":7622},{\"end\":12544,\"start\":12537},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":18041,\"start\":18034},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":18358,\"start\":18351},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":18851,\"start\":18844},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":20700,\"start\":20693},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":21533,\"start\":21526},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":22154,\"start\":22147},{\"end\":22454,\"start\":22447},{\"end\":22857,\"start\":22850},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":28949,\"start\":28942},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":30189,\"start\":30182},{\"attributes\":{\"ref_id\":\"tab_19\"},\"end\":31327,\"start\":31319},{\"end\":31368,\"start\":31361},{\"end\":32264,\"start\":32256},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32818,\"start\":32810},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":32859,\"start\":32852},{\"end\":32873,\"start\":32865},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":33396,\"start\":33388},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":33682,\"start\":33675},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":34045,\"start\":34037},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36463,\"start\":36456},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":38369,\"start\":38361}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1614,\"start\":1602},{\"attributes\":{\"n\":\"2\"},\"end\":7985,\"start\":7973},{\"attributes\":{\"n\":\"3\"},\"end\":10506,\"start\":10491},{\"attributes\":{\"n\":\"3.1\"},\"end\":10524,\"start\":10509},{\"end\":11160,\"start\":11141},{\"attributes\":{\"n\":\"3.2\"},\"end\":14518,\"start\":14495},{\"attributes\":{\"n\":\"4\"},\"end\":17657,\"start\":17646},{\"attributes\":{\"n\":\"4.1\"},\"end\":17693,\"start\":17660},{\"end\":18285,\"start\":18277},{\"attributes\":{\"n\":\"4.2\"},\"end\":19490,\"start\":19467},{\"attributes\":{\"n\":\"4.3\"},\"end\":22349,\"start\":22335},{\"attributes\":{\"n\":\"5\"},\"end\":23897,\"start\":23871},{\"end\":25665,\"start\":25631},{\"end\":26802,\"start\":26763},{\"end\":28632,\"start\":28615},{\"end\":29236,\"start\":29202},{\"end\":29265,\"start\":29239},{\"end\":30120,\"start\":30090},{\"end\":30512,\"start\":30473},{\"end\":31856,\"start\":31828},{\"end\":32695,\"start\":32647},{\"end\":33359,\"start\":33318},{\"end\":34814,\"start\":34770},{\"end\":38127,\"start\":38070},{\"end\":39100,\"start\":39090},{\"end\":39411,\"start\":39402},{\"end\":39859,\"start\":39849},{\"end\":40868,\"start\":40858},{\"end\":41120,\"start\":41110},{\"end\":41177,\"start\":41167},{\"end\":41651,\"start\":41627},{\"end\":44115,\"start\":44106},{\"end\":45397,\"start\":45388},{\"end\":45818,\"start\":45809},{\"end\":46188,\"start\":46179},{\"end\":46958,\"start\":46949},{\"end\":55082,\"start\":55073},{\"end\":55266,\"start\":55257},{\"end\":55512,\"start\":55502},{\"end\":56339,\"start\":56329},{\"end\":57179,\"start\":57169}]", "table": "[{\"end\":43089,\"start\":42700},{\"end\":44104,\"start\":44041},{\"end\":45214,\"start\":44881},{\"end\":45386,\"start\":45272},{\"end\":45807,\"start\":45639},{\"end\":46177,\"start\":46074},{\"end\":46947,\"start\":46373},{\"end\":47857,\"start\":47671},{\"end\":55071,\"start\":50096},{\"end\":55255,\"start\":55084},{\"end\":55500,\"start\":55369},{\"end\":55755,\"start\":55515},{\"end\":57913,\"start\":57329}]", "figure_caption": "[{\"end\":39283,\"start\":39102},{\"end\":39400,\"start\":39286},{\"end\":39847,\"start\":39413},{\"end\":40195,\"start\":39861},{\"end\":40856,\"start\":40198},{\"end\":41108,\"start\":40870},{\"end\":41165,\"start\":41122},{\"end\":41625,\"start\":41179},{\"end\":42501,\"start\":41656},{\"end\":42700,\"start\":42504},{\"end\":43956,\"start\":43092},{\"end\":44041,\"start\":43959},{\"end\":44881,\"start\":44117},{\"end\":45272,\"start\":45217},{\"end\":45639,\"start\":45399},{\"end\":46009,\"start\":45820},{\"end\":46074,\"start\":46012},{\"end\":46373,\"start\":46190},{\"end\":47671,\"start\":46960},{\"end\":50096,\"start\":47860},{\"end\":55369,\"start\":55268},{\"end\":56327,\"start\":55758},{\"end\":57167,\"start\":56342},{\"end\":57329,\"start\":57182}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10822,\"start\":10814},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13018,\"start\":13010},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13608,\"start\":13600},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16781,\"start\":16773},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23515,\"start\":23507},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28079,\"start\":28071},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":28738,\"start\":28730},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29198,\"start\":29179},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":29601,\"start\":29593},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":30263,\"start\":30255},{\"end\":35125,\"start\":35117},{\"end\":35335,\"start\":35327},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36315,\"start\":36307},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36775,\"start\":36767}]", "bib_author_first_name": "[{\"end\":58537,\"start\":58531},{\"end\":58552,\"start\":58547},{\"end\":58938,\"start\":58931},{\"end\":58948,\"start\":58944},{\"end\":58964,\"start\":58957},{\"end\":59239,\"start\":59233},{\"end\":59247,\"start\":59245},{\"end\":59261,\"start\":59254},{\"end\":59272,\"start\":59268},{\"end\":59549,\"start\":59544},{\"end\":59562,\"start\":59561},{\"end\":59574,\"start\":59569},{\"end\":59587,\"start\":59583},{\"end\":59601,\"start\":59595},{\"end\":59634,\"start\":59633},{\"end\":59653,\"start\":59644},{\"end\":59672,\"start\":59665},{\"end\":59683,\"start\":59679},{\"end\":60097,\"start\":60096},{\"end\":60101,\"start\":60098},{\"end\":60113,\"start\":60107},{\"end\":60115,\"start\":60114},{\"end\":60272,\"start\":60269},{\"end\":60286,\"start\":60280},{\"end\":60568,\"start\":60564},{\"end\":60580,\"start\":60574},{\"end\":60595,\"start\":60587},{\"end\":60604,\"start\":60602},{\"end\":60615,\"start\":60612},{\"end\":60628,\"start\":60621},{\"end\":60642,\"start\":60636},{\"end\":60656,\"start\":60649},{\"end\":60665,\"start\":60661},{\"end\":60674,\"start\":60671},{\"end\":61116,\"start\":61111},{\"end\":61129,\"start\":61123},{\"end\":61144,\"start\":61136},{\"end\":61159,\"start\":61151},{\"end\":61169,\"start\":61165},{\"end\":61181,\"start\":61176},{\"end\":61716,\"start\":61710},{\"end\":61730,\"start\":61723},{\"end\":61747,\"start\":61740},{\"end\":62133,\"start\":62119},{\"end\":62151,\"start\":62143},{\"end\":62160,\"start\":62157},{\"end\":62171,\"start\":62167},{\"end\":62184,\"start\":62178},{\"end\":62197,\"start\":62191},{\"end\":62900,\"start\":62895},{\"end\":62917,\"start\":62909},{\"end\":62931,\"start\":62925},{\"end\":62945,\"start\":62937},{\"end\":63306,\"start\":63300},{\"end\":63325,\"start\":63320},{\"end\":63342,\"start\":63333},{\"end\":63359,\"start\":63355},{\"end\":63380,\"start\":63373},{\"end\":63393,\"start\":63387},{\"end\":63414,\"start\":63407},{\"end\":63433,\"start\":63425},{\"end\":63449,\"start\":63444},{\"end\":63466,\"start\":63459},{\"end\":64017,\"start\":64007},{\"end\":64033,\"start\":64026},{\"end\":64049,\"start\":64041},{\"end\":64059,\"start\":64056},{\"end\":64070,\"start\":64064},{\"end\":64094,\"start\":64088},{\"end\":64653,\"start\":64652},{\"end\":64907,\"start\":64898},{\"end\":64927,\"start\":64920},{\"end\":64946,\"start\":64940},{\"end\":66054,\"start\":66007},{\"end\":66488,\"start\":66441}]", "bib_author_last_name": "[{\"end\":58545,\"start\":58538},{\"end\":58557,\"start\":58553},{\"end\":58942,\"start\":58939},{\"end\":58955,\"start\":58949},{\"end\":58971,\"start\":58965},{\"end\":59243,\"start\":59240},{\"end\":59252,\"start\":59248},{\"end\":59266,\"start\":59262},{\"end\":59276,\"start\":59273},{\"end\":59559,\"start\":59550},{\"end\":59567,\"start\":59563},{\"end\":59581,\"start\":59575},{\"end\":59593,\"start\":59588},{\"end\":59608,\"start\":59602},{\"end\":59615,\"start\":59610},{\"end\":59631,\"start\":59617},{\"end\":59642,\"start\":59635},{\"end\":59663,\"start\":59654},{\"end\":59677,\"start\":59673},{\"end\":59692,\"start\":59684},{\"end\":59703,\"start\":59694},{\"end\":60105,\"start\":60102},{\"end\":60123,\"start\":60116},{\"end\":60278,\"start\":60273},{\"end\":60293,\"start\":60287},{\"end\":60572,\"start\":60569},{\"end\":60585,\"start\":60581},{\"end\":60600,\"start\":60596},{\"end\":60610,\"start\":60605},{\"end\":60619,\"start\":60616},{\"end\":60634,\"start\":60629},{\"end\":60647,\"start\":60643},{\"end\":60659,\"start\":60657},{\"end\":60669,\"start\":60666},{\"end\":60679,\"start\":60675},{\"end\":61121,\"start\":61117},{\"end\":61134,\"start\":61130},{\"end\":61149,\"start\":61145},{\"end\":61163,\"start\":61160},{\"end\":61174,\"start\":61170},{\"end\":61185,\"start\":61182},{\"end\":61721,\"start\":61717},{\"end\":61738,\"start\":61731},{\"end\":61751,\"start\":61748},{\"end\":62141,\"start\":62134},{\"end\":62155,\"start\":62152},{\"end\":62165,\"start\":62161},{\"end\":62176,\"start\":62172},{\"end\":62189,\"start\":62185},{\"end\":62201,\"start\":62198},{\"end\":62907,\"start\":62901},{\"end\":62923,\"start\":62918},{\"end\":62935,\"start\":62932},{\"end\":62955,\"start\":62946},{\"end\":63318,\"start\":63307},{\"end\":63331,\"start\":63326},{\"end\":63353,\"start\":63343},{\"end\":63371,\"start\":63360},{\"end\":63385,\"start\":63381},{\"end\":63405,\"start\":63394},{\"end\":63423,\"start\":63415},{\"end\":63442,\"start\":63434},{\"end\":63457,\"start\":63450},{\"end\":63472,\"start\":63467},{\"end\":64024,\"start\":64018},{\"end\":64039,\"start\":64034},{\"end\":64054,\"start\":64050},{\"end\":64062,\"start\":64060},{\"end\":64086,\"start\":64071},{\"end\":64097,\"start\":64095},{\"end\":64103,\"start\":64099},{\"end\":64660,\"start\":64654},{\"end\":64666,\"start\":64662},{\"end\":64918,\"start\":64908},{\"end\":64938,\"start\":64928},{\"end\":64952,\"start\":64947},{\"end\":66058,\"start\":66055},{\"end\":66492,\"start\":66489}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":219966759},\"end\":58834,\"start\":58411},{\"attributes\":{\"doi\":\"arXiv:1803.01271\",\"id\":\"b1\"},\"end\":59184,\"start\":58836},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":235436185},\"end\":59542,\"start\":59186},{\"attributes\":{\"doi\":\"arXiv:2108.07258\",\"id\":\"b3\"},\"end\":60047,\"start\":59544},{\"attributes\":{\"id\":\"b4\"},\"end\":60213,\"start\":60049},{\"attributes\":{\"doi\":\"10.1098/rsta.2020.0209\",\"id\":\"b5\",\"matched_paper_id\":216562380},\"end\":60482,\"start\":60215},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":227276243},\"end\":61014,\"start\":60484},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":251518380},\"end\":61624,\"start\":61016},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":234335682},\"end\":62009,\"start\":61626},{\"attributes\":{\"doi\":\"10.24963/ijcai.2022/277\",\"id\":\"b9\",\"matched_paper_id\":248476444},\"end\":62811,\"start\":62011},{\"attributes\":{\"doi\":\"abs/1810.04805\",\"id\":\"b10\"},\"end\":63187,\"start\":62813},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":225039882},\"end\":63928,\"start\":63189},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":235658361},\"end\":64584,\"start\":63930},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":154899499},\"end\":64820,\"start\":64586},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":59413908},\"end\":65309,\"start\":64822},{\"attributes\":{\"doi\":\"96 0.766 0.419 0.671 0.381 0.477 0.305 0.401 0.267 0.367 0.251 0.365 0.251 192 0.725 0.398 0.616 0.356 0.471 0.299 0.406 0.268 0.385 0.259 0.382 0.258 336 0.752 0.410 0.635 0.364 0.485 0.305 0.421 0.277 0.398 0.265 0.398 0.267 720 0.786 0.427 0.673 0.383 0.518 0.325 0.452 0.297 0.434 0.287 0.436 0.289\",\"id\":\"b15\"},\"end\":65934,\"start\":65311},{\"attributes\":{\"id\":\"b16\"},\"end\":66227,\"start\":65936},{\"attributes\":{\"id\":\"b17\"},\"end\":66821,\"start\":66229},{\"attributes\":{\"doi\":\"96 0.3669\u00b10.0006 0.2504\u00b10.0007 0.3528\u00b10.0022 0.2443\u00b10.0016 192 0.3858\u00b10.0004 0.2586\u00b10.0004 0.3729\u00b10.0013 0.2531\u00b10.0009 336 0.3994\u00b10.0010 0.2672\u00b10.0016 0.3846\u00b10.0020 0.2588\u00b10.0011 720 0.4383\u00b10.0097 0.2913\u00b10.0104 0.4241\u00b10.0007 0.2816\u00b10.0010\",\"id\":\"b18\"},\"end\":67318,\"start\":66823},{\"attributes\":{\"id\":\"b19\"},\"end\":68065,\"start\":67320}]", "bib_title": "[{\"end\":58529,\"start\":58411},{\"end\":59231,\"start\":59186},{\"end\":60267,\"start\":60215},{\"end\":60562,\"start\":60484},{\"end\":61109,\"start\":61016},{\"end\":61708,\"start\":61626},{\"end\":62117,\"start\":62011},{\"end\":63298,\"start\":63189},{\"end\":64005,\"start\":63930},{\"end\":64650,\"start\":64586},{\"end\":64896,\"start\":64822},{\"end\":66005,\"start\":65936},{\"end\":66439,\"start\":66229},{\"end\":67617,\"start\":67320}]", "bib_author": "[{\"end\":58547,\"start\":58531},{\"end\":58559,\"start\":58547},{\"end\":58944,\"start\":58931},{\"end\":58957,\"start\":58944},{\"end\":58973,\"start\":58957},{\"end\":59245,\"start\":59233},{\"end\":59254,\"start\":59245},{\"end\":59268,\"start\":59254},{\"end\":59278,\"start\":59268},{\"end\":59561,\"start\":59544},{\"end\":59569,\"start\":59561},{\"end\":59583,\"start\":59569},{\"end\":59595,\"start\":59583},{\"end\":59610,\"start\":59595},{\"end\":59617,\"start\":59610},{\"end\":59633,\"start\":59617},{\"end\":59644,\"start\":59633},{\"end\":59665,\"start\":59644},{\"end\":59679,\"start\":59665},{\"end\":59694,\"start\":59679},{\"end\":59705,\"start\":59694},{\"end\":60107,\"start\":60096},{\"end\":60125,\"start\":60107},{\"end\":60280,\"start\":60269},{\"end\":60295,\"start\":60280},{\"end\":60574,\"start\":60564},{\"end\":60587,\"start\":60574},{\"end\":60602,\"start\":60587},{\"end\":60612,\"start\":60602},{\"end\":60621,\"start\":60612},{\"end\":60636,\"start\":60621},{\"end\":60649,\"start\":60636},{\"end\":60661,\"start\":60649},{\"end\":60671,\"start\":60661},{\"end\":60681,\"start\":60671},{\"end\":61123,\"start\":61111},{\"end\":61136,\"start\":61123},{\"end\":61151,\"start\":61136},{\"end\":61165,\"start\":61151},{\"end\":61176,\"start\":61165},{\"end\":61187,\"start\":61176},{\"end\":61723,\"start\":61710},{\"end\":61740,\"start\":61723},{\"end\":61753,\"start\":61740},{\"end\":62143,\"start\":62119},{\"end\":62157,\"start\":62143},{\"end\":62167,\"start\":62157},{\"end\":62178,\"start\":62167},{\"end\":62191,\"start\":62178},{\"end\":62203,\"start\":62191},{\"end\":62909,\"start\":62895},{\"end\":62925,\"start\":62909},{\"end\":62937,\"start\":62925},{\"end\":62957,\"start\":62937},{\"end\":63320,\"start\":63300},{\"end\":63333,\"start\":63320},{\"end\":63355,\"start\":63333},{\"end\":63373,\"start\":63355},{\"end\":63387,\"start\":63373},{\"end\":63407,\"start\":63387},{\"end\":63425,\"start\":63407},{\"end\":63444,\"start\":63425},{\"end\":63459,\"start\":63444},{\"end\":63474,\"start\":63459},{\"end\":64026,\"start\":64007},{\"end\":64041,\"start\":64026},{\"end\":64056,\"start\":64041},{\"end\":64064,\"start\":64056},{\"end\":64088,\"start\":64064},{\"end\":64099,\"start\":64088},{\"end\":64105,\"start\":64099},{\"end\":64662,\"start\":64652},{\"end\":64668,\"start\":64662},{\"end\":64920,\"start\":64898},{\"end\":64940,\"start\":64920},{\"end\":64954,\"start\":64940},{\"end\":66060,\"start\":66007},{\"end\":66494,\"start\":66441}]", "bib_venue": "[{\"end\":61342,\"start\":61273},{\"end\":62411,\"start\":62327},{\"end\":64284,\"start\":64203},{\"end\":58608,\"start\":58559},{\"end\":58929,\"start\":58836},{\"end\":59330,\"start\":59278},{\"end\":59772,\"start\":59721},{\"end\":60094,\"start\":60049},{\"end\":60339,\"start\":60317},{\"end\":60730,\"start\":60681},{\"end\":61271,\"start\":61187},{\"end\":61797,\"start\":61753},{\"end\":62325,\"start\":62226},{\"end\":62893,\"start\":62813},{\"end\":63526,\"start\":63474},{\"end\":64201,\"start\":64105},{\"end\":64690,\"start\":64668},{\"end\":65003,\"start\":64954},{\"end\":65622,\"start\":65615},{\"end\":66081,\"start\":66060},{\"end\":66515,\"start\":66494},{\"end\":67070,\"start\":67063},{\"end\":67624,\"start\":67619}]"}}}, "year": 2023, "month": 12, "day": 17}