{"id": 18698107, "updated": "2023-10-18 17:58:38.29", "metadata": {"title": "Arti\ufb01cial Neural Networks for Solving Ordinary and Partial Differential Equations", "authors": "[{\"first\":\"Isaac\",\"last\":\"Elias\",\"middle\":[]}]", "venue": "IEEE transactions on neural networks", "journal": "IEEE transactions on neural networks", "publication_date": {"year": 1998, "month": null, "day": null}, "abstract": "\u2014 We present a method to solve initial and boundary value problems using arti\ufb01cial neural networks. A trial solution of the differential equation is written as a sum of two parts. The \ufb01rst part satis\ufb01es the initial/boundary conditions and contains no adjustable parameters. The second part is constructed so as not to affect the initial/boundary conditions. This part involves a feedforward neural network containing adjustable parameters (the weights). Hence by construction the initial/boundary conditions are satis\ufb01ed and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ordinary differential equations (ODE\u2019s), to systems of coupled ODE\u2019s and also to partial differential equations (PDE\u2019s). In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galekrkin \ufb01nite element method for several cases of partial differential equations. With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.", "fields_of_study": "[\"Physics\"]", "external_ids": {"arxiv": "physics/9705023", "mag": "2158985775", "acl": null, "pubmed": "18255782", "pubmedcentral": null, "dblp": "journals/tnn/LagarisLF98", "doi": "10.1109/72.712178"}}, "content": {"source": {"pdf_hash": "6d6a83e67bda2e52ad1febcae13e671798c0c285", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/physics/9705023", "status": "GREEN"}}, "grobid": {"id": "8068272e2d0094cfd9955a14dfdc563f78ca9f37", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/6d6a83e67bda2e52ad1febcae13e671798c0c285.txt", "contents": "\n\n\n\nIsaac Elias \nDepartment of Computer Science\nUniversity of Ioannina\n45110IoanninaGRGreece\n\nthe University of Ioannina\nGreece\n\nMassa-chusetts Institute of Technology\nCambridge\n\nVisiting Researcher in RWTH\nAachenGermany\n\nMinnesota Supercomputer Institute and as Managing Director of Egnatia Epirus Foundation\nGreece\n4C63D9B2F982A9D59C602EFF03547B2F\nHe has many years of industrial and research experience in high-performance computing, computational medicine, and biomedical engineering.His current research interests include computational medicine and biomedical technology with emphasis on methods and instruments used for diagnosis and therapeutic reasons of various diseases.\n\nArtificial Neural Networks for Solving Ordinary and Partial Differential Equations\n\nIsaac Elias Lagaris, Aristidis Likas, Member, IEEE, and Dimitrios I. Fotiadis\n\nAbstract-We present a method to solve initial and boundary value problems using artificial neural networks.A trial solution of the differential equation is written as a sum of two parts.The first part satisfies the initial/boundary conditions and contains no adjustable parameters.The second part is constructed so as not to affect the initial/boundary conditions.This part involves a feedforward neural network containing adjustable parameters (the weights).Hence by construction the initial/boundary conditions are satisfied and the network is trained to satisfy the differential equation.The applicability of this approach ranges from single ordinary differential equations (ODE's), to systems of coupled ODE's and also to partial differential equations (PDE's).In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galekrkin finite element method for several cases of partial differential equations.With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.\n\nIndex Terms-Collocation method, finite elements, neural networks, neuroprocessors, ordinary differential equations, partial differential equations.\n\n\nI. INTRODUCTION\n\nM ANY methods have been developed so far for solving differential equations.Some of them produce a solution in the form of an array that contains the value of the solution at a selected group of points.Others use basis-functions to represent the solution in analytic form and transform the original problem usually to a system of algebraic equations.Most of the previous work in solving differential equations using neural networks is restricted to the case of solving the systems of algebraic equations which result from the discretization of the domain.The solution of a linear system of equations is mapped onto the architecture of a Hopfield neural network.The minimization of the network's energy function provides the solution to the system of equations [2], [6], [5].\n\nAnother approach to the solution of ordinary differential equations is based on the fact that certain types of splines, for instance -splines, can be derived by the superposition of piecewise linear activation functions [3], [4].The solution of a differential equation using -splines as basis functions, can be obtained by solving a system of linear or nonlinear equations in order to determine the coefficients of splines.Such a solution form is mapped directly on the architecture of a feedforward neural network by replacing each spline with the sum of piecewise linear activation functions that correspond to the hidden units.This method considers local basis-functions and in general requires many splines (and consequently network parameters) in order to yield accurate solutions.Furthermore, it is not easy to extend these techniques to multidimensional domains.\n\nIn this article we view the problem from a different angle.We present a method for solving both ordinary differential equations (ODE's) and partial differential equations (PDE's) (defined on orthogonal box domains) that relies on the function approximation capabilities of feedforward neural networks and results in the construction of a solution written in a differentiable, closed analytic form.This form employs a feedforward neural network as the basic approximation element, whose parameters (weights and biases) are adjusted to minimize an appropriate error function.To train the network we employ optimization techniques, which in turn require the computation of the gradient of the error with respect to the network parameters.In the proposed approach the model function is expressed as the sum of two terms: the first term satisfies the initial/boundary conditions and contains no adjustable parameters.The second term involves a feedforward neural network to be trained so as to satisfy the differential equation.Since it is known that a multilayer perceptron with one hidden layer can approximate any function to arbitrary accuracy, it is reasonable to consider this type of network architecture as a candidate model for treating differential equations.\n\nThe employment of a neural architecture adds many attractive features to the method:\n\n\u2022 The solution via ANN's is a differentiable, closed analytic form easily used in any subsequent calculation.Most other techniques offer a discrete solution (for example predictor-corrector, or Runge-Kutta methods) or a solution of limited differentiability (for example finite elements).\u2022 The method can be realized in hardware, using neuroprocessors, and hence offer the opportunity to tackle in real-time difficult differential equation problems arising in many engineering applications.\u2022 The method can also be efficiently implemented on parallel architectures.In the next section we describe the general formulation of the proposed approach and derive formulas for computing the gradient of the error function.Section III illustrates some classes of problems where the proposed method can be applied, and describes the appropriate form of the trial solution.Section IV presents numerical examples from the application of the technique to several test problems, and provides details concerning the implementation of the method and the accuracy of the obtained solution.We also make a comparison of our results with those obtained by the finite element method for the examined PDE problems.Finally, Section VI contains conclusions and directions for future research.\n\n\nII. DESCRIPTION OF THE METHOD\n\nThe proposed approach will be illustrated in terms of the following general differential equation definition:\n\n(1) subject to certain boundary conditions (BC's) (for instance Dirichlet and/or Neumann), where denotes the definition domain and is the solution to be computed.\n\nTo obtain a solution to the above differential equation, the collocation method is adopted [1] which assumes a discretization of the domain and its boundary into a set points and , respectively.The problem is then transformed into the following system of equations:\n\n(2) subject to the constraints imposed by the BC's.\n\nIf denotes a trial solution with adjustable parameters , the problem is transformed to (3) subject to the constraints imposed by the BC's.\n\nIn the proposed approach, the trial solution employs a feedforward neural network and the parameters correspond to the weights and biases of the neural architecture.We choose a form for the trial function such that by construction satisfies the BC's.This is achieved by writing it as a sum of two terms (4) where is a single-output feedforward neural network with parameters and input units fed with the input vector .\n\nThe term contains no adjustable parameters and satisfies the boundary conditions.The second term is constructed so as not to contribute to the BC's, since must also satisfy them.This term employs a neural network whose weights and biases are to be adjusted in order to deal with the minimization problem.Note at this point that the problem has been reduced from the original constrained optimization problem to an unconstrained one (which is much easier to handle) due to the choice of the form of the trial solution that satisfies by construction the BC's.\n\nIn the next section we present a systematic way to construct the trial solution, i.e., the functional forms of both and .We treat several common cases that one frequently encounters in various scientific fields.As indicated by our experiments, the approach based on the above formulation is very effective and provides in reasonable computing time accurate solutions with impressive generalization (interpolation) properties.\n\n\nA. Gradient Computation\n\nThe efficient minimization of (3) can be considered as a procedure of training the neural network, where the error corresponding to each input vector is the value which has to become zero.Computation of this error value involves not only the network output (as is the case in conventional training) but also the derivatives of the output with respect to any of its inputs.Therefore, in computing the gradient of the error with respect to the network weights, we need to compute not only the gradient of the network but also the gradient of the network derivatives with respect to its inputs.\n\nConsider a multilayer perceptron with input units, one hidden layer with sigmoid units and a linear output unit.The extension to the case of more than one hidden layers can be obtained accordingly.For a given input vector the output of the network is where denotes the weight from the input unit to the hidden unit denotes the weight from the hidden unit to the output, denotes the bias of hidden unit , and is the sigmoid transfer function.It is straightforward to show that (5) where and denotes the th-order derivative of the sigmoid.Moreover, it is readily verifiable that ( 6) where (7) and Equation (6) indicates that the derivative of the network with respect to any of its inputs is equivalent to a feedforward neural network with one hidden layer, having the same values for the weights and thresholds and with each weight being replaced with .Moreover, the transfer function of each hidden unit is replaced with the th-order derivative of the sigmoid.Therefore, the gradient of with respect to the parameters of the original network can be easily obtained as (8) (9) (10) Once the derivative of the error with respect to the network parameters has been defined it is then straightforward to employ almost any minimization technique.For example it is possible to use either the steepest descent (i.e., the backpropagation algorithm or any of its variants), or the conjugate gradient method or other techniques proposed in the literature.\n\nIn our experiments we have employed the quasi-Newton BFGS method [9] (independently proposed at 1970 by Broyden et al.) that is quadratically convergent and has demonstrated excellent performance.It must also be noted, that the derivatives of each network (or gradient network) with respect to the parameters for a given grid point may be obtained simultaneously in the case where parallel hardware is available.Moreover, in the case of backpropagation, the on-line or batch mode of weight updates may be employed.\n\n\nIII. ILLUSTRATION OF THE METHOD\n\n\nA. Solution of Single ODE's and Systems of Coupled ODE's\n\nTo illustrate the method, we consider the first-order ODE (11) with and the IC .A trial solution is written as (12) where is the output of a feedforward neural network with one input unit for and weights .Note that satisfies the IC by construction.The error quantity to be minimized is given by (13) where the 's are points in [0, 1].Since it is straightforward to compute the gradient of the error with respect to the parameters using ( 5)- (10).The same holds for all subsequent model problems.\n\nThe same procedure can be applied to the second-order ODE (14)\n\nFor the initial value problem: and , the trial solution can be cast as (15)\n\nFor the two point Dirichlet BC: and the trial solution is written as (16)\n\nIn the above two cases of second-order ODE's the error function to be minimized is given by the following equation:\n\n\nB. Solution of Single PDE's\n\nWe treat here two-dimensional problems only.However, it is straightforward to extend the method to more dimensions.For example, consider the Poisson equation\n\n\nIV. EXAMPLES\n\nIn this section we report on the solution of a number of model problems.In all cases we used a multilayer perceptron having one hidden layer with ten hidden units and one linear output unit.The sigmoid activation of each hidden unit is .For each test problem the exact analytic solution was known in advance.Therefore we test the accuracy of the obtained solutions by computing the deviation .To perform the error minimization we employed the Merlin/MCL 3.0 [7], [8] optimization package.From the several algorithms that are implemented therein, the quasi-Newton BFGS [9] method seemed to perform better in these problems and hence we used it in all of our experiments.A simple criterion for the gradient norm was used for termination.In order to illustrate the characteristics of the solutions provided by the neural method, we provide figures displaying the corresponding deviation both at the few points (training points) that were used for training and at many other points (test points) of the domain of each equation.The latter kind of figures are of major importance since they show the interpolation capabilities of the neural solutions which seem to be superior compared to other solutions.Moreover, in the case of ODE's we also consider points outside the training interval in order to obtain an estimate of the extrapolation performance of the obtained solution.\n\n\nA. ODE's and Systems of ODE's 1) Problem 1:\n\n(27) with and .The analytic solution is and is displayed in Fig. 1(a).According to (12) the trial neural form of the solution is taken to be .The network was trained using a grid of ten equidistant points in [0, 1].Fig. 2 displays the deviation from the exact solution corresponding at the grid points (diamonds) and the deviation at many other points  in [0, 1] as well as outside that interval (dashed line).It is clear that the solution is of high accuracy, although training was performed using a small number of points.Moreover, the extrapolation error remains low for points near the equation domain.\n\n\n2) Problem 2:\n\n(28) with and .The analytic solution is and is presented in Fig. 1(b).The trial neural form is according to (12).As before we used a grid of ten equidistant points in [0, 2] to perform the training.In analogy with the previous case, Fig. 3 displays the deviation at the grid points (diamonds) and at many other points inside and outside the training interval (dashed line).\n\n3) Problem 3: Given the differential equation From all the above cases it is clear that the method can handle effectively all kinds of ODE's and provide analytic solutions that retain the accuracy throughout the whole domain and not only at the training points.\n\n\nB. PDE's\n\nWe consider boundary value problems with Dirichlet and Neumann BC's.All subsequent problems were defined on the domain [0, 1] [0, 1] and in order to perform training we consider a mesh of 100 points obtained by considering ten equidistant points of the domain [0, 1] of each variable.In analogy with the previous cases the neural architecture was considered to be a MLP with two inputs (accepting the coordinates and of each point), ten sigmoid hidden units, and one linear output unit.\n\n1) Problem 5:\n\n(32)    form given by (23) Fig. 9 presents the deviation of the obtained solution at the 100 grid points that were selected for training while Fig. 10 displays the deviation at 900 other points of the equation domain.It clear that the solution is very accurate and the accuracy remains high at all points of the domain.\n\n2) Problem 6:\n\n(33) with and with mixed BC's: and .The analytic solution is and is presented in Fig. 11.The trial neural form is specified\n\n\nC. Comparison with Finite Elements\n\nThe above PDE problems were also solved with the finite element method which has been widely acknowledged as one of the most effective approaches to the solution of differential equations [10].The used Galerkin finite element method (GFEM) calls for the weighted residuals to vanish at each nodal position (36) where is given by ( 1) and is the Jacobian of the isoparametric mapping, the coordinates of the computational domain and the coordinates of the physical domain.This requirement along with the imposed boundary conditions constitute a set of nonlinear algebraic equations .The inner products involved in the finite element formulation  are computed using the nine-node Gaussian quadrature.The system of equations is solved for the nodal coefficients of the basis function expansion using the Newton's method forming the Jacobian of the system explicitly (for both linear and nonlinear differential operators).The initial guess is chosen at random.For linear problems convergence is achieved in one iteration and for nonlinear problems in one to five iterations.\n\nAll PDE problems 5-7 are solved on a rectangular domain of 18 18 elements resulting in a linear system with 1369 unknowns.This is in contrast with the neural approach which assumes a small number of parameters (30 for ODE's and 40 for PDE's), but requires more sophisticated minimization algorithms.As the number of employed elements increases the finite element approach requires an excessive number of parameters.This fact may lead to higher memory requirements particularly in the case of three or higher dimensional problems.\n\nIn the finite element case, interpolation is performed using a rectangular grid of 23 23 equidistant points (test points).It must be stressed that in the finite element case the solution is not expressed in closed analytical form as in the neural case, but additional interpolation computations are required in order to find the value of the solution at an arbitrary point in the domain.Figs. 14 and 15 display the deviation for PDE problem 6 at the training set and the interpolation set of points, respectively.Table I reports the maximum deviation corresponding to the neural and to the finite element method at the training and at the interpolation set of points for PDE problems 5-7.It is obvious that at the training points the solution of the finite element method is very satisfactory and in some cases it is better than that obtained using the neural method.It is also clear that the accuracy at the interpolation points is orders of magnitude lower as compared to that at the training points.On the contrary, the neural method provides solutions of excellent interpolation accuracy, since, as Table I indicates, the deviations at the training and at the interpolation points are comparable.It must also be stressed that the accuracy of the finite element method decreases as the grid becomes coarser, and that the neural approach considers a mesh of 10 10 points while in the finite element case a 18 18 mesh was employed.\n\nFig. 16 provides a plot of the logarithm of the interpolation error with respect to the number of parameters for the neural and the FEM case, respectively for the nonlinear problem 7. The number of parameters in the -axis is normalized, and in the neural case the actual number of parameters is 20 , while in the FEM case is 225 .It is clear that the neural method is superior and it is also obvious that the accuracy of the neural method can be controlled by increasing the number of hidden units.In what concerns execution time, the plots of Fig. 17 suggest that in the neural approach time increases linearly with the (normalized) number of parameters, while in the FEM case, time scales almost quadratically.It must be noted that our experiments have been carried out on a Sun Ultra Sparc workstation with 512Mb of main memory.\n\nFor a linear differential equation, accuracy control can be obtained using the following iterative improvement procedure [11]     while and all successive corrections as , The improved approximate solution is now .For MLP's with one hidden layer and a linear output, the sum of two (or more) of them can be written as a single network of the same type and having a number of hidden nodes equal to the sum of the hidden nodes of the parent networks.\n\nAs stated in the introduction, one of the attractive features of our approach is the possibility of effective parallel implementation.In the proposed approach the employement of neural networks makes the method attractive to parallelization.It is well-known that in neural network training the following types of parallelism have been identified: 1) Data parallelism, where the data set (grid points) is split into subsets each one assigned to a different processor and therefore the error values corresponding to different grid points can be computed simultaneously.2) Spatial parallelism, i.e., the outputs of the sigmoid units in the hidden layer are computed in parallel.This kind of parallelism is better exploited in the case where hardware implementations are used (neuroprocessors) and the speedup obtained is proportional to the number of hidden units.It is also possible to implement a combination of the above kinds of parallelism.\n\nIn the case of finite elements parallelism arises mainly in the solution of the linear system of equations.There are also approaches that exploit parallelism in the tasks of mesh generation and finite element construction.Parallelism in finite elements can be exploited mainly at a coarse grain level using general purpose multiprocessor architectures [12].In general it is much easier to exploit parallelism when using the neural method, since neural networks constitute models with intrinsic parallelization capabilities and various kinds of specialized harware have been developed to exploit this property.\n\n\nV. CONCLUSIONS AND FUTURE RESEARCH\n\nA method has been presented for solving differential equations defined on orthogonal box boundaries that relies upon the function approximation capabilities of feedforward neural networks and provides accurate and differentiable solutions in a closed analytic form.The success of the method can be attributed to two factors.The first is the employment of neural networks that are excellent function approximators and the second is the form of the trial solution that satisfies by construction the BC's and therefore the constrained optimization problem becomes a substantially simpler unconstrained one.\n\nUnlike most previous approaches, the method is general and can be applied to both ODE's and PDE's by constructing the appropriate form of the trial solution.As indicated by our experiments the method exhibits excellent generalization performance since the deviation at the test points was in no case greater than the maximum deviation at the training points.This is in contrast with the finite element method where the deviation at the testing points was significantly greater than the deviation at the training points.\n\nWe note that the neural architecture employed was fixed in all the experiments and we did not attempt to find optimal configurations or to consider architectures containing more than one hidden layers.A study of the effect of the neural architecture on the quality of the solution constitutes one of our research objectives.\n\nAnother issue that needs to be examined is related with the sampling of the grid points that are used for training.In the above experiments the grid was constructed in a simple way by considering equidistant points.It is expected that better results will be obtained in the case where the grid density will vary during training according to the corresponding error values.This means that it is possible to consider more training points at regions where the error values are higher.\n\nIt must also be stressed that the proposed method can easily be used for dealing with domains of higher dimensions (three or more).As the dimensionality increases, the number of training points becomes large.This fact becomes a serious problem for methods that consider local functions around each grid point since the required number of parameters becomes excessively large and, therefore, both memory and computation time requirements become extremely high.In the case of the neural method, the number of training parameters remains almost fixed as the problem dimensionality increases.The only effect on the computation time stems from the fact that each training pass requires the presentation of more points, i.e., the training set becomes larger.This problem can be tackled by considering either parallel implementations, or implementations on a neuroprocessor that can be embedded in a conventional machine and provide considerably better execution times.Such an implementation on neural hardware is one of our near future objectives, since it will permit the treatment of many difficult real-world problems.\n\nAnother important direction of research concerns differential equation problems defined on irregular boundaries.Such problems are very interesting and arise in many real engineering applications.Work is in progress to treat this kind of problems using a trial solution form employing a multilayer perceptron and a radial basis function network, where the latter is responsible for the satisfaction of the boundary conditions, while the former is used for minimizing the training error at the grid ponts.Initial experimental results are very promising.\n\nMoreover, we have already applied our approach to other types of problems of similar nature, as for example eigenvalue problems for differential operators.More specifically, we have considered eigenvalue problems arising in the field of quantum mechanics (solution of the Schrondinger equation) and obtained very accurate results [13].\n\n\n\n(17) For systems of first-order ODE's (18) with we consider one neural network for each trial solution which is written as (19) and we minimize the following error quantity: (20)\n\n\nFig. 1 .\n1\nFig. 1.Exact solutions of ODE problems 1 and 2.\n\n\nFig. 2 .\n2\nFig. 2. Problem 1: Accuracy of the computed solution.\n\n\nFig. 3 .\n3\nFig. 3. Problem 2: Accuracy of the computed solution.\n\n\nFig. 4 .\n4\nFig. 4. Problem 3 with initial conditions: Accuracy of the computed solution.\n\n\n\n\ninitial value problem: and with .The exact solution is and the trial neural form is [from (15)].Consider also the boundary value problem: and .The exact solution is the same as above, but the appropriate trial neural form is [from (16)].Again, as before, we used a grid of ten equidistant points and the plots of the deviation from the exact solution are displayed at Figs. 4 and 5 for the initial value and boundary value problem, respectively.The interpretation of the figures is the same as in the previous cases.\n\n\n4) Problem 4 :\n4\nConsider the system of two coupled firstorder ODEat Fig. 6(a) and (b), respectively.Following (19) the trial neural solutions are and where the networks and have the same architecture as in the previous cases.Results concerning the accuracy of the obtained solutions at the grid points (diamonds and crosses) and at many other points (dashed line) are presented in Fig. 7(a) and (b) for the functions and , respectively.\n\n\nFig. 5 .\n5\nFig. 5. Problem 3 with boundary conditions: Accuracy of the computed solution.\n\n\nFig. 6 .\n6\nFig. 6.Exact solutions of the system of coupled ODE's (problem 4).\n\n\nFig. 7 .\n7\nFig. 7. Problem 4: Accuracy of the computed solutions.\n\n\nFig. 8 .\n8\nFig. 8. Exact solution of PDE problem 5.\n\n\nFig. 9 .\n9\nFig. 9. Problem 5: Accuracy of the computed solution at the training points.\n\n\nFig. 10 .\n10\nFig. 10.Problem 5: Accuracy of the computed solution at the test points.\n\n\nFig. 11 .\n11\nFig. 11.Exact solution of PDE problems 6 and 7.\n\n\nFig. 12 .\n12\nFig. 12. Problem 6: Accuracy of the computed solution at the training points.\n\n\nFig. 13 .\n13\nFig. 13.Problem 6.: Accuracy of the computed solution at the test points.\n\n\nFig. 14 .\n14\nFig. 14.Problem 7: Accuracy of the FEM solution at the training points.\n\n\nFig. 15 .\n15\nFig. 15.Problem 7: Accuracy of the FEM solution at the test points.\n\n\nFig. 16 .\n16\nFig.16.Plot of logarithm of the maximum convergence error at the interpolation points as a function of the normalized number of parameters for the neural and the FEM approach.\n\n\nFig. 17 .\n17\nFig.17.Plot of the time to converge as a function of the normalized number of parameters for the neural and the FEM approach.\n\n\n\n\n\n\n\n\n\nThe method is general and can be applied to ODE's, systems of ODE's and to PDE's defined on orthogonal box boundaries.Moreover, work is in progress to treat the case of irregular (arbitrarily shaped) boundaries.\n\n\u2022 The employment of neural networks provides a solution with very good generalization properties.Comparative results with the finite element method presented in this work illustrate this point clearly.\u2022 The required number of model parameters is far less than any other solution technique and, therefore, compact solution models are obtained with very low demand on memory space.\u2022\n\nLagaris received the B.Sc. degree in physics from the University of Ioannina, Greece, in 1975.He received the M.Sc.degree in 1977 and the Ph.D. degree in 1981, both from the Physics Department of the University of Illinois, Urbana-Champaign.He was a Lecturer in the Physics Department of the University of Ioannina for a number of years and since 1994 he has been an Associate Professor in the Department of Computer Science.His research interests include modeling and simulation of classical and quantum systems, high-performance computing, optimization, and neural networks.Aristidis Likas (S'91-M'96) was born in Athens, Greece, in 1968.He received the Diploma degree in electrical engineering and the Ph.D. degree in electrical and computer engineering, both from the National Technical University of Athens.Since 1996, he has been with the Department of Computer Science, University of Ioannina, Greece, where he is currently a Lecturer.His research interests include neural networks, optimization, pattern recognition, and parallel processing.Dimitrios I. Fotiadis received the Diploma degree in chemical engineering from the National Technical University of Athens, Greece, in 1985 and the Ph.D. degree in chemical engineering from the University of Minnesota, Duluth, in 1990.\n. D Kincaid, W Cheney, Numerical Analysis. 1991Brooks/Cole\n\nNeural algorithms for solving differential equations. H Lee, I Kang, J. Comput. Phys. 911990\n\nThe numerical solution of linear ordinary differential equations by feedforward neural networks. A J Meade, Jr , A A Fernandez, Math. Comput. Modeling. 19121994\n\nSolution of nonlinear ordinary differential equations by feedforward neural networks. Math. Comput. Modeling. 2091994\n\nStructured trainable networks for matrix algebra. L Wang, J M Mendel, IEEE Int. Joint Conf. Neural Networks. 21990\n\nVLSI implementation of locally connected neural network for solving partial differential equations. R Yentis, M E Zaghoul, IEEE Trans. Circuits Syst. I. 4381996\n\nMerlin 3.0, A multidimensional optimization environment. D G Papageorgiou, I N Demetropoulos, I E Lagaris, Comput. Phys. Commun. 1091998\n\nThe Merlin control language for strategic optimization. Comput. Phys. Commun. 1091998\n\nR Fletcher, Practical Methods of Optimization. New YorkWiley19872nd ed\n\nO C Zienkiewicz, R L Taylor, The Finite Element Method. New YorkMcGraw-Hill198914th ed.\n\n. W L Briggs, Multigrid Tutorial, 1987SIAMPhiladelphia, PA\n\nParallel finite element computation of 3-D flows. T Tezduyar, S Aliabadi, M Behr, A Johnson, S Mittal, IEEE Comput. 26101993\n\nArtificial neural network methods in quantum mechanics. I E Lagaris, A Likas, D I Fotiadis, Comput. Phys. Commun. 1041997\n", "annotations": {"author": "[{\"end\":317,\"start\":4}]", "publisher": null, "author_last_name": "[{\"end\":15,\"start\":10}]", "author_first_name": "[{\"end\":9,\"start\":4}]", "author_affiliation": "[{\"end\":92,\"start\":17},{\"end\":127,\"start\":94},{\"end\":177,\"start\":129},{\"end\":220,\"start\":179},{\"end\":316,\"start\":222}]", "title": null, "venue": null, "abstract": "[{\"end\":680,\"start\":350}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2933,\"start\":2930},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2938,\"start\":2935},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2943,\"start\":2940},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3169,\"start\":3166},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3174,\"start\":3171},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6841,\"start\":6838},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7157,\"start\":7154},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7513,\"start\":7510},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9711,\"start\":9708},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9823,\"start\":9820},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9840,\"start\":9837},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10304,\"start\":10301},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10313,\"start\":10309},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10748,\"start\":10745},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11351,\"start\":11347},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11404,\"start\":11400},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11588,\"start\":11584},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11735,\"start\":11731},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12785,\"start\":12782},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12790,\"start\":12787},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12895,\"start\":12892},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13832,\"start\":13828},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14481,\"start\":14477},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16211,\"start\":16207},{\"end\":16329,\"start\":16325},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20014,\"start\":20010},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21639,\"start\":21635},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25870,\"start\":25866}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26054,\"start\":25872},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26115,\"start\":26055},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26182,\"start\":26116},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26249,\"start\":26183},{\"attributes\":{\"id\":\"fig_4\"},\"end\":26340,\"start\":26250},{\"attributes\":{\"id\":\"fig_5\"},\"end\":26861,\"start\":26341},{\"attributes\":{\"id\":\"fig_6\"},\"end\":27301,\"start\":26862},{\"attributes\":{\"id\":\"fig_7\"},\"end\":27393,\"start\":27302},{\"attributes\":{\"id\":\"fig_8\"},\"end\":27473,\"start\":27394},{\"attributes\":{\"id\":\"fig_9\"},\"end\":27541,\"start\":27474},{\"attributes\":{\"id\":\"fig_10\"},\"end\":27595,\"start\":27542},{\"attributes\":{\"id\":\"fig_11\"},\"end\":27685,\"start\":27596},{\"attributes\":{\"id\":\"fig_12\"},\"end\":27773,\"start\":27686},{\"attributes\":{\"id\":\"fig_13\"},\"end\":27836,\"start\":27774},{\"attributes\":{\"id\":\"fig_14\"},\"end\":27929,\"start\":27837},{\"attributes\":{\"id\":\"fig_15\"},\"end\":28018,\"start\":27930},{\"attributes\":{\"id\":\"fig_16\"},\"end\":28105,\"start\":28019},{\"attributes\":{\"id\":\"fig_17\"},\"end\":28188,\"start\":28106},{\"attributes\":{\"id\":\"fig_18\"},\"end\":28379,\"start\":28189},{\"attributes\":{\"id\":\"fig_19\"},\"end\":28520,\"start\":28380},{\"end\":28525,\"start\":28521},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29123,\"start\":28526}]", "paragraph": "[{\"end\":843,\"start\":766},{\"end\":2001,\"start\":845},{\"end\":2150,\"start\":2003},{\"end\":2944,\"start\":2170},{\"end\":3815,\"start\":2946},{\"end\":5081,\"start\":3817},{\"end\":5167,\"start\":5083},{\"end\":6438,\"start\":5169},{\"end\":6581,\"start\":6472},{\"end\":6745,\"start\":6583},{\"end\":7012,\"start\":6747},{\"end\":7065,\"start\":7014},{\"end\":7205,\"start\":7067},{\"end\":7625,\"start\":7207},{\"end\":8184,\"start\":7627},{\"end\":8611,\"start\":8186},{\"end\":9230,\"start\":8639},{\"end\":10678,\"start\":9232},{\"end\":11194,\"start\":10680},{\"end\":11785,\"start\":11289},{\"end\":11849,\"start\":11787},{\"end\":11926,\"start\":11851},{\"end\":12001,\"start\":11928},{\"end\":12118,\"start\":12003},{\"end\":12307,\"start\":12150},{\"end\":13697,\"start\":12324},{\"end\":14351,\"start\":13745},{\"end\":14742,\"start\":14369},{\"end\":15005,\"start\":14744},{\"end\":15504,\"start\":15018},{\"end\":15519,\"start\":15506},{\"end\":15840,\"start\":15521},{\"end\":15855,\"start\":15842},{\"end\":15980,\"start\":15857},{\"end\":17089,\"start\":16019},{\"end\":17620,\"start\":17091},{\"end\":19054,\"start\":17622},{\"end\":19887,\"start\":19056},{\"end\":20337,\"start\":19889},{\"end\":21281,\"start\":20339},{\"end\":21892,\"start\":21283},{\"end\":22534,\"start\":21931},{\"end\":23055,\"start\":22536},{\"end\":23381,\"start\":23057},{\"end\":23864,\"start\":23383},{\"end\":24981,\"start\":23866},{\"end\":25534,\"start\":24983},{\"end\":25871,\"start\":25536},{\"end\":26053,\"start\":25875},{\"end\":26114,\"start\":26067},{\"end\":26181,\"start\":26128},{\"end\":26248,\"start\":26195},{\"end\":26339,\"start\":26262},{\"end\":26860,\"start\":26344},{\"end\":27300,\"start\":26880},{\"end\":27392,\"start\":27314},{\"end\":27472,\"start\":27406},{\"end\":27540,\"start\":27486},{\"end\":27594,\"start\":27554},{\"end\":27684,\"start\":27608},{\"end\":27772,\"start\":27700},{\"end\":27835,\"start\":27788},{\"end\":27928,\"start\":27851},{\"end\":28017,\"start\":27944},{\"end\":28104,\"start\":28033},{\"end\":28187,\"start\":28120},{\"end\":28378,\"start\":28203},{\"end\":28519,\"start\":28394},{\"end\":28740,\"start\":28529},{\"end\":29122,\"start\":28742}]", "formula": null, "table_ref": "[{\"end\":18142,\"start\":18141},{\"end\":18732,\"start\":18731}]", "section_header": "[{\"end\":764,\"start\":682},{\"end\":2168,\"start\":2153},{\"end\":6470,\"start\":6441},{\"end\":8637,\"start\":8614},{\"end\":11228,\"start\":11197},{\"end\":11287,\"start\":11231},{\"end\":12148,\"start\":12121},{\"end\":12322,\"start\":12310},{\"end\":13743,\"start\":13700},{\"end\":14367,\"start\":14354},{\"end\":15016,\"start\":15008},{\"end\":16017,\"start\":15983},{\"end\":21929,\"start\":21895},{\"end\":26064,\"start\":26056},{\"end\":26125,\"start\":26117},{\"end\":26192,\"start\":26184},{\"end\":26259,\"start\":26251},{\"end\":26877,\"start\":26863},{\"end\":27311,\"start\":27303},{\"end\":27403,\"start\":27395},{\"end\":27483,\"start\":27475},{\"end\":27551,\"start\":27543},{\"end\":27605,\"start\":27597},{\"end\":27696,\"start\":27687},{\"end\":27784,\"start\":27775},{\"end\":27847,\"start\":27838},{\"end\":27940,\"start\":27931},{\"end\":28029,\"start\":28020},{\"end\":28116,\"start\":28107},{\"end\":28199,\"start\":28190},{\"end\":28390,\"start\":28381}]", "table": null, "figure_caption": "[{\"end\":26054,\"start\":25874},{\"end\":26115,\"start\":26066},{\"end\":26182,\"start\":26127},{\"end\":26249,\"start\":26194},{\"end\":26340,\"start\":26261},{\"end\":26861,\"start\":26343},{\"end\":27301,\"start\":26879},{\"end\":27393,\"start\":27313},{\"end\":27473,\"start\":27405},{\"end\":27541,\"start\":27485},{\"end\":27595,\"start\":27553},{\"end\":27685,\"start\":27607},{\"end\":27773,\"start\":27699},{\"end\":27836,\"start\":27787},{\"end\":27929,\"start\":27850},{\"end\":28018,\"start\":27943},{\"end\":28105,\"start\":28032},{\"end\":28188,\"start\":28119},{\"end\":28379,\"start\":28202},{\"end\":28520,\"start\":28393},{\"end\":28525,\"start\":28523},{\"end\":28741,\"start\":28528}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13814,\"start\":13810},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13966,\"start\":13965},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14438,\"start\":14434},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14608,\"start\":14607},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":15554,\"start\":15553},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":15671,\"start\":15669},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":15945,\"start\":15943},{\"attributes\":{\"ref_id\":\"fig_18\"},\"end\":19063,\"start\":19061},{\"attributes\":{\"ref_id\":\"fig_19\"},\"end\":19607,\"start\":19605}]", "bib_author_first_name": "[{\"end\":30412,\"start\":30411},{\"end\":30423,\"start\":30422},{\"end\":30524,\"start\":30523},{\"end\":30531,\"start\":30530},{\"end\":30661,\"start\":30660},{\"end\":30663,\"start\":30662},{\"end\":30673,\"start\":30671},{\"end\":30677,\"start\":30676},{\"end\":30679,\"start\":30678},{\"end\":30895,\"start\":30894},{\"end\":30903,\"start\":30902},{\"end\":30905,\"start\":30904},{\"end\":31061,\"start\":31060},{\"end\":31071,\"start\":31070},{\"end\":31073,\"start\":31072},{\"end\":31180,\"start\":31179},{\"end\":31182,\"start\":31181},{\"end\":31198,\"start\":31197},{\"end\":31200,\"start\":31199},{\"end\":31217,\"start\":31216},{\"end\":31219,\"start\":31218},{\"end\":31348,\"start\":31347},{\"end\":31420,\"start\":31419},{\"end\":31422,\"start\":31421},{\"end\":31437,\"start\":31436},{\"end\":31439,\"start\":31438},{\"end\":31511,\"start\":31510},{\"end\":31513,\"start\":31512},{\"end\":31531,\"start\":31522},{\"end\":31619,\"start\":31618},{\"end\":31631,\"start\":31630},{\"end\":31643,\"start\":31642},{\"end\":31651,\"start\":31650},{\"end\":31662,\"start\":31661},{\"end\":31751,\"start\":31750},{\"end\":31753,\"start\":31752},{\"end\":31764,\"start\":31763},{\"end\":31773,\"start\":31772},{\"end\":31775,\"start\":31774}]", "bib_author_last_name": "[{\"end\":30420,\"start\":30413},{\"end\":30430,\"start\":30424},{\"end\":30528,\"start\":30525},{\"end\":30536,\"start\":30532},{\"end\":30669,\"start\":30664},{\"end\":30689,\"start\":30680},{\"end\":30900,\"start\":30896},{\"end\":30912,\"start\":30906},{\"end\":31068,\"start\":31062},{\"end\":31081,\"start\":31074},{\"end\":31195,\"start\":31183},{\"end\":31214,\"start\":31201},{\"end\":31227,\"start\":31220},{\"end\":31357,\"start\":31349},{\"end\":31434,\"start\":31423},{\"end\":31446,\"start\":31440},{\"end\":31520,\"start\":31514},{\"end\":31540,\"start\":31532},{\"end\":31628,\"start\":31620},{\"end\":31640,\"start\":31632},{\"end\":31648,\"start\":31644},{\"end\":31659,\"start\":31652},{\"end\":31669,\"start\":31663},{\"end\":31761,\"start\":31754},{\"end\":31770,\"start\":31765},{\"end\":31784,\"start\":31776}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":30467,\"start\":30409},{\"attributes\":{\"id\":\"b1\"},\"end\":30561,\"start\":30469},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":122130053},\"end\":30723,\"start\":30563},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":119687298},\"end\":30842,\"start\":30725},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":30592997},\"end\":30958,\"start\":30844},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":61249876},\"end\":31120,\"start\":30960},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":62544816},\"end\":31258,\"start\":31122},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":62703417},\"end\":31345,\"start\":31260},{\"attributes\":{\"id\":\"b8\"},\"end\":31417,\"start\":31347},{\"attributes\":{\"id\":\"b9\"},\"end\":31506,\"start\":31419},{\"attributes\":{\"id\":\"b10\"},\"end\":31566,\"start\":31508},{\"attributes\":{\"id\":\"b11\"},\"end\":31692,\"start\":31568},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15610332},\"end\":31815,\"start\":31694}]", "bib_title": "[{\"end\":30521,\"start\":30469},{\"end\":30658,\"start\":30563},{\"end\":30809,\"start\":30725},{\"end\":30892,\"start\":30844},{\"end\":31058,\"start\":30960},{\"end\":31177,\"start\":31122},{\"end\":31314,\"start\":31260},{\"end\":31616,\"start\":31568},{\"end\":31748,\"start\":31694}]", "bib_author": "[{\"end\":30422,\"start\":30411},{\"end\":30432,\"start\":30422},{\"end\":30530,\"start\":30523},{\"end\":30538,\"start\":30530},{\"end\":30671,\"start\":30660},{\"end\":30676,\"start\":30671},{\"end\":30691,\"start\":30676},{\"end\":30902,\"start\":30894},{\"end\":30914,\"start\":30902},{\"end\":31070,\"start\":31060},{\"end\":31083,\"start\":31070},{\"end\":31197,\"start\":31179},{\"end\":31216,\"start\":31197},{\"end\":31229,\"start\":31216},{\"end\":31359,\"start\":31347},{\"end\":31436,\"start\":31419},{\"end\":31448,\"start\":31436},{\"end\":31522,\"start\":31510},{\"end\":31542,\"start\":31522},{\"end\":31630,\"start\":31618},{\"end\":31642,\"start\":31630},{\"end\":31650,\"start\":31642},{\"end\":31661,\"start\":31650},{\"end\":31671,\"start\":31661},{\"end\":31763,\"start\":31750},{\"end\":31772,\"start\":31763},{\"end\":31786,\"start\":31772}]", "bib_venue": "[{\"end\":31402,\"start\":31394},{\"end\":31483,\"start\":31475},{\"end\":30450,\"start\":30432},{\"end\":30553,\"start\":30538},{\"end\":30713,\"start\":30691},{\"end\":30833,\"start\":30811},{\"end\":30951,\"start\":30914},{\"end\":31111,\"start\":31083},{\"end\":31249,\"start\":31229},{\"end\":31336,\"start\":31316},{\"end\":31392,\"start\":31359},{\"end\":31473,\"start\":31448},{\"end\":31682,\"start\":31671},{\"end\":31806,\"start\":31786}]"}}}, "year": 2023, "month": 12, "day": 17}