{"id": 26501419, "updated": "2023-09-29 22:17:10.315", "metadata": {"title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "authors": "[{\"first\":\"Mandar\",\"last\":\"Joshi\",\"middle\":[]},{\"first\":\"Eunsol\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Weld\",\"middle\":[]},{\"first\":\"Luke\",\"last\":\"Zettlemoyer\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1705.03551", "mag": "2963339397", "acl": "P17-1147", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/JoshiCWZ17", "doi": "10.18653/v1/p17-1147"}}, "content": {"source": {"pdf_hash": "822cad9d86244951f65ca1cc0223e96cf97ccbc2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1705.03551v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/P17-1147.pdf", "status": "HYBRID"}}, "grobid": {"id": "78cd0acb2c2db5baf36ea3056ba907ea272beb02", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/822cad9d86244951f65ca1cc0223e96cf97ccbc2.txt", "contents": "\nTriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\n\n\nMandar Joshi mandar90@cs.washington.edu \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniv. of Washington\nSeattle, SeattleWA, WA\n\nEunsol Choi eunsol@cs.washington.edu \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniv. of Washington\nSeattle, SeattleWA, WA\n\nDaniel S Weld weld@cs.washington.edu \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniv. of Washington\nSeattle, SeattleWA, WA\n\nLuke Zettlemoyer lukez@allenai.org \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniv. of Washington\nSeattle, SeattleWA, WA\n\nPaul G Allen \nSchool of Computer Science & Engineering\nAllen Institute for Artificial Intelligence\nUniv. of Washington\nSeattle, SeattleWA, WA\n\nTriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\n\nWe present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K questionanswer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a featurebased classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that Trivi-aQA is a challenging testbed that is worth significant future study. 1\n\nIntroduction\n\nReading comprehension (RC) systems aim to answer any question that could be posed against the facts in some reference text. This goal is challenging for a number of reasons: (1) the questions can be complex (e.g. have highly compositional semantics), (2) finding the correct answer can require complex reasoning (e.g. combining facts from multiple sentences or background knowledge) and (3) individual facts can be difficult to Figure 1: Question-answer pairs with sample excerpts from evidence documents from TriviaQA exhibiting lexical and syntactic variability, and requiring reasoning from multiple sentences. recover from text (e.g. due to lexical and syntactic variation). Figure 1 shows examples of all these phenomena. This paper presents TriviaQA, a new reading comprehension dataset designed to simultaneously test all of these challenges.\n\nRecently, significant progress has been made by introducing large new reading comprehension datasets that primarily focus on one of the challenges listed above, for example by crowdsourcing the gathering of question answer pairs (Rajpurkar et al., 2016) or using cloze-style sentences instead of questions (Hermann et al., 2015;Onishi et al., 2016) (see Table 1 for more examples). In general, system performance has improved rapidly as each resource is released. The best models of-  (Rajpurkar et al., 2016) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) * WikiQA (Yang et al., 2016) TREC (Voorhees and Tice, 2000) Table 1: Comparison of TriviaQA with existing QA datasets. Our dataset is unique in that it is naturally occurring, well-formed questions collected independent of the evidences. *NewsQA uses evidence articles indirectly by using only article summaries. ten achieve near-human performance levels within months or a year, fueling a continual need to build ever more difficult datasets. We argue that Triv-iaQA is such a dataset, by demonstrating that a high percentage of its questions require solving these challenges and showing that there is a large gap between state-of-the-art methods and human performance levels.\n\nTriviaQA contains over 650K question-answerevidence triples, that are derived by combining 95K Trivia enthusiast authored question-answer pairs with on average six supporting evidence documents per question. To our knowledge, TriviaQA is the first dataset where full-sentence questions are authored organically (i.e. independently of an NLP task) and evidence documents are collected retrospectively from Wikipedia and the Web. This decoupling of question generation from evidence collection allows us to control for potential bias in question style or content, while offering organically generated questions from various topics. Designed to engage humans, TriviaQA presents a new challenge for RC models. They should be able to deal with large amount of text from various sources such as news articles, encyclopedic entries and blog articles, and should handle inference over multiple sentences. For example, our dataset contains three times as many questions that require inference over multiple sentences than the recently released SQuAD (Rajpurkar et al., 2016) dataset. Section 4 present a more detailed discussion of these challenges.\n\nFinally, we present baseline experiments on the TriviaQA dataset, including a linear classifier inspired by work on CNN Dailymail and MCTest (Chen et al., 2016;Richardson et al., 2013) and a state-of-the-art neural network baseline (Seo et al., 2017). The neural model performs best, but only achieves 40% for TriviaQA in comparison to 68% on SQuAD, perhaps due to the challenges listed above. The baseline results also fall far short of human performance levels, 79.7%, suggesting significant room for the future work. In summary, we make the following contributions.\n\n\u2022 We collect over 650K question-answerevidence triples, with questions originating from trivia enthusiasts independent of the evidence documents. A high percentage of the questions are challenging, with substantial syntactic and lexical variability and often requiring multi-sentence reasoning. The dataset and code are available at http://nlp.cs.washington. edu/triviaqa/, offering resources for training new reading-comprehension models.\n\n\u2022 We present a manual analysis quantifying the quality of the dataset and the challenges involved in solving the task.\n\n\u2022 We present experiments with two baseline methods, demonstrating that the TriviaQA tasks are not easily solved and are worthy of future study.\n\n\u2022 In addition to the automatically gathered large-scale (but noisy) dataset, we present a clean, human-annotated subset of 1975 question-document-answer triples whose documents are certified to contain all facts required to answer the questions.\n\n\nOverview\n\nProblem Formulation We frame reading comprehension as the problem of answering a question q given the textual evidence provided by document set D. We assume access to a dataset of tuples {(q i , a i , D i )|i = 1 . . . n} where a i is a text string that defines the correct answer to question q i . Following recent formulations (Rajpurkar et al., 2016), we further assume that a i appears as a substring for some document in the set D i . 2 However, we differ by setting D i as a set of documents, where previous work assumed a single document (Hermann et al., 2015) or even just a short paragraph (Rajpurkar et al., 2016).\n\nData and Distant Supervision Our evidence documents are automatically gathered from either Wikipedia or more general Web search results (details in Section 3). Because we gather evidence using an automated process, the documents are not guaranteed to contain all facts needed to answer the question. Therefore, they are best seen as a source of distant supervision, based on the assumption that the presence of the answer string in an evidence document implies that the document does answer the question. 3 Section 4 shows that this assumption is valid over 75% of the time, making evidence documents a strong source of distant supervision for training machine reading systems.\n\nIn particular, we consider two types of distant supervision, depending on the source of our documents. For web search results, we expect the documents that contain the correct answer a to be highly redundant, and therefore let each questionanswer-document tuple be an independent data point. (|D i | = 1 for all i and q i = q j for many i, j pairs). However, in Wikipedia we generally expect most facts to be stated only once, so we instead pool all of the evidence documents and never repeat the same question in the dataset (|D i | = 1.8 on average and q i = q j for all i, j). In other words, each question (paired with the union of all of its evidence documents) is a single data point.\n\nThese are far from the only assumptions that could be made in this distant supervision setup. For example, our data would also support multiinstance learning, which makes the at least once assumption, from relation extraction (Riedel et al., 2010;Hoffmann et al., 2011) or many other possibilities. However, the experiments in Section 6 show that these assumptions do present a strong 2 The data we will present in Section 3 would further support a task formulation where some documents D do not have the correct answer and the model must learn when to abstain. We leave this to future work. 3 An example context for the first question in Figure 1 where such an assumption fails would be the following evidence string: The Guns of Navarone is a 1961 British-American epic adventure war film directed by J. Lee Thompson.  signal for learning; we believe the data will fuel significant future study.\n\n\nDataset Collection\n\nWe collected a large dataset to support the reading comprehension task described above. First we gathered question-answer pairs from 14 trivia and quiz-league websites. We removed questions with less than four tokens, since these were generally either too simple or too vague. We then collected textual evidence to answer questions using two sources: documents from Web search results and Wikipedia articles for entities in the question. To collect the former, we posed each question 4 as a search query to the Bing Web search API, and collected the top 50 search result URLs. To exclude the trivia websites, we removed from the results all pages from the trivia websites we scraped and any page whose url included the keywords trivia, question, or answer. We then crawled the top 10 search result Web pages and pruned PDF and other ill formatted documents. The search output includes a diverse set of documents such as blog articles, news articles, and encyclopedic entries.\n\nWikipedia pages for entities mentioned in the question often provide useful information. We therefore collected an additional set of evidence documents by applying TAGME, an off-the-shelf entity linker (Ferragina and Scaiella, 2010), to find Wikipedia entities mentioned in the question, and added the corresponding pages as evidence documents.\n\nFinally, to support learning from distant supervision, we further filtered the evidence documents to exclude those missing the correct answer string and formed evidence document sets as described in Section 2. This left us with 95K questionanswer pairs organized into (1)    sis, we sampled 200 question answer pairs and manually analysed their properties. About 73.5% of these questions contain phrases that describe a fine grained category to which the answer belongs, while 15.5% hint at a coarse grained category (one of person, organization, location, and miscellaneous). Questions often involve reasoning over time frames, as well as making comparisons. A summary of the analysis is presented in Table 3. Answers in TriviaQA belong to a diverse set of types. 92.85% of the answers are titles in Wikipedia, 5 4.17% are numerical expressions (e.g., 9 kilometres) while the rest are open ended noun and verb phrases. A coarse grained type analysis of answers that are Wikipedia entities presented in Table 4. It should be noted that not all Wikipedia titles are named entities; many are common phrases such as barber or soup. Evidence analysis A qualitative analysis of TriviaQA shows that the evidence contains answers for 79.7% and 75.4% of questions from the Wikipedia and Web domains respectively. To analyse the quality of evidence and evaluate baselines, we asked a human annotator to answer 986 and 1345 (dev and test set) questions from the Wikipedia and Web domains respectively. Trivia Reasoning Lexical variation (synonym) Major correspondences between the question and the answer sentence are synonyms.  questions contain multiple clues about the answer(s) not all of which are referenced in the documents. The annotator was asked to answer a question if the minimal set of facts (ignoring temporal references like this year) required to answer the question are present in the document, and abstain otherwise. For example, it is possible to answer the question, Who became president of the Mormons in 1844, organised settlement of the Mormons in Utah 1847 and founded Salt Lake City? using only the fact that Salt Lake City was founded by Brigham Young. We found that the accuracy (evaluated using the original answers) for the Wikipedia and Web domains was 79.6 and 75.3 respectively. We use the correctly answered questions (and documents) as verified sets for evaluation (section 6).\n\nChallenging problem A comparison of evidence with respect to the questions shows a high proportion of questions require reasoning over multiple sentences. To compare our dataset against previous datasets, we classified 100 question-evidence pairs each from Wikipedia and the Web according to the form of reasoning required to answer them. We focus the analysis on Wikipedia since the analysis on Web documents are similar. Categories are not mutually exclusive: single example can fall into multiple categories. A summary of the analysis is presented in Table 5.\n\nOn comparing evidence sentences with their corresponding questions, we found that 69% of the questions had a different syntactic structure while 41% were lexically different. For 40% of the questions, we found that the information re-quired to answer them was scattered over multiple sentences. Compared to SQuAD, over three times as many questions in TriviaQA require reasoning over multiple sentences. Moreover, 17% of the examples required some form of world knowledge. Question-evidence pairs in Trivi-aQA display more lexical and syntactic variance than SQuAD. This supports our earlier assertion that decoupling question generation from evidence collection results in a more challenging problem.\n\n\nBaseline methods\n\nTo quantify the difficulty level of the dataset for current methods, we present results on neural and other models. We used a random entity baseline and a simple classifier inspired from previous work (Wang et al., 2015;Chen et al., 2016), and compare these to BiDAF (Seo et al., 2017), one of the best performing models for the SQuAD dataset.\n\n\nRandom entity baseline\n\nWe developed the random entity baseline for the Wikipedia domain since the provided documents can be directly mapped to candidate answers. In this heuristic approach, we first construct a candidate answer set using the entities associated with the provided Wikipedia pages for a given question (on average 1.8 per question). We then randomly pick a candidate that does not occur in the question. If no such candidate exists, we pick any random candidate from the candidate set.\n\n\nEntity classifier\n\nWe also frame the task as a ranking problem over candidate answers in the documents. More formally, given a question q i , an answer a + i , and a evidence document D i , we want to learn a scoring function score, such that\nscore(a + i |q i , D i ) > score(a \u2212 i |q i , D i )\nwhere a \u2212 i is any candidate other than the answer. The function score is learnt using LambdaMART (Wu et al., 2010), 6 a boosted tree based ranking algorithm.\n\nThis is similar to previous entity-centric classifiers for QA (Chen et al., 2016;Wang et al., 2015), and uses context and Wikipedia catalog based features. To construct the candidate answer set, we consider sentences that contain at least one word in common with the question. We then add every n-gram (n \u2208 [1, 5]) that occurs in these sentences and is a title of some Wikipedia article. 7\n\n\nNeural model\n\nRecurrent neural network models (RNNs) (Hermann et al., 2015;Chen et al., 2016) have been very effective for reading comprehension. For our task, we modified the BiDAF model (Seo et al., 2017), which takes a sequence of context words as input and outputs the start and end positions of the predicted answer in the context. The model utilizes an RNN at the character level, token level, and phrase level to encode context and question and uses attention mechanism between question and context. Authored independently from the evidence document, TriviaQA does not contain the exact spans of the answers. We approximate the answer span by finding the first match of answer string in the evidence document. Developed for a dataset where the evidence document is a single paragraph (average 122 words), the BiDAF model does not scale to long documents. To overcome this, we truncate the evidence document to the first 800 words. 8 When the data contains more than one evidence document, as in our Wikipedia domain, we predict for each document separately and aggregate the predictions by taking a sum of confidence scores. More specifically, when the model outputs a candidate answer A i from n documents D i,1 , ...D i,n with confidences c i,1 , ...c i,n , the score of A i is given by\nscore(A i ) = k c i,k\nWe select candidate answer with the highest score.\n\n\nExperiments\n\nAn evaluation of our baselines shows that both of our tasks are challenging, and that the TriviaQA dataset supports significant future work.  Table 6: Data statistics for each task setup. The Wikipedia domain is evaluated over questions while the web domain is evaluated over documents.\n\n\nEvaluation Metrics\n\nWe use the same evaluation metrics as SQuADexact match (EM) and F1 over words in the answer(s). For questions that have Numerical and FreeForm answers, we use a single given answer as ground truth. For questions that have Wikipedia entities as answers, we use Wikipedia aliases as valid answer along with the given answer.\n\nSince Wikipedia and the web are vastly different in terms of style and content, we report performance on each source separately. While using Wikipedia, we evaluate at the question level since facts needed to answer a question are generally stated only once. On the other hand, due to high information redundancy in web documents (around 6 documents per question), we report document level accuracy and F1 when evaluating on web documents. Lastly, in addition to distant supervision, we also report evaluation on the clean dev and test questions collection using a human annotator (section 4)\n\n\nExperimental Setup\n\nWe randomly partition QA pairs in the dataset into train (80%), development (10%), and test set (10%). In addition to distant supervision evaluation, we also evaluate baselines on verified subsets (see section 4) of the dev and test partitions. Table  6 contains the number of questions and documents for each task. We trained the entity classifier on a random sample of 50,000 questions from the training set. For training BiDAF on the web domain, we first randomly sampled 80,000 documents. For both domains, we used only those (training) documents where the answer appears in the first 400 tokens to keep training time manageable. Designing scalable techniques that can use the entirety of the data is an interesting direction for future work.\n\n\nResults\n\nThe performance of the proposed models is summarized in Table 7. The poor performance of the random entity baseline shows that the task is not already solved by information retrieval. For both Wikipedia and web documents, BiDAF (40%) outperforms the classifier (23%). The oracle score is the upper bound on the exact match accuracy. 9 All models lag significantly behind the human baseline of 79.7% on the Wikipedia domain, and 75.4% on the web domain.\n\nWe analyse the performance of BiDAF on the development set using Wikipedia as the evidence source by question length and answer type. The accuracy of the system steadily decreased as the length of the questions increased -with 50% for questions with 5 or fewer words to 32% for 20 or more words. This suggests that longer compositional questions are harder for current methods.\n\n\nError analysis\n\nOur qualitative error analysis reveals that compositionality in questions and lexical variation and low signal-to-noise ratio in (full) documents is still a challenge for current methods. We randomly sampled 100 incorrect BiDAF predictions from the development set and used Wikipedia evidence documents for manual analysis. We found that 19 examples lacked evidence in any of the provided documents, 3 had incorrect ground truth, and 3 were valid answers that were not included in the answer key. Furthermore, 12 predictions were partially correct (Napoleonic vs Napoleonic Wars). This seems to be consistent with human performance of 79.7%.\n\nFor the rest, we classified each example into one or more categories listed in Table 8. Distractor entities refers to the presence of entities similar to ground truth. E.g., for the question, Rebecca Front plays Detective Chief Superintendent Innocent in which TV series?, the evidence describes all roles played by Rebecca Front.\n\nThe first two rows suggest that long and noisy documents make the question answering task more difficult, as compared for example to the short passages in SQuAD. Furthermore, a high proportion of errors are caused by paraphrasing, and the answer is sometimes stated indirectly. For   \n\n\nRelated work\n\nRecent interest in question answering has resulted in the creation of several datasets. However, they are either limited in scale or suffer from biases stemming from their construction process. We group existing datasets according to their associated tasks, and compare them against TriviaQA. The analysis is summarized in Table 1.\n\n\nReading comprehension\n\nReading comprehension tasks aims to test the ability of a system to understand a document using questions based upon its contents. Researchers have constructed cloze-style datasets (Hill et al., 2015;Hermann et al., 2015;Paperno et al., 2016;Onishi et al., 2016), where the task is to predict missing words, often entities, in a document. Cloze-style datasets, while easier to construct large-scale automatically, do not contain natural language questions.\n\nDatasets with natural language questions include MCTest (Richardson et al., 2013), SQuAD (Rajpurkar et al., 2016), and NewsQA (Trischler et al., 2016). MCTest is limited in scale with only 2640 multiple choice questions. SQuAD contains 100K crowdsourced questions and answers paired with short Wikipedia passages. NewsQA uses crowdsourcing to create questions solely from news article summaries in order to control potential bias. The crucial difference between SQuAD/NewsQA and TriviaQA is that TriviaQA questions have not been crowdsourced from preselected passages. Additionally, our evidence set consists of web documents, while SQuAD and NewsQA are limited to Wikipedia and news articles respectively. Other recently released datasets include (Lai et al., 2017).\n\n\nOpen domain question answering\n\nThe recently released MS Marco dataset (Nguyen et al., 2016) also contains independently authored questions and documents drawn from the search results. However, the questions in the dataset are derived from search logs and the answers are crowdsourced. On the other hand, trivia enthusiasts provided both questions and answers for our dataset.\n\nKnowledge base question answering involves converting natural language questions to logical forms that can be executed over a KB. Proposed datasets (Cai and Yates, 2013;Berant et al., 2013; are either limited in scale or in the complexity of questions, and can only retrieve facts covered by the KB.\n\nA standard task for open domain IR-style QA is the annual TREC competitions (Voorhees and Tice, 2000), which contains questions from various domains but is limited in size. Many advances from the TREC competitions were used in the IBM Watson system for Jeopardy! (Ferrucci et al., 2010). Other datasets includes SearchQA (Dunn et al., 2017) where Jeopardy! questions are paired with search engine snippets, the Wik-iQA dataset (Yang et al., 2015) for answer sentence selection, and the Chinese language WebQA  dataset, which focuses on the task of answer phrase extraction. TriviaQA contains examples that could be used for both stages of the pipeline, although our focus on this paper is instead on using the data for reading comprehension where the answer is always present.\n\nOther recent approaches attempt to combine structured high precision KBs with semistructured information sources like OpenIE triples (Fader et al., 2014), HTML tables (Pasupat and Liang, 2015), and large (and noisy) corpora (Sawant and Chakrabarti, 2013;Joshi et al., 2014;Xu et al., 2015). TriviaQA, which has Wikipedia entities as answers, makes it possible to leverage structured KBs like Freebase, which we leave to future work. Furthermore, about 7% of the TriviaQA questions have answers in HTML tables and lists, which could be used to augment these existing resources.\n\nTrivia questions from quiz bowl have been previously used in other question answering tasks (Boyd-Graber et al., 2012). Quiz bowl questions are paragraph length and pyramidal. 10 A number of different aspects of this problem have been carefully studied, typically using classifiers over a pre-defined set of answers (Iyyer et al., 2014) and studying incremental answering to answer as quickly as possible (Boyd-Graber et al., 2012) or using reinforcement learning to model opponent behavior . These competitive challenges are not present in our single-sentence question setting. Developing joint models for multisentence reasoning for questions and answer documents is an important area for future work.\n\n\nConclusion and Future Work\n\nWe present TriviaQA, a new dataset of 650K question-document-evidence triples.\n\nTo our knowledge, TriviaQA is the first dataset where questions are authored by trivia enthusiasts, independently of the evidence documents. The evidence documents come from two domains -Web search results and Wikipedia pages -with highly differing levels of information redundancy. Results from current state-of-the-art baselines indi-cate that TriviaQA is a challenging testbed that deserves significant future study.\n\nWhile not the focus of this paper, TriviaQA also provides a provides a benchmark for a variety of other tasks such as IR-style question answering, QA over structured KBs and joint modeling of KBs and text, with much more data than previously available.\n\n\nFigure 2 shows diverse topics indicated by WordNet synsets of answer entities.\n\nTable 2 :\n2TriviaQA: Dataset statistics.\n\n\n650K training examples for the Web search results, each contain-Property \n\nExample annotation \nStatistics \n\nAvg. entities / question \nWhich politician won the Nobel Peace Prize in 2009? \n1.77 per question \nFine grained answer type \nWhat fragrant essential oil is obtained from Damask Rose? 73.5% of questions \nCoarse grained answer type Who won the Nobel Peace Prize in 2009? \n15.5% of questions \nTime frame \nWhat was photographed for the first time in October 1959 \n34% of questions \nComparisons \nWhat is the appropriate name of the largest type of frog? \n9% of questions \n\n\n\nTable 3 :\n3Properties of questions on 200 annotated examples show that a majority of TriviaQA questions contain multiple entities. The boldfaced words hint at the presence of corresponding property. 78K examples for the Wikipedia reading comprehension domain, containing on average 1.8 evidence documents per example.Table 2contains the dataset statistics. While not the focus of this paper, we have also released the full unfiltered dataset which contains 110,495 QA pairs and 740K evidence documents to support research in allied problems such as open domain and IRstyle question answering.Question and answer analysis TriviaQA questions, authored by trivia enthusiasts, cover various topics of people's interest. The average question length is 14 tokens indicating that many questions are highly compositional. For qualitative analy-Figure 2: Distribution of hierarchical WordNet \nsynsets for entities appearing in the answer. The \narc length is proportional to the number of ques-\ntions containing that category. \n\ning a single (combined) evidence document, and \n(2) 4 Dataset Analysis \n\nA quantitative and qualitative analysis of Trivi-\naQA shows it contains complex questions about a \ndiverse set of entities, which are answerable using \nthe evidence documents. \n\nType \nPercentage \n\nNumerical \n4.17 \nFree text \n2.98 \nWikipedia title \n92.85 \nPerson \n32 \nLocation \n23 \nOrganization \n5 \nMisc. \n40 \n\n\n\nTable 4 :\n4Distribution of answer types on 200 annotated examples.\n\n\nFrequency 41% in Wiki documents, 39% in web documents. Q What is solid CO2 commonly called? Examples S The frozen solid form of CO2, known as dry ice ... Q Who wrote the novel The Eagle Has landed? S The Eagle Has Landed is a book by British writer Jack Higgins Reasoning Lexical variation and world knowledge Major correspondences between the question and the document require common sense or external knowledge. Frequency 17% in Wiki documents, 17% in web documents. Who was the female member of the 1980's pop music duo, Eurythmics? S Eurythmics were a British music duo consisting of members Annie Lennox and David A. Stewart. Examples S The 1942 Battle of El Alamein in Egypt was actually two pivotal battles of World War II Q Whom was Ronald Reagan referring to when he uttered the famous phrase evil empire in a 1983 speech? S The phrase evil empire was first applied to the Soviet Union in 1983 by U.S. President Ronald Reagan. Frequency 40% in Wiki documents, 35% in web documents. Q Name the Greek Mythological hero who killed the gorgon Medusa. S Perseus asks god to aid him. So the goddess Athena and Hermes helps him out to kill Medusa. Examples Q Who starred in and directed the 1993 film A Bronx Tale? S Robert De Niro To Make His Broadway Directorial Debut With A Bronx Tale: The Musical. The actor starred and directed the 1993 film. Reasoning Lists, Table Answer found in tables or lists Frequency 7% in web documents.Q What is the first name of Madame Bovary in Flaubert's 1856 novel? \nS Madame Bovary (1856) is the French writer Gustave Flaubert's debut novel. The story focuses on a doctor's \nExamples \nwife, Emma Bovary \nQ Reasoning Syntactic Variation \nAfter the question is paraphrased into declarative form, its syntactic dependency structure does not match \nthat of the answer sentence \nFrequency 69% in Wiki documents, 65% in web documents. \nQ In which country did the Battle of El Alamein take place? \n\nReasoning Multiple sentences \nRequires reasoning over multiple sentences. \nExamples \nQ In Moh's Scale of hardness, Talc is at number 1, but what is number 2? \nQ What is the collective name for a group of hawks or falcons? \n\n\n\nTable 5 :\n5Analysis of reasoning used to answer TriviaQA questions shows that a high proportion of evidence sentence(s) exhibit syntactic and lexical variation with respect to questions. Answers are indicated by boldfaced text.\n\nTable 7 :\n7Performance of all systems on TriviaQA using distantly supervised evaluation. The best performing system is indicated in bold.Category \nProportion \n\nInsufficient evidence \n19 \nPrediction from incorrect document(s) \n7 \nAnswer not in clipped document \n15 \nParaphrasing \n29 \nDistractor entities \n11 \nReasoning over multiple sentences \n18 \n\n\n\nTable 8 :\n8Qualitative error analysis of BiDAF on \nWikipedia evidence documents. \n\nexample, the evidence for the question What was \nTruman Capote's last name before he was adopted \nby his stepfather? consists of the following text \nTruman Garcia Capote born Truman Streckfus \nPersons, was an American ... In 1933, he moved \nto New York City to live with his mother and her \nsecond husband, Joseph Capote, who adopted him \nas his stepson and renamed him Truman Garca \nCapote. \n\n\nData and code available at http://nlp.cs. washington.edu/triviaqa/\nNote that we did not use the answer as a part of the search query to avoid biasing the results.\nThis is a very large set since Wikipedia has more than 11 million titles.\nWe use the RankLib implementation https:// sourceforge.net/p/lemur/wiki/RankLib/\nUsing a named entity recognition system to generate candidate entities is not feasible as answers can be common nouns or phrases.8  We found that splitting documents into smaller sub documents degrades performance since a majority of sub documents do not contain the answer.\nA question q is considered answerable for the oracle score if the correct answer is found in the evidence D or, in case of the classifier, is a part of the candidate set. Since we truncate documents, the upper bound is not 100%.\nPyramidal questions consist of a series of clues about the answer arranged in order from most to least difficult.\nAcknowledgmentsThis work was supported by DARPA contract FA8750-13-2-0019, the WRF/Cable Professorship, gifts from Google and Tencent, and an Allen Distinguished Investigator Award. The authors would like to thank Minjoon Seo for the BiDAF code, and Noah Smith, Srinivasan Iyer, Mark Yatskar, Nicholas FitzGerald, Antoine Bosselut, Dallas Card, and anonymous reviewers for helpful comments.\nSemantic parsing on freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingGrand Hyatt Seattle, Seattle, Washington, USA2013A meeting of SIGDAT, a Special Interest Group of the ACLJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on free- base from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18- 21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Spe- cial Interest Group of the ACL. pages 1533-1544. http://aclweb.org/anthology/D/D13/D13-1160.pdf.\n\nLarge-scale simple question answering with memory networks. Antoine Bordes, Nicolas Usunier, Sumit Chopra, Jason Weston, CoRR abs/1506.02075Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple ques- tion answering with memory networks. CoRR abs/1506.02075. https://arxiv.org/abs/1506.02075.\n\nBesting the quiz master: Crowdsourcing incremental classification games. Jordan Boyd-Graber, Brianna Satinoff, He He, Hal Daum\u00e9, Iii , Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language LearningJeju Island, KoreaAssociation for Computational LinguisticsJordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daum\u00e9 III. 2012. Besting the quiz master: Crowdsourcing incremental classification games. In Proceedings of the 2012 Joint Con- ference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Lan- guage Learning. Association for Computational Linguistics, Jeju Island, Korea, pages 1290-1301. http://www.aclweb.org/anthology/D12-1118.\n\nLarge-scale semantic parsing via schema matching and lexicon extension. Qingqing Cai, Alexander Yates, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational LinguisticsSofia, Bulgaria1Long Papers). Association for Computational LinguisticsQingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Proceedings of the 51st Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa- tional Linguistics, Sofia, Bulgaria, pages 423-433. http://www.aclweb.org/anthology/P13-1042.\n\n. Danqi Chen, Jason Bolton, Christopher D Manning, A thorough examination of theDanqi Chen, Jason Bolton, and Christopher D. Man- ning. 2016. A thorough examination of the\n\ncnn/daily mail reading comprehension task. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics1Long Papers)cnn/daily mail reading comprehension task. In Pro- ceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pages 2358-2367. http://www.aclweb.org/anthology/P16-1223.\n\nMatthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, Kyunghyun Cho, Searchqa: A new q&a dataset aug. Matthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset aug- mented with context from a search engine. CoRR https://arxiv.org/abs/1704.05179.\n\nOpen question answering over curated and extracted knowledge bases. Anthony Fader, Luke Zettlemoyer, Oren Etzioni, 10.1145/2623330.2623677Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningNew York, NY, USAACMAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, USA, KDD '14, pages 1156-1165. https://doi.org/10.1145/2623330.2623677.\n\nTagme: On-the-fly annotation of short text fragments (by wikipedia entities). Paolo Ferragina, Ugo Scaiella, 10.1145/1871437.1871689Proceedings of the 19th ACM International Conference on Information and Knowledge Management. the 19th ACM International Conference on Information and Knowledge ManagementNew York, NY, USA, CIKM '10ACMPaolo Ferragina and Ugo Scaiella. 2010. Tagme: On-the-fly annotation of short text fragments (by wikipedia entities). In Proceedings of the 19th ACM International Conference on Informa- tion and Knowledge Management. ACM, New York, NY, USA, CIKM '10, pages 1625-1628. https://doi.org/10.1145/1871437.1871689.\n\nBuilding watson: An overview of the deepqa project. David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, Chris Welty, AI MAGAZINE. 313David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyan- pur, Adam Lally, J. William Murdock, Eric Ny- berg, John Prager, Nico Schlaefer, and Chris Welty. 2010. Building watson: An overview of the deepqa project. AI MAGAZINE 31(3):59-79.\n\nOpponent modeling in deep reinforcement learning. He He, Jordan Boyd-Graber, Kevin Kwok, Hal Daum\u00e9, Iii , Proceedings of The 33rd International Conference on Machine Learning. Maria Florina Balcan and Kilian Q. WeinbergerThe 33rd International Conference on Machine LearningNew York, New York, USAPMLR48He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum\u00e9 III. 2016. Opponent modeling in deep reinforcement learning. In Maria Flo- rina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Confer- ence on Machine Learning. PMLR, New York, New York, USA, volume 48 of Proceedings of Machine Learning Research, pages 1804-1813. http://proceedings.mlr.press/v48/he16.html.\n\nTeaching machines to read and comprehend. Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in Neural Information Processing Systems. Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teach- ing machines to read and comprehend. In Ad- vances in Neural Information Processing Systems. http://arxiv.org/abs/1506.03340.\n\nThe goldilocks principle: Reading children's books with explicit memory representations. Felix Hill, Antoine Bordes, Sumit Chopra, Jason Weston, Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children's books with explicit memory representa- tions. CoRR https://arxiv.org/abs/1511.02301.\n\nKnowledge-based weak supervision for information extraction of overlapping relations. Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S Weld, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational LinguisticsPortland, Oregon, USARaphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceed- ings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Lin- guistics, Portland, Oregon, USA, pages 541-550. http://www.aclweb.org/anthology/P11-1055.\n\nA neural network for factoid question answering over paragraphs. Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, Hal Daum\u00e9, Iii , Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational LinguisticsDoha, QatarMohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum\u00e9 III. 2014. A neural network for factoid question answering over paragraphs. In Proceedings of the 2014 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa- tional Linguistics, Doha, Qatar, pages 633-644. http://www.aclweb.org/anthology/D14-1070.\n\nKnowledge graph and corpus driven segmentation and answer inference for telegraphic entityseeking queries. Mandar Joshi, Uma Sawant, Soumen Chakrabarti, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational LinguisticsDoha, QatarMandar Joshi, Uma Sawant, and Soumen Chakrabarti. 2014. Knowledge graph and corpus driven segmen- tation and answer inference for telegraphic entity- seeking queries. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa- tional Linguistics, Doha, Qatar, pages 1104-1114. http://www.aclweb.org/anthology/D14-1117.\n\nRace: Large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy, Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. CoRR https://arxiv.org/abs/1704.04683.\n\nDataset and neural recurrent sequence labeling model for open-domain factoid question answering. Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, Wei Xu, Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. 2016. Dataset and neural recurrent sequence labeling model for open-domain factoid question answering. CoRR https://arxiv.org/abs/1607.06275.\n\nMS MARCO: A human generated machine reading comprehension dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, Workshop in Advances in Neural Information Processing Systems. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In Workshop in Ad- vances in Neural Information Processing Systems. https://arxiv.org/pdf/1611.09268.pdf.\n\nWho did what: A large-scale person-centered cloze dataset. Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, David Mcallester, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsTakeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim- pel, and David McAllester. 2016. Who did what: A large-scale person-centered cloze dataset. In Pro- ceedings of the 2016 Conference on Empirical Meth- ods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 2230-2235. https://aclweb.org/anthology/D16-\n\nThe lambada dataset: Word prediction requiring a broad discourse context. Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, Raquel Fernandez, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers). Association for Computational LinguisticsDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazari- dou, Ngoc Quan Pham, Raffaella Bernardi, San- dro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. 2016. The lambada dataset: Word prediction requiring a broad discourse con- text. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers). Association for Computa- tional Linguistics, Berlin, Germany, pages 1525- 1534. http://www.aclweb.org/anthology/P16-1144.\n\nCompositional semantic parsing on semi-structured tables. Panupong Pasupat, Percy Liang, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of thePanupong Pasupat and Percy Liang. 2015. Com- positional semantic parsing on semi-structured ta- bles. In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Lin- guistics and the 7th International Joint Confer- ence on Natural Language Processing of the\n\nAsian Federation of Natural Language Processing, ACL 2015. Beijing, ChinaLong Papers1Asian Federation of Natural Language Process- ing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers. pages 1470-1480. http://aclweb.org/anthology/P/P15/P15-1142.pdf.\n\nSquad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational LinguisticsAustin, TexasPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing. Association for Computa- tional Linguistics, Austin, Texas, pages 2383-2392. https://aclweb.org/anthology/D16-1264.\n\nMCTest: A challenge dataset for the open-domain machine comprehension of text. Matthew Richardson, J C Christopher, Erin Burges, Renshaw, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational LinguisticsSeattle, Washington, USAMatthew Richardson, Christopher J.C. Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro- cessing. Association for Computational Linguis- tics, Seattle, Washington, USA, pages 193-203. http://www.aclweb.org/anthology/D13-1020.\n\nModeling relations and their mentions without labeled text. Sebastian Riedel, Limin Yao, Andrew Mc-Callum, Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part III. the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part IIIBerlin, HeidelbergSpringer-VerlagECML PKDD'10Sebastian Riedel, Limin Yao, and Andrew Mc- Callum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the 2010 European Conference on Ma- chine Learning and Knowledge Discovery in Databases: Part III. Springer-Verlag, Berlin, Heidelberg, ECML PKDD'10, pages 148-163.\n\nLearning joint query interpretation and response ranking. Uma Sawant, Soumen Chakrabarti, 10.1145/2488388.2488484Proceedings of the 22Nd International Conference on World Wide Web. the 22Nd International Conference on World Wide WebNew York, NY, USAWWW '13Uma Sawant and Soumen Chakrabarti. 2013. Learn- ing joint query interpretation and response rank- ing. In Proceedings of the 22Nd International Conference on World Wide Web. ACM, New York, NY, USA, WWW '13, pages 1099-1110. https://doi.org/10.1145/2488388.2488484.\n\nBidirectional attention flow for machine comprehension. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In Proceedings of the International Conference on Learning Represen- tations (ICLR). https://arxiv.org/abs/1611.01603.\n\nNewsqa: A machine comprehension dataset. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, Kaheer Suleman, Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bach- man, and Kaheer Suleman. 2016. Newsqa: A machine comprehension dataset. CoRR https://arxiv.org/abs/1611.09830.\n\nBuilding a question answering test collection. Ellen M Voorhees, Dawn M Tice, 10.1145/345508.345577Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. the 23rd Annual International ACM SIGIR Conference on Research and Development in Information RetrievalNew York, NY, USA, SIGIR '00ACMEllen M. Voorhees and Dawn M. Tice. 2000. Build- ing a question answering test collection. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and De- velopment in Information Retrieval. ACM, New York, NY, USA, SIGIR '00, pages 200-207. https://doi.org/10.1145/345508.345577.\n\nMachine comprehension with syntax, frames, and semantics. Hai Wang, Mohit Bansal, Kevin Gimpel, David Mcallester, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaShort Papers). Association for Computational LinguisticsHai Wang, Mohit Bansal, Kevin Gimpel, and David McAllester. 2015. Machine comprehension with syntax, frames, and semantics. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol- ume 2: Short Papers). Association for Computa- tional Linguistics, Beijing, China, pages 700-706. http://www.aclweb.org/anthology/P15-2115.\n\nAdapting boosting for information retrieval measures. Qiang Wu, Christopher J Burges, Krysta M Svore, Jianfeng Gao, 10.1007/s10791-009-9112-1Inf. Retr. 133Qiang Wu, Christopher J. Burges, Krysta M. Svore, and Jianfeng Gao. 2010. Adapting boosting for infor- mation retrieval measures. Inf. Retr. 13(3):254-270. https://doi.org/10.1007/s10791-009-9112-1.\n\nShow, attend and tell: Neural image caption generation with visual attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. Show, at- tend and tell: Neural image caption generation with visual attention. In Proceedings of the International Conference on Machine Learning. https://arxiv.org/abs/1502.03044.\n\nWikiqa: A challenge dataset for open-domain question answering. Yi Yang, Wen-Tau Yih, Christopher Meek, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsYi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: A challenge dataset for open-domain ques- tion answering. In Proceedings of the 2015 Con- ference on Empirical Methods in Natural Lan- guage Processing. Association for Computational Linguistics, Lisbon, Portugal, pages 2013-2018. http://aclweb.org/anthology/D15-1237.\n\nHierarchical attention networks for document classification. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational LinguisticsSan Diego, CaliforniaZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierar- chical attention networks for document classifica- tion. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Lin- guistics, San Diego, California, pages 1480-1489. http://www.aclweb.org/anthology/N16-1174.\n", "annotations": {"author": "[{\"end\":261,\"start\":92},{\"end\":428,\"start\":262},{\"end\":595,\"start\":429},{\"end\":760,\"start\":596},{\"end\":903,\"start\":761}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":99},{\"end\":273,\"start\":269},{\"end\":442,\"start\":438},{\"end\":612,\"start\":601},{\"end\":773,\"start\":768}]", "author_first_name": "[{\"end\":98,\"start\":92},{\"end\":268,\"start\":262},{\"end\":435,\"start\":429},{\"end\":437,\"start\":436},{\"end\":600,\"start\":596},{\"end\":765,\"start\":761},{\"end\":767,\"start\":766}]", "author_affiliation": "[{\"end\":260,\"start\":133},{\"end\":427,\"start\":300},{\"end\":594,\"start\":467},{\"end\":759,\"start\":632},{\"end\":902,\"start\":775}]", "title": "[{\"end\":89,\"start\":1},{\"end\":992,\"start\":904}]", "venue": null, "abstract": "[{\"end\":1984,\"start\":994}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3104,\"start\":3080},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3179,\"start\":3157},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3198,\"start\":3179},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3360,\"start\":3336},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3391,\"start\":3370},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3423,\"start\":3399},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3452,\"start\":3433},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3483,\"start\":3458},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5168,\"start\":5144},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5405,\"start\":5386},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5429,\"start\":5405},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5495,\"start\":5477},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7132,\"start\":7108},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7346,\"start\":7324},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7402,\"start\":7378},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9023,\"start\":9002},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9045,\"start\":9023},{\"end\":9369,\"start\":9368},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10905,\"start\":10875},{\"end\":12555,\"start\":12518},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14928,\"start\":14909},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14946,\"start\":14928},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14993,\"start\":14975},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15968,\"start\":15951},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16094,\"start\":16075},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16112,\"start\":16094},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16480,\"start\":16458},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16498,\"start\":16480},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16611,\"start\":16593},{\"end\":17344,\"start\":17343},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22477,\"start\":22458},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22498,\"start\":22477},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22519,\"start\":22498},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22539,\"start\":22519},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22816,\"start\":22791},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22848,\"start\":22824},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22885,\"start\":22861},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23501,\"start\":23483},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23597,\"start\":23576},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24052,\"start\":24031},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24072,\"start\":24052},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24285,\"start\":24260},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24470,\"start\":24447},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24524,\"start\":24505},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24630,\"start\":24611},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25115,\"start\":25095},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25154,\"start\":25129},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25216,\"start\":25186},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25235,\"start\":25216},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25251,\"start\":25235},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25658,\"start\":25632},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25876,\"start\":25856},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25971,\"start\":25945}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27108,\"start\":27028},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27150,\"start\":27109},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27728,\"start\":27151},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":29132,\"start\":27729},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":29200,\"start\":29133},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31358,\"start\":29201},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31587,\"start\":31359},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":31937,\"start\":31588},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":32416,\"start\":31938}]", "paragraph": "[{\"end\":2849,\"start\":2000},{\"end\":4101,\"start\":2851},{\"end\":5243,\"start\":4103},{\"end\":5813,\"start\":5245},{\"end\":6254,\"start\":5815},{\"end\":6374,\"start\":6256},{\"end\":6519,\"start\":6376},{\"end\":6766,\"start\":6521},{\"end\":7403,\"start\":6779},{\"end\":8082,\"start\":7405},{\"end\":8774,\"start\":8084},{\"end\":9673,\"start\":8776},{\"end\":10671,\"start\":9696},{\"end\":11017,\"start\":10673},{\"end\":13420,\"start\":11019},{\"end\":13984,\"start\":13422},{\"end\":14687,\"start\":13986},{\"end\":15051,\"start\":14708},{\"end\":15555,\"start\":15078},{\"end\":15800,\"start\":15577},{\"end\":16011,\"start\":15853},{\"end\":16402,\"start\":16013},{\"end\":17700,\"start\":16419},{\"end\":17773,\"start\":17723},{\"end\":18075,\"start\":17789},{\"end\":18420,\"start\":18098},{\"end\":19013,\"start\":18422},{\"end\":19782,\"start\":19036},{\"end\":20246,\"start\":19794},{\"end\":20625,\"start\":20248},{\"end\":21285,\"start\":20644},{\"end\":21617,\"start\":21287},{\"end\":21903,\"start\":21619},{\"end\":22251,\"start\":21920},{\"end\":22733,\"start\":22277},{\"end\":23502,\"start\":22735},{\"end\":23881,\"start\":23537},{\"end\":24182,\"start\":23883},{\"end\":24960,\"start\":24184},{\"end\":25538,\"start\":24962},{\"end\":26243,\"start\":25540},{\"end\":26352,\"start\":26274},{\"end\":26773,\"start\":26354},{\"end\":27027,\"start\":26775}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15852,\"start\":15801},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17722,\"start\":17701}]", "table_ref": "[{\"end\":3212,\"start\":3205},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":11728,\"start\":11721},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":12029,\"start\":12022},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":13983,\"start\":13976},{\"end\":17938,\"start\":17931},{\"end\":19289,\"start\":19281},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":19857,\"start\":19850},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":21373,\"start\":21366},{\"end\":22250,\"start\":22243}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1998,\"start\":1986},{\"attributes\":{\"n\":\"2\"},\"end\":6777,\"start\":6769},{\"attributes\":{\"n\":\"3\"},\"end\":9694,\"start\":9676},{\"attributes\":{\"n\":\"5\"},\"end\":14706,\"start\":14690},{\"attributes\":{\"n\":\"5.1\"},\"end\":15076,\"start\":15054},{\"attributes\":{\"n\":\"5.2\"},\"end\":15575,\"start\":15558},{\"attributes\":{\"n\":\"5.3\"},\"end\":16417,\"start\":16405},{\"attributes\":{\"n\":\"6\"},\"end\":17787,\"start\":17776},{\"attributes\":{\"n\":\"6.1\"},\"end\":18096,\"start\":18078},{\"attributes\":{\"n\":\"6.2\"},\"end\":19034,\"start\":19016},{\"attributes\":{\"n\":\"6.3\"},\"end\":19792,\"start\":19785},{\"attributes\":{\"n\":\"6.4\"},\"end\":20642,\"start\":20628},{\"attributes\":{\"n\":\"7\"},\"end\":21918,\"start\":21906},{\"attributes\":{\"n\":\"7.1\"},\"end\":22275,\"start\":22254},{\"attributes\":{\"n\":\"7.2\"},\"end\":23535,\"start\":23505},{\"attributes\":{\"n\":\"8\"},\"end\":26272,\"start\":26246},{\"end\":27119,\"start\":27110},{\"end\":27739,\"start\":27730},{\"end\":29143,\"start\":29134},{\"end\":31369,\"start\":31360},{\"end\":31598,\"start\":31589},{\"end\":31948,\"start\":31939}]", "table": "[{\"end\":27728,\"start\":27217},{\"end\":29132,\"start\":28566},{\"end\":31358,\"start\":30639},{\"end\":31937,\"start\":31726},{\"end\":32416,\"start\":31950}]", "figure_caption": "[{\"end\":27108,\"start\":27030},{\"end\":27150,\"start\":27121},{\"end\":27217,\"start\":27153},{\"end\":28566,\"start\":27741},{\"end\":29200,\"start\":29145},{\"end\":30639,\"start\":29203},{\"end\":31587,\"start\":31371},{\"end\":31726,\"start\":31600}]", "figure_ref": "[{\"end\":2436,\"start\":2428},{\"end\":2687,\"start\":2679},{\"end\":9423,\"start\":9415}]", "bib_author_first_name": "[{\"end\":33809,\"start\":33801},{\"end\":33824,\"start\":33818},{\"end\":33834,\"start\":33831},{\"end\":33849,\"start\":33844},{\"end\":34610,\"start\":34603},{\"end\":34626,\"start\":34619},{\"end\":34641,\"start\":34636},{\"end\":34655,\"start\":34650},{\"end\":34951,\"start\":34945},{\"end\":34972,\"start\":34965},{\"end\":34985,\"start\":34983},{\"end\":34993,\"start\":34990},{\"end\":35004,\"start\":35001},{\"end\":35818,\"start\":35810},{\"end\":35833,\"start\":35824},{\"end\":36430,\"start\":36425},{\"end\":36442,\"start\":36437},{\"end\":36462,\"start\":36451},{\"end\":36464,\"start\":36463},{\"end\":37159,\"start\":37152},{\"end\":37172,\"start\":37166},{\"end\":37184,\"start\":37180},{\"end\":37198,\"start\":37194},{\"end\":37212,\"start\":37206},{\"end\":37229,\"start\":37220},{\"end\":37551,\"start\":37544},{\"end\":37563,\"start\":37559},{\"end\":37581,\"start\":37577},{\"end\":38220,\"start\":38215},{\"end\":38235,\"start\":38232},{\"end\":38837,\"start\":38832},{\"end\":38852,\"start\":38848},{\"end\":38868,\"start\":38860},{\"end\":38887,\"start\":38882},{\"end\":38898,\"start\":38893},{\"end\":38913,\"start\":38907},{\"end\":38915,\"start\":38914},{\"end\":38931,\"start\":38927},{\"end\":38940,\"start\":38939},{\"end\":38948,\"start\":38941},{\"end\":38962,\"start\":38958},{\"end\":38975,\"start\":38971},{\"end\":38988,\"start\":38984},{\"end\":39005,\"start\":39000},{\"end\":39356,\"start\":39354},{\"end\":39367,\"start\":39361},{\"end\":39386,\"start\":39381},{\"end\":39396,\"start\":39393},{\"end\":39407,\"start\":39404},{\"end\":40048,\"start\":40044},{\"end\":40070,\"start\":40065},{\"end\":40086,\"start\":40080},{\"end\":40106,\"start\":40101},{\"end\":40121,\"start\":40117},{\"end\":40134,\"start\":40127},{\"end\":40149,\"start\":40145},{\"end\":40563,\"start\":40558},{\"end\":40577,\"start\":40570},{\"end\":40591,\"start\":40586},{\"end\":40605,\"start\":40600},{\"end\":40904,\"start\":40897},{\"end\":40921,\"start\":40915},{\"end\":40933,\"start\":40929},{\"end\":40944,\"start\":40940},{\"end\":40964,\"start\":40958},{\"end\":40966,\"start\":40965},{\"end\":41791,\"start\":41786},{\"end\":41805,\"start\":41799},{\"end\":41827,\"start\":41819},{\"end\":41845,\"start\":41838},{\"end\":41857,\"start\":41854},{\"end\":41868,\"start\":41865},{\"end\":42631,\"start\":42625},{\"end\":42642,\"start\":42639},{\"end\":42657,\"start\":42651},{\"end\":43402,\"start\":43396},{\"end\":43413,\"start\":43408},{\"end\":43426,\"start\":43419},{\"end\":43438,\"start\":43432},{\"end\":43451,\"start\":43445},{\"end\":43738,\"start\":43734},{\"end\":43746,\"start\":43743},{\"end\":43759,\"start\":43751},{\"end\":43771,\"start\":43764},{\"end\":43782,\"start\":43778},{\"end\":43791,\"start\":43788},{\"end\":43801,\"start\":43798},{\"end\":44095,\"start\":44092},{\"end\":44107,\"start\":44104},{\"end\":44122,\"start\":44119},{\"end\":44137,\"start\":44129},{\"end\":44150,\"start\":44143},{\"end\":44165,\"start\":44159},{\"end\":44178,\"start\":44176},{\"end\":44591,\"start\":44584},{\"end\":44603,\"start\":44600},{\"end\":44615,\"start\":44610},{\"end\":44629,\"start\":44624},{\"end\":44643,\"start\":44638},{\"end\":45296,\"start\":45291},{\"end\":45312,\"start\":45306},{\"end\":45333,\"start\":45325},{\"end\":45349,\"start\":45345},{\"end\":45354,\"start\":45350},{\"end\":45370,\"start\":45361},{\"end\":45387,\"start\":45381},{\"end\":45403,\"start\":45398},{\"end\":45417,\"start\":45412},{\"end\":45432,\"start\":45426},{\"end\":46225,\"start\":46217},{\"end\":46240,\"start\":46235},{\"end\":47188,\"start\":47182},{\"end\":47204,\"start\":47200},{\"end\":47222,\"start\":47212},{\"end\":47237,\"start\":47232},{\"end\":47933,\"start\":47926},{\"end\":47947,\"start\":47946},{\"end\":47949,\"start\":47948},{\"end\":47967,\"start\":47963},{\"end\":48693,\"start\":48684},{\"end\":48707,\"start\":48702},{\"end\":48719,\"start\":48713},{\"end\":49346,\"start\":49343},{\"end\":49361,\"start\":49355},{\"end\":49870,\"start\":49863},{\"end\":49885,\"start\":49876},{\"end\":49899,\"start\":49896},{\"end\":49917,\"start\":49909},{\"end\":50371,\"start\":50367},{\"end\":50387,\"start\":50383},{\"end\":50400,\"start\":50394},{\"end\":50413,\"start\":50407},{\"end\":50432,\"start\":50422},{\"end\":50448,\"start\":50442},{\"end\":50464,\"start\":50458},{\"end\":50726,\"start\":50721},{\"end\":50728,\"start\":50727},{\"end\":50743,\"start\":50739},{\"end\":50745,\"start\":50744},{\"end\":51394,\"start\":51391},{\"end\":51406,\"start\":51401},{\"end\":51420,\"start\":51415},{\"end\":51434,\"start\":51429},{\"end\":52323,\"start\":52318},{\"end\":52339,\"start\":52328},{\"end\":52341,\"start\":52340},{\"end\":52356,\"start\":52350},{\"end\":52358,\"start\":52357},{\"end\":52374,\"start\":52366},{\"end\":52703,\"start\":52697},{\"end\":52713,\"start\":52708},{\"end\":52722,\"start\":52718},{\"end\":52739,\"start\":52730},{\"end\":52750,\"start\":52745},{\"end\":52768,\"start\":52762},{\"end\":52791,\"start\":52784},{\"end\":52805,\"start\":52799},{\"end\":53303,\"start\":53301},{\"end\":53317,\"start\":53310},{\"end\":53334,\"start\":53323},{\"end\":53952,\"start\":53946},{\"end\":53963,\"start\":53959},{\"end\":53975,\"start\":53970},{\"end\":53990,\"start\":53982},{\"end\":53999,\"start\":53995},{\"end\":54013,\"start\":54007}]", "bib_author_last_name": "[{\"end\":33816,\"start\":33810},{\"end\":33829,\"start\":33825},{\"end\":33842,\"start\":33835},{\"end\":33855,\"start\":33850},{\"end\":34617,\"start\":34611},{\"end\":34634,\"start\":34627},{\"end\":34648,\"start\":34642},{\"end\":34662,\"start\":34656},{\"end\":34963,\"start\":34952},{\"end\":34981,\"start\":34973},{\"end\":34988,\"start\":34986},{\"end\":34999,\"start\":34994},{\"end\":35822,\"start\":35819},{\"end\":35839,\"start\":35834},{\"end\":36435,\"start\":36431},{\"end\":36449,\"start\":36443},{\"end\":36472,\"start\":36465},{\"end\":37164,\"start\":37160},{\"end\":37178,\"start\":37173},{\"end\":37192,\"start\":37185},{\"end\":37204,\"start\":37199},{\"end\":37218,\"start\":37213},{\"end\":37233,\"start\":37230},{\"end\":37557,\"start\":37552},{\"end\":37575,\"start\":37564},{\"end\":37589,\"start\":37582},{\"end\":38230,\"start\":38221},{\"end\":38244,\"start\":38236},{\"end\":38846,\"start\":38838},{\"end\":38858,\"start\":38853},{\"end\":38880,\"start\":38869},{\"end\":38891,\"start\":38888},{\"end\":38905,\"start\":38899},{\"end\":38925,\"start\":38916},{\"end\":38937,\"start\":38932},{\"end\":38956,\"start\":38949},{\"end\":38969,\"start\":38963},{\"end\":38982,\"start\":38976},{\"end\":38998,\"start\":38989},{\"end\":39011,\"start\":39006},{\"end\":39359,\"start\":39357},{\"end\":39379,\"start\":39368},{\"end\":39391,\"start\":39387},{\"end\":39402,\"start\":39397},{\"end\":40063,\"start\":40049},{\"end\":40078,\"start\":40071},{\"end\":40099,\"start\":40087},{\"end\":40115,\"start\":40107},{\"end\":40125,\"start\":40122},{\"end\":40143,\"start\":40135},{\"end\":40157,\"start\":40150},{\"end\":40568,\"start\":40564},{\"end\":40584,\"start\":40578},{\"end\":40598,\"start\":40592},{\"end\":40612,\"start\":40606},{\"end\":40913,\"start\":40905},{\"end\":40927,\"start\":40922},{\"end\":40938,\"start\":40934},{\"end\":40956,\"start\":40945},{\"end\":40971,\"start\":40967},{\"end\":41797,\"start\":41792},{\"end\":41817,\"start\":41806},{\"end\":41836,\"start\":41828},{\"end\":41852,\"start\":41846},{\"end\":41863,\"start\":41858},{\"end\":42637,\"start\":42632},{\"end\":42649,\"start\":42643},{\"end\":42669,\"start\":42658},{\"end\":43406,\"start\":43403},{\"end\":43417,\"start\":43414},{\"end\":43430,\"start\":43427},{\"end\":43443,\"start\":43439},{\"end\":43456,\"start\":43452},{\"end\":43741,\"start\":43739},{\"end\":43749,\"start\":43747},{\"end\":43762,\"start\":43760},{\"end\":43776,\"start\":43772},{\"end\":43786,\"start\":43783},{\"end\":43796,\"start\":43792},{\"end\":43804,\"start\":43802},{\"end\":44102,\"start\":44096},{\"end\":44117,\"start\":44108},{\"end\":44127,\"start\":44123},{\"end\":44141,\"start\":44138},{\"end\":44157,\"start\":44151},{\"end\":44174,\"start\":44166},{\"end\":44183,\"start\":44179},{\"end\":44598,\"start\":44592},{\"end\":44608,\"start\":44604},{\"end\":44622,\"start\":44616},{\"end\":44636,\"start\":44630},{\"end\":44654,\"start\":44644},{\"end\":45304,\"start\":45297},{\"end\":45323,\"start\":45313},{\"end\":45343,\"start\":45334},{\"end\":45359,\"start\":45355},{\"end\":45379,\"start\":45371},{\"end\":45396,\"start\":45388},{\"end\":45410,\"start\":45404},{\"end\":45424,\"start\":45418},{\"end\":45442,\"start\":45433},{\"end\":46233,\"start\":46226},{\"end\":46246,\"start\":46241},{\"end\":47198,\"start\":47189},{\"end\":47210,\"start\":47205},{\"end\":47230,\"start\":47223},{\"end\":47243,\"start\":47238},{\"end\":47944,\"start\":47934},{\"end\":47961,\"start\":47950},{\"end\":47974,\"start\":47968},{\"end\":47983,\"start\":47976},{\"end\":48700,\"start\":48694},{\"end\":48711,\"start\":48708},{\"end\":48729,\"start\":48720},{\"end\":49353,\"start\":49347},{\"end\":49373,\"start\":49362},{\"end\":49874,\"start\":49871},{\"end\":49894,\"start\":49886},{\"end\":49907,\"start\":49900},{\"end\":49928,\"start\":49918},{\"end\":50381,\"start\":50372},{\"end\":50392,\"start\":50388},{\"end\":50405,\"start\":50401},{\"end\":50420,\"start\":50414},{\"end\":50440,\"start\":50433},{\"end\":50456,\"start\":50449},{\"end\":50472,\"start\":50465},{\"end\":50737,\"start\":50729},{\"end\":50750,\"start\":50746},{\"end\":51399,\"start\":51395},{\"end\":51413,\"start\":51407},{\"end\":51427,\"start\":51421},{\"end\":51445,\"start\":51435},{\"end\":52326,\"start\":52324},{\"end\":52348,\"start\":52342},{\"end\":52364,\"start\":52359},{\"end\":52378,\"start\":52375},{\"end\":52706,\"start\":52704},{\"end\":52716,\"start\":52714},{\"end\":52728,\"start\":52723},{\"end\":52743,\"start\":52740},{\"end\":52760,\"start\":52751},{\"end\":52782,\"start\":52769},{\"end\":52797,\"start\":52792},{\"end\":52812,\"start\":52806},{\"end\":53308,\"start\":53304},{\"end\":53321,\"start\":53318},{\"end\":53339,\"start\":53335},{\"end\":53957,\"start\":53953},{\"end\":53968,\"start\":53964},{\"end\":53980,\"start\":53976},{\"end\":53993,\"start\":53991},{\"end\":54005,\"start\":54000},{\"end\":54018,\"start\":54014}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6401679},\"end\":34541,\"start\":33744},{\"attributes\":{\"doi\":\"CoRR abs/1506.02075\",\"id\":\"b1\"},\"end\":34870,\"start\":34543},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":215514151},\"end\":35736,\"start\":34872},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2265838},\"end\":36421,\"start\":35738},{\"attributes\":{\"id\":\"b4\"},\"end\":36594,\"start\":36423},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6360322},\"end\":37150,\"start\":36596},{\"attributes\":{\"id\":\"b6\"},\"end\":37474,\"start\":37152},{\"attributes\":{\"doi\":\"10.1145/2623330.2623677\",\"id\":\"b7\",\"matched_paper_id\":207214527},\"end\":38135,\"start\":37476},{\"attributes\":{\"doi\":\"10.1145/1871437.1871689\",\"id\":\"b8\",\"matched_paper_id\":16178102},\"end\":38778,\"start\":38137},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1831060},\"end\":39302,\"start\":38780},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3393747},\"end\":40000,\"start\":39304},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6203757},\"end\":40467,\"start\":40002},{\"attributes\":{\"id\":\"b12\"},\"end\":40809,\"start\":40469},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16483125},\"end\":41719,\"start\":40811},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":216034672},\"end\":42516,\"start\":41721},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9526475},\"end\":43327,\"start\":42518},{\"attributes\":{\"id\":\"b16\"},\"end\":43635,\"start\":43329},{\"attributes\":{\"id\":\"b17\"},\"end\":44023,\"start\":43637},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1289517},\"end\":44523,\"start\":44025},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5761781},\"end\":45215,\"start\":44525},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2381275},\"end\":46157,\"start\":45217},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":9027681},\"end\":46850,\"start\":46159},{\"attributes\":{\"id\":\"b22\"},\"end\":47119,\"start\":46852},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11816014},\"end\":47845,\"start\":47121},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2100831},\"end\":48622,\"start\":47847},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2386383},\"end\":49283,\"start\":48624},{\"attributes\":{\"doi\":\"10.1145/2488388.2488484\",\"id\":\"b26\",\"matched_paper_id\":6858396},\"end\":49805,\"start\":49285},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8535316},\"end\":50324,\"start\":49807},{\"attributes\":{\"id\":\"b28\"},\"end\":50672,\"start\":50326},{\"attributes\":{\"doi\":\"10.1145/345508.345577\",\"id\":\"b29\",\"matched_paper_id\":11465263},\"end\":51331,\"start\":50674},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":8764466},\"end\":52262,\"start\":51333},{\"attributes\":{\"doi\":\"10.1007/s10791-009-9112-1\",\"id\":\"b31\",\"matched_paper_id\":5552139},\"end\":52617,\"start\":52264},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1055111},\"end\":53235,\"start\":52619},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1373518},\"end\":53883,\"start\":53237},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6857205},\"end\":54820,\"start\":53885}]", "bib_title": "[{\"end\":33799,\"start\":33744},{\"end\":34943,\"start\":34872},{\"end\":35808,\"start\":35738},{\"end\":36637,\"start\":36596},{\"end\":37542,\"start\":37476},{\"end\":38213,\"start\":38137},{\"end\":38830,\"start\":38780},{\"end\":39352,\"start\":39304},{\"end\":40042,\"start\":40002},{\"end\":40895,\"start\":40811},{\"end\":41784,\"start\":41721},{\"end\":42623,\"start\":42518},{\"end\":44090,\"start\":44025},{\"end\":44582,\"start\":44525},{\"end\":45289,\"start\":45217},{\"end\":46215,\"start\":46159},{\"end\":47180,\"start\":47121},{\"end\":47924,\"start\":47847},{\"end\":48682,\"start\":48624},{\"end\":49341,\"start\":49285},{\"end\":49861,\"start\":49807},{\"end\":50719,\"start\":50674},{\"end\":51389,\"start\":51333},{\"end\":52316,\"start\":52264},{\"end\":52695,\"start\":52619},{\"end\":53299,\"start\":53237},{\"end\":53944,\"start\":53885}]", "bib_author": "[{\"end\":33818,\"start\":33801},{\"end\":33831,\"start\":33818},{\"end\":33844,\"start\":33831},{\"end\":33857,\"start\":33844},{\"end\":34619,\"start\":34603},{\"end\":34636,\"start\":34619},{\"end\":34650,\"start\":34636},{\"end\":34664,\"start\":34650},{\"end\":34965,\"start\":34945},{\"end\":34983,\"start\":34965},{\"end\":34990,\"start\":34983},{\"end\":35001,\"start\":34990},{\"end\":35007,\"start\":35001},{\"end\":35824,\"start\":35810},{\"end\":35841,\"start\":35824},{\"end\":36437,\"start\":36425},{\"end\":36451,\"start\":36437},{\"end\":36474,\"start\":36451},{\"end\":37166,\"start\":37152},{\"end\":37180,\"start\":37166},{\"end\":37194,\"start\":37180},{\"end\":37206,\"start\":37194},{\"end\":37220,\"start\":37206},{\"end\":37235,\"start\":37220},{\"end\":37559,\"start\":37544},{\"end\":37577,\"start\":37559},{\"end\":37591,\"start\":37577},{\"end\":38232,\"start\":38215},{\"end\":38246,\"start\":38232},{\"end\":38848,\"start\":38832},{\"end\":38860,\"start\":38848},{\"end\":38882,\"start\":38860},{\"end\":38893,\"start\":38882},{\"end\":38907,\"start\":38893},{\"end\":38927,\"start\":38907},{\"end\":38939,\"start\":38927},{\"end\":38958,\"start\":38939},{\"end\":38971,\"start\":38958},{\"end\":38984,\"start\":38971},{\"end\":39000,\"start\":38984},{\"end\":39013,\"start\":39000},{\"end\":39361,\"start\":39354},{\"end\":39381,\"start\":39361},{\"end\":39393,\"start\":39381},{\"end\":39404,\"start\":39393},{\"end\":39410,\"start\":39404},{\"end\":40065,\"start\":40044},{\"end\":40080,\"start\":40065},{\"end\":40101,\"start\":40080},{\"end\":40117,\"start\":40101},{\"end\":40127,\"start\":40117},{\"end\":40145,\"start\":40127},{\"end\":40159,\"start\":40145},{\"end\":40570,\"start\":40558},{\"end\":40586,\"start\":40570},{\"end\":40600,\"start\":40586},{\"end\":40614,\"start\":40600},{\"end\":40915,\"start\":40897},{\"end\":40929,\"start\":40915},{\"end\":40940,\"start\":40929},{\"end\":40958,\"start\":40940},{\"end\":40973,\"start\":40958},{\"end\":41799,\"start\":41786},{\"end\":41819,\"start\":41799},{\"end\":41838,\"start\":41819},{\"end\":41854,\"start\":41838},{\"end\":41865,\"start\":41854},{\"end\":41871,\"start\":41865},{\"end\":42639,\"start\":42625},{\"end\":42651,\"start\":42639},{\"end\":42671,\"start\":42651},{\"end\":43408,\"start\":43396},{\"end\":43419,\"start\":43408},{\"end\":43432,\"start\":43419},{\"end\":43445,\"start\":43432},{\"end\":43458,\"start\":43445},{\"end\":43743,\"start\":43734},{\"end\":43751,\"start\":43743},{\"end\":43764,\"start\":43751},{\"end\":43778,\"start\":43764},{\"end\":43788,\"start\":43778},{\"end\":43798,\"start\":43788},{\"end\":43806,\"start\":43798},{\"end\":44104,\"start\":44092},{\"end\":44119,\"start\":44104},{\"end\":44129,\"start\":44119},{\"end\":44143,\"start\":44129},{\"end\":44159,\"start\":44143},{\"end\":44176,\"start\":44159},{\"end\":44185,\"start\":44176},{\"end\":44600,\"start\":44584},{\"end\":44610,\"start\":44600},{\"end\":44624,\"start\":44610},{\"end\":44638,\"start\":44624},{\"end\":44656,\"start\":44638},{\"end\":45306,\"start\":45291},{\"end\":45325,\"start\":45306},{\"end\":45345,\"start\":45325},{\"end\":45361,\"start\":45345},{\"end\":45381,\"start\":45361},{\"end\":45398,\"start\":45381},{\"end\":45412,\"start\":45398},{\"end\":45426,\"start\":45412},{\"end\":45444,\"start\":45426},{\"end\":46235,\"start\":46217},{\"end\":46248,\"start\":46235},{\"end\":47200,\"start\":47182},{\"end\":47212,\"start\":47200},{\"end\":47232,\"start\":47212},{\"end\":47245,\"start\":47232},{\"end\":47946,\"start\":47926},{\"end\":47963,\"start\":47946},{\"end\":47976,\"start\":47963},{\"end\":47985,\"start\":47976},{\"end\":48702,\"start\":48684},{\"end\":48713,\"start\":48702},{\"end\":48731,\"start\":48713},{\"end\":49355,\"start\":49343},{\"end\":49375,\"start\":49355},{\"end\":49876,\"start\":49863},{\"end\":49896,\"start\":49876},{\"end\":49909,\"start\":49896},{\"end\":49930,\"start\":49909},{\"end\":50383,\"start\":50367},{\"end\":50394,\"start\":50383},{\"end\":50407,\"start\":50394},{\"end\":50422,\"start\":50407},{\"end\":50442,\"start\":50422},{\"end\":50458,\"start\":50442},{\"end\":50474,\"start\":50458},{\"end\":50739,\"start\":50721},{\"end\":50752,\"start\":50739},{\"end\":51401,\"start\":51391},{\"end\":51415,\"start\":51401},{\"end\":51429,\"start\":51415},{\"end\":51447,\"start\":51429},{\"end\":52328,\"start\":52318},{\"end\":52350,\"start\":52328},{\"end\":52366,\"start\":52350},{\"end\":52380,\"start\":52366},{\"end\":52708,\"start\":52697},{\"end\":52718,\"start\":52708},{\"end\":52730,\"start\":52718},{\"end\":52745,\"start\":52730},{\"end\":52762,\"start\":52745},{\"end\":52784,\"start\":52762},{\"end\":52799,\"start\":52784},{\"end\":52814,\"start\":52799},{\"end\":53310,\"start\":53301},{\"end\":53323,\"start\":53310},{\"end\":53341,\"start\":53323},{\"end\":53959,\"start\":53946},{\"end\":53970,\"start\":53959},{\"end\":53982,\"start\":53970},{\"end\":53995,\"start\":53982},{\"end\":54007,\"start\":53995},{\"end\":54020,\"start\":54007}]", "bib_venue": "[{\"end\":33943,\"start\":33857},{\"end\":34601,\"start\":34543},{\"end\":35143,\"start\":35007},{\"end\":35928,\"start\":35841},{\"end\":36726,\"start\":36639},{\"end\":37266,\"start\":37235},{\"end\":37712,\"start\":37614},{\"end\":38361,\"start\":38269},{\"end\":39024,\"start\":39013},{\"end\":39478,\"start\":39410},{\"end\":40208,\"start\":40159},{\"end\":40556,\"start\":40469},{\"end\":41132,\"start\":40973},{\"end\":42008,\"start\":41871},{\"end\":42808,\"start\":42671},{\"end\":43394,\"start\":43329},{\"end\":43732,\"start\":43637},{\"end\":44246,\"start\":44185},{\"end\":44742,\"start\":44656},{\"end\":45531,\"start\":45444},{\"end\":46416,\"start\":46248},{\"end\":46909,\"start\":46852},{\"end\":47374,\"start\":47245},{\"end\":48114,\"start\":47985},{\"end\":48841,\"start\":48731},{\"end\":49464,\"start\":49398},{\"end\":50008,\"start\":49930},{\"end\":50365,\"start\":50326},{\"end\":50891,\"start\":50773},{\"end\":51608,\"start\":51447},{\"end\":52414,\"start\":52405},{\"end\":52877,\"start\":52814},{\"end\":53427,\"start\":53341},{\"end\":54205,\"start\":54020},{\"end\":34061,\"start\":33945},{\"end\":35284,\"start\":35145},{\"end\":36017,\"start\":35930},{\"end\":36815,\"start\":36728},{\"end\":37814,\"start\":37714},{\"end\":38467,\"start\":38363},{\"end\":39601,\"start\":39525},{\"end\":41299,\"start\":41134},{\"end\":42143,\"start\":42010},{\"end\":42943,\"start\":42810},{\"end\":44828,\"start\":44744},{\"end\":45620,\"start\":45533},{\"end\":46571,\"start\":46418},{\"end\":46925,\"start\":46911},{\"end\":47503,\"start\":47376},{\"end\":48254,\"start\":48116},{\"end\":48956,\"start\":48843},{\"end\":49534,\"start\":49466},{\"end\":50073,\"start\":50010},{\"end\":51024,\"start\":50893},{\"end\":51770,\"start\":51610},{\"end\":52927,\"start\":52879},{\"end\":53516,\"start\":53429},{\"end\":54398,\"start\":54207}]"}}}, "year": 2023, "month": 12, "day": 17}