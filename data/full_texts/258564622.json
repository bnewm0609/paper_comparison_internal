{"id": 258564622, "updated": "2023-10-05 01:31:39.657", "metadata": {"title": "SwinIA: Self-Supervised Blind-Spot Image Denoising with Zero Convolutions", "authors": "[{\"first\":\"Mikhail\",\"last\":\"Papkov\",\"middle\":[]},{\"first\":\"Pavel\",\"last\":\"Chizhov\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "The essence of self-supervised image denoising is to restore the signal from the noisy image alone. State-of-the-art solutions for this task rely on the idea of masking pixels and training a fully-convolutional neural network to impute them. This most often requires multiple forward passes, information about the noise model, and intricate regularization functions. In this paper, we propose a Swin Transformer-based Image Autoencoder (SwinIA), the first convolution-free architecture for self-supervised denoising. It can be trained end-to-end with a simple mean squared error loss without masking and does not require any prior knowledge about clean data or noise distribution. Despite its simplicity, SwinIA establishes state-of-the-art on several common benchmarks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.05651", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-05651", "doi": "10.48550/arxiv.2305.05651"}}, "content": {"source": {"pdf_hash": "b17b4123bafb2dbca21352e9f3d2d8f0bdbe016d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.05651v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cd8971a0e13bd7f7763bfe6b0569d018df2132c0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b17b4123bafb2dbca21352e9f3d2d8f0bdbe016d.txt", "contents": "\nSwinIA: Self-Supervised Blind-Spot Image Denoising with Zero Convolutions\n\n\nMikhail Papkov mikhail.papkov@ut.ee \nInstitute of Computer Science\nUniversity of Tartu Tartu\nEstonia\n\nPavel Chizhov pavel.chizhov@ut.ee \nInstitute of Computer Science\nUniversity of Tartu Tartu\nEstonia\n\nSwinIA: Self-Supervised Blind-Spot Image Denoising with Zero Convolutions\n\nThe essence of self-supervised image denoising is to restore the signal from the noisy image alone. State-of-theart solutions for this task rely on the idea of masking pixels and training a fully-convolutional neural network to impute them. This most often requires multiple forward passes, information about the noise model, and intricate regularization functions. In this paper, we propose a Swin Transformer-based Image Autoencoder (SwinIA), the first convolution-free architecture for self-supervised denoising. It can be trained end-to-end with a simple mean squared error loss without masking and does not require any prior knowledge about clean data or noise distribution. Despite its simplicity, SwinIA establishes state-of-the-art on several common benchmarks.\n\nIntroduction\n\nImage denoising aims to reconstruct true signal given corrupted input. The corruption depends on the camera sensor, signal processor, and other aspects of the image acquisition procedure, and can take various forms such as Gaussian noise, Poisson noise, salt-and-pepper noise, etc. Noise levels also vary with illumination and exposure, and some amount is always present in any image. This makes denoising an integral part of image processing pipelines.\n\nSimilar to other fields in computer vision, deep learning solutions [14,2,36,34,25,18,37,33,17,24] have also superseded classical methods [4,5,22,3] for denoising. However, when approached naively, neural networks require huge amounts of paired noisy and clean images for supervised learning. Collecting such a dataset is usually impracticable. Lehtinen et al. proposed Noise2Noise [17] and showed that supervision with independently corrupted data is equivalent to supervision with clean data. However, this approach still requires collecting multiple image copies, which may not be present in existing datasets.\n\nSelf-supervised denoising avoids the demand for paired data since it learns from the training dataset itself. Devel-(a) Text token sequence. The representation of each token is built upon all other tokens, excluding the current one.\n\n(b) Image pixel grid. Each pixel representation is derived from all other pixels, treating its own value as a blind spot. oping Noise2Noise ideas, Noisier2Noise [24] and Recor-rupted2Recorrupted [25] applied additional noise on training images to emulate the strongly supervised Noise2Noise scenario. This was accomplished by assuming prior knowledge of the noise model. Self2Self [27] proposed training a network with Bernoulli input dropout and inference by ensembling outputs for several corrupted inputs. Neigh-bor2Neighbor [12] sub-sampled the input image and treated the result as independently corrupted copies for regularized training.\n\nNoise2Self [2] and Noise2Void [14] introduced a different approach to self-supervised denoising -a blindspot network (BSN). This type of network reconstructs a pixel from its neighborhood, assuming spatially independent zero-mean noise. It is practically difficult to dissect the continuous receptive field of convolutional neural networks (CNN), so BSNs usually use a masking procedure to hide a small portion of pixels by substitution [14] or random noise [2] and learn solely from them. However, learning from few data points per image slows down convergence, and different masking approaches may produce drastically different results [36]. Laine et al. [15] proposed to restrict the blindness by constructing four denoising branches with unidirectional receptive fields. In practice, this was achieved by passing four differently rotated input copies through the network. Later, Honz\u00e0tko et al. [11] and Wu et al. [35] adopted dilated convolutions to create a true BSN which does not require masking. We further discuss these methods in Section 2. Subsequent works abandoned the idea of strict pixel blindness and adopted multiple forward passes through the network. Noise2Same [36] makes two forward passes (one with a random mask [2], one without) and regularizes the training with invariance loss in masked pixel locations. Blind2Unblind [34] utilizes global masking and combines the denoising result from passing five images through the network (four masked images with the gradient and one unmasked without).\n\nMost recently, vision transformers started to outperform CNNs across a variety of benchmarks, including denoising. SwinIR [18], based on Swin Transformer [20], achieved state-of-the-art results in JPEG compression artifact reduction. Uformer [33] and Restormer [37] concurrently improved the results in camera noise removal. Evolution of vision transformers closely followed their path in natural language processing [8]. Bao et al. introduced BERT-style pre-training for image datasets [1], and He et al. showed that transformers are able to confidently reconstruct from the context up to 95% of hidden data in a masked autoencoder fashion [10]. These results hint that similar performance gains from flexibly accounting for long-range interactions could also benefit denoising models.\n\nIn this paper, we propose Swin Transformer-based Image Autoencoder (SwinIA), the first fully transformer-based architecture for self-supervised image denoising. SwinIA does not require any prior knowledge of noise distribution. It also does not have access to clean images, either through pre-training or knowledge distillation. Neither does it use input masking, auxiliary regularization losses, or multiple forward passes. Instead, SwinIA is trained as a plain autoencoder by minimizing the mean squared error (MSE) computed over the full image. We rigorously test our SwinIA method on a variety of synthetic and real-world datasets and demonstrate its competitiveness against stateof-the-art self-supervised denoising solutions.\n\n\nRelated work\n\nWe further describe the foundational ideas of blind spot networks and denoising vision transformers that our model is related to. We introduce their properties and usual training schemes and give the context for our advances.\n\n\nBlind-spot networks\n\nThe blind-spot property is usually achieved by masking [14,2,16,34] or multiple forward passes through the network [15,34]. These techniques overcome the continuous receptive field issue of CNNs. It is also possible to maintain blindness with increasingly dilating convolutions. Honz\u00e0tko et al. [11] proposed a blind-spot convolutional layer with a virtual \"hole\" in the kernel center. They designed an architecture with ten such layers in the decoder aggregating information from the respective levels of an encoder. This work followed the Laine et al. [15] training setup and demonstrated similar performance on sRGB datasets. The main limitation of this approach is the assumption that the noise distribution is known, and the network is merely tasked with estimating its parameters. Finally, Wu et al. [35] also utilized a dilated blind-spot network (DBSN) in a multi-stage pipeline with clean images provided via knowledge distillation.\n\n\nDenoising vision transformers\n\nTransformers are widely used for image restoration in the supervised setting [18,33,37], but rarely for self-supervised denoising.\n\nZhang et al. proposed a Context-aware Denoise Transformer (CADT) [39] based on SwinIR [18] and Blind2Unblind [34] masking. They used Swin Transformer [20] blocks in the global branch of the network and trained them with patch embeddings, not pixel embeddings. However, they argued that a transformer alone is not suitable for the task and thus complemented it with convolutional local feature extraction. Liu et al. [19] built a single-image denoising transformer (DnT) from selfattention blocks interleaved with convolutional layers. This architecture was not tested for self-supervised denoising on large datasets.\n\n\nDesign\n\nWe were motivated by the performance of transformers on text and image data to develop them for a pure selfunaware denoising model.\n\n\nMotivation\n\nBSN was proposed many years ago [14,2], but to date, no implementation strictly adhering to the original idea has been proposed. Existing solutions use masking [14,2,16,34], assume known noise distributions [15,11] or learn from clean data through knowledge distillation [35]. Thus, creating an assumption-free BSN that is trained end-to-end as an autoencoder remains an open challenge. As transformers learn comprehensive representations from unstructured data, we hypothesize that a variant of transformer architecture is suitable for this task.\n\n\nInspiration\n\n\nTransformer-based image autoencoder\n\nShin et al. [31] introduced the idea of self-unaware text autoencoding using transformers (T-TA). They modified the transformer model so that each token representation is built based on all the other tokens, except itself, as in Figure 1a. T-TA builds text representations in one iteration without access to a token's own value, as opposed to the masked language modeling objective of BERT [7], where the tokens are processed one at a time. We transfer this idea to the image domain to create a vision transformer autoencoder with self-unaware pixel-level tokens (Figure 1b).\n\n\nSwin Transformer\n\nIn order to efficiently exploit a transformer-based model at the pixel level for high-dimensional image data, we need to use local attention. Swin Transformer [20] is a powerful multi-purpose vision model, which uses the (shifted) window multi-head self-attention ((S)W-MSA) mechanism that restricts self-attention to windows of fixed size. The windows are shifted from block to block to avoid bordering artifacts and spread the field of view [20].\n\nSwin Transformer was already efficiently utilized at the pixel level in SwinIR [18], where individual pixels were embedded into transformer tokens.\n\n\nRequirements\n\nCombining the aforementioned ideas, we formulate a list of requirements that guided us in the design of a blind-spot transformer. Self-unawareness. At any stage of the network, individual pixels should not have access to their own state on the previous levels. This is a primary feature of a blind-spot architecture that will prevent it from learning an identity function through minimizing MSE loss. Continuous field of view. Window transformers are widely used in image recognition tasks. However, they are usually rigid and therefore sometimes do not connect neighboring pixels. The usual diagonal window shift of Swin Transformer leads to some pixels never seeing their closest neighbors ( Figure 2), and thus denoising artifacts. We need to design a solution where each pixel has a symmetrical and continuous receptive field. Long-range interactions. Pooling layers downsample features and extract high-level information in the subsequent layers. However, in our setting, they disrupt input isolation by mixing together feature vectors of individual pixels. Therefore, we need a downsampling operation that enables attention between groups of pixels and at the same time, maintains the independence of each pixel. U-shaped architecture. U-Net is arguably the most common blueprint for denoising networks. It operates in multiple scales and can pass information from encoder to decoder through skip connections. We need to design an architecture with similar properties holding to the assumptions listed above.\n\n\nMethods\n\nIn this section, we describe the SwinIA model bottomup, justifying the design choices according to the listed requirements referenced in italics.\n\n\nTransformer block\n\nThe SwinIA transformer block ( Figure 4) is based on Swin Transformer [20]. In contrast with pixel-level SwinIR [18], SwinIA operates at the level of shuffle groups -flattened square patches in the image of size S \u00d7 S pixels. (S)W-MSA is computed between shuffle groups and is limited inside of a window of size M \u00d7 M shuffle groups. An example of a window with partition into shuffle groups is presented in Figure 3. As a specific case, S = 1 means operating at the pixel level. Shuffle groups establish the demanded downsampling for long-range interactions and, thus, allow for a larger receptive field.\n\nWe follow the ideas from T-TA [31] in satisfying the requirement of self-unawareness. In (S)W-MSA, we mask the main diagonal of the attention matrix, so that the SoftMax values there become infinitesimal. Thus, none of the pixels attend to their own values. To maintain the blind-spot property throughout the transformer, we need to ensure input isolation. Thus, we need to keep keys and values separate from the transformed query. They solely act as inputs for (S)W-MSA to influence the output of the block.\n\nAs in Swin Transformer [20], there are two shortcuts by addition around (S)W-MSA and the multi-layer perceptron (MLP). Layer Normalization, however, is done after the shortcuts, as in the classical Transformer model for natural language [32]. We also propose Shuffle Group Layer Normalization (SGLN) for the query, in which each shuffle group is normalized for each attention head before (S)W-MSA.\n\n\nResidual group\n\nA residual group consists of 4 transformer blocks (see Figure 4). To achieve input isolation, queries are propagated through the blocks sequentially, while keys and values are fixed and passed into each transformer block unchanged. The query is skip-connected around each block via concatenation and linear projection back to the original embedding dimension.\n\nAs Swin Transformer [20] violates the continuous field of view requirement (Figure 2), we propose a cyclic shifting scheme, when the window is shifted to the right, bottom, left, and back to the original in 4 consequent transformer blocks. An example demonstrating the difference between the two shifting schemes is given in Figure 2 \n\n\nSwinIA model\n\nThe full SwinIA model architecture is presented in Figure 5. The model has a hierarchical U-shaped structure, where shuffle S changes from layer to layer. Embedding. At the initial stage, the query is set to positional embeddings shared between all windows and attention heads. The input image is linearly projected separately into key and value and summed with the same positional embeddings. After this point, key and value stay unchanged and are passed as inputs into every residual group to maintain input isolation. Encoder. The encoder consists of three residual groups with S = 1, 2, 4. The first group, therefore, works directly with pixels, and the other two expand their attention to a broader context. Around each group, there exists a skip connection that merges with the output by concatenation and projection, similarly to the shortcuts within a residual group. In contrast with U-Net [29], where the tensors are propagated down through the encoder, in SwinIA we cannot pass the output of a block with shuffle S 1 as an input for a block with shuffle S 2 if S 1 < S 2 without breaking the rule of self-unawareness. Otherwise, there would exist mutual awareness between the elements of a single shuffle group coming from the previous layer. Thereby, each of the encoder shuffle groups takes identical inputs. Decoder. The decoder consists of two shuffle groups symmetrical to those of the encoder, excluding the bottom one. In contrast with the encoder, we now can sequentially propagate tensors from group to group, as shuffles decrease by the decoder flow. As in the encoder, there are concatenation-and-projection skip connections around the shuffle groups. Apart from that, the outputs of the corresponding encoder groups are merged into inputs of the decoder blocks, in this, we follow the idea of global skip connections in U-Net [29].\n\n\nExperimental results\n\nWe extensively test SwinIA against state-of-the-art selfsupervised denoising methods on synthetic and real-world data. Since SwinIA is a true BSN and inevitably loses information from the masking, we separately focus on comparison with methods with similar properties [15,35,11]. We use the common metrics of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) for evaluation. In addition to SwinIA, we implement and validate Noise2Self [2] and Noise2Same [36]. For the rest of the methods, we take the results from respective papers.\n\n\nTraining settings\n\nWe train SwinIA for 50 000 steps (unless mentioned otherwise) with batch size 64 and use Adam [13] with cosine annealing schedule [21] reducing learning rate from 10 \u22123 to 10 \u22126 . During training, we randomly crop 64 \u00d7 64 patches from images scaled to [0, 1], rotate them by multiples of 90 \u2022 , and flip them horizontally and vertically. Each patch is standardized independently for \u00b5 = 0 and \u03c3 = 1 except when synthetic Poisson noise is applied [36] During validation, we pad each image for divisibility by 32 with reflection and crop the padding after prediction. We use 144 embeddings throughout the network, each transformer block has 16 heads, window size is set to 8.\n\nWe implemented all models in Python 3.8.3 and PyTorch 1.12.1 [26], and trained them on NVIDIA A100 80GB GPUs (driver version: 470.57.02, CUDA version: 11.4). We used einops [28] library for shuffling and tensor partitioning to attention heads and windows.\n\n\nSynthetic noise (sRGB)\n\nWe follow [15,12,34] to create training and test sRGB datasets. For training, we select 44 328 images between 256 \u00d7 256 and 512 \u00d7 512 pixels from the ILSVRC2012 [6] validation set. For testing, we use Kodak [9], BSD300 [23], and Set14 [38], repeated by 10, 3, and 20 times, respectively. This adds up to 820 (240, 300, and 280) test images. We apply four types of noise in sRGB: (1) Gaussian noise with \u03c3 = 25, (2) Gaussian noise with \u03c3 \u2208 [5, 50], (3) Poisson noise with \u03bb = 30, (4) Poisson noise with \u03bb \u2208 [5,50].\n\nWe present the results in Table 1. SwinIA performed similar to Noise2Void and Self2Self, however, did not improve over other true BSN methods. To our surprise, the method by Honz\u00e0tko et al. [11] dominated the comparison despite being rarely mentioned in denoising literature.\n\n\nSynthetic noise (grayscale)\n\nFollowing [34], we use BSD400 [40]   BSD68, in ranked first for \u03c3 = 25, second for for \u03c3 = 50, and third for \u03c3 = 15. Interestingly, we obtained lower scores on Set12, but still consistently outperformed Noise2Self [2] as the only other method trained and inferred in a single forward pass.\n\n\nMixture synthetic noise\n\nWe experiment with the sRGB natural images dataset (ImageNet) and grayscale Chinese characters dataset (H\u00e0nZ\u00ec) with a mixture of multiple noise modalities, following [2,36]. ImageNet dataset was generated by randomly cropping 60 000 patches of size 128 \u00d7 128 from the first 20 000 images in ILSVRC2012 [6] validation set that consists of 50 000 instances. We use two sets of 1 000 images for validation and testing. Poisson noise (\u03bb = 30), additive Gaussian noise (\u03c3 = 60), and Bernoulli noise (p = 0.2) were applied to the clean images before the training.\n\nH\u00e0nZ\u00ec dataset consists of 78 174 noisy images with 13 029 different Chinese characters of size 64 \u00d7 64. Each noisy image is generated by applying Gaussian noise (\u03c3 = 0.7) and Bernoulli noise (p = 0.5) to a clean image. We select 10% of images for testing and use the rest for training.\n\nWe present the results in Table 3. On ImageNet, SwinIA outperformed the second-best method, Noise2Same [36], by +0.51dB PSNR and +0.013 SSIM. Compared to another true BSN by Laine et al. [15], SwinIA showed +2.47dB PSNR. On H\u00e0nZ\u00ec, SwinIA was worse than Noise2Same in PSNR (\u22120.5dB) but better in SSIM (+0.014). It also outperformed Laine et al. by +3.65dB PSNR. We compare example predictions with ground truth and noisy input in Figure 6 and 7.   Table 3: Denoising results on synthetic datasets in sRGB space with mixed noise [36]. The highest PSNR(dB)/SSIM among unsupervised denoising methods is highlighted in bold, while the second-best is underlined. \u2020 denotes the models that we implemented and trained ourselves.\n\n\nFluorescent microscopy\n\nWe use Confocal Fish, Confocal Mice, and Two-Photon Mice datasets from the Fluorescent Microscopy Denoising Dataset [41]. Each dataset consists of 20 views, with 50 grayscale images per view. Each image contains a mixture of natural Poisson and Gaussian noise. Ground truth image for the view is obtained by averaging all images. We follow [34] and select the 19th view for testing and the rest for training. To avoid overfitting, we set the number of steps to 20 000 and apply weight decay 10 \u22128 .\n\nExperimental results are presented in Table 4. SwinIA performed competitively across all datasets. Another true BSN method by Laine et al. [15] required knowledge about the noise model and performed worse for the real-world microscopy data with noise mixture: on average, SwinIA was better by 0.2db PSNR and 0.01 SSIM than Laine19-mu for  Table 4: Denoising results on Fluorescent Microscopy datasets. The highest PSNR(dB)/SSIM among unsupervised denoising methods is highlighted in bold, while the second-best is underlined. For Laine et al. [15], G stands for Gaussian, P is for Poisson. \u2020 denotes the models that we implemented and trained ourselves.\n\nboth Gaussian and Poisson assumed distributions.\n\n\nAblation study\n\nWe ran ablation experiments on mixed noise sRGB Im-ageNet dataset to validate architecture design elements proposed in Sections 3.3 and 4 as well as additional hyperparameters. The results are summarized in Table 5. In particular, we found that the original Swin Transformer [20] diagonal shift leads to a substantial performance decrease and visible tiling artifacts. Shuffle GroupNorm slightly improves the results, and Cosine Annealing learning rate schedule [21] leads to a better convergence compared to decreasing learning rate by half every 5 000 steps [36]. When we removed the U-shaped shuffle configuration and naively stacked transformer blocks without shuffle, training did not converge at all.\n\nDiagonal shift [20] Cyclic shift Lambda LR [36] Cosine LR [ Table 5: Ablation results on ImageNet dataset with mixed noise [36].\n\nWe also checked the necessity of shortcuts around transformer blocks and the last Layer Normalization in the block. The results of this ablation on H\u00e0nZ\u00ec and ImageNet are summarized in Table 6. Removing any of the components had a mixed effect on the performance. For example, without the last Layer Normalization, performance on Im-ageNet slightly improved, however, it drastically dropped for H\u00e0nZ\u00ec. The opposite was true for the shortcuts: excluding them provided slightly better results for H\u00e0nZ\u00ec, but considerably weakened the scores on ImageNet. For our final architecture, we chose the most balanced solution and included both elements.\n\nH\u00e0nZ\u00ec [2] ImageNet [36] Our best 14 \n\n\nDiscussion\n\nWe designed our SwinIA model to be flexible and robust in various noise conditions. It showed state-of-the-art results on ImageNet with a mixture of Gaussian, Poisson, and Bernoulli noise. It also performed better than other true blind-spot methods on a fluorescent microscopy dataset with natural noise. Among datasets with singe-distribution synthetic noise, SwinIA performed best for BSD68 across multiple \u03c3 of Gaussian. However, it did not match the best methods on other test datasets, performing on par with Noise2Void [14], DBSN [35], and Self2Self [27]. We hypothesize that this is due to the inherent limitations of blindspot models. The pixel itself contains the most useful information about its true signal, which is inevitably lost in the training process. However, this gives substantial room for improvement in our method. Different masking strategies Figure 7: Denoising results on H\u00e0nZ\u00ec [2] images. and regularization techniques were proposed to loosen the blindness requirement [34,36]. Similar approaches could be applicable to the diagonal attention mask as well.\n\nFrom the blind-spot property comes the second limitation of our model. Similarly to other BSNs, it assumes spatially uncorrelated noise. For many digital photography datasets with natural noise, this is not the case because of hardware pixel interpolation. We hypothesize that correlated noise can be mitigated with increased shuffle group size resembling the approach by Lee et al. [16] if the correlation is present only in a close neighborhood.\n\nFinally, transformers are considerably more computationally expensive than the usual denoising U-Net and require more time for training and inference. However, they are known for their ability to extract meaningful representations from large datasets, and we expect our method to improve with increasing training set size. Besides, being conceptually similar to the language modeling objective [31], our solution could be used in self-supervised pre-training to produce rich pixel embeddings for downstream image processing tasks.\n\n\nConclusion\n\nWe propose a Swin Transformer-based Image Autoencoder, the first convolution-free transformer architecture for blind-spot self-supervised denoising. Unlike its counterparts, it does not assume any knowledge about noise distribution. It also does not use masking and can be trained in an autoencoder fashion with a single forward pass and an MSE loss. Despite the loss of information from the blind spot and ignorance of the noise nature, it achieves competitive results, outperforming state-of-the-art denoising methods on several datasets. \n\n\nA. Model card\n\n\nModel Architecture\n\nFully transformer-based image autoencoder model for end-to-end self-supervised image denoising with no convolutions. For details, see Section 4.\n\n\nInput(s)\n\nThe model takes noisy images as input, batch and channel dimensions go first.\n\nOutput(s) The model outputs a batch of denoised images of the same shape as input.\n\n\nUsage\n\n\nApplication\n\nThe model can be used in self-supervised image denoising for any type of spatially-uncorrelated synthetic and natural noise on both grayscale and colored images. Also, it is theoretically possible to use the model in self-supervised pre-training for extracting features from images for downstream computer vision tasks.\n\n\nKnown Limitations\n\nThe model is computationally expensive and requires both powerful GPU hardware and considerable training time. Also, small datasets will most probably lead to overfitting. Finally, the model will most likely learn an identity function on the data with spatially correlated noise without proper tuning of shuffle group sizes (as discussed in Section 6).\n\n\nSystem Type\n\nSystem Description This is a standalone model.\n\n\nUpstream Dependencies\n\nNone.\n\nDownstream Dependencies None.\n\n\nImplementation Frameworks\n\nHardware & Software\n\nHardware: NVIDIA A100 80GB GPUs (driver version: 470.57.02, CUDA version: 11.4).\n\nSoftware: Python 3.8.3, PyTorch 1.12.1 [26], einops [28].\n\nCompute Requirements SwinIA was trained on 2 NVIDIA A100 80GB GPUs for different numbers of steps (see Section 5 for details).\n\n\nModel Characteristics\n\n\nModel Initialization\n\nThe model is trained from a random initialization.   \n\n\nB. Model complexity\n\nWe compare the training time and average inference time of Noise2Same [36] and SwinIA models in Table 8 and show the difference in the number of parameters and the number of floating point operations per second (FLOPS) in Table 9.\n\n\nC. Failed experiments\n\nWe tried a multitude of configurations and training schemes, and not all of them were successful. Here, we list some approaches that we found interesting but did not manage to train properly. Noise2Same training. We have tried to train SwinIA in Noise2Same [36] mode with two forward passes. In the first, masked, pass we applied diagonal masking as in the baseline. In the second pass, we did not mask the attention, allowing each pixel to look at itself. We also lifted the input isolation requirement and computed self-attention starting from the second transformer block. Thus, we obtained two outputs and measured invariance loss (RMSE) between them and MSE between the masked output and the input. In this configuration, training did not converge well. We assume that the network is confused about the modality shift between the two passes. Dilated attention. Instead of pixel shuffle, we tested dilated window attention as a way to improve the receptive field. However, such a model resulted in rigid pixel-level artifacts due to the attention sparsity that went against the importance of dense pixel interactions.\n\n\nD. Denoising results visualization\n\nIn Figures 8, 9, 10, and 11, we present additional examples from several experiments and discuss details in figure captions. Figure 8: Denoising examples on ImageNet dataset with mixed synthetic noise [36]. Each row contains noisy and ground truth images, along with the predictions of Noise2Self [2], Noise2Same [36], and SwinIA (ours) models with corresponding PSNR scores. Each image is center-cropped for visualization. SwinIA appears to be more capable of restoring random detailed structures (as in the image with tree bark and the corals) and does not tend to excessively smooth the background. Figure 9: Denoising examples on BSD68 dataset with synthetic Gaussian noise with \u03c3 = 25 [34]. Each row contains noisy and ground truth images, along with the predictions of Noise2Self [2], Noise2Same [36], and SwinIA (ours) models with corresponding PSNR scores. Each image is center-cropped for visualization. SwinIA shows competitive performance, achieving slightly better scores in most cases. Figure 10: Denoising examples on Set12 dataset with synthetic Gaussian noise with \u03c3 = 25 [34]. Each row contains noisy and ground truth images, along with the predictions of Noise2Self [2], Noise2Same [36], and SwinIA (ours) models with corresponding PSNR scores. Each image is center-cropped for visualization. In the image with striped clothes (third row), SwinIA outperforms its counterparts in the restoration of clothing patterns. Figure 11: Denoising results on fluorescent microscopy images with natural noise [41]. Each pair of rows shows an example from each of the three used datasets, zooming a partition of each image for a better view.\n\nFigure 1 :\n1Self-unaware autoencoding in text and images.\n\nFigure 2 :\n2Shifted windows alternation between consequent transformer blocks. Neighboring pixel pairs that never occur in one window during the shifting flow of Swin Transformer[20] are enumerated 1 through 8.\n\nFigure 3 :\n3Window partition example with shuffle S = 2 into four shuffle groups. Window size is M \u00d7 M = 2 \u00d7 2 shuffle groups, or 4 \u00d7 4 pixels.\n\nFigure 4 :\n4SwinIA residual group architecture. Circular portals \u2192 and \u2192 in the image represent inputting fixed Key and Value determined by input isolation.\n\n\n.\n\nFigure 5 :\n5SwinIA model architecture. Circular portals \u2192 and \u2192 in the image represent inputting fixed Key and Value determined by input isolation.\n\nFigure 6 :\n6BSD68 [30] (top) and ImageNet [36] (bottom) denoising examples.\n\n\nfor training and test on Set12 and BSD68[30] repeated 20 and 4 times, respectively. This results in 512 (240 and 272) testing images in total. We apply Gaussian noise with \u03c3 = {15, 25, 50} toNoise Type Method \n\nKODAK \nBSD300 \nSET14 \n\nGaussian \n\u03c3 = 25 \n\nBaseline, N2C [29] 32.43/0.884 31.05/0.879 31.40/0.869 \nBaseline, N2N [17] 32.41/0.884 31.04/0.878 31.37/0.868 \n\nCBM3D [4] \n31.87/0.868 30.48/0.861 30.88/0.854 \nSelf2Self [27] \n31.28/0.864 29.86/0.849 30.08/0.839 \nN2V [14] \n30.32/0.821 29.34/0.824 28.84/0.802 \nNoisier2Noise [24] 30.70/0.845 29.32/0.833 29.64/0.832 \nR2R [25] \n32.25/0.880 30.91/0.872 31.32/0.865 \nNBR2NBR [12] \n32.08/0.879 30.79/0.873 31.09/0.864 \nBlind2Undlind [34] 32.27/0.880 30.87/0.872 31.27/0.864 \nNoise2Same  \u2020 [36] \n30.77/0.841 29.50/0.834 29.53/0.827 \n\nLaine19-mu [15] \n30.62/0.840 28.62/0.803 29.93/0.830 \nLaine19-pme [15] \n32.40/0.883 30.99/0.877 31.36/0.866 \nDBSN [35] \n31.64/0.856 29.80/0.839 30.63/0.846 \nHonzatko20 [11] \n32.45/-\n31.02/-\n31.25/-\nOurs  \u2020 \n30.12/0.819 28.40/0.789 29.54/0.814 \n\nGaussian \n\u03c3 \u2208 [5, 50] \n\nBaseline, N2C [29] 32.51/0.875 31.07/0.866 31.41/0.863 \nBaseline, N2N [17] 32.50/0.875 31.07/0.866 31.39/0.863 \n\nCBM3D [4] \n32.02/0.860 30.56/0.847 30.94/0.849 \nSelf2Self [27] \n31.37/0.860 29.87/0.841 29.97/0.849 \nN2V [14] \n30.44/0.806 29.31/0.801 29.01/0.792 \nR2R [25] \n31.50/0.850 30.56/0.855 30.84/0.850 \nNBR2NBR [12] \n32.10/0.870 30.73/0.861 31.05/0.858 \nBlind2Undlind [34] 32.34/0.872 30.86/0.861 31.14/0.857 \nNoise2Same  \u2020 [36] \n30.78/0.835 29.49/0.823 29.34/0.817 \n\nLaine19-mu [15] \n30.52/0.833 28.43/0.794 29.71/0.822 \nLaine19-pme [15] \n32.40/0.870 30.95/0.861 31.21/0.855 \nDBSN [35] \n30.38/0.826 28.34/0.788 29.49/0.814 \nHonzatko20 [11] \n32.46/-\n31.18/-\n31.25/-\nOurs  \u2020 \n30.30/0.820 28.40/0.785 29.49/0.809 \n\nPoisson \n\u03bb = 30 \n\nBaseline, N2C [29] 31.78/0.876 30.36/0.868 30.57/0.858 \nBaseline, N2N [17] 31.77/0.876 30.35/0.868 30.56/0.857 \n\nAnscombe [22] \n30.53/0.856 29.18/0.842 29.44/0.837 \nSelf2Self [27] \n30.31/0.857 28.93/0.840 28.84/0.839 \nN2V [14] \n28.90/0.788 28.46/0.798 27.73/0.774 \nR2R [25] \n30.50/0.801 29.47/0.811 29.53/0.801 \nNBR2NBR [12] \n31.44/0.870 30.10/0.863 30.29/0.853 \nBlind2Undlind [34] 31.64/0.871 30.25/0.862 30.46/0.852 \nNoise2Same  \u2020 [36] \n27.73/0.747 26.69/0.714 26.78/0.735 \n\nLaine19-mu [15] \n30.19/0.833 28.25/0.794 29.35/0.820 \nLaine19-pme [15] \n31.67/0.874 30.25/0.866 30.47/0.855 \nDBSN [35] \n30.07/0.827 28.19/0.790 29.16/0.814 \nHonzatko20 [11] \n31.67/-\n30.25/-\n30.14/-\nOurs  \u2020 \n29.51/0.805 27.92/0.775 28.74/0.799 \n\nPoisson \n\u03bb \u2208 [5, 50] \n\nBaseline, N2C [29] 31.19/0.861 29.79/0.848 30.02/0.842 \nBaseline, N2N [17] 31.18/0.861 29.78/0.848 30.02/0.842 \n\nAnscombe [22] \n29.40/0.836 28.22/0.815 28.51/0.817 \nSelf2Self [27] \n29.06/0.834 28.15/0.817 28.83/0.841 \nN2V [14] \n28.78/0.758 27.92/0.766 27.43/0.745 \nR2R [25] \n29.14/0.732 28.68/0.771 28.77/0.765 \nNBR2NBR [12] \n30.86/0.855 29.54/0.843 29.79/0.838 \nBlind2Undlind [34] 31.07/0.857 29.92/0.852 30.10/0.844 \nNoise2Same  \u2020 [36] \n27.44/0.738 26.36/0.700 26.37/0.721 \n\nLaine19-mu [15] \n29.76/0.820 27.89/0.778 28.94/0.808 \nLaine19-pme [15] \n30.88/0.850 29.57/0.841 28.65/0.785 \nDBSN [35] \n29.60/0.811 27.81/0.771 28.72/0.800 \nOurs  \u2020 \n29.06/0.788 27.74/0.764 28.27/0.780 \n\nTable 1: Denoising results on synthetic sRGB datasets. \nThe highest PSNR(dB)/SSIM among unsupervised denois-\ning methods is highlighted in bold, while the second-best is \nunderlined.  \u2020 denotes the models that we implemented and \ntrained ourselves. \n\ncreate noisy images. \nThe results are summarized in Table 2. Overall, our \nmethod performed on par with the existing solution. On \n\nNoise Type \nMethod \nNetwork \nBSD68 \nSet12 \n\nGaussian \n\u03c3 = 15 \n\nN2C [29] \nU-Net [29] 31.58/0.889 32.60/0.899 \n\nR2R [25] \nU-Net [29] 31.54/0.885 32.54/0.897 \nNoise2Self  \u2020 [2] \nU-Net [29] 30.63/0.843 29.88/0.840 \nNoise2Same  \u2020 [36] U-Net [29] 30.85/0.850 30.02/0.849 \nBlind2Unblind [34] U-Net [29] 31.44/0.884 32.46/0.897 \nOurs \nSwinIA \n31.07/0.856 30.37/0.857 \n\nGaussian \n\u03c3 = 25 \n\nN2C [29] \nU-Net [29] 29.02/0.822 30.07/0.852 \n\nR2R [25] \nU-Net [29] 28.99/0.818 30.06/0.851 \nNoise2Self  \u2020 [2] \nU-Net [29] 28.88/0.789 28.37 0.799 \nNoise2Same  \u2020 [36] U-Net [29] 29.13/0.800 28.54/0.814 \nBlind2Unblind [34] U-Net [29] 28.99/0.820 30.09/0.854 \nOurs \nSwinIA \n29.17/0.801 28.72/0.817 \n\nGaussian \n\u03c3 = 50 \n\nN2C [29] \nU-Net [29] 26.08/0.715 26.88/0.777 \n\nR2R [25] \nU-Net [29] 26.02/0.705 26.86/0.771 \nNoise2Self  \u2020 [2] \nU-Net [29] 26.19/0.664 25.56/0.692 \nNoise2Same  \u2020 [36] U-Net [29] 26.75/0.714 26.13/0.744 \nBlind2Unblind [34] U-Net [29] 26.09/0.715 26.91/0.776 \nOurs  \u2020 \nSwinIA \n26.61/0.706 26.03/0.736 \n\n\n\nTable 2 :\n2Grayscale image denoising results for BSD68 and \nSet12 with synthetic noise. The highest PSNR(dB)/SSIM \namong unsupervised denoising methods is highlighted in \nbold, while the second-best is underlined.  \u2020 denotes the \nmodels that we implemented and trained ourselves. \n\n\n\nTable 6 :\n6Ablation results for transformer group architecture \ndetails on H\u00e0nZ\u00ec [2] and ImageNet [36]. \n\n\n\nTable 7 :\n7Model Card of SwinIA model.Model Summary \n\n\n\n\nModel StatusThis is a static model trained on offline datasets.Model Stats SwinIA model has 2.369 million parameters and performs 10.117 GFLOPS (floating point operations per second) for a single grayscale image of size 64 \u00d7 64. Natural noise (grayscale): Confocal Fish, Confocal Mice, and Two-Photon Mice datasets from the Fluorescent Microscopy Denoising Dataset [41] (Section 5.5). Mixture synthetic noise: ImageNet [36], H\u00e0nZ\u00ec [2, 36] (Section 5.4). Natural noise (grayscale): Confocal Fish, Confocal Mice, and Two-Photon Mice datasets from the Fluorescent Microscopy Denoising Dataset [41] (Section 5.5).Data Overview \n\nTraining Datasets \n\nSynthetic noise (sRGB): ILSVRC2012 [6] validation set (Section 5.2). \n\nSynthetic noise (grayscale): BSD400 [40] (Section 5.3). \n\nMixture synthetic noise: ImageNet [36], H\u00e0nZ\u00ec [2, 36] (Section 5.4). \n\nEvaluation Datasets \n\nSynthetic noise (sRGB): Kodak [9], BSD300 [23], Set14 [38] (Section 5.2). \n\nSynthetic noise (grayscale): Set12 and BSD68 [30] (Section 5.3). \n\nDataset \nNoise2Same \nSwinIA \nTT (h) AIT (ms) TT (h) AIT (ms) \n\nSynthetic (sRGB) [15, 12, 34] \n4 \n26 \n15 \n941 \nSynthetic (grayscale) [34] \n2 \n12 \n13 \n605 \nImageNet [36] \n1 \n14 \n15 \n765 \nH\u00e0nZ\u00ec [2] \n1 \n5 \n14 \n42 \nMicroscopy [15, 12, 34] \n1.5 \n20 \n5 \n845 \n\n\n\nTable 8 :\n8Comparison of training time (TT) in hours, and average inference time (AIT) on the test set in milliseconds of Noise2Same[36] and SwinIA (ours) on various datasets.Criterion \nNoise2Same SwinIA \n\nNumber of trainable parameters \n5.564M \n2.369M \nFLOPs per grayscale image 64 \u00d7 64 \n5.001G \n10.117G \n\n\n\nTable 9 :\n9Comparison of the number of parameters and FLOPS between Noise2Same[36] and SwinIA (ours). The number of FLOPS is calculated for inference time, as Noise2Same has two forward passes during training, and this number gets doubled.\n\nBeit: Bert pre-training of image transformers. Hangbo Bao, Li Dong, Songhao Piao, Furu Wei, arXiv:2106.08254arXiv preprintHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. 2\n\nNoise2Self: Blind denoising by self-supervision. Joshua Batson, Loic Royer, PMLR, 09- 15Proceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning9715Joshua Batson and Loic Royer. Noise2Self: Blind denois- ing by self-supervision. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 524-533. PMLR, 09- 15 Jun 2019. 1, 2, 5, 6, 7, 8, 11, 12, 13, 14, 15\n\nA non-local algorithm for image denoising. Antoni Buades, Bartomeu Coll, J-M Morel, IEEE computer society conference on computer vision and pattern recognition (CVPR'05). Ieee27Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image denoising. In 2005 IEEE computer so- ciety conference on computer vision and pattern recognition (CVPR'05), volume 2, pages 60-65. Ieee, 2005. 1, 7\n\nColor image denoising via sparse 3d collaborative filtering with grouping constraint in luminancechrominance space. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, 16Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Color image denoising via sparse 3d col- laborative filtering with grouping constraint in luminance- chrominance space. volume 1, pages I -313, 09 2007. 1, 6\n\nImage denoising by sparse 3-d transformdomain collaborative filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, IEEE Transactions on image processing. 1687Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform- domain collaborative filtering. IEEE Transactions on image processing, 16(8):2080-2095, 2007. 1, 7\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. 611Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248-255, 2009. 5, 6, 11\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational Linguistics. 3Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional trans- formers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Associa- tion for Computational Linguistics. 3\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, arXiv:2010.11929Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2\n\nKodak lossless true color image suite. Rich Franzen, 412Rich Franzen. Kodak lossless true color image suite. source: http://r0k. us/graphics/kodak, 4(2), 1999. 5, 12\n\nMasked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000- 16009, 2022. 2\n\nEfficient blind-spot neural network architecture for image denoising. David Honz\u00e1tko, A Siavash, Engin Bigdeli, L Andrea T\u00fcretken, Dunbar, 2020 7th Swiss Conference on Data Science (SDS). 56David Honz\u00e1tko, Siavash A Bigdeli, Engin T\u00fcretken, and L Andrea Dunbar. Efficient blind-spot neural network archi- tecture for image denoising. In 2020 7th Swiss Conference on Data Science (SDS), pages 59-60. IEEE, 2020. 2, 5, 6\n\nNeighbor2neighbor: Self-supervised denoising from single noisy images. Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, Jianzhuang Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition712Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2neighbor: Self-supervised de- noising from single noisy images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14781-14790, 2021. 1, 5, 6, 7, 12\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5\n\nNoise2void-learning denoising from single noisy images. Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition7Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 2129-2137, 2019. 1, 2, 6, 7, 8\n\nHigh-quality self-supervised deep image denoising. Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila, Advances in Neural Information Processing Systems. 3212Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. Ad- vances in Neural Information Processing Systems, 32, 2019. 1, 2, 5, 6, 7, 12\n\nApbsn: Self-supervised denoising for real-world images via asymmetric pd and blind-spot network. Wooseok Lee, Sanghyun Son, Kyoung Mu Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2Wooseok Lee, Sanghyun Son, and Kyoung Mu Lee. Ap- bsn: Self-supervised denoising for real-world images via asymmetric pd and blind-spot network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17725-17734, 2022. 2, 8\n\nJaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, arXiv:1803.04189Noise2noise: Learning image restoration without clean data. 67arXiv preprintJaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. arXiv preprint arXiv:1803.04189, 2018. 1, 6, 7\n\nJingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte, Swinir, arXiv:2108.10257Image restoration using swin transformer. 13arXiv preprintJingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. arXiv preprint arXiv:2108.10257, 2021. 1, 2, 3\n\nDnt: Learning unsupervised denoising transformer from single noisy image. Xiaolong Liu, Yusheng Hong, Qifang Yin, Shuo Zhang, Proceedings of the 4th International Conference on Image Processing and Machine Vision, IPMV '22. the 4th International Conference on Image Processing and Machine Vision, IPMV '22New York, NY, USA, 2022Association for Computing MachineryXiaolong Liu, Yusheng Hong, Qifang Yin, and Shuo Zhang. Dnt: Learning unsupervised denoising transformer from sin- gle noisy image. In Proceedings of the 4th International Conference on Image Processing and Machine Vision, IPMV '22, page 50-56, New York, NY, USA, 2022. Association for Computing Machinery. 2\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)7Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2, 3, 4, 7, 8\n\nSgdr: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, arXiv:1608.039837arXiv preprintIlya Loshchilov and Frank Hutter. Sgdr: Stochas- tic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 5, 7, 8\n\nOptimal inversion of the anscombe transformation in low-count poisson image denoising. Markku Makitalo, Alessandro Foi, IEEE Transactions on Image Processing. 201Markku Makitalo and Alessandro Foi. Optimal inversion of the anscombe transformation in low-count poisson image de- noising. IEEE Transactions on Image Processing, 20(1):99- 109, 2011. 1, 6\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik, ICCV. 212David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, volume 2, pages 416-423, 2001. 5, 12\n\nNoisier2noise: Learning to denoise from unpaired noisy data. Nick Moran, Dan Schmidt, Yu Zhong, Patrick Coady, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition16Nick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2noise: Learning to denoise from unpaired noisy data. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 12064-12072, 2020. 1, 6\n\nRecorrupted-to-recorrupted: unsupervised deep learning for image denoising. Tongyao Pang, Huan Zheng, Yuhui Quan, Hui Ji, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition16Tongyao Pang, Huan Zheng, Yuhui Quan, and Hui Ji. Recorrupted-to-recorrupted: unsupervised deep learning for image denoising. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 2043-2052, 2021. 1, 6\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, 511Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. 5, 11\n\nSelf2self with dropout: Learning self-supervised denoising from single image. Yuhui Quan, Mingqin Chen, Tongyao Pang, Hui Ji, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition6Yuhui Quan, Mingqin Chen, Tongyao Pang, and Hui Ji. Self2self with dropout: Learning self-supervised denoising from single image. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 1890-1898, 2020. 1, 6, 8\n\nEinops: Clear and reliable tensor manipulations with einstein-like notation. Alex Rogozhnikov, International Conference on Learning Representations. 511Alex Rogozhnikov. Einops: Clear and reliable tensor manip- ulations with einstein-like notation. In International Confer- ence on Learning Representations, 2022. 5, 11\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015. Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. FrangiChamSpringer International Publishing47Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi, editors, Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015, pages 234- 241, Cham, 2015. Springer International Publishing. 4, 5, 6, 7\n\nFields of experts: A framework for learning image priors. Stefan Roth, J Michael, Black, CVPR. IEEE712Stefan Roth and Michael J Black. Fields of experts: A frame- work for learning image priors. In CVPR, pages 860-867. IEEE, 2005. 5, 7, 12\n\nFast and accurate deep bidirectional language representations for unsupervised learning. Joongbo Shin, Yoonhyung Lee, Seunghyun Yoon, Kyomin Jung, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline2Joongbo Shin, Yoonhyung Lee, Seunghyun Yoon, and Ky- omin Jung. Fast and accurate deep bidirectional language representations for unsupervised learning. In Proceedings of the 58th Annual Meeting of the Association for Computa- tional Linguistics, pages 823-835, Online, July 2020. Asso- ciation for Computational Linguistics. 2, 3, 8\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc304Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Il- lia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish- wanathan, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems, volume 30. Curran Associates, Inc., 2017. 4\n\nUformer: A general u-shaped transformer for image restoration. Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, Houqiang Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition1Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general u-shaped transformer for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 17683-17693, 2022. 1, 2\n\nBlind2unblind: Self-supervised image denoising with visible blind spots. Zejin Wang, Jiazheng Liu, Guoqing Li, Hua Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition1415Zejin Wang, Jiazheng Liu, Guoqing Li, and Hua Han. Blind2unblind: Self-supervised image denoising with visi- ble blind spots. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2027- 2036, 2022. 1, 2, 5, 6, 7, 8, 12, 14, 15\n\nUnpaired learning of deep image denoising. Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, Wangmeng Zuo, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer6Proceedings, Part IVXiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, and Wang- meng Zuo. Unpaired learning of deep image denoising. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV, pages 352-368. Springer, 2020. 2, 5, 6, 8\n\nNoise2Same: Optimizing a self-supervised bound for image denoising. Yaochen Xie, Zhengyang Wang, Shuiwang Ji, Advances in Neural Information Processing Systems. 3315Yaochen Xie, Zhengyang Wang, and Shuiwang Ji. Noise2Same: Optimizing a self-supervised bound for image denoising. In Advances in Neural Information Processing Systems, volume 33, pages 20320-20330, 2020. 1, 2, 5, 6, 7, 8, 11, 12, 13, 14, 15\n\nRestormer: Efficient transformer for high-resolution image restoration. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition1Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu- nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5728- 5739, 2022. 1, 2\n\nOn single image scale-up using sparse-representations. Roman Zeyde, Michael Elad, Matan Protter, International conference on curves and surfaces. 512Roman Zeyde, Michael Elad, and Matan Protter. On sin- gle image scale-up using sparse-representations. In Interna- tional conference on curves and surfaces, pages 711-730, 2010. 5, 12\n\nSelf-supervised image denoising for real-world images with context-aware transformer. Dan Zhang, Fangfang Zhou, IEEE Access27Dan Zhang and Fangfang Zhou. Self-supervised image denoising for real-world images with context-aware trans- former. IEEE Access, 2023. 2, 7\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, TIP. 26711Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. TIP, 26(7):3142-3155, 2017. 5, 11\n\nA poisson-gaussian denoising dataset with real fluorescence microscopy images. Yide Zhang, Yinhao Zhu, Evan Nichols, Qingfei Wang, Siyuan Zhang, Cody Smith, Scott Howard, CVPR. 1216Yide Zhang, Yinhao Zhu, Evan Nichols, Qingfei Wang, Siyuan Zhang, Cody Smith, and Scott Howard. A poisson-gaussian denoising dataset with real fluorescence microscopy images. In CVPR, 2019. 7, 11, 12, 16\n", "annotations": {"author": "[{\"end\":178,\"start\":77},{\"end\":278,\"start\":179}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":85},{\"end\":192,\"start\":185}]", "author_first_name": "[{\"end\":84,\"start\":77},{\"end\":184,\"start\":179}]", "author_affiliation": "[{\"end\":177,\"start\":114},{\"end\":277,\"start\":214}]", "title": "[{\"end\":74,\"start\":1},{\"end\":352,\"start\":279}]", "venue": null, "abstract": "[{\"end\":1123,\"start\":354}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1666,\"start\":1662},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1668,\"start\":1666},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1671,\"start\":1668},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1674,\"start\":1671},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1677,\"start\":1674},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1680,\"start\":1677},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1683,\"start\":1680},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1686,\"start\":1683},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1689,\"start\":1686},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1692,\"start\":1689},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1735,\"start\":1732},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1737,\"start\":1735},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1740,\"start\":1737},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1742,\"start\":1740},{\"end\":1954,\"start\":1939},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1980,\"start\":1976},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2608,\"start\":2604},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2642,\"start\":2638},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2828,\"start\":2824},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2975,\"start\":2971},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3102,\"start\":3099},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3122,\"start\":3118},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3529,\"start\":3525},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3549,\"start\":3546},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3730,\"start\":3726},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3749,\"start\":3745},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3991,\"start\":3987},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4010,\"start\":4006},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4274,\"start\":4270},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4327,\"start\":4324},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4437,\"start\":4433},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4733,\"start\":4729},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4765,\"start\":4761},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4853,\"start\":4849},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4872,\"start\":4868},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5027,\"start\":5024},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5097,\"start\":5094},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5252,\"start\":5248},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6451,\"start\":6447},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6453,\"start\":6451},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6456,\"start\":6453},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6459,\"start\":6456},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6511,\"start\":6507},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6514,\"start\":6511},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6691,\"start\":6687},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6950,\"start\":6946},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7202,\"start\":7198},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7448,\"start\":7444},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7451,\"start\":7448},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7454,\"start\":7451},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7568,\"start\":7564},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7589,\"start\":7585},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7612,\"start\":7608},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7653,\"start\":7649},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7919,\"start\":7915},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8308,\"start\":8304},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8310,\"start\":8308},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8436,\"start\":8432},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8438,\"start\":8436},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8441,\"start\":8438},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8444,\"start\":8441},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8483,\"start\":8479},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8486,\"start\":8483},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8547,\"start\":8543},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8889,\"start\":8885},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9266,\"start\":9263},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9632,\"start\":9628},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9916,\"start\":9912},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10002,\"start\":9998},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11850,\"start\":11846},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11892,\"start\":11888},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12417,\"start\":12413},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12920,\"start\":12916},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13134,\"start\":13130},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13694,\"start\":13690},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14924,\"start\":14920},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15874,\"start\":15870},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16172,\"start\":16168},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16175,\"start\":16172},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16178,\"start\":16175},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16361,\"start\":16358},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16381,\"start\":16377},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16575,\"start\":16571},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16611,\"start\":16607},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16927,\"start\":16923},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17217,\"start\":17213},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17329,\"start\":17325},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17448,\"start\":17444},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17451,\"start\":17448},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17454,\"start\":17451},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17598,\"start\":17595},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17644,\"start\":17641},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17657,\"start\":17653},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17673,\"start\":17669},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17943,\"start\":17940},{\"end\":17946,\"start\":17943},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18143,\"start\":18139},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18270,\"start\":18266},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18290,\"start\":18286},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18473,\"start\":18470},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18742,\"start\":18739},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18745,\"start\":18742},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18878,\"start\":18875},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19526,\"start\":19522},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19610,\"start\":19606},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19950,\"start\":19946},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20286,\"start\":20282},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20510,\"start\":20506},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20809,\"start\":20805},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21213,\"start\":21209},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21667,\"start\":21663},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21854,\"start\":21850},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21952,\"start\":21948},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22115,\"start\":22111},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22143,\"start\":22139},{\"end\":22155,\"start\":22154},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22223,\"start\":22219},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22880,\"start\":22877},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22894,\"start\":22890},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22906,\"start\":22904},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23451,\"start\":23447},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23462,\"start\":23458},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23482,\"start\":23478},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23829,\"start\":23826},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23922,\"start\":23918},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23925,\"start\":23922},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24394,\"start\":24390},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24854,\"start\":24850},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26916,\"start\":26912},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26929,\"start\":26925},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27258,\"start\":27254},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27701,\"start\":27697},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28805,\"start\":28801},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28900,\"start\":28897},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28917,\"start\":28913},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29294,\"start\":29290},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29389,\"start\":29386},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29406,\"start\":29402},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29692,\"start\":29688},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29787,\"start\":29784},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29804,\"start\":29800},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30120,\"start\":30116},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30490,\"start\":30486},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31098,\"start\":31094},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":37500,\"start\":37496},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":37755,\"start\":37751}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30306,\"start\":30248},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30518,\"start\":30307},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30663,\"start\":30519},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30821,\"start\":30664},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30825,\"start\":30822},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30974,\"start\":30826},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31051,\"start\":30975},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35648,\"start\":31052},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35932,\"start\":35649},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":36040,\"start\":35933},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":36096,\"start\":36041},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":37362,\"start\":36097},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":37671,\"start\":37363},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":37912,\"start\":37672}]", "paragraph": "[{\"end\":1592,\"start\":1139},{\"end\":2207,\"start\":1594},{\"end\":2441,\"start\":2209},{\"end\":3086,\"start\":2443},{\"end\":4605,\"start\":3088},{\"end\":5393,\"start\":4607},{\"end\":6126,\"start\":5395},{\"end\":6368,\"start\":6143},{\"end\":7333,\"start\":6392},{\"end\":7497,\"start\":7367},{\"end\":8115,\"start\":7499},{\"end\":8257,\"start\":8126},{\"end\":8819,\"start\":8272},{\"end\":9448,\"start\":8873},{\"end\":9917,\"start\":9469},{\"end\":10066,\"start\":9919},{\"end\":11597,\"start\":10083},{\"end\":11754,\"start\":11609},{\"end\":12381,\"start\":11776},{\"end\":12891,\"start\":12383},{\"end\":13290,\"start\":12893},{\"end\":13668,\"start\":13309},{\"end\":14004,\"start\":13670},{\"end\":15875,\"start\":14021},{\"end\":16455,\"start\":15900},{\"end\":17150,\"start\":16477},{\"end\":17407,\"start\":17152},{\"end\":17947,\"start\":17434},{\"end\":18224,\"start\":17949},{\"end\":18545,\"start\":18256},{\"end\":19130,\"start\":18573},{\"end\":19417,\"start\":19132},{\"end\":20139,\"start\":19419},{\"end\":20664,\"start\":20166},{\"end\":21319,\"start\":20666},{\"end\":21369,\"start\":21321},{\"end\":22094,\"start\":21388},{\"end\":22224,\"start\":22096},{\"end\":22869,\"start\":22226},{\"end\":22907,\"start\":22871},{\"end\":24005,\"start\":22922},{\"end\":24454,\"start\":24007},{\"end\":24986,\"start\":24456},{\"end\":25542,\"start\":25001},{\"end\":25725,\"start\":25581},{\"end\":25815,\"start\":25738},{\"end\":25899,\"start\":25817},{\"end\":26242,\"start\":25923},{\"end\":26616,\"start\":26264},{\"end\":26678,\"start\":26632},{\"end\":26709,\"start\":26704},{\"end\":26740,\"start\":26711},{\"end\":26789,\"start\":26770},{\"end\":26871,\"start\":26791},{\"end\":26930,\"start\":26873},{\"end\":27058,\"start\":26932},{\"end\":27160,\"start\":27107},{\"end\":27414,\"start\":27184},{\"end\":28561,\"start\":27440},{\"end\":30247,\"start\":28600}]", "formula": null, "table_ref": "[{\"end\":17982,\"start\":17975},{\"end\":19452,\"start\":19445},{\"end\":19873,\"start\":19866},{\"end\":20711,\"start\":20704},{\"end\":21012,\"start\":21005},{\"end\":21602,\"start\":21595},{\"end\":22163,\"start\":22156},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22418,\"start\":22411},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":27287,\"start\":27280},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":27413,\"start\":27406}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1137,\"start\":1125},{\"attributes\":{\"n\":\"2.\"},\"end\":6141,\"start\":6129},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6390,\"start\":6371},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7365,\"start\":7336},{\"attributes\":{\"n\":\"3.\"},\"end\":8124,\"start\":8118},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8270,\"start\":8260},{\"attributes\":{\"n\":\"3.2.\"},\"end\":8833,\"start\":8822},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":8871,\"start\":8836},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":9467,\"start\":9451},{\"attributes\":{\"n\":\"3.3.\"},\"end\":10081,\"start\":10069},{\"attributes\":{\"n\":\"4.\"},\"end\":11607,\"start\":11600},{\"attributes\":{\"n\":\"4.1.\"},\"end\":11774,\"start\":11757},{\"attributes\":{\"n\":\"4.2.\"},\"end\":13307,\"start\":13293},{\"attributes\":{\"n\":\"4.3.\"},\"end\":14019,\"start\":14007},{\"attributes\":{\"n\":\"5.\"},\"end\":15898,\"start\":15878},{\"attributes\":{\"n\":\"5.1.\"},\"end\":16475,\"start\":16458},{\"attributes\":{\"n\":\"5.2.\"},\"end\":17432,\"start\":17410},{\"attributes\":{\"n\":\"5.3.\"},\"end\":18254,\"start\":18227},{\"attributes\":{\"n\":\"5.4.\"},\"end\":18571,\"start\":18548},{\"attributes\":{\"n\":\"5.5.\"},\"end\":20164,\"start\":20142},{\"attributes\":{\"n\":\"5.6.\"},\"end\":21386,\"start\":21372},{\"attributes\":{\"n\":\"6.\"},\"end\":22920,\"start\":22910},{\"attributes\":{\"n\":\"7.\"},\"end\":24999,\"start\":24989},{\"end\":25558,\"start\":25545},{\"end\":25579,\"start\":25561},{\"end\":25736,\"start\":25728},{\"end\":25907,\"start\":25902},{\"end\":25921,\"start\":25910},{\"end\":26262,\"start\":26245},{\"end\":26630,\"start\":26619},{\"end\":26702,\"start\":26681},{\"end\":26768,\"start\":26743},{\"end\":27082,\"start\":27061},{\"end\":27105,\"start\":27085},{\"end\":27182,\"start\":27163},{\"end\":27438,\"start\":27417},{\"end\":28598,\"start\":28564},{\"end\":30259,\"start\":30249},{\"end\":30318,\"start\":30308},{\"end\":30530,\"start\":30520},{\"end\":30675,\"start\":30665},{\"end\":30837,\"start\":30827},{\"end\":30986,\"start\":30976},{\"end\":35659,\"start\":35650},{\"end\":35943,\"start\":35934},{\"end\":36051,\"start\":36042},{\"end\":37373,\"start\":37364},{\"end\":37682,\"start\":37673}]", "table": "[{\"end\":35648,\"start\":31245},{\"end\":35932,\"start\":35661},{\"end\":36040,\"start\":35945},{\"end\":36096,\"start\":36080},{\"end\":37362,\"start\":36708},{\"end\":37671,\"start\":37539}]", "figure_caption": "[{\"end\":30306,\"start\":30261},{\"end\":30518,\"start\":30320},{\"end\":30663,\"start\":30532},{\"end\":30821,\"start\":30677},{\"end\":30825,\"start\":30824},{\"end\":30974,\"start\":30839},{\"end\":31051,\"start\":30988},{\"end\":31245,\"start\":31054},{\"end\":36080,\"start\":36053},{\"end\":36708,\"start\":36099},{\"end\":37539,\"start\":37375},{\"end\":37912,\"start\":37684}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9111,\"start\":9102},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9446,\"start\":9436},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10785,\"start\":10777},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11815,\"start\":11807},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12192,\"start\":12184},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13372,\"start\":13364},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13754,\"start\":13745},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14003,\"start\":13995},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":14080,\"start\":14072},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19856,\"start\":19848},{\"end\":23797,\"start\":23789},{\"end\":28615,\"start\":28603},{\"end\":28733,\"start\":28725},{\"end\":29210,\"start\":29202},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29608,\"start\":29599},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30044,\"start\":30035}]", "bib_author_first_name": "[{\"end\":37967,\"start\":37961},{\"end\":37975,\"start\":37973},{\"end\":37989,\"start\":37982},{\"end\":38000,\"start\":37996},{\"end\":38229,\"start\":38223},{\"end\":38242,\"start\":38238},{\"end\":38820,\"start\":38814},{\"end\":38837,\"start\":38829},{\"end\":38847,\"start\":38844},{\"end\":39298,\"start\":39290},{\"end\":39316,\"start\":39306},{\"end\":39330,\"start\":39322},{\"end\":39347,\"start\":39342},{\"end\":39674,\"start\":39666},{\"end\":39692,\"start\":39682},{\"end\":39706,\"start\":39698},{\"end\":39723,\"start\":39718},{\"end\":40050,\"start\":40047},{\"end\":40060,\"start\":40057},{\"end\":40074,\"start\":40067},{\"end\":40089,\"start\":40083},{\"end\":40097,\"start\":40094},{\"end\":40104,\"start\":40102},{\"end\":40374,\"start\":40369},{\"end\":40391,\"start\":40383},{\"end\":40405,\"start\":40399},{\"end\":40419,\"start\":40411},{\"end\":41229,\"start\":41223},{\"end\":41248,\"start\":41243},{\"end\":41265,\"start\":41256},{\"end\":41282,\"start\":41278},{\"end\":41303,\"start\":41296},{\"end\":41316,\"start\":41310},{\"end\":41337,\"start\":41330},{\"end\":41356,\"start\":41348},{\"end\":41372,\"start\":41367},{\"end\":41856,\"start\":41852},{\"end\":42037,\"start\":42030},{\"end\":42048,\"start\":42042},{\"end\":42062,\"start\":42055},{\"end\":42075,\"start\":42068},{\"end\":42085,\"start\":42080},{\"end\":42098,\"start\":42094},{\"end\":42585,\"start\":42580},{\"end\":42597,\"start\":42596},{\"end\":42612,\"start\":42607},{\"end\":42630,\"start\":42622},{\"end\":43004,\"start\":43001},{\"end\":43021,\"start\":43012},{\"end\":43028,\"start\":43026},{\"end\":43041,\"start\":43034},{\"end\":43056,\"start\":43046},{\"end\":43524,\"start\":43523},{\"end\":43540,\"start\":43535},{\"end\":43764,\"start\":43755},{\"end\":43782,\"start\":43772},{\"end\":43800,\"start\":43793},{\"end\":44250,\"start\":44244},{\"end\":44262,\"start\":44258},{\"end\":44277,\"start\":44271},{\"end\":44292,\"start\":44288},{\"end\":44650,\"start\":44643},{\"end\":44664,\"start\":44656},{\"end\":44679,\"start\":44670},{\"end\":45103,\"start\":45097},{\"end\":45119,\"start\":45114},{\"end\":45133,\"start\":45130},{\"end\":45152,\"start\":45146},{\"end\":45164,\"start\":45160},{\"end\":45178,\"start\":45173},{\"end\":45192,\"start\":45188},{\"end\":45512,\"start\":45505},{\"end\":45528,\"start\":45520},{\"end\":45540,\"start\":45534},{\"end\":45549,\"start\":45546},{\"end\":45560,\"start\":45557},{\"end\":45575,\"start\":45571},{\"end\":45931,\"start\":45923},{\"end\":45944,\"start\":45937},{\"end\":45957,\"start\":45951},{\"end\":45967,\"start\":45963},{\"end\":46597,\"start\":46595},{\"end\":46609,\"start\":46603},{\"end\":46618,\"start\":46615},{\"end\":46627,\"start\":46624},{\"end\":46638,\"start\":46632},{\"end\":46649,\"start\":46644},{\"end\":46664,\"start\":46657},{\"end\":46677,\"start\":46670},{\"end\":47154,\"start\":47150},{\"end\":47172,\"start\":47167},{\"end\":47443,\"start\":47437},{\"end\":47464,\"start\":47454},{\"end\":47848,\"start\":47843},{\"end\":47865,\"start\":47857},{\"end\":47880,\"start\":47875},{\"end\":47894,\"start\":47886},{\"end\":48226,\"start\":48222},{\"end\":48237,\"start\":48234},{\"end\":48249,\"start\":48247},{\"end\":48264,\"start\":48257},{\"end\":48740,\"start\":48733},{\"end\":48751,\"start\":48747},{\"end\":48764,\"start\":48759},{\"end\":48774,\"start\":48771},{\"end\":49215,\"start\":49211},{\"end\":49227,\"start\":49224},{\"end\":49242,\"start\":49235},{\"end\":49260,\"start\":49253},{\"end\":49275,\"start\":49269},{\"end\":49289,\"start\":49282},{\"end\":49304,\"start\":49298},{\"end\":49315,\"start\":49310},{\"end\":49331,\"start\":49327},{\"end\":49344,\"start\":49340},{\"end\":49636,\"start\":49631},{\"end\":49650,\"start\":49643},{\"end\":49664,\"start\":49657},{\"end\":49674,\"start\":49671},{\"end\":50160,\"start\":50156},{\"end\":50469,\"start\":50465},{\"end\":50490,\"start\":50483},{\"end\":50506,\"start\":50500},{\"end\":51116,\"start\":51110},{\"end\":51124,\"start\":51123},{\"end\":51389,\"start\":51382},{\"end\":51405,\"start\":51396},{\"end\":51420,\"start\":51411},{\"end\":51433,\"start\":51427},{\"end\":51976,\"start\":51970},{\"end\":51990,\"start\":51986},{\"end\":52004,\"start\":52000},{\"end\":52018,\"start\":52013},{\"end\":52035,\"start\":52030},{\"end\":52048,\"start\":52043},{\"end\":52050,\"start\":52049},{\"end\":52063,\"start\":52058},{\"end\":52684,\"start\":52676},{\"end\":52699,\"start\":52691},{\"end\":52712,\"start\":52705},{\"end\":52725,\"start\":52718},{\"end\":52742,\"start\":52732},{\"end\":52756,\"start\":52748},{\"end\":53260,\"start\":53255},{\"end\":53275,\"start\":53267},{\"end\":53288,\"start\":53281},{\"end\":53296,\"start\":53293},{\"end\":53770,\"start\":53764},{\"end\":53779,\"start\":53775},{\"end\":53788,\"start\":53785},{\"end\":53801,\"start\":53794},{\"end\":53815,\"start\":53807},{\"end\":54249,\"start\":54242},{\"end\":54264,\"start\":54255},{\"end\":54279,\"start\":54271},{\"end\":54659,\"start\":54653},{\"end\":54684,\"start\":54678},{\"end\":54699,\"start\":54692},{\"end\":54723,\"start\":54713},{\"end\":55251,\"start\":55246},{\"end\":55266,\"start\":55259},{\"end\":55278,\"start\":55273},{\"end\":55614,\"start\":55611},{\"end\":55630,\"start\":55622},{\"end\":55874,\"start\":55871},{\"end\":55890,\"start\":55882},{\"end\":55902,\"start\":55896},{\"end\":55913,\"start\":55909},{\"end\":55923,\"start\":55920},{\"end\":56202,\"start\":56198},{\"end\":56216,\"start\":56210},{\"end\":56226,\"start\":56222},{\"end\":56243,\"start\":56236},{\"end\":56256,\"start\":56250},{\"end\":56268,\"start\":56264},{\"end\":56281,\"start\":56276}]", "bib_author_last_name": "[{\"end\":37971,\"start\":37968},{\"end\":37980,\"start\":37976},{\"end\":37994,\"start\":37990},{\"end\":38004,\"start\":38001},{\"end\":38236,\"start\":38230},{\"end\":38248,\"start\":38243},{\"end\":38827,\"start\":38821},{\"end\":38842,\"start\":38838},{\"end\":38853,\"start\":38848},{\"end\":39304,\"start\":39299},{\"end\":39320,\"start\":39317},{\"end\":39340,\"start\":39331},{\"end\":39358,\"start\":39348},{\"end\":39680,\"start\":39675},{\"end\":39696,\"start\":39693},{\"end\":39716,\"start\":39707},{\"end\":39734,\"start\":39724},{\"end\":40055,\"start\":40051},{\"end\":40065,\"start\":40061},{\"end\":40081,\"start\":40075},{\"end\":40092,\"start\":40090},{\"end\":40100,\"start\":40098},{\"end\":40112,\"start\":40105},{\"end\":40381,\"start\":40375},{\"end\":40397,\"start\":40392},{\"end\":40409,\"start\":40406},{\"end\":40429,\"start\":40420},{\"end\":41241,\"start\":41230},{\"end\":41254,\"start\":41249},{\"end\":41276,\"start\":41266},{\"end\":41294,\"start\":41283},{\"end\":41308,\"start\":41304},{\"end\":41328,\"start\":41317},{\"end\":41346,\"start\":41338},{\"end\":41365,\"start\":41357},{\"end\":41380,\"start\":41373},{\"end\":41864,\"start\":41857},{\"end\":42040,\"start\":42038},{\"end\":42053,\"start\":42049},{\"end\":42066,\"start\":42063},{\"end\":42078,\"start\":42076},{\"end\":42092,\"start\":42086},{\"end\":42107,\"start\":42099},{\"end\":42594,\"start\":42586},{\"end\":42605,\"start\":42598},{\"end\":42620,\"start\":42613},{\"end\":42639,\"start\":42631},{\"end\":42647,\"start\":42641},{\"end\":43010,\"start\":43005},{\"end\":43024,\"start\":43022},{\"end\":43032,\"start\":43029},{\"end\":43044,\"start\":43042},{\"end\":43060,\"start\":43057},{\"end\":43533,\"start\":43525},{\"end\":43547,\"start\":43541},{\"end\":43551,\"start\":43549},{\"end\":43770,\"start\":43765},{\"end\":43791,\"start\":43783},{\"end\":43804,\"start\":43801},{\"end\":44256,\"start\":44251},{\"end\":44269,\"start\":44263},{\"end\":44286,\"start\":44278},{\"end\":44297,\"start\":44293},{\"end\":44654,\"start\":44651},{\"end\":44668,\"start\":44665},{\"end\":44683,\"start\":44680},{\"end\":45112,\"start\":45104},{\"end\":45128,\"start\":45120},{\"end\":45144,\"start\":45134},{\"end\":45158,\"start\":45153},{\"end\":45171,\"start\":45165},{\"end\":45186,\"start\":45179},{\"end\":45197,\"start\":45193},{\"end\":45518,\"start\":45513},{\"end\":45532,\"start\":45529},{\"end\":45544,\"start\":45541},{\"end\":45555,\"start\":45550},{\"end\":45569,\"start\":45561},{\"end\":45583,\"start\":45576},{\"end\":45591,\"start\":45585},{\"end\":45935,\"start\":45932},{\"end\":45949,\"start\":45945},{\"end\":45961,\"start\":45958},{\"end\":45973,\"start\":45968},{\"end\":46601,\"start\":46598},{\"end\":46613,\"start\":46610},{\"end\":46622,\"start\":46619},{\"end\":46630,\"start\":46628},{\"end\":46642,\"start\":46639},{\"end\":46655,\"start\":46650},{\"end\":46668,\"start\":46665},{\"end\":46681,\"start\":46678},{\"end\":47165,\"start\":47155},{\"end\":47179,\"start\":47173},{\"end\":47452,\"start\":47444},{\"end\":47468,\"start\":47465},{\"end\":47855,\"start\":47849},{\"end\":47873,\"start\":47866},{\"end\":47884,\"start\":47881},{\"end\":47900,\"start\":47895},{\"end\":48232,\"start\":48227},{\"end\":48245,\"start\":48238},{\"end\":48255,\"start\":48250},{\"end\":48270,\"start\":48265},{\"end\":48745,\"start\":48741},{\"end\":48757,\"start\":48752},{\"end\":48769,\"start\":48765},{\"end\":48777,\"start\":48775},{\"end\":49222,\"start\":49216},{\"end\":49233,\"start\":49228},{\"end\":49251,\"start\":49243},{\"end\":49267,\"start\":49261},{\"end\":49280,\"start\":49276},{\"end\":49296,\"start\":49290},{\"end\":49308,\"start\":49305},{\"end\":49325,\"start\":49316},{\"end\":49338,\"start\":49332},{\"end\":49350,\"start\":49345},{\"end\":49641,\"start\":49637},{\"end\":49655,\"start\":49651},{\"end\":49669,\"start\":49665},{\"end\":49677,\"start\":49675},{\"end\":50172,\"start\":50161},{\"end\":50481,\"start\":50470},{\"end\":50498,\"start\":50491},{\"end\":50511,\"start\":50507},{\"end\":51121,\"start\":51117},{\"end\":51132,\"start\":51125},{\"end\":51139,\"start\":51134},{\"end\":51394,\"start\":51390},{\"end\":51409,\"start\":51406},{\"end\":51425,\"start\":51421},{\"end\":51438,\"start\":51434},{\"end\":51984,\"start\":51977},{\"end\":51998,\"start\":51991},{\"end\":52011,\"start\":52005},{\"end\":52028,\"start\":52019},{\"end\":52041,\"start\":52036},{\"end\":52056,\"start\":52051},{\"end\":52070,\"start\":52064},{\"end\":52082,\"start\":52072},{\"end\":52689,\"start\":52685},{\"end\":52703,\"start\":52700},{\"end\":52716,\"start\":52713},{\"end\":52730,\"start\":52726},{\"end\":52746,\"start\":52743},{\"end\":52759,\"start\":52757},{\"end\":53265,\"start\":53261},{\"end\":53279,\"start\":53276},{\"end\":53291,\"start\":53289},{\"end\":53300,\"start\":53297},{\"end\":53773,\"start\":53771},{\"end\":53783,\"start\":53780},{\"end\":53792,\"start\":53789},{\"end\":53805,\"start\":53802},{\"end\":53819,\"start\":53816},{\"end\":54253,\"start\":54250},{\"end\":54269,\"start\":54265},{\"end\":54282,\"start\":54280},{\"end\":54676,\"start\":54660},{\"end\":54690,\"start\":54685},{\"end\":54704,\"start\":54700},{\"end\":54711,\"start\":54706},{\"end\":54742,\"start\":54724},{\"end\":54748,\"start\":54744},{\"end\":55257,\"start\":55252},{\"end\":55271,\"start\":55267},{\"end\":55286,\"start\":55279},{\"end\":55620,\"start\":55615},{\"end\":55635,\"start\":55631},{\"end\":55880,\"start\":55875},{\"end\":55894,\"start\":55891},{\"end\":55907,\"start\":55903},{\"end\":55918,\"start\":55914},{\"end\":55929,\"start\":55924},{\"end\":56208,\"start\":56203},{\"end\":56220,\"start\":56217},{\"end\":56234,\"start\":56227},{\"end\":56248,\"start\":56244},{\"end\":56262,\"start\":56257},{\"end\":56274,\"start\":56269},{\"end\":56288,\"start\":56282}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2106.08254\",\"id\":\"b0\"},\"end\":38172,\"start\":37914},{\"attributes\":{\"doi\":\"PMLR, 09- 15\",\"id\":\"b1\",\"matched_paper_id\":59523708},\"end\":38769,\"start\":38174},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11206708},\"end\":39172,\"start\":38771},{\"attributes\":{\"id\":\"b3\"},\"end\":39593,\"start\":39174},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1475121},\"end\":39992,\"start\":39595},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":57246310},\"end\":40285,\"start\":39994},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":41221,\"start\":40287},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b7\"},\"end\":41811,\"start\":41223},{\"attributes\":{\"id\":\"b8\"},\"end\":41978,\"start\":41813},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":243985980},\"end\":42508,\"start\":41980},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220734100},\"end\":42928,\"start\":42510},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":231419143},\"end\":43477,\"start\":42930},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b12\"},\"end\":43697,\"start\":43479},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53751136},\"end\":44191,\"start\":43699},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":173990648},\"end\":44544,\"start\":44193},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":247596985},\"end\":45095,\"start\":44546},{\"attributes\":{\"doi\":\"arXiv:1803.04189\",\"id\":\"b16\"},\"end\":45503,\"start\":45097},{\"attributes\":{\"doi\":\"arXiv:2108.10257\",\"id\":\"b17\"},\"end\":45847,\"start\":45505},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":250562472},\"end\":46520,\"start\":45849},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":232352874},\"end\":47094,\"start\":46522},{\"attributes\":{\"doi\":\"arXiv:1608.03983\",\"id\":\"b20\"},\"end\":47348,\"start\":47096},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10229455},\"end\":47701,\"start\":47350},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":64193},\"end\":48159,\"start\":47703},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":204904999},\"end\":48655,\"start\":48161},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":235719899},\"end\":49171,\"start\":48657},{\"attributes\":{\"id\":\"b25\"},\"end\":49551,\"start\":49173},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":219619080},\"end\":50077,\"start\":49553},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":251648812},\"end\":50398,\"start\":50079},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3719281},\"end\":51050,\"start\":50400},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2843211},\"end\":51291,\"start\":51052},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":215814272},\"end\":51941,\"start\":51293},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13756489},\"end\":52611,\"start\":51943},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":235358213},\"end\":53180,\"start\":52613},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":247447122},\"end\":53719,\"start\":53182},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":221376798},\"end\":54172,\"start\":53721},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":225062070},\"end\":54579,\"start\":54174},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":244346144},\"end\":55189,\"start\":54581},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2356330},\"end\":55523,\"start\":55191},{\"attributes\":{\"id\":\"b38\"},\"end\":55790,\"start\":55525},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":996788},\"end\":56117,\"start\":55792},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":56895444},\"end\":56503,\"start\":56119}]", "bib_title": "[{\"end\":38221,\"start\":38174},{\"end\":38812,\"start\":38771},{\"end\":39664,\"start\":39595},{\"end\":40045,\"start\":39994},{\"end\":40367,\"start\":40287},{\"end\":42028,\"start\":41980},{\"end\":42578,\"start\":42510},{\"end\":42999,\"start\":42930},{\"end\":43753,\"start\":43699},{\"end\":44242,\"start\":44193},{\"end\":44641,\"start\":44546},{\"end\":45921,\"start\":45849},{\"end\":46593,\"start\":46522},{\"end\":47435,\"start\":47350},{\"end\":47841,\"start\":47703},{\"end\":48220,\"start\":48161},{\"end\":48731,\"start\":48657},{\"end\":49629,\"start\":49553},{\"end\":50154,\"start\":50079},{\"end\":50463,\"start\":50400},{\"end\":51108,\"start\":51052},{\"end\":51380,\"start\":51293},{\"end\":51968,\"start\":51943},{\"end\":52674,\"start\":52613},{\"end\":53253,\"start\":53182},{\"end\":53762,\"start\":53721},{\"end\":54240,\"start\":54174},{\"end\":54651,\"start\":54581},{\"end\":55244,\"start\":55191},{\"end\":55869,\"start\":55792},{\"end\":56196,\"start\":56119}]", "bib_author": "[{\"end\":37973,\"start\":37961},{\"end\":37982,\"start\":37973},{\"end\":37996,\"start\":37982},{\"end\":38006,\"start\":37996},{\"end\":38238,\"start\":38223},{\"end\":38250,\"start\":38238},{\"end\":38829,\"start\":38814},{\"end\":38844,\"start\":38829},{\"end\":38855,\"start\":38844},{\"end\":39306,\"start\":39290},{\"end\":39322,\"start\":39306},{\"end\":39342,\"start\":39322},{\"end\":39360,\"start\":39342},{\"end\":39682,\"start\":39666},{\"end\":39698,\"start\":39682},{\"end\":39718,\"start\":39698},{\"end\":39736,\"start\":39718},{\"end\":40057,\"start\":40047},{\"end\":40067,\"start\":40057},{\"end\":40083,\"start\":40067},{\"end\":40094,\"start\":40083},{\"end\":40102,\"start\":40094},{\"end\":40114,\"start\":40102},{\"end\":40383,\"start\":40369},{\"end\":40399,\"start\":40383},{\"end\":40411,\"start\":40399},{\"end\":40431,\"start\":40411},{\"end\":41243,\"start\":41223},{\"end\":41256,\"start\":41243},{\"end\":41278,\"start\":41256},{\"end\":41296,\"start\":41278},{\"end\":41310,\"start\":41296},{\"end\":41330,\"start\":41310},{\"end\":41348,\"start\":41330},{\"end\":41367,\"start\":41348},{\"end\":41382,\"start\":41367},{\"end\":41866,\"start\":41852},{\"end\":42042,\"start\":42030},{\"end\":42055,\"start\":42042},{\"end\":42068,\"start\":42055},{\"end\":42080,\"start\":42068},{\"end\":42094,\"start\":42080},{\"end\":42109,\"start\":42094},{\"end\":42596,\"start\":42580},{\"end\":42607,\"start\":42596},{\"end\":42622,\"start\":42607},{\"end\":42641,\"start\":42622},{\"end\":42649,\"start\":42641},{\"end\":43012,\"start\":43001},{\"end\":43026,\"start\":43012},{\"end\":43034,\"start\":43026},{\"end\":43046,\"start\":43034},{\"end\":43062,\"start\":43046},{\"end\":43535,\"start\":43523},{\"end\":43549,\"start\":43535},{\"end\":43553,\"start\":43549},{\"end\":43772,\"start\":43755},{\"end\":43793,\"start\":43772},{\"end\":43806,\"start\":43793},{\"end\":44258,\"start\":44244},{\"end\":44271,\"start\":44258},{\"end\":44288,\"start\":44271},{\"end\":44299,\"start\":44288},{\"end\":44656,\"start\":44643},{\"end\":44670,\"start\":44656},{\"end\":44685,\"start\":44670},{\"end\":45114,\"start\":45097},{\"end\":45130,\"start\":45114},{\"end\":45146,\"start\":45130},{\"end\":45160,\"start\":45146},{\"end\":45173,\"start\":45160},{\"end\":45188,\"start\":45173},{\"end\":45199,\"start\":45188},{\"end\":45520,\"start\":45505},{\"end\":45534,\"start\":45520},{\"end\":45546,\"start\":45534},{\"end\":45557,\"start\":45546},{\"end\":45571,\"start\":45557},{\"end\":45585,\"start\":45571},{\"end\":45593,\"start\":45585},{\"end\":45937,\"start\":45923},{\"end\":45951,\"start\":45937},{\"end\":45963,\"start\":45951},{\"end\":45975,\"start\":45963},{\"end\":46603,\"start\":46595},{\"end\":46615,\"start\":46603},{\"end\":46624,\"start\":46615},{\"end\":46632,\"start\":46624},{\"end\":46644,\"start\":46632},{\"end\":46657,\"start\":46644},{\"end\":46670,\"start\":46657},{\"end\":46683,\"start\":46670},{\"end\":47167,\"start\":47150},{\"end\":47181,\"start\":47167},{\"end\":47454,\"start\":47437},{\"end\":47470,\"start\":47454},{\"end\":47857,\"start\":47843},{\"end\":47875,\"start\":47857},{\"end\":47886,\"start\":47875},{\"end\":47902,\"start\":47886},{\"end\":48234,\"start\":48222},{\"end\":48247,\"start\":48234},{\"end\":48257,\"start\":48247},{\"end\":48272,\"start\":48257},{\"end\":48747,\"start\":48733},{\"end\":48759,\"start\":48747},{\"end\":48771,\"start\":48759},{\"end\":48779,\"start\":48771},{\"end\":49224,\"start\":49211},{\"end\":49235,\"start\":49224},{\"end\":49253,\"start\":49235},{\"end\":49269,\"start\":49253},{\"end\":49282,\"start\":49269},{\"end\":49298,\"start\":49282},{\"end\":49310,\"start\":49298},{\"end\":49327,\"start\":49310},{\"end\":49340,\"start\":49327},{\"end\":49352,\"start\":49340},{\"end\":49643,\"start\":49631},{\"end\":49657,\"start\":49643},{\"end\":49671,\"start\":49657},{\"end\":49679,\"start\":49671},{\"end\":50174,\"start\":50156},{\"end\":50483,\"start\":50465},{\"end\":50500,\"start\":50483},{\"end\":50513,\"start\":50500},{\"end\":51123,\"start\":51110},{\"end\":51134,\"start\":51123},{\"end\":51141,\"start\":51134},{\"end\":51396,\"start\":51382},{\"end\":51411,\"start\":51396},{\"end\":51427,\"start\":51411},{\"end\":51440,\"start\":51427},{\"end\":51986,\"start\":51970},{\"end\":52000,\"start\":51986},{\"end\":52013,\"start\":52000},{\"end\":52030,\"start\":52013},{\"end\":52043,\"start\":52030},{\"end\":52058,\"start\":52043},{\"end\":52072,\"start\":52058},{\"end\":52084,\"start\":52072},{\"end\":52691,\"start\":52676},{\"end\":52705,\"start\":52691},{\"end\":52718,\"start\":52705},{\"end\":52732,\"start\":52718},{\"end\":52748,\"start\":52732},{\"end\":52761,\"start\":52748},{\"end\":53267,\"start\":53255},{\"end\":53281,\"start\":53267},{\"end\":53293,\"start\":53281},{\"end\":53302,\"start\":53293},{\"end\":53775,\"start\":53764},{\"end\":53785,\"start\":53775},{\"end\":53794,\"start\":53785},{\"end\":53807,\"start\":53794},{\"end\":53821,\"start\":53807},{\"end\":54255,\"start\":54242},{\"end\":54271,\"start\":54255},{\"end\":54284,\"start\":54271},{\"end\":54678,\"start\":54653},{\"end\":54692,\"start\":54678},{\"end\":54706,\"start\":54692},{\"end\":54713,\"start\":54706},{\"end\":54744,\"start\":54713},{\"end\":54750,\"start\":54744},{\"end\":55259,\"start\":55246},{\"end\":55273,\"start\":55259},{\"end\":55288,\"start\":55273},{\"end\":55622,\"start\":55611},{\"end\":55637,\"start\":55622},{\"end\":55882,\"start\":55871},{\"end\":55896,\"start\":55882},{\"end\":55909,\"start\":55896},{\"end\":55920,\"start\":55909},{\"end\":55931,\"start\":55920},{\"end\":56210,\"start\":56198},{\"end\":56222,\"start\":56210},{\"end\":56236,\"start\":56222},{\"end\":56250,\"start\":56236},{\"end\":56264,\"start\":56250},{\"end\":56276,\"start\":56264},{\"end\":56290,\"start\":56276}]", "bib_venue": "[{\"end\":38428,\"start\":38375},{\"end\":40724,\"start\":40575},{\"end\":42258,\"start\":42192},{\"end\":43211,\"start\":43145},{\"end\":43955,\"start\":43889},{\"end\":44834,\"start\":44768},{\"end\":46177,\"start\":46073},{\"end\":46826,\"start\":46763},{\"end\":48421,\"start\":48355},{\"end\":48928,\"start\":48862},{\"end\":49828,\"start\":49762},{\"end\":50664,\"start\":50660},{\"end\":51607,\"start\":51529},{\"end\":52910,\"start\":52844},{\"end\":53451,\"start\":53385},{\"end\":53885,\"start\":53874},{\"end\":54899,\"start\":54833},{\"end\":37959,\"start\":37914},{\"end\":38330,\"start\":38262},{\"end\":38940,\"start\":38855},{\"end\":39288,\"start\":39174},{\"end\":39773,\"start\":39736},{\"end\":40118,\"start\":40114},{\"end\":40573,\"start\":40431},{\"end\":41494,\"start\":41398},{\"end\":41850,\"start\":41813},{\"end\":42190,\"start\":42109},{\"end\":42696,\"start\":42649},{\"end\":43143,\"start\":43062},{\"end\":43521,\"start\":43479},{\"end\":43887,\"start\":43806},{\"end\":44348,\"start\":44299},{\"end\":44766,\"start\":44685},{\"end\":45273,\"start\":45215},{\"end\":45649,\"start\":45609},{\"end\":46071,\"start\":45975},{\"end\":46761,\"start\":46683},{\"end\":47148,\"start\":47096},{\"end\":47507,\"start\":47470},{\"end\":47906,\"start\":47902},{\"end\":48353,\"start\":48272},{\"end\":48860,\"start\":48779},{\"end\":49209,\"start\":49173},{\"end\":49760,\"start\":49679},{\"end\":50226,\"start\":50174},{\"end\":50584,\"start\":50513},{\"end\":51145,\"start\":51141},{\"end\":51527,\"start\":51440},{\"end\":52133,\"start\":52084},{\"end\":52842,\"start\":52761},{\"end\":53383,\"start\":53302},{\"end\":53872,\"start\":53821},{\"end\":54333,\"start\":54284},{\"end\":54831,\"start\":54750},{\"end\":55335,\"start\":55288},{\"end\":55609,\"start\":55525},{\"end\":55934,\"start\":55931},{\"end\":56294,\"start\":56290}]"}}}, "year": 2023, "month": 12, "day": 17}