{"id": 220919638, "updated": "2023-04-05 06:31:11.668", "metadata": {"title": "Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions", "authors": "[{\"first\":\"Yu-Siang\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Yi-Hsuan\",\"last\":\"Yang\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 28th ACM International Conference on Multimedia", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3046715528", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/HuangY20", "doi": "10.1145/3394171.3413671"}}, "content": {"source": {"pdf_hash": "ce510c6cfeac703706460680e977c54554840830", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1e90a68365e84b21b187f2cabae7f6d823e1faed", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ce510c6cfeac703706460680e977c54554840830.txt", "contents": "\nPop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions\n2020. October 12-16, 2020\n\nYu-Siang Huang yshuang@ailabs.tw \nYi-Hsuan Yang yhyang@ailabs.tw \nYu-Siang Huang \nYi-Hsuan Yang \n\nAI Labs & Academia Sinica Taipei\nTaiwan AI Labs & Academia Sinica Taipei\nTaiwan, Taiwan, Taiwan\n\n\nACM Reference Format\n\n\nPop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions\n\n28th ACM International Conference on Multimedia (MM '20)\nSeattle, WA, USA2020. October 12-16, 202010.1145/3394171.3413671.. ACM, New York, NY, USA, 9 pages. https: //Automatic music compositiontransformerneural sequence model\nA great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models.CCS CONCEPTS\u2022 Applied computing \u2192 Sound and music computing.\n\nINTRODUCTION\n\nMusic is sound that's organized on purpose on many time and frequency levels to express different ideas and emotions. For example, the organization of musical notes of different fundamental frequencies (from low to high) influences the melody, harmony and texture of music. The placement of strong and weak beats over time, on the other hand, gives rise to the perception of rhythm [28]. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM '20, October 12-16, 2020 [23,30] versus the proposed beat-based one, REMI. In the brackets, we show the ranges of the type of event.\n\nRepetition and long-term structure are also important factors that make a musical piece coherent and understandable. Building machines that can compose music like human beings is one of the most exciting tasks in multimedia and artificial intelligence [5,7,13,18,26,27,31,44,45]. Among the approaches that have been studied, neural sequence models, which consider music as a language, stand out in recent work [9,23,30,32] as a prominent approach with great potential. 1 In doing so, a digital representation of a musical score is converted into a time-ordered sequence of discrete tokens such as the Note-On events. Sequence models such as the Transformer [39] can then be applied to model the probability distribution of the event sequences, and to sample from the distribution to generate new music compositions. This approach has been shown to work well for composing minute-long pieces of classical piano music with expressive variations in note density and velocity [23], in the format of a MIDI file. 2 We note that there are two critical elements in the aforementioned approach-the way music is converted into discrete tokens for language modeling, and the machine learning algorithm used to build the model. Recent years have witnessed great progress regarding the second element, for example, by improving the self-attention in Transformers with \"sparse attention\" [8], or introducing a recurrence mechanism as in Transformer-XL for learning longer-range 1 However, whether music and language are related is actually debatable from a musicology point of view. The computational approach of modeling music in the same way as modeling natural language therefore may have limitations. 2 Therefore, the model generates the pitch, velocity (dynamics), onset and offset time of each note, including those involved in the melody line, the underlying chord progression, and the bass line, etc in an expressive piano performance.  3) for music generated by an adaptation of the state-of-the-art model Music Transformer [23], and the proposed model. We can see clearer presence of regularly-spaced downbeats in the probability curve of (b).\n\ndependency [11,12]. However, no much has been done for the first one. Most work simply follows the MIDI-like event representation proposed by [30] to set up the \"vocabulary\" for music.\n\nAs shown in Table 1, the MIDI-like representation, for the case of modeling classical piano music [9,23], uses Note-On events to indicate the action of hitting a specific key of the piano keyboard, and Note-Off for the release of the key. This representation has its roots in MIDI [22], a communication protocol that also uses Note-On and Note-Off messages to transmit data of real-time musical performance. Unlike words in human language, note messages in MIDI are associated with time. To convert a score into a sequence, [30] uses additionally the Time-Shift (\u0394 ) events to indicate the relative time gap between events (rather than the absolute time of the events), thereby representing the advance in time. This MIDIlike representation is general and represents keyboard-style music (such as piano music) fairly faithfully, therefore a good choice as the format for the input data fed to Transformers. A general idea here is that a deep network can learn abstractions of the MIDI data that contribute to the generative musical modeling on its own. Hence, high-level (semantic) information of music [4], such as downbeat, tempo and chord, are not included in this MIDI-like representation.\n\nHowever, we note that when humans compose music, we tend to organize regularly recurring patterns and accents over a metrical structure defined in terms of sub-beats, beats, and bars [10]. Such a structure is made clear on a score sheet or a MIDI file with notation of the time signature and bar lines, but is implicit in the MIDI-like event representation. A sequence model has to recover the metrical structure on its own from the provided sequence of tokens. When it comes to modeling music genres that feature the use of steady beats, we observe that the Transformer [23] has a hard time learning the regularity of beats over time, as shown in Figure 1a.  Figure 2: The diagram of the proposed beat-based music modeling and generation framework. The training stage entails the use of music information retrieval (MIR) techniques, such as automatic transcription, to convert an audio signal into an event sequence, which is then fed to a sequence model for training. At inference time, the model generates an original event sequence, which can then be converted into a MIDI file for playback.\n\nTo remedy this, and to study whether Transformer-based composition models can benefit from the addition of some human knowledge of music, we propose REMI, which stands for revamped MIDIderived events, to represent MIDI data following the way humans read them. Specifically, we introduce the Bar event to indicate the beginning of a bar, and the Position events to point to certain locations within a bar. For example, Position (9/16) indicates that we are pointing to the middle of a bar, which is quantized to 16 regions in this implementation (see Section 3 for details). The combination of Position and Bar therefore provides an explicit metrical grid to model music, in a beat-based manner. This new data representation informs models the presence of a beat-bar hierarchical structure in music and empirically leads to better rhythmic regularity in the case of modeling Pop music, as shown in Figure 1b.\n\nWe note that, while the incorporation of the bars is not new in the literature of automatic music composition [6,20,38], it represents a brand new attempt in the context of Transformer-based music modeling. This facilitates extensions of the Transformer, such as the Compressive Transformer [33], Insertion Transformer [37], and Tree Transformer [40], to be studied for this task in the future, since the models have now clearer ideas of segment boundaries in music.\n\nMoreover, we further explore adding other supportive musical tokens capturing higher-level information of music. Specifically, to model the expressive rhythmic freedom in music (e.g., tempo rubato), we add a set of Tempo events to allow for local tempo changes per beat. To have control over the chord progression underlying the music being composed, we introduce the Chord events to make the harmonic structure of music explicit, in the same vein of adding the Position and Bar events for rhythmic structure.\n\nIn achieving all these, the proposed framework for automatic music composition has to integrate audio-and symbolic-domain music information retrieval (MIR) techniques such as downbeat tracking [2,3,16] and chord recognition [17,35], which differentiates it from existing approaches. Moreover, we use automatic music transcription [1,21,25] to prepare the MIDI data, so the model learns from expressive piano performances. We show in Figure  2 an illustration of the overall framework. Table 1  with the commonly-adopted MIDI-like representation, and Figure  3 gives an example of a REMI event sequence.\n\nIn our experiment (see Section 5), we use Transformer-XL [11] to learn to compose Pop piano music using REMI as the underlying data representation. We conduct both objective and subjective evaluation comparing the proposed model against the Music Transformer [23], showing that we are able to improve the state-of-the-art with simple yet major modifications in the data presentation, rather than with sophisticated re-design of the neural network architecture.\n\nFor reproducibility, the code, data and pre-trained model are made publicly available. 3 \n\n\nRELATED WORK\n\nRecent neural network-based approaches for automatic music composition can be broadly categorized into two groups. Image-based approaches such as MidiNet [41] and MuseGAN [13] use an imagelike representation such as the piano roll to represent a score as a matrix of time steps and MIDI pitches, and then use convolutionbased operations to generate music. It is easier for such approaches to learn the rhythmic structure of music, as the time steps corresponding to the beats and bar lines are clear in such matrices. Language-based approaches such as the Music Transformer [23], on the other hand, may learn the temporal dependency between musical events such as Note-Ons better. Yet, due to the limits of the MIDI-like representation outlined in Section 1, existing work may fall short in the rhythmic aspect. The proposed approach combines the advantage of the image-and language-based approaches by embedding a metrical grid in the event representation.\n\nThe idea of designing events for music metadata is similar to CTRL [24], which provided more explicit controls for text generation. However, recent work in neural sequence modeling of music mostly adopt the same MIDI-like event representation proposed by [30], or its extensions. For example, extra events denoting the composer, instrumentation and global tempo are used in MuseNet [32] for conditioned generation, and events specifying the Note-On and Note-Off of different instruments are used to achieve multi-instrument music composition [12]. However, to our best knowledge, the use of Time-Shift to mark the advance in time has been taken for granted thus far. While this approach has its own merits, we endeavor to explore alternative representations in this paper, again in the specific context of Transformer-based modeling.\n\n\nNEW EVENT-BASED REPRESENTATION OF MUSIC\n\nIn this section, we discuss at length how 'REMI' is different from the commonly-adopted 'MIDI-like' representation (cf. Table 1), and how we design the proposed events. As discussed in [30], a score to be converted into events can be either a MIDI score with no expressive dynamics and timing, or a MIDI performance that has been converted from an expressive audio recording by means of, for example, a MIDI keyboard, or automatic music transcription [1]. We consider the latter case in this paper, but the discussion below can also be generalized to the former case.\n\n\nNote-On and Note Velocity\n\nThe collection of 128 Note-On events indicates the onset of MIDI pitches from 0 (C-1) to 127 (G9), and Note Velocity indicates the level of dynamics (which correspond to perceptual loudness) of the note event. 4 Both the MIDI-like and REMI representations have these two types of events.\n\n\nNote-Off versus Note Duration\n\nIn REMI, we use Note Duration events in replacement of the Note-Off events. Specifically, we represent each note in a given score with the following three consecutive tokens: a Note Velocity event, a Note-On event, and a Note Duration event. There are advantages in doing so:\n\n\u2022 In MIDI-like, the duration of a note has to be inferred from the time gap between a Note-On and the corresponding Note-Off, by accumulating the time span of the Time Shift events in between. In REMI, note duration is made explicit, facilitating modeling the rhythm of music. \u2022 In MIDI-like, a Note-On event and the corresponding Note-Off are usually several events apart. 5 As a result, a sequence model may find it difficult to learn that Note-On and Note-Off must appear in pairs, generating dangling Note-On events without the corresponding Note-Off. We would then have to use some heuristics (e.g., maximal note duration) to turn off a note in post-processing. With REMI, we are free of such an issue, because the sequence model can easily learn from the training data that a Note Velocity event has to be followed by a Note-On event, and then right away a Note Duration event.\n\n\nTime-Shift versus Position & Bar\n\nEmpirically, we find that it is not easy for a model to generate music with steady beats using the Time-Shift events. When listening to the music generated, the intended bar lines drift over time and the rhythm feels unstable. We attribute this to the absence of a metrical structure in the MIDI-like representation. Mistakes in Time-Shift 4 Following [30], in our implementation we quantize note velocity into 32 levels, giving rise to 32 different Note Velocity events. 5 For example, in our implementation, there are on average 21.7\u00b115.3 events between a pair of Note-On and Note-Off, when we adopt the MIDI-like event representation. would lead to accumulative error of timing in the inference phase, which is not obvious in the training phase due to the common use of teacher forcing strategy [14] in training recurrent models.\n\nTo address this issue, we propose to use the combination of Bar and Position events instead. Both of them are readily available from the musical scores, or from the result of automatic music transcription (with the help of a beat and downbeat tracker [3]); they are simply discarded in the MIDI-like representation. While Bar marks the bar lines, Position points to one of the possible discrete locations in a bar. Here is an integer that denotes the time resolution adopted to represent a bar. For example, if we consider a 16-th note time grid as [23,34], = 16. Adding Bar and Position therefore provides a metrical context for models to \"count the beats\" and to compose music bar-after-bar. 67 We note that there are extra benefits in using Position & Bar, including 1) we can more easily learn the dependency (e.g., repetition) of note events occurring at the same Position (\u2605/ ) across bars; 2) we can add bar-level conditions to condition the generation process if we want; 3) we have time reference to coordinate the generation of different tracks for the case of multi-instrument music.\n\n\nTempo\n\nIn an expressive musical performance, the temporal length (in seconds) of each bar may not be the same. To account for such local changes in tempo (i.e., beats per minute; BPM), we add Tempo events every beat (i.e., at Position (1/ ), Position ( 4 + 1 / ), etc). In this way, we have a flexible time grid for expressive rhythm. 8 Unlike the Position events, information regrading the Tempo events may not be always available in a MIDI score. But, for the case of MIDI performances, one can derive such Tempo events with the use of an audio-domain tempo estimation function [3].\n\n\nChord\n\nAs another set of supportive musical tokens, we propose to encode the chord information into input events. Specifically, chords are defined as any harmonic set of pitches consisting of multiple notes sounding together or one after another. A chord consists of a root note and a chord quality [29]. Here, we consider 12 chord roots (C,C#,D,D#,E,F,F#,G,G#,A,A#,B) and five chord qualities (major, minor, diminished, augmented, dominant), yielding 60 possible Chord events, covering the triad and seventh chords. However, the set of Chord events can be further expanded as long as there is a way to get these chords from a MIDI score or a MIDI performance in the data preparation process. We note that the Chord events are just \"symbols\"-the notes are still generated with the Note-on events after them.\n\nFollowing the time grid of REMI, each Tempo or Chord event is preceded by a Position event. 6 We find in our implementation that models learn the meaning of Bar and Position quickly-right after a few epochs the model knows that, e.g., Position (9/ ) cannot go before Position (3/ ), unless there is a Bar in between. No postprocessing is needed to correct errors. See Figure 4 for the learning curve seen in our implementation. 7 We note that the idea of using Bar and Position has been independently proposed before [19], though in the context of RNN-based not Transformer-based models. 8 As exemplified in Figure 3, we use a combination of Tempo Class events (low, mid, high) and Tempo Value events to represent local tempo values of 30-209 BPM.\n\nWe note that music composed by a model using the MIDI-like representation also exhibits the use of chords, as such note combinations can be found in the training data by the sequence model itself. However, by explicitly generating Tempo and Chord events, tempo and chord become controllable.\n\n\nPROPOSED FRAMEWORK\n\nAgain, a diagram of the proposed beat-based music modeling and generation framework can be found in Figure 2. We provide details of some of the major components below.\n\n\nBackbone Sequence Model\n\nThe Transformer [39] is a neural sequence model that uses selfattention to bias its prediction of the current token based on a subset of the past tokens. This design has been shown effective for modeling the structure of music. For example, with the help of a relative-positional encoding method [36], Music Transformer is claimed in [23] to be able to compose expressive classical piano music with a coherent structure of up to one minute.\n\nTransformer-XL [11] extends Transformer by introducing the notion of recurrence and revising the positional encoding scheme. The recurrence mechanism enables the model to leverage the information of past tokens beyond the current training segment, thereby looking further into the history. Theoretically, Transformer-XL can encode arbitrarily long context into a fixed-length representation. Therefore, we adopt Transformer-XL as our backbone model architecture. 9 In the training process, each input sequence is divided into \"segments\" of a specific length (set to 512 events in our implementation). Given such segments, the overall computational procedure for an -layer Transformer-XL with heads can be summarized as:\nh \u22121 = [stop_gradient(h \u22121 \u22121 ) \u2022 h \u22121 ] ,(1)q , k , v = h \u22121 W \u22a4 ,h \u22121 W \u22a4 ,h \u22121 W \u22a4 ,(2)a , = masked_softmax(q \u22a4 , k , + R)v , ,(3)a = [a ,1 \u2022 a ,2 \u2022 ... \u2022 a , ] \u22a4 W ,(4)o = layernorm(a + h \u22121 ) ,(5)h = max(0, o W 1 + b 1 )W 2 + b 2 ,(6)\nwhere h denotes the -th layer hidden features for the -th segment, R denotes the relative positional encodings designed for the Transformer-XL [11], a , indicates the attention features from the -th head, and q, k, v denote the query, key and values in the computation of self-attention [39], respectively. The main difference between Transformer and Transformer-XL lies in the usage of the features from the segments prior to the current segment (i.e., the segments are from the same input MIDI sequence of a music piece) when updating the model parameters based on that current segment. This mechanism brings in longer temporal context and speeds up both the training and inference processes. 9 We have implemented both Transformer-and Transformer-XL based models and found the latter composes Pop piano music with better temporal coherence perceptually. However, as the use of Transformer-XL for automatic music composition has been reported elsewhere [12], we omit such an empirical performance comparison between the XL and non-XL versions.  Transformer-like sequence models learn the dependency among elements (i.e., events here) of a sequence. Therefore, although we have quite a diverse set of events featuring different characteristics in the proposed REMI representation, we opt for the simple approach of using a single Transformer-XL to model all these events at once. In other words, we do not consider the alternative, possibly more computationally intensive, approach of establishing multiple Transformer models, one for a non-overlapping subset of the events, followed by some mechanisms to communicate between them.\n\n\nBeat Tracking and Downbeat Tracking\n\nTo create the Bar events, we employ the recurrent neural network model proposed by [3] to estimate from the audio files the position of the 'downbeats,' which correspond to the first beat in each bar. The same model is used to track the beat positions to create the Tempo events [2]. We obtain the tick positions between beats by linear interpolation, and then align the note onsets and offsets to the nearest tick. To eliminate the imprecision of transcription result, we further quantize the onset and offset times according to the assumed time grid (e.g., to the 16-th note when = 16).\n\n\nChord Recognition\n\nWe establish the 60 Chord events described in Section 3.5 by designing a heuristic rule-based symbolic-domain chord recognition algorithm to the transcribed MIDI files. First, we compute binaryvalued \"chroma features\" [17] for each tick, to represent the activity of 12 different pitch classes ignoring the pitch's octave. Then, we use a sliding window to assign \"likelihood scores\" to every active note for each 2-beat and 4-beat segment. After summarizing the chroma features of the current segment, we consider every note in a segment as a candidate root note of the Chord of that segment, and calculate its pitch intervals to all the other notes in that segment. The look-up table shown in Table 2 is employed to assign likelihood scores to a pair of root note and chord quality based on the pitch intervals. Each chord quality has its required set of pitch intervals, and scoring functions. Finally, we recursively label the segments by the Chord symbol with the highest likelihood score.\n\n\nEVALUATION\n\nWe report both objective and subjective evaluations aiming to validate the effectiveness of the proposed model over the Music Transformer [23] for the case of composing expressive Pop piano music.\n\nIn what follows, we present the training data we used to train both \n\n\nDataset\n\nWe intend to evaluate the effectiveness of the proposed approach for Pop piano composition, as Pop is a musical genre that features salient rhythmic structures. In doing so, we collect audio files of Pop piano music from the Internet. A total number of 775 pieces of piano music played by different people is collected, amounting to approximately 48 hours' worth of data. They are covers 10 of various Japanese anime, Korean popular and Western popular songs, playing only with the piano. We then apply \"Onsets and Frames\" [21], the state-of-the-art approach for automatic piano transcription, to estimate the pitch, onset time, offset time and velocity of the musical notes of each song, converting the audio recordings into MIDI performances. We select the Pop piano covers such that they are all in 4/4 time signature, simplifying the modeling task. Accordingly, a bar is composed of four beats. Furthermore, following [23], we consider the 16-th note time grid and quantize each bar into = 16 intervals.\n\nIn our preliminary experiments, we have attempted to use a finer time grid (e.g., 32-th or 64-th note) to reduce quantization errors and to improve the expression of music (e.g., to include triplets, swing, mirco-timing variations, etc). However, we found that this seems to go beyond the capability of the adopted Transformer-XL architecture. The generated compositions using a finer time grid tends to be fragmented and not pleasant to listen to. This is an issue that has to be addressed in the future. We stick with = 16 hereafter.\n\nDue to copyright restrictions, we plan to make the training data publicly available not as audio files but as the transcribed MIDI files and the converted REMI event sequences. 10 A 'cover' is a new recording by someone other than the original artist or composer of a commercially released song.  Table 3: Quantitative comparison of different models for Pop piano composition, evaluating how the composed music exhibit rhythmic structures. We report the average result across songs. For all the three objective metrics, the values are the closer to those of the 'Training data' shown in the last row the better-the training data are the transcribed and 16-th note quantized version of the 775 Pop songs. 'Baseline 1' represents a Transformer-XL version of the Music Transformer, adopting a MIDI-like event representation. The other two baselines are its improved variants. We highlight the two values closest to those of the training data in bold.\n\n\nBaseline 1\n\nBaseline 3 REMI Figure 5: Examples of the generated piano rolls of different models. These are 12 bars generated to continue a given prompt of 4-bar human piano performance. We show the result for three different prompts (from left to right). The thicker vertical lines indicate the bar lines; best viewed in color.\n\n\nBaselines & Model Settings\n\nWe consider three variants of the Music Transformer [23] as the baselines in our evaluation. The first one, dubbed Baseline 1, follows fairly faithfully the model settings of [23], except that we use Transformer-XL instead of Transformer (for fair comparison with our model). The other two differ from the first one only in the adopted event representation. While Baseline 1 employs exactly the MIDI-like representation, Baseline 2 replaces Note-Off by Note Duration, and Baseline 3 further modifies the time steps taken in Time-Shift from multiples of 10ms to multiples of the 16-th note. In this way, Baseline 3 has a time grid similar to that of REMI, making Baseline 3 a strong baseline. For either the baselines or the proposed model adopting the REMI representation, we train a single Transformer-XL with = 12 selfattention layers and = 8 attention heads. The length of the training input events (i.e., the segment length) and the recurrence length (i.e., the length of the segment \"cached\" [11]) are both set to 512. The total number of learnable parameters is approximately 41M. Training a model with an NVIDIA V100 with mini-batch size of 16 till the training loss (i.e., cross-entropy) reaches a certain level takes about 9 hours. We find that Baseline 1 needs a smaller learning rate and hence longer training time. For processing the MIDI files, we use the miditoolkit. 11 Figure 4 shows the training loss of different event types as the training unfolds. Figure 4a shows that Baseline 1 struggles the most for learning Time-Shift. Moreover, Note-Off has higher loss than Note-On, suggesting that they are not recognized as event pairs. In contrast, Figure 4b shows that the Position events in REMI are easy to learn, facilitating learning the rhythm of music. 12 \n\n\nObjective Evaluation\n\nThe objective evaluation assesses the rhythmic structure of the generated compositions. Specifically, we employ each model to randomly generate 1,000 sequences, each with 4,096 events, using the temperature-controlled stochastic sampling method with topk [24]. We convert the generated sequences into MIDI and then render them into audio via a piano synthesizer. 13 Then, we apply the joint beat and downbeat tracking model of [3] to the resulting audio clips. 14 The model [3] has two components. The first one estimates the probability, or salience, of observing beats and downbeats for each time frame via a recurrent neural network (RNN). The second one applies a dynamic Bayesian network (DBN) to the output of the RNN to make binary decisions of the occurrence of beats and downbeats. We use the output of the RNN to calculate the downbeat salience, and the output of the DBN for the beat STD and downbeat STD. Specifically, given a piece of audio signal, the model [3] returns a time-ordered series of ( B , B ), where B denotes the estimate time (in seconds) for the -th beat by the DBN, and B \u2208 [0, 1] the salience associated with that beat estimated by the RNN. It also returns similarly a series of ( D , D ) for the downbeats; see Figure 1 for examples of such series. From the tracking results, we calculate the following values for each audio clip:\n\n\u2022 Beat STD: the standard deviation of ( B \u2212 B \u22121 ), which is always positive, over the beat estimates for that clip.\n\n\u2022 Downbeat STD: similarly the standard deviation of ( D \u2212 D \u22121 ) over the downbeat estimates for that clip. Both beat STD and downbeat STD assess the consistency of the rhythm.\n\n\u2022 Downbeat salience: the mean of D over the downbeat estimates for that clip, indicating the salience of the rhythm.\n\nWe report the average values across the audio clips, assuming that these values are closer to the values calculated from the training data (which have been quantized to the 16-th note grid) the better. 15 Tables 3 shows the result of the baselines and a few variants (ablated versions) of the REMI-based model. From Beat STD and Downbeat STD, we see that the result of Baseline 1, which resembles Music Transformer [23], features fairly inconsistent rhythm, echoing the example shown in Figure 1a. The STD is lower when Note Duration is used in place of Note-Off, highlighting the effect of Note Duration in stabilizing the rhythm. Interestingly, we see that the REMI models without the Tempo events have much lower STD than the training data, suggesting that Tempo is important for expressive rhythmic freedom.\n\nFrom downbeat salience, the REMI models outnumber all the baselines, suggesting the effectiveness of Position & Bar. Moreover, the gap between the result of Baseline 1 and Baseline 2 further supports the use of the Note Duration events.\n\n\nSubjective Evaluation\n\nThe best way to evaluate a music composition model remains today to be via a listening test. To have a common ground to compare different models, we ask the models to continue the same given prompt. For doing so, we prepare an additional set of 100 Pop piano performances (that have no overlaps with the training set), process them according to the procedures described in Section 5.1, and take the first four bars from each of them as the prompts. The following three models are asked to generate 16 bars continuing each prompt, 13 https://github.com/YatingMusic/ReaRender 14 We choose to use the model of [3] for it achieved state-of-the-art performance for beat and downbeat tracking for a variety of musical genres, especially for Pop music. 15 Small beat/downbeat STD implies that the music is too rigid, whereas large STD implies that the rhythm is too random. We want the rhythm to be stable yet flexible.   and got evaluated in an online listening test: 'Baseline 1, ' 'Baseline 3' and 'REMI without Chord. ' We do not use Chord here to make the rhythmic aspect the major point of difference among the models.\n\nWe distribute the listening test over our social circles globally and solicit the response from 76 participants. 51 of them understand basic music theory and have the experience of being an amateur musician, so we consider them as professionals (denoted as 'pros'). A participant has to listen to two randomly picked sets of samples and evaluate, for each set, which sample they like the most and the least, in any evaluation criteria of their choice. Each set contains the result of the three models (in random order) for a given prompt. Table 4 shows the aggregated scores, and Table 5 the result of broken-down pairwise comparisons, along with the -value of the Wilcoxon signed-rank test. We see that REMI is preferred by both pros and non-pros, and that the difference between REMI and Baseline 1 is significant ( < 0.01). Figure 5 provides examples of the generated continuations. From the optional verbal feedbacks of the participants, the music generated by REMI are perceptually more pleasing and are more in line with the prompt. Audio examples can be found at our demo website. 16 \n\n\nControllable Chord and Tempo\n\nFinally, we demonstrate the controllability of Tempo and Chord of our model. To achieve this, we can simply force the model not to generate specific events by masking out the corresponding probabilities of the model output. Figure 6 shows the piano rolls, (optionally) the Tempo and Chord events generated by our model under different conditions. Figure 6b shows that the model selects chords that are harmonically close to F:minor when we prohibit it from generating F:minor. Figure 6c shows controlling the Tempo Class affects not only the Tempo Value but also the note events. 16  (c) The result when we enforce using Tempo Class (high) Figure 6: An example illustrating how we can condition the Tempo and Chord events (the first four bars is the prompt).\n\n\nCONCLUSION\n\nIn this paper, we have presented REMI, a novel MIDI-derived event representation for sequence model-based composition of expressive piano music. Using REMI as the underlying data representation, we have also built a Pop Music Transformer that outperforms that stateof-the-art Music Transformer for composing expressive Pop piano music, as validated in objective and subjective studies. We take this as a proof-of-concept that it is beneficial to embed prior human knowledge of music through including components of music information retrieval (MIR) techniques, such as downbeat estimation and chord recognition, to the overall generative framework. For future work, we plan to incorporate other high-level information of music, such as grooving [15] and musical emotion [42], to learn to compose additional tracks such as the guitar, bass, and drums, to further improve the resolution of the time grid, to condition the generative process with other multimedia input such as the lyrics [43] or a video clip [44], and also to study other model architectures that can model longer sequences (e.g., [33]) and thereby learn better the long-term structure in music.\n\nFigure 1 :\n1a) Transformer-XL \u00d7 MIDI-like ('Baseline 1') (b) Transformer-XL \u00d7 REMI (with Tempo and Chord) Examples of piano rolls and 'downbeat probability curves' (cf. Section 5.\n\nFigure 3 :\n3Tempo Value (12), Position (9/16), Note Velocity (14), Note On (67), Note Duration (8), Bar An example of a REMI event sequence and the corresponding music fragment in staff notation. The Bar, Position and Tempo-related events entail the use of audiodomain downbeat and beat tracking algorithms [3] (see Section 3.3), and the Chord events the use of a symbolicdomain chord recognition algorithm (see Section 3.5).\n\nFigure 4 :\n4Average cross-entropy loss for different types of events as the training process proceeds. (a) An adapted version of the Music Transformer [23]; (b) the proposed model. our model and variants of the the Music Transformer, then some implementation details, and finally the performance study.\n\n\n, Seattle, WA, USA. \u00a9 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00 https://doi.org/10.1145/3394171.3413671MIDI-like [30] REMI (this paper) \n\nNote onset \nNote-On \n(0-127) \n\nNote-On \n(0-127) \n\nNote offset \nNote-Off \n(0-127) \n\nNote Duration \n(32th note multiples; 1-64) \n\nTime grid \nTime-Shift \n(10-1000ms) \n\nPosition (16 bins; 1-16) \n& Bar (1) \n\nTempo changes \u2717 \nTempo \n(30-209 BPM) \n\nChord \n\u2717 \nChord \n(60 types) \n\n\n\nTable 1 :\n1The commonly-used MIDI-like event representation\n\nTable 2 :\n2The proposed rule-based scoring criteria for establishing the Chord events via pitch intervals in the chromatic scale.\n\nTable 4 :\n4The average scores for the subjective preference test on a three-point scale from 1 (like the least) to 3 (like the most).Pairs \nWins Losses p-value \n\nREMI \nBaseline 1 103 \n49 \n5.623e-5 \nREMI \nBaseline 3 92 \n60 \n0.0187 \nBaseline 3 Baseline 1 90 \n62 \n0.0440 \n\n\n\nTable 5 :\n5The result of pairwise comparison from the user study, with the p-value of the Wilcoxon signed-rank test.\n\n\nhttps://drive.google.com/drive/folders/1LzPBjHPip4S0CBOLquk5CNapvXSfys54 Oral Session E3: Music, Speech and Audio Processing in Multimedia & Social Media MM '20, October 12-16, 2020, Seattle, WA, USA The result when we enforce no F:minor chord after the 4th bar110 \n\n122 \n\nF \nmin \nF \nmin \nF \nmin/A# \nF \nmin \nF \nmin \nF \nmin \nF \nmin \nC \nmin/G \nF \nmin \nF \nmin \nF \nmin \nD# \nmaj/G \nF \nmin \nF \nmin \nF \nmin/D# \nF \nmin/C# \nC# \nmaj \nC# \nmaj \n\nC3 \n\nC5 \n\nC7 \n\n(a) A human-made musical piece containing the prompt used be-\nlow \n\nF \nmin \nF \nmin \nF \nmin \nF \nmin \nA# \nmin \nG# \nmaj \nA# \nmin \nA# \nmin \nC# \nmaj \nG# \nmaj \nG# \nmaj \nG# \nmaj \nA# \nmin \nA# \nmin \nA# \nmin \nG# \nmaj \nC \naug \nC# \nmaj \nG \ndom \nE \nmin \nD# \naug \nA# \nmin \nG# \nmaj \n\nC3 \n\nC5 \n\nC7 \n\n(b) 110 \n\n178 \n\nC3 \n\nC5 \n\nC7 \n\n\nhttps://github.com/YatingMusic/remi\nhttps://github.com/YatingMusic/miditoolkit 12 According to[21,25], piano transcription models do better in estimating note onsets and pitches, than offsets and velocities. This may be why our model has higher loss for Note Duration and Note Velocity.\nACKNOWLEDGEMENTThe authors would like to thank Wen-Yi Hsiao (Taiwan AI Labs) for helping with processing the MIDI data and with rendering the MIDIs to audios for subjective study.\nAutomatic music transcription: An overview. Emmanouil Benetos, Simon Dixon, Zhiyao Duan, Sebastian Ewert, IEEE Signal Processing Magazine. 36Emmanouil Benetos, Simon Dixon, Zhiyao Duan, and Sebastian Ewert. 2018. Automatic music transcription: An overview. IEEE Signal Processing Magazine 36, 1 (2018), 20-30.\n\nMadmom: A new python audio and music signal processing library. Sebastian B\u00f6ck, Filip Korzeniowski, Jan Schl\u00fcter, Florian Krebs, Gerhard Widmer, Proc. ACM Multimedia. ACM MultimediaSebastian B\u00f6ck, Filip Korzeniowski, Jan Schl\u00fcter, Florian Krebs, and Gerhard Widmer. 2016. Madmom: A new python audio and music signal processing library. In Proc. ACM Multimedia. 1174-1178.\n\nJoint beat and downbeat tracking with recurrent neural networks. Sebastian B\u00f6ck, Florian Krebs, Gerhard Widmer, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval ConfSebastian B\u00f6ck, Florian Krebs, and Gerhard Widmer. 2016. Joint beat and down- beat tracking with recurrent neural networks. In Proc. Int. Soc. Music Information Retrieval Conf. 255-261.\n\nFrom low-level to high-level: Comparative study of music similarity measures. Dmitry Bogdanov, J Serr\u00e0, Nicolas Wack, Perfecto Herrera, Proc. IEEE International Symposium on Multimedia. IEEE International Symposium on MultimediaDmitry Bogdanov, J. Serr\u00e0, Nicolas Wack, and Perfecto Herrera. 2009. From low-level to high-level: Comparative study of music similarity measures. In Proc. IEEE International Symposium on Multimedia.\n\nInteractive composition, performance and music generation through iterative structures. Paolo Bottoni, Anna Labella, Stefano Faralli, Mario Pierro, Claudio Scozzafava, Proc. ACM Multimedia. ACM MultimediaPaolo Bottoni, Anna Labella, Stefano Faralli, Mario Pierro, and Claudio Scoz- zafava. 2006. Interactive composition, performance and music generation through iterative structures. In Proc. ACM Multimedia. 189-192.\n\nDeep Learning Techniques for Music Generation, Computational Synthesis and Creative Systems. Jean-Pierre Briot, Ga\u00ebtan Hadjeres, Fran\u00e7ois Pachet, SpringerJean-Pierre Briot, Ga\u00ebtan Hadjeres, and Fran\u00e7ois Pachet. 2019. Deep Learning Techniques for Music Generation, Computational Synthesis and Creative Systems. Springer.\n\nMusicalitynovelty generative adversarial nets for algorithmic composition. Gong Chen, Yan Liu, Proc. ACM Multimedia. ACM MultimediaSheng-hua Zhong, and Xiang ZhangGong Chen, Yan Liu, Sheng-hua Zhong, and Xiang Zhang. 2018. Musicality- novelty generative adversarial nets for algorithmic composition. In Proc. ACM Multimedia. 1607-1615.\n\nGenerating long sequences with sparse Transformers. Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, arXiv:1904.10509arXiv preprintRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse Transformers. arXiv preprint arXiv:1904.10509 (2019).\n\nEncoding musical style with transformer autoencoders. Kristy Choi, Curtis Hawthorne, Ian Simon, Monica Dinculescu, Jesse Engel, Proc. Int. Conf. Machine Learning. Int. Conf. Machine LearningKristy Choi, Curtis Hawthorne, Ian Simon, Monica Dinculescu, and Jesse Engel. 2020. Encoding musical style with transformer autoencoders. In Proc. Int. Conf. Machine Learning.\n\nThe Rhythmic Structure of Music. W Grosvenor, Grosvenor Cooper, Leonard B Cooper, Meyer, University of Chicago PressGrosvenor W Cooper, Grosvenor Cooper, and Leonard B Meyer. 1963. The Rhythmic Structure of Music. University of Chicago Press.\n\nTransformer-XL: Attentive language models beyond a fixed-Length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, Ruslan Salakhutdinov, Proc. Annual Meeting of the Association for Computational Linguistics. Annual Meeting of the Association for Computational LinguisticsZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed- Length context. In Proc. Annual Meeting of the Association for Computational Linguistics. 2978-2988.\n\nLakhNES: Improving multi-instrumental music generation with cross-domain pre-training. Chris Donahue, Henry Huanru, Yiting Ethan Mao, Li, W Garrison, Julian Cottrell, Mcauley, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval ConfChris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garrison W. Cottrell, and Julian McAuley. 2019. LakhNES: Improving multi-instrumental music generation with cross-domain pre-training. In Proc. Int. Soc. Music Information Retrieval Conf. 685-692.\n\nMuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. Hao-Wen, Wen-Yi Dong, Li-Chia Hsiao, Yi-Hsuan Yang, Yang, Proc. AAAI. AAAIHao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. 2018. MuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. In Proc. AAAI. 34-41.\n\nBifurcations in the learning of recurrent neural networks 3. learning (RTRL). Kenji Doya, 317Kenji Doya. 1992. Bifurcations in the learning of recurrent neural networks 3. learning (RTRL) 3 (1992), 17.\n\nMusic on the timing grid: The influence of microtiming on the perceived groove quality of a simple drum pattern performance. Jan Fr\u00fchauf, Reinhard Kopiez, Friedrich Platz, Musicae Scientiae. 17Jan Fr\u00fchauf, Reinhard Kopiez, and Friedrich Platz. 2013. Music on the timing grid: The influence of microtiming on the perceived groove quality of a simple drum pattern performance. Musicae Scientiae 17, 2 (2013), 246-260.\n\nAnalysis of common design choices in deep learning systems for downbeat tracking. Magdalena Fuentes, Brian Mcfee, C H\u00e9l\u00e8ne Crayencour, Slim Essid, Pablo Juan Bello, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval ConfMagdalena Fuentes, Brian McFee, C. H\u00e9l\u00e8ne Crayencour, Slim Essid, and Pablo Juan Bello. 2018. Analysis of common design choices in deep learning systems for downbeat tracking. In Proc. Int. Soc. Music Information Retrieval Conf. 106-112.\n\nRealtime chord recognition of musical sound: A system using common Lisp. Takuya Fujishima, Proc. International Computer Music Conf. International Computer Music ConfTakuya Fujishima. 1999. Realtime chord recognition of musical sound: A system using common Lisp. In Proc. International Computer Music Conf. 464-467.\n\nAutomatic generation of lyrics parodies. Lorenzo Gatti, G\u00f6zde \u00d6zbal, Oliviero Stock, Carlo Strapparava, Proc. ACM Multimedia. ACM MultimediaLorenzo Gatti, G\u00f6zde \u00d6zbal, Oliviero Stock, and Carlo Strapparava. 2017. Auto- matic generation of lyrics parodies. In Proc. ACM Multimedia. 485-491.\n\nExplicitly conditioned melody generation: A case study with interdependent RNNs. Benjamin Genchel, Ashis Pati, Alexander Lerch, Proc. Computer Simulation of Music Creativity Conf. Computer Simulation of Music Creativity ConfBenjamin Genchel, Ashis Pati, and Alexander Lerch. 2019. Explicitly conditioned melody generation: A case study with interdependent RNNs. In Proc. Computer Simulation of Music Creativity Conf.\n\nDeepBach: A Steerable Model for Bach chorales generation. Ga\u00ebtan Hadjeres, Fran\u00e7ois Pachet, Frank Nielsen, Proc. Int. Conf. Machine Learning. Int. Conf. Machine LearningGa\u00ebtan Hadjeres, Fran\u00e7ois Pachet, and Frank Nielsen. 2017. DeepBach: A Steer- able Model for Bach chorales generation. In Proc. Int. Conf. Machine Learning. 1362-1371.\n\nOnsets and Frames: Dual-objective piano transcription. Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, Douglas Eck, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval ConfCurtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, and Douglas Eck. 2018. Onsets and Frames: Dual-objective piano transcription. In Proc. Int. Soc. Music Information Retrieval Conf. 50-57.\n\nA tutorial on MIDI and wavetable music synthesis. Application Note, CRYSTAL a division of CIRRUS LOGIC. Jim Heckroth, Jim Heckroth. 1998. A tutorial on MIDI and wavetable music synthesis. Applica- tion Note, CRYSTAL a division of CIRRUS LOGIC (1998).\n\nMusic Transformer: Generating music with long-term structure. A Cheng-Zhi, Huang, Proc. Int. Conf. Learning Representations. Int. Conf. Learning RepresentationsCheng-Zhi A. Huang et al. 2019. Music Transformer: Generating music with long-term structure. In Proc. Int. Conf. Learning Representations.\n\nBryan Nitish Shirish Keskar, Mccann, R Lav, Caiming Varshney, Richard Xiong, Socher, arXiv:1909.05858CTRL: A conditional transformer language model for controllable generation. arXiv preprintNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. CTRL: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858 (2019).\n\nAdversarial learning for improved onsets and frames music transcription. Jong Wook Kim, Juan Pablo Bello, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval ConfJong Wook Kim and Juan Pablo Bello. 2019. Adversarial learning for improved onsets and frames music transcription. In Proc. Int. Soc. Music Information Retrieval Conf. 670-677.\n\nComputational intelligence in music composition: A survey. Hung Chien, Chuan-Kang Liu, Ting, IEEE Transactions on Emerging Topics in Computational Intelligence. 1Chien-Hung Liu and Chuan-Kang Ting. 2017. Computational intelligence in mu- sic composition: A survey. IEEE Transactions on Emerging Topics in Computational Intelligence 1, 1 (2017), 2-15.\n\nOral Session E3: Music, Speech and Audio Processing in Multimedia & Social Media MM '20. Seattle, WA, USAOral Session E3: Music, Speech and Audio Processing in Multimedia & Social Media MM '20, October 12-16, 2020, Seattle, WA, USA\n\nPolyphonic music modelling with LSTM-RTRBM. Qi Lyu, Zhiyong Wu, Proc. ACM Multimedia. ACM MultimediaQi Lyu, Zhiyong Wu, and Jun Zhu. 2015. Polyphonic music modelling with LSTM-RTRBM. In Proc. ACM Multimedia. 991-994.\n\nThe Elements of Music: Melody, Rhythm, and Harmony. Jason Martineau, Bloomsbury Publishing USAJason Martineau. 2008. The Elements of Music: Melody, Rhythm, and Harmony. Bloomsbury Publishing USA.\n\nStructured training for large-vocabulary chord recognition. B Mcfee, J P Bello, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval ConfB. McFee and J. P. Bello. 2017. Structured training for large-vocabulary chord recognition. In Proc. Int. Soc. Music Information Retrieval Conf. 188-194.\n\nThis time with feeling: Learning expressive musical performance. Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, Karen Simonyan, Neural Computing and Applications. Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, and Karen Simonyan. 2018. This time with feeling: Learning expressive musical performance. Neural Computing and Applications (2018).\n\nA Joyful Ode to automatic orchestration. Fran\u00e7ois Pachet, ACM Transactions on Intelligent Systems and Technology. 82Fran\u00e7ois Pachet. 2016. A Joyful Ode to automatic orchestration. ACM Transactions on Intelligent Systems and Technology 8, 2 (2016).\n\n. Christine Mcleavy Payne, MuseNet. OpenAI Blog. Christine McLeavy Payne. 2019. MuseNet. OpenAI Blog (2019).\n\nCompressive Transformers for long-range sequence modelling. Jack W Rae, Anna Potapenko, M Siddhant, Chloe Jayakumar, Timothy P Hillier, Lillicrap, Proc. Int. Conf. Learning Representations. Int. Conf. Learning RepresentationsJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Tim- othy P. Lillicrap. 2020. Compressive Transformers for long-range sequence modelling. In Proc. Int. Conf. Learning Representations.\n\nA hierarchical latent vector model for learning long-term structure in music. Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck, Proc. Int. Conf. Machine Learning. Int. Conf. Machine LearningAdam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck. 2018. A hierarchical latent vector model for learning long-term structure in music. In Proc. Int. Conf. Machine Learning. 4361-4370.\n\nCochonut: Recognizing complex chords from MIDI guitar sequences. Ricardo Scholz, Geber Ramalho, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval ConfRicardo Scholz and Geber Ramalho. 2008. Cochonut: Recognizing complex chords from MIDI guitar sequences. In Proc. Int. Soc. Music Information Retrieval Conf. 27-32.\n\nSelf-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, Proc. Conf. the North American Chapter of the Association for Computational Linguistics. Conf. the North American Chapter of the Association for Computational LinguisticsPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proc. Conf. the North American Chapter of the Association for Computational Linguistics. 464-468.\n\nInsertion Transformer: Flexible sequence generation via insertion operations. Mitchell Stern, William Chan, Jamie Kiros, Jakob Uszkoreit, Proc. Int. Conf. Machine Learning. Int. Conf. Machine LearningMitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019. Insertion Transformer: Flexible sequence generation via insertion operations. In Proc. Int. Conf. Machine Learning. 5976-5985.\n\nMusic transcription modelling and composition using deep learning. Bob L Sturm, Jo\u00e3o F Santos, Proc. Conf. Computer Simulation of Musical Creativity. Conf. Computer Simulation of Musical CreativityOded Ben-Tal, and Iryna KorshunovaBob L. Sturm, Jo\u00e3o F. Santos, Oded Ben-Tal, and Iryna Korshunova. 2016. Music transcription modelling and composition using deep learning. In Proc. Conf. Computer Simulation of Musical Creativity.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Proc. Advances in Neural Information Processing Systems. Advances in Neural Information essing SystemsAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. Advances in Neural Information Processing Systems. 5998-6008.\n\nTree Transformer: Integrating tree structures into self-attention. Yaushian Wang, Hung-Yi Lee, Yun-Nung Chen, Proc. Conf. Empirical Methods in Natural Language Processing. Conf. Empirical Methods in Natural Language essingYaushian Wang, Hung-Yi Lee, and Yun-Nung Chen. 2019. Tree Transformer: Integrating tree structures into self-attention. In Proc. Conf. Empirical Methods in Natural Language Processing. 1061-1070.\n\nMidiNet: A convolutional generative adversarial network for symbolic-domain music generation. Li-Chia Yang, Szu-Yu Chou, Yi-Hsuan Yang, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval ConfLi-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. 2017. MidiNet: A convolutional generative adversarial network for symbolic-domain music generation. In Proc. Int. Soc. Music Information Retrieval Conf. 324-331.\n\nMusic Emotion Recognition. Yi-Hsuan Yang, Homer H Chen, CRC PressYi-Hsuan Yang and Homer H. Chen. 2011. Music Emotion Recognition. CRC Press.\n\nYi Yu, Simon Canales, arXiv:1908.05551Conditional LSTM-GAN for melody generation from lyrics. arXiv preprintYi Yu and Simon Canales. 2019. Conditional LSTM-GAN for melody generation from lyrics. arXiv preprint arXiv:1908.05551 (2019).\n\nAutomatic music soundtrack generation for outdoor videos from contextual sensor information. Yi Yu, Zhijie Shen, Roger Zimmermann, Proc. ACM Multimedia. ACM MultimediaYi Yu, Zhijie Shen, and Roger Zimmermann. 2012. Automatic music soundtrack generation for outdoor videos from contextual sensor information. In Proc. ACM Multimedia. 1377-1378.\n\nXiaoIce Band: A melody and arrangement generation framework for Pop music. Hongyuan Zhu, Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining. ACM SIGKDD Int. Conf. Knowledge Discovery & Data MiningHongyuan Zhu et al. 2018. XiaoIce Band: A melody and arrangement generation framework for Pop music. In Proc. ACM SIGKDD Int. Conf. Knowledge Discovery & Data Mining. 2837-2846.\n\nOral Session E3: Music, Speech and Audio Processing in Multimedia & Social Media MM '20. Seattle, WA, USAOral Session E3: Music, Speech and Audio Processing in Multimedia & Social Media MM '20, October 12-16, 2020, Seattle, WA, USA\n", "annotations": {"author": "[{\"end\":156,\"start\":123},{\"end\":188,\"start\":157},{\"end\":204,\"start\":189},{\"end\":219,\"start\":205},{\"end\":317,\"start\":220},{\"end\":341,\"start\":318}]", "publisher": null, "author_last_name": "[{\"end\":137,\"start\":132},{\"end\":170,\"start\":166},{\"end\":203,\"start\":198},{\"end\":218,\"start\":214}]", "author_first_name": "[{\"end\":131,\"start\":123},{\"end\":165,\"start\":157},{\"end\":197,\"start\":189},{\"end\":213,\"start\":205}]", "author_affiliation": "[{\"end\":316,\"start\":221},{\"end\":340,\"start\":319}]", "title": "[{\"end\":95,\"start\":1},{\"end\":436,\"start\":342}]", "venue": "[{\"end\":494,\"start\":438}]", "abstract": "[{\"end\":1822,\"start\":664}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2224,\"start\":2220},{\"end\":2829,\"start\":2805},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2834,\"start\":2830},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2837,\"start\":2834},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3194,\"start\":3191},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3196,\"start\":3194},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3199,\"start\":3196},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3202,\"start\":3199},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3205,\"start\":3202},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3208,\"start\":3205},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3211,\"start\":3208},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3214,\"start\":3211},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3217,\"start\":3214},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3352,\"start\":3349},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3355,\"start\":3352},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3358,\"start\":3355},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3361,\"start\":3358},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3409,\"start\":3408},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3600,\"start\":3596},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3915,\"start\":3911},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3948,\"start\":3947},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4317,\"start\":4314},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4405,\"start\":4404},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4963,\"start\":4959},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5096,\"start\":5092},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5099,\"start\":5096},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5227,\"start\":5223},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5368,\"start\":5365},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5371,\"start\":5368},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5552,\"start\":5548},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5795,\"start\":5791},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6373,\"start\":6370},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6649,\"start\":6645},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7037,\"start\":7033},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8581,\"start\":8578},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8584,\"start\":8581},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8587,\"start\":8584},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8763,\"start\":8759},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8791,\"start\":8787},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8818,\"start\":8814},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9643,\"start\":9640},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9645,\"start\":9643},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9648,\"start\":9645},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9675,\"start\":9671},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9678,\"start\":9675},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9780,\"start\":9777},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9783,\"start\":9780},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9786,\"start\":9783},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10112,\"start\":10108},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10314,\"start\":10310},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10601,\"start\":10600},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10777,\"start\":10773},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10794,\"start\":10790},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11197,\"start\":11193},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11649,\"start\":11645},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11837,\"start\":11833},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11964,\"start\":11960},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12124,\"start\":12120},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12644,\"start\":12640},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12909,\"start\":12906},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13263,\"start\":13262},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14025,\"start\":14024},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14911,\"start\":14910},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14926,\"start\":14922},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15043,\"start\":15042},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15372,\"start\":15368},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15658,\"start\":15655},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15957,\"start\":15953},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15960,\"start\":15957},{\"end\":16100,\"start\":16098},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16837,\"start\":16836},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17084,\"start\":17081},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17391,\"start\":17387},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17990,\"start\":17989},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18326,\"start\":18325},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18418,\"start\":18414},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18486,\"start\":18485},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19175,\"start\":19171},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19455,\"start\":19451},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19493,\"start\":19489},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19616,\"start\":19612},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20061,\"start\":20060},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20704,\"start\":20700},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20848,\"start\":20844},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21253,\"start\":21252},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21516,\"start\":21512},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22315,\"start\":22312},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22511,\"start\":22508},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23061,\"start\":23057},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23989,\"start\":23985},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24652,\"start\":24648},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25051,\"start\":25047},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25850,\"start\":25848},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27035,\"start\":27031},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27158,\"start\":27154},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27980,\"start\":27976},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28754,\"start\":28752},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29039,\"start\":29035},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29145,\"start\":29143},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29210,\"start\":29207},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29243,\"start\":29241},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29257,\"start\":29254},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29755,\"start\":29752},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30762,\"start\":30760},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30977,\"start\":30973},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32165,\"start\":32163},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32209,\"start\":32207},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32243,\"start\":32240},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32381,\"start\":32379},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33842,\"start\":33840},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34458,\"start\":34456},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":35398,\"start\":35394},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":35423,\"start\":35419},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35639,\"start\":35635},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":35660,\"start\":35656},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":35749,\"start\":35745},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38627,\"start\":38623},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38630,\"start\":38627}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35990,\"start\":35810},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36417,\"start\":35991},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36721,\"start\":36418},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37179,\"start\":36722},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37240,\"start\":37180},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":37371,\"start\":37241},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":37643,\"start\":37372},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":37761,\"start\":37644},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":38528,\"start\":37762}]", "paragraph": "[{\"end\":2937,\"start\":1838},{\"end\":5079,\"start\":2939},{\"end\":5265,\"start\":5081},{\"end\":6460,\"start\":5267},{\"end\":7557,\"start\":6462},{\"end\":8466,\"start\":7559},{\"end\":8934,\"start\":8468},{\"end\":9445,\"start\":8936},{\"end\":10049,\"start\":9447},{\"end\":10511,\"start\":10051},{\"end\":10602,\"start\":10513},{\"end\":11576,\"start\":10619},{\"end\":12411,\"start\":11578},{\"end\":13022,\"start\":12455},{\"end\":13339,\"start\":13052},{\"end\":13648,\"start\":13373},{\"end\":14533,\"start\":13650},{\"end\":15402,\"start\":14570},{\"end\":16498,\"start\":15404},{\"end\":17085,\"start\":16508},{\"end\":17895,\"start\":17095},{\"end\":18644,\"start\":17897},{\"end\":18937,\"start\":18646},{\"end\":19127,\"start\":18960},{\"end\":19595,\"start\":19155},{\"end\":20316,\"start\":19597},{\"end\":22189,\"start\":20557},{\"end\":22817,\"start\":22229},{\"end\":23832,\"start\":22839},{\"end\":24043,\"start\":23847},{\"end\":24113,\"start\":24045},{\"end\":25132,\"start\":24125},{\"end\":25669,\"start\":25134},{\"end\":26618,\"start\":25671},{\"end\":26948,\"start\":26633},{\"end\":28755,\"start\":26979},{\"end\":30142,\"start\":28780},{\"end\":30260,\"start\":30144},{\"end\":30438,\"start\":30262},{\"end\":30556,\"start\":30440},{\"end\":31369,\"start\":30558},{\"end\":31607,\"start\":31371},{\"end\":32750,\"start\":31633},{\"end\":33843,\"start\":32752},{\"end\":34634,\"start\":33876},{\"end\":35809,\"start\":34649}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20362,\"start\":20317},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20407,\"start\":20362},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20450,\"start\":20407},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20489,\"start\":20450},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20518,\"start\":20489},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20556,\"start\":20518}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":5286,\"start\":5279},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":9939,\"start\":9932},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12583,\"start\":12575},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":23540,\"start\":23533},{\"end\":25975,\"start\":25968},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":33298,\"start\":33291},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":33339,\"start\":33332}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1836,\"start\":1824},{\"attributes\":{\"n\":\"2\"},\"end\":10617,\"start\":10605},{\"attributes\":{\"n\":\"3\"},\"end\":12453,\"start\":12414},{\"attributes\":{\"n\":\"3.1\"},\"end\":13050,\"start\":13025},{\"attributes\":{\"n\":\"3.2\"},\"end\":13371,\"start\":13342},{\"attributes\":{\"n\":\"3.3\"},\"end\":14568,\"start\":14536},{\"attributes\":{\"n\":\"3.4\"},\"end\":16506,\"start\":16501},{\"attributes\":{\"n\":\"3.5\"},\"end\":17093,\"start\":17088},{\"attributes\":{\"n\":\"4\"},\"end\":18958,\"start\":18940},{\"attributes\":{\"n\":\"4.1\"},\"end\":19153,\"start\":19130},{\"attributes\":{\"n\":\"4.2\"},\"end\":22227,\"start\":22192},{\"attributes\":{\"n\":\"4.3\"},\"end\":22837,\"start\":22820},{\"attributes\":{\"n\":\"5\"},\"end\":23845,\"start\":23835},{\"attributes\":{\"n\":\"5.1\"},\"end\":24123,\"start\":24116},{\"end\":26631,\"start\":26621},{\"attributes\":{\"n\":\"5.2\"},\"end\":26977,\"start\":26951},{\"attributes\":{\"n\":\"5.3\"},\"end\":28778,\"start\":28758},{\"attributes\":{\"n\":\"5.4\"},\"end\":31631,\"start\":31610},{\"attributes\":{\"n\":\"5.5\"},\"end\":33874,\"start\":33846},{\"attributes\":{\"n\":\"6\"},\"end\":34647,\"start\":34637},{\"end\":35821,\"start\":35811},{\"end\":36002,\"start\":35992},{\"end\":36429,\"start\":36419},{\"end\":37190,\"start\":37181},{\"end\":37251,\"start\":37242},{\"end\":37382,\"start\":37373},{\"end\":37654,\"start\":37645}]", "table": "[{\"end\":37179,\"start\":36872},{\"end\":37643,\"start\":37506},{\"end\":38528,\"start\":38025}]", "figure_caption": "[{\"end\":35990,\"start\":35823},{\"end\":36417,\"start\":36004},{\"end\":36721,\"start\":36431},{\"end\":36872,\"start\":36724},{\"end\":37240,\"start\":37192},{\"end\":37371,\"start\":37253},{\"end\":37506,\"start\":37384},{\"end\":37761,\"start\":37656},{\"end\":38025,\"start\":37764}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7119,\"start\":7110},{\"end\":7130,\"start\":7122},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8465,\"start\":8456},{\"end\":9889,\"start\":9880},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10006,\"start\":9997},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18273,\"start\":18265},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18513,\"start\":18505},{\"end\":19068,\"start\":19060},{\"end\":26657,\"start\":26649},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28372,\"start\":28364},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28456,\"start\":28447},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28650,\"start\":28641},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30031,\"start\":30023},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31054,\"start\":31045},{\"end\":33587,\"start\":33579},{\"end\":34108,\"start\":34100},{\"end\":34232,\"start\":34223},{\"end\":34362,\"start\":34353},{\"end\":34524,\"start\":34516}]", "bib_author_first_name": "[{\"end\":39049,\"start\":39040},{\"end\":39064,\"start\":39059},{\"end\":39078,\"start\":39072},{\"end\":39094,\"start\":39085},{\"end\":39380,\"start\":39371},{\"end\":39392,\"start\":39387},{\"end\":39410,\"start\":39407},{\"end\":39428,\"start\":39421},{\"end\":39443,\"start\":39436},{\"end\":39754,\"start\":39745},{\"end\":39768,\"start\":39761},{\"end\":39783,\"start\":39776},{\"end\":40155,\"start\":40149},{\"end\":40167,\"start\":40166},{\"end\":40182,\"start\":40175},{\"end\":40197,\"start\":40189},{\"end\":40593,\"start\":40588},{\"end\":40607,\"start\":40603},{\"end\":40624,\"start\":40617},{\"end\":40639,\"start\":40634},{\"end\":40655,\"start\":40648},{\"end\":41023,\"start\":41012},{\"end\":41037,\"start\":41031},{\"end\":41056,\"start\":41048},{\"end\":41319,\"start\":41315},{\"end\":41329,\"start\":41326},{\"end\":41634,\"start\":41629},{\"end\":41647,\"start\":41642},{\"end\":41658,\"start\":41654},{\"end\":41672,\"start\":41668},{\"end\":41932,\"start\":41926},{\"end\":41945,\"start\":41939},{\"end\":41960,\"start\":41957},{\"end\":41974,\"start\":41968},{\"end\":41992,\"start\":41987},{\"end\":42273,\"start\":42272},{\"end\":42294,\"start\":42285},{\"end\":42312,\"start\":42303},{\"end\":42562,\"start\":42556},{\"end\":42574,\"start\":42568},{\"end\":42587,\"start\":42581},{\"end\":42599,\"start\":42594},{\"end\":42615,\"start\":42611},{\"end\":42626,\"start\":42620},{\"end\":43124,\"start\":43119},{\"end\":43139,\"start\":43134},{\"end\":43154,\"start\":43148},{\"end\":43160,\"start\":43155},{\"end\":43171,\"start\":43170},{\"end\":43188,\"start\":43182},{\"end\":43676,\"start\":43670},{\"end\":43690,\"start\":43683},{\"end\":43706,\"start\":43698},{\"end\":44021,\"start\":44016},{\"end\":44269,\"start\":44266},{\"end\":44287,\"start\":44279},{\"end\":44305,\"start\":44296},{\"end\":44649,\"start\":44640},{\"end\":44664,\"start\":44659},{\"end\":44673,\"start\":44672},{\"end\":44680,\"start\":44674},{\"end\":44697,\"start\":44693},{\"end\":44710,\"start\":44705},{\"end\":44715,\"start\":44711},{\"end\":45133,\"start\":45127},{\"end\":45418,\"start\":45411},{\"end\":45431,\"start\":45426},{\"end\":45447,\"start\":45439},{\"end\":45460,\"start\":45455},{\"end\":45750,\"start\":45742},{\"end\":45765,\"start\":45760},{\"end\":45781,\"start\":45772},{\"end\":46143,\"start\":46137},{\"end\":46162,\"start\":46154},{\"end\":46176,\"start\":46171},{\"end\":46478,\"start\":46472},{\"end\":46495,\"start\":46490},{\"end\":46509,\"start\":46503},{\"end\":46520,\"start\":46516},{\"end\":46533,\"start\":46530},{\"end\":46546,\"start\":46541},{\"end\":46560,\"start\":46555},{\"end\":46574,\"start\":46568},{\"end\":46588,\"start\":46581},{\"end\":47041,\"start\":47038},{\"end\":47249,\"start\":47248},{\"end\":47492,\"start\":47487},{\"end\":47525,\"start\":47524},{\"end\":47538,\"start\":47531},{\"end\":47556,\"start\":47549},{\"end\":47971,\"start\":47962},{\"end\":47981,\"start\":47977},{\"end\":47987,\"start\":47982},{\"end\":48328,\"start\":48324},{\"end\":48346,\"start\":48336},{\"end\":48896,\"start\":48894},{\"end\":48909,\"start\":48902},{\"end\":49125,\"start\":49120},{\"end\":49326,\"start\":49325},{\"end\":49335,\"start\":49334},{\"end\":49337,\"start\":49336},{\"end\":49663,\"start\":49657},{\"end\":49673,\"start\":49670},{\"end\":49687,\"start\":49681},{\"end\":49705,\"start\":49698},{\"end\":49716,\"start\":49711},{\"end\":49999,\"start\":49991},{\"end\":50218,\"start\":50201},{\"end\":50373,\"start\":50369},{\"end\":50375,\"start\":50374},{\"end\":50385,\"start\":50381},{\"end\":50398,\"start\":50397},{\"end\":50414,\"start\":50409},{\"end\":50433,\"start\":50426},{\"end\":50435,\"start\":50434},{\"end\":50824,\"start\":50820},{\"end\":50839,\"start\":50834},{\"end\":50852,\"start\":50847},{\"end\":50867,\"start\":50861},{\"end\":50886,\"start\":50879},{\"end\":51236,\"start\":51229},{\"end\":51250,\"start\":51245},{\"end\":51578,\"start\":51573},{\"end\":51590,\"start\":51585},{\"end\":51608,\"start\":51602},{\"end\":52086,\"start\":52078},{\"end\":52101,\"start\":52094},{\"end\":52113,\"start\":52108},{\"end\":52126,\"start\":52121},{\"end\":52468,\"start\":52465},{\"end\":52470,\"start\":52469},{\"end\":52482,\"start\":52478},{\"end\":52484,\"start\":52483},{\"end\":52860,\"start\":52854},{\"end\":52874,\"start\":52870},{\"end\":52888,\"start\":52884},{\"end\":52902,\"start\":52897},{\"end\":52919,\"start\":52914},{\"end\":52932,\"start\":52927},{\"end\":52934,\"start\":52933},{\"end\":52948,\"start\":52942},{\"end\":52962,\"start\":52957},{\"end\":53382,\"start\":53374},{\"end\":53396,\"start\":53389},{\"end\":53410,\"start\":53402},{\"end\":53827,\"start\":53820},{\"end\":53840,\"start\":53834},{\"end\":53855,\"start\":53847},{\"end\":54198,\"start\":54190},{\"end\":54210,\"start\":54205},{\"end\":54212,\"start\":54211},{\"end\":54308,\"start\":54306},{\"end\":54318,\"start\":54313},{\"end\":54637,\"start\":54635},{\"end\":54648,\"start\":54642},{\"end\":54660,\"start\":54655},{\"end\":54970,\"start\":54962}]", "bib_author_last_name": "[{\"end\":39057,\"start\":39050},{\"end\":39070,\"start\":39065},{\"end\":39083,\"start\":39079},{\"end\":39100,\"start\":39095},{\"end\":39385,\"start\":39381},{\"end\":39405,\"start\":39393},{\"end\":39419,\"start\":39411},{\"end\":39434,\"start\":39429},{\"end\":39450,\"start\":39444},{\"end\":39759,\"start\":39755},{\"end\":39774,\"start\":39769},{\"end\":39790,\"start\":39784},{\"end\":40164,\"start\":40156},{\"end\":40173,\"start\":40168},{\"end\":40187,\"start\":40183},{\"end\":40205,\"start\":40198},{\"end\":40601,\"start\":40594},{\"end\":40615,\"start\":40608},{\"end\":40632,\"start\":40625},{\"end\":40646,\"start\":40640},{\"end\":40666,\"start\":40656},{\"end\":41029,\"start\":41024},{\"end\":41046,\"start\":41038},{\"end\":41063,\"start\":41057},{\"end\":41324,\"start\":41320},{\"end\":41333,\"start\":41330},{\"end\":41640,\"start\":41635},{\"end\":41652,\"start\":41648},{\"end\":41666,\"start\":41659},{\"end\":41682,\"start\":41673},{\"end\":41937,\"start\":41933},{\"end\":41955,\"start\":41946},{\"end\":41966,\"start\":41961},{\"end\":41985,\"start\":41975},{\"end\":41998,\"start\":41993},{\"end\":42283,\"start\":42274},{\"end\":42301,\"start\":42295},{\"end\":42319,\"start\":42313},{\"end\":42326,\"start\":42321},{\"end\":42566,\"start\":42563},{\"end\":42579,\"start\":42575},{\"end\":42592,\"start\":42588},{\"end\":42609,\"start\":42600},{\"end\":42618,\"start\":42616},{\"end\":42640,\"start\":42627},{\"end\":43132,\"start\":43125},{\"end\":43146,\"start\":43140},{\"end\":43164,\"start\":43161},{\"end\":43168,\"start\":43166},{\"end\":43180,\"start\":43172},{\"end\":43197,\"start\":43189},{\"end\":43206,\"start\":43199},{\"end\":43668,\"start\":43661},{\"end\":43681,\"start\":43677},{\"end\":43696,\"start\":43691},{\"end\":43711,\"start\":43707},{\"end\":43717,\"start\":43713},{\"end\":44026,\"start\":44022},{\"end\":44277,\"start\":44270},{\"end\":44294,\"start\":44288},{\"end\":44311,\"start\":44306},{\"end\":44657,\"start\":44650},{\"end\":44670,\"start\":44665},{\"end\":44691,\"start\":44681},{\"end\":44703,\"start\":44698},{\"end\":44721,\"start\":44716},{\"end\":45143,\"start\":45134},{\"end\":45424,\"start\":45419},{\"end\":45437,\"start\":45432},{\"end\":45453,\"start\":45448},{\"end\":45472,\"start\":45461},{\"end\":45758,\"start\":45751},{\"end\":45770,\"start\":45766},{\"end\":45787,\"start\":45782},{\"end\":46152,\"start\":46144},{\"end\":46169,\"start\":46163},{\"end\":46184,\"start\":46177},{\"end\":46488,\"start\":46479},{\"end\":46501,\"start\":46496},{\"end\":46514,\"start\":46510},{\"end\":46528,\"start\":46521},{\"end\":46539,\"start\":46534},{\"end\":46553,\"start\":46547},{\"end\":46566,\"start\":46561},{\"end\":46579,\"start\":46575},{\"end\":46592,\"start\":46589},{\"end\":47050,\"start\":47042},{\"end\":47259,\"start\":47250},{\"end\":47266,\"start\":47261},{\"end\":47514,\"start\":47493},{\"end\":47522,\"start\":47516},{\"end\":47529,\"start\":47526},{\"end\":47547,\"start\":47539},{\"end\":47562,\"start\":47557},{\"end\":47570,\"start\":47564},{\"end\":47975,\"start\":47972},{\"end\":47993,\"start\":47988},{\"end\":48334,\"start\":48329},{\"end\":48350,\"start\":48347},{\"end\":48356,\"start\":48352},{\"end\":48900,\"start\":48897},{\"end\":48912,\"start\":48910},{\"end\":49135,\"start\":49126},{\"end\":49332,\"start\":49327},{\"end\":49343,\"start\":49338},{\"end\":49668,\"start\":49664},{\"end\":49679,\"start\":49674},{\"end\":49696,\"start\":49688},{\"end\":49709,\"start\":49706},{\"end\":49725,\"start\":49717},{\"end\":50006,\"start\":50000},{\"end\":50224,\"start\":50219},{\"end\":50379,\"start\":50376},{\"end\":50395,\"start\":50386},{\"end\":50407,\"start\":50399},{\"end\":50424,\"start\":50415},{\"end\":50443,\"start\":50436},{\"end\":50454,\"start\":50445},{\"end\":50832,\"start\":50825},{\"end\":50845,\"start\":50840},{\"end\":50859,\"start\":50853},{\"end\":50877,\"start\":50868},{\"end\":50890,\"start\":50887},{\"end\":51243,\"start\":51237},{\"end\":51258,\"start\":51251},{\"end\":51583,\"start\":51579},{\"end\":51600,\"start\":51591},{\"end\":51616,\"start\":51609},{\"end\":52092,\"start\":52087},{\"end\":52106,\"start\":52102},{\"end\":52119,\"start\":52114},{\"end\":52136,\"start\":52127},{\"end\":52476,\"start\":52471},{\"end\":52491,\"start\":52485},{\"end\":52868,\"start\":52861},{\"end\":52882,\"start\":52875},{\"end\":52895,\"start\":52889},{\"end\":52912,\"start\":52903},{\"end\":52925,\"start\":52920},{\"end\":52940,\"start\":52935},{\"end\":52955,\"start\":52949},{\"end\":52973,\"start\":52963},{\"end\":53387,\"start\":53383},{\"end\":53400,\"start\":53397},{\"end\":53415,\"start\":53411},{\"end\":53832,\"start\":53828},{\"end\":53845,\"start\":53841},{\"end\":53860,\"start\":53856},{\"end\":54203,\"start\":54199},{\"end\":54217,\"start\":54213},{\"end\":54311,\"start\":54309},{\"end\":54326,\"start\":54319},{\"end\":54640,\"start\":54638},{\"end\":54653,\"start\":54649},{\"end\":54671,\"start\":54661},{\"end\":54974,\"start\":54971}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":57191022},\"end\":39305,\"start\":38996},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":259414},\"end\":39678,\"start\":39307},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15831944},\"end\":40069,\"start\":39680},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18102512},\"end\":40498,\"start\":40071},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16470538},\"end\":40917,\"start\":40500},{\"attributes\":{\"id\":\"b5\"},\"end\":41238,\"start\":40919},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":53038230},\"end\":41575,\"start\":41240},{\"attributes\":{\"doi\":\"arXiv:1904.10509\",\"id\":\"b7\"},\"end\":41870,\"start\":41577},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":209324251},\"end\":42237,\"start\":41872},{\"attributes\":{\"id\":\"b9\"},\"end\":42481,\"start\":42239},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":57759363},\"end\":43030,\"start\":42483},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":195886341},\"end\":43546,\"start\":43032},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":19098155},\"end\":43936,\"start\":43548},{\"attributes\":{\"id\":\"b13\"},\"end\":44139,\"start\":43938},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":145464636},\"end\":44556,\"start\":44141},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53871841},\"end\":45052,\"start\":44558},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":38716842},\"end\":45368,\"start\":45054},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8077079},\"end\":45659,\"start\":45370},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":195886444},\"end\":46077,\"start\":45661},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1215640},\"end\":46415,\"start\":46079},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":37446424},\"end\":46932,\"start\":46417},{\"attributes\":{\"id\":\"b21\"},\"end\":47184,\"start\":46934},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":86421707},\"end\":47485,\"start\":47186},{\"attributes\":{\"doi\":\"arXiv:1909.05858\",\"id\":\"b23\"},\"end\":47887,\"start\":47487},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":195218660},\"end\":48263,\"start\":47889},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14367030},\"end\":48615,\"start\":48265},{\"attributes\":{\"id\":\"b26\"},\"end\":48848,\"start\":48617},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6522832},\"end\":49066,\"start\":48850},{\"attributes\":{\"id\":\"b28\"},\"end\":49263,\"start\":49068},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3072806},\"end\":49590,\"start\":49265},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":51980304},\"end\":49948,\"start\":49592},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6764025},\"end\":50197,\"start\":49950},{\"attributes\":{\"id\":\"b32\"},\"end\":50307,\"start\":50199},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":207930593},\"end\":50740,\"start\":50309},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3891811},\"end\":51162,\"start\":50742},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":12943711},\"end\":51516,\"start\":51164},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3725815},\"end\":51998,\"start\":51518},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":60440448},\"end\":52396,\"start\":52000},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":18876242},\"end\":52825,\"start\":52398},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":13756489},\"end\":53305,\"start\":52827},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":202577372},\"end\":53724,\"start\":53307},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2002865},\"end\":54161,\"start\":53726},{\"attributes\":{\"id\":\"b42\"},\"end\":54304,\"start\":54163},{\"attributes\":{\"doi\":\"arXiv:1908.05551\",\"id\":\"b43\"},\"end\":54540,\"start\":54306},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2677478},\"end\":54885,\"start\":54542},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":50776363},\"end\":55271,\"start\":54887},{\"attributes\":{\"id\":\"b46\"},\"end\":55504,\"start\":55273}]", "bib_title": "[{\"end\":39038,\"start\":38996},{\"end\":39369,\"start\":39307},{\"end\":39743,\"start\":39680},{\"end\":40147,\"start\":40071},{\"end\":40586,\"start\":40500},{\"end\":41313,\"start\":41240},{\"end\":41924,\"start\":41872},{\"end\":42554,\"start\":42483},{\"end\":43117,\"start\":43032},{\"end\":43659,\"start\":43548},{\"end\":44264,\"start\":44141},{\"end\":44638,\"start\":44558},{\"end\":45125,\"start\":45054},{\"end\":45409,\"start\":45370},{\"end\":45740,\"start\":45661},{\"end\":46135,\"start\":46079},{\"end\":46470,\"start\":46417},{\"end\":47246,\"start\":47186},{\"end\":47960,\"start\":47889},{\"end\":48322,\"start\":48265},{\"end\":48892,\"start\":48850},{\"end\":49323,\"start\":49265},{\"end\":49655,\"start\":49592},{\"end\":49989,\"start\":49950},{\"end\":50367,\"start\":50309},{\"end\":50818,\"start\":50742},{\"end\":51227,\"start\":51164},{\"end\":51571,\"start\":51518},{\"end\":52076,\"start\":52000},{\"end\":52463,\"start\":52398},{\"end\":52852,\"start\":52827},{\"end\":53372,\"start\":53307},{\"end\":53818,\"start\":53726},{\"end\":54633,\"start\":54542},{\"end\":54960,\"start\":54887}]", "bib_author": "[{\"end\":39059,\"start\":39040},{\"end\":39072,\"start\":39059},{\"end\":39085,\"start\":39072},{\"end\":39102,\"start\":39085},{\"end\":39387,\"start\":39371},{\"end\":39407,\"start\":39387},{\"end\":39421,\"start\":39407},{\"end\":39436,\"start\":39421},{\"end\":39452,\"start\":39436},{\"end\":39761,\"start\":39745},{\"end\":39776,\"start\":39761},{\"end\":39792,\"start\":39776},{\"end\":40166,\"start\":40149},{\"end\":40175,\"start\":40166},{\"end\":40189,\"start\":40175},{\"end\":40207,\"start\":40189},{\"end\":40603,\"start\":40588},{\"end\":40617,\"start\":40603},{\"end\":40634,\"start\":40617},{\"end\":40648,\"start\":40634},{\"end\":40668,\"start\":40648},{\"end\":41031,\"start\":41012},{\"end\":41048,\"start\":41031},{\"end\":41065,\"start\":41048},{\"end\":41326,\"start\":41315},{\"end\":41335,\"start\":41326},{\"end\":41642,\"start\":41629},{\"end\":41654,\"start\":41642},{\"end\":41668,\"start\":41654},{\"end\":41684,\"start\":41668},{\"end\":41939,\"start\":41926},{\"end\":41957,\"start\":41939},{\"end\":41968,\"start\":41957},{\"end\":41987,\"start\":41968},{\"end\":42000,\"start\":41987},{\"end\":42285,\"start\":42272},{\"end\":42303,\"start\":42285},{\"end\":42321,\"start\":42303},{\"end\":42328,\"start\":42321},{\"end\":42568,\"start\":42556},{\"end\":42581,\"start\":42568},{\"end\":42594,\"start\":42581},{\"end\":42611,\"start\":42594},{\"end\":42620,\"start\":42611},{\"end\":42642,\"start\":42620},{\"end\":43134,\"start\":43119},{\"end\":43148,\"start\":43134},{\"end\":43166,\"start\":43148},{\"end\":43170,\"start\":43166},{\"end\":43182,\"start\":43170},{\"end\":43199,\"start\":43182},{\"end\":43208,\"start\":43199},{\"end\":43670,\"start\":43661},{\"end\":43683,\"start\":43670},{\"end\":43698,\"start\":43683},{\"end\":43713,\"start\":43698},{\"end\":43719,\"start\":43713},{\"end\":44028,\"start\":44016},{\"end\":44279,\"start\":44266},{\"end\":44296,\"start\":44279},{\"end\":44313,\"start\":44296},{\"end\":44659,\"start\":44640},{\"end\":44672,\"start\":44659},{\"end\":44693,\"start\":44672},{\"end\":44705,\"start\":44693},{\"end\":44723,\"start\":44705},{\"end\":45145,\"start\":45127},{\"end\":45426,\"start\":45411},{\"end\":45439,\"start\":45426},{\"end\":45455,\"start\":45439},{\"end\":45474,\"start\":45455},{\"end\":45760,\"start\":45742},{\"end\":45772,\"start\":45760},{\"end\":45789,\"start\":45772},{\"end\":46154,\"start\":46137},{\"end\":46171,\"start\":46154},{\"end\":46186,\"start\":46171},{\"end\":46490,\"start\":46472},{\"end\":46503,\"start\":46490},{\"end\":46516,\"start\":46503},{\"end\":46530,\"start\":46516},{\"end\":46541,\"start\":46530},{\"end\":46555,\"start\":46541},{\"end\":46568,\"start\":46555},{\"end\":46581,\"start\":46568},{\"end\":46594,\"start\":46581},{\"end\":47052,\"start\":47038},{\"end\":47261,\"start\":47248},{\"end\":47268,\"start\":47261},{\"end\":47516,\"start\":47487},{\"end\":47524,\"start\":47516},{\"end\":47531,\"start\":47524},{\"end\":47549,\"start\":47531},{\"end\":47564,\"start\":47549},{\"end\":47572,\"start\":47564},{\"end\":47977,\"start\":47962},{\"end\":47995,\"start\":47977},{\"end\":48336,\"start\":48324},{\"end\":48352,\"start\":48336},{\"end\":48358,\"start\":48352},{\"end\":48902,\"start\":48894},{\"end\":48914,\"start\":48902},{\"end\":49137,\"start\":49120},{\"end\":49334,\"start\":49325},{\"end\":49345,\"start\":49334},{\"end\":49670,\"start\":49657},{\"end\":49681,\"start\":49670},{\"end\":49698,\"start\":49681},{\"end\":49711,\"start\":49698},{\"end\":49727,\"start\":49711},{\"end\":50008,\"start\":49991},{\"end\":50226,\"start\":50201},{\"end\":50381,\"start\":50369},{\"end\":50397,\"start\":50381},{\"end\":50409,\"start\":50397},{\"end\":50426,\"start\":50409},{\"end\":50445,\"start\":50426},{\"end\":50456,\"start\":50445},{\"end\":50834,\"start\":50820},{\"end\":50847,\"start\":50834},{\"end\":50861,\"start\":50847},{\"end\":50879,\"start\":50861},{\"end\":50892,\"start\":50879},{\"end\":51245,\"start\":51229},{\"end\":51260,\"start\":51245},{\"end\":51585,\"start\":51573},{\"end\":51602,\"start\":51585},{\"end\":51618,\"start\":51602},{\"end\":52094,\"start\":52078},{\"end\":52108,\"start\":52094},{\"end\":52121,\"start\":52108},{\"end\":52138,\"start\":52121},{\"end\":52478,\"start\":52465},{\"end\":52493,\"start\":52478},{\"end\":52870,\"start\":52854},{\"end\":52884,\"start\":52870},{\"end\":52897,\"start\":52884},{\"end\":52914,\"start\":52897},{\"end\":52927,\"start\":52914},{\"end\":52942,\"start\":52927},{\"end\":52957,\"start\":52942},{\"end\":52975,\"start\":52957},{\"end\":53389,\"start\":53374},{\"end\":53402,\"start\":53389},{\"end\":53417,\"start\":53402},{\"end\":53834,\"start\":53820},{\"end\":53847,\"start\":53834},{\"end\":53862,\"start\":53847},{\"end\":54205,\"start\":54190},{\"end\":54219,\"start\":54205},{\"end\":54313,\"start\":54306},{\"end\":54328,\"start\":54313},{\"end\":54642,\"start\":54635},{\"end\":54655,\"start\":54642},{\"end\":54673,\"start\":54655},{\"end\":54976,\"start\":54962}]", "bib_venue": "[{\"end\":39133,\"start\":39102},{\"end\":39472,\"start\":39452},{\"end\":39840,\"start\":39792},{\"end\":40255,\"start\":40207},{\"end\":40688,\"start\":40668},{\"end\":41010,\"start\":40919},{\"end\":41355,\"start\":41335},{\"end\":41627,\"start\":41577},{\"end\":42033,\"start\":42000},{\"end\":42270,\"start\":42239},{\"end\":42711,\"start\":42642},{\"end\":43256,\"start\":43208},{\"end\":43729,\"start\":43719},{\"end\":44014,\"start\":43938},{\"end\":44330,\"start\":44313},{\"end\":44771,\"start\":44723},{\"end\":45184,\"start\":45145},{\"end\":45494,\"start\":45474},{\"end\":45839,\"start\":45789},{\"end\":46219,\"start\":46186},{\"end\":46642,\"start\":46594},{\"end\":47036,\"start\":46934},{\"end\":47309,\"start\":47268},{\"end\":47662,\"start\":47588},{\"end\":48043,\"start\":47995},{\"end\":48424,\"start\":48358},{\"end\":48704,\"start\":48617},{\"end\":48934,\"start\":48914},{\"end\":49118,\"start\":49068},{\"end\":49393,\"start\":49345},{\"end\":49760,\"start\":49727},{\"end\":50062,\"start\":50008},{\"end\":50246,\"start\":50226},{\"end\":50497,\"start\":50456},{\"end\":50925,\"start\":50892},{\"end\":51308,\"start\":51260},{\"end\":51705,\"start\":51618},{\"end\":52171,\"start\":52138},{\"end\":52546,\"start\":52493},{\"end\":53030,\"start\":52975},{\"end\":53477,\"start\":53417},{\"end\":53910,\"start\":53862},{\"end\":54188,\"start\":54163},{\"end\":54398,\"start\":54344},{\"end\":54693,\"start\":54673},{\"end\":55037,\"start\":54976},{\"end\":55360,\"start\":55273},{\"end\":39488,\"start\":39474},{\"end\":39884,\"start\":39842},{\"end\":40299,\"start\":40257},{\"end\":40704,\"start\":40690},{\"end\":41371,\"start\":41357},{\"end\":42062,\"start\":42035},{\"end\":42776,\"start\":42713},{\"end\":43300,\"start\":43258},{\"end\":43735,\"start\":43731},{\"end\":44815,\"start\":44773},{\"end\":45219,\"start\":45186},{\"end\":45510,\"start\":45496},{\"end\":45885,\"start\":45841},{\"end\":46248,\"start\":46221},{\"end\":46686,\"start\":46644},{\"end\":47346,\"start\":47311},{\"end\":48087,\"start\":48045},{\"end\":48722,\"start\":48706},{\"end\":48950,\"start\":48936},{\"end\":49437,\"start\":49395},{\"end\":50534,\"start\":50499},{\"end\":50954,\"start\":50927},{\"end\":51352,\"start\":51310},{\"end\":51788,\"start\":51707},{\"end\":52200,\"start\":52173},{\"end\":52595,\"start\":52548},{\"end\":53077,\"start\":53032},{\"end\":53529,\"start\":53479},{\"end\":53954,\"start\":53912},{\"end\":54709,\"start\":54695},{\"end\":55094,\"start\":55039},{\"end\":55378,\"start\":55362}]"}}}, "year": 2023, "month": 12, "day": 17}