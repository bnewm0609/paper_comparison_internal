{"id": 251040952, "updated": "2023-10-05 12:26:07.495", "metadata": {"title": "Improving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes", "authors": "[{\"first\":\"Sungha\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"Seunghan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Seokeon\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"Sungrack\",\"last\":\"Yun\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This paper proposes a novel test-time adaptation strategy that adjusts the model pre-trained on the source domain using only unlabeled online data from the target domain to alleviate the performance degradation due to the distribution shift between the source and target domains. Adapting the entire model parameters using the unlabeled online data may be detrimental due to the erroneous signals from an unsupervised objective. To mitigate this problem, we propose a shift-agnostic weight regularization that encourages largely updating the model parameters sensitive to distribution shift while slightly updating those insensitive to the shift, during test-time adaptation. This regularization enables the model to quickly adapt to the target domain without performance degradation by utilizing the benefit of a high learning rate. In addition, we present an auxiliary task based on nearest source prototypes to align the source and target features, which helps reduce the distribution shift and leads to further performance improvement. We show that our method exhibits state-of-the-art performance on various standard benchmarks and even outperforms its supervised counterpart.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.11707", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ChoiYCY22", "doi": "10.48550/arxiv.2207.11707"}}, "content": {"source": {"pdf_hash": "911770d6614af8f46a352b7427ad47a76984ab2f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2207.11707v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8f9feb1baa85f15b9544924417a2b50d12e44161", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/911770d6614af8f46a352b7427ad47a76984ab2f.txt", "contents": "\nImproving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes\n\n\nSungha Choi sunghac@qti.qualcomm.com \nQualcomm AI Research \u2020\n\n\n\u22c6 Seunghan seunghan@qti.qualcomm.com \nQualcomm AI Research \u2020\n\n\nYang Seokeon seokchoi@qti.qualcomm.com \nQualcomm AI Research \u2020\n\n\nChoi Sungrack sungrack@qti.qualcomm.com \nQualcomm AI Research \u2020\n\n\nYun \nQualcomm AI Research \u2020\n\n\nImproving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes\nTest-time adaptationDomain generalizationSource-free domain adaptationOn-Device AI\nThis paper proposes a novel test-time adaptation strategy that adjusts the model pre-trained on the source domain using only unlabeled online data from the target domain to alleviate the performance degradation due to the distribution shift between the source and target domains. Adapting the entire model parameters using the unlabeled online data may be detrimental due to the erroneous signals from an unsupervised objective. To mitigate this problem, we propose a shift-agnostic weight regularization that encourages largely updating the model parameters sensitive to distribution shift while slightly updating those insensitive to the shift, during test-time adaptation. This regularization enables the model to quickly adapt to the target domain without performance degradation by utilizing the benefit of a high learning rate. In addition, we present an auxiliary task based on nearest source prototypes to align the source and target features, which helps reduce the distribution shift and leads to further performance improvement. We show that our method exhibits state-of-the-art performance on various standard benchmarks and even outperforms its supervised counterpart.After deep neural networks (DNNs) trained on a given dataset (i.e., source domain) are deployed to a new environment (i.e., target domain), the DNNs make predictions from the data in the target domain. However, in most cases, the distribution of the source and target domains varies significantly, which degrades the model's performance in the target domain. If the deployed model does not remain stationary during test time but adapts to the new environment using clues about unlabeled target data, its performance can be improved[55,62,66,44,27,40,41,69].Recently, several studies[62,66,44,27]  have proposed test-time adaptation to update the model during test time after model deployment. However, it is extremely challenging to adapt the model to the target domain with only unlabeled \u22c6 Corresponding author.\n Fig. 1\n: Comparison of average error (%) between our approach and other methods with varying learning rates on CIFAR-100-C [22]. The x-and y-axes are the learning rate and average error rate, respectively. (a) Our method significantly outperforms the other three methods: (b) updating the entire parameters with only entropy minimization, (c) the state-of-the-art method, TENT [62], and (d) a supervised method. (e) Our proposed SWR keeps the performance stable with the combining of entropy minimization even at higher learning rates: [1e-3, 1e-4].\n\nonline data. As shown in Fig. 1(b), the adaptation of the entire model parameters may be detrimental due to the erroneous signals from the unsupervised objective such as entropy minimization [17,28,42,60,62], and the performance may be highly dependent on the learning rate. In addition, since the test-time adaptation can access unlabeled target data only once, and the adaptation proceeds simultaneously with the evaluation, updating all network parameters may result in overfitting [65,19]. Thus, several approaches present the methods to update only some part of the network architecture [62,66,44,27] such as batch normalization [26] or classifier layers. Especially, T3A [27] proposes an optimization-free method to adapt only the classifier layers using unlabeled target data, and TENT [62] updates batch statistics and affine parameters in the batch normalization layers by entropy minimization on unlabeled target data. However, updating only partial parameters or layers of the model may only result in marginal performance improvement, as shown in Fig. 1(c). Furthermore, such methods cannot be applied to the model architecture without a specific layer such as batch normalization or classifier layers. Other approaches [55,41] propose to jointly optimize a main task 1 and a selfsupervised task, such as rotation prediction [15] or instance discrimination [20,7], during pre-training in the source domain, and then update the model using only the self-supervised task during test time. In contrast to the unsupervised objective for the main task that highly depends on the model's prediction accuracy, the self-supervised task always obtains a proper supervisory signal. However, the self-supervised task may interfere with the main task if both tasks are not properly aligned [41,53,67]. In addition, these approaches cannot be applied to adapt arbitrary pre-trained models to the target domain since they require specific pre-training methods in the source domain.\n\nTo resolve these issues, we present two novel approaches for the test-time adaptation. First, we consider a shift-agnostic weight regularization (SWR) that enables the model to quickly adapt to the target domain, which is beneficial when updating the entire model parameters with a high learning rate. In contrast to Fig. 1(b), the entropy minimization with the proposed SWR shows superior performance and less dependency on the learning rate choice, as shown in Fig. 1(e). In terms of distribution shift, the SWR identifies the entire model parameters into shift-agnostic and shift-biased parameters, updating the former less and the latter more. Second, we present an auxiliary task based on a non-parametric nearest source prototype (NSP) classifier, which pulls the target representation closer to its nearest source prototype. With the NSP classifier, both source and target representations can be well aligned, which significantly improves the performance of the main task. Our proposed method ( Fig. 1(a)) outperforms the state-of-the-art method [62] (Fig. 1(c)) and even the supervised method using ground-truth labels ( Fig. 1(d)).\n\nOur method requires access to the source data to identify shift-agnostic and biased parameters and generate source prototypes before the model deployment, but it is applicable to any model regardless of its architecture or pre-training procedure. If a given model is pre-trained on open datasets, or if the source data owner deploys the model, source data is accessible before model deployment. In this case, our method significantly enhances the test-time adaptation capability by leveraging the source data without modifying the pre-trained model. Unlike TTT [55] and TTT++ [41], we do not change the pre-training method of a given model, so our method can take benefit from any pre-trained strong models, such as AugMix [23] ( Table 1) or CORAL [54] (Table 6), as a good starting point for test-time adaptation. In these respects, we believe our method is practical.\n\nThe major contributions of this paper can be summarized as follows\n\n\u2022 Two novel approaches for test-time adaptation are presented in this paper. The proposed SWR enables the model to quickly and reliably adapt to the target domain, and the NSP classifier aligns the source and target features to reduce the distribution shift, leading to further performance improvement. \u2022 Our test-time adaptation method is model-agnostic and not dependent on the pre-training method in the source domain, and thus it can be applied to any pre-trained model. Therefore, our method can also complement other domain generalization approaches that mainly focus on the pre-training method in the source domain before model deployment. \u2022 We show that our method achieves state-of-the-art performance through extensive experiments on CIFAR-10-C, CIFAR-100-C, ImageNet-C [22] and domain generalization benchmarks including PACS [33], OfficeHome [58], VLCS [12], and TerraIncognita [5]. Especially, our method even outperforms its supervised counterpart on CIFAR-100-C dataset.\n\n\nRelated Work\n\n\nSource-Free Domain Adaptation\n\nUnsupervised domain adaptation (UDA) [54,60,56,13,48,24,14] assumes simultaneous access to both the source and target domains. Data is often distributed across multiple devices. In such cases, UDA requires data sharing for simultaneous access to all data. However, it is often impossible due to data privacy concerns, limited bandwidth, and computational cost. Source-free domain adaptation [40,38,32,61,63,64,1] overcomes this challenge by adapting a source pretrained model to the target domain using only unlabeled target data. These approaches focus on offline adaptation in which the same target sample is fed to the model multiple times during target adaptation, whereas our method concentrates on online adaptation.\n\n\nTest-Time Adaptation and Training\n\nTest-time adaptation focuses on online adaptation where all test data can be accessed only once, and adaptation is performed simultaneously with evaluation. More specifically, it forward propagates target samples through the model for evaluation and then backpropagates the error signal from the model's output in an unsupervised manner for training [62]. Several studies adopt self-supervised learning, such as rotation prediction [55] or instance discrimination [41], to jointly optimize the main and self-supervised tasks on the source domain and then optimize only the self-supervised task on the target domain. However, these methods are not universally applicable to arbitrary pre-trained models as they require specific pre-training methods in the source domain. Recently, several methods [62,27,44,66] have proposed model-agnostic test-time adaptation, which is independent of the pre-training method in the source domain. TENT [62] uses the batch statistics of the target domain and optimizes channel-wise affine parameters using entropy minimization loss. T3A [27] proposes an optimizationfree method that adjusts a pre-trained linear classifier by updating the prototype for each class during test time. However, since these methods update only partial parameters or layers of the model, such as the batch normalization [62,44,66] \n\n\nProposed Method\n\nAssume that the model parameters \u03b8 trained on the source domain consist of an encoder part \u03b8 e and a classifier part \u03b8 c , as shown in Fig. 2(c). After being deployed to the target domain, the model infers the class probability distribution of the target sample and then optimizes our proposed test-time adaptation loss L target \u03b8e,\u03b8c . The overall loss of our proposed method is defined as\nL target \u03b8e,\u03b8c = L main \u03b8e,\u03b8c + L aux \u03b8e + \u03bb r l w l \u2225\u03b8 l \u2212 \u03b8 * l \u2225 2 2 ,(1)\nwhere w l denotes the l-th element of the penalty vector w used to control the update of the model parameters, \u03b8 l is the parameter vector of the l-th layer 2 of the model, \u03b8 * is the parameters from the previous update step, \u03bb r is the importance of the regularization term, and L main \u03b8e,\u03b8c and L aux \u03b8e denote the main and auxiliary task losses, respectively. Optimizing the main task loss updates the entire model parameters \u03b8 e and \u03b8 c , whereas optimizing the auxiliary task loss updates only the encoder part \u03b8 e . We first present a shift-agnostic weight regularization (SWR) and then describe an entropy objective of the main task. Finally, we propose an auxiliary task based on a nearest source prototype (NSP) classifier, which directly benefits the main task.\n\n\nShift-agnostic Weight Regularization\n\nThe main idea of the SWR is to impose different penalties for each parameter update during test-time adaptation, depending on the sensitivity of each model parameter to the distribution shift. Assuming that the distribution shift is mainly caused by color and blur shifts, we mimic the distribution shift using transformation techniques such as color distortion and Gaussian blur. Experiments on variations of the SWR, including the use of other transform functions, can be found in the supplementary Section B.\n\nTo obtain the penalty vector w specified in Eq. (1), we first forward-propagate two input images (i.e., the original and its transformed image) through the source pre-trained model and then back-propagate the task loss (i.e., cross entropy)  using the source labels to produce two sets g and g \u2032 of L gradient vectors, respectively. Note that L is the total number of layers in the model. Then the l-th element w l of the penalty vector w is calculated by employing the average cosine similarity s l between two gradient vectors, g l and g \u2032 l from N source samples as\ns l = 1 N N i=1 g i l \u00b7 g \u2032i l \u2225g i l \u2225\u2225g \u2032i l \u2225 \u2208 R, w = (\u03bd [s 1 , . . . , s l , . . . , s L ]) 2 \u2208 R L ,(2)\nwhere \u03bd [\u00b7] denotes min-max normalization with the range of [0,1], g i l and g \u2032i l denote the l-th gradient vectors for i-th source sample and its transformed sample, respectively. N denotes the total number of samples. Note that the penalty vector w is obtained from a frozen pre-trained source model before model deployment. Therefore, this process is independent of the source model's pre-training method and does not require source data after model deployment, as shown in Fig. 2. As shown in Eq. (1) and Fig. 3, during test-time adaptation, we apply the layer-wise penalty value w l to the difference between previous and current model parameters for each layer, and this controls the update of model parameters differently for each layer. Therefore, the model parameters belonging to the layers with high cosine similarity between the two gradient vectors are considered shift-agnostic, and we less update them by imposing high penalties. Section 4.6 experimentally shows that SWR takes advantage of using high learning rates to adapt the model to the target domain quickly.\n\n\nEntropy Objective for the Main Task\n\nThe main task of the model f \u03b8 is defined as the task performed by the parameters \u03b8 e and \u03b8 c . The loss function for the main task during test time is built using the entropy of model predictions\u1ef9 on test samples from the target distribution. We adopt information maximization loss [29,50,25 (1) and (2) until prototypes of all classes are generated, then train the projector and update the source prototype at the same time through an iterative process from (1) to (6) on the source data. (a) and (b) pull the original source projection and its transformed source projection, respectively, such that they become closer to the nearest source prototype from the original one.\n\u2a2f (a) (b) (a) (a) (b) (b)L main \u03b8e,\u03b8c = \u03bb m1 1 N N i=1 H(\u1ef9 i ) \u2212 \u03bb m2 H(\u0233),(3)\nwhere H(p) = \u2212 C k=1 p k log p k ,\u0233 = 1 N i\u1ef9 i , \u03bb m1 and \u03bb m2 indicate the importance of each term. The number of classes and the batch size are denoted by C and N . Intuitively, entropy minimization makes individual predictions confident, and mean entropy maximization encourages average prediction within a batch to be close to the uniform distribution.\n\n\nAuxiliary Task based on the Nearest Source Prototype\n\nDue to the distribution shift between the source and target domains, the target features deviate from the source features at test time. To resolve this issue, we propose an auxiliary task based on the nearest source prototype (NSP) classifier, which pulls the target embeddings closer to their nearest source prototypes in the embedding space. Eventually, optimizing the auxiliary task improves performance significantly since it directly supports the main task by aligning the source and target representations. We first explain how to generate source prototypes and define the NSP classifier based on them.\n\nSource prototype generation The source prototypes are defined as the averages over source embeddings for each class. As shown in Fig. 4, we freeze the model f \u03b8 trained on the source data and attach an additional projection layer h \u03c8 behind the encoder f \u03b8e . The encoder f \u03b8e infers the representation h from the source sample x, and the projector h \u03c8 maps h to the projection z in another embedding space where the loss L emb \u03c8 is applied as z = h \u03c8 (f \u03b8e (x)). The source prototype q k t for class k is updated through exponential moving average (EMA) with the projection z k t of the source sample (x, y k ) k\u2208 [1,C] at time t during the optimization trajectory as  ) and (c) pull the original target projection and its transformed target projection, respectively, such that they become closer to the nearest source prototype from the original one.\nq k t = \u03b1 \u00b7 q k t\u22121 + (1 \u2212 \u03b1) \u00b7 z k t ,(4)\nwhere \u03b1=0.99 and q k 0 = z k 0 . We define the NSP classifier as a non-parametric classifier. It measures the cosine similarity of a given target embedding to the source prototypes for all classes and then generates a class probability distribution\u0177 a\u015d\ny = C k=1 exp S(z, q k )/\u03c4 C j=1 exp (S(z, q j )/\u03c4 ) y k ,(5)\nwhere S(\u00b7, \u00b7) is a cosine similarity function, S(a, b) = (a \u00b7 b)/\u2225a\u2225\u2225b\u2225, \u03c4 denotes a temperature that controls the sharpness of the distribution, and y k is the one-hot ground-truth label vector of k-th class. In addition, inspired by recent self-supervised contrastive learning methods [7,20,4], we enable the projector h \u03c8 to learn transformation-invariant mapping. We obtain projection z \u2032 of the transformed source sample by z \u2032 = h \u03c8 (f \u03b8e (T (x))), where T (\u00b7) denotes an image transform function. The embedding loss L emb \u03c8 consisting of two cross entropy loss terms is applied to the embedding space to train the projector h \u03c8 as\nL emb \u03c8 = 1 N N i=1 (CE (y i ,\u0177 i ) + CE (y i ,\u0177 \u2032 i )) ,(6)\nwhere CE (p, q) = \u2212 C k=1 p k log q k , and y i is the ground-truth label of i-th source sample. Here,\u0177 and\u0177 \u2032 denote the outputs of the NSP classifier for the projections z and z \u2032 of the source sample and its transformed one, respectively. As shown in Fig. 4, optimizing the embedding loss encourages the projector h \u03c8 to learn a mapping that pulls the projections belonging to the same class closer together and pushes source prototypes farther away from each other.\n\nNote that this process is applied to a frozen pre-trained source model and completed before model deployment. Therefore, it is model-agnostic and does not require source data during test time.\n\nAuxiliary task loss at test time Once the source prototypes are generated and the projection layer is trained, we can deploy the model and then jointly optimize both main and auxiliary tasks on unlabeled online data. The auxiliary task loss L aux \u03b8e consists of two objective functions: the entropy objective L aux ent \u03b8e using the entropy of the NSP classifier's prediction\u0177, and the self-supervised loss L aux sel \u03b8e that encourages the model's encoder f \u03b8e to learn transformationinvariant mappings as\nL aux \u03b8e = L aux ent \u03b8e + \u03bb s L aux sel \u03b8e ,(7)\nwhere \u03bb s denotes the importance of the self-supervised loss term. Similarly to Eq. (3), the entropy objective is built by using the entropy of the prediction\u0177 of the NSP classifier on the target sample as\nL aux ent \u03b8e = \u03bb a1 1 N N i=1 H(\u0177 i ) \u2212 \u03bb a2 H(\u0233),(8)\nwhere N is batch size, \u03bb a1 and \u03bb a2 indicate the importance of each term,\nH(p) = \u2212 C k=1 p k log p k , and\u0233 = 1 N N i=1\u0177 i .\nThe self-supervised loss is applied to the prediction\u0177 \u2032 of the NSP classifier on the transformed target sample as\nL aux sel \u03b8e = \u2212 1 N N i=1 C k=1\u0177 k i log\u0177 \u2032k i .(9)\nAs shown in Fig. 5, the entropy objective function ( Fig. 5(b)) pulls the projection z of the target sample to move closer to its nearest source prototype, and the selfsupervised objective (Fig. 5(c)) encourages the projection z \u2032 of the transformed target sample to get closer to the same target as z.\n\n\nExperiments\n\nThis section describes the experimental setup, implementation details, and the experimental results of the comparisons with other state-of-the-art methods in test-time adaptation. We also show that generalization performance can be further improved by combining our proposed method with an existing domain generalization strategy that mainly focuses on training time in the source domain.\n\n\nExperimental Setup\n\nFollowing TENT [62] and T3A [27], all experiments in this paper are conducted on the online adaptation setting, where adaptation is performed concurrently with evaluation at test time without seeing the same data twice or more. After a prediction is obtained, the model is updated via back-propagation. We evaluate our proposed method on CIFAR-10-C, CIFAR-100-C, ImageNet-C 3 [22] and four domain generalization benchmarks such as PACS [33], Of-ficeHome [58], VLCS [12], and TerraIncognita [5]. Since our method can be used independently of the backbone networks and its pre-training method, we apply our method to publicly available pre-trained models for evaluation.\n\nWe perform experiments on CIFAR datasets using WideResNet-28-10 [68] and WideResNet-40-2 [68] as backbone networks, based on RobustBench [11].  Fig. 3 and Fig. 5, and random cropping and random horizontal flipping are additionally applied for the image transformations in Fig. 4. We use batch statistics on test data instead of using running estimates. The hyper-parameters are empirically set as \u03bb m1 =0.2, \u03bb a1 =0.8 \u03bb m2 =0.25, \u03bb a2 =0.25, \u03bb s =0.1, \u03bb r =250, and softmax temperature \u03c4 =0.1. The epoch for training the projector is 20, and N =1024 in Eq. (2). Since these hyper-parameters are not sensitive to the backbone and datasets, they are fixed without individual tuning in most experiments in this paper unless noted otherwise. The projector as described in Section 3.3 can be configured as a single-or multi-layer perceptron (MLP). The MLP consists of a linear layer followed by batch normalization [26], ReLU [45], and a final linear layer with output dimension 512. The performance change according to the projector configuration is shown in Table 3, and the detailed architecture is described in the supplementary Section C.\n\n\nRobustness against Image Corruptions\n\nTable 1(a) shows a comparison of the robustness between our method and recent test-time adaptation methods for the most severe corruptions on CIFAR-100-C. TFA [41] and TTT++ [41] were originally implemented as offline adaptation methods that train a model by observing the same data multiple times across numerous epochs, so we change these methods to the online adaptation setting to reproduce the results. Our proposed method significantly outperforms other state-of-the-art methods with large margins of 3.89% for ResNet-50 and 2.59% for WRN-40-2. Table 1(b) shows the results on the most severe corruptions of CIFAR-10-C. Our method consistently outperforms other methods on CIFAR-10/100-C datasets across various backbone networks. In particular, WRN-40-2, which is trained with AugMix [23] for a data processing to increase the robustness  Table 2 shows the effectiveness of our proposed shift-agnostic weight regularization (SWR) and nearest source prototype (NSP) classifier through ablation studies. At a high learning rate, optimizing only the main task loss based on the entropy of the model prediction results in poor performance, but adjusting the learning rate reduces the error rate to 39.44%. Adding the NSP to the main task loss leads to the performance improvement of 1.89%, and including the SWR improves the performance by 1.68% even at a high learning rate. Our method  with both SWR and NSP achieves 35.65% error rate with 3.79% performance enhancement compared to using only the main task loss. Table 3 shows the performance impact of changing the projector depth (i.e., number of projection layer). In addition, we conduct experiments to apply the auxiliary task loss L aux \u03b8e directly to the feature representation h, the encoder's output without using the projector. The model with the projector outperforms the one without the projector on CIFAR-100-C, and opposite results are obtained on CIFAR-10-C. Since the auxiliary task loss is applied to the embedding space based on the cosine similarity between the source prototypes and the target embeddings, its effect may be minimal if they are severely misaligned. To compensate for this issue, we attach and train the projector that minimizes the misalignment between the source and target embeddings by enabling transformation-invariant mapping and bringing the projections belonging to the same class closer together in the new embedding space. However, if the number of classes is small (e.g., CIFAR-10-C), the source and target may already be relatively well aligned compared to the case with a large number of classes (e.g., CIFAR-100-C). In this case, we conjecture that applying the auxiliary task loss directly to the encoder's output h rather than the new embedding space z, the projector's output, generates a better-aligned representation h between the source and target, which can be more helpful to the classifier. Table 4 shows the experimental results according to (a) the projector width (i.e., output dimension of the last layer), (b) the transformation used for training the projector, and (c) whether to fine-tune or freeze the projector during testtime adaptation. Our default settings are marked with gray-colored cells, and  these settings are also applied to the domain generalization benchmarks in the following section without additional tuning.\n\n\nAblation Studies\n\n\nProjector Design and Hyper-parameter Impacts\n\n\nQuick Adaptation\n\nAs shown in Table 5, it is natural that the supervised method performs perfectly when learning and evaluating the same test samples iteratively. However, interestingly our method outperforms the supervised one in an online setting where the test sample is seen only once. Unlike the other methods that require a low learning rate to train ( Fig. 1(b),(d)), our method updates the entire parameters at a high learning rate. We conjecture that SWR enables quick convergence without performance degradation because only parameters sensitive to distribution shift (i.e., parameters that need to quickly adapt to a new domain) are largely updated with a high learning rate.\n\n\nDomain Generalization Benchmarks\n\nTo evaluate our method on the DG benchmarks, we follow the protocol proposed by DomainBed [18] and T3A [27]. Our method is model-agnostic, so we apply it to the pre-trained models using empirical risk minimization (ERM) [57] or CORAL [54] on the source domain in order to adapt the models to the target domain at test time. We use the leave-one-domain-out validation [18] for model selection in all experiments in Table 6. Our methods show state-of-the-art performance on average over four datasets and especially outperform T3A [27] and the source pre-trained models with a large margin on PACS, OfficeHome, and TerraIncognita datasets. The detailed experimental setup can be found in the supplementary Section C.  \n\n\nQualitative Results\n\n\nMotion Blur Frost Contrast\n\n\nImpulse Noise Snow\n\n\nNo Adapt Ours\n\nPixelate\n\n\nWRN-40-2\n\nResNet-50 the second row are from ResNet-50. Even without test-time adaptation, WRN-40-2 (AugMix) [23] is more robust against corruptions than ResNet-50, so better results can be obtained. Our method significantly improves the performance in terms of intra-class cohesion and inter-class separation in both backbones.\n\n\nNo Adapt Ours No Adapt Ours\n\n\nConclusions\n\nThis paper proposed two novel approaches for model-agnostic test-time adaptation. Our proposed shift-agnostic weight regularization enables the model to reliably and quickly adapt to unlabeled online data from the target domain by controlling the update of the model parameters according to their sensitivity to the distribution shift. In addition, our proposed auxiliary task based on the nearest source prototype classifier boosts the performance by aligning the source and target representations. Test-time adaptation is a challenging but promising area in terms of allowing the model to evolve itself while adapting to a new environment without human intervention. In this regard, our efforts aim to promote the importance of this field and stimulate new research directions.  This supplementary material begins with a discussion of our two proposed approaches and then provides additional quantitative results examining hyperparameter impacts, the performance of the NSP classifier, the transform function of the SWR, and the SWR variants. We also evaluate our approach on a largescale dataset, ImageNet-C [22], and expand the proposed SWR to support pixellevel classification (i.e., semantic segmentation) on Cityscapes-C [10,22]. Finally, we provide further implementation details for the projector and additional information about the experiments on the domain generalization benchmarks.\n\n\nA Discussion\n\n\nA.1 Shift-agnostic Weight Regularization\n\nThe intuition of SWR is to control the update of model parameters depending on each parameter's sensitivity to the distribution shift. If \u03b8 * in Eq. (1) is fixed as source model parameters without being updated with the model parameters from the previous step, it is difficult to adapt the model to the target data sufficiently. Constraining the model parameters not to move away significantly from the source model parameters is not the purpose of SWR (Instead, NSP aligns source and target features based on the source prototypes). Updating \u03b8 * shows better performance than freezing \u03b8 * , and these results are included in the supplementary Section B.7.\n\nWe assumed color and blur as a distribution shift to find shift-agnostic and shift-biased weights. Using too various augmentations increases the number of shift-biased weights. This means that more parameters are largely updated, which may result in performance degradation (Table 8). We conjecture that the augmentations such as color and blur, which can be commonly included in various domain gaps, are suitable for finding shift-agnostic weights.\n\nWe generated the penalty vector w in parameter-wise, output channel-wise, and layer 1 -wise manners, and we chose the best method experimentally. While  , which is much more granular than SpotTune.\n\n\nA.2 Superiority of Nearest Source Prototypes\n\nThe purpose of the NSP is twofold: (1) aligning target and source features by leveraging the source prototypes as reference points (Fig. 5(b), L aux ent \u03b8e ), and (2) learning input consistency (Fig. 5(c), L aux sel \u03b8e ). As shown in Table 2, L aux ent \u03b8e has more crucial contribution than L aux sel \u03b8e . To further validate the superiority of NSP, we conduct experiments while keeping SWR but replacing NSP with FixMatch [51] using this Pytorch implementation 2 on settings (a) and (b) in Table 1. FixMatch improves performance by up to 0.13% compared to using SWR alone (up to 2.11% increase when NSP is applied). We can find that learning only input consistency, such as applying FixMatch, is not sufficient to handle the distribution shift between source and target.\n\n\nB Further Experiments\n\n\nB.1 Impact of Number of Source Samples in SWR\n\nAs described in Section 3.1, the penalty vector w is calculated by employing the average cosine similarity between two gradient vectors g and g \u2032 from N source samples. We conduct the experiment to analyze the impact of the number of source samples on performance. As shown in Fig. 7, the smaller the number N of source samples, the higher the error rate. However, with more than 256 images, performance tends to remain stable and low regardless of the number of source samples. We use 1k source samples as the default setting for all experiments, as mentioned in Section 4.2. This experiment shows that not all source data is required for the SWR.\n\n\nB.2 Importance of Each Loss Term\n\nAs described in Section 3, our proposed losses are defined as L target \u03b8e,\u03b8c = L main \u03b8e,\u03b8c + L aux \u03b8e + \u03bb r l w l \u2225\u03b8 l \u2212 \u03b8 * l \u2225 2 2 = L main \u03b8e,\u03b8c + L aux ent where H(p) = \u2212 C k=1 p k log p k and CE (p, q) = \u2212 C k=1 p k log q k . Here,\u1ef9 denotes the prediction of the main classifier,\u0177 is the prediction of the NSP classifier, symbol\u00afindicates average class probability distribution over batch samples, and symbol \u2032 denotes the prediction of the transformed sample. The hyperparameters indicating the importance of each term are empirically set as \u03bb m1 =0.2, \u03bb a1 =0.8 \u03bb m2 =0.25, \u03bb a2 =0.25, \u03bb s =0.1, and \u03bb r =250. Table 7 shows the error rate (%) according to the changes in importance of each term. The default settings are indicated by gray-colored cells.\n\u03b8e + \u03bb s L aux sel \u03b8e + \u03bb r l w l \u2225\u03b8 l \u2212 \u03b8 * l \u2225 2 2 = \u03bb m1 1 N N i=1 H(\u1ef9 i ) \u2212 \u03bb m2 H(\u0233) L main \u03b8e,\u03b8c + \u03bb a1 1 N N i=1 H(\u0177 i ) \u2212 \u03bb a2 H(\u0233) L aux ent \u03b8e + \u03bb s 1 N N i=1 CE \u0177 k i ,\u0177 \u2032k i L aux sel \u03b8e + \u03bb r l w l \u2225\u03b8 l \u2212 \u03b8 * l \u2225 2 2 ,(10)\n\nB.3 Ablation Studies on Transform Functions of SWR\n\nAs described in Section 3.1, the SWR employs the sensitivity of each model parameter to distribution shift, and we simulate the distribution shift through the transform functions such as color distortion and Gaussian blur. We conduct ablation studies on transform functions to find the optimal combination of transformations. Table 8 shows the experimental results by adding each transformation in order. We use the following combinations as default setting: Col-orJitter, RandomGrayscale, RandomInvert, and Gaussian Blur in Pytorch. The pseudo-code for our default setting using Pytorch is as follows.     Table 8.   The goal of the NSP classifier is to improve the performance of the main classifier by aligning the source and target representations through its optimization, so the NSP classifier itself is not used for the classification. Table 9 shows the performance of the NSP classifier, and we can see that it is not as good as the performance of the main classifier. Table 10 shows the performance comparison according to the methods of obtaining the source prototypes. The source prototypes can be generated and updated by an exponential moving average of projection z or feature representation h inferred across the source samples. Additionally, since the network weights for each class of the source pre-trained linear classifier can be considered as a source prototype, we report the result of employing the main classifier's parameter vectors \u03b8c as source prototypes without forward-propagation of the source samples. Alternatively, we can consider freezing \u03b8 * as a source pre-trained model and compare the performance between updating \u03b8 * and freezing \u03b8 * , as shown in Table 11. Although freezing \u03b8 * performs worse than updating \u03b8 * , the advantage of restricting the model parameters not to deviate significantly from the source pre-trained model (i.e., freezing \u03b8 * ) seems worth exploring in future work.\n\n\nB.4 Variations of Shift-agnostic Weight Regularization\n\n\nB.6 Source Prototypes\n\n\nB.8 Experiments on ImageNet-C\n\nWe further validate our method on ImageNet-C [22] dataset. Following the ImageNet-C experiment in TENT [62], the batch size and learning rate are set to 64 and 0.00025, respectively. We only change the importance of the regularization term, \u03bbr, to 3000 from the default value of the hyper-parameters specified in Section 4.2 for experiments in this section. Table 12 demonstrates that our method is superior to TENT [62].   \n\n\nB.9 Experiments on Cityscapes-C\n\nAlthough this paper has focused on the image classification task, we further demonstrate the scalability of our proposed method by expanding it to support the pixel-level classification (i.e., semantic segmentation). We use two ResNet-50-based pre-trained models, DeepLabV3+ [6] and RobustNet [9], as baseline models for test-time adaptation. RobustNet can be a better starting point for test-time adaptation as it is more robust to distribution shift than DeepLabV3 due to improved domain generalization. We consider the original Cityscapes [10] dataset to be the source data and generate Cityscapes-C dataset by applying algorithmically created corruption [22] to the original Cityscapes. The Cityscapes-C dataset is regarded as unlabeled online test data. We adapt the Cityscapes pre-trained models to the Cityscapes-C using the loss as L target \u03b8e,\u03b8c = L main \u03b8e,\u03b8c + \u03bbr l w l \u2225\u03b8 l \u2212 \u03b8 * l \u2225 2 2 .\n\nNote that the loss includes only SWR, not NSP. The integration of NSP into the testtime adaptation loss for semantic segmentation is left for future work. The batch size, learning rate, and the importance \u03bbr of the regularization term are set to 2, 1e-5, and 40, respectively. In contrast to the test-time adaptation on the image classification task, which discards running estimates and uses batch statistics on test data, the running statistics of batch normalization layers are kept and updated on test data with a momentum of 0.1 during test time, and we set the \u03b8 * as the parameters of the source pre-trained model. Table 13 and Fig. 9 show that our proposed SWR greatly outperforms its baseline model and takes advantage of the strong baseline model, RobustNet, as a good starting point for test-time adaptation. In addition, our proposed SWR shows stable performance enhancement regardless of the learning rate in test-time adaptation, as shown in Table 14.\n\nInput Image (Snow Corruption) DeepLabV3+ RobustNet RobustNet + Ours (SWR) Fig. 9: Comparison of segmentation results on Cityscapes-C [10,22] (snow corruption) between DeepLabV3+ [6], RobustNet [9], and ours \u210e !  \n\n\nC Further Implementation Details\n\n\nC.1 Detailed Projector Design\n\nAs described in Section 3.3, we attach and train a projector behind the encoder to map the feature representation h to the projection z. The projector minimizes the misalignment between the source and target embeddings by enabling transformation-invariant mapping and bringing the projections belonging to the same class closer together in the new embedding space. However, in Section 4.5, we showed that applying auxiliary task loss directly to feature representation h without the projector may perform better on datasets with fewer classes (e.g., CIFAR-10-C). Fig. 10 shows the architectural differences between our proposed methods with and without the projector. If there is no projector, only steps (1) and (2) are repeated to obtain the prototype for each class by averaging over the feature representations h inferred across the source samples, as shown in Fig. 10(a). Also, the auxiliary task loss is applied to the feature representation h that is the encoder's output without using the projector, as shown in Fig. 10(c). Fig. 11 shows the detailed architecture of the projector.\n\n\nC.2 Experimental Setup for Domain Generalization Benchmarks\n\nAs described in Section 4.7, our implementation uses DomainBed 3 [18] and T3A 4 [27] framework to conduct the experiments on four domain generalization benchmarks such as PACS [33], OfficeHome [58], VLCS [12], and TerraIncognita [5]. Following T3A [27], we first train ResNet-50 backbone networks on each dataset by using ERM [57] and CORAL [54]. For this pre-training, we conduct a random search of five trials over the hyper-parameter distribution and repeat this procedure three times independently with a different random seed. Then, we apply our proposed method to each pre-trained model. Our method has two hyper-parameters: learning rate (LR) and projector. We set the search space for LR to LR \u2208 {5e-5, 1e-6} and provide two projector options: the model with or without the projector, as described in Section 4.5 and C.1. Note that the hyper-parameter selection is completed before deployment to the test domain according to leave-one-domain-out cross-validation [18]. We use a batch size of 200, and the other hyper-parameters described in Section 4.2 are set the same as the experiments on CIFAR datasets.\n\nFig. 3 :\n3Overall process of our proposed SWR. We first obtain the penalty vector w before model deployment and then use it as layer-wise penalties to control the update of the model parameters at test-time adaptation after model deployment.\n\nFig. 5 :\n5Test-time adaptation phase after model deployment. (a) main task loss. (b),(c) auxiliary task loss. (b\n\nFig. 6\n6visualizes the features on CIFAR-10-C using t-SNE [43]. The results in the first row are from WRN-40-2 as a source pre-trained model, and the results in\n\nFig. 6 :\n6t-SNE visualization of features from the target domain (CIFAR-10-C).\n\n\nGulrajani, I., Lopez-Paz, D.: In search of lost domain generalization. In: International Conference on Learning Representations (ICLR) (2020) 10, 13, 14, 28 19. Guo, Y., Shi, H., Kumar, A., Grauman, K., Rosing, T., Feris, R.: Spottune: transfer learning through adaptive fine-tuning. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 2, 20 20. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual representation learning. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 2, 8 21. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 10, 11, 22, 25 22. Hendrycks, D., Dietterich, T.: Benchmarking neural network robustness to common corruptions and perturbations. In: International Conference on Learning Representations (ICLR) (2018) 2, 3, 9, 19, 20, 25, 26, 27 23. Hendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.: Augmix: A simple data processing method to improve robustness and uncertainty. In: International Conference on Learning Representations (ICLR) (2019) 3, 10, 11, 14, 22, 25 24. Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A., Darrell, T.: Cycada: Cycle-consistent adversarial domain adaptation. In: International Conference on Machine Learning (ICML) (2018) 4 25. Hu, W., Miyato, T., Tokui, S., Matsumoto, E., Sugiyama, M.: Learning discrete representations via information maximizing self-augmented training. In: International Conference on Machine Learning (ICML) (2017) 6 26. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: International Conference on Machine Learning (ICML) (2015) 2, 10 27. Iwasawa, Y., Matsuo, Y.: Test-time classifier adjustment module for modelagnostic domain generalization. Advances in Neural Information Processing Systems (NeurIPS) (2021) 1, 2, 4, 9, 10, 13, 28 28. Jain, H., Zepeda, J., P\u00e9rez, P., Gribonval, R.: Learning a complete image indexing pipeline. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 2 29. Krause, A., Perona, P., Gomes, R.: Discriminative clustering by regularized information maximization. Advances in Neural Information Processing Systems (NeurIPS) (2010) 6 30. Krause, A., Perona, P., Gomes, R.: Discriminative clustering by regularized information maximization. In: Lafferty, J., Williams, C., Shawe-Taylor, J., Zemel, R., Culotta, A. (eds.) Advances in Neural Information Processing Systems (NeurIPS) (2010) 6 31. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009) 10, 20 32. Kundu, J.N., Venkat, N., Babu, R.V., et al.: Universal source-free domain adaptation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 4 33. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Deeper, broader and artier domain generalization. In: International Conference on Computer Vision (ICCV) , D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Learning to generalize: Metalearning for domain generalization. arXiv preprint arXiv:1710.03463 (2017) 4 35. Li, D., Zhang, J., Yang, Y., Liu, C., Song, Y.Z., Hospedales, T.M.: Episodic training for domain generalization. In: International Conference on Computer Vision (ICCV) (2019) 4 36. Li, H., Jialin Pan, S., Wang, S., Kot, A.C.: Domain generalization with adversarial feature learning. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 4 37. Li, L., Gao, K., Cao, J., Huang, Z., Weng, Y., Mi, X., Yu, Z., Li, X., Xia, B.: Progressive domain expansion network for single domain generalization. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 4 38. Li, R., Jiao, Q., Cao, W., Wong, H.S., Wu, S.: Model adaptation: Unsupervised domain adaptation without source data. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 4 39. Li, Y., Tian, X., Gong, M., Liu, Y., Liu, T., Zhang, K., Tao, D.: Deep domain generalization via conditional invariant adversarial networks. In: European Conference on Computer Vision (ECCV) (2018) 4 40. Liang, J., Hu, D., Feng, J.: Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In: International Conference on Machine Learning (ICML) (2020) 1, 4, 6, 11 41. Liu, Y., Kothari, P., van Delft, B., Bellot-Gurlet, B., Mordan, T., Alahi, A.: Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems (NeurIPS) (2021) 1, 2, 3, 4, 10, 11 42. Long, M., Zhu, H., Wang, J., Jordan, M.I.: Unsupervised domain adaptation with residual transfer networks. In: Advances in Neural Information Processing Systems (NeurIPS) (2016) 2 43. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research (2008) 13 44. Mummadi, C.K., Hutmacher, R., Rambach, K., Levinkov, E., Brox, T., Metzen, J.H.: Test-time adaptation to distribution shift by confidence maximization and input transformation. arXiv preprint arXiv:2106.14999 (2021) 1, 2, 4, 6 45. Nair, V., Hinton, G.E.: Rectified linear units improve restricted boltzmann machines. In: International Conference on Machine Learning (ICML) (2010) 10 46. Pan, X., Luo, P., Shi, J., Tang, X.: Two at once: Enhancing learning and generalization capacities via ibn-net. In: European Conference on Computer Vision (ECCV) (2018) 4 47. Rahman, M.M., Fookes, C., Baktashmotlagh, M., Sridharan, S.: Correlation-aware adversarial domain adaptation and generalization. Pattern Recognition (2020) 4 48. Saito, K., Watanabe, K., Ushiku, Y., Harada, T.: Maximum classifier discrepancy for unsupervised domain adaptation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 4 49. Seo, S., Suh, Y., Kim, D., Han, J., Han, B.: Learning to optimize domain specific normalization for domain generalization. arXiv preprint arXiv:1907.04275 (2019) 4 50. Shi, Y., Sha, F.: Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In: International Conference on Machine Learning (ICML) (2012) 6 51. Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.A., Cubuk, E.D., Kurakin, A., Li, C.L.: Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in Neural Information Processing Systems (NeurIPS) (2020) 20\n\n\u22c6\nCorresponding author. \u2020 Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 1 torch.nn.Module units defined in Pytorch.\n\nFig. 7 :\n7Comparison of error rate (%) according to the number of source samples (i.e., the images in CIFAR-10 [31] train set) used to obtain the sensitivity of each model parameter to distribution shift in the SWR. The x-and y-axes denote the number of source samples and the error rate on CIFAR-10-C [22], respectively. SpotTune [19] considers residual block units (16 units for ResNet-50) in terms of where to fine-tune, we generate the penalty values on layer 1 units (161 units for ResNet-50)\n\nFig. 8 :\n8Comparison of scatter plots of penalty vectors in various SWR variants. Xand y-axes indicate the layer index of the model and penalty value, respectively. Experiments using ResNet-50 and WRN-40-2 are conducted on CIFAR-100-C, and experiments with WRN-28-10 are performed on CIFAR-10-C. The number in each scatter plot indicates the error rate (%), and the number in parentheses denotes the difference from the error rate of our proposed SWR.\n\nFig. 8\n8visualizes the penalty vector w in various SWR variants. X-and y-axes denote the layer index and penalty value, respectively. We can see that the penalty vector is different for each backbone network, and a high penalty is applied to the later layers. (b) is the result of using the combination of all transform functions listed in\n\n\n(c) and (d) are the results of changing the exponent value of 2 in Eq. (2) into 1 and 3, respectively (i.e., w = (\u03bd [s1, . . . , s l , . . . , sL]) 1 and w = (\u03bd [s1, . . . , s l , . . . , sL]) 3 ). (e) and (f) are the results of flipping the original penalty vector of our proposed SWR vertically and horizontally, respectively. (g) to (j) are the results of employing manually designed functions without calculating the cosine similarity between two gradient vectors generated by back-propagation from the model's prediction of the source samples. Our proposed SWR outperforms the various variants of the SWR.\n\nFig. 10 :Fig. 11 :\n1011Comparison of models with and without a projector. (a) and (c) describe the model without the projector, and (b) and (d) demonstrate the model with the projector. (a) The source prototypes are generated from the inferred feature representation h without going through the projector. (b) The source prototypes are generated from the inferred projection z through the projector. The auxiliary task loss is applied to the feature representation (c) or the embedding space behind the projector (d). Detailed architecture of the projector.\n\n\nor classifier layer [27], they may be suboptimal for test-time adaptation.2.3 Domain Generalization \n\nSince UDA aims to adapt the model to the predefined target domain before \nmodel deployment, it is not suitable to guarantee generalization performance to \nother arbitrary target domains. On the other hand, domain generalization (DG) \ndiffers from UDA in that it assumes that the model accesses only the source \ndomain during training time before model deployment and aims to improve \nthe generalization capability in arbitrary unseen target domains. Numerous DG \napproaches using meta-learning [34,3,35], normalization [46,49,9,8], adversarial \ntraining [36,39,47], and data augmentation [70,59,16,37] have been proposed to \nlearn domain-agnostic feature representations for the target domain. However, \nthese studies only focus on methods at training time before model deployment, \nwhereas our method focuses on a test-time adaptation after model deployment. \nencoder \nencoder \nencoder \n\n(Model-agnostic) our proposed method \n\nprojector \n\nAny model (architecture, training procedure) \n\nBefore deployment After deployment \n\nfreeze \nupdate \noptional \n\n(b) Stage 1 \n(c) Stage 2 \n(a) Pre-training \n\nclassifier \nclassifier \nclassifier \n\nprojector \n\n\u210e \n\n: weight regularization \n\nEq. (1) \nEq. (3) \nEq. (7) \nEq. (8) \nEq. (9) \n\nEq. (2) \nEq. (4) \nEq. (5) \nEq. (6) \n\nclassifier \nencoder \nprojector \n\nFig. 2: Our method consists of two stages: (b) and (c). (a) our method takes the \npre-trained model in an off-the-shelf manner and (b) generates penalty vector \nw and source prototypes q while keeping the model frozen before model deploy-\nment. After model deployment, (c) our method does not access labeled source \ndata D s other than unlabeled online target data D t during test-time adaptation. \n\n\n\n\nIn the domain generalization setup, we use ResNet-50 [21] without the batch normalization layer, which is the default setting of DomainBed [18], DG benchmark framework. CIFAR-10/100 dataset [31] contains 50k images for training and 10k images for testing. Corruptions such as noise, blur, weather, and digital are applied to 10k images from CIFAR test set to create CIFAR-C test images. For test-time adaptation, 50k images for CIFAR training set are defined as the source domain, and 10k images for CIFAR-C test set are defined as the target domain. We integrate our proposed method within the frameworks officially provided by other state-of-the-art methods [62,41,27] for fair comparisons. Specifically, different frameworks are used for each experiment as follows: TENT framework [62] for all experiments with WRN-40-2 and WRN-28-10 backbone networks on CIFAR-10/100-C, TTT++ framework [41] for all experiments with ResNet-50 on CIFAR-10/100-C, and T3A framework [27] for all domain generalization benchmarks. For experiments on CIFAR, we follow the default values provided by each framework for experimental settings such as batch size and optimizer.Color distortion, random grayscale and Gaussian blurring are used as the image transformations specified in4.2 Implementation details \n\n\n\nTable 1 :\n1Comparison with other methods. * denotes the reported results from the original paper, and the others are reproduced values in our environment based on the official framework provided by TENT[62] and TTT++[41]. Source denotes the source pre-trained model without test-time adaptation.(a) Comparison of error rate (%) on CIFAR-100-C with severity level 5 Backbone Methods Avg. err Gaus. Shot Impu. Defo. Glas. Moti. Zoom Snow Fros. Fog Brig. Cont. Elas. Pixe. Jpeg Backbone Methods Avg. err Gaus. Shot Impu. Defo. Glas. Moti. Zoom Snow Fros. Fog Brig. Cont. Elas. Pixe. JpegWRN-40-2 \n(AugMix) \n[68,23] \n\nSource \n46.75 \n65.7 60.1 59.1 \n32.0 51.0 33.6 32.3 41.4 45.2 51.4 31.6 55.5 40.3 59.7 42.4 \n\nTENT [62] \n35.53 \n40.1 39.5 42.0 \n29.6 41.9 30.7 29.7 34.5 34.8 39.1 27.5 32.9 37.6 32.8 40.3 \n\nCore* [66] \n35.30 \n39.8 39.3 41.5 \n29.5 41.7 30.6 29.8 34.2 34.9 38.6 27.5 32.6 37.1 32.7 40.1 \n\nOurs \n32.71 \n37.6 36.6 35.1 28.0 39.5 28.7 28.5 31.3 32.6 34.4 26.3 29.0 35.5 30.1 37.7 \n\nResNet-50 \n[21] \n\nSource \n60.35 \n80.8 77.8 87.8 \n39.6 82.3 54.2 38.4 54.6 60.2 68.1 28.9 50.9 59.5 72.3 50.0 \n\nSHOT [40] \n43.53 \n49.0 47.1 61.2 \n33.8 58.3 41.0 31.3 45.0 42.0 52.0 29.7 33.4 47.9 41.8 39.4 \n\nTFA [41] \n44.13 \n49.0 47.0 61.4 \n34.2 58.9 41.5 32.1 46.8 43.0 54.6 31.2 33.7 48.9 39.8 39.7 \n\nTTT++ [41] 44.38 \n50.2 47.7 66.1 \n35.8 61.0 38.7 35.0 44.6 43.8 48.6 28.8 30.8 49.9 39.2 45.5 \n\nTENT [62] \n39.54 \n43.8 42.0 54.1 \n31.2 51.7 37.0 29.9 42.3 39.6 45.6 30.1 30.9 44.5 34.2 36.4 \n\nOurs \n35.65 \n40.0 38.4 46.3 29.3 46.0 32.5 27.9 37.3 36.6 37.3 27.5 28.8 41.0 31.4 34.7 \n\n(b) Comparison of error rate (%) on CIFAR-10-C with severity level 5 \n\nWRN-40-2 \n(AugMix) \n[68,23] \n\nSource \n18.27 \n28.8 23.0 26.2 \n9.5 \n20.6 10.6 \n9.3 \n14.2 15.3 17.5 \n7.6 \n20.9 14.7 41.3 14.7 \n\nTENT [62] \n12.08 \n15.6 13.2 18.8 \n7.9 \n18.2 \n9.0 \n8.0 \n10.4 10.9 12.4 \n6.7 \n10.0 14.0 11.4 14.8 \n\nOurs \n10.37 \n13.1 11.4 14.7 \n7.4 15.8 8.3 \n7.4 \n9.2 \n9.4 \n9.5 \n6.3 \n7.8 13.1 9.4 12.9 \n\nWRN-28-10 \n[68] \n\nSource \n43.51 \n72.3 65.7 72.9 \n49.9 54.3 34.8 42.0 25.1 41.3 26.0 \n9.3 \n46.7 26.6 58.5 30.3 \n\nTENT [62] \n18.58 \n24.8 23.5 33.1 \n11.9 31.8 13.7 10.8 15.9 16.2 13.7 \n7.9 \n12.0 22.0 17.3 24.2 \n\nCore* [66] \n16.80 \n22.5 20.3 29.8 \n11.0 29.2 12.3 10.2 14.4 14.8 12.4 \n7.7 \n10.6 20.4 15.3 21.4 \n\nOurs \n15.70 \n20.1 18.4 26.2 10.8 28.9 12.1 10.2 13.7 13.9 11.1 7.6 \n8.8 20.2 14.2 19.4 \n\nResNet-50 \n[21] \n\nSource \n29.14 \n48.7 44.0 57.0 \n11.8 50.8 23.4 10.8 21.9 28.2 29.4 \n7.0 13.13 23.4 47.9 19.5 \n\nSHOT [40] \n16.19 \n20.0 18.8 29.6 \n9.9 \n27.1 15.0 \n8.5 \n15.4 14.5 19.8 \n7.3 \n8.5 \n18.7 15.8 14.0 \n\nTFA [41] \n15.97 \n18.8 17.9 29.2 \n9.8 \n27.3 14.6 \n8.0 \n16.0 14.0 20.3 \n7.8 \n8.6 \n19.4 14.1 13.9 \n\nTTT++ [41] 15.82 \n18.0 17.1 30.8 \n10.4 29.9 13.0 \n9.9 \n14.8 14.1 15.8 \n7.0 \n7.8 \n19.3 12.7 16.4 \n\nTENT [62] \n14.02 \n16.0 14.5 24.7 \n9.1 \n23.5 12.6 \n7.6 \n14.3 13.1 16.8 \n8.2 \n8.0 \n18.1 10.8 13.3 \n\nOurs \n12.52 \n14.1 13.4 20.9 \n8.3 20.7 11.2 \n7.3 12.4 11.7 14.4 7.3 \n7.4 16.5 9.7 12.4 \n\n(c) Comparison of average error (%) on CIFAR-100-C with all severity levels \n\nMethods \nBackbone \nLv.5 \nLv.4 \nLv.3 \nLv.2 \nLv.1 \nTENT [62] \nResNet-50 \n39.54 \n3.89 \u2193 \n36.09 \n3.27 \u2193 \n33.35 \n2.81 \u2193 \n31.30 \n2.38 \u2193 \n29.62 \n2.11 \u2193 \nOurs \n35.65 \n32.82 \n30.54 \n28.92 \n27.51 \nTENT [62] WRN-40-2 \n35.53 \n2.82 \u2193 \n32.89 \n2.40 \u2193 \n30.72 \n1.87 \u2193 \n29.06 \n1.53 \u2193 \n27.67 \n1.29 \u2193 \nOurs \n32.71 \n30.49 \n28.85 \n27.53 \n26.38 \n\nof the model, outperforms the other backbone networks, and our method further \nenhances the performance by complementing it. Table 1(c) shows the results \non CIFAR-100-C with all severity levels. Because severity denotes the strength \nof the corruption, it shows how much the distribution shift presents, and our \nmethod outperforms TENT [62] at all levels with a large margin. \n\n\n\nTable 2 :\n2Ablation study on CIFAR-100-C. ResNet-50 is used.Methods \nLearning rate Average err (%) \n\nMain \n1e-3 \n75.70 \n\nMain (optimal learning rate) \n1e-4 \n39.44 \n\nMain + NSP L aux ent \n\n\u03b8e \n\n+ L aux sel \n\n\u03b8e \n\n1e-4 \n37.55 \n\nMain + SWR \n1e-3 \n37.76 \n\nMain + SWR + NSP L aux ent \n\n\u03b8e \n\n1e-3 \n35.80 \n\nMain + SWR + NSP L aux ent \n\n\u03b8e \n\n+ L aux sel \n\n\u03b8e \n\n1e-3 \n35.65 \n\n\n\nTable 3 :\n3Comparison of error rate (%) according to changes in projector depth.Datasets \nBackbone \nProjector depth \n\nNone \n1 \n2 \n3 \n\nCIFAR-100-C \nWRN-40-2 \n33.04 \n32.79 32.71 32.89 \n\nResNet-50 \n36.34 35.43 35.65 36.81 \n\nCIFAR-10-C \n\nWRN-40-2 10.37 10.52 \n10.42 10.46 \n\nWRN-28-10 15.70 16.45 \n16.09 16.39 \n\nResNet-50 \n12.52 12.91 \n12.95 12.87 \n\n\n\nTable 4 :\n4Hyper-parameter impacts on CIFAR-100-C. ResNet-50 is used.(a) \n\nWidth Error (%) \n128 \n36.34 \n256 \n35.85 \n512 \n35.65 \n1024 \n36.09 \n\n(b) \n\nTransformation Error (%) \n\nNo transform \n37.88 \n\nColor distortion \n35.79 \n\n+Crop. & Blur. \n35.65 \n\n(c) \n\nBackbone \n\nError (%) \n\nWRN-40-2 ResNet-50 \n\nFreeze. \n32.71 \n35.65 \n\nFinetune. \n32.85 \n35.96 \n\n\n\nTable 5 :\n5Comparison of error rate (%) on CIFAR-100-C. Our method outper-\nforms the supervised method in an online setting. LR denotes a learning rate. \n\nMethods \nGT \nLabel \n\nOptimal \nLR \n\nEpoch \n\n1 (online) 2 (offline) 3 (offline) \n\nEntropy Minimization \nNo \n1e-4 \n39.81 \n38.84 \n39.08 \n\nCross Entropy (Supervised) \nYes \n2e-4 \n38.27 \n7.41 \n0.86 \n\nOurs \nNo \n1e-3 \n35.65 \n33.34 \n33.25 \n\n\n\nTable 6 :\n6Comparison of accuracy (%) on four DG benchmarks. \u2020 denotes the reported results from DomainBed [18], and the others are reproduced values.Methods \nVLCS \nPACS \nOfficeHome \nTerra \nAverage \n\nERM  \u2020 \n76.8\u00b11.0 \n83.3\u00b10.6 \n67.3\u00b10.3 \n46.2\u00b10.2 \n68.4 \n\nCORAL  \u2020 \n77.0\u00b10.5 \n83.6\u00b10.6 \n68.6\u00b10.2 \n48.1\u00b11.3 \n69.3 \n\nERM \n77.4\u00b10.9 \n83.5\u00b10.7 \n65.6\u00b10.4 \n47.1\u00b11.1 \n68.4 \n+T3A \n79.4\u00b10.4 \n86.5\u00b10.3 \n67.8\u00b10.5 \n45.6\u00b10.7 \n69.8 \n+Ours \n77.0\u00b10.5 \n88.9\u00b10.1 \n69.2\u00b10.1 \n49.5\u00b10.8 \n71.2 \n\nCORAL \n77.9\u00b10.9 \n85.3\u00b10.1 \n67.8\u00b10.3 \n44.1\u00b10.4 \n68.8 \n+T3A \n79.3\u00b10.3 \n86.3\u00b10.2 \n69.5\u00b10.2 \n45.4\u00b11.2 \n70.1 \n+Ours \n78.7\u00b10.4 \n89.9\u00b10.1 \n71.0\u00b10.0 \n47.5\u00b10.6 \n71.8 \n\n\n\nTable 7 :\n7Comparison of error rate (%) according to the changes in importance of each loss term. Gray-colored cells denote the default value. H(p) \u2193 denotes entropy minimization, and H(p) \u2191 indicates mean entropy maximization.H(p) \u2193 \nH(p) \u2191 \nSSL \nReg. \nErr. \n\u03bbm 1 \n\u03bba 1 \n\u03bbm 2 \n\u03bba 2 \n\u03bbs \n\u03bbr \n\n0.2 \n0.8 \n0.25 \n0.25 \n0.1 \n250 \n35.65 \n0.5 \n0.5 \n0.25 \n0.25 \n0.1 \n250 \n36.05 \n0.8 \n0.2 \n0.25 \n0.25 \n0.1 \n250 \n36.73 \n\n0.2 \n0.8 \n0.1 \n0.1 \n0.1 \n250 \n35.85 \n0.2 \n0.8 \n0.25 \n0.25 \n0.1 \n250 \n35.65 \n0.2 \n0.8 \n0.5 \n0.5 \n0.1 \n250 \n35.92 \n\n0.2 \n0.8 \n0.25 \n0.25 \n0.01 \n250 \n35.79 \n0.2 \n0.8 \n0.25 \n0.25 \n0.1 \n250 \n35.65 \n0.2 \n0.8 \n0.25 \n0.25 \n0.5 \n250 \n36.13 \n\n0.2 \n0.8 \n0.25 \n0.25 \n0.1 \n10 \n53.01 \n0.2 \n0.8 \n0.25 \n0.25 \n0.1 \n100 \n36.78 \n0.2 \n0.8 \n0.25 \n0.25 \n0.1 \n250 \n35.65 \n0.2 \n0.8 \n0.25 \n0.25 \n0.1 \n500 \n36.29 \n0.2 \n0.8 \n0.25 \n0.25 \n0.1 \n1000 \n37.43 \n\n\n\nTable 8 :\n8Comparison of error rate (%) according to the combination of trans-\nform functions. We use the following transformations in Pytorch: ColorJitter \n(Color), RandomGrayscale (Gray), RandomInvert (Inve.), GaussianBlur (Blur), \nRandomHorizontalFlip (H.Fli.), and RandomResizedCrop (Crop.). \n\nDatasets \nBackbone \nColor +Gray. +Inve. +Blur. +H.Fli. +Crop. \n\nCIFAR-100-C \nWRN-40-2 [68,23] 33.23 \n33.04 \n32.99 \n32.71 \n32.79 \n33.31 \n\nResNet-50 [21] \n37.61 \n36.58 \n36.15 \n35.65 \n35.70 \n35.94 \n\nCIFAR-10-C \n\nWRN-40-2 \n10.97 \n10.76 \n10.63 \n10.37 \n10.31 \n10.68 \n\nWRN-28-10 [68] \n16.73 \n16.48 \n16.24 \n15.70 \n15.73 \n25.61 \n\nResNet-50 [21] \n12.47 \n12.38 \n12.76 \n12.52 \n12.55 \n11.81 \n\n\n\nTable 9 :\n9Comparison of error rate (%) between the main and NSP classifiers.Datasets \nBackbone \nClassifier \n\nMain \nNSP \n\nCIFAR-100-C \nWRN-40-2 \n32.71 \n35.48 \n\nResNet-50 \n35.65 \n36.52 \n\nCIFAR-10-C \n\nWRN-40-2 \n10.37 \n10.41 \n\nWRN-28-10 \n15.70 \n15.75 \n\nResNet-50 \n12.52 \n12.90 \n\n\n\nTable 10 :\n10Comparison of error rate (%) according to the methods of obtaining the source prototypes. Source domain \u2715 B.5 Performance of Nearest Source Prototype ClassifierDatasets \nBackbone \nSource prototypes \n\nz \nh \n\u03b8c \n\nCIFAR-100-C \nWRN-40-2 \n32.71 \n33.04 \n34.48 \n\nResNet-50 \n35.65 \n36.34 \n39.89 \n\nCIFAR-10-C \n\nWRN-40-2 \n10.42 \n10.37 \n11.62 \n\nWRN-28-10 \n16.09 \n15.70 \n15.67 \n\nResNet-50 \n12.95 \n12.52 \n14.74 \n\nProjector \nEncoder \n\nSource domain \n\nEncoder \n\nSource domain \n\nClassifer \nEncoder \n\n\n\nTable 11 :\n11Comparison of error rate (%) between updating \u03b8 * and freezing \u03b8 * in shift-agnostic regularization term.B.7 Freezing \u03b8 * as source model parameters in SWRRecall shift-agnostic weight regularization (SWR) term in Eq. (1). We update \u03b8 * with the model parameters from the previous update step during the optimization trajectory.Datasets \nBackbone Updating \u03b8  *  Freezing \u03b8  *  \n\nCIFAR-100-C \nWRN-40-2 \n32.71 \n33.49 \n\nResNet-50 \n35.65 \n37.00 \n\nCIFAR-10-C \n\nWRN-40-2 \n10.37 \n10.87 \n\nWRN-28-10 \n15.70 \n18.30 \n\nResNet-50 \n12.52 \n13.02 \n\n\n\nTable 12 :\n12Comparison with other methods on ImageNet-C [22] dataset. These results are reproduced in our environment. Source denotes the source pre-trained model without test-time adaptation. (a) Comparison of error rate (%) on ImageNet-C with severity level 5 Backbone Methods Avg. err Gaus. Shot Impu. Defo. Glas. Moti. Zoom Snow Fros. Fog Brig. Cont. Elas. Pixe. Jpeg (b) Comparison of error rate (%) on ImageNet-C with all severity levelsWRN-40-2 \n(AugMix) \n[68,23] \n\nSource \n74.32 \n89.8 84.9 89.3 \n78.7 86.5 75.5 66.6 78.3 72.8 77.1 42.4 87.0 74.5 58.3 53.2 \n\nTENT [62] 51.17 \n66.8 63.2 63.6 \n64.3 65.1 47.8 42.8 45.1 52.0 40.6 31.9 60.8 40.8 37.6 45.2 \n\nOurs \n48.01 \n59.8 56.5 58.4 61.1 62.8 44.6 41.5 41.6 47.8 38.5 30.2 58.2 39.8 36.3 43.1 \n\nResNet-50 \n[21] \n\nSource \n93.34 \n96.1 95.9 96.3 \n98.3 98.3 97.2 94.1 97.1 93.5 97.0 74.5 99.9 96.1 87.3 78.6 \n\nTENT [62] 66.56 \n84.5 81.9 79.6 \n86.3 88.7 69.5 55.2 56.7 69.0 46.8 35.4 98.1 48.4 45.4 53.0 \n\nOurs \n64.41 \n78.1 75.8 76.3 85.4 87.6 62.6 55.1 52.8 64.4 46.5 36.2 99.2 48.5 45.8 51.9 \n\nBackbone \nMethods \nAvg. err \nLv.5 \nLv.4 \nLv.3 \nLv.2 \nLv.1 \n\nWRN-40-2 \nTENT [62] \n39.27 \n1.88 \u2193 \n51.17 \n3.16 \u2193 \n42.94 \n2.11 \u2193 \n37.36 \n1.65 \u2193 \n34.35 \n1.33 \u2193 \n30.54 \n1.18 \u2193 \nOurs \n37.39 \n48.01 \n40.83 \n35.71 \n33.02 \n29.36 \n\nResNet-50 \nTENT [62] \n47.93 \n0.54 \u2193 \n66.56 \n2.15 \u2193 \n54.06 \n2.05 \u2193 \n45.04 \n0.24 \u2193 \n39.82 0.57 \u2191 \n34.15 1.18 \u2191 \nOurs \n47.39 \n64.41 \n52.01 \n44.80 \n40.39 \n35.33 \n\n\nTable 13 :\n13Test-time adaptation (TTA) performance using SWR on Cityscapes-C dataset. DG denotes domain generalization.Models (Cityscapes\u2192Cityscapes-C) mIoU \n\nDeepLabV3+ [6] (ResNet-50) \n27.3% \n+TTA (Main+SWR) \n49.8% (\u2191 22.5%) \n\nRobustNet [9] (ResNet-50+DG) \n44.4% \n+TTA (Main+SWR) \n55.2% (\u2191 10.8%) \n\n\n\nTable 14 :\n14Comparison of performance with and without SWR according to the learning rate change. Test-time adaptation with the proposed SWR shows superior performance and less sensitivity to changes in the learning rate. LR denotes a learning rate.Models (Cityscapes\u2192Cityscapes-C) LR: 1e-4 LR: 1e-5 LR: 1e-6 \n\nRobustNet [9]+TTA (Main) \n7.3% \n28.5% \n53.1% \n\nRobustNet [9]+TTA (Main+SWR) \n53.7% \n55.2% \n54.0% \n\n\nrefers to the ultimate objective of the model (e.g., classification).\ndenotes a part divided into torch.nn.Module units defined in Pytorch. The gradient vector of each layer can be easily obtained using torch.nn.module.parameters().\nExperiments on ImageNet-C are in the supplementary Section B.\nhttps://github.com/kekmodel/FixMatch-pytorch/blob/master/train.py\nhttps://github.com/facebookresearch/DomainBed 4 https://github.com/matsuolab/T3A\nAcknowledgement We would like to thank Kyuwoong Hwang, Simyung Chang, Hyunsin Park, Juntae Lee, Janghoon Cho, Hyoungwoo Park, Byeonggeun Kim, and Hyesu Lim of the Qualcomm AI Research team for their valuable discussions.\nUnsupervised robust domain adaptation without source data. P Agarwal, D P Paudel, J N Zaech, L Van Gool, IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) (2022). 4Agarwal, P., Paudel, D.P., Zaech, J.N., Van Gool, L.: Unsupervised robust domain adaptation without source data. In: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) (2022) 4\n\nSemi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. M Assran, M Caron, I Misra, P Bojanowski, A Joulin, N Ballas, M Rabbat, International Conference on Computer Vision (ICCV). 6Assran, M., Caron, M., Misra, I., Bojanowski, P., Joulin, A., Ballas, N., Rabbat, M.: Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. In: International Conference on Computer Vision (ICCV) (2021) 6\n\nMetareg: Towards domain generalization using meta-regularization. Y Balaji, S Sankaranarayanan, R Chellappa, Advances in Neural Information Processing Systems. 4Balaji, Y., Sankaranarayanan, S., Chellappa, R.: Metareg: Towards domain gener- alization using meta-regularization. In: Advances in Neural Information Processing Systems (NeurIPS) (2018) 4\n\nVicreg: Variance-invariance-covariance regularization for self-supervised learning. A Bardes, J Ponce, Y Lecun, arXiv:2105.049068arXiv preprintBardes, A., Ponce, J., LeCun, Y.: Vicreg: Variance-invariance-covariance regular- ization for self-supervised learning. arXiv preprint arXiv:2105.04906 (2021) 8\n\nRecognition in terra incognita. S Beery, G Van Horn, P Perona, European Conference on Computer Vision (ECCV). 328Beery, S., Van Horn, G., Perona, P.: Recognition in terra incognita. In: European Conference on Computer Vision (ECCV) (2018) 3, 9, 28\n\nEncoder-decoder with atrous separable convolution for semantic image segmentation. L C Chen, Y Zhu, G Papandreou, F Schroff, H Adam, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2627Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Proceedings of the European conference on computer vision (ECCV). pp. 801-818 (2018) 26, 27\n\nA simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, International Conference on Machine Learning (ICML). 2Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con- trastive learning of visual representations. In: International Conference on Machine Learning (ICML) (2020) 2, 8\n\nMeta batch-instance normalization for generalizable person re-identification. S Choi, T Kim, M Jeong, H Park, C Kim, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 4Choi, S., Kim, T., Jeong, M., Park, H., Kim, C.: Meta batch-instance normalization for generalizable person re-identification. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 4\n\nRobustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. S Choi, S Jung, H Yun, J T Kim, S Kim, J Choo, IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 4. 2627Choi, S., Jung, S., Yun, H., Kim, J.T., Kim, S., Choo, J.: Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whiten- ing. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 4, 26, 27\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR2627Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 19, 26, 27\n\nF Croce, M Andriushchenko, V Sehwag, E Debenedetti, N Flammarion, M Chiang, P Mittal, M Hein, arXiv:2010.09670Robustbench: a standardized adversarial robustness benchmark. 10arXiv preprintCroce, F., Andriushchenko, M., Sehwag, V., Debenedetti, E., Flammarion, N., Chi- ang, M., Mittal, P., Hein, M.: Robustbench: a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670 (2020) 10\n\nUnbiased metric learning: On the utilization of multiple datasets and web images for softening bias. C Fang, Y Xu, D N Rockmore, International Conference on Computer Vision (ICCV. 328Fang, C., Xu, Y., Rockmore, D.N.: Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In: International Conference on Computer Vision (ICCV) (2013) 3, 9, 28\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, International Conference on Machine Learning (ICML). 4Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation. In: International Conference on Machine Learning (ICML) (2015) 4\n\nDomain-adversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, The journal of machine learning research. 4Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V.: Domain-adversarial training of neural networks. The journal of machine learning research (2016) 4\n\nUnsupervised representation learning by predicting image rotations. S Gidaris, P Singh, N Komodakis, International Conference on Learning Representations (ICLR). Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by pre- dicting image rotations. In: International Conference on Learning Representations (ICLR) (2018) 2\n\nDlow: Domain flow for adaptation and generalization. R Gong, W Li, Y Chen, L V Gool, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 4Gong, R., Li, W., Chen, Y., Gool, L.V.: Dlow: Domain flow for adaptation and generalization. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 4\n\nUnsupervised and semi-supervised learning with categorical generative adversarial networks. J T Springenberg, International Conference on Learning Representations (ICLR). 6Springenberg, J.T.: Unsupervised and semi-supervised learning with categorical generative adversarial networks. International Conference on Learning Represen- tations (ICLR) (2016) 6\n\nWhen does self-supervision improve few-shot learning?. J C Su, S Maji, B Hariharan, European Conference on Computer Vision (ECCV. 3Su, J.C., Maji, S., Hariharan, B.: When does self-supervision improve few-shot learning? In: European Conference on Computer Vision (ECCV) (2020) 3\n\nDeep coral: Correlation alignment for deep domain adaptation. B Sun, K Saenko, European Conference on Computer Vision (ECCV) (2016) 3, 4. 1328Sun, B., Saenko, K.: Deep coral: Correlation alignment for deep domain adaptation. In: European Conference on Computer Vision (ECCV) (2016) 3, 4, 13, 28\n\nTest-time training with self-supervision for generalization under distribution shifts. Y Sun, X Wang, Z Liu, J Miller, A Efros, M Hardt, International Conference on Machine Learning (ICML) (2020) 1. 24Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., Hardt, M.: Test-time training with self-supervision for generalization under distribution shifts. In: International Conference on Machine Learning (ICML) (2020) 1, 2, 3, 4\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. 4Tzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial discriminative domain adaptation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) 4\n\nAn overview of statistical learning theory. V N Vapnik, 1328Vapnik, V.N.: An overview of statistical learning theory. IEEE transactions on neural networks (1999) 13, 28\n\nDeep hashing network for unsupervised domain adaptation. H Venkateswara, J Eusebio, S Chakraborty, S Panchanathan, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. 328Venkateswara, H., Eusebio, J., Chakraborty, S., Panchanathan, S.: Deep hashing network for unsupervised domain adaptation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) 3, 9, 28\n\nGeneralizing to unseen domains via adversarial data augmentation. R Volpi, H Namkoong, O Sener, J C Duchi, V Murino, S Savarese, Advances in neural information processing systems. 314Volpi, R., Namkoong, H., Sener, O., Duchi, J.C., Murino, V., Savarese, S.: Gener- alizing to unseen domains via adversarial data augmentation. Advances in neural information processing systems 31 (2018) 4\n\nAdvent: Adversarial entropy minimization for domain adaptation in semantic segmentation. T H Vu, H Jain, M Bucher, M Cord, P P\u00e9rez, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. 26Vu, T.H., Jain, H., Bucher, M., Cord, M., P\u00e9rez, P.: Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In: IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR) (2019) 2, 4, 6\n\nD Wang, S Liu, S Ebrahimi, E Shelhamer, T Darrell, arXiv:2109.01087On-target adaptation. 46arXiv preprintWang, D., Liu, S., Ebrahimi, S., Shelhamer, E., Darrell, T.: On-target adaptation. arXiv preprint arXiv:2109.01087 (2021) 4, 6\n\nTent: Fully testtime adaptation by entropy minimization. D Wang, E Shelhamer, S Liu, B Olshausen, T Darrell, International Conference on Learning Representations (ICLR. 1025Wang, D., Shelhamer, E., Liu, S., Olshausen, B., Darrell, T.: Tent: Fully test- time adaptation by entropy minimization. In: International Conference on Learning Representations (ICLR) (2020) 1, 2, 3, 4, 6, 9, 10, 11, 25\n\nGeneralized sourcefree domain adaptation. S Yang, Y Wang, J Van De Weijer, L Herranz, S Jui, International Conference on Computer Vision (ICCV) (2021). 4Yang, S., Wang, Y., van de Weijer, J., Herranz, L., Jui, S.: Generalized source- free domain adaptation. In: International Conference on Computer Vision (ICCV) (2021) 4\n\nSofa: Source-data-free feature alignment for unsupervised domain adaptation. H W Yeh, B Yang, P C Yuen, T Harada, IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 4Yeh, H.W., Yang, B., Yuen, P.C., Harada, T.: Sofa: Source-data-free feature align- ment for unsupervised domain adaptation. In: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) (2021) 4\n\nHow transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, Advances in Neural Information Processing Systems (NeurIPS). Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in deep neural networks? Advances in Neural Information Processing Systems (NeurIPS) (2014) 2\n\nF You, J Li, Z Zhao, arXiv:2110.04065Test-time batch statistics calibration for covariate shift. 111arXiv preprintYou, F., Li, J., Zhao, Z.: Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065 (2021) 1, 2, 4, 11\n\nGradient surgery for multi-task learning. T Yu, S Kumar, A Gupta, S Levine, K Hausman, C Finn, Advances in Neural Information Processing Systems (NeurIPS. 3Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., Finn, C.: Gradient surgery for multi-task learning. Advances in Neural Information Processing Sys- tems (NeurIPS) (2020) 3\n\nS Zagoruyko, N Komodakis, Wide residual networks. British Machine Vision Conference (BMVC). 1025Zagoruyko, S., Komodakis, N.: Wide residual networks. British Machine Vision Conference (BMVC) (2016) 10, 11, 22, 25\n\nAuxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. Y Zhang, S Borse, H Cai, F Porikli, IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 1Zhang, Y., Borse, S., Cai, H., Porikli, F.: Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) (2022) 1\n\nLearning to generate novel domains for domain generalization. K Zhou, Y Yang, T Hospedales, T Xiang, European Conference on Computer Vision (ECCV) (2020). 4Zhou, K., Yang, Y., Hospedales, T., Xiang, T.: Learning to generate novel domains for domain generalization. In: European Conference on Computer Vision (ECCV) (2020) 4\n", "annotations": {"author": "[{\"end\":167,\"start\":105},{\"end\":230,\"start\":168},{\"end\":295,\"start\":231},{\"end\":361,\"start\":296},{\"end\":391,\"start\":362}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":112},{\"end\":178,\"start\":170},{\"end\":243,\"start\":236},{\"end\":309,\"start\":301}]", "author_first_name": "[{\"end\":111,\"start\":105},{\"end\":169,\"start\":168},{\"end\":235,\"start\":231},{\"end\":300,\"start\":296},{\"end\":365,\"start\":362}]", "author_affiliation": "[{\"end\":166,\"start\":143},{\"end\":229,\"start\":206},{\"end\":294,\"start\":271},{\"end\":360,\"start\":337},{\"end\":390,\"start\":367}]", "title": "[{\"end\":102,\"start\":1},{\"end\":493,\"start\":392}]", "venue": null, "abstract": "[{\"end\":2571,\"start\":577}]", "bib_ref": "[{\"end\":2700,\"start\":2696},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2954,\"start\":2950},{\"end\":3319,\"start\":3315},{\"end\":3322,\"start\":3319},{\"end\":3325,\"start\":3322},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3328,\"start\":3325},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3331,\"start\":3328},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3613,\"start\":3609},{\"end\":3616,\"start\":3613},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3720,\"start\":3716},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3723,\"start\":3720},{\"end\":3726,\"start\":3723},{\"end\":3729,\"start\":3726},{\"end\":3762,\"start\":3758},{\"end\":3805,\"start\":3801},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3921,\"start\":3917},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4360,\"start\":4356},{\"end\":4363,\"start\":4360},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4465,\"start\":4461},{\"end\":4497,\"start\":4493},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4499,\"start\":4497},{\"end\":4918,\"start\":4914},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4921,\"start\":4918},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4924,\"start\":4921},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6162,\"start\":6158},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6812,\"start\":6808},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6999,\"start\":6995},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8044,\"start\":8040},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8055,\"start\":8051},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8079,\"start\":8076},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8261,\"start\":8257},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8264,\"start\":8261},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8267,\"start\":8264},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8270,\"start\":8267},{\"end\":8273,\"start\":8270},{\"end\":8276,\"start\":8273},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8279,\"start\":8276},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9334,\"start\":9330},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9416,\"start\":9412},{\"end\":9448,\"start\":9444},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9780,\"start\":9776},{\"end\":9783,\"start\":9780},{\"end\":9786,\"start\":9783},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9789,\"start\":9786},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9920,\"start\":9916},{\"end\":10054,\"start\":10050},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10315,\"start\":10311},{\"end\":10318,\"start\":10315},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10321,\"start\":10318},{\"end\":14222,\"start\":14218},{\"end\":14225,\"start\":14222},{\"end\":14227,\"start\":14225},{\"end\":16333,\"start\":16328},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17214,\"start\":17211},{\"end\":17217,\"start\":17214},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17219,\"start\":17217},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20143,\"start\":20139},{\"end\":20156,\"start\":20152},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20499,\"start\":20498},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20582,\"start\":20578},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20593,\"start\":20589},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20617,\"start\":20614},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20862,\"start\":20858},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20887,\"start\":20883},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20935,\"start\":20931},{\"end\":26218,\"start\":26214},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26335,\"start\":26331},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26349,\"start\":26345},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28533,\"start\":28529},{\"end\":28536,\"start\":28533},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34840,\"start\":34836},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":35153,\"start\":35149},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35471,\"start\":35468},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35489,\"start\":35486},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35739,\"start\":35735},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37200,\"start\":37196},{\"end\":37203,\"start\":37200},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37244,\"start\":37241},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37259,\"start\":37256},{\"end\":38581,\"start\":38575},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38694,\"start\":38690},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":38705,\"start\":38701},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38729,\"start\":38726},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38827,\"start\":38823},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":38842,\"start\":38838},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":52638,\"start\":52634}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":39855,\"start\":39613},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39969,\"start\":39856},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40131,\"start\":39970},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40211,\"start\":40132},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46735,\"start\":40212},{\"attributes\":{\"id\":\"fig_6\"},\"end\":46875,\"start\":46736},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47374,\"start\":46876},{\"attributes\":{\"id\":\"fig_9\"},\"end\":47827,\"start\":47375},{\"attributes\":{\"id\":\"fig_10\"},\"end\":48168,\"start\":47828},{\"attributes\":{\"id\":\"fig_11\"},\"end\":48781,\"start\":48169},{\"attributes\":{\"id\":\"fig_13\"},\"end\":49340,\"start\":48782},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":51136,\"start\":49341},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":52430,\"start\":51137},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56157,\"start\":52431},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":56526,\"start\":56158},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":56873,\"start\":56527},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":57222,\"start\":56874},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":57610,\"start\":57223},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":58240,\"start\":57611},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":59082,\"start\":58241},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":59762,\"start\":59083},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":60040,\"start\":59763},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":60539,\"start\":60041},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":61086,\"start\":60540},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":62514,\"start\":61087},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":62818,\"start\":62515},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":63231,\"start\":62819}]", "paragraph": "[{\"end\":3122,\"start\":2580},{\"end\":5103,\"start\":3124},{\"end\":6245,\"start\":5105},{\"end\":7116,\"start\":6247},{\"end\":7184,\"start\":7118},{\"end\":8171,\"start\":7186},{\"end\":8942,\"start\":8220},{\"end\":10322,\"start\":8980},{\"end\":10732,\"start\":10342},{\"end\":11581,\"start\":10810},{\"end\":12133,\"start\":11622},{\"end\":12703,\"start\":12135},{\"end\":13895,\"start\":12814},{\"end\":14610,\"start\":13935},{\"end\":15046,\"start\":14690},{\"end\":15711,\"start\":15103},{\"end\":16565,\"start\":15713},{\"end\":16861,\"start\":16609},{\"end\":17561,\"start\":16924},{\"end\":18092,\"start\":17623},{\"end\":18286,\"start\":18094},{\"end\":18792,\"start\":18288},{\"end\":19046,\"start\":18841},{\"end\":19175,\"start\":19101},{\"end\":19341,\"start\":19227},{\"end\":19697,\"start\":19395},{\"end\":20101,\"start\":19713},{\"end\":20792,\"start\":20124},{\"end\":21932,\"start\":20794},{\"end\":25319,\"start\":21973},{\"end\":26074,\"start\":25406},{\"end\":26827,\"start\":26111},{\"end\":26925,\"start\":26917},{\"end\":27255,\"start\":26938},{\"end\":28696,\"start\":27301},{\"end\":29412,\"start\":28756},{\"end\":29863,\"start\":29414},{\"end\":30062,\"start\":29865},{\"end\":30882,\"start\":30111},{\"end\":31604,\"start\":30956},{\"end\":32402,\"start\":31641},{\"end\":34618,\"start\":32692},{\"end\":35157,\"start\":34733},{\"end\":36094,\"start\":35193},{\"end\":37061,\"start\":36096},{\"end\":37275,\"start\":37063},{\"end\":38433,\"start\":37344},{\"end\":39612,\"start\":38497}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10809,\"start\":10733},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12813,\"start\":12704},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14636,\"start\":14611},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14689,\"start\":14636},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16608,\"start\":16566},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16923,\"start\":16862},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17622,\"start\":17562},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18840,\"start\":18793},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19100,\"start\":19047},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19226,\"start\":19176},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19394,\"start\":19342},{\"attributes\":{\"id\":\"formula_11\"},\"end\":32638,\"start\":32403}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":6985,\"start\":6977},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":7009,\"start\":7000},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":21856,\"start\":21849},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22531,\"start\":22524},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22826,\"start\":22819},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23498,\"start\":23491},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":24884,\"start\":24877},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":25425,\"start\":25418},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":26532,\"start\":26525},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":29697,\"start\":29688},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30352,\"start\":30345},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30609,\"start\":30602},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":32266,\"start\":32259},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":33025,\"start\":33018},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":33306,\"start\":33299},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":33542,\"start\":33535},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33677,\"start\":33669},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34387,\"start\":34379},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35099,\"start\":35091},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36726,\"start\":36718},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":37060,\"start\":37052}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":8186,\"start\":8174},{\"attributes\":{\"n\":\"2.1\"},\"end\":8218,\"start\":8189},{\"attributes\":{\"n\":\"2.2\"},\"end\":8978,\"start\":8945},{\"attributes\":{\"n\":\"3\"},\"end\":10340,\"start\":10325},{\"attributes\":{\"n\":\"3.1\"},\"end\":11620,\"start\":11584},{\"attributes\":{\"n\":\"3.2\"},\"end\":13933,\"start\":13898},{\"attributes\":{\"n\":\"3.3\"},\"end\":15101,\"start\":15049},{\"attributes\":{\"n\":\"4\"},\"end\":19711,\"start\":19700},{\"attributes\":{\"n\":\"4.1\"},\"end\":20122,\"start\":20104},{\"attributes\":{\"n\":\"4.3\"},\"end\":21971,\"start\":21935},{\"attributes\":{\"n\":\"4.4\"},\"end\":25338,\"start\":25322},{\"attributes\":{\"n\":\"4.5\"},\"end\":25385,\"start\":25341},{\"attributes\":{\"n\":\"4.6\"},\"end\":25404,\"start\":25388},{\"attributes\":{\"n\":\"4.7\"},\"end\":26109,\"start\":26077},{\"attributes\":{\"n\":\"4.8\"},\"end\":26849,\"start\":26830},{\"end\":26878,\"start\":26852},{\"end\":26899,\"start\":26881},{\"end\":26915,\"start\":26902},{\"end\":26936,\"start\":26928},{\"end\":27285,\"start\":27258},{\"attributes\":{\"n\":\"5\"},\"end\":27299,\"start\":27288},{\"end\":28711,\"start\":28699},{\"end\":28754,\"start\":28714},{\"end\":30109,\"start\":30065},{\"end\":30906,\"start\":30885},{\"end\":30954,\"start\":30909},{\"end\":31639,\"start\":31607},{\"end\":32690,\"start\":32640},{\"end\":34675,\"start\":34621},{\"end\":34699,\"start\":34678},{\"end\":34731,\"start\":34702},{\"end\":35191,\"start\":35160},{\"end\":37310,\"start\":37278},{\"end\":37342,\"start\":37313},{\"end\":38495,\"start\":38436},{\"end\":39622,\"start\":39614},{\"end\":39865,\"start\":39857},{\"end\":39977,\"start\":39971},{\"end\":40141,\"start\":40133},{\"end\":46738,\"start\":46737},{\"end\":46885,\"start\":46877},{\"end\":47384,\"start\":47376},{\"end\":47835,\"start\":47829},{\"end\":48801,\"start\":48783},{\"end\":52441,\"start\":52432},{\"end\":56168,\"start\":56159},{\"end\":56537,\"start\":56528},{\"end\":56884,\"start\":56875},{\"end\":57233,\"start\":57224},{\"end\":57621,\"start\":57612},{\"end\":58251,\"start\":58242},{\"end\":59093,\"start\":59084},{\"end\":59773,\"start\":59764},{\"end\":60052,\"start\":60042},{\"end\":60551,\"start\":60541},{\"end\":61098,\"start\":61088},{\"end\":62526,\"start\":62516},{\"end\":62830,\"start\":62820}]", "table": "[{\"end\":51136,\"start\":49417},{\"end\":52430,\"start\":52401},{\"end\":56157,\"start\":53016},{\"end\":56526,\"start\":56219},{\"end\":56873,\"start\":56608},{\"end\":57222,\"start\":56944},{\"end\":57610,\"start\":57235},{\"end\":58240,\"start\":57762},{\"end\":59082,\"start\":58469},{\"end\":59762,\"start\":59095},{\"end\":60040,\"start\":59841},{\"end\":60539,\"start\":60215},{\"end\":61086,\"start\":60881},{\"end\":62514,\"start\":61532},{\"end\":62818,\"start\":62636},{\"end\":63231,\"start\":63070}]", "figure_caption": "[{\"end\":39855,\"start\":39624},{\"end\":39969,\"start\":39867},{\"end\":40131,\"start\":39979},{\"end\":40211,\"start\":40143},{\"end\":46735,\"start\":40214},{\"end\":46875,\"start\":46739},{\"end\":47374,\"start\":46887},{\"end\":47827,\"start\":47386},{\"end\":48168,\"start\":47837},{\"end\":48781,\"start\":48171},{\"end\":49340,\"start\":48806},{\"end\":49417,\"start\":49343},{\"end\":52401,\"start\":51139},{\"end\":53016,\"start\":52443},{\"end\":56219,\"start\":56170},{\"end\":56608,\"start\":56539},{\"end\":56944,\"start\":56886},{\"end\":57762,\"start\":57623},{\"end\":58469,\"start\":58253},{\"end\":59841,\"start\":59775},{\"end\":60215,\"start\":60055},{\"end\":60881,\"start\":60554},{\"end\":61532,\"start\":61101},{\"end\":62636,\"start\":62529},{\"end\":63070,\"start\":62833}]", "figure_ref": "[{\"end\":2579,\"start\":2573},{\"end\":3158,\"start\":3149},{\"end\":4192,\"start\":4183},{\"end\":5431,\"start\":5422},{\"end\":5577,\"start\":5568},{\"end\":6116,\"start\":6107},{\"end\":6173,\"start\":6163},{\"end\":6243,\"start\":6234},{\"end\":10483,\"start\":10477},{\"end\":13298,\"start\":13292},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13330,\"start\":13324},{\"end\":15848,\"start\":15842},{\"end\":17883,\"start\":17877},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19413,\"start\":19407},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19454,\"start\":19448},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19591,\"start\":19584},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20944,\"start\":20938},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20955,\"start\":20949},{\"end\":21072,\"start\":21066},{\"end\":25756,\"start\":25747},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30252,\"start\":30242},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30315,\"start\":30305},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31239,\"start\":31233},{\"end\":36737,\"start\":36731},{\"end\":37143,\"start\":37137},{\"end\":37914,\"start\":37907},{\"end\":38219,\"start\":38209},{\"end\":38374,\"start\":38364},{\"end\":38383,\"start\":38376}]", "bib_author_first_name": "[{\"end\":63955,\"start\":63954},{\"end\":63966,\"start\":63965},{\"end\":63968,\"start\":63967},{\"end\":63978,\"start\":63977},{\"end\":63980,\"start\":63979},{\"end\":63989,\"start\":63988},{\"end\":64391,\"start\":64390},{\"end\":64401,\"start\":64400},{\"end\":64410,\"start\":64409},{\"end\":64419,\"start\":64418},{\"end\":64433,\"start\":64432},{\"end\":64443,\"start\":64442},{\"end\":64453,\"start\":64452},{\"end\":64849,\"start\":64848},{\"end\":64859,\"start\":64858},{\"end\":64879,\"start\":64878},{\"end\":65219,\"start\":65218},{\"end\":65229,\"start\":65228},{\"end\":65238,\"start\":65237},{\"end\":65472,\"start\":65471},{\"end\":65481,\"start\":65480},{\"end\":65493,\"start\":65492},{\"end\":65772,\"start\":65771},{\"end\":65774,\"start\":65773},{\"end\":65782,\"start\":65781},{\"end\":65789,\"start\":65788},{\"end\":65803,\"start\":65802},{\"end\":65814,\"start\":65813},{\"end\":66252,\"start\":66251},{\"end\":66260,\"start\":66259},{\"end\":66273,\"start\":66272},{\"end\":66284,\"start\":66283},{\"end\":66618,\"start\":66617},{\"end\":66626,\"start\":66625},{\"end\":66633,\"start\":66632},{\"end\":66642,\"start\":66641},{\"end\":66650,\"start\":66649},{\"end\":67037,\"start\":67036},{\"end\":67045,\"start\":67044},{\"end\":67053,\"start\":67052},{\"end\":67060,\"start\":67059},{\"end\":67062,\"start\":67061},{\"end\":67069,\"start\":67068},{\"end\":67076,\"start\":67075},{\"end\":67481,\"start\":67480},{\"end\":67491,\"start\":67490},{\"end\":67500,\"start\":67499},{\"end\":67509,\"start\":67508},{\"end\":67520,\"start\":67519},{\"end\":67533,\"start\":67532},{\"end\":67545,\"start\":67544},{\"end\":67555,\"start\":67554},{\"end\":67563,\"start\":67562},{\"end\":68004,\"start\":68003},{\"end\":68013,\"start\":68012},{\"end\":68031,\"start\":68030},{\"end\":68041,\"start\":68040},{\"end\":68056,\"start\":68055},{\"end\":68070,\"start\":68069},{\"end\":68080,\"start\":68079},{\"end\":68090,\"start\":68089},{\"end\":68510,\"start\":68509},{\"end\":68518,\"start\":68517},{\"end\":68524,\"start\":68523},{\"end\":68526,\"start\":68525},{\"end\":68850,\"start\":68849},{\"end\":68859,\"start\":68858},{\"end\":69117,\"start\":69116},{\"end\":69126,\"start\":69125},{\"end\":69138,\"start\":69137},{\"end\":69148,\"start\":69147},{\"end\":69159,\"start\":69158},{\"end\":69173,\"start\":69172},{\"end\":69187,\"start\":69186},{\"end\":69199,\"start\":69198},{\"end\":69533,\"start\":69532},{\"end\":69544,\"start\":69543},{\"end\":69553,\"start\":69552},{\"end\":69863,\"start\":69862},{\"end\":69871,\"start\":69870},{\"end\":69877,\"start\":69876},{\"end\":69885,\"start\":69884},{\"end\":69887,\"start\":69886},{\"end\":70228,\"start\":70227},{\"end\":70230,\"start\":70229},{\"end\":70547,\"start\":70546},{\"end\":70549,\"start\":70548},{\"end\":70555,\"start\":70554},{\"end\":70563,\"start\":70562},{\"end\":70834,\"start\":70833},{\"end\":70841,\"start\":70840},{\"end\":71155,\"start\":71154},{\"end\":71162,\"start\":71161},{\"end\":71170,\"start\":71169},{\"end\":71177,\"start\":71176},{\"end\":71187,\"start\":71186},{\"end\":71196,\"start\":71195},{\"end\":71539,\"start\":71538},{\"end\":71548,\"start\":71547},{\"end\":71559,\"start\":71558},{\"end\":71569,\"start\":71568},{\"end\":71866,\"start\":71865},{\"end\":71868,\"start\":71867},{\"end\":72049,\"start\":72048},{\"end\":72065,\"start\":72064},{\"end\":72076,\"start\":72075},{\"end\":72091,\"start\":72090},{\"end\":72452,\"start\":72451},{\"end\":72461,\"start\":72460},{\"end\":72473,\"start\":72472},{\"end\":72482,\"start\":72481},{\"end\":72484,\"start\":72483},{\"end\":72493,\"start\":72492},{\"end\":72503,\"start\":72502},{\"end\":72864,\"start\":72863},{\"end\":72866,\"start\":72865},{\"end\":72872,\"start\":72871},{\"end\":72880,\"start\":72879},{\"end\":72890,\"start\":72889},{\"end\":72898,\"start\":72897},{\"end\":73205,\"start\":73204},{\"end\":73213,\"start\":73212},{\"end\":73220,\"start\":73219},{\"end\":73232,\"start\":73231},{\"end\":73245,\"start\":73244},{\"end\":73495,\"start\":73494},{\"end\":73503,\"start\":73502},{\"end\":73516,\"start\":73515},{\"end\":73523,\"start\":73522},{\"end\":73536,\"start\":73535},{\"end\":73875,\"start\":73874},{\"end\":73883,\"start\":73882},{\"end\":73891,\"start\":73890},{\"end\":73908,\"start\":73907},{\"end\":73919,\"start\":73918},{\"end\":74233,\"start\":74232},{\"end\":74235,\"start\":74234},{\"end\":74242,\"start\":74241},{\"end\":74250,\"start\":74249},{\"end\":74252,\"start\":74251},{\"end\":74260,\"start\":74259},{\"end\":74604,\"start\":74603},{\"end\":74616,\"start\":74615},{\"end\":74625,\"start\":74624},{\"end\":74635,\"start\":74634},{\"end\":74880,\"start\":74879},{\"end\":74887,\"start\":74886},{\"end\":74893,\"start\":74892},{\"end\":75175,\"start\":75174},{\"end\":75181,\"start\":75180},{\"end\":75190,\"start\":75189},{\"end\":75199,\"start\":75198},{\"end\":75209,\"start\":75208},{\"end\":75220,\"start\":75219},{\"end\":75468,\"start\":75467},{\"end\":75481,\"start\":75480},{\"end\":75789,\"start\":75788},{\"end\":75798,\"start\":75797},{\"end\":75807,\"start\":75806},{\"end\":75814,\"start\":75813},{\"end\":76192,\"start\":76191},{\"end\":76200,\"start\":76199},{\"end\":76208,\"start\":76207},{\"end\":76222,\"start\":76221}]", "bib_author_last_name": "[{\"end\":63963,\"start\":63956},{\"end\":63975,\"start\":63969},{\"end\":63986,\"start\":63981},{\"end\":63998,\"start\":63990},{\"end\":64398,\"start\":64392},{\"end\":64407,\"start\":64402},{\"end\":64416,\"start\":64411},{\"end\":64430,\"start\":64420},{\"end\":64440,\"start\":64434},{\"end\":64450,\"start\":64444},{\"end\":64460,\"start\":64454},{\"end\":64856,\"start\":64850},{\"end\":64876,\"start\":64860},{\"end\":64889,\"start\":64880},{\"end\":65226,\"start\":65220},{\"end\":65235,\"start\":65230},{\"end\":65244,\"start\":65239},{\"end\":65478,\"start\":65473},{\"end\":65490,\"start\":65482},{\"end\":65500,\"start\":65494},{\"end\":65779,\"start\":65775},{\"end\":65786,\"start\":65783},{\"end\":65800,\"start\":65790},{\"end\":65811,\"start\":65804},{\"end\":65819,\"start\":65815},{\"end\":66257,\"start\":66253},{\"end\":66270,\"start\":66261},{\"end\":66281,\"start\":66274},{\"end\":66291,\"start\":66285},{\"end\":66623,\"start\":66619},{\"end\":66630,\"start\":66627},{\"end\":66639,\"start\":66634},{\"end\":66647,\"start\":66643},{\"end\":66654,\"start\":66651},{\"end\":67042,\"start\":67038},{\"end\":67050,\"start\":67046},{\"end\":67057,\"start\":67054},{\"end\":67066,\"start\":67063},{\"end\":67073,\"start\":67070},{\"end\":67081,\"start\":67077},{\"end\":67488,\"start\":67482},{\"end\":67497,\"start\":67492},{\"end\":67506,\"start\":67501},{\"end\":67517,\"start\":67510},{\"end\":67530,\"start\":67521},{\"end\":67542,\"start\":67534},{\"end\":67552,\"start\":67546},{\"end\":67560,\"start\":67556},{\"end\":67571,\"start\":67564},{\"end\":68010,\"start\":68005},{\"end\":68028,\"start\":68014},{\"end\":68038,\"start\":68032},{\"end\":68053,\"start\":68042},{\"end\":68067,\"start\":68057},{\"end\":68077,\"start\":68071},{\"end\":68087,\"start\":68081},{\"end\":68095,\"start\":68091},{\"end\":68515,\"start\":68511},{\"end\":68521,\"start\":68519},{\"end\":68535,\"start\":68527},{\"end\":68856,\"start\":68851},{\"end\":68869,\"start\":68860},{\"end\":69123,\"start\":69118},{\"end\":69135,\"start\":69127},{\"end\":69145,\"start\":69139},{\"end\":69156,\"start\":69149},{\"end\":69170,\"start\":69160},{\"end\":69184,\"start\":69174},{\"end\":69196,\"start\":69188},{\"end\":69209,\"start\":69200},{\"end\":69541,\"start\":69534},{\"end\":69550,\"start\":69545},{\"end\":69563,\"start\":69554},{\"end\":69868,\"start\":69864},{\"end\":69874,\"start\":69872},{\"end\":69882,\"start\":69878},{\"end\":69892,\"start\":69888},{\"end\":70243,\"start\":70231},{\"end\":70552,\"start\":70550},{\"end\":70560,\"start\":70556},{\"end\":70573,\"start\":70564},{\"end\":70838,\"start\":70835},{\"end\":70848,\"start\":70842},{\"end\":71159,\"start\":71156},{\"end\":71167,\"start\":71163},{\"end\":71174,\"start\":71171},{\"end\":71184,\"start\":71178},{\"end\":71193,\"start\":71188},{\"end\":71202,\"start\":71197},{\"end\":71545,\"start\":71540},{\"end\":71556,\"start\":71549},{\"end\":71566,\"start\":71560},{\"end\":71577,\"start\":71570},{\"end\":71875,\"start\":71869},{\"end\":72062,\"start\":72050},{\"end\":72073,\"start\":72066},{\"end\":72088,\"start\":72077},{\"end\":72104,\"start\":72092},{\"end\":72458,\"start\":72453},{\"end\":72470,\"start\":72462},{\"end\":72479,\"start\":72474},{\"end\":72490,\"start\":72485},{\"end\":72500,\"start\":72494},{\"end\":72512,\"start\":72504},{\"end\":72869,\"start\":72867},{\"end\":72877,\"start\":72873},{\"end\":72887,\"start\":72881},{\"end\":72895,\"start\":72891},{\"end\":72904,\"start\":72899},{\"end\":73210,\"start\":73206},{\"end\":73217,\"start\":73214},{\"end\":73229,\"start\":73221},{\"end\":73242,\"start\":73233},{\"end\":73253,\"start\":73246},{\"end\":73500,\"start\":73496},{\"end\":73513,\"start\":73504},{\"end\":73520,\"start\":73517},{\"end\":73533,\"start\":73524},{\"end\":73544,\"start\":73537},{\"end\":73880,\"start\":73876},{\"end\":73888,\"start\":73884},{\"end\":73905,\"start\":73892},{\"end\":73916,\"start\":73909},{\"end\":73923,\"start\":73920},{\"end\":74239,\"start\":74236},{\"end\":74247,\"start\":74243},{\"end\":74257,\"start\":74253},{\"end\":74267,\"start\":74261},{\"end\":74613,\"start\":74605},{\"end\":74622,\"start\":74617},{\"end\":74632,\"start\":74626},{\"end\":74642,\"start\":74636},{\"end\":74884,\"start\":74881},{\"end\":74890,\"start\":74888},{\"end\":74898,\"start\":74894},{\"end\":75178,\"start\":75176},{\"end\":75187,\"start\":75182},{\"end\":75196,\"start\":75191},{\"end\":75206,\"start\":75200},{\"end\":75217,\"start\":75210},{\"end\":75225,\"start\":75221},{\"end\":75478,\"start\":75469},{\"end\":75491,\"start\":75482},{\"end\":75795,\"start\":75790},{\"end\":75804,\"start\":75799},{\"end\":75811,\"start\":75808},{\"end\":75822,\"start\":75815},{\"end\":76197,\"start\":76193},{\"end\":76205,\"start\":76201},{\"end\":76219,\"start\":76209},{\"end\":76228,\"start\":76223}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":232379985},\"end\":64272,\"start\":63895},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":233444047},\"end\":64780,\"start\":64274},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":53979606},\"end\":65132,\"start\":64782},{\"attributes\":{\"doi\":\"arXiv:2105.04906\",\"id\":\"b3\"},\"end\":65437,\"start\":65134},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49744838},\"end\":65686,\"start\":65439},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3638670},\"end\":66178,\"start\":65688},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211096730},\"end\":66537,\"start\":66180},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":227228813},\"end\":66929,\"start\":66539},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":232404762},\"end\":67415,\"start\":66931},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":502946},\"end\":68001,\"start\":67417},{\"attributes\":{\"doi\":\"arXiv:2010.09670\",\"id\":\"b10\"},\"end\":68406,\"start\":68003},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":722896},\"end\":68796,\"start\":68408},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6755881},\"end\":69066,\"start\":68798},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2871880},\"end\":69462,\"start\":69068},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4009713},\"end\":69807,\"start\":69464},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":55829476},\"end\":70133,\"start\":69809},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6230637},\"end\":70489,\"start\":70135},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":203902614},\"end\":70769,\"start\":70491},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":12453047},\"end\":71065,\"start\":70771},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220301705},\"end\":71490,\"start\":71067},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4357800},\"end\":71819,\"start\":71492},{\"attributes\":{\"id\":\"b21\"},\"end\":71989,\"start\":71821},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2928248},\"end\":72383,\"start\":71991},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":44178122},\"end\":72772,\"start\":72385},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":54216961},\"end\":73202,\"start\":72774},{\"attributes\":{\"doi\":\"arXiv:2109.01087\",\"id\":\"b25\"},\"end\":73435,\"start\":73204},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":232278031},\"end\":73830,\"start\":73437},{\"attributes\":{\"id\":\"b27\"},\"end\":74153,\"start\":73832},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":230104791},\"end\":74545,\"start\":74155},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":362467},\"end\":74877,\"start\":74547},{\"attributes\":{\"doi\":\"arXiv:2110.04065\",\"id\":\"b30\"},\"end\":75130,\"start\":74879},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":210839011},\"end\":75465,\"start\":75132},{\"attributes\":{\"id\":\"b32\"},\"end\":75679,\"start\":75467},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":239768699},\"end\":76127,\"start\":75681},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":220380867},\"end\":76452,\"start\":76129}]", "bib_title": "[{\"end\":63952,\"start\":63895},{\"end\":64388,\"start\":64274},{\"end\":64846,\"start\":64782},{\"end\":65469,\"start\":65439},{\"end\":65769,\"start\":65688},{\"end\":66249,\"start\":66180},{\"end\":66615,\"start\":66539},{\"end\":67034,\"start\":66931},{\"end\":67478,\"start\":67417},{\"end\":68507,\"start\":68408},{\"end\":68847,\"start\":68798},{\"end\":69114,\"start\":69068},{\"end\":69530,\"start\":69464},{\"end\":69860,\"start\":69809},{\"end\":70225,\"start\":70135},{\"end\":70544,\"start\":70491},{\"end\":70831,\"start\":70771},{\"end\":71152,\"start\":71067},{\"end\":71536,\"start\":71492},{\"end\":72046,\"start\":71991},{\"end\":72449,\"start\":72385},{\"end\":72861,\"start\":72774},{\"end\":73492,\"start\":73437},{\"end\":73872,\"start\":73832},{\"end\":74230,\"start\":74155},{\"end\":74601,\"start\":74547},{\"end\":75172,\"start\":75132},{\"end\":75786,\"start\":75681},{\"end\":76189,\"start\":76129}]", "bib_author": "[{\"end\":63965,\"start\":63954},{\"end\":63977,\"start\":63965},{\"end\":63988,\"start\":63977},{\"end\":64000,\"start\":63988},{\"end\":64400,\"start\":64390},{\"end\":64409,\"start\":64400},{\"end\":64418,\"start\":64409},{\"end\":64432,\"start\":64418},{\"end\":64442,\"start\":64432},{\"end\":64452,\"start\":64442},{\"end\":64462,\"start\":64452},{\"end\":64858,\"start\":64848},{\"end\":64878,\"start\":64858},{\"end\":64891,\"start\":64878},{\"end\":65228,\"start\":65218},{\"end\":65237,\"start\":65228},{\"end\":65246,\"start\":65237},{\"end\":65480,\"start\":65471},{\"end\":65492,\"start\":65480},{\"end\":65502,\"start\":65492},{\"end\":65781,\"start\":65771},{\"end\":65788,\"start\":65781},{\"end\":65802,\"start\":65788},{\"end\":65813,\"start\":65802},{\"end\":65821,\"start\":65813},{\"end\":66259,\"start\":66251},{\"end\":66272,\"start\":66259},{\"end\":66283,\"start\":66272},{\"end\":66293,\"start\":66283},{\"end\":66625,\"start\":66617},{\"end\":66632,\"start\":66625},{\"end\":66641,\"start\":66632},{\"end\":66649,\"start\":66641},{\"end\":66656,\"start\":66649},{\"end\":67044,\"start\":67036},{\"end\":67052,\"start\":67044},{\"end\":67059,\"start\":67052},{\"end\":67068,\"start\":67059},{\"end\":67075,\"start\":67068},{\"end\":67083,\"start\":67075},{\"end\":67490,\"start\":67480},{\"end\":67499,\"start\":67490},{\"end\":67508,\"start\":67499},{\"end\":67519,\"start\":67508},{\"end\":67532,\"start\":67519},{\"end\":67544,\"start\":67532},{\"end\":67554,\"start\":67544},{\"end\":67562,\"start\":67554},{\"end\":67573,\"start\":67562},{\"end\":68012,\"start\":68003},{\"end\":68030,\"start\":68012},{\"end\":68040,\"start\":68030},{\"end\":68055,\"start\":68040},{\"end\":68069,\"start\":68055},{\"end\":68079,\"start\":68069},{\"end\":68089,\"start\":68079},{\"end\":68097,\"start\":68089},{\"end\":68517,\"start\":68509},{\"end\":68523,\"start\":68517},{\"end\":68537,\"start\":68523},{\"end\":68858,\"start\":68849},{\"end\":68871,\"start\":68858},{\"end\":69125,\"start\":69116},{\"end\":69137,\"start\":69125},{\"end\":69147,\"start\":69137},{\"end\":69158,\"start\":69147},{\"end\":69172,\"start\":69158},{\"end\":69186,\"start\":69172},{\"end\":69198,\"start\":69186},{\"end\":69211,\"start\":69198},{\"end\":69543,\"start\":69532},{\"end\":69552,\"start\":69543},{\"end\":69565,\"start\":69552},{\"end\":69870,\"start\":69862},{\"end\":69876,\"start\":69870},{\"end\":69884,\"start\":69876},{\"end\":69894,\"start\":69884},{\"end\":70245,\"start\":70227},{\"end\":70554,\"start\":70546},{\"end\":70562,\"start\":70554},{\"end\":70575,\"start\":70562},{\"end\":70840,\"start\":70833},{\"end\":70850,\"start\":70840},{\"end\":71161,\"start\":71154},{\"end\":71169,\"start\":71161},{\"end\":71176,\"start\":71169},{\"end\":71186,\"start\":71176},{\"end\":71195,\"start\":71186},{\"end\":71204,\"start\":71195},{\"end\":71547,\"start\":71538},{\"end\":71558,\"start\":71547},{\"end\":71568,\"start\":71558},{\"end\":71579,\"start\":71568},{\"end\":71877,\"start\":71865},{\"end\":72064,\"start\":72048},{\"end\":72075,\"start\":72064},{\"end\":72090,\"start\":72075},{\"end\":72106,\"start\":72090},{\"end\":72460,\"start\":72451},{\"end\":72472,\"start\":72460},{\"end\":72481,\"start\":72472},{\"end\":72492,\"start\":72481},{\"end\":72502,\"start\":72492},{\"end\":72514,\"start\":72502},{\"end\":72871,\"start\":72863},{\"end\":72879,\"start\":72871},{\"end\":72889,\"start\":72879},{\"end\":72897,\"start\":72889},{\"end\":72906,\"start\":72897},{\"end\":73212,\"start\":73204},{\"end\":73219,\"start\":73212},{\"end\":73231,\"start\":73219},{\"end\":73244,\"start\":73231},{\"end\":73255,\"start\":73244},{\"end\":73502,\"start\":73494},{\"end\":73515,\"start\":73502},{\"end\":73522,\"start\":73515},{\"end\":73535,\"start\":73522},{\"end\":73546,\"start\":73535},{\"end\":73882,\"start\":73874},{\"end\":73890,\"start\":73882},{\"end\":73907,\"start\":73890},{\"end\":73918,\"start\":73907},{\"end\":73925,\"start\":73918},{\"end\":74241,\"start\":74232},{\"end\":74249,\"start\":74241},{\"end\":74259,\"start\":74249},{\"end\":74269,\"start\":74259},{\"end\":74615,\"start\":74603},{\"end\":74624,\"start\":74615},{\"end\":74634,\"start\":74624},{\"end\":74644,\"start\":74634},{\"end\":74886,\"start\":74879},{\"end\":74892,\"start\":74886},{\"end\":74900,\"start\":74892},{\"end\":75180,\"start\":75174},{\"end\":75189,\"start\":75180},{\"end\":75198,\"start\":75189},{\"end\":75208,\"start\":75198},{\"end\":75219,\"start\":75208},{\"end\":75227,\"start\":75219},{\"end\":75480,\"start\":75467},{\"end\":75493,\"start\":75480},{\"end\":75797,\"start\":75788},{\"end\":75806,\"start\":75797},{\"end\":75813,\"start\":75806},{\"end\":75824,\"start\":75813},{\"end\":76199,\"start\":76191},{\"end\":76207,\"start\":76199},{\"end\":76221,\"start\":76207},{\"end\":76230,\"start\":76221}]", "bib_venue": "[{\"end\":65936,\"start\":65887},{\"end\":67723,\"start\":67652},{\"end\":64075,\"start\":64000},{\"end\":64512,\"start\":64462},{\"end\":64940,\"start\":64891},{\"end\":65216,\"start\":65134},{\"end\":65547,\"start\":65502},{\"end\":65885,\"start\":65821},{\"end\":66344,\"start\":66293},{\"end\":66721,\"start\":66656},{\"end\":67157,\"start\":67083},{\"end\":67650,\"start\":67573},{\"end\":68173,\"start\":68113},{\"end\":68586,\"start\":68537},{\"end\":68922,\"start\":68871},{\"end\":69251,\"start\":69211},{\"end\":69624,\"start\":69565},{\"end\":69959,\"start\":69894},{\"end\":70304,\"start\":70245},{\"end\":70619,\"start\":70575},{\"end\":70907,\"start\":70850},{\"end\":71264,\"start\":71204},{\"end\":71643,\"start\":71579},{\"end\":71863,\"start\":71821},{\"end\":72170,\"start\":72106},{\"end\":72563,\"start\":72514},{\"end\":72970,\"start\":72906},{\"end\":73291,\"start\":73271},{\"end\":73604,\"start\":73546},{\"end\":73982,\"start\":73925},{\"end\":74337,\"start\":74269},{\"end\":74703,\"start\":74644},{\"end\":74974,\"start\":74916},{\"end\":75285,\"start\":75227},{\"end\":75557,\"start\":75493},{\"end\":75892,\"start\":75824},{\"end\":76282,\"start\":76230}]"}}}, "year": 2023, "month": 12, "day": 17}