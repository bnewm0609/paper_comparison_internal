{"id": 235377145, "updated": "2023-10-06 02:32:19.119", "metadata": {"title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training", "authors": "[{\"first\":\"Kimin\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Laura\",\"last\":\"Smith\",\"middle\":[]},{\"first\":\"Pieter\",\"last\":\"Abbeel\",\"middle\":[]}]", "venue": "ICML", "journal": "6152-6163", "publication_date": {"year": 2021, "month": 6, "day": 9}, "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.05091", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/LeeSA21", "doi": null}}, "content": {"source": {"pdf_hash": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.05091v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ea92d1b506ba93a15f906ce0b420664dd2d7a656", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/45f573f302dc7e77cbc5d1a74ccbac3564bbebc8.txt", "contents": "\nPEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training\n9 Jun 2021\n\nKimin Lee <kiminlee@berkeley.edu> \nUniversity of California\nBerkeley\n\nLaura Smith <smithlaura@berkeley.edu>. \nUniversity of California\nBerkeley\n\nPieter Abbeel \nUniversity of California\nBerkeley\n\n\nLaura Smith\n\nPEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training\n9 Jun 202184E6C7C6D06E522A2569F7AC0D76E448arXiv:2106.05091v1[cs.LG]\nConveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide.Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive.In this work, we aim to make this process more sample-and feedback-efficient.We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning.Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent.To enable off-policy learning, we relabel all the agent's past experience when its reward model changes.We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries.We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills.We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.\n\nIntroduction\n\nDeep reinforcement learning (RL) has emerged as a powerful method whereby agents learn complex behaviors on their own through trial and error (Kohl & Stone, 2004; Despite these costly measures, it may still be difficult to construct a suitable reward function due to reward exploitation.That is, RL algorithms often discover ways to achieve high returns by unexpected, unintended means.In general, there is nuance in how we might want agents to behave, such as obeying social norms, that are difficult to account for and communicate effectively through an engineered reward function (Amodei et al., 2016;Shah et al., 2019;Turner et al., 2020).A popular way to avoid reward engineering is through imitation learning, during which a learner distills information about its objectives or tries to directly follow an expert (Schaal, 1997;Ng et al., 2000;Abbeel & Ng, 2004;Argall et al., 2009).While imitation learning is a powerful tool, suitable demonstrations are often prohibitively expensive to obtain in practice (Calinon et al., 2009;Pastor et al., 2011;Akgun et al., 2012;Zhang et al., 2018).\n\nIn contrast, humans often learn fairly autonomously, relying on occasional external feedback from a teacher.Part of what makes a teacher effective is their ability to interactively guide students according to their progress, providing corrective or increasingly advanced instructions as needed.Such an interactive learning process is also alluring for artificial agents since the agent's behavior can naturally be tailored to one's preference (avoiding reward exploitation) without requiring extensive engineering.This approach is only feasible if the feedback is both practical for a human to provide and sufficiently high-bandwidth.As such, human-in-theloop (HiL) RL (Knox & Stone, 2009;Christiano et al., 2017;MacGlashan et al., 2017) has not yet been widely adopted.\n\n\n!\n\nFigure 1.Illustration of our method.First, the agent engages in unsupervised pre-training during which it is encouraged to visit a diverse set of states so its queries can provide more meaningful signal than on randomly collected experience (left).Then, a teacher provides preferences between two clips of behavior, and we learn a reward model based on them.The agent is updated to maximize the expected return under the model.We also relabel all its past experiences with this model to maximize their utilization to update the policy (right).\n\nIn this work, we aim to substantially reduce the amount of human effort required for HiL learning.To this end, we present PEBBLE: unsupervised PrE-training and preference-Based learning via relaBeLing Experience, a feedback-efficient RL algorithm by which learning is largely autonomous and supplemented by a practical number of binary labels (i.e.preferences) provided by a supervisor.Our method relies on two main, synergistic ingredients: unsupervised pre-training and off-policy learning (see Figure 1).For generality, we do not assume the agent is privy to rewards from its environment.Instead, we first allow the agent to explore using only intrinsic motivation (Oudeyer et al., 2007;Schmidhuber, 2010) to diversify its experience and produce coherent behaviors.Collecting a breadth of experiences enables the teacher to provide more meaningful feedback, as compared to feedback on data collected in an indeliberate manner.The supervisor then steps in to teach the agent by expressing their preferences between pairs of clips of the agent's behavior (Christiano et al., 2017).The agent distills this information into a reward model and uses RL to optimize this inferred reward function.\n\nLeveraging unsupervised pre-training increases the efficiency of the teacher's initial feedback; however, RL requires a large enough number of samples such that supervising the learning process is still quite expensive for humans.It is thus especially critical to enable off-policy algorithms that can reuse data to maximize the agent's, and thereby human's, efficiency.However, on-policy methods have typically been used thus far for HiL RL because of their ability to mitigate the effects of non-stationarity in reward caused by online learning.We show that by simply relabeling all of the agent's past experience every time the reward model is updated, we can make use and reuse of all the agent's collected experience to improve sample and feedback efficiency by a large margin.Source code and videos are available at https://sites.google.com/view/icml21pebble.\n\nWe summarize the main contributions of PEBBLE:\n\n\u2022 For the first time, we show that unsupervised pre-training and off-policy learning can significantly improve the sample-and feedback-efficiency of HiL RL.\n\n\u2022 PEBBLE outperforms prior preference-based RL baselines on complex locomotion and robotic manipulation tasks from DeepMind Control Suite (DMControl; Tassa et al. 2018;2020) and Meta-world (Yu et al., 2020).\n\n\u2022 We demonstrate that PEBBLE can learn behaviors for which a typical reward function is difficult to engineer very efficiently.\n\n\u2022 We also show that PEBBLE can avoid reward exploitation, leading to more desirable behaviors compared to an agent trained with respect to an engineered reward function.\n\n\nRelated Work\n\nLearning from human feedback.Several works have successfully utilized feedback from real humans to train agents where it is assumed that the feedback is available at all times (Pilarski et al., 2011;MacGlashan et al., 2017;Arumugam et al., 2019).Due to this high feedback frequency, these approaches are difficult to scale to more complex learning problems that require substantial agent experience.\n\nBetter suited to learning in complex domains is to learn a reward model so the agent can learn without a supervisor's perpetual presence.One simple yet effective direction in reward learning is to train a classifier that recognizes task success and use it as basis for a reward function (Pinto & Gupta, 2016;Levine et al., 2018;Fu et al., 2018;Xie et al., 2018).Positive examples may be designated or reinforced through human feedback (Zhang et al., 2019;Singh et al., 2019;Smith et al., 2020).Another promising direction has focused on simply training a reward model via regres-sion using unbounded real-valued feedback (Knox & Stone, 2009;Warnell et al., 2018), but this has been challenging to scale because it is difficult for humans to reliably provide a particular utility value for certain behaviors of the RL agent.\n\nMuch easier for humans is to make relative judgments, i.e., comparing behaviors as better or worse.Preference-based learning is thus an attractive alternative because the supervision is easy to provide yet information-rich (Akrour et al., 2011;Pilarski et al., 2011;Akrour et al., 2012;Wilson et al., 2012;Sugiyama et al., 2012;Wirth & F\u00fcrnkranz, 2013;Wirth et al., 2016;Sadigh et al., 2017;Biyik & Sadigh, 2018;Leike et al., 2018;Biyik et al., 2020).Christiano et al. (2017) scaled preference-based learning to utilize modern deep learning techniques-they learn a reward function, modeled with deep neural networks, that is consistent with the observed preferences and use it to optimize an agent using RL.They choose on-policy RL methods (Schulman et al., 2015;Mnih et al., 2016) since they are more robust to the non-stationarity in rewards caused by online learning.Although they demonstrate that preference-based learning provides a fairly efficient (requiring feedback on less than 1% of the agent's experience) means of distilling information from feedback, they rely on notoriously sampleinefficient on-policy RL, so a large burden can yet be placed on the human.Subsequent works have aimed to improve the efficiency of this method by introducing additional forms of feedback such as demonstrations (Ibarz et al., 2018) or non-binary rankings (Cao et al., 2020).Our proposed approach similarly focuses on developing a more sample-and feedback-efficient preference-based RL algorithm without adding any additional forms of supervision.Instead, we enable off-policy learning as well as utilize unsupervised pre-training to substantially improve efficiency.\n\nUnsupervised pre-training for RL.Unsupervised pretraining has been studied for extracting strong behavioral priors that can be utilized to solve downstream tasks efficiently in the context of RL (Daniel et al., 2016;Florensa et al., 2018;Achiam et al., 2018;Eysenbach et al., 2019;Sharma et al., 2020).Specifically, agents are encouraged to expand the boundary of seen states by maximizing various intrinsic rewards, such as prediction errors (Houthooft et al., 2016;Pathak et al., 2017;Burda et al., 2019), count-based state novelty (Bellemare et al., 2016;Tang et al., 2017;Ostrovski et al., 2017), mutual information (Eysenbach et al., 2019) and state entropy (Hazan et al., 2019;Lee et al., 2019;Hao & Pieter, 2021).Such unsupervised pre-training methods allow learning diverse behaviors without extrinsic rewards, effectively facilitating accelerated learning of downstream tasks.In this work, we show that unsupervised pre-training enables a teacher to provide more meaningful signal by showing them a diverse set of behaviors.\n\n\nPreliminaries\n\nReinforcement learning.We consider a standard RL framework where an agent interacts with an environment in discrete time.Formally, at each timestep t, the agent receives a state s t from the environment and chooses an action a t based on its policy \u03c0.The environment returns a reward r t and the agent transitions to the next state s t+1 .\n\nThe return R t = \u221e k=0 \u03b3 k r t+k is the discounted sum of rewards from timestep t with discount factor \u03b3 \u2208 [0, 1).RL then maximizes the expected return from each state s t .\n\nSoft Actor-Critic.SAC (Haarnoja et al., 2018) is an offpolicy actor-critic method based on the maximum entropy RL framework (Ziebart, 2010), which encourages exploration and greater robustness to noise by maximizing a weighted objective of the reward and the policy entropy.To update the parameters, SAC alternates between a soft policy evaluation and a soft policy improvement.At the soft policy evaluation step, a soft Q-function, which is modeled as a neural network with parameters \u03b8, is updated by minimizing the following soft Bellman residual:\nL SAC critic = E \u03c4t\u223cB Q \u03b8 (s t , a t ) \u2212 r t \u2212 \u03b3 V (s t+1 ) 2 , (1) with V (s t ) = E at\u223c\u03c0 \u03c6 Q\u03b8(s t , a t ) \u2212 \u03b1 log \u03c0 \u03c6 (a t |s t ) ,\nwhere \u03c4 t = (s t , a t , s t+1 , r t ) is a transition, B is a replay buffer, \u03b8 are the delayed parameters, and \u03b1 is a temperature parameter.At the soft policy improvement step, the policy \u03c0 \u03c6 is updated by minimizing the following objective:\nL SAC act = E st\u223cB,at\u223c\u03c0 \u03c6 \u03b1 log \u03c0 \u03c6 (a t |s t ) \u2212 Q \u03b8 (s t , a t ) . (2)\nSAC enjoys good sample-efficiency relative to its on-policy counterparts by reusing its past experiences.However, for the same reason, SAC is not robust to a non-stationary reward function.\n\nReward learning from preferences.We follow the basic framework for learning a reward function r \u03c8 from preferences in which the function is trained to be consistent with observed human feedback (Wilson et al., 2012;Christiano et al., 2017).In this framework, a segment \u03c3 is a sequence of observations and actions {s k , a k , ..., s k+H , a k+H }.We elicit preferences y for segments \u03c3 0 and \u03c3 1 , where y is a distribution indicating which segment a human prefers, i.e., y \u2208 {(0, 1), (1, 0), (0.5, 0.5)}.The judgment is recorded in a dataset D as a triple (\u03c3 0 , \u03c3 1 , y).By following the Bradley-Terry model (Bradley & Terry, 1952), we model a preference predictor using the reward function r \u03c8 as follows:\nP \u03c8 [\u03c3 1 \u03c3 0 ] = exp t r \u03c8 (s 1 t , a 1 t ) i\u2208{0,1} exp t r \u03c8 (s i t , a i t ) ,(3)\nwhere \u03c3 i \u03c3 j denotes the event that segment i is preferable to segment j. Intuitively, this can be interpreted as assuming the probability of preferring a segment depends exponentially on the sum over the segment of an underlying reward function.While r \u03c8 is not a binary classifier, learning r \u03c8 amounts to binary classification with labels y provided by a supervisor.Concretely, the reward function, modeled as a neural network with parameters \u03c8, is updated by minimizing the following loss:\nL Reward = \u2212 E (\u03c3 0 ,\u03c3 1 ,y)\u223cD y(0) log P \u03c8 [\u03c3 0 \u03c3 1 ] + y(1) log P \u03c8 [\u03c3 1 \u03c3 0 ] . (4)\n\nPEBBLE\n\nIn this section, we present PEBBLE: unsupervised PrEtraining and preference-Based learning via relaBeLing Experience, an off-policy actor-critic algorithm for HiL RL.Formally, we consider a policy \u03c0 \u03c6 , Q-function Q \u03b8 and reward function r \u03c8 , which are updated by the following processes (see Algorithm 2 for the full procedure):\n\n\u2022\n\nStep 0 (unsupervised pre-training): We pre-train the policy \u03c0 \u03c6 only using intrinsic motivation to explore and collect diverse experiences (see Section 4.1).\n\n\u2022\n\nStep 1 (reward learning): We learn a reward function r \u03c8 that can lead to the desired behavior by getting feedback from a teacher (see Section 4.2).\n\n\u2022\n\nStep 2 (agent learning): We update the policy \u03c0 \u03c6 and Q-function Q \u03b8 using an off-policy RL algorithm with relabeling to mitigate the effects of a non-stationary reward function r \u03c8 (see Section 4.3).\n\n\n\u2022 Repeat\n\nStep 1 and Step 2.\n\n\nAccelerating Learning via Unsupervised Pre-training\n\nIn our setting, we assume the agent is given feedback in the form of preferences between segments.In the beginning of training, though, a naive agent executes a random policy, which does not provide good state coverage nor coherent behaviors.The agent's queries are thus quite limited and likely difficult for human teachers to judge.As a result, it requires many samples (and thus queries) for these methods to show initial progress.Recent work has addressed this issue by means of providing demonstrations; however, this is not ideal since these are notoriously hard to procure (Ibarz et al., 2018).Instead, our insight is to produce informative queries at the start of training by utilizing unsupervised pretraining to collect diverse samples solely through intrinsic motivation (Oudeyer et al., 2007;Schmidhuber, 2010).\n\nSpecifically, we encourage our agent to visit a wider range of states by using the state entropy H(s) = Algorithm 1 EXPLORE: Unsupervised exploration 1: Initialize parameters of Q \u03b8 and \u03c0 \u03c6 and a replay buffer B \u2190 \u2205 2: for each iteration do 3: for each timestep t do 4:\n\nCollect st+1 by taking at \u223c \u03c0 \u03c6 (at|st) 5:\n\nCompute intrinsic reward r int t \u2190 r int (st) as in (5) 6:\n\nStore transitions B \u2190 B \u222a {(st, at, st+1, r int t )} 7:\n\nend for 8:\n\nfor each gradient step do 9:\n\nSample minibatch { sj, aj, sj+1, r int j } B j=1 \u223c B 10:\n\nOptimize L SAC critic in (1) and L SAC act in (2) with respect to \u03b8 and \u03c6 11:\n\nend for 12: end for 13: return B, \u03c0 \u03c6 \u2212E s\u223cp(s) [log p(s)] as an intrinsic reward (Hazan et al., 2019;Lee et al., 2019;Hao & Pieter, 2021;Seo et al., 2021).By updating the agent to maximize the sum of expected intrinsic rewards, it can efficiently explore an environment and learn how to generate diverse behaviors.However, this intrinsic reward is intractable to compute in most settings.To handle this issue, we employ the simplified version of particle-based entropy estimator (Beirlant et al., 1997;Singh et al., 2003) (see the supplementary material for more details):\nH(s) \u221d i log(||s i \u2212 s k i ||),\nwhere H denotes the particle-based entropy estimator and s k i is the k-th nearest neighbor (k-NN) of s i .This implies that maximizing the distance between a state and its nearest neighbor increases the overall state entropy.Inspired by this, we define the intrinsic reward of the current state s t as the distance between s t and its k-th nearest neighbor by following the idea of Hao & Pieter (2021) that treats each transition as a particle:\nr int (s t ) = log(||s t \u2212 s k t ||).(5)\nIn our experiments, we compute k-NN distances between a sample and all samples in the replay buffer and normalize the intrinsic reward by dividing it by a running estimate of the standard deviation.The full procedure of unsupervised pre-training is summarized in Algorithm 1.\n\n\nSelecting Informative Queries\n\nAs previously mentioned, we learn our reward function by modeling the probability that a teacher prefers one sampled segment over another as proportional to the exponentiated sum of rewards over the segment (see Section 3).Ideally, one should solicit preferences so as to maximize expected value of information (EVOI; Savage 1972): the improvement of an agent caused by optimizing with respect to the resulting reward model (Viappiani, 2012;Akrour et al., 2012).if iteration % K == 0 then 9:\n\nfor m in 1 . . .M do 10:\n\n(\u03c3 0 , \u03c3 1 ) \u223c SAMPLE() (see Section 4.2) 11:\n\nQuery instructor for y 12:\n\nStore preference D \u2190 D \u222a {(\u03c3 0 , \u03c3 1 , y)} 13:\n\nend for 14:\n\nfor each gradient step do 15:\n\nSample minibatch {(\u03c3 0 , \u03c3 1 , y)j} D j=1 \u223c D 16:\n\nOptimize L Reward in (4) with respect to \u03c8 17:\n\nend for 18:\n\nRelabel entire replay buffer B using r \u03c8 19:\n\nend if 20:\n\nfor each timestep t do 21:\n\nCollect st+1 by taking at \u223c \u03c0 \u03c6 (at|st) 22:\n\nStore transitions B \u2190 B \u222a {(st, at, st+1, r \u03c8 (st))} 23:\n\nend for 24:\n\nfor each gradient step do 25:\n\nSample random minibatch {(\u03c4j)} B j=1 \u223c B 26:\n\nOptimize L SAC critic in (1) and L SAC act in (2) with respect to \u03b8 and \u03c6, respectively 27: end for 28: end for Computing the EVOI is intractable since it involves taking an expectation over all possible trajectories induced by the updated policy.To handle this issue, several approximations have been explored by prior works to sample queries that are likely to change the reward model (Daniel et al., 2014;Christiano et al., 2017;Ibarz et al., 2018).In this work, we consider the sampling schemes employed by Christiano et al. (2017): (1) uniform sampling and (2) ensemble-based sampling, which selects pairs of segments with high variance across ensemble reward models.We explore an additional third method, entropy-based sampling, which seeks to disambiguate pairs of segments nearest the decision boundary.That is, we sample a large batch of segment pairs and select pairs that maximize H(P \u03c8 ).We evaluate the effects of these sampling methods in Section 5.\n\n\nUsing Off-policy RL with Non-Stationary Reward\n\nOnce we learn a reward function r \u03c8 , we can update the policy \u03c0 \u03c6 and Q-function Q \u03b8 using any RL algorithm.A caveat is that the reward function r \u03c8 may be non-stationary because we update it during training.Christiano et al. (2017) used on-policy RL algorithms, TRPO (Schulman et al., 2015) and A2C (Mnih et al., 2016), to address this issue.However, their poor sample-efficiency leads to poor feedbackefficiency of the overall HiL method.In this work, we use an off-policy RL algorithm, which provides for sample-efficient learning by reusing past experiences that are stored in the replay buffer.However, the learning process of off-policy RL algorithms can be unstable because previous experiences in the replay buffer are labeled with previous learned rewards.To handle this issue, we relabel all of the agent's past experience every time we update the reward model.We find that this simple technique stabilizes the learning process and provides large gains in performance (see Figure 5(a) for supporting results).The full procedure ofPEBBLE is summarized in Algorithm 2.\n\n\nExperiments\n\nWe design our experiments to investigate the following:\n\n1. How does PEBBLE compare to existing methods in terms of sample and feedback efficiency?\n\n2. What is the contribution of each of the proposed techniques in PEBBLE?\n\n3. Can PEBBLE learn novel behaviors for which a typical reward function is difficult to engineer?\n\n4. Can PEBBLE mitigate the effects of reward exploitation?\n\n\nSetups\n\nWe evaluate PEBBLE on several continuous control tasks involving locomotion and robotic manipulation from Deep-Mind Control Suite (DMControl; Tassa et al. 2018;2020) and Meta-world (Yu et al., 2020).In order to verify the efficacy of our method, we first focus on having an agent solve a range of tasks without being able to directly observe the We also run experiments with actual human trainers (the authors) to show the benefits of human-in-the-loop RL.First, we show that human trainers can teach novel behaviors (e.g., waving a leg), which are not defined in original benchmarks.Second, we show that agents trained with the handengineered rewards from benchmarks can perform the task in an undesirable way (i.e., the agent exploits a misspecified reward function), while agents trained using human feedback can perform the same task in the desired way.For all experiments, each trajectory segment is presented to the human as a 1 second video clip, and a maximum of one hour of human time is required.\n\nFor evaluation, we compare to Christiano et al. (2017), which is the current state-of-the-art approach using the same type of feedback.The primary differences in our method are (1) the introduction of unsupervised pre-training, (2) the accommodation of off-policy RL, and (3) entropy-based sampling.We re-implemented Christiano et al. ( 2017) using the state-of-the-art on-policy RL algorithm: PPO (Schulman et al., 2017).We use the same reward learning framework and ensemble disagreement-based sampling as they proposed.We refer to this baseline as Preference PPO.\n\nAs an upper bound, since we evaluate against the task reward function, we also compare to SAC (Haarnoja et al., 2018) and PPO using the same ground truth reward.For our method, we pre-train an agent for 10K timesteps and include these pre-training steps in all learning curves.We do not alter any hyperparameters of the original SAC algorithm and use an ensemble of three reward models.Unless stated otherwise, we use entropy-based sampling.More experimental details including model architectures, sampling schemes, and reward learning are in the supplementary material.\n\n\nBenchmark Tasks with Unobserved Rewards\n\nLocomotion tasks from DMControl.Figure 3 shows the learning curves of PEBBLE with 1400, 700 or 400 pieces of feedback1 and that of Preference PPO with 2100 or 1400 pieces of feedback on three complex environments: Cheetahrun, Walker-walk and Quadruped-walk.Note that we explicitly give preference PPO an advantage by providing it with more feedback.We find that given a budget of 1400 queries, PEBBLE (green) reaches the same performance as SAC (pink) while Preference PPO (purple) is unable to match PPO (black).That PEBBLE requires less feedback than Preference PPO to match its respective oracle performance corroborates that PEBBLE is indeed more feedbackefficient.These results demonstrate that PEBBLE can enable the agent to solve the tasks without directly observing the ground truth reward function.\n\nFor further analysis, we incorporated our pre-training with Preference PPO (red) and find that it improves performance for Quadruped and Walker.We emphasize that our insight of using pre-training is able to improve both methods in terms of feedback-efficiency and asymptotic performance, but PEBBLE is uniquely positioned to benefit as it is able to utilize unsupervised experience for policy learning.\n\nRobotic manipulation tasks from Meta-world.One application area in which HiL methods could have significant real-world impact is robotic manipulation, since learning often requires extensive engineering in the real world (Yahya et al., 2017;Schenck & Fox, 2017;Kormushev et al., 2010;Rusu et al., 2017;Akkaya et al., 2019;Peng et al., 2020).However, the common approach is to perform goalconditioned learning with classifiers (Singh et al., 2019),  which can only capture limited information about what goal states are, and not about how they can be achieved.To study how we can utilize preference-based learning to perform more complex skills, we also consider six tasks covering a range of fundamental robotic manipulation skills from Meta-world (see Figure 2).As shown in Figure 4, PEB-BLE matches the performance of SAC using the ground truth reward and outperforms Preference PPO, given comparable (and more) feedback, on every task.By demonstrating the applicability of PEBBLE to learning a variety of robotic manipulation tasks, we believe that we are taking an important step towards anyone (non-experts included) being able to teach robots in real-world settings.\n\n\nAblation Study\n\nContribution of each technique.In order to evaluate the individual effects of each technique in PEBBLE, we incre-\nQ i n U p O c W v b I 3 A 1 0 l / o I U y Q K 1 n v v V 6 S c 8 j S B G L p k x b d 9 T 2 M 2 Y R s E l T A q d 1 I C y r 7 M B t C 2 N W Q S m m 8 3 2 n 9 B z q / R p m G h b M d K Z + n s i Y 5 E x 4 y i w n R H D o V n 2 p u J / X j v F 8 L q b i V i l C D G f f x S m k m J C p 2 H Q v t D A U Y 4 t Y V w L u y v l Q 6 Y Z R x t ZQ i n U p O c W v b I 3 A 1 0 l / o I U y Q K 1 n v v V 6 S c 8 j S B G L p k x b d 9 T 2 M 2 Y R s E l T A q d 1 I C y r 7 M B t C 2 N W Q S m m 8 3 2 n 9 B z q / R p m G h b M d K Z + n s i Y 5 E x 4 y i w n R H D o V n 2 p u J / X j v F 8 L q b i V i l C D G f f x S m k m J C p 2 H Q v t D A U Y 4 t Y V w L u y v l Q 6 Y Z R x t ZQ i n U p O c W v b I 3 A 1 0 l / o I U y Q K 1 n v v V 6 S c 8 j S B G L p k x b d 9 T 2 M 2 Y R s E l T A q d 1 I C y r 7 M B t C 2 N W Q S m m 8 3 2 n 9 B z q / R p m G h b M d K Z + n s i Y 5 E x 4 y i w n R H D o V n 2 p u J / X j v F 8 L q b i V i l C D G f f x S m k m J C p 2 H Q v t D A U Y 4 t Y V w L u y v l Q 6 Y Z R x t ZQ i n U p O c W v b I 3 A 1 0 l / o I U y Q K 1 n v v V 6 S c 8 j S B G L p k x b d 9 T 2 M 2 Y R s E l T A q d 1 I C y r 7 M B t C 2 N W Q S m m 8 3 2 n 9 B z q / R p m G h b M d K Z + n s i Y 5 E x 4 y i w n R H D o V n 2 p u J / X j v F 8 L q b i V i l C D G f f x S m k m J C p 2 H Q v t D A U Y 4 t Y V w L u y v l Q 6 Y Z R x t ZQ W Z B C Q Q 0 F S r i N D b A o l N A I h 5 d T v 3 E H x g q t b n A c Q z t i f S V 6 g j N 0 U q d Q b C H c Y 3 q p E 4 V g K J e a D 4Q W Z B C Q Q 0 F S r i N D b A o l N A I h 5 d T v 3 E H x g q t b n A c Q z t i f S V 6 g j N 0 U q d Q b C H c Y 3 q p E 4 V g K J e a D 4Q W Z B C Q Q 0 F S r i N D b A o l N A I h 5 d T v 3 E H x g q t b n A c Q z t i f S V 6 g j N 0 U q d Q b C H c Y 3 q p E 4 V g K J e a D 4Q W Z B C Q Q 0 F S r i N D b A o l N A I h 5 d T v 3 E H x g q t b n A c Q z t i f S V 6 g j N 0 U q d Q b C H c Y 3 q p E 4 V g K J e a D 4f z U 7 E k x A U c s m M a f h e j K 2 U a R R c w j j b T A z E j A 9 Y D x q W K h a C a a X T H 8 b 0 z C o d 2 o 2 0 L Y V 0 q v 6 e S F l o z C g M b G f I s G 8 W v Y n 4 n 9 d I s H v d S o W K E w T F Z 4 u 6 i a Q Y 0 U k g t C M 0 c J Q j S\nx j X w t 5 K e Z 9 p x t H G l r U h + I s v L 5 P q R c H 3 C v 7 t Z a 6 Y n 8 e R I S f k l O S J T 6 5 I k d y Q M q k Q T h 7 J M 3 k l b 8 6 T 8 + K 8 O x + z 1 h V n P n N E / s D 5 / A G J h 5 f / < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" S C H r e j p J 2 u 5\nF D G d B f J R 3 E U n C y q E = \" > A A A C A 3 i c b V D L S g N B E J z 1 G e N r 1 Z t e B o O Q i 2 F X B D 0 G c v E Y w T w g C W F 2 0 k m G z M 4 u M 7 3 G s A S 8 + C t e P C j i 1 Z / w 5 t 8 4 e R w 0 s a C h q O q m u y u I p T D o e d / O y u r a + s Z m Z i u 7 v b O 7 t + 8 e H F Z N l G g O F R 7 J S N c D Z k A K B R U U K K E e a 2 B h I K E W D E o T v 3 Y P 2 o h I 3 e E o h l b I e k p 0 B W d o p b Z 7 3 E R 4 w L Q k I z 4 4 H w o D d C h U J x R S j t t u z i t 4 U 9 B l 4 s 9 J j s x R b r t f z U 7 E k x A U c s m M a f h e j K 2 U a R R c w j j b T A z E j A 9 Y D x q W K h a C a a X T H 8 b 0 z C o d 2 o 2 0 L Y V 0 q v 6 e S F l o z C g M b G f I s G 8 W v Y n 4 n 9 d I s H v d S o W K E w T F Z 4 u 6 i a Q Y 0 U k g t C M 0 c J Q j S\nx j X w t 5 K e Z 9 p x t H G l r U h + I s v L 5 P q R c H 3 C v 7 t Z a 6 Y n 8 e R I S f k l O S J T 6 5 I k d y Q M q k Q T h 7 J M 3 k l b 8 6 T 8 + K 8 O x + z 1 h V n P n N E / s D 5 / A G J h 5 f / < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" S C H r e j p J 2 u 5\nF D G d B f J R 3 E U n C y q E = \" > A A A C A 3 i c b V D L S g N B E J z 1 G e N r 1 Z t e B o O Q i 2 F X B D 0 G c v E Y w T w g C W F 2 0 k m G z M 4 u M 7 3 G s A S 8 + C t e P C j i 1 Z / w 5 t 8 4 e R w 0 s a C h q O q m u y u I p T D o e d / O y u r a + s Z m Z i u 7 v b O 7 t + 8 e H F Z N l G g O F R 7 J S N c D Z k A K B R U U K K E e a 2 B h I K E W D E o T v 3 Y P 2 o h I 3 e E o h l b I e k p 0 B W d o p b Z 7 3 E R 4 w L Q k I z 4 4 H w o D d C h U J x R S j t t u z i t 4 U 9 B l 4 s 9 J j s x R b r t f z U 7 E k x A U c s m M a f h e j K 2 U a R R c w j j b T A z E j A 9 Y D x q W K h a C a a X T H 8 b 0 z C o d 2 o 2 0 L Y V 0 q v 6 e S F l o z C g M b G f I s G 8 W v Y n 4 n 9 d I s H v d S o W K E w T F Z 4 u 6 i a Q Y 0 U k g t C M 0 c J Q j S\nx j X w t 5 K e Z 9 p x t H G l r U h + I s v L 5 P q R c H 3 C v 7 t Z a 6 Y n 8 e R I S f k l O S J T 6 5 I k d y Q M q k Q T h 7 J M 3 k l b 8 6 T 8 + K 8 O x + z 1 h V n P n N E / s D 5 / A G J h 5 f / < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" S C H r e j p J 2 u 5\nF D G d B f J R 3 E U n C y q E = \" > A A A C A 3 i c b V D L S g N B E J z 1 G e N r 1 Z t e B o O Q i 2 F X B D 0 G c v E Y w T w g C W F 2 0 k m G z M 4 u M 7 3 G s A S 8 + C t e P C j i 1 Z / w 5 t 8 4 e R w 0 s a C h q O q m u y u I p T D o e d / O y u r a + s Z m Z i u 7 v b O 7 t + 8 e H F Z N l G g O F R 7 J S N c D Z k A K B R U U K K E e a 2 B h I K E W D E o T v 3 Y P 2 o h I 3 e E o h l b I e k p 0 B W d o p b Z 7 3 E R 4 w L Q k I z 4 4 H w o D d C h U J x R S j t t u z i t 4 U 9 B l 4 s 9 J j s x R b r t f z U 7 E k x A U c s m M a f h e j K 2 U a R R c w j j b T A z E j A 9 Y D x q W K h a C a a X T H 8 b 0 z C o d 2 o 2 0 L Y V 0 q v 6 e S F l o z C g M b G f I s G 8 W v Y n 4 n 9 d I s H v d S o W K E w T F Z 4 u 6 i a Q Y 0 U k g t C M 0 c J Q j S x j X w t 5 K e Z 9 p x t H G l r U h + I s v L 5 P q R c H 3 C v 7 t Z a 6 Y n 8 e R I S f k l O S J T 6 5 I k d y Q M q k Q T h 7 J M 3 k l b 8 6 T 8 + K 8 O x + z 1 h V n P n N E / s D 5 / A G J h 5 f / < / l a t e x i t >\nQuadruped waving its right front leg < l a t e x i t s h a 1 _ b a s e 6 4 = \" M W Q 4    mentally apply unsupervised pre-training and relabeling.\n3 g E 1 S G P q F I N M g K j r 1 E f v 4 X g = \" > A A A C F H i c b V B N S 8 N A E N 3 4 W e t X 1 K O X x S I I Q k l E 0 K P g x W M L t h X a U j a b S b q 4 2 Y T d i V p C f 4 Q X / 4 o X D 4 p 4 9 e D N f + O 2 z c G v B w O P 9 2 a Y m R d k U h j 0 v E 9 n b n 5 h c W m 5 s l J d X V v f 2 H S 3 t t s m z T W H F k 9 l q q 8 C Z k A K B S 0 U K O E q 0 8 C S Q E I n u D 6 f + J 0 b 0 E a k 6 h J H G f Q T F i s R C c 7 Q S g P 3 s I d w h 0 U z Z 6 H O M w j p L b s R K q Y C D d U i H i K N d K q Q S o j H A 7 f m 1 b 0 p 6 F / i l 6 R G S j Q G 7 k c v T H m e g E I u m T F d 3 8 u w X z C N g k s Y V 3 u 5 g Y z x a x Z D 1 1 L F E j D 9 Y v r U m O 5 b J a R R q m 3 Z A 6 b q 9 4 m C J c a M k s B 2 J g y H 5 r c 3 E f / z u j l G p / 1 C q C x H U H y 2 K M o l x Z R O E q K h 0 M B R j i x h X A t 7 K + V D p h l H m 2 P V h u D / f v k v a R / V f a / uM W Q 4 3 g E 1 S G P q F I N M g K j r 1 E f v 4 X g = \" > A A A C F H i c b V B N S 8 N A E N 3 4 W e t X 1 K O X x S I I Q k l E 0 K P g x W M L t h X a U j a b S b q 4 2 Y T d i V p C f 4 Q X /f i q F Q d f 9 c h Y W l 5 Z X V k t r 5 f W N z a 3 t y s 5 u 2 y S Z 5 t D i i U z 0 t c 8 M S K G g h Q I l X K c a W O x L 6 P i 3 F 5 N 6 Z w j a i E R d 4 S i F f s w i J U L B G V p r U D n u I d x j 3 s x Y o L M U A n r H h k J F V K C h E k K k o U 4 U W o z G g 0 r V r b l T 0 X n w C q i S Q o 1 B 5 b M X J D y L Q S G X z J if i q F Q d f 9 c h Y W l 5 Z X V k t r 5 f W N z a 3 t y s 5 u 2 y S Z 5 t D i i U z 0 t c 8 M S K G g h Q I l X K c a W O x L 6 P i 3 F 5 N 6 Z w j a i E R d 4 S i F f s w i J U L B G V p r U D n u I d x j 3 s x Y o L M U A n r H h k J F V K C h E k K k o U 4 U W o z G g 0 r V r b l T 0 X n w C q i S Q o 1 B 5 b M X J D y L Q S G X z J i\nFigure 5(a) shows the learning curves of PEBBLE with 1400 queries on Quadruped-walk.First, we remark that relabeling significantly improves performance because it enables the agent to be robust to changes in its reward model.By additionally utilizing unsupervised pre-training, both sample-efficiency and asymptotic performance of PEB-BLE are further improved because showing diverse behaviors to a teacher can induce a better-shaped reward.This shows that PEBBLE's key ingredients are fruitfully wed, and their unique combination is crucial to our method's success.\n\nEffects of sampling schemes.We also analyze the effects of different sampling schemes to select queries.Figure 5(b) shows the learning curves of PEBBLE with three different sampling schemes: uniform sampling, disagreement sampling and entropy sampling on Quadruped-walk.For this complex domain, we find that the uncertainty-based sampling schemes (using ensemble disagreement or entropy) are superior to the naive uniform sampling scheme.However, we note that they did not lead to extra gains on relatively simple environments, like Walker and Cheetah, similar to observations from Ibarz et al. (2018) (see the supplementary material for more results).\n\nComparison with step-wise feedback.We also measure the performance of PEBBLE by varying the length of segments.Figure 5(c) shows that feedback from longer segments (green curve) provide more meaningful signal than step-wise feedback (red curve).We believe that this is because longer segments can provide more context in reward learning.\n\n\nHuman Experiments\n\nNovel behaviors.We show that agents can perform various novel behaviors based on human feedback using PEBBLE in Figure 6.Specifically, we demonstrate (a) the Cart agent swinging a pole (using 50 queries), (b) the Quadruped agent waving a front leg (using 200 queries), and (c) the Hopper performing a backflip (using 50 queries).We note that the human is indeed able to guide the agent in a controlled way, as evidenced by training the same agent to perform several variations of the same task (e.g., waving different legs or spinning in opposite directions).The videos of all behaviors and examples of selected queries are available in the supplementary material.\n\nReward exploitation.One concern in utilizing handengineered rewards is that an agent can exploit unexpected sources of reward, leading to unintended behaviors.Indeed, we find that the Walker agent learns to walk using only one leg even though it achieves the maximum scores as shown in Figure 7(b).However, using 200 human queries, we were able to train the Walker to walk in a more natural, humanlike manner (using both legs) as shown in Figure 7(a).This result clearly shows the advantage of HiL RL to avoid reward exploitation.\n\n\nDiscussion\n\nIn this work, we present PEBBLE, a feedback-efficient algorithm for HiL RL.By leveraging unsupervised pre-training and off-policy learning, we show that sample-and feedbackefficiency of HiL RL can be significantly improved and this framework can be applied to tasks of higher complexity than previously considered by previous methods, including a variety of locomotion and robotic manipulation skills.Additionally, we demonstrate that PEBBLE can learn novel behaviors and avoid reward exploitation, leading to more desirable behaviors compared to an agent trained with respect to an engineered reward function.We believe by making preference-based learning more tractable, PEBBLE may facilitate broadening the impact of RL beyond settings in which experts can carefully craft reward functions to those in which laypeople can likewise utilize the advances of robot learning in the real world.\n\nFigures 8 and 9 show the learning curves of PEBBLE with various sampling schemes.For Quadruped, we find that the uncertainty-based sampling schemes (using ensemble disagreement or entropy) are superior to the naive uniform sampling scheme.However, they did not lead to extra gains on relatively simple environments, like Walker and Cheetah, similar to observations from Ibarz et al. (2018).Similarly, on the robotic manipulation tasks, we find little difference in performance for simpler tasks (Drawer Close, Window Open).However, we find that the uncertainty-based sampling schemes generally fare better on the other environments.\n\n\nD. Examples of Selected Queries\n\n\n\nfrequency of teacher feedback K Require: number of queries M per feedback session 1: Initialize parameters of Q \u03b8 and r \u03c8 2: Initialize a dataset of preferences D \u2190 \u2205 3: // EXPLORATION PHASE 4: B, \u03c0 \u03c6 \u2190 EXPLORE() in Algorithm 1 5: // POLICY LEARNING 6: for each iteration do 7: // REWARD LEARNING 8:\n\n\nFigure 2 .\n2\nFigure2.Examples from the environments we test on.We consider learning a variety of complex locomotion and manipulation skills through interacting with a scripted or human trainer.\n\n\nFigure 4 .Figure 5 .\n45\nFigure 4. Learning curves on robotic manipulation tasks as measured on the success rate.The solid line and shaded regions represent the mean and standard deviation, respectively, across ten runs.Asymptotic performance of PPO and Preference PPO is indicated by dotted lines of the corresponding color.\n\n\n\n\nt e x i t s h a 1 _ b a s e 6 4 = \" j 0 sx Z Q 2 Y f 7 A 8 8 n c 3 a A o U W E t F 0 g Q = \" > A A A B / 3 i c b V D L S g N B E J y N r x h f q 4 I X L 4 N B y C n s i q D H g J c c I 5 g H J C H M T n q T I b O 7 w 0 y v G N Y c / B U vH h T x 6 m 9 4 8 2 + c P A 6 a W N B Q V H X P d F e g p D D o e d 9 O b m 1 9 Y 3 M r v 1 3 Y 2 d 3 b P 3 A P j x o m S T W H O k 9 k o l s B M y B F D H U U K K G l N L A o k N A M R j d T v 3 k P 2 o g k v s O x g m 7 E B r E I B W d o p Z 5 7 0 k F 4 w K y a K A W a B o y P\n\n\n\n\nw Y b g L 5 + 8 S h o X Z d 8 r + 7 e X x U p p E U e e n J I z U i I + u S I V U i U 1 U i e c P J J n 8 k r e n C f n x X l 3 P u a t O W c x c 0 z + w P n 8 A X / K l l A = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" j 0 sx Z Q 2 Y f 7 A 8 8 n c 3 a A o U W E t F 0 g Q = \" > A A A B / 3 i c b V D L S g N B E J y N r x h f q 4 I X L4 N B y C n s i q D H g J c c I 5 g H J C H M T n q T I b O 7 w 0 y v G N Y c / B U v H h T x 6 m 9 4 8 2 + c P A 6 a W N B Q V H X P d F e g p D D o e d 9 O b m 1 9 Y 3 M r v 1 3 Y 2 d 3 b P 3 A P j x o m S T W H O k 9 k o l s B M y B F D H U U K K G l N L A o k N A M R j d T v 3 k P 2 o g k v s O x g m 7 E B r E I B W d o p Z 5 7 0 k F 4 w K y a K A W a B o y P\n\n\n\n\nw Y b g L 5 + 8 S h o X Z d 8 r + 7 e X x U p p E U e e n J I z U i I + u S I V U i U 1 U i e c P J J n 8 k r e n C f n x X l 3 P u a t O W c x c 0 z + w P n 8 A X / K l l A = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" j 0 sx Z Q 2 Y f 7 A 8 8 n c 3 a A o U W E t F 0 g Q = \" > A A A B / 3 i c b V D L S g N B E J y N r x h f q 4 I X L4 N B y C n s i q D H g J c c I 5 g H J C H M T n q T I b O 7 w 0 y v G N Y c / B U v H h T x 6 m 9 4 8 2 + c P A 6 a W N B Q V H X P d F e g p D D o e d 9 O b m 1 9 Y 3 M r v 1 3 Y 2 d 3 b P 3 A P j x o m S T W H O k 9 k o l s B M y B F D H U U K K G l N L A o k N A M R j d T v 3 k P 2 o g k v s O x g m 7 E B r E I B W d o p Z 5 7 0 k F 4 w K y a K A W a B o y P\n\n\n\n\nw Y b g L 5 + 8 S h o X Z d 8 r + 7 e X x U p p E U e e n J I z U i I + u S I V U i U 1 U i e c P J J n 8 k r e n C f n x l 3 P u a t O W c x c 0 z + w P n 8 A X / K l l A = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" j 0 sx Z Q 2 Y f 7 A 8 8 n c 3 a A o U W E t F 0 g Q = \" > A A A B / 3 i c b V D L S g N B E J y N r x h f q 4 I X L4 N B y C n s i q D H g J c c I 5 g H J C H M T n q T I b O 7 w 0 y v G N Y c / B U v H h T x 6 m 9 4 8 2 + c P A 6 a W N B Q V H X P d F e g p D D o e d 9 O b m 1 9 Y 3 M r v 1 3 Y 2 d 3 b P 3 A P j x o m S T W H O k 9 k o l s B M y B F D H U U K K G l N L A o k N A M R j d T v 3 k P 2 o g k v s O x g m 7 E B r E I B W d o p Z 5 7 0 k F 4 w K y a K A W a B o y P\n\n\n\n\nw Y b g L 5 + 8 S h o X Z d 8 r + 7 e X x U p p E U e e n J I z U i I + u S I V U i U 1 U i e c P J J n 8 k r e n C f n x X l 3 P u a t O W c x c 0 z + w P n 8 A X / K l l A = < / l a t e x i t > Counter clock-wise windmill < l a t e x i t s h a 1 _ b a s e 6 4 = \" d e K 7 g D 6 J V 6 s B J j b y N / t / l 0 x n C 7 M = \" > A A A C C 3 i c b V A 9 S w N B E N 3 z M 8 a v q K X N k i C k M d y J o K V g Y x n B J E I S w t 5 m k i z Z 2 z 1 2 5 4 z h S G / j X 7 G x U M T W P 2 D n v 3 E T r 9 D E B w O P 9 2 a Y m R f G U l j 0 / S 9 v a X l l d W 0 9 t 5 H f 3 N r e 2 S 3 s 7 d e t T g y H G t d S m 9 u\n\n\n\n\n9 H w g I d C d W N h J S T T q H k V / w Z 6 C I J M l I i G a q d w m e r q 3 k S g U I u m b X N w I +x n T K D g k u Y 5 F u J h Z j x I e t D 0 1 H F I r D t d P b L h B 4 5 p U t 7 2 r h S S G f q 7 4 m U R d a O o 9 B 1 R g w H d t 6 b i v 9 5 z Q R 7 5 + 1 U q D h B U P x n U S + R F D W d B k O 7 w g B H O X a E c S P c r Z Q P m G H c 5 W L z L o R g / u V F U j + p B H 4 l u D 4 t X Z S z O H L k k B R J m Q T k j F y Q K 1 I l N c L J A 3 k i L + T V e / Se v T f v / a d 1 y c t m D s g f e B / f z o m b b w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" d e K 7 g D 6 J V 6 s B J j b y N / t / l 0 x n C 7 M = \" > A A A C C 3 i c b V A 9 S w N B E N 3 z M 8 a v q K X N k i C k M d y J o K V g Y x n B J E I S w t 5 m k i z Z 2 z 1 2 5 4 z h S G / j X 7 G x U M T W P 2 D n v 3 E T r 9 D E B w O P 9 2 a Y m R f G U l j 0 / S 9 v a X l l d W 0 9 t 5 H f 3 N r e 2 S 3 s 7 d e t T g y H G t d S m 9 u\n\n\n\n\n9 H w g I d C d W N h J S T T q H k V / w Z 6 C I J M l I i G a q d w m e r q 3 k S g U I u m b X N w I +x n T K D g k u Y 5 F u J h Z j x I e t D 0 1 H F I r D t d P b L h B 4 5 p U t 7 2 r h S S G f q 7 4 m U R d a O o 9 B 1 R g w H d t 6 b i v 9 5 z Q R 7 5 + 1 U q D h B U P x n U S + R F D W d B k O 7 w g B H O X a E c S P c r Z Q P m G H c 5 W L z L o R g / u V F U j + p B H 4 l u D 4 t X Z S z O H L k k B R J m Q T k j F y Q K 1 I l N c L J A 3 k i L + T V e / Se v T f v / a d 1 y c t m D s g f e B / f z o m b b w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" d e K 7 g D 6 J V 6 s B J j b y N / t / l 0 x n C 7 M = \" > A A A C C 3 i c b V A 9 S w N B E N 3 z M 8 a v q K X N k i C k M d y J o K V g Y x n B J E I S w t 5 m k i z Z 2 z 1 2 5 4 z h S G / j X 7 G x U M T W P 2 D n v 3 E T r 9 D E B w O P 9 2 a Y m R f G U l j 0 / S 9 v a X l l d W 0 9 t 5 H f 3 N r e 2 S 3 s 7 d e t T g y H G t d S m 9 u\n\n\n\n\n9 H w g I d C d W N h J S T T q H k V / w Z 6 C I J M l I i G a q d w m e r q 3 k S g U I u m b X N w I +x n T K D g k u Y 5 F u J h Z j x I e t D 0 1 H F I r D t d P b L h B 4 5 p U t 7 2 r h S S G f q 7 4 m U R d a O o 9 B 1 R g w H d t 6 b i v 9 5 z Q R 7 5 + 1 U q D h B U P x n U S + R F D W d B k O 7 w g B H O X a E c S P c r Z Q P m G H c 5 W L z L o R g / u V F U j + p B H 4 l u D 4 t X Z S z O H L k k B R J m Q T k j F y Q K 1 I l N c L J A 3 k i L + T V e / Se v T f v / a d 1 y c t m D s g f e B / f z o m b b w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" d e K 7 g D 6 J V 6 s B J j b y N / t / l 0 x n C 7 M = \" > A A A C C 3 i c b V A 9 S w N B E N 3 z M 8 a v q K X N k i C k M d y J o K V g Y x n B J E I S w t 5 m k i z Z 2 z 1 2 5 4 z h S G / j X 7 G x U M T W P 2 D n v 3 E T r 9 D E B w O P 9 2 a Y m R f G U l j 0 / S 9 v a X l l d W 0 9 t 5 H f 3 N r e 2 S 3 s 7 d e t T g y H G t d S m 9 u\n\n\n\n\n9 H w g I d C d W N h J S T T q H k V / w Z 6 C I J M l I i G a q d w m e r q 3 k S g U I u m b X N w I +x n T K D g k u Y 5 F u J h Z j x I e t D 0 1 H F I r D t d P b L h B 4 5 p U t 7 2 r h S S G f q 7 4 m U R d a O o 9 B 1 R g w H d t 6 b i v 9 5 z Q R 7 5 + 1 U q D h B U P x n U S + R F D W d B k O 7 w g B H O X a E c S P c r Z Q P m G H c 5 W L z L o R g / u V F U j + p B H 4 l u D 4 t X Z S z O H L k k B R J m Q T k j F y Q K 1 I l N c L J A 3 k i L + T V e / Se v T f v / a d 1 y c t m D s g f e B / f z o m b b w = = < / l a t e x i t > Clock-wise windmill < l a t e x i t s h a 1 _ b a s e 6 4 = \" S C H r e j p J 2 u 5 F D G d B f J R 3 E U n C y q E = \" > A A A C A 3 i c b V D L S g N B E J z 1 G e N r 1 Z t e B o O Q i 2 F X B D 0 G c v E Y w T w g C W F 2 0 k m G z M 4 u M 7 3 G s A S 8 + C t e P C j i 1 Z / w 5 t 8 4 e R w 0 s a C h q O q m u y u I p T D o e d / O y u r a + s Z m Z i u 7 v b O 7 t + 8 e H F Z N l G g O F R 7 J S N c D Z k A K B R U U K K E e a 2 B h I K E W D E o T v 3 Y P 2 o h I 3 e E o h l b I e k p 0 B W d o p b Z 7 3 E R 4 w L Q k I z 4 4 H w o D d C h U J x R S j t t u z i t 4 U 9 B l 4 s 9 J j s x R b r t\n\n\n\n\nN 4 9 r Z w d l H B W y S / b I A f H J C T k j F 6 R B W o S T e / J I n s m L 8 + A 8 O a / O 2 6 x 1 z i l n d s g P O O 9 f g S e e / g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \"\n\n\n\n\n4 o X D 4 p 4 9 e D N f + O 2 z c G v B w O P 9 2 a Y m R d k U h j 0 v E 9 n b n 5 h c W m 5 s l J d X V v f 2 H S 3 t t s m z T W H F k 9 l q q 8 C Z k A K B S 0 U K O E q 0 8 C S Q E I n u D 6 f + J 0 b 0 E a k 6 h J H G f Q T F i s R C c 7 Q S g P 3 s I d w h 0 U z Z 6 H O M w j p L b s R K q Y C D d U i H i K N d K q Q S o j H A 7 f m 1 b 0 p 6 F / i l 6 R G S j Q G 7 k c v T H m e g E I u m T F d 3 8 u w X z C N g k s Y V 3 u 5 g Y z x a x Z D 1 1L F E j D 9 Y v r U m O 5 b J a R R q m 3 Z A 6 b q 9 4 m C J c a M k s B 2 J g y H 5 r c 3 E f / z u j l G p / 1 C q C x H U H y 2 K M o l x Z R O E q K h 0 M B R j i x h X A t 7 K + V D p h l H m 2 P V h u D / f v k v a R / V f a / uN 4 9 r Z w d l H B W y S / b I A f H J C T k j F 6 R B W o S T e / J I n s m L 8 + A 8 O a / O 2 6 x 1 z i l n d s g P O O 9 f g S e e / g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \"M W Q 4 3 g E 1 S G P q F I N M g K j r 1 E f v 4 X g = \" > A A A C F H i c b V B N S 8 N A E N 3 4 W e t X 1 K O X x S I I Q k l E 0 K P g x W M L t h X a U j a b S b q 4 2 Y T d i V p C f 4 Q X / 4 o X D 4 p 4 9 e D N f + O 2 z c G v B w O P 9 2 a Y m R d k U h j 0 v E 9 n b n 5 h c W m 5 s l J d X V v f 2 H S 3 t t s m z T W H F k 9 l q q 8 C Z k A K B S 0 U K O E q 0 8 C S Q E I n u D 6 f + J 0 b 0 E a k 6 h J H G f Q T F i s R C c 7 Q S g P 3 s I d w h 0 U z Z 6 H O M w j p L b s R K q Y C D d U i H i K N d K q Q S o j H A 7 f m 1 b 0 p 6 F / i l 6 R G S j Q G 7 k c v T H m e g E I u m T F d 3 8 u w X z C N g k s Y V 3 u 5 g Y z x a x Z D 1 1 L F E j D 9 Y v r U m O 5 b J a R R q m 3 Z A 6 b q 9 4 m C J c a M k s B 2 J g y H 5 r c 3 E f / z u j l G p / 1 C q C x H U H y 2 K M o l x Z R O E q K h 0 M B R j i x h X A t 7 K + V D p h l H m 2 P V h u D / f v k v a R / V f a / u N 4 9 r Z w d l H B W y S / b I A f H J C T k j F 6 R B Wo S T e / J I n s m L 8 + A 8 O a / O 2 6 x 1 z i l n d s g P O O 9 f g S e e / g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \"M W Q 4 3 g E 1 S G P q F I N M g K j r 1 E f v 4 X g = \" > A A A C F H i c b V B N S 8 N A E N 3 4 W e t X 1 K O X x S I I Q k l E 0 K P g x W M L t h X a U j a b S b q 4 2 Y T d i V p C f 4 Q X / 4 o X D 4 p 4 9 e D N f + O 2 z c G v B w O P 9 2 a Y m R d k U h j 0 v E 9 n b n 5 h c W m 5 s l J d X V v f 2 H S 3 t t s m z T W H F k 9 l q q 8 C Z k A K B S 0 U K O E q 0 8 C S Q E I n u D 6 f + J 0 b 0 E a k 6 h J H G f Q T F i s R C c 7 Q S g P 3 s I d w h 0 U z Z 6 H O M w j p L b s R K q Y C D d U i H i K N d K q Q S o j H A 7 f m 1 b 0 p 6 F / i l 6 R G S j Q G 7 k c v T H m e g E I u m T F d 3 8 u w X z C N g k s Y V 3 u 5 g Y z x a x Z D 1 1 L F E j D 9 Y v r U m O 5 b J a R R q m 3 Z A 6 b q 9 4 m C J c a M k s B 2 J g y H 5 r c 3 E f / z u j l G p / 1 C q C x H U H y 2 K M o l x Z R O E q K h 0 M B R j i x h X A t 7 K + V D p h l H m 2 P V h u D / f v k v a R / Vf a / u N 4 9 r Z w d l H B W y S / b I A f H J C T k j F 6 R B W o S T e / J I n s m L 8 + A 8 O a / O 2 6 x 1 z i l n d s g P O O 9 f g S e e / g = = < / l a t e x i t > Quadruped waving its left front leg < l a t e x i t s h a 1 _ b a s e 6 4 = \" g 9 j k j O 5 b R f g F 5 a k X e 0 n / n + 5 V x y w = \" > A A A C E 3 i c b Z A 9 S w N B E I b 3 / I z x K 2 p p s x g E s Q h 3 I m g Z s L F M w C R C E s L e 3 t y 5 u L d 3 7 M 5 F w 5 H / Y O N f s b F Q x N b G z n / j J l 6 h i S 8 s P L w z w 8 6 8f i q F Q d f 9 c h Y W l 5 Z X V k t r 5 f W N z a 3 t y s 5 u 2 y S Z 5 t D i i U z 0 t c 8 M S K G g h Q I l X K c a W O x L 6 P i 3 F 5 N 6 Z w j a i E R d 4 S i F f s w i J U L B G V p r U D n u I d x j 3 s x Y o L M U A n r H h k J F V K C h E k K k o U 4 U W o z G g 0 r V r b l T 0 X n w C q i S Q o 1 B 5 b M X J D y L Q S G X z J i u 5 6 b Y z 5 l G w S W M y 7 3 M Q M r 4 L Y u g a 1 G x G E w / n 9 4 0 p o f W C W i Y a P v s A l P 3 9 0 T O Y m N G s W 8 7 Y 4 Y 3 Z r Y 2 M f + r d T M M z / u 5 U G m G o P j P R 2 E m K S Z 0 E h A N h A a O c m S B c S 3 s r p T f M M 0 4 2 h j L N g R v 9 uR 5 a J / U P L f m N U + r 9 e M i j h L Z J w f k i H j k j N T J J W m Q F u H k g T y R F / L q P D r P z p v z / t O 6 4 B Q z e + S P n I 9 v n N i e g w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" g 9 j k j O 5 b R f g F 5 a k X e 0 n / n + 5 V x y w = \" > A A A C E 3 i c b Z A 9 S w N B E I b 3 / I z x K 2 p p s x g E s Q h 3 I m g Z s L F M w C R C E s L e 3 t y 5 u L d 3 7 M 5 F w 5 H / Y O N f s b F Q x N b G z n / j J l 6 h i S 8 s P L w z w 8 6 8 f i q F Q d f 9 c h Y W l 5 Z X V k t r 5 f W N z a 3 t y s 5 u 2 y S Z 5 t D i i U z 0 t c 8 M S K G g h Q I l X K c a W O x L 6 P i 3 F 5 N 6 Z w j a i E R d 4 S i F f s wi J U L B G V p r U D n u I d x j 3 s x Y o L M U A n r H h k J F V K C h E k K k o U 4 U W o z G g 0 r V r b l T 0 X n w C q i S Q o 1 B 5 b M X J D y L Q S G X z J i u 5 6 b Y z 5 l G w S W M y 7 3 M Q M r 4 L Y u g a 1 G x G E w / n 9 4 0 p o f W C W i Y a P v s A l P 3 9 0 T O Y m N G s W 8 7 Y 4 Y 3 Z r Y 2 M f + r d T M M z / u 5 U G m G o P j P R 2 E m K S Z 0 E h A N h A a O cm S B c S 3 s r p T f M M 0 4 2 h j L N g R v 9 u R 5 a J / U P L f m N U + r 9 e M i j h L Z J w f k i H j k j N T J J W m Q F u H k g T y R F / L q P D r P z p v z / t O 6 4 B Q z e + S P n I 9 v n N i e g w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" g 9 j k j O 5 b R f g F 5 a k X e 0 n / n + 5 V x y w = \" > A A A C E 3 i c b Z A 9 S w N B E I b 3 / I z x K 2 p p s x g E s Q h 3 I m g Z s L F M w C R C E s L e 3 t y 5 u L d 3 7 M 5 F w 5 H / Y O N f s b F Q x N b G z n / j J l 6 h i S 8 s P L w z w 8 6 8\n\n\n\n\nu 5 6 b Y z 5 l G w S W M y 7 3 M Q M r 4 L Y u g a 1 G x G E w / n 9 4 0 p o f W C W i Y a P v s A l P 3 9 0 T O Y m N G s W 8 7 Y 4 Y 3 Z r Y 2 M f + r d T M M z / u 5 U G m G o P j P R 2 E m K S Z 0 E h A N h A a O c m S B c S 3 s r p T f M M 0 4 2 h j L N g R v 9 u R 5 a J / U P L f m N U + r 9 e M i j h L Z J w f k i H j k j N T J J W m Q F u H k g T y R F / L q P D r P z p v z / t O 6 4 B Q z e + S P n I 9 v n N i e g w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" g 9 j k j O 5 b R f g F 5 a k X e 0 n / n + 5 V x y w = \" > A A A C E 3 i c b Z A 9 S w N B E I b 3 / I z x K 2 p p s x g E s Q h 3 I m g Z s L F M w C R C E s L e 3 t y 5 u L d 3 7 M 5 F w 5 H / Y O N f s b F Q x N b G z n / j J l 6 h i S 8 s P L w z w 8 6 8\n\n\nFigure 6 .Figure 7 .\n67\nFigure 6.Novel behaviors trained using feedback from human trainers.The corresponding videos and examples of selected queries are available at the supplementary material.\n\n\nFigure 8 .\n8\nFigure 8. Learning curves of PEBBLE with 1400 pieces of feedback by varying sampling schemes.The solid line and shaded regions represent the mean and standard deviation, respectively, across ten runs.\n\n\nFigure 10 ,Figure 9 .\n109\nFigure 10, 11 and 12  show some examples from the selected queries to teach the agents.\n\n\nFigure 11 .\n11\nFigure 11.Examples from the selected queries to teach the Quadruped agent.\n\n\nFigure 12 .\n12\nFigure 12.Examples from the selected queries to teach the Hopper agent.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIbarz et al. (2018)2017)otion tasks as measured on the ground truth reward.The solid line and shaded regions represent the mean and standard deviation, respectively, across ten runs.Asymptotic performance of PPO and Preference PPO is indicated by dotted lines of the corresponding color.groundtruthrewardfunction.Instead, similar toChristiano et al. (2017)andIbarz et al. (2018), the agent learns to perform a task only by getting feedback from a scripted teacher that provides preferences between trajectory segments according to the true, underlying task reward.Because this scripted teacher's preferences are immediately generated by a ground truth reward, we are able to evaluate the agent quantitatively by measuring the true average return and do more rapid experiments.For all experiments, we report the mean and standard deviation across ten runs.\nEpisode Return250 500 750 1,000QuadrupedEpisode Return250 500 750 1,000CheetahEpisode Return250 500 750 1,000WalkerSAC with task reward (oracle) PPO with task reward (oracle) Preference PPO (feedback=2100) Preference PPO (feedback=1400) Preference PPO + pre-train (feedback=1400) PEBBLE (feedback=1400) PEBBLE (feedback=700) PEBBLE (feedback=400)0.0 00.25 Environment Steps (\u00d710 6 ) 0.5 0.751.00.0 00.25 Environment Steps (\u00d710 6 ) 0.5 0.751.00.0 00.25 Environment Steps (\u00d710 6 ) 0.5 0.751.0Figure 3.\nKober & Peters, 2011;Kober et al., 2013;Silver et al., 2017;Andrychowicz et al., 2020;Kalashnikov et al., 2018;Vinyals et al., 2019). Scaling RL to many applications, however, is yet precluded by a number of challenges. One such challenge lies in providing a suitable reward function.For example, while it may be desirable to provide sparse rewards out of ease, they are often insufficient to train successful RL agents. Thus, to provide adequately dense signal, real-world problems may require extensive instrumentation, such as accelerometers to detect door opening(Yahya et al., 2017), thermal cameras to detect pouring(Schenck & Fox, 2017) or motion capture for object tracking(Kormushev et al., 2010;Akkaya et al., 2019;Peng et al., 2020).\nOne piece of feedback corresponds to one preference query.\nAcknowledgementsThis research is supported in part by ONR PECASE N000141612723, NSF NRI #2024675, Tencent, and Berkeley Deep Drive.Laura Smith was supported by NSF Graduate Research Fellowship.We thank Abhishek Gupta, Joey Hejna, Qiyang (Colin) Li, Fangchen Liu, Olivia Watkins, and Mandi Zhao for providing helpful feedbacks and suggestions.We also thank anonymous reviewers for critically reading the manuscript and suggesting substantial improvements.Appendix A. State Entropy EstimatorTo approximate state entropy, we employ the simplified version of particle-based entropy estimator(Beirlant et al., 1997;Singh et al., 2003).Specifically, let s be a random variable with a probability density function p whose support is a set S \u2282 R q .Then its differential entropy is given as H(s) = \u2212E s\u223cp(s)[log p(s)].When the distribution p is not available, this quantity can be estimated given N i.i.d realizations of {s i } N i=1(Beirlant et al., 1997).However, since it is difficult to estimate p with high-dimensional data, particle-based k-nearest neighbors (k-NN) entropy estimator(Singh et al., 2003)can be employed:where \u03c0 is the ratio of a circle's circumference to its diameter,a bias correction term, \u03a8 the digamma function, \u0393 the gamma function, q the dimension of s, and the transition from (6) to (7) always holds for q > 0.Then, from Equation7, we define the intrinsic reward of the current state s t as follows:B. Experimental DetailsTraining details.For our method, we use the publicly released implementation repository of the SAC algorithm (https: //github.com/denisyarats/pytorch_sac)with a full list of hyperparameters in Table1.On the DMControl environments, we use segments of length 50 and a frequency of teacher feedback (K in Algorithm 2) of 20K timesteps, which corresponds to roughly 20 episodes.We choose the number of queries per feedback session M = 140, 70, 40 for the maximum budget of 1400, 700, 400 on Walker and Cheetah, and choose M = 70, 35, 20 for the maximum budget of 1400, 700, 400 on Quadruped.For Meta-world, we use segments of length 10 and set M = 64, K = 2400 for the maximum budget of 2500, 5000, and 10000 on Drawer Close, Window Open, Door Open, and Button Press and M = 128, K = 4800 for maximum budget of 25000, 50000 on Sweep Into and Drawer Open.\nApprenticeship learning via inverse reinforcement learning. P Abbeel, A Y Ng, International Conference on Machine Learning. 2004\n\nJ Achiam, H Edwards, D Amodei, P Abbeel, arXiv:1807.10299Variational option discovery algorithms. 2018arXiv preprint\n\nTrajectories and keyframes for kinesthetic teaching: A humanrobot interaction perspective. B Akgun, M Cakmak, J Yoo, A Thomaz, International Conference on Human-Robot Interaction. 2012\n\nSolving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R Ribas, arXiv:1910.071132019arXiv preprint\n\nPreferencebased policy learning. R Akrour, M Schoenauer, M Sebag, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. 2011\n\nActive preference learning-based reinforcement learning. R Akrour, M Schoenauer, M Sebag, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. April. 2012\n\nLearning dexterous in-hand manipulation. D Amodei, C Olah, J Steinhardt, P Christiano, J Schulman, D Man\u00e9, O M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, arXiv:1606.06565The International Journal of Robotics Research. 3912016. 2020arXiv preprintConcrete problems in ai safety\n\nA survey of robot learning from demonstration. B D Argall, S Chernova, M Veloso, B Browning, Robotics and autonomous systems. 5752009\n\nD Arumugam, J K Lee, S Saskin, M L Littman, arXiv:1902.04257Deep reinforcement learning from policy-dependent human feedback. 2019arXiv preprint\n\nNonparametric entropy estimation: An overview. J Beirlant, E J Dudewicz, L Gy\u00f6rfi, E C Van Der Meulen, International Journal of Mathematical and Statistical Sciences. 611997\n\nUnifying count-based exploration and intrinsic motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in Neural Information Processing Systems. 2016\n\nBatch active preference-based learning of reward functions. E Biyik, D Sadigh, Conference on Robot Learning. 2018\n\nActive preference-based gaussian process regression for reward learning. E Biyik, N Huynh, M J Kochenderfer, D Sadigh, Robotics: Science and Systems. 2020\n\nRank analysis of incomplete block designs: I. the method of paired comparisons. R A Bradley, M E Terry, Biometrika. 393/41952\n\nExploration by random network distillation. Y Burda, H Edwards, A Storkey, O Klimov, International Conference on Learning Representations. 2019\n\nLearning collaborative manipulation tasks by demonstration using a haptic interface. S Calinon, P Evrard, E Gribovskaya, A Billard, A Kheddar, International Conference on Advanced Robotics. 2009\n\nHuman preference scaling with demonstrations for deep reinforcement learning. Z Cao, K Wong, C.-T Lin, arXiv:2007.129042020arXiv preprint\n\nDeep reinforcement learning from human preferences. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, Advances in Neural Information Processing Systems. 2017\n\nActive reward learning. C Daniel, M Viering, J Metz, O Kroemer, J Peters, Robotics: Science and Systems. 2014\n\nHierarchical relative entropy policy search. C Daniel, G Neumann, O Kroemer, J Peters, The Journal of Machine Learning Research. 1712016\n\nDiversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, International Conference on Learning Representations. 2019\n\nStochastic neural networks for hierarchical reinforcement learning. C Florensa, Y Duan, P Abbeel, International Conference on Learning Representations. 2018\n\nVariational inverse control with events: A general framework for data-driven reward definition. J Fu, A Singh, D Ghosh, L Yang, S Levine, Advances in Neural Information Processing Systems. 2018\n\nSoft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International Conference on Machine Learning. 2018\n\nBehavior from the void: Unsupervised active pre-training. L Hao, A Pieter, International Conference on Machine Learning. 2021\n\nProvably efficient maximum entropy exploration. E Hazan, S Kakade, K Singh, A Van Soest, International Conference on Machine Learning. 2019\n\nVime: Variational information maximizing exploration. R Houthooft, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, Advances in Neural Information Processing Systems. 2016\n\nReward learning from human preferences and demonstrations in atari. B Ibarz, J Leike, T Pohlen, G Irving, S Legg, D Amodei, Advances in Neural Information Processing Systems. 2018\n\nQt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, Conference on Robot Learning. 2018\n\nA method for stochastic optimization. D P Kingma, J Ba, Adam, International Conference on Learning Representations. 2015\n\nInteractively shaping agents via human reinforcement: The tamer framework. W B Knox, P Stone, International Conference on Knowledge Capture. 2009\n\nPolicy search for motor primitives in robotics. J Kober, J Peters, Machine learning. 841-22011\n\nReinforcement learning in robotics: A survey. J Kober, J A Bagnell, J Peters, The International Journal of Robotics Research. 32112013\n\nPolicy gradient reinforcement learning for fast quadrupedal locomotion. N Kohl, P Stone, International Conference on Robotics and Automation. 2004\n\nRobot motor skill coordination with EM-based reinforcement learning. P Kormushev, S Calinon, D Caldwell, International Conference on Intelligent Robots and Systems. 2010\n\nL Lee, B Eysenbach, E Parisotto, E Xing, S Levine, R Salakhutdinov, arXiv:1906.05274Efficient exploration via state marginal matching. 2019arXiv preprint\n\nLearning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. J Leike, D Krueger, T Everitt, M Martic, V Maini, S Legg, S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, arXiv:1811.07871The International Journal of Robotics Research. 374-52018. 2018arXiv preprintScalable agent alignment via reward modeling: a research direction\n\nInteractive learning from policy-dependent human feedback. J Macglashan, M K Ho, R Loftin, B Peng, D Roberts, M E Taylor, M L Littman, International Conference on Machine Learning. 2017\n\nAsynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International Conference on Machine Learning. 2016\n\nAlgorithms for inverse reinforcement learning. A Y Ng, S J Russell, International Conference on Machine Learning. 2000\n\nCount-based exploration with neural density models. G Ostrovski, M G Bellemare, A V D Oord, R Munos, International Conference on Machine Learning. 2017\n\nIntrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, IEEE transactions on evolutionary computation. 1122007\n\nOnline movement adaptation based on previous sensor experiences. P Pastor, L Righetti, M Kalakrishnan, S Schaal, International Conference on Intelligent Robots and Systems. 2011\n\nCuriosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, International Conference on Machine Learning. 2017\n\nLearning agile robotic locomotion skills by imitating animals. X B Peng, E Coumans, T Zhang, T.-W Lee, J Tan, S Levine, Robotics: Science and Systems. 2020\n\nOnline human training of a myoelectric prosthesis controller via actor-critic reinforcement learning. P M Pilarski, M R Dawson, T Degris, F Fahimi, J P Carey, R S Sutton, International Conference on Rehabilitation Robotics. 2011\n\nSupersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. L Pinto, A Gupta, A Rusu, M Vecer\u00edk, T Roth\u00f6rl, N Heess, R Pascanu, R Hadsell, International Conference on Robotics and Automation. 2016. 2017Conference on Robot Learning\n\nActive preference-based learning of reward functions. D Sadigh, A D Dragan, S Sastry, S A Seshia, Robotics: Science and Systems. 2017\n\nThe foundations of statistics. L J Savage, Courier Corporation. 1972\n\nLearning from demonstration. S Schaal, Advances in Neural Information Processing Systems. 1997\n\nVisual closed-loop control for pouring liquids. C Schenck, D Fox, International Conference on Robotics and Automation. 2017\n\nFormal theory of creativity, fun, and intrinsic motivation. J Schmidhuber, IEEE Transactions on Autonomous Mental Development. 231990-2010. 2010\n\nTrust region policy optimization. J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, International Conference on Machine Learning. 2015\n\nJ Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint\n\nState entropy maximization with random encoders for efficient exploration. Y Seo, L Chen, J Shin, H Lee, P Abbeel, K Lee, International Conference on Machine Learning. 2021\n\nPreferences implicit in the state of the world. R Shah, D Krasheninnikov, J Alexander, P Abbeel, A Dragan, International Conference on Learning Representations. 2019\n\nDynamics-aware unsupervised discovery of skills. A Sharma, S Gu, S Levine, V Kumar, K Hausman, International Conference on Learning Representations. 2020\n\nMastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Nature. 55076763542017\n\nEnd-to-end robotic reinforcement learning without reward engineering. A Singh, L Yang, K Hartikainen, C Finn, S Levine, Robotics: Science and Systems. 2019\n\nNearest neighbor estimates of entropy. H Singh, N Misra, V Hnizdo, A Fedorowicz, E Demchuk, American journal of mathematical and management sciences. 233-42003\n\nLearning multi-stage tasks via pixel-level translation of human videos. L Smith, N Dhawan, M Zhang, P Abbeel, S Levine, Avid, Robotics: Science and Systems. 2020\n\nPreferencelearning based inverse reinforcement learning for dialog control. H Sugiyama, T Meguro, Y Minami, Conference of the International Speech Communication Association. 2012\n\n# exploration: A study of count-based exploration for deep reinforcement learning. H Tang, R Houthooft, D Foote, A Stooke, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, Advances in Neural Information Processing Systems. 2017\n\nY Tassa, Y Doron, A Muldal, T Erez, Y Li, D D L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. 2018arXiv preprint\n\nY Tassa, S Tunyasuvunakool, A Muldal, Y Doron, S Liu, S Bohez, J Merel, T Erez, T Lillicrap, N Heess, arXiv:2006.12983dm_control: Software and tasks for continuous control. 2020arXiv preprint\n\nAvoiding side effects in complex environments. A M Turner, N Ratzlaff, P Tadepalli, arXiv:2006.065472020arXiv preprint\n\nMonte carlo methods for preference learning. P Viappiani, International Conference on Learning and Intelligent Optimization. 2012\n\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, Nature. 57577822019\n\nDeep tamer: Interactive agent shaping in highdimensional state spaces. G Warnell, N Waytowich, V Lawhern, P Stone, Conference on Artificial Intelligence. 2018\n\nA bayesian approach for policy learning from trajectory preference queries. A Wilson, A Fern, P Tadepalli, Advances in Neural Information Processing Systems. 2012\n\nPreference-based reinforcement learning: A preliminary survey. C Wirth, J F\u00fcrnkranz, ECML/PKDD Workshop on Reinforcement Learning from Generalized Feedback: Beyond Numeric Rewards. 2013\n\nModel-free preference-based reinforcement learning. C Wirth, J F\u00fcrnkranz, G Neumann, Conference on Artificial Intelligence. 2016\n\nFew-shot goal inference for visuomotor learning and planning. A Xie, A Singh, S Levine, C Finn, Conference on Robot Learning. 2018\n\nCollective robot reinforcement learning with distributed asynchronous guided policy search. A Yahya, A Li, M Kalakrishnan, Y Chebotar, S Levine, International Conference on Intelligent Robots and Systems. 2017\n\nMeta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, Conference on Robot Learning. 2020\n\nDeep structured representations for model-based reinforcement learning. M Zhang, S Vikram, L Smith, P Abbeel, M Johnson, S Levine, Solar, International Conference on Machine Learning. 2019\n\nDeep imitation learning for complex manipulation tasks from virtual reality teleoperation. T Zhang, Z Mccarthy, O Jow, D Lee, K Goldberg, P Abbeel, Ziebart, B. D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. 2018. 2010International Conference on Robotics and Automation\n\nReward model. For the reward model, we use a three-layer neural network with 256 hidden units each, using leaky ReLUs. To improve the stability in reward learning, we use an ensemble of three reward models, and bound the output using tanh function. Each model is trained by optimizing the cross-entropy loss defined in (4) using ADAM learning rule (Kingma & Ba, 2015) with the initial learning rate of 0.0003. Environments. We follow the standard evaluation protocol for the benchmark locomotion tasks from DMControl. The Meta-world single-task benchmark involves training and testing on a single instantiation (fixed reset and goal) of the task. To constitute a more realistic single-task manipulation setting, we randomize the reset and goal positions in all our experiments. ) with a full list of hyperparameters in Table 2. We choose the number of queries per feedback session M = 70, 45 for the maximum budget of 2100, 1400 on the DMControl environments. For the reward model, we use same setups for our method. For Meta-world, we use segments of length 10 and set M = 256, K = 2400 for all environments and budgets of feedback. We also use new reward function, which are nicely normalized and make the tasks stable\n", "annotations": {"author": "[{\"end\":200,\"start\":131},{\"end\":275,\"start\":201},{\"end\":325,\"start\":276},{\"end\":339,\"start\":326}]", "publisher": null, "author_last_name": "[{\"end\":140,\"start\":137},{\"end\":212,\"start\":207},{\"end\":289,\"start\":283}]", "author_first_name": "[{\"end\":136,\"start\":131},{\"end\":206,\"start\":201},{\"end\":282,\"start\":276}]", "author_affiliation": "[{\"end\":199,\"start\":166},{\"end\":274,\"start\":241},{\"end\":324,\"start\":291},{\"end\":338,\"start\":327}]", "title": "[{\"end\":118,\"start\":1},{\"end\":457,\"start\":340}]", "venue": null, "abstract": "[{\"end\":1922,\"start\":526}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2100,\"start\":2080},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2542,\"start\":2521},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2560,\"start\":2542},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":2580,\"start\":2560},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2771,\"start\":2757},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2787,\"start\":2771},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2805,\"start\":2787},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2825,\"start\":2805},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2973,\"start\":2951},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2993,\"start\":2973},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3012,\"start\":2993},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":3031,\"start\":3012},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3723,\"start\":3703},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3747,\"start\":3723},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3771,\"start\":3747},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5045,\"start\":5023},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5063,\"start\":5045},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5436,\"start\":5411},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":6790,\"start\":6772},{\"end\":6795,\"start\":6790},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":6828,\"start\":6811},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7345,\"start\":7322},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7369,\"start\":7345},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7391,\"start\":7369},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7855,\"start\":7834},{\"end\":7875,\"start\":7855},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7891,\"start\":7875},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":7908,\"start\":7891},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":8002,\"start\":7982},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8021,\"start\":8002},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":8040,\"start\":8021},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8188,\"start\":8168},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8209,\"start\":8188},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8616,\"start\":8595},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8638,\"start\":8616},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8658,\"start\":8638},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8678,\"start\":8658},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8700,\"start\":8678},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":8724,\"start\":8700},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":8743,\"start\":8724},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8763,\"start\":8743},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8784,\"start\":8763},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8803,\"start\":8784},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8822,\"start\":8803},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8847,\"start\":8823},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9135,\"start\":9112},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9153,\"start\":9135},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9699,\"start\":9679},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9741,\"start\":9723},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10252,\"start\":10231},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10274,\"start\":10252},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10294,\"start\":10274},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10317,\"start\":10294},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10337,\"start\":10317},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10503,\"start\":10479},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10523,\"start\":10503},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10542,\"start\":10523},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10594,\"start\":10570},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":10612,\"start\":10594},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10635,\"start\":10612},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10680,\"start\":10656},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10719,\"start\":10699},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10736,\"start\":10719},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10755,\"start\":10736},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11648,\"start\":11625},{\"end\":11742,\"start\":11727},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":13010,\"start\":12989},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13034,\"start\":13010},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13428,\"start\":13405},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15716,\"start\":15696},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15920,\"start\":15898},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15938,\"start\":15920},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16654,\"start\":16634},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16671,\"start\":16654},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16690,\"start\":16671},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16707,\"start\":16690},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17055,\"start\":17032},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":17074,\"start\":17055},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17560,\"start\":17541},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":18395,\"start\":18378},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18415,\"start\":18395},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19439,\"start\":19418},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19463,\"start\":19439},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19482,\"start\":19463},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19566,\"start\":19542},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20278,\"start\":20254},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20337,\"start\":20314},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20365,\"start\":20346},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":21690,\"start\":21672},{\"end\":21695,\"start\":21690},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":21728,\"start\":21711},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22592,\"start\":22568},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22959,\"start\":22936},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23223,\"start\":23200},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":25174,\"start\":25154},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25194,\"start\":25174},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25217,\"start\":25194},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":25235,\"start\":25217},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25255,\"start\":25235},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25273,\"start\":25255},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":25379,\"start\":25359},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34897,\"start\":34878},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":37802,\"start\":37783},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":54855,\"start\":54834},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":54874,\"start\":54855},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":54894,\"start\":54874},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":54920,\"start\":54894},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":54945,\"start\":54920},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":54966,\"start\":54945},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":55421,\"start\":55401},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":55477,\"start\":55456},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":55539,\"start\":55515},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":55559,\"start\":55539},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":55577,\"start\":55559}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38383,\"start\":38080},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38579,\"start\":38384},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38906,\"start\":38580},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39428,\"start\":38907},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40152,\"start\":39429},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40876,\"start\":40153},{\"attributes\":{\"id\":\"fig_7\"},\"end\":41598,\"start\":40877},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42214,\"start\":41599},{\"attributes\":{\"id\":\"fig_9\"},\"end\":43156,\"start\":42215},{\"attributes\":{\"id\":\"fig_10\"},\"end\":44098,\"start\":43157},{\"attributes\":{\"id\":\"fig_11\"},\"end\":45040,\"start\":44099},{\"attributes\":{\"id\":\"fig_12\"},\"end\":46202,\"start\":45041},{\"attributes\":{\"id\":\"fig_13\"},\"end\":46412,\"start\":46203},{\"attributes\":{\"id\":\"fig_14\"},\"end\":51994,\"start\":46413},{\"attributes\":{\"id\":\"fig_15\"},\"end\":52754,\"start\":51995},{\"attributes\":{\"id\":\"fig_16\"},\"end\":52951,\"start\":52755},{\"attributes\":{\"id\":\"fig_17\"},\"end\":53167,\"start\":52952},{\"attributes\":{\"id\":\"fig_18\"},\"end\":53283,\"start\":53168},{\"attributes\":{\"id\":\"fig_19\"},\"end\":53375,\"start\":53284},{\"attributes\":{\"id\":\"fig_20\"},\"end\":53464,\"start\":53376},{\"end\":53469,\"start\":53465},{\"end\":53474,\"start\":53470},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54833,\"start\":53475}]", "paragraph": "[{\"end\":3032,\"start\":1938},{\"end\":3804,\"start\":3034},{\"end\":4353,\"start\":3810},{\"end\":5547,\"start\":4355},{\"end\":6414,\"start\":5549},{\"end\":6462,\"start\":6416},{\"end\":6620,\"start\":6464},{\"end\":6829,\"start\":6622},{\"end\":6958,\"start\":6831},{\"end\":7129,\"start\":6960},{\"end\":7545,\"start\":7146},{\"end\":8370,\"start\":7547},{\"end\":10034,\"start\":8372},{\"end\":11069,\"start\":10036},{\"end\":11426,\"start\":11087},{\"end\":11601,\"start\":11428},{\"end\":12153,\"start\":11603},{\"end\":12530,\"start\":12288},{\"end\":12793,\"start\":12604},{\"end\":13503,\"start\":12795},{\"end\":14082,\"start\":13588},{\"end\":14509,\"start\":14179},{\"end\":14512,\"start\":14511},{\"end\":14671,\"start\":14514},{\"end\":14674,\"start\":14673},{\"end\":14824,\"start\":14676},{\"end\":14827,\"start\":14826},{\"end\":15029,\"start\":14829},{\"end\":15060,\"start\":15042},{\"end\":15939,\"start\":15116},{\"end\":16210,\"start\":15941},{\"end\":16254,\"start\":16212},{\"end\":16314,\"start\":16256},{\"end\":16371,\"start\":16316},{\"end\":16383,\"start\":16373},{\"end\":16413,\"start\":16385},{\"end\":16471,\"start\":16415},{\"end\":16550,\"start\":16473},{\"end\":17125,\"start\":16552},{\"end\":17603,\"start\":17158},{\"end\":17920,\"start\":17645},{\"end\":18445,\"start\":17954},{\"end\":18471,\"start\":18447},{\"end\":18518,\"start\":18473},{\"end\":18546,\"start\":18520},{\"end\":18594,\"start\":18548},{\"end\":18607,\"start\":18596},{\"end\":18638,\"start\":18609},{\"end\":18689,\"start\":18640},{\"end\":18737,\"start\":18691},{\"end\":18750,\"start\":18739},{\"end\":18796,\"start\":18752},{\"end\":18808,\"start\":18798},{\"end\":18836,\"start\":18810},{\"end\":18881,\"start\":18838},{\"end\":18939,\"start\":18883},{\"end\":18952,\"start\":18941},{\"end\":18983,\"start\":18954},{\"end\":19029,\"start\":18985},{\"end\":19994,\"start\":19031},{\"end\":21122,\"start\":20045},{\"end\":21193,\"start\":21138},{\"end\":21285,\"start\":21195},{\"end\":21360,\"start\":21287},{\"end\":21459,\"start\":21362},{\"end\":21519,\"start\":21461},{\"end\":22536,\"start\":21530},{\"end\":23104,\"start\":22538},{\"end\":23676,\"start\":23106},{\"end\":24527,\"start\":23720},{\"end\":24931,\"start\":24529},{\"end\":26105,\"start\":24933},{\"end\":26237,\"start\":26124},{\"end\":28683,\"start\":28394},{\"end\":29749,\"start\":29460},{\"end\":30815,\"start\":30526},{\"end\":31964,\"start\":31818},{\"end\":34294,\"start\":33728},{\"end\":34948,\"start\":34296},{\"end\":35287,\"start\":34950},{\"end\":35973,\"start\":35309},{\"end\":36505,\"start\":35975},{\"end\":37411,\"start\":36520},{\"end\":38045,\"start\":37413},{\"end\":38382,\"start\":38083},{\"end\":38578,\"start\":38398},{\"end\":38905,\"start\":38605},{\"end\":39427,\"start\":38910},{\"end\":40151,\"start\":39432},{\"end\":40875,\"start\":40156},{\"end\":41597,\"start\":40880},{\"end\":42213,\"start\":41602},{\"end\":43155,\"start\":42218},{\"end\":44097,\"start\":43160},{\"end\":45039,\"start\":44102},{\"end\":46201,\"start\":45044},{\"end\":46411,\"start\":46206},{\"end\":51993,\"start\":46416},{\"end\":52753,\"start\":51998},{\"end\":52950,\"start\":52780},{\"end\":53166,\"start\":52966},{\"end\":53282,\"start\":53195},{\"end\":53374,\"start\":53300},{\"end\":53463,\"start\":53392},{\"end\":54333,\"start\":53478}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12287,\"start\":12154},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12603,\"start\":12531},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13587,\"start\":13504},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14169,\"start\":14083},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17157,\"start\":17126},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17644,\"start\":17604},{\"attributes\":{\"id\":\"formula_6\"},\"end\":26573,\"start\":26238},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26908,\"start\":26573},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27243,\"start\":26908},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27578,\"start\":27243},{\"attributes\":{\"id\":\"formula_10\"},\"end\":27719,\"start\":27578},{\"attributes\":{\"id\":\"formula_11\"},\"end\":27860,\"start\":27719},{\"attributes\":{\"id\":\"formula_12\"},\"end\":28001,\"start\":27860},{\"attributes\":{\"id\":\"formula_13\"},\"end\":28142,\"start\":28001},{\"attributes\":{\"id\":\"formula_14\"},\"end\":28393,\"start\":28142},{\"attributes\":{\"id\":\"formula_15\"},\"end\":29459,\"start\":28684},{\"attributes\":{\"id\":\"formula_16\"},\"end\":30525,\"start\":29750},{\"attributes\":{\"id\":\"formula_17\"},\"end\":31817,\"start\":30816},{\"attributes\":{\"id\":\"formula_18\"},\"end\":32848,\"start\":31965},{\"attributes\":{\"id\":\"formula_19\"},\"end\":33045,\"start\":32848},{\"attributes\":{\"id\":\"formula_20\"},\"end\":33386,\"start\":33045},{\"attributes\":{\"id\":\"formula_21\"},\"end\":33727,\"start\":33386}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1936,\"start\":1924},{\"end\":3808,\"start\":3807},{\"attributes\":{\"n\":\"2.\"},\"end\":7144,\"start\":7132},{\"attributes\":{\"n\":\"3.\"},\"end\":11085,\"start\":11072},{\"attributes\":{\"n\":\"4.\"},\"end\":14177,\"start\":14171},{\"end\":15040,\"start\":15032},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15114,\"start\":15063},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17952,\"start\":17923},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20043,\"start\":19997},{\"attributes\":{\"n\":\"5.\"},\"end\":21136,\"start\":21125},{\"attributes\":{\"n\":\"5.1.\"},\"end\":21528,\"start\":21522},{\"attributes\":{\"n\":\"5.2.\"},\"end\":23718,\"start\":23679},{\"attributes\":{\"n\":\"5.3.\"},\"end\":26122,\"start\":26108},{\"attributes\":{\"n\":\"5.4.\"},\"end\":35307,\"start\":35290},{\"attributes\":{\"n\":\"6.\"},\"end\":36518,\"start\":36508},{\"end\":38079,\"start\":38048},{\"end\":38395,\"start\":38385},{\"end\":38601,\"start\":38581},{\"end\":52776,\"start\":52756},{\"end\":52963,\"start\":52953},{\"end\":53190,\"start\":53169},{\"end\":53296,\"start\":53285},{\"end\":53388,\"start\":53377}]", "table": "[{\"end\":54833,\"start\":54334}]", "figure_caption": "[{\"end\":38383,\"start\":38082},{\"end\":38579,\"start\":38397},{\"end\":38906,\"start\":38604},{\"end\":39428,\"start\":38909},{\"end\":40152,\"start\":39431},{\"end\":40876,\"start\":40155},{\"end\":41598,\"start\":40879},{\"end\":42214,\"start\":41601},{\"end\":43156,\"start\":42217},{\"end\":44098,\"start\":43159},{\"end\":45040,\"start\":44101},{\"end\":46202,\"start\":45043},{\"end\":46412,\"start\":46205},{\"end\":51994,\"start\":46415},{\"end\":52754,\"start\":51997},{\"end\":52951,\"start\":52779},{\"end\":53167,\"start\":52965},{\"end\":53283,\"start\":53194},{\"end\":53375,\"start\":53299},{\"end\":53464,\"start\":53391},{\"end\":53469,\"start\":53467},{\"end\":53474,\"start\":53472},{\"end\":54334,\"start\":53477}]", "figure_ref": "[{\"end\":3818,\"start\":3817},{\"end\":4860,\"start\":4859},{\"end\":21037,\"start\":21036},{\"end\":23760,\"start\":23759},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25694,\"start\":25693},{\"end\":25716,\"start\":25715},{\"end\":33736,\"start\":33735},{\"end\":34411,\"start\":34407},{\"end\":35069,\"start\":35068},{\"end\":35429,\"start\":35428},{\"end\":36269,\"start\":36268},{\"end\":36422,\"start\":36421},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":37428,\"start\":37421}]", "bib_author_first_name": "[{\"end\":57994,\"start\":57993},{\"end\":58004,\"start\":58003},{\"end\":58006,\"start\":58005},{\"end\":58064,\"start\":58063},{\"end\":58074,\"start\":58073},{\"end\":58085,\"start\":58084},{\"end\":58095,\"start\":58094},{\"end\":58273,\"start\":58272},{\"end\":58282,\"start\":58281},{\"end\":58292,\"start\":58291},{\"end\":58299,\"start\":58298},{\"end\":58408,\"start\":58407},{\"end\":58418,\"start\":58417},{\"end\":58434,\"start\":58433},{\"end\":58445,\"start\":58444},{\"end\":58455,\"start\":58454},{\"end\":58465,\"start\":58464},{\"end\":58475,\"start\":58474},{\"end\":58484,\"start\":58483},{\"end\":58496,\"start\":58495},{\"end\":58506,\"start\":58505},{\"end\":58584,\"start\":58583},{\"end\":58594,\"start\":58593},{\"end\":58608,\"start\":58607},{\"end\":58764,\"start\":58763},{\"end\":58774,\"start\":58773},{\"end\":58788,\"start\":58787},{\"end\":58935,\"start\":58934},{\"end\":58945,\"start\":58944},{\"end\":58953,\"start\":58952},{\"end\":58967,\"start\":58966},{\"end\":58981,\"start\":58980},{\"end\":58993,\"start\":58992},{\"end\":59001,\"start\":59000},{\"end\":59003,\"start\":59002},{\"end\":59019,\"start\":59018},{\"end\":59028,\"start\":59027},{\"end\":59039,\"start\":59038},{\"end\":59053,\"start\":59052},{\"end\":59063,\"start\":59062},{\"end\":59075,\"start\":59074},{\"end\":59085,\"start\":59084},{\"end\":59097,\"start\":59096},{\"end\":59107,\"start\":59106},{\"end\":59284,\"start\":59283},{\"end\":59286,\"start\":59285},{\"end\":59296,\"start\":59295},{\"end\":59308,\"start\":59307},{\"end\":59318,\"start\":59317},{\"end\":59372,\"start\":59371},{\"end\":59384,\"start\":59383},{\"end\":59386,\"start\":59385},{\"end\":59393,\"start\":59392},{\"end\":59403,\"start\":59402},{\"end\":59405,\"start\":59404},{\"end\":59565,\"start\":59564},{\"end\":59577,\"start\":59576},{\"end\":59579,\"start\":59578},{\"end\":59591,\"start\":59590},{\"end\":59601,\"start\":59600},{\"end\":59603,\"start\":59602},{\"end\":59752,\"start\":59751},{\"end\":59765,\"start\":59764},{\"end\":59779,\"start\":59778},{\"end\":59792,\"start\":59791},{\"end\":59802,\"start\":59801},{\"end\":59812,\"start\":59811},{\"end\":59938,\"start\":59937},{\"end\":59947,\"start\":59946},{\"end\":60066,\"start\":60065},{\"end\":60075,\"start\":60074},{\"end\":60084,\"start\":60083},{\"end\":60086,\"start\":60085},{\"end\":60102,\"start\":60101},{\"end\":60229,\"start\":60228},{\"end\":60231,\"start\":60230},{\"end\":60242,\"start\":60241},{\"end\":60244,\"start\":60243},{\"end\":60320,\"start\":60319},{\"end\":60329,\"start\":60328},{\"end\":60340,\"start\":60339},{\"end\":60351,\"start\":60350},{\"end\":60506,\"start\":60505},{\"end\":60517,\"start\":60516},{\"end\":60527,\"start\":60526},{\"end\":60542,\"start\":60541},{\"end\":60553,\"start\":60552},{\"end\":60695,\"start\":60694},{\"end\":60702,\"start\":60701},{\"end\":60713,\"start\":60709},{\"end\":60808,\"start\":60807},{\"end\":60810,\"start\":60809},{\"end\":60824,\"start\":60823},{\"end\":60833,\"start\":60832},{\"end\":60842,\"start\":60841},{\"end\":60852,\"start\":60851},{\"end\":60860,\"start\":60859},{\"end\":60951,\"start\":60950},{\"end\":60961,\"start\":60960},{\"end\":60972,\"start\":60971},{\"end\":60980,\"start\":60979},{\"end\":60991,\"start\":60990},{\"end\":61083,\"start\":61082},{\"end\":61093,\"start\":61092},{\"end\":61104,\"start\":61103},{\"end\":61115,\"start\":61114},{\"end\":61246,\"start\":61245},{\"end\":61259,\"start\":61258},{\"end\":61268,\"start\":61267},{\"end\":61277,\"start\":61276},{\"end\":61415,\"start\":61414},{\"end\":61427,\"start\":61426},{\"end\":61435,\"start\":61434},{\"end\":61601,\"start\":61600},{\"end\":61607,\"start\":61606},{\"end\":61616,\"start\":61615},{\"end\":61625,\"start\":61624},{\"end\":61633,\"start\":61632},{\"end\":61799,\"start\":61798},{\"end\":61811,\"start\":61810},{\"end\":61819,\"start\":61818},{\"end\":61829,\"start\":61828},{\"end\":61949,\"start\":61948},{\"end\":61956,\"start\":61955},{\"end\":62066,\"start\":62065},{\"end\":62075,\"start\":62074},{\"end\":62085,\"start\":62084},{\"end\":62094,\"start\":62093},{\"end\":62213,\"start\":62212},{\"end\":62226,\"start\":62225},{\"end\":62234,\"start\":62233},{\"end\":62242,\"start\":62241},{\"end\":62254,\"start\":62253},{\"end\":62266,\"start\":62265},{\"end\":62401,\"start\":62400},{\"end\":62410,\"start\":62409},{\"end\":62419,\"start\":62418},{\"end\":62429,\"start\":62428},{\"end\":62439,\"start\":62438},{\"end\":62447,\"start\":62446},{\"end\":62598,\"start\":62597},{\"end\":62613,\"start\":62612},{\"end\":62622,\"start\":62621},{\"end\":62632,\"start\":62631},{\"end\":62641,\"start\":62640},{\"end\":62651,\"start\":62650},{\"end\":62659,\"start\":62658},{\"end\":62670,\"start\":62669},{\"end\":62679,\"start\":62678},{\"end\":62695,\"start\":62694},{\"end\":62782,\"start\":62781},{\"end\":62784,\"start\":62783},{\"end\":62794,\"start\":62793},{\"end\":62941,\"start\":62940},{\"end\":62943,\"start\":62942},{\"end\":62951,\"start\":62950},{\"end\":63061,\"start\":63060},{\"end\":63070,\"start\":63069},{\"end\":63155,\"start\":63154},{\"end\":63164,\"start\":63163},{\"end\":63166,\"start\":63165},{\"end\":63177,\"start\":63176},{\"end\":63317,\"start\":63316},{\"end\":63325,\"start\":63324},{\"end\":63462,\"start\":63461},{\"end\":63475,\"start\":63474},{\"end\":63486,\"start\":63485},{\"end\":63564,\"start\":63563},{\"end\":63571,\"start\":63570},{\"end\":63584,\"start\":63583},{\"end\":63597,\"start\":63596},{\"end\":63605,\"start\":63604},{\"end\":63615,\"start\":63614},{\"end\":63823,\"start\":63822},{\"end\":63832,\"start\":63831},{\"end\":63843,\"start\":63842},{\"end\":63854,\"start\":63853},{\"end\":63864,\"start\":63863},{\"end\":63873,\"start\":63872},{\"end\":63881,\"start\":63880},{\"end\":63891,\"start\":63890},{\"end\":63901,\"start\":63900},{\"end\":63915,\"start\":63914},{\"end\":63924,\"start\":63923},{\"end\":64155,\"start\":64154},{\"end\":64169,\"start\":64168},{\"end\":64171,\"start\":64170},{\"end\":64177,\"start\":64176},{\"end\":64187,\"start\":64186},{\"end\":64195,\"start\":64194},{\"end\":64206,\"start\":64205},{\"end\":64208,\"start\":64207},{\"end\":64218,\"start\":64217},{\"end\":64220,\"start\":64219},{\"end\":64337,\"start\":64336},{\"end\":64345,\"start\":64344},{\"end\":64347,\"start\":64346},{\"end\":64356,\"start\":64355},{\"end\":64365,\"start\":64364},{\"end\":64375,\"start\":64374},{\"end\":64388,\"start\":64387},{\"end\":64398,\"start\":64397},{\"end\":64408,\"start\":64407},{\"end\":64522,\"start\":64521},{\"end\":64524,\"start\":64523},{\"end\":64530,\"start\":64529},{\"end\":64532,\"start\":64531},{\"end\":64647,\"start\":64646},{\"end\":64660,\"start\":64659},{\"end\":64662,\"start\":64661},{\"end\":64675,\"start\":64674},{\"end\":64679,\"start\":64676},{\"end\":64687,\"start\":64686},{\"end\":64815,\"start\":64811},{\"end\":64826,\"start\":64825},{\"end\":64836,\"start\":64835},{\"end\":64838,\"start\":64837},{\"end\":64969,\"start\":64968},{\"end\":64979,\"start\":64978},{\"end\":64991,\"start\":64990},{\"end\":65007,\"start\":65006},{\"end\":65143,\"start\":65142},{\"end\":65153,\"start\":65152},{\"end\":65164,\"start\":65163},{\"end\":65166,\"start\":65165},{\"end\":65175,\"start\":65174},{\"end\":65301,\"start\":65300},{\"end\":65303,\"start\":65302},{\"end\":65311,\"start\":65310},{\"end\":65322,\"start\":65321},{\"end\":65334,\"start\":65330},{\"end\":65341,\"start\":65340},{\"end\":65348,\"start\":65347},{\"end\":65497,\"start\":65496},{\"end\":65499,\"start\":65498},{\"end\":65511,\"start\":65510},{\"end\":65513,\"start\":65512},{\"end\":65523,\"start\":65522},{\"end\":65533,\"start\":65532},{\"end\":65543,\"start\":65542},{\"end\":65545,\"start\":65544},{\"end\":65554,\"start\":65553},{\"end\":65556,\"start\":65555},{\"end\":65709,\"start\":65708},{\"end\":65718,\"start\":65717},{\"end\":65727,\"start\":65726},{\"end\":65735,\"start\":65734},{\"end\":65746,\"start\":65745},{\"end\":65757,\"start\":65756},{\"end\":65766,\"start\":65765},{\"end\":65777,\"start\":65776},{\"end\":65935,\"start\":65934},{\"end\":65945,\"start\":65944},{\"end\":65947,\"start\":65946},{\"end\":65957,\"start\":65956},{\"end\":65967,\"start\":65966},{\"end\":65969,\"start\":65968},{\"end\":66047,\"start\":66046},{\"end\":66049,\"start\":66048},{\"end\":66115,\"start\":66114},{\"end\":66230,\"start\":66229},{\"end\":66241,\"start\":66240},{\"end\":66367,\"start\":66366},{\"end\":66487,\"start\":66486},{\"end\":66499,\"start\":66498},{\"end\":66509,\"start\":66508},{\"end\":66519,\"start\":66518},{\"end\":66529,\"start\":66528},{\"end\":66591,\"start\":66590},{\"end\":66603,\"start\":66602},{\"end\":66613,\"start\":66612},{\"end\":66625,\"start\":66624},{\"end\":66636,\"start\":66635},{\"end\":66798,\"start\":66797},{\"end\":66805,\"start\":66804},{\"end\":66813,\"start\":66812},{\"end\":66821,\"start\":66820},{\"end\":66828,\"start\":66827},{\"end\":66838,\"start\":66837},{\"end\":66945,\"start\":66944},{\"end\":66953,\"start\":66952},{\"end\":66971,\"start\":66970},{\"end\":66984,\"start\":66983},{\"end\":66994,\"start\":66993},{\"end\":67113,\"start\":67112},{\"end\":67123,\"start\":67122},{\"end\":67129,\"start\":67128},{\"end\":67139,\"start\":67138},{\"end\":67148,\"start\":67147},{\"end\":67269,\"start\":67268},{\"end\":67279,\"start\":67278},{\"end\":67296,\"start\":67295},{\"end\":67308,\"start\":67307},{\"end\":67322,\"start\":67321},{\"end\":67331,\"start\":67330},{\"end\":67339,\"start\":67338},{\"end\":67349,\"start\":67348},{\"end\":67358,\"start\":67357},{\"end\":67365,\"start\":67364},{\"end\":67469,\"start\":67468},{\"end\":67478,\"start\":67477},{\"end\":67486,\"start\":67485},{\"end\":67501,\"start\":67500},{\"end\":67509,\"start\":67508},{\"end\":67595,\"start\":67594},{\"end\":67604,\"start\":67603},{\"end\":67613,\"start\":67612},{\"end\":67623,\"start\":67622},{\"end\":67637,\"start\":67636},{\"end\":67789,\"start\":67788},{\"end\":67798,\"start\":67797},{\"end\":67808,\"start\":67807},{\"end\":67817,\"start\":67816},{\"end\":67827,\"start\":67826},{\"end\":67956,\"start\":67955},{\"end\":67968,\"start\":67967},{\"end\":67978,\"start\":67977},{\"end\":68143,\"start\":68142},{\"end\":68151,\"start\":68150},{\"end\":68164,\"start\":68163},{\"end\":68173,\"start\":68172},{\"end\":68183,\"start\":68182},{\"end\":68191,\"start\":68190},{\"end\":68199,\"start\":68198},{\"end\":68211,\"start\":68210},{\"end\":68223,\"start\":68222},{\"end\":68290,\"start\":68289},{\"end\":68299,\"start\":68298},{\"end\":68308,\"start\":68307},{\"end\":68318,\"start\":68317},{\"end\":68326,\"start\":68325},{\"end\":68332,\"start\":68331},{\"end\":68336,\"start\":68333},{\"end\":68345,\"start\":68344},{\"end\":68355,\"start\":68354},{\"end\":68370,\"start\":68369},{\"end\":68379,\"start\":68378},{\"end\":68451,\"start\":68450},{\"end\":68460,\"start\":68459},{\"end\":68479,\"start\":68478},{\"end\":68489,\"start\":68488},{\"end\":68498,\"start\":68497},{\"end\":68505,\"start\":68504},{\"end\":68514,\"start\":68513},{\"end\":68523,\"start\":68522},{\"end\":68531,\"start\":68530},{\"end\":68544,\"start\":68543},{\"end\":68691,\"start\":68690},{\"end\":68693,\"start\":68692},{\"end\":68703,\"start\":68702},{\"end\":68715,\"start\":68714},{\"end\":68809,\"start\":68808},{\"end\":68971,\"start\":68970},{\"end\":68982,\"start\":68981},{\"end\":68996,\"start\":68995},{\"end\":68998,\"start\":68997},{\"end\":69011,\"start\":69010},{\"end\":69022,\"start\":69021},{\"end\":69032,\"start\":69031},{\"end\":69041,\"start\":69040},{\"end\":69043,\"start\":69042},{\"end\":69051,\"start\":69050},{\"end\":69061,\"start\":69060},{\"end\":69071,\"start\":69070},{\"end\":69175,\"start\":69174},{\"end\":69186,\"start\":69185},{\"end\":69199,\"start\":69198},{\"end\":69210,\"start\":69209},{\"end\":69340,\"start\":69339},{\"end\":69350,\"start\":69349},{\"end\":69358,\"start\":69357},{\"end\":69491,\"start\":69490},{\"end\":69500,\"start\":69499},{\"end\":69667,\"start\":69666},{\"end\":69676,\"start\":69675},{\"end\":69689,\"start\":69688},{\"end\":69807,\"start\":69806},{\"end\":69814,\"start\":69813},{\"end\":69823,\"start\":69822},{\"end\":69833,\"start\":69832},{\"end\":69969,\"start\":69968},{\"end\":69978,\"start\":69977},{\"end\":69984,\"start\":69983},{\"end\":70000,\"start\":69999},{\"end\":70012,\"start\":70011},{\"end\":70175,\"start\":70174},{\"end\":70181,\"start\":70180},{\"end\":70192,\"start\":70191},{\"end\":70198,\"start\":70197},{\"end\":70208,\"start\":70207},{\"end\":70219,\"start\":70218},{\"end\":70227,\"start\":70226},{\"end\":70345,\"start\":70344},{\"end\":70354,\"start\":70353},{\"end\":70364,\"start\":70363},{\"end\":70373,\"start\":70372},{\"end\":70383,\"start\":70382},{\"end\":70394,\"start\":70393},{\"end\":70554,\"start\":70553},{\"end\":70563,\"start\":70562},{\"end\":70575,\"start\":70574},{\"end\":70582,\"start\":70581},{\"end\":70589,\"start\":70588},{\"end\":70601,\"start\":70600}]", "bib_author_last_name": "[{\"end\":58001,\"start\":57995},{\"end\":58009,\"start\":58007},{\"end\":58071,\"start\":58065},{\"end\":58082,\"start\":58075},{\"end\":58092,\"start\":58086},{\"end\":58102,\"start\":58096},{\"end\":58279,\"start\":58274},{\"end\":58289,\"start\":58283},{\"end\":58296,\"start\":58293},{\"end\":58306,\"start\":58300},{\"end\":58415,\"start\":58409},{\"end\":58431,\"start\":58419},{\"end\":58442,\"start\":58435},{\"end\":58452,\"start\":58446},{\"end\":58462,\"start\":58456},{\"end\":58472,\"start\":58466},{\"end\":58481,\"start\":58476},{\"end\":58493,\"start\":58485},{\"end\":58503,\"start\":58497},{\"end\":58512,\"start\":58507},{\"end\":58591,\"start\":58585},{\"end\":58605,\"start\":58595},{\"end\":58614,\"start\":58609},{\"end\":58771,\"start\":58765},{\"end\":58785,\"start\":58775},{\"end\":58794,\"start\":58789},{\"end\":58942,\"start\":58936},{\"end\":58950,\"start\":58946},{\"end\":58964,\"start\":58954},{\"end\":58978,\"start\":58968},{\"end\":58990,\"start\":58982},{\"end\":58998,\"start\":58994},{\"end\":59016,\"start\":59004},{\"end\":59025,\"start\":59020},{\"end\":59036,\"start\":59029},{\"end\":59050,\"start\":59040},{\"end\":59060,\"start\":59054},{\"end\":59072,\"start\":59064},{\"end\":59082,\"start\":59076},{\"end\":59094,\"start\":59086},{\"end\":59104,\"start\":59098},{\"end\":59111,\"start\":59108},{\"end\":59293,\"start\":59287},{\"end\":59305,\"start\":59297},{\"end\":59315,\"start\":59309},{\"end\":59327,\"start\":59319},{\"end\":59381,\"start\":59373},{\"end\":59390,\"start\":59387},{\"end\":59400,\"start\":59394},{\"end\":59413,\"start\":59406},{\"end\":59574,\"start\":59566},{\"end\":59588,\"start\":59580},{\"end\":59598,\"start\":59592},{\"end\":59618,\"start\":59604},{\"end\":59762,\"start\":59753},{\"end\":59776,\"start\":59766},{\"end\":59789,\"start\":59780},{\"end\":59799,\"start\":59793},{\"end\":59809,\"start\":59803},{\"end\":59818,\"start\":59813},{\"end\":59944,\"start\":59939},{\"end\":59954,\"start\":59948},{\"end\":60072,\"start\":60067},{\"end\":60081,\"start\":60076},{\"end\":60099,\"start\":60087},{\"end\":60109,\"start\":60103},{\"end\":60239,\"start\":60232},{\"end\":60250,\"start\":60245},{\"end\":60326,\"start\":60321},{\"end\":60337,\"start\":60330},{\"end\":60348,\"start\":60341},{\"end\":60358,\"start\":60352},{\"end\":60514,\"start\":60507},{\"end\":60524,\"start\":60518},{\"end\":60539,\"start\":60528},{\"end\":60550,\"start\":60543},{\"end\":60561,\"start\":60554},{\"end\":60699,\"start\":60696},{\"end\":60707,\"start\":60703},{\"end\":60717,\"start\":60714},{\"end\":60821,\"start\":60811},{\"end\":60830,\"start\":60825},{\"end\":60839,\"start\":60834},{\"end\":60849,\"start\":60843},{\"end\":60857,\"start\":60853},{\"end\":60867,\"start\":60861},{\"end\":60958,\"start\":60952},{\"end\":60969,\"start\":60962},{\"end\":60977,\"start\":60973},{\"end\":60988,\"start\":60981},{\"end\":60998,\"start\":60992},{\"end\":61090,\"start\":61084},{\"end\":61101,\"start\":61094},{\"end\":61112,\"start\":61105},{\"end\":61122,\"start\":61116},{\"end\":61256,\"start\":61247},{\"end\":61265,\"start\":61260},{\"end\":61274,\"start\":61269},{\"end\":61284,\"start\":61278},{\"end\":61424,\"start\":61416},{\"end\":61432,\"start\":61428},{\"end\":61442,\"start\":61436},{\"end\":61604,\"start\":61602},{\"end\":61613,\"start\":61608},{\"end\":61622,\"start\":61617},{\"end\":61630,\"start\":61626},{\"end\":61640,\"start\":61634},{\"end\":61808,\"start\":61800},{\"end\":61816,\"start\":61812},{\"end\":61826,\"start\":61820},{\"end\":61836,\"start\":61830},{\"end\":61953,\"start\":61950},{\"end\":61963,\"start\":61957},{\"end\":62072,\"start\":62067},{\"end\":62082,\"start\":62076},{\"end\":62091,\"start\":62086},{\"end\":62104,\"start\":62095},{\"end\":62223,\"start\":62214},{\"end\":62231,\"start\":62227},{\"end\":62239,\"start\":62235},{\"end\":62251,\"start\":62243},{\"end\":62263,\"start\":62255},{\"end\":62273,\"start\":62267},{\"end\":62407,\"start\":62402},{\"end\":62416,\"start\":62411},{\"end\":62426,\"start\":62420},{\"end\":62436,\"start\":62430},{\"end\":62444,\"start\":62440},{\"end\":62454,\"start\":62448},{\"end\":62610,\"start\":62599},{\"end\":62619,\"start\":62614},{\"end\":62629,\"start\":62623},{\"end\":62638,\"start\":62633},{\"end\":62648,\"start\":62642},{\"end\":62656,\"start\":62652},{\"end\":62667,\"start\":62660},{\"end\":62676,\"start\":62671},{\"end\":62692,\"start\":62680},{\"end\":62705,\"start\":62696},{\"end\":62791,\"start\":62785},{\"end\":62797,\"start\":62795},{\"end\":62803,\"start\":62799},{\"end\":62948,\"start\":62944},{\"end\":62957,\"start\":62952},{\"end\":63067,\"start\":63062},{\"end\":63077,\"start\":63071},{\"end\":63161,\"start\":63156},{\"end\":63174,\"start\":63167},{\"end\":63184,\"start\":63178},{\"end\":63322,\"start\":63318},{\"end\":63331,\"start\":63326},{\"end\":63472,\"start\":63463},{\"end\":63483,\"start\":63476},{\"end\":63495,\"start\":63487},{\"end\":63568,\"start\":63565},{\"end\":63581,\"start\":63572},{\"end\":63594,\"start\":63585},{\"end\":63602,\"start\":63598},{\"end\":63612,\"start\":63606},{\"end\":63629,\"start\":63616},{\"end\":63829,\"start\":63824},{\"end\":63840,\"start\":63833},{\"end\":63851,\"start\":63844},{\"end\":63861,\"start\":63855},{\"end\":63870,\"start\":63865},{\"end\":63878,\"start\":63874},{\"end\":63888,\"start\":63882},{\"end\":63898,\"start\":63892},{\"end\":63912,\"start\":63902},{\"end\":63921,\"start\":63916},{\"end\":63932,\"start\":63925},{\"end\":64166,\"start\":64156},{\"end\":64174,\"start\":64172},{\"end\":64184,\"start\":64178},{\"end\":64192,\"start\":64188},{\"end\":64203,\"start\":64196},{\"end\":64215,\"start\":64209},{\"end\":64228,\"start\":64221},{\"end\":64342,\"start\":64338},{\"end\":64353,\"start\":64348},{\"end\":64362,\"start\":64357},{\"end\":64372,\"start\":64366},{\"end\":64385,\"start\":64376},{\"end\":64395,\"start\":64389},{\"end\":64405,\"start\":64399},{\"end\":64420,\"start\":64409},{\"end\":64527,\"start\":64525},{\"end\":64540,\"start\":64533},{\"end\":64657,\"start\":64648},{\"end\":64672,\"start\":64663},{\"end\":64684,\"start\":64680},{\"end\":64693,\"start\":64688},{\"end\":64823,\"start\":64816},{\"end\":64833,\"start\":64827},{\"end\":64845,\"start\":64839},{\"end\":64976,\"start\":64970},{\"end\":64988,\"start\":64980},{\"end\":65004,\"start\":64992},{\"end\":65014,\"start\":65008},{\"end\":65150,\"start\":65144},{\"end\":65161,\"start\":65154},{\"end\":65172,\"start\":65167},{\"end\":65183,\"start\":65176},{\"end\":65308,\"start\":65304},{\"end\":65319,\"start\":65312},{\"end\":65328,\"start\":65323},{\"end\":65338,\"start\":65335},{\"end\":65345,\"start\":65342},{\"end\":65355,\"start\":65349},{\"end\":65508,\"start\":65500},{\"end\":65520,\"start\":65514},{\"end\":65530,\"start\":65524},{\"end\":65540,\"start\":65534},{\"end\":65551,\"start\":65546},{\"end\":65563,\"start\":65557},{\"end\":65715,\"start\":65710},{\"end\":65724,\"start\":65719},{\"end\":65732,\"start\":65728},{\"end\":65743,\"start\":65736},{\"end\":65754,\"start\":65747},{\"end\":65763,\"start\":65758},{\"end\":65774,\"start\":65767},{\"end\":65785,\"start\":65778},{\"end\":65942,\"start\":65936},{\"end\":65954,\"start\":65948},{\"end\":65964,\"start\":65958},{\"end\":65976,\"start\":65970},{\"end\":66056,\"start\":66050},{\"end\":66122,\"start\":66116},{\"end\":66238,\"start\":66231},{\"end\":66245,\"start\":66242},{\"end\":66379,\"start\":66368},{\"end\":66496,\"start\":66488},{\"end\":66506,\"start\":66500},{\"end\":66516,\"start\":66510},{\"end\":66526,\"start\":66520},{\"end\":66536,\"start\":66530},{\"end\":66600,\"start\":66592},{\"end\":66610,\"start\":66604},{\"end\":66622,\"start\":66614},{\"end\":66633,\"start\":66626},{\"end\":66643,\"start\":66637},{\"end\":66802,\"start\":66799},{\"end\":66810,\"start\":66806},{\"end\":66818,\"start\":66814},{\"end\":66825,\"start\":66822},{\"end\":66835,\"start\":66829},{\"end\":66842,\"start\":66839},{\"end\":66950,\"start\":66946},{\"end\":66968,\"start\":66954},{\"end\":66981,\"start\":66972},{\"end\":66991,\"start\":66985},{\"end\":67001,\"start\":66995},{\"end\":67120,\"start\":67114},{\"end\":67126,\"start\":67124},{\"end\":67136,\"start\":67130},{\"end\":67145,\"start\":67140},{\"end\":67156,\"start\":67149},{\"end\":67276,\"start\":67270},{\"end\":67293,\"start\":67280},{\"end\":67305,\"start\":67297},{\"end\":67319,\"start\":67309},{\"end\":67328,\"start\":67323},{\"end\":67336,\"start\":67332},{\"end\":67346,\"start\":67340},{\"end\":67355,\"start\":67350},{\"end\":67362,\"start\":67359},{\"end\":67372,\"start\":67366},{\"end\":67475,\"start\":67470},{\"end\":67483,\"start\":67479},{\"end\":67498,\"start\":67487},{\"end\":67506,\"start\":67502},{\"end\":67516,\"start\":67510},{\"end\":67601,\"start\":67596},{\"end\":67610,\"start\":67605},{\"end\":67620,\"start\":67614},{\"end\":67634,\"start\":67624},{\"end\":67645,\"start\":67638},{\"end\":67795,\"start\":67790},{\"end\":67805,\"start\":67799},{\"end\":67814,\"start\":67809},{\"end\":67824,\"start\":67818},{\"end\":67834,\"start\":67828},{\"end\":67840,\"start\":67836},{\"end\":67965,\"start\":67957},{\"end\":67975,\"start\":67969},{\"end\":67985,\"start\":67979},{\"end\":68148,\"start\":68144},{\"end\":68161,\"start\":68152},{\"end\":68170,\"start\":68165},{\"end\":68180,\"start\":68174},{\"end\":68188,\"start\":68184},{\"end\":68196,\"start\":68192},{\"end\":68208,\"start\":68200},{\"end\":68220,\"start\":68212},{\"end\":68230,\"start\":68224},{\"end\":68296,\"start\":68291},{\"end\":68305,\"start\":68300},{\"end\":68315,\"start\":68309},{\"end\":68323,\"start\":68319},{\"end\":68329,\"start\":68327},{\"end\":68342,\"start\":68337},{\"end\":68352,\"start\":68346},{\"end\":68367,\"start\":68356},{\"end\":68376,\"start\":68371},{\"end\":68388,\"start\":68380},{\"end\":68457,\"start\":68452},{\"end\":68476,\"start\":68461},{\"end\":68486,\"start\":68480},{\"end\":68495,\"start\":68490},{\"end\":68502,\"start\":68499},{\"end\":68511,\"start\":68506},{\"end\":68520,\"start\":68515},{\"end\":68528,\"start\":68524},{\"end\":68541,\"start\":68532},{\"end\":68550,\"start\":68545},{\"end\":68700,\"start\":68694},{\"end\":68712,\"start\":68704},{\"end\":68725,\"start\":68716},{\"end\":68819,\"start\":68810},{\"end\":68979,\"start\":68972},{\"end\":68993,\"start\":68983},{\"end\":69008,\"start\":68999},{\"end\":69019,\"start\":69012},{\"end\":69029,\"start\":69023},{\"end\":69038,\"start\":69033},{\"end\":69048,\"start\":69044},{\"end\":69058,\"start\":69052},{\"end\":69068,\"start\":69062},{\"end\":69080,\"start\":69072},{\"end\":69183,\"start\":69176},{\"end\":69196,\"start\":69187},{\"end\":69207,\"start\":69200},{\"end\":69216,\"start\":69211},{\"end\":69347,\"start\":69341},{\"end\":69355,\"start\":69351},{\"end\":69368,\"start\":69359},{\"end\":69497,\"start\":69492},{\"end\":69510,\"start\":69501},{\"end\":69673,\"start\":69668},{\"end\":69686,\"start\":69677},{\"end\":69697,\"start\":69690},{\"end\":69811,\"start\":69808},{\"end\":69820,\"start\":69815},{\"end\":69830,\"start\":69824},{\"end\":69838,\"start\":69834},{\"end\":69975,\"start\":69970},{\"end\":69981,\"start\":69979},{\"end\":69997,\"start\":69985},{\"end\":70009,\"start\":70001},{\"end\":70019,\"start\":70013},{\"end\":70178,\"start\":70176},{\"end\":70189,\"start\":70182},{\"end\":70195,\"start\":70193},{\"end\":70205,\"start\":70199},{\"end\":70216,\"start\":70209},{\"end\":70224,\"start\":70220},{\"end\":70234,\"start\":70228},{\"end\":70351,\"start\":70346},{\"end\":70361,\"start\":70355},{\"end\":70370,\"start\":70365},{\"end\":70380,\"start\":70374},{\"end\":70391,\"start\":70384},{\"end\":70401,\"start\":70395},{\"end\":70408,\"start\":70403},{\"end\":70560,\"start\":70555},{\"end\":70572,\"start\":70564},{\"end\":70579,\"start\":70576},{\"end\":70586,\"start\":70583},{\"end\":70598,\"start\":70590},{\"end\":70608,\"start\":70602}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207155342},\"end\":58061,\"start\":57933},{\"attributes\":{\"doi\":\"arXiv:1807.10299\",\"id\":\"b1\"},\"end\":58179,\"start\":58063},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16864200},\"end\":58365,\"start\":58181},{\"attributes\":{\"doi\":\"arXiv:1910.07113\",\"id\":\"b3\"},\"end\":58548,\"start\":58367},{\"attributes\":{\"id\":\"b4\"},\"end\":58704,\"start\":58550},{\"attributes\":{\"id\":\"b5\"},\"end\":58891,\"start\":58706},{\"attributes\":{\"doi\":\"arXiv:1606.06565\",\"id\":\"b6\",\"matched_paper_id\":51894399},\"end\":59234,\"start\":58893},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":263626240},\"end\":59369,\"start\":59236},{\"attributes\":{\"doi\":\"arXiv:1902.04257\",\"id\":\"b8\"},\"end\":59515,\"start\":59371},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5722994},\"end\":59690,\"start\":59517},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8310565},\"end\":59875,\"start\":59692},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":52957426},\"end\":59990,\"start\":59877},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":218516600},\"end\":60146,\"start\":59992},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":121987403},\"end\":60273,\"start\":60148},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":53115163},\"end\":60418,\"start\":60275},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":215540177},\"end\":60614,\"start\":60420},{\"attributes\":{\"doi\":\"arXiv:2007.12904\",\"id\":\"b16\"},\"end\":60753,\"start\":60616},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4787508},\"end\":60924,\"start\":60755},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16043466},\"end\":61035,\"start\":60926},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":11711975},\"end\":61173,\"start\":61037},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3521071},\"end\":61344,\"start\":61175},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7774489},\"end\":61502,\"start\":61346},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":44070464},\"end\":61697,\"start\":61504},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":28202810},\"end\":61888,\"start\":61699},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":232146715},\"end\":62015,\"start\":61890},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":54447842},\"end\":62156,\"start\":62017},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1620800},\"end\":62330,\"start\":62158},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":53424488},\"end\":62511,\"start\":62332},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":49470584},\"end\":62741,\"start\":62513},{\"attributes\":{\"id\":\"b29\"},\"end\":62863,\"start\":62743},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2994241},\"end\":63010,\"start\":62865},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3143757},\"end\":63106,\"start\":63012},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1932843},\"end\":63242,\"start\":63108},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":7013049},\"end\":63390,\"start\":63244},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":10837930},\"end\":63561,\"start\":63392},{\"attributes\":{\"doi\":\"arXiv:1906.05274\",\"id\":\"b35\"},\"end\":63716,\"start\":63563},{\"attributes\":{\"doi\":\"arXiv:1811.07871\",\"id\":\"b36\",\"matched_paper_id\":13072941},\"end\":64093,\"start\":63718},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":8818528},\"end\":64280,\"start\":64095},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6875312},\"end\":64472,\"start\":64282},{\"attributes\":{\"id\":\"b39\"},\"end\":64592,\"start\":64474},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":2924063},\"end\":64745,\"start\":64594},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":260429077},\"end\":64901,\"start\":64747},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":10161599},\"end\":65080,\"start\":64903},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":20045336},\"end\":65235,\"start\":65082},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":214775281},\"end\":65392,\"start\":65237},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":341617},\"end\":65622,\"start\":65394},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":3177253},\"end\":65878,\"start\":65624},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":12226563},\"end\":66013,\"start\":65880},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":64353872},\"end\":66083,\"start\":66015},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":291920},\"end\":66179,\"start\":66085},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":534730},\"end\":66304,\"start\":66181},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":234198},\"end\":66450,\"start\":66306},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":16046818},\"end\":66588,\"start\":66452},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b53\"},\"end\":66720,\"start\":66590},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":231951291},\"end\":66894,\"start\":66722},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":60441381},\"end\":67061,\"start\":66896},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":195791369},\"end\":67216,\"start\":67063},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":205261034},\"end\":67396,\"start\":67218},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":119295213},\"end\":67553,\"start\":67398},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":122506029},\"end\":67714,\"start\":67555},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":209140723},\"end\":67877,\"start\":67716},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":15950188},\"end\":68057,\"start\":67879},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":14357699},\"end\":68287,\"start\":68059},{\"attributes\":{\"doi\":\"arXiv:1801.00690\",\"id\":\"b63\"},\"end\":68448,\"start\":68289},{\"attributes\":{\"doi\":\"arXiv:2006.12983\",\"id\":\"b64\"},\"end\":68641,\"start\":68450},{\"attributes\":{\"doi\":\"arXiv:2006.06547\",\"id\":\"b65\"},\"end\":68761,\"start\":68643},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":33227681},\"end\":68892,\"start\":68763},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":204972004},\"end\":69101,\"start\":68894},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":4130751},\"end\":69261,\"start\":69103},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":6019958},\"end\":69425,\"start\":69263},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":6049287},\"end\":69612,\"start\":69427},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":17740071},\"end\":69742,\"start\":69614},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":52896545},\"end\":69874,\"start\":69744},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":7429049},\"end\":70085,\"start\":69876},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":204852201},\"end\":70270,\"start\":70087},{\"attributes\":{\"id\":\"b75\"},\"end\":70460,\"start\":70272},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":3720790},\"end\":70770,\"start\":70462},{\"attributes\":{\"id\":\"b77\"},\"end\":71992,\"start\":70772}]", "bib_title": "[{\"end\":57991,\"start\":57933},{\"end\":58270,\"start\":58181},{\"end\":58581,\"start\":58550},{\"end\":58761,\"start\":58706},{\"end\":58932,\"start\":58893},{\"end\":59281,\"start\":59236},{\"end\":59562,\"start\":59517},{\"end\":59749,\"start\":59692},{\"end\":59935,\"start\":59877},{\"end\":60063,\"start\":59992},{\"end\":60226,\"start\":60148},{\"end\":60317,\"start\":60275},{\"end\":60503,\"start\":60420},{\"end\":60805,\"start\":60755},{\"end\":60948,\"start\":60926},{\"end\":61080,\"start\":61037},{\"end\":61243,\"start\":61175},{\"end\":61412,\"start\":61346},{\"end\":61598,\"start\":61504},{\"end\":61796,\"start\":61699},{\"end\":61946,\"start\":61890},{\"end\":62063,\"start\":62017},{\"end\":62210,\"start\":62158},{\"end\":62398,\"start\":62332},{\"end\":62595,\"start\":62513},{\"end\":62779,\"start\":62743},{\"end\":62938,\"start\":62865},{\"end\":63058,\"start\":63012},{\"end\":63152,\"start\":63108},{\"end\":63314,\"start\":63244},{\"end\":63459,\"start\":63392},{\"end\":63820,\"start\":63718},{\"end\":64152,\"start\":64095},{\"end\":64334,\"start\":64282},{\"end\":64519,\"start\":64474},{\"end\":64644,\"start\":64594},{\"end\":64809,\"start\":64747},{\"end\":64966,\"start\":64903},{\"end\":65140,\"start\":65082},{\"end\":65298,\"start\":65237},{\"end\":65494,\"start\":65394},{\"end\":65706,\"start\":65624},{\"end\":65932,\"start\":65880},{\"end\":66044,\"start\":66015},{\"end\":66112,\"start\":66085},{\"end\":66227,\"start\":66181},{\"end\":66364,\"start\":66306},{\"end\":66484,\"start\":66452},{\"end\":66795,\"start\":66722},{\"end\":66942,\"start\":66896},{\"end\":67110,\"start\":67063},{\"end\":67266,\"start\":67218},{\"end\":67466,\"start\":67398},{\"end\":67592,\"start\":67555},{\"end\":67786,\"start\":67716},{\"end\":67953,\"start\":67879},{\"end\":68140,\"start\":68059},{\"end\":68806,\"start\":68763},{\"end\":68968,\"start\":68894},{\"end\":69172,\"start\":69103},{\"end\":69337,\"start\":69263},{\"end\":69488,\"start\":69427},{\"end\":69664,\"start\":69614},{\"end\":69804,\"start\":69744},{\"end\":69966,\"start\":69876},{\"end\":70172,\"start\":70087},{\"end\":70342,\"start\":70272},{\"end\":70551,\"start\":70462},{\"end\":71417,\"start\":70772}]", "bib_author": "[{\"end\":58003,\"start\":57993},{\"end\":58011,\"start\":58003},{\"end\":58073,\"start\":58063},{\"end\":58084,\"start\":58073},{\"end\":58094,\"start\":58084},{\"end\":58104,\"start\":58094},{\"end\":58281,\"start\":58272},{\"end\":58291,\"start\":58281},{\"end\":58298,\"start\":58291},{\"end\":58308,\"start\":58298},{\"end\":58417,\"start\":58407},{\"end\":58433,\"start\":58417},{\"end\":58444,\"start\":58433},{\"end\":58454,\"start\":58444},{\"end\":58464,\"start\":58454},{\"end\":58474,\"start\":58464},{\"end\":58483,\"start\":58474},{\"end\":58495,\"start\":58483},{\"end\":58505,\"start\":58495},{\"end\":58514,\"start\":58505},{\"end\":58593,\"start\":58583},{\"end\":58607,\"start\":58593},{\"end\":58616,\"start\":58607},{\"end\":58773,\"start\":58763},{\"end\":58787,\"start\":58773},{\"end\":58796,\"start\":58787},{\"end\":58944,\"start\":58934},{\"end\":58952,\"start\":58944},{\"end\":58966,\"start\":58952},{\"end\":58980,\"start\":58966},{\"end\":58992,\"start\":58980},{\"end\":59000,\"start\":58992},{\"end\":59018,\"start\":59000},{\"end\":59027,\"start\":59018},{\"end\":59038,\"start\":59027},{\"end\":59052,\"start\":59038},{\"end\":59062,\"start\":59052},{\"end\":59074,\"start\":59062},{\"end\":59084,\"start\":59074},{\"end\":59096,\"start\":59084},{\"end\":59106,\"start\":59096},{\"end\":59113,\"start\":59106},{\"end\":59295,\"start\":59283},{\"end\":59307,\"start\":59295},{\"end\":59317,\"start\":59307},{\"end\":59329,\"start\":59317},{\"end\":59383,\"start\":59371},{\"end\":59392,\"start\":59383},{\"end\":59402,\"start\":59392},{\"end\":59415,\"start\":59402},{\"end\":59576,\"start\":59564},{\"end\":59590,\"start\":59576},{\"end\":59600,\"start\":59590},{\"end\":59620,\"start\":59600},{\"end\":59764,\"start\":59751},{\"end\":59778,\"start\":59764},{\"end\":59791,\"start\":59778},{\"end\":59801,\"start\":59791},{\"end\":59811,\"start\":59801},{\"end\":59820,\"start\":59811},{\"end\":59946,\"start\":59937},{\"end\":59956,\"start\":59946},{\"end\":60074,\"start\":60065},{\"end\":60083,\"start\":60074},{\"end\":60101,\"start\":60083},{\"end\":60111,\"start\":60101},{\"end\":60241,\"start\":60228},{\"end\":60252,\"start\":60241},{\"end\":60328,\"start\":60319},{\"end\":60339,\"start\":60328},{\"end\":60350,\"start\":60339},{\"end\":60360,\"start\":60350},{\"end\":60516,\"start\":60505},{\"end\":60526,\"start\":60516},{\"end\":60541,\"start\":60526},{\"end\":60552,\"start\":60541},{\"end\":60563,\"start\":60552},{\"end\":60701,\"start\":60694},{\"end\":60709,\"start\":60701},{\"end\":60719,\"start\":60709},{\"end\":60823,\"start\":60807},{\"end\":60832,\"start\":60823},{\"end\":60841,\"start\":60832},{\"end\":60851,\"start\":60841},{\"end\":60859,\"start\":60851},{\"end\":60869,\"start\":60859},{\"end\":60960,\"start\":60950},{\"end\":60971,\"start\":60960},{\"end\":60979,\"start\":60971},{\"end\":60990,\"start\":60979},{\"end\":61000,\"start\":60990},{\"end\":61092,\"start\":61082},{\"end\":61103,\"start\":61092},{\"end\":61114,\"start\":61103},{\"end\":61124,\"start\":61114},{\"end\":61258,\"start\":61245},{\"end\":61267,\"start\":61258},{\"end\":61276,\"start\":61267},{\"end\":61286,\"start\":61276},{\"end\":61426,\"start\":61414},{\"end\":61434,\"start\":61426},{\"end\":61444,\"start\":61434},{\"end\":61606,\"start\":61600},{\"end\":61615,\"start\":61606},{\"end\":61624,\"start\":61615},{\"end\":61632,\"start\":61624},{\"end\":61642,\"start\":61632},{\"end\":61810,\"start\":61798},{\"end\":61818,\"start\":61810},{\"end\":61828,\"start\":61818},{\"end\":61838,\"start\":61828},{\"end\":61955,\"start\":61948},{\"end\":61965,\"start\":61955},{\"end\":62074,\"start\":62065},{\"end\":62084,\"start\":62074},{\"end\":62093,\"start\":62084},{\"end\":62106,\"start\":62093},{\"end\":62225,\"start\":62212},{\"end\":62233,\"start\":62225},{\"end\":62241,\"start\":62233},{\"end\":62253,\"start\":62241},{\"end\":62265,\"start\":62253},{\"end\":62275,\"start\":62265},{\"end\":62409,\"start\":62400},{\"end\":62418,\"start\":62409},{\"end\":62428,\"start\":62418},{\"end\":62438,\"start\":62428},{\"end\":62446,\"start\":62438},{\"end\":62456,\"start\":62446},{\"end\":62612,\"start\":62597},{\"end\":62621,\"start\":62612},{\"end\":62631,\"start\":62621},{\"end\":62640,\"start\":62631},{\"end\":62650,\"start\":62640},{\"end\":62658,\"start\":62650},{\"end\":62669,\"start\":62658},{\"end\":62678,\"start\":62669},{\"end\":62694,\"start\":62678},{\"end\":62707,\"start\":62694},{\"end\":62793,\"start\":62781},{\"end\":62799,\"start\":62793},{\"end\":62805,\"start\":62799},{\"end\":62950,\"start\":62940},{\"end\":62959,\"start\":62950},{\"end\":63069,\"start\":63060},{\"end\":63079,\"start\":63069},{\"end\":63163,\"start\":63154},{\"end\":63176,\"start\":63163},{\"end\":63186,\"start\":63176},{\"end\":63324,\"start\":63316},{\"end\":63333,\"start\":63324},{\"end\":63474,\"start\":63461},{\"end\":63485,\"start\":63474},{\"end\":63497,\"start\":63485},{\"end\":63570,\"start\":63563},{\"end\":63583,\"start\":63570},{\"end\":63596,\"start\":63583},{\"end\":63604,\"start\":63596},{\"end\":63614,\"start\":63604},{\"end\":63631,\"start\":63614},{\"end\":63831,\"start\":63822},{\"end\":63842,\"start\":63831},{\"end\":63853,\"start\":63842},{\"end\":63863,\"start\":63853},{\"end\":63872,\"start\":63863},{\"end\":63880,\"start\":63872},{\"end\":63890,\"start\":63880},{\"end\":63900,\"start\":63890},{\"end\":63914,\"start\":63900},{\"end\":63923,\"start\":63914},{\"end\":63934,\"start\":63923},{\"end\":64168,\"start\":64154},{\"end\":64176,\"start\":64168},{\"end\":64186,\"start\":64176},{\"end\":64194,\"start\":64186},{\"end\":64205,\"start\":64194},{\"end\":64217,\"start\":64205},{\"end\":64230,\"start\":64217},{\"end\":64344,\"start\":64336},{\"end\":64355,\"start\":64344},{\"end\":64364,\"start\":64355},{\"end\":64374,\"start\":64364},{\"end\":64387,\"start\":64374},{\"end\":64397,\"start\":64387},{\"end\":64407,\"start\":64397},{\"end\":64422,\"start\":64407},{\"end\":64529,\"start\":64521},{\"end\":64542,\"start\":64529},{\"end\":64659,\"start\":64646},{\"end\":64674,\"start\":64659},{\"end\":64686,\"start\":64674},{\"end\":64695,\"start\":64686},{\"end\":64825,\"start\":64811},{\"end\":64835,\"start\":64825},{\"end\":64847,\"start\":64835},{\"end\":64978,\"start\":64968},{\"end\":64990,\"start\":64978},{\"end\":65006,\"start\":64990},{\"end\":65016,\"start\":65006},{\"end\":65152,\"start\":65142},{\"end\":65163,\"start\":65152},{\"end\":65174,\"start\":65163},{\"end\":65185,\"start\":65174},{\"end\":65310,\"start\":65300},{\"end\":65321,\"start\":65310},{\"end\":65330,\"start\":65321},{\"end\":65340,\"start\":65330},{\"end\":65347,\"start\":65340},{\"end\":65357,\"start\":65347},{\"end\":65510,\"start\":65496},{\"end\":65522,\"start\":65510},{\"end\":65532,\"start\":65522},{\"end\":65542,\"start\":65532},{\"end\":65553,\"start\":65542},{\"end\":65565,\"start\":65553},{\"end\":65717,\"start\":65708},{\"end\":65726,\"start\":65717},{\"end\":65734,\"start\":65726},{\"end\":65745,\"start\":65734},{\"end\":65756,\"start\":65745},{\"end\":65765,\"start\":65756},{\"end\":65776,\"start\":65765},{\"end\":65787,\"start\":65776},{\"end\":65944,\"start\":65934},{\"end\":65956,\"start\":65944},{\"end\":65966,\"start\":65956},{\"end\":65978,\"start\":65966},{\"end\":66058,\"start\":66046},{\"end\":66124,\"start\":66114},{\"end\":66240,\"start\":66229},{\"end\":66247,\"start\":66240},{\"end\":66381,\"start\":66366},{\"end\":66498,\"start\":66486},{\"end\":66508,\"start\":66498},{\"end\":66518,\"start\":66508},{\"end\":66528,\"start\":66518},{\"end\":66538,\"start\":66528},{\"end\":66602,\"start\":66590},{\"end\":66612,\"start\":66602},{\"end\":66624,\"start\":66612},{\"end\":66635,\"start\":66624},{\"end\":66645,\"start\":66635},{\"end\":66804,\"start\":66797},{\"end\":66812,\"start\":66804},{\"end\":66820,\"start\":66812},{\"end\":66827,\"start\":66820},{\"end\":66837,\"start\":66827},{\"end\":66844,\"start\":66837},{\"end\":66952,\"start\":66944},{\"end\":66970,\"start\":66952},{\"end\":66983,\"start\":66970},{\"end\":66993,\"start\":66983},{\"end\":67003,\"start\":66993},{\"end\":67122,\"start\":67112},{\"end\":67128,\"start\":67122},{\"end\":67138,\"start\":67128},{\"end\":67147,\"start\":67138},{\"end\":67158,\"start\":67147},{\"end\":67278,\"start\":67268},{\"end\":67295,\"start\":67278},{\"end\":67307,\"start\":67295},{\"end\":67321,\"start\":67307},{\"end\":67330,\"start\":67321},{\"end\":67338,\"start\":67330},{\"end\":67348,\"start\":67338},{\"end\":67357,\"start\":67348},{\"end\":67364,\"start\":67357},{\"end\":67374,\"start\":67364},{\"end\":67477,\"start\":67468},{\"end\":67485,\"start\":67477},{\"end\":67500,\"start\":67485},{\"end\":67508,\"start\":67500},{\"end\":67518,\"start\":67508},{\"end\":67603,\"start\":67594},{\"end\":67612,\"start\":67603},{\"end\":67622,\"start\":67612},{\"end\":67636,\"start\":67622},{\"end\":67647,\"start\":67636},{\"end\":67797,\"start\":67788},{\"end\":67807,\"start\":67797},{\"end\":67816,\"start\":67807},{\"end\":67826,\"start\":67816},{\"end\":67836,\"start\":67826},{\"end\":67842,\"start\":67836},{\"end\":67967,\"start\":67955},{\"end\":67977,\"start\":67967},{\"end\":67987,\"start\":67977},{\"end\":68150,\"start\":68142},{\"end\":68163,\"start\":68150},{\"end\":68172,\"start\":68163},{\"end\":68182,\"start\":68172},{\"end\":68190,\"start\":68182},{\"end\":68198,\"start\":68190},{\"end\":68210,\"start\":68198},{\"end\":68222,\"start\":68210},{\"end\":68232,\"start\":68222},{\"end\":68298,\"start\":68289},{\"end\":68307,\"start\":68298},{\"end\":68317,\"start\":68307},{\"end\":68325,\"start\":68317},{\"end\":68331,\"start\":68325},{\"end\":68344,\"start\":68331},{\"end\":68354,\"start\":68344},{\"end\":68369,\"start\":68354},{\"end\":68378,\"start\":68369},{\"end\":68390,\"start\":68378},{\"end\":68459,\"start\":68450},{\"end\":68478,\"start\":68459},{\"end\":68488,\"start\":68478},{\"end\":68497,\"start\":68488},{\"end\":68504,\"start\":68497},{\"end\":68513,\"start\":68504},{\"end\":68522,\"start\":68513},{\"end\":68530,\"start\":68522},{\"end\":68543,\"start\":68530},{\"end\":68552,\"start\":68543},{\"end\":68702,\"start\":68690},{\"end\":68714,\"start\":68702},{\"end\":68727,\"start\":68714},{\"end\":68821,\"start\":68808},{\"end\":68981,\"start\":68970},{\"end\":68995,\"start\":68981},{\"end\":69010,\"start\":68995},{\"end\":69021,\"start\":69010},{\"end\":69031,\"start\":69021},{\"end\":69040,\"start\":69031},{\"end\":69050,\"start\":69040},{\"end\":69060,\"start\":69050},{\"end\":69070,\"start\":69060},{\"end\":69082,\"start\":69070},{\"end\":69185,\"start\":69174},{\"end\":69198,\"start\":69185},{\"end\":69209,\"start\":69198},{\"end\":69218,\"start\":69209},{\"end\":69349,\"start\":69339},{\"end\":69357,\"start\":69349},{\"end\":69370,\"start\":69357},{\"end\":69499,\"start\":69490},{\"end\":69512,\"start\":69499},{\"end\":69675,\"start\":69666},{\"end\":69688,\"start\":69675},{\"end\":69699,\"start\":69688},{\"end\":69813,\"start\":69806},{\"end\":69822,\"start\":69813},{\"end\":69832,\"start\":69822},{\"end\":69840,\"start\":69832},{\"end\":69977,\"start\":69968},{\"end\":69983,\"start\":69977},{\"end\":69999,\"start\":69983},{\"end\":70011,\"start\":69999},{\"end\":70021,\"start\":70011},{\"end\":70180,\"start\":70174},{\"end\":70191,\"start\":70180},{\"end\":70197,\"start\":70191},{\"end\":70207,\"start\":70197},{\"end\":70218,\"start\":70207},{\"end\":70226,\"start\":70218},{\"end\":70236,\"start\":70226},{\"end\":70353,\"start\":70344},{\"end\":70363,\"start\":70353},{\"end\":70372,\"start\":70363},{\"end\":70382,\"start\":70372},{\"end\":70393,\"start\":70382},{\"end\":70403,\"start\":70393},{\"end\":70410,\"start\":70403},{\"end\":70562,\"start\":70553},{\"end\":70574,\"start\":70562},{\"end\":70581,\"start\":70574},{\"end\":70588,\"start\":70581},{\"end\":70600,\"start\":70588},{\"end\":70610,\"start\":70600}]", "bib_venue": "[{\"end\":58055,\"start\":58011},{\"end\":58159,\"start\":58120},{\"end\":58359,\"start\":58308},{\"end\":58405,\"start\":58367},{\"end\":58698,\"start\":58616},{\"end\":58878,\"start\":58796},{\"end\":59175,\"start\":59129},{\"end\":59360,\"start\":59329},{\"end\":59495,\"start\":59431},{\"end\":59682,\"start\":59620},{\"end\":59869,\"start\":59820},{\"end\":59984,\"start\":59956},{\"end\":60140,\"start\":60111},{\"end\":60262,\"start\":60252},{\"end\":60412,\"start\":60360},{\"end\":60608,\"start\":60563},{\"end\":60692,\"start\":60616},{\"end\":60918,\"start\":60869},{\"end\":61029,\"start\":61000},{\"end\":61164,\"start\":61124},{\"end\":61338,\"start\":61286},{\"end\":61496,\"start\":61444},{\"end\":61691,\"start\":61642},{\"end\":61882,\"start\":61838},{\"end\":62009,\"start\":61965},{\"end\":62150,\"start\":62106},{\"end\":62324,\"start\":62275},{\"end\":62505,\"start\":62456},{\"end\":62735,\"start\":62707},{\"end\":62857,\"start\":62805},{\"end\":63004,\"start\":62959},{\"end\":63095,\"start\":63079},{\"end\":63232,\"start\":63186},{\"end\":63384,\"start\":63333},{\"end\":63555,\"start\":63497},{\"end\":63696,\"start\":63647},{\"end\":63996,\"start\":63950},{\"end\":64274,\"start\":64230},{\"end\":64466,\"start\":64422},{\"end\":64586,\"start\":64542},{\"end\":64739,\"start\":64695},{\"end\":64892,\"start\":64847},{\"end\":65074,\"start\":65016},{\"end\":65229,\"start\":65185},{\"end\":65386,\"start\":65357},{\"end\":65616,\"start\":65565},{\"end\":65838,\"start\":65787},{\"end\":66007,\"start\":65978},{\"end\":66077,\"start\":66058},{\"end\":66173,\"start\":66124},{\"end\":66298,\"start\":66247},{\"end\":66431,\"start\":66381},{\"end\":66582,\"start\":66538},{\"end\":66700,\"start\":66661},{\"end\":66888,\"start\":66844},{\"end\":67055,\"start\":67003},{\"end\":67210,\"start\":67158},{\"end\":67380,\"start\":67374},{\"end\":67547,\"start\":67518},{\"end\":67703,\"start\":67647},{\"end\":67871,\"start\":67842},{\"end\":68051,\"start\":67987},{\"end\":68281,\"start\":68232},{\"end\":68428,\"start\":68406},{\"end\":68621,\"start\":68568},{\"end\":68688,\"start\":68643},{\"end\":68886,\"start\":68821},{\"end\":69088,\"start\":69082},{\"end\":69255,\"start\":69218},{\"end\":69419,\"start\":69370},{\"end\":69606,\"start\":69512},{\"end\":69736,\"start\":69699},{\"end\":69868,\"start\":69840},{\"end\":70079,\"start\":70021},{\"end\":70264,\"start\":70236},{\"end\":70454,\"start\":70410},{\"end\":70707,\"start\":70610},{\"end\":71548,\"start\":71419}]"}}}, "year": 2023, "month": 12, "day": 17}