{"id": 259924919, "updated": "2023-10-04 22:16:29.999", "metadata": {"title": "Software Testing with Large Language Model: Survey, Landscape, and Vision", "authors": "[{\"first\":\"Junjie\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yuchao\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Chunyang\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Zhe\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Song\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Qing\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, and making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 52 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative ones. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2307.07221", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2307-07221", "doi": "10.48550/arxiv.2307.07221"}}, "content": {"source": {"pdf_hash": "f43b8a87a96f8abc2467b90538b643a6061416e9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2307.07221v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f1201e5cc412c1eae118d0a8575af9340f993e70", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f43b8a87a96f8abc2467b90538b643a6061416e9.txt", "contents": "\nSoftware Testing with Large Language Model: Survey, Landscape, and Vision\n\n\nJunjie Wang \nYuchao Huang \nChunyang Chen \nZhe Liu \nSong Wang \nQing Wang \nSoftware Testing with Large Language Model: Survey, Landscape, and Vision\n1Index Terms-Pre-trained Large Language ModelSoftware TestingLLM !\nPre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, and making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 52 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative ones. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.\n\nINTRODUCTION\n\nSoftware testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. Without the rigorous process of software testing, software enterprises would be reluctant to release their products into the market, knowing the potential consequences of delivering flawed software to end-users. By conducting thorough and meticulous testing procedures, software enterprises can minimize the occurrence of critical software failures, usability issues, or security breaches that could potentially lead to financial losses or jeopardize user trust. Additionally, software testing helps to reduce maintenance costs by identifying and resolving issues early in the development lifecycle, preventing more significant complications down the line.\n\nThe significance of software testing has garnered substantial attention within the research and industrial communities. In the field of software engineering, it stands as an immensely popular and vibrant research area. One can observe the undeniable prominence of software testing by simply examining the landscape of conferences and symposiums focused on software engineering. Amongst these events, topics related to software testing consistently dominate the submission numbers and are frequently selected for publication.\n\nWhile the field of software testing has gained significant popularity, there still remain dozens of challenges that have not been effectively addressed. For example, one such challenge is automated unit test cases generation. Although various approaches, including search-based [1], [2], constraint-based [3] or random-based [4] techniques to generate a suite of unit tests, the coverage and the meaningfulness of the generated tests are still far from satisfactory. Similarly, when it comes to mobile GUI testing, existing studies with random-/rule-based methods [5], [6], model-based methods [7], [8], and learning-based methods [9] are unable to understand the semantic information of the GUI page and often fall short in achieving comprehensive coverage. Considering these limitations, numerous research efforts are currently underway to explore innovative techniques that can enhance the efficacy of software testing tasks, among which large language models are the most promising ones.\n\nLarge language models (LLMs) such as T5, GPT-3 have revolutionized the field of natural language processing (NLP) and artificial intelligence (AI). These models, initially pre-trained on extensive corpora, have exhibited remarkable performance across a wide range of NLP tasks including question answering, machine translation, and text generation. In recent years, there has been a significant advancement in LLMs with the emergence of models capable of handling even larger-scale datasets. This expansion in model size has not only led to improved performance but also opened up new possibilities for applying LLMs as Artificial General Intelligence. Among these advanced LLMs, models like ChatGPT 1 and LLaMA 2 boast billions of parameters. Such models hold tremendous potential for tackling complex practical tasks in domains like code generation and artistic creation. With their expanded capacity and enhanced capabilities, LLMs have become game-changers in NLP and AI, and are driving advancements in other fields like coding, software testing.\n\nActually LLMs have been used for various coding related tasks including code generation and code recommendation [10], [11]. However, there have been concerns about the correctness and reliability of the code generated by LLMs, as some studies have shown that the code generated by LLMs may not always be correct, or may not meet the expected software requirements. By comparison, when LLMs are used for software testing tasks, such as generating test cases or validating the correctness of software behavior, the impact of this problem is relatively weaker. This is because the primary goal of software testing is to identify issues or problems in the software system, rather than to generate correct code or meet specific software requirements. At worst, the only consequence is that the corresponding defects are not discovered. Furthermore, in some cases, the seemingly incorrect outputs from LLMs may actually be beneficial for testing corner cases in software, and can help uncover defects. Taken in this sense, we think the LLMs are a natural match with software testing.\n\nThis article presents a comprehensive review of the utilization of LLMs in software testing. We collect 52 relevant papers and conduct a thorough analysis from both software testing and LLMs perspectives, as roughly summarized in Figure 1.\n\nFrom the viewpoints of software testing, our analysis involves an examination of the specific software testing tasks for which LLMs are employed. Results show that LLMs are commonly used for test case preparation (including unit test case generation, test oracle generation, and test input generation), program debug and repair, while we do not find the practices for applying LLMs in the tasks of early testing life-cycle (such as test requirements, test plan, etc).\n\nFrom the viewpoints of LLMs, our analysis includes the commonly used LLMs in these studies, the types of prompt engineering, input of the LLMs, as well as the accompanied techniques with these LLMs. Results show that about one third of the studies utilize the LLMs through pre-training or fine-tuning schema, while the others employ the prompt engineering to communicate with LLMs to steer its behavior for desired outcomes. For prompt engineering, the zeroshot learning and few-shot learning are most commonly used, while other advances ones like chain-of-thought promoting and self-consistency are rarely utilized. Results also show that traditional testing techniques like differential testing and mutation testing are usually accompanied with the LLMs to help generate more diversified tests.\n\nFurthermore, we summarize the key challenges and potential opportunities in this direction. These encompass leveraging LLMs for a wider array of software testing tasks and stages, extending their applications to a broader range of testing types and software categories, providing more comprehensive benchmark datasets along with rigorous experimental validations, incorporating advanced prompt This paper makes the following contributions:\n\n\u2022 We thoroughly analyze 52 relevant studies that used LLMs for software testing, regarding publication trends, distribution of publication venues, etc.\n\n\u2022 We conduct a comprehensive analysis to understand the distribution of software testing tasks with LLM, and present a thorough discussion about how these tasks are solved with LLM.\n\n\u2022 We conduct a comprehensive analysis from the perspective of LLMs, and uncover the commonly-used LLMs, the types of prompt engineering, input of the LLMs, as well as the accompanied techniques with these LLMs.\n\n\u2022 We highlight the challenges in existing studies and present potential opportunities for further studies. We believe that this work will be valuable to both researchers and practitioners in the field of software engineering, as it provides a comprehensive overview of the current state and future vision of using LLMs for software testing. For researchers, this work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration and identifying gaps in our current understanding of the use of LLMs in software testing. For practitioners, this work can provide insights into the potential benefits and limitations of using LLMs for software testing, as well as practical guidance on how to effectively integrate them into existing testing processes. By providing a detailed landscape of the current state and future vision of using LLMs for software testing, this work can help accelerate the adoption of this technology in the software engineering community and ultimately contribute to improving the quality and reliability of software systems.\n\n\nBACKGROUND\n\n\nLarge Language Model (LLM)\n\nRecently, pre-trained language models (PLMs) have been proposed by pretraining Transformer-based models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks [12]- [15]. Studies have shown that model scaling can lead to improved model capacity, prompting researchers to investigate the scaling effect through further parameter size increases. Interestingly, when the parameter scale exceeds a certain threshold, these larger language models demonstrate not only significant performance improvements, but also special abilities such as in-context learning, which are absent in smaller models such as BERT.\n\nTo discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size. LLMs typically refer to language models that have hundreds of billions (or more) of parameters, and are trained on massive text data such as GPT-3, PaLM, Codex, and LLaMA. LLMs are built using the Transformer architecture, which stacks multi-head attention layers in a very deep neural network. Existing LLMs adopt similar model architectures (Transformer) and pre-training objectives (language modeling) as small language models, but largely scale up the model size, pre-training data, and total compute power. This enables LLMs to better understand natural language and generate high-quality text based on given context or prompts.\n\nNote that, in existing literature, there is no formal consensus on the minimum parameter scale for LLMs, since the model capacity is also related to data size and total compute. In a recent survey of LLMs [13], the authors focuses on discussing the language models with a model size larger than 10B. Under their criteria, the first LLM is T5 released by Google in 2019, followed by GPT-3 released by OpenAI in 2020, and there are more than thirty LLMs released between 2021 and 2023 indicating its popularity. In another survey of unifying LLMs and knowledge graphs [16], the authors categorize the LLMs into three types: encoder-only (e.g., BERT), encoder-decoder (e.g., T5), and decoder-only network architecture (e.g., GPT-3). In our review, we take into account the categorization criteria of the two surveys and only consider the encoder-decoder and decoder-only network architecture of pre-training language models, since they can both support generative tasks. We do not consider the encoder-only network architecture because they cannot handle generative tasks, were proposed relatively early (e.g., BERT in 2018), and there is almost no models using this architecture after 2021. In other words, the LLMs discussed in this paper not only include models with parameters of over 10B (as mentioned in [13]), but also include other models that use the encoder-decoder and decoder-only network architecture (as mentioned in [16]), such as BART with 140M parameters and GPT-2 with parameter sizes ranging from 117M to 1.5B. This is also to potentially include more studies to demonstrate the landscape of this topic.\n\n\nSoftware Testing\n\nSoftware testing is a crucial process in software development that involves evaluating the quality of a software product. The primary goal of software testing is to identify defects or errors in the software system that could potentially lead to incorrect or unexpected behavior. The whole life cycle of software testing typically includes the following tasks (demonstrated in Figure 4):\n\n\u2022 Requirements Analysis: analyze the software requirements and identify the testing objectives, scope, and criteria.\n\n\u2022 Test Plan: develop a test plan that outlines the testing strategy, test objectives, and schedule.\n\n\u2022 Test Design and Review: develop and review the test cases and test suites that align with the test plan and the requirements of the software application.\n\n\u2022 Test Case Preparation: the actual test cases are prepared based on the designs created in previous stage.\n\n\u2022 Test Execution: execute the tests that were designed in the previous stage. The software system is executed with the test cases and the results are recorded.\n\n\u2022 Test Reporting: analyze the results of the tests and generate reports that summarize the testing process and identify any defects or issues that were discovered.\n\n\u2022 Bug Fixing and Regression Testing: defects or issues identified during testing are reported to the development team for fixing. Once the defects are fixed, regression testing is performed to ensure that the changes have not introduced new defects or issues.\n\n\u2022 Software Release: once the software system has passed all of the testing stages and the defects have been fixed, the software can be released to the customer or end user.\n\nThe testing process is iterative and may involve multiple cycles of the above stages, depending on the complexity of the software system and the testing requirements.\n\nDuring the testing phase, various types of tests may be performed, including unit tests, integration tests, system tests, and acceptance tests.\n\n\u2022 Unit Testing involves testing individual units or components of the software application to ensure that they function correctly.\n\n\u2022 Integration Testing involves testing different modules or components of the software application together to ensure that they work correctly as a system.\n\n\u2022 System Testing involves testing the entire software system as a whole, including all the integrated components and external dependencies.\n\n\u2022 Acceptance Testing involves testing the software application to ensure that it meets the business requirements and is ready for deployment.\n\nIn addition, there can be functional testing, performance testing, unit testing, security testing, accessibility testing, and etc, which explores various aspects of the software under test [17].\n\n\nPAPER SELECTION AND REVIEW SCHEMA\n\n\nSurvey Scope\n\nThe scope of our paper is software testing with LLMs. We apply the following inclusion criteria when collecting papers. If a paper satisfies any of the following criteria, we will include it.\n\n\u2022 The paper proposes or improves an approach, study, or tool/framework that targets testing specific software or systems with LLMs.\n\n\u2022 The paper applies LLMs to software testing practice, including all tasks within software testing lifecyle as demonstrated in Section 2.2.\n\n\u2022 The paper presents an empirical or experimental study about utilizing LLMs to software testing practice.\n\n\u2022 The paper involves specific testing techniques (e.g., fuzz testing) employing LLMs. In addition, the following studies would be excluded during study selection:\n\n\u2022 The paper does not involve software testing tasks, e.g., code comment generation.\n\n\u2022 The paper does not utilize LLMs, e.g., using recurrent neural networks.\n\n\u2022 The paper mentions LLMs only in future work or discussions rather than using LLMs in the approach.\n\n\u2022 The paper utilizes language models with encoder-only architecture, e.g., BERT, which can not directly be utilized for generation tasks (as demonstrated in Section 2.1).\n\n\u2022 The paper focuses on testing the performance of LLMs, such as fairness, stability, security, etc. [18]- [20] \u2022 The paper focuses on evaluating the performance of LLM-enabled tool, e.g., evaluating the code quality of code generation tool Copilot [21]- [23].\n\n\nPaper Collection Methodology\n\nTo ensure that we collect papers from diverse research areas, we conduct an extensive search using four popular scientific databases: ACM digital library, IEEE Xplore digital library, arXiv, and DBLP. We search for papers whose titles contained keywords related to software testing tasks and testing techniques in the first three databases. In the case of DBLP, we use additional keywords related to LLMs to filter out irrelevant studies, as relying solely on testing-related keywords would result in a large number of candidate studies. We use two sets of keywords for DBLP because a significant portion of the studies in this database can be found in the first three databases, thus this fourth database serves as a supplementary source of paper collection.\n\n\u2022 Keywords related with software testing: test |bug |issue |defect |terms of testing tasks (e.g., debug, repair) |terms of testing techniques (e.g., fuzz, mutation, metamorphic).\n\n\u2022 Keywords related with LLMs: LLM |language model |name of the LLM (e.g., ChatGPT). We use the LLMs in [13] and [16] except those in our exclusion criteria.\n\nTo compensate for the potential omissions that may result from automated searches, we also conduct manual searches. Specifically, we manually review the titles and abstracts of papers presented at major conferences in the software engineering field to include additional possible papers.\n\nWe conduct both the keywords search and the manual search about the papers from Jan. 2019 to Apr. 2023. We further conduct the second round of paper search to include the papers from Apr. 2023 to Jun. 2023.\n\nTo further determine which papers are relevant to this survey, we conduct a three-stage paper filtering. First, we automatically filter the papers whose abstract include words \"model\". Second, we automatically filter the paper whose content contains the name of the LLMs, using the LLMs in [13], [16] except those in our exclusion criteria. Third, we conduct manual inspection to check whether its satisfies our inclusion criteria. The first filtering stage is mainly to eliminate the papers which donot involve the neural models, and the second filtering stage is mainly to eliminate papers that leverage machine learning, deep learning, etc. In the final filtering stage, two authors read each paper to carefully determine whether it should be included based on the inclusion criteria and exclusion criteria, and any paper with different decision will be handed over to a third author to make the final decision.\n\nIn addition, we establish quality assessment criteria to exclude low-quality studies as shown below. For each question, the study's quality is rated as \"yes\", \"partial\" or \"no\" which are assigned values of 1, 0.5, and 0, respectively. Papers with a total score of less than 8 will be excluded from our study.\n\n\u2022 Is there a clearly stated research goal related to software testing?\n\n\u2022 Is there a defined and repeatable technique?\n\n\u2022 Is there any explicit contribution to software testing?\n\n\u2022 Is there an explicit description about which LLMs are utilized?\n\n\u2022 Is there an explicit explanation about how the LLMs are utilized?\n\n\u2022 Is there a clear methodology for validating the technique?\n\n\u2022 Are the subject projects selected for validation suitable for the research goals?\n\n\u2022 Are there control techniques or baselines to demonstrate the effectiveness of the proposed technique?\n\n\u2022 Are the evaluation metrics relevant (e.g., evaluate the effectiveness of the proposed technique) to the research objectives?\n\n\u2022 Do the results presented in the study align with the research objectives and are they presented in a clear and relevant manner?\n\n\nCollection Results\n\nWe retrieve a total of 14,623 papers from four databases by keyword searching. After the first filtering stage, we obtain 8,748 papers that may use the neural model. After the second filtering stage, we obtain 922 papers that may use the LLM. After the final filtering and quality assessment stage, we collect a total of 52 papers involving the software testing with LLMs. Table 1 shows the details of the collected papers. Note that, there are two papers which are the extension of previous publications from the same authors. In the  [74] Note that: we put [36] and [37], [48] and [49] together considering they are respectively the original paper and its extension by the same authors. following analysis, we only include the extended paper. In other words, the following analysis is based on 50 collected studies.\n\n\nGeneral Overview of Collected Paper\n\nAmong the papers, 38% papers are published in software engineering venues, among which 7 papers are from ICSE, . This is understandable because this field is emerging and many works are just completed and in the process of submission. Although these papers did not undergo peer review, we have a quality assessment process - [77], the number in bracket indicates the number of collected studies per task, and one paper might involve multiple tasks) that eliminates papers with low quality, which potentially ensures the quality of this survey. Figure 2 demonstrates the trend of our collected papers per year. We can see that as the years go by, the number of papers in this field is growing almost exponentially. In 2020 and 2021, there were only 1 and 3 papers, respectively. In 2022, there were 13 papers, and in the first half of 2023, there were already 33 papers. It is conceivable that there will be even more papers in the future, which indicates the popularity and attention that this field is receiving.\n\nTo provide a general overview of these papers, Figure 3 shows the word cloud created by the abstracts of the collected papers, from which we can generally observe the involved topics in these collected papers.\n\n\nANALYSIS FROM SOFTWARE TESTING\n\nThis section conducts the analysis based on the viewpoints of software testing, and will organize the collected studies in terms of testing tasks. Figure 4 lists the distribution of each involved testing tasks, aligned with the software testing life cycle. We first provide some general overview of the distribution, following by the further analysis for each task. NOTE that: for each following subsection, the cumulative total of subcategories may not always match the total number of papers since a paper might belong to more than one subcategory.\n\nWe can see that LLMs have been effectively used in both mid to late stages of the software testing lifecycle. In the test case preparation phase, LLMs have been utilized for tasks such as generating unit test cases, test oracle generation, test input generation. These tasks are crucial in the mid-phase of software testing to help catch issues and prevent further development until issues are resolved. Furthermore, in later phases such as the test report/bug reports, bug fix and regression test phase, LLMs have been employed for tasks such as bug analysis, debugging, and repair. These tasks are critical towards the end of the testing phase when software bugs need to be resolved to prepare for the product's release.\n\n\nUnit Test Case Generation\n\nUnit test case generation involves writing unit test cases to check individual units/components of the software independently and ensure that they work correctly. For a method under test (i.e., often called as the focal method), its corresponding unit test consists of a test prefix and a test oracle. In particular, the test prefix is typically a series of method invocation statements or assignment statements, which aims at driving the focal method to a testable state; and then the test oracle serves as the specification to check whether the current behavior of the focal method satisfies the expected one, e.g., the test assertion.\n\nTo alleviate manual efforts in writing unit tests, researchers have proposed various techniques to facilitate automated unit test generation. Traditional unit test generation techniques leverage search-based [1], [2], constraint-based [3] or random-based strategies [4] to generate a suite of unit tests with the main goal of maximizing the coverage in the software under test. Nevertheless, the coverage and the meaningfulness of the generated tests are still far from satisfactory.\n\nSince LLMs have demonstrated promising results in tasks such as code generation, and given that both code generation and unit test case generation involve generating source code, recent research has extended the domain of code generation to encompass unit test case generation. Despite initial success, there are nuances that set unit test case generation apart from general code generation, signaling the need for more tailored approaches.\n\nEarly practices for fine-tuning LLMs. The early practices mainly focus on how to pre-train or fine-tune LLMs with domain-specific data, in order to enhance their effectiveness on the unit test generation task. For example, [27] first pre-trained the LLM with the focal method and asserted statements to enable the LLM having a stronger foundation knowledge of assertions, then fine-tuned the LLM for the test case generation task where the objective is to learn the relationship between the focal method and the corresponding test case. [24] utilized a similar schema by pretraining the LLM on a large unsupervised Java corpus, and supervised fine-tuning a downstream translation task for generating unit tests.\n\nLater studies focus on designing effective prompts. Subsequently, LLMs possess enhanced abilities, allowing them to excel at targeted tasks without the pre-training or fine-tuning. Therefore the later studies typically focus on how to design the prompt, to make the LLM better at understanding the context and nuances of this task. [29] generated unit test cases by parsing the project, extracting essential information, and creating an adaptive focal context that includes a focal method and its dependencies within the pre-defined maximum prompt token limit of the LLM, and incorporate these context into a prompt to query the LLM. [32] first performed an empirical study to evaluate ChatGPT's capability of unit test generation with both a quantitative analysis and a user study in terms of correctness, sufficiency, readability, and usability. And results show that the generated tests still suffer from correctness issues, including diverse compilation errors and execution failures. They further proposed an approach that leveraged the ChatGPT itself to improve the quality of its generated tests with an initial test generator and an iterative test refiner. Specifically, the iterative test refiner iteratively fixed the compilation errors in the tests generated by the initial test generator, which follows a validate-and-fix paradigm to prompt the LLM based on the compilation error messages and additional code context.\n\nIncorporating LLM with search based software testing for unit test generation. The aforementioned studies utilize LLMs for the whole unit test case generation task, while [30] focus on a different direction, i.e., first letting the traditional search-based software testing techniques (e.g., Pynguin [78]) in generating unit test case until its coverage improvements stall, then asking the LLM to provide the example test cases for under-covered functions. These examples can help the original test generation redirects its search to more useful areas of the search space.\n\nPerformance of unit test case generation. Since the aforementioned studies of unit test case generation are based different datasets, one can hardly derive a fair comparison and we present the details in Table 2 to let the readers obtain a general view. We can see that in SF110 benchmark, all three evaluated LLMs have quite low performance, i.e., 2% coverage [31]. This SF110 dataset, which is an Evosuite (a search-based unit test case generation technique) benchmark consisting of 111 opensource Java projects retrieved from SourceForge, contains 23,886 classes, over 800,000 bytecode-level branches, and 6.6 million lines of code. The authors did not present the detailed reasons for the low performance, and can be further explored in future.\n\n\nTest Oracle Generation\n\nA test oracle is a source of information about whether the output of a software system (or program or function or method) is correct or not. Most the collected studies in this category target at the test assertion generation, which is inside a unit test case. Nevertheless, we opted to treat these studies as separate sections to facilitate a more thorough analysis.\n\nTest assertion, which is to indicate the potential issues in the tested code, is an important aspect that can distinguish the unit test cases from the regular code. This is why some researches specifically focus on the generation of effective test assertion. Actually, before using LLMs, researchers have proposed RNN-based approach which aims at learning from thousands of unit test methods to generate meaningful assert statements [79], yet only 17% of the generated asserts can exactly match with the ground truth asserts. Subsequently, to improve the performance, several researchers utilized the LLMs for this task.\n\n[36], [37] pre-trained a T5 model on a dataset composed of natural language English text and source code. Then, it fine-tuned such a model by reusing datasets used in four previous works that used deep learning techniques (such as RNN as mentioned before) including test assertion generation and program repair, etc. Results showed that the extract match rate of the generated test assertion is 57%. [33] proposed a similar approach which separately pre-trained the LLM with English corpus and code corpus, and then fine-tuned it on the asserts dataset (with test methods, focal methods, and asserts). This further improved the performance to 62% of exact match rate. Besides the syntax-level data as previous studies, [34] fine-tuned the LLMs with six kinds of code semantics data, including the execution result (e.g., types of the local variables) and execution context (e.g., the last called method in the test method), which enabled LLMs to learn to understand the code execution information. The exact match rate is 17% (note that this paper is based on a different dataset with all other studies mentioned under this topic).\n\nThe aforementioned studies utilized the pre-training and fine-tuning schema when using LLMs, and with the increasingly powerful capabilities of LLMs, they are able to perform well on specific tasks without these specialized pretraining or fine-tuning datasets. Subsequently, [35] utilized the prompt engineering for this task, and proposed a technique for prompt creation that automatically retrieves code demonstrations similar to the task, based on embedding or frequency analysis. They also present evaluations about the few-shot learning with various numbers (e.g., zero-shot, one-shot, or n-shot) and forms (e.g., random vs. systematic, or with vs. without natural language descriptions) of the prompts, to investigate its feasibility on test assertion generation. With only a few relevant code demonstrations, this approach can achieve an accuracy of 76% for exact matches in test assertion generation, which is the state-of-the-art performance for this task.\n\n\nTest Input Generation\n\nThis category covers the studies related to creating test input for enabling the automation of test execution, and for our collected studies, this category is mainly for system testing. The generation of system-level test inputs for software testing varies for different types of software. For example, for mobile applications, the test input generation requires providing a diverse range of text inputs or operation combinations (e.g., click a button, long press a list), which is key to test the application's functionality and user interface; while for Deep Learning (DL) libraries, the test input is a program which covers diversified DL APIs.\n\nMoreover, different from the unit test case generation, for these system-level testing tasks, the test assertions usually cannot be obtained directly, so this type of work usually requires cooperation with a specialized testing technology to complete the entire testing task (e.g., differential testing); or only considers those bugs with observable oracles (e.g., crash). In other words, these studies typically only involve the creation of test inputs without a corresponding test oracle; thereby they are usually paired with automated testing techniques to help derive a test oracle which can be used to evaluate the correctness of the software under test. We will present more details about these collected studies.\n\nTest input generation for DL libraries. The input for testing DL libraries is DL programs, and the difficulty for generating the diversified input DL programs is that they need to satisfy both the input language (e.g., Python) syntax/semantics and the API input/shape constraints for tensor computations. Traditional techniques with API-level fuzzing [80], [81] or model-level fuzzing [82], [83] suffer from the following limitations: 1) lack of diverse API sequence thus cannot reveal bugs caused by chained API sequences; 2) cannot generate arbitrary code thus cannot explore the huge search space that exists when using the DL libraries. Since LLMs can include numerous code snippets invoking DL library APIs in their training corpora, they can implicitly learn both language syntax/semantics and intricate API constraints for valid DL program generation. Taken in this sense, [46] uses both generative and infilling LLMs to generate and mutate valid/diverse input DL programs for fuzzing DL libraries. In detail, it first uses a generative LLM (CodeX) to generate a set of seed program (i.e., code snippets that use the target DL APIs). Then it replaces part of the seed program with masked tokens using different mutation oprators, and leverages the ability of infilling LLM (InCoder) to perform code infilling to generate new code that replaces the masked tokens. And their follow-up study [45] goes a step further to prime LLMs to synthesize unusual programs for the fuzzing DL libraries. It is built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding, and shows improved bug detection performance.\n\nTest input generation for mobile apps. For mobile app testing, one difficulty is to generate the appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), traditional techniques with heuristic-based or constraint-based techniques [6], [84] are far from generating the meaningful text input. [44] employs the LLM to intelligently generate the semantic input text according to the GUI context. In detail, it automatically extracts the component information related to the EditText and generates the prompt, and then inputs the prompt into the LLM to generate the input text.\n\nBesides the text input, there are other forms of input for mobile apps, i.e., operations like click a button, select a list. To fully test an app, it is required to cover more GUI pages and conduct more meaningful exploration traces through the GUI operations, yet existing studies with random-/rulebased methods [5], [6], model-based methods [7], [8], and learning-based methods [9] are unable to understand the semantic information of the GUI page thus could not conduct the trace planning effectively. [42] formulates the test input generation of mobile GUI testing problem as a Q&A task, which asks LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts (i.e., GUI operation), and executing them to keep passing the app feedback to LLM, iterating the whole process. Within it, the approach extracts the static context of the GUI page and the dynamic context of the iterative testing process, designs prompts for inputting these information to LLM which enables the LLM better understand the GUI page as well as the whole testing process.\n\nTest input generation for others. Findings bugs in a commercial cyber-physical system (CPS) development tool such as Simulink is even challenging. Given the complexity of Simulink language, generating valid Simulink model files for testing is an ambitious task for traditional machine learning or deep learning techniques. [40] employs a small set of Simulink-specific training data to fine-tune the LLM for generating Simulink models. Results show that it can create Simulink models quite similar to the open-source models, and can find a super-set of the bugs traditional fuzzing approaches found.\n\nIn addition, there are studies employing the LLMs for test input generation to serve as the testing of JavaScript engine, autonomous driving systems, and video game. For example, [38] utilizes the LLM for generating the JavaScript programs, and utilizes the well-structured ECMAScript specifications to automatically generate test data along with the test programs, and then applies differential testing to expose bugs. [47] uses LLM to extract key information related to the test scenario from a traffic rule, and represents the extracted information in a test scenario schema, then synthesizes the corresponding scenario scripts to construct the test scenario.\n\n\nBug Analysis\n\nThis category involves analyzing identified software bugs to facilitate the understanding of the bug. [51] proposes to explain software bugs with LLM, which generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. [50] targets at automatically generating the bug title from the descriptions of the bug, which aims to help developers write issue titles and facilitate the bug triaging and follow-up fixing process.\n\n\nDebug\n\nThis category focuses on identifying and resolving software errors to ensure that the code works as expected, which may involve analyzing code, tracing execution, collecting error information, and outputting information to find and fix issues. For example, [52] proposes a unified Detect-Localize-Repair framework based on the LLM for debugging, which first determines whether a given code snippet is buggy or not, then identifies the buggy lines, and translates the buggy code to its fixed version. [56] conducts a study of the deep learning program debugging ability, including fault detection, fault localization and program repair of LLM. [54] proposes automated scientific debugging, a technique that given buggy code and a bug-revealing test, prompts LLMs to automatically generate hypotheses, uses debuggers to actively interact with buggy code, and thus automatically reaches conclusions prior to patch generation.\n\nThere are also studies focusing on a sub-phase of the debugging process. For example, [53] proposes a framework to harness the LLM to reproduce bugs, and suggest bug reproducing test cases to the developer for facilitating debug. [55] focuses on a similar aspect about finding the failureinducing test cases whose test input can trigger the software's fault. It synergistically combines LLM and differential testing to do that.\n\n\nProgram Repair\n\nThis category denotes the task to fix the identified software bugs. The high frequency of repair related studies can be attributed to the close relationship between this task and the source code. With their advanced natural language processing and understanding capabilities, LLM are well-equipped to process and analyze source code, making them an ideal tool to perform code-related tasks such as fixing bugs.\n\nThere have been template-based [85], heuristic-based [86], and constraint-based [87], [88] automatic program repair techniques. And with the development of deep learning techniques in the past few years, there have been several studies employing deep learning techniques for program repair. They typically adapt deep learning models to take a buggy software program as input and generate a patched program. They would build a neural network model from a training set, and learns the relations between the buggy code and corresponding fixed code. Nevertheless, these techniques still fail to fix a large portion of bugs, and they typically have to generate hundreds to thousands of candidate patches and take hours to validate these patches to fix enough bugs. Furthermore, the deep learning based program repair models need to be trained with huge amount of labeled training data (typically pairs of buggy and fixed code), which is time-and effortconsuming to collect the high quality dataset. Subsequently, with the popularity and demonstrated capability of the LLMs, researchers begin to explore the LLMs for program repair.\n\nPatch single-line bugs. In the early era of program repair, the focus is mainly on addressing defects related to single-line code errors, which is relatively uncomplicated and did not require the repair of complex program logic. [62] proposes to fine-tune the LLM with JavaScript code snippets to serve as the purpose for the JavaScript program repair. [72] employs the program slicing to extract contextual information directly related to the given buggy statement as repair ingredients from the corresponding program dependence graph, which makes the fine-tuning more focus on the buggy code. Since most real-world bugs would involve multiple-lines of code, and later studies explore these more complex situations (although some of them can also patch the single-line bugs).\n\nPatch multiple-lines bugs. The studies in this category would input a buggy function to the LLM, and the goal is to output the patched function, which might involve complex semantic understanding, code hunk modification, as well as program refactoring. Earlier studies typically employ the fine-tuning strategy to enable the LLM better understand the code semantics. [74] fine-tunes the LLM by employing BPE tokenization to handle Out-Of-Vocabulary (OOV) issues which makes the approach can generate new tokens that never appear in a training function but are newly introduced in the repair. The aforementioned studies (including the ones in patching single-line bugs) would predict the fixed programs directly, and [59] utilizes a different setup which predicts the scripts that can fix the bugs when executed with the delete and insert grammar. For example, it predicts whether an original line of code should be deleted, and what content should be inserted.\n\nNevertheless, fine-tuning may face limitations in terms of its reliance on abundant high-quality labeled data, significant computational resources, and the possibility of overfitting. To approach the program repair problem more effectively, later studies focus on how to design the effective prompt for program repair. Several studies empirically investigate the effectiveness of prompt variants of the latest LLMs for program repair under different repair settings and commonly-used benchmarks [56], [58], [60], [63], [68], [69], which will be explored in depth later, while other studies focus on proposing new techniques. [67] takes advantage of LLM to conduct the code completion in a buggy line for patch generation, and elaborate on how to circumvent the open-ended nature of code generation to appropriately fit the new code in the original program. [71] proposes the conversation-driven program repair approach that interleaves patch generation with instant feedback to perform the repair in a conversational style. They first feed the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful repair. For earlier patches that failed to pass all tests, they combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch, in order to avoid making the same mistakes. For earlier patches that passed all the tests (i.e., plausible patches), they further ask the LLM to generate alternative variations of the original plausible patches. This can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. [61] proposes a similar approach design by leveraging multimodal prompts (e.g., natural language description, error message, input-output-based test cases), iterative querying, test-case-based few-shot selection to produce repairs.\n\nRepair from natural language issue descriptions. The aforementioned studies all consider the buggy code as the input, and let the LLM conduct the automatic program repair. [73] focuses on program repair from natural language issue descriptions, i.e., generating the patch with the bug and fix related information described in issue reports in repositories.\n\nRepair with static code analyzer. Most of the program repair studies would suppose the bug has been detected, while [70] proposes a program repair framework paired with a static analyzer to first detect the bugs, and then fix it. ChatGPT-175B [56] Note that, for studies with multiple datasets or LLMs, we only present the best performance or in the most commonly utilized dataset.\n\nIn detail, the static analyzer first detects an error (e.g., null pointer dereference) and the context information provided by the static analyzer will be sent into the LLM for querying the patch for this specific error. Empirical study about program repair. There are several studies related with the empirical or experimental evaluation of the various LLMs on program repair, and we summarize the performance in Table 3. [60], [69] conduct relatively comprehensive experimental evaluations with various LLMs and on different automated program repair benchmarks, while [56], [58], [63] focus on a specific LLM and on one dataset, i.e., QuixBugs. There are two commonly-used repair settings one can use LLMs to generate patches: 1) complete function generation (i.e., generating the entire patch function), 2) correct code infilling (i.e., filling in a chunk of code given the prefix and suffix), and different studies might utilize different setting which is explicitly marked in the table. The commonly-used datasets are QuixBugs, Defects4J, etc. These datasets only involve the fundamental functionalities such as sorting algorithm, each program's average number of lines ranging from 13 to 22, implementing one functionality, and involving few dependencies. [56] conducts an empirical study on a more complex dataset with DL programs collected from StackOverflow. Every program contains about 46 lines of code on average, implementing several functionalities including data preprocessing, DL model construction, model training, and evaluation. And the dataset involves more than 6 dependencies for each program, including TensorFlow, Keras, and Pytorch. Their results demonstrate a much lower rate of correct patches than in other datasets, which again reveals the potential difficulty of this task.\n\n\nANALYSIS FROM LLM\n\nThis section discusses the analysis based on the viewpoints of LLM, specifically, it's unfolded from the viewpoints of utilized LLMs, types of promot engineering, input of the LLMs, as well as the accompanied techniques when utilizing LLM.\n\n\nLLM Models\n\nAs shown in Figure 5, the most commonly utilized LLM is Codex (a LLM based on GPT-3) which is trained on a massive code corpus containing examples from many programming languages such as JavaScript, Python, C/C++, and Java. Codex is released on Sep. 2021 by OpenAI and powers GitHub Copilot-an AI pair programmer that generates whole code snippets, given a natural language description as a prompt. Since a large portion of our collected studies involve the source code (e.g., repair, unit test case generation), it is not surprising that researchers choose Codex as the LLM in assisting them accomplishing the coding related tasks.\n\nChatGPT, which is released on Nov. 2022, is the second most commonly used LLM in our collected studies. It is trained on a large corpus of natural language text data, and primarily designed for natural language processing and conversation. Since Codex is released more than one year earlier than ChatGPT, it is most commonly utilized by the studies earlier, and later studies tend to employ ChatGPT to investigate its feasibility in various software testing tasks and tackle the challenges of generating human-like text responses. In software testing domain, we have not observed there are studies employing the newly developed GPT-4, which is demonstrated to be more powerful in various tasks. This maybe because it is lunched on Mar. 2023, which is just a few months earlier before we conduct the paper collection. And we believe there would be more attempts about employing GPT-4 for software testing tasks, especially these related with multimodal such as UI screenshots. The third-ranked LLM is CodeT5, which is an open sourced LLM developed by salesforce 3 . Thanks to its open source, researchers can easily conduct the pre-training and fine-tuning with domain specific data to achieve better performance. Similarly CodeGen is also open sourced and ranked relatively higher. Besides, for CodeT5 and CodeGen, there are more than half of the related studies involve the empirical evaluations (which employ multiple LLMs), e.g., program repair [68], [69], unit test case generation [31].\n\nThere are also five studies utilizing GPT-3, which is also widely popular in the general domain, for software testing. As GPT-3 can provide good performance in general natural language understanding, it is mainly used for some general tasks, such as the input generation for automatic mobile application testing [42], [44], and the traffic rule parsing in autonomous driving tests [47].\n\n\nTypes of Prompt Engineering\n\nAs shown in Figure 6, among our collected studies, 21 studies utilize the LLMs through pre-training or finetuning schema, while 33 studies employ the prompt engineering to communicate with LLMs to steer its behavior for desired outcomes without updating the model weights. When using the early LLMs, their performances might not be as impressive, so researchers often use pre-training or fine-tuning techniques to adjust the models for specific domains and tasks in order to improve their performance. Then with the upgrading of LLM technology, especially with the introduction of GPT-3 and later LLMs, the knowledge contained within the models and their understanding/inference capability has increased significantly. Therefore, researchers will typically rely on prompt engineering to consider how to design appropriate prompts to stimulate the model's knowledge.\n\nAmong the 33 studies with prompt engineering, 27 studies involve zero-shot learning, and 9 studies involve the few-shot learning (a study may involve multiple types). There are also respectively 3 and 1 studies involving chain-of-though and self-consistency.\n\nZero-shot learning is to simply feed the task text to the model and ask for results. Many of the collected studies employ the Codex, CodeT5, and CodeGen (as shown in Section 5.1), which is already trained on source code. Hence, 3. https://blog.salesforceairesearch.com/codet5/ for the tasks dealing with source code like unit test case generation and program repair as demonstrated in previous sections, directly query the LLM with prompts is the common practice. There are generally two types of manners of zero-shot learning, i.e., with and without instructions. For example, [29] would provide the LLMs with the instructions as \"please help me generate a JUnit test for a specific Java method ...\" to facilitate the unit test case generation. In contrast, [31] only provides the code header of the unit test case (e.g., \"class ${className}${suffix}Test {\"), and the LLMs would carry out the unit test case generation automatically. Generally speaking, prompts with clear instructions will yield more accurate results, while prompts without instructions are typically suitable for very specific situations.\n\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees the examples, it can better understand human intention and criteria for what kinds of answers are wanted, which is especially important for the tasks that is not so straightforward or intuitive to the LLM. For example, when conducting the automatic test generation from general bug reports, [53] provides examples of bug reports (questions) and the corresponding bug reproducing tests (answers) to the LLM, and their results show that two examples can achieve the highest performance than no examples or other number of examples. Another example test assertion generation, [35] provides demonstrations of the focal method, the test method containing an <AssertPlaceholder>, and the expected assertion, which enables the LLMs better understand the task.\n\nChain-of-thought (CoT) prompting generates a sequence of short sentences to describe reasoning logics step by step (also known as reasoning chains or rationales), to eventually lead to the final answer. For example, for program repair from the natural language issue descriptions [73], given the buggy code and issue report, the authors first ask the LLM to localize the bug, then they ask to explain why the localized lines are buggy, finally they ask to fix the bug. Another example is for generating unusual programs for fuzzing deep learning libraries, [45] first generates a possible \"bug\" (bug description) before generating the actual \"bug-triggering\" code snippet that invokes the target API. The predicted bug description provides additional hint to the LLM, indicating that the generated code should try to cover specific potential buggy behavior.  Fig. 7: Input of LLM Self-consistency involves evaluating the coherence and consistency of the LLM's responses on the same input in different contexts. There is one study with this prompt type, and it is about debug. [54] employs a hypothesizeobserve-conclude loop, which first generates a hypothesis about what the bug is and construct an experiment to verify, using an LLM, then decide whether the hypothesis is correct based on the experiment result (with a debugger or code execution) using an LLM, after that, depending on the conclusion, it either starts with a new hypothesis or opts to terminate the debugging process and generate a fix.\n\nWe also want to mention that there are eight studies apply the iterative prompt design when using zero-shot or few-shot learning, in which the approach continuously refines the prompts with the running information of the testing task, e.g., the test failure information. For example, for program repair, [71] interleaves patch generation with test validation feedback to prompt future generation in an iterative manner. In detail, they incorporate various information from a failing test including its name, the relevant code line(s) triggering the test failure, and the error message produced in the next round of prompt which can help the model understand the failure reason and provide guidance towards generating the correct fix. Another example is for mobile GUI testing, [42] iteratively query the LLM about the operation (e.g., click a button, enter a text) to be conducted in the mobile app, and at each iteration, they would provide the LLM with current context information like which GUI pages and widgets have just explored.\n\n\nInput of LLM\n\nWe also find that different testing tasks or software under test might involve diversified input when querying the LLM, as demonstrated in Figure 7.\n\nThe most commonly utilized input is the source code, since a large portion of collected studies relate with program repair or unit test case generation whose input are source code. For unit test case generation, typical code related information would be (i) the complete focal method, including the signature and body; (ii) the name of the focal class (i.e., the class that the focal method belongs to); (iii) the field in the focal class; and (iv) the signatures of all methods defined in the focal class [24], [32]. For program repair, there can be different setups and involves different inputs, including (i) inputting a buggy function with the goal to output the patched function, (ii) inputting the buggy location with the goal to generate the correct replacement code (can be a single line change) given the prefix and suffix of the buggy function [60]. Besides, there can be variations for the buggy location input, i.e., (i) not contain the buggy lines (but the bug location is still known), (ii) give the buggy lines as lines of comments.\n\nThere are also five studies taking the bug description as input for the LLM. For example, [53] takes the bug description as input when querying LLM and let the LLM generate the bug reproducing test cases, and [73] inputs the natural language descriptions of bugs to the LLM, and generate the correct code fixes.\n\nThere are four studies which would provide the intermediate error information, e.g., test failure information, to the LLM, and would conduct the iterative prompt (as described in Section 5.2) to enrich the context provided to the LLM. These studies are related to the unit test case generation and program repair, since in these scenarios, the running information can be acquired easily.\n\nWhen testing mobile apps, since the utilized LLM could not understand the image of the GUI page, the researches would provide the LLM with the view hierarchy file which represents the details of the GUI page. Nevertheless, with the emerging of GPT-4 which is a multimodal model and accepts both image and text inputs for model input, the GUI screenshots might be directly utilized for LLM's input.\n\n\nIncorporating X with LLM\n\nThere are divided opinions on whether LM has reached an all-powerful status that requires no other techniques. As shown in Figure 8, among our collected studies, 28 of them utilize LLMs to address the entire testing task, while 22 studies incorporate additional techniques. These techniques include mutation testing, differential testing, syntactic checking, program analysis, statistical analysis, as well as natural language processing and formal methods.\n\nThe reason why researchers still choose to combine LLMs with other techniques might be because, despite exhibiting enormous potential in various tasks, LLM still possess limitations such as comprehending code semantics and handling complex program structures. Therefore, combining LLMs with other techniques optimizes their strengths and weaknesses to achieve better outcomes in specific scenarios. In addition, it is important to note that while LLMs are capable of generating correct code, they may not necessarily produce sufficient test cases to check for edge cases or rare scenarios. This is where mutation and other testing techniques come into play, as they allow for the generation of more diverse and complex code that can better simulate real-world scenarios. Taken in this sense, a testing approach can incorporate a combination of different techniques, including both LLMs and other testing strategies, to ensure comprehensive coverage and effectiveness.\n\nLLM + program analysis. When utilizing LLMs to accomplish tasks such as generating unit test cases and repairing software code, it is important to consider that software code inherently possesses structural information, which may not be fully understood by LLMs. Hence, researchers often utilize program analysis techniques, including code abstract syntax trees (ASTs) [51], to represent the structure of code more effectively and increase the LLM's ability to comprehend the code accurately. Researchers also perform the structure-based subsetting of code lines to narrow the focus for LLM [61], or extracts additional code context from other code files [32], to enable the models to focus on the most task-relevant information in the codebase and lead to more accurate predictions. LLM + statistic analysis. As LLMs can often generate a multitude of outputs, manually sifting through and identifying the correct output can be overwhelmingly laborious. As such, researchers have turned to statistical analysis techniques like ranking and clustering [26], [34], [53], [60], [72] to efficiently filter through LLM's outputs and ultimately obtain more accurate results.\n\nLLM + differential testing. Differential testing is wellsuited to find semantic or logic bugs that do not exhibit explicit erroneous behaviors like crashes or assertion failures. In this category of our collected studies, the LLM is mainly response for generating the valid and diversified inputs, while the differential testing helps to determine whether there is a triggered bug based on the software's output. For example, [38] first uses LLM to produce random JavaScript programs, and leverages the language specification document to generate test data, then conduct the differential testing on JavaScript engines as JavaScriptCore, ChakraCore, SpiderMonkey, QuickJS, etc. [45], [46] design a similar approach to let the LLM generate the test input and then conduct the differential testing for fuzzing DL libraries. [55] employs the LLM in finding the failure-inducing test cases. In detail, given a program under test, they first request the LLM to infer the intention of the program, then request the LLM to generate programs that have the same intention, which are alternative implementations of the program, and are likely free of the program's bug. Then they performs the differential testing with the program under test and the generated programs to find the failure-inducing test cases.\n\nLLM + mutation testing. It is mainly targeting at generating more diversified test inputs. For example, [46] first uses LLM to generate the seed programs (e.g., code snippets using a target DL API) for fuzzing deep learning libraries.\n\nTo enrich the pool of these test programs, they replace parts of the seed program with masked tokens using mutation operators (e.g., replaces the API call arguments with the span token) to produce masked inputs, and again utilize the LLMs to perform code infilling to generate new code that replaces the masked tokens.\n\nLLM + syntactic repair. Although LLMs have shown remarkable performance in various natural language processing tasks, the generated code from these models can sometimes syntactically incorrect, leading to potential errors and reduced usability. Therefore, researchers have proposed to leverage syntax checking to identify and correct errors in the generated code. For example, in [27] for unit test case generation, the authors additionally introduce a verification method to check and repair the naming consistency (i.e., revising the test method name to be consistent with the focal method name) and the test signatures (i.e., adding missing keywords like public, void, or @test annotations). Another example is [29] which also validates the generated unit test case and employs rule-based repair to fix syntactic and simple compile errors.\n\nBesides, interpreting LLMs' predictions and outputs remain challenging, thus combining various techniques like the natural language processing can help to post-process LLM's outcome and filtering unsatisfied results.\n\n\nCHALLENGES AND OPPORTUNITIES\n\nBased on the above analysis from the viewpoints of software testing and LLM, we summarize the challenges and opportunities when conducting software testing with LLM. These include utilizing LLMs for a more diverse set of software testing tasks and phases, expanding their usage to a wider variety of testing types and software, supplying more comprehensive benchmark datasets together with thorough experimental evaluations, integrating advanced prompt engineering, and combining LLMs with relevant existing techniques, etc.\n\n\nExtending to More Tasks and More Phases\n\nUtilizing LLMs for tasks in early stage of testing. As shown in Figure 4, LLMs have not been used in the early stage of testing, e.g., test requirements, test planning. There might be two main reasons behind that. The first is the subjectivity in early-stage testing tasks. Many tasks in the early stages of testing, such as requirements gathering, test plan creation, and design reviews, may involve subjective assessments that require significant input from human experts. This could make it less suitable for LLMs that rely heavily on data-driven approaches. The second might be the lack of open-sourced data in the early stages. Unlike in later stages of testing, there may be limited data available online during early stage activities. This could mean that LLMs may not have seen much of this type of data, and therefore may not perform well on these tasks.\n\nNevertheless, challenges and opportunities always coexist. At the early stages, a crucial task is to automatically generate test requirements based on software requirement specifications and/or user manuals. This task is akin to machine translation, where LLMs have already demonstrated impressive performance [89]. However, unlike machine translation, which involves relatively clear semantic mapping, generating test requirements requires significant domain knowledge and may involve integrating information about the operation of the software being tested. It is also essential to consider the coverage of the generated test requirements for the software under test.\n\nExploring LLMs for tasks that popularly studied with machine/deep learning techniques. Despite its success in task related to test case prepartion and program repair, there are testing tasks which have been popularly studied with traditional techniques or machine/deep learning techniques, e.g., test prioritization [90], regression testing [91], bug triage [92], yet have not been touched by the LLM. For test prioritization and regression testing, the common practice with machine/deep learning techniques would analyze the code and its dependencies and prioritize the execution order of test cases based on their impact or likelihood of detecting faults. With the LLM, these can be conducted more accurately. In addition, LLMs can assist in analyzing and triaging bug reports by leveraging their comprehension and inference abilities. They can identify duplicates, classify bugs based on severity or priority, extract relevant information, and suggest potential resolutions or workarounds.\n\nExploring LLMs for integration testing and acceptance testing. We further analyze the distribution of testing phases for the collected studies. As shown in Figure 9, we can observe that LLMs are most commonly used in unit testing, followed by system testing. However, there is still no research on the use of LLMs in integration testing and acceptance testing.\n\nFor integration testing, it involves testing the interfaces between different software modules. In some software organizations, the integration testing might be merged with unit testing, which can be a possible reason why LLM is rarely utilized in integration testing. Another reason might be that the size and complexity of the input data in this circumstance may exceed the capacity of the LLM to process and analyze (e.g., the source code of all involved software modules), which can lead to errors or unreliable results.\n\nFor tackle this, a potential reference can be found in Section 4.1, where [29] designs a method to organize the necessary information into the pre-defined maximum prompt 3DSHU&RXQW 8QLWWHVW ,QWHJUDWLRQWHVW 6\\VWHPWHVW $FFHSWDQFHWHVW 7HVWLQJ3KDVHV Fig. 9: Distribution of testing phases (note that we omit the studies which donot explicitly specify the testing phases, e.g., program repair) token limit of the LLM. Furthermore, integration testing requires diversified data to be generated to sufficiently test the interface among multiple modules. As demonstrated in Section 4.3, previous work has demonstrated the LLM's capability in generating diversified test input for system testing, in conjunction with mutation testing techniques [38], [46]. And these can provide the insights about generating the diversified interface data for integration testing.\n\nFor acceptance testing, it is usually conducted by business analysts or end-users to validate the system's functionality and usability, which require more non-technical language and domain-specific knowledge, thus making it challenging to apply LLM effectively. Since acceptance testing involves humans, it is well-suited for the use of humanin-the-loop schema with LLMs. This has been studied in traditional machine learning [93], but has not yet been explored with LLMs. Specifically, the LLMs can be responsible for automatically generating test cases, evaluating test coverage, etc, while human testers are responsible for checking the program's behavior and verifying test oracle.\n\n\nServing Other Types of Testing and Software\n\nEmploying LLMs for non-functional testing. In our collected studies, LLMs are primarily used for functional testing, with only few applications in security testing, and no practice in performance testing, usability testing or others.\n\nOne possible reason for the prevalence of LLM-based solutions in functional testing is that they are able to convert functional testing problems into code generation or natural language generation problems, which LLMs are particularly adept at solving. For our collected studies related with security testing, all of them involve the vulnerability detection and repair in the source code, rather than the security of the whole software system. This also attributes to the capability of LLM in understanding and processing the source code.\n\nOn the other hand, performance testing and usability testing may require more specialized models that are designed to detect and analyze specific types of data, handle complex statistical analyses, or determine the buggy criteria. Moreover, there have been dozens of performance testing tools (e.g., LoadRunner) which can generate a workload that simulates real-world usage scenarios and achieve relative satisfactory performance.\n\nThe potential opportunities might let the LLM integrates the performance testing tools and acts like the LangChain [94], for better simulating different types of workloads based on real user behavior. Furthermore, the LLMs can identify the parameter combinations and values that have the highest potential to trigger performance problems. It is essentially a way to rank and prioritize different parameter settings based on their impact on performance and improve the efficiency of performance testing.\n\nEmploying LLMs for other types of software. We also analyze what types of software have been explored in the collected studies. Results show that the following types of software are explored, as shown in Figure 10. Note that, since a large portion of studies are focused on the unit testing or program repair, they are conducted on publicly available datasets and do not involve specific software types. From the analysis in Section 4.3, the LLM can generate not only the source code for testing DL libraries, but also the textual input for testing mobile apps, even the models for testing CPS. Overall, the LLM provides a flexible and powerful framework for generating test inputs for a wide range of applications, including testing DL libraries, mobile apps, and CPS. Its versatility would makes it useful for testing the software in other domains.\n\nFrom one point of view, some propose techniques can be applicable to other types of software. For example, in the paper proposed for testing deep learning libraries [45], since it proposes techniques for generating the diversified, complicated and human-like DL programs, the authors state that the approach can be easily extended to test software systems from other application domains, e.g., compilers, interpreters, database systems, SMT solvers, and other popular libraries. And we also notice there are exploration about using LLM for database testing in a technical blog [95], which can generate an equivalent SQL based on the Ternary Logic Partitioning (TLP) principle to help derive the test oracle.\n\nFrom other point of view, other types of software can also benefit from the capabilities of LLMs. For example, for Web applications, the LLMs can generate test inputs such as HTTP requests, form inputs, and data payloads to test the functionality, security, and performance of web applications. For network protocols, the LLMs can generate network traffic and protocol messages to test the robustness of network protocols, which can include testing communication between different network components, handling of error and edge cases, and adherence to protocol specifications. For IoT devices, the LLMs can generate test inputs and commands to test device communication, sensor data processing, firmware updates, and integration with other systems.\n\n\nMore Solid Benchmarks and Rigorous Evaluations\n\nWe find the following limitations which hinders the rigorous evaluations and thorough comparison of the proposed techniques. These limitations highlight the need for more solid benchmarks and rigorous evaluations.\n\nThe first is the data leakage issue, i.e., the LLMs may have seen these benchmarks in their pre-training data. [69] checks the CodeSearchNet and BigQuery, which are the datasources of common LLMs, the results show that four repositories used by the Defect4J benchmark are also in CodeSearchNet, and the whole Defects4J repository is included by BigQuery. Therefore, it is very likely that existing program repair benchmarks are seen by the LLMs during pre-training. This data leakage issue has also been investigated in machine learning related studies. For example [96] focus on the data leakage in issue tracking data, and results show that information leaked from the \"future\" makes prediction models misleadingly optimistic. This reminds us that the performance of LLMs on software testing tasks may not be as good as reported in previous researches. It also suggests that we need more specialized datasets that are not seen by LLMs to serve as benchmarks. One way is to collecting it from specialized sources, e.g., user-generated content from niche online communities.\n\nThe second is the limited number of benchmarks. For unit test case generation, there is even no widely recognized benchmarks, and different studies would utilize different datasets for performance evaluation, as demonstrated in Table 2. For program repair, there are only two wellknown and commonly-used benchmarks, i.e., Defect4J and QuixBugs, as demonstrated in Table 3. Furthermore, these datasets are not specially designed for testing the LLMs. For example, as reported in [60], 39 out of 40 Python bugs in QuixBugs dataset can be fixed by Codex, yet in real-world practice, the successful fix rate can be nowhere near as high. This motivates to build more specialized and diversified benchmark as mentioned in the first limitation.\n\nThe third is the performance variation across datasets. This is not only the problem for LLMs, but also for traditional machine learning and deep learning models. For example for unit test case generation, as shown in Table 2, with the same technique [31], on HumanEval dataset it can generate 78% correct unit test cases, and the line coverage is 87%, yet on SF110 dataset, both the correctness of generated tests and the line coverage is merely 2%. The performance variation among different datasets indicates the fragile of current unit test case generation techniques. This performance variation is also frequently observed in traditional machine learning tasks, e.g., in defect prediction [97], [98]. Typically solution evolves more reliable benchmark and extension comparison as mentioned in the first and second limitations.\n\nThe fourth is the performance inconsistency under the same setting. For example for program repair, even based on the same dataset, with the same LLM, and use the same repair setup, the results report in different studies might differ. For example, as shown in Table 3, on QuixBugs Python dataset, with Codex (12B), for the completion function generation setting, the correctly repaired bugs are respectively 23 [58] and 37 [60]. We assume there might be other imperceptible differences in these two studies. For instance, they might employ different prompt expression when querying the LLM, which results in performance variation, since existing studies have also reveal that a slight modification in the prompts can result in dramatic performance changes. This phenomenon is not frequently observed in traditional machine learning tasks, but it is worth noting in the context of LLMs due to the unique nature of prompts. More detailed description of the proposed approach and experimental setup is encouraged in future work to facilitate the follow-up comparison.\n\n\nBoosting LLM Performance\n\nExploring advanced prompt engineering. There are a total of 11 commonly used prompt engineering techniques as list in a pupluar prompt engineering guide [99], as shown in Figure 11. Currently, in our collected studies, only the first four techniques are being utilized. The more advanced techniques have not been employed yet, and can be explored in the future for prompt design. For instance, graph prompting involves the representation of information using graphs or visual structures to facilitate understanding and problem-solving. Graph prompting can be a natural match with software engineering, consider it involves various dependencies, control flow, data flow, state transitions, or other relevant graph structure. Graph prompting can be beneficial in analyzing these structural information, and enabling the LLMs to comprehend the software under test effectively. For instance, testers can use graph prompts to visualize test coverage, identify untested areas or paths, and ensure adequate test execution.\n\nMultimodal chain of thought prompting involves using diverse sensory and cognitive cues to stimulate thinking and creativity in LLMs. By providing images (e.g., GUI screenshots) or audio recordings related to the software under test can help the LLM better understand the software's context and potential issues. Besides, try to prompt the LLM to imagine itself in different roles, such as a developer, user, or quality assurance specialist. This perspective-shifting exercise enables the LLM to approach software testing from multiple viewpoints and uncover different aspects that might require attention or investigation.\n\nLLMs with relevant existing techniques. There is currently no clear consensus on the extent to which LLMs can solve software testing problems. From the analysis in Section 5.4, we have seen some promising results from studies that have combined LLMs with traditional software testing techniques. This implies the LLMs are not the sole silver bullet for software testing. Since there are already many mature software testing techniques and tools, while the capabilities of LLMs are not yet outstanding, therefore it is necessary to explore other better ways to combine LLMs with traditional testing techniques and tools for better software testing.\n\nFrom the collected studies, the LLMs have successfully utilized together with differential testing and mutation testing as shown in Figure 8. More explorations are encouraged in terms of combining LLMs with other testing techniques. For instance, metamorphic testing involves generating test cases based on the expected relationship between inputs and outputs, rather than on specific input-output pairs, to determine whether the software system behaves correctly when the input changes in a predictable way. LLMs can be used to recommend the potential metamorphic relations, which can help people divergent thinking. Following that, LLMs can be used to generate test cases automatically based on the metamorphic relations that have been defined, which can covers a variety of inputs. They can also be used to augment existing test suites with additional test cases generated based on metamorphic relations, thus improve the diversity of the test suite and increase the chances of detecting bugs. Another example is model-based testing. We have mentioned that the LLMs have successfully been utilized in generating the Simulink model files and find bugs in the Simulink toolchain [40]. And LLMs might be used to learn the model of the software system by analyzing natural language descriptions of the system's behavior. Specifically, LLMs can be used to analyze the system documentation, user manuals, or other sources of information to learn about the system's behavior, so as to improve the accuracy of the model for testing. In addition, LLMs can be used to help evolve the model of the software system as the system changes or evolves, e.g., analyze the natural language descriptions of system changes or bug reports to automatically update the model or generate new test cases.\n\nThe LLMs can be utilized through the LLM in the loop manner, which invokes the LLM when it is needed. Besides, since there are many mature software testing tools, and one can let the LLMs integrate these tools and acts like the LangChain for better explore the potential of these tools.\n\nFine-tuning open-source LLMs with software-specific data. As we mentioned in Section 5.2, in the early days of using LLMs, pre-training and fine-tuning are commonlyused practice, considering the model parameters are relatively few resulting in weaker model capabilities (e.g., T5). As time progressed, the number of model parameters increased significantly which leading to the emergence of models with greater capabilities (e.g., chatGPT). And in recent studies, prompt engineering has become a common approach. However, due to concerns regarding data privacy, when considering real-world practice, most software organizations tend to avoid using commercial LLMs and would prefer to adopt open-source ones instead. In such cases, using the software-specific data within the organization for model fine-tuning can further improve the performance.\n\nBuilding high-quality datasets for fine-tuning is crucial, and existing research has shown that high-quality training data can significantly improve the performance of tasks such as code search [100]. However, manually constructing such datasets can be time-consuming and labor-intensive. Meanwhile, in the past few years, researchers have explored automated techniques to extract the key information (e.g., issue and solution) from Stack Overflow and Gitter chatrooms [101]. Since these platforms provide a wealth of information on programming languages, frameworks, libraries, as well as common coding patterns and practices, automatically extracting the targeted information can serve as an important source for the fine-tuning dataset.\n\nIn addition, exploring the methodology about how to better fine-tune the LLMs with software-specific data is worth considering because software-specific data differs from natural language data in that it contains more structural information, such as data flow and control flow. Previous research on code representations has considered employing the data flow which is a semantic-level structure of code that depicts the relation of \"whether-value-comesfrom\" between variables, and demonstrates its advantages than considering the code without structure [102]. Besides, researches also utilize the abstract syntax tree and control flow [103], [104] for code representation and achieve good performance. These can provide valuable insights for fine-tuning LLMs with software-specific data.\n\n\nCONCLUSION\n\nThis paper provides a comprehensive review of the use of LLMs in software testing. We have analyzed relevant studies that have utilized LLMs in software testing from both the software testing and LLMs perspectives. This paper also highlights the challenges and potential opportunities in this direction. It can serve as a roadmap for future research in this area, identifying gaps in our current understanding of the use of LLMs in software testing and highlighting potential avenues for exploration. We believe that the insights provided in this paper will be valuable to both researchers and practitioners in the field of software engineering, assisting them in leveraging LLMs to improve software testing practices and ultimately enhance the quality and reliability of software systems.\n\nFig. 1 :\n1Structure of the contents in this paper (the numbers in bracket indicates the number of involved papers, and a paper might involve zero or multiple items) engineering techniques, and integrating LLMs with other testing techniques, etc.\n\nFig. 2 :\n2Trend in the number of papers with year\n\nFig. 3 :\n3Topics discussed in the collected papers 2 papers are from FSE, and 3 papers are from ISSTA. 4% papers are published in artificial intelligence venues such as EMNLP and ICLR, and 6% papers are published in program analysis or security venues like PLDI and S&P. Besides, 52% of the papers have not yet published via peer-reviewed venues (i.e., arXiv)\n\nFig. 4 :\n4Distribution of testing tasks with LLMs (aligned with software testing life cycle[75]\n\nFig. 5 :\n5LLMs used in the collected papers\n\nFig. 6 :\n6Distribution about how LLM is used (Note that, a study can involve multiple types of prompt engineering)\n\nFig. 8 :\n8Distribution about LLM + X (Note that, a study can involve multiple types LLM + X schema)\n\nFig. 10 :\n10Distribution of software under test\n\nFig. 11 :\n11List of advanced prompt engineering practices and those utilized in the collected papers\n\nTABLE 1 :\n1Details of the collected papers \n\n\n\nTABLE 2 :\n2Performance of unit test case generation Note that,[31] experiments with Codex, CodeGen, and ChatGPT, and the best performance was achieved by Codex.Dataset \nCorrectness \nCoverage \nLLM \nPaper \n5 Java projects from Defects4J \n16.21% \n5%-13% (line coverage) \nBART \n[24] \n10 Jave projects \n40% \n89% (line coverage), 90% (branch coverage) \nChatGPT \n[29] \nCodeSearchNet \n41% \nN/A \nChatGPT \n[32] \nHumanEval \n78% \n87% (line coverage), 92% (branch coverage) \nCodex \n[31] \nSF110 \n2% \n2% (line coverage), 1% (brach coverage) \nCodex \n[31] \n\n\nTABLE 3 :\n3Performance of program repairDataset \n% Correct patches \nLLM \nPaper \nDefects4J v1.2, Defects4J \nv2.0, \nQuixBugs, \nHumanEval-Java \n\n22/40 Jave bugs (QuixBugs dataset, with InCoder-6B, correct \ncode infilling setting) \n\nPLBART, CodeT5, CodeGen, In-\nCoder (each with variant pa-\nrameters, 10 LLMs in total) \n\n[69] \n\nQuixBugs \n23/40 Python bugs, 14/40 Java bugs (complete function genera-\ntion setting) \n\nCodex-12B \n[58] \n\nDefects4J v1.2, Defects4J \nv2.0, QuixBugs, Many-\nBugs \n\n39/40 Python bugs, 34/40 Java bugs (QuixBugs dataset, with \nCodex-12B, correct code infilling setting); 37/40 Python bugs, \n32/40 Java bugs (QuixBugs dataset, with Codex-12B, complete \nfunction generation setting) \n\nCodex, GPT-Neo, CodeT5, In-\nCoder (each with variant pa-\nrameters, 9 LLMs in total) \n\n[60] \n\nQuixBugs \n31/40 Python bugs (completion function generation setting) \nChatGPT-175B \n[63] \nDL programs from Stack-\nOverflow \n\n16/72 Python bugs (complete function generation setting) \n\n\nA theoretical and empirical study of search-based testing: Local, global, and hybrid search. M Harman, P Mcminn, 36M. Harman and P. McMinn, \"A theoretical and empirical study of search-based testing: Local, global, and hybrid search,\" vol. 36, no. 2, 2010, pp. 226-247.\n\nInterevo-tr: Interactive evolutionary test generation with readability assessment. P Delgado-P\u00e9rez, A Ram\u00edrez, K J Valle-G\u00f3mez, I Medina-Bulo, J R Romero, IEEE Trans. Software Eng. 494P. Delgado-P\u00e9rez, A. Ram\u00edrez, K. J. Valle-G\u00f3mez, I. Medina- Bulo, and J. R. Romero, \"Interevo-tr: Interactive evolutionary test generation with readability assessment,\" IEEE Trans. Software Eng., vol. 49, no. 4, pp. 2580-2596, 2023.\n\nCharacteristic studies of loop problems for structural test generation via symbolic execution. X Xiao, S Li, T Xie, N Tillmann, 2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013. E. Denney, T. Bultan, and A. ZellerSilicon Valley, CA, USAIEEEX. Xiao, S. Li, T. Xie, and N. Tillmann, \"Characteristic studies of loop problems for structural test generation via symbolic execution,\" in 2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013, Silicon Valley, CA, USA, November 11-15, 2013, E. Denney, T. Bultan, and A. Zeller, Eds. IEEE, 2013, pp. 246-256.\n\nFeedbackdirected random test generation. C Pacheco, S K Lahiri, M D Ernst, T Ball, 29th International Conference on Software Engineering (ICSE 2007). Minneapolis, MN, USAIEEE Computer SocietyC. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball, \"Feedback- directed random test generation,\" in 29th International Conference on Software Engineering (ICSE 2007), Minneapolis, MN, USA, May 20-26, 2007. IEEE Computer Society, 2007, pp. 75-84.\n\nUi/application exerciser monkey. A Developers, A. Developers, \"Ui/application exerciser monkey,\" 2012.\n\nDroidbot: a lightweight uiguided test input generator for android. Y Li, Z Yang, Y Guo, X Chen, ICSE. IEEEY. Li, Z. Yang, Y. Guo, and X. Chen, \"Droidbot: a lightweight ui- guided test input generator for android,\" in ICSE. IEEE, 2017.\n\nGuided, stochastic model-based gui testing of android apps. T Su, G Meng, Y Chen, K Wu, W Yang, Y Yao, G Pu, Y Liu, Z Su, Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. the 2017 11th Joint Meeting on Foundations of Software EngineeringT. Su, G. Meng, Y. Chen, K. Wu, W. Yang, Y. Yao, G. Pu, Y. Liu, and Z. Su, \"Guided, stochastic model-based gui testing of android apps,\" in Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, 2017, pp. 245-256.\n\nTimetravel testing of android apps. Z Dong, M B\u00f6hme, L Cojocaru, A Roychoudhury, ICSE. IEEEZ. Dong, M. B\u00f6hme, L. Cojocaru, and A. Roychoudhury, \"Time- travel testing of android apps,\" in ICSE. IEEE, 2020.\n\nReinforcement learning based curiosity-driven testing of android applications. M Pan, A Huang, G Wang, T Zhang, X Li, Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis. the 29th ACM SIGSOFT International Symposium on Software Testing and AnalysisM. Pan, A. Huang, G. Wang, T. Zhang, and X. Li, \"Reinforcement learning based curiosity-driven testing of android applications,\" in Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, 2020, pp. 153-164.\n\nEditsum: A retrieve-and-edit framework for source code summarization. J Li, Y Li, G Li, X Hu, X Xia, Z Jin, 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021. Melbourne, AustraliaJ. Li, Y. Li, G. Li, X. Hu, X. Xia, and Z. Jin, \"Editsum: A retrieve-and-edit framework for source code summarization,\" in 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021, Melbourne, Australia, November 15-19, 2021. IEEE, 2021, pp. 155-166. [Online].\n\n. 10.1109/ASE51524.2021.9678724Available: https://doi.org/10.1109/ASE51524.2021.9678724\n\nSelf-collaboration code generation via chatgpt. Y Dong, X Jiang, Z Jin, G Li, 10.48550/arXiv.2304.07590abs/2304.07590CoRR. Y. Dong, X. Jiang, Z. Jin, and G. Li, \"Self-collaboration code generation via chatgpt,\" CoRR, vol. abs/2304.07590, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2304.07590\n\nTalking about large language models. M Shanahan, 10.48550/arXiv.2212.03551abs/2212.03551CoRR. M. Shanahan, \"Talking about large language models,\" CoRR, vol. abs/2212.03551, 2022. [Online]. Available: https: //doi.org/10.48550/arXiv.2212.03551\n\nA survey of large language models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J Nie, J Wen, 10.48550/arXiv.2303.18223abs/2303.18223CoRR. W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J. Nie, and J. Wen, \"A survey of large language models,\" CoRR, vol. abs/2303.18223, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2303.18223\n\nLarge language models are zeroshot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, in NeurIPS, 2022. [Online]. AvailableT. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, \"Large language models are zero- shot reasoners,\" in NeurIPS, 2022. [Online]. Avail- able: http://papers.nips.cc/paper files/paper/2022/hash/ 8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html\n\nChain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, in NeurIPS, 2022. [Online]. AvailableJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou, \"Chain-of-thought prompting elicits reasoning in large language models,\" in NeurIPS, 2022. [Online]. Avail- able: http://papers.nips.cc/paper files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html\n\nUnifying large language models and knowledge graphs: A roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, 10.48550/arXiv.2306.08302abs/2306.08302CoRR. S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, \"Unifying large language models and knowledge graphs: A roadmap,\" CoRR, vol. abs/2306.08302, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2306.08302\n\nThe art of software testing. G J Myers, T Badgett, T M Thomas, C Sandler, Wiley Online Library2G. J. Myers, T. Badgett, T. M. Thomas, and C. Sandler, The art of software testing. Wiley Online Library, 2004, vol. 2.\n\nShe elicits requirements and he tests: Software engineering gender bias in large language models. C Treude, H Hata, 10.48550/arXiv.2303.10131abs/2303.10131CoRR. C. Treude and H. Hata, \"She elicits requirements and he tests: Software engineering gender bias in large language models,\" CoRR, vol. abs/2303.10131, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2303.10131\n\nAutobiastest: Controllable sentence generation for automated and open-ended social bias testing in language models. R Kocielnik, S Prabhumoye, V Zhang, R M Alvarez, A Anandkumar, 10.48550/arXiv.2302.07371abs/2302.07371CoRR. R. Kocielnik, S. Prabhumoye, V. Zhang, R. M. Alvarez, and A. Anandkumar, \"Autobiastest: Controllable sentence generation for automated and open-ended social bias testing in language models,\" CoRR, vol. abs/2302.07371, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2302.07371\n\nTo what extent do deep learning-based code recommenders generate predictions by cloning code from the training set. M Ciniselli, L Pascarella, G Bavota, 10.1145/3524842.352844019th IEEE/ACM International Conference on Mining Software Repositories, MSR 2022. Pittsburgh, PA, USAACMM. Ciniselli, L. Pascarella, and G. Bavota, \"To what extent do deep learning-based code recommenders generate predictions by cloning code from the training set?\" in 19th IEEE/ACM International Conference on Mining Software Repositories, MSR 2022, Pittsburgh, PA, USA, May 23-24, 2022. ACM, 2022, pp. 167-178. [Online]. Available: https://doi.org/10.1145/3524842.3528440\n\nMeasuring the runtime performance of code produced with github copilot. D Erhabor, S Udayashankar, M Nagappan, S Al-Kiswany, abs/2305.06439CoRR. D. Erhabor, S. Udayashankar, M. Nagappan, and S. Al-Kiswany, \"Measuring the runtime performance of code produced with github copilot,\" CoRR, vol. abs/2305.06439, 2023. [Online].\n\n. 10.48550/arXiv.2305.06439Available: https://doi.org/10.48550/arXiv.2305.06439\n\nInvestigating and designing for trust in ai-powered code generation tools. R Wang, R Cheng, D Ford, T Zimmermann, 10.48550/arXiv.2305.11248abs/2305.11248CoRR. R. Wang, R. Cheng, D. Ford, and T. Zimmermann, \"Investigating and designing for trust in ai-powered code generation tools,\" CoRR, vol. abs/2305.11248, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2305.11248\n\nEvaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt. B Yetistiren, I \u00d6zsoy, M Ayerdem, E T\u00fcz\u00fcn, 10.48550/arXiv.2304.10778abs/2304.10778CoRR. B. Yetistiren, I.\u00d6zsoy, M. Ayerdem, and E. T\u00fcz\u00fcn, \"Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt,\" CoRR, vol. abs/2304.10778, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2304.10778\n\nUnit test case generation with transformers and focal context. M Tufano, D Drain, A Svyatkovskiy, S K Deng, N Sundaresan, arXiv:2009.05617arXiv preprintM. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, and N. Sun- daresan, \"Unit test case generation with transformers and focal context,\" arXiv preprint arXiv:2009.05617, 2020.\n\nCodet: Code generation with generated tests. B Chen, F Zhang, A Nguyen, D Zan, Z Lin, J.-G Lou, W Chen, arXiv:2207.10397arXiv preprintB. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and W. Chen, \"Codet: Code generation with generated tests,\" arXiv preprint arXiv:2207.10397, 2022.\n\nInteractive code generation via test-driven user-intent formalization. S K Lahiri, A Naik, G Sakkas, P Choudhury, C Veh, M Musuvathi, J P Inala, C Wang, J Gao, arXiv:2208.05950arXiv preprintS. K. Lahiri, A. Naik, G. Sakkas, P. Choudhury, C. von Veh, M. Musuvathi, J. P. Inala, C. Wang, and J. Gao, \"Interactive code generation via test-driven user-intent formalization,\" arXiv preprint arXiv:2208.05950, 2022.\n\nA3test: Assertion-augmented automated test case generation. S Alagarsamy, C Tantithamthavorn, A Aleti, arXiv:2302.10352arXiv preprintS. Alagarsamy, C. Tantithamthavorn, and A. Aleti, \"A3test: Assertion-augmented automated test case generation,\" arXiv preprint arXiv:2302.10352, 2023.\n\nAdaptive test generation using a large language model. M Sch\u00e4fer, S Nadi, A Eghbali, F Tip, arXiv:2302.06527arXiv preprintM. Sch\u00e4fer, S. Nadi, A. Eghbali, and F. Tip, \"Adaptive test generation using a large language model,\" arXiv preprint arXiv:2302.06527, 2023.\n\nChatunitest: a chatgpt-based automated unit test generation tool. Z Xie, Y Chen, C Zhi, S Deng, J Yin, arXiv:2305.04764arXiv preprintZ. Xie, Y. Chen, C. Zhi, S. Deng, and J. Yin, \"Chatunitest: a chatgpt-based automated unit test generation tool,\" arXiv preprint arXiv:2305.04764, 2023.\n\nCodamosa: Escaping coverage plateaus in test generation with pre-trained large language models. C Lemieux, J P Inala, S K Lahiri, S Sen, International conference on software engineering (ICSE). 2023C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen, \"Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models,\" in International conference on software engineering (ICSE), 2023.\n\nExploring the effectiveness of large language models in generating unit tests. M L Siddiq, J Santos, R H Tanvir, N Ulfat, F A Rifat, V C Lopes, arXiv:2305.00418arXiv preprintM. L. Siddiq, J. Santos, R. H. Tanvir, N. Ulfat, F. A. Rifat, and V. C. Lopes, \"Exploring the effectiveness of large language models in generating unit tests,\" arXiv preprint arXiv:2305.00418, 2023.\n\nNo more manual tests? evaluating and improving chatgpt for unit test generation. Z Yuan, Y Lou, M Liu, S Ding, K Wang, Y Chen, X Peng, arXiv:2305.04207arXiv preprintZ. Yuan, Y. Lou, M. Liu, S. Ding, K. Wang, Y. Chen, and X. Peng, \"No more manual tests? evaluating and improving chatgpt for unit test generation,\" arXiv preprint arXiv:2305.04207, 2023.\n\nGenerating accurate assert statements for unit test cases using pretrained transformers. M Tufano, D Drain, A Svyatkovskiy, N Sundaresan, Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test. the 3rd ACM/IEEE International Conference on Automation of Software TestM. Tufano, D. Drain, A. Svyatkovskiy, and N. Sundaresan, \"Generating accurate assert statements for unit test cases using pretrained transformers,\" in Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test, 2022, pp. 54-64.\n\nLearning deep semantics for test completion. P Nie, R Banerjee, J J Li, R J Mooney, M Gligoric, arXiv:2302.10166arXiv preprintP. Nie, R. Banerjee, J. J. Li, R. J. Mooney, and M. Gligoric, \"Learning deep semantics for test completion,\" arXiv preprint arXiv:2302.10166, 2023.\n\nRetrieval-based prompt selection for code-related few-shot learning. N Nashid, M Sintaha, A Mesbah, Proceedings of the 45th International Conference on Software Engineering (ICSE'23). the 45th International Conference on Software Engineering (ICSE'23)2023N. Nashid, M. Sintaha, and A. Mesbah, \"Retrieval-based prompt selection for code-related few-shot learning,\" in Proceedings of the 45th International Conference on Software Engineering (ICSE'23), 2023.\n\nStudying the usage of text-to-text transfer transformer to support code-related tasks. A Mastropaolo, S Scalabrino, N Cooper, D Nader-Palacio, D Poshyvanyk, R Oliveto, G Bavota, 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021. Madrid, SpainIEEEA. Mastropaolo, S. Scalabrino, N. Cooper, D. Nader-Palacio, D. Poshyvanyk, R. Oliveto, and G. Bavota, \"Studying the usage of text-to-text transfer transformer to support code-related tasks,\" in 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 2021, pp. 336- 347.\n\nUsing transfer learning for code-related tasks. A Mastropaolo, N Cooper, D Nader-Palacio, S Scalabrino, D Poshyvanyk, R Oliveto, G Bavota, 10.1109/TSE.2022.3183297IEEE Trans. Software Eng. 494A. Mastropaolo, N. Cooper, D. Nader-Palacio, S. Scalabrino, D. Poshyvanyk, R. Oliveto, and G. Bavota, \"Using transfer learning for code-related tasks,\" IEEE Trans. Software Eng., vol. 49, no. 4, pp. 1580-1598, 2023. [Online]. Available: https://doi.org/10.1109/TSE.2022.3183297\n\nAutomated conformance testing for javascript engines via deep compiler fuzzing. G Ye, Z Tang, S H Tan, S Huang, D Fang, X Sun, L Bian, H Wang, Z Wang, Proceedings of the 42nd ACM SIGPLAN international conference on programming language design and implementation. the 42nd ACM SIGPLAN international conference on programming language design and implementationG. Ye, Z. Tang, S. H. Tan, S. Huang, D. Fang, X. Sun, L. Bian, H. Wang, and Z. Wang, \"Automated conformance testing for javascript engines via deep compiler fuzzing,\" in Proceedings of the 42nd ACM SIGPLAN international conference on programming language design and implementation, 2021, pp. 435-450.\n\nLarge language models are pretty good zero-shot video game bug detectors. M R Taesiri, F Macklon, Y Wang, H Shen, C.-P Bezemer, arXiv:2210.02506arXiv preprintM. R. Taesiri, F. Macklon, Y. Wang, H. Shen, and C.-P. Bezemer, \"Large language models are pretty good zero-shot video game bug detectors,\" arXiv preprint arXiv:2210.02506, 2022.\n\nSlgpt: using transfer learning to directly generate simulink model files and find bugs in the simulink toolchain. S L Shrestha, C Csallner, Evaluation and Assessment in Software Engineering. S. L. Shrestha and C. Csallner, \"Slgpt: using transfer learning to directly generate simulink model files and find bugs in the simulink toolchain,\" in Evaluation and Assessment in Software Engineering, 2021, pp. 260-265.\n\nAutomating gui-based software testing with gpt-3. D Zimmermann, A Koziolek, 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW). D. Zimmermann and A. Koziolek, \"Automating gui-based soft- ware testing with gpt-3,\" in 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW), 2023, pp. 62-65.\n\nChatting with gpt-3 for zero-shot human-like mobile automated gui testing. Z Liu, C Chen, J Wang, M Chen, B Wu, X Che, D Wang, Q Wang, arXiv:2305.09434arXiv preprintZ. Liu, C. Chen, J. Wang, M. Chen, B. Wu, X. Che, D. Wang, and Q. Wang, \"Chatting with gpt-3 for zero-shot human-like mobile automated gui testing,\" arXiv preprint arXiv:2305.09434, 2023.\n\nEfficient mutation testing via pre-trained language models. A Khanfir, R Degiovanni, M Papadakis, Y L Traon, arXiv:2301.03543arXiv preprintA. Khanfir, R. Degiovanni, M. Papadakis, and Y. L. Traon, \"Ef- ficient mutation testing via pre-trained language models,\" arXiv preprint arXiv:2301.03543, 2023.\n\nFill in the blank: Context-aware automated text input generation for mobile gui testing. Z Liu, C Chen, J Wang, X Che, Y Huang, J Hu, Q Wang, arXiv:2212.04732arXiv preprintZ. Liu, C. Chen, J. Wang, X. Che, Y. Huang, J. Hu, and Q. Wang, \"Fill in the blank: Context-aware automated text input generation for mobile gui testing,\" arXiv preprint arXiv:2212.04732, 2022.\n\nLarge language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt. Y Deng, C S Xia, C Yang, S D Zhang, S Yang, L Zhang, arXiv:2304.02014arXiv preprintY. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, \"Large language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt,\" arXiv preprint arXiv:2304.02014, 2023.\n\nLarge language models are zero shot fuzzers: Fuzzing deep learning libraries via large language models. arXiv:2209.11515arXiv preprint--, \"Large language models are zero shot fuzzers: Fuzzing deep learning libraries via large language models,\" arXiv preprint arXiv:2209.11515, 2023.\n\nTarget: Traffic rule-based test generation for autonomous driving systems. Y Deng, J Yao, Z Tu, X Zheng, M Zhang, T Zhang, arXiv:2305.06018arXiv preprintY. Deng, J. Yao, Z. Tu, X. Zheng, M. Zhang, and T. Zhang, \"Tar- get: Traffic rule-based test generation for autonomous driving systems,\" arXiv preprint arXiv:2305.06018, 2023.\n\nLarge language models: The next frontier for variable discovery within metamorphic testing?. C Tsigkanos, P Rani, S M\u00fcller, T Kehrer, 10.1109/SANER56733.2023.00070IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2023. T. Zhang, X. Xia, and N. NovielliTaipa, MacaoIEEEC. Tsigkanos, P. Rani, S. M\u00fcller, and T. Kehrer, \"Large language models: The next frontier for variable discovery within metamorphic testing?\" in IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2023, Taipa, Macao, March 21-24, 2023, T. Zhang, X. Xia, and N. Novielli, Eds. IEEE, 2023, pp. 678-682. [Online]. Available: https://doi.org/10.1109/SANER56733.2023.00070\n\nVariable discovery with large language models for metamorphic testing of scientific software. 10.1007/978-3-031-35995-8_23Computational Science -ICCS 2023 -23rd International Conference. J. Mikyska, C. de Mulatier, M. Paszynski, V. V. Krzhizhanovskaya, J. J. Dongarra, and P. M. A. SlootPrague, Czech RepublicSpringer14073Proceedings, Part I, ser. Lecture Notes in Computer Science--, \"Variable discovery with large language models for metamorphic testing of scientific software,\" in Computational Science -ICCS 2023 -23rd International Conference, Prague, Czech Republic, July 3-5, 2023, Proceedings, Part I, ser. Lecture Notes in Computer Science, J. Mikyska, C. de Mulatier, M. Paszynski, V. V. Krzhizhanovskaya, J. J. Dongarra, and P. M. A. Sloot, Eds., vol. 14073. Springer, 2023, pp. 321-335. [Online]. Available: https://doi.org/10.1007/978-3-031-35995-8 23\n\nitiger: an automatic issue title generation tool. T Zhang, I C Irsan, F Thung, D Han, D Lo, L Jiang, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software EngineeringT. Zhang, I. C. Irsan, F. Thung, D. Han, D. Lo, and L. Jiang, \"itiger: an automatic issue title generation tool,\" in Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2022, pp. 1637-1641.\n\nExplaining software bugs leveraging code structures in neural machine translation. P Mahbub, O Shuvo, M M Rahman, arXiv:2212.04584arXiv preprintP. Mahbub, O. Shuvo, and M. M. Rahman, \"Explaining software bugs leveraging code structures in neural machine translation,\" arXiv preprint arXiv:2212.04584, 2022.\n\nDetect-localize-repair: A unified framework for learning to debug with codet5. N D Bui, Y Wang, S Hoi, arXiv:2211.14875arXiv preprintN. D. Bui, Y. Wang, and S. Hoi, \"Detect-localize-repair: A unified framework for learning to debug with codet5,\" arXiv preprint arXiv:2211.14875, 2022.\n\nLarge language models are few-shot testers: Exploring llm-based general bug reproduction. S Kang, J Yoon, S Yoo, arXiv:2209.11515arXiv preprintS. Kang, J. Yoon, and S. Yoo, \"Large language models are few-shot testers: Exploring llm-based general bug reproduction,\" arXiv preprint arXiv:2209.11515, 2022.\n\nExplainable automated debugging via large language model-driven scientific debugging. S Kang, B Chen, S Yoo, J.-G Lou, arXiv:2304.02195arXiv preprintS. Kang, B. Chen, S. Yoo, and J.-G. Lou, \"Explainable automated debugging via large language model-driven scientific debug- ging,\" arXiv preprint arXiv:2304.02195, 2023.\n\nFinding failure-inducing test cases with chatgpt. T.-O Li, W Zong, Y Wang, H Tian, Y Wang, S.-C Cheung, arXiv:2304.11686arXiv preprintT.-O. Li, W. Zong, Y. Wang, H. Tian, Y. Wang, and S.-C. Cheung, \"Finding failure-inducing test cases with chatgpt,\" arXiv preprint arXiv:2304.11686, 2023.\n\nA study on prompt design, advantages and limitations of chatgpt for deep learning program repair. J Cao, M Li, M Wen, S.-C Cheung, arXiv:2304.08191arXiv preprintJ. Cao, M. Li, M. Wen, and S.-c. Cheung, \"A study on prompt design, advantages and limitations of chatgpt for deep learning program repair,\" arXiv preprint arXiv:2304.08191, 2023.\n\nAutomated repair of programs from large language models. Z Fan, X Gao, A Roychoudhury, S H Tan, arXiv:2205.10583arXiv preprintZ. Fan, X. Gao, A. Roychoudhury, and S. H. Tan, \"Automated repair of programs from large language models,\" arXiv preprint arXiv:2205.10583, 2022.\n\nCan openai's codex fix bugs? an evaluation on quixbugs. J A Prenner, H Babii, R Robbes, Proceedings of the Third International Workshop on Automated Program Repair. the Third International Workshop on Automated Program RepairJ. A. Prenner, H. Babii, and R. Robbes, \"Can openai's codex fix bugs? an evaluation on quixbugs,\" in Proceedings of the Third International Workshop on Automated Program Repair, 2022, pp. 69- 75.\n\nFix bugs with transformer through a neural-symbolic edit grammar. Y Hu, X Shi, Q Zhou, L Pike, arXiv:2204.06643arXiv preprintY. Hu, X. Shi, Q. Zhou, and L. Pike, \"Fix bugs with trans- former through a neural-symbolic edit grammar,\" arXiv preprint arXiv:2204.06643, 2022.\n\nPractical program repair in the era of large pre-trained language models. C S Xia, Y Wei, L Zhang, arXiv:2210.14179arXiv preprintC. S. Xia, Y. Wei, and L. Zhang, \"Practical program repair in the era of large pre-trained language models,\" arXiv preprint arXiv:2210.14179, 2022.\n\nRepairing bugs in python assignments using large language models. J Zhang, J Cambronero, S Gulwani, V Le, R Piskac, G Soares, G Verbruggen, arXiv:2209.14876arXiv preprintJ. Zhang, J. Cambronero, S. Gulwani, V. Le, R. Piskac, G. Soares, and G. Verbruggen, \"Repairing bugs in python assignments using large language models,\" arXiv preprint arXiv:2209.14876, 2022.\n\nTowards javascript program repair with generative pre-trained transformer (gpt-2). M Lajk\u00f3, V Csuvik, L Vid\u00e1cs, Proceedings of the Third International Workshop on Automated Program Repair. the Third International Workshop on Automated Program RepairM. Lajk\u00f3, V. Csuvik, and L. Vid\u00e1cs, \"Towards javascript program repair with generative pre-trained transformer (gpt-2),\" in Pro- ceedings of the Third International Workshop on Automated Program Repair, 2022, pp. 61-68.\n\nAn analysis of the automatic bug fixing performance of chatgpt. D Sobania, M Briesch, C Hanna, J Petke, arXiv:2301.08653arXiv preprintD. Sobania, M. Briesch, C. Hanna, and J. Petke, \"An analysis of the automatic bug fixing performance of chatgpt,\" arXiv preprint arXiv:2301.08653, 2023.\n\nCircle: continual repair across programming languages. W Yuan, Q Zhang, T He, C Fang, N Q V Hung, X Hao, H Yin, Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis. the 31st ACM SIGSOFT International Symposium on Software Testing and AnalysisW. Yuan, Q. Zhang, T. He, C. Fang, N. Q. V. Hung, X. Hao, and H. Yin, \"Circle: continual repair across programming languages,\" in Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, 2022, pp. 678-690.\n\nDomain knowledge matters: Improving prompts with fix templates for repairing python type errors. Y Peng, S Gao, C Gao, Y Huo, M R Lyu, 10.48550/arXiv.2306.01394abs/2306.01394CoRR. Y. Peng, S. Gao, C. Gao, Y. Huo, and M. R. Lyu, \"Domain knowledge matters: Improving prompts with fix templates for repairing python type errors,\" CoRR, vol. abs/2306.01394, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2306.01394\n\nExamining zero-shot vulnerability repair with large language models. H Pearce, B Tan, B Ahmad, R Karri, B Dolan-Gavitt, 2023 IEEE Symposium on Security and Privacy (SP). IEEE Computer SocietyH. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan-Gavitt, \"Examining zero-shot vulnerability repair with large language models,\" in 2023 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society, 2022, pp. 1-18.\n\nFraming program repair as code completion. F Ribeiro, R Abreu, J Saraiva, Proceedings of the Third International Workshop on Automated Program Repair, 2022. the Third International Workshop on Automated Program Repair, 2022F. Ribeiro, R. Abreu, and J. Saraiva, \"Framing program repair as code completion,\" in Proceedings of the Third International Workshop on Automated Program Repair, 2022, pp. 38-45.\n\nHow effective are neural networks for fixing security vulnerabilities. Y Wu, N Jiang, H V Pham, T Lutellier, J Davis, L Tan, P Babkin, S Shah, arXiv:2305.18607arXiv preprintY. Wu, N. Jiang, H. V. Pham, T. Lutellier, J. Davis, L. Tan, P. Babkin, and S. Shah, \"How effective are neural networks for fixing security vulnerabilities,\" arXiv preprint arXiv:2305.18607, 2023.\n\nImpact of code language models on automated program repair. N Jiang, K Liu, T Lutellier, L Tan, arXiv:2302.05020arXiv preprintN. Jiang, K. Liu, T. Lutellier, and L. Tan, \"Impact of code language models on automated program repair,\" arXiv preprint arXiv:2302.05020, 2023.\n\nInferfix: End-to-end program repair with llms. M Jin, S Shahriar, M Tufano, X Shi, S Lu, N Sundaresan, A Svyatkovskiy, arXiv:2303.07263arXiv preprintM. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, and A. Svyatkovskiy, \"Inferfix: End-to-end program repair with llms,\" arXiv preprint arXiv:2303.07263, 2023.\n\nKeep the conversation going: Fixing 162 out of 337 bugs for $0.42 each using chatgpt. C S Xia, L Zhang, arXiv:2304.00385arXiv preprintC. S. Xia and L. Zhang, \"Keep the conversation going: Fixing 162 out of 337 bugs for $0.42 each using chatgpt,\" arXiv preprint arXiv:2304.00385, 2023.\n\nNeural program repair with program dependence analysis and effective filter mechanism. Y Zhang, G Li, Z Jin, Y Xing, arXiv:2305.09315arXiv preprintY. Zhang, G. Li, Z. Jin, and Y. Xing, \"Neural program repair with program dependence analysis and effective filter mechanism,\" arXiv preprint arXiv:2305.09315, 2023.\n\nTowards generating functionally correct code edits from natural language issue descriptions. S Fakhoury, S Chakraborty, M Musuvathi, S K Lahiri, arXiv:2304.03816arXiv preprintS. Fakhoury, S. Chakraborty, M. Musuvathi, and S. K. Lahiri, \"Towards generating functionally correct code edits from natu- ral language issue descriptions,\" arXiv preprint arXiv:2304.03816, 2023.\n\nVulrepair: a t5-based automated software vulnerability repair. M Fu, C Tantithamthavorn, T Le, V Nguyen, D Phung, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software EngineeringM. Fu, C. Tantithamthavorn, T. Le, V. Nguyen, and D. Phung, \"Vulrepair: a t5-based automated software vulnerability repair,\" in Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineer- ing, 2022, pp. 935-947.\n\nThe art of software testing. G J Myers, WileyG. J. Myers, The art of software testing (2. ed.). Wiley, 2004. [Online]. Available: http://eu.wiley.com/WileyCDA/ WileyTitle/productCd-0471469122.html\n\nManage software testing. P Farrell-Vinay, Auerbach PublP. Farrell-Vinay, Manage software testing. Auerbach Publ., 2008.\n\nA Mili, F Tchier, Software testing: Concepts and operations. John Wiley & SonsA. Mili and F. Tchier, Software testing: Concepts and operations. John Wiley & Sons, 2015.\n\nPynguin: Automated unit test generation for python. S Lukasczyk, G Fraser, 10.1145/3510454.351682944th IEEE/ACM International Conference on Software Engineering: Companion Proceedings, ICSE Companion 2022. Pittsburgh, PA, USAACM/IEEE2022S. Lukasczyk and G. Fraser, \"Pynguin: Automated unit test generation for python,\" in 44th IEEE/ACM International Conference on Software Engineering: Companion Proceedings, ICSE Companion 2022, Pittsburgh, PA, USA, May 22-24, 2022. ACM/IEEE, 2022, pp. 168-172. [Online]. Available: https://doi.org/10.1145/ 3510454.3516829\n\nOn learning meaningful assert statements for unit test cases. C Watson, M Tufano, K Moran, G Bavota, D Poshyvanyk, ICSE '20: 42nd International Conference on Software Engineering. G. Rothermel and D. BaeSeoul, South KoreaACMC. Watson, M. Tufano, K. Moran, G. Bavota, and D. Poshyvanyk, \"On learning meaningful assert statements for unit test cases,\" in ICSE '20: 42nd International Conference on Software Engineering, Seoul, South Korea, 27 June -19 July, 2020, G. Rothermel and D. Bae, Eds. ACM, 2020, pp. 1398-1409.\n\nFree lunch for testing: Fuzzing deep-learning libraries from open source. A Wei, Y Deng, C Yang, L Zhang, 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022. Pittsburgh, PA, USAACMA. Wei, Y. Deng, C. Yang, and L. Zhang, \"Free lunch for test- ing: Fuzzing deep-learning libraries from open source,\" in 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 2022, pp. 995-1007.\n\nDocter: documentation-guided fuzzing for testing deep learning API functions. D Xie, Y Li, M Kim, H V Pham, L Tan, X Zhang, M W Godfrey, ISSTA '22: 31st ACM SIGSOFT International Symposium on Software Testing and Analysis. S. Ryu and Y. SmaragdakisACMD. Xie, Y. Li, M. Kim, H. V. Pham, L. Tan, X. Zhang, and M. W. Godfrey, \"Docter: documentation-guided fuzzing for testing deep learning API functions,\" in ISSTA '22: 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Event, South Korea, July 18 -22, 2022, S. Ryu and Y. Smaragdakis, Eds. ACM, 2022, pp. 176-188.\n\nAudee: Automated testing for deep learning frameworks. Q Guo, X Xie, Y Li, X Zhang, Y Liu, X Li, C Shen, 35th IEEE/ACM International Conference on Automated Software Engineering. Melbourne, Australia2020Q. Guo, X. Xie, Y. Li, X. Zhang, Y. Liu, X. Li, and C. Shen, \"Audee: Automated testing for deep learning frameworks,\" in 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020, Melbourne, Australia, September 21-25, 2020. IEEE, 2020, pp. 486-498.\n\nDeep learning library testing via effective model generation. Z Wang, M Yan, J Chen, S Liu, D Zhang, ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. P. Devanbu, M. B. Cohen, and T. ZimmermannACMVirtual Event, USAZ. Wang, M. Yan, J. Chen, S. Liu, and D. Zhang, \"Deep learning library testing via effective model generation,\" in ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020, P. Devanbu, M. B. Cohen, and T. Zimmermann, Eds. ACM, 2020, pp. 788-799.\n\nTextexerciser: Feedback-driven text input exercising for android applications. Y He, L Zhang, Z Yang, Y Cao, K Lian, S Li, W Yang, Z Zhang, M Yang, Y Zhang, H Duan, 2020 IEEE Symposium on Security and Privacy. San Francisco, CA, USAIEEE2020Y. He, L. Zhang, Z. Yang, Y. Cao, K. Lian, S. Li, W. Yang, Z. Zhang, M. Yang, Y. Zhang, and H. Duan, \"Textexerciser: Feedback-driven text input exercising for android applications,\" in 2020 IEEE Symposium on Security and Privacy, SP 2020, San Francisco, CA, USA, May 18-21, 2020. IEEE, 2020, pp. 1071-1087.\n\nShaping program repair space with existing patches and similar code. J Jiang, Y Xiong, H Zhang, Q Gao, X Chen, 10.1145/3213846.3213871Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis. the 27th ACM SIGSOFT International Symposium on Software Testing and AnalysisNew York, NY, USAAssociation for Computing MachineryJ. Jiang, Y. Xiong, H. Zhang, Q. Gao, and X. Chen, \"Shaping program repair space with existing patches and similar code,\" in Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis, ser. ISSTA 2018. New York, NY, USA: Association for Computing Machinery, 2018, p. 298-309. [Online]. Available: https://doi.org/10.1145/3213846.3213871\n\nContext-aware patch generation for better automated program repair. M Wen, J Chen, R Wu, D Hao, S.-C Cheung, Proceedings of the 40th International Conference on Software Engineering, ser. ICSE '18. the 40th International Conference on Software Engineering, ser. ICSE '18New York, NY, USAAssociation for Computing MachineryM. Wen, J. Chen, R. Wu, D. Hao, and S.-C. Cheung, \"Context-aware patch generation for better automated program repair,\" in Proceedings of the 40th International Conference on Software Engineering, ser. ICSE '18. New York, NY, USA: Association for Computing Machinery, 2018, p. 1-11. [Online].\n\n. 10.1145/3180155.3180233Available: https://doi.org/10.1145/3180155.3180233\n\nPrecise condition synthesis for program repair. Y Xiong, J Wang, R Yan, J Zhang, S Han, G Huang, L Zhang, 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE. Y. Xiong, J. Wang, R. Yan, J. Zhang, S. Han, G. Huang, and L. Zhang, \"Precise condition synthesis for program repair,\" in 2017 IEEE/ACM 39th International Conference on Software Engineer- ing (ICSE), 2017, pp. 416-426.\n\nNopol: Automatic repair of conditional statement bugs in java programs. J Xuan, M Martinez, F Demarco, M Cl\u00e9ment, S L Marcote, T Durieux, D Le Berre, M Monperrus, IEEE Transactions on Software Engineering. 431J. Xuan, M. Martinez, F. DeMarco, M. Cl\u00e9ment, S. L. Marcote, T. Durieux, D. Le Berre, and M. Monperrus, \"Nopol: Automatic repair of conditional statement bugs in java programs,\" IEEE Transactions on Software Engineering, vol. 43, no. 1, pp. 34-55, 2017.\n\nPrompting large language model for machine translation: A case study. B Zhang, B Haddow, A Birch, arXiv:2301.07069arXiv preprintB. Zhang, B. Haddow, and A. Birch, \"Prompting large language model for machine translation: A case study,\" arXiv preprint arXiv:2301.07069, 2023.\n\nEmpirically revisiting and enhancing ir-based test-case prioritization. Q Peng, A Shi, L Zhang, Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis. the 29th ACM SIGSOFT International Symposium on Software Testing and AnalysisQ. Peng, A. Shi, and L. Zhang, \"Empirically revisiting and enhancing ir-based test-case prioritization,\" in Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, 2020, pp. 324-336.\n\nAn information retrieval approach for regression test prioritization based on program changes. R K Saha, L Zhang, S Khurshid, D E Perry, 37th IEEE/ACM International Conference on Software Engineering. A. Bertolino, G. Canfora, and S. G. ElbaumFlorence, ItalyIEEE Computer Society1R. K. Saha, L. Zhang, S. Khurshid, and D. E. Perry, \"An information retrieval approach for regression test prioritization based on program changes,\" in 37th IEEE/ACM International Conference on Software Engineering, ICSE 2015, Florence, Italy, May 16-24, 2015, Volume 1, A. Bertolino, G. Canfora, and S. G. Elbaum, Eds. IEEE Computer Society, 2015, pp. 268-279. [Online].\n\n. 10.1109/ICSE.2015.47Available: https://doi.org/10.1109/ICSE.2015.47\n\nReducing bug triaging confusion by learning from mistakes with a bug tossing knowledge graph. Y Su, Z Xing, X Peng, X Xia, C Wang, X Xu, L Zhu, 10.1109/ASE51524.2021.967857436th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021. Melbourne, AustraliaY. Su, Z. Xing, X. Peng, X. Xia, C. Wang, X. Xu, and L. Zhu, \"Reducing bug triaging confusion by learning from mistakes with a bug tossing knowledge graph,\" in 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021, Melbourne, Australia, November 15-19, 2021. IEEE, 2021, pp. 191-202. [Online]. Available: https://doi.org/10. 1109/ASE51524.2021.9678574\n\nLsun: Construction of a large-scale image dataset using deep learning with humans in the loop. F Yu, A Seff, Y Zhang, S Song, T Funkhouser, J Xiao, arXiv:1506.03365arXiv preprintF. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and J. Xiao, \"Lsun: Construction of a large-scale image dataset us- ing deep learning with humans in the loop,\" arXiv preprint arXiv:1506.03365, 2015.\n\n. Inc Langchain, Langchain, LangChain, Inc., \"Langchain,\" 2023, https://docs.langchain. com/docs/.\n\nDatabase testing with llm. celerdata.com, \"Database testing with llm,\" 2023, https:// celerdata.com/blog/chatgpt-is-now-finding-bugs-in-databases? s=05.\n\nBe careful of when: an empirical study on time-related misuse of issue tracking data. F Tu, J Zhu, Q Zheng, M Zhou, 10.1145/3236024.3236054Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018. G. T. Leavens, A. Garcia, and C. S. Pasareanuthe 2018 ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018Lake Buena Vista, FL, USAACMF. Tu, J. Zhu, Q. Zheng, and M. Zhou, \"Be careful of when: an empirical study on time-related misuse of issue tracking data,\" in Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November 04-09, 2018, G. T. Leavens, A. Garcia, and C. S. Pasareanu, Eds. ACM, 2018, pp. 307-318. [Online]. Available: https://doi.org/10.1145/3236024.3236054\n\nEvaluating defect prediction approaches: a benchmark and an extensive comparison. M D&apos;ambros, M Lanza, R Robbes, Empirical Software Engineering. 17M. D'Ambros, M. Lanza, and R. Robbes, \"Evaluating defect pre- diction approaches: a benchmark and an extensive comparison,\" Empirical Software Engineering, vol. 17, pp. 531-577, 2012.\n\nTowards reliable online just-intime software defect prediction. G G Cabral, L L Minku, IEEE Transactions on Software Engineering. 493G. G. Cabral and L. L. Minku, \"Towards reliable online just-in- time software defect prediction,\" IEEE Transactions on Software Engineering, vol. 49, no. 3, pp. 1342-1358, 2022.\n\nPrompt engineering guide. Prompt engineeringPrompt engineering, \"Prompt engineering guide,\" 2023, https: //github.com/dair-ai/Prompt-Engineering-Guide.\n\nOn the importance of building high-quality training datasets for neural code search. Z Sun, L Li, Y Liu, X Du, L Li, 10.1145/3510003.351016044th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022. Pittsburgh, PA, USAACMZ. Sun, L. Li, Y. Liu, X. Du, and L. Li, \"On the importance of building high-quality training datasets for neural code search,\" in 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25- 27, 2022. ACM, 2022, pp. 1609-1620. [Online]. Available: https://doi.org/10.1145/3510003.3510160\n\nISPY: automatic issue-solution pair extraction from community live chats. L Shi, Z Jiang, Y Yang, X Chen, Y Zhang, F Mu, H Jiang, Q Wang, 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021. Melbourne, AustraliaL. Shi, Z. Jiang, Y. Yang, X. Chen, Y. Zhang, F. Mu, H. Jiang, and Q. Wang, \"ISPY: automatic issue-solution pair extraction from community live chats,\" in 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021, Melbourne, Australia, November 15-19, 2021. IEEE, 2021, pp. 142-154. [Online].\n\n. 10.1109/ASE51524.2021.9678894Available: https://doi.org/10.1109/ASE51524.2021.9678894\n\nGraphcodebert: Pre-training code representations with data flow. D Guo, S Ren, S Lu, Z Feng, D Tang, S Liu, L Zhou, N Duan, A Svyatkovskiy, S Fu, M Tufano, S K Deng, C B Clement, D Drain, N Sundaresan, J Yin, D Jiang, M Zhou, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaD. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. B. Clement, D. Drain, N. Sundaresan, J. Yin, D. Jiang, and M. Zhou, \"Graphcodebert: Pre-training code representations with data flow,\" in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [Online]. Available: https: //openreview.net/forum?id=jLoC4ez43PZ\n\nModular tree network for source code representation learning. W Wang, G Li, S Shen, X Xia, Z Jin, ACM Transactions on Software Engineering and Methodology (TOSEM). 294W. Wang, G. Li, S. Shen, X. Xia, and Z. Jin, \"Modular tree network for source code representation learning,\" ACM Transactions on Software Engineering and Methodology (TOSEM), vol. 29, no. 4, pp. 1-23, 2020.\n\nDetecting code clones with graph neural network and flow-augmented abstract syntax tree. W Wang, G Li, B Ma, X Xia, Z Jin, 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEEW. Wang, G. Li, B. Ma, X. Xia, and Z. Jin, \"Detecting code clones with graph neural network and flow-augmented abstract syntax tree,\" in 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2020, pp. 261-271.\n", "annotations": {"author": "[{\"end\":89,\"start\":77},{\"end\":103,\"start\":90},{\"end\":118,\"start\":104},{\"end\":127,\"start\":119},{\"end\":138,\"start\":128},{\"end\":149,\"start\":139}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":84},{\"end\":102,\"start\":97},{\"end\":117,\"start\":113},{\"end\":126,\"start\":123},{\"end\":137,\"start\":133},{\"end\":148,\"start\":144}]", "author_first_name": "[{\"end\":83,\"start\":77},{\"end\":96,\"start\":90},{\"end\":112,\"start\":104},{\"end\":122,\"start\":119},{\"end\":132,\"start\":128},{\"end\":143,\"start\":139}]", "author_affiliation": null, "title": "[{\"end\":74,\"start\":1},{\"end\":223,\"start\":150}]", "venue": null, "abstract": "[{\"end\":1777,\"start\":291}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3392,\"start\":3389},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3397,\"start\":3394},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3419,\"start\":3416},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3439,\"start\":3436},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3678,\"start\":3675},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3683,\"start\":3680},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3708,\"start\":3705},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3713,\"start\":3710},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3745,\"start\":3742},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5273,\"start\":5269},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5279,\"start\":5275},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10086,\"start\":10082},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10092,\"start\":10088},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11546,\"start\":11542},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11907,\"start\":11903},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12648,\"start\":12644},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12769,\"start\":12765},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15691,\"start\":15687},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17022,\"start\":17018},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17028,\"start\":17024},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17170,\"start\":17166},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17176,\"start\":17172},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18258,\"start\":18254},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18267,\"start\":18263},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19100,\"start\":19096},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19106,\"start\":19102},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":21419,\"start\":21415},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21442,\"start\":21438},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21451,\"start\":21447},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21457,\"start\":21453},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21466,\"start\":21462},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":22065,\"start\":22061},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25149,\"start\":25146},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25154,\"start\":25151},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25176,\"start\":25173},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25207,\"start\":25204},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26092,\"start\":26088},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26406,\"start\":26402},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26914,\"start\":26910},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27216,\"start\":27212},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28184,\"start\":28180},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":28313,\"start\":28309},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28948,\"start\":28944},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":30163,\"start\":30159},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30358,\"start\":30354},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30752,\"start\":30748},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31071,\"start\":31067},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31760,\"start\":31756},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":34197,\"start\":34193},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":34203,\"start\":34199},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":34231,\"start\":34227},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":34237,\"start\":34233},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":34726,\"start\":34722},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35242,\"start\":35238},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35926,\"start\":35923},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":35932,\"start\":35928},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":35988,\"start\":35984},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36583,\"start\":36580},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36588,\"start\":36585},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36613,\"start\":36610},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36618,\"start\":36615},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36650,\"start\":36647},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36776,\"start\":36772},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37684,\"start\":37680},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":38141,\"start\":38137},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":38382,\"start\":38378},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":38743,\"start\":38739},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":38908,\"start\":38904},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":39374,\"start\":39370},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":39617,\"start\":39613},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":39760,\"start\":39756},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":40127,\"start\":40123},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":40271,\"start\":40267},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":40930,\"start\":40926},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":40952,\"start\":40948},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":40979,\"start\":40975},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":40985,\"start\":40981},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":42256,\"start\":42252},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":42380,\"start\":42376},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":43172,\"start\":43168},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":43521,\"start\":43517},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":44262,\"start\":44258},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":44268,\"start\":44264},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":44274,\"start\":44270},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":44280,\"start\":44276},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":44286,\"start\":44282},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":44292,\"start\":44288},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":44392,\"start\":44388},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":44624,\"start\":44620},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":45556,\"start\":45552},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":45961,\"start\":45957},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":46263,\"start\":46259},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":46390,\"start\":46386},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":46953,\"start\":46949},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":46959,\"start\":46955},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":47100,\"start\":47096},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":47106,\"start\":47102},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":47112,\"start\":47108},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":47792,\"start\":47788},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":50691,\"start\":50687},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":50697,\"start\":50693},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":50729,\"start\":50725},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":51048,\"start\":51044},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":51054,\"start\":51050},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":51117,\"start\":51113},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":52506,\"start\":52505},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":52859,\"start\":52855},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":53040,\"start\":53036},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":53841,\"start\":53837},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":54123,\"start\":54119},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":54584,\"start\":54580},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":54861,\"start\":54857},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":55380,\"start\":55376},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":56114,\"start\":56110},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":56587,\"start\":56583},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":57518,\"start\":57514},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":57524,\"start\":57520},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":57867,\"start\":57863},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":58152,\"start\":58148},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":58271,\"start\":58267},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":60987,\"start\":60983},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":61209,\"start\":61205},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":61273,\"start\":61269},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":61668,\"start\":61664},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":61674,\"start\":61670},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":61680,\"start\":61676},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":61686,\"start\":61682},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":61692,\"start\":61688},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":62213,\"start\":62209},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":62464,\"start\":62460},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":62470,\"start\":62466},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":62608,\"start\":62604},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":63191,\"start\":63187},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":64023,\"start\":64019},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":64357,\"start\":64353},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":66479,\"start\":66475},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":67156,\"start\":67152},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":67181,\"start\":67177},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":67198,\"start\":67194},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":68796,\"start\":68792},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":69458,\"start\":69454},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":69464,\"start\":69460},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":70005,\"start\":70001},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":71634,\"start\":71630},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":73040,\"start\":73036},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":73452,\"start\":73448},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":74709,\"start\":74705},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":75164,\"start\":75160},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":76152,\"start\":76148},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":76664,\"start\":76660},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":77107,\"start\":77103},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":77113,\"start\":77109},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":77658,\"start\":77654},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":77670,\"start\":77666},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":78493,\"start\":78489},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":81811,\"start\":81807},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":83746,\"start\":83741},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":84021,\"start\":84016},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":84846,\"start\":84841},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":84928,\"start\":84923},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":84935,\"start\":84930},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":86635,\"start\":86631},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":87163,\"start\":87159}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":86126,\"start\":85880},{\"attributes\":{\"id\":\"fig_1\"},\"end\":86177,\"start\":86127},{\"attributes\":{\"id\":\"fig_2\"},\"end\":86538,\"start\":86178},{\"attributes\":{\"id\":\"fig_3\"},\"end\":86635,\"start\":86539},{\"attributes\":{\"id\":\"fig_4\"},\"end\":86680,\"start\":86636},{\"attributes\":{\"id\":\"fig_5\"},\"end\":86796,\"start\":86681},{\"attributes\":{\"id\":\"fig_6\"},\"end\":86897,\"start\":86797},{\"attributes\":{\"id\":\"fig_7\"},\"end\":86946,\"start\":86898},{\"attributes\":{\"id\":\"fig_8\"},\"end\":87048,\"start\":86947},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":87095,\"start\":87049},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":87637,\"start\":87096},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":88617,\"start\":87638}]", "paragraph": "[{\"end\":2583,\"start\":1793},{\"end\":3109,\"start\":2585},{\"end\":4102,\"start\":3111},{\"end\":5155,\"start\":4104},{\"end\":6234,\"start\":5157},{\"end\":6475,\"start\":6236},{\"end\":6944,\"start\":6477},{\"end\":7742,\"start\":6946},{\"end\":8183,\"start\":7744},{\"end\":8336,\"start\":8185},{\"end\":8519,\"start\":8338},{\"end\":8731,\"start\":8521},{\"end\":9821,\"start\":8733},{\"end\":10528,\"start\":9865},{\"end\":11335,\"start\":10530},{\"end\":12956,\"start\":11337},{\"end\":13364,\"start\":12977},{\"end\":13482,\"start\":13366},{\"end\":13583,\"start\":13484},{\"end\":13740,\"start\":13585},{\"end\":13849,\"start\":13742},{\"end\":14010,\"start\":13851},{\"end\":14175,\"start\":14012},{\"end\":14436,\"start\":14177},{\"end\":14610,\"start\":14438},{\"end\":14778,\"start\":14612},{\"end\":14923,\"start\":14780},{\"end\":15055,\"start\":14925},{\"end\":15212,\"start\":15057},{\"end\":15353,\"start\":15214},{\"end\":15496,\"start\":15355},{\"end\":15692,\"start\":15498},{\"end\":15936,\"start\":15745},{\"end\":16069,\"start\":15938},{\"end\":16210,\"start\":16071},{\"end\":16318,\"start\":16212},{\"end\":16482,\"start\":16320},{\"end\":16567,\"start\":16484},{\"end\":16642,\"start\":16569},{\"end\":16744,\"start\":16644},{\"end\":16916,\"start\":16746},{\"end\":17177,\"start\":16918},{\"end\":17969,\"start\":17210},{\"end\":18149,\"start\":17971},{\"end\":18307,\"start\":18151},{\"end\":18596,\"start\":18309},{\"end\":18804,\"start\":18598},{\"end\":19720,\"start\":18806},{\"end\":20030,\"start\":19722},{\"end\":20102,\"start\":20032},{\"end\":20150,\"start\":20104},{\"end\":20209,\"start\":20152},{\"end\":20276,\"start\":20211},{\"end\":20345,\"start\":20278},{\"end\":20407,\"start\":20347},{\"end\":20492,\"start\":20409},{\"end\":20597,\"start\":20494},{\"end\":20725,\"start\":20599},{\"end\":20856,\"start\":20727},{\"end\":21696,\"start\":20879},{\"end\":22749,\"start\":21736},{\"end\":22960,\"start\":22751},{\"end\":23545,\"start\":22995},{\"end\":24269,\"start\":23547},{\"end\":24936,\"start\":24299},{\"end\":25421,\"start\":24938},{\"end\":25863,\"start\":25423},{\"end\":26576,\"start\":25865},{\"end\":28007,\"start\":26578},{\"end\":28581,\"start\":28009},{\"end\":29331,\"start\":28583},{\"end\":29724,\"start\":29358},{\"end\":30346,\"start\":29726},{\"end\":31479,\"start\":30348},{\"end\":32446,\"start\":31481},{\"end\":33119,\"start\":32472},{\"end\":33840,\"start\":33121},{\"end\":35534,\"start\":33842},{\"end\":36265,\"start\":35536},{\"end\":37355,\"start\":36267},{\"end\":37956,\"start\":37357},{\"end\":38620,\"start\":37958},{\"end\":39103,\"start\":38637},{\"end\":40035,\"start\":39113},{\"end\":40464,\"start\":40037},{\"end\":40893,\"start\":40483},{\"end\":42021,\"start\":40895},{\"end\":42799,\"start\":42023},{\"end\":43761,\"start\":42801},{\"end\":45783,\"start\":43763},{\"end\":46141,\"start\":45785},{\"end\":46524,\"start\":46143},{\"end\":48329,\"start\":46526},{\"end\":48590,\"start\":48351},{\"end\":49237,\"start\":48605},{\"end\":50730,\"start\":49239},{\"end\":51118,\"start\":50732},{\"end\":52015,\"start\":51150},{\"end\":52275,\"start\":52017},{\"end\":53385,\"start\":52277},{\"end\":54298,\"start\":53387},{\"end\":55804,\"start\":54300},{\"end\":56841,\"start\":55806},{\"end\":57006,\"start\":56858},{\"end\":58056,\"start\":57008},{\"end\":58369,\"start\":58058},{\"end\":58758,\"start\":58371},{\"end\":59157,\"start\":58760},{\"end\":59643,\"start\":59186},{\"end\":60612,\"start\":59645},{\"end\":61781,\"start\":60614},{\"end\":63081,\"start\":61783},{\"end\":63317,\"start\":63083},{\"end\":63637,\"start\":63319},{\"end\":64481,\"start\":63639},{\"end\":64699,\"start\":64483},{\"end\":65256,\"start\":64732},{\"end\":66163,\"start\":65300},{\"end\":66834,\"start\":66165},{\"end\":67828,\"start\":66836},{\"end\":68190,\"start\":67830},{\"end\":68716,\"start\":68192},{\"end\":69573,\"start\":68718},{\"end\":70260,\"start\":69575},{\"end\":70541,\"start\":70308},{\"end\":71081,\"start\":70543},{\"end\":71513,\"start\":71083},{\"end\":72017,\"start\":71515},{\"end\":72869,\"start\":72019},{\"end\":73578,\"start\":72871},{\"end\":74328,\"start\":73580},{\"end\":74592,\"start\":74379},{\"end\":75668,\"start\":74594},{\"end\":76407,\"start\":75670},{\"end\":77240,\"start\":76409},{\"end\":78307,\"start\":77242},{\"end\":79351,\"start\":78336},{\"end\":79976,\"start\":79353},{\"end\":80625,\"start\":79978},{\"end\":82409,\"start\":80627},{\"end\":82697,\"start\":82411},{\"end\":83545,\"start\":82699},{\"end\":84286,\"start\":83547},{\"end\":85075,\"start\":84288},{\"end\":85879,\"start\":85090}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21259,\"start\":21252},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28794,\"start\":28787},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":46947,\"start\":46940},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":75905,\"start\":75898},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":76041,\"start\":76034},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":76634,\"start\":76627},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":77510,\"start\":77503}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1791,\"start\":1779},{\"attributes\":{\"n\":\"2\"},\"end\":9834,\"start\":9824},{\"attributes\":{\"n\":\"2.1\"},\"end\":9863,\"start\":9837},{\"attributes\":{\"n\":\"2.2\"},\"end\":12975,\"start\":12959},{\"attributes\":{\"n\":\"3\"},\"end\":15728,\"start\":15695},{\"attributes\":{\"n\":\"3.1\"},\"end\":15743,\"start\":15731},{\"attributes\":{\"n\":\"3.2\"},\"end\":17208,\"start\":17180},{\"attributes\":{\"n\":\"3.3\"},\"end\":20877,\"start\":20859},{\"attributes\":{\"n\":\"3.4\"},\"end\":21734,\"start\":21699},{\"attributes\":{\"n\":\"4\"},\"end\":22993,\"start\":22963},{\"attributes\":{\"n\":\"4.1\"},\"end\":24297,\"start\":24272},{\"attributes\":{\"n\":\"4.2\"},\"end\":29356,\"start\":29334},{\"attributes\":{\"n\":\"4.3\"},\"end\":32470,\"start\":32449},{\"attributes\":{\"n\":\"4.4\"},\"end\":38635,\"start\":38623},{\"attributes\":{\"n\":\"4.5\"},\"end\":39111,\"start\":39106},{\"attributes\":{\"n\":\"4.6\"},\"end\":40481,\"start\":40467},{\"attributes\":{\"n\":\"5\"},\"end\":48349,\"start\":48332},{\"attributes\":{\"n\":\"5.1\"},\"end\":48603,\"start\":48593},{\"attributes\":{\"n\":\"5.2\"},\"end\":51148,\"start\":51121},{\"attributes\":{\"n\":\"5.3\"},\"end\":56856,\"start\":56844},{\"attributes\":{\"n\":\"5.4\"},\"end\":59184,\"start\":59160},{\"attributes\":{\"n\":\"6\"},\"end\":64730,\"start\":64702},{\"attributes\":{\"n\":\"6.1\"},\"end\":65298,\"start\":65259},{\"attributes\":{\"n\":\"6.2\"},\"end\":70306,\"start\":70263},{\"attributes\":{\"n\":\"6.3\"},\"end\":74377,\"start\":74331},{\"attributes\":{\"n\":\"6.4\"},\"end\":78334,\"start\":78310},{\"attributes\":{\"n\":\"7\"},\"end\":85088,\"start\":85078},{\"end\":85889,\"start\":85881},{\"end\":86136,\"start\":86128},{\"end\":86187,\"start\":86179},{\"end\":86548,\"start\":86540},{\"end\":86645,\"start\":86637},{\"end\":86690,\"start\":86682},{\"end\":86806,\"start\":86798},{\"end\":86908,\"start\":86899},{\"end\":86957,\"start\":86948},{\"end\":87059,\"start\":87050},{\"end\":87106,\"start\":87097},{\"end\":87648,\"start\":87639}]", "table": "[{\"end\":87095,\"start\":87061},{\"end\":87637,\"start\":87257},{\"end\":88617,\"start\":87679}]", "figure_caption": "[{\"end\":86126,\"start\":85891},{\"end\":86177,\"start\":86138},{\"end\":86538,\"start\":86189},{\"end\":86635,\"start\":86550},{\"end\":86680,\"start\":86647},{\"end\":86796,\"start\":86692},{\"end\":86897,\"start\":86808},{\"end\":86946,\"start\":86911},{\"end\":87048,\"start\":86960},{\"end\":87257,\"start\":87108},{\"end\":87679,\"start\":87650}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6474,\"start\":6466},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13362,\"start\":13354},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22288,\"start\":22280},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22806,\"start\":22798},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23150,\"start\":23142},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":48625,\"start\":48617},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":51170,\"start\":51162},{\"end\":55165,\"start\":55159},{\"end\":57005,\"start\":56997},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":59317,\"start\":59309},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":65372,\"start\":65364},{\"end\":67994,\"start\":67986},{\"end\":68970,\"start\":68964},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":72232,\"start\":72223},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78516,\"start\":78507},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":80767,\"start\":80759}]", "bib_author_first_name": "[{\"end\":88713,\"start\":88712},{\"end\":88723,\"start\":88722},{\"end\":88974,\"start\":88973},{\"end\":88991,\"start\":88990},{\"end\":89002,\"start\":89001},{\"end\":89004,\"start\":89003},{\"end\":89019,\"start\":89018},{\"end\":89034,\"start\":89033},{\"end\":89036,\"start\":89035},{\"end\":89404,\"start\":89403},{\"end\":89412,\"start\":89411},{\"end\":89418,\"start\":89417},{\"end\":89425,\"start\":89424},{\"end\":89974,\"start\":89973},{\"end\":89985,\"start\":89984},{\"end\":89987,\"start\":89986},{\"end\":89997,\"start\":89996},{\"end\":89999,\"start\":89998},{\"end\":90008,\"start\":90007},{\"end\":90404,\"start\":90403},{\"end\":90542,\"start\":90541},{\"end\":90548,\"start\":90547},{\"end\":90556,\"start\":90555},{\"end\":90563,\"start\":90562},{\"end\":90771,\"start\":90770},{\"end\":90777,\"start\":90776},{\"end\":90785,\"start\":90784},{\"end\":90793,\"start\":90792},{\"end\":90799,\"start\":90798},{\"end\":90807,\"start\":90806},{\"end\":90814,\"start\":90813},{\"end\":90820,\"start\":90819},{\"end\":90827,\"start\":90826},{\"end\":91261,\"start\":91260},{\"end\":91269,\"start\":91268},{\"end\":91278,\"start\":91277},{\"end\":91290,\"start\":91289},{\"end\":91510,\"start\":91509},{\"end\":91517,\"start\":91516},{\"end\":91526,\"start\":91525},{\"end\":91534,\"start\":91533},{\"end\":91543,\"start\":91542},{\"end\":92036,\"start\":92035},{\"end\":92042,\"start\":92041},{\"end\":92048,\"start\":92047},{\"end\":92054,\"start\":92053},{\"end\":92060,\"start\":92059},{\"end\":92067,\"start\":92066},{\"end\":92602,\"start\":92601},{\"end\":92610,\"start\":92609},{\"end\":92619,\"start\":92618},{\"end\":92626,\"start\":92625},{\"end\":92899,\"start\":92898},{\"end\":93141,\"start\":93140},{\"end\":93143,\"start\":93142},{\"end\":93151,\"start\":93150},{\"end\":93159,\"start\":93158},{\"end\":93165,\"start\":93164},{\"end\":93173,\"start\":93172},{\"end\":93181,\"start\":93180},{\"end\":93188,\"start\":93187},{\"end\":93195,\"start\":93194},{\"end\":93204,\"start\":93203},{\"end\":93213,\"start\":93212},{\"end\":93221,\"start\":93220},{\"end\":93227,\"start\":93226},{\"end\":93235,\"start\":93234},{\"end\":93243,\"start\":93242},{\"end\":93251,\"start\":93250},{\"end\":93260,\"start\":93259},{\"end\":93267,\"start\":93266},{\"end\":93273,\"start\":93272},{\"end\":93281,\"start\":93280},{\"end\":93288,\"start\":93287},{\"end\":93295,\"start\":93294},{\"end\":93302,\"start\":93301},{\"end\":93729,\"start\":93728},{\"end\":93739,\"start\":93738},{\"end\":93741,\"start\":93740},{\"end\":93747,\"start\":93746},{\"end\":93755,\"start\":93754},{\"end\":93765,\"start\":93764},{\"end\":94142,\"start\":94141},{\"end\":94149,\"start\":94148},{\"end\":94157,\"start\":94156},{\"end\":94171,\"start\":94170},{\"end\":94180,\"start\":94179},{\"end\":94190,\"start\":94189},{\"end\":94197,\"start\":94196},{\"end\":94199,\"start\":94198},{\"end\":94206,\"start\":94205},{\"end\":94208,\"start\":94207},{\"end\":94214,\"start\":94213},{\"end\":94642,\"start\":94641},{\"end\":94649,\"start\":94648},{\"end\":94656,\"start\":94655},{\"end\":94664,\"start\":94663},{\"end\":94672,\"start\":94671},{\"end\":94680,\"start\":94679},{\"end\":94977,\"start\":94976},{\"end\":94979,\"start\":94978},{\"end\":94988,\"start\":94987},{\"end\":94999,\"start\":94998},{\"end\":95001,\"start\":95000},{\"end\":95011,\"start\":95010},{\"end\":95262,\"start\":95261},{\"end\":95272,\"start\":95271},{\"end\":95661,\"start\":95660},{\"end\":95674,\"start\":95673},{\"end\":95688,\"start\":95687},{\"end\":95697,\"start\":95696},{\"end\":95699,\"start\":95698},{\"end\":95710,\"start\":95709},{\"end\":96173,\"start\":96172},{\"end\":96186,\"start\":96185},{\"end\":96200,\"start\":96199},{\"end\":96780,\"start\":96779},{\"end\":96791,\"start\":96790},{\"end\":96807,\"start\":96806},{\"end\":96819,\"start\":96818},{\"end\":97188,\"start\":97187},{\"end\":97196,\"start\":97195},{\"end\":97205,\"start\":97204},{\"end\":97213,\"start\":97212},{\"end\":97632,\"start\":97631},{\"end\":97646,\"start\":97645},{\"end\":97655,\"start\":97654},{\"end\":97666,\"start\":97665},{\"end\":98071,\"start\":98070},{\"end\":98081,\"start\":98080},{\"end\":98090,\"start\":98089},{\"end\":98106,\"start\":98105},{\"end\":98108,\"start\":98107},{\"end\":98116,\"start\":98115},{\"end\":98381,\"start\":98380},{\"end\":98389,\"start\":98388},{\"end\":98398,\"start\":98397},{\"end\":98408,\"start\":98407},{\"end\":98415,\"start\":98414},{\"end\":98425,\"start\":98421},{\"end\":98432,\"start\":98431},{\"end\":98698,\"start\":98697},{\"end\":98700,\"start\":98699},{\"end\":98710,\"start\":98709},{\"end\":98718,\"start\":98717},{\"end\":98728,\"start\":98727},{\"end\":98741,\"start\":98740},{\"end\":98748,\"start\":98747},{\"end\":98761,\"start\":98760},{\"end\":98763,\"start\":98762},{\"end\":98772,\"start\":98771},{\"end\":98780,\"start\":98779},{\"end\":99098,\"start\":99097},{\"end\":99112,\"start\":99111},{\"end\":99132,\"start\":99131},{\"end\":99378,\"start\":99377},{\"end\":99389,\"start\":99388},{\"end\":99397,\"start\":99396},{\"end\":99408,\"start\":99407},{\"end\":99653,\"start\":99652},{\"end\":99660,\"start\":99659},{\"end\":99668,\"start\":99667},{\"end\":99675,\"start\":99674},{\"end\":99683,\"start\":99682},{\"end\":99970,\"start\":99969},{\"end\":99981,\"start\":99980},{\"end\":99983,\"start\":99982},{\"end\":99992,\"start\":99991},{\"end\":99994,\"start\":99993},{\"end\":100004,\"start\":100003},{\"end\":100367,\"start\":100366},{\"end\":100369,\"start\":100368},{\"end\":100379,\"start\":100378},{\"end\":100389,\"start\":100388},{\"end\":100391,\"start\":100390},{\"end\":100401,\"start\":100400},{\"end\":100410,\"start\":100409},{\"end\":100412,\"start\":100411},{\"end\":100421,\"start\":100420},{\"end\":100423,\"start\":100422},{\"end\":100743,\"start\":100742},{\"end\":100751,\"start\":100750},{\"end\":100758,\"start\":100757},{\"end\":100765,\"start\":100764},{\"end\":100773,\"start\":100772},{\"end\":100781,\"start\":100780},{\"end\":100789,\"start\":100788},{\"end\":101104,\"start\":101103},{\"end\":101114,\"start\":101113},{\"end\":101123,\"start\":101122},{\"end\":101139,\"start\":101138},{\"end\":101617,\"start\":101616},{\"end\":101624,\"start\":101623},{\"end\":101636,\"start\":101635},{\"end\":101638,\"start\":101637},{\"end\":101644,\"start\":101643},{\"end\":101646,\"start\":101645},{\"end\":101656,\"start\":101655},{\"end\":101916,\"start\":101915},{\"end\":101926,\"start\":101925},{\"end\":101937,\"start\":101936},{\"end\":102392,\"start\":102391},{\"end\":102407,\"start\":102406},{\"end\":102421,\"start\":102420},{\"end\":102431,\"start\":102430},{\"end\":102448,\"start\":102447},{\"end\":102462,\"start\":102461},{\"end\":102473,\"start\":102472},{\"end\":102950,\"start\":102949},{\"end\":102965,\"start\":102964},{\"end\":102975,\"start\":102974},{\"end\":102992,\"start\":102991},{\"end\":103006,\"start\":103005},{\"end\":103020,\"start\":103019},{\"end\":103031,\"start\":103030},{\"end\":103453,\"start\":103452},{\"end\":103459,\"start\":103458},{\"end\":103467,\"start\":103466},{\"end\":103469,\"start\":103468},{\"end\":103476,\"start\":103475},{\"end\":103485,\"start\":103484},{\"end\":103493,\"start\":103492},{\"end\":103500,\"start\":103499},{\"end\":103508,\"start\":103507},{\"end\":103516,\"start\":103515},{\"end\":104107,\"start\":104106},{\"end\":104109,\"start\":104108},{\"end\":104120,\"start\":104119},{\"end\":104131,\"start\":104130},{\"end\":104139,\"start\":104138},{\"end\":104150,\"start\":104146},{\"end\":104485,\"start\":104484},{\"end\":104487,\"start\":104486},{\"end\":104499,\"start\":104498},{\"end\":104834,\"start\":104833},{\"end\":104848,\"start\":104847},{\"end\":105247,\"start\":105246},{\"end\":105254,\"start\":105253},{\"end\":105262,\"start\":105261},{\"end\":105270,\"start\":105269},{\"end\":105278,\"start\":105277},{\"end\":105284,\"start\":105283},{\"end\":105291,\"start\":105290},{\"end\":105299,\"start\":105298},{\"end\":105586,\"start\":105585},{\"end\":105597,\"start\":105596},{\"end\":105611,\"start\":105610},{\"end\":105624,\"start\":105623},{\"end\":105626,\"start\":105625},{\"end\":105916,\"start\":105915},{\"end\":105923,\"start\":105922},{\"end\":105931,\"start\":105930},{\"end\":105939,\"start\":105938},{\"end\":105946,\"start\":105945},{\"end\":105955,\"start\":105954},{\"end\":105961,\"start\":105960},{\"end\":106284,\"start\":106283},{\"end\":106292,\"start\":106291},{\"end\":106294,\"start\":106293},{\"end\":106301,\"start\":106300},{\"end\":106309,\"start\":106308},{\"end\":106311,\"start\":106310},{\"end\":106320,\"start\":106319},{\"end\":106328,\"start\":106327},{\"end\":106923,\"start\":106922},{\"end\":106931,\"start\":106930},{\"end\":106938,\"start\":106937},{\"end\":106944,\"start\":106943},{\"end\":106953,\"start\":106952},{\"end\":106962,\"start\":106961},{\"end\":107271,\"start\":107270},{\"end\":107284,\"start\":107283},{\"end\":107292,\"start\":107291},{\"end\":107302,\"start\":107301},{\"end\":108804,\"start\":108803},{\"end\":108813,\"start\":108812},{\"end\":108815,\"start\":108814},{\"end\":108824,\"start\":108823},{\"end\":108833,\"start\":108832},{\"end\":108840,\"start\":108839},{\"end\":108846,\"start\":108845},{\"end\":109459,\"start\":109458},{\"end\":109469,\"start\":109468},{\"end\":109478,\"start\":109477},{\"end\":109480,\"start\":109479},{\"end\":109763,\"start\":109762},{\"end\":109765,\"start\":109764},{\"end\":109772,\"start\":109771},{\"end\":109780,\"start\":109779},{\"end\":110060,\"start\":110059},{\"end\":110068,\"start\":110067},{\"end\":110076,\"start\":110075},{\"end\":110361,\"start\":110360},{\"end\":110369,\"start\":110368},{\"end\":110377,\"start\":110376},{\"end\":110387,\"start\":110383},{\"end\":110648,\"start\":110644},{\"end\":110654,\"start\":110653},{\"end\":110662,\"start\":110661},{\"end\":110670,\"start\":110669},{\"end\":110678,\"start\":110677},{\"end\":110689,\"start\":110685},{\"end\":110983,\"start\":110982},{\"end\":110990,\"start\":110989},{\"end\":110996,\"start\":110995},{\"end\":111006,\"start\":111002},{\"end\":111284,\"start\":111283},{\"end\":111291,\"start\":111290},{\"end\":111298,\"start\":111297},{\"end\":111314,\"start\":111313},{\"end\":111316,\"start\":111315},{\"end\":111556,\"start\":111555},{\"end\":111558,\"start\":111557},{\"end\":111569,\"start\":111568},{\"end\":111578,\"start\":111577},{\"end\":111988,\"start\":111987},{\"end\":111994,\"start\":111993},{\"end\":112001,\"start\":112000},{\"end\":112009,\"start\":112008},{\"end\":112268,\"start\":112267},{\"end\":112270,\"start\":112269},{\"end\":112277,\"start\":112276},{\"end\":112284,\"start\":112283},{\"end\":112538,\"start\":112537},{\"end\":112547,\"start\":112546},{\"end\":112561,\"start\":112560},{\"end\":112572,\"start\":112571},{\"end\":112578,\"start\":112577},{\"end\":112588,\"start\":112587},{\"end\":112598,\"start\":112597},{\"end\":112918,\"start\":112917},{\"end\":112927,\"start\":112926},{\"end\":112937,\"start\":112936},{\"end\":113369,\"start\":113368},{\"end\":113380,\"start\":113379},{\"end\":113391,\"start\":113390},{\"end\":113400,\"start\":113399},{\"end\":113648,\"start\":113647},{\"end\":113656,\"start\":113655},{\"end\":113665,\"start\":113664},{\"end\":113671,\"start\":113670},{\"end\":113679,\"start\":113678},{\"end\":113683,\"start\":113680},{\"end\":113691,\"start\":113690},{\"end\":113698,\"start\":113697},{\"end\":114217,\"start\":114216},{\"end\":114225,\"start\":114224},{\"end\":114232,\"start\":114231},{\"end\":114239,\"start\":114238},{\"end\":114246,\"start\":114245},{\"end\":114248,\"start\":114247},{\"end\":114613,\"start\":114612},{\"end\":114623,\"start\":114622},{\"end\":114630,\"start\":114629},{\"end\":114639,\"start\":114638},{\"end\":114648,\"start\":114647},{\"end\":115002,\"start\":115001},{\"end\":115013,\"start\":115012},{\"end\":115022,\"start\":115021},{\"end\":115434,\"start\":115433},{\"end\":115440,\"start\":115439},{\"end\":115449,\"start\":115448},{\"end\":115451,\"start\":115450},{\"end\":115459,\"start\":115458},{\"end\":115472,\"start\":115471},{\"end\":115481,\"start\":115480},{\"end\":115488,\"start\":115487},{\"end\":115498,\"start\":115497},{\"end\":115794,\"start\":115793},{\"end\":115803,\"start\":115802},{\"end\":115810,\"start\":115809},{\"end\":115823,\"start\":115822},{\"end\":116053,\"start\":116052},{\"end\":116060,\"start\":116059},{\"end\":116072,\"start\":116071},{\"end\":116082,\"start\":116081},{\"end\":116089,\"start\":116088},{\"end\":116095,\"start\":116094},{\"end\":116109,\"start\":116108},{\"end\":116413,\"start\":116412},{\"end\":116415,\"start\":116414},{\"end\":116422,\"start\":116421},{\"end\":116700,\"start\":116699},{\"end\":116709,\"start\":116708},{\"end\":116715,\"start\":116714},{\"end\":116722,\"start\":116721},{\"end\":117020,\"start\":117019},{\"end\":117032,\"start\":117031},{\"end\":117047,\"start\":117046},{\"end\":117060,\"start\":117059},{\"end\":117062,\"start\":117061},{\"end\":117363,\"start\":117362},{\"end\":117369,\"start\":117368},{\"end\":117389,\"start\":117388},{\"end\":117395,\"start\":117394},{\"end\":117405,\"start\":117404},{\"end\":117975,\"start\":117974},{\"end\":117977,\"start\":117976},{\"end\":118169,\"start\":118168},{\"end\":118265,\"start\":118264},{\"end\":118273,\"start\":118272},{\"end\":118487,\"start\":118486},{\"end\":118500,\"start\":118499},{\"end\":119057,\"start\":119056},{\"end\":119067,\"start\":119066},{\"end\":119077,\"start\":119076},{\"end\":119086,\"start\":119085},{\"end\":119096,\"start\":119095},{\"end\":119588,\"start\":119587},{\"end\":119595,\"start\":119594},{\"end\":119603,\"start\":119602},{\"end\":119611,\"start\":119610},{\"end\":120065,\"start\":120064},{\"end\":120072,\"start\":120071},{\"end\":120078,\"start\":120077},{\"end\":120085,\"start\":120084},{\"end\":120087,\"start\":120086},{\"end\":120095,\"start\":120094},{\"end\":120102,\"start\":120101},{\"end\":120111,\"start\":120110},{\"end\":120113,\"start\":120112},{\"end\":120638,\"start\":120637},{\"end\":120645,\"start\":120644},{\"end\":120652,\"start\":120651},{\"end\":120658,\"start\":120657},{\"end\":120667,\"start\":120666},{\"end\":120674,\"start\":120673},{\"end\":120680,\"start\":120679},{\"end\":121124,\"start\":121123},{\"end\":121132,\"start\":121131},{\"end\":121139,\"start\":121138},{\"end\":121147,\"start\":121146},{\"end\":121154,\"start\":121153},{\"end\":121791,\"start\":121790},{\"end\":121797,\"start\":121796},{\"end\":121806,\"start\":121805},{\"end\":121814,\"start\":121813},{\"end\":121821,\"start\":121820},{\"end\":121829,\"start\":121828},{\"end\":121835,\"start\":121834},{\"end\":121843,\"start\":121842},{\"end\":121852,\"start\":121851},{\"end\":121860,\"start\":121859},{\"end\":121869,\"start\":121868},{\"end\":122329,\"start\":122328},{\"end\":122338,\"start\":122337},{\"end\":122347,\"start\":122346},{\"end\":122356,\"start\":122355},{\"end\":122363,\"start\":122362},{\"end\":123057,\"start\":123056},{\"end\":123064,\"start\":123063},{\"end\":123072,\"start\":123071},{\"end\":123078,\"start\":123077},{\"end\":123088,\"start\":123084},{\"end\":123730,\"start\":123729},{\"end\":123739,\"start\":123738},{\"end\":123747,\"start\":123746},{\"end\":123754,\"start\":123753},{\"end\":123763,\"start\":123762},{\"end\":123770,\"start\":123769},{\"end\":123779,\"start\":123778},{\"end\":124155,\"start\":124154},{\"end\":124163,\"start\":124162},{\"end\":124175,\"start\":124174},{\"end\":124186,\"start\":124185},{\"end\":124197,\"start\":124196},{\"end\":124199,\"start\":124198},{\"end\":124210,\"start\":124209},{\"end\":124221,\"start\":124220},{\"end\":124224,\"start\":124222},{\"end\":124233,\"start\":124232},{\"end\":124617,\"start\":124616},{\"end\":124626,\"start\":124625},{\"end\":124636,\"start\":124635},{\"end\":124894,\"start\":124893},{\"end\":124902,\"start\":124901},{\"end\":124909,\"start\":124908},{\"end\":125406,\"start\":125405},{\"end\":125408,\"start\":125407},{\"end\":125416,\"start\":125415},{\"end\":125425,\"start\":125424},{\"end\":125437,\"start\":125436},{\"end\":125439,\"start\":125438},{\"end\":126129,\"start\":126128},{\"end\":126135,\"start\":126134},{\"end\":126143,\"start\":126142},{\"end\":126151,\"start\":126150},{\"end\":126158,\"start\":126157},{\"end\":126166,\"start\":126165},{\"end\":126172,\"start\":126171},{\"end\":126789,\"start\":126788},{\"end\":126795,\"start\":126794},{\"end\":126803,\"start\":126802},{\"end\":126812,\"start\":126811},{\"end\":126820,\"start\":126819},{\"end\":126834,\"start\":126833},{\"end\":127078,\"start\":127075},{\"end\":127414,\"start\":127413},{\"end\":127420,\"start\":127419},{\"end\":127427,\"start\":127426},{\"end\":127436,\"start\":127435},{\"end\":128422,\"start\":128421},{\"end\":128439,\"start\":128438},{\"end\":128448,\"start\":128447},{\"end\":128741,\"start\":128740},{\"end\":128743,\"start\":128742},{\"end\":128753,\"start\":128752},{\"end\":128755,\"start\":128754},{\"end\":129227,\"start\":129226},{\"end\":129234,\"start\":129233},{\"end\":129240,\"start\":129239},{\"end\":129247,\"start\":129246},{\"end\":129253,\"start\":129252},{\"end\":129796,\"start\":129795},{\"end\":129803,\"start\":129802},{\"end\":129812,\"start\":129811},{\"end\":129820,\"start\":129819},{\"end\":129828,\"start\":129827},{\"end\":129837,\"start\":129836},{\"end\":129843,\"start\":129842},{\"end\":129852,\"start\":129851},{\"end\":130437,\"start\":130436},{\"end\":130444,\"start\":130443},{\"end\":130451,\"start\":130450},{\"end\":130457,\"start\":130456},{\"end\":130465,\"start\":130464},{\"end\":130473,\"start\":130472},{\"end\":130480,\"start\":130479},{\"end\":130488,\"start\":130487},{\"end\":130496,\"start\":130495},{\"end\":130512,\"start\":130511},{\"end\":130518,\"start\":130517},{\"end\":130528,\"start\":130527},{\"end\":130530,\"start\":130529},{\"end\":130538,\"start\":130537},{\"end\":130540,\"start\":130539},{\"end\":130551,\"start\":130550},{\"end\":130560,\"start\":130559},{\"end\":130574,\"start\":130573},{\"end\":130581,\"start\":130580},{\"end\":130590,\"start\":130589},{\"end\":131203,\"start\":131202},{\"end\":131211,\"start\":131210},{\"end\":131217,\"start\":131216},{\"end\":131225,\"start\":131224},{\"end\":131232,\"start\":131231},{\"end\":131605,\"start\":131604},{\"end\":131613,\"start\":131612},{\"end\":131619,\"start\":131618},{\"end\":131625,\"start\":131624},{\"end\":131632,\"start\":131631}]", "bib_author_last_name": "[{\"end\":88720,\"start\":88714},{\"end\":88730,\"start\":88724},{\"end\":88988,\"start\":88975},{\"end\":88999,\"start\":88992},{\"end\":89016,\"start\":89005},{\"end\":89031,\"start\":89020},{\"end\":89043,\"start\":89037},{\"end\":89409,\"start\":89405},{\"end\":89415,\"start\":89413},{\"end\":89422,\"start\":89419},{\"end\":89434,\"start\":89426},{\"end\":89982,\"start\":89975},{\"end\":89994,\"start\":89988},{\"end\":90005,\"start\":90000},{\"end\":90013,\"start\":90009},{\"end\":90415,\"start\":90405},{\"end\":90545,\"start\":90543},{\"end\":90553,\"start\":90549},{\"end\":90560,\"start\":90557},{\"end\":90568,\"start\":90564},{\"end\":90774,\"start\":90772},{\"end\":90782,\"start\":90778},{\"end\":90790,\"start\":90786},{\"end\":90796,\"start\":90794},{\"end\":90804,\"start\":90800},{\"end\":90811,\"start\":90808},{\"end\":90817,\"start\":90815},{\"end\":90824,\"start\":90821},{\"end\":90830,\"start\":90828},{\"end\":91266,\"start\":91262},{\"end\":91275,\"start\":91270},{\"end\":91287,\"start\":91279},{\"end\":91303,\"start\":91291},{\"end\":91514,\"start\":91511},{\"end\":91523,\"start\":91518},{\"end\":91531,\"start\":91527},{\"end\":91540,\"start\":91535},{\"end\":91546,\"start\":91544},{\"end\":92039,\"start\":92037},{\"end\":92045,\"start\":92043},{\"end\":92051,\"start\":92049},{\"end\":92057,\"start\":92055},{\"end\":92064,\"start\":92061},{\"end\":92071,\"start\":92068},{\"end\":92607,\"start\":92603},{\"end\":92616,\"start\":92611},{\"end\":92623,\"start\":92620},{\"end\":92629,\"start\":92627},{\"end\":92908,\"start\":92900},{\"end\":93148,\"start\":93144},{\"end\":93156,\"start\":93152},{\"end\":93162,\"start\":93160},{\"end\":93170,\"start\":93166},{\"end\":93178,\"start\":93174},{\"end\":93185,\"start\":93182},{\"end\":93192,\"start\":93189},{\"end\":93201,\"start\":93196},{\"end\":93210,\"start\":93205},{\"end\":93218,\"start\":93214},{\"end\":93224,\"start\":93222},{\"end\":93232,\"start\":93228},{\"end\":93240,\"start\":93236},{\"end\":93248,\"start\":93244},{\"end\":93257,\"start\":93252},{\"end\":93264,\"start\":93261},{\"end\":93270,\"start\":93268},{\"end\":93278,\"start\":93274},{\"end\":93285,\"start\":93282},{\"end\":93292,\"start\":93289},{\"end\":93299,\"start\":93296},{\"end\":93306,\"start\":93303},{\"end\":93736,\"start\":93730},{\"end\":93744,\"start\":93742},{\"end\":93752,\"start\":93748},{\"end\":93762,\"start\":93756},{\"end\":93773,\"start\":93766},{\"end\":94146,\"start\":94143},{\"end\":94154,\"start\":94150},{\"end\":94168,\"start\":94158},{\"end\":94177,\"start\":94172},{\"end\":94187,\"start\":94181},{\"end\":94194,\"start\":94191},{\"end\":94203,\"start\":94200},{\"end\":94211,\"start\":94209},{\"end\":94219,\"start\":94215},{\"end\":94646,\"start\":94643},{\"end\":94653,\"start\":94650},{\"end\":94661,\"start\":94657},{\"end\":94669,\"start\":94665},{\"end\":94677,\"start\":94673},{\"end\":94683,\"start\":94681},{\"end\":94985,\"start\":94980},{\"end\":94996,\"start\":94989},{\"end\":95008,\"start\":95002},{\"end\":95019,\"start\":95012},{\"end\":95269,\"start\":95263},{\"end\":95277,\"start\":95273},{\"end\":95671,\"start\":95662},{\"end\":95685,\"start\":95675},{\"end\":95694,\"start\":95689},{\"end\":95707,\"start\":95700},{\"end\":95721,\"start\":95711},{\"end\":96183,\"start\":96174},{\"end\":96197,\"start\":96187},{\"end\":96207,\"start\":96201},{\"end\":96788,\"start\":96781},{\"end\":96804,\"start\":96792},{\"end\":96816,\"start\":96808},{\"end\":96830,\"start\":96820},{\"end\":97193,\"start\":97189},{\"end\":97202,\"start\":97197},{\"end\":97210,\"start\":97206},{\"end\":97224,\"start\":97214},{\"end\":97643,\"start\":97633},{\"end\":97652,\"start\":97647},{\"end\":97663,\"start\":97656},{\"end\":97672,\"start\":97667},{\"end\":98078,\"start\":98072},{\"end\":98087,\"start\":98082},{\"end\":98103,\"start\":98091},{\"end\":98113,\"start\":98109},{\"end\":98127,\"start\":98117},{\"end\":98386,\"start\":98382},{\"end\":98395,\"start\":98390},{\"end\":98405,\"start\":98399},{\"end\":98412,\"start\":98409},{\"end\":98419,\"start\":98416},{\"end\":98429,\"start\":98426},{\"end\":98437,\"start\":98433},{\"end\":98707,\"start\":98701},{\"end\":98715,\"start\":98711},{\"end\":98725,\"start\":98719},{\"end\":98738,\"start\":98729},{\"end\":98745,\"start\":98742},{\"end\":98758,\"start\":98749},{\"end\":98769,\"start\":98764},{\"end\":98777,\"start\":98773},{\"end\":98784,\"start\":98781},{\"end\":99109,\"start\":99099},{\"end\":99129,\"start\":99113},{\"end\":99138,\"start\":99133},{\"end\":99386,\"start\":99379},{\"end\":99394,\"start\":99390},{\"end\":99405,\"start\":99398},{\"end\":99412,\"start\":99409},{\"end\":99657,\"start\":99654},{\"end\":99665,\"start\":99661},{\"end\":99672,\"start\":99669},{\"end\":99680,\"start\":99676},{\"end\":99687,\"start\":99684},{\"end\":99978,\"start\":99971},{\"end\":99989,\"start\":99984},{\"end\":100001,\"start\":99995},{\"end\":100008,\"start\":100005},{\"end\":100376,\"start\":100370},{\"end\":100386,\"start\":100380},{\"end\":100398,\"start\":100392},{\"end\":100407,\"start\":100402},{\"end\":100418,\"start\":100413},{\"end\":100429,\"start\":100424},{\"end\":100748,\"start\":100744},{\"end\":100755,\"start\":100752},{\"end\":100762,\"start\":100759},{\"end\":100770,\"start\":100766},{\"end\":100778,\"start\":100774},{\"end\":100786,\"start\":100782},{\"end\":100794,\"start\":100790},{\"end\":101111,\"start\":101105},{\"end\":101120,\"start\":101115},{\"end\":101136,\"start\":101124},{\"end\":101150,\"start\":101140},{\"end\":101621,\"start\":101618},{\"end\":101633,\"start\":101625},{\"end\":101641,\"start\":101639},{\"end\":101653,\"start\":101647},{\"end\":101665,\"start\":101657},{\"end\":101923,\"start\":101917},{\"end\":101934,\"start\":101927},{\"end\":101944,\"start\":101938},{\"end\":102404,\"start\":102393},{\"end\":102418,\"start\":102408},{\"end\":102428,\"start\":102422},{\"end\":102445,\"start\":102432},{\"end\":102459,\"start\":102449},{\"end\":102470,\"start\":102463},{\"end\":102480,\"start\":102474},{\"end\":102962,\"start\":102951},{\"end\":102972,\"start\":102966},{\"end\":102989,\"start\":102976},{\"end\":103003,\"start\":102993},{\"end\":103017,\"start\":103007},{\"end\":103028,\"start\":103021},{\"end\":103038,\"start\":103032},{\"end\":103456,\"start\":103454},{\"end\":103464,\"start\":103460},{\"end\":103473,\"start\":103470},{\"end\":103482,\"start\":103477},{\"end\":103490,\"start\":103486},{\"end\":103497,\"start\":103494},{\"end\":103505,\"start\":103501},{\"end\":103513,\"start\":103509},{\"end\":103521,\"start\":103517},{\"end\":104117,\"start\":104110},{\"end\":104128,\"start\":104121},{\"end\":104136,\"start\":104132},{\"end\":104144,\"start\":104140},{\"end\":104158,\"start\":104151},{\"end\":104496,\"start\":104488},{\"end\":104508,\"start\":104500},{\"end\":104845,\"start\":104835},{\"end\":104857,\"start\":104849},{\"end\":105251,\"start\":105248},{\"end\":105259,\"start\":105255},{\"end\":105267,\"start\":105263},{\"end\":105275,\"start\":105271},{\"end\":105281,\"start\":105279},{\"end\":105288,\"start\":105285},{\"end\":105296,\"start\":105292},{\"end\":105304,\"start\":105300},{\"end\":105594,\"start\":105587},{\"end\":105608,\"start\":105598},{\"end\":105621,\"start\":105612},{\"end\":105632,\"start\":105627},{\"end\":105920,\"start\":105917},{\"end\":105928,\"start\":105924},{\"end\":105936,\"start\":105932},{\"end\":105943,\"start\":105940},{\"end\":105952,\"start\":105947},{\"end\":105958,\"start\":105956},{\"end\":105966,\"start\":105962},{\"end\":106289,\"start\":106285},{\"end\":106298,\"start\":106295},{\"end\":106306,\"start\":106302},{\"end\":106317,\"start\":106312},{\"end\":106325,\"start\":106321},{\"end\":106334,\"start\":106329},{\"end\":106928,\"start\":106924},{\"end\":106935,\"start\":106932},{\"end\":106941,\"start\":106939},{\"end\":106950,\"start\":106945},{\"end\":106959,\"start\":106954},{\"end\":106968,\"start\":106963},{\"end\":107281,\"start\":107272},{\"end\":107289,\"start\":107285},{\"end\":107299,\"start\":107293},{\"end\":107309,\"start\":107303},{\"end\":108810,\"start\":108805},{\"end\":108821,\"start\":108816},{\"end\":108830,\"start\":108825},{\"end\":108837,\"start\":108834},{\"end\":108843,\"start\":108841},{\"end\":108852,\"start\":108847},{\"end\":109466,\"start\":109460},{\"end\":109475,\"start\":109470},{\"end\":109487,\"start\":109481},{\"end\":109769,\"start\":109766},{\"end\":109777,\"start\":109773},{\"end\":109784,\"start\":109781},{\"end\":110065,\"start\":110061},{\"end\":110073,\"start\":110069},{\"end\":110080,\"start\":110077},{\"end\":110366,\"start\":110362},{\"end\":110374,\"start\":110370},{\"end\":110381,\"start\":110378},{\"end\":110391,\"start\":110388},{\"end\":110651,\"start\":110649},{\"end\":110659,\"start\":110655},{\"end\":110667,\"start\":110663},{\"end\":110675,\"start\":110671},{\"end\":110683,\"start\":110679},{\"end\":110696,\"start\":110690},{\"end\":110987,\"start\":110984},{\"end\":110993,\"start\":110991},{\"end\":111000,\"start\":110997},{\"end\":111013,\"start\":111007},{\"end\":111288,\"start\":111285},{\"end\":111295,\"start\":111292},{\"end\":111311,\"start\":111299},{\"end\":111320,\"start\":111317},{\"end\":111566,\"start\":111559},{\"end\":111575,\"start\":111570},{\"end\":111585,\"start\":111579},{\"end\":111991,\"start\":111989},{\"end\":111998,\"start\":111995},{\"end\":112006,\"start\":112002},{\"end\":112014,\"start\":112010},{\"end\":112274,\"start\":112271},{\"end\":112281,\"start\":112278},{\"end\":112290,\"start\":112285},{\"end\":112544,\"start\":112539},{\"end\":112558,\"start\":112548},{\"end\":112569,\"start\":112562},{\"end\":112575,\"start\":112573},{\"end\":112585,\"start\":112579},{\"end\":112595,\"start\":112589},{\"end\":112609,\"start\":112599},{\"end\":112924,\"start\":112919},{\"end\":112934,\"start\":112928},{\"end\":112944,\"start\":112938},{\"end\":113377,\"start\":113370},{\"end\":113388,\"start\":113381},{\"end\":113397,\"start\":113392},{\"end\":113406,\"start\":113401},{\"end\":113653,\"start\":113649},{\"end\":113662,\"start\":113657},{\"end\":113668,\"start\":113666},{\"end\":113676,\"start\":113672},{\"end\":113688,\"start\":113684},{\"end\":113695,\"start\":113692},{\"end\":113702,\"start\":113699},{\"end\":114222,\"start\":114218},{\"end\":114229,\"start\":114226},{\"end\":114236,\"start\":114233},{\"end\":114243,\"start\":114240},{\"end\":114252,\"start\":114249},{\"end\":114620,\"start\":114614},{\"end\":114627,\"start\":114624},{\"end\":114636,\"start\":114631},{\"end\":114645,\"start\":114640},{\"end\":114661,\"start\":114649},{\"end\":115010,\"start\":115003},{\"end\":115019,\"start\":115014},{\"end\":115030,\"start\":115023},{\"end\":115437,\"start\":115435},{\"end\":115446,\"start\":115441},{\"end\":115456,\"start\":115452},{\"end\":115469,\"start\":115460},{\"end\":115478,\"start\":115473},{\"end\":115485,\"start\":115482},{\"end\":115495,\"start\":115489},{\"end\":115503,\"start\":115499},{\"end\":115800,\"start\":115795},{\"end\":115807,\"start\":115804},{\"end\":115820,\"start\":115811},{\"end\":115827,\"start\":115824},{\"end\":116057,\"start\":116054},{\"end\":116069,\"start\":116061},{\"end\":116079,\"start\":116073},{\"end\":116086,\"start\":116083},{\"end\":116092,\"start\":116090},{\"end\":116106,\"start\":116096},{\"end\":116122,\"start\":116110},{\"end\":116419,\"start\":116416},{\"end\":116428,\"start\":116423},{\"end\":116706,\"start\":116701},{\"end\":116712,\"start\":116710},{\"end\":116719,\"start\":116716},{\"end\":116727,\"start\":116723},{\"end\":117029,\"start\":117021},{\"end\":117044,\"start\":117033},{\"end\":117057,\"start\":117048},{\"end\":117069,\"start\":117063},{\"end\":117366,\"start\":117364},{\"end\":117386,\"start\":117370},{\"end\":117392,\"start\":117390},{\"end\":117402,\"start\":117396},{\"end\":117411,\"start\":117406},{\"end\":117983,\"start\":117978},{\"end\":118183,\"start\":118170},{\"end\":118270,\"start\":118266},{\"end\":118280,\"start\":118274},{\"end\":118497,\"start\":118488},{\"end\":118507,\"start\":118501},{\"end\":119064,\"start\":119058},{\"end\":119074,\"start\":119068},{\"end\":119083,\"start\":119078},{\"end\":119093,\"start\":119087},{\"end\":119107,\"start\":119097},{\"end\":119592,\"start\":119589},{\"end\":119600,\"start\":119596},{\"end\":119608,\"start\":119604},{\"end\":119617,\"start\":119612},{\"end\":120069,\"start\":120066},{\"end\":120075,\"start\":120073},{\"end\":120082,\"start\":120079},{\"end\":120092,\"start\":120088},{\"end\":120099,\"start\":120096},{\"end\":120108,\"start\":120103},{\"end\":120121,\"start\":120114},{\"end\":120642,\"start\":120639},{\"end\":120649,\"start\":120646},{\"end\":120655,\"start\":120653},{\"end\":120664,\"start\":120659},{\"end\":120671,\"start\":120668},{\"end\":120677,\"start\":120675},{\"end\":120685,\"start\":120681},{\"end\":121129,\"start\":121125},{\"end\":121136,\"start\":121133},{\"end\":121144,\"start\":121140},{\"end\":121151,\"start\":121148},{\"end\":121160,\"start\":121155},{\"end\":121794,\"start\":121792},{\"end\":121803,\"start\":121798},{\"end\":121811,\"start\":121807},{\"end\":121818,\"start\":121815},{\"end\":121826,\"start\":121822},{\"end\":121832,\"start\":121830},{\"end\":121840,\"start\":121836},{\"end\":121849,\"start\":121844},{\"end\":121857,\"start\":121853},{\"end\":121866,\"start\":121861},{\"end\":121874,\"start\":121870},{\"end\":122335,\"start\":122330},{\"end\":122344,\"start\":122339},{\"end\":122353,\"start\":122348},{\"end\":122360,\"start\":122357},{\"end\":122368,\"start\":122364},{\"end\":123061,\"start\":123058},{\"end\":123069,\"start\":123065},{\"end\":123075,\"start\":123073},{\"end\":123082,\"start\":123079},{\"end\":123095,\"start\":123089},{\"end\":123736,\"start\":123731},{\"end\":123744,\"start\":123740},{\"end\":123751,\"start\":123748},{\"end\":123760,\"start\":123755},{\"end\":123767,\"start\":123764},{\"end\":123776,\"start\":123771},{\"end\":123785,\"start\":123780},{\"end\":124160,\"start\":124156},{\"end\":124172,\"start\":124164},{\"end\":124183,\"start\":124176},{\"end\":124194,\"start\":124187},{\"end\":124207,\"start\":124200},{\"end\":124218,\"start\":124211},{\"end\":124230,\"start\":124225},{\"end\":124243,\"start\":124234},{\"end\":124623,\"start\":124618},{\"end\":124633,\"start\":124627},{\"end\":124642,\"start\":124637},{\"end\":124899,\"start\":124895},{\"end\":124906,\"start\":124903},{\"end\":124915,\"start\":124910},{\"end\":125413,\"start\":125409},{\"end\":125422,\"start\":125417},{\"end\":125434,\"start\":125426},{\"end\":125445,\"start\":125440},{\"end\":126132,\"start\":126130},{\"end\":126140,\"start\":126136},{\"end\":126148,\"start\":126144},{\"end\":126155,\"start\":126152},{\"end\":126163,\"start\":126159},{\"end\":126169,\"start\":126167},{\"end\":126176,\"start\":126173},{\"end\":126792,\"start\":126790},{\"end\":126800,\"start\":126796},{\"end\":126809,\"start\":126804},{\"end\":126817,\"start\":126813},{\"end\":126831,\"start\":126821},{\"end\":126839,\"start\":126835},{\"end\":127088,\"start\":127079},{\"end\":127099,\"start\":127090},{\"end\":127417,\"start\":127415},{\"end\":127424,\"start\":127421},{\"end\":127433,\"start\":127428},{\"end\":127441,\"start\":127437},{\"end\":128436,\"start\":128423},{\"end\":128445,\"start\":128440},{\"end\":128455,\"start\":128449},{\"end\":128750,\"start\":128744},{\"end\":128761,\"start\":128756},{\"end\":129231,\"start\":129228},{\"end\":129237,\"start\":129235},{\"end\":129244,\"start\":129241},{\"end\":129250,\"start\":129248},{\"end\":129256,\"start\":129254},{\"end\":129800,\"start\":129797},{\"end\":129809,\"start\":129804},{\"end\":129817,\"start\":129813},{\"end\":129825,\"start\":129821},{\"end\":129834,\"start\":129829},{\"end\":129840,\"start\":129838},{\"end\":129849,\"start\":129844},{\"end\":129857,\"start\":129853},{\"end\":130441,\"start\":130438},{\"end\":130448,\"start\":130445},{\"end\":130454,\"start\":130452},{\"end\":130462,\"start\":130458},{\"end\":130470,\"start\":130466},{\"end\":130477,\"start\":130474},{\"end\":130485,\"start\":130481},{\"end\":130493,\"start\":130489},{\"end\":130509,\"start\":130497},{\"end\":130515,\"start\":130513},{\"end\":130525,\"start\":130519},{\"end\":130535,\"start\":130531},{\"end\":130548,\"start\":130541},{\"end\":130557,\"start\":130552},{\"end\":130571,\"start\":130561},{\"end\":130578,\"start\":130575},{\"end\":130587,\"start\":130582},{\"end\":130595,\"start\":130591},{\"end\":131208,\"start\":131204},{\"end\":131214,\"start\":131212},{\"end\":131222,\"start\":131218},{\"end\":131229,\"start\":131226},{\"end\":131236,\"start\":131233},{\"end\":131610,\"start\":131606},{\"end\":131616,\"start\":131614},{\"end\":131622,\"start\":131620},{\"end\":131629,\"start\":131626},{\"end\":131636,\"start\":131633}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":88888,\"start\":88619},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":254528252},\"end\":89306,\"start\":88890},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14450169},\"end\":89930,\"start\":89308},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1425467},\"end\":90368,\"start\":89932},{\"attributes\":{\"id\":\"b4\"},\"end\":90472,\"start\":90370},{\"attributes\":{\"id\":\"b5\"},\"end\":90708,\"start\":90474},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":37620879},\"end\":91222,\"start\":90710},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":211544609},\"end\":91428,\"start\":91224},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":220497623},\"end\":91963,\"start\":91430},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":246080157},\"end\":92462,\"start\":91965},{\"attributes\":{\"doi\":\"10.1109/ASE51524.2021.9678724\",\"id\":\"b10\"},\"end\":92551,\"start\":92464},{\"attributes\":{\"doi\":\"10.48550/arXiv.2304.07590\",\"id\":\"b11\",\"matched_paper_id\":258179537},\"end\":92859,\"start\":92553},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":254366666},\"end\":93103,\"start\":92861},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":257900969},\"end\":93680,\"start\":93105},{\"attributes\":{\"id\":\"b14\"},\"end\":94068,\"start\":93682},{\"attributes\":{\"id\":\"b15\"},\"end\":94575,\"start\":94070},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":259165563},\"end\":94945,\"start\":94577},{\"attributes\":{\"id\":\"b17\"},\"end\":95161,\"start\":94947},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":257622664},\"end\":95542,\"start\":95163},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":256868507},\"end\":96054,\"start\":95544},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":248178008},\"end\":96705,\"start\":96056},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":258615423},\"end\":97029,\"start\":96707},{\"attributes\":{\"id\":\"b22\"},\"end\":97110,\"start\":97031},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":258822853},\"end\":97490,\"start\":97112},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":258291698},\"end\":98005,\"start\":97492},{\"attributes\":{\"id\":\"b25\"},\"end\":98333,\"start\":98007},{\"attributes\":{\"id\":\"b26\"},\"end\":98624,\"start\":98335},{\"attributes\":{\"id\":\"b27\"},\"end\":99035,\"start\":98626},{\"attributes\":{\"id\":\"b28\"},\"end\":99320,\"start\":99037},{\"attributes\":{\"id\":\"b29\"},\"end\":99584,\"start\":99322},{\"attributes\":{\"id\":\"b30\"},\"end\":99871,\"start\":99586},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":259860757},\"end\":100285,\"start\":99873},{\"attributes\":{\"id\":\"b32\"},\"end\":100659,\"start\":100287},{\"attributes\":{\"id\":\"b33\"},\"end\":101012,\"start\":100661},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":221655313},\"end\":101569,\"start\":101014},{\"attributes\":{\"id\":\"b35\"},\"end\":101844,\"start\":101571},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":259860357},\"end\":102302,\"start\":101846},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":231786586},\"end\":102899,\"start\":102304},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":249718125},\"end\":103370,\"start\":102901},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":233241113},\"end\":104030,\"start\":103372},{\"attributes\":{\"id\":\"b40\"},\"end\":104368,\"start\":104032},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":234370607},\"end\":104781,\"start\":104370},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":258978980},\"end\":105169,\"start\":104783},{\"attributes\":{\"id\":\"b43\"},\"end\":105523,\"start\":105171},{\"attributes\":{\"id\":\"b44\"},\"end\":105824,\"start\":105525},{\"attributes\":{\"id\":\"b45\"},\"end\":106191,\"start\":105826},{\"attributes\":{\"id\":\"b46\"},\"end\":106561,\"start\":106193},{\"attributes\":{\"id\":\"b47\"},\"end\":106845,\"start\":106563},{\"attributes\":{\"id\":\"b48\"},\"end\":107175,\"start\":106847},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":258733566},\"end\":107885,\"start\":107177},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":259335479},\"end\":108751,\"start\":107887},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":249926621},\"end\":109373,\"start\":108753},{\"attributes\":{\"id\":\"b52\"},\"end\":109681,\"start\":109375},{\"attributes\":{\"id\":\"b53\"},\"end\":109967,\"start\":109683},{\"attributes\":{\"id\":\"b54\"},\"end\":110272,\"start\":109969},{\"attributes\":{\"id\":\"b55\"},\"end\":110592,\"start\":110274},{\"attributes\":{\"id\":\"b56\"},\"end\":110882,\"start\":110594},{\"attributes\":{\"id\":\"b57\"},\"end\":111224,\"start\":110884},{\"attributes\":{\"id\":\"b58\"},\"end\":111497,\"start\":111226},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":250318108},\"end\":111919,\"start\":111499},{\"attributes\":{\"id\":\"b60\"},\"end\":112191,\"start\":111921},{\"attributes\":{\"id\":\"b61\"},\"end\":112469,\"start\":112193},{\"attributes\":{\"id\":\"b62\"},\"end\":112832,\"start\":112471},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":250322225},\"end\":113302,\"start\":112834},{\"attributes\":{\"id\":\"b64\"},\"end\":113590,\"start\":113304},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":248986947},\"end\":114117,\"start\":113592},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":259064177},\"end\":114541,\"start\":114119},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":251563966},\"end\":114956,\"start\":114543},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":250322226},\"end\":115360,\"start\":114958},{\"attributes\":{\"id\":\"b69\"},\"end\":115731,\"start\":115362},{\"attributes\":{\"id\":\"b70\"},\"end\":116003,\"start\":115733},{\"attributes\":{\"id\":\"b71\"},\"end\":116324,\"start\":116005},{\"attributes\":{\"id\":\"b72\"},\"end\":116610,\"start\":116326},{\"attributes\":{\"id\":\"b73\"},\"end\":116924,\"start\":116612},{\"attributes\":{\"id\":\"b74\"},\"end\":117297,\"start\":116926},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":253421714},\"end\":117943,\"start\":117299},{\"attributes\":{\"id\":\"b76\"},\"end\":118141,\"start\":117945},{\"attributes\":{\"id\":\"b77\"},\"end\":118262,\"start\":118143},{\"attributes\":{\"id\":\"b78\"},\"end\":118432,\"start\":118264},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":246706202},\"end\":118992,\"start\":118434},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":211088142},\"end\":119511,\"start\":118994},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":246016475},\"end\":119984,\"start\":119513},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":249494506},\"end\":120580,\"start\":119986},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":229703397},\"end\":121059,\"start\":120582},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":226274278},\"end\":121709,\"start\":121061},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":214754942},\"end\":122257,\"start\":121711},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":49269943},\"end\":122986,\"start\":122259},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":3374770},\"end\":123602,\"start\":122988},{\"attributes\":{\"id\":\"b88\"},\"end\":123679,\"start\":123604},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":3155193},\"end\":124080,\"start\":123681},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":15132155},\"end\":124544,\"start\":124082},{\"attributes\":{\"id\":\"b91\"},\"end\":124819,\"start\":124546},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":220497397},\"end\":125308,\"start\":124821},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":10413560},\"end\":125961,\"start\":125310},{\"attributes\":{\"id\":\"b94\"},\"end\":126032,\"start\":125963},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":246081537},\"end\":126691,\"start\":126034},{\"attributes\":{\"id\":\"b96\"},\"end\":127071,\"start\":126693},{\"attributes\":{\"id\":\"b97\"},\"end\":127171,\"start\":127073},{\"attributes\":{\"id\":\"b98\"},\"end\":127325,\"start\":127173},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":53082159},\"end\":128337,\"start\":127327},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":7953114},\"end\":128674,\"start\":128339},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":248927370},\"end\":128986,\"start\":128676},{\"attributes\":{\"id\":\"b102\"},\"end\":129139,\"start\":128988},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":246823539},\"end\":129719,\"start\":129141},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":237513696},\"end\":130280,\"start\":129721},{\"attributes\":{\"id\":\"b105\"},\"end\":130369,\"start\":130282},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":221761146},\"end\":131138,\"start\":130371},{\"attributes\":{\"id\":\"b107\",\"matched_paper_id\":222142942},\"end\":131513,\"start\":131140},{\"attributes\":{\"id\":\"b108\",\"matched_paper_id\":211205176},\"end\":132001,\"start\":131515}]", "bib_title": "[{\"end\":88971,\"start\":88890},{\"end\":89401,\"start\":89308},{\"end\":89971,\"start\":89932},{\"end\":90768,\"start\":90710},{\"end\":91258,\"start\":91224},{\"end\":91507,\"start\":91430},{\"end\":92033,\"start\":91965},{\"end\":92599,\"start\":92553},{\"end\":92896,\"start\":92861},{\"end\":93138,\"start\":93105},{\"end\":94639,\"start\":94577},{\"end\":95259,\"start\":95163},{\"end\":95658,\"start\":95544},{\"end\":96170,\"start\":96056},{\"end\":96777,\"start\":96707},{\"end\":97185,\"start\":97112},{\"end\":97629,\"start\":97492},{\"end\":99967,\"start\":99873},{\"end\":101101,\"start\":101014},{\"end\":101913,\"start\":101846},{\"end\":102389,\"start\":102304},{\"end\":102947,\"start\":102901},{\"end\":103450,\"start\":103372},{\"end\":104482,\"start\":104370},{\"end\":104831,\"start\":104783},{\"end\":107268,\"start\":107177},{\"end\":107979,\"start\":107887},{\"end\":108801,\"start\":108753},{\"end\":111553,\"start\":111499},{\"end\":112915,\"start\":112834},{\"end\":113645,\"start\":113592},{\"end\":114214,\"start\":114119},{\"end\":114610,\"start\":114543},{\"end\":114999,\"start\":114958},{\"end\":117360,\"start\":117299},{\"end\":118484,\"start\":118434},{\"end\":119054,\"start\":118994},{\"end\":119585,\"start\":119513},{\"end\":120062,\"start\":119986},{\"end\":120635,\"start\":120582},{\"end\":121121,\"start\":121061},{\"end\":121788,\"start\":121711},{\"end\":122326,\"start\":122259},{\"end\":123054,\"start\":122988},{\"end\":123727,\"start\":123681},{\"end\":124152,\"start\":124082},{\"end\":124891,\"start\":124821},{\"end\":125403,\"start\":125310},{\"end\":126126,\"start\":126034},{\"end\":127411,\"start\":127327},{\"end\":128419,\"start\":128339},{\"end\":128738,\"start\":128676},{\"end\":129224,\"start\":129141},{\"end\":129793,\"start\":129721},{\"end\":130434,\"start\":130371},{\"end\":131200,\"start\":131140},{\"end\":131602,\"start\":131515}]", "bib_author": "[{\"end\":88722,\"start\":88712},{\"end\":88732,\"start\":88722},{\"end\":88990,\"start\":88973},{\"end\":89001,\"start\":88990},{\"end\":89018,\"start\":89001},{\"end\":89033,\"start\":89018},{\"end\":89045,\"start\":89033},{\"end\":89411,\"start\":89403},{\"end\":89417,\"start\":89411},{\"end\":89424,\"start\":89417},{\"end\":89436,\"start\":89424},{\"end\":89984,\"start\":89973},{\"end\":89996,\"start\":89984},{\"end\":90007,\"start\":89996},{\"end\":90015,\"start\":90007},{\"end\":90417,\"start\":90403},{\"end\":90547,\"start\":90541},{\"end\":90555,\"start\":90547},{\"end\":90562,\"start\":90555},{\"end\":90570,\"start\":90562},{\"end\":90776,\"start\":90770},{\"end\":90784,\"start\":90776},{\"end\":90792,\"start\":90784},{\"end\":90798,\"start\":90792},{\"end\":90806,\"start\":90798},{\"end\":90813,\"start\":90806},{\"end\":90819,\"start\":90813},{\"end\":90826,\"start\":90819},{\"end\":90832,\"start\":90826},{\"end\":91268,\"start\":91260},{\"end\":91277,\"start\":91268},{\"end\":91289,\"start\":91277},{\"end\":91305,\"start\":91289},{\"end\":91516,\"start\":91509},{\"end\":91525,\"start\":91516},{\"end\":91533,\"start\":91525},{\"end\":91542,\"start\":91533},{\"end\":91548,\"start\":91542},{\"end\":92041,\"start\":92035},{\"end\":92047,\"start\":92041},{\"end\":92053,\"start\":92047},{\"end\":92059,\"start\":92053},{\"end\":92066,\"start\":92059},{\"end\":92073,\"start\":92066},{\"end\":92609,\"start\":92601},{\"end\":92618,\"start\":92609},{\"end\":92625,\"start\":92618},{\"end\":92631,\"start\":92625},{\"end\":92910,\"start\":92898},{\"end\":93150,\"start\":93140},{\"end\":93158,\"start\":93150},{\"end\":93164,\"start\":93158},{\"end\":93172,\"start\":93164},{\"end\":93180,\"start\":93172},{\"end\":93187,\"start\":93180},{\"end\":93194,\"start\":93187},{\"end\":93203,\"start\":93194},{\"end\":93212,\"start\":93203},{\"end\":93220,\"start\":93212},{\"end\":93226,\"start\":93220},{\"end\":93234,\"start\":93226},{\"end\":93242,\"start\":93234},{\"end\":93250,\"start\":93242},{\"end\":93259,\"start\":93250},{\"end\":93266,\"start\":93259},{\"end\":93272,\"start\":93266},{\"end\":93280,\"start\":93272},{\"end\":93287,\"start\":93280},{\"end\":93294,\"start\":93287},{\"end\":93301,\"start\":93294},{\"end\":93308,\"start\":93301},{\"end\":93738,\"start\":93728},{\"end\":93746,\"start\":93738},{\"end\":93754,\"start\":93746},{\"end\":93764,\"start\":93754},{\"end\":93775,\"start\":93764},{\"end\":94148,\"start\":94141},{\"end\":94156,\"start\":94148},{\"end\":94170,\"start\":94156},{\"end\":94179,\"start\":94170},{\"end\":94189,\"start\":94179},{\"end\":94196,\"start\":94189},{\"end\":94205,\"start\":94196},{\"end\":94213,\"start\":94205},{\"end\":94221,\"start\":94213},{\"end\":94648,\"start\":94641},{\"end\":94655,\"start\":94648},{\"end\":94663,\"start\":94655},{\"end\":94671,\"start\":94663},{\"end\":94679,\"start\":94671},{\"end\":94685,\"start\":94679},{\"end\":94987,\"start\":94976},{\"end\":94998,\"start\":94987},{\"end\":95010,\"start\":94998},{\"end\":95021,\"start\":95010},{\"end\":95271,\"start\":95261},{\"end\":95279,\"start\":95271},{\"end\":95673,\"start\":95660},{\"end\":95687,\"start\":95673},{\"end\":95696,\"start\":95687},{\"end\":95709,\"start\":95696},{\"end\":95723,\"start\":95709},{\"end\":96185,\"start\":96172},{\"end\":96199,\"start\":96185},{\"end\":96209,\"start\":96199},{\"end\":96790,\"start\":96779},{\"end\":96806,\"start\":96790},{\"end\":96818,\"start\":96806},{\"end\":96832,\"start\":96818},{\"end\":97195,\"start\":97187},{\"end\":97204,\"start\":97195},{\"end\":97212,\"start\":97204},{\"end\":97226,\"start\":97212},{\"end\":97645,\"start\":97631},{\"end\":97654,\"start\":97645},{\"end\":97665,\"start\":97654},{\"end\":97674,\"start\":97665},{\"end\":98080,\"start\":98070},{\"end\":98089,\"start\":98080},{\"end\":98105,\"start\":98089},{\"end\":98115,\"start\":98105},{\"end\":98129,\"start\":98115},{\"end\":98388,\"start\":98380},{\"end\":98397,\"start\":98388},{\"end\":98407,\"start\":98397},{\"end\":98414,\"start\":98407},{\"end\":98421,\"start\":98414},{\"end\":98431,\"start\":98421},{\"end\":98439,\"start\":98431},{\"end\":98709,\"start\":98697},{\"end\":98717,\"start\":98709},{\"end\":98727,\"start\":98717},{\"end\":98740,\"start\":98727},{\"end\":98747,\"start\":98740},{\"end\":98760,\"start\":98747},{\"end\":98771,\"start\":98760},{\"end\":98779,\"start\":98771},{\"end\":98786,\"start\":98779},{\"end\":99111,\"start\":99097},{\"end\":99131,\"start\":99111},{\"end\":99140,\"start\":99131},{\"end\":99388,\"start\":99377},{\"end\":99396,\"start\":99388},{\"end\":99407,\"start\":99396},{\"end\":99414,\"start\":99407},{\"end\":99659,\"start\":99652},{\"end\":99667,\"start\":99659},{\"end\":99674,\"start\":99667},{\"end\":99682,\"start\":99674},{\"end\":99689,\"start\":99682},{\"end\":99980,\"start\":99969},{\"end\":99991,\"start\":99980},{\"end\":100003,\"start\":99991},{\"end\":100010,\"start\":100003},{\"end\":100378,\"start\":100366},{\"end\":100388,\"start\":100378},{\"end\":100400,\"start\":100388},{\"end\":100409,\"start\":100400},{\"end\":100420,\"start\":100409},{\"end\":100431,\"start\":100420},{\"end\":100750,\"start\":100742},{\"end\":100757,\"start\":100750},{\"end\":100764,\"start\":100757},{\"end\":100772,\"start\":100764},{\"end\":100780,\"start\":100772},{\"end\":100788,\"start\":100780},{\"end\":100796,\"start\":100788},{\"end\":101113,\"start\":101103},{\"end\":101122,\"start\":101113},{\"end\":101138,\"start\":101122},{\"end\":101152,\"start\":101138},{\"end\":101623,\"start\":101616},{\"end\":101635,\"start\":101623},{\"end\":101643,\"start\":101635},{\"end\":101655,\"start\":101643},{\"end\":101667,\"start\":101655},{\"end\":101925,\"start\":101915},{\"end\":101936,\"start\":101925},{\"end\":101946,\"start\":101936},{\"end\":102406,\"start\":102391},{\"end\":102420,\"start\":102406},{\"end\":102430,\"start\":102420},{\"end\":102447,\"start\":102430},{\"end\":102461,\"start\":102447},{\"end\":102472,\"start\":102461},{\"end\":102482,\"start\":102472},{\"end\":102964,\"start\":102949},{\"end\":102974,\"start\":102964},{\"end\":102991,\"start\":102974},{\"end\":103005,\"start\":102991},{\"end\":103019,\"start\":103005},{\"end\":103030,\"start\":103019},{\"end\":103040,\"start\":103030},{\"end\":103458,\"start\":103452},{\"end\":103466,\"start\":103458},{\"end\":103475,\"start\":103466},{\"end\":103484,\"start\":103475},{\"end\":103492,\"start\":103484},{\"end\":103499,\"start\":103492},{\"end\":103507,\"start\":103499},{\"end\":103515,\"start\":103507},{\"end\":103523,\"start\":103515},{\"end\":104119,\"start\":104106},{\"end\":104130,\"start\":104119},{\"end\":104138,\"start\":104130},{\"end\":104146,\"start\":104138},{\"end\":104160,\"start\":104146},{\"end\":104498,\"start\":104484},{\"end\":104510,\"start\":104498},{\"end\":104847,\"start\":104833},{\"end\":104859,\"start\":104847},{\"end\":105253,\"start\":105246},{\"end\":105261,\"start\":105253},{\"end\":105269,\"start\":105261},{\"end\":105277,\"start\":105269},{\"end\":105283,\"start\":105277},{\"end\":105290,\"start\":105283},{\"end\":105298,\"start\":105290},{\"end\":105306,\"start\":105298},{\"end\":105596,\"start\":105585},{\"end\":105610,\"start\":105596},{\"end\":105623,\"start\":105610},{\"end\":105634,\"start\":105623},{\"end\":105922,\"start\":105915},{\"end\":105930,\"start\":105922},{\"end\":105938,\"start\":105930},{\"end\":105945,\"start\":105938},{\"end\":105954,\"start\":105945},{\"end\":105960,\"start\":105954},{\"end\":105968,\"start\":105960},{\"end\":106291,\"start\":106283},{\"end\":106300,\"start\":106291},{\"end\":106308,\"start\":106300},{\"end\":106319,\"start\":106308},{\"end\":106327,\"start\":106319},{\"end\":106336,\"start\":106327},{\"end\":106930,\"start\":106922},{\"end\":106937,\"start\":106930},{\"end\":106943,\"start\":106937},{\"end\":106952,\"start\":106943},{\"end\":106961,\"start\":106952},{\"end\":106970,\"start\":106961},{\"end\":107283,\"start\":107270},{\"end\":107291,\"start\":107283},{\"end\":107301,\"start\":107291},{\"end\":107311,\"start\":107301},{\"end\":108812,\"start\":108803},{\"end\":108823,\"start\":108812},{\"end\":108832,\"start\":108823},{\"end\":108839,\"start\":108832},{\"end\":108845,\"start\":108839},{\"end\":108854,\"start\":108845},{\"end\":109468,\"start\":109458},{\"end\":109477,\"start\":109468},{\"end\":109489,\"start\":109477},{\"end\":109771,\"start\":109762},{\"end\":109779,\"start\":109771},{\"end\":109786,\"start\":109779},{\"end\":110067,\"start\":110059},{\"end\":110075,\"start\":110067},{\"end\":110082,\"start\":110075},{\"end\":110368,\"start\":110360},{\"end\":110376,\"start\":110368},{\"end\":110383,\"start\":110376},{\"end\":110393,\"start\":110383},{\"end\":110653,\"start\":110644},{\"end\":110661,\"start\":110653},{\"end\":110669,\"start\":110661},{\"end\":110677,\"start\":110669},{\"end\":110685,\"start\":110677},{\"end\":110698,\"start\":110685},{\"end\":110989,\"start\":110982},{\"end\":110995,\"start\":110989},{\"end\":111002,\"start\":110995},{\"end\":111015,\"start\":111002},{\"end\":111290,\"start\":111283},{\"end\":111297,\"start\":111290},{\"end\":111313,\"start\":111297},{\"end\":111322,\"start\":111313},{\"end\":111568,\"start\":111555},{\"end\":111577,\"start\":111568},{\"end\":111587,\"start\":111577},{\"end\":111993,\"start\":111987},{\"end\":112000,\"start\":111993},{\"end\":112008,\"start\":112000},{\"end\":112016,\"start\":112008},{\"end\":112276,\"start\":112267},{\"end\":112283,\"start\":112276},{\"end\":112292,\"start\":112283},{\"end\":112546,\"start\":112537},{\"end\":112560,\"start\":112546},{\"end\":112571,\"start\":112560},{\"end\":112577,\"start\":112571},{\"end\":112587,\"start\":112577},{\"end\":112597,\"start\":112587},{\"end\":112611,\"start\":112597},{\"end\":112926,\"start\":112917},{\"end\":112936,\"start\":112926},{\"end\":112946,\"start\":112936},{\"end\":113379,\"start\":113368},{\"end\":113390,\"start\":113379},{\"end\":113399,\"start\":113390},{\"end\":113408,\"start\":113399},{\"end\":113655,\"start\":113647},{\"end\":113664,\"start\":113655},{\"end\":113670,\"start\":113664},{\"end\":113678,\"start\":113670},{\"end\":113690,\"start\":113678},{\"end\":113697,\"start\":113690},{\"end\":113704,\"start\":113697},{\"end\":114224,\"start\":114216},{\"end\":114231,\"start\":114224},{\"end\":114238,\"start\":114231},{\"end\":114245,\"start\":114238},{\"end\":114254,\"start\":114245},{\"end\":114622,\"start\":114612},{\"end\":114629,\"start\":114622},{\"end\":114638,\"start\":114629},{\"end\":114647,\"start\":114638},{\"end\":114663,\"start\":114647},{\"end\":115012,\"start\":115001},{\"end\":115021,\"start\":115012},{\"end\":115032,\"start\":115021},{\"end\":115439,\"start\":115433},{\"end\":115448,\"start\":115439},{\"end\":115458,\"start\":115448},{\"end\":115471,\"start\":115458},{\"end\":115480,\"start\":115471},{\"end\":115487,\"start\":115480},{\"end\":115497,\"start\":115487},{\"end\":115505,\"start\":115497},{\"end\":115802,\"start\":115793},{\"end\":115809,\"start\":115802},{\"end\":115822,\"start\":115809},{\"end\":115829,\"start\":115822},{\"end\":116059,\"start\":116052},{\"end\":116071,\"start\":116059},{\"end\":116081,\"start\":116071},{\"end\":116088,\"start\":116081},{\"end\":116094,\"start\":116088},{\"end\":116108,\"start\":116094},{\"end\":116124,\"start\":116108},{\"end\":116421,\"start\":116412},{\"end\":116430,\"start\":116421},{\"end\":116708,\"start\":116699},{\"end\":116714,\"start\":116708},{\"end\":116721,\"start\":116714},{\"end\":116729,\"start\":116721},{\"end\":117031,\"start\":117019},{\"end\":117046,\"start\":117031},{\"end\":117059,\"start\":117046},{\"end\":117071,\"start\":117059},{\"end\":117368,\"start\":117362},{\"end\":117388,\"start\":117368},{\"end\":117394,\"start\":117388},{\"end\":117404,\"start\":117394},{\"end\":117413,\"start\":117404},{\"end\":117985,\"start\":117974},{\"end\":118185,\"start\":118168},{\"end\":118272,\"start\":118264},{\"end\":118282,\"start\":118272},{\"end\":118499,\"start\":118486},{\"end\":118509,\"start\":118499},{\"end\":119066,\"start\":119056},{\"end\":119076,\"start\":119066},{\"end\":119085,\"start\":119076},{\"end\":119095,\"start\":119085},{\"end\":119109,\"start\":119095},{\"end\":119594,\"start\":119587},{\"end\":119602,\"start\":119594},{\"end\":119610,\"start\":119602},{\"end\":119619,\"start\":119610},{\"end\":120071,\"start\":120064},{\"end\":120077,\"start\":120071},{\"end\":120084,\"start\":120077},{\"end\":120094,\"start\":120084},{\"end\":120101,\"start\":120094},{\"end\":120110,\"start\":120101},{\"end\":120123,\"start\":120110},{\"end\":120644,\"start\":120637},{\"end\":120651,\"start\":120644},{\"end\":120657,\"start\":120651},{\"end\":120666,\"start\":120657},{\"end\":120673,\"start\":120666},{\"end\":120679,\"start\":120673},{\"end\":120687,\"start\":120679},{\"end\":121131,\"start\":121123},{\"end\":121138,\"start\":121131},{\"end\":121146,\"start\":121138},{\"end\":121153,\"start\":121146},{\"end\":121162,\"start\":121153},{\"end\":121796,\"start\":121790},{\"end\":121805,\"start\":121796},{\"end\":121813,\"start\":121805},{\"end\":121820,\"start\":121813},{\"end\":121828,\"start\":121820},{\"end\":121834,\"start\":121828},{\"end\":121842,\"start\":121834},{\"end\":121851,\"start\":121842},{\"end\":121859,\"start\":121851},{\"end\":121868,\"start\":121859},{\"end\":121876,\"start\":121868},{\"end\":122337,\"start\":122328},{\"end\":122346,\"start\":122337},{\"end\":122355,\"start\":122346},{\"end\":122362,\"start\":122355},{\"end\":122370,\"start\":122362},{\"end\":123063,\"start\":123056},{\"end\":123071,\"start\":123063},{\"end\":123077,\"start\":123071},{\"end\":123084,\"start\":123077},{\"end\":123097,\"start\":123084},{\"end\":123738,\"start\":123729},{\"end\":123746,\"start\":123738},{\"end\":123753,\"start\":123746},{\"end\":123762,\"start\":123753},{\"end\":123769,\"start\":123762},{\"end\":123778,\"start\":123769},{\"end\":123787,\"start\":123778},{\"end\":124162,\"start\":124154},{\"end\":124174,\"start\":124162},{\"end\":124185,\"start\":124174},{\"end\":124196,\"start\":124185},{\"end\":124209,\"start\":124196},{\"end\":124220,\"start\":124209},{\"end\":124232,\"start\":124220},{\"end\":124245,\"start\":124232},{\"end\":124625,\"start\":124616},{\"end\":124635,\"start\":124625},{\"end\":124644,\"start\":124635},{\"end\":124901,\"start\":124893},{\"end\":124908,\"start\":124901},{\"end\":124917,\"start\":124908},{\"end\":125415,\"start\":125405},{\"end\":125424,\"start\":125415},{\"end\":125436,\"start\":125424},{\"end\":125447,\"start\":125436},{\"end\":126134,\"start\":126128},{\"end\":126142,\"start\":126134},{\"end\":126150,\"start\":126142},{\"end\":126157,\"start\":126150},{\"end\":126165,\"start\":126157},{\"end\":126171,\"start\":126165},{\"end\":126178,\"start\":126171},{\"end\":126794,\"start\":126788},{\"end\":126802,\"start\":126794},{\"end\":126811,\"start\":126802},{\"end\":126819,\"start\":126811},{\"end\":126833,\"start\":126819},{\"end\":126841,\"start\":126833},{\"end\":127090,\"start\":127075},{\"end\":127101,\"start\":127090},{\"end\":127419,\"start\":127413},{\"end\":127426,\"start\":127419},{\"end\":127435,\"start\":127426},{\"end\":127443,\"start\":127435},{\"end\":128438,\"start\":128421},{\"end\":128447,\"start\":128438},{\"end\":128457,\"start\":128447},{\"end\":128752,\"start\":128740},{\"end\":128763,\"start\":128752},{\"end\":129233,\"start\":129226},{\"end\":129239,\"start\":129233},{\"end\":129246,\"start\":129239},{\"end\":129252,\"start\":129246},{\"end\":129258,\"start\":129252},{\"end\":129802,\"start\":129795},{\"end\":129811,\"start\":129802},{\"end\":129819,\"start\":129811},{\"end\":129827,\"start\":129819},{\"end\":129836,\"start\":129827},{\"end\":129842,\"start\":129836},{\"end\":129851,\"start\":129842},{\"end\":129859,\"start\":129851},{\"end\":130443,\"start\":130436},{\"end\":130450,\"start\":130443},{\"end\":130456,\"start\":130450},{\"end\":130464,\"start\":130456},{\"end\":130472,\"start\":130464},{\"end\":130479,\"start\":130472},{\"end\":130487,\"start\":130479},{\"end\":130495,\"start\":130487},{\"end\":130511,\"start\":130495},{\"end\":130517,\"start\":130511},{\"end\":130527,\"start\":130517},{\"end\":130537,\"start\":130527},{\"end\":130550,\"start\":130537},{\"end\":130559,\"start\":130550},{\"end\":130573,\"start\":130559},{\"end\":130580,\"start\":130573},{\"end\":130589,\"start\":130580},{\"end\":130597,\"start\":130589},{\"end\":131210,\"start\":131202},{\"end\":131216,\"start\":131210},{\"end\":131224,\"start\":131216},{\"end\":131231,\"start\":131224},{\"end\":131238,\"start\":131231},{\"end\":131612,\"start\":131604},{\"end\":131618,\"start\":131612},{\"end\":131624,\"start\":131618},{\"end\":131631,\"start\":131624},{\"end\":131638,\"start\":131631}]", "bib_venue": "[{\"end\":89583,\"start\":89560},{\"end\":90102,\"start\":90082},{\"end\":90981,\"start\":90915},{\"end\":91719,\"start\":91642},{\"end\":92177,\"start\":92157},{\"end\":96333,\"start\":96314},{\"end\":101313,\"start\":101241},{\"end\":102097,\"start\":102030},{\"end\":102570,\"start\":102557},{\"end\":103730,\"start\":103635},{\"end\":107478,\"start\":107466},{\"end\":108196,\"start\":108174},{\"end\":109103,\"start\":108987},{\"end\":111724,\"start\":111664},{\"end\":113083,\"start\":113023},{\"end\":113875,\"start\":113798},{\"end\":115181,\"start\":115115},{\"end\":117662,\"start\":117546},{\"end\":118659,\"start\":118640},{\"end\":119215,\"start\":119197},{\"end\":119718,\"start\":119699},{\"end\":120781,\"start\":120761},{\"end\":121943,\"start\":121921},{\"end\":122581,\"start\":122487},{\"end\":123275,\"start\":123186},{\"end\":125088,\"start\":125011},{\"end\":125568,\"start\":125553},{\"end\":126311,\"start\":126291},{\"end\":127853,\"start\":127678},{\"end\":129380,\"start\":129361},{\"end\":129963,\"start\":129943},{\"end\":130688,\"start\":130681},{\"end\":88710,\"start\":88619},{\"end\":89069,\"start\":89045},{\"end\":89523,\"start\":89436},{\"end\":90080,\"start\":90015},{\"end\":90401,\"start\":90370},{\"end\":90539,\"start\":90474},{\"end\":90913,\"start\":90832},{\"end\":91309,\"start\":91305},{\"end\":91640,\"start\":91548},{\"end\":92155,\"start\":92073},{\"end\":92674,\"start\":92670},{\"end\":92953,\"start\":92949},{\"end\":93351,\"start\":93347},{\"end\":93726,\"start\":93682},{\"end\":94139,\"start\":94070},{\"end\":94728,\"start\":94724},{\"end\":94974,\"start\":94947},{\"end\":95322,\"start\":95318},{\"end\":95766,\"start\":95762},{\"end\":96312,\"start\":96232},{\"end\":96850,\"start\":96846},{\"end\":97269,\"start\":97265},{\"end\":97717,\"start\":97713},{\"end\":98068,\"start\":98007},{\"end\":98378,\"start\":98335},{\"end\":98695,\"start\":98626},{\"end\":99095,\"start\":99037},{\"end\":99375,\"start\":99322},{\"end\":99650,\"start\":99586},{\"end\":100065,\"start\":100010},{\"end\":100364,\"start\":100287},{\"end\":100740,\"start\":100661},{\"end\":101239,\"start\":101152},{\"end\":101614,\"start\":101571},{\"end\":102028,\"start\":101946},{\"end\":102555,\"start\":102482},{\"end\":103088,\"start\":103064},{\"end\":103633,\"start\":103523},{\"end\":104104,\"start\":104032},{\"end\":104559,\"start\":104510},{\"end\":104960,\"start\":104859},{\"end\":105244,\"start\":105171},{\"end\":105583,\"start\":105525},{\"end\":105913,\"start\":105826},{\"end\":106281,\"start\":106193},{\"end\":106665,\"start\":106563},{\"end\":106920,\"start\":106847},{\"end\":107431,\"start\":107340},{\"end\":108072,\"start\":108009},{\"end\":108985,\"start\":108854},{\"end\":109456,\"start\":109375},{\"end\":109760,\"start\":109683},{\"end\":110057,\"start\":109969},{\"end\":110358,\"start\":110274},{\"end\":110642,\"start\":110594},{\"end\":110980,\"start\":110884},{\"end\":111281,\"start\":111226},{\"end\":111662,\"start\":111587},{\"end\":111985,\"start\":111921},{\"end\":112265,\"start\":112193},{\"end\":112535,\"start\":112471},{\"end\":113021,\"start\":112946},{\"end\":113366,\"start\":113304},{\"end\":113796,\"start\":113704},{\"end\":114297,\"start\":114293},{\"end\":114711,\"start\":114663},{\"end\":115113,\"start\":115032},{\"end\":115431,\"start\":115362},{\"end\":115791,\"start\":115733},{\"end\":116050,\"start\":116005},{\"end\":116410,\"start\":116326},{\"end\":116697,\"start\":116612},{\"end\":117017,\"start\":116926},{\"end\":117544,\"start\":117413},{\"end\":117972,\"start\":117945},{\"end\":118166,\"start\":118143},{\"end\":118323,\"start\":118282},{\"end\":118638,\"start\":118532},{\"end\":119172,\"start\":119109},{\"end\":119697,\"start\":119619},{\"end\":120207,\"start\":120123},{\"end\":120759,\"start\":120687},{\"end\":121288,\"start\":121162},{\"end\":121919,\"start\":121876},{\"end\":122485,\"start\":122393},{\"end\":123184,\"start\":123097},{\"end\":123860,\"start\":123787},{\"end\":124286,\"start\":124245},{\"end\":124614,\"start\":124546},{\"end\":125009,\"start\":124917},{\"end\":125509,\"start\":125447},{\"end\":126289,\"start\":126207},{\"end\":126786,\"start\":126693},{\"end\":127198,\"start\":127173},{\"end\":127631,\"start\":127466},{\"end\":128487,\"start\":128457},{\"end\":128804,\"start\":128763},{\"end\":129012,\"start\":128988},{\"end\":129359,\"start\":129281},{\"end\":129941,\"start\":129859},{\"end\":130679,\"start\":130597},{\"end\":131302,\"start\":131238},{\"end\":131735,\"start\":131638}]"}}}, "year": 2023, "month": 12, "day": 17}