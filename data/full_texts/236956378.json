{"id": 236956378, "updated": "2023-10-02 07:21:27.274", "metadata": {"title": "Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions", "authors": "[{\"first\":\"Ewin\",\"last\":\"Tang\",\"middle\":[]}]", "venue": "Phys. Rev. Lett. 127, 060503 (2021)", "journal": null, "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "A central roadblock to analyzing quantum algorithms on quantum states is the lack of a comparable input model for classical algorithms. Inspired by recent work of the author [E. Tang, STOC'19], we introduce such a model, where we assume we can efficiently perform $\\ell^2$-norm samples of input data, a natural analogue to quantum algorithms that assume efficient state preparation of classical data. Though this model produces less practical algorithms than the (stronger) standard model of classical computation, it captures versions of many of the features and nuances of quantum linear algebra algorithms. With this model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's quantum algorithms for principal component analysis [Nat. Phys. 10, 631 (2014)] and nearest-centroid clustering [arXiv:1307.0411]. Since they are only polynomially slower, these algorithms suggest that the exponential speedups of their quantum counterparts are simply an artifact of state preparation assumptions.", "fields_of_study": "[\"Computer Science\",\"Physics\"]", "external_ids": {"arxiv": "1811.00414", "mag": "3192002582", "acl": null, "pubmed": "34420330", "pubmedcentral": null, "dblp": null, "doi": "10.1103/physrevlett.127.060503"}}, "content": {"source": {"pdf_hash": "b11098db4bce5f6b71c5bc31896fda83cd09ae23", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1811.00414v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1811.00414", "status": "GREEN"}}, "grobid": {"id": "37c51325abc5faec4f716ff2fe4eaccee185f6c0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b11098db4bce5f6b71c5bc31896fda83cd09ae23.txt", "contents": "\nQuantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions\n(Dated: August 10, 2021)\n\nEwin Tang \nUniversity of Washington\n\n\nQuantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions\n(Dated: August 10, 2021)\nA central roadblock to analyzing quantum algorithms on quantum states is the lack of a comparable input model for classical algorithms. Inspired by recent work of the author [E. Tang, STOC'19], we introduce such a model, where we assume we can efficiently perform 2 -norm samples of input data, a natural analogue to quantum algorithms that assume efficient state preparation of classical data. Though this model produces less practical algorithms than the (stronger) standard model of classical computation, it captures versions of many of the features and nuances of quantum linear algebra algorithms. With this model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's quantum algorithms for principal component analysis [Nat. Phys. 10, 631 (2014)] and nearest-centroid clustering [arXiv:1307.0411]. Since they are only polynomially slower, these algorithms suggest that the exponential speedups of their quantum counterparts are simply an artifact of state preparation assumptions.\n\nINTRODUCTION\n\nQuantum machine learning (QML) has shown great promise toward yielding new exponential quantum speedups in machine learning, ever since the pioneering linear systems algorithm of Harrow, Hassidim, and Lloyd [1]. Since machine learning (ML) routines often push real-world limits of computing power, an exponential improvement to algorithm speed would allow for ML systems with vastly greater capabilities. While we have found many fast QML subroutines for ML problems since HHL [2][3][4][5][6], researchers have not been able to prove that these subroutines can be used to achieve an exponentially faster algorithm for a classical ML problem, even in the strongest input and output models [7,8]. A recent work of the author [9] suggests a surprising reason why: even our best QML algorithms, with issues with input and output models resolved, fail to achieve exponential speedups. This previous work constructs a classical algorithm matching, up to polynomial slowdown, a corresponding quantum algorithm for recommendation systems [10], which was previously believed to be one of the best candidates for an exponential speedup in machine learning [11]. In light of this result, we need to question our intuitions and reconsider one of the guiding questions of the field: when is quantum linear algebra exponentially faster than classical linear algebra?\n\nThe main challenge in answering this question is not in finding fast classical algorithms, as one might expect. Rather, it is that most QML algorithms are incomparable to classical algorithms, since they take quantum states as input and output quantum states: we don't even know an analogous classical model of computation where we can search for similar classical algorithms [8]. The quantum recommendation system is unique in that it has a classical input, a data structure implementing QRAM, and classical output, a sample from a vector in the computational basis, allowing for rigorous comparisons with classical algorithms.\n\nIn our previous work we suggest an idea for developing classical analogues to QML algorithms beyond this exceptional case [9]:\n\nWhen QML algorithms are compared to classical ML algorithms in the context of finding speedups, any state preparation assumptions in the QML model should be matched with 2 -norm sampling assumptions in the classical ML model.\n\nIn this work, we implement this idea by introducing a new input model, sample and query access (SQ access), which is an 2 -norm sampling assumption. We can get SQ access to data under typical state preparation assumptions, so fast classical algorithms in this model are strong barriers to their QML counterparts admitting exponential speedups. To support our contention that the resulting model is the right notion to consider, we use it to dequantize two seminal and well-known QML algorithms, quantum principal component analysis [12] and quantum supervised clustering [13]. That is, we give classical algorithms that, with classical SQ access assumptions replacing quantum state preparation assumptions, match the bounds and runtime of the corresponding quantum algorithms up to polynomial slowdown. Surprisingly, we do so using only the classical toolkit originally applied to the recommendation systems problem, demonstrating the power of this model in analyzing QML algorithms. From this work, we conclude that the exponential speedups of the quantum algorithms that we consider arise from strong input assumptions, rather than from the quantumness of the algorithms, since the speedups vanish when classical algorithms are given analogous assumptions. In other words, in a wide swathe of settings, on classical data, these algorithms do not give exponential speedups. QML algorithms can still be useful for quantum data (say, states generated from a quantum system), though a priori it's not clear if they give a speedup in that case, since the analogous \"classical algorithm on quantum data\" isn't well-defined.\n\nOur dequantized algorithms in the SQ access model provide the first formal evidence supporting the crucial concern about strong input and output assumptions in QML. Based on these results, we recommend exercising care when analyzing quantum linear algebra algorithms, since some algorithms with poly-logarithmic runtimes only admit polynomial speedups. BQP-complete QML problems, such as sparse matrix inversion [1] and quantum Boltzmann machine training [14], still cannot be dequantized in full unless BQP=BPP. However, many QML problems that are not BQP-complete have strong input model assumptions (like QRAM) and low-rank-type assumptions (which makes sense for machine learning, where highdimensional data often exhibits low-dimensional trends). This regime is precisely when the classical approaches we outline here work, so such problems are highly susceptible to dequantization. We believe continuing to explore the capabilities and limitations of this model is a fruitful direction for QML research.\n\nNotation.\n\n[n] := {1, . . . , n}. Consider a vector x \u2208 C n and matrix A \u2208 C m\u00d7n . A i, * and A * ,i will refer to A's ith row and column, respectively.\n\nx , A F , and A will refer to 2 , Frobenius, and spectral norm, respectively. |x := 1\nx n i=1 x i |i and |A := 1 A F m i=1 A i, * |i |A i, * (where, by the previous defini- tion, |A i, * = 1 Ai, * n j=1 A i,j |j ). A = min m,n i=1 \u03c3 i u i v \u2020 i is A's singular value decomposition, where u i \u2208 C m , v i \u2208 C n , \u03c3 i \u2208 R,\n{u i } and {v i } are sets of orthonormal vectors, and \u03c3 1 \u2265 \u03c3 2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3 min m,n \u2265 0.\nA \u03c3 := \u03c3i\u2265\u03c3 \u03c3 i u i v \u2020 i and A k := k i=1 \u03c3 i u i v \u2020\ni denote lowrank approximations to A. We assume basic arithmetic operations take unit time, and\u00d5(f ) := O(f log f ).\n\n\nTHE DEQUANTIZATION MODEL\n\nA typical QML algorithm works in the model where state preparation of input is efficient and a quantum state is output for measurement and post-processing. (Here, we assume an ideal and fault-tolerant quantum computer.) In particular, given a data point x \u2208 C n as input, we assume we can prepare copies of |x . For m input data points as a matrix A \u2208 C m\u00d7n , we additionally assume efficient preparation of |A , to preserve relative scale. We wish to compare QML and classical ML on classical data, so state preparation usually requires access to this data and its normalization factors. This informs the classical input model for our quantum-inspired algorithms, where we assume such access, and instead of preparing states, we can prepare measurements of these states.\n\nDefinition. We have O(T )-time sample and query access to x \u2208 C n (notated SQ(x)) if, in O(T ) time, we can query an index i \u2208 [n] for its entry x i ; produce an independent measurement of |x in the computational basis; and query for x . If we can only query for an estimate of the squared normx \u2208 (1 \u00b1 \u03bd) x 2 , then we denote this by SQ \u03bd (x). For A \u2208 C m\u00d7n , sample and query access to A (notated SQ(A)) is SQ(A 1, * , . . . , A n, * ) along with SQ(\u00c3) where\u00c3 is the vector of row norms, i.e.\u00c3 i := A i, * .\n\nSample and query (SQ) access will be our classical analogue to quantum state preparation. As we noted previously [9], we should be able to assume that classical analogues can efficiently measure input states: QML algorithms shouldn't rely on fast state preparation as the \"source\" of an exponential speedup. The algorithm itself should create the speedup.\n\nFor typical instantiations of state preparation oracles on classical input, we can get efficient SQ access to input. For example, given input in QRAM [15], a strong proposed generalization of classical RAM that supports state preparation, we can get log-dimension-time SQ access to input [9,Proposition 3.2]. Similarly, sparse and close-to-uniform vectors can be prepared efficiently, and correspondingly admit efficient SQ access [16]. So, in usual QML settings, SQ assumptions are easier to satisfy than state preparation assumptions.\n\nThis leads to a model based on SQ access that we codify with the informal definition of \"dequantization\". We say we dequantize a quantum protocol S : O(T )-time state preparation of |\u03c6 1 , . . . , |\u03c6 c \u2192 |\u03c8 if we describe a classical algorithm of the form C S : O(T )-time SQ(\u03c6 1 , . . . , \u03c6 c ) \u2192 SQ \u03bd (\u03c8) with similar guarantees to S up to polynomial slowdown. This is the sense in which we dequantized the quantum recommendation system in prior work [10]. In the rest of this article, we will dequantize two quantum algorithms, giving detailed sketches of the classical algorithms and leaving proofs of correctness to the supplemental material [16]. These algorithms are applications of three protocols from our previous work [9] rephrased in our access model.\n\n\nNEAREST-CENTROID CLASSIFICATION\n\nLloyd, Mohseni, and Rebentrost's quantum algorithm for clustering estimates the distance of a data point to the centroid of a cluster of points [13]. The paper claims [17] that this quantum algorithm gives an exponential speedup over classical algorithms. We dequantize Lloyd et al's quantum supervised clustering algorithm [13] with only quadratic slowdown. Though classical algorithms by Aaronson [8] and Wiebe et al. [18,Section 7] dequantize this algorithm for close-to-uniform and sparse input data, respectively, we are the first to give a general classical algorithm for this problem.\n\nProblem 1 (Centroid distance). Suppose we are given access to V \u2208 C n\u00d7d and u \u2208 C d . Estimate u \u2212 1 n 1V 2 to \u03b5 additive error with probability \u2265 1 \u2212 \u03b4.\n\nNote that we are treating vectors as rows, with 1 the vector of ones. Let\u016b := u u and letV be V , normalized so all rows have unit norm. Both classical and quantum algorithms argue about M \u2208 R (n+1)\u00d7d and w \u2208 R n+1 instead of u and V , where\nM := \u016b 1 \u221a nV and w := u \u2212 1 \u221a n\u1e7c .\nBecause wM = u \u2212 1 n 1V , we wish to estimate wM 2 = wM M \u2020 w \u2020 . Let Z := w 2 = u 2 + 1 n V 2 F be an \"average norm\" parameter appearing in our algorithms.\n\nTheorem 2 (Quantum Nearest-Centroid [13]). Suppose that, in O(T ) time, we can (1) determine u and V F ; or (2) prepare a state |u , |V 1 , . . . , |V n , or |\u1e7c . Then we can solve Problem 1 in O(T Z \u03b5 log 1 \u03b4 ) time.\n\nThe quantum algorithm proceeds by constructing the states |M and |w , then performing a swap test to get |wM . The swap test succeeds with probability 1 Z wM M \u2020 w \u2020 , so we can run amplitude amplification to get an estimate up to \u03b5 error with O( 1 \u03b5 log 1 \u03b4 ) overhead. Dequantizing this algorithm is simply a matter of dequantizing the swap test, which is done in Algorithm 1.\n\nHere, Q(y) is query access to y, which supports querying y's entries in O(1) time, but not querying samples or norms.\n\n\nAlgorithm 1 Inner product estimation\n\nInput: O(T )-time SQ \u03bd (x) \u2208 C n , Q(y) \u2208 C n Output: an estimate of x|y 1: Let s = 54 1 \u03b5 2 log 2 \u03b4 2: Collect measurements i1, . . . , is from |x\n3: Let zj = x \u2020 i j yi j x 2 |x i j | 2 for all j \u2208 [s] E[zj] = x|y 4:\nSeparate the zj's into 6 log 2 \u03b4 buckets of size 9 \u03b5 2 , and take the mean of each bucket 5: Output the (component-wise) median of the means From a simple analysis of the random variable z i 's, we get the following result. . For x, y \u2208 C n , given SQ \u03bd (x) and Q(y), Algorithm 1 outputs an estimate of x|y to (\u03b5+\u03bd+\u03b5\u03bd) x y error with probability \u2265 1\u2212\u03b4 in time O( T \u03b5 2 log 1 \u03b4 ).\n\nFor this protocol, quantum algorithms can achieve a quadratic speedup via amplitude estimation (but no more, by unstructured search lower bounds [19]). To apply this to nearest-centroid, we write wM M \u2020 w \u2020 as an inner product of tensors a|b , where\na := d i=1 n+1 j=1 n+1 k=1 M ji M k, * |i |j |k = M \u2297M ; b := d i=1 n+1 j=1 n+1 k=1 w \u2020 j w k M ki M k, * |i |j |k .\nThen, we show we have SQ access to one of the tensors (a). With this, we see that the quadratic speedup from amplitude amplification is the only speedup that quantum nearest-centroid achieves:\n\nTheorem 4 (Classical Nearest-Centroid). Suppose we are given O(T )-time SQ(V ) \u2208 C n\u00d7d and SQ(u) \u2208 C d . Then one can output a solution to Problem 1 in O(T Z 2 \u03b5 2 log 1 \u03b4 ) time.\n\n\nPRINCIPAL COMPONENT ANALYSIS\n\nWe now dequantize Lloyd, Mohseni, and Rebentrost's quantum principal component analysis (QPCA) algorithm [12], an influential early example of QML [20,21]. While the paper describes a more general strategy for Hamiltonian simulation of density matrices, their central claim is an exponential speedup in an immediate application: producing quantum states corresponding to the top principal components of a low-rank dataset [12].\n\nThe setup for the problem is as follows: suppose we are given a matrix A \u2208 R n\u00d7d whose rows correspond to data in a dataset. We will find the principal eigenvectors and eigenvalues of A \u2020 A; when A is a mean zero dataset, this corresponds to the top principal components.\n\nProblem 5 (Principal component analysis). Suppose we are given access to A \u2208 C n\u00d7d with singular values \u03c3 i and right singular vectors v i . Further suppose we are given \u03c3, k, and \u03b7 with the guarantee that, for all i \u2208 [k], \u03c3 i \u2265 \u03c3 and\n\u03c3 2 i \u2212\u03c3 2 i+1 \u2265 \u03b7 A 2 F . With probability \u2265 1\u2212\u03b4, output estimates\u03c3 2 1 , . . . ,\u03c3 2 k andv 1 , . . . ,v k satisfying |\u03c3 2 i \u2212 \u03c3 2 i | \u2264 \u03b5 \u03c3 A 2 F and v i \u2212 v i \u2264 \u03b5 v for all i \u2208 [k]. Denote A 2 F /\u03c3 2 by K.\nLloyd et al. get the following: Theorem 6. Given A F and the ability to prepare copies of |A in O(T ) time, a quantum algorithm can output the desired estimates for Problem 5\u03c3 2 1 , . . . ,\u03c3 2 k and |v 1 , . . . , |v k in\u00d5(T K min(\u03b5 \u03c3 , \u03b4) \u22123 ) time.\n\nLater results [10,22,Theorems 5.2,27] improve the runtime here to\u00d5(T K\u03b5 \u22121 \u03c3 polylog(nd/\u03b4)) when A is given in QRAM. We will compare to the original QPCA result.\n\nTo dequantize QPCA, we use a similar high-level idea to that of the quantum-inspired recommendation system [9]. We begin by using a low-rank approximation algorithm, Algorithm 2, to output a description of approximate top singular values and vectors. Algorithm 2 finds the large singular vectors of A by reducing its dimension down to W , whose singular value decomposition we can compute quickly. Then, S,\u00db ,\u03a3 define approximate large singular vectorsV := S \u2020\u00db\u03a3\u22121 . The full set of guarantees on the output of Algorithm 2 are in the supplemental material [16], but in brief, for the right setting of parameters, the columns ofV and the diagonal entries of\u03a3 satisfy the desired constraints for our v i 's and\u03c3 i 's in Problem 5. The\u03c3 i 's are output explicitly, but thev i 's are described implicitly:v i = S \u2020\u00db * ,i /\u03c3 i . We have O(T )-time SQ(S) because all rows are normalized, and rows of S are simply rows of A. Thus, sampling from S is a uniform sample from [q] and sampling from S i, * is sampling from a row of A.\u00db * ,i is an explicit vector, so in essence, we need SQ access to a linear combination of vectors, each of which we have SQ access to.\n\n\nAlgorithm 2 Low-rank approximation [23]\nInput: O(T )-time SQ(A) \u2208 R m\u00d7n , \u03c3, \u03b5, \u03b4 Output: SQ(S) \u2208 C \u00d7n , Q(\u00db ) \u2208 C q\u00d7 , Q(\u03a3) \u2208 C \u00d7 1: Set K = A 2 F /\u03c3 2 and q = \u0398 K 4 \u03b5 2 log( 1 \u03b4 ) 2:\nSample rows i1, . . . , iq from\u00c3 and define S \u2208 R q\u00d7n such that Sr, * := Ai r , * A F \u221a q A ir , * 3: Sample columns j1, . . . , jq from F, where F denotes the distribution given by sampling a uniform r \u223c [q], then sampling c from Sr. 4: Let W \u2208 C q\u00d7q be the normalized submatrix W * ,c := S * ,jc qF(jc) 5: Compute the left singular vectors of W\u00fb (1) , . . . ,\u00fb ( ) that correspond to singular values\u03c3 (1) , . . . ,\u03c3 ( ) larger than \u03c3 6: Output SQ(S),\u00db \u2208 R q\u00d7 the matrix with columns\u00fb (i) , and\u03a3 \u2208 R \u00d7 the diagonal matrix with entries\u03c3 (i) .\n\nAlgorithm 3 does exactly this: it uses rejection sampling to dequantize the swap test over a subset of qubits (getting |V w via V | (|w \u2297 I)).\n\n\nAlgorithm 3 Matrix-vector SQ access\nInput: O(T )-time SQ(V \u2020 ) \u2208 C k\u00d7n , Q(w) \u2208 C k Output: SQ \u03bd (V w) 1: function RejectionSample(SQ(V \u2020 ), Q(w)) 2: Sample i \u2208 [k] proportional to |wi| 2 V * ,i 2 by manually calculating all k probabilities 3: Sample s \u2208 [n] from V * ,i using SQ(V \u2020 ) 4: Compute rs = (V w) 2 s /(k k j=1 (Vsjwj) 2 )\n(after querying for wj and Vsj for all j \u2208 [k])\n\n\n5:\n\nOutput s with probability rs (success); otherwise, output \u2205 (failure) 6: end function 7: Query: output (V w)s 8: Sample: run RejectionSample until success (outputting s) or kC(V, w) log 1 \u03b4 failures (outputting \u2205) 9: Norm(\u03bd): Let p be the fraction of successes from running RejectionSample k\n\u03bd 2 C(V, w) log 1 \u03b4 times; output pk k i=1 |wi| 2 V * ,i 2 Proposition 7 ([9, Proposition 4.3]). For V \u2208 C n\u00d7k , w \u2208 C k , given SQ(V \u2020 ) and Q(w), Algorithm 3 simu- lates SQ \u03bd (V w) where the time to query is O(T k), sample is O(T k 2 C(V, w) log 1 \u03b4 ), and query norm is O(T k 2 C(V, w) 1 \u03bd 2 log 1 \u03b4 ). Here, \u03b4 is the desired failure probability and C(V, w) = w i V * ,i 2 / V w 2 .\nIn general, C(V, w) may be arbitrarily large, but in this application it is O(K). Quantum algorithms achieve a speedup here when k is large and C(V, w) is small, such as when V is a high-dimensional unitary, confirming our intuition that unitary operations are hard to simulate classically.\n\nAltogether, we get our desired result.\nTheorem 8. Given O(T )-time SQ(A) \u2208 C n\u00d7d , with \u03b5 \u03c3 , \u03b5 v , \u03b4 \u2208 (0, 0.01),\nthere is an algorithm that output the desired estimates for Problem 5\n\u03c3 1 , . . . ,\u03c3 k and O(T K 9 \u03b5 4 log 3 ( k \u03b4 ))-time SQ 0.01 (v 1 , . . . ,v k ) in O( K 12 \u03b5 6 log 3 ( k \u03b4 ) + T K 8 \u03b5 4 log 2 ( k \u03b4 )) time, where \u03b5 = min(0.1\u03b5 \u03c3 K 1.5 , \u03b5 2 v \u03b7, 1 4 K \u22121/2 ). Under the non-degeneracy condition \u03b7 \u2264 1 4 K \u22121/2 , this runtime is\u00d5(T K 12 \u03b5 6 \u03c3 \u03b5 12 v log 3 ( 1 \u03b4 )).\nWhile the classical runtime depends on \u03b5 v , note that a quantum algorithm must also incur this error term to learn about v i from copies of |v i . For example, computing entries or expectations of observables of v i given copies of |v i requires poly( 1 \u03b5v ) or poly(n) time. DISCUSSION We have introduced the SQ access assumption as a classical analogue to the QML state preparation assumption and demonstrated two examples where, in this classical model, we can dequantize QML algorithms with ease. We now discuss the implications of this work with respect to related literature.\n\nA natural question is of this work's relation to classical literature: does this work improve on classical algorithms for linear algebra in any regime? The answer may be no, for a subtle but fundamental reason: recall that our main idea is to introduce an input model strong enough to give classical versions of QML while being weak enough to extend to settings like QRAM, where classical computers can only access the input in very limited ways. In particular, the SQ access model that we study is weaker than the typical input model used for classical sketching algorithms [24][25][26]. O(T )-time algorithms in the quantum-inspired access model are\u00d5(nnz + T )-time algorithms in the usual RAM model (where nnz is the number of nonzero entries of the input), but not vice versa: typical sketching algorithms can exploit better data structures provided they only take O(nnz) time (e.g. oblivious sketches), whereas the quantum-inspired model can only use the QRAM data structure. The crucial insight of this work is that some algorithms (such as Algorithm 2 of Frieze et al. [23]) generalize to the weaker quantum-inspired model. Our algorithms give exponential speedups in the quantum-inspired setting, but since the model is weaker, one might expect that they perform worse in typical settings for classical computation (see [27]). These model considerations also explain why we use Frieze et al. [23]: to our knowledge, this algorithm is the only one from the classical literature that naturally generalizes to the SQ input model.\n\nThe closest analogue to these results and techniques is a work by Van den Nest on probabilistic quantum simulation [28], which describes a notion of \"computationally tractable\" (CT) states that corresponds to our notion of SQ access for vectors. With this notion, the author describes special types of circuits on CT states where weak simulation is possible, using variants of Propositions 3 and 7. However, Van den Nest's work does not have a version of Algorithm 2, since this technique only runs quickly on low-rank matrices, making it ineffective on generic quantum circuits. We exploit this low-rank structure for efficient quantum simulation of a small-butpractically-relevant class of circuits: quantum linear algebra on data with low-rank structure. So, our techniques used for supervised clustering are within the scope of Van den Nest's work, whereas our techniques for PCA are new to this line of work.\n\nThese techniques are not new to quantum simulation in general. Others have considered applying randomized numerical linear algebra to quantum simulation [29], but have not made the connection towards dequantizing quantum algorithms, especially in large generality. Low-rank approximation is crucial for tensor network simulations of quantum systems [30,31], where simulation can be done efficiently provided the input is, say, a matrix product state with low tensor rank. In this context, low-rank approximation is often performed exactly and only on a subset of the space, instead of approximately done on the full state, as is done here. This reflects the fact that tensor network algorithms assume that the system is reasonably approximated by a tensor network and aims to work well in practice, whereas our \"dequantized\" algorithms must work on a broader class of input and prioritizes provable guarantees in an abstract computational model over real-world performance. Nevertheless, some of these dequantized algorithms might be able to be matched by tensor network contraction techniques, when the input has low tensor rank. See the supplemental material for further discussion of this comparison [16].\n\nSince this work, numerous follow-ups have cemented the significance of the SQ access model introduced here [32][33][34][35]. In particular, a recent work [34] essentially dequantizes the singular value transformation framework of Gilyen et al. [36] when input is given in QRAM. These works use fundamentally the same techniques to dequan-tize a wide swathe of low-rank quantum machine learningan exciting step forward in understanding QML. \n\n\nMOTIVATING SAMPLE AND QUERY ACCESS\n\nIn this section, we demonstrate how typical input models where efficient state preparation oracles are possible also admit efficient sample and query access to input. Throughout, we will try to be precise about time complexities, but note that for classical algorithms we use the word RAM model of classical computation, which suppresses some log(n) factors (e.g. for reading indices) that are not conventionally suppressed for quantum circuit complexity.\n\nFor all these settings, the input vector x \u2208 C n is given classically, meaning that there is some way to efficiently compute x i for all i \u2208 [n]. This is typical for the QML literature, and makes sense practically in the context of including QML into a classical ML pipeline. However, we note that this assumption is crucial for our techniques, and when it is no longer satisfied (say, the input comes from a quantum system), we cannot get SQ access to input.\n\nQRAM. Quantum random access memory is a proposal to implement state preparation of an n-dimensional quantum state in time polynomial in log n through the use of a clever data structure and parallelization [1][2][3][4]. If the same input is reused or dynamically updated [5], using QRAM for QML can amortize the cost of data loading, removing the input bottleneck of these algorithms in certain cost models. In our previous work [6, 3.1, 3.2], we noted that this gives sample and query access to input as well.\n\nGrover-Rudolph state preparation. Suppose we have an efficient way to apply an integration function on the state x \u2208 C n we wish to prepare: I(s, t) = t i=s |x i | 2 . Then, Grover-Rudolph state preparation [7] can perform the rotations done in a QRAM protocol without issues with parallelization, only assuming the ability to query entries of x in superposition. So, one can prepare |x with O(log(n)) applications of the integration oracle.\n\nGiven an efficient integration oracle, we can also gain sample and query access to x: because all of the nodes of a QRAM can be expressed as an integral of x over some interval, we can simply perform that sample and query access protocol, replacing calls to the data structure with calls to the integration oracle. If integrating takes O(T ) time, this gives O(T log n)-time SQ(x), the equivalent result to the quantum setting.\n\nSparsity. When x \u2208 C n has only s nonzero entries, and we know their locations, we can prepare |x in O(s log n) gates. We can also get SQ(x) by querying all of the nonzero entries to compute the corresponding distribution for sampling and norm in O(s) time.\n\nUniformity. When entries of x are close to uniform, the corresponding state can be prepared given the ability to query x in superposition. Classically, samples and norm estimates can be found quickly using rejection sampling: sample i \u2208 [n] uniformly at random, and choose it with probability n|xi| 2 C x 2 , where C is chosen so that C \u2265 max i\u2208n n|xi| 2 x 2 ; otherwise, restart. Generic state preparation. Finally, we can generically convert a state preparation procedure to sample and query access: given input vectors x \u2208 C n classically, norms x , and a quantum procedure to create copies of |x , we can get SQ(x) by measuring |x in the computational basis. This could apply to cases where preparing |x is feasible, but a full QRAM algorithm is impractical due to the space and time overhead inherent to current QRAM proposals (e.g. from storing magic states or error-correcting).\n\n\nRELATION TO TENSOR NETWORKS\n\nSince this work can be seen as a form of quantum simulation, it makes sense to compare and contrast the algorithms presented here to tensor network algorithms. Although the most direct analogue of our work, van den Nest's paper [8], does not use low-rank approximation, it is a typical subroutine in the tensor networks context.\n\nAs a reminder, tensor network algorithms work by using tensor network representations of quantum states, such as matrix product states (MPS) or projected entangled pair states (PEPS). For a small-but-practicallyrelevant set of quantum states, such as the ground states of one-dimensional quantum spin systems, this tensor network representation is highly space-efficient [9,10]. Since this representation is also fairly time-efficient, particularly excelling for 1D tensor networks like MPS, finding and analyzing these ground states through techniques like the density-matrix renormalization group method (DMRG) [11,12] and time-evolving block decimation (TEBD) [13,14] is surprisingly tractable. Specifically, in this 1D case, many tensor network manipulations can be done in time polynomial in the bond dimension of the tensor network (sometimes called the tensor rank ) [15]. These tensor network algorithms still work even when the states being studied cannot be exactly represented with arXiv:1811.00414v3 [cs.DS] 6 Aug 2021 a small tensor network, provided they can still be closely approximated by a tensor network of the desired shape. This approximation is typically done through iterative low-rank approximation on local pieces of the state, as is done in the aforementioned DMRG and TEBD algorithms [12,16]. This technique is heuristic, though: since NP-hard problems can be encoded into the ground states of local Hamiltonians, algorithms that find MPS approximations to ground states must perform poorly on some inputs [17].\n\nWe can observe that tensor network algorithms, at least naively, do not suffice to recover the results described here. First, our algorithms require far weaker assumptions: instead of assuming the input state can be represented as an MPS with low tensor rank, we only assume that the input mixed state has a density matrix with low linear algebraic rank. For example, the results presented here support simulating QPCA applied to arbitrary pure states (though this is true for trivial reasons, since the only principal component of a pure state is the pure state itself) and probability distributions over a constant number of pure states. A tensor network algorithm might run into issues representing this pure state, since without any other restrictions, one needs an exponential number of parameters to specify it. Of course, one could perform repeated low-rank approximation to an input state to force it into, say, an MPS, but because this is done without considering the structure of the input state, this would likely incur large error. Our model bypasses these issues of representation by assuming an efficient representation of the input already exists in the form of SQ access: our algorithm works by observing that a representation for the output of QPCA can be found efficiently classically if it's allowed to depend on the input's representation. With our model abstracting away representation details, these classical analogues can run in time independent of any notion of bond dimension.\n\nIn summary: our algorithms can be run efficiently on the set of low-rank states, and this set is much larger than the set of states that are efficiently represented by tensor networks. This is not because the classical algorithms are particularly good compared to state-of-the-art tensor network methods, but because the SQ model abstracts away the difficulties that arise from representing states. One might be able to use the tensor network literature to prove that, say, QPCA on matrix product states can be simulated classically, but these results work in a far more general regime. This is the main advantage of the SQ model: the wide generality in which it applies.\n\nThough tensor network algorithms and the dequantized algorithms presented here have drastically different goals, we can still compare how the differences in their desiderata affect their application and analysis of lowrank approximation. Because tensor network algorithms perform repeated low-rank approximation on different parts of the space, the iterative algorithm that results generally has no provable guarantees, especially on general inputs, but performs exceptionally well in practice for the settings where this low-rank approximation makes sense physically [18]. The algorithms presented here are not intended to be used in practice, since (as discussed in the main text) we developed them with the intention of being barriers for asymptotic speedup for quantum machine learning algorithms. Instead, we want provable guarantees, which our algorithm for QPCA achieves by performing low-rank approximation only once, which produces low error for precisely the class of input density matrices under consideration.\n\n\nDETAILS FOR NEAREST-CENTROID CLASSIFICATION\n\nNow, we give proofs to propositions and theorems stated in the main text, beginning with the dequantization of nearest-centroid classification. Several of these results are versions of protocols from prior work [6]; we provide proofs for completeness, but encourage readers to look there for more details. . For x, y \u2208 C n , given SQ \u03bd (x) and Q(y), Algorithm 1 outputs an estimate of x|y to (\u03b5+\u03bd+\u03b5\u03bd) x y error with probability \u2265 1\u2212\u03b4 in time O( T \u03b5 2 log 1 \u03b4 ). Proof. We analyze Algorithm 1. Computing the z i 's, means, and medians all take linear time, so the runtime is dominated by the collection of samples, Line 2, as desired.\n\nAs for correctness, first consider the case that \u03bd = 0. Then the z j 's satisfy that E[z j ] = x|y and Var[z j ] \u2264 x 2 y 2 . Taking the mean of 9 \u03b5 2 copies of z j reduces the variance to \u03b5 2 x 2 y 2 /9, so by Chebyshev's inequality, the probability that the mean is \u03b5 \u221a 2 x y -far from x|y is \u2264 2 9 . Next, we treat the real and imaginary components of the means separately, and take the median of the means component-wise. To show that this median of means satisfies the desired error bound, notice that if half of the 6 log 2 \u03b4 means are within \u03b5 \u221a 2 x y of x|y in the real axis, then the median of these 6 log 2 \u03b4 means is also within \u03b5 \u221a 2 x y of x|y in the real axis. Let E i , for i \u2208 [6 log 2 \u03b4 ], be the random variable that is 0 if the ith mean is within \u03b5 \u221a 2 x y of x|y in the real axis, and 1 otherwise. Further, let Z i , for i \u2208 [6 log 2 \u03b4 ], be independent random Bernoulli variables that are 1 with probability 2 9 and 0 otherwise. We previously established that Pr[E i = 1] \u2264 2 9 , so by a Hoeffding-Chernoff bound [19],\nPr[ 6 log 2 \u03b4 i=1 E i \u2265 3 log 2 \u03b4 ] \u2264 Pr[ 6 log 2 \u03b4 i=1 Z i \u2265 3 log 2 \u03b4 ] \u2264 2/9 1/2 1/2 7/9 1/2 1/2 6 log(2/\u03b4) \u2264 \u03b4 2 .\nWe get the same bound for the imaginary component, and combine the two to get the desired correctness property. When \u03bd > 0, we cannot query for x 2 exactly, only for an estimatex \u2208 (1 \u00b1 \u03bd) x 2 . Consider the output of Algorithm 1, where we replace the x 2 withx, and call this output \u03b1. The output satisfies | x 2\n\nx \u03b1 \u2212 x|y | \u2264 \u03b5 x y with probability \u2265 1 \u2212 \u03b4. Re-arranging, we can achieve the bound\n|\u03b1 \u2212 x|y | \u2264 |\u03b1 \u2212x x 2 x|y | + |(x x 2 \u2212 1) x|y | \u2264 (1 + \u03bd)\u03b5 x y + \u03bd| x|y | \u2264 ((1 + \u03bd)\u03b5 + \u03bd) x y .\nNote that having SQ(y) instead of Q(y) improves the scaling by no more than constant factors.\n\nWe asserted in the main text that, for this protocol, quantum algorithms can achieve a quadratic speedup via amplitude estimation but no more. This holds because a quantum algorithm for inner product estimation scaling better than 1 \u03b5 would imply the ability to detect the existence of a marked item with faster than \u221a N oracle queries, contradicting unstructured search lower bounds [20].\n\n\nProof of Theorem 4\n\nWe first recall the problem we wish to solve and present the algorithm that solves it.\n\nProblem 1 (Centroid distance). Suppose we are given access to V \u2208 C n\u00d7d and u \u2208 C d . Estimate u \u2212 1 n 1V 2 to \u03b5 additive error with probability \u2265 1 \u2212 \u03b4. M 's rows are rescaled vectors that we have SQ access to; andM := [1 1 \u221a n \u00b7 \u00b7 \u00b7 1 \u221a n ], so we have SQ(M ) trivially; and w's entries can be queried using our given access. Next, use that wM M \u2020 w \u2020 = a|b for a, b the flattened tensors We recall the problem that QPCA solves.\na := d i=1 n+1 j=1 n+1 k=1 M ji M k, * |i |j |k = M \u2297M ; (1) b := d i=1 n+1 j=1 n+1 k=1 w \u2020 j w k M ki M k, * |i |j |k .(2)\nProblem 5 (Principal component analysis). Suppose we are given access to A \u2208 C n\u00d7d with singular values \u03c3 i and right singular vectors v i . Further suppose we are given \u03c3, k, and \u03b7 with the guarantee that, for all i \u2208 [k], \u03c3 i \u2265 \u03c3 and\n\u03c3 2 i \u2212\u03c3 2 i+1 \u2265 \u03b7 A 2 F . With probability \u2265 1\u2212\u03b4, output estimates\u03c3 2 1 , . . . ,\u03c3 2 k andv 1 , . . . ,v k satisfying |\u03c3 2 i \u2212 \u03c3 2 i | \u2264 \u03b5 \u03c3 A 2 F and v i \u2212 v i \u2264 \u03b5 v for all i \u2208 [k]\n. Theorem 6. Given A F and the ability to prepare copies of |A in O(T ) time, a quantum algorithm can output the desired estimates for Problem 5\u03c3 2 1 , . . . ,\u03c3 2 k and |v 1 , . . . , |v k in\u00d5(T K min(\u03b5 \u03c3 , \u03b4) \u22123 ) time.\n\nThe quantum algorithm we use is QPCA [21], and in particular, we use Prakash's analysis of Lloyd et al's QPCA:\n\nProposition (3.2.1 [3]). Let \u03c1 be a density matrix with eigenvalues \u03bb i and eigenvectors v i . Given the ability to prepare states with density matrix \u03c1 in T time, Lloyd et al's PCA algorithm [21] runs in time\u00d5(T min(\u03b5 \u03c3 , \u03b4) \u22123 ) and returns a sample (|v ,\u03bb) where Pr[|v = |v j ] = \u03bb j and\u03bb \u2208 \u03bb j \u00b1 \u03b5 \u03c3 /4 with probability at least 1 \u2212 \u03b4.\n\nIn other words, QPCA run on states with density matrix 1 A 2 F A \u2020 A outputs a random singular vector |v i with probability proportional to its singular value \u03c3 2 i . As we assume our eigenvalues have an \u03b7 A 2 F gap, the precise eigenvector |v j sampled can be identified by the eigenvalue estimate. Then, by computing enough samples, we can learn all of the eigenvalues of at least \u03c3 2 and get the corresponding states with only\u00d5( A 2 F /\u03c3 2 ) overhead, giving Theorem 6.\n\nProof of Theorem 6. Because we can prepare |A in O(T ) time, we can also prepare \u03c1 with density matrix 1 i , respectively, from the SVD of A. By assumption, we know that the first k eigenvectors have size \u2265 \u03c3 2 , and because we estimate our \u03bb i 's to \u03b5 \u03c3 /4 error, we can identify an eigenvector by its eigenvalue estimate. For each i \u2208 [k], |v i appears with probability \u2265 \u03c3 2 / A 2 F , so we see it in A 2 F \u03c3 2 log 1 \u03b4 samples with probability \u2265 1 \u2212 \u03b4. By union bound, we can find |v i and their corresponding eigenvalue estimates for all k with O(\nA 2 F A \u2020 A in O(T ) time,A 2 F \u03c3 2 log k \u03b4 )\nsamples. The eigenvectors are exactly correct and the eigenvalues have \u03b5 \u03c3 A 2 F error, as desired: so, this is the desired output. The runtime is the time to output O(\nA 2 F \u03c3 2 log k \u03b4 ) samples.\nNote that the quantum algorithm crucially uses the gap assumption in Problem 5. Without a gap assumption, it's still possible to efficiently learn the subspace spanned by the top k singular vectors. In this work, we restrict our scope to the task of finding individual singular vectors as stated [21].\n\nAlso note that the QPCA result isn't particularly useful if A is not low-rank: if \u03c1 is high-dimensional, we would expect that \u03bb j 's are around O(1/n), and if we wanted to distinguish individual eigenvectors, we would also have that \u03b5 \u03c3 = O(1/n), ruining any exponential speedup.\n\n\nAnalysis of Low-Rank Approximation Algorithm\n\nAlgorithm 2 Low-rank approximation [22] Input: O(T )-time SQ(A) \u2208 R m\u00d7n , \u03c3, \u03b5, \u03b4 Output: SQ(S) \u2208 C \u00d7n , Q(\u00db ) \u2208 C q\u00d7 , Q(\u03a3) \u2208 C \u00d7 1: Set K = A 2 F /\u03c3 2 and q = \u0398 K 4 \u03b5 2 log( 1 \u03b4 ) 2: Sample rows i1, . . . , iq from\u00c3 and define S \u2208 R q\u00d7n such that Sr, * := Ai r , * A F \u221a q A ir , * 3: Sample columns j1, . . . , jq from F, where F denotes the distribution given by sampling a uniform r \u223c [q], then sampling c from Sr. 4: Let W \u2208 C q\u00d7q be the normalized submatrix W * ,c := S * ,jc qF(jc) 5: Compute the left singular vectors of W\u00fb (1) , . . . ,\u00fb ( ) that correspond to singular values\u03c3 (1) , . . . ,\u03c3 ( ) larger than \u03c3 6: Output SQ(S),\u00db \u2208 R q\u00d7 the matrix with columns\u00fb (i) , and\u03a3 \u2208 R \u00d7 the diagonal matrix with entries\u03c3 (i) .\n\nProposition 9 ([6, Theorem 4.4], adapted from Frieze et al. [22]). Suppose we are given O(T )-time SQ(A) \u2208 C n\u00d7d , a singular value threshold \u03c3, and an error parameter\n\u03b5 \u2208 (0, \u03c3/ A F /4]. Denote K := A 2 F /\u03c3 2 . Then in O K 12 \u03b5 6 log 3 1 \u03b4 + T K 8 \u03b5 4 log 2 1 \u03b4 time, Algorithm 2 outputs O(T )-time SQ(S) \u2208 C q\u00d7n and U \u2208 C q\u00d7 ,\u03a3 \u2208 R \u00d7 (for = \u0398( K 4 \u03b5 2 log 1 \u03b4 ))\nimplicitly describing a low-rank approximation to A, D := AVV \u2020 withV := S \u2020\u00db\u03a3\u22121 (notice rank D \u2264 ). This description satisfies the following with probability \u2265 1 \u2212 \u03b4:\n(a) \u03c3 \u2265 \u03c3 \u2212 \u03b5 A F and \u03c3 +1 < \u03c3 + \u03b5 A F ; (b) A \u2212 D 2 F \u2264 A \u2212 A 2 F + \u03b5 A 2 F ; (c) i=1 |\u03c3 2 i \u2212 \u03c3 2 i | \u2264 \u03b5 A 2 F /K 1.5 = \u03b5\u03c3 3 / A F ; (d) V \u2212\u039b F \u2264 \u03b5\nfor some \u039b with orthonormal columns and the same image asV .\n\nIntuitively, (a) shows that is the right place to truncate up to \u03b5 error, (b) is a low-rank approximation guarantee, (c) shows that our singular values are approximately correct, and (d) shows that our singular vectors are approximately orthonormal. These additive-error bounds are weaker than the relative-error bounds common in the classical ML literature. This seems inherent, since our SQ access assumes less about input compared to the classical literature (see discussion). Quantum algorithms also only achieve additive-error bounds.\n\nProof. Because the only difference between Algorithm 2 and ModFKV from previous work [6] is the different choice of \u03b5, this result mostly follows directly from previous work. The algorithm and runtime are given in Section 4; (a) follows from Lemma 4.5(\u2665); (b) follows from Lemma 4.5(\u2666); (d) follows from Proposition 4.6. For (c), we know that\nA \u2020 A \u2212 S \u2020 S F , SS \u2020 \u2212 CC \u2020 F \u2264 A 2 F / \u221a q = \u03b5\u03c3 2 /K.\nThis is stated in the proof of [6,Proposition 4.6]; no factor of log(1/\u03b4) appears because it is used to amplify the probability of this equation being true. Let \u03c3 M,i denote the singular values of M , respectively. By applying the Hoffman-Wielandt inequality to both, we get that\n|\u03c3 2 A,i \u2212 \u03c3 2 S,i | 2 , |\u03c3 2 S,i \u2212 \u03c3 2 C,i | 2 \u2264 \u03b5 A 2 F /K 2 . so k i=1 |\u03c3 2 A,i \u2212 \u03c3 2 C,i | \u2264 \u221a k k i=1 |\u03c3 2 A,i \u2212 \u03c3 2 C,i | 2 1/2 \u2264 \u221a k |\u03c3 2 A,i \u2212 \u03c3 2 C,i | 2 1/2 \u2264 \u221a k\u03b5 A 2 F /K 2 \u2264 \u03b5 A 2 F /K 1.5 .\n\nProof of Proposition 7\n\nAlgorithm 3 Matrix-vector SQ access\nInput: O(T )-time SQ(V \u2020 ) \u2208 C k\u00d7n , Q(w) \u2208 C k Output: SQ \u03bd (V w) 1: function RejectionSample(SQ(V \u2020 ), Q(w)) 2: Sample i \u2208 [k]\nproportional to |wi| 2 V * ,i 2 by manually calculating all k probabilities Compute rs = (V w) 2 s /(k k j=1 (Vsjwj) 2 ) (after querying for wj and Vsj for all j \u2208 [k])\n\n\n5:\n\nOutput s with probability rs (success); otherwise, output \u2205 (failure) 6: end function 7: Query: output (V w)s 8: Sample: run RejectionSample until success (outputting s) or kC(V, w) log 1 \u03b4 failures (outputting \u2205) 9: Norm(\u03bd): Let p be the fraction of successes from running RejectionSample k \u03bd 2 C(V, w) log 1 \u03b4 times; output pk k i=1 |wi| 2 V * ,i 2 Proposition 7 ([6, Proposition 4.3]). For V \u2208 C n\u00d7k , w \u2208 C k , given SQ(V \u2020 ) and Q(w), Algorithm 3 simulates SQ \u03bd (V w) where the time to query is O(T k), sample is O(T k 2 C(V, w) log 1 \u03b4 ), and query norm is\nO(T k 2 C(V, w) 1 \u03bd 2 log 1 \u03b4 ).\nHere, \u03b4 is the desired failure probability and C(V, w) = w i V * ,i 2 / V w 2 .\n\nProof. We analyze Algorithm 3. We use the following easyto-verify facts about the function RejectionSample: (1) r s \u2264 1 by Cauchy-Schwarz, so the protocol is well-defined;\n\n(2) conditioned on success, s is a measurement from |V w , because the denominator of r s is the probability s is selected in Line 3, up to normalization; (3) the function succeeds with probability (kC(V, w)) \u22121 ; (4) running it takes O(T k) time.\n\nQuery clearly has the stated properties. (2) implies that Sample is correct if it succeeds, (3) implies that it has the stated failure probability, and (4) implies the stated runtime. (3) implies that Norm is correct (by a Chernoff bound), and (4) implies the stated runtime.\n\nProof of Theorem 8\n\n\nAlgorithm 5 Principal component analysis\n\nInput: SQ(A) \u2208 C n\u00d7d , \u03c3, k, \u03b7 as in Problem 5 Output:\u03c3i andvi for i \u2208 [k] as in Problem 5 1: Set \u03b5 \u2190 min(\u03b5\u03c3K 1.5 , \u03b5 2 v \u03b7, 1 4 K \u22121/2 ) and \u03c3 \u2190 \u03c3 \u2212 \u03b5 A F 2: Run Algorithm 2 with parameters \u03c3 , \u03b5, and \u03b4/k to find a low-rank approximation described by SQ(S) \u2208 C q\u00d7n , U \u2208 C q\u00d7 ,\u03a3 \u2208 C \u00d7 3: For i \u2208 [k], let\u03c3i :=\u03a3ii,vi := S \u2020\u00db * ,i/\u03a3ii 4: Output\u03c3 2 i and SQ \u03bd (vi); SQ access follows from Algorithm 3 with matrix S \u2020 and vector\u00db (i) /\u03a3ii. Theorem 8. Given O(T )-time SQ(A) \u2208 C n\u00d7d , with \u03b5 \u03c3 , \u03b5 v , \u03b4 \u2208 (0, 0.01), Algorithm 5 outputs the desired estimates for Problem 5\u03c3 1 , . . . ,\u03c3 k and O(T K 9\n\u03b5 4 log 3 ( k \u03b4 ))-time SQ 0.01 (v 1 , . . . ,v k ) in O( K 12 \u03b5 6 log 3 ( k \u03b4 ) + T K 8 \u03b5 4 log 2 ( k \u03b4 )) time, where \u03b5 = min(0.1\u03b5 \u03c3 K 1.5 , \u03b5 2 v \u03b7, 1 4 K \u22121/2 ).\nMost of this theorem simply follows by combining the guarantees from Proposition 9 and Proposition 7 to get the PCA guarantees. The only nontrivial piece of the proof is showing that v i \u2212v i \u2264 \u03b5 \u03c3 , since the low-rank approximation algorithm gives no guarantee about controlling individualv i 's. The 9(b) bound, even with \u03b5 = 0, only implies that {v i } span the same subspace as {v i }. So, for r \u2208 [k], we consider the rank-r approximation formed by using onlyv 1 , . . . ,v r . The 9(b) bound for all r of these low-rank approximations give the desired bounds: using the gap assumption, the firstv r to deviate from v r will violate the bound for that rank approximation. So, allv r must be close to their corresponding v r .\n\nProof. First, consider runtime. From Proposition 9, using that \u03c3 \u2265 3\u03c3/4, we have that the runtime of Algorithm 5 is correct. Further, for ourv i 's, applying Algorithm 3 gives the desired runtimes, since\nC(S \u2020 ,\u00db * ,i /\u03a3 ii ) = j S j, * 2 (\u00db ji /\u03a3 ii ) 2 v i 2 \u2264 S 2 F \u00db * ,i 2 \u03a3 2 ii v i 2 = A 2 F \u03a3 2 ii v i 2 \u2264 A 2 F \u03c3 2 (1 \u2212 \u03b5) = O(K).\nwhere we used Cauchy-Schwarz, that S 2 F = A 2 F , \u00db * ,i = 1,\u03a3 ii \u2265 \u03c3, and Proposition 9(d). For correctness: by 9(a), \u03c3 +1 < \u03c3, so \u2265 k, and Line 3 of Algorithm 5 is well-defined. Further, by 9(c), we can bound\u03c3 i error |\u03c3 2 i \u2212 \u03c3 2 i | \u2264 \u03b5 \u03c3 3 A F \u2264 \u03b5 \u03c3 A 2 F . Now, we show that v i \u2212v i \u2264 \u03b5 \u03c3 . LetV r denoteV truncated to its first r columns. Observe that for all r \u2264 , A \u2212 AV rV \u2020 r 2 F \u2264 A \u2212 A r 2 F + \u03b5 A 2 F holds with probability \u2265 1 \u2212 \u03b4. This holds for r = by Proposition 3(b), so suppose r < . We can imagine that our run of Algorithm 3 is equivalent to simultaneous runs with different singular value thresholds \u03b3 r := 1 2 (\u03c3 r + \u03c3 r+1 ), the only differences being that we truncate to threshold \u03c3 instead of \u03b3 r , and q is larger. Truncating to \u03b3 r is equivalent to truncating at the rth singular value, sinc\u00ea \u03c3 r > \u03b3 r >\u03c3 r+1 :\n|\u03c3 i \u2212 \u03c3 i | \u2264 1 \u03c3 |\u03c3 2 i \u2212 \u03c3 2 i | \u2264 1 \u03c3 i=1 |\u03c3 2 i \u2212 \u03c3 2 i | \u2264 \u03b7 10 A F ; |\u03c3 i \u2212 \u03b3 i | = 1 2 |\u03c3 i \u2212 \u03c3 i+1 | \u2265 1 4 A F |\u03c3 2 i \u2212 \u03c3 2 i+1 | \u2265 \u03b7 4 A F .\nSo,\u03c3 r \u2265 \u03c3 r \u2212 \u03b7 10 A F > \u03c3 r \u2212 \u03b7 4 A F \u2265 \u03b3 r , and similarly for the other side. Increasing q is equivalent to decreasing \u03b5, so for our truncatedV [r] 's, the guarantees from Proposition 3 hold. Namely, 3(b) holds as desired. By choosing failure probability \u03b4/k, we can guarantee, with probability \u2265 1\u2212\u03b4, A\u2212AV rV \u2020 r 2\nF \u2264 A\u2212A r 2 F +\u03b5 A 2 F\nholds for all r \u2208 [k]. Fix \u039b as the isometry from Proposition 3(d) satisfying V \u2212 \u039b F \u2264 \u03b5. Now, consider a truncationV r for any r \u2208 [k], and notice that V r \u2212 \u039b r F \u2264 \u03b5.\nA \u2212 A\u039b r \u039b \u2020 r 2 F \u2264 ( A \u2212 AV rV \u2020 r F + O(\u03b5) A F ) 2 \u2264 A \u2212 AV rV \u2020 r 2 F + O(\u03b5) A 2 F \u2264 A \u2212 A r 2 F + O(\u03b5) A 2 F\nNow, we rewrite the expression, heavily using that \u039b r and V r (the matrix with columns v 1 , . . . , v r from A's SVD) are isometries.\nO(\u03b5) A 2 F \u2265 A \u2212 A\u039b r \u039b \u2020 r 2 F \u2212 A \u2212 A r 2 F = A 2 F \u2212 A\u039b r \u039b \u2020 r 2 F \u2212 ( A 2 F \u2212 A r 2 F ) = A r 2 F \u2212 A\u039b r \u039b \u2020 r 2 F = r i=1 \u03c3 2 i \u2212 min m,n i=1 \u03c3 2 i \u039b r \u039b \u2020 r v i 2 = r i=1 \u03c3 2 i (1 \u2212 \u039b r \u039b \u2020 r v i 2 ) \u2212 min m,n i=r+1 \u03c3 2 i \u039b r \u039b \u2020 r v i 2 \u2265 \u03c3 2 r r i=1 (1 \u2212 \u039b r \u039b \u2020 r v i 2 ) \u2212 \u03c3 2 r+1 min m,n i=r+1 \u039b r \u039b \u2020 r v i 2 = \u03c3 2 r (I \u2212 \u039b r \u039b \u2020 r )V r V \u2020 r 2 F \u2212 \u03c3 2 r+1 \u039b r \u039b \u2020 r (I \u2212 V r V \u2020 r ) 2 F = \u03c3 2 r (r \u2212 \u039b r \u039b \u2020 r V r V \u2020 r 2 F ) \u2212 \u03c3 2 r+1 (r \u2212 \u039b r \u039b \u2020 r V r V \u2020 r 2 F ) \u2265 \u03b7 A 2 F (r \u2212 \u039b r \u039b \u2020 r V r V \u2020 r 2\nF ) Here, we used the Pythagorean theorem ( A\u03a0 2 F + A(I \u2212 \u03a0) 2 F = A 2 F for \u03a0 an orthogonal projector), that \u039b\u039b \u2020 v i 2 \u2208 [0, 1], that V M F = M F if V is an isometry, that Frobenius norm decomposes M 2 F = M * ,i 2 , and the gap assumption from the PCA problem statement.\n\nSo, \u039b r \u039b \u2020 r V r V \u2020 r 2 F = \u039b \u2020 r V r 2 F \u2265 r \u2212 O(\u03b5/\u03b7), and equivalently, (V r V \u2020 r \u2212 I)\u039b r \u039b \u2020 r 2\nF = O(\u03b5/\u03b7). v r \u2212v r \u2264 v r \u2212 \u039b * ,r + \u039b * ,r \u2212v r \u2264 v r \u2212 \u039b * ,r + \u03b5 \u2264 v r \u2212 V k V \u2020 k \u039b * ,r + V k V \u2020 k \u039b * ,r \u2212 \u039b * ,r + \u03b5 \u2264 v r \u2212 V k V \u2020 k \u039b * ,r + (V k V \u2020 k \u2212 I)\u039b k \u039b \u2020 k F + \u03b5 \u2264 v r \u2212 V k V \u2020 k \u039b * ,r + O( \u03b5/\u03b7) = (v r \u2212 V k V \u2020 k \u039b * ,r ) \u2020 (v r \u2212 V k V \u2020 k \u039b * ,r ) + O( \u03b5/\u03b7) \u2264 2 \u2212 2 v r , \u039b * ,r + O( \u03b5/\u03b7) = O( \u03b5/\u03b7) = O(\u03b5 v )\nThe last line follows from combining previously-seen inequalities: let x i,j = v i , \u039b * ,j 2 . Then we can rewrite\n\u039b \u2020 s V s 2 F \u2265 s \u2212 O(\u03b5/\u03b7) \u21d0\u21d2 s i,j=1 x i,j \u2265 s \u2212 O(\u03b5/\u03b7); \u039b \u2020 v i 2 \u2264 1 \u21d0\u21d2 k j=1 x i,j \u2264 1; (\u039b * ,j ) \u2020 V k 2 \u2264 1 \u21d0\u21d2 k i=1\nx i,j \u2264 1.\n\nNow, consider taking a linear combination of these inequalities: add together the first inequality for s = r \u2212 1, r and subtract the second and third inequalities for i, j \u2208 [r \u2212 1].\n\nr\u22121 i,j=1\n\nx i,j + r i,j=1\nx i,j \u2265 2r \u2212 1 \u2212 O(\u03b5/\u03b7) \u2212 r\u22121 i=1 k j=1 x i,j \u2265 1 \u2212 r \u2212 r\u22121 j=1 k i=1 x i,j \u2265 1 \u2212 r =\u21d2 x rr \u2212 r\u22121 i=1 k j=r+1 (x i,j + x j,i ) \u2265 1 \u2212 O(\u03b5/\u03b7)\nSo, x rr \u2265 1 \u2212 O(\u03b5/\u03b7), implying that v r , \u039b * ,r \u2265 1 \u2212 O(\u03b5/\u03b7).\n\n\nInner product estimation Input: O(T )-time SQ \u03bd (x) \u2208 C n , Q(y) \u2208 C n Output: an estimate of x|y 1: Let s = 54 1 \u03b5 2 log 2 \u03b4 2: Collect measurements i1, . . . , is from |x 3: Let zj = x \u2020 i j yi j x 2 |x i j | 2 for all j \u2208 [s] E[zj] = x|y 4: Separate the zj's into 6 log 2 \u03b4 buckets of size 9 \u03b5 2 , and take the mean of each bucket 5: Output the (component-wise) median of the means Proposition 3 ([6, Proposition 4.2])\n\nAlgorithm 4\n4Classical supervised clustering Input: O(T )-time SQ(V1, . . . , Vn, u) Output: An estimate of \u03bb = u \u2212 1 n V 1 2 1: Achieve O(T )-time SQ(a), Q(b) for a, b as in (1), (2) 2: Run Algorithm 1 to estimate a|b to \u03b5 error and output the result Theorem 4 (Classical Nearest-Centroid). Suppose we are given O(T )-time SQ(V ) \u2208 C n\u00d7d and SQ(u) \u2208 C d . Then Algorithm 4 outputs a solution to Problem 1 in O(T Z 2 \u03b5 2 log 1 \u03b4 ) time. Proof. Recall from the main text that the quantity we wish to estimate can be written as wM M \u2020 w and w := u \u2212 1 \u221a n\u1e7c . First, notice that we have O(T )-time SQ(M ) and Q(w):\n\nGiven\nO(T )-time SQ(M ) and Q(w), we have O(T )-time SQ(a) and Q(b): namely, we can sample from a by sampling j and k fromM , and then sampling i from M j, * . Thus, we can apply Proposition 3 to estimate w T M T M w to \u03b5(4Z) error with probability 1 \u2212 \u03b4 in O(T 1 \u03b5 2 log 1 \u03b4 ) time, using that a = M 2 F = 4 and b = w 2 = Z. Rescaling \u03b5 by 4Z gives the desired result.\n\n\nsince this is the state of the second set of qubits of |A . So, we can get a sample (|v ,\u03bb) in O(T min(\u03b5 \u03c3 , \u03b4) \u22123 ) time. The eigenvectors and eigenvalues of \u03c1 are v i and \u03c3 2\n\n3 :\n3Sample s \u2208 [n] from V * ,i using SQ(V \u2020 ) 4:\n\n\nThanks to Ronald de Wolf for giving the initial idea to look at QPCA. Thanks to Nathan Wiebe for helpful comments on this document. Thanks to Daniel Liang and Patrick Rall for their help fleshing out these ideas and reviewing a draft of this document. Thanks to Scott Aaronson for helpful discussions. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1762114. * ewint@cs.washington.edu; ewintang.] Z. Zhao, J. K. Fitzsimons, and J. F. Fitzsimons, Quantumassisted Gaussian process regression, Physical Review A 99, 052331 (2019), arXiv:1512.03929 [quant-ph]. [6] F. G. Brandao and K. M. Svore, Quantum speed-ups forcom \n[1] A. W. Harrow, A. Hassidim, and S. Lloyd, Quantum \nalgorithm for linear systems of equations, Physical review \nletters 103, 150502 (2009). \n[2] P. Rebentrost, M. Mohseni, and S. Lloyd, Quantum sup-\nport vector machine for big data classification, Physical \nreview letters 113, 130503 (2014). \n[3] N. Wiebe, D. Braun, and S. Lloyd, Quantum algorithm for \ndata fitting, Physical review letters 109, 050505 (2012). \n[4] S. Lloyd, S. Garnerone, and P. Zanardi, Quantum al-\ngorithms for topological and geometric analysis of data, \nNature Communications 7, 10138 (2016). \n[5solving semidefinite programs, in 2017 IEEE 58th Annual \nSymposium on Foundations of Computer Science (FOCS) \n(IEEE, 2017). \n[7] A. M. Childs, Equation solving by simulation, Nature \nPhysics 5, 861 (2009). \n[8] S. Aaronson, Read the fine print, Nature Physics 11, 291 \n(2015). \n[9] E. Tang, A quantum-inspired classical algorithm for rec-\nommendation systems, in Proceedings of the 51st Annual \nACM SIGACT Symposium on Theory of Computing -\nSTOC 2019 (ACM Press, 2019) arXiv:1807.04271 [cs.IR]. \n[10] I. Kerenidis and A. Prakash, Quantum recommendation \nsystems, in 8th Innovations in Theoretical Computer Sci-\nence Conference (ITCS 2017), LIPIcs, Vol. 67 (Schloss \nDagstuhl, 2017) pp. 49:1-49:21. \n[11] J. Preskill, Quantum computing in the NISQ era and \nbeyond, Quantum 2, 79 (2018). \n[12] S. Lloyd, M. Mohseni, and P. Rebentrost, Quantum prin-\ncipal component analysis, Nature Physics 10, 631 (2014). \n[13] S. Lloyd, M. Mohseni, and P. Rebentrost, Quantum algo-\nrithms for supervised and unsupervised machine learning, \narXiv (2013), arXiv:1307.0411 [quant-ph]. \n[14] M. Kieferov\u00e1 and N. Wiebe, Tomography and generative \ntraining with quantum boltzmann machines, Phys. Rev. \nA 96, 062327 (2017). \n[15] V. Giovannetti, S. Lloyd, and L. Maccone, Quantum ran-\ndom access memory, Physical review letters 100, 160501 \n(2008). \n[16] See supplemental material for details on when sample and \nquery access is possible; discussion on the relation of this \nwork to the tensor networks literature; and full proofs for \nthe results stated here. It includes Refs. [37-47]. \n[17] We were not able to verify the quantum algorithm (namely, \nthe Hamiltonian simulation for preparing |\u03c6 ) as stated. \nFor our purposes, we can make the minor additional \nassumption of efficient state preparation access to |\u03c6 , \nwhich makes correctness obvious. When we refer to the \nquantum algorithm in this letter, we mean this version of \nit. Supplemental Material for \"Quantum principal component analysis only achieves an \nexponential speedup because of its state preparation assumptions\" \n\nEwin Tang  *  \n\nUniversity of Washington \n(Dated: August 10, 2021) \n\n\n* ewint@cs.washington.edu; ewintang.com\n\nQuantum algorithms for nearest-neighbor methods for supervised and unsupervised learning. N Wiebe, A Kapoor, K M Svore, Quantum Information and Computation. 15N. Wiebe, A. Kapoor, and K. M. Svore, Quantum algo- rithms for nearest-neighbor methods for supervised and unsupervised learning, Quantum Information and Com- putation 15, 316-356 (2015).\n\nStrengths and weaknesses of quantum computing. C H Bennett, E Bernstein, G Brassard, U Vazirani, SIAM Journal on Computing. 261510C. H. Bennett, E. Bernstein, G. Brassard, and U. Vazirani, Strengths and weaknesses of quantum computing, SIAM Journal on Computing 26, 1510 (1997).\n\nQuantum machine learning. J Biamonte, P Wittek, N Pancotti, P Rebentrost, N Wiebe, S Lloyd, Nature. 549195J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and S. Lloyd, Quantum machine learning, Na- ture 549, 195 (2017).\n\nQuantum machine learning: a classical perspective. C Ciliberto, M Herbster, A D Ialongo, M Pontil, A Rocchetto, S Severini, L Wossnig, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 47420170551C. Ciliberto, M. Herbster, A. D. Ialongo, M. Pontil, A. Rocchetto, S. Severini, and L. Wossnig, Quantum ma- chine learning: a classical perspective, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 474, 20170551 (2018).\n\nThe power of block-encoded matrix powers: improved regression techniques via faster Hamiltonian simulation. S Chakraborty, A Gily\u00e9n, S Jeffery, arXiv:1804.0197346th International Colloquium on Automata, Languages, and Programming (ICALP 2019), LIPIcs (Schloss Dagstuhl. quant-phS. Chakraborty, A. Gily\u00e9n, and S. Jeffery, The power of block-encoded matrix powers: improved regression techniques via faster Hamiltonian simulation, in 46th International Colloquium on Automata, Languages, and Programming (ICALP 2019), LIPIcs (Schloss Dagstuhl, 2019) arXiv:1804.01973 [quant-ph].\n\nFast Monte-Carlo algorithms for finding low-rank approximations. A Frieze, R Kannan, S Vempala, Journal of the ACM (JACM). 511025A. Frieze, R. Kannan, and S. Vempala, Fast Monte-Carlo algorithms for finding low-rank approximations, Journal of the ACM (JACM) 51, 1025 (2004).\n\nRandomized algorithms for matrices and data. M W Mahoney, Foundations and Trends\u00ae in Machine Learning. 3123M. W. Mahoney, Randomized algorithms for matrices and data, Foundations and Trends\u00ae in Machine Learning 3, 123 (2011).\n\nSketching as a tool for numerical linear algebra. D P Woodruff, 10.1561/0400000060Foundations and Trends\u00ae in Theoretical Computer Science. 10D. P. Woodruff, Sketching as a tool for numerical lin- ear algebra, Foundations and Trends\u00ae in Theoretical Computer Science 10, 10.1561/0400000060 (2014).\n\nRandomized algorithms in numerical linear algebra. R Kannan, S Vempala, Acta Numerica. 2695R. Kannan and S. Vempala, Randomized algorithms in numerical linear algebra, Acta Numerica 26, 95 (2017).\n\nQuantum-inspired algorithms in practice. J M Arrazola, A Delgado, B R Bardhan, S Lloyd, Quan-tum10.22331/q-2020-08-13-307arXiv:1905.10415quant-phJ. M. Arrazola, A. Delgado, B. R. Bardhan, and S. Lloyd, Quantum-inspired algorithms in practice, Quan- tum 10.22331/q-2020-08-13-307 (2020), arXiv:1905.10415 [quant-ph].\n\nSimulating quantum computers with probabilistic methods. M Van Den, Nest, Quantum Info. Comput. 11M. Van Den Nest, Simulating quantum computers with probabilistic methods, Quantum Info. Comput. 11 (2011).\n\nApproximating Hamiltonian dynamics with the Nystr\u00f6m method. A Rudi, L Wossnig, C Ciliberto, A Rocchetto, M Pontil, S Severini, arXiv:1804.024844quant-phA. Rudi, L. Wossnig, C. Ciliberto, A. Rocchetto, M. Pon- til, and S. Severini, Approximating Hamiltonian dynam- ics with the Nystr\u00f6m method, Quantum 4, 234 (2020), arXiv:1804.02484 [quant-ph].\n\nThe density-matrix renormalization group in the age of matrix product states. U Schollw\u00f6ck, Annals of Physics. 32696U. Schollw\u00f6ck, The density-matrix renormalization group in the age of matrix product states, Annals of Physics 326, 96 (2011).\n\nA practical introduction to tensor networks: Matrix product states and projected entangled pair states. R Or\u00fas, Annals of Physics. 349117R. Or\u00fas, A practical introduction to tensor networks: Matrix product states and projected entangled pair states, Annals of Physics 349, 117 (2014).\n\nQuantum-Inspired Algorithms for Solving Low-Rank Linear Equation Systems with Logarithmic Dependence on the Dimension. N.-H Chia, A Gily\u00e9n, H.-H Lin, S Lloyd, E Tang, C Wang, 31st International Symposium on Algorithms and Computation (ISAAC 2020), LIPIcs (Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik. N.-H. Chia, A. Gily\u00e9n, H.-H. Lin, S. Lloyd, E. Tang, and C. Wang, Quantum-Inspired Algorithms for Solving Low-Rank Linear Equation Systems with Logarithmic Dependence on the Dimension, in 31st International Sym- posium on Algorithms and Computation (ISAAC 2020), LIPIcs (Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2020).\n\nQuantum-Inspired Sublinear Algorithm for Solving Low-Rank Semidefinite Programming. N.-H Chia, T Li, H.-H Lin, C Wang, arXiv:1901.0325445th International Symposium on Mathematical Foundations of Computer Science (MFCS 2020), LIPIcs (Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik. cs.DSN.-H. Chia, T. Li, H.-H. Lin, and C. Wang, Quantum- Inspired Sublinear Algorithm for Solving Low-Rank Semidefinite Programming, in 45th International Sympo- sium on Mathematical Foundations of Computer Science (MFCS 2020), LIPIcs (Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2020) arXiv:1901.03254 [cs.DS].\n\nSampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. N.-H Chia, A Gily\u00e9n, T Li, H.-H Lin, E Tang, C Wang, arXiv:1910.06151Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing -STOC 2020. the 52nd Annual ACM SIGACT Symposium on Theory of Computing -STOC 2020ACM Presscs.DSN.-H. Chia, A. Gily\u00e9n, T. Li, H.-H. Lin, E. Tang, and C. Wang, Sampling-based sublinear low-rank matrix arith- metic framework for dequantizing quantum machine learn- ing, in Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing -STOC 2020 (ACM Press, 2020) arXiv:1910.06151 [cs.DS].\n\nQuantum-Inspired Classical Algorithms for Singular Value Transformation. D Jethwani, F L Gall, S K Singh, arXiv:1910.0569945th International Symposium on Mathematical Foundations of Computer Science (MFCS 2020), LIPIcs (Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2020. cs.DSD. Jethwani, F. L. Gall, and S. K. Singh, Quantum- Inspired Classical Algorithms for Singular Value Transfor- mation, in 45th International Symposium on Mathematical Foundations of Computer Science (MFCS 2020), LIPIcs (Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2020) arXiv:1910.05699 [cs.DS].\n\nQuantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. A Gily\u00e9n, Y Su, G H Low, N Wiebe, arXiv:1806.01838Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing -STOC 2019. the 51st Annual ACM SIGACT Symposium on Theory of Computing -STOC 2019ACM Pressquant-phA. Gily\u00e9n, Y. Su, G. H. Low, and N. Wiebe, Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics, in Pro- ceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing -STOC 2019 (ACM Press, 2019) arXiv:1806.01838 [quant-ph].\n\nQuantum algorithms for linear algebra and machine learning. A Prakash, UC BerkeleyPh.D. thesisA. Prakash, Quantum algorithms for linear algebra and machine learning., Ph.D. thesis, UC Berkeley (2014).\n\nL Grover, T Rudolph, arXiv:0208112Creating superpositions that correspond to efficiently integrable probability distributions, arXiv. quant-phL. Grover and T. Rudolph, Creating superpositions that correspond to efficiently integrable probability distribu- tions, arXiv (2002), arXiv:0208112 [quant-ph].\n\nMatrix product states represent ground states faithfully. F Verstraete, J I Cirac, Phys. Rev. B. 7394423F. Verstraete and J. I. Cirac, Matrix product states rep- resent ground states faithfully, Phys. Rev. B 73, 094423 (2006).\n\nDensity matrix formulation for quantum renormalization groups. S R White, Phys. Rev. Lett. 692863S. R. White, Density matrix formulation for quantum renormalization groups, Phys. Rev. Lett. 69, 2863 (1992).\n\nEfficient classical simulation of slightly entangled quantum computations. G Vidal, Phys. Rev. Lett. 91147902G. Vidal, Efficient classical simulation of slightly entan- gled quantum computations, Phys. Rev. Lett. 91, 147902 (2003).\n\nEfficient simulation of one-dimensional quantum many-body systems. G Vidal, Phys. Rev. Lett. 9340502G. Vidal, Efficient simulation of one-dimensional quantum many-body systems, Phys. Rev. Lett. 93, 040502 (2004).\n\nTensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions. A Cichocki, N Lee, I Oseledets, A.-H Phan, Q Zhao, D P Mandic, Found. Trends Mach. Learn. 9A. Cichocki, N. Lee, I. Oseledets, A.-H. Phan, Q. Zhao, and D. P. Mandic, Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions, Found. Trends Mach. Learn. 9, 249-429 (2016).\n\nHand-waving and interpretive dance: an introductory course on tensor networks. J C Bridgeman, C T Chubb, Journal of Physics A: Mathematical and Theoretical. 50223001J. C. Bridgeman and C. T. Chubb, Hand-waving and inter- pretive dance: an introductory course on tensor networks, Journal of Physics A: Mathematical and Theoretical 50, 223001 (2017).\n\nComputational difficulty of global variations in the density matrix renormalization group. J Eisert, Phys. Rev. Lett. 97260501J. Eisert, Computational difficulty of global variations in the density matrix renormalization group, Phys. Rev. Lett. 97, 260501 (2006).\n\nA polynomial time algorithm for the ground state of one-dimensional gapped local hamiltonians. Z Landau, U Vazirani, T Vidick, Nature Physics. 11566Z. Landau, U. Vazirani, and T. Vidick, A polynomial time algorithm for the ground state of one-dimensional gapped local hamiltonians, Nature Physics 11, 566 (2015).\n\nW Hoeffding, Probability inequalities for sums of bounded random variables. N. I. Fisher and P. K. SenNew York, New York, NYSpringerThe Collected Works of Wassily HoeffdingW. Hoeffding, Probability inequalities for sums of bounded random variables, in The Collected Works of Wassily Hoeffding, edited by N. I. Fisher and P. K. Sen (Springer New York, New York, NY, 1994) pp. 409-426.\n\nQuantum random access memory. V Giovannetti, S Lloyd, L Maccone, Physical review letters. 100160501V. Giovannetti, S. Lloyd, and L. Maccone, Quantum ran- dom access memory, Physical review letters 100, 160501 (2008).\n\nQuantum computing in the NISQ era and beyond. J , 279J. Preskill, Quantum computing in the NISQ era and beyond, Quantum 2, 79 (2018).\n\nQuantum algorithms for linear algebra and machine learning. A Prakash, UC BerkeleyPh.D. thesisA. Prakash, Quantum algorithms for linear algebra and machine learning., Ph.D. thesis, UC Berkeley (2014).\n\nRead the fine print. S Aaronson, Nature Physics. 11291S. Aaronson, Read the fine print, Nature Physics 11, 291 (2015).\n\nQuantum recommendation systems. I Kerenidis, A Prakash, 8th Innovations in Theoretical Computer Science Conference (ITCS 2017), LIPIcs. 6721I. Kerenidis and A. Prakash, Quantum recommendation systems, in 8th Innovations in Theoretical Computer Sci- ence Conference (ITCS 2017), LIPIcs, Vol. 67 (Schloss Dagstuhl, 2017) pp. 49:1-49:21.\n\nA quantum-inspired classical algorithm for recommendation systems. E Tang, arXiv:1807.04271Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing -STOC 2019. the 51st Annual ACM SIGACT Symposium on Theory of Computing -STOC 2019ACM Presscs.IRE. Tang, A quantum-inspired classical algorithm for rec- ommendation systems, in Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing - STOC 2019 (ACM Press, 2019) arXiv:1807.04271 [cs.IR].\n\nL Grover, T Rudolph, arXiv:0208112Creating superpositions that correspond to efficiently integrable probability distributions, arXiv. quant-phL. Grover and T. Rudolph, Creating superpositions that correspond to efficiently integrable probability distribu- tions, arXiv (2002), arXiv:0208112 [quant-ph].\n\nSimulating quantum computers with probabilistic methods. M Van Den, Nest, Quantum Info. Comput. 11M. Van Den Nest, Simulating quantum computers with probabilistic methods, Quantum Info. Comput. 11 (2011).\n\nMatrix product states represent ground states faithfully. F Verstraete, J I Cirac, Phys. Rev. B. 7394423F. Verstraete and J. I. Cirac, Matrix product states rep- resent ground states faithfully, Phys. Rev. B 73, 094423 (2006).\n\nA practical introduction to tensor networks: Matrix product states and projected entangled pair states. R Or\u00fas, Annals of Physics. 349117R. Or\u00fas, A practical introduction to tensor networks: Matrix product states and projected entangled pair states, Annals of Physics 349, 117 (2014).\n\nDensity matrix formulation for quantum renormalization groups. S R White, Phys. Rev. Lett. 692863S. R. White, Density matrix formulation for quantum renormalization groups, Phys. Rev. Lett. 69, 2863 (1992).\n\nThe density-matrix renormalization group in the age of matrix product states. U Schollw\u00f6ck, Annals of Physics. 32696U. Schollw\u00f6ck, The density-matrix renormalization group in the age of matrix product states, Annals of Physics 326, 96 (2011).\n\nEfficient classical simulation of slightly entangled quantum computations. G Vidal, Phys. Rev. Lett. 91147902G. Vidal, Efficient classical simulation of slightly entan- gled quantum computations, Phys. Rev. Lett. 91, 147902 (2003).\n\nEfficient simulation of one-dimensional quantum many-body systems. G Vidal, Phys. Rev. Lett. 9340502G. Vidal, Efficient simulation of one-dimensional quantum many-body systems, Phys. Rev. Lett. 93, 040502 (2004).\n\nTensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions. A Cichocki, N Lee, I Oseledets, A.-H Phan, Q Zhao, D P Mandic, Found. Trends Mach. Learn. 9A. Cichocki, N. Lee, I. Oseledets, A.-H. Phan, Q. Zhao, and D. P. Mandic, Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions, Found. Trends Mach. Learn. 9, 249-429 (2016).\n\nHand-waving and interpretive dance: an introductory course on tensor networks. J C Bridgeman, C T Chubb, Journal of Physics A: Mathematical and Theoretical. 50223001J. C. Bridgeman and C. T. Chubb, Hand-waving and inter- pretive dance: an introductory course on tensor networks, Journal of Physics A: Mathematical and Theoretical 50, 223001 (2017).\n\nComputational difficulty of global variations in the density matrix renormalization group. J Eisert, Phys. Rev. Lett. 97260501J. Eisert, Computational difficulty of global variations in the density matrix renormalization group, Phys. Rev. Lett. 97, 260501 (2006).\n\nA polynomial time algorithm for the ground state of one-dimensional gapped local hamiltonians. Z Landau, U Vazirani, T Vidick, Nature Physics. 11566Z. Landau, U. Vazirani, and T. Vidick, A polynomial time algorithm for the ground state of one-dimensional gapped local hamiltonians, Nature Physics 11, 566 (2015).\n\nW Hoeffding, Probability inequalities for sums of bounded random variables. N. I. Fisher and P. K. SenNew York, New York, NYSpringerThe Collected Works of Wassily HoeffdingW. Hoeffding, Probability inequalities for sums of bounded random variables, in The Collected Works of Wassily Hoeffding, edited by N. I. Fisher and P. K. Sen (Springer New York, New York, NY, 1994) pp. 409-426.\n\nStrengths and weaknesses of quantum computing. C H Bennett, E Bernstein, G Brassard, U Vazirani, SIAM Journal on Computing. 261510C. H. Bennett, E. Bernstein, G. Brassard, and U. Vazirani, Strengths and weaknesses of quantum computing, SIAM Journal on Computing 26, 1510 (1997).\n\nQuantum principal component analysis. S Lloyd, M Mohseni, P Rebentrost, Nature Physics. 10631S. Lloyd, M. Mohseni, and P. Rebentrost, Quantum prin- cipal component analysis, Nature Physics 10, 631 (2014).\n\nFast Monte-Carlo algorithms for finding low-rank approximations. A Frieze, R Kannan, S Vempala, Journal of the ACM (JACM). 511025A. Frieze, R. Kannan, and S. Vempala, Fast Monte-Carlo algorithms for finding low-rank approximations, Journal of the ACM (JACM) 51, 1025 (2004).\n", "annotations": {"author": "[{\"end\":183,\"start\":146}]", "publisher": null, "author_last_name": "[{\"end\":155,\"start\":151}]", "author_first_name": "[{\"end\":150,\"start\":146}]", "author_affiliation": "[{\"end\":182,\"start\":157}]", "title": "[{\"end\":119,\"start\":1},{\"end\":302,\"start\":184}]", "venue": null, "abstract": "[{\"end\":1330,\"start\":328}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1556,\"start\":1553},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1826,\"start\":1823},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1829,\"start\":1826},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1832,\"start\":1829},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1835,\"start\":1832},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1838,\"start\":1835},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2037,\"start\":2034},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2039,\"start\":2037},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2072,\"start\":2069},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2380,\"start\":2376},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2496,\"start\":2492},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3079,\"start\":3076},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3455,\"start\":3452},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4221,\"start\":4217},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4260,\"start\":4256},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5721,\"start\":5718},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5765,\"start\":5761},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8482,\"start\":8479},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8877,\"start\":8873},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9014,\"start\":9011},{\"end\":9030,\"start\":9014},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9158,\"start\":9154},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9718,\"start\":9714},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9912,\"start\":9908},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9993,\"start\":9990},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10208,\"start\":10204},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10231,\"start\":10227},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10388,\"start\":10384},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10462,\"start\":10459},{\"end\":10484,\"start\":10480},{\"end\":10494,\"start\":10484},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11284,\"start\":11280},{\"end\":12750,\"start\":12746},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13483,\"start\":13479},{\"end\":13525,\"start\":13521},{\"end\":13528,\"start\":13525},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13800,\"start\":13796},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14791,\"start\":14787},{\"end\":14794,\"start\":14791},{\"end\":14807,\"start\":14794},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14810,\"start\":14807},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15046,\"start\":15043},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15496,\"start\":15492},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16686,\"start\":16683},{\"end\":19099,\"start\":19089},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19975,\"start\":19971},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19979,\"start\":19975},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19983,\"start\":19979},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20476,\"start\":20472},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20728,\"start\":20724},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20800,\"start\":20796},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21051,\"start\":21047},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22004,\"start\":22000},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22200,\"start\":22196},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22203,\"start\":22200},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":23054,\"start\":23050},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23168,\"start\":23164},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23172,\"start\":23168},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23176,\"start\":23172},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23180,\"start\":23176},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23215,\"start\":23211},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23305,\"start\":23301},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24662,\"start\":24659},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24665,\"start\":24662},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24668,\"start\":24665},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24671,\"start\":24668},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24727,\"start\":24724},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25175,\"start\":25172},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27244,\"start\":27241},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27717,\"start\":27714},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27720,\"start\":27717},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27960,\"start\":27956},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27963,\"start\":27960},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28010,\"start\":28006},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28013,\"start\":28010},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28221,\"start\":28217},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28658,\"start\":28654},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":28661,\"start\":28658},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28880,\"start\":28876},{\"end\":31632,\"start\":31628},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32343,\"start\":32340},{\"end\":33801,\"start\":33797},{\"end\":34904,\"start\":34900},{\"end\":36254,\"start\":36250},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":36347,\"start\":36344},{\"end\":36521,\"start\":36517},{\"end\":38236,\"start\":38232},{\"end\":38606,\"start\":38602},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":39103,\"start\":39100},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":39158,\"start\":39155},{\"end\":39360,\"start\":39356},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":40672,\"start\":40669},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":41018,\"start\":41015},{\"end\":41034,\"start\":41018},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":42976,\"start\":42973},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":43027,\"start\":43024},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":43119,\"start\":43116},{\"end\":46252,\"start\":46249}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":49187,\"start\":48764},{\"attributes\":{\"id\":\"fig_2\"},\"end\":49800,\"start\":49188},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50171,\"start\":49801},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50350,\"start\":50172},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50401,\"start\":50351},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":53815,\"start\":50402}]", "paragraph": "[{\"end\":2698,\"start\":1346},{\"end\":3328,\"start\":2700},{\"end\":3456,\"start\":3330},{\"end\":3683,\"start\":3458},{\"end\":5304,\"start\":3685},{\"end\":6315,\"start\":5306},{\"end\":6326,\"start\":6317},{\"end\":6469,\"start\":6328},{\"end\":6556,\"start\":6471},{\"end\":6881,\"start\":6792},{\"end\":7053,\"start\":6937},{\"end\":7853,\"start\":7082},{\"end\":8364,\"start\":7855},{\"end\":8721,\"start\":8366},{\"end\":9259,\"start\":8723},{\"end\":10024,\"start\":9261},{\"end\":10651,\"start\":10060},{\"end\":10806,\"start\":10653},{\"end\":11049,\"start\":10808},{\"end\":11242,\"start\":11086},{\"end\":11461,\"start\":11244},{\"end\":11841,\"start\":11463},{\"end\":11960,\"start\":11843},{\"end\":12148,\"start\":12001},{\"end\":12599,\"start\":12220},{\"end\":12850,\"start\":12601},{\"end\":13160,\"start\":12968},{\"end\":13341,\"start\":13162},{\"end\":13801,\"start\":13374},{\"end\":14074,\"start\":13803},{\"end\":14311,\"start\":14076},{\"end\":14771,\"start\":14521},{\"end\":14934,\"start\":14773},{\"end\":16092,\"start\":14936},{\"end\":16822,\"start\":16280},{\"end\":16966,\"start\":16824},{\"end\":17350,\"start\":17303},{\"end\":17648,\"start\":17357},{\"end\":18325,\"start\":18035},{\"end\":18365,\"start\":18327},{\"end\":18511,\"start\":18442},{\"end\":19394,\"start\":18812},{\"end\":20930,\"start\":19396},{\"end\":21845,\"start\":20932},{\"end\":23055,\"start\":21847},{\"end\":23497,\"start\":23057},{\"end\":23991,\"start\":23536},{\"end\":24452,\"start\":23993},{\"end\":24963,\"start\":24454},{\"end\":25406,\"start\":24965},{\"end\":25835,\"start\":25408},{\"end\":26094,\"start\":25837},{\"end\":26981,\"start\":26096},{\"end\":27341,\"start\":27013},{\"end\":28881,\"start\":27343},{\"end\":30385,\"start\":28883},{\"end\":31058,\"start\":30387},{\"end\":32081,\"start\":31060},{\"end\":32762,\"start\":32129},{\"end\":33802,\"start\":32764},{\"end\":34235,\"start\":33922},{\"end\":34321,\"start\":34237},{\"end\":34514,\"start\":34421},{\"end\":34905,\"start\":34516},{\"end\":35014,\"start\":34928},{\"end\":35446,\"start\":35016},{\"end\":35806,\"start\":35571},{\"end\":36211,\"start\":35991},{\"end\":36323,\"start\":36213},{\"end\":36664,\"start\":36325},{\"end\":37138,\"start\":36666},{\"end\":37691,\"start\":37140},{\"end\":37906,\"start\":37738},{\"end\":38237,\"start\":37936},{\"end\":38518,\"start\":38239},{\"end\":39294,\"start\":38567},{\"end\":39463,\"start\":39296},{\"end\":39829,\"start\":39662},{\"end\":40041,\"start\":39981},{\"end\":40582,\"start\":40043},{\"end\":40926,\"start\":40584},{\"end\":41263,\"start\":40984},{\"end\":41528,\"start\":41493},{\"end\":41826,\"start\":41658},{\"end\":42395,\"start\":41833},{\"end\":42508,\"start\":42429},{\"end\":42681,\"start\":42510},{\"end\":42930,\"start\":42683},{\"end\":43207,\"start\":42932},{\"end\":43227,\"start\":43209},{\"end\":43868,\"start\":43272},{\"end\":44765,\"start\":44035},{\"end\":44970,\"start\":44767},{\"end\":45949,\"start\":45107},{\"end\":46420,\"start\":46101},{\"end\":46614,\"start\":46444},{\"end\":46864,\"start\":46729},{\"end\":47657,\"start\":47383},{\"end\":47761,\"start\":47659},{\"end\":48213,\"start\":48098},{\"end\":48347,\"start\":48337},{\"end\":48531,\"start\":48349},{\"end\":48542,\"start\":48533},{\"end\":48559,\"start\":48544},{\"end\":48763,\"start\":48700}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6791,\"start\":6557},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6936,\"start\":6882},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11085,\"start\":11050},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12219,\"start\":12149},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12967,\"start\":12851},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14520,\"start\":14312},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16279,\"start\":16135},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17302,\"start\":17005},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18034,\"start\":17649},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18441,\"start\":18366},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18811,\"start\":18512},{\"attributes\":{\"id\":\"formula_11\"},\"end\":33921,\"start\":33803},{\"attributes\":{\"id\":\"formula_12\"},\"end\":34420,\"start\":34322},{\"attributes\":{\"id\":\"formula_13\"},\"end\":35570,\"start\":35447},{\"attributes\":{\"id\":\"formula_14\"},\"end\":35990,\"start\":35807},{\"attributes\":{\"id\":\"formula_15\"},\"end\":37718,\"start\":37692},{\"attributes\":{\"id\":\"formula_16\"},\"end\":37737,\"start\":37718},{\"attributes\":{\"id\":\"formula_17\"},\"end\":37935,\"start\":37907},{\"attributes\":{\"id\":\"formula_18\"},\"end\":39661,\"start\":39464},{\"attributes\":{\"id\":\"formula_19\"},\"end\":39980,\"start\":39830},{\"attributes\":{\"id\":\"formula_20\"},\"end\":40983,\"start\":40927},{\"attributes\":{\"id\":\"formula_21\"},\"end\":41467,\"start\":41264},{\"attributes\":{\"id\":\"formula_22\"},\"end\":41657,\"start\":41529},{\"attributes\":{\"id\":\"formula_23\"},\"end\":42428,\"start\":42396},{\"attributes\":{\"id\":\"formula_24\"},\"end\":44034,\"start\":43869},{\"attributes\":{\"id\":\"formula_25\"},\"end\":45106,\"start\":44971},{\"attributes\":{\"id\":\"formula_26\"},\"end\":46100,\"start\":45950},{\"attributes\":{\"id\":\"formula_27\"},\"end\":46443,\"start\":46421},{\"attributes\":{\"id\":\"formula_28\"},\"end\":46728,\"start\":46615},{\"attributes\":{\"id\":\"formula_29\"},\"end\":47382,\"start\":46865},{\"attributes\":{\"id\":\"formula_30\"},\"end\":48097,\"start\":47762},{\"attributes\":{\"id\":\"formula_31\"},\"end\":48336,\"start\":48214},{\"attributes\":{\"id\":\"formula_32\"},\"end\":48699,\"start\":48560}]", "table_ref": null, "section_header": "[{\"end\":1344,\"start\":1332},{\"end\":7080,\"start\":7056},{\"end\":10058,\"start\":10027},{\"end\":11999,\"start\":11963},{\"end\":13372,\"start\":13344},{\"end\":16134,\"start\":16095},{\"end\":17004,\"start\":16969},{\"end\":17355,\"start\":17353},{\"end\":23534,\"start\":23500},{\"end\":27011,\"start\":26984},{\"end\":32127,\"start\":32084},{\"end\":34926,\"start\":34908},{\"end\":38565,\"start\":38521},{\"end\":41491,\"start\":41469},{\"end\":41831,\"start\":41829},{\"end\":43270,\"start\":43230},{\"end\":49200,\"start\":49189},{\"end\":49807,\"start\":49802},{\"end\":50355,\"start\":50352}]", "table": "[{\"end\":53815,\"start\":51104}]", "figure_caption": "[{\"end\":49187,\"start\":48766},{\"end\":49800,\"start\":49202},{\"end\":50171,\"start\":49808},{\"end\":50350,\"start\":50174},{\"end\":50401,\"start\":50357},{\"end\":51104,\"start\":50404}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":53948,\"start\":53947},{\"end\":53957,\"start\":53956},{\"end\":53967,\"start\":53966},{\"end\":53969,\"start\":53968},{\"end\":54253,\"start\":54252},{\"end\":54255,\"start\":54254},{\"end\":54266,\"start\":54265},{\"end\":54279,\"start\":54278},{\"end\":54291,\"start\":54290},{\"end\":54512,\"start\":54511},{\"end\":54524,\"start\":54523},{\"end\":54534,\"start\":54533},{\"end\":54546,\"start\":54545},{\"end\":54560,\"start\":54559},{\"end\":54569,\"start\":54568},{\"end\":54772,\"start\":54771},{\"end\":54785,\"start\":54784},{\"end\":54797,\"start\":54796},{\"end\":54799,\"start\":54798},{\"end\":54810,\"start\":54809},{\"end\":54820,\"start\":54819},{\"end\":54833,\"start\":54832},{\"end\":54845,\"start\":54844},{\"end\":55316,\"start\":55315},{\"end\":55331,\"start\":55330},{\"end\":55341,\"start\":55340},{\"end\":55851,\"start\":55850},{\"end\":55861,\"start\":55860},{\"end\":55871,\"start\":55870},{\"end\":56107,\"start\":56106},{\"end\":56109,\"start\":56108},{\"end\":56339,\"start\":56338},{\"end\":56341,\"start\":56340},{\"end\":56637,\"start\":56636},{\"end\":56647,\"start\":56646},{\"end\":56825,\"start\":56824},{\"end\":56827,\"start\":56826},{\"end\":56839,\"start\":56838},{\"end\":56850,\"start\":56849},{\"end\":56852,\"start\":56851},{\"end\":56863,\"start\":56862},{\"end\":57158,\"start\":57157},{\"end\":57367,\"start\":57366},{\"end\":57375,\"start\":57374},{\"end\":57386,\"start\":57385},{\"end\":57399,\"start\":57398},{\"end\":57412,\"start\":57411},{\"end\":57422,\"start\":57421},{\"end\":57731,\"start\":57730},{\"end\":58001,\"start\":58000},{\"end\":58305,\"start\":58301},{\"end\":58313,\"start\":58312},{\"end\":58326,\"start\":58322},{\"end\":58333,\"start\":58332},{\"end\":58342,\"start\":58341},{\"end\":58350,\"start\":58349},{\"end\":58903,\"start\":58899},{\"end\":58911,\"start\":58910},{\"end\":58920,\"start\":58916},{\"end\":58927,\"start\":58926},{\"end\":59524,\"start\":59520},{\"end\":59532,\"start\":59531},{\"end\":59542,\"start\":59541},{\"end\":59551,\"start\":59547},{\"end\":59558,\"start\":59557},{\"end\":59566,\"start\":59565},{\"end\":60140,\"start\":60139},{\"end\":60152,\"start\":60151},{\"end\":60154,\"start\":60153},{\"end\":60162,\"start\":60161},{\"end\":60164,\"start\":60163},{\"end\":60755,\"start\":60754},{\"end\":60765,\"start\":60764},{\"end\":60771,\"start\":60770},{\"end\":60773,\"start\":60772},{\"end\":60780,\"start\":60779},{\"end\":61328,\"start\":61327},{\"end\":61470,\"start\":61469},{\"end\":61480,\"start\":61479},{\"end\":61832,\"start\":61831},{\"end\":61846,\"start\":61845},{\"end\":61848,\"start\":61847},{\"end\":62065,\"start\":62064},{\"end\":62067,\"start\":62066},{\"end\":62285,\"start\":62284},{\"end\":62510,\"start\":62509},{\"end\":62771,\"start\":62770},{\"end\":62783,\"start\":62782},{\"end\":62790,\"start\":62789},{\"end\":62806,\"start\":62802},{\"end\":62814,\"start\":62813},{\"end\":62822,\"start\":62821},{\"end\":62824,\"start\":62823},{\"end\":63176,\"start\":63175},{\"end\":63178,\"start\":63177},{\"end\":63191,\"start\":63190},{\"end\":63193,\"start\":63192},{\"end\":63538,\"start\":63537},{\"end\":63807,\"start\":63806},{\"end\":63817,\"start\":63816},{\"end\":63829,\"start\":63828},{\"end\":64026,\"start\":64025},{\"end\":64441,\"start\":64440},{\"end\":64456,\"start\":64455},{\"end\":64465,\"start\":64464},{\"end\":64675,\"start\":64674},{\"end\":64824,\"start\":64823},{\"end\":64987,\"start\":64986},{\"end\":65118,\"start\":65117},{\"end\":65131,\"start\":65130},{\"end\":65489,\"start\":65488},{\"end\":65897,\"start\":65896},{\"end\":65907,\"start\":65906},{\"end\":66258,\"start\":66257},{\"end\":66465,\"start\":66464},{\"end\":66479,\"start\":66478},{\"end\":66481,\"start\":66480},{\"end\":66739,\"start\":66738},{\"end\":66984,\"start\":66983},{\"end\":66986,\"start\":66985},{\"end\":67207,\"start\":67206},{\"end\":67448,\"start\":67447},{\"end\":67673,\"start\":67672},{\"end\":67934,\"start\":67933},{\"end\":67946,\"start\":67945},{\"end\":67953,\"start\":67952},{\"end\":67969,\"start\":67965},{\"end\":67977,\"start\":67976},{\"end\":67985,\"start\":67984},{\"end\":67987,\"start\":67986},{\"end\":68339,\"start\":68338},{\"end\":68341,\"start\":68340},{\"end\":68354,\"start\":68353},{\"end\":68356,\"start\":68355},{\"end\":68701,\"start\":68700},{\"end\":68970,\"start\":68969},{\"end\":68980,\"start\":68979},{\"end\":68992,\"start\":68991},{\"end\":69189,\"start\":69188},{\"end\":69621,\"start\":69620},{\"end\":69623,\"start\":69622},{\"end\":69634,\"start\":69633},{\"end\":69647,\"start\":69646},{\"end\":69659,\"start\":69658},{\"end\":69892,\"start\":69891},{\"end\":69901,\"start\":69900},{\"end\":69912,\"start\":69911},{\"end\":70125,\"start\":70124},{\"end\":70135,\"start\":70134},{\"end\":70145,\"start\":70144}]", "bib_author_last_name": "[{\"end\":53954,\"start\":53949},{\"end\":53964,\"start\":53958},{\"end\":53975,\"start\":53970},{\"end\":54263,\"start\":54256},{\"end\":54276,\"start\":54267},{\"end\":54288,\"start\":54280},{\"end\":54300,\"start\":54292},{\"end\":54521,\"start\":54513},{\"end\":54531,\"start\":54525},{\"end\":54543,\"start\":54535},{\"end\":54557,\"start\":54547},{\"end\":54566,\"start\":54561},{\"end\":54575,\"start\":54570},{\"end\":54782,\"start\":54773},{\"end\":54794,\"start\":54786},{\"end\":54807,\"start\":54800},{\"end\":54817,\"start\":54811},{\"end\":54830,\"start\":54821},{\"end\":54842,\"start\":54834},{\"end\":54853,\"start\":54846},{\"end\":55328,\"start\":55317},{\"end\":55338,\"start\":55332},{\"end\":55349,\"start\":55342},{\"end\":55858,\"start\":55852},{\"end\":55868,\"start\":55862},{\"end\":55879,\"start\":55872},{\"end\":56117,\"start\":56110},{\"end\":56350,\"start\":56342},{\"end\":56644,\"start\":56638},{\"end\":56655,\"start\":56648},{\"end\":56836,\"start\":56828},{\"end\":56847,\"start\":56840},{\"end\":56860,\"start\":56853},{\"end\":56869,\"start\":56864},{\"end\":57166,\"start\":57159},{\"end\":57172,\"start\":57168},{\"end\":57372,\"start\":57368},{\"end\":57383,\"start\":57376},{\"end\":57396,\"start\":57387},{\"end\":57409,\"start\":57400},{\"end\":57419,\"start\":57413},{\"end\":57431,\"start\":57423},{\"end\":57742,\"start\":57732},{\"end\":58006,\"start\":58002},{\"end\":58310,\"start\":58306},{\"end\":58320,\"start\":58314},{\"end\":58330,\"start\":58327},{\"end\":58339,\"start\":58334},{\"end\":58347,\"start\":58343},{\"end\":58355,\"start\":58351},{\"end\":58908,\"start\":58904},{\"end\":58914,\"start\":58912},{\"end\":58924,\"start\":58921},{\"end\":58932,\"start\":58928},{\"end\":59529,\"start\":59525},{\"end\":59539,\"start\":59533},{\"end\":59545,\"start\":59543},{\"end\":59555,\"start\":59552},{\"end\":59563,\"start\":59559},{\"end\":59571,\"start\":59567},{\"end\":60149,\"start\":60141},{\"end\":60159,\"start\":60155},{\"end\":60170,\"start\":60165},{\"end\":60762,\"start\":60756},{\"end\":60768,\"start\":60766},{\"end\":60777,\"start\":60774},{\"end\":60786,\"start\":60781},{\"end\":61336,\"start\":61329},{\"end\":61477,\"start\":61471},{\"end\":61488,\"start\":61481},{\"end\":61843,\"start\":61833},{\"end\":61854,\"start\":61849},{\"end\":62073,\"start\":62068},{\"end\":62291,\"start\":62286},{\"end\":62516,\"start\":62511},{\"end\":62780,\"start\":62772},{\"end\":62787,\"start\":62784},{\"end\":62800,\"start\":62791},{\"end\":62811,\"start\":62807},{\"end\":62819,\"start\":62815},{\"end\":62831,\"start\":62825},{\"end\":63188,\"start\":63179},{\"end\":63199,\"start\":63194},{\"end\":63545,\"start\":63539},{\"end\":63814,\"start\":63808},{\"end\":63826,\"start\":63818},{\"end\":63836,\"start\":63830},{\"end\":64036,\"start\":64027},{\"end\":64453,\"start\":64442},{\"end\":64462,\"start\":64457},{\"end\":64473,\"start\":64466},{\"end\":64832,\"start\":64825},{\"end\":64996,\"start\":64988},{\"end\":65128,\"start\":65119},{\"end\":65139,\"start\":65132},{\"end\":65494,\"start\":65490},{\"end\":65904,\"start\":65898},{\"end\":65915,\"start\":65908},{\"end\":66266,\"start\":66259},{\"end\":66272,\"start\":66268},{\"end\":66476,\"start\":66466},{\"end\":66487,\"start\":66482},{\"end\":66744,\"start\":66740},{\"end\":66992,\"start\":66987},{\"end\":67218,\"start\":67208},{\"end\":67454,\"start\":67449},{\"end\":67679,\"start\":67674},{\"end\":67943,\"start\":67935},{\"end\":67950,\"start\":67947},{\"end\":67963,\"start\":67954},{\"end\":67974,\"start\":67970},{\"end\":67982,\"start\":67978},{\"end\":67994,\"start\":67988},{\"end\":68351,\"start\":68342},{\"end\":68362,\"start\":68357},{\"end\":68708,\"start\":68702},{\"end\":68977,\"start\":68971},{\"end\":68989,\"start\":68981},{\"end\":68999,\"start\":68993},{\"end\":69199,\"start\":69190},{\"end\":69631,\"start\":69624},{\"end\":69644,\"start\":69635},{\"end\":69656,\"start\":69648},{\"end\":69668,\"start\":69660},{\"end\":69898,\"start\":69893},{\"end\":69909,\"start\":69902},{\"end\":69923,\"start\":69913},{\"end\":70132,\"start\":70126},{\"end\":70142,\"start\":70136},{\"end\":70153,\"start\":70146}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":37339559},\"end\":54203,\"start\":53857},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13403194},\"end\":54483,\"start\":54205},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":25062002},\"end\":54718,\"start\":54485},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3306944},\"end\":55205,\"start\":54720},{\"attributes\":{\"doi\":\"arXiv:1804.01973\",\"id\":\"b4\",\"matched_paper_id\":4614529},\"end\":55783,\"start\":55207},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2483891},\"end\":56059,\"start\":55785},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":47503887},\"end\":56286,\"start\":56061},{\"attributes\":{\"doi\":\"10.1561/0400000060\",\"id\":\"b7\",\"matched_paper_id\":51783444},\"end\":56583,\"start\":56288},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":26830284},\"end\":56781,\"start\":56585},{\"attributes\":{\"doi\":\"Quan-tum10.22331/q-2020-08-13-307\",\"id\":\"b9\"},\"end\":57098,\"start\":56783},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":16943113},\"end\":57304,\"start\":57100},{\"attributes\":{\"id\":\"b11\"},\"end\":57650,\"start\":57306},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":118735367},\"end\":57894,\"start\":57652},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":118349602},\"end\":58180,\"start\":57896},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":227276230},\"end\":58813,\"start\":58182},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":221005852},\"end\":59413,\"start\":58815},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":204509632},\"end\":60064,\"start\":59415},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":204509006},\"end\":60645,\"start\":60066},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":46941335},\"end\":61265,\"start\":60647},{\"attributes\":{\"id\":\"b19\"},\"end\":61467,\"start\":61267},{\"attributes\":{\"id\":\"b20\"},\"end\":61771,\"start\":61469},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":16329119},\"end\":61999,\"start\":61773},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9665382},\"end\":62207,\"start\":62001},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":15188855},\"end\":62440,\"start\":62209},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":30670203},\"end\":62654,\"start\":62442},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":125410214},\"end\":63094,\"start\":62656},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3483562},\"end\":63444,\"start\":63096},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":24011290},\"end\":63709,\"start\":63446},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5369932},\"end\":64023,\"start\":63711},{\"attributes\":{\"id\":\"b29\"},\"end\":64408,\"start\":64025},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":570390},\"end\":64626,\"start\":64410},{\"attributes\":{\"id\":\"b31\"},\"end\":64761,\"start\":64628},{\"attributes\":{\"id\":\"b32\"},\"end\":64963,\"start\":64763},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2246294},\"end\":65083,\"start\":64965},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":579463},\"end\":65419,\"start\":65085},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":44036160},\"end\":65894,\"start\":65421},{\"attributes\":{\"id\":\"b36\"},\"end\":66198,\"start\":65896},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":16943113},\"end\":66404,\"start\":66200},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":16329119},\"end\":66632,\"start\":66406},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":118349602},\"end\":66918,\"start\":66634},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9665382},\"end\":67126,\"start\":66920},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":118735367},\"end\":67370,\"start\":67128},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":15188855},\"end\":67603,\"start\":67372},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":30670203},\"end\":67817,\"start\":67605},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":125410214},\"end\":68257,\"start\":67819},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3483562},\"end\":68607,\"start\":68259},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":24011290},\"end\":68872,\"start\":68609},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":5369932},\"end\":69186,\"start\":68874},{\"attributes\":{\"id\":\"b48\"},\"end\":69571,\"start\":69188},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":13403194},\"end\":69851,\"start\":69573},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":5589954},\"end\":70057,\"start\":69853},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":2483891},\"end\":70333,\"start\":70059}]", "bib_title": "[{\"end\":53945,\"start\":53857},{\"end\":54250,\"start\":54205},{\"end\":54509,\"start\":54485},{\"end\":54769,\"start\":54720},{\"end\":55313,\"start\":55207},{\"end\":55848,\"start\":55785},{\"end\":56104,\"start\":56061},{\"end\":56336,\"start\":56288},{\"end\":56634,\"start\":56585},{\"end\":57155,\"start\":57100},{\"end\":57728,\"start\":57652},{\"end\":57998,\"start\":57896},{\"end\":58299,\"start\":58182},{\"end\":58897,\"start\":58815},{\"end\":59518,\"start\":59415},{\"end\":60137,\"start\":60066},{\"end\":60752,\"start\":60647},{\"end\":61829,\"start\":61773},{\"end\":62062,\"start\":62001},{\"end\":62282,\"start\":62209},{\"end\":62507,\"start\":62442},{\"end\":62768,\"start\":62656},{\"end\":63173,\"start\":63096},{\"end\":63535,\"start\":63446},{\"end\":63804,\"start\":63711},{\"end\":64438,\"start\":64410},{\"end\":64984,\"start\":64965},{\"end\":65115,\"start\":65085},{\"end\":65486,\"start\":65421},{\"end\":66255,\"start\":66200},{\"end\":66462,\"start\":66406},{\"end\":66736,\"start\":66634},{\"end\":66981,\"start\":66920},{\"end\":67204,\"start\":67128},{\"end\":67445,\"start\":67372},{\"end\":67670,\"start\":67605},{\"end\":67931,\"start\":67819},{\"end\":68336,\"start\":68259},{\"end\":68698,\"start\":68609},{\"end\":68967,\"start\":68874},{\"end\":69618,\"start\":69573},{\"end\":69889,\"start\":69853},{\"end\":70122,\"start\":70059}]", "bib_author": "[{\"end\":53956,\"start\":53947},{\"end\":53966,\"start\":53956},{\"end\":53977,\"start\":53966},{\"end\":54265,\"start\":54252},{\"end\":54278,\"start\":54265},{\"end\":54290,\"start\":54278},{\"end\":54302,\"start\":54290},{\"end\":54523,\"start\":54511},{\"end\":54533,\"start\":54523},{\"end\":54545,\"start\":54533},{\"end\":54559,\"start\":54545},{\"end\":54568,\"start\":54559},{\"end\":54577,\"start\":54568},{\"end\":54784,\"start\":54771},{\"end\":54796,\"start\":54784},{\"end\":54809,\"start\":54796},{\"end\":54819,\"start\":54809},{\"end\":54832,\"start\":54819},{\"end\":54844,\"start\":54832},{\"end\":54855,\"start\":54844},{\"end\":55330,\"start\":55315},{\"end\":55340,\"start\":55330},{\"end\":55351,\"start\":55340},{\"end\":55860,\"start\":55850},{\"end\":55870,\"start\":55860},{\"end\":55881,\"start\":55870},{\"end\":56119,\"start\":56106},{\"end\":56352,\"start\":56338},{\"end\":56646,\"start\":56636},{\"end\":56657,\"start\":56646},{\"end\":56838,\"start\":56824},{\"end\":56849,\"start\":56838},{\"end\":56862,\"start\":56849},{\"end\":56871,\"start\":56862},{\"end\":57168,\"start\":57157},{\"end\":57174,\"start\":57168},{\"end\":57374,\"start\":57366},{\"end\":57385,\"start\":57374},{\"end\":57398,\"start\":57385},{\"end\":57411,\"start\":57398},{\"end\":57421,\"start\":57411},{\"end\":57433,\"start\":57421},{\"end\":57744,\"start\":57730},{\"end\":58008,\"start\":58000},{\"end\":58312,\"start\":58301},{\"end\":58322,\"start\":58312},{\"end\":58332,\"start\":58322},{\"end\":58341,\"start\":58332},{\"end\":58349,\"start\":58341},{\"end\":58357,\"start\":58349},{\"end\":58910,\"start\":58899},{\"end\":58916,\"start\":58910},{\"end\":58926,\"start\":58916},{\"end\":58934,\"start\":58926},{\"end\":59531,\"start\":59520},{\"end\":59541,\"start\":59531},{\"end\":59547,\"start\":59541},{\"end\":59557,\"start\":59547},{\"end\":59565,\"start\":59557},{\"end\":59573,\"start\":59565},{\"end\":60151,\"start\":60139},{\"end\":60161,\"start\":60151},{\"end\":60172,\"start\":60161},{\"end\":60764,\"start\":60754},{\"end\":60770,\"start\":60764},{\"end\":60779,\"start\":60770},{\"end\":60788,\"start\":60779},{\"end\":61338,\"start\":61327},{\"end\":61479,\"start\":61469},{\"end\":61490,\"start\":61479},{\"end\":61845,\"start\":61831},{\"end\":61856,\"start\":61845},{\"end\":62075,\"start\":62064},{\"end\":62293,\"start\":62284},{\"end\":62518,\"start\":62509},{\"end\":62782,\"start\":62770},{\"end\":62789,\"start\":62782},{\"end\":62802,\"start\":62789},{\"end\":62813,\"start\":62802},{\"end\":62821,\"start\":62813},{\"end\":62833,\"start\":62821},{\"end\":63190,\"start\":63175},{\"end\":63201,\"start\":63190},{\"end\":63547,\"start\":63537},{\"end\":63816,\"start\":63806},{\"end\":63828,\"start\":63816},{\"end\":63838,\"start\":63828},{\"end\":64038,\"start\":64025},{\"end\":64455,\"start\":64440},{\"end\":64464,\"start\":64455},{\"end\":64475,\"start\":64464},{\"end\":64678,\"start\":64674},{\"end\":64834,\"start\":64823},{\"end\":64998,\"start\":64986},{\"end\":65130,\"start\":65117},{\"end\":65141,\"start\":65130},{\"end\":65496,\"start\":65488},{\"end\":65906,\"start\":65896},{\"end\":65917,\"start\":65906},{\"end\":66268,\"start\":66257},{\"end\":66274,\"start\":66268},{\"end\":66478,\"start\":66464},{\"end\":66489,\"start\":66478},{\"end\":66746,\"start\":66738},{\"end\":66994,\"start\":66983},{\"end\":67220,\"start\":67206},{\"end\":67456,\"start\":67447},{\"end\":67681,\"start\":67672},{\"end\":67945,\"start\":67933},{\"end\":67952,\"start\":67945},{\"end\":67965,\"start\":67952},{\"end\":67976,\"start\":67965},{\"end\":67984,\"start\":67976},{\"end\":67996,\"start\":67984},{\"end\":68353,\"start\":68338},{\"end\":68364,\"start\":68353},{\"end\":68710,\"start\":68700},{\"end\":68979,\"start\":68969},{\"end\":68991,\"start\":68979},{\"end\":69001,\"start\":68991},{\"end\":69201,\"start\":69188},{\"end\":69633,\"start\":69620},{\"end\":69646,\"start\":69633},{\"end\":69658,\"start\":69646},{\"end\":69670,\"start\":69658},{\"end\":69900,\"start\":69891},{\"end\":69911,\"start\":69900},{\"end\":69925,\"start\":69911},{\"end\":70134,\"start\":70124},{\"end\":70144,\"start\":70134},{\"end\":70155,\"start\":70144}]", "bib_venue": "[{\"end\":54012,\"start\":53977},{\"end\":54327,\"start\":54302},{\"end\":54583,\"start\":54577},{\"end\":54938,\"start\":54855},{\"end\":55475,\"start\":55367},{\"end\":55906,\"start\":55881},{\"end\":56162,\"start\":56119},{\"end\":56425,\"start\":56370},{\"end\":56670,\"start\":56657},{\"end\":56822,\"start\":56783},{\"end\":57194,\"start\":57174},{\"end\":57364,\"start\":57306},{\"end\":57761,\"start\":57744},{\"end\":58025,\"start\":58008},{\"end\":58485,\"start\":58357},{\"end\":59095,\"start\":58950},{\"end\":59674,\"start\":59589},{\"end\":60339,\"start\":60188},{\"end\":60889,\"start\":60804},{\"end\":61325,\"start\":61267},{\"end\":61601,\"start\":61503},{\"end\":61868,\"start\":61856},{\"end\":62090,\"start\":62075},{\"end\":62308,\"start\":62293},{\"end\":62533,\"start\":62518},{\"end\":62858,\"start\":62833},{\"end\":63251,\"start\":63201},{\"end\":63562,\"start\":63547},{\"end\":63852,\"start\":63838},{\"end\":64099,\"start\":64038},{\"end\":64498,\"start\":64475},{\"end\":64672,\"start\":64628},{\"end\":64821,\"start\":64763},{\"end\":65012,\"start\":64998},{\"end\":65219,\"start\":65141},{\"end\":65597,\"start\":65512},{\"end\":66028,\"start\":65930},{\"end\":66294,\"start\":66274},{\"end\":66501,\"start\":66489},{\"end\":66763,\"start\":66746},{\"end\":67009,\"start\":66994},{\"end\":67237,\"start\":67220},{\"end\":67471,\"start\":67456},{\"end\":67696,\"start\":67681},{\"end\":68021,\"start\":67996},{\"end\":68414,\"start\":68364},{\"end\":68725,\"start\":68710},{\"end\":69015,\"start\":69001},{\"end\":69262,\"start\":69201},{\"end\":69695,\"start\":69670},{\"end\":69939,\"start\":69925},{\"end\":70180,\"start\":70155},{\"end\":59746,\"start\":59676},{\"end\":60961,\"start\":60891},{\"end\":64149,\"start\":64127},{\"end\":65669,\"start\":65599},{\"end\":69312,\"start\":69290}]"}}}, "year": 2023, "month": 12, "day": 17}