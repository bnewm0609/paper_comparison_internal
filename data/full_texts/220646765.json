{"id": 220646765, "updated": "2023-10-06 13:28:06.116", "metadata": {"title": "Deep Anomaly Detection for Time-series Data in Industrial IoT: A Communication-Efficient On-device Federated Learning Approach", "authors": "[{\"first\":\"Yi\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Sahil\",\"last\":\"Garg\",\"middle\":[]},{\"first\":\"Jiangtian\",\"last\":\"Nie\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zehui\",\"last\":\"Xiong\",\"middle\":[]},{\"first\":\"Jiawen\",\"last\":\"Kang\",\"middle\":[]},{\"first\":\"M.\",\"last\":\"Hossain\",\"middle\":[\"Shamim\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 7, "day": 19}, "abstract": "Since edge device failures (i.e., anomalies) seriously affect the production of industrial products in Industrial IoT (IIoT), accurately and timely detecting anomalies is becoming increasingly important. Furthermore, data collected by the edge device may contain the user's private data, which is challenging the current detection approaches as user privacy is calling for the public concern in recent years. With this focus, this paper proposes a new communication-efficient on-device federated learning (FL)-based deep anomaly detection framework for sensing time-series data in IIoT. Specifically, we first introduce a FL framework to enable decentralized edge devices to collaboratively train an anomaly detection model, which can improve its generalization ability. Second, we propose an Attention Mechanism-based Convolutional Neural Network-Long Short Term Memory (AMCNN-LSTM) model to accurately detect anomalies. The AMCNN-LSTM model uses attention mechanism-based CNN units to capture important fine-grained features, thereby preventing memory loss and gradient dispersion problems. Furthermore, this model retains the advantages of LSTM unit in predicting time series data. Third, to adapt the proposed framework to the timeliness of industrial anomaly detection, we propose a gradient compression mechanism based on Top-\\textit{k} selection to improve communication efficiency. Extensive experiment studies on four real-world datasets demonstrate that the proposed framework can accurately and timely detect anomalies and also reduce the communication overhead by 50\\% compared to the federated learning framework that does not use a gradient compression scheme.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2007.09712", "mag": "3105324058", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/iotj/LiuGNZXKH21", "doi": "10.1109/jiot.2020.3011726"}}, "content": {"source": {"pdf_hash": "64553f320cb3e9ea5285e14315380f04d4168ff4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.09712v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.09712", "status": "GREEN"}}, "grobid": {"id": "1bfdd0ea8f44142e137fcc4f1b885760741aad22", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/64553f320cb3e9ea5285e14315380f04d4168ff4.txt", "contents": "\nNational Natural Science Foundation of China (Grant No. 51806157), Young Innovation Talents Project in Higher Education of Guangdong Province\n\n\nYi Liu \nMember, IEEESahil Garg sahil.garg@ieee.org.j.nie \nJiangtian Nie \nYang Zhang yangzhang@whut.edu.cn.z. \nZehui Xiong \nJiawen Kang \nSenior Member, IEEEM Shamim Hossain \nHubei \n\nElectrical Engineering Department\nState Key Laboratory of Industrial Control Technology\nZhejiang University\n\u00c9cole de technologie sup\u00e9rieureHarbinChina, China\n\n\nis with Energy Research Institute, Interdisciplinary Graduate Programme and School of Computer Science and Engineering\nUni-versit\u00e9 du Qu\u00e9bec\nMontr\u00e9alCanada\n\n\nSchool of Com-puter Science and Technology\nKey Laboratory of Transportation Internet of Things\nNanyang Technological University\nSingapore\n\n\nWuhan University of Technology\nChina\n\n\nDepartment of Software Engineering\nCollege of Computer and Information Sciences\nNanyang Technological University\nSingapore\n\n\nKing Saud University\nSaudi Arabia\n\nNational Natural Science Foundation of China (Grant No. 51806157), Young Innovation Talents Project in Higher Education of Guangdong Province\nXiong is with Alibaba-NTU Joint Re-search Institute and also School of Computer Science and Engineering, NTU, Singapore. (Email: zxiong002@e.ntu.edu.sg). J. Kang is with Energy Research Institute, gradient compression, industrial internet of thingsIndex Terms-Federated learning, deep anomaly detection,\nSince edge device failures (i.e., anomalies) seriously affect the production of industrial products in Industrial IoT (IIoT), accurately and timely detecting anomalies is becoming increasingly important. Furthermore, data collected by the edge device may contain the user's private data, which is challenging the current detection approaches as user privacy is calling for the public concern in recent years. With this focus, this paper proposes a new communication-efficient ondevice federated learning (FL)-based deep anomaly detection framework for sensing time-series data in IIoT. Specifically, we first introduce a FL framework to enable decentralized edge devices to collaboratively train an anomaly detection model, which can improve its generalization ability. Second, we propose an Attention Mechanism-based Convolutional Neural Network-Long Short Term Memory (AMCNN-LSTM) model to accurately detect anomalies. The AMCNN-LSTM model uses attention mechanism-based CNN units to capture important fine-grained features, thereby preventing memory loss and gradient dispersion problems. Furthermore, this model retains the advantages of LSTM unit in predicting time series data. Third, to adapt the proposed framework to the timeliness of industrial anomaly detection, we propose a gradient compression mechanism based on Top-k selection to improve communication efficiency. Extensive experiment studies on four real-world datasets demonstrate that the proposed framework can accurately and timely detect anomalies and also reduce the communication overhead by 50% compared to the federated learning framework that does not use gradient compression scheme.\n\nI. INTRODUCTION\n\nT HE widespread deployment of edge devices in the Industrial Internet of Things (IIoT) paradigm has spawned a variety of emerging applications with edge computing, such as smart manufacturing, intelligent transportation, and intelligent logistics [1]. The edge devices provide powerful computation resources to enable real-time, flexible, and quick decision making for the IIoT applications, which has greatly promoted the development of Industry 4.0 [2]. However, the IIoT applications are suffering from critical security risks caused by abnormal IIoT nodes which hinders the rapid development of IIoT. For example, in smart manufacturing scenarios, industrial devices acting as IIoT nodes, e.g., engines with sensors, that have abnormal behaviors (e.g., abnormal traffic and irregular reporting frequency) may cause industrial production interruption thus resulting in huge economic losses for factories [3], [4]. Edge devices (e.g., industrial robots), generally collect sensing data from IIoT nodes, especially time-series data, to analyze and capture the behaviors and operating condition of IIoT nodes by edge computing [5]. Therefore, these sensing time series data can be used to detect the anomaly behaviors of IIoT nodes [6].\n\nTo solve the abnormality problems from IIoT devices, typical methods are to perform abnormal detection for the affected IIoT devices [7]- [10]. Previous work focused on utilizing deep anomaly detection (DAD) [11] approaches to detect abnormal behaviors of IIoT devices by analyzing sensing time series data. DAD techniques can learn hierarchical discriminative features from historical time-series data. In [12]- [14], the authors proposed a Long Short Term Memory (LSTM) networksbased deep learning model to achieve anomaly detection in sensing time series data. Munir et al. in [15] proposed a novel DAD approach, called DeepAnT, to achieve anomaly detection by utilizing deep Convolutional Neural Network (CNN) to predict anomaly value. Although the existing DAD approaches have achieved success in anomaly detection, they cannot be directly applied to the IIoT scenarios with distributed edge devices for timely and accurate anomaly detection. The reasons are two-fold: (i) the most of detection models are not flexible enough in traditional approaches, the edge devices lack dynamic and automatically updated detection models for different scenarios, and hence the models fail to accurately arXiv:2007.09712v1 [cs.LG] 19 Jul 2020 predict frequently updated time-series data [8]; (2) due to privacy concerns, the edge devices are not willing to share their own collected time-series data with each other, thus the data exists in the form of \"islands.\" The data islands significantly degrade the performance of anomaly detection. Furthermore, it is often overlooked that the data may contain sensitive private information, which leads to potential privacy leakage. There are some privacy issues in the anomaly detection context. For example, the anomaly detection model will reveal the patient's heart disease history when detecting the patient's abnormal pulse [16], [17].\n\nTo address the above challenges, a promising on-device privacy-preserving distributed machine learning paradigm, called on-device federated learning (FL), was proposed for edge devices to train a global DAD model while keeps the training datasets locally without sharing raw training data [18]. Such a framework allows edge devices to collaboratively train an on-device DAD model without compromising privacy. For example, the authors in [19] proposed an ondevice FL-based approach to achieve collaborative anomaly detection. Tsukada et al. in [20] utilized FL framework to propose a Backpropagation Neural Networks (i.e., BPNNs) based approaches for anomaly detection. However, previous researches ignore the communication overhead during model training using FL among large-scale edge devices. Expensive communication overhead may cause excessive overhead and long convergence time for edge devices so that the on-device DAD model cannot quickly detect anomalies. Therefore, it is necessary to develop a communication-efficient on-device FL framework to achieve accurate and timely anomaly detection for edge devices.\n\nIn this paper, we propose a communication-efficient ondevice FL framework that leverages an attention mechanismbased CNN-LSTM (AMCNN-LSTM) model to achieve accurate and timely anomaly detection for edge devices. First, we introduce a FL framework to enable distributed edge devices to collaboratively train a global DAD model without compromising privacy. Second, we propose an AMCNN-LSTM model to detect anomalies. Specifically, we use attention-based CNNs to extract fine-grained features of historical observationsensing time-series data and use LSTM modules for timeseries prediction. Such a model can prevent memory loss and gradient dispersion problems. Third, to further improve the communication efficiency of the proposed framework, we propose a gradient compression mechanism based on Topk selection to reduce the number of gradients uploaded by edge devices. We evaluate the proposed framework on four real-world datasets: power demand, space shuttle, ECG, and engine. Experimental results show that the proposed framework can achieve high-efficiency communication and achieve accurate and timely anomaly detection. The contributions of this paper are summarized as follows:\n\n\u2022 We introduce a federated learning framework to develop an on-device collaborative deep anomaly detection model for edge devices in IIoT. \u2022 We propose an attention mechanism-based CNN-LSTM model to detect anomalies, which uses CNN to capture the fine-grained features of time series data and uses LSTM module to accurately and timely detect anomalies. \u2022 We propose a Top-k selection-based gradient compression scheme to improve the proposed framework's communication efficiency. Such a scheme decreases communication overhead by reducing the exchanged gradient parameters between the edge devices and the cloud aggregator. \u2022 We conduct extensive experiments on four real-world datasets to demonstrate that the proposed framework can accurately detect anomalies with low communication overhead.\n\n\nII. RELATED WORK A. Deep Anomaly Detection\n\nDeep Anomaly Detection (DAD) has always been a hot issue in IIoT, which serves as a function of detecting anomalies. Previous researches about DAD generally can be divided into three categories: supervised DAD, semi-supervised DAD, and unsupervised DAD approaches.\n\nSupervised Deep Anomaly Detection: Supervised deep anomaly detection typically uses the labels of normal and abnormal data to train a deep-supervised binary or multiclass classifier. For example, Erfani et al. in [21] proposed a supervised Support Vector Machine (SVM) classifier for high-dimensional data to classify normal and abnormal data. Despite the success of supervised DAD methods in anomaly detection, these methods are not as popular as semi-supervised or unsupervised methods due to the lack of labeled training data [22]. Furthermore, the supervised DAD method has poor performance for data with class imbalance (the total number of positive class data is much larger than the total number of negative class data) [12].\n\nSemi-supervised Deep Anomaly Detection: Since normal instances are easier to obtain the labels than that of anomalies, semi-supervised DAD techniques are proposed to utilize a single (normally positive class) existing label to separate outliers [11]. For example, Wulsin et al. in [23] applied Deep Belief Nets (DBNs) in a semi-supervised paradigm to model Electroencephalogram (EEG) waveforms for classification and anomaly detection. The semi-supervised DBN performance is comparable to the standard classifier on EEG dataset. The semi-supervised DAD approach is popular because it can use only a single class of labels to detect anomalies.\n\nUnsupervised Deep Anomaly Detection: Unsupervised deep anomaly detection techniques use the inherent properties of data instances to detect outliers [11]. For example, Zong et al. in [24] proposed a deep automatic coding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Schlegl et al. in [25] proposed a deep convolutional generative adversarial network, called AnoGAN, which detects abnormal anatomical images by learning a variety of normal anatomical images. They trained such a model in an unsupervised manner. Unsupervised DAD is widely used since it does not require the characteristics of labeled training data.\n\n\nB. Communication-Efficient Federated Learning\n\nGoogle proposed a privacy-preserving distributed machine learning framework, called FL, to train machine learning mod-els without compromising privacy [26]. Inspired by this framework, different edge devices can contribute to the global model training while keeping the training data locally. However, communication overhead is the bottleneck of FL being widely used in IIoT [18]. Previous work has focused on designing efficient stochastic gradient descent (SGD) algorithms and using model compression to reduce the communication overhead of FL. Agarwal et al. in [27] proposed an efficient cpSGD algorithm to achieve communication-efficient FL. Reisizadeh et al. in [28] used Periodic Averaging and Quantization methods to design a communication-efficient FL framework. Jeong et al. in [29] proposed a federated model distillation method to reduce the communication overhead of FL.\n\nHowever, the above methods do not substantially reduce the number of gradients exchanged between edge devices and the cloud aggregator. The fact is that a large number of gradients exchanged between the edge devices and the cloud aggregator may cause excessive communication overhead for FL [30]. Therefore, in this paper, we propose a Top-k selection-based gradient compression scheme to improve the communication efficiency of FL.\n\n\nIII. PRELIMINARY\n\nIn this section, we briefly introduce anomalies, federated deep learning, and gradient compression as follows.\n\n\nA. Anomalies\n\nIn statistics, anomalies (also called outliers and abnormalities) are the data points that are significantly different from other observations [11]. We assume that N 1 , N 2 , and N 3 are regions composed of most observations, so they are regarded as normal data instance regions. If data points O 1 and O 2 are far from these regions, they can be classified as anomalies. To define anomalies more formally, we assume that an n-dimensional dataset x i = (x i,1 , . . . , x i,n ) follows a normal distribution and its mean \u00b5 j and variance \u03c3 j for each dimension where i \u2208 {1, . . . , m} and j \u2208 {1, . . . , n}. Specifically, for j \u2208 {1, . . . , n}, under the assumption of the normal distribution, we have:\n\u00b5 j = m i=1 x i,j /m, \u03c3 2 j = m i=1 (x i,j \u2212 \u00b5 j ) 2 /m,(1)\nif there is a new vector x, the probability p( x) of anomaly can be calculated as follows:\np( x) = n j=1 p x j ; \u00b5 j , \u03c3 2 j = n j=1 1 \u221a 2\u03c0\u03c3 j exp \u2212 (x j \u2212 \u00b5 j ) 2 2\u03c3 2 j .\n(2) We can then judge whether vector x belongs to an anomaly according to the probability value.\n\n\nB. Federated Learning\n\nTraditional distributed deep learning techniques require a certain amount of private data to be aggregated and analyzed at central servers (e.g., cloud servers) during the model training phase by using distributed stochastic gradient descent (D-SGD) algorithm [31]. Such the training process suffers from potential data privacy leakage risks for IIoT devices. To address such privacy challenges, a collaboratively distributed deep learning paradigm, called federated deep learning, was proposed for edge devices to train a global model while keeping the training datasets locally without sharing raw training data [18]. The procedure of FL is divided into three phases: the initialization phase, the aggregation phase, and the update phase. In the initialization phase, we consider that FL with N edge devices and a parameter aggregator, i.e., a cloud aggregator, distributes a pre-trained global model \u03c9 t on the public datasets (e.g., MNIST [32], CIFAR-10 [33]) to each edge devices. Following that, each device uses local dataset D k of size D k to train and improve the current global model \u03c9 t in each iteration. In the aggregation phase, the cloud aggregator collects local gradients uploaded by the edge nodes (i.e., edge devices). To do so, the local loss function to be optimized is defined as follows:\nmin x\u2208R d F k (x) = 1 D k i\u2208D k E zi\u223cD k f (x; z i ) + \u03bbh(x),(3)\nwhere f (\u00b7; \u00b7) is the local loss function for edge device k, \u2200\u03bb \u2208 [0, 1], h(\u00b7) is a regularizer function for edge device k, and \u2200i \u2208 [1, \u00b7 \u00b7 \u00b7 , n], z i is sampled from the local dataset D k on the k device. In the update phase, the cloud aggregator uses Federated Averaging (FedAVG) algorithm [26] to obtain a new global model \u03c9 t+1 for the next iteration, thus we have:\n\u03c9 t+1 \u2190 \u03c9 t + 1 n N n=1 F n t+1 ,(4)\nwhere N n=1 F n t+1 denotes model updates aggregation and 1 n N n=1 F n t+1 denotes the average aggregation (i.e., FedAVG algorithm). Both the edge devices and the cloud aggregator repeat the above process until the global model reaches convergence. This paradigm significantly reduces the risks of privacy leakage by decoupling the model training from direct access to the raw training data on edge nodes.\n\n\nC. Gradient Compression\n\nLarge-scale FL training requires significant communication bandwidth for gradient exchange, which limits the scalability of multi-nodes training [34]. In this context, Lin et al. in [34] stated that 99.9% of the gradient exchange in D-SGD is redundant. To avoid expensive communication bandwidth limiting large-scale distributed training, gradient compression is proposed to greatly reduce communication bandwidth. Researchers generally use gradient quantization [35] and gradient sparsification [36] to achieve gradient compression. Gradient quantization reduces communication bandwidth by quantizing gradients to low-precision values. Gradient sparsification uses threshold quantization to reduce communication bandwidth.\n\nFor a fully connected (FC) layer in a deep neural network,\nwe have: b = f (W * a + v),\nwhere a is the input, v is the bias, W is the weight, f is the nonlinear mapping, and b is the output. This formula is the most basic operation in a neural network. For each specific neuron i, the above formula can be simplified to the following:\nb i = ReLU n\u22121 j=0 W ij a j ,\nWhere ReLU is the activation function. Gradient compression compresses the corresponding weight matrix into a sparse matrix, and hence the corresponding formula is given as follows:\nb i = ReLU \uf8eb \uf8ed j\u2208Xi\u2229Y Sparse [I ij ] a j \uf8f6 \uf8f8 ,(5)\nwhere j\u2208Xi\u2229Y S [I ij ] represents the compressed weight matrix and i, j represent the position information of the gradient in the weight matrix W . Such a method reduces the communication overhead through sparsing the gradient in the weight matrix W .\n\n\nIV. SYSTEM MODEL\n\nWe consider the generic setting for on-device deep anomaly detection in IIoT, where a cloud aggregator and edge devices work collaboratively to train a DAD model by using a given training algorithm (e.g., LSTM) for a specific task (i.e., anomaly detection task), as illustrated in Fig. 1. The edge devices train a shared global model locally on their own local dataset (i.e., sensing time series data from IIoT nodes) and upload their model updates (i.e., gradients) to the cloud aggregator. The cloud aggregator uses the FedAVG algorithm or other aggregation algorithms to aggregate these model updates and obtains a new global model. In the end, the edge devices will receive the new global model sent by the cloud aggregator and use it to achieve accurate and timely anomaly detection.\n\n\nA. System Model Limitations\n\nThe proposed framework focus on a DAD model learning task involving N distributed edge devices and a cloud aggregator. In this context, this framework has two limitations:: missing labels and communication overhead.\n\nFor missing-label limitation, we assume that the labels of the training sample with proportion p (0 < p < 1) are missing. The lack of the label of the sample will cause the problem of class imbalance, thereby reducing the accuracy of DAD model. For communication-overhead limitation, we consider that there exists an excessive communication overhead when a large number of gradients exchanged between the edge devices and the cloud aggregator, which may make the model fail to converge [29].\n\nThe above restrictions hinder the deployment of DAD model in edge devices, which motivates us to develop a communication-efficient FL-based unsupervised DAD framework to achieve accurate and timely anomaly detection. \n\n\nB. The Proposed Framework\n\nWe consider an on-device communication-efficient deep anomaly detection framework that involves multiple edge devices for collaborative model training in IIoT, as illustrated in Fig. 2. In particular, this framework consists of a cloud aggregator and edge devices. Furthermore, the proposed framework also includes two mechanisms: an anomaly detection mechanism and a gradient compression mechanism. More details are described as follows:\n\n\u2022 Cloud Aggregator: The cloud aggregator is generally a cloud server with strong computing power and rich computing resources. The cloud aggregator contains two functions: (1) Fig. 3. The overview of the attention mechanism-based CNN-LSTM Model.\n\n\u2022 Deep Anomaly Detection Mechanism: The deep anomaly detection mechanism is deployed in the edge devices, which can detect anomalies to reduce economic losses. \u2022 Gradient Compression Mechanism: The gradient compression mechanism is deployed in the edge devices, which can compress the local gradients to reduce the number of gradients exchanged between the edge devices and the cloud aggregator, thereby reducing communication overhead.\n\n\nC. Design Goals\n\nIn this paper, our goal is to develop an on-device communication-efficient FL framework for deep anomaly detection in IIoT. First, the proposed framework needs to detect anomalies accurately in an unsupervised manner. The proposed framework uses an unsupervised AMCNN-LSTM model to detect anomalies. Second, the proposed framework can significantly improve communication efficiency by using a gradient compression mechanism. Third, the performance of the proposed framework is comparable to traditional FL frameworks.\n\n\nV. A COMMUNICATION-EFFICIENT ON-DEVICE DEEP ANOMALY DETECTION FRAMEWORK\n\nIn this section, we first present the attention mechanismbased CNN-LSTM model. This model uses CNN to capture the fine-grained features of sensing time series data and uses LSTM module to accurately and timely detect anomalies. We then propose a deep gradient compression mechanism to further improve the communication efficiency of the proposed framework.\n\n\nA. Attention Mechanism-based CNN-LSTM Model\n\nWe present an unsupervised AMCNN-LSTM model including an input layer, an attention mechanism-based CNN unit, an LSTM unit, and an output layer shown in Fig. 3. First, we use the preprocessed data as input to the input layer. Second, we use CNN to capture the fine-grained features of the input and utilize the attention mechanism to focus on the important features of CNN captured features. Third, we use the output of the attention mechanism-based CNN unit as the input of the LSTM unit and use LSTM to predict future time-series data. Finally, we propose an anomaly detection score to detect anomalies.\n\nPreprocessing: We normalize the sensing time series data collected by the IIoT nodes into [0,1] to accelerate the model convergence.\n\nAttention Mechanism-based CNN Unit: First, we introduce an attention mechanism in CNN unit to improve the focus on important features. In cognitive science, due to the bottleneck of information processing, humans will selectively focus on important parts of information while ignoring other visible information [37]. Inspired by the above facts, attention mechanisms are proposed for various tasks, such as computer vision and natural language processing [37]- [39]. Therefore, the attention mechanism can improve the performance of the model by paying attention to important features. The formal definition of the attention mechanism is given as follows:\ne i = a(u, v i ) (Compute Attention Scores), \u03b1 i = ei i ei (Normalize), c = i \u03b1 i v i (Encode),(6)\nwhere u is the matching feature vector based on the current task and is used to interact with the context, v i is the feature vector of a timestamp in the time series, e i is the unnormalized attention score, \u03b1 i is the normalized attention score, and c is the context feature of the current timestamp calculated based on the attention score and feature sequence v. In most instances, e i = u T W v, where W is the weight matrix.\n\nSecond, we use CNN unit to extract fine-grained features of time series data. The CNN module is formed by stacking multiple layers of one-dimensional (1-D) CNN, and each layer includes a convolution layer, a batch normalization layer, and a non-linear layer. Such modules implement sampling aggregation by using pooling layers and create hierarchical structures that gradually extract more abstract features through the stacking of convolutional layers. This module outputs m feature sequences of length n, and the size can be expressed as (n\u00d7m). To further extract significant time-series data features, we propose a parallel feature extraction branch by combining the attention mechanisms and CNN. The attention mechanism module is composed of feature aggregation and scale restoration. The feature aggregation part uses the stacking of multiple convolutions and pooling layers to extract key features from the sequence and uses a convolution kernel of size 1 \u00d7 1 to mine the linear relationship. The scale restoration part restores the key features to (n\u00d7m), which is consistent with the size of the output features of CNN module, and then uses the sigmoid function to constrain the values to [0,1].\n\nThird, we multiply element-wise the output features of CNN module and the output of the important features by the corresponding attention mechanism module. We assume that the sequence X i = {x i 1 , x i 2 , \u00b7 \u00b7 \u00b7 , x i n }(0 \u2264 i < I). The output of the sequence X i processed by CNN module is represented by W CNN , and the output of the corresponding attention module is represented as W attention . We multiply the two outputs element by element, as follows:\nW (i, c) = W CNN (i, c) W attention (i, c),(7)\nwhere represents element-wise multiplication, i is the corresponding position of the time series in the feature layer, and c is the channel. We use the final feature layer W (i, c) as the input of LSTM block.\n\nWe introduce the attention mechanism to expand the receptive field of the input, which allows the model to obtain more comprehensive contextual information, thereby learning the important features of the current local sequence. Furthermore, we use the attention module to suppress the interference of unimportant features to the model, thereby solving the problem that the model cannot distinguish the importance of the time series data features.\n\nLSTM Unit: In this paper, we use a variant of a recurrent neural network, called LSTM, to support accurately predict the sensing time series data to detect anomalies, as shown in Fig.  3. LSTM uses a well-designed \"gate\" structure to remove or add information to the state of the cell. The \"gate\" structure is a method of selectively passing information. LSTM cells include forget gates f t , input gates i t , and output gates o t . The calculations on the three gate structures are defined as follows:\nf t = \u03c3 l (W f \u00b7 [h t\u22121 , x t ] + b f ), i t = \u03c3 l (W i \u00b7 [h t\u22121 , x t ] + b i ), C t = tanh(W C \u00b7 [h t\u22121 , x t ] + b C ), C t = f t * C t\u22121 + i t * C t , o t = \u03c3 l (W o \u00b7 [h t\u22121 , x t ] + b o ), h t = o t * tanh(C t ),(8)where W f , W i , W C , W o , and b f , b i , b C , b o\nare the weight matrices and the bias vectors for input vector x t at time step t, respectively. \u03c3 l is the activation function, * represents elementwise multiplication of a matrix, C t represents the cell state, h t\u22121 is the state of the hidden layer at time step t \u2212 1, and h t is the state of the hidden layer at time step t. Anomaly Detection: We use AMCNN-LSTM model to predict real-time and future sensing time series data in different edge devices:\n[x i n\u2212T +1 , x i n\u2212T +2 , \u00b7 \u00b7 \u00b7 , x i n ] f (\u00b7)\n\u2192 [x i n+1 , x i n+2 , \u00b7 \u00b7 \u00b7 , x i n+T ], (9) where f (\u00b7) is the prediction function. In this paper, we use LSTM unit for time series prediction. We use anomaly scores for anomaly detection, which is defined as follows:\nA n = (\u03b2 n \u2212 \u00b5) T \u03c3 \u22121 (\u03b2 n \u2212 \u00b5),(10)\nwhere A n is the anomaly score, \u03b2 n = |x i n \u2212 x i n | is the reconstruction error vector, and the error vectors \u03b2 n for the time series in the sequences X i are used to estimate the parameters \u00b5 and \u03c3 of a Normal distribution N (\u00b5; \u03c3) using Maximum Likelihood Estimation.\n\nIn an unsupervised setting, when A n \u2265 \u03c2 (\u03c2 = max F \u03b8 = (1+\u03b8 2 )\u00d7P \u00d7R \u03b8 2 P +R ), where P is precision, R is recall, and \u03b8 is the parameter, a point in a sequence can be predicted to be \"anomalous\", otherwise \"normal\".\n\n\nB. Gradient Compression Mechanism\n\nIf the gradients reach 99.9% sparsity, only the 0.1% gradients with the largest absolute value are useful for model aggregation [30]. Therefore, we only need to aggregate the gradient with a larger absolute value to update the model. This way reduces the byte size of the gradient matrix, which can reduce the number of gradients exchanged between the device and the cloud to improve communication efficiency, especially for distributed machine learning systems. Inspired by the above facts, we propose a gradient compression mechanism to reduce the gradients exchanged between the cloud aggregator and the edge devices. We expect that this mechanism can further improve the communication efficiency of the proposed framework.\n\nWhen we choose a gradient with a larger absolute value, we will meet the following situations: (1) All gradient values in the gradient matrix are not greater than the given threshold;\n\n(2) There are some gradient values in the gradient matrix that are very close to the given threshold. If we set these gradients that do not meet the threshold requirements to 0, it will cause information loss. Therefore, the device uses a local gradient accumulation scheme to prevent information loss. Specifically, the cloud returns smaller gradients to the device instead of filtering the gradients. The device keeps the smaller gradient in the buffer and accumulates all the smaller gradients until it reaches a given threshold. Note that we use D-SGD for iterative updates, and the loss function to be optimized is defined as follows:\nF (\u03c9) = 1 D k x\u2208D k f (x, \u03c9),(11)\u03c9 t+1 = \u03c9 t \u2212 \u03b7 1 N b N k=1 x\u2208B k,t \u2207f (x, \u03c9 t ) ,(12)\nwhere F (\u03c9) is the loss function, f (x, \u03c9) is the loss function for the local device, \u03c9 are the weights of the model, N is the total edge devices, \u03b7 is the learning rate, B k,t represents the data sample for the t-th round of training, and each local dataset size of b. When the gradients' sparsification reaches a high value (e.g., 99%), it will affect the model convergence. By following [30], [36], we use momentum correction and local gradient clipping to mitigate this effect. Momentum correction can make the accumulated small local gradients converge toward the gradients with a larger absolute value, thereby accelerating the model's convergence speed. Local gradient clipping is used to alleviate the problem of gradient explosions [30]. Next, we prove that local gradient accumulation scheme will not affect the model convergence: We assume that g (i) is the ith gradient, u (i) denotes the sum of the gradients using the aggregation algorithm in [26], v (i) denotes the sum of the gradients using the local gradient accumulation scheme, and m is the rate of gradient descent. If the i-th gradient does not exceed threshold until the (t \u2212 1)-th iteration and triggers the model update, we have:\nu (i) t\u22121 = m t\u22122 g (i) 1 + \u00b7 \u00b7 \u00b7 + mg (i) t\u22122 + g (i) t\u22121 ,(13)\nv (i)\nt\u22121 = 1 + \u00b7 \u00b7 \u00b7 + m t\u22122 g (i) 1 + \u00b7 \u00b7 \u00b7 + (1 + m)g (i) t\u22122 + g (i) t\u22121 ,(14)\nthen we can update \u03c9\n(i) t = w (i) 1 \u2212 \u03b7 \u00d7 v (i)\nt\u22121 and set v (i) t\u22121 = 0. If the i-th gradient reaches the threshold at the t-th iteration, model update is triggered, thus we have:\nu (i) t = m t\u22121 g (i) 1 + \u00b7 \u00b7 \u00b7 + mg (i) t\u22121 + g (i) t ,(15)v (i) t = m t\u22121 g (i) 1 + \u00b7 \u00b7 \u00b7 + mg (i) t\u22121 + g (i) t .(16)\nThen we can update \u03c9\n(i) t+1 = \u03c9 (i) t \u2212 \u03b7 \u00d7 v (i) t = \u03c9 1 (i) \u2212 \u03b7 \u00d7 1 + \u00b7 \u00b7 \u00b7 + m t\u22121 g (i) 1 + \u00b7 \u00b7 \u00b7 + (1 + m)g (i) t\u22121 + g (i) t = w (i) 1 \u2212 \u03b7 \u00d7 v (i)\nt\u22121 , so the result of using the local gradient accumulation scheme is consistent with the usage effect of the optimization algorithm in [26].\n\nThe specific implementation phases of the gradient compression mechanism are given as follows:\n\ni) Phase 1, Local Training: Edge devices use the local dataset to train the local model. In particular, we use the gradient accumulation scheme to accumulate local small gradients. ii) Phase 2, Gradient Compression: Each edge device uses Algorithm 1 to compress the gradients and upload sparse gradients (i.e., only gradients larger than a threshold are transmitted.) to the cloud aggregator. Note that the edge devices send the remaining local gradient to the cloud aggregator when the local gradient accumulation is greater than a threshold. iii) Phase 3, Gradient Aggregation: The cloud aggregator obtains the global model by aggregating sparse gradients and sends this global model to the edge devices. The gradient compression algorithm is thus presented in Algorithm 1.\n\n\nVI. EXPERIMENTS\n\nIn this section, the proposed framework is applied to four real-world datasets, i.e., power demand 1 , space shuttle 2 , ECG 3 , and engine 4 for performance demonstration. These datasets are time series datasets collected by different types of sensors from different fields [6]. For example, the power demand dataset is composed of electricity consumption data recorded by the electricity meter. There are normal subsequences and anomalous subsequences in these datasets. As shown in Table  I, X, X n , and X a is a number of original sequences, normal subsequences, and anomalous subsequences, respectively. For the power demand dataset, the anomalous subsequences indicate that the electricity meter has failed or stop working. Therefore, we need to use these datasets to train a FL model that can detect anomalies. We divide all datasets into a training set and a test set in a 7: 3 ratio. We implement the proposed framework by using Pytorch and PySyft [40]. The experiment is conducted on a virtual workstation with the Ubuntu 18.04 operation system, Intel (R) Core (TM) i5-4210M CPU, 16GB RAM, 512GB SSD. , \n\n\nA. Evaluation Setup\n\nIn this experiment, to determine the hyperparameter \u03c1 of the gradient compression mechanism, we first apply a simple CNN network (i.e., CNN with 2 convolutional layers followed by 1 fully connected layer) in the proposed framework to perform the classification task on MNIST and CIFAR-10 dataset. The pixels in all datasets are normalized into [0,1]. During the simulation, the number of edge devices is N = 10, the learning rate is \u03b7 = 0.001, the training epoch is E = 1000, the minibatch size is B = 128, and we follow reference [41] and set \u03b8 as 0.05.\n\nWe adopt Root Mean Square Error (RMSE) to indicate the performance of AMCNN-LSTM model as follows:\nRMSE = [ 1 n n i=1 (|y i \u2212\u0177 p |) 2 ] 1 2 ,(17)\nwhere y i is the observed sensing time series data, and\u0177 p is \n\n\nB. Hyperparameters Selection of the Proposed Framework\n\nIn the context of deep gradient compression scheme, proper hyperparameter selection, i.e., a threshold of absolute gradient value, is a notable factor that determines the proposed framework performance. In this section, we investigate the performance of the proposed framework with different thresholds and try to find a best-performing threshold for it. In particular, we employ \u03c1 \u2208 {0.1, 0.2, 0.3, 0.5, 0.9, 1, 100} to adjust the best threshold of the proposed framework. We use MNIST and CIFAR-10 datasets to evaluate the performance of the proposed framework with the selected threshold. As shown in Fig. 4, we observe that the larger \u03c1, the better the performance of the proposed framework. For MNIST task, the results show that when \u03c1 = 0.3, the accuracy is 97.25%; when \u03c1 = 100, the accuracy is 99.08%. This means that the model increases gradient size by about 300 times, but the accuracy is only improved by 1.83%. Furthermore, we observe a tradeoff between the gradient threshold and accuracy. Therefore, to achieve a good trade-off between the gradient threshold and accuracy, we choose \u03c1 = 0.3 as the best threshold of our scheme.\n\n\nC. Performance of the Proposed Framework\n\nWe compared the performance of the proposed model with that of CNN-LSTM [42], LSTM [41], Gate Recurrent Unit (GRU) [43], Stacked Auto Encoders (SAEs) [44], and Support Machine Vector (SVM) [45] method with an identical simulation configuration. Among these competing methods, AMCNN-LSTM is a FL-based model, and the rest of the methods are centralized ones. All models are popular DAD models for general anomaly detection applications. We evaluate these models on four real-world datasets, i.e., power demand, space shuttle, ECG, and engine.\n\nFirst, we compare the accuracy of the proposed model with competing methods in anomaly detection. We determine the max F \u03b8 and hyperparameter \u03c2 based on the accuracy and recall of the model on the training set. The hyperparameters \u03c2 of the dataset power demand, space shuttle, ECG, and engine are 0.75, 0.80, 0.80, and 0.60. In Fig. 5, experimental results show that the proposed model achieves the highest accuracy on all four datasets. For example, for the dataset power demand, the accuracy of AMCNN-LSTM model is 96.85%, which is 7.87% higher than that of SVM model. From the experimental results, AMCNN-LSTM has better robustness to different datasets. The reason is that we use the on-device FL framework to train and update the model, which can learn the time-series features from different edge devices as much as possible, thereby improving the robustness of the model. Furthermore, the FL framework provides opportunities for edge devices to update models in a timely manner. This helps the edge device owner to update the model on the edge devices in time.\n\nSecond, we need to evaluate the prediction error of the proposed model and the competing methods. As shown in Fig.  5, experimental results show that the proposed model achieves the best performance on four real-world datasets. For the ECG dataset, RMSE of AMCNN-LSTM model is 63.9% lower than that of SVM model. The reason is that AMCNN-LSTM model uses AMCNN units to capture important fine-grained features and prevent memory loss and gradient dispersion problems. Memory loss and gradient dispersion problems often occur in encoder-decoder models such as LSTM and GRU models. Furthermore, the proposed model retains the advantages of LSTM unit in predicting time series data. Therefore, the proposed model can accurately predict time series data.\n\nTherefore, the proposed model not only accurately detects abnormalities, but also accurately predicts time series data. \n\n\nD. Communication Efficiency of the Proposed Framework\n\nIn this section, we compare the communication efficiency between FL framework with the gradient compression mechanism (GCM) and the traditional FL framework without GCM. We apply the same model (i.e., AMCNN-LSTM, CNN-LSTM, LSTM, GRU, SAEs, and SVM) in the proposed framework and the traditional FL framework. Note that we fix the communication overhead of each round, so we can compare the running time of the model to compare the communication efficiency. In Fig. 7, we show the running time of FL with GCM and FL without GCM using different models. As shown in Fig. 7, we observe that the running time of FL framework with GCM is about 50% that of the framework without GCM. The reason is that GCM can reduce the number of gradients exchanged between the edge devices and the cloud aggregator. In section V-B, we show that GCM can compress the gradient by 300 times without compromising accuracy. Therefore, the proposed communication efficient framework is practical and effective in real-world applications.\n\n\nE. Discussion\n\nDue to the trade-off between privacy and model performance, we will discuss the privacy analysis of the proposed framework in terms of data access and model performance:\n\n\u2022 Data Access: FL framework allows edge devices to keep the dataset locally and collaboratively learn deep learning models, which means that any third party cannot access the user's raw data. Therefore, the FL-based model can achieve anomaly detection without compromising privacy. \u2022 Model Performance: Although the FL-based model can protect privacy, the model performance is still an important metric to measure the quality of the model. It can be seen from the experimental results that the performance of the proposed model is comparable to many advanced centralized machine learning models, such as CNN-LSTM, LSTM, GRU, and SVM model. In other words, the proposed model makes a good compromise between privacy and model performance.\n\n\nVII. CONCLUSION\n\nIn this paper, we propose a novel communication-efficient on-device FL-based deep anomaly detection framework for sensing time series data in IIoT. First, we introduce a FL framework to enable decentralized edge devices to collaboratively train an anomaly detection model, which can solve the problem of data islands. Second, we propose an attention mechanism-based CNN-LSTM (AMCNN-LSTM) model to accurately detect anomalies. AMCNN-LSTM model uses attention mechanism-based CNN units to capture important fine-grained features and prevent memory loss and gradient dispersion problems. Furthermore, this model retains the advantages of LSTM unit in predicting time series data. We evaluate the performance of the proposed model on four real-world datasets and compare it with CNN-LSTM, LSTM, GRU, SAEs, and SVM methods. The experimental results show that the AMCNN-LSTM model can achieve the highest accuracy on all four datasets. Third, we propose a gradient compression mechanism based on Top-k selection to improve communication efficiency. Experimental results validate that this mechanism can compress the gradient by 300 times without losing accuracy. To the best of our knowledge, this is one of the pioneering work for deep anomaly detection by using on-device FL.\n\nIn the future, we will focus on researching privacy-enhanced FL frameworks and more robust anomaly detection models. The reason is that the FL framework is vulnerable to malicious attacks by malicious participants and a more robust model can be applied to a wider range of application scenarios.\n\nYang Zhang (M11) is currently an associate professor at the School of Computer Science and Technology, Wuhan University of Technology, China. He received B. Eng. and M. Eng. from Beijing University of Aeronautics and Astronautics in 2008 and 2011, respectively, and obtained a Ph.D. degree in Computer Engineering from Nanyang Technological University (NTU), Singapore, in 2015. He is an associate editor of EURASIP Journal on Wireless Communications and Networking, and a technical committee member of Computer Communications (Elsevier). He is also an advisory expert member of Shenzhen FinTech Laboratory, China. His current research interests include: market-oriented modeling for network resource allocation, multiple agent machine learning, and deep reinforcement learning in network systems. Jiawen Kang received the M.S. degree from the Guangdong University of Technology, China, in 2015, and the Ph.D. degree at the same school in 2018. He is currently a postdoc at Nanyang Technological University, Singapore. His research interests mainly focus on blockchain, security and privacy protection in wireless communications and networking.\n\nFig. 1 .\n1The workflow of the on-device deep anomaly detection in IIoT.\n\nFig. 2 .\n2The overview of on-device communication-efficient deep anomaly detection framework in IIoT. This framework's workflow consists of five steps, as follows: (i) The edge device uses the sensing time series data collected from IIoT nodes as a local dataset (as shown in 1 ). (ii) The edge device performs local model (i.e., AMCNN-LSTM model) training on the local dataset (as shown in 2 ). (iii) The edge device uploads the sparse gradients\u2206\u03c9 to the cloud aggregator by using a gradient compression mechanism (as shown in 3 ). (iv) The cloud aggregator obtains a new global model by aggregating the sparse gradients uploaded by the edge device (as shown in 4 ). (v) The cloud aggregator sends the new global model to each edge device. The above steps are executed cyclically until the global model reaches optimal convergence (as shown in 5 ). Decentralized devices can use this optimal global model to perform anomaly detection tasks.\n\nFig. 4 .Fig. 5 .\n45The accuracy of the proposed framework with different \u03c1 on MNIST and CIFAR-10 datasets. Performance comparsion of detection accuracy for AMCNN-LSTM, CNN-LSTM, LSTM, GRU, SAEs, and SVM on different datasets: power demand, space shuttle, ECG, and engine. the predicted sensing time series data.\n\nFig. 6 .\n6Performance comparsion of RMSE for AMCNN-LSTM, CNN-LSTM, LSTM, GRU, SAEs, and SVM on different datasets: power demand, space shuttle, ECG, and engine.\n\nFig. 7 .\n7Comparison of communication efficiency between FL with GCM and FL without GCM with different models.\n\nTABLE I DETAILS\nIOF FOUR REAL-WORLD DATASETSDatasets \nDimensions \nX \nXn \nXa \n\nPower Demand \n1 \n1 \n45 \n6 \nSpace Shuttle \n1 \n3 \n20 \n8 \nECG \n1 \n1 \n215 \n1 \nEngine \n12 \n30 \n240 \n152 \n\n\n\n\nZehui Xiong (S'17, M'20) is currently a researcher with Alibaba-NTU Singapore Joint Research Institute, Singapore. He obtained the B.Eng degree with the highest honors in Telecommunications Engineering at Huazhong University of Science and Technology, Wuhan, China, in Jul 2016. He received the Ph.D. degree in Computer Science and Engineering at Nanyang Technological University, Singapore, in Apr 2020. He was a visiting scholar at Princeton University and University of Waterloo. His research interests include resource allocation in wireless communications, network games and economics, blockchain, and edge intelligence. He has won several Best Paper Awards including IEEE WCNC 2020, and IEEE Vehicular Technology Society Singapore Best Paper Award in 2019. He is an Editor for Elsevier Computer Networks and Elsevier Physical Communication, and an Associate Editor for IET Communications. He serves as a Guest Editor for IEEE Transactions on Cognitive Communications and Networking, IEEE Open Journal of Vehicular Technology, and EURASIP Journal on Wireless Communications and Networking. He is the recipient of the Chinese Government Award for Outstanding Students Abroad in 2019, and NTU SCSE Outstanding PhD Thesis Runner-Up Award in 2020.\nhttps://archive.ics.uci.edu/ml/datasets/ 2 https://archive.ics.uci.edu/ml/datasets/Statlog+(Shuttle) 3 https://physionet.org/about/database/ 4 https://archive.ics.uci.edu/ml/datasets.php\nAlgorithm 1: Gradient compression mechanism on edge node k.Input: G = {g 1 , g 2 , . . . , g k } is the edge node's gradient, B is the local mini-batch size, D k is the local dataset, \u03b7 is the learning rate, f (\u00b7, \u00b7) is the edge node's loss function, and the optimization function SGD.Sample data x from D k ;8 if Gradient Clipping then 9 g k t \u2190 Local Gradient Clipping (g k t ); 10 foreach g kj t \u2208 {g k t } and j = 1, 2, \u00b7 \u00b7 \u00b7 do11Thr \u2190 |Top \u03c1% of {g k t }|;12if |g kj t |\u2265 Thr then13Send this gradient to the cloud aggregator;14if |g kj t |< Thr then15The edge node k uses the local gradient accumulation scheme to accumulate gradients until the gradient reaches Thr;16Aggregate g k t : g t \u2190 N k=1 (sparseg k t );17 \u03c9 t+1 \u2190 SGD (\u03c9 t , g t ).18return \u03c9.\nDeep reinforcement learning based resource management for multi-access edge computing in vehicular networks. H Peng, X Shen, IEEE Transaction on Network Science and Engineering. to appearH. Peng and X. Shen, \"Deep reinforcement learning based resource management for multi-access edge computing in vehicular networks,\" IEEE Transaction on Network Science and Engineering, to appear.\n\nDominant data set selection algorithms for electricity consumption time-series data analysis based on affine transformation. Y Wu, IEEE Internet of Things Journal. 75Y. Wu et al., \"Dominant data set selection algorithms for electricity consumption time-series data analysis based on affine transformation,\" IEEE Internet of Things Journal, vol. 7, no. 5, pp. 4347-4360, 2020.\n\nHierarchical edge computing: A novel multi-source multi-dimensional data anomaly detection scheme for industrial internet of things. Y Peng, IEEE Access. 7Y. Peng et al., \"Hierarchical edge computing: A novel multi-source multi-dimensional data anomaly detection scheme for industrial internet of things,\" IEEE Access, vol. 7, pp. 111 257-111 270, 2019.\n\nToward energy-efficient and robust large-scale wsns: a scale-free network approach. H Peng, IEEE Journal on Selected Areas in Communications. 3412H. Peng et al., \"Toward energy-efficient and robust large-scale wsns: a scale-free network approach,\" IEEE Journal on Selected Areas in Communications, vol. 34, no. 12, pp. 4035-4047, 2016.\n\nLstm-based encoder-decoder for multi-sensor anomaly detection. P Malhotra, arXiv:1607.00148arXiv preprintP. Malhotra et al., \"Lstm-based encoder-decoder for multi-sensor anomaly detection,\" arXiv preprint arXiv:1607.00148, 2016.\n\nDistributed anomaly detection using autoencoder neural networks in wsn for iot. T Luo, 2018 IEEE International Conference on Communications. IEEET. Luo et al., \"Distributed anomaly detection using autoencoder neural networks in wsn for iot,\" in 2018 IEEE International Conference on Communications. IEEE, 2018, pp. 1-6.\n\nA hybrid deep learning-based model for anomaly detection in cloud datacenter networks. S Garg, IEEE Transactions on Network and Service Management. 163S. Garg et al., \"A hybrid deep learning-based model for anomaly detection in cloud datacenter networks,\" IEEE Transactions on Network and Service Management, vol. 16, no. 3, pp. 924-935, 2019.\n\nD\u00efot: A federated self-learning anomaly detection system for iot. T D Nguyen, 2019 IEEE 39th International Conference on Distributed Computing Systems. IEEET. D. Nguyen et al., \"D\u00efot: A federated self-learning anomaly detec- tion system for iot,\" in 2019 IEEE 39th International Conference on Distributed Computing Systems. IEEE, 2019, pp. 756-767.\n\nA multi-stage anomaly detection scheme for augmenting the security in iot-enabled applications. S Garg, Future Generation Computer Systems. 104S. Garg et al., \"A multi-stage anomaly detection scheme for augmenting the security in iot-enabled applications,\" Future Generation Computer Systems, vol. 104, pp. 105-118, 2020.\n\nEn-abc: An ensemble artificial bee colony based anomaly detection scheme for cloud environment. S Garg, Journal of Parallel and Distributed Computing. 135S. Garg et al., \"En-abc: An ensemble artificial bee colony based anomaly detection scheme for cloud environment,\" Journal of Parallel and Distributed Computing, vol. 135, pp. 219-233, 2020.\n\nDeep learning for anomaly detection: A survey. R Chalapathy, S Chawla, arXiv:1901.03407arXiv preprintR. Chalapathy and S. Chawla, \"Deep learning for anomaly detection: A survey,\" arXiv preprint arXiv:1901.03407, 2019.\n\nLong short term memory networks for anomaly detection in time series. P Malhotra, Presses universitaires de Louvain. 89ProceedingsP. Malhotra et al.,\"Long short term memory networks for anomaly detection in time series,\" in Proceedings, vol. 89. Presses universitaires de Louvain, 2015.\n\nDeep hierarchical encoding model for sentence semantic matching. W Lu, Journal of Visual Communication and Image Representation. 102794W. Lu et al., \"Deep hierarchical encoding model for sentence semantic matching,\" Journal of Visual Communication and Image Representation, p. 102794, 2020.\n\nDeep fuzzy hashing network for efficient image retrieval. H Lu, IEEE Transactions on Fuzzy Systems. H. Lu et al., \"Deep fuzzy hashing network for efficient image retrieval,\" IEEE Transactions on Fuzzy Systems, 2020.\n\nDeepant: A deep learning approach for unsupervised anomaly detection in time series. M Munir, IEEE Access. 7M. Munir et al., \"Deepant: A deep learning approach for unsupervised anomaly detection in time series,\" IEEE Access, vol. 7, pp. 1991-2005, 2018.\n\nAnomaly-based intrusion detection: privacy concerns and other problems. E Lundin, Computer networks. 344E. Lundin et al., \"Anomaly-based intrusion detection: privacy concerns and other problems,\" Computer networks, vol. 34, no. 4, pp. 623-640, 2000.\n\nAnomaly detection and privacy preservation in cloudcentric internet of things. I Butun, 2015 IEEE International Conference on Communication Workshop. IEEEI. Butun et al., \"Anomaly detection and privacy preservation in cloud- centric internet of things,\" in 2015 IEEE International Conference on Communication Workshop. IEEE, 2015, pp. 2610-2615.\n\nPrivacy-preserving traffic flow prediction: A federated learning approach. Y Liu, IEEE Internet of Things Journal. Y. Liu et al., \"Privacy-preserving traffic flow prediction: A federated learning approach,\" IEEE Internet of Things Journal, pp. 1-1, 2020.\n\nAn on-device federated learning approach for cooperative anomaly detection. R Ito, arXiv:2002.12301arXiv preprintR. Ito et al., \"An on-device federated learning approach for cooperative anomaly detection,\" arXiv preprint arXiv:2002.12301, 2020.\n\nA neural network based on-device learning anomaly detector for edge devices. M Tsukada, arXiv:1907.10147arXiv preprintM. Tsukada et al., \"A neural network based on-device learning anomaly detector for edge devices,\" arXiv preprint arXiv:1907.10147, 2019.\n\nHigh-dimensional and large-scale anomaly detection using a linear one-class svm with deep learning. S M Erfani, Pattern Recognition. 58S. M. Erfani et al., \"High-dimensional and large-scale anomaly detection using a linear one-class svm with deep learning,\" Pattern Recognition, vol. 58, pp. 121-134, 2016.\n\nDeepad: A generic framework based on deep learning for time series anomaly detection. T S Buda, Pacific-Asia Conference on Knowledge Discovery and Data Mining. SpringerT. S. Buda et al., \"Deepad: A generic framework based on deep learning for time series anomaly detection,\" in Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 2018, pp. 577- 588.\n\nModeling electroencephalography waveforms with semi-supervised deep belief nets: fast classification and anomaly measurement. D Wulsin, Journal of neural engineering. 8336015D. Wulsin et al., \"Modeling electroencephalography waveforms with semi-supervised deep belief nets: fast classification and anomaly mea- surement,\" Journal of neural engineering, vol. 8, no. 3, p. 036015, 2011.\n\nDeep autoencoding gaussian mixture model for unsupervised anomaly detection. B Zong, B. Zong et al., \"Deep autoencoding gaussian mixture model for unsupervised anomaly detection,\" 2018.\n\nUnsupervised anomaly detection with generative adversarial networks to guide marker discovery. T Schlegl, International conference on information processing in medical imaging. SpringerT. Schlegl et al., \"Unsupervised anomaly detection with generative adversarial networks to guide marker discovery,\" in International con- ference on information processing in medical imaging. Springer, 2017, pp. 146-157.\n\nFederated learning: Strategies for improving communication efficiency. J Kone\u010dn\u1ef3, arXiv:1610.05492arXiv preprintJ. Kone\u010dn\u1ef3 et al., \"Federated learning: Strategies for improving com- munication efficiency,\" arXiv preprint arXiv:1610.05492, 2016.\n\ncpsgd: Communication-efficient and differentiallyprivate distributed sgd. N , Advances in Neural Information Processing Systems. N. Agarwal et al., \"cpsgd: Communication-efficient and differentially- private distributed sgd,\" in Advances in Neural Information Processing Systems, 2018, pp. 7564-7575.\n\nFedpaq: A communication-efficient federated learning method with periodic averaging and quantization. A Reisizadeh, arXiv:1909.13014arXiv preprintA. Reisizadeh et al., \"Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization,\" arXiv preprint arXiv:1909.13014, 2019.\n\nCommunication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data. E Jeong, arXiv:1811.11479arXiv preprintE. Jeong et al., \"Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data,\" arXiv preprint arXiv:1811.11479, 2018.\n\nGradient sparsification for communication-efficient distributed optimization. J Wangni, Advances in Neural Information Processing Systems. J. Wangni et al., \"Gradient sparsification for communication-efficient distributed optimization,\" in Advances in Neural Information Processing Systems, 2018, pp. 1299-1309.\n\nShielding collaborative learning: Mitigating poisoning attacks through client-side detection. L Zhao, arXiv:1910.13111arXiv preprintL. Zhao et al., \"Shielding collaborative learning: Mitigating poisoning attacks through client-side detection,\" arXiv preprint arXiv:1910.13111, 2019.\n\nGradient-based learning applied to document recognition. Y Lecun, Proceedings of the IEEE. 8611Y. LeCun et al., \"Gradient-based learning applied to document recog- nition,\" Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, A. Krizhevsky, G. Hinton et al., \"Learning multiple layers of features from tiny images,\" 2009.\n\nDeep gradient compression: Reducing the communication bandwidth for distributed training. Y Lin, arXiv:1712.01887arXiv preprintY. Lin et al., \"Deep gradient compression: Reducing the communication bandwidth for distributed training,\" arXiv preprint arXiv:1712.01887, 2017.\n\nQsgd: Communication-efficient sgd via gradient quantization and encoding. D Alistarh, Advances in Neural Information Processing Systems. D. Alistarh et al., \"Qsgd: Communication-efficient sgd via gradient quantization and encoding,\" in Advances in Neural Information Pro- cessing Systems, 2017, pp. 1709-1720.\n\nGradient sparsification for communication-efficient distributed optimization. J Wangni, Advances in Neural Information Processing Systems. J. Wangni et al., \"Gradient sparsification for communication-efficient distributed optimization,\" in Advances in Neural Information Processing Systems, 2018, pp. 1299-1309.\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.0473arXiv preprintD. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation by jointly learning to align and translate,\" arXiv preprint arXiv:1409.0473, 2014.\n\nRecurrent models of visual attention. V Mnih, Advances in neural information processing systems. V. Mnih et al., \"Recurrent models of visual attention,\" in Advances in neural information processing systems, 2014, pp. 2204-2212.\n\nAttention is all you need. A Vaswani, Advances in neural information processing systems. A. Vaswani et al., \"Attention is all you need,\" in Advances in neural information processing systems, 2017, pp. 5998-6008.\n\nA generic framework for privacy preserving deep learning. T , arXiv:1811.04017arXiv preprintT. Ryffel et al., \"A generic framework for privacy preserving deep learning,\" arXiv preprint arXiv:1811.04017, 2018.\n\nLstm-based encoder-decoder for multi-sensor anomaly detection. P Malhotra, arXiv:1607.00148arXiv preprintP. Malhotra et al.,\"Lstm-based encoder-decoder for multi-sensor anomaly detection,\" arXiv preprint arXiv:1607.00148, 2016.\n\nWeb traffic anomaly detection using c-lstm neural networks. T.-Y Kim, S.-B Cho, Expert Systems with Applications. 106T.-Y. Kim and S.-B. Cho, \"Web traffic anomaly detection using c-lstm neural networks,\" Expert Systems with Applications, vol. 106, pp. 66-76, 2018.\n\nMultidimensional time series anomaly detection: A gru-based gaussian mixture variational autoencoder approach. Y Guo, Asian Conference on Machine Learning. Y. Guo et al., \"Multidimensional time series anomaly detection: A gru-based gaussian mixture variational autoencoder approach,\" in Asian Conference on Machine Learning, 2018, pp. 97-112.\n\nNetwork anomaly detection using channel boosted and residual learning based deep convolutional neural network. N Chouhan, A Khan, Applied Soft Computing. 83105612N. Chouhan, A. Khan et al., \"Network anomaly detection using channel boosted and residual learning based deep convolutional neural network,\" Applied Soft Computing, vol. 83, p. 105612, 2019.\n\nHigh-dimensional and large-scale anomaly detection using a linear one-class svm with deep learning. S M Erfani, Pattern Recognition. 58S. M. Erfani et al., \"High-dimensional and large-scale anomaly detection using a linear one-class svm with deep learning,\" Pattern Recognition, vol. 58, pp. 121-134, 2016.\n\nHe is currently pursuing a Ph.D. degree at the Faculty of Information Technology. Yi Liu, S'19) received the B.Eng. degree in network engineering from Heilongjiang University. Harbin, China; Melbourne, AustraliaMonash UniversityHis research interests include security & privacy. federated learning, edge computing, and blockchainYi Liu (S'19) received the B.Eng. degree in network engineering from Heilongjiang University, Harbin, China, in 2019. He is currently pursuing a Ph.D. degree at the Faculty of Information Technology, Monash University, Melbourne, Australia. His re- search interests include security & privacy, federated learning, edge computing, and blockchain.\n\nFuture Generation Computer Systems, and Wileys International Journal of Communication Systems. In addition, he also serves as a Workshops and Symposia Officer of the IEEE ComSoc Emerging Technology Initiative on Aerial Communications. Special Issues in top-cited journals including IEEE T-ITS, IEEE TII. Patiala, India; Kansas City, MissouriACMUniversit du Qubec, Montral, Canada. He received his Ph.D. degree from the Thapar Institute of Engineering and TechnologyElseviers Applied Soft ComputingSahil Garg (S'15, M'18) is a postdoctoral research fellow at cole de technologie suprieure, Universit du Qubec, Montral, Canada. He received his Ph.D. degree from the Thapar Institute of Engineering and Technology, Patiala, India, in 2018. He has many research contributions in the area of machine learning, big data analytics, security and privacy, the Internet of Things, and cloud computing. He has over 50 publications in high ranked journals and con- ferences, including 25+ IEEE transactions/journal papers. He received the IEEE ICC best paper award in 2018 in Kansas City, Missouri. He serves as the Managing Editor of Springers Human-Centric Computing and Information Sciences journal. He is also an Associate Editor of IEEE Network, IEEE System Journal, Elseviers Applied Soft Computing, Future Generation Computer Systems, and Wileys International Journal of Communication Systems. In addition, he also serves as a Workshops and Symposia Officer of the IEEE ComSoc Emerging Technology Initiative on Aerial Communications. He has guest edited a number of Special Issues in top-cited journals including IEEE T- ITS, IEEE TII, the IEEE IoT Journal, IEEE Network, and Future Generation Computer Systems. He serves/served as the Workshop Chair/Publicity Co- Chair for several IEEE/ACM conferences including IEEE INFOCOM, IEEE GLOBECOM, IEEE ICC, ACM MobiCom, and more. He is a member of ACM.\n\nShe is currently working towards the Ph.D. degree with ERI@N in the Interdisciplinary Graduate School. Her research interests include incentive mechanism design in crowdsensing and game theory. Wuhan, China; SingaporeJiangtian Nie received her B.Eng degree with honors in Electronics and Information Engineering from Huazhong University of Science and Technology ; Nanyang Technological UniversityJiangtian Nie received her B.Eng degree with hon- ors in Electronics and Information Engineering from Huazhong University of Science and Technology, Wuhan, China, in 2016. She is currently working towards the Ph.D. degree with ERI@N in the In- terdisciplinary Graduate School, Nanyang Techno- logical University, Singapore. Her research interests include incentive mechanism design in crowdsensing and game theory.\n\nHis research interests include cloud networking, smart environment (smart city, smart health), AI, deep learning, edge computing, Internet of Things (IoT), multimedia for health care, and multimedia big data. He has authored and coauthored more than 250 publications including refereed journals, conference papers, books, and book chapters. Recently, he co-edited a book on \"Connected Health in Smart Cities. M Shamim Hossain, Canada. SpringerCollege of Computer and Information Sciences, King Saud University ; Electrical Engineering and Computer Science, University of Ottawa, Canada. He received his Ph.D. in Electrical and Computer Engineering from the University of OttawaHe has served as cochair, general chair, workshop chair, publication chair, and TPC for over 12 IEEE and ACM conferences and workshops. Currently, he is the cochair of the 3rd IEEE ICME workshop on Multimedia Services and Tools for smart-health (MUST-SHM. Shamim Hossain (SM'19) is a Professor at the Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia. He is also an adjunct professor at the School of Electrical Engineering and Computer Science, University of Ottawa, Canada. He received his Ph.D. in Electrical and Computer Engineering from the University of Ottawa, Canada. His research interests include cloud networking, smart environment (smart city, smart health), AI, deep learn- ing, edge computing, Internet of Things (IoT), multimedia for health care, and multimedia big data. He has authored and coauthored more than 250 publications including refereed journals, conference papers, books, and book chapters. Recently, he co-edited a book on \"Connected Health in Smart Cities\", published by Springer. He has served as cochair, general chair, workshop chair, publication chair, and TPC for over 12 IEEE and ACM conferences and workshops. Currently, he is the cochair of the 3rd IEEE ICME workshop on Multimedia Services and Tools for smart-health (MUST-SH\n\nHe also presently serves as a lead guest editor of and Multimedia systems Journal. He serves/served as a guest editor of IEEE Communications Magazine. He is a recipient of a number of awards, including the Best Conference Paper Award and the 2016 ACM Transactions on Multimedia Computing, Communications and Applications (TOMM) Nicolas D. Georganas Best Paper Award. He is on the editorial board of the IEEE Transactions on Multimedia, IEEE Multimedia. ACMFuture Generation Computer Systems. He is a senior member of both the IEEEHe is a recipient of a number of awards, including the Best Conference Paper Award and the 2016 ACM Transactions on Multimedia Computing, Communications and Applications (TOMM) Nicolas D. Georganas Best Paper Award. He is on the editorial board of the IEEE Transactions on Multimedia, IEEE Multimedia, IEEE Network, IEEE Wireless Communications, IEEE Access, Journal of Network and Computer Applications (Elsevier), and International Journal of Multimedia Tools and Applications (Springer). He also presently serves as a lead guest editor of and Multimedia systems Journal. He serves/served as a guest editor of IEEE Communications Magazine, IEEE Network, ACM Transactions on Internet Technology, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), IEEE Transactions on Information Technology in Biomedicine (currently JBHI), IEEE Transactions on Cloud Computing, Multimedia Systems, International Journal of Multimedia Tools and Applications (Springer), Cluster Computing (Springer), Future Generation Computer Systems (Elsevier). He is a senior member of both the IEEE, and ACM.\n", "annotations": {"author": "[{\"end\":152,\"start\":145},{\"end\":202,\"start\":153},{\"end\":217,\"start\":203},{\"end\":254,\"start\":218},{\"end\":267,\"start\":255},{\"end\":280,\"start\":268},{\"end\":317,\"start\":281},{\"end\":324,\"start\":318},{\"end\":484,\"start\":325},{\"end\":642,\"start\":485},{\"end\":782,\"start\":643},{\"end\":821,\"start\":783},{\"end\":946,\"start\":822},{\"end\":982,\"start\":947}]", "publisher": null, "author_last_name": "[{\"end\":151,\"start\":148},{\"end\":175,\"start\":171},{\"end\":216,\"start\":213},{\"end\":228,\"start\":223},{\"end\":266,\"start\":261},{\"end\":279,\"start\":275},{\"end\":316,\"start\":302},{\"end\":323,\"start\":318}]", "author_first_name": "[{\"end\":147,\"start\":145},{\"end\":170,\"start\":165},{\"end\":212,\"start\":203},{\"end\":222,\"start\":218},{\"end\":260,\"start\":255},{\"end\":274,\"start\":268},{\"end\":301,\"start\":300}]", "author_affiliation": "[{\"end\":483,\"start\":326},{\"end\":641,\"start\":486},{\"end\":781,\"start\":644},{\"end\":820,\"start\":784},{\"end\":945,\"start\":823},{\"end\":981,\"start\":948}]", "title": "[{\"end\":142,\"start\":1},{\"end\":1124,\"start\":983}]", "venue": null, "abstract": "[{\"end\":3090,\"start\":1429}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3359,\"start\":3356},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3563,\"start\":3560},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4019,\"start\":4016},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4024,\"start\":4021},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4239,\"start\":4236},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4344,\"start\":4341},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4483,\"start\":4480},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4489,\"start\":4485},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4559,\"start\":4555},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4758,\"start\":4754},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4764,\"start\":4760},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4931,\"start\":4927},{\"end\":5566,\"start\":5562},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5629,\"start\":5626},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5634,\"start\":5631},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6216,\"start\":6212},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6222,\"start\":6218},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6518,\"start\":6514},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6667,\"start\":6663},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6773,\"start\":6769},{\"end\":8673,\"start\":8672},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9857,\"start\":9853},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10173,\"start\":10169},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10371,\"start\":10367},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10623,\"start\":10619},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10659,\"start\":10655},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11171,\"start\":11167},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11205,\"start\":11201},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11328,\"start\":11324},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11859,\"start\":11855},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12083,\"start\":12079},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12273,\"start\":12269},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12376,\"start\":12372},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12496,\"start\":12492},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12884,\"start\":12880},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13316,\"start\":13312},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14495,\"start\":14491},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14849,\"start\":14845},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15178,\"start\":15174},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15193,\"start\":15189},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15906,\"start\":15902},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16600,\"start\":16596},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16637,\"start\":16633},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16918,\"start\":16914},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16951,\"start\":16947},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19571,\"start\":19567},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23016,\"start\":23012},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23166,\"start\":23162},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28463,\"start\":28459},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30366,\"start\":30362},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30372,\"start\":30368},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30717,\"start\":30713},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30933,\"start\":30929},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31924,\"start\":31920},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33096,\"start\":33093},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33780,\"start\":33776},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34491,\"start\":34487},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36042,\"start\":36038},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36053,\"start\":36049},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36085,\"start\":36081},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36120,\"start\":36116},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36159,\"start\":36155}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43251,\"start\":43179},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44194,\"start\":43252},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44507,\"start\":44195},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44669,\"start\":44508},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44781,\"start\":44670},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44962,\"start\":44782},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":46213,\"start\":44963}]", "paragraph": "[{\"end\":4345,\"start\":3109},{\"end\":6223,\"start\":4347},{\"end\":7344,\"start\":6225},{\"end\":8531,\"start\":7346},{\"end\":9327,\"start\":8533},{\"end\":9638,\"start\":9374},{\"end\":10372,\"start\":9640},{\"end\":11016,\"start\":10374},{\"end\":11654,\"start\":11018},{\"end\":12587,\"start\":11704},{\"end\":13021,\"start\":12589},{\"end\":13152,\"start\":13042},{\"end\":13875,\"start\":13169},{\"end\":14026,\"start\":13936},{\"end\":14205,\"start\":14109},{\"end\":15542,\"start\":14231},{\"end\":15979,\"start\":15608},{\"end\":16423,\"start\":16017},{\"end\":17174,\"start\":16451},{\"end\":17234,\"start\":17176},{\"end\":17509,\"start\":17263},{\"end\":17721,\"start\":17540},{\"end\":18023,\"start\":17772},{\"end\":18832,\"start\":18044},{\"end\":19079,\"start\":18864},{\"end\":19572,\"start\":19081},{\"end\":19791,\"start\":19574},{\"end\":20259,\"start\":19821},{\"end\":20506,\"start\":20261},{\"end\":20944,\"start\":20508},{\"end\":21481,\"start\":20964},{\"end\":21913,\"start\":21557},{\"end\":22565,\"start\":21961},{\"end\":22699,\"start\":22567},{\"end\":23356,\"start\":22701},{\"end\":23885,\"start\":23456},{\"end\":25089,\"start\":23887},{\"end\":25551,\"start\":25091},{\"end\":25807,\"start\":25599},{\"end\":26255,\"start\":25809},{\"end\":26760,\"start\":26257},{\"end\":27493,\"start\":27039},{\"end\":27762,\"start\":27543},{\"end\":28073,\"start\":27801},{\"end\":28293,\"start\":28075},{\"end\":29057,\"start\":28331},{\"end\":29242,\"start\":29059},{\"end\":29883,\"start\":29244},{\"end\":31176,\"start\":29972},{\"end\":31247,\"start\":31242},{\"end\":31345,\"start\":31325},{\"end\":31507,\"start\":31374},{\"end\":31649,\"start\":31629},{\"end\":31925,\"start\":31783},{\"end\":32021,\"start\":31927},{\"end\":32798,\"start\":32023},{\"end\":33932,\"start\":32818},{\"end\":34510,\"start\":33956},{\"end\":34610,\"start\":34512},{\"end\":34720,\"start\":34658},{\"end\":35921,\"start\":34779},{\"end\":36507,\"start\":35966},{\"end\":37576,\"start\":36509},{\"end\":38327,\"start\":37578},{\"end\":38449,\"start\":38329},{\"end\":39518,\"start\":38507},{\"end\":39705,\"start\":39536},{\"end\":40444,\"start\":39707},{\"end\":41735,\"start\":40464},{\"end\":42032,\"start\":41737},{\"end\":43178,\"start\":42034}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13935,\"start\":13876},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14108,\"start\":14027},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15607,\"start\":15543},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16016,\"start\":15980},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17262,\"start\":17235},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17539,\"start\":17510},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17771,\"start\":17722},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23455,\"start\":23357},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25598,\"start\":25552},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26983,\"start\":26761},{\"attributes\":{\"id\":\"formula_10\"},\"end\":27038,\"start\":26983},{\"attributes\":{\"id\":\"formula_11\"},\"end\":27542,\"start\":27494},{\"attributes\":{\"id\":\"formula_12\"},\"end\":27800,\"start\":27763},{\"attributes\":{\"id\":\"formula_13\"},\"end\":29917,\"start\":29884},{\"attributes\":{\"id\":\"formula_14\"},\"end\":29971,\"start\":29917},{\"attributes\":{\"id\":\"formula_15\"},\"end\":31241,\"start\":31177},{\"attributes\":{\"id\":\"formula_16\"},\"end\":31324,\"start\":31248},{\"attributes\":{\"id\":\"formula_17\"},\"end\":31373,\"start\":31346},{\"attributes\":{\"id\":\"formula_18\"},\"end\":31568,\"start\":31508},{\"attributes\":{\"id\":\"formula_19\"},\"end\":31628,\"start\":31568},{\"attributes\":{\"id\":\"formula_20\"},\"end\":31782,\"start\":31650},{\"attributes\":{\"id\":\"formula_21\"},\"end\":34657,\"start\":34611}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33311,\"start\":33303}]", "section_header": "[{\"end\":3107,\"start\":3092},{\"end\":9372,\"start\":9330},{\"end\":11702,\"start\":11657},{\"end\":13040,\"start\":13024},{\"end\":13167,\"start\":13155},{\"end\":14229,\"start\":14208},{\"end\":16449,\"start\":16426},{\"end\":18042,\"start\":18026},{\"end\":18862,\"start\":18835},{\"end\":19819,\"start\":19794},{\"end\":20962,\"start\":20947},{\"end\":21555,\"start\":21484},{\"end\":21959,\"start\":21916},{\"end\":28329,\"start\":28296},{\"end\":32816,\"start\":32801},{\"end\":33954,\"start\":33935},{\"end\":34777,\"start\":34723},{\"end\":35964,\"start\":35924},{\"end\":38505,\"start\":38452},{\"end\":39534,\"start\":39521},{\"end\":40462,\"start\":40447},{\"end\":43188,\"start\":43180},{\"end\":43261,\"start\":43253},{\"end\":44212,\"start\":44196},{\"end\":44517,\"start\":44509},{\"end\":44679,\"start\":44671},{\"end\":44798,\"start\":44783}]", "table": "[{\"end\":44962,\"start\":44827}]", "figure_caption": "[{\"end\":43251,\"start\":43190},{\"end\":44194,\"start\":43263},{\"end\":44507,\"start\":44215},{\"end\":44669,\"start\":44519},{\"end\":44781,\"start\":44681},{\"end\":44827,\"start\":44800},{\"end\":46213,\"start\":44965}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18331,\"start\":18325},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20005,\"start\":19999},{\"end\":20443,\"start\":20437},{\"end\":22119,\"start\":22113},{\"end\":26443,\"start\":26436},{\"end\":35389,\"start\":35383},{\"end\":36843,\"start\":36837},{\"end\":37695,\"start\":37688},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":38973,\"start\":38967},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39076,\"start\":39070}]", "bib_author_first_name": "[{\"end\":47269,\"start\":47268},{\"end\":47277,\"start\":47276},{\"end\":47669,\"start\":47668},{\"end\":48054,\"start\":48053},{\"end\":48360,\"start\":48359},{\"end\":48676,\"start\":48675},{\"end\":48923,\"start\":48922},{\"end\":49251,\"start\":49250},{\"end\":49575,\"start\":49574},{\"end\":49577,\"start\":49576},{\"end\":49955,\"start\":49954},{\"end\":50278,\"start\":50277},{\"end\":50574,\"start\":50573},{\"end\":50588,\"start\":50587},{\"end\":50816,\"start\":50815},{\"end\":51099,\"start\":51098},{\"end\":51384,\"start\":51383},{\"end\":51628,\"start\":51627},{\"end\":51870,\"start\":51869},{\"end\":52128,\"start\":52127},{\"end\":52471,\"start\":52470},{\"end\":52728,\"start\":52727},{\"end\":52975,\"start\":52974},{\"end\":53254,\"start\":53253},{\"end\":53256,\"start\":53255},{\"end\":53548,\"start\":53547},{\"end\":53550,\"start\":53549},{\"end\":53961,\"start\":53960},{\"end\":54298,\"start\":54297},{\"end\":54503,\"start\":54502},{\"end\":54886,\"start\":54885},{\"end\":55135,\"start\":55134},{\"end\":55465,\"start\":55464},{\"end\":55795,\"start\":55794},{\"end\":56091,\"start\":56090},{\"end\":56420,\"start\":56419},{\"end\":56667,\"start\":56666},{\"end\":56902,\"start\":56901},{\"end\":56916,\"start\":56915},{\"end\":57113,\"start\":57112},{\"end\":57371,\"start\":57370},{\"end\":57686,\"start\":57685},{\"end\":57992,\"start\":57991},{\"end\":58004,\"start\":58003},{\"end\":58011,\"start\":58010},{\"end\":58236,\"start\":58235},{\"end\":58454,\"start\":58453},{\"end\":58698,\"start\":58697},{\"end\":58913,\"start\":58912},{\"end\":59142,\"start\":59138},{\"end\":59152,\"start\":59148},{\"end\":59456,\"start\":59455},{\"end\":59800,\"start\":59799},{\"end\":59811,\"start\":59810},{\"end\":60143,\"start\":60142},{\"end\":60145,\"start\":60144},{\"end\":60434,\"start\":60432},{\"end\":64144,\"start\":64143}]", "bib_author_last_name": "[{\"end\":47274,\"start\":47270},{\"end\":47282,\"start\":47278},{\"end\":47672,\"start\":47670},{\"end\":48059,\"start\":48055},{\"end\":48365,\"start\":48361},{\"end\":48685,\"start\":48677},{\"end\":48927,\"start\":48924},{\"end\":49256,\"start\":49252},{\"end\":49584,\"start\":49578},{\"end\":49960,\"start\":49956},{\"end\":50283,\"start\":50279},{\"end\":50585,\"start\":50575},{\"end\":50595,\"start\":50589},{\"end\":50825,\"start\":50817},{\"end\":51102,\"start\":51100},{\"end\":51387,\"start\":51385},{\"end\":51634,\"start\":51629},{\"end\":51877,\"start\":51871},{\"end\":52134,\"start\":52129},{\"end\":52475,\"start\":52472},{\"end\":52732,\"start\":52729},{\"end\":52983,\"start\":52976},{\"end\":53263,\"start\":53257},{\"end\":53555,\"start\":53551},{\"end\":53968,\"start\":53962},{\"end\":54303,\"start\":54299},{\"end\":54511,\"start\":54504},{\"end\":54894,\"start\":54887},{\"end\":55476,\"start\":55466},{\"end\":55801,\"start\":55796},{\"end\":56098,\"start\":56092},{\"end\":56425,\"start\":56421},{\"end\":56673,\"start\":56668},{\"end\":56913,\"start\":56903},{\"end\":56923,\"start\":56917},{\"end\":57117,\"start\":57114},{\"end\":57380,\"start\":57372},{\"end\":57693,\"start\":57687},{\"end\":58001,\"start\":57993},{\"end\":58008,\"start\":58005},{\"end\":58018,\"start\":58012},{\"end\":58241,\"start\":58237},{\"end\":58462,\"start\":58455},{\"end\":58922,\"start\":58914},{\"end\":59146,\"start\":59143},{\"end\":59156,\"start\":59153},{\"end\":59460,\"start\":59457},{\"end\":59808,\"start\":59801},{\"end\":59816,\"start\":59812},{\"end\":60152,\"start\":60146},{\"end\":60438,\"start\":60435},{\"end\":64159,\"start\":64145}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":216168000},\"end\":47541,\"start\":47159},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207852519},\"end\":47918,\"start\":47543},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":199582571},\"end\":48273,\"start\":47920},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10045756},\"end\":48610,\"start\":48275},{\"attributes\":{\"doi\":\"arXiv:1607.00148\",\"id\":\"b4\"},\"end\":48840,\"start\":48612},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51881855},\"end\":49161,\"start\":48842},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":199011204},\"end\":49506,\"start\":49163},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":56482410},\"end\":49856,\"start\":49508},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":204077191},\"end\":50179,\"start\":49858},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":204077051},\"end\":50524,\"start\":50181},{\"attributes\":{\"doi\":\"arXiv:1901.03407\",\"id\":\"b10\"},\"end\":50743,\"start\":50526},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":43680425},\"end\":51031,\"start\":50745},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":216226369},\"end\":51323,\"start\":51033},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":216267212},\"end\":51540,\"start\":51325},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":57756774},\"end\":51795,\"start\":51542},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8798021},\"end\":52046,\"start\":51797},{\"attributes\":{\"id\":\"b16\"},\"end\":52393,\"start\":52048},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":213175498},\"end\":52649,\"start\":52395},{\"attributes\":{\"doi\":\"arXiv:2002.12301\",\"id\":\"b18\"},\"end\":52895,\"start\":52651},{\"attributes\":{\"doi\":\"arXiv:1907.10147\",\"id\":\"b19\"},\"end\":53151,\"start\":52897},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":19002372},\"end\":53459,\"start\":53153},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":49317514},\"end\":53832,\"start\":53461},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":947305},\"end\":54218,\"start\":53834},{\"attributes\":{\"id\":\"b23\"},\"end\":54405,\"start\":54220},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":17427022},\"end\":54812,\"start\":54407},{\"attributes\":{\"doi\":\"arXiv:1610.05492\",\"id\":\"b25\"},\"end\":55058,\"start\":54814},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":44113205},\"end\":55360,\"start\":55060},{\"attributes\":{\"doi\":\"arXiv:1909.13014\",\"id\":\"b27\"},\"end\":55672,\"start\":55362},{\"attributes\":{\"doi\":\"arXiv:1811.11479\",\"id\":\"b28\"},\"end\":56010,\"start\":55674},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13070099},\"end\":56323,\"start\":56012},{\"attributes\":{\"doi\":\"arXiv:1910.13111\",\"id\":\"b30\"},\"end\":56607,\"start\":56325},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14542261},\"end\":56844,\"start\":56609},{\"attributes\":{\"id\":\"b32\"},\"end\":57020,\"start\":56846},{\"attributes\":{\"doi\":\"arXiv:1712.01887\",\"id\":\"b33\"},\"end\":57294,\"start\":57022},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1193239},\"end\":57605,\"start\":57296},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":13070099},\"end\":57918,\"start\":57607},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b36\"},\"end\":58195,\"start\":57920},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":17195923},\"end\":58424,\"start\":58197},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13756489},\"end\":58637,\"start\":58426},{\"attributes\":{\"doi\":\"arXiv:1811.04017\",\"id\":\"b39\"},\"end\":58847,\"start\":58639},{\"attributes\":{\"doi\":\"arXiv:1607.00148\",\"id\":\"b40\"},\"end\":59076,\"start\":58849},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":44126005},\"end\":59342,\"start\":59078},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":53639324},\"end\":59686,\"start\":59344},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":198464990},\"end\":60040,\"start\":59688},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":19002372},\"end\":60348,\"start\":60042},{\"attributes\":{\"id\":\"b45\"},\"end\":61024,\"start\":60350},{\"attributes\":{\"id\":\"b46\"},\"end\":62919,\"start\":61026},{\"attributes\":{\"id\":\"b47\"},\"end\":63732,\"start\":62921},{\"attributes\":{\"id\":\"b48\"},\"end\":65750,\"start\":63734},{\"attributes\":{\"id\":\"b49\"},\"end\":67393,\"start\":65752}]", "bib_title": "[{\"end\":47266,\"start\":47159},{\"end\":47666,\"start\":47543},{\"end\":48051,\"start\":47920},{\"end\":48357,\"start\":48275},{\"end\":48920,\"start\":48842},{\"end\":49248,\"start\":49163},{\"end\":49572,\"start\":49508},{\"end\":49952,\"start\":49858},{\"end\":50275,\"start\":50181},{\"end\":50813,\"start\":50745},{\"end\":51096,\"start\":51033},{\"end\":51381,\"start\":51325},{\"end\":51625,\"start\":51542},{\"end\":51867,\"start\":51797},{\"end\":52125,\"start\":52048},{\"end\":52468,\"start\":52395},{\"end\":53251,\"start\":53153},{\"end\":53545,\"start\":53461},{\"end\":53958,\"start\":53834},{\"end\":54500,\"start\":54407},{\"end\":55132,\"start\":55060},{\"end\":56088,\"start\":56012},{\"end\":56664,\"start\":56609},{\"end\":57368,\"start\":57296},{\"end\":57683,\"start\":57607},{\"end\":58233,\"start\":58197},{\"end\":58451,\"start\":58426},{\"end\":59136,\"start\":59078},{\"end\":59453,\"start\":59344},{\"end\":59797,\"start\":59688},{\"end\":60140,\"start\":60042},{\"end\":60430,\"start\":60350},{\"end\":61259,\"start\":61026},{\"end\":63022,\"start\":62921},{\"end\":64141,\"start\":63734},{\"end\":65901,\"start\":65752}]", "bib_author": "[{\"end\":47276,\"start\":47268},{\"end\":47284,\"start\":47276},{\"end\":47674,\"start\":47668},{\"end\":48061,\"start\":48053},{\"end\":48367,\"start\":48359},{\"end\":48687,\"start\":48675},{\"end\":48929,\"start\":48922},{\"end\":49258,\"start\":49250},{\"end\":49586,\"start\":49574},{\"end\":49962,\"start\":49954},{\"end\":50285,\"start\":50277},{\"end\":50587,\"start\":50573},{\"end\":50597,\"start\":50587},{\"end\":50827,\"start\":50815},{\"end\":51104,\"start\":51098},{\"end\":51389,\"start\":51383},{\"end\":51636,\"start\":51627},{\"end\":51879,\"start\":51869},{\"end\":52136,\"start\":52127},{\"end\":52477,\"start\":52470},{\"end\":52734,\"start\":52727},{\"end\":52985,\"start\":52974},{\"end\":53265,\"start\":53253},{\"end\":53557,\"start\":53547},{\"end\":53970,\"start\":53960},{\"end\":54305,\"start\":54297},{\"end\":54513,\"start\":54502},{\"end\":54896,\"start\":54885},{\"end\":55138,\"start\":55134},{\"end\":55478,\"start\":55464},{\"end\":55803,\"start\":55794},{\"end\":56100,\"start\":56090},{\"end\":56427,\"start\":56419},{\"end\":56675,\"start\":56666},{\"end\":56915,\"start\":56901},{\"end\":56925,\"start\":56915},{\"end\":57119,\"start\":57112},{\"end\":57382,\"start\":57370},{\"end\":57695,\"start\":57685},{\"end\":58003,\"start\":57991},{\"end\":58010,\"start\":58003},{\"end\":58020,\"start\":58010},{\"end\":58243,\"start\":58235},{\"end\":58464,\"start\":58453},{\"end\":58701,\"start\":58697},{\"end\":58924,\"start\":58912},{\"end\":59148,\"start\":59138},{\"end\":59158,\"start\":59148},{\"end\":59462,\"start\":59455},{\"end\":59810,\"start\":59799},{\"end\":59818,\"start\":59810},{\"end\":60154,\"start\":60142},{\"end\":60440,\"start\":60432},{\"end\":64161,\"start\":64143}]", "bib_venue": "[{\"end\":47335,\"start\":47284},{\"end\":47705,\"start\":47674},{\"end\":48072,\"start\":48061},{\"end\":48415,\"start\":48367},{\"end\":48673,\"start\":48612},{\"end\":48981,\"start\":48929},{\"end\":49309,\"start\":49258},{\"end\":49658,\"start\":49586},{\"end\":49996,\"start\":49962},{\"end\":50330,\"start\":50285},{\"end\":50571,\"start\":50526},{\"end\":50860,\"start\":50827},{\"end\":51160,\"start\":51104},{\"end\":51423,\"start\":51389},{\"end\":51647,\"start\":51636},{\"end\":51896,\"start\":51879},{\"end\":52196,\"start\":52136},{\"end\":52508,\"start\":52477},{\"end\":52725,\"start\":52651},{\"end\":52972,\"start\":52897},{\"end\":53284,\"start\":53265},{\"end\":53619,\"start\":53557},{\"end\":53999,\"start\":53970},{\"end\":54295,\"start\":54220},{\"end\":54582,\"start\":54513},{\"end\":54883,\"start\":54814},{\"end\":55187,\"start\":55138},{\"end\":55462,\"start\":55362},{\"end\":55792,\"start\":55674},{\"end\":56149,\"start\":56100},{\"end\":56417,\"start\":56325},{\"end\":56698,\"start\":56675},{\"end\":56899,\"start\":56846},{\"end\":57110,\"start\":57022},{\"end\":57431,\"start\":57382},{\"end\":57744,\"start\":57695},{\"end\":57989,\"start\":57920},{\"end\":58292,\"start\":58243},{\"end\":58513,\"start\":58464},{\"end\":58695,\"start\":58639},{\"end\":58910,\"start\":58849},{\"end\":59190,\"start\":59158},{\"end\":59498,\"start\":59462},{\"end\":59840,\"start\":59818},{\"end\":60173,\"start\":60154},{\"end\":60524,\"start\":60440},{\"end\":61328,\"start\":61261},{\"end\":63113,\"start\":63024},{\"end\":64167,\"start\":64161},{\"end\":66203,\"start\":65903},{\"end\":60561,\"start\":60526},{\"end\":61367,\"start\":61330},{\"end\":63138,\"start\":63115}]"}}}, "year": 2023, "month": 12, "day": 17}