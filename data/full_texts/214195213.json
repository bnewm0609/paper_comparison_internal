{"id": 214195213, "updated": "2023-04-27 14:08:47.946", "metadata": {"title": "Rethinking the Image Fusion: A Fast Uni\ufb01ed Image Fusion Network based on Proportional Maintenance of Gradient and Intensity", "authors": "[{\"first\":\"Hao\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Han\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Xiao\",\"middle\":[]},{\"first\":\"Xiaojie\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Jiayi\",\"last\":\"Ma\",\"middle\":[]}]", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "In this paper, we propose a fast uni\ufb01ed image fusion network based on proportional maintenance of gradient and intensity (PMGI), which can end-to-end realize a variety of image fu- sion tasks, including infrared and visible image fusion, multi-exposure image fusion, medical image fusion, multi-focus image fusion and pan-sharpening. We unify the image fusion problem into the texture and intensity proportional mainte- nance problem of the source images. On the one hand, the network is divided into gradient path and intensity path for information extraction. We perform feature reuse in the same path to avoid loss of information due to convolution. At the same time, we introduce the pathwise transfer block to exchange information between different paths, which can not only pre-fuse the gradient information and intensity information, but also enhance the information to be processed later. On the other hand, we de\ufb01ne a uniform form of loss function based on these two kinds of information, which can adapt to different fusion tasks. Experiments on publicly available datasets demonstrate the superiority of our PMGI over the state-of-the-art in terms of both visual effect and quantitative metric in a variety of fusion tasks. In addition, our method is faster compared with the state-of-the-art.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2998012573", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/ZhangXXGM20", "doi": "10.1609/aaai.v34i07.6975"}}, "content": {"source": {"pdf_hash": "6a4c0d60b4ecf95ce2d51d7206a10652a616e269", "pdf_src": "Anansi", "pdf_uri": "[\"https://ojs.aaai.org/index.php/AAAI/article/download/6975/6829\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/6975/6829", "status": "GOLD"}}, "grobid": {"id": "afa2140d4b8874edf9332a3a65e52755b81458a5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6a4c0d60b4ecf95ce2d51d7206a10652a616e269.txt", "contents": "\nRethinking the Image Fusion: A Fast Unified Image Fusion Network based on Proportional Maintenance of Gradient and Intensity\n\n\nHao Zhang \nElectronic Information School\nWuhan University\n430072WuhanChina\n\nHan Xu xuhan@whu.edu.cn \nElectronic Information School\nWuhan University\n430072WuhanChina\n\nYang Xiao yangxiao@hust.edu.cn \nSchool of Automation\nHuazhong University of Science and Technology\n430074WuhanChina\n\nXiaojie Guo \nCollege of Intelligence and Computing\nTianjin University\n300350TianjinChina\n\nJiayi Ma \nElectronic Information School\nWuhan University\n430072WuhanChina\n\nRethinking the Image Fusion: A Fast Unified Image Fusion Network based on Proportional Maintenance of Gradient and Intensity\n\nIn this paper, we propose a fast unified image fusion network based on proportional maintenance of gradient and intensity (PMGI), which can end-to-end realize a variety of image fusion tasks, including infrared and visible image fusion, multiexposure image fusion, medical image fusion, multi-focus image fusion and pan-sharpening. We unify the image fusion problem into the texture and intensity proportional maintenance problem of the source images. On the one hand, the network is divided into gradient path and intensity path for information extraction. We perform feature reuse in the same path to avoid loss of information due to convolution. At the same time, we introduce the pathwise transfer block to exchange information between different paths, which can not only pre-fuse the gradient information and intensity information, but also enhance the information to be processed later. On the other hand, we define a uniform form of loss function based on these two kinds of information, which can adapt to different fusion tasks. Experiments on publicly available datasets demonstrate the superiority of our PMGI over the state-of-the-art in terms of both visual effect and quantitative metric in a variety of fusion tasks. In addition, our method is faster compared with the state-of-the-art.\n\nIntroduction\n\nImage fusion is to extract the most meaningful information from images acquired by different sensors, and combine the information to generate a single image, which contains more abundant information and is more conducive to subsequent applications. Common image fusions include infrared and visible image fusion, multi-exposure image fusion, multi-focus image fusion, medical image fusion, and remote sensing image fusion (also called pan-sharpening). They are used in target detection, HDTV, medical diagnosis and other fields (Ma, Ma, and Li 2019;Ma et al. 2017;Xing et al. 2018).\n\nExisting fusion methods can be divided into two categories. i) Traditional methods. These methods usually use the related mathematical transformation and manually designed fusion rules to get fused images. Paul et al. (Paul, Sevcenco, and Agathoklis 2016) propose a general algorithm for multi-focus and multi-exposure image fusion, which is based on blending the gradients of the luminance components of source images using the maximum gradient magnitude at each pixel location and then obtaining the fused luminance using a Haar wavelet-based image reconstruction technique. The pyramid methods achieve medical image fusion by the diverse resolutions in the level and the iteration of the images (Patil and Mudengudi 2011). Rahmani et al. (Rahmani et al. 2010) come up with an adaptive IHS pan-sharpening Method, which produces images with higher spectral resolution while maintaining the high-quality spatial resolution of the original IHS. ii) Deep learningbased methods. Due to the strong fitting ability of neural networks, these methods can usually achieve better fused results by designing objective functions. In multi-focus image fusion, Liu et al. ) used the convolutional neural network to jointly generate activity level measurement and fusion rules. Ma et al. (Ma et al. 2019a) proposed an unsupervised network to generate the decision map for fusion. In multi-exposure image fusion, Prabhakar et al. (Ram Prabhakar, Sai Srikar, and Venkatesh Babu 2017) proposed an unsupervised deep learning framework that utilizes a no-reference quality metric MEF-SSIM (Ma, Zeng, and Wang 2015) as the loss function and can produce satisfactory fused results. In medical image fusion, PA-PCNN proposed by Yin et al. (Yin et al. 2018) can realize the fusion of different types of medical images, which is a novel method in nonsubsampled shearlet transform (NSST) domain. Ma et al. (Ma et al. 2019b) proposed an end-to-end model called FusionGAN, which generates a fused image with dominant infrared intensities and additional visible gradients on the basis of GAN. Subsequently, they proposed to use a dual-discriminator to avoid loss of information in source images (Xu et al. 2019). In addition, they also introduced a detail loss and a target edge-enhancement loss based on FusionGAN to further enhance the texture details in the fused results (Ma et al. 2020).\n\nAlthough existing methods can achieve good results in corresponding fusion tasks, there are still several aspects to be improved. First, existing methods usually need to manually design the activity level measurement and fusion rules.\n\nConsidering the diversity of source images, it will become more and more complex. Second, most methods are only applicable for specific fusion tasks and not generalized. It is significant to design a general method from the essence of image fusion. Third, because of computational complexity and large parameter quantities, existing fusion methods are often less competitive in terms of time.\n\nTo address these challenges, we propose a fast unified image fusion network based on proportional maintenance of gradient and intensity (PMGI), which can end-to-end implement various types of image fusion tasks with high efficiency. Firstly, PMGI is an end-to-end model with source images as the input and the fused image as the output, without any manual intervention in the middle. Secondly, we transform the fusion problem as the maintenance of gradient and intensity information. The intensity information gives the fused image similar histogram distribution with source images, while the gradient information gives finer texture details. Accordingly, we define the loss function of a uniform form for multiple image fusion tasks. To make the network adapt to different image fusion tasks, we can selective more effective and interesting information to be preserved in the fused result by adjusting the weight of each loss item. Finally, we divide the network into the gradient path and the intensity path to extract the corresponding information in source images respectively. In order to minimize the information loss caused by convolution, features of each layer in the same extraction path are reused. We also introduce the pathwise transfer block between the two paths. On the one hand, it can pre-fuse gradient and intensity information. On the other hand, it can enhance the information to be processed later. It is worth noting that due to the use of 1 \u00d7 1 convolution kernels and the control of the number of feature channels, the quantity of parameters in our network is limited within a certain range. As a result, our method can achieve fusion at a high speed.\n\nThe contributions of our work include the following three aspects:\n\n\u2022 We propose a new end-to-end image fusion network, which can realize a variety of image fusion tasks uniformly. The proposed PMGI can fuse infrared and visible images, multi-exposure images, medical images, multifocus images and remote sensing images well. \u2022 We design a specific loss function, which is applicable to almost all image fusion tasks and can achieve expected results by adjusting the weight of each loss item. \u2022 Our method can perform image fusion with higher efficiency in multiple fusion tasks. The code is available at: https://github.com/HaoZhang1018/PMGI AAAI2020.\n\n\nMethod Problem Formulation\n\nThe essence of image fusion is to combine the most important information in source images, and generate a single image with richer information and better visual effects. In different image fusion tasks, there are great differences in the properties of source images, and it is inappropriate to treat them in the same way. However, in most cases, the two types of source images maintain an underlying correlation, as they characterize the same scene and the source images contain the complementary information. So we try to solve different kinds of fusion tasks in a unified manner through reasonable design of network architecture and loss function.\n\nAs the most essential element of the image is the pixel, the intensity of pixels can represent the histogram distribution of the image, and the difference between the pixels constitutes the gradient, which can represent the texture details in the image. Thus, we characterize the whole image with information from these two aspects: the gradient and the pixel intensity. And it is reflected in both the network architecture and the loss function.\n\nWe divide the network into two information extraction paths: the gradient path and the intensity path. For the gradient path, it is responsible for extracting texture information, i.e., high-frequency features. Similarly, for the intensity path, it is responsible for the extraction of intensity information. Because both the gradient information and the intensity information are needed to be extracted and preserved from two types of source images at the same time, the input of each information extraction path is composed of different source images concatenated along the channel dimension for remaining potentially relevant. We set the concatenating ratio of these two source images as \u03b2. In addition, we also perform feature reuse and information exchange operations. First of all, loss of information is inevitable during the convolution process. Feature reuse can reduce information loss to a certain extent and increase the feature utilization. The exchange of different kinds of information between the information extraction paths can pre-fuse the gradient and the intensity information, and is also an enhancement of the information before the next extraction.\n\nIn addition to the general network structure described above, we also design a loss function with a uniform form based on the nature of the image. We transform the image fusion problem into the proportional maintenance problem of the gradient and the intensity information. Our loss function consists of two types of loss terms: the gradient loss and the intensity loss. They are both constructed for two source images. Separately, the intensity constraint can provide a rough pixel distribution, while the gradient constraint can enhance the texture details. And the joint constraint of them can achieve a satisfactory fused result with the reasonable intensity distribution and rich texture details. Because the fused image cannot preserve all the information of source images, we have to trade-off between the intensity distribution and texture details to preserve more important gradient and intensity information. Therefore, we can adjust the weight of each loss term to change the proportion of various types of information, so that it can be adapted to different image fusion tasks.\n\n\nNetwork Architecture\n\nThe proposed PMGI is a very fast convolutional neural network. As shown in Fig. 1, we divide the network into the gradient and intensity path for corresponding information extraction. Gradient and intensity information is communi- cated through the pathwise transfer block. It is worth noting that after many trials, the concatenating ratio \u03b2 of the two source images in the inputs is determined to be 1 : 2.\n\nIn both paths, we use four convolutional layers for feature extraction. Using the idea of DenseNet (Huang et al. 2017) for reference, we carry on dense connection in the same path to realize feature reuse. In addition, the pathwise transfer block is used to communicate information across these two paths, so the input of the third and fourth convolution layers depends not only on the output of all the previous convolution layers, but also on the output of the convolution layer in the other path. The first layer uses 5 \u00d7 5 convolution kernels and the latter three layers use 3 \u00d7 3 convolution kernels combined with batch normalization and leaky ReLU activation function. The structure of the pathwise transfer block is also shown in the lower right corner in Fig. 1. It uses a 1 \u00d7 1 convolution kernel combined with batch normalization and leaky ReLU activation function.\n\nThen, we use the strategies of concatenating and convolution to fuse the features extracted from the two paths. We concatenate the two feature maps along the channel. It is worth noting that the idea of feature reuse is still used here. Eight feature maps involved in the concatenating come from a total of eight convolution layers of two paths. The kernel size of the last convolutional layer is 1\u00d71, and the activation function is tanh. In all convolution layers, the padding is set to SAM E and stride is set to 1. As a result, none of these convolutional layers change the size of the feature map.\n\n\nLoss Function\n\nThe loss function determines the type of information extracted and the proportional relationship between various types of information. The loss function of our network consists of two types of loss terms, the intensity loss and the gradient loss. The intensity loss constrains the fused image to maintain a similar intensity distribution as the source images, while the gradient loss forces the fused image to contain rich texture details. It is important to note that we construct these two types of losses for each source image. Therefore, the loss function contains four terms, which are expressed as:\nL P MGI = \u03bb Aint L Aint +\u03bb Agrad L Agrad +\u03bb Bint L Bint +\u03bb Bgrad L Bgrad ,\n(1) where A and B refer to the two source images, L (\u00b7)int represents the intensity loss term for one source image and L (\u00b7)grad means the corresponding gradient constraint. \u03bb (\u00b7) is the weight of each loss item.\n\nThe intensity loss is defined as:\nL Aint = 1 HW I fused \u2212 I A 2 2 , L Bint = 1 HW I fused \u2212 I B 2 2 ,(2)\nwhere I fused is the fused image generated by PMGI. I A and I B are two source images. H and W are the height and width of the image respectively. Similarly, with \u2207 denotes the gradient operator, definition of gradient loss is as follows:\nL Agrad = 1 HW \u2207I fused \u2212 \u2207I A 2 2 , L Bgrad = 1 HW \u2207I fused \u2212 \u2207I B 2 2 .(3)\nNote that \u03bb (\u00b7) in Eq.\n\n(1) can be adjusted to change the proportion of various types of information in the fused image to adapt to different tasks. The parameter setting rules corresponding to specific tasks are summarized below.\n\nFor infrared and visible image fusion, we expect that the gradient information of visible images and the intensity information of infrared images are mainly preserved in the fused results, while the intensity information of visible images and the gradient information of infrared images are of secondary importance. Then the parameter \u03bb (\u00b7) should meet the following setting rules:\n\u03bb irint > \u03bb visint , \u03bb irgrad < \u03bb visgrad .(4)\nFor multi-exposure image fusion, both overexposed and underexposed images contain equal texture details, but their intensities are too strong or too weak. So we set the same weights to balance them to get the appropriate intensity and rich texture details, which can be formalized as:\n\u03bb overint = \u03bb underint , \u03bb overgrad = \u03bb undergrad .(5)\nFor multi-focus image fusion, the two types of information (gradient and intensity) of the two source images are equally important. This is because we want to maintain the intensity and texture information of the two source images at the same time, and the defocused (blurred) regions can be supplemented by the focused (clear) regions in the other source image. Therefore, it is also necessary to set the corresponding parameters to remain the same:\n\u03bb focus1int = \u03bb focus2int , \u03bb focus1grad = \u03bb focus2grad . (6)\nSimilarly, for medical image fusion, the structural medical image reflects the texture information of the organ, and the functional medical image indicates the functional information, such as the metabolic intensity. We use MRI and PET images as the example of the structural image and the functional image respectively. We obtain the main texture information from the MRI image and the main intensity information from the PET image. However, considering that the pixel intensity of the I component of the PET image is much larger than the MRI, if the pixel intensity of the PET image is mainly constrained, the excessive intensity of the fused image masks the texture. So in order to balance the texture and intensity, we make the pixel intensity of PET and MRI equally constrained. So \u03bb (\u00b7) should be set as:\n\u03bb P ETint = \u03bb MRIint , \u03bb P ETgrad < \u03bb MRIgrad .(7)\nFinally, for pan-sharpening, the panchromatic image has a high spatial resolution (rich texture details), and the multispectral image contains rich color information. The purpose is to improve clarity while keeping the spectrum undistorted. Therefore, we only constrain the texture information of the panchromatic image without constraining the intensity to avoid spectral distortion, which can be formalized as:  (Cai, Gu, and Zhang 2018) for task2; MRI and PET images from Harvard medical school website 3 for task3; the dataset provided by (Nejati, Samavi, and Shirani 2015) and sametaymaz 4 for task4; and the panchromatic and multi-spectral images generated by the Quickbird satellite for task5.\n\u03bb P ANint = 0, \u03bb P ANgrad > \u03bb MSgrad .(8\nFor these five fusion tasks, we enter the entire image as test data into the network. The number of image pairs used for testing is 17, 19, 20, 18 and 50, respectively. In order to obtain more training data, we adopt the expansion strategy of tailoring and decomposition. In particular, we crop each image into 120 \u00d7 120 image patches in all tasks except multi-focus image fusion. For the multi-focus image fusion task, we expect that there is only one attribute (focused or defocused) in each image patch. So we crop each image into image patches with smaller size 60 \u00d7 60. The number of patch pairs for training is 20036, 33961, 4698, 139001, and 210000, respectively.\n\nTraining Details Among these image fusion tasks, source images and fused images in infrared and visible image fusion and multi-focus image fusion tasks are grayscale images, while those in the other fusion tasks are color images. In medical image fusion, the multi-spectral PET image with RGB channels are firstly transformed to IHS channels. We fuse the I component and the MRI image. Then the fused result is concatenated with H and S components and transformed to get the final RGB fused result. For multi-exposure image fusion, we transform the images from RGB to YCbCr color space. Because the Y channel (luminance channel) can represent structural details and the brightness variation, we just devote to fusing the Y channel values. And for Cb and Cr channels (chrominance channels), we fuse them in a traditional way. Then, the fused components of these channels are transferred to RGB color space to obtain the final result. In the pan-sharpening task, the multi-spectral image contains four channels and the panchromatic image is onechannel. The resolution ratio of them is 1 : 4. First, we use the bicubic method to upsample the multi-spectral image to the same size as the panchromatic image. Then, each channel of the multi-spectral image is fused with the panchromatic image separately to obtain four one-channel fused results. Finally, they are concatenated to obtain the final fused image.\n\nIt is worth noting that all deep learning based methods run on the same GPU RTX 2080Ti, while other methods run on the same CPU i7-8750H.\n\n\nResults on Infrared and Visible Image Fusion\n\nIn infrared and visible image fusion, we compare our PMGI with five existing methods: LPP (Toet 1989), GTF (Ma et al. 2016), DDLatLRR (Li and Wu 2018a), LatLRR (Li and Wu 2018b) and FusionGAN (Ma et al. 2019b). Qualitative and quantitative experimental results are shown below.  Qualitative Comparisons Comparison results on three typical image pairs qualitatively demonstrate the superiority of our method, as shown in Fig. 2. According to the characteristics of fused results, these methods can be divided into two categories. In the first category, fused results are biased towards visible images, which contain rich textures but lose thermal radiation information, such as LatLRR. Results of the other type of methods are closer to infrared images in the respect of contrast, but the texture details are not rich enough, such as GTF and FusionGAN. By comparison, our method is more like a combination of these two categories. First, our results can maintain thermal radiation information by effectively highlighting the target, such as the lake in the first row and the human in the third row. At the same time, our results contain rich texture details, such as the grass in the first row and the car in the third row, which are more in line with human observation characteristics.\n\n\nQuantitative Comparisons\n\nIn order to validate more comprehensively, we conduct quantitative comparisons on seventeen image pairs. We used six objective metrics to evaluate the results: entropy (EN), sum of the correlations of differences (SCD) (Aslantas and Bendes 2015), correlation coefficient (CC), Feature mutual information (FMI), standard deviation (SD) and mutual information (MI).\n\nThe quantitative results are shown in Fig. 3. Our method achieves the largest average values on four metrics including EN, SCD, CC and SD. From these results, we can conclude that our results contain the largest amount of information, strong correlation with source images and significant contrast. In addition, our results contains comparable amount of feature information transferred from source images, as our method only ranks second behind DDLatLRR on FMI. Last but not least, although GTF and FusionGAN have larger values on MI, our method achieves a better balance between the information of two source images.\n\nThe average run time of LPP, GTF, DDLatLRR, LatLRR, FusionGAN and our PMGI on the test data is about 0.1156, 4.5426, 49.4816, 133.2983, 0.2027 and 0.0831 (unit:second). Obviously, our PMGI is the fastest.\n\n\nResults on Multi-exposure Image Fusion\n\nTo verify the effectiveness in multi-exposure image fusion, we compare it with three existing methods: DSIFT (Hayat and Imran 2019), EF (Mertens, Kautz, and Van Reeth 2007) and AWPIGG (Lee, Park, and Cho 2018).\n\nQualitative Experiment We firstly perform qualitative experiments on three typical image sequences. The fused results are shown in Fig. 4. We analyze them from two aspects: the whole image and local details. As for the whole image, compared with other methods, our results are more suitable for human eye perception with more vivid colors and more appropriate exposure. And in our results, there are no strange shadows like those in the results of the comparative methods, such as dark shadows in the clouds. As for local details, our results have clearer and more realistic textures such as grasses and rocks highlighted in images. In general, our method has great advantages over the comparative methods.\n\nQuantitative Experiment We still choose EN, SCD, CC, FMI, SD and MI described above to evaluate fused results. The comparison results on nineteen image pairs are shown in Fig. 5. As for EN, SCD, CC, SD and MI, our results can achieve the highest average values. From these results, we can see that our fused results have the most information, the strongest correlation with the source images, the most information from the source images, and the best contrast. In addition, although our method follows behind DSIFT, EF and AWPIGG on FMI, we can also see that our results get comparable feature information from source images, because the values of all the methods are very close.\n\nThe average run time of DSIFT, EF, AWPIGG and PMGI is 1.3307, 0.0053, 0.4887 and 0.0908 (unit: second). PMGI is second only to EF, but still stay within 100 milliseconds.\n\n\nResults on Medical Image Fusion\n\nWe employ three existing medical image fusion methods to compare with our PMGI: DCHWT (Kumar 2013), ASR (Liu and Wang 2014) and PCA (Naidu and Raol 2008).\n\nQualitative Experiment Three typical and intuitive results on three different transaxial sections of the brainhemispheric are exhibited in Fig. 6. Among them, the fused results of PCA can retain the functional information (presented as color) of PET images, but lose much texture structure information (presented as texture details) of MRI images. On the contrary, the fused results of DCHWT have significant structural information, but reduce the color intensity in PET images. Results of ASR, DCHWT and our  method not only have vivid color intensity, but also contain rich texture structure. But by contrast, the texture details of our results are more clear, as shown in the highlighted parts.\n\nQuantitative Experiment Because of the particularity of medical images, we need to strictly control the preservation of edge information, which represents the shape and area of the organ. So we add the metric Q AB/F to measure the edge information that is transferred from source images to the fused image. In addition, spatial frequency (SF) and mean gradient (MG) are added to evaluate the gradient information in order to measure the degree of structural information (texture details) retention of MRI images. Therefore, the six objective metrics we select to evaluate medical fused images are EN, FMI, MG, MI, SF and Q AB/F . The quantitative results on twenty image pairs are shown in Fig. 7. As can be seen from the statistical results, our PMGI can generate the largest average values on the five metrics: EN, FMI, MG, SF and Q AB/F . It demonstrates that our results have the most information, the most abundant texture details, and the most feature and edge information from source images. As for MI, our method ranks second only to PCA. It is due to the reason that our results are more balanced between structural information and functional information compared with PCA.\n\nThe average run time of DCHWT, ASR, PCA and our PMGI is about 0.8636, 23.6055, 0.0046, and 0.0150 (unit: second). Our method is second to PCA, but is much faster than the other two methods.  \n\n\nResults on Multi-focus Image Fusion\n\nFive multi-focus image fusion methods are selected to compare with our PMGI: CNN ), Dct-Var (Haghighat, Aghagolzadeh, and Seyedarabi 2011), DWTDE (Liu and Wang 2013), GD (Paul, Sevcenco, and Agathoklis 2016) and QBMF (Bai et al. 2015).\n\nQualitative Experiment To give intuitive results, we select three typical image pairs for qualitative evaluation, as shown in Fig. 8. It can be seen from the results that there are some intensity distortions and unnatural black shadows in the fused results of GD. The results of DWTDE have a good visual effect, which can restore the details of far and nearfocused regions clearly. Although there is a slight of loss of details of the far-focused regions in the results of QBMF and our method, they can also achieve relatively satisfactory results. In addition, it should be noted that our method is a general end-to-end fusion method. Considering the universality, we do not perform multi-focus image fusion by detecting the focused areas but extracting and fusing the texture and intensity information of source images. Nevertheless, the qualitative results still prove the effectiveness of our method.\n\nQuantitative Experiment The essence of multi-focus image fusion is the fusion of focused (clear) and defocused (blurred) parts. Focused and defocused areas have similar intensity distributions but large differences in sharp appearance, so the key is the preservation of edge and texture information. Therefore, we use the structural similarity index measure (SSIM) and Q AB/F to evaluate the maintained  structure and the transferred edge information in the fused results. To maintain consistency with other tasks, EN, FMI, SCD and CC are also used. The experimental results on eighteen image pairs are shown in Fig. 9.\n\nOur PMGI can obtain the largest average values on SCD and CC, and the second largest average values on EN and SSIM. For QABF, the average value of our method is ranked third behind GD and QBMF and ranked fifth on the FMI. It can be seen that our results have the strongest correlation with source images, a lot of information, and strong structural similarity with source images. However, our results are not good enough in maintaining of the edge information and the acquisition of the feature information, which also corresponds to the fact that the details of our results are not rich enough in far-focused regions.\n\nThe average run time of CNN, DctVar, DWTDE, GD, QBMF and our PMGI on the testing data is 105.4371, 0.7388, 10.3615, 0.7905, 0.4313 and 0.0691(unit: second). Obviously, our PMGI has a significant advantage in running time, which is about one order of magnitude faster compared with other five methods.\n\n\nResults on Pan-sharpening\n\nTo demonstrate the versatility of our approach, we use PMGI to perform pan-sharpening in remote sensing image fusion tasks. We compare our method with classical methods PCA (Chavez et al. 1991), FISH (Tu et al. 2004) and Brovey (Gillespie, Kahle, and Walker 1986). Some typical qualitative results are shown in Fig. 10. It is obvious that PMGI can accomplish the pan-sharpening task well, and the fused results are color images with high resolution with a good visual effect. First, because we only constrain the texture of the panchromatic image without constraining its intensity, the fused results only maintain the color information of the multi-spectral image without introducing large spectral distortion like PCA. Second, the fused results have the richest and high-quality texture details, which are almost as good as those in the panchromatic image.\n\n\nConclusion\n\nIn this paper, we propose a fast unified image fusion network based on proportional maintenance of gradient and intensity, termed as PMGI. It is an end-to-end model which can perform a variety of different image fusion tasks well, including infrared and visible image fusion, multi-exposure image fusion, medical image fusion, multi-focus image fusion and pan-sharpening. We divide the network into the gradient path and the intensity path, which carry out feature reuse in the same path and information exchange in different paths. At the same time, we unify multiple image fusion tasks into the maintaining problem of intensity and gradient information, and define a specific loss function, which is composed of the gradient loss and the intensity loss. The PMGI can adapt to different image fusion tasks by adjusting the weight of each loss item. Qualitative and quantitative experiments show that our PMGI has advantages in various image fusion tasks compared to existing methods, and it is also faster than the competitors.\n\nFigure 1 :\n1Network architecture of the proposed PMGI.\n\nFigure 2 :\n2Qualitative Results of infrared and visible image fusion. From left to right: infrared image, visible image, and fused results of GTF, LatLRR, FusionGAN and our PMGI.\n\nFigure 3 :\n3Quantitative results of infrared and visible image fusion on six metrics.\n\nFigure 4 :\n4Qualitative results of multi-exposure image fusion. From left to right: underexposed image, overexposed image, and the results of DSIFT, EF, AWPIGG and our PMGI.\n\nFigure 5 :\n5Quantitative results of multi-exposure image fusion.\n\nFigure 6 :\n6Qualitative results of medical image fusion. From left to right: PET image, MRI image, and fused results of DCHWT, ASR, PCA and our PMGI.\n\nFigure 7 :\n7Quantitative results of medical image fusion.\n\nFigure 8 :\n8Qualitative results of multi-focus image fusion. The first two columns are source images with different focus. The last four columns are fused results of DWTDE, GD, QBMF and our PMGI.\n\nFigure 9 :\n9Quantitative results of multi-focus image fusion.\n\nFigure 10 :\n10Qualitative results of Pan-Sharpening. From left to right: multi-spectral image, panchromatic image, and fused results of PCA, FISH, Brovey and our PMGI.\n\n\n)Experimental Results \n\nExperimental Settings \n\nData We perform PMGI on five fusion tasks: 1) visible \nand infrared image fusion; 2) multi-exposure image fusion; \n3) medical image fusion; 4) multi-focus image fusion; 5) \npan-sharpening. The training and test sets are from publicly \navailable datasets: TNO 1 dataset for task1; MEF dataset 2 and \n\n1 http://figshare.com/articles/TNO Image Fusion Dataset/ \n1008029. \n2 https://drive.google.com/drive/folders/ \n0BzXT0LnoyRqlVjhtOEhiUzU5a2M \n\nthe dataset provided by \nhttp://www.med.harvard.edu/AANLIB/home.html 4 https://github.com/sametaymaz/Multi-focus-Image-Fusion-Dataset\nAcknowledgmentsThe authors gratefully acknowledge the financial supports from the National Natural Science Foundation of China under Grant nos. 61773295 and 61772512, and the Natural Science Fund of Hubei Province under Grant no. 2019CFA037.\nA new image quality metric for image fusion: the sum of the correlations of differences. V Aslantas, E Bendes, Aslantas, V., and Bendes, E. 2015. A new image quality met- ric for image fusion: the sum of the correlations of differences.\n\n. Aeu-International Journal of Electronics and Communications. 6912Aeu-International Journal of Electronics and Communications 69(12):1890-1896.\n\nQuadtree-based multi-focus image fusion using a weighted focus-measure. X Bai, Y Zhang, F Zhou, B Xue, Information Fusion. 22Bai, X.; Zhang, Y.; Zhou, F.; and Xue, B. 2015. Quadtree-based multi-focus image fusion using a weighted focus-measure. Infor- mation Fusion 22:105-118.\n\nLearning a deep single image contrast enhancer from multi-exposure images. J Cai, S Gu, L Zhang, IEEE Transactions on Image Processing. 274Cai, J.; Gu, S.; and Zhang, L. 2018. Learning a deep single image contrast enhancer from multi-exposure images. IEEE Transactions on Image Processing 27(4):2049-2062.\n\nComparison of three different methods to merge multiresolution and multispectral data-landsat tm and spot panchromatic. P Chavez, S C Sides, J A Anderson, Photogrammetric Engineering and Remote Sensing. 573Chavez, P.; Sides, S. C.; Anderson, J. A.; et al. 1991. Compari- son of three different methods to merge multiresolution and multi- spectral data-landsat tm and spot panchromatic. Photogrammetric Engineering and Remote Sensing 57(3):295-303.\n\nColor enhancement of highly correlated images. i. decorrelation and hsi contrast stretches. A R Gillespie, A B Kahle, R E Walker, Remote Sensing of Environment. 203Gillespie, A. R.; Kahle, A. B.; and Walker, R. E. 1986. Color enhancement of highly correlated images. i. decorrelation and hsi contrast stretches. Remote Sensing of Environment 20(3):209-235.\n\nMulti-focus image fusion for visual sensor networks in dct domain. M B A Haghighat, A Aghagolzadeh, H Seyedarabi, Computers & Electrical Engineering. 375Haghighat, M. B. A.; Aghagolzadeh, A.; and Seyedarabi, H. 2011. Multi-focus image fusion for visual sensor networks in dct domain. Computers & Electrical Engineering 37(5):789-797.\n\nGhost-free multi exposure image fusion technique using dense sift descriptor and guided filter. N Hayat, M Imran, Journal of Visual Communication and Image Representation. 62Hayat, N., and Imran, M. 2019. Ghost-free multi exposure image fusion technique using dense sift descriptor and guided filter. Jour- nal of Visual Communication and Image Representation 62:295- 308.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHuang, G.; Liu, Z.; Van Der Maaten, L.; and Weinberger, K. Q. 2017. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, 4700-4708.\n\nMultifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform. Signal. B S Kumar, Image and Video Processing. 76Kumar, B. S. 2013. Multifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform. Signal, Image and Video Processing 7(6):1125-1143.\n\nA multi-exposure image fusion based on the adaptive weights reflecting the relative pixel intensity and global gradient. S Lee, J S Park, N Cho, Proceedings of the IEEE International Conference on Image Processing. the IEEE International Conference on Image ProcessingLee, S.-h.; Park, J. S.; and Cho, N. I. 2018. A multi-exposure image fusion based on the adaptive weights reflecting the relative pixel intensity and global gradient. In Proceedings of the IEEE International Conference on Image Processing, 1737-1741.\n\nInfrared and visible image fusion using a novel deep decomposition method. H Li, X.-J Wu, arXiv:1811.02291arXiv preprintLi, H., and Wu, X.-J. 2018a. Infrared and visible image fu- sion using a novel deep decomposition method. arXiv preprint arXiv:1811.02291.\n\nInfrared and visible image fusion using latent low-rank representation. H Li, X.-J Wu, arXiv:1804.08992arXiv preprintLi, H., and Wu, X.-J. 2018b. Infrared and visible im- age fusion using latent low-rank representation. arXiv preprint arXiv:1804.08992.\n\nMulti-focus image fusion based on wavelet transform and adaptive block. Y Liu, Wang , Z , Journal of Image and Graphics. 1811Liu, Y., and Wang, Z. 2013. Multi-focus image fusion based on wavelet transform and adaptive block. Journal of Image and Graphics 18(11):1435-1444.\n\nSimultaneous image fusion and denoising with adaptive sparse representation. Y Liu, Wang , Z , IET Image Processing. 95Liu, Y., and Wang, Z. 2014. Simultaneous image fusion and de- noising with adaptive sparse representation. IET Image Processing 9(5):347-357.\n\nMulti-focus image fusion with a deep convolutional neural network. Y Liu, X Chen, H Peng, Z Wang, Information Fusion. 36Liu, Y.; Chen, X.; Peng, H.; and Wang, Z. 2017. Multi-focus im- age fusion with a deep convolutional neural network. Information Fusion 36:191-207.\n\nInfrared and visible image fusion via gradient transfer and total variation minimization. J Ma, C Chen, C Li, J Huang, Information Fusion. 31Ma, J.; Chen, C.; Li, C.; and Huang, J. 2016. Infrared and visible image fusion via gradient transfer and total variation minimization. Information Fusion 31:100-109.\n\nRobust multi-exposure image fusion: a structural patch decomposition approach. K Ma, H Li, H Yong, Z Wang, D Meng, L Zhang, IEEE Transactions on Image Processing. 265Ma, K.; Li, H.; Yong, H.; Wang, Z.; Meng, D.; and Zhang, L. 2017. Robust multi-exposure image fusion: a structural patch de- composition approach. IEEE Transactions on Image Processing 26(5):2519-2532.\n\nSesf-fuse: An unsupervised deep model for multi-focus image fusion. B Ma, X Ban, H Huang, Y Zhu, arXiv:1908.01703arXiv preprintMa, B.; Ban, X.; Huang, H.; and Zhu, Y. 2019a. Sesf-fuse: An unsupervised deep model for multi-focus image fusion. arXiv preprint arXiv:1908.01703.\n\nFusiongan: A generative adversarial network for infrared and visible image fusion. J Ma, W Yu, P Liang, C Li, J Jiang, Information Fusion. 48Ma, J.; Yu, W.; Liang, P.; Li, C.; and Jiang, J. 2019b. Fusiongan: A generative adversarial network for infrared and visible image fu- sion. Information Fusion 48:11-26.\n\nInformation fusion of passive sensors for detection of moving targets in dynamic environments. J Ma, P Liang, W Yu, C Chen, X Guo, J Wu, J Jiang, Information Fusion. 54Ma, J.; Liang, P.; Yu, W.; Chen, C.; Guo, X.; Wu, J.; and Jiang, J. 2020. Information fusion of passive sensors for detection of mov- ing targets in dynamic environments. Information Fusion 54:85- 98.\n\nInfrared and visible image fusion methods and applications: A survey. J Ma, Y Ma, C Li, Information Fusion. 45Ma, J.; Ma, Y.; and Li, C. 2019. Infrared and visible image fusion methods and applications: A survey. Information Fusion 45:153- 178.\n\nPerceptual quality assessment for multi-exposure image fusion. K Ma, K Zeng, Z Wang, IEEE Transactions on Image Processing. 2411Ma, K.; Zeng, K.; and Wang, Z. 2015. Perceptual quality assess- ment for multi-exposure image fusion. IEEE Transactions on Im- age Processing 24(11):3345-3356.\n\nExposure fusion. T Mertens, J Kautz, F Van Reeth, Proceedings of the Pacific Conference on Computer Graphics and Applications. the Pacific Conference on Computer Graphics and ApplicationsMertens, T.; Kautz, J.; and Van Reeth, F. 2007. Exposure fusion. In Proceedings of the Pacific Conference on Computer Graphics and Applications, 382-390.\n\nPixel-level image fusion using wavelets and principal component analysis. V Naidu, J R Raol, Defence Science Journal. 583Naidu, V., and Raol, J. R. 2008. Pixel-level image fusion using wavelets and principal component analysis. Defence Science Jour- nal 58(3):338-352.\n\nMulti-focus image fusion using dictionary-based sparse representation. M Nejati, S Samavi, S Shirani, Information Fusion. 25Nejati, M.; Samavi, S.; and Shirani, S. 2015. Multi-focus image fusion using dictionary-based sparse representation. Information Fusion 25:72-84.\n\nImage fusion using hierarchical pca. U Patil, U Mudengudi, Proceedings of the International Conference on Image Information Processing. the International Conference on Image Information ProcessingPatil, U., and Mudengudi, U. 2011. Image fusion using hierarchi- cal pca. In Proceedings of the International Conference on Image Information Processing, 1-6.\n\nMulti-exposure and multi-focus image fusion in gradient domain. S Paul, I S Sevcenco, P Agathoklis, Systems and Computers. 25101650123Journal of CircuitsPaul, S.; Sevcenco, I. S.; and Agathoklis, P. 2016. Multi-exposure and multi-focus image fusion in gradient domain. Journal of Cir- cuits, Systems and Computers 25(10):1650123.\n\nAn adaptive ihs pan-sharpening method. S Rahmani, M Strait, D Merkurjev, M Moeller, T Wittman, IEEE Geosci. Remote Sens. Lett. 74Rahmani, S.; Strait, M.; Merkurjev, D.; Moeller, M.; and Wittman, T. 2010. An adaptive ihs pan-sharpening method. IEEE Geosci. Remote Sens. Lett. 7(4):746-750.\n\nDeepfuse: a deep unsupervised approach for exposure fusion with extreme exposure image pairs. Ram Prabhakar, K Sai Srikar, V Babu, R , Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionRam Prabhakar, K.; Sai Srikar, V.; and Venkatesh Babu, R. 2017. Deepfuse: a deep unsupervised approach for exposure fusion with extreme exposure image pairs. In Proceedings of the IEEE Inter- national Conference on Computer Vision, 4724-4732.\n\nImage fusion by a ratio of low-pass pyramid. A Toet, Pattern Recognition Letters. 94Toet, A. 1989. Image fusion by a ratio of low-pass pyramid. Pattern Recognition Letters 9(4):245-253.\n\nA fast intensity-hue-saturation fusion technique with spectral adjustment for ikonos imagery. T.-M Tu, P S Huang, C.-L Hung, C.-P Chang, IEEE Geoscience and Remote Sensing Letters. 14Tu, T.-M.; Huang, P. S.; Hung, C.-L.; and Chang, C.-P. 2004. A fast intensity-hue-saturation fusion technique with spectral adjustment for ikonos imagery. IEEE Geoscience and Remote Sensing Letters 1(4):309-312.\n\nA multi-scale contrast-based image quality assessment model for multi-exposure image fusion. L Xing, L Cai, H Zeng, J Chen, J Zhu, J Hou, Signal Processing. 145Xing, L.; Cai, L.; Zeng, H.; Chen, J.; Zhu, J.; and Hou, J. 2018. A multi-scale contrast-based image quality assessment model for multi-exposure image fusion. Signal Processing 145:233-240.\n\nLearning a generative model for fusing infrared and visible images via conditional generative adversarial network with dual discriminators. H Xu, P Liang, W Yu, J Jiang, J Ma, Proceedings of the International Joint Conference on Artificial Intelligence. the International Joint Conference on Artificial IntelligenceXu, H.; Liang, P.; Yu, W.; Jiang, J.; and Ma, J. 2019. Learning a generative model for fusing infrared and visible images via condi- tional generative adversarial network with dual discriminators. In Proceedings of the International Joint Conference on Artificial In- telligence, 3954-3960.\n\nMedical image fusion with parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform domain. M Yin, X Liu, Y Liu, X Chen, IEEE Transactions on Instrumentation and Measurement. 681Yin, M.; Liu, X.; Liu, Y.; and Chen, X. 2018. Medical image fusion with parameter-adaptive pulse coupled neural network in nonsub- sampled shearlet transform domain. IEEE Transactions on Instru- mentation and Measurement 68(1):49-64.\n", "annotations": {"author": "[{\"end\":203,\"start\":128},{\"end\":293,\"start\":204},{\"end\":410,\"start\":294},{\"end\":500,\"start\":411},{\"end\":575,\"start\":501}]", "publisher": null, "author_last_name": "[{\"end\":137,\"start\":132},{\"end\":210,\"start\":208},{\"end\":303,\"start\":299},{\"end\":422,\"start\":419},{\"end\":509,\"start\":507}]", "author_first_name": "[{\"end\":131,\"start\":128},{\"end\":207,\"start\":204},{\"end\":298,\"start\":294},{\"end\":418,\"start\":411},{\"end\":506,\"start\":501}]", "author_affiliation": "[{\"end\":202,\"start\":139},{\"end\":292,\"start\":229},{\"end\":409,\"start\":326},{\"end\":499,\"start\":424},{\"end\":574,\"start\":511}]", "title": "[{\"end\":125,\"start\":1},{\"end\":700,\"start\":576}]", "venue": null, "abstract": "[{\"end\":2003,\"start\":702}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2568,\"start\":2547},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2583,\"start\":2568},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2600,\"start\":2583},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2858,\"start\":2821},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3327,\"start\":3301},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3364,\"start\":3344},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3893,\"start\":3867},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4070,\"start\":4001},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4198,\"start\":4173},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4337,\"start\":4309},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4501,\"start\":4474},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4785,\"start\":4770},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4966,\"start\":4950},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11875,\"start\":11856},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17383,\"start\":17358},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17521,\"start\":17487},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20051,\"start\":20040},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20073,\"start\":20057},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20101,\"start\":20084},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20127,\"start\":20110},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20158,\"start\":20142},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21509,\"start\":21483},{\"end\":22415,\"start\":22349},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22667,\"start\":22631},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22704,\"start\":22679},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24400,\"start\":24388},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24425,\"start\":24406},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24455,\"start\":24434},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26710,\"start\":26664},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26737,\"start\":26718},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26779,\"start\":26742},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26806,\"start\":26789},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29479,\"start\":29459},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29501,\"start\":29486},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29549,\"start\":29514}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31243,\"start\":31188},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31423,\"start\":31244},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31510,\"start\":31424},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31685,\"start\":31511},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31751,\"start\":31686},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31902,\"start\":31752},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31961,\"start\":31903},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32158,\"start\":31962},{\"attributes\":{\"id\":\"fig_8\"},\"end\":32221,\"start\":32159},{\"attributes\":{\"id\":\"fig_9\"},\"end\":32390,\"start\":32222},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32907,\"start\":32391}]", "paragraph": "[{\"end\":2601,\"start\":2019},{\"end\":4967,\"start\":2603},{\"end\":5203,\"start\":4969},{\"end\":5597,\"start\":5205},{\"end\":7275,\"start\":5599},{\"end\":7343,\"start\":7277},{\"end\":7929,\"start\":7345},{\"end\":8609,\"start\":7960},{\"end\":9057,\"start\":8611},{\"end\":10231,\"start\":9059},{\"end\":11322,\"start\":10233},{\"end\":11755,\"start\":11347},{\"end\":12632,\"start\":11757},{\"end\":13235,\"start\":12634},{\"end\":13857,\"start\":13253},{\"end\":14145,\"start\":13933},{\"end\":14180,\"start\":14147},{\"end\":14490,\"start\":14252},{\"end\":14590,\"start\":14568},{\"end\":14798,\"start\":14592},{\"end\":15181,\"start\":14800},{\"end\":15513,\"start\":15229},{\"end\":16019,\"start\":15569},{\"end\":16892,\"start\":16082},{\"end\":17644,\"start\":16944},{\"end\":18356,\"start\":17686},{\"end\":19762,\"start\":18358},{\"end\":19901,\"start\":19764},{\"end\":21235,\"start\":19950},{\"end\":21627,\"start\":21264},{\"end\":22246,\"start\":21629},{\"end\":22452,\"start\":22248},{\"end\":22705,\"start\":22495},{\"end\":23413,\"start\":22707},{\"end\":24094,\"start\":23415},{\"end\":24266,\"start\":24096},{\"end\":24456,\"start\":24302},{\"end\":25155,\"start\":24458},{\"end\":26339,\"start\":25157},{\"end\":26532,\"start\":26341},{\"end\":26807,\"start\":26572},{\"end\":27713,\"start\":26809},{\"end\":28334,\"start\":27715},{\"end\":28954,\"start\":28336},{\"end\":29256,\"start\":28956},{\"end\":30144,\"start\":29286},{\"end\":31187,\"start\":30159}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13932,\"start\":13858},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14251,\"start\":14181},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14567,\"start\":14491},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15228,\"start\":15182},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15568,\"start\":15514},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16081,\"start\":16020},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16943,\"start\":16893},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17685,\"start\":17645}]", "table_ref": null, "section_header": "[{\"end\":2017,\"start\":2005},{\"end\":7958,\"start\":7932},{\"end\":11345,\"start\":11325},{\"end\":13251,\"start\":13238},{\"end\":19948,\"start\":19904},{\"end\":21262,\"start\":21238},{\"end\":22493,\"start\":22455},{\"end\":24300,\"start\":24269},{\"end\":26570,\"start\":26535},{\"end\":29284,\"start\":29259},{\"end\":30157,\"start\":30147},{\"end\":31199,\"start\":31189},{\"end\":31255,\"start\":31245},{\"end\":31435,\"start\":31425},{\"end\":31522,\"start\":31512},{\"end\":31697,\"start\":31687},{\"end\":31763,\"start\":31753},{\"end\":31914,\"start\":31904},{\"end\":31973,\"start\":31963},{\"end\":32170,\"start\":32160},{\"end\":32234,\"start\":32223}]", "table": "[{\"end\":32907,\"start\":32394}]", "figure_caption": "[{\"end\":31243,\"start\":31201},{\"end\":31423,\"start\":31257},{\"end\":31510,\"start\":31437},{\"end\":31685,\"start\":31524},{\"end\":31751,\"start\":31699},{\"end\":31902,\"start\":31765},{\"end\":31961,\"start\":31916},{\"end\":32158,\"start\":31975},{\"end\":32221,\"start\":32172},{\"end\":32390,\"start\":32237},{\"end\":32394,\"start\":32393}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11428,\"start\":11422},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12526,\"start\":12520},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20376,\"start\":20370},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21673,\"start\":21667},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22844,\"start\":22838},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23592,\"start\":23586},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24603,\"start\":24597},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25853,\"start\":25847},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26941,\"start\":26935},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":28333,\"start\":28327},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29604,\"start\":29597}]", "bib_author_first_name": "[{\"end\":33349,\"start\":33348},{\"end\":33361,\"start\":33360},{\"end\":33716,\"start\":33715},{\"end\":33723,\"start\":33722},{\"end\":33732,\"start\":33731},{\"end\":33740,\"start\":33739},{\"end\":33998,\"start\":33997},{\"end\":34005,\"start\":34004},{\"end\":34011,\"start\":34010},{\"end\":34350,\"start\":34349},{\"end\":34360,\"start\":34359},{\"end\":34362,\"start\":34361},{\"end\":34371,\"start\":34370},{\"end\":34373,\"start\":34372},{\"end\":34771,\"start\":34770},{\"end\":34773,\"start\":34772},{\"end\":34786,\"start\":34785},{\"end\":34788,\"start\":34787},{\"end\":34797,\"start\":34796},{\"end\":34799,\"start\":34798},{\"end\":35104,\"start\":35103},{\"end\":35108,\"start\":35105},{\"end\":35121,\"start\":35120},{\"end\":35137,\"start\":35136},{\"end\":35468,\"start\":35467},{\"end\":35477,\"start\":35476},{\"end\":35788,\"start\":35787},{\"end\":35797,\"start\":35796},{\"end\":35804,\"start\":35803},{\"end\":35822,\"start\":35821},{\"end\":35824,\"start\":35823},{\"end\":36313,\"start\":36312},{\"end\":36315,\"start\":36314},{\"end\":36666,\"start\":36665},{\"end\":36673,\"start\":36672},{\"end\":36675,\"start\":36674},{\"end\":36683,\"start\":36682},{\"end\":37140,\"start\":37139},{\"end\":37149,\"start\":37145},{\"end\":37397,\"start\":37396},{\"end\":37406,\"start\":37402},{\"end\":37651,\"start\":37650},{\"end\":37661,\"start\":37657},{\"end\":37665,\"start\":37664},{\"end\":37930,\"start\":37929},{\"end\":37940,\"start\":37936},{\"end\":37944,\"start\":37943},{\"end\":38182,\"start\":38181},{\"end\":38189,\"start\":38188},{\"end\":38197,\"start\":38196},{\"end\":38205,\"start\":38204},{\"end\":38474,\"start\":38473},{\"end\":38480,\"start\":38479},{\"end\":38488,\"start\":38487},{\"end\":38494,\"start\":38493},{\"end\":38772,\"start\":38771},{\"end\":38778,\"start\":38777},{\"end\":38784,\"start\":38783},{\"end\":38792,\"start\":38791},{\"end\":38800,\"start\":38799},{\"end\":38808,\"start\":38807},{\"end\":39130,\"start\":39129},{\"end\":39136,\"start\":39135},{\"end\":39143,\"start\":39142},{\"end\":39152,\"start\":39151},{\"end\":39421,\"start\":39420},{\"end\":39427,\"start\":39426},{\"end\":39433,\"start\":39432},{\"end\":39442,\"start\":39441},{\"end\":39448,\"start\":39447},{\"end\":39745,\"start\":39744},{\"end\":39751,\"start\":39750},{\"end\":39760,\"start\":39759},{\"end\":39766,\"start\":39765},{\"end\":39774,\"start\":39773},{\"end\":39781,\"start\":39780},{\"end\":39787,\"start\":39786},{\"end\":40090,\"start\":40089},{\"end\":40096,\"start\":40095},{\"end\":40102,\"start\":40101},{\"end\":40329,\"start\":40328},{\"end\":40335,\"start\":40334},{\"end\":40343,\"start\":40342},{\"end\":40572,\"start\":40571},{\"end\":40583,\"start\":40582},{\"end\":40592,\"start\":40591},{\"end\":40971,\"start\":40970},{\"end\":40980,\"start\":40979},{\"end\":40982,\"start\":40981},{\"end\":41238,\"start\":41237},{\"end\":41248,\"start\":41247},{\"end\":41258,\"start\":41257},{\"end\":41475,\"start\":41474},{\"end\":41484,\"start\":41483},{\"end\":41858,\"start\":41857},{\"end\":41866,\"start\":41865},{\"end\":41868,\"start\":41867},{\"end\":41880,\"start\":41879},{\"end\":42164,\"start\":42163},{\"end\":42175,\"start\":42174},{\"end\":42185,\"start\":42184},{\"end\":42198,\"start\":42197},{\"end\":42209,\"start\":42208},{\"end\":42511,\"start\":42508},{\"end\":42524,\"start\":42523},{\"end\":42538,\"start\":42537},{\"end\":42546,\"start\":42545},{\"end\":42960,\"start\":42959},{\"end\":43199,\"start\":43195},{\"end\":43205,\"start\":43204},{\"end\":43207,\"start\":43206},{\"end\":43219,\"start\":43215},{\"end\":43230,\"start\":43226},{\"end\":43591,\"start\":43590},{\"end\":43599,\"start\":43598},{\"end\":43606,\"start\":43605},{\"end\":43614,\"start\":43613},{\"end\":43622,\"start\":43621},{\"end\":43629,\"start\":43628},{\"end\":43989,\"start\":43988},{\"end\":43995,\"start\":43994},{\"end\":44004,\"start\":44003},{\"end\":44010,\"start\":44009},{\"end\":44019,\"start\":44018},{\"end\":44574,\"start\":44573},{\"end\":44581,\"start\":44580},{\"end\":44588,\"start\":44587},{\"end\":44595,\"start\":44594}]", "bib_author_last_name": "[{\"end\":33358,\"start\":33350},{\"end\":33368,\"start\":33362},{\"end\":33720,\"start\":33717},{\"end\":33729,\"start\":33724},{\"end\":33737,\"start\":33733},{\"end\":33744,\"start\":33741},{\"end\":34002,\"start\":33999},{\"end\":34008,\"start\":34006},{\"end\":34017,\"start\":34012},{\"end\":34357,\"start\":34351},{\"end\":34368,\"start\":34363},{\"end\":34382,\"start\":34374},{\"end\":34783,\"start\":34774},{\"end\":34794,\"start\":34789},{\"end\":34806,\"start\":34800},{\"end\":35118,\"start\":35109},{\"end\":35134,\"start\":35122},{\"end\":35148,\"start\":35138},{\"end\":35474,\"start\":35469},{\"end\":35483,\"start\":35478},{\"end\":35794,\"start\":35789},{\"end\":35801,\"start\":35798},{\"end\":35819,\"start\":35805},{\"end\":35835,\"start\":35825},{\"end\":36321,\"start\":36316},{\"end\":36670,\"start\":36667},{\"end\":36680,\"start\":36676},{\"end\":36687,\"start\":36684},{\"end\":37143,\"start\":37141},{\"end\":37152,\"start\":37150},{\"end\":37400,\"start\":37398},{\"end\":37409,\"start\":37407},{\"end\":37655,\"start\":37652},{\"end\":37934,\"start\":37931},{\"end\":38186,\"start\":38183},{\"end\":38194,\"start\":38190},{\"end\":38202,\"start\":38198},{\"end\":38210,\"start\":38206},{\"end\":38477,\"start\":38475},{\"end\":38485,\"start\":38481},{\"end\":38491,\"start\":38489},{\"end\":38500,\"start\":38495},{\"end\":38775,\"start\":38773},{\"end\":38781,\"start\":38779},{\"end\":38789,\"start\":38785},{\"end\":38797,\"start\":38793},{\"end\":38805,\"start\":38801},{\"end\":38814,\"start\":38809},{\"end\":39133,\"start\":39131},{\"end\":39140,\"start\":39137},{\"end\":39149,\"start\":39144},{\"end\":39156,\"start\":39153},{\"end\":39424,\"start\":39422},{\"end\":39430,\"start\":39428},{\"end\":39439,\"start\":39434},{\"end\":39445,\"start\":39443},{\"end\":39454,\"start\":39449},{\"end\":39748,\"start\":39746},{\"end\":39757,\"start\":39752},{\"end\":39763,\"start\":39761},{\"end\":39771,\"start\":39767},{\"end\":39778,\"start\":39775},{\"end\":39784,\"start\":39782},{\"end\":39793,\"start\":39788},{\"end\":40093,\"start\":40091},{\"end\":40099,\"start\":40097},{\"end\":40105,\"start\":40103},{\"end\":40332,\"start\":40330},{\"end\":40340,\"start\":40336},{\"end\":40348,\"start\":40344},{\"end\":40580,\"start\":40573},{\"end\":40589,\"start\":40584},{\"end\":40602,\"start\":40593},{\"end\":40977,\"start\":40972},{\"end\":40987,\"start\":40983},{\"end\":41245,\"start\":41239},{\"end\":41255,\"start\":41249},{\"end\":41266,\"start\":41259},{\"end\":41481,\"start\":41476},{\"end\":41494,\"start\":41485},{\"end\":41863,\"start\":41859},{\"end\":41877,\"start\":41869},{\"end\":41891,\"start\":41881},{\"end\":42172,\"start\":42165},{\"end\":42182,\"start\":42176},{\"end\":42195,\"start\":42186},{\"end\":42206,\"start\":42199},{\"end\":42217,\"start\":42210},{\"end\":42521,\"start\":42512},{\"end\":42535,\"start\":42525},{\"end\":42543,\"start\":42539},{\"end\":42965,\"start\":42961},{\"end\":43202,\"start\":43200},{\"end\":43213,\"start\":43208},{\"end\":43224,\"start\":43220},{\"end\":43236,\"start\":43231},{\"end\":43596,\"start\":43592},{\"end\":43603,\"start\":43600},{\"end\":43611,\"start\":43607},{\"end\":43619,\"start\":43615},{\"end\":43626,\"start\":43623},{\"end\":43633,\"start\":43630},{\"end\":43992,\"start\":43990},{\"end\":44001,\"start\":43996},{\"end\":44007,\"start\":44005},{\"end\":44016,\"start\":44011},{\"end\":44022,\"start\":44020},{\"end\":44578,\"start\":44575},{\"end\":44585,\"start\":44582},{\"end\":44592,\"start\":44589},{\"end\":44600,\"start\":44596}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":33495,\"start\":33259},{\"attributes\":{\"id\":\"b1\"},\"end\":33641,\"start\":33497},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":26353389},\"end\":33920,\"start\":33643},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":19494799},\"end\":34227,\"start\":33922},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":128556577},\"end\":34676,\"start\":34229},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":127180222},\"end\":35034,\"start\":34678},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":38131177},\"end\":35369,\"start\":35036},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195570375},\"end\":35743,\"start\":35371},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9433631},\"end\":36182,\"start\":35745},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":195070237},\"end\":36542,\"start\":36184},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52186895},\"end\":37062,\"start\":36544},{\"attributes\":{\"doi\":\"arXiv:1811.02291\",\"id\":\"b11\"},\"end\":37322,\"start\":37064},{\"attributes\":{\"doi\":\"arXiv:1804.08992\",\"id\":\"b12\"},\"end\":37576,\"start\":37324},{\"attributes\":{\"id\":\"b13\"},\"end\":37850,\"start\":37578},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1890730},\"end\":38112,\"start\":37852},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11925688},\"end\":38381,\"start\":38114},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":205432776},\"end\":38690,\"start\":38383},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4816060},\"end\":39059,\"start\":38692},{\"attributes\":{\"doi\":\"arXiv:1908.01703\",\"id\":\"b18\"},\"end\":39335,\"start\":39061},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":71142966},\"end\":39647,\"start\":39337},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":12771759},\"end\":40017,\"start\":39649},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52073194},\"end\":40263,\"start\":40019},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4828378},\"end\":40552,\"start\":40265},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12230513},\"end\":40894,\"start\":40554},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":109701453},\"end\":41164,\"start\":40896},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":45235944},\"end\":41435,\"start\":41166},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":17285521},\"end\":41791,\"start\":41437},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":17113905},\"end\":42122,\"start\":41793},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14608308},\"end\":42412,\"start\":42124},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":216738},\"end\":42912,\"start\":42414},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6965804},\"end\":43099,\"start\":42914},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":12060714},\"end\":43495,\"start\":43101},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":46841777},\"end\":43846,\"start\":43497},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":199466224},\"end\":44453,\"start\":43848},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":54462396},\"end\":44892,\"start\":44455}]", "bib_title": "[{\"end\":33713,\"start\":33643},{\"end\":33995,\"start\":33922},{\"end\":34347,\"start\":34229},{\"end\":34768,\"start\":34678},{\"end\":35101,\"start\":35036},{\"end\":35465,\"start\":35371},{\"end\":35785,\"start\":35745},{\"end\":36310,\"start\":36184},{\"end\":36663,\"start\":36544},{\"end\":37648,\"start\":37578},{\"end\":37927,\"start\":37852},{\"end\":38179,\"start\":38114},{\"end\":38471,\"start\":38383},{\"end\":38769,\"start\":38692},{\"end\":39418,\"start\":39337},{\"end\":39742,\"start\":39649},{\"end\":40087,\"start\":40019},{\"end\":40326,\"start\":40265},{\"end\":40569,\"start\":40554},{\"end\":40968,\"start\":40896},{\"end\":41235,\"start\":41166},{\"end\":41472,\"start\":41437},{\"end\":41855,\"start\":41793},{\"end\":42161,\"start\":42124},{\"end\":42506,\"start\":42414},{\"end\":42957,\"start\":42914},{\"end\":43193,\"start\":43101},{\"end\":43588,\"start\":43497},{\"end\":43986,\"start\":43848},{\"end\":44571,\"start\":44455}]", "bib_author": "[{\"end\":33360,\"start\":33348},{\"end\":33370,\"start\":33360},{\"end\":33722,\"start\":33715},{\"end\":33731,\"start\":33722},{\"end\":33739,\"start\":33731},{\"end\":33746,\"start\":33739},{\"end\":34004,\"start\":33997},{\"end\":34010,\"start\":34004},{\"end\":34019,\"start\":34010},{\"end\":34359,\"start\":34349},{\"end\":34370,\"start\":34359},{\"end\":34384,\"start\":34370},{\"end\":34785,\"start\":34770},{\"end\":34796,\"start\":34785},{\"end\":34808,\"start\":34796},{\"end\":35120,\"start\":35103},{\"end\":35136,\"start\":35120},{\"end\":35150,\"start\":35136},{\"end\":35476,\"start\":35467},{\"end\":35485,\"start\":35476},{\"end\":35796,\"start\":35787},{\"end\":35803,\"start\":35796},{\"end\":35821,\"start\":35803},{\"end\":35837,\"start\":35821},{\"end\":36323,\"start\":36312},{\"end\":36672,\"start\":36665},{\"end\":36682,\"start\":36672},{\"end\":36689,\"start\":36682},{\"end\":37145,\"start\":37139},{\"end\":37154,\"start\":37145},{\"end\":37402,\"start\":37396},{\"end\":37411,\"start\":37402},{\"end\":37657,\"start\":37650},{\"end\":37664,\"start\":37657},{\"end\":37668,\"start\":37664},{\"end\":37936,\"start\":37929},{\"end\":37943,\"start\":37936},{\"end\":37947,\"start\":37943},{\"end\":38188,\"start\":38181},{\"end\":38196,\"start\":38188},{\"end\":38204,\"start\":38196},{\"end\":38212,\"start\":38204},{\"end\":38479,\"start\":38473},{\"end\":38487,\"start\":38479},{\"end\":38493,\"start\":38487},{\"end\":38502,\"start\":38493},{\"end\":38777,\"start\":38771},{\"end\":38783,\"start\":38777},{\"end\":38791,\"start\":38783},{\"end\":38799,\"start\":38791},{\"end\":38807,\"start\":38799},{\"end\":38816,\"start\":38807},{\"end\":39135,\"start\":39129},{\"end\":39142,\"start\":39135},{\"end\":39151,\"start\":39142},{\"end\":39158,\"start\":39151},{\"end\":39426,\"start\":39420},{\"end\":39432,\"start\":39426},{\"end\":39441,\"start\":39432},{\"end\":39447,\"start\":39441},{\"end\":39456,\"start\":39447},{\"end\":39750,\"start\":39744},{\"end\":39759,\"start\":39750},{\"end\":39765,\"start\":39759},{\"end\":39773,\"start\":39765},{\"end\":39780,\"start\":39773},{\"end\":39786,\"start\":39780},{\"end\":39795,\"start\":39786},{\"end\":40095,\"start\":40089},{\"end\":40101,\"start\":40095},{\"end\":40107,\"start\":40101},{\"end\":40334,\"start\":40328},{\"end\":40342,\"start\":40334},{\"end\":40350,\"start\":40342},{\"end\":40582,\"start\":40571},{\"end\":40591,\"start\":40582},{\"end\":40604,\"start\":40591},{\"end\":40979,\"start\":40970},{\"end\":40989,\"start\":40979},{\"end\":41247,\"start\":41237},{\"end\":41257,\"start\":41247},{\"end\":41268,\"start\":41257},{\"end\":41483,\"start\":41474},{\"end\":41496,\"start\":41483},{\"end\":41865,\"start\":41857},{\"end\":41879,\"start\":41865},{\"end\":41893,\"start\":41879},{\"end\":42174,\"start\":42163},{\"end\":42184,\"start\":42174},{\"end\":42197,\"start\":42184},{\"end\":42208,\"start\":42197},{\"end\":42219,\"start\":42208},{\"end\":42523,\"start\":42508},{\"end\":42537,\"start\":42523},{\"end\":42545,\"start\":42537},{\"end\":42549,\"start\":42545},{\"end\":42967,\"start\":42959},{\"end\":43204,\"start\":43195},{\"end\":43215,\"start\":43204},{\"end\":43226,\"start\":43215},{\"end\":43238,\"start\":43226},{\"end\":43598,\"start\":43590},{\"end\":43605,\"start\":43598},{\"end\":43613,\"start\":43605},{\"end\":43621,\"start\":43613},{\"end\":43628,\"start\":43621},{\"end\":43635,\"start\":43628},{\"end\":43994,\"start\":43988},{\"end\":44003,\"start\":43994},{\"end\":44009,\"start\":44003},{\"end\":44018,\"start\":44009},{\"end\":44024,\"start\":44018},{\"end\":44580,\"start\":44573},{\"end\":44587,\"start\":44580},{\"end\":44594,\"start\":44587},{\"end\":44602,\"start\":44594}]", "bib_venue": "[{\"end\":35978,\"start\":35916},{\"end\":36812,\"start\":36759},{\"end\":40741,\"start\":40681},{\"end\":41633,\"start\":41573},{\"end\":42670,\"start\":42618},{\"end\":44163,\"start\":44102},{\"end\":33346,\"start\":33259},{\"end\":33558,\"start\":33499},{\"end\":33764,\"start\":33746},{\"end\":34056,\"start\":34019},{\"end\":34430,\"start\":34384},{\"end\":34837,\"start\":34808},{\"end\":35184,\"start\":35150},{\"end\":35541,\"start\":35485},{\"end\":35914,\"start\":35837},{\"end\":36349,\"start\":36323},{\"end\":36757,\"start\":36689},{\"end\":37137,\"start\":37064},{\"end\":37394,\"start\":37324},{\"end\":37697,\"start\":37668},{\"end\":37967,\"start\":37947},{\"end\":38230,\"start\":38212},{\"end\":38520,\"start\":38502},{\"end\":38853,\"start\":38816},{\"end\":39127,\"start\":39061},{\"end\":39474,\"start\":39456},{\"end\":39813,\"start\":39795},{\"end\":40125,\"start\":40107},{\"end\":40387,\"start\":40350},{\"end\":40679,\"start\":40604},{\"end\":41012,\"start\":40989},{\"end\":41286,\"start\":41268},{\"end\":41571,\"start\":41496},{\"end\":41914,\"start\":41893},{\"end\":42249,\"start\":42219},{\"end\":42616,\"start\":42549},{\"end\":42994,\"start\":42967},{\"end\":43280,\"start\":43238},{\"end\":43652,\"start\":43635},{\"end\":44100,\"start\":44024},{\"end\":44654,\"start\":44602}]"}}}, "year": 2023, "month": 12, "day": 17}