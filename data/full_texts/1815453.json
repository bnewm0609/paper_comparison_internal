{"id": 1815453, "updated": "2023-03-25 18:27:51.04", "metadata": {"title": "A Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition", "authors": "[{\"first\":\"Shiqi\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Daoliang\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Tieniu\",\"last\":\"Tan\",\"middle\":[]}]", "venue": "18th International Conference on Pattern Recognition (ICPR'06)", "journal": "18th International Conference on Pattern Recognition (ICPR'06)", "publication_date": {"year": 2006, "month": null, "day": null}, "abstract": "Gait recognition has gained increasing interest from researchers, but there is still no standard evaluation method to compare the performance of different gait recognition algorithms. In this paper, a framework is proposed in an attempt to tackle this problem. The framework consists of a large gait database, a large set of well designed experiments and some evaluation metrics. There are 124 subjects in the database, and the gait data was captured from 11 views. Three variations, namely view angle, clothing and carrying condition changes, are separately considered in the database. The database is one of the largest database among the existing databases. Three sets of experiments, including a total of 363 experiments, are designed in the framework. Some metrics are proposed to evaluate gait recognition algorithms", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2104335344", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icpr/YuTT06", "doi": "10.1109/icpr.2006.67"}}, "content": {"source": {"pdf_hash": "0268d1744377ffbff48b014f513ea3e5e4a4dab1", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "66719b3210d9ec236e5ad70106f0b08de0427788", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0268d1744377ffbff48b014f513ea3e5e4a4dab1.txt", "contents": "\nA Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition\n\n\nShiqi Yu sqyu@nlpr.ia.ac.cn \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nDaoliang Tan dltan@nlpr.ia.ac.cn \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nTieniu Tan \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nA Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition\n\nGait recognition has gained increasing interest from researchers, but there is still no standard evaluation method to compare the performance of different gait recognition algorithms. In this paper, a framework is proposed in an attempt to tackle this problem. The framework consists of a large gait database, a large set of well designed experiments and some evaluation metrics. There are 124 subjects in the database, and the gait data was captured from 11 views. Three variations, namely view angle, clothing and carrying condition changes, are separately considered in the database. The database is one of the largest database among the existing databases. Three sets of experiments, including a total of 363 experiments, are designed in the framework. Some metrics are proposed to evaluate gait recognition algorithms.\n\nIntroduction\n\nGait is an attractive biometric feature for human identification at a distance, and recently has gained much interest from computer vision researchers. Compared with those traditional biometric features, such as face, iris and fingerprint, gait has many unique advantages such as non-contact, non-invasive and perceivable at a distance. Hence gait has been considered as a suitable biometric feature for human identification at a distance in visual surveillance.\n\nIn recent years many gait recognition algorithms have been developed. Some of them are model-based approaches [9,11], and some are appearance-based ones [7,12,13]. Even though many algorithms have been proposed, comparison of different algorithms and evaluation of robustness to some variations such as the variations of view angle, clothing, shoe types, surface types, carrying condition, illumination, and time are still hard and open problems. These variations should be fully studied to develop robust and ac-curate gait recognition algorithms.\n\nThe HumanID Gait Challenge Problem [1,8], which consists of a large database, a baseline algorithm and twelve experiments, tried to handle these problems. The data in the HumanID Gait Challenge Problem was collected in an outdoor environment with complex background, so it is a little hard to extract good quality human silhouettes, and this will affect the analysis of other factors. The twelve experiments were designed to evaluate an algorithm's robustness to view, shoe, surface, time, clothing and carrying condition changes. However, for these factors twelve experiments are not enough. Besides, the subjects walked in an elliptical path, and then the view angle kept changing while the subjects was walking, so the relationship between view angle and algorithm's performance can not be obtained. In conclusion, a database that is more suitable for evaluation and some well designed experiments are needed.\n\nA framework that consists of a large database, some experiments and metrics is proposed. In the database, data acquired from 11 views are included and also three most important factors, view angle, clothing and carrying condition changes, are separately considered. A total of 363 experiments were designed to thoroughly investigate these factors.\n\nThe organization of this paper is as follows. Section 2 describes the gait database. Experiment design is presented in Section 3, and metrics is in Section 4. Section 5 concludes this paper.\n\n\nDatabase\n\n\nOverview of other gait databases\n\nIn addition to the database in the HumanID Gait Challenge Problem [1,8], there also exist many other gait databases, such as UCSD Database, CASIA Gait Database (Dataset A) [2], Georgia Tech Database [3], MIT AI Gait Data, CMU Mobo Database [4,6], HID-UMD Database, and Soton Database [5,10]. Among these gait databases used in recent work there are only two databases which contain more than 100 subjects: one is the Soton Large Database, and the other is the Gait Challenge Database. The Gait Challenge Database has been mentioned in the previous section. The Soton Large Database contains only side views of normal walking, and does not include many factors.\n\n\nCASIA Gait Database: Dataset B\n\nAs the framework focuses on the effect of factors other than human detection or segmentation, simple background was used to simplify silhouette segmentation, and all the videos were captured in an indoor environment. We set up a lab specially for this data acquisition as illustrated in Figure  1. There were 11 USB cameras (Model: Fametech 318SC) around the left hand side of the subject when he/she was walking, and the angle between two nearest view directions is 18 \u2022 . Since many images would be taken from different positions, some geometry information of subjects could be reconstructed aided by some calibration equipments. Four calibration taps were placed to help reconstruct geometry information. There were 20cm\u00d720cm white-green alternative blocks on these taps. Two taps were put on the vertical wall, and two were on the flow, as shown in Figure 1. When a subject walked in the scene, he/she was asked to walk naturally along a straight line 6 times first, and 11\u00d76 = 66 normal walking video sequences were captured for each subject. Some normal walk examples are shown in Figure 2. After normal walk, the subjects were asked to put on their coats, and then walked twice along the straight line. We also recorded 2 sequences of each subject at each view when the subject carried a bag. The bag could be a knapsack, a satchel, or a handbag. The subjects could choose the bag that they liked. Figure 3 shows some sequences captured under these three conditions. The subjects' information, such as height, gender, was also recorded. All the video sequences were stored as video files encoded with mjpeg codec. The frame size of the video files is 320 \u00d7 240, and the frame rate is 25 fps. There are 2 to 3 gait cycles in each sequence. Gait data of 124 subjects were captured at last. There were 93 males and 31 females, 123 Asians and 1 European among all subjects. Most subjects were young people and they aged between 20 and 30. Every subject walked 10 times in the scene (6 normal + 2 with a coat + 2 with a bag). There are a total of 10 \u00d7 11 \u00d7 124 = 13640 video sequences in our database. The database is about 17G in size. It is a large gait database in terms of size or the number of subjects.\n\nThe CASIA Gait Database is provided free of charge at web site http://www.cbsr.ia.ac.cn/.\n\n\nExperiments\n\nWe designed three sets of experiments (Experiment Set A, B and C) for gait recognition algorithm evaluation. Experiment Set A is for investigating how view angle affect the gait recognition performance and an algorithm's robustness to view variation. For each view of a subject, there are 6 normal walking sequences. In Experiment Set A the first 4 sequences are taken as gallery set, and the other 2 sequences are taken as probe set. There are 11\u00d711 = 121 experiments in Experiment Set A as shown in Table 1. Experiment Set B is for investigating how clothing affect the performance and an algorithm's robustness to clothing change. Experiment Set C is for carrying condition change. The gallery sets of Experiment Set B and C are the same with A, but the probe sets are different. All the sequences of walking with a coat are put into the probe set of Experiment Set B, and all those of walking with a bag are put into the probe set of Experiment Set C. These three experiment set are used to evaluate an algorithm's robustness to view change, clothing change and carrying condition change.\n\nIn the following, a gait recognition algorithm is evaluated as an example by the metrics proposed in the paper. We take the well known average silhouette (also called Gait Energy Image, GEI) algorithm [7] as an example, which has been reported as a good feature robust to silhouette errors and image noise, Given a fixed camera, the human silhouette can be ex-\n\n\nFigure 2. Normal walking sequences\n\ntracted by background subtraction and thresholding. The method in [12] is used to segment human silhouette from image sequences. The sizes of the silhouettes are not unique, and the silhouettes need to be normalized to be the same size. Gait energy image is defined in [7] as\nG(x, y) = 1 N N t=1 I(x, y, t) (1)\nwhere N is the number of frames in the sequence I(x, y, t), t is the frame number, x and y are the image coordinate. Euclidian distance is employed to measure the similarity between two GEIs, and the nearest neighbor classifier is to recognition different subjects. The experimental results (correct classification rates) of Experiment Set A, B and C are listed in Table 1 \n\n\nEvaluation\n\nThe correct classification rates (CCRs) in Table 1, 2   ing and carrying condition change. But it is not straightforward to compare two algorithms or evaluate an algorithm's robustness using these values in the three tables. Two metrics (C \u0394 and \u03c3) are designed for each experiment set. C A \u0394 , which is the average of the CCRs on the diagonal (\u03b8 g = \u03b8 p ) of Table 1, is defined as\nC A \u0394 = 1 11 10 n=0 C A n\u00b7n(2)\nwhere C A n\u00b7n is the CCR in Experiment Set A when the gallery and probe angles are all n \u00b7 18 \u2022 . C B \u0394 and C C \u0394 are defined similarly. C A \u0394 can be used to indicate the algorithm's accuracy of recognition when there is no great variation, and C B \u0394 and C C \u0394 can be used to indicate the algorithm's robustness to clothing and carrying condition changes respectively.\n\nThe standard deviation is a statistic that tells how tightly all the various examples are clustered around the mean. So the standard deviations of the three experiment set, \u03c3 A , \u03c3 B and \u03c3 C , are used as metrics. \u03c3 A can indicate the algorithm's robustness to view angle change. \u03c3 B and \u03c3 C are slightly different from \u03c3 A . They indicate the algorithm's robustness not only to view angle change, but also respectively to clothing and carrying condition changes.\n\nAll these metrics values for the GEI algorithm are listed in Table 4. A better gait recognition algorithm should has larger C \u0394 and smaller \u03c3. \n\n\nMetrics Exp. Set\n\n\nConclusions and Future Work\n\nIn the framework proposed in the paper there are a large gait database, three experiment sets and some evaluation metrics. The gait database, which has 124 subjects and 11 views, is one of the largest databases. The database can be used for not only gait recognition, but also human body tracking, human body reconstruction, human motion analysis, etc. The framework provides a platform to evaluate gait recognition algorithms. It can promote the development of gait recognition. In future we will enlarge the database to include more data of different time, outdoor environment, other sensors (infrared camera) etc. In addition, a more systematic evaluation for gait recognition as FRVT in face recognition is also our goal.\n\nFigure 1 .\n1Set-up for gait data collection\n\nFigure 3 .\n3Walking under different conditions\n\n\nProbe angle \u03b8p (normal walking #5-6) 0 \u2022 18 \u2022 36 \u2022 54 \u2022 72 \u2022 90 \u2022 108 \u2022 126 \u2022 144 \u2022 162 \u2022 180 \u2022 Gallery angle \u03b8g (normal #1-4) 0 \u2022 99.\n\nTable 2 .\n2and 3 can show the robustness of an algorithm to view, cloth-Probe angle \u03b8p (walking with a coat #1-2) 0 \u2022 18 \u2022 36 \u2022 54 \u2022 72 \u2022 90 \u2022 108 \u2022 126 \u2022 144 \u2022 162 \u2022 180 \u2022 Gallery angle \u03b8g(normal #1-4) 0 \u2022 24.The experimental results of Set B (%) Probe angle \u03b8p (walking with a bag #1-2) 0 \u2022 18 \u2022 36 \u2022 54 \u2022 72 \u2022 90 \u2022 108 \u2022 126 \u2022 144 \u2022 162 \u2022 180 \u2022 Gallery angle \u03b8g(normal #1-4) 0 \u2022 80.\n\nTable 1 .\n1The experimental results of Set A (%)\n\nTable 3 .\n3The experimental results of Set C (%)The 18th International Conference on Pattern Recognition (ICPR'06) \n\n\nA Exp. Set B Exp. Set C Table 4. Evaluation metrics for the GEI algorithmC \u0394 \n0.977 \n0.289 \n0.678 \n\u03c3 \n0.302 \n0.086 \n0.195 \n\n\nThe 18th International Conference on Pattern Recognition (ICPR'06) 0-7695-2521-0/06 $20.00 \u00a9 2006\nAcknowledgementWe would like to thank Stan Z. Li, Liangsheng Wang, Wenhai Chen, Lun Xin, Zhang Zhang, Wenfeng Jin and other members in CASIA for their help on setting up the capture lab. We would also like to thank all volunteers taking part in the data collection.This work is partly supported by National Natural\n. Biometrics Center For, Security Research, Casia , Center for Biometrics and Security Research, CASIA. http://www.cbsr.ia.ac.cn.\n\n. Human, Human ID at CMU. http://www.hid.ri.cmu.edu.\n\nAutomatic Gait Recognition for Human ID at a Distance at Soton. Automatic Gait Recognition for Human ID at a Distance at Soton. http://www.gait.ecs.soton.ac.uk.\n\nThe cmu motion of body (mobo) database. R Gross, J Shi, CMU-RI-TR-01-18Robotics Institute, Carnegie Mellon UniversityTechnical ReportR. Gross and J. Shi. The cmu motion of body (mobo) database. Technical Report CMU-RI-TR-01-18, Robotics Institute, Carnegie Mellon University, June 2001.\n\nStatistical feature fusion for gait-based human recognition. J Han, B Bhanu, Proc. of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern RecognitionWashington DC, USAJ. Han and B. Bhanu. Statistical feature fusion for gait-based human recognition. In Proc. of the 2004 IEEE Computer So- ciety Conference on Computer Vision and Pattern Recogni- tion, pages II842-II847. Washington DC, USA, June 2004.\n\nThe humanid gait challenge problem: Data sets, performance, and analysis. S Sardar, P J Phillips, Z Liu, I R Vega, P Grother, Kevin W Bowyer, IEEE Transactions on Pattern Analysis and Machine Intelligence. 272S. Sardar, P. J. Phillips, Z. Liu, I. R. Vega, P. Grother, and kevin W. Bowyer. The humanid gait challenge problem: Data sets, performance, and analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(2):162-177, Feb. 2002.\n\nIntegrated face and gait recognition from multiple views. G Shakhnarovich, L Lee, T Darrell, Proc. of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern RecognitionHawaii, USAG. Shakhnarovich, L. Lee, and T. Darrell. Integrated face and gait recognition from multiple views. In Proc. of the 2001 IEEE Computer Society Conference on Computer Vi- sion and Pattern Recognition, pages I439-I446. Hawaii, USA, Dec. 2001.\n\nOn a large sequence-based human gait database. J D Shutler, M G Grant, M S Nixon, J N Carter, Proc. of 4th International Conference on Recent Advances in Soft Computing. of 4th International Conference on Recent Advances in Soft ComputingNottingham,UKJ. D. Shutler, M. G. Grant, M. S. Nixon, and J. N. Carter. On a large sequence-based human gait database. In Proc. of 4th International Conference on Recent Advances in Soft Computing, pages 66-71. Nottingham,UK, 2002.\n\nL Wang, H Ning, T Tan, W Hu, Fusion of static and dynamic body biometrics for gait recognition. IEEE Transactions on Circuits and Systems for Video Technology. 14L. Wang, H. Ning, T. Tan, and W. Hu. Fusion of static and dynamic body biometrics for gait recognition. IEEE Transactions on Circuits and Systems for Video Technology, 14(2):149-158, Feb. 2004.\n\nSilhouette analysisbased gait recognition for human identification. L Wang, T Tan, H Ning, W Hu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2512L. Wang, T. Tan, H. Ning, and W. Hu. Silhouette analysis- based gait recognition for human identification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(12):1505-1518, December 2003.\n\nGait analysis for human identification in frequency domain. S Yu, L Wang, W Hu, T Tan, Proc. of the 3rd International Conference on Image and Graphics. of the 3rd International Conference on Image and GraphicsHong Kong, ChinaS. Yu, L. Wang, W. Hu, and T. Tan. Gait analysis for human identification in frequency domain. In Proc. of the 3rd In- ternational Conference on Image and Graphics, pages 282- 285. Hong Kong, China, December 2004.\n", "annotations": {"author": "[{\"end\":233,\"start\":108},{\"end\":364,\"start\":234},{\"end\":473,\"start\":365}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":114},{\"end\":246,\"start\":243},{\"end\":375,\"start\":372}]", "author_first_name": "[{\"end\":113,\"start\":108},{\"end\":242,\"start\":234},{\"end\":371,\"start\":365}]", "author_affiliation": "[{\"end\":232,\"start\":137},{\"end\":363,\"start\":268},{\"end\":472,\"start\":377}]", "title": "[{\"end\":105,\"start\":1},{\"end\":578,\"start\":474}]", "venue": null, "abstract": "[{\"end\":1403,\"start\":580}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1996,\"start\":1993},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1999,\"start\":1996},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2039,\"start\":2036},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2042,\"start\":2039},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2045,\"start\":2042},{\"end\":2471,\"start\":2468},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2473,\"start\":2471},{\"end\":4003,\"start\":4000},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4005,\"start\":4003},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4109,\"start\":4106},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4177,\"start\":4174},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4179,\"start\":4177},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4221,\"start\":4218},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4224,\"start\":4221},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8244,\"start\":8241},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8509,\"start\":8505},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8711,\"start\":8708}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":11351,\"start\":11307},{\"attributes\":{\"id\":\"fig_1\"},\"end\":11399,\"start\":11352},{\"attributes\":{\"id\":\"fig_2\"},\"end\":11536,\"start\":11400},{\"attributes\":{\"id\":\"fig_3\"},\"end\":11923,\"start\":11537},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":11973,\"start\":11924},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":12090,\"start\":11974},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":12217,\"start\":12091}]", "paragraph": "[{\"end\":1881,\"start\":1419},{\"end\":2431,\"start\":1883},{\"end\":3345,\"start\":2433},{\"end\":3694,\"start\":3347},{\"end\":3886,\"start\":3696},{\"end\":4594,\"start\":3934},{\"end\":6839,\"start\":4629},{\"end\":6930,\"start\":6841},{\"end\":8038,\"start\":6946},{\"end\":8400,\"start\":8040},{\"end\":8714,\"start\":8439},{\"end\":9123,\"start\":8750},{\"end\":9520,\"start\":9138},{\"end\":9920,\"start\":9552},{\"end\":10385,\"start\":9922},{\"end\":10530,\"start\":10387},{\"end\":11306,\"start\":10581}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8749,\"start\":8715},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9551,\"start\":9521}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7454,\"start\":7447},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9122,\"start\":9115},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9191,\"start\":9181},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9505,\"start\":9498},{\"end\":10455,\"start\":10448}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1417,\"start\":1405},{\"attributes\":{\"n\":\"2.\"},\"end\":3897,\"start\":3889},{\"attributes\":{\"n\":\"2.1.\"},\"end\":3932,\"start\":3900},{\"attributes\":{\"n\":\"2.2.\"},\"end\":4627,\"start\":4597},{\"attributes\":{\"n\":\"3.\"},\"end\":6944,\"start\":6933},{\"end\":8437,\"start\":8403},{\"attributes\":{\"n\":\"4.\"},\"end\":9136,\"start\":9126},{\"end\":10549,\"start\":10533},{\"attributes\":{\"n\":\"5.\"},\"end\":10579,\"start\":10552},{\"end\":11318,\"start\":11308},{\"end\":11363,\"start\":11353},{\"end\":11547,\"start\":11538},{\"end\":11934,\"start\":11925},{\"end\":11984,\"start\":11975}]", "table": "[{\"end\":12090,\"start\":12023},{\"end\":12217,\"start\":12166}]", "figure_caption": "[{\"end\":11351,\"start\":11320},{\"end\":11399,\"start\":11365},{\"end\":11536,\"start\":11402},{\"end\":11923,\"start\":11549},{\"end\":11973,\"start\":11936},{\"end\":12023,\"start\":11986},{\"end\":12166,\"start\":12093}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4925,\"start\":4916},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5490,\"start\":5482},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":5724,\"start\":5716},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6042,\"start\":6034}]", "bib_author_first_name": "[{\"end\":12643,\"start\":12633},{\"end\":12664,\"start\":12656},{\"end\":12680,\"start\":12675},{\"end\":13019,\"start\":13018},{\"end\":13028,\"start\":13027},{\"end\":13328,\"start\":13327},{\"end\":13335,\"start\":13334},{\"end\":13853,\"start\":13852},{\"end\":13863,\"start\":13862},{\"end\":13865,\"start\":13864},{\"end\":13877,\"start\":13876},{\"end\":13884,\"start\":13883},{\"end\":13886,\"start\":13885},{\"end\":13894,\"start\":13893},{\"end\":13909,\"start\":13904},{\"end\":13911,\"start\":13910},{\"end\":14291,\"start\":14290},{\"end\":14308,\"start\":14307},{\"end\":14315,\"start\":14314},{\"end\":14808,\"start\":14807},{\"end\":14810,\"start\":14809},{\"end\":14821,\"start\":14820},{\"end\":14823,\"start\":14822},{\"end\":14832,\"start\":14831},{\"end\":14834,\"start\":14833},{\"end\":14843,\"start\":14842},{\"end\":14845,\"start\":14844},{\"end\":15232,\"start\":15231},{\"end\":15240,\"start\":15239},{\"end\":15248,\"start\":15247},{\"end\":15255,\"start\":15254},{\"end\":15657,\"start\":15656},{\"end\":15665,\"start\":15664},{\"end\":15672,\"start\":15671},{\"end\":15680,\"start\":15679},{\"end\":16019,\"start\":16018},{\"end\":16025,\"start\":16024},{\"end\":16033,\"start\":16032},{\"end\":16039,\"start\":16038}]", "bib_author_last_name": "[{\"end\":12654,\"start\":12644},{\"end\":12673,\"start\":12665},{\"end\":12769,\"start\":12764},{\"end\":13025,\"start\":13020},{\"end\":13032,\"start\":13029},{\"end\":13332,\"start\":13329},{\"end\":13341,\"start\":13336},{\"end\":13860,\"start\":13854},{\"end\":13874,\"start\":13866},{\"end\":13881,\"start\":13878},{\"end\":13891,\"start\":13887},{\"end\":13902,\"start\":13895},{\"end\":13918,\"start\":13912},{\"end\":14305,\"start\":14292},{\"end\":14312,\"start\":14309},{\"end\":14323,\"start\":14316},{\"end\":14818,\"start\":14811},{\"end\":14829,\"start\":14824},{\"end\":14840,\"start\":14835},{\"end\":14852,\"start\":14846},{\"end\":15237,\"start\":15233},{\"end\":15245,\"start\":15241},{\"end\":15252,\"start\":15249},{\"end\":15258,\"start\":15256},{\"end\":15662,\"start\":15658},{\"end\":15669,\"start\":15666},{\"end\":15677,\"start\":15673},{\"end\":15683,\"start\":15681},{\"end\":16022,\"start\":16020},{\"end\":16030,\"start\":16026},{\"end\":16036,\"start\":16034},{\"end\":16043,\"start\":16040}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":12760,\"start\":12631},{\"attributes\":{\"id\":\"b1\"},\"end\":12814,\"start\":12762},{\"attributes\":{\"id\":\"b2\"},\"end\":12976,\"start\":12816},{\"attributes\":{\"doi\":\"CMU-RI-TR-01-18\",\"id\":\"b3\"},\"end\":13264,\"start\":12978},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15556935},\"end\":13776,\"start\":13266},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7693282},\"end\":14230,\"start\":13778},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14230118},\"end\":14758,\"start\":14232},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":45064783},\"end\":15229,\"start\":14760},{\"attributes\":{\"id\":\"b8\"},\"end\":15586,\"start\":15231},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13874338},\"end\":15956,\"start\":15588},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7896681},\"end\":16396,\"start\":15958}]", "bib_title": "[{\"end\":13325,\"start\":13266},{\"end\":13850,\"start\":13778},{\"end\":14288,\"start\":14232},{\"end\":14805,\"start\":14760},{\"end\":15654,\"start\":15588},{\"end\":16016,\"start\":15958}]", "bib_author": "[{\"end\":12656,\"start\":12633},{\"end\":12675,\"start\":12656},{\"end\":12683,\"start\":12675},{\"end\":12771,\"start\":12764},{\"end\":13027,\"start\":13018},{\"end\":13034,\"start\":13027},{\"end\":13334,\"start\":13327},{\"end\":13343,\"start\":13334},{\"end\":13862,\"start\":13852},{\"end\":13876,\"start\":13862},{\"end\":13883,\"start\":13876},{\"end\":13893,\"start\":13883},{\"end\":13904,\"start\":13893},{\"end\":13920,\"start\":13904},{\"end\":14307,\"start\":14290},{\"end\":14314,\"start\":14307},{\"end\":14325,\"start\":14314},{\"end\":14820,\"start\":14807},{\"end\":14831,\"start\":14820},{\"end\":14842,\"start\":14831},{\"end\":14854,\"start\":14842},{\"end\":15239,\"start\":15231},{\"end\":15247,\"start\":15239},{\"end\":15254,\"start\":15247},{\"end\":15260,\"start\":15254},{\"end\":15664,\"start\":15656},{\"end\":15671,\"start\":15664},{\"end\":15679,\"start\":15671},{\"end\":15685,\"start\":15679},{\"end\":16024,\"start\":16018},{\"end\":16032,\"start\":16024},{\"end\":16038,\"start\":16032},{\"end\":16045,\"start\":16038}]", "bib_venue": "[{\"end\":12878,\"start\":12816},{\"end\":13016,\"start\":12978},{\"end\":13436,\"start\":13343},{\"end\":13982,\"start\":13920},{\"end\":14418,\"start\":14325},{\"end\":14928,\"start\":14854},{\"end\":15389,\"start\":15260},{\"end\":15747,\"start\":15685},{\"end\":16108,\"start\":16045},{\"end\":13543,\"start\":13438},{\"end\":14518,\"start\":14420},{\"end\":15011,\"start\":14930},{\"end\":16183,\"start\":16110}]"}}}, "year": 2023, "month": 12, "day": 17}