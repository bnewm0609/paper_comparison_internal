{"id": 221970729, "updated": "2023-10-06 11:27:19.404", "metadata": {"title": "BOML: A Modularized Bilevel Optimization Library in Python for Meta Learning", "authors": "[{\"first\":\"Yaohua\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Risheng\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 9, "day": 28}, "abstract": "Meta-learning (a.k.a. learning to learn) has recently emerged as a promising paradigm for a variety of applications. There are now many meta-learning methods, each focusing on different modeling aspects of base and meta learners, but all can be (re)formulated as specific bilevel optimization problems. This work presents BOML, a modularized optimization library that unifies several meta-learning algorithms into a common bilevel optimization framework. It provides a hierarchical optimization pipeline together with a variety of iteration modules, which can be used to solve the mainstream categories of meta-learning methods, such as meta-feature-based and meta-initialization-based formulations. The library is written in Python and is available at https://github.com/dut-media-lab/BOML.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2009.13357", "mag": "3175433886", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icmcs/LiuL21", "doi": "10.1109/icmew53276.2021.9455948"}}, "content": {"source": {"pdf_hash": "b30e8c414f1585d20f853e7e39c5f5133d80b3d8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.13357v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2009.13357", "status": "GREEN"}}, "grobid": {"id": "f773b9df38862f03aef88b328a3a9006f7360cdc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b30e8c414f1585d20f853e7e39c5f5133d80b3d8.txt", "contents": "\nA Modularized Bilevel Optimization Library in Python for Meta Learning BOML: A Modularized Bilevel Optimization Library in Python for Meta Learning\n\n\nYaohua Liu liuyaohua@mail.dlut.edu.cn \nInternational School of Information Science and Engineering\nKey Laboratory for Ubiquitous Network and Service Software of Liaoning Province Dalian\nDalian University of Technology\nLiaoningP. R. China\n\nRisheng Liu rsliu@dlut.edu.cn \nInternational School of Information Science and Engineering\nKey Laboratory for Ubiquitous Network and Service Software of Liaoning Province Dalian\nDalian University of Technology\nLiaoningP. R. China\n\nA Modularized Bilevel Optimization Library in Python for Meta Learning BOML: A Modularized Bilevel Optimization Library in Python for Meta Learning\nbileveloptimizationmeta-learningfew-shot learningPython\nMeta-learning (a.k.a. learning to learn) has recently emerged as a promising paradigm for a variety of applications. There are now many meta-learning methods, each focusing on different modeling aspects of base and meta learners, but all can be (re)formulated as specific bilevel optimization problems. This work presents BOML, a modularized optimization library that unifies several meta-learning algorithms into a common bilevel optimization framework. It provides a hierarchical optimization pipeline together with a variety of iteration modules, which can be used to solve the mainstream categories of meta-learning methods, such as meta-feature-based and meta-initialization-based formulations. The library is written in Python and is available at https://github.com/dut-media-lab/BOML.\n\nIntroduction\n\nMeta-learning is the branch of machine learning that deals with the problem of \"learning to learn\" and has recently emerged as a potential learning paradigm that can gain experience over previous tasks and generalize that experience to unseen tasks proficiently. The applications of meta-learning span from few-shot classification (Nichol et al., 2018;Lee and Choi, 2018), and deep reinforcement learning (Finn et al., 2017;Rajeswaran et al., 2019), to neural architecture search (Liu et al., 2019;Hospedales et al., 2020). However, due to the complex learning paradigms and the problem-specific formulations (for example, use different strategies to construct the base and meta learners), it actually requires significant optimization expertise to design efficient algorithms to solve these meta-learning problems.\n\nIn this work, by formulating meta-learning tasks from the bilevel optimization perspective, we establish a unified and modularized library, named BOML, for different categories of meta-learning approaches. Specifically, in BOML, we support two main categories of meta-learning paradigms, including meta-initialization-based (Finn et al., 2017) and metafeature-based (Franceschi et al., 2018) and implement a variety of recently developed bilevel optimization techniques, such as Reverse Hyper-Gradient (RHG) (Franceschi et al., 2017), Truncated RHG (TRHG) (Shaban et al., 2019), Meta-SGD (Li et al., 2017), MT-net (Lee and Choi, 2018), WarpGrad (Flennerhag et al., 2020), HOAG (Pedregosa, 2016) and Bilevel Descent Aggregation (BDA) (Liu et al., 2020), for solving the meta-learning problems. Several first-order approximation schemes, including First-Order MAML (FMAML) (Finn et al., 2017) and DARTS (Liu et al., 2019), are also integrated into our BOML.\n\nThe key features of BOML can be summarized as follows: It provides a unified bilevel optimization framework to address different categories of existing meta-learning paradigms, offers a modularized algorithmic structure to integrate a variety of optimization techniques, and is flexible and extensible for potential meta-learning applications. We implement continuous code integration with Travis CI and Codecov to obtain high code converge (more than 98%). We also follow PEP8 naming convention to guarantee the code consistency. The documentations are developed with sphinx and rendered using Read the Docs (available at https://boml.readthedocs.io).\n\n\nA Unified Bilevel Optimization Paradigm for Meta Learning\n\nWe first present a general bilevel optimization paradigm to unify different types of metalearning approaches. Specifically, we define the meta data set as\nD = {D i } N i=1 , where D i = D i tr \u222a D i\nval is linked to the i-th task and D i tr and D i val respectively denote the training and validation sets. We denote the parameters of the base-learner as y i for the i-th task. Then the meta-learner can be thought of as a function that maps the data set to the parameters of base-learner for new tasks, that is, y i = \u03a8(x, D i ), where x is the parameter of the meta-leaner and should be shared across tasks. With the above notations, we can formulate the general purpose of meta-learning tasks as the following bilevel optimization model: min\nx F (x, {y i } N i=1 ), s.t. y i \u2208 arg min y i f (x, y i ), i = 1, \u00b7 \u00b7 \u00b7 , N,(1)where f (x, y i ) = (x, y i , D i tr ) and F (x, {y i } N i=1 ) = 1/N N i=1 (x, y i , D i val )\nare called the Lower-Level (LL) and Upper-Level (UL) objectives, respectively. Here denotes taskspecific loss functions such as cross-entropy. By reformulating the optimization process of y i (with fixed x) as a dynamical system, that is,\ny i 0 = \u03a8 0 (x, D i ) and y i t = \u03a8 t (x, y i t\u22121 , D i ), the meta-learner can be established as \u03a8 = \u03a8 T \u2022 \u03a8 T \u22121 \u2022 \u00b7 \u00b7 \u00b7 \u2022 \u03a8 0 .\nBased on the above construction, we can formulate different categories of meta-learning methods within the bilevel model in Eq. (1). Specifically, for meta-feature-based methods, we partition the learning model into a cross-task feature mapping (parameterized by x) and task specific sub-models (parameterized by {y i } N i=1 ). While in meta-initialization-based approaches, we actually only need to consider y i as the model parameters for the i-th task and set x as the initialization of the meta-learner.\n\n\nDesign and Features of BOML\n\nIn this section, we elaborate on BOML's design and features in three subsections.\n\n\nOptimization Process\n\nWe first illustrate the optimization process of BOML for meta-learning in Figure 1. It can be seen that BOML constructs two nested optimization subproblems (blue and green dashed rectangles), which are respectively related to the LL variable y and UL variable x in Eq. (1). For the LL subproblem (w.r.t. y), we can establish the dynamical system (parameterized by fixed x) by performing Gradient Descent (GD) on the LL objective (that is, f ). We also consider the recently proposed aggregation technique in BDA to integrate both the LL and UL objectives (that is, f and F ) to generate the dynamical system. As for the UL subproblem, we actually consider Back-Propagation (BP) and Back-Propagation-Through-Time (BPTT) (Baydin et al., 2017) to calculate the gradients (with respect to x) in meta-initialization-based and meta-feature-based tasks, respectively. 1\n\n\nImplementation Details\n\nBOML consists of the following six modules: boml optimizer, load data, setup model, lower iter, upper iter and optimizer. BOML first instantiates BOMLOptimizer encapsulated in boml optimizer to store necessary parameters of model configuration and help manage the following procedures. (i) load data is invoked to process and sample batches of data for specific tasks. (ii) setup model defines network structure and initializes network parameters of meta-learner and base-learner on the basis of the data formats returned by load data. (iii) BOMLOptimizer chooses built-in or extended strategies in lower iter and returns iterative formats of the dynamical system. (iv) BOMLOptimizer conducts the UL calculation with strategies in upper iter, that calls lower iter during the back propagation process in turn. (v) To adapt to the nested gradient computation of dynamical systems in lower iter and lower iter, BOML integrates different mainstream stochastic gradient updating schemes in optimizer.\n\nWe consider few-shot classification (Snell et al., 2017) as our demo application and implement network structures for both meta-initialization-based and meta-feature-based approaches. Various loss functions (cross-entropy and mean-squared error) and regularization terms (L1 and L2 norms) are supported in BOML. We also provide loading configurations for several widely-used data sets, including MNIST (Lecun et al., 1998), Omniglot (Lake et al., 2011), and MiniImageNet (Ravi and Larochelle, 2017. Listing 1 presents a code snippet to demonstrate how to build bilevel optimization model for meta-learning. Listing 1: Code snippet of BOML on building bilevel optimization model for meta-learning.\n\n\nComparison to Existing Libraries\n\nIn the past few years, some libraries, such as Meta-Blocks 2 and Far-HO 3 (Franceschi et al., 2017), have also been developed for meta-learning. Meta-Blocks is proposed in 2020 and still under development. This library stands out with its implementations for different modern meta-initialization-based algorithms. Far-HO is another library, which formulates meta-learning as specific hyper-parameter optimization task but is purely focused on metafeature-based models. In contrast, BOML establishes a general bilevel optimization framework to unify both above two categories of meta-learning methods. We also implement a series of recently proposed extensions and accelerations in BOML. Table 1 provides an extensive comparison of available algorithms for meta-learning in these libraries.  \n\n\nConclusions and Future Works\n\nWe (re)formulated mainstream meta-learning approaches into a unified bilevel optimization paradigm, and presented BOML to integrate common meta-learning approaches under our proposed taxonomy. For future work direction, we plan to continuously update the library to extend more gradient computation modules and related methods to support reinforcement learning strategies and neural architecture search.\n\n2. https://github.com/alshedivat/meta-blocks. 3. https://github.com/lucfra/FAR-HO.\n\nFigure 1 :\n1Illustrating the optimization process of BOML.\n\n\nboml_ho.base_learner(ex.x, meta_learner) # define base-learner 7 loss_inner = utils.cross_entropy(model.out, ex.y) # LL objective 8 inner_grad = bopt.ll_problem(loss_inner, learning_rate, T, var_list=model.var_list, experiment=ex) # LL subproblem 9 val_out = model.re_forward(ex.x_).out # output with validation set 10 loss_outer = utils.cross_entropy(val_out, ex.y_) # UL objective 11 bopt.ul_problem(loss_outer, meta_learning_rate, inner_grad, 12 extension.outer_parameters()) # UL subproblem 13 bopt.aggregate_all() # aggregate defined operations\n\nTable 1 :\n1Availability of meta-learning algorithms published in the literature.\nfrom boml import utils, load_data, extension 1. In fact, stochastic GD with momentum(Hospedales et al., 2020) is used as our default updating rule and other mainstream schemes (for example, Adam Kingma and Ba, 2014) are also available in BOML.\nAcknowledgmentsAcknowledgments We would like to thank support from National Natural Science Foundation of China (Nos. 61922019 and 61672125), LiaoNing Revitalization Talents Program (XLYC1807088) and the Fundamental Research Funds for the Central Universities.\nAutomatic differentiation in machine learning: a survey. At\u0131l\u0131m G\u00fcnes Baydin, A Barak, Alexey Pearlmutter, Jeffrey Mark Andreyevich Radul, Siskind, The Journal of Machine Learning Research. 181At\u0131l\u0131m G\u00fcnes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1):5595-5637, 2017.\n\nModel-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, AustraliaChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1126- 1135, 2017.\n\nMeta-learning with warped gradient descent. Sebastian Flennerhag, Andrei A Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, Raia Hadsell, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Sebastian Flennerhag, Andrei A. Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia Hadsell. Meta-learning with warped gradient descent. In 8th International Con- ference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\n\nForward and reverse gradient-based hyperparameter optimization. Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, AustraliaLuca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th Inter- national Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1165-1173, 2017.\n\nBilevel programming for hyperparameter optimization and meta-learning. Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, Massimilano Pontil, arXiv:1806.04910arXiv preprintLuca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimilano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. arXiv preprint arXiv:1806.04910, 2018.\n\nMetalearning in neural networks: A survey. CoRR, abs. Timothy M Hospedales, Antreas Antoniou, Paul Micaelli, Amos J Storkey, Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey. Meta- learning in neural networks: A survey. CoRR, abs/2004.05439, 2020.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nOne shot learning of simple visual concepts. M Brenden, Ruslan Lake, Jason Salakhutdinov, Joshua B Gross, Tenenbaum, Proceedings of the 33th Annual Meeting of the Cognitive Science Society. the 33th Annual Meeting of the Cognitive Science SocietyBoston, Massachusetts, USABrenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the 33th Annual Meeting of the Cognitive Science Society, CogSci 2011, Boston, Massachusetts, USA, July 20-23, 2011, 2011.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, 10.1109/5.726791Proceedings of the IEEE. the IEEE86Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to docu- ment recognition. Proceedings of the IEEE, 86(11):2278-2324, Nov 1998. ISSN 1558-2256. doi: 10.1109/5.726791.\n\nGradient-based meta-learning with learned layerwise metric and subspace. Yoonho Lee, Seungjin Choi, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm, SwedenYoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, pages 2933-2942, 2018.\n\nMeta-sgd: Learning to learn quickly for few shot learning. Zhenguo Li, Fengwei Zhou, Fei Chen, Hang Li, abs/1707.09835CoRRZhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few shot learning. CoRR, abs/1707.09835, 2017.\n\nDARTS: differentiable architecture search. Hanxiao Liu, Karen Simonyan, Yiming Yang, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAHanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n\nA generic first-order algorithmic framework for bi-level programming beyond lower-level singleton. CoRR, abs. Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang, Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton. CoRR, abs/2006.04045, 2020.\n\nOn first-order meta-learning algorithms. Alex Nichol, Joshua Achiam, John Schulman, abs/1803.02999CoRRAlex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. CoRR, abs/1803.02999, 2018.\n\nHyperparameter optimization with approximate gradient. Fabian Pedregosa, Proceedings of the 33nd International Conference on Machine Learning. the 33nd International Conference on Machine LearningNew York City, NY, USAFabian Pedregosa. Hyperparameter optimization with approximate gradient. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 737-746, 2016.\n\nMeta-learning with implicit gradients. Aravind Rajeswaran, Chelsea Finn, M Sham, Sergey Kakade, Levine, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, CanadaAravind Rajeswaran, Chelsea Finn, Sham M. Kakade, and Sergey Levine. Meta-learning with implicit gradients. In Advances in Neural Information Processing Systems 32: An- nual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 113-124, 2019.\n\nOptimization as a model for few-shot learning. Sachin Ravi, Hugo Larochelle, 5th International Conference on Learning Representations. Toulon, FranceConference Track ProceedingsSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.\n\nTruncated backpropagation for bilevel optimization. Amirreza Shaban, Ching-An Cheng, Nathan Hatch, Byron Boots, The 22nd International Conference on Artificial Intelligence and Statistics. Naha, Okinawa, Japan2019Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back- propagation for bilevel optimization. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 1723-1732, 2019.\n\nPrototypical networks for few-shot learning. CoRR, abs/1703.05175. Jake Snell, Kevin Swersky, Richard S Zemel, Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learn- ing. CoRR, abs/1703.05175, 2017. URL http://arxiv.org/abs/1703.05175.\n", "annotations": {"author": "[{\"end\":389,\"start\":151},{\"end\":620,\"start\":390}]", "publisher": null, "author_last_name": "[{\"end\":161,\"start\":158},{\"end\":401,\"start\":398}]", "author_first_name": "[{\"end\":157,\"start\":151},{\"end\":397,\"start\":390}]", "author_affiliation": "[{\"end\":388,\"start\":190},{\"end\":619,\"start\":421}]", "title": "[{\"end\":148,\"start\":1},{\"end\":768,\"start\":621}]", "venue": null, "abstract": "[{\"end\":1616,\"start\":825}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1984,\"start\":1963},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2003,\"start\":1984},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2056,\"start\":2037},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2080,\"start\":2056},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2130,\"start\":2112},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2154,\"start\":2130},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2792,\"start\":2773},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2840,\"start\":2815},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2982,\"start\":2957},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3026,\"start\":3005},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3054,\"start\":3037},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3083,\"start\":3063},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3119,\"start\":3094},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3143,\"start\":3126},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3200,\"start\":3182},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3339,\"start\":3320},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3368,\"start\":3350},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8000,\"start\":7980},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8365,\"start\":8346},{\"end\":8395,\"start\":8365},{\"end\":8441,\"start\":8395},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8776,\"start\":8751},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10792,\"start\":10767}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":10048,\"start\":9989},{\"attributes\":{\"id\":\"fig_1\"},\"end\":10600,\"start\":10049},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":10682,\"start\":10601}]", "paragraph": "[{\"end\":2447,\"start\":1632},{\"end\":3404,\"start\":2449},{\"end\":4058,\"start\":3406},{\"end\":4274,\"start\":4120},{\"end\":4864,\"start\":4319},{\"end\":5279,\"start\":5041},{\"end\":5919,\"start\":5411},{\"end\":6032,\"start\":5951},{\"end\":6919,\"start\":6057},{\"end\":7942,\"start\":6946},{\"end\":8640,\"start\":7944},{\"end\":9468,\"start\":8677},{\"end\":9904,\"start\":9501},{\"end\":9988,\"start\":9906}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4318,\"start\":4275},{\"attributes\":{\"id\":\"formula_1\"},\"end\":4945,\"start\":4865},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5040,\"start\":4945},{\"attributes\":{\"id\":\"formula_3\"},\"end\":5410,\"start\":5280}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9371,\"start\":9364}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1630,\"start\":1618},{\"attributes\":{\"n\":\"2.\"},\"end\":4118,\"start\":4061},{\"attributes\":{\"n\":\"3.\"},\"end\":5949,\"start\":5922},{\"attributes\":{\"n\":\"3.1\"},\"end\":6055,\"start\":6035},{\"attributes\":{\"n\":\"3.2\"},\"end\":6944,\"start\":6922},{\"attributes\":{\"n\":\"3.3\"},\"end\":8675,\"start\":8643},{\"attributes\":{\"n\":\"4.\"},\"end\":9499,\"start\":9471},{\"end\":10000,\"start\":9990},{\"end\":10611,\"start\":10602}]", "table": null, "figure_caption": "[{\"end\":10048,\"start\":10002},{\"end\":10600,\"start\":10051},{\"end\":10682,\"start\":10613}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6139,\"start\":6131}]", "bib_author_first_name": "[{\"end\":11267,\"start\":11266},{\"end\":11281,\"start\":11275},{\"end\":11302,\"start\":11295},{\"end\":11307,\"start\":11303},{\"end\":11672,\"start\":11665},{\"end\":11685,\"start\":11679},{\"end\":11700,\"start\":11694},{\"end\":12173,\"start\":12164},{\"end\":12192,\"start\":12186},{\"end\":12194,\"start\":12193},{\"end\":12207,\"start\":12201},{\"end\":12226,\"start\":12217},{\"end\":12239,\"start\":12234},{\"end\":12249,\"start\":12245},{\"end\":12677,\"start\":12673},{\"end\":12697,\"start\":12690},{\"end\":12711,\"start\":12706},{\"end\":12734,\"start\":12722},{\"end\":13253,\"start\":13249},{\"end\":13271,\"start\":13266},{\"end\":13289,\"start\":13282},{\"end\":13305,\"start\":13297},{\"end\":13325,\"start\":13314},{\"end\":13625,\"start\":13618},{\"end\":13627,\"start\":13626},{\"end\":13647,\"start\":13640},{\"end\":13662,\"start\":13658},{\"end\":13677,\"start\":13673},{\"end\":13679,\"start\":13678},{\"end\":13885,\"start\":13884},{\"end\":13901,\"start\":13896},{\"end\":14104,\"start\":14103},{\"end\":14120,\"start\":14114},{\"end\":14132,\"start\":14127},{\"end\":14154,\"start\":14148},{\"end\":14156,\"start\":14155},{\"end\":14652,\"start\":14651},{\"end\":14661,\"start\":14660},{\"end\":14671,\"start\":14670},{\"end\":14681,\"start\":14680},{\"end\":15021,\"start\":15015},{\"end\":15035,\"start\":15027},{\"end\":15532,\"start\":15525},{\"end\":15544,\"start\":15537},{\"end\":15554,\"start\":15551},{\"end\":15565,\"start\":15561},{\"end\":15775,\"start\":15768},{\"end\":15786,\"start\":15781},{\"end\":15803,\"start\":15797},{\"end\":16221,\"start\":16214},{\"end\":16230,\"start\":16227},{\"end\":16243,\"start\":16235},{\"end\":16258,\"start\":16250},{\"end\":16268,\"start\":16265},{\"end\":16515,\"start\":16511},{\"end\":16530,\"start\":16524},{\"end\":16543,\"start\":16539},{\"end\":16750,\"start\":16744},{\"end\":17174,\"start\":17167},{\"end\":17194,\"start\":17187},{\"end\":17202,\"start\":17201},{\"end\":17215,\"start\":17209},{\"end\":17740,\"start\":17734},{\"end\":17751,\"start\":17747},{\"end\":18148,\"start\":18140},{\"end\":18165,\"start\":18157},{\"end\":18179,\"start\":18173},{\"end\":18192,\"start\":18187},{\"end\":18648,\"start\":18644},{\"end\":18661,\"start\":18656},{\"end\":18678,\"start\":18671},{\"end\":18680,\"start\":18679}]", "bib_author_last_name": "[{\"end\":11264,\"start\":11245},{\"end\":11273,\"start\":11268},{\"end\":11293,\"start\":11282},{\"end\":11325,\"start\":11308},{\"end\":11334,\"start\":11327},{\"end\":11677,\"start\":11673},{\"end\":11692,\"start\":11686},{\"end\":11707,\"start\":11701},{\"end\":12184,\"start\":12174},{\"end\":12199,\"start\":12195},{\"end\":12215,\"start\":12208},{\"end\":12232,\"start\":12227},{\"end\":12243,\"start\":12240},{\"end\":12257,\"start\":12250},{\"end\":12688,\"start\":12678},{\"end\":12704,\"start\":12698},{\"end\":12720,\"start\":12712},{\"end\":12741,\"start\":12735},{\"end\":13264,\"start\":13254},{\"end\":13280,\"start\":13272},{\"end\":13295,\"start\":13290},{\"end\":13312,\"start\":13306},{\"end\":13332,\"start\":13326},{\"end\":13638,\"start\":13628},{\"end\":13656,\"start\":13648},{\"end\":13671,\"start\":13663},{\"end\":13687,\"start\":13680},{\"end\":13894,\"start\":13886},{\"end\":13908,\"start\":13902},{\"end\":13912,\"start\":13910},{\"end\":14112,\"start\":14105},{\"end\":14125,\"start\":14121},{\"end\":14146,\"start\":14133},{\"end\":14162,\"start\":14157},{\"end\":14173,\"start\":14164},{\"end\":14658,\"start\":14653},{\"end\":14668,\"start\":14662},{\"end\":14678,\"start\":14672},{\"end\":14689,\"start\":14682},{\"end\":15025,\"start\":15022},{\"end\":15040,\"start\":15036},{\"end\":15535,\"start\":15533},{\"end\":15549,\"start\":15545},{\"end\":15559,\"start\":15555},{\"end\":15568,\"start\":15566},{\"end\":15779,\"start\":15776},{\"end\":15795,\"start\":15787},{\"end\":15808,\"start\":15804},{\"end\":16225,\"start\":16222},{\"end\":16233,\"start\":16231},{\"end\":16248,\"start\":16244},{\"end\":16263,\"start\":16259},{\"end\":16274,\"start\":16269},{\"end\":16522,\"start\":16516},{\"end\":16537,\"start\":16531},{\"end\":16552,\"start\":16544},{\"end\":16760,\"start\":16751},{\"end\":17185,\"start\":17175},{\"end\":17199,\"start\":17195},{\"end\":17207,\"start\":17203},{\"end\":17222,\"start\":17216},{\"end\":17230,\"start\":17224},{\"end\":17745,\"start\":17741},{\"end\":17762,\"start\":17752},{\"end\":18155,\"start\":18149},{\"end\":18171,\"start\":18166},{\"end\":18185,\"start\":18180},{\"end\":18198,\"start\":18193},{\"end\":18654,\"start\":18649},{\"end\":18669,\"start\":18662},{\"end\":18686,\"start\":18681}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3766791},\"end\":11596,\"start\":11188},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6719686},\"end\":12118,\"start\":11598},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":202539544},\"end\":12607,\"start\":12120},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8026824},\"end\":13176,\"start\":12609},{\"attributes\":{\"doi\":\"arXiv:1806.04910\",\"id\":\"b4\"},\"end\":13562,\"start\":13178},{\"attributes\":{\"id\":\"b5\"},\"end\":13838,\"start\":13564},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b6\"},\"end\":14056,\"start\":13840},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15373038},\"end\":14592,\"start\":14058},{\"attributes\":{\"doi\":\"10.1109/5.726791\",\"id\":\"b8\",\"matched_paper_id\":14542261},\"end\":14940,\"start\":14594},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3350728},\"end\":15464,\"start\":14942},{\"attributes\":{\"doi\":\"abs/1707.09835\",\"id\":\"b10\"},\"end\":15723,\"start\":15466},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49411844},\"end\":16102,\"start\":15725},{\"attributes\":{\"id\":\"b12\"},\"end\":16468,\"start\":16104},{\"attributes\":{\"doi\":\"abs/1803.02999\",\"id\":\"b13\"},\"end\":16687,\"start\":16470},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":552516},\"end\":17126,\"start\":16689},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202542766},\"end\":17685,\"start\":17128},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":67413369},\"end\":18086,\"start\":17687},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":121190562},\"end\":18575,\"start\":18088},{\"attributes\":{\"id\":\"b18\"},\"end\":18848,\"start\":18577}]", "bib_title": "[{\"end\":11243,\"start\":11188},{\"end\":11663,\"start\":11598},{\"end\":12162,\"start\":12120},{\"end\":12671,\"start\":12609},{\"end\":14101,\"start\":14058},{\"end\":14649,\"start\":14594},{\"end\":15013,\"start\":14942},{\"end\":15766,\"start\":15725},{\"end\":16742,\"start\":16689},{\"end\":17165,\"start\":17128},{\"end\":17732,\"start\":17687},{\"end\":18138,\"start\":18088}]", "bib_author": "[{\"end\":11266,\"start\":11245},{\"end\":11275,\"start\":11266},{\"end\":11295,\"start\":11275},{\"end\":11327,\"start\":11295},{\"end\":11336,\"start\":11327},{\"end\":11679,\"start\":11665},{\"end\":11694,\"start\":11679},{\"end\":11709,\"start\":11694},{\"end\":12186,\"start\":12164},{\"end\":12201,\"start\":12186},{\"end\":12217,\"start\":12201},{\"end\":12234,\"start\":12217},{\"end\":12245,\"start\":12234},{\"end\":12259,\"start\":12245},{\"end\":12690,\"start\":12673},{\"end\":12706,\"start\":12690},{\"end\":12722,\"start\":12706},{\"end\":12743,\"start\":12722},{\"end\":13266,\"start\":13249},{\"end\":13282,\"start\":13266},{\"end\":13297,\"start\":13282},{\"end\":13314,\"start\":13297},{\"end\":13334,\"start\":13314},{\"end\":13640,\"start\":13618},{\"end\":13658,\"start\":13640},{\"end\":13673,\"start\":13658},{\"end\":13689,\"start\":13673},{\"end\":13896,\"start\":13884},{\"end\":13910,\"start\":13896},{\"end\":13914,\"start\":13910},{\"end\":14114,\"start\":14103},{\"end\":14127,\"start\":14114},{\"end\":14148,\"start\":14127},{\"end\":14164,\"start\":14148},{\"end\":14175,\"start\":14164},{\"end\":14660,\"start\":14651},{\"end\":14670,\"start\":14660},{\"end\":14680,\"start\":14670},{\"end\":14691,\"start\":14680},{\"end\":15027,\"start\":15015},{\"end\":15042,\"start\":15027},{\"end\":15537,\"start\":15525},{\"end\":15551,\"start\":15537},{\"end\":15561,\"start\":15551},{\"end\":15570,\"start\":15561},{\"end\":15781,\"start\":15768},{\"end\":15797,\"start\":15781},{\"end\":15810,\"start\":15797},{\"end\":16227,\"start\":16214},{\"end\":16235,\"start\":16227},{\"end\":16250,\"start\":16235},{\"end\":16265,\"start\":16250},{\"end\":16276,\"start\":16265},{\"end\":16524,\"start\":16511},{\"end\":16539,\"start\":16524},{\"end\":16554,\"start\":16539},{\"end\":16762,\"start\":16744},{\"end\":17187,\"start\":17167},{\"end\":17201,\"start\":17187},{\"end\":17209,\"start\":17201},{\"end\":17224,\"start\":17209},{\"end\":17232,\"start\":17224},{\"end\":17747,\"start\":17734},{\"end\":17764,\"start\":17747},{\"end\":18157,\"start\":18140},{\"end\":18173,\"start\":18157},{\"end\":18187,\"start\":18173},{\"end\":18200,\"start\":18187},{\"end\":18656,\"start\":18644},{\"end\":18671,\"start\":18656},{\"end\":18688,\"start\":18671}]", "bib_venue": "[{\"end\":11854,\"start\":11779},{\"end\":12338,\"start\":12317},{\"end\":12888,\"start\":12813},{\"end\":14330,\"start\":14248},{\"end\":14740,\"start\":14732},{\"end\":15200,\"start\":15112},{\"end\":15899,\"start\":15879},{\"end\":16907,\"start\":16832},{\"end\":17376,\"start\":17346},{\"end\":17836,\"start\":17822},{\"end\":18297,\"start\":18277},{\"end\":11376,\"start\":11336},{\"end\":11777,\"start\":11709},{\"end\":12315,\"start\":12259},{\"end\":12811,\"start\":12743},{\"end\":13247,\"start\":13178},{\"end\":13616,\"start\":13564},{\"end\":13882,\"start\":13840},{\"end\":14246,\"start\":14175},{\"end\":14730,\"start\":14707},{\"end\":15110,\"start\":15042},{\"end\":15523,\"start\":15466},{\"end\":15877,\"start\":15810},{\"end\":16212,\"start\":16104},{\"end\":16509,\"start\":16470},{\"end\":16830,\"start\":16762},{\"end\":17344,\"start\":17232},{\"end\":17820,\"start\":17764},{\"end\":18275,\"start\":18200},{\"end\":18642,\"start\":18577}]"}}}, "year": 2023, "month": 12, "day": 17}