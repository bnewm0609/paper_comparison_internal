{"id": 235743051, "updated": "2023-10-06 01:26:15.513", "metadata": {"title": "Depth-supervised NeRF: Fewer Views and Faster Training for Free", "authors": "[{\"first\":\"Kangle\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Jun-Yan\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Deva\",\"last\":\"Ramanan\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as\"free\"depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2107.02791", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/DengLZR22", "doi": "10.1109/cvpr52688.2022.01254"}}, "content": {"source": {"pdf_hash": "988952b0e737c8ab9b6c1fbd6d54db86e299d270", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2107.02791v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "83ebae788da926113ce2ea82237faca8fb514942", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/988952b0e737c8ab9b6c1fbd6d54db86e299d270.txt", "contents": "\nDepth-supervised NeRF: Fewer Views and Faster Training for Free\n\n\nKangle Deng \nCarnegie Mellon University\n\n\nAndrew Liu \nGoogle\n\nJun-Yan Zhu \nCarnegie Mellon University\n\n\nDeva Ramanan \nCarnegie Mellon University\n\n\nDepth-supervised NeRF: Fewer Views and Faster Training for Free\n3 Argo AI !\"#$%&'()'*+,-.% )&\"./0!1\"&$2,%&3'4&56 781$%9 !\"#$%&'2,&:% 4&1$#;'5#3,#-<&'6,&;3%'74&569 !\"#\"$%&'()$*+,+\"-%=+$'&#</'\",>&; .)(/0%&'()$*+,+\"-%=+$'&#</'?&@\"+,-. A#B&$#'*#$#B&.&$% !\"#\"$% &'()$*+,+\"-% .)(/0 &'()$*+,+\"-% !\"#\"$% &'()$*+,+\"-% 1 !.$1<.1$& 6$+B C+.,+-!\"#$%&$'$&\"#()\"*$'+,%(-,.%\"/,0-1 !\"234\"/$5-6 D'5&-3&$&3'A+;+$ D'E$+1-3.$1./ A+;+$ !\"#$5'.7$8-,.%\"9''1\nFigure 1. Training NeRFs can be difficult when given insufficient input images. We utilize additional supervision from depth recovered from 3D point clouds estimated from running structure-from-motion and impose a loss to ensure the rendered ray's termination distribution respects the surface priors given by the each keypoint. Because our supervision is complementary to NeRF, it can be combined with any such approach to reduce overfitting and speed up training.AbstractA commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as \"free\" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.\n Figure 1\n. Training NeRFs can be difficult when given insufficient input images. We utilize additional supervision from depth recovered from 3D point clouds estimated from running structure-from-motion and impose a loss to ensure the rendered ray's termination distribution respects the surface priors given by the each keypoint. Because our supervision is complementary to NeRF, it can be combined with any such approach to reduce overfitting and speed up training.\n\n\nAbstract\n\nA commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as \"free\" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.\n\n\nIntroduction\n\nNeural rendering with implicit representations has become a widely-used technique for solving many vision and graphics tasks ranging from view synthesis [5,15,25], to re-lighting [12,13], to pose and shape estimation [17,21,31], to 3D-aware image synthesis and editing [3,11,23], to modeling dynamic scenes [9,18,19]. The seminal work of Neural Radiance Fields (NeRF) [15] demonstrated impressive view synthesis results by using implicit functions to encode volumetric density and color observations. NeRF DS-NeRF Figure 2. Few view NeRF. NeRF is susceptible to overfitting when given few training views. As seen by the PSNR gap between train and test renders (left), NeRF has overfit and fails at synthesizing novel views. Further, the depth map (right) and depth error (middle) for NeRF suggest that its density function has failed to extract the surface geometry and can only reconstruct the training views' colors. Our depth-supervised NeRF model is able to render plausible geometry with consistently lower depth errors.\n\nIn spite of this, NeRF has several limitations. Reconstructing both the scene appearance and geometry can be ill-posed given a small number of input views. Figure 2 shows that NeRF can learn wildly inaccurate scene geometries that still accurately render train-views. However, such models produce poor renderings of novel test-views, essentially overfitting to the train set. Furthermore, even given a large number of input views, NeRF can still be time-consuming to train; it often takes between ten hours to several days to model a single scene at moderate resolutions on a single GPU. The training is slow due to both the expensive ray-casting operations and lengthy optimization process.\n\nIn this work, we explore depth as an additional, cheap source of supervision to guide the geometry learned by NeRF. Typical NeRF pipelines require images and camera poses, where the latter are estimated from structure-from-motion (SFM) solvers such as COLMAP [22]. In addition to returning cameras, COLMAP also outputs sparse 3D point clouds as well as their reprojection errors. We impose a loss to encourage the distribution of a ray's termination to match the 3D keypoint, incorporating reprojection error as an uncertainty measure. This is a significantly stronger signal than reconstructing only RGB. Without depth supervision, NeRF is implicitly solving a 3D correspondence problem between multiple views. However, the sparse version of this exact problem has already been solved by SFM, whose solution is given by the sparse 3D keypoints. Therefore depth supervision improves NeRF by (softly) anchoring its search over implicit correspondences with sparse explicit ones.\n\nOur experiments show that this simple idea translates to massive improvements in training NeRFs and its variations, regarding both the training speed and the amount of training data needed. We observe that depth-supervised NeRF can accelerate model training by 2-3x while producing results with the same quality. For sparse view settings, experiments show that our method synthesizes better results compared to the original NeRF and recent sparse-views NeRF models [26,33] on both NeRF Real [15] and Redwood-3dscan [6] We also show that our depth supervision loss works well with depth derived from other sources such as a depth camera. Our code and more results are available at https://www. cs.cmu.edu/\u02dcdsnerf/.\n\n\nRelated Work\n\nNeRF from few views. NeRF [15] was originally shown to work on a large number of images with the LLFF NeRF Real dataset [14] consisting of nearly 50 images per scene. This is because fitting the NeRF volume often requires a large number of views to avoid arriving at degenerate representations. Recent works have sought to decrease the data-hungriness of NeRF in a variety of different ways. PixelNeRF [33] and metaNeRF [26] use data-driven priors recovered from a domain of training scenes to fill in missing information from test scenes. Such an approach works well when given sufficient training scenes and limited gap between the training and test distribution, but such assumptions are not particularly flexible. Another approach is to leverage priors recovered from a different task like semantic consistency [7] or depth prediction [30].\n\nSimilar to our insight that the primary difficulty in fitting few view NeRF is correctly modeling 3D geometry, MVS-NeRF [4] combines both 3D knowledge with scene priors by constructing a plane sweep volume before using a pretrained network with generalizable priors to render scenes. One appeal of an approach that utilizes 3D information is the lack of assumption it makes on the problem statement. Unlike the aforementioned approaches which depend on the availability of training data or the applicability of prior assumptions, our approach only requires the existence of 3D keypoints. This gives depth supervision the flexibility to be used not only as a standalone method, but one that can be freely incorporated into existing NeRF methods easily.\n\nFaster NeRF. Another drawback of NeRF is the lengthy optimization time required to fit the volumetric representation. Indeed Mildenhall et al. [15] trained a single scene's NeRF model for twelve hours of GPU compute. Many works [20,32] have found that the limiting factor is not learning the radiance itself, but rather oversampling the empty space during training. Indeed this is a similar intuition to the fact that the majority of the volume is actually empty, but NeRF's initialization is a median uniform density. Our insight is to apply a supervisory signal directly to the NeRF density to increase the convergence of the geometry and to encourage NeRF's density function to mimic the behavior of real world surface geometries.\n\nDepth and NeRF. Several prior works have explored ways to leverage depth information for view synthesis [24,27] and NeRF training [9,10,16,18,30]. For instance, 3D keypoints have been demonstrated to be helpful when extending NeRFs with relaxed assumptions like deformable surfaces [18] or dynamic scene flows [9]. Other works like DONeRF [16] proposed training a depth oracle to improve rendering speed by directly smartly sampling the surface of a NeRF density function. Similar to DONeRF, NerfingMVS [30] shows how a monocular depth network can be used to induce depth priors to do smarter sampling during training and inference.\n\nOur work attempts to improve NeRF-based methods by directly supervising the NeRF density function. As depth becomes a more accessible source of data, being able to apply depth supervision becomes increasingly more powerful. For example, recent works have demonstrated how depth extracted from sensors like time-of-flight cameras [1] or RGB-D Kinect sensor [2] can be applied to fit implicit functions. Building upon their insights, we provide a probabilistic formulation of the depth supervision, and show this results in meaningful improvements to NeRF and its variants.\n\n\nDepth-Supervised Ray Termination\n\nWe now present our proposed depth-supervised loss for training NeRFs. We first revisit volumetric rendering and then analyze the termination distribution for rays. We conclude with our depth-supervised distribution loss.\n\n\nVolumetric rendering revisited\n\nA Neural Radiance Field takes a set of posed images and encodes a scene as a volume density and emitted radiance. More specifically, for a given 3D point x \u2208 R 3 and a particular viewing direction d \u2208 R 3 , NeRF learns an implicit function f that estimates the differential density \u03c3 and RGB color c like so: f (x, d) = (\u03c3, c).\n\nTo render a 2D image given a pose P, we cast rays r originating from the P's center of projection o in direction d derived from its intrinsics. We integrate the implicit radiance field along this ray to compute the incoming radiance from any object that lies along d:\nC = \u221e 0 T (t)\u03c3(t)c(t)dt,(1)\nwhere t parameterizes the aforementioned ray as r(t) = o + td and T (t) = exp(\u2212 t 0 \u03c3(s)ds) checks for occlusions by integrating the differential density between 0 to t. Because the density and radiance are the outputs of neural networks, NeRF methods approximate this integral using a samplingbased Riemann sum instead. The final NeRF rendering loss is given by a reconstruction loss over colors returned from rendering the set of rays R(P) produced by a particular camera parameter P.\nL Color = E r\u2208R(P) \u0108 (r) \u2212 C(r) 2 2 .\n(2)\nRay distribution. Let us write h(t) = T (t)\u03c3(t).\nIn the appendix, we show that it is a continuous probability distribution over ray distance t that describes the likelihood of a ray terminating at t. Due to practical constraints, NeRFs assume that the scene lies between a near and far bound (t n , t f ). To ensure h(t) sums to one, NeRF implementations often treat t f as an opaque wall. With this definition, the rendered color can be written as an expectation:\nC = \u221e 0 h(t)c(t)dt = E h(t) [c(t)].\nIdealized distribution. The distribution h(t) describes the weighed contribution of sampled radiances along a ray to the final rendered value. Most scenes consist of empty spaces and opaque surfaces that restrict the weighted contribution to stem from the closest surface. This implies that the ideal ray distribution of image point with a closest-surface depth of D should be \u03b4(t \u2212 D). Figure 3(c) shows that the empirical variance of NeRF termination distributions decreases with more training views, suggesting that high quality NeRFs (trained with many views) tend to have ray distributions that approach the \u03b4-function. This insight motivates our depthsupervised ray termination loss.\n\n\nDeriving depth-supervision\n\nRecall that most NeRF pipelines require images with associated camera matrices (P 1 , P 2 , . . .), often estimated with SFM packages such as COLMAP [22]. Importantly, SFM makes use of bundle adjustment, which also returns 3D keypoints {X : x 1 , x 2 , . . . \u2208 R 3 } and visibility flags for which keypoints are seen from camera j: X j \u2282 X. Given image I j and its camera P j , we estimate the depth of visible keypoints x i \u2208 X j by simply projecting x i with P j , taking the re-projected z value as the keypoint's depth D ij . Depth uncertainty. Unsurprisingly D ij are inherently noisy estimates due to spurious correspondences, noisy camera Even if a ray traverses through multiple objects (as indicated by the multiple peaks of density \u03c3(t)), we find that the ray termination distribution h(t) is still unimodal. We find that NeRF models trained with sufficient supervision tend to have peaky, unimodal ray termination distributions as seen by the decreasing variance with more views in (c). We posit that the ideal ray termination distribution approaches a \u03b4 impulse function.\n\nparameters, or poor COLMAP optimization. The reliability of a particular keypoint x i can be measured using the average reprojection error\u03c3 i across views over which the keypoint was detected. Specifically, we model the location of the first surface encountered by a ray as a random variable D ij that is normally distributed around the COLMAP-estimated depth\nD ij with variance\u03c3 i : D ij \u223c N(D ij ,\u03c3 i ).\nCombining the intuition regarding behavior of ideal termination distribution, our objective is to minimize the KL divergence between the rendered ray distribution h ij (t) of x i 's image coordinates and the noisy depth distribution:\nE D ij KL[\u03b4(t \u2212 Dij)||hij(t)] = KL[N(Dij,\u03c3i)||hij(t)] + const.\nRay distribution loss. The above equivalence (see our appendix for proof) allows the termination distribution h(t) to be trained with probabilisitic COLMAP depth supervision:\nL Depth = Ex i \u2208X j log h(t) exp (\u2212 (t \u2212 Dij) 2 2\u03c3 2 i )dt \u2248 Ex i \u2208X j k log h k exp (\u2212 (t k \u2212 Dij) 2 2\u03c3 2 i )\u2206t k .\nOur overall training loss for NeRF is L = L Color + \u03bb D L Depth where \u03bb D is a hyper-parameter balancing color and depth supervision.\n\n\nExperiments\n\nWe first evaluate the input data efficiency on view synthesis over several datasets in Section 4.3. For relevant NeRFrelated methods, we also evaluate the error of rendered depth maps in Section 4.4. Finally, we analyze training speed improvements in Section 4.5.\n\n\nDatasets\n\nDTU MVS Dataset (DTU) [8] captures various objects from multiple viewpoints. Following Yu et al.'s setup in Pix-elNeRF [33], we evaluated on the same test scenes and views. For each scene, we used their subsets of size 3, 6, 9 training views. We run COLMAP with the ground truth calibrated camera poses to get keypoints. Images are down-sampled to a resolution of 400 \u00d7 300 for training and evaluation.\n\nNeRF Real-world Data (NeRF Real) [14,15] contains 8 real world scenes captured from many forward-facing views. We create subsets of training images for each scene of sizes 2, 5, and 10 views. For every subset, we run COLMAP [22] over its training images to estimate cameras and collect sparse keypoints for depth supervision.\n\nRedwood-3dscan (Redwood) [6] contains RGB-D videos of various objects. We select 5 RGB-D sequences and create subsets of 2, 5, and 10 training frames for each object. We run COLMAP to get their camera poses and sparse point clouds. To connect the scale of COLMAP's pose with the scanned depth, we solve a least-squares that best fits detected keypoints to the scanned depth value. Please refer to our appendix for full details.\n\n\nComparisons\n\nFirst we consider Local Lightfield Fusion (LLFF) [14], an MPI-based representation that learns from multiple view points. Next we consider a set of NeRF baselines.\n\nPixelNeRF [33] Table 1. View Synthesis on NeRF Real. We evaluate view synthesis quality for various methods when given 2, 5, 10 views from NeRF Real. We find that metaNeRF-DTU and pixelNeRF-DTU struggle to learn on NeRF Real due to its domain gap to DTU. PixelNeRF, IBRNet and MVSNeRF can benefit from incorporating the depth supervision loss to achieve their best performance. We find that our DS-NeRF outperforms these methods on a variety of metrics, but especially for the few view settings like 2 and 5 views.\n\n40K meta-iterations and then finetunes for 1000 steps on new scenes. We follow metaNeRF's ShapeNet experiments to demonstrate its susceptibility to differences between training and testing domains.\n\nIBRNet [28] extends NeRF by using a MLP and ray transformer to estimate radiance and volume density.\n\nMVSNeRF [4] initializes a plane sweep volume from 3 views before converting it to a NeRF by a pretrained network.\n\n\n!\"#$%&'\n\n!\"#$%&' ()*+,-\n.%/0$%&' 123%4$%&' #1+%/+025%6 $%&'\n\n(#$%&' )#$%&'\n\n123%4$%&' #725%/*5%689:8!\" Figure 5. Qualitative Comparison on NeRF Real. We render novel views and depth for various NeRF models trained on 2, 5, and 10 views. We find that methods trained with DTU struggle on NeRF Real while methods that use depth-supervision are able to render test views with realistic depth maps, even when only 2 views are provided. Please refer to Table 1 for quantitative comparisons. We render novel views for NeRF and DS-NeRF trained on 2 views and 5 views. NeRF fails to render novel views as evident by the many artifacts. Using MSE between rendered and sparse depth improves results slightly, but with KL Divergence, DS-NeRF is able to render images with the fewest artifacts.\n\nMVSNeRF can be further optimized using RGB supervision.\n\n\nDS-NeRF (Ours).\n\nTo illustrate the effectiveness of KL divergence, we include a variant of DS-NeRF with an MSE loss between the SFM-estimated and the rendered depth. Figure 6 qualitatively shows that KL divergence penalty produces views with less artifacts on NeRF Real sequences.  Table 2. View Synthesis on DTU. We evaluate on 3, 6, and 9 views respectively for 15 test scenes from the DTU dataset. pixelNeRF-DTU and metaNeRF-DTU perform well given that the domain overlap between training and testing. This is especially true for the few view setting as the lack of information is supplemented by exploiting dataset priors. In spite of this, DS-NeRF is still competitive on view synthesis for 6 and 9 views. DS with existing methods. As our DS loss does not require additional annotation or assumptions, our loss can be inserted into many NeRF-based methods. Here, we also incorporate our loss when finetuning pixelNeRF and IBRNet.\n\n\nFew-input view synthesis\n\nWe start by comparing each method on rendering test views from few inputs. For view synthesis, we report three metrics (PSNR, SSIM [29], and LPIPS [35]) that evaluate the quality of rendered views against a ground truth. DTU. We show evaluations on DTU in Table 2 and qualitative results in Figure 4. We find that DS-NeRF renders images from 6 and 9 input views that are competitive with pixelNeRF-DTU, however metaNeRF-DTU and pixelNeRF-DTU are able to outperform DS-NeRF on 3-views. This is not particularly surprising as both methods are trained on DTU scenes and therefore can fully leverage dataset priors. NeRF Real. As seen in Table 1, our approach renders images with better scores than than NeRF and LLFF, especially when only two and fives input views are available. We also find that metaNeRF-DTU and pixelNeRF struggle which highlights their apparent weakness. These DTU-pretrained models struggle to perform well outside of DTU. Our full approach is capable of achieving good rendering results because we do not utilize assumptions on the test scene's structure. We also add our depth supervision loss to other methods like pixelNeRF and IBRNet and find their performances improve, showing that many methods can benefit from adding depth supervision. MVSNeRF has an existing geometry prior handled by its PSV-initialization, thus we did not see an improvement from adding depth supervision.\n\nRedwood. Like NeRF Real, we find similar improvements in performance across the Redwood dataset in Table 3. Because Redwood includes depth measurements collected with a sensor, we also consider how alternative sources of depth supervision can improve results. We train DS-NeRF, replacing COLMAP supervision with the scaled Redwood depth measurements and find that the denser depth helps even more, achieving a PSNR of 20.3 on 2-views.\n\n\nDepth error\n\nWe evaluate NeRF's rendered depth by comparing them to reference \"ground truth\" depth measurements. For NeRF Real, we use reference depth of test keypoints recovered from running an all-view dense stereo reconstruction. For Redwood [6], we align their released 3D models with our cameras by running 3dMatch [34] and generate reference depths for each test view. Please refer to our arXiv version for more details regarding depth error evaluation. As shown in Table 4  not particularly surprising, it does highlight the weakness of training NeRFs only using RGB supervision. For example, in Figure 5, NeRF tends to ignore geometry and fails to produce any coherent depth map. RBG-D inputs. We consider a variant of depth supervision using RGB-D input from Redwood. We derive dense depth map for each training view using 3DMatch [34] with RGB-D input. With dense depth supervision, we can render rays for any pixel in the valid region, and apply our KL depthsupervision loss. As shown in Table 3 and Table 4, dense depth supervision produces even better-quality images and significantly lower depth errors.\n\n\nAnalysis\n\nOverfitting. Figure 2 Figure 7.\n\nDS-NeRF achieves a particular test PSNR threshold using 2-3x less training iterations than NeRF. These benefits are significantly magnified when given fewer views. In the extreme case of only 2-views, NeRF is completely unable to match DS-NeRF's performance. While these results are given in terms of training iteration, we can translate them into wall time improvements. On a single RTX A5000, a training loop of DS-NeRF takes \u223c 362.4 ms/iter while NeRF needs \u223c 359.8 ms/iter. Thus in the 5-view case, DS-NeRF achieves NeRF's peak test PSNR around 13 hours faster, a massive improvement considering the negligible cost. Discussion. We introduce Depth-supervised NeRF, a model for learning neural radiance fields that takes advantage of depth supervision. Our model uses \"free\" supervision provided by sparse 3D point clouds computed during standard SFM pre-processing steps. This additional supervision has a significant impact; DS-NeRF trains 2-3x faster and produces better results from fewer training views (improving PSNR from 13.5 to 20.2). While recent research has sought to improve NeRF by exploiting priors learned from categoryspecific training data, our approach requires no training and thus generalizes (in principle) to any scenes on which SFM succeeds. This allows us to integrate depth supervision to many NeRF-based methods and observe significant benefits. Finally, we provide cursory experiments that explore alternate forms of depth supervision such as active depth sensors. Please see our arXiv version for a discussion on limitations and societal impact of our paper.\n\n\nA. Discussion\n\nLimitations. Depth supervision is only as good as the estimates of depth, as such poor SfM or bad depth measurements can result in failure of the optimization process. Next we assume a Gaussian distribution models the uncertainty of the keypoint's location, but such a simplifying assumption is not necessarily true especially for depth derived from other sources.\n\nSocietal Impact. Depth supervision is a technique which empowers NeRF to operate on a wider range of experimental setups. While novel view synthesis is not synthetic media, it can open the door to abuse when generating trajectories through a scene. There may also be privacy concerns as using ubiquitous sensor technology to better render sharper details of a scene could capture personally identifiable information.\n\n\nB. Derivation Details\n\n\nB.1. Derivation of h(t) as a probability distribution\n\nIn Section 3.1, we claim that h(t), which is a function that describes a contribution weight from a particular distance t, is a continuous probability distribution over ray termination. We can verify this by proving that h(t) is non-negative and the integral of h(t) over t's domain is equal to 1.\nh(t) = T (t)\u03c3(t) = exp[\u2212 t 0 \u03c3(s)ds]\u03c3(t)\nWe start by assuming that \u03c3(s) is a real-valued, nonnegative (typically a ReLU or softplus activation) function describing the differential density s units away from the camera origin. Because \u03c3(\u00b7) is real-valued, the inner-value integral over \u03c3(s) must also be real, therefore T (t) must be non-negative. As a result, h(t) is the product of two nonnegative functions and is therefore non-negative for all values of t, satisfying the first property of a probability distribution. The next step is to show that the integral of h(t) over the domain of t is 1.\n\nTo do this we need to make an additional assumption that for every ray cast in a scene will eventually intersect an opaque object: \u2200a \u2265 0 \u221e a \u03c3(s)ds = \u221e. This is true for most scenes we care about modeling as radiance is emitted by surfaces. \n\u221e 0 h(t)dt = \u2212 \u221e 0 exp[u(t)] du(t) dt dt = exp[u(t)] 0 \u221e = exp[u(0)] \u2212 exp[u(\u221e)] = exp[0] \u2212 exp[\u2212\u221e] = 1\nThis shows that under the above assumptions, h(t) is guaranteed to be a probability distribution. Due to practical constraints, NeRFs are unable to sample the volume to infinity and instead assume that the scene lies between a near and far bound. To ensure the above assumption still holds true, NeRF implementations will often treat the furthest radiance as an opaque wall.\n\n\nB.2. Depth-supervision implementation\n\nDepth supervision is implemented by projecting a ray with direction (in local camera coordinates) given by the image coordinates of a detected keypoint and \u22121 in the camera axis. We shoot this ray into a scene and render its depth using the same sampling procedure described in NeRF.\n\nFor setups where the training data can fit into GPU memory, the most time-consuming part during training comes from the many forward passes required for a single ray marching rendering step. To gain the benefits of faster training, we must simultaneously train with color supervision and depth supervision with a single ray marching procedure.\n\nTo do this, we exploit the fact that image coordinates are actually continuous and the pixels are only samples of the color function at discrete intervals. Therefore we can interpolate RGB supervision for rays corresponding to detected keypoints, allowing us to supervise RGB and depth at the same time. We allocate a portion of the training rays to this.\n\n\nB.3. COLMAP details\n\nWe run the COLMAP with the default configuration on the limited views (the same as NeRF training inputs) (e.g., 2 views). The SfM output is used only during training and is not required for synthesizing novel view.\n\nB.4. metaNeRF and pixelNeRF baselines metaNeRF. We use the metaNeRF implementation released by Tancik et al. [26] which uses Jax. For meta-initialization comparison, we adopt their ShapeNet experimental setup of training a category-specific initialization and further finetuning for a fixed number of iterations. In our case, we treat the entire DTU dataset as a single category. We used 64 inner loop optimizations and trained for 40K outer loop steps. We subsequently fine-tune for 1K steps.\n\nFor adapting metaNeRF across different domains, we have to deal with different ray bounds and coordinate scales. This is challenging, and we do not directly address this issue as devising approaches to transferring NeRF metainitialization across different datasets is beyond the scope of this paper. Instead, we used the default scaling and ray bounds provided by previous NeRF works for DTU and NeRF Real [15,33].\n\npixelNeRF-DTU with finetuning. We start with Yu et al. [33]'s pre-trained models on DTU dataset. For a new input scene, we finetune the weights of pixelNeRF on training views for 20K iterations before subsequently evaluating view synthesis on test views.\n\npixelNeRF-DTU with finetuning and DS. This variation is implemented similarly as pixelNeRF-DTU with finetuning, but the pixelNeRF finetuning stage incorporates our depth supervision loss.\n\n\nB.5. Dataset Splits\n\nNeRF Real-world. We split each scene from NeRF Real into training views and test views. Because the number of views of each scene varies, we split every eighth image id into the test set (0, 8, 16, 24, . . . ) and construct training views that are evenly distributed over the remaining viewpoint id numbers. This setting gives us sufficient coverage to train different NeRF experiments. We create subsets from these training views of specific sizes to evaluate performance on few-input view synthesis.\n\nRedwood 3d-scan. We selected five test scene from the Redwood 3dscan dataset: table, plant, chair, car, and stool.\n\nEach scene is constructed using 15 frames of RGB and depth. These 15 frames are further sub-divided into training views and test views. We construct training sets with the following viewpoints [5,11,2,8,14,1,4,7,10,13], truncating when working with a smaller subset (e.g. 2-views uses only 5 and 11). We evaluate the quality of view synthesis on the test views [0, 3,6,9,12].\n\n\nB.6. Depth Error Evaluation\n\nTo evaluate the depth error of these different baselines, we need to first compute a reference depth of an input scene from test camera poses.\n\nDepth evaluation on NeRF Real. For a given set of training views from a scene, we use COLMAP's SfM algorithm to get sparse keypoints and camera poses. In addition, we run dense MVS on all training and test views to get a reference depth map from every view and test poses. To align dense MVS depth with the SfM keypoint depth obtained from the training views, we compute a scale a and shift b scalar that aligns the keypoint depth visible in a training view to its MVS depth. More specifically, we solve the following least squares optimization on those detected keypoints:\nmin a,b p\u2208P (aD SfM (p) \u2212 b \u2212 D MVS (p)) 2 ,(3)\nwhere P is the set of detected and visible keypoints p for a scene, D SfM is the sparse depth we use for training, and D MVS is the dense depth maps we use for evaluation. Depth error can be computed by transforming the rendered depthD from a test camera c.\nErr(D) = aD \u2212 b \u2212 D MVS 2(4)\nNote that dense MVS depth is only used for depth evaluation. It is not used during training and test by any method.\n\n\nC. Additional Experiments\n\n\nC.1. NeRF Real with pixelNeRF and metaNeRF\n\nWhile pixelNeRF and metaNeRF can achieve reasonable results when given sufficient pre-training data and a small train-test domain gap, many real-world applications cannot rely on the assumptions of similar test domain and sufficient training samples. We further highlight this by showing what happens to these baselines in such a scenario. To construct a more realistic setting for NeRF-based applications, we perform a 4-fold evaluation by splitting NeRF Real into 6 training scenes and 2 test scenes. The 4 test-splits are: fernhorn, flower-fortress, leaves-orchids, and room-trex.\n\nFor every split, we use the remaining 6 training scenes to learn NeRF Real priors for pixelNeRF and metaNeRF and evaluate the view synthesis results after fine-tuning the model on the test scenes. We show these baseline results in Table 5 and find that meta-learning NeRF baselines struggle to properly leverage the priors observed during training.\n\nFor metaNeRF, we use the same ShapeNet setup described in Section B.4. We train on NeRF Real training scenes for 40K steps and then finetune for 1K steps on test scenes (metaNeRF-NeRF Real). We also tried a slightly longer finetuning duration of 5K steps, but found no significant improvement in results. We observed the following failure mode; within NeRF Real training, certain scenes were dominating the learned prior such that even with many iterations of fine-tuning on test-scenes, NeRF would only render cloudy looking textures from training scenes like flowers and room.\n\nFor pixelNeRF we train the network from scratch on the 6 training scenes. Our training procedure uses 3 input views for each training iteration to finetune the encoder and decoder over 40K iterations. Here we again find that the categoryspecific models struggle due to the lack of sufficient training data to learn these priors.  Table 5. View Synthesis on NeRF Real (4-fold cross-validation): Because metaNeRF and pixelNeRF may perform better by training on sequences similar to the validation set, we perform 4-fold cross validation on 8 NeRF Real scenes. We find that training on NeRF Real scenes does not improve metaNerf/pixelNerf performance over training on DTU (see main paper), possibly because DTU-train has 88 scenes while a 4-fold NeRF Real training dataset has 6 scenes.\n\nGroundtruth LLFF NeRF DS-NeRF (Ours) Figure 8. We visualize novel views on NeRF Real rendered by LLFF (MPI-based), NeRF, and DS-NeRF inferred from 2 input views. We find that despite LLFF being an MPI-based method, it still struggles to visually match the quality produced by DS-NeRF, agreeing with our quantitive experiments.\n\n2-view 5-view 10-view Figure 9. We visualize the keypoints detected by COLMAP with 2, 5, 10 input views to show the available depth supervision.\n\n\nC.2. MPI-based Experiments\n\nWe consider Local Light Field Fusion (LLFF) [14], an MPI-based method, as a baseline on their dataset (referred to as NeRF Real). We have included qualitative comparisons on NeRF Real with LLFF in Figure 8. For quantitative comparison refer to Tab. 1 in the main paper.\n\nWe find that while multi-plane images are a powerful representation, they have significant restrictions such as assuming the renderable scene lies within a central frustrum and that depths from different views are related by a homography. As such applying LLFF to scenes like DTU and Redwood is impractical given the complex camera viewpoints compared to NeRF Real.\n\n\nC.3. Sparseness of keypoints\n\nWe report the performance with varying sparseness of keypoint supervision. We ablate DS-NeRF by uniformly removing detected keypoints while fixing the input to 5 views. When given (20%, 50%, and 100%) of possible keypoints, DSNeRF's PSNR on test view is (21.5, 22.2, 22.6) respectively. Dropping out keypoints reasonably weakens the performance while still outperforming NeRF baseline (18.2).\n\nWe additionally consider how the quantity of keypoints degrade as fewer views are provided to COLMAP specifically. On average, there are 1615, 2172, 2621 keypoints for depth supervision per training view when given 2, 5, 10 views respectively. We also show qualitative examples in Figure 9 for different number of input views.\n\nFigure 3 .\n3Ray Termination Distribution. (a) We plot various NeRF components over the distance traveled by the ray.\n\nFigure 6 .\n6Depth Supervision Ablations.\n\n\ns)ds]\u03c3(t)dtLet u(t) = \u2212 t 0 \u03c3(s)ds. We can compute the derivative du(t) dt = \u2212\u03c3(t) and rewrite the above equation in terms of u(t) and du(t) dt .\n\n\nexpands upon NeRF by using an encoder to train a general model across multiple scenes. pixelNeRF-DTU is evaluated using the released DTU checkpoint. For cases where the train and test domain are different, we finetune using RGB supervision for additional iterations on each test scene to get pixelNeRF finetuned.MetaNeRF[26] finds a better NeRF initialization over a domain of training scenes before running test-time optimization on new scenes. Because DTU is the only dataset large enough for meta-learning, we only consider the metaNeRF-DTU baseline which learns an initialization over DTU forFigure 4. View Synthesis on DTU and Redwood. PixelNeRF, which is pre-trained on DTU, performs the best when given 3-views, although we find DS-NeRF to be visually competitive when more views are available. On Redwood, DS-NeRF is the only baseline to perform well on the 2-views setting.DSNeRF \n\nmetaNeRF \n-DTU \n\npixelNeRF \n-DTU \n\nNeRF \n\nDTU \n\n3-view \n6-view \n9-view \n2-view \n5-view \n10-view \n\nRedwood \n\nPSNR\u2191 \nSSIM\u2191 \nLPIPS\u2193 \nNeRF Real [14] \n2-view \n5-view \n10-view \n2-view \n5-view \n10-view \n2-view \n5-view \n10-view \n\nLLFF \n14.3 \n17.6 \n22.3 \n0.48 \n0.49 \n0.53 \n0.55 \n0.51 \n0.53 \nNeRF \n13.5 \n18.2 \n22.5 \n0.39 \n0.57 \n0.67 \n0.56 \n0.50 \n0.52 \nmetaNeRF-DTU \n13.1 \n13.8 \n14.3 \n0.43 \n0.45 \n0.46 \n0.89 \n0.88 \n0.87 \npixelNeRF-DTU \n9.6 \n9.5 \n9.7 \n0.39 \n0.40 \n0.40 \n0.82 \n0.87 \n0.81 \nfinetuned \n18.2 \n22.0 \n24.1 \n0.56 \n0.59 \n0.63 \n0.53 \n0.53 \n0.41 \nfinetuned w/ DS \n18.9 \n22.1 \n24.4 \n0.54 \n0.61 \n0.66 \n0.55 \n0.47 \n0.42 \nIBRNet \n14.4 \n21.8 \n24.3 \n0.50 \n0.51 \n0.54 \n0.53 \n0.54 \n0.51 \nfinetuned w/ DS \n19.3 \n22.3 \n24.5 \n0.63 \n0.66 \n0.68 \n0.39 \n0.36 \n0.38 \nMVSNeRF \n-\n17.2 \n17.2 \n-\n0.61 \n0.60 \n-\n0.37 \n0.36 \nfintuned \n-\n21.8 \n22.9 \n-\n0.70 \n0.74 \n-\n0.27 \n0.23 \nfintuned w/ DS \n-\n22.0 \n22.9 \n-\n0.70 \n0.75 \n-\n0.27 \n0.25 \n\nDS-NeRF \nMSE \n19.5 \n22.2 \n24.7 \n0.65 \n0.69 \n0.71 \n0.43 \n0.40 \n0.37 \nKL divergence \n20.2 \n22.6 \n24.9 \n0.67 \n0.69 \n0.72 \n0.39 \n0.35 \n0.34 \n\n\n\n\nFigure 7. Faster Training. We train DS-NeRF and NeRF under identical conditions and observe that DS-NeRF is able to reach NeRF's peak PSNR quality in a fraction of the number of iterations across. For 2 views, we find that NeRF is unable to match DS-NeRF's performance.2-view \n\n5-view \n10-view \n\nIterations (in thousands) \n\nTest PSNR \n\nIterations (in thousands) \nIterations (in thousands) \n\n2x \nSpeedup \n\n3x \nSpeedup \n\nPSNR\u2191 \nSSIM\u2191 \nLPIPS\u2193 \nDTU [8] \n3-view \n6-view \n9-view \n3-view \n6-view \n9-view \n3-view \n6-view \n9-view \n\nNeRF \n9.9 \n18.6 \n22.1 \n0.37 \n0.72 \n0.82 \n0.62 \n0.35 \n0.26 \nmetaNeRF-DTU \n18.2 \n18.8 \n20.2 \n0.60 \n0.61 \n0.67 \n0.40 \n0.41 \n0.35 \npixelNeRF-DTU \n19.3 \n20.4 \n21.1 \n0.70 \n0.73 \n0.76 \n0.39 \n0.36 \n0.34 \n\nDS-NeRF \nMSE \n16.5 \n20.5 \n22.2 \n0.54 \n0.73 \n0.77 \n0.48 \n0.31 \n0.26 \nKL divergence \n16.9 \n20.6 \n22.3 \n0.57 \n0.75 \n0.81 \n0.45 \n0.29 \n0.24 \n\n\n\n\n, DS-NeRF, trained with supervision obtained only from depth in training views, is able to estimate depth more accurately than all the other NeRF models. While this isPSNR\u2191 \n\nSSIM\u2191 \nLPIPS\u2193 \nRedwood-3dscan [6] \n2-view \n5-view \n10-view \n2-view \n5-view \n10-view \n2-view \n5-view \n10-view \n\nNeRF \n10.5 \n22.4 \n23.4 \n0.38 \n0.75 \n0.82 \n0.51 \n0.45 \n0.45 \nmetaNeRF-DTU \n14.3 \n14.6 \n15.1 \n0.37 \n0.39 \n0.40 \n0.76 \n0.76 \n0.75 \npixelNeRF-DTU \n12.7 \n12.9 \n12.8 \n0.43 \n0.47 \n0.50 \n0.76 \n0.75 \n0.70 \nMVSNeRF-DTU \n-\n17.1 \n17.1 \n-\n0.54 \n0.53 \n-\n0.63 \n0.63 \nfinetuned \n-\n22.7 \n23.1 \n-\n0.78 \n0.78 \n-\n0.36 \n0.34 \n\nDS-NeRF \n18.1 \n22.9 \n23.8 \n0.62 \n0.78 \n0.81 \n0.40 \n0.34 \n0.42 \n\nDS-NeRF w/ RGB-D \n20.3 \n23.4 \n23.9 \n0.73 \n0.77 \n0.84 \n0.36 \n0.35 \n0.28 \n\nTable 3. View Synthesis on Redwood. We evaluate view synthesis on 2, 5, and 10 input views on the Redwood dataset. DS-NeRF (with \nCOLMAP [22] inputs) outperforms baselines on various metrics across varying numbers of views. Learning DS-NeRF with the RGB-D \nreconstruction output [34] further improves performance, highlighting the potential of applying our method alongside other sources of depth. \n\nDepth err%\u2193 \nNeRF real-world \nRedwood-3dscan \n2-view \n5-view \n10-view \n2-view \n5-view \n10-view \n\nNeRF \n20.32 \n15.00 \n12.41 \n25.32 \n24.34 \n21.34 \nmetaNeRF-DTU \n22.23 \n22.07 \n22.30 \n20.84 \n21.12 \n20.96 \npixelNeRF-DTU \n22.12 \n22.09 \n22.06 \n19.46 \n19.87 \n19.54 \n\nDS-NeRF \n10.41 \n8.61 \n8.15 \n11.42 \n10.43 \n9.43 \nDS-NeRF w/ RGBD \n-\n-\n-\n5.81 \n5.31 \n4.22 \n\n\n\nTable 4 .\n4Depth Error. We compare rendered depth to reference \"ground-truth\" depth obtained from NeRF Real and Redwood RGB-D. DS-NeRF is able to extract better geometry as indicated by the lower depth errors from test views. We also show DS-NeRF trained with Redwood's dense supervision can significantly improve NeRF's ability to model the underlying geometry.\nAcknowledgments. We thank Takuya Narihira, Akio Hayakawa, Sheng-Yu Wang, Richard Tucker, Konstantinos Rematas, and Michaelu Zollh\u00f6fer for helpful discussion. We are grateful for the support from Sony Corporation, Singapore DSTA, and the CMU Argo AI Center for Autonomous Vehicle Research.AppendixWe provide additional implementation details, discussions, and experimental results.\nT\u00f6rf: Time-of-flight radiance fields for dynamic scene view synthesis. Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, Matthew O&apos; Toole, Advances in Neural Information Processing Systems (NeurIPS). Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, and Matthew O'Toole. T\u00f6rf: Time-of-flight radiance fields for dynamic scene view synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3\n\nNeural rgb-d surface reconstruction. Dejan Azinovi\u0107, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, Justus Thies, 2022. 3IEEE Conference on Computer Vision and Pattern Recognition. Dejan Azinovi\u0107, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, and Justus Thies. Neural rgb-d surface reconstruction. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3\n\npi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. Marco Eric R Chan, Petr Monteiro, Jiajun Kellnhofer, Gordon Wu, Wetzstein, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1\n\nMvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su, IEEE International Conference on Computer Vision (ICCV), 2021. 25Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generaliz- able radiance field reconstruction from multi-view stereo. In IEEE International Conference on Computer Vision (ICCV), 2021. 2, 5\n\nStereo radiance fields (srf): Learning view synthesis from sparse views of novel scenes. Julian Chibane, Aayush Bansal, Verica Lazova, Gerard Pons-Moll, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEJulian Chibane, Aayush Bansal, Verica Lazova, and Gerard Pons-Moll. Stereo radiance fields (srf): Learning view synthe- sis from sparse views of novel scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, jun 2021. 1\n\nSungjoon Choi, Qian-Yi Zhou, Stephen Miller, Vladlen Koltun, arXiv:1602.02481A large dataset of object scans. 7Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen Koltun. A large dataset of object scans. arXiv:1602.02481, 2016. 2, 4, 7, 8\n\nPutting nerf on a diet: Semantically consistent few-shot view synthesis. Ajay Jain, Matthew Tancik, Pieter Abbeel, IEEE International Conference on Computer Vision (ICCV). 2021Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In IEEE International Conference on Computer Vision (ICCV), 2021. 2\n\nLarge scale multi-view stereopsis evaluation. Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, Henrik Aanaes, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 47Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanaes. Large scale multi-view stereopsis evalu- ation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014. 4, 7\n\nNeural scene flow fields for space-time view synthesis of dynamic scenes. Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)13Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1, 3\n\nNeural sparse voxel fields. Lingjie Liu, Jiatao Gu, Tat-Seng Kyaw Zaw Lin, Christian Chua, Theobalt, Advances in Neural Information Processing Systems (NeurIPS). 2020Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 3\n\nEditing conditional radiance fields. Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell, IEEE International Conference on Computer Vision (ICCV). 2021Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional radiance fields. In IEEE International Conference on Computer Vision (ICCV), 2021. 1\n\nNeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. Ricardo Martin-Brualla, Noha Radwan, S M Mehdi, Jonathan T Sajjadi, Alexey Barron, Daniel Dosovitskiy, Duckworth, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Saj- jadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1\n\nNeural rerendering in the wild. Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, Ricardo Martin-Brualla, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Moustafa Meshry, Dan B. Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin- Brualla. Neural rerendering in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1\n\nLocal light field fusion: Practical view synthesis with prescriptive sampling guidelines. Ben Mildenhall, P Pratul, Rodrigo Srinivasan, Nima Khademi Ortiz-Cayon, Ravi Kalantari, Ren Ramamoorthi, Abhishek Ng, Kar, ACM Transactions on Graphics. 513Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthe- sis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 2019. 2, 4, 5, 13\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, European Conference on Computer Vision (ECCV). 12Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. In European Conference on Computer Vision (ECCV), 2020. 1, 2, 3, 4, 12\n\nDONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, H Joerg, Chakravarty R Alla Mueller, Anton S Chaitanya, Markus Kaplanyan, Steinberger, 2021. 3Computer Graphics Forum. 404Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An- ton S. Kaplanyan, and Markus Steinberger. DONeRF: To- wards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Computer Graphics Forum, 40(4), 2021. 3\n\nDeepsdf: Learning continuous signed distance functions for shape representation. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jeong Joon Park, Peter Florence, Julian Straub, Richard New- combe, and Steven Lovegrove. Deepsdf: Learning continu- ous signed distance functions for shape representation. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2019. 1\n\nDeformable neural radiance fields. Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, Ricardo Martin-Brualla, IEEE International Conference on Computer Vision (ICCV). 13Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Deformable neural radiance fields. In IEEE International Conference on Computer Vision (ICCV), 2021. 1, 3\n\nD-nerf: Neural radiance fields for dynamic scenes. Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1\n\nKilonerf: Speeding up neural radiance fields with thousands of tiny mlps. Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger, 2021. 3IEEE International Conference on Computer Vision (ICCV. Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In IEEE International Conference on Computer Vision (ICCV), 2021. 3\n\nPifu: Pixel-aligned implicit function for high-resolution clothed human digitization. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li, IEEE International Conference on Computer Vision (ICCV). Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor- ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitiza- tion. In IEEE International Conference on Computer Vision (ICCV), 2019. 1\n\nStructure-from-motion revisited. Johannes Lutz Sch\u00f6nberger, Jan-Michael Frahm, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. Structure-from-motion revisited. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2, 3, 4, 8\n\nGraf: Generative radiance fields for 3d-aware image synthesis. Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger, Advances in Neural Information Processing Systems (NeurIPS). 2020Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 1\n\n3d photography using context-aware layered depth inpainting. Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 3d photography using context-aware layered depth inpainting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3\n\nScene representation networks: Continuous 3d-structureaware neural scene representations. Vincent Sitzmann, Michael Zollh\u00f6fer, Gordon Wetzstein, Advances in Neural Information Processing Systems (NeurIPS). Vincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure- aware neural scene representations. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 1\n\nLearned initializations for optimizing coordinate-based neural representations. Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, Ren Ng, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 11Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 4, 11\n\nSingle-view view synthesis with multiplane images. Richard Tucker, Noah Snavely, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020Richard Tucker and Noah Snavely. Single-view view synthe- sis with multiplane images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3\n\nIbrnet: Learning multi-view image-based rendering. Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini- vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin- Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), 2021. 5\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, IEEE transactions on image processing. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004. 7\n\nNerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, Jie Zhou, IEEE International Conference on Computer Vision (ICCV). 23Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. In IEEE Inter- national Conference on Computer Vision (ICCV), 2021. 2, 3\n\niNeRF: Inverting neural radiance fields for pose estimation. Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. iNeRF: Invert- ing neural radiance fields for pose estimation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021. 1\n\nPlenOctrees for real-time rendering of neural radiance fields. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa, 2021. 3IEEE International Conference on Computer Vision (ICCV. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for real-time rendering of neural radiance fields. In IEEE International Conference on Computer Vision (ICCV), 2021. 3\n\npixelnerf: Neural radiance fields from one or few images. Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 12Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2021. 2, 4, 12\n\nLearning local geometric descriptors from rgb-d reconstructions. Andy Zeng, Shuran Song, Matthias Nie\u00dfner, Matthew Fisher, Jianxiong Xiao, Thomas Funkhouser, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. 3Andy Zeng, Shuran Song, Matthias Nie\u00dfner, Matthew Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2017. 7, 8\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2018. 7\n", "annotations": {"author": "[{\"end\":108,\"start\":67},{\"end\":128,\"start\":109},{\"end\":170,\"start\":129},{\"end\":213,\"start\":171}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":74},{\"end\":119,\"start\":116},{\"end\":140,\"start\":137},{\"end\":183,\"start\":176}]", "author_first_name": "[{\"end\":73,\"start\":67},{\"end\":115,\"start\":109},{\"end\":136,\"start\":129},{\"end\":175,\"start\":171}]", "author_affiliation": "[{\"end\":107,\"start\":80},{\"end\":127,\"start\":121},{\"end\":169,\"start\":142},{\"end\":212,\"start\":185}]", "title": "[{\"end\":64,\"start\":1},{\"end\":277,\"start\":214}]", "venue": null, "abstract": "[{\"end\":2421,\"start\":648}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4375,\"start\":4372},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4378,\"start\":4375},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4381,\"start\":4378},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4402,\"start\":4398},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4405,\"start\":4402},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4440,\"start\":4436},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4443,\"start\":4440},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4446,\"start\":4443},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4491,\"start\":4488},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4494,\"start\":4491},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4497,\"start\":4494},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4529,\"start\":4526},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4532,\"start\":4529},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4535,\"start\":4532},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4591,\"start\":4587},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6202,\"start\":6198},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7387,\"start\":7383},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7390,\"start\":7387},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7413,\"start\":7409},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7436,\"start\":7433},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7678,\"start\":7674},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7772,\"start\":7768},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8054,\"start\":8050},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8072,\"start\":8068},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8466,\"start\":8463},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8491,\"start\":8487},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8617,\"start\":8614},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9394,\"start\":9390},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9479,\"start\":9475},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9482,\"start\":9479},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10090,\"start\":10086},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10093,\"start\":10090},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10115,\"start\":10112},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10118,\"start\":10115},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10121,\"start\":10118},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10124,\"start\":10121},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10127,\"start\":10124},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10268,\"start\":10264},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10295,\"start\":10292},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10325,\"start\":10321},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10489,\"start\":10485},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10948,\"start\":10945},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10975,\"start\":10972},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14007,\"start\":14003},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16384,\"start\":16381},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16482,\"start\":16478},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16800,\"start\":16796},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16803,\"start\":16800},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16991,\"start\":16987},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17118,\"start\":17115},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17586,\"start\":17582},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17712,\"start\":17708},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18424,\"start\":18420},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18526,\"start\":18523},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20571,\"start\":20567},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20587,\"start\":20583},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22526,\"start\":22523},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22602,\"start\":22598},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23122,\"start\":23118},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28912,\"start\":28908},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29704,\"start\":29700},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29707,\"start\":29704},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29769,\"start\":29765},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30992,\"start\":30989},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30995,\"start\":30992},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30997,\"start\":30995},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30999,\"start\":30997},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31002,\"start\":30999},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31004,\"start\":31002},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31006,\"start\":31004},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31008,\"start\":31006},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31011,\"start\":31008},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31014,\"start\":31011},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31163,\"start\":31161},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31165,\"start\":31163},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31167,\"start\":31165},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31170,\"start\":31167},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35297,\"start\":35293},{\"end\":36190,\"start\":36172},{\"end\":36309,\"start\":36303},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":37273,\"start\":37269}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36756,\"start\":36639},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36798,\"start\":36757},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36946,\"start\":36799},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38885,\"start\":36947},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39746,\"start\":38886},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41227,\"start\":39747},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41591,\"start\":41228}]", "paragraph": "[{\"end\":2889,\"start\":2432},{\"end\":4202,\"start\":2902},{\"end\":5244,\"start\":4219},{\"end\":5937,\"start\":5246},{\"end\":6916,\"start\":5939},{\"end\":7631,\"start\":6918},{\"end\":8492,\"start\":7648},{\"end\":9245,\"start\":8494},{\"end\":9980,\"start\":9247},{\"end\":10614,\"start\":9982},{\"end\":11187,\"start\":10616},{\"end\":11444,\"start\":11224},{\"end\":11806,\"start\":11479},{\"end\":12075,\"start\":11808},{\"end\":12590,\"start\":12104},{\"end\":12632,\"start\":12629},{\"end\":13097,\"start\":12682},{\"end\":13823,\"start\":13134},{\"end\":14937,\"start\":13854},{\"end\":15298,\"start\":14939},{\"end\":15578,\"start\":15345},{\"end\":15816,\"start\":15642},{\"end\":16067,\"start\":15934},{\"end\":16346,\"start\":16083},{\"end\":16761,\"start\":16359},{\"end\":17088,\"start\":16763},{\"end\":17517,\"start\":17090},{\"end\":17696,\"start\":17533},{\"end\":18212,\"start\":17698},{\"end\":18411,\"start\":18214},{\"end\":18513,\"start\":18413},{\"end\":18628,\"start\":18515},{\"end\":18654,\"start\":18640},{\"end\":19413,\"start\":18707},{\"end\":19470,\"start\":19415},{\"end\":20407,\"start\":19490},{\"end\":21839,\"start\":20436},{\"end\":22275,\"start\":21841},{\"end\":23395,\"start\":22291},{\"end\":23439,\"start\":23408},{\"end\":25031,\"start\":23441},{\"end\":25413,\"start\":25049},{\"end\":25831,\"start\":25415},{\"end\":26210,\"start\":25913},{\"end\":26809,\"start\":26252},{\"end\":27053,\"start\":26811},{\"end\":27532,\"start\":27158},{\"end\":27857,\"start\":27574},{\"end\":28202,\"start\":27859},{\"end\":28559,\"start\":28204},{\"end\":28797,\"start\":28583},{\"end\":29292,\"start\":28799},{\"end\":29708,\"start\":29294},{\"end\":29964,\"start\":29710},{\"end\":30153,\"start\":29966},{\"end\":30678,\"start\":30177},{\"end\":30794,\"start\":30680},{\"end\":31171,\"start\":30796},{\"end\":31345,\"start\":31203},{\"end\":31920,\"start\":31347},{\"end\":32226,\"start\":31969},{\"end\":32371,\"start\":32256},{\"end\":33029,\"start\":32446},{\"end\":33379,\"start\":33031},{\"end\":33959,\"start\":33381},{\"end\":34744,\"start\":33961},{\"end\":35072,\"start\":34746},{\"end\":35218,\"start\":35074},{\"end\":35518,\"start\":35249},{\"end\":35885,\"start\":35520},{\"end\":36310,\"start\":35918},{\"end\":36638,\"start\":36312}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12103,\"start\":12076},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12628,\"start\":12591},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12681,\"start\":12633},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13133,\"start\":13098},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15344,\"start\":15299},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15641,\"start\":15579},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15933,\"start\":15817},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18690,\"start\":18655},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26251,\"start\":26211},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27157,\"start\":27054},{\"attributes\":{\"id\":\"formula_10\"},\"end\":31968,\"start\":31921},{\"attributes\":{\"id\":\"formula_11\"},\"end\":32255,\"start\":32227}]", "table_ref": "[{\"end\":17720,\"start\":17713},{\"end\":19086,\"start\":19079},{\"end\":19762,\"start\":19755},{\"end\":20699,\"start\":20692},{\"end\":21077,\"start\":21070},{\"end\":21947,\"start\":21940},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22757,\"start\":22750},{\"end\":23284,\"start\":23277},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23296,\"start\":23289},{\"end\":33269,\"start\":33262},{\"end\":34298,\"start\":34291}]", "section_header": "[{\"end\":2900,\"start\":2892},{\"attributes\":{\"n\":\"1.\"},\"end\":4217,\"start\":4205},{\"attributes\":{\"n\":\"2.\"},\"end\":7646,\"start\":7634},{\"attributes\":{\"n\":\"3.\"},\"end\":11222,\"start\":11190},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11477,\"start\":11447},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13852,\"start\":13826},{\"attributes\":{\"n\":\"4.\"},\"end\":16081,\"start\":16070},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16357,\"start\":16349},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17531,\"start\":17520},{\"end\":18638,\"start\":18631},{\"end\":18705,\"start\":18692},{\"end\":19488,\"start\":19473},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20434,\"start\":20410},{\"attributes\":{\"n\":\"4.4.\"},\"end\":22289,\"start\":22278},{\"attributes\":{\"n\":\"4.5.\"},\"end\":23406,\"start\":23398},{\"end\":25047,\"start\":25034},{\"end\":25855,\"start\":25834},{\"end\":25911,\"start\":25858},{\"end\":27572,\"start\":27535},{\"end\":28581,\"start\":28562},{\"end\":30175,\"start\":30156},{\"end\":31201,\"start\":31174},{\"end\":32399,\"start\":32374},{\"end\":32444,\"start\":32402},{\"end\":35247,\"start\":35221},{\"end\":35916,\"start\":35888},{\"end\":36650,\"start\":36640},{\"end\":36768,\"start\":36758},{\"end\":41238,\"start\":41229}]", "table": "[{\"end\":38885,\"start\":37831},{\"end\":39746,\"start\":39157},{\"end\":41227,\"start\":39916}]", "figure_caption": "[{\"end\":36756,\"start\":36652},{\"end\":36798,\"start\":36770},{\"end\":36946,\"start\":36801},{\"end\":37831,\"start\":36949},{\"end\":39157,\"start\":38888},{\"end\":39916,\"start\":39749},{\"end\":41591,\"start\":41240}]", "figure_ref": "[{\"end\":2431,\"start\":2423},{\"end\":4741,\"start\":4733},{\"end\":5410,\"start\":5402},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13529,\"start\":13521},{\"end\":18742,\"start\":18734},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19647,\"start\":19639},{\"end\":20735,\"start\":20727},{\"end\":22889,\"start\":22881},{\"end\":23429,\"start\":23421},{\"end\":23438,\"start\":23430},{\"end\":34791,\"start\":34783},{\"end\":35104,\"start\":35096},{\"end\":35454,\"start\":35446},{\"end\":36601,\"start\":36593}]", "bib_author_first_name": "[{\"end\":42052,\"start\":42044},{\"end\":42065,\"start\":42060},{\"end\":42080,\"start\":42075},{\"end\":42098,\"start\":42091},{\"end\":42113,\"start\":42104},{\"end\":42129,\"start\":42124},{\"end\":42154,\"start\":42139},{\"end\":42525,\"start\":42520},{\"end\":42543,\"start\":42536},{\"end\":42563,\"start\":42560},{\"end\":42565,\"start\":42564},{\"end\":42583,\"start\":42575},{\"end\":42599,\"start\":42593},{\"end\":42974,\"start\":42969},{\"end\":42992,\"start\":42988},{\"end\":43009,\"start\":43003},{\"end\":43028,\"start\":43022},{\"end\":43444,\"start\":43439},{\"end\":43458,\"start\":43451},{\"end\":43470,\"start\":43463},{\"end\":43486,\"start\":43477},{\"end\":43499,\"start\":43494},{\"end\":43513,\"start\":43507},{\"end\":43521,\"start\":43518},{\"end\":43933,\"start\":43927},{\"end\":43949,\"start\":43943},{\"end\":43964,\"start\":43958},{\"end\":43979,\"start\":43973},{\"end\":44318,\"start\":44310},{\"end\":44332,\"start\":44325},{\"end\":44346,\"start\":44339},{\"end\":44362,\"start\":44355},{\"end\":44632,\"start\":44628},{\"end\":44646,\"start\":44639},{\"end\":44661,\"start\":44655},{\"end\":44971,\"start\":44965},{\"end\":44986,\"start\":44980},{\"end\":44999,\"start\":44993},{\"end\":45016,\"start\":45011},{\"end\":45029,\"start\":45023},{\"end\":45395,\"start\":45388},{\"end\":45405,\"start\":45400},{\"end\":45419,\"start\":45415},{\"end\":45435,\"start\":45429},{\"end\":45879,\"start\":45872},{\"end\":45891,\"start\":45885},{\"end\":45904,\"start\":45896},{\"end\":45928,\"start\":45919},{\"end\":46231,\"start\":46225},{\"end\":46244,\"start\":46237},{\"end\":46260,\"start\":46252},{\"end\":46275,\"start\":46268},{\"end\":46290,\"start\":46283},{\"end\":46301,\"start\":46296},{\"end\":46653,\"start\":46646},{\"end\":46674,\"start\":46670},{\"end\":46684,\"start\":46683},{\"end\":46686,\"start\":46685},{\"end\":46702,\"start\":46694},{\"end\":46704,\"start\":46703},{\"end\":46720,\"start\":46714},{\"end\":46735,\"start\":46729},{\"end\":47146,\"start\":47138},{\"end\":47158,\"start\":47155},{\"end\":47160,\"start\":47159},{\"end\":47175,\"start\":47170},{\"end\":47190,\"start\":47184},{\"end\":47203,\"start\":47198},{\"end\":47216,\"start\":47212},{\"end\":47233,\"start\":47226},{\"end\":47639,\"start\":47636},{\"end\":47653,\"start\":47652},{\"end\":47669,\"start\":47662},{\"end\":47686,\"start\":47682},{\"end\":47694,\"start\":47687},{\"end\":47712,\"start\":47708},{\"end\":47727,\"start\":47724},{\"end\":47749,\"start\":47741},{\"end\":48141,\"start\":48138},{\"end\":48155,\"start\":48154},{\"end\":48171,\"start\":48164},{\"end\":48192,\"start\":48184},{\"end\":48194,\"start\":48193},{\"end\":48207,\"start\":48203},{\"end\":48219,\"start\":48216},{\"end\":48641,\"start\":48635},{\"end\":48654,\"start\":48648},{\"end\":48674,\"start\":48667},{\"end\":48690,\"start\":48683},{\"end\":48698,\"start\":48697},{\"end\":48717,\"start\":48706},{\"end\":48724,\"start\":48718},{\"end\":48739,\"start\":48734},{\"end\":48741,\"start\":48740},{\"end\":48759,\"start\":48753},{\"end\":49209,\"start\":49199},{\"end\":49221,\"start\":49216},{\"end\":49238,\"start\":49232},{\"end\":49254,\"start\":49247},{\"end\":49271,\"start\":49265},{\"end\":49647,\"start\":49639},{\"end\":49661,\"start\":49654},{\"end\":49677,\"start\":49669},{\"end\":49679,\"start\":49678},{\"end\":49694,\"start\":49688},{\"end\":49707,\"start\":49704},{\"end\":49709,\"start\":49708},{\"end\":49725,\"start\":49719},{\"end\":49727,\"start\":49726},{\"end\":49742,\"start\":49735},{\"end\":50108,\"start\":50102},{\"end\":50124,\"start\":50119},{\"end\":50139,\"start\":50133},{\"end\":50159,\"start\":50151},{\"end\":50532,\"start\":50523},{\"end\":50548,\"start\":50541},{\"end\":50559,\"start\":50555},{\"end\":50573,\"start\":50566},{\"end\":50945,\"start\":50937},{\"end\":50957,\"start\":50953},{\"end\":50970,\"start\":50965},{\"end\":50986,\"start\":50980},{\"end\":51004,\"start\":50998},{\"end\":51018,\"start\":51015},{\"end\":51370,\"start\":51362},{\"end\":51400,\"start\":51389},{\"end\":51713,\"start\":51708},{\"end\":51727,\"start\":51723},{\"end\":51741,\"start\":51734},{\"end\":51759,\"start\":51752},{\"end\":52101,\"start\":52094},{\"end\":52117,\"start\":52108},{\"end\":52130,\"start\":52122},{\"end\":52144,\"start\":52137},{\"end\":52522,\"start\":52515},{\"end\":52540,\"start\":52533},{\"end\":52558,\"start\":52552},{\"end\":52942,\"start\":52935},{\"end\":52954,\"start\":52951},{\"end\":52975,\"start\":52967},{\"end\":52986,\"start\":52982},{\"end\":53002,\"start\":52996},{\"end\":53004,\"start\":53003},{\"end\":53025,\"start\":53017},{\"end\":53027,\"start\":53026},{\"end\":53039,\"start\":53036},{\"end\":53452,\"start\":53445},{\"end\":53465,\"start\":53461},{\"end\":53770,\"start\":53762},{\"end\":53785,\"start\":53777},{\"end\":53796,\"start\":53792},{\"end\":53811,\"start\":53805},{\"end\":53830,\"start\":53824},{\"end\":53845,\"start\":53837},{\"end\":53847,\"start\":53846},{\"end\":53863,\"start\":53856},{\"end\":53884,\"start\":53880},{\"end\":53900,\"start\":53894},{\"end\":54349,\"start\":54345},{\"end\":54360,\"start\":54356},{\"end\":54362,\"start\":54361},{\"end\":54371,\"start\":54370},{\"end\":54385,\"start\":54379},{\"end\":54739,\"start\":54737},{\"end\":54752,\"start\":54745},{\"end\":54766,\"start\":54758},{\"end\":54776,\"start\":54772},{\"end\":54788,\"start\":54783},{\"end\":54796,\"start\":54793},{\"end\":55158,\"start\":55155},{\"end\":55173,\"start\":55169},{\"end\":55192,\"start\":55184},{\"end\":55194,\"start\":55193},{\"end\":55210,\"start\":55203},{\"end\":55229,\"start\":55222},{\"end\":55245,\"start\":55237},{\"end\":55646,\"start\":55642},{\"end\":55658,\"start\":55651},{\"end\":55670,\"start\":55663},{\"end\":55682,\"start\":55679},{\"end\":55690,\"start\":55687},{\"end\":55701,\"start\":55695},{\"end\":56043,\"start\":56039},{\"end\":56054,\"start\":56048},{\"end\":56066,\"start\":56059},{\"end\":56081,\"start\":56075},{\"end\":56433,\"start\":56429},{\"end\":56446,\"start\":56440},{\"end\":56461,\"start\":56453},{\"end\":56478,\"start\":56471},{\"end\":56496,\"start\":56487},{\"end\":56509,\"start\":56503},{\"end\":56923,\"start\":56916},{\"end\":56938,\"start\":56931},{\"end\":56952,\"start\":56946},{\"end\":56954,\"start\":56953},{\"end\":56965,\"start\":56962},{\"end\":56983,\"start\":56977}]", "bib_author_last_name": "[{\"end\":42058,\"start\":42053},{\"end\":42073,\"start\":42066},{\"end\":42089,\"start\":42081},{\"end\":42102,\"start\":42099},{\"end\":42122,\"start\":42114},{\"end\":42137,\"start\":42130},{\"end\":42160,\"start\":42155},{\"end\":42534,\"start\":42526},{\"end\":42558,\"start\":42544},{\"end\":42573,\"start\":42566},{\"end\":42591,\"start\":42584},{\"end\":42605,\"start\":42600},{\"end\":42986,\"start\":42975},{\"end\":43001,\"start\":42993},{\"end\":43020,\"start\":43010},{\"end\":43031,\"start\":43029},{\"end\":43042,\"start\":43033},{\"end\":43449,\"start\":43445},{\"end\":43461,\"start\":43459},{\"end\":43475,\"start\":43471},{\"end\":43492,\"start\":43487},{\"end\":43505,\"start\":43500},{\"end\":43516,\"start\":43514},{\"end\":43524,\"start\":43522},{\"end\":43941,\"start\":43934},{\"end\":43956,\"start\":43950},{\"end\":43971,\"start\":43965},{\"end\":43989,\"start\":43980},{\"end\":44323,\"start\":44319},{\"end\":44337,\"start\":44333},{\"end\":44353,\"start\":44347},{\"end\":44369,\"start\":44363},{\"end\":44637,\"start\":44633},{\"end\":44653,\"start\":44647},{\"end\":44668,\"start\":44662},{\"end\":44978,\"start\":44972},{\"end\":44991,\"start\":44987},{\"end\":45009,\"start\":45000},{\"end\":45021,\"start\":45017},{\"end\":45036,\"start\":45030},{\"end\":45398,\"start\":45396},{\"end\":45413,\"start\":45406},{\"end\":45427,\"start\":45420},{\"end\":45440,\"start\":45436},{\"end\":45883,\"start\":45880},{\"end\":45894,\"start\":45892},{\"end\":45917,\"start\":45905},{\"end\":45933,\"start\":45929},{\"end\":45943,\"start\":45935},{\"end\":46235,\"start\":46232},{\"end\":46250,\"start\":46245},{\"end\":46266,\"start\":46261},{\"end\":46281,\"start\":46276},{\"end\":46294,\"start\":46291},{\"end\":46309,\"start\":46302},{\"end\":46668,\"start\":46654},{\"end\":46681,\"start\":46675},{\"end\":46692,\"start\":46687},{\"end\":46712,\"start\":46705},{\"end\":46727,\"start\":46721},{\"end\":46747,\"start\":46736},{\"end\":46758,\"start\":46749},{\"end\":47153,\"start\":47147},{\"end\":47168,\"start\":47161},{\"end\":47182,\"start\":47176},{\"end\":47196,\"start\":47191},{\"end\":47210,\"start\":47204},{\"end\":47224,\"start\":47217},{\"end\":47248,\"start\":47234},{\"end\":47650,\"start\":47640},{\"end\":47660,\"start\":47654},{\"end\":47680,\"start\":47670},{\"end\":47706,\"start\":47695},{\"end\":47722,\"start\":47713},{\"end\":47739,\"start\":47728},{\"end\":47752,\"start\":47750},{\"end\":47757,\"start\":47754},{\"end\":48152,\"start\":48142},{\"end\":48162,\"start\":48156},{\"end\":48182,\"start\":48172},{\"end\":48201,\"start\":48195},{\"end\":48214,\"start\":48208},{\"end\":48231,\"start\":48220},{\"end\":48235,\"start\":48233},{\"end\":48646,\"start\":48642},{\"end\":48665,\"start\":48655},{\"end\":48681,\"start\":48675},{\"end\":48695,\"start\":48691},{\"end\":48704,\"start\":48699},{\"end\":48732,\"start\":48725},{\"end\":48751,\"start\":48742},{\"end\":48769,\"start\":48760},{\"end\":48782,\"start\":48771},{\"end\":49214,\"start\":49210},{\"end\":49230,\"start\":49222},{\"end\":49245,\"start\":49239},{\"end\":49263,\"start\":49255},{\"end\":49281,\"start\":49272},{\"end\":49652,\"start\":49648},{\"end\":49667,\"start\":49662},{\"end\":49686,\"start\":49680},{\"end\":49702,\"start\":49695},{\"end\":49717,\"start\":49710},{\"end\":49733,\"start\":49728},{\"end\":49757,\"start\":49743},{\"end\":50117,\"start\":50109},{\"end\":50131,\"start\":50125},{\"end\":50149,\"start\":50140},{\"end\":50173,\"start\":50160},{\"end\":50539,\"start\":50533},{\"end\":50553,\"start\":50549},{\"end\":50564,\"start\":50560},{\"end\":50580,\"start\":50574},{\"end\":50951,\"start\":50946},{\"end\":50963,\"start\":50958},{\"end\":50978,\"start\":50971},{\"end\":50996,\"start\":50987},{\"end\":51013,\"start\":51005},{\"end\":51021,\"start\":51019},{\"end\":51387,\"start\":51371},{\"end\":51406,\"start\":51401},{\"end\":51721,\"start\":51714},{\"end\":51732,\"start\":51728},{\"end\":51750,\"start\":51742},{\"end\":51766,\"start\":51760},{\"end\":52106,\"start\":52102},{\"end\":52120,\"start\":52118},{\"end\":52135,\"start\":52131},{\"end\":52150,\"start\":52145},{\"end\":52531,\"start\":52523},{\"end\":52550,\"start\":52541},{\"end\":52568,\"start\":52559},{\"end\":52949,\"start\":52943},{\"end\":52965,\"start\":52955},{\"end\":52980,\"start\":52976},{\"end\":52994,\"start\":52987},{\"end\":53015,\"start\":53005},{\"end\":53034,\"start\":53028},{\"end\":53042,\"start\":53040},{\"end\":53459,\"start\":53453},{\"end\":53473,\"start\":53466},{\"end\":53775,\"start\":53771},{\"end\":53790,\"start\":53786},{\"end\":53803,\"start\":53797},{\"end\":53822,\"start\":53812},{\"end\":53835,\"start\":53831},{\"end\":53854,\"start\":53848},{\"end\":53878,\"start\":53864},{\"end\":53892,\"start\":53885},{\"end\":53911,\"start\":53901},{\"end\":54354,\"start\":54350},{\"end\":54368,\"start\":54363},{\"end\":54377,\"start\":54372},{\"end\":54392,\"start\":54386},{\"end\":54404,\"start\":54394},{\"end\":54743,\"start\":54740},{\"end\":54756,\"start\":54753},{\"end\":54770,\"start\":54767},{\"end\":54781,\"start\":54777},{\"end\":54791,\"start\":54789},{\"end\":54801,\"start\":54797},{\"end\":55167,\"start\":55159},{\"end\":55182,\"start\":55174},{\"end\":55201,\"start\":55195},{\"end\":55220,\"start\":55211},{\"end\":55235,\"start\":55230},{\"end\":55249,\"start\":55246},{\"end\":55649,\"start\":55647},{\"end\":55661,\"start\":55659},{\"end\":55677,\"start\":55671},{\"end\":55685,\"start\":55683},{\"end\":55693,\"start\":55691},{\"end\":55710,\"start\":55702},{\"end\":56046,\"start\":56044},{\"end\":56057,\"start\":56055},{\"end\":56073,\"start\":56067},{\"end\":56090,\"start\":56082},{\"end\":56438,\"start\":56434},{\"end\":56451,\"start\":56447},{\"end\":56469,\"start\":56462},{\"end\":56485,\"start\":56479},{\"end\":56501,\"start\":56497},{\"end\":56520,\"start\":56510},{\"end\":56929,\"start\":56924},{\"end\":56944,\"start\":56939},{\"end\":56960,\"start\":56955},{\"end\":56975,\"start\":56966},{\"end\":56988,\"start\":56984}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":238226969},\"end\":42481,\"start\":41973},{\"attributes\":{\"doi\":\"2022. 3\",\"id\":\"b1\",\"matched_paper_id\":233210013},\"end\":42879,\"start\":42483},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":227247980},\"end\":43355,\"start\":42881},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":232404617},\"end\":43836,\"start\":43357},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":232768311},\"end\":44308,\"start\":43838},{\"attributes\":{\"doi\":\"arXiv:1602.02481\",\"id\":\"b5\"},\"end\":44553,\"start\":44310},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":232478424},\"end\":44917,\"start\":44555},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":18412989},\"end\":45312,\"start\":44919},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":227208781},\"end\":45842,\"start\":45314},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":220686483},\"end\":46186,\"start\":45844},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":234482509},\"end\":46566,\"start\":46188},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":220968781},\"end\":47104,\"start\":46568},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":104292390},\"end\":47544,\"start\":47106},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219947110},\"end\":48064,\"start\":47546},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":213175590},\"end\":48534,\"start\":48066},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b15\",\"matched_paper_id\":234365467},\"end\":49116,\"start\":48536},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":58007025},\"end\":49602,\"start\":49118},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":227162435},\"end\":50049,\"start\":49604},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":227227965},\"end\":50447,\"start\":50051},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b19\",\"matched_paper_id\":232352619},\"end\":50849,\"start\":50449},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":152282359},\"end\":51327,\"start\":50851},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1728538},\"end\":51643,\"start\":51329},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":220364071},\"end\":52031,\"start\":51645},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":215548442},\"end\":52423,\"start\":52033},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":174798113},\"end\":52853,\"start\":52425},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":227254925},\"end\":53392,\"start\":52855},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":216080881},\"end\":53709,\"start\":53394},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":232045969},\"end\":54269,\"start\":53711},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":207761262},\"end\":54647,\"start\":54271},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":237386110},\"end\":55092,\"start\":54649},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":228083990},\"end\":55577,\"start\":55094},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b31\",\"matched_paper_id\":232352425},\"end\":55979,\"start\":55579},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":227254854},\"end\":56362,\"start\":55981},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":11446141},\"end\":56842,\"start\":56364},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4766599},\"end\":57286,\"start\":56844}]", "bib_title": "[{\"end\":42042,\"start\":41973},{\"end\":42518,\"start\":42483},{\"end\":42967,\"start\":42881},{\"end\":43437,\"start\":43357},{\"end\":43925,\"start\":43838},{\"end\":44626,\"start\":44555},{\"end\":44963,\"start\":44919},{\"end\":45386,\"start\":45314},{\"end\":45870,\"start\":45844},{\"end\":46223,\"start\":46188},{\"end\":46644,\"start\":46568},{\"end\":47136,\"start\":47106},{\"end\":47634,\"start\":47546},{\"end\":48136,\"start\":48066},{\"end\":48633,\"start\":48536},{\"end\":49197,\"start\":49118},{\"end\":49637,\"start\":49604},{\"end\":50100,\"start\":50051},{\"end\":50521,\"start\":50449},{\"end\":50935,\"start\":50851},{\"end\":51360,\"start\":51329},{\"end\":51706,\"start\":51645},{\"end\":52092,\"start\":52033},{\"end\":52513,\"start\":52425},{\"end\":52933,\"start\":52855},{\"end\":53443,\"start\":53394},{\"end\":53760,\"start\":53711},{\"end\":54343,\"start\":54271},{\"end\":54735,\"start\":54649},{\"end\":55153,\"start\":55094},{\"end\":55640,\"start\":55579},{\"end\":56037,\"start\":55981},{\"end\":56427,\"start\":56364},{\"end\":56914,\"start\":56844}]", "bib_author": "[{\"end\":42060,\"start\":42044},{\"end\":42075,\"start\":42060},{\"end\":42091,\"start\":42075},{\"end\":42104,\"start\":42091},{\"end\":42124,\"start\":42104},{\"end\":42139,\"start\":42124},{\"end\":42162,\"start\":42139},{\"end\":42536,\"start\":42520},{\"end\":42560,\"start\":42536},{\"end\":42575,\"start\":42560},{\"end\":42593,\"start\":42575},{\"end\":42607,\"start\":42593},{\"end\":42988,\"start\":42969},{\"end\":43003,\"start\":42988},{\"end\":43022,\"start\":43003},{\"end\":43033,\"start\":43022},{\"end\":43044,\"start\":43033},{\"end\":43451,\"start\":43439},{\"end\":43463,\"start\":43451},{\"end\":43477,\"start\":43463},{\"end\":43494,\"start\":43477},{\"end\":43507,\"start\":43494},{\"end\":43518,\"start\":43507},{\"end\":43526,\"start\":43518},{\"end\":43943,\"start\":43927},{\"end\":43958,\"start\":43943},{\"end\":43973,\"start\":43958},{\"end\":43991,\"start\":43973},{\"end\":44325,\"start\":44310},{\"end\":44339,\"start\":44325},{\"end\":44355,\"start\":44339},{\"end\":44371,\"start\":44355},{\"end\":44639,\"start\":44628},{\"end\":44655,\"start\":44639},{\"end\":44670,\"start\":44655},{\"end\":44980,\"start\":44965},{\"end\":44993,\"start\":44980},{\"end\":45011,\"start\":44993},{\"end\":45023,\"start\":45011},{\"end\":45038,\"start\":45023},{\"end\":45400,\"start\":45388},{\"end\":45415,\"start\":45400},{\"end\":45429,\"start\":45415},{\"end\":45442,\"start\":45429},{\"end\":45885,\"start\":45872},{\"end\":45896,\"start\":45885},{\"end\":45919,\"start\":45896},{\"end\":45935,\"start\":45919},{\"end\":45945,\"start\":45935},{\"end\":46237,\"start\":46225},{\"end\":46252,\"start\":46237},{\"end\":46268,\"start\":46252},{\"end\":46283,\"start\":46268},{\"end\":46296,\"start\":46283},{\"end\":46311,\"start\":46296},{\"end\":46670,\"start\":46646},{\"end\":46683,\"start\":46670},{\"end\":46694,\"start\":46683},{\"end\":46714,\"start\":46694},{\"end\":46729,\"start\":46714},{\"end\":46749,\"start\":46729},{\"end\":46760,\"start\":46749},{\"end\":47155,\"start\":47138},{\"end\":47170,\"start\":47155},{\"end\":47184,\"start\":47170},{\"end\":47198,\"start\":47184},{\"end\":47212,\"start\":47198},{\"end\":47226,\"start\":47212},{\"end\":47250,\"start\":47226},{\"end\":47652,\"start\":47636},{\"end\":47662,\"start\":47652},{\"end\":47682,\"start\":47662},{\"end\":47708,\"start\":47682},{\"end\":47724,\"start\":47708},{\"end\":47741,\"start\":47724},{\"end\":47754,\"start\":47741},{\"end\":47759,\"start\":47754},{\"end\":48154,\"start\":48138},{\"end\":48164,\"start\":48154},{\"end\":48184,\"start\":48164},{\"end\":48203,\"start\":48184},{\"end\":48216,\"start\":48203},{\"end\":48233,\"start\":48216},{\"end\":48237,\"start\":48233},{\"end\":48648,\"start\":48635},{\"end\":48667,\"start\":48648},{\"end\":48683,\"start\":48667},{\"end\":48697,\"start\":48683},{\"end\":48706,\"start\":48697},{\"end\":48734,\"start\":48706},{\"end\":48753,\"start\":48734},{\"end\":48771,\"start\":48753},{\"end\":48784,\"start\":48771},{\"end\":49216,\"start\":49199},{\"end\":49232,\"start\":49216},{\"end\":49247,\"start\":49232},{\"end\":49265,\"start\":49247},{\"end\":49283,\"start\":49265},{\"end\":49654,\"start\":49639},{\"end\":49669,\"start\":49654},{\"end\":49688,\"start\":49669},{\"end\":49704,\"start\":49688},{\"end\":49719,\"start\":49704},{\"end\":49735,\"start\":49719},{\"end\":49759,\"start\":49735},{\"end\":50119,\"start\":50102},{\"end\":50133,\"start\":50119},{\"end\":50151,\"start\":50133},{\"end\":50175,\"start\":50151},{\"end\":50541,\"start\":50523},{\"end\":50555,\"start\":50541},{\"end\":50566,\"start\":50555},{\"end\":50582,\"start\":50566},{\"end\":50953,\"start\":50937},{\"end\":50965,\"start\":50953},{\"end\":50980,\"start\":50965},{\"end\":50998,\"start\":50980},{\"end\":51015,\"start\":50998},{\"end\":51023,\"start\":51015},{\"end\":51389,\"start\":51362},{\"end\":51408,\"start\":51389},{\"end\":51723,\"start\":51708},{\"end\":51734,\"start\":51723},{\"end\":51752,\"start\":51734},{\"end\":51768,\"start\":51752},{\"end\":52108,\"start\":52094},{\"end\":52122,\"start\":52108},{\"end\":52137,\"start\":52122},{\"end\":52152,\"start\":52137},{\"end\":52533,\"start\":52515},{\"end\":52552,\"start\":52533},{\"end\":52570,\"start\":52552},{\"end\":52951,\"start\":52935},{\"end\":52967,\"start\":52951},{\"end\":52982,\"start\":52967},{\"end\":52996,\"start\":52982},{\"end\":53017,\"start\":52996},{\"end\":53036,\"start\":53017},{\"end\":53044,\"start\":53036},{\"end\":53461,\"start\":53445},{\"end\":53475,\"start\":53461},{\"end\":53777,\"start\":53762},{\"end\":53792,\"start\":53777},{\"end\":53805,\"start\":53792},{\"end\":53824,\"start\":53805},{\"end\":53837,\"start\":53824},{\"end\":53856,\"start\":53837},{\"end\":53880,\"start\":53856},{\"end\":53894,\"start\":53880},{\"end\":53913,\"start\":53894},{\"end\":54356,\"start\":54345},{\"end\":54370,\"start\":54356},{\"end\":54379,\"start\":54370},{\"end\":54394,\"start\":54379},{\"end\":54406,\"start\":54394},{\"end\":54745,\"start\":54737},{\"end\":54758,\"start\":54745},{\"end\":54772,\"start\":54758},{\"end\":54783,\"start\":54772},{\"end\":54793,\"start\":54783},{\"end\":54803,\"start\":54793},{\"end\":55169,\"start\":55155},{\"end\":55184,\"start\":55169},{\"end\":55203,\"start\":55184},{\"end\":55222,\"start\":55203},{\"end\":55237,\"start\":55222},{\"end\":55251,\"start\":55237},{\"end\":55651,\"start\":55642},{\"end\":55663,\"start\":55651},{\"end\":55679,\"start\":55663},{\"end\":55687,\"start\":55679},{\"end\":55695,\"start\":55687},{\"end\":55712,\"start\":55695},{\"end\":56048,\"start\":56039},{\"end\":56059,\"start\":56048},{\"end\":56075,\"start\":56059},{\"end\":56092,\"start\":56075},{\"end\":56440,\"start\":56429},{\"end\":56453,\"start\":56440},{\"end\":56471,\"start\":56453},{\"end\":56487,\"start\":56471},{\"end\":56503,\"start\":56487},{\"end\":56522,\"start\":56503},{\"end\":56931,\"start\":56916},{\"end\":56946,\"start\":56931},{\"end\":56962,\"start\":56946},{\"end\":56977,\"start\":56962},{\"end\":56990,\"start\":56977}]", "bib_venue": "[{\"end\":42221,\"start\":42162},{\"end\":42672,\"start\":42614},{\"end\":43109,\"start\":43044},{\"end\":43587,\"start\":43526},{\"end\":44056,\"start\":43991},{\"end\":44418,\"start\":44387},{\"end\":44725,\"start\":44670},{\"end\":45103,\"start\":45038},{\"end\":45530,\"start\":45442},{\"end\":46004,\"start\":45945},{\"end\":46366,\"start\":46311},{\"end\":46825,\"start\":46760},{\"end\":47315,\"start\":47250},{\"end\":47787,\"start\":47759},{\"end\":48282,\"start\":48237},{\"end\":48814,\"start\":48791},{\"end\":49348,\"start\":49283},{\"end\":49814,\"start\":49759},{\"end\":50240,\"start\":50175},{\"end\":50643,\"start\":50589},{\"end\":51078,\"start\":51023},{\"end\":51473,\"start\":51408},{\"end\":51827,\"start\":51768},{\"end\":52217,\"start\":52152},{\"end\":52629,\"start\":52570},{\"end\":53109,\"start\":53044},{\"end\":53540,\"start\":53475},{\"end\":53978,\"start\":53913},{\"end\":54443,\"start\":54406},{\"end\":54858,\"start\":54803},{\"end\":55325,\"start\":55251},{\"end\":55773,\"start\":55719},{\"end\":56157,\"start\":56092},{\"end\":56586,\"start\":56522},{\"end\":57055,\"start\":56990},{\"end\":45605,\"start\":45532}]"}}}, "year": 2023, "month": 12, "day": 17}