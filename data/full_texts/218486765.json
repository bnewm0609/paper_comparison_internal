{"id": 218486765, "updated": "2023-10-06 15:46:49.183", "metadata": {"title": "Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset", "authors": "[{\"first\":\"Xiang\",\"last\":\"Yue\",\"middle\":[]},{\"first\":\"Bernal\",\"last\":\"Gutierrez\",\"middle\":[\"Jimenez\"]},{\"first\":\"Huan\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics", "publication_date": {"year": 2020, "month": 5, "day": 1}, "abstract": "Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. Recently, Pampari et al. (EMNLP'18) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes. In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC) task. From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge. From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert's performance, and (v) BERT models do not beat the best performing base model. Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts. We argue that both should be considered when creating future datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.00574", "mag": "3035129496", "acl": "2020.acl-main.410", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/YueGS20", "doi": "10.18653/v1/2020.acl-main.410"}}, "content": {"source": {"pdf_hash": "f0280fbefb60858757c4a7a348cbc426c80e9f18", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2020.acl-main.410.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.acl-main.410.pdf", "status": "HYBRID"}}, "grobid": {"id": "a415919dd4f55623ac6d4a1a4a39f84e3236e061", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f0280fbefb60858757c4a7a348cbc426c80e9f18.txt", "contents": "\nClinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsJuly 5 -10, 2020. 2020\n\nXiang Yue \nThe Ohio State University\nHuan Sun\n\n\nBernal Jimenez Gutierrez jimenezgutierrez.1@osu.edu \nThe Ohio State University\nHuan Sun\n\n\nClinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset\n\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\nthe 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 5 -10, 2020. 20204474\nMachine reading comprehension has made great progress in recent years owing to largescale annotated datasets. In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. Recently, Pampari et al. (2018)   tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes. In this paper, we provide an indepth analysis of this dataset and the clinical reading comprehension (CliniRC) task. From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge. From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert's performance, and (v) BERT models do not beat the best performing base model. Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts. We argue that both should be considered when creating future datasets. 1\n\nIntroduction\n\nMedical professionals often query over clinical notes in Electronic Medical Records (EMRs) to find information that can support their decision making (Demner-Fushman et al., 2009;Rosenbloom et al., 2011;Wang et al., 2018). One way to facilitate such information seeking activities is to build a natural language question answering (QA) system that can extract precise answers from clinical notes (Cairns et al., 2011;Cao et al., 2011;Wren, 2011;Demner-Fushman, 2016, 2019). 1 Our code is available at https://github.com/ xiangyue9607/CliniRC. Context: ... For HTN control, pt was given HCTZ and lopressor which sufficiently controlled his BP. Pt was sent home on HCTZ 25mg daily and atenolol 50mg daily. ... ADDITIONAL COMMENTS: 1.) Take hydrochlorothiazide 25mg daily and atenolol 50mg daily for your blood pressure. You should also take aspirin 81mg daily.\n\nQuestion: What was the dosage prescribed of hydrochlorothiazide? Answer: ADDITIONAL COMMENTS: 1.) Take hydrochlorothiazide 25mg daily and atenolol 50mg daily for your RECORD #992321, Date: 2145-09-22 Question: Why has the patient been prescribed hctz? Answer: For HTN control, pt was given HCTZ and lopressor which sufficiently Machine reading comprehension (RC) aims to automatically answer questions based on a given document or text corpus and has drawn wide attention in recent years. Many neural models (Cheng et al., 2016;Wang and Jiang, 2017;Seo et al., 2017;Chen et al., 2017;Devlin et al., 2019) have achieved very promising results on this task, owing to large-scale QA datasets (Hermann et al., 2015;Rajpurkar et al., 2016;Trischler et al., 2017;Joshi et al., 2017;Yang et al., 2018). Unfortunately, clinical reading comprehension (CliniRC) has not observed as much progress due to the lack of such QA datasets.\n\nIn order to create QA pairs on clinical texts, annotators must have considerable medical expertise and data handling must be specifically designed to address ethical issues and privacy concerns. Due to these requirements, using crowdsourcing like in the open domain to create large-scale clinical QA datasets becomes highly impractical .\n\nRecently, Pampari et al. (2018) found a smart way to tackle this issue and created emrQA, the first large-scale QA dataset on clinical texts. Instead of relying on crowdsourcing, emrQA was semiautomatically generated based on annotated question templates and existing annotations from the n2c2 (previously called i2b2) challenge datasets 2 . Example QA pairs from the dataset are shown in Figure 1.\n\nIn this paper, we aim to gain a deep understanding of the CliniRC task and conduct a thorough analysis of the emrQA dataset. We first explore the dataset directly by carrying out a meticulous qualitative analysis on randomly-sampled QA pairs and we find that: 1) Many answers in the emrQA dataset are incomplete and hence are hard to read and ineffective for training ( \u00a73.1). 2) Many questions are simple: More than 96% of the examples contain the same key phrases in both questions and answers. Though Pampari et al. (2018) claims that 39% of the questions may need knowledge to answer, our error analysis suggests only a very small portion of the errors (2%) made by a state-of-theart reader might be due to missing external domain knowledge ( \u00a73.2).\n\nFollowing our qualitative analysis of the emrQA dataset, we conduct a comprehensive quantitative analysis based on state-of-the-art readers and BERT models (BERT-base (Devlin et al., 2019) as well as its biomedical and clinical versions: BioBERT  and ClinicalBERT (Alsentzer et al., 2019)) to understand how different systems behave on the emrQA dataset. Surprising results include: 1) Using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, suggesting that many examples in the dataset are redundant ( \u00a74.1).\n\n2) The performance of the best base model is close to the human expert's performance 3 ( \u00a74.2). 3) The performance of BERT models is around 1%-5% worse than the best performing base model ( \u00a74.3).\n\nAfter completing our analysis of the dataset, we explore two potential needs for systems doing CliniRC: 1) The need to represent and use clinical domain knowledge effectively ( \u00a75.1) and 2) the need to generalize to unseen questions and contexts ( \u00a75.2). To investigate the first one, we analyze sev-2 https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/ 3 Which is obtained by comparing emrQA answers to answers created by our medical experts on sampled QA pairs. In summary, given our analysis of the emrQA dataset and the task in general, we conclude that future work still needs to create better datasets to advance CliniRC. Such datasets should be not only large-scale, but also less noisy, more diverse, and allow researchers to directly evaluate a system's ability to encode domain knowledge and to generalize to new questions and contexts.\n\n\nOverview of the emrQA dataset\n\nSimilar to the open-domain reading comprehension task, the Clinical Reading Comprehension (CliniRC) task is defined as follows:\n\nDefinition 2.1. Given a patient's clinical note (context) C = {c 1 , ..., c n } and a question Q = {t 1 , ..., t m }, the CliniRC task aims to extract a continuous span A = {c i , c i+1 , ..., c i+k }(1 \u2264 i \u2264 i + k \u2264 n) from the context as the answer, where c i , t j are tokens.\n\nThe emrQA dataset (Pampari et al., 2018) was semi-automatically generated from expertannotated question templates and existing i2b2 annotations. More specifically, clinical question templates were first created by human experts. Then, manual annotations from the medication information extraction, relation learning, and coreference Has the patient ever been on | medication | ? Question Template <Medication = \"Flagyl\", Line Index = 128>\n\n\nExisting i2b2 Annotation\n\nHas the patient ever been on Flagyl ?\n\n\nGenerated Question\n\nFlagyl. By discharge, the patient was afebrile (line 128) Generated Answer Figure 2: An example to illustrate how emrQA generates QA pairs. resolution i2b2 challenges were re-framed into answers for the question templates. After linking question templates to i2b2 annotations, the gold annotation entities were used to both replace placeholders in the question templates and extract the sentence around them as answers. An example of this generation process can be seen in Figure 2.\n\nThe emrQA dataset contains 5 subsets: Medication, Relation, Heart Disease, Obesity and Smoking, which were generated from 5 i2b2 challenge datasets respectively. The answer format in each dataset is different. For the Obesity and Smoking datasets, answers are categorized into 7 classes and the task is to predict the question's class based on the context. For the Medication, Relation, and Heart Disease datasets, answers are usually short snippets from the text accompanied by a longer span around it which we refer to as an evidence. The short snippet is a single entity or multiple entities while the evidence contains the entire line around those entities in the clinical note. For questions that cannot be answered via entities, only the evidence is provided as an answer. Given that some questions do not have short answers and that entire evidence spans are usually important for supporting clinical decision making (Demner-Fushman et al., 2009), we treat the answer evidence 4 as our answer just as is done in (Pampari et al., 2018).\n\nIn this work, we mainly focus on the Medication and Relation datasets because (1) they make up 80% of the entire emrQA dataset and (2) their format is consistent with the span extraction task, which is more challenging and meaningful for clinical decision making support. We filter the answers whose lengths (number of tokens) are more than 20. The detailed statistics of the two datasets are shown in Table 1. 4 For simplicity, we use \"answer\" directly henceforth.\n\n\nMetric\n\nMedication Relation  \n\n\nIn-depth Qualitative Analysis\n\nIn this section, we carry out an in-depth analysis of the emrQA dataset. We aim to examine (1) the quality and (2) level of difficulty for the generated QA pairs in the emrQA dataset.\n\n\nHow clean are the emrQA answers?\n\nSince the emrQA dataset was created via a generation framework unlike human-labeled or crowdsourcing datasets, the quality of the datasets remains largely unknown. In order to use this dataset to explore the CliniRC task, it is essential to determine whether it is meaningful. In order to do this, we randomly sample 50 QA pairs from the Medication and the Relation datasets respectively. Since some questions share the same answer due to automatic generation, we make sure all the samples have different answers.\n\nSince the questions were generated from expert created templates, most of them are humanreadable and unambiguous. We therefore mainly focus on evaluating answer quality. We ask two human experts to score each answer from 1 to 5 depending on the relevance of the answer to the question (1: irrelevant or incorrect; 2: missing key parts; 3: contains key parts but is not humanreadable or contains many irrelevant parts; 4: contains key parts and is only missing a few parts or has a few irrelevant extra segments; 5: perfect answer). We also ask human annotators to label the gold answers and then calculate the Exact Match (EM) and F1 score (F1) of the emrQA answers v.s. human gold answers. The answer quality score, EM and F1 in both datasets, are shown in Table 2.\n\nThe scores of the Medication dataset are low since most of the answers are broken sentences or contain unnecessary segments. For instance, in the Figure 2 example, the correct answer should be \"Clindamycin was changed to Flagyl\", how-  ever, the emrQA answer misses important parts \"Clindamycin was changed to\" and contains irrelevant parts \"By discharge, the patient was afebrile\". These issues are common in the Medication dataset and make it difficult to train a good system. To understand why the generated answers contain such noise, we explored the \"i2b2 2009 Medication\" challenge dataset which was used to create these QA pairs. We found that most documents in this dataset contain many complete sentences split into separate lines. Since the i2b2 annotation are token based and the emrQA obtains full lines around the token as evidence spans, these lines often end up being broken sentences. We tried to relabel the answers with existing sentence segmentation tools and heuristic measures but found that it is very challenging to obtain concise and complete text spans as answers. Compared with the Medication dataset, the answer quality of the Relation dataset is much better. In most cases, the answers are complete and meaningful sentences with no unnecessary parts.\n\n\nHow challenging are the emrQA pairs?\n\nAnother observation from the 50 samples is that 96% of the answers in the Medication dataset and 100% of the answers in the Relation dataset contain the key phrase in the question. This is due to the generation procedure illustrated in Figure 2. In this example, the key phrase or entity (\"Flagyl\") in the question is also included in the answer. This undoubtedly makes the answer easier to extract as long as the model can recognize significant words and do \"word matching\".\n\nTo further explore how much clinical language understanding is needed and what kind of errors do the state-of-the-art reader make, we conduct error analysis using DocReader (Chen et al., 2017) (also used in (Pampari et al., 2018)) on the emrQA dataset. More specifically, we randomly sample 50 questions that are answered incorrectly by the model (based on exact match metric) from the Medication and Relation dev set respectively 5 . The results are shown in Table 3 (examples for each error type are also given for better understanding).\n\nSince emrQA answers are often incomplete in the dataset, we deem span mismatch errors acceptable as long as the predictions include the key part of the ground truths. Surprisingly, span mismatchinclude key info errors, along with ambiguous questions, incorrect golds and false negatives (the prediction is correct but it is not in the emrQA answers) errors, which are caused by the dataset itself, account for 90% of total errors, suggesting that the accuracy of these models is even higher than we report.\n\nAnother interesting finding from the error analysis is that to our surprise, only a very small amount (2%) of errors may have been caused by a lack of external domain knowledge while Pampari et al. (2018) claim that 39% of the questions in the em-rQA dataset need domain knowledge. This surprising result might be due to: (1) neural models being able to encode relational or associative knowledge from the text corpora as has also been reported in recent studies (Petroni et al., 2019;Bouraoui et al., 2020), and (2) questions and answers sharing key phrases (as we mentioned earlier in \u00a73.1) in many samples, making it more likely that fewer questions need external knowledge to be answered than previously reported.\n\n\nComprehensive Quantitative Analysis\n\nIn this section, we conduct comprehensive experiments on the emrQA dataset with state-of-theart readers and recently dominating BERT models. Full experimental settings are described in Appendix A.\n\n\nHow redundant are the emrQA pairs?\n\nThough there are more than 1 million questions in the emrQA dataset (as shown in Table 1), many questions and their patterns are very similar since they are generated from the same question templates. This observation leads to a natural question: do we really need so many questions to train an CliniRC system? If many questions are similar to each other, it is very likely that using a sampled subset can achieve roughly the same performance that is based on the entire dataset.\n\nTo verify our hypothesis, we first split the two datasets into train, dev, and test set with the proportion of 7:1:2 w.r.t. the contexts (full statistics are shown in Appendix Table A1). Then we randomly sample {5%, 10%, 20%, 40%, 60%} and {1%, 3%, 5%, 10%, 15%} 6 of the QA pairs in each document (context) of the Medication and the Relation training sets respectively. We run DocReader (Chen et al., 2017) on the sampled subsets and evaluate them on the same dev and test set.\n\nAs shown in Figure 3, using 20% of the questions in the Medication and 5% of the questions in the Relation dataset can achieve roughly the same performance as using the entire training sets. 6 The sampling percentage of the Relation dataset is smaller than the Medication dataset since the former one has more QA pairs (roughly 4 times). These verify our hypothesis, and illustrate learning a good and robust reader system based on the emrQA dataset does not need so many questionanswer pairs. While deep models are often datahungry, it does not mean more data can always lead to better performance. In addition to the training size, diversity should also be considered as another important criterion for data quality.\n\nIn the following experiments, we use the sampled subsets (20% for Medication and 5% for Relation) considering the time and memory cost as well as performance.\n\n\nLittle room for improvement\n\nSince the answers in emrQA are often incomplete, the performance of a model is more appropriately reflected by its F1 score. As shown in Table 2, we obtain F1 scores of 74% and 95% on two datasets respectively when we test human-labeled answers against the emrQA answers on a sampled dataset. We can see from Table 4 that the best performing reader, DocReader, achieves around 70% and 94% F1 performance on the Medication and Relation test set respectively, which are very close to the human performance just described. Though designing more complex and advanced models may achieve better scores, such scores are obtained w.r.t. noisy emrQA answers and may not translate meaningfully to real cases.\n\n\nBERT does not always win\n\nBERT models have achieved very promising results recently in various NLP tasks including RC (Devlin et al., 2019). We follow their experiment setting of BERT for doing reading comprehension on the SQuAD (Rajpurkar et al., 2016) dataset. To our surprise, as shown in Table 4, BERT models (BERT-base, its biomedical version BioBERT , and its clinical version ClinicalBERT   (Zhu et al., 2015) and PubMed articles respectively, both of which may have different vocabularies and use different language expressions from clinical texts. Though ClinicalBERT was pretrained on MIMIC-III (Johnson et al., 2016) clinical texts, the training size of the corpus (\u223c50M words) is far less than that used in BERT (\u223c3300M words), which may make the model less powerful as it is on the open-domain tasks. 2) Longer Contexts. As can be seen from Table 1, the number of tokens in the contexts is commonly larger than open-domain RC datasets like SQuAD (\u223c1000 v.s.\u223c116 avg). We suspect that long contexts might make it more challenging to model sequential information. For sequences that are longer than the max length of the BERT model, they are truncated into a set of short sequences, which may hinder the model from capturing long dependencies (Dai et al., 2019) and global information in the entire document. 3) Easy Questions. Another possible reason might be the question patterns are too easy and a simpler reader with far less parameters can learn the patterns and obtain satisfying performance.\n\nAdditionally, to further evaluate the models in the fine-grained level, inspired by (Gururangan et al., 2018), we partition the Medication and Relation test sets into Easy and Hard subsets using a base model. The details of Easy/Hard splits can be found in Appendix C. As can be seen from Table  A4, most of the questions in the two datasets are easy, which indicates the emrQA dataset might not be challenging for the current QA models. More difficult datasets are needed to advance the Clinical Reading Comprehension task.\n\n\nDesiderata in Real-World CliniRC\n\nFollowing our analysis of the emrQA dataset, we further study two aspects of clinical reading comprehension systems that we believe are crucial for their real-world applicability: the need to encode clinical domain knowledge and to generalize to unseen questions and documents.\n\n\nExternal domain knowledge is needed\n\nSo far, we have shown that domain knowledge may not be very useful for models answering questions in the emrQA dataset; however, we argue that systems in real-world CliniRC need to be able to encode and use clinical domain knowledge effectively.\n\nClinical text often contains high variability in many domain-specific words due to abbreviations and synonyms. The presence of different aliases in the question and context can make it difficult for a model to represent semantics accurately and choose the correct span. Besides, medical domainspecific relations (e.g., treats, caused by) and hierarchical relations (e.g., isa) between medical concepts would be likely to appear. The process followed to generate the current emrQA dataset leads to these problems being largely under-represented, even though they can be very common in real cases. We use the following 3 examples as representatives to illustrate the real cases we may encounter. Synonym. For example, for the question in Figure 2, \"Has this patient ever been on Flagyl?\", it is easy for the model to answer since \"Flagyl\" appears in the context. However, if we change \"Flagyl\" to its synonyms \"Metronidazole\" (which may not appear in training) in the question, it is hard for the reader to extract the correct answer, as it is not possible for model to capture the semantic meaning of \"Metronidazole\" as \"Flagyl\". Clinical Relations. Another example is the ques-tion shown in Figure 1, \"Why has the patient been prescribed hctz?\". Currently, machines can easily find the answer since keyword \"hctz\" is mentioned in the answer. However, given a situation where the drug \"hctz\" does not appear in the local context of \"HTN\", our model may have a better chance to extract the correct answers if it stores the relation \"(hctz, treats, HTN)\". Hierarchical Relation. For the question \"Is there a history of mental illness?\", it is more likely that the medical report describes a specific type of psychological condition rather than mention the general phrase \"mental illness\" since clinical support require specifics. To obtain the correct answer in this case \"Depression with previous suicidal ideation.\", encoding the relation \"(depression, isa, mental illness)\" would probably help the model make a correct prediction.\n\nThese three cases help illustrate how complex medical relations affect the real CliniRC task. Without leveraging external domain knowledge, it is difficult for models to capture the semantic relations necessary to resolve such cases.\n\nIn order to verify our claim quantitatively, we select synonym as a representative relation type and manipulate each question by replacing its entities with plausible synonyms or abbreviations. We then introduce external domain knowledge into current models and compare their performance against base models on these augmented questions.\n\nMore specifically, we first detect entities in the questions and link them to a medical knowledge base (KB): UMLS (Bodenreider, 2004) using a biomedical and clinical text NLP pipeline tool, ScispaCy (Neumann et al., 2019). Synonyms of detected entities are then retrieved from UMLS and used to replace the original mention. We filter the questions that do not contain entities or that contain entities with no synonyms. We focus on the Relation dataset and only modify the questions in the dev and test set; the questions in the training set are not modified. Finally, we get 69,912 and 125,338 questions in the dev and test set.\n\nWe then introduce a simple Knowledge Incorporation Module (KIM) to evaluate the usefulness of external domain knowledge. Formally, given a question q : {w q 1 , w q 2 , ..., w q l } and its context c : {w c 1 , w c 2 , ..., w c m }, where w q i , w c j are words (tokens), all the words can be mapped to d 1 dimensional vectors via a word embedding matrix E w \u2208 R d 1 \u00d7|V| , where V denotes the word vocab- ulary. So we have q : w q 1 , ..., w q l \u2208 R d 1 and c : w c 1 , ..., w c m \u2208 R d 1 . We then detect entities {e q 1 , e q 2 , ..., e q n } in the question and entities {e c 1 , e c 2 , ..., e q o } in the context and map them to a medical knowledge base (KB), UMLS (Bodenreider, 2004) using scispacy (Neumann et al., 2019). Note that l is not equal to n and m is not equal to o, since not every token can be mapped to a entity in KB. For entities that contain multiple words, we align them to the first token, same as the alignment used in (Zhang et al., 2019). We then map detected entities to d 2 dimensional vectors {e q 1 , e q 2 , ..., e q n } and {e c 1 , e c 2 , ..., e c o } via a entity embedding matrix E e \u2208 R d 2 \u00d7|U | , which is pretrained on the entire UMLS KB using the knowledge embedding method TransE (Bordes et al., 2013). U denotes the entity vocabulary.\n\nWe merge the word embeddings with entity embeddings to feed them into a Multi-layer Perceptron (MLP):\nh q i = \u03c3(W c w q i + W e e q i + b) h c j = \u03c3(W c w c j + W e e c j + b)(1)\nwhere \u03c3 is activation function, W c , W e , b are trainable parameters and h q i , h c j denote the integrated embeddings that contain information from both the word c j and the entity e j in the question and context respectively. For the word that is not mapped to an entity, e j will be set to 0. The merged embeddings are used as the input to the base reader.\n\nAs shown in Figure 4, by adding a basic Knowledge Incorporation Module to the base model, we obtain around 5% increase of F1 score on the manipulated questions in the test set. This suggests that for questions that involve relations between medical concepts, external domain knowledge may be quite important.  Table 5: Results of models when tested on new questions and unseen clinical notes (not in emrQA, but from MIMIC-III dataset). Performance drops around 40% compared with previously reported on the Relation test set, highlighting generalizability as an essential future direction for CliniRC.\n\n\nGeneralizing to unseen questions and documents\n\nThe aim of CliniRC is to build robust QA systems for doctors to retrieve information buried in clinical texts. When deploying a CliniRC system to a new environment (e.g., a new set of clinical records, a new hospital, etc.), it is infeasible to create new QA pairs for training every time. Thus, an ideal CliniRC system is able to generalize to unseen documents and questions after being fully trained.\n\nTo test the generalizability of models trained on emrQA (we focus on the Relation dataset here), our medical experts created 50 new questions that were not present in the emrQA dataset and extracted answers from unseen patient notes in the MIMIC-III (Johnson et al., 2016) dataset. This dataset consists of three types of questions: 12 questions were made from emrQA question templates but contain entities which do not appear in the training set (e.g., \"How was the diagnosis of acute cholecystitis made?\" was created from the template \"How was the diagnosis of |problem| made?\"). The other 38 questions have different forms from existing question templates: 21 paraphrase existing questions from emrQA (e.g., \"Was an edema found in the physical exam?\") was paraphrased from \"Does he have any evidence of |problem| in |test|?\") and 17 are completely semantically different from the ones in the emrQA dataset (e.g., \"What chemotherapy drugs are being administered to the patient?\").\n\nAs could be expected, we see in Table 5 that the more the new questions deviate from the original emrQA, the more the models struggle to answer them. We observe a performance drop of roughly 20% compared to the Relation test set on questions made from emrQA templates using MIMIC III clinical notes which were not in the original dataset. For question that are more significantly different, we notice an approximate 40% and 60% loss in F1 score when predicting paraphrased questions and entirely new questions respectively. This steep drop in performance for these new settings, espe-cially for paraphrased and new questions, shows how much work there is to be done on this front and highlights generalizability as an important future direction in CliniRC. We also notice that Clinical-BERT works slightly better than the base model DocReader. The reason might be ClinicalBERT was pretrained on the MIMIC-III dataset, which might help the model have a better understanding of the context.\n\nSummary. Based on these two aspects and our previous thorough analysis of the emrQA dataset, it is clear that better datasets are needed to advance CliniRC. Such datasets should be not only largescale, but also less noisy, more diverse, and moreover allow researchers to systematically evaluate a model's ability to encode domain knowledge and to generalize to new questions and contexts.\n\n\nRelated Work\n\nWe present a brief overview of open-domain, biomedical and clinical question answering tasks, which are most related to our work:\n\nQuestion Answering (QA) aims to automatically answer questions asked by humans based on external sources, such as Web (Sun et al., 2016), knowledge base Sun et al., 2015) and free text (Chen et al., 2016). As an important type of QA, reading comprehension intends to answer a question after reading the passage (Hirschman et al., 1999). Recently, the release of large-scale RC datasets, such as CNN & Daily Mail (Hermann et al., 2015), Stanford Question-Answering Dataset (SQuAD) (Rajpurkar et al., 2016(Rajpurkar et al., , 2018 makes it possible to solve RC tasks by building deep neural models (Hermann et al., 2015;Wang and Jiang, 2017;Seo et al., 2017;Chen et al., 2017).\n\nMore recently, contextualized word representations and pretrained language models, such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), have been demonstrated to be very useful in various NLP tasks including RC. By seeing diverse contexts in large corpora, these pretrained language models can capture the rich semantic meaning and produce more accurate and precise representations for words given different contexts. Even a simple classifier or score function built upon these pretrained contextualized word representations perform well in extracting answer spans (Devlin et al., 2019). Biomedical and Clinical QA. Due to the lack of large-scale annotated biomedical or clinical data, QA and RC systems in these domains are often rule-based and heuristic feature-based (Lee et al., 2006;Niu et al., 2006;Athenikos and Han, 2010).\n\nIn recent years, BioASQ challenges (Tsatsaronis et al., 2012) proposed the Biomedical Semantic QA task, where the participants need to respond to each test question with relevant articles, snippets and exact answers.\u0160uster and Daelemans (2018) use summary points of clinical case reports to build a large-scale cloze-style dataset (CliCR), which is similar to the style of CNN & Daily Mail dataset. Jin et al. (2019b) presents PubMedQA, which extracts question-style titles and their corresponding abstracts as the questions and contexts respectively. A few QA pairs are annotated by human experts and most of them are annotated based a simple heuristic rule with \"yes/no/maybe\".\n\nDue to the great power of contextualized word representations, pretrained language models also have been introduced to biomedical and clinical domain, e.g., BioELMo (Jin et al., 2019a) \n\n\nConclusion\n\nWe study the Clinical Reading Comprehension (CliniRC) task with the recently created emrQA dataset. Our qualitative and quantitative analysis as well as exploration of the two desired aspects of CliniRC systems show that future clinical QA datasets should not only be large-scale but also less noisy and more diverse. Moreover, questions that involve complex relations and are across different domains should be included, and then more advanced external knowledge incorporation methods as well as domain adaptation methods can be carefully designed and systematically evaluated.  \n\n\nA Experimental Set-up\n\nWe split the two datasets Medication and Relation based on the documents (clinical texts) into train, dev, test with the ratio 7:1:2. The statistics are shown in Table A1. We adopt Exact Match (EM) and F1 score (F1) as our evaluation metrics, same as the open-domain RC (Rajpurkar et al., 2016). We use SQuAD v1.1 official evaluation script 1 to evaluate all the models. All the models used in the paper, BiDAF 2 , DocReader 3 , QANet 4 , BERT 5 , BioBERT 6 , Clin-icalBERT 7 are run based on the implementations listed here and strictly followed the instructions.\n\nFor reproducibility, we list all the key hyperparaters we use for each method in the Table A2.\n\nWe implement our Knowledge Incorporation Module based on DocReader implementations. Entity embeddings are pretrained using TransE (Bordes et al., 2013) with the dimension of 100. The hyperparameters are kept same as the DocReader. All the models are run on NVIDIA GeForce GTX 1080 GPUs. We save the best model (with the highest EM) on the dev set and use it for test set.\n\n\nB Performance on Shorter Contexts\n\nUsing the entire clinical record as the context might be too long for models to capture sequential information. We also try to split the entire record into different sections (e.g., \"medical history\", \"family history\") based on some heuristic measures. Specifically, in order to split the clinical notes into sections, we notice that most sections begin with easily identifiable headers. To detect these headers we use a combination of heuristics such as whether the line contains colons, all uppercase formatting   Table A3: Performance of the two models on the shorter context setting. or phrases found in a list of clinical headers taken from SecTag (Denny et al., 2009). We then select the section that contains the answer as the context (\u223c100 words avg). We select DocReader and ClinicalBERT as representative methods and re-run them on the modified shorter context. The results are shown in Table A3. The performance of the two models is improved compared with the performance of models built on the whole record (long context). However, ClinicalBERT still does not outperform DocReader in this setting, indicating that longer context may not explain why BERT models do not win on this dataset or that shortening context in a such manner might break long dependencies.\n\nThis experiment setting may also inspire future research on \"Open Clinical Reading Comprehension\". Given that patients often have multiple clini- cal records, it may not be feasible to jointly use all of them as context for one question. Given multiple records for one patient (instead of just one) and a question, the model would first need to retrieve the most relevant paragraphs and do reading comprehension on each of them or find clever ways to merge them. Such a setting would be interesting for future CliniRC datasets to explore.\n\n\nC Easy/Hard Questions Split\n\nWe partition the questions into Easy and Hard. Specifically, we first train a BiLSTM reader and do the prediction on the test set. We obtain the performance of each question template by averaging the performance of all the questions made by this template (such template and question mappings are included in the emrQA dataset). Question templates that obtain higher performance than the overall performance are labeled as \"Easy\" and \"Hard\" otherwise. Then we map the difficulty level of question templates back to each question. The reason why we focus on splitting on the question template level is that we can avoid some random noise (e.g., random errors produced by the model on some questions). Also, we release the difficulty level of each question template so that users can easily know which questions are easy or hard and do not need to run a base model to obtain such mappings again. Distributions of easy/hard questions and results of the two selected models are shown in Table A4.\n\nFigure 1 :\n1Examples from the emrQA dataset: Part of a clinical note as context and 2 question-answer pairs. Due to the original emrQA generation issues, oftentimes answers are incomplete or contain irrelevant parts to the questions (the underlined parts are what we think the most relevant to the questions).\n\nFigure 3 :\n3Impact of training size on the performance of DocReader(Chen et al., 2017) based on the Medication and Relation dataset.\n\nFigure 4 :\n4Performances of DocReader and DocReader + Knowledge Incorporation Module (KIM) on our created questions modified from the Relation dataset.\n\n\n, BioBERT (Lee et al., 2019), and ClinicalBERT (Alsentzer et al., 2019). They adopt similar architectures of the original models but pretrained on the medical and clinical corpus, such as PubMed articles and MIMIC-III (Johnson et al., 2016) clinical notes.\n\n\nWe find that the performance of the best model trained on emrQA drops by 40% under this new setting, showing how critical it is for us to develop more robust and generalizable models for the CliniRC task.Medication Relation \n# Question \n222,957 904,592 \n# Context \n261 \n423 \n# Question Template \n80 \n139 \nQuestion: avg. tokens \n8.00 \n7.91 \nAnswers: avg. tokens \n9.47 \n10.41 \nContext: avg. tokens \n1062.66 \n889.23 \n\nTable 1: Statistics of two major subsets, Medication \nand Relation, of the emrQA dataset. \n\neral types of clinical questions that require domain \nknowledge and can frequently appear in the real \nclinical setting. We also carry out an experiment \nshowing that adding knowledge explicitly yields \naround 5% increase in F1 over the base model when \ntested on samples that we created by altering the \noriginal questions to involve semantic relations. To \nstudy generalizability, we ask medical experts to \ncreate new questions based on the unseen clinical \nnotes from MIMIC-III (Johnson et al., 2016), a \nfreely accessible critical care database. \n\nTable 2 :\n2An estimate of the quality of answers in theMedication and Relation datasets based on the analysis \nof our randomly sampled 50 questions for each dataset. \nQuality scores are the average of two human annota-\ntors' (maximum: 5). EM and F1 scores are calculated \nbetween human-labeled answers v.s. emrQA answers. \n\n\n\nTable 3 :\n3Error analysis on 50 sampled questions from the Medication and Relation dev sets respectively. Example question, ground truth and prediction from either Medication or Relation are given for each type of error.\n\nTable 4 :\n4Overall performance of all models on the Medication and Relation dataset. All numbers are percentages. (Alsentzer et al., 2019)) do not dominate as they do in the open-domain RC tasks. The reasons may be three-fold: 1) BERT benefits the most from large training corpora. The training corpora of BERTbase and BioBERT are Wikipedia + BookCorpus\n\n\nMedicationRelation # Train (Q / C) 154,684 /182 621,428 / 296 # Dev (Q / C)23,081 / 26 \n101,700 / 42 \n# Test (Q / C) \n45,192 / 53 \n181,464 / 85 \nTotal \n222,957 / 261 904,592 / 423 \n\n\n\nTable A1 :\nA1Statistics of train, dev, test set of the Medication and Relation datasets.\n\nTable A2 :\nA2Hyperparameters settings for all the methods used in the experiments.Dataset \nModel \nDev \nTest \nEM \nF1 \nEM \nF1 \n\nmedication \nDocReader \n32.19 76.21 33.45 77.08 \nClinicalBERT 30.16 74.81 32.18 75.79 \n\nrelation \nDocReader \n87.21 94.32 87.54 94.97 \nClinicalBERT 85.46 93.92 85.67 93.14 \n\n\n\nTable A4 :\nA4Distribution ofEasy/Hard Questions Performance of DocReader and ClinicalBERT on the easy/hard questions split.Easy \nHard \nTotal \nMedication \n33,037 (73.1%) 12,155 (26.9%) 45,192 (100%) \nRelation \n165,271 (91.1%) 16,193 (8.9%) 181,464 (100%) \n\nResults \n\nModel \nEasy \nHard \nTotal \nEM \nF1 \nEM \nF1 \nEM \nF1 \n\nMedication \nDocReader (Chen et al., 2017) \n30.25 \n73.78 \n13.26 61.46 25.68 \n70.45 \nClinicalBERT (Alsentzer et al., 2019) 28.25 \n72.02 \n12.64 60.98 24.06 \n69.05 \n\nRelation \nDocReader (Chen et al., 2017) \n87.66 \n95.39 \n79.85 89.62 86.94 \n94.85 \nClinicalBERT (Alsentzer et al., 2019) 86.06 \n93.71 \n78.09 86.57 85.33 \n93.06 \n\n\nNote that these 100 samples are sampled from errors, which are different from the previously sampled ones.\nhttps://rajpurkar.github.io/SQuAD-explorer/ 2 https://github.com/allenai/bi-att-flow 3 https://github.com/facebookresearch/DrQA 4 https://github.com/BangLiu/QANet-PyTorch\nAcknowledgmentsWe thank our medical experts for their annotations. We thank Ping Zhang, Changchang Yin and anonymous reviewers for their helpful comments. This research was sponsored in part by the Patient-Centered Outcomes Research Institute Funding ME-2017C1-6413, the Army Research Office under cooperative agreements W911NF-17-1-0412, NSF Grant IIS1815674, and Ohio Supercomputer Center(Center, 1987). The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S.Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.\nRecognizing question entailment for medical question answering. Asma Ben Abacha, Dina Demner-Fushman, AMIA Annual Symposium Proceedings. 2016310Asma Ben Abacha and Dina Demner-Fushman. 2016. Recognizing question entailment for medical ques- tion answering. In AMIA Annual Symposium Pro- ceedings, volume 2016, page 310.\n\nA question-entailment approach to question answering. Asma Ben Abacha, Dina Demner-Fushman, BMC bioinformatics. 201511Asma Ben Abacha and Dina Demner-Fushman. 2019. A question-entailment approach to question answer- ing. BMC bioinformatics, 20(1):511.\n\nAskhermes: An online question answering system for complex clinical questions. Hong Yu, Journal of biomedical informatics. 442and Hong Yu. 2011. Askhermes: An online ques- tion answering system for complex clinical ques- tions. Journal of biomedical informatics, 44(2):277- 288.\n\nOhio supercomputer center. Ohio Supercomputer, Center, Ohio Supercomputer Center. 1987. Ohio supercom- puter center.\n\nA thorough examination of the cnn/daily mail reading comprehension task. Danqi Chen, Jason Bolton, Christopher D Manning, ACL'16. Danqi Chen, Jason Bolton, and Christopher D Man- ning. 2016. A thorough examination of the cnn/daily mail reading comprehension task. In ACL'16, pages 2358-2367.\n\nReading wikipedia to answer opendomain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, ACL'17. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open- domain questions. In ACL'17, pages 1870-1879.\n\nLong short-term memory-networks for machine reading. Jianpeng Cheng, Li Dong, Mirella Lapata, EMNLP'16. Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long short-term memory-networks for machine reading. In EMNLP'16, pages 551-561.\n\nTransformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, G Jaime, Quoc Carbonell, Ruslan Le, Salakhutdinov, ACL'19. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car- bonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL'19, pages 2978- 2988.\n\nWhat can natural language processing do for clinical decision support. Dina Demner-Fushman, Wendy W Chapman, Clement J Mcdonald, Journal of biomedical informatics. 425Dina Demner-Fushman, Wendy W Chapman, and Clement J McDonald. 2009. What can natural lan- guage processing do for clinical decision support? Journal of biomedical informatics, 42(5):760-772.\n\nEvaluation of a method to identify and categorize section headers in clinical documents. C Joshua, Anderson Denny, Iii Spickard, Kevin B Johnson, B Neeraja, Josh F Peterson, Randolph A Peterson, Miller, Journal of the American Medical Informatics Association. 166Joshua C Denny, Anderson Spickard III, Kevin B John- son, Neeraja B Peterson, Josh F Peterson, and Ran- dolph A Miller. 2009. Evaluation of a method to identify and categorize section headers in clinical documents. Journal of the American Medical Infor- matics Association, 16(6):806-815.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT'19. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In NAACL-HLT'19, pages 4171-4186.\n\nAnnotation artifacts in natural language inference data. Swabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Samuel Schwartz, Noah A Bowman, Smith, NAACL-HLT'18. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural lan- guage inference data. In NAACL-HLT'18, pages 107-112.\n\nTeaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, NeurIPS'15. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In NeurIPS'15, pages 1693-1701.\n\nDeep read: A reading comprehension system. Lynette Hirschman, Marc Light, Eric Breck, John D Burger, ACL'1999. Lynette Hirschman, Marc Light, Eric Breck, and John D Burger. 1999. Deep read: A reading com- prehension system. In ACL'1999, pages 325-332.\n\nProbing biomedical embeddings from language models. Qiao Jin, Bhuwan Dhingra, William Cohen, Xinghua Lu, NAACL'19 RepE-val2019 Workshop. Qiao Jin, Bhuwan Dhingra, William Cohen, and Xinghua Lu. 2019a. Probing biomedical embed- dings from language models. In NAACL'19 RepE- val2019 Workshop, pages 82-89.\n\nPubmedqa: A dataset for biomedical research question answering. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, Xinghua Lu, EMNLP-IJCNLP'19. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019b. Pubmedqa: A dataset for biomedical research question answering. In EMNLP-IJCNLP'19, pages 2567-2577.\n\nMimiciii, a freely accessible critical care database. E W Alistair, Johnson, J Tom, Lu Pollard, H Lehman Shen, Mengling Li-Wei, Mohammad Feng, Benjamin Ghassemi, Peter Moody, Leo Anthony Szolovits, Roger G Celi, Mark, Scientific data. 3160035Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Moham- mad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. Mimic- iii, a freely accessible critical care database. Scien- tific data, 3:160035.\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, S Daniel, Luke Weld, Zettlemoyer, ACL'17. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL'17, pages 1601-1611.\n\nBioBERT: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics.\n\nBeyond information retrieval-medical question answering. Minsuk Lee, James Cimino, Hai Ran Zhu, Carl Sable, Vijay Shanker, John Ely, Hong Yu, AMIA annual symposium proceedings. 469Minsuk Lee, James Cimino, Hai Ran Zhu, Carl Sable, Vijay Shanker, John Ely, and Hong Yu. 2006. Be- yond information retrieval-medical question an- swering. In AMIA annual symposium proceedings, volume 2006, page 469.\n\nScispacy: Fast and robust models for biomedical natural language processing. Mark Neumann, Daniel King, Iz Beltagy, Waleed Ammar, ACL BioNLP Workshop. Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. Scispacy: Fast and robust models for biomedical natural language processing. In ACL BioNLP Workshop 2019.\n\nUsing outcome polarity in sentence extraction for medical question-answering. Yun Niu, Xiaodan Zhu, Graeme Hirst, AMIA Annual Symposium Proceedings. 599Yun Niu, Xiaodan Zhu, and Graeme Hirst. 2006. Using outcome polarity in sentence extraction for medical question-answering. In AMIA Annual Symposium Proceedings, volume 2006, page 599.\n\nemrqa: A large corpus for question answering on electronic medical records. Anusri Pampari, Preethi Raghavan, Jennifer Liang, Jian Peng, EMNLP'18. Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018. emrqa: A large corpus for ques- tion answering on electronic medical records. In EMNLP'18, pages 2357-2368.\n\nDeep contextualized word representations. E Matthew, Mark Peters, Mohit Neumann, Matt Iyyer, Christopher Gardner, Kenton Clark, Luke Lee, Zettlemoyer, NAACL-HLT'18. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In NAACL-HLT'18, pages 2227-2237.\n\nLanguage models as knowledge bases. Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, EMNLP-IJCNLP'19. Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowl- edge bases? In EMNLP-IJCNLP'19, pages 2463- 2473.\n\nImproving language understanding by generative pre-training. Technical report. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, OpenAIAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training. Technical re- port, OpenAI.\n\nKnow what you don't know: Unanswerable questions for squad. Pranav Rajpurkar, Robin Jia, Percy Liang, ACL'18. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable ques- tions for squad. In ACL'18, pages 784-789.\n\nSquad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, EMNLP'16. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In EMNLP'16, pages 2383-2392.\n\nData from clinical notes: a perspective on the tension between structure and flexible documentation. Joshua C Trent Rosenbloom, Hua Denny, Nancy Xu, Lorenzi, W William, Kevin B Johnson Stead, Journal of the American Medical Informatics Association. 182S Trent Rosenbloom, Joshua C Denny, Hua Xu, Nancy Lorenzi, William W Stead, and Kevin B Johnson. 2011. Data from clinical notes: a perspective on the tension between structure and flexible documen- tation. Journal of the American Medical Informatics Association, 18(2):181-186.\n\nBidirectional attention flow for machine comprehension. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hananneh Hajishirzi, 17Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hananneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR'17.\n\nTable cell search for question answering. Huan Sun, Hao Ma, Xiaodong He, Wen-Tau Yih, Yu Su, Xifeng Yan, WWW'16. Huan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su, and Xifeng Yan. 2016. Table cell search for question answering. In WWW'16, pages 771-782.\n\nOpen domain question answering via semantic enrichment. Huan Sun, Hao Ma, Wen-Tau Yih, Chen-Tse Tsai, Jingjing Liu, Ming-Wei Chang, WWW'15. Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai, Jingjing Liu, and Ming-Wei Chang. 2015. Open do- main question answering via semantic enrichment. In WWW'15, pages 1045-1055.\n\nClicr: A dataset of clinical case reports for machine reading comprehension. Walter Simon\u0161uster, Daelemans, NAACL-HLT'18. Simon\u0160uster and Walter Daelemans. 2018. Clicr: A dataset of clinical case reports for machine reading comprehension. In NAACL-HLT'18, pages 1551- 1563.\n\nNewsQA: A machine comprehension dataset. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, Kaheer Suleman, ACL'17 RepL4NLP Workshop. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har- ris, Alessandro Sordoni, Philip Bachman, and Ka- heer Suleman. 2017. NewsQA: A machine compre- hension dataset. In ACL'17 RepL4NLP Workshop, pages 191-200.\n\nBioasq: A challenge on large-scale biomedical semantic indexing and question answering. George Tsatsaronis, Michael Schroeder, Georgios Paliouras, Yannis Almirantis, Ion Androutsopoulos, Eric Gaussier, Patrick Gallinari, Thierry Artieres, Matthias Michael R Alvers, Zschunke, 2012 AAAI Fall Symposium Series. George Tsatsaronis, Michael Schroeder, Georgios Paliouras, Yannis Almirantis, Ion Androutsopoulos, Eric Gaussier, Patrick Gallinari, Thierry Artieres, Michael R Alvers, Matthias Zschunke, et al. 2012. Bioasq: A challenge on large-scale biomedical se- mantic indexing and question answering. In 2012 AAAI Fall Symposium Series.\n\nMachine comprehension using match-lstm and answer pointer. Shuohang Wang, Jing Jiang, 17Shuohang Wang and Jing Jiang. 2017. Machine com- prehension using match-lstm and answer pointer. ICLR'17.\n\nGated self-matching networks for reading comprehension and question answering. Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, Ming Zhou, ACL'17. Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching net- works for reading comprehension and question an- swering. In ACL'17, pages 189-198.\n\nClinical information extraction applications: a literature review. Yanshan Wang, Liwei Wang, Majid Rastegar-Mojarad, Sungrim Moon, Feichen Shen, Naveed Afzal, Sijia Liu, Yuqun Zeng, Saeed Mehrabi, Sunghwan Sohn, Journal of biomedical informatics. 77Yanshan Wang, Liwei Wang, Majid Rastegar-Mojarad, Sungrim Moon, Feichen Shen, Naveed Afzal, Sijia Liu, Yuqun Zeng, Saeed Mehrabi, Sunghwan Sohn, et al. 2018. Clinical information extraction appli- cations: a literature review. Journal of biomedical informatics, 77:34-49.\n\nClinical text annotation-what factors are associated with the cost of time?. Qiang Wei, Amy Franklin, Trevor Cohen, Hua Xu, AMIA Annual Symposium Proceedings. 20181552Qiang Wei, Amy Franklin, Trevor Cohen, and Hua Xu. 2018. Clinical text annotation-what factors are as- sociated with the cost of time? In AMIA Annual Symposium Proceedings, volume 2018, page 1552.\n\nQuestion answering systems in biology and medicine-the time is now. D Jonathan, Wren, 27BioinformaticsJonathan D Wren. 2011. Question answering systems in biology and medicine-the time is now. Bioinfor- matics, 27(14):2025-2026.\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, EMNLP'18. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo- pher D Manning. 2018. Hotpotqa: A dataset for di- verse, explainable multi-hop question answering. In EMNLP'18, pages 2369-2380.\n\nSemantic parsing via staged query graph generation: Question answering with knowledge base. Ming-Wei Wen-Tau Yih, Xiaodong Chang, Jianfeng He, Gao, ACL'15. Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In ACL'15, pages 1321-1331.\n\nQanet: Combining local convolution with global self-attention for reading comprehension. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, Quoc V Le, ICLR'18Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehen- sion. ICLR'18.\n\nERNIE: Enhanced language representation with informative entities. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, ACL'19. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: En- hanced language representation with informative en- tities. In ACL'19.\n\nAligning books and movies: Towards story-like visual explanations by watching movies and reading books. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, ICCV'15. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In ICCV'15, pages 19-27.\n", "annotations": {"author": "[{\"end\":237,\"start\":190},{\"end\":327,\"start\":238}]", "publisher": "[{\"end\":115,\"start\":74},{\"end\":603,\"start\":562}]", "author_last_name": "[{\"end\":199,\"start\":196},{\"end\":262,\"start\":245}]", "author_first_name": "[{\"end\":195,\"start\":190},{\"end\":244,\"start\":238}]", "author_affiliation": "[{\"end\":236,\"start\":201},{\"end\":326,\"start\":291}]", "title": "[{\"end\":73,\"start\":1},{\"end\":400,\"start\":328}]", "venue": "[{\"end\":489,\"start\":402}]", "abstract": "[{\"end\":1975,\"start\":630}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2170,\"start\":2141},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2194,\"start\":2170},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2212,\"start\":2194},{\"end\":2408,\"start\":2387},{\"end\":2425,\"start\":2408},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2436,\"start\":2425},{\"end\":2463,\"start\":2436},{\"end\":2466,\"start\":2465},{\"end\":3050,\"start\":3018},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3379,\"start\":3359},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3400,\"start\":3379},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3417,\"start\":3400},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3435,\"start\":3417},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3455,\"start\":3435},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3562,\"start\":3540},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3585,\"start\":3562},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3608,\"start\":3585},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3627,\"start\":3608},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3645,\"start\":3627},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4145,\"start\":4124},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5039,\"start\":5018},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5457,\"start\":5436},{\"end\":5557,\"start\":5520},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7387,\"start\":7365},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9311,\"start\":9282},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9399,\"start\":9377},{\"end\":9814,\"start\":9813},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13424,\"start\":13405},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13461,\"start\":13439},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14485,\"start\":14464},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14766,\"start\":14744},{\"end\":14788,\"start\":14766},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16161,\"start\":16142},{\"end\":16426,\"start\":16425},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17984,\"start\":17963},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18098,\"start\":18074},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18261,\"start\":18243},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19117,\"start\":19099},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23309,\"start\":23287},{\"end\":24411,\"start\":24392},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24449,\"start\":24427},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24687,\"start\":24667},{\"end\":24967,\"start\":24946},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26873,\"start\":26851},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29247,\"start\":29229},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29281,\"start\":29264},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29315,\"start\":29296},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29446,\"start\":29422},{\"end\":29545,\"start\":29506},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29614,\"start\":29591},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29639,\"start\":29614},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29729,\"start\":29707},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29750,\"start\":29729},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29767,\"start\":29750},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29785,\"start\":29767},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29905,\"start\":29884},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29933,\"start\":29911},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29961,\"start\":29940},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30413,\"start\":30392},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30615,\"start\":30597},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30632,\"start\":30615},{\"end\":30656,\"start\":30632},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30720,\"start\":30694},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31076,\"start\":31058},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31524,\"start\":31505},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32440,\"start\":32416},{\"end\":32959,\"start\":32938},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33890,\"start\":33870},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36453,\"start\":36434}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36365,\"start\":36055},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36499,\"start\":36366},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36652,\"start\":36500},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36911,\"start\":36653},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37972,\"start\":36912},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38298,\"start\":37973},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38520,\"start\":38299},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38875,\"start\":38521},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":39060,\"start\":38876},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":39150,\"start\":39061},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":39450,\"start\":39151},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":40091,\"start\":39451}]", "paragraph": "[{\"end\":2849,\"start\":1991},{\"end\":3773,\"start\":2851},{\"end\":4112,\"start\":3775},{\"end\":4512,\"start\":4114},{\"end\":5267,\"start\":4514},{\"end\":5854,\"start\":5269},{\"end\":6052,\"start\":5856},{\"end\":6903,\"start\":6054},{\"end\":7064,\"start\":6937},{\"end\":7345,\"start\":7066},{\"end\":7785,\"start\":7347},{\"end\":7851,\"start\":7814},{\"end\":8356,\"start\":7874},{\"end\":9400,\"start\":8358},{\"end\":9867,\"start\":9402},{\"end\":9899,\"start\":9878},{\"end\":10116,\"start\":9933},{\"end\":10666,\"start\":10153},{\"end\":11434,\"start\":10668},{\"end\":12714,\"start\":11436},{\"end\":13230,\"start\":12755},{\"end\":13771,\"start\":13232},{\"end\":14279,\"start\":13773},{\"end\":14998,\"start\":14281},{\"end\":15234,\"start\":15038},{\"end\":15752,\"start\":15273},{\"end\":16232,\"start\":15754},{\"end\":16952,\"start\":16234},{\"end\":17112,\"start\":16954},{\"end\":17842,\"start\":17144},{\"end\":19355,\"start\":17871},{\"end\":19881,\"start\":19357},{\"end\":20195,\"start\":19918},{\"end\":20480,\"start\":20235},{\"end\":22512,\"start\":20482},{\"end\":22747,\"start\":22514},{\"end\":23086,\"start\":22749},{\"end\":23717,\"start\":23088},{\"end\":25001,\"start\":23719},{\"end\":25104,\"start\":25003},{\"end\":25544,\"start\":25182},{\"end\":26146,\"start\":25546},{\"end\":26599,\"start\":26197},{\"end\":27583,\"start\":26601},{\"end\":28573,\"start\":27585},{\"end\":28963,\"start\":28575},{\"end\":29109,\"start\":28980},{\"end\":29786,\"start\":29111},{\"end\":30657,\"start\":29788},{\"end\":31338,\"start\":30659},{\"end\":31525,\"start\":31340},{\"end\":32120,\"start\":31540},{\"end\":32710,\"start\":32146},{\"end\":32806,\"start\":32712},{\"end\":33179,\"start\":32808},{\"end\":34491,\"start\":33217},{\"end\":35031,\"start\":34493},{\"end\":36054,\"start\":35063}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":25181,\"start\":25105}]", "table_ref": "[{\"end\":9811,\"start\":9804},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":11433,\"start\":11426},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":13699,\"start\":13692},{\"end\":15361,\"start\":15354},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":15938,\"start\":15930},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17288,\"start\":17281},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":17460,\"start\":17453},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":18144,\"start\":18137},{\"end\":18706,\"start\":18699},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":19655,\"start\":19646},{\"end\":25863,\"start\":25856},{\"end\":27624,\"start\":27617},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":32316,\"start\":32308},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":32805,\"start\":32797},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33741,\"start\":33733},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34122,\"start\":34114},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36053,\"start\":36045}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1989,\"start\":1977},{\"attributes\":{\"n\":\"2\"},\"end\":6935,\"start\":6906},{\"end\":7812,\"start\":7788},{\"end\":7872,\"start\":7854},{\"end\":9876,\"start\":9870},{\"attributes\":{\"n\":\"3\"},\"end\":9931,\"start\":9902},{\"attributes\":{\"n\":\"3.1\"},\"end\":10151,\"start\":10119},{\"attributes\":{\"n\":\"3.2\"},\"end\":12753,\"start\":12717},{\"attributes\":{\"n\":\"4\"},\"end\":15036,\"start\":15001},{\"attributes\":{\"n\":\"4.1\"},\"end\":15271,\"start\":15237},{\"attributes\":{\"n\":\"4.2\"},\"end\":17142,\"start\":17115},{\"attributes\":{\"n\":\"4.3\"},\"end\":17869,\"start\":17845},{\"attributes\":{\"n\":\"5\"},\"end\":19916,\"start\":19884},{\"attributes\":{\"n\":\"5.1\"},\"end\":20233,\"start\":20198},{\"attributes\":{\"n\":\"5.2\"},\"end\":26195,\"start\":26149},{\"attributes\":{\"n\":\"6\"},\"end\":28978,\"start\":28966},{\"attributes\":{\"n\":\"7\"},\"end\":31538,\"start\":31528},{\"end\":32144,\"start\":32123},{\"end\":33215,\"start\":33182},{\"end\":35061,\"start\":35034},{\"end\":36066,\"start\":36056},{\"end\":36377,\"start\":36367},{\"end\":36511,\"start\":36501},{\"end\":37983,\"start\":37974},{\"end\":38309,\"start\":38300},{\"end\":38531,\"start\":38522},{\"end\":39072,\"start\":39062},{\"end\":39162,\"start\":39152},{\"end\":39462,\"start\":39452}]", "table": "[{\"end\":37972,\"start\":37118},{\"end\":38298,\"start\":38029},{\"end\":39060,\"start\":38953},{\"end\":39450,\"start\":39234},{\"end\":40091,\"start\":39575}]", "figure_caption": "[{\"end\":36365,\"start\":36068},{\"end\":36499,\"start\":36379},{\"end\":36652,\"start\":36513},{\"end\":36911,\"start\":36655},{\"end\":37118,\"start\":36914},{\"end\":38029,\"start\":37985},{\"end\":38520,\"start\":38311},{\"end\":38875,\"start\":38533},{\"end\":38953,\"start\":38878},{\"end\":39150,\"start\":39075},{\"end\":39234,\"start\":39165},{\"end\":39575,\"start\":39465}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4511,\"start\":4503},{\"end\":7957,\"start\":7949},{\"end\":8355,\"start\":8347},{\"end\":11590,\"start\":11582},{\"end\":12999,\"start\":12991},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16254,\"start\":16246},{\"end\":21226,\"start\":21218},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21681,\"start\":21673},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25566,\"start\":25558}]", "bib_author_first_name": "[{\"end\":41201,\"start\":41197},{\"end\":41218,\"start\":41214},{\"end\":41512,\"start\":41508},{\"end\":41529,\"start\":41525},{\"end\":41790,\"start\":41786},{\"end\":42183,\"start\":42178},{\"end\":42195,\"start\":42190},{\"end\":42217,\"start\":42204},{\"end\":42453,\"start\":42448},{\"end\":42464,\"start\":42460},{\"end\":42477,\"start\":42472},{\"end\":42493,\"start\":42486},{\"end\":42716,\"start\":42708},{\"end\":42726,\"start\":42724},{\"end\":42740,\"start\":42733},{\"end\":42971,\"start\":42965},{\"end\":42983,\"start\":42977},{\"end\":42996,\"start\":42990},{\"end\":43004,\"start\":43003},{\"end\":43016,\"start\":43012},{\"end\":43034,\"start\":43028},{\"end\":43340,\"start\":43336},{\"end\":43362,\"start\":43357},{\"end\":43364,\"start\":43363},{\"end\":43383,\"start\":43374},{\"end\":43714,\"start\":43713},{\"end\":43731,\"start\":43723},{\"end\":43742,\"start\":43739},{\"end\":43758,\"start\":43753},{\"end\":43760,\"start\":43759},{\"end\":43771,\"start\":43770},{\"end\":43785,\"start\":43781},{\"end\":43787,\"start\":43786},{\"end\":43808,\"start\":43798},{\"end\":44264,\"start\":44259},{\"end\":44281,\"start\":44273},{\"end\":44295,\"start\":44289},{\"end\":44309,\"start\":44301},{\"end\":44589,\"start\":44583},{\"end\":44613,\"start\":44609},{\"end\":44630,\"start\":44627},{\"end\":44643,\"start\":44637},{\"end\":44660,\"start\":44654},{\"end\":44931,\"start\":44927},{\"end\":44953,\"start\":44948},{\"end\":44969,\"start\":44963},{\"end\":44989,\"start\":44984},{\"end\":45004,\"start\":45000},{\"end\":45017,\"start\":45010},{\"end\":45032,\"start\":45028},{\"end\":45306,\"start\":45299},{\"end\":45322,\"start\":45318},{\"end\":45334,\"start\":45330},{\"end\":45348,\"start\":45342},{\"end\":45565,\"start\":45561},{\"end\":45577,\"start\":45571},{\"end\":45594,\"start\":45587},{\"end\":45609,\"start\":45602},{\"end\":45882,\"start\":45878},{\"end\":45894,\"start\":45888},{\"end\":45913,\"start\":45904},{\"end\":45926,\"start\":45919},{\"end\":45941,\"start\":45934},{\"end\":46199,\"start\":46198},{\"end\":46201,\"start\":46200},{\"end\":46222,\"start\":46221},{\"end\":46230,\"start\":46228},{\"end\":46248,\"start\":46240},{\"end\":46263,\"start\":46255},{\"end\":46280,\"start\":46272},{\"end\":46295,\"start\":46287},{\"end\":46311,\"start\":46306},{\"end\":46322,\"start\":46319},{\"end\":46330,\"start\":46323},{\"end\":46349,\"start\":46342},{\"end\":46741,\"start\":46735},{\"end\":46755,\"start\":46749},{\"end\":46763,\"start\":46762},{\"end\":46776,\"start\":46772},{\"end\":47094,\"start\":47087},{\"end\":47106,\"start\":47100},{\"end\":47121,\"start\":47113},{\"end\":47136,\"start\":47127},{\"end\":47148,\"start\":47142},{\"end\":47158,\"start\":47154},{\"end\":47172,\"start\":47166},{\"end\":47469,\"start\":47463},{\"end\":47480,\"start\":47475},{\"end\":47492,\"start\":47489},{\"end\":47496,\"start\":47493},{\"end\":47506,\"start\":47502},{\"end\":47519,\"start\":47514},{\"end\":47533,\"start\":47529},{\"end\":47543,\"start\":47539},{\"end\":47885,\"start\":47881},{\"end\":47901,\"start\":47895},{\"end\":47910,\"start\":47908},{\"end\":47926,\"start\":47920},{\"end\":48206,\"start\":48203},{\"end\":48219,\"start\":48212},{\"end\":48231,\"start\":48225},{\"end\":48545,\"start\":48539},{\"end\":48562,\"start\":48555},{\"end\":48581,\"start\":48573},{\"end\":48593,\"start\":48589},{\"end\":48833,\"start\":48832},{\"end\":48847,\"start\":48843},{\"end\":48861,\"start\":48856},{\"end\":48875,\"start\":48871},{\"end\":48894,\"start\":48883},{\"end\":48910,\"start\":48904},{\"end\":48922,\"start\":48918},{\"end\":49193,\"start\":49188},{\"end\":49206,\"start\":49203},{\"end\":49229,\"start\":49220},{\"end\":49245,\"start\":49238},{\"end\":49258,\"start\":49253},{\"end\":49275,\"start\":49268},{\"end\":49289,\"start\":49280},{\"end\":49595,\"start\":49591},{\"end\":49612,\"start\":49605},{\"end\":49628,\"start\":49625},{\"end\":49643,\"start\":49639},{\"end\":49893,\"start\":49887},{\"end\":49910,\"start\":49905},{\"end\":49921,\"start\":49916},{\"end\":50145,\"start\":50139},{\"end\":50161,\"start\":50157},{\"end\":50179,\"start\":50169},{\"end\":50194,\"start\":50189},{\"end\":50484,\"start\":50478},{\"end\":50486,\"start\":50485},{\"end\":50508,\"start\":50505},{\"end\":50521,\"start\":50516},{\"end\":50536,\"start\":50535},{\"end\":50561,\"start\":50546},{\"end\":50971,\"start\":50964},{\"end\":50986,\"start\":50977},{\"end\":51000,\"start\":50997},{\"end\":51018,\"start\":51010},{\"end\":51225,\"start\":51221},{\"end\":51234,\"start\":51231},{\"end\":51247,\"start\":51239},{\"end\":51259,\"start\":51252},{\"end\":51267,\"start\":51265},{\"end\":51278,\"start\":51272},{\"end\":51494,\"start\":51490},{\"end\":51503,\"start\":51500},{\"end\":51515,\"start\":51508},{\"end\":51529,\"start\":51521},{\"end\":51544,\"start\":51536},{\"end\":51558,\"start\":51550},{\"end\":51830,\"start\":51824},{\"end\":52067,\"start\":52063},{\"end\":52083,\"start\":52079},{\"end\":52096,\"start\":52090},{\"end\":52109,\"start\":52103},{\"end\":52128,\"start\":52118},{\"end\":52144,\"start\":52138},{\"end\":52160,\"start\":52154},{\"end\":52499,\"start\":52493},{\"end\":52520,\"start\":52513},{\"end\":52540,\"start\":52532},{\"end\":52558,\"start\":52552},{\"end\":52574,\"start\":52571},{\"end\":52596,\"start\":52592},{\"end\":52614,\"start\":52607},{\"end\":52633,\"start\":52626},{\"end\":52652,\"start\":52644},{\"end\":53109,\"start\":53101},{\"end\":53120,\"start\":53116},{\"end\":53322,\"start\":53316},{\"end\":53332,\"start\":53329},{\"end\":53343,\"start\":53339},{\"end\":53355,\"start\":53349},{\"end\":53367,\"start\":53363},{\"end\":53634,\"start\":53627},{\"end\":53646,\"start\":53641},{\"end\":53658,\"start\":53653},{\"end\":53684,\"start\":53677},{\"end\":53698,\"start\":53691},{\"end\":53711,\"start\":53705},{\"end\":53724,\"start\":53719},{\"end\":53735,\"start\":53730},{\"end\":53747,\"start\":53742},{\"end\":53765,\"start\":53757},{\"end\":54164,\"start\":54159},{\"end\":54173,\"start\":54170},{\"end\":54190,\"start\":54184},{\"end\":54201,\"start\":54198},{\"end\":54516,\"start\":54515},{\"end\":54758,\"start\":54752},{\"end\":54769,\"start\":54765},{\"end\":54782,\"start\":54774},{\"end\":54796,\"start\":54790},{\"end\":54812,\"start\":54805},{\"end\":54826,\"start\":54820},{\"end\":54855,\"start\":54842},{\"end\":55208,\"start\":55200},{\"end\":55230,\"start\":55222},{\"end\":55246,\"start\":55238},{\"end\":55545,\"start\":55540},{\"end\":55549,\"start\":55546},{\"end\":55559,\"start\":55554},{\"end\":55577,\"start\":55567},{\"end\":55588,\"start\":55585},{\"end\":55598,\"start\":55595},{\"end\":55613,\"start\":55605},{\"end\":55629,\"start\":55623},{\"end\":55921,\"start\":55913},{\"end\":55931,\"start\":55929},{\"end\":55944,\"start\":55937},{\"end\":55953,\"start\":55950},{\"end\":55968,\"start\":55961},{\"end\":55977,\"start\":55974},{\"end\":56263,\"start\":56258},{\"end\":56273,\"start\":56269},{\"end\":56285,\"start\":56281},{\"end\":56299,\"start\":56293},{\"end\":56321,\"start\":56315},{\"end\":56338,\"start\":56331},{\"end\":56354,\"start\":56349}]", "bib_author_last_name": "[{\"end\":41212,\"start\":41202},{\"end\":41233,\"start\":41219},{\"end\":41523,\"start\":41513},{\"end\":41544,\"start\":41530},{\"end\":41793,\"start\":41791},{\"end\":42032,\"start\":42014},{\"end\":42040,\"start\":42034},{\"end\":42188,\"start\":42184},{\"end\":42202,\"start\":42196},{\"end\":42225,\"start\":42218},{\"end\":42458,\"start\":42454},{\"end\":42470,\"start\":42465},{\"end\":42484,\"start\":42478},{\"end\":42500,\"start\":42494},{\"end\":42722,\"start\":42717},{\"end\":42731,\"start\":42727},{\"end\":42747,\"start\":42741},{\"end\":42975,\"start\":42972},{\"end\":42988,\"start\":42984},{\"end\":43001,\"start\":42997},{\"end\":43010,\"start\":43005},{\"end\":43026,\"start\":43017},{\"end\":43037,\"start\":43035},{\"end\":43052,\"start\":43039},{\"end\":43355,\"start\":43341},{\"end\":43372,\"start\":43365},{\"end\":43392,\"start\":43384},{\"end\":43721,\"start\":43715},{\"end\":43737,\"start\":43732},{\"end\":43751,\"start\":43743},{\"end\":43768,\"start\":43761},{\"end\":43779,\"start\":43772},{\"end\":43796,\"start\":43788},{\"end\":43817,\"start\":43809},{\"end\":43825,\"start\":43819},{\"end\":44271,\"start\":44265},{\"end\":44287,\"start\":44282},{\"end\":44299,\"start\":44296},{\"end\":44319,\"start\":44310},{\"end\":44607,\"start\":44590},{\"end\":44625,\"start\":44614},{\"end\":44635,\"start\":44631},{\"end\":44652,\"start\":44644},{\"end\":44667,\"start\":44661},{\"end\":44674,\"start\":44669},{\"end\":44946,\"start\":44932},{\"end\":44961,\"start\":44954},{\"end\":44982,\"start\":44970},{\"end\":44998,\"start\":44990},{\"end\":45008,\"start\":45005},{\"end\":45026,\"start\":45018},{\"end\":45040,\"start\":45033},{\"end\":45316,\"start\":45307},{\"end\":45328,\"start\":45323},{\"end\":45340,\"start\":45335},{\"end\":45355,\"start\":45349},{\"end\":45569,\"start\":45566},{\"end\":45585,\"start\":45578},{\"end\":45600,\"start\":45595},{\"end\":45612,\"start\":45610},{\"end\":45886,\"start\":45883},{\"end\":45902,\"start\":45895},{\"end\":45917,\"start\":45914},{\"end\":45932,\"start\":45927},{\"end\":45944,\"start\":45942},{\"end\":46210,\"start\":46202},{\"end\":46219,\"start\":46212},{\"end\":46226,\"start\":46223},{\"end\":46238,\"start\":46231},{\"end\":46253,\"start\":46249},{\"end\":46270,\"start\":46264},{\"end\":46285,\"start\":46281},{\"end\":46304,\"start\":46296},{\"end\":46317,\"start\":46312},{\"end\":46340,\"start\":46331},{\"end\":46354,\"start\":46350},{\"end\":46360,\"start\":46356},{\"end\":46747,\"start\":46742},{\"end\":46760,\"start\":46756},{\"end\":46770,\"start\":46764},{\"end\":46781,\"start\":46777},{\"end\":46794,\"start\":46783},{\"end\":47098,\"start\":47095},{\"end\":47111,\"start\":47107},{\"end\":47125,\"start\":47122},{\"end\":47140,\"start\":47137},{\"end\":47152,\"start\":47149},{\"end\":47164,\"start\":47159},{\"end\":47177,\"start\":47173},{\"end\":47473,\"start\":47470},{\"end\":47487,\"start\":47481},{\"end\":47500,\"start\":47497},{\"end\":47512,\"start\":47507},{\"end\":47527,\"start\":47520},{\"end\":47537,\"start\":47534},{\"end\":47546,\"start\":47544},{\"end\":47893,\"start\":47886},{\"end\":47906,\"start\":47902},{\"end\":47918,\"start\":47911},{\"end\":47932,\"start\":47927},{\"end\":48210,\"start\":48207},{\"end\":48223,\"start\":48220},{\"end\":48237,\"start\":48232},{\"end\":48553,\"start\":48546},{\"end\":48571,\"start\":48563},{\"end\":48587,\"start\":48582},{\"end\":48598,\"start\":48594},{\"end\":48841,\"start\":48834},{\"end\":48854,\"start\":48848},{\"end\":48869,\"start\":48862},{\"end\":48881,\"start\":48876},{\"end\":48902,\"start\":48895},{\"end\":48916,\"start\":48911},{\"end\":48926,\"start\":48923},{\"end\":48939,\"start\":48928},{\"end\":49201,\"start\":49194},{\"end\":49218,\"start\":49207},{\"end\":49236,\"start\":49230},{\"end\":49251,\"start\":49246},{\"end\":49266,\"start\":49259},{\"end\":49278,\"start\":49276},{\"end\":49296,\"start\":49290},{\"end\":49603,\"start\":49596},{\"end\":49623,\"start\":49613},{\"end\":49637,\"start\":49629},{\"end\":49653,\"start\":49644},{\"end\":49903,\"start\":49894},{\"end\":49914,\"start\":49911},{\"end\":49927,\"start\":49922},{\"end\":50155,\"start\":50146},{\"end\":50167,\"start\":50162},{\"end\":50187,\"start\":50180},{\"end\":50200,\"start\":50195},{\"end\":50503,\"start\":50487},{\"end\":50514,\"start\":50509},{\"end\":50524,\"start\":50522},{\"end\":50533,\"start\":50526},{\"end\":50544,\"start\":50537},{\"end\":50567,\"start\":50562},{\"end\":50975,\"start\":50972},{\"end\":50995,\"start\":50987},{\"end\":51008,\"start\":51001},{\"end\":51029,\"start\":51019},{\"end\":51229,\"start\":51226},{\"end\":51237,\"start\":51235},{\"end\":51250,\"start\":51248},{\"end\":51263,\"start\":51260},{\"end\":51270,\"start\":51268},{\"end\":51282,\"start\":51279},{\"end\":51498,\"start\":51495},{\"end\":51506,\"start\":51504},{\"end\":51519,\"start\":51516},{\"end\":51534,\"start\":51530},{\"end\":51548,\"start\":51545},{\"end\":51564,\"start\":51559},{\"end\":51842,\"start\":51831},{\"end\":51853,\"start\":51844},{\"end\":52077,\"start\":52068},{\"end\":52088,\"start\":52084},{\"end\":52101,\"start\":52097},{\"end\":52116,\"start\":52110},{\"end\":52136,\"start\":52129},{\"end\":52152,\"start\":52145},{\"end\":52168,\"start\":52161},{\"end\":52511,\"start\":52500},{\"end\":52530,\"start\":52521},{\"end\":52550,\"start\":52541},{\"end\":52569,\"start\":52559},{\"end\":52590,\"start\":52575},{\"end\":52605,\"start\":52597},{\"end\":52624,\"start\":52615},{\"end\":52642,\"start\":52634},{\"end\":52669,\"start\":52653},{\"end\":52679,\"start\":52671},{\"end\":53114,\"start\":53110},{\"end\":53126,\"start\":53121},{\"end\":53327,\"start\":53323},{\"end\":53337,\"start\":53333},{\"end\":53347,\"start\":53344},{\"end\":53361,\"start\":53356},{\"end\":53372,\"start\":53368},{\"end\":53639,\"start\":53635},{\"end\":53651,\"start\":53647},{\"end\":53675,\"start\":53659},{\"end\":53689,\"start\":53685},{\"end\":53703,\"start\":53699},{\"end\":53717,\"start\":53712},{\"end\":53728,\"start\":53725},{\"end\":53740,\"start\":53736},{\"end\":53755,\"start\":53748},{\"end\":53770,\"start\":53766},{\"end\":54168,\"start\":54165},{\"end\":54182,\"start\":54174},{\"end\":54196,\"start\":54191},{\"end\":54204,\"start\":54202},{\"end\":54525,\"start\":54517},{\"end\":54531,\"start\":54527},{\"end\":54763,\"start\":54759},{\"end\":54772,\"start\":54770},{\"end\":54788,\"start\":54783},{\"end\":54803,\"start\":54797},{\"end\":54818,\"start\":54813},{\"end\":54840,\"start\":54827},{\"end\":54863,\"start\":54856},{\"end\":55220,\"start\":55209},{\"end\":55236,\"start\":55231},{\"end\":55249,\"start\":55247},{\"end\":55254,\"start\":55251},{\"end\":55552,\"start\":55550},{\"end\":55565,\"start\":55560},{\"end\":55583,\"start\":55578},{\"end\":55593,\"start\":55589},{\"end\":55603,\"start\":55599},{\"end\":55621,\"start\":55614},{\"end\":55632,\"start\":55630},{\"end\":55927,\"start\":55922},{\"end\":55935,\"start\":55932},{\"end\":55948,\"start\":55945},{\"end\":55959,\"start\":55954},{\"end\":55972,\"start\":55969},{\"end\":55981,\"start\":55978},{\"end\":56267,\"start\":56264},{\"end\":56279,\"start\":56274},{\"end\":56291,\"start\":56286},{\"end\":56313,\"start\":56300},{\"end\":56329,\"start\":56322},{\"end\":56347,\"start\":56339},{\"end\":56361,\"start\":56355}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17677052},\"end\":41452,\"start\":41133},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":59222825},\"end\":41705,\"start\":41454},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15241604},\"end\":41985,\"start\":41707},{\"attributes\":{\"id\":\"b3\"},\"end\":42103,\"start\":41987},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6360322},\"end\":42396,\"start\":42105},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3618568},\"end\":42653,\"start\":42398},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6506243},\"end\":42890,\"start\":42655},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":57759363},\"end\":43263,\"start\":42892},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13780041},\"end\":43622,\"start\":43265},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18902545},\"end\":44175,\"start\":43624},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52967399},\"end\":44524,\"start\":44177},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4537113},\"end\":44883,\"start\":44526},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6203757},\"end\":45254,\"start\":44885},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15197674},\"end\":45507,\"start\":45256},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":102353852},\"end\":45812,\"start\":45509},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202572622},\"end\":46142,\"start\":45814},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":33285731},\"end\":46643,\"start\":46144},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":26501419},\"end\":46993,\"start\":46645},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":59291975},\"end\":47404,\"start\":46995},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14578129},\"end\":47802,\"start\":47406},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":67788603},\"end\":48123,\"start\":47804},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":16033035},\"end\":48461,\"start\":48125},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52158121},\"end\":48788,\"start\":48463},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3626819},\"end\":49150,\"start\":48790},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202539551},\"end\":49510,\"start\":49152},{\"attributes\":{\"id\":\"b25\"},\"end\":49825,\"start\":49512},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":47018994},\"end\":50076,\"start\":49827},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11816014},\"end\":50375,\"start\":50078},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3763690},\"end\":50906,\"start\":50377},{\"attributes\":{\"id\":\"b29\"},\"end\":51177,\"start\":50908},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1322576},\"end\":51432,\"start\":51179},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2725912},\"end\":51745,\"start\":51434},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4311819},\"end\":52020,\"start\":51747},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1167588},\"end\":52403,\"start\":52022},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":10306599},\"end\":53040,\"start\":52405},{\"attributes\":{\"id\":\"b35\"},\"end\":53235,\"start\":53042},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12501880},\"end\":53558,\"start\":53237},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3632923},\"end\":54080,\"start\":53560},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":73482002},\"end\":54445,\"start\":54082},{\"attributes\":{\"id\":\"b39\"},\"end\":54675,\"start\":54447},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":52822214},\"end\":55106,\"start\":54677},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":18309765},\"end\":55449,\"start\":55108},{\"attributes\":{\"id\":\"b42\"},\"end\":55844,\"start\":55451},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":158046772},\"end\":56152,\"start\":55846},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":6866988},\"end\":56617,\"start\":56154}]", "bib_title": "[{\"end\":41195,\"start\":41133},{\"end\":41506,\"start\":41454},{\"end\":41784,\"start\":41707},{\"end\":42176,\"start\":42105},{\"end\":42446,\"start\":42398},{\"end\":42706,\"start\":42655},{\"end\":42963,\"start\":42892},{\"end\":43334,\"start\":43265},{\"end\":43711,\"start\":43624},{\"end\":44257,\"start\":44177},{\"end\":44581,\"start\":44526},{\"end\":44925,\"start\":44885},{\"end\":45297,\"start\":45256},{\"end\":45559,\"start\":45509},{\"end\":45876,\"start\":45814},{\"end\":46196,\"start\":46144},{\"end\":46733,\"start\":46645},{\"end\":47085,\"start\":46995},{\"end\":47461,\"start\":47406},{\"end\":47879,\"start\":47804},{\"end\":48201,\"start\":48125},{\"end\":48537,\"start\":48463},{\"end\":48830,\"start\":48790},{\"end\":49186,\"start\":49152},{\"end\":49885,\"start\":49827},{\"end\":50137,\"start\":50078},{\"end\":50476,\"start\":50377},{\"end\":51219,\"start\":51179},{\"end\":51488,\"start\":51434},{\"end\":51822,\"start\":51747},{\"end\":52061,\"start\":52022},{\"end\":52491,\"start\":52405},{\"end\":53314,\"start\":53237},{\"end\":53625,\"start\":53560},{\"end\":54157,\"start\":54082},{\"end\":54750,\"start\":54677},{\"end\":55198,\"start\":55108},{\"end\":55911,\"start\":55846},{\"end\":56256,\"start\":56154}]", "bib_author": "[{\"end\":41214,\"start\":41197},{\"end\":41235,\"start\":41214},{\"end\":41525,\"start\":41508},{\"end\":41546,\"start\":41525},{\"end\":41795,\"start\":41786},{\"end\":42034,\"start\":42014},{\"end\":42042,\"start\":42034},{\"end\":42190,\"start\":42178},{\"end\":42204,\"start\":42190},{\"end\":42227,\"start\":42204},{\"end\":42460,\"start\":42448},{\"end\":42472,\"start\":42460},{\"end\":42486,\"start\":42472},{\"end\":42502,\"start\":42486},{\"end\":42724,\"start\":42708},{\"end\":42733,\"start\":42724},{\"end\":42749,\"start\":42733},{\"end\":42977,\"start\":42965},{\"end\":42990,\"start\":42977},{\"end\":43003,\"start\":42990},{\"end\":43012,\"start\":43003},{\"end\":43028,\"start\":43012},{\"end\":43039,\"start\":43028},{\"end\":43054,\"start\":43039},{\"end\":43357,\"start\":43336},{\"end\":43374,\"start\":43357},{\"end\":43394,\"start\":43374},{\"end\":43723,\"start\":43713},{\"end\":43739,\"start\":43723},{\"end\":43753,\"start\":43739},{\"end\":43770,\"start\":43753},{\"end\":43781,\"start\":43770},{\"end\":43798,\"start\":43781},{\"end\":43819,\"start\":43798},{\"end\":43827,\"start\":43819},{\"end\":44273,\"start\":44259},{\"end\":44289,\"start\":44273},{\"end\":44301,\"start\":44289},{\"end\":44321,\"start\":44301},{\"end\":44609,\"start\":44583},{\"end\":44627,\"start\":44609},{\"end\":44637,\"start\":44627},{\"end\":44654,\"start\":44637},{\"end\":44669,\"start\":44654},{\"end\":44676,\"start\":44669},{\"end\":44948,\"start\":44927},{\"end\":44963,\"start\":44948},{\"end\":44984,\"start\":44963},{\"end\":45000,\"start\":44984},{\"end\":45010,\"start\":45000},{\"end\":45028,\"start\":45010},{\"end\":45042,\"start\":45028},{\"end\":45318,\"start\":45299},{\"end\":45330,\"start\":45318},{\"end\":45342,\"start\":45330},{\"end\":45357,\"start\":45342},{\"end\":45571,\"start\":45561},{\"end\":45587,\"start\":45571},{\"end\":45602,\"start\":45587},{\"end\":45614,\"start\":45602},{\"end\":45888,\"start\":45878},{\"end\":45904,\"start\":45888},{\"end\":45919,\"start\":45904},{\"end\":45934,\"start\":45919},{\"end\":45946,\"start\":45934},{\"end\":46212,\"start\":46198},{\"end\":46221,\"start\":46212},{\"end\":46228,\"start\":46221},{\"end\":46240,\"start\":46228},{\"end\":46255,\"start\":46240},{\"end\":46272,\"start\":46255},{\"end\":46287,\"start\":46272},{\"end\":46306,\"start\":46287},{\"end\":46319,\"start\":46306},{\"end\":46342,\"start\":46319},{\"end\":46356,\"start\":46342},{\"end\":46362,\"start\":46356},{\"end\":46749,\"start\":46735},{\"end\":46762,\"start\":46749},{\"end\":46772,\"start\":46762},{\"end\":46783,\"start\":46772},{\"end\":46796,\"start\":46783},{\"end\":47100,\"start\":47087},{\"end\":47113,\"start\":47100},{\"end\":47127,\"start\":47113},{\"end\":47142,\"start\":47127},{\"end\":47154,\"start\":47142},{\"end\":47166,\"start\":47154},{\"end\":47179,\"start\":47166},{\"end\":47475,\"start\":47463},{\"end\":47489,\"start\":47475},{\"end\":47502,\"start\":47489},{\"end\":47514,\"start\":47502},{\"end\":47529,\"start\":47514},{\"end\":47539,\"start\":47529},{\"end\":47548,\"start\":47539},{\"end\":47895,\"start\":47881},{\"end\":47908,\"start\":47895},{\"end\":47920,\"start\":47908},{\"end\":47934,\"start\":47920},{\"end\":48212,\"start\":48203},{\"end\":48225,\"start\":48212},{\"end\":48239,\"start\":48225},{\"end\":48555,\"start\":48539},{\"end\":48573,\"start\":48555},{\"end\":48589,\"start\":48573},{\"end\":48600,\"start\":48589},{\"end\":48843,\"start\":48832},{\"end\":48856,\"start\":48843},{\"end\":48871,\"start\":48856},{\"end\":48883,\"start\":48871},{\"end\":48904,\"start\":48883},{\"end\":48918,\"start\":48904},{\"end\":48928,\"start\":48918},{\"end\":48941,\"start\":48928},{\"end\":49203,\"start\":49188},{\"end\":49220,\"start\":49203},{\"end\":49238,\"start\":49220},{\"end\":49253,\"start\":49238},{\"end\":49268,\"start\":49253},{\"end\":49280,\"start\":49268},{\"end\":49298,\"start\":49280},{\"end\":49605,\"start\":49591},{\"end\":49625,\"start\":49605},{\"end\":49639,\"start\":49625},{\"end\":49655,\"start\":49639},{\"end\":49905,\"start\":49887},{\"end\":49916,\"start\":49905},{\"end\":49929,\"start\":49916},{\"end\":50157,\"start\":50139},{\"end\":50169,\"start\":50157},{\"end\":50189,\"start\":50169},{\"end\":50202,\"start\":50189},{\"end\":50505,\"start\":50478},{\"end\":50516,\"start\":50505},{\"end\":50526,\"start\":50516},{\"end\":50535,\"start\":50526},{\"end\":50546,\"start\":50535},{\"end\":50569,\"start\":50546},{\"end\":50977,\"start\":50964},{\"end\":50997,\"start\":50977},{\"end\":51010,\"start\":50997},{\"end\":51031,\"start\":51010},{\"end\":51231,\"start\":51221},{\"end\":51239,\"start\":51231},{\"end\":51252,\"start\":51239},{\"end\":51265,\"start\":51252},{\"end\":51272,\"start\":51265},{\"end\":51284,\"start\":51272},{\"end\":51500,\"start\":51490},{\"end\":51508,\"start\":51500},{\"end\":51521,\"start\":51508},{\"end\":51536,\"start\":51521},{\"end\":51550,\"start\":51536},{\"end\":51566,\"start\":51550},{\"end\":51844,\"start\":51824},{\"end\":51855,\"start\":51844},{\"end\":52079,\"start\":52063},{\"end\":52090,\"start\":52079},{\"end\":52103,\"start\":52090},{\"end\":52118,\"start\":52103},{\"end\":52138,\"start\":52118},{\"end\":52154,\"start\":52138},{\"end\":52170,\"start\":52154},{\"end\":52513,\"start\":52493},{\"end\":52532,\"start\":52513},{\"end\":52552,\"start\":52532},{\"end\":52571,\"start\":52552},{\"end\":52592,\"start\":52571},{\"end\":52607,\"start\":52592},{\"end\":52626,\"start\":52607},{\"end\":52644,\"start\":52626},{\"end\":52671,\"start\":52644},{\"end\":52681,\"start\":52671},{\"end\":53116,\"start\":53101},{\"end\":53128,\"start\":53116},{\"end\":53329,\"start\":53316},{\"end\":53339,\"start\":53329},{\"end\":53349,\"start\":53339},{\"end\":53363,\"start\":53349},{\"end\":53374,\"start\":53363},{\"end\":53641,\"start\":53627},{\"end\":53653,\"start\":53641},{\"end\":53677,\"start\":53653},{\"end\":53691,\"start\":53677},{\"end\":53705,\"start\":53691},{\"end\":53719,\"start\":53705},{\"end\":53730,\"start\":53719},{\"end\":53742,\"start\":53730},{\"end\":53757,\"start\":53742},{\"end\":53772,\"start\":53757},{\"end\":54170,\"start\":54159},{\"end\":54184,\"start\":54170},{\"end\":54198,\"start\":54184},{\"end\":54206,\"start\":54198},{\"end\":54527,\"start\":54515},{\"end\":54533,\"start\":54527},{\"end\":54765,\"start\":54752},{\"end\":54774,\"start\":54765},{\"end\":54790,\"start\":54774},{\"end\":54805,\"start\":54790},{\"end\":54820,\"start\":54805},{\"end\":54842,\"start\":54820},{\"end\":54865,\"start\":54842},{\"end\":55222,\"start\":55200},{\"end\":55238,\"start\":55222},{\"end\":55251,\"start\":55238},{\"end\":55256,\"start\":55251},{\"end\":55554,\"start\":55540},{\"end\":55567,\"start\":55554},{\"end\":55585,\"start\":55567},{\"end\":55595,\"start\":55585},{\"end\":55605,\"start\":55595},{\"end\":55623,\"start\":55605},{\"end\":55634,\"start\":55623},{\"end\":55929,\"start\":55913},{\"end\":55937,\"start\":55929},{\"end\":55950,\"start\":55937},{\"end\":55961,\"start\":55950},{\"end\":55974,\"start\":55961},{\"end\":55983,\"start\":55974},{\"end\":56269,\"start\":56258},{\"end\":56281,\"start\":56269},{\"end\":56293,\"start\":56281},{\"end\":56315,\"start\":56293},{\"end\":56331,\"start\":56315},{\"end\":56349,\"start\":56331},{\"end\":56363,\"start\":56349}]", "bib_venue": "[{\"end\":41268,\"start\":41235},{\"end\":41564,\"start\":41546},{\"end\":41828,\"start\":41795},{\"end\":42012,\"start\":41987},{\"end\":42233,\"start\":42227},{\"end\":42508,\"start\":42502},{\"end\":42757,\"start\":42749},{\"end\":43060,\"start\":43054},{\"end\":43427,\"start\":43394},{\"end\":43882,\"start\":43827},{\"end\":44333,\"start\":44321},{\"end\":44688,\"start\":44676},{\"end\":45052,\"start\":45042},{\"end\":45365,\"start\":45357},{\"end\":45644,\"start\":45614},{\"end\":45961,\"start\":45946},{\"end\":46377,\"start\":46362},{\"end\":46802,\"start\":46796},{\"end\":47193,\"start\":47179},{\"end\":47581,\"start\":47548},{\"end\":47953,\"start\":47934},{\"end\":48272,\"start\":48239},{\"end\":48608,\"start\":48600},{\"end\":48953,\"start\":48941},{\"end\":49313,\"start\":49298},{\"end\":49589,\"start\":49512},{\"end\":49935,\"start\":49929},{\"end\":50210,\"start\":50202},{\"end\":50624,\"start\":50569},{\"end\":50962,\"start\":50908},{\"end\":51290,\"start\":51284},{\"end\":51572,\"start\":51566},{\"end\":51867,\"start\":51855},{\"end\":52194,\"start\":52170},{\"end\":52712,\"start\":52681},{\"end\":53099,\"start\":53042},{\"end\":53380,\"start\":53374},{\"end\":53805,\"start\":53772},{\"end\":54239,\"start\":54206},{\"end\":54513,\"start\":54447},{\"end\":54873,\"start\":54865},{\"end\":55262,\"start\":55256},{\"end\":55538,\"start\":55451},{\"end\":55989,\"start\":55983},{\"end\":56370,\"start\":56363}]"}}}, "year": 2023, "month": 12, "day": 17}