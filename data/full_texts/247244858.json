{"id": 247244858, "updated": "2023-10-05 16:40:57.464", "metadata": {"title": "DiT: Self-supervised Pre-training for Document Image Transformer", "authors": "[{\"first\":\"Junlong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yiheng\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Tengchao\",\"last\":\"Lv\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Cui\",\"middle\":[]},{\"first\":\"Cha\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Furu\",\"last\":\"Wei\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose \\textbf{DiT}, a self-supervised pre-trained \\textbf{D}ocument \\textbf{I}mage \\textbf{T}ransformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 $\\rightarrow$ 92.69), document layout analysis (91.0 $\\rightarrow$ 94.9), table detection (94.23 $\\rightarrow$ 96.55) and text detection for OCR (93.07 $\\rightarrow$ 94.29). The code and pre-trained models are publicly available at \\url{https://aka.ms/msdit}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.02378", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/LiXL0ZW22", "doi": "10.1145/3503161.3547911"}}, "content": {"source": {"pdf_hash": "05a819f67543e1b4db37f541857406a19401f7f6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2203.02378v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d7703d62beeefc35359cbe42f09e14dafad3a494", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/05a819f67543e1b4db37f541857406a19401f7f6.txt", "contents": "\nDiT: Self-supervised Pre-training for Document Image Transformer\nOctober 10-14, 2022\n\nJunlong Li \nYiheng Xu t-yihengxu@microsoft.com \nLei Cui \nCha Zhang chazhang@microsoft.com \nFuru Wei fuwei@microsoft.com \nJunlong Li \nYiheng Xu \nTengchao Lv tengchaolv@microsoft.com \nLei Cui \nCha Zhang \nFuru Wei \nLei Cui \nFuru Wei \n\nShanghai Jiao Tong University Shanghai\nChina\n\n\nTengchao Lv\nMicrosoft Research Asia\nBeijingChina\n\n\nMicrosoft Research Asia\nBeijingChina\n\n\nMicrosoft Research Asia\nBeijingChina\n\n\nMicrosoft Research Asia\nBeijingChina\n\nDiT: Self-supervised Pre-training for Document Image Transformer\n\nProceedings of the 30th ACM International Conference on Multimedia (MM '22), MM '22\nthe 30th ACM International Conference on Multimedia (MM '22), MM '22Lisboa, Portugal2022October 10-14, 202210.1145/3503161.3547911Microsoft Azure AI Redmond, United States ACM Reference Format: * Contributions during internship at Microsoft Research Asia. Corresponding authors: October 10-14, 2022, Lisboa, Portugal. ACM, New York, NY, USA, 10 pages.CCS CONCEPTS \u2022 Computing methodologies \u2192 Computer vision KEYWORDS document image transformerself-supervised pre-trainingdocu- ment image classificationdocument layout analysistable detectiontext detectionOCR\nImage Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose DiT, a self-supervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the selfsupervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 \u2192 92.69), document layout analysis (91.0 \u2192 94.9), table detection (94.23 \u2192 96.55) and text detection for OCR (93.07 \u2192 94.29). The code and pre-trained models are publicly available at https://aka.ms/msdit.\n\nINTRODUCTION\n\nSelf-supervised pre-training techniques have been the de facto common practice for Document AI [10] in the past several years, where the image, text, and layout information is often jointly trained using a unified Transformer architecture [2,19,21,25,28,32,33,38,[41][42][43][44]48]. Among all these approaches, a typical pipeline for pretraining Document AI models usually start with the vision-based understanding such as Optical Character Recognition (OCR) or document layout analysis, which still heavily relies on the supervised computer vision backbone models with human-labeled training samples. Although good results have been achieved on benchmark datasets, these vision models are often confronted with the performance gap in real-world applications due to domain shift and template/format mismatch from the training data. Such accuracy regression [26,46] also has an essential influence on the pre-trained models as well as downstream tasks. Therefore, it is inevitable to investigate how to leverage the self-supervised pre-training for the backbone of document image understanding, which can better facilitate general Document AI models for different domains.\n\nImage Transformer [3,9,12,13,17,31,36,47] has recently achieved great success for natural image understanding including classification, detection and segmentation tasks, either with supervised pre-training on the ImageNet or self-supervised pretraining. The pre-trained image Transformer models can achieve comparable and even better performance compared with CNNbased pre-trained models under a similar parameter size. However, for document image understanding, there is no commonlyused large-scale human-labeled benchmark like ImageNet, which makes large-scale supervised pre-training impractical. Even though weakly supervised methods have been used to create Document AI benchmarks [26,27,45,46], the domain of these datasets is often from the academic papers that share similar templates and formats, which are different from real-world documents such as forms, invoice/receipts, reports, and many others as shown in Figure 1. This may lead to unsatisfactory results for general Document AI problems. Therefore, it is vital to pre-train the document image backbone models with large-scale unlabeled data from general domains, which can support a variety of Document AI tasks. To this end, we propose DiT, a self-supervised pre-trained Document Image Transformer model for general Document AI tasks, which does not rely on any human-labeled document images. Inspired by the recently proposed BEiT model [3], we adopt a similar pre-training strategy using document images. An input text image is first resized into 224 \u00d7 224 and then the image is split into a sequence of 16 \u00d7 16 patches which are used as the input to the image Transformer. Distinct from the BEiT model where visual tokens are from the discrete VAE in DALL-E [34], we re-train the discrete VAE (dVAE) model with large-scale document images, so that the generated visual tokens are more domain relevant to the Document AI tasks. The pre-training objective is to recover visual tokens from dVAE based on the corrupted input document images using the Masked Image Modeling (MIM) in BEiT. In this way, the DiT model does not rely on any human-labeled document images, but only leverages large-scale unlabeled data to learn the global patch relationship within each document image. We evaluate the pre-trained DiT models on four publicly available Document AI benchmarks, including the RVL-CDIP dataset [16] for document image classification, the PubLayNet dataset [46] for document layout analysis, the ICDAR 2019 cTDaR dataset [15] for table detection, as well as the FUNSD dataset [22] for OCR text detection. Experiment results have illustrated that the pre-trained DiT model has outperformed the existing supervised and self-supervised pre-trained models and achieved new state-of-the-art on these tasks.\n\nThe contributions of this paper are summarized as follows:\n\n( \n\n\nRELATED WORK\n\nImage Transformer has recently achieved significant progress in computer vision problems, including classification, object detection, and segmentation. [12] first applied a standard Transformer directly to images with the fewest modifications. They split an image into 16 \u00d7 16 patches and provide the sequence of linear embeddings of these patches as an input to a Transformer named ViT. The ViT model is trained on image classification in a supervised fashion and outperforms the ResNet baselines. [36] proposed data-efficient image transformers & distillation through attention, namely DeiT, which solely relies on the ImageNet dataset for supervised pretraining and achieves SOTA results compared with ViT. [31] proposed a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. In addition to supervised pre-trained models, [8] trained a sequence Transformer called iGPT to auto-regressively predict pixels without incorporating knowledge of the 2D input structure, which is the first attempt at self-supervised image transformer pre-training. After that, self-supervised pre-training for image Transformer became a hot topic in computer vision. [7] proposed DINO, which pre-trains the image Transformer using self-distillation with no labels. [9] proposed MoCov3 that is based on Siamese networks for self-supervised learning. More recently, [3] adopted a BERT-style pre-training strategy, which first tokenizes the original image into visual tokens, then randomly masks some image patches and feeds them into the backbone Transformer. Similar to the masked language modeling, they proposed a masked image modeling task as the pre-training objective that achieves SOTA performance. [47] presented a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage pipeline where the tokenizer is pre-trained beforehand.\n\nThe vision-based Document AI usually denote document analysis tasks that leverage the computer vision models, such as OCR, document layout analysis, and document image classification. Due to the lack of large-scale human-labeled datasets in this domain, existing approaches are usually based on the ConvNets models that are pre-trained with ImageNet/COCO datasets. Then, the models are continuously trained with task-specific labeled samples. To the best of our knowledge, the pre-trained DiT model is the first largescale self-supervised pre-trained model for vision-based Document AI tasks. Meanwhile, it can be further leveraged for the multimodal pre-training for Document AI.\n\n\nDOCUMENT IMAGE TRANSFORMER\n\nIn this section, we first present the architecture of DiT and the pre-training procedure. Then, we describe the application of DiT models in different downstream tasks.\n\n\nModel Architecture\n\nFollowing ViT [12], we use the vanilla Transformer architecture [37] as the backbone of DiT. We divide a document image into nonoverlapping patches and obtain a sequence of patch embeddings. After adding the 1d position embedding, these image patches are passed into a stack of Transformer blocks with multi-head attention. Finally, we take the output of the Transformer encoder as the representation of image patches, which is shown in Figure 2.\n\n\nPre-training\n\nInspired by BEiT [3], we use Masked Image Modeling (MIM) as our pre-training objective. In this procedure, the images are represented as image patches and visual tokens in two views respectively. During pre-training, DiT accepts the image patches as input and predicts the visual tokens with the output representation.\n\nLike text tokens in natural language, an image can be represented as a sequence of discrete tokens obtained by an image tokenizer. BEiT uses the discrete variational auto-encoder (dVAE) from DALL-E [34] as the image tokenizer, which is trained on a large data collection including 400 million images. However, there exists a domain mismatch between natural images and document images, which makes the DALL-E tokenizer not appropriate for the document images. Therefore, to get better discrete visual tokens for the document image domain, we train a dVAE on the IIT-CDIP [24] dataset that includes 42 million document images.\n\nTo effectively pre-train the DiT model, we randomly mask a subset of inputs with a special token [MASK] given a sequence of image patches. The DiT encoder embeds the masked patch sequence by a linear projection with added positional embeddings, and then contextualizes it with a stack of Transformer blocks. The model is required to predict the index of visual tokens with the output from masked positions. Instead of predicting the raw pixels, the masked image modeling task requires the model to predict the discrete visual tokens obtained by the image tokenizer.\n\n\nFine-tuning\n\nWe fine-tune our model on four Document AI benchmarks, including the RVL-CDIP dataset for document image classification, the PubLayNet dataset for document layout analysis, the ICDAR 2019 cTDaR dataset for table detection, and the FUNSD dataset for text detection. These benchmark datasets can be formalized as two common tasks: image classification and object detection.\n\nImage Classification. For image classification, we use average pooling to aggregate the representation of image patches. Next, we pass the global representation into a simple linear classifier.\n\nObject Detection. For object detection, as in Figure 3, we leverage Mask R-CNN [18] and Cascade R-CNN [5] as detection frameworks and use ViT-based models as the backbone. Our code is implemented based on Detectron2 [39]. Following [14,29], we use resolutionmodifying modules at four different transformer blocks to adapt the single-scale ViT to the multi-scale FPN. Let be the total number of blocks, the 1 /3th block is upsampled by 4\u00d7 using a module with 2 stride-two 2\u00d72 transposed convolution. For the output of the 1 /2th block, we use a single stride-two 2\u00d72 transposed convolution to upsample 2\u00d7. The output of the 2 /3th block is utilized without additional operations. Finally, the output of 3 /3th block is downsampled by 2\u00d7 with stride-two 2\u00d72 max pooling.\n\n\nEXPERIMENTS 4.1 Tasks\n\nWe briefly introduce the datasets mentioned in section 3.3 here.\n\nRVL-CDIP. The RVL-CDIP [16] dataset consists of 400,000 grayscale images in 16 classes, with 25,000 per class. There are 320,000 training images, 40,000 validation images, and 40,000 test images. The 16 classes include {letter, form, email, handwritten, advertisement, scientific report, scientific publication, specification, file folder, news article, budget, invoice, presentation, questionnaire, resume, memo}. The evaluation metric is the overall classification accuracy.  Figure 3: Illustration of applying DiT as the backbone network in different detection frameworks.\n\nPubLayNet. PubLayNet [46] is a large-scale document layout analysis dataset. More than 360,000 document images are constructed by automatically parsing PubMed XML files. The resulting annotations cover typical document layout elements such as text, title, list, figure, and table. The model needs to detect the regions of the assigned elements. We use the category-wise and overall mean average precision (MAP) @ intersection over union (IOU) [0.50:0.95] of bounding boxes as the evaluation metrics.\n\nICDAR 2019 cTDaR. The cTDaR datasets [15] consist of two tracks, including table detection and table structure recognition. In this paper, we focus on Track A where document images with one or several table annotations are provided. This dataset has two subsets, one for archival documents and the other for modern documents. The archival subset includes 600 training images and 199 testing images, which shows a wide variety of tables containing hand-drawn accounting books, stock exchange lists, train timetables, production census, etc. The modern subset consists of 600 training images and 240 testing images, which contain different kinds of PDF files, such as scientific journals, forms, financial statements, etc. The dataset contains Chinese and English documents in various formats, including scanned document images and borndigital formats. Metrics for evaluating this task are the precision, recall, and F1 scores computed from the model's ranked output w.r.t. different Intersection over Union (IoU) threshold. We calculate the values with IoU thresholds of 0.6, 0.7, 0.8, and 0.9 respectively, and merge them into a final weighted F1 score: 1 = 0.6 1 0.6 + 0.7 1 0.7 + 0.8 1 0.8 + 0.9 1 0.9 0.6 + 0.7 + 0.8 + 0.9 This task further requires models to combine the modern and archival set as a whole to get a final evaluation result.\n\nFUNSD. FUNSD [22] is a noisy scanned document dataset labeled for three tasks: Text detection, Text recognition with Optical Character Recognition (OCR), and Form understanding. In this paper, we focus on Task #1 in FUNSD, which aims to detect the text bounding boxes for scanned form documents. FUNSD includes 199 fully annotated forms with 31,485 words, whereas the training set contains 150 forms and the testing set includes 49 forms. The evaluation metrics are the precision, recall, and F1 score at IoU@0.5.\n\n\nSettings\n\nPre-training Setup. We pre-train DiT on the IIT-CDIP Test Collection 1.0 [24]. We pre-process the dataset by splitting multi-page documents into single pages, and obtain 42 million document images. We also introduce random resized cropping to augment training data during training. We train our DiT-B model with the same architecture as the ViT base: a 12-layer Transformer with 768 hidden sizes, and 12 attention heads. The intermediate size of feed-forward networks is 3,072. A larger version, DiT-L, is also trained with 24 layers, 1,024 hidden sizes, and 16 attention heads. The intermediate size of feed-forward networks is 4,096.\n\nThe dVAE Tokenizer. BEiT borrows the image tokenizer trained by DALL-E, which is not aligned with the document image data. In this case, we fully utilize the 42 million document images in the IIT-CDIP dataset and train a document dVAE image tokenizer to obtain the visual tokens. Like the DALL-E image tokenizer, the document image tokenizer has the codebook dimensionality of 8,192 and the image encoder with three layers. Each layer consists of a 2D convolution with a stride of 2 and a ResNet block. Therefore, the tokenizer eventually has a downsampling factor of 8. In this Raw Document Image Adaptively Binarized Image case, given a 112\u00d7112 image, it ends up with a 14\u00d714 discrete token map aligning with the 14\u00d714 input patches.\n\nWe implement our dVAE codebase from open-sourced DALL-E implementation 1 and train the dVAE model with the entire IIT-CDIP dataset containing 42 million document images. The new dVAE tokenizer is trained with a combination of a MSE loss to reconstructe the input image, and a perplexity loss to increase the use of the quantized codebook representations. The input image size is 224\u00d7224, and we train the tokenizer with a learning rate of 5e-4 and a minimum temperature of 1e-10 for 3 epochs. We compare our dVAE tokenizer with the original DALL-E tokenizer by reconstructing the document image samples from downstream tasks, which is shown in Figure 4. We sample images from the document layout analysis dataset PubLayNet and table detection dataset ICDAR 2019 cTDaR. After being reconstructed by the DALL-E and our tokenizer, the image tokenizer by DALL-E is hard to distinguish the border of lines and tokens, but the image tokenizer by our dVAE is closer to the original image and the border is sharper and clearer. We confirm that a better tokenizer can produce more accurate tokens that better describe the original images.\n\nEquipped with the pre-training data and image tokenizer, we pre-train DiT for 500K steps with a batch size of 2,048, a learning rate of 1e-3, warmup steps of 10K, and weight decay of 0.05. The 1 and 2 of Adam [23] optimizer are 0.9 and 0.999 respectively. We employ stochastic depth [20] with a 0.1 rate and disable dropout as in BEiT pre-training. We also apply blockwise masking in the pre-training of DiT with 40% patches masked as BEiT.\n\nFine-tuning on RVL-CDIP. We evaluate the pre-trained DiT models and other image backbones on RVL-CDIP for document image classification. We fine-tune the image transformers for 90 epochs with a batch size of 128 and a learning rate of 1e-3. For all settings, we resize the original images to 224 \u00d7 224 with the RandomResized-Crop operation.   tasks is much larger than classification, we limit the batch size to 16. The learning rate is 1e-4 and 5e-5 for archival and modern subsets respectively. In the preliminary experiments, we found that directly using the raw images in the archival subset leads to suboptimal performance when fine-tuning DiT, so we apply an adaptive image binarization algorithm implemented by OpenCV [4] to binarize the images. An example of the pre-procession is shown in Figure 5. During training, we apply the data augmentation method used in DETR [6] as a multi-scale training strategy. Specifically, the input image is cropped with probability 0.5 to a random rectangular patch which is then resized again such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1,333.\n\n\nFine\n\nFine-tuning on PubLayNet. We evaluate the pre-trained DiT models and other image backbones on the PubLayNet dataset for document layout analysis. Similar to the ICDAR 2019 cTDaR dataset, the batch size is 16, and the learning rate is 4e-4 for the base version. and 1e-4 for the large version. The data augmentation method for DETR [6] is also used.  Fine-tuning on FUNSD. We use the same object detection framework for fine-tuning the pre-trained DiT models and other backbones on the text detection task in FUNSD. In document layout analysis and table detection, we use anchor box sizes [32,64,128,256,512] in the detection process since the detected areas are usually paragraph-level. Different from document layout analysis, text detection aims to locate smaller objects at the word level in document images. Therefore, we use anchor box sizes [4,8,16,32,64] in the detection process. The batch size is set to 16 and the learning rate is 1e-4 for the base model and 5e-5 for the large model.\n\nThe image backbone models selected as baselines have a comparable number of parameters compared with our DiT-B. They include the following two kinds: CNN and image Transformer. For CNN-based models, we choose ResNext101-32\u00d78d [40]. For image Transformers, we choose the base version of DeiT [36], BEiT [3] and MAE [17] which are pre-trained on ImageNet-1K dataset with a 224\u00d7224 input size. We rerun the fine-tuning of all baselines.\n\n\nResults\n\n\nRVL-CDIP.\n\nThe results of document image classification on RVL-CDIP are shown in Table 1. To make a fair comparison, the approaches in the table use only image information from the dataset. DiT-B performs significantly better than all selected single-model baselines. Since DiT shares the same model structure with other image Transformer baselines, the higher score indicates the effectiveness of our document-specific pre-training strategy. The larger version, DiT-L, gets a comparable score with the previous SOTA ensemble model under the single-model setting, which further highlights its modeling capability on document images.\n\nPubLayNet. The results of document layout analysis on Pub-LayNet are shown in Table 2. Since this task has a large number of training and testing samples and requires a comprehensive analysis of the common document elements, it clearly demonstrates the learning ability of different image Transformer models. It is observed that the DeiT-B, BEiT-B, and MAE-B are obviously better than ResNeXt-101, and DiT-B is even stronger than these powerful image Transformer baselines. According to the results, the improvement mainly comes from the List and Figure category, and on the basis of DiT-B, DiT-L gives out a much higher mAP score. We also investigate the impact of different object detection algorithms, and the results show that a more advanced detection algorithm (Cascade R-CNN in our case) can push the model performance to a higher level. We also apply Cascade R-CNN on the ResNeXt-101-32\u00d78d baseline, and DiT surpasses it by 1% and 1.4% absolute score for the base and large settings respectively, indicating the superiority of DiT on a different detection framework.\n\nICDAR 2019 cTDaR. The results of table detection on ICDAR 2019 cTDaR dataset are shown in Table 3. The size of this dataset is relatively small, so it aims at evaluating the few-shot learning capability of models under a low-resource scenario. We first analyze the model performance on the archival and modern subsets separately. In Table 3b, DiT surpasses all the baselines except BEiT for the archival subset. This is because in the pre-training of BEiT, it directly uses the DALL-E dVAE which is trained on an extremely large dataset with 400M images with different colors. While for DiT, the image tokenizer is trained with grayscale images, which may not be sufficient for historical document images with colors. The improvement when switching from Mask R-CNN to Cascade R-CNN is also observed which is similar to PubLayNet settings, and DiT still outperforms other baselines significantly. The conclusion is similar to the results on the modern subset in Table 3c. We further combine the predictions of the two subsets into a single set. The results in 3a show DiT-L achieves the highest wF1 score among all Mask R-CNN methods, demonstrating the versatility of DiT under different categories of documents. It is worth noting that the metrics of IoU@0.9 are significantly better, which means DiT has a better fine-grained object detection capability. Under all the three settings, we have pushed the SOTA results to a new level by more than 2% (94.23\u219296.55) absolute wF1 score with our best model and the Cascade R-CNN detection algorithm.  has been a long-standing real-world problem, we obtain the wordlevel text detection results from a popular commercial OCR engine to set a high-level baseline. In addition, DBNet [30] is a widely used text detection model for online OCR engines, we also fine-tune a pre-trained DBNet model with FUNSD training data and evaluate its accuracy. Both of them achieve around 0.85 F1 scores for IoU@0.5. Next, we use the Mask R-CNN framework to compare different backbone networks (CNN and ViT) including ResNeXt-101, DeiT, BEiT, MAE, and DiT. It is shown that CNN-based and ViT-based text detection models outperform the baselines significantly due to advanced model design and more parameters. We also observe that the DiT models achieve new SOTA results compared with other models. Finally, we further train the DiT models with a synthetic dataset that contains 1 million document images, leading to an F1 of 0.9429 being achieved by the DiT-L model.\n\n\nFUNSD (Text Detection\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this paper, we present DiT, a self-supervised foundation model for general Document AI tasks. The DiT model is pre-trained with largescale unlabeled document images that cover a variety of templates and formats, which is ideal for downstream Document AI tasks in different domains. We evaluate the pre-trained DiT on several vision-based Document AI benchmarks, including table detection, document layout analysis, document image classification, and text detection. Experimental results have shown that DiT outperforms several strong baselines across the board and achieves new SOTA performance. We will make the pre-trained DiT models publicly available to facilitate the Document AI research. For future research, we will pre-train DiT with a much larger dataset to further push the SOTA results in Document AI. Meanwhile, we will also integrate DiT as the foundation model in multimodal pre-training for visually-rich document understanding such as the next-gen layout-based models like LayoutLM, where a unified Transformer-based architecture may be sufficient for both CV and NLP applications in Document AI.\n\nFigure 1 :\n1Visually-rich business documents with different layouts and formats for pre-training DiT.\n\nFigure 2 :\n2The model architecture of DiT with MIM pre-training.\n\nFigure 4 :\n4Document image reconstruction with different tokenizers. From left to right: the original document image, image reconstruction using the self-trained dVAE tokenizer, image reconstruction using the DALL-E tokenizer.\n\nFigure 5 :\n5An example of pre-processing with adaptive image binarization on the ICDAR 2019 cTDaR archival subset.\n\n\n1) We propose DiT, a self-supervised pre-trained document image Transformer model, which can leverage large-scale unlabeled document images for pre-training.(2) We leverage the pre-trained DiT models as the backbone for The code and pre-trained models are publicly available at https://aka.ms/msdit.a variety of Document AI tasks, including document image \nclassification, document layout analysis, table detection, as \nwell as text detection for OCR, and achieve new state-of-the-\nart results. \n(3) \n\n\n-tuning on ICDAR 2019 cTDaR. We evaluate the pre-trained DiT models and other image backbones on the ICDAR 2019 dataset for table detection. Since the image resolution for object detection 1 https://github.com/lucidrains/DALLE-pytorchModel \nType \nAccuracy #Param \n\n[1] \nSingle \n90.97 \n-\n[11] \nSingle \n91.11 \n-\n[11] \nEnsemble \n92.21 \n-\n[35] \nEnsemble \n92.77 \n-\n\nResNext-101-32\u00d78d \nSingle \n90.65 \n88M \nDeiT-B [36] \nSingle \n90.32 \n87M \nBEiT-B [3] \nSingle \n91.09 \n87M \nMAE-B [17] \nSingle \n91.42 \n87M \n\nDiT-B \nSingle \n92.11 \n87M \nDiT-L \nSingle \n92.69 \n304M \n\nTable 1: Document Image Classification accuracy (%) on RVL-\nCDIP, where all the models use the pure image information \n(w/o text information) with the 224\u00d7224 resolution. \n\nModel \nText Title List Table Figure Overall \n\n\n\nTable 2 :\n2Document Layout Analysis mAP @ IOU [0.50:0.95] on PubLayNet validation set. ResNext-101-32\u00d78d is shortened as ResNext and Cascade as C.\n\nTable 3 :\n3Table detection accuracy (F1) on ICDAR 2019 cTDaR.\n\n\n). The results of text detection on the FUNSD dataset are shown inTable 4. Since text detection for OCRModel \nPrecision Recall \nF1 \n\nFaster R-CNN [22] \n0.704 \n0.848 \n0.76 \nDBNet [30] \n0.8764 \n0.8400 0.8578 \nA Commercial OCR Engine \n0.8762 \n0.8260 0.8504 \n\nResNeXt-101-32\u00d78d \n0.9387 \n0.9229 0.9307 \nDeiT-B \n0.9429 \n0.9237 0.9332 \nBEiT-B \n0.9412 \n0.9263 0.9337 \nMAE-B \n0.9441 \n0.9321 0.9381 \n\nDiT-B \n0.9470 \n0.9307 0.9388 \nDiT-L \n0.9452 \n0.9336 0.9393 \n\nDiT-B (+syn) \n0.9539 \n0.9315 0.9425 \nDiT-L (+syn) \n0.9543 \n0.9317 0.9429 \n\n\n\nTable 4 :\n4Text detection accuracy (IoU@0.5) on FUNSD Task #1, where Mask R-CNN is used with different backbones (ResNeXt, DeiT, BEiT, MAE and DiT). \"+syn\" denotes that DiT is trained with a synthetic dataset including 1M document images, then fine-tuned with the FUNSD training data.\n\nCutting the Error by Half: Investigation of Very Deep CNN and Advanced Training Strategies for Document Image Classification. Andreas Muhammad Zeshan Afzal, Sheraz K\u00f6lsch, Marcus Ahmed, Liwicki, 14th IAPR International Conference on Document Analysis and Recognition (ICDAR). Muhammad Zeshan Afzal, Andreas K\u00f6lsch, Sheraz Ahmed, and Marcus Liwicki. 2017. Cutting the Error by Half: Investigation of Very Deep CNN and Advanced Training Strategies for Document Image Classification. 2017 14th IAPR Inter- national Conference on Document Analysis and Recognition (ICDAR) 01 (2017), 883-888.\n\nDocFormer: End-to-End Transformer for Document Understanding. Srikar Appalaraju, Bhavan Jasani, Yusheng Bhargava Urala Kota, R Xie, Manmatha, arXiv:2106.11539cs.CVSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Man- matha. 2021. DocFormer: End-to-End Transformer for Document Understanding. arXiv:2106.11539 [cs.CV]\n\nHangbo Bao, Li Dong, Furu Wei, arXiv:2106.08254BEiT: BERT Pre-Training of Image Transformers. cs.CVHangbo Bao, Li Dong, and Furu Wei. 2021. BEiT: BERT Pre-Training of Image Transformers. arXiv:2106.08254 [cs.CV]\n\nThe OpenCV Library. G Bradski, Dr. Dobb's Journal of Software Tools. G. Bradski. 2000. The OpenCV Library. Dr. Dobb's Journal of Software Tools (2000).\n\nCascade R-CNN: Delving Into High Quality Object Detection. Zhaowei Cai, Nuno Vasconcelos, 10.1109/CVPR.2018.006442018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAIEEE Computer SocietyZhaowei Cai and Nuno Vasconcelos. 2018. Cascade R-CNN: Delving Into High Quality Object Detection. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. IEEE Computer Society, 6154-6162. https://doi.org/10.1109/CVPR.2018.00644\n\nEnd-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European conference on computer vision. SpringerNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan- der Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In European conference on computer vision. Springer, 213-229.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, arXiv:2104.14294Piotr Bojanowski, and Armand Joulin. 2021. Emerging Properties in Self-Supervised Vision Transformers. cs.CVMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. Emerging Properties in Self-Supervised Vision Transformers. arXiv:2104.14294 [cs.CV]\n\nGenerative Pretraining From Pixels. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever, PMLRProceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020Virtual Event (Proceedings of Machine Learning ResearchMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. 2020. Generative Pretraining From Pixels. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119). PMLR, 1691-1703. http://proceedings.mlr.press/v119/chen20s.html\n\nAn Empirical Study of Training Self-Supervised Vision Transformers. Xinlei Chen, Saining Xie, Kaiming He, arXiv:2104.02057cs.CVXinlei Chen, Saining Xie, and Kaiming He. 2021. An Empirical Study of Training Self-Supervised Vision Transformers. arXiv:2104.02057 [cs.CV]\n\nLei Cui, Yiheng Xu, Tengchao Lv, Furu Wei, arXiv:2111.08609Document AI: Benchmarks, Models and Applications. cs.CLLei Cui, Yiheng Xu, Tengchao Lv, and Furu Wei. 2021. Document AI: Benchmarks, Models and Applications. arXiv:2111.08609 [cs.CL]\n\nDocument Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks. Arindam Das, Saikat Roy, Ujjwal Bhattacharya, 24th International Conference on Pattern Recognition (ICPR). Arindam Das, Saikat Roy, and Ujjwal Bhattacharya. 2018. Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks. 2018 24th International Conference on Pattern Recognition (ICPR) (2018), 3180-3185.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR (2021).\n\nAlaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, arXiv:2106.09681Gabriel Synnaeve, Jakob Verbeek, and Herv\u00e9 Jegou. 2021. XCiT: Cross-Covariance Image Transformers. cs.CVAlaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, and Herv\u00e9 Jegou. 2021. XCiT: Cross-Covariance Image Transformers. arXiv:2106.09681 [cs.CV]\n\nAlaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, ArXiv abs/2106.09681Gabriel Synnaeve, Jakob Verbeek, and Herv\u00e9 J\u00e9gou. 2021. XCiT: Cross-Covariance Image Transformers. Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, and Herv\u00e9 J\u00e9gou. 2021. XCiT: Cross-Covariance Image Transformers. ArXiv abs/2106.09681 (2021).\n\nICDAR 2019 Competition on Table Detection and Recognition (cTDaR). Liangcai Gao, Yilun Huang, Herv\u00e9 D\u00e9jean, Jean-Luc Meunier, Qinqin Yan, Yu Fang, Florian Kleber, Eva Lang, 10.1109/ICDAR.2019.002432019 International Conference on Document Analysis and Recognition (ICDAR). 1510-1515. Liangcai Gao, Yilun Huang, Herv\u00e9 D\u00e9jean, Jean-Luc Meunier, Qinqin Yan, Yu Fang, Florian Kleber, and Eva Lang. 2019. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR). In 2019 International Conference on Document Analysis and Recognition (ICDAR). 1510-1515. https://doi.org/10.1109/ICDAR.2019.00243\n\nEvaluation of Deep Convolutional Nets for Document Image Classification and Retrieval. W Adam, Alex Harley, Konstantinos G Ufkes, Derpanis, International Conference on Document Analysis and Recognition (ICDAR). Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis. 2015. Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval. In International Conference on Document Analysis and Recognition (ICDAR).\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, arXiv:2111.06377Piotr Doll\u00e1r, and Ross Girshick. 2021. Masked Autoencoders Are Scalable Vision Learners. cs.CVKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. 2021. Masked Autoencoders Are Scalable Vision Learners. arXiv:2111.06377 [cs.CV]\n\nMask R-CNN. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross B Girshick, 10.1109/ICCV.2017.322IEEE International Conference on Computer Vision. Venice, ItalyIEEE Computer SocietyKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. 2017. Mask R-CNN. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. IEEE Computer Society, 2980-2988. https://doi.org/10. 1109/ICCV.2017.322\n\nDaehyun Nam, and Sungrae Park. 2021. BROS: A Pre-trained Language Model for Understanding Texts in Document. Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and Sungrae Park. 2021. BROS: A Pre-trained Language Model for Understanding Texts in Document. https://openreview.net/forum?id=punMXQEsPr0\n\nDeep Networks with Stochastic Depth. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q Weinberger, ECCV. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. 2016. Deep Networks with Stochastic Depth. In ECCV.\n\nLayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei, MM '22: The 30th ACM International Conference on Multimedia. Lisbon,PortugalYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. In MM '22: The 30th ACM International Conference on Multimedia, Lisbon,Portugal, October 10-14, 2022.\n\nFUNSD: A Dataset for Form Understanding in Noisy Scanned Documents. Guillaume Jaume, Jean-Philippe Hazim Kemal Ekenel, Thiran, International Conference on Document Analysis and Recognition Workshops (ICDARW). 2Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. 2019. FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents. 2019 Interna- tional Conference on Document Analysis and Recognition Workshops (ICDARW) 2 (2019), 1-6.\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. Bengio and Yann LeCunSan Diego, CA, USAConference Track ProceedingsDiederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti- mization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980\n\nBuilding a Test Collection for Complex Document Information Processing. D Lewis, G Agam, S Argamon, O Frieder, D Grossman, J Heard, Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. the 29th Annual International ACM SIGIR Conference on Research and Development in Information RetrievalSeattle, Washington, USASIGIR '06D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard. 2006. Building a Test Collection for Complex Document Information Processing. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Seattle, Washington, USA) (SIGIR '06).\n\n. 10.1145/1148170.1148307ACMNew York, NY, USAACM, New York, NY, USA, 665-666. https://doi.org/10.1145/1148170.1148307\n\nStructuralLM: Structural Pre-training for Form Understanding. Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, Luo Si, 10.18653/v1/2021.acl-long.493Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2021. StructuralLM: Structural Pre-training for Form Understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Online, 6309- 6318. https://doi.org/10.18653/v1/2021.acl-long.493\n\nTableBank: Table Benchmark for Image-based Table Detection and Recognition. Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, Zhoujun Li, Proceedings of the 12th Language Resources and Evaluation Conference. European Language Resources Association. the 12th Language Resources and Evaluation Conference. European Language Resources AssociationMarseille, FranceMinghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. 2020. TableBank: Table Benchmark for Image-based Table Detection and Recog- nition. In Proceedings of the 12th Language Resources and Evaluation Confer- ence. European Language Resources Association, Marseille, France, 1918-1925. https://aclanthology.org/2020.lrec-1.236\n\nDocBank: A Benchmark Dataset for Document Layout Analysis. Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, Ming Zhou, 10.18653/v1/2020.coling-main.82Proceedings of the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics. the 28th International Conference on Computational Linguistics. International Committee on Computational LinguisticsBarcelona, Spain (OnlineMinghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. 2020. DocBank: A Benchmark Dataset for Document Layout Analysis. In Proceedings of the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), 949-960. https://doi.org/10.18653/v1/2020.coling-main.82\n\nPeizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv Jain, arXiv:2106.03331Varun Manjunatha, and Hongfu Liu. 2021. SelfDoc: Self-Supervised Document Representation Learning. cs.CVPeizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I. Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, and Hongfu Liu. 2021. SelfDoc: Self-Supervised Document Representation Learning. arXiv:2106.03331 [cs.CV]\n\nBenchmarking Detection Transfer Learning with Vision Transformers. Yanghao Li, Saining Xie, Xinlei Chen, Piotr Doll\u00e1r, Kaiming He, Ross B Girshick, ArXiv abs/2111.11429Yanghao Li, Saining Xie, Xinlei Chen, Piotr Doll\u00e1r, Kaiming He, and Ross B. Girshick. 2021. Benchmarking Detection Transfer Learning with Vision Trans- formers. ArXiv abs/2111.11429 (2021).\n\nMinghui Liao, Zhisheng Zou, Zhaoyi Wan, Cong Yao, Xiang Bai, arXiv:2202.10304Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion. cs.CVMinghui Liao, Zhisheng Zou, Zhaoyi Wan, Cong Yao, and Xiang Bai. 2022. Real- Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion. arXiv:2202.10304 [cs.CV]\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, arXiv:2103.14030Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. cs.CVZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv:2103.14030 [cs.CV]\n\nRafa\u0142 Powalski, \u0141ukasz Borchmann, Dawid Jurkiewicz, arXiv:2102.09550Tomasz Dwojak, Micha\u0142 Pietruszka, and Gabriela Pa\u0142ka. 2021. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. cs.CLRafa\u0142 Powalski, \u0141ukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Micha\u0142 Pietruszka, and Gabriela Pa\u0142ka. 2021. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. arXiv:2102.09550 [cs.CL]\n\nTowards a Multi-modal. Subhojeet Pramanik, Shashank Mujumdar, Hima Patel, arXiv:2009.14457Multi-task Learning based Pre-training Framework for Document Representation Learning. cs.CLSubhojeet Pramanik, Shashank Mujumdar, and Hima Patel. 2020. Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning. arXiv:2009.14457 [cs.CL]\n\n. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, arXiv:2102.120922021. Zero-Shot Text-to-Image Generationcs.CVAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad- ford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. arXiv:2102.12092 [cs.CV]\n\nDeterministic Routing between Layout Abstractions for Multi-Scale Classification of Visually Rich Documents. Ritesh Sarkhel, Arnab Nandi, 10.24963/ijcai.2019/466Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019Macao, Chinaijcai.orgRitesh Sarkhel and Arnab Nandi. 2019. Deterministic Routing between Lay- out Abstractions for Multi-Scale Classification of Visually Rich Documents. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intel- ligence, IJCAI 2019, Macao, China, August 10-16, 2019, Sarit Kraus (Ed.). ijcai.org, 3360-3366. https://doi.org/10.24963/ijcai.2019/466\n\nAlexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. 2021. Training data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, PMLRInternational Conference on Machine Learning. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. 2021. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning. PMLR, 10347-10357.\n\nAttention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USA, Isabelle GuyonAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: An- nual Conference on Neural Information Processing Systems 2017, December 4- 9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998-6008. https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n\nTe-Lin Wu, Cheng Li, Mingyang Zhang, Tao Chen, arXiv:2104.08405Spurthi Amba Hombaiah, and Michael Bendersky. 2021. LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding. cs.CLTe-Lin Wu, Cheng Li, Mingyang Zhang, Tao Chen, Spurthi Amba Hombaiah, and Michael Bendersky. 2021. LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding. arXiv:2104.08405 [cs.CL]\n\n. Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick, Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. 2019. Detectron2. https://github.com/facebookresearch/detectron2.\n\nAggregated Residual Transformations for Deep Neural Networks. Saining Xie, Ross B Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, 10.1109/CVPR.2017.6342017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAIEEE Computer SocietySaining Xie, Ross B. Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. 2017. Aggregated Residual Transformations for Deep Neural Networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. IEEE Computer Society, 5987-5995. https://doi.org/10. 1109/CVPR.2017.634\n\nLayoutLM: Pre-training of Text and Layout for Document Image Understanding. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, https:/dl.acm.org/doi/10.1145/3394486.3403172KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya PrakashVirtual Event, CA, USAACMYiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020. LayoutLM: Pre-training of Text and Layout for Document Image Understanding. In KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (Eds.). ACM, 1192-1200. https://dl.acm.org/doi/10. 1145/3394486.3403172\n\nYiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei, arXiv:2104.08836LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. cs.CLYiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, and Furu Wei. 2021. LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. arXiv:2104.08836 [cs.CL]\n\nXFUND: A Benchmark Dataset for Multilingual Visually Rich Form Understanding. Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei, 10.18653/v1/2022.findings-acl.253Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsYiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, and Furu Wei. 2022. XFUND: A Benchmark Dataset for Multilingual Vi- sually Rich Form Understanding. In Findings of the Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics, Dublin, Ireland, 3214-3224. https://doi.org/10.18653/v1/2022.findings-acl.253\n\nLayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou, 10.18653/v1/2021.acl-long.201Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. 2021. LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Un- derstanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Online, 2579-2591. https://doi.org/10.18653/v1/2021.acl-long.201\n\nImage-based table recognition: data, model, and evaluation. Xu Zhong, Elaheh Shafieibavani, Antonio Jimeno Yepes, arXiv:1911.10683cs.CVXu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. 2020. Image-based table recognition: data, model, and evaluation. arXiv:1911.10683 [cs.CV]\n\nPubLayNet: largest dataset ever for document layout analysis. Xu Zhong, Jianbin Tang, Antonio Jimeno Yepes, 10.1109/ICDAR.2019.001662019 International Conference on Document Analysis and Recognition (ICDAR). IEEEXu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019. PubLayNet: largest dataset ever for document layout analysis. In 2019 International Conference on Document Analysis and Recognition (ICDAR). IEEE, 1015-1022. https://doi.org/ 10.1109/ICDAR.2019.00166\n\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, Tao Kong, arXiv:2111.07832iBOT: Image BERT Pre-Training with Online Tokenizer. cs.CVJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. 2021. iBOT: Image BERT Pre-Training with Online Tokenizer. arXiv:2111.07832 [cs.CV]\n\n\u0141ukasz Garncarek, Rafa\u0142 Powalski, Tomasz Stanis\u0142awek, Bartosz Topolski, Piotr Halama, arXiv:2002.08087Micha\u0142 Turski, and Filip Grali\u0144ski. 2021. LAMBERT: Layout-Aware (Language) Modeling for information extraction. cs.CL\u0141ukasz Garncarek, Rafa\u0142 Powalski, Tomasz Stanis\u0142awek, Bartosz Topolski, Piotr Halama, Micha\u0142 Turski, and Filip Grali\u0144ski. 2021. LAMBERT: Layout-Aware (Language) Modeling for information extraction. arXiv:2002.08087 [cs.CL]\n", "annotations": {"author": "[{\"end\":98,\"start\":87},{\"end\":134,\"start\":99},{\"end\":143,\"start\":135},{\"end\":177,\"start\":144},{\"end\":207,\"start\":178},{\"end\":219,\"start\":208},{\"end\":230,\"start\":220},{\"end\":268,\"start\":231},{\"end\":277,\"start\":269},{\"end\":288,\"start\":278},{\"end\":298,\"start\":289},{\"end\":307,\"start\":299},{\"end\":317,\"start\":308},{\"end\":364,\"start\":318},{\"end\":415,\"start\":365},{\"end\":454,\"start\":416},{\"end\":493,\"start\":455},{\"end\":532,\"start\":494}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":95},{\"end\":108,\"start\":106},{\"end\":142,\"start\":139},{\"end\":153,\"start\":148},{\"end\":186,\"start\":183},{\"end\":218,\"start\":216},{\"end\":229,\"start\":227},{\"end\":242,\"start\":240},{\"end\":276,\"start\":273},{\"end\":287,\"start\":282},{\"end\":306,\"start\":303}]", "author_first_name": "[{\"end\":94,\"start\":87},{\"end\":105,\"start\":99},{\"end\":138,\"start\":135},{\"end\":147,\"start\":144},{\"end\":182,\"start\":178},{\"end\":215,\"start\":208},{\"end\":226,\"start\":220},{\"end\":239,\"start\":231},{\"end\":272,\"start\":269},{\"end\":281,\"start\":278},{\"end\":293,\"start\":289},{\"end\":297,\"start\":294},{\"end\":302,\"start\":299},{\"end\":312,\"start\":308},{\"end\":316,\"start\":313}]", "author_affiliation": "[{\"end\":363,\"start\":319},{\"end\":414,\"start\":366},{\"end\":453,\"start\":417},{\"end\":492,\"start\":456},{\"end\":531,\"start\":495}]", "title": "[{\"end\":65,\"start\":1},{\"end\":597,\"start\":533}]", "venue": "[{\"end\":682,\"start\":599}]", "abstract": "[{\"end\":2310,\"start\":1242}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2425,\"start\":2421},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2568,\"start\":2565},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2571,\"start\":2568},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2574,\"start\":2571},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2577,\"start\":2574},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2580,\"start\":2577},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2583,\"start\":2580},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2586,\"start\":2583},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2589,\"start\":2586},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2593,\"start\":2589},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2597,\"start\":2593},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2601,\"start\":2597},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2605,\"start\":2601},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2608,\"start\":2605},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3188,\"start\":3184},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3191,\"start\":3188},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3521,\"start\":3518},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3523,\"start\":3521},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3526,\"start\":3523},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3529,\"start\":3526},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3532,\"start\":3529},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3535,\"start\":3532},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3538,\"start\":3535},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3540,\"start\":3538},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4190,\"start\":4186},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4193,\"start\":4190},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4196,\"start\":4193},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4199,\"start\":4196},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4910,\"start\":4907},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5234,\"start\":5230},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5873,\"start\":5869},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5935,\"start\":5931},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5999,\"start\":5995},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6054,\"start\":6050},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6512,\"start\":6508},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6859,\"start\":6855},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7070,\"start\":7066},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7379,\"start\":7376},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7701,\"start\":7698},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7799,\"start\":7796},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8239,\"start\":8235},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9419,\"start\":9415},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9469,\"start\":9465},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9884,\"start\":9881},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10386,\"start\":10382},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10758,\"start\":10754},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12042,\"start\":12038},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12064,\"start\":12061},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12179,\"start\":12175},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12195,\"start\":12191},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12198,\"start\":12195},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12846,\"start\":12842},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13421,\"start\":13417},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13938,\"start\":13934},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15259,\"start\":15255},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15845,\"start\":15841},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16784,\"start\":16782},{\"end\":16787,\"start\":16784},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17214,\"start\":17213},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18486,\"start\":18482},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18560,\"start\":18556},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19443,\"start\":19440},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19594,\"start\":19591},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20193,\"start\":20190},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20451,\"start\":20447},{\"end\":20454,\"start\":20451},{\"end\":20458,\"start\":20454},{\"end\":20462,\"start\":20458},{\"end\":20466,\"start\":20462},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20709,\"start\":20706},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20711,\"start\":20709},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20714,\"start\":20711},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20717,\"start\":20714},{\"end\":20720,\"start\":20717},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21085,\"start\":21081},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21150,\"start\":21146},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21160,\"start\":21157},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21173,\"start\":21169},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24739,\"start\":24735}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26777,\"start\":26675},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26843,\"start\":26778},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27071,\"start\":26844},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27187,\"start\":27072},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27690,\"start\":27188},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28466,\"start\":27691},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28614,\"start\":28467},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28677,\"start\":28615},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":29207,\"start\":28678},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":29493,\"start\":29208}]", "paragraph": "[{\"end\":3498,\"start\":2326},{\"end\":6275,\"start\":3500},{\"end\":6335,\"start\":6277},{\"end\":6339,\"start\":6337},{\"end\":8497,\"start\":6356},{\"end\":9179,\"start\":8499},{\"end\":9378,\"start\":9210},{\"end\":9847,\"start\":9401},{\"end\":10182,\"start\":9864},{\"end\":10808,\"start\":10184},{\"end\":11375,\"start\":10810},{\"end\":11762,\"start\":11391},{\"end\":11957,\"start\":11764},{\"end\":12727,\"start\":11959},{\"end\":12817,\"start\":12753},{\"end\":13394,\"start\":12819},{\"end\":13895,\"start\":13396},{\"end\":15240,\"start\":13897},{\"end\":15755,\"start\":15242},{\"end\":16403,\"start\":15768},{\"end\":17140,\"start\":16405},{\"end\":18271,\"start\":17142},{\"end\":18713,\"start\":18273},{\"end\":19850,\"start\":18715},{\"end\":20853,\"start\":19859},{\"end\":21288,\"start\":20855},{\"end\":21933,\"start\":21312},{\"end\":23009,\"start\":21935},{\"end\":25503,\"start\":23011},{\"end\":26674,\"start\":25558}]", "formula": null, "table_ref": "[{\"end\":21389,\"start\":21382},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22020,\"start\":22013},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23108,\"start\":23101},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23352,\"start\":23344},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23980,\"start\":23972}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2324,\"start\":2312},{\"attributes\":{\"n\":\"2\"},\"end\":6354,\"start\":6342},{\"attributes\":{\"n\":\"3\"},\"end\":9208,\"start\":9182},{\"attributes\":{\"n\":\"3.1\"},\"end\":9399,\"start\":9381},{\"attributes\":{\"n\":\"3.2\"},\"end\":9862,\"start\":9850},{\"attributes\":{\"n\":\"3.3\"},\"end\":11389,\"start\":11378},{\"attributes\":{\"n\":\"4\"},\"end\":12751,\"start\":12730},{\"attributes\":{\"n\":\"4.2\"},\"end\":15766,\"start\":15758},{\"end\":19857,\"start\":19853},{\"attributes\":{\"n\":\"4.3\"},\"end\":21298,\"start\":21291},{\"end\":21310,\"start\":21301},{\"end\":25527,\"start\":25506},{\"attributes\":{\"n\":\"5\"},\"end\":25556,\"start\":25530},{\"end\":26686,\"start\":26676},{\"end\":26789,\"start\":26779},{\"end\":26855,\"start\":26845},{\"end\":27083,\"start\":27073},{\"end\":28477,\"start\":28468},{\"end\":28625,\"start\":28616},{\"end\":29218,\"start\":29209}]", "table": "[{\"end\":27690,\"start\":27489},{\"end\":28466,\"start\":27927},{\"end\":29207,\"start\":28783}]", "figure_caption": "[{\"end\":26777,\"start\":26688},{\"end\":26843,\"start\":26791},{\"end\":27071,\"start\":26857},{\"end\":27187,\"start\":27085},{\"end\":27489,\"start\":27190},{\"end\":27927,\"start\":27693},{\"end\":28614,\"start\":28479},{\"end\":28677,\"start\":28627},{\"end\":28783,\"start\":28680},{\"end\":29493,\"start\":29220}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4430,\"start\":4422},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9846,\"start\":9838},{\"end\":12013,\"start\":12005},{\"end\":13305,\"start\":13297},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17794,\"start\":17786},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19521,\"start\":19513}]", "bib_author_first_name": "[{\"end\":29628,\"start\":29621},{\"end\":29658,\"start\":29652},{\"end\":29673,\"start\":29667},{\"end\":30152,\"start\":30146},{\"end\":30171,\"start\":30165},{\"end\":30187,\"start\":30180},{\"end\":30210,\"start\":30209},{\"end\":30434,\"start\":30428},{\"end\":30442,\"start\":30440},{\"end\":30453,\"start\":30449},{\"end\":30662,\"start\":30661},{\"end\":30860,\"start\":30853},{\"end\":30870,\"start\":30866},{\"end\":31366,\"start\":31359},{\"end\":31384,\"start\":31375},{\"end\":31399,\"start\":31392},{\"end\":31417,\"start\":31410},{\"end\":31436,\"start\":31427},{\"end\":31453,\"start\":31447},{\"end\":31749,\"start\":31741},{\"end\":31761,\"start\":31757},{\"end\":31776,\"start\":31771},{\"end\":31789,\"start\":31784},{\"end\":31803,\"start\":31797},{\"end\":32176,\"start\":32172},{\"end\":32187,\"start\":32183},{\"end\":32202,\"start\":32197},{\"end\":32217,\"start\":32210},{\"end\":32228,\"start\":32222},{\"end\":32239,\"start\":32234},{\"end\":32250,\"start\":32246},{\"end\":32892,\"start\":32886},{\"end\":32906,\"start\":32899},{\"end\":32919,\"start\":32912},{\"end\":33090,\"start\":33087},{\"end\":33102,\"start\":33096},{\"end\":33115,\"start\":33107},{\"end\":33124,\"start\":33120},{\"end\":33469,\"start\":33462},{\"end\":33481,\"start\":33475},{\"end\":33493,\"start\":33487},{\"end\":33848,\"start\":33842},{\"end\":33867,\"start\":33862},{\"end\":33884,\"start\":33875},{\"end\":33901,\"start\":33897},{\"end\":33922,\"start\":33915},{\"end\":33935,\"start\":33929},{\"end\":33956,\"start\":33949},{\"end\":33975,\"start\":33967},{\"end\":33991,\"start\":33986},{\"end\":34008,\"start\":34001},{\"end\":34455,\"start\":34446},{\"end\":34470,\"start\":34466},{\"end\":34488,\"start\":34480},{\"end\":34501,\"start\":34496},{\"end\":34522,\"start\":34514},{\"end\":34536,\"start\":34530},{\"end\":34549,\"start\":34545},{\"end\":34565,\"start\":34558},{\"end\":34960,\"start\":34951},{\"end\":34975,\"start\":34971},{\"end\":34993,\"start\":34985},{\"end\":35006,\"start\":35001},{\"end\":35027,\"start\":35019},{\"end\":35041,\"start\":35035},{\"end\":35054,\"start\":35050},{\"end\":35070,\"start\":35063},{\"end\":35534,\"start\":35526},{\"end\":35545,\"start\":35540},{\"end\":35558,\"start\":35553},{\"end\":35575,\"start\":35567},{\"end\":35591,\"start\":35585},{\"end\":35599,\"start\":35597},{\"end\":35613,\"start\":35606},{\"end\":35625,\"start\":35622},{\"end\":36146,\"start\":36145},{\"end\":36157,\"start\":36153},{\"end\":36180,\"start\":36166},{\"end\":36500,\"start\":36493},{\"end\":36511,\"start\":36505},{\"end\":36525,\"start\":36518},{\"end\":36538,\"start\":36531},{\"end\":36837,\"start\":36830},{\"end\":36849,\"start\":36842},{\"end\":36865,\"start\":36860},{\"end\":36878,\"start\":36874},{\"end\":36880,\"start\":36879},{\"end\":37370,\"start\":37363},{\"end\":37385,\"start\":37377},{\"end\":37396,\"start\":37391},{\"end\":37408,\"start\":37401},{\"end\":37663,\"start\":37660},{\"end\":37673,\"start\":37671},{\"end\":37685,\"start\":37679},{\"end\":37697,\"start\":37691},{\"end\":37711,\"start\":37705},{\"end\":37713,\"start\":37712},{\"end\":37939,\"start\":37934},{\"end\":37955,\"start\":37947},{\"end\":37963,\"start\":37960},{\"end\":37975,\"start\":37969},{\"end\":37984,\"start\":37980},{\"end\":38390,\"start\":38381},{\"end\":38411,\"start\":38398},{\"end\":38809,\"start\":38808},{\"end\":38825,\"start\":38820},{\"end\":39327,\"start\":39326},{\"end\":39336,\"start\":39335},{\"end\":39344,\"start\":39343},{\"end\":39355,\"start\":39354},{\"end\":39366,\"start\":39365},{\"end\":39378,\"start\":39377},{\"end\":40143,\"start\":40134},{\"end\":40151,\"start\":40148},{\"end\":40160,\"start\":40156},{\"end\":40169,\"start\":40166},{\"end\":40184,\"start\":40176},{\"end\":40195,\"start\":40192},{\"end\":40206,\"start\":40203},{\"end\":41140,\"start\":41133},{\"end\":41148,\"start\":41145},{\"end\":41161,\"start\":41154},{\"end\":41173,\"start\":41169},{\"end\":41183,\"start\":41179},{\"end\":41197,\"start\":41190},{\"end\":41835,\"start\":41828},{\"end\":41846,\"start\":41840},{\"end\":41854,\"start\":41851},{\"end\":41867,\"start\":41860},{\"end\":41879,\"start\":41875},{\"end\":41892,\"start\":41885},{\"end\":41901,\"start\":41897},{\"end\":42589,\"start\":42582},{\"end\":42602,\"start\":42594},{\"end\":42612,\"start\":42607},{\"end\":42623,\"start\":42619},{\"end\":42625,\"start\":42624},{\"end\":42642,\"start\":42635},{\"end\":42654,\"start\":42649},{\"end\":43060,\"start\":43053},{\"end\":43072,\"start\":43065},{\"end\":43084,\"start\":43078},{\"end\":43096,\"start\":43091},{\"end\":43112,\"start\":43105},{\"end\":43121,\"start\":43117},{\"end\":43123,\"start\":43122},{\"end\":43352,\"start\":43345},{\"end\":43367,\"start\":43359},{\"end\":43379,\"start\":43373},{\"end\":43389,\"start\":43385},{\"end\":43400,\"start\":43395},{\"end\":43709,\"start\":43707},{\"end\":43721,\"start\":43715},{\"end\":43730,\"start\":43727},{\"end\":43739,\"start\":43736},{\"end\":43750,\"start\":43744},{\"end\":43761,\"start\":43756},{\"end\":43776,\"start\":43769},{\"end\":43789,\"start\":43782},{\"end\":44091,\"start\":44086},{\"end\":44108,\"start\":44102},{\"end\":44125,\"start\":44120},{\"end\":44559,\"start\":44550},{\"end\":44578,\"start\":44570},{\"end\":44593,\"start\":44589},{\"end\":44914,\"start\":44908},{\"end\":44930,\"start\":44923},{\"end\":44946,\"start\":44939},{\"end\":44957,\"start\":44952},{\"end\":44971,\"start\":44964},{\"end\":44982,\"start\":44978},{\"end\":44996,\"start\":44992},{\"end\":45007,\"start\":45003},{\"end\":45380,\"start\":45374},{\"end\":45395,\"start\":45390},{\"end\":46140,\"start\":46136},{\"end\":46158,\"start\":46150},{\"end\":46173,\"start\":46165},{\"end\":46190,\"start\":46181},{\"end\":46536,\"start\":46530},{\"end\":46550,\"start\":46546},{\"end\":46564,\"start\":46560},{\"end\":46578,\"start\":46573},{\"end\":46595,\"start\":46590},{\"end\":46608,\"start\":46603},{\"end\":46610,\"start\":46609},{\"end\":46624,\"start\":46618},{\"end\":46638,\"start\":46633},{\"end\":47458,\"start\":47452},{\"end\":47468,\"start\":47463},{\"end\":47481,\"start\":47473},{\"end\":47492,\"start\":47489},{\"end\":47850,\"start\":47845},{\"end\":47864,\"start\":47855},{\"end\":47884,\"start\":47875},{\"end\":47899,\"start\":47892},{\"end\":47908,\"start\":47904},{\"end\":48133,\"start\":48126},{\"end\":48143,\"start\":48139},{\"end\":48145,\"start\":48144},{\"end\":48161,\"start\":48156},{\"end\":48177,\"start\":48170},{\"end\":48189,\"start\":48182},{\"end\":48731,\"start\":48725},{\"end\":48743,\"start\":48736},{\"end\":48751,\"start\":48748},{\"end\":48764,\"start\":48757},{\"end\":48776,\"start\":48772},{\"end\":48786,\"start\":48782},{\"end\":49420,\"start\":49414},{\"end\":49433,\"start\":49425},{\"end\":49441,\"start\":49438},{\"end\":49453,\"start\":49447},{\"end\":49466,\"start\":49460},{\"end\":49476,\"start\":49471},{\"end\":49491,\"start\":49488},{\"end\":49503,\"start\":49499},{\"end\":49925,\"start\":49919},{\"end\":49938,\"start\":49930},{\"end\":49946,\"start\":49943},{\"end\":49958,\"start\":49952},{\"end\":49971,\"start\":49965},{\"end\":49981,\"start\":49976},{\"end\":49996,\"start\":49993},{\"end\":50008,\"start\":50004},{\"end\":50634,\"start\":50630},{\"end\":50645,\"start\":50639},{\"end\":50658,\"start\":50650},{\"end\":50666,\"start\":50663},{\"end\":50676,\"start\":50672},{\"end\":50688,\"start\":50682},{\"end\":50701,\"start\":50695},{\"end\":50711,\"start\":50706},{\"end\":50726,\"start\":50723},{\"end\":50742,\"start\":50734},{\"end\":50751,\"start\":50748},{\"end\":50765,\"start\":50759},{\"end\":51761,\"start\":51759},{\"end\":51775,\"start\":51769},{\"end\":51805,\"start\":51791},{\"end\":52048,\"start\":52046},{\"end\":52063,\"start\":52056},{\"end\":52084,\"start\":52070},{\"end\":52460,\"start\":52453},{\"end\":52471,\"start\":52467},{\"end\":52482,\"start\":52477},{\"end\":52492,\"start\":52489},{\"end\":52505,\"start\":52499},{\"end\":52515,\"start\":52511},{\"end\":52527,\"start\":52524},{\"end\":52784,\"start\":52778},{\"end\":52801,\"start\":52796},{\"end\":52818,\"start\":52812},{\"end\":52839,\"start\":52832},{\"end\":52855,\"start\":52850}]", "bib_author_last_name": "[{\"end\":29650,\"start\":29629},{\"end\":29665,\"start\":29659},{\"end\":29679,\"start\":29674},{\"end\":29688,\"start\":29681},{\"end\":30163,\"start\":30153},{\"end\":30178,\"start\":30172},{\"end\":30207,\"start\":30188},{\"end\":30214,\"start\":30211},{\"end\":30224,\"start\":30216},{\"end\":30438,\"start\":30435},{\"end\":30447,\"start\":30443},{\"end\":30457,\"start\":30454},{\"end\":30670,\"start\":30663},{\"end\":30864,\"start\":30861},{\"end\":30882,\"start\":30871},{\"end\":31373,\"start\":31367},{\"end\":31390,\"start\":31385},{\"end\":31408,\"start\":31400},{\"end\":31425,\"start\":31418},{\"end\":31445,\"start\":31437},{\"end\":31463,\"start\":31454},{\"end\":31755,\"start\":31750},{\"end\":31769,\"start\":31762},{\"end\":31782,\"start\":31777},{\"end\":31795,\"start\":31790},{\"end\":31810,\"start\":31804},{\"end\":32181,\"start\":32177},{\"end\":32195,\"start\":32188},{\"end\":32208,\"start\":32203},{\"end\":32220,\"start\":32218},{\"end\":32232,\"start\":32229},{\"end\":32244,\"start\":32240},{\"end\":32260,\"start\":32251},{\"end\":32897,\"start\":32893},{\"end\":32910,\"start\":32907},{\"end\":32922,\"start\":32920},{\"end\":33094,\"start\":33091},{\"end\":33105,\"start\":33103},{\"end\":33118,\"start\":33116},{\"end\":33128,\"start\":33125},{\"end\":33473,\"start\":33470},{\"end\":33485,\"start\":33482},{\"end\":33506,\"start\":33494},{\"end\":33860,\"start\":33849},{\"end\":33873,\"start\":33868},{\"end\":33895,\"start\":33885},{\"end\":33913,\"start\":33902},{\"end\":33927,\"start\":33923},{\"end\":33947,\"start\":33936},{\"end\":33965,\"start\":33957},{\"end\":33984,\"start\":33976},{\"end\":33999,\"start\":33992},{\"end\":34014,\"start\":34009},{\"end\":34464,\"start\":34456},{\"end\":34478,\"start\":34471},{\"end\":34494,\"start\":34489},{\"end\":34512,\"start\":34502},{\"end\":34528,\"start\":34523},{\"end\":34543,\"start\":34537},{\"end\":34556,\"start\":34550},{\"end\":34574,\"start\":34566},{\"end\":34969,\"start\":34961},{\"end\":34983,\"start\":34976},{\"end\":34999,\"start\":34994},{\"end\":35017,\"start\":35007},{\"end\":35033,\"start\":35028},{\"end\":35048,\"start\":35042},{\"end\":35061,\"start\":35055},{\"end\":35079,\"start\":35071},{\"end\":35538,\"start\":35535},{\"end\":35551,\"start\":35546},{\"end\":35565,\"start\":35559},{\"end\":35583,\"start\":35576},{\"end\":35595,\"start\":35592},{\"end\":35604,\"start\":35600},{\"end\":35620,\"start\":35614},{\"end\":35630,\"start\":35626},{\"end\":36151,\"start\":36147},{\"end\":36164,\"start\":36158},{\"end\":36186,\"start\":36181},{\"end\":36196,\"start\":36188},{\"end\":36503,\"start\":36501},{\"end\":36516,\"start\":36512},{\"end\":36529,\"start\":36526},{\"end\":36541,\"start\":36539},{\"end\":36840,\"start\":36838},{\"end\":36858,\"start\":36850},{\"end\":36872,\"start\":36866},{\"end\":36889,\"start\":36881},{\"end\":37375,\"start\":37371},{\"end\":37389,\"start\":37386},{\"end\":37399,\"start\":37397},{\"end\":37414,\"start\":37409},{\"end\":37669,\"start\":37664},{\"end\":37677,\"start\":37674},{\"end\":37689,\"start\":37686},{\"end\":37703,\"start\":37698},{\"end\":37724,\"start\":37714},{\"end\":37945,\"start\":37940},{\"end\":37958,\"start\":37956},{\"end\":37967,\"start\":37964},{\"end\":37978,\"start\":37976},{\"end\":37988,\"start\":37985},{\"end\":38396,\"start\":38391},{\"end\":38430,\"start\":38412},{\"end\":38438,\"start\":38432},{\"end\":38818,\"start\":38810},{\"end\":38832,\"start\":38826},{\"end\":38836,\"start\":38834},{\"end\":39333,\"start\":39328},{\"end\":39341,\"start\":39337},{\"end\":39352,\"start\":39345},{\"end\":39363,\"start\":39356},{\"end\":39375,\"start\":39367},{\"end\":39384,\"start\":39379},{\"end\":40146,\"start\":40144},{\"end\":40154,\"start\":40152},{\"end\":40164,\"start\":40161},{\"end\":40174,\"start\":40170},{\"end\":40190,\"start\":40185},{\"end\":40201,\"start\":40196},{\"end\":40209,\"start\":40207},{\"end\":41143,\"start\":41141},{\"end\":41152,\"start\":41149},{\"end\":41167,\"start\":41162},{\"end\":41177,\"start\":41174},{\"end\":41188,\"start\":41184},{\"end\":41200,\"start\":41198},{\"end\":41838,\"start\":41836},{\"end\":41849,\"start\":41847},{\"end\":41858,\"start\":41855},{\"end\":41873,\"start\":41868},{\"end\":41883,\"start\":41880},{\"end\":41895,\"start\":41893},{\"end\":41906,\"start\":41902},{\"end\":42592,\"start\":42590},{\"end\":42605,\"start\":42603},{\"end\":42617,\"start\":42613},{\"end\":42633,\"start\":42626},{\"end\":42647,\"start\":42643},{\"end\":42659,\"start\":42655},{\"end\":43063,\"start\":43061},{\"end\":43076,\"start\":43073},{\"end\":43089,\"start\":43085},{\"end\":43103,\"start\":43097},{\"end\":43115,\"start\":43113},{\"end\":43132,\"start\":43124},{\"end\":43357,\"start\":43353},{\"end\":43371,\"start\":43368},{\"end\":43383,\"start\":43380},{\"end\":43393,\"start\":43390},{\"end\":43404,\"start\":43401},{\"end\":43713,\"start\":43710},{\"end\":43725,\"start\":43722},{\"end\":43734,\"start\":43731},{\"end\":43742,\"start\":43740},{\"end\":43754,\"start\":43751},{\"end\":43767,\"start\":43762},{\"end\":43780,\"start\":43777},{\"end\":43793,\"start\":43790},{\"end\":44100,\"start\":44092},{\"end\":44118,\"start\":44109},{\"end\":44136,\"start\":44126},{\"end\":44568,\"start\":44560},{\"end\":44587,\"start\":44579},{\"end\":44599,\"start\":44594},{\"end\":44921,\"start\":44915},{\"end\":44937,\"start\":44931},{\"end\":44950,\"start\":44947},{\"end\":44962,\"start\":44958},{\"end\":44976,\"start\":44972},{\"end\":44990,\"start\":44983},{\"end\":45001,\"start\":44997},{\"end\":45017,\"start\":45008},{\"end\":45388,\"start\":45381},{\"end\":45401,\"start\":45396},{\"end\":46148,\"start\":46141},{\"end\":46163,\"start\":46159},{\"end\":46179,\"start\":46174},{\"end\":46196,\"start\":46191},{\"end\":46544,\"start\":46537},{\"end\":46558,\"start\":46551},{\"end\":46571,\"start\":46565},{\"end\":46588,\"start\":46579},{\"end\":46601,\"start\":46596},{\"end\":46616,\"start\":46611},{\"end\":46631,\"start\":46625},{\"end\":46649,\"start\":46639},{\"end\":47461,\"start\":47459},{\"end\":47471,\"start\":47469},{\"end\":47487,\"start\":47482},{\"end\":47497,\"start\":47493},{\"end\":47853,\"start\":47851},{\"end\":47873,\"start\":47865},{\"end\":47890,\"start\":47885},{\"end\":47902,\"start\":47900},{\"end\":47917,\"start\":47909},{\"end\":48137,\"start\":48134},{\"end\":48154,\"start\":48146},{\"end\":48168,\"start\":48162},{\"end\":48180,\"start\":48178},{\"end\":48192,\"start\":48190},{\"end\":48734,\"start\":48732},{\"end\":48746,\"start\":48744},{\"end\":48755,\"start\":48752},{\"end\":48770,\"start\":48765},{\"end\":48780,\"start\":48777},{\"end\":48791,\"start\":48787},{\"end\":49423,\"start\":49421},{\"end\":49436,\"start\":49434},{\"end\":49445,\"start\":49442},{\"end\":49458,\"start\":49454},{\"end\":49469,\"start\":49467},{\"end\":49486,\"start\":49477},{\"end\":49497,\"start\":49492},{\"end\":49507,\"start\":49504},{\"end\":49928,\"start\":49926},{\"end\":49941,\"start\":49939},{\"end\":49950,\"start\":49947},{\"end\":49963,\"start\":49959},{\"end\":49974,\"start\":49972},{\"end\":49991,\"start\":49982},{\"end\":50002,\"start\":49997},{\"end\":50012,\"start\":50009},{\"end\":50637,\"start\":50635},{\"end\":50648,\"start\":50646},{\"end\":50661,\"start\":50659},{\"end\":50670,\"start\":50667},{\"end\":50680,\"start\":50677},{\"end\":50693,\"start\":50689},{\"end\":50704,\"start\":50702},{\"end\":50721,\"start\":50712},{\"end\":50732,\"start\":50727},{\"end\":50746,\"start\":50743},{\"end\":50757,\"start\":50752},{\"end\":50770,\"start\":50766},{\"end\":51767,\"start\":51762},{\"end\":51789,\"start\":51776},{\"end\":51811,\"start\":51806},{\"end\":52054,\"start\":52049},{\"end\":52068,\"start\":52064},{\"end\":52090,\"start\":52085},{\"end\":52465,\"start\":52461},{\"end\":52475,\"start\":52472},{\"end\":52487,\"start\":52483},{\"end\":52497,\"start\":52493},{\"end\":52509,\"start\":52506},{\"end\":52522,\"start\":52516},{\"end\":52532,\"start\":52528},{\"end\":52794,\"start\":52785},{\"end\":52810,\"start\":52802},{\"end\":52830,\"start\":52819},{\"end\":52848,\"start\":52840},{\"end\":52862,\"start\":52856}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":607606},\"end\":30082,\"start\":29495},{\"attributes\":{\"doi\":\"arXiv:2106.11539\",\"id\":\"b1\"},\"end\":30426,\"start\":30084},{\"attributes\":{\"doi\":\"arXiv:2106.08254\",\"id\":\"b2\"},\"end\":30639,\"start\":30428},{\"attributes\":{\"id\":\"b3\"},\"end\":30792,\"start\":30641},{\"attributes\":{\"doi\":\"10.1109/CVPR.2018.00644\",\"id\":\"b4\",\"matched_paper_id\":206596979},\"end\":31310,\"start\":30794},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":218889832},\"end\":31739,\"start\":31312},{\"attributes\":{\"doi\":\"arXiv:2104.14294\",\"id\":\"b6\"},\"end\":32134,\"start\":31741},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b7\",\"matched_paper_id\":219781060},\"end\":32816,\"start\":32136},{\"attributes\":{\"doi\":\"arXiv:2104.02057\",\"id\":\"b8\"},\"end\":33085,\"start\":32818},{\"attributes\":{\"doi\":\"arXiv:2111.08609\",\"id\":\"b9\"},\"end\":33328,\"start\":33087},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9888225},\"end\":33840,\"start\":33330},{\"attributes\":{\"id\":\"b11\"},\"end\":34444,\"start\":33842},{\"attributes\":{\"doi\":\"arXiv:2106.09681\",\"id\":\"b12\"},\"end\":34949,\"start\":34446},{\"attributes\":{\"doi\":\"ArXiv abs/2106.09681\",\"id\":\"b13\"},\"end\":35457,\"start\":34951},{\"attributes\":{\"doi\":\"10.1109/ICDAR.2019.00243\",\"id\":\"b14\",\"matched_paper_id\":211026773},\"end\":36056,\"start\":35459},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2760893},\"end\":36491,\"start\":36058},{\"attributes\":{\"doi\":\"arXiv:2111.06377\",\"id\":\"b16\"},\"end\":36816,\"start\":36493},{\"attributes\":{\"doi\":\"10.1109/ICCV.2017.322\",\"id\":\"b17\",\"matched_paper_id\":54465873},\"end\":37252,\"start\":36818},{\"attributes\":{\"id\":\"b18\"},\"end\":37621,\"start\":37254},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6773885},\"end\":37854,\"start\":37623},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":248228056},\"end\":38311,\"start\":37856},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":173188931},\"end\":38762,\"start\":38313},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6628106},\"end\":39252,\"start\":38764},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":19516087},\"end\":39951,\"start\":39254},{\"attributes\":{\"doi\":\"10.1145/1148170.1148307\",\"id\":\"b24\"},\"end\":40070,\"start\":39953},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.493\",\"id\":\"b25\",\"matched_paper_id\":235166279},\"end\":41055,\"start\":40072},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":67877016},\"end\":41767,\"start\":41057},{\"attributes\":{\"doi\":\"10.18653/v1/2020.coling-main.82\",\"id\":\"b27\",\"matched_paper_id\":219176849},\"end\":42580,\"start\":41769},{\"attributes\":{\"doi\":\"arXiv:2106.03331\",\"id\":\"b28\"},\"end\":42984,\"start\":42582},{\"attributes\":{\"doi\":\"ArXiv abs/2111.11429\",\"id\":\"b29\"},\"end\":43343,\"start\":42986},{\"attributes\":{\"doi\":\"arXiv:2202.10304\",\"id\":\"b30\"},\"end\":43705,\"start\":43345},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b31\"},\"end\":44084,\"start\":43707},{\"attributes\":{\"doi\":\"arXiv:2102.09550\",\"id\":\"b32\"},\"end\":44525,\"start\":44086},{\"attributes\":{\"doi\":\"arXiv:2009.14457\",\"id\":\"b33\",\"matched_paper_id\":125744068},\"end\":44904,\"start\":44527},{\"attributes\":{\"doi\":\"arXiv:2102.12092\",\"id\":\"b34\"},\"end\":45263,\"start\":44906},{\"attributes\":{\"doi\":\"10.24963/ijcai.2019/466\",\"id\":\"b35\",\"matched_paper_id\":199466355},\"end\":46010,\"start\":45265},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b36\",\"matched_paper_id\":229363322},\"end\":46501,\"start\":46012},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":13756489},\"end\":47450,\"start\":46503},{\"attributes\":{\"doi\":\"arXiv:2104.08405\",\"id\":\"b38\"},\"end\":47841,\"start\":47452},{\"attributes\":{\"id\":\"b39\"},\"end\":48062,\"start\":47843},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.634\",\"id\":\"b40\",\"matched_paper_id\":8485068},\"end\":48647,\"start\":48064},{\"attributes\":{\"doi\":\"https:/dl.acm.org/doi/10.1145/3394486.3403172\",\"id\":\"b41\",\"matched_paper_id\":209515395},\"end\":49412,\"start\":48649},{\"attributes\":{\"doi\":\"arXiv:2104.08836\",\"id\":\"b42\"},\"end\":49839,\"start\":49414},{\"attributes\":{\"doi\":\"10.18653/v1/2022.findings-acl.253\",\"id\":\"b43\",\"matched_paper_id\":248779909},\"end\":50549,\"start\":49841},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.201\",\"id\":\"b44\",\"matched_paper_id\":229923949},\"end\":51697,\"start\":50551},{\"attributes\":{\"doi\":\"arXiv:1911.10683\",\"id\":\"b45\"},\"end\":51982,\"start\":51699},{\"attributes\":{\"doi\":\"10.1109/ICDAR.2019.00166\",\"id\":\"b46\",\"matched_paper_id\":201124789},\"end\":52451,\"start\":51984},{\"attributes\":{\"doi\":\"arXiv:2111.07832\",\"id\":\"b47\"},\"end\":52776,\"start\":52453},{\"attributes\":{\"doi\":\"arXiv:2002.08087\",\"id\":\"b48\"},\"end\":53219,\"start\":52778}]", "bib_title": "[{\"end\":29619,\"start\":29495},{\"end\":30659,\"start\":30641},{\"end\":30851,\"start\":30794},{\"end\":31357,\"start\":31312},{\"end\":32170,\"start\":32136},{\"end\":33460,\"start\":33330},{\"end\":35524,\"start\":35459},{\"end\":36143,\"start\":36058},{\"end\":36828,\"start\":36818},{\"end\":37658,\"start\":37623},{\"end\":37932,\"start\":37856},{\"end\":38379,\"start\":38313},{\"end\":38806,\"start\":38764},{\"end\":39324,\"start\":39254},{\"end\":40132,\"start\":40072},{\"end\":41131,\"start\":41057},{\"end\":41826,\"start\":41769},{\"end\":44548,\"start\":44527},{\"end\":45372,\"start\":45265},{\"end\":46134,\"start\":46012},{\"end\":46528,\"start\":46503},{\"end\":48124,\"start\":48064},{\"end\":48723,\"start\":48649},{\"end\":49917,\"start\":49841},{\"end\":50628,\"start\":50551},{\"end\":52044,\"start\":51984}]", "bib_author": "[{\"end\":29652,\"start\":29621},{\"end\":29667,\"start\":29652},{\"end\":29681,\"start\":29667},{\"end\":29690,\"start\":29681},{\"end\":30165,\"start\":30146},{\"end\":30180,\"start\":30165},{\"end\":30209,\"start\":30180},{\"end\":30216,\"start\":30209},{\"end\":30226,\"start\":30216},{\"end\":30440,\"start\":30428},{\"end\":30449,\"start\":30440},{\"end\":30459,\"start\":30449},{\"end\":30672,\"start\":30661},{\"end\":30866,\"start\":30853},{\"end\":30884,\"start\":30866},{\"end\":31375,\"start\":31359},{\"end\":31392,\"start\":31375},{\"end\":31410,\"start\":31392},{\"end\":31427,\"start\":31410},{\"end\":31447,\"start\":31427},{\"end\":31465,\"start\":31447},{\"end\":31757,\"start\":31741},{\"end\":31771,\"start\":31757},{\"end\":31784,\"start\":31771},{\"end\":31797,\"start\":31784},{\"end\":31812,\"start\":31797},{\"end\":32183,\"start\":32172},{\"end\":32197,\"start\":32183},{\"end\":32210,\"start\":32197},{\"end\":32222,\"start\":32210},{\"end\":32234,\"start\":32222},{\"end\":32246,\"start\":32234},{\"end\":32262,\"start\":32246},{\"end\":32899,\"start\":32886},{\"end\":32912,\"start\":32899},{\"end\":32924,\"start\":32912},{\"end\":33096,\"start\":33087},{\"end\":33107,\"start\":33096},{\"end\":33120,\"start\":33107},{\"end\":33130,\"start\":33120},{\"end\":33475,\"start\":33462},{\"end\":33487,\"start\":33475},{\"end\":33508,\"start\":33487},{\"end\":33862,\"start\":33842},{\"end\":33875,\"start\":33862},{\"end\":33897,\"start\":33875},{\"end\":33915,\"start\":33897},{\"end\":33929,\"start\":33915},{\"end\":33949,\"start\":33929},{\"end\":33967,\"start\":33949},{\"end\":33986,\"start\":33967},{\"end\":34001,\"start\":33986},{\"end\":34016,\"start\":34001},{\"end\":34466,\"start\":34446},{\"end\":34480,\"start\":34466},{\"end\":34496,\"start\":34480},{\"end\":34514,\"start\":34496},{\"end\":34530,\"start\":34514},{\"end\":34545,\"start\":34530},{\"end\":34558,\"start\":34545},{\"end\":34576,\"start\":34558},{\"end\":34971,\"start\":34951},{\"end\":34985,\"start\":34971},{\"end\":35001,\"start\":34985},{\"end\":35019,\"start\":35001},{\"end\":35035,\"start\":35019},{\"end\":35050,\"start\":35035},{\"end\":35063,\"start\":35050},{\"end\":35081,\"start\":35063},{\"end\":35540,\"start\":35526},{\"end\":35553,\"start\":35540},{\"end\":35567,\"start\":35553},{\"end\":35585,\"start\":35567},{\"end\":35597,\"start\":35585},{\"end\":35606,\"start\":35597},{\"end\":35622,\"start\":35606},{\"end\":35632,\"start\":35622},{\"end\":36153,\"start\":36145},{\"end\":36166,\"start\":36153},{\"end\":36188,\"start\":36166},{\"end\":36198,\"start\":36188},{\"end\":36505,\"start\":36493},{\"end\":36518,\"start\":36505},{\"end\":36531,\"start\":36518},{\"end\":36543,\"start\":36531},{\"end\":36842,\"start\":36830},{\"end\":36860,\"start\":36842},{\"end\":36874,\"start\":36860},{\"end\":36891,\"start\":36874},{\"end\":37377,\"start\":37363},{\"end\":37391,\"start\":37377},{\"end\":37401,\"start\":37391},{\"end\":37416,\"start\":37401},{\"end\":37671,\"start\":37660},{\"end\":37679,\"start\":37671},{\"end\":37691,\"start\":37679},{\"end\":37705,\"start\":37691},{\"end\":37726,\"start\":37705},{\"end\":37947,\"start\":37934},{\"end\":37960,\"start\":37947},{\"end\":37969,\"start\":37960},{\"end\":37980,\"start\":37969},{\"end\":37990,\"start\":37980},{\"end\":38398,\"start\":38381},{\"end\":38432,\"start\":38398},{\"end\":38440,\"start\":38432},{\"end\":38820,\"start\":38808},{\"end\":38834,\"start\":38820},{\"end\":38838,\"start\":38834},{\"end\":39335,\"start\":39326},{\"end\":39343,\"start\":39335},{\"end\":39354,\"start\":39343},{\"end\":39365,\"start\":39354},{\"end\":39377,\"start\":39365},{\"end\":39386,\"start\":39377},{\"end\":40148,\"start\":40134},{\"end\":40156,\"start\":40148},{\"end\":40166,\"start\":40156},{\"end\":40176,\"start\":40166},{\"end\":40192,\"start\":40176},{\"end\":40203,\"start\":40192},{\"end\":40211,\"start\":40203},{\"end\":41145,\"start\":41133},{\"end\":41154,\"start\":41145},{\"end\":41169,\"start\":41154},{\"end\":41179,\"start\":41169},{\"end\":41190,\"start\":41179},{\"end\":41202,\"start\":41190},{\"end\":41840,\"start\":41828},{\"end\":41851,\"start\":41840},{\"end\":41860,\"start\":41851},{\"end\":41875,\"start\":41860},{\"end\":41885,\"start\":41875},{\"end\":41897,\"start\":41885},{\"end\":41908,\"start\":41897},{\"end\":42594,\"start\":42582},{\"end\":42607,\"start\":42594},{\"end\":42619,\"start\":42607},{\"end\":42635,\"start\":42619},{\"end\":42649,\"start\":42635},{\"end\":42661,\"start\":42649},{\"end\":43065,\"start\":43053},{\"end\":43078,\"start\":43065},{\"end\":43091,\"start\":43078},{\"end\":43105,\"start\":43091},{\"end\":43117,\"start\":43105},{\"end\":43134,\"start\":43117},{\"end\":43359,\"start\":43345},{\"end\":43373,\"start\":43359},{\"end\":43385,\"start\":43373},{\"end\":43395,\"start\":43385},{\"end\":43406,\"start\":43395},{\"end\":43715,\"start\":43707},{\"end\":43727,\"start\":43715},{\"end\":43736,\"start\":43727},{\"end\":43744,\"start\":43736},{\"end\":43756,\"start\":43744},{\"end\":43769,\"start\":43756},{\"end\":43782,\"start\":43769},{\"end\":43795,\"start\":43782},{\"end\":44102,\"start\":44086},{\"end\":44120,\"start\":44102},{\"end\":44138,\"start\":44120},{\"end\":44570,\"start\":44550},{\"end\":44589,\"start\":44570},{\"end\":44601,\"start\":44589},{\"end\":44923,\"start\":44908},{\"end\":44939,\"start\":44923},{\"end\":44952,\"start\":44939},{\"end\":44964,\"start\":44952},{\"end\":44978,\"start\":44964},{\"end\":44992,\"start\":44978},{\"end\":45003,\"start\":44992},{\"end\":45019,\"start\":45003},{\"end\":45390,\"start\":45374},{\"end\":45403,\"start\":45390},{\"end\":46150,\"start\":46136},{\"end\":46165,\"start\":46150},{\"end\":46181,\"start\":46165},{\"end\":46198,\"start\":46181},{\"end\":46546,\"start\":46530},{\"end\":46560,\"start\":46546},{\"end\":46573,\"start\":46560},{\"end\":46590,\"start\":46573},{\"end\":46603,\"start\":46590},{\"end\":46618,\"start\":46603},{\"end\":46633,\"start\":46618},{\"end\":46651,\"start\":46633},{\"end\":47463,\"start\":47452},{\"end\":47473,\"start\":47463},{\"end\":47489,\"start\":47473},{\"end\":47499,\"start\":47489},{\"end\":47855,\"start\":47845},{\"end\":47875,\"start\":47855},{\"end\":47892,\"start\":47875},{\"end\":47904,\"start\":47892},{\"end\":47919,\"start\":47904},{\"end\":48139,\"start\":48126},{\"end\":48156,\"start\":48139},{\"end\":48170,\"start\":48156},{\"end\":48182,\"start\":48170},{\"end\":48194,\"start\":48182},{\"end\":48736,\"start\":48725},{\"end\":48748,\"start\":48736},{\"end\":48757,\"start\":48748},{\"end\":48772,\"start\":48757},{\"end\":48782,\"start\":48772},{\"end\":48793,\"start\":48782},{\"end\":49425,\"start\":49414},{\"end\":49438,\"start\":49425},{\"end\":49447,\"start\":49438},{\"end\":49460,\"start\":49447},{\"end\":49471,\"start\":49460},{\"end\":49488,\"start\":49471},{\"end\":49499,\"start\":49488},{\"end\":49509,\"start\":49499},{\"end\":49930,\"start\":49919},{\"end\":49943,\"start\":49930},{\"end\":49952,\"start\":49943},{\"end\":49965,\"start\":49952},{\"end\":49976,\"start\":49965},{\"end\":49993,\"start\":49976},{\"end\":50004,\"start\":49993},{\"end\":50014,\"start\":50004},{\"end\":50639,\"start\":50630},{\"end\":50650,\"start\":50639},{\"end\":50663,\"start\":50650},{\"end\":50672,\"start\":50663},{\"end\":50682,\"start\":50672},{\"end\":50695,\"start\":50682},{\"end\":50706,\"start\":50695},{\"end\":50723,\"start\":50706},{\"end\":50734,\"start\":50723},{\"end\":50748,\"start\":50734},{\"end\":50759,\"start\":50748},{\"end\":50772,\"start\":50759},{\"end\":51769,\"start\":51759},{\"end\":51791,\"start\":51769},{\"end\":51813,\"start\":51791},{\"end\":52056,\"start\":52046},{\"end\":52070,\"start\":52056},{\"end\":52092,\"start\":52070},{\"end\":52467,\"start\":52453},{\"end\":52477,\"start\":52467},{\"end\":52489,\"start\":52477},{\"end\":52499,\"start\":52489},{\"end\":52511,\"start\":52499},{\"end\":52524,\"start\":52511},{\"end\":52534,\"start\":52524},{\"end\":52796,\"start\":52778},{\"end\":52812,\"start\":52796},{\"end\":52832,\"start\":52812},{\"end\":52850,\"start\":52832},{\"end\":52864,\"start\":52850}]", "bib_venue": "[{\"end\":30995,\"start\":30972},{\"end\":32389,\"start\":32336},{\"end\":36975,\"start\":36962},{\"end\":38066,\"start\":38051},{\"end\":38935,\"start\":38917},{\"end\":39633,\"start\":39506},{\"end\":40551,\"start\":40404},{\"end\":41424,\"start\":41313},{\"end\":42212,\"start\":42072},{\"end\":45629,\"start\":45530},{\"end\":46892,\"start\":46857},{\"end\":48297,\"start\":48280},{\"end\":48998,\"start\":48976},{\"end\":50131,\"start\":50116},{\"end\":51112,\"start\":50965},{\"end\":29769,\"start\":29690},{\"end\":30144,\"start\":30084},{\"end\":30520,\"start\":30475},{\"end\":30708,\"start\":30672},{\"end\":30970,\"start\":30907},{\"end\":31503,\"start\":31465},{\"end\":31929,\"start\":31828},{\"end\":32334,\"start\":32266},{\"end\":32884,\"start\":32818},{\"end\":33194,\"start\":33146},{\"end\":33567,\"start\":33508},{\"end\":34137,\"start\":34016},{\"end\":34689,\"start\":34592},{\"end\":35198,\"start\":35101},{\"end\":35741,\"start\":35656},{\"end\":36267,\"start\":36198},{\"end\":36646,\"start\":36559},{\"end\":36960,\"start\":36912},{\"end\":37361,\"start\":37254},{\"end\":37730,\"start\":37726},{\"end\":38049,\"start\":37990},{\"end\":38520,\"start\":38440},{\"end\":38894,\"start\":38838},{\"end\":39504,\"start\":39386},{\"end\":40402,\"start\":40240},{\"end\":41311,\"start\":41202},{\"end\":42070,\"start\":41939},{\"end\":42774,\"start\":42677},{\"end\":43051,\"start\":42986},{\"end\":43511,\"start\":43422},{\"end\":43882,\"start\":43811},{\"end\":44297,\"start\":44154},{\"end\":44702,\"start\":44617},{\"end\":45528,\"start\":45426},{\"end\":46246,\"start\":46202},{\"end\":46763,\"start\":46651},{\"end\":47638,\"start\":47515},{\"end\":48278,\"start\":48215},{\"end\":48916,\"start\":48838},{\"end\":49613,\"start\":49525},{\"end\":50114,\"start\":50047},{\"end\":50963,\"start\":50801},{\"end\":51757,\"start\":51699},{\"end\":52190,\"start\":52116},{\"end\":52601,\"start\":52550},{\"end\":52990,\"start\":52880}]"}}}, "year": 2023, "month": 12, "day": 17}