{"id": 253761189, "updated": "2023-11-06 14:20:13.05", "metadata": {"title": "PromptTTS: Controllable Text-to-Speech with Text Descriptions", "authors": "[{\"first\":\"Zhifang\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Yichong\",\"last\":\"Leng\",\"middle\":[]},{\"first\":\"Yihan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Sheng\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Xu\",\"last\":\"Tan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Using a text description as prompt to guide the generation of text or images (e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and image generation, in this work, we explore the possibility of utilizing text descriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS) system (dubbed as PromptTTS) that takes a prompt with both style and content descriptions as input to synthesize the corresponding speech. Specifically, PromptTTS consists of a style encoder and a content encoder to extract the corresponding representations from the prompt, and a speech decoder to synthesize speech according to the extracted style and content representations. Compared with previous works in controllable TTS that require users to have acoustic knowledge to understand style factors such as prosody and pitch, PromptTTS is more user-friendly since text descriptions are a more natural way to express speech style (e.g., ''A lady whispers to her friend slowly''). Given that there is no TTS dataset with prompts, to benchmark the task of PromptTTS, we construct and release a dataset containing prompts with style and content information and the corresponding speech. Experiments show that PromptTTS can generate speech with precise style control and high speech quality. Audio samples and our dataset are publicly available.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2211.12171", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icassp/GuoLWZT23", "doi": "10.1109/icassp49357.2023.10096285"}}, "content": {"source": {"pdf_hash": "961dad067e4481ed4a3dbe9a7630bdb884952171", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.12171v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f8772a58f48f71b7808096a561066acb410c2d97", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/961dad067e4481ed4a3dbe9a7630bdb884952171.txt", "contents": "\nPROMPTTTS: CONTROLLABLE TEXT-TO-SPEECH WITH TEXT DESCRIPTIONS\n\n\nZhifang Guo zhifangguo9@gmail.com \nUniverisity of Science and Technology of China\nRenmin University of China\nMicrosoft Azure Speech\nMicrosoft Research Asia\n\n\nYichong Leng \nUniverisity of Science and Technology of China\nRenmin University of China\nMicrosoft Azure Speech\nMicrosoft Research Asia\n\n\nYihan Wu yihanwu@ruc.edu.cn \nUniverisity of Science and Technology of China\nRenmin University of China\nMicrosoft Azure Speech\nMicrosoft Research Asia\n\n\nSheng Zhao szhao@microsoft.com \nUniverisity of Science and Technology of China\nRenmin University of China\nMicrosoft Azure Speech\nMicrosoft Research Asia\n\n\nXu Tan \nUniverisity of Science and Technology of China\nRenmin University of China\nMicrosoft Azure Speech\nMicrosoft Research Asia\n\n\nPROMPTTTS: CONTROLLABLE TEXT-TO-SPEECH WITH TEXT DESCRIPTIONS\nIndex Terms-Style ControlText-to-SpeechPrompt\nUsing a text description as prompt to guide the generation of text or images (e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and image generation, in this work, we explore the possibility of utilizing text descriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS) system (dubbed as PromptTTS) that takes a prompt with both style and content descriptions as input to synthesize the corresponding speech. Specifically, PromptTTS consists of a style encoder and a content encoder to extract the corresponding representations from the prompt, and a speech decoder to synthesize speech according to the extracted style and content representations. Compared with previous works in controllable TTS that require users to have acoustic knowledge to understand style factors such as prosody and pitch, PromptTTS is more user-friendly since text descriptions are a more natural way to express speech style (e.g., \"A lady whispers to her friend slowly\"). Given that there is no TTS dataset with prompts, to benchmark the task of PromptTTS, we construct and release a dataset containing prompts with style and content information and the corresponding speech. Experiments show that PromptTTS can generate speech with precise style control and high speech quality. Audio samples and our dataset are publicly available 1 .\n\nINTRODUCTION\n\nRecent research has achieved great success in text and image generation guided with a text description as prompt [1,2,3,4] (e.g., GPT-3 [5] or DALLE-2 [6]). Beyond text and image generation, there is little research on prompt-based guidance for text-to-speech (TTS) synthesis [7,8] with different styles such as pitch, speaking speed, and emotion. Previous works on controllable TTS focus on controlling specific style factors: prosody control with word-level prosody tags [9], speaking speed control with sentence-level speaking-rate [10], and pitch control with pitch contours [11]. Style control can be achieved in an explicit manner by using the value of style factors such as pitch [12] or an implicit manner by learning a style token [13] from the reference speech. However, all previous works require users to provide the specific value of style factors with acoustic knowledge or choose the reference speech that meets the requirements, which are timeconsuming and not user-friendly. Therefore, it is a better choice if style control is achieved with a text description in natural language.\n\nIn this work, we explore the possibility of leveraging a text description (denoted as prompt) to guide speech synthesis.\n\nTo be more specific, as shown in Table 1, the input prompt consists of a style description (denoted as style prompt) and a content description containing the text to be converted to speech (denoted as content prompt) with a colon in between. For example, an input prompt, \"A lady whispers to her friend slowly: everything will go fine, right?\", means that the model needs to synthesize the speech with content of \"everything will go fine, right?\" in a female voice, a slow speaking speed, and a whispering manner. In this way, users are able to create speech from a prompt, resulting in style control without the requirements for acoustic knowledge or reference speech.\n\nAs the first exploration on TTS guided with prompts, there not exist datasets or systems for this task. Thus, we design a dataset, a system, and an evaluation metric for this task.\n\n(1) Dataset: we construct and release a dataset containing prompts with style and content information and the corresponding speech. The prompts describe the speech in 5 style factors including gender, pitch, speaking speed, volume, and emotion. (2) System: to synthesize speech according to a prompt, we propose PromptTTS to serve as baseline for future research in this task, which consists of a style encoder, a content encoder, and a speech decoder. The style and content encoders extract style and content representations from the prompt, respectively. The speech decoder utilizes both representations to synthesize speech accordingly. (3) Evaluation metric: we calculate the accuracy between the style factors from output speech and those from prompts as evaluation metric for style control.  The contributions of our work include: (1) we propose PromptTTS to synthesize the speech that is consistent with prompts in style and content, which is more user-friendly than previous works; (2) we collect and release a dataset consisting of prompts and the corresponding speech for this task; (3) the experiments show that PromptTTS can generate speech with precise style control and high speech quality.\n\n\nMETHOD\n\n\nModel Overview\n\nAs shown in Fig. 1, PromptTTS consists of a style encoder, a content encoder, and a speech decoder. The style encoder maps a style prompt to a semantic space to extract the style representation, which is used to guide the content encoder and the speech decoder. The content encoder takes a content prompt as input to extract the content representation. And the speech decoder concatenates the style representation and the content representation as input to produce the speech that is consistent with both style and content prompts. We introduce the above modules as follows.\n\n\nStyle Encoder\n\nThe style encoder extracts the style representation from the style prompt with a BERT model [14], which serves as guidance to control the style of output speech. The input (style prompt) sequence T = [T 1 , T 2 , \u00b7 \u00b7 \u00b7 , T M ] is prepended with a [CLS] token, converted into a word embedding, and fed into the BERT model, where M refers to the length of style prompt. The hidden vector corresponding to the [CLS] token is regarded as the style representation to guide the content encoder and the speech decoder. In order to better recognize the semantic information related to speech style, the BERT model is fine-tuned on an auxiliary classification task to pre-dict gender, pitch, speaking speed, volume, and emotion information from style prompts. In this way, the BERT model focuses more on style information and thus is more suitable for PromptTTS to guide speech synthesis.\n\n\nContent Encoder\n\nThe content encoder extracts the content representation conditioned on the style representation from the style encoder, since the variance adaptor [12] in the content encoder predicts information such as duration and pitch that is closely related to the style of output speech. We utilize a grapheme-to-phoneme conversion tool [15] to convert the input (content prompt) sequence to a phoneme sequence P = [P 1 , P 2 , \u00b7 \u00b7 \u00b7 , P N ], where N refers to the length of phoneme sequence, and we map it to a phoneme embedding to feed into Transformer blocks. Considering that solely adding the style representation to the input embedding may not be powerful enough to guide the model [16], we prepend the style representation to the input of every Transformer block. Following FastSpeech 2 [12], the topmost module of the content encoder is a variance adaptor to predict the duration, pitch, and energy, which can provide enough information to synthesize variant speech and alleviate the one-to-many mapping problem in TTS.\n\n\nSpeech Decoder\n\nThe speech decoder utilizes the style and content representations from both encoders to generate the mel-spectrogram in the corresponding style and content. To be more specific, both representations are concatenated to form the input of the speech decoder. The style representation is also prepended to the input of every Transformer block following the same mechanism in the content encoder.\n\n\nDATASET\n\nGiven that there are no TTS datasets with prompts, we construct and release a dataset called PromptSpeech which consists of speech and the corresponding prompts. We synthesize speech with 5 different style factors (gender, pitch, speaking speed, volume, and emotion) from a commercial TTS API 2 . The emotion factor has 5 categories 3 and the gender factor has 2 categories. For the rest of style factors including pitch, speaking speed, and volume, we extract the value of style factors from speech with signal processing tools and divide speech into 3 categories (high/normal/low) according to the proportion. Considering that the style of speech can be described in natural language, we ask experts to write style prompts for each category. To further augment the style prompts, we utilize SimBERT [17] to generate more style prompts with similar semantics. The speech and the corresponding prompt describing the same style and content are used as paired data in our dataset.\n\nSince there could be a gap between the synthesized speech and the real speech, besides the dataset mentioned above (denoted as synthesized version), we also construct a dataset (denoted as real version) with real speech from LibriTTS [18] by similar construction process. Due to the lack of emotion in LibriTTS, the real version of PromptSpeech only has 4 style factors. As shown in Table 2, we split both versions of PromptSpeech into training sets and test sets, which are released in public 4 . The experiments are conducted on Prompt-Speech to verify the effectiveness of PromptTTS. 4. EXPERIMENT\n\n\nModel Configuration\n\nStyle Encoder We utilize a pre-trained BERT model consisting of 12 hidden layers with 110M parameters [14]. The BERT model is fine-tuned on an auxiliary classification task on 5 style factors with P-tuning v2 [16]. Content Encoder The content encoder consists of a variance adaptor and 4 Transformer blocks, where the dimensions of both style and content representations are set to 256. The variance adaptor consists of a duration predictor, a pitch predictor and an energy predictor following FastSpeech 2 [12].\n\nSpeech Decoder The Transformer blocks in the speech decoder and the content encoder share the same model architecture. The output mel-spectrogram of PromptTTS is transformed into speech using a pre-trained HiFiGAN [19] 5 .\n\n\nBaseline System\n\nGiven that there are no previous works designed for this task, we construct a straight-forward two-stage system as baseline.\n\nIn the first stage, the system explicitly predicts the value of style factors in style prompts with a BERT-based model finetuned with P-tuning v2 [16]. In the second stage, the value of style factors is converted to a style embedding to guide the style of the output speech [12].\n\n\nEvaluation Metric\n\nThe evaluation metric is the accuracy between the style factors from output speech and those from prompts. The accuracy of pitch, speaking speed, and volume can be calculated with tools in signal processing 6 . For the rest of style factors including gender and emotion that cannot be recognized with signal processing tools, we train classifiers to recognize the categories of output speech. According to Table 3, the accuracy of classifiers is high enough (\u2265 98%) to show that: (1) PromptSpeech is well-established and distinguishable; (2) the classifiers can serve as evaluation metric for PromptTTS. \n\n\nResults\n\nIn this section, we first introduce the accuracy of PromptTTS and the two-stage baseline system in style control. Then, we conduct ablation studies on the style encoder and analyse the advantages of PromptTTS over the two-stage baseline system. Finally, we conduct mean opinion score (MOS) [20] to evaluate the speech quality from human perspective. Table 4 shows the accuracy of PromptTTS and the two-stage baseline system on 5 style factors. We have the following observations: (1) PromptTTS outperforms the two-stage baseline system on all style factors on PromptSpeech since it could alleviate the cascaded error of the two-stage system, which is discussed in detail in 4.4.3; (2) PromptTTS achieves the average accuracy of 90.31% on PromptSpeech, outperforming baseline by 3.25%, which shows that PromptTTS can synthesize speech in a more consistent style with the intention of style prompts. \n\n\nMain Results\n\n\nAblation Studies\n\nSince the style encoder plays an important role in learning and controlling the style of output speech, we conduct ablation studies on it by changing fine-tuning methods from P-tuning v2 [16] to the standard fine-tuning method in the BERT model [14] or no fine-tuning methods at all. From the results in Table 5, we can draw the following conclusions:\n\n(1) P-tuning v2 slightly outperforms the standard fine-tuning method in BERT model, which proves that P-tuning v2 is suitable for PromptTTS to understand style prompts in Prompt-Speech; (2) the large performance drops in the setting without fine-tuning methods indicate that fine-tuning is necessary to help PromptTTS focus on keywords containing style information. \n\n\nPerformance Analysis\n\nTo analyze the advantages of PromptTTS over the two-stage baseline system, we show the average accuracy of the twostage baseline system at each stage in Table 6. Note that stage one corresponds to the explicit classification on style factors and stage two corresponds to the speech synthesis given ground-truth style factors. From the table, we can see that the average accuracy of each stage is 94.97% and 91.60%, respectively, which would propagate mistakes in every stage. However, PromptTTS is an end-to-end system that relies on a latent space to transfer information without cascaded error, thus making it achieve better accuracy. \n\n\nSpeech Quality\n\nTo evaluate the perceptual quality, we perform MOS [20] on the test sets. We compare the MOS of audio samples including: (1) GT, the ground-truth recordings; (2) GT mel + HiFiGAN, where we first convert ground-truth speech into mel-spectrogram, and then convert the mel-spectrogram back to speech using HiFiGAN [19]; (3) Two-stage; (4) PromptTTS. Both systems in (3) and (4) use HiFiGAN as vocoder. We also conduct comparison MOS (CMOS) [21] to compare the speech quality between PromptTTS and the twostage baseline system. According to Table 7, it can be seen that PromptTTS outperforms the two-stage baseline system slightly in terms of speech quality, which is acceptable since the PromptTTS is designed for style control with prompts. \n\n\nCONCLUSION\n\nIn this work, to explore the possibility of guiding TTS with a prompt, we propose PromptTTS that can synthesize the speech consistent with the prompt in style and content. Compared with previous works in controllable TTS, PromptTTS controls the generation of speech in a more user-friendly way. We also collect and release a dataset called PromptSpeech that consists of prompts and the corresponding speech for this task. The experiments verify that PromptTTS can generate speech with precise style control and high speech quality. In the future, we will explore the zero-shot ability of PromptTTS to control unseen style factors and improve the pre-training methods to extract better representations from prompts.\n\nFig. 1 :\n1The architecture of PromptTTS consisting of a style encoder, a content encoder, and a speech decoder.\n\nTable 1 :\n1The example of prompts containing style prompts and content prompts with colons in between.Style Prompt \nContent Prompt \n\nI need a man with a deep and loud voice to talk cheerfully: No important letter come in a parcel, is there? \nPlease ask a girl to shout loudly with a sharp voice: But folks hereabouts don't like him. \nA lady whispers to her friend slowly: Everything will go fine, right? \n\n\n\nTable 2 :\n2The number of samples in both versions of Prompt-Speech.Version \nTraining \nTest \n\nSynthesized \n155092 \n5032 \n\nReal \n26588 \n1305 \n\n\n\nTable 3 :\n3The accuracy (%) of classifiers on gender and emotion.Version \nGender Emotion \n\nSynthesized \n99.97 \n98.80 \n\nReal \n98.26 \n-\n\n\n\nTable 4 :\n4Theaccuracy (%) of PromptTTS and the two-stage \nbaseline system on 5 style factors. Note that emotion is not \navailable in real speech. \n\nVersion \nSetting Gender Pitch Speed Volume Emotion Mean \n\nSynthesized \nTwo-stage 97.84 73.58 82.79 81.92 \n88.16 84.86 \nPromptTTS 99.57 82.60 86.55 87.35 \n91.78 89.57 \n\nReal \nTwo-stage 97.36 81.14 89.05 89.39 \n-\n89.24 \nPromptTTS 99.16 82.69 92.57 89.73 \n-\n91.04 \n\n\n\nTable 5 :\n5Theaccuracy (%) of PromptTTS with different \nfine-tuning methods (\"FT Method\" in table) in the style en-\ncoder. \"Standard\" stands for the standard fine-tuning method \nin BERT. \n\nVersion \nFT Method Gender Pitch Speed Volume Emotion Mean \n\nSynthesized \n\nP-tuning V2 99.57 82.60 86.55 87.35 \n91.78 89.57 \nStandard \n99.02 83.40 86.27 86.39 \n91.40 89.30 \nNo fine-tuning 38.21 42.45 65.37 72.25 \n15.94 46.84 \n\nReal \n\nP-tuning V2 99.16 82.69 92.57 89.73 \n-\n91.04 \nStandard \n99.16 81.70 91.66 89.21 \n-\n90.43 \nNo fine-tuning 52.60 33.57 61.50 60.14 \n-\n51.95 \n\n\n\nTable 6 :\n6The accuracy (%) of the two-stage baseline system in each stage (\"Stage\" in table). Stage one corresponds to the explicit classification on style factors and stage two corresponds to the speech synthesis given ground-truth style factors.Version \nStage Gender Pitch Speed Volume Emotion Mean \n\nSynthesized \nOne \n99.15 80.54 89.17 91.14 \n92.31 \n90.46 \nTwo \n98.68 91.36 92.85 89.89 \n95.51 \n93.66 \n\nReal \nOne 100.00 84.51 96.85 89.58 \n-\n92.74 \nTwo \n97.36 96.01 91.95 99.79 \n-\n96.28 \n\n\n\nTable 7 :\n7The results of speech quality with 95% confidence intervals.Version \nSetting \nMOS \nCMOS \n(vs. Two-stage) \n\nSynthesized \n\nGT \n4.28 \u00b1 0.10 \n-\nGT mel + HiFiGAN 4.22 \u00b1 0.07 \n-\nTwo-stage \n3.95 \u00b1 0.07 \n0 \nPromptTTS \n4.00 \u00b1 0.08 \n0.057 \n\nReal \n\nGT \n4.42 \u00b1 0.11 \n-\nGT mel + HiFiGAN 4.40 \u00b1 0.08 \n-\nTwo-stage \n3.78 \u00b1 0.08 \n0 \nPromptTTS \n3.80 \u00b1 0.07 \n0.025 \n\n\nhttps://azure.microsoft.com/en-us/services/cog nitive-services/text-to-speech/#overview 3 General, shout, whisper, cheerful, and sad. 4 https://speechresearch.github.io/prompttts/\nhttps://github.com/jik876/hifi-gan 6 https://github.com/JeremyCCHsu/Python-Wrapper -for-World-Vocoder\n\nPangu-\u03b1: Large-scale autoregressive pretrained chinese language models with auto-parallel computation. Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, et al., \"Pangu-\u03b1: Large-scale autoregressive pretrained chinese language models with auto-parallel computa- tion,\" 2021.\n\nCpm-2: Large-scale costeffective pre-trained language models. Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, AI Open. 2Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, et al., \"Cpm-2: Large-scale cost- effective pre-trained language models,\" AI Open, vol. 2, pp. 216-224, 2021.\n\nGenerative adversarial text to image synthesis. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, International Conference on Machine Learning (ICML). Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo- geswaran, Bernt Schiele, et al., \"Generative adversarial text to image synthesis,\" in International Conference on Machine Learning (ICML), 2016, pp. 1060-1069.\n\nGenerative pretraining from pixels. Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, International Conference on Machine Learning (ICML). 2020Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Hee- woo Jun, Prafulla Dhariwal, et al., \"Generative pretrain- ing from pixels,\" in International Conference on Ma- chine Learning (ICML), 2020.\n\nLanguage models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Dhariwal, Conference and Workshop on Neural Information Processing Systems (NIPS), 2020. 33Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Dhariwal, et al., \"Language models are few-shot learners,\" in Conference and Work- shop on Neural Information Processing Systems (NIPS), 2020, vol. 33, pp. 1877-1901.\n\nHierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, 2022Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen, \"Hierarchical text-conditional im- age generation with clip latents,\" 2022.\n\nA survey on neural speech synthesis. Xu Tan, Tao Qin, Frank Soong, Tie-Yan Liu, Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu, \"A survey on neural speech synthesis,\" 2021.\n\nNaturalspeech: End-to-end text to speech synthesis with human-level quality. Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He, 2022Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He, et al., \"Naturalspeech: End-to-end text to speech synthesis with human-level quality,\" 2022.\n\nUnsupervised word-level prosody tagging for controllable speech synthesis. Yiwei Guo, Chenpeng Du, Kai Yu, International Conference on Acoustics, Speech and Signal Processing. Yiwei Guo, Chenpeng Du, and Kai Yu, \"Unsupervised word-level prosody tagging for controllable speech syn- thesis,\" in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 7597-7601.\n\nSpeaking speed control of end-to-end speech synthesis using sentence-level conditioning. Jae-Sung Bae, Hanbin Bae, Young-Sun Joo, Junmo Lee, Gyeong-Hoon Lee, Jae-Sung Bae, Hanbin Bae, Young-Sun Joo, Junmo Lee, Gyeong-Hoon Lee, et al., \"Speaking speed control of end-to-end speech synthesis using sentence-level condi- tioning,\" 2020.\n\nFastpitchformant: Source-filter based decomposed modeling for speech synthesis. Taejun Bak, Jae-Sung Bae, Hanbin Bae, Young-Ik Kim, Hoon-Young Cho, Conference of the International Speech Communication Association (Interspeech). 2021Taejun Bak, Jae-Sung Bae, Hanbin Bae, Young-Ik Kim, and Hoon-Young Cho, \"Fastpitchformant: Source-filter based decomposed modeling for speech synthesis,\" in Conference of the International Speech Communication Association (Interspeech), 2021.\n\nFastspeech 2: Fast and high-quality end-to-end text to speech. Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, International Conference on Learning Representations (ICLR). 2021Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, et al., \"Fastspeech 2: Fast and high-quality end-to-end text to speech,\" in International Conference on Learn- ing Representations (ICLR), 2021.\n\nStyle tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. Yuxuan Wang, Daisy Stanton, Yu Zhang, Eric Skerry-Ryan, Battenberg, International Conference on Machine Learning (ICML). Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry- Ryan, Eric Battenberg, et al., \"Style tokens: Unsuper- vised style modeling, control and transfer in end-to-end speech synthesis,\" in International Conference on Ma- chine Learning (ICML), 2018.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, \"Bert: Pre-training of deep bidi- rectional transformers for language understanding,\" in Annual Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies (NAACL), 2019, pp. 4171- 4186.\n\nToken-level ensemble distillation for grapheme-to-phoneme conversion. Hao Sun, Xu Tan, Jun-Wei Gan, Hongzhi Liu, Sheng Zhao, Conference of the International Speech Communication Association (Interspeech). Hao Sun, Xu Tan, Jun-Wei Gan, Hongzhi Liu, Sheng Zhao, et al., \"Token-level ensemble distillation for grapheme-to-phoneme conversion,\" in Conference of the International Speech Communication Association (Interspeech), 2019.\n\nP-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Annual Meeting of the Association for Computational Linguistics (ACL). Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, et al., \"P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks,\" in Annual Meeting of the Association for Computational Linguistics (ACL), 2022, pp. 61-68.\n\nSimbert: Integrating retrieval and generation into bert. Jianlin Su, Tech. Rep.Jianlin Su, \"Simbert: Integrating retrieval and genera- tion into bert,\" Tech. Rep., 2020.\n\nLibritts: A corpus derived from librispeech for text-to-speech. Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Conference of the International Speech Communication Association. InterspeechHeiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, et al., \"Libritts: A corpus derived from lib- rispeech for text-to-speech,\" in Conference of the In- ternational Speech Communication Association (Inter- speech), 2019, pp. 1526-1530.\n\nHifigan: Generative adversarial networks for efficient and high fidelity speech synthesis. Jungil Kong, Jaehyeon Kim, Jaekyoung Bae, Conference and Workshop on Neural Information Processing Systems (NIPS). 2020Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae, \"Hifi- gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\" in Conference and Work- shop on Neural Information Processing Systems (NIPS), 2020.\n\nAn objective measure for estimating mos of synthesized speech. Chu Min, Peng Hu, Conference of the International Speech Communication Association (Interspeech). Chu Min and Peng Hu, \"An objective measure for es- timating mos of synthesized speech,\" in Conference of the International Speech Communication Association (Interspeech), 2001.\n\nMultimedia analysis, processing and communications. C Philipos, Loizou, Speech quality assessmentPhilipos C Loizou, \"Speech quality assessment,\" Mul- timedia analysis, processing and communications, pp. 623-654, 2011.\n", "annotations": {"author": "[{\"end\":222,\"start\":65},{\"end\":359,\"start\":223},{\"end\":511,\"start\":360},{\"end\":666,\"start\":512},{\"end\":797,\"start\":667}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":73},{\"end\":235,\"start\":231},{\"end\":368,\"start\":366},{\"end\":522,\"start\":518},{\"end\":673,\"start\":670}]", "author_first_name": "[{\"end\":72,\"start\":65},{\"end\":230,\"start\":223},{\"end\":365,\"start\":360},{\"end\":517,\"start\":512},{\"end\":669,\"start\":667}]", "author_affiliation": "[{\"end\":221,\"start\":100},{\"end\":358,\"start\":237},{\"end\":510,\"start\":389},{\"end\":665,\"start\":544},{\"end\":796,\"start\":675}]", "title": "[{\"end\":62,\"start\":1},{\"end\":859,\"start\":798}]", "venue": null, "abstract": "[{\"end\":2260,\"start\":906}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2392,\"start\":2389},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2394,\"start\":2392},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2396,\"start\":2394},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2398,\"start\":2396},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2415,\"start\":2412},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2430,\"start\":2427},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2555,\"start\":2552},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2557,\"start\":2555},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2752,\"start\":2749},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2815,\"start\":2811},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2859,\"start\":2855},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2967,\"start\":2963},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3020,\"start\":3016},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6271,\"start\":6267},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7225,\"start\":7221},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7405,\"start\":7401},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7756,\"start\":7752},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7862,\"start\":7858},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9319,\"start\":9315},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9732,\"start\":9728},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9989,\"start\":9988},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10224,\"start\":10220},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10331,\"start\":10327},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10629,\"start\":10625},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10850,\"start\":10846},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11150,\"start\":11146},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11278,\"start\":11274},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11509,\"start\":11508},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12211,\"start\":12207},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13042,\"start\":13038},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13100,\"start\":13096},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14306,\"start\":14302},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14566,\"start\":14562},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14692,\"start\":14688}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":15832,\"start\":15720},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":16240,\"start\":15833},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":16383,\"start\":16241},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":16520,\"start\":16384},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":16934,\"start\":16521},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":17498,\"start\":16935},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":17991,\"start\":17499},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":18352,\"start\":17992}]", "paragraph": "[{\"end\":3374,\"start\":2276},{\"end\":3496,\"start\":3376},{\"end\":4167,\"start\":3498},{\"end\":4349,\"start\":4169},{\"end\":5555,\"start\":4351},{\"end\":6157,\"start\":5583},{\"end\":7054,\"start\":6175},{\"end\":8091,\"start\":7074},{\"end\":8502,\"start\":8110},{\"end\":9492,\"start\":8514},{\"end\":10094,\"start\":9494},{\"end\":10630,\"start\":10118},{\"end\":10854,\"start\":10632},{\"end\":10998,\"start\":10874},{\"end\":11279,\"start\":11000},{\"end\":11905,\"start\":11301},{\"end\":12815,\"start\":11917},{\"end\":13202,\"start\":12851},{\"end\":13570,\"start\":13204},{\"end\":14232,\"start\":13595},{\"end\":14990,\"start\":14251},{\"end\":15719,\"start\":15005}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":3538,\"start\":3531},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":9884,\"start\":9877},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":11714,\"start\":11707},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12274,\"start\":12267},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":13162,\"start\":13155},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":13755,\"start\":13748},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":14795,\"start\":14788}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2274,\"start\":2262},{\"attributes\":{\"n\":\"2.\"},\"end\":5564,\"start\":5558},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5581,\"start\":5567},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6173,\"start\":6160},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7072,\"start\":7057},{\"attributes\":{\"n\":\"2.4.\"},\"end\":8108,\"start\":8094},{\"attributes\":{\"n\":\"3.\"},\"end\":8512,\"start\":8505},{\"attributes\":{\"n\":\"4.1.\"},\"end\":10116,\"start\":10097},{\"attributes\":{\"n\":\"4.2.\"},\"end\":10872,\"start\":10857},{\"attributes\":{\"n\":\"4.3.\"},\"end\":11299,\"start\":11282},{\"attributes\":{\"n\":\"4.4.\"},\"end\":11915,\"start\":11908},{\"attributes\":{\"n\":\"4.4.1.\"},\"end\":12830,\"start\":12818},{\"attributes\":{\"n\":\"4.4.2.\"},\"end\":12849,\"start\":12833},{\"attributes\":{\"n\":\"4.4.3.\"},\"end\":13593,\"start\":13573},{\"attributes\":{\"n\":\"4.4.4.\"},\"end\":14249,\"start\":14235},{\"attributes\":{\"n\":\"5.\"},\"end\":15003,\"start\":14993},{\"end\":15729,\"start\":15721},{\"end\":15843,\"start\":15834},{\"end\":16251,\"start\":16242},{\"end\":16394,\"start\":16385},{\"end\":16531,\"start\":16522},{\"end\":16945,\"start\":16936},{\"end\":17509,\"start\":17500},{\"end\":18002,\"start\":17993}]", "table": "[{\"end\":16240,\"start\":15936},{\"end\":16383,\"start\":16309},{\"end\":16520,\"start\":16450},{\"end\":16934,\"start\":16536},{\"end\":17498,\"start\":16950},{\"end\":17991,\"start\":17748},{\"end\":18352,\"start\":18064}]", "figure_caption": "[{\"end\":15832,\"start\":15731},{\"end\":15936,\"start\":15845},{\"end\":16309,\"start\":16253},{\"end\":16450,\"start\":16396},{\"end\":16536,\"start\":16533},{\"end\":16950,\"start\":16947},{\"end\":17748,\"start\":17511},{\"end\":18064,\"start\":18004}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5601,\"start\":5595}]", "bib_author_first_name": "[{\"end\":18742,\"start\":18739},{\"end\":18756,\"start\":18749},{\"end\":18766,\"start\":18762},{\"end\":18774,\"start\":18771},{\"end\":18783,\"start\":18781},{\"end\":19033,\"start\":19025},{\"end\":19047,\"start\":19041},{\"end\":19054,\"start\":19052},{\"end\":19067,\"start\":19060},{\"end\":19081,\"start\":19074},{\"end\":19325,\"start\":19320},{\"end\":19338,\"start\":19332},{\"end\":19353,\"start\":19346},{\"end\":19368,\"start\":19359},{\"end\":19386,\"start\":19381},{\"end\":19703,\"start\":19699},{\"end\":19714,\"start\":19710},{\"end\":19729,\"start\":19724},{\"end\":19741,\"start\":19737},{\"end\":19752,\"start\":19746},{\"end\":19766,\"start\":19758},{\"end\":20069,\"start\":20066},{\"end\":20071,\"start\":20070},{\"end\":20087,\"start\":20079},{\"end\":20098,\"start\":20094},{\"end\":20113,\"start\":20106},{\"end\":20128,\"start\":20123},{\"end\":20540,\"start\":20534},{\"end\":20557,\"start\":20549},{\"end\":20572,\"start\":20568},{\"end\":20586,\"start\":20581},{\"end\":20596,\"start\":20592},{\"end\":20796,\"start\":20794},{\"end\":20805,\"start\":20802},{\"end\":20816,\"start\":20811},{\"end\":20831,\"start\":20824},{\"end\":21009,\"start\":21007},{\"end\":21021,\"start\":21015},{\"end\":21033,\"start\":21028},{\"end\":21043,\"start\":21039},{\"end\":21054,\"start\":21050},{\"end\":21069,\"start\":21062},{\"end\":21077,\"start\":21075},{\"end\":21091,\"start\":21084},{\"end\":21105,\"start\":21098},{\"end\":21113,\"start\":21110},{\"end\":21407,\"start\":21402},{\"end\":21421,\"start\":21413},{\"end\":21429,\"start\":21426},{\"end\":21818,\"start\":21810},{\"end\":21830,\"start\":21824},{\"end\":21845,\"start\":21836},{\"end\":21856,\"start\":21851},{\"end\":21873,\"start\":21862},{\"end\":22142,\"start\":22136},{\"end\":22156,\"start\":22148},{\"end\":22168,\"start\":22162},{\"end\":22182,\"start\":22174},{\"end\":22198,\"start\":22188},{\"end\":22597,\"start\":22595},{\"end\":22609,\"start\":22603},{\"end\":22616,\"start\":22614},{\"end\":22625,\"start\":22622},{\"end\":22636,\"start\":22631},{\"end\":23004,\"start\":22998},{\"end\":23016,\"start\":23011},{\"end\":23028,\"start\":23026},{\"end\":23040,\"start\":23036},{\"end\":23451,\"start\":23446},{\"end\":23468,\"start\":23460},{\"end\":23482,\"start\":23476},{\"end\":23496,\"start\":23488},{\"end\":24031,\"start\":24028},{\"end\":24039,\"start\":24037},{\"end\":24052,\"start\":24045},{\"end\":24065,\"start\":24058},{\"end\":24076,\"start\":24071},{\"end\":24474,\"start\":24470},{\"end\":24487,\"start\":24480},{\"end\":24499,\"start\":24492},{\"end\":24512,\"start\":24504},{\"end\":24527,\"start\":24518},{\"end\":24913,\"start\":24906},{\"end\":25089,\"start\":25084},{\"end\":25099,\"start\":25095},{\"end\":25109,\"start\":25106},{\"end\":25119,\"start\":25117},{\"end\":25130,\"start\":25127},{\"end\":25132,\"start\":25131},{\"end\":25556,\"start\":25550},{\"end\":25571,\"start\":25563},{\"end\":25586,\"start\":25577},{\"end\":25961,\"start\":25958},{\"end\":25971,\"start\":25967},{\"end\":26287,\"start\":26286}]", "bib_author_last_name": "[{\"end\":18747,\"start\":18743},{\"end\":18760,\"start\":18757},{\"end\":18769,\"start\":18767},{\"end\":18779,\"start\":18775},{\"end\":18788,\"start\":18784},{\"end\":19039,\"start\":19034},{\"end\":19050,\"start\":19048},{\"end\":19058,\"start\":19055},{\"end\":19072,\"start\":19068},{\"end\":19086,\"start\":19082},{\"end\":19330,\"start\":19326},{\"end\":19344,\"start\":19339},{\"end\":19357,\"start\":19354},{\"end\":19379,\"start\":19369},{\"end\":19394,\"start\":19387},{\"end\":19708,\"start\":19704},{\"end\":19722,\"start\":19715},{\"end\":19735,\"start\":19730},{\"end\":19744,\"start\":19742},{\"end\":19756,\"start\":19753},{\"end\":19775,\"start\":19767},{\"end\":20077,\"start\":20072},{\"end\":20092,\"start\":20088},{\"end\":20104,\"start\":20099},{\"end\":20121,\"start\":20114},{\"end\":20135,\"start\":20129},{\"end\":20145,\"start\":20137},{\"end\":20547,\"start\":20541},{\"end\":20566,\"start\":20558},{\"end\":20579,\"start\":20573},{\"end\":20590,\"start\":20587},{\"end\":20601,\"start\":20597},{\"end\":20800,\"start\":20797},{\"end\":20809,\"start\":20806},{\"end\":20822,\"start\":20817},{\"end\":20835,\"start\":20832},{\"end\":21013,\"start\":21010},{\"end\":21026,\"start\":21022},{\"end\":21037,\"start\":21034},{\"end\":21048,\"start\":21044},{\"end\":21060,\"start\":21055},{\"end\":21073,\"start\":21070},{\"end\":21082,\"start\":21078},{\"end\":21096,\"start\":21092},{\"end\":21108,\"start\":21106},{\"end\":21116,\"start\":21114},{\"end\":21411,\"start\":21408},{\"end\":21424,\"start\":21422},{\"end\":21432,\"start\":21430},{\"end\":21822,\"start\":21819},{\"end\":21834,\"start\":21831},{\"end\":21849,\"start\":21846},{\"end\":21860,\"start\":21857},{\"end\":21877,\"start\":21874},{\"end\":22146,\"start\":22143},{\"end\":22160,\"start\":22157},{\"end\":22172,\"start\":22169},{\"end\":22186,\"start\":22183},{\"end\":22202,\"start\":22199},{\"end\":22601,\"start\":22598},{\"end\":22612,\"start\":22610},{\"end\":22620,\"start\":22617},{\"end\":22629,\"start\":22626},{\"end\":22641,\"start\":22637},{\"end\":23009,\"start\":23005},{\"end\":23024,\"start\":23017},{\"end\":23034,\"start\":23029},{\"end\":23052,\"start\":23041},{\"end\":23064,\"start\":23054},{\"end\":23458,\"start\":23452},{\"end\":23474,\"start\":23469},{\"end\":23486,\"start\":23483},{\"end\":23506,\"start\":23497},{\"end\":24035,\"start\":24032},{\"end\":24043,\"start\":24040},{\"end\":24056,\"start\":24053},{\"end\":24069,\"start\":24066},{\"end\":24081,\"start\":24077},{\"end\":24478,\"start\":24475},{\"end\":24490,\"start\":24488},{\"end\":24502,\"start\":24500},{\"end\":24516,\"start\":24513},{\"end\":24530,\"start\":24528},{\"end\":24916,\"start\":24914},{\"end\":25093,\"start\":25090},{\"end\":25104,\"start\":25100},{\"end\":25115,\"start\":25110},{\"end\":25125,\"start\":25120},{\"end\":25138,\"start\":25133},{\"end\":25561,\"start\":25557},{\"end\":25575,\"start\":25572},{\"end\":25590,\"start\":25587},{\"end\":25965,\"start\":25962},{\"end\":25974,\"start\":25972},{\"end\":26296,\"start\":26288},{\"end\":26304,\"start\":26298}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":18961,\"start\":18636},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":235490263},\"end\":19270,\"start\":18963},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1563370},\"end\":19661,\"start\":19272},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":219781060},\"end\":20025,\"start\":19663},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":218971783},\"end\":20466,\"start\":20027},{\"attributes\":{\"id\":\"b5\"},\"end\":20755,\"start\":20468},{\"attributes\":{\"id\":\"b6\"},\"end\":20928,\"start\":20757},{\"attributes\":{\"id\":\"b7\"},\"end\":21325,\"start\":20930},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":246863474},\"end\":21719,\"start\":21327},{\"attributes\":{\"id\":\"b9\"},\"end\":22054,\"start\":21721},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":235670101},\"end\":22530,\"start\":22056},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219531522},\"end\":22900,\"start\":22532},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4349820},\"end\":23362,\"start\":22902},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52967399},\"end\":23956,\"start\":23364},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":102352294},\"end\":24386,\"start\":23958},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":248780177},\"end\":24847,\"start\":24388},{\"attributes\":{\"id\":\"b16\"},\"end\":25018,\"start\":24849},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":102352475},\"end\":25457,\"start\":25020},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":222291664},\"end\":25893,\"start\":25459},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14872883},\"end\":26232,\"start\":25895},{\"attributes\":{\"id\":\"b20\"},\"end\":26451,\"start\":26234}]", "bib_title": "[{\"end\":19023,\"start\":18963},{\"end\":19318,\"start\":19272},{\"end\":19697,\"start\":19663},{\"end\":20064,\"start\":20027},{\"end\":21400,\"start\":21327},{\"end\":22134,\"start\":22056},{\"end\":22593,\"start\":22532},{\"end\":22996,\"start\":22902},{\"end\":23444,\"start\":23364},{\"end\":24026,\"start\":23958},{\"end\":24468,\"start\":24388},{\"end\":25082,\"start\":25020},{\"end\":25548,\"start\":25459},{\"end\":25956,\"start\":25895}]", "bib_author": "[{\"end\":18749,\"start\":18739},{\"end\":18762,\"start\":18749},{\"end\":18771,\"start\":18762},{\"end\":18781,\"start\":18771},{\"end\":18790,\"start\":18781},{\"end\":19041,\"start\":19025},{\"end\":19052,\"start\":19041},{\"end\":19060,\"start\":19052},{\"end\":19074,\"start\":19060},{\"end\":19088,\"start\":19074},{\"end\":19332,\"start\":19320},{\"end\":19346,\"start\":19332},{\"end\":19359,\"start\":19346},{\"end\":19381,\"start\":19359},{\"end\":19396,\"start\":19381},{\"end\":19710,\"start\":19699},{\"end\":19724,\"start\":19710},{\"end\":19737,\"start\":19724},{\"end\":19746,\"start\":19737},{\"end\":19758,\"start\":19746},{\"end\":19777,\"start\":19758},{\"end\":20079,\"start\":20066},{\"end\":20094,\"start\":20079},{\"end\":20106,\"start\":20094},{\"end\":20123,\"start\":20106},{\"end\":20137,\"start\":20123},{\"end\":20147,\"start\":20137},{\"end\":20549,\"start\":20534},{\"end\":20568,\"start\":20549},{\"end\":20581,\"start\":20568},{\"end\":20592,\"start\":20581},{\"end\":20603,\"start\":20592},{\"end\":20802,\"start\":20794},{\"end\":20811,\"start\":20802},{\"end\":20824,\"start\":20811},{\"end\":20837,\"start\":20824},{\"end\":21015,\"start\":21007},{\"end\":21028,\"start\":21015},{\"end\":21039,\"start\":21028},{\"end\":21050,\"start\":21039},{\"end\":21062,\"start\":21050},{\"end\":21075,\"start\":21062},{\"end\":21084,\"start\":21075},{\"end\":21098,\"start\":21084},{\"end\":21110,\"start\":21098},{\"end\":21118,\"start\":21110},{\"end\":21413,\"start\":21402},{\"end\":21426,\"start\":21413},{\"end\":21434,\"start\":21426},{\"end\":21824,\"start\":21810},{\"end\":21836,\"start\":21824},{\"end\":21851,\"start\":21836},{\"end\":21862,\"start\":21851},{\"end\":21879,\"start\":21862},{\"end\":22148,\"start\":22136},{\"end\":22162,\"start\":22148},{\"end\":22174,\"start\":22162},{\"end\":22188,\"start\":22174},{\"end\":22204,\"start\":22188},{\"end\":22603,\"start\":22595},{\"end\":22614,\"start\":22603},{\"end\":22622,\"start\":22614},{\"end\":22631,\"start\":22622},{\"end\":22643,\"start\":22631},{\"end\":23011,\"start\":22998},{\"end\":23026,\"start\":23011},{\"end\":23036,\"start\":23026},{\"end\":23054,\"start\":23036},{\"end\":23066,\"start\":23054},{\"end\":23460,\"start\":23446},{\"end\":23476,\"start\":23460},{\"end\":23488,\"start\":23476},{\"end\":23508,\"start\":23488},{\"end\":24037,\"start\":24028},{\"end\":24045,\"start\":24037},{\"end\":24058,\"start\":24045},{\"end\":24071,\"start\":24058},{\"end\":24083,\"start\":24071},{\"end\":24480,\"start\":24470},{\"end\":24492,\"start\":24480},{\"end\":24504,\"start\":24492},{\"end\":24518,\"start\":24504},{\"end\":24532,\"start\":24518},{\"end\":24918,\"start\":24906},{\"end\":25095,\"start\":25084},{\"end\":25106,\"start\":25095},{\"end\":25117,\"start\":25106},{\"end\":25127,\"start\":25117},{\"end\":25140,\"start\":25127},{\"end\":25563,\"start\":25550},{\"end\":25577,\"start\":25563},{\"end\":25592,\"start\":25577},{\"end\":25967,\"start\":25958},{\"end\":25976,\"start\":25967},{\"end\":26298,\"start\":26286},{\"end\":26306,\"start\":26298}]", "bib_venue": "[{\"end\":18737,\"start\":18636},{\"end\":19095,\"start\":19088},{\"end\":19447,\"start\":19396},{\"end\":19828,\"start\":19777},{\"end\":20224,\"start\":20147},{\"end\":20532,\"start\":20468},{\"end\":20792,\"start\":20757},{\"end\":21005,\"start\":20930},{\"end\":21501,\"start\":21434},{\"end\":21808,\"start\":21721},{\"end\":22282,\"start\":22204},{\"end\":22702,\"start\":22643},{\"end\":23117,\"start\":23066},{\"end\":23641,\"start\":23508},{\"end\":24161,\"start\":24083},{\"end\":24601,\"start\":24532},{\"end\":24904,\"start\":24849},{\"end\":25204,\"start\":25140},{\"end\":25663,\"start\":25592},{\"end\":26054,\"start\":25976},{\"end\":26284,\"start\":26234}]"}}}, "year": 2023, "month": 12, "day": 17}