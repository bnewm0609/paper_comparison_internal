{"id": 220934367, "updated": "2023-04-27 13:18:36.252", "metadata": {"title": "U2Fusion: A Uni\ufb01ed Unsupervised Image Fusion Network", "authors": "[{\"first\":\"Han\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Jiayi\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Junjun\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Xiaojie\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Haibin\",\"last\":\"Ling\",\"middle\":[]}]", "venue": "IEEE transactions on pattern analysis and machine intelligence", "journal": "IEEE transactions on pattern analysis and machine intelligence", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "\u2014This study proposes a novel uni\ufb01ed and unsupervised end-to-end image fusion network, termed as U2Fusion , which is capable of solving different fusion problems, including multi-modal, multi-exposure, and multi-focus cases. Using feature extraction and information measurement, U2Fusion automatically estimates the importance of corresponding source images and comes up with adaptive information preservation degrees. Hence, different fusion tasks are uni\ufb01ed in the same framework. Based on the adaptive degrees, a network is trained to preserve the adaptive similarity between the fusion result and source images. Therefore, the stumbling blocks in applying deep learning for image fusion, e.g. , the requirement of ground-truth and speci\ufb01cally designed metrics, are greatly mitigated. By avoiding the loss of previous fusion capabilities when training a single model for different tasks sequentially, we obtain a uni\ufb01ed model that is applicable to multiple fusion tasks. Moreover, a new aligned infrared and visible image dataset, RoadScene (available at https://github.com/hanna-xu/RoadScene), is released to provide a new option for benchmark evaluation. Qualitative and quantitative experimental results on three typical image fusion tasks validate the effectiveness and universality of U2Fusion. Our code is publicly available at https://github.com/hanna-xu/U2Fusion.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "3046194589", "acl": null, "pubmed": "32750838", "pubmedcentral": null, "dblp": "journals/pami/XuMJGL22", "doi": "10.1109/tpami.2020.3012548"}}, "content": {"source": {"pdf_hash": "353266126f5971832a5360f493f25f3289ae238b", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "790333549249c0faa8366540031de7fc75f28181", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/353266126f5971832a5360f493f25f3289ae238b.txt", "contents": "\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 U2Fusion: A Unified Unsupervised Image Fusion Network\n\n\nHan Xu \nJiayi Ma \nJunjun Jiang \nXiaojie Guo \nHaibin Ling \nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 U2Fusion: A Unified Unsupervised Image Fusion Network\nIndex Terms-Image fusionunified modelunsupervised learningcontinual learning\nThis study proposes a novel unified and unsupervised end-to-end image fusion network, termed as U2Fusion, which is capable of solving different fusion problems, including multi-modal, multi-exposure, and multi-focus cases. Using feature extraction and information measurement, U2Fusion automatically estimates the importance of corresponding source images and comes up with adaptive information preservation degrees. Hence, different fusion tasks are unified in the same framework. Based on the adaptive degrees, a network is trained to preserve the adaptive similarity between the fusion result and source images. Therefore, the stumbling blocks in applying deep learning for image fusion, e.g., the requirement of ground-truth and specifically designed metrics, are greatly mitigated. By avoiding the loss of previous fusion capabilities when training a single model for different tasks sequentially, we obtain a unified model that is applicable to multiple fusion tasks. Moreover, a new aligned infrared and visible image dataset, RoadScene (available at https://github.com/hanna-xu/RoadScene), is released to provide a new option for benchmark evaluation. Qualitative and quantitative experimental results on three typical image fusion tasks validate the effectiveness and universality of U2Fusion. Our code is publicly available at https://github.com/hanna-xu/U2Fusion.\n\nINTRODUCTION\n\nI MAGE fusion has a wide variety of applications, ranging from security to industrial and civilian fields [1], [2]. With the limitation of hardware devices or optical imaging, an image captured with one type of sensor or one single shooting setting can merely capture a part of the information. For instance, information of reflected lighting, with brightness in a limited range and within a predefined depth-of-field, is a typical representation of incomplete information. The target of image fusion is to generate a synthesized image by integrating complementary information from several source images that are captured with different sensors or optical settings. A schematic illustration of different image fusion tasks is shown in Fig. 1. A single fusion image with superior scene representation and better visual perception is suitable for subsequent visual tasks, such as video surveillance, scene understanding, and target recognition, etc. [3], [4].\n\nTypically, image fusion operates on multi-modal, multiexposure, or multi-focus images. To solve these problems, a large number of algorithms have been developed. They can be roughly divided into two categories: those based on a traditional fusion framework and those based on end-toend models [9]. Although these algorithms have achieved promising results in their respective fusion tasks, some problems remain to be solved. In methods based on the traditional fusion framework, the finite choices of fusion rules and the complexity of manual design limit the improvement of the performance. In end-to-end models, the fusion problem is solved by relying on ground truth for supervised learning or the specifically designed metrics for unsupervised learning. However, universal ground truth or noreference metric for multiple tasks does not exist. These issues form the major stumbling blocks in the unity of models and the application of supervised or unsupervised learning.\n\nMeanwhile, different fusion tasks often share the similar goal, that is, to synthesize an image by integrating vital and complementary information from several source images. Nevertheless, in different tasks, the vital information to be integrated varies largely as source images are of different types (see detailed explanation in Sec. 3.1), thus limiting the effectiveness of most methods to specific tasks. With the strong ability of feature representation in neural networks, the varied information can be represented in a unified way. It potentially leads to a unified fusion framework, which will be explored in this study.\n\nMoreover, by solving different fusion problems in a unified model, these tasks can promote one another. For instance, given that the unified model has been trained for multiexposure image fusion, it is capable of improving the fusion performance of under/overexposed regions in the multimodal or multi-focus images. Thus, by gathering the strengths of multiple tasks, the unified model can achieve better results for each single fusion task with stronger generalization than multiple individually trained models.\n\nTo address these issues, we propose a unified unsupervised image fusion network known as U2Fusion. For information preservation, a feature extractor is first adopted to extract abundant and comprehensive features from source images. Then, the richness of information in features is measured to define the relative importance of these features, which indi-  [5], U2Fusion, NSCT [6], U2Fusion, Deepfuse [7], U2Fusion, GFDF [8] and U2Fusion).\n\ncates the similarity relationship between the source images and the fusion result. A higher similarity entails that more information in this source image is preserved in the result, thus leading to a higher information preservation degree.\n\nOn the basis of these strategies, a DenseNet [10] module is trained to generate the fusion result without the need for ground truth. The characteristics and contributions of our work are summarized as follows:\n\n\u2022 We propose a unified framework for various image fusion tasks. More concretely, we solve different fusion problems with a unified model and unified parameters. Our solution alleviates shortcomings, such as separate solutions for different problems, storage and computation issues for training, and catastrophic forgetting for continual learning. \u2022 We develop a new unsupervised network for image fusion by constraining the similarity between the fusion image and source images to overcome the universal stumbling blocks in most image fusion problems, i.e., the lack of universal ground truth and no-reference metric.\n\n\n\u2022\n\nWe release a new aligned infrared and visible image dataset, RoadScene, to provide a new option for image fusion benchmark evaluation. It is made available at https://github.com/hanna-xu/RoadScene.\n\n\n\u2022\n\nWe test the proposed method on six datasets for multi-modal, multi-exposure, and multi-focus image fusions. Qualitative and quantitative results validate the effectiveness and universality of U2Fusion.\n\nA preliminary version of this paper appears in [11]. The new contributions are mainly from four aspects. First, the strategy for information preservation degree assignment is improved. Instead of the amount and quality of information in original source images, the information preservation degrees are assigned by the information measurement performed on extracted features. By considering additional aspects, the modified strategy provides an improved comprehensive measurement to capture the essential characteristics of source images. Second, the loss function is modified. The removal of the gradient loss alleviates the false edges, and the added pixel intensity-based loss helps reduce the luminance deviation in the fusion image. Third, we replace the first task from visible (VIS) and infrared (IR) image fusion to multi-modal image fusion where VIS-IR and medical image fusion are included. Lastly, we validate U2Fusion on additional publicly available datasets. For the ablation study, to validate the effectiveness of elastic weight consolidation (EWC) for continual learning from new tasks [12], we analyze the EWC from two additional aspects, namely, the statistical distributions of the weight for EWC and the intermediate results of all the tasks during the training phase. As for the adaptive information preservation degrees, the validation of their effectiveness is also performed.\n\n\nRELATED WORK\n\n\nImage Fusion Methods\n\n\nMethods Based on Traditional Fusion Framework\n\nThe traditional fusion framework can be roughly summarized as Fig. 2. As reconstruction is usually an inverse process of extraction, the key to these algorithms lies in two important factors: feature extraction and feature fusion. By modifying them, these methods can be designed for solving multi-modal, multi-exposure, or multi-focus image fusion.\n\nTo solve the issue of feature extraction, a large number of traditional methods have been proposed. The theories on which they are based can be divided into four representative categories: i) multi-scale transform, such as Laplacian pyramid (LP), ratio of low-pass pyramid (RP), gradient pyramid (GP), discrete wavelet (DWT), discrete cosine (DCT) [13], curvelet transform (CVT), shearlet, etc.; ii) sparse representation [14]; iii) subspace analysis, e.g., independent component analysis (ICA), principal component analysis (PCA), nonnegative matrix factorization (NMF), etc.; and iv) hybrid methods. However, these manually designed extraction approaches make fusion methods increasingly complex, thus intensifying the difficulty of designing fusion rules. The extraction methods need to be modified correspondingly to solve different fusion tasks. Furthermore, much attention needs to be given to the appropriateness of extraction methods to ensure the completeness of features. To overcome these limitations, some methods introduce convolutional neural networks (CNN) in feature extraction, either as some subparts [15], [16] or as the entire part [17], [18].\n\nThen, the fusion rules are determined on the basis of extracted features. The commonly used rules include maximum, minimum, addition, l 1 -norm, etc. However, the limit choices of these manually designed fusion rules produce a glass ceiling on the performance improvement even in some CNN-based methods. Notably, there are some methods breaking away from the framework, such as the method based on gradient transfer and total variation minimization for VIS-IR image fusion [19], the multi-exposure image fusion method by optimizing a structural similarity index [20], and the method based on dense SIFT for multi-focus image fusion [21], etc. However, the algorithms or metrics on which these methods are based are dedicated to specific fusion tasks and may not generalize well.\n\n\nEnd-to-end Models\n\nTo avoid designing fusion rules, many deep learning-based algorithms have been put forward. Unlike the methods in Sec. 2.1.1, these methods are usually end-to-end models tailored to specific fusion tasks.\n\n\nMulti-modal Image Fusion.\n\nThe end-to-end models for multi-modal image fusion are typically designed for VIS and IR image fusion. Ma et al. proposed FusionGAN [5] by establishing an adversarial game between a generator and a discriminator to preserve the pixel intensity distribution in the IR image and details in the VIS image. Later, its variant [22] was proposed to sharpen the edges of thermal targets by introducing the target-enhancement loss. DDcGAN [23], [24] enhances the prominence of thermal targets by introducing the dual-discriminator architecture. However, the unique issue in VIS and IR image fusion is the preservation of the pixel intensity distribution and details, which does not apply to other fusion tasks. In addition, ground truth is usually not present in this type of task. Thus, it is the major obstacle in utilizing supervised learning in multi-modal image fusion.\n\nMulti-exposure Image Fusion. To solve this problem, some unsupervised methods have been put forward. Prabhakar et al. proposed Deepfuse [7], where the no-reference metric MEF-SSIM is adopted as the loss function. However, MEF-SSIM is especially designed for multi-exposure images by discarding the luminance component, as it is not significant in this problem. Nevertheless, it still plays an important role in other tasks. Thus, MEF-SSIM is not applicable to other problems. In some multi-exposure datasets, there are no ground truths for supervised learning.\n\nMulti-focus Image Fusion. For this problem, Liu et al. put forward a network to generate the focus map [25]. The predefined labels, which indicate whether they are highquality images or Gaussian blurred images, are used for supervised learning. Then, it was extended to a general image fusion framework [26]. Depending on the generalization, the model trained on multi-focus image fusion can be employed to solve other tasks. In addition, Guo et al. proposed FuseGAN [27] where the generator directly produces a binary focus mask and the discriminator attempts to distinguish the generated masks from the ground truths, which are synthesized by utilizing a normalized disk point spread function and separating the background and foreground. The focus maps/masks are significant for multifocus image fusion, whereas they are not necessary or even not applicable in other tasks. All these methods are based on supervised learning.\n\nOur method. By considering the abovementioned limitations, we propose a unified unsupervised image fusion network, which has the following characteristics. i) It is an end-to-end model not restricted by the limit of manually designed fusion rules. ii) It is a unified model for various fusion tasks instead of specific objectives, e.g., distinctive issues, the specificity of metrics, the need of binary masks, etc. iii) It is an unsupervised model without the need of ground truth. iv) By continuously learning to solve new tasks without losing old capabilities, it solves multiple tasks with unified parameters.\n\n\nContinual Learning\n\nIn a continual learning setting, the learning is considered as a sequence of tasks to be learned. During the training phase, the weights are adapted to new tasks without forgetting the previously learned ones. To avoid storing any training data from previously learned tasks, many algorithms based on elastic weight consolidation (EWC) are proposed [28], [29], which include a regularization term to force parameters to remain close to those trained for the previous tasks. These technologies have been widely applied in many practical problems, such as person reidentification [30], real-time vehicle detection [31], and emotion recognition [32], etc. In this study, we perform continual learning for solving multiple fusion tasks.\n\n\nMETHODOLOGY\n\nOur system allows signals captured with different sensors and/or shooting settings from the same camera position. In this section, we provide the problem formulation, the design of loss functions, the technology of elastic weight consolidation, and the network architecture.\n\n\nProblem Formulation\n\nFocusing on the primary goal of image fusion, i.e., to preserve the vital information in source images, our model is based on the measurement to determine the richness of such information. If the source image contains abundant information, it is of great importance to the fusion result, which should show a high similarity with the source image. Therefore, the key issue of our method is to explore a unified measurement to determine the information preservation degrees of source images. Rather than maximizing the similarity between the fusion result and the ground truth in supervised learning, our method depends on such degrees to preserve the adaptive similarity with source images. And, as an unsupervised model, it is applicable to multiple fusion problems where ground truth is hardly available.\n\nFor the desired measurement, a major problem is that the vital information in different types of source images varies greatly. For example, in IR and positron emission tomography (PET) images, the vital information is the thermal radiation and functional responses that are presented as the pixel intensity distribution. In VIS and magnetic resonance imaging (MRI) images, the vital information is the reflected light and structural content represented by image gradients [19], [23]. In multi-focus images, the information to be preserved includes the objects within the depth-offield (DoF). In multi-exposure images, the vital information concerns scene content can be enhanced. The above variability brings considerable difficulty to designing a unified information measurement, which are designed for the specific tasks cease to be effective when facing other problems. They are based on certain surface-level characteristics or specific properties while in different tasks, and are difficult to be predetermined in a unified way. We solve this problem by taking a comprehensive consideration of multifaceted properties of source images. To this end, we extract both shallow-level features (textures, local shapes, etc.) and deeplevel features (content, spatial structures, etc.) for estimating the information measurement.\n\nThe pipeline of U2Fusion is summarized as Fig. 3. With source images denoted as I 1 and I 2 , a DenseNet is trained to generate the fusion image I f . The outputs of feature extraction are the feature maps \u03c6 C1 (I 1 ), \u00b7 \u00b7 \u00b7 , \u03c6 C5 (I 1 ) and \u03c6 C1 (I 2 ), \u00b7 \u00b7 \u00b7 , \u03c6 C5 (I 2 ). Then the information measurement is performed on these feature maps, producing two measurements denoted by g I1 and g I2 . With subsequent processing, the final information preservation degrees are denoted as \u03c9 1 and \u03c9 2 . I 1 , I 2 , I f , \u03c9 1 and \u03c9 2 are used in the loss function without the need for ground truth. In the training phase, \u03c9 1 and \u03c9 2 are measured and applied in defining the loss function. Then, a DenseNet module is optimized to minimize the loss function. In the testing phase, \u03c9 1 and \u03c9 2 do not need to be measured, as the DenseNet has been optimized. The detailed definitions or descriptions are given in the following subsections.\n\n\nFeature Extraction\n\nCompared with models trained in fusion tasks, models for other computer vision tasks are usually trained with larger and more diversified datasets. Thus, features extracted by such models are abundant and comprehensive [33], [34]. Inspired by the perceptual loss [35], [36], we adopt the pretrained VGG-16 network [37] for feature extraction, as shown in Fig. 4. The input I has been unified in a single channel in our model (we will discuss this transformation in Sec. 3.5), and we duplicate it into three channels and then  Fig. 4. Perceptual feature maps extracted by VGG-16 for input image I, and \u03c6 C j (I) represents the feature map extracted by the convolutional layer before the j-th max-pooling layer. The last row is the shape of extracted feature maps in the form of [batchsize, height, width, channel].\n\nfeeding them into VGG-16. The outputs of the convolutional layers before max-pooling layers are feature maps for the subsequent information measurement, which are shown in Fig. 4 as \u03c6 C1 (I), \u00b7 \u00b7 \u00b7 , \u03c6 C5 (I) with their shapes shown below. For intuitive analysis, some feature maps of a multiexposure image pair are shown in Fig. 5. In the original source images, the overexposed image contains much more texture details or larger gradients than the underexposed image, as the latter suffers from much lower luminance. In Fig. 5, features in \u03c6 C1 (I) and \u03c6 C2 (I) are based on shallow features, such as textures and shape details. In these layers, feature maps of the overexposed image still shows more information than the underexposed one. By comparison, feature maps of higher layers, e.g., \u03c6 C4 (I) and \u03c6 C5 (I), mainly preserve deep-level features, such as the content or spatial structures. In these layers, comparable and additional information are present in the feature maps of the underexposed image. Therefore, the combination of shallow-and deep-level features forms a comprehensive representation of the essential information that may not be easily perceived by the human visual perception system.\n\n\nInformation Measurement\n\nTo measure the information contained in the extracted feature maps, their gradients are used for evaluation. Compared with entities derived from general information theory, image gradient is a metric based on local spatial structures with small receptive fields. When used in the deep learning framework, gradients are much more efficient in both computation and storage. Thus, they are more suitable for application in CNN for information measurement. The information measurement is defined as follows:\ng I = 1 5 5 j=1 1 H j W j D j Dj k=1 \u2207\u03c6 C k j (I) 2 F ,(1)\nwhere \u03c6 Cj (I) is the feature map by the convolutional layer before the j-th max-pooling layer in Fig. 4. k denotes the feature map in the k-th channel of D j channels. \u00b7 F denotes the Frobenius norm, and \u2207 is the Laplacian operator.\n\n\nInformation Preservation Degree\n\nTo preserve the information in source images, two adaptive weights are assigned as the information preservation degrees, which define the weights of similarities between the fusion image and the source images. The higher the weight, the higher the similarity is expected to be, and the higher the information preservation degree of the corresponding source image is. These adaptive weights, denoted as \u03c9 1 and \u03c9 2 , are estimated according to the information measurement results g I1 and g I2 obtained by Eq. (1). Given that the difference between g I1 and g I2 is the absolute value instead of the relative one, it may be too small compared with themselves to reflect their difference. Thus, to enhance and embody the difference in weights, a predefined positive constant c is used to scale values for better weight assignments. Thus, \u03c9 1 and \u03c9 2 are defined as:\n[\u03c9 1 , \u03c9 2 ] = softmax g I1 c , g I2 c ,(2)\nwhere we use the softmax function to map\ng I 1 c , g I 2 c\nto real numbers between 0 and 1, and guarantee that the sum of \u03c9 1 and \u03c9 2 is 1. Then, \u03c9 1 and \u03c9 2 are employed in the loss function to control the information preservation degrees of specific source images.\n\n\nLoss Function\n\nThe loss function is mainly designed for preserving vital information and for training a single model, which is applicable for multiple tasks. It consists of two parts defined as follows:\nL(\u03b8, D) = L sim (\u03b8, D) + \u03bbL ewc (\u03b8, D),(3)\nwhere \u03b8 denotes the parameters in DenseNet, and D is the training dataset. L sim (\u03b8, D) is the similarity loss between the result and source images. L ewc (\u03b8, D) is the item designed for continual learning, as described in next subsection. \u03bb is a hyperparameter to control the trade-off. We realize the similarity constraint from two aspects, i.e., the structure similarity and the intensity distribution. Given that the structural similarity index measure (SSIM) is the most widely used metric that models the distortion according to similarities in the information on light, contrast, and data1 data2 data3  Fig. 6. Illustration of joint training and sequential training. The dashed arrow between DenseNets means that it is kept and set as the initial parameters of the next task. On this basis, these parameters are optimized according to the new objective.\n\nstructure [38], we use it to constrain the structural similarity between I 1 , I 2 , and I f . Thus, with \u03c9 1 and \u03c9 2 to control the information degree, the first item of L sim (\u03b8, D) is formulated as:\nL ssim (\u03b8, D) = E[\u03c9 1 \u00b7 (1 \u2212 S I f ,I1 ) + \u03c9 2 \u00b7 (1 \u2212 S I f ,I2 )],(4)\nwhere S x,y denotes the SSIM value between two images. While SSIM focuses on the changes of contrast and structure, it shows weaker constraints on the difference of the intensity distribution. We supplement L ssim (\u03b8, D) with the second item, which is defined by the mean square error (MSE) between two images:\nL mse (\u03b8, D) = E[\u03c9 1 \u00b7 MSE I f ,I1 + \u03c9 2 \u00b7 MSE I f ,I2 ].(5)\nMeanwhile, the results obtained by constraining MSE suffer from relatively blurred appearance by averaging all plausible outputs, whereas SSIM can make up for this issue. Thus, these two items compensate for each other. With \u03b1 controlling the trade-off, L sim (\u03b8, D) is formulated as:\nL sim (\u03b8, D) = L ssim (\u03b8, D) + \u03b1L mse (\u03b8, D).(6)\n\nSingle Model for Multi-fusion Tasks with Elastic Weight Consolidation (EWC)\n\nVarious fusion tasks usually lead to differences in feature extraction and/or fusion, as directly reflected in diverse values of DenseNet parameters. It leads to training multiple models with the same architecture but diverse parameters. However, as some parameters are redundant, the utilization of these models can be greatly improved. It motivates us to train a single model with unified parameters that integrates these models and thus become applicable for multiple tasks. This purpose can be achieved in two ways, i.e., joint training and sequential training, as shown in Fig. 6. Joint training is a simple method where all the training data are kept throughout the training process. In each batch, data from multiple tasks are randomly selected for training. Nevertheless, as the number of tasks increases, two urgent issues become difficult to solve: i) the storage issue caused by always keeping the data of previous tasks and ii) the computation issue caused by using all the data for training, in terms of both the difficulty of computation and time cost.\n\nIn sequential training, we need to change the training data for different tasks, as shown in Fig. 6(b). Thus, only the data of the current task needs to be stored in the training process, which solves storage and computation issues. However, a new problem arises when we train the model on another task for a new capability: the previous training data are unavailable [39]. As the training process continues, the parameters are optimized to solve the new problems while losing the capacity learned from previous tasks. This problem is called catastrophic forgetting. To avoid this drawback, we apply the elastic weight consolidation (EWC) algorithm [12] to safeguard against it. In EWC, the squared distance between the parameter values of the current task \u03b8 and those of the previous task \u03b8 * are weighted according to their importance to \u03b8 * . Those important parameters are given higher weights to prevent forgetting what has been learned from old tasks, while the parameters with less importance can be modified to a greater extent to learn from the new task. In this way, the model is capable of continual learning with elastic weight consolidation. Thus, the loss for continual learning, termed as L ewc (\u03b8, D), is included in the total loss function in Eq. (3). With these importance-related weights defined as \u00b5 i , L ewc (\u03b8, D) is formulated as:\nL ewc (\u03b8, D) = 1 2 i \u00b5 i (\u03b8 i \u2212 \u03b8 * i ) 2 ,(7)\nwhere i represents the i-th parameter in the network and \u00b5 i represents the weight of corresponding squared distance.\n\nTo evaluate the importance, \u00b5 i is assigned as the diagonal terms of the Fisher information matrix and approximated by computing the square of gradients with the data in previous tasks as defined below:\n\u00b5 i = E ( \u2202 \u2202\u03b8 * i log p(D * |\u03b8 * )) 2 |\u03b8 * ,(8)\nwhere D * represents the data of previous tasks. log p(D * |\u03b8 * ) can be approximately replaced by \u2212L(\u03b8 * , D * ) [12]. Thus, Eq. (8) is converted to:\n\u00b5 i = E \u2212 \u2202 \u2202\u03b8 * i L(\u03b8 * , D * ) 2 \u03b8 * .(9)\nGiven that the Fisher information matrix can be computed before throwing away the old data D * , the model does not require D * for training the current task.\n\nIf several previous tasks exist, L ewc (\u03b8, D) is adapted according to specific tasks and corresponding data. Then, the squares of these gradients are averaged for the final \u00b5 i . The training process and the data flow are illustrated in Fig. 7.\n\nIn multi-task image fusion, \u03b8 is the parameters of the DenseNet. First, the DenseNet is trained to solve Task1, i.e., the multi-modal image fusion problem by minimizing the similarity loss defined in Eq. (6). When adding the capacity of solving Task2, i.e., the multi-exposure image fusion problem, the importance-related weights \u00b5 i are first computed. In particular, \u00b5 i indicates the importance of each parameter in the DenseNet to multi-modal image fusion. Then, the important parameters are consolidated to avoid catastrophic forgetting by minimizing the item L ewc in Eq. (3); while the parameters of little significance are updated to solve the multi-exposure image fusion by minimizing the similarity loss L sim correspondingly. Lastly, when we train the DenseNet on multi-focus image fusion, \u00b5 i is computed according to the previous two tasks. The subsequent elastic weight consolidation strategy is the same as before. In this way, EWC can be customized to the scenario of multi-task adaptive image fusion.\n\n\nNetwork Architecture\n\nIn our method, DenseNet is employed to generate the fusion result I f , of which the input is the concatenation of I 1 and I 2 . Thus, it is an end-to-end model without the need for designing fusion rules. As shown in Fig. 8, the architecture of DenseNet in U2Fusion consists of 10 layers, each with a convolution followed by an activation function. The kernel size of all convolutional layers is set to 3 \u00d7 3 and the stride to 1. Reflection padding is employed before the convolution to reduce boundary artifacts. No pooling layer is used to avoid information loss. The activation functions in the first nine layers are LeakyReLU with the slope set to 0.2, while that of the last layer is tanh.\n\nMoreover, research has proven that CNNs can be significantly deeper and trained efficiently if shorter connections are built between layers close to the input and those close to the output. Therefore, in the first seven layers, the densely connected blocks from densely connected CNNs [10] are employed to improve the information flow and performance. In these layers, shortcut direct connections are built between each layer and all layers in a feed-forward fashion, as shown in the concatenation operation in Fig. 8. This way, the problem of vanishing gradients can be reduced. Meanwhile, feature propagation can be further strengthened while reducing the number of parameters [40]. The channels of feature maps are all set to 44. The subsequent four layers reduce the channels of feature maps gradually until reaching a single-channel fusion result, as shown in Fig. 8.\n\n\nDealing with RGB Input\n\nRGB inputs are first converted into the YCbCr color space. Then, the Y (luminance) channel is used for fusion, as structural details are mainly in this channel and the brightness variation in this channel is more prominent than chrominance channels. Data in the Cb and Cr (chrominance) channels are fused traditionally as:\nC f = C 1 (|C 1 \u2212 \u03c4 |) + C 2 (|C 2 \u2212 \u03c4 |) |C 1 \u2212 \u03c4 | + |C 2 \u2212 \u03c4 | ,(10)\nwhere C 1 and C 2 are the Cb/Cr channel values of the first and second source image, respectively. C f is the corresponding channel of the fusion result. \u03c4 is set as 128. Then, through the inverse conversion, the fusion images can be converted into the RGB space. Thus, all the problems are unified into the single-channel image fusion problem. \n\n\nDealing with Multiple Inputs\n\nIn multi-exposure and multi-focus fusion, we need to fuse a source image sequence, i.e., more than two source images are available. In this case, these source images can be fused sequentially. As shown in Figs. 9 and 10, we initially fuse two of these source images. Then, the intermediate result is fused with another source image. In this way, U2Fusion is capable of fusing any number of inputs in theory.\n\n\nEXPERIMENTAL RESULTS AND DISCUSSIONS\n\nIn this section, we compare U2Fusion with several state-ofthe-art methods on multiple tasks with multiple datasets by qualitative and quantitative comparisons.\n\n\nTraining Details\n\nWe perform U2Fusion on three types of fusion tasks: i) multi-modal image fusion, including VIS-IR and medical image (PET-MRI) fusion; ii) multi-exposure image fusion; and iii) multi-focus image fusion. Given that VIS-IR and PET-MRI fusion are similar in nature (see detailed explanation in Sec. 3.1), they are jointly seen as multi-modal image fusion (task 1). The training datasets are from four publicly available datasets: RoadScene 1 (VIS-IR) and Harvard 2 (PET-MRI) for task 1, the dataset in [41] 3 for task 2, and Lytro 4 for task 3. To validate the universality, the test datasets also contain two additional ones: TNO 5 for VIS-IR image fusion and EMPA HDR 6 for multi-exposure image fusion. On the basis of the FLIR video 7 , we have released the RoadScene, which is a new aligned VIS-IR image dataset used to remedy shortcomings in existing ones. First, we select image pairs with highly repetitive scenes from the video. Second, the thermal noise in original IR images is reduced. Third, to align the image pairs accurately, we select feature points carefully and align each image pair with homography and bi-cubic interpolation. Moreover, given that some regions cannot be exactly aligned with homography because of camera distortion or imaging time elapse, we cut out the exact registration regions. RoadScene has 221 aligned image pairs containing rich scenes, such as roads, vehicles, and pedestrians. It solves the problems in benchmark datasets, such as few image pairs, low spatial resolution, and lack of detailed information in IR images.\n\nSource images in all the datasets are cropped to patches of size 64\u00d764. For multi-focus images, images are enlarged and flipped for additional training data because of the insufficient aligned image pairs. We set \u03b1 = 20 and \u03bb = 8e4. c is set as 3e3, 3.5e3, and 1e2, and the epoches are set as 3, 2, and 2 correspondingly. The parameters are updated by RMSPropOptimizer with a learning rate 1e-4. The batch size is 18. Experiments are performed on a NVIDIA Geforce GTX Titan X GPU and 3.4 GHz Intel Core i5-7500 CPU. \n\n\nMulti-modal Image Fusion\n\n\nVisible and Infrared Image Fusion\n\nWe compare U2Fusion with five state-of-the-art methods: HMSD [42], GTF [19], DenseFuse [17], FusionGAN [5], and DDcGAN [24]. The qualitative results on the TNO and RoadScene datasets are shown respectively in Figs. 11 and 12. Overall, U2Fusion exhibits a sharper appearance than its competitors. As shown in the highlighted regions, the competitors lose some details, e.g., cars, the logo, and the license plate. In comparison, U2Fusion alleviates this problem by presenting more details. Moreover, in the extreme case where little information is available in one of the source images, U2Fusion preserves the information in the other source image more completely in the fusion result, as shown in the last row in Fig. 11 and the first row in Fig. 12. Furthermore, U2Fusion is also applied to fuse VIS (RGB) and gray IR images in the RoadScene. As shown in Fig. 13, fusion results are more like VIS images enhanced by IR images for better scene representation because the fusion process is performed only on the Y channel and the chromatic information all come from VIS images. Quantitative comparisons are performed on the remaining 20 and 45 image pairs in TNO and RoadScene. Four metrics, namely, correlation coefficient (CC), SSIM, peak signalto-noise ratio (PSNR), and the sum of the correlations of differences (SCD) [43], are used for evaluation. CC measures the linear correlation degree between source images and the result. PSNR evaluates the distortion caused by the fusion process. SCD quantifies the quality of fusion images. As shown in Tab. 1, U2Fusion ranks first on CC, SSIM, and PSNR on both datasets. Although it ranks second on SCD, it achieves comparable results. The promising results show that U2Fusion achieves high fidelity with source images and less distortion, noise, or artifacts.\n\n\nMedical Image Fusion\n\nWe compare U2Fusion with RPCNN [44], CNN [16], PA-PCNN [45], and NSCT [6] on the Harvard dataset. As shown in Fig. 14, our results have more structural (texture) information under the premise of little loss of functional (color) information. The quantitative evaluation of four  \n\n\nMulti-exposure Image Fusion\n\nWe compare U2Fusion with GFF [46], DSIFT [47], GBM [48], Deepfuse [7], and FLER [49] on the more challenging problem where source images have a large exposure ratio our result, these representations are further enhanced with appropriate exposure. The local dark regions in GFF, DSIFT, and FLER are improved in U2Fusion. Moreover, compared with GBM and Deepfuse, our results are enriched with clearer details or higher contrast to provide better detail representation, as shown in the red boxes. Quantitative comparisons are performed on 30 and 15 image pairs in the dataset in [41] and the EMPA HDR dataset, respectively. In addition to SSIM, PSNR, and CC, an additional metric, edge intensity (EI), is used for evaluation. EI reflects the gradient amplitude of edge point. The mean and standard deviation are shown in Tab. 3. On the dataset in [41], U2Fusion achieves the optimal mean on SSIM and PSNR. The results on EI and CC follow behind FusionDN and Deepfuse by 0.02 and 0.011, respectively. On the EMPA HDR dataset, our mean on SSIM is the best one. For other metrics, U2Fusion achieves 0.037, 0.064, and 0.009, which are close to the best values. These results show that in U2Fusion, the similarity and correlation between the fusion image and source images are higher and have less distortion and larger gradient amplitude.\n\n\nMulti-focus Image Fusion\n\nWe compare our method with DSIFT [50], GBM [48], CNN [25], GFDF [8], and SESF-Fuse [18] with qualitative results shown in Fig. 17. Although U2Fusion does not use the ground truth for supervision nor does it extract and fill focused regions in fusion images, it still achieves comparable results. As shown in the first row, edges blurred at the boundary of focused and defocused regions are fused into results in the competitors. In U2Fusion, this phenomenon has been alleviated as it attempts to reconstruct the focused regions after judging their relative blurring relationship. The other difference is shown in the last two rows, in DSIFT, CNN, GFDF, and SESF-Fuse, at the boundary of focused and defocused regions. Some details in the far-focused images are lost, e.g., the golf and the edge of the ear. Although GBM retains these details, noticeable brightness and color deviations can be observed in the results. By comparison, U2Fusion preserves these details to a greater extent.\n\nMetrics for evaluation include EI, CC, visual information fidelity (VIF) [51], and mean gradient (MG). VIF measures the information fidelity by computing the distortion between source images and the fusion result. The larger MG, the more gradients the image contains and the better fusion performance. As shown in Tab. 4, U2Fusion achieves the optimal results on EI and CC. The best result on EI and the suboptimal result on MG indicate more gradients in our results for sharper appearance. The results are consistent with the qualitative results shown in Fig. 17. Moreover, TABLE 3 Mean and standard deviation of four metrics on multi-exposure image fusion on the dataset in [41] 1.953\u00b10.01 0.194\u00b10.07 55.992\u00b11.09 0.848\u00b10.05 1 the best result on CC and the optimal result on VIF show that U2Fusion maintains the highest linear correlation with source images and achieves comparable information fidelity.\n\n\nABLATION EXPERIMENTS\n\n\nAblation Study about EWC\n\nIn U2Fusion, we use EWC to train a single model for three fusion tasks to overcome catastrophic forgetting. To vali-date its effectiveness, we perform a comparison experiment where tasks are sequentially trained without EWC. The effectiveness is analyzed from three aspects: i) the similarity loss, ii) statistical distributions of \u00b5 i , and iii) intermediate fusion results during the training phase. Changes in the similarity loss, L sim (\u03b8, D) in Eq. (3), are shown in Fig. 18. The first plot is the similarity loss of each task without applying EWC, and the second plot is that with EWC. The difference is not evident between the losses of tasks 1 and 2. However, when training DenseNet on task 3 without EWC, the loss on the validation dataset of task 2 increases evidently. It indicates that the performance of the current network on multi-exposure image fusion is declining. With EWC, the similarity losses of previous tasks are basically the same as those when they were trained. Thus, by applying EWC, we can obtain a single model applicable to these tasks.\n\nWe also compare the statistical distributions of \u00b5 i with/without EWC, as shown in Fig. 19. \u00b5 i is computed by the similarity loss and corresponding datasets after each task has been trained. For example, the distribution after training task 3 is the statistical distribution of the mean \u00b5 i obtained by averaging \u00b5 i computed by the similarity loss and dataset of task 1 and those of task 2. Without EWC, not much difference is observed among the three  distributions of \u00b5 i obtained after three tasks, as shown in the first plot. The parameters are only related to the current task, as \u00b5 i only shows the importance of parameters to the current task. However, with EWC, the proportion of larger \u00b5 i has increased significantly. This increase shows that more important parameters are present in the network. These parameters are significant not only to the current task but also to previous ones. Meanwhile, the decreased proportion of small values also shows that the redundancy of the network is decreasing. An increasing number of parameters play an import role in improving the fusion performance. The intuitive qualitative comparisons of results with/without EWC are given in Fig. 20. After training the model on tasks 1 and 2, the models with and without EWC achieve satisfactory results on multi-modal and multiexposure image fusion. Given that it has not been trained on task 3, the results of multi-focus image fusion show blurred edges, as shown in the results of task 3 in Figs. 20 (a), (b), and (d). However, by training the model on task 3, the results exhibit a sharper appearance, as shown in the results of task 3 in Figs. 20 (d) and (e). When the model is trained without EWC, the performance on task 2 declines, e.g., the lower luminance of the whole image. Moreover, evident difference is observed between the results of task 1 in Figs. 20 (b) and (c). With EWC, these two problems have been alleviated, as shown in Figs. 20 (d) and (e).\n\n\nA Unified Model for Mutual Promotion between Different Tasks\n\nIn U2Fusion, we employ EWC to learn from new tasks continuously. In this way, the unified model is capable of fusing multiple types of source images. Thus, with unified parameters, the information learned by U2Fusion from a single task can promote other tasks. For verification, we train an individual model for each task. Thus, no interaction occurs among different tasks. The fusion results are shown in Fig. 21. Although multi-modal and multi-focus image fusions are different from multi-exposure image fusion, multi-modal and multi-focus images also have overexposed regions, which can be evidently seen from the visible images in the first three columns and the far-focused image in the last column. With a unified model that has been trained for multi-exposure image fusion, U2Fusion shows better performance for these overexposed regions with clearer representation than individual models. Another instance is shown in the results of multi-exposure image fusion, i.e., the sixth column. The highlighted regions in the source images are similar to multi-focus images. Given that the model has learned from multi-focus image fusion, the result of U2Fusion exhibits clearer and sharper edges than that of the individually trained model. Thus, by gathering the strengths of multiple tasks, U2Fusion obtains the strong generalization not only for multiple types of source images but also for multiple types of regions in the same type of source images. Therefore, a unified model can realize the mutual promotion of different fusion tasks.\n\n\nAblation Study about Adaptive Information Preservation Degrees\n\nTo validate the effectiveness of adaptive information preservation degrees, we perform the experiments where \u03c9 1 and \u03c9 2 are directly set to 0.5. The comparative results on the six datasets are shown in Fig. 22. The results in the first row are obtained when \u03c9 1 and \u03c9 2 are fixed to 0.5, and those in the second row are the results of U2Fusion. In multimodal image fusion, the results without adaptive information preservation degrees show worse detail representation,   as shown in the edges of the cloud, textures of the jeep, details of the net, and the structural information. In multiexposure image fusion, the difference is clearly seen in the overexposed regions. Without the adaptive degrees, these regions still look overexposed, such as the flower, window, and the sun. The phenomenon is most noticeable in the results of multi-focus image fusion. When \u03c9 1 and \u03c9 2 are directly set to 0.5, the network fails to distinguish between focused and defocused regions. Therefore, the results suffer from blurred edges, while U2Fusion generates a much sharper appearance.\n\n\nEffect of Training Order\n\nIn the three fusion tasks, multi-focus image fusion is a little different from multi-modal and multi-exposure image fusion. For multi-modal and multi-exposure image patches, the fusion patch can be seen as the combination of two source images. However, for multi-focus image patches, the fusion process can be seen as the selection of the focused regions in the source images. Thus, the fusion result is expected to exhibit a high similarity with source images in the focused region. Therefore, we perform two comparison experiments in this section. For quantitative comparison, we use the correlation coefficient (CC) to measure the correlation between the result and source images and mean gradient (MG) to measure the performance of the fusion results.\n\nOn the one hand, we change the order of multi-modal and multi-exposure image fusion. The training order is reset as multi-exposure\u2192multi-modal\u2192multi-focus image fusion. The qualitative results are shown in Fig. 23, and the quantitative results are shown in Tab. 5. As shown in the results, the exchange of the training orders of multi-modal and multi-exposure image fusion has little effect on fusing multi-focus images. For these two tasks, the results exhibit higher brightness and mean gradient. However, the results of the original training order maintain a higher correlation with the source images.\n\nOn the other hand, considering the difference between Fig. 22. Comparative qualitative results of our method without (the first row) and with (the second row) adaptive information preservation degrees. From left to right: fusion images of image pairs from the TNO, RoadScene, Harvard, the dataset in [41], EMPA HDR and Lytro datasets.   the multi-focus image fusion and the two other fusion tasks, we set multi-focus image fusion as the first task. Then, the training order is reset as multi-focus\u2192multi-modal\u2192multiexposure image fusion. Evidently, the result of multi-focus image fusion is more blurred than those of other orders, which can be seen from the rightmost column in Fig. 23. This phenomenon is also reflected by the substantially reduced mean gradient in Tab. 5, which drops from 0.0677 or 0.0700 to 0.0563. The ability of U2Fusion for continual learning benefits from L ewc (\u03b8, D) is defined in Eq. (7). Some unimportant parameters are updated to learn from new tasks, resulting in a slight performance degradation on the previous tasks. Given the particularity of multi-focus image fusion, the performance degradation is more evident, especially reflecting in the blurring of shape edges. Therefore, the training orders of multi-modal and multiexposure image fusion have little effect on the fusion results, while that of multi-focus has a relatively significant effect. Comparing the quantitative results in Tab. 5, the order of multi-modal\u2192multi-exposure\u2192multi-focus shows the best performance. Thus, we adopt it in U2Fusion.\nmulti-exposure \u2192 multi-modal \u2192 multi-focus multi-modal \u2192 multi-exposure \u2192 multi-focus multi-focus \u2192 multi-modal \u2192 multi-exposure\n\nU2Fusion vs. FusionDN\n\nThe preliminary version of the proposed method is Fu-sionDN [11], and the improvements are described in Sec. 1. To validate the effectiveness of these improvements, we compare the results of FusionDN and U2Fusion, as shown in Fig. 24.\n\nFirst, we improve the strategy for information preservation degree assignment by modifying the amount and quality of information in source images. The effect of this improvement is shown in the first and second columns in Fig. 24. Relying on the amount and quality information in original source images, FusionDN preserves the high contrast in VIS regions, such as that between the smoke and the background. Nevertheless, a considerable amount of details in the corresponding IR regions have been lost. In U2Fusion, by considering the information in abundant extracted features, the information preservation degrees are changed, and more details in source images are preserved.\n\nSecond, we modify the loss function by removing the gradient loss and adding the MSE loss. In FusionDN, the gradient loss is introduced to preserve more gradients. However, it causes some false edges, as in the results of FusionDN in the fourth and fifth columns. By removing it, we rely on SSIM and the improved information preservation degree assignment strategy to preserve the structural information. The results still show sharp appearance and alleviate false edges. Moreover, given that the intensity distribution is preserved solely by SSIM, the luminance component of the result shows slight deviation from source images, as shown in the result of FusionDN in the last column. In U2Fusion, to overcome the luminance deviation, we add the MSE loss. As in the last column, the intensity of U2Fusion is more similar to that of source images.\n\nLastly, we replace the first fusion task from VIS-IR image fusion to multi-modal image fusion. In this task, VIS-IR and PET-MRI image fusion are included. As the model in FusionDN has not been trained on the medical dataset, the result seems unsatisfactory with weak edges and gray background, as shown in the third column.\n\n\nCONCLUSION\n\nIn this study, we propose a novel unified and unsupervised end-to-end image fusion network, termed as U2Fusion, to solve multiple fusion problems. First, adaptive information preservation degrees are obtained as the measurement of the amount of information contained in source images. Thus, different tasks are solved under a unified framework. In particular, the adaptive degrees allows the network to be trained to preserve the adaptive similarity between the fusion result and source images. Consequently, the ground truth is not required. Moreover, we solve the catastrophic forgetting problem as well as the storage and computation issues to train a single model applicable to multiple problems. This single model is capable of solving multimodal, multi-exposure, and multi-focus image fusion problems with high-quality results. The qualitative and quantitative results validate the effectiveness and universality of U2Fusion. Moreover, we release a new aligned infrared and visible image dataset RoadScene on the basis of FLIR video to provide a new option for image fusion benchmark evaluation.\n\nFig. 1 .\n1Schematic illustration of different image fusion tasks (first row: source images, second row (from left to right): fusion results of FusionGAN\n\nFig. 2 .\n2Traditional image fusion framework.\n\nFig. 3 .\n3Pipeline of the proposed U2Fusion. Dashed lines represent the data used in the loss function.\n\nFig. 5 .\n5Illustration of feature maps extracted by VGGNet for overexposed and underexposed images.\n\nFig. 7 .\n7Intuitive description of data flow during the process of EWC. Thin lines indicate that only a small subset of data are kept, which are merely used to calculate \u00b5 i and not applied to train DenseNet.\n\nFig. 8 .Fig. 9 .Fig. 10 .\n8910Architecture of DenseNet used in our model. Numbers shown after concatenation/LeakyReLU/tanh functions are the channels of corresponding feature maps. U2Fusion to fuse multi-exposure image sequence. U2Fusion to fuse multi-focus image sequence.\n\nFig. 11 .Fig. 12 .\n1112Qualitative comparison of our U2Fusion with 5 state-of-the-art methods on 4 typical VIS and IR image pairs in the TNO dataset. Qualitative comparison of U2Fusion with 5 state-of-the-art methods on 5 typical VIS and IR image pairs in the RoadScene dataset.\n\nFig. 14 .Fig. 15 .Fig. 16 .\n141516Qualitative comparison of U2Fusion with 4 state-of-the-art methods on 4 typical PET and MRI image pairs in Harvard medical dataset.and thus contain little information. Qualitative results on the dataset in[41] and the EMPA HDR dataset are respectively reported in Figs. 15 and 16. Given the inappropriate exposure settings in source images, the representations of the scene are weakened with poor visual perception. In Qualitative comparison of U2Fusion with 5 state-of-the-art methods on 5 typical multi-exposure image pairs in the dataset in[41]. Qualitative comparison of U2Fusion with 5 state-of-the-art methods on 3 typical multi-exposure image pairs in the EMPA HDR dataset.\n\nFig. 17 .\n17.967\u00b10.02 0.181\u00b10.10 58.518\u00b13.20 0.840\u00b10.10 FLER 1.947\u00b10.01 0.212\u00b10.07 55.425\u00b10.81 0.337\u00b10.31 1.961\u00b10.02 0.235\u00b10.11 57.770\u00b13.41 0.518\u00b10.34 FusionDN 1.943\u00b10.01 0.285\u00b10.12 54.969\u00b10.85 0.817\u00b10.056 1.965\u00b10.02 0.277\u00b10.12 57.780\u00b12.44 0.801\u00b10.12 U2Fusion 1.954\u00b10.01 0.265\u00b10.09 56.074\u00b10.96 0.837\u00b10.05 1.968\u00b10.02 0.240\u00b10.13 58.454\u00b13.07 0.831\u00b10.10 Qualitative comparison of U2Fusion with 5 state-of-the-art methods on 3 typical far-/near-focused image pairs in the Lytro dataset.TABLE 4 Mean and standard deviation of four metrics on multi-focus image fusion on the Lytro dataset. 297\u00b10.11 0.970\u00b10.01 1.130\u00b10.05 34.294\u00b113.56 GFDF 0.299\u00b10.11 0.969\u00b10.01 1.136\u00b10.06 34.436\u00b113.57 SESF-Fuse 0.300\u00b10.11 0.969\u00b10.01 1.145\u00b10.06 34.568\u00b113.58 FusionDN 0.315\u00b10.11 0.969\u00b10.01 1.505\u00b10.34 35.080\u00b112.79 U2Fusion 0.316\u00b10.11 0.972\u00b10.01 1.466\u00b10.20 34.767\u00b112.67\n\nFig. 18 .\n18Changes of the similarity loss without EWC (the first plot) or with EWC (the second plot).\n\nFig. 19 .\n19Changes of statistical distributions of \u00b5 i without EWC (the first plot) or with EWC (the second plot).\n\nFig. 20 .\n20Intermediate fusion results. From left to right: (a) fusion results after training the model on task 1; (b) and (c): fusion results after training the model on task 2 and task 3 without EWC; (d) and (e): fusion results after training the model on task 2 and task 3 with EWC.\n\nFig. 21 .\n21Illustration of mutual promotion between different tasks in a unified model. From top to bottom: source images, fusion results by training individual models for each fusion task and fusion results of U2Fusion. From left to right: images from the TNO, RoadScene (the second and third columns), Harvard, the dataset in[41], EMPA HDR and Lytro datasets.\n\nFig. 23 .\n23Fusion results of different training orders. From left to right: results of image pairs from the TNO, RoadScene, Harvard, the dataset in[41], EMPA HDR and Lytro datasets.\n\n\n0.5319/0.0383 0.6613/0.0516 0.8406/0.0873 0.8322/0.0611 0.8081/0.0572 0.9803/0.0563\n\nFig. 24 .\n24Comparative results of our U2Fusion with the previous version FusionDN. From top to bottom: source images, result of FusionDN and U2Fusion. From left to right: image pairs from the TNO, RoadScene, Harvard, the dataset in [41], EMPA HDR and Lytro datasets.\n\nTABLE 1\n1Mean and standard deviation of four metrics on VIS-IR image fusion on the TNO and RoadScene datasets (red: the best, blue: the second best). 533\u00b10.10 1.9797\u00b10.010 59.953\u00b11.99 1.635\u00b10.16 0.565\u00b10.21 1.9873\u00b10.005 61.126\u00b11.73 1.310\u00b10.30 FusionGAN 0.458\u00b10.10 1.9824\u00b10.008 60.535\u00b11.98 1.403\u00b10.31 0.494\u00b10.26 1.9850\u00b10.009 61.341\u00b12.59 0.844\u00b10.Method \nTNO \nRoadScene \nCC \nSSIM \nPSNR \nSCD \nCC \nSSIM \nPSNR \nSCD \n\nHMSD \n0.464\u00b10.13 1.9889\u00b10.007 62.687\u00b12.67 1.666\u00b10.15 0.600\u00b10.20 1.9904\u00b10.005 63.146\u00b12.58 1.508\u00b10.27 \nGTF \n0.352\u00b10.12 1.9860\u00b10.007 61.782\u00b13.02 0.977\u00b10.20 0.499\u00b10.26 1.9863\u00b10.009 62.013\u00b13.26 1.007\u00b10.17 \nDenseFuse 0.52 \nDDcGAN \n0.414\u00b10.11 1.9824\u00b10.006 60.248\u00b11.49 1.269\u00b10.18 0.506\u00b10.20 1.9805\u00b10.009 60.051\u00b12.32 1.187\u00b10.26 \nFusionDN \n0.499\u00b10.12 1.9875\u00b10.004 61.691\u00b11.29 1.805\u00b10.12 0.627\u00b10.21 1.9866\u00b10.007 61.684\u00b12.43 1.778\u00b10.17 \nU2Fusion \n0.537\u00b10.11 1.9909\u00b10.005 62.914\u00b12.07 1.780\u00b10.11 0.635\u00b10.20 1.9909\u00b10.005 63.305\u00b12.35 1.635\u00b10.24 \n\nVIS \n\n(RGB) \n\nIR \n\nFusionGAN \n\nU2Fusion \n\nFig. 13. Qualitative results on 3 typical VIS (RGB) and IR image pairs in \nthe RoadScene dataset. \n\n\n\nTABLE 2\n2Mean and standard deviation of four metrics on medical image fusion on the Harvard dataset.U2Fusion 1.312\u00b10.04 0.834\u00b10.08 1.9921\u00b10.002 63.458\u00b11.15 metrics in Sec. 4.2.1 is performed on the remaining 10 test image pairs, as reported inTab.2. The best results on CC, SSIM, and PSNR indicate that U2Fusion achieves higher correlation and similarity with source images and produces less distortion/noise. The suboptimal result on SCD shows that U2Fusion achieves comparable correlation between the difference and source images.Method \nSCD \nCC \nSSIM \nPSNR \n\nRPCNN \n\n1.429\u00b10.08 0.785\u00b10.09 1.9865\u00b10.004 61.213\u00b11.28 \n\nCNN \n\n1.272\u00b10.23 0.798\u00b10.09 1.9885\u00b10.005 62.137\u00b11.79 \n\nPAPCNN 1.289\u00b10.12 0.784\u00b10.09 1.9872\u00b10.005 61.565\u00b11.41 \nNSCT \n\n0.969\u00b10.20 0.769\u00b10.09 1.9875\u00b10.005 61.695\u00b11.49 \nFusionDN 0.742\u00b10.23 0.805\u00b10.09 1.9769\u00b10.008 59.178\u00b11.34 \n\n\n\n\nand the EMPA HDR dataset.Method \ndataset in [41] \nEMPA HDR \nSSIM \nEI \nPSNR \nCC \nSSIM \nEI \nPSNR \nCC \n\nGFF \n1.937\u00b10.01 0.218\u00b10.07 54.604\u00b10.95 0.065\u00b10.35 \n1.954\u00b10.03 0.242\u00b10.11 57.108\u00b13.59 0.423\u00b10.36 \nDSIFT \n1.940\u00b10.01 0.193\u00b10.06 54.856\u00b10.83 0.088\u00b10.35 \n1.958\u00b10.02 0.222\u00b10.11 57.415\u00b13.49 0.524\u00b10.29 \nGBM \n1.953\u00b10.01 0.230\u00b10.07 55.965\u00b10.97 0.793\u00b10.05 \n1.966\u00b10.02 0.237\u00b10.12 58.145\u00b12.87 0.782\u00b10.12 \nDeepfuse \n\n\nTask1: multi-modal Task2: multi-exposure Task3: multi-focus Task1: multi-modal Task2: multi-exposure Task3: multi-focus Task1: multi-modal Task2: multi-exposure Task3: multi-focusTraining \n\nProcess \n\nTraining \n\nProcess \n\nTraining \n\nProcess \n\n(a) \n(b) \n(c) \n\nVIS-IR \nPET-MRI \nVIS-IR \nPET-MRI \nVIS-IR \nPET-MRI \n\nTask1: multi-modal \n\nTask2: multi-exposure \n\nTask3: multi-focus \n\nTraining \n\nProcess \n\nTask1: multi-modal \n\nTask2: multi-exposure \n\nTask3: multi-focus \n\nTraining \n\nProcess \n\n(d) \n(e) \n\nVIS-IR \nPET-MRI \nVIS-IR \nPET-MRI \n\n\n\nTABLE 5\n5Mean of two metrics (correlation coefficient/mean gradient) of different training orders on different datasets.Training Order \nmulti-modal dataset \nmulti-exposure dataset \nmulti-focus dataset \nTNO \nRoadScene \nHarvard \ndataset in [41] \nEMPA HDR \nLytro \n\n\nACKNOWLEDGEMENTS\nPixel-level image fusion: A survey of the state of the art. S Li, X Kang, L Fang, J Hu, H Yin, Inf. Fusion. 33S. Li, X. Kang, L. Fang, J. Hu, and H. Yin, \"Pixel-level image fusion: A survey of the state of the art,\" Inf. Fusion, vol. 33, pp. 100-112, 2017.\n\nInfrared and visible image fusion methods and applications: A survey. J Ma, Y Ma, C Li, Inf. Fusion. 45J. Ma, Y. Ma, and C. Li, \"Infrared and visible image fusion methods and applications: A survey,\" Inf. Fusion, vol. 45, pp. 153-178, 2019.\n\nActive and dynamic information fusion for facial expression understanding from image sequences. Y Zhang, Q Ji, IEEE Trans. Pattern Anal. Mach. Intell. 275Y. Zhang and Q. Ji, \"Active and dynamic information fusion for facial expression understanding from image sequences,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 5, pp. 699-714, 2005.\n\nObjective assessment of multiresolution image fusion algorithms for context enhancement in night vision: a comparative study. Z Liu, E Blasch, Z Xue, J Zhao, R Laganiere, W Wu, IEEE Trans. Pattern Anal. Mach. Intell. 341Z. Liu, E. Blasch, Z. Xue, J. Zhao, R. Laganiere, and W. Wu, \"Objective assessment of multiresolution image fusion algorithms for context enhancement in night vision: a comparative study,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 1, pp. 94-109, 2011.\n\nFusiongan: A generative adversarial network for infrared and visible image fusion. J Ma, W Yu, P Liang, C Li, J Jiang, Inf. Fusion. 48J. Ma, W. Yu, P. Liang, C. Li, and J. Jiang, \"Fusiongan: A generative adversarial network for infrared and visible image fusion,\" Inf. Fusion, vol. 48, pp. 11-26, 2019.\n\nA phase congruency and local laplacian energy based multi-modality medical image fusion method in nsct domain. Z Zhu, M Zheng, G Qi, D Wang, Y Xiang, IEEE Access. 7Z. Zhu, M. Zheng, G. Qi, D. Wang, and Y. Xiang, \"A phase con- gruency and local laplacian energy based multi-modality medical image fusion method in nsct domain,\" IEEE Access, vol. 7, pp. 20 811-20 824, 2019.\n\nDeepfuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs. K R Prabhakar, V S Srikar, R V Babu, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisK. R. Prabhakar, V. S. Srikar, and R. V. Babu, \"Deepfuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs,\" in Proc. IEEE Int. Conf. Comput. Vis, 2017, pp. 4724-4732.\n\nGuided filter-based multifocus image fusion through focus region detection. X Qiu, M Li, L Zhang, X Yuan, Image Commun. 72Signal ProcessX. Qiu, M. Li, L. Zhang, and X. Yuan, \"Guided filter-based multi- focus image fusion through focus region detection,\" Signal Process. Image Commun., vol. 72, pp. 35-46, 2019.\n\nDeep learning for pixel-level image fusion: Recent advances and future prospects. Y Liu, X Chen, Z Wang, Z J Wang, R K Ward, X Wang, Inf. Fusion. 42Y. Liu, X. Chen, Z. Wang, Z. J. Wang, R. K. Ward, and X. Wang, \"Deep learning for pixel-level image fusion: Recent advances and future prospects,\" Inf. Fusion, vol. 42, pp. 158-173, 2018.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionG. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, \"Densely connected convolutional networks,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 4700-4708.\n\nFusiondn: A unified densely connected network for image fusion. H Xu, J Ma, Z Le, J Jiang, X Guo, Proc. AAAI Conf. AAAI ConfH. Xu, J. Ma, Z. Le, J. Jiang, and X. Guo, \"Fusiondn: A unified densely connected network for image fusion,\" in Proc. AAAI Conf. Artif. Intell., 2020, pp. 12 484-12 491.\n\nOvercoming catastrophic forgetting in neural networks. J Kirkpatrick, R Pascanu, N Rabinowitz, J Veness, G Desjardins, A A Rusu, K Milan, J Quan, T Ramalho, A Grabska-Barwinska, Proc. Natl. Acad. Sci. 11413J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska et al., \"Overcoming catastrophic forgetting in neural networks,\" Proc. Natl. Acad. Sci., vol. 114, no. 13, pp. 3521-3526, 2017.\n\nMultifocus image fusion based on spatial frequency in discrete cosine transform domain. L Cao, L Jin, H Tao, G Li, Z Zhuang, Y Zhang, IEEE Signal Process. Lett. 222L. Cao, L. Jin, H. Tao, G. Li, Z. Zhuang, and Y. Zhang, \"Multi- focus image fusion based on spatial frequency in discrete cosine transform domain,\" IEEE Signal Process. Lett., vol. 22, no. 2, pp. 220-224, 2014.\n\nSparse representation based multi-sensor image fusion for multi-focus and multi-modality images: A review. Q Zhang, Y Liu, R S Blum, J Han, D Tao, Inf. Fusion. 40Q. Zhang, Y. Liu, R. S. Blum, J. Han, and D. Tao, \"Sparse rep- resentation based multi-sensor image fusion for multi-focus and multi-modality images: A review,\" Inf. Fusion, vol. 40, pp. 57-75, 2018.\n\nImage fusion with convolutional sparse representation. Y Liu, X Chen, R K Ward, Z J Wang, IEEE Signal Process. Lett. 2312Y. Liu, X. Chen, R. K. Ward, and Z. J. Wang, \"Image fusion with convolutional sparse representation,\" IEEE Signal Process. Lett., vol. 23, no. 12, pp. 1882-1886, 2016.\n\nA medical image fusion method based on convolutional neural networks. Y Liu, X Chen, J Cheng, H Peng, Proc. Int. Conf. Inf. Fusion. Int. Conf. Inf. FusionY. Liu, X. Chen, J. Cheng, and H. Peng, \"A medical image fusion method based on convolutional neural networks,\" in Proc. Int. Conf. Inf. Fusion, 2017, pp. 1-7.\n\nDensefuse: A fusion approach to infrared and visible images. H Li, X.-J Wu, IEEE Trans. Image Process. 285H. Li and X.-J. Wu, \"Densefuse: A fusion approach to infrared and visible images,\" IEEE Trans. Image Process., vol. 28, no. 5, pp. 2614-2623, 2018.\n\nSesf-fuse: An unsupervised deep model for multi-focus image fusion. B Ma, X Ban, H Huang, Y Zhu, arXiv:1908.01703arXiv preprintB. Ma, X. Ban, H. Huang, and Y. Zhu, \"Sesf-fuse: An unsuper- vised deep model for multi-focus image fusion,\" arXiv preprint arXiv:1908.01703, 2019.\n\nInfrared and visible image fusion via gradient transfer and total variation minimization. J Ma, C Chen, C Li, J Huang, Inf. Fusion. 31J. Ma, C. Chen, C. Li, and J. Huang, \"Infrared and visible image fusion via gradient transfer and total variation minimization,\" Inf. Fusion, vol. 31, pp. 100-109, 2016.\n\nMulti-exposure image fusion by optimizing a structural similarity index. K Ma, Z Duanmu, H Yeganeh, Z Wang, IEEE Transactions on Computational Imaging. 41K. Ma, Z. Duanmu, H. Yeganeh, and Z. Wang, \"Multi-exposure image fusion by optimizing a structural similarity index,\" IEEE Transactions on Computational Imaging, vol. 4, no. 1, pp. 60-72, 2017.\n\nMulti-focus image fusion with dense sift. Y Liu, S Liu, Z Wang, Inf. Fusion. 23Y. Liu, S. Liu, and Z. Wang, \"Multi-focus image fusion with dense sift,\" Inf. Fusion, vol. 23, pp. 139-155, 2015.\n\nInfrared and visible image fusion via detail preserving adversarial learning. J Ma, P Liang, W Yu, C Chen, X Guo, J Wu, J Jiang, Inf. Fusion. 54J. Ma, P. Liang, W. Yu, C. Chen, X. Guo, J. Wu, and J. Jiang, \"In- frared and visible image fusion via detail preserving adversarial learning,\" Inf. Fusion, vol. 54, pp. 85-98, 2020.\n\nLearning a generative model for fusing infrared and visible images via conditional generative adversarial network with dual discriminators. H Xu, P Liang, W Yu, J Jiang, J Ma, Proc. Int. Joint Conf. Int. Joint ConfH. Xu, P. Liang, W. Yu, J. Jiang, and J. Ma, \"Learning a generative model for fusing infrared and visible images via conditional generative adversarial network with dual discriminators,\" in Proc. Int. Joint Conf. Artif. Intell., 2019, pp. 3954-3960.\n\nDdcgan: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion. J Ma, H Xu, J Jiang, X Mei, X.-P Zhang, IEEE Trans. Image Process. 29J. Ma, H. Xu, J. Jiang, X. Mei, and X.-P. Zhang, \"Ddcgan: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion,\" IEEE Trans. Image Process., vol. 29, pp. 4980-4995, 2020.\n\nMulti-focus image fusion with a deep convolutional neural network. Y Liu, X Chen, H Peng, Z Wang, Inf. Fusion. 36Y. Liu, X. Chen, H. Peng, and Z. Wang, \"Multi-focus image fusion with a deep convolutional neural network,\" Inf. Fusion, vol. 36, pp. 191-207, 2017.\n\nIfcnn: A general image fusion framework based on convolutional neural network. Y Zhang, Y Liu, P Sun, H Yan, X Zhao, L Zhang, Inf. Fusion. 54Y. Zhang, Y. Liu, P. Sun, H. Yan, X. Zhao, and L. Zhang, \"Ifcnn: A general image fusion framework based on convolutional neural network,\" Inf. Fusion, vol. 54, pp. 99-118, 2020.\n\nFusegan: Learning to fuse multi-focus image via conditional generative adversarial network. X Guo, R Nie, J Cao, D Zhou, L Mei, K He, IEEE Trans. Multimed. 218X. Guo, R. Nie, J. Cao, D. Zhou, L. Mei, and K. He, \"Fusegan: Learning to fuse multi-focus image via conditional generative adversarial network,\" IEEE Trans. Multimed., vol. 21, no. 8, pp. 1982-1996, 2019.\n\nOvercoming catastrophic forgetting by incremental moment matching. S.-W Lee, J.-H Kim, J Jun, J.-W Ha, B.-T Zhang, Advances in Neural Information Processing Systems. S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang, \"Overcom- ing catastrophic forgetting by incremental moment matching,\" in Advances in Neural Information Processing Systems, 2017, pp. 4652- 4662.\n\nContinual learning through synaptic intelligence. F Zenke, B Poole, S Ganguli, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. LearnF. Zenke, B. Poole, and S. Ganguli, \"Continual learning through synaptic intelligence,\" in Proc. Int. Conf. Mach. Learn., 2017, pp. 3987-3995.\n\nContinuous learning without forgetting for person re-identification. N Sugianto, D Tjondronegoro, G Sorwar, P Chakraborty, E I Yuwono, Proc. IEEE Int. Conf. Advanced Video and Signal Based Surveillance. IEEE Int. Conf. Advanced Video and Signal Based SurveillanceN. Sugianto, D. Tjondronegoro, G. Sorwar, P. Chakraborty, and E. I. Yuwono, \"Continuous learning without forgetting for person re-identification,\" in Proc. IEEE Int. Conf. Advanced Video and Signal Based Surveillance, 2019, pp. 1-8.\n\nDiverse large-scale its dataset created from continuous learning for real-time vehicle detection. J A Eichel, A Mishra, N Miller, N Jankovic, M A Thomas, T Abbott, D Swanson, J Keller, arXiv:1510.02055arXiv preprintJ. A. Eichel, A. Mishra, N. Miller, N. Jankovic, M. A. Thomas, T. Abbott, D. Swanson, and J. Keller, \"Diverse large-scale its dataset created from continuous learning for real-time vehicle detection,\" arXiv preprint arXiv:1510.02055, 2015.\n\nA multi-task learning framework for emotion recognition using 2d continuous space. R Xia, Y Liu, IEEE Trans. Affect. Comput. 81R. Xia and Y. Liu, \"A multi-task learning framework for emotion recognition using 2d continuous space,\" IEEE Trans. Affect. Com- put., vol. 8, no. 1, pp. 3-14, 2015.\n\nHcp: A flexible cnn framework for multi-label image classification. Y Wei, W Xia, M Lin, J Huang, B Ni, J Dong, Y Zhao, S Yan, IEEE Trans. Pattern Anal. Mach. Intell. 389Y. Wei, W. Xia, M. Lin, J. Huang, B. Ni, J. Dong, Y. Zhao, and S. Yan, \"Hcp: A flexible cnn framework for multi-label image classification,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 9, pp. 1901-1907, 2015.\n\nHedging deep features for visual tracking. Y Qi, S Zhang, L Qin, Q Huang, H Yao, J Lim, M.-H Yang, IEEE Trans. Pattern Anal. Mach. Intell. 415Y. Qi, S. Zhang, L. Qin, Q. Huang, H. Yao, J. Lim, and M.-H. Yang, \"Hedging deep features for visual tracking,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 5, pp. 1116-1130, 2018.\n\nPerceptual losses for real-time style transfer and super-resolution. J Johnson, A Alahi, L Fei-Fei, Proc. Europ. Conf. Comput. Vis. Europ. Conf. Comput. VisJ. Johnson, A. Alahi, and L. Fei-Fei, \"Perceptual losses for real-time style transfer and super-resolution,\" in Proc. Europ. Conf. Comput. Vis., 2016, pp. 694-711.\n\nOn the reconstruction of face images from deep face templates. G Mai, K Cao, P C Yuen, A K Jain, IEEE Trans. Pattern Anal. Mach. Intell. 415G. Mai, K. Cao, P. C. Yuen, and A. K. Jain, \"On the reconstruction of face images from deep face templates,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 5, pp. 1188-1202, 2018.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" arXiv preprint arXiv:1409.1556, 2014.\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Trans. Image Process. 134Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli et al., \"Image quality assessment: from error visibility to structural similarity,\" IEEE Trans. Image Process., vol. 13, no. 4, pp. 600-612, 2004.\n\nLearning without forgetting. Z Li, D Hoiem, IEEE Trans. Pattern Anal. Mach. Intell. 4012Z. Li and D. Hoiem, \"Learning without forgetting,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 12, pp. 2935-2947, 2017.\n\nMulti-scale single image dehazing using perceptual pyramid deep network. H Zhang, V Sindagi, V M Patel, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops. IEEE Conf. Comput. Vis. Pattern Recognit. WorkshopsH. Zhang, V. Sindagi, and V. M. Patel, \"Multi-scale single image dehazing using perceptual pyramid deep network,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, 2018, pp. 902-911.\n\nLearning a deep single image contrast enhancer from multi-exposure images. J Cai, S Gu, L Zhang, IEEE Trans. Image Process. 274J. Cai, S. Gu, and L. Zhang, \"Learning a deep single image contrast enhancer from multi-exposure images,\" IEEE Trans. Image Process., vol. 27, no. 4, pp. 2049-2062, 2018.\n\nPerceptual fusion of infrared and visible images through a hybrid multi-scale decomposition with gaussian and bilateral filters. Z Zhou, B Wang, S Li, M Dong, Inf. Fusion. 30Z. Zhou, B. Wang, S. Li, and M. Dong, \"Perceptual fusion of infrared and visible images through a hybrid multi-scale decom- position with gaussian and bilateral filters,\" Inf. Fusion, vol. 30, pp. 15-26, 2016.\n\nA new image quality metric for image fusion: the sum of the correlations of differences. V Aslantas, E Bendes, Aeu-Int. J. Electr. Commun. 6912V. Aslantas and E. Bendes, \"A new image quality metric for image fusion: the sum of the correlations of differences,\" Aeu-Int. J. Electr. Commun., vol. 69, no. 12, pp. 1890-1896, 2015.\n\nA neuro-fuzzy approach for medical image fusion. S Das, M K Kundu, IEEE Trans. Biomed. Eng. 6012S. Das and M. K. Kundu, \"A neuro-fuzzy approach for medical image fusion,\" IEEE Trans. Biomed. Eng., vol. 60, no. 12, pp. 3347- 3353, 2013.\n\nMedical image fusion with parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform domain. M Yin, X Liu, Y Liu, X Chen, IEEE Trans. Instrum. Meas. 681M. Yin, X. Liu, Y. Liu, and X. Chen, \"Medical image fusion with parameter-adaptive pulse coupled neural network in nonsub- sampled shearlet transform domain,\" IEEE Trans. Instrum. Meas., vol. 68, no. 1, pp. 49-64, 2018.\n\nImage fusion with guided filtering. S Li, X Kang, J Hu, IEEE Trans. Image Process. 227S. Li, X. Kang, and J. Hu, \"Image fusion with guided filtering,\" IEEE Trans. Image Process., vol. 22, no. 7, pp. 2864-2875, 2013.\n\nDense sift for ghost-free multi-exposure fusion. Y Liu, Z Wang, J. Visual Commun. Image Represent. 31Y. Liu and Z. Wang, \"Dense sift for ghost-free multi-exposure fusion,\" J. Visual Commun. Image Represent., vol. 31, pp. 208-224, 2015.\n\nMulti-exposure and multi-focus image fusion in gradient domain. S Paul, I S Sevcenco, P Agathoklis, J. Circuit. Syst. Comp. 25101650123S. Paul, I. S. Sevcenco, and P. Agathoklis, \"Multi-exposure and multi-focus image fusion in gradient domain,\" J. Circuit. Syst. Comp., vol. 25, no. 10, p. 1650123, 2016.\n\nMulti-scale fusion of two largeexposure-ratio images. Y Yang, W Cao, S Wu, Z Li, IEEE Signal Process. Lett. 2512Y. Yang, W. Cao, S. Wu, and Z. Li, \"Multi-scale fusion of two large- exposure-ratio images,\" IEEE Signal Process. Lett., vol. 25, no. 12, pp. 1885-1889, 2018.\n\nMulti-focus image fusion with dense sift. Y Liu, S Liu, Z Wang, Inf. Fusion. 23Y. Liu, S. Liu, and Z. Wang, \"Multi-focus image fusion with dense sift,\" Inf. Fusion, vol. 23, pp. 139-155, 2015.\n\nA new image fusion performance metric based on visual information fidelity. Y Han, Y Cai, Y Cao, X Xu, Inf. Fusion. 142Y. Han, Y. Cai, Y. Cao, and X. Xu, \"A new image fusion perfor- mance metric based on visual information fidelity,\" Inf. Fusion, vol. 14, no. 2, pp. 127-135, 2013.\n", "annotations": {"author": "[{\"end\":129,\"start\":122},{\"end\":139,\"start\":130},{\"end\":153,\"start\":140},{\"end\":166,\"start\":154},{\"end\":179,\"start\":167}]", "publisher": null, "author_last_name": "[{\"end\":128,\"start\":126},{\"end\":138,\"start\":136},{\"end\":152,\"start\":147},{\"end\":165,\"start\":162},{\"end\":178,\"start\":174}]", "author_first_name": "[{\"end\":125,\"start\":122},{\"end\":135,\"start\":130},{\"end\":146,\"start\":140},{\"end\":161,\"start\":154},{\"end\":173,\"start\":167}]", "author_affiliation": null, "title": "[{\"end\":119,\"start\":1},{\"end\":298,\"start\":180}]", "venue": null, "abstract": "[{\"end\":1750,\"start\":376}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1875,\"start\":1872},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1880,\"start\":1877},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2717,\"start\":2714},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2722,\"start\":2719},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3021,\"start\":3018},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5206,\"start\":5203},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5226,\"start\":5223},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5250,\"start\":5247},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5270,\"start\":5267},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5577,\"start\":5573},{\"end\":6088,\"start\":6087},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6820,\"start\":6816},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7875,\"start\":7871},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8959,\"start\":8955},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9033,\"start\":9029},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9730,\"start\":9726},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9736,\"start\":9732},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9763,\"start\":9759},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9769,\"start\":9765},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10249,\"start\":10245},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10338,\"start\":10334},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10408,\"start\":10404},{\"end\":10941,\"start\":10909},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11132,\"start\":11128},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11241,\"start\":11237},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11247,\"start\":11243},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11813,\"start\":11810},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12343,\"start\":12339},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12543,\"start\":12539},{\"end\":12707,\"start\":12675},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14155,\"start\":14151},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14161,\"start\":14157},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14384,\"start\":14380},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14418,\"start\":14414},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14448,\"start\":14444},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16131,\"start\":16127},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16137,\"start\":16133},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18161,\"start\":18157},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18167,\"start\":18163},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18205,\"start\":18201},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18211,\"start\":18207},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18256,\"start\":18252},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21335,\"start\":21332},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23122,\"start\":23118},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25605,\"start\":25601},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25886,\"start\":25882},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27124,\"start\":27120},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27814,\"start\":27811},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29635,\"start\":29631},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30029,\"start\":30025},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":33853,\"start\":33849},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33863,\"start\":33859},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33879,\"start\":33875},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33894,\"start\":33891},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33911,\"start\":33907},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":35114,\"start\":35110},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35656,\"start\":35652},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35666,\"start\":35662},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":35680,\"start\":35676},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35694,\"start\":35691},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":35965,\"start\":35961},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35977,\"start\":35973},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":35987,\"start\":35983},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36001,\"start\":35998},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":36016,\"start\":36012},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36513,\"start\":36509},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36781,\"start\":36777},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":37330,\"start\":37326},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":37340,\"start\":37336},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37350,\"start\":37346},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":37360,\"start\":37357},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37380,\"start\":37376},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":38358,\"start\":38354},{\"end\":38863,\"start\":38856},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38961,\"start\":38957},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":46705,\"start\":46701},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":48161,\"start\":48157},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":52716,\"start\":52712},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":53054,\"start\":53050},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":54875,\"start\":54871},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":55059,\"start\":55055},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":55604,\"start\":55602}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51453,\"start\":51300},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51500,\"start\":51454},{\"attributes\":{\"id\":\"fig_2\"},\"end\":51605,\"start\":51501},{\"attributes\":{\"id\":\"fig_3\"},\"end\":51706,\"start\":51606},{\"attributes\":{\"id\":\"fig_4\"},\"end\":51916,\"start\":51707},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52191,\"start\":51917},{\"attributes\":{\"id\":\"fig_6\"},\"end\":52471,\"start\":52192},{\"attributes\":{\"id\":\"fig_7\"},\"end\":53187,\"start\":52472},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54032,\"start\":53188},{\"attributes\":{\"id\":\"fig_9\"},\"end\":54136,\"start\":54033},{\"attributes\":{\"id\":\"fig_10\"},\"end\":54253,\"start\":54137},{\"attributes\":{\"id\":\"fig_11\"},\"end\":54541,\"start\":54254},{\"attributes\":{\"id\":\"fig_12\"},\"end\":54905,\"start\":54542},{\"attributes\":{\"id\":\"fig_13\"},\"end\":55089,\"start\":54906},{\"attributes\":{\"id\":\"fig_14\"},\"end\":55175,\"start\":55090},{\"attributes\":{\"id\":\"fig_15\"},\"end\":55444,\"start\":55176},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":56529,\"start\":55445},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":57373,\"start\":56530},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":57779,\"start\":57374},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":58312,\"start\":57780},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":58576,\"start\":58313}]", "paragraph": "[{\"end\":2723,\"start\":1766},{\"end\":3699,\"start\":2725},{\"end\":4330,\"start\":3701},{\"end\":4844,\"start\":4332},{\"end\":5285,\"start\":4846},{\"end\":5526,\"start\":5287},{\"end\":5737,\"start\":5528},{\"end\":6357,\"start\":5739},{\"end\":6560,\"start\":6363},{\"end\":6767,\"start\":6566},{\"end\":8168,\"start\":6769},{\"end\":8605,\"start\":8256},{\"end\":9770,\"start\":8607},{\"end\":10550,\"start\":9772},{\"end\":10776,\"start\":10572},{\"end\":11672,\"start\":10806},{\"end\":12234,\"start\":11674},{\"end\":13164,\"start\":12236},{\"end\":13779,\"start\":13166},{\"end\":14534,\"start\":13802},{\"end\":14824,\"start\":14550},{\"end\":15653,\"start\":14848},{\"end\":16981,\"start\":15655},{\"end\":17915,\"start\":16983},{\"end\":18751,\"start\":17938},{\"end\":19963,\"start\":18753},{\"end\":20494,\"start\":19991},{\"end\":20787,\"start\":20554},{\"end\":21686,\"start\":20823},{\"end\":21771,\"start\":21731},{\"end\":21997,\"start\":21790},{\"end\":22202,\"start\":22015},{\"end\":23106,\"start\":22246},{\"end\":23309,\"start\":23108},{\"end\":23691,\"start\":23381},{\"end\":24037,\"start\":23753},{\"end\":25231,\"start\":24165},{\"end\":26587,\"start\":25233},{\"end\":26752,\"start\":26635},{\"end\":26956,\"start\":26754},{\"end\":27156,\"start\":27006},{\"end\":27359,\"start\":27201},{\"end\":27605,\"start\":27361},{\"end\":28624,\"start\":27607},{\"end\":29344,\"start\":28649},{\"end\":30218,\"start\":29346},{\"end\":30567,\"start\":30245},{\"end\":30985,\"start\":30640},{\"end\":31425,\"start\":31018},{\"end\":31625,\"start\":31466},{\"end\":33205,\"start\":31646},{\"end\":33723,\"start\":33207},{\"end\":35596,\"start\":33788},{\"end\":35900,\"start\":35621},{\"end\":37264,\"start\":35932},{\"end\":38279,\"start\":37293},{\"end\":39185,\"start\":38281},{\"end\":40303,\"start\":39237},{\"end\":42262,\"start\":40305},{\"end\":43868,\"start\":42327},{\"end\":45009,\"start\":43935},{\"end\":45793,\"start\":45038},{\"end\":46399,\"start\":45795},{\"end\":47943,\"start\":46401},{\"end\":48331,\"start\":48097},{\"end\":49010,\"start\":48333},{\"end\":49858,\"start\":49012},{\"end\":50183,\"start\":49860},{\"end\":51299,\"start\":50198}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20553,\"start\":20495},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21730,\"start\":21687},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21789,\"start\":21772},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22245,\"start\":22203},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23380,\"start\":23310},{\"attributes\":{\"id\":\"formula_5\"},\"end\":23752,\"start\":23692},{\"attributes\":{\"id\":\"formula_6\"},\"end\":24086,\"start\":24038},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26634,\"start\":26588},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27005,\"start\":26957},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27200,\"start\":27157},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30639,\"start\":30568},{\"attributes\":{\"id\":\"formula_11\"},\"end\":48072,\"start\":47944}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1764,\"start\":1752},{\"end\":6361,\"start\":6360},{\"end\":6564,\"start\":6563},{\"attributes\":{\"n\":\"2\"},\"end\":8183,\"start\":8171},{\"attributes\":{\"n\":\"2.1\"},\"end\":8206,\"start\":8186},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":8254,\"start\":8209},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":10570,\"start\":10553},{\"end\":10804,\"start\":10779},{\"attributes\":{\"n\":\"2.2\"},\"end\":13800,\"start\":13782},{\"attributes\":{\"n\":\"3\"},\"end\":14548,\"start\":14537},{\"attributes\":{\"n\":\"3.1\"},\"end\":14846,\"start\":14827},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":17936,\"start\":17918},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":19989,\"start\":19966},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":20821,\"start\":20790},{\"attributes\":{\"n\":\"3.2\"},\"end\":22013,\"start\":22000},{\"attributes\":{\"n\":\"3.3\"},\"end\":24163,\"start\":24088},{\"attributes\":{\"n\":\"3.4\"},\"end\":28647,\"start\":28627},{\"attributes\":{\"n\":\"3.5\"},\"end\":30243,\"start\":30221},{\"attributes\":{\"n\":\"3.6\"},\"end\":31016,\"start\":30988},{\"attributes\":{\"n\":\"4\"},\"end\":31464,\"start\":31428},{\"attributes\":{\"n\":\"4.1\"},\"end\":31644,\"start\":31628},{\"attributes\":{\"n\":\"4.2\"},\"end\":33750,\"start\":33726},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":33786,\"start\":33753},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":35619,\"start\":35599},{\"attributes\":{\"n\":\"4.3\"},\"end\":35930,\"start\":35903},{\"attributes\":{\"n\":\"4.4\"},\"end\":37291,\"start\":37267},{\"attributes\":{\"n\":\"5\"},\"end\":39208,\"start\":39188},{\"attributes\":{\"n\":\"5.1\"},\"end\":39235,\"start\":39211},{\"attributes\":{\"n\":\"5.2\"},\"end\":42325,\"start\":42265},{\"attributes\":{\"n\":\"5.3\"},\"end\":43933,\"start\":43871},{\"attributes\":{\"n\":\"5.4\"},\"end\":45036,\"start\":45012},{\"attributes\":{\"n\":\"5.5\"},\"end\":48095,\"start\":48074},{\"attributes\":{\"n\":\"6\"},\"end\":50196,\"start\":50186},{\"end\":51309,\"start\":51301},{\"end\":51463,\"start\":51455},{\"end\":51510,\"start\":51502},{\"end\":51615,\"start\":51607},{\"end\":51716,\"start\":51708},{\"end\":51943,\"start\":51918},{\"end\":52211,\"start\":52193},{\"end\":52500,\"start\":52473},{\"end\":53198,\"start\":53189},{\"end\":54043,\"start\":54034},{\"end\":54147,\"start\":54138},{\"end\":54264,\"start\":54255},{\"end\":54552,\"start\":54543},{\"end\":54916,\"start\":54907},{\"end\":55186,\"start\":55177},{\"end\":55453,\"start\":55446},{\"end\":56538,\"start\":56531},{\"end\":58321,\"start\":58314}]", "table": "[{\"end\":56529,\"start\":55789},{\"end\":57373,\"start\":57063},{\"end\":57779,\"start\":57401},{\"end\":58312,\"start\":57961},{\"end\":58576,\"start\":58434}]", "figure_caption": "[{\"end\":51453,\"start\":51311},{\"end\":51500,\"start\":51465},{\"end\":51605,\"start\":51512},{\"end\":51706,\"start\":51617},{\"end\":51916,\"start\":51718},{\"end\":52191,\"start\":51948},{\"end\":52471,\"start\":52216},{\"end\":53187,\"start\":52507},{\"end\":54032,\"start\":53201},{\"end\":54136,\"start\":54046},{\"end\":54253,\"start\":54150},{\"end\":54541,\"start\":54267},{\"end\":54905,\"start\":54555},{\"end\":55089,\"start\":54919},{\"end\":55175,\"start\":55092},{\"end\":55444,\"start\":55189},{\"end\":55789,\"start\":55455},{\"end\":57063,\"start\":56540},{\"end\":57401,\"start\":57376},{\"end\":57961,\"start\":57782},{\"end\":58434,\"start\":58323}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2507,\"start\":2501},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8324,\"start\":8318},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17031,\"start\":17025},{\"end\":18299,\"start\":18293},{\"end\":18470,\"start\":18464},{\"end\":18931,\"start\":18925},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19084,\"start\":19078},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19281,\"start\":19275},{\"end\":20658,\"start\":20652},{\"end\":22862,\"start\":22856},{\"end\":24749,\"start\":24743},{\"end\":25335,\"start\":25326},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27604,\"start\":27598},{\"end\":28873,\"start\":28867},{\"end\":29863,\"start\":29857},{\"end\":30217,\"start\":30211},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34508,\"start\":34501},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34537,\"start\":34530},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34651,\"start\":34644},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35738,\"start\":35731},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37422,\"start\":37415},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38844,\"start\":38837},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39006,\"start\":38962},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39716,\"start\":39709},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40395,\"start\":40388},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41494,\"start\":41487},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":42168,\"start\":42156},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42740,\"start\":42733},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44145,\"start\":44138},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":46008,\"start\":46001},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":46462,\"start\":46455},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":47087,\"start\":47080},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48330,\"start\":48323},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48562,\"start\":48555}]", "bib_author_first_name": "[{\"end\":58655,\"start\":58654},{\"end\":58661,\"start\":58660},{\"end\":58669,\"start\":58668},{\"end\":58677,\"start\":58676},{\"end\":58683,\"start\":58682},{\"end\":58923,\"start\":58922},{\"end\":58929,\"start\":58928},{\"end\":58935,\"start\":58934},{\"end\":59191,\"start\":59190},{\"end\":59200,\"start\":59199},{\"end\":59570,\"start\":59569},{\"end\":59577,\"start\":59576},{\"end\":59587,\"start\":59586},{\"end\":59594,\"start\":59593},{\"end\":59602,\"start\":59601},{\"end\":59615,\"start\":59614},{\"end\":60012,\"start\":60011},{\"end\":60018,\"start\":60017},{\"end\":60024,\"start\":60023},{\"end\":60033,\"start\":60032},{\"end\":60039,\"start\":60038},{\"end\":60344,\"start\":60343},{\"end\":60351,\"start\":60350},{\"end\":60360,\"start\":60359},{\"end\":60366,\"start\":60365},{\"end\":60374,\"start\":60373},{\"end\":60701,\"start\":60700},{\"end\":60703,\"start\":60702},{\"end\":60716,\"start\":60715},{\"end\":60718,\"start\":60717},{\"end\":60728,\"start\":60727},{\"end\":60730,\"start\":60729},{\"end\":61079,\"start\":61078},{\"end\":61086,\"start\":61085},{\"end\":61092,\"start\":61091},{\"end\":61101,\"start\":61100},{\"end\":61397,\"start\":61396},{\"end\":61404,\"start\":61403},{\"end\":61412,\"start\":61411},{\"end\":61420,\"start\":61419},{\"end\":61422,\"start\":61421},{\"end\":61430,\"start\":61429},{\"end\":61432,\"start\":61431},{\"end\":61440,\"start\":61439},{\"end\":61694,\"start\":61693},{\"end\":61703,\"start\":61702},{\"end\":61710,\"start\":61709},{\"end\":61728,\"start\":61727},{\"end\":61730,\"start\":61729},{\"end\":62156,\"start\":62155},{\"end\":62162,\"start\":62161},{\"end\":62168,\"start\":62167},{\"end\":62174,\"start\":62173},{\"end\":62183,\"start\":62182},{\"end\":62442,\"start\":62441},{\"end\":62457,\"start\":62456},{\"end\":62468,\"start\":62467},{\"end\":62482,\"start\":62481},{\"end\":62492,\"start\":62491},{\"end\":62506,\"start\":62505},{\"end\":62508,\"start\":62507},{\"end\":62516,\"start\":62515},{\"end\":62525,\"start\":62524},{\"end\":62533,\"start\":62532},{\"end\":62544,\"start\":62543},{\"end\":62943,\"start\":62942},{\"end\":62950,\"start\":62949},{\"end\":62957,\"start\":62956},{\"end\":62964,\"start\":62963},{\"end\":62970,\"start\":62969},{\"end\":62980,\"start\":62979},{\"end\":63338,\"start\":63337},{\"end\":63347,\"start\":63346},{\"end\":63354,\"start\":63353},{\"end\":63356,\"start\":63355},{\"end\":63364,\"start\":63363},{\"end\":63371,\"start\":63370},{\"end\":63649,\"start\":63648},{\"end\":63656,\"start\":63655},{\"end\":63664,\"start\":63663},{\"end\":63666,\"start\":63665},{\"end\":63674,\"start\":63673},{\"end\":63676,\"start\":63675},{\"end\":63954,\"start\":63953},{\"end\":63961,\"start\":63960},{\"end\":63969,\"start\":63968},{\"end\":63978,\"start\":63977},{\"end\":64260,\"start\":64259},{\"end\":64269,\"start\":64265},{\"end\":64522,\"start\":64521},{\"end\":64528,\"start\":64527},{\"end\":64535,\"start\":64534},{\"end\":64544,\"start\":64543},{\"end\":64820,\"start\":64819},{\"end\":64826,\"start\":64825},{\"end\":64834,\"start\":64833},{\"end\":64840,\"start\":64839},{\"end\":65108,\"start\":65107},{\"end\":65114,\"start\":65113},{\"end\":65124,\"start\":65123},{\"end\":65135,\"start\":65134},{\"end\":65426,\"start\":65425},{\"end\":65433,\"start\":65432},{\"end\":65440,\"start\":65439},{\"end\":65656,\"start\":65655},{\"end\":65662,\"start\":65661},{\"end\":65671,\"start\":65670},{\"end\":65677,\"start\":65676},{\"end\":65685,\"start\":65684},{\"end\":65692,\"start\":65691},{\"end\":65698,\"start\":65697},{\"end\":66046,\"start\":66045},{\"end\":66052,\"start\":66051},{\"end\":66061,\"start\":66060},{\"end\":66067,\"start\":66066},{\"end\":66076,\"start\":66075},{\"end\":66478,\"start\":66477},{\"end\":66484,\"start\":66483},{\"end\":66490,\"start\":66489},{\"end\":66499,\"start\":66498},{\"end\":66509,\"start\":66505},{\"end\":66831,\"start\":66830},{\"end\":66838,\"start\":66837},{\"end\":66846,\"start\":66845},{\"end\":66854,\"start\":66853},{\"end\":67106,\"start\":67105},{\"end\":67115,\"start\":67114},{\"end\":67122,\"start\":67121},{\"end\":67129,\"start\":67128},{\"end\":67136,\"start\":67135},{\"end\":67144,\"start\":67143},{\"end\":67439,\"start\":67438},{\"end\":67446,\"start\":67445},{\"end\":67453,\"start\":67452},{\"end\":67460,\"start\":67459},{\"end\":67468,\"start\":67467},{\"end\":67475,\"start\":67474},{\"end\":67783,\"start\":67779},{\"end\":67793,\"start\":67789},{\"end\":67800,\"start\":67799},{\"end\":67810,\"start\":67806},{\"end\":67819,\"start\":67815},{\"end\":68134,\"start\":68133},{\"end\":68143,\"start\":68142},{\"end\":68152,\"start\":68151},{\"end\":68428,\"start\":68427},{\"end\":68440,\"start\":68439},{\"end\":68457,\"start\":68456},{\"end\":68467,\"start\":68466},{\"end\":68482,\"start\":68481},{\"end\":68484,\"start\":68483},{\"end\":68954,\"start\":68953},{\"end\":68956,\"start\":68955},{\"end\":68966,\"start\":68965},{\"end\":68976,\"start\":68975},{\"end\":68986,\"start\":68985},{\"end\":68998,\"start\":68997},{\"end\":69000,\"start\":68999},{\"end\":69010,\"start\":69009},{\"end\":69020,\"start\":69019},{\"end\":69031,\"start\":69030},{\"end\":69395,\"start\":69394},{\"end\":69402,\"start\":69401},{\"end\":69674,\"start\":69673},{\"end\":69681,\"start\":69680},{\"end\":69688,\"start\":69687},{\"end\":69695,\"start\":69694},{\"end\":69704,\"start\":69703},{\"end\":69710,\"start\":69709},{\"end\":69718,\"start\":69717},{\"end\":69726,\"start\":69725},{\"end\":70039,\"start\":70038},{\"end\":70045,\"start\":70044},{\"end\":70054,\"start\":70053},{\"end\":70061,\"start\":70060},{\"end\":70070,\"start\":70069},{\"end\":70077,\"start\":70076},{\"end\":70087,\"start\":70083},{\"end\":70398,\"start\":70397},{\"end\":70409,\"start\":70408},{\"end\":70418,\"start\":70417},{\"end\":70713,\"start\":70712},{\"end\":70720,\"start\":70719},{\"end\":70727,\"start\":70726},{\"end\":70729,\"start\":70728},{\"end\":70737,\"start\":70736},{\"end\":70739,\"start\":70738},{\"end\":71046,\"start\":71045},{\"end\":71058,\"start\":71057},{\"end\":71313,\"start\":71312},{\"end\":71321,\"start\":71320},{\"end\":71323,\"start\":71322},{\"end\":71332,\"start\":71331},{\"end\":71334,\"start\":71333},{\"end\":71344,\"start\":71343},{\"end\":71346,\"start\":71345},{\"end\":71620,\"start\":71619},{\"end\":71626,\"start\":71625},{\"end\":71883,\"start\":71882},{\"end\":71892,\"start\":71891},{\"end\":71903,\"start\":71902},{\"end\":71905,\"start\":71904},{\"end\":72295,\"start\":72294},{\"end\":72302,\"start\":72301},{\"end\":72308,\"start\":72307},{\"end\":72648,\"start\":72647},{\"end\":72656,\"start\":72655},{\"end\":72664,\"start\":72663},{\"end\":72670,\"start\":72669},{\"end\":72993,\"start\":72992},{\"end\":73005,\"start\":73004},{\"end\":73282,\"start\":73281},{\"end\":73289,\"start\":73288},{\"end\":73291,\"start\":73290},{\"end\":73588,\"start\":73587},{\"end\":73595,\"start\":73594},{\"end\":73602,\"start\":73601},{\"end\":73609,\"start\":73608},{\"end\":73904,\"start\":73903},{\"end\":73910,\"start\":73909},{\"end\":73918,\"start\":73917},{\"end\":74134,\"start\":74133},{\"end\":74141,\"start\":74140},{\"end\":74386,\"start\":74385},{\"end\":74394,\"start\":74393},{\"end\":74396,\"start\":74395},{\"end\":74408,\"start\":74407},{\"end\":74682,\"start\":74681},{\"end\":74690,\"start\":74689},{\"end\":74697,\"start\":74696},{\"end\":74703,\"start\":74702},{\"end\":74942,\"start\":74941},{\"end\":74949,\"start\":74948},{\"end\":74956,\"start\":74955},{\"end\":75170,\"start\":75169},{\"end\":75177,\"start\":75176},{\"end\":75184,\"start\":75183},{\"end\":75191,\"start\":75190}]", "bib_author_last_name": "[{\"end\":58658,\"start\":58656},{\"end\":58666,\"start\":58662},{\"end\":58674,\"start\":58670},{\"end\":58680,\"start\":58678},{\"end\":58687,\"start\":58684},{\"end\":58926,\"start\":58924},{\"end\":58932,\"start\":58930},{\"end\":58938,\"start\":58936},{\"end\":59197,\"start\":59192},{\"end\":59203,\"start\":59201},{\"end\":59574,\"start\":59571},{\"end\":59584,\"start\":59578},{\"end\":59591,\"start\":59588},{\"end\":59599,\"start\":59595},{\"end\":59612,\"start\":59603},{\"end\":59618,\"start\":59616},{\"end\":60015,\"start\":60013},{\"end\":60021,\"start\":60019},{\"end\":60030,\"start\":60025},{\"end\":60036,\"start\":60034},{\"end\":60045,\"start\":60040},{\"end\":60348,\"start\":60345},{\"end\":60357,\"start\":60352},{\"end\":60363,\"start\":60361},{\"end\":60371,\"start\":60367},{\"end\":60380,\"start\":60375},{\"end\":60713,\"start\":60704},{\"end\":60725,\"start\":60719},{\"end\":60735,\"start\":60731},{\"end\":61083,\"start\":61080},{\"end\":61089,\"start\":61087},{\"end\":61098,\"start\":61093},{\"end\":61106,\"start\":61102},{\"end\":61401,\"start\":61398},{\"end\":61409,\"start\":61405},{\"end\":61417,\"start\":61413},{\"end\":61427,\"start\":61423},{\"end\":61437,\"start\":61433},{\"end\":61445,\"start\":61441},{\"end\":61700,\"start\":61695},{\"end\":61707,\"start\":61704},{\"end\":61725,\"start\":61711},{\"end\":61741,\"start\":61731},{\"end\":62159,\"start\":62157},{\"end\":62165,\"start\":62163},{\"end\":62171,\"start\":62169},{\"end\":62180,\"start\":62175},{\"end\":62187,\"start\":62184},{\"end\":62454,\"start\":62443},{\"end\":62465,\"start\":62458},{\"end\":62479,\"start\":62469},{\"end\":62489,\"start\":62483},{\"end\":62503,\"start\":62493},{\"end\":62513,\"start\":62509},{\"end\":62522,\"start\":62517},{\"end\":62530,\"start\":62526},{\"end\":62541,\"start\":62534},{\"end\":62562,\"start\":62545},{\"end\":62947,\"start\":62944},{\"end\":62954,\"start\":62951},{\"end\":62961,\"start\":62958},{\"end\":62967,\"start\":62965},{\"end\":62977,\"start\":62971},{\"end\":62986,\"start\":62981},{\"end\":63344,\"start\":63339},{\"end\":63351,\"start\":63348},{\"end\":63361,\"start\":63357},{\"end\":63368,\"start\":63365},{\"end\":63375,\"start\":63372},{\"end\":63653,\"start\":63650},{\"end\":63661,\"start\":63657},{\"end\":63671,\"start\":63667},{\"end\":63681,\"start\":63677},{\"end\":63958,\"start\":63955},{\"end\":63966,\"start\":63962},{\"end\":63975,\"start\":63970},{\"end\":63983,\"start\":63979},{\"end\":64263,\"start\":64261},{\"end\":64272,\"start\":64270},{\"end\":64525,\"start\":64523},{\"end\":64532,\"start\":64529},{\"end\":64541,\"start\":64536},{\"end\":64548,\"start\":64545},{\"end\":64823,\"start\":64821},{\"end\":64831,\"start\":64827},{\"end\":64837,\"start\":64835},{\"end\":64846,\"start\":64841},{\"end\":65111,\"start\":65109},{\"end\":65121,\"start\":65115},{\"end\":65132,\"start\":65125},{\"end\":65140,\"start\":65136},{\"end\":65430,\"start\":65427},{\"end\":65437,\"start\":65434},{\"end\":65445,\"start\":65441},{\"end\":65659,\"start\":65657},{\"end\":65668,\"start\":65663},{\"end\":65674,\"start\":65672},{\"end\":65682,\"start\":65678},{\"end\":65689,\"start\":65686},{\"end\":65695,\"start\":65693},{\"end\":65704,\"start\":65699},{\"end\":66049,\"start\":66047},{\"end\":66058,\"start\":66053},{\"end\":66064,\"start\":66062},{\"end\":66073,\"start\":66068},{\"end\":66079,\"start\":66077},{\"end\":66481,\"start\":66479},{\"end\":66487,\"start\":66485},{\"end\":66496,\"start\":66491},{\"end\":66503,\"start\":66500},{\"end\":66515,\"start\":66510},{\"end\":66835,\"start\":66832},{\"end\":66843,\"start\":66839},{\"end\":66851,\"start\":66847},{\"end\":66859,\"start\":66855},{\"end\":67112,\"start\":67107},{\"end\":67119,\"start\":67116},{\"end\":67126,\"start\":67123},{\"end\":67133,\"start\":67130},{\"end\":67141,\"start\":67137},{\"end\":67150,\"start\":67145},{\"end\":67443,\"start\":67440},{\"end\":67450,\"start\":67447},{\"end\":67457,\"start\":67454},{\"end\":67465,\"start\":67461},{\"end\":67472,\"start\":67469},{\"end\":67478,\"start\":67476},{\"end\":67787,\"start\":67784},{\"end\":67797,\"start\":67794},{\"end\":67804,\"start\":67801},{\"end\":67813,\"start\":67811},{\"end\":67825,\"start\":67820},{\"end\":68140,\"start\":68135},{\"end\":68149,\"start\":68144},{\"end\":68160,\"start\":68153},{\"end\":68437,\"start\":68429},{\"end\":68454,\"start\":68441},{\"end\":68464,\"start\":68458},{\"end\":68479,\"start\":68468},{\"end\":68491,\"start\":68485},{\"end\":68963,\"start\":68957},{\"end\":68973,\"start\":68967},{\"end\":68983,\"start\":68977},{\"end\":68995,\"start\":68987},{\"end\":69007,\"start\":69001},{\"end\":69017,\"start\":69011},{\"end\":69028,\"start\":69021},{\"end\":69038,\"start\":69032},{\"end\":69399,\"start\":69396},{\"end\":69406,\"start\":69403},{\"end\":69678,\"start\":69675},{\"end\":69685,\"start\":69682},{\"end\":69692,\"start\":69689},{\"end\":69701,\"start\":69696},{\"end\":69707,\"start\":69705},{\"end\":69715,\"start\":69711},{\"end\":69723,\"start\":69719},{\"end\":69730,\"start\":69727},{\"end\":70042,\"start\":70040},{\"end\":70051,\"start\":70046},{\"end\":70058,\"start\":70055},{\"end\":70067,\"start\":70062},{\"end\":70074,\"start\":70071},{\"end\":70081,\"start\":70078},{\"end\":70092,\"start\":70088},{\"end\":70406,\"start\":70399},{\"end\":70415,\"start\":70410},{\"end\":70426,\"start\":70419},{\"end\":70717,\"start\":70714},{\"end\":70724,\"start\":70721},{\"end\":70734,\"start\":70730},{\"end\":70744,\"start\":70740},{\"end\":71055,\"start\":71047},{\"end\":71068,\"start\":71059},{\"end\":71318,\"start\":71314},{\"end\":71329,\"start\":71324},{\"end\":71341,\"start\":71335},{\"end\":71357,\"start\":71347},{\"end\":71623,\"start\":71621},{\"end\":71632,\"start\":71627},{\"end\":71889,\"start\":71884},{\"end\":71900,\"start\":71893},{\"end\":71911,\"start\":71906},{\"end\":72299,\"start\":72296},{\"end\":72305,\"start\":72303},{\"end\":72314,\"start\":72309},{\"end\":72653,\"start\":72649},{\"end\":72661,\"start\":72657},{\"end\":72667,\"start\":72665},{\"end\":72675,\"start\":72671},{\"end\":73002,\"start\":72994},{\"end\":73012,\"start\":73006},{\"end\":73286,\"start\":73283},{\"end\":73297,\"start\":73292},{\"end\":73592,\"start\":73589},{\"end\":73599,\"start\":73596},{\"end\":73606,\"start\":73603},{\"end\":73614,\"start\":73610},{\"end\":73907,\"start\":73905},{\"end\":73915,\"start\":73911},{\"end\":73921,\"start\":73919},{\"end\":74138,\"start\":74135},{\"end\":74146,\"start\":74142},{\"end\":74391,\"start\":74387},{\"end\":74405,\"start\":74397},{\"end\":74419,\"start\":74409},{\"end\":74687,\"start\":74683},{\"end\":74694,\"start\":74691},{\"end\":74700,\"start\":74698},{\"end\":74706,\"start\":74704},{\"end\":74946,\"start\":74943},{\"end\":74953,\"start\":74950},{\"end\":74961,\"start\":74957},{\"end\":75174,\"start\":75171},{\"end\":75181,\"start\":75178},{\"end\":75188,\"start\":75185},{\"end\":75194,\"start\":75192}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9263669},\"end\":58850,\"start\":58594},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52073194},\"end\":59092,\"start\":58852},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1483488},\"end\":59441,\"start\":59094},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9248856},\"end\":59926,\"start\":59443},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":71142966},\"end\":60230,\"start\":59928},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":67872646},\"end\":60604,\"start\":60232},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":216738},\"end\":61000,\"start\":60606},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":67868933},\"end\":61312,\"start\":61002},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":46849537},\"end\":61649,\"start\":61314},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9433631},\"end\":62089,\"start\":61651},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":213637621},\"end\":62384,\"start\":62091},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4704285},\"end\":62852,\"start\":62386},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":18530739},\"end\":63228,\"start\":62854},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2880403},\"end\":63591,\"start\":63230},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":18649677},\"end\":63881,\"start\":63593},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7573433},\"end\":64196,\"start\":63883},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5060429},\"end\":64451,\"start\":64198},{\"attributes\":{\"doi\":\"arXiv:1908.01703\",\"id\":\"b17\"},\"end\":64727,\"start\":64453},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":205432776},\"end\":65032,\"start\":64729},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4801840},\"end\":65381,\"start\":65034},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":34363444},\"end\":65575,\"start\":65383},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":199688641},\"end\":65903,\"start\":65577},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":199466224},\"end\":66368,\"start\":65905},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":212708911},\"end\":66761,\"start\":66370},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11925688},\"end\":67024,\"start\":66763},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":199677411},\"end\":67344,\"start\":67026},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":86757630},\"end\":67710,\"start\":67346},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5160005},\"end\":68081,\"start\":67712},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10409742},\"end\":68356,\"start\":68083},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":208280698},\"end\":68853,\"start\":68358},{\"attributes\":{\"doi\":\"arXiv:1510.02055\",\"id\":\"b30\"},\"end\":69309,\"start\":68855},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":16119343},\"end\":69603,\"start\":69311},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":15486640},\"end\":69993,\"start\":69605},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206769127},\"end\":70326,\"start\":69995},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":980236},\"end\":70647,\"start\":70328},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4741938},\"end\":70975,\"start\":70649},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b36\"},\"end\":71236,\"start\":70977},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":207761262},\"end\":71588,\"start\":71238},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4853851},\"end\":71807,\"start\":71590},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":52860200},\"end\":72217,\"start\":71809},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":19494799},\"end\":72516,\"start\":72219},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":41156725},\"end\":72901,\"start\":72518},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":120241709},\"end\":73230,\"start\":72903},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":8788358},\"end\":73467,\"start\":73232},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":54462396},\"end\":73865,\"start\":73469},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":493320},\"end\":74082,\"start\":73867},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":16101144},\"end\":74319,\"start\":74084},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":17113905},\"end\":74625,\"start\":74321},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":53278663},\"end\":74897,\"start\":74627},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":34363444},\"end\":75091,\"start\":74899},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":41804650},\"end\":75374,\"start\":75093}]", "bib_title": "[{\"end\":58652,\"start\":58594},{\"end\":58920,\"start\":58852},{\"end\":59188,\"start\":59094},{\"end\":59567,\"start\":59443},{\"end\":60009,\"start\":59928},{\"end\":60341,\"start\":60232},{\"end\":60698,\"start\":60606},{\"end\":61076,\"start\":61002},{\"end\":61394,\"start\":61314},{\"end\":61691,\"start\":61651},{\"end\":62153,\"start\":62091},{\"end\":62439,\"start\":62386},{\"end\":62940,\"start\":62854},{\"end\":63335,\"start\":63230},{\"end\":63646,\"start\":63593},{\"end\":63951,\"start\":63883},{\"end\":64257,\"start\":64198},{\"end\":64817,\"start\":64729},{\"end\":65105,\"start\":65034},{\"end\":65423,\"start\":65383},{\"end\":65653,\"start\":65577},{\"end\":66043,\"start\":65905},{\"end\":66475,\"start\":66370},{\"end\":66828,\"start\":66763},{\"end\":67103,\"start\":67026},{\"end\":67436,\"start\":67346},{\"end\":67777,\"start\":67712},{\"end\":68131,\"start\":68083},{\"end\":68425,\"start\":68358},{\"end\":69392,\"start\":69311},{\"end\":69671,\"start\":69605},{\"end\":70036,\"start\":69995},{\"end\":70395,\"start\":70328},{\"end\":70710,\"start\":70649},{\"end\":71310,\"start\":71238},{\"end\":71617,\"start\":71590},{\"end\":71880,\"start\":71809},{\"end\":72292,\"start\":72219},{\"end\":72645,\"start\":72518},{\"end\":72990,\"start\":72903},{\"end\":73279,\"start\":73232},{\"end\":73585,\"start\":73469},{\"end\":73901,\"start\":73867},{\"end\":74131,\"start\":74084},{\"end\":74383,\"start\":74321},{\"end\":74679,\"start\":74627},{\"end\":74939,\"start\":74899},{\"end\":75167,\"start\":75093}]", "bib_author": "[{\"end\":58660,\"start\":58654},{\"end\":58668,\"start\":58660},{\"end\":58676,\"start\":58668},{\"end\":58682,\"start\":58676},{\"end\":58689,\"start\":58682},{\"end\":58928,\"start\":58922},{\"end\":58934,\"start\":58928},{\"end\":58940,\"start\":58934},{\"end\":59199,\"start\":59190},{\"end\":59205,\"start\":59199},{\"end\":59576,\"start\":59569},{\"end\":59586,\"start\":59576},{\"end\":59593,\"start\":59586},{\"end\":59601,\"start\":59593},{\"end\":59614,\"start\":59601},{\"end\":59620,\"start\":59614},{\"end\":60017,\"start\":60011},{\"end\":60023,\"start\":60017},{\"end\":60032,\"start\":60023},{\"end\":60038,\"start\":60032},{\"end\":60047,\"start\":60038},{\"end\":60350,\"start\":60343},{\"end\":60359,\"start\":60350},{\"end\":60365,\"start\":60359},{\"end\":60373,\"start\":60365},{\"end\":60382,\"start\":60373},{\"end\":60715,\"start\":60700},{\"end\":60727,\"start\":60715},{\"end\":60737,\"start\":60727},{\"end\":61085,\"start\":61078},{\"end\":61091,\"start\":61085},{\"end\":61100,\"start\":61091},{\"end\":61108,\"start\":61100},{\"end\":61403,\"start\":61396},{\"end\":61411,\"start\":61403},{\"end\":61419,\"start\":61411},{\"end\":61429,\"start\":61419},{\"end\":61439,\"start\":61429},{\"end\":61447,\"start\":61439},{\"end\":61702,\"start\":61693},{\"end\":61709,\"start\":61702},{\"end\":61727,\"start\":61709},{\"end\":61743,\"start\":61727},{\"end\":62161,\"start\":62155},{\"end\":62167,\"start\":62161},{\"end\":62173,\"start\":62167},{\"end\":62182,\"start\":62173},{\"end\":62189,\"start\":62182},{\"end\":62456,\"start\":62441},{\"end\":62467,\"start\":62456},{\"end\":62481,\"start\":62467},{\"end\":62491,\"start\":62481},{\"end\":62505,\"start\":62491},{\"end\":62515,\"start\":62505},{\"end\":62524,\"start\":62515},{\"end\":62532,\"start\":62524},{\"end\":62543,\"start\":62532},{\"end\":62564,\"start\":62543},{\"end\":62949,\"start\":62942},{\"end\":62956,\"start\":62949},{\"end\":62963,\"start\":62956},{\"end\":62969,\"start\":62963},{\"end\":62979,\"start\":62969},{\"end\":62988,\"start\":62979},{\"end\":63346,\"start\":63337},{\"end\":63353,\"start\":63346},{\"end\":63363,\"start\":63353},{\"end\":63370,\"start\":63363},{\"end\":63377,\"start\":63370},{\"end\":63655,\"start\":63648},{\"end\":63663,\"start\":63655},{\"end\":63673,\"start\":63663},{\"end\":63683,\"start\":63673},{\"end\":63960,\"start\":63953},{\"end\":63968,\"start\":63960},{\"end\":63977,\"start\":63968},{\"end\":63985,\"start\":63977},{\"end\":64265,\"start\":64259},{\"end\":64274,\"start\":64265},{\"end\":64527,\"start\":64521},{\"end\":64534,\"start\":64527},{\"end\":64543,\"start\":64534},{\"end\":64550,\"start\":64543},{\"end\":64825,\"start\":64819},{\"end\":64833,\"start\":64825},{\"end\":64839,\"start\":64833},{\"end\":64848,\"start\":64839},{\"end\":65113,\"start\":65107},{\"end\":65123,\"start\":65113},{\"end\":65134,\"start\":65123},{\"end\":65142,\"start\":65134},{\"end\":65432,\"start\":65425},{\"end\":65439,\"start\":65432},{\"end\":65447,\"start\":65439},{\"end\":65661,\"start\":65655},{\"end\":65670,\"start\":65661},{\"end\":65676,\"start\":65670},{\"end\":65684,\"start\":65676},{\"end\":65691,\"start\":65684},{\"end\":65697,\"start\":65691},{\"end\":65706,\"start\":65697},{\"end\":66051,\"start\":66045},{\"end\":66060,\"start\":66051},{\"end\":66066,\"start\":66060},{\"end\":66075,\"start\":66066},{\"end\":66081,\"start\":66075},{\"end\":66483,\"start\":66477},{\"end\":66489,\"start\":66483},{\"end\":66498,\"start\":66489},{\"end\":66505,\"start\":66498},{\"end\":66517,\"start\":66505},{\"end\":66837,\"start\":66830},{\"end\":66845,\"start\":66837},{\"end\":66853,\"start\":66845},{\"end\":66861,\"start\":66853},{\"end\":67114,\"start\":67105},{\"end\":67121,\"start\":67114},{\"end\":67128,\"start\":67121},{\"end\":67135,\"start\":67128},{\"end\":67143,\"start\":67135},{\"end\":67152,\"start\":67143},{\"end\":67445,\"start\":67438},{\"end\":67452,\"start\":67445},{\"end\":67459,\"start\":67452},{\"end\":67467,\"start\":67459},{\"end\":67474,\"start\":67467},{\"end\":67480,\"start\":67474},{\"end\":67789,\"start\":67779},{\"end\":67799,\"start\":67789},{\"end\":67806,\"start\":67799},{\"end\":67815,\"start\":67806},{\"end\":67827,\"start\":67815},{\"end\":68142,\"start\":68133},{\"end\":68151,\"start\":68142},{\"end\":68162,\"start\":68151},{\"end\":68439,\"start\":68427},{\"end\":68456,\"start\":68439},{\"end\":68466,\"start\":68456},{\"end\":68481,\"start\":68466},{\"end\":68493,\"start\":68481},{\"end\":68965,\"start\":68953},{\"end\":68975,\"start\":68965},{\"end\":68985,\"start\":68975},{\"end\":68997,\"start\":68985},{\"end\":69009,\"start\":68997},{\"end\":69019,\"start\":69009},{\"end\":69030,\"start\":69019},{\"end\":69040,\"start\":69030},{\"end\":69401,\"start\":69394},{\"end\":69408,\"start\":69401},{\"end\":69680,\"start\":69673},{\"end\":69687,\"start\":69680},{\"end\":69694,\"start\":69687},{\"end\":69703,\"start\":69694},{\"end\":69709,\"start\":69703},{\"end\":69717,\"start\":69709},{\"end\":69725,\"start\":69717},{\"end\":69732,\"start\":69725},{\"end\":70044,\"start\":70038},{\"end\":70053,\"start\":70044},{\"end\":70060,\"start\":70053},{\"end\":70069,\"start\":70060},{\"end\":70076,\"start\":70069},{\"end\":70083,\"start\":70076},{\"end\":70094,\"start\":70083},{\"end\":70408,\"start\":70397},{\"end\":70417,\"start\":70408},{\"end\":70428,\"start\":70417},{\"end\":70719,\"start\":70712},{\"end\":70726,\"start\":70719},{\"end\":70736,\"start\":70726},{\"end\":70746,\"start\":70736},{\"end\":71057,\"start\":71045},{\"end\":71070,\"start\":71057},{\"end\":71320,\"start\":71312},{\"end\":71331,\"start\":71320},{\"end\":71343,\"start\":71331},{\"end\":71359,\"start\":71343},{\"end\":71625,\"start\":71619},{\"end\":71634,\"start\":71625},{\"end\":71891,\"start\":71882},{\"end\":71902,\"start\":71891},{\"end\":71913,\"start\":71902},{\"end\":72301,\"start\":72294},{\"end\":72307,\"start\":72301},{\"end\":72316,\"start\":72307},{\"end\":72655,\"start\":72647},{\"end\":72663,\"start\":72655},{\"end\":72669,\"start\":72663},{\"end\":72677,\"start\":72669},{\"end\":73004,\"start\":72992},{\"end\":73014,\"start\":73004},{\"end\":73288,\"start\":73281},{\"end\":73299,\"start\":73288},{\"end\":73594,\"start\":73587},{\"end\":73601,\"start\":73594},{\"end\":73608,\"start\":73601},{\"end\":73616,\"start\":73608},{\"end\":73909,\"start\":73903},{\"end\":73917,\"start\":73909},{\"end\":73923,\"start\":73917},{\"end\":74140,\"start\":74133},{\"end\":74148,\"start\":74140},{\"end\":74393,\"start\":74385},{\"end\":74407,\"start\":74393},{\"end\":74421,\"start\":74407},{\"end\":74689,\"start\":74681},{\"end\":74696,\"start\":74689},{\"end\":74702,\"start\":74696},{\"end\":74708,\"start\":74702},{\"end\":74948,\"start\":74941},{\"end\":74955,\"start\":74948},{\"end\":74963,\"start\":74955},{\"end\":75176,\"start\":75169},{\"end\":75183,\"start\":75176},{\"end\":75190,\"start\":75183},{\"end\":75196,\"start\":75190}]", "bib_venue": "[{\"end\":60799,\"start\":60772},{\"end\":61884,\"start\":61822},{\"end\":62215,\"start\":62206},{\"end\":64037,\"start\":64015},{\"end\":66119,\"start\":66104},{\"end\":68214,\"start\":68192},{\"end\":68621,\"start\":68561},{\"end\":70484,\"start\":70460},{\"end\":72023,\"start\":71972},{\"end\":58700,\"start\":58689},{\"end\":58951,\"start\":58940},{\"end\":59243,\"start\":59205},{\"end\":59658,\"start\":59620},{\"end\":60058,\"start\":60047},{\"end\":60393,\"start\":60382},{\"end\":60770,\"start\":60737},{\"end\":61120,\"start\":61108},{\"end\":61458,\"start\":61447},{\"end\":61820,\"start\":61743},{\"end\":62204,\"start\":62189},{\"end\":62585,\"start\":62564},{\"end\":63013,\"start\":62988},{\"end\":63388,\"start\":63377},{\"end\":63708,\"start\":63683},{\"end\":64013,\"start\":63985},{\"end\":64299,\"start\":64274},{\"end\":64519,\"start\":64453},{\"end\":64859,\"start\":64848},{\"end\":65184,\"start\":65142},{\"end\":65458,\"start\":65447},{\"end\":65717,\"start\":65706},{\"end\":66102,\"start\":66081},{\"end\":66542,\"start\":66517},{\"end\":66872,\"start\":66861},{\"end\":67163,\"start\":67152},{\"end\":67500,\"start\":67480},{\"end\":67876,\"start\":67827},{\"end\":68190,\"start\":68162},{\"end\":68559,\"start\":68493},{\"end\":68951,\"start\":68855},{\"end\":69434,\"start\":69408},{\"end\":69770,\"start\":69732},{\"end\":70132,\"start\":70094},{\"end\":70458,\"start\":70428},{\"end\":70784,\"start\":70746},{\"end\":71043,\"start\":70977},{\"end\":71384,\"start\":71359},{\"end\":71672,\"start\":71634},{\"end\":71970,\"start\":71913},{\"end\":72341,\"start\":72316},{\"end\":72688,\"start\":72677},{\"end\":73040,\"start\":73014},{\"end\":73322,\"start\":73299},{\"end\":73641,\"start\":73616},{\"end\":73948,\"start\":73923},{\"end\":74181,\"start\":74148},{\"end\":74443,\"start\":74421},{\"end\":74733,\"start\":74708},{\"end\":74974,\"start\":74963},{\"end\":75207,\"start\":75196}]"}}}, "year": 2023, "month": 12, "day": 17}