{"id": 233481695, "updated": "2023-10-06 04:21:49.215", "metadata": {"title": "IPatch: A Remote Adversarial Patch", "authors": "[{\"first\":\"Yisroel\",\"last\":\"Mirsky\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Applications such as autonomous vehicles and medical screening use deep learning models to localize and identify hundreds of objects in a single frame. In the past, it has been shown how an attacker can fool these models by placing an adversarial patch within a scene. However, these patches must be placed in the target location and do not explicitly alter the semantics elsewhere in the image. In this paper, we introduce a new type of adversarial patch which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch. We call this new class of adversarial examples `remote adversarial patches' (RAP). We implement our own RAP called IPatch and perform an in-depth analysis on image segmentation RAP attacks using five state-of-the-art architectures with eight different encoders on the CamVid street view dataset. Moreover, we demonstrate that the attack can be extended to object recognition models with preliminary results on the popular YOLOv3 model. We found that the patch can change the classification of a remote target region with a success rate of up to 93% on average.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.00113", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/cybersec/Mirsky23", "doi": "10.1186/s42400-023-00145-0"}}, "content": {"source": {"pdf_hash": "6b60da73fd18a73d12482c09558c07d6a48a575f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.00113v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2d5df48d2a00302f025994a690e84f1d5cd23d01", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6b60da73fd18a73d12482c09558c07d6a48a575f.txt", "contents": "\nIPatch: A Remote Adversarial Patch Yisroel Mirsky\n\n\nJournal Of L At E X Class \nFiles \nVol \nXx \nNo \nMonth Xx \nXxxx \nIPatch: A Remote Adversarial Patch Yisroel Mirsky\nIndex Terms-Adversarial machine learningadversarial patchmisdirectionAI securityoffensive AI\nApplications such as autonomous vehicles and medical screening use deep learning models to localize and identify hundreds of objects in a single frame. In the past, it has been shown how an attacker can fool these models by placing an adversarial patch within a scene. However, these patches must be placed in the target location and do not explicitly alter the semantics elsewhere in the image.In this paper, we introduce a new type of adversarial patch which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch. We call this new class of adversarial examples 'remote adversarial patches' (RAP).We implement our own RAP called IPatch and perform an in-depth analysis on image segmentation RAP attacks using five state-of-the-art architectures with eight different encoders on the CamVid street view dataset. Moreover, we demonstrate that the attack can be extended to object recognition models with preliminary results on the popular YOLOv3 model. We found that the patch can change the classification of a remote target region with a success rate of up to 93% on average.\n\nI. INTRODUCTION\n\nDeep learning has become the go-to method for automating image-based tasks.This is because, deep neural networks (DNNs) are excellent at learning and identifying spatial patterns and abstract concepts. With advances in both hardware and neural architectures, deep learning has become both a practical and reliable solution. Companies now use image-based deep learning to automate tasks in life critical operations such as autonomous driving [1], [2], surveillance [3], and medical image screening [4].\n\nIn tasks such as these, multiple objects must be identified per image. One way to accomplish this is to predict a class probability for each pixel in the input image . This approach is called image segmentation and companies such as Telsa use it to guide their autonomous vehicles safely through an environment [2]. Another approach is called object detection where is split into a grid of cells or regions and the model predicts both a class probability and a bounding box for each of them [5], [6]. In both cases, these models rely on image semantics to successfully parse and interpret a scene.\n\nJust like other deep learning models, these semantic models are also susceptible to adversarial attacks. In 2017, researchers demonstrated how a small 'adversarial' patch can be placed in a real world scene and override an image-classifier's prediction, regardless of the patch's location or orientation [7]. This gave rise to a number of works which demonstrated the concept of adversarial patches against image segmentation and object detection models [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19]. However, current adversarial patches are limited in the following ways: Location Only predictions around the patch itself are explicitly affected. This limits where objects can be made to 'appear'\n\nThe author is with Ben-Gurion University (e-mail: yisroel@post.bgu.ac.il see https://ymirsky.github.io/).\n\n\nOriginal\n\nWith Patch Scene Segmentation Car Tree Pavement Road Building Sky Other in a scene. For example, a patch cannot make a plane appear in the sky and it is difficult to put a patch in the middle of a busy road. Furthermore, patches in noticeable areas can raise suspicion (e.g., a stop sign with a colorful patch on it). Interpretation Existing patches do not explicitly alter the shape or layout of a scene's perceived semantics. Changes to these semantics can be used to guide behaviors (e.g., drive a car off the road [20] or change a head count [21]) and has wide implications on tasks such as surveillance [22], [3] and medical screening [23] among others. In this paper we identify a new type of attack which we call a Remote Adversarial Patch (RAP). A RAP is an adversarial patch which can alter an image's perceived semantics from a remote location in the image. Our implementation of a RAP (IPatch) can be placed anywhere in the field of view and alter the predictions of nearly any predetermined location within the same view. This is demonstrated in Fig. 1 where an attacker has crafted an IPatch which causes a segmentation model to think that there is pavement (a sidewalk) in the middle of the road. Moreover, this adversarial attack is robust because the same patch works on different images using different positions and scales. Therefore, this attack more flexible and more covert than previous approaches. later in section III we discuss the attack model further.\n\nSince the IPatch can alter an image's perceived semantics, and attacker can craft patches which cause these models to see objects of arbitrary shapes and classes. For example, in Fig. 2  tree shaped like the USENIX security symposium logo (top) or the NDSS logo (bottom). This is possible because semantic models rely on global and contextual features to parse an image. However, an object and its contextual information can be very far apart in . For example, consider an image with a boat next to the water. Here, the water will boost the confidence of the boat's classification even though the boat is not in the water. The IPatch exploits these correlations by masquerading as these contextual features.\n\nCreating a robust RAP is more challenging than existing adversarial patches. This is because the content of directly affects the leverage of the patch. For example, an IPatch cannot make a segmentation model perceive remote semantics on a blank image. However, to create a robust patch, we must be able to generalize to different images which have not been seen before. To overcome these challenges, we (1) use an incremental training strategy to slowly increase the entropy of the expectation over transformation (EoT) objective and (2) use Kullback-Leibler divergence loss to help the optimizer leverage and exploit the contextual relationships.\n\nIn this paper, we focus on the use of IPatches as a RAP against semantic segmentation models. We also demonstrate that the same technique can be applied to object detectors, such as YOLO, as well. To evaluate the IPatch, we train 37 segmentation models using 8 different encoders and 5 state-of-the-art architectures. In our evaluations, we focus on the autonomous car scenario [2], [24], and perform rigorous tests to determine the limitations and capabilities of the attack. On the top 4 classes, we found that the attack works up to 93% of the time on average, depending on the victim's model. We also found that all of the segmentation models are susceptible to the attack, where the most susceptible architectures were the FPN and Unet++ and the least susceptible architecture was the PSPNet. Finally, even if the attacker does not have the same architecture as the victim, we found that without any additional training effort, an IPatch trained on one architecture works on others with an attack success rate of up to 25.3%.\n\nThe contributions of this paper are as follows:\n\n\u2022 We introduce a new class of adversarial patches (RAP) which can manipulate a scene's interpretation remotely and explicitly. This type of attack not only has significant implications on the security of autonomous vehicles, but also on a wide range of semantic-based applications such as medical scan analysis, surveillance, and robotics (section III). \u2022 We present a training framework which enables the creation of a robust RAP (IPatch) by incrementally increasing the training entropy. Without this strategy, the entropy starts too high which makes it difficult to converge on some learning objectives, especially given large patch transformations on scale, shift, and so on (section IV). \u2022 We provide an in-depth evaluation of the patch used as a remote adversarial attack against road segmentation models (section V). We show that the attack is robust, universal (works on unseen images sampled from the same distribution), and has transferability (works across multiple models). We also provide initial results which demonstrate that the attack works on object detectors as well (specifically YOLOv3). \u2022 We identify the attack's limitations and provide insight as to why this attack can alter the perception of remote regions in an image. Building on these observations, we suggest countermeasures and directions for future work (section VII). \u2022 To the best of our knowledge, this the first adversarial patch demonstrated on segmentation models (section II).\n\nTo reproduce our results, the reader can access the code and models used in this paper online. 1\n\n\nII. RELATED WORKS\n\nSoon after the popularization of deep learning, researchers demonstrated that DNNs can be exploited using adversarial examples [25]. In 2014 it was shown how an attacker can alter an image-classification model's predictions by adding an imperceivable amount of noise to the input image [26], [27], [28]. Initially, these attacks were impractical to perform in a real environment since every combination of lighting, camera noise, and perspective would require a different adversarial perturbation [29], [30]. However, in 2017 the authors of [31] showed that an adversary can consider these distortions while generating the adversarial example in a process called Expectation over Transformation (EoT). Using this method, the authors were able to generate robust adversarial samples which can be deployed in the real world. In the same year, the authors of [7] used EoT to create adversarial patches. Their adversarial patches were designed to fool image-classifiers (single-object detection models).\n\nLater in 2018, the authors of [8] developed an adversarial patch that works on object detection models (multi-object detection models). More recently, researchers have proposed patches which can remove objects which wear the patch [9], [14], [13], [19], [17], [16], [15] and patches which can perform denial of service (DoS) attacks by corrupting a scene's interpretation [9], [12].\n\nIn Table I, we summarize the related works on adversarial examples against image segmentation and object detection models (the domain of the proposed attack). In general, the attack goals of these papers are either add/change an object in the scene or to remove all objects altogether (DoS). The methods which add adversarial perturbations (noise) can change the semantics of an image at any location [32], [33], [34], [35], [36], but they cannot be deployed in the real world since they are applied directly to an image itself. Currently, there no patches for image segmentation models, and the patches for object detection models only affect the prediction around the patch  itself. The exception are patches which perform DoS attacks by removing/corrupting all objects detected in the scene like [9], [12]. Therefore, to the best of our knowledge, the attack which we introduce is the first RAP, and (1) the only method which can add, change, or remove objects in a scene remotely (far from the location of the patch itself), (2) the first adversarial patch proposed for segmentation networks, and (3) the first adversarial patch which can cause a model to perceive custom semantic shapes.\n\n\nIII. THREAT MODEL\n\nThe Vulnerability. The vulnerability which this paper introduces is that semantic models, such as image segmentation models, utilize global and contextual features in an image to improve their predictive capabilities. However, these dependencies expose channels which can an attacker can exploit to change the interpretation of an image from one remote location to another. The Attack Scenario. In this work we will focus on the remote adversarial attack scenario. In this attack scenario, Alice has an application which uses the image segmentation model . Mallory wants to predict a specific class at a specific location , while looking at a certain scene. To accomplish this, Mallory needs a training set of 1 or more images and a segmentation model to work with.\n\nFor the training set, Mallory has two options: (1) obtain images similar to those used to train , or (2) take pictures of the target scene. For the model, Mallory can either follow a white-box or black-box approach: In a white box-approach, Mallory obtains a copy of to achieve the most accurate results. The white-box approach is a common assumption for adversarial patches. Alternatively, Mallory can follow a black-box approach and train a surrogate model on a similar dataset used to train . Although the black box approach performs worse, we have found that there is some transferability between a patch trained on one model and then used against another (section V). Finally, Mallory generates an IPatch which targets using and .\n\nMotivation. There are several reasons why an adversary would want to use an IPatch over an ordinary adversarial patch (illustrated in Fig. 3 contextually identified as malicious [11] but a sticker on a nearby billboard is less obvious. Another example, is in the domain of medicine where segmentation models are used to highlight and identify different lesions such as tumors. Here, an attacker can't put a patch in the image in the location of the lesion since it would be an obvious attack. However, the attack could be trigger remotely by placing a dark RAP in the  dark space of a scan where it is common to have noise, or in a location of the scan which is not under investigation (e.g., the first few slices on the z-axis). For motivations why an attacker would want to target medical scans, see [44]. Practicality The attacker may want to generate an object or semantic illusion in a location which is hard to reach or impractical to place a patch on it. For example, in the sky region, on the back of an arbitrary car on the freeway. Flexibility The attacker may need to craft or alter specific semantics for a scene. For example, many works show how image segmentation can be used to identify homes, roads and resources from satellite and drone footage [45]. Here an attacker can feed false intel by hiding or increasing the number of structures, people, and resources before it can be investigated manually. Overall, the IPatch attack is more flexible and enables more attack vectors than location-based patches (e.g., [8], [16]). However, it is significantly more challenging to generate an IPatch. Therefore, its flexibility comes with a trade-off in terms of attack performance.\n\n\nIV. MAKING AN IPATCH\n\nIn this section we first provide an overview of how image segmentation models work. Then we present our approach on how to create an IPatch.\n\n\nA. Technical Background\n\nThere are a wide variety of deep learning models for image segmentation [46], [47]. The most common form involves an encoder and decoder such that\n( ) = ( ( ))(1)\nillustrated in Fig. 4. The objective of a segmentation model is to take an -by-image ( ) with 1-3 color channels and predict an -by--by-probability mapping ( ). The output can be mapped directly to the pixels of such that [ , , ] is the probability that pixel [ , ] belongs to the -th class (among the possible classes).\n\nTo train , the common approach is to follow two phases: In the first phase, the encoder network is trained as an image-classifier on a large image dataset in a supervised manner (i.e., where each image is associated with a label ). Note that the classifier's task is to predict a single class for the entire image (e.g., is a dog). After training the classifier, we discard the dense layers at the end of the network (used to predict ) and retain the convolutional layers at the front of . In this way, we can use the feature mapping learned by the classifier to perform image segmentation. In the second phase, the decoder architecture is added on and is trained end-to-end. Often, the weights of are locked during this phase, and we do the same in this paper.\n\nOne reason why the encoder-decoder approach is so popular, is because obtaining a labeled segmentation ground truth is significantly more challenging than for image classification (massive datasets for classification exist and new datasets can be crowd sourced as well). Therefore, by using a pre-trained encoder, far fewer examples of segmentations are needed to achieve quality results.\n\nTo train , a differentiable loss function L is used to compare the model's predicted output to the ground truth in order to perform backpropagation and update network's weights. There are many loss functions used in for segmentation. One common approach is to simply apply the binary cross entropy loss (L ) since is essentially trying to solve a multi-class classification problem. However, L does not consider whether a pixel is on the boundary or not so results tends to be blurry and be biased to large segments such as backgrounds [48]. To counter this issue, in 2016 the authors of [49] proposed using Dice loss (L ) for medical image segmentation, and it has since been considered a state-of-the-art approach. The Dice loss is defined as\nL ( , ) = 2 2 + 2(2)\nWe use L to train all of the image segmentation models in this paper.\n\nWhen selecting the encoder's model, there are a wide variety of options. Some include ResNext, DenseNet, xception, EfficientNet, MobileNet, DPN, VGG, and variations thereof. However, regarding the decoder's architecture, there are several which are considered state-of-the-art. Many of them utilize a 'feature pyramid' approach and skip connections to identify features at multiple scales, or an autoencoder (encoder decoder pair) to encode and extract the semantics. We will now briefly describe the five architectures used in this paper: Unet++ [50]: An autoencoder architecture which improves on its predecessor, the Unet. The encoder and decoder are connected through a series of nested dense skip connections which reduce the semantic gap between the feature maps of the two networks. Linknet [51]: An efficient autoencoder which passes spatial information across the network to avoid losing it in the encoder's compression. FPN [52]: A feature pyramid network which uses lateral connections across a fully convolution neural network (FCN) to utilize feature maps learned from multiple image scales. PSPNet [53]: An FCN which uses a pyramid parsing module on different sub-region representations in order to better capture global category clues. The architecture won first place in multiple segmentation challenge contests. PAN [54]: A network which uses both pyramid and global attention mechanisms to capture spatial and global semantic information.\n\n\nB. Approach\n\nIn a remote adversarial attack, the attacker wants a region around the location = ( , ) to be predicted as class . To ensure that the optimizer does not waste energy on other semantics in the scene, we focus the effort to a region of operation. Let denote the region operation and let be the target pattern for that region. To capture , we use an -by--by-mask of zeros. To select , a square or circle with a radius of pixels 2 around in is marked with ones along the -th channel. To insert an object, we set = since our objective is to change the probability of those pixels to one. To insert a custom shape (like in Fig. 2) is set accordingly.\n\nTo generate a patch for the objective ( , ), we follow the EoT approach similar to previous works [7], [8], [9], but using our semantic masks. Concretely, we would like to find a patch which is trained to optimize the following objective function\n=argmin E \u223c , \u223c\u2113, \u223c [ ( ( ,\u02c6, , )) \u2212 ](3)\nwhere is a distribution of input images, \u2113 is a distribution of patch locations, and is a set of scales to resize\u02c6. The operator is the 'Apply' procedure which takes the current\u02c6and inserts it into while sampling uniformly on the distributions , \u2113, and . Loss Function. We experimented with many different loss functions on the CamVid dataset [55]: L ,L ,L 1 ,L 2 , and L (Kullback-Leibler Divergence loss). Most of these loss functions took too long to converge or got stuck in local optima. Instead, we found that L 1 works best in emptier scenes (like Fig. 2) and L works best in busy scenes like those in CamVid. We believe the reason why L performs well busy scenes is because it measures the relative entropy from one distribution to another. As a result, the optimizer had an easier time 'leeching' nearby features and contexts in to match the goal in . Creating a Robust RAP. In order to make an RAP which is robust to different transformations (scale and location), and universal to different images (not in the training set of the patch), we must use EoT. However, in some cases we found that the patch does not converge well when the range of ( , \u2113, ) is large (i.e, large shifts, hundreds of images, etc). This is because (1) the IPatch leverages the variable contents of to impact ( ) and (2) the placement of in affects the influence of on ( ). For these cases, we propose an incremental training strategy where we gradually increase the placement radius of the patch \u2113. Whenever the training has converged or a time limit has elapsed, we increase the radius by one pixel. We repeat this process until the entire dataset is covered. At the start of each epoch, we give the optimizer time to adjust by setting the learning rate to a fraction of its value and then slowly ramp it back up. A similar strategy can be applied to the other distributions, such as the number of images in , the shift size, or the patch scale. This strategy works well 2 For an with a dimension of 384x480, we found that a radius of 50 pixels empirically performs best when targeting region with a radius of 10 pixels. because we gradually increase the entropy, enabling the optimizer capture foundational concepts. It can also be viewed that at each epoch we are placing the gradient descent optimizer at a more advantageous position instead of a random starting point.\n\nTo demonstrate the value of the incremental strategy, we performed an experiment. We trained a RAP using 70k images from the BDD100k street segmentation dataset [56]. The RAP was configured to make the center of th image perceived as the class 'tree' when placed in any location within the image (i.e., a placement radius of 500 pixels). The experiment was performed using (1) our incremental training strategy by increasing the placement radius up to the maximum radius and (2) the baseline approach of training the patch using the maximum radius from the start. For the incremental strategy, the placement radius was increased by one pixel whenever the attack success rate reached 25%. In Fig. 5, we plot the results from the experiment. We found that that our approach reaches the maximum radius after one hour and then exceeds the performance of the baseline shortly after by a margin of 35%.\n\nIn summary, the training framework for creating an IPatch is as follows (illustrated in Fig. 6):\n\n\nTraining Procedure for an IPatch\n\nInitialize\u02c6with random values and set its origin (default location in ) to be . If incremental, then add one image to . Otherwise, add all images to . Repeat until\u02c6has converged on the entire dataset: 1) Apply: Draw a batch of samples from . For each sample in the batch, perform a random transformation: scale down and shift its location from origin . 2) Forward pass: Pass the batch through and obtain the segmentation maps (as a set of ). 3) Apply mask: Take the product of each with the mask to omit irrelevant semantics. 4) Loss & gradient: Compute the loss L ( * , ) and use it to perform back propagation through to\u02c6. 5-6. Update: Use gradient descent (e.g., Adamax) to update the values of\u02c6. 7. If incremental and the has time elapsed or training has converged, then increase the entropy (e.g., patch placement radius) and ramp the learning rate.\n\n\nV. EVALUATION\n\nTo evaluate the IPatch as a RAP, we will focus our evaluation on the scenario of autonomous vehicles. The task of street view segmentation is challenging because the scenes are typically very busy with many layers, objects, and wide perspectives [24]. Therefore attacking this application is will provide us with good insights into the IPatch's capabilities. Datasets. We use the CamVid dataset [55] to train our segmentation models and evaluate our adversarial patches. The CamVid dataset is a well-known benchmark dataset used for image segmentation. It contains 46,869 street view images with a resolution of 360x480 from the point of view of a car. The images are supplied with pixel-wise annotations which indicate the class of the corresponding content (e.g., car, building, etc). The dataset comes split into three partitions: train , test , and validation . We use to train the segmentation models and the rest to train the patches. This way there is will be no bias on the images which we attack. The dataset is used to evaluate the influence of the patch's parameters (size and location) and to train robust patches with EoT. Finally, the dataset is use to validate that the robust patches work on unseen imagery. Segmentation Models. In our evaluations we trained and attacked 37 different models which were combinations of 8 different encoders and 5 state-of-the-art segmentation architectures. 3 The encoders were the vgg19, densenet121, efficientnet-b4, efficientnet-b7, mobilenet_v2, resnext50_32x4d, dpn68, and xception. All of the models were obtained from the Torch library and were pretrained on the ImageNet Dataset [57]. For the architectures, we used the implementations 4 of the state-of-the-art segmentation networks described in section IV-A. The models were trained on for 100 epochs each, with a batch size of 8, learning rate of 1e-4, using Dice Loss L and an Adam optimizer. Finally, to increase the training set size and improve generalization, we performed data augmentation. The augmentations were: flip, shift, crop, blur, sharpen and change perspective, brightness, and gamma. The Experiments. We performed three experiments: EXP1: In this experiment we investigate the influence which a patch's size and location have on the attack performance. We also investigate the influence of the remote target's size and location. Here patches are crafted to target individual images. 3 Every combination of encoder and architecture except for the architecture PAN which was incompatible with three of the encoders. 4 https://github.com/qubvel/segmentation_models.pytorch Therefore, the results of this experiment also tell us how well the attack performs on static images. EXP2: To use this attack in the wild, the patch must work under various transformations and in new scenes. This experiment evaluates the attack's robustness by (1) training the patches with EoT according to (3), and by (2) measuring the performance of these patches on new images (unseen during training). EXP3: To get an idea of the vulnerability's prevalence, we attack 32 different segmentation models and measure their performance. To evaluate the case where the attacker has no information on the model, we take the robust patches trained in EXP2 and use them on the other 36 models to measure the attack's transferability. When measuring attack performance, we omit all cases where the targeted region already contains the target class. For all of the experiments, we trained on an NVIDIA Titan RTX with 24GB of RAM. For the optimizer, we experimented on a variety of options in the Torch library. We found that the Adamax optimizer works best on the CamVid Dataset.\n\n\nA. EXP1: The Impact of Size and Location\n\nThe purpose of this experiment is to see how the size and locations of a patch and its target affect the attack's performance. In this experiment, we craft patches which target a single image. Later in section V-B we evaluate multi-image 'robust' patches. Experiment Setup. For EXP1 we attacked the efficientnet-b7_FPN model since it performed best on the CamVid dataset. A list the evaluations and parameters used in EXP1 can be found in Table II. For each of these parameters, we varied their values while locking the rest to measure their influence. This was repeated for each of the model's top six performing classes. Due to time restrictions, 5 we only used the entirety of for the fixed parameter experiment. For the other experiments, we used 20 random images from . The training procedure was as follows: For each patch, we used a learning rate of 2.5 and stopped the training after three minutes to ensure that each of the five experiments would take no more than 5 days. We note that in many cases, the patches were still converging  so the results can be improved. Finally, we count a successful attack as any image with at least 80% of marked by the model as the target class.\n\n1) Performance with all Parameters Locked: The results for the experiment, where the patch parameters are locked, can be found in Fig. 7. The top of the figure shows that the attack has a greater impact on structural classes than others. This might be because these semantics have the largest regions CamVid dataset (i.e., are common). As a result, the patch is able to leverage these contexts better from one side of an image to another. For example, if there are is a row of buildings on one side of the road, then there is a higher probability that the other side will have one too. This kind of correlation is exploited by the patch. The road class out performs all the rest because the target in this experiment is in the center of the image, where the road is most commonly found (77% of the images). However, the patch is able to successfully attack the classes of pavement, building, and tree at the same location, even though on the clean images, the model predicts 1.4%, 0.3%, and 0% of them to have these classes respectively.\n\nAt the bottom of Fig. 7 we can see the aggregated confidence of the model for each of the images in . The plot shows that all of the images are susceptible to the attack for at least one of the target classes.\n\n2) Impact of the Patch Size: Figure 9 plots the model's confidence over increasingly larger patch sizes. In the figure, we have marked 0.5 as the decision threshold which is the default for segmentation models. This is because segmentation models perform binary-classification on each pixel. As a result, the confidence scores per class are either close to zero or one, but not so much in between (as seen in Fig. 7).\n\nAs expected, larger patches increase the attack success rate. However, the trade off appears to be linear (captured by the average in red). What is meaningful about these results is that some classes excel with smaller patch sizes (e.g., pavement and building) while others require larger ones to succeed (e.g., tree). This is probably because some of the remote contextual semantics which the model considers cannot be compressed into small spaces when others can. Overall, we observe that the minimum patch size required to fool the model on a static image this size is about 60-75 pixels in width, and with a patch width of 100 pixels, nearly all attacks succeed.\n\n3) Impact of the Patch Location: In Fig. 10 we can see that the attack is highly effective for all classes up to about 62% of the distance away from the target (image center). The sharp drop in attack performance for the tree and sky classes is understandable since there are fewer contextual semantics which can be exploited by the patch in the bottom right of the image. On the other hand, in areas just below the horizon (0-0.5 on the x-axis), the patch can exploit contextual semantics which the model uses (e.g., features such as lighting, reflections, and building geometry).\n\nThese results indicate that an attacker may be able to increase the likelihood of success by placing the patch on objects which have some contextual influence on the target region. For example, to create a crosswalk, it may be advantageous to put the sticker on a lamp post or parking meter since these objects may be found near crosswalks.\n\n4) Impact of the Target Size: In this experiment, we increased the size of but observe the performance of the same 20x20 pixel region at the center of (i.e., our objective). In Fig. 11 we can see that large targets do not perform well. The reason for this is that having a large target requires the IPatch to subdue more semantics. As a result, the patch fails and the region of becomes patchy and an corrupted. Small targets fail because it is hard for the patch to make high precision results. Rather, there is a balance between the intended 20x20 target and the actual target painted in . We found that increasing the target size by a factor of 3 improves the performance at the intended region.\n\nThe reason why a larger target helps the patch reach the 20x20 region is that the patch tends to 'leech' nearby semantic regions. This makes sense since it is easier to change the boundaries of existing semantics (e.g., perceive a larger car) than generate new ones which are isolated (e.g., a tree in the middle of the road). Therefore, the added target size encourages the model to perform similar tactics.  Fig. 8 we present the attack performance when targeting different remote locations in . It is clear that the influence of a patch on different regions is dependent on both the image's content and the targeted class. For example, it is easier to convince the model that any space under the horizon is a road, yet it is hard to change the class of the top-center to building because it is rarely found there. Overall, this experiment demonstrates that the patch can target locations on far remote locations within the image. However, this capability is not uniform across the classes, as we can see with the class 'car'.\n\n\n5) Impact of the Target Location: In\n\n\nB. EXP2: Patch Robustness\n\nIn is experiment we evaluate how well a single patch performs on (1) different transformations and (2) Fig. 11. The average attack performance on the same 20x20 region, but with different target sizes. framework from IV-B was used with the patch origin set to (370,270). For the patch size , we sampled uniformly on the range of [50,80] pixels. For the shifts \u2113, we sampled uniformly within the entire bottom-right quadrant of . We found that training the patch in one region helps it converge using the incremental strategy, while still generalizing to the opposite side. We targeted the same segmentation model used in EXP1, and the training was performed using a batch size of 20 (the maximum for a 24GB GPU) with a learning rate of 0.5. Generalization to Multiple Images In Fig. 12 we present the performance of the patches in the form of the model's perception. The images demonstrate that the IPatch generalizes well to multiple images, even at different locations and scales. In Fig. 13 we present the attack performance when training on different numbers of images from (evaluated against the same set). From here we can see that an exponential number of examples are needed to increase the performance. Generalization to New Images. To use the patch in a real world setting, it must work well in scenes which were not in the attacker's training set. Figure 14 presents the attack performance of patches trained on (those displayed in Fig. 12) when applied to images in . The results show that the patches generalize well to unseen images. More interestingly, the performance of some classes are dramatically different compared to patches trained on single images without EoT (EXP1, Fig. 7). For example, 'tree' now has a 0.98% success rate compared to 50% and 'building' is now 25% compared to 65%. We learn from this that by considering multiple images, the model can learn stronger tactics. At the same time, the variability of the transformations prevent the model from using highly specific  adversarial patterns. We also note that the class 'car' does not transfer to unseen images like the other classes. We attribute this to the segmentation model's poor performance on detecting cars in general. 6 \n\n\nC. EXP3: The Impact on Different Models\n\nIn is experiment we explore the suceptibilty and transferability of patches between models.\n\n1) Model Susceptibility: Experiment Setup. To evaluate the performance of the attack on different model architectures, we used 32 of the 37 segmentation models described at the beginning of section V (PAN was omitted since it was not compatible with Torch's autograd in our framework). Due to time limitations, the 6 Although the selected efficientnet-b7_FPN achieves a lesser intersection over union score of 0.75 on that class, it outperforms the other models overall.  Results. We found that all 36 models are susceptible to the RAP attack for at least one class (Fig. 15). By observing the patterns in the columns, we note that some architectures are less susceptible to attacks on certain classes. For example, Linknet, PSPNet, and Unet++ on pavement and PSPNet on car.\n\nIn Fig. 16, we can see the suceptibilty of the encoders and architectures overall. Some of the most susceptible encoders (xception and resnext) and architectures (FPN and Unet++) use skip connections or residual pathways in their networks. These pathways enable the networks to capture features at multiple scales and capture the global contexts better. However, just as these network utilize these pathways to obtain better perspectives, so can the IPatch in order to reach deeper into the image. Interestingly we found that the dpn68 encoder is consistently resilient against the attack. This encoder is formally called a Dual Path Network [58]. It uses a residual path like a ResNet to reuse learned features and a densely connected path like DenseNet to encourage the network to explore new features. These diverse features may be preventing the IPatch, and possibly the segmentation model, from reaching remote contexts.\n\n2) Inter-Model Transferability: In the case where the attacker does not have knowledge of the victim's model, we would like to know well a patch trained on one model transfers to others. Experiment Setup. To perform this experiment, we took the robust patches trained using the efficientnet-b7_FPN (EXP2) and attacked each of the other 36 models (listed in Fig. 17). The patches for the top 4 classes (sky, building, pavement, tree) were applied to the images in using the random transformations described in EXP2. Results. We found that patch from efficientnet-b7_FPN can influence the other models' predictions on the target region with an attack success rate of 11-37% (about 1-4 times in every 10 cases). We note that a cameras on an autonomous car processes at least 30 frames per second. Therefore, there is a high likelihood that the car's model will be susceptible to the attack while driving by. Fig. 17 shows the largest confidences for each model, measured as the relative increase from the original confidence (on clean image). Interestingly models using the Unet++ architecture were the most susceptible, followed by Linknet. We believe the reason for this is that both of these models use skip-connections to allow for feature maps to bypass the encoding process. As a result, features in the patch have a more direct impact on the output. It is known that skip-connections make models more vulnerable to adversarial examples [59] but it is interesting to see that they are vulnerable to transfer attacks as well. Another observation is that there does not seem to be a correlation between the results and the encoder used. This is probably because all of the encoders were trained on the same ImageNet Dataset.\n\nFinally, we note that there are ways in which an adversarial example can be made to be more transferable [60], [61]. We leave this for future work.\n\n\nVI. EXTENDING TO OBJECT RECOGNITION\n\nIn section V we performed an in-depth evaluation and analysis of the IPatch as a RAP against segmentation models. However, the same training framework in IV-B can be used on other semantic models as well. In this section, we present preliminary results against a popular object recognition model called YOLOv3  \n\n\nA. Technical Background\n\nThe family of YOLO models follow a similar architecture (Fig.  18). is passed through a series of convolutional layers ( in the figure) and then those feature maps are shuttled to various decoders. The decoders predict coarse maps to the image at different scales using the semantic information shared between them. The multiple scales help the model detect objects of different sizes (e.g., 1 detects large objects). Each cell in a map, contains an objectness score, class probability, and a bounding box (obtained via regression). If a cell has an objectness score above some threshold, then there is an object there with the associated class probability. Finally, a non-maximal suppression (NMS) algorithm is used on the maps to identify and unify the detections.\n\n\nB. Evaluation\n\nExperiment Setup. To see if the attack would work on YOLO, we created an IPatch which convinces YOLO that there is a person standing in the middle of the road. To accomplish this, we used a pre-trained YOLOv3 model implementation 7 as the victim, and trained our patch using 30k images: 15k random samples from the Bdd100k dataset [62] and 15k frames from a Toronto car driving video on YouTube 8 .\n\nFor training, we needed to ensure that both the objectness score and probability of the class 'person' were high. This was done by taking the product of 1's probability map and objectness map as , and by setting to highlight the cell in the lower-center of the image. For the loss functions, we took the sum of L and L 1 since it increased the rate of convergence. EoT was used to scale the patch between 60-70 pixels in width and shift it randomly within the bottom-right quadrant of the image. Finally, we trained the patch for 3 days with a learning rate of 0.05. Results. We found that the YOLOv3 object detector is susceptible to the attack with an 85% attack success rate. In Fig. 19 we present an example frame which shows the objectness and probability maps in 1 during the attack. We also found that smaller patches ranging from 50-60 pixels in width achieve an 80% attack success rate. Overall, it was relatively easy for the framework to change the objectness score of arbitrary locations in the image, compared to the class probability. We also observed that it is significantly harder to target the maps from 2 and 3 which capture smaller objects. We believe this is because 2 and 3 rely less on contexts around the image, giving the IPatch less leverage to perform a remote attack. 7 https://github.com/eriklindernoren/PyTorch-YOLOv3 8 https://youtu.be/50Uf_T12OGY\n\nObjectness Probability (person) Fig. 19. An example of the YOLOv3 object detector being attacked by an IPatch.\n\nHere, the patch has convinced the model that there is a person standing in the middle of the road.\n\nAs future work, we plan to explore RAPs on other object detectors and investigate other semantic models as well.\n\n\nVII. DISCUSSION & COUNTERMEASURES\n\nThe concept of a remote adversarial patch, introduced in this paper, opens up wide range of possible attack vectors against image-based semantic models. Through our observations in V, we were able to identify some of the attack's capabilities and limitations. Trade-Offs. Due to its flexibility, it may seem like the IPatch is harder to defend against compared to an ordinary adversarial patch. However, the performance of the patch is less compared to a 'point-based' patch. This means that the adversary must consider whether a more reliable attack needed over having flexibility and stealth. Another consideration is that the adversary may want to experiment to find the optimal placement of the patch. This is because some regions give the patch more leverage based on the local semantics (section V-A3). One strategy is that the attacker can first scout the target region by videoing the scene from multiple perspectives and then optimize the patch location using that dataset. Defenses. Although the IPatch can be placed in arbitrary locations, we noticed that its presence highly noticeable in the semantic segmentations (e.g., Fig. 12). We found that it is very hard to generate a patch which both achieves the attack and masks its own presence at the same time. Concretely, when setting = except for the target region (as done in Fig 2), we found that the model struggles to influence remote locations to the same extent. In future work, this may be improved through a custom loss function which balances the trade-off between the two objectives. Another solution might be to generate RAPs using a conditional GAN which considers the errors on the semantic map in (\u00ac ). Doing so may also reduce the corruptions to nearby semantics as well.\n\nAnother direction for defending against this attack is to limit the model's dependency on global features. Although these global features are key to state-of-the-art models [52], [54], [63], it is possible to utilize them while also considering their layout and origin. One option may be to integrate capsule networks [64] as part of the model's architecture, since capsule networks are good at considering the spatial relationship in images. Improvements. We noticed that the RAP attack is dependent on an image's content when targeting segmentation models, but less so for the object detector YOLO. For example, we were able to perform remote adversarial attacks on a blank image with YOLO. The reason for this is not clear to us, and investigating it may lead to improvements in the proposed training methodology. Moreover, as future work, it would be interesting to investigate which types of features and classes the a RAP can manipulate best and why. This research may lead to deeper insights into the vulnerability's extents and limitations. Finally, to improve transferabilty, we suggest two directions: (1) include multiple models in the training loop to help the model identify common features, and (2) use adversarial training to improve the generalization of the patch.\n\nWe also noticed that performance improves when the pixel values of the patch are not clipped to the range 0-255. This is because larger inputs enable the patch to exploit weaker contextual pathways by saturating activations in the network. As future work, it would be interesting to see if an incremental strategy can be used on the clipping range to ultimately produce stronger patches.\n\n\nVIII. CONCLUSION\n\nIn this paper, we have introduced the concept of a 'remote adversarial patch' (RAP) which can alter the semantic interpretation of an image while being placed anywhere within the field of view. We have implemented an RAP called IPatch and demonstrated that it is robust, can generalize to new scenes, and can impact other semantic models such as object detectors. With an average attack success rate of up to 93%, this attack forms a tangible threat. Although RAPs are in their infancy, we hope that this paper has laid some of the groundwork for exploring this new adversarial example.\n\nIn summary, neural networks are notorious for being black-boxes which are difficult to interpret. However, they are still used in critical tasks because their advantages outweigh their potential disadvantages. We hope that our findings will help the community improve the security of deep learning applications so that we may continue to benefit from safe and reliable autonomous systems.\n\nFig. 1 .\n1An example of a remote adversarial attack on a segmentation model. Here the patch has been designed to alter a predetermined location (the red box) to the class of 'pavement'. The same patch works on different images and with different positions and scales.\n\nFig. 2 .\n2a street view segmentation model is convinced that a slice of bread is a arXiv:2105.00113v2 [cs.CV] An example of how an IPatch can change the semantics of an image to an arbitrary shape. Here, a street view segmentation model is convinced that the slice of bread is a tree in the shape of the USENIX logo (top) and NDSS logo (bottom).\n\nFig. 3 .\n3Some illustrative examples showing how a remote adversarial patch can be used by an attacker.\n\nFig. 5 .\n5The benefit of using the proposed incremental training strategy (incrementing the patch placement radius) when training on the BDD100k dataset.\n\nFig. 6 .\n6An overview of the IPatch training framework for creating a RAP. For simplicity, only one sample is shown, though in practice batches of images are used.\n\nFig. 7 .\n7The attack performance with locked parameters. Top-The average attack performance per class. Bottom-the aggregated confidence of the model on each image, where the max confidence per class is one.\n\nFig. 8 .Fig. 9 .Fig. 10 .\n8910The remote attack performance on ten images. A region in the heat map indicates the performance of the attack when targeting that location. The average attack performance with different patch sizes. The average attack performance with different patch locations, shifted from near-center of to the bottom-right.\n\nFig. 12 .\n12Examples showing the semantic segmentation maps of efficientnet-b7_FPN when attacked with 'robust' patches trained using EoT. Top two rowsthe original image and the original segmentation map, where the target and patch are marked in red and white accordingly. Right-the IPatch used in the attack, each one has been trained for a different class but the same remote target location.\n\nFig. 13 .\n13The performance of a robust patch when trained on different set sizes.\n\nFig. 14 .\n14The average attack performance of the patches when applied to new images. attacks on each model were limited to 4 classes, 10 images, and 3 minutes training time for each image.\n\nFig. 15 .Fig. 16 .\n1516The average confidence of different models when attacked with an IPatch. Values greater than 0.5 indicate a successful attack on average. The average confidence of different encoders and architectures when under attack.\n\nFig. 17 .\n17The average attack success rate on each model when attacked with a patch trained on efficientnet-b7_FPN.\n\n\nobjectness, class probability, and bounding box for every location (different resolutions, 3 sets each) Fig. 18. The architecture of the YOLOv3 object detector.\n\n\nmap\u2032 \n\nactual \nclass \n\nSingle object \nimage \n\nMulti-object \n(semantic) \nimage \n\n\u2032 \n\nsegmentations: \npred. & actual \n\nMulti-object \n(semantic) \nimage \n\n\u2032 \n\n\u2112 \n\npredicted \nclass \n\nparameter update \n\nconvolutional \nlayers \n\ndense \nlayers \n\n\u2112 \n\nTrain Phase I \n\nTrain \n\nPhase \n\nII \n\nExecution \n\nFig. 4. A basic schematic showing how a common Image Segmentation model \nis trained and executed. \n\n\n\nTABLE II THE\nIIPARAMETERS USED IN EXP1. THE VALUES LISTED FOR 'SIZE' ARE BOTH THE HEIGHT AND WIDTH.\nThe code and models will be made available here https://github.com/ymirsky/ IPatch\nEach of these experiments on takes 3-5 days on a NVIDIA Titan RTX with 24GB RAM.\n\nWaymo reminds us: Successful complex ai combines deep learning and traditional code. K Leetaru, Accessed on 02/04/2021K. Leetaru, \"Waymo reminds us: Successful complex ai combines deep learning and traditional code,\" https://www.forbes.com/sites/kalevleetaru/ 2019/04/18/waymo-reminds-us-\\successful-complex-ai-combines-deep\\ -learning-and-traditional\\-code/?sh=4eb596723a2c, 2019, (Accessed on 02/04/2021).\n\nAutopilot ai-tesla. Accessed on 02/04/2021\"Autopilot ai-tesla,\" https://www.tesla.com/autopilotAI, 2021, (Accessed on 02/04/2021).\n\nArtificial intelligence is going to supercharge surveillance the verge. J Vincent, Accessed on 02/04/2021J. Vincent, \"Artificial intelligence is going to supercharge surveillance the verge,\" https://www.theverge.com/2018/1/23/16907238/artificial-intelligence-\\ surveillance-cameras-\\security, 2018, (Accessed on 02/04/2021).\n\nDeep resolve. Siemens, Accessed on 02/04/2021Siemens, \"Deep resolve,\" https://www.siemens-healthineers.com/en-us/ magnetic-resonance-imaging/technologies-and-innovations/deep-resolve, 2021, (Accessed on 02/04/2021).\n\nYolov3: An incremental improvement. J Redmon, A Farhadi, arXiv:1804.02767arXiv preprintJ. Redmon and A. Farhadi, \"Yolov3: An incremental improvement,\" arXiv preprint arXiv:1804.02767, 2018.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, arXiv:1506.01497arXiv preprintS. Ren, K. He, R. Girshick, and J. Sun, \"Faster r-cnn: Towards real-time object detection with region proposal networks,\" arXiv preprint arXiv:1506.01497, 2015.\n\nAdversarial patch. T B Brown, D Man\u00e9, A Roy, M Abadi, J Gilmer, arXiv:1712.09665arXiv preprintT. B. Brown, D. Man\u00e9, A. Roy, M. Abadi, and J. Gilmer, \"Adversarial patch,\" arXiv preprint arXiv:1712.09665, 2017.\n\nPhysical adversarial examples for object detectors. D Song, K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, F Tramer, A Prakash, T Kohno, 12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18). D. Song, K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, F. Tramer, A. Prakash, and T. Kohno, \"Physical adversarial examples for object detectors,\" in 12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18), 2018.\n\nDpatch: An adversarial patch attack on object detectors. X Liu, H Yang, Z Liu, L Song, H Li, Y Chen, arXiv:1806.02299arXiv preprintX. Liu, H. Yang, Z. Liu, L. Song, H. Li, and Y. Chen, \"Dpatch: An adversarial patch attack on object detectors,\" arXiv preprint arXiv:1806.02299, 2018.\n\nShapeshifter: Robust physical adversarial attack on faster r-cnn object detector. S.-T Chen, C Cornelius, J Martin, D H P Chau, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerS.-T. Chen, C. Cornelius, J. Martin, and D. H. P. Chau, \"Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector,\" in Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2018, pp. 52-68.\n\nDarts: Deceiving autonomous cars with toxic signs. C Sitawarin, A N Bhagoji, A Mosenia, M Chiang, P Mittal, arXiv:1802.06430arXiv preprintC. Sitawarin, A. N. Bhagoji, A. Mosenia, M. Chiang, and P. Mittal, \"Darts: Deceiving autonomous cars with toxic signs,\" arXiv preprint arXiv:1802.06430, 2018.\n\nOn physical adversarial patches for object detection. M Lee, Z Kolter, arXiv:1906.11897arXiv preprintM. Lee and Z. Kolter, \"On physical adversarial patches for object detection,\" arXiv preprint arXiv:1906.11897, 2019.\n\nFooling automated surveillance cameras: adversarial patches to attack person detection. S Thys, W Van Ranst, T Goedem\u00e9, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkshopsS. Thys, W. Van Ranst, and T. Goedem\u00e9, \"Fooling automated surveillance cameras: adversarial patches to attack person detection,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0-0.\n\nSeeing isn't believing: Towards more robust adversarial attack against real world object detectors. Y Zhao, H Zhu, R Liang, Q Shen, S Zhang, K Chen, Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. the 2019 ACM SIGSAC Conference on Computer and Communications SecurityY. Zhao, H. Zhu, R. Liang, Q. Shen, S. Zhang, and K. Chen, \"Seeing isn't believing: Towards more robust adversarial attack against real world object detectors,\" in Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, 2019, pp. 1989-2004.\n\nAdaptive square attack: Fooling autonomous cars with adversarial traffic signs. Y Li, X Xu, J Xiao, S Li, H T Shen, IEEE Internet of Things Journal. Y. Li, X. Xu, J. Xiao, S. Li, and H. T. Shen, \"Adaptive square attack: Fooling autonomous cars with adversarial traffic signs,\" IEEE Internet of Things Journal, 2020.\n\nAdversarial patch camouflage against aerial detection. R Hollander, A Adhikari, I Tolios, M Van Bekkum, A Bal, S Hendriks, M Kruithof, D Gross, N Jansen, G Perez, Artificial Intelligence and Machine Learning in Defense Applications II. 11543115430R. den Hollander, A. Adhikari, I. Tolios, M. van Bekkum, A. Bal, S. Hendriks, M. Kruithof, D. Gross, N. Jansen, G. Perez et al., \"Adversarial patch camouflage against aerial detection,\" in Artificial Intelligence and Machine Learning in Defense Applications II, vol. 11543. International Society for Optics and Photonics, 2020, p. 115430F.\n\nUniversal physical camouflage attacks on object detectors. L Huang, C Gao, Y Zhou, C Xie, A L Yuille, C Zou, N Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionL. Huang, C. Gao, Y. Zhou, C. Xie, A. L. Yuille, C. Zou, and N. Liu, \"Universal physical camouflage attacks on object detectors,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 720-729.\n\nDynamic adversarial patch for evading object detection models. S Hoory, T Shapira, A Shabtai, Y Elovici, arXiv:2010.13070arXiv preprintS. Hoory, T. Shapira, A. Shabtai, and Y. Elovici, \"Dynamic adversarial patch for evading object detection models,\" arXiv preprint arXiv:2010.13070, 2020.\n\nMaking an invisibility cloak: Real world adversarial attacks on object detectors. Z Wu, S.-N Lim, L S Davis, T Goldstein, European Conference on Computer Vision. SpringerZ. Wu, S.-N. Lim, L. S. Davis, and T. Goldstein, \"Making an invisibility cloak: Real world adversarial attacks on object detectors,\" in European Conference on Computer Vision. Springer, 2020, pp. 1-17.\n\nMultinet: Real-time joint semantic reasoning for autonomous driving. M Teichmann, M Weber, M Zoellner, R Cipolla, R Urtasun, 2018 IEEE Intelligent Vehicles Symposium (IV). IEEEM. Teichmann, M. Weber, M. Zoellner, R. Cipolla, and R. Urtasun, \"Multinet: Real-time joint semantic reasoning for autonomous driving,\" in 2018 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2018, pp. 1013-1020.\n\nCpspnet: Crowd counting via semantic segmentation framework. J He, X Wu, J Yang, W Hu, 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI). IEEEJ. He, X. Wu, J. Yang, and W. Hu, \"Cpspnet: Crowd counting via semantic segmentation framework,\" in 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI). IEEE, 2020, pp. 1104-1110.\n\nSatellite imagery feature detection using deep convolutional neural network: A kaggle competition. V Iglovikov, S Mushinskiy, V Osin, arXiv:1706.06169arXiv preprintV. Iglovikov, S. Mushinskiy, and V. Osin, \"Satellite imagery feature detection using deep convolutional neural network: A kaggle competition,\" arXiv preprint arXiv:1706.06169, 2017.\n\nA survey of semantic segmentation on biomedical images using deep learning. Y Prajna, M K Nath, Advances in VLSI, Communication, and Signal Processing. SpringerY. Prajna and M. K. Nath, \"A survey of semantic segmentation on biomedical images using deep learning,\" in Advances in VLSI, Communication, and Signal Processing. Springer, 2021, pp. 347-357.\n\nA comparative study of real-time semantic segmentation for autonomous driving. M Siam, M Gamal, M Abdel-Razek, S Yogamani, M Jagersand, H Zhang, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsM. Siam, M. Gamal, M. Abdel-Razek, S. Yogamani, M. Jagersand, and H. Zhang, \"A comparative study of real-time semantic segmentation for autonomous driving,\" in Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2018, pp. 587-597.\n\nWild patterns: Ten years after the rise of adversarial machine learning. B Biggio, F Roli, Pattern Recognition. 84B. Biggio and F. Roli, \"Wild patterns: Ten years after the rise of adversarial machine learning,\" Pattern Recognition, vol. 84, pp. 317-331, 2018.\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, arXiv:1312.6199arXiv preprintC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \"Intriguing properties of neural networks,\" arXiv preprint arXiv:1312.6199, 2013.\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, arXiv:1412.6572arXiv preprintI. J. Goodfellow, J. Shlens, and C. Szegedy, \"Explaining and harnessing adversarial examples,\" arXiv preprint arXiv:1412.6572, 2014.\n\nDeep neural networks are easily fooled: High confidence predictions for unrecognizable images. A Nguyen, J Yosinski, J Clune, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionA. Nguyen, J. Yosinski, and J. Clune, \"Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 427-436.\n\nFoveation-based mechanisms alleviate adversarial examples. Y Luo, X Boix, G Roig, T Poggio, Q Zhao, arXiv:1511.06292arXiv preprintY. Luo, X. Boix, G. Roig, T. Poggio, and Q. Zhao, \"Foveation-based mecha- nisms alleviate adversarial examples,\" arXiv preprint arXiv:1511.06292, 2015.\n\nNo need to worry about adversarial examples in object detection in autonomous vehicles. J Lu, H Sibai, E Fabry, D Forsyth, arXiv:1707.03501arXiv preprintJ. Lu, H. Sibai, E. Fabry, and D. Forsyth, \"No need to worry about adversarial examples in object detection in autonomous vehicles,\" arXiv preprint arXiv:1707.03501, 2017.\n\nSynthesizing robust adversarial examples. A Athalye, L Engstrom, A Ilyas, K Kwok, International conference on machine learning. PMLRA. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, \"Synthesizing robust adversarial examples,\" in International conference on machine learning. PMLR, 2018, pp. 284-293.\n\nUniversal adversarial perturbations against semantic image segmentation. J Hendrik Metzen, M Kumar, T Brox, V Fischer, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJ. Hendrik Metzen, M. Chaithanya Kumar, T. Brox, and V. Fischer, \"Universal adversarial perturbations against semantic image segmentation,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2755-2764.\n\nAdversarial examples for semantic image segmentation. V Fischer, M C Kumar, J H Metzen, T Brox, arXiv:1703.01101arXiv preprintV. Fischer, M. C. Kumar, J. H. Metzen, and T. Brox, \"Adversarial examples for semantic image segmentation,\" arXiv preprint arXiv:1703.01101, 2017.\n\nOn the robustness of semantic segmentation models to adversarial attacks. A Arnab, O Miksik, P H Torr, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Arnab, O. Miksik, and P. H. Torr, \"On the robustness of semantic segmentation models to adversarial attacks,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 888-897.\n\nImpact of adversarial examples on deep learning models for biomedical image segmentation. U Ozbulak, A Van Messem, W De Neve, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerU. Ozbulak, A. Van Messem, and W. De Neve, \"Impact of adversarial examples on deep learning models for biomedical image segmentation,\" in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2019, pp. 300-308.\n\nAdversarial attacks for image segmentation on multiple lightweight models. X Kang, B Song, X Du, M Guizani, IEEE Access. 8X. Kang, B. Song, X. Du, and M. Guizani, \"Adversarial attacks for image segmentation on multiple lightweight models,\" IEEE Access, vol. 8, pp. 31 359-31 370, 2020.\n\nTransferable adversarial attacks for image and video object detection. X Wei, S Liang, N Chen, X Cao, arXiv:1811.12641arXiv preprintX. Wei, S. Liang, N. Chen, and X. Cao, \"Transferable adversarial attacks for image and video object detection,\" arXiv preprint arXiv:1811.12641, 2018.\n\nObject hider: Adversarial patch attack against object detectors. Y Zhao, H Yan, X Wei, arXiv:2010.14974arXiv preprintY. Zhao, H. Yan, and X. Wei, \"Object hider: Adversarial patch attack against object detectors,\" arXiv preprint arXiv:2010.14974, 2020.\n\nAdversarial objectness gradient attacks in real-time object detection systems. K.-H Chow, L Liu, M Loper, J Bae, M E Gursoy, S Truex, W Wei, Y Wu, 2020 Second IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications. IEEEK.-H. Chow, L. Liu, M. Loper, J. Bae, M. E. Gursoy, S. Truex, W. Wei, and Y. Wu, \"Adversarial objectness gradient attacks in real-time object detection systems,\" in 2020 Second IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA). IEEE, 2020, pp. 263-272.\n\nContextual adversarial attacks for object detection. H Zhang, W Zhou, H Li, 2020 IEEE International Conference on Multimedia and Expo (ICME). IEEEH. Zhang, W. Zhou, and H. Li, \"Contextual adversarial attacks for object detection,\" in 2020 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2020, pp. 1-6.\n\nThe translucent patch: A physical and universal attack on object detectors. A Zolfi, M Kravchik, Y Elovici, A Shabtai, arXiv:2012.12528arXiv preprintA. Zolfi, M. Kravchik, Y. Elovici, and A. Shabtai, \"The translucent patch: A physical and universal attack on object detectors,\" arXiv preprint arXiv:2012.12528, 2020.\n\nFa: A fast method to attack real-time object detection systems. Y Li, G Xu, W Li, 2020 IEEE/CIC International Conference on Communications in China (ICCC). IEEEY. Li, G. Xu, and W. Li, \"Fa: A fast method to attack real-time object detection systems,\" in 2020 IEEE/CIC International Conference on Communications in China (ICCC). IEEE, 2020, pp. 1268-1273.\n\nAdversarial examples for semantic segmentation and object detection. C Xie, J Wang, Z Zhang, Y Zhou, L Xie, A Yuille, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionC. Xie, J. Wang, Z. Zhang, Y. Zhou, L. Xie, and A. Yuille, \"Adversarial examples for semantic segmentation and object detection,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1369-1378.\n\nCt-gan: Malicious tampering of 3d medical imagery using deep learning. Y Mirsky, T Mahler, I Shelef, Y Elovici, 28th USENIX Security Symposium (USENIX Security 19). Santa Clara, CAUSENIX AssociationY. Mirsky, T. Mahler, I. Shelef, and Y. Elovici, \"Ct-gan: Malicious tampering of 3d medical imagery using deep learning,\" in 28th USENIX Security Symposium (USENIX Security 19). Santa Clara, CA: USENIX Association, Aug. 2019, pp. 461-478. [Online]. Available: https://www.usenix.org/conference/usenixsecurity19/presentation/mirsky\n\nSegment-before-detect: Vehicle detection and classification through semantic segmentation of aerial images. N Audebert, B Le Saux, S Lef\u00e8vre, Remote Sensing. 94368N. Audebert, B. Le Saux, and S. Lef\u00e8vre, \"Segment-before-detect: Vehicle detection and classification through semantic segmentation of aerial images,\" Remote Sensing, vol. 9, no. 4, p. 368, 2017.\n\nA survey on deep learning techniques for image and video semantic segmentation. A Garcia-Garcia, S Orts-Escolano, S Oprea, V Villena-Martinez, P Martinez-Gonzalez, J Garcia-Rodriguez, Applied Soft Computing. 70A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, P. Martinez- Gonzalez, and J. Garcia-Rodriguez, \"A survey on deep learning techniques for image and video semantic segmentation,\" Applied Soft Computing, vol. 70, pp. 41-65, 2018.\n\nImage segmentation using deep learning: A survey. S Minaee, Y Boykov, F Porikli, A Plaza, N Kehtarnavaz, D Terzopoulos, arXiv:2001.05566arXiv preprintS. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos, \"Image segmentation using deep learning: A survey,\" arXiv preprint arXiv:2001.05566, 2020.\n\nLearning to predict crisp boundaries. R Deng, C Shen, S Liu, H Wang, X Liu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)R. Deng, C. Shen, S. Liu, H. Wang, and X. Liu, \"Learning to predict crisp boundaries,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 562-578.\n\nV-net: Fully convolutional neural networks for volumetric medical image segmentation. F Milletari, N Navab, S.-A Ahmadi, 2016 fourth international conference on 3D vision (3DV). IEEEF. Milletari, N. Navab, and S.-A. Ahmadi, \"V-net: Fully convolutional neural networks for volumetric medical image segmentation,\" in 2016 fourth international conference on 3D vision (3DV). IEEE, 2016, pp. 565-571.\n\nUnet++: A nested u-net architecture for medical image segmentation,\" in Deep learning in medical image analysis and multimodal learning for clinical decision support. Z Zhou, M M R Siddiquee, N Tajbakhsh, J Liang, SpringerZ. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, \"Unet++: A nested u-net architecture for medical image segmentation,\" in Deep learning in medical image analysis and multimodal learning for clinical decision support. Springer, 2018, pp. 3-11.\n\nLinknet: Exploiting encoder representations for efficient semantic segmentation. A Chaurasia, E Culurciello, 2017 IEEE Visual Communications and Image Processing (VCIP). IEEEA. Chaurasia and E. Culurciello, \"Linknet: Exploiting encoder representations for efficient semantic segmentation,\" in 2017 IEEE Visual Communications and Image Processing (VCIP). IEEE, 2017, pp. 1-4.\n\nFeature pyramid networks for object detection. T.-Y Lin, P Doll\u00e1r, R Girshick, K He, B Hariharan, S Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionT.-Y. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie, \"Feature pyramid networks for object detection,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2117-2125.\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionH. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \"Pyramid scene parsing network,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2881-2890.\n\nPyramid attention network for semantic segmentation. H Li, P Xiong, J An, L Wang, arXiv:1805.10180arXiv preprintH. Li, P. Xiong, J. An, and L. Wang, \"Pyramid attention network for semantic segmentation,\" arXiv preprint arXiv:1805.10180, 2018.\n\nSemantic object classes in video: A high-definition ground truth database. G J Brostow, J Fauqueur, R Cipolla, Pattern Recognition Letters. 302G. J. Brostow, J. Fauqueur, and R. Cipolla, \"Semantic object classes in video: A high-definition ground truth database,\" Pattern Recognition Letters, vol. 30, no. 2, pp. 88-97, 2009.\n\nBdd100k: A diverse driving video database with scalable annotation tooling. F Yu, W Xian, Y Chen, F Liu, M Liao, V Madhavan, T Darrell, arXiv:1805.0468726arXiv preprintF. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell, \"Bdd100k: A diverse driving video database with scalable annotation tooling,\" arXiv preprint arXiv:1805.04687, vol. 2, no. 5, p. 6, 2018.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248-255.\n\nDual path networks. Y Chen, J Li, H Xiao, X Jin, S Yan, J Feng, arXiv:1707.01629arXiv preprintY. Chen, J. Li, H. Xiao, X. Jin, S. Yan, and J. Feng, \"Dual path networks,\" arXiv preprint arXiv:1707.01629, 2017.\n\nD Wu, Y Wang, S.-T Xia, J Bailey, X Ma, arXiv:2002.05990Skip connections matter: On the transferability of adversarial examples generated with resnets. arXiv preprintD. Wu, Y. Wang, S.-T. Xia, J. Bailey, and X. Ma, \"Skip connections matter: On the transferability of adversarial examples generated with resnets,\" arXiv preprint arXiv:2002.05990, 2020.\n\nImproving transferability of adversarial examples with input diversity. C Xie, Z Zhang, Y Zhou, S Bai, J Wang, Z Ren, A L Yuille, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionC. Xie, Z. Zhang, Y. Zhou, S. Bai, J. Wang, Z. Ren, and A. L. Yuille, \"Improving transferability of adversarial examples with input diversity,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 2730-2739.\n\nEnhancing adversarial example transferability with an intermediate level attack. Q Huang, I Katsman, H He, Z Gu, S Belongie, S.-N Lim, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionQ. Huang, I. Katsman, H. He, Z. Gu, S. Belongie, and S.-N. Lim, \"Enhancing adversarial example transferability with an intermediate level attack,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 4733-4742.\n\nBdd100k: A diverse driving dataset for heterogeneous multitask learning. F Yu, H Chen, X Wang, W Xian, Y Chen, F Liu, V Madhavan, T Darrell, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionF. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell, \"Bdd100k: A diverse driving dataset for heterogeneous multitask learning,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 2636-2645.\n\nMa-net: A multi-scale attention network for liver and tumor segmentation. T Fan, G Wang, Y Li, H Wang, IEEE Access. 8T. Fan, G. Wang, Y. Li, and H. Wang, \"Ma-net: A multi-scale attention network for liver and tumor segmentation,\" IEEE Access, vol. 8, pp. 179 656-179 665, 2020.\n\nDynamic routing between capsules. S Sabour, N Frosst, G E Hinton, arXiv:1710.09829arXiv preprintS. Sabour, N. Frosst, and G. E. Hinton, \"Dynamic routing between capsules,\" arXiv preprint arXiv:1710.09829, 2017.\n\nHe received his Ph.D. from BGU in 2018 and was a postdoctoral fellow for two years in the at the Georgia Institute of Technology in the research labs of Prof. Wenke Lee. His main research interests include deepfakes, adversarial machine learning, anomaly detection, and intrusion detection. Ndss Euro, S&amp;p , Black Hat, Def Con, Rsa, Csf, Aisec, Dr. Mirsky has published his work in some of the best security venues: USENIX, CCS. Department of Software and Information Systems Engineering at Ben-Gurion UniversityThe Wall Street Journal, Forbes, and BBC. Some of his works. include the exposure of vulnerabilities in the US 911 emergency services and research into the threat of deepfakes in medical scans, both featured in The Washington PostYisroel Mirsky is a tenure-track lecturer in the Department of Software and Informa- tion Systems Engineering at Ben-Gurion University. He received his Ph.D. from BGU in 2018 and was a postdoctoral fellow for two years in the at the Georgia Institute of Technology in the research labs of Prof. Wenke Lee. His main research interests in- clude deepfakes, adversarial machine learning, anomaly detection, and intrusion detec- tion. Dr. Mirsky has published his work in some of the best security venues: USENIX, CCS, NDSS, Euro S&P, Black Hat, DEF CON, RSA, CSF, AISec, etc. His research has also been featured in many well-known media outlets: Popular Science, Scientific American, Wired, The Wall Street Journal, Forbes, and BBC. Some of his works, include the exposure of vulnerabilities in the US 911 emergency services and research into the threat of deepfakes in medical scans, both featured in The Washington Post.\n", "annotations": {"author": "[{\"end\":79,\"start\":53},{\"end\":86,\"start\":80},{\"end\":91,\"start\":87},{\"end\":95,\"start\":92},{\"end\":99,\"start\":96},{\"end\":109,\"start\":100},{\"end\":115,\"start\":110}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":53},{\"end\":85,\"start\":80},{\"end\":90,\"start\":87},{\"end\":94,\"start\":92},{\"end\":98,\"start\":96},{\"end\":108,\"start\":106},{\"end\":114,\"start\":110}]", "author_first_name": "[{\"end\":105,\"start\":100}]", "author_affiliation": null, "title": "[{\"end\":50,\"start\":1},{\"end\":165,\"start\":116}]", "venue": null, "abstract": "[{\"end\":1460,\"start\":259}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1923,\"start\":1920},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1928,\"start\":1925},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1946,\"start\":1943},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1979,\"start\":1976},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2296,\"start\":2293},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2476,\"start\":2473},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2481,\"start\":2478},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2888,\"start\":2885},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3038,\"start\":3035},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3043,\"start\":3040},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3049,\"start\":3045},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3055,\"start\":3051},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3061,\"start\":3057},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3067,\"start\":3063},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3073,\"start\":3069},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3079,\"start\":3075},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3085,\"start\":3081},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3091,\"start\":3087},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3097,\"start\":3093},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3103,\"start\":3099},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3943,\"start\":3939},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3971,\"start\":3967},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4033,\"start\":4029},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4038,\"start\":4035},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4065,\"start\":4061},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6640,\"start\":6637},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6646,\"start\":6642},{\"end\":8034,\"start\":8033},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9056,\"start\":9052},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9215,\"start\":9211},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9221,\"start\":9217},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9227,\"start\":9223},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9426,\"start\":9422},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9432,\"start\":9428},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9470,\"start\":9466},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9784,\"start\":9781},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9959,\"start\":9956},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10160,\"start\":10157},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10166,\"start\":10162},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10172,\"start\":10168},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10178,\"start\":10174},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10184,\"start\":10180},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10190,\"start\":10186},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10196,\"start\":10192},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10301,\"start\":10298},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10307,\"start\":10303},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10715,\"start\":10711},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10721,\"start\":10717},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10727,\"start\":10723},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10733,\"start\":10729},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10739,\"start\":10735},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11112,\"start\":11109},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11118,\"start\":11114},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13210,\"start\":13206},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13834,\"start\":13830},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14294,\"start\":14290},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14560,\"start\":14557},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14566,\"start\":14562},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14988,\"start\":14984},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14994,\"start\":14990},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17090,\"start\":17086},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":17142,\"start\":17138},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17938,\"start\":17934},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18189,\"start\":18185},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":18325,\"start\":18321},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18503,\"start\":18499},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":18724,\"start\":18720},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19606,\"start\":19603},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19611,\"start\":19608},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19616,\"start\":19613},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":20141,\"start\":20137},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21753,\"start\":21752},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":22320,\"start\":22316},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24308,\"start\":24304},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":24457,\"start\":24453},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25466,\"start\":25465},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25698,\"start\":25694},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26469,\"start\":26468},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26600,\"start\":26599},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28424,\"start\":28423},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":34357,\"start\":34353},{\"end\":34360,\"start\":34357},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36238,\"start\":36237},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36692,\"start\":36691},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":37798,\"start\":37794},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":39523,\"start\":39519},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":39915,\"start\":39911},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":39921,\"start\":39917},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":41451,\"start\":41447},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":42813,\"start\":42812},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":45185,\"start\":45181},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":45191,\"start\":45187},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":45197,\"start\":45193},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":45330,\"start\":45326}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47944,\"start\":47676},{\"attributes\":{\"id\":\"fig_1\"},\"end\":48291,\"start\":47945},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48396,\"start\":48292},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48551,\"start\":48397},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48716,\"start\":48552},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48924,\"start\":48717},{\"attributes\":{\"id\":\"fig_7\"},\"end\":49266,\"start\":48925},{\"attributes\":{\"id\":\"fig_8\"},\"end\":49661,\"start\":49267},{\"attributes\":{\"id\":\"fig_9\"},\"end\":49745,\"start\":49662},{\"attributes\":{\"id\":\"fig_11\"},\"end\":49936,\"start\":49746},{\"attributes\":{\"id\":\"fig_12\"},\"end\":50180,\"start\":49937},{\"attributes\":{\"id\":\"fig_13\"},\"end\":50298,\"start\":50181},{\"attributes\":{\"id\":\"fig_15\"},\"end\":50461,\"start\":50299},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":50854,\"start\":50462},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50955,\"start\":50855}]", "paragraph": "[{\"end\":1980,\"start\":1479},{\"end\":2579,\"start\":1982},{\"end\":3301,\"start\":2581},{\"end\":3408,\"start\":3303},{\"end\":4899,\"start\":3421},{\"end\":5608,\"start\":4901},{\"end\":6257,\"start\":5610},{\"end\":7289,\"start\":6259},{\"end\":7338,\"start\":7291},{\"end\":8805,\"start\":7340},{\"end\":8903,\"start\":8807},{\"end\":9924,\"start\":8925},{\"end\":10308,\"start\":9926},{\"end\":11502,\"start\":10310},{\"end\":12289,\"start\":11524},{\"end\":13026,\"start\":12291},{\"end\":14719,\"start\":13028},{\"end\":14884,\"start\":14744},{\"end\":15058,\"start\":14912},{\"end\":15395,\"start\":15075},{\"end\":16158,\"start\":15397},{\"end\":16548,\"start\":16160},{\"end\":17294,\"start\":16550},{\"end\":17385,\"start\":17316},{\"end\":18843,\"start\":17387},{\"end\":19503,\"start\":18859},{\"end\":19751,\"start\":19505},{\"end\":22153,\"start\":19794},{\"end\":23051,\"start\":22155},{\"end\":23149,\"start\":23053},{\"end\":24040,\"start\":23186},{\"end\":27729,\"start\":24058},{\"end\":28963,\"start\":27774},{\"end\":30002,\"start\":28965},{\"end\":30213,\"start\":30004},{\"end\":30632,\"start\":30215},{\"end\":31300,\"start\":30634},{\"end\":31883,\"start\":31302},{\"end\":32225,\"start\":31885},{\"end\":32925,\"start\":32227},{\"end\":33955,\"start\":32927},{\"end\":36239,\"start\":34024},{\"end\":36374,\"start\":36283},{\"end\":37150,\"start\":36376},{\"end\":38077,\"start\":37152},{\"end\":39804,\"start\":38079},{\"end\":39953,\"start\":39806},{\"end\":40304,\"start\":39993},{\"end\":41098,\"start\":40332},{\"end\":41514,\"start\":41116},{\"end\":42894,\"start\":41516},{\"end\":43006,\"start\":42896},{\"end\":43106,\"start\":43008},{\"end\":43220,\"start\":43108},{\"end\":45006,\"start\":43258},{\"end\":46289,\"start\":45008},{\"end\":46678,\"start\":46291},{\"end\":47285,\"start\":46699},{\"end\":47675,\"start\":47287}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15074,\"start\":15059},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17315,\"start\":17295},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19793,\"start\":19752}]", "table_ref": "[{\"end\":10320,\"start\":10313},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28221,\"start\":28213}]", "section_header": "[{\"end\":1477,\"start\":1462},{\"end\":3419,\"start\":3411},{\"end\":8923,\"start\":8906},{\"end\":11522,\"start\":11505},{\"end\":14742,\"start\":14722},{\"end\":14910,\"start\":14887},{\"end\":18857,\"start\":18846},{\"end\":23184,\"start\":23152},{\"end\":24056,\"start\":24043},{\"end\":27772,\"start\":27732},{\"end\":33994,\"start\":33958},{\"end\":34022,\"start\":33997},{\"end\":36281,\"start\":36242},{\"end\":39991,\"start\":39956},{\"end\":40330,\"start\":40307},{\"end\":41114,\"start\":41101},{\"end\":43256,\"start\":43223},{\"end\":46697,\"start\":46681},{\"end\":47685,\"start\":47677},{\"end\":47954,\"start\":47946},{\"end\":48301,\"start\":48293},{\"end\":48406,\"start\":48398},{\"end\":48561,\"start\":48553},{\"end\":48726,\"start\":48718},{\"end\":48951,\"start\":48926},{\"end\":49277,\"start\":49268},{\"end\":49672,\"start\":49663},{\"end\":49756,\"start\":49747},{\"end\":49956,\"start\":49938},{\"end\":50191,\"start\":50182},{\"end\":50868,\"start\":50856}]", "table": "[{\"end\":50854,\"start\":50467}]", "figure_caption": "[{\"end\":47944,\"start\":47687},{\"end\":48291,\"start\":47956},{\"end\":48396,\"start\":48303},{\"end\":48551,\"start\":48408},{\"end\":48716,\"start\":48563},{\"end\":48924,\"start\":48728},{\"end\":49266,\"start\":48956},{\"end\":49661,\"start\":49280},{\"end\":49745,\"start\":49675},{\"end\":49936,\"start\":49759},{\"end\":50180,\"start\":49961},{\"end\":50298,\"start\":50194},{\"end\":50461,\"start\":50301},{\"end\":50467,\"start\":50464},{\"end\":50955,\"start\":50871}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4485,\"start\":4479},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5086,\"start\":5080},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13168,\"start\":13162},{\"end\":15096,\"start\":15090},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19483,\"start\":19476},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20355,\"start\":20349},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22852,\"start\":22846},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23147,\"start\":23141},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29101,\"start\":29095},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30027,\"start\":30021},{\"end\":30252,\"start\":30244},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30631,\"start\":30624},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31345,\"start\":31338},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32411,\"start\":32404},{\"end\":33343,\"start\":33337},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34134,\"start\":34127},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34809,\"start\":34802},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35017,\"start\":35010},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35392,\"start\":35383},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35474,\"start\":35467},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35722,\"start\":35708},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36950,\"start\":36942},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37162,\"start\":37155},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38443,\"start\":38436},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38991,\"start\":38984},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40398,\"start\":40388},{\"end\":40467,\"start\":40460},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42205,\"start\":42198},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42935,\"start\":42928},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44400,\"start\":44393},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44603,\"start\":44597}]", "bib_author_first_name": "[{\"end\":51207,\"start\":51206},{\"end\":51735,\"start\":51734},{\"end\":52242,\"start\":52241},{\"end\":52252,\"start\":52251},{\"end\":52477,\"start\":52476},{\"end\":52484,\"start\":52483},{\"end\":52490,\"start\":52489},{\"end\":52502,\"start\":52501},{\"end\":52720,\"start\":52719},{\"end\":52722,\"start\":52721},{\"end\":52731,\"start\":52730},{\"end\":52739,\"start\":52738},{\"end\":52746,\"start\":52745},{\"end\":52755,\"start\":52754},{\"end\":52963,\"start\":52962},{\"end\":52971,\"start\":52970},{\"end\":52982,\"start\":52981},{\"end\":52993,\"start\":52992},{\"end\":53006,\"start\":53005},{\"end\":53012,\"start\":53011},{\"end\":53023,\"start\":53022},{\"end\":53033,\"start\":53032},{\"end\":53044,\"start\":53043},{\"end\":53401,\"start\":53400},{\"end\":53408,\"start\":53407},{\"end\":53416,\"start\":53415},{\"end\":53423,\"start\":53422},{\"end\":53431,\"start\":53430},{\"end\":53437,\"start\":53436},{\"end\":53713,\"start\":53709},{\"end\":53721,\"start\":53720},{\"end\":53734,\"start\":53733},{\"end\":53744,\"start\":53743},{\"end\":53748,\"start\":53745},{\"end\":54154,\"start\":54153},{\"end\":54167,\"start\":54166},{\"end\":54169,\"start\":54168},{\"end\":54180,\"start\":54179},{\"end\":54191,\"start\":54190},{\"end\":54201,\"start\":54200},{\"end\":54455,\"start\":54454},{\"end\":54462,\"start\":54461},{\"end\":54708,\"start\":54707},{\"end\":54716,\"start\":54715},{\"end\":54729,\"start\":54728},{\"end\":55250,\"start\":55249},{\"end\":55258,\"start\":55257},{\"end\":55265,\"start\":55264},{\"end\":55274,\"start\":55273},{\"end\":55282,\"start\":55281},{\"end\":55291,\"start\":55290},{\"end\":55809,\"start\":55808},{\"end\":55815,\"start\":55814},{\"end\":55821,\"start\":55820},{\"end\":55829,\"start\":55828},{\"end\":55835,\"start\":55834},{\"end\":55837,\"start\":55836},{\"end\":56101,\"start\":56100},{\"end\":56114,\"start\":56113},{\"end\":56126,\"start\":56125},{\"end\":56136,\"start\":56135},{\"end\":56150,\"start\":56149},{\"end\":56157,\"start\":56156},{\"end\":56169,\"start\":56168},{\"end\":56181,\"start\":56180},{\"end\":56190,\"start\":56189},{\"end\":56200,\"start\":56199},{\"end\":56693,\"start\":56692},{\"end\":56702,\"start\":56701},{\"end\":56709,\"start\":56708},{\"end\":56717,\"start\":56716},{\"end\":56724,\"start\":56723},{\"end\":56726,\"start\":56725},{\"end\":56736,\"start\":56735},{\"end\":56743,\"start\":56742},{\"end\":57198,\"start\":57197},{\"end\":57207,\"start\":57206},{\"end\":57218,\"start\":57217},{\"end\":57229,\"start\":57228},{\"end\":57507,\"start\":57506},{\"end\":57516,\"start\":57512},{\"end\":57523,\"start\":57522},{\"end\":57525,\"start\":57524},{\"end\":57534,\"start\":57533},{\"end\":57867,\"start\":57866},{\"end\":57880,\"start\":57879},{\"end\":57889,\"start\":57888},{\"end\":57901,\"start\":57900},{\"end\":57912,\"start\":57911},{\"end\":58249,\"start\":58248},{\"end\":58255,\"start\":58254},{\"end\":58261,\"start\":58260},{\"end\":58269,\"start\":58268},{\"end\":58680,\"start\":58679},{\"end\":58693,\"start\":58692},{\"end\":58707,\"start\":58706},{\"end\":59004,\"start\":59003},{\"end\":59014,\"start\":59013},{\"end\":59016,\"start\":59015},{\"end\":59360,\"start\":59359},{\"end\":59368,\"start\":59367},{\"end\":59377,\"start\":59376},{\"end\":59392,\"start\":59391},{\"end\":59404,\"start\":59403},{\"end\":59417,\"start\":59416},{\"end\":59929,\"start\":59928},{\"end\":59939,\"start\":59938},{\"end\":60160,\"start\":60159},{\"end\":60171,\"start\":60170},{\"end\":60182,\"start\":60181},{\"end\":60195,\"start\":60194},{\"end\":60204,\"start\":60203},{\"end\":60213,\"start\":60212},{\"end\":60227,\"start\":60226},{\"end\":60485,\"start\":60484},{\"end\":60487,\"start\":60486},{\"end\":60501,\"start\":60500},{\"end\":60511,\"start\":60510},{\"end\":60780,\"start\":60779},{\"end\":60790,\"start\":60789},{\"end\":60802,\"start\":60801},{\"end\":61248,\"start\":61247},{\"end\":61255,\"start\":61254},{\"end\":61263,\"start\":61262},{\"end\":61271,\"start\":61270},{\"end\":61281,\"start\":61280},{\"end\":61560,\"start\":61559},{\"end\":61566,\"start\":61565},{\"end\":61575,\"start\":61574},{\"end\":61584,\"start\":61583},{\"end\":61840,\"start\":61839},{\"end\":61851,\"start\":61850},{\"end\":61863,\"start\":61862},{\"end\":61872,\"start\":61871},{\"end\":62170,\"start\":62169},{\"end\":62188,\"start\":62187},{\"end\":62197,\"start\":62196},{\"end\":62205,\"start\":62204},{\"end\":62625,\"start\":62624},{\"end\":62636,\"start\":62635},{\"end\":62638,\"start\":62637},{\"end\":62647,\"start\":62646},{\"end\":62649,\"start\":62648},{\"end\":62659,\"start\":62658},{\"end\":62919,\"start\":62918},{\"end\":62928,\"start\":62927},{\"end\":62938,\"start\":62937},{\"end\":62940,\"start\":62939},{\"end\":63394,\"start\":63393},{\"end\":63405,\"start\":63404},{\"end\":63419,\"start\":63418},{\"end\":63857,\"start\":63856},{\"end\":63865,\"start\":63864},{\"end\":63873,\"start\":63872},{\"end\":63879,\"start\":63878},{\"end\":64140,\"start\":64139},{\"end\":64147,\"start\":64146},{\"end\":64156,\"start\":64155},{\"end\":64164,\"start\":64163},{\"end\":64418,\"start\":64417},{\"end\":64426,\"start\":64425},{\"end\":64433,\"start\":64432},{\"end\":64688,\"start\":64684},{\"end\":64696,\"start\":64695},{\"end\":64703,\"start\":64702},{\"end\":64712,\"start\":64711},{\"end\":64719,\"start\":64718},{\"end\":64721,\"start\":64720},{\"end\":64731,\"start\":64730},{\"end\":64740,\"start\":64739},{\"end\":64747,\"start\":64746},{\"end\":65239,\"start\":65238},{\"end\":65248,\"start\":65247},{\"end\":65256,\"start\":65255},{\"end\":65584,\"start\":65583},{\"end\":65593,\"start\":65592},{\"end\":65605,\"start\":65604},{\"end\":65616,\"start\":65615},{\"end\":65890,\"start\":65889},{\"end\":65896,\"start\":65895},{\"end\":65902,\"start\":65901},{\"end\":66251,\"start\":66250},{\"end\":66258,\"start\":66257},{\"end\":66266,\"start\":66265},{\"end\":66275,\"start\":66274},{\"end\":66283,\"start\":66282},{\"end\":66290,\"start\":66289},{\"end\":66716,\"start\":66715},{\"end\":66726,\"start\":66725},{\"end\":66736,\"start\":66735},{\"end\":66746,\"start\":66745},{\"end\":67283,\"start\":67282},{\"end\":67295,\"start\":67294},{\"end\":67298,\"start\":67296},{\"end\":67306,\"start\":67305},{\"end\":67615,\"start\":67614},{\"end\":67632,\"start\":67631},{\"end\":67649,\"start\":67648},{\"end\":67658,\"start\":67657},{\"end\":67678,\"start\":67677},{\"end\":67699,\"start\":67698},{\"end\":68043,\"start\":68042},{\"end\":68053,\"start\":68052},{\"end\":68063,\"start\":68062},{\"end\":68074,\"start\":68073},{\"end\":68083,\"start\":68082},{\"end\":68098,\"start\":68097},{\"end\":68353,\"start\":68352},{\"end\":68361,\"start\":68360},{\"end\":68369,\"start\":68368},{\"end\":68376,\"start\":68375},{\"end\":68384,\"start\":68383},{\"end\":68768,\"start\":68767},{\"end\":68781,\"start\":68780},{\"end\":68793,\"start\":68789},{\"end\":69247,\"start\":69246},{\"end\":69255,\"start\":69254},{\"end\":69259,\"start\":69256},{\"end\":69272,\"start\":69271},{\"end\":69285,\"start\":69284},{\"end\":69635,\"start\":69634},{\"end\":69648,\"start\":69647},{\"end\":69980,\"start\":69976},{\"end\":69987,\"start\":69986},{\"end\":69997,\"start\":69996},{\"end\":70009,\"start\":70008},{\"end\":70015,\"start\":70014},{\"end\":70028,\"start\":70027},{\"end\":70438,\"start\":70437},{\"end\":70446,\"start\":70445},{\"end\":70453,\"start\":70452},{\"end\":70459,\"start\":70458},{\"end\":70467,\"start\":70466},{\"end\":70850,\"start\":70849},{\"end\":70856,\"start\":70855},{\"end\":70865,\"start\":70864},{\"end\":70871,\"start\":70870},{\"end\":71116,\"start\":71115},{\"end\":71118,\"start\":71117},{\"end\":71129,\"start\":71128},{\"end\":71141,\"start\":71140},{\"end\":71444,\"start\":71443},{\"end\":71450,\"start\":71449},{\"end\":71458,\"start\":71457},{\"end\":71466,\"start\":71465},{\"end\":71473,\"start\":71472},{\"end\":71481,\"start\":71480},{\"end\":71493,\"start\":71492},{\"end\":71799,\"start\":71798},{\"end\":71807,\"start\":71806},{\"end\":71815,\"start\":71814},{\"end\":71828,\"start\":71824},{\"end\":71834,\"start\":71833},{\"end\":71840,\"start\":71839},{\"end\":72151,\"start\":72150},{\"end\":72159,\"start\":72158},{\"end\":72165,\"start\":72164},{\"end\":72173,\"start\":72172},{\"end\":72180,\"start\":72179},{\"end\":72187,\"start\":72186},{\"end\":72341,\"start\":72340},{\"end\":72347,\"start\":72346},{\"end\":72358,\"start\":72354},{\"end\":72365,\"start\":72364},{\"end\":72375,\"start\":72374},{\"end\":72766,\"start\":72765},{\"end\":72773,\"start\":72772},{\"end\":72782,\"start\":72781},{\"end\":72790,\"start\":72789},{\"end\":72797,\"start\":72796},{\"end\":72805,\"start\":72804},{\"end\":72812,\"start\":72811},{\"end\":72814,\"start\":72813},{\"end\":73306,\"start\":73305},{\"end\":73315,\"start\":73314},{\"end\":73326,\"start\":73325},{\"end\":73332,\"start\":73331},{\"end\":73338,\"start\":73337},{\"end\":73353,\"start\":73349},{\"end\":73807,\"start\":73806},{\"end\":73813,\"start\":73812},{\"end\":73821,\"start\":73820},{\"end\":73829,\"start\":73828},{\"end\":73837,\"start\":73836},{\"end\":73845,\"start\":73844},{\"end\":73852,\"start\":73851},{\"end\":73864,\"start\":73863},{\"end\":74361,\"start\":74360},{\"end\":74368,\"start\":74367},{\"end\":74376,\"start\":74375},{\"end\":74382,\"start\":74381},{\"end\":74600,\"start\":74599},{\"end\":74610,\"start\":74609},{\"end\":74620,\"start\":74619},{\"end\":74622,\"start\":74621},{\"end\":75072,\"start\":75068},{\"end\":75086,\"start\":75079},{\"end\":75094,\"start\":75089},{\"end\":75103,\"start\":75100}]", "bib_author_last_name": "[{\"end\":51215,\"start\":51208},{\"end\":51743,\"start\":51736},{\"end\":52009,\"start\":52002},{\"end\":52249,\"start\":52243},{\"end\":52260,\"start\":52253},{\"end\":52481,\"start\":52478},{\"end\":52487,\"start\":52485},{\"end\":52499,\"start\":52491},{\"end\":52506,\"start\":52503},{\"end\":52728,\"start\":52723},{\"end\":52736,\"start\":52732},{\"end\":52743,\"start\":52740},{\"end\":52752,\"start\":52747},{\"end\":52762,\"start\":52756},{\"end\":52968,\"start\":52964},{\"end\":52979,\"start\":52972},{\"end\":52990,\"start\":52983},{\"end\":53003,\"start\":52994},{\"end\":53009,\"start\":53007},{\"end\":53020,\"start\":53013},{\"end\":53030,\"start\":53024},{\"end\":53041,\"start\":53034},{\"end\":53050,\"start\":53045},{\"end\":53405,\"start\":53402},{\"end\":53413,\"start\":53409},{\"end\":53420,\"start\":53417},{\"end\":53428,\"start\":53424},{\"end\":53434,\"start\":53432},{\"end\":53442,\"start\":53438},{\"end\":53718,\"start\":53714},{\"end\":53731,\"start\":53722},{\"end\":53741,\"start\":53735},{\"end\":53753,\"start\":53749},{\"end\":54164,\"start\":54155},{\"end\":54177,\"start\":54170},{\"end\":54188,\"start\":54181},{\"end\":54198,\"start\":54192},{\"end\":54208,\"start\":54202},{\"end\":54459,\"start\":54456},{\"end\":54469,\"start\":54463},{\"end\":54713,\"start\":54709},{\"end\":54726,\"start\":54717},{\"end\":54737,\"start\":54730},{\"end\":55255,\"start\":55251},{\"end\":55262,\"start\":55259},{\"end\":55271,\"start\":55266},{\"end\":55279,\"start\":55275},{\"end\":55288,\"start\":55283},{\"end\":55296,\"start\":55292},{\"end\":55812,\"start\":55810},{\"end\":55818,\"start\":55816},{\"end\":55826,\"start\":55822},{\"end\":55832,\"start\":55830},{\"end\":55842,\"start\":55838},{\"end\":56111,\"start\":56102},{\"end\":56123,\"start\":56115},{\"end\":56133,\"start\":56127},{\"end\":56147,\"start\":56137},{\"end\":56154,\"start\":56151},{\"end\":56166,\"start\":56158},{\"end\":56178,\"start\":56170},{\"end\":56187,\"start\":56182},{\"end\":56197,\"start\":56191},{\"end\":56206,\"start\":56201},{\"end\":56699,\"start\":56694},{\"end\":56706,\"start\":56703},{\"end\":56714,\"start\":56710},{\"end\":56721,\"start\":56718},{\"end\":56733,\"start\":56727},{\"end\":56740,\"start\":56737},{\"end\":56747,\"start\":56744},{\"end\":57204,\"start\":57199},{\"end\":57215,\"start\":57208},{\"end\":57226,\"start\":57219},{\"end\":57237,\"start\":57230},{\"end\":57510,\"start\":57508},{\"end\":57520,\"start\":57517},{\"end\":57531,\"start\":57526},{\"end\":57544,\"start\":57535},{\"end\":57877,\"start\":57868},{\"end\":57886,\"start\":57881},{\"end\":57898,\"start\":57890},{\"end\":57909,\"start\":57902},{\"end\":57920,\"start\":57913},{\"end\":58252,\"start\":58250},{\"end\":58258,\"start\":58256},{\"end\":58266,\"start\":58262},{\"end\":58272,\"start\":58270},{\"end\":58690,\"start\":58681},{\"end\":58704,\"start\":58694},{\"end\":58712,\"start\":58708},{\"end\":59011,\"start\":59005},{\"end\":59021,\"start\":59017},{\"end\":59365,\"start\":59361},{\"end\":59374,\"start\":59369},{\"end\":59389,\"start\":59378},{\"end\":59401,\"start\":59393},{\"end\":59414,\"start\":59405},{\"end\":59423,\"start\":59418},{\"end\":59936,\"start\":59930},{\"end\":59944,\"start\":59940},{\"end\":60168,\"start\":60161},{\"end\":60179,\"start\":60172},{\"end\":60192,\"start\":60183},{\"end\":60201,\"start\":60196},{\"end\":60210,\"start\":60205},{\"end\":60224,\"start\":60214},{\"end\":60234,\"start\":60228},{\"end\":60498,\"start\":60488},{\"end\":60508,\"start\":60502},{\"end\":60519,\"start\":60512},{\"end\":60787,\"start\":60781},{\"end\":60799,\"start\":60791},{\"end\":60808,\"start\":60803},{\"end\":61252,\"start\":61249},{\"end\":61260,\"start\":61256},{\"end\":61268,\"start\":61264},{\"end\":61278,\"start\":61272},{\"end\":61286,\"start\":61282},{\"end\":61563,\"start\":61561},{\"end\":61572,\"start\":61567},{\"end\":61581,\"start\":61576},{\"end\":61592,\"start\":61585},{\"end\":61848,\"start\":61841},{\"end\":61860,\"start\":61852},{\"end\":61869,\"start\":61864},{\"end\":61877,\"start\":61873},{\"end\":62185,\"start\":62171},{\"end\":62194,\"start\":62189},{\"end\":62202,\"start\":62198},{\"end\":62213,\"start\":62206},{\"end\":62633,\"start\":62626},{\"end\":62644,\"start\":62639},{\"end\":62656,\"start\":62650},{\"end\":62664,\"start\":62660},{\"end\":62925,\"start\":62920},{\"end\":62935,\"start\":62929},{\"end\":62945,\"start\":62941},{\"end\":63402,\"start\":63395},{\"end\":63416,\"start\":63406},{\"end\":63427,\"start\":63420},{\"end\":63862,\"start\":63858},{\"end\":63870,\"start\":63866},{\"end\":63876,\"start\":63874},{\"end\":63887,\"start\":63880},{\"end\":64144,\"start\":64141},{\"end\":64153,\"start\":64148},{\"end\":64161,\"start\":64157},{\"end\":64168,\"start\":64165},{\"end\":64423,\"start\":64419},{\"end\":64430,\"start\":64427},{\"end\":64437,\"start\":64434},{\"end\":64693,\"start\":64689},{\"end\":64700,\"start\":64697},{\"end\":64709,\"start\":64704},{\"end\":64716,\"start\":64713},{\"end\":64728,\"start\":64722},{\"end\":64737,\"start\":64732},{\"end\":64744,\"start\":64741},{\"end\":64750,\"start\":64748},{\"end\":65245,\"start\":65240},{\"end\":65253,\"start\":65249},{\"end\":65259,\"start\":65257},{\"end\":65590,\"start\":65585},{\"end\":65602,\"start\":65594},{\"end\":65613,\"start\":65606},{\"end\":65624,\"start\":65617},{\"end\":65893,\"start\":65891},{\"end\":65899,\"start\":65897},{\"end\":65905,\"start\":65903},{\"end\":66255,\"start\":66252},{\"end\":66263,\"start\":66259},{\"end\":66272,\"start\":66267},{\"end\":66280,\"start\":66276},{\"end\":66287,\"start\":66284},{\"end\":66297,\"start\":66291},{\"end\":66723,\"start\":66717},{\"end\":66733,\"start\":66727},{\"end\":66743,\"start\":66737},{\"end\":66754,\"start\":66747},{\"end\":67292,\"start\":67284},{\"end\":67303,\"start\":67299},{\"end\":67314,\"start\":67307},{\"end\":67629,\"start\":67616},{\"end\":67646,\"start\":67633},{\"end\":67655,\"start\":67650},{\"end\":67675,\"start\":67659},{\"end\":67696,\"start\":67679},{\"end\":67716,\"start\":67700},{\"end\":68050,\"start\":68044},{\"end\":68060,\"start\":68054},{\"end\":68071,\"start\":68064},{\"end\":68080,\"start\":68075},{\"end\":68095,\"start\":68084},{\"end\":68110,\"start\":68099},{\"end\":68358,\"start\":68354},{\"end\":68366,\"start\":68362},{\"end\":68373,\"start\":68370},{\"end\":68381,\"start\":68377},{\"end\":68388,\"start\":68385},{\"end\":68778,\"start\":68769},{\"end\":68787,\"start\":68782},{\"end\":68800,\"start\":68794},{\"end\":69252,\"start\":69248},{\"end\":69269,\"start\":69260},{\"end\":69282,\"start\":69273},{\"end\":69291,\"start\":69286},{\"end\":69645,\"start\":69636},{\"end\":69660,\"start\":69649},{\"end\":69984,\"start\":69981},{\"end\":69994,\"start\":69988},{\"end\":70006,\"start\":69998},{\"end\":70012,\"start\":70010},{\"end\":70025,\"start\":70016},{\"end\":70037,\"start\":70029},{\"end\":70443,\"start\":70439},{\"end\":70450,\"start\":70447},{\"end\":70456,\"start\":70454},{\"end\":70464,\"start\":70460},{\"end\":70471,\"start\":70468},{\"end\":70853,\"start\":70851},{\"end\":70862,\"start\":70857},{\"end\":70868,\"start\":70866},{\"end\":70876,\"start\":70872},{\"end\":71126,\"start\":71119},{\"end\":71138,\"start\":71130},{\"end\":71149,\"start\":71142},{\"end\":71447,\"start\":71445},{\"end\":71455,\"start\":71451},{\"end\":71463,\"start\":71459},{\"end\":71470,\"start\":71467},{\"end\":71478,\"start\":71474},{\"end\":71490,\"start\":71482},{\"end\":71501,\"start\":71494},{\"end\":71804,\"start\":71800},{\"end\":71812,\"start\":71808},{\"end\":71822,\"start\":71816},{\"end\":71831,\"start\":71829},{\"end\":71837,\"start\":71835},{\"end\":71848,\"start\":71841},{\"end\":72156,\"start\":72152},{\"end\":72162,\"start\":72160},{\"end\":72170,\"start\":72166},{\"end\":72177,\"start\":72174},{\"end\":72184,\"start\":72181},{\"end\":72192,\"start\":72188},{\"end\":72344,\"start\":72342},{\"end\":72352,\"start\":72348},{\"end\":72362,\"start\":72359},{\"end\":72372,\"start\":72366},{\"end\":72378,\"start\":72376},{\"end\":72770,\"start\":72767},{\"end\":72779,\"start\":72774},{\"end\":72787,\"start\":72783},{\"end\":72794,\"start\":72791},{\"end\":72802,\"start\":72798},{\"end\":72809,\"start\":72806},{\"end\":72821,\"start\":72815},{\"end\":73312,\"start\":73307},{\"end\":73323,\"start\":73316},{\"end\":73329,\"start\":73327},{\"end\":73335,\"start\":73333},{\"end\":73347,\"start\":73339},{\"end\":73357,\"start\":73354},{\"end\":73810,\"start\":73808},{\"end\":73818,\"start\":73814},{\"end\":73826,\"start\":73822},{\"end\":73834,\"start\":73830},{\"end\":73842,\"start\":73838},{\"end\":73849,\"start\":73846},{\"end\":73861,\"start\":73853},{\"end\":73872,\"start\":73865},{\"end\":74365,\"start\":74362},{\"end\":74373,\"start\":74369},{\"end\":74379,\"start\":74377},{\"end\":74387,\"start\":74383},{\"end\":74607,\"start\":74601},{\"end\":74617,\"start\":74611},{\"end\":74629,\"start\":74623},{\"end\":75077,\"start\":75073},{\"end\":75098,\"start\":75095},{\"end\":75107,\"start\":75104},{\"end\":75112,\"start\":75109},{\"end\":75117,\"start\":75114},{\"end\":75124,\"start\":75119}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":51528,\"start\":51121},{\"attributes\":{\"id\":\"b1\"},\"end\":51660,\"start\":51530},{\"attributes\":{\"id\":\"b2\"},\"end\":51986,\"start\":51662},{\"attributes\":{\"id\":\"b3\"},\"end\":52203,\"start\":51988},{\"attributes\":{\"doi\":\"arXiv:1804.02767\",\"id\":\"b4\"},\"end\":52394,\"start\":52205},{\"attributes\":{\"doi\":\"arXiv:1506.01497\",\"id\":\"b5\"},\"end\":52698,\"start\":52396},{\"attributes\":{\"doi\":\"arXiv:1712.09665\",\"id\":\"b6\"},\"end\":52908,\"start\":52700},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49904930},\"end\":53341,\"start\":52910},{\"attributes\":{\"doi\":\"arXiv:1806.02299\",\"id\":\"b8\"},\"end\":53625,\"start\":53343},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4891043},\"end\":54100,\"start\":53627},{\"attributes\":{\"doi\":\"arXiv:1802.06430\",\"id\":\"b10\"},\"end\":54398,\"start\":54102},{\"attributes\":{\"doi\":\"arXiv:1906.11897\",\"id\":\"b11\"},\"end\":54617,\"start\":54400},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":121124946},\"end\":55147,\"start\":54619},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":207947087},\"end\":55726,\"start\":55149},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":226453930},\"end\":56043,\"start\":55728},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":221377254},\"end\":56631,\"start\":56045},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":219099302},\"end\":57132,\"start\":56633},{\"attributes\":{\"doi\":\"arXiv:2010.13070\",\"id\":\"b17\"},\"end\":57422,\"start\":57134},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":207757900},\"end\":57795,\"start\":57424},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5064446},\"end\":58185,\"start\":57797},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":229703676},\"end\":58578,\"start\":58187},{\"attributes\":{\"doi\":\"arXiv:1706.06169\",\"id\":\"b21\"},\"end\":58925,\"start\":58580},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":226323724},\"end\":59278,\"start\":58927},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":53533016},\"end\":59853,\"start\":59280},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53107276},\"end\":60115,\"start\":59855},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b25\"},\"end\":60434,\"start\":60117},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b26\"},\"end\":60682,\"start\":60436},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206592585},\"end\":61186,\"start\":60684},{\"attributes\":{\"doi\":\"arXiv:1511.06292\",\"id\":\"b28\"},\"end\":61469,\"start\":61188},{\"attributes\":{\"doi\":\"arXiv:1707.03501\",\"id\":\"b29\"},\"end\":61795,\"start\":61471},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2645819},\"end\":62094,\"start\":61797},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":5737015},\"end\":62568,\"start\":62096},{\"attributes\":{\"doi\":\"arXiv:1703.01101\",\"id\":\"b32\"},\"end\":62842,\"start\":62570},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4670132},\"end\":63301,\"start\":62844},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":199001136},\"end\":63779,\"start\":63303},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":211244412},\"end\":64066,\"start\":63781},{\"attributes\":{\"doi\":\"arXiv:1811.12641\",\"id\":\"b36\"},\"end\":64350,\"start\":64068},{\"attributes\":{\"doi\":\"arXiv:2010.14974\",\"id\":\"b37\"},\"end\":64603,\"start\":64352},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":231684241},\"end\":65183,\"start\":64605},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":221120281},\"end\":65505,\"start\":65185},{\"attributes\":{\"doi\":\"arXiv:2012.12528\",\"id\":\"b40\"},\"end\":65823,\"start\":65507},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":226852126},\"end\":66179,\"start\":65825},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":3350285},\"end\":66642,\"start\":66181},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":57825725},\"end\":67172,\"start\":66644},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":33264528},\"end\":67532,\"start\":67174},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":52815664},\"end\":67990,\"start\":67534},{\"attributes\":{\"doi\":\"arXiv:2001.05566\",\"id\":\"b46\"},\"end\":68312,\"start\":67992},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":50783395},\"end\":68679,\"start\":68314},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206429151},\"end\":69077,\"start\":68681},{\"attributes\":{\"id\":\"b49\"},\"end\":69551,\"start\":69079},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":3666466},\"end\":69927,\"start\":69553},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":10716717},\"end\":70404,\"start\":69929},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":5299559},\"end\":70794,\"start\":70406},{\"attributes\":{\"doi\":\"arXiv:1805.10180\",\"id\":\"b53\"},\"end\":71038,\"start\":70796},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":10759568},\"end\":71365,\"start\":71040},{\"attributes\":{\"doi\":\"arXiv:1805.04687\",\"id\":\"b55\"},\"end\":71743,\"start\":71367},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":57246310},\"end\":72128,\"start\":71745},{\"attributes\":{\"doi\":\"arXiv:1707.01629\",\"id\":\"b57\"},\"end\":72338,\"start\":72130},{\"attributes\":{\"doi\":\"arXiv:2002.05990\",\"id\":\"b58\"},\"end\":72691,\"start\":72340},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":3972825},\"end\":73222,\"start\":72693},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":198893678},\"end\":73731,\"start\":73224},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":215415900},\"end\":74284,\"start\":73733},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":222223726},\"end\":74563,\"start\":74286},{\"attributes\":{\"doi\":\"arXiv:1710.09829\",\"id\":\"b63\"},\"end\":74775,\"start\":74565},{\"attributes\":{\"id\":\"b64\"},\"end\":76442,\"start\":74777}]", "bib_title": "[{\"end\":52960,\"start\":52910},{\"end\":53707,\"start\":53627},{\"end\":54705,\"start\":54619},{\"end\":55247,\"start\":55149},{\"end\":55806,\"start\":55728},{\"end\":56098,\"start\":56045},{\"end\":56690,\"start\":56633},{\"end\":57504,\"start\":57424},{\"end\":57864,\"start\":57797},{\"end\":58246,\"start\":58187},{\"end\":59001,\"start\":58927},{\"end\":59357,\"start\":59280},{\"end\":59926,\"start\":59855},{\"end\":60777,\"start\":60684},{\"end\":61837,\"start\":61797},{\"end\":62167,\"start\":62096},{\"end\":62916,\"start\":62844},{\"end\":63391,\"start\":63303},{\"end\":63854,\"start\":63781},{\"end\":64682,\"start\":64605},{\"end\":65236,\"start\":65185},{\"end\":65887,\"start\":65825},{\"end\":66248,\"start\":66181},{\"end\":66713,\"start\":66644},{\"end\":67280,\"start\":67174},{\"end\":67612,\"start\":67534},{\"end\":68350,\"start\":68314},{\"end\":68765,\"start\":68681},{\"end\":69632,\"start\":69553},{\"end\":69974,\"start\":69929},{\"end\":70435,\"start\":70406},{\"end\":71113,\"start\":71040},{\"end\":71796,\"start\":71745},{\"end\":72763,\"start\":72693},{\"end\":73303,\"start\":73224},{\"end\":73804,\"start\":73733},{\"end\":74358,\"start\":74286},{\"end\":75066,\"start\":74777}]", "bib_author": "[{\"end\":51217,\"start\":51206},{\"end\":51745,\"start\":51734},{\"end\":52011,\"start\":52002},{\"end\":52251,\"start\":52241},{\"end\":52262,\"start\":52251},{\"end\":52483,\"start\":52476},{\"end\":52489,\"start\":52483},{\"end\":52501,\"start\":52489},{\"end\":52508,\"start\":52501},{\"end\":52730,\"start\":52719},{\"end\":52738,\"start\":52730},{\"end\":52745,\"start\":52738},{\"end\":52754,\"start\":52745},{\"end\":52764,\"start\":52754},{\"end\":52970,\"start\":52962},{\"end\":52981,\"start\":52970},{\"end\":52992,\"start\":52981},{\"end\":53005,\"start\":52992},{\"end\":53011,\"start\":53005},{\"end\":53022,\"start\":53011},{\"end\":53032,\"start\":53022},{\"end\":53043,\"start\":53032},{\"end\":53052,\"start\":53043},{\"end\":53407,\"start\":53400},{\"end\":53415,\"start\":53407},{\"end\":53422,\"start\":53415},{\"end\":53430,\"start\":53422},{\"end\":53436,\"start\":53430},{\"end\":53444,\"start\":53436},{\"end\":53720,\"start\":53709},{\"end\":53733,\"start\":53720},{\"end\":53743,\"start\":53733},{\"end\":53755,\"start\":53743},{\"end\":54166,\"start\":54153},{\"end\":54179,\"start\":54166},{\"end\":54190,\"start\":54179},{\"end\":54200,\"start\":54190},{\"end\":54210,\"start\":54200},{\"end\":54461,\"start\":54454},{\"end\":54471,\"start\":54461},{\"end\":54715,\"start\":54707},{\"end\":54728,\"start\":54715},{\"end\":54739,\"start\":54728},{\"end\":55257,\"start\":55249},{\"end\":55264,\"start\":55257},{\"end\":55273,\"start\":55264},{\"end\":55281,\"start\":55273},{\"end\":55290,\"start\":55281},{\"end\":55298,\"start\":55290},{\"end\":55814,\"start\":55808},{\"end\":55820,\"start\":55814},{\"end\":55828,\"start\":55820},{\"end\":55834,\"start\":55828},{\"end\":55844,\"start\":55834},{\"end\":56113,\"start\":56100},{\"end\":56125,\"start\":56113},{\"end\":56135,\"start\":56125},{\"end\":56149,\"start\":56135},{\"end\":56156,\"start\":56149},{\"end\":56168,\"start\":56156},{\"end\":56180,\"start\":56168},{\"end\":56189,\"start\":56180},{\"end\":56199,\"start\":56189},{\"end\":56208,\"start\":56199},{\"end\":56701,\"start\":56692},{\"end\":56708,\"start\":56701},{\"end\":56716,\"start\":56708},{\"end\":56723,\"start\":56716},{\"end\":56735,\"start\":56723},{\"end\":56742,\"start\":56735},{\"end\":56749,\"start\":56742},{\"end\":57206,\"start\":57197},{\"end\":57217,\"start\":57206},{\"end\":57228,\"start\":57217},{\"end\":57239,\"start\":57228},{\"end\":57512,\"start\":57506},{\"end\":57522,\"start\":57512},{\"end\":57533,\"start\":57522},{\"end\":57546,\"start\":57533},{\"end\":57879,\"start\":57866},{\"end\":57888,\"start\":57879},{\"end\":57900,\"start\":57888},{\"end\":57911,\"start\":57900},{\"end\":57922,\"start\":57911},{\"end\":58254,\"start\":58248},{\"end\":58260,\"start\":58254},{\"end\":58268,\"start\":58260},{\"end\":58274,\"start\":58268},{\"end\":58692,\"start\":58679},{\"end\":58706,\"start\":58692},{\"end\":58714,\"start\":58706},{\"end\":59013,\"start\":59003},{\"end\":59023,\"start\":59013},{\"end\":59367,\"start\":59359},{\"end\":59376,\"start\":59367},{\"end\":59391,\"start\":59376},{\"end\":59403,\"start\":59391},{\"end\":59416,\"start\":59403},{\"end\":59425,\"start\":59416},{\"end\":59938,\"start\":59928},{\"end\":59946,\"start\":59938},{\"end\":60170,\"start\":60159},{\"end\":60181,\"start\":60170},{\"end\":60194,\"start\":60181},{\"end\":60203,\"start\":60194},{\"end\":60212,\"start\":60203},{\"end\":60226,\"start\":60212},{\"end\":60236,\"start\":60226},{\"end\":60500,\"start\":60484},{\"end\":60510,\"start\":60500},{\"end\":60521,\"start\":60510},{\"end\":60789,\"start\":60779},{\"end\":60801,\"start\":60789},{\"end\":60810,\"start\":60801},{\"end\":61254,\"start\":61247},{\"end\":61262,\"start\":61254},{\"end\":61270,\"start\":61262},{\"end\":61280,\"start\":61270},{\"end\":61288,\"start\":61280},{\"end\":61565,\"start\":61559},{\"end\":61574,\"start\":61565},{\"end\":61583,\"start\":61574},{\"end\":61594,\"start\":61583},{\"end\":61850,\"start\":61839},{\"end\":61862,\"start\":61850},{\"end\":61871,\"start\":61862},{\"end\":61879,\"start\":61871},{\"end\":62187,\"start\":62169},{\"end\":62196,\"start\":62187},{\"end\":62204,\"start\":62196},{\"end\":62215,\"start\":62204},{\"end\":62635,\"start\":62624},{\"end\":62646,\"start\":62635},{\"end\":62658,\"start\":62646},{\"end\":62666,\"start\":62658},{\"end\":62927,\"start\":62918},{\"end\":62937,\"start\":62927},{\"end\":62947,\"start\":62937},{\"end\":63404,\"start\":63393},{\"end\":63418,\"start\":63404},{\"end\":63429,\"start\":63418},{\"end\":63864,\"start\":63856},{\"end\":63872,\"start\":63864},{\"end\":63878,\"start\":63872},{\"end\":63889,\"start\":63878},{\"end\":64146,\"start\":64139},{\"end\":64155,\"start\":64146},{\"end\":64163,\"start\":64155},{\"end\":64170,\"start\":64163},{\"end\":64425,\"start\":64417},{\"end\":64432,\"start\":64425},{\"end\":64439,\"start\":64432},{\"end\":64695,\"start\":64684},{\"end\":64702,\"start\":64695},{\"end\":64711,\"start\":64702},{\"end\":64718,\"start\":64711},{\"end\":64730,\"start\":64718},{\"end\":64739,\"start\":64730},{\"end\":64746,\"start\":64739},{\"end\":64752,\"start\":64746},{\"end\":65247,\"start\":65238},{\"end\":65255,\"start\":65247},{\"end\":65261,\"start\":65255},{\"end\":65592,\"start\":65583},{\"end\":65604,\"start\":65592},{\"end\":65615,\"start\":65604},{\"end\":65626,\"start\":65615},{\"end\":65895,\"start\":65889},{\"end\":65901,\"start\":65895},{\"end\":65907,\"start\":65901},{\"end\":66257,\"start\":66250},{\"end\":66265,\"start\":66257},{\"end\":66274,\"start\":66265},{\"end\":66282,\"start\":66274},{\"end\":66289,\"start\":66282},{\"end\":66299,\"start\":66289},{\"end\":66725,\"start\":66715},{\"end\":66735,\"start\":66725},{\"end\":66745,\"start\":66735},{\"end\":66756,\"start\":66745},{\"end\":67294,\"start\":67282},{\"end\":67305,\"start\":67294},{\"end\":67316,\"start\":67305},{\"end\":67631,\"start\":67614},{\"end\":67648,\"start\":67631},{\"end\":67657,\"start\":67648},{\"end\":67677,\"start\":67657},{\"end\":67698,\"start\":67677},{\"end\":67718,\"start\":67698},{\"end\":68052,\"start\":68042},{\"end\":68062,\"start\":68052},{\"end\":68073,\"start\":68062},{\"end\":68082,\"start\":68073},{\"end\":68097,\"start\":68082},{\"end\":68112,\"start\":68097},{\"end\":68360,\"start\":68352},{\"end\":68368,\"start\":68360},{\"end\":68375,\"start\":68368},{\"end\":68383,\"start\":68375},{\"end\":68390,\"start\":68383},{\"end\":68780,\"start\":68767},{\"end\":68789,\"start\":68780},{\"end\":68802,\"start\":68789},{\"end\":69254,\"start\":69246},{\"end\":69271,\"start\":69254},{\"end\":69284,\"start\":69271},{\"end\":69293,\"start\":69284},{\"end\":69647,\"start\":69634},{\"end\":69662,\"start\":69647},{\"end\":69986,\"start\":69976},{\"end\":69996,\"start\":69986},{\"end\":70008,\"start\":69996},{\"end\":70014,\"start\":70008},{\"end\":70027,\"start\":70014},{\"end\":70039,\"start\":70027},{\"end\":70445,\"start\":70437},{\"end\":70452,\"start\":70445},{\"end\":70458,\"start\":70452},{\"end\":70466,\"start\":70458},{\"end\":70473,\"start\":70466},{\"end\":70855,\"start\":70849},{\"end\":70864,\"start\":70855},{\"end\":70870,\"start\":70864},{\"end\":70878,\"start\":70870},{\"end\":71128,\"start\":71115},{\"end\":71140,\"start\":71128},{\"end\":71151,\"start\":71140},{\"end\":71449,\"start\":71443},{\"end\":71457,\"start\":71449},{\"end\":71465,\"start\":71457},{\"end\":71472,\"start\":71465},{\"end\":71480,\"start\":71472},{\"end\":71492,\"start\":71480},{\"end\":71503,\"start\":71492},{\"end\":71806,\"start\":71798},{\"end\":71814,\"start\":71806},{\"end\":71824,\"start\":71814},{\"end\":71833,\"start\":71824},{\"end\":71839,\"start\":71833},{\"end\":71850,\"start\":71839},{\"end\":72158,\"start\":72150},{\"end\":72164,\"start\":72158},{\"end\":72172,\"start\":72164},{\"end\":72179,\"start\":72172},{\"end\":72186,\"start\":72179},{\"end\":72194,\"start\":72186},{\"end\":72346,\"start\":72340},{\"end\":72354,\"start\":72346},{\"end\":72364,\"start\":72354},{\"end\":72374,\"start\":72364},{\"end\":72380,\"start\":72374},{\"end\":72772,\"start\":72765},{\"end\":72781,\"start\":72772},{\"end\":72789,\"start\":72781},{\"end\":72796,\"start\":72789},{\"end\":72804,\"start\":72796},{\"end\":72811,\"start\":72804},{\"end\":72823,\"start\":72811},{\"end\":73314,\"start\":73305},{\"end\":73325,\"start\":73314},{\"end\":73331,\"start\":73325},{\"end\":73337,\"start\":73331},{\"end\":73349,\"start\":73337},{\"end\":73359,\"start\":73349},{\"end\":73812,\"start\":73806},{\"end\":73820,\"start\":73812},{\"end\":73828,\"start\":73820},{\"end\":73836,\"start\":73828},{\"end\":73844,\"start\":73836},{\"end\":73851,\"start\":73844},{\"end\":73863,\"start\":73851},{\"end\":73874,\"start\":73863},{\"end\":74367,\"start\":74360},{\"end\":74375,\"start\":74367},{\"end\":74381,\"start\":74375},{\"end\":74389,\"start\":74381},{\"end\":74609,\"start\":74599},{\"end\":74619,\"start\":74609},{\"end\":74631,\"start\":74619},{\"end\":75079,\"start\":75068},{\"end\":75089,\"start\":75079},{\"end\":75100,\"start\":75089},{\"end\":75109,\"start\":75100},{\"end\":75114,\"start\":75109},{\"end\":75119,\"start\":75114},{\"end\":75126,\"start\":75119}]", "bib_venue": "[{\"end\":54908,\"start\":54832},{\"end\":55455,\"start\":55385},{\"end\":56898,\"start\":56832},{\"end\":59586,\"start\":59514},{\"end\":60951,\"start\":60889},{\"end\":62336,\"start\":62284},{\"end\":63088,\"start\":63026},{\"end\":66420,\"start\":66368},{\"end\":66824,\"start\":66809},{\"end\":68505,\"start\":68456},{\"end\":70180,\"start\":70118},{\"end\":70614,\"start\":70552},{\"end\":72972,\"start\":72906},{\"end\":73488,\"start\":73432},{\"end\":74023,\"start\":73957},{\"end\":51204,\"start\":51121},{\"end\":51548,\"start\":51530},{\"end\":51732,\"start\":51662},{\"end\":52000,\"start\":51988},{\"end\":52239,\"start\":52205},{\"end\":52474,\"start\":52396},{\"end\":52717,\"start\":52700},{\"end\":53112,\"start\":53052},{\"end\":53398,\"start\":53343},{\"end\":53837,\"start\":53755},{\"end\":54151,\"start\":54102},{\"end\":54452,\"start\":54400},{\"end\":54830,\"start\":54739},{\"end\":55383,\"start\":55298},{\"end\":55875,\"start\":55844},{\"end\":56279,\"start\":56208},{\"end\":56830,\"start\":56749},{\"end\":57195,\"start\":57134},{\"end\":57584,\"start\":57546},{\"end\":57967,\"start\":57922},{\"end\":58359,\"start\":58274},{\"end\":58677,\"start\":58580},{\"end\":59077,\"start\":59023},{\"end\":59512,\"start\":59425},{\"end\":59965,\"start\":59946},{\"end\":60157,\"start\":60117},{\"end\":60482,\"start\":60436},{\"end\":60887,\"start\":60810},{\"end\":61245,\"start\":61188},{\"end\":61557,\"start\":61471},{\"end\":61923,\"start\":61879},{\"end\":62282,\"start\":62215},{\"end\":62622,\"start\":62570},{\"end\":63024,\"start\":62947},{\"end\":63515,\"start\":63429},{\"end\":63900,\"start\":63889},{\"end\":64137,\"start\":64068},{\"end\":64415,\"start\":64352},{\"end\":64864,\"start\":64752},{\"end\":65325,\"start\":65261},{\"end\":65581,\"start\":65507},{\"end\":65979,\"start\":65907},{\"end\":66366,\"start\":66299},{\"end\":66807,\"start\":66756},{\"end\":67330,\"start\":67316},{\"end\":67740,\"start\":67718},{\"end\":68040,\"start\":67992},{\"end\":68454,\"start\":68390},{\"end\":68857,\"start\":68802},{\"end\":69244,\"start\":69079},{\"end\":69721,\"start\":69662},{\"end\":70116,\"start\":70039},{\"end\":70550,\"start\":70473},{\"end\":70847,\"start\":70796},{\"end\":71178,\"start\":71151},{\"end\":71441,\"start\":71367},{\"end\":71913,\"start\":71850},{\"end\":72148,\"start\":72130},{\"end\":72490,\"start\":72396},{\"end\":72904,\"start\":72823},{\"end\":73430,\"start\":73359},{\"end\":73955,\"start\":73874},{\"end\":74400,\"start\":74389},{\"end\":74597,\"start\":74565},{\"end\":75208,\"start\":75126}]"}}}, "year": 2023, "month": 12, "day": 17}