{"id": 225066771, "updated": "2023-10-06 10:12:16.32", "metadata": {"title": "Temporal Reasoning on Implicit Events from Distant Supervision", "authors": "[{\"first\":\"Ben\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Kyle\",\"last\":\"Richardson\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Ning\",\"middle\":[]},{\"first\":\"Tushar\",\"last\":\"Khot\",\"middle\":[]},{\"first\":\"Ashish\",\"last\":\"Sabharwal\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Roth\",\"middle\":[]}]", "venue": "NAACL", "journal": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events\u2014events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SymTime, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. SymTime outperforms strong baseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1%-9% on MATRES, an explicit event benchmark.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.12753", "mag": "3169993339", "acl": "2021.naacl-main.107", "pubmed": null, "pubmedcentral": null, "dblp": "conf/naacl/ZhouRNKSR21", "doi": "10.18653/v1/2021.naacl-main.107"}}, "content": {"source": {"pdf_hash": "ec0f0e5f1abec87235d410dc83444dfbfaeccef0", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2021.naacl-main.107.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2021.naacl-main.107.pdf", "status": "HYBRID"}}, "grobid": {"id": "ecfa8ce41711f1b25629f471133ca880dcca5f34", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ec0f0e5f1abec87235d410dc83444dfbfaeccef0.txt", "contents": "\nTemporal Reasoning on Implicit Events from Distant Supervision\nJune 6-11, 2021\n\nBen Zhou \nAllen Institute for AI\n\n\nUniversity of Pennsylvania\n3 Amazon\n\nKyle Richardson \nAllen Institute for AI\n\n\nQiang Ning qning@amazon.com \nTushar Khot \nAllen Institute for AI\n\n\nAshish Sabharwal ashishs@allenai.orgxyzhou \nAllen Institute for AI\n\n\nDan Roth danroth@cis.upenn.edu \nUniversity of Pennsylvania\n3 Amazon\n\nTemporal Reasoning on Implicit Events from Distant Supervision\n\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\nthe 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20211361\nWe propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit eventsevents that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SYMTIME, which exploits distant supervision signals from largescale text and uses temporal rules to combine start times and durations to infer end times. SYMTIME outperforms strong baseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1%-9% on MATRES, an explicit event benchmark.\n\nIntroduction\n\nUnderstanding temporal relations between events in narrative text is a crucial part of text understanding. When reading a story, a human can construct a latent timeline about events' start and end times, similar to the one shown in Fig. 1 about an automobile accident. This timeline not only contains the placements of explicitly mentioned events (e.g., ride a bicycle), but also accounts for implicit events (e.g., Farrah was distracted so she looked away). Such a latent timeline explains the dynamics between events; for example, the possible chain of events between ride and recovered in this context * Most of the work was done when the third author was employed at the Allen Institute for AI and the first author was an intern there.\n\nFarrah was driving home from school. A person was riding a bicycle in front of her. Farrah looked away for a second. She didn't notice that he stopped. She tried to brake but it was too late. The person recovered soon.  Figure 1: A story, its latent timeline, and example TRA-CIE instances from it. For simplicity, events are shortened to single verbs and the timeline is exaggerated.\n\n\nContext\n\ncontains get hit and injured. The ability to construct such a timeline is essential for understanding the causal dynamics of a situation. Without it, NLP systems cannot truly understand situations and reliably solve tasks such as temporal question-answering, causal inference, and scheduling assistance. To better evaluate this ability, we introduce a new dataset called TRACIE (TempoRAl Closure InfErence) that focuses on temporal relations on implicit events in short stories. Our dataset contains high-quality annotations of both start and end time queries that test a system's understanding of the full temporal closure (i.e., both start and end time) of events. As a task that requires considerable commonsense knowledge, we follow  in minimizing the size of the training set, therefore making TRACIE mainly an evaluation set. The final TRACIE dataset contains a total of 5.4k human-curated instances, provided in a (multi-premise) textual entailment (TE) format, as illustrated at the bottom of Fig 1. A Pre-trained language model such as T5-Large (Raffel et al., 2020) fine-tuned on our new dataset achieves a modest binary prediction accuracy of 67.9%. 1 Consistent with other studies on temporal reasoning , these results reveal serious limitations in existing pre-trained language models.\n\nTo build models better capable of understanding time with minimal direct training data, we propose a novel distant supervision technique that improves generalization by extracting temporal patterns in large-scale free text as part of an additional pretraining step. In contrast to other attempts at extracting temporal data through patterns at a sentence level (Gusev et al., 2011;, we extract over large windows of text such as paragraphs. This allows for capturing global information related to multiple events and extracting signals that do not appear in small-window local contexts. The resulting model, PTNTIME (Pattern-Time), achieves a 76.6% accuracy on TRACIE, a 9% gain over using standard T5-Large. We also show the applicability of PTNTIME on a standard temporal reasoning benchmark involving only explicit events, MATRES (Ning et al., 2018b), with a 9 point gain in a low-resource setting.\n\nWe achieve further improvements by coupling PTNTIME with a duration model from  to create a neural-symbolic reasoning model called SYMTIME. The key idea in SYMTIME is to decompose the computation of temporal relations to the predictions of relative distances between start times and those of durations. For example, in Fig 1, we can decide that distracted likely ends before try starts because the duration of distracted is likely to be shorter than the distance between the two start times. This allows for better prediction on the end time, which rarely appears in the natural text and has been previously shown to be difficult to annotate (Ning et al., 2018b). Such a symbolic computation involves a logical combination of the individual models in a way that formalizes part of the Allen interval algebra (Allen, 1983). This model, which supports a wider range of temporal computation and can be used with and without taskspecific supervision, achieves a final accuracy of 78.9% on TRACIE's binary classification metric. We also show that SYMTIME is more robust to different distributions of the training data, demonstrating the benefits of using a temporal model with a transparent reasoning process. 1 The same model achieves 77.4% on MATRES (Ning et al., 2018b) with a similar amount of training instances. All TRACIE numbers reported in this section are from Table 2. In summary, we make the following 3 contributions: (1) a temporal relation dataset TRACIE focusing on implicit events ( \u00a73); (2) a distant supervision process for temporal understanding of implicit events ( \u00a74); and (3) a reasoning model that makes end-time comparisons using predictions of start-time distances and durations ( \u00a75). Finally, we demonstrate the effectiveness of our models on TRACIE, as well as the applicability of our approach to an existing temporal benchmark ( \u00a76).\n\n\nRelated Work\n\nTemporal reasoning has received much attention in the NLP community, and to date, there are many datasets that focus on temporal ordering (Pustejovsky et al., 2003;Bethard et al., 2007;Cassidy et al., 2014;Reimers et al., 2016;O'Gorman et al., 2016;Ning et al., 2018bNing et al., , 2020b, and other temporal knowledge (Pan et al., 2006;Zhou et al., 2019). We focus here on modeling implicit events, which has received relatively little attention. Multiple systems have been proposed as part of research into temporal ordering (Do et al., 2012;Moens and Leeuwenberg, 2017;Leeuwenberg and Moens, 2018;Meng and Rumshisky, 2018;Ning et al., 2018c;Han et al., 2019), duration prediction (Vashishtha et al., 2019) and other tasks. Our decision to use a textual entailment style follows recent work on natural language inference (Williams et al., 2017;Nie et al., 2020;Bhagavatula et al., 2020), which tends to not focus on time (for recent work on temporal NLI, see Vashishtha et al. (2020)). Many have used distant supervision for temporal reasoning (Gusev et al., 2011;Ning et al., 2018a;. Comparatively, our work captures longer-range dependencies in narrative text (for related ideas, see Ammanabrolu et al. (2021)).\n\nWe are inspired by structural predictions and constraints that combat the sparsity of temporal knowledge (Ning et al., 2017;Do et al., 2012), as well as neural module networks (Andreas et al., 2016; and other decomposition-based approaches (Talmor and Berant, 2018;Khashabi et al., 2018;Li et al., 2019;Wolfson et al., 2020;Khot et al., 2021). In particular, we build neuralsymbolic transformer models that operationalize some of the classical interval-based computations used in earlier work on temporal reasoning (Allen, 1983;Gerevini and Schubert, 1995) (for related ideas, compare with Leeuwenberg and Moens (2018); Vashishtha et al. (2019)).\n\n\nContext Story (Premise)\n\nHypothesis Inference Label Tom needed to get braces. He was afraid of them. The dentist assured him everything would be fine. Tom had them on for a while. Once removed he felt it was worth it.\n\nTom avoids foods he can't eat with braces starts before the braces are removed.\n\n\nentailment\n\nWe were all watching Spongebob as a family. It is a kid's show but all really enjoyed it. This one episode was especially funny for the adults. It has humor in it that is funny for kids and adults. It is something we can all watch...\n\nThe adults laughed at the jokes ends before we watch Spongebob as a family contradiction I was throwing the baseball with my son. He threw one past me that landed in the lake. I reached in to get the ball. I lost my balance and fell in. I got the ball and a bath all in one shot!\n\nThe ball was in the boys hand starts after he reached for the ball contradiction Figure 2: Example TRACIE instances. The comparator l \u2208{starts,ends} and relation r \u2208{before,after} in each hypothesis are highlighted, in addition to the corresponding explicit event from the story. This work is broadly related to works on causal dynamics (Pearl, 2009). The nature of combined temporal and causal focuses is also related to procedural text modeling (Tandon et al., 2018(Tandon et al., , 2020.\n\n\nThe TRACIE Dataset\n\nIn this section, we introduce the TRACIE dataset. 2\n\n\nTask Overview and Dataset Construction\n\nThe goal of TRACIE is to test a system's ability to compare start and end times of non-extractive implicit event phrases instead of extractive triggers from the context. Such tests in TRACIE take the form of multi-premise textual entailment (TE) (Lai et al., 2017). Each TRACIE instance contains 1) a context story (or premise) consisting of a sequence of explicit narrative events; 2) an implicit event in the form of a natural language phrase that is unmentioned but has some role in the story; 3) a comparator of either {starts,ends}; 4) an explicit event also in the form of a phrase, and 5) a temporal relation of either {before,after} that marks the relationship in the dimension defined by the comparator between the implicit-event and the explicit-event. With these 4 components, we are able to generate TE-style instances, using the context story as the premise and temporal queries about pair-wise relations between implicit and explicit events as hypotheses. For example, in the first positive instance shown in Fig. 1, \"distracted\" is the implicit-event, \"starts\" is the comparator, \"try\" is explicit-event and \"before\" is the temporal-relation. They form a positive hypothesis \"distracted starts before try.\" 3 We flip the temporal-relation (i.e., \"before\" to \"after\" and vice versa) to create negative  Figure 3: TRACIE's label definition and its relation to Allen's interval algebra, with a graph illustration between an implicit event and an explicit event.\n\n(contradiction) instances, as shown in the second example instance in Fig. 1.\n\nSince the start times of explicit-events are more obvious to human annotators, we use them as reference points and compare the implicit-event's start or end time with them (depending on the comparator), according to the label definitions shown in Fig. 3. In rare cases where two time points are the same (e.g., hit and get hit start at the same time in Fig.1), we use the causal relation to decide the order, so that hit starts before get hit. Such instances are created through a multi-stage annotation process as detailed (in respective order) below. All steps are implemented with the CrowdAQ platform (Ning et al., 2020a) with qualification exams.\n\n\nImplicit Event Generation\n\nWe randomly sample short stories from the ROCStories dataset (Mostafazadeh et al., 2016). For each story, one annotator writes 5 implicit event phrases that are not explicitly mentioned by the given story, but are inferable and relevant. The annotator additionally rewrites two explicit events closest to the implicit event's start and end time, respectively. With these two events, we can build two TRACIE instances (minus the temporal-relation) per implicit event, which accounts for 10 instances in total per story.\n\nAutomatic Instance Generation We use Al-lenNLP (Gardner et al., 2018) to extract all verbs and relevant arguments with its semantic role labeling (SRL) model. With all the verbs and their arguments, we construct a pool of explicit events in the form of short phrases. For each implicit event, we randomly select two {explicit-event, compara-tor} pairs from the pool and build 10 additional instances (without temporal-relation).\n\nLabel Collection For each of the 20 instances per story, we annotate the temporal-relation with four different annotators. Annotators follow the label definition in \u00a73.1 to produce four temporalrelations for each instance. We use the majority agreement as the final label and filter out unagreeable instances. Two authors additionally verify the instances with ambiguous verbs (e.g., \"have\") and corrected 5% of the end-time instances.\n\n\nSplits and Analysis\n\nWe split the data under the independent and identically distributed (i.i.d.) assumption based on stories, with a 20/80 train/test ratio. We use a small training set, following Zhou et al. (2019), as we believe temporal relations involve much commonsense knowledge. As we later show in \u00a76.3, it is infeasible to collect a large enough human-annotated training set to capture all the knowledge needed to tackle this problem completely, and a system must acquire knowledge from external resources. As a result, we use a small training set just to define the task, and at the same time, use an extensive testing set for more robust evaluation.\n\nThe authors conduct a human upper-bound analysis on 100 randomly sampled instances, following the procedure in . There is a 94% agreement and a 98% resolved accuracy, 4 suggesting that TRACIE has a high annotation quality.\n\n\nPattern-Based Pre-Training\n\nAs argued in \u00a73.2, we believe that it is more efficient to build a model that learns the prior knowledge needed for the task with distant signals and only subsequently learns the task definition through a small training set. This section describes how we collect the distant signals related to events' starttime comparisons and pre-train a novel temporallyaware transformer model called PTNTIME. While PTNTIME will be used for fine-tuning directly on TRACIE, it will also form the basis of a more general temporal reasoning model called SYMTIME that we describe in \u00a75.\n\n\nDistant Supervision Collection\n\nWe describe the sources of distant supervision signals with the goal of understanding the relative order between two events' start times as well as the relative distance between them.\n\nI went to the park on January 1 st . I was very hungry after some hiking. Luckily, I purchased a lot of food before I went to the park. I enjoyed the trip and wrote an online review about the trip on the 10 th .\n\n[I purchased food, I went to the park.]: before [I went to the park, I wrote a review]: before, weeks Within-Sentence Extraction We collect start time comparisons between pairs of events heuristically from free-text using \"before/after\" keywords (following much prior work in temporal modeling and extraction (Do et al., 2012)). We use Al-lenNLP's SRL model to process each input sentence and find verbs with a temporal argument that starts with either \"before\" or \"after\", and contains at least another verb. If there are multiple verbs in the temporal argument, we take the one with the largest number of tokens as arguments. We match the two extracted verbs with the relation indicated by the first word of either \"before\" or \"after\". As the example in Fig. 4 shows, the extractor identifies that purchase food is before go to park as indicated by the \"before\" keyword mentioned in the text. We acquire 2.8 million instances from the May 2020 Wikipedia dump using this process.\n\n\nCross-Sentence Extraction\n\nThe data collected from the within-sentence patterns does not reveal the relative distance between two start times. In addition, because writers often save trivial inferences for efficiency, certain event pairs rarely co-occur within a small textual window, making one event often implicit to the other one in these pairs. To better collect such signals, we employ a cross-sentence extraction that finds direct temporal expressions of hours and dates. Because these temporal expressions (e.g., 2021-01-01) are globally comparable, the compared events can be anywhere in a document. Therefore, this process collects more supervision signals about time-point comparisons and their relative distance on event pairs with trivial causal relations. We apply the SRL model and find all temporal arguments and their associated verbs. We find the exact temporal values by filling unmentioned elements of a temporal expression with the nearest previous mention (e.g., we add \"January\" to the expression of \"the 10th\" in Fig. 4.) These extractions have high precision, as the SRL model does well on identifying temporal arguments.\n\nWe then construct supervision instances under the assumption that the extracted temporal expressions describe the start times of the associated verbs (e.g., went started on January 1 st in Fig. 4) . Each instance comprises an event pair, a temporal relation, and an estimation on the temporal difference between the two start times. Each event is a phrase constructed by taking all relevant arguments of the predicate verb in the SRL parses. We represent the differences between the two start times as one of seven coarse temporal units: {\u2264minutes, hours, days, weeks, months, years, \u2265decades}. For example, we get go to park is weeks before write review as shown in Fig. 4. In addition to the event pairs, we randomly sample sentences within the paragraph to use as the context that better defines the events. We collect 700k instances from this cross-sentence extraction process from Wikipedia.\n\nLanguage Model (LM) Pre-Training Data We couple the specialized temporal pre-training data described above with additional paragraphs that are used to perform conventional language model pretraining using the original denoising task proposed in Raffel et al. (2020). This is done to maintain part of the original language model's semantics and to avoid overfitting. We use the Gutenberg Dataset (Lahiri, 2014) as the source and collect 1 million paragraphs for this purpose. Here [EventA] represents the tokens that describe the first event; [EventB] represents the ones that describe the second event; and [Paragraph] represents the tokens of the context, which is non-empty only for cross-sentence extractions. [Relation] is either before or after, and [Label] is either positive or negative. When the label is positive, the relation will be the gold relation extracted from the text; when it is negative, the relation will be the inverse of the extracted relation. We randomly make 50% of the instances negative. [Distance] is one of the 7 coarse temporal units represented with a set of blank tokens [extra_id_N]. We leave it to be blank for the within-sentence extractions so that the objective function will not include it in loss computations. The LM pre-training data follows the original format in Raffel et al. (2020).\n\n\nData Format\n\n\nPattern-Based Temporal Model (PTNTIME)\n\nWe use a pre-trained sequence-to-sequence model as our base model and additionally pre-train this model using the data collected in \u00a74.1 (for modeling details, see \u00a76.1). We call the resulting model PTNTIME. As a result of this additional pre-training step, PTNTIME serves as new set of temporally-aware model weights that can be used in place of existing pre-trained models and finetuned on TRACIE. As we describe next, we also use PTNTIME to build a modular temporal reasoning model called SYMTIME that attempts to go beyond a standard language modeling approach and improve start and end point prediction.\n\n\nSymbolic Temporal Reasoning Model (SYMTIME)\n\nTo address the challenge of predicting event end times for which it is difficult to obtain high-quality direct or distant supervision, we introduce a new reasoning model called SYMTIME in this section. This model makes end-time comparisons by symbolically combining start time distance and duration from separate predictions based on some of the components introduced in the previous section. Different from Leeuwenberg and Moens (2018) and Vashishtha et al. (2019), our model does not rely on explicit annotations on timepoints, but only relative comparisons between them.\n\n\nFormulation\n\nAs described in \u00a73.1, hypotheses in TRACIE make pair-wise comparisons between two events e 1 and e 2 using a comparator l from {starts, ends} and a query-relation r from {before, after} based on a provided story context. We associate each e j with a latent start time start j and an end comparator l relation r l (e1, e2)= ends before if end1 < start2 after otherwise starts before if start1 < start2 after otherwise Figure 5: Decomposition of the relation functions that solve TRACIE instances (equal timepoints ignored).\n\ntime end j , as well as, for convenience, a duration duration j = end j \u2212 start j . Under this formulation, a symbolic approach to solving TRACIE involves computing the relation functions r l shown in Figure 5. For example, given exact numeric values end 1 and start 2 , as one would assume in a classical interval-based approach to temporal reasoning (Allen, 1983) 5 , determining if the first event ends before the second involves simply computing whether end 1 is less than start 2 . Given that the exact values of start and end times are latent, we use the intervals to do the same comparisons, as they are more context-invariant. For example, we do not need the exact date to know that lunch starts before dinner in the same day, because there is a typical distribution of the relative distance between the two start times. Based on this idea, we build a neural-symbolic model that learns approximations of these simple functions in Fig. 5 in a differentiable way. Specifically, we use individual neural modules that make predictions about event intervals via distance and duration functions dist(e i , e j ) and dur(e j ), respectively.\n\nTo understand this decomposition, we define the distance and duration functions computed by these two modules as dist(e i , e j ) = start i \u2212 start j and dur(e j ) = duration j . By exploiting the rule that an end point end j can be computed as end j = start j + duration j , we can, for example, decompose the relation r ends (e 1 , e 2 ) = before (i.e., e 1 ends before e 2 ) in terms of our two modules as follows via simple algebraic manipulation: r ends (e 1 , e 2 ) = before \u21d4 end 1 < start 2 \u21d4 start 1 + duration 1 < start 2 \u21d4 start 1 \u2212 start 2 + duration 1 < 0 \u21d4 dist(e 1 , e 2 ) + dur(e 1 ) < 0 5 In the Allen algebra, the values endx and starty correspond to the right and left end points x + , y \u2212 in the intervals (x \u2212 , x + ), (y \u2212 , y + ). Likewise, our durationx corresponds to the value (x + \u2212 x \u2212 ). Hence, we have reduced the computation of the relation ends before to a symbolic computation over two numeric intervals. Conversely, we have r ends (e 1 , e 2 ) = after \u21d4 dist(e 1 , e 2 ) + dur(e 1 ) > 0, 6 For the starts comparator, we have r starts (e 1 , e 2 ) = before \u21d4 dist(e 1 , e 2 ) < 0 and vice versa for the after relation.\n\n\nEvent A Event B\n\n\nQuery on A's Duration Query on A and B's Distance\nencoder decoder dur() encoder decoder dist() v d p c T v c T d g(p) x + = pred g(x)=tanh(x 2 -x 1 ) Duration of A Start\nIn what follows, we describe how we approximate the values of the two functions via individual neural modules (see illustration in Fig. 6).\n\n\nDuration Estimation\n\nTo obtain a model to estimate dur(\u00b7), we pre-train a sequence-to-sequence model with the duration data from , which is similarly collected from pattern-based extraction. The data contains over 1 million events with their corresponding duration values. We map each instance to an input sequence event:[Event]story: [Story] and a corresponding output sequence answer:[Value], where [Event] represents the tokens of an event with the trigger verb marked by a special token to its left, [Story] represents down-sampled tokens from the context, and [Value] is one of the 7 unit labels as described in \u00a74.1 (i.e., { \u2264minutes, hours, days, weeks, months, years, \u2265decades }).\n\n\nComputation and Learning\n\nWe use the output from PTNTIME to approximate the function dist(\u00b7). Following the sequence formulation of PTNTIME in \u00a74, we replace [EventA] with the textual description of e 1 , [EventB] with the textual description of e 2 , and [Paragraph] with the context (premise), and fix [Relation] to be before. By taking the values of the vocabulary indices corresponding to \"positive\" and \"negative\" from the logits of [Label] and applying a softmax operation, we get P before and P after . These are the probability of e 1 starting before and after e 2 , respectively, and are used to define the vector p = [P before , P after ]. Similarly, we apply softmax to the logits of [Distance] over the 7 words representing the temporal units to obtain 7 values that approximate the probabilities of the distance between two events' start times being closest to each temporal unit. We place the 7 values in temporal units' increasing order in vector d. To represent |start 1 \u2212start 2 | with a single value, we dot product the probabilities with an incremental constant vector c = [0, 1, 2, 3, 4, 5, 6]. To get the direction, we apply the tanh function to the difference between the probabilities in p. 7 As a result, we have:\ndist(\u00b7) = start 1 \u2212 start 2 = c T d * tanh(INT max * (p 2 \u2212 p 1 ))(1)\nWe use the pre-trained model in \u00a75.2 to approximate the function dur(\u00b7). Because the model is pre-trained with markers to the left of trigger verbs, we run a part-of-speech tagger on input phrases and add a marker to the left of the first verb. We apply softmax to the logit values of [Value] over the 7 temporal unit words and get, as above, 7 values representing the probabilities of the input event's duration being closest to each unit. We form v by placing these values at the temporal unit's increasing order. With the same constant vector, we have:\ndur(\u00b7) = duration 1 = c T v(2)\nFor hypotheses with comparator starts, we use PTNTIME and its sequence-to-sequence objective to learn (i.e., we take the input hypothesis and context as is and use [Label] directly as the prediction). For hypotheses where the comparator is ends, we use the inference process in \u00a75.1 and the computation process described above to construct logits = [pred, \u2212pred], pred = dist(e 1 , e 2 ) + dur(e 1 ) as detailed in Fig. 6. We find the gold-temporal-relation in each training instance and compute a two-class cross-entropy loss with logits. The PTNTIME that predicts starts 7 To ensure that tanh returns a value close to 1 or -1, we multiply the distance by a big number denoted as INTmax.\n\nhypotheses shares weights with the one used in computing logits. The final model SYMTIME can also be used to predict TRACIE instances without any task-specific supervision as the two functions are initialized with distant supervision.\n\n\nExperiments\n\nIn this section, we detail our experimental setup ( \u00a76.1-6.2) and report our main results ( \u00a76.3-6.5). 8\n\n\nBaselines and Systems\n\nWe use T5-Large implemented by Wolf et al. (2019) as our base sequence-to-sequence model for both PTNTIME and the duration model in \u00a75.2 as it provides for faster iterations. We use early stopping, batch size of 32 and other default parameters. PTNTIME converges after 45k steps (\u223c1.4M instances) and the duration model converges after 80k steps (\u223c2.6M instances). We use these pretrained weights in SYMTIME as well as SYMTIME-ZEROSHOT which uses no TRACIE supervision.\n\nWe compare with our proposed models with a host of baselines based on the same pre-trained language model, including BaseLM: T5-Large, and BaseLM-MATRES: T5-Large fine-tuned on 20k MATRES training data. We also compare with other architectures/models, including BiLSTM as used in Williams et al. (2017), Roberta-Large (Liu et al., 2019) and T5-3B. All models and baselines follow a standard TE setup and default parameters. We report a 3-run average and each model is run until convergence.\n\n\nMetrics and Settings\n\nWe measure system performance on TRACIE separately for start-time hypotheses and end-time hypotheses. We also employ a story-wide exact match metric, which is the percentage of stories with all its related hypotheses answered correctly.\n\nIn addition to TRACIE's standard i.i.d. split, we propose a pruned version of the training set with balanced prior distributions. For example, in the i.i.d. training set, 70% of the examples with the comparator ends and relation after are positive. We randomly remove instances from the majority classes to produce a uniform-prior training set such that a model can no longer rely on such prior distributions. We believe this setting better evaluates a system's true understanding of the task.   Table 1 shows system performance on TRACIE's i.i.d. setting. We observe that PTNTIME improves on all metrics over the base language model, with 6% on start-time comparisons and 8% on storywide exact match. It also outperforms BaseLM-MATRES, suggesting that distant supervision is more efficient than extensive human annotation. With a symbolic end-time inference, SYMTIME further improves on all metrics, with 7%, 4%, and 9% gains over the base language model on start time, end time and story-wide exact match, respectively. SYMTIME can further improve the performance on start-time hypotheses over PTNTIME even though they use the same model to predict start-time queries. This is because PTNTIME is not designed to understand end time from pre-training, and fine-tuning on such data hurts its representation in general. This illustrates the benefits of models using explicit and sensible reasoning processes.   (2020) is not strictly comparable with the rest.\n\n\nMain Results\n\na system cannot exploit prior knowledge about the label distribution when making predictions. Given this, we see that all baselines produce a much lower performance, e.g., the BiLSTM, which is a model that lacks much of the pre-requisite knowledge for reasoning, suddenly performs near random chance. Compared to the baseline models, PTNTIME only drops 2.7%, suggesting that it is more invariant to evaluation settings and better understands temporal common sense. SYMTIME has the smallest drop among all models (1.7%) because of its explicit reasoning process on end-time hypotheses. SYMTIME-ZEROSHOT does not use any TRACIE training examples, so it has the same performance in the uniform-prior setting which outperforms all supervised baselines including T5-3B.\n\n\nExtrinsic Evaluation\n\nTo show that our model is not limited to the TRA-CIE dataset and is general in temporal relation reasoning, we also evaluate on MATRES (Ning et al., 2018b), a temporal relation dataset focused on comparing explicit events' start times. We train and evaluate only the instances with a label of either \"before\" or \"after\", which accounts for about 80% of all instances. We compare the performance of SYMTIME 9 with BaseLM. We report four results -  . In OT-NS, we also report a SOTA system from Wang et al. (2020) under the same two-label 10 setting. Table 3 shows the performance of our model and the baselines. We see that our model is consistently   better than BaseLM, and at the same time, comparable to Wang et al. (2020). Our model benefits more from input contexts, and only drops 4% in the OT-MS setting with minimal supervision (from 89.6 to 86.1), comparing to the 10% drop from T5-Large. This shows the effectiveness of our distant signals in \u00a74.1, which are also designed to encourage contextual understandings.\n\n\nAblation Studies and Analysis\n\nTo better understand the improvements from our models, we conduct several ablation studies. Table 4 shows the results on TRACIE where the story is not provided as part of the inputs to systems (a no-story setting). While such a setting bares some resemblance to the partial-input baselines often employed in TE (Poliak et al., 2018), in our setting, it is often possible to predict temporal relations in the absence of stories because of strong commonsense priors. Indeed, we estimate that 65% of the instances can be correctly predicted from the hypotheses alone, based on expert analysis in \u00a7 3.2. This suggests a 82.5% human upper-bound 11 in this no-story setting. Hence, such a setting partly evaluates a model's ability to incorporate commonsense priors when making decisions.\n\nWe see that BaseLM is close to random chance, whereas PTNTIME and SYMTIME improve 20% and 22% respectively. This suggests that our models better understand temporal common sense through the distant supervision on both start times and duration. On the other hand, we observe much smaller drops in our model's performances in this no-story setting. This suggests that our models do not improve as much on the 35% instances that require multi-hop timeline constructions over more than two events, motivating future work. Table 5 compares the two pre-training sources 11 We assume that the remaining 35% non-predictable instances are decided by random guessing. described in \u00a74.1 by individually pre-training two models with only within-sentence or cross-sentence extracted data. We see that the cross-sentence extraction brings the most performance gain on TRA-CIE's start-time binary metric under the uniformprior training setting. This suggests that the global extraction rule is able to introduce new knowledge that is not seen in localized language model pretraining. Combining the within-sentence data further improves the performance.\n\nThrough analysis on the interval predictions made by SYMTIME, we notice a tendency for the model to predict \"after\" for end-time instances, possibly due to overly-estimated durations: a byproduct of natural biases in text. Given the weak signal used to learn such intervals and these potential biases, this is not altogether surprising. We leave the task of learning more robust and faithful interval representations for future work.\n\n\nConclusion\n\nWe introduce a challenging dataset TRACIE, to evaluate systems' temporal understanding of implicit events. We propose a distant supervision process that improves language models' understanding of start times of both explicit and implicit events. We further combine this process with a distantly supervised model that estimates events' duration to compare event end times, under the explicit rule that end times are start times plus durations. We show that our model improves over TRACIE and MATRES, suggesting the effectiveness of highprecision pre-training and symbolic temporal reasoning. Despite these advances, TRACIE continues to be a challenging task for future work on general temporal reasoning.\n\nFigure 4 :\n4Extraction for start-time comparisons applied to an example paragraph.\n\nFigure 6 :\n6A schematic overview of SYMTIME to compare event A's end time with event B's start time via modular predictions about A's duration and distance from B and their symbolic combination (bottom).\n\n\nWe then format the within / crosssentence extraction data to consistent instances that have input sequences of event:[EventA] starts [Relation][EventB].story:[Paragraph] and output sequences of answer:[Label][Distance].\n\nTable 2 :\n2Performance on TRACIE uniform-prior training setting. \u2206All compares the difference withTable 1; Majority is equivalent to random guessing.\n\nTable 2\n2compares systems in the uniform-prior training setting. Compared to the setting inTable 1,System \nOT-NS \nOT \nOT-MS \nPT \n\nWang et al. (2020) \n85.9 \n-\n-\n-\nBaseLM \n86.0 \n87.5 \n77.4 \n69.0 \nSYMTIME \n87.3 \n89.6 \n86.1 \n75.1 \n\n\n\nTable 3 :\n3Performance on MATRES. Wang et al.\n\n\nOT-NS (original test, no story): train and test with only the sentences containing the trigger verbs; OT: train and test with the entire document (down-sampled to be below the maximum sequence length) as an auxiliary input; OT-MS (original test, minimal supervision): train with 1.2k (6%) training instances; PT (perturbed test): train with the complete training set and test on a perturbed test set from\n\n\nSys. BaseLM PTNTIME SYMTIME HumanAcc. \n52.6 \n72.2 \n75.3 \n82.5 \n\n\n\nTable 4 :\n4Performance on no-story TRACIE under the uniform-prior training setting.Sys. PTNTIME cross-sentence within-sentence \n\nAcc. \n80.6 \n79.9 \n63.7 \n\n\n\nTable 5 :\n5Comparison of pre-training data sources on TRACIE's start time prediction accuracy, under the uniform-prior training setting.\nWe release TRACIE and its leaderboard at https:// leaderboard.allenai.org/tracie 3 All event phrases are shortened to triggers here for simplicity. SeeFig. 2for actual phrases.\nThis is obtained after the authors discuss and resolve any disagreements before comparing with the annotated labels.\nWe note that one drawback of this inference rule is that it does not predict causal relations and, therefore, cannot handle instances where end1 = start2 as our label definitions describe in \u00a73.1. We leave this problem for future research.\nWe release the systems for reproduction at http:// cogcomp.org/page/publication_view/937\nThis is virtually the same as using PTNTIME as MATRES does not evaluate duration nor end times.10 Wang et al. (2020)  is trained with two additional labels. We constraint the output space to only \"before\" and \"after\" using argmax, but this process makes it not directly comparable.\nAcknowledgments\nMaintaining knowledge about temporal intervals. James F Allen, Communications of the ACM. 2611James F Allen. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM, 26(11):832-843.\n\nAutomated Storytelling via Causal, Commonsense Plot Ordering. Prithviraj Ammanabrolu, Wesley Cheung, William Broniec, Mark O Riedl, AAAI. Prithviraj Ammanabrolu, Wesley Cheung, William Broniec, and Mark O Riedl. 2021. Automated Story- telling via Causal, Commonsense Plot Ordering. In AAAI.\n\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, D Klein, Neural module networks. CVPR. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and D. Klein. 2016. Neural module networks. CVPR.\n\nTimelines from Text: Identification of Syntactic Temporal Relations. Steven Bethard, James H Martin, Sara Klingenstein, ICSCSteven Bethard, James H. Martin, and Sara Klingen- stein. 2007. Timelines from Text: Identification of Syntactic Temporal Relations. ICSC.\n\nAbductive Commonsense Reasoning. Chandra Bhagavatula, Chaitanya Ronan Le Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Yih Wen-Tau, Yejin Choi, ICLR. Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han- nah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. 2020. Abductive Commonsense Reason- ing. In ICLR.\n\nAn Annotation Framework for Dense Event Ordering. Taylor Cassidy, Bill Mcdowell, Nathanel Chambers, Steven Bethard, ACL. Taylor Cassidy, Bill McDowell, Nathanel Chambers, and Steven Bethard. 2014. An Annotation Frame- work for Dense Event Ordering. In ACL.\n\nJoint Inference for Event Timeline Construction. Quang Do, Wei Lu, D Roth, EMNLP-CoNLL. Quang Do, Wei Lu, and D. Roth. 2012. Joint Infer- ence for Event Timeline Construction. In EMNLP- CoNLL.\n\nEvaluating Models' Local Decision Boundaries via Contrast Sets. Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Findings of EMNLP. Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, R. Tsarfaty, Eric Wallace, A. Zhang, and Ben ZhouMatt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel- son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, R. Tsarfaty, Eric Wallace, A. Zhang, and Ben Zhou. 2020. Evaluating Models' Local Decision Bound- aries via Contrast Sets. In Findings of EMNLP.\n\nAl-lenNLP: A Deep Semantic Natural Language Processing Platform. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F Liu, Matthew E Peters, M Schmitz, L Zettlemoyer, abs/1803.07640NLP-OSS. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew E. Peters, M. Schmitz, and L. Zettlemoyer. 2018. Al- lenNLP: A Deep Semantic Natural Language Pro- cessing Platform. NLP-OSS, abs/1803.07640.\n\nEfficient algorithms for qualitative reasoning about time. Alfonso Gerevini, Lenhart Schubert, Artificial intelligence. 742Alfonso Gerevini and Lenhart Schubert. 1995. Effi- cient algorithms for qualitative reasoning about time. Artificial intelligence, 74(2):207-248.\n\nNeural Module Networks for Reasoning over Text. Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, Matt Gardner, ICLR. Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2019. Neural Module Networks for Reasoning over Text. In ICLR.\n\nUsing Query Patterns to Learn the Duration of Events. Andrey Gusev, Nathanael Chambers, Divye Raj Khilnani, Pranav Khaitan, Steven Bethard, Dan Jurafsky, IWCS. Andrey Gusev, Nathanael Chambers, Divye Raj Khilnani, Pranav Khaitan, Steven Bethard, and Dan Jurafsky. 2011. Using Query Patterns to Learn the Duration of Events. In IWCS.\n\nJoint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction. Rujun Han, Qiang Ning, Nanyun Peng, EMNLP. Rujun Han, Qiang Ning, and Nanyun Peng. 2019. Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction. In EMNLP.\n\nQuestion Answering as Global Reasoning over Semantic Abstractions. Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Dan Roth, AAAI. Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and Dan Roth. 2018. Question Answering as Global Reasoning over Semantic Abstractions. In AAAI.\n\nText Modular Networks: Learning to Decompose Tasks in the Language of Existing Models. Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, Ashish Sabharwal, NAACL. Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2021. Text Modular Networks: Learning to Decompose Tasks in the Lan- guage of Existing Models. In NAACL.\n\nComplexity of Word Collocation Networks: A Preliminary Structural Analysis. Shibamouli Lahiri, Proceedings of ACL-SRW. ACL-SRWShibamouli Lahiri. 2014. Complexity of Word Collo- cation Networks: A Preliminary Structural Analysis. In Proceedings of ACL-SRW.\n\nNatural Language Inference from Multiple Premises. Alice Lai, Yonatan Bisk, Julia Hockenmaier, IJCNLP. Alice Lai, Yonatan Bisk, and Julia Hockenmaier. 2017. Natural Language Inference from Multiple Premises. IJCNLP.\n\nTemporal information extraction by predicting relative time-lines. A Leeuwenberg, Marie-Francine Moens, EMNLP. A. Leeuwenberg and Marie-Francine Moens. 2018. Temporal information extraction by predicting rela- tive time-lines. In EMNLP.\n\nA logic-driven framework for consistency of neural models. Tao Li, Vivek Gupta, Maitrey Mehta, V Srikumar, EMNLP. Tao Li, Vivek Gupta, Maitrey Mehta, and V. Srikumar. 2019. A logic-driven framework for consistency of neural models. In EMNLP.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, abs/1907.11692RoBERTa: A Robustly Optimized BERT Pretraining Approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretrain- ing Approach. ArXiv, abs/1907.11692.\n\nContextaware neural model for temporal information extraction. Yuanliang Meng, Anna Rumshisky, ACL. Yuanliang Meng and Anna Rumshisky. 2018. Context- aware neural model for temporal information extrac- tion. In ACL.\n\nStructured learning for temporal relation extraction from clinical records. Marie-Francine Moens, A Leeuwenberg, EACL. Marie-Francine Moens and A. Leeuwenberg. 2017. Structured learning for temporal relation extraction from clinical records. In EACL.\n\nA Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen, NAACL. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A Corpus and Evaluation Framework for Deeper Understand- ing of Commonsense Stories. In NAACL.\n\nAdversarial NLI: A New Benchmark for Natural Language Understanding. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, ACL. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Ad- versarial NLI: A New Benchmark for Natural Lan- guage Understanding. In ACL.\n\nA Structured Learning Approach to Temporal Relation Extraction. Qiang Ning, Z Feng, D Roth, EMNLP. Qiang Ning, Z. Feng, and D. Roth. 2017. A Structured Learning Approach to Temporal Relation Extraction. In EMNLP.\n\nImproving Temporal Relation Extraction with a Globally Acquired Statistical Resource. Qiang Ning, H Wu, H Peng, D Roth, NAACL. Qiang Ning, H. Wu, H. Peng, and D. Roth. 2018a. Im- proving Temporal Relation Extraction with a Glob- ally Acquired Statistical Resource. In NAACL.\n\nEasy, Reproducible and Quality-Controlled Data Collection with CROWDAQ. Qiang Ning, Hao Wu, Pradeep Dasigi, Dheeru Dua, Matt Gardner, Robert L Logan, I V , Zhen Nie, EMNLP. Qiang Ning, Hao Wu, Pradeep Dasigi, Dheeru Dua, Matt Gardner, Robert L. Logan IV, and Zhen Nie. 2020a. Easy, Reproducible and Quality-Controlled Data Collection with CROWDAQ. In EMNLP.\n\nTorque: A reading comprehension dataset of temporal ordering questions. Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt Gardner, Dan Roth, EMNLP. Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt Gardner, and Dan Roth. 2020b. Torque: A reading comprehension dataset of temporal ordering ques- tions. In EMNLP.\n\nA multiaxis annotation scheme for event temporal relations. Qiang Ning, Hao Wu, Dan Roth, ACL. Qiang Ning, Hao Wu, and Dan Roth. 2018b. A multi- axis annotation scheme for event temporal relations. In ACL.\n\nCogCompTime: A Tool for Understanding Time in Natural Language. Qiang Ning, Ben Zhou, Z Feng, H Peng, D Roth, EMNLP. Qiang Ning, Ben Zhou, Z. Feng, H. Peng, and D. Roth. 2018c. CogCompTime: A Tool for Understanding Time in Natural Language. In EMNLP.\n\nRicher event description: Integrating event coreference with temporal, causal and bridging annotation. Kristin Tim O&apos;gorman, Martha Wright-Bettner, Palmer, CNS. Tim O'Gorman, Kristin Wright-Bettner, and Martha Palmer. 2016. Richer event description: Integrating event coreference with temporal, causal and bridg- ing annotation. In CNS.\n\nExtending TimeML with Typical Durations of Events. Feng Pan, Rutu Mulkar-Mehta, Jerry R Hobbs, Proceedings of the Workshop on Annotating and Reasoning about Time and Events. the Workshop on Annotating and Reasoning about Time and EventsFeng Pan, Rutu Mulkar-Mehta, and Jerry R Hobbs. 2006. Extending TimeML with Typical Durations of Events. In Proceedings of the Workshop on Anno- tating and Reasoning about Time and Events.\n\nCausal inference in statistics: An overview. J Pearl, Statistics Surveys. 3J. Pearl. 2009. Causal inference in statistics: An overview. Statistics Surveys, 3:96-146.\n\nAdam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, Benjamin Van Durme, Hypothesis only Baselines in Natural Language Inference. Proceedings of *SEM. Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only Baselines in Natural Language In- ference. Proceedings of *SEM.\n\nTimeM: Robust Specification of Event and Temporal Expressions in Text. James Pustejovsky, M Jos\u00e9, Robert Castano, Roser Ingria, Sauri, J Robert, Andrea Gaizauskas, Graham Setzer, Katz, Dragomir R Radev, New Directions in Question Answering. James Pustejovsky, Jos\u00e9 M Castano, Robert Ingria, Roser Sauri, Robert J Gaizauskas, Andrea Set- zer, Graham Katz, and Dragomir R Radev. 2003. TimeM: Robust Specification of Event and Temporal Expressions in Text. In New Directions in Question Answering.\n\nExploring the Limits of Transfer Learning with a Unified Text-to-text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, JMLR. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Lim- its of Transfer Learning with a Unified Text-to-text Transformer. JMLR, 21(140):1-67.\n\nTemporal Anchoring of Events for the Timebank corpus. Nils Reimers, Nazanin Dehghani, Iryna Gurevych, ACL. Nils Reimers, Nazanin Dehghani, and Iryna Gurevych. 2016. Temporal Anchoring of Events for the Time- bank corpus. In ACL.\n\nThe Web as a Knowledge-base for Answering Complex Questions. Alon Talmor, Jonathan Berant, NAACL. Alon Talmor and Jonathan Berant. 2018. The Web as a Knowledge-base for Answering Complex Questions. In NAACL.\n\nReasoning about actions and state changes by injecting commonsense knowledge. Niket Tandon, Bhavana Dalvi, Joel Grus, Antoine Wen Tau Yih, Peter Bosselut, Clark, abs/1808.10012ArXiv. Niket Tandon, Bhavana Dalvi, Joel Grus, Wen tau Yih, Antoine Bosselut, and Peter Clark. 2018. Reason- ing about actions and state changes by injecting com- monsense knowledge. ArXiv, abs/1808.10012.\n\nA dataset for tracking entities in open domain procedural text. Niket Tandon, Keisuke Sakaguchi, Dheeraj Bhavana Dalvi Mishra, Peter Rajagopal, Michal Clark, Kyle Guerquin, Eduard Richardson, Hovy, EMNLP. Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi Mishra, Dheeraj Rajagopal, Peter Clark, Michal Guerquin, Kyle Richardson, and Eduard Hovy. 2020. A dataset for tracking entities in open domain proce- dural text. In EMNLP.\n\nTemporal Reasoning in Natural Language Inference. Siddharth Vashishtha, Adam Poliak, Yash Kumar Lal, Benjamin Van Durme, Aaron Steven White, Finding of EMNLP. Siddharth Vashishtha, Adam Poliak, Yash Kumar Lal, Benjamin Van Durme, and Aaron Steven White. 2020. Temporal Reasoning in Natural Language In- ference. In Finding of EMNLP.\n\nFine-grained Temporal Relation Extraction. Siddharth Vashishtha, Benjamin Van Durme, Aaron Steven White, ACL. Siddharth Vashishtha, Benjamin Van Durme, and Aaron Steven White. 2019. Fine-grained Temporal Relation Extraction. In ACL.\n\nJoint constrained learning for event-event relation extraction. Haoyu Wang, Muhao Chen, Hongming Zhang, Dan Roth, EMNLP. Haoyu Wang, Muhao Chen, Hongming Zhang, and Dan Roth. 2020. Joint constrained learning for event-event relation extraction. In EMNLP.\n\nA broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel R Bowman, NAACL. Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2017. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, arXiv:1910.03771Transformers: State-of-theart natural language processing. arXiv preprintThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Fun- towicz, et al. 2019. Transformers: State-of-the- art natural language processing. arXiv preprint arXiv:1910.03771.\n\nTomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, Jonathan Berant, Break it Down: A Question Understanding Benchmark. TACL. 8Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it Down: A Question Understanding Benchmark. TACL, 8:183-198.\n\ngoing on a vacation\" takes longer than \"going for a walk\": A study of temporal commonsense understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, D Roth, EMNLP. Ben Zhou, Daniel Khashabi, Qiang Ning, and D. Roth. 2019. \"going on a vacation\" takes longer than \"go- ing for a walk\": A study of temporal commonsense understanding. In EMNLP.\n\nTemporal Common Sense Acquisition with Minimal Supervision. Ben Zhou, Qiang Ning, Daniel Khashabi, Dan Roth, ACL. Ben Zhou, Qiang Ning, Daniel Khashabi, and Dan Roth. 2020. Temporal Common Sense Acquisition with Minimal Supervision. In ACL.\n", "annotations": {"author": "[{\"end\":152,\"start\":81},{\"end\":194,\"start\":153},{\"end\":223,\"start\":195},{\"end\":261,\"start\":224},{\"end\":330,\"start\":262},{\"end\":399,\"start\":331}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":85},{\"end\":168,\"start\":158},{\"end\":205,\"start\":201},{\"end\":235,\"start\":231},{\"end\":278,\"start\":269},{\"end\":339,\"start\":335}]", "author_first_name": "[{\"end\":84,\"start\":81},{\"end\":157,\"start\":153},{\"end\":200,\"start\":195},{\"end\":230,\"start\":224},{\"end\":268,\"start\":262},{\"end\":334,\"start\":331}]", "author_affiliation": "[{\"end\":114,\"start\":91},{\"end\":151,\"start\":116},{\"end\":193,\"start\":170},{\"end\":260,\"start\":237},{\"end\":329,\"start\":306},{\"end\":398,\"start\":363}]", "title": "[{\"end\":63,\"start\":1},{\"end\":462,\"start\":400}]", "venue": "[{\"end\":606,\"start\":464}]", "abstract": "[{\"end\":1887,\"start\":754}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4115,\"start\":4094},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4721,\"start\":4701},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5193,\"start\":5173},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5905,\"start\":5885},{\"end\":6449,\"start\":6448},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6510,\"start\":6490},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7284,\"start\":7258},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7305,\"start\":7284},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7326,\"start\":7305},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7347,\"start\":7326},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7369,\"start\":7347},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7387,\"start\":7369},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7407,\"start\":7387},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7456,\"start\":7438},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7474,\"start\":7456},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7663,\"start\":7646},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7691,\"start\":7663},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7719,\"start\":7691},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7744,\"start\":7719},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7763,\"start\":7744},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7780,\"start\":7763},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7827,\"start\":7802},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7965,\"start\":7942},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7982,\"start\":7965},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8007,\"start\":7982},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8104,\"start\":8080},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8185,\"start\":8165},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8204,\"start\":8185},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8332,\"start\":8307},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8460,\"start\":8441},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8476,\"start\":8460},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8534,\"start\":8512},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8601,\"start\":8576},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8623,\"start\":8601},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8639,\"start\":8623},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8660,\"start\":8639},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8678,\"start\":8660},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8864,\"start\":8851},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8892,\"start\":8864},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8954,\"start\":8926},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8980,\"start\":8956},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10163,\"start\":10151},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10281,\"start\":10261},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10303,\"start\":10281},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10685,\"start\":10667},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12600,\"start\":12580},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12744,\"start\":12717},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13245,\"start\":13223},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14259,\"start\":14241},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16286,\"start\":16269},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19254,\"start\":19234},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19398,\"start\":19384},{\"end\":19477,\"start\":19469},{\"end\":19539,\"start\":19531},{\"end\":19607,\"start\":19596},{\"end\":19712,\"start\":19702},{\"end\":19751,\"start\":19744},{\"end\":20015,\"start\":20005},{\"end\":20105,\"start\":20093},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20316,\"start\":20296},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21466,\"start\":21438},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21495,\"start\":21471},{\"end\":25113,\"start\":25106},{\"end\":25179,\"start\":25172},{\"end\":25282,\"start\":25275},{\"end\":25343,\"start\":25336},{\"end\":25628,\"start\":25620},{\"end\":25675,\"start\":25667},{\"end\":25729,\"start\":25718},{\"end\":25776,\"start\":25766},{\"end\":25907,\"start\":25900},{\"end\":27528,\"start\":27521},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28476,\"start\":28458},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29200,\"start\":29178},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29234,\"start\":29216},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32070,\"start\":32050},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32426,\"start\":32408},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32640,\"start\":32622},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33303,\"start\":33282},{\"end\":34321,\"start\":34319}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36129,\"start\":36046},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36334,\"start\":36130},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36556,\"start\":36335},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36707,\"start\":36557},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36937,\"start\":36708},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":36984,\"start\":36938},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":37391,\"start\":36985},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":37458,\"start\":37392},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":37614,\"start\":37459},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":37752,\"start\":37615}]", "paragraph": "[{\"end\":2642,\"start\":1903},{\"end\":3028,\"start\":2644},{\"end\":4338,\"start\":3040},{\"end\":5241,\"start\":4340},{\"end\":7103,\"start\":5243},{\"end\":8334,\"start\":7120},{\"end\":8982,\"start\":8336},{\"end\":9202,\"start\":9010},{\"end\":9283,\"start\":9204},{\"end\":9531,\"start\":9298},{\"end\":9812,\"start\":9533},{\"end\":10304,\"start\":9814},{\"end\":10378,\"start\":10327},{\"end\":11894,\"start\":10421},{\"end\":11973,\"start\":11896},{\"end\":12626,\"start\":11975},{\"end\":13174,\"start\":12656},{\"end\":13604,\"start\":13176},{\"end\":14041,\"start\":13606},{\"end\":14704,\"start\":14065},{\"end\":14928,\"start\":14706},{\"end\":15527,\"start\":14959},{\"end\":15745,\"start\":15562},{\"end\":15958,\"start\":15747},{\"end\":16940,\"start\":15960},{\"end\":18089,\"start\":16970},{\"end\":18987,\"start\":18091},{\"end\":20317,\"start\":18989},{\"end\":20982,\"start\":20374},{\"end\":21603,\"start\":21030},{\"end\":22141,\"start\":21619},{\"end\":23285,\"start\":22143},{\"end\":24438,\"start\":23287},{\"end\":24768,\"start\":24629},{\"end\":25459,\"start\":24792},{\"end\":26699,\"start\":25488},{\"end\":27325,\"start\":26770},{\"end\":28045,\"start\":27357},{\"end\":28281,\"start\":28047},{\"end\":28401,\"start\":28297},{\"end\":28896,\"start\":28427},{\"end\":29388,\"start\":28898},{\"end\":29649,\"start\":29413},{\"end\":31109,\"start\":29651},{\"end\":31890,\"start\":31126},{\"end\":32937,\"start\":31915},{\"end\":33753,\"start\":32971},{\"end\":34892,\"start\":33755},{\"end\":35327,\"start\":34894},{\"end\":36045,\"start\":35342}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":24628,\"start\":24509},{\"attributes\":{\"id\":\"formula_1\"},\"end\":26769,\"start\":26700},{\"attributes\":{\"id\":\"formula_2\"},\"end\":27356,\"start\":27326}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":6617,\"start\":6609},{\"end\":30154,\"start\":30147},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32471,\"start\":32464},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":33070,\"start\":33063},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":34280,\"start\":34273}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1901,\"start\":1889},{\"end\":3038,\"start\":3031},{\"attributes\":{\"n\":\"2\"},\"end\":7118,\"start\":7106},{\"end\":9008,\"start\":8985},{\"end\":9296,\"start\":9286},{\"attributes\":{\"n\":\"3\"},\"end\":10325,\"start\":10307},{\"attributes\":{\"n\":\"3.1\"},\"end\":10419,\"start\":10381},{\"end\":12654,\"start\":12629},{\"attributes\":{\"n\":\"3.2\"},\"end\":14063,\"start\":14044},{\"attributes\":{\"n\":\"4\"},\"end\":14957,\"start\":14931},{\"attributes\":{\"n\":\"4.1\"},\"end\":15560,\"start\":15530},{\"end\":16968,\"start\":16943},{\"end\":20331,\"start\":20320},{\"attributes\":{\"n\":\"4.2\"},\"end\":20372,\"start\":20334},{\"attributes\":{\"n\":\"5\"},\"end\":21028,\"start\":20985},{\"attributes\":{\"n\":\"5.1\"},\"end\":21617,\"start\":21606},{\"end\":24456,\"start\":24441},{\"end\":24508,\"start\":24459},{\"attributes\":{\"n\":\"5.2\"},\"end\":24790,\"start\":24771},{\"attributes\":{\"n\":\"5.3\"},\"end\":25486,\"start\":25462},{\"attributes\":{\"n\":\"6\"},\"end\":28295,\"start\":28284},{\"attributes\":{\"n\":\"6.1\"},\"end\":28425,\"start\":28404},{\"attributes\":{\"n\":\"6.2\"},\"end\":29411,\"start\":29391},{\"attributes\":{\"n\":\"6.3\"},\"end\":31124,\"start\":31112},{\"attributes\":{\"n\":\"6.4\"},\"end\":31913,\"start\":31893},{\"attributes\":{\"n\":\"6.5\"},\"end\":32969,\"start\":32940},{\"attributes\":{\"n\":\"7\"},\"end\":35340,\"start\":35330},{\"end\":36057,\"start\":36047},{\"end\":36141,\"start\":36131},{\"end\":36567,\"start\":36558},{\"end\":36716,\"start\":36709},{\"end\":36948,\"start\":36939},{\"end\":37469,\"start\":37460},{\"end\":37625,\"start\":37616}]", "table": "[{\"end\":36937,\"start\":36808},{\"end\":37458,\"start\":37427},{\"end\":37614,\"start\":37543}]", "figure_caption": "[{\"end\":36129,\"start\":36059},{\"end\":36334,\"start\":36143},{\"end\":36556,\"start\":36337},{\"end\":36707,\"start\":36569},{\"end\":36808,\"start\":36718},{\"end\":36984,\"start\":36950},{\"end\":37391,\"start\":36987},{\"end\":37427,\"start\":37394},{\"end\":37543,\"start\":37471},{\"end\":37752,\"start\":37627}]", "figure_ref": "[{\"end\":2141,\"start\":2135},{\"end\":2872,\"start\":2864},{\"end\":4047,\"start\":4041},{\"end\":5568,\"start\":5562},{\"end\":9903,\"start\":9895},{\"end\":11450,\"start\":11444},{\"end\":11746,\"start\":11738},{\"end\":11972,\"start\":11966},{\"end\":12228,\"start\":12222},{\"end\":12333,\"start\":12328},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16722,\"start\":16716},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17986,\"start\":17980},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18287,\"start\":18280},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18764,\"start\":18758},{\"end\":22044,\"start\":22036},{\"end\":22352,\"start\":22344},{\"end\":23087,\"start\":23081},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24766,\"start\":24760},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27778,\"start\":27772}]", "bib_author_first_name": "[{\"end\":38953,\"start\":38943},{\"end\":38973,\"start\":38967},{\"end\":38989,\"start\":38982},{\"end\":39003,\"start\":38999},{\"end\":39005,\"start\":39004},{\"end\":39178,\"start\":39173},{\"end\":39194,\"start\":39188},{\"end\":39211,\"start\":39205},{\"end\":39222,\"start\":39221},{\"end\":39434,\"start\":39428},{\"end\":39449,\"start\":39444},{\"end\":39451,\"start\":39450},{\"end\":39464,\"start\":39460},{\"end\":39663,\"start\":39656},{\"end\":39686,\"start\":39677},{\"end\":39709,\"start\":39702},{\"end\":39723,\"start\":39720},{\"end\":39741,\"start\":39735},{\"end\":39756,\"start\":39752},{\"end\":39771,\"start\":39766},{\"end\":39783,\"start\":39780},{\"end\":39798,\"start\":39793},{\"end\":40073,\"start\":40067},{\"end\":40087,\"start\":40083},{\"end\":40106,\"start\":40098},{\"end\":40123,\"start\":40117},{\"end\":40329,\"start\":40324},{\"end\":40337,\"start\":40334},{\"end\":40343,\"start\":40342},{\"end\":40537,\"start\":40533},{\"end\":40551,\"start\":40547},{\"end\":40567,\"start\":40559},{\"end\":40585,\"start\":40577},{\"end\":40597,\"start\":40594},{\"end\":40610,\"start\":40605},{\"end\":40624,\"start\":40617},{\"end\":40639,\"start\":40633},{\"end\":40650,\"start\":40645},{\"end\":40665,\"start\":40659},{\"end\":40686,\"start\":40680},{\"end\":40699,\"start\":40694},{\"end\":40719,\"start\":40712},{\"end\":40735,\"start\":40729},{\"end\":41482,\"start\":41478},{\"end\":41496,\"start\":41492},{\"end\":41507,\"start\":41503},{\"end\":41523,\"start\":41517},{\"end\":41540,\"start\":41533},{\"end\":41555,\"start\":41549},{\"end\":41557,\"start\":41556},{\"end\":41570,\"start\":41563},{\"end\":41572,\"start\":41571},{\"end\":41582,\"start\":41581},{\"end\":41593,\"start\":41592},{\"end\":41933,\"start\":41926},{\"end\":41951,\"start\":41944},{\"end\":42191,\"start\":42185},{\"end\":42204,\"start\":42199},{\"end\":42213,\"start\":42210},{\"end\":42226,\"start\":42220},{\"end\":42238,\"start\":42234},{\"end\":42445,\"start\":42439},{\"end\":42462,\"start\":42453},{\"end\":42478,\"start\":42473},{\"end\":42482,\"start\":42479},{\"end\":42499,\"start\":42493},{\"end\":42515,\"start\":42509},{\"end\":42528,\"start\":42525},{\"end\":42824,\"start\":42819},{\"end\":42835,\"start\":42830},{\"end\":42848,\"start\":42842},{\"end\":43092,\"start\":43086},{\"end\":43109,\"start\":43103},{\"end\":43122,\"start\":43116},{\"end\":43137,\"start\":43134},{\"end\":43388,\"start\":43382},{\"end\":43401,\"start\":43395},{\"end\":43416,\"start\":43412},{\"end\":43434,\"start\":43429},{\"end\":43448,\"start\":43442},{\"end\":43741,\"start\":43731},{\"end\":43968,\"start\":43963},{\"end\":43981,\"start\":43974},{\"end\":43993,\"start\":43988},{\"end\":44197,\"start\":44196},{\"end\":44225,\"start\":44211},{\"end\":44429,\"start\":44426},{\"end\":44439,\"start\":44434},{\"end\":44454,\"start\":44447},{\"end\":44463,\"start\":44462},{\"end\":44616,\"start\":44610},{\"end\":44626,\"start\":44622},{\"end\":44637,\"start\":44632},{\"end\":44652,\"start\":44645},{\"end\":44663,\"start\":44657},{\"end\":44676,\"start\":44671},{\"end\":44687,\"start\":44683},{\"end\":44698,\"start\":44694},{\"end\":44710,\"start\":44706},{\"end\":44731,\"start\":44724},{\"end\":45112,\"start\":45103},{\"end\":45123,\"start\":45119},{\"end\":45347,\"start\":45333},{\"end\":45356,\"start\":45355},{\"end\":45598,\"start\":45592},{\"end\":45622,\"start\":45613},{\"end\":45641,\"start\":45633},{\"end\":45650,\"start\":45646},{\"end\":45664,\"start\":45659},{\"end\":45676,\"start\":45672},{\"end\":45698,\"start\":45690},{\"end\":45711,\"start\":45706},{\"end\":46033,\"start\":46028},{\"end\":46044,\"start\":46039},{\"end\":46060,\"start\":46055},{\"end\":46073,\"start\":46068},{\"end\":46087,\"start\":46082},{\"end\":46101,\"start\":46096},{\"end\":46356,\"start\":46351},{\"end\":46364,\"start\":46363},{\"end\":46372,\"start\":46371},{\"end\":46592,\"start\":46587},{\"end\":46600,\"start\":46599},{\"end\":46606,\"start\":46605},{\"end\":46614,\"start\":46613},{\"end\":46854,\"start\":46849},{\"end\":46864,\"start\":46861},{\"end\":46876,\"start\":46869},{\"end\":46891,\"start\":46885},{\"end\":46901,\"start\":46897},{\"end\":46917,\"start\":46911},{\"end\":46919,\"start\":46918},{\"end\":46928,\"start\":46927},{\"end\":46930,\"start\":46929},{\"end\":46937,\"start\":46933},{\"end\":47213,\"start\":47208},{\"end\":47223,\"start\":47220},{\"end\":47233,\"start\":47228},{\"end\":47245,\"start\":47239},{\"end\":47256,\"start\":47252},{\"end\":47269,\"start\":47266},{\"end\":47512,\"start\":47507},{\"end\":47522,\"start\":47519},{\"end\":47530,\"start\":47527},{\"end\":47723,\"start\":47718},{\"end\":47733,\"start\":47730},{\"end\":47741,\"start\":47740},{\"end\":47749,\"start\":47748},{\"end\":47757,\"start\":47756},{\"end\":48016,\"start\":48009},{\"end\":48042,\"start\":48036},{\"end\":48304,\"start\":48300},{\"end\":48314,\"start\":48310},{\"end\":48334,\"start\":48329},{\"end\":48336,\"start\":48335},{\"end\":48721,\"start\":48720},{\"end\":48846,\"start\":48842},{\"end\":48860,\"start\":48855},{\"end\":48882,\"start\":48873},{\"end\":48897,\"start\":48891},{\"end\":48916,\"start\":48908},{\"end\":49259,\"start\":49254},{\"end\":49274,\"start\":49273},{\"end\":49287,\"start\":49281},{\"end\":49302,\"start\":49297},{\"end\":49319,\"start\":49318},{\"end\":49334,\"start\":49328},{\"end\":49353,\"start\":49347},{\"end\":49767,\"start\":49762},{\"end\":49780,\"start\":49776},{\"end\":49794,\"start\":49790},{\"end\":49813,\"start\":49804},{\"end\":49825,\"start\":49819},{\"end\":49841,\"start\":49834},{\"end\":49855,\"start\":49850},{\"end\":49865,\"start\":49862},{\"end\":49877,\"start\":49870},{\"end\":50189,\"start\":50185},{\"end\":50206,\"start\":50199},{\"end\":50222,\"start\":50217},{\"end\":50426,\"start\":50422},{\"end\":50443,\"start\":50435},{\"end\":50653,\"start\":50648},{\"end\":50669,\"start\":50662},{\"end\":50681,\"start\":50677},{\"end\":50695,\"start\":50688},{\"end\":50714,\"start\":50709},{\"end\":51022,\"start\":51017},{\"end\":51038,\"start\":51031},{\"end\":51057,\"start\":51050},{\"end\":51085,\"start\":51080},{\"end\":51103,\"start\":51097},{\"end\":51115,\"start\":51111},{\"end\":51132,\"start\":51126},{\"end\":51438,\"start\":51429},{\"end\":51455,\"start\":51451},{\"end\":51468,\"start\":51464},{\"end\":51488,\"start\":51480},{\"end\":51505,\"start\":51500},{\"end\":51512,\"start\":51506},{\"end\":51765,\"start\":51756},{\"end\":51786,\"start\":51778},{\"end\":51803,\"start\":51798},{\"end\":51810,\"start\":51804},{\"end\":52016,\"start\":52011},{\"end\":52028,\"start\":52023},{\"end\":52043,\"start\":52035},{\"end\":52054,\"start\":52051},{\"end\":52288,\"start\":52283},{\"end\":52305,\"start\":52299},{\"end\":52320,\"start\":52314},{\"end\":52322,\"start\":52321},{\"end\":52495,\"start\":52489},{\"end\":52510,\"start\":52502},{\"end\":52524,\"start\":52518},{\"end\":52537,\"start\":52531},{\"end\":52555,\"start\":52548},{\"end\":52573,\"start\":52566},{\"end\":52586,\"start\":52579},{\"end\":52598,\"start\":52595},{\"end\":52610,\"start\":52606},{\"end\":52623,\"start\":52617},{\"end\":52987,\"start\":52982},{\"end\":53000,\"start\":52997},{\"end\":53012,\"start\":53007},{\"end\":53024,\"start\":53020},{\"end\":53038,\"start\":53034},{\"end\":53055,\"start\":53049},{\"end\":53072,\"start\":53064},{\"end\":53426,\"start\":53423},{\"end\":53439,\"start\":53433},{\"end\":53455,\"start\":53450},{\"end\":53463,\"start\":53462},{\"end\":53718,\"start\":53715},{\"end\":53730,\"start\":53725},{\"end\":53743,\"start\":53737},{\"end\":53757,\"start\":53754}]", "bib_author_last_name": "[{\"end\":38735,\"start\":38722},{\"end\":38965,\"start\":38954},{\"end\":38980,\"start\":38974},{\"end\":38997,\"start\":38990},{\"end\":39011,\"start\":39006},{\"end\":39186,\"start\":39179},{\"end\":39203,\"start\":39195},{\"end\":39219,\"start\":39212},{\"end\":39228,\"start\":39223},{\"end\":39442,\"start\":39435},{\"end\":39458,\"start\":39452},{\"end\":39477,\"start\":39465},{\"end\":39675,\"start\":39664},{\"end\":39700,\"start\":39687},{\"end\":39718,\"start\":39710},{\"end\":39733,\"start\":39724},{\"end\":39750,\"start\":39742},{\"end\":39764,\"start\":39757},{\"end\":39778,\"start\":39772},{\"end\":39791,\"start\":39784},{\"end\":39803,\"start\":39799},{\"end\":40081,\"start\":40074},{\"end\":40096,\"start\":40088},{\"end\":40115,\"start\":40107},{\"end\":40131,\"start\":40124},{\"end\":40332,\"start\":40330},{\"end\":40340,\"start\":40338},{\"end\":40348,\"start\":40344},{\"end\":40545,\"start\":40538},{\"end\":40557,\"start\":40552},{\"end\":40575,\"start\":40568},{\"end\":40592,\"start\":40586},{\"end\":40603,\"start\":40598},{\"end\":40615,\"start\":40611},{\"end\":40631,\"start\":40625},{\"end\":40643,\"start\":40640},{\"end\":40657,\"start\":40651},{\"end\":40678,\"start\":40666},{\"end\":40692,\"start\":40687},{\"end\":40710,\"start\":40700},{\"end\":40727,\"start\":40720},{\"end\":40744,\"start\":40736},{\"end\":41490,\"start\":41483},{\"end\":41501,\"start\":41497},{\"end\":41515,\"start\":41508},{\"end\":41531,\"start\":41524},{\"end\":41547,\"start\":41541},{\"end\":41561,\"start\":41558},{\"end\":41579,\"start\":41573},{\"end\":41590,\"start\":41583},{\"end\":41605,\"start\":41594},{\"end\":41942,\"start\":41934},{\"end\":41960,\"start\":41952},{\"end\":42197,\"start\":42192},{\"end\":42208,\"start\":42205},{\"end\":42218,\"start\":42214},{\"end\":42232,\"start\":42227},{\"end\":42246,\"start\":42239},{\"end\":42451,\"start\":42446},{\"end\":42471,\"start\":42463},{\"end\":42491,\"start\":42483},{\"end\":42507,\"start\":42500},{\"end\":42523,\"start\":42516},{\"end\":42537,\"start\":42529},{\"end\":42828,\"start\":42825},{\"end\":42840,\"start\":42836},{\"end\":42853,\"start\":42849},{\"end\":43101,\"start\":43093},{\"end\":43114,\"start\":43110},{\"end\":43132,\"start\":43123},{\"end\":43142,\"start\":43138},{\"end\":43393,\"start\":43389},{\"end\":43410,\"start\":43402},{\"end\":43427,\"start\":43417},{\"end\":43440,\"start\":43435},{\"end\":43458,\"start\":43449},{\"end\":43748,\"start\":43742},{\"end\":43972,\"start\":43969},{\"end\":43986,\"start\":43982},{\"end\":44005,\"start\":43994},{\"end\":44209,\"start\":44198},{\"end\":44231,\"start\":44226},{\"end\":44432,\"start\":44430},{\"end\":44445,\"start\":44440},{\"end\":44460,\"start\":44455},{\"end\":44472,\"start\":44464},{\"end\":44620,\"start\":44617},{\"end\":44630,\"start\":44627},{\"end\":44643,\"start\":44638},{\"end\":44655,\"start\":44653},{\"end\":44669,\"start\":44664},{\"end\":44681,\"start\":44677},{\"end\":44692,\"start\":44688},{\"end\":44704,\"start\":44699},{\"end\":44722,\"start\":44711},{\"end\":44740,\"start\":44732},{\"end\":45117,\"start\":45113},{\"end\":45133,\"start\":45124},{\"end\":45353,\"start\":45348},{\"end\":45368,\"start\":45357},{\"end\":45611,\"start\":45599},{\"end\":45631,\"start\":45623},{\"end\":45644,\"start\":45642},{\"end\":45657,\"start\":45651},{\"end\":45670,\"start\":45665},{\"end\":45688,\"start\":45677},{\"end\":45704,\"start\":45699},{\"end\":45717,\"start\":45712},{\"end\":46037,\"start\":46034},{\"end\":46053,\"start\":46045},{\"end\":46066,\"start\":46061},{\"end\":46080,\"start\":46074},{\"end\":46094,\"start\":46088},{\"end\":46107,\"start\":46102},{\"end\":46361,\"start\":46357},{\"end\":46369,\"start\":46365},{\"end\":46377,\"start\":46373},{\"end\":46597,\"start\":46593},{\"end\":46603,\"start\":46601},{\"end\":46611,\"start\":46607},{\"end\":46619,\"start\":46615},{\"end\":46859,\"start\":46855},{\"end\":46867,\"start\":46865},{\"end\":46883,\"start\":46877},{\"end\":46895,\"start\":46892},{\"end\":46909,\"start\":46902},{\"end\":46925,\"start\":46920},{\"end\":46941,\"start\":46938},{\"end\":47218,\"start\":47214},{\"end\":47226,\"start\":47224},{\"end\":47237,\"start\":47234},{\"end\":47250,\"start\":47246},{\"end\":47264,\"start\":47257},{\"end\":47274,\"start\":47270},{\"end\":47517,\"start\":47513},{\"end\":47525,\"start\":47523},{\"end\":47535,\"start\":47531},{\"end\":47728,\"start\":47724},{\"end\":47738,\"start\":47734},{\"end\":47746,\"start\":47742},{\"end\":47754,\"start\":47750},{\"end\":47762,\"start\":47758},{\"end\":48034,\"start\":48017},{\"end\":48057,\"start\":48043},{\"end\":48065,\"start\":48059},{\"end\":48308,\"start\":48305},{\"end\":48327,\"start\":48315},{\"end\":48342,\"start\":48337},{\"end\":48727,\"start\":48722},{\"end\":48853,\"start\":48847},{\"end\":48871,\"start\":48861},{\"end\":48889,\"start\":48883},{\"end\":48906,\"start\":48898},{\"end\":48926,\"start\":48917},{\"end\":49271,\"start\":49260},{\"end\":49279,\"start\":49275},{\"end\":49295,\"start\":49288},{\"end\":49309,\"start\":49303},{\"end\":49316,\"start\":49311},{\"end\":49326,\"start\":49320},{\"end\":49345,\"start\":49335},{\"end\":49360,\"start\":49354},{\"end\":49366,\"start\":49362},{\"end\":49384,\"start\":49368},{\"end\":49774,\"start\":49768},{\"end\":49788,\"start\":49781},{\"end\":49802,\"start\":49795},{\"end\":49817,\"start\":49814},{\"end\":49832,\"start\":49826},{\"end\":49848,\"start\":49842},{\"end\":49860,\"start\":49856},{\"end\":49868,\"start\":49866},{\"end\":49881,\"start\":49878},{\"end\":50197,\"start\":50190},{\"end\":50215,\"start\":50207},{\"end\":50231,\"start\":50223},{\"end\":50433,\"start\":50427},{\"end\":50450,\"start\":50444},{\"end\":50660,\"start\":50654},{\"end\":50675,\"start\":50670},{\"end\":50686,\"start\":50682},{\"end\":50707,\"start\":50696},{\"end\":50723,\"start\":50715},{\"end\":50730,\"start\":50725},{\"end\":51029,\"start\":51023},{\"end\":51048,\"start\":51039},{\"end\":51078,\"start\":51058},{\"end\":51095,\"start\":51086},{\"end\":51109,\"start\":51104},{\"end\":51124,\"start\":51116},{\"end\":51143,\"start\":51133},{\"end\":51149,\"start\":51145},{\"end\":51449,\"start\":51439},{\"end\":51462,\"start\":51456},{\"end\":51478,\"start\":51469},{\"end\":51498,\"start\":51489},{\"end\":51518,\"start\":51513},{\"end\":51776,\"start\":51766},{\"end\":51796,\"start\":51787},{\"end\":51816,\"start\":51811},{\"end\":52021,\"start\":52017},{\"end\":52033,\"start\":52029},{\"end\":52049,\"start\":52044},{\"end\":52059,\"start\":52055},{\"end\":52297,\"start\":52289},{\"end\":52312,\"start\":52306},{\"end\":52329,\"start\":52323},{\"end\":52500,\"start\":52496},{\"end\":52516,\"start\":52511},{\"end\":52529,\"start\":52525},{\"end\":52546,\"start\":52538},{\"end\":52564,\"start\":52556},{\"end\":52577,\"start\":52574},{\"end\":52593,\"start\":52587},{\"end\":52604,\"start\":52599},{\"end\":52615,\"start\":52611},{\"end\":52633,\"start\":52624},{\"end\":52995,\"start\":52988},{\"end\":53005,\"start\":53001},{\"end\":53018,\"start\":53013},{\"end\":53032,\"start\":53025},{\"end\":53047,\"start\":53039},{\"end\":53062,\"start\":53056},{\"end\":53079,\"start\":53073},{\"end\":53431,\"start\":53427},{\"end\":53448,\"start\":53440},{\"end\":53460,\"start\":53456},{\"end\":53468,\"start\":53464},{\"end\":53723,\"start\":53719},{\"end\":53735,\"start\":53731},{\"end\":53752,\"start\":53744},{\"end\":53762,\"start\":53758}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16729000},\"end\":38879,\"start\":38674},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":221446544},\"end\":39171,\"start\":38881},{\"attributes\":{\"id\":\"b2\"},\"end\":39357,\"start\":39173},{\"attributes\":{\"id\":\"b3\"},\"end\":39621,\"start\":39359},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":201058651},\"end\":40015,\"start\":39623},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7294125},\"end\":40273,\"start\":40017},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7359050},\"end\":40467,\"start\":40275},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":222124366},\"end\":41411,\"start\":40469},{\"attributes\":{\"doi\":\"abs/1803.07640\",\"id\":\"b8\",\"matched_paper_id\":3994096},\"end\":41865,\"start\":41413},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5100655},\"end\":42135,\"start\":41867},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":209202200},\"end\":42383,\"start\":42137},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":293098},\"end\":42717,\"start\":42385},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":202565622},\"end\":43017,\"start\":42719},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":20638934},\"end\":43293,\"start\":43019},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":221448158},\"end\":43653,\"start\":43295},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8913567},\"end\":43910,\"start\":43655},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":29033327},\"end\":44127,\"start\":43912},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52111780},\"end\":44365,\"start\":44129},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":202539612},\"end\":44608,\"start\":44367},{\"attributes\":{\"doi\":\"abs/1907.11692\",\"id\":\"b19\"},\"end\":45038,\"start\":44610},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":44161166},\"end\":45255,\"start\":45040},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":17894632},\"end\":45507,\"start\":45257},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15337246},\"end\":45957,\"start\":45509},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":207756753},\"end\":46285,\"start\":45959},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":28982109},\"end\":46499,\"start\":46287},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4957206},\"end\":46775,\"start\":46501},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":222341636},\"end\":47134,\"start\":46777},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":218470560},\"end\":47445,\"start\":47136},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5066019},\"end\":47652,\"start\":47447},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53223504},\"end\":47904,\"start\":47654},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15139323},\"end\":48247,\"start\":47906},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2896894},\"end\":48673,\"start\":48249},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":355118},\"end\":48840,\"start\":48675},{\"attributes\":{\"id\":\"b33\"},\"end\":49181,\"start\":48842},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":341734},\"end\":49677,\"start\":49183},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":204838007},\"end\":50129,\"start\":49679},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6677927},\"end\":50359,\"start\":50131},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3986974},\"end\":50568,\"start\":50361},{\"attributes\":{\"doi\":\"abs/1808.10012\",\"id\":\"b38\",\"matched_paper_id\":52136770},\"end\":50951,\"start\":50570},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":226262266},\"end\":51377,\"start\":50953},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":226283745},\"end\":51711,\"start\":51379},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":59599681},\"end\":51945,\"start\":51713},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":222306079},\"end\":52201,\"start\":51947},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":3432876},\"end\":52487,\"start\":52203},{\"attributes\":{\"doi\":\"arXiv:1910.03771\",\"id\":\"b44\"},\"end\":52980,\"start\":52489},{\"attributes\":{\"id\":\"b45\"},\"end\":53315,\"start\":52982},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":202541184},\"end\":53653,\"start\":53317},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":218581125},\"end\":53895,\"start\":53655}]", "bib_title": "[{\"end\":38720,\"start\":38674},{\"end\":38941,\"start\":38881},{\"end\":39654,\"start\":39623},{\"end\":40065,\"start\":40017},{\"end\":40322,\"start\":40275},{\"end\":40531,\"start\":40469},{\"end\":41476,\"start\":41413},{\"end\":41924,\"start\":41867},{\"end\":42183,\"start\":42137},{\"end\":42437,\"start\":42385},{\"end\":42817,\"start\":42719},{\"end\":43084,\"start\":43019},{\"end\":43380,\"start\":43295},{\"end\":43729,\"start\":43655},{\"end\":43961,\"start\":43912},{\"end\":44194,\"start\":44129},{\"end\":44424,\"start\":44367},{\"end\":45101,\"start\":45040},{\"end\":45331,\"start\":45257},{\"end\":45590,\"start\":45509},{\"end\":46026,\"start\":45959},{\"end\":46349,\"start\":46287},{\"end\":46585,\"start\":46501},{\"end\":46847,\"start\":46777},{\"end\":47206,\"start\":47136},{\"end\":47505,\"start\":47447},{\"end\":47716,\"start\":47654},{\"end\":48007,\"start\":47906},{\"end\":48298,\"start\":48249},{\"end\":48718,\"start\":48675},{\"end\":49252,\"start\":49183},{\"end\":49760,\"start\":49679},{\"end\":50183,\"start\":50131},{\"end\":50420,\"start\":50361},{\"end\":50646,\"start\":50570},{\"end\":51015,\"start\":50953},{\"end\":51427,\"start\":51379},{\"end\":51754,\"start\":51713},{\"end\":52009,\"start\":51947},{\"end\":52281,\"start\":52203},{\"end\":53421,\"start\":53317},{\"end\":53713,\"start\":53655}]", "bib_author": "[{\"end\":38737,\"start\":38722},{\"end\":38967,\"start\":38943},{\"end\":38982,\"start\":38967},{\"end\":38999,\"start\":38982},{\"end\":39013,\"start\":38999},{\"end\":39188,\"start\":39173},{\"end\":39205,\"start\":39188},{\"end\":39221,\"start\":39205},{\"end\":39230,\"start\":39221},{\"end\":39444,\"start\":39428},{\"end\":39460,\"start\":39444},{\"end\":39479,\"start\":39460},{\"end\":39677,\"start\":39656},{\"end\":39702,\"start\":39677},{\"end\":39720,\"start\":39702},{\"end\":39735,\"start\":39720},{\"end\":39752,\"start\":39735},{\"end\":39766,\"start\":39752},{\"end\":39780,\"start\":39766},{\"end\":39793,\"start\":39780},{\"end\":39805,\"start\":39793},{\"end\":40083,\"start\":40067},{\"end\":40098,\"start\":40083},{\"end\":40117,\"start\":40098},{\"end\":40133,\"start\":40117},{\"end\":40334,\"start\":40324},{\"end\":40342,\"start\":40334},{\"end\":40350,\"start\":40342},{\"end\":40547,\"start\":40533},{\"end\":40559,\"start\":40547},{\"end\":40577,\"start\":40559},{\"end\":40594,\"start\":40577},{\"end\":40605,\"start\":40594},{\"end\":40617,\"start\":40605},{\"end\":40633,\"start\":40617},{\"end\":40645,\"start\":40633},{\"end\":40659,\"start\":40645},{\"end\":40680,\"start\":40659},{\"end\":40694,\"start\":40680},{\"end\":40712,\"start\":40694},{\"end\":40729,\"start\":40712},{\"end\":40746,\"start\":40729},{\"end\":41492,\"start\":41478},{\"end\":41503,\"start\":41492},{\"end\":41517,\"start\":41503},{\"end\":41533,\"start\":41517},{\"end\":41549,\"start\":41533},{\"end\":41563,\"start\":41549},{\"end\":41581,\"start\":41563},{\"end\":41592,\"start\":41581},{\"end\":41607,\"start\":41592},{\"end\":41944,\"start\":41926},{\"end\":41962,\"start\":41944},{\"end\":42199,\"start\":42185},{\"end\":42210,\"start\":42199},{\"end\":42220,\"start\":42210},{\"end\":42234,\"start\":42220},{\"end\":42248,\"start\":42234},{\"end\":42453,\"start\":42439},{\"end\":42473,\"start\":42453},{\"end\":42493,\"start\":42473},{\"end\":42509,\"start\":42493},{\"end\":42525,\"start\":42509},{\"end\":42539,\"start\":42525},{\"end\":42830,\"start\":42819},{\"end\":42842,\"start\":42830},{\"end\":42855,\"start\":42842},{\"end\":43103,\"start\":43086},{\"end\":43116,\"start\":43103},{\"end\":43134,\"start\":43116},{\"end\":43144,\"start\":43134},{\"end\":43395,\"start\":43382},{\"end\":43412,\"start\":43395},{\"end\":43429,\"start\":43412},{\"end\":43442,\"start\":43429},{\"end\":43460,\"start\":43442},{\"end\":43750,\"start\":43731},{\"end\":43974,\"start\":43963},{\"end\":43988,\"start\":43974},{\"end\":44007,\"start\":43988},{\"end\":44211,\"start\":44196},{\"end\":44233,\"start\":44211},{\"end\":44434,\"start\":44426},{\"end\":44447,\"start\":44434},{\"end\":44462,\"start\":44447},{\"end\":44474,\"start\":44462},{\"end\":44622,\"start\":44610},{\"end\":44632,\"start\":44622},{\"end\":44645,\"start\":44632},{\"end\":44657,\"start\":44645},{\"end\":44671,\"start\":44657},{\"end\":44683,\"start\":44671},{\"end\":44694,\"start\":44683},{\"end\":44706,\"start\":44694},{\"end\":44724,\"start\":44706},{\"end\":44742,\"start\":44724},{\"end\":45119,\"start\":45103},{\"end\":45135,\"start\":45119},{\"end\":45355,\"start\":45333},{\"end\":45370,\"start\":45355},{\"end\":45613,\"start\":45592},{\"end\":45633,\"start\":45613},{\"end\":45646,\"start\":45633},{\"end\":45659,\"start\":45646},{\"end\":45672,\"start\":45659},{\"end\":45690,\"start\":45672},{\"end\":45706,\"start\":45690},{\"end\":45719,\"start\":45706},{\"end\":46039,\"start\":46028},{\"end\":46055,\"start\":46039},{\"end\":46068,\"start\":46055},{\"end\":46082,\"start\":46068},{\"end\":46096,\"start\":46082},{\"end\":46109,\"start\":46096},{\"end\":46363,\"start\":46351},{\"end\":46371,\"start\":46363},{\"end\":46379,\"start\":46371},{\"end\":46599,\"start\":46587},{\"end\":46605,\"start\":46599},{\"end\":46613,\"start\":46605},{\"end\":46621,\"start\":46613},{\"end\":46861,\"start\":46849},{\"end\":46869,\"start\":46861},{\"end\":46885,\"start\":46869},{\"end\":46897,\"start\":46885},{\"end\":46911,\"start\":46897},{\"end\":46927,\"start\":46911},{\"end\":46933,\"start\":46927},{\"end\":46943,\"start\":46933},{\"end\":47220,\"start\":47208},{\"end\":47228,\"start\":47220},{\"end\":47239,\"start\":47228},{\"end\":47252,\"start\":47239},{\"end\":47266,\"start\":47252},{\"end\":47276,\"start\":47266},{\"end\":47519,\"start\":47507},{\"end\":47527,\"start\":47519},{\"end\":47537,\"start\":47527},{\"end\":47730,\"start\":47718},{\"end\":47740,\"start\":47730},{\"end\":47748,\"start\":47740},{\"end\":47756,\"start\":47748},{\"end\":47764,\"start\":47756},{\"end\":48036,\"start\":48009},{\"end\":48059,\"start\":48036},{\"end\":48067,\"start\":48059},{\"end\":48310,\"start\":48300},{\"end\":48329,\"start\":48310},{\"end\":48344,\"start\":48329},{\"end\":48729,\"start\":48720},{\"end\":48855,\"start\":48842},{\"end\":48873,\"start\":48855},{\"end\":48891,\"start\":48873},{\"end\":48908,\"start\":48891},{\"end\":48928,\"start\":48908},{\"end\":49273,\"start\":49254},{\"end\":49281,\"start\":49273},{\"end\":49297,\"start\":49281},{\"end\":49311,\"start\":49297},{\"end\":49318,\"start\":49311},{\"end\":49328,\"start\":49318},{\"end\":49347,\"start\":49328},{\"end\":49362,\"start\":49347},{\"end\":49368,\"start\":49362},{\"end\":49386,\"start\":49368},{\"end\":49776,\"start\":49762},{\"end\":49790,\"start\":49776},{\"end\":49804,\"start\":49790},{\"end\":49819,\"start\":49804},{\"end\":49834,\"start\":49819},{\"end\":49850,\"start\":49834},{\"end\":49862,\"start\":49850},{\"end\":49870,\"start\":49862},{\"end\":49883,\"start\":49870},{\"end\":50199,\"start\":50185},{\"end\":50217,\"start\":50199},{\"end\":50233,\"start\":50217},{\"end\":50435,\"start\":50422},{\"end\":50452,\"start\":50435},{\"end\":50662,\"start\":50648},{\"end\":50677,\"start\":50662},{\"end\":50688,\"start\":50677},{\"end\":50709,\"start\":50688},{\"end\":50725,\"start\":50709},{\"end\":50732,\"start\":50725},{\"end\":51031,\"start\":51017},{\"end\":51050,\"start\":51031},{\"end\":51080,\"start\":51050},{\"end\":51097,\"start\":51080},{\"end\":51111,\"start\":51097},{\"end\":51126,\"start\":51111},{\"end\":51145,\"start\":51126},{\"end\":51151,\"start\":51145},{\"end\":51451,\"start\":51429},{\"end\":51464,\"start\":51451},{\"end\":51480,\"start\":51464},{\"end\":51500,\"start\":51480},{\"end\":51520,\"start\":51500},{\"end\":51778,\"start\":51756},{\"end\":51798,\"start\":51778},{\"end\":51818,\"start\":51798},{\"end\":52023,\"start\":52011},{\"end\":52035,\"start\":52023},{\"end\":52051,\"start\":52035},{\"end\":52061,\"start\":52051},{\"end\":52299,\"start\":52283},{\"end\":52314,\"start\":52299},{\"end\":52331,\"start\":52314},{\"end\":52502,\"start\":52489},{\"end\":52518,\"start\":52502},{\"end\":52531,\"start\":52518},{\"end\":52548,\"start\":52531},{\"end\":52566,\"start\":52548},{\"end\":52579,\"start\":52566},{\"end\":52595,\"start\":52579},{\"end\":52606,\"start\":52595},{\"end\":52617,\"start\":52606},{\"end\":52635,\"start\":52617},{\"end\":52997,\"start\":52982},{\"end\":53007,\"start\":52997},{\"end\":53020,\"start\":53007},{\"end\":53034,\"start\":53020},{\"end\":53049,\"start\":53034},{\"end\":53064,\"start\":53049},{\"end\":53081,\"start\":53064},{\"end\":53433,\"start\":53423},{\"end\":53450,\"start\":53433},{\"end\":53462,\"start\":53450},{\"end\":53470,\"start\":53462},{\"end\":53725,\"start\":53715},{\"end\":53737,\"start\":53725},{\"end\":53754,\"start\":53737},{\"end\":53764,\"start\":53754}]", "bib_venue": "[{\"end\":38762,\"start\":38737},{\"end\":39017,\"start\":39013},{\"end\":39258,\"start\":39230},{\"end\":39426,\"start\":39359},{\"end\":39809,\"start\":39805},{\"end\":40136,\"start\":40133},{\"end\":40361,\"start\":40350},{\"end\":40763,\"start\":40746},{\"end\":41628,\"start\":41621},{\"end\":41985,\"start\":41962},{\"end\":42252,\"start\":42248},{\"end\":42543,\"start\":42539},{\"end\":42860,\"start\":42855},{\"end\":43148,\"start\":43144},{\"end\":43465,\"start\":43460},{\"end\":43772,\"start\":43750},{\"end\":44013,\"start\":44007},{\"end\":44238,\"start\":44233},{\"end\":44479,\"start\":44474},{\"end\":44811,\"start\":44756},{\"end\":45138,\"start\":45135},{\"end\":45374,\"start\":45370},{\"end\":45724,\"start\":45719},{\"end\":46112,\"start\":46109},{\"end\":46384,\"start\":46379},{\"end\":46626,\"start\":46621},{\"end\":46948,\"start\":46943},{\"end\":47281,\"start\":47276},{\"end\":47540,\"start\":47537},{\"end\":47769,\"start\":47764},{\"end\":48070,\"start\":48067},{\"end\":48421,\"start\":48344},{\"end\":48747,\"start\":48729},{\"end\":49004,\"start\":48928},{\"end\":49422,\"start\":49386},{\"end\":49887,\"start\":49883},{\"end\":50236,\"start\":50233},{\"end\":50457,\"start\":50452},{\"end\":50751,\"start\":50746},{\"end\":51156,\"start\":51151},{\"end\":51536,\"start\":51520},{\"end\":51821,\"start\":51818},{\"end\":52066,\"start\":52061},{\"end\":52336,\"start\":52331},{\"end\":52708,\"start\":52651},{\"end\":53136,\"start\":53081},{\"end\":53475,\"start\":53470},{\"end\":53767,\"start\":53764},{\"end\":43781,\"start\":43774},{\"end\":48485,\"start\":48423}]"}}}, "year": 2023, "month": 12, "day": 17}