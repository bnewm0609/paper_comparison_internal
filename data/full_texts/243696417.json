{"id": 243696417, "updated": "2022-10-10 21:33:48.671", "metadata": {"title": "UniKER: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference", "authors": "[{\"first\":\"Kewei\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Ziqing\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Ming\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yizhou\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional logical rule reasoning and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and logical rules for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use probabilistic model to approximate the exact logical inference (i.e., MAX-SAT). Even worse, both approaches need to sample ground rules to tackle the scalability issue, as the total number of ground rules is intractable in practice, making them less effective in handling logical rules. In this paper, we propose a novel framework UniKER to address these challenges by restricting logical rules to be definite Horn rules, which can fully exploit the knowledge in logical rules and enable the mutual enhancement of logical rule-based reasoning and KGE in an extremely efficient way. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms in terms of both efficiency and effectiveness.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2021.emnlp-main.769", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/ChengYZS21", "doi": "10.18653/v1/2021.emnlp-main.769"}}, "content": {"source": {"pdf_hash": "f3ef386ff88acd00a1d337f29a95ff8002a1269f", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2021.emnlp-main.769.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://aclanthology.org/2021.emnlp-main.769.pdf", "status": "HYBRID"}}, "grobid": {"id": "5fd6d0cfad73c8a60ce4d9a829e09823b1833d46", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f3ef386ff88acd00a1d337f29a95ff8002a1269f.txt", "contents": "\nUniKER: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsNovember 7-11, 2021. 2021\n\nKewei Cheng \nDepartment of Computer Science\nUniversity of California\nLos Angeles, Los AngelesCA\n\nZiqing Yang yangziqing@pku.edu.cnmzhang_cs@pku.edu.cn \nDepartment of Computer Science\nSchool of EECS\nPeking University\nBeijingChina\n\nMing Zhang \nDepartment of Computer Science\nSchool of EECS\nPeking University\nBeijingChina\n\nYizhou Sun yzsun@cs.ucla.edu \nDepartment of Computer Science\nUniversity of California\nLos Angeles, Los AngelesCA\n\nUniKER: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference\n\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\nthe 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 7-11, 2021. 20219753\nKnowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional logical rule reasoning and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and logical rules for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use probabilistic models to approximate the exact logical inference (i.e., MAX-SAT). Even worse, both approaches need to sample ground rules to tackle the scalability issue, as the total number of ground rules is intractable in practice, making them less effective in handling logical rules. In this paper, we propose a novel framework UniKER to address these challenges by restricting logical rules to be definite Horn rules, which can fully exploit the knowledge in logical rules and enable the mutual enhancement of logical rule-based reasoning and KGE in an extremely efficient way. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms in terms of both efficiency and effectiveness. Thomas Alva Edison Mary Stilwell isMarriedTo USA liveIn officialLang English Mina Miller speakLanguage(Person, Language) \u21d0 liveIn(Person, Country) \u2227 officialLanguage (Country, Language) (a) Knowledge Graph (b) Definite Horn Rules UniKER (e) Enriched by Both: 1+1>2! (c) Enriched by KGE (d) Enriched by Logical Reasoning Entities Observed links Inferred links isMarriedTo Thomas Alva Edison Mary Stilwell isMarriedTo USA liveIn officialLang English Mina Miller isMarriedTo Thomas Alva Edison Mary Stilwell isMarriedTo USA liveIn officialLang English Mina Miller isMarriedTo Thomas Alva Edison Mary Stilwell isMarriedTo USA liveIn officialLang English Mina Miller isMarriedTo liveIn liveIn speakLang speakLang speakLang Figure 1: Given (a) a KG with observed facts and (b) a set of definite Horn rules, different inference results can be obtained using (c) KGE, (d) logical inference, and (e) the proposed UniKER approach. The synergy of KGE and logical reasoning via UniKER is more powerful than a simple union of (c) and (d).quality embedding learned by KGE models, in turn, will help to prepare a more complete KG for logical rule-based reasoning(Fig. 1(c)).\n\nIntroduction\n\nKnowledge Graphs (KGs) have grown rapidly recently which provide remarkably valuable resources for many real-world applications (Auer et al., 2007;Bollacker et al., 2008;Suchanek et al., 2007). KG reasoning, which aims at inferring missing knowledge through the existing facts, is the key to the success of many downstream tasks and have received wide attention.\n\nKnowledge Graph Embedding (KGE) methods currently hold the state-of-the-art in KG reasoning (Bordes et al., 2013;Wang et al., 2014;Yang et al., 2014;Sun et al., 2019). They aim to capture the similarity of entities via exploring rich structure information in KGs to predict unseen triples. Despite the excellent performance of KGE methods, the ignorance of possible high-order constraints specified by logical rules limits their application in more complex reasoning tasks. In Fig. 1, for example, we can infer Stilwell lives in USA based on KGE, due to her similarity with Miller in terms of embeddings, as both can be reached via the same relation transformation (i.e. isMarriedTo) from the same entity (Edison). But without the capability of leveraging logical rules, we cannot infer Stilwell and Miller speak English.\n\nAn alternative solution is to infer missing facts via logical rules, which have been extensively explored by traditional logical rule-based methods (Richardson and Domingos, 2006;De Raedt and Kersting, 2008). As shown in Fig. 1, two entities (e.g., Miller and English) that are not directly connected in a KG could participate in the same ground logical rule, (e.g., speakLanguage(Miller, English) \u2190 liveIn(Miller, USA) \u2227 officialLanguage(USA, English)), and a relation between them can be inferred if all predicates in the rule body are true. Different from KGE, logical inference treats triples as independent units and ignores the correlation among them. As a result, the performance of logical inference highly depends on the completeness of KGs, which suffers from severe insufficiency in reality. For example, due to the absence of the triple liveIn(Mary Stilwell, USA), the triple speakLanguage(Mary Stilwell, English) can not be inferred in Fig. 1. Besides its sensitivity to the quality of KG, logical inference is also known for its high computation complexity as it requires instantiating universally quantified rules into ground rules, which is extremely time-consuming.\n\nAlthough both embedding-based methods and logical rule-based methods have their limitations, they are complementary for better reasoning capability. As shown in Fig. 1, on one hand, logical rules are useful to provide additional information by exploiting the higher-order dependency of KG relations ( Fig. 1(d)). On the other hand, high-Despite several attempts made to combine KGE and logical rules for KG reasoning, they either simply treat logical rules as additional constraints into KGE loss (Guo et al., 2016;Rockt\u00e4schel et al., 2015;Demeester et al., 2016) or use probabilistic model to approximate the exact logical inference (i.e., MAX-SAT) (Qu and Tang, 2019;Zhang et al., 2019;Harsha Vardhan et al., 2020). Moreover, these methods rely on ground rules, the total number of which is intractable in practice. To tackle the scalability issue, only a small portion of ground predicates/ground rules are sampled to approximate the inference process, which causes further information loss from the logic side.\n\nTo overcome the above issues, we propose a novel Unified framework for combining Knowledge graph Embedding with logical Rules (UniKER) for better KG reasoning, to handle a special type of first-order logic, i.e., the definite Horn rules. First, we combine logical rule reasoning and KG embedding in an iterative manner, to make sure the inferred knowledge via both techniques can benefit each other as shown in Fig. 1. Second, we propose an iterative grounding algorithm to extend the classic forward chaining algorithm that is designed for definite Horn rule reasoning in an extremely efficient way. Consequently, UniKER can fully exploit the knowledge contained in logical rules and enrich KGs for better embedding. Meanwhile, KGE enhances the forward chaining by including more potential useful hidden facts (See Fig. 1(c)). In this way, two procedures mutually enhance each other. The main contributions of this paper are summarized as follows:\n\n\u2022 We investigate the problem of combining embedding and definite Horn rules, a much simpler yet popular form of logical rules, for KG inference.\n\n\u2022 A unified framework, UniKER, is proposed, which provides a simple yet effective iterative mechanism to let logical inference and KGE mutually enhance each other in an efficient way.\n\n\u2022 We theoretically and experimentally show that UniKER is superior to existing SOTA methods in terms of efficiency and effectiveness.\n\n\nPreliminaries\n\n\nKnowledge Graphs in the Language of Symbolic Logic\n\nA KG, denoted by G = {E, R, O}, consists of a set of entities E, a set of relations R, and a set of observed facts O. Each fact in O is represented by a triple (e i , r k , e j ), where e i , e j \u2208 E and r k \u2208 R. In the language of logic, entities can also be considered as constants and relations are called predicates. Each predicate in KGs is a binary logical function defined over two constants, denoted as r(\u00b7, \u00b7). A ground predicate is a predicate whose arguments are all instantiated by constants. For example, given a predicate liveIn(\u00b7, \u00b7), by assigning constants Miller and USA to it, we get a ground predicate liveIn(Miller, USA). A triple (e i , r k , e j ) is essentially a ground predicate, denoted as r k (e i , e j ) in the language of logic. In the reasoning task, a ground predicate can be regarded as a binary random variable: r k (e i , e j ) = 1 when the triple (e i , r k , e j ) holds true, and r k (e i , e j ) = 0 otherwise. Given the observed facts v O = {r k (e i , e j )|(e i , r k , e j ) \u2208 O}, the task of knowledge graph inference is to predict the truth value for all hidden triples (i.e., unobserved triples)\nv H = {r k (e i , e j )|(e i , r k , e j ) \u2208 H}, where H = {(e i , r k , e j )|(e i , r k , e j ) / \u2208 O, e i , e j \u2208 E, r k \u2208 R}.\n\nFirst Order Logic and Definite Horn Rules\n\nFirst-order logic (FOL) rules are constructed over predicates using logical connectives and quantifiers, which usually require extensive human supervision to create and validate and thus severely limit their applications. Instead, definite Horn rules, as a special case of FOL rules, can be extracted automatically and efficiently via modern rule mining systems, such as WARMR (Dehaspe and Toivonen, 1999) and AMIE (Gal\u00e1rraga et al., 2015) with high quality, which are widely used in practice. Definite Horn rules are composed of a body of conjunctive predicates and a single positive head predicate. They are usually written in the form of implication as shown below:\n\u2200x, y r0(x, y) \u2190 r1(x, z1) \u2227 r2(z1, z2) \u2227 r3(z2, y) (1)\nwhere r 0 (x, y) is called the head of the rule while r 1 (x, z 1 ) \u2227 r 2 (z 1 , z 2 ) \u2227 r 3 (z 2 , y) is the body of the rule. By substituting the variables x, z 1 , z 2 , y with concrete entities e i , e p , e q , e j , we get a ground definite clause as follows:\nr0(ei, ej) \u2190 r1(ei, ep) \u2227 r2(ep, eq) \u2227 r3(eq, ej)(2)\n\nLogical Reasoning\n\nTraditional logical inference aims to find an assignment of truth values to all hidden ground predicates, leading to maximizing satisfied ground rules. Thus, it can be mathematically modeled as a MAX-SAT problem, which is NP-hard (Shimony, 1994).\n\n\nKnowledge Graph Embedding\n\nKGE aims to capture the similarity of entities by embedding entities and relations into lowdimensional vectors. Scoring functions, which measure the plausibility of triples in KGs, are the crux of KGE models. We denote the score of a triple (e i , r k , e j ) calculated following scoring function as f r k (e i , e j ). Representative KGE algorithms include TransE (Bordes et al., 2013), TransH (Wang et al., 2014), TransR (Lin et al., 2015), DistMult (Yang et al., 2014), Com-plEx (Trouillon et al., 2016), andRotatE (Sun et al., 2019), which differ in their scoring functions.\n\n\nRelated Work on Integrating\n\nEmbedding and Logical Rules MAX-SAT problem is defined using boolean logic while scoring function of KGE provides a soft truth value to triples in KG. Probabilistic logic is widely used to integrate both worlds into the same framework, which is able to extend boolean logic to probabilistic logic to enable uncertain inference. These approaches can be divided into two categories: (1) designing a Probablistic Soft Logic (PSL)-based regularization to embedding models and (2)  PSL-based Regularization in Embedding Loss.\n\nThe first way to combine two worlds is to treat logical rules as additional regularization to embedding models, where the satisfaction loss of ground rules is integrated into the original embedding loss. Probabilistic Soft Logic (PSL) (Bach et al., 2015) is used to compute the satisfaction loss, where the probability of each predicate is determined by the embedding. KALE (Guo et al., 2016), RUGE (Guo et al., 2017) and Rockt\u00e4schel et al. (Rockt\u00e4schel et al., 2015) are some of the representative methods. A summary of these methods can be found in Appendix E. All approaches in this category have to instantiate universally quantified rules into ground rules before model learning. When including all ground rules into the calculation of satisfaction loss, the additional regularization becomes the convex program which reasons analogous MAX-SAT problem defined over Lukasiewicz logic (Klir and Yuan, 1996) whose solution is an approximation to MAX-SAT problem (Bach et al., 2015). Detailed proof is given in Appendix C. As the total number of ground rules is intractable in practice, only a small portion of ground rules will be sampled to tackle the scalability issue, which further leads to the loss of logical information. Moreover, most methods in this category make only a one-time injection of logical rules to enhance embedding, ignoring the interactive nature between embedding and logical inference (Guo et al., 2016;Rockt\u00e4schel et al., 2015).  English)), and adds their conclusion (e.g., speakLanguage (Mina Miller, English)) to the known facts until no facts can be added anymore. As illustrated in Fig. 1, unlike other logical inference algorithms, which require all ground predicates (including both observed and unobserved ground predicates) into calculation, forward chaining adopts \"lazy inference\" instead. It involves only a small subset of \"active\" ground predicates/rules, and activates more if necessary as the inference proceeds. The mechanism dramatically improves inference efficiency by avoiding the computation for massive ground predicates/rules that are never used. Moreover, considering that definite Horn rules which can be extracted efficiently via modern rule mining systems are usually chainlike Horn rules, which is in the form as shown in Eq.\n\n\nEmbedding-based\n\n(1). The conjunctive body of a ground chain-like Horn rules is essentially a path in a KG, which can be extracted efficiently using sparse matrix multiplication. More general implementation of forward chaining algorithm can be found in Appendix G. \n\nin which L(e i , r k , e j ) is defined as: where (e i , r k , e j ) denotes their corresponding negative samples, and \u03b3 is a margin to separate them. The score f r k (e i , e j ) of a triple (e i , r k , e j ) can be calculated following any scoring functions of KGE models. To reduce the effects of randomness, we sample multiple negative triples for each positive sample, which is denoted as N (e i , r k , e j ). To ensure true but unseen triples will not be sampled, the selection of N (e i , r k , e j ) is restricted to v F H * .\nmax(0, \u03b3 \u2212 fr k (ei, ej) + (e i ,\nUpdate KG with KGE-based Inference. Although forward chaining can find the satisfying truth assignment for all hidden triples efficiently, its reasoning ability is severely limited by the coverage of rules, the incompleteness of KGs, and the errors/noise contained in KGs. Considering its strong reasoning ability and robustness, KGE models are not only useful to (1) prepare a more complete KG by adding useful hidden triples but also helpful to (2) eliminate incorrect triples in both KGs and inferred results.\n\n(1) Including Potential Useful Hidden Triples (\u2206+). Since the body of a definite Horn rule is a conjunction of predicates, its ground rule can get activated and contribute to logical inference only if all the predicates in its body are completely observed. Due to the sparsity of real-world KGs, only a small portion of ground rules can participate in logical inference, which severely limits the reasoning ability of definite Horn rules. A straightforward solution would be computing the score for every hidden triple and adding the most promising ones with the highest scores to the KG. Unfortunately, the number of hidden triples is quadratic to the number of entities (i.e. O(|R||E| 2 )), thus it is too expensive to compute scores for all of them. Instead, we adopt \"lazy inference\" strategy to select only a small subset of \"potentially useful\" triples. Take the ground rule in Eq. (2) as an example, if r 1 (e i , e p ) \u2208 v O , r 3 (e q , e j ) \u2208 v O , and r 2 (e p , e q ) \u2208 v H , we would not be able to infer the head r 0 (e i , e j ) as whether r 2 (e p , e q ) is true or not is unknown. Thus, r 2 (e p , e q ) becomes the crux to determine the truth value of the head, which is called \"potentially useful\". In general, given a ground rule whose body includes only one unobserved ground predicate, this unobserved ground predicate can be regarded as a \"potentially useful\" triple. We denote the set of all \"potentially useful\" triples as \u2206 + . According to their positions, \"potentially useful\" triples can be divided into two categories: (1) triples that are the first or the last predicate in a ground rule; and (2) triples that are neither the first nor the last. We propose algorithms to identify both types of \"potentially useful\" triples respectively as illustrated in Fig. 2 by taking Eq. (2) as an example. More details are summarized in Appendix A. Score f r k (e i , e j ) will be computed by KGE model to predict whether a \"potentially useful\" triple is true. If f r k (e i , e j ) is larger than the given threshold \u03a8, the triple is classified as true. Otherwise, the triple is classified as false. And we experimentally analysed the effect of \u03a8 in the Appendix J.2. Note that a dynamic programming algorithm can also be used to alleviate the computational complexity for long rules. The detailed algorithm can be found in the Appendix B.\n\n(2) Excluding Potential Incorrect Triples (\u2206\u2212). In addition, due to the symbolic nature, logical rules cannot handle noisy data as well. If the KGs contain any error, based on incorrect observations, forward chaining will not be able to make the correct inference. Even worse, it may contribute to the propagation of the error by including incorrectly inferred triples into KGs. Therefore, a clean KG is significant for logical inference. Since KGE models show great power in capturing the network structure of KGs, incorrect triples usually result in contradictions and get lower prediction scores in KGE models compared to correct ones. Therefore, score f r k (e i , e j ) computed by KGE model is able to measure reliability of triple (e i , r k , e j ) in O \u222a V T H * . We denote bottom \u03b8% triples with lowest prediction scores as \u2206 \u2212 . It will be excluded from O \u222a V T H * to alleviate the impact of noise.\n\n\nIntegrating Embedding and Logical Rules in an Iterative Manner\n\nSince logical rules and KGE can mutually enhance each other, we propose a unified framework, known as UniKER, to integrate KGE and definite Horn rules-based inference iteratively. The pseudo-code of UniKER can be found in Algorithm 1. MAX_ITER is the user specified max iterations to run the algorithm, which highly depends on KG datasets. According to results in Fig. 4, MAX_ITER is usually set as 2 to 4. For each iteration of UniKER, it is comprised of two steps. First, we focus on logical reasoning to update KG. Following forward chaining algorithm, by triggering all rules whose premises are satisfied, we derive entailed triple set \nv T H t * at t-th itera- tion, which is a subset of v T H * (v T H * = \u222a +\u221e t=1 v T H t * ).\n\nConnection to Existing Approaches\n\nConnection to PSL-based Regularization Approaches. The general objective of PSL-based regularization approaches can be written as:\nL KGE + \u03bbL P SL(5)\nwhere L KGE denotes the loss of the base KGE model while L P SL corresponds to the satisfaction loss of the sampled ground rules. When including all ground rules into the calculation of L P SL , L P SL becomes the convex program which reasons analogous MAX-SAT problem defined over Lukasiewicz logic, which only approximates the exact logical inference. Detailed proof is given in Appendix C. Instead of guiding the embedding learning approximately, UniKER directly take the optimum of MAX-SAT problem as targets to optimize the embedding model. Thus it can better exploit the knowledge contained in definite Horn rules. Moreover, L P SL makes only a one-time injection of logical rules to enhance embedding, where logical reasoning will not be further enhanced even after the KGE gets improved. On the contrary, UniKER is able to capture the interactive nature between embedding and logical inference.\n\nConnection to Embedding-based Variational Inference to MLN. The general objective of embedding-based variational inference for MLN can be written as:\nL ELBO (Q \u03b8 , P w ) + \u03bbL KGE (Q \u03b8 )(6)\nwhere the variational distribution Q \u03b8 is defined using a KGE model and P w is the true posterior defined over MLN. L KGE (Q \u03b8 ) denotes the loss of the base KGE model. By optimizing L ELBO (Q \u03b8 , P w ), the KL divergence between Q \u03b8 and P w can be minimized. In this way, the knowledge contained in rules can be transferred into the embeddings. Due to the nature of the approximate solution provided by variational inference and the information loss caused the sampling procedure, Q \u03b8 can only approximate the optimum of MAX-SAT problem and no guarantees are provided on the quality of the solutions obtained. Instead of guiding the learning of embedding model via variational inference, we directly solve MAX-SAT problem and use the derived knowledge v T H * to train the embedding model, which leads to superior reasoning.\n\nAdvantages of UniKER compared to SOTA methods. We categorize all existing methods according to two aspects: (1) whether they capture mutual interaction between KGE and logical inference; and (2) whether they conduct exact logical inference. The summary is given in Table 1. For the first aspect, most PSL-based regularization approaches make only a one-time injection of logical rules to enhance embedding, while embeddingbased variational inference to MLN and UniKER provide the interaction between embedding and logical inference. For the second aspect, both PSLbased regularization approaches and embeddingbased variational inference to MLN follow the framework of probabilistic logic to combine logical rule and KGE, which can only approximate the optimal solution of MAX-SAT problem. UniKER is the first to use forward chaining to conduct exact inference, which provides an optimal solution to the original MAX-SAT problem.  \n\n\nKG Completion\n\nTo compare different algorithms on KG inference task, we mask the head or tail entity of each test triple, and require each method to predict the   Table 2 shows the comparison results, from which we find that: (1) UniKER consistently outperforms KGE models in most cases with significant performance gain, which can ascribe to the utilization of additional knowledge from logical rules; (2) UniKER also obtains better performance than both classes of approaches which combine embedding model with logical rules as it provides an exact optimal solution to satisfiable problem defined over all ground rules rather than employ sampling strategies to do approximation.\n\nImpact of Iterative Algorithm on KG Completion. To investigate how iterative process helps improve reasoning ability of UniKER, we conduct experiments on Family dataset and record the performance of UniKER on test data in terms of Hit@1, Hit@10 and MRR in every iteration. In   Table 3: Ablation study on noise threshold \u03b8% on Family dataset (whose train set is injected with noise) particular, KGE model is trained based on the original data without any inferred triples included in iteration 0. As presented in Fig. 3, we observed that (1) with the increase of iterations, the performance improves rapidly first, and gradually slows down;\n\n(2) UniKER has a bigger impact on Hit@k compared to MRR. Robustness Analysis. To investigate the robustness of UniKER, we compare the reasoning ability of UniKER with TransE on Family dataset with noise. Complete details of injecting noise are summarized in Appendix J.1. We vary \u03b8 among {10, 20, 30, 40, 50} to study the effect of the threshold used to eliminate noisy triples. The comparison results are presented in Table 3. We observe that (1) UniKER outperforms TransE on noisy KG with significant performance gain; (2) with the increase of \u03b8, the performance first increases and then decreases. The best performance is achieved when \u03b8 = 40%.\n\nEffect of Threshold \u03a8 Used to Include Poten-  tial Useful Hidden Triples To investigate effect of threshold used to include useful hidden triples, we also compare the reasoning ability of UniKER with TransE on Family dataset with different thresholds \u03a8. As threshold can vary a lot for different data sets, to propose a unified way to determine proper threshold \u03a8, we take score f r k (e i , e j ) corresponding to the triple which ranks as top \u03c8% in test dataset as threshold \u03a8. We vary \u03c8 among {10, 20, 30, 40, 50}. The comparison results are presented in Table 9. We can observe that reasoning ability of UniKER does not vary a lot with different thresholds. In other words, the performance is less sensitive to the parameter \u03c8, which is appealing in practice.\n\n\nEfficiency Analysis\n\nBesides the promising results on KG reasoning, our UniKER is superior in terms of efficiency. Though we have theoretically analyzed the computational complexity of UniKER in Appendix H, the efficiency of forward chaining highly depends on KG datasets. Note that forward chaining learns the optimal truth assignment for the satisfiable problem iteratively, the number of iterations required to achieve the optimal solution may influence its scalability. We first conduct two experiments on six datasets (details are introduced in Appendix I):\n\n(1) as presented in Fig. 4, we record the proportion of inferred triples accumulated in every iteration over all inferred triples. The result shows that forward chaining can achieve the optimal solution within 12 iterations, and infer most correct triples within only 4 iterations; (2) \n\n\nMutual Enhancement between KGE and Logical Inference\n\nIn this section, we aim to show that the synergy of KGE and logical inference via UniKER is more powerful than a plain union. Enhancement of Logical Inference via KGE. On one hand, high quality embedding learned by KGE models is useful to prepare more complete KGs via including useful hidden triples, which the performance of logical inference highly depends on. To show the benefit brought by KGE over logical inference, we evaluate UniKER-TransE against forward chaining on Family Dataset with the triple classification task, which aims to predict correct facts in the testing data. In order to create a testing set for classification, we randomly corrupt relations of correct testing triplets for negative triples construction. It results in a total of 2 \u00d7 #Test triplets with equal number of positive and negative examples. During evaluation, we adopt three evaluation metrics, i.e., precision, recall and F1. As shown in   As some triples in test dataset can be directly derived from logical rules, to ensure the improvement comes from the reasoning ability enhancement of KGE model, we exclude the triples derived directly from rules from the test data. As presented in Table 6, we can observe that UniKER-TransE outperforms TransE model with huge performance gain, especially in terms of Hit@1, which can ascribe to the added value brought by logical rules over KGE.\n\n\nConclusion\n\nIn this paper, we proposed a novel framework, known as UniKER, to integrate embedding and definite Horn rules in an iterative manner for better KG inference. We have shown that UniKER can fully leverage the knowledge in definite Horn rules and completely transfer them into the embeddings in an extremely efficient way. \n\n\nA Illustration of Potential Useful Hidden Triples\n\nLet every relation r k in KG associate with an |E| \u00d7 |E| matrix M (k) , in which the element M (k) ij = 1 if the triple (e i , r k , e j ) \u2208 O, and 0 otherwise. The algorithms to identify both types of \"potential useful\" triples as illustrated in Fig. 2 in main context are given as follows.\n\n\u2022 When the \"potential useful\" triple is the first or the last predicate in a ground rule, other observed triples in a chain-like definite Horn rule still constitute a complete path, which can be extracted efficiently by sparse matrix multiplication. Take Fig. 2 (c) in main context as an example, to identify the \"potential useful\" triple r 1 (e i , e p ), we have to first extract all connected path r 2 (e p , e q ) \u2227 r 3 (e q , e j ) by calculating M = M (2) M (3) , where M (2) and M (3) are adjacency matrices corresponding to relations r 2 and r 3 . Each nonzero element M pj indicates a connected path between e p and e j . We denote all indexes correspond to nonzero rows in M as \u03b4 = {p|( j M pj ) = 0}, which indicates that there is always a connected path starting at p. For specific p \u2208 \u03b4, \u2206 p = {(e i , r 1 , e p )|e i \u2208 E} defines a set \"potential useful\" triples. If (e i , r 1 , e p ) in \u2206 p is predicted to be true via KGE, the head predicates r 0 (e i , e j ) can be inferred.\n\n\u2022 Otherwise, the path corresponds to the conjunctive body of the ground rule get broken into two paths by the \"potential useful\" triple, which we have to extract separately. As shown in Fig.2 (d) in main context , when identifying \"potential useful\" triples r 2 (e p , e q ) \u2208 v H , two paths to be extracted are essentially two single relations r 1 and r 3 , whose corresponding matrices are M (1) and M (3) , respectively. We denote all indexes correspond to nonzero columns in M (1) as \u03b4 1 = {p|( i M qj ) = 0}. \u2206 12 = {(e p , r 2 , e q )|p \u2208 \u03b4 1 , q \u2208 \u03b4 2 } defines a set \"potential useful\" triples. If (e p , r 2 , e q ) in \u2206 12 is predicted to be true via KGE, the head predicates {r 0 (e i , e j )|M  . . , C m }, where C j \u2208 C is a disjunction of variable r k (e i , e j ) or its negation \u00acr k (e i , e j ), which can be written as:\n(1) ip = 0, M(31 for F k \u2208 F do 2 l \u2190 len(B(F k )) 3 for i \u2208 range(l) do 4 for j \u2208 range(i + 1, l) do 5 if B(F k )[i : j] / \u2208 T then 6 T [B(F k )[i : j]] = T [B(F k )[i : j \u2212 1]] * T [B(F k )[j]](\u2228 r k (e i ,e j )\u2208I + j r k (e i , e j )) \u2228(\u2228 r k (e i ,e j )\u2208I \u2212 j \u00acr k (e i , e j ))(7)\nwhere I + j (resp. I \u2212 j ) is the set of variables that are not negated (resp. negated). Instead of interpreting the clauses C using Boolean logic, Lukasiewicz logic allow variables r k (e i , e j ) to take soft truth values I(r k (e i , e j )) in an interval between [0, 1]. Given two variables x i and x j , the formulas for the relaxation of the logical conjunction (\u2227), disjunction (\u2228), and negation (\u00ac) are as follows:\nI(x i \u2227 x j ) = max{0, I(x i ) + I(x j ) \u2212 1} I(x i \u2228 x j ) = min{1, I(x i ) + I(x j )} I(\u00acx i ) = 1 \u2212 I(x i ).\nTherefore, by associating each C j \u2208 C with weight w j , the analogous MAX-SAT problem defined over C in Lukasiewicz logic can be written as:\nmax {r k (e i ,e j )}\u2208[0,1] n C j \u2208C w j min{ r k (e i ,e j )\u2208I + j r k (e i , e j ) + r k (e i ,e j )\u2208I \u2212 j (1 \u2212 r k (e i , e j )), 1}\n(8) where w j is the weight of C j . It is equivalent to the relaxation of MAX-SAT problem. The proof is as follows.\n\nThe MAX-SAT problem defined over weighted C can be formulated as the integer linear program as follows:\nmax {r k (e i ,e j )}\u2208{0,1} n C j \u2208C w j min{ r k (e i ,e j )\u2208I + j r k (e i , e j ) + r k (e i ,e j )\u2208I \u2212 j (1 \u2212 r k (e i , e j )), 1}\n(9) where w j is the weight of C j . Finding a most probable assignment to the variables r k (e i , e j ) is NPhard (Shimony, 1994). Using relaxation techniques developed in the randomized algorithms community, we can independently round each Boolean variable r k (e i , e j ) to true with probability p ijk . Then, the expected satisfaction score\u0174 of clauses C is:\nW = C j \u2208C w j (1 \u2212 r k (e i ,e j )\u2208I + j (1 \u2212 p ijk ) r k (e i ,e j )\u2208I \u2212 j p ijk )(10)\nThe optimal\u0174 would give the exact MAX-SAT solution. According to (Bach et al., 2015), to approximately optimize\u0174 , we can relax Eq.(9) as following:\nmax y\u2208[0,1] n C j \u2208C w j min{ r k (e i ,e j )\u2208I + j\u0177 ijk + r k (e i ,e j )\u2208I \u2212 j (1 \u2212\u0177 ijk ), 1}(11)\nThis results in the equivalence of Eq.(8) and MAX-SAT relaxation. Therefore, the optimum of Eq. (8) can only approximate the optimum of MAX-SAT problem.\n\n\nD Satisfiability of KG Inference under Restriction of Definite Horn Rules\n\nGiven a set of logical rules F and their ground rules F g , if there exists at least one truth assignment that satisfies all ground rules F g , we call it satisfiable. We will show there exists a truth assignment to all hidden triples in a KG such that all ground rules are satisfied when restricting logical rules to be definite Horn rules.\n\nTheorem 1. Knowledge graph inference is satisfiable when restricting logical rules to be definite Horn rules.\n\nProof. A set of ground rules is unsatisfiable if we can derive a pair of opposite ground predicates (i.e., r 0 (e i , e j ) and \u00acr 0 (e i , e j )) from them. It is the case if and only if \u00acr 0 (e i , e j ) is defined in KG as definite Horn rules can only include one single positive head predicate which results in its incapability in deriving negative triples. However, a typical KG will not explicitly include negative triples (i.e., \u00acr 0 (e i , e j )). Thus we can never derive such a pair of opposite ground predicates, which confirms that KG inference is satisfiable when restricting logical rules to be definite Horn rules.\n\n\nE Summary of PSL-based Regularization Approaches.\n\nPSL-based regularization methods treat logical rules as additional regularization, where satisfaction of rules is integrated into the original embedding loss. A typical integration is defined as follows: (1) sampling ground logical rules given the template logical rules; (2) mapping each related triple (i.e., predicate) into a confidence score (i.e., soft truth value); (3) computing the satisfaction score to each ground rule based on its predicates' scores; and (4) defining proper loss based on the satisfaction score for all the ground rules. We now use KALE (Guo et al., 2016) as an example to illustrate the procedure described above. First, a set of positive and negative ground rules (i.e., f + and f \u2212 ) are sampled given the template logical rules. Together with atomic formulas (i.e., positive and sampled negative triples), the whole set of formulas is denoted as F g . Second, each predicate r k (e i , e j ) is assigned with a soft truth value, which is a transformation of TransE-based scoring function: I(r k (e i , e j )) = 1 \u2212 1 3 \u221a d e i + r k \u2212 e j 1 , where e i , r k , and e j are embedding vectors to the corresponding entities and relations and d is the dimensionality of the embeddings. Third, the soft truth value of a ground rule is computed according to Lukasiewicz Logic, where the basic operations are summarized in appendix C: Given the basic operations, the truth value of any ground formula can be calculated recursively. Finally, KALE defines a loss function over formulas from F g , which contain both triples and ground rules. Similar to marginbased ranking loss, embeddings are learned via maximizing the difference between the soft truth value of positive formulae I(f + ) and its negative samplings I(f \u2212 ). Note that, by removing ground rules from F g , the loss is degenerated to regular TransE-based embedding loss. Rockt\u00e4schel (Rockt\u00e4schel et al., 2015) devised a model similar to KALE. However, instead of learning entity embeddings for individual entity, they utilize matrix factorization to learn joint embeddings of pairs of entities v e i ,e j as well as embeddings of relations v r k . Logistic loss is used to maximize the soft truth value of positive formulaes I(f + ). Different from above methods, RUGE (Guo et al., 2017) defines a loss function over triples. It employs scoring function in ComplEx (Trouillon et al., 2016), \u03c3(Re( e i , r k , e j )), to model triples. Triples are divided into two categories, including observed triples (i.e., v O ) and hidden triples (i.e., v H ). Observed triples have labels y r k (e i ,e j ) = 1 whereas sampled negative triples have labels y r k (e i ,e j ) = 0. The soft label of hidden triples s r k (e i ,e j ) \u2208 [0, 1] have to be predicted following t-norm fuzzy logic. With y r k (e i ,e j ) and s r k (e i ,e j ) , RUGE learns embedding by enforcing triples to be consistent with their labels. The summary of all logical rule-based regularization approaches can be found in Table 7.\n\n\nF Summary of Embedding-based\n\nVariational Inference for MLN.\n\nTo specify probability distributions over complex relational domains compactly, Markov Logic Network (MLN) (Richardson and Domingos, 2006) provides a probabilistic extension of FOL via probabilistic graphical models. Given a set of FOL formulas F and their corresponding weight vector w, it defines a Markov network with one node per ground predicate and one feature per ground rule.\n\nThe weight of a feature is the weight of its original FOL rules. Under the MLN model, the joint probability of all triples is defined as:\np w (v O , v H ) = 1 Z(w) exp( i:F i \u2208F w i n i (v O , v H )) (12) where n i (v O , v H )\nis the number of true groundings of F i based on the values of v O and v H , and Z(w) is a normalization constant for w to make the probabilities of all worlds sum up to one. Since MLN inference subsumes probabilistic inference, which is #P-complete, and logical inference, which is NP-complete even in finite domains (Richardson and Domingos, 2006), it is a very challenging problem computational wise. Several methods including pGAT (Harsha Vardhan et al., 2020), Ex-pressGNN (Zhang et al., 2019) and pLogicNet (Qu and Tang, 2019) propose to conduct variational inference of MLN to alleviate the time complexity. We now use pLogicNet (Qu and Tang, 2019) as an example to illustrate the procedure. pLogicNet aims to train the MLN model by optimizing the evidence lower bound (ELBO) for the likelihood function of observed triples v O :\nlog p w (v O ) \u2265L(q \u03b8 , p w ) =E q \u03b8 (v H ) [log p w (v O , v H ) \u2212 log q \u03b8 (v H )]\nwhere the variational distribution q \u03b8 (v H ) is defined using a knowledge graph embedding model, by assuming each triple independently follows a Bernoulli distribution, with parameters specified by the embedding score function:\nq \u03b8 (v H ) = (e i ,r k ,e j )\u2208H q \u03b8 (r k (e i , e j )) = (e i ,r k ,e j )\u2208H Ber(r k (e i , e j )|f r k (e i , e j ))\nwhere Ber represents the Bernoulli distribution and f r k (e i , e j ) is an embedding scoring function denoting the probability of triple (e i , r k , e j ) to be true. For example, in DistMult, f r k (e i , e j ) can be defined as \u03c3(e T i diag(r k )e j ). This lower bound can be effectively optimized using varitional EM algorithm (Neal and Hinton, 1998). In variational Estep, p w is fixed and q \u03b8 is updated to minimize the KL divergence between q \u03b8 (v H ) and p w (v H |v O ). In M-step, q \u03b8 is fixed and the weights of the rules w is updated to maximize the joint probability of both observed and hidden triples (i.e., ei, ej, rk \u2208 C d \nE q \u03b8 (v H ) [log p w (v O , v H )]\n). However, due to the expensive computational cost of MLN inference, even for variational inference algorithms that are developed to alleviate the time complexity, the efficiency issue remains a big problem.\n\n\nG Implementation of Forward Chaining.\n\nWe have discussed the implementation of forward chaining algorithm for chain-like definite Horn rules in Section 4.1. Next, we discuss more general implementation of forward chaining algorithm to handle definite Horn rules in any form. As shown in Fig. 5, unlike traditional logical inference methods, instead of instantiating the rules with all potential triples in KG (including both observed and unobserved triples), only observed triples are considered when apply forward chaining. Let us take the definite Horn rule in Fig. 5 as an example. To apply forward chaining to infer new facts, we first focus on all observed triples related to first predicate in the body, liveIn(person, country). Consequently, \"country\" is limited to a small set of concrete entities \"USA\",\"Denmark\". By restricting \"country\" within the set \"USA\",\"Denmark\", we again ground officialLanguage (country, language) with observed triples. The candidate entities for \"country\" will be further limited to \"USA\". Finally, only triples speakLanguage (Mina Miller, English) can be inferred as the new facts and add to the KG.\n\n\nH Theoretical Computational Complexity Analysis of UniKER.\n\nTo theoretically demonstrate the superiority of our proposed UniKER in terms of efficiency, we compare the space and time complexity of UniKER and other methods that combine KG embedding and logical rules. More precisely, we only include logical rule-based regularization approaches because embedding-based variational inference for MLN is essentially a #P problem. Obviously, they have a much higher computational cost than we do. As both logical rule-based regularization approaches and our UniKER consists of two parts, materialization (i.e., sampling ground logical rules and inference U using forward chaining) and KG embedding learning, we include the complexity of both parts in Table 11. Note that materialization only contributes to the time complexity without affecting space complexity. We denote n e /n r /n t /l/n l /\u03b8/a/d as the number of entities/relations/observed triples/length of rule body/number of rules/sampling ratio/average degree of entities/dimension of the embedding space. We can observe that: (1) For space complexity, our proposed UniKER is the same as other logical rule-based regularization approaches;\n\n(2) For time complexity, considering a n e , if the sampling ratio is not small enough, our proposed UniKER is much smaller than other logical rule-based regularization approaches.\n\n\nI Data Statistics\n\nThe detailed statistics of three large scale real-world KGs (e.g., Family, FB15k-237 and WN18RR) are provided in Table 12. FB15K237 and WN18RR are the most widely used benchmark datasets for KGE models, which don't suffer from test triple leakage in the training set. The Family dataset is selected due to better interpretability and high intuitiveness. In addition, three small scale datasets (e.g., RC1000, sub-YAGO3-10 and sub-Family) are also included in our experiments to evaluate the scalability of forward chaining against a number of SOTA inference algorithms for MLN as shown in J.3 due to the poor scalability of MLN .\n\n\u2022 RC1000 is a typical benchmark dataset for inference in MLN. It involves the task of relational classification with hand-code rules given.\n\n\u2022 sub-YAGO3-10 is a subset of a well known  For the large scale knowledge graph, we adopt three commonly used benchmark datasets, including Family, FB15k-237 and WN18RR.\n\n\u2022 Family contains family relationships among members of a family (Denham, 1973). We substract a subset from Family dataset and call it sub-Family.\n\n\u2022 FB15k-237 is the most commonly used benchmark knowledge graph datasets introduced in (Bordes et al., 2013). It is an online collection of structured data harvested from many sources, including individual, user-submitted wiki contributions.\n\n\u2022 WN18RR is another widely used benchmark knowledge graph datasets introduced in (Bordes et al., 2013). It is designed to produce an intuitively usable dictionary and thesaurus, and support automatic text analysis. Its entities correspond to word senses, and relationships define lexical relations between them.\n\nJ Experimental Details.\n\n\nJ.1 Setting for Knowledge Graph Completion\n\nTo compare among the reasoning ability of UniKER and aforementioned baseline algorithms, we mask the head or tail entity of each test triple, and require each method to predict the masked entity. We use three large-scale datasets including Family, FB15K-237 and WN18RR. During evaluation, we use the filtered setting (Bordes et al., 2013) and three evaluation metrics, i.e., Hit@1, Hit@10 and MRR. We randomly split the data into training set and test set with the ratio of 8:2 and do not exclude the triples derived from rules from test data. To fairly compare among all baseline methods, we consistently apply this same setting to all of them. Due to the unavailable codes, we take the results of BLP from the corresponding paper (Qu and Tang, 2019) and the results of pGAT from the corresponding paper (Harsha Vardhan et al., 2020). As only the results on the FB15k-237 and WN18RR datasets are reported, we only compare with them on these two datasets.\n\nHyperparameter Settings Adam (Kingma and Ba, 2014) is adopted as the optimizer. We set the parameters for all methods by a grid search strategy. The range of different parameters is set as follows: embedding dimension k \u2208 {250, 500, 1000}, batch size b \u2208 {256, 512, 1024}, and fixed margin \u03b3 \u2208 {6, 9, 12, 24}. Afterwards, we compare the best results of different methods. Both the entity embeddings and the relation embeddings are uniformly initialized and no regularization is imposed on them. The detailed hyperparameter settings can be found in Table 8.  Robustness Analysis. As all kinds of noise might be contained in the process of constructing KGs, we introduce noise by substituting the true head entity or tail entity with randomly selected entity. Following this approach, we construct a noisy Family dataset with noisy triples to be 40% of original data. All generated noisy triples only fused into the original training set while validation and test sets remain the same.\n\n\nJ.2 Effect of Threshold \u03a8 Used to Include Potential Useful Hidden Triples\n\nTo investigate effect of threshold used to include useful hidden triples, we also compare the reasoning ability of UniKER with TransE on Family dataset with different thresholds \u03a8. As threshold can vary a lot for different data sets, to propose a unified way to determine proper threshold \u03a8, we take score f r k (e i , e j ) corresponding to the triple which ranks as top \u03c8% in test dataset as threshold \u03a8. We vary \u03c8 among {10, 20, 30, 40, 50}. The comparison results are presented in Table 9. We can observe that reasoning ability of UniKER does not vary a lot with different thresholds. In other words, the performance is less sensitive to the parameter \u03c8, which is appealing in practice.\n\n\nJ.3 Efficiency Analysis\n\nWe evaluate the scalability of forward chaining against a number of state-of- The results of their inference time are given in Table 13. Additionally, we compared the overall efficiency of our proposed UniKER with other methods. As shown in Table 10, UniKER showed its efficiency in the time cost per epoch. Even with the inference period, UniKER is faster than other methods combining embedding with logical rules experimentally.  \n\n\nJ.4 Impact of Coverage of Logical Rules on Family Dataset\n\nTo further analyze the impact of coverage of logical rules on KG inference, we measure the coverage of logical rules using the total number of triples, which can be inferred from the given set of definite Horn rules. Due to space limitations, we only show the results on the Family dataset as we have similar observations on the remaining datasets. To ensure enough coverage of logical rules, we take the whole Family dataset as training data while the triples, which can be inferred from the training data using all 41 logical rules, are regarded as test data. To investigate the effects of coverage of logical rules, we vary the number of definite Horn rules among {10, 20, 30, 35, 36, 38, 41}. We provide the number of triples that can be inferred from these sets of rules in Table 14. As shown in Figure 6, we compare UniKER-DistMult against its default model DistMult as well as forward chaining algorithm.\n\nIn particular, we regard link prediction in KG as binary classification and evaluate all methods in terms of triple True/False classification accuracy. We make the observations that: (1) When the coverage of logical rule is not enough, traditional rulebased methods have shown poor performance; (2) Without the incorporation of logical rules, DistMult has already shown pretty good reasoning ability;\n\n(3) The performance of UniKER sustainedly and steadily increases with the increase of coverage of logical rules; (4) When we include only 30 rules, UniKER has already achieved accuracy close to 1, which is much higher than forward chaining. A small number of logical rules is very appealing in practice as it is costly and labor-intensive to obtain high-quality logical rules.\n\n\nJ.5 Example of Logical Rules Used in Experiments\n\nTo generate candidate rules, we automatically mine rules using AMIE+ (Gal\u00e1rraga et al., 2015). Considering that longer generated rules usually suffer from lower quality and require much more computational resource, the candidate rules we used in      Family f ather(x, y) \u2190 husband(x, z) \u2227 mother(z, y) nephew(x, y) \u2190 son(x, z) \u2227 sister(z, y) uncle(x, y) \u2190 brother(x, z) \u2227 f ather(z, y)\n\nFB15k-237\n\naward_nominations.award_nominee(x, y) \u2190 awards_won.award_winner(x, z) \u2227 award_nominations.award_nominee(y, z) release_date.f ilm_release_region(x, y)\n\n\u2190 release_date.f ilm_release_region(x, z) \u2227 military_conf licts.combatants(z, y) release_date.f ilm_release_region(x, y)\n\n\u2190 actor/f ilm.perf ormance/f ilm(z, x) \u2227 nationality(z, y)\n\n\nWN18RR\n\n_also_see(x, y) \u2190 _also_see(y, x) _hypernym(x, y) \u2190 _verb_group(x, z) \u2227 _hypernym(z, y) _synset_domain_topic_of (x, y) \u2190 _derivationally_related_f orm(x, z) \u2227 _synset_domain_topic_of (z, y) \n\nFigure 2 :\n2r,e j )\u2208N (e i ,r,e j ) fr k (e i , e j ) Illustration of potential useful hidden triples by taking Eq. (2) as an example.\n\n\n2011), SimplE (Kazemi and Poole, 2018), Hy-pER (Bala\u017eevi\u0107 et al., 2019a), TuckER (Bala\u017eevi\u0107 et al., 2019b), TransE (Bordes et al., 2013), DistMult (Yang et al., 2014) and RotatE (Sun et al., 2019)), (2) traditional logical rule-based methods (e.g., MLN (Richardson and Domingos, 2006) and BLP (De Raedt and Kersting, 2008)), (3) two classes of approaches combining embedding model with logical rules (two representative methods KALE (Guo et al., 2016) and RUGE (Guo et al., 2017) for PSL-based regularization approaches, and pLogicNet (Qu and Tang, 2019), Express-GNN (Zhang et al., 2019) and pGAT (Harsha Vardhan et al., 2020) for embedding-based variational inference of MLN), and(4)other approaches to combining embedding model with logical rules (e.g., BoxE(Abboud et al., 2020)). To show that UniKER can be easily generalized to various KGE models, we chose TransE (Bordes et al., 2013), DistMult (Yang et al., 2014) and RotatE (Sun et al., 2019) as the scoring function for UniKER.\n\nFigure 3 :\n3Impact of #iterations on UniKER (KG completion task on Family dataset).\n\nFigure 4 :\n4Proportion to the optimal number of inferred triples w.r.t. #iterations for efficiency analysis of Forward Chaining. masked entity. More detailed settings are in Appendix J.1.\n\n( 1 )\n1ip ) = 0} and all indexes correspond to nonzero rows in M (3) as \u03b4 2 = {q|( j M (3)\n\n\nset of logical clauses C = {C 1 , .\n\n\n\u2208N f + [\u03b3 \u2212 I(f + ) + I(f \u2212 )]+ e 2 \u2264 1; r 2 \u2264 1Rockt\u00e4schel et al.(Rockt\u00e4schel et al., 2015)   \u03c3(vr k .ve i ,e j ) f + \u2208Fg L(I(f + )) r k (e i ,e j )\u2208v H L(I(rk(ei, ej)), s r k (e i ,e j ) )\n\nFigure 5 :\n5Traditional logical inference v.s. Forward chaining.benchmark dataset of knowledge graph, YAGO3-10.\n\nFigure 6 :\n6Impact of Coverage of Logical Rules on Family Dataset on Triple True/False Classification Task.\n\n\nAppendix F. One main drawback is that they only approximate the exact logical inference (i.e., MAX-SAT) due to the nature of the approximate solution provided by variational inference. Besides, inference efficiency is another challenge. Given the fact that KGs usually contain a large number of entities, it is impractical to optimize on all hidden triples. Therefore, only a small portion of hidden triples are sampled to reduce the computational complexity. This brings in the similar issue of information loss from the logical side.Variational Inference for \nMLN. The second type extends Markov Logic \nNetwork (MLN) (Richardson and Domingos, 2006) \ninstead. Several methods including pGAT (Har-\nsha Vardhan et al., 2020), ExpressGNN (Zhang \net al., 2019) and pLogicNet (Qu and Tang, 2019) \nare proposed to leverage graph embedding to de-\nfine variational distribution for all possible hidden \ntriples to conduct variational inference of MLN. \nA detailed introduction of these methods can be \nfound in 4 A Unified Framework for Knowledge \nGraph Inference: UniKER \n\nRather than follow probabilistic logic to integrate \nlogical rules and KGE, we show that by leverag-\ning the nice properties of definite Horn rules, there \nis a much simpler way to directly derive an opti-\nmal boolean solution for MAX-SAT problem. To \ncapture the mutual interaction between KGE and \nlogical inference, we proposed an iterative mecha-\nnism, which ensures that UniKER is more potent \nthan a simple union of KGE and logical rules. \n\n4.1 Update KG via Forward Chaining-based \nLogical Reasoning \n\nWe prove that there exists a truth assignment that \nsatisfies all ground rules when restricting logical \nrules to be definite Horn rules. Detailed proof can \nbe found in Appendix D. The next question is how \n\nto conduct such an assignment efficiently. We de-\nnote the satisfying truth assignment as v T \n\nH \n\n *  (hidden \n\ntriples that are true) and v F \n\nH \n\n *  (hidden triples that are \n\nfalse), i.e., v T \n\nH \n\n *  = {r k (e i , e j ) = 1 | r k (e i , e j ) \u2208 \n\nv H } and v F \n\nH \n\n *  = {r k (e i , e j ) = 0 | r k (e i , e j ) \u2208 \n\nv H }. An existing algorithm called forward chain-\ning (Salvat and Mugnier, 1996) can derive v T \n\nH \n\n *  \n\nand v F \n\nH \n\n *  efficiently. Starting from known facts \n\n(e.g., liveIn (Mina Miller, USA), officialLanguage \n(USA, English)), it triggers all ground rules whose \npremises are satisfied (e.g., speakLanguage (Mina \nMiller, English) \u2190 liveIn (Mina Miller, USA) \u2227 \nofficialLanguage (USA, \n\n\nKG embedding learning based on O; Compute \u2206\u2212 and update O \u2190 O \u2212 \u2206\u2212; Compute \u2206+ and update O \u2190 O \u222a \u2206+; 8 endAlgorithm 1: Learning Procedure of UniKER \n\nInput: Observed facts in knowledge bases O; \nthreshold to eliminate noise \u03b8%; threshold to \ninclude useful hidden triples \u03a8; a set of \ndefinite Horn rules F \nOutput: KG embeddings \n1 for t = 1 : MAX_ITER do \n\n2 \n\n// Update KG via Logical Reasoning \n\n3 \n\nDerive v T \n\nH \n\nt *  from O and update O \u2190 O \u222a v T \n\nH \n\nt *  \n\n4 \n\n// Update KG via Embedding-based Inference \n\n5 \n\n6 \n\n7 \n\n\n\n\nThen, the newly inferred triples v TH \n\nt *  are added \n\nto KG by updating O = O \u222a v T \n\nH \n\nt *  . Second, we \n\nfocus on embedding-based inference to update KG. \nKGE can be learned based on the updated KG after \nthe first step. With the learned embeddings, \u2206 \u2212 , \nwhich is the bottom \u03b8% triples with lowest predic-\ntion scores, can be eliminated from O. Meanwhile, \n\u2206 + , which are potentially useful hidden triples, can \nbe added to O. \n\n\n\n\nKALE (Guo et al., 2016)   \u00d7 \u00d7 RUGE(Guo et al., 2017)   \u00d7 Rockt\u00e4schel et al.(Rockt\u00e4schel et al., 2015)   \u00d7 \u00d7 More details about datasets, rule generation and examples of logical rules are provided in Appendix I and J.5.Compared Methods. We evaluate our proposed method against SOTA algorithms, including (1) basic KGE models (e.g.,RESCAL (Nickel et al.,   Categories \nMethods \nInteractive \nExact Logical \nInference \n\nPSL-based \nRegularization \n\nEmbedding-based \nVariational \nInference to MLN \n\npLogicNet (Qu and Tang, 2019) \n\u00d7 \nExpressGNN (Zhang et al., 2019) \n\u00d7 \npGAT (Harsha Vardhan et al., 2020) \n\u00d7 \n\nOur Proposed Method \nUniKER \n\nTable 1: Capabilities of different methods. \n\n5 Experiments \n\n5.1 Experimental Setup \n\nDatasets. We implement experiments on three \nlarge-scale real-world KGs (i.e., Family (Den-\nham, 1973), FB15k-237 (Bordes et al., 2013) and \nWN18RR (Bordes et al., 2013)). AMIE+ (Gal\u00e1r-\nraga et al., 2015) is used to generate candidate \nrules automatically. \n\n\nResults on FB15k-237 and WN18RR are taken from the original papers.Model \nFamily \nFB15k-237 \nWN18RR \nHit@1 Hit@10 MRR Hit@1 Hit@10 MRR Hit@1 Hit@10 MRR \n\nRESCAL \n0.489 \n0.894 \n0.639 \n0.108 \n0.322 \n0.179 \n0.123 \n0.239 \n0.162 \nSimplE \n0.335 \n0.888 \n0.528 \n0.150 \n0.443 \n0.249 \n0.290 \n0.351 \n0.311 \nHypER  \u2020 \n0.364 \n0.903 \n0.551 \n0.252 \n0.520 \n0.341 \n0.436 \n0.522 \n0.465 \nTuckER  \u2020 \n0.373 \n0.898 \n0.567 \n0.266 \n0.544 \n0.358 \n0.443 \n0.526 \n0.470 \n\nBLP  \u2020 \n-\n-\n-\n0.062 \n0.150 \n0.092 \n0.187 \n0.358 \n0.254 \nMLN \n0.655 \n0.732 \n0.694 \n0.067 \n0.160 \n0.098 \n0.191 \n0.361 \n0.259 \n\nKALE \n0.433 \n0.869 \n0.598 \n0.131 \n0.424 \n0.230 \n0.032 \n0.353 \n0.172 \nRUGE \n0.495 \n0.962 \n0.677 \n0.098 \n0.376 \n0.191 \n0.251 \n0.327 \n0.280 \n\nExpressGNN \n0.105 \n0.282 \n0.164 \n0.150 \n0.317 \n0.207 \n0.036 \n0.093 \n0.054 \npLogicNet \n0.683 \n0.874 \n0.768 \n0.261 \n0.567 \n0.364 \n0.301 \n0.410 \n0.340 \npGAT  \u2020 \n-\n-\n-\n0.377 \n0.609 \n0.457 \n0.395 \n0.578 \n0.459 \n\nBoxE  \u2020 \n-\n-\n-\n-\n0.538 \n0.337 \n-\n0.541 \n0.451 \n\nTransE \n0.221 \n0.874 \n0.453 \n0.231 \n0.527 \n0.330 \n0.007 \n0.406 \n0.165 \nUniKER-TransE \n0.873 \n0.971 \n0.916 \n0.463 \n0.630 \n0.522 \n0.040 \n0.561 \n0.307 \nDistMult \n0.360 \n0.885 \n0.543 \n0.220 \n0.486 \n0.308 \n0.304 \n0.409 \n0.338 \nUniKER-DistMult \n0.770 \n0.945 \n0.823 \n0.507 \n0.587 \n0.533 \n0.432 \n0.538 \n0.485 \nRotatE \n0.787 \n0.933 \n0.862 \n0.237 \n0.526 \n0.334 \n0.421 \n0.563 \n0.469 \nUniKER-RotatE \n0.886 \n0.971 \n0.924 \n0.495 \n0.612 \n0.539 \n0.437 \n0.580 \n0.492 \n\n \u2020 \n\nTable 2 :\n2Effectiveness on KG completion taskModel \n\u03b8 \nHit@1 \nHit@10 \nMRR \n\nTransE \n-\n0.026 \n0.800 \n0.319 \n\n10 \n0.286 \n0.776 \n0.466 \n20 \n0.311 \n0.816 \n0.503 \nUniKER-TransE \n30 \n0.322 \n0.833 \n0.520 \n40 \n0.352 \n0.812 \n0.523 \n50 \n0.292 \n0.791 \n0.486 \n\n\n\nTable 4 :\n4Results of Reasoning on Family Dataset with Different Thresholds (\u03c8%) to Include Useful Hidden Triples.\n\nTable 5 ,\n5we can observe that although the precision slightly decreases, UniKER outperforms forward chaining with significant performance gain in termsModel \nPrecision \nRecall \nF1 \n\nForward Chaining \n1.000 \n0.919 \n0.958 \nUniKER-TransE \n0.991 \n0.955 \n0.973 \n\n\n\nTable 5 :\n5UniKER-TransE v. Forward Chaining on Familydataset (whose test set only retains triples that can be inferred \nby logical rules) on triple True/False classification task. \n\nModel \nHit@1 \nHit@3 \nHit@10 \nMRR \n\nTransE \n0.267 \n0.651 \n0.803 \n0.476 \nUniKER-TransE \n0.710 \n0.866 \n0.904 \n0.816 \n\n\n\nTable 6 :\n6Results of reasoning of UniKER-TransE v.s. TransE On the other hand, logical rules are useful to gather more reliable triples for KGE by exploiting symbolic compositionality of KG relations, which leads to the enhancement of the reasoning ability of KGE model. To investigate the added value brought by logical rules over KGE, we evaluate UniKER-TransE against TransE on Family dataset on the KG completion task.on Family dataset (whose test set eliminates triples that can \nderive from logical rules). \n\nof recall and F1, which validates the enhancement \nbrought by KGE model over logical inference. \nEnhancement of KGE via Logical Inference. \n\n\n\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247-1250. ACM. Chain Monte Carlo. 2004. Markov chain monte carlo and gibbs sampling. Lecture notes for EEB, 581. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Twenty-ninth AAAI conference on artificial intelligence.Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by translating on hyperplanes. In Twenty-Eighth AAAI conference on artificial intelligence.Stephen H Bach, Matthias Broecheler, Bert Huang, \nand Lise Getoor. 2015. Hinge-loss markov random \nfields and probabilistic soft logic. arXiv preprint \narXiv:1505.04406. \n\nIvana Bala\u017eevi\u0107, Carl Allen, and Timothy M \nHospedales. 2019a. \nHypernetwork knowledge \ngraph embeddings. In International Conference \non Artificial Neural Networks, pages 553-565. \nSpringer. \n\nIvana Bala\u017eevi\u0107, Carl Allen, and Timothy M \nHospedales. 2019b. Tucker: Tensor factorization \nfor knowledge graph completion. arXiv preprint \narXiv:1901.09590. \n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko. \n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in neural information \nprocessing systems, pages 2787-2795. \n\nLuc De Raedt and Kristian Kersting. 2008. Probabilis-\ntic inductive logic programming. In Probabilistic In-\nductive Logic Programming, pages 1-27. Springer. \n\nLuc Dehaspe and Hannu Toivonen. 1999. Discovery of \nfrequent datalog patterns. Data Mining and knowl-\nedge discovery, 3(1):7-36. \n\nThomas Demeester, Tim Rockt\u00e4schel, and Sebastian \nRiedel. 2016. Lifted rule injection for relation em-\nbeddings. arXiv preprint arXiv:1606.08359. \n\nWoodrow W Denham. 1973. The detection of patterns \nin Alyawara nonverbal behavior. Ph.D. thesis, Uni-\nversity of Washington, Seattle. \n\nLuis Gal\u00e1rraga, Christina Teflioudi, Katja Hose, and \nFabian M Suchanek. 2015. Fast rule mining in on-\ntological knowledge bases with amie+. The VLDB \nJournal-The International Journal on Very Large \nData Bases, 24(6):707-730. \n\nShu Guo, Quan Wang, Lihong Wang, Bin Wang, and \nLi Guo. 2016. Jointly embedding knowledge graphs \nand logical rules. pages 192-202. \n\nShu Guo, Quan Wang, Lihong Wang, Bin Wang, and \nLi Guo. 2017. Knowledge graph embedding with \niterative guidance from soft rules. \n\nL Vivek Harsha Vardhan, Guo Jia, and Stanley Kok. \n2020. Probabilistic logic graph attention networks \nfor reasoning. In Companion Proceedings of the \nWeb Conference 2020, WWW '20, page 669-673, \nNew York, NY, USA. Association for Computing \nMachinery. \n\nSeyed Mehran Kazemi and David Poole. 2018. Simple \nembedding for link prediction in knowledge graphs. \nIn Advances in Neural Information Processing Sys-\ntems, pages 4284-4295. \n\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A \nmethod for stochastic optimization. arXiv preprint \narXiv:1412.6980. \n\nGeorge J Klir and Bo Yuan. 1996. Fuzzy sets and fuzzy \nlogic: theory and applications. Possibility Theory \nversus Probab. Theory, 32(2):207-208. \n\nRadford M Neal and Geoffrey E Hinton. 1998. A view \nof the em algorithm that justifies incremental, sparse, \nand other variants. In Learning in graphical models, \npages 355-368. Springer. \n\nMaximilian Nickel, Volker Tresp, and Hans-Peter \nKriegel. 2011. A three-way model for collective \nlearning on multi-relational data. In ICML, vol-\nume 11, pages 809-816. \n\nFeng Niu, Christopher R\u00e9, AnHai Doan, and Jude Shav-\nlik. 2011. Tuffy: Scaling up statistical inference in \nmarkov logic networks using an rdbms. Proceed-\nings of the VLDB Endowment, 4(6):373-384. \n\nHoifung Poon and Pedro Domingos. 2006. Sound and \nefficient inference with probabilistic and determinis-\ntic dependencies. In AAAI, volume 6, pages 458-\n463. \n\nMeng Qu and Jian Tang. 2019. Probabilistic logic neu-\nral networks for reasoning. \n\nMatthew Richardson and Pedro Domingos. 2006. \nMarkov logic networks. Machine learning, 62(1-\n2):107-136. \n\nTim Rockt\u00e4schel, Sameer Singh, and Sebastian Riedel. \n2015. Injecting logical background knowledge into \nembeddings for relation extraction. In Proceedings \nof the 2015 Conference of the North American Chap-\nter of the Association for Computational Linguistics: \nHuman Language Technologies, pages 1119-1129. \nEric Salvat and Marie-Laure Mugnier. 1996. Sound \nand complete forward and backward chainings of \ngraph rules. In International Conference on Concep-\ntual Structures, pages 248-262. Springer. \n\nSolomon Eyal Shimony. 1994. Finding maps for be-\nlief networks is np-hard. Artificial intelligence, \n68(2):399-410. \n\nParag Singla and Pedro M Domingos. 2008. Lifted \nfirst-order belief propagation. In AAAI, volume 8, \npages 1094-1099. \n\nFabian M Suchanek, Gjergji Kasneci, and Gerhard \nWeikum. 2007. YAGO: a core of semantic knowl-\nedge. In Proceedings of the 16th international con-\nference on World Wide Web, pages 697-706. ACM. \n\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian \nTang. 2019. Rotate: Knowledge graph embed-\nding by relational rotation in complex space. arXiv \npreprint arXiv:1902.10197. \n\nTh\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric \nGaussier, and Guillaume Bouchard. 2016. Com-\nplex embeddings for simple link prediction. In In-\nternational Conference on Machine Learning, pages \n2071-2080. \n\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng \nGao, and Li Deng. 2014. Embedding entities and \nrelations for learning and inference in knowledge \nbases. arXiv preprint arXiv:1412.6575. \n\nJonathan S Yedidia, William T Freeman, and Yair \nWeiss. 2001. Generalized belief propagation. In \nAdvances in neural information processing systems, \npages 689-695. \n\nYuyu Zhang, Xinshi Chen, Yuan Yang, Arun Rama-\nmurthy, Bo Li, Yuan Qi, and Le Song. 2019. Can \ngraph neural networks help logic reasoning? \n\n\n)\nqj = 0} can be inferred. Input: A set of chain-like definite Horn rules F; A table T = {k : M (k) }B Dynamic Programming to Calculate \nM for Long Rules \n\nConsidering that the body of a chain-like defi-\nnite Horn rules F k can be regard as sequence \nof relations, which can be denoted as B(F k ) = \n[r n , . . . , r m ]. As mentioned in subsection Includ-\ning Potential Useful Hidden Triples, to identify \nall \"potential useful\" triples for rule F k , we have \nto compute all possible sub-sequences of its body. \nTo alleviate computational complexity, we adopt \na dynamic programming algorithm. In particular, \nwe maintain a table T to record all previous calcu-\nlation and compute new results based on T . The \nkeys of T are all possible sub-sequences of B(F k ) \nfor all rules F k and the values of T are the corre-\nsponding M matrix. We initialize T by including \nadjacent matrix corresponding to all relations in \nthe KG (e.g., T = {k : M (k) } \n\n|R| \n\nk=1 ). \n\nAlgorithm 2: Dynamic Programming to Calcu-\n\nlate M \n|R| \nk=1 \n\n\n\nTable 7 :\n7Summary of PSL-based Regularization Approaches.\n\n\nForward Chaining Traditional Logical Inference language(person, language) \u21d0 liveIn(person, country) \u2227 officialLanguage (country, language) language(person, language) \u21d0 liveIn(person, country) \u2227 officialLanguage (country, language) Language (Thomas Alva Edison, English) \u21d0 liveIn (Thomas Alva Edison, USA) \u2227 officialLanguage (USA, English) \u2026 Language (Thomas Alva Edison, Dansk) \u21d0 liveIn (Thomas Alva Edison, USA) \u2227 officialLanguage (USA, Dansk) liveIn (Thomas Alva Edison, USA) T liveIn (Thomas Alva Edison, Denmark) ? \u2026 liveIn (Valdemar Poulsen , Canada) ? officialLanguage (USA, English) T officialLanguage (USA, Dansk) ? \u2026 officialLanguage (Canada, Dansk) ?Entities \nCountry \nUSA, Denmark, Canada \nPerson \nThomas Alva Edison, Mary Stilwell, \nMina Miller, Valdemar Poulsen \nLanguage \nEnglish, Dansk \n\nKnowledge Graph \n\nlanguage(person, language) \u21d0 liveIn(person, country) \u2227 \nofficialLanguage (country, language) \n\nPredicates \nisMarriedTo \nliveIn \nofficialLanguage \nlanguage \n\nObserved Facts \nisMarriedTo (Thomas Alva Edison, Mary Stilwell) \nisMarriedTo (Thomas Alva Edison, Mina Miller) \nlanguage (Valdemar Poulsen, Dansk) \nliveIn (Mina Miller, USA) \nliveIn (Valdemar Poulsen, Denmark) \nofficialLanguage (USA, English) \nofficialLanguage (Canada, English) \n\nperson = Mina Miller \nperson = Valdemar Poulsen \n\ncountry = USA \ncountry = Canada \n\nlanguage(person, language) \u21d0 liveIn(person, country) \u2227 \nofficialLanguage (country, language) \n\ncountry = USA \ncountry = Denmark \n\nLanguage (Mina \nMiller, English) \n\nofficialLanguage (USA, English) \nofficialLanguage (Canada, English) \n\ncountry = USA \n\nlanguage = English \n\nperson = \nMina Miller \n\nAll ground predicates \n\nNew fact \n\ncountry = USA \ncountry = Denmark \n\nAll ground rules \n\nTruth value : T \n\nSAT Solver \n\n\n\nTable 8 :\n8The best hyperparameter setting of UniKER on several benchmarks.Model \n\u03c8 \nHit@1 Hit@10 MRR \n\nUniKER-TransE 10 \n0.873 \n0.971 \n0.916 \nUniKER-TransE 20 \n0.878 \n0.972 \n0.919 \nUniKER-TransE 30 \n0.873 \n0.972 \n0.916 \nUniKER-TransE 40 \n0.874 \n0.973 \n0.917 \nUniKER-TransE 50 \n0.871 \n0.970 \n0.915 \n\n\n\nTable 9 :\n9Results of Reasoning on Family Dataset with Different Thresholds (\u03c8%) to Include Useful Hidden Triples.\n\n\nthe-art inference algorithms for MLN (e.g.,MCMC (Carlo, 2004),MC-SAT (Poon and Domingos, 2006),BP (Yedidia  et al., 2001), liftedBP (Singla and Domingos, 2008)   andTuffy (Niu et al., 2011)) over all the six datasets given inTable 12. Details of each baseline are listed below: \u2022 Markov Chain Monte Carlo (MCMC) (Carlo, 2004) is the most widely used method for approximate inference in MLN. The basic idea is to utilize Markov blanket to calculate marginal probabilities of ground predicates. \u2022 MC-SAT (Poon and Domingos, 2006) is an efficient MCMC algorithm that combines slice sampling with satisfiability testing. \u2022 Belief Propagation (BP) (Yedidia et al., 2001) is another widely used method for approximate inference in MLN. It is a messagepassing algorithm for performing inference on graphical models. \u2022 lifted Belief Propagation (liftedBP) (Singla and Domingos, 2008) is a lifted version of belief propagation algorithm. \u2022 Tuffy (Niu et al., 2011) is an open-source MLN inference engine that achieves scalability compared to prior art implementations.\n\n\nModelTime per Epoch #Epochs for ConvergenceKALE \n>1000 s \n500 \nRUGE \n>1000 s \n800 \nExpressGNN \n168 s \n200 \npLogicNet \n7.2 s \n600 \nUniKER-TransE \n6.5 s \n400 \n\n\n\nTable 10 :\n10Efficiency Analysis on Family Dataset.\n\n\nexperiments are of length at most three. Several logical rule examples for each dataset are presented in Table 15. Logical Rule-based Regularization KALE (Guo et al., 2016) O(ned + nrd) O(\u03b8nlne l+1 ) O(ntd + \u03b8nlne l+1 d) Rockt\u00e4schel et al. (Rockt\u00e4schel et al., 2015) O(ned + nrd + nenr) O(nlnea l ) O(nenrd + ned 2 + nrd 2 + nlnea l d) RUGE (Guo et al., 2017) O(ned + nrd) O(\u03b8nlne l+1 ) O(ntd + \u03b8nlne l+1 d)Method \nSpace Complexity \nTime Complexity \nMaterialisation \nEmbedding \n\nKGE \nTransE (Bordes et al., 2013) \nO(ned + nrd) \n-\nO(ntd) \nDismult (Yang et al., 2014) \nO(ned + nrd) \n-\nO(ntd) \n\nOur Model \nUniKER \nO(ned + nrd) \nO(nlnea l ) \nO(ntd + nlnea l d) \n\n\n\nTable 11 :\n11Comparison of Space and Time Complexity for Model Training.Dataset \nType \n#Entity #Relation #Triple #Rule \n\nRC1000 \nCitation network \n656 \n4 \n1006 \n3 \nsub-Family \nFamily network \n68 \n12 \n412 \n41 \nsub-YAGO3-10 YAGO knowledge \n55 \n8 \n61 \n5 \nFamily \nFamily network \n3007 \n12 \n28356 \n41 \nFB15k-237 \nFreebase knowledge \n14541 \n237 \n310116 \n300 \nWN18RR \nLexical network \n40943 \n11 \n93003 \n11 \n\n\n\nTable 12 :\n12Data Statistics. sub-YAGO3-10 sub-Family RC1000 Family FB15k-237 WN18RRModel \nMCMC \n76433s \n-\n-\n-\n-\n-\nMCSAT \n1292s \n25912s \n-\n-\n-\n-\nBP \n10s \n16343s \n-\n-\n-\n-\nliftedBP \n15s \n16075s \n-\n-\n-\n-\n\nTuffy \n0.849s \n1.398s \n4.899s \n-\n-\n-\n\nForward Chaining \n0.003s \n0.034s \n0.007s \n0.593s \n186s \n30s \n\n\n\nTable 13 :\n13Comparison of Inference Time for Forward Chaining vs. MLN.#Rule \n5 \n10 \n20 \n30 \n35 \n36 \n38 \n41 \n\n#Inferred Triple 870 6276 7915 10973 15302 18429 19780 21549 \n\n\n\nTable 14 :\n14Coverage of Different Number of Rules on Family Dataset.Dateset \nLogical Rule Examples \n\n\n\nTable 15 :\n15Example of Logical Rules on Family, FB15k-237 and WN18RR Datasets.\nAcknowledgementsThis work was partially supported by NSF III-1705169, NSF 1937599, DARPA HR00112090027, Okawa Foundation Grant, and Amazon Research Awards.\nRalph Abboud, Thomas Ismail Ilkan Ceylan, Tommaso Lukasiewicz, Salvatori, arXiv:2007.06267Boxe: A box embedding model for knowledge base completion. arXiv preprintRalph Abboud, Ismail Ilkan Ceylan, Thomas Lukasiewicz, and Tommaso Salvatori. 2020. Boxe: A box embedding model for knowledge base completion. arXiv preprint arXiv:2007.06267.\n\nDbpedia: A nucleus for a web of open data. S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The semantic web. SpringerS\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722-735. Springer.\n", "annotations": {"author": "[{\"end\":331,\"start\":235},{\"end\":464,\"start\":332},{\"end\":554,\"start\":465},{\"end\":668,\"start\":555}]", "publisher": "[{\"end\":157,\"start\":116},{\"end\":984,\"start\":943}]", "author_last_name": "[{\"end\":246,\"start\":241},{\"end\":343,\"start\":339},{\"end\":475,\"start\":470},{\"end\":565,\"start\":562}]", "author_first_name": "[{\"end\":240,\"start\":235},{\"end\":338,\"start\":332},{\"end\":469,\"start\":465},{\"end\":561,\"start\":555}]", "author_affiliation": "[{\"end\":330,\"start\":248},{\"end\":463,\"start\":387},{\"end\":553,\"start\":477},{\"end\":667,\"start\":585}]", "title": "[{\"end\":115,\"start\":1},{\"end\":783,\"start\":669}]", "venue": "[{\"end\":871,\"start\":785}]", "abstract": "[{\"end\":3353,\"start\":1014}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3516,\"start\":3497},{\"end\":3539,\"start\":3516},{\"end\":3561,\"start\":3539},{\"end\":3846,\"start\":3825},{\"end\":3864,\"start\":3846},{\"end\":3882,\"start\":3864},{\"end\":3899,\"start\":3882},{\"end\":4735,\"start\":4704},{\"end\":4763,\"start\":4735},{\"end\":6255,\"start\":6237},{\"end\":6280,\"start\":6255},{\"end\":6303,\"start\":6280},{\"end\":6409,\"start\":6390},{\"end\":6428,\"start\":6409},{\"end\":6456,\"start\":6428},{\"end\":9962,\"start\":9934},{\"end\":9996,\"start\":9967},{\"end\":11284,\"start\":11256},{\"end\":11312,\"start\":11286},{\"end\":11339,\"start\":11314},{\"end\":11369,\"start\":11341},{\"end\":11409,\"start\":11371},{\"end\":11434,\"start\":11409},{\"end\":12284,\"start\":12265},{\"end\":12422,\"start\":12399},{\"end\":12447,\"start\":12424},{\"end\":12497,\"start\":12452},{\"end\":12939,\"start\":12918},{\"end\":13013,\"start\":12978},{\"end\":13460,\"start\":13442},{\"end\":13485,\"start\":13460},{\"end\":31908,\"start\":31886},{\"end\":32316,\"start\":32297},{\"end\":34432,\"start\":34409},{\"end\":35747,\"start\":35721},{\"end\":36227,\"start\":36195},{\"end\":37034,\"start\":37003},{\"end\":37858,\"start\":37827},{\"end\":37973,\"start\":37944},{\"end\":38007,\"start\":37975},{\"end\":38041,\"start\":38022},{\"end\":39133,\"start\":39110},{\"end\":40752,\"start\":40730},{\"end\":43227,\"start\":43213},{\"end\":44756,\"start\":44727},{\"end\":47847,\"start\":47819},{\"end\":48989,\"start\":48965},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":50748,\"start\":50727}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":49963,\"start\":49828},{\"attributes\":{\"id\":\"fig_1\"},\"end\":50953,\"start\":49964},{\"attributes\":{\"id\":\"fig_2\"},\"end\":51038,\"start\":50954},{\"attributes\":{\"id\":\"fig_3\"},\"end\":51227,\"start\":51039},{\"attributes\":{\"id\":\"fig_4\"},\"end\":51319,\"start\":51228},{\"attributes\":{\"id\":\"fig_5\"},\"end\":51357,\"start\":51320},{\"attributes\":{\"id\":\"fig_6\"},\"end\":51550,\"start\":51358},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51663,\"start\":51551},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51772,\"start\":51664},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54283,\"start\":51773},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":54817,\"start\":54284},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":55260,\"start\":54818},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56240,\"start\":55261},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":57661,\"start\":56241},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":57913,\"start\":57662},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":58029,\"start\":57914},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":58290,\"start\":58030},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":58590,\"start\":58291},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":59248,\"start\":58591},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":65268,\"start\":59249},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":66300,\"start\":65269},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":66360,\"start\":66301},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":68122,\"start\":66361},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":68424,\"start\":68123},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":68540,\"start\":68425},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":69602,\"start\":68541},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":69763,\"start\":69603},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":69816,\"start\":69764},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":70478,\"start\":69817},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":70881,\"start\":70479},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":71185,\"start\":70882},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":71360,\"start\":71186},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":71464,\"start\":71361},{\"attributes\":{\"id\":\"tab_28\",\"type\":\"table\"},\"end\":71545,\"start\":71465}]", "paragraph": "[{\"end\":3731,\"start\":3369},{\"end\":4554,\"start\":3733},{\"end\":5738,\"start\":4556},{\"end\":6754,\"start\":5740},{\"end\":7704,\"start\":6756},{\"end\":7850,\"start\":7706},{\"end\":8035,\"start\":7852},{\"end\":8170,\"start\":8037},{\"end\":9382,\"start\":8241},{\"end\":10225,\"start\":9557},{\"end\":10547,\"start\":10282},{\"end\":10867,\"start\":10621},{\"end\":11476,\"start\":10897},{\"end\":12028,\"start\":11508},{\"end\":14311,\"start\":12030},{\"end\":14579,\"start\":14331},{\"end\":15117,\"start\":14581},{\"end\":15664,\"start\":15152},{\"end\":18028,\"start\":15666},{\"end\":18941,\"start\":18030},{\"end\":19648,\"start\":19008},{\"end\":19908,\"start\":19778},{\"end\":20830,\"start\":19928},{\"end\":20981,\"start\":20832},{\"end\":21846,\"start\":21021},{\"end\":22778,\"start\":21848},{\"end\":23461,\"start\":22796},{\"end\":24103,\"start\":23463},{\"end\":24752,\"start\":24105},{\"end\":25517,\"start\":24754},{\"end\":26082,\"start\":25541},{\"end\":26370,\"start\":26084},{\"end\":27801,\"start\":26427},{\"end\":28136,\"start\":27816},{\"end\":28481,\"start\":28190},{\"end\":29476,\"start\":28483},{\"end\":30318,\"start\":29478},{\"end\":31028,\"start\":30605},{\"end\":31282,\"start\":31141},{\"end\":31535,\"start\":31419},{\"end\":31640,\"start\":31537},{\"end\":32142,\"start\":31777},{\"end\":32380,\"start\":32232},{\"end\":32634,\"start\":32482},{\"end\":33053,\"start\":32712},{\"end\":33164,\"start\":33055},{\"end\":33795,\"start\":33166},{\"end\":36831,\"start\":33849},{\"end\":36894,\"start\":36864},{\"end\":37279,\"start\":36896},{\"end\":37418,\"start\":37281},{\"end\":38345,\"start\":37509},{\"end\":38658,\"start\":38430},{\"end\":39419,\"start\":38776},{\"end\":39664,\"start\":39456},{\"end\":40804,\"start\":39706},{\"end\":42001,\"start\":40867},{\"end\":42183,\"start\":42003},{\"end\":42834,\"start\":42205},{\"end\":42975,\"start\":42836},{\"end\":43146,\"start\":42977},{\"end\":43294,\"start\":43148},{\"end\":43537,\"start\":43296},{\"end\":43850,\"start\":43539},{\"end\":43875,\"start\":43852},{\"end\":44877,\"start\":43922},{\"end\":45862,\"start\":44879},{\"end\":46630,\"start\":45940},{\"end\":47090,\"start\":46658},{\"end\":48063,\"start\":47152},{\"end\":48465,\"start\":48065},{\"end\":48843,\"start\":48467},{\"end\":49282,\"start\":48896},{\"end\":49293,\"start\":49284},{\"end\":49444,\"start\":49295},{\"end\":49566,\"start\":49446},{\"end\":49626,\"start\":49568},{\"end\":49827,\"start\":49637}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9512,\"start\":9383},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10281,\"start\":10226},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10600,\"start\":10548},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15151,\"start\":15118},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19741,\"start\":19649},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19927,\"start\":19909},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21020,\"start\":20982},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30334,\"start\":30319},{\"attributes\":{\"id\":\"formula_9\"},\"end\":30514,\"start\":30334},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30604,\"start\":30514},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31140,\"start\":31029},{\"attributes\":{\"id\":\"formula_12\"},\"end\":31418,\"start\":31283},{\"attributes\":{\"id\":\"formula_13\"},\"end\":31776,\"start\":31641},{\"attributes\":{\"id\":\"formula_14\"},\"end\":32231,\"start\":32143},{\"attributes\":{\"id\":\"formula_15\"},\"end\":32481,\"start\":32381},{\"attributes\":{\"id\":\"formula_16\"},\"end\":37508,\"start\":37419},{\"attributes\":{\"id\":\"formula_17\"},\"end\":38429,\"start\":38346},{\"attributes\":{\"id\":\"formula_18\"},\"end\":38775,\"start\":38659},{\"attributes\":{\"id\":\"formula_19\"},\"end\":39455,\"start\":39420}]", "table_ref": "[{\"end\":22120,\"start\":22113},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":22951,\"start\":22944},{\"end\":23748,\"start\":23741},{\"end\":24531,\"start\":24524},{\"attributes\":{\"ref_id\":\"tab_19\"},\"end\":25319,\"start\":25312},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":27611,\"start\":27604},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":36830,\"start\":36823},{\"attributes\":{\"ref_id\":\"tab_24\"},\"end\":41561,\"start\":41553},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":42326,\"start\":42318},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":45434,\"start\":45427},{\"attributes\":{\"ref_id\":\"tab_19\"},\"end\":46432,\"start\":46425},{\"attributes\":{\"ref_id\":\"tab_26\"},\"end\":46793,\"start\":46785},{\"attributes\":{\"ref_id\":\"tab_22\"},\"end\":46907,\"start\":46899},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":47939,\"start\":47931}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3367,\"start\":3355},{\"attributes\":{\"n\":\"2\"},\"end\":8186,\"start\":8173},{\"attributes\":{\"n\":\"2.1\"},\"end\":8239,\"start\":8189},{\"attributes\":{\"n\":\"2.2\"},\"end\":9555,\"start\":9514},{\"attributes\":{\"n\":\"2.3\"},\"end\":10619,\"start\":10602},{\"attributes\":{\"n\":\"2.4\"},\"end\":10895,\"start\":10870},{\"attributes\":{\"n\":\"3\"},\"end\":11506,\"start\":11479},{\"end\":14329,\"start\":14314},{\"attributes\":{\"n\":\"4.3\"},\"end\":19006,\"start\":18944},{\"attributes\":{\"n\":\"4.4\"},\"end\":19776,\"start\":19743},{\"attributes\":{\"n\":\"5.2\"},\"end\":22794,\"start\":22781},{\"attributes\":{\"n\":\"5.3\"},\"end\":25539,\"start\":25520},{\"attributes\":{\"n\":\"5.4\"},\"end\":26425,\"start\":26373},{\"attributes\":{\"n\":\"6\"},\"end\":27814,\"start\":27804},{\"end\":28188,\"start\":28139},{\"end\":32710,\"start\":32637},{\"end\":33847,\"start\":33798},{\"end\":36862,\"start\":36834},{\"end\":39704,\"start\":39667},{\"end\":40865,\"start\":40807},{\"end\":42203,\"start\":42186},{\"end\":43920,\"start\":43878},{\"end\":45938,\"start\":45865},{\"end\":46656,\"start\":46633},{\"end\":47150,\"start\":47093},{\"end\":48894,\"start\":48846},{\"end\":49635,\"start\":49629},{\"end\":49839,\"start\":49829},{\"end\":50965,\"start\":50955},{\"end\":51050,\"start\":51040},{\"end\":51234,\"start\":51229},{\"end\":51562,\"start\":51552},{\"end\":51675,\"start\":51665},{\"end\":57672,\"start\":57663},{\"end\":57924,\"start\":57915},{\"end\":58040,\"start\":58031},{\"end\":58301,\"start\":58292},{\"end\":58601,\"start\":58592},{\"end\":65271,\"start\":65270},{\"end\":66311,\"start\":66302},{\"end\":68133,\"start\":68124},{\"end\":68435,\"start\":68426},{\"end\":69775,\"start\":69765},{\"end\":70490,\"start\":70480},{\"end\":70893,\"start\":70883},{\"end\":71197,\"start\":71187},{\"end\":71372,\"start\":71362},{\"end\":71476,\"start\":71466}]", "table": "[{\"end\":54283,\"start\":52310},{\"end\":54817,\"start\":54393},{\"end\":55260,\"start\":54856},{\"end\":56240,\"start\":55618},{\"end\":57661,\"start\":56310},{\"end\":57913,\"start\":57709},{\"end\":58290,\"start\":58183},{\"end\":58590,\"start\":58346},{\"end\":59248,\"start\":59015},{\"end\":65268,\"start\":60002},{\"end\":66300,\"start\":65371},{\"end\":68122,\"start\":67023},{\"end\":68424,\"start\":68199},{\"end\":69763,\"start\":69648},{\"end\":70478,\"start\":70226},{\"end\":70881,\"start\":70552},{\"end\":71185,\"start\":70967},{\"end\":71360,\"start\":71258},{\"end\":71464,\"start\":71431}]", "figure_caption": "[{\"end\":49963,\"start\":49841},{\"end\":50953,\"start\":49966},{\"end\":51038,\"start\":50967},{\"end\":51227,\"start\":51052},{\"end\":51319,\"start\":51236},{\"end\":51357,\"start\":51322},{\"end\":51550,\"start\":51360},{\"end\":51663,\"start\":51564},{\"end\":51772,\"start\":51677},{\"end\":52310,\"start\":51775},{\"end\":54393,\"start\":54286},{\"end\":54856,\"start\":54820},{\"end\":55618,\"start\":55263},{\"end\":56310,\"start\":56243},{\"end\":57709,\"start\":57674},{\"end\":58029,\"start\":57926},{\"end\":58183,\"start\":58042},{\"end\":58346,\"start\":58303},{\"end\":59015,\"start\":58603},{\"end\":60002,\"start\":59251},{\"end\":65371,\"start\":65272},{\"end\":66360,\"start\":66313},{\"end\":67023,\"start\":66363},{\"end\":68199,\"start\":68135},{\"end\":68540,\"start\":68437},{\"end\":69602,\"start\":68543},{\"end\":69648,\"start\":69605},{\"end\":69816,\"start\":69778},{\"end\":70226,\"start\":69819},{\"end\":70552,\"start\":70493},{\"end\":70967,\"start\":70896},{\"end\":71258,\"start\":71200},{\"end\":71431,\"start\":71375},{\"end\":71545,\"start\":71479}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":4216,\"start\":4210},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":4783,\"start\":4777},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":5511,\"start\":5505},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":5907,\"start\":5901},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":6050,\"start\":6041},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":7173,\"start\":7167},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":7581,\"start\":7572},{\"end\":13496,\"start\":13488},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13650,\"start\":13644},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17459,\"start\":17453},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19378,\"start\":19372},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23982,\"start\":23976},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26110,\"start\":26104},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28443,\"start\":28437},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28744,\"start\":28738},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29669,\"start\":29664},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39960,\"start\":39954},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":40236,\"start\":40230},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":47961,\"start\":47953}]", "bib_author_first_name": "[{\"end\":71707,\"start\":71702},{\"end\":71722,\"start\":71716},{\"end\":71751,\"start\":71744},{\"end\":72090,\"start\":72085},{\"end\":72106,\"start\":72097},{\"end\":72120,\"start\":72114},{\"end\":72136,\"start\":72132},{\"end\":72153,\"start\":72146},{\"end\":72171,\"start\":72164}]", "bib_author_last_name": "[{\"end\":71714,\"start\":71708},{\"end\":71742,\"start\":71723},{\"end\":71763,\"start\":71752},{\"end\":71774,\"start\":71765},{\"end\":72095,\"start\":72091},{\"end\":72112,\"start\":72107},{\"end\":72130,\"start\":72121},{\"end\":72144,\"start\":72137},{\"end\":72162,\"start\":72154},{\"end\":72176,\"start\":72172}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2007.06267\",\"id\":\"b0\"},\"end\":72040,\"start\":71702},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7278297},\"end\":72395,\"start\":72042}]", "bib_title": "[{\"end\":72083,\"start\":72042}]", "bib_author": "[{\"end\":71716,\"start\":71702},{\"end\":71744,\"start\":71716},{\"end\":71765,\"start\":71744},{\"end\":71776,\"start\":71765},{\"end\":72097,\"start\":72085},{\"end\":72114,\"start\":72097},{\"end\":72132,\"start\":72114},{\"end\":72146,\"start\":72132},{\"end\":72164,\"start\":72146},{\"end\":72178,\"start\":72164}]", "bib_venue": "[{\"end\":71849,\"start\":71792},{\"end\":72194,\"start\":72178}]"}}}, "year": 2023, "month": 12, "day": 17}