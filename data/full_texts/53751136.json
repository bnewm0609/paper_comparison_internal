{"id": 53751136, "updated": "2023-09-30 08:29:30.174", "metadata": {"title": "Noise2Void - Learning Denoising from Single Noisy Images", "authors": "[{\"first\":\"Alexander\",\"last\":\"Krull\",\"middle\":[]},{\"first\":\"Tim-Oliver\",\"last\":\"Buchholz\",\"middle\":[]},{\"first\":\"Florian\",\"last\":\"Jug\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 11, "day": 27}, "abstract": "The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1811.10980", "mag": "2949866910", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/KrullBJ19", "doi": "10.1109/cvpr.2019.00223"}}, "content": {"source": {"pdf_hash": "0852797ec9bd25b3e618a7c58f8de2246623a7b4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1811.10980v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1811.10980", "status": "GREEN"}}, "grobid": {"id": "abae8c7d355d195aa3b2c41dd5e49eb45aae9e56", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0852797ec9bd25b3e618a7c58f8de2246623a7b4.txt", "contents": "\nNoise2Void -Learning Denoising from Single Noisy Images\n\n\nAlexander Krull 1krull@mpi-cbg.de \nAuthors contributed equally MPI-CBG/PKS (CSBD)\nDresdenGermany\n\nTim-Oliver Buchholz \nAuthors contributed equally MPI-CBG/PKS (CSBD)\nDresdenGermany\n\nFlorian Jug \nAuthors contributed equally MPI-CBG/PKS (CSBD)\nDresdenGermany\n\nNoise2Void -Learning Denoising from Single Noisy Images\n\nThe field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as NOISE2NOISE (N2N). Here, we introduce NOISE2VOID (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of NOISE2VOID drops in moderation and compares favorably to training-free denoising methods.Ground Truth BSD68 Input BM3D\n\nIntroduction\n\nImage denoising is the task of inspecting a noisy image x = s + n in order to separate it into two components: its signal s and the signal degrading noise n we would like to remove. Denoising methods typically rely on the assumption that pixel values in s are not statistically independent. In other words, observing the image context of an unobserved pixel might very well allow us to make sensible predictions on the pixel intensity.\n\nA large body of work (e.g. [16,19]) explicitly modeled these interdependencies via Markov Random Fields (MRFs). In recent years, convolutional neural networks (CNNs) have been trained in various ways to predict pixel values from surrounding image patches, i.e. from the recep-noisy clean Traditional Traditional Traditional Traditional Traditional Traditional Traditional Traditional Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional Traditional Traditional Traditional Traditional Traditional Traditional Traditional Traditional   Input Input Input Input Input Input Input Input Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input Input Input Input Input Input Input Input Input   noisy  noisy   NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE   noisy  void   NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID Figure 1: Training schemes for CNN-based denoising. Traditionally, training networks for denoising requires pairs of noisy and clean images. For many practical applications, however, clean target images are not available. NOISE2NOISE (N2N) [12] enables the training of CNNs from independent pairs of noisy images. Still, also noisy image pairs are not usually available. This motivated us to propose NOISE2VOID (N2V), a novel training procedure that does not require noisy image pairs, nor clean target images. By enabling CNNs to be trained directly on a body of noisy images, we open the door to a plethora of new applications, e.g. on biomedical data.\n\ntive field of that pixel [24,11,26,6,23,25,18,14]. Typically, such systems require training pairs (x j , s j ) of noisy input images x j and their respective clean target images s j (ground truth). Network parameters are then tuned to minimize an adequately formulated error metric (loss) between network predictions and known ground truth.\n\nWhenever ground truth images are not available, these methods cannot be trained and are therefore rendered useless for the denoising task at hand. Recent work by Lehtinen et al. [12] offers an elegant solution for this problem. Instead of training a CNN to map noisy inputs to clean ground truth images, their NOISE2NOISE (N2N) train-ing attempts to learn a mapping between pairs of independently degraded versions of the same training image, i.e. (s + n, s + n ), that incorporate the same signal s, but independently drawn noise n and n . Naturally, a neural network cannot learn to perfectly predict one noisy image from another one. However, networks trained on this impossible training task can produce results that converge to the same predictions as traditionally trained networks that do have access to ground truth images [12]. In cases where ground truth data is physically unobtainable, N2N can still enable the training of denoising networks. However, this requires that two images capturing the same content (s) with independent noises (n, n ) can be acquired [3].\n\nDespite these advantages of N2N training, there are at least two shortcomings to this approach: (i) N2N training requires the availability of pairs of noisy images, and (ii) the acquisition of such pairs with (quasi) constant s is only possible for (quasi) static scenes.\n\nHere we present NOISE2VOID (N2V), a novel training scheme that overcomes both limitations. Just as N2N, also N2V leverages on the observation that high quality denoising models can be trained without the availability of clean ground truth data. However, unlike N2N or traditional training, N2V can also be applied to data for which neither noisy image pairs nor clean target images are available, i.e. N2V is a self-supervised training method. In this work we make two simple statistical assumptions: (i) the signal s is not pixel-wise independent, (ii) the noise n is conditionally pixel-wise independent given the signal s.\n\nWe evaluate the performance of N2V on the BSD68 dataset [17] and simulated microscopy data 1 . We then compare our results to the ones obtained by a traditionally trained network [24], a N2N trained network and several self-supervised methods like BM3D [5], non-local means [2], and to mean-and median-filters. While it cannot be expected that our approach outperforms methods that have additional information available during training, we observe that the denoising performance of our results only drops moderately and is still outperforming BM3D.\n\nAdditionally, we apply N2V training and prediction to three biomedical datasets: cryo-TEM images from [3], and two datasets from the Cell Tracking Challenge 2 [20]. For all these examples, the traditional training scheme cannot be applied due to the lack of ground truth data and N2N training is only applicable on the cryo-TEM data. This demonstrates the tremendous practical utility of our method.\n\nIn summary, our main contributions are: \u2022 Introduction of NOISE2VOID, a novel approach for training denoising CNNs that requires only a body of single, noisy images. \u2022 Comparison of our N2V trained denoising results 1 For simulated microscopy data we know the perfect ground truth. 2 http://celltrackingchallenge.net/ to results obtained with existing CNN training schemes [24,12,25] and non-trained methods [18,2]. \u2022 A sound theoretical motivation for our approach as well as a detailed description of an efficient implementation. The remaining manuscript is structured as follows: Section 2 contains a brief overview of related work. In Section 3, we introduce the baseline methods we later compare our own results to. This is followed by a detailed description of our proposed method and its efficient implementation. All experiments and their results are described in Section 4, and our findings are finally discussed in Section 5.\n\n\nRelated Work\n\nBelow, we will discuss other methods that consider not the denoising task as mentioned above, but instead the more general task of image restoration. This includes the removal of perturbations such as JPEG artifacts or blur. With N2V we have to stick to the more narrow task of denoising, as we rely on the fact that multiple noisy observations can help us to retrieve the true signal [12]. This is not the case for general perturbations such as blur.\n\nWe see N2V at the intersection of multiple methodological categories. We will briefly discuss the most relevant works in each of them. Note that N2N is omitted here, as it has been discussed above.\n\nIn concurrent work [1], Batson et al. also introduce a method for self-supervised training of neural networks and other systems that is based on the idea of removing parts of the input. They show that this scheme can not only be applied by removing pixels, but also groups of variables in general.\n\n\nDiscriminative Deep Learning Methods\n\nDiscriminative deep learning methods are trained offline, extracting information from ground truth annotated training sets before they are applied to test data.\n\nIn [9], Jain et al. first apply CNNs for the denoising task. They introduce the basic setup that is still used by successful methods today: Denoising is seen as a regression task and the CNN learns to minimize a loss calculated between its prediction and clean ground truth data.\n\nIn [25], Zhang et al. achieve state-of-the-art results, by introducing a very deep CNN architecture for denoising. The approach is based on the idea of residual learning [7]. Their CNN attempts to predict not the clean signal, but instead the noise at every pixel, allowing for the computation of the signal in a subsequent step. This structure allows them to train a single CNN for denoising of images corrupted by a wide range of noise levels. Their architecture completely dispenses with pooling layers.\n\nAt about the same time Mao et al. introduce a complementary very deep encoder-decoder-architecture [14] for the denoising task. They too make use of residual learning, but do so by introducing symmetric skip connections between the corresponding encoding and decoding modules. Just as [25], they are able to use a single network for various levels of noise.\n\nIn [18] Tai et al. use recurrent persistent memory units as part of their architecture, and further improve on previous methods.\n\nRecently Weigert et al. presented the CARE software framework for image restoration in the context of fluorescence microscopy data [24]. They acquire their training data by recording pairs of low-and high-exposure-images. This can be a difficult procedure since the biological sample must not move between exposures. We use their implementation as starting point for our experiments, including their specific U-Net [15] architecture.\n\nNote that N2V could in principle be applied with any of the mentioned architectures. However, [18] and [25] present an interesting peculiarity in this respect, as their residual architecture requires knowledge of the noisy input at each pixel. In N2V, this input is masked when the gradient is calculated (see Section 3).\n\n\nInternal Statistics Methods\n\nInternal Statistics Methods do not have to be trained on ground truth data beforehand. Instead, they can be directly applied to a test image where they extract all required information [27]. N2V can be seen as member of this category, as it enables training directly on a test image.\n\nIn [2], Buades et al. introduced non-local means, a classic denoising approach. Like N2V, this method predicts pixel values based on their noisy surroundings. BM3D, introduced by Dabov et al. [5], is a classic internal statistics based method. It is based on the idea, that natural images usually contain repeated patterns. BM3D performs denoising of an image by grouping similar patterns together and jointly filtering them. The downside of this approach is the computational cost during test time. In contrast, N2V requires extensive computation only during training. Once a CNN is trained for a particular kind of data, it can be applied efficiently to any number of additional images.\n\nIn [21], Ulyanov et al. show that the structure of CNNs, inherently resonates with the distribution of natural images and can be utilized for image restoration without requiring additional training data. They feed a random but constant input into a CNN and train it to approximate a single noisy image as output. Ulyanov et al. find that when they interrupt the training process at the right moment before convergence, the network produces a regularized denoised image as output.\n\n\nGenerative Models\n\nIn [4], Chen et al. present an image restoration approach based on generative adversarial networks (GANs). The authors use unpaired training samples consisting of noisy and clean images. The GAN-generator learns to generate noise and create pairs of corresponding clean and noisy images, which are in turn used as training data in a traditional supervised setup. Unlike N2V, this approach requires clean images during training.\n\nFinally, we want to mention the work by Van Den Oord et al. [22]. They present a generative model that is not used for denoising, but in spirit similar to N2V. Like N2V, Van Den Oord et al. train a neural network to predict an unseen pixel value based on its surroundings. The network is then used to generate synthetic images. However, while we train our network for a regression task, they predict a probability distribution for each pixel. Another difference lies in the structure of the receptive fields. While Van Den Oord et al. use an asymmetric structure that is shifted over the image, we always mask the central pixel in a square receptive field.\n\n\nMethods\n\nHere, we will begin by discussing our image formation model. Then, we will give a short recap of the traditional CNN training and of the N2N method. Finally, we will introduce N2V and its implementation.\n\n\nImage Formation\n\nWe see the generation of an image x = s + n as a draw from the joint distribution p(s, n) = p(s)p(n|s).\n\n(1)\n\nWe assume p(s) to be an arbitrary distribution satisfying\np(s i |s j ) = p(s i ),(2)\nfor two pixels i and j within a certain radius of each other. That is, the pixels s i of the signal are not statistically independent. With respect to the noise n, we assume a conditional distribution of the form\np(n|s) = i p(n i |s i ).(3)\nThat is, pixels values n i of the noise are conditionally independent given the signal. We furthermore assume the noise to be zero-mean\nE [n i ] = 0,(4)\nwhich leads to\nE [x i ] = s i .(5)\nIn other words, if we were to acquire multiple images with the same signal, but different realizations of noise and average them, the result would approach the true signal. An example of this would be recording multiple photographs of a static scene using a fixed tripod-mounted camera.\n\n\nTraditional Supervised Training\n\nWe are now interested in training a CNN to implement a mapping from x to s. We will assume a fully convolutional network (FCN) [13], taking one image as input and predicting another one as output.\n\nHere we want to take a slightly different but equivalent view on such a network. Every pixel prediction\u015d i in the output of the CNN is has a certain receptive field x RF(i) of input pixels, i.e. the set of pixels that influence the pixel prediction. A pixel's receptive field is usually a square patch around that pixel.\n\nBased on this consideration, we can also see our CNN as a function that takes a patch x RF(i) as input and outputs a prediction\u015d i for the single pixel i located at the patch center. Following this view, the denoising of an entire image can be achieved by extracting overlapping patches and feeding them to the network one by one. Consequently, we can define the CNN as the function\nf (x RF(i) ; \u03b8) =\u015d i ,(6)\nwhere \u03b8 denotes the vector of CNN parameters we would like to train.\n\nIn traditional supervised training we are presented with a set of training pairs (x j , s j ), each consisting of a noisy input image x j and a clean ground truth target s j . By again applying our patch-based view of the CNN, we can see our training data as pairs (x j RF(i) , s j i ). Where x j RF(i) is a patch around pixel i, extracted from training input image x j , and s j i is the corresponding target pixel value, extracted from the ground truth image s j at same position. We now use these pairs to tune the parameters \u03b8 to minimize pixel-wise loss arg min\n\u03b8 j i L f (x j RF(i) ; \u03b8) =\u015d j i , s j i .(7)\nHere we consider the standard MSE loss\nL \u015d j i , s j i = (\u015d j i \u2212 s j i ) 2 .(8)\n\nNoise2Noise Training\n\nNow let us consider the training procedure according to [12]. N2N allows us to cope without clean ground truth training data. Instead we start out with noisy image pairs (x j , x j ), where\n\nx j = s j + n j and x j = s j + n j ,\n\nthat is the two training images are identical up to their noise components n j and n j , which are, in our image generation If we train such a network using the same noisy image as input and as target, the network will degenerate and simply learn the identity. (b) In a blind-spot network, as we propose it, the receptive field of each pixel excludes the pixel itself, preventing it from learning the identity. We show that blind-spot networks can learn to remove pixel wise independent noise when they are trained on the same noisy images as input and target.\n\nmodel, just two independent samples from the same distribution (see Eq. 3).\n\nWe can now again apply our patch-based perspective and view our training data as pairs (x j RF(i) , x j i ) consisting of a noisy input patch x j RF(i) , extracted from x j , and a noisy target x j i , taken from x j at the position i. As in traditional training, we tune our parameters to minimize a loss, similar to Eq. 7, this time however using our noisy target x j i instead of the ground truth signal s j i . Even though we are attempting to learn a mapping from a noisy input to a noisy target, the training will still converge to the correct solution.\n\nThe key to this phenomenon lies in the fact that the expected value of the noisy input is equal to the clean signal [12] (see Eq. 5).\n\n\nNoise2Void Training\n\nHere, we go a step further. We propose to derive both parts of our training sample, the input and the target, from a single noisy training image x j . If we were to simply extract a patch as input and use its center pixel as target, our network would just learn the identity, by directly mapping the value at the center of the input patch to the output (see Figure 2 a).\n\nTo understand how training from single noisy images is possible nonetheless, let us assume that we use a network architecture with a special receptive field. We assume the receptive fieldx RF(i) of this network to have a blind-spot in its center. The CNN prediction\u015d i for a pixel is affected by all input pixels in a square neighborhood except for the input pixel x i at its very location. We term this type of network blind-spot network (see Figure 2 \n\n\nb).\n\nA blind-spot network can be trained using any of the training schemes described above. Like with a normal network, we can apply the traditional training or N2N, using a clean target, or a noisy target respectively. The blindspot network has a little bit less information available for its predictions, and we can expect its accuracy to be slightly impaired compared to a normal network. Considering however that only one pixel out of the entire receptive field is removed, we can assume it to still perform reasonably well.\n\nThe essential advantage of the blind-spot architecture is its inability to learn the identity. Let us consider why this is the case. Since we assume the noise to be pixel-wise independent given the signal (see Eq. 3), the neighboring pixels carry no information about the value of n i . It is thus impossible for the network to produce an estimate that is better than its a priori expected value (see Eq. 4).\n\nThe signal however is assumed to contain statistical dependencies (see Eq. 2). As a result, the network can still estimate the signal s i of a pixel by looking at its surroundings.\n\nConsequently, a blind-spot network allows us to extract the input patch and target value from the same noisy training image. We can train it by minimizing the empirical risk arg min\n\u03b8 j i L f (x j RF(i) ; \u03b8), x j i .(10)\nNote that the target x j i , is just as good as the N2N target x j i , which has to be extracted from a second noisy image. This becomes clear when we consider Eqs. 9 and 3: The two target values x j i and x j i have an equal signal s j i and their noise components are just two independent samples from the same distribution p(n i |s j i ). We have seen that a blind-spot network can in principle be trained using only individual noisy training images. However, implementing such a network that can still operate efficiently is not trivial. We propose a masking scheme to avoid this problem and achieve the same properties with any standard CNN: We replace the value in the center of each input patch with a randomly selected value form the surrounding area (see supplementary material for details). This effectively erases the pixel's information and prevents the network from learning the identity.\n\n\nImplementation Details\n\nIf we implement the above training scheme naively, it is unfortunately still not very efficient: We have to process an entire patch to calculate the gradients for a single output pixel. To mitigate this issue, we use the following approximation technique: Given a noisy training image x i , we randomly extract patches of size 64 \u00d7 64 pixels, which are bigger than our networks receptive field (see supplementary material for details). Within each patch we randomly select N pixels, using stratified sampling to avoid clustering. We then mask these pixels and use the original noisy input values as targets at their position (see Figure 3). Further details on the masking scheme can be found in the supplementary note. We can now simultaneously calculate the gradients for all of them, while ignoring the rest of the predicted image. This is achieved using the standard Keras pipeline with a specialized loss function that is zero for all but the selected pixels. We use the CSBDeep framework [23] as basis for our implementation. Following the standard CSBDeep setup, we use a U-Net [15] architecture, to which we added batch normalization [8] before each activation function.\n\n\nExperiments\n\nWe evaluate NOISE2VOID on natural images, simulated biological image data, and acquired microscopy images. N2V results are then compared to results of traditional and NOISE2NOISE training, as well as results of training-free denoising methods like BM3D, non-local means, and meanand median filters. Please refer to the supplementary material for more details on all experiments.\n\n\nDenoising of BSD68 Data\n\nFor the evaluation on natural image data we follow the example of [25] and take 400 gray scale images with 180 \u00d7 180 pixels as our training dataset. For testing we use the gray scale version of the BSD68 dataset. Noisy versions of all images are generated by adding zero mean Gaussian noise with standard deviation \u03c3 = 25. Furthermore, we used data augmentation on the training dataset. More precisely, we rotated each image three times by 90 \u2022 and also 4: Results and average PSNR values obtained by BM3D, traditionally trained, N2N trained, and N2V trained denoising networks. For BSD68 data and simulated data all methods are applicable. For cryo-TEM data ground truth images are unobtainable. Since pairs of noisy images are available, we can still perform NOISE2NOISE training. Red, yellow, and blue arrowheads indicate an ice artifact, two tubulin protofilaments that are known to be 4nm apart, and a 10nm gold bead, respectively. For the CTC-MSC and CTC-N2DH data only single noisy images exist. Hence, neither traditional nor N2N training is applicable, while our proposed training scheme can still be applied. added all mirrored versions. During training we draw random 64 \u00d7 64 pixel patches from this augmented training dataset.\n\nThe network architecture we use for all BSD68 experiments is a U-Net [15] with depth 2, kernel size 3, batch normalization, and a linear activation function in the last layer. The network has 96 feature maps on the initial level, which get doubled while the network gets deeper. We use a learning rate of 0.0004 and the default CSBDeep learning rate schedule, halving the learning rate when a plateau on the validation loss is detected.\n\nWe used batch size 128 for traditional training and batch size 16 for NOISE2NOISE, where we found that a larger batch leads to slightly diminished results. For NOISE2VOID training we use a batch size of 128 and simultaneously manipulate N = 64 pixels per input patch (see Section 3.5), as before with an initial learning rate of 0.0004.\n\nIn the first row of Figure 4, we compare our results to the ones obtained by BM3D, traditional training, and NOISE2NOISE training. We report the average PSNR numbers on each dataset. As mentioned earlier, N2V is not expected to outperform other training methods, as it can utilize less information for its prediction. Still, here we observe that the denoising performance of N2V drops moderately below the performance of BM3D (which is not the case for other data).\n\n\nDenoising of Simulated Microscopy Data\n\nThe acquisition of close to ground truth quality microscopy data is either impossible or at the very least, difficult and expensive. Since we need ground truth data to compute desired PSNR values, we decided to use a simulated dataset for our second set of experiments. To this end, we simulated membrane labeled cells epithelia and mimicked the typical image degradation of fluorescence microscopy by first applying Poisson noise and then adding zero mean Gaussian noise. We used this simulation scheme to generate high-SNR ground truth images and two corresponding low-SNR input images. This data enables us to perform traditional, N2N, as well as N2V training. We used the same data augmentation scheme as described in Section 4.1.\n\nThe network architecture we use for all experiments on simulated data is a U-Net [15] of depth 2, kernel size 5, batch norm, 32 initial feature maps, and a linear activation function in the last layer. Traditional and NOISE2NOISE training was performed with batch size 16 and an initial learning rate of 0.0004. The NOISE2VOID training was performed with a batch size of 128. We simultaneously manipulate N = 64 pixels per input patch (see Section 3.5). We again use the standard CSBDeep learning rate schedule for all three training methods.\n\nIn the second row of Figure 4 one can appreciate the denoising quality of NOISE2VOID training, which reaches virtually the same quality as traditional and NOISE2NOISE training. All trained networks clearly outperform the results obtained by BM3D.\n\n\nDenoising of Real Microscopy Data\n\nAs mentioned in the previous section, ground truth quality microscopy data is typically not available. Hence, we can no longer compute PSNR values.\n\nThe network architecture we use for all experiments on real microscopy data is a U-Net [15] of depth 2, kernel size 3, batch norm, 32 initial feature maps, and a linear activation function in the last layer. For an efficient training of NOISE2VOID we simultaneously manipulate N = 64 pixels per input patch (see Section 3.5). We use a batch size of 128 and a initial learning rate of 0.0004. For all three tasks we extracted random patches of 64 \u00d7 64 pixels and augmented them as described in previous sections.\n\n\nCryo-TEM Data\n\nIn cryo-TEM, the acquisition of high-SNR images is not possible due to beam induced damage [10]. Buchholz et al. show in [3] how NOISE2NOISE training can be applied to data acquired with a direct electron detector. To enable a qualitative assessment, we applied N2V to the same data as in [3].\n\nIn the third row of Figure 4, we show the raw image data, results obtained by BM3D, NOISE2NOISE results of [3], and our NOISE2VOID results. The runtime of both trained methods is roughly equal and about 25 times faster then the one of BM3D. For better orientation we marked some known structures in the shown cryo-TEM image (see figure caption for details). Unlike BM3D, the N2V trained network is able to preserve these as good as the N2N baseline.\n\n\nFluorescence Microscopy Data\n\nFinally, we tested NOISE2VOID on fluorescence microscopy data from the Cell Tracking Challenge. More specifically, we used the datasets Fluo-C2DL-MSC (CTC-MSC) and Fluo-N2DH-GOWT1 (CTC-N2DH). As before, no ground truth images or second noisy images are available. Hence, only BM3D and N2V training can be applied to this data.\n\nIn the last two rows of Figure 4, we compare our results to BM3D. In the absence of ground truth data, we can only judge the results visually. We find that the N2V trained network gives subjectively smooth and appealing result, while requiring only a fraction of the BM3D runtime. \n\n\nErrors and Limitations\n\nWe want to start this section by showing extreme error cases of N2V trained network predictions on real images (for which our training method performs least convincing). Figure 5 shows the ground truth image, and prediction results of traditionally trained and N2V trained networks. While the upper row contains the image with the largest squared single pixel error, the lower row shows the image with the largest sum of squared pixel errors.\n\nWe see these errors as an excellent illustration, showing a limitation of the N2V method. One of the underlying assumptions of N2V is the predictability of the signal s (see Eq. 2). Both test images shown in Figure 5 include high irregularities, that are difficult to predict. The more difficult it is to predict a pixel's signal from its surroundings the more errors are expected to appear in N2V predictions. This is of course true for traditional training and N2N as well. However, while these methods can utilize the value in the center pixel of the receptive field, this value is blocked for N2V. In Figure 6, we illustrate another limitation of our method. N2V cannot distinguish between the signal and structured noise that violates the assumption of pixel-wise independence (see Eq. 3). We demonstrate this behaviour using artificially generated structured noise applied to an image. The N2V trained CNN removes the unpredictable components of the noise, but reveals the hidden pattern. Interestingly, we find the same phenomenon in real microscopy data from the Fluo-C2DL-MSC dataset. Denoising with a N2V trained CNN reveals a systematic error of the imaging system, visible as a striped pattern.\n\n\nPerformance over Various Noise Levels\n\nWe additionally ran our method and multiple baselines, including mean and median filters, as well as the classical non-local means [2], on the BSD68 dataset using various levels of noise. To find the optimal parameter h for nonlocal means we performed a grid search. We also include a  Figure 7: Performance of N2V on the BSD68 dataset compared to various baselines. Left: Average PSNR values as a function of the amount of added Gaussian noise. We consider square mean and median filters of 3, 5, and 7 pixels width/height, and show the best avg. PSNR for each noise level. * : Method uses ground truth for training; \u2020: uses noisy image pairs; \u2021: uses only single noisy images. Right: Qualitative results of the best performing mean filer, median filter, and N2V on an image with Gaussian noise (std. 40).\n\ncomparison to DnCNN using the numbers reported in [25]. All results can be found in Figure 7.\n\n\nConclusion\n\nWe have introduced NOISE2VOID, a novel training scheme that only requires single noisy acquisitions to train denoising CNNs. We have demonstrated the applicability of N2V on a variety of imaging modalities i.e. photography, fluorescence microscopy, and cryo-Transmission Electron Microscopy. As long as our initial assumptions of a predictable signal and pixel-wise independent noise are met, N2V trained networks can compete with traditionally and N2N trained networks. Additionally, we have analyzed the behaviour of N2V training when these assumptions are violated.\n\nWe believe that the NOISE2VOID training scheme, as we propose it here, will allow us to train powerful denoising networks. We have shown multiple examples how denoising networks can be trained on the same body of data which is to be processed in the first place. Hence, N2V training will open the doors to a plethora of applications, i.e. on biomedical image data.\n\nFigure 2 :\n2A conventional network versus our proposed blind-spot network. (a) In the conventional network the prediction for an individual pixel depends an a square patch of input pixels, known as a pixel's receptive field (pixels under blue cone).\n\nFigure 3 :\n3Blind-spot masking scheme used during NOISE2VOID training. (a) A noisy training image. (b) A magnified image patch from (a). During N2V training, a randomly selected pixel is chosen (blue rectangle) and its intensity copied over to create a blind-spot (red and striped square). This modified image is then used as input image during training. (c) The target patch corresponding to (b). We use the original input with unmodified values also as target. The loss is only calculated for the blind-spot pixels we masked in (b).\n\nFigure 5 :\n5Failure cases of N2V trained networks. (a) A crop from the ground truth test image with the largest individual pixel error (indicated by red arrow). (b) Result of a traditionally trained network on the same image. (c) Result of our N2V trained network. The network fails to predict this bright and isolated pixel. (d) A crop from the ground truth test image with the largest total error. (e) Result of a traditionally trained network on the same image. (f) Result of our N2V trained network. Both networks are not able to preserve the grainy structure of the image, but the N2V trained network loses more high-frequency detail.\n\nFigure 6 :\n6Effect of structured noise on N2V trained network predictions. Structured noise violates our assumption that noise is pixel-independent (see also Eq. 3).(a) A photograph corrupted by structured noise. The hidden checkerboard pattern is barely visible. (b) The denoised result of a traditionally trained CNN. (c) The denoised result of an N2V trained CNN. The independent components of the noise are removed, but the structured components remain. (d) Structured noise in real microscopy data. (e) The denoised result of an N2V trained CNN. A hidden pattern in the noise is revealed. Note that due to the lacking training data, it is not possible to use N2N or the traditional training scheme in this case.\nAcknowledgementsWe thank Uwe Schmidt, Martin Weigert, Alexander Dibrov, and Vladimir Ulman for the helpful discussions and for their assistance in data preparation. We thank Tobias Pietzsch for proof reading.\nNoise2self: Blind denoising by selfsupervision. J Batson, L Royer, arXiv:1901.11365arXiv preprintJ. Batson and L. Royer. Noise2self: Blind denoising by self- supervision. arXiv preprint arXiv:1901.11365, 2019. 2\n\nA non-local algorithm for image denoising. A Buades, B Coll, J.-M Morel, CVPR. A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm for image denoising. In CVPR, 2005. 2, 3, 8\n\nCryocare: Content-aware image restoration for cryo-transmission electron microscopy data. T.-O Buchholz, M Jordan, G Pigino, F Jug, arXiv:1810.0542027arXiv preprintT.-O. Buchholz, M. Jordan, G. Pigino, and F. Jug. Cryo- care: Content-aware image restoration for cryo-transmission electron microscopy data. arXiv preprint arXiv:1810.05420, 2018. 2, 7\n\nImage blind denoising with generative adversarial network based noise modeling. J Chen, J Chen, H Chao, M Yang, CVPR. J. Chen, J. Chen, H. Chao, and M. Yang. Image blind denois- ing with generative adversarial network based noise model- ing. In CVPR, pages 3155-3164, 2018. 3\n\nImage denoising by sparse 3-d transform-domain collaborative filtering. K Dabov, A Foi, V Katkovnik, K Egiazarian, IEEE Transactions on image processing. 1683K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-domain collaborative fil- tering. IEEE Transactions on image processing, 16(8):2080- 2095, 2007. 2, 3\n\nS Guo, Z Yan, K Zhang, W Zuo, L Zhang, arXiv:1807.04686Toward convolutional blind denoising of real photographs. arXiv preprintS. Guo, Z. Yan, K. Zhang, W. Zuo, and L. Zhang. Toward convolutional blind denoising of real photographs. arXiv preprint arXiv:1807.04686, 2018. 1\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016. 2\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167arXiv preprintS. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 5\n\nNatural image denoising with convolutional networks. V Jain, S Seung, Advances in Neural Information Processing Systems. V. Jain and S. Seung. Natural image denoising with convo- lutional networks. In Advances in Neural Information Pro- cessing Systems, pages 769-776, 2009. 2\n\nBeam damage to organic material is considerably reduced in cryo-electron microscopy. E Knapek, J Dubochet, Journal of molecular biology. 1412E. Knapek and J. Dubochet. Beam damage to organic ma- terial is considerably reduced in cryo-electron microscopy. Journal of molecular biology, 141(2):147-161, 1980. 7\n\nUniversal denoising networks: A novel cnn architecture for image denoising. S Lefkimmiatis, CVPR. S. Lefkimmiatis. Universal denoising networks: A novel cnn architecture for image denoising. In CVPR, pages 3204- 3213, 2018. 1\n\nNoise2Noise: Learning image restoration without clean data. J Lehtinen, J Munkberg, J Hasselgren, S Laine, T Karras, M Aittala, T Aila, ICML. J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Kar- ras, M. Aittala, and T. Aila. Noise2Noise: Learning image restoration without clean data. In ICML, pages 2965-2974, 2018. 1, 2, 4\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431- 3440, 2015. 4\n\nImage restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. X Mao, C Shen, Y.-B Yang, Advances in neural information processing systems. 13X. Mao, C. Shen, and Y.-B. Yang. Image restoration us- ing very deep convolutional encoder-decoder networks with symmetric skip connections. In Advances in neural informa- tion processing systems, pages 2802-2810, 2016. 1, 3\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, MIC-CAI. Springer67O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu- tional networks for biomedical image segmentation. In MIC- CAI, pages 234-241. Springer, 2015. 3, 5, 6, 7\n\nFields of experts: A framework for learning image priors. S Roth, M J Black, CVPR. IEEE2S. Roth and M. J. Black. Fields of experts: A framework for learning image priors. In CVPR, volume 2, pages 860-867. IEEE, 2005. 1\n\nFields of experts. S Roth, M J Black, International Journal of Computer Vision. 822205S. Roth and M. J. Black. Fields of experts. International Journal of Computer Vision, 82(2):205, 2009. 2\n\nMemnet: A persistent memory network for image restoration. Y Tai, J Yang, X Liu, C Xu, CVPR. 13Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persis- tent memory network for image restoration. In CVPR, pages 4539-4547, 2017. 1, 2, 3\n\nLearning gaussian conditional random fields for low-level vision. M F Tappen, C Liu, E H Adelson, W T Freeman, CVPR. IEEEM. F. Tappen, C. Liu, E. H. Adelson, and W. T. Freeman. Learning gaussian conditional random fields for low-level vi- sion. In CVPR, pages 1-8. IEEE, 2007. 1\n\nAn objective comparison of cell-tracking algorithms. V Ulman, M Ma\u0161ka, K E Magnusson, O Ronneberger, C Haubold, N Harder, P Matula, P Matula, D Svoboda, M Radojevic, Nature methods. 14121141V. Ulman, M. Ma\u0161ka, K. E. Magnusson, O. Ronneberger, C. Haubold, N. Harder, P. Matula, P. Matula, D. Svoboda, M. Radojevic, et al. An objective comparison of cell-tracking algorithms. Nature methods, 14(12):1141, 2017. 2\n\nDeep image prior. CoRR, abs/1711.10925. D Ulyanov, A Vedaldi, V S Lempitsky, D. Ulyanov, A. Vedaldi, and V. S. Lempitsky. Deep image prior. CoRR, abs/1711.10925, 2017. 3\n\nPixel recurrent neural networks. A Van Den, N Oord, K Kalchbrenner, Kavukcuoglu, ICML. A. Van Den Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. In ICML, pages 1747-1756. JMLR.org, 2016. 3\n\nIsotropic reconstruction of 3d fluorescence microscopy images using convolutional neural networks. M Weigert, L Royer, F Jug, G Myers, MICCAI. M. Descoteaux, L. Maier-Hein, A. Franz, P. Jannin, D. L. Collins, and S. DuchesneChamSpringer International Publishing15M. Weigert, L. Royer, F. Jug, and G. Myers. Isotropic recon- struction of 3d fluorescence microscopy images using con- volutional neural networks. In M. Descoteaux, L. Maier- Hein, A. Franz, P. Jannin, D. L. Collins, and S. Duchesne, editors, MICCAI, pages 126-134, Cham, 2017. Springer In- ternational Publishing. 1, 5\n\nContent-aware image restoration: Pushing the limits of fluorescence microscopy. M Weigert, U Schmidt, T Boothe, A M\u00fcller, A Dibrov, A Jain, B Wilhelm, D Schmidt, C Broaddus, S Culley, M Rocha-Martins, F Segovia-Miranda, C Norden, R Henriques, M Zerial, M Solimena, J Rink, P Tomancak, L Royer, F Jug, E W Myers, Nature Methods. 13M. Weigert, U. Schmidt, T. Boothe, A. M\u00fcller, A. Dibrov, A. Jain, B. Wilhelm, D. Schmidt, C. Broaddus, S. Cul- ley, M. Rocha-Martins, F. Segovia-Miranda, C. Norden, R. Henriques, M. Zerial, M. Solimena, J. Rink, P. Tomancak, L. Royer, F. Jug, and E. W. Myers. Content-aware image restoration: Pushing the limits of fluorescence microscopy. Nature Methods, 2018. 1, 2, 3\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. K Zhang, W Zuo, Y Chen, D Meng, L Zhang, IEEE Transactions on Image Processing. 267K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Be- yond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing, 26(7):3142-3155, 2017. 1, 2, 3, 5, 8\n\nFfdnet: Toward a fast and flexible solution for cnn based image denoising. K Zhang, W Zuo, L Zhang, IEEE Transactions on Image Processing. 1K. Zhang, W. Zuo, and L. Zhang. Ffdnet: Toward a fast and flexible solution for cnn based image denoising. IEEE Transactions on Image Processing, 2018. 1\n\nInternal statistics of a single natural image. M Zontak, M Irani, CVPR. IEEEM. Zontak and M. Irani. Internal statistics of a single natural image. In CVPR, pages 977-984. IEEE, 2011. 3\n", "annotations": {"author": "[{\"end\":156,\"start\":59},{\"end\":240,\"start\":157},{\"end\":316,\"start\":241}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":69},{\"end\":176,\"start\":168},{\"end\":252,\"start\":249}]", "author_first_name": "[{\"end\":68,\"start\":59},{\"end\":167,\"start\":157},{\"end\":248,\"start\":241}]", "author_affiliation": "[{\"end\":155,\"start\":94},{\"end\":239,\"start\":178},{\"end\":315,\"start\":254}]", "title": "[{\"end\":56,\"start\":1},{\"end\":372,\"start\":317}]", "venue": null, "abstract": "[{\"end\":1572,\"start\":374}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2056,\"start\":2052},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2059,\"start\":2056},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4008,\"start\":4004},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4449,\"start\":4445},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4452,\"start\":4449},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4455,\"start\":4452},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4457,\"start\":4455},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4460,\"start\":4457},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4463,\"start\":4460},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4466,\"start\":4463},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4469,\"start\":4466},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4944,\"start\":4940},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5597,\"start\":5593},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5838,\"start\":5835},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6801,\"start\":6797},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6924,\"start\":6920},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6997,\"start\":6994},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7018,\"start\":7015},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7396,\"start\":7393},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7454,\"start\":7450},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7909,\"start\":7908},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8069,\"start\":8065},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8072,\"start\":8069},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8075,\"start\":8072},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8104,\"start\":8100},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8106,\"start\":8104},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9033,\"start\":9029},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9318,\"start\":9315},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9802,\"start\":9799},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10084,\"start\":10080},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10250,\"start\":10247},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10688,\"start\":10684},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10874,\"start\":10870},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10951,\"start\":10947},{\"end\":11097,\"start\":11083},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11209,\"start\":11205},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11493,\"start\":11489},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11607,\"start\":11603},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11616,\"start\":11612},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12051,\"start\":12047},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12153,\"start\":12150},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12342,\"start\":12339},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12844,\"start\":12840},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13344,\"start\":13341},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13831,\"start\":13827},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15735,\"start\":15731},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17380,\"start\":17376},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18870,\"start\":18866},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23003,\"start\":22999},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23094,\"start\":23090},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23150,\"start\":23147},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23675,\"start\":23671},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24918,\"start\":24914},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26950,\"start\":26946},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27933,\"start\":27929},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28466,\"start\":28462},{\"end\":28483,\"start\":28468},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28495,\"start\":28492},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28663,\"start\":28660},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28776,\"start\":28773},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31610,\"start\":31607},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32338,\"start\":32334}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33577,\"start\":33327},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34113,\"start\":33578},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34754,\"start\":34114},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35472,\"start\":34755}]", "paragraph": "[{\"end\":2023,\"start\":1588},{\"end\":4418,\"start\":2025},{\"end\":4760,\"start\":4420},{\"end\":5839,\"start\":4762},{\"end\":6112,\"start\":5841},{\"end\":6739,\"start\":6114},{\"end\":7289,\"start\":6741},{\"end\":7690,\"start\":7291},{\"end\":8627,\"start\":7692},{\"end\":9095,\"start\":8644},{\"end\":9294,\"start\":9097},{\"end\":9593,\"start\":9296},{\"end\":9794,\"start\":9634},{\"end\":10075,\"start\":9796},{\"end\":10583,\"start\":10077},{\"end\":10942,\"start\":10585},{\"end\":11072,\"start\":10944},{\"end\":11507,\"start\":11074},{\"end\":11830,\"start\":11509},{\"end\":12145,\"start\":11862},{\"end\":12835,\"start\":12147},{\"end\":13316,\"start\":12837},{\"end\":13765,\"start\":13338},{\"end\":14423,\"start\":13767},{\"end\":14638,\"start\":14435},{\"end\":14761,\"start\":14658},{\"end\":14766,\"start\":14763},{\"end\":14825,\"start\":14768},{\"end\":15065,\"start\":14853},{\"end\":15229,\"start\":15094},{\"end\":15261,\"start\":15247},{\"end\":15568,\"start\":15282},{\"end\":15800,\"start\":15604},{\"end\":16122,\"start\":15802},{\"end\":16506,\"start\":16124},{\"end\":16601,\"start\":16533},{\"end\":17169,\"start\":16603},{\"end\":17254,\"start\":17216},{\"end\":17509,\"start\":17320},{\"end\":17548,\"start\":17511},{\"end\":18110,\"start\":17550},{\"end\":18187,\"start\":18112},{\"end\":18748,\"start\":18189},{\"end\":18883,\"start\":18750},{\"end\":19277,\"start\":18907},{\"end\":19732,\"start\":19279},{\"end\":20263,\"start\":19740},{\"end\":20673,\"start\":20265},{\"end\":20855,\"start\":20675},{\"end\":21038,\"start\":20857},{\"end\":21979,\"start\":21078},{\"end\":23183,\"start\":22006},{\"end\":23577,\"start\":23199},{\"end\":24843,\"start\":23605},{\"end\":25281,\"start\":24845},{\"end\":25619,\"start\":25283},{\"end\":26086,\"start\":25621},{\"end\":26863,\"start\":26129},{\"end\":27407,\"start\":26865},{\"end\":27655,\"start\":27409},{\"end\":27840,\"start\":27693},{\"end\":28353,\"start\":27842},{\"end\":28664,\"start\":28371},{\"end\":29115,\"start\":28666},{\"end\":29474,\"start\":29148},{\"end\":29757,\"start\":29476},{\"end\":30226,\"start\":29784},{\"end\":31434,\"start\":30228},{\"end\":32282,\"start\":31476},{\"end\":32377,\"start\":32284},{\"end\":32960,\"start\":32392},{\"end\":33326,\"start\":32962}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14852,\"start\":14826},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15093,\"start\":15066},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15246,\"start\":15230},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15281,\"start\":15262},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16532,\"start\":16507},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17215,\"start\":17170},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17296,\"start\":17255},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21077,\"start\":21039}]", "table_ref": "[{\"end\":3664,\"start\":2313}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1586,\"start\":1574},{\"attributes\":{\"n\":\"2.\"},\"end\":8642,\"start\":8630},{\"attributes\":{\"n\":\"2.1.\"},\"end\":9632,\"start\":9596},{\"attributes\":{\"n\":\"2.2.\"},\"end\":11860,\"start\":11833},{\"attributes\":{\"n\":\"2.3.\"},\"end\":13336,\"start\":13319},{\"attributes\":{\"n\":\"3.\"},\"end\":14433,\"start\":14426},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14656,\"start\":14641},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15602,\"start\":15571},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17318,\"start\":17298},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18905,\"start\":18886},{\"end\":19738,\"start\":19735},{\"attributes\":{\"n\":\"3.5.\"},\"end\":22004,\"start\":21982},{\"attributes\":{\"n\":\"4.\"},\"end\":23197,\"start\":23186},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23603,\"start\":23580},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26127,\"start\":26089},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27691,\"start\":27658},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":28369,\"start\":28356},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":29146,\"start\":29118},{\"attributes\":{\"n\":\"4.4.\"},\"end\":29782,\"start\":29760},{\"attributes\":{\"n\":\"4.5.\"},\"end\":31474,\"start\":31437},{\"attributes\":{\"n\":\"5.\"},\"end\":32390,\"start\":32380},{\"end\":33338,\"start\":33328},{\"end\":33589,\"start\":33579},{\"end\":34125,\"start\":34115},{\"end\":34766,\"start\":34756}]", "table": null, "figure_caption": "[{\"end\":33577,\"start\":33340},{\"end\":34113,\"start\":33591},{\"end\":34754,\"start\":34127},{\"end\":35472,\"start\":34768}]", "figure_ref": "[{\"end\":3772,\"start\":3764},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19273,\"start\":19265},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19731,\"start\":19723},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22644,\"start\":22636},{\"end\":25649,\"start\":25641},{\"end\":27438,\"start\":27430},{\"end\":28694,\"start\":28686},{\"end\":29508,\"start\":29500},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29962,\"start\":29954},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30444,\"start\":30436},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30841,\"start\":30833},{\"end\":31770,\"start\":31762},{\"end\":32376,\"start\":32368}]", "bib_author_first_name": "[{\"end\":35731,\"start\":35730},{\"end\":35741,\"start\":35740},{\"end\":35939,\"start\":35938},{\"end\":35949,\"start\":35948},{\"end\":35960,\"start\":35956},{\"end\":36172,\"start\":36168},{\"end\":36184,\"start\":36183},{\"end\":36194,\"start\":36193},{\"end\":36204,\"start\":36203},{\"end\":36510,\"start\":36509},{\"end\":36518,\"start\":36517},{\"end\":36526,\"start\":36525},{\"end\":36534,\"start\":36533},{\"end\":36779,\"start\":36778},{\"end\":36788,\"start\":36787},{\"end\":36795,\"start\":36794},{\"end\":36808,\"start\":36807},{\"end\":37059,\"start\":37058},{\"end\":37066,\"start\":37065},{\"end\":37073,\"start\":37072},{\"end\":37082,\"start\":37081},{\"end\":37089,\"start\":37088},{\"end\":37380,\"start\":37379},{\"end\":37386,\"start\":37385},{\"end\":37395,\"start\":37394},{\"end\":37402,\"start\":37401},{\"end\":37625,\"start\":37624},{\"end\":37634,\"start\":37633},{\"end\":37889,\"start\":37888},{\"end\":37897,\"start\":37896},{\"end\":38199,\"start\":38198},{\"end\":38209,\"start\":38208},{\"end\":38500,\"start\":38499},{\"end\":38711,\"start\":38710},{\"end\":38723,\"start\":38722},{\"end\":38735,\"start\":38734},{\"end\":38749,\"start\":38748},{\"end\":38758,\"start\":38757},{\"end\":38768,\"start\":38767},{\"end\":38779,\"start\":38778},{\"end\":39039,\"start\":39038},{\"end\":39047,\"start\":39046},{\"end\":39060,\"start\":39059},{\"end\":39314,\"start\":39313},{\"end\":39321,\"start\":39320},{\"end\":39332,\"start\":39328},{\"end\":39684,\"start\":39683},{\"end\":39699,\"start\":39698},{\"end\":39710,\"start\":39709},{\"end\":39959,\"start\":39958},{\"end\":39967,\"start\":39966},{\"end\":39969,\"start\":39968},{\"end\":40140,\"start\":40139},{\"end\":40148,\"start\":40147},{\"end\":40150,\"start\":40149},{\"end\":40372,\"start\":40371},{\"end\":40379,\"start\":40378},{\"end\":40387,\"start\":40386},{\"end\":40394,\"start\":40393},{\"end\":40612,\"start\":40611},{\"end\":40614,\"start\":40613},{\"end\":40624,\"start\":40623},{\"end\":40631,\"start\":40630},{\"end\":40633,\"start\":40632},{\"end\":40644,\"start\":40643},{\"end\":40646,\"start\":40645},{\"end\":40879,\"start\":40878},{\"end\":40888,\"start\":40887},{\"end\":40897,\"start\":40896},{\"end\":40899,\"start\":40898},{\"end\":40912,\"start\":40911},{\"end\":40927,\"start\":40926},{\"end\":40938,\"start\":40937},{\"end\":40948,\"start\":40947},{\"end\":40958,\"start\":40957},{\"end\":40968,\"start\":40967},{\"end\":40979,\"start\":40978},{\"end\":41278,\"start\":41277},{\"end\":41289,\"start\":41288},{\"end\":41300,\"start\":41299},{\"end\":41302,\"start\":41301},{\"end\":41442,\"start\":41441},{\"end\":41453,\"start\":41452},{\"end\":41461,\"start\":41460},{\"end\":41727,\"start\":41726},{\"end\":41738,\"start\":41737},{\"end\":41747,\"start\":41746},{\"end\":41754,\"start\":41753},{\"end\":42292,\"start\":42291},{\"end\":42303,\"start\":42302},{\"end\":42314,\"start\":42313},{\"end\":42324,\"start\":42323},{\"end\":42334,\"start\":42333},{\"end\":42344,\"start\":42343},{\"end\":42352,\"start\":42351},{\"end\":42363,\"start\":42362},{\"end\":42374,\"start\":42373},{\"end\":42386,\"start\":42385},{\"end\":42396,\"start\":42395},{\"end\":42413,\"start\":42412},{\"end\":42432,\"start\":42431},{\"end\":42442,\"start\":42441},{\"end\":42455,\"start\":42454},{\"end\":42465,\"start\":42464},{\"end\":42477,\"start\":42476},{\"end\":42485,\"start\":42484},{\"end\":42497,\"start\":42496},{\"end\":42506,\"start\":42505},{\"end\":42513,\"start\":42512},{\"end\":42515,\"start\":42514},{\"end\":42992,\"start\":42991},{\"end\":43001,\"start\":43000},{\"end\":43008,\"start\":43007},{\"end\":43016,\"start\":43015},{\"end\":43024,\"start\":43023},{\"end\":43358,\"start\":43357},{\"end\":43367,\"start\":43366},{\"end\":43374,\"start\":43373},{\"end\":43625,\"start\":43624},{\"end\":43635,\"start\":43634}]", "bib_author_last_name": "[{\"end\":35738,\"start\":35732},{\"end\":35747,\"start\":35742},{\"end\":35946,\"start\":35940},{\"end\":35954,\"start\":35950},{\"end\":35966,\"start\":35961},{\"end\":36181,\"start\":36173},{\"end\":36191,\"start\":36185},{\"end\":36201,\"start\":36195},{\"end\":36208,\"start\":36205},{\"end\":36515,\"start\":36511},{\"end\":36523,\"start\":36519},{\"end\":36531,\"start\":36527},{\"end\":36539,\"start\":36535},{\"end\":36785,\"start\":36780},{\"end\":36792,\"start\":36789},{\"end\":36805,\"start\":36796},{\"end\":36819,\"start\":36809},{\"end\":37063,\"start\":37060},{\"end\":37070,\"start\":37067},{\"end\":37079,\"start\":37074},{\"end\":37086,\"start\":37083},{\"end\":37095,\"start\":37090},{\"end\":37383,\"start\":37381},{\"end\":37392,\"start\":37387},{\"end\":37399,\"start\":37396},{\"end\":37406,\"start\":37403},{\"end\":37631,\"start\":37626},{\"end\":37642,\"start\":37635},{\"end\":37894,\"start\":37890},{\"end\":37903,\"start\":37898},{\"end\":38206,\"start\":38200},{\"end\":38218,\"start\":38210},{\"end\":38513,\"start\":38501},{\"end\":38720,\"start\":38712},{\"end\":38732,\"start\":38724},{\"end\":38746,\"start\":38736},{\"end\":38755,\"start\":38750},{\"end\":38765,\"start\":38759},{\"end\":38776,\"start\":38769},{\"end\":38784,\"start\":38780},{\"end\":39044,\"start\":39040},{\"end\":39057,\"start\":39048},{\"end\":39068,\"start\":39061},{\"end\":39318,\"start\":39315},{\"end\":39326,\"start\":39322},{\"end\":39337,\"start\":39333},{\"end\":39696,\"start\":39685},{\"end\":39707,\"start\":39700},{\"end\":39715,\"start\":39711},{\"end\":39964,\"start\":39960},{\"end\":39975,\"start\":39970},{\"end\":40145,\"start\":40141},{\"end\":40156,\"start\":40151},{\"end\":40376,\"start\":40373},{\"end\":40384,\"start\":40380},{\"end\":40391,\"start\":40388},{\"end\":40397,\"start\":40395},{\"end\":40621,\"start\":40615},{\"end\":40628,\"start\":40625},{\"end\":40641,\"start\":40634},{\"end\":40654,\"start\":40647},{\"end\":40885,\"start\":40880},{\"end\":40894,\"start\":40889},{\"end\":40909,\"start\":40900},{\"end\":40924,\"start\":40913},{\"end\":40935,\"start\":40928},{\"end\":40945,\"start\":40939},{\"end\":40955,\"start\":40949},{\"end\":40965,\"start\":40959},{\"end\":40976,\"start\":40969},{\"end\":40989,\"start\":40980},{\"end\":41286,\"start\":41279},{\"end\":41297,\"start\":41290},{\"end\":41312,\"start\":41303},{\"end\":41450,\"start\":41443},{\"end\":41458,\"start\":41454},{\"end\":41474,\"start\":41462},{\"end\":41487,\"start\":41476},{\"end\":41735,\"start\":41728},{\"end\":41744,\"start\":41739},{\"end\":41751,\"start\":41748},{\"end\":41760,\"start\":41755},{\"end\":42300,\"start\":42293},{\"end\":42311,\"start\":42304},{\"end\":42321,\"start\":42315},{\"end\":42331,\"start\":42325},{\"end\":42341,\"start\":42335},{\"end\":42349,\"start\":42345},{\"end\":42360,\"start\":42353},{\"end\":42371,\"start\":42364},{\"end\":42383,\"start\":42375},{\"end\":42393,\"start\":42387},{\"end\":42410,\"start\":42397},{\"end\":42429,\"start\":42414},{\"end\":42439,\"start\":42433},{\"end\":42452,\"start\":42443},{\"end\":42462,\"start\":42456},{\"end\":42474,\"start\":42466},{\"end\":42482,\"start\":42478},{\"end\":42494,\"start\":42486},{\"end\":42503,\"start\":42498},{\"end\":42510,\"start\":42507},{\"end\":42521,\"start\":42516},{\"end\":42998,\"start\":42993},{\"end\":43005,\"start\":43002},{\"end\":43013,\"start\":43009},{\"end\":43021,\"start\":43017},{\"end\":43030,\"start\":43025},{\"end\":43364,\"start\":43359},{\"end\":43371,\"start\":43368},{\"end\":43380,\"start\":43375},{\"end\":43632,\"start\":43626},{\"end\":43641,\"start\":43636}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1901.11365\",\"id\":\"b0\"},\"end\":35893,\"start\":35682},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11206708},\"end\":36076,\"start\":35895},{\"attributes\":{\"doi\":\"arXiv:1810.05420\",\"id\":\"b2\"},\"end\":36427,\"start\":36078},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":51989956},\"end\":36704,\"start\":36429},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1475121},\"end\":37056,\"start\":36706},{\"attributes\":{\"doi\":\"arXiv:1807.04686\",\"id\":\"b5\"},\"end\":37331,\"start\":37058},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206594692},\"end\":37528,\"start\":37333},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b7\"},\"end\":37833,\"start\":37530},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14260640},\"end\":38111,\"start\":37835},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":29863013},\"end\":38421,\"start\":38113},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4350405},\"end\":38648,\"start\":38423},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3846544},\"end\":38980,\"start\":38650},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1629541},\"end\":39205,\"start\":38982},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10987457},\"end\":39616,\"start\":39207},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3719281},\"end\":39898,\"start\":39618},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2843211},\"end\":40118,\"start\":39900},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":13058320},\"end\":40310,\"start\":40120},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8550762},\"end\":40543,\"start\":40312},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1616901},\"end\":40823,\"start\":40545},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":205427260},\"end\":41235,\"start\":40825},{\"attributes\":{\"id\":\"b20\"},\"end\":41406,\"start\":41237},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8142135},\"end\":41625,\"start\":41408},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6391660},\"end\":42209,\"start\":41627},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":53749133},\"end\":42910,\"start\":42211},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":996788},\"end\":43280,\"start\":42912},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10514149},\"end\":43575,\"start\":43282},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14152023},\"end\":43761,\"start\":43577}]", "bib_title": "[{\"end\":35936,\"start\":35895},{\"end\":36507,\"start\":36429},{\"end\":36776,\"start\":36706},{\"end\":37377,\"start\":37333},{\"end\":37886,\"start\":37835},{\"end\":38196,\"start\":38113},{\"end\":38497,\"start\":38423},{\"end\":38708,\"start\":38650},{\"end\":39036,\"start\":38982},{\"end\":39311,\"start\":39207},{\"end\":39681,\"start\":39618},{\"end\":39956,\"start\":39900},{\"end\":40137,\"start\":40120},{\"end\":40369,\"start\":40312},{\"end\":40609,\"start\":40545},{\"end\":40876,\"start\":40825},{\"end\":41439,\"start\":41408},{\"end\":41724,\"start\":41627},{\"end\":42289,\"start\":42211},{\"end\":42989,\"start\":42912},{\"end\":43355,\"start\":43282},{\"end\":43622,\"start\":43577}]", "bib_author": "[{\"end\":35740,\"start\":35730},{\"end\":35749,\"start\":35740},{\"end\":35948,\"start\":35938},{\"end\":35956,\"start\":35948},{\"end\":35968,\"start\":35956},{\"end\":36183,\"start\":36168},{\"end\":36193,\"start\":36183},{\"end\":36203,\"start\":36193},{\"end\":36210,\"start\":36203},{\"end\":36517,\"start\":36509},{\"end\":36525,\"start\":36517},{\"end\":36533,\"start\":36525},{\"end\":36541,\"start\":36533},{\"end\":36787,\"start\":36778},{\"end\":36794,\"start\":36787},{\"end\":36807,\"start\":36794},{\"end\":36821,\"start\":36807},{\"end\":37065,\"start\":37058},{\"end\":37072,\"start\":37065},{\"end\":37081,\"start\":37072},{\"end\":37088,\"start\":37081},{\"end\":37097,\"start\":37088},{\"end\":37385,\"start\":37379},{\"end\":37394,\"start\":37385},{\"end\":37401,\"start\":37394},{\"end\":37408,\"start\":37401},{\"end\":37633,\"start\":37624},{\"end\":37644,\"start\":37633},{\"end\":37896,\"start\":37888},{\"end\":37905,\"start\":37896},{\"end\":38208,\"start\":38198},{\"end\":38220,\"start\":38208},{\"end\":38515,\"start\":38499},{\"end\":38722,\"start\":38710},{\"end\":38734,\"start\":38722},{\"end\":38748,\"start\":38734},{\"end\":38757,\"start\":38748},{\"end\":38767,\"start\":38757},{\"end\":38778,\"start\":38767},{\"end\":38786,\"start\":38778},{\"end\":39046,\"start\":39038},{\"end\":39059,\"start\":39046},{\"end\":39070,\"start\":39059},{\"end\":39320,\"start\":39313},{\"end\":39328,\"start\":39320},{\"end\":39339,\"start\":39328},{\"end\":39698,\"start\":39683},{\"end\":39709,\"start\":39698},{\"end\":39717,\"start\":39709},{\"end\":39966,\"start\":39958},{\"end\":39977,\"start\":39966},{\"end\":40147,\"start\":40139},{\"end\":40158,\"start\":40147},{\"end\":40378,\"start\":40371},{\"end\":40386,\"start\":40378},{\"end\":40393,\"start\":40386},{\"end\":40399,\"start\":40393},{\"end\":40623,\"start\":40611},{\"end\":40630,\"start\":40623},{\"end\":40643,\"start\":40630},{\"end\":40656,\"start\":40643},{\"end\":40887,\"start\":40878},{\"end\":40896,\"start\":40887},{\"end\":40911,\"start\":40896},{\"end\":40926,\"start\":40911},{\"end\":40937,\"start\":40926},{\"end\":40947,\"start\":40937},{\"end\":40957,\"start\":40947},{\"end\":40967,\"start\":40957},{\"end\":40978,\"start\":40967},{\"end\":40991,\"start\":40978},{\"end\":41288,\"start\":41277},{\"end\":41299,\"start\":41288},{\"end\":41314,\"start\":41299},{\"end\":41452,\"start\":41441},{\"end\":41460,\"start\":41452},{\"end\":41476,\"start\":41460},{\"end\":41489,\"start\":41476},{\"end\":41737,\"start\":41726},{\"end\":41746,\"start\":41737},{\"end\":41753,\"start\":41746},{\"end\":41762,\"start\":41753},{\"end\":42302,\"start\":42291},{\"end\":42313,\"start\":42302},{\"end\":42323,\"start\":42313},{\"end\":42333,\"start\":42323},{\"end\":42343,\"start\":42333},{\"end\":42351,\"start\":42343},{\"end\":42362,\"start\":42351},{\"end\":42373,\"start\":42362},{\"end\":42385,\"start\":42373},{\"end\":42395,\"start\":42385},{\"end\":42412,\"start\":42395},{\"end\":42431,\"start\":42412},{\"end\":42441,\"start\":42431},{\"end\":42454,\"start\":42441},{\"end\":42464,\"start\":42454},{\"end\":42476,\"start\":42464},{\"end\":42484,\"start\":42476},{\"end\":42496,\"start\":42484},{\"end\":42505,\"start\":42496},{\"end\":42512,\"start\":42505},{\"end\":42523,\"start\":42512},{\"end\":43000,\"start\":42991},{\"end\":43007,\"start\":43000},{\"end\":43015,\"start\":43007},{\"end\":43023,\"start\":43015},{\"end\":43032,\"start\":43023},{\"end\":43366,\"start\":43357},{\"end\":43373,\"start\":43366},{\"end\":43382,\"start\":43373},{\"end\":43634,\"start\":43624},{\"end\":43643,\"start\":43634}]", "bib_venue": "[{\"end\":41855,\"start\":41851},{\"end\":35728,\"start\":35682},{\"end\":35972,\"start\":35968},{\"end\":36166,\"start\":36078},{\"end\":36545,\"start\":36541},{\"end\":36858,\"start\":36821},{\"end\":37169,\"start\":37113},{\"end\":37412,\"start\":37408},{\"end\":37622,\"start\":37530},{\"end\":37954,\"start\":37905},{\"end\":38248,\"start\":38220},{\"end\":38519,\"start\":38515},{\"end\":38790,\"start\":38786},{\"end\":39074,\"start\":39070},{\"end\":39388,\"start\":39339},{\"end\":39724,\"start\":39717},{\"end\":39981,\"start\":39977},{\"end\":40198,\"start\":40158},{\"end\":40403,\"start\":40399},{\"end\":40660,\"start\":40656},{\"end\":41005,\"start\":40991},{\"end\":41275,\"start\":41237},{\"end\":41493,\"start\":41489},{\"end\":41768,\"start\":41762},{\"end\":42537,\"start\":42523},{\"end\":43069,\"start\":43032},{\"end\":43419,\"start\":43382},{\"end\":43647,\"start\":43643}]"}}}, "year": 2023, "month": 12, "day": 17}