{"id": 236976127, "updated": "2023-10-06 00:29:07.096", "metadata": {"title": "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset", "authors": "[{\"first\":\"Hasam\",\"last\":\"Khalid\",\"middle\":[]},{\"first\":\"Shahroz\",\"last\":\"Tariq\",\"middle\":[]},{\"first\":\"Minha\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Simon\",\"last\":\"Woo\",\"middle\":[\"S.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "While the significant advancements have made in the generation of deepfakes using deep learning technologies, its misuse is a well-known issue now. Deepfakes can cause severe security and privacy issues as they can be used to impersonate a person's identity in a video by replacing his/her face with another person's face. Recently, a new problem of generating synthesized human voice of a person is emerging, where AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake audios and videos, a new generation of deepfake detectors is needed to focus on both video and audio collectively. To develop a competent deepfake detector, a large amount of high-quality data is typically required to capture real-world (or practical) scenarios. Existing deepfake datasets either contain deepfake videos or audios, which are racially biased as well. As a result, it is critical to develop a high-quality video and audio deepfake dataset that can be used to detect both audio and video deepfakes simultaneously. To fill this gap, we propose a novel Audio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake videos but also respective synthesized lip-synced fake audios. We generate this dataset using the most popular deepfake generation methods. We selected real YouTube videos of celebrities with four ethnic backgrounds to develop a more realistic multimodal dataset that addresses racial bias, and further help develop multimodal deepfake detectors. We performed several experiments using state-of-the-art detection methods to evaluate our deepfake dataset and demonstrate the challenges and usefulness of our multimodal Audio-Video deepfake dataset.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2108.05080", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/KhalidTKW21", "doi": null}}, "content": {"source": {"pdf_hash": "5935d6005b6568308bf0942f2cc9bb7f9d457c11", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.05080v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ffeeec87d31152e11adcb3bd343f8580e592b29a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5935d6005b6568308bf0942f2cc9bb7f9d457c11.txt", "contents": "\nFakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset\n\n\nHasam Khalid hasam.khalid@g.skku.edu \nDepartment of Computer Science and Engineering\nSungkyunkwan University\nSouth Korea\n\nShahroz Tariq shahroz@g.skku.edu \nDepartment of Computer Science and Engineering\nSungkyunkwan University\nSouth Korea\n\nMinha Kim kimminha@g.skku.edu \nDepartment of Computer Science and Engineering\nSungkyunkwan University\nSouth Korea\n\nSimon S Woo swoo@g.skku.edu \nDepartment of Computer Science and Engineering\nSungkyunkwan University\nSouth Korea\n\nDepartment of Applied Data Science\nSungkyunkwan University\nSouth Korea\n\nDepartment of Artificial Intelligence\nSungkyunkwan University\nSouth Korea\n\nFakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset\n\nWhile the significant advancements have made in the generation of deepfakes using deep learning technologies, its misuse is a well-known issue now. Deepfakes can cause severe security and privacy issues as they can be used to impersonate a person's identity in a video by replacing his/her face with another person's face. Recently, a new problem of generating synthesized human voice of a person is emerging, where AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake audios and videos, a new generation of deepfake detectors is needed to focus on both video and audio collectively. To develop a competent deepfake detector, a large amount of high-quality data is typically required to capture real-world (or practical) scenarios. Existing deepfake datasets either contain deepfake videos or audios, which are racially biased as well. As a result, it is critical to develop a high-quality video and audio deepfake dataset that can be used to detect both audio and video deepfakes simultaneously. To fill this gap, we propose a novel Audio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake videos but also respective synthesized lip-synced fake audios. We generate this dataset using the most popular deepfake generation methods. We selected real YouTube videos of celebrities with four ethnic backgrounds to develop a more realistic multimodal dataset that addresses racial bias, and further help develop multimodal deepfake detectors. We performed several experiments using state-ofthe-art detection methods to evaluate our deepfake dataset and demonstrate the challenges and usefulness of our multimodal Audio-Video deepfake dataset.\n\nIntroduction\n\nWith the advent of new AI technologies, particularly deep neural networks (DNNs), a rise in forged or manipulated images, videos, and audios has been skyrocketed. Even though forging and manipulating images or videos has been performed in the past [1], generating highly realistic fake human face images [2] or videos [3], as well as cloning human voices [4] have become much easier and faster than before. Recently, generating deepfakes using a DNN based technique to replace a person's face with another person, has significantly increased. The most common deep learning-based generation methods make use of Autoencoders (AEs) [5], Variational Autoencoders (VAEs) [5], and Generative Adversarial Networks (GANs) [2]. These methods are used to combine or superimpose a source human face image onto a target image. In addition, the recent advancements in deep learning-based deepfake generation methods have created voice-cloned human voices in real-time [4,6], where human voice cloning is a network-based speech synthesis method to generate high-quality speech of target speakers [4]. One famous example of deepfakes is of former U.S. Presidents Barack Obama, Donald Trump, and George W. Bush, generated as a part of research [7]. The individuals in the videos can be seen speaking with very accurate lip-sync. As a result, deepfakes have become a challenging well-known technical, social, and ethical issue now. Numerous discussions are being held on state news channels and social media related to the potential harms of deepfakes [8]. A recent article was published on Forbes [9], which discussed a TV commercial ad by ESPN [10] that became a center of discussion over social media, where the ad showed footage from 1998 of an ESPN analyst making shockingly accurate predictions about the year 2020. It later turned out that the clip was fake and generated using cutting-edge AI technology [11]. Moreover, Tariq et al. [12] demonstrate how deepfakes impersonation attacks can severely impact face recognition technologies.\n\nTherefore, many ethical, security, and privacy concerns arise due to the ease of generating deepfakes. Keeping in mind the misuse of deepfakes and the potential harm they can cause, the need for deepfake detection methods is inevitable. Many researchers have dived into this domain and proposed a variety of different deepfake detection methods [12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31]. First, high quality deepfake datasets are required to create an efficient and usable deepfake detection method. Researchers have also proposed different deepfake datasets generated using the latest deepfake generation methods [32][33][34][35][36][37] to aid other fellow scholars in developing deepfake detection methods. Most of the recent deepfake detection methods [16, 20, 21, 24, 28, 28-30, 38, 39] make use of these publicly available datasets. However, these deepfake datasets only focus on generating realistic deepfake videos but do not consider generating fake audio for them. While some neural network-based synthesized audio datasets exist [40][41][42][43][44], they do not contain the respective lip-synced or deepfake videos. This limitation of deepfake datasets also hinders the development of multimodal deepfake detection methods, which can detect deepfakes that are possibly any of audio or video combinations. And the current available datasets are limited to be used for detecting only one type of deepfakes.\n\nAs per our knowledge, there exists only one deepfake dataset, Deepfake Detection Challenge (DFDC) [45], which contains a mix of deepfake video and synthesized cloned audio, or both. However, the dataset is not labeled with respect to audio and video. It is not trivial to determine if the audio was fake or the video due to the lack of labels. Therefore, we propose a novel Audio-Video Multimodal Deepfake Detection dataset (FakeAVCeleb), which contains not only deepfake videos but also respective synthesized cloned deepfake audios. Table 1 summarizes a quantitative comparison of FakeAVCeleb with other publicly available deepfake datasets. FakeAVCeleb consists of videos of celebrities with different ethnic backgrounds belonging to diverse age groups with equal proportions of each gender. To evaluate and analyze the usefulness of our dataset, we performed the comprehensive experimentation using 11 different unimodal, ensemble-based and multimodal baseline deepfake methods [13-15, 36, 46-51]. In addition, we present the detection results with other popular deepfake datasets. The main contributions of our work are summarized as follows:\n\n\u2022 We present a novel Audio-Video Multimodal Deepfake Detection dataset, FakeAVCeleb, which contains both, video and audio deepfakes with accurate lip-sync with fine-grained labels. Such a multimodal deepfake dataset has not been developed in the past.\n\n\u2022 Our FakeAVCeleb dataset contains three different types of Audio-Video deepfakes, generated from a carefully selected real YouTube video dataset using recently proposed popular deepfake generation methods.\n\n\u2022 The individuals in the dataset are selected based on five major ethnic backgrounds speaking English to eliminate racial bias issues. Moreover, we performed the comprehensive baseline benchmark evaluation and demonstrated the crucial need and usefulness for a multimodal deepfake dataset.\n\n\nBACKGROUND AND MOTIVATION\n\nThere are several publicly deepfake detection datasets proposed by different researchers [14,33,34,37,45,52]. Most of these datasets are manipulated images of a person's face, i.e., swapped with another person, and these datasets contain real and respective manipulated fake videos. Nowadays, many different methods exist to generate deepfakes [2,18]. Recently, researchers have proposed more realistic deepfake datasets with better quality and larger quantity [36,37,53]. However, their focus was to generate deepfake videos and not their respective fake audios. Moreover, these datasets either contain real audio or no audio at all. In this paper, we propose a novel deepfake audio and video dataset. We generate a cloned voice of the target speaker and apply it to lip-sync with the video using facial reenactment. As per our knowledge, this is the first of its kind dataset containing deepfake videos with their respective fake audios. We believe our dataset will help researchers develop multimodal deepfake detectors that can detect both the deepfake audio and video simultaneously.\n\nThe UADFV [14] and Deepfake TIMIT [41] are some early deepfake datasets. These datasets contain fewer real videos and respective deepfake videos, and act as baseline datasets. However, their quality and quantity are low. For example, UADFV consists of only 98 videos. Meanwhile, Deepfake TIMIT contains the audio, and the video. But their audios are real, and not synthesized or fake. In our FakeAVCeleb, we propose an Audio-Video Deepfake dataset that contains not only deepfake videos but also synthesized lip-synced fake audios.\n\nDue to the limitations of the quality and quantity of previous deepfake datasets, researchers proposed more deepfake datasets with a large number of high quality videos. FaceForensics++ (FF++) [36] and Deepfake Detection Challenge (DFDC) [35] dataset were the first large-scale datasets containing a huge amount of deepfake videos. FF++ contains 5,000 and DFDC contains 128,154 videos which were generated using multiple deepfake generation methods (FF++: 4, DFDC: 8). FF++ used a base set of 1,000 real YouTube videos and used four deepfake generations models, resulting in 5,000 deepfake videos. Later, FF++ added two more deepfake datasets, Deepfake Detection (DFD) [36] and FaceShifter [54] datasets. On the other hand, Amazon Web Services, Facebook, Microsoft, and researchers belonging to academics collaborated and released Deepfake Detection Challenge Dataset (DFDC) [35]. The videos in the DFDC dataset were captured in different environmental settings and used eight types of synthesizing methods to generate deepfake videos.\n\nSo far, most deepfake detectors [16,17,22,24,[28][29][30][55][56][57] use FF++ and DFDC datasets to train their models. However, most of these datasets lack diversity as the individuals in videos belong to specific ethnic groups. Moreover, DFDC contains videos in which participants record videos, while walking and not looking towards the camera with extreme environmental settings (i.e., dark or very bright lighting conditions), making it much harder to detect. As per our knowledge, DFDC is the only dataset containing synthesized audio with the video, but they label the entire video as fake.\n\nAnd they do not specify if the video is fake or the audio. Furthermore, the synthesized audios are not lip-synced with the respective videos. They even label a video fake if the voice in the video was replaced with another person's voice. Meanwhile, our FakeAVCeleb addresses these issues of environmental conditions, diversity, and respective audio-video labeling, and contains real and fake videos of people with different ethnic backgrounds, ages, and gender. We carefully selected 500 videos belonging to different ages/gender and ethnic groups from the VoxCeleb2 dataset [58], consisting of a large amount of real YouTube videos.\n\nRecently, some new deepfake datasets have come into the light. Researchers have tried to overcome previous datasets' limitations and used new deepfake generation methods to generate deepfake videos. Celeb-DF [34] was proposed in 2020, in which researchers used 500 YouTube real videos of 59 celebrities. They applied the modified version of the popular FaceSwap method [59] to generate deepfake videos. Google also proposed a Deepfake Detection dataset (DFD) [36] containing 363 real videos and 3,000 deepfake videos, respectively. The real videos belong to 28 individuals having different ages and gender. Deepfake Videos in the Wild [53] and DeeperForensics-1.0 [33] are the most recent deepfake datasets. In particular, Deepfake Videos in the Wild dataset contains 1,869 samples of real-world deepfake videos from YouTube, and comprehensively analyzes the popularity, creators, manipulation strategies, and deepfake generation methods. The latter consists of real videos recorded by 100 paid consensual actors. They used 1,000 real videos from the FF++ dataset as target videos to apply FaceSwap. On the other hand, DeeperForensics-1.0 used a single face-swapping method and applied augmentation on real and fake videos, producing 50,000 real and 10,000 fake videos. However, all of the aforementioned datasets contain much fewer real videos than fake ones, except for DeeperForensics-1.0, which contains more real videos than fake ones.\n\nMoreover, there exists an issue of racial bias in several deepfake datasets. The datasets that are known to be partially biased include UADFV [14], Deepfake TIMIT [41], and KoDF [37], where the KoDF dataset contains videos of people having Korean ethnicity. Similarly, UADFV mainly contains 49 real videos from YouTube, and Deepfake TIMIT mentions that it contains only English-speaking Americans. In particular, DeeperForensics-1.0 is racially unbiased since the real videos consist of actors from 26 countries, covering four typical skin tones; white, black, yellow, and brown. On the other hand, Celeb-DF has an unbalanced number of videos for different ethnic groups and gender classes, and mainly contains Western celebrities. To the best of our knowledge, no extensive study explores the racial bias in the deepfake dataset. In our FakeAVCeleb, since we selected real videos from the VoxCeleb2 dataset, which contains 1,092,009 real videos, the number of real videos is not limited to 500. Researchers can use more real videos from the VoxCeleb2 dataset to train their models if required. Moreover, all of the datasets mentioned above only contain deepfake videos and not fake audios.\n\nFor research in human voice synthesis, a variety of new research has been conducted to simulate a human voice using neural networks. Most of these models use Tacotron [60] by Google to generate increasingly realistic, human-like human voices. Also, Google proposed the Automatic Speaker Verification Spoofing (ASVspoof) [40] challenge dataset with the goal of speaker verification and spoofed voice detection. However, all of the datasets mentioned earlier either contain deepfake videos or synthesized audios but not both. In our FakeAVCeleb, we propose a novel deepfake dataset containing deepfake videos with respective lip-synced synthesized audios. To generate fake audios, we used a transfer learning-based real-time voice cloning tool (SV2TTS) [4] that takes a few seconds of audio of a target person along with text and generates a cloned audio. Moreover, each sample in our fake audios is unique, since we clone the audio of every real video we have in our real video set. Hence, our dataset is unique and better, as it contains fake audios of multiple speakers.\n\n\nDataset Collection and Generation\n\nReal Dataset Collection. To generate our FakeAVCeleb, we gathered real videos from the Vox-Celeb2 [58] dataset, where VoxCeleb2 consists of real YouTube videos of 6,112 celebrities. It contains 1,092,009 videos in the development set and 36,237 in the test set, where each video contains interviews of celebrities and the speech audio spoken in the video. We chose 500 videos from VoxCeleb2 with an average duration of 7.8 seconds, one video for each celebrity to generate our FakeAVCeleb dataset. All the selected videos were kept with the same dimension as in the VoxCeleb2 dataset. Since the VoxCeleb2 dataset is relatively gender-biased, we tried to select videos equally based on gender, ethnicity, and age. The individuals in the real video set belong to the different ethnic groups, Caucasian, Black, South Asian, and East Asian. Each ethnic group contains 100 real videos of 100 celebrities. The male and female ratio of each ethnic group is 50%, i.e., 50 videos of men and 50 videos of women out of 100 videos.\n\nAfter carefully watching and listening to each of the sampled videos, we incorporated 500 unique real videos to our FakeAVCeleb as a real baseline video set, each belonging to a single individual, with an average of 7.8 seconds duration. Since we focus on a specific and practical usage of deepfakes, each video was selected based on some specific criteria, i.e., there is a single person in the video with a clear and centered face, and he or she is not wearing any hat, glasses, mask or anything that might cover the face. Furthermore, the video quality should be good, and he or she must speak English, regardless of their ethnic background. Since we selected videos from the VoxCeleb2 dataset, which consists of real videos, more videos of the same celebrity can be selected if required to increase the number of real videos set, as we provide the original celebrity IDs used in the VoxCeleb2 dataset. Due to this reason, our FakeAVCeleb dataset is highly scalable, and we can easily generate more deepfake videos to increase the number of real and deepfake videos if required.\n\nDeepfake Dataset Generation. We used the latest deepfake and synthetic voice generation methods to generate our FakeAVCeleb dataset. We used face-swapping methods, Faceswap [59], and FSGAN [61], to generate swapped deepfake videos. To generate cloned audios, we used a transfer learning-based real-time voice cloning tool (SV2TTS) [4], (see Figure 2). After generating fake videos and audios, we apply Wav2Lip [62] on generated deepfake videos to reenact the videos based  Figure 1: Samples from the Dataset. We divide the dataset into 5 ethnic groups Black, South Asian, East Asian, Caucasian (American) and Caucasian (European). There are total 4 combinations of our dataset: on generated fake audios. As a result, we have a fake video with a fake audio dataset that is perfectly lip-synced. This type of FakeAVCeleb dataset is minacious, as an attacker can generate fake video and fake audio and impersonate any potential target person. It is unique of its kind as all of the previous datasets either contain deepfake videos or synthesized audios. This type of deepfake dataset can be used for training a detector for both, deepfake video and deepfake audio datasets.\nA R V R (500), A F V R (500), A R V F (9,000), and A F V F (10,000).\nTo generate deepfake videos, we used 500 real videos as a base set and generated around 20,000 deepfake videos using several deepfake generation methods, including face-swapping and facial reenactment methods. We also used synthetic speech generation methods to generate cloned voice samples of the people in the videos. Throughout the following sections, we will use the term source to refer to the source video from which we will extract the frames, and the term target will refer to the target video in which the face from the extracted frames will be swapped. To generate deepfake videos, we use Faceswap [59], and Faceswap GAN (FSGAN) [61] to perform face swaps, and use Wav2Lip [62] for facial reenactment based on source audio. On the other hand, the Real-Time Voice Cloning tool (RTVC) such as SV2TTS [4] is used for synthetic cloned voice generation.\n\nSince our real video set contains people from four different ethnic backgrounds, we apply above mentioned chosen synthesis methods for each ethnicity separately (i.e., Caucasian with Caucasian or Asian with Asian). In addition, we apply synthesis for each gender separately (i.e., Men with Men and Women with Women). We applied such mappings to make a more realistic and natural fake dataset, since cross-ethnic and cross-gender face swaps result in non-natural poor deepfakes, thereby they are easily detectable. Moreover, we use a facial recognition service called Face++ [63], which measures the similarity between two faces. The similarity score helps us find the most similar source and target pairs, resulting in more realistic deepfakes. We used the Face++ API to measure the similarity of a face in a source video with faces in many other target videos. We selected the top 5 videos with the highest similarity score. After calculating the similarity score, each video was synthesized with the chosen five videos by applying synthesis methods to produce high quality realistic deepfakes. In particular, we use a total of 4 different video and audio synthesis methods. After applying all methods on each input video, the total number of fake videos comes out to be more than 20, 000.\n\nAfter applying synthesis methods to generate deepfake videos, these videos are manually inspected by our researchers. Since the generated video count is high and removing bad samples manually is very difficult, we created a subset of the videos with respect to each fake type and then performed the \n\n\nDataset\n\nReal\nAudio (A R ) Fake Audio (A F ) Real Video (V R ) A R V R : VoxCeleb2 A F V R : SV2TTS Fake Video (V F ) A R V F : FSGAN, FaceSwap, Wav2Lip A F V F : FSGAN, FaceSwap, Wav2Lip, SV2TTS\ninspection. While inspecting the generated videos, we filter the videos according to the following criteria: 1) The resulting fake video must be of good quality and realistic, i.e., hard to detect through the human eye, 2) The synthesized cloned audio should also be good i.e., the text given as input is synthesized properly, 3) The video and corresponding audio should be lip-synced. After the inspection, the final video count is 20,000. We found that some of the synthesis methods, FSGAN and Wav2Lip, resulted in many fake videos with excellent and realistic quality. Meanwhile, FaceSwap produced several defective videos, since it is sensitive to different lightning conditions. Also, it requires excessive time and resources to train. Some of the frames from the final real and fake videos are shown in Figure 1. We provide details of the audio and video synthesis methods used for FakeAVCeleb Section B in Appendix.\n\n\nFakeAVCeleb Dataset Description\n\nIn this section, we discuss dataset generation using the aforementioned synthesis methods and explain the resulting types of deepfake datasets. Since we are generating cloned voices along with the fake video, we can create four possible combinations of audio-video pairs as shown in Table 2. Also, a pictorial representation of these four combinations is presented in Figure 1. The top-left images in Figure 1 are the samples from our base dataset, which contains real videos with real audio (A R V R ). The top-right images are the samples from real video with fake audio (A F V R ), which are developed using the cloned speech of the target speaker. And the bottom-left images are the samples with fake videos with real audio (A R V F ), representing the existing deepfakes benchmark datasets such as FF++. The bottom-right images are samples with fake video and fake audio (A F V F ), representing the main contribution of our work. We will explain these combinations and their respective use-cases below:\n\nReal-Audio and Real-Video (A R V R ). We sampled 500 videos with diverse ethnicity, gender, and age. This dataset is the base set of real videos with real audios that we selected from the VoxCeleb2 dataset. Since VoxCeleb2 contains more than a million videos, more real videos from the VoxCeleb2 dataset can be utilized to train a deepfake detector for real audios and real videos (see A R V R in Table 2).\n\nFake-Audio and Real-Video (A F V R ). This deepfake type contains the cloned fake audio of a person along with the real video. We generate cloned fake audio using a transfer learning-based real-time voice cloning tool (SV2TTS), which takes real audio and text as an input, and outputs synthesized audio (with voice matching) of the same person, as shown in Table 2 (A F V R ). Please refer to the top-right block in Figure 1 for some sample results, and Figure 2 for spectrograms from real and fake audio. Since we do not have the text spoken in the video, we used IBM Watson speech-to-text service [64] that converts audio into text. This text, along with the corresponding audio, is then passed to the SV2TTS. Later, we merge the synthesized audio with the original video, resulting in the A F V R pair (see Figure 3). As a result, we were able to generate 500 fake videos. Note that our pipeline is not dependent on IBM Watson speech-to-text service; any speech-to-text service can be used as we are just extracting text from our audios. Since it is impossible to generate fake audio with the same timestamp as the original audio, this type of deepfake is not lip-synced. The possible use-case of this type of deepfake is a person performing identity fraud by impersonating a person or speaker recognition system. This dataset type can be also used to defend against anti-voice spoofing attacks, since we have real-fake audio pairs with similar text.\n\nReal-Audio and Fake-Video (A R V F ). This type of deepfake consists of a face-swap or facereenacted video of a person along with the real audio. To generate deepfake videos of this type, we employ three deepfake generation methods, FaceSwap and FSGAN for face-swapping, and Wav2Lip for audio-driven facial reenactment A R V F as shown in Table 2  The aforementioned face-swapping methods are the most popular and most recent deepfake generation methods. In particular, Wav2Lip was chosen because of its efficiency, lesser time consumption, and better quality output (see Figure 3). The sample results can be seen in the bottom-left block in Figure 1. We generated 9,000 deepfake videos of this type. Attackers can employ this type of deepfake for identity fraud, making a person say anything by reenacting the face given any input audio, forging a person's image by swapping his or her face with someone else. Since we kept the audio intact for this type, i.e., used real audio, and manipulation is conducted with the video only, the audio is perfectly lip-synced with video. Researchers can use this A R V F dataset to train their detection models for a possible forged or face-swapped video detection.\n\nFake-Audio and Fake-Video (A F V F ). This type of dataset contains both fake video and respective fake audio. It combines the two types mentioned above (A F V R and A R V F ). See the bottomright block in Figure 1 for sample images. We employ four types of deepfake generation methods, FaceSwap and FSGAN for face-swapping, Wav2Lip for audio-driven facial reenactment, and SV2TTS for real-time cloning a person's voice (see A F V F in Table 2). We first generate cloned fake audio using the SV2TTS tool by giving a text-audio pair as input. Then, we apply Wav2Lip to reenact the video based on cloned audio (see Figure 1). This type contains 10,000 deepfake videos. To the best of our knowledge, this type of deepfake dataset has not been released in the past.\n\n\nOverall Dataset Generation Pipeline\n\nA pictorial representation of our FakeAVCeleb generation pipeline is provided in Figure 3. For A F V R , we begin with a source and target inputs having real audio and video. Then, we extract text from audio using IBM Watson speech-to-text service and generate a cloned speech using SV2TTS by providing the target's speech signal and extracted audio as an input (see Figure 3 left block;\n\nStep 1-3). Lastly, we combine them to make the A F V R , as shown in Step 5. The middle block in Figure 3 illustrates A R V F , which is a typical deepfake video generation pipeline. The source and target videos are processed with a faceswap method such as FSGAN or reenactment method such as Wav2Lip [62] to create deepfakes, as shown in Step 1-4. We combine the previous two methods (A F V R and A R V F ) to create Audio-Video deepfakes in the A F V F block (right) in Figure 3. We use the videos from source and target for faceswaping. At the same time, we use the target's speech signal to generate a cloned speech using IBM Watson speech-to-text service and SV2TTS (Step 1-3). We combine this cloned speech with the face-swapped video or use Wav2Lip to enhance its quality further, as shown in Steps 4-5.\n\n\nBenchmark Experiments and Results\n\nIn this section, we present our experimental setup along with the detailed training procedures. We report the performance of different state-of-the-art baseline deepfake detection methods and discuss their limitations. The purpose of this comprehensive benchmark performance evaluation is to show the complexity levels and usefulness of our FakeAVCeleb, compared to various other deepfake datasets.\n\nPreprocessing. To train our baseline evaluation models, we first preprocess the dataset. Preprocessing was performed separately for videos and audios. Since we collected videos from VoxCeleb2 dataset, these videos are already face-centered. For audios, we compute MFCC features per audio frame and store them as a three channel image, which is then passed to the model as an input. The details of preprocessing are provided in Section B in Appendix.\n\nDeepFake Detection Methods. To perform our experiments, we employed eight different deepfake detection methods to compare ours with other deepfake datasets, which are Capsule [13], HeadPose [14], Visual Artifacts (VA)-MLP/LogReg [51], Xception [36], Meso4 [15], and MesoInception [15]. We chose these methods based on their code availability, and performed detailed experiments to compare the detection performance of our FakeAVCeleb with other deepfake datasets.\n\nWe have briefly explained each of seven baseline deepfake detection methods in Section A in Appendix. Furthermore, since our FakeAVCeleb is a multimodal dataset, we evaluated our model with two different techniques, i.e., the ensemble of two networks and multimodal-based detection, and used audios along with the frames as well. For ensemble networks, we used Meso4, MesoInception, Xception, as well as the latest SOTA models such as Face X-ray [65], F3Net [66] and LipForensics [67]. The details of these models and the detection performances are provided in Appendix. In particular, the detection performance of Face X-ray, F3Net, and LipForensics are 53.5%, 59.8%, and 50.4%, respectively. Therefore, the latest SOTA detection models have achieved mediocre or low detection performance, in contrast to their high detection performance on existing deepfake datasets such as UADFV, DF-TIMIT, FF++, DFD and DFDC.\n\nResults and Analysis. The results of this work are based on the FakeAVCeleb v1.2 database. In the future, we will release new versions of FakeAVCeleb as the dataset is updated and further improved. Please visit our GitHub link 2 for the most recent results for each baseline on newer versions of FakeAVCeleb. The results for each experiment are shown in Table 3. We evaluate the performance of each detection method using the Area Under the ROC curve (AUC) at the frame-level. We use the frame-level AUC scores, because all the compared methods utilize individual frames and output a classification score. Moreover, all the compared datasets contain video frames and not respective audios. Table 3 shows AUC scores for each detection model over eight different deepfake dataset, including ours. Lower AUC score represents higher complexity of the dataset to be detected by the model. It can be observed that our FakeAVCeleb have the lowest AUC values mostly, and is closer to the performance of Celeb-DF dataset.\n\nThe classification scores for each method are rounded to three digits after the decimal point, i.e., with a precision of 10 \u22123 . Figure 4 presents the frame-level ROC curves of Xception, MesoInception4, and Meso4 on several datasets. The ROC curves in Figure 4 are plotted using only video frames for all dataset except our FakeAVCeleb, in which we used both audio (MFCCs) and video (frames) with our ensemble-based models of Xception, MesoInception4, and Meso4. The average AUC performance of all detection methods on each dataset is provided in Figure 6 in Appendix. It can be observed that our FakeAVCeleb is generally the most complex and challenging to these baseline detection methods, compared to all other datasets. The overall performance is close to that of Celeb-DF dataset. The detection performance of DFDC, DFD, and Celeb-DF methods is pretty low on recent datasets, producing average AUC less than 70%. Some of the detection methods achieve very high AUC scores on early datasets (UADF, DF-TIMIT, and FF-DF) with average AUC score more than 75%. However, the average AUC of our FakeAVCeleb is around 65%, where these averages are based on frame-level AUC scores and do not consider the audio modality.\n\nFor the performance of individual detection method, Figure 5 in Appendix shows the average AUC performance of each detection method on all evaluated datasets. Moreover, since the videos downloaded from online platform undergo compression because of the upload and distribution that causes change in video quality, we also show frame-level AUC of Xception-comp on medium (15) degrees of H.264 compressed videos of our FakeAVCeleb in Table 3. The results show that the detection performance of the compressed version of the dataset is close to raw dataset. Interested readers may refer to [68] for a more in-depth examination of FakeAVCeleb in unimodal, multimodal, and ensemble settings.  \n\n\nDiscussion and Future Work\n\nData Quality. In the process of collecting and generating our FakeAVCeleb dataset, we tried to ensure and maintain the high quality of our dataset. All of the selected real YouTube videos went through a manual screening process in which we carefully selected those videos having high quality and faces center-aligned and not covered. The generated deepfake videos also went through the same rigorous process in which we manually removed the corrupted videos. Before we apply face-swapping methods, we employed a facial recognition service, Face++ [63], in order to measure the similarity between two faces, and then applied face-swapping methods between faces having the highest similarity score.\n\nData Availability and Social Impact. Since we used the deepfake video and audio generation methods that are open-sourced and anyone can access and use them, we are not releasing a separate code repository. Please visit the DOI link 3 , which contains the information related to FakeAVCeleb such as structure and version history of our dataset. While our dataset is openly available, it can be misused to evade existing deepfake detectors. To stop or limit the misuse of our FakeAVCeleb by bad actors, we have made a dataset request form 4 . We review the requests that we receive and allow access for a legitimate use. The dataset we share contains the real and all types of deepfake videos that we generated as a zip file. The package also contains the detailed documentation with all relevant metadata specified to users.\n\nFuture Directions. We plan to provide future updates to FakeAVCeleb, keeping updated with the latest deepfake video and audio generation methods. Also, we will consider the potential adversarial attacks and construct the dataset accordingly. Another area of improvement is that we will make use of recent deepfake polishing methods that will help remove the certain artifacts caused by deepfake generation methods. Since this is the first version of the dataset of this type and covers a variety of video and audio combinations with multiple deepfake generation methods, the number of deepfake videos may be small in numbers as compared to other large-scale deepfake datasets. We plan to increase the dataset size in future releases by utilizing more generally and publicly accessible videos such as YouTube-8M [69] in our future release. And we will include them in our dataset maintenance plan.\n\n\nConclusion\n\nWe present a novel Audio-Video multimodal deepfake dataset, FakeAVCeleb, which can be used to detect not only deepfake videos but deepfake audios as well. Our FakeAVCeleb contains deepfake videos along with the respective synthesized cloned audios. We designed FakeAVCeleb to be gender and racially unbiased as it contains videos of men and women of four major races across different age groups. We employed a range of recent, most popular deepfake video and audio generation methods to generate nearly perfectly lip-synced deepfake videos with respective fake audios. We compare the performance of our FakeAVCeleb dataset with seven existing deepfake detection datasets. We performed extensive experiments using several state-of-the-art methods in unimodal, ensemble-based, and multimodal settings (see Appendix C for results). We hope FakeAVCeleb will help researchers build stronger deepfake detectors, and provide a firm foundation for building multimodal deepfake detectors.\n\n\nBroader Impact\n\nTo build a strong deepfake detector, a high-quality and realistic deepfake dataset is required. Recent deepfake datasets contain only forged videos or synthesized audio, resulting in methods for detecting deepfakes that are unimodal. Our FakeAVCeleb dataset includes deepfake videos as well as synthesized fake audio that is lip-synced to the video. To generate our FakeAVCeleb, we used four popular deepfake generation and synthetic voice generation methods. We believe that the multimodal deepfake dataset we are providing will aid researchers and open new avenues for multimodal deepfake detection methods.\n\n\nA Dataset Publication\n\n\nA.1 Links\n\nWe provide the following links to access our dataset: https://sites.google.com/view/ fakeavcelebdash-lab/.\n\nWe provide the dataset through Google drive after receiving the Data Use Agreement (DUA) by Google form. DOI: http://doi.org/10.23056/FAKEAVCELEB_DASHLAB.\n\n\nA.2 Hosting Platform\n\nWe host our dataset on Google Drive account, which belongs to DASH Lab managed by Simon S. Woo (corresponding authors of this paper) at Sungkyunkwan University, South Korea.\n\n\nA.3 Access to Dataset\n\nWe have made a dataset request form to monitor and restrict the free use of our deepfake dataset (see Figure 10). As it has been suggested by experts that deepfake dataset can used by malicious actors to evade deepfake detectors. We have uploaded a small sample of our dataset on our GitHub. Note: Everyone has to fill the dataset request form, which we will manually screen to limit misuse.\n\n\nA.4 Licensing\n\nOur FakeAVCeleb dataset is available under Creative Commons 4.0 license and code is under MIT license (https://creativecommons.org/licenses/by/4.0/).\n\n\nB Dataset Generation Methods\n\nWe used a total of 4 deepfake generation/synthesis methods. We will briefly explain each synthesis method below:\nFaceSwap [59]\nFaceSwap is a general deepfake generation method to swap faces between images or videos, retaining the body and environment context. We used Faceswap [59] software, an opensource face-swapping tool used to generate high-quality deepfake videos. Due to its popularity, it was used in FaceForensics++ datasets to generate the face-swapped dataset. The core architecture of this method consists of the encoder-decoder paradigm. A single encoder and two decoders (one for each source and target video) are trained simultaneously to build the face-swap model. The encoder extracts features from both videos while decoders reconstruct the source and target videos, respectively. The model is fed with frame-by-frame images of source and target video and trained for at least 80,000 iterations. We use this method because of its popularity as being a widely used deepfake generation method.\n\nFSGAN [61] FSGAN is proposed by Nirkin et al. [61], which is the latest face-swapping method that has become popular recently. The key feature of this method is that it performs reenactment along with the face-swap. First, it applies reenactment on the target video based on the source video's pose, angle, and expression by selecting multiple frames from the source having the most correspondence to the target video. Then, it transfers the missing parts and blends them with the target video. This process makes it much easier to train and does not take much time to generate face-swapped video. We use the code from the official FSGAN GitHub repository [70]. We used the best quality swapping model recommended by the authors of FSGAN to prepare our dataset, by fine-tuning the input video pairs and generating better quality results. We adopt this method because of its efficiency and better quality of the results.\n\nWav2Lip [62] Recently, audio-based facial reenactment techniques along with lip-syncing have been proposed by researchers [62,71]. In lip-sync, the source person controls the mouth movement, and in face reenactment, facial features are manipulated in the target video. One of the most recent audio-driven facial reenactment methods is Wav2Lip [62], which aims to lip-sync the video with respect to any desired speech signal by reenacting the face. Unlike LipGAN [71], which further fine-tuned the model on the generated frames, using a pretrained lip-sync discriminator to learn the lip-sync with respect to the desired audio accurately, Wav2Lip used five video frames and the respective audio spectrogram to capture the video's temporal context. We used this facial reenactment method because of the efficiency of its synthesis process for generating lip-synced video.\n\n\nSV2TTS [4]\n\nTransfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis (SV2TTS) [4] is a real-time voice cloning (RTVC) tool that allows us to clone a voice from a few seconds of input audio. SV2TTS consists of three sub-models which are trained independently. First, an encoder network is trained on a speaker verification task. It generates a fixed-dimensional embedding vector of input audio, a synthesis network based on Tacotron 2 that generates Mel spectrogram, and a WaveNet-based vocoder network that converts the Mel spectrogram into time-domain waveform samples. SV2TTS works in real-time, taking the text and reference audio as input, and generating a cloned audio based on the input audio. We used this tool to generate cloned audios of our real video dataset.\n\n\nB.1 Deepfake Detection Baseline Methods\n\nWe use eleven different deepfake detection methods to evaluate our FakeAVCeleb. We use these methods based on their code availability and frame-level AUC scores, where these methods analyze individual frames and output a classification score. We use the default parameters provided with each compared detection method. We briefly discuss each of the detection methods below:\n\nCapsule [13] This method is based in capsule structures to perform deepfake classification. This original model is trained on the FaceForensics++ dataset.\n\nHeadPose [14] This method detects deepFake videos based on the inconsistencies in the head poses caused by deepfake generation methods. The model is based on SVM, and is trained on the UADFV dataset.\n\nVisual Artifacts (VA) [51] This method detects videos based on visual artifacts in the eyes, teeth and facial areas in synthesized videos. Two variants of this model is provided: VA-LogReg and VA-MLP. And we used both of them to evaluate our dataset. VA-LogReg uses a simpler logistic regression model, while VA-MLP uses a feed forward neural network classifier. Both models are trained on real videos from CelebA dataset and fake videos from YouTube.\n\nXception [36] This baseline method is based on the XceptionNet model [72]. We use Xception in two different settings, Xception-raw (detection on raw frames) and Xception-comp15 (detection on compressed frames).\n\nMeso4 [15] This method is based on Deep Neural Network (DNN) which targets mesoscopic properties of real and fake images. The model is trained on anonymous deepfake dataset. We use two variant of MesoNet, which are, Meso4 and MesoInception4.\n\nF3Net [66] This deepfake detection method achieves the state-of-the-art performance by employing frequency-aware clues and local frequency statistic for deepfake detection in frequency domain.\n\nFace X-ray [65] Li et al. [65] proposed a method that detect deepfakes by combining classification and segmentation, based on blending boundaries of manipulated images.\n\nLipForensics [67] LipForensics is a recent work to detect facial forgeries based on unnatural lip movement by paying attention to the mouth area. They employ spatio-temporal network for the detection.\n\n\nMultimodal-1 [46]\n\nMultimodal-1 was developed for the classification of food recipes. They use two neural networks to extract features from visual and textual modality, and them performs classification using a third neural network. To perform the classification on our FakeAVCeleb dataset, we modified the model with respect to the video and audio modalities. We removed the neural network for textual modality and replicated the neural network of visual modality to use this model on the multimodal dataset.\n\n\nMultimodal-2 [47]\n\nMultimodal-2 is an open-source method that is developed for movie genre prediction. It takes movie poster (image) as input for a CNN block, movie genre (text) as input for an LSTM block, and then concatenate the output to perform classification. To use this model on our multimodal dataset, we removed the LSTM block and replaced it with the same CNN block, resulting in two CNN blocks, one for visual and one for audio modality.\n\nCDCN [49] Central Difference Convolutional Networks (CDCN) [49] was developed to solve the task of face anti-spoofing. The model takes three-level fused features (low-level, mid-level, high-level) extracted for predicting facial depth. To perform the experiment, we modified the model by removing the third modality since it contains all three visual modalities.\n\nPreprocessing We first preprocess the dataset before passing it to the models for training. As mentioned in main text, preprocessing was performed separately for videos and audios. Since we collected videos from VoxCeleb2 dataset, these videos are already face-centered and cropped. We extract respective frames from each video and store them separately, and then extract audios from the videos and store them in .wav format with a sampling rate of 16 kHz. Before inputting audio directly to the model for training, we first compute Mel-Frequency Cepstral Coefficients (MFCC) features by applying a 25ms Hann window [73] with 10ms window shifts, followed by a fast Fourier transform (FFT) with 512 points. As a result, we obtain a 2D array of 80 MFCC features (D = 80) per audio frame and store the resulting MFCC features as a three channel image, which is then passed to the model as an input to extract speech features so that it learns the difference between real and fake human voices.\n\n\nB.2 Summary of Results\n\nIn Figure 5, we present the average of the AUC score for each baseline deepfake detector on eight different datasets, which are FF-DF, UADFV, DFD, DF-TIMIT LQ, DF-TIMIT HQ, FakeAVCeleb, DFDC, and Celeb-DF. We find that on average Xception-comp demonstrates the best (72.5%) and Headpose shows the worst (49.0%) detection performance.\n\nIn Figure 6, we present the average AUC score for each dataset based on eight different detection methods, which are Headpose, Xception-raw, Xception-comp, VA-MLP, VA-LogReg, MesoIncep-tion4, Meso4 and Capsule. The FF-DF dataset shows the highest detection score making it the easiest one to detect, while CelebDF shows the lowest, making it the hardest one to detect. The detection score for our FakeAVCeleb dataset is relatively close to CelebDF. However, our dataset has an additional advantage of having fake audio data.\n\n\nC Additional Experiments\n\nWe also performed additional experiments on our dataset in unimodal, ensemble, and multimodal settings. The following sections cover the details of these experiments. Note: The results of this work are based on the FakeAVCeleb v1.2 database. In the future, we will release new versions of FakeAVCeleb as the dataset's quality improves. Please visit our GitHub page 5 for the most recent results for each baseline on newer versions of FakeAVCeleb.  \n\n\nC.1 Unimodal Results\n\nThe performance of the baseline trained only on audio (A only ) or only on video (V only ) on the test set, which contains real and all three categories of fake videos from the FakeAVCeleb dataset, is described in this section.\n\nThe results of deepfake detection using unimodal baselines for A only and V only are presented in Figure 7. We can observe that the best AUC scores for A only and V only are approximately 97% and 93%, respectively.\n\n\nC.2 Results for V only Trained Classifier\n\nIn terms of video, as shown in Figure 7, EfficientNet-B0 [48] and VGG [50] achieves the performance of 93.3% and 49.6%, which are the best and lowest results of AUC score, respectively. In particular, Meso4's recall score suggests that the model fails to detect most deepfake videos (V F ). On the other hand, EfficientNet-B0 outperformed Xception [72] on this task, though Xception is the best performer on other deepfake datasets such as FaceForensics++ [36].\n\n\nC.2.1 Results for A only Trained Classifier\n\nFor audio, as shown in Figure 7, VGG achieves the best AUC score of 97.8%, and Meso4 [15] shows the lowest AUC score of 73.5%, which means that Meso4 overfit real class and fake class for audio detection, respectively. Moreover, we can observe that there are no baselines to provide satisfactory detection performance, indicating that SOTA deepfake detection methods are not suitable for deepfake audio (A F ) detection. The models developed for human speech verification or detection may perform better in detecting A F . However, we have not considered such methods in this work. And, we expect that such methods can be explored for future work. Figure 7: The AUC scores (%) of the five unimodal methods. We use three types of detection methods to evaluate the FakeAVCeleb dataset. We use a single modality, i.e., either A only or V only , to train the model. Figure 8: The AUC scores (%) of six models on our FakeAVCeleb which are used as an ensemble of two models (one for each modality), but trained separately. We use three types of evaluation settings, soft-voting, hard voting and average of both, to evaluate the dataset.\n\n\nC.2.2 Summary of Unimodal Results\n\nEfficientNet-B0 exhibits the most stable average performance of 95% for A only and V only . Overall, the poor performance of SOTA deepfake detection models in Figure 7 indicates that the fake audios and videos in our dataset are of realistic quality, making it difficult for detectors to distinguish them from real ones.\n\n\nC.3 Ensemble Results\n\nIn Figure 8, we used the SOTA unimodal baselines to make ensemble of unimodal A only and unimodal V only classifier. The ensemble network of EfficientNet-B0 performs the best (82.8%) as compared to an ensemble of Xception [36]   achieves the third highest AUC score of 55.9%. On the other hand, Face X-ray [65] ensemble achieves mediocre AUC scores of 53.5%. Overall, as shown in Figure 8, we can observe that the choice of soft-or hard-voting did not have a significant impact on the performance of the ensemble classifier, as they provide the similar prediction score. Note that this is because we have only two classifiers in our ensemble. Moreover, none of the ensemble-based methods could achieve a high detection score (i.e., > 90%), which represents that detecting multimodal A F V F deepfakes is significantly harder, and more effective and advanced multimodal deepfake detection methods are required in the future.\n\n\nC.4 Multimodal Results\n\nWe report on how the three baseline multimodals performed on the two modalities, A and V, of the FakeAVCeleb multimodal dataset. The Multimodal-1 [46] was trained over 50 epochs. After selecting the best performing epoch, the model could classify with 53.3% AUC score. For Multimodal-2 [47], we trained on 50 epochs and evaluated them on our dataset, which shows 68.8% score. The third model, CDCN [49], was also trained on 50 epochs and provided 66.2% score. We can observe that Multimodal-1 and CDCN performed poorly compared to Multimodal-2. The possible reason for this result is that these models are designed to perform specific tasks, i.e., food recipe classification and movie genre prediction. Furthermore, the multimodal methods make it challenging to detect deepfakes when either the video is fake or the audio. Therefore, more research is needed in the development of multimodal deepfake detectors. Figure 10: A screenshot of Google form (https://bit.ly/38prlVO) to obtain access to FakeAVCeleb. If a researcher is using the FakeAVCeleb dataset, they can use the citation contained in Google form. The users must fill-in correct information, and should agree to follow the terms and conditions.\n\n\nD Dataset Request Form\n\nFigure 2 :\n2Sample spectrogram of real audio A R (left) and fake audio A F (right).\n\nFigure 3 :\n3A step-by-step description of our FakeAVCeleb generation pipeline. The first, second, and the third method represents A F V R , A R V F , and A F V F generation methods, respectively, where the second (A R V F ) and the third method (A F V F ) contains lip-synched, and the first method (A F V R ) contains lip-unsynced deepfake videos.\n\nFigure 4 :\n4ROC curves of three state-of-the-art detection methods on five deepfake datasets. Only video frames are used from all datasets except FakeAVCeleb, where we used both audio (MFCCs) and video (frames) to form an ensemble model (see Appendix for more results). The AUC scores of these SOTA models on our FakeAVCeleb are 72.5%, 61.7%, and 60.9%.\n\nFigure 5 :\n5Average AUC score of deepfake detectors over all datasets.\n\nFigure 6 :\n6Average AUC score for each deepfake dataset on SOTA baseline methods.\n\nFigure 9 :\n9Multimodal detection performance (V and A) on three different open source multimodal methods.\n\nTable 1 :\n1Quantitative comparison of FakeAVCeleb to existing publicly available Deepfake dataset.Dataset \nReal \nVideos \n\nFake \nVideos \n\nTotal \nVideos \n\nRights \nCleared \n\nAgreeing \nsubjects \n\nTotal \nSubjects \n\nSynthesis \nMethods \n\nReal \nAudio \n\nDeepfake \nAudio \n\nFine-grained \nlabeling \nUADFV [14] \n49 \n49 \n98 \nNo \n0 \n49 \n1 \nNo \nNo \nNo \nDeepfakeTIMIT [41] \n640 \n320 \n960 \nNo \n0 \n32 \n2 \nNo \nYes \nNo \nFF++ [36] \n1,000 \n4,000 \n5,000 \nNo \n0 \nN/A \n4 \nNo \nNo \nNo \nCeleb-DF [34] \n590 \n5,639 \n6,229 \nNo \n0 \n59 \n1 \nNo \nNo \nNo \nGoogle DFD [36] \n363 \n3,000 \n3,363 \nYes \n28 \n28 \n5 \nNo \nNo \nNo \nDeeperForensics [33] 50,000 \n10,000 \n60,000 \nYes \n100 \n100 \n1 \nNo \nNo \nNo \nDFDC [35] \n23,654 104,500 128,154 \nYes \n960 \n960 \n8 \nYes \nYes \nNo \nKoDF [37] \n62,166 175,776 237,942 \nYes \n403 \n403 \n6 \nYes \nNo \nNo \nFakeAVCeleb \n500 \n19,500 \n20,000 \nNo \n0 \n500 \n4 \nYes \nYes \nYes \n\n\n\nTable 2 :\n2All possible combinations of real and fake video and audio datasets that we covered in our FakeAVCeleb.\n\n\n.2. Text \n\nSV2TTS \n\n2. Speech \n\nWat son \nSTT \n\n4. Combine \n\nFake Audio & Real Video \n\n(A F VR) \n\n2. Text \n\n3. Cl oned \nSpeech \n\n1. Appl y \n\nFace swap \n\n(f sgan, f aceswap) \n\nWav2Lip \n\n3. Appl y \n\nWav2Lip \n\n1. Appl y \n\nReal Audio & Fake Video \n\n(A R VF) \n\nFace swap \n\n(f sgan, f aceswap) \n\nWat son \nSTT \n\nWav2Lip \n\n1. Appl y \n\nFake Audio & Fake Video \n\n(A F VF) \n\n1.Speech \n\nSV2TTS \n\n2. Text \n\n2. Speech \n\nWat son \nSTT \n\nWav2Lip \n\n4. Appl y \n\n4. Appl y \n\n3. Cl oned \nSpeech \n\n5. Resul t (C) \n\n5. Resul t (B) \n\n5. Resul t (A) \n4. Combine \n2. Resul t (A) \n\n4. Resul t (C) \n\n2. Resul t (B) \n\n5. Resul t \n\n1.Speeh \n\nInput s \nInput s \nInput s \n\n\n\nTable 3 :\n3Frame-level AUC scores (%) of various methods on compared datasets. Bold faces correspond to the top performance.Dataset \nUADFV \nDF-TIMIT FF-DF DFD DFDC Celeb-DF FakeAVCeleb \nLQ HQ \nCapsule [13] \n61.3 \n78.4 74.4 \n96.6 \n64.0 \n53.3 \n57.5 \n70.9 \nHeadPose [14] \n89.0 \n55.1 53.2 \n47.3 \n56.1 \n55.9 \n54.6 \n49.0 \nVA-MLP [51] \n70.2 \n61.4 62.1 \n66.4 \n69.1 \n61.9 \n55.0 \n67.0 \nVA-LogReg [51] \n54.0 \n77.0 77.3 \n78.0 \n77.2 \n66.2 \n55.1 \n67.9 \nXception-raw [36] \n80.4 \n56.7 54.0 \n99.7 \n53.9 \n49.9 \n48.2 \n71.5 \nXception-comp [36] \n91.2 \n95.9 94.4 \n99.7 \n85.9 \n72.2 \n65.3 \n72.5 \nMeso4 [15] \n84.3 \n87.8 68.4 \n84.7 \n76.0 \n75.3 \n54.8 \n60.9 \nMesoInception4 [15] \n82.1 \n80.4 62.7 \n83.0 \n75.9 \n73.2 \n53.6 \n61.7 \n\n\n\n\nclassifier (51.4%) and F3Net [66] (47.6%) on the test set, respectively. Meanwhile, Meso4 shows the second best performance of 58.2% and MesoInception4 [65] ensemble\nhttps://github.com/DASH-Lab/FakeAVCeleb\nhttp://doi.org/10.23056/FAKEAVCELEB_DASHLAB 4 https://bit.ly/38prlVO\nhttps://github.com/DASH-Lab/FakeAVCeleb\nAcknowledgments and Disclosure of Funding\nComparative study of image forgery and copy-move techniques. M Sridevi, Siddhant Mala, Sanyam, Advances in Computer Science, Engineering & Applications. M Sridevi, C Mala, and Siddhant Sanyam. Comparative study of image forgery and copy-move techniques. Advances in Computer Science, Engineering & Applications, pages 715-723, 2012.\n\n. J Ian, Jean Goodfellow, Mehdi Pouget-Abadie, Bing Mirza, David Xu, Sherjil Warde-Farley, Aaron Ozair, Yoshua Courville, Bengio, arXiv:1406.2661Generative adversarial networks. arXiv preprintIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661, 2014.\n\nDeepfakes and beyond: A survey of face manipulation and fake detection. Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, Javier Ortega-Garcia, Information Fusion. 64Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, and Javier Ortega- Garcia. Deepfakes and beyond: A survey of face manipulation and fake detection. Information Fusion, 64:131-148, 2020.\n\nTransfer learning from speaker verification to multispeaker text-to-speech synthesis. Ye Jia, Yu Zhang, Ron J Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu, arXiv:1806.04558Ye Jia, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, and Yonghui Wu. Transfer learning from speaker verification to multispeaker text-to-speech synthesis, 2019. arXiv:1806.04558.\n\nLearning internal representations by error propagation. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, California Univ San Diego La Jolla Inst for Cognitive ScienceTechnical reportDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal represen- tations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.\n\nNeural voice cloning with a few samples. O Sercan, Jitong Arik, Kainan Chen, Wei Peng, Yanqi Ping, Zhou, arXiv:1802.06006Sercan O. Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning with a few samples, 2018. arXiv:1802.06006.\n\nAyush Tewari, Christian Theobalt, and Matthias Nie\u00dfner. Justus Thies, Mohamed Elgharib, European Conference on Computer Vision. SpringerNeural voice puppetry: Audio-driven facial reenactmentJustus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, and Matthias Nie\u00dfner. Neural voice puppetry: Audio-driven facial reenactment. In European Conference on Computer Vision, pages 716-731. Springer, 2020.\n\nDeepfakes are the most dangerous crime of the future, researchers. Adam Smith, 31Adam Smith. Deepfakes are the most dangerous crime of the future, researchers, 2020. [Online; accessed 31-May-2021].\n\nDeepfakes are going to wreak havoc on society. we are not prepared. Rob Toews, 31Rob Toews. Deepfakes are going to wreak havoc on society. we are not prepared, 2020. [Online; accessed 31-May-2021]. URL: https://www.forbes.com/sites/robtoews/ 2020/05/25/deepfakes-are-going-to-wreak-havoc-on-society-we-are-not- prepared/?sh=7885d8737494.\n\nDeepfakes are going to wreak havoc on society. we are not prepared. State Farm Insurance. 31State Farm Insurance. Deepfakes are going to wreak havoc on society. we are not prepared, 2020. [Online; accessed 31-May-2021]. URL: https://www.youtube.com/watch?v= FzOVqClci_s.\n\nState farm and kenny mayne brilliantly faked us all out during the last dance. David Griner, 31David Griner. State farm and kenny mayne brilliantly faked us all out during the last dance, 2020. [Online; accessed 31-May-2021]. URL: https://www.adweek.com/brand- marketing/state-farm-and-kenny-mayne-brilliantly-faked-us-all-out- during-the-last-dance/.\n\nAm I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web APIs under Deepfake Impersonation Attack. Shahroz Tariq, Sowon Jeon, Simon S Woo, arXiv:2103.00847arXiv preprintShahroz Tariq, Sowon Jeon, and Simon S Woo. Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web APIs under Deepfake Impersonation Attack. arXiv preprint arXiv:2103.00847, 2021.\n\nUse of a capsule network to detect fake images and videos. H Huy, Junichi Nguyen, Isao Yamagishi, Echizen, arXiv:1910.12467arXiv preprintHuy H Nguyen, Junichi Yamagishi, and Isao Echizen. Use of a capsule network to detect fake images and videos. arXiv preprint arXiv:1910.12467, 2019.\n\nExposing deep fakes using inconsistent head poses. Xin Yang, Yuezun Li, Siwei Lyu, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEXin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8261-8265. IEEE, 2019.\n\nMesonet: a compact facial video forgery detection network. Darius Afchar, Vincent Nozick, Junichi Yamagishi, Isao Echizen, 2018 IEEE International Workshop on Information Forensics and Security (WIFS). IEEEDarius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. Mesonet: a compact facial video forgery detection network. In 2018 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1-7. IEEE, 2018.\n\nOC-FakeDect: Classifying Deepfakes Using One-class Variational Autoencoder. Hasam Khalid, Simon S Woo, 10.1109/CVPRW50498.2020.003362020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Hasam Khalid and Simon S. Woo. OC-FakeDect: Classifying Deepfakes Using One-class Variational Autoencoder. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 2794-2803, 2020. doi:10.1109/CVPRW50498. 2020.00336.\n\nProtecting world leaders against deep fakes. Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, Hao Li, CVPR Workshops. Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, and Hao Li. Protecting world leaders against deep fakes. In CVPR Workshops, pages 38-45, 2019.\n\nDeepfake video detection using recurrent neural networks. David G\u00fcera, J Edward, Delp, 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). IEEEDavid G\u00fcera and Edward J Delp. Deepfake video detection using recurrent neural networks. In 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pages 1-6. IEEE, 2018.\n\nYuezun Li, Ming-Ching Chang, Siwei Lyu, arXiv:1806.02877ictu oculi: Exposing ai generated fake face videos by detecting eye blinking. arXiv preprintYuezun Li, Ming-Ching Chang, and Siwei Lyu. In ictu oculi: Exposing ai generated fake face videos by detecting eye blinking. arXiv preprint arXiv:1806.02877, 2018.\n\nDetecting Both Machine and Human Created Fake Face Images In the Wild. Shahroz Tariq, Sangyup Lee, Hoyoung Kim, Youjin Shin, Simon S Woo, 10.1145/3267357.3267367Proceedings of the 2nd International Workshop on Multimedia Privacy and Security, MPS '18. the 2nd International Workshop on Multimedia Privacy and Security, MPS '18New York, NY, USAAssociation for Computing MachineryShahroz Tariq, Sangyup Lee, Hoyoung Kim, Youjin Shin, and Simon S. Woo. Detecting Both Machine and Human Created Fake Face Images In the Wild. In Proceedings of the 2nd International Workshop on Multimedia Privacy and Security, MPS '18, page 81-87, New York, NY, USA, 2018. Association for Computing Machinery. doi:10.1145/3267357.3267367.\n\nGAN is a Friend or Foe? A Framework to Detect Various Fake Face Images. Shahroz Tariq, Sangyup Lee, Hoyoung Kim, Youjin Shin, Simon S Woo, Proceedings of the 34th. the 34thShahroz Tariq, Sangyup Lee, Hoyoung Kim, Youjin Shin, and Simon S. Woo. GAN is a Friend or Foe? A Framework to Detect Various Fake Face Images. In Proceedings of the 34th\n\n10.1145/3297280.3297410ACM/SIGAPP Symposium on Applied Computing, SAC '19. New York, NY, USAAssociation for Computing MachineryACM/SIGAPP Symposium on Applied Computing, SAC '19, page 1296-1303, New York, NY, USA, 2019. Association for Computing Machinery. doi:10.1145/3297280.3297410.\n\nOne Detector to Rule Them All: Towards a General Deepfake Attack Detection Framework. Shahroz Tariq, Sangyup Lee, Simon Woo, 10.1145/3442381.3449809Proceedings of the Web Conference 2021, WWW '21. the Web Conference 2021, WWW '21New York, NY, USAAssociation for Computing MachineryShahroz Tariq, Sangyup Lee, and Simon Woo. One Detector to Rule Them All: Towards a General Deepfake Attack Detection Framework. In Proceedings of the Web Conference 2021, WWW '21, page 3625-3637, New York, NY, USA, 2021. Association for Computing Machinery. doi:10.1145/3442381.3449809.\n\nDetecting handcrafted facial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet. Sangyup Lee, Shahroz Tariq, Youjin Shin, Simon S Woo, Applied Soft Computing. 105Sangyup Lee, Shahroz Tariq, Youjin Shin, and Simon S. Woo. Detecting handcrafted fa- cial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet. Applied Soft Computing, 105:107256, 2021. URL: https://www.sciencedirect.\n\n. 10.1016/j.asoc.2021.107256com/science/article/pii/S1568494621001794, doi:https://doi.org/10.1016/j. asoc.2021.107256.\n\nTAR: Generalized Forensic Framework to Detect Deepfakes Using Weakly Supervised Learning. Sangyup Lee, Shahroz Tariq, Junyaup Kim, Simon S Woo, ICT Systems Security and Privacy Protection. Audun J\u00f8sang, Lynn Futcher, and Janne HagenChamSpringer International PublishingSangyup Lee, Shahroz Tariq, Junyaup Kim, and Simon S. Woo. TAR: Generalized Forensic Framework to Detect Deepfakes Using Weakly Supervised Learning. In Audun J\u00f8sang, Lynn Futcher, and Janne Hagen, editors, ICT Systems Security and Privacy Protection, pages 351-366, Cham, 2021. Springer International Publishing.\n\nFakeTalkerDetect: Effective and Practical Realistic Neural Talking Head Detection with a Highly Unbalanced Dataset. Hyeonseong Jeon, Youngoh Bang, Simon S Woo, Proceedings of the IEEE International Conference on Computer Vision Workshops. the IEEE International Conference on Computer Vision WorkshopsHyeonseong Jeon, Youngoh Bang, and Simon S Woo. FakeTalkerDetect: Effective and Practical Realistic Neural Talking Head Detection with a Highly Unbalanced Dataset. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 0-0, 2019.\n\nFDFtNet: Facing off fake images using fake detection fine-tuning network. Hyeonseong Jeon, Youngoh Bang, Simon S Woo, IFIP International Conference on ICT Systems Security and Privacy Protection. SpringerHyeonseong Jeon, Youngoh Bang, and Simon S Woo. FDFtNet: Facing off fake images using fake detection fine-tuning network. In IFIP International Conference on ICT Systems Security and Privacy Protection, pages 416-430. Springer, 2020.\n\nTransferable GANgenerated Images Detection Framework. Hyeonseong Jeon, Young Oh Bang, Junyaup Kim, Simon Woo, . T-Gd , International Conference on Machine Learning. PMLRHyeonseong Jeon, Young Oh Bang, Junyaup Kim, and Simon Woo. T-GD: Transferable GAN- generated Images Detection Framework. In International Conference on Machine Learning, pages 4746-4761. PMLR, 2020.\n\nA Convolutional LSTM based Residual Network for Deepfake Video Detection. Shahroz Tariq, Sangyup Lee, Simon S Woo, arXiv:2009.07480arXiv preprintShahroz Tariq, Sangyup Lee, and Simon S Woo. A Convolutional LSTM based Residual Network for Deepfake Video Detection. arXiv preprint arXiv:2009.07480, 2020.\n\nFReTAL: Generalizing Deepfake Detection Using Knowledge Distillation and Representation Learning. Minha Kim, Shahroz Tariq, Simon S Woo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) WorkshopsMinha Kim, Shahroz Tariq, and Simon S. Woo. FReTAL: Generalizing Deepfake Detection Using Knowledge Distillation and Representation Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1001-1012, June 2021.\n\nCoReD: Generalizing Fake Media Detection with Continual Representation Using Distillation. Minha Kim, Shahroz Tariq, Simon S Woo, 10.1145/3474085.3475535Proceedings of the 29th ACM International Conference on Multimedia, MM '21. the 29th ACM International Conference on Multimedia, MM '21New York, NY, USAAssociation for Computing MachineryMinha Kim, Shahroz Tariq, and Simon S. Woo. CoReD: Generalizing Fake Media Detection with Continual Representation Using Distillation. In Proceedings of the 29th ACM International Conference on Multimedia, MM '21, page 337-346, New York, NY, USA, 2021. Association for Computing Machinery. doi:10.1145/3474085.3475535.\n\nClassifying Genuine Face images from Disguised Face Images. Junyaup Kim, Siho Han, Simon S Woo, 2019 IEEE International Conference on Big Data (Big Data). IEEEJunyaup Kim, Siho Han, and Simon S Woo. Classifying Genuine Face images from Disguised Face Images. In 2019 IEEE International Conference on Big Data (Big Data), pages 6248-6250. IEEE, 2019.\n\nContributing data to deepfake detection research. Nick Dufour, Andrew Gully, Google AI Blog. Nick Dufour and Andrew Gully. Contributing data to deepfake detection research. Google AI Blog, 2019.\n\nDeeperforensics-1.0: A large-scale dataset for real-world face forgery detection. Liming Jiang, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLiming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2889-2898, 2020.\n\nCeleb-df: A large-scale challenging dataset for deepfake forensics. Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, Siwei Lyu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-df: A large-scale challenging dataset for deepfake forensics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3207-3216, 2020.\n\n. Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, Cristian Canton Ferrer, arXiv:2006.07397The deepfake detection challenge dataset. arXiv preprintBrian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cris- tian Canton Ferrer. The deepfake detection challenge dataset. arXiv preprint arXiv:2006.07397, 2020.\n\nFaceforensics++: Learning to detect manipulated facial images. Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, Matthias Nie\u00dfner, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionAndreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. Faceforensics++: Learning to detect manipulated facial images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1-11, 2019.\n\nKodf: A large-scale korean deepfake detection dataset. Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, Gyeongsu Chae, arXiv:2103.10094arXiv preprintPatrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and Gyeongsu Chae. Kodf: A large-scale korean deepfake detection dataset. arXiv preprint arXiv:2103.10094, 2021.\n\nFakebuster: A deepfakes detection tool for video conferencing scenarios. Vineet Mehta, Parul Gupta, Ramanathan Subramanian, Abhinav Dhall, 26th International Conference on Intelligent User Interfaces. Vineet Mehta, Parul Gupta, Ramanathan Subramanian, and Abhinav Dhall. Fakebuster: A deepfakes detection tool for video conferencing scenarios. In 26th International Conference on Intelligent User Interfaces, pages 61-63, 2021.\n\nAdd: Frequency attention and multi-view based knowledge distillation to detect low-quality compressed deepfake images. M Binh, Simon S Le, Woo, arXiv:2112.03553arXiv preprintBinh M Le and Simon S Woo. Add: Frequency attention and multi-view based knowledge distillation to detect low-quality compressed deepfake images. arXiv preprint arXiv:2112.03553, 2021.\n\nAsvspoof 2019: A large-scale public database of synthesized, converted and replayed speech. Xin Wang, Junichi Yamagishi, Massimiliano Todisco, H\u00e9ctor Delgado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vestman, Tomi Kinnunen, Kong Aik Lee, Computer Speech & Language. 64101114Xin Wang, Junichi Yamagishi, Massimiliano Todisco, H\u00e9ctor Delgado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vestman, Tomi Kinnunen, Kong Aik Lee, et al. Asvspoof 2019: A large-scale public database of synthesized, converted and replayed speech. Computer Speech & Language, 64:101114, 2020.\n\nDeepfakes: a new threat to face recognition? assessment and detection. Pavel Korshunov, S\u00e9bastien Marcel, arXiv:1812.08685arXiv preprintPavel Korshunov and S\u00e9bastien Marcel. Deepfakes: a new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018.\n\nThe ami meeting corpus. Iain Mccowan, Jean Carletta, Wessel Kraaij, Simone Ashby, Bourban, M Flynn, Thomas Guillemot, Hain, Vasilis Kadlec, Karaiskos, Proceedings of the 5th international conference on methods and techniques in behavioral research. the 5th international conference on methods and techniques in behavioral researchCiteseer88100Iain McCowan, Jean Carletta, Wessel Kraaij, Simone Ashby, S Bourban, M Flynn, M Guillemot, Thomas Hain, J Kadlec, Vasilis Karaiskos, et al. The ami meeting corpus. In Proceedings of the 5th international conference on methods and techniques in behavioral research, volume 88, page 100. Citeseer, 2005.\n\nExploring the asynchronous of the frequency spectra of gangenerated facial images. M Binh, Simon S Le, Woo, arXiv:2112.08050arXiv preprintBinh M Le and Simon S Woo. Exploring the asynchronous of the frequency spectra of gan- generated facial images. arXiv preprint arXiv:2112.08050, 2021.\n\nAm i a real or fake celebrity? measuring commercial face recognition web apis under deepfake impersonation attack. Shahroz Tariq, Sowon Jeon, Simon S Woo, arXiv:2103.00847arXiv preprintShahroz Tariq, Sowon Jeon, and Simon S Woo. Am i a real or fake celebrity? measuring commercial face recognition web apis under deepfake impersonation attack. arXiv preprint arXiv:2103.00847, 2021.\n\nNicole Baram, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) preview dataset. Brian Dolhansky, Russ Howes, Ben Pflaum, arXiv:1910.08854Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) preview dataset, 2019. arXiv:1910.08854.\n\nMultimodal classification. 31xkaple01. Multimodal classification, 2019. [Online; accessed 31-May-2021]. URL: https: //github.com/xkaple01/multimodal-classification.\n\nMultimodal for movie genre prediction. Dhruv Verma, 31Dhruv Verma. Multimodal for movie genre prediction, 2021. [Online; accessed 31-July-2021].\n\nEfficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, International Conference on Machine Learning. PMLRMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pages 6105-6114. PMLR, 2019.\n\nSearching central difference convolutional networks for face anti-spoofing. Zitong Yu, Chenxu Zhao, Zezheng Wang, Yunxiao Qin, Zhuo Su, Xiaobai Li, Feng Zhou, Guoying Zhao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZitong Yu, Chenxu Zhao, Zezheng Wang, Yunxiao Qin, Zhuo Su, Xiaobai Li, Feng Zhou, and Guoying Zhao. Searching central difference convolutional networks for face anti-spoofing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5295-5305, 2020.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nExploiting visual artifacts to expose deepfakes and face manipulations. Falko Matern, Christian Riess, Marc Stamminger, 2019 IEEE Winter Applications of Computer Vision Workshops (WACVW). IEEEFalko Matern, Christian Riess, and Marc Stamminger. Exploiting visual artifacts to expose deepfakes and face manipulations. In 2019 IEEE Winter Applications of Computer Vision Workshops (WACVW), pages 83-92. IEEE, 2019.\n\nDeepfakes: a new threat to face recognition? assessment and detection. Pavel Korshunov, S\u00e9bastien Marcel, arXiv:1812.08685arXiv preprintPavel Korshunov and S\u00e9bastien Marcel. Deepfakes: a new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018.\n\nJiameng Pu, Neal Mangaokar, Lauren Kelly, Parantapa Bhattacharya, Kavya Sundaram, arXiv:2103.04263Mobin Javed, Bolun Wang, and Bimal Viswanath. Deepfake videos in the wild: Analysis and detection. arXiv preprintJiameng Pu, Neal Mangaokar, Lauren Kelly, Parantapa Bhattacharya, Kavya Sundaram, Mobin Javed, Bolun Wang, and Bimal Viswanath. Deepfake videos in the wild: Analysis and detection. arXiv preprint arXiv:2103.04263, 2021.\n\nFaceshifter: Towards high fidelity and occlusion aware face swapping. Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen, arXiv:1912.13457arXiv preprintLingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. Faceshifter: Towards high fidelity and occlusion aware face swapping. arXiv preprint arXiv:1912.13457, 2019.\n\nEmotions don't lie: An audio-visual deepfake detection method using affective cues. Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, arXiv:2003.067112020Aniket Bera, and Dinesh ManochaTrisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha. Emotions don't lie: An audio-visual deepfake detection method using affective cues, 2020. arXiv:2003.06711.\n\nDeepfake video detection through optical flow based cnn. Irene Amerini, Leonardo Galteri, Roberto Caldelli, Alberto Del Bimbo, Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. the IEEE/CVF International Conference on Computer Vision WorkshopsIrene Amerini, Leonardo Galteri, Roberto Caldelli, and Alberto Del Bimbo. Deepfake video detection through optical flow based cnn. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0-0, 2019.\n\nEkraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAlmageed, Iacopo Masi, and Prem Natarajan. Recurrent convolutional strategies for face manipulation detection in videos. Interfaces (GUI). 3Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAlmageed, Iacopo Masi, and Prem Natara- jan. Recurrent convolutional strategies for face manipulation detection in videos. Interfaces (GUI), 3(1), 2019.\n\nVoxceleb2: Deep speaker recognition. Arsha Joon Son Chung, Andrew Nagrani, Zisserman, arXiv:1806.05622arXiv preprintJoon Son Chung, Arsha Nagrani, and Andrew Zisserman. Voxceleb2: Deep speaker recognition. arXiv preprint arXiv:1806.05622, 2018.\n\nFast face-swap using convolutional neural networks. Iryna Korshunova, Wenzhe Shi, Joni Dambre, Lucas Theis, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Iryna Korshunova, Wenzhe Shi, Joni Dambre, and Lucas Theis. Fast face-swap using convo- lutional neural networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n\nYuxuan Wang, Daisy Skerry-Ryan, Yonghui Stanton, Ron J Wu, Navdeep Weiss, Zongheng Jaitly, Ying Yang, Zhifeng Xiao, Samy Chen, Bengio, arXiv:1703.10135Towards end-to-end speech synthesis. arXiv preprintYuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech synthesis. arXiv preprint arXiv:1703.10135, 2017.\n\nFsgan: Subject agnostic face swapping and reenactment. Yuval Nirkin, Yosi Keller, Tal Hassner, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionYuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject agnostic face swapping and reenactment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7184-7193, 2019.\n\nA lip sync expert is all you need for speech to lip generation in the wild. Rudrabha Kr Prajwal, Mukhopadhyay, P Vinay, C V Namboodiri, Jawahar, Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on MultimediaKR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and CV Jawahar. A lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM International Conference on Multimedia, pages 484-492, 2020.\n\nFace comparing -protection against fake face attacks -seasoned solution -accurate in real world. Face++, Face++. Face comparing -protection against fake face attacks -seasoned solution -accurate in real world, 2021. URL: https://www.faceplusplus.com/face-comparing.\n\nWatson speech to text. Ibm, 31IBM. Watson speech to text, 2019. [Online; accessed 31-May-2021]. URL: https://www. ibm.com/cloud/watson-speech-to-text.\n\nFace x-ray for more general face forgery detection. Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, Baining Guo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, and Baining Guo. Face x-ray for more general face forgery detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5001-5010, 2020.\n\nThinking in frequency: Face forgery detection by mining frequency-aware clues. Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, Jing Shao, European Conference on Computer Vision. SpringerYuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing Shao. Thinking in frequency: Face forgery detection by mining frequency-aware clues. In European Conference on Computer Vision, pages 86-103. Springer, 2020.\n\nLips don't lie: A generalisable and robust approach to face forgery detection. Alexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAlexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. Lips don't lie: A generalisable and robust approach to face forgery detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5039-5049, 2021.\n\nEvaluation of an Audio-Video Multimodal Deepfake Dataset Using Unimodal and Multimodal Detectors. Hasam Khalid, Minha Kim, Shahroz Tariq, Simon S Woo, 10.1145/3476099.3484315Proceedings of the 1st Workshop on Synthetic Multimedia -Audiovisual Deepfake Generation and Detection, ADGD '21. the 1st Workshop on Synthetic Multimedia -Audiovisual Deepfake Generation and Detection, ADGD '21New York, NY, USAAssociation for Computing MachineryHasam Khalid, Minha Kim, Shahroz Tariq, and Simon S. Woo. Evaluation of an Audio-Video Multimodal Deepfake Dataset Using Unimodal and Multimodal Detectors. In Proceedings of the 1st Workshop on Synthetic Multimedia -Audiovisual Deepfake Generation and Detection, ADGD '21, page 7-15, New York, NY, USA, 2021. Association for Computing Machinery. doi:10.1145/3476099.3484315.\n\nYoutube-8m: A large-scale video classification benchmark. Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan, arXiv:1609.08675arXiv preprintSami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016.\n\nFsgan -official pytorch implementation. Yuval Nirkin, Yosi Keller, Tal Hassner, 31Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan -official pytorch implementation, 2020. [Online; accessed 31-May-2021]. URL: https://github.com/YuvalNirkin/fsgan.\n\nTowards automatic extraction of monolingual and bilingual terminology. B\u00e9atrice Daille, \u00c9ric Gaussier, Jean-Marc Lang\u00e9, The 15th International Conference on Computational Linguistics. 1B\u00e9atrice Daille, \u00c9ric Gaussier, and Jean-Marc Lang\u00e9. Towards automatic extraction of monolin- gual and bilingual terminology. In COLING 1994 Volume 1: The 15th International Conference on Computational Linguistics, 1994.\n\nXception: Deep learning with depthwise separable convolutions. Fran\u00e7ois Chollet, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionFran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 1251-1258, 2017.\n\nWikipedia contributors. Hann function -Wikipedia, the free encyclopedia. Online; accessed 9Wikipedia contributors. Hann function -Wikipedia, the free encyclopedia. https://en. wikipedia.org/w/index.php?title=Hann_function&oldid=1001711522, 2021. [On- line; accessed 9-March-2021].\n\n. Deepfaketimit, DeepfakeTIMIT [41] https://www.idiap.ch/en/dataset/deepfaketimit 2018.12\n\n. Celeb-Df, Celeb-DF [34] https://github.com/yuezunli/celeb-deepfakeforensics 2019.11\n\n. Dfd Google, Google DFD [36] https://github.com/ondyari/FaceForensics 2019.09\n\n. Deeperforensics, DeeperForensics [33] https://github.com/EndlessSora/DeeperForensics-1.0 2020.05\n\n. Kodf, KoDF [37] https://aihub.or.kr/aidata/8005 2021.06\n\n. Fakeavceleb, FakeAVCeleb (Ours) https://github.com/DASH-Lab/FakeAVCeleb 2021.12\n", "annotations": {"author": "[{\"end\":185,\"start\":64},{\"end\":303,\"start\":186},{\"end\":418,\"start\":304},{\"end\":678,\"start\":419}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":70},{\"end\":199,\"start\":194},{\"end\":313,\"start\":310},{\"end\":430,\"start\":427}]", "author_first_name": "[{\"end\":69,\"start\":64},{\"end\":193,\"start\":186},{\"end\":309,\"start\":304},{\"end\":424,\"start\":419},{\"end\":426,\"start\":425}]", "author_affiliation": "[{\"end\":184,\"start\":102},{\"end\":302,\"start\":220},{\"end\":417,\"start\":335},{\"end\":530,\"start\":448},{\"end\":602,\"start\":532},{\"end\":677,\"start\":604}]", "title": "[{\"end\":61,\"start\":1},{\"end\":739,\"start\":679}]", "venue": null, "abstract": "[{\"end\":2512,\"start\":741}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2779,\"start\":2776},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2835,\"start\":2832},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2849,\"start\":2846},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2886,\"start\":2883},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3160,\"start\":3157},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3197,\"start\":3194},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3245,\"start\":3242},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3486,\"start\":3483},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3488,\"start\":3486},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3613,\"start\":3610},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3759,\"start\":3756},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4066,\"start\":4063},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4112,\"start\":4109},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4161,\"start\":4157},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4427,\"start\":4423},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4456,\"start\":4452},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4906,\"start\":4902},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4910,\"start\":4906},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4914,\"start\":4910},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4918,\"start\":4914},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4922,\"start\":4918},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4926,\"start\":4922},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4930,\"start\":4926},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4934,\"start\":4930},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4938,\"start\":4934},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4942,\"start\":4938},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4946,\"start\":4942},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4950,\"start\":4946},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4954,\"start\":4950},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4958,\"start\":4954},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4962,\"start\":4958},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4966,\"start\":4962},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4970,\"start\":4966},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4974,\"start\":4970},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4978,\"start\":4974},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4982,\"start\":4978},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5214,\"start\":5210},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5218,\"start\":5214},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5222,\"start\":5218},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5226,\"start\":5222},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5230,\"start\":5226},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5234,\"start\":5230},{\"end\":5387,\"start\":5352},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5640,\"start\":5636},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5644,\"start\":5640},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5648,\"start\":5644},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5652,\"start\":5648},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5656,\"start\":5652},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6116,\"start\":6112},{\"end\":7014,\"start\":6996},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8036,\"start\":8032},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8039,\"start\":8036},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8042,\"start\":8039},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8045,\"start\":8042},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8048,\"start\":8045},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8051,\"start\":8048},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8290,\"start\":8287},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8293,\"start\":8290},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8408,\"start\":8404},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8411,\"start\":8408},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8414,\"start\":8411},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9047,\"start\":9043},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9071,\"start\":9067},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9763,\"start\":9759},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9808,\"start\":9804},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10239,\"start\":10235},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10260,\"start\":10256},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10445,\"start\":10441},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10639,\"start\":10635},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10642,\"start\":10639},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10645,\"start\":10642},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10648,\"start\":10645},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10652,\"start\":10648},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10656,\"start\":10652},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10660,\"start\":10656},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10664,\"start\":10660},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10668,\"start\":10664},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":10672,\"start\":10668},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11782,\"start\":11778},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12050,\"start\":12046},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12211,\"start\":12207},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12301,\"start\":12297},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12477,\"start\":12473},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12506,\"start\":12502},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13426,\"start\":13422},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13447,\"start\":13443},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13462,\"start\":13458},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":14643,\"start\":14639},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14796,\"start\":14792},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15226,\"start\":15223},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":15683,\"start\":15679},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":17862,\"start\":17858},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":17878,\"start\":17874},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18019,\"start\":18016},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":18099,\"start\":18095},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":19538,\"start\":19534},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":19569,\"start\":19565},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":19613,\"start\":19609},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19737,\"start\":19734},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20364,\"start\":20360},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":24555,\"start\":24551},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":28108,\"start\":28104},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29681,\"start\":29677},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29696,\"start\":29692},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":29735,\"start\":29731},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29750,\"start\":29746},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29762,\"start\":29758},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29786,\"start\":29782},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":30417,\"start\":30413},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":30429,\"start\":30425},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":30451,\"start\":30447},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33488,\"start\":33484},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":33705,\"start\":33701},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":34384,\"start\":34380},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35069,\"start\":35068},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":36171,\"start\":36167},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":39270,\"start\":39266},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":40011,\"start\":40007},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":40051,\"start\":40047},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":40661,\"start\":40657},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":40934,\"start\":40930},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":41048,\"start\":41044},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":41051,\"start\":41048},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":41269,\"start\":41265},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":41388,\"start\":41384},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":41903,\"start\":41900},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":43024,\"start\":43020},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":43181,\"start\":43177},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":43395,\"start\":43391},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":43835,\"start\":43831},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":43895,\"start\":43891},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":44044,\"start\":44040},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":44287,\"start\":44283},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":44486,\"start\":44482},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":44501,\"start\":44497},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":44658,\"start\":44654},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":45814,\"start\":45810},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":45868,\"start\":45864},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":46789,\"start\":46785},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":49097,\"start\":49093},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":49110,\"start\":49106},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":49388,\"start\":49384},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":49496,\"start\":49492},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":49634,\"start\":49630},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":51284,\"start\":51280},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":51368,\"start\":51364},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":52158,\"start\":52154},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":52298,\"start\":52294},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":52410,\"start\":52406}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53324,\"start\":53240},{\"attributes\":{\"id\":\"fig_1\"},\"end\":53674,\"start\":53325},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54029,\"start\":53675},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54101,\"start\":54030},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54184,\"start\":54102},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54291,\"start\":54185},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":55148,\"start\":54292},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":55264,\"start\":55149},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":55906,\"start\":55265},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56608,\"start\":55907},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56776,\"start\":56609}]", "paragraph": "[{\"end\":4555,\"start\":2528},{\"end\":6012,\"start\":4557},{\"end\":7161,\"start\":6014},{\"end\":7414,\"start\":7163},{\"end\":7622,\"start\":7416},{\"end\":7913,\"start\":7624},{\"end\":9031,\"start\":7943},{\"end\":9564,\"start\":9033},{\"end\":10601,\"start\":9566},{\"end\":11200,\"start\":10603},{\"end\":11836,\"start\":11202},{\"end\":13278,\"start\":11838},{\"end\":14470,\"start\":13280},{\"end\":15543,\"start\":14472},{\"end\":16600,\"start\":15581},{\"end\":17683,\"start\":16602},{\"end\":18855,\"start\":17685},{\"end\":19784,\"start\":18925},{\"end\":21076,\"start\":19786},{\"end\":21377,\"start\":21078},{\"end\":21393,\"start\":21389},{\"end\":22498,\"start\":21576},{\"end\":23542,\"start\":22534},{\"end\":23950,\"start\":23544},{\"end\":25405,\"start\":23952},{\"end\":26611,\"start\":25407},{\"end\":27374,\"start\":26613},{\"end\":27801,\"start\":27414},{\"end\":28613,\"start\":27803},{\"end\":29049,\"start\":28651},{\"end\":29500,\"start\":29051},{\"end\":29965,\"start\":29502},{\"end\":30880,\"start\":29967},{\"end\":31894,\"start\":30882},{\"end\":33112,\"start\":31896},{\"end\":33802,\"start\":33114},{\"end\":34529,\"start\":33833},{\"end\":35354,\"start\":34531},{\"end\":36252,\"start\":35356},{\"end\":37246,\"start\":36267},{\"end\":37874,\"start\":37265},{\"end\":38018,\"start\":37912},{\"end\":38174,\"start\":38020},{\"end\":38372,\"start\":38199},{\"end\":38789,\"start\":38398},{\"end\":38956,\"start\":38807},{\"end\":39101,\"start\":38989},{\"end\":39999,\"start\":39116},{\"end\":40920,\"start\":40001},{\"end\":41791,\"start\":40922},{\"end\":42592,\"start\":41806},{\"end\":43010,\"start\":42636},{\"end\":43166,\"start\":43012},{\"end\":43367,\"start\":43168},{\"end\":43820,\"start\":43369},{\"end\":44032,\"start\":43822},{\"end\":44275,\"start\":44034},{\"end\":44469,\"start\":44277},{\"end\":44639,\"start\":44471},{\"end\":44841,\"start\":44641},{\"end\":45352,\"start\":44863},{\"end\":45803,\"start\":45374},{\"end\":46167,\"start\":45805},{\"end\":47159,\"start\":46169},{\"end\":47519,\"start\":47186},{\"end\":48045,\"start\":47521},{\"end\":48522,\"start\":48074},{\"end\":48774,\"start\":48547},{\"end\":48990,\"start\":48776},{\"end\":49497,\"start\":49036},{\"end\":50675,\"start\":49545},{\"end\":51033,\"start\":50713},{\"end\":51981,\"start\":51058},{\"end\":53214,\"start\":52008}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18924,\"start\":18856},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21575,\"start\":21394},{\"attributes\":{\"id\":\"formula_2\"},\"end\":39115,\"start\":39102}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6556,\"start\":6549},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22824,\"start\":22817},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23948,\"start\":23941},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24316,\"start\":24309},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25753,\"start\":25746},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27056,\"start\":27049},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31243,\"start\":31236},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31579,\"start\":31572},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33553,\"start\":33546}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2526,\"start\":2514},{\"attributes\":{\"n\":\"2\"},\"end\":7941,\"start\":7916},{\"attributes\":{\"n\":\"3\"},\"end\":15579,\"start\":15546},{\"end\":21387,\"start\":21380},{\"attributes\":{\"n\":\"3.1\"},\"end\":22532,\"start\":22501},{\"attributes\":{\"n\":\"3.2\"},\"end\":27412,\"start\":27377},{\"attributes\":{\"n\":\"4\"},\"end\":28649,\"start\":28616},{\"attributes\":{\"n\":\"5\"},\"end\":33831,\"start\":33805},{\"attributes\":{\"n\":\"6\"},\"end\":36265,\"start\":36255},{\"end\":37263,\"start\":37249},{\"end\":37898,\"start\":37877},{\"end\":37910,\"start\":37901},{\"end\":38197,\"start\":38177},{\"end\":38396,\"start\":38375},{\"end\":38805,\"start\":38792},{\"end\":38987,\"start\":38959},{\"end\":41804,\"start\":41794},{\"end\":42634,\"start\":42595},{\"end\":44861,\"start\":44844},{\"end\":45372,\"start\":45355},{\"end\":47184,\"start\":47162},{\"end\":48072,\"start\":48048},{\"end\":48545,\"start\":48525},{\"end\":49034,\"start\":48993},{\"end\":49543,\"start\":49500},{\"end\":50711,\"start\":50678},{\"end\":51056,\"start\":51036},{\"end\":52006,\"start\":51984},{\"end\":53239,\"start\":53217},{\"end\":53251,\"start\":53241},{\"end\":53336,\"start\":53326},{\"end\":53686,\"start\":53676},{\"end\":54041,\"start\":54031},{\"end\":54113,\"start\":54103},{\"end\":54196,\"start\":54186},{\"end\":54302,\"start\":54293},{\"end\":55159,\"start\":55150},{\"end\":55917,\"start\":55908}]", "table": "[{\"end\":55148,\"start\":54391},{\"end\":55906,\"start\":55268},{\"end\":56608,\"start\":56032}]", "figure_caption": "[{\"end\":53324,\"start\":53253},{\"end\":53674,\"start\":53338},{\"end\":54029,\"start\":53688},{\"end\":54101,\"start\":54043},{\"end\":54184,\"start\":54115},{\"end\":54291,\"start\":54198},{\"end\":54391,\"start\":54304},{\"end\":55264,\"start\":55161},{\"end\":55268,\"start\":55267},{\"end\":56032,\"start\":55919},{\"end\":56776,\"start\":56611}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18034,\"start\":18026},{\"end\":18166,\"start\":18158},{\"end\":22393,\"start\":22385},{\"end\":22910,\"start\":22902},{\"end\":22943,\"start\":22935},{\"end\":24376,\"start\":24368},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24414,\"start\":24406},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24771,\"start\":24762},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25987,\"start\":25979},{\"end\":26057,\"start\":26049},{\"end\":26827,\"start\":26819},{\"end\":27234,\"start\":27226},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27503,\"start\":27495},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27789,\"start\":27781},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27908,\"start\":27900},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28283,\"start\":28275},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28484,\"start\":28474},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32033,\"start\":32025},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32156,\"start\":32148},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32451,\"start\":32443},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33174,\"start\":33166},{\"end\":38509,\"start\":38500},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":47197,\"start\":47189},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":47532,\"start\":47524},{\"end\":48882,\"start\":48874},{\"end\":49075,\"start\":49067},{\"end\":49576,\"start\":49568},{\"end\":50201,\"start\":50193},{\"end\":50415,\"start\":50407},{\"end\":50880,\"start\":50872},{\"end\":51069,\"start\":51061},{\"end\":51446,\"start\":51438},{\"end\":52928,\"start\":52919}]", "bib_author_first_name": "[{\"end\":57030,\"start\":57029},{\"end\":57048,\"start\":57040},{\"end\":57305,\"start\":57304},{\"end\":57315,\"start\":57311},{\"end\":57333,\"start\":57328},{\"end\":57353,\"start\":57349},{\"end\":57366,\"start\":57361},{\"end\":57378,\"start\":57371},{\"end\":57398,\"start\":57393},{\"end\":57412,\"start\":57406},{\"end\":57774,\"start\":57769},{\"end\":57790,\"start\":57785},{\"end\":57813,\"start\":57807},{\"end\":57830,\"start\":57823},{\"end\":57846,\"start\":57840},{\"end\":58181,\"start\":58179},{\"end\":58189,\"start\":58187},{\"end\":58200,\"start\":58197},{\"end\":58202,\"start\":58201},{\"end\":58214,\"start\":58210},{\"end\":58229,\"start\":58221},{\"end\":58239,\"start\":58236},{\"end\":58252,\"start\":58245},{\"end\":58266,\"start\":58259},{\"end\":58282,\"start\":58275},{\"end\":58296,\"start\":58289},{\"end\":58302,\"start\":58297},{\"end\":58318,\"start\":58311},{\"end\":58663,\"start\":58655},{\"end\":58665,\"start\":58664},{\"end\":58693,\"start\":58685},{\"end\":59038,\"start\":59037},{\"end\":59053,\"start\":59047},{\"end\":59066,\"start\":59060},{\"end\":59076,\"start\":59073},{\"end\":59088,\"start\":59083},{\"end\":59313,\"start\":59307},{\"end\":59328,\"start\":59321},{\"end\":59731,\"start\":59727},{\"end\":59930,\"start\":59927},{\"end\":60554,\"start\":60549},{\"end\":60945,\"start\":60938},{\"end\":60958,\"start\":60953},{\"end\":60972,\"start\":60965},{\"end\":61267,\"start\":61266},{\"end\":61280,\"start\":61273},{\"end\":61293,\"start\":61289},{\"end\":61548,\"start\":61545},{\"end\":61561,\"start\":61555},{\"end\":61571,\"start\":61566},{\"end\":61966,\"start\":61960},{\"end\":61982,\"start\":61975},{\"end\":61998,\"start\":61991},{\"end\":62014,\"start\":62010},{\"end\":62421,\"start\":62416},{\"end\":62435,\"start\":62430},{\"end\":62437,\"start\":62436},{\"end\":62867,\"start\":62861},{\"end\":62881,\"start\":62877},{\"end\":62895,\"start\":62889},{\"end\":62908,\"start\":62900},{\"end\":62917,\"start\":62913},{\"end\":62929,\"start\":62926},{\"end\":63174,\"start\":63169},{\"end\":63183,\"start\":63182},{\"end\":63511,\"start\":63505},{\"end\":63526,\"start\":63516},{\"end\":63539,\"start\":63534},{\"end\":63896,\"start\":63889},{\"end\":63911,\"start\":63904},{\"end\":63924,\"start\":63917},{\"end\":63936,\"start\":63930},{\"end\":63948,\"start\":63943},{\"end\":63950,\"start\":63949},{\"end\":64616,\"start\":64609},{\"end\":64631,\"start\":64624},{\"end\":64644,\"start\":64637},{\"end\":64656,\"start\":64650},{\"end\":64668,\"start\":64663},{\"end\":64670,\"start\":64669},{\"end\":65261,\"start\":65254},{\"end\":65276,\"start\":65269},{\"end\":65287,\"start\":65282},{\"end\":65853,\"start\":65846},{\"end\":65866,\"start\":65859},{\"end\":65880,\"start\":65874},{\"end\":65892,\"start\":65887},{\"end\":65894,\"start\":65893},{\"end\":66389,\"start\":66382},{\"end\":66402,\"start\":66395},{\"end\":66417,\"start\":66410},{\"end\":66428,\"start\":66423},{\"end\":66430,\"start\":66429},{\"end\":67001,\"start\":66991},{\"end\":67015,\"start\":67008},{\"end\":67029,\"start\":67022},{\"end\":67524,\"start\":67514},{\"end\":67538,\"start\":67531},{\"end\":67552,\"start\":67545},{\"end\":67943,\"start\":67933},{\"end\":67955,\"start\":67950},{\"end\":67958,\"start\":67956},{\"end\":67972,\"start\":67965},{\"end\":67983,\"start\":67978},{\"end\":67995,\"start\":67989},{\"end\":68330,\"start\":68323},{\"end\":68345,\"start\":68338},{\"end\":68358,\"start\":68351},{\"end\":68656,\"start\":68651},{\"end\":68669,\"start\":68662},{\"end\":68682,\"start\":68677},{\"end\":68684,\"start\":68683},{\"end\":69243,\"start\":69238},{\"end\":69256,\"start\":69249},{\"end\":69269,\"start\":69264},{\"end\":69271,\"start\":69270},{\"end\":69874,\"start\":69867},{\"end\":69884,\"start\":69880},{\"end\":69897,\"start\":69890},{\"end\":70212,\"start\":70208},{\"end\":70227,\"start\":70221},{\"end\":70442,\"start\":70436},{\"end\":70453,\"start\":70450},{\"end\":70463,\"start\":70458},{\"end\":70472,\"start\":70468},{\"end\":70490,\"start\":70479},{\"end\":70975,\"start\":70969},{\"end\":70983,\"start\":70980},{\"end\":70992,\"start\":70990},{\"end\":71006,\"start\":70998},{\"end\":71016,\"start\":71011},{\"end\":71413,\"start\":71408},{\"end\":71431,\"start\":71425},{\"end\":71443,\"start\":71440},{\"end\":71457,\"start\":71452},{\"end\":71466,\"start\":71462},{\"end\":71481,\"start\":71474},{\"end\":71503,\"start\":71488},{\"end\":71846,\"start\":71839},{\"end\":71862,\"start\":71856},{\"end\":71879,\"start\":71874},{\"end\":71900,\"start\":71891},{\"end\":71914,\"start\":71908},{\"end\":71930,\"start\":71922},{\"end\":72394,\"start\":72387},{\"end\":72409,\"start\":72401},{\"end\":72423,\"start\":72415},{\"end\":72436,\"start\":72429},{\"end\":72451,\"start\":72443},{\"end\":72737,\"start\":72731},{\"end\":72750,\"start\":72745},{\"end\":72768,\"start\":72758},{\"end\":72789,\"start\":72782},{\"end\":73207,\"start\":73206},{\"end\":73221,\"start\":73214},{\"end\":73542,\"start\":73539},{\"end\":73556,\"start\":73549},{\"end\":73580,\"start\":73568},{\"end\":73596,\"start\":73590},{\"end\":73613,\"start\":73606},{\"end\":73631,\"start\":73623},{\"end\":73641,\"start\":73639},{\"end\":73659,\"start\":73654},{\"end\":73673,\"start\":73669},{\"end\":74114,\"start\":74109},{\"end\":74135,\"start\":74126},{\"end\":74351,\"start\":74347},{\"end\":74365,\"start\":74361},{\"end\":74382,\"start\":74376},{\"end\":74397,\"start\":74391},{\"end\":74415,\"start\":74414},{\"end\":74429,\"start\":74423},{\"end\":74454,\"start\":74447},{\"end\":75053,\"start\":75052},{\"end\":75067,\"start\":75060},{\"end\":75381,\"start\":75374},{\"end\":75394,\"start\":75389},{\"end\":75408,\"start\":75401},{\"end\":75747,\"start\":75742},{\"end\":75763,\"start\":75759},{\"end\":75774,\"start\":75771},{\"end\":76174,\"start\":76169},{\"end\":76358,\"start\":76350},{\"end\":76368,\"start\":76364},{\"end\":76684,\"start\":76678},{\"end\":76695,\"start\":76689},{\"end\":76709,\"start\":76702},{\"end\":76723,\"start\":76716},{\"end\":76733,\"start\":76729},{\"end\":76745,\"start\":76738},{\"end\":76754,\"start\":76750},{\"end\":76768,\"start\":76761},{\"end\":77284,\"start\":77279},{\"end\":77301,\"start\":77295},{\"end\":77563,\"start\":77558},{\"end\":77581,\"start\":77572},{\"end\":77593,\"start\":77589},{\"end\":77975,\"start\":77970},{\"end\":77996,\"start\":77987},{\"end\":78191,\"start\":78184},{\"end\":78200,\"start\":78196},{\"end\":78218,\"start\":78212},{\"end\":78235,\"start\":78226},{\"end\":78255,\"start\":78250},{\"end\":78693,\"start\":78686},{\"end\":78705,\"start\":78698},{\"end\":78714,\"start\":78711},{\"end\":78725,\"start\":78721},{\"end\":78736,\"start\":78732},{\"end\":79032,\"start\":79026},{\"end\":79048,\"start\":79041},{\"end\":79068,\"start\":79063},{\"end\":79385,\"start\":79380},{\"end\":79403,\"start\":79395},{\"end\":79420,\"start\":79413},{\"end\":79438,\"start\":79431},{\"end\":79442,\"start\":79439},{\"end\":79840,\"start\":79834},{\"end\":79854,\"start\":79848},{\"end\":79867,\"start\":79862},{\"end\":80272,\"start\":80267},{\"end\":80295,\"start\":80289},{\"end\":80533,\"start\":80528},{\"end\":80552,\"start\":80546},{\"end\":80562,\"start\":80558},{\"end\":80576,\"start\":80571},{\"end\":80929,\"start\":80923},{\"end\":80941,\"start\":80936},{\"end\":80962,\"start\":80955},{\"end\":80975,\"start\":80972},{\"end\":80977,\"start\":80976},{\"end\":80989,\"start\":80982},{\"end\":81005,\"start\":80997},{\"end\":81018,\"start\":81014},{\"end\":81032,\"start\":81025},{\"end\":81043,\"start\":81039},{\"end\":81417,\"start\":81412},{\"end\":81430,\"start\":81426},{\"end\":81442,\"start\":81439},{\"end\":81864,\"start\":81856},{\"end\":81892,\"start\":81891},{\"end\":81901,\"start\":81900},{\"end\":81903,\"start\":81902},{\"end\":82762,\"start\":82755},{\"end\":82774,\"start\":82767},{\"end\":82784,\"start\":82780},{\"end\":82795,\"start\":82792},{\"end\":82806,\"start\":82802},{\"end\":82817,\"start\":82813},{\"end\":82830,\"start\":82823},{\"end\":83317,\"start\":83311},{\"end\":83330,\"start\":83324},{\"end\":83338,\"start\":83336},{\"end\":83352,\"start\":83346},{\"end\":83363,\"start\":83359},{\"end\":83723,\"start\":83713},{\"end\":83747,\"start\":83735},{\"end\":83767,\"start\":83760},{\"end\":83782,\"start\":83778},{\"end\":84314,\"start\":84309},{\"end\":84328,\"start\":84323},{\"end\":84341,\"start\":84334},{\"end\":84354,\"start\":84349},{\"end\":84356,\"start\":84355},{\"end\":85086,\"start\":85082},{\"end\":85107,\"start\":85101},{\"end\":85125,\"start\":85117},{\"end\":85135,\"start\":85131},{\"end\":85150,\"start\":85144},{\"end\":85173,\"start\":85161},{\"end\":85197,\"start\":85187},{\"end\":85527,\"start\":85522},{\"end\":85540,\"start\":85536},{\"end\":85552,\"start\":85549},{\"end\":85809,\"start\":85801},{\"end\":85822,\"start\":85818},{\"end\":85842,\"start\":85833},{\"end\":86208,\"start\":86200},{\"end\":87013,\"start\":87010}]", "bib_author_last_name": "[{\"end\":57038,\"start\":57031},{\"end\":57053,\"start\":57049},{\"end\":57061,\"start\":57055},{\"end\":57309,\"start\":57306},{\"end\":57326,\"start\":57316},{\"end\":57347,\"start\":57334},{\"end\":57359,\"start\":57354},{\"end\":57369,\"start\":57367},{\"end\":57391,\"start\":57379},{\"end\":57404,\"start\":57399},{\"end\":57422,\"start\":57413},{\"end\":57430,\"start\":57424},{\"end\":57783,\"start\":57775},{\"end\":57805,\"start\":57791},{\"end\":57821,\"start\":57814},{\"end\":57838,\"start\":57831},{\"end\":57860,\"start\":57847},{\"end\":58185,\"start\":58182},{\"end\":58195,\"start\":58190},{\"end\":58208,\"start\":58203},{\"end\":58219,\"start\":58215},{\"end\":58234,\"start\":58230},{\"end\":58243,\"start\":58240},{\"end\":58257,\"start\":58253},{\"end\":58273,\"start\":58267},{\"end\":58287,\"start\":58283},{\"end\":58309,\"start\":58303},{\"end\":58321,\"start\":58319},{\"end\":58683,\"start\":58666},{\"end\":58700,\"start\":58694},{\"end\":58710,\"start\":58702},{\"end\":59045,\"start\":59039},{\"end\":59058,\"start\":59054},{\"end\":59071,\"start\":59067},{\"end\":59081,\"start\":59077},{\"end\":59093,\"start\":59089},{\"end\":59099,\"start\":59095},{\"end\":59319,\"start\":59314},{\"end\":59337,\"start\":59329},{\"end\":59737,\"start\":59732},{\"end\":59936,\"start\":59931},{\"end\":60561,\"start\":60555},{\"end\":60951,\"start\":60946},{\"end\":60963,\"start\":60959},{\"end\":60976,\"start\":60973},{\"end\":61271,\"start\":61268},{\"end\":61287,\"start\":61281},{\"end\":61303,\"start\":61294},{\"end\":61312,\"start\":61305},{\"end\":61553,\"start\":61549},{\"end\":61564,\"start\":61562},{\"end\":61575,\"start\":61572},{\"end\":61973,\"start\":61967},{\"end\":61989,\"start\":61983},{\"end\":62008,\"start\":61999},{\"end\":62022,\"start\":62015},{\"end\":62428,\"start\":62422},{\"end\":62441,\"start\":62438},{\"end\":62875,\"start\":62868},{\"end\":62887,\"start\":62882},{\"end\":62898,\"start\":62896},{\"end\":62911,\"start\":62909},{\"end\":62924,\"start\":62918},{\"end\":62932,\"start\":62930},{\"end\":63180,\"start\":63175},{\"end\":63190,\"start\":63184},{\"end\":63196,\"start\":63192},{\"end\":63514,\"start\":63512},{\"end\":63532,\"start\":63527},{\"end\":63543,\"start\":63540},{\"end\":63902,\"start\":63897},{\"end\":63915,\"start\":63912},{\"end\":63928,\"start\":63925},{\"end\":63941,\"start\":63937},{\"end\":63954,\"start\":63951},{\"end\":64622,\"start\":64617},{\"end\":64635,\"start\":64632},{\"end\":64648,\"start\":64645},{\"end\":64661,\"start\":64657},{\"end\":64674,\"start\":64671},{\"end\":65267,\"start\":65262},{\"end\":65280,\"start\":65277},{\"end\":65291,\"start\":65288},{\"end\":65857,\"start\":65854},{\"end\":65872,\"start\":65867},{\"end\":65885,\"start\":65881},{\"end\":65898,\"start\":65895},{\"end\":66393,\"start\":66390},{\"end\":66408,\"start\":66403},{\"end\":66421,\"start\":66418},{\"end\":66434,\"start\":66431},{\"end\":67006,\"start\":67002},{\"end\":67020,\"start\":67016},{\"end\":67033,\"start\":67030},{\"end\":67529,\"start\":67525},{\"end\":67543,\"start\":67539},{\"end\":67556,\"start\":67553},{\"end\":67948,\"start\":67944},{\"end\":67963,\"start\":67959},{\"end\":67976,\"start\":67973},{\"end\":67987,\"start\":67984},{\"end\":68336,\"start\":68331},{\"end\":68349,\"start\":68346},{\"end\":68362,\"start\":68359},{\"end\":68660,\"start\":68657},{\"end\":68675,\"start\":68670},{\"end\":68688,\"start\":68685},{\"end\":69247,\"start\":69244},{\"end\":69262,\"start\":69257},{\"end\":69275,\"start\":69272},{\"end\":69878,\"start\":69875},{\"end\":69888,\"start\":69885},{\"end\":69901,\"start\":69898},{\"end\":70219,\"start\":70213},{\"end\":70233,\"start\":70228},{\"end\":70448,\"start\":70443},{\"end\":70456,\"start\":70454},{\"end\":70466,\"start\":70464},{\"end\":70477,\"start\":70473},{\"end\":70494,\"start\":70491},{\"end\":70978,\"start\":70976},{\"end\":70988,\"start\":70984},{\"end\":70996,\"start\":70993},{\"end\":71009,\"start\":71007},{\"end\":71020,\"start\":71017},{\"end\":71423,\"start\":71414},{\"end\":71438,\"start\":71432},{\"end\":71450,\"start\":71444},{\"end\":71460,\"start\":71458},{\"end\":71472,\"start\":71467},{\"end\":71486,\"start\":71482},{\"end\":71510,\"start\":71504},{\"end\":71854,\"start\":71847},{\"end\":71872,\"start\":71863},{\"end\":71889,\"start\":71880},{\"end\":71906,\"start\":71901},{\"end\":71920,\"start\":71915},{\"end\":71938,\"start\":71931},{\"end\":72399,\"start\":72395},{\"end\":72413,\"start\":72410},{\"end\":72427,\"start\":72424},{\"end\":72441,\"start\":72437},{\"end\":72456,\"start\":72452},{\"end\":72743,\"start\":72738},{\"end\":72756,\"start\":72751},{\"end\":72780,\"start\":72769},{\"end\":72795,\"start\":72790},{\"end\":73212,\"start\":73208},{\"end\":73224,\"start\":73222},{\"end\":73229,\"start\":73226},{\"end\":73547,\"start\":73543},{\"end\":73566,\"start\":73557},{\"end\":73588,\"start\":73581},{\"end\":73604,\"start\":73597},{\"end\":73621,\"start\":73614},{\"end\":73637,\"start\":73632},{\"end\":73652,\"start\":73642},{\"end\":73667,\"start\":73660},{\"end\":73682,\"start\":73674},{\"end\":73696,\"start\":73684},{\"end\":74124,\"start\":74115},{\"end\":74142,\"start\":74136},{\"end\":74359,\"start\":74352},{\"end\":74374,\"start\":74366},{\"end\":74389,\"start\":74383},{\"end\":74403,\"start\":74398},{\"end\":74412,\"start\":74405},{\"end\":74421,\"start\":74416},{\"end\":74439,\"start\":74430},{\"end\":74445,\"start\":74441},{\"end\":74461,\"start\":74455},{\"end\":74472,\"start\":74463},{\"end\":75058,\"start\":75054},{\"end\":75070,\"start\":75068},{\"end\":75075,\"start\":75072},{\"end\":75387,\"start\":75382},{\"end\":75399,\"start\":75395},{\"end\":75412,\"start\":75409},{\"end\":75757,\"start\":75748},{\"end\":75769,\"start\":75764},{\"end\":75781,\"start\":75775},{\"end\":76180,\"start\":76175},{\"end\":76362,\"start\":76359},{\"end\":76371,\"start\":76369},{\"end\":76687,\"start\":76685},{\"end\":76700,\"start\":76696},{\"end\":76714,\"start\":76710},{\"end\":76727,\"start\":76724},{\"end\":76736,\"start\":76734},{\"end\":76748,\"start\":76746},{\"end\":76759,\"start\":76755},{\"end\":76773,\"start\":76769},{\"end\":77293,\"start\":77285},{\"end\":77311,\"start\":77302},{\"end\":77570,\"start\":77564},{\"end\":77587,\"start\":77582},{\"end\":77604,\"start\":77594},{\"end\":77985,\"start\":77976},{\"end\":78003,\"start\":77997},{\"end\":78194,\"start\":78192},{\"end\":78210,\"start\":78201},{\"end\":78224,\"start\":78219},{\"end\":78248,\"start\":78236},{\"end\":78264,\"start\":78256},{\"end\":78696,\"start\":78694},{\"end\":78709,\"start\":78706},{\"end\":78719,\"start\":78715},{\"end\":78730,\"start\":78726},{\"end\":78740,\"start\":78737},{\"end\":79039,\"start\":79033},{\"end\":79061,\"start\":79049},{\"end\":79076,\"start\":79069},{\"end\":79393,\"start\":79386},{\"end\":79411,\"start\":79404},{\"end\":79429,\"start\":79421},{\"end\":79448,\"start\":79443},{\"end\":79846,\"start\":79841},{\"end\":79860,\"start\":79855},{\"end\":79875,\"start\":79868},{\"end\":80287,\"start\":80273},{\"end\":80303,\"start\":80296},{\"end\":80314,\"start\":80305},{\"end\":80544,\"start\":80534},{\"end\":80556,\"start\":80553},{\"end\":80569,\"start\":80563},{\"end\":80582,\"start\":80577},{\"end\":80934,\"start\":80930},{\"end\":80953,\"start\":80942},{\"end\":80970,\"start\":80963},{\"end\":80980,\"start\":80978},{\"end\":80995,\"start\":80990},{\"end\":81012,\"start\":81006},{\"end\":81023,\"start\":81019},{\"end\":81037,\"start\":81033},{\"end\":81048,\"start\":81044},{\"end\":81056,\"start\":81050},{\"end\":81424,\"start\":81418},{\"end\":81437,\"start\":81431},{\"end\":81450,\"start\":81443},{\"end\":81875,\"start\":81865},{\"end\":81889,\"start\":81877},{\"end\":81898,\"start\":81893},{\"end\":81914,\"start\":81904},{\"end\":81923,\"start\":81916},{\"end\":82387,\"start\":82381},{\"end\":82577,\"start\":82574},{\"end\":82765,\"start\":82763},{\"end\":82778,\"start\":82775},{\"end\":82790,\"start\":82785},{\"end\":82800,\"start\":82796},{\"end\":82811,\"start\":82807},{\"end\":82821,\"start\":82818},{\"end\":82834,\"start\":82831},{\"end\":83322,\"start\":83318},{\"end\":83334,\"start\":83331},{\"end\":83344,\"start\":83339},{\"end\":83357,\"start\":83353},{\"end\":83368,\"start\":83364},{\"end\":83733,\"start\":83724},{\"end\":83758,\"start\":83748},{\"end\":83776,\"start\":83768},{\"end\":83789,\"start\":83783},{\"end\":84321,\"start\":84315},{\"end\":84332,\"start\":84329},{\"end\":84347,\"start\":84342},{\"end\":84360,\"start\":84357},{\"end\":85099,\"start\":85087},{\"end\":85115,\"start\":85108},{\"end\":85129,\"start\":85126},{\"end\":85142,\"start\":85136},{\"end\":85159,\"start\":85151},{\"end\":85185,\"start\":85174},{\"end\":85214,\"start\":85198},{\"end\":85534,\"start\":85528},{\"end\":85547,\"start\":85541},{\"end\":85560,\"start\":85553},{\"end\":85816,\"start\":85810},{\"end\":85831,\"start\":85823},{\"end\":85848,\"start\":85843},{\"end\":86216,\"start\":86209},{\"end\":86845,\"start\":86832},{\"end\":86931,\"start\":86923},{\"end\":87020,\"start\":87014},{\"end\":87105,\"start\":87090},{\"end\":87194,\"start\":87190},{\"end\":87260,\"start\":87249}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":58526038},\"end\":57300,\"start\":56968},{\"attributes\":{\"doi\":\"arXiv:1406.2661\",\"id\":\"b1\"},\"end\":57695,\"start\":57302},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":209531954},\"end\":58091,\"start\":57697},{\"attributes\":{\"doi\":\"arXiv:1806.04558\",\"id\":\"b3\"},\"end\":58597,\"start\":58093},{\"attributes\":{\"id\":\"b4\"},\"end\":58994,\"start\":58599},{\"attributes\":{\"doi\":\"arXiv:1802.06006\",\"id\":\"b5\"},\"end\":59249,\"start\":58996},{\"attributes\":{\"id\":\"b6\"},\"end\":59658,\"start\":59251},{\"attributes\":{\"id\":\"b7\"},\"end\":59857,\"start\":59660},{\"attributes\":{\"id\":\"b8\"},\"end\":60196,\"start\":59859},{\"attributes\":{\"id\":\"b9\"},\"end\":60468,\"start\":60198},{\"attributes\":{\"id\":\"b10\"},\"end\":60821,\"start\":60470},{\"attributes\":{\"doi\":\"arXiv:2103.00847\",\"id\":\"b11\"},\"end\":61205,\"start\":60823},{\"attributes\":{\"doi\":\"arXiv:1910.12467\",\"id\":\"b12\"},\"end\":61492,\"start\":61207},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53295714},\"end\":61899,\"start\":61494},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52157475},\"end\":62338,\"start\":61901},{\"attributes\":{\"doi\":\"10.1109/CVPRW50498.2020.00336\",\"id\":\"b15\",\"matched_paper_id\":220891392},\"end\":62814,\"start\":62340},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195732375},\"end\":63109,\"start\":62816},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":61808533},\"end\":63503,\"start\":63111},{\"attributes\":{\"doi\":\"arXiv:1806.02877\",\"id\":\"b18\"},\"end\":63816,\"start\":63505},{\"attributes\":{\"doi\":\"10.1145/3267357.3267367\",\"id\":\"b19\",\"matched_paper_id\":53352098},\"end\":64535,\"start\":63818},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":142503473},\"end\":64879,\"start\":64537},{\"attributes\":{\"doi\":\"10.1145/3297280.3297410\",\"id\":\"b21\"},\"end\":65166,\"start\":64881},{\"attributes\":{\"doi\":\"10.1145/3442381.3449809\",\"id\":\"b22\",\"matched_paper_id\":233481268},\"end\":65736,\"start\":65168},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":233643408},\"end\":66169,\"start\":65738},{\"attributes\":{\"doi\":\"10.1016/j.asoc.2021.107256\",\"id\":\"b24\"},\"end\":66290,\"start\":66171},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":234482487},\"end\":66873,\"start\":66292},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":207971941},\"end\":67438,\"start\":66875},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":209862460},\"end\":67877,\"start\":67440},{\"attributes\":{\"id\":\"b28\"},\"end\":68247,\"start\":67879},{\"attributes\":{\"doi\":\"arXiv:2009.07480\",\"id\":\"b29\"},\"end\":68551,\"start\":68249},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":235248011},\"end\":69145,\"start\":68553},{\"attributes\":{\"doi\":\"10.1145/3474085.3475535\",\"id\":\"b31\",\"matched_paper_id\":235742737},\"end\":69805,\"start\":69147},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":211298577},\"end\":70156,\"start\":69807},{\"attributes\":{\"id\":\"b33\"},\"end\":70352,\"start\":70158},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":210116488},\"end\":70899,\"start\":70354},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":212726430},\"end\":71404,\"start\":70901},{\"attributes\":{\"doi\":\"arXiv:2006.07397\",\"id\":\"b36\"},\"end\":71774,\"start\":71406},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":59292011},\"end\":72330,\"start\":71776},{\"attributes\":{\"doi\":\"arXiv:2103.10094\",\"id\":\"b38\"},\"end\":72656,\"start\":72332},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":231572709},\"end\":73085,\"start\":72658},{\"attributes\":{\"doi\":\"arXiv:2112.03553\",\"id\":\"b40\"},\"end\":73445,\"start\":73087},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":211532840},\"end\":74036,\"start\":73447},{\"attributes\":{\"doi\":\"arXiv:1812.08685\",\"id\":\"b42\"},\"end\":74321,\"start\":74038},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":8156476},\"end\":74967,\"start\":74323},{\"attributes\":{\"doi\":\"arXiv:2112.08050\",\"id\":\"b44\"},\"end\":75257,\"start\":74969},{\"attributes\":{\"doi\":\"arXiv:2103.00847\",\"id\":\"b45\"},\"end\":75641,\"start\":75259},{\"attributes\":{\"doi\":\"arXiv:1910.08854\",\"id\":\"b46\"},\"end\":75962,\"start\":75643},{\"attributes\":{\"id\":\"b47\"},\"end\":76128,\"start\":75964},{\"attributes\":{\"id\":\"b48\"},\"end\":76274,\"start\":76130},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":167217261},\"end\":76600,\"start\":76276},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":212633673},\"end\":77209,\"start\":76602},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b51\"},\"end\":77484,\"start\":77211},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":61810032},\"end\":77897,\"start\":77486},{\"attributes\":{\"doi\":\"arXiv:1812.08685\",\"id\":\"b53\"},\"end\":78182,\"start\":77899},{\"attributes\":{\"doi\":\"arXiv:2103.04263\",\"id\":\"b54\"},\"end\":78614,\"start\":78184},{\"attributes\":{\"doi\":\"arXiv:1912.13457\",\"id\":\"b55\"},\"end\":78940,\"start\":78616},{\"attributes\":{\"doi\":\"arXiv:2003.06711\",\"id\":\"b56\"},\"end\":79321,\"start\":78942},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":207900737},\"end\":79832,\"start\":79323},{\"attributes\":{\"id\":\"b58\"},\"end\":80228,\"start\":79834},{\"attributes\":{\"doi\":\"arXiv:1806.05622\",\"id\":\"b59\"},\"end\":80474,\"start\":80230},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":14191515},\"end\":80921,\"start\":80476},{\"attributes\":{\"doi\":\"arXiv:1703.10135\",\"id\":\"b61\"},\"end\":81355,\"start\":80923},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":201058495},\"end\":81778,\"start\":81357},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":221266065},\"end\":82282,\"start\":81780},{\"attributes\":{\"id\":\"b64\"},\"end\":82549,\"start\":82284},{\"attributes\":{\"id\":\"b65\"},\"end\":82701,\"start\":82551},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":209516424},\"end\":83230,\"start\":82703},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":220647499},\"end\":83632,\"start\":83232},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":229156239},\"end\":84209,\"start\":83634},{\"attributes\":{\"doi\":\"10.1145/3476099.3484315\",\"id\":\"b69\",\"matched_paper_id\":237431429},\"end\":85022,\"start\":84211},{\"attributes\":{\"doi\":\"arXiv:1609.08675\",\"id\":\"b70\"},\"end\":85480,\"start\":85024},{\"attributes\":{\"id\":\"b71\"},\"end\":85728,\"start\":85482},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":6116235},\"end\":86135,\"start\":85730},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":2375110},\"end\":86546,\"start\":86137},{\"attributes\":{\"id\":\"b74\"},\"end\":86828,\"start\":86548},{\"attributes\":{\"id\":\"b75\"},\"end\":86919,\"start\":86830},{\"attributes\":{\"id\":\"b76\"},\"end\":87006,\"start\":86921},{\"attributes\":{\"id\":\"b77\"},\"end\":87086,\"start\":87008},{\"attributes\":{\"id\":\"b78\"},\"end\":87186,\"start\":87088},{\"attributes\":{\"id\":\"b79\"},\"end\":87245,\"start\":87188},{\"attributes\":{\"id\":\"b80\"},\"end\":87328,\"start\":87247}]", "bib_title": "[{\"end\":57027,\"start\":56968},{\"end\":57767,\"start\":57697},{\"end\":59305,\"start\":59251},{\"end\":60264,\"start\":60198},{\"end\":61543,\"start\":61494},{\"end\":61958,\"start\":61901},{\"end\":62414,\"start\":62340},{\"end\":62859,\"start\":62816},{\"end\":63167,\"start\":63111},{\"end\":63887,\"start\":63818},{\"end\":64607,\"start\":64537},{\"end\":65252,\"start\":65168},{\"end\":65844,\"start\":65738},{\"end\":66380,\"start\":66292},{\"end\":66989,\"start\":66875},{\"end\":67512,\"start\":67440},{\"end\":67931,\"start\":67879},{\"end\":68649,\"start\":68553},{\"end\":69236,\"start\":69147},{\"end\":69865,\"start\":69807},{\"end\":70206,\"start\":70158},{\"end\":70434,\"start\":70354},{\"end\":70967,\"start\":70901},{\"end\":71837,\"start\":71776},{\"end\":72729,\"start\":72658},{\"end\":73537,\"start\":73447},{\"end\":74345,\"start\":74323},{\"end\":76348,\"start\":76276},{\"end\":76676,\"start\":76602},{\"end\":77556,\"start\":77486},{\"end\":79378,\"start\":79323},{\"end\":80526,\"start\":80476},{\"end\":81410,\"start\":81357},{\"end\":81854,\"start\":81780},{\"end\":82753,\"start\":82703},{\"end\":83309,\"start\":83232},{\"end\":83711,\"start\":83634},{\"end\":84307,\"start\":84211},{\"end\":85799,\"start\":85730},{\"end\":86198,\"start\":86137}]", "bib_author": "[{\"end\":57040,\"start\":57029},{\"end\":57055,\"start\":57040},{\"end\":57063,\"start\":57055},{\"end\":57311,\"start\":57304},{\"end\":57328,\"start\":57311},{\"end\":57349,\"start\":57328},{\"end\":57361,\"start\":57349},{\"end\":57371,\"start\":57361},{\"end\":57393,\"start\":57371},{\"end\":57406,\"start\":57393},{\"end\":57424,\"start\":57406},{\"end\":57432,\"start\":57424},{\"end\":57785,\"start\":57769},{\"end\":57807,\"start\":57785},{\"end\":57823,\"start\":57807},{\"end\":57840,\"start\":57823},{\"end\":57862,\"start\":57840},{\"end\":58187,\"start\":58179},{\"end\":58197,\"start\":58187},{\"end\":58210,\"start\":58197},{\"end\":58221,\"start\":58210},{\"end\":58236,\"start\":58221},{\"end\":58245,\"start\":58236},{\"end\":58259,\"start\":58245},{\"end\":58275,\"start\":58259},{\"end\":58289,\"start\":58275},{\"end\":58311,\"start\":58289},{\"end\":58323,\"start\":58311},{\"end\":58685,\"start\":58655},{\"end\":58702,\"start\":58685},{\"end\":58712,\"start\":58702},{\"end\":59047,\"start\":59037},{\"end\":59060,\"start\":59047},{\"end\":59073,\"start\":59060},{\"end\":59083,\"start\":59073},{\"end\":59095,\"start\":59083},{\"end\":59101,\"start\":59095},{\"end\":59321,\"start\":59307},{\"end\":59339,\"start\":59321},{\"end\":59739,\"start\":59727},{\"end\":59938,\"start\":59927},{\"end\":60563,\"start\":60549},{\"end\":60953,\"start\":60938},{\"end\":60965,\"start\":60953},{\"end\":60978,\"start\":60965},{\"end\":61273,\"start\":61266},{\"end\":61289,\"start\":61273},{\"end\":61305,\"start\":61289},{\"end\":61314,\"start\":61305},{\"end\":61555,\"start\":61545},{\"end\":61566,\"start\":61555},{\"end\":61577,\"start\":61566},{\"end\":61975,\"start\":61960},{\"end\":61991,\"start\":61975},{\"end\":62010,\"start\":61991},{\"end\":62024,\"start\":62010},{\"end\":62430,\"start\":62416},{\"end\":62443,\"start\":62430},{\"end\":62877,\"start\":62861},{\"end\":62889,\"start\":62877},{\"end\":62900,\"start\":62889},{\"end\":62913,\"start\":62900},{\"end\":62926,\"start\":62913},{\"end\":62934,\"start\":62926},{\"end\":63182,\"start\":63169},{\"end\":63192,\"start\":63182},{\"end\":63198,\"start\":63192},{\"end\":63516,\"start\":63505},{\"end\":63534,\"start\":63516},{\"end\":63545,\"start\":63534},{\"end\":63904,\"start\":63889},{\"end\":63917,\"start\":63904},{\"end\":63930,\"start\":63917},{\"end\":63943,\"start\":63930},{\"end\":63956,\"start\":63943},{\"end\":64624,\"start\":64609},{\"end\":64637,\"start\":64624},{\"end\":64650,\"start\":64637},{\"end\":64663,\"start\":64650},{\"end\":64676,\"start\":64663},{\"end\":65269,\"start\":65254},{\"end\":65282,\"start\":65269},{\"end\":65293,\"start\":65282},{\"end\":65859,\"start\":65846},{\"end\":65874,\"start\":65859},{\"end\":65887,\"start\":65874},{\"end\":65900,\"start\":65887},{\"end\":66395,\"start\":66382},{\"end\":66410,\"start\":66395},{\"end\":66423,\"start\":66410},{\"end\":66436,\"start\":66423},{\"end\":67008,\"start\":66991},{\"end\":67022,\"start\":67008},{\"end\":67035,\"start\":67022},{\"end\":67531,\"start\":67514},{\"end\":67545,\"start\":67531},{\"end\":67558,\"start\":67545},{\"end\":67950,\"start\":67933},{\"end\":67965,\"start\":67950},{\"end\":67978,\"start\":67965},{\"end\":67989,\"start\":67978},{\"end\":67998,\"start\":67989},{\"end\":68338,\"start\":68323},{\"end\":68351,\"start\":68338},{\"end\":68364,\"start\":68351},{\"end\":68662,\"start\":68651},{\"end\":68677,\"start\":68662},{\"end\":68690,\"start\":68677},{\"end\":69249,\"start\":69238},{\"end\":69264,\"start\":69249},{\"end\":69277,\"start\":69264},{\"end\":69880,\"start\":69867},{\"end\":69890,\"start\":69880},{\"end\":69903,\"start\":69890},{\"end\":70221,\"start\":70208},{\"end\":70235,\"start\":70221},{\"end\":70450,\"start\":70436},{\"end\":70458,\"start\":70450},{\"end\":70468,\"start\":70458},{\"end\":70479,\"start\":70468},{\"end\":70496,\"start\":70479},{\"end\":70980,\"start\":70969},{\"end\":70990,\"start\":70980},{\"end\":70998,\"start\":70990},{\"end\":71011,\"start\":70998},{\"end\":71022,\"start\":71011},{\"end\":71425,\"start\":71408},{\"end\":71440,\"start\":71425},{\"end\":71452,\"start\":71440},{\"end\":71462,\"start\":71452},{\"end\":71474,\"start\":71462},{\"end\":71488,\"start\":71474},{\"end\":71512,\"start\":71488},{\"end\":71856,\"start\":71839},{\"end\":71874,\"start\":71856},{\"end\":71891,\"start\":71874},{\"end\":71908,\"start\":71891},{\"end\":71922,\"start\":71908},{\"end\":71940,\"start\":71922},{\"end\":72401,\"start\":72387},{\"end\":72415,\"start\":72401},{\"end\":72429,\"start\":72415},{\"end\":72443,\"start\":72429},{\"end\":72458,\"start\":72443},{\"end\":72745,\"start\":72731},{\"end\":72758,\"start\":72745},{\"end\":72782,\"start\":72758},{\"end\":72797,\"start\":72782},{\"end\":73214,\"start\":73206},{\"end\":73226,\"start\":73214},{\"end\":73231,\"start\":73226},{\"end\":73549,\"start\":73539},{\"end\":73568,\"start\":73549},{\"end\":73590,\"start\":73568},{\"end\":73606,\"start\":73590},{\"end\":73623,\"start\":73606},{\"end\":73639,\"start\":73623},{\"end\":73654,\"start\":73639},{\"end\":73669,\"start\":73654},{\"end\":73684,\"start\":73669},{\"end\":73698,\"start\":73684},{\"end\":74126,\"start\":74109},{\"end\":74144,\"start\":74126},{\"end\":74361,\"start\":74347},{\"end\":74376,\"start\":74361},{\"end\":74391,\"start\":74376},{\"end\":74405,\"start\":74391},{\"end\":74414,\"start\":74405},{\"end\":74423,\"start\":74414},{\"end\":74441,\"start\":74423},{\"end\":74447,\"start\":74441},{\"end\":74463,\"start\":74447},{\"end\":74474,\"start\":74463},{\"end\":75060,\"start\":75052},{\"end\":75072,\"start\":75060},{\"end\":75077,\"start\":75072},{\"end\":75389,\"start\":75374},{\"end\":75401,\"start\":75389},{\"end\":75414,\"start\":75401},{\"end\":75759,\"start\":75742},{\"end\":75771,\"start\":75759},{\"end\":75783,\"start\":75771},{\"end\":76182,\"start\":76169},{\"end\":76364,\"start\":76350},{\"end\":76373,\"start\":76364},{\"end\":76689,\"start\":76678},{\"end\":76702,\"start\":76689},{\"end\":76716,\"start\":76702},{\"end\":76729,\"start\":76716},{\"end\":76738,\"start\":76729},{\"end\":76750,\"start\":76738},{\"end\":76761,\"start\":76750},{\"end\":76775,\"start\":76761},{\"end\":77295,\"start\":77279},{\"end\":77313,\"start\":77295},{\"end\":77572,\"start\":77558},{\"end\":77589,\"start\":77572},{\"end\":77606,\"start\":77589},{\"end\":77987,\"start\":77970},{\"end\":78005,\"start\":77987},{\"end\":78196,\"start\":78184},{\"end\":78212,\"start\":78196},{\"end\":78226,\"start\":78212},{\"end\":78250,\"start\":78226},{\"end\":78266,\"start\":78250},{\"end\":78698,\"start\":78686},{\"end\":78711,\"start\":78698},{\"end\":78721,\"start\":78711},{\"end\":78732,\"start\":78721},{\"end\":78742,\"start\":78732},{\"end\":79041,\"start\":79026},{\"end\":79063,\"start\":79041},{\"end\":79078,\"start\":79063},{\"end\":79395,\"start\":79380},{\"end\":79413,\"start\":79395},{\"end\":79431,\"start\":79413},{\"end\":79450,\"start\":79431},{\"end\":79848,\"start\":79834},{\"end\":79862,\"start\":79848},{\"end\":79877,\"start\":79862},{\"end\":80289,\"start\":80267},{\"end\":80305,\"start\":80289},{\"end\":80316,\"start\":80305},{\"end\":80546,\"start\":80528},{\"end\":80558,\"start\":80546},{\"end\":80571,\"start\":80558},{\"end\":80584,\"start\":80571},{\"end\":80936,\"start\":80923},{\"end\":80955,\"start\":80936},{\"end\":80972,\"start\":80955},{\"end\":80982,\"start\":80972},{\"end\":80997,\"start\":80982},{\"end\":81014,\"start\":80997},{\"end\":81025,\"start\":81014},{\"end\":81039,\"start\":81025},{\"end\":81050,\"start\":81039},{\"end\":81058,\"start\":81050},{\"end\":81426,\"start\":81412},{\"end\":81439,\"start\":81426},{\"end\":81452,\"start\":81439},{\"end\":81877,\"start\":81856},{\"end\":81891,\"start\":81877},{\"end\":81900,\"start\":81891},{\"end\":81916,\"start\":81900},{\"end\":81925,\"start\":81916},{\"end\":82389,\"start\":82381},{\"end\":82579,\"start\":82574},{\"end\":82767,\"start\":82755},{\"end\":82780,\"start\":82767},{\"end\":82792,\"start\":82780},{\"end\":82802,\"start\":82792},{\"end\":82813,\"start\":82802},{\"end\":82823,\"start\":82813},{\"end\":82836,\"start\":82823},{\"end\":83324,\"start\":83311},{\"end\":83336,\"start\":83324},{\"end\":83346,\"start\":83336},{\"end\":83359,\"start\":83346},{\"end\":83370,\"start\":83359},{\"end\":83735,\"start\":83713},{\"end\":83760,\"start\":83735},{\"end\":83778,\"start\":83760},{\"end\":83791,\"start\":83778},{\"end\":84323,\"start\":84309},{\"end\":84334,\"start\":84323},{\"end\":84349,\"start\":84334},{\"end\":84362,\"start\":84349},{\"end\":85101,\"start\":85082},{\"end\":85117,\"start\":85101},{\"end\":85131,\"start\":85117},{\"end\":85144,\"start\":85131},{\"end\":85161,\"start\":85144},{\"end\":85187,\"start\":85161},{\"end\":85216,\"start\":85187},{\"end\":85536,\"start\":85522},{\"end\":85549,\"start\":85536},{\"end\":85562,\"start\":85549},{\"end\":85818,\"start\":85801},{\"end\":85833,\"start\":85818},{\"end\":85850,\"start\":85833},{\"end\":86218,\"start\":86200},{\"end\":86847,\"start\":86832},{\"end\":86933,\"start\":86923},{\"end\":87022,\"start\":87010},{\"end\":87107,\"start\":87090},{\"end\":87196,\"start\":87190},{\"end\":87262,\"start\":87249}]", "bib_venue": "[{\"end\":64161,\"start\":64070},{\"end\":64709,\"start\":64701},{\"end\":64973,\"start\":64956},{\"end\":65414,\"start\":65365},{\"end\":66528,\"start\":66524},{\"end\":67176,\"start\":67114},{\"end\":68873,\"start\":68790},{\"end\":69452,\"start\":69376},{\"end\":70645,\"start\":70579},{\"end\":71171,\"start\":71105},{\"end\":72069,\"start\":72013},{\"end\":74653,\"start\":74572},{\"end\":76924,\"start\":76858},{\"end\":79599,\"start\":79533},{\"end\":80719,\"start\":80660},{\"end\":81581,\"start\":81525},{\"end\":82044,\"start\":81993},{\"end\":82985,\"start\":82919},{\"end\":83940,\"start\":83874},{\"end\":84613,\"start\":84499},{\"end\":86359,\"start\":86297},{\"end\":57119,\"start\":57063},{\"end\":57880,\"start\":57862},{\"end\":58177,\"start\":58093},{\"end\":58653,\"start\":58599},{\"end\":59035,\"start\":58996},{\"end\":59377,\"start\":59339},{\"end\":59725,\"start\":59660},{\"end\":59925,\"start\":59859},{\"end\":60286,\"start\":60266},{\"end\":60547,\"start\":60470},{\"end\":60936,\"start\":60823},{\"end\":61264,\"start\":61207},{\"end\":61675,\"start\":61577},{\"end\":62101,\"start\":62024},{\"end\":62557,\"start\":62472},{\"end\":62948,\"start\":62934},{\"end\":63287,\"start\":63198},{\"end\":63637,\"start\":63561},{\"end\":64068,\"start\":63979},{\"end\":64699,\"start\":64676},{\"end\":64954,\"start\":64904},{\"end\":65363,\"start\":65316},{\"end\":65922,\"start\":65900},{\"end\":66479,\"start\":66436},{\"end\":67112,\"start\":67035},{\"end\":67634,\"start\":67558},{\"end\":68042,\"start\":67998},{\"end\":68321,\"start\":68249},{\"end\":68788,\"start\":68690},{\"end\":69374,\"start\":69300},{\"end\":69960,\"start\":69903},{\"end\":70249,\"start\":70235},{\"end\":70577,\"start\":70496},{\"end\":71103,\"start\":71022},{\"end\":72011,\"start\":71940},{\"end\":72385,\"start\":72332},{\"end\":72857,\"start\":72797},{\"end\":73204,\"start\":73087},{\"end\":73724,\"start\":73698},{\"end\":74107,\"start\":74038},{\"end\":74570,\"start\":74474},{\"end\":75050,\"start\":74969},{\"end\":75372,\"start\":75259},{\"end\":75740,\"start\":75643},{\"end\":75989,\"start\":75964},{\"end\":76167,\"start\":76130},{\"end\":76417,\"start\":76373},{\"end\":76856,\"start\":76775},{\"end\":77277,\"start\":77211},{\"end\":77672,\"start\":77606},{\"end\":77968,\"start\":77899},{\"end\":78379,\"start\":78282},{\"end\":78684,\"start\":78616},{\"end\":79024,\"start\":78942},{\"end\":79531,\"start\":79450},{\"end\":80022,\"start\":79877},{\"end\":80265,\"start\":80230},{\"end\":80658,\"start\":80584},{\"end\":81109,\"start\":81074},{\"end\":81523,\"start\":81452},{\"end\":81991,\"start\":81925},{\"end\":82379,\"start\":82284},{\"end\":82572,\"start\":82551},{\"end\":82917,\"start\":82836},{\"end\":83408,\"start\":83370},{\"end\":83872,\"start\":83791},{\"end\":84497,\"start\":84385},{\"end\":85080,\"start\":85024},{\"end\":85520,\"start\":85482},{\"end\":85912,\"start\":85850},{\"end\":86295,\"start\":86218},{\"end\":86619,\"start\":86548}]"}}}, "year": 2023, "month": 12, "day": 17}