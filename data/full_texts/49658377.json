{"id": 49658377, "updated": "2023-11-07 20:25:01.064", "metadata": {"title": "Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry", "authors": "[{\"first\":\"Nan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Rui\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jorg\",\"last\":\"Stuckler\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Cremers\",\"middle\":[]}]", "venue": "ECCV", "journal": "835-852", "publication_date": {"year": 2018, "month": 7, "day": 6}, "abstract": "Monocular visual odometry approaches that purely rely on geometric cues are prone to scale drift and require sufficient motion parallax in successive frames for motion estimation and 3D reconstruction. In this paper, we propose to leverage deep monocular depth prediction to overcome limitations of geometry-based monocular visual odometry. To this end, we incorporate deep depth predictions into Direct Sparse Odometry (DSO) as direct virtual stereo measurements. For depth prediction, we design a novel deep network that refines predicted depth from a single image in a two-stage process. We train our network in a semi-supervised way on photoconsistency in stereo images and on consistency with accurate sparse depth reconstructions from Stereo DSO. Our deep predictions excel state-of-the-art approaches for monocular depth on the KITTI benchmark. Moreover, our Deep Virtual Stereo Odometry clearly exceeds previous monocular and deep learning based methods in accuracy. It even achieves comparable performance to the state-of-the-art stereo methods, while only relying on a single camera.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1807.02570", "mag": "2952681062", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/YangWSC18", "doi": "10.1007/978-3-030-01237-3_50"}}, "content": {"source": {"pdf_hash": "a49b661e42aea6f205e543a80106fc9c6ff0f9d4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1807.02570v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1807.02570", "status": "GREEN"}}, "grobid": {"id": "f172733e3aab84e79b81c392cfb9c09abac332b2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a49b661e42aea6f205e543a80106fc9c6ff0f9d4.txt", "contents": "\nDeep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry\n\n\nNan Yang yangn@in.tum.de \nTechnical University of Munich\n\n\nArtisense\n\nRui Wang wangr@in.tum.de \nTechnical University of Munich\n\n\nArtisense\n\nJ\u00f6rg St\u00fcckler stueckle@in.tum.de \nTechnical University of Munich\n\n\nDaniel Cremers cremers@in.tum.de \nTechnical University of Munich\n\n\nArtisense\n\nDeep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry\nMonocular depth estimation \u00b7 Monocular visual odometry \u00b7 Semi-supervised learning\nMonocular visual odometry approaches that purely rely on geometric cues are prone to scale drift and require sufficient motion parallax in successive frames for motion estimation and 3D reconstruction. In this paper, we propose to leverage deep monocular depth prediction to overcome limitations of geometry-based monocular visual odometry. To this end, we incorporate deep depth predictions into Direct Sparse Odometry (DSO) as direct virtual stereo measurements. For depth prediction, we design a novel deep network that refines predicted depth from a single image in a two-stage process. We train our network in a semisupervised way on photoconsistency in stereo images and on consistency with accurate sparse depth reconstructions from Stereo DSO. Our deep predictions excel state-of-the-art approaches for monocular depth on the KITTI benchmark. Moreover, our Deep Virtual Stereo Odometry clearly exceeds previous monocular and deep-learning based methods in accuracy. It even achieves comparable performance to the state-of-the-art stereo methods, while only relying on a single camera.\n\nIntroduction\n\nVisual odometry (VO) is a highly active field of research in computer vision with a plethora of applications in domains such as autonomous driving, robotics, and augmented reality. VO with a single camera using traditional geometric approaches inherently suffers from the fact that camera trajectory and map can only be estimated up to an unknown scale which also leads to scale drift. Moreover, sufficient motion parallax is required to estimate motion and structure from successive frames. To avoid these issues, typically more complex sensors such as active depth cameras or stereo rigs are employed. However, these sensors require larger efforts in calibration and increase the costs of the vision system. Metric depth can also be recovered from a single image if a-priori knowledge about the typical sizes or appearances of objects is used. Deep learning based approaches tackle this by training deep neural networks on large amounts of data.  In this paper, we propose a novel approach to monocular visual odometry, Deep Virtual Stereo Odometry (DVSO), which incorporates deep depth predictions into a geometric monocular odometry pipeline. We use deep stereo disparity for virtual direct image alignment constraints within a framework for windowed direct bundle adjustment (e.g. Direct Sparse Odometry [8]). DVSO achieves comparable performance to the state-of-the-art stereo visual odometry systems on the KITTI odometry benchmark. It can even outperform the state-of-theart geometric VO methods when tuning scale-dependent parameters such as the virtual stereo baseline.\n\nAs an additional contribution, we propose a novel stacked residual network architecture that refines disparity estimates in two stages and is trained in a semisupervised way. In typical supervised learning approaches [6,25,24], depth ground truth needs to be acquired for training with active sensors like RGB-D cameras and 3D laser scanners which are costly to obtain. Requiring a large amount of such labeled data is an additional burden that limits generalization to new environments. Self-supervised [11,14] and unsupervised learning approaches [49], on the other hand, overcome this limitation and do not require additional active sensors. Commonly, they train the networks on photometric consistency, for example in stereo imagery [11,14], which reduces the effort for collecting training data. Still, the current self-supervised approaches are not as accurate as supervised methods [23]. We combine self-supervised and supervised training, but avoid the costly collection of LiDAR data in our approach. Instead, we make use of Stereo Direct Sparse Odometry (Stereo DSO [40]) to provide accurate sparse 3D reconstructions on the training set. Our deep depth prediction network outperforms the current state-of-the-art methods on KITTI.\n\nA video demonstrating our methods as well as the results is available at https://youtu.be/sLZOeC9z_tw.\n\n\nRelated Work\n\nDeep learning for monocular depth estimation. Deep learning based approaches have recently achieved great advances in monocular depth estimation.\n\nEmploying deep neural network avoids the hand-crafted features used in previous methods [36,19]. Supervised deep learning [6,25,24] has recently shown great success for monocular depth estimation. Eigen et al. [6,5] propose a two scale CNN architecture which directly predicts the depth map from a single image. Laina et al. [24] propose a residual network [17] based fully convolutional encoder-decoder architecture [27] with a robust regression loss function. The aforementioned supervised learning approaches need large amounts of groundtruth depth data for training. Self-supervised approaches [11,44,14] overcome this limitation by exploiting photoconsistency and geometric constraints to define loss functions, for example, in a stereo camera setup. This way, only stereo images are needed for training which are typically easier to obtain than accurate depth measurements from active sensors such as 3D lasers or RGB-D cameras. Godard et al. [14] achieve the state-of-the-art depth estimation accuracy for a fully self-supervised approach. The semi-supervised scheme proposed by Kuznietsov et al. [23] combines the self-supervised loss with supervision with sparse LiDAR ground truth. They do not need multi-scale depth supervision or left-right consistency in their loss, and achieve better performance than the self-supervised approach in [14]. The limitation of this semi-supervised approach is the requirement for LiDAR data which are costly to collect. In our approach we use Stereo Direct Sparse Odometry to obtain sparse depth ground-truth for semi-supervised training. Since the extracted depth maps are even sparser than LiDAR data, we also employ multi-scale self-supervised training and left-right consistency as in Godard et al. [14]. Inspired by [20,34], we design a stacked network architecture leveraging the concept of residual learning [17].\n\nDeep learning for VO / SLAM. In recent years, large progress has been achieved in the development of monocular VO and SLAM methods [31,9,8,32]. Due to projective geometry, metric scale cannot be observed with a single camera [37] which introduces scale drift. A popular approach is hence to use stereo cameras for VO [10,8,31] which avoid scale ambiguity and leverage stereo matching with a fixed baseline for estimating 3D structure. While stereo VO delivers more reliable depth estimation, it requires self-calibration for long-term operation [4,46]. The integration of a second camera also introduces additional costs. Some recent monocular VO approaches have integrated monocular depth estimation [46,39] to recover the metric scale by scale-matching. CNN-SLAM [39] extends LSD-SLAM [9] by predicting depth with a CNN and refining the depth maps using Bayesian filtering [9,7]. Their method shows superior performance over monocular SLAM [9,30,45,35] on indoor datasets [15,38]. Yin et al. [46] propose to use convolutional neural fields and consecutive frames to improve the monocular depth estimation from a CNN. Camera motion is estimated using the refined depth. CodeSLAM [2] focuses on the challenge of dense 3D reconstruction. It jointly optimizes a learned compact representation of the dense geometry with camera poses. Our work tackles the problem of odometry with monocular cameras and integrates deep depth prediction with multi-view stereo to improve camera pose estimation. Another line of research trains networks to directly predict the ego-motion end-to-end using supervised [41] or unsuper- vised learning [49,26]. However, the estimated ego-motion of these methods is still by far inferior to geometric visual odometry approaches. In our approach, we phrase visual odometry as a geometric optimization problem but incorporate photoconsistency constraints with state-of-the-art deep monocular depth predictions into the optimization. This way, we obtain a highly accurate monocular visual odometry that is not prone to scale drift and achieves comparable results to traditional stereo VO methods.\n\n\nSemi-Supervised Deep Monocular Depth Estimation\n\nIn this section, we will introduce our semi-supervised approach to deep monocular depth estimation. It builds on three key ingredients: self-supervised learning from photoconsistency in a stereo setup similar to [14], supervised learning based on accurate sparse depth reconstruction by Stereo DSO, and two-stage refinement of the network predictions in a stacked encoder-decoder architecture.\n\n\nNetwork Architecture\n\nWe coin our architecture StackNet since it stacks two sub-networks, SimpleNet and ResidualNet, as depicted in Figure 2. Both sub-networks are fully convolutional deep neural network adopted from DispNet [28] with an encoder-decoder scheme. ResidualNet has fewer layers and takes the outputs of SimpleNet as inputs. Its purpose is to refine the disparity maps predicted by SimpleNet by learning an additive residual signal. Similar residual learning architectures have been successfully applied to related deep learning tasks [20,34]. The detailed network architecture is illustrated in the supplementary material.\n\nSimpleNet. SimpleNet is an encoder-decoder architecture with a ResNet-50 based encoder and skip connections between corresponding encoder and decoder layers. The decoder upprojects the feature maps to the original resolution and generates 4 pairs of disparity maps disp left simple,s and disp right simple,s in different resolutions s \u2208 [0, 3]. The upprojection is implemented by resize-convolution [33], i.e. a nearest-neighbor upsampling layer by a factor of two followed by a convolutional layer. The usage of skip connections enables the decoder to recover high-resolution results with fine-grained details.\n\nResidualNet. The purpose of ResidualNet is to further refine the disparity maps predicted by SimpleNet. ResidualNet learns the residual signals disp res,s to the disparity maps disp simple,s (both left and right and for all resolutions). Inspired by FlowNet 2.0 [20], the inputs to ResidualNet contain various information on the prediction and the errors made by SimpleNet: we input I left , For the warping, rectified stereo images are required while stereo camera intrinsics and extrinsics are not needed as our network directly outputs disparity.\n\nThe final refined outputs disp s are disp s = disp simple,s \u2295 disp res,s , s \u2208 [0, 3], where \u2295 is element-wise summation. The encoder of ResidualNet contains 12 residual blocks in total and predicts 4 scales of residual disparity maps as Sim-pleNet. Adding more layers does not further improve performance in our experiments. Notably, only the left image is used as an input to either SimpleNet and ResidualNet, while the right image is not required. However, the network outputs a refined disparity map for the left and right stereo image. Both facts will be important for our monocular visual odometry approach.\n\n\nLoss Function\n\nWe define a loss L s at each output scale s, resulting in the total loss L = 3 s=0 L s . The loss at each scale L s is a linear combination of five terms which are symmetric in left and right images,\nL s = \u03b1 U L left U + L right U + \u03b1 S L left S + L right S + \u03b1 lr L left lr + L right lr + \u03b1 smooth L left smooth + L right smooth + \u03b1 occ L left occ + L right occ ,(1)\nwhere L U is a self-supervised loss, L S is a supervised loss, L lr is a left-right consistency loss, L smooth is a smoothness term encouraging the predicted disparities to be locally smooth and L occ is an occlusion regularization term. In the following, we detail the left components L left of the loss function at each scale. The right components L right are defined symmetrically.\n\nSelf-supervised loss. The self-supervised loss measures the quality of the reconstructed images. The reconstructed image is generated by warping the input image into the view of the other rectified stereo image. This procedure is fully (sub-)differentiable for bilinear sampling [21]. Inspired by [14,47], the quality of the reconstructed image is measured with the combination of the 1 loss and single scale structural similarity (SSIM) [42]:\nL left U = 1 N x,y \u03b1 1 \u2212 SSIM I left (x, y), I left recons (x, y) 2 + (1 \u2212 \u03b1) I left (x, y) \u2212 I left recons (x, y) 1 ,(2)\nwith a 3 \u00d7 3 box filter for SSIM and \u03b1 set to 0.84. Supervised loss. The supervised loss measures the deviation of the predicted disparity map from the disparities estimated by Stereo DSO at a sparse set of pixels:\nL left s = 1 N (x,y)\u2208\u2126 DSO,left \u03b2 disp left (x, y) \u2212 disp left DSO (x, y)(3)\nwhere \u2126 DSO,left is the set of pixels with disparities estimated by DSO and \u03b2 (x) is the reverse Huber (berHu) norm introduced in [24] which lets the training focus more on larger residuals. The threshold is adaptively set as a batch-dependent\nvalue = 0.2 max (x,y)\u2208\u2126 DSO,left disp left (x, y) \u2212 disp left DSO (x, y)\n. Left-right disparity consistency loss. Given only the left image as input, the network predicts the disparity map of the left as well as the right image as in [14]. As proposed in [14,47], consistency between the left and right disparity image is improved by\nL left lr = 1 N x,y disp left (x, y) \u2212 disp right (x \u2212 disp left (x, y), y) .(4)\nDisparity smoothness regularization. Depth reconstruction based on stereo image matching is an ill-posed problem on its own: the depth of homogeneously textured areas and occluded areas cannot be determined. For these areas, we apply the regularization term\nL left smooth = 1 N x,y \u2207 2 x disp left (x, y) e \u2212 \u2207 2 x I left (x,y) + \u2207 2 y disp left (x, y) e \u2212 \u2207 2 y I left (x,y)(5)\nthat assumes that the predicted disparity map should be locally smooth. We use a second-order smoothness prior [43] and downweight it when the image gradient is high [18].\n\nOcclusion regularization. L left smooth itself tends to generate a shadow area where values gradually change from foreground to background due to stereo occlusion. To favor background depths and hard transitions at occlusions [48], we impose L left occ which penalizes the total sum of absolute disparities. The combination of smoothness-and occlusion regularizer prefers to directly take the (smaller) closeby background disparity which better corresponds to the assumption that the background part is uncovered from the disparities. Beyond this rather straight-forward approach, we also incorporate virtual direct image alignment constraints into the windowed direct bundle adjustment of DSO. We obtain these constraints by warping images with the estimated depth by bundle adjustment and the predicted right disparities by our network assuming a virtual stereo setup. As shown in Figure 3, DVSO integrates both the predicted left disparities and right disparities for the left image. The right image of the stereo setup is not used for our VO method at any stage, making it a monocular VO method.\nL left occ = 1 N x,y disp left (x, y) .(6\nIn the following, we use D L and D R as shorthands to represent the predicted left (disp lef t 0 ) and right disparity map (disp right 0 ) at scale s = 0, respectively. When using purely geometric cues, scale drift is one of the main sources of error of monocular VO due to scale unobservability [37]. In DVSO we use the left disparity map D L predicted by StackNet for initialization instead of randomly initializing the depth like in monocular DSO [8]. The disparity value of an image point with coordinate p is converted to the inverse depth d p using the rectified camera intrinsics and stereo baseline of the training set of StackNet [16],\nd p = D L (p)\nfxb . In this way, the initialization of DVSO becomes more stable than monocular DSO and the depths are initialized with a consistent metric scale.\n\nThe point selection strategy of DVSO is similar to monocular DSO [8], while we also introduce a left-right consistency check (similar to Equation (4)) to filter out the pixels which likely lie in the occluded area\ne lr = D L (p) \u2212 D R (p ) with p = p \u2212 D L (p) 0 .(7)\nThe pixels with e lr > 1 are not selected. Every new frame is firstly tracked with respect to the reference keyframe using direct image alignment in a coarse-to-fine manner [8]. Afterwards DVSO decides if a new keyframe has to be created for the new frame following the criteria proposed by [8]. When a new keyframe is created, the temporal multi-view energy function E photo := i\u2208F p\u2208Pi j\u2208obs(p) E p ij needs to be optimized, where F is a fixed-sized window containing the active keyframes, P i is the set of points selected from its host keyframe with index i and j \u2208 obs(p) is the index of the keyframe which observes p. E p ij is the photometric error of the point p when projected from the host keyframe I i onto the other keyframe I j :\nE p ij := \u03c9 p (I j [p] \u2212 b j ) \u2212 e aj e ai (I i [p] \u2212 b i ) \u03b3 ,(8)\nwherep is the projected image coordinate using the relative rotation matrix R \u2208 SO(3) and translation vector t \u2208 R 3 [16],p = \u03a0 c R\u03a0 \u22121 c (p, d p ) + t , where \u03a0 c and \u03a0 \u22121 c are the camera projection and back-projection functions. The parameters a i , a j , b i and b j are used for modeling the affine brightness transformation [8]. The weight \u03c9 p penalizes the points with high image gradient [8] with the intuition that the error originating from bilinear interpolation of the discrete image values is larger. \u00b7 \u03b3 is the Huber norm with the threshold \u03b3. For the detailed explanation of the energy function, please refer to [8].\n\nTo further improve the accuracy of DVSO, inspired by Stereo DSO [40] which couples the static stereo term with the temporal multi-view energy function, we introduce a novel virtual stereo term E \u2020p for each point p\nE \u2020p i = \u03c9 p I \u2020 i p \u2020 \u2212 I i [p] \u03b3 with I \u2020 i p \u2020 = I i p \u2020 \u2212 D R p \u2020 0 ,(9)\nwhere\np \u2020 = \u03a0 c (\u03a0 \u22121 c (p, d p ) + t b )\nis the virtual projected coordinate of p using the vector t b denoting the virtual stereo baseline which is known during the training of StackNet. The intuition behind this term is to optimize the estimated depth of the visual odometry to become consistent with the disparity prediction of StackNet. Instead of imposing the consistency directly on the estimated and predicted disparities, we formulate the residuals in photoconsistency which better reflects the uncertainties of the prediction of StackNet and also keeps the unit of the residuals consistent with the temporal direct image alignment terms.\n\nWe then optimize the total energy\nE photo := i\u2208F p\u2208Pi \uf8eb \uf8ed \u03bbE \u2020p i + j\u2208obs(p) E p ij \uf8f6 \uf8f8 ,(10)\nwhere the coupling factor \u03bb balances the temporal and the virtual stereo term. All the parameters of the total energy are jointly optimized using the Gauss Newton method [8]. In order to keep a fixed size of the active window (N = 7 keyframes in our experiments), old keyframes are removed from the system by marginalization using the Schur complement [8]. Unlike sliding window bundle adjustment, the parameter estimates outside the optimization window including camera poses and depths in a marginalization prior are also incorporated into the optimization. In contrast to the MSCKF [29], the depths of pixels are explicitly maintained in the state and optimized for. In our optimization framework we trade off predicted depth and triangulated depth using robust norms.\n\n\nExperiments\n\nWe quantitatively evaluate our StackNet with other state-of-the-art monocular depth prediction methods on the publicly available KITTI dataset [12]. In the Input GT Ours Kuznietsov et al. [23] Godard et al. [14] Garg et al. [11] Eigen et al. [6] Fig. 4: Qualitative comparison with state-of-the-art methods. The ground truth is interpolated for better visualization. Our approach shows better prediction on thin structures than the self-supervised approach [14], and delivers more detailed disparity maps than the semi-supervised approach using LiDAR data [23].\n\nsupplementary materials, we demonstrate results on the Cityscapes dataset [3] and the Make3D dataset [36] to show the generalization ability. For DVSO, we evaluate its tracking accuracy on the KITTI odometry benchmark with other state-of-the-art monocular as well as stereo visual odometry systems. In the supplementary material, we also demonstrate its results on the Frankfurt sequence of the Cityscapes dataset to show the generalization of DVSO.\n\n\nMonocular Depth Estimation\n\nDataset. We train StackNet using the train/test split (K) of Eigen et al. [6]. The training set contains 23488 images from 28 scenes belonging to the categories \"city\", \"residential\" and \"road\". We used 22600 images of them for training and the remaining ones for validation. We further split K into 2 subsets K o and K r . K o contains the images of the sequences which appear in the training set (but not the test set) of the KITTI odometry benchmark on which we use Stereo DSO [40] to extract sparse ground-truth depth data. K r contains the remaining images in K. Specifically, K o contains the images of sequences 01, 02, 06, 08, 09 and 10 of the KITTI odometry benchmark. Implementation details. StackNet is implemented in TensorFlow [1] and trained from scratch on a single Titan X Pascal GPU. We resize the images to 512 \u00d7 256 for training and it takes less than 40ms for inference including the I/O overhead. The weights are set to \u03b1 u = 1, \u03b1 s = 10, \u03b1 lr = 1, \u03b1 smooth = 0.1/2 s and \u03b1 occ = 0.01, where s is the output scale. As suggested by [14], we use exponential linear units (ELUs) for SimpleNet, while we use leaky rectified linear units (Leaky ReLUs) for ResidualNet. We first train SimpleNet on K o in the semi-supervised way for 80 epochs with a batch size of 8 using the Adam optimizer [22]. The learning rate is initially set to \u03bb = 10 \u22124 for the first 50 epochs and halved every 15 epochs afterwards until the end. Then we train SimpleNet with \u03bb = 5 \u00d7 10 \u22125 on K r for 40 epochs in the self-supervised way without L S . In the end, we train again on K o without L U using \u03bb = 10 \u22125 for 5 epochs. We explain the dataset schedule as well as the parameter tuning in detail in the supplementary material.\n\nAfter training SimpleNet, we freeze its weights and train StackNet by cascading ResidualNet. StackNet is trained with \u03bb = 5 \u00d7 10 \u22125 in the same dataset schedules but with less epochs, i.e. 30, 15, 3 epochs, respectively. We apply random gamma, brightness and color augmentations [14]. We also employ the post-processing for left disparities proposed by Godard et al. [14] to reduce the effect of stereo disocclusions. In the supplementary material we also provide an ablation study on the various loss terms.\n\nKITTI. Table 1 shows the evaluation results with the error metrics used in [6]. We crop the images as applied by Eigen et al. [6] to compare with [14], [23] within different depth ranges. The best performance of our network is achieved with the dataset schedule K o \u2192 K r \u2192 K o as we described above. We outperform the state-of-the-art self-supervised approach proposed by Godard et al. [14] by a large margin. Our method also outperforms the state-of-the-art semi-supervised method using the LiDAR ground truth proposed by Kuznietsov et al. [23] on all the metrics except for the less restrictive \u03b4 < 1.25 2 and \u03b4 < 1.25 3 . Figure 4 shows a qualitative comparison with other state-of-the-art methods. Compared to the semi-supervised approach, our results contain more details and deliver comparable prediction on thin structures like the poles. Although the results of Godard et al. [14] appear more detailed on some parts, they are not actually accurate, which can be inferred by the quantitative evaluation. In general, the predictions of Godard et al. [14] on thin objects are not as accurate as our method. In the supplementary material, we show the error maps for the predicted depth maps. Figure 5 further show the advantages of our method compared to the state-of-the-art self-supervised and semi-supervised approaches. The results of Godard et al. [14] are predicted by the network trained with both the Cityscapes dataset and the KITTI dataset. On the wall of the far building in the left figure, our network can better predict consistent depth on the surface, while the prediction of the self-supervised network shows strong checkerboard artifact, which is apparently inaccurate. The semi-supervised approach also shows checkerboard artifact (but much slighter). The right side of the figure shows shadow artifacts for the approach of Godard et al. [14] around the boundaries of the traffic sign, while the result of Kuznietsov et al. [23] fails to predict the structure. Please refer to our supplementary material for further results. We also demonstrate how our trained depth prediction network generalizes to other datasets in the supplementary material.\n\n\nMonocular Visual Odometry\n\nKITTI odometry benchmark. The KITTI odometry benchmark contains 11 (0-10) training sequences and 11 (11)(12)(13)(14)(15)(16)(17)(18)(19)(20)(21) test sequences. Ground-truth 6D poses are provided for the training sequences, whereas for the test sequences evaluation results are obtained by submitting to the KITTI website. We use the error metrics proposed in [12].\n\nWe firstly provide an ablation study for DVSO to show the effectiveness of the design choices in our approach. In Table 2 we give results for DVSO in different variants with the following components: initializing the depth with the left disparity prediction (in), using the right disparity for the virtual stereo term in windowed bundle adjustment (vs), checking left-right disparity consistency RMSE RMSE (log) ARD SRD \u03b4 < 1.25 \u03b4 < 1.25 2 \u03b4 < 1. 25 [14] shows a strong shadow effect around object contours, while our result does not. The result of Kuznietsov [23] shows failure on predicting the traffic sign. Both other methods [14,23] predict checkerboard artifacts on the far building, while our approach predicts such artifacts less.\n\nfor point selection (lr), and tuning the virtual stereo baseline tb. The intuition behind the virtual stereo baseline is that StackNet is trained over various camera parameters and hence provides a depth scale for an average baseline. For tb, we therefore tune the scale factors of different sequences with different cameras intrinsics, to better align the estimated scale with the ground truth. Baselines are tuned for each of the 3 different camera parameter sets in the training set individually using grid search on one training sequence. Specifically, we tuned the baselines on sequences 00, 03 and 05 which correspond to 3 different camera parameter sets. The test set contains the same camera parameter sets as the training set and we map the virtual baselines for tb correspondingly. Monocular DSO (after Sim(3) alignment) is also shown as the baseline. The results show that our full approach achieves the best average performance. Our StackNet also adds significantly to the performance of DVSO compared with using depth predictions from [14].\n\nWe also compare DVSO with other state-of-the-art stereo visual odometry systems on the sequences 00-10. The sequences with marker * are used for training StackNet and the sequences with marker \u2020 are not used for training the network. In Table 3  t rel r rel t rel r rel t rel r rel t rel r rel t rel r rel t rel r rel t rel r rel 00 \u2020 188 0.   with baseline tuning (in, vs, lr, tb). The average RMSE of DVSO without baseline tuning is better than Stereo LSD-VO, but not as good as Stereo DSO [40] or ORB-SLAM2 [31] (stereo, without global optimization and loop closure). Importantly, DVSO uses only monocular images. With the baseline tuning, DVSO achieves even better average performance than all other stereo systems on both rotational and translational errors. Figure 6 shows the estimated trajectory on sequence 00. Both monocular ORB-SLAM2 and DSO suffer from strong scale drift, while DVSO achieves superior performance on eliminating the scale drift. We also show the estimated trajectory on 00 by running DVSO using the depth map predicted by Godard et al. [14] with the model trained on the Cityscapes and the KITTI dataset. For the results in Figure 6 our depth predictions are more accurate. Figure 7 shows the evaluation result of the sequences 11-21 by submitting results of DVSO with and without baseline tuning to the KITTI odometry benchmark. Note that in Figure 7, Stereo LSD-SLAM and ORB-SLAM2 are both full stereo SLAM approaches with global optimization and loop closure. For qualitative comparisons of further estimated trajectories, please refer to our supplementary material.\n\nWe also compare DVSO with DeepVO [41], UnDeepVO [26] and SfMLearner [49] which are deep learning based visual odometry systems trained end-to-end on KITTI. As shown in Table 4, on all available sequences, DVSO achieves better performance than the other two end-to-end approaches. Table 4 also shows the comparison with the deep learning based scale recovery methods for monocular VO proposed by Yin et al. [46]. DVSO also outperforms their method. In the supplementary material, we also show the estimated trajectory on the Cityscapes Frankfurt sequence to demonstrate generalization capabilities.\n\nSt. LSD-VO [10] ORB-SLAM2 [31] St. DSO [40] in, vs, lr DVSO Seq.\n\nt rel r rel t rel r rel t rel r rel t rel r rel t rel r rel 00 \u2020  \n\n\nConclusion\n\nWe presented a novel monocular visual odometry system, DVSO, which recovers metric scale and reduces scale drift in geometric monocular VO. A deep learning approach predicts monocular depth maps for the input images which are used to initialize sparse depths in DSO to a consistent metric scale. Odometry is further improved by a novel virtual stereo term that couples estimated depth in windowed bundle adjustment with the monocular depth predictions. For monocular depth prediction we have presented a semi-supervised deep learning approach, which utilizes a self-supervised image reconstruction loss and sparse depth predictions from Stereo DSO as ground truth depths for supervision. A stacked network architecture predicts state-of-the-art refined disparity estimates. Our evaluation conducted on the KITTI odometry benchmark demonstrates that DVSO outperforms the state-of-the-art monocular methods by a large margin and achieves comparable results to stereo VO methods. With virtual base-   [41] is trained on sequences 00, 02, 08 and 09 of the KITTI Odometry Benchmark. UnDeepVO [26] and SfMLearner [49] are trained unsupervised on seqs 00-08 end-to-end. Results of DeepVO and UnDeepVO taken from [41] and [26] while for SfMLearner we ran their pre-trained model. Our DVSO clearly outperforms state-of-the-art deep learning based VO methods.\n\nline tuning, DVSO can even outperform state-of-the-art stereo VO methods, i.e., Stereo LSD-VO, ORB-SLAM2 without global optimization and loop closure, and Stereo DSO, while using only monocular images.\n\nThe key practical benefit of the proposed method is that it allows us to recover accurate and scale-consistent odometry with only a single camera. Future work could comprise fine-tuning of the network inside the odometry pipeline endto-end. This could enable the system to adapt online to new scenes and camera setups. Given that the deep net was trained on driving sequences, in future work we also plan to investigate how much the proposed approach can generalize to other camera trajectories and environments.\n\nFig. 1 :\n1DVSO achieves monocular visual odometry on KITTI on par with state-of-the-art stereo methods. It uses deep-learning based left-right disparity predictions (lower left) for initialization and virtual stereo constraints in an optimization-based direct visual odometry pipeline. This allows for recovering accurate metric estimates.\n\nFig. 2 :\n2Overview of StackNet architecture.\n\n\ndisp left simple,0 , I right recons , I left recons and e l , where -I right recons is the reconstructed right image by warping I left using disp right simple,0 . -I left recons is the generated left image by back-warping I right recons using disp left simple,0 . e l is the 1 reconstruction error between I left and I left recons\n\n) 3\n)Deep Virtual Stereo OdometryDeep Virtual Stereo Odometry (DVSO) builds on the windowed sparse direct bundle adjustment formulation of monocular DSO. We use our disparity predictions for DSO in two key ways: Firstly, we initialize depth maps of new keyframesFig. 3: System overview of DVSO. Every new frame is used for visual odometry and fed into the proposed StackNet to predict left and right disparity. The predicted left and right disparities are used for depth initialization, while the right disparity is used to form the virtual stereo term in direct sparse bundle adjustment.Joint Optimization \nNew KF? \n\nNo \nNo \nYes \nInitialized? \nTracking \nMake KF \n\nYes \nMarginalization \n\nRefine KFs \n\nInitialization \n\n\n\n\nFig. 5: Qualitative results on Eigen et al.'s KITTI Raw test split. The result of Godard et al.3 \nApproach \nDataset \nlower is better \nhigher is better \nGodard et al. [14], ResNet \nCS\u2192K \n4.935 \n0.206 \n0.114 0.898 \n0.861 \n0.949 \n0.976 \nKuznietsov et al. [23] \nK \n4.621 \n0.189 \n0.113 0.741 \n0.862 \n0.960 \n0.986 \nOurs, SimpleNet \nKo \n4.886 \n0.209 \n0.112 0.888 \n0.862 \n0.950 \n0.976 \nOurs, SimpleNet \nKo \u2192 Kr \n4.817 \n0.202 \n0.108 0.862 \n0.867 \n0.950 \n0.977 \nOurs, SimpleNet \nKr \u2192 Ko \n4.890 \n0.208 \n0.115 0.870 \n0.863 \n0.950 \n0.977 \nOurs, SimpleNet \nKo \u2192 Kr \u2192 Ko \n4.785 \n0.199 \n0.107 0.852 \n0.866 \n0.950 \n0.978 \nOurs, StackNet \nKo \u2192 Kr \u2192 Ko 4.442 \n0.187 \n0.097 0.734 \n0.888 \n0.958 \n0.980 \n\nGarg et al. [11] L12 Aug 8\u00d7 \nK \n5.104 \n0.273 \n0.169 1.080 \n0.740 \n0.904 \n0.962 \nGodard et al. [14], ResNet \nCS\u2192K \n3.729 \n0.194 \n0.108 0.657 \n0.873 \n0.954 \n0.979 \nKuznietsov et al. [23] \nK \n3.518 \n0.179 \n0.108 0.595 \n0.875 \n0.964 \n0.988 \nOurs, StackNet \nKo \u2192 Kr \u2192 Ko 3.390 \n0.177 \n0.092 0.547 \n0.898 \n0.962 \n0.982 \n\nTable 1: Evaluation results on the KITTI [13] Raw test split of Eigen et al. [6]. CS \nrefers to the Cityscapes dataset [3]. Upper part: depth range 0-80 m, lower part: \n1-50 m. All results are obtained using the crop from [6]. Our SimpleNet trained \non K o outperforms [14] (self-supervised) trained on CS and K. StackNet also \noutperforms semi-supervision with LiDAR [23] on most metrics. \n\nInput \nOurs \nGodard et al. [14] \nKuznietsov et al. [23] \n\n\n\n\nand the following tables, DVSO means our full approachMono DSO \nin \nin, vs \nin, vs, lr in, vs, tb DVSO'([14]) \nDVSO \nSeq. \n\n\nTable 2 :\n2Ablation study for DVSO.*  and  \u2020 indicate the sequences used and not \nused for training StackNet, respectively. t rel (%) and r rel ( \u2022 ) are translational-\nand rotational RMSE, respectively. Both t rel and r rel are averaged over 100 to \n800 m intervals. in: D L is used for depth initialization. vs: virtual stereo term \nis used with D R . lr: left-right disparity consistency is checked using predictions. \ntb: tuned virtual baseline is used. DVSO'([14]): full (in, vs, lr, tb) with depth \nfrom [14]. DVSO: full with depth from StackNet. Best results are shown as bold, \nsecond best italic. DVSO clearly outperforms the other variants. \n\n\n\nTable 3 :\n3Comparison with state-of-the-art stereo visual odometry. DVSO: our full approach (in, vs, lr, tb). Global optimization and loop-closure are turned off for stereo ORB-SLAM2 and Stereo LSD-SLAM. DVSO (monocular) achieves comparable performance to these stereo methods. and stereo methods. DVSO provides significantly more consistent trajectories than other monocular methods and compares well to stereo approaches. Bottom: DVSO with StackNet produces more accurate trajectory and map than with[14].Fig. 6: Results on KITTI \nodometry seq. 00. Top: com-\nparisons with monocular \nmethods \n(Sim(3)-aligned) \nOurs \nGodard et al. 6] \n\n-300 \n-200 \n-100 \n0 \n100 \n200 \n300 \n\nx[m] \n\n0 \n\n50 \n\n100 \n\n150 \n\n200 \n\n250 \n\n300 \n\n350 \n\n400 \n\n450 \n\ny[m] \n\nGT \nStereo ORB \nStereo DSO \nin,vs,lr \nDVSO \n\n-400 \n-300 \n-200 \n-100 \n0 \n100 \n200 \n300 \n\nx[m] \n\n-100 \n\n0 \n\n100 \n\n200 \n\n300 \n\n400 \n\ny[m] \n\nGT \nMono ORB \nMono DSO \nin,vs,lr \nDVSO \n\n\n\n\nFig. 7: Evaluation results on the KITTI odometry test set. We show translational and rotational errors with respect to path length intervals. For translational errors, DVSO achieves comparable performance to Stereo LSD-SLAM, while for rotational errors, DVSO achieves comparable results to Stereo DSO and better results than all other methods. Note that with virtual baseline tuning, DVSO achieves the best performance among all the methods evaluated.100 \n200 \n300 \n400 \n500 \n600 \n700 \n800 \n\nPath Length [m] \n\n0.6 \n\n0.8 \n\n1 \n\n1.2 \n\n1.4 \n\n1.6 \n\n1.8 \n\nTranslational \n\nError \n\n[%] \n\nORB-SLAM2 \nStereo LSD-SLAM \nStereo DSO \nin,vs,lr \nDVSO \n\n100 \n200 \n300 \n400 \n500 \n600 \n700 \n800 \n\nPath Length [m] \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\nRotational \n\nError \n\n[deg/100m] \n\nORB-SLAM2 \nStereo LSD-SLAM \nStereo DSO \nin,vs,lr \nDVSO \n\nDeepVO [41] UnDeepVO [26] Yin et al. [46] SfMLearner [49] \nin, vs, lr \nDVSO \nSeq. \nt rel \nr rel \nt rel \nr rel \nt rel \nr rel \nt rel \nr rel \nt rel \nr rel \nt rel \nr rel \n00  \u2020 \n\u2212 \n\u2212 \n4.41 \n1.92 \n\u2212 \n\u2212 \n66.35 \n6.13 \n0.93 \n0.24 \n0.71 \n0.24 \n\n03  \u2020 \n8.49 \n6.89 \n5.00 \n6.17 \n\u2212 \n\u2212 \n10.78 \n3.92 \n2.56 \n0.18 \n0.77 \n0.18 \n\n04  \u2020 \n7.19 \n6.97 \n4.49 \n2.13 \n\u2212 \n\u2212 \n4.49 \n5.24 \n0.67 \n0.07 \n0.35 \n0.06 \n\n05  \u2020 \n2.62 \n3.61 \n3.40 \n1.50 \n\u2212 \n\u2212 \n18.67 \n4.10 \n0.64 \n0.23 \n0.58 \n0.22 \n\n07  \u2020 \n3.91 \n4.60 \n3.15 \n2.48 \n\u2212 \n\u2212 \n21.33 \n6.65 \n0.80 \n0.38 \n0.73 \n0.35 \n01  *  \n\u2212 \n\u2212 \n69.07 \n1.60 \n\u2212 \n\u2212 \n35.17 \n2.74 \n1.52 \n0.12 \n1.18 \n0.11 \n02  *  \n\u2212 \n\u2212 \n5.58 \n2.44 \n\u2212 \n\u2212 \n58.75 \n3.58 \n1.05 \n0.23 \n0.84 \n0.22 \n06  *  \n5.42 \n5.82 \n6.20 \n1.98 \n\u2212 \n\u2212 \n25.88 \n4.80 \n0.80 \n0.24 \n0.71 \n0.20 \n08  *  \n\u2212 \n\u2212 \n4.08 \n1.79 \n2.22 \n0.10 \n21.90 \n2.91 \n1.10 \n0.26 \n1.03 \n0.25 \n09  *  \n\u2212 \n\u2212 \n7.01 \n3.61 \n4.14 \n0.11 \n18.77 \n3.21 \n0.95 \n0.21 \n0.83 \n0.21 \n10  *  \n8.11 \n8.83 \n10.63 \n4.65 \n1.70 \n0.17 \n14.33 \n3.30 \n0.59 \n0.22 \n0.74 \n0.21 \n\n\n\nTable 4 :\n4Comparison with deep learning approaches. Note that Deep VO\nAcknowledgements We would like to thank Cl\u00e9ment Godard, Yevhen Kuznietsov, Ruihao Li and Tinghui Zhou for providing the data and code. We also would like to thank Martin Schw\u00f6rer for the fruitful discussion.\nTensorflow: A system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, In: OSDI. 169Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghe- mawat, S., Irving, G., Isard, M., et al.: Tensorflow: A system for large-scale machine learning. In: OSDI. vol. 16, pp. 265-283 (2016) 9\n\nCodeslamlearning a compact, optimisable representation for dense visual slam. M Bloesch, J Czarnowski, R Clark, S Leutenegger, A J Davison, arXiv:1804.008743arXiv preprintBloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., Davison, A.J.: Codeslam- learning a compact, optimisable representation for dense visual slam. arXiv preprint arXiv:1804.00874 (2018) 3\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)911Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 9, 11\n\nContinuous stereo self-calibration by camera parameter tracking. T Dang, C Hoffmann, C Stiller, IEEE Transactions on Image Processing. 1873Dang, T., Hoffmann, C., Stiller, C.: Continuous stereo self-calibration by camera parameter tracking. IEEE Transactions on Image Processing 18(7), 1536-1550 (2009) 3\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision3Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In: Proceedings of the IEEE In- ternational Conference on Computer Vision. pp. 2650-2658 (2015) 3\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Advances in neural information processing systems. 211Eigen, D., Puhrsch, C., Fergus, R.: Depth map prediction from a single image using a multi-scale deep network. In: Advances in neural information processing systems. pp. 2366-2374 (2014) 2, 3, 9, 10, 11\n\nSemi-dense visual odometry for a monocular camera. J Engel, J Sturm, D Cremers, IEEE International Conference on Computer Vision (ICCV. 3Engel, J., Sturm, J., Cremers, D.: Semi-dense visual odometry for a monocular camera. In: IEEE International Conference on Computer Vision (ICCV) (2013) 3\n\nDirect sparse odometry. J Engel, V Koltun, D Cremers, 2, 3, 7, 8Engel, J., Koltun, V., Cremers, D.: Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence (2017) 2, 3, 7, 8\n\nLsd-slam: Large-scale direct monocular slam. J Engel, T Sch\u00f6ps, D Cremers, European Conference on Computer Vision. Springer3Engel, J., Sch\u00f6ps, T., Cremers, D.: Lsd-slam: Large-scale direct monocular slam. In: European Conference on Computer Vision. pp. 834-849. Springer (2014) 3\n\nLarge-scale direct slam with stereo cameras. J Engel, J St\u00fcckler, D Cremers, Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on. IEEE313Engel, J., St\u00fcckler, J., Cremers, D.: Large-scale direct slam with stereo cameras. In: Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on. pp. 1935-1942. IEEE (2015) 3, 13\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. R Garg, G Carneiro, I Reid, European Conference on Computer Vision. Springer211Garg, R., Carneiro, G., Reid, I.: Unsupervised cnn for single view depth estimation: Geometry to the rescue. In: European Conference on Computer Vision. pp. 740- 756. Springer (2016) 2, 3, 9, 11\n\nVision meets robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, International Journal of Robotics Research (IJRR). 810Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR) (2013) 8, 10\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, Conference on Computer Vision and Pattern Recognition (CVPR). 11Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision benchmark suite. In: Conference on Computer Vision and Pattern Recogni- tion (CVPR) (2012) 11\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, arXiv:1609.036771013arXiv preprintGodard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocular depth esti- mation with left-right consistency. arXiv preprint arXiv:1609.03677 (2016) 2, 3, 4, 5, 6, 9, 10, 11, 12, 13\n\nA benchmark for rgb-d visual odometry, 3d reconstruction and slam. A Handa, T Whelan, J Mcdonald, A J Davison, Robotics and automation (ICRA), 2014 IEEE international conference on. IEEE3Handa, A., Whelan, T., McDonald, J., Davison, A.J.: A benchmark for rgb-d visual odometry, 3d reconstruction and slam. In: Robotics and automation (ICRA), 2014 IEEE international conference on. pp. 1524-1531. IEEE (2014) 3\n\nMultiple view geometry in computer vision. R Hartley, A Zisserman, Cambridge university press7Hartley, R., Zisserman, A.: Multiple view geometry in computer vision. Cambridge university press (2003) 7, 8\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition3He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016) 3\n\nPm-huber: Patchmatch with huber regularization for stereo matching. P Heise, S Klose, B Jensen, A Knoll, Computer Vision (ICCV), 2013 IEEE International Conference on. IEEE6Heise, P., Klose, S., Jensen, B., Knoll, A.: Pm-huber: Patchmatch with huber regularization for stereo matching. In: Computer Vision (ICCV), 2013 IEEE In- ternational Conference on. pp. 2360-2367. IEEE (2013) 6\n\nAutomatic photo pop-up. D Hoiem, A A Efros, M Hebert, ACM transactions on graphics (TOG). ACM243Hoiem, D., Efros, A.A., Hebert, M.: Automatic photo pop-up. In: ACM transac- tions on graphics (TOG). vol. 24, pp. 577-584. ACM (2005) 3\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 25Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: Flownet 2.0: Evolution of optical flow estimation with deep networks. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). vol. 2 (2017) 3, 4, 5\n\nM Jaderberg, K Simonyan, A Zisserman, Advances in neural information processing systems. 5Spatial transformer networksJaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks. In: Advances in neural information processing systems. pp. 2017-2025 (2015) 5\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.69809arXiv preprintKingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 9\n\nSemi-supervised deep learning for monocular depth map prediction. Y Kuznietsov, J St\u00fcckler, B Leibe, arXiv:1702.02706211arXiv preprintKuznietsov, Y., St\u00fcckler, J., Leibe, B.: Semi-supervised deep learning for monoc- ular depth map prediction. arXiv preprint arXiv:1702.02706 (2017) 2, 3, 9, 10, 11\n\nDeeper depth prediction with fully convolutional residual networks. I Laina, C Rupprecht, V Belagiannis, F Tombari, N Navab, 3D Vision (3DV). Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F., Navab, N.: Deeper depth prediction with fully convolutional residual networks. In: 3D Vision (3DV), 2016\n\nFourth International Conference on. 26Fourth International Conference on. pp. 239-248. IEEE (2016) 2, 3, 6\n\nDepth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs. B Li, C Shen, Y Dai, A Van Den Hengel, M He, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition23Li, B., Shen, C., Dai, Y., van den Hengel, A., He, M.: Depth and surface normal estimation from monocular images using regression on deep features and hierarchi- cal crfs. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1119-1127 (2015) 2, 3\n\nUndeepvo: Monocular visual odometry through unsupervised deep learning. R Li, S Wang, Z Long, D Gu, arXiv:1709.06841314arXiv preprintLi, R., Wang, S., Long, Z., Gu, D.: Undeepvo: Monocular visual odometry through unsupervised deep learning. arXiv preprint arXiv:1709.06841 (2017) 3, 12, 14\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition3Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3431-3440 (2015) 3\n\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. N Mayer, E Ilg, P Hausser, P Fischer, D Cremers, A Dosovitskiy, T Brox, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition4Mayer, N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox, T.: A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4040-4048 (2016) 4\n\nA multi-state constraint kalman filter for visionaided inertial navigation. A I Mourikis, S I Roumeliotis, IEEE international conference on. IEEE8Robotics and automationMourikis, A.I., Roumeliotis, S.I.: A multi-state constraint kalman filter for vision- aided inertial navigation. In: Robotics and automation, 2007 IEEE international conference on. pp. 3565-3572. IEEE (2007) 8\n\nOrb-slam: a versatile and accurate monocular slam system. R Mur-Artal, J M M Montiel, J D Tardos, IEEE Transactions on Robotics. 3153Mur-Artal, R., Montiel, J.M.M., Tardos, J.D.: Orb-slam: a versatile and accurate monocular slam system. IEEE Transactions on Robotics 31(5), 1147-1163 (2015) 3\n\nOrb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. R Mur-Artal, J D Tard\u00f3s, IEEE Transactions on Robotics. 33513Mur-Artal, R., Tard\u00f3s, J.D.: Orb-slam2: An open-source slam system for monoc- ular, stereo, and rgb-d cameras. IEEE Transactions on Robotics 33(5), 1255-1262 (2017) 3, 12, 13\n\nDtam: Dense tracking and mapping in real-time. R A Newcombe, S J Lovegrove, A J Davison, Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE3Newcombe, R.A., Lovegrove, S.J., Davison, A.J.: Dtam: Dense tracking and map- ping in real-time. In: Computer Vision (ICCV), 2011 IEEE International Confer- ence on. pp. 2320-2327. IEEE (2011) 3\n\nDeconvolution and checkerboard artifacts. A Odena, V Dumoulin, C Olah, 10.23915/distill.00003Distill. Odena, A., Dumoulin, V., Olah, C.: Deconvolution and checkerboard artifacts. Distill (2016). https://doi.org/10.23915/distill.00003, http://distill.pub/2016/ deconv-checkerboard 4\n\nCascade residual learning: A twostage convolutional neural network for stereo matching. J Pang, W Sun, J Ren, C Yang, Q Yan, International Conf. on Computer Vision-Workshop on Geometry Meets Deep Learning. 34Pang, J., Sun, W., Ren, J., Yang, C., Yan, Q.: Cascade residual learning: A two- stage convolutional neural network for stereo matching. In: International Conf. on Computer Vision-Workshop on Geometry Meets Deep Learning (ICCVW 2017). vol. 3 (2017) 3, 4\n\nRemode: Probabilistic, monocular dense reconstruction in real time. M Pizzoli, C Forster, D Scaramuzza, Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEE3Pizzoli, M., Forster, C., Scaramuzza, D.: Remode: Probabilistic, monocular dense reconstruction in real time. In: Robotics and Automation (ICRA), 2014 IEEE International Conference on. pp. 2609-2616. IEEE (2014) 3\n\nLearning depth from single monocular images. A Saxena, S H Chung, A Y Ng, Advances in neural information processing systems. 39Saxena, A., Chung, S.H., Ng, A.Y.: Learning depth from single monocular images. In: Advances in neural information processing systems. pp. 1161-1168 (2006) 3, 9\n\nScale drift-aware large scale monocular slam. H Strasdat, J Montiel, A J Davison, Robotics: Science and Systems VI. 27Strasdat, H., Montiel, J., Davison, A.J.: Scale drift-aware large scale monocular slam. Robotics: Science and Systems VI 2 (2010) 3, 7\n\nA benchmark for the evaluation of rgb-d slam systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE3Sturm, J., Engelhard, N., Endres, F., Burgard, W., Cremers, D.: A benchmark for the evaluation of rgb-d slam systems. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. pp. 573-580. IEEE (2012) 3\n\nCnn-slam: Real-time dense monocular slam with learned depth prediction. K Tateno, F Tombari, I Laina, N Navab, arXiv:1704.034893arXiv preprintTateno, K., Tombari, F., Laina, I., Navab, N.: Cnn-slam: Real-time dense monoc- ular slam with learned depth prediction. arXiv preprint arXiv:1704.03489 (2017) 3\n\nStereo dso: Large-scale direct sparse visual odometry with stereo cameras. R Wang, M Schw\u00f6rer, D Cremers, International Conference on Computer Vision (ICCV). Venice, Italy213Wang, R., Schw\u00f6rer, M., Cremers, D.: Stereo dso: Large-scale direct sparse visual odometry with stereo cameras. In: International Conference on Computer Vision (ICCV). Venice, Italy (October 2017) 2, 8, 9, 12, 13\n\nDeepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. S Wang, R Clark, H Wen, N Trigoni, 2017 IEEE International Conference on. IEEE314Robotics and Automation (ICRAWang, S., Clark, R., Wen, H., Trigoni, N.: Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. In: Robotics and Automation (ICRA), 2017 IEEE International Conference on. pp. 2043-2050. IEEE (2017) 3, 12, 14\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE transactions on image processing. 1345Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13(4), 600-612 (2004) 5\n\nGlobal stereo reconstruction under second-order smoothness priors. O Woodford, P Torr, I Reid, A Fitzgibbon, 6Woodford, O., Torr, P., Reid, I., Fitzgibbon, A.: Global stereo reconstruction un- der second-order smoothness priors. IEEE transactions on pattern analysis and machine intelligence 31(12), 2115-2128 (2009) 6\n\nDeep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks. J Xie, R Girshick, A Farhadi, European Conference on Computer Vision. Springer3Xie, J., Girshick, R., Farhadi, A.: Deep3d: Fully automatic 2d-to-3d video conver- sion with deep convolutional neural networks. In: European Conference on Com- puter Vision. pp. 842-857. Springer (2016) 3\n\nChallenges in monocular visual odometry: Photometric calibration, motion bias and rolling shutter effect. N Yang, R Wang, X Gao, D Cremers, 10.1109/LRA.2018.2846813IEEE Robotics and Automation Letters (RA-L). 3Yang, N., Wang, R., Gao, X., Cremers, D.: Challenges in monocular vi- sual odometry: Photometric calibration, motion bias and rolling shutter ef- fect. IEEE Robotics and Automation Letters (RA-L) 3, 2878-2885 (Oct 2018). https://doi.org/10.1109/LRA.2018.2846813 3\n\nScale recovery for monocular visual odometry using depth estimated with deep convolutional neural fields. X Yin, X Wang, X Du, Q Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition314Yin, X., Wang, X., Du, X., Chen, Q.: Scale recovery for monocular visual odometry using depth estimated with deep convolutional neural fields. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5870-5878 (2017) 3, 12, 14\n\nH Zhao, O Gallo, I Frosio, J Kautz, Is l2 a good loss function for neural networks for image processing? ArXiv e-prints 1511. 56Zhao, H., Gallo, O., Frosio, I., Kautz, J.: Is l2 a good loss function for neural networks for image processing? ArXiv e-prints 1511 (2015) 5, 6\n\nSelf-supervised learning for stereo matching with selfimproving ability. Y Zhong, Y Dai, H Li, arXiv:1709.009306arXiv preprintZhong, Y., Dai, Y., Li, H.: Self-supervised learning for stereo matching with self- improving ability. arXiv preprint arXiv:1709.00930 (2017) 6\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, CVPR. 214Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised learning of depth and ego-motion from video. In: CVPR. vol. 2, p. 7 (2017) 2, 3, 12, 14\n", "annotations": {"author": "[{\"end\":172,\"start\":103},{\"end\":242,\"start\":173},{\"end\":309,\"start\":243},{\"end\":387,\"start\":310}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":107},{\"end\":181,\"start\":177},{\"end\":256,\"start\":248},{\"end\":324,\"start\":317}]", "author_first_name": "[{\"end\":106,\"start\":103},{\"end\":176,\"start\":173},{\"end\":247,\"start\":243},{\"end\":316,\"start\":310}]", "author_affiliation": "[{\"end\":160,\"start\":129},{\"end\":171,\"start\":162},{\"end\":230,\"start\":199},{\"end\":241,\"start\":232},{\"end\":308,\"start\":277},{\"end\":375,\"start\":344},{\"end\":386,\"start\":377}]", "title": "[{\"end\":100,\"start\":1},{\"end\":487,\"start\":388}]", "venue": null, "abstract": "[{\"end\":1662,\"start\":570}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2990,\"start\":2987},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3479,\"start\":3476},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3482,\"start\":3479},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3485,\"start\":3482},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3767,\"start\":3763},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3770,\"start\":3767},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3812,\"start\":3808},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4000,\"start\":3996},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4003,\"start\":4000},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4152,\"start\":4148},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4339,\"start\":4335},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4860,\"start\":4856},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4863,\"start\":4860},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4893,\"start\":4890},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4896,\"start\":4893},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4899,\"start\":4896},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4981,\"start\":4978},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4983,\"start\":4981},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5097,\"start\":5093},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5129,\"start\":5125},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5189,\"start\":5185},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5370,\"start\":5366},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5373,\"start\":5370},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5376,\"start\":5373},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5721,\"start\":5717},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5876,\"start\":5872},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6120,\"start\":6116},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6520,\"start\":6516},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6538,\"start\":6534},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6541,\"start\":6538},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6632,\"start\":6628},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6770,\"start\":6766},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6772,\"start\":6770},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6774,\"start\":6772},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6777,\"start\":6774},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6864,\"start\":6860},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6956,\"start\":6952},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6958,\"start\":6956},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6961,\"start\":6958},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7183,\"start\":7180},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7186,\"start\":7183},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7340,\"start\":7336},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7343,\"start\":7340},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7404,\"start\":7400},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7425,\"start\":7422},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7513,\"start\":7510},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7515,\"start\":7513},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7580,\"start\":7577},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7583,\"start\":7580},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7586,\"start\":7583},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7589,\"start\":7586},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7613,\"start\":7609},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7616,\"start\":7613},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7633,\"start\":7629},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7818,\"start\":7815},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8234,\"start\":8230},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8266,\"start\":8262},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8269,\"start\":8266},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9020,\"start\":9016},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9429,\"start\":9425},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9751,\"start\":9747},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9754,\"start\":9751},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10240,\"start\":10236},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10716,\"start\":10712},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12669,\"start\":12665},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12687,\"start\":12683},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12690,\"start\":12687},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12828,\"start\":12824},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13378,\"start\":13374},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13726,\"start\":13722},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13747,\"start\":13743},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13750,\"start\":13747},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14397,\"start\":14393},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14452,\"start\":14448},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14685,\"start\":14681},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15897,\"start\":15893},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16050,\"start\":16047},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16240,\"start\":16236},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16473,\"start\":16470},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16849,\"start\":16846},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16967,\"start\":16964},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17604,\"start\":17600},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17816,\"start\":17813},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17882,\"start\":17879},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18113,\"start\":18110},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18184,\"start\":18180},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19324,\"start\":19321},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19506,\"start\":19503},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19740,\"start\":19736},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20085,\"start\":20081},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20130,\"start\":20126},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20149,\"start\":20145},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20166,\"start\":20162},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20183,\"start\":20180},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20399,\"start\":20395},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20498,\"start\":20494},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20578,\"start\":20575},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20606,\"start\":20602},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21058,\"start\":21055},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21465,\"start\":21461},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21724,\"start\":21721},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22037,\"start\":22033},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22291,\"start\":22287},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22988,\"start\":22984},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23076,\"start\":23072},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23293,\"start\":23290},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23344,\"start\":23341},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23365,\"start\":23361},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23371,\"start\":23367},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23606,\"start\":23602},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23761,\"start\":23757},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23838,\"start\":23837},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24104,\"start\":24100},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24276,\"start\":24272},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24577,\"start\":24573},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25080,\"start\":25076},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25166,\"start\":25162},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25518,\"start\":25514},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25522,\"start\":25518},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25526,\"start\":25522},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25530,\"start\":25526},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25534,\"start\":25530},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25538,\"start\":25534},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25542,\"start\":25538},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25546,\"start\":25542},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25550,\"start\":25546},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25554,\"start\":25550},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25558,\"start\":25554},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25778,\"start\":25774},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26230,\"start\":26228},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26235,\"start\":26231},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26345,\"start\":26341},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26415,\"start\":26411},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26418,\"start\":26415},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27573,\"start\":27569},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28072,\"start\":28068},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28090,\"start\":28086},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28645,\"start\":28641},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29213,\"start\":29209},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29228,\"start\":29224},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":29248,\"start\":29244},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":29586,\"start\":29582},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29790,\"start\":29786},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29805,\"start\":29801},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29818,\"start\":29814},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30924,\"start\":30920},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31013,\"start\":31009},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":31033,\"start\":31029},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31131,\"start\":31127},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31140,\"start\":31136},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36168,\"start\":36164}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":32329,\"start\":31989},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32375,\"start\":32330},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32708,\"start\":32376},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33428,\"start\":32709},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34879,\"start\":33429},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35005,\"start\":34880},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35660,\"start\":35006},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":36586,\"start\":35661},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":38393,\"start\":36587},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":38465,\"start\":38394}]", "paragraph": "[{\"end\":3257,\"start\":1678},{\"end\":4500,\"start\":3259},{\"end\":4604,\"start\":4502},{\"end\":4766,\"start\":4621},{\"end\":6633,\"start\":4768},{\"end\":8752,\"start\":6635},{\"end\":9197,\"start\":8804},{\"end\":9835,\"start\":9222},{\"end\":10448,\"start\":9837},{\"end\":10999,\"start\":10450},{\"end\":11614,\"start\":11001},{\"end\":11831,\"start\":11632},{\"end\":12384,\"start\":12000},{\"end\":12829,\"start\":12386},{\"end\":13166,\"start\":12952},{\"end\":13487,\"start\":13244},{\"end\":13821,\"start\":13561},{\"end\":14160,\"start\":13903},{\"end\":14453,\"start\":14282},{\"end\":15554,\"start\":14455},{\"end\":16241,\"start\":15597},{\"end\":16403,\"start\":16256},{\"end\":16618,\"start\":16405},{\"end\":17415,\"start\":16673},{\"end\":18114,\"start\":17483},{\"end\":18330,\"start\":18116},{\"end\":18413,\"start\":18408},{\"end\":19055,\"start\":18450},{\"end\":19090,\"start\":19057},{\"end\":19922,\"start\":19151},{\"end\":20499,\"start\":19938},{\"end\":20950,\"start\":20501},{\"end\":22703,\"start\":20981},{\"end\":23213,\"start\":22705},{\"end\":25384,\"start\":23215},{\"end\":25779,\"start\":25414},{\"end\":26519,\"start\":25781},{\"end\":27574,\"start\":26521},{\"end\":29174,\"start\":27576},{\"end\":29773,\"start\":29176},{\"end\":29839,\"start\":29775},{\"end\":29907,\"start\":29841},{\"end\":31271,\"start\":29922},{\"end\":31474,\"start\":31273},{\"end\":31988,\"start\":31476}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11999,\"start\":11832},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12951,\"start\":12830},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13243,\"start\":13167},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13560,\"start\":13488},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13902,\"start\":13822},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14281,\"start\":14161},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15596,\"start\":15555},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16255,\"start\":16242},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16672,\"start\":16619},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17482,\"start\":17416},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18407,\"start\":18331},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18449,\"start\":18414},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19150,\"start\":19091}]", "table_ref": "[{\"end\":23229,\"start\":23222},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25902,\"start\":25895},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27820,\"start\":27813},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":29351,\"start\":29344},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":29463,\"start\":29456}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1676,\"start\":1664},{\"attributes\":{\"n\":\"1.1\"},\"end\":4619,\"start\":4607},{\"attributes\":{\"n\":\"2\"},\"end\":8802,\"start\":8755},{\"attributes\":{\"n\":\"2.1\"},\"end\":9220,\"start\":9200},{\"attributes\":{\"n\":\"2.2\"},\"end\":11630,\"start\":11617},{\"attributes\":{\"n\":\"4\"},\"end\":19936,\"start\":19925},{\"attributes\":{\"n\":\"4.1\"},\"end\":20979,\"start\":20953},{\"attributes\":{\"n\":\"4.2\"},\"end\":25412,\"start\":25387},{\"attributes\":{\"n\":\"5\"},\"end\":29920,\"start\":29910},{\"end\":31998,\"start\":31990},{\"end\":32339,\"start\":32331},{\"end\":32713,\"start\":32710},{\"end\":35016,\"start\":35007},{\"end\":35671,\"start\":35662},{\"end\":38404,\"start\":38395}]", "table": "[{\"end\":33428,\"start\":33298},{\"end\":34879,\"start\":33526},{\"end\":35005,\"start\":34936},{\"end\":35660,\"start\":35042},{\"end\":36586,\"start\":36169},{\"end\":38393,\"start\":37040}]", "figure_caption": "[{\"end\":32329,\"start\":32000},{\"end\":32375,\"start\":32341},{\"end\":32708,\"start\":32378},{\"end\":33298,\"start\":32715},{\"end\":33526,\"start\":33431},{\"end\":34936,\"start\":34882},{\"end\":35042,\"start\":35018},{\"end\":36169,\"start\":35673},{\"end\":37040,\"start\":36589},{\"end\":38465,\"start\":38406}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9340,\"start\":9332},{\"end\":15346,\"start\":15338},{\"end\":23849,\"start\":23841},{\"end\":24420,\"start\":24412},{\"end\":28348,\"start\":28340},{\"end\":28737,\"start\":28729},{\"end\":28787,\"start\":28779},{\"end\":28956,\"start\":28948}]", "bib_author_first_name": "[{\"end\":38730,\"start\":38729},{\"end\":38739,\"start\":38738},{\"end\":38749,\"start\":38748},{\"end\":38757,\"start\":38756},{\"end\":38765,\"start\":38764},{\"end\":38774,\"start\":38773},{\"end\":38782,\"start\":38781},{\"end\":38791,\"start\":38790},{\"end\":38803,\"start\":38802},{\"end\":38813,\"start\":38812},{\"end\":39131,\"start\":39130},{\"end\":39142,\"start\":39141},{\"end\":39156,\"start\":39155},{\"end\":39165,\"start\":39164},{\"end\":39180,\"start\":39179},{\"end\":39182,\"start\":39181},{\"end\":39481,\"start\":39480},{\"end\":39491,\"start\":39490},{\"end\":39500,\"start\":39499},{\"end\":39509,\"start\":39508},{\"end\":39520,\"start\":39519},{\"end\":39533,\"start\":39532},{\"end\":39545,\"start\":39544},{\"end\":39555,\"start\":39554},{\"end\":39563,\"start\":39562},{\"end\":40065,\"start\":40064},{\"end\":40073,\"start\":40072},{\"end\":40085,\"start\":40084},{\"end\":40414,\"start\":40413},{\"end\":40423,\"start\":40422},{\"end\":40860,\"start\":40859},{\"end\":40869,\"start\":40868},{\"end\":40880,\"start\":40879},{\"end\":41199,\"start\":41198},{\"end\":41208,\"start\":41207},{\"end\":41217,\"start\":41216},{\"end\":41465,\"start\":41464},{\"end\":41474,\"start\":41473},{\"end\":41484,\"start\":41483},{\"end\":41692,\"start\":41691},{\"end\":41701,\"start\":41700},{\"end\":41711,\"start\":41710},{\"end\":41973,\"start\":41972},{\"end\":41982,\"start\":41981},{\"end\":41994,\"start\":41993},{\"end\":42372,\"start\":42371},{\"end\":42380,\"start\":42379},{\"end\":42392,\"start\":42391},{\"end\":42689,\"start\":42688},{\"end\":42699,\"start\":42698},{\"end\":42707,\"start\":42706},{\"end\":42718,\"start\":42717},{\"end\":43008,\"start\":43007},{\"end\":43018,\"start\":43017},{\"end\":43026,\"start\":43025},{\"end\":43354,\"start\":43353},{\"end\":43364,\"start\":43363},{\"end\":43377,\"start\":43376},{\"end\":43379,\"start\":43378},{\"end\":43677,\"start\":43676},{\"end\":43686,\"start\":43685},{\"end\":43696,\"start\":43695},{\"end\":43708,\"start\":43707},{\"end\":43710,\"start\":43709},{\"end\":44064,\"start\":44063},{\"end\":44075,\"start\":44074},{\"end\":44272,\"start\":44271},{\"end\":44278,\"start\":44277},{\"end\":44287,\"start\":44286},{\"end\":44294,\"start\":44293},{\"end\":44699,\"start\":44698},{\"end\":44708,\"start\":44707},{\"end\":44717,\"start\":44716},{\"end\":44727,\"start\":44726},{\"end\":45040,\"start\":45039},{\"end\":45049,\"start\":45048},{\"end\":45051,\"start\":45050},{\"end\":45060,\"start\":45059},{\"end\":45320,\"start\":45319},{\"end\":45327,\"start\":45326},{\"end\":45336,\"start\":45335},{\"end\":45346,\"start\":45345},{\"end\":45356,\"start\":45355},{\"end\":45371,\"start\":45370},{\"end\":45683,\"start\":45682},{\"end\":45696,\"start\":45695},{\"end\":45708,\"start\":45707},{\"end\":46006,\"start\":46005},{\"end\":46008,\"start\":46007},{\"end\":46018,\"start\":46017},{\"end\":46227,\"start\":46226},{\"end\":46241,\"start\":46240},{\"end\":46253,\"start\":46252},{\"end\":46528,\"start\":46527},{\"end\":46537,\"start\":46536},{\"end\":46550,\"start\":46549},{\"end\":46565,\"start\":46564},{\"end\":46576,\"start\":46575},{\"end\":46987,\"start\":46986},{\"end\":46993,\"start\":46992},{\"end\":47001,\"start\":47000},{\"end\":47008,\"start\":47007},{\"end\":47026,\"start\":47025},{\"end\":47529,\"start\":47528},{\"end\":47535,\"start\":47534},{\"end\":47543,\"start\":47542},{\"end\":47551,\"start\":47550},{\"end\":47804,\"start\":47803},{\"end\":47812,\"start\":47811},{\"end\":47825,\"start\":47824},{\"end\":48283,\"start\":48282},{\"end\":48292,\"start\":48291},{\"end\":48299,\"start\":48298},{\"end\":48310,\"start\":48309},{\"end\":48321,\"start\":48320},{\"end\":48332,\"start\":48331},{\"end\":48347,\"start\":48346},{\"end\":48870,\"start\":48869},{\"end\":48872,\"start\":48871},{\"end\":48884,\"start\":48883},{\"end\":48886,\"start\":48885},{\"end\":49232,\"start\":49231},{\"end\":49245,\"start\":49244},{\"end\":49249,\"start\":49246},{\"end\":49260,\"start\":49259},{\"end\":49262,\"start\":49261},{\"end\":49548,\"start\":49547},{\"end\":49561,\"start\":49560},{\"end\":49563,\"start\":49562},{\"end\":49832,\"start\":49831},{\"end\":49834,\"start\":49833},{\"end\":49846,\"start\":49845},{\"end\":49848,\"start\":49847},{\"end\":49861,\"start\":49860},{\"end\":49863,\"start\":49862},{\"end\":50180,\"start\":50179},{\"end\":50189,\"start\":50188},{\"end\":50201,\"start\":50200},{\"end\":50509,\"start\":50508},{\"end\":50517,\"start\":50516},{\"end\":50524,\"start\":50523},{\"end\":50531,\"start\":50530},{\"end\":50539,\"start\":50538},{\"end\":50952,\"start\":50951},{\"end\":50963,\"start\":50962},{\"end\":50974,\"start\":50973},{\"end\":51324,\"start\":51323},{\"end\":51334,\"start\":51333},{\"end\":51336,\"start\":51335},{\"end\":51345,\"start\":51344},{\"end\":51347,\"start\":51346},{\"end\":51614,\"start\":51613},{\"end\":51626,\"start\":51625},{\"end\":51637,\"start\":51636},{\"end\":51639,\"start\":51638},{\"end\":51876,\"start\":51875},{\"end\":51885,\"start\":51884},{\"end\":51898,\"start\":51897},{\"end\":51908,\"start\":51907},{\"end\":51919,\"start\":51918},{\"end\":52321,\"start\":52320},{\"end\":52331,\"start\":52330},{\"end\":52342,\"start\":52341},{\"end\":52351,\"start\":52350},{\"end\":52629,\"start\":52628},{\"end\":52637,\"start\":52636},{\"end\":52649,\"start\":52648},{\"end\":53036,\"start\":53035},{\"end\":53044,\"start\":53043},{\"end\":53053,\"start\":53052},{\"end\":53060,\"start\":53059},{\"end\":53470,\"start\":53469},{\"end\":53478,\"start\":53477},{\"end\":53480,\"start\":53479},{\"end\":53489,\"start\":53488},{\"end\":53491,\"start\":53490},{\"end\":53501,\"start\":53500},{\"end\":53503,\"start\":53502},{\"end\":53819,\"start\":53818},{\"end\":53831,\"start\":53830},{\"end\":53839,\"start\":53838},{\"end\":53847,\"start\":53846},{\"end\":54163,\"start\":54162},{\"end\":54170,\"start\":54169},{\"end\":54182,\"start\":54181},{\"end\":54555,\"start\":54554},{\"end\":54563,\"start\":54562},{\"end\":54571,\"start\":54570},{\"end\":54578,\"start\":54577},{\"end\":55030,\"start\":55029},{\"end\":55037,\"start\":55036},{\"end\":55045,\"start\":55044},{\"end\":55051,\"start\":55050},{\"end\":55461,\"start\":55460},{\"end\":55469,\"start\":55468},{\"end\":55478,\"start\":55477},{\"end\":55488,\"start\":55487},{\"end\":55808,\"start\":55807},{\"end\":55817,\"start\":55816},{\"end\":55824,\"start\":55823},{\"end\":56064,\"start\":56063},{\"end\":56072,\"start\":56071},{\"end\":56081,\"start\":56080},{\"end\":56092,\"start\":56091},{\"end\":56094,\"start\":56093}]", "bib_author_last_name": "[{\"end\":38736,\"start\":38731},{\"end\":38746,\"start\":38740},{\"end\":38754,\"start\":38750},{\"end\":38762,\"start\":38758},{\"end\":38771,\"start\":38766},{\"end\":38779,\"start\":38775},{\"end\":38788,\"start\":38783},{\"end\":38800,\"start\":38792},{\"end\":38810,\"start\":38804},{\"end\":38819,\"start\":38814},{\"end\":39139,\"start\":39132},{\"end\":39153,\"start\":39143},{\"end\":39162,\"start\":39157},{\"end\":39177,\"start\":39166},{\"end\":39190,\"start\":39183},{\"end\":39488,\"start\":39482},{\"end\":39497,\"start\":39492},{\"end\":39506,\"start\":39501},{\"end\":39517,\"start\":39510},{\"end\":39530,\"start\":39521},{\"end\":39542,\"start\":39534},{\"end\":39552,\"start\":39546},{\"end\":39560,\"start\":39556},{\"end\":39571,\"start\":39564},{\"end\":40070,\"start\":40066},{\"end\":40082,\"start\":40074},{\"end\":40093,\"start\":40086},{\"end\":40420,\"start\":40415},{\"end\":40430,\"start\":40424},{\"end\":40866,\"start\":40861},{\"end\":40877,\"start\":40870},{\"end\":40887,\"start\":40881},{\"end\":41205,\"start\":41200},{\"end\":41214,\"start\":41209},{\"end\":41225,\"start\":41218},{\"end\":41471,\"start\":41466},{\"end\":41481,\"start\":41475},{\"end\":41492,\"start\":41485},{\"end\":41698,\"start\":41693},{\"end\":41708,\"start\":41702},{\"end\":41719,\"start\":41712},{\"end\":41979,\"start\":41974},{\"end\":41991,\"start\":41983},{\"end\":42002,\"start\":41995},{\"end\":42377,\"start\":42373},{\"end\":42389,\"start\":42381},{\"end\":42397,\"start\":42393},{\"end\":42696,\"start\":42690},{\"end\":42704,\"start\":42700},{\"end\":42715,\"start\":42708},{\"end\":42726,\"start\":42719},{\"end\":43015,\"start\":43009},{\"end\":43023,\"start\":43019},{\"end\":43034,\"start\":43027},{\"end\":43361,\"start\":43355},{\"end\":43374,\"start\":43365},{\"end\":43387,\"start\":43380},{\"end\":43683,\"start\":43678},{\"end\":43693,\"start\":43687},{\"end\":43705,\"start\":43697},{\"end\":43718,\"start\":43711},{\"end\":44072,\"start\":44065},{\"end\":44085,\"start\":44076},{\"end\":44275,\"start\":44273},{\"end\":44284,\"start\":44279},{\"end\":44291,\"start\":44288},{\"end\":44298,\"start\":44295},{\"end\":44705,\"start\":44700},{\"end\":44714,\"start\":44709},{\"end\":44724,\"start\":44718},{\"end\":44733,\"start\":44728},{\"end\":45046,\"start\":45041},{\"end\":45057,\"start\":45052},{\"end\":45067,\"start\":45061},{\"end\":45324,\"start\":45321},{\"end\":45333,\"start\":45328},{\"end\":45343,\"start\":45337},{\"end\":45353,\"start\":45347},{\"end\":45368,\"start\":45357},{\"end\":45376,\"start\":45372},{\"end\":45693,\"start\":45684},{\"end\":45705,\"start\":45697},{\"end\":45718,\"start\":45709},{\"end\":46015,\"start\":46009},{\"end\":46021,\"start\":46019},{\"end\":46238,\"start\":46228},{\"end\":46250,\"start\":46242},{\"end\":46259,\"start\":46254},{\"end\":46534,\"start\":46529},{\"end\":46547,\"start\":46538},{\"end\":46562,\"start\":46551},{\"end\":46573,\"start\":46566},{\"end\":46582,\"start\":46577},{\"end\":46990,\"start\":46988},{\"end\":46998,\"start\":46994},{\"end\":47005,\"start\":47002},{\"end\":47023,\"start\":47009},{\"end\":47029,\"start\":47027},{\"end\":47532,\"start\":47530},{\"end\":47540,\"start\":47536},{\"end\":47548,\"start\":47544},{\"end\":47554,\"start\":47552},{\"end\":47809,\"start\":47805},{\"end\":47822,\"start\":47813},{\"end\":47833,\"start\":47826},{\"end\":48289,\"start\":48284},{\"end\":48296,\"start\":48293},{\"end\":48307,\"start\":48300},{\"end\":48318,\"start\":48311},{\"end\":48329,\"start\":48322},{\"end\":48344,\"start\":48333},{\"end\":48352,\"start\":48348},{\"end\":48881,\"start\":48873},{\"end\":48898,\"start\":48887},{\"end\":49242,\"start\":49233},{\"end\":49257,\"start\":49250},{\"end\":49269,\"start\":49263},{\"end\":49558,\"start\":49549},{\"end\":49570,\"start\":49564},{\"end\":49843,\"start\":49835},{\"end\":49858,\"start\":49849},{\"end\":49871,\"start\":49864},{\"end\":50186,\"start\":50181},{\"end\":50198,\"start\":50190},{\"end\":50206,\"start\":50202},{\"end\":50514,\"start\":50510},{\"end\":50521,\"start\":50518},{\"end\":50528,\"start\":50525},{\"end\":50536,\"start\":50532},{\"end\":50543,\"start\":50540},{\"end\":50960,\"start\":50953},{\"end\":50971,\"start\":50964},{\"end\":50985,\"start\":50975},{\"end\":51331,\"start\":51325},{\"end\":51342,\"start\":51337},{\"end\":51350,\"start\":51348},{\"end\":51623,\"start\":51615},{\"end\":51634,\"start\":51627},{\"end\":51647,\"start\":51640},{\"end\":51882,\"start\":51877},{\"end\":51895,\"start\":51886},{\"end\":51905,\"start\":51899},{\"end\":51916,\"start\":51909},{\"end\":51927,\"start\":51920},{\"end\":52328,\"start\":52322},{\"end\":52339,\"start\":52332},{\"end\":52348,\"start\":52343},{\"end\":52357,\"start\":52352},{\"end\":52634,\"start\":52630},{\"end\":52646,\"start\":52638},{\"end\":52657,\"start\":52650},{\"end\":53041,\"start\":53037},{\"end\":53050,\"start\":53045},{\"end\":53057,\"start\":53054},{\"end\":53068,\"start\":53061},{\"end\":53475,\"start\":53471},{\"end\":53486,\"start\":53481},{\"end\":53498,\"start\":53492},{\"end\":53514,\"start\":53504},{\"end\":53828,\"start\":53820},{\"end\":53836,\"start\":53832},{\"end\":53844,\"start\":53840},{\"end\":53858,\"start\":53848},{\"end\":54167,\"start\":54164},{\"end\":54179,\"start\":54171},{\"end\":54190,\"start\":54183},{\"end\":54560,\"start\":54556},{\"end\":54568,\"start\":54564},{\"end\":54575,\"start\":54572},{\"end\":54586,\"start\":54579},{\"end\":55034,\"start\":55031},{\"end\":55042,\"start\":55038},{\"end\":55048,\"start\":55046},{\"end\":55056,\"start\":55052},{\"end\":55466,\"start\":55462},{\"end\":55475,\"start\":55470},{\"end\":55485,\"start\":55479},{\"end\":55494,\"start\":55489},{\"end\":55814,\"start\":55809},{\"end\":55821,\"start\":55818},{\"end\":55827,\"start\":55825},{\"end\":56069,\"start\":56065},{\"end\":56078,\"start\":56073},{\"end\":56089,\"start\":56082},{\"end\":56099,\"start\":56095}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6287870},\"end\":39050,\"start\":38674},{\"attributes\":{\"doi\":\"arXiv:1804.00874\",\"id\":\"b1\"},\"end\":39415,\"start\":39052},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":502946},\"end\":39997,\"start\":39417},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3195114},\"end\":40303,\"start\":39999},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":102496818},\"end\":40782,\"start\":40305},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2255738},\"end\":41145,\"start\":40784},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7110290},\"end\":41438,\"start\":41147},{\"attributes\":{\"id\":\"b7\"},\"end\":41644,\"start\":41440},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14547347},\"end\":41925,\"start\":41646},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206943983},\"end\":42294,\"start\":41927},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":299085},\"end\":42644,\"start\":42296},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9455111},\"end\":42934,\"start\":42646},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6724907},\"end\":43282,\"start\":42936},{\"attributes\":{\"doi\":\"arXiv:1609.03677\",\"id\":\"b13\"},\"end\":43607,\"start\":43284},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206850587},\"end\":44018,\"start\":43609},{\"attributes\":{\"id\":\"b15\"},\"end\":44223,\"start\":44020},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206594692},\"end\":44628,\"start\":44225},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5849243},\"end\":45013,\"start\":44630},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":834882},\"end\":45247,\"start\":45015},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3759573},\"end\":45680,\"start\":45249},{\"attributes\":{\"id\":\"b20\"},\"end\":45959,\"start\":45682},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b21\"},\"end\":46158,\"start\":45961},{\"attributes\":{\"doi\":\"arXiv:1702.02706\",\"id\":\"b22\"},\"end\":46457,\"start\":46160},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11091110},\"end\":46761,\"start\":46459},{\"attributes\":{\"id\":\"b24\"},\"end\":46869,\"start\":46763},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206592782},\"end\":47454,\"start\":46871},{\"attributes\":{\"doi\":\"arXiv:1709.06841\",\"id\":\"b26\"},\"end\":47745,\"start\":47456},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1629541},\"end\":48176,\"start\":47747},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206594275},\"end\":48791,\"start\":48178},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12751695},\"end\":49171,\"start\":48793},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206775100},\"end\":49465,\"start\":49173},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206775640},\"end\":49782,\"start\":49467},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1336659},\"end\":50135,\"start\":49784},{\"attributes\":{\"doi\":\"10.23915/distill.00003\",\"id\":\"b33\",\"matched_paper_id\":64200748},\"end\":50418,\"start\":50137},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":31762881},\"end\":50881,\"start\":50420},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":9632354},\"end\":51276,\"start\":50883},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10748875},\"end\":51565,\"start\":51278},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":9307872},\"end\":51819,\"start\":51567},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206942855},\"end\":52246,\"start\":51821},{\"attributes\":{\"doi\":\"arXiv:1704.03489\",\"id\":\"b39\"},\"end\":52551,\"start\":52248},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":8515741},\"end\":52939,\"start\":52553},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":9114952},\"end\":53393,\"start\":52941},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":207761262},\"end\":53749,\"start\":53395},{\"attributes\":{\"id\":\"b43\"},\"end\":54069,\"start\":53751},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":5458522},\"end\":54446,\"start\":54071},{\"attributes\":{\"doi\":\"10.1109/LRA.2018.2846813\",\"id\":\"b45\",\"matched_paper_id\":6761996},\"end\":54921,\"start\":54448},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":8176169},\"end\":55458,\"start\":54923},{\"attributes\":{\"id\":\"b47\"},\"end\":55732,\"start\":55460},{\"attributes\":{\"doi\":\"arXiv:1709.00930\",\"id\":\"b48\"},\"end\":56003,\"start\":55734},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":11977588},\"end\":56256,\"start\":56005}]", "bib_title": "[{\"end\":38727,\"start\":38674},{\"end\":39478,\"start\":39417},{\"end\":40062,\"start\":39999},{\"end\":40411,\"start\":40305},{\"end\":40857,\"start\":40784},{\"end\":41196,\"start\":41147},{\"end\":41689,\"start\":41646},{\"end\":41970,\"start\":41927},{\"end\":42369,\"start\":42296},{\"end\":42686,\"start\":42646},{\"end\":43005,\"start\":42936},{\"end\":43674,\"start\":43609},{\"end\":44269,\"start\":44225},{\"end\":44696,\"start\":44630},{\"end\":45037,\"start\":45015},{\"end\":45317,\"start\":45249},{\"end\":46525,\"start\":46459},{\"end\":46984,\"start\":46871},{\"end\":47801,\"start\":47747},{\"end\":48280,\"start\":48178},{\"end\":48867,\"start\":48793},{\"end\":49229,\"start\":49173},{\"end\":49545,\"start\":49467},{\"end\":49829,\"start\":49784},{\"end\":50177,\"start\":50137},{\"end\":50506,\"start\":50420},{\"end\":50949,\"start\":50883},{\"end\":51321,\"start\":51278},{\"end\":51611,\"start\":51567},{\"end\":51873,\"start\":51821},{\"end\":52626,\"start\":52553},{\"end\":53033,\"start\":52941},{\"end\":53467,\"start\":53395},{\"end\":54160,\"start\":54071},{\"end\":54552,\"start\":54448},{\"end\":55027,\"start\":54923},{\"end\":56061,\"start\":56005}]", "bib_author": "[{\"end\":38738,\"start\":38729},{\"end\":38748,\"start\":38738},{\"end\":38756,\"start\":38748},{\"end\":38764,\"start\":38756},{\"end\":38773,\"start\":38764},{\"end\":38781,\"start\":38773},{\"end\":38790,\"start\":38781},{\"end\":38802,\"start\":38790},{\"end\":38812,\"start\":38802},{\"end\":38821,\"start\":38812},{\"end\":39141,\"start\":39130},{\"end\":39155,\"start\":39141},{\"end\":39164,\"start\":39155},{\"end\":39179,\"start\":39164},{\"end\":39192,\"start\":39179},{\"end\":39490,\"start\":39480},{\"end\":39499,\"start\":39490},{\"end\":39508,\"start\":39499},{\"end\":39519,\"start\":39508},{\"end\":39532,\"start\":39519},{\"end\":39544,\"start\":39532},{\"end\":39554,\"start\":39544},{\"end\":39562,\"start\":39554},{\"end\":39573,\"start\":39562},{\"end\":40072,\"start\":40064},{\"end\":40084,\"start\":40072},{\"end\":40095,\"start\":40084},{\"end\":40422,\"start\":40413},{\"end\":40432,\"start\":40422},{\"end\":40868,\"start\":40859},{\"end\":40879,\"start\":40868},{\"end\":40889,\"start\":40879},{\"end\":41207,\"start\":41198},{\"end\":41216,\"start\":41207},{\"end\":41227,\"start\":41216},{\"end\":41473,\"start\":41464},{\"end\":41483,\"start\":41473},{\"end\":41494,\"start\":41483},{\"end\":41700,\"start\":41691},{\"end\":41710,\"start\":41700},{\"end\":41721,\"start\":41710},{\"end\":41981,\"start\":41972},{\"end\":41993,\"start\":41981},{\"end\":42004,\"start\":41993},{\"end\":42379,\"start\":42371},{\"end\":42391,\"start\":42379},{\"end\":42399,\"start\":42391},{\"end\":42698,\"start\":42688},{\"end\":42706,\"start\":42698},{\"end\":42717,\"start\":42706},{\"end\":42728,\"start\":42717},{\"end\":43017,\"start\":43007},{\"end\":43025,\"start\":43017},{\"end\":43036,\"start\":43025},{\"end\":43363,\"start\":43353},{\"end\":43376,\"start\":43363},{\"end\":43389,\"start\":43376},{\"end\":43685,\"start\":43676},{\"end\":43695,\"start\":43685},{\"end\":43707,\"start\":43695},{\"end\":43720,\"start\":43707},{\"end\":44074,\"start\":44063},{\"end\":44087,\"start\":44074},{\"end\":44277,\"start\":44271},{\"end\":44286,\"start\":44277},{\"end\":44293,\"start\":44286},{\"end\":44300,\"start\":44293},{\"end\":44707,\"start\":44698},{\"end\":44716,\"start\":44707},{\"end\":44726,\"start\":44716},{\"end\":44735,\"start\":44726},{\"end\":45048,\"start\":45039},{\"end\":45059,\"start\":45048},{\"end\":45069,\"start\":45059},{\"end\":45326,\"start\":45319},{\"end\":45335,\"start\":45326},{\"end\":45345,\"start\":45335},{\"end\":45355,\"start\":45345},{\"end\":45370,\"start\":45355},{\"end\":45378,\"start\":45370},{\"end\":45695,\"start\":45682},{\"end\":45707,\"start\":45695},{\"end\":45720,\"start\":45707},{\"end\":46017,\"start\":46005},{\"end\":46023,\"start\":46017},{\"end\":46240,\"start\":46226},{\"end\":46252,\"start\":46240},{\"end\":46261,\"start\":46252},{\"end\":46536,\"start\":46527},{\"end\":46549,\"start\":46536},{\"end\":46564,\"start\":46549},{\"end\":46575,\"start\":46564},{\"end\":46584,\"start\":46575},{\"end\":46992,\"start\":46986},{\"end\":47000,\"start\":46992},{\"end\":47007,\"start\":47000},{\"end\":47025,\"start\":47007},{\"end\":47031,\"start\":47025},{\"end\":47534,\"start\":47528},{\"end\":47542,\"start\":47534},{\"end\":47550,\"start\":47542},{\"end\":47556,\"start\":47550},{\"end\":47811,\"start\":47803},{\"end\":47824,\"start\":47811},{\"end\":47835,\"start\":47824},{\"end\":48291,\"start\":48282},{\"end\":48298,\"start\":48291},{\"end\":48309,\"start\":48298},{\"end\":48320,\"start\":48309},{\"end\":48331,\"start\":48320},{\"end\":48346,\"start\":48331},{\"end\":48354,\"start\":48346},{\"end\":48883,\"start\":48869},{\"end\":48900,\"start\":48883},{\"end\":49244,\"start\":49231},{\"end\":49259,\"start\":49244},{\"end\":49271,\"start\":49259},{\"end\":49560,\"start\":49547},{\"end\":49572,\"start\":49560},{\"end\":49845,\"start\":49831},{\"end\":49860,\"start\":49845},{\"end\":49873,\"start\":49860},{\"end\":50188,\"start\":50179},{\"end\":50200,\"start\":50188},{\"end\":50208,\"start\":50200},{\"end\":50516,\"start\":50508},{\"end\":50523,\"start\":50516},{\"end\":50530,\"start\":50523},{\"end\":50538,\"start\":50530},{\"end\":50545,\"start\":50538},{\"end\":50962,\"start\":50951},{\"end\":50973,\"start\":50962},{\"end\":50987,\"start\":50973},{\"end\":51333,\"start\":51323},{\"end\":51344,\"start\":51333},{\"end\":51352,\"start\":51344},{\"end\":51625,\"start\":51613},{\"end\":51636,\"start\":51625},{\"end\":51649,\"start\":51636},{\"end\":51884,\"start\":51875},{\"end\":51897,\"start\":51884},{\"end\":51907,\"start\":51897},{\"end\":51918,\"start\":51907},{\"end\":51929,\"start\":51918},{\"end\":52330,\"start\":52320},{\"end\":52341,\"start\":52330},{\"end\":52350,\"start\":52341},{\"end\":52359,\"start\":52350},{\"end\":52636,\"start\":52628},{\"end\":52648,\"start\":52636},{\"end\":52659,\"start\":52648},{\"end\":53043,\"start\":53035},{\"end\":53052,\"start\":53043},{\"end\":53059,\"start\":53052},{\"end\":53070,\"start\":53059},{\"end\":53477,\"start\":53469},{\"end\":53488,\"start\":53477},{\"end\":53500,\"start\":53488},{\"end\":53516,\"start\":53500},{\"end\":53830,\"start\":53818},{\"end\":53838,\"start\":53830},{\"end\":53846,\"start\":53838},{\"end\":53860,\"start\":53846},{\"end\":54169,\"start\":54162},{\"end\":54181,\"start\":54169},{\"end\":54192,\"start\":54181},{\"end\":54562,\"start\":54554},{\"end\":54570,\"start\":54562},{\"end\":54577,\"start\":54570},{\"end\":54588,\"start\":54577},{\"end\":55036,\"start\":55029},{\"end\":55044,\"start\":55036},{\"end\":55050,\"start\":55044},{\"end\":55058,\"start\":55050},{\"end\":55468,\"start\":55460},{\"end\":55477,\"start\":55468},{\"end\":55487,\"start\":55477},{\"end\":55496,\"start\":55487},{\"end\":55816,\"start\":55807},{\"end\":55823,\"start\":55816},{\"end\":55829,\"start\":55823},{\"end\":56071,\"start\":56063},{\"end\":56080,\"start\":56071},{\"end\":56091,\"start\":56080},{\"end\":56101,\"start\":56091}]", "bib_venue": "[{\"end\":38829,\"start\":38821},{\"end\":39128,\"start\":39052},{\"end\":39651,\"start\":39573},{\"end\":40132,\"start\":40095},{\"end\":40499,\"start\":40432},{\"end\":40938,\"start\":40889},{\"end\":41281,\"start\":41227},{\"end\":41462,\"start\":41440},{\"end\":41759,\"start\":41721},{\"end\":42084,\"start\":42004},{\"end\":42437,\"start\":42399},{\"end\":42777,\"start\":42728},{\"end\":43096,\"start\":43036},{\"end\":43351,\"start\":43284},{\"end\":43789,\"start\":43720},{\"end\":44061,\"start\":44020},{\"end\":44377,\"start\":44300},{\"end\":44796,\"start\":44735},{\"end\":45103,\"start\":45069},{\"end\":45443,\"start\":45378},{\"end\":45769,\"start\":45720},{\"end\":46003,\"start\":45961},{\"end\":46224,\"start\":46160},{\"end\":46599,\"start\":46584},{\"end\":46797,\"start\":46763},{\"end\":47108,\"start\":47031},{\"end\":47526,\"start\":47456},{\"end\":47912,\"start\":47835},{\"end\":48431,\"start\":48354},{\"end\":48932,\"start\":48900},{\"end\":49300,\"start\":49271},{\"end\":49601,\"start\":49572},{\"end\":49934,\"start\":49873},{\"end\":50237,\"start\":50230},{\"end\":50624,\"start\":50545},{\"end\":51056,\"start\":50987},{\"end\":51401,\"start\":51352},{\"end\":51681,\"start\":51649},{\"end\":52009,\"start\":51929},{\"end\":52318,\"start\":52248},{\"end\":52709,\"start\":52659},{\"end\":53107,\"start\":53070},{\"end\":53553,\"start\":53516},{\"end\":53816,\"start\":53751},{\"end\":54230,\"start\":54192},{\"end\":54655,\"start\":54612},{\"end\":55135,\"start\":55058},{\"end\":55584,\"start\":55496},{\"end\":55805,\"start\":55734},{\"end\":56105,\"start\":56101},{\"end\":39725,\"start\":39653},{\"end\":40553,\"start\":40501},{\"end\":44441,\"start\":44379},{\"end\":47172,\"start\":47110},{\"end\":47976,\"start\":47914},{\"end\":48495,\"start\":48433},{\"end\":52724,\"start\":52711},{\"end\":55199,\"start\":55137}]"}}}, "year": 2023, "month": 12, "day": 17}