{"id": 256826851, "updated": "2023-10-06 08:39:24.127", "metadata": {"title": "Developing an Effective and Automated Patient Engagement Estimator for Telehealth: A Machine Learning Approach", "authors": "[{\"first\":\"Pooja\",\"last\":\"Guhan\",\"middle\":[]},{\"first\":\"Naman\",\"last\":\"Awasthi\",\"middle\":[]},{\"first\":\"and\",\"last\":\"McDonald\",\"middle\":[\"Kathryn\"]},{\"first\":\"Kristin\",\"last\":\"Bussell\",\"middle\":[]},{\"first\":\"Dinesh\",\"last\":\"Manocha\",\"middle\":[]},{\"first\":\"Gloria\",\"last\":\"Reeves\",\"middle\":[]},{\"first\":\"Aniket\",\"last\":\"Bera\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We discuss MET, a learning-based algorithm proposed for perceiving a patient's level of engagement during telehealth sessions. We leverage latent vectors corresponding to Affective and Cognitive features frequently used in psychology literature to understand a person's level of engagement in a semi-supervised GAN-based framework. We showcase the efficacy of this method from the perspective of mental health and more specifically how this can be leveraged for a better understanding of patient engagement during telemental health sessions. To further the development of similar technologies that can be useful for telehealth, we also plan to release a dataset MEDICA containing 1299 video clips, each 3 seconds long and show experiments on the same. Our framework reports a 40% improvement in RMSE (Root Mean Squared Error) over state-of-the-art methods for engagement estimation. In our real-world tests, we also observed positive correlations between the working alliance inventory scores reported by psychotherapists. This indicates the potential of the proposed model to present patient engagement estimations that aligns well with the engagement measures used by psychotherapists.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2011.08690", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": null}}, "content": {"source": {"pdf_hash": "1701c4641299fac565bf9b20485d94c7d6f6c6fe", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2011.08690v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "717e217ea70088ecefaa1c9eaf00b3f28552d764", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1701c4641299fac565bf9b20485d94c7d6f6c6fe.txt", "contents": "\nDeveloping an Effective and Automated Patient Engagement Estimator for Telehealth: A Machine Learning Approach\n\n\nPooja Guhan pguhan@umd.edu \nUniversity of Maryland College Park\n\n\nNaman Awasthi nawasthi@umd.edu \nUniversity of Maryland College Park\n\n\nKathryn Mcdonald kmcdonald@som.umaryland.edu \nUniversity of Maryland College Park\n\n\nKristin Bussell kristin.bussell@som.umaryland.edu \nUniversity of Maryland\nBaltimore County\n\nGloria Reeves greeves@som.umaryland.edu \nUniversity of Maryland\nBaltimore County\n\nDinesh Manocha dmanocha@umd.edu \nUniversity of Maryland College Park\n\n\nAniket Bera \nPurdue University\nWest LafayetteUSA\n\nDeveloping an Effective and Automated Patient Engagement Estimator for Telehealth: A Machine Learning Approach\n\nAbstractBackground: Patient engagement is a critical but challenging public health priority in behavioral healthcare. During telehealth sessions, healthcare providers need to rely more on verbal strategies than typical non-verbal cues to engage patients. Hence, the typical patient engagement behaviors are now different, and provider training on telehealth patient engagement is unavailable or quite limited. Therefore, we explore the application of machine learning for estimating patient engagement to assist psychotherapists in better diagnosis of mental disorders during telemental health sessions.Objective: The objective of this study was to examine the ability of machine learning models to estimate patient engagement levels during a telemental health session and understand whether the machine learning approach could support mental disorder diagnosis by psychotherapists.Methods:We propose a multimodal learning-based framework, MET. We uniquely leverage latent vectors corresponding to Affective and Cognitive features frequently used in psychology literature to understand a person's level of engagement. Given the labeled data constraints that exist in healthcare, we explore a semi-supervised solution using GANs. To further the development of similar technologies that can be useful for telehealth, we also plan to release a dataset MEDICA containing 1299 video clips, each 3 seconds long and show experiments on the same. The efficacy of our method is also demonstrated through real-world experiments.Results: Our framework reports a 40% improvement in RMSE (Root Mean Squared Error) over state-of-the-art methods for engagement estimation. In our real-world tests, we also observed positive correlations between the working alliance inventory scores reported by psychotherapists. This indicates the potential of the proposed model to present patient engagement estimations that aligns well with the engagement measures used by psychotherapists.Conclusion:The performance of the framework described here has been compared against other existing engagement detection machine learning models. We also validated the model using a limited sample of real-world data. Patient engagement in literature has been identified to be important to improve therapeutic alliance. But little research has been undertaken to measure it in a telehealth setting wherein the conventional cues are not available to the therapist to take a confident decision. The framework developed is an attempt to model person-oriented engagement modeling theories within machine learning frameworks to estimate the level of engagement of the patient accurately and reliably in telehealth. The results are encouraging and emphasize the value of combining psychology and machine learning to understand patient engagement. Further testing 1 arXiv:2011.08690v4 [cs.CV] 13 Feb 2023 in actual telehealth settings is necessary to fully assess its usefulness in helping therapists gauge patient engagement during virtual sessions. However, the proposed approach and the creation of the new dataset, MEDICA, opens avenues for future research and development of impactful tools for telehealth.\n\nIntroduction\n\n\nOverview\n\nThe World Health Organization defines mental health as \"a state of well-being\" that allows a person to lead a fulfilling and productive life and contribute to society [1]. With increasing stress and pressure leading to poor mental health, improved telemental healthcare is becoming a need of the hour as they serve as an effective way to get access to mental health services and treatment in all countries and cultures across the globe. [2] estimated that one-fourth of the adult population is affected by some kind of mental disorder. However, there are only approximately 9 psychiatrists per 100,000 people in developed countries and only around 0.1 for every 1,000,000 in lower-income countries [3,4]. Therefore, it is not surprising that there has been an upward trend in the demand for telemental health (the process of providing psychotherapy remotely, typically utilizing HIPAA-compliant video conferencing) [5] to address the chronic shortage of psychotherapists. These services eliminate some practical barriers to care (e.g., transportation), are affordable, and give access to an actual therapist. Despite these undeniable benefits, this emerging treatment modality raises new challenges in patient engagement compared to in-person care. By engagement, we refer to the connection between a therapist and patient that includes a sense of basic trust and willingness/interest to collaborate which is essential for the therapeutic process. Patient engagement is a critical but challenging public health priority in behavioral health care. There are no objective measurements of patient engagement in behavioral health care. Measurement of engagement is most commonly assessed by patient reports, which may be prone to response bias, and the variable use of different questionnaires makes it challenging to compare patient engagement across different health systems. Behavioral health services often require more frequent appointments than other specialties to promote behavior change, so maintaining a positive relationship with a provider is essential for evidence-based care. However, patient engagement is not routinely or systematically measured in healthcare settings. Health systems often use \"show rate\" and \"patient satisfaction\" as a proxy for engagement, but these terms do not necessarily reflect provider-patient alliance in treatment.\n\nIn telehealth appointments, therapists have limited visual data (e.g. the therapist can only view the patient's face rather than their full body). They must rely more on verbal strategies to engage patients than in-person care since they cannot use typical non-verbal cues to convey interest and be responsive to the patient (e.g., handshake at the beginning of a session, adjusting the distance between the patient and provider by moving a chair closer or further away, observing a patient's response to questions while maintaining eye contact). It is also more difficult for therapists to convey attentiveness since eye contact requires the therapist to look at a camera rather than observing or looking at a person. Additionally, provider training on telehealth patient engagement is quite limited. Providers are currently implementing telehealth services without having clear guidance on how to improve or measure patient telehealth engagement. For example, the abrupt transition to virtual care to prevent COVID-19 transmission did not allow providers to receive training on the use of technology-based care beyond basic orientation to web-based platforms.\n\nThus, systems that can provide feedback on engagement, using multi-modalities of data, have the potential to improve therapeutic outcomes. Engagement is critical for both retention in care as well as the accuracy of diagnoses. These two factors are potential targets to enhance the quality of technology-delivered care. Therefore, developing a system that can provide feedback on engagement using multimodal data has the potential to improve therapeutic outcomes while performing telemental health.\n\n\nResearch Background\n\nPatient engagement has been established as one of the critical indicators of a successful therapy session. The existing literature in this space largely explores ways of improving it. However, methods to measure or quantify the levels of patient engagement, especially in telehealth settings remain largely unexplored. Some of the prior works in the realm of engagement detection consider using just facial expressions [6,7], speech [8], body posture [9], gaze direction [10] and head pose [11] have been used as single modalities for detecting engagement. Combining different modalities has been observed to improve engagement detection accuracy [12,13,14]. [15] proposed a multimodal framework to detect the level of engagement of participants during project meetings in a work environment. The authors expanded the work of Stanford's PBL Labs, eRing [16], by including information streams such as facial expressions, voice, and other biometric data. [17] proposed an approach to detect engagement levels in students during a writing task by not only making use of facial features but also features obtained from remote video-based detection of heart rate. The dataset used was generated by the authors, and they used self-reports instead of external annotation for classification purposes. [18] make use of facial expressions as well as body posture for detecting engagement in learners. [19] proposes the use of audio, facial, and body pose features to detect engagement and disengagement for an imbalanced in-the-wild dataset.\n\nDespite the existence of a variety of such algorithms to perform engagement detection, the results obtained from these approaches (especially single modality based) could be misleading in a telehealth setting due to factors like camera position, resistant or guarded clients etc. The multimodal architectures have atleast one modality that requires data which cannot be reliably represented or collected. For instance, in video conference calls, it is difficult to get biometric data such as heart rate and observe the body posture of the person. Therefore, we eliminate all these uncertainties by proposing a framework that needs only face visuals, audio and text data. Additionally, unlike other approaches, we leverage theories in psychology to develop our model design.\n\n\nMethod\n\n\nProposed Model Design\n\nSince the patient population is individuals with mental illness, we used psychology and psychiatry literature to build our algorithm so that the recognition and understanding of engagement are as close as possible to a psychotherapist's method of engagement evaluation during a session. We, therefore, take a multi-componential approach and propose a framework MET(Multimodal Perception of Engagement for Telehealth) that estimates the engagement levels of the patient in terms of their affective and cognitive states. These modes (affective and cognitive) are basically the categories of the different cues used by mental health therapists to assess their patients. Additionally, since the extent to which patients remain engaged during the telehealth session is temporal in nature, we are interested in analyzing it across micro-level time scales in the range of a few seconds. These characteristics of our approach align perfectly with the person-oriented analysis discussed by [20].\n\n\nCognitive State Mode\n\nThe Cognitive state involves comprehending complex concepts and issues and acquiring difficult skills. It conveys deep (rather than surface-level) processing of information whereby the person gains a critical or higher-order understanding of the subject matter and solves challenging problems.\n\nPsychotherapists usually measure and evaluate the cognitive state of the person using neuropsychological exams that are typically conducted via in-person interviews or self-evaluations to gauge memory, thinking, and the extent of understanding of the topic of discussion. There has been a lot of work around determining biomarkers for detecting signs of a person's cognitive state. However, these methods are either offline or fail to consider various essential perceptual indicators. Recently, there has been a lot of work around using speech as a potential biomarker for detecting cognitive decline. For instance, stress negatively affects the cognitive functions of a person, and this can be easily detected using speech signals. Moreover, speech-based methods are attractive because they are non-intrusive, inexpensive, and can potentially be real-time. The following 4 audio features have proven to be extremely useful for checking signs of cognitive impairment and are increasingly being used to detect conditions such as Alzheimer's and Parkinson's:-1. Glottal features (f g ) help in characterizing speech under stress. During periods of stress, there is an aberration in the amount of tension applied in the opening (abduction) and closing (adduction) of the vocal cords.\n\n\n2.\n\nProsody features (f pr ) characterize the speaker's intonation and speaking styles. Under this feature, we analyze variables like timing, intonation, and loudness during the production of speech.\n\n3. Phonation (f ph ) in people with cognitive decline is characterized by bowing and inadequate closure of vocal cords, which produce problems in stability and periodicity of the vibration. They are analyzed in terms of features related to perturbation measures such as jitter (temporal perturbations of the fundamental frequency), shimmer (temporal perturbation of the amplitude of the signal), amplitude perturbation quotient (APQ) and pitch perturbation quotient (PPQ). Apart from these, the degree of unvoiced is also included.\n\n4\n\n. Articulation (f ar ) is related to reduced amplitude and velocity of lip, tongue, and jaw movements. The analysis is based primarily on the first two vocal formants F1 and F2.\n\nWe, therefore, define features corresponding to cognitive state as the concatenation of these 4 audio features. Therefore, cognitive state features h c = concat(f g , f pr , f ph , f ar )\n\n\nAffective State Mode\n\nThe Affective State encompasses affective reactions such as excitement, boredom, curiosity, and anger. The range of affective expressions will vary based on individual demographic factors (e.g., age), cultural backgrounds/norms, and mental health symptoms. In order to understand the affective state, we check if there exists any inconsistency between the emotions perceived and the statement the person made. [21,22] suggest that when different modalities are modeled and projected onto a common space, they should point to similar affective cues; otherwise, the incongruity suggests distraction, deception, etc. In other words, if E1, E2, and E3 represent the emotions perceived individually from what the patient said (text), the way they said it or sounded (audio) and how they looked/expressed (visuals) respectively, then the patient would be considered engaged if E1, E2, and E3 are similar otherwise they are disengaged. Therefore, we adopt pretrained emotion recognition models to extract affective features corresponding to audio, visuals and text from each video sample separately:\n\n1. Audio (f a ): Mel-frequency cepstrum (MFCC) features were extracted from the audio clips available in the data. The affective features were extracted using an MLP network that has been trained for emotion recognition in speech using the data available in the CREMA-D dataset. A feature vector was obtained corresponding to each audio clip.\n\n2. Visuals (f v ): The VGG-B architecture suggested in [23] was used to extract affective features from the video frames. The output dimensions of the second last layer were modified to give a feature vector of length 100.\n\n3. Text (f t ): We extract affect features from the text using a bert-based model that has been trained network on GoEmotions dataset.\n\nWe, therefore, represent the affective state of the patient as a concatenation of f a , f v , and f t .\nHence, affective state features h a = concat(f a , f v , f t )\n\nLearning Network\n\nObtaining a large amount of high-quality labeled data to train a robust model for predicting patient engagement is inevitably laborious and requires expert medical knowledge. Considering that unlabeled data is relatively easy to collect, we propose a semi-supervised learning-based solution. Semi-supervised learning (SSL) enables us to deploy machine learning systems in real-life applications (e.g., image search [24], speech analysis [25,26], natural language processing) where we have few labeled data samples and a lot of unlabeled data. There have also been some prior works that explore SSL to do engagement detection in non-medical domains. One of the earliest works in this direction includes [27] where they consider the development of an engagement detection system, more specifically emotional or affective engagement of the student in a semi-supervised fashion to personalize systems like Intelligent Tutoring Systems according to their needs. [28] conducted experiments to detect user engagement using a facial feature based semi-supervised model. Most state-of-the-art SSL methods use Generative Adversarial Nets (GANs) [29]. GANs are a class of machine learning models and typically have two neural networks competing with each other to generate more accurate predictions. These two neural networks are referred to as the generator and the discriminator. The generator's goal is to artificially manufacture outputs that could easily be mistaken as real data. The goal of the discriminator is to identify the real from the artificially generated data. In trying to generate high-quality outputs, the generator learns to capture the different possible variations in the input variables and therefore, the data manifold well. This is extremely helpful when we may not be able to access data containing a wide variety of similar engagement-related cues visible across different patients. We use a multimodal semisupervised GAN-based network architecture to regress the values of an engagement corresponding to each feature tuple h T . This improves our model's generalizability and makes it more robust than the previously defined semi-supervised learning approaches. The network is similar to the semi-supervision framework SR-GAN proposed by [30]. The main distinction as discussed is that unlike the original model, we train the generator to model the feature maps generated by the Cognitive and Affective state modules. The discriminator needs to distinguish between the true (labeled and unlabeled) feature maps with the corresponding fake feature maps and gives an estimate for engagement. However, we develop a generator to model the feature maps generated by Cognitive and Affective state modules (h T ). 4 loss functions are used to train this network -L lab , L un , L f ake , L gen . We also make use of a gradient penalty (P) to keep the gradient of the discriminator in check which helps convergence. The gradient penalty is calculated with respect to a randomly chosen point on the convex manifold connecting the unlabeled samples to the fake samples. The overall loss function used for training the network is L = L lab + L un + L f ake + L gen + \u03bbP (1)  \n\n\nDatasets\n\n\nMultimodal Engagement Detection in Clinical Analysis (MEDICA)\n\nEngagement is an overloaded term, and the definition varies with the application, making it difficult and expensive to collect, annotate and analyze such data. As a result, we find too few multimodalbased engagement detection datasets currently available for us to use. Our problem statement revolves specifically around detecting patient engagement during a telemental health session. In such a setting, the only information we can work with includes the patient's face and speech (audio and text). There exist datasets like CMU-MOSI [31], CMU-MOSEI [32], and SEND [33] that capture such settings. However, they are not specifically for engagement detection. Given the lack of a dataset that allows researchers to use multimodal features (video, text, and audio) for engagement, we propose MEDICA, a novel dataset developed specifically to cater to engagement detection using telemental health session videos. To use this data to address a broader range of issues related to mental health, we also include labels pertaining to stress and emotions. According to the author's knowledge, this dataset is one of the first publicly available datasets that caters specifically to multimodal research in patient engagement in mental health. Table 1 presents a comparison between MEDICA and other related datasets. Despite the rise in telehealth services and poor mental health patient-to-therapist ratios, there are no datasets that even try modeling telehealth sessions to give the community an opportunity to innovate and develop new technologies. MEDICA is a humble attempt by us to kick-start interesting research opportunities Acquisition: MEDICA has been developed by collecting publicly available mock therapy session videos created by different psychiatry medical schools for training their students. The patients in these videos are being advised for depression, social anxiety, and PTSD. We have collected 13 videos, each having a duration of around 20mins-30mins. We limit the videos to the setup wherein both the therapist and the patient are not visible together in the same frame. Additionally, we also take only those videos where there is only one patient. Each video has a unique English-speaking patient.\n\nProcessing and Annotation Since our only focus was to create a dataset that depicted the behavior of mental health patients during their sessions, we considered only parts of the videos where we had only the patient visible in the frames, which were scattered across the video for different durations. We took these scattered clips and divided them into smaller clips of 3 seconds each, resulting in a dataset of size 1229. We use Moviepy and speech-recognition libraries to extract audio and text from the video clips. Each video was annotated for attentiveness, stress, and engagement, which were scored on a Likert scale of [-3, 3]; hesitation was a binary target variable (Yes or No). Humans tend to have multiple emotions with varying intensities while expressing their thoughts and feelings. Therefore, the videos have been labeled for 8 emotions related to mental health: happy, sad, irritated, neutral, anxious, embarrassed, scared, and surprised. This will enable us to develop systems capable of understanding the various interacting emotions of the users.  \n\n\nReal-World Data\n\nWe also wanted to test our methods in the real world and we collaborated with 8 child psychotherapists to do so. 20 caregivers voluntarily agreed to be part of this research after a psychotherapist explained to them its purpose and potential benefits, and that they could expect one or at max two of their telemental sessions to be recorded to test our proposed framework. They were also informed about the equipments that would be provided to them to ensure we get a clean recording. \"Clean\" refers to a recording executed with a camera of good quality with appropriate lighting conditions. The equipments mainly consisted of a smartphone having a good quality camera, a ring light with stand to ensure that the session was recorded in a well-lit environment, and internet connection to ensure that the session occurred smoothly without any network glitches. They were also given the assurance regarding preserving the confidentiality of the data being collected. The caregivers were informed that, during the video storage process, we would be \"de-identifying\" any facial images (using methods like blurring, etc) beside the caregivers, who may appear in the session video. We do this for two reasons. First, we are interested in only estimating the level of engagement of the caregivers and no other participant of the session. Second, the experiment is an agreement between only the caregiver and the therapist under the condition that no other person in the caregiver's family (including the child) will be analyzed. Efforts were also made to limit their personal information, including limiting the experiment evaluations and medical records to only those people who are part of the study. On average, each of these sessions lasted around 20mins. The demographics of the caregivers who participated in our real-world experiments appear in Table2. The entire data collection process can be divided into three parts:\n\n1. Pre-session: Before each telemental health session of a caregiver with their therapist, a research assistant helped the caregiver with set up the equipments to record their session. The assistant also ensured that the caregivers were comfortable using the equipment.\n\n2. During the session: We ensured that the telemental health session ran just as it would normally. After the pre-session process, the research assistant would log off. Therefore, during the session, it would be just the therapist and the caregiver having a conversation.\n\nNo one else from the study would be a part of it. The only thing different about this session was that the caregiver was being recorded using the smartphone given to them. We don't record the therapist. 3. Post-session: After the session was complete, a research assistant guided the participant regarding the steps to stop the recording and save the data collected.  After a telemental health session is complete, the therapists score the collaborative relationship (therapeutic alliance) that was established between them and the caregiver during the session. The quality of this therapeutic alliance is measured using the working alliance inventory (WAI). WAI was modeled on Bordin's theoretical work [39]. It captures 3 dimensions of the alliance -Bond, Task, and Goals. Extensive tests showed 12 items per dimension to be the minimum length for effective representations of the inventory. A composite score is computed based on these 12 items for each of the sessions conducted. Henceforth, we refer to this score as the WAI score.\n\n\nMeasure\n\n\nResults and Discussion\n\nMotivated by recent works in clinical psychotherapy [40], we use the standard evaluation metric of RMSE to evaluate our approach.\n\n\nStudy-1: Testing our proposed frameworks on MEDICA\n\nThe purpose of the first study is to demonstrate the ability of our model to estimate the level of engagement exhibited by the patient in the video. This study was performed on the MEDICA dataset. As our proposed methodology leverages a semi-supervised approach, we extract labeled samples from MEDICA and unlabeled samples from the MOSEI dataset. After preprocessing, we extract 12854 unlabeled data points from MOSEI. We split the 1299 labeled data points from MEDICA into 70:10:20 for training, validation, and testing respectively. Therefore, the split of the labeled training data to unlabeled training data points is 909:12854. We compare our model with the following SOTA methods for engagement detection.  [28] perform a semi-supervised engagement detection using a semi-supervised support vector machine.\n\nIn addition to being SOTA, these methods can be used in a telehealth setting like ours. We use the publicly available implementation for LBP-TOP [37] and train the entire model on MEDICA. S3VM [28] does not have a publicly available implementation. We reproduce the method to the best of our understanding.   Table 3 summarizes the RMSE values obtained for all the methods described above and ours. We observe an improvement of at least 40%. Our approach is one of the first methods of engagement estimation built on the principles of psychotherapy. The modules used, specifically cognitive and affective states help the overall framework to effectively mimic the ways a psychotherapist perceives the patient's level of engagement. Like psychotherapists, these modules also look for specific engagement-related cues exhibited by the patient in the video.\n\n\nStudy-2: Ablation Studies\n\nTo show the importance of the different components (Affective and Cognitive) used in our approach, we run our method on MEDICA by removing either one of the modules corresponding to affective or cognitive state and report our findings. Table 4 summarizes the results obtained from the ablation experiments. We can observe that the ablated frameworks (i.e. only using affective (A) or cognitive (C) modules) do not perform as well as when we have both modules available. In order to understand and verify the contribution of these modules further, we leveraged the other labels (stress, hesitation, and attention) available in MEDICA and performed regression tasks using our proposed architecture on all of them. We observed that mode C performs better when predicting stress and hesitation values. Mode A performed better in estimating a patient's level of attentiveness. These results agree with our understanding of cognitive state and affective state. Therefore, the combination of affective and cognitive state modes helps in efficiently predicting the engagement level of the patient.  \n\n\nStudy-3:Analysis on Real-World Data\n\nMET trained for estimating engagement levels was tested on the processed real-world data. WAI scoring is based on certain observations the therapist makes during the session with the patient. The score obtained from our model is different than that from WAI, but we claim that like WAI, our estimates also capture the engagement levels of the patient well. If this is indeed the case, then both WAI and our estimates should be correlated. As discussed earlier, a single WAI score is reported by the therapist (provider) for the entire session. Since our framework performs microanalysis, we have engagement level estimates available for many instances during the session. Therefore, to make our comparison meaningful, we took the mean of the estimates obtained from MET for each session. We then observed the correlation between the mean scores of WAI and MET for the sessions. Instead of just taking the mean, we also took the median of the engagement level estimates available at different instances of the sessions and checked for their correlation with the WAI scores. Additionally, to quantify the quality of our framework's ability to capture the behavior of WAI, we performed the same correlation experiments with the comparison methods, S3VM and LBP-TOP frameworks. Table 5 shows the results of our experiments. Clearly, as compared to prior methods, our framework has been able to better understand WAI patterns and showcases a positive correlation.  Table 5: Correlation comparisons between different patient engagement estimates obtained from different methods and WAI for real-world data.\n\n\nMethod\n\nThe conceptual model of MET is also supported by Bordin's 1979 theoretical work [39]. According to this theory, the therapist-provider alliance is driven by three factors -bond, agreement on goals, and agreement on tasks-and these factors fit nicely with the features identified in this work. While bond would correspond with affective, goals and task agreement correspond with cognitive. The merit of Bordin's approach is that it has been used for child therapy and adults, and it is one of the more widely studied therapeutic alliance measures. Therefore, it is no surprise that our framework can work well to provide an estimate of engagement levels in a telemental health session.\n\n\nConclusion\n\nTelehealth behavioral services that are delivered to homes via videoconferencing systems have become the most cost-effective, dependable, and secure option for mental health treatment, especially in recent times. Engagement is considered one of the key standards for mental health care. Given the difficulty in gauging the level of patient engagement during telehealth, an artificial intelligencebased approach has been shown to be promising for assisting psychotherapists. We propose MET, a novel multimodal semi-supervised GAN framework that leverages affective and cognitive features from the psychology literature to estimate useful psychological state indicators like engagement and valence-arousal of a person. The method makes it possible to use the modalities easily available during a video call, namely, visuals, audio, and text to understand the audience, their reactions, and actions better. This can in turn help us have better social interactions. To the best of our knowledge, we are the first ones to do so. MET can be an incredible asset for therapists during telemental health sessions. The lack of non-verbal cues and sensory data like heart rate makes it very difficult for them to make an accurate assessment of engagement (a critical mental health indicator). The lack of datasets has always been a big challenge to use AI to solve this and other mental-health-related tasks. Therefore, to promote better research opportunities, we release a new dataset for engagement detection in mental health patients called MEDICA. We show our model's usefulness on this as well as real-world data. As part of future work, we hope to build this dataset further to accommodate other related tasks apart from looking into possible kinds of variations arising due to cultural and geographical differences among patients and, therefore, making it more inclusive. Our work has some limitations and may not work well in case of occlusions, missing modality, and data corruptions due to low internet bandwidth. We plan to address this as part of future work. We would also like to explore making the predictions more explainable to enable psychotherapists to receive evidence-guided suggestions to make their final decisions.\n\n1 .\n1Labeled Loss (L lab ) : Mean squared error of model output (\u0177 t ) with ground truth (y t ).2. Unlabeled Loss (L un ): Minimize the distance between the unlabeled and labeled dataset's feature space. 3. Fake Loss (L f ake ): Maximize the distance between unlabeled dataset's features with respect to fake images. 4. Generator Loss (L gen ): Minimize the distance between the feature space of fake and unlabeled data\n\nFigure 1 :\n1Overall block diagram of the proposed architecture. TASK here refers to the patient engagement estimation.\n\nFigure 2 :\n2Examples from the MEDICA dataset created for mental health research. This dataset has been created using publicly available videos that are usually used for training purposes by different medical schools.\n\nFigure 3 :\n3A few frames from the real-world videos we collected. The faces have been blurred here to protect the identity of the patients. However, the consent of the patients was taken to use their unblurred faces as input to MET.\n\nTable 1 :\n1Comparison of the MEDICA dataset with other related datasets. Modes indicate the subset of modalities present from(v)visual,(a)audio,(t)text. *: Current status of the dataset. The size of the dataset will be increased.\n\nTable 2 :\n2Demographic information for caregivers participating in the experiment\n\nTable 3 :\n3Comparisons on MEDICA Dataset\n\n\nModality RMSE for Engage RMSE for Stress RMSE for Hesitate RMSE for AttentionA \n0.24 \n0.15 \n0.146 \n0.07 \nC \n0.3 \n0.13 \n0.16 \n0.08 \nA & C \n0.10 \n0.12 \n0.14 \n0.1 \n\n\n\nTable 4 :\n4Ablation Experiments on MEDICA Dataset. We refer to Affective state mode by A and Cognitive state mode by C.\n\nWHO, WHO highlights urgent need to transform mental health and mental health care. WHO, WHO highlights urgent need to transform mental health and mental health care, 2022.\n\nThe World Health Report 2001: Mental health: new understanding, new hope. World Health Organization. W H Organization, W. H. Organization, The World Health Report 2001: Mental health: new understanding, new hope. World Health Organization, 2001.\n\nDisability-adjusted life years (dalys) for 291 diseases and injuries in 21 regions, 1990-2010: a systematic analysis for the global burden of disease study 2010. C J Murray, T Vos, R Lozano, M Naghavi, A D Flaxman, C Michaud, M Ezzati, K Shibuya, J A Salomon, S Abdalla, The lancet. 3809859C. J. Murray, T. Vos, R. Lozano, M. Naghavi, A. D. Flaxman, C. Michaud, M. Ezzati, K. Shibuya, J. A. Salomon, S. Abdalla, et al., \"Disability-adjusted life years (dalys) for 291 diseases and injuries in 21 regions, 1990-2010: a systematic analysis for the global burden of disease study 2010,\" The lancet, vol. 380, no. 9859, pp. 2197-2223, 2012.\n\nBrain drain: a challenge to global mental health. B D Oladeji, O Gureje, BJPsych international. 133B. D. Oladeji and O. Gureje, \"Brain drain: a challenge to global mental health,\" BJPsych international, vol. 13, no. 3, pp. 61-63, 2016.\n\n. Telemental Adaa, Health, ADAA, Telemental Health, 2020.\n\nThe faces of engagement: Automatic recognition of student engagementfrom facial expressions. J Whitehill, Z Serpell, Y.-C Lin, A Foster, J R Movellan, IEEE Transactions on Affective Computing. 51J. Whitehill, Z. Serpell, Y.-C. Lin, A. Foster, and J. R. Movellan, \"The faces of engagement: Automatic recognition of student engagementfrom facial expressions,\" IEEE Transactions on Affective Computing, vol. 5, no. 1, pp. 86-98, 2014.\n\nEngagement detection in e-learning environments using convolutional neural networks. M Murshed, M A A Dewan, F Lin, D Wen, 2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress. IEEEM. Murshed, M. A. A. Dewan, F. Lin, and D. Wen, \"Engagement detection in e-learning environments using convolutional neural networks,\" in 2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), pp. 80-86, IEEE, 2019.\n\nDetecting user engagement in everyday conversations. C Yu, P M Aoki, A Woodruff, cs/0410027arXiv preprintC. Yu, P. M. Aoki, and A. Woodruff, \"Detecting user engagement in everyday conversations,\" arXiv preprint cs/0410027, 2004.\n\nAutomatic analysis of affective postures and body motion to detect engagement with a game companion. J Sanghvi, G Castellano, I Leite, A Pereira, P W Mcowan, A Paiva, Proceedings of the 6th international conference on Human-robot interaction. the 6th international conference on Human-robot interactionJ. Sanghvi, G. Castellano, I. Leite, A. Pereira, P. W. McOwan, and A. Paiva, \"Automatic analysis of affective postures and body motion to detect engagement with a game companion,\" in Proceedings of the 6th international conference on Human-robot interaction, pp. 305-312, 2011.\n\nEstimating user's engagement from eye-gaze behaviors in humanagent conversations. Y I Nakano, R Ishii, Proceedings of the 15th international conference on Intelligent user interfaces. the 15th international conference on Intelligent user interfacesY. I. Nakano and R. Ishii, \"Estimating user's engagement from eye-gaze behaviors in human- agent conversations,\" in Proceedings of the 15th international conference on Intelligent user interfaces, pp. 139-148, 2010.\n\nStudent engagement detection using emotion analysis, eye tracking and head movement with machine learning. P Sharma, S Joshi, S Gautam, V Filipe, M J Reis, arXiv:1909.12913arXiv preprintP. Sharma, S. Joshi, S. Gautam, V. Filipe, and M. J. Reis, \"Student engagement detection us- ing emotion analysis, eye tracking and head movement with machine learning,\" arXiv preprint arXiv:1909.12913, 2019.\n\nMultimodal student engagement recognition in prosocial games. A Psaltis, K C Apostolakis, K Dimitropoulos, P Daras, IEEE Transactions on Games. 103A. Psaltis, K. C. Apostolakis, K. Dimitropoulos, and P. Daras, \"Multimodal student engage- ment recognition in prosocial games,\" IEEE Transactions on Games, vol. 10, no. 3, pp. 292- 303, 2017.\n\nEmbodied affect in tutorial dialogue: student gesture and posture. J F Grafsgaard, J B Wiggins, K E Boyer, E N Wiebe, J C Lester, International Conference on Artificial Intelligence in Education. SpringerJ. F. Grafsgaard, J. B. Wiggins, K. E. Boyer, E. N. Wiebe, and J. C. Lester, \"Embodied affect in tutorial dialogue: student gesture and posture,\" in International Conference on Artificial Intelligence in Education, pp. 1-10, Springer, 2013.\n\nLearner engagement measurement and classification in 1: 1 learning. S Aslan, Z Cataltepe, I Diner, O Dundar, A A Esme, R Ferens, G Kamhi, E Oktay, C Soysal, M Yener, 2014 13th International Conference on Machine Learning and Applications. IEEES. Aslan, Z. Cataltepe, I. Diner, O. Dundar, A. A. Esme, R. Ferens, G. Kamhi, E. Ok- tay, C. Soysal, and M. Yener, \"Learner engagement measurement and classification in 1: 1 learning,\" in 2014 13th International Conference on Machine Learning and Applications, pp. 545-552, IEEE, 2014.\n\nEngagement detection in meetings. M Frank, G Tofighi, H Gu, R Fruchter, arXiv:1608.08711arXiv preprintM. Frank, G. Tofighi, H. Gu, and R. Fruchter, \"Engagement detection in meetings,\" arXiv preprint arXiv:1608.08711, 2016.\n\nering: Body motion engagement detection and feedback in global teams. J Ma, R Fruchter, SAVI Symposium on New ways to teach and learn for student engagement. Stanford UniversityJ. Ma and R. Fruchter, \"ering: Body motion engagement detection and feedback in global teams,\" in SAVI Symposium on New ways to teach and learn for student engagement, Stanford University, 2015.\n\nAutomated detection of engagement using video-based estimation of facial expressions and heart rate. H Monkaresi, N Bosch, R A Calvo, S K D&apos;mello, IEEE Transactions on Affective Computing. 81H. Monkaresi, N. Bosch, R. A. Calvo, and S. K. D'Mello, \"Automated detection of engagement using video-based estimation of facial expressions and heart rate,\" IEEE Transactions on Affective Computing, vol. 8, no. 1, pp. 15-28, 2016.\n\nAn ensemble model using face and body tracking for engagement detection. C Chang, C Zhang, L Chen, Y Liu, Proceedings of the 20th ACM International Conference on Multimodal Interaction. the 20th ACM International Conference on Multimodal InteractionC. Chang, C. Zhang, L. Chen, and Y. Liu, \"An ensemble model using face and body track- ing for engagement detection,\" in Proceedings of the 20th ACM International Conference on Multimodal Interaction, pp. 616-622, 2018.\n\nMultimodal approach to engagement and disengagement detection with highly imbalanced in-the-wild data. D Fedotov, O Perepelkina, E Kazimirova, M Konstantinova, W Minker, Proceedings of the Workshop on Modeling Cognitive Processes from Multimodal Data. the Workshop on Modeling Cognitive Processes from Multimodal DataD. Fedotov, O. Perepelkina, E. Kazimirova, M. Konstantinova, and W. Minker, \"Multimodal approach to engagement and disengagement detection with highly imbalanced in-the-wild data,\" in Proceedings of the Workshop on Modeling Cognitive Processes from Multimodal Data, pp. 1-9, 2018.\n\nThe challenges of defining and measuring student engagement in science. G M Sinatra, B C Heddy, D Lombardi, G. M. Sinatra, B. C. Heddy, and D. Lombardi, \"The challenges of defining and measuring student engagement in science,\" 2015.\n\nEmotion analysis in man-machine interaction systems. T Balomenos, A Raouzaiou, S Ioannou, A Drosopoulos, K Karpouzis, S Kollias, International Workshop on Machine Learning for Multimodal Interaction. SpringerT. Balomenos, A. Raouzaiou, S. Ioannou, A. Drosopoulos, K. Karpouzis, and S. Kollias, \"Emotion analysis in man-machine interaction systems,\" in International Workshop on Ma- chine Learning for Multimodal Interaction, pp. 318-328, Springer, 2004.\n\nReading between the lies: Identifying concealed and falsified emotions in universal facial expressions. S Porter, L Ten Brinke, Psychological science. 195S. Porter and L. Ten Brinke, \"Reading between the lies: Identifying concealed and falsified emotions in universal facial expressions,\" Psychological science, vol. 19, no. 5, pp. 508-514, 2008.\n\n@articleadair2017attitude, title=Attitude-Scenario-Emotion (ASE) sentiments are superficial, author=Adair, Heather and Carruthers, Peter, journal=Behavioral and Brain Sciences, volume=40, year=2017. O Arriaga, M Valdenegro-Toro, P Pl\u00f6ger, arXiv:1710.07557publisher=Cambridge University PressarXiv preprintO. Arriaga, M. Valdenegro-Toro, and P. Pl\u00f6ger, \"@articleadair2017attitude, title=Attitude- Scenario-Emotion (ASE) sentiments are superficial, author=Adair, Heather and Carruthers, Peter, journal=Behavioral and Brain Sciences, volume=40, year=2017, publisher=Cambridge University Press ,\" arXiv preprint arXiv:1710.07557, 2017.\n\nSemi-supervised learning for image retrieval using support vector machines. K Lu, J Zhao, M Xia, J Zeng, International Symposium on Neural Networks. SpringerK. Lu, J. Zhao, M. Xia, and J. Zeng, \"Semi-supervised learning for image retrieval using support vector machines,\" in International Symposium on Neural Networks, pp. 677-681, Springer, 2005.\n\nActive learning and semi-supervised learning for speech recognition: A unified framework using the global entropy reduction maximization criterion. D Yu, B Varadarajan, L Deng, A Acero, Computer Speech & Language. 243D. Yu, B. Varadarajan, L. Deng, and A. Acero, \"Active learning and semi-supervised learning for speech recognition: A unified framework using the global entropy reduction maximization criterion,\" Computer Speech & Language, vol. 24, no. 3, pp. 433-444, 2010.\n\nGraph-based semi-supervised learning for phone and segment classification.,\" in INTERSPEECH. Y Liu, K Kirchhoff, Y. Liu and K. Kirchhoff, \"Graph-based semi-supervised learning for phone and segment clas- sification.,\" in INTERSPEECH, pp. 1840-1843, 2013.\n\nSemi-supervised model personalization for improved detection of learner's emotional engagement. N Alyuz, E Okur, E Oktay, U Genc, S Aslan, S E Mete, B Arnrich, A A Esme, Proceedings of the 18th ACM International Conference on Multimodal Interaction. the 18th ACM International Conference on Multimodal InteractionN. Alyuz, E. Okur, E. Oktay, U. Genc, S. Aslan, S. E. Mete, B. Arnrich, and A. A. Esme, \"Semi-supervised model personalization for improved detection of learner's emotional engage- ment,\" in Proceedings of the 18th ACM International Conference on Multimodal Interaction, pp. 100-107, 2016.\n\nSemi-supervised detection of student engagement. O M Nezami, D Richards, L Hamey, 21st Pacfici-Asia Conference on Information Systems. AIS Electronic Library (AISeLO. M. Nezami, D. Richards, and L. Hamey, \"Semi-supervised detection of student engage- ment,\" in 21st Pacfici-Asia Conference on Information Systems, PACIS 2017, pp. 2-8, AIS Electronic Library (AISeL), 2017.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \"Generative adversarial nets,\" in Advances in neural information processing systems, pp. 2672-2680, 2014.\n\nGeneralizing semi-supervised generative adversarial networks to regression. G Olmschenk, Z Zhu, H Tang, abs/1811.11269CoRR. G. Olmschenk, Z. Zhu, and H. Tang, \"Generalizing semi-supervised generative adversarial networks to regression,\" CoRR, vol. abs/1811.11269, 2018.\n\nMosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos. A Zadeh, R Zellers, E Pincus, L.-P Morency, arXiv:1606.06259arXiv preprintA. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, \"Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos,\" arXiv preprint arXiv:1606.06259, 2016.\n\nMultimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. A B Zadeh, P P Liang, S Poria, E Cambria, L.-P Morency, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1A. B. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L.-P. Morency, \"Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph,\" in Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2236-2246, 2018.\n\nModeling emotion in complex stories: the stanford emotional narratives dataset. D Ong, Z Wu, Z.-X Tan, M Reddan, I Kahhale, A Mattek, J Zaki, IEEE Transactions on Affective Computing. D. Ong, Z. Wu, Z.-X. Tan, M. Reddan, I. Kahhale, A. Mattek, and J. Zaki, \"Modeling emotion in complex stories: the stanford emotional narratives dataset,\" IEEE Transactions on Affective Computing, 2019.\n\nIntroducing the recola multimodal corpus of remote collaborative and affective interactions. F Ringeval, A Sonderegger, J Sauer, D Lalanne, 2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG). IEEEF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne, \"Introducing the recola multimodal corpus of remote collaborative and affective interactions,\" in 2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG), pp. 1-8, IEEE, 2013.\n\nMoseas: A multimodal language dataset for spanish, portuguese, german and french. A B Zadeh, Y Cao, S Hessner, P P Liang, S Poria, L.-P Morency, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)A. B. Zadeh, Y. Cao, S. Hessner, P. P. Liang, S. Poria, and L.-P. Morency, \"Moseas: A multimodal language dataset for spanish, portuguese, german and french,\" in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1801-1812, 2020.\n\nDaisee: Towards user engagement recognition in the wild. A Gupta, A Cunha, K Awasthi, V Balasubramanian, arXiv:1609.01885arXiv preprintA. Gupta, A. D'Cunha, K. Awasthi, and V. Balasubramanian, \"Daisee: Towards user engage- ment recognition in the wild,\" arXiv preprint arXiv:1609.01885, 2016.\n\nPrediction and localization of student engagement in the wild. A Kaur, A Mustafa, L Mehta, A Dhall, 2018 Digital Image Computing: Techniques and Applications (DICTA). IEEEA. Kaur, A. Mustafa, L. Mehta, and A. Dhall, \"Prediction and localization of student engage- ment in the wild,\" in 2018 Digital Image Computing: Techniques and Applications (DICTA), pp. 1-8, IEEE, 2018.\n\nTowards automated understanding of student-tutor interactions using visual deictic gestures. S Sathayanarayana, R Kumar, A Satzoda, M Carini, L Lee, J Salamanca, D Reilly, M Forster, G Bartlett, Littlewort, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsS. Sathayanarayana, R. Kumar Satzoda, A. Carini, M. Lee, L. Salamanca, J. Reilly, D. Forster, M. Bartlett, and G. Littlewort, \"Towards automated understanding of student-tutor interac- tions using visual deictic gestures,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 474-481, 2014.\n\nThe generalizability of the psychoanalytic concept of the working alliance. E S Bordin, Psychotherapy: Theory, research & practice. 16252E. S. Bordin, \"The generalizability of the psychoanalytic concept of the working alliance.,\" Psychotherapy: Theory, research & practice, vol. 16, no. 3, p. 252, 1979.\n\nPsychotherapists' acceptance of telepsychotherapy during the covid-19 pandemic: A machine learning approach. V B\u00e9k\u00e9s, K Aafjes-Van Doorn, S Zilcha-Mano, T Prout, L Hoffman, Clinical Psychology & Psychotherapy. 286V. B\u00e9k\u00e9s, K. Aafjes-van Doorn, S. Zilcha-Mano, T. Prout, and L. Hoffman, \"Psychother- apists' acceptance of telepsychotherapy during the covid-19 pandemic: A machine learning approach,\" Clinical Psychology & Psychotherapy, vol. 28, no. 6, pp. 1403-1415, 2021.\n", "annotations": {"author": "[{\"end\":179,\"start\":114},{\"end\":249,\"start\":180},{\"end\":333,\"start\":250},{\"end\":425,\"start\":334},{\"end\":507,\"start\":426},{\"end\":578,\"start\":508},{\"end\":628,\"start\":579}]", "publisher": null, "author_last_name": "[{\"end\":125,\"start\":120},{\"end\":193,\"start\":186},{\"end\":266,\"start\":258},{\"end\":349,\"start\":342},{\"end\":439,\"start\":433},{\"end\":522,\"start\":515},{\"end\":590,\"start\":586}]", "author_first_name": "[{\"end\":119,\"start\":114},{\"end\":185,\"start\":180},{\"end\":257,\"start\":250},{\"end\":341,\"start\":334},{\"end\":432,\"start\":426},{\"end\":514,\"start\":508},{\"end\":585,\"start\":579}]", "author_affiliation": "[{\"end\":178,\"start\":142},{\"end\":248,\"start\":212},{\"end\":332,\"start\":296},{\"end\":424,\"start\":385},{\"end\":506,\"start\":467},{\"end\":577,\"start\":541},{\"end\":627,\"start\":592}]", "title": "[{\"end\":111,\"start\":1},{\"end\":739,\"start\":629}]", "venue": null, "abstract": "[{\"end\":3906,\"start\":741}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4103,\"start\":4100},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4373,\"start\":4370},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4634,\"start\":4631},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4636,\"start\":4634},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4851,\"start\":4848},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8397,\"start\":8394},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8399,\"start\":8397},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8411,\"start\":8408},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8429,\"start\":8426},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8450,\"start\":8446},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8469,\"start\":8465},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8626,\"start\":8622},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8629,\"start\":8626},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8632,\"start\":8629},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8638,\"start\":8634},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8832,\"start\":8828},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8932,\"start\":8928},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9272,\"start\":9268},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9370,\"start\":9366},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11301,\"start\":11297},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14447,\"start\":14443},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14450,\"start\":14447},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15530,\"start\":15526},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16436,\"start\":16432},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16458,\"start\":16454},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16461,\"start\":16458},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16723,\"start\":16719},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16978,\"start\":16974},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17156,\"start\":17152},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18277,\"start\":18273},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19815,\"start\":19811},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19831,\"start\":19827},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19846,\"start\":19842},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25756,\"start\":25752},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26177,\"start\":26173},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27023,\"start\":27019},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27269,\"start\":27265},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27317,\"start\":27313},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30830,\"start\":30826}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34094,\"start\":33674},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34214,\"start\":34095},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34432,\"start\":34215},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34666,\"start\":34433},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34897,\"start\":34667},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34980,\"start\":34898},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":35022,\"start\":34981},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35187,\"start\":35023},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":35308,\"start\":35188}]", "paragraph": "[{\"end\":6288,\"start\":3933},{\"end\":7451,\"start\":6290},{\"end\":7951,\"start\":7453},{\"end\":9506,\"start\":7975},{\"end\":10281,\"start\":9508},{\"end\":11302,\"start\":10316},{\"end\":11620,\"start\":11327},{\"end\":12902,\"start\":11622},{\"end\":13104,\"start\":12909},{\"end\":13637,\"start\":13106},{\"end\":13640,\"start\":13639},{\"end\":13819,\"start\":13642},{\"end\":14008,\"start\":13821},{\"end\":15125,\"start\":14033},{\"end\":15469,\"start\":15127},{\"end\":15693,\"start\":15471},{\"end\":15829,\"start\":15695},{\"end\":15934,\"start\":15831},{\"end\":19199,\"start\":16017},{\"end\":21492,\"start\":19276},{\"end\":22562,\"start\":21494},{\"end\":24502,\"start\":22582},{\"end\":24773,\"start\":24504},{\"end\":25046,\"start\":24775},{\"end\":26084,\"start\":25048},{\"end\":26250,\"start\":26121},{\"end\":27118,\"start\":26305},{\"end\":27974,\"start\":27120},{\"end\":29095,\"start\":28004},{\"end\":30735,\"start\":29135},{\"end\":31430,\"start\":30746},{\"end\":33673,\"start\":31445}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15997,\"start\":15935}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20518,\"start\":20511},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27436,\"start\":27429},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28247,\"start\":28240},{\"end\":30416,\"start\":30409},{\"end\":30602,\"start\":30595}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":3920,\"start\":3908},{\"attributes\":{\"n\":\"2.1\"},\"end\":3931,\"start\":3923},{\"attributes\":{\"n\":\"2.2\"},\"end\":7973,\"start\":7954},{\"attributes\":{\"n\":\"3\"},\"end\":10290,\"start\":10284},{\"attributes\":{\"n\":\"3.1\"},\"end\":10314,\"start\":10293},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":11325,\"start\":11305},{\"end\":12907,\"start\":12905},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":14031,\"start\":14011},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":16015,\"start\":15999},{\"attributes\":{\"n\":\"4\"},\"end\":19210,\"start\":19202},{\"attributes\":{\"n\":\"4.1\"},\"end\":19274,\"start\":19213},{\"attributes\":{\"n\":\"4.2\"},\"end\":22580,\"start\":22565},{\"end\":26094,\"start\":26087},{\"attributes\":{\"n\":\"5\"},\"end\":26119,\"start\":26097},{\"attributes\":{\"n\":\"5.1\"},\"end\":26303,\"start\":26253},{\"attributes\":{\"n\":\"5.2\"},\"end\":28002,\"start\":27977},{\"attributes\":{\"n\":\"5.3\"},\"end\":29133,\"start\":29098},{\"end\":30744,\"start\":30738},{\"attributes\":{\"n\":\"5.4\"},\"end\":31443,\"start\":31433},{\"end\":33678,\"start\":33675},{\"end\":34106,\"start\":34096},{\"end\":34226,\"start\":34216},{\"end\":34444,\"start\":34434},{\"end\":34677,\"start\":34668},{\"end\":34908,\"start\":34899},{\"end\":34991,\"start\":34982},{\"end\":35198,\"start\":35189}]", "table": "[{\"end\":35187,\"start\":35102}]", "figure_caption": "[{\"end\":34094,\"start\":33680},{\"end\":34214,\"start\":34108},{\"end\":34432,\"start\":34228},{\"end\":34666,\"start\":34446},{\"end\":34897,\"start\":34679},{\"end\":34980,\"start\":34910},{\"end\":35022,\"start\":34993},{\"end\":35102,\"start\":35025},{\"end\":35308,\"start\":35200}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":35585,\"start\":35584},{\"end\":35587,\"start\":35586},{\"end\":35893,\"start\":35892},{\"end\":35895,\"start\":35894},{\"end\":35905,\"start\":35904},{\"end\":35912,\"start\":35911},{\"end\":35922,\"start\":35921},{\"end\":35933,\"start\":35932},{\"end\":35935,\"start\":35934},{\"end\":35946,\"start\":35945},{\"end\":35957,\"start\":35956},{\"end\":35967,\"start\":35966},{\"end\":35978,\"start\":35977},{\"end\":35980,\"start\":35979},{\"end\":35991,\"start\":35990},{\"end\":36419,\"start\":36418},{\"end\":36421,\"start\":36420},{\"end\":36432,\"start\":36431},{\"end\":36617,\"start\":36607},{\"end\":36758,\"start\":36757},{\"end\":36771,\"start\":36770},{\"end\":36785,\"start\":36781},{\"end\":36792,\"start\":36791},{\"end\":36802,\"start\":36801},{\"end\":36804,\"start\":36803},{\"end\":37183,\"start\":37182},{\"end\":37194,\"start\":37193},{\"end\":37198,\"start\":37195},{\"end\":37207,\"start\":37206},{\"end\":37214,\"start\":37213},{\"end\":37899,\"start\":37898},{\"end\":37905,\"start\":37904},{\"end\":37907,\"start\":37906},{\"end\":37915,\"start\":37914},{\"end\":38177,\"start\":38176},{\"end\":38188,\"start\":38187},{\"end\":38202,\"start\":38201},{\"end\":38211,\"start\":38210},{\"end\":38222,\"start\":38221},{\"end\":38224,\"start\":38223},{\"end\":38234,\"start\":38233},{\"end\":38739,\"start\":38738},{\"end\":38741,\"start\":38740},{\"end\":38751,\"start\":38750},{\"end\":39229,\"start\":39228},{\"end\":39239,\"start\":39238},{\"end\":39248,\"start\":39247},{\"end\":39258,\"start\":39257},{\"end\":39268,\"start\":39267},{\"end\":39270,\"start\":39269},{\"end\":39580,\"start\":39579},{\"end\":39591,\"start\":39590},{\"end\":39593,\"start\":39592},{\"end\":39608,\"start\":39607},{\"end\":39625,\"start\":39624},{\"end\":39926,\"start\":39925},{\"end\":39928,\"start\":39927},{\"end\":39942,\"start\":39941},{\"end\":39944,\"start\":39943},{\"end\":39955,\"start\":39954},{\"end\":39957,\"start\":39956},{\"end\":39966,\"start\":39965},{\"end\":39968,\"start\":39967},{\"end\":39977,\"start\":39976},{\"end\":39979,\"start\":39978},{\"end\":40373,\"start\":40372},{\"end\":40382,\"start\":40381},{\"end\":40395,\"start\":40394},{\"end\":40404,\"start\":40403},{\"end\":40414,\"start\":40413},{\"end\":40416,\"start\":40415},{\"end\":40424,\"start\":40423},{\"end\":40434,\"start\":40433},{\"end\":40443,\"start\":40442},{\"end\":40452,\"start\":40451},{\"end\":40462,\"start\":40461},{\"end\":40869,\"start\":40868},{\"end\":40878,\"start\":40877},{\"end\":40889,\"start\":40888},{\"end\":40895,\"start\":40894},{\"end\":41129,\"start\":41128},{\"end\":41135,\"start\":41134},{\"end\":41533,\"start\":41532},{\"end\":41546,\"start\":41545},{\"end\":41555,\"start\":41554},{\"end\":41557,\"start\":41556},{\"end\":41566,\"start\":41565},{\"end\":41568,\"start\":41567},{\"end\":41935,\"start\":41934},{\"end\":41944,\"start\":41943},{\"end\":41953,\"start\":41952},{\"end\":41961,\"start\":41960},{\"end\":42435,\"start\":42434},{\"end\":42446,\"start\":42445},{\"end\":42461,\"start\":42460},{\"end\":42475,\"start\":42474},{\"end\":42492,\"start\":42491},{\"end\":43003,\"start\":43002},{\"end\":43005,\"start\":43004},{\"end\":43016,\"start\":43015},{\"end\":43018,\"start\":43017},{\"end\":43027,\"start\":43026},{\"end\":43218,\"start\":43217},{\"end\":43231,\"start\":43230},{\"end\":43244,\"start\":43243},{\"end\":43255,\"start\":43254},{\"end\":43270,\"start\":43269},{\"end\":43283,\"start\":43282},{\"end\":43724,\"start\":43723},{\"end\":43734,\"start\":43733},{\"end\":43738,\"start\":43735},{\"end\":44167,\"start\":44166},{\"end\":44178,\"start\":44177},{\"end\":44197,\"start\":44196},{\"end\":44677,\"start\":44676},{\"end\":44683,\"start\":44682},{\"end\":44691,\"start\":44690},{\"end\":44698,\"start\":44697},{\"end\":45098,\"start\":45097},{\"end\":45104,\"start\":45103},{\"end\":45119,\"start\":45118},{\"end\":45127,\"start\":45126},{\"end\":45520,\"start\":45519},{\"end\":45527,\"start\":45526},{\"end\":45779,\"start\":45778},{\"end\":45788,\"start\":45787},{\"end\":45796,\"start\":45795},{\"end\":45805,\"start\":45804},{\"end\":45813,\"start\":45812},{\"end\":45822,\"start\":45821},{\"end\":45824,\"start\":45823},{\"end\":45832,\"start\":45831},{\"end\":45843,\"start\":45842},{\"end\":45845,\"start\":45844},{\"end\":46336,\"start\":46335},{\"end\":46338,\"start\":46337},{\"end\":46348,\"start\":46347},{\"end\":46360,\"start\":46359},{\"end\":46690,\"start\":46689},{\"end\":46704,\"start\":46703},{\"end\":46721,\"start\":46720},{\"end\":46730,\"start\":46729},{\"end\":46736,\"start\":46735},{\"end\":46752,\"start\":46751},{\"end\":46761,\"start\":46760},{\"end\":46774,\"start\":46773},{\"end\":47124,\"start\":47123},{\"end\":47137,\"start\":47136},{\"end\":47144,\"start\":47143},{\"end\":47418,\"start\":47417},{\"end\":47427,\"start\":47426},{\"end\":47438,\"start\":47437},{\"end\":47451,\"start\":47447},{\"end\":47785,\"start\":47784},{\"end\":47787,\"start\":47786},{\"end\":47796,\"start\":47795},{\"end\":47798,\"start\":47797},{\"end\":47807,\"start\":47806},{\"end\":47816,\"start\":47815},{\"end\":47830,\"start\":47826},{\"end\":48403,\"start\":48402},{\"end\":48410,\"start\":48409},{\"end\":48419,\"start\":48415},{\"end\":48426,\"start\":48425},{\"end\":48436,\"start\":48435},{\"end\":48447,\"start\":48446},{\"end\":48457,\"start\":48456},{\"end\":48804,\"start\":48803},{\"end\":48816,\"start\":48815},{\"end\":48831,\"start\":48830},{\"end\":48840,\"start\":48839},{\"end\":49316,\"start\":49315},{\"end\":49318,\"start\":49317},{\"end\":49327,\"start\":49326},{\"end\":49334,\"start\":49333},{\"end\":49345,\"start\":49344},{\"end\":49347,\"start\":49346},{\"end\":49356,\"start\":49355},{\"end\":49368,\"start\":49364},{\"end\":49891,\"start\":49890},{\"end\":49900,\"start\":49899},{\"end\":49909,\"start\":49908},{\"end\":49920,\"start\":49919},{\"end\":50191,\"start\":50190},{\"end\":50199,\"start\":50198},{\"end\":50210,\"start\":50209},{\"end\":50219,\"start\":50218},{\"end\":50596,\"start\":50595},{\"end\":50615,\"start\":50614},{\"end\":50624,\"start\":50623},{\"end\":50635,\"start\":50634},{\"end\":50645,\"start\":50644},{\"end\":50652,\"start\":50651},{\"end\":50665,\"start\":50664},{\"end\":50675,\"start\":50674},{\"end\":50686,\"start\":50685},{\"end\":51282,\"start\":51281},{\"end\":51284,\"start\":51283},{\"end\":51620,\"start\":51619},{\"end\":51629,\"start\":51628},{\"end\":51649,\"start\":51648},{\"end\":51664,\"start\":51663},{\"end\":51673,\"start\":51672}]", "bib_author_last_name": "[{\"end\":35600,\"start\":35588},{\"end\":35902,\"start\":35896},{\"end\":35909,\"start\":35906},{\"end\":35919,\"start\":35913},{\"end\":35930,\"start\":35923},{\"end\":35943,\"start\":35936},{\"end\":35954,\"start\":35947},{\"end\":35964,\"start\":35958},{\"end\":35975,\"start\":35968},{\"end\":35988,\"start\":35981},{\"end\":35999,\"start\":35992},{\"end\":36429,\"start\":36422},{\"end\":36439,\"start\":36433},{\"end\":36622,\"start\":36618},{\"end\":36630,\"start\":36624},{\"end\":36768,\"start\":36759},{\"end\":36779,\"start\":36772},{\"end\":36789,\"start\":36786},{\"end\":36799,\"start\":36793},{\"end\":36813,\"start\":36805},{\"end\":37191,\"start\":37184},{\"end\":37204,\"start\":37199},{\"end\":37211,\"start\":37208},{\"end\":37218,\"start\":37215},{\"end\":37902,\"start\":37900},{\"end\":37912,\"start\":37908},{\"end\":37924,\"start\":37916},{\"end\":38185,\"start\":38178},{\"end\":38199,\"start\":38189},{\"end\":38208,\"start\":38203},{\"end\":38219,\"start\":38212},{\"end\":38231,\"start\":38225},{\"end\":38240,\"start\":38235},{\"end\":38748,\"start\":38742},{\"end\":38757,\"start\":38752},{\"end\":39236,\"start\":39230},{\"end\":39245,\"start\":39240},{\"end\":39255,\"start\":39249},{\"end\":39265,\"start\":39259},{\"end\":39275,\"start\":39271},{\"end\":39588,\"start\":39581},{\"end\":39605,\"start\":39594},{\"end\":39622,\"start\":39609},{\"end\":39631,\"start\":39626},{\"end\":39939,\"start\":39929},{\"end\":39952,\"start\":39945},{\"end\":39963,\"start\":39958},{\"end\":39974,\"start\":39969},{\"end\":39986,\"start\":39980},{\"end\":40379,\"start\":40374},{\"end\":40392,\"start\":40383},{\"end\":40401,\"start\":40396},{\"end\":40411,\"start\":40405},{\"end\":40421,\"start\":40417},{\"end\":40431,\"start\":40425},{\"end\":40440,\"start\":40435},{\"end\":40449,\"start\":40444},{\"end\":40459,\"start\":40453},{\"end\":40468,\"start\":40463},{\"end\":40875,\"start\":40870},{\"end\":40886,\"start\":40879},{\"end\":40892,\"start\":40890},{\"end\":40904,\"start\":40896},{\"end\":41132,\"start\":41130},{\"end\":41144,\"start\":41136},{\"end\":41543,\"start\":41534},{\"end\":41552,\"start\":41547},{\"end\":41563,\"start\":41558},{\"end\":41581,\"start\":41569},{\"end\":41941,\"start\":41936},{\"end\":41950,\"start\":41945},{\"end\":41958,\"start\":41954},{\"end\":41965,\"start\":41962},{\"end\":42443,\"start\":42436},{\"end\":42458,\"start\":42447},{\"end\":42472,\"start\":42462},{\"end\":42489,\"start\":42476},{\"end\":42499,\"start\":42493},{\"end\":43013,\"start\":43006},{\"end\":43024,\"start\":43019},{\"end\":43036,\"start\":43028},{\"end\":43228,\"start\":43219},{\"end\":43241,\"start\":43232},{\"end\":43252,\"start\":43245},{\"end\":43267,\"start\":43256},{\"end\":43280,\"start\":43271},{\"end\":43291,\"start\":43284},{\"end\":43731,\"start\":43725},{\"end\":43745,\"start\":43739},{\"end\":44175,\"start\":44168},{\"end\":44194,\"start\":44179},{\"end\":44204,\"start\":44198},{\"end\":44680,\"start\":44678},{\"end\":44688,\"start\":44684},{\"end\":44695,\"start\":44692},{\"end\":44703,\"start\":44699},{\"end\":45101,\"start\":45099},{\"end\":45116,\"start\":45105},{\"end\":45124,\"start\":45120},{\"end\":45133,\"start\":45128},{\"end\":45524,\"start\":45521},{\"end\":45537,\"start\":45528},{\"end\":45785,\"start\":45780},{\"end\":45793,\"start\":45789},{\"end\":45802,\"start\":45797},{\"end\":45810,\"start\":45806},{\"end\":45819,\"start\":45814},{\"end\":45829,\"start\":45825},{\"end\":45840,\"start\":45833},{\"end\":45850,\"start\":45846},{\"end\":46345,\"start\":46339},{\"end\":46357,\"start\":46349},{\"end\":46366,\"start\":46361},{\"end\":46701,\"start\":46691},{\"end\":46718,\"start\":46705},{\"end\":46727,\"start\":46722},{\"end\":46733,\"start\":46731},{\"end\":46749,\"start\":46737},{\"end\":46758,\"start\":46753},{\"end\":46771,\"start\":46762},{\"end\":46781,\"start\":46775},{\"end\":47134,\"start\":47125},{\"end\":47141,\"start\":47138},{\"end\":47149,\"start\":47145},{\"end\":47424,\"start\":47419},{\"end\":47435,\"start\":47428},{\"end\":47445,\"start\":47439},{\"end\":47459,\"start\":47452},{\"end\":47793,\"start\":47788},{\"end\":47804,\"start\":47799},{\"end\":47813,\"start\":47808},{\"end\":47824,\"start\":47817},{\"end\":47838,\"start\":47831},{\"end\":48407,\"start\":48404},{\"end\":48413,\"start\":48411},{\"end\":48423,\"start\":48420},{\"end\":48433,\"start\":48427},{\"end\":48444,\"start\":48437},{\"end\":48454,\"start\":48448},{\"end\":48462,\"start\":48458},{\"end\":48813,\"start\":48805},{\"end\":48828,\"start\":48817},{\"end\":48837,\"start\":48832},{\"end\":48848,\"start\":48841},{\"end\":49324,\"start\":49319},{\"end\":49331,\"start\":49328},{\"end\":49342,\"start\":49335},{\"end\":49353,\"start\":49348},{\"end\":49362,\"start\":49357},{\"end\":49376,\"start\":49369},{\"end\":49897,\"start\":49892},{\"end\":49906,\"start\":49901},{\"end\":49917,\"start\":49910},{\"end\":49936,\"start\":49921},{\"end\":50196,\"start\":50192},{\"end\":50207,\"start\":50200},{\"end\":50216,\"start\":50211},{\"end\":50225,\"start\":50220},{\"end\":50612,\"start\":50597},{\"end\":50621,\"start\":50616},{\"end\":50632,\"start\":50625},{\"end\":50642,\"start\":50636},{\"end\":50649,\"start\":50646},{\"end\":50662,\"start\":50653},{\"end\":50672,\"start\":50666},{\"end\":50683,\"start\":50676},{\"end\":50695,\"start\":50687},{\"end\":50707,\"start\":50697},{\"end\":51291,\"start\":51285},{\"end\":51626,\"start\":51621},{\"end\":51646,\"start\":51630},{\"end\":51661,\"start\":51650},{\"end\":51670,\"start\":51665},{\"end\":51681,\"start\":51674}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35481,\"start\":35310},{\"attributes\":{\"id\":\"b1\"},\"end\":35728,\"start\":35483},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":205967479},\"end\":36366,\"start\":35730},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":28078365},\"end\":36603,\"start\":36368},{\"attributes\":{\"id\":\"b4\"},\"end\":36662,\"start\":36605},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":177004},\"end\":37095,\"start\":36664},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207889061},\"end\":37843,\"start\":37097},{\"attributes\":{\"doi\":\"cs/0410027\",\"id\":\"b7\"},\"end\":38073,\"start\":37845},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1212510},\"end\":38654,\"start\":38075},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14476611},\"end\":39119,\"start\":38656},{\"attributes\":{\"doi\":\"arXiv:1909.12913\",\"id\":\"b10\"},\"end\":39515,\"start\":39121},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":52289449},\"end\":39856,\"start\":39517},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7388551},\"end\":40302,\"start\":39858},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15366064},\"end\":40832,\"start\":40304},{\"attributes\":{\"doi\":\"arXiv:1608.08711\",\"id\":\"b14\"},\"end\":41056,\"start\":40834},{\"attributes\":{\"id\":\"b15\"},\"end\":41429,\"start\":41058},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6452456},\"end\":41859,\"start\":41431},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52898615},\"end\":42329,\"start\":41861},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53719969},\"end\":42928,\"start\":42331},{\"attributes\":{\"id\":\"b19\"},\"end\":43162,\"start\":42930},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":948552},\"end\":43617,\"start\":43164},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":220643923},\"end\":43965,\"start\":43619},{\"attributes\":{\"doi\":\"arXiv:1710.07557\",\"id\":\"b22\"},\"end\":44598,\"start\":43967},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":34933795},\"end\":44947,\"start\":44600},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11364150},\"end\":45424,\"start\":44949},{\"attributes\":{\"id\":\"b25\"},\"end\":45680,\"start\":45426},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7949846},\"end\":46284,\"start\":45682},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":44666337},\"end\":46658,\"start\":46286},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1033682},\"end\":47045,\"start\":46660},{\"attributes\":{\"doi\":\"abs/1811.11269\",\"id\":\"b29\",\"matched_paper_id\":53850453},\"end\":47316,\"start\":47047},{\"attributes\":{\"doi\":\"arXiv:1606.06259\",\"id\":\"b30\"},\"end\":47682,\"start\":47318},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":51868869},\"end\":48320,\"start\":47684},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":209202583},\"end\":48708,\"start\":48322},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206651806},\"end\":49231,\"start\":48710},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":226262333},\"end\":49831,\"start\":49233},{\"attributes\":{\"doi\":\"arXiv:1609.01885\",\"id\":\"b35\"},\"end\":50125,\"start\":49833},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4873905},\"end\":50500,\"start\":50127},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":16200807},\"end\":51203,\"start\":50502},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":144282001},\"end\":51508,\"start\":51205},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":240355400},\"end\":51982,\"start\":51510}]", "bib_title": "[{\"end\":35890,\"start\":35730},{\"end\":36416,\"start\":36368},{\"end\":36755,\"start\":36664},{\"end\":37180,\"start\":37097},{\"end\":38174,\"start\":38075},{\"end\":38736,\"start\":38656},{\"end\":39577,\"start\":39517},{\"end\":39923,\"start\":39858},{\"end\":40370,\"start\":40304},{\"end\":41126,\"start\":41058},{\"end\":41530,\"start\":41431},{\"end\":41932,\"start\":41861},{\"end\":42432,\"start\":42331},{\"end\":43215,\"start\":43164},{\"end\":43721,\"start\":43619},{\"end\":44674,\"start\":44600},{\"end\":45095,\"start\":44949},{\"end\":45776,\"start\":45682},{\"end\":46333,\"start\":46286},{\"end\":46687,\"start\":46660},{\"end\":47121,\"start\":47047},{\"end\":47782,\"start\":47684},{\"end\":48400,\"start\":48322},{\"end\":48801,\"start\":48710},{\"end\":49313,\"start\":49233},{\"end\":50188,\"start\":50127},{\"end\":50593,\"start\":50502},{\"end\":51279,\"start\":51205},{\"end\":51617,\"start\":51510}]", "bib_author": "[{\"end\":35602,\"start\":35584},{\"end\":35904,\"start\":35892},{\"end\":35911,\"start\":35904},{\"end\":35921,\"start\":35911},{\"end\":35932,\"start\":35921},{\"end\":35945,\"start\":35932},{\"end\":35956,\"start\":35945},{\"end\":35966,\"start\":35956},{\"end\":35977,\"start\":35966},{\"end\":35990,\"start\":35977},{\"end\":36001,\"start\":35990},{\"end\":36431,\"start\":36418},{\"end\":36441,\"start\":36431},{\"end\":36624,\"start\":36607},{\"end\":36632,\"start\":36624},{\"end\":36770,\"start\":36757},{\"end\":36781,\"start\":36770},{\"end\":36791,\"start\":36781},{\"end\":36801,\"start\":36791},{\"end\":36815,\"start\":36801},{\"end\":37193,\"start\":37182},{\"end\":37206,\"start\":37193},{\"end\":37213,\"start\":37206},{\"end\":37220,\"start\":37213},{\"end\":37904,\"start\":37898},{\"end\":37914,\"start\":37904},{\"end\":37926,\"start\":37914},{\"end\":38187,\"start\":38176},{\"end\":38201,\"start\":38187},{\"end\":38210,\"start\":38201},{\"end\":38221,\"start\":38210},{\"end\":38233,\"start\":38221},{\"end\":38242,\"start\":38233},{\"end\":38750,\"start\":38738},{\"end\":38759,\"start\":38750},{\"end\":39238,\"start\":39228},{\"end\":39247,\"start\":39238},{\"end\":39257,\"start\":39247},{\"end\":39267,\"start\":39257},{\"end\":39277,\"start\":39267},{\"end\":39590,\"start\":39579},{\"end\":39607,\"start\":39590},{\"end\":39624,\"start\":39607},{\"end\":39633,\"start\":39624},{\"end\":39941,\"start\":39925},{\"end\":39954,\"start\":39941},{\"end\":39965,\"start\":39954},{\"end\":39976,\"start\":39965},{\"end\":39988,\"start\":39976},{\"end\":40381,\"start\":40372},{\"end\":40394,\"start\":40381},{\"end\":40403,\"start\":40394},{\"end\":40413,\"start\":40403},{\"end\":40423,\"start\":40413},{\"end\":40433,\"start\":40423},{\"end\":40442,\"start\":40433},{\"end\":40451,\"start\":40442},{\"end\":40461,\"start\":40451},{\"end\":40470,\"start\":40461},{\"end\":40877,\"start\":40868},{\"end\":40888,\"start\":40877},{\"end\":40894,\"start\":40888},{\"end\":40906,\"start\":40894},{\"end\":41134,\"start\":41128},{\"end\":41146,\"start\":41134},{\"end\":41545,\"start\":41532},{\"end\":41554,\"start\":41545},{\"end\":41565,\"start\":41554},{\"end\":41583,\"start\":41565},{\"end\":41943,\"start\":41934},{\"end\":41952,\"start\":41943},{\"end\":41960,\"start\":41952},{\"end\":41967,\"start\":41960},{\"end\":42445,\"start\":42434},{\"end\":42460,\"start\":42445},{\"end\":42474,\"start\":42460},{\"end\":42491,\"start\":42474},{\"end\":42501,\"start\":42491},{\"end\":43015,\"start\":43002},{\"end\":43026,\"start\":43015},{\"end\":43038,\"start\":43026},{\"end\":43230,\"start\":43217},{\"end\":43243,\"start\":43230},{\"end\":43254,\"start\":43243},{\"end\":43269,\"start\":43254},{\"end\":43282,\"start\":43269},{\"end\":43293,\"start\":43282},{\"end\":43733,\"start\":43723},{\"end\":43747,\"start\":43733},{\"end\":44177,\"start\":44166},{\"end\":44196,\"start\":44177},{\"end\":44206,\"start\":44196},{\"end\":44682,\"start\":44676},{\"end\":44690,\"start\":44682},{\"end\":44697,\"start\":44690},{\"end\":44705,\"start\":44697},{\"end\":45103,\"start\":45097},{\"end\":45118,\"start\":45103},{\"end\":45126,\"start\":45118},{\"end\":45135,\"start\":45126},{\"end\":45526,\"start\":45519},{\"end\":45539,\"start\":45526},{\"end\":45787,\"start\":45778},{\"end\":45795,\"start\":45787},{\"end\":45804,\"start\":45795},{\"end\":45812,\"start\":45804},{\"end\":45821,\"start\":45812},{\"end\":45831,\"start\":45821},{\"end\":45842,\"start\":45831},{\"end\":45852,\"start\":45842},{\"end\":46347,\"start\":46335},{\"end\":46359,\"start\":46347},{\"end\":46368,\"start\":46359},{\"end\":46703,\"start\":46689},{\"end\":46720,\"start\":46703},{\"end\":46729,\"start\":46720},{\"end\":46735,\"start\":46729},{\"end\":46751,\"start\":46735},{\"end\":46760,\"start\":46751},{\"end\":46773,\"start\":46760},{\"end\":46783,\"start\":46773},{\"end\":47136,\"start\":47123},{\"end\":47143,\"start\":47136},{\"end\":47151,\"start\":47143},{\"end\":47426,\"start\":47417},{\"end\":47437,\"start\":47426},{\"end\":47447,\"start\":47437},{\"end\":47461,\"start\":47447},{\"end\":47795,\"start\":47784},{\"end\":47806,\"start\":47795},{\"end\":47815,\"start\":47806},{\"end\":47826,\"start\":47815},{\"end\":47840,\"start\":47826},{\"end\":48409,\"start\":48402},{\"end\":48415,\"start\":48409},{\"end\":48425,\"start\":48415},{\"end\":48435,\"start\":48425},{\"end\":48446,\"start\":48435},{\"end\":48456,\"start\":48446},{\"end\":48464,\"start\":48456},{\"end\":48815,\"start\":48803},{\"end\":48830,\"start\":48815},{\"end\":48839,\"start\":48830},{\"end\":48850,\"start\":48839},{\"end\":49326,\"start\":49315},{\"end\":49333,\"start\":49326},{\"end\":49344,\"start\":49333},{\"end\":49355,\"start\":49344},{\"end\":49364,\"start\":49355},{\"end\":49378,\"start\":49364},{\"end\":49899,\"start\":49890},{\"end\":49908,\"start\":49899},{\"end\":49919,\"start\":49908},{\"end\":49938,\"start\":49919},{\"end\":50198,\"start\":50190},{\"end\":50209,\"start\":50198},{\"end\":50218,\"start\":50209},{\"end\":50227,\"start\":50218},{\"end\":50614,\"start\":50595},{\"end\":50623,\"start\":50614},{\"end\":50634,\"start\":50623},{\"end\":50644,\"start\":50634},{\"end\":50651,\"start\":50644},{\"end\":50664,\"start\":50651},{\"end\":50674,\"start\":50664},{\"end\":50685,\"start\":50674},{\"end\":50697,\"start\":50685},{\"end\":50709,\"start\":50697},{\"end\":51293,\"start\":51281},{\"end\":51628,\"start\":51619},{\"end\":51648,\"start\":51628},{\"end\":51663,\"start\":51648},{\"end\":51672,\"start\":51663},{\"end\":51683,\"start\":51672}]", "bib_venue": "[{\"end\":35391,\"start\":35310},{\"end\":35582,\"start\":35483},{\"end\":36011,\"start\":36001},{\"end\":36462,\"start\":36441},{\"end\":36855,\"start\":36815},{\"end\":37431,\"start\":37220},{\"end\":37896,\"start\":37845},{\"end\":38316,\"start\":38242},{\"end\":38838,\"start\":38759},{\"end\":39226,\"start\":39121},{\"end\":39659,\"start\":39633},{\"end\":40052,\"start\":39988},{\"end\":40541,\"start\":40470},{\"end\":40866,\"start\":40834},{\"end\":41214,\"start\":41146},{\"end\":41623,\"start\":41583},{\"end\":42045,\"start\":41967},{\"end\":42581,\"start\":42501},{\"end\":43000,\"start\":42930},{\"end\":43362,\"start\":43293},{\"end\":43768,\"start\":43747},{\"end\":44164,\"start\":43967},{\"end\":44747,\"start\":44705},{\"end\":45161,\"start\":45135},{\"end\":45517,\"start\":45426},{\"end\":45930,\"start\":45852},{\"end\":46419,\"start\":46368},{\"end\":46832,\"start\":46783},{\"end\":47169,\"start\":47165},{\"end\":47415,\"start\":47318},{\"end\":47927,\"start\":47840},{\"end\":48504,\"start\":48464},{\"end\":48950,\"start\":48850},{\"end\":49472,\"start\":49378},{\"end\":49888,\"start\":49833},{\"end\":50292,\"start\":50227},{\"end\":50796,\"start\":50709},{\"end\":51335,\"start\":51293},{\"end\":51718,\"start\":51683},{\"end\":38377,\"start\":38318},{\"end\":38904,\"start\":38840},{\"end\":42110,\"start\":42047},{\"end\":42648,\"start\":42583},{\"end\":45995,\"start\":45932},{\"end\":48001,\"start\":47929},{\"end\":49553,\"start\":49474},{\"end\":50870,\"start\":50798}]"}}}, "year": 2023, "month": 12, "day": 17}