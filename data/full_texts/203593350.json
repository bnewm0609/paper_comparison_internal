{"id": 203593350, "updated": "2023-12-08 20:31:38.326", "metadata": {"title": "Well-calibrated Model Uncertainty with Temperature Scaling for Dropout Variational Inference", "authors": "[{\"first\":\"Max-Heinrich\",\"last\":\"Laves\",\"middle\":[]},{\"first\":\"Sontje\",\"last\":\"Ihler\",\"middle\":[]},{\"first\":\"Karl-Philipp\",\"last\":\"Kortmann\",\"middle\":[]},{\"first\":\"Tobias\",\"last\":\"Ortmaier\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Model uncertainty obtained by variational Bayesian inference with Monte Carlo dropout is prone to miscalibration. The uncertainty does not represent the model error well. In this paper, temperature scaling is extended to dropout variational inference to calibrate model uncertainty. Expected uncertainty calibration error (UCE) is presented as a metric to measure miscalibration of uncertainty. The effectiveness of this approach is evaluated on CIFAR-10/100 for recent CNN architectures. Experimental results show, that temperature scaling considerably reduces miscalibration by means of UCE and enables robust rejection of uncertain predictions. The proposed approach can easily be derived from frequentist temperature scaling and yields well-calibrated model uncertainty. It is simple to implement and does not affect the model accuracy.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": null, "mag": "2976356773", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1909-13550", "doi": null}}, "content": {"source": {"pdf_hash": "e7308644562065bc1b1690d382d61cf3c8725262", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1909.13550v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1fd372565f1b51de4e08cb011f52539675511471", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e7308644562065bc1b1690d382d61cf3c8725262.txt", "contents": "\nWell-calibrated Model Uncertainty with Temperature Scaling for Dropout Variational Inference\n\n\nMax-Heinrich Laves laves@imes.uni-hannover.de \nInstitute of Mechatronic Systems Leibniz University Hannover\nGermany\n\nSontje Ihler ihler@imes.uni-hannover.de \nInstitute of Mechatronic Systems Leibniz University Hannover\nGermany\n\nKarl-Philipp Kortmann kortmann@imes.uni-hannover.de \nInstitute of Mechatronic Systems Leibniz University Hannover\nGermany\n\nTobias Ortmaier ortmaier@imes.uni-hannover.de \nInstitute of Mechatronic Systems Leibniz University Hannover\nGermany\n\nWell-calibrated Model Uncertainty with Temperature Scaling for Dropout Variational Inference\n\nModel uncertainty obtained by variational Bayesian inference with Monte Carlo dropout is prone to miscalibration. The uncertainty does not represent the model error well. In this paper, temperature scaling is extended to dropout variational inference to calibrate model uncertainty. Expected uncertainty calibration error (UCE) is presented as a metric to measure miscalibration of uncertainty. The effectiveness of this approach is evaluated on CIFAR-10/100 for recent CNN architectures. Experimental results show, that temperature scaling considerably reduces miscalibration by means of UCE and enables robust rejection of uncertain predictions. The proposed approach can easily be derived from frequentist temperature scaling and yields well-calibrated model uncertainty. It is simple to implement and does not affect the model accuracy.4th workshop on Bayesian Deep Learning (NeurIPS 2019), Vancouver, Canada.\n\nIntroduction\n\nFor safety-critical vision tasks such as autonomous driving or computer-aided diagnosis, it is essential to consider the prediction uncertainty of deep learning models. Bayesian neural networks and recent advances in their approximation provide the mathematical tools for quantification of uncertainty [1,2]. One practical approximation is variational inference with Monte Carlo (MC) dropout [3]. It is applied to obtain epistemic uncertainty, which is caused by uncertainty in the model weights due to training with data sets of limited size [1,4]. However, it tends to be miscalibrated, i. e. the uncertainty does not correspond well to the model error [5,6].\n\nFirst, we consider the problem of miscalibration of the frequentist approach: The weights of a deep model are obtained by maximum likelihood estimation [1], and the normalized output likelihood for an unseen test input does not consider uncertainty in the weights [4]. The likelihood is generally unjustifiably high [5], and can be misinterpreted as high prediction confidence. This miscalibration can also be observed for model uncertainty provided by MC dropout variational inference. However, calibrated uncertainty is essential as miscalibration can lead to decisions with fatal consequences in the aforementioned task domains.\n\nOverconfident predictions of neural networks have been addressed by entropy regularization techniques. Szegedy et al. present label smoothing as regularization of models during supervised training for classification [7]. They state that a model trained with one-hot encoded labels is prone to becoming overconfident about its predictions, which causes overfitting and poor generalization. Pereyra et al. link label smoothing to confidence penalty (CP) and propose a simple way to prevent overconfident networks [8]. Low entropy output distributions are penalized by adding the negative entropy to the training objective. However, the referred works do not apply entropy regularization to the calibration of confidence or uncertainty. In the last decades, several non-parametric and parametric calibration approaches such as isotonic regression [9] or Platt scaling [10] have been presented. Recently, temperature scaling (TS) has been demonstrated to lead to well-calibrated model likelihood in non-Bayesian deep neural networks [5]. It uses a single scalar to smooth the softmax output and regularize the entropy. Scaling has also been introduced to approximate categorical distributions by the Gumbel-Softmax or Concrete distribution [11,12].\n\nOur work extends temperature scaling to variational Bayesian inference with dropout to obtain well-calibrated model uncertainty. The main contributions of this paper are 1. definition for perfect calibration of uncertainty and definition for the expected uncertainty calibration error, 2. the derivation of temperature scaling for dropout variational inference, and 3. experimental results of different network architectures on CIFAR-10/100 [13], that demonstrate the improvement of calibration by the proposed method and superiority over confidence penalty. By using temperature scaling together with Bayesian inference, we expect better calibrated uncertainty. To the best of our knowledge, temperature scaling has not yet been used to calibrate model uncertainty in variational Bayesian inference. Our code is available at: https://github.com/mlaves/bayesian-temperature-scaling.\n\n\nMethods\n\nThe presented approach for obtaining well-calibrated uncertainty is applied to a general multi-class classification task. Let input x \u2208 X be a random variable with corresponding label y \u2208 Y = {1, . . . , C}. Let f w (x) be the output (logits) of a neural network with weight matrices w, and with model likelihood p(y = c | f w (x)) for class c, which is sampled from a probability vector p = \u03c3 SM (f w (x)), obtained by passing the model output through the softmax function \u03c3 SM (\u00b7). From a frequentist perspective, the softmax likelihood is often interpreted as confidence of prediction. Throughout this paper, we follow this definition. However, due to optimizing the weights w via minimization of the negative log-likelihood of p(y | f w (x)), modern deep models are prone to overly confident predictions and are therefore miscalibrated [5,6]. Let\u0177 = arg max p be the most likely class prediction of input x with likelihoodp = max p and true label y. Then, following Guo et al. [5], perfect calibration is defined as\nP (\u0177 = y |p = q) = q, \u2200q \u2208 [0, 1] .(1)\nTo determine model uncertainty, dropout variational inference is performed by training the model f w with dropout [14] and using dropout at test time to sample from the approximate posterior by performing N stochastic forward passes [3,4]. This is also referred to as MC dropout. In MC dropout, the final probability vector is obtained by MC integration:\np(x) = 1 N N i=1 \u03c3 SM (f wi (x)) .(2)\nEntropy of the softmax likelihood is used to describe uncertainty of prediction [4]. In contrast to confidence as a measure of goodness of prediction, uncertainty takes into account the likelihoods of all C classes. We introduce normalization to scale the values to a range between 0 and 1:\nH(p) := \u2212 1 log C C c=1 p (c) log p (c) ,H \u2208 [0, 1] .(3)\nFrom Eq. (1) and Eq. (3), we define perfect calibration of uncertainty as\nP(\u0177 = y |H(p) = q) = q, \u2200q \u2208 [0, 1] .(4)\nThat is, in a batch of inputs all predicted with uncertainty of e.g. 0.2, a top-1 error of 20 % is expected.\n\n\nExpected Uncertainty Calibration Error (UCE)\n\nA popular way to quantify miscalibration of neural networks with a scalar value is the expectation of the difference between predicted softmax likelihoodp and accuracy  which can be approximated by the Expected Calibration Error (ECE) [15,5]. The output of a neural network is partitioned into M bins with equal width and a weighted average of the difference between accuracy and confidence (softmax likelihood) is taken:\nEp [ |P (\u0177 = y |p = q) \u2212 q| ] , \u2200q \u2208 [0, 1] ,(5)ECE = M m=1 |B m | n acc(B m ) \u2212 conf(B m ) ,(6)\nwith total number of inputs n and set of indices B m of inputs whose confidence falls into that bin (see [5] for more details). We propose the following slightly modified notion of Eq. (5) to quantify miscalibration of uncertainty:\nEH[ |P(\u0177 = y |H(p) = q) \u2212 q| ], \u2200q \u2208 [0, 1] .(7)\nWe refer to this as Expected Uncertainty Calibration Error (UCE) and approximate analogously with\nUCE := M m=1 |B m | n err(B m ) \u2212 uncert(B m ) .(8)\nSee \u00a7 A.1 for definitions of err(B m ) and uncert(B m ).\n\n\nTemperature Scaling for Dropout Variational Inference\n\nState-of-the-art deep neural networks are generally miscalibrated with regard to softmax likelihood [5].\n\nHowever, when obtaining model uncertainty with dropout variational inference, this also tends to be not well-calibrated [6]. Fig. 1 (top row) shows reliability diagrams [16] for uncalibrated ResNet-101 [17] trained on CIFAR-100 [13]. The divergence from the identity function reveals miscalibration.\n\nIn this work, dropout is inserted before the last layer with fixed dropout probability of 0.5 as in [3]. Temperature scaling with T > 0 is inserted before final softmax activation and before MC integration:\np(x) = 1 N N i=1 \u03c3 SM T \u22121 f wi (x) .(9)\nT is optimized with respect to negative log-likelihood while performing MC dropout on the validation set. This is equivalent to maximizing the entropy ofp [5]. See \u00a7 A.2 for more details on T .   \n\n\nExperiments & Results\n\nThe experimental results of the proposed method are presented threefold: First, TS is used to calibrate confidence and uncertainty obtained by MC dropout; second, TS calibration is compared with calibration by entropy regularization using confidence penalty; and finally, uncertain predictions are rejected based on well-calibrated uncertainty. Details on the training procedure can be found in \u00a7 A.3.\n\n\nUncertainty Calibration\n\nTab. 1 reports test set results for different networks [17,18] and data sets used to evaluate the performance of temperature scaling for dropout variational inference. The proposed UCE metric is used to quantify calibration of uncertainty. Fig. 1 \n\n\nTemperature Scaling vs. Confidence Penalty\n\nLow entropy output distributions are penalized by adding the negative entropy H of the softmax output to the negative log-likelihood training objective, weighted by an additional hyperparameter \u03b2. This leads to the following optimization function: We reproduce the experiment of Pereyra et al. on supervised image classification [8] and compare the goodness of calibration of confidence and uncertainty to our presented approach. DenseNet-121 with dropout is trained on CIFAR-10 as described in \u00a7 A.3. We fix \u03b2 = 0.1 for CP loss and omit data augmentation for this experiment as presented in [8]. Fig. 2 compares training with confidence penalty to our approach. CP reduces miscalibration (ECE = 5.20 % without CP vs. ECE = 3.37 % with CP for DenseNet-121). However, it is not as effective as TS and still produces largely miscalibrated uncertainty. A combination of CP during training and subsequent TS is conceivable and could possibly lead to an even better calibration. We have not followed this approach yet.\nL CP (w) = \u2212 X ,Y log p w (y|x) \u2212 \u03b2 H (p w (y|x)) .(10)\n\nExample: Rejection of Uncertain Predictions\n\nAn example application of well-calibrated prediction uncertainty is the rejection of uncertain predictions. We define an uncertainty threshold H max and reject all predictions from the test set wher\u1ebd H(p) > H max . A decrease in false predictions of the remaining test set is expected. Fig. 3 shows the top-1 error as a function of decreasing H max . For both uncalibrated and calibrated uncertainty, decreasing H max reduces the top-1 error. Using calibrated uncertainty, the relationship is almost linear (for H max < 0.8), allowing robust rejection of uncertain predictions.\n\n\nConclusion\n\nTemperature scaling calibrates uncertainty obtained by dropout variational inference with high effectiveness. The experimental results confirm the hypothesis that the presented approach yields better calibrated uncertainty. In addition, substantially better calibrated softmax probability was achieved. MC dropout TS is simple to implement, more effective than confidence penalty during training and the scaling does not change the maximum of the output of a network, thus model accuracy is not compromised. Therefore, it is an obvious choice in Bayesian deep learning with dropout variational inference because well-calibrated uncertainties are of utmost importance for safety-critical decision-making. However, there are many factors (e. g. network architecture, weight decay, dropout configuration) influencing the uncertainty in Bayesian deep learning that have not been discussed in this paper and are open to future work.\n\n\nA Appendix\n\n\nA.1 Expected Uncertainty Calibration Error\n\nWe restate the definition of Expected Uncertainty Calibration Error (UCE) from Eq. (8):\nUCE = M m=1 |B m | n err(B m ) \u2212 uncert(B m ) .\nThe error per bin is defined as\nerr(B m ) = 1 |B m | i\u2208Bm 1(\u0177 i = y) ,(11)\nwhere 1(\u0177 i = y) = 1 and 1(\u0177 i = y) = 0. Uncertainty per bin is defined as\nuncert(B m ) = 1 |B m | i\u2208BmH (p i ) .(12)\n\nA.2 Temperature Scaling with Monte Carlo Dropout\n\nTemperature scaling with MC dropout variational inference is derived by closely following the derivation of frequentist temperature scaling in the appendix of [5]. Let {z 1,j , . . . , z N,j } be a set of logit vectors obtained by MC dropout with N stochastic forward passes for each input x j \u2208 {x 1 , . . . , x M } with true labels {y 1 , . . . , y M }. Temperature scaling is the solutionp to entropy maximization\nmax p \u2212 1 N N i=1 M j=1 C c=1p (z i,j ) (c) logp (z i,j ) (c) ,(13)subject top (z i,j ) (c) \u2265 0 \u2200i, j, c ,(14)C c=1p (z j ) (c) = 1 \u2200j ,(15)1 N N i=1 M j=1 z (yj ) i,j = 1 N N i=1 M j=1 C c=1 z (c) i,jp (z i,j ) (c) .(16)\nGuo et al. solve this constrained optimization problem with the method of Lagrange multipliers. We skip reviewing their proof as one can see that the solution top in the case of MC dropout integration provides\n1 N N i=1p i (z j ) (c) = 1 N N i=1 e \u03bbz (c) i,j C =1 e \u03bbz ( ) i,j(17)= 1 N N i=1 \u03c3 SM (\u03bbf wi (x j )) (c) ,(18)\nwhich recovers temperature scaling for \u03bb = T \u22121 [5]. T is optimized with respect to negative log-likelihood on the validation set using MC dropout.\n\n\nA.3 Training Settings\n\nThe model implementations from PyTorch 1.2 [19] are used and trained with following settings:\n\n\u2022 batch size of 256\n\n\u2022 AdamW optimizer [20] with initial learn rate of 0.01 and \u03b2 1 = 0.9, \u03b2 2 = 0.999\n\n\u2022 weight decay of 0.01 \u2022 negative-log likelihood (cross entropy) loss \u2022 reduce-on-plateau learn rate scheduler (patience of 10 epochs) with factor of 0.1 \u2022 additional validation set is randomly extracted from the training set (5000 samples)\n\n\u2022 dropout with probability of 0.5 before the last linear layer was used in all models during training \u2022 in MC dropout, N = 25 forward passes with dropout probability of 0.5 were performed\n\nCode is available at: https://github.com/mlaves/bayesian-temperature-scaling.\n\n\nA.4 Additional Reliability Diagrams\n\nIn this section, reliability diagrams for the other data set/model combinations from Tab. 1 are visualized to provide additional insight into the calibration performance. The proposed method is able to calibrate all models with respect to both UCE and ECE across all bins.   \n\nFigure 1 :\n1Reliability diagrams (M = 15 bins) for ResNet-101 on CIFAR-100. Top row: Uncalibrated frequentist confidence (left), and confidence and uncertainty obtained by dropout variational inference (right). Bottom row: Results from calibration with TS. Dashed lines denote perfect calibration.\n\nFigure 2 :\n2Reliability diagrams (M = 15 bins) for DenseNet-121 on CIFAR-10. Top row: Training with confidence penalty. Bottom row: TS calibrated (trained without confidence penalty).\n\nFigure 3 :\n3The effect of the uncertainty threshold H max on the test set error for the rejection of uncertain predictions (orange: uncalibrated, blue: TS calibrated). As H max decreases, more uncertain predictions are rejected, which results in a lower error.\n\nFigure 4 :\n4Reliability diagrams (M = 15 bins) for ResNet-18 on CIFAR-10.\n\nFigure 5 :\n5Reliability diagrams (M = 15 bins) for DenseNet-169 on CIFAR-100.\n\nTable 1 :\n1ECE and UCE test set results in % (M = 15 bins). 0 % means perfect calibration. In TS calibration with MC dropout the same value of T was used to report both ECE and UCE.Uncalibrated \nTS Calibrated \n\nFreq. \nMC Dropout \nFreq. MC Dropout \n\nData Set \nModel \nECE \nECE \nUCE \nECE ECE UCE \n\nCIFAR-10 \nResNet-18 \n8.95 \n8.41 \n7.60 \n1.40 \n0.47 \n5.27 \nCIFAR-100 \nResNet-101 \n29.63 24.62 30.33 3.50 \n1.92 \n2.41 \nCIFAR-100 DenseNet-169 30.62 23.98 29.62 6.10 \n2.89 \n2.69 \n\n0.0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\naccuracy \n\nECE = 3.37 \n\nConfidence Penalty \n\nFrequentist Confidence \n\n\nAcknowledgmentsThis work has received funding from European Union EFRE projects OPhonLas and ProMoPro. We thank the reviewers for their helpful comments.\nChristopher M Bishop, Pattern Recognition and Machine Learning. SpringerChristopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, ICLR. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.\n\nDropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, ICML. Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In ICML, pages 1050-1059, 2016.\n\nWhat uncertainties do we need in bayesian deep learning for computer vision? In NeurIPS. Alex Kendall, Yarin Gal, Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In NeurIPS, pages 5574-5584, 2017.\n\nOn calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, ICML. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In ICML, pages 1321-1330, 2017.\n\nConcrete dropout. Yarin Gal, Jiri Hron, Alex Kendall, NeurIPS. Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In NeurIPS, pages 3581-3590, 2017.\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, CVPR. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, pages 2818-2826, 2016.\n\nRegularizing neural networks by penalizing confident output distributions. Gabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, Geoffrey Hinton, arXiv:1701.06548arXiv:1701.06548arXiv preprintGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, page arXiv:1701.06548, 2017.\n\nTransforming classifier scores into accurate multiclass probability estimates. Bianca Zadrozny, Charles Elkan, KDD. Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In KDD, pages 694-699, 2002.\n\nProbabilistic outputs for support vector machines and comparisons to regularized likelihood methods. John C Platt, Advances in Large Margin Classifiers. John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, pages 61-74, 1999.\n\nCategorical Reparameterization with Gumbel-Softmax. Eric Jang, Shixiang Gu, Ben Poole, Bayesian Deep Learning Workshop. NeurIPSEric Jang, Shixiang Gu, and Ben Poole. Categorical Reparameterization with Gumbel-Softmax. In Bayesian Deep Learning Workshop, NeurIPS, 2016.\n\nThe concrete distribution: A continuous relaxation of discrete random variables. Chris J Maddison, Andriy Mnih, Yee Whye Teh, Bayesian Deep Learning Workshop. NeurIPSChris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In Bayesian Deep Learning Workshop, NeurIPS, 2016.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009.\n\nDropout: A simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, JMLR. 15Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15:1929-1958, 2014.\n\nObtaining Well Calibrated Probabilities Using Bayesian Binning. Gregory F Mahdi Pakdaman Naeini, Milos Cooper, Hauskrecht, AAAI. Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabili- ties Using Bayesian Binning. In AAAI, pages 2901-2907, 2015.\n\nPredicting good probabilities with supervised learning. Alexandru Niculescu, - Mizil, Rich Caruana, ICML. Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In ICML, pages 625-632, 2005.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016.\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger, CVPR. Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR, pages 2261-2269, 2017.\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NeurIPS Autodiff Workshop. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NeurIPS Autodiff Workshop, 2017.\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n", "annotations": {"author": "[{\"end\":212,\"start\":96},{\"end\":323,\"start\":213},{\"end\":446,\"start\":324},{\"end\":563,\"start\":447}]", "publisher": null, "author_last_name": "[{\"end\":114,\"start\":109},{\"end\":225,\"start\":220},{\"end\":345,\"start\":337},{\"end\":462,\"start\":454}]", "author_first_name": "[{\"end\":108,\"start\":96},{\"end\":219,\"start\":213},{\"end\":336,\"start\":324},{\"end\":453,\"start\":447}]", "author_affiliation": "[{\"end\":211,\"start\":143},{\"end\":322,\"start\":254},{\"end\":445,\"start\":377},{\"end\":562,\"start\":494}]", "title": "[{\"end\":93,\"start\":1},{\"end\":656,\"start\":564}]", "venue": null, "abstract": "[{\"end\":1571,\"start\":658}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1892,\"start\":1889},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1894,\"start\":1892},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1982,\"start\":1979},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2133,\"start\":2130},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2135,\"start\":2133},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2245,\"start\":2242},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2247,\"start\":2245},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2405,\"start\":2402},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2517,\"start\":2514},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2569,\"start\":2566},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3102,\"start\":3099},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3397,\"start\":3394},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3730,\"start\":3727},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3752,\"start\":3748},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3915,\"start\":3912},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4123,\"start\":4119},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4126,\"start\":4123},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4574,\"start\":4570},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5866,\"start\":5863},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5868,\"start\":5866},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6007,\"start\":6004},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6200,\"start\":6196},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6318,\"start\":6315},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6320,\"start\":6318},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6558,\"start\":6555},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7334,\"start\":7330},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7336,\"start\":7334},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7722,\"start\":7719},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8262,\"start\":8259},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8388,\"start\":8385},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8438,\"start\":8434},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8471,\"start\":8467},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8497,\"start\":8493},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8669,\"start\":8666},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8972,\"start\":8969},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9524,\"start\":9520},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9527,\"start\":9524},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10091,\"start\":10088},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10354,\"start\":10351},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12996,\"start\":12993},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13846,\"start\":13843},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14015,\"start\":14011},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14106,\"start\":14102}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":15289,\"start\":14991},{\"attributes\":{\"id\":\"fig_3\"},\"end\":15474,\"start\":15290},{\"attributes\":{\"id\":\"fig_4\"},\"end\":15736,\"start\":15475},{\"attributes\":{\"id\":\"fig_6\"},\"end\":15811,\"start\":15737},{\"attributes\":{\"id\":\"fig_7\"},\"end\":15890,\"start\":15812},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":16469,\"start\":15891}]", "paragraph": "[{\"end\":2248,\"start\":1587},{\"end\":2881,\"start\":2250},{\"end\":4127,\"start\":2883},{\"end\":5011,\"start\":4129},{\"end\":6042,\"start\":5023},{\"end\":6436,\"start\":6082},{\"end\":6765,\"start\":6475},{\"end\":6896,\"start\":6823},{\"end\":7046,\"start\":6938},{\"end\":7516,\"start\":7095},{\"end\":7845,\"start\":7614},{\"end\":7992,\"start\":7895},{\"end\":8101,\"start\":8045},{\"end\":8263,\"start\":8159},{\"end\":8564,\"start\":8265},{\"end\":8772,\"start\":8566},{\"end\":9010,\"start\":8814},{\"end\":9437,\"start\":9036},{\"end\":9712,\"start\":9465},{\"end\":10772,\"start\":9759},{\"end\":11452,\"start\":10875},{\"end\":12394,\"start\":11467},{\"end\":12541,\"start\":12454},{\"end\":12621,\"start\":12590},{\"end\":12739,\"start\":12665},{\"end\":13250,\"start\":12834},{\"end\":13682,\"start\":13473},{\"end\":13942,\"start\":13795},{\"end\":14061,\"start\":13968},{\"end\":14082,\"start\":14063},{\"end\":14165,\"start\":14084},{\"end\":14407,\"start\":14167},{\"end\":14596,\"start\":14409},{\"end\":14675,\"start\":14598},{\"end\":14990,\"start\":14715}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6081,\"start\":6043},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6474,\"start\":6437},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6822,\"start\":6766},{\"attributes\":{\"id\":\"formula_3\"},\"end\":6937,\"start\":6897},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7565,\"start\":7517},{\"attributes\":{\"id\":\"formula_5\"},\"end\":7613,\"start\":7565},{\"attributes\":{\"id\":\"formula_6\"},\"end\":7894,\"start\":7846},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8044,\"start\":7993},{\"attributes\":{\"id\":\"formula_8\"},\"end\":8813,\"start\":8773},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10828,\"start\":10773},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12589,\"start\":12542},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12664,\"start\":12622},{\"attributes\":{\"id\":\"formula_12\"},\"end\":12782,\"start\":12740},{\"attributes\":{\"id\":\"formula_13\"},\"end\":13318,\"start\":13251},{\"attributes\":{\"id\":\"formula_14\"},\"end\":13361,\"start\":13318},{\"attributes\":{\"id\":\"formula_15\"},\"end\":13391,\"start\":13361},{\"attributes\":{\"id\":\"formula_16\"},\"end\":13472,\"start\":13391},{\"attributes\":{\"id\":\"formula_17\"},\"end\":13753,\"start\":13683},{\"attributes\":{\"id\":\"formula_18\"},\"end\":13794,\"start\":13753}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1585,\"start\":1573},{\"attributes\":{\"n\":\"2\"},\"end\":5021,\"start\":5014},{\"attributes\":{\"n\":\"2.1\"},\"end\":7093,\"start\":7049},{\"attributes\":{\"n\":\"2.2\"},\"end\":8157,\"start\":8104},{\"attributes\":{\"n\":\"3\"},\"end\":9034,\"start\":9013},{\"attributes\":{\"n\":\"3.1\"},\"end\":9463,\"start\":9440},{\"attributes\":{\"n\":\"3.2\"},\"end\":9757,\"start\":9715},{\"attributes\":{\"n\":\"3.3\"},\"end\":10873,\"start\":10830},{\"attributes\":{\"n\":\"4\"},\"end\":11465,\"start\":11455},{\"end\":12407,\"start\":12397},{\"end\":12452,\"start\":12410},{\"end\":12832,\"start\":12784},{\"end\":13966,\"start\":13945},{\"end\":14713,\"start\":14678},{\"end\":15002,\"start\":14992},{\"end\":15301,\"start\":15291},{\"end\":15486,\"start\":15476},{\"end\":15748,\"start\":15738},{\"end\":15823,\"start\":15813},{\"end\":15901,\"start\":15892}]", "table": "[{\"end\":16469,\"start\":16073}]", "figure_caption": "[{\"end\":15289,\"start\":15004},{\"end\":15474,\"start\":15303},{\"end\":15736,\"start\":15488},{\"end\":15811,\"start\":15750},{\"end\":15890,\"start\":15825},{\"end\":16073,\"start\":15903}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8406,\"start\":8390},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9711,\"start\":9705},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10362,\"start\":10356},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11167,\"start\":11161}]", "bib_author_first_name": "[{\"end\":16635,\"start\":16624},{\"end\":16637,\"start\":16636},{\"end\":16812,\"start\":16811},{\"end\":16826,\"start\":16823},{\"end\":17026,\"start\":17021},{\"end\":17038,\"start\":17032},{\"end\":17302,\"start\":17298},{\"end\":17317,\"start\":17312},{\"end\":17511,\"start\":17506},{\"end\":17522,\"start\":17517},{\"end\":17533,\"start\":17531},{\"end\":17545,\"start\":17539},{\"end\":17547,\"start\":17546},{\"end\":17723,\"start\":17718},{\"end\":17733,\"start\":17729},{\"end\":17744,\"start\":17740},{\"end\":17925,\"start\":17916},{\"end\":17942,\"start\":17935},{\"end\":17960,\"start\":17954},{\"end\":17971,\"start\":17968},{\"end\":17988,\"start\":17980},{\"end\":18260,\"start\":18253},{\"end\":18276,\"start\":18270},{\"end\":18288,\"start\":18285},{\"end\":18306,\"start\":18300},{\"end\":18323,\"start\":18315},{\"end\":18684,\"start\":18678},{\"end\":18702,\"start\":18695},{\"end\":18964,\"start\":18960},{\"end\":18966,\"start\":18965},{\"end\":19245,\"start\":19241},{\"end\":19260,\"start\":19252},{\"end\":19268,\"start\":19265},{\"end\":19545,\"start\":19540},{\"end\":19547,\"start\":19546},{\"end\":19564,\"start\":19558},{\"end\":19579,\"start\":19571},{\"end\":19867,\"start\":19863},{\"end\":19888,\"start\":19880},{\"end\":20069,\"start\":20063},{\"end\":20090,\"start\":20082},{\"end\":20103,\"start\":20099},{\"end\":20120,\"start\":20116},{\"end\":20138,\"start\":20132},{\"end\":20422,\"start\":20415},{\"end\":20424,\"start\":20423},{\"end\":20453,\"start\":20448},{\"end\":20708,\"start\":20699},{\"end\":20721,\"start\":20720},{\"end\":20733,\"start\":20729},{\"end\":20933,\"start\":20926},{\"end\":20945,\"start\":20938},{\"end\":20961,\"start\":20953},{\"end\":20971,\"start\":20967},{\"end\":21160,\"start\":21157},{\"end\":21174,\"start\":21168},{\"end\":21187,\"start\":21180},{\"end\":21210,\"start\":21204},{\"end\":21212,\"start\":21211},{\"end\":21421,\"start\":21417},{\"end\":21433,\"start\":21430},{\"end\":21448,\"start\":21441},{\"end\":21466,\"start\":21459},{\"end\":21481,\"start\":21475},{\"end\":21495,\"start\":21488},{\"end\":21510,\"start\":21504},{\"end\":21521,\"start\":21516},{\"end\":21537,\"start\":21533},{\"end\":21550,\"start\":21546},{\"end\":21848,\"start\":21844},{\"end\":21866,\"start\":21861}]", "bib_author_last_name": "[{\"end\":16644,\"start\":16638},{\"end\":16821,\"start\":16813},{\"end\":16833,\"start\":16827},{\"end\":16842,\"start\":16835},{\"end\":17030,\"start\":17027},{\"end\":17049,\"start\":17039},{\"end\":17310,\"start\":17303},{\"end\":17321,\"start\":17318},{\"end\":17515,\"start\":17512},{\"end\":17529,\"start\":17523},{\"end\":17537,\"start\":17534},{\"end\":17558,\"start\":17548},{\"end\":17727,\"start\":17724},{\"end\":17738,\"start\":17734},{\"end\":17752,\"start\":17745},{\"end\":17933,\"start\":17926},{\"end\":17952,\"start\":17943},{\"end\":17966,\"start\":17961},{\"end\":17978,\"start\":17972},{\"end\":17994,\"start\":17989},{\"end\":18268,\"start\":18261},{\"end\":18283,\"start\":18277},{\"end\":18298,\"start\":18289},{\"end\":18313,\"start\":18307},{\"end\":18330,\"start\":18324},{\"end\":18693,\"start\":18685},{\"end\":18708,\"start\":18703},{\"end\":18972,\"start\":18967},{\"end\":19250,\"start\":19246},{\"end\":19263,\"start\":19261},{\"end\":19274,\"start\":19269},{\"end\":19556,\"start\":19548},{\"end\":19569,\"start\":19565},{\"end\":19583,\"start\":19580},{\"end\":19878,\"start\":19868},{\"end\":19895,\"start\":19889},{\"end\":20080,\"start\":20070},{\"end\":20097,\"start\":20091},{\"end\":20114,\"start\":20104},{\"end\":20130,\"start\":20121},{\"end\":20152,\"start\":20139},{\"end\":20446,\"start\":20425},{\"end\":20460,\"start\":20454},{\"end\":20472,\"start\":20462},{\"end\":20718,\"start\":20709},{\"end\":20727,\"start\":20722},{\"end\":20741,\"start\":20734},{\"end\":20936,\"start\":20934},{\"end\":20951,\"start\":20946},{\"end\":20965,\"start\":20962},{\"end\":20975,\"start\":20972},{\"end\":21166,\"start\":21161},{\"end\":21178,\"start\":21175},{\"end\":21202,\"start\":21188},{\"end\":21223,\"start\":21213},{\"end\":21428,\"start\":21422},{\"end\":21439,\"start\":21434},{\"end\":21457,\"start\":21449},{\"end\":21473,\"start\":21467},{\"end\":21486,\"start\":21482},{\"end\":21502,\"start\":21496},{\"end\":21514,\"start\":21511},{\"end\":21531,\"start\":21522},{\"end\":21544,\"start\":21538},{\"end\":21556,\"start\":21551},{\"end\":21859,\"start\":21849},{\"end\":21873,\"start\":21867}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":16776,\"start\":16624},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":216078090},\"end\":16933,\"start\":16778},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":160705},\"end\":17207,\"start\":16935},{\"attributes\":{\"id\":\"b3\"},\"end\":17462,\"start\":17209},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":28671436},\"end\":17698,\"start\":17464},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":19840332},\"end\":17855,\"start\":17700},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206593880},\"end\":18176,\"start\":17857},{\"attributes\":{\"doi\":\"arXiv:1701.06548\",\"id\":\"b7\"},\"end\":18597,\"start\":18178},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3349576},\"end\":18857,\"start\":18599},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":56563878},\"end\":19187,\"start\":18859},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2428314},\"end\":19457,\"start\":19189},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14307651},\"end\":19806,\"start\":19459},{\"attributes\":{\"id\":\"b12\"},\"end\":19994,\"start\":19808},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6844431},\"end\":20349,\"start\":19996},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6292807},\"end\":20641,\"start\":20351},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207158152},\"end\":20878,\"start\":20643},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206594692},\"end\":21113,\"start\":20880},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9433631},\"end\":21377,\"start\":21115},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":40027675},\"end\":21803,\"start\":21379},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53592270},\"end\":21968,\"start\":21805}]", "bib_title": "[{\"end\":16809,\"start\":16778},{\"end\":17019,\"start\":16935},{\"end\":17504,\"start\":17464},{\"end\":17716,\"start\":17700},{\"end\":17914,\"start\":17857},{\"end\":18676,\"start\":18599},{\"end\":18958,\"start\":18859},{\"end\":19239,\"start\":19189},{\"end\":19538,\"start\":19459},{\"end\":20061,\"start\":19996},{\"end\":20413,\"start\":20351},{\"end\":20697,\"start\":20643},{\"end\":20924,\"start\":20880},{\"end\":21155,\"start\":21115},{\"end\":21415,\"start\":21379},{\"end\":21842,\"start\":21805}]", "bib_author": "[{\"end\":16646,\"start\":16624},{\"end\":16823,\"start\":16811},{\"end\":16835,\"start\":16823},{\"end\":16844,\"start\":16835},{\"end\":17032,\"start\":17021},{\"end\":17051,\"start\":17032},{\"end\":17312,\"start\":17298},{\"end\":17323,\"start\":17312},{\"end\":17517,\"start\":17506},{\"end\":17531,\"start\":17517},{\"end\":17539,\"start\":17531},{\"end\":17560,\"start\":17539},{\"end\":17729,\"start\":17718},{\"end\":17740,\"start\":17729},{\"end\":17754,\"start\":17740},{\"end\":17935,\"start\":17916},{\"end\":17954,\"start\":17935},{\"end\":17968,\"start\":17954},{\"end\":17980,\"start\":17968},{\"end\":17996,\"start\":17980},{\"end\":18270,\"start\":18253},{\"end\":18285,\"start\":18270},{\"end\":18300,\"start\":18285},{\"end\":18315,\"start\":18300},{\"end\":18332,\"start\":18315},{\"end\":18695,\"start\":18678},{\"end\":18710,\"start\":18695},{\"end\":18974,\"start\":18960},{\"end\":19252,\"start\":19241},{\"end\":19265,\"start\":19252},{\"end\":19276,\"start\":19265},{\"end\":19558,\"start\":19540},{\"end\":19571,\"start\":19558},{\"end\":19585,\"start\":19571},{\"end\":19880,\"start\":19863},{\"end\":19897,\"start\":19880},{\"end\":20082,\"start\":20063},{\"end\":20099,\"start\":20082},{\"end\":20116,\"start\":20099},{\"end\":20132,\"start\":20116},{\"end\":20154,\"start\":20132},{\"end\":20448,\"start\":20415},{\"end\":20462,\"start\":20448},{\"end\":20474,\"start\":20462},{\"end\":20720,\"start\":20699},{\"end\":20729,\"start\":20720},{\"end\":20743,\"start\":20729},{\"end\":20938,\"start\":20926},{\"end\":20953,\"start\":20938},{\"end\":20967,\"start\":20953},{\"end\":20977,\"start\":20967},{\"end\":21168,\"start\":21157},{\"end\":21180,\"start\":21168},{\"end\":21204,\"start\":21180},{\"end\":21225,\"start\":21204},{\"end\":21430,\"start\":21417},{\"end\":21441,\"start\":21430},{\"end\":21459,\"start\":21441},{\"end\":21475,\"start\":21459},{\"end\":21488,\"start\":21475},{\"end\":21504,\"start\":21488},{\"end\":21516,\"start\":21504},{\"end\":21533,\"start\":21516},{\"end\":21546,\"start\":21533},{\"end\":21558,\"start\":21546},{\"end\":21861,\"start\":21844},{\"end\":21875,\"start\":21861}]", "bib_venue": "[{\"end\":16686,\"start\":16646},{\"end\":16848,\"start\":16844},{\"end\":17055,\"start\":17051},{\"end\":17296,\"start\":17209},{\"end\":17564,\"start\":17560},{\"end\":17761,\"start\":17754},{\"end\":18000,\"start\":17996},{\"end\":18251,\"start\":18178},{\"end\":18713,\"start\":18710},{\"end\":19010,\"start\":18974},{\"end\":19307,\"start\":19276},{\"end\":19616,\"start\":19585},{\"end\":19861,\"start\":19808},{\"end\":20158,\"start\":20154},{\"end\":20478,\"start\":20474},{\"end\":20747,\"start\":20743},{\"end\":20981,\"start\":20977},{\"end\":21229,\"start\":21225},{\"end\":21583,\"start\":21558},{\"end\":21879,\"start\":21875},{\"end\":19316,\"start\":19309},{\"end\":19625,\"start\":19618}]"}}}, "year": 2023, "month": 12, "day": 17}