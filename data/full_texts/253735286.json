{"id": 253735286, "updated": "2023-10-05 08:22:53.572", "metadata": {"title": "Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification", "authors": "[{\"first\":\"Yue\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Artemis\",\"last\":\"Panagopoulou\",\"middle\":[]},{\"first\":\"Shenghao\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Jin\",\"middle\":[]},{\"first\":\"Chris\",\"last\":\"Callison-Burch\",\"middle\":[]},{\"first\":\"Mark\",\"last\":\"Yatskar\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are first to show how to construct high-performance CBMs without manual specification of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to define a large space of possible bottlenecks. Given a problem domain, LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. Ultimately, GPT-3's sentential concepts can be aligned to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for concepts important to visual recognition. In the evaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot classification: they are 11.7% more accurate than black box linear probes at 1 shot and comparable with more data. Overall, LaBo demonstrates that inherently interpretable models can be widely applied at similar, or better, performance than black box approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2211.11158", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/YangPZJCY23", "doi": "10.1109/cvpr52729.2023.01839"}}, "content": {"source": {"pdf_hash": "6d1badc223cfca6ae23fcce811b9dbcb8645cdef", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.11158v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7cee08bd1125e60cc0aca79279f718efeba969bf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6d1badc223cfca6ae23fcce811b9dbcb8645cdef.txt", "contents": "\nLanguage in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification\n\n\nYue Yang yueyang1@seas.upenn.edu \nUniversity of Pennsylvania\n\n\nArtemis Panagopoulou artemisp@seas.upenn.edu \nUniversity of Pennsylvania\n\n\nShenghao Zhou shzhou2@seas.upenn.edu \nUniversity of Pennsylvania\n\n\nDaniel Jin \nUniversity of Pennsylvania\n\n\nChris Callison-Burch \nUniversity of Pennsylvania\n\n\nMark Yatskar myatskar@seas.upenn.edu \nUniversity of Pennsylvania\n\n\nLanguage in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification\n\nConcept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into humanreadable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are first to show how to construct high-performance CBMs without manual specification of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to define a large space of possible bottlenecks. Given a problem domain, LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. Ultimately, GPT-3's sentential concepts can be aligned to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for concepts important to visual recognition. In the evaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot classification: they are 11.7% more accurate than black box linear probes at 1 shot and comparable with more data. Overall, LaBo demonstrates that inherently interpretable models can be widely applied at similar, or better, performance than black box approaches.\n\nIntroduction\n\nAs deep learning systems improve, their applicability to critical domains is hampered because of a lack of transparency. Efforts to address this have largely focused on post-hoc explanations [47,54,72]. Such explanations can be problematic because they may be incomplete or unfaithful with respect to the model's computations [49]. Models  can also be designed to be inherently interpretable, but it is believed that such models will perform more poorly than their black box alternatives [16]. In this work, we provide evidence to the contrary. We show how to construct highperformance interpretable-by-design classifiers by combining a language model, , and a language-vision model, CLIP [44].\n\nOur method builds on Concept Bottleneck Models (CBM) [25], which construct predictors through a linear combination of human-designed concepts. For example, as seen in Figure 1, a qualified person can design concepts, such as \"nape color,\" as intermediate targets for a black box model before classifying a bird. CBMs provide abstractions that people can use to understand errors or intervene on, contributing to increased trust.\n\nApplication of CBMs is limited because they require costly attribute annotations by domain experts and often under-perform their black box counterparts. In contexts where CBM performance is competitive with black box alternatives, interpretability properties are sacrificed [34,70]. To address both of these challenges, we propose to build systems that automatically construct CBMs.\n\nOur Language Model Guided Concept Bottleneck Model (LaBo), Figure 2, allows for the automatic construction of high-performance CBMs for arbitrary classification problems without concept annotations. Large language models Figure 2. We present an overview of our Language-Model-Guided Concept Bottleneck Model (LaBo), which is interpretable by design image classification system. First, we prompt the large language model (GPT-3) to generate candidate concepts (Sec 3.4). Second, we employ a submodular function to select concepts from all candidates to construct the bottleneck (Sec 3.2). Third, we apply a pretrained alignment model (CLIP) to obtain the embeddings of concepts and images, which is used to compute concept scores. Finally, we train a linear function in which the weight W denotes the concept-class association user to predict targets based on concept scores (Sec 3.3).\n\n(LLMs) contain significant world knowledge [21, 42,61], that can be elicited by inputting a string prefix and allowing LLMs to complete the string (prompting). For example, in Figure 1, GPT-3 is prompted about sparrows and completes with information such as \"brown head with white stripes.\" LaBo leverages this by constructing bottlenecks where the concepts are such GPT-3 generated sentences. Since our concepts are textual, we use CLIP to score their presence in an image and form a bottleneck layer out of these scores.\n\nA key advantage of LaBo is the ability to control the selection of concepts in the bottleneck by generating candidates from the language model. We develop selection principles targeting both interpretability and classification accuracy. For example, we prefer smaller bottlenecks that include shorter sentences that do not include class names. Furthermore, to maximize performance, we prefer attributes that CLIP can easily recognize and are highly discriminative. To account for appearance variation, we select attributes that cover a variety of information and are not repetitive. We formulate these factors into a novel sub-modular criterion that allows us to select good bottlenecks efficiently [38].\n\nWe have evaluated LaBo-created bottlenecks on 11 diverse image classification tasks, spanning recognition of common objects [11,26] to skin tumors [64]. fine-grained types [3,32,39,67], textures [10], actions [59], skin tumors [64], and satellite photographed objects [8]. 2 Our 2 The only dataset specialization we perform is prompt tuning for GPT-main finding is that LaBo is a highly effective prior for what concepts to look for, especially in low data regimes. In evaluations comparing with linear probes, LaBo outperforms by as much as 11.7% at 1-shot and marginally underperforms given larger data settings. Averaged over many dataset sizes, LaBo bottlenecks are 1.5% more accurate than linear probes. In comparison to modifications of CBMs that improve performance by circumventing the bottleneck [70], we achieve similar or better results without breaking the CBM abstraction. In extensive ablations, we study key trade-offs in bottleneck design and show our selection criteria are crucial and highlight several other critical design choices.\n\nHuman evaluations indicate that our bottlenecks are largely understandable, visual, and factual. Finally, annotators find our GPT-3 sourced bottlenecks are more factual and groundable than those constructed from WordNet or Wikipedia sentences. Overall, our experiments demonstrate that automatically designed CBMs can be as effective as black box models while maintaining critical factors contributing to their interpretability.\n\n\nRelated Work\n\nBroadly, interpretability methods fall into two categories: post-hoc and by design. While ours is an instance of the latter, post-hoc methods have the advantage of not imposing any model constraints. For example, Gradient-weighted Class Activation Mapping approaches [2,19,36,54] trace network gradients to identify the input areas that guide predictions. Similarly, Explanation Generation methods [17,23,40,57] require models to produce explanations for visual tasks by conditioning their predictions on captioning models and [18,41] incorporate visual evidence to ground explanations.\n\nDespite their advantages, there is no guarantee that posthoc methods faithfully represent model reasoning [49]. In contrast, our work falls under interpretable by design methods, which constrain explanations to align with the model's reasoning. For example, Prototype methods [6, 37, 51, 58, 66] optimize a metric space that guides classification by computing distances to prototype representations of each class. While such methods identify important regions in the input for classification, they still require featurized region representations that obfuscate the semantic content of the region.\n\nThis work extends another family of interpretable by design methods known as Concept Bottleneck Models [25,52]. Following early attempts in few shot learning [27] and attribute learning [50,68], CBMs predict targets by linearly combining an intermediate layer of human-understandable attributes. Recently, Computational Derivation Learning (CompDL) [71] proposed a CBM architecture that applies a linear layer over CLIP scores between human expert designed concepts and images to predict targets in the context of an evaluation framework to measure how well CLIP grounds concepts. CBMs generally suffer from the need for costly class description annotations and lower performance compared to end-to-end counterparts. Post-hoc Concept Bottleneck (PCBM) [70] was proposed to fill these two gaps by leveraging information from a static knowledge base, such as ConceptNet [60], and adding a residual connection from image features to the final prediction to improve accuracy [70]. However, PCBMs cannot be expanded to larger-scale (e.g., ImageNet [11]) or domain-specific tasks (e.g., fine-grained [32]) because knowledge bases have limited coverage. In addition, they include a residual predictor, which effectively ensembles CBM with an end-to-end model, undermining interpretability.\n\nInspired by previous work on using textual knowledge to guide vision models [5,22,48,55], we circumvent the requirement for external knowledge bases, and instead query LLMs to collect concepts. We remove the need for direct mapping from image features to targets by fully automating the extraction and filtering of LLM knowledge. Our model surpasses end-to-end models in few shot scenarios and achieves comparable performance in large data settings, while concurrent work [35] only evaluates on zero-shot settings.\n\nOur work capitalizes on improvements in vision-language pretraining from earlier BERT-based models [7,30,31,62] to more scalable contrastive architectures [20,28,44,69], which are very effective for few shot image classification [9,63]. Our work can be viewed as interpretability-focused prompt tuning of CLIP [44]. Significant efforts have been devoted to prompting vision language models [12,14,29,43,46,73,74]. These focus on searching over text prompts to improve classification performance, and resemble earlier techniques in LLM prompt tuning [15,53,56]. Figure 2 presents an overview of our method. Our model prompts a large language model, GPT-3 [4] to generate a set of candidate concepts for each class (Section 3.4). We employ submodular optimization to greedily select a subset of concepts for each class such that we maximize discriminability and diversity (Section 3.2). We then align the selected concepts to images using CLIP [44]. We apply a linear layer over the similarity scores of concepts and images to learn a weight matrix representing the importance of each concept in the final classification. This weight matrix is initialized using a language model prior from GPT-3 (Section 3.3).\n\n\nMethod\n\n\nProblem Formulation\n\nConsider a training set of image-label pairs D = {(i, y)} where i is the image and y \u2208 Y, is a label from a set of N classes. Suppose we have a pretrained multimodal alignment model (e.g., CLIP [44]), which has an image encoder I and a text encoder T . I and T can map images and text into the shared feature space, respectively. The dot product of the image and text features reflects the alignment score between the two modalities. We extract the features of all images in D as x = I(i) \u2208 R d , and the dataset can be represented as D = {(x, y)}. Let S be the superset of candidate textual concepts generated from language models. We use a submodular function F to select a bottleneck, C, where C \u2286 S, made of N C concepts, C = {c 1 , c 2 , ..., c N C }. We can construct a bottleneck embedding, E C \u2208 R N C \u00d7d , and each row of E C is the text feature T (c) \u2208 R d of a concept c extracted by the text encoder T .\n\nConcept bottleneck models produce a prediction by composing two functions,\u0177 = f g (x, E C ) , in which g : R d \u2192 R N C maps the image feature to a score for every element of the bottleneck and f : R N C \u2192 Y makes the final prediction on the label space given the concept scores. In our setting, we find a bottleneck C and appropriate f by solving the following minimization problem:\nmin f,C E (x,y)\u223cD L f g (x, E C ) , y \u2212 F (C, D) (1)\nin which L(\u0177, y) is the cross-entropy loss on the label prediction and F(C, D) is the quality of the bottleneck as mea-sured by the submodular function. In practice, we optimize sequentially: we first find a high scoring C under F. Then, we use the dot product of image and concept embeddings as g. Finally, we find an f that minimizes L. In the following sections, we will illustrate how we: construct the submodular function F to select a subset of concepts C from the candidates S (Section 3.2) and learn f (Section 3.3).\n\n\nSubmodular Concept Selection\n\nWe create a superset of candidate concepts, S, out of class-specific subsets. For every label y \u2208 Y, we construct S y by prompting a language model to produce textual knowledge about y (Section 3.4). Instead of directly choosing N C concepts from S, we select k concepts for each class, such that N \u00d7 k = N C , to ensure each class has an equal number of relevant concepts in the bottleneck.\n\nWe employ submodular optimization [1] to select a subset C y \u2286 S y , |C y | = k. Specifically, we need to design a score function F : 2 |Sy| \u2192 R to evaluate the utility of the subset. Submodular functions should satisfy the diminishing returns property. 3 If a submodular function is monotone, 4 a greedy algorithm [38] can be used to find a solution within a constant factor of the optimal one. We propose the following monotone submodular function 5 to select the subset C y from the candidate set S y :\nF(C y ) = \u03b1 \u00b7 c\u2208Cy D(c) discriminability + \u03b2 \u00b7 c1\u2208Sy max c2\u2208Cy \u03d5(c 1 , c 2 ) coverage ,(2)\nwhere D(c) denotes the discriminability score of the concept c and \u03d5(\u00b7) is the intra-concept similarity. Generally, the first term tends to select more informative concepts, and the second term ensures the subset has good coverage of the candidate set. The hyperparameters \u03b1 and \u03b2 control the weights of the two sub-functions. Here we present how to compute these two scores:\n\nDiscriminability Score. We introduce a discriminability score to encourage the selection of concepts that are aligned with many images in class y, but few images in other classes.We first define the similarity score Sim (y, c) between a class and concept by taking the mean of the dot product between the images and text features:\nSim (y, c) = 1 |X y | x\u2208Xy x \u00b7 T (c) \u22a4 ,(3)\nwhere X y is the set of training images labeled with y 6 and T is the text encoder. We define the normalized class association, which measures the conditional likelihood of aligning featurized images of a class given a concept's textual embedding, Sim(y|c) = Sim(y, c)/ y \u2032 \u2208Y Sim(y \u2032 , c), and compute its negative entropy:\nD(c) = y \u2032 \u2208Y\nSim y \u2032 |c \u00b7 log Sim y \u2032 |c (4)\n\nMaximizing D(c) will result in the selection of concepts that have peaked Sim(y|c), indicating that a concept is strongly associated with only a few classes.\n\nCoverage Score. The second term of equation 2 is a minimax facility location function that tries to minimize the maximum distance between each element in the subset and the candidate set. For distance, we use the cosine between the features of the two concepts extracted by the text encoder: \u03d5(c 1 , c 2 ) = cos T (c 1 ), T (c 2 ) . A high coverage score yields a diverse bottleneck that covers different possible appearances for a target class.\n\n\nOptimize Class-concept Association\n\nIn this section, we explain how we compute g (the concept predictor) and learn f (the label predictor) of the bottleneck.\n\nPredict the Concept Scores. The concept predictor g is not learned in our method because the alignment model we use can measure the correlation between image and text through dot product. We treat the dot product of input image feature x and the concept space E C defined in Section 3.2 as g: g (x, E C ) = x \u00b7 E \u22a4 C , where g (x, E C ) \u2208 R N C , and each element is the score of image x on a concept.\n\nConcept Weight Matrix. We learn a linear function for the label predictor f that maps from concept scores to the final prediction. Intuitively, these weights encode the affinity of the concept to the class, allowing the model to represent that classes depend differently on the same concept. To normalize the class-concept association distributed over the weight matrix, we regularize the matrix with the softmax activation function. Concretely, we learn a concept weight matrix W \u2208 R N \u00d7N C , that is used for prediction:\ny = argmax g (x, E C ) \u00b7 \u03c3 (W ) \u22a4 , in which \u03c3(\u00b7)\nis the softmax activation which is applied along the concepts axis:\nW y,c = e W y,c / y \u2032 \u2208Y e W y \u2032 ,c .\nInitializing the Weight Matrix with Language Priors. Previous work trains the concept weight matrix freely from scratch, which is not feasible in low-resource scenarios where we don't have enough data to learn the weight effectively. To extend the application of CBM to few-shot image classification, we consider biasing the weights toward the initial association from the language model used to propose concepts. If a concept c was present in C y , we initialize the elements of W corresponding to the weight between class y and concept c to a higher value before optimization: W y,c = 1, if c \u2208 C y , otherwise 0.  \n\n\nPrepare the Candidates\n\nTo collect the candidates S to feed into our model, we prompt GPT-3 to generate relevant sentences by incorporating the class name in 5 templates shown in supplementary materials. 7 For example, as shown in the top-left of Figure  2, we prompt GPT-3 by asking \"describe what the axolotl looks like\", and the GPT-3 returns a sentence about the target class. We obtain 500 sentences for each class and automatically split these sentences into shorter concepts using a T5 model [45] fine-tuned on a small set of annotated sentenceconcept pairs. We use string match to identify and remove class name tokens in each concept. (see supplementary)\n\n\nExperimental Setup\n\nWe evaluate our method on a diverse set of 11 datasets (Section 4.1) and compare it to its end-to-end counterpart and other interpretable CBM methods (Section 4.2).\n\n\nDataset\n\nWe select a comprehensive benchmark of 11 image classification datasets spanning a diverse set of domains, including (1) Common objects: ImageNet Detailed statistics are presented in the supplementary material. We follow the few-shot evaluation protocol proposed by CLIP [44] with 1, 2, 4, 8, and 16 images randomly sampled from the training set for each class. We also evaluate in the fully-supervised setting where we train on all available images. For all experiments, we report the test accuracy.\n\n\nBaselines\n\nWe compare our model, LaBo, with black-box linear probing and two interpretable methods.\n\nLinear Probe Following previous evaluations on CBM [25,70], linear probing serves as our primary baseline for comparison. We follow the implementation of CLIP [44] by training the scikit-learn's L-BFGS logistic regression with a hyperparameter sweep on the L2 regularization weight.\n\nPCBM Post-hoc Concept Bottleneck Model [70] designs a residual modeling step that directly maps the original image embedding into the label space. PCBM treats the attributes of each class in ConceptNet [60] as concepts.\n\nCompDL Compositional Derivation Learning [71] learns a linear layer over CLIP similarity scores between humandesigned class descriptions and images to predict targets.\n\n\nImplementation Details\n\nWe prompt GPT-3-text-davinci-002 to generate concepts. The CLIP model is adapted from OpenAI's public repo with ViT-L/14 as the default vision backbone. We only use CLIP-RN50 as the backbone when comparing with PCBM, and ViT-B/32 with CompDL for a fair comparison. We implement the submodular function using the apricot package and \n\n\nEvaluation\n\n\nMain Results\n\nWe compare LaBo's performance with a linear probe and other interpretable baselines to evaluate if we can maintain black box accuracy without sacrificing interpretability.\n\nComparison with End-to-End Model. One of our goals is to close the performance gap between interpretable and black box models. Table 1 reports the mean test accuracy of LaBo and the linear probe on 11 datasets. LaBo significantly outperforms the end-to-end model when little data is available and continues to be competitive as the number of data increases. On average, LaBo surpasses the linear probe by 1.5%. Figure 3 provides analytic performance comparisons between LaBo and Linear Probe on each dataset.\n\nIn general, LaBo's performance depends on the quality of knowledge extracted from GPT-3. For common categories, GPT-3 contains high-quality knowledge allowing substantial improvement over linear probes. For some fine-grained datasets, such as Flower-102, GPT-3's knowledge is largely non-visual, as seen in Figure 7. In such cases, specialized language models could be used to improve LaBo.\n\nComparison with other Interpretable Methods.  Table 5. Ablation results on concept selection methods. We report mean test accuracy on 11 datasets. LaBo outperforms PCBM by 3.4% on CIFAR-10 and 13.1% on CIFAR-100. LaBo maintains comparable performance to PCBM with a residual predictor (PCBM-h), without circumventing the bottleneck. In Table 3, LaBo is more accurate than CompDL [71] without manually constructed concepts.\n\n\nAblation Study\n\nWe evaluate the importance of each of our model's components on final performance. Specifically, we compare results with different concept selection methods, language and random weight initialization, and bottleneck sizes.\n\nConcept Selection Methods. We compare our submodular function with four concept selection methods: (1) RANDOM: we randomly sample a subset of concepts from the candidates for each class; (2) SIMILARITY: we select the top concepts ranked by their similarity scores with the class calculated by equation 3; (3) COVERAGE: we only consider the coverage score for concept selection; (4) DISCRIM: we only consider the discriminability score for concept selection. As shown in Table 5, our submodular function, which jointly optimizes coverage and discriminability, achieves the best performance across different numbers of shots. We notice that using coverage or discriminability alone still outperforms using similarity between the class and random selection. The selection method plays an important role in all data settings, but its impact decreases with more supervision.\n\nInitialization with Language Priors. We deactivate the LM initialization and use random initialization instead. Figure 4 shows that the LM prior is more important for low shot settings since there is less signal to guide concept importance.\n\nBottleneck Size. In Table 4 Groundability Figure 5. Human evaluation on Factuality and Groundability for different bottlenecks on ImageNet. \"w/o Submod\" denotes without submodular function, i.e., random concept selection. \"w/o LM\" denotes no language prior weight initialization.\n\nselected by the submodular function. Larger bottlenecks are usually better, but with more data, similar performance is achievable with smaller bottlenecks.\n\n\nHuman Evaluation\n\nIt is important for interpretability that the vision-language alignment model correctly grounds concepts to images. For example, if a concept \"usually round\" ranks both circles and stripes highly, the name of the attribute does not faithfully represent the computation. In addition, it is important that the automatically generated concept bottlenecks factually correspond to the class they describe. To this end, we introduce two metrics to evaluate the quality of our concept bottleneck items: (1) Factuality measures how accurate the concepts are in describing their designated class by requiring annotators to judge whether they describe ground truth images, and (2) Groundability measures how consistent the vision-language model grounding of the concepts to images with human interpretations by requiring annotators to judge their applicability on the top-10 images ranked by CLIP alignment scores.\n\nSetup. Both metrics are computed by asking annotators to select images that describe a highly ranked concept in our bottlenecks. Formally, the two metrics are represented by:  Groundability(c) = number of images selected top-k aligned images of the concept where we set k = 10. 8 In addition to the two main metrics, we ask the annotator to select whether the concept is nonvisual, nonsensical, or contains unknown vocabulary. We randomly sample 20 classes for each dataset and evaluate the top 5 concepts (ranked by the weights of the linear function) for each class, 100 concepts per dataset. We release our human evaluation task on Amazon Mechanical Turk and collect three annotations for each concept. More details on the task and the results can be found in the supplement.\n\nBaselines. We evaluate the bottlenecks under full supervision and compare them with two main baselines: (1) LaBo (w/o Submod), which randomly selects the concepts instead of using the submodular function, and (2) LaBo (w/o LM), which initializes the concept weight matrix randomly without leveraging the priors of the language model. For Ima-geNet, we add two additional baselines using human-written text: (1) WordNet [13] definitions and (2) Wikipedia sentences [22]. We adopt the same preprocessing pipeline as LaBo to extract concepts from human-written resources and utilize the submodular function to select the bottlenecks.\n\nResults. Figure 5 shows the evaluation on ImageNet, and we observe that LaBo has significantly higher Factuality and Groundability than human-written text. We further observe that removing components from our system (submodular and LM Prior) hurt both human evaluation metrics, indicating their collective importance in our system. In addition, Figure  Class   6 shows that LaBo has significantly fewer invalid concepts than other baselines. Table 6 summarizes the average human evaluation results over the 11 datasets 9 . On average, we observe a trade-off between Factuality and Groundability. Increasing coverage and discriminability leads to more variable and specific concepts that CLIP finds more difficult to ground. This could be due to challenges in capturing composite concepts [33,71]. For individual analysis of the datasets, refer to the supplementary material. Finally, Figure  7 shows several CBMs we constructed. Across many types of tasks, the bottlenecks are largely coherent, factual, and groundable by CLIP.\n\n\nConclusion and Limitation\n\nOverall, our approach demonstrates that the accuracy and interpretability of vision systems may be less at odds than previously believed. Leveraging LLMs was crucial, as they encode important visual knowledge. In the future, our approach can easily be enriched with new factors that capture different priors on bottleneck construction. The limits of knowledge in GPT-3 are not known, but likely there are 9 The low resolution of CIFAR images partially affects those metrics since annotators have greater difficulty in completing the task. domains where prompting generates few useful facts. Even in contexts where GPT-3 can generate useful information, our method depends on CLIP being able to recognize those aspects in images. The alignment between GPT-3 and CLIP likely does not hold for all cases. Future work could focus on dynamically prompting GPT-3 to make this coupling more robust. Finally, our work depends on large models trained at scales that are not currently reproducible. It is possible unrevealed aspects of training by OpenAI will require a reevaluation of our claims.  Table 7 depicts detailed statistics for all datasets. For each dataset, we provide in parentheses a one-word description of the type of classes it contains, which we refer to as super class of a dataset. We use the same train/dev/test splits of Food-101, Aircraft, Flower-102, UCF-101, and DTD provided by CoOp [74]. For CUB, we randomly sample 10 training images for each category as the development set. For CIFAR-10 and CIFAR-100, we randomly split 10% of the training data as the dev set. For HAM10000, we adopt 80/10/10 splits on the images of each class. For ImageNet, we only evaluate the dev set.  Table 7. Detailed statistics of the 11 datasets. The text in parentheses that follows the dataset name corresponds to the super class name, which is used to remove class names in concepts.\n\n\nReferences\n\n\nA. Dataset Statistics\n\n\nB. Implementation Details\n\n\nB.1. Linear Probe\n\nFollowing CLIP's implementation of Linear Probe, we use the encoded images, before their projection to the visiontext embedding space, as input to the classifier. We use sklearn's L-BFGS implementation of logistic regression with 1,000 maximum iterations. To determine the best performing values for the L2 regularization strength C, we perform binary search on the validation set initialized with [1e 6 , 1e 4 , 1e 2 , 1, 1e \u22122 , 1e \u22124 , 1e \u22126 ]. After determining the left and right bounds of C, we iteratively halve the interval with 8 steps to get the final hyperparameter value. We compare our Linear Probe results on ImageNet with CoOp. To perform a fair comparison, we select CLIP-RN50 as the vision encoder and perform 3 random runs to select the few shot images. As shown in Table 8, we marginally outperform CoOp in all data settings. Table 9 presents the prompts used to query GPT-3. We design 5 general prompts and 5 additional prompts for UCF-101. The general prompts are used for all datasets, with a slight modification: we add the super-class name that de- scribes the type of data present in more fine-grained datasets. For example, when prompting for Flower-102, we add the super class name flower after each class name. In this way we reduce ambiguity problems: e.g., for the class bishop of llandaff, without the super class name, GPT-3 returns results for bishop instead of the flower. While this approach reduces ambiguities, it does not completely eliminate them. For example, we found that GPT-3 generates sentences about the mouse (device), but in fact, the class mouse on ImageNet refers to the animal. Future work can explore better prompting methods, such as providing a detailed definition for each class or designing customized prompts for each dataset. Table 9. The prompt templates used to generate the raw sentences from GPT-3. The UCF-101 has a different set of prompts, while the other datasets share the same set of general templates.\n\n\nB.2. Prompt\n\n\nB.3. T5 concept extractor\n\nThe raw outputs of language models are long sentences and sometimes contain class names that need to be removed from the bottlenecks for the sake of interpretability. For example, GPT-3 generates a sentence \"The hen is brown and has a white chest.\" for the class hen, which could be decomposed to two concepts: \"brown\" and \"white chest\". We annotate a random sample of 100 sentence-concepts pairs from each of the following datasets: Food-101, CIFAR-100, Aircraft, Flower, and ImageNet. In total, we collect 500 sentences. An example annotation is depicted below:\n\nThe 737-400 has a long and slender fuselage with tapered wings and a small tail. (737-400) long and slender fuselage; tapered wings; small tail The class name is concatenated with the raw sentence, and the concepts are separated by semicolons. We train a T5large model [45] using the Huggingface API. We add a task prefix -\"extract concepts from sentence: \" for each example. We train the model with Adam optimizer for 5 epochs, setting the batch size to 8 and learning rate to 1e \u22125 .\n\n\nB.4. Remove Class Name\n\nAfter extracting the short concepts using T5, some still contain class names. To ensure there are no class names in the bottleneck, we design two heuristics: (1) If we find the class name in the concept using string match, we replace it with the super class name 10 , e.g., the concept \"leaves of the orange dahlia are long and narrow\" for the class orange dahlia in Flower-102 is modified as \"leaves of the flower are long and narrow\".\n\n(2) For class names with multiple tokens, the tokens are not always in the same order as the class name. In this case, if a concept with all tokens for the class name is present, we remove it. For instance, the concept \"a cake made of carrot\" for the class carrot cake will be deleted. The two heuristics are applied to each concept by considering all class names in the dataset. 10 The super class name depends on the datasets. For example, the super class name for the Flower-102 dataset is flower (see Table 7).\n\n\nB.5. Hyperparameters\n\nWe apply grid search with 5 runs to find the best weights for the submodular function for different datasets and shots. We determine the learning rate and batch size by monitoring the validation accuracy with wandb. Table 16 lists all the hyperparameters of our best-performing models.\n\n\nB.6. Other Details\n\nGPT-3 Generation. Generating 500 sentences for one class takes around 5 minutes by calling the OpenAI APIs. The price of GPT-3-Davinci is $ 0.02 / 1k tokens, and it costs about $ 0.2 for each class.\n\nRunning Time. Because we use CLIP with frozen weights, we only need to extract the image features once and reuse them in the rest experiments. Since we only fit a single linear layer, our training time is low. For example, training the full ImageNet for one epoch on an NVIDIA RTX A6000 takes less than 1 minute.\n\nFull Results. The full numerical results are shown in Table  10. Both validation and test accuracy are provided.\n\n\nC. Additional Analysis\n\n\nC.1. Activation Function\n\nWe ablate the impact of the softmax activation by removing it or replacing it with other activation functions such as   Table 11, not using an activation function significantly hurts performance, while using other activation functions performs poorly compared to softmax.\n\n\nReLU and sigmoid. As shown in\n\n\nC.2. Language Model Size vs. Performace\n\nWe experiment with different sizes of GPT-3: Curie, Babbage, and Ada (sorted from larger to smaller). Figure 12 compares the different GPT-3 variants on ImageNet, showing that larger language models result in better performance, especially in the few show settings. However, there is only a marginal difference in performance when enough data is available. Table 13 compares the performance of LaBo between using GPT-3 generated concepts and human-designed concepts sourced from WordNet and Wikipedia. We observe that GPT-3 generated concepts outperform human-written ones in 1-shot experiments, while there is less than 1% drop in performance on average in larger data settings. In addition, our human evaluation on Imagenet (see Figure 5 and 6 in Section 5.3) shows that humans judge the quality of GPT-3 generated concepts to be better than that of human-designed. We visualize the embeddings of concepts and class names using t-SNE [65] to identify the reason behind the perceived higher quality of GPT-3 concepts. We encode the 1,000 class names of ImageNet using the CLIP text encoder along with the top-1 concept of each class (1,000 concepts in total) from each bottleneck (LaBo, WordNet, and Wikipedia). Figure 8 reflects that compared to GPT-3, the embeddings of WordNet and Wikipedia concepts have a higher overlap with the embeddings of class names. In other words, Wikipedia and WordNet concepts are more likely to replicate the text features of class names rather than describe the class. This explains why human-written text has higher accuracy but is less interpretable. Table 14 compares the performance between LaBo and CoOp [74], which employs a soft prompt tuning method (not interpretable) on five datasets. Even though LaBo does not use class names, its performance is similar to that of CoOp. Adding class names to LaBo leads to performance gains, such that it outperforms CoOp on Aircraft and UCF-101.  \n\n\nC.3. Performance of Human-Written Text\n\n\nC.4. Comparison with the Prompt Tuning Method\n\n\nD. Human Evaluation\n\nWe introduce two qualitative metrics to evaluate the automatically generated concept bottlenecks to highlight areas of possible improvement. We introduce two metrics that evaluate the bottleneck items along two dimensions: Factuality and Groundability (see Section 5.3).\n\nAnnotator Statistics. Both metrics rely on human annotations, which we collect on Amazon Mechanical Turk. To ensure confidence in the results, we collect 3 annotations per concept. Annotators are paid on average $14.5 per hour, and the total cost of the annotation was $2,100. Our rate was computed by estimating the time it takes to complete the task by 4 different control annotators. 11 In total, our task was completed by a diverse set of 477 annotators. The average pairwise annotator agreement for all annotated data without 11 Our focus group was graduate students. Since this is not representative of the average population, we doubled the time estimate.  Figure 6). Lower percentage is better.\n\n\nfeta cheese and kalamata olives\n\nIf you think that this concept is not good for singling out relevant images, select one or more of the following reasons (if any).\n\nNon-sensical or ungramatical. Unknown vocabulary Non visual phrase. Figure 11. Sample user interface for measuring Factuality. We provide 10 ground truth images with 2 control images randomly positioned. Annotators are required to select the images that can be described by the phrase. The user interface for Groundability is identical, but the images presented are the top-10 images in the dataset sorted by CLIP [44] similarity score.\n\n\nSubmit\n\nany pre-processing is 69.83%.\n\nInterface. Figure 11 displays the annotation interface. Given a concept phrase, annotators are prompted to select from 12 images, 10 of which correspond to the ground truth target corresponding to the concept, and 2 control images randomly sampled from other classes. The user interface was accompanied by a set of instructions presented in Figure  12.\n\nInvalid Annotations. In reporting Factuality and Ground-ability, we disregard annotations that select any of the control images unless all annotators failed the control for a particular concept. In total, we disregard 18% of annotations for this reason. In reporting invalid concepts (non-visual, non-sensical, or unknown vocabulary), we consider all annotations but consider a bottleneck invalid if at least 2 out of 3 annotators agree.\n\nAnalytic Results. Table 15 displays analytic results of Factuality and Groundability for all datasets. Figure 10 presents the invalid concept distribution for all datasets separately. It is worth noting the high percentage of non-visual concepts in CIFAR-10 and CIFAR-100 compared to other datasets. We hypothesize that this reflects the annotators' inability to see the images clearly due to the low resolution (see Figure  9) rather than the lack of visual content in the concept. For example, the concepts \"small and black\" and \"blue nose and tail\" were annotated as non-visual for CIFAR-10, and the concepts \"color of trees and grass\" and \"two large pincers on its front legs\" for CIFAR-100. Figure 9 shows the additional qualitative examples for the rest 6 datasets (CIFAR-10, CIFAR-100, DTD, Aircraft, Food101, and RESISC45).\n\n\nE. Qualitative Examples\n\n\nInstructions\n\nIn this task you will be provided with a phrase, and a set of images and you will select which images have a part or aspect that can be described by the phrase. Below are three examples.\n\n\nExample 1\n\nPhrase: spiky, jagged pattern\n\nYou would select all images since we observe all flowers have a spiky petals.\n\n\nExample 2 Phrase: deep red color with yellow accents\n\nYou would select no images, since they flowers are mostly pink and white not red with yellow accents.\n\n\nExample 3 Phrase: beautiful, soft pink\n\nYou would select the first image, since this is the only image that has a pink color.\n\nIn some cases, there may be problems with the phrase that make it difficult to associate with any image. In these cases, please select an option that best describes the issue:\n\nNon-sensical The phrase is ungramatical or is not understandable. Unknown vocubulary The phrase uses words you do not know. For example, the phrase member of the genus lilium and the family liliaceae Non-visual The phrase does not clearly refer to image content. For example associated with passion, love, and excitement\n\nHit submit once you are done to register your hit Select the images that you could describe a part or aspect of using the phrase: Figure 12. Instructions provided to annotators to compute Factuality and Groundability. \n\n1\nCode and data are available at https://github.com/YueYANG1996/LaBo Input Image x \" Human Designed Concepts ... has nape color :: grey has bill shape :: cone ... has head pattern :: eyebow Ours: LLM Generated Concepts ... grayish brown back and wings black throat with a white boarder brown head with white stripes ...\n\nFigure 1 .\n1describe what the black-throated sparrow looks like: Our proposed high-performance Concept Bottleneck Model alleviates the need for human-designed concepts by prompting large language models (LLMs) such as GPT-3 [4].\n\nFigure 3 .\n3Test accuracy (%) comparison between LaBo and Linear Probe on 11 datasets. The x-axis represents the number of labeled images.\n\n\n[11], CIFAR-10 and CIFAR-100 [26]; (2) Fine-grained objects: Food-101 [3], FGVC-Aircraft [32], Flower-102 [39], CUB-200-2011 [67]; (3) Actions: UCF-101 [59]; (4) Textures: DTD [10]; (5) Skin tumors: HAM10000 [64] and (6) Satellite images: RE-SISC45 [8]. We use train/dev/test splits for all the datasets.\n\nFactualityFigure 6 .\n6(c) = number of images selected k ground truth images of the class Percentage of invalid concepts identified by humans for different bottlenecks on ImageNet. Lower percentage is better.\n\nFigure 7 .\n7Several example bottlenecks generated by LaBo. The top-3 concepts, ranked by their weights in the linear function, for randomly selected classes, paired with a random image from the class, across 6 datasets.\n\nFigure 8 .\n8t-SNE visualization of the embeddings of concepts (blue) and class names (pink) on ImageNet. For the three bottlenecks constructed from GPT-3, WordNet, and Wikipedia, we visualize the top-1 concept of each class ranked by the weights of the linear function.\n\nFigure 9 .\n9Additional qualitative examples for CIFAR-10, CIFAR-100, DTD, Aircraft, Food101 and RESISC45.\n\nFigure 10 .\n10Percentage of invalid concepts identified by humans for different bottlenecks for all 10 datasets except ImageNet (see\n\n\nTable 1. Mean accuracy across all datasets, at different shots .Table 2. Test accuracy comparison between LaBo and Post-hoc Concept Bottleneck Model (PCBM) on CIFAR-10 and CIFAR-100. \"w/ end-to-end\" denotes whether the model employs an end-to-end residual predictor from image features to targets.Table 3. LaBo and CompDL evaluated on CUB for 1/5/full shots. set the default number of concepts selected for each class to 50. To train the linear function, we use the Pytorch-lightning library with Adam [24] optimizer. We tune the batch size, learning rate, and submodular weights on the development set. Model checkpoints with the highest validation accuracy are evaluated on the test set. We list the hyperparameters for all datasets and shots in the supplementary material.Method \n\n1 \n2 \n4 \n8 \n16 \nFull \nAvg \nLinear Probe 51.69 65.13 72.33 77.38 81.53 87.38 72.57 \nLaBo (Ours) 63.35 68.10 72.08 76.19 79.11 85.72 74.09 \n\nMethod \nw/ end-to-end CIFAR-10 CIFAR-100 \nPCBM [70] \n\u2717 \n84.5 \n56.0 \nLaBo (Ours) \n\u2717 \n87.9 \n69.1 \nPCBM-h [70] \n\u2713 \n87.6 \n69.9 \nLinear Probe \n\u2713 \n88.8 \n70.1 \n\nMethod \nw/ manual concepts \n1 \n5 \nFull \nCompDL [71] \n\u2713 \n13.6 33.2 52.6 \nLaBo (Ours) \n\u2717 \n35.1 55.7 71.8 \nLinear Probe \n-\n28.4 55.4 75.5 \n\n\n\nTable 2 compares\n2LaBo's performance with PCBM and Linear Probe.Table 4. Ablation results on bottleneck sizes. We vary the sizes of the bottlenecks and report the average performance on 11 datasets.n. of concepts \nper class (k) \n\nn. of shots \n1 \n2 \n4 \n8 \n16 \nFull \n1 \n41.89 52.45 61.76 65.99 69.61 78.95 \n5 \n52.54 61.13 67.22 72.90 75.62 83.83 \n10 \n58.00 64.59 69.90 74.50 77.43 84.66 \n25 \n61.72 66.33 71.39 75.28 79.04 85.26 \n50 \n63.03 67.79 71.88 76.08 79.10 85.71 \n\nSelection \nMethod \n\nn. of shots \n1 \n2 \n4 \n8 \n16 \nFull \nRANDOM \n59.24 64.71 70.42 74.07 78.29 85.06 \nSIMILARITY \n54.59 61.42 67.17 72.66 77.32 84.88 \nCOVERAGE \n59.73 65.93 70.82 74.71 78.90 85.60 \nDISCRIM \n60.99 66.49 70.93 74.81 77.90 85.31 \nSUBMODULAR 63.03 67.79 71.88 76.08 79.10 85.71 \n\n\n\n\n, we compare performance for different bottleneck sizes ranging from 1 to 50 conceptsFigure 4. Language Prior vs. Random Weight initialization average over all datasets.1 \n\n2 \n4 \n8 \n16 \nfull \n# of shots \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nAccuracy (%) \nLinear Probe \nLaBo (init w/ LM) \nLaBo (rand init) \n\nLaBo \nLaBo \n(w/o Submod) \nLaBo \n(w/o LM) \nWordNet Wikipedia \n0 \n25 \n50 \nScore(%) \n51.3 \n41.8 \n13.0 \n17.7 \n28.7 \n\nFactuality \n\nLaBo \nLaBo \n(w/o Submod) \nLaBo \n(w/o LM) \nWordNet Wikipedia \n0 \n\n25 \n\n50 \nScore(%) \n34.8 \n32.7 \n29.7 \n11.1 \n25.3 \n\n\n\nTable 6 .\n6Average human evaluation results of LaBo on 11 datasets.We also evaluate LaBo by removing the submodular function (w/o \nSubmod) and language model priors (w/o LM). \n\n\n\n[ 1 ]\n1Francis Bach. Convex analysis and optimization with submod-Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural language descriptions of deep visual features. In International Conference on Learning Representations, 2021. 3 [20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig.ular functions: a tutorial. arXiv preprint arXiv:1010.4207, \n2010. 4 \n\n[2] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Anto-\nnio Torralba. Network dissection: Quantifying interpretability \nof deep visual representations. In Proceedings of the IEEE \nconference on computer vision and pattern recognition, pages \n6541-6549, 2017. 3 \n\n[3] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. \nFood-101 -mining discriminative components with random \nforests. In European Conference on Computer Vision, 2014. \n2, 5 \n\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom \nHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jef-\nfrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric \nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack \nClark, Christopher Berner, Sam McCandlish, Alec Radford, \nIlya Sutskever, and Dario Amodei. Language models are few-\nshot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. \nBalcan, and H. Lin, editors, Advances in Neural Information \nProcessing Systems, volume 33, pages 1877-1901. Curran \nAssociates, Inc., 2020. 1, 3 \n\n[5] Sebastian Bujwid and Josephine Sullivan. Large-scale zero-\nshot image classification from rich and diverse textual de-\nscriptions. In Proceedings of the Third Workshop on Beyond \nVision and LANguage: inTEgrating Real-world kNowledge \n(LANTERN), pages 38-52, 2021. 3 \n\n[6] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia \nRudin, and Jonathan K Su. This looks like that: deep learn-\ning for interpretable image recognition. Advances in neural \ninformation processing systems, 32, 2019. 3 \n\n[7] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, \nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: \nUniversal image-text representation learning. In ECCV, 2020. \n3 \n\n[8] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing \nimage scene classification: Benchmark and state of the art. \nProceedings of the IEEE, 105(10):1865-1883, 2017. 2, 5 \n\n[9] Arkabandhu Chowdhury, Mingchao Jiang, Swarat Chaudhuri, \nand Chris Jermaine. Few-shot image classification: Just use a \nlibrary of pre-trained feature extractors and a simple classifier. \nIn Proceedings of the IEEE/CVF International Conference \non Computer Vision, pages 9445-9454, 2021. 3 \n\n[10] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy \nMohamed, and Andrea Vedaldi. Describing textures in the \nwild. In Proceedings of the IEEE conference on computer \nvision and pattern recognition, pages 3606-3613, 2014. 2, 5 \n\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li \nFei-Fei. Imagenet: A large-scale hierarchical image database. \nIn 2009 IEEE conference on computer vision and pattern \nrecognition, pages 248-255. Ieee, 2009. 2, 3, 5 \n\n[12] Sinuo Deng, Lifang Wu, Ge Shi, Lehao Xing, and Meng Jian. \nLearning to compose diversified prompts for image emotion \nclassification. arXiv preprint arXiv:2201.10963, 2022. 3 \n[13] Christiane Fellbaum. Wordnet. In Theory and applications of \nontology: computer applications, pages 231-243. Springer, \n2010. 7 \n[14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao \nFang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-\nadapter: Better vision-language models with feature adapters. \narXiv preprint arXiv:2110.04544, 2021. 3 \n[15] Tianyu Gao, Adam Fisch, and Danqi Chen. Making \npre-trained language models better few-shot learners. In \nACL/IJCNLP (1), 2021. 3 \n[16] David Gunning and David Aha. Darpa's explainable artificial \nintelligence (xai) program. AI magazine, 40(2):44-58, 2019. \n1 \n[17] Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff \nDonahue, Bernt Schiele, and Trevor Darrell. Generating \nvisual explanations. In European conference on computer \nvision, pages 3-19. Springer, 2016. 3 \n[18] Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, and \nZeynep Akata. Grounding visual explanations. In Proceed-\nings of the European Conference on Computer Vision (ECCV), \npages 264-279, 2018. 3 \n[19] Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom \nDuerig. Scaling up visual and vision-language representation \nlearning with noisy text supervision. In International Confer-\nence on Machine Learning, pages 4904-4916. PMLR, 2021. \n3 \n[21] How can we know what language models know? Transactions \nof the Association for Computational Linguistics, 8:423-438, \n2020. 2 \n[22] Jihyung Kil and Wei-Lun Chao. Revisiting document repre-\nsentations for large-scale zero-shot learning. In Proceedings \nof the 2021 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Lan-\nguage Technologies, pages 3117-3128, Online, June 2021. \nAssociation for Computational Linguistics. 3, 7 \n[23] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, \nand Zeynep Akata. Textual explanations for self-driving \nvehicles. In Proceedings of the European conference on \ncomputer vision (ECCV), pages 563-578, 2018. 3 \n[24] Diederik P Kingma and Jimmy Ba. Adam: A method for \nstochastic optimization. arXiv preprint arXiv:1412.6980, \n2014. 6 \n[25] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Muss-\nmann, Emma Pierson, Been Kim, and Percy Liang. Concept \nbottleneck models. In International Conference on Machine \nLearning, pages 5338-5348. PMLR, 2020. 1, 3, 5 \n[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple \nlayers of features from tiny images. 2009. 2, 5 \n\n\n\nTable 10. Full results of Linear Prob and LaBo on the development and test sets of 11 datasets.Dataset \n\nMethod \nDev \nTest \n1 \n2 \n4 \n8 \n16 \nFull \n1 \n2 \n4 \n8 \n16 \nFull \n\nFood-101 \nLinear Prob 58.04 75.24 84.16 87.48 89.87 93.11 57.75 75.34 84.21 87.90 90.02 93.17 \nLaBo (Ours) 80.32 84.15 85.76 87.07 88.74 92.53 80.41 84.05 85.68 87.39 88.77 92.45 \n\nAircraft \nLinear Prob 27.63 34.86 41.40 49.72 57.91 62.89 28.26 35.07 41.55 50.26 56.38 64.03 \nLaBo (Ours) 33.12 35.97 42.90 49.08 56.41 61.96 32.73 37.71 41.04 48.81 54.97 61.42 \n\nFlower-102 \nLinear Prob 89.20 94.06 97.00 98.40 98.91 99.11 88.06 93.65 97.67 98.56 99.32 99.45 \nLaBo (Ours) 82.24 88.18 94.92 96.20 98.16 98.65 82.05 90.09 95.21 97.08 98.66 99.35 \n\nCUB \nLinear Prob 48.55 60.40 72.50 78.25 83.35 83.60 47.69 61.06 72.82 79.60 83.74 84.54 \nLaBo (Ours) 55.20 64.80 72.45 76.55 79.90 81.00 54.19 64.60 71.21 77.22 80.69 81.90 \n\nUCF-101 \nLinear Prob 65.54 76.34 85.83 90.25 93.63 98.63 60.56 73.22 80.62 85.70 87.63 90.67 \nLaBo (Ours) 80.72 83.77 88.46 90.73 93.05 97.68 78.75 82.05 84.56 86.39 87.39 90.11 \n\nDTD \nLinear Prob 43.62 53.19 60.55 68.79 74.47 80.50 41.67 51.71 60.76 69.03 74.70 81.68 \nLaBo (Ours) 55.59 56.47 62.15 68.44 70.92 76.86 53.61 55.26 61.17 66.43 70.21 77.30 \n\nHAM10000 \nLinear Prob 32.30 55.40 45.40 50.90 63.10 84.40 33.13 55.32 44.48 48.26 61.69 83.18 \nLaBo (Ours) 34.90 46.40 45.80 54.40 58.20 81.40 36.62 45.17 45.87 52.04 55.72 81.39 \n\nRESISC45 \nLinear Prob 68.62 79.10 86.72 89.89 92.49 95.24 67.57 77.75 86.50 89.27 92.17 94.98 \nLaBo (Ours) 73.02 76.03 81.37 85.05 88.86 91.65 73.66 76.11 81.40 85.71 88.63 91.22 \n\nCIFAR-10 \nLinear Prob 62.36 80.32 92.94 95.36 96.06 98.16 62.44 80.27 92.54 95.14 95.90 98.10 \nLaBo (Ours) 91.24 91.04 92.98 94.40 95.06 97.90 91.06 90.79 93.03 94.11 94.93 97.75 \n\nCIFAR-100 \nLinear Prob 39.66 57.84 70.06 76.52 80.34 87.70 39.26 57.35 69.73 76.22 80.16 87.48 \nLaBo (Ours) 62.84 66.56 71.78 75.30 78.08 86.82 62.73 65.80 70.82 74.49 77.67 86.04 \n\nImageNet \nLinear Prob 42.25 55.71 64.80 71.23 75.08 83.90 \n-\n-\n-\n-\n-\n-\nLaBo (Ours) 51.09 57.43 62.94 68.45 72.60 83.97 \n-\n-\n-\n-\n-\n-\n\nAverage \nLinear Prob 52.53 65.68 72.85 77.89 82.29 87.93 51.69 65.13 72.33 77.38 81.53 87.38 \nLaBo (Ours) 63.66 68.25 72.86 76.88 80.00 86.40 63.35 68.10 72.08 76.19 79.11 85.72 \n\n\n\n\nTable 11. Compare different activation functions. We report the mean accuracy across the 11 datasets.Table 12. The performance of LaBo on ImageNet using different sizes of GPT-3 to generate concepts. The number in the parenthesis is the number of parameters of the corresponding language model.Activation \n1 \n2 \n4 \n8 \n16 \nFull \n-\n52.66 58.01 63.02 68.93 73.52 81.32 \nrelu \n50.40 53.53 56.61 59.82 61.75 68.01 \nsigmoid 52.15 57.86 62.59 69.08 73.43 81.42 \nsoftmax 63.03 67.79 71.88 76.08 79.10 85.71 \n\nGPT-3 type \n1 \n2 \n4 \n8 \n16 \nFull \nDavinci (175B) 51.09 57.43 62.94 68.45 72.60 83.97 \nCurie (13B) \n45.75 53.89 60.36 66.96 71.65 84.00 \nBabbage (6.7B) 44.61 52.91 60.22 67.06 71.66 83.86 \nAda (2.7B) \n43.12 53.26 60.99 67.90 72.42 83.96 \n\n\n\n\nTable 14. Compare LaBo with prompt tuning methods on 5 datasets (16 shots). w/ cls stands for using class names in the context. LaBo \u2020 is our method without removing the class names in the concepts. All methods use CLIP-ViT-B/32 as the vision backbone.Concept Source \n1 \n2 \n4 \n8 \n16 \nFull \nGPT-3 \n51.09 57.43 62.94 68.45 72.60 83.97 \nWikipedia \n48.76 56.73 63.00 68.96 73.07 84.07 \nWordNet \n49.37 57.84 64.10 69.92 73.35 83.93 \n\nTable 13. The performance of LaBo on ImageNet using different \nsources of concepts to construct the bottlenecks. \n\nMethod \nw/ cls Aircraft Food Flower DTD UCF \nLP \n-\n39.42 \n76.99 \n95.89 \n68.74 80.04 \nLaBo \n\u2717 \n37.29 \n76.04 \n92.37 \n64.78 80.07 \nCoOp [74] \n\u2713 \n33.22 \n78.45 \n94.97 \n65.37 78.66 \nLaBo  \u2020 \n\u2713 \n37.53 \n77.83 \n93.18 \n65.37 80.10 \n\n\n\n\nTable 15. Analytic Factuality and Groundability for all datasets except Imagenet (seeFigure 5)Food Aircraft HAM10K RESISC Flower CUB \nUCF \nDTD CIFAR10 CIFAR100 \nFactuality \u2191 \nP@10 \nP@10 \nP@10 \nP@10 \nP@8 \nP@10 P@10 P@10 \nP@10 \nP@10 \nLaBo \n33.07 \n11.57 \n15.05 \n14.80 \n11.48 \n27.97 37.78 23.90 \n14.70 \n22.48 \nw/o submod \n27.08 \n8.10 \n9.57 \n16.40 \n18.58 \n23.12 37.22 25.27 \n20.70 \n22.72 \nw/o LM \n21.63 \n8.97 \n19.71 \n12.15 \n9.98 \n12.17 20.43 14.83 \n6.87 \n14.97 \nGroundability\u2191 P@10 \nP@10 \nP@10 \nP@10 \nP@8 \nP@10 P@10 P@10 \nP@10 \nP@10 \nLaBo \n10.98 \n8.48 \n18.83 \n13.87 \n9.53 \n15.63 \n8.08 \n8.90 \n5.70 \n19.83 \nw/o submod \n21.52 \n13.67 \n17.22 \n17.90 \n21.52 \n23.07 29.93 20.02 \n23.10 \n21.78 \nw/o LM \n20.58 \n12.00 \n20.00 \n14.38 \n17.93 \n25.02 27.96 20.31 \n7.15 \n27.04 \n\n\n\n\nn. of shots Bottleneck Size Discriminability (\u03b1) Coverage (\u03b2) Learning Rate Batch Size Table 16. All hyperparameters used for the main experiments which are tuned on the development set.Food-101 \n\n1 \n5,050 \n1e 7 \n0.5 \n1e \u22125 \n16 \n2 \n5,050 \n1e 7 \n1 \n1e \u22124 \n32 \n4 \n5,050 \n1e 7 \n1 \n1e \u22124 \n64 \n8 \n5,050 \n1e 7 \n1 \n1e \u22124 \n128 \n16 \n5,050 \n1e 7 \n1 \n1e \u22124 \n256 \nFull \n5,050 \n1e 7 \n5 \n1e \u22125 \n1024 \n\nAircraft \n\n1 \n5,100 \n1e 7 \n0.5 \n5e \u22125 \n16 \n2 \n5,100 \n1e 7 \n1 \n5e \u22125 \n32 \n4 \n5,100 \n1e 7 \n0.1 \n5e \u22125 \n64 \n8 \n5,100 \n1e 7 \n0 \n5e \u22125 \n128 \n16 \n5,100 \n1e 7 \n1 \n5e \u22125 \n256 \nFull \n5,100 \n1e 7 \n0.5 \n5e \u22125 \n256 \n\nFlower-102 \n\n1 \n2,050 \n1e 7 \n10 \n1e \u22125 \n16 \n2 \n2,050 \n1e 7 \n100 \n1e \u22125 \n32 \n4 \n2,050 \n1e 7 \n10 \n1e \u22125 \n64 \n8 \n2,050 \n1e 7 \n10 \n1e \u22125 \n128 \n16 \n2,050 \n1e 7 \n1 \n1e \u22125 \n256 \nFull \n2,050 \n1e 7 \n1 \n1e \u22125 \n256 \n\nCUB \n\n1 \n2,000 \n1e 7 \n0 \n5e \u22125 \n32 \n2 \n2,000 \n1e 7 \n0 \n5e \u22125 \n64 \n4 \n2,000 \n1e 7 \n0.1 \n5e \u22125 \n128 \n8 \n2,000 \n1e 7 \n0 \n5e \u22125 \n256 \n16 \n2,000 \n1e 7 \n1 \n5e \u22125 \n512 \nFull \n2,000 \n1e 7 \n0.1 \n5e \u22125 \n512 \n\nUCF-101 \n\n1 \n5,050 \n1e 7 \n1 \n1e \u22125 \n8 \n2 \n5,050 \n1e 7 \n1 \n1e \u22125 \n16 \n4 \n5,050 \n1e 7 \n100 \n1e \u22125 \n32 \n8 \n5,050 \n1e 7 \n10 \n1e \u22125 \n64 \n16 \n5,050 \n1e 7 \n100 \n1e \u22125 \n128 \nFull \n5,050 \n1e 7 \n10 \n1e \u22125 \n256 \n\nDTD \n\n1 \n2,350 \n1e 7 \n10 \n1e \u22125 \n8 \n2 \n2,350 \n1e 7 \n10 \n1e \u22125 \n16 \n4 \n2,350 \n1e 7 \n5 \n1e \u22125 \n32 \n8 \n2,350 \n1e 7 \n1 \n1e \u22125 \n64 \n16 \n2,350 \n1e 7 \n2.5 \n5e \u22125 \n256 \nFull \n2,350 \n1e 7 \n7.5 \n1e \u22124 \n512 \n\nHAM10000 \n\n1 \n350 \n1e 7 \n0.1 \n1e \u22123 \n4 \n2 \n350 \n1e 7 \n0.1 \n1e \u22123 \n4 \n4 \n350 \n1e 7 \n1 \n1e \u22124 \n8 \n8 \n350 \n1e 7 \n10 \n1e \u22123 \n8 \n16 \n350 \n1e 7 \n15 \n1e \u22123 \n16 \nFull \n350 \n1e 7 \n0.1 \n5e \u22124 \n256 \n\nRESISC45 \n\n1 \n2,250 \n1e 7 \n5 \n5e \u22125 \n8 \n2 \n2,250 \n1e 7 \n5 \n5e \u22125 \n16 \n4 \n2,250 \n1e 7 \n10 \n5e \u22125 \n32 \n8 \n2,250 \n1e 7 \n15 \n5e \u22125 \n64 \n16 \n2,250 \n1e 7 \n15 \n5e \u22125 \n128 \nFull \n2,250 \n1e 7 \n15 \n5e \u22125 \n256 \n\nCIFAR-10 \n\n1 \n500 \n1e 7 \n1 \n1e \u22124 \n2 \n2 \n500 \n1e 7 \n5 \n5e \u22124 \n4 \n4 \n500 \n1e 7 \n5 \n1e \u22124 \n8 \n8 \n500 \n1e 7 \n1 \n1e \u22124 \n16 \n16 \n500 \n1e 7 \n10 \n1e \u22124 \n32 \nFull \n500 \n1e 7 \n5 \n1e \u22124 \n512 \n\nCIFAR-100 \n\n1 \n5,000 \n1e 7 \n7.5 \n1e \u22125 \n16 \n2 \n5,000 \n1e 7 \n2.5 \n1e \u22125 \n32 \n4 \n5,000 \n1e 7 \n7.5 \n1e \u22125 \n64 \n8 \n5,000 \n1e 7 \n7.5 \n1e \u22125 \n128 \n16 \n5,000 \n1e 7 \n5 \n1e \u22125 \n256 \nFull \n5,000 \n1e 7 \n0 \n1e \u22125 \n512 \n\nImageNet \n\n1 \n50,000 \n1e 8 \n0 \n1e \u22125 \n128 \n2 \n50,000 \n1e 8 \n0 \n1e \u22125 \n256 \n4 \n50,000 \n1e 8 \n0 \n1e \u22125 \n256 \n8 \n50,000 \n1e 8 \n0 \n1e \u22125 \n512 \n16 \n50,000 \n1e 8 \n0 \n1e \u22125 \n1024 \nFull \n50,000 \n1e 8 \n0 \n1e \u22125 \n2048 \n\n\nwhen creating candidate attributes. This is largely done to overcome problems of word sense. For example, when naively prompted to produce knowledge about the flower \"bird of paradise\" GPT-3 yields information about birds instead of flowers. In general, specialization here was also minimal. See appendix for prompts.\ndiminishing returns property means \u2200A \u2286 B \u2286 V \\ v, we have F (A + {v}) \u2212 F (A) \u2265 F (B + {v}) \u2212 F (B). 4 A submodular function is monotone if \u2200A \u2286 B, F (A) \u2264 F (B). 5 Any linear combination of submodular functions is still submodular.6  In N -way-K-shot setting, |Xy| = K.\nWe use the same set of prompts for all datasets except UCF-101 since it is very different to describe an action.\nWith the only exception of Factuality for Flower-102 where we set k = 8 because there are not enough images in the dev set.\nClass Name\nAttribute-based classification for zero-shot visual object categorization. H Christoph, Lampert, IEEE transactions on pattern analysis and machine intelligence. 36Hannes Nickisch, and Stefan HarmelingChristoph H Lampert, Hannes Nickisch, and Stefan Harmel- ing. Attribute-based classification for zero-shot visual object categorization. IEEE transactions on pattern analysis and machine intelligence, 36(3):453-465, 2013. 3\n\nBlip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, arXiv:2201.120862022arXiv preprintJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022. 3\n\nSupporting vision-language model inference with causality-pruning knowledge prompt. Jiangmeng Li, Wenyi Mo, Wenwen Qiang, Bing Su, Changwen Zheng, arXiv:2205.111002022arXiv preprintJiangmeng Li, Wenyi Mo, Wenwen Qiang, Bing Su, and Changwen Zheng. Supporting vision-language model in- ference with causality-pruning knowledge prompt. arXiv preprint arXiv:2205.11100, 2022. 3\n\nVisualbert: A simple and performant baseline for vision and language. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang, arXiv:1908.03557arXiv preprintLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and perfor- mant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. 3\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Advances in neural information processing systems. 32Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019. 3\n\nFine-grained visual classification of aircraft. Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, Andrea Vedaldi, arXiv:1306.515135arXiv preprintSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual clas- sification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 2, 3, 5\n\nOpen world compositional zeroshot learning. Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, Zeynep Akata, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionMassimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, and Zeynep Akata. Open world compositional zero- shot learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5222-5230, 2021. 8\n\nDo concept bottleneck models learn as intended?. Andrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi Chen, Mateja Jamnik, Adrian Weller, arXiv:2105.04289arXiv preprintAndrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi Chen, Mateja Jamnik, and Adrian Weller. Do concept bottleneck models learn as intended? arXiv preprint arXiv:2105.04289, 2021. 1\n\nVisual classification via description from large language models. Sachit Menon, Carl Vondrick, arXiv:2210.071832022arXiv preprintSachit Menon and Carl Vondrick. Visual classification via description from large language models. arXiv preprint arXiv:2210.07183, 2022. 3\n\nCompositional explanations of neurons. Jesse Mu, Jacob Andreas, Advances in Neural Information Processing Systems. 33Jesse Mu and Jacob Andreas. Compositional explanations of neurons. Advances in Neural Information Processing Systems, 33:17153-17163, 2020. 3\n\nNeural prototype trees for interpretable fine-grained image recognition. Meike Nauta, Ron Van Bree, Christin Seifert, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMeike Nauta, Ron van Bree, and Christin Seifert. Neural pro- totype trees for interpretable fine-grained image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14933-14943, 2021. 3\n\nAn analysis of approximations for maximizing submodular set functions-i. Mathematical programming. L George, Laurence A Nemhauser, Marshall L Wolsey, Fisher, 144George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functions-i. Mathematical programming, 14(1):265-294, 1978. 2, 4\n\nAutomated flower classification over a large number of classes. Maria-Elena Nilsback, Andrew Zisserman, dian Conference on Computer Vision, Graphics and Image Processing. 25Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In In- dian Conference on Computer Vision, Graphics and Image Processing, Dec 2008. 2, 5\n\nImproving few-shot image classification using machine-and user-generated natural language descriptions. Kosuke Nishida, Kyosuke Nishida, Shuichi Nishioka, Findings of the Association for Computational Linguistics: NAACL 2022. 2022Kosuke Nishida, Kyosuke Nishida, and Shuichi Nishioka. Improving few-shot image classification using machine-and user-generated natural language descriptions. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1421-1430, 2022. 3\n\nMultimodal explanations: Justifying decisions and pointing to the evidence. Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, Marcus Rohrbach, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionDong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. Multimodal explanations: Justifying decisions and pointing to the evidence. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8779-8788, 2018. 3\n\nLanguage models as knowledge bases?. Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. 2\n\nWhat does a platypus look like? generating customized prompts for zeroshot image classification. Sarah Pratt, Rosanne Liu, Ali Farhadi, arXiv:2209.033202022arXiv preprintSarah Pratt, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero- shot image classification. arXiv preprint arXiv:2209.03320, 2022. 3\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International Conference on Machine Learning. 16PMLR, 2021. 1, 3, 5Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021. 1, 3, 5, 16\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 2114013Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. 5, 13\n\nDenseclip: Language-guided dense prediction with contextaware prompting. Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context- aware prompting. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pages 18082-18091, 2022. 3\n\nwhy should i trust you?\" explaining the predictions of any classifier. Sameer Marco Tulio Ribeiro, Carlos Singh, Guestrin, Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on knowledge discovery and data miningMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD interna- tional conference on knowledge discovery and data mining, pages 1135-1144, 2016. 1\n\nIntegrating language guidance into vision-based deep metric learning. Karsten Roth, Oriol Vinyals, Zeynep Akata, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022Karsten Roth, Oriol Vinyals, and Zeynep Akata. Integrating language guidance into vision-based deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16177-16189, 2022. 3\n\nStop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Cynthia Rudin, Nature Machine Intelligence. 153Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206-215, 2019. 1, 3\n\nAttribute learning in largescale datasets. Olga Russakovsky, Li Fei-Fei, European Conference on Computer Vision. Springer3Olga Russakovsky and Li Fei-Fei. Attribute learning in large- scale datasets. In European Conference on Computer Vision, pages 1-14. Springer, 2010. 3\n\nFew-shot learning with graph neural networks. Garcia Victor, Joan Bruna Satorras, Estrach, In International Conference on Learning Representations. 3Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In International Confer- ence on Learning Representations, 2018. 3\n\nConcept bottleneck model with additional unsupervised concepts. Yoshihide Sawada, Keigo Nakamura, IEEE Access. 103Yoshihide Sawada and Keigo Nakamura. Concept bottleneck model with additional unsupervised concepts. IEEE Access, 10:41758-41765, 2022. 3\n\nExploiting cloze-questions for few-shot text classification and natural language inference. Timo Schick, Hinrich Sch\u00fctze, Timo Schick and Hinrich Sch\u00fctze. Exploiting cloze-questions for few-shot text classification and natural language inference.\n\nProceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeIn Proceedings of the 16th Conference of the European Chap- ter of the Association for Computational Linguistics: Main Volume, pages 255-269, 2021. 3\n\nGradcam: Visual explanations from deep networks via gradientbased localization. R Ramprasaath, Michael Selvaraju, Abhishek Cogswell, Ramakrishna Das, Devi Vedantam, Dhruv Parikh, Batra, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision13Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In Proceedings of the IEEE international conference on computer vision, pages 618-626, 2017. 1, 3\n\nSheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Anna Rohrbach, Zhe Gan, Lijuan Wang, Lu Yuan, arXiv:2204.09222Learning transferable visual models with external knowledge. 2022arXiv preprintSheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jian- wei Yang, Pengchuan Zhang, Anna Rohrbach, Zhe Gan, Lijuan Wang, Lu Yuan, et al. K-lite: Learning transfer- able visual models with external knowledge. arXiv preprint arXiv:2204.09222, 2022. 3\n\nAutoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, I V Robert L Logan, Eric Wallace, Sameer Singh, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wal- lace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, 2020. 3\n\nExplaining patterns in data with language models via interpretable autoprompting. Chandan Singh, X John, Jyoti Morris, Alexander M Aneja, Jianfeng Rush, Gao, arXiv:2210.018482022arXiv preprintChandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, and Jianfeng Gao. Explaining patterns in data with lan- guage models via interpretable autoprompting. arXiv preprint arXiv:2210.01848, 2022. 3\n\nPrototypical networks for few-shot learning. Advances in neural information processing systems. Jake Snell, Kevin Swersky, Richard Zemel, 30Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural informa- tion processing systems, 30, 2017. 3\n\nUcf101: A dataset of 101 human actions classes from videos in the wild. Khurram Soomro, Mubarak Amir Roshan Zamir, Shah, arXiv:1212.040225arXiv preprintKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 2, 5\n\nConceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Thirty-first AAAI conference on artificial intelligence. 35Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-first AAAI conference on artificial intelligence, 2017. 3, 5\n\nCan language models be biomedical knowledge bases?. Mujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, Jaewoo Kang, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsOnline and Punta CanaMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, and Jaewoo Kang. Can language models be biomedical knowledge bases? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4723-4734, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. 2\n\nLxmert: Learning cross-modality encoder representations from transformers. Hao Tan, Mohit Bansal, arXiv:1908.07490arXiv preprintHao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019. 3\n\nRethinking few-shot image classification: a good embedding is all you need. Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, Phillip Isola, European Conference on Computer Vision. SpringerYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen- baum, and Phillip Isola. Rethinking few-shot image classi- fication: a good embedding is all you need? In European Conference on Computer Vision, pages 266-282. Springer, 2020. 3\n\nThe ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data. Philipp Tschandl, Cliff Rosendahl, Harald Kittler, 55Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermato- scopic images of common pigmented skin lesions. Scientific data, 5(1):1-9, 2018. 2, 5\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, 2008. 14Journal of machine learning research. 911Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. 14\n\nMatching networks for one shot learning. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, Advances in neural information processing systems. 29Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016. 3\n\nThe caltech-ucsd birds. Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, Serge Belongie, Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 2, 5\n\nAttribute prototype network for zero-shot learning. Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, Zeynep Akata, Advances in Neural Information Processing Systems. 33Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, and Zeynep Akata. Attribute prototype network for zero-shot learning. Advances in Neural Information Processing Systems, 33:21969-21980, 2020. 3\n\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, arXiv:2111.11432A new foundation model for computer vision. arXiv preprintLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021. 3\n\nPosthoc concept bottleneck models. Mert Yuksekgonul, Maggie Wang, James Zou, ICLR 2022 Workshop on PAIR 2 Struct. 56Mert Yuksekgonul, Maggie Wang, and James Zou. Post- hoc concept bottleneck models. In ICLR 2022 Workshop on PAIR 2 Struct, 2022. 1, 2, 3, 5, 6\n\nDo vision-language pretrained models learn primitive concepts?. Tian Yun, Usha Bhalla, Ellie Pavlick, Chen Sun, arXiv:2203.172716arXiv preprintTian Yun, Usha Bhalla, Ellie Pavlick, and Chen Sun. Do vision-language pretrained models learn primitive concepts? arXiv preprint arXiv:2203.17271, 2022. 3, 5, 6, 8\n\nAle\u0161 Leonardis, and Ke Tang. A survey on neural network interpretability. Yu Zhang, Peter Ti\u0148o, IEEE Transactions on Emerging Topics in Computational Intelligence. 20211Yu Zhang, Peter Ti\u0148o, Ale\u0161 Leonardis, and Ke Tang. A survey on neural network interpretability. IEEE Transactions on Emerging Topics in Computational Intelligence, 2021. 1\n\nConditional prompt learning for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816-16825, 2022. 3\n\nLearning to prompt for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, International Journal of Computer Vision. 130914Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. Interna- tional Journal of Computer Vision, 130(9):2337-2348, 2022. 3, 12, 14\n", "annotations": {"author": "[{\"end\":168,\"start\":106},{\"end\":243,\"start\":169},{\"end\":310,\"start\":244},{\"end\":351,\"start\":311},{\"end\":402,\"start\":352},{\"end\":469,\"start\":403}]", "publisher": null, "author_last_name": "[{\"end\":114,\"start\":110},{\"end\":189,\"start\":177},{\"end\":257,\"start\":253},{\"end\":321,\"start\":318},{\"end\":372,\"start\":358},{\"end\":415,\"start\":408}]", "author_first_name": "[{\"end\":109,\"start\":106},{\"end\":176,\"start\":169},{\"end\":252,\"start\":244},{\"end\":317,\"start\":311},{\"end\":357,\"start\":352},{\"end\":407,\"start\":403}]", "author_affiliation": "[{\"end\":167,\"start\":140},{\"end\":242,\"start\":215},{\"end\":309,\"start\":282},{\"end\":350,\"start\":323},{\"end\":401,\"start\":374},{\"end\":468,\"start\":441}]", "title": "[{\"end\":103,\"start\":1},{\"end\":572,\"start\":470}]", "venue": null, "abstract": "[{\"end\":2055,\"start\":574}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2266,\"start\":2262},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2269,\"start\":2266},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2272,\"start\":2269},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2401,\"start\":2397},{\"end\":2563,\"start\":2559},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2764,\"start\":2760},{\"end\":2824,\"start\":2820},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3475,\"start\":3471},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3478,\"start\":3475},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4518,\"start\":4515},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4521,\"start\":4518},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5694,\"start\":5690},{\"end\":5825,\"start\":5821},{\"end\":5828,\"start\":5825},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5848,\"start\":5844},{\"end\":5872,\"start\":5869},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5875,\"start\":5872},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5878,\"start\":5875},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5881,\"start\":5878},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5910,\"start\":5906},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5928,\"start\":5924},{\"end\":5968,\"start\":5965},{\"end\":5971,\"start\":5970},{\"end\":5977,\"start\":5976},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6506,\"start\":6502},{\"end\":7465,\"start\":7462},{\"end\":7468,\"start\":7465},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7471,\"start\":7468},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7474,\"start\":7471},{\"end\":7597,\"start\":7593},{\"end\":7600,\"start\":7597},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7603,\"start\":7600},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7606,\"start\":7603},{\"end\":7726,\"start\":7722},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7729,\"start\":7726},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7893,\"start\":7889},{\"end\":8488,\"start\":8484},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8491,\"start\":8488},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8543,\"start\":8539},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8571,\"start\":8567},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8574,\"start\":8571},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8734,\"start\":8730},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9137,\"start\":9133},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9253,\"start\":9249},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9356,\"start\":9352},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9479,\"start\":9475},{\"end\":9744,\"start\":9741},{\"end\":9747,\"start\":9744},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9750,\"start\":9747},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9753,\"start\":9750},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10141,\"start\":10137},{\"end\":10283,\"start\":10280},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10286,\"start\":10283},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10289,\"start\":10286},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10292,\"start\":10289},{\"end\":10340,\"start\":10336},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10343,\"start\":10340},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10346,\"start\":10343},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10349,\"start\":10346},{\"end\":10413,\"start\":10410},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10416,\"start\":10413},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10495,\"start\":10491},{\"end\":10575,\"start\":10571},{\"end\":10578,\"start\":10575},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10581,\"start\":10578},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10584,\"start\":10581},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10587,\"start\":10584},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10590,\"start\":10587},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10593,\"start\":10590},{\"end\":10734,\"start\":10730},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10737,\"start\":10734},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10740,\"start\":10737},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11127,\"start\":11123},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11620,\"start\":11616},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14044,\"start\":14040},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18417,\"start\":18413},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19051,\"start\":19047},{\"end\":19435,\"start\":19431},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19438,\"start\":19435},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19543,\"start\":19539},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19707,\"start\":19703},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19870,\"start\":19866},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19930,\"start\":19926},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21899,\"start\":21895},{\"end\":25905,\"start\":25901},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26861,\"start\":26857},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":26864,\"start\":26861},{\"end\":27532,\"start\":27531},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28530,\"start\":28526},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31948,\"start\":31944},{\"end\":33007,\"start\":33005},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":35439,\"start\":35435},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":36146,\"start\":36142},{\"end\":37344,\"start\":37342},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38099,\"start\":38095}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41460,\"start\":41140},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41690,\"start\":41461},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41830,\"start\":41691},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42137,\"start\":41831},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42346,\"start\":42138},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42567,\"start\":42347},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42838,\"start\":42568},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42945,\"start\":42839},{\"attributes\":{\"id\":\"fig_9\"},\"end\":43079,\"start\":42946},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44296,\"start\":43080},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":45058,\"start\":44297},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45598,\"start\":45059},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45777,\"start\":45599},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":51699,\"start\":45778},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":53986,\"start\":51700},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":54728,\"start\":53987},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":55498,\"start\":54729},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":56257,\"start\":55499},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":58649,\"start\":56258}]", "paragraph": "[{\"end\":2765,\"start\":2071},{\"end\":3195,\"start\":2767},{\"end\":3579,\"start\":3197},{\"end\":4465,\"start\":3581},{\"end\":4989,\"start\":4467},{\"end\":5695,\"start\":4991},{\"end\":6748,\"start\":5697},{\"end\":7178,\"start\":6750},{\"end\":7781,\"start\":7195},{\"end\":8379,\"start\":7783},{\"end\":9663,\"start\":8381},{\"end\":10179,\"start\":9665},{\"end\":11389,\"start\":10181},{\"end\":12337,\"start\":11422},{\"end\":12721,\"start\":12339},{\"end\":13299,\"start\":12775},{\"end\":13723,\"start\":13332},{\"end\":14230,\"start\":13725},{\"end\":14697,\"start\":14322},{\"end\":15029,\"start\":14699},{\"end\":15398,\"start\":15074},{\"end\":15444,\"start\":15413},{\"end\":15603,\"start\":15446},{\"end\":16050,\"start\":15605},{\"end\":16210,\"start\":16089},{\"end\":16613,\"start\":16212},{\"end\":17137,\"start\":16615},{\"end\":17255,\"start\":17188},{\"end\":17911,\"start\":17294},{\"end\":18577,\"start\":17938},{\"end\":18764,\"start\":18600},{\"end\":19276,\"start\":18776},{\"end\":19378,\"start\":19290},{\"end\":19662,\"start\":19380},{\"end\":19883,\"start\":19664},{\"end\":20052,\"start\":19885},{\"end\":20411,\"start\":20079},{\"end\":20612,\"start\":20441},{\"end\":21122,\"start\":20614},{\"end\":21514,\"start\":21124},{\"end\":21938,\"start\":21516},{\"end\":22179,\"start\":21957},{\"end\":23050,\"start\":22181},{\"end\":23292,\"start\":23052},{\"end\":23573,\"start\":23294},{\"end\":23730,\"start\":23575},{\"end\":24655,\"start\":23751},{\"end\":25435,\"start\":24657},{\"end\":26067,\"start\":25437},{\"end\":27096,\"start\":26069},{\"end\":29009,\"start\":27126},{\"end\":31066,\"start\":29096},{\"end\":31673,\"start\":31110},{\"end\":32160,\"start\":31675},{\"end\":32623,\"start\":32187},{\"end\":33139,\"start\":32625},{\"end\":33449,\"start\":33164},{\"end\":33670,\"start\":33472},{\"end\":33984,\"start\":33672},{\"end\":34098,\"start\":33986},{\"end\":34423,\"start\":34152},{\"end\":36426,\"start\":34499},{\"end\":36809,\"start\":36539},{\"end\":37513,\"start\":36811},{\"end\":37679,\"start\":37549},{\"end\":38117,\"start\":37681},{\"end\":38157,\"start\":38128},{\"end\":38511,\"start\":38159},{\"end\":38950,\"start\":38513},{\"end\":39783,\"start\":38952},{\"end\":40012,\"start\":39826},{\"end\":40055,\"start\":40026},{\"end\":40134,\"start\":40057},{\"end\":40292,\"start\":40191},{\"end\":40420,\"start\":40335},{\"end\":40597,\"start\":40422},{\"end\":40919,\"start\":40599},{\"end\":41139,\"start\":40921}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12774,\"start\":12722},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14321,\"start\":14231},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15073,\"start\":15030},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15412,\"start\":15399},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17187,\"start\":17138},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17293,\"start\":17256}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":20748,\"start\":20741},{\"end\":21569,\"start\":21562},{\"end\":21859,\"start\":21852},{\"end\":22658,\"start\":22651},{\"end\":23321,\"start\":23314},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26518,\"start\":26511},{\"end\":28222,\"start\":28215},{\"end\":28828,\"start\":28821},{\"end\":29887,\"start\":29880},{\"end\":29948,\"start\":29941},{\"end\":30887,\"start\":30880},{\"end\":33137,\"start\":33130},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33388,\"start\":33380},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34049,\"start\":34040},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34280,\"start\":34272},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34864,\"start\":34856},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36094,\"start\":36086},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38978,\"start\":38970}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2069,\"start\":2057},{\"attributes\":{\"n\":\"2.\"},\"end\":7193,\"start\":7181},{\"attributes\":{\"n\":\"3.\"},\"end\":11398,\"start\":11392},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11420,\"start\":11401},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13330,\"start\":13302},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16087,\"start\":16053},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17936,\"start\":17914},{\"attributes\":{\"n\":\"4.\"},\"end\":18598,\"start\":18580},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18774,\"start\":18767},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19288,\"start\":19279},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20077,\"start\":20055},{\"attributes\":{\"n\":\"5.\"},\"end\":20424,\"start\":20414},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20439,\"start\":20427},{\"attributes\":{\"n\":\"5.2.\"},\"end\":21955,\"start\":21941},{\"attributes\":{\"n\":\"5.3.\"},\"end\":23749,\"start\":23733},{\"attributes\":{\"n\":\"6.\"},\"end\":27124,\"start\":27099},{\"end\":29022,\"start\":29012},{\"end\":29046,\"start\":29025},{\"end\":29074,\"start\":29049},{\"end\":29094,\"start\":29077},{\"end\":31080,\"start\":31069},{\"end\":31108,\"start\":31083},{\"end\":32185,\"start\":32163},{\"end\":33162,\"start\":33142},{\"end\":33470,\"start\":33452},{\"end\":34123,\"start\":34101},{\"end\":34150,\"start\":34126},{\"end\":34455,\"start\":34426},{\"end\":34497,\"start\":34458},{\"end\":36467,\"start\":36429},{\"end\":36515,\"start\":36470},{\"end\":36537,\"start\":36518},{\"end\":37547,\"start\":37516},{\"end\":38126,\"start\":38120},{\"end\":39809,\"start\":39786},{\"end\":39824,\"start\":39812},{\"end\":40024,\"start\":40015},{\"end\":40189,\"start\":40137},{\"end\":40333,\"start\":40295},{\"end\":41142,\"start\":41141},{\"end\":41472,\"start\":41462},{\"end\":41702,\"start\":41692},{\"end\":42159,\"start\":42139},{\"end\":42358,\"start\":42348},{\"end\":42579,\"start\":42569},{\"end\":42850,\"start\":42840},{\"end\":42958,\"start\":42947},{\"end\":44314,\"start\":44298},{\"end\":45609,\"start\":45600},{\"end\":45784,\"start\":45779}]", "table": "[{\"end\":44296,\"start\":43857},{\"end\":45058,\"start\":44496},{\"end\":45598,\"start\":45230},{\"end\":45777,\"start\":45667},{\"end\":51699,\"start\":46189},{\"end\":53986,\"start\":51797},{\"end\":54728,\"start\":54283},{\"end\":55498,\"start\":54983},{\"end\":56257,\"start\":55595},{\"end\":58649,\"start\":56446}]", "figure_caption": "[{\"end\":41460,\"start\":41143},{\"end\":41690,\"start\":41474},{\"end\":41830,\"start\":41704},{\"end\":42137,\"start\":41833},{\"end\":42346,\"start\":42161},{\"end\":42567,\"start\":42360},{\"end\":42838,\"start\":42581},{\"end\":42945,\"start\":42852},{\"end\":43079,\"start\":42961},{\"end\":43857,\"start\":43082},{\"end\":44496,\"start\":44316},{\"end\":45230,\"start\":45061},{\"end\":45667,\"start\":45611},{\"end\":46189,\"start\":45786},{\"end\":51797,\"start\":51702},{\"end\":54283,\"start\":53989},{\"end\":54983,\"start\":54731},{\"end\":55595,\"start\":55501},{\"end\":56446,\"start\":56260}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2942,\"start\":2934},{\"end\":3648,\"start\":3640},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4651,\"start\":4643},{\"end\":10750,\"start\":10742},{\"end\":18170,\"start\":18161},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21033,\"start\":21025},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21439,\"start\":21431},{\"end\":23172,\"start\":23164},{\"end\":23344,\"start\":23336},{\"end\":26086,\"start\":26078},{\"end\":26427,\"start\":26414},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26962,\"start\":26953},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34610,\"start\":34601},{\"end\":35238,\"start\":35230},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":35720,\"start\":35712},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37483,\"start\":37475},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37758,\"start\":37749},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38179,\"start\":38170},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38510,\"start\":38500},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39064,\"start\":39055},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":39378,\"start\":39369},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":39656,\"start\":39648},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41060,\"start\":41051}]", "bib_author_first_name": "[{\"end\":59564,\"start\":59563},{\"end\":60025,\"start\":60019},{\"end\":60036,\"start\":60030},{\"end\":60048,\"start\":60041},{\"end\":60062,\"start\":60056},{\"end\":60396,\"start\":60387},{\"end\":60406,\"start\":60401},{\"end\":60417,\"start\":60411},{\"end\":60429,\"start\":60425},{\"end\":60442,\"start\":60434},{\"end\":60763,\"start\":60749},{\"end\":60772,\"start\":60768},{\"end\":60784,\"start\":60782},{\"end\":60797,\"start\":60790},{\"end\":60812,\"start\":60805},{\"end\":61143,\"start\":61137},{\"end\":61153,\"start\":61148},{\"end\":61165,\"start\":61161},{\"end\":61180,\"start\":61174},{\"end\":61511,\"start\":61502},{\"end\":61521,\"start\":61518},{\"end\":61533,\"start\":61529},{\"end\":61550,\"start\":61543},{\"end\":61567,\"start\":61561},{\"end\":61840,\"start\":61828},{\"end\":61858,\"start\":61850},{\"end\":61865,\"start\":61859},{\"end\":61880,\"start\":61873},{\"end\":61893,\"start\":61887},{\"end\":62340,\"start\":62334},{\"end\":62359,\"start\":62352},{\"end\":62373,\"start\":62368},{\"end\":62387,\"start\":62381},{\"end\":62400,\"start\":62394},{\"end\":62415,\"start\":62409},{\"end\":62710,\"start\":62704},{\"end\":62722,\"start\":62718},{\"end\":62951,\"start\":62946},{\"end\":62961,\"start\":62956},{\"end\":63245,\"start\":63240},{\"end\":63256,\"start\":63253},{\"end\":63275,\"start\":63267},{\"end\":63772,\"start\":63771},{\"end\":63789,\"start\":63781},{\"end\":63791,\"start\":63790},{\"end\":63813,\"start\":63803},{\"end\":64096,\"start\":64085},{\"end\":64113,\"start\":64107},{\"end\":64501,\"start\":64495},{\"end\":64518,\"start\":64511},{\"end\":64535,\"start\":64528},{\"end\":64964,\"start\":64956},{\"end\":64975,\"start\":64971},{\"end\":64980,\"start\":64976},{\"end\":64998,\"start\":64992},{\"end\":65010,\"start\":65006},{\"end\":65026,\"start\":65021},{\"end\":65042,\"start\":65036},{\"end\":65058,\"start\":65052},{\"end\":65553,\"start\":65548},{\"end\":65566,\"start\":65563},{\"end\":65589,\"start\":65580},{\"end\":65605,\"start\":65598},{\"end\":65618,\"start\":65613},{\"end\":65635,\"start\":65628},{\"end\":65649,\"start\":65640},{\"end\":66578,\"start\":66573},{\"end\":66593,\"start\":66586},{\"end\":66602,\"start\":66599},{\"end\":66905,\"start\":66901},{\"end\":66919,\"start\":66915},{\"end\":66924,\"start\":66920},{\"end\":66935,\"start\":66930},{\"end\":66951,\"start\":66945},{\"end\":66967,\"start\":66960},{\"end\":66981,\"start\":66973},{\"end\":66997,\"start\":66991},{\"end\":67012,\"start\":67006},{\"end\":67027,\"start\":67021},{\"end\":67041,\"start\":67037},{\"end\":67521,\"start\":67516},{\"end\":67534,\"start\":67530},{\"end\":67548,\"start\":67544},{\"end\":67567,\"start\":67558},{\"end\":67579,\"start\":67573},{\"end\":67595,\"start\":67588},{\"end\":67609,\"start\":67604},{\"end\":67619,\"start\":67616},{\"end\":67629,\"start\":67624},{\"end\":67631,\"start\":67630},{\"end\":68037,\"start\":68029},{\"end\":68051,\"start\":68043},{\"end\":68065,\"start\":68058},{\"end\":68079,\"start\":68072},{\"end\":68091,\"start\":68086},{\"end\":68101,\"start\":68097},{\"end\":68112,\"start\":68109},{\"end\":68124,\"start\":68119},{\"end\":68654,\"start\":68648},{\"end\":68682,\"start\":68676},{\"end\":69220,\"start\":69213},{\"end\":69232,\"start\":69227},{\"end\":69248,\"start\":69242},{\"end\":69761,\"start\":69754},{\"end\":70032,\"start\":70028},{\"end\":70048,\"start\":70046},{\"end\":70311,\"start\":70305},{\"end\":70324,\"start\":70320},{\"end\":70330,\"start\":70325},{\"end\":70642,\"start\":70633},{\"end\":70656,\"start\":70651},{\"end\":70918,\"start\":70914},{\"end\":70934,\"start\":70927},{\"end\":71529,\"start\":71528},{\"end\":71550,\"start\":71543},{\"end\":71570,\"start\":71562},{\"end\":71592,\"start\":71581},{\"end\":71602,\"start\":71598},{\"end\":71618,\"start\":71613},{\"end\":72054,\"start\":72049},{\"end\":72069,\"start\":72061},{\"end\":72081,\"start\":72074},{\"end\":72091,\"start\":72086},{\"end\":72104,\"start\":72097},{\"end\":72120,\"start\":72111},{\"end\":72132,\"start\":72128},{\"end\":72146,\"start\":72143},{\"end\":72158,\"start\":72152},{\"end\":72167,\"start\":72165},{\"end\":72613,\"start\":72607},{\"end\":72627,\"start\":72620},{\"end\":72638,\"start\":72637},{\"end\":72640,\"start\":72639},{\"end\":72661,\"start\":72657},{\"end\":72677,\"start\":72671},{\"end\":73248,\"start\":73241},{\"end\":73257,\"start\":73256},{\"end\":73269,\"start\":73264},{\"end\":73287,\"start\":73278},{\"end\":73289,\"start\":73288},{\"end\":73305,\"start\":73297},{\"end\":73656,\"start\":73652},{\"end\":73669,\"start\":73664},{\"end\":73686,\"start\":73679},{\"end\":73932,\"start\":73925},{\"end\":73948,\"start\":73941},{\"end\":74244,\"start\":74239},{\"end\":74258,\"start\":74252},{\"end\":74274,\"start\":74265},{\"end\":74585,\"start\":74579},{\"end\":74599,\"start\":74592},{\"end\":74609,\"start\":74605},{\"end\":74619,\"start\":74614},{\"end\":74634,\"start\":74626},{\"end\":74646,\"start\":74640},{\"end\":75307,\"start\":75304},{\"end\":75318,\"start\":75313},{\"end\":75584,\"start\":75576},{\"end\":75594,\"start\":75591},{\"end\":75606,\"start\":75601},{\"end\":75623,\"start\":75617},{\"end\":75625,\"start\":75624},{\"end\":75644,\"start\":75637},{\"end\":76073,\"start\":76066},{\"end\":76089,\"start\":76084},{\"end\":76107,\"start\":76101},{\"end\":76364,\"start\":76357},{\"end\":76389,\"start\":76381},{\"end\":76622,\"start\":76617},{\"end\":76639,\"start\":76632},{\"end\":76657,\"start\":76650},{\"end\":76673,\"start\":76669},{\"end\":76949,\"start\":76940},{\"end\":76960,\"start\":76955},{\"end\":76975,\"start\":76970},{\"end\":76992,\"start\":76986},{\"end\":77006,\"start\":77001},{\"end\":77209,\"start\":77203},{\"end\":77221,\"start\":77214},{\"end\":77234,\"start\":77228},{\"end\":77246,\"start\":77241},{\"end\":77262,\"start\":77256},{\"end\":77524,\"start\":77522},{\"end\":77539,\"start\":77531},{\"end\":77553,\"start\":77546},{\"end\":77564,\"start\":77560},{\"end\":77580,\"start\":77574},{\"end\":77594,\"start\":77586},{\"end\":77607,\"start\":77600},{\"end\":77619,\"start\":77612},{\"end\":77632,\"start\":77627},{\"end\":77645,\"start\":77637},{\"end\":77994,\"start\":77990},{\"end\":78014,\"start\":78008},{\"end\":78026,\"start\":78021},{\"end\":78283,\"start\":78279},{\"end\":78293,\"start\":78289},{\"end\":78307,\"start\":78302},{\"end\":78321,\"start\":78317},{\"end\":78600,\"start\":78598},{\"end\":78613,\"start\":78608},{\"end\":78929,\"start\":78922},{\"end\":78944,\"start\":78936},{\"end\":78955,\"start\":78951},{\"end\":78962,\"start\":78956},{\"end\":78973,\"start\":78968},{\"end\":79417,\"start\":79410},{\"end\":79432,\"start\":79424},{\"end\":79443,\"start\":79439},{\"end\":79450,\"start\":79444},{\"end\":79461,\"start\":79456}]", "bib_author_last_name": "[{\"end\":59574,\"start\":59565},{\"end\":59583,\"start\":59576},{\"end\":60028,\"start\":60026},{\"end\":60039,\"start\":60037},{\"end\":60054,\"start\":60049},{\"end\":60066,\"start\":60063},{\"end\":60399,\"start\":60397},{\"end\":60409,\"start\":60407},{\"end\":60423,\"start\":60418},{\"end\":60432,\"start\":60430},{\"end\":60448,\"start\":60443},{\"end\":60766,\"start\":60764},{\"end\":60780,\"start\":60773},{\"end\":60788,\"start\":60785},{\"end\":60803,\"start\":60798},{\"end\":60818,\"start\":60813},{\"end\":61146,\"start\":61144},{\"end\":61159,\"start\":61154},{\"end\":61172,\"start\":61166},{\"end\":61184,\"start\":61181},{\"end\":61516,\"start\":61512},{\"end\":61527,\"start\":61522},{\"end\":61541,\"start\":61534},{\"end\":61559,\"start\":61551},{\"end\":61575,\"start\":61568},{\"end\":61848,\"start\":61841},{\"end\":61871,\"start\":61866},{\"end\":61885,\"start\":61881},{\"end\":61899,\"start\":61894},{\"end\":62350,\"start\":62341},{\"end\":62366,\"start\":62360},{\"end\":62379,\"start\":62374},{\"end\":62392,\"start\":62388},{\"end\":62407,\"start\":62401},{\"end\":62422,\"start\":62416},{\"end\":62716,\"start\":62711},{\"end\":62731,\"start\":62723},{\"end\":62954,\"start\":62952},{\"end\":62969,\"start\":62962},{\"end\":63251,\"start\":63246},{\"end\":63265,\"start\":63257},{\"end\":63283,\"start\":63276},{\"end\":63779,\"start\":63773},{\"end\":63801,\"start\":63792},{\"end\":63820,\"start\":63814},{\"end\":63828,\"start\":63822},{\"end\":64105,\"start\":64097},{\"end\":64123,\"start\":64114},{\"end\":64509,\"start\":64502},{\"end\":64526,\"start\":64519},{\"end\":64544,\"start\":64536},{\"end\":64969,\"start\":64965},{\"end\":64990,\"start\":64981},{\"end\":65004,\"start\":64999},{\"end\":65019,\"start\":65011},{\"end\":65034,\"start\":65027},{\"end\":65050,\"start\":65043},{\"end\":65067,\"start\":65059},{\"end\":65561,\"start\":65554},{\"end\":65578,\"start\":65567},{\"end\":65596,\"start\":65590},{\"end\":65611,\"start\":65606},{\"end\":65626,\"start\":65619},{\"end\":65638,\"start\":65636},{\"end\":65656,\"start\":65650},{\"end\":66584,\"start\":66579},{\"end\":66597,\"start\":66594},{\"end\":66610,\"start\":66603},{\"end\":66913,\"start\":66906},{\"end\":66928,\"start\":66925},{\"end\":66943,\"start\":66936},{\"end\":66958,\"start\":66952},{\"end\":66971,\"start\":66968},{\"end\":66989,\"start\":66982},{\"end\":67004,\"start\":66998},{\"end\":67019,\"start\":67013},{\"end\":67035,\"start\":67028},{\"end\":67047,\"start\":67042},{\"end\":67528,\"start\":67522},{\"end\":67542,\"start\":67535},{\"end\":67556,\"start\":67549},{\"end\":67571,\"start\":67568},{\"end\":67586,\"start\":67580},{\"end\":67602,\"start\":67596},{\"end\":67614,\"start\":67610},{\"end\":67622,\"start\":67620},{\"end\":67635,\"start\":67632},{\"end\":68041,\"start\":68038},{\"end\":68056,\"start\":68052},{\"end\":68070,\"start\":68066},{\"end\":68084,\"start\":68080},{\"end\":68095,\"start\":68092},{\"end\":68107,\"start\":68102},{\"end\":68117,\"start\":68113},{\"end\":68127,\"start\":68125},{\"end\":68674,\"start\":68655},{\"end\":68688,\"start\":68683},{\"end\":68698,\"start\":68690},{\"end\":69225,\"start\":69221},{\"end\":69240,\"start\":69233},{\"end\":69254,\"start\":69249},{\"end\":69767,\"start\":69762},{\"end\":70044,\"start\":70033},{\"end\":70056,\"start\":70049},{\"end\":70318,\"start\":70312},{\"end\":70339,\"start\":70331},{\"end\":70348,\"start\":70341},{\"end\":70649,\"start\":70643},{\"end\":70665,\"start\":70657},{\"end\":70925,\"start\":70919},{\"end\":70942,\"start\":70935},{\"end\":71541,\"start\":71530},{\"end\":71560,\"start\":71551},{\"end\":71579,\"start\":71571},{\"end\":71596,\"start\":71593},{\"end\":71611,\"start\":71603},{\"end\":71625,\"start\":71619},{\"end\":71632,\"start\":71627},{\"end\":72059,\"start\":72055},{\"end\":72072,\"start\":72070},{\"end\":72084,\"start\":72082},{\"end\":72095,\"start\":72092},{\"end\":72109,\"start\":72105},{\"end\":72126,\"start\":72121},{\"end\":72141,\"start\":72133},{\"end\":72150,\"start\":72147},{\"end\":72163,\"start\":72159},{\"end\":72172,\"start\":72168},{\"end\":72618,\"start\":72614},{\"end\":72635,\"start\":72628},{\"end\":72655,\"start\":72641},{\"end\":72669,\"start\":72662},{\"end\":72683,\"start\":72678},{\"end\":73254,\"start\":73249},{\"end\":73262,\"start\":73258},{\"end\":73276,\"start\":73270},{\"end\":73295,\"start\":73290},{\"end\":73310,\"start\":73306},{\"end\":73315,\"start\":73312},{\"end\":73662,\"start\":73657},{\"end\":73677,\"start\":73670},{\"end\":73692,\"start\":73687},{\"end\":73939,\"start\":73933},{\"end\":73966,\"start\":73949},{\"end\":73972,\"start\":73968},{\"end\":74250,\"start\":74245},{\"end\":74263,\"start\":74259},{\"end\":74281,\"start\":74275},{\"end\":74590,\"start\":74586},{\"end\":74603,\"start\":74600},{\"end\":74612,\"start\":74610},{\"end\":74624,\"start\":74620},{\"end\":74638,\"start\":74635},{\"end\":74651,\"start\":74647},{\"end\":75311,\"start\":75308},{\"end\":75325,\"start\":75319},{\"end\":75589,\"start\":75585},{\"end\":75599,\"start\":75595},{\"end\":75615,\"start\":75607},{\"end\":75635,\"start\":75626},{\"end\":75650,\"start\":75645},{\"end\":76082,\"start\":76074},{\"end\":76099,\"start\":76090},{\"end\":76115,\"start\":76108},{\"end\":76379,\"start\":76365},{\"end\":76396,\"start\":76390},{\"end\":76630,\"start\":76623},{\"end\":76648,\"start\":76640},{\"end\":76667,\"start\":76658},{\"end\":76682,\"start\":76674},{\"end\":76953,\"start\":76950},{\"end\":76968,\"start\":76961},{\"end\":76984,\"start\":76976},{\"end\":76999,\"start\":76993},{\"end\":77015,\"start\":77007},{\"end\":77212,\"start\":77210},{\"end\":77226,\"start\":77222},{\"end\":77239,\"start\":77235},{\"end\":77254,\"start\":77247},{\"end\":77268,\"start\":77263},{\"end\":77529,\"start\":77525},{\"end\":77544,\"start\":77540},{\"end\":77558,\"start\":77554},{\"end\":77572,\"start\":77565},{\"end\":77584,\"start\":77581},{\"end\":77598,\"start\":77595},{\"end\":77610,\"start\":77608},{\"end\":77625,\"start\":77620},{\"end\":77635,\"start\":77633},{\"end\":77648,\"start\":77646},{\"end\":78006,\"start\":77995},{\"end\":78019,\"start\":78015},{\"end\":78030,\"start\":78027},{\"end\":78287,\"start\":78284},{\"end\":78300,\"start\":78294},{\"end\":78315,\"start\":78308},{\"end\":78325,\"start\":78322},{\"end\":78606,\"start\":78601},{\"end\":78618,\"start\":78614},{\"end\":78934,\"start\":78930},{\"end\":78949,\"start\":78945},{\"end\":78966,\"start\":78963},{\"end\":78977,\"start\":78974},{\"end\":79422,\"start\":79418},{\"end\":79437,\"start\":79433},{\"end\":79454,\"start\":79451},{\"end\":79465,\"start\":79462}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7016601},\"end\":59911,\"start\":59488},{\"attributes\":{\"doi\":\"arXiv:2201.12086\",\"id\":\"b1\"},\"end\":60301,\"start\":59913},{\"attributes\":{\"doi\":\"arXiv:2205.11100\",\"id\":\"b2\"},\"end\":60677,\"start\":60303},{\"attributes\":{\"doi\":\"arXiv:1908.03557\",\"id\":\"b3\"},\"end\":61037,\"start\":60679},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":199453025},\"end\":61452,\"start\":61039},{\"attributes\":{\"doi\":\"arXiv:1306.5151\",\"id\":\"b5\"},\"end\":61782,\"start\":61454},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":231728349},\"end\":62283,\"start\":61784},{\"attributes\":{\"doi\":\"arXiv:2105.04289\",\"id\":\"b7\"},\"end\":62636,\"start\":62285},{\"attributes\":{\"doi\":\"arXiv:2210.07183\",\"id\":\"b8\"},\"end\":62905,\"start\":62638},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":220055965},\"end\":63165,\"start\":62907},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":227254464},\"end\":63670,\"start\":63167},{\"attributes\":{\"id\":\"b11\"},\"end\":64019,\"start\":63672},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15193013},\"end\":64389,\"start\":64021},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":250334627},\"end\":64878,\"start\":64391},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3604848},\"end\":65509,\"start\":64880},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202539551},\"end\":66474,\"start\":65511},{\"attributes\":{\"doi\":\"arXiv:2209.03320\",\"id\":\"b16\"},\"end\":66828,\"start\":66476},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":231591445},\"end\":67431,\"start\":66830},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":204838007},\"end\":67954,\"start\":67433},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":244800733},\"end\":68575,\"start\":67956},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13029170},\"end\":69141,\"start\":68577},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":247476067},\"end\":69638,\"start\":69143},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":182656421},\"end\":69983,\"start\":69640},{\"attributes\":{\"id\":\"b23\"},\"end\":70257,\"start\":69985},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3431470},\"end\":70567,\"start\":70259},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":246485723},\"end\":70820,\"start\":70569},{\"attributes\":{\"id\":\"b26\"},\"end\":71068,\"start\":70822},{\"attributes\":{\"id\":\"b27\"},\"end\":71446,\"start\":71070},{\"attributes\":{\"id\":\"b28\"},\"end\":72047,\"start\":71448},{\"attributes\":{\"doi\":\"arXiv:2204.09222\",\"id\":\"b29\"},\"end\":72514,\"start\":72049},{\"attributes\":{\"id\":\"b30\"},\"end\":73157,\"start\":72516},{\"attributes\":{\"doi\":\"arXiv:2210.01848\",\"id\":\"b31\"},\"end\":73554,\"start\":73159},{\"attributes\":{\"id\":\"b32\"},\"end\":73851,\"start\":73556},{\"attributes\":{\"doi\":\"arXiv:1212.0402\",\"id\":\"b33\"},\"end\":74172,\"start\":73853},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15206880},\"end\":74525,\"start\":74174},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":237513360},\"end\":75227,\"start\":74527},{\"attributes\":{\"doi\":\"arXiv:1908.07490\",\"id\":\"b36\"},\"end\":75498,\"start\":75229},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":214641252},\"end\":75935,\"start\":75500},{\"attributes\":{\"id\":\"b38\"},\"end\":76325,\"start\":75937},{\"attributes\":{\"doi\":\"2008. 14\",\"id\":\"b39\",\"matched_paper_id\":5855042},\"end\":76574,\"start\":76327},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":8909022},\"end\":76914,\"start\":76576},{\"attributes\":{\"id\":\"b41\"},\"end\":77149,\"start\":76916},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":221172666},\"end\":77520,\"start\":77151},{\"attributes\":{\"doi\":\"arXiv:2111.11432\",\"id\":\"b43\"},\"end\":77953,\"start\":77522},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":249209990},\"end\":78213,\"start\":77955},{\"attributes\":{\"doi\":\"arXiv:2203.17271\",\"id\":\"b45\"},\"end\":78522,\"start\":78215},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":229678413},\"end\":78864,\"start\":78524},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":247363011},\"end\":79361,\"start\":78866},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":237386023},\"end\":79700,\"start\":79363}]", "bib_title": "[{\"end\":59561,\"start\":59488},{\"end\":61135,\"start\":61039},{\"end\":61826,\"start\":61784},{\"end\":62944,\"start\":62907},{\"end\":63238,\"start\":63167},{\"end\":64083,\"start\":64021},{\"end\":64493,\"start\":64391},{\"end\":64954,\"start\":64880},{\"end\":65546,\"start\":65511},{\"end\":66899,\"start\":66830},{\"end\":67514,\"start\":67433},{\"end\":68027,\"start\":67956},{\"end\":68646,\"start\":68577},{\"end\":69211,\"start\":69143},{\"end\":69752,\"start\":69640},{\"end\":70026,\"start\":69985},{\"end\":70303,\"start\":70259},{\"end\":70631,\"start\":70569},{\"end\":71526,\"start\":71448},{\"end\":72605,\"start\":72516},{\"end\":74237,\"start\":74174},{\"end\":74577,\"start\":74527},{\"end\":75574,\"start\":75500},{\"end\":76355,\"start\":76327},{\"end\":76615,\"start\":76576},{\"end\":77201,\"start\":77151},{\"end\":77988,\"start\":77955},{\"end\":78596,\"start\":78524},{\"end\":78920,\"start\":78866},{\"end\":79408,\"start\":79363}]", "bib_author": "[{\"end\":59576,\"start\":59563},{\"end\":59585,\"start\":59576},{\"end\":60030,\"start\":60019},{\"end\":60041,\"start\":60030},{\"end\":60056,\"start\":60041},{\"end\":60068,\"start\":60056},{\"end\":60401,\"start\":60387},{\"end\":60411,\"start\":60401},{\"end\":60425,\"start\":60411},{\"end\":60434,\"start\":60425},{\"end\":60450,\"start\":60434},{\"end\":60768,\"start\":60749},{\"end\":60782,\"start\":60768},{\"end\":60790,\"start\":60782},{\"end\":60805,\"start\":60790},{\"end\":60820,\"start\":60805},{\"end\":61148,\"start\":61137},{\"end\":61161,\"start\":61148},{\"end\":61174,\"start\":61161},{\"end\":61186,\"start\":61174},{\"end\":61518,\"start\":61502},{\"end\":61529,\"start\":61518},{\"end\":61543,\"start\":61529},{\"end\":61561,\"start\":61543},{\"end\":61577,\"start\":61561},{\"end\":61850,\"start\":61828},{\"end\":61873,\"start\":61850},{\"end\":61887,\"start\":61873},{\"end\":61901,\"start\":61887},{\"end\":62352,\"start\":62334},{\"end\":62368,\"start\":62352},{\"end\":62381,\"start\":62368},{\"end\":62394,\"start\":62381},{\"end\":62409,\"start\":62394},{\"end\":62424,\"start\":62409},{\"end\":62718,\"start\":62704},{\"end\":62733,\"start\":62718},{\"end\":62956,\"start\":62946},{\"end\":62971,\"start\":62956},{\"end\":63253,\"start\":63240},{\"end\":63267,\"start\":63253},{\"end\":63285,\"start\":63267},{\"end\":63781,\"start\":63771},{\"end\":63803,\"start\":63781},{\"end\":63822,\"start\":63803},{\"end\":63830,\"start\":63822},{\"end\":64107,\"start\":64085},{\"end\":64125,\"start\":64107},{\"end\":64511,\"start\":64495},{\"end\":64528,\"start\":64511},{\"end\":64546,\"start\":64528},{\"end\":64971,\"start\":64956},{\"end\":64992,\"start\":64971},{\"end\":65006,\"start\":64992},{\"end\":65021,\"start\":65006},{\"end\":65036,\"start\":65021},{\"end\":65052,\"start\":65036},{\"end\":65069,\"start\":65052},{\"end\":65563,\"start\":65548},{\"end\":65580,\"start\":65563},{\"end\":65598,\"start\":65580},{\"end\":65613,\"start\":65598},{\"end\":65628,\"start\":65613},{\"end\":65640,\"start\":65628},{\"end\":65658,\"start\":65640},{\"end\":66586,\"start\":66573},{\"end\":66599,\"start\":66586},{\"end\":66612,\"start\":66599},{\"end\":66915,\"start\":66901},{\"end\":66930,\"start\":66915},{\"end\":66945,\"start\":66930},{\"end\":66960,\"start\":66945},{\"end\":66973,\"start\":66960},{\"end\":66991,\"start\":66973},{\"end\":67006,\"start\":66991},{\"end\":67021,\"start\":67006},{\"end\":67037,\"start\":67021},{\"end\":67049,\"start\":67037},{\"end\":67530,\"start\":67516},{\"end\":67544,\"start\":67530},{\"end\":67558,\"start\":67544},{\"end\":67573,\"start\":67558},{\"end\":67588,\"start\":67573},{\"end\":67604,\"start\":67588},{\"end\":67616,\"start\":67604},{\"end\":67624,\"start\":67616},{\"end\":67637,\"start\":67624},{\"end\":68043,\"start\":68029},{\"end\":68058,\"start\":68043},{\"end\":68072,\"start\":68058},{\"end\":68086,\"start\":68072},{\"end\":68097,\"start\":68086},{\"end\":68109,\"start\":68097},{\"end\":68119,\"start\":68109},{\"end\":68129,\"start\":68119},{\"end\":68676,\"start\":68648},{\"end\":68690,\"start\":68676},{\"end\":68700,\"start\":68690},{\"end\":69227,\"start\":69213},{\"end\":69242,\"start\":69227},{\"end\":69256,\"start\":69242},{\"end\":69769,\"start\":69754},{\"end\":70046,\"start\":70028},{\"end\":70058,\"start\":70046},{\"end\":70320,\"start\":70305},{\"end\":70341,\"start\":70320},{\"end\":70350,\"start\":70341},{\"end\":70651,\"start\":70633},{\"end\":70667,\"start\":70651},{\"end\":70927,\"start\":70914},{\"end\":70944,\"start\":70927},{\"end\":71543,\"start\":71528},{\"end\":71562,\"start\":71543},{\"end\":71581,\"start\":71562},{\"end\":71598,\"start\":71581},{\"end\":71613,\"start\":71598},{\"end\":71627,\"start\":71613},{\"end\":71634,\"start\":71627},{\"end\":72061,\"start\":72049},{\"end\":72074,\"start\":72061},{\"end\":72086,\"start\":72074},{\"end\":72097,\"start\":72086},{\"end\":72111,\"start\":72097},{\"end\":72128,\"start\":72111},{\"end\":72143,\"start\":72128},{\"end\":72152,\"start\":72143},{\"end\":72165,\"start\":72152},{\"end\":72174,\"start\":72165},{\"end\":72620,\"start\":72607},{\"end\":72637,\"start\":72620},{\"end\":72657,\"start\":72637},{\"end\":72671,\"start\":72657},{\"end\":72685,\"start\":72671},{\"end\":73256,\"start\":73241},{\"end\":73264,\"start\":73256},{\"end\":73278,\"start\":73264},{\"end\":73297,\"start\":73278},{\"end\":73312,\"start\":73297},{\"end\":73317,\"start\":73312},{\"end\":73664,\"start\":73652},{\"end\":73679,\"start\":73664},{\"end\":73694,\"start\":73679},{\"end\":73941,\"start\":73925},{\"end\":73968,\"start\":73941},{\"end\":73974,\"start\":73968},{\"end\":74252,\"start\":74239},{\"end\":74265,\"start\":74252},{\"end\":74283,\"start\":74265},{\"end\":74592,\"start\":74579},{\"end\":74605,\"start\":74592},{\"end\":74614,\"start\":74605},{\"end\":74626,\"start\":74614},{\"end\":74640,\"start\":74626},{\"end\":74653,\"start\":74640},{\"end\":75313,\"start\":75304},{\"end\":75327,\"start\":75313},{\"end\":75591,\"start\":75576},{\"end\":75601,\"start\":75591},{\"end\":75617,\"start\":75601},{\"end\":75637,\"start\":75617},{\"end\":75652,\"start\":75637},{\"end\":76084,\"start\":76066},{\"end\":76101,\"start\":76084},{\"end\":76117,\"start\":76101},{\"end\":76381,\"start\":76357},{\"end\":76398,\"start\":76381},{\"end\":76632,\"start\":76617},{\"end\":76650,\"start\":76632},{\"end\":76669,\"start\":76650},{\"end\":76684,\"start\":76669},{\"end\":76955,\"start\":76940},{\"end\":76970,\"start\":76955},{\"end\":76986,\"start\":76970},{\"end\":77001,\"start\":76986},{\"end\":77017,\"start\":77001},{\"end\":77214,\"start\":77203},{\"end\":77228,\"start\":77214},{\"end\":77241,\"start\":77228},{\"end\":77256,\"start\":77241},{\"end\":77270,\"start\":77256},{\"end\":77531,\"start\":77522},{\"end\":77546,\"start\":77531},{\"end\":77560,\"start\":77546},{\"end\":77574,\"start\":77560},{\"end\":77586,\"start\":77574},{\"end\":77600,\"start\":77586},{\"end\":77612,\"start\":77600},{\"end\":77627,\"start\":77612},{\"end\":77637,\"start\":77627},{\"end\":77650,\"start\":77637},{\"end\":78008,\"start\":77990},{\"end\":78021,\"start\":78008},{\"end\":78032,\"start\":78021},{\"end\":78289,\"start\":78279},{\"end\":78302,\"start\":78289},{\"end\":78317,\"start\":78302},{\"end\":78327,\"start\":78317},{\"end\":78608,\"start\":78598},{\"end\":78620,\"start\":78608},{\"end\":78936,\"start\":78922},{\"end\":78951,\"start\":78936},{\"end\":78968,\"start\":78951},{\"end\":78979,\"start\":78968},{\"end\":79424,\"start\":79410},{\"end\":79439,\"start\":79424},{\"end\":79456,\"start\":79439},{\"end\":79467,\"start\":79456}]", "bib_venue": "[{\"end\":59647,\"start\":59585},{\"end\":60017,\"start\":59913},{\"end\":60385,\"start\":60303},{\"end\":60747,\"start\":60679},{\"end\":61235,\"start\":61186},{\"end\":61500,\"start\":61454},{\"end\":61982,\"start\":61901},{\"end\":62332,\"start\":62285},{\"end\":62702,\"start\":62638},{\"end\":63020,\"start\":62971},{\"end\":63366,\"start\":63285},{\"end\":63769,\"start\":63672},{\"end\":64190,\"start\":64125},{\"end\":64615,\"start\":64546},{\"end\":65146,\"start\":65069},{\"end\":65833,\"start\":65658},{\"end\":66571,\"start\":66476},{\"end\":67093,\"start\":67049},{\"end\":67673,\"start\":67637},{\"end\":68210,\"start\":68129},{\"end\":68798,\"start\":68700},{\"end\":69337,\"start\":69256},{\"end\":69796,\"start\":69769},{\"end\":70096,\"start\":70058},{\"end\":70405,\"start\":70350},{\"end\":70678,\"start\":70667},{\"end\":70912,\"start\":70822},{\"end\":71190,\"start\":71070},{\"end\":71701,\"start\":71634},{\"end\":72249,\"start\":72190},{\"end\":72779,\"start\":72685},{\"end\":73239,\"start\":73159},{\"end\":73650,\"start\":73556},{\"end\":73923,\"start\":73853},{\"end\":74338,\"start\":74283},{\"end\":74739,\"start\":74653},{\"end\":75302,\"start\":75229},{\"end\":75690,\"start\":75652},{\"end\":76064,\"start\":75937},{\"end\":76442,\"start\":76406},{\"end\":76733,\"start\":76684},{\"end\":76938,\"start\":76916},{\"end\":77319,\"start\":77270},{\"end\":77708,\"start\":77666},{\"end\":78067,\"start\":78032},{\"end\":78277,\"start\":78215},{\"end\":78686,\"start\":78620},{\"end\":79060,\"start\":78979},{\"end\":79507,\"start\":79467},{\"end\":62050,\"start\":61984},{\"end\":63434,\"start\":63368},{\"end\":65210,\"start\":65148},{\"end\":66011,\"start\":65835},{\"end\":68278,\"start\":68212},{\"end\":68883,\"start\":68800},{\"end\":69405,\"start\":69339},{\"end\":71297,\"start\":71192},{\"end\":71755,\"start\":71703},{\"end\":72860,\"start\":72781},{\"end\":74830,\"start\":74741},{\"end\":79128,\"start\":79062}]"}}}, "year": 2023, "month": 12, "day": 17}