{"id": 235417032, "updated": "2023-10-06 02:02:52.768", "metadata": {"title": "DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning", "authors": "[{\"first\":\"Daochen\",\"last\":\"Zha\",\"middle\":[]},{\"first\":\"Jingru\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Wenye\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Sheng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiangru\",\"last\":\"Lian\",\"middle\":[]},{\"first\":\"Xia\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Ji\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ICML", "journal": "12333-12344", "publication_date": {"year": 2021, "month": 6, "day": 11}, "abstract": "Games are abstractions of the real world, where artificial agents learn to compete and cooperate with other agents. While significant achievements have been made in various perfect- and imperfect-information games, DouDizhu (a.k.a. Fighting the Landlord), a three-player card game, is still unsolved. DouDizhu is a very challenging domain with competition, collaboration, imperfect information, large state space, and particularly a massive set of possible actions where the legal actions vary significantly from turn to turn. Unfortunately, modern reinforcement learning algorithms mainly focus on simple and small action spaces, and not surprisingly, are shown not to make satisfactory progress in DouDizhu. In this work, we propose a conceptually simple yet effective DouDizhu AI system, namely DouZero, which enhances traditional Monte-Carlo methods with deep neural networks, action encoding, and parallel actors. Starting from scratch in a single server with four GPUs, DouZero outperformed all the existing DouDizhu AI programs in days of training and was ranked the first in the Botzone leaderboard among 344 AI agents. Through building DouZero, we show that classic Monte-Carlo methods can be made to deliver strong results in a hard domain with a complex action space. The code and an online demo are released at https://github.com/kwai/DouZero with the hope that this insight could motivate future work.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.06135", "mag": "3172924075", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/ZhaXMZLHL21", "doi": null}}, "content": {"source": {"pdf_hash": "93de1dc558edf1be761aec5b5011c3447eaf820c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.06135v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e0976259319a66eb7b6dd7396bd8fc37adfd90cc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/93de1dc558edf1be761aec5b5011c3447eaf820c.txt", "contents": "\nDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning\n\n\nDaochen Zha \nJingru Xie \nWenye Ma \nSheng Zhang \nXiangru Lian \nXia Hu \nJi Liu \nDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning\n\nGames are abstractions of the real world, where artificial agents learn to compete and cooperate with other agents. While significant achievements have been made in various perfect-and imperfectinformation games, DouDizhu (a.k.a. Fighting the Landlord), a three-player card game, is still unsolved. DouDizhu is a very challenging domain with competition, collaboration, imperfect information, large state space, and particularly a massive set of possible actions where the legal actions vary significantly from turn to turn. Unfortunately, modern reinforcement learning algorithms mainly focus on simple and small action spaces, and not surprisingly, are shown not to make satisfactory progress in DouDizhu. In this work, we propose a conceptually simple yet effective DouDizhu AI system, namely DouZero, which enhances traditional Monte-Carlo methods with deep neural networks, action encoding, and parallel actors. Starting from scratch in a single server with four GPUs, DouZero outperformed all the existing DouDizhu AI programs in days of training and was ranked the first in the Botzone leaderboard among 344 AI agents. Through building DouZero, we show that classic Monte-Carlo methods can be made to deliver strong results in a hard domain with a complex action space. The code and an online demo are released 1 with the hope that this insight could motivate future work.\n\nIntroduction\n\nGames often serve as benchmarks of AI since they are abstractions of many real-world problems. Significant achievements have been made in perfect-information games. For\n\nProceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). 1 https://github.com/kwai/DouZero example, AlphaGo (Silver et al., 2016), AlphaZero (Silver et al., 2018) and MuZero (Schrittwieser et al., 2020) have established state-of-the-art performance on Go game. Recent research has evolved to more challenging imperfectinformation games, where the agents compete or cooperate with others in a partially observable environment. Encouraging progress has been made from two-player games, such as simple Leduc Hold'em and limit/no-limit Texas Hold'em (Zinkevich et al., 2008;Heinrich & Silver, 2016;Morav\u010d\u00edk et al., 2017;Brown & Sandholm, 2018), to multi-player games, such as multi-player Texas hold'em (Brown & Sandholm, 2019b) This work aims at building AI programs for DouDizhu 2 (a.k.a. Fighting the Landlord), the most popular card game in China with hundreds of millions of daily active players. DouDizhu has two interesting properties that pose great challenges for AI systems. First, the players in DouDizhu need to both compete and cooperate with others in a partially observable environment with limited communication. Specifically, two Peasants players will play as a team to fight against the Landlord player. Popular algorithms for poker games, such as Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2008)) and its variants, are often not sound in this complex three-player setting. Second, DouDizhu has a large number of information sets with a very large average size and has a very complex and large action space of up to 10 4 possible actions due to combinations of cards (Zha et al., 2019a). Unlike Texas Hold'em, the actions in DouDizhu can not be easily abstracted, which makes search computationally expensive and commonly used reinforcement learning algorithms less effective. Deep Q-Learning (DQN) (Mnih et al., 2015) is problematic in very large action space due to overestimating issue (Zahavy et al., 2018); policy gradient methods, such as A3C (Mnih et al., 2016), cannot leverage the action features in DouDizhu, and thus cannot generalize over unseen actions as naturally as DQN (Dulac-Arnold et al., 2015). Not surprisingly, previous work shows that DQN and A3C can not make satisfactory progress in DouDizhu. In (You et al., 2019), DQN and A3C are shown to have less than 20% winning percentage against simple rule-based agents even with twenty days of training; the DQN in (Zha et al., 2019a) is only slightly better than random agents that sample legal moves uniformly.\n\nSome previous efforts have been made to build DouDizhu AI by combining human heuristics with learning and search. Combination Q-Network (CQN) (You et al., 2019) proposes to reduce the action space by decoupling the actions into decomposition selection and final move selection. However, decomposition relies on human heuristics and is extremely slow. In practice, CQN can not even beat simple heuristic rules after twenty days of training. DeltaDou (Jiang et al., 2019) is the first AI program that reaches human-level performance compared with top human players. It enables an AlphaZero-like algorithm by using Bayesian methods to infer hidden information and sampling the other players' actions based on their own policy networks. To abstract the action space, DeltaDou pre-trains a kicker network based on heuristic rules. However, the kicker plays an important role in DouDizhu and can not be easily abstracted. A bad selection of the kicker may directly result in losing a game since it may break some other card categories, e.g., a Chain of Solo. Moreover, the Bayesian inference and the search are computationally expensive. It takes more than two months to train DeltaDou even when initializing the networks with supervised regression to heuristics (Jiang et al., 2019). Therefore, the existing DouDizhu AI programs are computationally expensive and could be sub-optimal since they highly rely on abstractions with human knowledge.\n\nIn this work, we present DouZero, a conceptually simple yet effective AI system for DouDizhu without the abstraction of the state/action space or any human knowledge. DouZero enhances traditional Monte-Carlo methods (Sutton & Barto, 2018) with deep neural networks, action encoding, and parallel actors. DouZero has two desirable properties. First, unlike DQN, it is not susceptible to overestimation bias. Second, by encoding the actions into card matrices, it can naturally generalize over the actions that are not frequently seen throughout the training process. Both of these two properties are crucial in dealing with the huge and complex action space of DouDizhu. Unlike many tree search algorithms, DouZero is based on sampling, which allows us to use complex neural architectures and generate much more data per second, given the same computational resources. Unlike many prior poker AI studies that rely on domain-specific abstractions, DouZero does not require any domain knowledge or knowledge of the underlying dynamics. Trained from scratch in a single server with only 48 cores and four 1080Ti GPUs, DouZero outperforms CQN and the heuristic rules in half a day, beats our internal supervised agents in two days, and surpasses DeltaDou in ten days. Extensive evaluations suggest that DouZero is the strongest DouDizhu AI system up to date.  Through building DouZero system, we demonstrate that classical Monte-Carlo methods can be made to deliver strong results in large-scale and complex card games that need to reason about both competing and cooperation over huge state and action spaces. We note that some work also discovers that Monte-Carlo methods can achieve competitive performance (Mania et al., 2018;Zha et al., 2021a) and help in sparse rewards settings (Guo et al., 2018;Zha et al., 2021b). Unlike these studies that focus on simple and small environments, we demonstrate the strong performance of Monte-Carlo methods on a large-scale card game. With the hope that this insight could facilitate future research on tackling multi-agent learning, sparse reward, complex action spaces, and imperfect information, we have released our environment and the training code. Unlike many Poker AI systems that require thousands of CPUs in training, e.g., DeepStack (Morav\u010d\u00edk et al., 2017) and Libratus (Brown & Sandholm, 2018), DouZero enables a reasonable experimental pipeline, which only requires days of training on a single GPU server that is affordable for most research labs. We hope that it could motivate future research in this domain and serve as a strong baseline.\n\n\nBackground of DouDizhu\n\nDouDizhu is a popular three-player card game that is easy to learn but difficult to master. It has attracted hundreds of millions of players in China, with many tournaments held every year. It is a shedding-type game where the player's objective is to empty one's hand of all cards before other players. Two of the Peasants players play as a team to fight against the other Landlord player. The Peasants win if either of the Peasants players is the first to have no cards left. Each game has a bidding phase, where the players bid for the Landlord based on the strengths of the hand cards, and a card-playing phase, where the players play cards in turn.\n\nWe provide a detailed introduction in Appendix A.\n\nDouDizhu is still an unsolved benchmark for multi-agent reinforcement learning (Zha et al., 2019a;Terry et al., 2020). Two interesting properties make DouDizhu particularly challenging to solve. First, the Peasants need to cooperate in fighting against the Landlord. For example, Figure 10 shows a typical situation where the bottom Peasant can choose to play a small Solo to help the Peasant on the right-hand side to win. Second, DouDizhu has a complex and large action space due to the combination of cards. There are 27, 472 possible combinations, where different subsets of these combinations will be legal for different hands. Figure 1 shows an example of the hand, which has 391 legal combinations, including Solo, Pair, Trio, Bomb, Plane, Quad, etc. The action space can not be easily abstracted since improperly playing a card may break other categories and directly result in losing a game. Thus, building DouDizhu AI is challenging since the players in DouDizhu need to reason about both competing and cooperation over a huge action space.\n\n\nDeep Monte-Carlo\n\nIn this section, we revisit Monte-Carlo (MC) methods and introduce Deep Monte-Carlo (DMC), which generalizes MC with deep neural networks for function approximation. Then we discuss and compare DMC with policy gradient methods (e.g., A3C) and DQN, which are shown to fail in DouDizhu (You et al., 2019;Zha et al., 2019a).\n\n\nMonte-Carlo Methods with Deep Neural Networks\n\nMonte-Carlo (MC) methods are traditional reinforcement learning algorithms based on averaging sample returns (Sutton & Barto, 2018). MC methods are designed for episodic tasks, where experiences can be divided into episodes and all the episodes eventually terminate. To optimize a policy \u03c0, every-visit MC can be used to estimate Q-table Q(s, a) by iteratively executing the following procedure:\n\n1. Generate an episode using \u03c0.\n\n2. For each s, a appeared in the episode, calculate and update Q(s, a) with the return averaged over all the samples concerning s, a. 3. For each s in the episode, \u03c0(s) \u2190 arg max a Q(s, a).\n\n\nThe average return in\n\nStep 2 is usually obtained by the discounted cumulative reward. Different from Q-learning that relies on bootstrapping, MC methods directly approximate the target Q-value. In step 1, we can use epsilon-greedy to balance exploration and exploitation. The above procedure can be naturally combined with deep neural networks, which leads to Deep Monte-Carlo (DMC). Specifically, we can replace the Q-table with a neural network and use meansquare-error (MSE) to update the Q-network in Step 2.\n\nWhile MC methods are criticized not to be able to deal with incomplete episodes and believed to be inefficient due to the high variance (Sutton & Barto, 2018), DMC is very suitable for DouDizhu. First, DouDizhu is an episodic task so that we do not need to handle incomplete episodes. Second, DMC can be easily parallelized to efficiently generate many samples per second to alleviate the high variance issue.\n\n\nComparison with Policy Gradient Methods\n\nPolicy gradients methods, such as REINFORCE (Williams, 1992), A3C (Mnih et al., 2016), PPO (Schulman et al., 2017), andIMPALA (Espeholt et al., 2018), are very popular for reinforcement learning. They target modeling and optimizing the policy directly with gradient descent. In policy gradient methods, we often use a classifier-like function approximator, where the output scales linearly with the number of actions. While policy gradients methods work well in large action space, they cannot use the action features to reason about previously unseen actions (Dulac-Arnold et al., 2015). In practice, the actions in DouDizhu can be naturally encoded into card matrices, which are crucial for reasoning. For example, if the agent is rewarded by the action 3KKK because it chooses a nice kicker, it could also generalize this knowledge to unseen actions in the future, such as 3JJJ. This property is crucial in dealing with very large action spaces and accelerating the learning since many of the actions are not frequently seen in the simulated data.\n\nDMC can naturally leverage the action features to generalize over unseen actions by taking as input the action features. While it might have high execution complexity if the action size is large, in most states of DouDizhu, only a subset of the actions is legal, so that we do not need to iterate over all the actions. Thus, DMC is overall an efficient algorithm for DouDizhu. While it is possible to introduce action features into an actor-critic framework (e.g., by using a Q-network as the critic), the classifier-like actor will still suffer from the large action space. Our preliminary experiments confirm that this strategy is not very effective (see Figure 7).\n\n\nComparison with Deep Q-Learning\n\nThe most popular value-based algorithm is Deep Q-Learning (DQN) (Mnih et al., 2015), which is a bootstrapping method that updates the Q-value based on the Q-values in the next step. While both DMC and DQN approximate the Q-values, DMC has several advantages in DouDizhu.\n\nFirst, the overestimation bias caused by approximating the maximum action value in DQN is difficult to control when using function approximation (Thrun & Schwartz, 1993;Hasselt, 2010) and becomes more pronounced with very large action space (Zahavy et al., 2018). While some techniques, such as double Q-learning (van Hasselt et al., 2016) and experience replay (Lin, 1992), might alleviate this issue, we find in practice that DQN is very unstable and often diverges in DouDizhu. Whereas, Monte-Carlo estimation is not susceptible to bias since it directly approximates the true values without bootstrapping (Sutton & Barto, 2018).\n\nSecond, DouDizhu is a task with long horizons and sparse reward, i.e., the agent will need to go though a long chain of states without feedback, and the only time a nonzero reward is incurred is at the end of a game. This may slow down the convergence of Q-learning because estimating the Q-value in the current state needs to wait until the value in the next state gets close to its true value (Szepesv\u00e1ri, 2009;Beleznay et al., 1999). Unlike DQN, the convergence of Monte-Carlo estimation is not impacted by the episode length since it directly approximates the true target values. \n\n\nDouZero System\n\nIn this section, we introduce DouZero system by first describing the state/action representations and neural architecture and then elaborating on how we parallelize DMC with multiple processes to stabilize and accelerate training.\n\n\nCard Representation and Neural Architecture\n\nWe encode each card combination with a one-hot 4 \u00d7 15 matrix ( Figure 2). Since suits are irrelevant in DouDizhu, we use each row to represent the number of cards of a specific rank or joker. Figure 3 shows the architecture of the Q-network. For the state, we extract several card matrices to represent the hand cards, the union of the other players' hand cards and the most recent moves, and some one-hot vectors to represent the number of cards of the other players and the number of bombs played so far. Similarly, we use one card matrix to encode the action. For the neural architecture, LSTM is used to encode historical moves, and the output is concatenated with the other state/action features. Finally, we use six layers of MLP with a hidden size of 512 to produce Q-values. We provide more details in Appendix C.1.\n\n\nParallel Actors\n\nWe denote Landlord as L, the player that moves before the Landlord as U, and the player that moves after the Landlord as D. We parallelize DMC with multiple actor processes and one learner process, summarized in Algorithm 1 and Algorithm 2, respectively. The learner maintains three global Q-networks for the three positions and updates the networks with MSE loss to approximate the target values based on the data provided by the actor processes. Each actor maintains three local Q-networks, which are synchronized with the global networks periodically. The actor will repeatedly sample trajectories from the game engine and calculate cumulative reward for each state-action pair. The communication of learner and actors are implemented with three shared buffers. Each buffer is divided into several entries, where each entry consists of several data instances.\n\n\nExperiments\n\nThe experiments are designed to answer the following research questions. end for 20: end for methods (Section 5.5)? RQ5: Does the learned card playing strategies of DouZero align with human knowledge (Section 5.6)? RQ6: Is DouZero computationally efficient in inference compared with existing programs (Section 5.7)? RQ7: Can the two Peasants of DouZero learn to cooperate with each other (Section 5.8)?\n\n\nExperimental Setup\n\nA commonly used measure of strategy strength in poker games is exploitability (Johanson et al., 2011). However, in DouDizhu, calculating exploitability itself is intractable since DouDizhu has huge state/action spaces, and there are three players. To evaluate the performance, following (Jiang et al., 2019), we launch tournaments that include the two opponent sides of Landlord and Peasants. We reduce the variance by playing each deck twice. Specifically, for two competing algorithms A and B, they will first play as Landlord and Peasants positions, respectively, for a given deck. Then they switch sides, i.e., A takes Peasants position, and B takes Landlord position, and play the same deck again. To simulate the real environment, in Section 5.3, we further train a bidding network with supervised learning, and the agents will bid the Landlord in each game based on the strengths of the hand cards (more details in Appendix C.2). We consider the following competing algorithms. \u2022 SL: A supervised learning baseline. We internally collect 226, 230 human expert matches from the players of the highest level in league in our DouDizhu game mobile app. Then we use the same state representation and neural architecture as DouZero to train supervised agents with 49, 990, 075 samples generated from these data. See Appendix C.2 for more details.\n\n\nAlgorithm 2 Learner Process of DouZero\n\n\u2022 Rule-Based Programs:\n\nWe collect some opensourced heuristic-based programs, including RHCP 4 , an improved version called RHCP-v2 5 , and the rule model in RLCard package 6 (Zha et al., 2019a). In addition, we consider a Random program that samples legal moves uniformly.\n\nMetrics. Following (Jiang et al., 2019), given an algorithm A and an opponent B, we use two metrics to compare the performance of A and B:\n\n\u2022 WP (Winning Percentage): The number of the games won by A divided by the total number of games. We find in practice that these two metrics encourage different styles of strategies. For example, if using ADP as reward, the agent tends to be very cautious about playing bombs since playing a bomb is risky and may lead to larger ADP loss. In contrast, with WP as objective, the agent tends to aggressively play bombs even if it will lose because a bomb will not affect WP. We observe that the agent trained with ADP performs slightly better than the agent trained with WP in terms of ADP and vice versa. In what follows, we train and report the results of two DouZero agents with ADP and WP as objectives, respectively 7 . More discussions of the two objectives are provided in Appendix D.2.\n\nWe first launch a preliminary tournament by letting each pair of the algorithms play 10,000 decks. We then compute the Elo rating score for the top 3 algorithms for a more reliable comparison, i.e., DouZero, DeltaDou, and SL, by playing 100,000 decks. An algorithm wins a deck if it achieves higher WP or ADP summed over the two games played on this deck. We repeat this process five times with different randomly sampled decks and report the mean and standard deviation of the Elo scores. For the evaluation with the bidding phase, each deck is played six times with different perturbations of DouZero, DeltaDou, and SL in different positions. We report the result with 100,000 decks.\n\nImplementation Details. We run all the experiments on a single server with 48 processors of Intel(R) Xeon(R) Silver 4214R CPU @ 2.40GHz and four 1080 Ti GPUs. We use 45 actors, which are allocated across three GPUs. We run a learner in the remaining GPU to train the Q-networks. Our implementation is based on TorchBeast framework (K\u00fcttler et al., 2019). The detailed training curves are provided in 7 For WP, we give a +1 or -1 reward to the final timestep based on whether the agent wins or loses a game. For ADP, we directly use ADP as the rewards. DeltaDou and CQN were trained with ADP and WP as objectives, respectively.  Figure 4. Left: Elo rating scores of DouZero, DeltaDou, and SL by playing 100,000 randomly sampled decks. We report the mean and standard deviation across 5 different random seeds. Right: Elo rating scores on Botzone, an online platform for DouDizhu competition. DouZero ranked the first among the 344 bots, achieving an Elo rating score of 1625.11 as of October 30, 2020.\n\nAppendix D.5. Each shared buffer has B = 50 entries with size S = 100, batch size M = 32, and = 0.01. We set discount factor \u03b3 = 1 since DouDizhu only has a nonzero reward in the last timestep and early moves are very important. We use ReLU as the activation function for each layer of MLP. We adopt RMSprop optimizer with a learning rate \u03c8 = 0.0001, smoothing constant 0.99 and = 10 \u22125 . We train DouZero for 30 days.\n\n\nPerformance against Existing Programs\n\nTo answer RQ1, we compare DouZero with the baselines offline and report its result on Botzone (Zhou et al., 2018), an online platform for DouDizhu competition (more details are provided in Appendix E).  . WP and ADP of DouZero against SL and DeltaDou w.r.t. the number of training days. DouZero outperforms SL with 2 days of training, i.e., the overall WP is larger than the threshold of 0.5 and the overall ADP is larger than the threshold of 0, and surpasses DeltaDou within 10 days, using a single server with four 1080 Ti GPUs and 48 processors. We provide the full curves for each position and the curves w.r.t. timesteps in Appendix D.3. The left-hand side of Figure 4 shows the Elo rating scores of DouZero, DeltaDou, and SL by playing 100, 000 decks. We observe that DouZero outperforms DeltaDou and SL in terms of both WP and ADP significantly. This again demonstrates the strong performance of DouZero.\n\nThe right-hand side of Figure 4 illustrates the performance of DouZero on Botzone leaderboard. We note that Botzone adopts a different scoring mechanism. In addition to WP, it gives additional bonuses to some specific card categories, such as Chain of Pair and Rocket (detailed in Appendix E). While it is very likely that DouZero can achieve better performance if using the scoring mechanism of Botzone as the objective, we directly upload the pre-trained model of DouZero that is trained with WP as objective. We observe that this model is strong enough to beat the other bots.\n\n\nComparison with Bidding Phase\n\nTo investigate RQ2, we train a bidding network with supervised learning using human expert data. We place the top-3 algorithms, i.e., DouZero, DeltaDou, and SL, into the three seats of a DouDizhu game. In each game, we randomly choose the first bidder and simulate the bidding phase with the pre-trained bidding network. The same bidding net- work is used for all three algorithms for a fair comparison.\n\nThe results are summarized in Table 2. Although DouZero is trained on randomly generated decks without the bidding network, we observe that DouZero dominates the other two algorithms in both WP and ADP. This demonstrates the applicability of DouZero in real-world competitions where the bidding phase needs to be considered.\n\n\nAnalysis of Learning Progress\n\nTo study RQ3, we visualize the learning progress of DouZero in Figure 5. We use SL and DeltaDou as opponents to draw the curves of WP and ADP w.r.t. the number of training days. We make two observations as follows. First, DouZero outperforms SL in one day and two days of training in terms of WP and ADP, respectively. We note that DouZero and SL use the exactly same neural architecture for training. Thus, we attribute the superiority of DouZero to self-play reinforcement learning. While SL also performs well, it relies on a large amount of data, which is not flexible and could limit its performance. Second, DouZero outperforms DeltaDou in three days and ten days of training in terms of WP and ADP, respectively. We note that DeltaDou is initialized with supervised learning on heuristics and is trained for more than two months. Whereas, DouZero starts from scratch and only needs days of training to beat DeltaDou. This suggests that model-free reinforcement learning without search is indeed effective in DouDizhu. We further analyze the learning speed when using different numbers of actors. Figure 6 reports the performance against SL when using 15, 30, and 45 actors. We observe that using more actors can accelerate the training in wall-clock time.\n\nWe also find that all three settings show similar sample efficiency. In the future, we will explore the possibility of using more actors across multiple servers to further improve the training efficiency.\n\n\nComparison with SARSA and Actor-Critic\n\nTo answer RQ4, we implement two variants based on DouZero. First, we replace the DMC objective with the Temporal-Difference (TD) objective. This leads to a deep version of SARSA. Second, we implement an Actor-Critic variant with action features. Specifically, we use Q-network as a critic with action features and train policy as an actor with action masks to remove illegal actions. Figure 7 shows the results of SARSA and Actor-Critic with a single run. First, we do not observe a clear benefit of using TD learning. We observe that DMC learns slightly faster than SARSA in wall-clock time and sample efficiency. The possible reason is that TD learning will not help much in the sparse reward setting. We believe more studies are needed to understand when TD learning will help. Second, we observe the Actor-Critic fails. This suggests that simply adding action features to the critic may not be enough to resolve the complex action space issue. In the future, we will investigate whether we can effectively incorporate action features into the actor-critic framework.\n\n\nAnalysis of DouZero on Expert Data\n\nFor RQ5, we calculate the accuracy of DouZero on the human data throughout the training process. We report the model trained with ADP as objective since the game app from which the human data is collected also adopts ADP. Figure 8 shows the results. We make two interesting observations as follows. First, at the early stages, i.e., the first five days of training, the accuracy keeps improving. This suggests that the agents may have learned some strategies that align with human expertise with purely self-play. Second, after five days of training, the accuracy decreases dramatically. We note that the ADP against SL is still improving after five days. This suggests that the agents may have discovered some novel and stronger strategies that humans can not easily discover, which again verifies the effectiveness of self-play reinforcement learning.\n\n\nComparison of Inference Time\n\nTo answer RQ6, we report the average inference time per step in Figure 9. For a fair comparison, we evaluate all the algorithms on the CPU. We observe that DouZero is orders of magnitude faster than DeltaDou, CQN, RHCP, and RHCP-v2. This is expected since DeltaDou needs to perform a large number of Monte Carlo simulations, and CQN, RHCP, and RHCP-v2 require expensive card decomposition. Whereas, DouZero only performs one forward pass of neural networks in each step. The efficient inference of DouZero enables us to generate a large number of samples per second for reinforcement learning. It also makes it affordable to deploy the models in real-world applications.\n\n\nCase Study\n\nTo investigate RQ7, we conduct case studies to understand the decisions made by DouZero. We dump the logs of the competitions from Botzone and visualize the top actions with their predicted Q-values. We provide most of the case studies, including both good and bad cases, in Appendix F. Second, the predicted Q-value of action 4 (0.808) is much lower than that of action 3 (0.971). A possible explanation is that there is still a 4 out there, so that playing 4 may not necessarily help the Peasant win. In practice, in this specific case, the other Peasant's only card is not higher than 4 in rank. Overall, action 3 is indeed the best move in this case.\n\n\nRelated Work\n\nSearch for Imperfect-Information Games. \n\n\nConclusions and Future Work\n\nThis work presents a strong AI system for DouDizhu. Some unique properties make DouDizhu particularly challenging to solve, e.g., huge state/action space and reasoning about both competing and cooperation. To address these challenges, we enhance classic Monte-Carlo methods with deep neural networks, action encoding, and parallel actors. This leads to a pure RL solution, namely DouZero, which is conceptually simple yet effective and efficient. Extensive evaluations demonstrate that DouZero is the strongest AI program for DouDizhu up to date. We hope the insight that simple Monte-Carlo methods can lead to strong policies in such a hard domain will motivate future research.\n\nFor future work, we will explore the following directions. First, we plan to try other neural architectures, such as convolutional neural networks and ResNet (He et al., 2016). Second, we will involve bidding in the loop for reinforcement learning. Third, we will combine DouZero with search at training and/or test time as in (Brown et al., 2020), and study how to balance RL and search. Fourth, we will explore off-policy learning to improve the training efficiency. Specifically, we will study whether and how we can improve the wall-clock time and the sample efficiency with experience replay (Lin, 1992;Zhang & Sutton, 2017;Zha et al., 2019b;Fedus et al., 2020). Fifth, we will try explicitly modeling collaboration of the Gray, J., Lerer, A., Bakhtin, A., and Brown, N. Humanlevel performance in no-press diplomacy via equilibrium search. arXiv preprint arXiv:2010.02923, 2020.\n\nGuo, Y., Oh, J., Singh, S., and Lee, H. Generative adversarial self-imitation learning. arXiv preprint arXiv:1812.00950, 2018.\n\nHasselt  \n\n\nA. Introduction of DouDizhu\n\nAs the most popular card game in China, DouDizhu has attracted hundreds of millions of players with many tournaments held every year. DouDizhu is known to be easy to learn but challenging to master. It requires careful planning and strategic thinking. DouDizhu is played among three players. In each game, the players will first bid for the Landlord position. After the bidding phase, one player will become the Landlord, and the other two players will become the Peasants. The two Peasants play as a team to fight against the Landlord. The objective of the game is to be the first player to have no cards left. In addition to the huge state/action spaces and incomplete information, the two Peasants need to cooperate to beat the Landlord. Thus, existing algorithms for poker games, which usually operate on small games and are only designed for two players, are not applicable in DouDizhu. In what follows, we first give an overview of the game rule of DouDizhu and then analyze the state/action spaces of DouDizhu. Readers who are familiar with the game may skip Section A.1. Readers who are not familiar with DouDizhu may also refer to Wikipedia 9 for more introduction.\n\n\nA.1. Rules\n\nDouDizhu is played with one pack of cards, including the two jokers. Suits are irrelevant in DouDizhu. The cards are ranked by Red Joker, Black Joker, 2, A, K, Q, J, 10, 9, 8, 7, 6, 5, 4, 3. Each game has three phases as follows.\n\n\u2022 Dealing: A shuffled pack of 54 cards will be dealt to the three players. Each player will be dealt 17 cards, and the last three leftover cards will be kept on the deck, face down. These three cards will be dealt to the Landlord, which are decided in the bidding phase.\n\n\u2022 Bidding: The three players will analyze their own cards without showing to other players. The players decide whether they would like to bid the Landlord based on their hand cards' strength. There are many versions of bidding rules. In this paper, we consider a version adopted in most online DouDizhu app. The first bidder will be randomly chosen. The first bidder will then decide whether she bids. If the first bidder does not bid, the other players will become the bidder in turn until someone bids. If no one bids, a new pack of cards will be dealt to the players. If one chooses to bid, the other players will decide whether she accepts the bid or she wants to outbid. Each player only has one chance to outbid. The last player who bids or outbids will become the Landlord. Once the Landlord is settled, the Landlord will be dealt with the three cards on the deck. The other two players will play as the Peasants to fight against the Landlord.\n\n\u2022 Card-Playing: In this phase, the players will play cards in turn starting from the Landlord. The first player can choose either category of cards such as Solo, Pair, etc. (detailed in the next paragraph). Then the next player must play the cards in the same category with a higher rank. The next player can also choose \"PASS\" if she does not have a higher rank in hand or she does not want to follow the category. If all the other players choose \"PASS,\" the player who first plays the category can freely play cards in other categories. The players will play cards in turn until one player has no cards left. The Landlord wins if she has no cards left. The Peasant team wins if either of the peasants has no cards left. The two Peasants need to cooperate to increase the possibility of winning. A Peasant may still win a game by helping the other Peasant win even if she has terrible hand cards.\n\nOne challenge of DouDizhu is the rich categories, which consist of various combinations of cards. For some categories, the player can choose a kicker card, which can be any card in hand. One will usually choose a useless card as a kicker card so that she can more easily go out of hand. As a result, the player needs to carefully plan how to play the cards to win a game. The categories in DouDizhu are listed as follows. Note that Bomb and Rocket defy the category rules and can dominate all the other categories.\n\n\u2022 Solo: Any single card.\n\n\u2022 Pair: Two matching cards of equal rank.\n\n\u2022 Trio: Three individual cards of equal rank.\n\n\u2022 Trio with Solo: Three individual cards of equal rank with a Solo as the kicker.\n\n\u2022 Trio with Pair: Three individual cards of equal rank with a Pair as the kicker. 9 https://en.wikipedia.org/wiki/Dou_dizhu \u2022 Chain of Solo: \u2265Five consecutive individual cards.\n\n\u2022 Chain of Pair: \u2265Three consecutive Pairs.\n\n\u2022 Chain of Trio: \u2265Two consecutive Trios.\n\n\u2022 Plane with Solo: \u2265Two consecutive trios with each has a distinct individual kicker card.\n\n\u2022 Quad with Pair: Four-of-a-kind with two sets of Pair as the kicker.\n\n\u2022 Bomb: Four-of-a-kind.\n\n\u2022 Rocket: Red and Black jokers.\n\n\nA.2. State and Action Space of DouDizhu\n\nAccording to the estimation in RLCard (Zha et al., 2019a), the number of information sets in DouDizhu is up to 10 83 and the average size of each information set is up to 10 23 . While the number of information sets is smaller than that of No-limit Texas Hold'em (10 126 ), the average size of each information set is much larger than that of No-limit Texas Hold'em (10 4 ). Different from Hold'em games, the state space of DouDizhu can not be easily abstracted. Specifically, every card matters in DouDizhu towards winning. For example, the number of cards of rank 2 in the historical moves is crucial since the players need to decide whether their cards will be dominated by other players with a 2. Thus, a very slight difference in the state representation could significantly impact the strategy. While the size of the state space of DouDizhu is not as large as that of No-limit Texas Hold'em, learning an effective strategy is very challenging since the agents need to distinguish different states accurately. DouZero approaches this problem by extracting representations and learning the strategy automatically with deep neural networks.\n\nDouDizhu suffers from an explosion of action space due to the combinations of cards. We summarize the action space in Table 3. The size of the action space of DouDizhu is 27, 472, which is much larger than Mahjong (10 2 ). It is also much more complicated than No-limit Texas Hold'em, whose action space can be easily abstracted. Specifically, in DouDizhu, every card matters. For example, for the action type Trio with Solo, wrongly choosing the kicker may directly result in a loss since it could potentially break a chain. Thus, it is difficult to abstract the action space. This poses challenges for reinforcement learning since most of the algorithms only work well on small action space. In contrast to the previous work that abstracts the action space with heuristics (Jiang et al., 2019), DouZero approaches this issue with Monte-Carlo methods, which allow flexible exploration of the action space to potentially discover better moves.  \n\n\nC. Additional Details of Feature Representation and Neural Architecture\n\n\nC.1. Action and State Representation\n\nThe input of the neural network is the concatenated representation of state and action. For each 15 \u00d7 4 card matrix, we first flatten the matrix into a 1-dimensional vector of size 60. Then we remove six entries that are always zero since there is only one black or red joker. In other words, each card matrix is transformed into a one-hot vector of size 54. In addition to card matrices, we further use a one-hot vector to represent the other two players' current hand cards. For example, for Peasant, we use a vector of size 17, where each entry corresponds to the number of hand cards in the current state. For the Landlord, the vector's size is 20 since the Landlord can have at most 20 cards in hand. Similarly, we use a 15-dimension vector to represent the number of bombs in the current state. For historical moves, we consider the most recent 15 moves and concatenate the representations of every three consecutive moves; that is, the historical moves are encoded into a 5 \u00d7 162 matrix. The historical moves are fed into an LSTM, and we use the hidden representation in the last cell to represent the historical moves. If there are less than 15 moves historically, we use zero matrices for the missing moves. We summarize the encoded features of Landlord and each Peasant in Table 4 and Table 5, respectively. In order to train an agent with supervised learning, we collect user data internally from a popular DouDizhu game mobile app. The users in the app have different leagues, which represent the strengths of the users. We filter out the raw data by only keeping the data generated by the players of the highest league to ensure the quality of the data. After filtering, we obtain 226,230 human expert matches. We treat each move as an instance and use a supervised loss to train the networks. The problem can be formulated as a classification problem, where we aim at predicting the action based on a given state, with a total of 27, 472 classes. However, we find in practice that most of the actions are illegal, and it is expensive to iterate over all the classes. Motivated by Q-network's design, we transform the problem into a binary classification task, as shown in Figure 12. Specifically, we use the same neural architecture as DouZero and add a Sigmoid function to the output. We then use binary cross-entropy loss to train the network. We randomly sample 10% of the data for validation purposes and use the rest for training. We transform the user data into positive instances and generate negative instances based on the legal moves that are not selected. Eventually, the training data consists of 49, 990, 075 instances. We further find that the data is imbalanced, where the number of negative instances is much larger than that of positive instances. Thus, we adopt a re-weighted cross-entropy loss based on the distribution of positive and negative instances. We find in practice that the re-weighted loss can improve the performance. We set the batch size to be 8096 and train 20 epochs. The prediction is made by choosing the action that leads to the highest score. We output the model that has the highest accuracy on the validation data. We do this process three times for the three positions, respectively. We plot the validation accuracy w.r.t. the number epochs in Figure 13. The network can achieve around 84% accuracy for all positions. Sigmoid Figure 12. We use the same neural architecture as DouZero for SL. We add a Sigmoid function to the output and transform the problem into a binary classification task. The agent will perform the action that leads to the highest prediction score. \n\n\nAct i on\n\n\nC.3. Neural Architecture and Training Details of Bidding Network\n\nThe bidding phase's goal is to determine whether a player should become the landlord based on the strengths of the hand cards. This decision is much simpler than card-playing since the agent only needs to consider the hand cards and the other players' decisions, and we only need to make a binary prediction, i.e., whether we bid. At the beginning of the bidding phase, a randomly chosen player will decide whether to bid or not. Then the other two players will also choose whether to bid. If only one player bids, then that player will become the landlord. Suppose two or more players bid, the player who bids first will have the priority to decide whether she wants to become the landlord. We extract 128 features to represent hand cards and the players' moves, as summarized in Table 6. For the network architecture, we use a (512,256,128,64,32,16) MLP. Like the supervised card playing agent, we add a Sigmoid function to the output and train the network with binary cross-entropy loss. We plot the validation accuracy w.r.t. the number epochs in Figure 14. The network can achieve 83.1% accuracy. \n\n\nFeature Size\n\nCard matrix of hand cards 54 A vector representing solos of ranks 3 to A 12 A vector representing pairs of ranks 3 to 2 13 A vector representing trios of ranks 3 to 2 13 A vector representing bombs of ranks 3 to 2 and the rocket 14 The number of cards of rank 2 and the jokers 10 A vector encoding historical bidding moves 12\n\nTotal 128 \n\n\nD. Additional Results of DouZero\n\n\nD.1. Full WP and ADP Results for Landlord and Peasants\n\nWe report the results for Landlord and Peasants in Table 7 and Table 8 for WP and ADP, respectively. We observe that the advantage of DouZero for Peasants tends to be larger than that of Landlord. A possible explanation is that the two Peasants agents in DouZero have learned cooperation skills, which could be hardly covered by the heuristics and other algorithms. Table 7. WP of DouZero and the baselines. L: WP of A as Landlord; P: WP of A as Peasants. If the average WP of L and P is higher than 0.5, we conclude that A outperforms B and highlight both L and P in boldface. The algorithms are ranked according to the number of the other algorithms that they beat.\n\n. \n\n\nRank\n\n\nD.2. Comparison of Using WP and ADP as Objectives\n\nIn our experiments, we find that the agents will learn different styles of card playing strategies when using WP and ADP as objectives. Specifically, we observe that the agents trained with WP play more aggressively about bombs even if it will lose. We visualize this phenomenon in Appendix F.4. The possible explanation is that a bomb will not double the points so that playing a bomb or rocket will not harm WP. Aggressively playing bombs may benefit WP since they will dominate other payers, which allows them to play hand cards freely. In contrast, the agents trained with ADP tend to be very cautious of playing bombs since improperly playing a bomb may double the ADP loss if the agents lose the game in the end.\n\nTo better interpret the differences between WP and ADP, we show the results of the agents trained with ADP and WP against the baselines. In Table 9, we report the results of DouZero trained with ADP using WP as the metric. We observe that the performance is slightly worse than that in Table 7. In Table 10, we show the results of DouZero trained with WP using ADP as the metric. Similarly, the ADP result is slightly worse than thate in Table 8. We observe similar results if considering the bidding phase (see Table 11 and Table 12). Finally, we launch a head-to-head competition of these two agents in Table 13. The results again verify that the agents trained with WP are better in terms of WP and vice versa. The above results suggest that WP and ADP are indeed different and encourage different card playing strategies.\n\nIn addition to WP and ADP, some other metrics could also be adopted in real-word DouDizhu completions. For example, some apps allow users to double the base score at the beginning of a game. We argue that we should adjust the objectives to achieve the best performance according to different scenarios.  Table 10. ADP of DouZero against baselines when using WP as the reward. L: ADP of A as Landlord; P: ADP of A as Peasants. If the average ADP of L and P is higher than 0, we conclude that A outperforms B and highlight both L and P in boldface. .  Table 13. Head-to-head comparison between using ADP and WP as objectives. DouZero (ADP) outperforms DouZero (WP) in terms of ADP but is worse than DouZero (WP) in terms of WP. The agents tend to learn different skills with different objectives. . WP and ADP of DouZero against SL and DeltaDou w.r.t. the number of training days. DouZero outperforms SL with 2 days of training, i.e., the overall WP is larger than the threshold of 0.5 and the overall ADP is larger than the threshold of 0, and surpasses DeltaDou within 10 days, using a single server with four 1080 Ti GPUs and 48 processors. . Accuracy for the three positions on the human data w.r.t. the number of training days for DouZero. We fit the data points with a polynomial with four terms for better visualizing the trend. LandlordUp stands for the Peasant that moves before the Landlord. LandlordDown stands for the Peasant that moves after the Landlord. The accuracies of SL for the Landlord, LandlordUp, LandlordDown are 83.3%, 86.1%, 83.1%, respectively. DouZero aligns with human expertise in the beginning training stages but discovers novel strategies beyond human knowledge in the later training stages.\nA B DouZero (WP) DeltaDou SL RHCP-v2 RHCP RLCard CQN Random L P L P L P L P L P L P L P L P DouZero(\n\nD.5. Training Curves of DouZero\n\nIn this work, we train three DouZero agents with self-play for all the positions (i.e., the Landlord and the two Peasants) without considering the bidding phase. For each episode, a deck will be randomly generated. Then the three agents will perform self-play with partially-observed states. The generated episodes will be passed to the learner process to update the three DouZero agents. In what follows, we show the self-play rewards and the losses throughout the training process. \n\n\nE. More Details of Botzone\n\nBotzone is a comprehensive multi-game, multi-agent online game AI platform hosted by AILab, Peking University 10 . Besides DouDizhu, Botzone supports more than 20 games, including Go, Mahjong, and Ataxx, to name a few. Botzone currently has more than 3,500 users and has hosted various games, from in-class and campus contests within Peking University to nation-and worldwide game AI competitions such as the recent IJCAI 2020 Mahjong AI competition, which attracted top AI researchers worldwide.\n\nOn the Botzone platform, users upload their bot program as a virtual agent to compete with other bots in a selected game. In order to do so, a user can either nominate opponents and manually start a game or add their bots to the Botzone Elo system, where games among bots will be scheduled automatically. A bot added to the Botzone Elo system will also be associated with the so-called Elo rating score and will be added to the rank list (leaderboard) of the game she plays. Botzone assigns an initial Elo rating score of 1000 to a new bot in the Elo, and the score is updated after each time the bot played an Elo rating game, as long as the bot remains in the Elo system.\n\n\nE.1. Interacting with Botzone\n\nBotzone provides a \"Judge\" program that runs in the background and interacts with bot programs of users. A bot receives a request to act from the \"Judge\" every time he has to take an action, i.e., it is her turn to play cards. In DouDizhu, the input sent to the bot contains three parts of information: her own hand cards, the public card, and a sequence of cards played by each player. This is consistent with the incomplete information known to a human player in a common DouDizhu game setting.\n\nThe platform also sets constraints on the file size, memory, and running time of bots. Each decision made by the bot has to be completed within 1 second (or 6 seconds for a Python program) with no more than 256 MB of memory. The model size is limited to 140 MB.\n\n\nE.2. Botzone Ranking Rules\n\nBotzone maintains a leaderboard for each game, which ranks all the bots in the Botzone Elo system by their Elo rating scores in descending order. Every five minutes, Botzone schedules match with randomly selected bots, with priority given to new bots with recent updates. The Elo rating score of participating bots will be updated after the match.\n\nIn the Botzone Elo of DouDizhu (named \"FightTheLandlord\" on the Botzone platform), each game is played by two bots, with one bot acting as the Landlord and the other as Peasants. A pair of games are played simultaneously, in which the two bots will play different roles; that is, the bot who plays the Landlord in one game will play the Peasants in another, and vice versa. The two games form a match, and the Elo rating of each bot is updated according to the match outcome, as well as their relative ratings.\n\nThe game score of a bot is mainly determined by whether she wins or loses a game. The winning bot receives a score of two points, while the losing bot receives a score of zero 11 . To encourage more complicated card-playing strategies, Botzone associates small points with categories of the cards played by a bot and adds that to the game score. The total game score is thus the winning point (2 for the winner and 0 for the loser) plus the card-playing advantage, the sum of weights assigned according to categories of cards (Table 14) played by a bot in the entire game divided by 100. Since by convention, two Peasants always receive the same game score, Peasants' game score is the average of their individual scores.\n\nThe match score is determined by the sum of game scores of the two games in the round, which further determines how the Elo rating will change for each player. If the match score of one bot is higher than the other, then the bot is considered the winner of this match. The winning bot will receive an increase in its Elo rating, while the same amount of rating points will be taken off from the losing bot.\n\n\nE.3. Discussion of Ranking Stability\n\nAlthough Elo rating is generally considered a stable measurement of relative strength among a pool of players in games like Chess and Go, DouDizhu Elo ranking on Botzone suffers from some fluidity. This could be attributed to the nature of the high variance of the game and also the design of Botzone Elo. Firstly, the game outcomes of DouDizhu relies on the luck of initial hand cards. In particular, if a player with bad hand cards is playing the Landlord, he will have a low chance to win a game. In practice, the bidding phase could compensate for this randomness, such that the player with bad initial hand cards can choose not to bid for the Landlord. However, Botzone does not incorporate the bidding phase into the game playing; rather, the Landlord position is specified even before the dealing phase happened. Secondly, although two bots exchange roles between games in each match, these two games are not initialized with the same hand cards. It is not rare to see that one bot was assigned bad initial hand cards in both games, making it infeasible for her to win the match. Finally, Elo rating games are not scheduled as frequently on Botzone, potentially due to limited server resources. We observe that, on average, DouZero has the chance to play one Elo rating game about every 2 hours. As such, it might take a long time for a bot to achieve a stable ranking. With bots continuously added to or leaving the Elo system, it might be just impossible to observe absolute stable ranking. Nonetheless, since ranked top on Botzone for the first time on October 30, 2020, DouZero has remained in the top-5 most of the time (at the time of submission deadline, DouZero was still ranked first with around 1600 points). While the rank of DouZero is impacted by the high variance of the BotZone platform, DouZero has maintained an Elo rating score of at least 1480 points during the months between October 30, 2020, to the ICML submission deadline, suggesting that DouZero has at least 95% chance of winning in a match with an average bot.\n\n\nF. Additional Case Studies\n\nIn this section, we conduct case studies for DouZero. We show both per-step decision with figures and the logs of full games. For simplicity, we use \"T\" to denote \"10\",\"P\" to denote \"PASS\", \"B\" to denote Black Joker, and \"R\" to denote Red Joker. Each move is represented as \"position:move\", where position can be \"L\" for Landlord, \"U\" for LandlordUp (i.e., the Peasant that moves before the Landlord), \"D\" for LandlordDown (i.e., the Peasant that moves after the Landlord). For example, \"L:3555\" means Landlord plays 3555, and \"U:T\" means LandlordUp plays 10. The initial hands are represented as \"H:Landlord Hand; LandlordDown Hand; LandlordUp Hand\". The moves and the hands are separated by \",\". Note that except Section F.4, we focus on the agent with WP as objective. Thus, the agents tend to ambitiously play bombs even when they will lose, and will not try to play more bombs when they think they will win.     Table 17. Case 3: Comparison of DouZero in early stage and later stage. DouZero plays the Landlord position on the same deck with the same opponents. In the early stage, the first move of DouZero is 9. However there is a 4 in the hand, which causes troubles in later turns. In the later stage, DouZero plays in a different style by starting with 33 and finally wins the game. Although playing 9 seems to be not bad, it may lead to losing the game later. . Case 2: Comparison of using WP and ADP as objectives. The two agents play the same deck twice. While both agents win this game, they have different styles. The WP agent plays Quad with Solo instead of a bomb because it can empty the hand more quickly. This is reasonable since playing one more bomb will not double WP. In contrast, the ADP agent first plays a 2 so that it can play a bomb later. The ADP agent will try to play every bomb when it thinks it can win the game. \n\n\nF.2. Cooperation of Peasants\n\n\nLogs\n\n\n, Starcraft (Vinyals et al., 2019), DOTA (Berner et al., 2019), Hanabi (Lerer et al., 2020), Mahjong (Li et al., 2020a), Honor of Kings (Ye et al., 2020b;a), and No-Press Diplomacy (Gray et al., 2020).\n\nFigure 1 .\n1A hand and its corresponding legal moves.\n\nFigure 2 .\n2Cards for both states and actions are encoded into a 4\u00d715 one-hot matrix, where columns correspond to the 13 ranks and the jokers, and each row corresponds to the number of cards of a specific rank or joker. More examples are provided in Appendix B.\n\nFigure 3 .\n3The Q-network of DouZero consists of an LSTM to encode historical moves and six layers of MLP with hidden dimension of 512. The network predicts a value for a given state-action pair based on the concatenated representation of action and state. More details are provided in Appendix C.1.\n\nRQ1 :\nRQ1How does DouZero compare with existing DouDizhu programs, such as rule-based strategies, supervised learning, RL-based methods, and MCTS-based solutions (Section 5.2)? RQ2: How will DouZero perform if we consider bidding phase (Section 5.3)? RQ3: How efficient is the training of DouZero (Section 5.4)? RQ4: How does DouZero compare with bootstrapping and actor critic Algorithm 1 Actor Process of DouZero 1: Input: Shared buffers B L , B U and B D with B entries and size S for each entry, exploration hyperparameter , discount factor \u03b3 2: Initialize local Q-networks Q L , Q U and Q D , and local buffers D L , D U and D D 3: for iteration = 1, 2, ... do 4: Synchronize Q L , Q U and Q D with the learner process 5: for t = 1, 2, ... T do Generate an episode 6: Q \u2190 one of Q L , Q U , Q D based on position 7: a t \u2190 arg max a Q(s t , a) with prob (1 \u2212 ) random action with prob 8: Perform a t , observe s t+1 and reward r t 9: Store {s t , a t , r t } to D L , D U , or D \u2190 r t + \u03b3r t+1 and update r t in D L , D U , or D p \u2208 {L, U, D} do Optimized by multi-thread 15: if D p .length \u2265 L then 16: Request and wait for an empty entry in B p 17:Move {s t , a t , r t } of size L from D p to B p\n\nFigure 5\n5Figure 5. WP and ADP of DouZero against SL and DeltaDou w.r.t. the number of training days. DouZero outperforms SL with 2 days of training, i.e., the overall WP is larger than the threshold of 0.5 and the overall ADP is larger than the threshold of 0, and surpasses DeltaDou within 10 days, using a single server with four 1080 Ti GPUs and 48 processors. We provide the full curves for each position and the curves w.r.t. timesteps in Appendix D.3.\n\nFigure 6 .\n6Left: The WP against SL w.r.t. training time using different number of actors Right: The WP against SL w.r.t. timesteps using different number of actors.\n\nFigure 7 .Figure 8 .\n78Left: The WP against SL w.r.t. training time for SARSA and Actor-Critic Right: The WP against SL w.r.t. timesteps for SARSA and Actor-Critic. Average accuracy across the three positions on the human data w.r.t. the number of training days for DouZero. We fit the data points with a polynomial with four terms for better visualizing the trend. The accuracy for SL is 84.2%. DouZero aligns with human expertise in the first five days of training but discovers novel strategies beyond human knowledge in the later training stages. The curves for all the three positions are provided in Appendix D.4.\n\nFigure 9 .\n9Comparison of inference time.\n\n\n, H. V. Double q-learning. In Advances in Neural Information Processing Systems, 2010.He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition, 2016.Heinrich, J. and Silver, D. Deep reinforcement learning from self-play in imperfect-information games. arXiv preprint arXiv:1603.01121, 2016.Heinrich, J., Lanctot, M., and Silver, D. Fictitious self-play in extensive-form games.In International Conference on Machine Learning, 2015. Jiang, Q., Li, K., Du, B., Chen, H., and Fang, H. Deltadou: Expert-level doudizhu ai through self-play. In International Joint Conferences on Artificial Intelligence, 2019. Johanson, M., Waugh, K., Bowling, M., and Zinkevich, M. Accelerating best response calculation in large extensive games. In International Joint Conferences on Artificial Intelligence, 2011. K\u00fcttler, H., Nardelli, N., Lavril, T., Selvatici, M., Sivakumar, V., Rockt\u00e4schel, T., and Grefenstette, E. Torchbeast: A pytorch platform for distributed rl. arXiv preprint arXiv:1910.03552, 2019. Lai, K.-H., Zha, D., Li, Y., and Hu, X. Dual policy distillation. In International Joint Conference on Artificial Intelligence, 2020. Lanctot, M., Waugh, K., Zinkevich, M., and Bowling, M. Monte carlo sampling for regret minimization in extensive games. Advances in Neural Information Processing Systems, 2009.Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., P\u00e9rolat, J., Silver, D., and Graepel, T. A unified game-theoretic approach to multiagent reinforcement learning. In Advances in neural information processing systems, 2017.Lanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V., Upadhyay, S., P\u00e9rolat, J., Srinivasan, S., Timbers, F., Tuyls, K., Omidshafiei, S., et al. Openspiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453, 2019.Lerer, A., Hu, H., Foerster, J. N., and Brown, N. Improving policies via search in cooperative partially observable games. In AAAI Conference on Artificial Intelligence, 2020.Li, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang, R., Zhao, L., Qin, T., Liu, T.-Y., and Hon, H.-W. Suphx: Mastering mahjong with deep reinforcement learning. arXiv preprint arXiv:2003.13590, 2020a. Li, K., Xu, H., Zhang, M., Zhao, E., Wu, Z., Xing, J., and Huang, K. Openholdem: An open toolkit for large-scale imperfect-information game research. arXiv preprint arXiv:2012.06168, 2020b. Lin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8(3-4):293-321, 1992. Mania, H., Guy, A., and Recht, B. Simple random search provides a competitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055, 2018. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540): 529-533, 2015. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016. Morav\u010d\u00edk, M., Schmid, M., Burch, N., Lis\u1ef3, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowling, M. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017. Panait, L. and Luke, S. Cooperative multi-agent learning: The state of the art. Autonomous agents and multi-agent systems, 11(3):387-434, 2005. Raileanu, R., Denton, E., Szlam, A., and Fergus, R. Modeling others using oneself in multi-agent reinforcement learning. In International conference on machine Learning, 2018. Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839): 604-609, 2020. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-1144, 2018. Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018. Szepesv\u00e1ri, C. Algorithms for reinforcement learning. Morgan and Claypool, 2009. Terry, J. K., Black, B., Jayakumar, M., Hari, A., Sullivan, R., Santos, L., Dieffendahl, C., Williams, N. L., Lokesh, Y., Horsch, C., et al. Pettingzoo: Gym for multi-agent reinforcement learning. arXiv preprint arXiv:2009.14471, 2020. Thrun, S. and Schwartz, A. Issues in using function approximation for reinforcement learning. In Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum, 1993. van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In AAAI Conference on Artificial Intelligence, 2016. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575 (7782):350-354, 2019. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229-256, 1992.Ye, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B., Chen, J., Liu, Z., Qiu, F., Yu, H., et al. Towards playing full moba games with deep reinforcement learning. arXiv preprint arXiv:2011.12692, 2020a.\n\nFigure 11 .\n11Additional examples of encoding different types of cards.\n\nFigure 13 .\n13Accuracy w.r.t. the number of training epochs of SL for the three positions. LandlordUp stands for the Peasant that moves before the Landlord. LandlordDown stands for the Peasant that moves after the Landlord.\n\nFigure 14 .\n14Accuracy w.r.t. the number of training epochs of the bidding network.\n\nFigure 15\n15DouZero (ADP) vs DouZero (WP) -0.3101 0.4476 .3617 .5151 D.3. Additional Results of Learning Progress Over al l Peasan t s Lan dl or d Over al l Th r esh ol d\n\nFigure 16 .Figure 17\n1617WP and ADP of DouZero against SL and DeltaDou w.r.t. the number of training timesteps, i.e., the number of actions played by the agent. DouZero outperforms SL with around 5 \u00d7 10 8 training timesteps, i.e., the overall WP is larger than the threshold of 0.5 and the overall ADP is larger than the threshold of 0, and surpasses DeltaDou within 5 \u00d7 10 9 training timesteps, using a single server with four 1080 Ti GPUs and 48 processors.D.4. Full Results of DouZero on Expert Data\n\nFigure 18 .Figure 19 .Figure 20 .\n181920) WP w.r.t. timesteps ADP and WP w.r.t. training days and the number of timesteps for the Landlord and Peasants of DouZero during the training progress. At the early stage, the Peasants win the Landlord by a small margin. The peasants become stronger and stronger compared with the Landlord in the later training stages. Losses for different positions for DouZero trained with ADP as rewards. LandlordUp stands for the Peasant that moves before the Landlord. LandlordDown stands for the Peasant that moves after the Landlord.Lan dl or dLan dl or d Dow n Lan dl or d Up Losses for different positions for DouZero trained with WP as rewards. LandlordUp stands for the Peasant that moves before the Landlord. LandlordDown stands for the Peasant that moves after the Landlord.\n\nF. 1 .Figure 21 .Figure 22 .Figure 23 .Figure 24 .Figure 25 .\n12122232425Strategic Thinking of the Agents PASS PASS Top -3 M oves Lan dl or dDow n (Dou Zer o) Lan dl or d Lan dl or dUp (Dou Zer o) Case 1: Strategic thinking (turn 7). The LandlordUp has 4444 in hand. However, DouZero strategically chooses to break the bomb and play 45678. This is because playing this Chain of Solo can empty the hand more quickly. Full game: H:333456778889TJJKAA2R; 355667999TTJKKA2B; 4445678TJQQQQKA22, L:45678, D:P, U:TJQKA, L:P, D:P, U:45678, L:789TJ, D:P, U:P, L:3338, D:999J, U:4QQQ, L:P, D:P, U:22, L:P, D:P, U:4. Top -3 M oves Lan dl or dUp Lan dl or dDow n Lan dl or d (Dou Zer o) Case 2: Strategic thinking (turn 1). In this case, 56789TJQ is a very good move because the agent can play another Chain of Solo afterwards, i.e., TJQKA. Full game: H:333356778889TTJJQQKA; 44599TTJQQKKAA22R; 44556667789JKA22B, L:56789TJQ, D:P, U:P, L:TJQKA, D:P, U:P, L:7, D:J, U:K, L:P, D:P, U:44, L:88, D:99, U:22, L:3333. Case 3: Strategic thinking (turn 31). The Landlord has two pairs (AA and TT) and a small Solo in hand. DouZero chooses not to play AA in this turn. This is a nice move because there is a pair of K out there. If the Landlord plays AA, then the TT will be dominated by the KK in later turns. DouZero patiently chooses PASS and wins the game eventually. Full game: H:33455556889TTJAA22BR; 334677778TTJQKA22; 44668999JJQQQKKKA, L:33, D:22, U:P, L:BR, D:P, U:P, L:6, D:8, U:A, L:2, D:7777, U:P, L:P, D:TJQKA, U:P, L:5555, D:P, U:P, L:9, D:T, U:P, L:J, D:P, U:K, L:2, D:P, U:P, L:88, D:P, U:JJ, L:P, D:P, U:44999, L:P, D:P, U:66QQQ, L:P, D:P, U:KK, L:AA, D:P, U:P, L:TT, D:P, U:P, L:4.PASS Top -3 M oves Lan dl or dUp Lan dl or dDow n Lan dl or d (Dou Zer o) Case 4: Strategic thinking (turn 13). When the LandlordUp plays 333J, DouZero chooses not to play 888. This is a nice move because playing 888 will break a Chain of Solo, i.e., 456789T. Full game: H:45567788899TQKKKK22B; 34557789JJJQQAAA2; 333446669TTTJQA2R, L:7, D:8, U:9, L:Q, D:2, U:P, L:B, D:P, U:R, L:P, D:P, U:333J, L:P, D:P, U:666Q, L:P, D:P, U:TTTA, L:P, D:P, U:44, L:55, D:77, U:P, L:88, D:QQ, U:P, L:22, D:P, U:P, L:49KKKK, D:P, U:P, L:6789T. Case 5: Strategic thinking (turn 4). DouZero can choose a move from many possible legal moves. The top-1 move is nice because it is a very long Chain of Solo. The second and the third moves are also very good because they choose a good kicker. Full game: H:34446667899TJQQKA22R; 3455788TJJQKAA22B; 3355677899TTJQKKA, L:3444, D:P, U:P, L:789TJQKA, D:P, U:P, L:6669, D:P, U:P, L:22, D:P, U:P, L:Q, D:B, U:P, L:R.\n\nFigure 26 .Figure 27 .Figure 28 .\n262728or dDow n (Dou Zer o)Lan dl or dUp(Dou Zer o)    Case 1: Cooperation of Peasants (turn 29). Although the Landlord has much better hand than the Peasants (a Bomb plus a Rocket), the Peasants manage to win the game with cooperation. In this turn, the LandlordUp chooses PASS so that the LandlordDown can empty her hand. DouZero has learned not to fight against the teammate.Full game: H:3335556799JJJJQK22BR; 44445677899TQKKAA; 3667888TTTQQKAA22, L:3336, D:P, U:3888, L:JJJJ, D:P, U:P, L:5557, D:P, U:7TTT, L:BR, D:P, U:P, L:99, D:AA, U:22, L:P, D:P, U:AA, L:22, D:4444, U:P, L:P, D:Q, U:K, L:P, D:P, U:66, L:P, D:P, U:QQ. Case 2: Cooperation of Peasants (turn 33). The LandlordUp plays a T. DouZero chooses PASS because there is no card out there larger than T in rank. This suggests that DouZero has learned to reason about the cards that have not been played. Full game: H:334444556689TTJJJQ2R; 3577889TQQKKKKAA2; 356677899TJQAA22B, L:33444455, D:7788KKKK;, U:P, L:P, D:3, U:J, L:2, D:P, U:B, L:R, D:P, U:P, L:89TJQ, D:P\u00a1 U:P, L:66, D:QQ, U:P, L:P, D:5, U:2, L:P, D:P, U:99, L:JJ, D:AA, D:P, L:P, D:2, U:P, L:P, D:T, U:P, L:P, D:9. PASS Top -3 M oves Lan dl or dDow n (Dou Zer o) Lan dl or d Lan dl or dUp (Dou Zer o) Case 3: Cooperation of Peasants (turn 36). The LandlordDown plays a Trio with Solo. While LandlordUp has a very good hand and can win the game by itself, DouZero chooses PASS to let her teammate win. Full game: H:3344566788999JQK222; 34577788TJJQQQKAA; 3455669TTTKKAA2BR; L:45678, D:P, U:P, L:4999, D:5QQQ, U:P, L:6222, D:P, U:BR, L:P, D:P, U:55, L:JJ, D:P, U:KK, L:P, D:P, U:9, L:Q, D:K, U:A, L:P, D:P, U:3, L:8, D:A, U:P, L:P, D:88, U:P, L:P, D:A, U:P, L:P, D:4777, U:P, L:P, D:3, U:T, L:K, D:P, U:A, L:P, D:P, U:T, L:P, D:P, U:T, L:P, D:J, U:P, L:P, D:J, U:P, L:P, D:T. F.3. Comparison of the Models in Early Stage and Later Stage Table 15. Case 1: Comparison of DouZero in early stage and later stage. DouZero plays the Landlord position on the same deck with the same opponents. DouZero loses in the early stage but wins the game in the later stage. DouZero in the later stage tends to perform a better planning of the hand. Specifically, DouZero in the later stage tends to first play small pairs, such as 88 and TT, so that it can easily empty the hand later. 455557777889TTKKAA22; 3446668999QQKA2BR; 333468TTJJJJQQKA2, L:TT, D:QQ, U:P, L:KK, D:P, U:P, L:88, D:99, U:TT, L:AA, D:P, U:P, L:477779, D:P, U:46JJJJ, L:P, D:P, U:3338, L:5555, D:BR, U:P, L:P, D:3666, U:P, L:P, D:8, U:K, L:2, D:P, U:P, L:2\n\n4 Table 18 .Figure 29 .Figure 30\n41829307777, U:P, L:P, D:TJQKA, U:P, L:5555, D:P, U:P, L:6, D:8, U:P, L:J, D:2, U:P, L:R, D:P, U:P, L:88, D:P, U:JJ, L:AA, D:P, U:P, L:33, D:P, U:66, L:TT, D:P, U:KK, L:P, D:P, U:8999, L:P, D:P, U:QQQK, 33455556889TTJAA22BR; 334677778TTJQKA22; 446688999JJQQQKKKA, L:33, D:22, U:P, L:BR, D:P, U:P, L:6, D:8, U:A, L:2, D:7777, U:P, L:P, D:TJQKA, U:P, L:5555, D:P, U:P, L:9, D:T, U:P, L:J, D:P, U:K, L:2, D:P, U:P, L:88, D:P, U:JJ, L:P, D:P, U:44999, L:P, D:P, U:66QQQ, L:P, D:P, U:KK, L:AA, D:P, U:P, L:TT, D:P, U:P, L:Case 4: Comparison of DouZero in early stage and later stage. DouZero plays the LandlordUp and LandlordDown positions on the same deck with the same opponent. While DouZero wins both games. DouZero in the later stage looks more reasonable. In turn 16, DouZero in the early stage chooses to break a pair of 2, which is hard to be explained. DouZero in the later stage tends to play more smoothly and wins the games quickly. Logs Early stage H:3455567788TJJQKKKKA2; 33446678899JQQQA2; 3456799TTTJAA22BR, L:345678, D:P, U:P, L:TJQKA, D:P, U:P, L:55KKK, D:P, U:P, L:7, D:P, U:R, L:P, D:P, U:2, L:P, D:P, U:34567, L:P, D:P, U:J, L:2, D:P, U:B, L:P, D:P, U:2, L:P, D:P, U:AA, L:P, D:P, U:99TTT Later stage H:3455567788TJJQKKKKA2; 33446678899JQQQA2; 3456799TTTJAA22BR, L:345678, D:P, U:P, L:TJQKA, D:P, U:P, L:55KKK, D:P, U:BR, L:P, D:P, U:99TTT, L:P, D:P, U:34567, L:P, D:P, U:AA, L:P, D:P, U:22, L:P, D:P, Case 1: Comparison of using WP and ADP as objectives. The two agents play the same deck twice. It is difficult for the Landlord to win this game since the LandlordUp has a very good hand. When LandlordUp plays a Plane with Solo, WP agent tends to ambitiously play bombs because playing bombs has no cost. However, ADP agent tends to be very cautious of playing bombs since it may lead to a larger loss of ADP. Full game of (a): H:3344445666689JQQKK22; 3556688TJJJQQKKA2; 35668999TTTAAA2BR, L:33, D:55, U:66, L:QQ, D:KK, U:P, L:22, D:P, U:BR, L:P, D:P, U:58999TTT, L:7777, D:P, U:P, L:8, D:T, U:2, L:P, D:P, U:3AAA. Full game of (b): H:3344445666689JQQKK22; 3556688TJJJQQKKA2; 35668999TTTAAA2BR, L:33, D:55, U:66, L:KK, D:P, U:AA, L:P, D:P, U:35999TTT, L:P, D:P, U:8, L:J, D:A, U:2, L:P, D:P, U:BR, L:P, D:P, U:A.\n\nFigure 31 .Figure 32 .Figure 33 .\n313233Full game of (a): H:455557777889TTKKAA22; 3446668999QQKA2BR; 333468TTJJJJQQKA2, L:TT, D:QQ, U:P, L:KK, D:P, U:P, L:88, D:99, U:TT, L:AA, D:P, U:P, L:477779, D:P, U:46JJJJ, L:P, D:P, U:3338, L:5555, D:BR, U:P, L:P, D:3666, U:P, L:P, D:8, U:K, L:2, D:P, U:P, L:2. Full game of (b): H:455557777889TTKKAA22; 3446668999QQKA2BR; 333468TTJJJJQQKA2, L:88, D:QQ, U:P, L:KK, D:P, U:P, L:TT, D:P, U:QQ, L:AA, D:P, U:P, L:9, D:K, U:P, L:2, D:B, U:P, L:P, D:3666, U:4JJJ, L:7777, D:P, U:P, Case 3: Comparison of using WP and ADP as objectives. The two agents play the same deck twice. In this game, the Landlord is very likely to win. The WP agent chooses a more conservative move by playing Quad with Pair to quickly empty the hand. In contrast, the ADP agent plays a small Solo first so that it can play 5555 later to double the ADP. Full game of (a): H:3355556778889JJQKABR; 344446679JJQQKA22; 367899TTTTQKKAA22, L:33, D:66, U:KK, L:P, D:P, U:6789T, L:P, D:P, U:3TTT, L:P, D:P, U:9, L:Q, D:K, U:P, L:A, D:2, U:P, L:P, D:3, U:Q, L:K, D:A, U:P, L:B, D:P, U:P, L:6, D:7, U:A, L:R, D:P, U:P, L:555577JJ, D:P, U:P, L:8889. Full game of (b): H:3355556778889JJQKABR; 344446679JJQQKA22; 367899TTTTQKKAA22, L:33, D:66, U:KK, L:P, D:P, U:6789T, L:P, D:P, U:3TTT, L:P, D:P, U:9, L:Q, D:K, U:P, L:A, D:2, U:P, L:P, D:3, U:Q, L:K, D:A, U:P, L:P, D:4, U:A, L:P, D:P, U:A, L:R, D:P, U:P, L:9, D:2, U:P, L:B, D:P, U:P, L:77888, D:P, U:P, L:6, D:7, U:2, L:5555, D:P, U:P, L:JJ.F.5. Bad CasesPASS Top -3 M oves Lan dl or d Dow n (Dou Zer o) Lan dl or d (Dou Zer o) Lan dl or d Up (Dou Zer o) Case 1: Bad case (turn 21). The Landlord plays a 2 with only 3 cards left. While the LandlordUp has Black Joker in hand, DouZero chooses not to play it. Although the Peasants will lose whatever the LandlordUp plays in this specific case, playing the Black Joker should have a larger chance to win (if the Red Joker happens to be in the hand of the LandlordDown). Thus, it is worth a try to play Black Joker. Full game: H: 3556788TQQQKKKAAA22R; 344455677999TTQK2; 334667889TJJJJA2B; L:55, D:P, U:P, L:88, D:P, U:P, L:3AAA, D:P, U:P, L:7KKK, D:P, U:P, L:6QQQ, D:P, U:JJJJ, L:P, D:P, Case 2: Bad case (turn 22). This could be a bad case. DouZero aggressively chooses Black Joker as the kicker instead of K. While it is true that DouZero can win whichever kicker it chooses, choosing Black Joker is risky because this knowledge could be generalized to other cases with neural networks and results in losing a game. In fact, choosing K as the kicker will always be at least as good as Black Joker given the current hand. Full game: H:333578889TJQKKAAA22B; 444455779JJJQKA2R; 3566667899TTTQQK2, L:3335, D:9JJJ, U:P, L:7AAA, D:4444, U:P, L:P, D:Q, U:K, L:P, D:P, U:3TTT, L:P, D:P, U:56789, L:9TJQK, D:P, U:P, L:22, D:P, U:P, L:888B, D:P, U:P, L:K.\n\n\nDMC is greatly outweighed by the scalability it provides, and DMC is very efficient in wall-clock time.Third, it is inconvenient to efficiently implement DQN in \nDouDizhu due to the large and variable action space. Specif-\nically, the max operation of DQN in every update step will \ncause high computation cost since it requires iterating across \nall the legal actions on a very costly deep Q-network. More-\nover, the legal moves differ in different states, which makes \nit inconvenient to do batch learning. As a result, we find \nDQN is too slow in terms of wall-clock time. While Monte-\nCarlo methods might suffer from high variance (Sutton & \nBarto, 2018), which means it might require more samples to \nconverge, it can be easily parallelized to generate thousands \nof samples per second to alleviate the high variance issue \nand accelerate training. We find that the high variance of \n\n\n\n1 :\n1Input: Shared buffers B L , B U and B D with B entries and size S for each entry, batch size M , learning rate \u03c82: Initialize global Q-networks Q g \nL , Q g \nU and Q g \n\nD \n\n3: for iteration = 1, 2, ... until convergence do \n\n4: \n\nfor p \u2208 {L, U, D} do Optimized by multi-thread \n\n5: \n\nif the number of full entries in B p \u2265 M then \n\n6: \n\nSample a batch of {s t , a t , r t } with M \u00d7 S in-\nstances from B p and free the entries \n\n7: \n\nUpdate Q g \np with MSE loss and learning rate \u03c8 \n\n8: \n\nend if \n\n9: \n\nend for \n10: end for \n\n\u2022 DeltaDou: A strong AI program which uses Bayesian \nmethods to infer hidden information and searches the \nmoves with MCTS (Jiang et al., 2019). We use the code \nand the pre-trained model provided by the authors. The \nmodel is trained for two months and is shown to have \non par performance with top human players. \n\n\u2022 CQN: Combinational Q-Learning (You et al., 2019) \nis a program based on card decomposition and Deep \nQ-Learning. We use the open-sourced code and the \npre-trained model provided by the authors 3 . \n\n\n\nTable 1 .\n1Performance of DouZero against existing DouDizhu programs by playing 10,000 randomly sampled decks. Algorithm A outperforms B if WP is larger than 0.5 or ADP is larger than 0 (highlighted in boldface). The algorithms are ranked according to the number of the other algorithms that they beat. The full results of each position are provided in Appendix D.1. ADP WP ADP WP ADP WP ADP WP ADP WP ADP WP ADP WP ADP. \n\n\nTable 1\n1summarizes the WP and ADP of head-to-head com-\npletions among DouZero and all the baselines. We make \nthree observationss. First, DouZero dominates all the \nrule-based strategies and supervised learning, which demon-\nstrates the effectiveness of adopting reinforcement learning \nin DouDizhu. Second, DouZero achieves significantly bet-\nter performance than CQN. Recall that CQN similarly trains \nthe Q-networks with action decomposition and DQN. The \nsuperiority of DouZero suggests that DMC is indeed an \neffective way to train the Q-networks in DouDizhu. Third, \nDouZero outperforms DeltaDou, the strongest DouDizhu \n\n\nTable 2 .\n2Comparison of DouZero, DeltaDou and SL with the bidding phase by playing 100,000 randomly sampled decks. AI in the literature. We note that DouDizhu has very high variance, i.e., to win a game relies on the strength of the initial hand card, which is highly dependent on luck. Thus, a WP of 0.586 and an ADP of 0.258 suggest a significant improvement over DeltaDou. Moreover, DeltaDou requires searching at both training and testing time. Whereas, DouZero does not do the searching, which verifies that the Q-networks learned by DouZero are very strong.DouZero DeltaDou \nSL \n\nWP \n0.580 \n0.461 \n0.381 \nADP \n0.323 \n-0.004 \n-0.320 \n\n\n\n\nFigure 10. A case study dumped from Botzone, where the three players play cards in counter-clockwise order. The Peasant agent learns to play small Solo to cooperate with the other Peasant to win the game. Note that the other players' hands are showed face up solely for better visualization but are hidden in the real game. More case studies are provided in Appendix F.Figure 10shows a typical case when the two Peasants can cooperate to beat the Landlord. The Peasant on the righthand side only has one card left. Here, the Peasant at the bottom can play a small Solo to help the other Peasant win. When looking into the top three actions predicted by DouZero, we make two interesting observations. First, we find that all the top actions outputted by DouZero are small Solos with high confidence to win, suggesting that the two Peasants of DouZero may have learned to cooperate.Top -3 M oves \n\nPASS \n\nPASS \n\nLan dl or d \n\nPeasan t (Dou Zer o) \n\nPeasan t (Dou Zer o) \n\n0.971 \n0.808 \n0.784 \n\n\n\n\nCounterfactual Regret Minimization (CFR)(Zinkevich et al., 2008) is a leading iterative algorithm for poker games, with many variants(Lanctot et al., 2009; Gibson et al., 2012; Bowling et al.,  2015; Morav\u010d\u00edk et al., 2017; Brown & Sandholm, 2018;  2019a; Brown et al., 2019; Lanctot et al., 2019; Li et al.,  2020b). However, traversing the game tree of DouDizhu is computationally intensive since it has a huge tree with a large branching factor. Moreover, most of the prior studies focus on zero-sum settings. While some efforts have been devoted to addressing the cooperative settings, e.g., with blueprint policy (Lerer et al., 2020), it remains challenging to reason about both competing and cooperation. Thus, DouDizhu has not seen an effective CFR-like solution.RL for Imperfect-Information Games. Recent studies show that Reinforcement Learning (RL) can achieve competitive performance in poker games(Heinrich et al., 2015;  Heinrich & Silver, 2016; Lanctot et al., 2017). Unlike CFR, RL is based on sampling so that it can easily generalize to large-scale games. RL has been successfully applied in some complex imperfect-information games, such asStarcraft (Vinyals et al., 2019),DOTA (Berner et al., 2019)  andMahjong (Li et al., 2020a). More recently, RL+search is explored and shown to be effective in poker games(Brown  et al., 2020). DeltaDou adopts a similar idea, which first infers the hidden information and then uses MCTS to combine RL with search inDouDizhu (Jiang et al., 2019). However, DeltaDou is computationally expensive and heavily relies on human expertise. In practice, even without search, our DouZero outperforms DeltaDou in days of training.\n\n\nPeasants (Panait & Luke, 2005;  Foerster et al., 2016; Raileanu et al., 2018; Lai et al., 2020). Sixth, we plan to try scalable frameworks, such as SEED RL (Espeholt et al., 2019). Last but not least, we will test the applicability of Monte-Carlo methods on other tasks. Beleznay, F., Grobler, T., and Szepesvari, C. Comparing value-function estimation algorithms in undiscounted problems. Technical Report TR-99-02, MindMaker Ltd, 1999.Berner, C., Brockman, G., Chan, B., Cheung, V., Dkebiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.References \n\nBowling, M., Burch, N., Johanson, M., and Tammelin, O. \nHeads-up limit hold'em poker is solved. Science, 347 \n(6218):145-149, 2015. \n\nBrown, N. and Sandholm, T. Superhuman ai for heads-up \nno-limit poker: Libratus beats top professionals. Science, \n359(6374):418-424, 2018. \n\nBrown, N. and Sandholm, T. Solving imperfect-information \ngames via discounted regret minimization. In AAAI Con-\nference on Artificial Intelligence, 2019a. \n\nBrown, N. and Sandholm, T. Superhuman ai for multiplayer \npoker. Science, 365(6456):885-890, 2019b. \n\nBrown, N., Lerer, A., Gross, S., and Sandholm, T. Deep \ncounterfactual regret minimization. In International Con-\nference on Machine Learning, 2019. \n\nBrown, N., Bakhtin, A., Lerer, A., and Gong, Q. Combining \ndeep reinforcement learning and search for imperfect-\ninformation games. arXiv preprint arXiv:2007.13544, \n2020. \n\nDulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P., \nLillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., and \nCoppin, B. Deep reinforcement learning in large discrete \naction spaces. arXiv preprint arXiv:1512.07679, 2015. \n\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, \nV., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, \nI., et al. Impala: Scalable distributed deep-rl with im-\nportance weighted actor-learner architectures. In Interna-\ntional Conference on Machine Learning, 2018. \n\nEspeholt, L., Marinier, R., Stanczyk, P., Wang, K., and \nMichalski, M. Seed rl: Scalable and efficient deep-rl with \naccelerated central inference. In International Conference \non Learning Representations, 2019. \n\nFedus, W., Ramachandran, P., Agarwal, R., Bengio, Y., \nLarochelle, H., Rowland, M., and Dabney, W. Revisit-\ning fundamentals of experience replay. In International \nConference on Machine Learning, 2020. \n\nFoerster, J. N., Assael, Y. M., De Freitas, N., and Whiteson, \nS. Learning to communicate with deep multi-agent re-\ninforcement learning. arXiv preprint arXiv:1605.06676, \n2016. \n\nGibson, R., Lanctot, M., Burch, N., Szafron, D., and Bowl-\ning, M. Generalized sampling and variance in counterfac-\ntual regret minimization. In AAAI Conference on Artificial \nIntelligence, 2012. \n\n\n\n\nZha, D., Ma, W., Yuan, L., Hu, X., and Liu, J. Rank the episodes: A simple approach for exploration in procedurally-generated environments. In International Conference on Learning Representations, 2021b.Ye, D., Liu, Z., Sun, M., Shi, B., Zhao, P., Wu, H., Yu, H., \nYang, S., Wu, X., Guo, Q., et al. Mastering complex \ncontrol in moba games with deep reinforcement learning. \nIn AAAI Conference on Artificial Intelligence, 2020b. \n\nYou, Y., Li, L., Guo, B., Wang, W., and Lu, C. Com-\nbinational q-learning for dou di zhu. arXiv preprint \narXiv:1901.08925, 2019. \n\nZahavy, T., Haroush, M., Merlis, N., Mankowitz, D. J., and \nMannor, S. Learn what not to learn: Action elimination \nwith deep reinforcement learning. In Advances in Neural \nInformation Processing Systems, 2018. \n\nZha, D., Lai, K.-H., Cao, Y., Huang, S., Wei, R., Guo, J., \nand Hu, X. Rlcard: A toolkit for reinforcement learning \nin card games. arXiv preprint arXiv:1910.04376, 2019a. \n\nZha, D., Lai, K.-H., Zhou, K., and Hu, X. Experience \nreplay optimization. In International Joint Conference \non Artificial Intelligence, 2019b. \n\nZha, D., Lai, K.-H., Zhou, K., and Hu, X. Simplifying \ndeep reinforcement learning via self-supervision. arXiv \npreprint arXiv:2106.05526, 2021a. \n\nZhang, S. and Sutton, R. S. A deeper look at experience \nreplay. NIPS Deep Reinforcement Learning Symposium, \n2017. \n\nZhou, H., Zhang, H., Zhou, Y., Wang, X., and Li, W. Bot-\nzone: an online multi-agent competitive platform for \nai education. In Proceedings of the 23rd Annual ACM \nConference on Innovation and Technology in Computer \nScience Education, pp. 33-38, 2018. \n\nZinkevich, M., Johanson, M., Bowling, M., and Piccione, \nC. Regret minimization in games with incomplete infor-\nmation. In Advances in Neural Information Processing \nSystems, 2008. \n\n\nTable 3 .\n3Summary of the action space of DouDizhu. We follow the summary provided inRLCard (Zha et al., 2019a).Action Type \nNumber of Actions \n\nSolo \n15 \nPair \n13 \nTrio \n13 \nTrio with Solo \n182 \nTrio with Pair \n156 \nChain of Solo \n36 \nChain of Pair \n52 \nChain of Trio \n45 \nPlane with Solo 21, 822 \nPlane with Pair 2, 939 \nQuad with Solo 1, 326 \nQuad with Pair \n858 \nBomb \n13 \nRocket \n1 \nPass \n1 \n\nTotal \n27, 472 \n\n\nTable 4 .Table 5 .\n45Features of the Landlord. Card matrix of the union of the other two players' hand cards 54 Card matrix of the most recent move 54 Card matrix of the the played cards of the first Peasant 54 Card matrix of the the played cards of the second Peasant 54 One-hot vector representing the number cards left of the first Peasant 17 One-hot vector representing the number cards left of the second Peasant 17 One-hot vector representing the number bombs in the current state 15 Concatenated matrix of the most recent 15 moves 5 \u00d7 162 Features of the Peasants. Card matrix of the union of the other two players' hand cards 54 Card matrix of the most recent move 54 Card matrix of the most recent move performed by the Landlord 54 Card matrix of the most recent move performed by the other Peasant 54 Card matrix of the the played cards of the Landlord 54 Card matrix of the the played cards of the other Peasant 54 One-hot vector representing the number cards left of the Landlord 20 One-hot vector representing the number cards left of the other Peasant 17 One-hot vector representing the number bombs in the current state 15 Concatenated matrix of the most recent 15 moves 5 \u00d7 162 C.2. Data Collection and Neural Architecture of Supervised LearningFeature \n\n\nTable 6 .\n6Features of the bidding network.\n\n\nTable 8. ADP of DouZero and the baselines. L: ADP of A as Landlord; P: ADP of A as Peasants. If the average ADP of L and P is higher than 0, we conclude that A outperforms B and highlight both L and P in boldface. The algorithms are ranked according to the number of the other algorithms that they beat..A \nB DouZero \nDeltaDou \nSL \nRHCP-v2 \nRHCP \nRLCard \nCQN \nRandom \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \n\n1 \nDouZero .4159 .5841 .4870 .6843 .5692 .7494 .6844 .8303 .7253 .8033 .8695 .9089 .7686 .8513 .9858 .9920 \n2 \nDeltaDou .3166 .5130 .4120 .5880 .5130 .7211 .6701 .8165 .7048 .7899 .8563 .8955 .7326 .8351 .9871 .9960 \n3 \nSL \n.2506 .4308 .2789 .5130 .4072 .5928 .5370 .6857 .5831 .6810 .7605 .8650 .6450 .7428 .9599 .9927 \n4 \nRHCP-v2 .1697 .3156 .1835 .3299 .3143 .4630 .4595 .5405 .5134 .5165 .6813 .7018 .6313 .6116 .9519 .9821 \n5 \nRHCP \n.1967 .2747 .2101 .2952 .3190 .4179 .4835 .4866 .4971 .5029 .6718 .6913 .6416 .5640 .9092 .9725 \n6 \nRLCard \n.0911 .1305 .1045 .1437 .1350 .2395 .2982 .3187 .3087 .3282 .4465 .5535 .5839 .4603 .9314 .9539 \n7 \nCQN \n.1487 .2314 .1649 .2674 .2572 .3550 .3884 .3687 .4360 .3584 .5397 .4161 .5238 .4762 .8566 .9213 \n8 \nRandom \n.0080 .0142 .0040 .0129 .0073 .0401 .0179 .0481 .0025 .0908 .0461 .0686 .0787 .1434 .3461 .6539 \n\nRank A \nB DouZero \nDeltaDou \nSL \nRHCP-v2 \nRHCP \nRLCard \nCQN \nRandom \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \n\n1 DouZero -0.435 0.435 -0.342 0.858 0.287 1.112 1.436 1.888 1.492 1.850 2.222 2.354 1.368 2.001 3.254 2.818 \n2 DeltaDou 0.342 -0.858 -0.476 0.476 0.268 1.038 1.297 1.703 1.312 1.715 2.270 2.648 1.218 1.849 3.268 2.930 \n3 SL \n-1.112 -0.287 -1.038 -0.268 -0.364 0.364 0.564 1.142 0.658 1.114 1.652 1.990 0.878 1.196 3.026 2.415 \n4 RHCP-v2 -1.888 -1.436 -1.703 -1.297 -1.142 -0.564 -0.209 0.209 0.074 0.029 1.011 1.230 0.750 0.677 2.638 2.624 \n5 RHCP \n-1.850 -1.492 -1.715 -1.312 -1.114 -0.658 -0.029 -0.074 -0.007 0.007 1.190 1.328 0.927 -0.432 2.722 2.717 \n6 RLCard \n-2.354 -2.222 -2.648 -2.270 -1.990 -1.652 -1.230 -1.011 -1.328 -1.190 -0.266 0.266 0.474 -0.138 2.630 2.312 \n7 CQN \n-2.001 -1.368 -1.849 -1.218 -1.196 -0.878 -0.677 -0.750 0.432 -0.927 0.138 -0.474 0.056 -0.056 1.832 1.992 \n8 Random \n-2.818 -3.254 -2.930 -3.268 -2.415 -3.026 -2.624 -2.638 -2.717 -2.722 -2.312 -2.629 -1.991 -1.832 -0.883 0.883 \n\n\n\nTable 9 .\n9WP of DouZero against baselines when using ADP as the reward. L: WP of A as Landlord; P: WP of A as Peasants. If the average WP of L and P is higher than 0.5, we conclude that A outperforms B and highlight both L and P in boldface..A \n\nB \nDouZero (ADP) DeltaDou \nSL \nRHCP-v2 \nRHCP \nRLCard \nCQN \nRandom \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \n\nDouZero (ADP) .4281 \n.5719 \n.4177 .6319 .5039 .6815 .6615 .7543 .6950 .7628 .8416 .8668 .7198 .8280 .9801 .9895 \n\n\n\n\nTable 12. DouZero against DeltaDou and SL when using WP as reward with bidding network.WP) -0.411 0.411 -0.360 0.664 0.224 1.001 1.252 1.880 1.378 1.794 2.094 2.298 1.418 1.872 2.947 2.518 \n\nTable 11. DouZero against DeltaDou and SL when using ADP as reward with bidding network. \n\nDouZero (ADP) DeltaDou \nSL \n\nWP \n0.535 \n0.477 \n0.407 \nADP \n0.323 \n-0.004 \n-0.320 \n\nDouZero (WP) DeltaDou \nSL \n\nWP \n0.580 \n0.461 \n0.381 \nADP \n0.315 \n0.075 \n-0.390 \n\n\n\nTable 14 .\n14Summary of weights assigned to each category in Botzone.Action Type \nWeight \n\nSolo \n1 \nPair \n2 \nTrio (or with Solo / Pair) \n4 \nChain of Solo \n6 \nChain of Pair \n6 \nChain of Trio \n8 \nPlane with Solo / Pair \n8 \nQuad with Solo / Pair \n8 \nSpace shuttle A (2 consecutive Quad) \n10 \nBomb \n10 \nRocket \n16 \nSpace shuttle B (more than 3 consecutive Quads) 20 \nPass \n0 \n\n\n\nTable 16 .\n16Case 2: Comparison of DouZero in early stage and later stage. DouZero plays the Landlord position on the same deck with the same opponents. In the early stage, DouZero keeps playing PASS because it does not want to break the Rocket. As result, DouZero loses the game. In the later stage, DouZero smartly breaks the Rocket to dominate other Solos, and eventually wins the game.\nDepartment of Computer Science and Engineering, Texas A&M University 2 AI Platform, Kwai Inc. 3 Georgia Institute of Technology. Correspondence to: Daochen Zha <daochen.zha@tamu.edu>.\nhttps://en.wikipedia.org/wiki/Dou_dizhu\nhttps://github.com/qq456cvb/doudizhu-C 4 https://blog.csdn.net/sm9sun/article/ details/70787814 5 https://github.com/deecamp2019-group20/ RuleBasedModelV2 6 https://github.com/datamllab/rlcard\nhttps://github.com/hsywhu\nhttps://wiki.botzone.org.cn/index.php?title=%E9%A6%96%E9%A1%B5/en 11 https://wiki.botzone.org.cn/index.php?title=FightTheLandlord\nAcknowledgementsWe thank our colleagues in Kuai Inc. for building the DouDizhu environment and the helpful discussions, Qiqi Jiang from DeltaDou team for helping us set up DeltaDou models, and Songyi Huang 8 from RLCard team for developing demo. We would also like to thank the anonymous reviewers and the meta-reviewer for the insightful feedback.\nU:Qq , L Kk, D:P , U Aa, L :P, D:P, U:9, L:T, D:A, U:P, L:2, D:B, U:P, L:P, D:9, U:J, L:A, D:2, U:P, L:R, D:P, U:P, L:5559, D:777j, U:P, L:P, D:J, U:K, L:P, D:P, U ; 345668899tttjqkaa; 33456679tjjqkka2r, L :44, D:P, U:Jj, L Qq, D Aa, U:P, L:P, D:3, U:T, L:J, D:P, U:Q, L:K, D:P, U:A, L:P, D:P, U ; Q, U:P, L:2, D:P, U:R, L:P, D:P, U:34567, L:P, D:P, U:Kk, L:22, D:P, U:P, L:77788, D:66ttt, U:P, L:P, D:J, U:P, L:P, D:9, U:P, L:P, D:9, U:P, L:P, D:8, U:P, L:P, D:5, U:P, L:P, D:K, U:P, L:P, D:8 H:34455666789ttjqqka2r; 335688899tjqa222b; 34457779tjjqkkkaa, L :445566, D:P, U:P, L:6789t, D:89tjq , U:9tjqk , L Tjqka, D :P, U:P, L:Q, D:A, U:P, L:2, D:B, U:P, L:R, D:P, U:P, L:3 H:334447779tttjqkkk22r; 3345566678tjqka2b; 556888999jjqqaaa2, U:888jj , L:Ttt22 , D :P, U:55aaa, L:P, D:P, U:Qq, L:P, D:P, U:6999, L Qkkk, D :P, U:P, L:R, D:P, U:P, L:777j, D:P, U:P, L ; L:J , D :P, U:K, L:P, D:P, U:K, L:P, D:P, U ; , U:B, L:P, D:P, U:J, L:Q, D:K, U:P, L:A, D:P, U:P, Table 19. Case 1: Randomly selected (rather than cherry-picked) full self-play games of DouZero. DouZero plays the Landlord, LandlordUP and LandlordDown positions. Q, L:2, D:P, U:P, L:469999, D:P, U:P, L:7KKK, D:P, U:P, L:B H:3455667899TTTJJQQKAB33567788JJQQKA22R3444567899TKKAA22, L:6, D:K, U:A, L:B, D:P, U:P, L:34567, D:P, U:56789, L:89TJQ, D:P, U:P, L:5, D:6, U:A, L:P, D:P, U:T, L:A, D:2, U:P, L:P, D:5, U:9U:K, L:A, D:2, U:P, L6All DouZero agents were trained using WP as objectives. Logs H:335556788899TTJKKA2R; 6677789TTJJQQKA2B; 334444569JQQKAA22, L:33, D:66,. 455577899TTJJKKAA22B; 3346789TJJQQAA22R; 334456667889TQQKK, L:4, D:P. L:P, D:89TJQ, U:P, L:P, D:44, U:P, L:88, D:AA, U:P, L:22, D:7777, U:P, L:P, D:5 H:334679999TTQQKKKAA2B; 3455677888JQKA22R; 344556678TTJJJQA2, L:33, D:55, U:TT, L:QQ, D:P, U:P, L:TT, D:P, U:JJ, L:AA, D:22, U:P, L:P, D:4. L:P, D:P, U:3444 H:345578TTJJJQKKKA2222; 3334566667789QQKR; 445788999TTJQAAAB, L:3, D:8, U:Q, L:2, D:P. L:4, D:9, U:T, L:P, D:P, U:88, L:TT, D:QQ, U:P, L:P, D:4, U:9, L:2, D:6666, U:P, L:P, D:5, U:9, L:K, D:R, U:P, L:P, D:33377 H:3334577889TTJQKAAA2B; 345566789TJQQKKA2; 445667899TJJQK22R, L:5, D:6, U:K, L:P, D:A, U:P, L:2, D:P, U:P, L:T, D:K, U:P, L:B, D:P, U:R, L:P, D:P, U:456789TJQ, L:P, D:P, U:4, L:K, D:2, U:P, L:P, D:3, U:9, L:A, D:P, U:2, L:P, D:P, U:J, L:A, D:P, U:2, L:P, D:PTable 19. Case 1: Randomly selected (rather than cherry-picked) full self-play games of DouZero. DouZero plays the Landlord, LandlordUP and LandlordDown positions. All DouZero agents were trained using WP as objectives. Logs H:335556788899TTJKKA2R; 6677789TTJJQQKA2B; 334444569JQQKAA22, L:33, D:66, U:QQ, L:KK, D:P, U:AA, L:P, D:P, U:9, L:T, D:A, U:P, L:2, D:B, U:P, L:P, D:9, U:J, L:A, D:2, U:P, L:R, D:P, U:P, L:5559, D:777J, U:P, L:P, D:J, U:K, L:P, D:P, U:5, L:6, D:8, U:4444, L:P, D:P, U:33, L:88, D:TT, U:22, L:P, D:P, U:6 H:34455777889JQQKA222B; 345668899TTTJQKAA; 33456679TJJQKKA2R, L:44, D:P, U:JJ, L:QQ, D:AA, U:P, L:P, D:3, U:T, L:J, D:P, U:Q, L:K, D:P, U:A, L:P, D:P, U:9, L:A, D:P, U:2, L:B, D:P, U:P, L:3, D:4, U:6, L:9, D:Q, U:P, L:2, D:P, U:R, L:P, D:P, U:34567, L:P, D:P, U:KK, L:22, D:P, U:P, L:77788, D:66TTT, U:P, L:P, D:J, U:P, L:P, D:9, U:P, L:P, D:9, U:P, L:P, D:8, U:P, L:P, D:5, U:P, L:P, D:K, U:P, L:P, D:8 H:34455666789TTJQQKA2R; 335688899TJQA222B; 34457779TJJQKKKAA, L:445566, D:P, U:P, L:6789T, D:89TJQ, U:9TJQK, L:TJQKA, D:P, U:P, L:Q, D:A, U:P, L:2, D:B, U:P, L:R, D:P, U:P, L:3 H:334447779TTTJQKKK22R; 3345566678TJQKA2B; 556888999JJQQAAA2, L:33444, D:55666, U:888JJ, L:TTT22, D:P, U:55AAA, L:P, D:P, U:QQ, L:P, D:P, U:6999, L:QKKK, D:P, U:P, L:R, D:P, U:P, L:777J, D:P, U:P, L:9 H:455577899TTJJKKAA22B; 3346789TJJQQAA22R; 334456667889TQQKK, L:4, D:P, U:K, L:A, D:2, U:P, L:P, D:6789TJ, U:P, L:P, D:4, U:Q, L:A, D:2, U:P, L:B, D:R, U:P, L:P, D:33, U:88, L:TT, D:QQ, U:P, L:KK, D:AA, U:P, L:22, D:P, U:P, L:JJ, D:P, U:P, L:77, D:P, U:P, L:99, D:P, U:P, L:5558 H:3345666889JQQQKAA22B; 4457777899TJQAA2R; 33455689TTTJJKKK2, L:5, D:9, U:J, L:K, D:2, U:P, L:B, D:R, U:P, L:P, D:89TJQ, U:P, L:P, D:44, U:P, L:88, D:AA, U:P, L:22, D:7777, U:P, L:P, D:5 H:334679999TTQQKKKAA2B; 3455677888JQKA22R; 344556678TTJJJQA2, L:33, D:55, U:TT, L:QQ, D:P, U:P, L:TT, D:P, U:JJ, L:AA, D:22, U:P, L:P, D:4, U:Q, L:2, D:P, U:P, L:469999, D:P, U:P, L:7KKK, D:P, U:P, L:B H:3455667899TTTJJQQKAB; 33567788JJQQKA22R; 3444567899TKKAA22, L:6, D:K, U:A, L:B, D:P, U:P, L:34567, D:P, U:56789, L:89TJQ, D:P, U:P, L:5, D:6, U:A, L:P, D:P, U:T, L:A, D:2, U:P, L:P, D:5, U:9, L:J, D:P, U:K, L:P, D:P, U:K, L:P, D:P, U:22, L:P, D:P, U:3444 H:345578TTJJJQKKKA2222; 3334566667789QQKR; 445788999TTJQAAAB, L:3, D:8, U:Q, L:2, D:P, U:B, L:P, D:P, U:J, L:Q, D:K, U:P, L:A, D:P, U:P, L:4, D:9, U:T, L:P, D:P, U:88, L:TT, D:QQ, U:P, L:P, D:4, U:9, L:2, D:6666, U:P, L:P, D:5, U:9, L:K, D:R, U:P, L:P, D:33377 H:3334577889TTJQKAAA2B; 345566789TJQQKKA2; 445667899TJJQK22R, L:5, D:6, U:K, L:P, D:A, U:P, L:2, D:P, U:P, L:T, D:K, U:P, L:B, D:P, U:R, L:P, D:P, U:456789TJQ, L:P, D:P, U:4, L:K, D:2, U:P, L:P, D:3, U:9, L:A, D:P, U:2, L:P, D:P, U:J, L:A, D:P, U:2, L:P, D:P, U:6\n\n. L:48999ttt 344566689jjjqqa22, D Br, U:P, L:P, D:34567, U:P, L:P, D:Tjqka, U:P, L:P, D:88, U:Qq, L Kk, D :P, U:22, L:P, D:P, U:3666, L:P, D:P, U:5jjj, L:P, D:P, U:44, L Aa, D :P, U:P, L:2, D:P, U:P, L:77, D:P, U:P, L:33, D:P, U:P, L:55, D:P, U:P, L:Q Peasants, H:34456779ttjkkkaaaa22; 34455668888jqqq2r; 335677999ttjjqk2b , L :34567, D:P, U:9tjqk, L:P, D:P, U:T, L:J, D:Q, U:P, L:A, D:2, U:P, L:P, D:445566, U:P, L:P, D:38888q, U:P, L:P, D:Q, U:P, L:P, D:R, U:P, L:P, D:J Landlord, H:334455668899tjqqaa2b; 34567777tjjkkkaa2; 34568899ttjqqk22r , L :33445566, D:P, U:P, L:9, D:T, U:P, L:Q, D:A, U:P, L:P, D:777jj, U:P, L:P, D:34567, U:P, L:89tjq, D:P, U:9tjqk, L:P , D:P , U , P, D:P, U:T Peasants H:3355667789TJJQA222BR334446788TQQKKAA2455678999TTJJQKKA, L:33, D:88, U:TT, L:P, D:P, U:45678, L:89TJQ, D:P, U:P, L:556677, D:QQKKAA, U:P, L:P, D:4, U:A, L:2, D:P, U:P, L:J, D:P, U:Q, L:A, D:P3456789U:P, L:2, D:P, U:P, L:2, D:P, U:P, LTable 20. Case 2: Randomly selected (rather than cherry-picked) full games of DouZero played with other agents in Botzone. DouZero plays the position as noted in the left column. All DouZero agents were trained using WP as objectives. Position Logs Landlord H:33455778999TTTQKKAA2; 34567788TJQKKA2BR;. D:22, U:P, L:P, D:K, U:A, L:P, D:P, U:A, L:P, D:P, U:3, L:T, D:P, U:2, L:P, D:P, U:66, L:P, D:P, U:44. Landlord H:334566778899TTQQKA2B; 3455689TJJQKKAA22; 344567789TJJQKA2R, L:456789T, D:89TJQKA, U:P, L:P, D:3, U:4, L:K, D:A, U:2, L:P, D:P, U:34567, L:6789T, D:P, U:P, L:33, D:55, U:P, L:QQ, D:22, U:P, L:P, D:4, U:J, L:B, D:P, U:R, L:P, D:P, U:789TJQKA Landlord H:33455668TTTJJJQKAA22; 3445677789QQKKA2R; 3456788999TJQKA2B, L:4, D:A, U:P, L:P, D:4, U:9, L:Q, D:K, U:2, L:P, D:P, U:89TJQKA, L:P, D:P, U:B, L:P, D:PTable 20. Case 2: Randomly selected (rather than cherry-picked) full games of DouZero played with other agents in Botzone. DouZero plays the position as noted in the left column. All DouZero agents were trained using WP as objectives. Position Logs Landlord H:33455778999TTTQKKAA2; 34567788TJQKKA2BR; 344566689JJJQQA22, L:48999TTT, D:BR, U:P, L:P, D:34567, U:P, L:P, D:TJQKA, U:P, L:P, D:88, U:QQ, L:KK, D:P, U:22, L:P, D:P, U:3666, L:P, D:P, U:5JJJ, L:P, D:P, U:44, L:AA, D:P, U:P, L:2, D:P, U:P, L:77, D:P, U:P, L:33, D:P, U:P, L:55, D:P, U:P, L:Q Peasants H:34456779TTJKKKAAAA22; 34455668888JQQQ2R; 335677999TTJJQK2B, L:34567, D:P, U:9TJQK, L:P, D:P, U:T, L:J, D:Q, U:P, L:A, D:2, U:P, L:P, D:445566, U:P, L:P, D:38888Q, U:P, L:P, D:Q, U:P, L:P, D:R, U:P, L:P, D:J Landlord H:334455668899TJQQAA2B; 34567777TJJKKKAA2; 34568899TTJQQK22R, L:33445566, D:P, U:P, L:9, D:T, U:P, L:Q, D:A, U:P, L:P, D:777JJ, U:P, L:P, D:34567, U:P, L:89TJQ, D:P, U:9TJQK, L:P, D:P, U:3, L:8, D:A, U:P, L:2, D:P, U:R, L:P, D:P, U:88, L:AA, D:P, U:22, L:P, D:P, U:4, L:B Peasants H:34455677889TTJJQKK2R; 33557899JQQQKKA22; 344666789TTJAAA2B, L:345678, D:P, U:6789TJ, L:789TJQ, D:P, U:P, L:4, D:7, U:A, L:2, D:P, U:B, L:R, D:P, U:P, L:5, D:A, U:P, L:P, D:JQQQ, U:P, L:P, D:55, U:P, L:KK, D:22, U:P, L:P, D:K, U:A, L:P, D:P, U:A, L:P, D:P, U:3, L:T, D:P, U:2, L:P, D:P, U:66, L:P, D:P, U:44, L:P, D:P, U:T Peasants H:3355667789TJJQA222BR; 334446788TQQKKAA2; 455678999TTJJQKKA, L:33, D:88, U:TT, L:P, D:P, U:45678, L:89TJQ, D:P, U:P, L:556677, D:QQKKAA, U:P, L:P, D:4, U:A, L:2, D:P, U:P, L:J, D:P, U:Q, L:A, D:P, U:P, L:2, D:P, U:P, L:2, D:P, U:P, L:BR Landlord H:334566778899TTQQKA2B; 3455689TJJQKKAA22; 344567789TJJQKA2R, L:456789T, D:89TJQKA, U:P, L:P, D:3, U:4, L:K, D:A, U:2, L:P, D:P, U:34567, L:6789T, D:P, U:P, L:33, D:55, U:P, L:QQ, D:22, U:P, L:P, D:4, U:J, L:B, D:P, U:R, L:P, D:P, U:789TJQKA Landlord H:33455668TTTJJJQKAA22; 3445677789QQKKA2R; 3456788999TJQKA2B, L:4, D:A, U:P, L:P, D:4, U:9, L:Q, D:K, U:2, L:P, D:P, U:89TJQKA, L:P, D:P, U:B, L:P, D:P, U:3456789\n", "annotations": {"author": "[{\"end\":86,\"start\":74},{\"end\":98,\"start\":87},{\"end\":108,\"start\":99},{\"end\":121,\"start\":109},{\"end\":135,\"start\":122},{\"end\":143,\"start\":136},{\"end\":151,\"start\":144}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":82},{\"end\":97,\"start\":94},{\"end\":107,\"start\":105},{\"end\":120,\"start\":115},{\"end\":134,\"start\":130},{\"end\":142,\"start\":140},{\"end\":150,\"start\":147}]", "author_first_name": "[{\"end\":81,\"start\":74},{\"end\":93,\"start\":87},{\"end\":104,\"start\":99},{\"end\":114,\"start\":109},{\"end\":129,\"start\":122},{\"end\":139,\"start\":136},{\"end\":146,\"start\":144}]", "author_affiliation": null, "title": "[{\"end\":71,\"start\":1},{\"end\":222,\"start\":152}]", "venue": null, "abstract": "[{\"end\":1603,\"start\":224}]", "bib_ref": "[{\"end\":1981,\"start\":1952},{\"end\":2014,\"start\":1993},{\"end\":2054,\"start\":2019},{\"end\":2422,\"start\":2398},{\"end\":2446,\"start\":2422},{\"end\":2468,\"start\":2446},{\"end\":2491,\"start\":2468},{\"end\":2576,\"start\":2551},{\"end\":3179,\"start\":3155},{\"end\":3469,\"start\":3450},{\"end\":3701,\"start\":3682},{\"end\":3793,\"start\":3772},{\"end\":3851,\"start\":3832},{\"end\":3996,\"start\":3969},{\"end\":4052,\"start\":4041},{\"end\":4122,\"start\":4104},{\"end\":4285,\"start\":4266},{\"end\":4525,\"start\":4507},{\"end\":4834,\"start\":4814},{\"end\":5642,\"start\":5622},{\"end\":6044,\"start\":6022},{\"end\":7531,\"start\":7511},{\"end\":7549,\"start\":7531},{\"end\":7604,\"start\":7586},{\"end\":7622,\"start\":7604},{\"end\":8111,\"start\":8078},{\"end\":8149,\"start\":8125},{\"end\":9230,\"start\":9211},{\"end\":9249,\"start\":9230},{\"end\":10505,\"start\":10478},{\"end\":10523,\"start\":10505},{\"end\":10705,\"start\":10683},{\"end\":11869,\"start\":11847},{\"end\":12224,\"start\":12208},{\"end\":12249,\"start\":12226},{\"end\":12283,\"start\":12251},{\"end\":12313,\"start\":12283},{\"end\":12751,\"start\":12724},{\"end\":14002,\"start\":13983},{\"end\":14360,\"start\":14336},{\"end\":14374,\"start\":14360},{\"end\":14453,\"start\":14432},{\"end\":14530,\"start\":14504},{\"end\":14564,\"start\":14553},{\"end\":14822,\"start\":14800},{\"end\":15238,\"start\":15220},{\"end\":15260,\"start\":15238},{\"end\":17954,\"start\":17931},{\"end\":30852,\"start\":30832},{\"end\":31113,\"start\":31102},{\"end\":31134,\"start\":31113},{\"end\":31152,\"start\":31134},{\"end\":31171,\"start\":31152},{\"end\":43091,\"start\":43086},{\"end\":43095,\"start\":43091},{\"end\":43099,\"start\":43095},{\"end\":43102,\"start\":43099},{\"end\":43105,\"start\":43102},{\"end\":43108,\"start\":43105}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":56649,\"start\":56446},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56704,\"start\":56650},{\"attributes\":{\"id\":\"fig_3\"},\"end\":56967,\"start\":56705},{\"attributes\":{\"id\":\"fig_4\"},\"end\":57268,\"start\":56968},{\"attributes\":{\"id\":\"fig_5\"},\"end\":58473,\"start\":57269},{\"attributes\":{\"id\":\"fig_6\"},\"end\":58933,\"start\":58474},{\"attributes\":{\"id\":\"fig_7\"},\"end\":59100,\"start\":58934},{\"attributes\":{\"id\":\"fig_8\"},\"end\":59721,\"start\":59101},{\"attributes\":{\"id\":\"fig_9\"},\"end\":59764,\"start\":59722},{\"attributes\":{\"id\":\"fig_10\"},\"end\":65786,\"start\":59765},{\"attributes\":{\"id\":\"fig_11\"},\"end\":65859,\"start\":65787},{\"attributes\":{\"id\":\"fig_13\"},\"end\":66084,\"start\":65860},{\"attributes\":{\"id\":\"fig_14\"},\"end\":66169,\"start\":66085},{\"attributes\":{\"id\":\"fig_15\"},\"end\":66341,\"start\":66170},{\"attributes\":{\"id\":\"fig_16\"},\"end\":66845,\"start\":66342},{\"attributes\":{\"id\":\"fig_17\"},\"end\":67659,\"start\":66846},{\"attributes\":{\"id\":\"fig_18\"},\"end\":70276,\"start\":67660},{\"attributes\":{\"id\":\"fig_19\"},\"end\":72844,\"start\":70277},{\"attributes\":{\"id\":\"fig_21\"},\"end\":75109,\"start\":72845},{\"attributes\":{\"id\":\"fig_22\"},\"end\":77955,\"start\":75110},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":78848,\"start\":77956},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":79900,\"start\":78849},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":80324,\"start\":79901},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":80954,\"start\":80325},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":81597,\"start\":80955},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":82592,\"start\":81598},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":84269,\"start\":82593},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":87116,\"start\":84270},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":88919,\"start\":87117},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":89335,\"start\":88920},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":90607,\"start\":89336},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":90652,\"start\":90608},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":92960,\"start\":90653},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":93439,\"start\":92961},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":93888,\"start\":93440},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":94263,\"start\":93889},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":94654,\"start\":94264}]", "paragraph": "[{\"end\":1787,\"start\":1619},{\"end\":4363,\"start\":1789},{\"end\":5804,\"start\":4365},{\"end\":8399,\"start\":5806},{\"end\":9079,\"start\":8426},{\"end\":9130,\"start\":9081},{\"end\":10182,\"start\":9132},{\"end\":10524,\"start\":10203},{\"end\":10969,\"start\":10574},{\"end\":11002,\"start\":10971},{\"end\":11193,\"start\":11004},{\"end\":11709,\"start\":11219},{\"end\":12120,\"start\":11711},{\"end\":13214,\"start\":12164},{\"end\":13883,\"start\":13216},{\"end\":14189,\"start\":13919},{\"end\":14823,\"start\":14191},{\"end\":15409,\"start\":14825},{\"end\":15658,\"start\":15428},{\"end\":16529,\"start\":15706},{\"end\":17411,\"start\":16549},{\"end\":17830,\"start\":17427},{\"end\":19200,\"start\":17853},{\"end\":19265,\"start\":19243},{\"end\":19516,\"start\":19267},{\"end\":19656,\"start\":19518},{\"end\":20449,\"start\":19658},{\"end\":21136,\"start\":20451},{\"end\":22138,\"start\":21138},{\"end\":22558,\"start\":22140},{\"end\":23512,\"start\":22600},{\"end\":24093,\"start\":23514},{\"end\":24530,\"start\":24127},{\"end\":24856,\"start\":24532},{\"end\":26152,\"start\":24890},{\"end\":26358,\"start\":26154},{\"end\":27471,\"start\":26401},{\"end\":28363,\"start\":27510},{\"end\":29066,\"start\":28396},{\"end\":29735,\"start\":29081},{\"end\":29792,\"start\":29752},{\"end\":30503,\"start\":29824},{\"end\":31388,\"start\":30505},{\"end\":31516,\"start\":31390},{\"end\":31527,\"start\":31518},{\"end\":32733,\"start\":31559},{\"end\":32977,\"start\":32748},{\"end\":33249,\"start\":32979},{\"end\":34201,\"start\":33251},{\"end\":35100,\"start\":34203},{\"end\":35616,\"start\":35102},{\"end\":35642,\"start\":35618},{\"end\":35685,\"start\":35644},{\"end\":35732,\"start\":35687},{\"end\":35815,\"start\":35734},{\"end\":35993,\"start\":35817},{\"end\":36037,\"start\":35995},{\"end\":36079,\"start\":36039},{\"end\":36171,\"start\":36081},{\"end\":36242,\"start\":36173},{\"end\":36267,\"start\":36244},{\"end\":36300,\"start\":36269},{\"end\":37487,\"start\":36344},{\"end\":38434,\"start\":37489},{\"end\":42177,\"start\":38549},{\"end\":43359,\"start\":42257},{\"end\":43701,\"start\":43376},{\"end\":43713,\"start\":43703},{\"end\":44474,\"start\":43807},{\"end\":44478,\"start\":44476},{\"end\":45257,\"start\":44539},{\"end\":46084,\"start\":45259},{\"end\":47808,\"start\":46086},{\"end\":48428,\"start\":47944},{\"end\":48955,\"start\":48459},{\"end\":49630,\"start\":48957},{\"end\":50160,\"start\":49664},{\"end\":50423,\"start\":50162},{\"end\":50801,\"start\":50454},{\"end\":51313,\"start\":50803},{\"end\":52036,\"start\":51315},{\"end\":52444,\"start\":52038},{\"end\":54529,\"start\":52485},{\"end\":56407,\"start\":54560}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":47909,\"start\":47809}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24569,\"start\":24562},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":37614,\"start\":37607},{\"end\":39839,\"start\":39832},{\"end\":39851,\"start\":39844},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":43045,\"start\":43038},{\"end\":43877,\"start\":43858},{\"end\":44180,\"start\":44173},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":45406,\"start\":45399},{\"end\":45552,\"start\":45545},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45565,\"start\":45557},{\"end\":45704,\"start\":45697},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45792,\"start\":45771},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45872,\"start\":45864},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":46398,\"start\":46390},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":46644,\"start\":46636},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":51851,\"start\":51841},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":55485,\"start\":55477}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1617,\"start\":1605},{\"attributes\":{\"n\":\"2.\"},\"end\":8424,\"start\":8402},{\"attributes\":{\"n\":\"3.\"},\"end\":10201,\"start\":10185},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10572,\"start\":10527},{\"end\":11217,\"start\":11196},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12162,\"start\":12123},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13917,\"start\":13886},{\"attributes\":{\"n\":\"4.\"},\"end\":15426,\"start\":15412},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15704,\"start\":15661},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16547,\"start\":16532},{\"attributes\":{\"n\":\"5.\"},\"end\":17425,\"start\":17414},{\"attributes\":{\"n\":\"5.1.\"},\"end\":17851,\"start\":17833},{\"end\":19241,\"start\":19203},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22598,\"start\":22561},{\"attributes\":{\"n\":\"5.3.\"},\"end\":24125,\"start\":24096},{\"attributes\":{\"n\":\"5.4.\"},\"end\":24888,\"start\":24859},{\"attributes\":{\"n\":\"5.5.\"},\"end\":26399,\"start\":26361},{\"attributes\":{\"n\":\"5.6.\"},\"end\":27508,\"start\":27474},{\"attributes\":{\"n\":\"5.7.\"},\"end\":28394,\"start\":28366},{\"attributes\":{\"n\":\"5.8.\"},\"end\":29079,\"start\":29069},{\"attributes\":{\"n\":\"6.\"},\"end\":29750,\"start\":29738},{\"attributes\":{\"n\":\"7.\"},\"end\":29822,\"start\":29795},{\"end\":31557,\"start\":31530},{\"end\":32746,\"start\":32736},{\"end\":36342,\"start\":36303},{\"end\":38508,\"start\":38437},{\"end\":38547,\"start\":38511},{\"end\":42188,\"start\":42180},{\"end\":42255,\"start\":42191},{\"end\":43374,\"start\":43362},{\"end\":43748,\"start\":43716},{\"end\":43805,\"start\":43751},{\"end\":44485,\"start\":44481},{\"end\":44537,\"start\":44488},{\"end\":47942,\"start\":47911},{\"end\":48457,\"start\":48431},{\"end\":49662,\"start\":49633},{\"end\":50452,\"start\":50426},{\"end\":52483,\"start\":52447},{\"end\":54558,\"start\":54532},{\"end\":56438,\"start\":56410},{\"end\":56445,\"start\":56441},{\"end\":56661,\"start\":56651},{\"end\":56716,\"start\":56706},{\"end\":56979,\"start\":56969},{\"end\":57275,\"start\":57270},{\"end\":58483,\"start\":58475},{\"end\":58945,\"start\":58935},{\"end\":59122,\"start\":59102},{\"end\":59733,\"start\":59723},{\"end\":65799,\"start\":65788},{\"end\":65872,\"start\":65861},{\"end\":66097,\"start\":66086},{\"end\":66180,\"start\":66171},{\"end\":66363,\"start\":66343},{\"end\":66880,\"start\":66847},{\"end\":67722,\"start\":67661},{\"end\":70311,\"start\":70278},{\"end\":72878,\"start\":72846},{\"end\":75144,\"start\":75111},{\"end\":78853,\"start\":78850},{\"end\":79911,\"start\":79902},{\"end\":80333,\"start\":80326},{\"end\":80965,\"start\":80956},{\"end\":88930,\"start\":88921},{\"end\":89355,\"start\":89337},{\"end\":90618,\"start\":90609},{\"end\":92971,\"start\":92962},{\"end\":93900,\"start\":93890},{\"end\":94275,\"start\":94265}]", "table": "[{\"end\":78848,\"start\":78061},{\"end\":79900,\"start\":78967},{\"end\":80324,\"start\":80321},{\"end\":80954,\"start\":80335},{\"end\":81597,\"start\":81520},{\"end\":82592,\"start\":82480},{\"end\":87116,\"start\":84929},{\"end\":88919,\"start\":87322},{\"end\":89335,\"start\":89033},{\"end\":90607,\"start\":90598},{\"end\":92960,\"start\":90959},{\"end\":93439,\"start\":93205},{\"end\":93888,\"start\":93529},{\"end\":94263,\"start\":93959}]", "figure_caption": "[{\"end\":56649,\"start\":56448},{\"end\":56704,\"start\":56663},{\"end\":56967,\"start\":56718},{\"end\":57268,\"start\":56981},{\"end\":58473,\"start\":57279},{\"end\":58933,\"start\":58485},{\"end\":59100,\"start\":58947},{\"end\":59721,\"start\":59125},{\"end\":59764,\"start\":59735},{\"end\":65786,\"start\":59767},{\"end\":65859,\"start\":65802},{\"end\":66084,\"start\":65875},{\"end\":66169,\"start\":66100},{\"end\":66341,\"start\":66183},{\"end\":66845,\"start\":66368},{\"end\":67659,\"start\":66887},{\"end\":70276,\"start\":67734},{\"end\":72844,\"start\":70318},{\"end\":75109,\"start\":72886},{\"end\":77955,\"start\":75151},{\"end\":78061,\"start\":77958},{\"end\":78967,\"start\":78855},{\"end\":80321,\"start\":79913},{\"end\":81520,\"start\":80967},{\"end\":82480,\"start\":81600},{\"end\":84269,\"start\":82595},{\"end\":84929,\"start\":84272},{\"end\":87322,\"start\":87119},{\"end\":89033,\"start\":88932},{\"end\":90598,\"start\":89358},{\"end\":90652,\"start\":90620},{\"end\":90959,\"start\":90655},{\"end\":93205,\"start\":92973},{\"end\":93529,\"start\":93442},{\"end\":93959,\"start\":93903},{\"end\":94654,\"start\":94278}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9421,\"start\":9412},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9773,\"start\":9765},{\"end\":13881,\"start\":13873},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15777,\"start\":15769},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15906,\"start\":15898},{\"end\":21774,\"start\":21766},{\"end\":23274,\"start\":23266},{\"end\":23545,\"start\":23537},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24961,\"start\":24953},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26001,\"start\":25993},{\"end\":26793,\"start\":26785},{\"end\":27740,\"start\":27732},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":28468,\"start\":28460},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40744,\"start\":40735},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":41859,\"start\":41850},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":41941,\"start\":41932},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":43317,\"start\":43308}]", "bib_author_first_name": "[{\"end\":95581,\"start\":95577},{\"end\":95585,\"start\":95584},{\"end\":95593,\"start\":95590},{\"end\":95597,\"start\":95596},{\"end\":95603,\"start\":95602},{\"end\":95782,\"start\":95781},{\"end\":95800,\"start\":95799},{\"end\":95806,\"start\":95805},{\"end\":96134,\"start\":96133},{\"end\":96170,\"start\":96163},{\"end\":96180,\"start\":96173},{\"end\":96184,\"start\":96183},{\"end\":96193,\"start\":96192},{\"end\":96321,\"start\":96314},{\"end\":96331,\"start\":96324},{\"end\":96335,\"start\":96334},{\"end\":96384,\"start\":96383},{\"end\":96392,\"start\":96391},{\"end\":96442,\"start\":96435},{\"end\":96446,\"start\":96445},{\"end\":100657,\"start\":100647},{\"end\":100678,\"start\":100677},{\"end\":100744,\"start\":100743},{\"end\":100750,\"start\":100749},{\"end\":100814,\"start\":100813},{\"end\":100820,\"start\":100819},{\"end\":100967,\"start\":100907},{\"end\":100971,\"start\":100970},{\"end\":101117,\"start\":101114},{\"end\":101188,\"start\":101128},{\"end\":101192,\"start\":101191},{\"end\":101308,\"start\":101305},{\"end\":101314,\"start\":101311},{\"end\":101318,\"start\":101317}]", "bib_author_last_name": "[{\"end\":95588,\"start\":95586},{\"end\":95600,\"start\":95598},{\"end\":95779,\"start\":95604},{\"end\":95797,\"start\":95783},{\"end\":95803,\"start\":95801},{\"end\":96131,\"start\":95807},{\"end\":96161,\"start\":96135},{\"end\":96190,\"start\":96185},{\"end\":96312,\"start\":96194},{\"end\":96381,\"start\":96336},{\"end\":96389,\"start\":96385},{\"end\":96433,\"start\":96393},{\"end\":96535,\"start\":96447},{\"end\":100675,\"start\":100658},{\"end\":100741,\"start\":100679},{\"end\":100747,\"start\":100745},{\"end\":100811,\"start\":100751},{\"end\":100817,\"start\":100815},{\"end\":100905,\"start\":100821},{\"end\":101112,\"start\":100972},{\"end\":101126,\"start\":101118},{\"end\":101303,\"start\":101193}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":100643,\"start\":95577},{\"attributes\":{\"id\":\"b1\"},\"end\":104444,\"start\":100645}]", "bib_title": null, "bib_author": "[{\"end\":95584,\"start\":95577},{\"end\":95590,\"start\":95584},{\"end\":95596,\"start\":95590},{\"end\":95602,\"start\":95596},{\"end\":95781,\"start\":95602},{\"end\":95799,\"start\":95781},{\"end\":95805,\"start\":95799},{\"end\":96133,\"start\":95805},{\"end\":96163,\"start\":96133},{\"end\":96173,\"start\":96163},{\"end\":96183,\"start\":96173},{\"end\":96192,\"start\":96183},{\"end\":96314,\"start\":96192},{\"end\":96324,\"start\":96314},{\"end\":96334,\"start\":96324},{\"end\":96383,\"start\":96334},{\"end\":96391,\"start\":96383},{\"end\":96435,\"start\":96391},{\"end\":96445,\"start\":96435},{\"end\":96537,\"start\":96445},{\"end\":100677,\"start\":100647},{\"end\":100743,\"start\":100677},{\"end\":100749,\"start\":100743},{\"end\":100813,\"start\":100749},{\"end\":100819,\"start\":100813},{\"end\":100907,\"start\":100819},{\"end\":100970,\"start\":100907},{\"end\":101114,\"start\":100970},{\"end\":101128,\"start\":101114},{\"end\":101191,\"start\":101128},{\"end\":101305,\"start\":101191},{\"end\":101311,\"start\":101305},{\"end\":101317,\"start\":101311},{\"end\":101321,\"start\":101317}]", "bib_venue": "[{\"end\":96970,\"start\":96949},{\"end\":96699,\"start\":96537}]"}}}, "year": 2023, "month": 12, "day": 17}