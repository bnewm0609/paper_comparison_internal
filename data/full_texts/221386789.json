{"id": 221386789, "updated": "2022-01-22 12:01:56.174", "metadata": {"title": "Human 3D pose estimation in a lying position by RGB-D images for medical diagnosis and rehabilitation", "authors": "[{\"middle\":[],\"last\":\"Wu\",\"first\":\"Qingqiang\"},{\"middle\":[],\"last\":\"Xu\",\"first\":\"Guanghua\"},{\"middle\":[],\"last\":\"Zhang\",\"first\":\"Sicong\"},{\"middle\":[],\"last\":\"Li\",\"first\":\"Yu\"},{\"middle\":[],\"last\":\"Wei\",\"first\":\"Fan\"}]", "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)", "journal": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Posture recognition in the human lying position is of great significance for the rehabilitation evaluation of lying patients and the diagnosis of infants with early cerebral palsy. In this paper, we proposed a novel method for human 3D pose estimation in a lying position with the RGB image and corresponding depth information. Firstly, we employ current pose estimation method on RGB images to achieve the human full body 2D keypoints. By combining the depth information and coordinate transformation, the 3D movement of human in lying position can be obtained. We validate our method with two public datasets. The results show that the accuracy can reach the state-of-the-art.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "3082648061", "acl": null, "pubmed": "33019293", "pubmedcentral": null, "dblp": "conf/embc/WuXZLW20", "doi": "10.1109/embc44109.2020.9176407"}}, "content": {"source": {"pdf_hash": "7bc9cc7e8f1f13d6c2c988da1bf186b3cf483ced", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "437408e8b40e38c0fc4064c0c42b191195160185", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7bc9cc7e8f1f13d6c2c988da1bf186b3cf483ced.txt", "contents": "\n\n\n\uf020\nPosture recognition in the human lying position is of great significance for the rehabilitation evaluation of lying patients and the diagnosis of infants with early cerebral palsy. In this paper, we proposed a novel method for human 3D pose estimation in a lying position with the RGB image and corresponding depth information. Firstly, we employ current pose estimation method on RGB images to achieve the human full body 2D keypoints. By combining the depth information and coordinate transformation, the 3D movement of human in lying position can be obtained. We validate our method with two public datasets. The results show that the accuracy can reach the state-of-the-art.\n\nI. INTRODUCTION\n\nMost researchers focus on the recognition of human pose with normal positions. There are still many scenarios where the human body is in a lying position, such as infants' movements and patients with reduced mobility in the early stages of rehabilitation. Moreover, the baby's body size is quite different from that of an adult, it's more difficult for infants pose recognition.\n\nShotton et al. [1] use single depth image to estimate the 3D body parts. The full body depth images were segmented into about 31 parts by manual as training samples, but this method can't segment background and foreground in a lying position because of similar depth values. Therefore, it is difficult to directly use Kinect to obtain the movements of an infant in a lying position. Nikolas Hesse et al. [2] adopt a statistical 3D Skinned Multi-infant Linear body model to track the infant movements. They restricted their subjects to infants, and didn't consider adult gesture recognition in a lying position. In [3], Wu et al use model matching recursion method for occlusion in 3D pose estimation.\n\nAnther researchers use RGB images to estimation human 2D pose by designing Convolutional Neural Network structures. Cao et al. [4] propose a part affinity field to characterize the connections between body parts. Newell et al. [5] adopt associative embedding to encode the key points from the same person. In [6] [7]. For a balance of accuracy and computational efficiency, we use PAF method as our 2D infant pose estimation algorithm. Thanks to a rich public RGB datasets of human poses, researchers have proposed a large number of 2D human joint recognition algorithms based on RGB. These methods are not affected by the background and can well estimate the 2D joint point coordinates of the human body in a lying position, although they do not contain spatial information.\n\nThe main contributions of our works as follows: Firstly, we expand the research object of gesture recognition. Our 3D human pose estimation method works not only for adults but also for infants. Secondly, we extend the range of human positions including normal positions and lying position for gesture recognition.\n\nThe rest of the sections of this paper are organized as follows: Section II presents the process of the suggested method. Section III depicts experiments and results on public adult and infant movement datasets. The last Section draws out conclusions and our future work.\n\n\nII. PROPOSED METHOD\n\nAn overview of the proposed method is provided in Fig. 1. We use Kinect to capture color and depth videos of subject movements. Then Openpose also called PAF method is applied to estimate 2D movement of the subject. Combine the corresponding depth information and perform coordinate transformation, the rough 3D pose can be obtained. Afterwards, we need to correct the rough 3D pose. Then the lying position 3D pose estimation can be achieved finally.\n\n\nA. 2D pose estimation\n\nUnlike the top-down approaches, the PAF network focus on the detection of keypoints firstly, then by Part Affinity Field to find the proper connections among these body parts. The method avoids the common problem existed in top-down pipeline, that person bounding box accuracy plays an important role overall performance of pose estimation directly, especially some small objects or indistinguishable parts appear in image.\n\nThe PAF method can only obtain the 2D keypoints coordinates. Through the introduction of depth information, we have the possibility to further obtain the 3d coordinates of the key points.   Then the transformation from world coordinate system to RGB system and to Depth system can be described using the following (1) and (2):\n\n\nHuman 3D pose estimation in a lying position by RGB-D images for medical diagnosis and rehabilitation\n\u2212 = \u2212 + (1) \u2212 = \u2212 +(2)\nWhile in imaging system, the RGB image coordinate system [ , , 1] can be described by (3).\n[ ] = [ ](3)\nSo as to depth image system [ , , ] :\n[ ] = [ ](4)\nWhere the and are sensor inherent parameters. Synthesize the above equations, we can get the transformation form [ , , 1] to [ , , 1] . Due to the close distance between the depth camera and the color camera, we assume \u2248 . Thus:\n[ 1 ] \u2248 \u22121 \u22121 [ 1 ] + ( \u2212 \u22121 )(5)\nSince the Kinect sensor is a standardized production device, the calibration parameters in [8] are used to solve the spatial mapping. If the joint ( , ) in the color image is solved to obtain a depth image map as ' ( , ), the depth value of the P' is in the depth camera coordinate system. So far, the three dimensional coordinates according to the world coordinate system ( , , ) relative to ( , ) have been obtained according to Eq.1 and Eq.3.\n\n\nC. Coordinate correction\n\nSince the depth information captured by Kinect is body surface information, while the true joint position is hidden below the body surface, this will cause error if only use coordinate transformation.\n\nWe introduce \"Chinese Adult Body Size (GB / T 10000-1988)\" [9] and [10] for static body size modeling of different types of test groups. These models mainly include the relative semi-depth values of various parts of the body in a lying position.\n\nThe measurement error of the depth information is mainly caused by the half-depth in the direction of the sagittal axis of the human body. Therefore, the static size model of the human body for correction include the relative half-length of each node. Considering the stability of the human torso, the ratio of the half-depth of the sagittal axis to the length of the torso of the upper body is taken as the parameter of the static size model. The model parameters of the adult male and female and infant are also solved separately for individual differences.\n\nWe assume that each limb is cylindrical. By referring to standard body model in [9,10], the radius information of the cylindrical model of each body part in the standard body model can be obtained. The length of the upper standard body is the standard d. then the ratio {\u03b1 i } of each part to the upper body length d can be obtained by = / . Here, i indicates different body parts: head (he), neck (ne), shoulder (sh), elbow (el), wrist (wr), hip (hi), knee (kn) and ankle (an). We calculated the scale factors { } of body parts for different standard body models as shown in Table 1. Then the exact = + , where is the i-th joint depth value measured by Kinect and = \u2016 \u2212 \u210e \u2016 2 . In order to verify the validity of the static size model for joint depth information correction, we use Kinect sensors to collect static data when the human body is standing, and make the SDK as the ground truth. The comparison of errors is shown in Fig. 3. The errors before correction are all large negative errors, and the mean error is 42.07mm. After correction, the errors are greatly reduced, and the mean error is 9.86mm. \n\n\nIII. EXPERIMENTS AND RESULTS\n\n\nA. Public datasets\n\nIn order to verify the proposed method, we test our pose estimation approach using two public datasets: UTD-MHAD and MINI-RGBD. UTD-MHAD [11] is a human RGB-D action dataset, captured by one Microsoft Kinect camera and one wearable inertial sensor. MINI-RGBD dataset (Moving Infants In RGB-D) [12] containing 12 sequences of real infant movements. In order to make the comparison convincing, we only selected the keypoints of the two datasets of that roughly coincide with our definition of keypoint. Our experimental protocol was approved by a Xi'an Jiaotong University Office of Research Ethics.\n\n\nB. Adult pose estimation\n\nWe perform our method on a total of 10,361 frames of UTD-MHAD datasets. The result shows that the average joint position error (AJPE) of the length of each part of the body is 31.75mm. The average error of each body part is shown in Fig. 4. It can be seen that a larger error appears in the upper limbs. This is because the upper limb movements are the majority and the movements are more complex in the data set, which affects the accuracy of the estimation. We apply Percentage of Correctly Localized Parts (PCP) [13] metric and Percentage of Correct Key-point (PCK) [14] metric to test the accuracy of the proposed method. In PCP method, the threshold k generally takes 0.5, while in PCK, k=0.1. Fig. 5 shows the trend of the PCP and PCK of our method on UTD-MHAD with the changing k. With k gradually increasing, the whole-body detection rate increased to varying degrees. The PCP and PCK of the torso and thighs are significantly higher than that of other parts. This is because the whole part of the human body is thicker and has less movement and deformation in the data set, which is easy to detect and locate. The PCP and PCK of the right upper limb are slightly lower than other parts, which is caused by the richer and more variable right upper limb movement in UTD-MHAD.\n\n\nC. Infant 3D pose estimation in lying position\n\nWe set the experiment as [12], and the result is shown in Fig. 6. AJPE over all sequences and joints except head is 13.76mm. Comparing to the AJPE baseline 29.6mm in [12], we have a higher accuracy. Note that, our AJPE don't consist of head joint because of the different definitions of head joint. Figure 6. The AJPE of all frames on MINI-RGBD Under the PCP and PCK metric, the accuracy of the neck is higher (showed in Fig. 7). This is because the head and neck movements are less in each group of samples, resulting in a relatively fixed neck position and easier detection. The baseline of PCK in [12] is 64.2% with the full head segment length, and a PCK rate of 90.4% with two times the head segment length. Notice that our PCK metric is stricter than [12]. Our PCP of the average detection accuracy of the dataset is 80.7%, and PCK is 86.1%. \n\n\nD. Discussion\n\nWe present some pose estimation results in Fig. 8. The first column shows our result on UTD-MHAD with adults in normal positions, and the second column shows the result on MINI-RGBD with infants in lying position. The left hand-side is the ground truth, and the right hand-side is our result.\n\nOur method is highly dependent on 2D human pose recognition. In most cases, the 3D estimate poses in a lying position using our method are similar to the ground truth. However, when the pose has a harder view or obstructive gesture, our method may be misidentified, such as bottom right example in Fig. 8. Due to the occlusion of the infant's lower limbs, the 2D pose recognition is incorrect, which affects the 3D recognition accuracy. For the lying position, we can avoid this problem as much as possible by adjusting the viewing angle appropriately. In this study, we proposed a new human in a lying position pose estimation method, and extent the method to infants. We also verify our algorithm on two datasets. The results show that the method has a good prospect in the auto-GMA of infants and the rehabilitation evaluation of patients. Although the proposed method can better infer the posture of the infants, there are still some recognition errors, such as occlusion.\n\nFor the future work, we plan to replace the PAF with a better 2D pose algorithm to solve occlusion issues and build a larger infant movement dataset to analysis auto-GMA with our method. We also plan to apply the proposed method to the diagnosis of ultra-early cerebral palsy in infants and the rehabilitation evaluation of stroke patients.\n\nFigure 2 .\n2Transformation relationship between different coordinate systems\n\nFigure 3 .\n3Effect of depth information correction\n\nFigure 4 .\n4The AJPE of all frames on UTD-MHAD\n\nFigure 5 .\n5The PCP with k from 0 to 0.5 and PCK with k from 0 to 0.1 on UTD-MHAD\n\nFigure 7 .\n7The PCP with k from 0 to 0.5 and PCK with k from 0 to 0.1 on MINI-RGBD\n\nFigure 8 .\n8The results of some examples IV. CONCLUSION\n\n\n, He et al. used mask RCNN to address each human body bounding box with Region Proposal Network (RPN) as first, they designed a Region of Interest Align (RoIAlign) to avoid boundaries or bins shift of Region and Interest (RoI), then locate each body's keypoints. Chen et al. predict key point locations with person bounding boxes obtained by a person detector network in\n\n\nGuanghua Xu (corresponding author) is with the State Key Laboratory for Manufacturing Systems Engineering, School of Mechanical Engineering, Xi'an Jiaotong University Xi'an 710049 China (phone: 86-29-83395054; fax: 86-29-82664257; e-mail: ghxu@mail.xjtu.edu.cn). Qingqiang Wu, Sicong Zhang, Yu Li and Fan Wei are with School of Mechanical Engineering, Xi'an Jiaotong University Xi'an 710049 China.\n\n\nQingqiang Wu, Guanghua Xu, IEEE Member, Sicong Zhang, Yu Li, Fan Wei Figure 1. The overview of the proposed method by = + , where R is the rotation matrix and T is the transition matrix. In Kinect, the coordinate transform model can be expressed as Fig. 2.B. Coordinate transformation \nAccording to imaging theory, Imaging coordinate system \n\n-x \n\ncan be transformed from World coordinate system \n\n-x \n\n\n\nTABLE I .\nISCALE FACTORS { } FOR DIFFERENT MODELS\u00d7 \n10 \u22123 \nhe \nne \nsh \nel \nwr \nhi \nkn \nan \n\nmale \n200 \n235 \n110 \n81 \n53 \n143 \n126 \n76 \n\nfemale \n208 \n242 \n114 \n84 \n55 \n156 \n127 \n76 \n\ninfant \n270 \n200 \n100 \n75 \n50 \n200 \n115 \n70 \n\nThe research leading to these results has received funding from Key\nReal-Time Human Pose Recognition in Parts from Single Depth Images. Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard Moore, Alex Kipman, Andrew Blake, CVPR 2011. IeeeShotton, Jamie, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard Moore, Alex Kipman and Andrew Blake. \"Real- Time Human Pose Recognition in Parts from Single Depth Images.\" In CVPR 2011, 1297-1304: Ieee, 2011.\n\nLearning and Tracking the 3d Body Shape of Freely Moving Infants from Rgb-D Sequences. Nikolas Hesse, Sergi Pujades, Michael Black, Michael Arens, Ulrich Hofmann, Sebastian Schroeder, IEEE transactions. Hesse, Nikolas, Sergi Pujades, Michael Black, Michael Arens, Ulrich Hofmann and Sebastian Schroeder. \"Learning and Tracking the 3d Body Shape of Freely Moving Infants from Rgb-D Sequences.\" IEEE transactions on pattern analysis and machine intelligence, (2019).\n\nHuman Pose Estimation Method Based on Single Depth Image. Qingqiang Wu, Guanghua Xu, Min Li, Longting Chen, Xin Zhang, Jun Xie, IET Computer Vision. 126Wu, Qingqiang, Guanghua Xu, Min Li, Longting Chen, Xin Zhang and Jun Xie. \"Human Pose Estimation Method Based on Single Depth Image.\" IET Computer Vision 12, no. 6 (2018): 919-924.\n\nRealtime Multi-Person 2d Pose Estimation Using Part Affinity Fields. Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCao, Zhe, Tomas Simon, Shih-En Wei and Yaser Sheikh. \"Realtime Multi-Person 2d Pose Estimation Using Part Affinity Fields.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7291-7299, 2017.\n\nAssociative Embedding: End-to-End Learning for Joint Detection and Grouping. Alejandro Newell, Zhiao Huang, Jia Deng, Advances in Neural Information Processing Systems. Newell, Alejandro, Zhiao Huang and Jia Deng. \"Associative Embedding: End-to-End Learning for Joint Detection and Grouping.\" In Advances in Neural Information Processing Systems, 2277-2287, 2017.\n\nMask R-Cnn. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionHe, Kaiming, Georgia Gkioxari, Piotr Doll\u00e1r and Ross Girshick. \"Mask R-Cnn.\" In Proceedings of the IEEE international conference on computer vision, 2961-2969, 2017.\n\nCascaded Pyramid Network for Multi-Person Pose Estimation. Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChen, Yilun, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu and Jian Sun. \"Cascaded Pyramid Network for Multi-Person Pose Estimation.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7103-7112, 2018.\n\nCalibration of Kinect for Xbox One and Comparison between the Two Generations of Microsoft Sensors. Diana Pagliari, Livio Pinto, Sensors. 1511Pagliari, Diana and Livio Pinto. \"Calibration of Kinect for Xbox One and Comparison between the Two Generations of Microsoft Sensors.\" Sensors 15, no. 11 (2015): 27569-27589.\n\nChinese Adult Body Size\" Standard and Its Application. L W Xi, China Standards Review. 6Xi, LW. \"\"Chinese Adult Body Size\" Standard and Its Application.\" China Standards Review, no. 6 (1994): 22-23.\n\nResearch of Body Indexes of Newborn of Various Gestational Age in China. X Liu, B Zhang, B Wang, Med. J. Chinese People's Armed Police Forces. 1110X. Liu, B. Zhang, and B. Wang. \"Research of Body Indexes of Newborn of Various Gestational Age in China.\" Med. J. Chinese People's Armed Police Forces 11, no. 10 (2000): 588-590.\n\nUtd-Mhad: A Multimodal Dataset for Human Action Recognition Utilizing a Depth Camera and a Wearable Inertial Sensor. Chen Chen, Roozbeh Jafari, Nasser Kehtarnavaz, 2015 IEEE International conference on image processing (ICIP). IEEEChen, Chen, Roozbeh Jafari and Nasser Kehtarnavaz. \"Utd-Mhad: A Multimodal Dataset for Human Action Recognition Utilizing a Depth Camera and a Wearable Inertial Sensor.\" In 2015 IEEE International conference on image processing (ICIP), 168-172: IEEE, 2015.\n\nComputer Vision for Medical Infant Motion Analysis: State of the Art and Rgb-D Data Set. Nikolas Hesse, Christoph Bodensteiner, Michael Arens, G Ulrich, Raphael Hofmann, A Sebastian Weinberger, Schroeder, Proceedings of the European Conference on Computer Vision (ECCV. the European Conference on Computer Vision (ECCVHesse, Nikolas, Christoph Bodensteiner, Michael Arens, Ulrich G Hofmann, Raphael Weinberger and A Sebastian Schroeder. \"Computer Vision for Medical Infant Motion Analysis: State of the Art and Rgb-D Data Set.\" In Proceedings of the European Conference on Computer Vision (ECCV), 0-0, 2018\n\nProgressive Search Space Reduction for Human Pose Estimation. Vittorio Ferrari, Manuel Marin, - Jimenez, Andrew Zisserman, 2008 IEEE Conference on Computer Vision and Pattern Recognition. IEEEFerrari, Vittorio, Manuel Marin-Jimenez and Andrew Zisserman. \"Progressive Search Space Reduction for Human Pose Estimation.\" In 2008 IEEE Conference on Computer Vision and Pattern Recognition, 1-8: IEEE, 2008.\n\nArticulated Human Detection with Flexible Mixtures of Parts. Yi Yang, Deva Ramanan, IEEE transactions. 3512Yang, Yi and Deva Ramanan. \"Articulated Human Detection with Flexible Mixtures of Parts.\" IEEE transactions on pattern analysis and machine intelligence 35, no. 12 (2012): 2878-2890.\n", "annotations": {"author": null, "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": null, "title": null, "venue": null, "abstract": "[{\"start\":\"5\",\"end\":\"683\"}]", "bib_ref": "[{\"start\":\"1097\",\"end\":\"1100\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"1486\",\"end\":\"1489\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"1696\",\"end\":\"1699\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"1911\",\"end\":\"1914\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"2011\",\"end\":\"2014\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2093\",\"end\":\"2096\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"2097\",\"end\":\"2100\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"4808\",\"end\":\"4816\"},{\"start\":\"5037\",\"end\":\"5040\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"5681\",\"end\":\"5684\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"5689\",\"end\":\"5693\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"6510\",\"end\":\"6513\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"6513\",\"end\":\"6516\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"7729\",\"end\":\"7733\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"7885\",\"end\":\"7889\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"8733\",\"end\":\"8737\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"8787\",\"end\":\"8791\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"9576\",\"end\":\"9580\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"9717\",\"end\":\"9721\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"10151\",\"end\":\"10155\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"10308\",\"end\":\"10312\",\"attributes\":{\"ref_id\":\"b11\"}}]", "figure": "[{\"start\":\"12030\",\"end\":\"12107\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"12108\",\"end\":\"12159\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"12160\",\"end\":\"12207\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"12208\",\"end\":\"12290\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"12291\",\"end\":\"12374\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"12375\",\"end\":\"12431\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"12432\",\"end\":\"12804\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"12805\",\"end\":\"13204\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"13205\",\"end\":\"13610\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"13611\",\"end\":\"13839\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"702\",\"end\":\"1080\"},{\"start\":\"1082\",\"end\":\"1782\"},{\"start\":\"1784\",\"end\":\"2559\"},{\"start\":\"2561\",\"end\":\"2875\"},{\"start\":\"2877\",\"end\":\"3148\"},{\"start\":\"3172\",\"end\":\"3623\"},{\"start\":\"3649\",\"end\":\"4072\"},{\"start\":\"4074\",\"end\":\"4400\"},{\"start\":\"4528\",\"end\":\"4618\"},{\"start\":\"4632\",\"end\":\"4669\"},{\"start\":\"4683\",\"end\":\"4911\"},{\"start\":\"4946\",\"end\":\"5391\"},{\"start\":\"5420\",\"end\":\"5620\"},{\"start\":\"5622\",\"end\":\"5867\"},{\"start\":\"5869\",\"end\":\"6428\"},{\"start\":\"6430\",\"end\":\"7538\"},{\"start\":\"7592\",\"end\":\"8189\"},{\"start\":\"8218\",\"end\":\"9500\"},{\"start\":\"9551\",\"end\":\"10399\"},{\"start\":\"10417\",\"end\":\"10709\"},{\"start\":\"10711\",\"end\":\"11687\"},{\"start\":\"11689\",\"end\":\"12029\"}]", "formula": "[{\"start\":\"4505\",\"end\":\"4527\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"4619\",\"end\":\"4631\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"4670\",\"end\":\"4682\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"4912\",\"end\":\"4945\",\"attributes\":{\"id\":\"formula_3\"}}]", "table_ref": "[{\"start\":\"7006\",\"end\":\"7013\"}]", "section_header": "[{\"start\":\"685\",\"end\":\"700\"},{\"start\":\"3151\",\"end\":\"3170\"},{\"start\":\"3626\",\"end\":\"3647\"},{\"start\":\"4403\",\"end\":\"4504\"},{\"start\":\"5394\",\"end\":\"5418\"},{\"start\":\"7541\",\"end\":\"7569\"},{\"start\":\"7572\",\"end\":\"7590\"},{\"start\":\"8192\",\"end\":\"8216\"},{\"start\":\"9503\",\"end\":\"9549\"},{\"start\":\"10402\",\"end\":\"10415\"},{\"start\":\"12031\",\"end\":\"12041\"},{\"start\":\"12109\",\"end\":\"12119\"},{\"start\":\"12161\",\"end\":\"12171\"},{\"start\":\"12209\",\"end\":\"12219\"},{\"start\":\"12292\",\"end\":\"12302\"},{\"start\":\"12376\",\"end\":\"12386\"},{\"start\":\"13612\",\"end\":\"13621\"}]", "table": "[{\"start\":\"13463\",\"end\":\"13610\"},{\"start\":\"13661\",\"end\":\"13839\"}]", "figure_caption": "[{\"start\":\"12043\",\"end\":\"12107\"},{\"start\":\"12121\",\"end\":\"12159\"},{\"start\":\"12173\",\"end\":\"12207\"},{\"start\":\"12221\",\"end\":\"12290\"},{\"start\":\"12304\",\"end\":\"12374\"},{\"start\":\"12388\",\"end\":\"12431\"},{\"start\":\"12434\",\"end\":\"12804\"},{\"start\":\"12807\",\"end\":\"13204\"},{\"start\":\"13207\",\"end\":\"13463\"},{\"start\":\"13623\",\"end\":\"13661\"}]", "figure_ref": "[{\"start\":\"3222\",\"end\":\"3228\"},{\"start\":\"7359\",\"end\":\"7365\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"8451\",\"end\":\"8457\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"8917\",\"end\":\"8923\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"9609\",\"end\":\"9615\"},{\"start\":\"9850\",\"end\":\"9858\"},{\"start\":\"9972\",\"end\":\"9979\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"10460\",\"end\":\"10466\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"11009\",\"end\":\"11015\",\"attributes\":{\"ref_id\":\"fig_5\"}}]", "bib_author_first_name": "[{\"start\":\"13976\",\"end\":\"13981\"},{\"start\":\"13991\",\"end\":\"13997\"},{\"start\":\"14010\",\"end\":\"14013\"},{\"start\":\"14020\",\"end\":\"14024\"},{\"start\":\"14032\",\"end\":\"14036\"},{\"start\":\"14048\",\"end\":\"14055\"},{\"start\":\"14063\",\"end\":\"14067\"},{\"start\":\"14076\",\"end\":\"14082\"},{\"start\":\"14419\",\"end\":\"14426\"},{\"start\":\"14434\",\"end\":\"14439\"},{\"start\":\"14449\",\"end\":\"14456\"},{\"start\":\"14464\",\"end\":\"14471\"},{\"start\":\"14479\",\"end\":\"14485\"},{\"start\":\"14495\",\"end\":\"14504\"},{\"start\":\"14856\",\"end\":\"14865\"},{\"start\":\"14870\",\"end\":\"14878\"},{\"start\":\"14883\",\"end\":\"14886\"},{\"start\":\"14891\",\"end\":\"14899\"},{\"start\":\"14906\",\"end\":\"14909\"},{\"start\":\"14917\",\"end\":\"14920\"},{\"start\":\"15201\",\"end\":\"15204\"},{\"start\":\"15210\",\"end\":\"15215\"},{\"start\":\"15223\",\"end\":\"15230\"},{\"start\":\"15236\",\"end\":\"15241\"},{\"start\":\"15692\",\"end\":\"15701\"},{\"start\":\"15710\",\"end\":\"15715\"},{\"start\":\"15723\",\"end\":\"15726\"},{\"start\":\"15992\",\"end\":\"15999\"},{\"start\":\"16004\",\"end\":\"16011\"},{\"start\":\"16022\",\"end\":\"16027\"},{\"start\":\"16036\",\"end\":\"16040\"},{\"start\":\"16398\",\"end\":\"16403\"},{\"start\":\"16410\",\"end\":\"16418\"},{\"start\":\"16425\",\"end\":\"16432\"},{\"start\":\"16439\",\"end\":\"16447\"},{\"start\":\"16455\",\"end\":\"16459\"},{\"start\":\"16464\",\"end\":\"16468\"},{\"start\":\"16956\",\"end\":\"16961\"},{\"start\":\"16972\",\"end\":\"16977\"},{\"start\":\"17229\",\"end\":\"17230\"},{\"start\":\"17231\",\"end\":\"17232\"},{\"start\":\"17447\",\"end\":\"17448\"},{\"start\":\"17454\",\"end\":\"17455\"},{\"start\":\"17463\",\"end\":\"17464\"},{\"start\":\"17818\",\"end\":\"17822\"},{\"start\":\"17829\",\"end\":\"17836\"},{\"start\":\"17845\",\"end\":\"17851\"},{\"start\":\"18279\",\"end\":\"18286\"},{\"start\":\"18294\",\"end\":\"18303\"},{\"start\":\"18318\",\"end\":\"18325\"},{\"start\":\"18333\",\"end\":\"18334\"},{\"start\":\"18343\",\"end\":\"18350\"},{\"start\":\"18360\",\"end\":\"18371\"},{\"start\":\"18860\",\"end\":\"18868\"},{\"start\":\"18878\",\"end\":\"18884\"},{\"start\":\"18892\",\"end\":\"18893\"},{\"start\":\"18903\",\"end\":\"18909\"},{\"start\":\"19263\",\"end\":\"19265\"},{\"start\":\"19272\",\"end\":\"19276\"}]", "bib_author_last_name": "[{\"start\":\"13982\",\"end\":\"13989\"},{\"start\":\"13998\",\"end\":\"14008\"},{\"start\":\"14014\",\"end\":\"14018\"},{\"start\":\"14025\",\"end\":\"14030\"},{\"start\":\"14037\",\"end\":\"14046\"},{\"start\":\"14056\",\"end\":\"14061\"},{\"start\":\"14068\",\"end\":\"14074\"},{\"start\":\"14083\",\"end\":\"14088\"},{\"start\":\"14427\",\"end\":\"14432\"},{\"start\":\"14440\",\"end\":\"14447\"},{\"start\":\"14457\",\"end\":\"14462\"},{\"start\":\"14472\",\"end\":\"14477\"},{\"start\":\"14486\",\"end\":\"14493\"},{\"start\":\"14505\",\"end\":\"14514\"},{\"start\":\"14866\",\"end\":\"14868\"},{\"start\":\"14879\",\"end\":\"14881\"},{\"start\":\"14887\",\"end\":\"14889\"},{\"start\":\"14900\",\"end\":\"14904\"},{\"start\":\"14910\",\"end\":\"14915\"},{\"start\":\"14921\",\"end\":\"14924\"},{\"start\":\"15205\",\"end\":\"15208\"},{\"start\":\"15216\",\"end\":\"15221\"},{\"start\":\"15231\",\"end\":\"15234\"},{\"start\":\"15242\",\"end\":\"15248\"},{\"start\":\"15702\",\"end\":\"15708\"},{\"start\":\"15716\",\"end\":\"15721\"},{\"start\":\"15727\",\"end\":\"15731\"},{\"start\":\"16000\",\"end\":\"16002\"},{\"start\":\"16012\",\"end\":\"16020\"},{\"start\":\"16028\",\"end\":\"16034\"},{\"start\":\"16041\",\"end\":\"16049\"},{\"start\":\"16404\",\"end\":\"16408\"},{\"start\":\"16419\",\"end\":\"16423\"},{\"start\":\"16433\",\"end\":\"16437\"},{\"start\":\"16448\",\"end\":\"16453\"},{\"start\":\"16460\",\"end\":\"16462\"},{\"start\":\"16469\",\"end\":\"16472\"},{\"start\":\"16962\",\"end\":\"16970\"},{\"start\":\"16978\",\"end\":\"16983\"},{\"start\":\"17233\",\"end\":\"17235\"},{\"start\":\"17449\",\"end\":\"17452\"},{\"start\":\"17456\",\"end\":\"17461\"},{\"start\":\"17465\",\"end\":\"17469\"},{\"start\":\"17823\",\"end\":\"17827\"},{\"start\":\"17837\",\"end\":\"17843\"},{\"start\":\"17852\",\"end\":\"17863\"},{\"start\":\"18287\",\"end\":\"18292\"},{\"start\":\"18304\",\"end\":\"18316\"},{\"start\":\"18326\",\"end\":\"18331\"},{\"start\":\"18335\",\"end\":\"18341\"},{\"start\":\"18351\",\"end\":\"18358\"},{\"start\":\"18372\",\"end\":\"18382\"},{\"start\":\"18384\",\"end\":\"18393\"},{\"start\":\"18869\",\"end\":\"18876\"},{\"start\":\"18885\",\"end\":\"18890\"},{\"start\":\"18894\",\"end\":\"18901\"},{\"start\":\"18910\",\"end\":\"18919\"},{\"start\":\"19266\",\"end\":\"19270\"},{\"start\":\"19277\",\"end\":\"19284\"}]", "bib_entry": "[{\"start\":\"13908\",\"end\":\"14330\",\"attributes\":{\"matched_paper_id\":\"7731948\",\"id\":\"b0\"}},{\"start\":\"14332\",\"end\":\"14796\",\"attributes\":{\"matched_paper_id\":\"52985787\",\"id\":\"b1\"}},{\"start\":\"14798\",\"end\":\"15130\",\"attributes\":{\"matched_paper_id\":\"52048251\",\"id\":\"b2\"}},{\"start\":\"15132\",\"end\":\"15613\",\"attributes\":{\"matched_paper_id\":\"16224674\",\"id\":\"b3\"}},{\"start\":\"15615\",\"end\":\"15978\",\"attributes\":{\"matched_paper_id\":\"340420\",\"id\":\"b4\"}},{\"start\":\"15980\",\"end\":\"16337\",\"attributes\":{\"matched_paper_id\":\"54465873\",\"id\":\"b5\"}},{\"start\":\"16339\",\"end\":\"16854\",\"attributes\":{\"matched_paper_id\":\"4703058\",\"id\":\"b6\"}},{\"start\":\"16856\",\"end\":\"17172\",\"attributes\":{\"matched_paper_id\":\"6531073\",\"id\":\"b7\"}},{\"start\":\"17174\",\"end\":\"17372\",\"attributes\":{\"id\":\"b8\"}},{\"start\":\"17374\",\"end\":\"17699\",\"attributes\":{\"id\":\"b9\"}},{\"start\":\"17701\",\"end\":\"18188\",\"attributes\":{\"matched_paper_id\":\"549946\",\"id\":\"b10\"}},{\"start\":\"18190\",\"end\":\"18796\",\"attributes\":{\"matched_paper_id\":\"59222289\",\"id\":\"b11\"}},{\"start\":\"18798\",\"end\":\"19200\",\"attributes\":{\"matched_paper_id\":\"2845360\",\"id\":\"b12\"}},{\"start\":\"19202\",\"end\":\"19491\",\"attributes\":{\"matched_paper_id\":\"3532973\",\"id\":\"b13\"}}]", "bib_title": "[{\"start\":\"13908\",\"end\":\"13974\"},{\"start\":\"14332\",\"end\":\"14417\"},{\"start\":\"14798\",\"end\":\"14854\"},{\"start\":\"15132\",\"end\":\"15199\"},{\"start\":\"15615\",\"end\":\"15690\"},{\"start\":\"15980\",\"end\":\"15990\"},{\"start\":\"16339\",\"end\":\"16396\"},{\"start\":\"16856\",\"end\":\"16954\"},{\"start\":\"17174\",\"end\":\"17227\"},{\"start\":\"17374\",\"end\":\"17445\"},{\"start\":\"17701\",\"end\":\"17816\"},{\"start\":\"18190\",\"end\":\"18277\"},{\"start\":\"18798\",\"end\":\"18858\"},{\"start\":\"19202\",\"end\":\"19261\"}]", "bib_author": "[{\"start\":\"13976\",\"end\":\"13991\"},{\"start\":\"13991\",\"end\":\"14010\"},{\"start\":\"14010\",\"end\":\"14020\"},{\"start\":\"14020\",\"end\":\"14032\"},{\"start\":\"14032\",\"end\":\"14048\"},{\"start\":\"14048\",\"end\":\"14063\"},{\"start\":\"14063\",\"end\":\"14076\"},{\"start\":\"14076\",\"end\":\"14090\"},{\"start\":\"14419\",\"end\":\"14434\"},{\"start\":\"14434\",\"end\":\"14449\"},{\"start\":\"14449\",\"end\":\"14464\"},{\"start\":\"14464\",\"end\":\"14479\"},{\"start\":\"14479\",\"end\":\"14495\"},{\"start\":\"14495\",\"end\":\"14516\"},{\"start\":\"14856\",\"end\":\"14870\"},{\"start\":\"14870\",\"end\":\"14883\"},{\"start\":\"14883\",\"end\":\"14891\"},{\"start\":\"14891\",\"end\":\"14906\"},{\"start\":\"14906\",\"end\":\"14917\"},{\"start\":\"14917\",\"end\":\"14926\"},{\"start\":\"15201\",\"end\":\"15210\"},{\"start\":\"15210\",\"end\":\"15223\"},{\"start\":\"15223\",\"end\":\"15236\"},{\"start\":\"15236\",\"end\":\"15250\"},{\"start\":\"15692\",\"end\":\"15710\"},{\"start\":\"15710\",\"end\":\"15723\"},{\"start\":\"15723\",\"end\":\"15733\"},{\"start\":\"15992\",\"end\":\"16004\"},{\"start\":\"16004\",\"end\":\"16022\"},{\"start\":\"16022\",\"end\":\"16036\"},{\"start\":\"16036\",\"end\":\"16051\"},{\"start\":\"16398\",\"end\":\"16410\"},{\"start\":\"16410\",\"end\":\"16425\"},{\"start\":\"16425\",\"end\":\"16439\"},{\"start\":\"16439\",\"end\":\"16455\"},{\"start\":\"16455\",\"end\":\"16464\"},{\"start\":\"16464\",\"end\":\"16474\"},{\"start\":\"16956\",\"end\":\"16972\"},{\"start\":\"16972\",\"end\":\"16985\"},{\"start\":\"17229\",\"end\":\"17237\"},{\"start\":\"17447\",\"end\":\"17454\"},{\"start\":\"17454\",\"end\":\"17463\"},{\"start\":\"17463\",\"end\":\"17471\"},{\"start\":\"17818\",\"end\":\"17829\"},{\"start\":\"17829\",\"end\":\"17845\"},{\"start\":\"17845\",\"end\":\"17865\"},{\"start\":\"18279\",\"end\":\"18294\"},{\"start\":\"18294\",\"end\":\"18318\"},{\"start\":\"18318\",\"end\":\"18333\"},{\"start\":\"18333\",\"end\":\"18343\"},{\"start\":\"18343\",\"end\":\"18360\"},{\"start\":\"18360\",\"end\":\"18384\"},{\"start\":\"18384\",\"end\":\"18395\"},{\"start\":\"18860\",\"end\":\"18878\"},{\"start\":\"18878\",\"end\":\"18892\"},{\"start\":\"18892\",\"end\":\"18903\"},{\"start\":\"18903\",\"end\":\"18921\"},{\"start\":\"19263\",\"end\":\"19272\"},{\"start\":\"19272\",\"end\":\"19286\"}]", "bib_venue": "[{\"start\":\"15329\",\"end\":\"15391\"},{\"start\":\"16120\",\"end\":\"16172\"},{\"start\":\"16553\",\"end\":\"16615\"},{\"start\":\"18460\",\"end\":\"18508\"},{\"start\":\"14090\",\"end\":\"14099\"},{\"start\":\"14516\",\"end\":\"14533\"},{\"start\":\"14926\",\"end\":\"14945\"},{\"start\":\"15250\",\"end\":\"15327\"},{\"start\":\"15733\",\"end\":\"15782\"},{\"start\":\"16051\",\"end\":\"16118\"},{\"start\":\"16474\",\"end\":\"16551\"},{\"start\":\"16985\",\"end\":\"16992\"},{\"start\":\"17237\",\"end\":\"17259\"},{\"start\":\"17471\",\"end\":\"17515\"},{\"start\":\"17865\",\"end\":\"17926\"},{\"start\":\"18395\",\"end\":\"18458\"},{\"start\":\"18921\",\"end\":\"18984\"},{\"start\":\"19286\",\"end\":\"19303\"}]"}}}, "year": 2023, "month": 12, "day": 17}