{"id": 202782600, "updated": "2023-10-06 22:59:45.703", "metadata": {"title": "An Information-Theoretic Approach to Transferability in Task Transfer Learning", "authors": "[{\"first\":\"Yajie\",\"last\":\"Bao\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Shao-Lun\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Lin\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lizhong\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Amir\",\"last\":\"Zamir\",\"middle\":[]},{\"first\":\"Leonidas\",\"last\":\"Guibas\",\"middle\":[]}]", "venue": "2019 IEEE International Conference on Image Processing (ICIP) (pp. 2309-2313). IEEE", "journal": "2019 IEEE International Conference on Image Processing (ICIP)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Task transfer learning is a popular technique in image processing applications that uses pre-trained models to reduce the supervision cost of related tasks. An important question is to determine task transferability, i.e. given a common input domain, estimating to what extent representations learned from a source task can help in learning a target task. Typically, transferability is either measured experimentally or inferred through task relatedness, which is often defined without a clear operational meaning. In this paper, we present a novel metric, H-score, an easily-computable evaluation function that estimates the performance of transferred representations from one task to another in classification problems using statistical and information theoretic principles. Experiments on real image data show that our metric is not only consistent with the empirical transferability measurement, but also useful to practitioners in applications such as source model selection and task transfer curriculum learning.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.10082", "mag": "2970677506", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2212-10082", "doi": "10.1109/icip.2019.8803726"}}, "content": {"source": {"pdf_hash": "22cd6e98f4b6df9384560336d47318a28736db57", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.10082v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bed661180d02c1855da31de253763f6474de8317", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/22cd6e98f4b6df9384560336d47318a28736db57.txt", "contents": "\nAN INFORMATION-THEORETIC APPROACH TO TRANSFERABILITY IN TASK TRANSFER LEARNING\n\n\nYajie Bao \nTsinghua-Berkeley Shenzhen Institute\n\n\nYang Li \nTsinghua-Berkeley Shenzhen Institute\n\n\nShao-Lun Huang \nTsinghua-Berkeley Shenzhen Institute\n\n\nLin Zhang \nTsinghua-Berkeley Shenzhen Institute\n\n\nLizhong Zheng \nMassachusetts Institute of Technology\n\n\nAmir Zamir \nStanford University\n\n\nUniversity of California\nBerkeley\n\nLeonidas Guibas \nStanford University\n\n\nAN INFORMATION-THEORETIC APPROACH TO TRANSFERABILITY IN TASK TRANSFER LEARNING\nIndex Terms-Task transfer learningTransferabilityH- ScoreImage recognition & classification\nTask transfer learning is a popular technique in image processing applications that uses pre-trained models to reduce the supervision cost of related tasks. An important question is to determine task transferability, i.e. given a common input domain, estimating to what extent representations learned from a source task can help in learning a target task. Typically, transferability is either measured experimentally or inferred through task relatedness, which is often defined without a clear operational meaning. In this paper, we present a novel metric, H-score, an easily-computable evaluation function that estimates the performance of transferred representations from one task to another in classification problems using statistical and information theoretic principles. Experiments on real image data show that our metric is not only consistent with the empirical transferability measurement, but also useful to practitioners in applications such as source model selection and task transfer curriculum learning.\n\nIntroduction\n\nTransfer learning is a learning paradigm that exploits the relatedness between different learning tasks in order to gain certain benefits, e.g. reducing the demand for supervision ([1]). In task transfer learning, we assume that the input domain of the different tasks are the same. Then for a target task T T , instead of learning a model from scratch, we can initialize the parameters from a previously trained model for some related source task T S (Figure 1). For example, deep convolutional neural networks trained for the ImageNet classification task have been used as the source network in transfer learning for target tasks with fewer labeled data [2], such as medical image analysis [3] and structural damage recognition in buildings [4]. An imperative question in task transfer learning is transferability, i.e. when a transfer may work and to what extent. Given a metric capable of efficiently and accurately measuring transferability across arbitrary tasks, the problem of task transfer learning, to a large extent, is simplified to search procedures over potential transfer sources and targets as quantified by the metric. Traditionally, transferability is measured purely empirically using model loss or accuracy on the validation set ([5, 6, 7]). There have been theoretical studies that focus on task relatedness ([8, 9, 10, 11]). However, they either cannot be computed explicitly from data or do not directly explain task transfer performance. In this study, we aim to estimate transferability analytically, directly from the training data.\n\nWe quantify the transferability of feature representations across tasks via an approach grounded in statistics and information theory. The key idea of our method is to show that the expected log-loss of using a feature of the input data to predict the label of a given task under the probabilistic model can be characterized by an analytically expression, which we refer as the H-score of the feature. H-score is particularly useful to quantify feature transferability among tasks. Using this idea, we define task transferability as the normalized H-score of the optimal source task feature with respect to the target task.\n\nAs we demonstrate in this paper, the advantage of our transferability metric is threefold. (i) it is theoretically driven and has a strong operational meaning rooted in statistics and information theory; (ii) it can be computed directly and efficiently from the input data, with fewer samples than those needed for empirical learning; (iii) it can be shown to be strongly consistent with empirical transferability measurements.\n\n\nMeasuring Feature Effectiveness\n\nLet X and Y denote the input and output space respectively. Denote the transferred feature representation by f : X \u2192 R k .\n\nFor a classification task, let h f : X \u00d7 Y \u2192 [0, 1] |Y| be a predictor function with the log-loss function L(f (x), y) for a given (x, y) sample. The traditional machine learning approach uses stochastic gradient descent to minimize L(h) = E X,Y [L(f (x), y)]. We will show that the optimal log loss when f is given can be characterized analytically using concepts in information theory and statistics.\n\nDefinition 1. The Divergence Transition Matrix (DTM) of discrete random variables X and Y is a |Y| by |X | matrixB with entriesB y,x =\nP XY (x,y) \u221a P X (x) \u221a P Y (y) \u2212 P Y (y) P X (x) for all x \u2208 X and y \u2208 Y. Given m training examples {(x (i) , y (i) )} m i=1 , L(h) can be written as L(f, \u03b8) = \u2212 1 m m i=1 |Y| k=1 1{y (i) = k} log e \u2212\u03b8 T k f (x (i) ) |Y| j=1 e \u2212\u03b8 T j f (x (i) )\nUsing concepts in Euclidean information geometry, it is shown in [12] that under a local assumption, for a given feature dimension k,\nargmin f,\u03b8 L(f, \u03b8) = argmin \u03a8\u2208R |X |\u00d7k ,\u03a6\u2208R |Y|\u00d7k 1 2 B \u2212\u03a8\u03a6 T 2 F +o( 2 ) (1) Let \u03c6(x) represent row vectors of \u03a6 for any x \u2208 X . By defin- ing a one-to-one mapping f (x) \u2194 \u03c6(x) such that \u03c6(x) = P X (x)f (x), Eq.\n(1) reveals a close connection between the optimal log-loss and the modal decomposition ofB. In consequence, it is reasonable to measure the classification performance with B \u2212 \u03a8\u03a6 T 2 F given f (X). i.e. Since \u03a6 is fixed, we can find the optimal \u03a8 , \u03a8 * by taking the derivative of the objective function with respect to \u03a8:\n\u03a8 * =B\u03a6(\u03a6 T \u03a6) \u22121\n(2) Substituting (2) in the Objective of (1), we can derive the following close-form solution for the log loss:\nB 2 F \u2212 B \u03a6(\u03a6 T \u03a6) \u2212 1 2 2 F\n(3) The first term in (3) does not depend on f (X), therefore it is sufficient to use the second term to estimate classification performance with transferred feature f . We can further rewrite ||B\u03a6(\u03a6 T \u03a6) \u2212 1 2 || 2 F as follows and denote it as the H-score.\n\nDefinition 2. Given data matrix X \u2208 R m\u00d7d and label Y , let f (X) be a k-dim, zero-mean feature function. The H-Score of f with respect to a task with joint probability P Y X is:\nH(f ) = tr(cov(f (X)) \u22121 cov(E P X|Y [f (X)|Y ])) (4)\nThe derivation of (4) can be found in Section 1 of the Supplementary Material 1 . This formulation can be intuitively interpreted from a nearest neighbor perspective. i.e. a high H-score implies the inter-class variance cov(E P X|Y [f (X)|Y ]) of f is large, while feature redundancy tr(cov(f (X))) is small. Comparing to finding the optimal log-loss through gradient descent, H-score can be computed analytically and only requires estimating the conditional expectation E[f (X)|Y ] from sample data. Moreover, H(f ) has an operational meaning that characterizes the asymptotic error probability of using f (X) to estimate Y in the hypothesis testing context. (See Section 2 in the Supplementary Material for details).\n\nThe upper bound of H(f ) is obvious from its first definition:\nmax \u03a6 ||B\u03a6(\u03a6 T \u03a6) \u2212 1 2 || 2 F = ||B|| 2 F .\nWe call features that achieve this bound the minimum error probability features for a given task.\n\n\nTransferability\n\nNext, we apply H-score to efficiently measure the effectiveness of task transfer learning. We will use subscripts S and T to distinguish variables for the source and the target tasks.\n\nDefinition 3 (Task transferability). Given source task T S , target task T T and pre-trained source feature f S (x), the transfer-\nability from T S to T T is T(S, T ) H T (f S ) H T (f T opt ) , where f Topt (x)\nis the minimum error probability feature of the target task.\n\nThis definition implies 0 \u2264 T(S, T ) \u2264 1. With a known f , computing H-score from m sample data only takes O(mk 2 ) time, where k is the dimension of f (x) for k < m. The majority of the computation time is spent on computing the sample covariance matrix cov(f (X)).\n\nThe remaining question is how to obtain H T (f Topt ) efficiently. This question has been addressed in [13], which shows that ||B T || 2\nF = E[f (X) T g(Y )\n], where f and g are the solutions of the HGR-Maximum Correlation problem.\n\n\u03c1(X; Y ) = sup\nf : X \u2192 R k , g : Y \u2192 R k E[f (X)] = E[g(Y )] = 0 E[f (X)f (X) T ] = I E[f (X) T g(Y )] (5)\nEq. (6) can be solved efficiently using the Alternating Conditional Expectation (ACE) algorithm [14] for discrete X , or using the neural network approach based on Generalized Maximal HGR Correlation [15] for a generic X . The sample complexity of ACE is only 1/k of the complexity of estimating P Y X directly [16]. This result also applies to the Generalized HGR problem due to their theoretical equivalence. A common technique in task transfer learning is finetuning, which adds before the target classifier additional free layers, whose parameters are optimized with respect to the target label. For the operational meaning of transferability to hold exactly, we require the fine tuning layers consist of only linear transformations. Nevertheless, later we will demonstrate empirically that this transferability metric can still be used for comparing the relative task transferability with fine-tuning. In many cases though, the computation of H T (f opt ) can even be skipped entirely, such as the problem below: Definition 4 (Source task selection). Given N source tasks T S1 , . . . , T S N with labels Y S1 , . . . , Y S N and a target task T T with label Y T . Let f S1 , . . . , f S N be optimal representations for the source tasks. Find the source task T Si that maximizes the testing accuracy of predicting Y T with feature f Si .\n\nWe can solve this problem by selecting the source task with the largest transferability to T T . In fact, we only need to compute the numerator in the transferability definition since the denominator is the same for all source tasks, i.e. argmax i T(S i , T ) = argmax i H(f Si ).\n\n\nExperiments\n\nIn this section, we present validation results and potential application of our transferability metric on real image data. (For implementation details, see Section 3 of the Supplementary Material.)\n\n\nValidation of transfer performance\n\nWe validate H-score and transferability definitions in a transfer learning problem from ImageNet 1000-class classification (ImageNet-1000) to Cifar 100-class classification (Cifar-100). Figure S2.a compares the H-score and empirical performance of transferring from five different layers (4a-4f) of the ResNet-50 model pretrained on ImageNet1000. As H-score increases, log-loss of the target network decreases almost linearly while the training and testing accuracy increase, which validates the relationship between the expected log-loss and H-score. The training and testing accuracy are also positively correlated with H-score. It also shows that H-score can be applied for selecting the most suitable layer for fine-tuning in transfer learning.\n\n(a) (b) Fig. 2: H-score and transferability vs. the empirical transfer performance measured by log-loss, training and testing accuracy. a.) Performance of ImageNet-1000 features from layers 4a-4f for Cifar-100 classification. b.): Transferability from ImageNet-1000 to 4 different target tasks based on Cifar-100.\n\nWe further tested our transferability metric for selecting the best target task for a given source task. In particular, we constructed 4 target classification tasks with 3, 5, 10, and 20 object categories from the Cifar-100 dataset. We then computed the task transferability from ImageNet-1000 (using the feature representation of layer 4f) to the target tasks. In Figure2.b, we observe a similar behavior as the H-score in the case of a single target task in Figure 2.a, showing that transferability can directly predict the empirical transfer performance.\n\n\nTask transfer for 3D scene understanding\n\nNext, we apply our transferability metric to solve the source task selection problem among 8 image-based recognition tasks for 3D scene understanding using the Taskonomy dataset [6]. We also compared the task transferability ranking based on H-score with the ranking using task affinity, an empirical transferability metric proposed by [6] with non-linear fine tuning.\n\nFor a fair comparison, we use the same trained encoders in [6] to extract source features with dimension k = 2048. It's worth noting that, six of eight tasks have images as their output. To compute the transferability for these pixel-to-pixel tasks, we cluster the pixel values of the output images in the training data into a palette of 16 colors and then compute the H-score of the source features with respect to each pixel. The transferability of the task is computed as the average of the H-scores over all pixels. For larger images, H-score can be evaluated on super pixels instead of pixels to improve efficiency.  Fig. 3: Ranking comparison between transferability and affinity score.\n\nPairwise Transfer Results. Source task ranking results using transferability and affinity are visualized side by side in Figure 3, with columns representing source tasks and rows representing target tasks. For classification tasks (the bottom two rows in the transferability matrix), the top two transferable source tasks are identical for both methods. Similar observations can be found in 2D pixel-to-pixel tasks (top two rows). A slightly larger difference between the two rankings can be found in 3D pixel-to-pixel tasks, especially 3D Occlusion Edges and 3D Keypoints. Though the top four ranked tasks of both methods are exactly the four 3D tasks. It could indicate that these low level vision tasks are closely related to each other so that the transferability among them are inherently ambiguous. We also computed the ranking correlations between transferability and affinity using Spearman's R and Discounted Cumulative Gain (DCG). Both criterion show positive correlations for all target tasks. The correlation is especially strong with DCG as higher ranking entities are given larger weights.\n\nTo show the task relatedness, we represent each task with a vector consisting of H-scores of all the source tasks for the given task, then apply agglomerative clustering over the task vectors. As shown in the dendrogram in Figure 3, 2D tasks and most 3D tasks are grouped into different clusters, but on a higher level, all pixel-to-pixel tasks are considered one category compared to the classifications tasks. Higher Order Transfer. A common way for higher order transfer is to concatenate features from multiple models in deep neural networks. Our transferability definition can be easily adapted to such problems. Figure 4 shows the ranking results of all combinations of source task pairs for each target task. For all tasks except for Edge3D and Depth, the best seond-order source feature is the combination of the top two tasks of the first-order ranking. We examine the exception in Figure 5, by visualizing the pixel-by-pixel H-scores of first and second order transfers to Depth using a heatmap (lighter color implies a higher H-score). Note that different source tasks can be good at predicting different parts of the image. The top row shows the results of combining tasks with two different \"transferability patterns\" while the bottom row shows those with similar patterns. Combining tasks with different transferability patterns has a more significant improvement to the overall performance of the target task.\n\n\nTask transfer learning curriculum\n\nA potential application of our transferability metric is developing an optimal task transfer curriculum, a directed acyclic graph over tasks that specifies the order in which to obtain labeled data for each task. For each task in the curriculum, an optimal feature representation can be learned using both its raw input and the representations of its parent tasks to improve  training efficiency. We use a heuristic based on the minimum spanning tree of a task graph, whose edge weights are inversely correlated with the larger transferability score between two tasks. Fig. 7.a shows the task curriculum for the eight tasks in Section 4.2. Furthermore, we did a similar experiments on a collection of binary object classification tasks using the NUS-WIDE multi-label dataset [17] (Fig. 7.b). We set a threshold to find the most salient transfers and the resulting curriculum is in line with human perception.\n\n\nConclusion\n\nIn this paper, we presented H-score, an information theoretic approach to estimating the performance of features when transferred across classification tasks. Then we used it to define a notion of task transferability in multi-task transfer learning problems, that is both time and sample complexity efficient. Our transferability score successfully predicted the performance for transfering features from ImageNet-1000 classification task to Cifar-100 task. Moreover, we showed how the transferability metric can be applied to a set of diverse computer vision and image-based recognition tasks using the Taskonomy and NUS-WIDE datasets. In future works, we will investigate properties of higher order transferability, developing more scalable algorithms that avoid computing the H-score of all task pairs. We also hope to design better task curriculum for task transfer learning in practical applications.\n\n(\"TRI\") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.\n\n\nReferences\n\n[1] Lorien Y Pratt, \"Discriminability-based transfer between neural networks,\" in Advances in neural information processing systems, 1993, pp. 204-211.\n\n[2] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell, \"Decaf: A deep convolutional activation feature for generic visual recognition,\" in International conference on machine learning, 2014, pp. 647-655.\n\n[ [4] Yuqing Gao and Khalid M Mosalam, \"Deep transfer learning for image-based structural damage recognition,\" Computer-Aided Civil and Infrastructure Engineering.\n\n[5] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson, \"How transferable are features in deep neural networks?,\" in Advances in neural information processing systems, 2014, pp. 3320-3328.\n\n[6] Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese, \"Taskonomy: Disentangling task transfer learning,\" Computer Vision and Pattern Recognition (CVPR), 2018.\n\n[7] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes, \"Supervised learning of universal sentence representations from natural language inference data,\" arXiv preprint arXiv:1705.02364, 2017.\n\n[ [19] Alex Krizhevsky and Geoffrey Hinton, \"Learning multiple layers of features from tiny images,\" 2009.\n\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, \"Imagenet classification with deep convolutional neural networks,\" in Advances in neural information processing systems, 2012, pp. 1097-1105.\n\n[21] Mei Wang and Weihong Deng, \"Deep visual domain adaptation: A survey,\" Neurocomputing, 2018.\n\n[22] Lisa Torrey and Jude Shavlik, \"Transfer learning,\" in Handbook of research on machine learning applications and trends: algorithms, methods, and techniques, pp. 242-264. IGI Global, 2010.\n\n[23] Edwin V Bonilla, Kian M Chai, and Christopher Williams, \"Multi-task gaussian process prediction,\" in Advances in neural information processing systems, 2008, pp. 153-160.\n\n[24] Yu Zhang, \"Heterogeneous-neighborhood-based multitask local learning algorithms,\" in Advances in neural information processing systems, 2013, pp. 1896-1904.\n\n[25] Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krishnapuram, \"Multi-task learning for classification with dirichlet process priors,\" Journal of Machine Learning Research, vol. 8, no. Jan, pp. 35-63, 2007.\n\n[26] Laurent Jacob, Jean-philippe Vert, and Francis R Bach, \"Clustered multi-task learning: A convex formulation,\" in Advances in neural information processing systems, 2009, pp. 745-752.\n\n[27] Hanchuan Peng, Fuhui Long, and Chris Ding, \"Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy,\" IEEE Transactions on pattern analysis and machine intelligence, vol. 27, no. 8, pp. 1226-1238, 2005.\n\n[28] Mark Andrew Hall, \"Correlation-based feature selection for machine learning,\" 1999.\n\n[29] Xuejun Liao and Lawrence Carin, \"Radial basis function network for multi-task learning,\" in Advances in Neural Information Processing Systems, 2006, pp. 792-802.\n\n[30] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil, \"Multi-task feature learning,\" in Advances in neural information processing systems, 2007, pp. 41-48.\n\nSupplementary Material S1 Derivation of Equation (4) First, we need to introduce some additional notations. Let X, x, X and P X represent a random variable, a value, the alphabet and the probability distribution respectively. \u221a P X denotes the vector with entries P X (x) and [ \u221a P X ] \u2208 R |X |\u00d7|X | denotes the diagonal matrix of \u221a P X . For joint distribution P Y X , P Y X \u2208 R |Y|\u00d7|X | represents the probability matrix. Given k feature functions f i : X \u2192 R, i = 1, ..., k, let f (x) = [f 1 (x), ..., f k (x)] \u2208 R k be the feature vector of x, and F = [f (x 1 ) T , ..., f (x |X | ) T ] T \u2208 R |X |\u00d7k be the feature matrix over all elements in X .\n\nThe left hand side of Equation (4) can be expressed as\n||B\u03a6(\u03a6 T \u03a6) \u2212 1 2 || 2 F =tr (\u03a6 T \u03a6) \u2212 1 2 \u03a6 TBTB \u03a6(\u03a6 T \u03a6) \u2212 1 2 =tr (\u03a6 T \u03a6) \u22121 \u03a6 TBTB \u03a6(6)\nSince any feature function can be centered by subtracting the mean, without the loss of generality, we assume E[f (X)] = 0.\n\nUsing the one-to-one correspondence between \u03a6 and F , i.e. \u03a6 = \u221a P X F \u2208 R |X |\u00d7k , we have\n\u03a6 T \u03a6 = P X F T P X F = E[f (X) T f (X)]\n= cov(f (X)) (7) The DTM matrixB introduced in Definition 1 can be written in matrix notation:\nB = \u221a P Y \u22121 P Y X \u221a P X \u22121 \u2212 \u221a P Y \u221a P X T . Then we have, B\u03a6 = P Y \u22121 P Y X P X \u22121 \u2212 P Y P X T \u00b7 P X F = P Y [P Y ] \u22121 P Y X F \u2212 1 \u00b7 E[f (X)] T ,\nwhere 1 is a column vector with all entries 1 and length |Y|. It follows that,\n\u03a6 TBTB \u03a6 = [P Y ] \u22121 P Y X F \u2212 1 \u00b7 E[f (X)] T T \u00b7 [P Y ] [P Y ] \u22121 P Y X F \u2212 1 \u00b7 E[f (X)] T =E P Y (E[f (X)|Y ] \u2212 1 \u00b7 E[f (X)] T ) T \u00b7 (E[f (X)|Y ] \u2212 1 \u00b7 E[f (X)] T )\n=cov (E[f (X)|Y ]) (8) By substituting (7) and (8) \ninto (6), we have ||B\u03a6(\u03a6 T \u03a6) \u2212 1 2 || 2 F = tr cov(f (X)) \u22121 cov(E[f (X)|Y ])\n\nS2 Operational Meaning of H-Score\n\nIn this section, we will show that H-score characterizes the asymptotic probability of error in the hypothesis testing context. We will start with some background on error exponents from statistics, then explain how to estimate it using informa- Fig. S1: The binary hypothesis testing problem. The blue curves shows the probility density functions for P 1 and P 2 . The rejection region A c and the acceptance region A are highlighted in red and blue, respectively. The vertical line indicates the decision threshold.\nH 0 : P 1 H 1 : P 2 \u03b2 = P 2 (A) \u03b1 = P 1 (A c ) A: fail to reject H 0 A c :reject H 0 P x m : D(P x m P 2 ) \u2212 D(P x m P 1 ) = log T m\ntion geometry. Finally, we will show how computing H-score is in fact estimating the error exponent of a feature function on the sample data.\n\n\nS2.1 Error Exponent and Hypothesis Testing\n\nConsider the binary hypothesis testing problem over m i.i.d.\nsampled observations {x (i) } m i=1\nx m with the following hypotheses:\nH 0 : x m \u223c P 1 or H 1 : x m \u223c P 2 .\nLet P x m be the empirical distribution of the samples. The optimal test, i.e., the log likelihood ratio test log(T ) = log P1(x m ) P2(x m ) can be stated in terms of information-theoretic quantities as follows:\nm[D(P x m ||P 2 ) \u2212 D(P x m ||P 1 )] H0 \u2277 H1 log T\nwhere D is the Kullback-Leibler (KL) divergence operator.\n\nFurther, using Sannov's theorem, we have the asymptotic probability of type I error:\n\u03b1 = P 1 (A c ) \u2248 2 \u2212mD(P * 1 ||P1)\nwhere P * 1 = argmin P \u2208A c D(P ||P 1 ) and A c (T ) = {x m : D(P x m ||P 2 ) \u2212 D(P x m ||P 1 ) < 1 m log T } represents the rejection region. Similarly, the asymptotic probability of type II error is\n\u03b2 = P 2 (A) \u2248 2 \u2212mD(P * 2 ||P2) ,\nwhere P * 2 = argmin P \u2208A D(P ||P 2 ) and A = {x m : D(P x m ||P 2 ) \u2212 D(P x m ||P 1 ) > 1 m log T } represents the acceptance region (See Figure S1). Using the Bayesian approach for hypothesis testing, the overall error probability of the log likelihood ratio test is defined as: P (m) e = \u03b1P 1 + \u03b2P 2 and the best achievable exponent in the Bayesian probability of error (a.k.a. error exponent) is defined as:\nE = lim m\u2192\u221e min A\u2286X m \u2212 1 m log P (m) e\nError exponent E expresses the best rate at which the error probability decays as sample size increases for a particular hypothesis testing problem. See [18] for more background information on error exponents and its related theorems.\n\n\nS2.2 Estimating Error Exponents\n\nSuppose P 1 and P 2 are sampled from the -neighborhood\nN (P 0 ) {P | x\u2208X (P (x)\u2212P0(x)) 2 P0(x) \u2264 2 } centered at a ref-\nerence distributon P 0 , and let \u03c6 1 , \u03c6 2 \u2208 R |X | be vectors defined as:\n\u03c6 i (x) P i (x) \u2212 P 0 (x) P 0 (x)\nfor i = 1, 2. The following lemma express the optimal error exponent E using \u03c6 1 and \u03c6 2 :\n\nLemma 1. Under the local assumption defined earlier, the best achievable error exponent of the binary hypothesis testing problem with probabilities P 1 and P 2 is:\nE = 2 8 \u03c6 1 \u2212 \u03c6 2 2 + o( 2 )\nwhere is a constant [16].\n\nWhile the above lemma characterizes the asymptotic error probability distinguishing P 1 and P 2 based on the optimal decision function, most decision functions we learn from data are not optimal, as P 1 and P 2 are unknown. Given sample data x m and an arbitrary feature function f : X \u2192 R, which could be learned from a pre-trained model, the error exponent of the decision function based on f is reduced in a way defined by the following Lemma:\n\nLemma 2. Given a zero-mean, unit variance feature function f : X \u2192 R and i.i.d. sampled data x m , the error probability of a mismatched decision function of the form l = 1 m m i=1 (f (x (i) )) has an exponent\nE f = 2 8 \u03be, \u03c6 1 \u2212 \u03c6 2 2 + o( 2 ) where \u03be \u2208 R |X | is a vector with entries \u03be(x) = P 0 (x)f (x) [16].\nThis lemma characterizes the error probability of using a normalized feature of the input data to solve a learning task by a linear projection of this feature between the input and output domains. Note that normalizing features to zero-mean and unit variance results in an equivalent decision function with a different threshold value, thus we can apply Lemma 2 to any features without the loss of generality. Further, it's obvious that the reduced exponent E f is maximized when \u03be = \u03c6 1 \u2212 \u03c6 2 , and the optimal value is exactly the optimal error exponent E in Lemma 1. To estimate the reduced error exponent for multi-dimensional features, we present the kdimensional generalization of Lemma 2 below:\n\nLemma 3. Given k normalized feature functions f (x) = [f 1 (x), . . . , f k (x)], such that E[f i (X)] = 0 for all i, and cov(f (X)) = I , we define a k-d statistics of the form l k = (l 1 , ..., l k ) where l i = 1 m m l=1 f i (x (l) ). Let \u03be 1 , . . . , \u03be k be k vectors with entries \u03be i (x) = P X (x)f i (x) , 0 \u2264 i \u2264 k. Then the error exponent of l k is\nE k f = k i=1 E fi = k i=l 2 8 \u03be i , \u03c6 1 \u2212 \u03c6 2 2 + o( 2 )(9)\nThe proof of this Lemma can be found in [12].\n\n\nS2.3 H-score and Error Exponents\n\nNow we return to the binary classification problem. Using Lemma 3, we will show the linear relationship between Hscore and error exponents.\n\nTheorem 1. Given P X|Y =0 , P X|Y =1 \u2208 N X (P 0,X ) and fea-\ntures f such that E [f (X)] = 0 and E[f (X)f (X) T ] = I, there exists some constant c independent of f such that E k f = cH(f ).\nProof. By Lemma 3, the L.H.S. of the equation can be written\nas E k f = c 0 k i=l \u03be i , \u03c6 1 \u2212 \u03c6 2 2 for some constant c 0 . It follows that c 0 k i=l \u03be i , \u03c6 1 \u2212 \u03c6 2 2 =c 0 P X|Y =0 \u2212 P X|Y =1 T F P X|Y =0 \u2212 P X|Y =1 T F T =c 0 (E[f (X)|Y = 0] \u2212 E[f (X)|Y = 1]) T \u00b7 (E[f (X)|Y = 0] \u2212 E[f (X)|Y = 1]) =c 0 P Y (0) + P Y (1) P Y (0)P Y (1) P Y (0)P Y (1) + P Y (1) 2 P Y (0) \u00b7 E [f (X)|Y = 1] T E [f (X)|Y = 1] =c tr (cov(E[f (X)|Y ]))\n=c H(f ) The last equation uses the fact cov(f (X)) = I.\n\n\nS3 Experiment Details\n\nS3.1 Experiment 4.1 Experiment Setup. The training data for the target task in this experiment consists of 20, 000 images randomly sampled from the Cifar-100 dataset [19]. It is further split 9:1 into a training set and a testing set.\n\nWe first extracted features of the Cifar-100 training images from five different layers (4a -4f) of the ResNet-50 model pretrained on ImageNet-1000 [20]. Then we computed the H-score and the empirical transfer performance of each feature function for the Cifar-100 task. To compute the empirical performance, we trained the transfer network using stochastic gradient descent with batch size 20, 000 for 100 epochs. Result Discussion. As shown in Fig. 2.a of the main paper, transfer performance is better when an upper layer of the source networks is transferred. This could be due to the inherent similarity between the target task and the source task, such that the optimal representation learned for one task can still be suitable for the other. For the experiment of selecting the best target task (Fig. 2.b), we used the same network as the former experiment to compute the empirical transfer performance with batch size 64 for 50 epochs.\n\nIn addition, we validated H-score under different target sample sizes between 5-50K. Fig. S2 shows that target sample size does not affect the relationship between H-score and logloss, which further demonstrates that the H-score computation is sample efficient.\n\n\nS3.2 Experiment 4.2\n\nData and Tasks.\n\nThe Taskonomy dataset [6] contains 4,000,000 images of indoor scenes collected from 600 buildings. Every image has annotations for 26 computer vision tasks. For the transferability experiment, we randomly sampled 20, 000 images as the target task training data, and selected eight supervised tasks, shown in Table S1.  [6] trained a fully supervised network with an encoder-decoder structure. When testing the transfer performance from source task T S to target task T T , the encoder output of T S is used for training the decoder of T T . For a fair comparison, we used the same trained encoders to extract source features. The output dimension of all encoders are 16 \u00d7 16 \u00d7 8 and we flattened the output into a vector of length 2048. To reduce the computational complexity, we also resized the ground truth images into 64 \u00d7 64. Label Quantization. Fig. S3 illustrates the resizing and quantization process of a pixel-to-pixel task label. During the quantization process, we are primarily concerned with two factors: computational complexity and information loss. Too much information loss will lead to bad approximation of the original problem. On the other hand, having little information loss requires larger label space (cluster size) and higher computation cost. To test the sensitivity of the cluster size, we use cluster centroids to recover the ground truth image pixel-by-pixel. The recovery results for 3D occlusion edge detection is shown in Figure S4. When the cluster size is N = 5 (right), most detected edges in the ground truth image (left) are lost. We found that N = 16 strikes a good balance between recoverability and computation cost. Comparison of H-scores and Affinities. Table S2 presents the numerical values of the transferability and affinity scores between every pair of tasks, with columns representing source tasks and rows representing target tasks. This table is in direct correspondence with the ranking matrices in Fig. 3 of the main paper. For each target task, the upper row shows our results while the lower one shows the results in [6]. Score values are included in parenteses.\n\nHere we present some detailed results on the comparison between H-score and the affinity score in [6] for pairwise transfer. The results of transferring from all tasks to the two classification tasks (Object Class. and Scene Class.) are shown in Figure S5; The results of transferring to Depth is shown in S6. We can see in general, although affinity and transferability have totally different value ranges, they tend to agree on the order of the top few ranked tasks.  Computing Efficiency. We ran the experiment on a workstation with 3.40 GHz \u00d78 CPU and 16 GB memory. Each pairwise H-score computation finished in less than one hour including preprocessing.\n\n\nS3.3 Experiment 4.3\n\nData and Tasks. The NUS-WIDE dataset [17] contains 161,789 web images for training and 107,859 images for evaluation. Its tag set consists of 81 concepts, among which we selected two subsets, shown in Table S3. One subset contains 36 common concepts including scenes, animals and objects; The other subset contains only animal concepts. The NUS-WIDE dataset provides six types of low-level features. In this   Feature Extraction and Data Preprocessing. We consider the prediction of each concept as an unbalanced binary classification task and train a 4-layer fully-connected neural network ( Fig. S7) with batch-size 2048 for 50 epochs. For an arbitrary target task, the layer 3 activation output of the source models are used to calculate the H-scores. Task Transfer Curriculum. Given n tasks T 1 , . . . , T n , first we compute the pairwise transferability matrix M \u2208 R n\u00d7n using H-score, where M (i, j) =\nH T j (f T i )\nH T j (f T j ) for all 1 \u2264 i, j \u2264 n. We assume that the task-specific features are close to optimal, such that H Tj (f Tj ) \u2248 H Tj (f Tj opt ). Then by Definition 3, M (i, j) \u2248 T(T i , T j ). Using the transferability matrix, we define an undirected graph G over the tasks, where the edge weight between node i and node j is defined by\nW (i, j) = \uf8f1 \uf8f2 \uf8f3 1 \u2212 max{M (i, j), M (j, i)} if max{M (i, j), M (j, i)} \u2265 \u03b1 0 otherwise\nParameter \u03b1 defines the threshold to filter out less related task pairs, as the transferred representation in these cases contribute very little to the training of the target task. If G is connected, the minimum spanning tree outputs a set of task pairs that maximizes the total transferability with n \u2212 1 pairwise transfers. If G is not connected, we have a minimum spanning forest that represents several task groups that can be learned independently. Finally, we recover the transfer directions on the minimum spanning tree from the transferability matrix. For the subset with 36 common concepts in the NUS-WIDE experiment, we found that when \u03b1 = 0, i.e. no edges are filtered, 33 of the 35 edges in the resulting tree indicate transfers to concept 'sky' from other concepts. This phenomenon is reasonable in that sky and other concepts coexist in images with a high probability. To demonstrate the most significant task relationships, we set edge threshold \u03b1 to be the 2.3 percentile of all weights, resulting in the minimum spanning tree in Fig. 7 of the main paper. On the other hand, edge filtering is not needed for the Taskonomy tasks and the animal concepts in the NUS-WIDE experiment, since most tasks are transferable to a similar extent. Therefore we chose \u03b1 = 0 in these cases.\n\n\nS4 Related Works\n\nTransfer learning. Transfer learning can be devided into two categories: domain adaptation, where knowledge transfer is achieved by making representations learned from one input domain work on a different input domain, e.g. adapt models for RGB images to infrared images [21]; and task transfer learning, where knowledge is transferred between different tasks on the same input domain [22]. Our paper focus on the latter prolem. Empirical studies on transferability.\n\n[5] compared the transfer accuracy of features from different layers in a neural network between image classification tasks. A similar study was performed for NLP tasks by [7]. [6] determined the optimal transfer hierarchy over a collection of perceptual indoor scene understanidng tasks, while transferability was measured by a non-parameteric score called \"task affinity\" derived from neural network transfer losses coupled with an ordinal normalization scheme. Task relatedness. One approach to define task relatedness is based on task generation. Generalization bounds have been derived for multi-task learning [8], learning-to-learn [9] and life-long learning [10]. Although these studies show theoretical results on transferability, it is hard to infer from data whether the assumptions are satisfied. Another approach is estimating task relatedness from data, either explicitly [23,24] or implicitly as a regularization term on the network weights [25,26]. Most works in this category are limited to shallow ones in terms of the model parameters. Representation learning and evaluation. Selecting optimal features for a given task is traditionally performed via feature subset selection or feature weight learning. Subset selection chooses features with maximal relevance and minimal redundancy according to information theoretic or statistical criteria [27,28]. The feature weight approach learns the task while regularizing feature weights with sparsity constraints, which is common in multi-task learning [29,30]. In a different perspective, [13] consider the universal feature selection problem, which finds the most informative features from data when the exact inference problem is unknown. When the target task is given, the universal feature is equivalent to the minimum error probability feature used in this work.\n\nFig. 1 :\n1A generic model of task transfer learning. The proposed transferability metric T(S, T ) can predict the target task's performance without training the task transfer network.\n\nFig. 4 :\n4Ranking of 2nd-order transferability for all tasksFig. 5: 1st and 2nd order pixel-wise transferability to Depth.\n\nFig. 6 :\n6Minimum spanning tree of task transferability.\n\nFig. 7 :\n7Minimum spanning trees of binary image classification tasks using the NUS-WIDE multi-label dataset.\n\n\n3] C. K. Shie, C. H. Chuang, C. N. Chou, M. H. Wu, and E. Y. Chang, \"Transfer representation learning for medical image analysis,\" in 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Aug 2015, pp. 711-714.\n\nFig\n. S2: H-score and transferability vs. the empirical transfer performance measured by log-loss for different target sample size (5K-50K).\n\nFig. S3 :\nS3Quantization. Recover is done with the centroid of corresponding cluster of each pixel.\n\nFig. S4 :\nS4Effect of quantization cluster size for 3D occlusion Edge detection.\n\nFig. S5 :\nS5Source task transferability ranking for classification tasks. For each target task, the left figure shows H-score results, and the right figure shows task affinity results.\n\nFig. S6 :\nS6Comparison between source task rankings for Depth. with H-score results on the left and affinity scores [6] on the right. The Top 3 transferable source tasks in both methods are the same: Depth, Image Reshading and 3D Occlusion Edges.\n\nFig. S7 :\nS7Network structure for NUS-WIDE experiment.\n\nTable . 1\n.: List of image scene understanding tasksClassification tasks: Object Class., Scene Class. \nPixel-to-pixel tasks: Keypint2D, Edge3D, Keypoint2D, \nEdge2D, Reshading, Depth \n\nTransferability Task Hierarchy \nAffinity \nRanking Correlation \n\nDCG \n\n\n\nTable S1 :\nS1Task descriptionsFeature Extraction and Data Preprocessing. For each task,\n\nTable S2 :\nS2Transferability ranking comparison, between H-score's estimation and task affinity experiment, we used the 500 dimensional bag-of-words feature based on the SIFT descriptors.Tasks \n2D Edges \n2D Keypoints 3D Edges \n3D Keypoints Reshading \nDepth \nObject Class. Scene Class. \n\n2D Edges \n1 (1.8216) \n2 (1.7334) \n5 (1.5704) \n6 (1.5696) \n4 (1.6146) \n3 (1.6201) \n7 (1.5097) \n8 (1.4402) \n\n1 (0.0389) \n2 (0.0117) \n4 (5.8920e-5) 3 (8.8011e-5) 7 (2.9001e-5) 8 (2.2110e-5) 5 (4.9141e-5) 6 (4.8720e-5) \n\n2D Keypoints \n2 (1.6698) \n1 (1.7859) \n7 (1.5248) \n5 (1.5287) \n4 (1.5481) \n3 (1.5632) \n6 (1.5253) \n8 (1.4725) \n\n2 (0.0002) \n1 (0.0542) \n7 (7.7797e-5) 5 (8.1029e-5) 6 (7.8464e-5) 8 (7.2724e-5) 3 (0.0002) \n4 (0.0001) \n\n3D Edges \n5 (1.4828) \n4 (1.4910) \n3 (1.5167) \n7 (1.4701) \n2 (1.5405) \n1 (1.6739) \n8 (1.4644) \n6 (1.4730) \n\n6 (0.0117) \n7 (0.0108) \n1 (0.1179) \n2 (0.0734) \n4 (0.0622) \n3 (0.0636) \n8 (0.0094) \n5 (0.0151) \n\n3D Keypoints \n6 (1.5375) \n5 (1.5466) \n4 (1.5910) \n3 (1.6456) \n1 (1.7198) \n2 (1.7122) \n7 (1.4709) \n8 (1.4121) \n\n5 (0.0141) \n6 (0.0136) \n2 (0.0531) \n1(0.1275) \n3 (0.0400) \n4 (0.0247) \n7 (0.0132) \n8 (0.0121) \n\nReshading \n5 (1.5504) \n6 (1.5426) \n3 (1.8174) \n4 (1.7990) \n1 (2.2339) \n2 (2.1200) \n7 (1.4774) \n8 (1.3804) \n\n6 (0.0147) \n8 (0.0143) \n2 (0.0781) \n4(0.0545) \n1 (0.1121) \n3 (0.0765) \n7 (0.0144) \n5 (0.0174) \n\nDepth \n6 (1.6542) \n5 (1.6870) \n3 (1.8504) \n4 (1.8176) \n2 (2.1700) \n1 (2.2441) \n7 (1.6008) \n8 (1.5099) \n\n7 (0.0175) \n8 (0.0154) \n3 (0.0595) \n4 (0.0617) \n2 (0.0867) \n1 (0.0989) \n6 (0.0217) \n5 (0.0237) \n\nObject Class. \n5 (22.866) \n4 (23.627) \n7 (22.371) \n8 (21.950) \n6 (22.452) \n3 (23.697) \n1 (33.468) \n2 (28.013) \n\n7 (0.0205) \n6 (0.0217) \n3 (0.0350) \n4 (0.0318) \n5 (0.0286) \n8 (0.0147) \n1 (0.0959) \n2 (0.0774) \n\nScene Class. \n5 (14.575) \n4 (15.074) \n7 (14.206) \n8 (13.801) \n6 (14.332) \n3 (15.474) \n2 (25.750) \n1 (25.962) \n\n8 (0.0149) \n7 (0.0165) \n3 (0.0335) \n4 (0.0305) \n5 (0.0263) \n6 (0.0198) \n2 (0.0504) \n1 (0.1474) \n\n\n\n\nCommon concepts beach, birds, boats, bridge, buildings, cars, cat, clouds, dancing, dog, fish, flowers, garden, grass, house, lake, leaf, moon, mountain, ocean, person, plants, rainbow, rocks, running, sand, sky, sports, street, sun, swimmers, town, tree, vehicle, water, window Animal concepts animal, bear, cat, cow, dog, elk, fox, horses, tiger, zebra\n\nTable S3 :\nS3Subsets of NUS-WIDE tag concepts used in Experiment 4.3\nSupplementary materials, data and code are available at http://yanglifeasibility.com/home/ttl.html\n", "annotations": {"author": "[{\"end\":131,\"start\":82},{\"end\":179,\"start\":132},{\"end\":234,\"start\":180},{\"end\":284,\"start\":235},{\"end\":339,\"start\":285},{\"end\":408,\"start\":340},{\"end\":447,\"start\":409}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":88},{\"end\":139,\"start\":137},{\"end\":194,\"start\":189},{\"end\":244,\"start\":239},{\"end\":298,\"start\":293},{\"end\":350,\"start\":345},{\"end\":424,\"start\":418}]", "author_first_name": "[{\"end\":87,\"start\":82},{\"end\":136,\"start\":132},{\"end\":188,\"start\":180},{\"end\":238,\"start\":235},{\"end\":292,\"start\":285},{\"end\":344,\"start\":340},{\"end\":417,\"start\":409}]", "author_affiliation": "[{\"end\":130,\"start\":93},{\"end\":178,\"start\":141},{\"end\":233,\"start\":196},{\"end\":283,\"start\":246},{\"end\":338,\"start\":300},{\"end\":372,\"start\":352},{\"end\":407,\"start\":374},{\"end\":446,\"start\":426}]", "title": "[{\"end\":79,\"start\":1},{\"end\":526,\"start\":448}]", "venue": null, "abstract": "[{\"end\":1637,\"start\":619}]", "bib_ref": "[{\"end\":1837,\"start\":1833},{\"end\":2312,\"start\":2309},{\"end\":2348,\"start\":2345},{\"end\":2399,\"start\":2396},{\"end\":2912,\"start\":2902},{\"end\":2997,\"start\":2982},{\"end\":8311,\"start\":8307},{\"end\":8644,\"start\":8640},{\"end\":8859,\"start\":8855},{\"end\":12269,\"start\":12266},{\"end\":20096,\"start\":20054},{\"end\":24986,\"start\":24982},{\"end\":25637,\"start\":25633},{\"end\":28622,\"start\":28618},{\"end\":32010,\"start\":32007},{\"end\":35673,\"start\":35669},{\"end\":35787,\"start\":35783},{\"end\":36041,\"start\":36038},{\"end\":36535,\"start\":36531},{\"end\":36755,\"start\":36751},{\"end\":36758,\"start\":36755},{\"end\":36825,\"start\":36821},{\"end\":36828,\"start\":36825},{\"end\":37231,\"start\":37227},{\"end\":37234,\"start\":37231},{\"end\":37385,\"start\":37381},{\"end\":37388,\"start\":37385}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37881,\"start\":37697},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38005,\"start\":37882},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38063,\"start\":38006},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38174,\"start\":38064},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38439,\"start\":38175},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38581,\"start\":38440},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38682,\"start\":38582},{\"attributes\":{\"id\":\"fig_7\"},\"end\":38764,\"start\":38683},{\"attributes\":{\"id\":\"fig_8\"},\"end\":38950,\"start\":38765},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39198,\"start\":38951},{\"attributes\":{\"id\":\"fig_10\"},\"end\":39254,\"start\":39199},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39510,\"start\":39255},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39599,\"start\":39511},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41554,\"start\":39600},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":41911,\"start\":41555},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41981,\"start\":41912}]", "paragraph": "[{\"end\":3211,\"start\":1653},{\"end\":3836,\"start\":3213},{\"end\":4265,\"start\":3838},{\"end\":4423,\"start\":4301},{\"end\":4827,\"start\":4425},{\"end\":4963,\"start\":4829},{\"end\":5342,\"start\":5209},{\"end\":5879,\"start\":5556},{\"end\":6009,\"start\":5898},{\"end\":6297,\"start\":6039},{\"end\":6477,\"start\":6299},{\"end\":7250,\"start\":6532},{\"end\":7314,\"start\":7252},{\"end\":7457,\"start\":7360},{\"end\":7660,\"start\":7477},{\"end\":7792,\"start\":7662},{\"end\":7934,\"start\":7874},{\"end\":8202,\"start\":7936},{\"end\":8340,\"start\":8204},{\"end\":8435,\"start\":8361},{\"end\":8451,\"start\":8437},{\"end\":9887,\"start\":8544},{\"end\":10169,\"start\":9889},{\"end\":10382,\"start\":10185},{\"end\":11169,\"start\":10421},{\"end\":11484,\"start\":11171},{\"end\":12043,\"start\":11486},{\"end\":12456,\"start\":12088},{\"end\":13150,\"start\":12458},{\"end\":14255,\"start\":13152},{\"end\":15681,\"start\":14257},{\"end\":16627,\"start\":15719},{\"end\":17548,\"start\":16642},{\"end\":17731,\"start\":17550},{\"end\":17897,\"start\":17746},{\"end\":18152,\"start\":17899},{\"end\":18317,\"start\":18154},{\"end\":18514,\"start\":18319},{\"end\":18719,\"start\":18516},{\"end\":18941,\"start\":18721},{\"end\":19049,\"start\":18943},{\"end\":19253,\"start\":19051},{\"end\":19351,\"start\":19255},{\"end\":19545,\"start\":19353},{\"end\":19722,\"start\":19547},{\"end\":19885,\"start\":19724},{\"end\":20097,\"start\":19887},{\"end\":20286,\"start\":20099},{\"end\":20547,\"start\":20288},{\"end\":20637,\"start\":20549},{\"end\":20805,\"start\":20639},{\"end\":20976,\"start\":20807},{\"end\":21628,\"start\":20978},{\"end\":21684,\"start\":21630},{\"end\":21900,\"start\":21777},{\"end\":21993,\"start\":21902},{\"end\":22129,\"start\":22035},{\"end\":22356,\"start\":22278},{\"end\":22575,\"start\":22524},{\"end\":23208,\"start\":22691},{\"end\":23483,\"start\":23342},{\"end\":23590,\"start\":23530},{\"end\":23661,\"start\":23627},{\"end\":23911,\"start\":23699},{\"end\":24020,\"start\":23963},{\"end\":24106,\"start\":24022},{\"end\":24342,\"start\":24142},{\"end\":24788,\"start\":24377},{\"end\":25063,\"start\":24829},{\"end\":25153,\"start\":25099},{\"end\":25293,\"start\":25219},{\"end\":25418,\"start\":25328},{\"end\":25583,\"start\":25420},{\"end\":25638,\"start\":25613},{\"end\":26086,\"start\":25640},{\"end\":26297,\"start\":26088},{\"end\":27101,\"start\":26400},{\"end\":27460,\"start\":27103},{\"end\":27567,\"start\":27522},{\"end\":27743,\"start\":27604},{\"end\":27805,\"start\":27745},{\"end\":27996,\"start\":27936},{\"end\":28426,\"start\":28370},{\"end\":28686,\"start\":28452},{\"end\":29631,\"start\":28688},{\"end\":29894,\"start\":29633},{\"end\":29933,\"start\":29918},{\"end\":32052,\"start\":29935},{\"end\":32713,\"start\":32054},{\"end\":33646,\"start\":32737},{\"end\":33997,\"start\":33662},{\"end\":35377,\"start\":34086},{\"end\":35864,\"start\":35398},{\"end\":37696,\"start\":35866}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5208,\"start\":4964},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5555,\"start\":5343},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5897,\"start\":5880},{\"attributes\":{\"id\":\"formula_3\"},\"end\":6038,\"start\":6010},{\"attributes\":{\"id\":\"formula_4\"},\"end\":6531,\"start\":6478},{\"attributes\":{\"id\":\"formula_5\"},\"end\":7359,\"start\":7315},{\"attributes\":{\"id\":\"formula_6\"},\"end\":7873,\"start\":7793},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8360,\"start\":8341},{\"attributes\":{\"id\":\"formula_8\"},\"end\":8543,\"start\":8452},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21776,\"start\":21685},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22034,\"start\":21994},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22277,\"start\":22130},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22523,\"start\":22357},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22654,\"start\":22576},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23341,\"start\":23209},{\"attributes\":{\"id\":\"formula_15\"},\"end\":23626,\"start\":23591},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23698,\"start\":23662},{\"attributes\":{\"id\":\"formula_17\"},\"end\":23962,\"start\":23912},{\"attributes\":{\"id\":\"formula_18\"},\"end\":24141,\"start\":24107},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24376,\"start\":24343},{\"attributes\":{\"id\":\"formula_20\"},\"end\":24828,\"start\":24789},{\"attributes\":{\"id\":\"formula_21\"},\"end\":25218,\"start\":25154},{\"attributes\":{\"id\":\"formula_22\"},\"end\":25327,\"start\":25294},{\"attributes\":{\"id\":\"formula_23\"},\"end\":25612,\"start\":25584},{\"attributes\":{\"id\":\"formula_24\"},\"end\":26399,\"start\":26298},{\"attributes\":{\"id\":\"formula_25\"},\"end\":27521,\"start\":27461},{\"attributes\":{\"id\":\"formula_26\"},\"end\":27935,\"start\":27806},{\"attributes\":{\"id\":\"formula_27\"},\"end\":28369,\"start\":27997},{\"attributes\":{\"id\":\"formula_28\"},\"end\":33661,\"start\":33647},{\"attributes\":{\"id\":\"formula_29\"},\"end\":34085,\"start\":33998}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30251,\"start\":30243},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31640,\"start\":31632},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32946,\"start\":32938}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1651,\"start\":1639},{\"attributes\":{\"n\":\"2\"},\"end\":4299,\"start\":4268},{\"attributes\":{\"n\":\"3\"},\"end\":7475,\"start\":7460},{\"attributes\":{\"n\":\"4\"},\"end\":10183,\"start\":10172},{\"attributes\":{\"n\":\"4.1\"},\"end\":10419,\"start\":10385},{\"attributes\":{\"n\":\"4.2\"},\"end\":12086,\"start\":12046},{\"attributes\":{\"n\":\"4.3\"},\"end\":15717,\"start\":15684},{\"attributes\":{\"n\":\"5\"},\"end\":16640,\"start\":16630},{\"attributes\":{\"n\":\"6\"},\"end\":17744,\"start\":17734},{\"end\":22689,\"start\":22656},{\"end\":23528,\"start\":23486},{\"end\":25097,\"start\":25066},{\"end\":27602,\"start\":27570},{\"end\":28450,\"start\":28429},{\"end\":29916,\"start\":29897},{\"end\":32735,\"start\":32716},{\"end\":35396,\"start\":35380},{\"end\":37706,\"start\":37698},{\"end\":37891,\"start\":37883},{\"end\":38015,\"start\":38007},{\"end\":38073,\"start\":38065},{\"end\":38444,\"start\":38441},{\"end\":38592,\"start\":38583},{\"end\":38693,\"start\":38684},{\"end\":38775,\"start\":38766},{\"end\":38961,\"start\":38952},{\"end\":39209,\"start\":39200},{\"end\":39265,\"start\":39256},{\"end\":39522,\"start\":39512},{\"end\":39611,\"start\":39601},{\"end\":41923,\"start\":41913}]", "table": "[{\"end\":39510,\"start\":39308},{\"end\":41554,\"start\":39788}]", "figure_caption": "[{\"end\":37881,\"start\":37708},{\"end\":38005,\"start\":37893},{\"end\":38063,\"start\":38017},{\"end\":38174,\"start\":38075},{\"end\":38439,\"start\":38177},{\"end\":38581,\"start\":38445},{\"end\":38682,\"start\":38595},{\"end\":38764,\"start\":38696},{\"end\":38950,\"start\":38778},{\"end\":39198,\"start\":38964},{\"end\":39254,\"start\":39212},{\"end\":39308,\"start\":39267},{\"end\":39599,\"start\":39525},{\"end\":39788,\"start\":39614},{\"end\":41911,\"start\":41557},{\"end\":41981,\"start\":41926}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2114,\"start\":2105},{\"end\":10616,\"start\":10607},{\"end\":11185,\"start\":11179},{\"end\":11954,\"start\":11946},{\"end\":13086,\"start\":13080},{\"end\":13281,\"start\":13273},{\"end\":14488,\"start\":14480},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14883,\"start\":14875},{\"end\":15156,\"start\":15148},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16294,\"start\":16288},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16509,\"start\":16499},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22944,\"start\":22937},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24525,\"start\":24516},{\"end\":29140,\"start\":29134},{\"end\":29500,\"start\":29490},{\"end\":29725,\"start\":29718},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30793,\"start\":30786},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31399,\"start\":31390},{\"end\":31892,\"start\":31886},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":32309,\"start\":32300},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33337,\"start\":33330},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":35138,\"start\":35132}]", "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}