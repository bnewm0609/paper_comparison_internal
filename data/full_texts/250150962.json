{"id": 250150962, "updated": "2023-04-05 20:40:59.756", "metadata": {"title": "RadQA: A Question Answering Dataset to Improve Comprehension of Radiology Reports", "authors": "[{\"first\":\"Sarvesh\",\"last\":\"Soni\",\"middle\":[]},{\"first\":\"Meghana\",\"last\":\"Gudala\",\"middle\":[]},{\"first\":\"Atieh\",\"last\":\"Pajouhi\",\"middle\":[]},{\"first\":\"Kirk\",\"last\":\"Roberts\",\"middle\":[]}]", "venue": "LREC", "journal": "Proceedings of the Thirteenth Language Resources and Evaluation Conference", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We present a radiology question answering dataset, RadQA, with 3074 questions posed against radiology reports and annotated with their corresponding answer spans (resulting in a total of 6148 question-answer evidence pairs) by physicians. The questions are manually created using the clinical referral section of the reports that take into account the actual information needs of ordering physicians and eliminate bias from seeing the answer context (and, further, organically create unanswerable questions). The answer spans are marked within the Findings and Impressions sections of a report. The dataset aims to satisfy the complex clinical requirements by including complete (yet concise) answer phrases (which are not just entities) that can span multiple lines. We conduct a thorough analysis of the proposed dataset by examining the broad categories of disagreement in annotation (providing insights on the errors made by humans) and the reasoning requirements to answer a question (uncovering the huge dependence on medical knowledge for answering the questions). The advanced transformer language models achieve the best F1 score of 63.55 on the test set, however, the best human performance is 90.31 (with an average of 84.52). This demonstrates the challenging nature of RadQA that leaves ample scope for future method research.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2022.lrec-1.672", "pubmed": null, "pubmedcentral": null, "dblp": "conf/lrec/SoniGPR22", "doi": null}}, "content": {"source": {"pdf_hash": "bd0d9d7b373f18ace4dea46a7038a3a0269ac947", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5a12e4bdf2b27b2a8dc9cfbe60d0666f7c1970f8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bd0d9d7b373f18ace4dea46a7038a3a0269ac947.txt", "contents": "\nRadQA: A Question Answering Dataset to Improve Comprehension of Radiology Reports\nJune 2022\n\nSarvesh Soni 1sarvesh.soni@uth.tmc.edu \nSchool of Biomedical Informatics\nThe University of Texas Health Science Center at Houston\n\n\nMeghana Gudala \nSchool of Biomedical Informatics\nThe University of Texas Health Science Center at Houston\n\n\nAtieh Pajouhi \nSchool of Biomedical Informatics\nThe University of Texas Health Science Center at Houston\n\n\nKirk Roberts 2kirk.roberts@uth.tmc.edu \nSchool of Biomedical Informatics\nThe University of Texas Health Science Center at Houston\n\n\nRadQA: A Question Answering Dataset to Improve Comprehension of Radiology Reports\n\nProceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022)\nthe 13th Conference on Language Resources and Evaluation (LREC 2022)MarseilleJune 2022Language Resources Association (ELRA), licensed under CC-BY-NC-4.0 6250question answeringmachine reading comprehensionradiology reportsclinical notes\nWe present a radiology question answering dataset, RadQA, with 3074 questions posed against radiology reports and annotated with their corresponding answer spans (resulting in a total of 6148 question-answer evidence pairs) by physicians. The questions are manually created using the clinical referral section of the reports that take into account the actual information needs of ordering physicians and eliminate bias from seeing the answer context (and, further, organically create unanswerable questions). The answer spans are marked within the Findings and Impressions sections of a report. The dataset aims to satisfy the complex clinical requirements by including complete (yet concise) answer phrases (which are not just entities) that can span multiple lines. We conduct a thorough analysis of the proposed dataset by examining the broad categories of disagreement in annotation (providing insights on the errors made by humans) and the reasoning requirements to answer a question (uncovering the huge dependence on medical knowledge for answering the questions). The advanced transformer language models achieve the best F1 score of 63.55 on the test set, however, the best human performance is 90.31 (with an average of 84.52). This demonstrates the challenging nature of RadQA that leaves ample scope for future method research.\n\nIntroduction\n\nQuestion answering (QA) is an intuitive means to query text data and it is especially helpful in the case of large and complex documents. Machine reading comprehension (MRC) has been widely explored to this end in order to better comprehend unstructured text, by enabling machines to answer specific questions given a textual passage (Zeng et al., 2020). Much of these pursuits are powered by neural models and since a well-constructed dataset is pivotal to building a suitable model (for a given requirement, domain, or task) there is an explosion of MRC datasets in recent years (Dzendzik et al., 2021). However, little work is drawn toward the clinical domain to build challenging MRC datasets for improving comprehension of the semantically complex and diverse electronic health record (EHR) data.\n\nIn medicine, much work in MRC is targeted toward biomedical scientific articles, which comes under the umbrella of biomedical QA (Athenikos and Han, 2010;Jin et al., 2021). But, owing to the fundamental differences between the text data present in scientific articles and EHRs (Friedman et al., 2002), the datasets (and models) for the former cannot be directly used for the latter. Moreover, between the two types of EHR data, adequately-sized QA datasets are constructed for the structured part (e.g., semantic parsing) (Roberts and Demner-Fushman, 2016;Pampari et al., 2018;Raghavan et al., 2021) more so than that for the unstructured part. Since the information in both the structured and the unstructured parts of EHR are different and offer unique perspectives about care provision (Tayefi et al., 2021), it is crucial to dig deeper into the unstructured data as they include richer information. FINDINGS: The heart is (enlarged in size) but stable in the interval. Mediastinal contour is unchanged. There is upper zone redistribution of the pulmonary artery vasculature. Perihilar haziness as well as diffuse bilateral pulmonary opacities. These findings are consistent with acute CHF. There are also bilateral pleural effusions. There is barium in the left colon from previous study. IMPRESSION: 1. Findings consistent with pulmonary edema due to CHF. 2. Bilateral pleural effusions.   , underlined, or (in parentheses). Fndg -Findings. Imp -Impression.\n\n\nQ -\n\nThe current MRC datasets for unstructured EHR data fall short of many important considerations in order to build an advanced model for the task. Most of these datasets are too small (to build advanced models) (Fan, 2019) or publicly unavailable (making them  nonexistent for building any models)  or both (Yue et al., 2020a;Oliveira et al., 2021). Additionally, the questions for most of these datasets are collected in a manner that induces bias and does not reflect the real-world user needs, including for an available dataset where the users are shown candidate questions (with answers) for reference (Yue et al., 2020b). Lastly, one of the \"large\" EHR MRC datasets, emrQA (Pampari et al., 2018), has gained much traction. However, the variety in emrQA is severely limited due to templatization, as is also found in a separate systematic analysis of emrQA's MRC data, where they achieved about the same model performance when trained on 5-20% of the dataset versus on the entire training data (Yue et al., 2020a). Thus, there is a need to build a representative dataset for the task of EHR MRC that encompasses user needs, is adequately sized to train advanced models, and is publicly available to push research in model development forward. Furthermore, almost all existing datasets for EHR MRC use discharge summaries as documents. However, other types of clinical texts such as radiology reports (that have vastly different semantic content and vocabulary) are markedly underrepresented in the MRC task. In a recent study, radiology information extraction task is framed as QA to extract radiological entities from report text (Datta and Roberts, 2021), still, it is not illustrative of an actual MRC task because the predefined question templates are limited (only to the specific types of entities extracted) and the queries themselves are not natural (lacks variation). To our knowledge, no existing work focused on the task of radiology MRC in its true sense.\n\nIn this work, we propose RadQA 1 , a new EHR 1 https://github.com/krobertslab/ datasets/tree/master/radqa MRC dataset, that aims to overcome the issues with the existing resources for the MRC task in clinical domain (an example from the dataset is shown in Table 1). The following are the main characteristics of RadQA:\n\n\u2022 The questions reflect true information needs of clinicians ordering radiology reports (as the queries are inspired from the clinical referral section of the radiology reports). \u2022 The corpus contains 3074 unique question-report pairs encompassing 1009 radiology reports from 100 patients. \u2022 Each question has two answers for a radiology report (in its Findings and Impressions sections), resulting in a total of 6148 distinct question-answer evidence pairs (including unanswerable questions, that no available MRC dataset includes). \u2022 The answers are oftentimes present in the form of phrases or span multiple lines (as opposed to only multi-word answer entities in available MRC datasets), fulfilling the clinical information needs. \u2022 The questions require a wide variety of reasoning and domain knowledge to answer, that makes it a challenging dataset for advanced models. \u2022 The distribution of the sampled radiology reports is similar to that in the MIMIC-III database. \u2022 The dataset is publicly available (as the radiology reports come from the publicly available MIMIC-III database).\n\n\nRelated Work\n\nThe current datasets for the task of EHR MRC are summarized in Table 2, along with our proposed dataset, RadQA, for comparison. Raghavan et al. (2018) described a dataset for EHR MRC, where they had medical students create questions while reviewing a clinical summary or the latest clinical note of a patient alongside a set of reference questions. Since the annotators were shown a set of candidate questions for reference, they may be more likely to ask questions that look similar to the referred questions (and thus the actual information need may not be met by the created questions). Moreover, the dataset is unavailable. Pampari et al. (2018) employed a template-based approach to build a large dataset of question-logical form pairs by leveraging an existing set of natural language processing (NLP) annotations. The dataset claims over 400k question-answer evidence pairs, however, this number reduces to 73k after mapping the dataset to a span-extraction MRC task (where each question has a definite span as an answer in the associated evidence) (Soni and Roberts, 2020). Regardless, the variety in the dataset (of questions etc.) is severely limited due to templatization. This is also found in a separate systematic analysis of emrQA's MRC data, where Yue et al. (2020a) achieved about the same model performance when trained on 5-20% of the dataset versus on the entire training data. For evaluating emrQA on unseen data, Yue et al. (2020a) also created a small MRC dataset (limited information is available about the dataset creation).\n\nIn another work, Yue et al. (2020b) created a test set for evaluating their proposed framework for EHR MRC. During annotation, annotators view a clinical note along with candidate question-answer pairs (on this note) and are asked to create new questions (encouraged) and/or select some from the shown candidates. In their final set, over 75% of the questions are selected from the automatically-generated candidates.\n\nFan (2019) focused on why-question answering where the question-answer pairs were cultivated from sentences containing 'because' and/or 'due to'. The representation of this dataset is limited to these sentences and thus do not reflect the actual information needs. Further, the questions created do not involve cross-sentence synthesis and may be biased (both in content and style) and thus lack diversity. Oliveira et al. (2021) explored the EHR MRC task for Portuguese, building a small Portuguese dataset in order to evaluate a transfer learning model.\n\nThere are several efforts toward EHR QA for collecting patient-specific questions (that can be answered using EHR data) (Patrick and Li, 2012) and focusing on retrieving information from the structured part of EHRs by creating datasets (Roberts and Demner-Fushman, 2016;Wang et al., 2020;Raghavan et al., 2021) and/or building models (Roberts and Patra, 2017;Pan et al., 2021). However, because both the structure and the information content vary significantly between the two types of EHR data (structured and unstructured), the models and datasets built for the structured data cannot be readily applied to structured EHR data. Thus, we do not compare the other methods and datasets built for structured data as it is not the focus of this work.  Table 3: Descriptive statistics of the sampled reports and MIMIC-III data (after removing outlier patients).\n\nTop modalities determined separately after filtering out the report types with proportions less than 0.1%.\n\n\nRadQA Dataset\n\n\nDocument Sampling\n\nWe source the radiology reports (used as documents) for our dataset from the MIMIC-III database (Johnson et al., 2016), which is a publicly available resource containing information on intensive care unit patients. MIMIC-III includes over 2M clinical notes, out of which more than a quarter are radiology reports (over 500k) covering a wide variety of modalities such as chest x-ray, computed tomography (CT), and magnetic resonance imaging (MRI). We sample a realistic set of reports by selecting the documents at the patient level, i.e., we first sampled patients and then included all the associated radiology reports in our dataset for annotations. Specifically, we randomly sample 100 patients from the set of patients with at least 1 and at most 36 (to remove outliers) radiology reports, thereby resulting in a total of 1009 reports in our final set. We further divide our data of 100 patients into training, development, and testing splits in the ratio of 8:1:1, respectively. The descriptive statistics of our sampled reports are shown in Table 3 alongside the MIMIC-III statistics for comparison.\n\n\nQuestion Creation\n\nWe take a novel approach to create the set of questions that satisfy the true information needs of the readers of radiology reports, i.e., the physicians who order radiology exams. The ordering physicians include their requirements in the form of a clinical referral that is sent to the radiologist along with the radiographs. The radiologists refer to these expressed requirements while writing their interpretation of the radiology images in the form of a radiology report, the final version of which includes the referral section at the beginning. We harness the clinical referral section to capture the actual information needs of the ordering physicians. Figure 1 illustrates our question creation process. In order to align well to the information needs mentioned in the referral portion, we hide the other contents of the report from annotators in the question creation phase. This forces the annotators to focus only on the aspects related to conducting the exam (as provided in referral), and not diverge into creating biased questions based on the remaining full report content. The referral contains, along with the examination type, mainly two sections of our interest, namely, Medical Condition (gives a brief overview about the condition of the patient) and Reason for this Examination (provides the reasons for ordering the radiology study). The referral shown in Figure 1 is for a Chest (Portable AP) study (a type of X-ray), where the patient is 64 year old male with certain conditions and the exam is ordered to evaluate for CHF (Congestive Heart Failure) and effusions.\n\nThe annotators were asked to create questions thinking about both implicit and explicit information needs expressed by the referral section. E.g., \"Is there any sign of pleural effusion?\" asks about the explicit needs conveyed by the reason effusions while the question \"Did the cardiac silhouette enlarge?\", instead of directly asking about the reason CHF, asks about an implicit detail associated with it, i.e., the enlargement of heart due to CHF. Our annotators possess the medical knowledge to understand both the types of information needs in the referral and thus were able to incorporate those needs into their created questions.\n\nTo create specific questions, we instructed the annotators to ask separate questions for each reason mentioned in the referral. In other words, we refrained from creating questions encompassing all the sub-reasons present in the reason section of the referral. E.g., we asked annotators to create separate questions for reasons CHF and effusions, instead of asking about both in the same question. Further, annotators were asked to create at least one question for each reason. This was helpful to create individual questions capturing the distinct information needs.\n\nWe advised the annotators to employ different variations in phrasing the questions and to not merely fill in reason information in a set of templates. E.g., for the sample in Figure 1, we asked not to create all questions like \"Does the patient have CHF?\" and \"Does the patient have effusions?\". Several other examples were also provided to help them understand the task. This helped us create a syntactically diverse set of questions.\n\nTwo annotators independently constructed questions for all the reports in our dataset. This further improved the heterogeneity of our set of questions. We assessed the created questions at regular intervals through the annotation process to reinforce all the instructions (provided to the annotators under annotation guidelines). All the questions are associated with the report whose clinical referral section was used while constructing them. Finally, the questions are deduplicated at report level, i.e., if the two annotators ended up asking the exact same question (in terms of its text) about a report, we remove the duplicate.\n\n\nAnswer Annotation\n\nFor marking answers, the annotators were shown the whole radiology report (including the referral) along with the corresponding set of questions. There are two main sections in radiology reports, namely, Findings and Impressions, where the former describes the characteristics of underlying medical image(s) in detail while the latter summarizes the findings (largely inspired by the requirements mentioned in the referral section). We tasked the annotators to annotate answer spans in the report text, at most one span each in Findings and Impressions sections. We instructed them to annotate the shortest possible span that answers a question to the point. Specifically, the selected answer span should be sufficient by itself to answer the question but, simultaneously, it should not contain any additional information that is not required by the question.\n\nIn the example from Table 1, the answer span for the question \"Did the cardiac silhouette enlarge?\" is annotated as \"enlarged in size\" because this is the shortest span in the Findings that can sufficiently to answer the question. Note that we did not include in the answer span any other portion of the sentence \"heart is enlarged in size but stable in the interval\". Though other information in the whole sentence (or even in the other parts of the report) may be relevant to the question at hand, we do not include it because it is extraneous to the exact information needs of the question. In other words, we asked the annotators to keep this in mind while selecting the answers -if a clinician asks the question at hand, would the selected span be just enough to answer it (given that the clinician can view the origin of the returned answer from report text).\n\nAgain, the medical knowledge of our annotators is vital in this phase. Because the questions are not con-  structed after viewing the full report text or deciding an answer in advance, they may not have direct answers in the report. E.g., in \"Are there any infiltrates in the lung?\" (Table 1), both the medical keywords infiltrates and lung are not even present in report text. But the annotators used their medical expertise to annotate \"diffuse bilateral pulmonary opacities\" (as they represent infiltrates on a X-ray) and \"pulmonary edema\" (characterized by infiltrates) (Tuddenham, 1984;Hansell et al., 2008). This creates a unique challenging aspect in RadQA for next generation clinical models. Further, owing to the question creation phase, all the questions in our dataset are not required to have answers in the report. Thus, there was also an option for the annotators to mark a question as unanswerable, in case they are unable to find its answer in any section of the given report. All the answers are marked by two annotators independently and reconciled at regular intervals. We use the Haystack annotation tool 2 (modified to our needs) for annotating answers. Note that the Impressions section is sometimes also present with heading \"Conclusion\" and Findings are oftentimes included under \"Procedure and Findings\".  \n\n\nReconciliation Process and Challenges\n\nWe adjudicated the annotated answers frequently in order to ensure the quality of our dataset. For the first 100 reports, we reconciled in the batches of 10, giving both the annotators sufficient time to ramp up on the annotation scheme. Afterward, we reconciled in the batches of 100 reports. We calculate inter-annotator agreement using F1 measure as used in an earlier study for span-based MRC (Rajpurkar et al., 2016). We manually reviewed the disagreements and characterized the challenges encountered during reconciliation (Table 4). The agreement statistics are reported in Table 5.\n\nWe reconciled all the answer spans down to one unique answer for our training split. For the dev and test splits, however, as long as both the annotated spans answered the question at hand, we kept both the answers in the dataset. This facilitates a natural development and evaluation of models which respects the presence of more than one ways of answering a question. \n\n\nDataset Analysis\n\nThe descriptive statistics of our proposed dataset are shown in Table 6. We note that the RadQA dataset is built using more than 3 times the number of reports used to generate emrQA and the average number of questions generated per paragraph in the emrQA data is significantly higher than those created in the RadQA dataset. These numbers underline a greater variation of passage data in the RadQA data as compared to emrQA. The paragraph length is a characteristic of the type of reports used in both the datasets, radiology reports in RadQA versus discharge summaries (or similar notes) in emrQA. Both the average and median answer lengths of the RadQA data (16.21 and 7, respectively) is significantly higher than that for emrQA (1.88 and 2, respectively). This emphasizes the wide variety of answers (both structurally and semantically) available in the RadQA dataset. The answers are usually present as phrases (offering a complete answer satisfying the clinical information needs of ordering physicians) in contrast to emrQA (and most other available EHR MRC datasets), which only includes answers as clinical entities such as \"aspirin\" and \"50 MG\". We illustrate the structural variety of questions by  plotting 4-grams prefixes in Figure 2. The sunburst graph for the RadQA dataset is well-distributed while for the emrQA dataset it is skewed (having more than half of the questions beginning with \"what\"). This can be attributed to the mechanism of creating questions in the dataset, i.e., manual for the RadQA versus automatic template filling for the emrQA.\n\nTo analyze the types of reasoning that are required to answer the questions in the RadQA dataset, we perform a human evaluation by randomly sampling 100 answerable questions (see Table 7). We report the emrQA statistics directly from (Pampari et al., 2018). A majority of the questions in RadQA data require medical knowledge (73%) to answer, which justifiably compensates the lower proportion of questions with coreference reasoning. We also characterize the questions based on an additional set of reasoning categories that are peculiar to radiology (or clinical) domain.\n\n\nBaselines\n\nDeep learning models based on transformer architectures are shown to achieve state-of-the-art performance for MRC, where a model extracts specific answer spans from the context paragraph given a question (Liu et al., 2019). In order to understand the current level of comprehension of the advanced transformer language models, we gauged their performance on RadQA. To identify the effect of transfer learning and domain language information, we employed different variations of finetuning strategies and used models that are pre-trained on various open-domain and domain-specific datasets.\n\nWe use BERT (Devlin et al., 2019) and BERT-MIMIC (Si et al., 2019) as our baseline models. Devlin et al. (2019) employ masked language modelling to learn (pre-train) Bidirectional Encoder Representations from Transformers (BERT), the knowledge from which can be easily transferred to downstream tasks (such as MRC) by further fine-tuning these models. This helps transfer learning from large unstructured data to specific tasks without building the models from scratch each time and is especially helpful in the specific domains (such as clinical) with limited availability of datasets. The BERT model is pre-trained on massive textual corpora from BooksCorpus and English Wikipedia (3.3B words) for 1M steps. Si et al. (2019) further pre-train BERT on MIMIC-III notes (786M \n\n\nIncomplete Context\n\nMissing contextual information in ans sentences Q: I: Do we find any stenosis in the carotid arteries that require grafting during/after CABG? S: Right ICA stenosis 40-59%.\n\n\n16%\n\n13.3%\n\n\nChange information\n\nQues related to interval changes Q: Has thyroid cancer progressed? S: The right neck mass appears to have significantly increased in size and surrounding mass effect compared with the prior . . .\n\n\n18% -\n\n\nDiagnosis knowledge\n\nQues require diagnosis understanding to ans Q: Are there signs of pneumonia? S: Marked improvement in left perihilar alveolar process with residual well-marginated mass-like opacity . . .\n\n\n26% -\n\n\nAnatomy knowledge\n\nQues require anatomy understanding to ans Q: Did the gastric cancer metastasize to chest? S: There are no lung nodules or masses. No destructive lytic or blastic lesions are seen in the osseous structures of the torso.\n\n\n21% -\n\n\nRequire specification\n\nQues require specific information in ans Q: What is the status of the skull fracture through midface? S: 5. Possible nondisplaced fracture of the anterior wall of the right maxillary sinus. 6. Displaced fracture of the right nasal bone.\n\n13% -Negative answer Ans is present but negated Q: I: Is there any mediastinal shift due to pneumothorax? S: No pneumothorax. 23% - words) for 300k steps, to build BERT-MIMIC. Both the models achieved state-of-the-art performances on challenge NLP datasets, that resulted in a wide adoption of transformer models in NLP. We choose the same base cased variant of both the models for a fair comparison.\n\n\nEvaluation\n\nWe formulate the task to, given a question and a paragraph, either extract a single answer span or mark it unanswerable. We feed into the models the amalgam of Findings and Impressions sections as paragraphs along with questions and their corresponding answers. Inspired from our previous work on evaluating the finetuning variations of the transformer models (Soni and Roberts, 2020), we fine-tune the baseline models on different combinations of MRC datasets, both from general and clinical domains. Specifically, we explore fine-tuning on a single dataset along with a combination of two and three datasets. Here, the model is finetuned on each of the involved dataset for 2 epochs. Thus, the single-dataset variation is fine-tuned for 2 epochs while the double-and triple-dataset variations are fine-tuned on a total of 4 and 6 epochs, respectively.\n\nWe use SQuAD 2.0 (Rajpurkar et al., 2016;Rajpurkar et al., 2018) and emrQA (Pampari et al., 2018) for additional fine-tuning, aside from RadQA. SQuAD 2.0 is a large open-domain MRC dataset (over 150k questions), built from Wikipedia paragraphs through crowdsourcing, containing both answerable (singlespan) and unanswerable questions.\n\nThe dataset is split into training, development, and testing sets at patient level (in the ratio of 8:1:1) for a realistic evaluation. The training set is used to train the models while the development and testing sets are used to tune the models and evaluate the final models, respectively. We calculate standard evaluation metrics for MRC, i.e., exact match or EM (strict metric that matches predicted answer phrases exactly with ground truth) and F1 (calculates F1 at word level matches between the prediction and ground truth). We tune the models on our development set. The maximum sequence length is 384; document stride is 128; maximum query length is 128; learning rate is 3e-5.\n\n\nResults\n\nThe evaluation results from baseline models on RadQA is shown in Table 8. Unsurprisingly, among the singledataset variations, the best model performances come from fine-tuning on the RadQA dataset. Fine-tuning on emrQA by itself did not generalize well and essen-   tially marked all the questions as unanswerable. Note that the evaluated models are trained the same way for answerable and unanswerable questions and the main difference lies in the prediction phase where a post-processing pipeline decides, on the basis of output model probabilities, whether to mark a question unanswerable or not. The predictions from the emrQAonly model are in contrast to the dataset characteristic of having only answerable questions. However, after turning off the option to mark a question unanswerable, the emrQA-only model performed even worse. Almost all the BERT-MIMIC models performed better than their BERT equivalent variations. This echoes the usefulness of injecting clinical text information in the language models. The model variant finetuned on SQuAD and RadQA performed the best in a majority of cases. Thus, the additional fine-tuning on SQuAD before tuning the model on the RadQA dataset was helpful. Note that the performance jump, when compared to the RadQA-only variant, after an additional round of fine-tuning on SQuAD is higher than that with the emrQA. Note that there is a significant gap between the best baseline model performance and the average human performance (Table 9) on development and testing sets (a difference of 25 and 26 points in exact match for dev and test, respectively).\n\n\nDiscussion\n\nWe propose the RadQA (Radiology Question Answering) dataset encapsulating the actual information needs of clinicians who order the radiology exams. We present a thorough analysis of our dataset, highlighting its complexity and the reasoning required to answer (or not) the questions. The substantial gap between the baseline model and human performance presents an ample opportunity to implement sophisticated models to better comprehend radiology report text.\n\nThe evaluation results from the baseline models are consistent with our prior work (Soni and Roberts, 2020) on evaluating the task of clinical MRC, where we saw a similar trend in performance improvements when the models were fine-tuned on the different variation of open-and specific-domain datasets. Notably, the current findings reiterate that additional fine-tuning on an open-domain dataset, SQuAD, results in better performance gains as compared to additionally finetuning on a different clinical-domain dataset, emrQA. This may be attributed to the quality of a manuallycreated dataset over an automatically-generated corpus.\n\nInjecting additional information while training the models may be helpful in improving their performance. We use the Findings and Impressions sections from radiology reports to train and test the baseline models, in order to understand the performance under standard settings. Besides, it will be interesting to prepend clinical referral section during evaluation to understand how these models use this additional information to their advantage. Moreover, as radiology reports are oftentimes long (average length of 274 tokens), a good future direction will be to explore the recent efforts toward improving the comprehension of long documents (Beltagy et al., 2020;Zaheer et al., 2020). Further, as the RadQA questions generally require medical knowledge (73% in our analysis) to answer, incorporating such knowledge into the models, such as in Hao et al. (2020), will be another appealing avenue of research for improving comprehension of radiology reports.\n\n\nConclusion\n\nWith the aim to improve the comprehension of radiology reports, we propose the RadQA dataset that encapsulates the information needs of ordering physicians in its questions and includes complete answers in the form of phrases. The exhaustive analysis of RadQA uncovers the common disagreements and reasoning requirements while answering the questions. The performance of the best transformer language model, MIMIC-BERT, is 63.55 (F1), which falls significantly short of the best human performance of 90.31. This indicates that the RadQA dataset is challenging and provides scope for future research in EHR MRC.\n\n\nFINAL REPORT INDICATION: 64 year old male with status post recent STE MI. Now with increasing edema and shortness of breath.\n\n\nAre there any infiltrates in the lung? Adiffuse bilateral pulmonary opacities (Fndg), pulmonary edema (Imp) Q -Did the cardiac silhouette enlarge? A -(enlarged in size) (Fndg) Q -Is there any sign of pleural effusion? A -[b/B]ilateral pleural effusions (Fndg and Imp)\n\nFigure 1 :\n1An example of clinical referral section with the corresponding constructed questions. Only the referral section is shown to the annotators in the question creation phase, not the whole report.\n\nFigure 2 :\n2Types of questions in RadQA and emrQA.\n\nTable 1 :\n1An example from RadQA. The answers are italicized only\n\nTable 2 :\n2Existing EHR MC datasets alongside our proposed RadQA dataset. # -Count. Ques -Questions. Ans -Answers. Docs -Documents. UN-Q -Unanswerable questions. n2c2 -formerly i2b2.\n\nTable 4 :\n4Common disagreement categories with examples from manual evaluation of 100 randomly sampled questions with any disagreement. The main differences between the answer spans are underlined. Percentages do not add to 100% as some questions fall into multiple categories. Final reconciled answers are marked with a checkmark (\u2713). % -Percentage. Q -Questions. A1 -Annotator 1. A2 -Annotator 2. RC -Reconciled.\n\n\n2 https://github.com/deepset-ai/haystack \n\nSplit \nDocs \nQues \nEM \nF1 \nFirst \n100 \n296 \n52.40 \n68.02 \nRemaining \n909 \n5522 \n50.16 \n69.48 \nAll \n1009 \n6148 \n50.39 \n69.34 \n\n\n\nTable 5 :\n5Inter-annotator agreement.\n\nTable 6 :\n6Descriptive statistics of RadQA (with emrQA \nfor comparison). Lengths are in tokens. # -Count. \nUnAns -Unanswerable. Med -Median. \n\n\n\nTable 7 :\n7Reasoning categories with examples from a manual evaluation of 100 randomly sampled answerable questions from RadQA. Words relevant to the reasoning are bolded; the ground truth answers are underlined. % do not add to 1 as questions fall into multiple categories. Q -Question. S -Sentence. Hyphen (-) -unavailable.\n\n\nSQuAD \u21d2 emrQA \u21d2 RadQA 48.53 63.01 46.65 60.98 53.26 67.79 48.32 62.29BERT \nBERT-MIMIC \n\nFine-tuned on \n\nDev \nTest \nDev \nTest \nEM \nF1 \nEM \nF1 \nEM \nF1 \nEM \nF1 \nemrQA \n25.08 25.08 35.21 35.21 24.92 24.92 35.21 35.21 \nSQuAD \n25.41 36.73 30.79 42.92 25.57 42.81 24.39 40.37 \nRadQA \n42.02 58.67 40.09 55.04 48.05 65.85 45.73 60.08 \nemrQA \u21d2 RadQA \n43.16 59.75 41.92 57.60 50.65 67.97 47.71 61.60 \nSQuAD \u21d2 RadQA \n49.51 65.80 46.04 60.71 52.28 69.42 49.39 63.55 \n\n\nTable 8 :\n8Model performances on the RadQA dev and test sets when fine-tuned on different data combinations. 85.02 92.07 81.40 90.31 66.97 81.11 70.32 83.18 Annotator 2 71.66 81.41 69.36 78.72 72.76 84.73 72.28 83.76Dev \nTest \nTrain \nAll \n\nEM \nF1 \nEM \nF1 \nEM \nF1 \nEM \nF1 \nAnnotator 1 Avg \n78.34 86.74 75.38 84.52 69.87 82.92 71.30 83.47 \n\n\n\nTable 9 :\n9Human performance on RadQA.\nAcknowledgements This work was supported by the U.S. National Library of Medicine, National Institutes of Health, (R00LM012104); the National Institute of Biomedical Imaging and Bioengineering (R21EB029575); and UTHealth Innovation for Cancer Prevention Research Training Program Predoctoral Fellowship (CPRIT RP210042).\nBiomedical question answering: A survey. S J Athenikos, H Han, Computer Methods and Programs in Biomedicine. 99Athenikos, S. J. and Han, H. (2010). Biomedical ques- tion answering: A survey. Computer Methods and Programs in Biomedicine, 99:1-24.\n\nLongformer: The Long-Document Transformer. I Beltagy, M E Peters, A Cohan, arXiv:2004.05150csBeltagy, I., Peters, M. E., and Cohan, A. (2020). Longformer: The Long-Document Transformer. arXiv:2004.05150 [cs].\n\nFine-grained spatial information extraction in radiology as two-turn question answering. S Datta, K Roberts, International Journal of Medical Informatics. 158104628Datta, S. and Roberts, K. (2021). Fine-grained spa- tial information extraction in radiology as two-turn question answering. International Journal of Medi- cal Informatics, 158:104628.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, NAACL-HLT. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirec- tional Transformers for Language Understanding. In NAACL-HLT, pages 4171-4186.\n\nEnglish Machine Reading Comprehension Datasets: A Survey. D Dzendzik, C Vogel, J Foster, arXiv:2101.10421csDzendzik, D., Vogel, C., and Foster, J. (2021). English Machine Reading Comprehension Datasets: A Sur- vey. arXiv:2101.10421 [cs].\n\nAnnotating and Characterizing Clinical Sentences with Explicit Why-QA Cues. J Fan, Proceedings of the 2nd Clinical Natural Language Processing Workshop. the 2nd Clinical Natural Language Processing WorkshopFan, J. (2019). Annotating and Characterizing Clinical Sentences with Explicit Why-QA Cues. In Proceed- ings of the 2nd Clinical Natural Language Process- ing Workshop, pages 101-106.\n\nTwo biomedical sublanguages: A description based on the theories of Zellig Harris. C Friedman, P Kra, A Rzhetsky, Journal of Biomedical Informatics. 35Friedman, C., Kra, P., and Rzhetsky, A. (2002). Two biomedical sublanguages: A description based on the theories of Zellig Harris. Journal of Biomedical Informatics, 35:222-235.\n\n. D M Hansell, A A Bankier, H Macmahon, T C Mcloud, N L M\u00fcller, Remy , J , Hansell, D. M., Bankier, A. A., MacMahon, H., McLoud, T. C., M\u00fcller, N. L., and Remy, J. (2008).\n\nFleischner Society: Glossary of Terms for Thoracic Imaging. Radiology. 246Fleischner Society: Glossary of Terms for Thoracic Imaging. Radiology, 246:697-722.\n\nEnhancing Clinical BERT Embedding using a Biomedical Knowledge Base. B Hao, H Zhu, I Paschalidis, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsHao, B., Zhu, H., and Paschalidis, I. (2020). Enhanc- ing Clinical BERT Embedding using a Biomedical Knowledge Base. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 657-661.\n\n. Q Jin, Z Yuan, G Xiong, Q Yu, H Ying, C Tan, M Chen, S Huang, X Liu, Yu , S , Jin, Q., Yuan, Z., Xiong, G., Yu, Q., Ying, H., Tan, C., Chen, M., Huang, S., Liu, X., and Yu, S. (2021).\n\narXiv:2102.05281Biomedical Question Answering: A Survey of Approaches and Challenges. Biomedical Question Answering: A Survey of Ap- proaches and Challenges. arXiv:2102.05281 [cs].\n\nMIMIC-III, a freely accessible critical care database. A E W Johnson, T J Pollard, L Shen, L.-W H Lehman, M Feng, M Ghassemi, B Moody, P Szolovits, L Anthony Celi, R G Mark, Scientific Data. 3160035Johnson, A. E. W., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi, L., and Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific Data, 3:160035.\n\nNeural Machine Reading Comprehension. S Liu, X Zhang, S Zhang, H Wang, W Zhang, Methods and Trends. Applied Sciences. 93698Liu, S., Zhang, X., Zhang, S., Wang, H., and Zhang, W. (2019). Neural Machine Reading Comprehension: Methods and Trends. Applied Sciences, 9:3698.\n\nExperiments on Portuguese Clinical Question Answering. L E S Oliveira, E T R Schneider, Y B Gumiel, M A P Da Luz, E C Paraiso, C Moro, Intelligent Systems. Andr\u00e9 Britto et al.Oliveira, L. E. S., Schneider, E. T. R., Gumiel, Y. B., da Luz, M. A. P., Paraiso, E. C., and Moro, C. (2021). Experiments on Portuguese Clinical Ques- tion Answering. In Andr\u00e9 Britto et al., editors, Intel- ligent Systems, pages 133-145.\n\nemrQA: A Large Corpus for Question Answering on Electronic Medical Records. A Pampari, P Raghavan, J Liang, J Peng, EMNLP. Pampari, A., Raghavan, P., Liang, J., and Peng, J. (2018). emrQA: A Large Corpus for Question An- swering on Electronic Medical Records. In EMNLP, pages 2357-2368.\n\nA BERT-Based Generation Model to Transform Medical Texts to SQL Queries for Electronic Medical Records: Model Development and Validation. Y Pan, C Wang, B Hu, Y Xiang, X Wang, Q Chen, J Chen, J Du, JMIR Med Inform. 932698Pan, Y., Wang, C., Hu, B., Xiang, Y., Wang, X., Chen, Q., Chen, J., and Du, J. (2021). A BERT-Based Generation Model to Transform Med- ical Texts to SQL Queries for Electronic Medical Records: Model Development and Validation. JMIR Med Inform, 9:e32698.\n\nAn ontology for clinical questions about the contents of patient notes. J Patrick, M Li, Journal of Biomedical Informatics. 45Patrick, J. and Li, M. (2012). An ontology for clinical questions about the contents of patient notes. Jour- nal of Biomedical Informatics, 45:292-306.\n\nP Raghavan, S Patwardhan, J J Liang, M V Devarakonda, arXiv:1805.06816Annotating Electronic Medical Records for Question Answering. Raghavan, P., Patwardhan, S., Liang, J. J., and Devarakonda, M. V. (2018). Annotating Elec- tronic Medical Records for Question Answering. arXiv:1805.06816.\n\nemrKBQA: A Clinical Knowledge-Base Question Answering Dataset. P Raghavan, J J Liang, D Mahajan, R Chandra, P Szolovits, Proceedings of the 20th Workshop on Biomedical Language Processing. the 20th Workshop on Biomedical Language ProcessingRaghavan, P., Liang, J. J., Mahajan, D., Chandra, R., and Szolovits, P. (2021). emrKBQA: A Clinical Knowledge-Base Question Answering Dataset. In Proceedings of the 20th Workshop on Biomedical Language Processing, pages 64-73.\n\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016). SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Lan- guage Processing, pages 2383-2392.\n\nKnow What You Don't Know: Unanswerable Questions for SQuAD. P Rajpurkar, R Jia, P Liang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsShort Papers2Rajpurkar, P., Jia, R., and Liang, P. (2018). Know What You Don't Know: Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789.\n\nAnnotating logical forms for EHR questions. K Roberts, D Demner-Fushman, LREC. Roberts, K. and Demner-Fushman, D. (2016). Anno- tating logical forms for EHR questions. In LREC, pages 3772-3778.\n\nA Semantic Parsing Method for Mapping Clinical Questions to Logical Forms. K Roberts, B G Patra, AMIA Annual Symposium Proceedings. 2017Roberts, K. and Patra, B. G. (2017). A Semantic Pars- ing Method for Mapping Clinical Questions to Log- ical Forms. In AMIA Annual Symposium Proceed- ings, volume 2017, pages 1478-1487.\n\nEnhancing clinical concept extraction with contextual embeddings. Y Si, J Wang, H Xu, K Roberts, Journal of the American Medical Informatics Association. 26Si, Y., Wang, J., Xu, H., and Roberts, K. (2019). En- hancing clinical concept extraction with contextual embeddings. Journal of the American Medical In- formatics Association, 26:1297-1304.\n\nEvaluation of Dataset Selection for Pre-Training and Fine-Tuning Transformer Language Models for Clinical Question Answering. S Soni, K Roberts, LREC. Soni, S. and Roberts, K. (2020). Evaluation of Dataset Selection for Pre-Training and Fine-Tuning Trans- former Language Models for Clinical Question An- swering. In LREC, pages 5534-5540.\n\nChallenges and opportunities beyond structured data in analysis of electronic health records. M Tayefi, P Ngo, T Chomutare, H Dalianis, E Salvi, A Budrionis, F Godtliebsen, WIREs Computational Statistics. 131549Tayefi, M., Ngo, P., Chomutare, T., Dalianis, H., Salvi, E., Budrionis, A., and Godtliebsen, F. (2021). Chal- lenges and opportunities beyond structured data in analysis of electronic health records. WIREs Com- putational Statistics, 13:e1549.\n\nGlossary of terms for thoracic radiology: Recommendations of the Nomenclature Committee of the Fleischner Society. W Tuddenham, American Journal of Roentgenology. 143Tuddenham, W. (1984). Glossary of terms for tho- racic radiology: Recommendations of the Nomen- clature Committee of the Fleischner Society. Ameri- can Journal of Roentgenology, 143:509-517.\n\nText-to-SQL Generation for Question Answering on Electronic Medical Records. P Wang, T Shi, C K Reddy, Proceedings of The Web Conference. The Web ConferenceWang, P., Shi, T., and Reddy, C. K. (2020). Text-to- SQL Generation for Question Answering on Elec- tronic Medical Records. In Proceedings of The Web Conference, pages 350-361.\n\nClinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset. X Yue, B Jimenez Gutierrez, H Sun, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsYue, X., Jimenez Gutierrez, B., and Sun, H. (2020a). Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics, pages 4474-4486.\n\nX Yue, X F Zhang, Z Yao, S Lin, H Sun, arXiv:2010.16021CliniQG4QA: Generating Diverse Questions for Domain Adaptation of Clinical Question Answering. csYue, X., Zhang, X. F., Yao, Z., Lin, S., and Sun, H. (2020b). CliniQG4QA: Generating Diverse Ques- tions for Domain Adaptation of Clinical Question Answering. arXiv:2010.16021 [cs].\n\nBig bird: Transformers for longer sequences. M Zaheer, G Guruganesh, K A Dubey, J Ainslie, C Alberti, S Ontanon, P Pham, A Ravula, Q Wang, L Yang, Ahmed , A , Advances in Neural Information Processing Systems. H. Larochelle, et al.33Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. (2020). Big bird: Transformers for longer sequences. In H. Larochelle, et al., editors, Advances in Neural Information Processing Systems, volume 33, pages 17283-17297.\n\nA Survey on Machine Reading Comprehension-Tasks. C Zeng, S Li, Q Li, J Hu, J Hu, Evaluation Metrics and Benchmark Datasets. Applied Sciences. 107640Zeng, C., Li, S., Li, Q., Hu, J., and Hu, J. (2020). A Survey on Machine Reading Compre- hension-Tasks, Evaluation Metrics and Benchmark Datasets. Applied Sciences, 10:7640.\n", "annotations": {"author": "[{\"end\":225,\"start\":94},{\"end\":333,\"start\":226},{\"end\":440,\"start\":334},{\"end\":572,\"start\":441}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":102},{\"end\":240,\"start\":234},{\"end\":347,\"start\":340},{\"end\":453,\"start\":446}]", "author_first_name": "[{\"end\":101,\"start\":94},{\"end\":233,\"start\":226},{\"end\":339,\"start\":334},{\"end\":445,\"start\":441}]", "author_affiliation": "[{\"end\":224,\"start\":134},{\"end\":332,\"start\":242},{\"end\":439,\"start\":349},{\"end\":571,\"start\":481}]", "title": "[{\"end\":82,\"start\":1},{\"end\":654,\"start\":573}]", "venue": "[{\"end\":739,\"start\":656}]", "abstract": "[{\"end\":2315,\"start\":976}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2684,\"start\":2665},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2935,\"start\":2912},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3288,\"start\":3263},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3305,\"start\":3288},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3434,\"start\":3411},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3690,\"start\":3656},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3711,\"start\":3690},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3733,\"start\":3711},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3944,\"start\":3923},{\"end\":4562,\"start\":4529},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4824,\"start\":4813},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4928,\"start\":4909},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4950,\"start\":4928},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5228,\"start\":5209},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5303,\"start\":5281},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5620,\"start\":5601},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6263,\"start\":6238},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8153,\"start\":8131},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8652,\"start\":8631},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9083,\"start\":9059},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9285,\"start\":9267},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9456,\"start\":9438},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9589,\"start\":9571},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10402,\"start\":10380},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10671,\"start\":10650},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10800,\"start\":10766},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10818,\"start\":10800},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10840,\"start\":10818},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10889,\"start\":10864},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10906,\"start\":10889},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11651,\"start\":11629},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18871,\"start\":18854},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18892,\"start\":18871},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20075,\"start\":20051},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22462,\"start\":22440},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23015,\"start\":22997},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23417,\"start\":23396},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23450,\"start\":23433},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23495,\"start\":23475},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24110,\"start\":24094},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26123,\"start\":26099},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26635,\"start\":26611},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26658,\"start\":26635},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26691,\"start\":26669},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29816,\"start\":29792},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31010,\"start\":30988},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31030,\"start\":31010},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31207,\"start\":31190}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32055,\"start\":31929},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32325,\"start\":32056},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32531,\"start\":32326},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32583,\"start\":32532},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32650,\"start\":32584},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32834,\"start\":32651},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33250,\"start\":32835},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33422,\"start\":33251},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":33461,\"start\":33423},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":33606,\"start\":33462},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":33933,\"start\":33607},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":34390,\"start\":33934},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":34731,\"start\":34391},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":34771,\"start\":34732}]", "paragraph": "[{\"end\":3132,\"start\":2331},{\"end\":4596,\"start\":3134},{\"end\":6574,\"start\":4604},{\"end\":6895,\"start\":6576},{\"end\":7986,\"start\":6897},{\"end\":9552,\"start\":8003},{\"end\":9971,\"start\":9554},{\"end\":10528,\"start\":9973},{\"end\":11387,\"start\":10530},{\"end\":11495,\"start\":11389},{\"end\":12639,\"start\":11533},{\"end\":14250,\"start\":12661},{\"end\":14889,\"start\":14252},{\"end\":15458,\"start\":14891},{\"end\":15895,\"start\":15460},{\"end\":16530,\"start\":15897},{\"end\":17411,\"start\":16552},{\"end\":18278,\"start\":17413},{\"end\":19612,\"start\":18280},{\"end\":20243,\"start\":19654},{\"end\":20615,\"start\":20245},{\"end\":22204,\"start\":20636},{\"end\":22779,\"start\":22206},{\"end\":23382,\"start\":22793},{\"end\":24159,\"start\":23384},{\"end\":24354,\"start\":24182},{\"end\":24367,\"start\":24362},{\"end\":24585,\"start\":24390},{\"end\":24804,\"start\":24617},{\"end\":25052,\"start\":24834},{\"end\":25322,\"start\":25086},{\"end\":25724,\"start\":25324},{\"end\":26592,\"start\":25739},{\"end\":26928,\"start\":26594},{\"end\":27616,\"start\":26930},{\"end\":29232,\"start\":27628},{\"end\":29707,\"start\":29247},{\"end\":30341,\"start\":29709},{\"end\":31303,\"start\":30343},{\"end\":31928,\"start\":31318}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6840,\"start\":6833},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":8073,\"start\":8066},{\"end\":11286,\"start\":11279},{\"end\":12588,\"start\":12581},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17440,\"start\":17433},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18572,\"start\":18563},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":20191,\"start\":20183},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":20242,\"start\":20235},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":20707,\"start\":20700},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":22392,\"start\":22385},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":27700,\"start\":27693},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":29117,\"start\":29109}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2329,\"start\":2317},{\"end\":4602,\"start\":4599},{\"attributes\":{\"n\":\"2.\"},\"end\":8001,\"start\":7989},{\"attributes\":{\"n\":\"3.\"},\"end\":11511,\"start\":11498},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11531,\"start\":11514},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12659,\"start\":12642},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16550,\"start\":16533},{\"attributes\":{\"n\":\"3.4.\"},\"end\":19652,\"start\":19615},{\"attributes\":{\"n\":\"3.5.\"},\"end\":20634,\"start\":20618},{\"attributes\":{\"n\":\"4.\"},\"end\":22791,\"start\":22782},{\"end\":24180,\"start\":24162},{\"end\":24360,\"start\":24357},{\"end\":24388,\"start\":24370},{\"end\":24593,\"start\":24588},{\"end\":24615,\"start\":24596},{\"end\":24812,\"start\":24807},{\"end\":24832,\"start\":24815},{\"end\":25060,\"start\":25055},{\"end\":25084,\"start\":25063},{\"attributes\":{\"n\":\"5.\"},\"end\":25737,\"start\":25727},{\"attributes\":{\"n\":\"6.\"},\"end\":27626,\"start\":27619},{\"attributes\":{\"n\":\"7.\"},\"end\":29245,\"start\":29235},{\"attributes\":{\"n\":\"8.\"},\"end\":31316,\"start\":31306},{\"end\":32337,\"start\":32327},{\"end\":32543,\"start\":32533},{\"end\":32594,\"start\":32585},{\"end\":32661,\"start\":32652},{\"end\":32845,\"start\":32836},{\"end\":33433,\"start\":33424},{\"end\":33472,\"start\":33463},{\"end\":33617,\"start\":33608},{\"end\":34401,\"start\":34392},{\"end\":34742,\"start\":34733}]", "table": "[{\"end\":33422,\"start\":33285},{\"end\":33606,\"start\":33474},{\"end\":34390,\"start\":34005},{\"end\":34731,\"start\":34608}]", "figure_caption": "[{\"end\":32055,\"start\":31931},{\"end\":32325,\"start\":32058},{\"end\":32531,\"start\":32339},{\"end\":32583,\"start\":32545},{\"end\":32650,\"start\":32596},{\"end\":32834,\"start\":32663},{\"end\":33250,\"start\":32847},{\"end\":33285,\"start\":33253},{\"end\":33461,\"start\":33435},{\"end\":33933,\"start\":33619},{\"end\":34005,\"start\":33936},{\"end\":34608,\"start\":34403},{\"end\":34771,\"start\":34744}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13329,\"start\":13321},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14048,\"start\":14040},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15643,\"start\":15635},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21883,\"start\":21875}]", "bib_author_first_name": "[{\"end\":35135,\"start\":35134},{\"end\":35137,\"start\":35136},{\"end\":35150,\"start\":35149},{\"end\":35384,\"start\":35383},{\"end\":35395,\"start\":35394},{\"end\":35397,\"start\":35396},{\"end\":35407,\"start\":35406},{\"end\":35640,\"start\":35639},{\"end\":35649,\"start\":35648},{\"end\":35983,\"start\":35982},{\"end\":35996,\"start\":35992},{\"end\":36005,\"start\":36004},{\"end\":36012,\"start\":36011},{\"end\":36271,\"start\":36270},{\"end\":36283,\"start\":36282},{\"end\":36292,\"start\":36291},{\"end\":36528,\"start\":36527},{\"end\":36926,\"start\":36925},{\"end\":36938,\"start\":36937},{\"end\":36945,\"start\":36944},{\"end\":37175,\"start\":37174},{\"end\":37177,\"start\":37176},{\"end\":37188,\"start\":37187},{\"end\":37190,\"start\":37189},{\"end\":37201,\"start\":37200},{\"end\":37213,\"start\":37212},{\"end\":37215,\"start\":37214},{\"end\":37225,\"start\":37224},{\"end\":37227,\"start\":37226},{\"end\":37240,\"start\":37236},{\"end\":37244,\"start\":37243},{\"end\":37574,\"start\":37573},{\"end\":37581,\"start\":37580},{\"end\":37588,\"start\":37587},{\"end\":37963,\"start\":37962},{\"end\":37970,\"start\":37969},{\"end\":37978,\"start\":37977},{\"end\":37987,\"start\":37986},{\"end\":37993,\"start\":37992},{\"end\":38001,\"start\":38000},{\"end\":38008,\"start\":38007},{\"end\":38016,\"start\":38015},{\"end\":38025,\"start\":38024},{\"end\":38033,\"start\":38031},{\"end\":38037,\"start\":38036},{\"end\":38385,\"start\":38384},{\"end\":38389,\"start\":38386},{\"end\":38400,\"start\":38399},{\"end\":38402,\"start\":38401},{\"end\":38413,\"start\":38412},{\"end\":38424,\"start\":38420},{\"end\":38426,\"start\":38425},{\"end\":38436,\"start\":38435},{\"end\":38444,\"start\":38443},{\"end\":38456,\"start\":38455},{\"end\":38465,\"start\":38464},{\"end\":38478,\"start\":38477},{\"end\":38494,\"start\":38493},{\"end\":38496,\"start\":38495},{\"end\":38804,\"start\":38803},{\"end\":38811,\"start\":38810},{\"end\":38820,\"start\":38819},{\"end\":38829,\"start\":38828},{\"end\":38837,\"start\":38836},{\"end\":39092,\"start\":39091},{\"end\":39096,\"start\":39093},{\"end\":39108,\"start\":39107},{\"end\":39112,\"start\":39109},{\"end\":39125,\"start\":39124},{\"end\":39127,\"start\":39126},{\"end\":39137,\"start\":39136},{\"end\":39141,\"start\":39138},{\"end\":39151,\"start\":39150},{\"end\":39153,\"start\":39152},{\"end\":39164,\"start\":39163},{\"end\":39528,\"start\":39527},{\"end\":39539,\"start\":39538},{\"end\":39551,\"start\":39550},{\"end\":39560,\"start\":39559},{\"end\":39878,\"start\":39877},{\"end\":39885,\"start\":39884},{\"end\":39893,\"start\":39892},{\"end\":39899,\"start\":39898},{\"end\":39908,\"start\":39907},{\"end\":39916,\"start\":39915},{\"end\":39924,\"start\":39923},{\"end\":39932,\"start\":39931},{\"end\":40288,\"start\":40287},{\"end\":40299,\"start\":40298},{\"end\":40495,\"start\":40494},{\"end\":40507,\"start\":40506},{\"end\":40521,\"start\":40520},{\"end\":40523,\"start\":40522},{\"end\":40532,\"start\":40531},{\"end\":40534,\"start\":40533},{\"end\":40848,\"start\":40847},{\"end\":40860,\"start\":40859},{\"end\":40862,\"start\":40861},{\"end\":40871,\"start\":40870},{\"end\":40882,\"start\":40881},{\"end\":40893,\"start\":40892},{\"end\":41314,\"start\":41313},{\"end\":41327,\"start\":41326},{\"end\":41336,\"start\":41335},{\"end\":41347,\"start\":41346},{\"end\":41808,\"start\":41807},{\"end\":41821,\"start\":41820},{\"end\":41828,\"start\":41827},{\"end\":42296,\"start\":42295},{\"end\":42307,\"start\":42306},{\"end\":42522,\"start\":42521},{\"end\":42533,\"start\":42532},{\"end\":42535,\"start\":42534},{\"end\":42836,\"start\":42835},{\"end\":42842,\"start\":42841},{\"end\":42850,\"start\":42849},{\"end\":42856,\"start\":42855},{\"end\":43244,\"start\":43243},{\"end\":43252,\"start\":43251},{\"end\":43553,\"start\":43552},{\"end\":43563,\"start\":43562},{\"end\":43570,\"start\":43569},{\"end\":43583,\"start\":43582},{\"end\":43595,\"start\":43594},{\"end\":43604,\"start\":43603},{\"end\":43617,\"start\":43616},{\"end\":44030,\"start\":44029},{\"end\":44350,\"start\":44349},{\"end\":44358,\"start\":44357},{\"end\":44365,\"start\":44364},{\"end\":44367,\"start\":44366},{\"end\":44681,\"start\":44680},{\"end\":44688,\"start\":44687},{\"end\":44709,\"start\":44708},{\"end\":45116,\"start\":45115},{\"end\":45123,\"start\":45122},{\"end\":45125,\"start\":45124},{\"end\":45134,\"start\":45133},{\"end\":45141,\"start\":45140},{\"end\":45148,\"start\":45147},{\"end\":45496,\"start\":45495},{\"end\":45506,\"start\":45505},{\"end\":45520,\"start\":45519},{\"end\":45522,\"start\":45521},{\"end\":45531,\"start\":45530},{\"end\":45542,\"start\":45541},{\"end\":45553,\"start\":45552},{\"end\":45564,\"start\":45563},{\"end\":45572,\"start\":45571},{\"end\":45582,\"start\":45581},{\"end\":45590,\"start\":45589},{\"end\":45602,\"start\":45597},{\"end\":45606,\"start\":45605},{\"end\":46040,\"start\":46039},{\"end\":46048,\"start\":46047},{\"end\":46054,\"start\":46053},{\"end\":46060,\"start\":46059},{\"end\":46066,\"start\":46065}]", "bib_author_last_name": "[{\"end\":35147,\"start\":35138},{\"end\":35154,\"start\":35151},{\"end\":35392,\"start\":35385},{\"end\":35404,\"start\":35398},{\"end\":35413,\"start\":35408},{\"end\":35646,\"start\":35641},{\"end\":35657,\"start\":35650},{\"end\":35990,\"start\":35984},{\"end\":36002,\"start\":35997},{\"end\":36009,\"start\":36006},{\"end\":36022,\"start\":36013},{\"end\":36280,\"start\":36272},{\"end\":36289,\"start\":36284},{\"end\":36299,\"start\":36293},{\"end\":36532,\"start\":36529},{\"end\":36935,\"start\":36927},{\"end\":36942,\"start\":36939},{\"end\":36954,\"start\":36946},{\"end\":37185,\"start\":37178},{\"end\":37198,\"start\":37191},{\"end\":37210,\"start\":37202},{\"end\":37222,\"start\":37216},{\"end\":37234,\"start\":37228},{\"end\":37578,\"start\":37575},{\"end\":37585,\"start\":37582},{\"end\":37600,\"start\":37589},{\"end\":37967,\"start\":37964},{\"end\":37975,\"start\":37971},{\"end\":37984,\"start\":37979},{\"end\":37990,\"start\":37988},{\"end\":37998,\"start\":37994},{\"end\":38005,\"start\":38002},{\"end\":38013,\"start\":38009},{\"end\":38022,\"start\":38017},{\"end\":38029,\"start\":38026},{\"end\":38397,\"start\":38390},{\"end\":38410,\"start\":38403},{\"end\":38418,\"start\":38414},{\"end\":38433,\"start\":38427},{\"end\":38441,\"start\":38437},{\"end\":38453,\"start\":38445},{\"end\":38462,\"start\":38457},{\"end\":38475,\"start\":38466},{\"end\":38491,\"start\":38479},{\"end\":38501,\"start\":38497},{\"end\":38808,\"start\":38805},{\"end\":38817,\"start\":38812},{\"end\":38826,\"start\":38821},{\"end\":38834,\"start\":38830},{\"end\":38843,\"start\":38838},{\"end\":39105,\"start\":39097},{\"end\":39122,\"start\":39113},{\"end\":39134,\"start\":39128},{\"end\":39148,\"start\":39142},{\"end\":39161,\"start\":39154},{\"end\":39169,\"start\":39165},{\"end\":39536,\"start\":39529},{\"end\":39548,\"start\":39540},{\"end\":39557,\"start\":39552},{\"end\":39565,\"start\":39561},{\"end\":39882,\"start\":39879},{\"end\":39890,\"start\":39886},{\"end\":39896,\"start\":39894},{\"end\":39905,\"start\":39900},{\"end\":39913,\"start\":39909},{\"end\":39921,\"start\":39917},{\"end\":39929,\"start\":39925},{\"end\":39935,\"start\":39933},{\"end\":40296,\"start\":40289},{\"end\":40302,\"start\":40300},{\"end\":40504,\"start\":40496},{\"end\":40518,\"start\":40508},{\"end\":40529,\"start\":40524},{\"end\":40546,\"start\":40535},{\"end\":40857,\"start\":40849},{\"end\":40868,\"start\":40863},{\"end\":40879,\"start\":40872},{\"end\":40890,\"start\":40883},{\"end\":40903,\"start\":40894},{\"end\":41324,\"start\":41315},{\"end\":41333,\"start\":41328},{\"end\":41344,\"start\":41337},{\"end\":41353,\"start\":41348},{\"end\":41818,\"start\":41809},{\"end\":41825,\"start\":41822},{\"end\":41834,\"start\":41829},{\"end\":42304,\"start\":42297},{\"end\":42322,\"start\":42308},{\"end\":42530,\"start\":42523},{\"end\":42541,\"start\":42536},{\"end\":42839,\"start\":42837},{\"end\":42847,\"start\":42843},{\"end\":42853,\"start\":42851},{\"end\":42864,\"start\":42857},{\"end\":43249,\"start\":43245},{\"end\":43260,\"start\":43253},{\"end\":43560,\"start\":43554},{\"end\":43567,\"start\":43564},{\"end\":43580,\"start\":43571},{\"end\":43592,\"start\":43584},{\"end\":43601,\"start\":43596},{\"end\":43614,\"start\":43605},{\"end\":43629,\"start\":43618},{\"end\":44040,\"start\":44031},{\"end\":44355,\"start\":44351},{\"end\":44362,\"start\":44359},{\"end\":44373,\"start\":44368},{\"end\":44685,\"start\":44682},{\"end\":44706,\"start\":44689},{\"end\":44713,\"start\":44710},{\"end\":45120,\"start\":45117},{\"end\":45131,\"start\":45126},{\"end\":45138,\"start\":45135},{\"end\":45145,\"start\":45142},{\"end\":45152,\"start\":45149},{\"end\":45503,\"start\":45497},{\"end\":45517,\"start\":45507},{\"end\":45528,\"start\":45523},{\"end\":45539,\"start\":45532},{\"end\":45550,\"start\":45543},{\"end\":45561,\"start\":45554},{\"end\":45569,\"start\":45565},{\"end\":45579,\"start\":45573},{\"end\":45587,\"start\":45583},{\"end\":45595,\"start\":45591},{\"end\":46045,\"start\":46041},{\"end\":46051,\"start\":46049},{\"end\":46057,\"start\":46055},{\"end\":46063,\"start\":46061},{\"end\":46069,\"start\":46067}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13578700},\"end\":35338,\"start\":35093},{\"attributes\":{\"doi\":\"arXiv:2004.05150\",\"id\":\"b1\"},\"end\":35548,\"start\":35340},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":243837396},\"end\":35898,\"start\":35550},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52967399},\"end\":36210,\"start\":35900},{\"attributes\":{\"doi\":\"arXiv:2101.10421\",\"id\":\"b4\"},\"end\":36449,\"start\":36212},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":195742686},\"end\":36840,\"start\":36451},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4589433},\"end\":37170,\"start\":36842},{\"attributes\":{\"id\":\"b7\"},\"end\":37343,\"start\":37172},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":207583334},\"end\":37502,\"start\":37345},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":227230468},\"end\":37958,\"start\":37504},{\"attributes\":{\"id\":\"b10\"},\"end\":38145,\"start\":37960},{\"attributes\":{\"doi\":\"arXiv:2102.05281\",\"id\":\"b11\"},\"end\":38327,\"start\":38147},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":33285731},\"end\":38763,\"start\":38329},{\"attributes\":{\"id\":\"b13\"},\"end\":39034,\"start\":38765},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":244711514},\"end\":39449,\"start\":39036},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52158121},\"end\":39737,\"start\":39451},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":240142320},\"end\":40213,\"start\":39739},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9855359},\"end\":40492,\"start\":40215},{\"attributes\":{\"doi\":\"arXiv:1805.06816\",\"id\":\"b18\"},\"end\":40782,\"start\":40494},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235097193},\"end\":41250,\"start\":40784},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11816014},\"end\":41745,\"start\":41252},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":47018994},\"end\":42249,\"start\":41747},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":27080528},\"end\":42444,\"start\":42251},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":30193453},\"end\":42767,\"start\":42446},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":67856085},\"end\":43115,\"start\":42769},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218974465},\"end\":43456,\"start\":43117},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":233927972},\"end\":43912,\"start\":43458},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":22282728},\"end\":44270,\"start\":43914},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":210838505},\"end\":44604,\"start\":44272},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":218486765},\"end\":45113,\"start\":44606},{\"attributes\":{\"doi\":\"arXiv:2010.16021\",\"id\":\"b30\"},\"end\":45448,\"start\":45115},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":220831004},\"end\":45988,\"start\":45450},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":219965751},\"end\":46311,\"start\":45990}]", "bib_title": "[{\"end\":35132,\"start\":35093},{\"end\":35637,\"start\":35550},{\"end\":35980,\"start\":35900},{\"end\":36525,\"start\":36451},{\"end\":36923,\"start\":36842},{\"end\":37403,\"start\":37345},{\"end\":37571,\"start\":37504},{\"end\":38382,\"start\":38329},{\"end\":38801,\"start\":38765},{\"end\":39089,\"start\":39036},{\"end\":39525,\"start\":39451},{\"end\":39875,\"start\":39739},{\"end\":40285,\"start\":40215},{\"end\":40845,\"start\":40784},{\"end\":41311,\"start\":41252},{\"end\":41805,\"start\":41747},{\"end\":42293,\"start\":42251},{\"end\":42519,\"start\":42446},{\"end\":42833,\"start\":42769},{\"end\":43241,\"start\":43117},{\"end\":43550,\"start\":43458},{\"end\":44027,\"start\":43914},{\"end\":44347,\"start\":44272},{\"end\":44678,\"start\":44606},{\"end\":45493,\"start\":45450},{\"end\":46037,\"start\":45990}]", "bib_author": "[{\"end\":35149,\"start\":35134},{\"end\":35156,\"start\":35149},{\"end\":35394,\"start\":35383},{\"end\":35406,\"start\":35394},{\"end\":35415,\"start\":35406},{\"end\":35648,\"start\":35639},{\"end\":35659,\"start\":35648},{\"end\":35992,\"start\":35982},{\"end\":36004,\"start\":35992},{\"end\":36011,\"start\":36004},{\"end\":36024,\"start\":36011},{\"end\":36282,\"start\":36270},{\"end\":36291,\"start\":36282},{\"end\":36301,\"start\":36291},{\"end\":36534,\"start\":36527},{\"end\":36937,\"start\":36925},{\"end\":36944,\"start\":36937},{\"end\":36956,\"start\":36944},{\"end\":37187,\"start\":37174},{\"end\":37200,\"start\":37187},{\"end\":37212,\"start\":37200},{\"end\":37224,\"start\":37212},{\"end\":37236,\"start\":37224},{\"end\":37243,\"start\":37236},{\"end\":37247,\"start\":37243},{\"end\":37580,\"start\":37573},{\"end\":37587,\"start\":37580},{\"end\":37602,\"start\":37587},{\"end\":37969,\"start\":37962},{\"end\":37977,\"start\":37969},{\"end\":37986,\"start\":37977},{\"end\":37992,\"start\":37986},{\"end\":38000,\"start\":37992},{\"end\":38007,\"start\":38000},{\"end\":38015,\"start\":38007},{\"end\":38024,\"start\":38015},{\"end\":38031,\"start\":38024},{\"end\":38036,\"start\":38031},{\"end\":38040,\"start\":38036},{\"end\":38399,\"start\":38384},{\"end\":38412,\"start\":38399},{\"end\":38420,\"start\":38412},{\"end\":38435,\"start\":38420},{\"end\":38443,\"start\":38435},{\"end\":38455,\"start\":38443},{\"end\":38464,\"start\":38455},{\"end\":38477,\"start\":38464},{\"end\":38493,\"start\":38477},{\"end\":38503,\"start\":38493},{\"end\":38810,\"start\":38803},{\"end\":38819,\"start\":38810},{\"end\":38828,\"start\":38819},{\"end\":38836,\"start\":38828},{\"end\":38845,\"start\":38836},{\"end\":39107,\"start\":39091},{\"end\":39124,\"start\":39107},{\"end\":39136,\"start\":39124},{\"end\":39150,\"start\":39136},{\"end\":39163,\"start\":39150},{\"end\":39171,\"start\":39163},{\"end\":39538,\"start\":39527},{\"end\":39550,\"start\":39538},{\"end\":39559,\"start\":39550},{\"end\":39567,\"start\":39559},{\"end\":39884,\"start\":39877},{\"end\":39892,\"start\":39884},{\"end\":39898,\"start\":39892},{\"end\":39907,\"start\":39898},{\"end\":39915,\"start\":39907},{\"end\":39923,\"start\":39915},{\"end\":39931,\"start\":39923},{\"end\":39937,\"start\":39931},{\"end\":40298,\"start\":40287},{\"end\":40304,\"start\":40298},{\"end\":40506,\"start\":40494},{\"end\":40520,\"start\":40506},{\"end\":40531,\"start\":40520},{\"end\":40548,\"start\":40531},{\"end\":40859,\"start\":40847},{\"end\":40870,\"start\":40859},{\"end\":40881,\"start\":40870},{\"end\":40892,\"start\":40881},{\"end\":40905,\"start\":40892},{\"end\":41326,\"start\":41313},{\"end\":41335,\"start\":41326},{\"end\":41346,\"start\":41335},{\"end\":41355,\"start\":41346},{\"end\":41820,\"start\":41807},{\"end\":41827,\"start\":41820},{\"end\":41836,\"start\":41827},{\"end\":42306,\"start\":42295},{\"end\":42324,\"start\":42306},{\"end\":42532,\"start\":42521},{\"end\":42543,\"start\":42532},{\"end\":42841,\"start\":42835},{\"end\":42849,\"start\":42841},{\"end\":42855,\"start\":42849},{\"end\":42866,\"start\":42855},{\"end\":43251,\"start\":43243},{\"end\":43262,\"start\":43251},{\"end\":43562,\"start\":43552},{\"end\":43569,\"start\":43562},{\"end\":43582,\"start\":43569},{\"end\":43594,\"start\":43582},{\"end\":43603,\"start\":43594},{\"end\":43616,\"start\":43603},{\"end\":43631,\"start\":43616},{\"end\":44042,\"start\":44029},{\"end\":44357,\"start\":44349},{\"end\":44364,\"start\":44357},{\"end\":44375,\"start\":44364},{\"end\":44687,\"start\":44680},{\"end\":44708,\"start\":44687},{\"end\":44715,\"start\":44708},{\"end\":45122,\"start\":45115},{\"end\":45133,\"start\":45122},{\"end\":45140,\"start\":45133},{\"end\":45147,\"start\":45140},{\"end\":45154,\"start\":45147},{\"end\":45505,\"start\":45495},{\"end\":45519,\"start\":45505},{\"end\":45530,\"start\":45519},{\"end\":45541,\"start\":45530},{\"end\":45552,\"start\":45541},{\"end\":45563,\"start\":45552},{\"end\":45571,\"start\":45563},{\"end\":45581,\"start\":45571},{\"end\":45589,\"start\":45581},{\"end\":45597,\"start\":45589},{\"end\":45605,\"start\":45597},{\"end\":45609,\"start\":45605},{\"end\":46047,\"start\":46039},{\"end\":46053,\"start\":46047},{\"end\":46059,\"start\":46053},{\"end\":46065,\"start\":46059},{\"end\":46071,\"start\":46065}]", "bib_venue": "[{\"end\":35200,\"start\":35156},{\"end\":35381,\"start\":35340},{\"end\":35703,\"start\":35659},{\"end\":36033,\"start\":36024},{\"end\":36268,\"start\":36212},{\"end\":36602,\"start\":36534},{\"end\":36989,\"start\":36956},{\"end\":37414,\"start\":37405},{\"end\":37679,\"start\":37602},{\"end\":38231,\"start\":38163},{\"end\":38518,\"start\":38503},{\"end\":38881,\"start\":38845},{\"end\":39190,\"start\":39171},{\"end\":39572,\"start\":39567},{\"end\":39952,\"start\":39937},{\"end\":40337,\"start\":40304},{\"end\":40624,\"start\":40564},{\"end\":40971,\"start\":40905},{\"end\":41441,\"start\":41355},{\"end\":41923,\"start\":41836},{\"end\":42328,\"start\":42324},{\"end\":42576,\"start\":42543},{\"end\":42921,\"start\":42866},{\"end\":43266,\"start\":43262},{\"end\":43661,\"start\":43631},{\"end\":44075,\"start\":44042},{\"end\":44408,\"start\":44375},{\"end\":44802,\"start\":44715},{\"end\":45263,\"start\":45170},{\"end\":45658,\"start\":45609},{\"end\":46130,\"start\":46071},{\"end\":36657,\"start\":36604},{\"end\":37743,\"start\":37681},{\"end\":41024,\"start\":40973},{\"end\":41514,\"start\":41443},{\"end\":41997,\"start\":41925},{\"end\":44428,\"start\":44410},{\"end\":44876,\"start\":44804}]"}}}, "year": 2023, "month": 12, "day": 17}