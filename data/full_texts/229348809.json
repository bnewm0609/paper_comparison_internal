{"id": 229348809, "updated": "2023-10-06 07:51:01.121", "metadata": {"title": "Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video", "authors": "[{\"first\":\"Edgar\",\"last\":\"Tretschk\",\"middle\":[]},{\"first\":\"Ayush\",\"last\":\"Tewari\",\"middle\":[]},{\"first\":\"Vladislav\",\"last\":\"Golyanik\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Zollhofer\",\"middle\":[]},{\"first\":\"Christoph\",\"last\":\"Lassner\",\"middle\":[]},{\"first\":\"Christian\",\"last\":\"Theobalt\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 12, "day": 22}, "abstract": "We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input (e.g., from a monocular video recording), and creates a high-quality space-time geometry and appearance representation. We show that a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, e.g. a `bullet-time' video effect. NR-NeRF disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly. We also propose a novel rigidity network to better constrain rigid regions of the scene, leading to more stable results. The ray bending and rigidity network are trained without explicit supervision. Our formulation enables dense correspondence estimation across views and time, and compelling video editing applications such as motion exaggeration. Our code will be open sourced.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2012.12247", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/TretschkTGZLT21", "doi": "10.1109/iccv48922.2021.01272"}}, "content": {"source": {"pdf_hash": "eaee62ae051a9cd8065db7f5a444a3209d31e989", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.12247v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1e6aeae38a5ac74071b2bbd0496265ffd457a3e7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/eaee62ae051a9cd8065db7f5a444a3209d31e989.txt", "contents": "\nNon-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video\n\n\nEdgar Tretschk \nMPI for Informatics, SIC\nMPI for Informatics, SIC\nFacebook Reality Labs Research\nFacebook Reality Labs Research\nMPI for Informatics, SIC\nMPI for Informatics\nSIC\n\nAyush Tewari \nMPI for Informatics, SIC\nMPI for Informatics, SIC\nFacebook Reality Labs Research\nFacebook Reality Labs Research\nMPI for Informatics, SIC\nMPI for Informatics\nSIC\n\nVladislav Golyanik \nMPI for Informatics, SIC\nMPI for Informatics, SIC\nFacebook Reality Labs Research\nFacebook Reality Labs Research\nMPI for Informatics, SIC\nMPI for Informatics\nSIC\n\nMichael Zollh\u00f6fer \nMPI for Informatics, SIC\nMPI for Informatics, SIC\nFacebook Reality Labs Research\nFacebook Reality Labs Research\nMPI for Informatics, SIC\nMPI for Informatics\nSIC\n\nChristoph Lassner \nMPI for Informatics, SIC\nMPI for Informatics, SIC\nFacebook Reality Labs Research\nFacebook Reality Labs Research\nMPI for Informatics, SIC\nMPI for Informatics\nSIC\n\nChristian Theobalt \nMPI for Informatics, SIC\nMPI for Informatics, SIC\nFacebook Reality Labs Research\nFacebook Reality Labs Research\nMPI for Informatics, SIC\nMPI for Informatics\nSIC\n\nNon-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video\n\nFigure 1. Given a monocular image sequence, NR-NeRF reconstructs a single canonical neural radiance field to represent geometry and appearance, and a per-time-step deformation field. We can render the scene into a novel spatio-temporal camera trajectory that significantly differs from the input trajectory. NR-NeRF also learns rigidity scores and correspondences without direct supervision on either. We can use the rigidity scores to remove the foreground, we can supersample along the time dimension, and we can exaggerate or dampen motion.AbstractWe present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input (e.g., from a monocular video recording), and creates a high-quality space-time geometry and appearance representation. We show that a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, e.g. a 'bullet-time' video effect. NR-NeRF disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly. We also propose a novel rigidity network to better constrain rigid regions of the scene, leading to more stable results. The ray bending and rigidity network are trained without explicit supervision. Our formulation enables dense correspondence estimation across views and time, and compelling video editing applications such as motion exaggeration. Our code will be open sourced.\n\nIntroduction\n\nFree viewpoint rendering is a well-studied problem due to its wide range of applications in movies and virtual/augmented reality [76,9,48]. In this work, we are interested in dynamic scenes, which change over time, from novel user-controlled viewpoints. Traditionally, multi-view recordings are required for free viewpoint rendering of dynamic scenes [99,86,56]. However, such multi-view captures are expensive and cumbersome. We would like to enable the setting in which a casual user records a dynamic scene with a single, moving consumer-grade camera. Access to only a monocular video of the deforming scene leads to a severely under-constrained problem. Most existing approaches thus limit themselves to a single object category, such as the human body [23,91,32] or face [12]. Some approaches allow for the reconstruction of general non-rigid objects [104,17,33,72], but most methods only reconstruct the geometry without the appearance of the objects in the scene. In contrast, our objective is to reconstruct a general dynamic scene, including its appearance, such that it can be rendered from novel spatio-temporal viewpoints.\n\nRecent neural rendering approaches have shown impressive novel-view synthesis of general static scenes from multi-view input [82]. These approaches represent scenes using trained neural networks and rely on less constraints about the type of scene, compared to traditional approaches. The closest prior work to our method is NeRF [46], which learns a continuous volume of the scene encoded in a neural network using multiple camera views. However, NeRF assumes the scene to be static. Neural Volumes [41] is another closely related approach that uses multiple views of a deforming scene to enable free viewpoint rendering. However, it uses a fixed-size voxel grid to represent the reconstruction of the scene, restricting the resolution. In addition, it requires multi-view input for training, which limits the applicability to in-the-wild outdoor settings or existing monocular footage. Our new neural rendering approach instead targets the more challenging setting of using just a monocular video of a general dynamic scene. Due to the non-rigidity, each image of the video records a different, deformed state of the scene, violating the constraints of standard neural rendering approaches. Our approach disentangles the observations in any image into a canonical scene and its deformations, without direct supervision on either.\n\nWe tackle this problem using several innovations. We represent the non-rigid scene by two components: (1) a canonical neural radiance field for capturing geometry and appearance and (2) the scene deformation field. The canonical volume is a static representation of the scene encoded as a Multi-Layered Perceptron (MLP), which is not directly supervised. This volume is deformed into each individual image using the estimated scene deformation. Specifically, the scene deformation is implemented as ray bending, where straight camera rays can deform non-rigidly. The ray bending is modeled using an MLP that takes point samples on the ray as well as a latent code for each image as input. Both the ray bending and the canonical scene MLPs are jointly trained using the monocular observations. Since the ray bending MLP deforms the entire space independent of camera parameters, we can render the deforming volume from static or time-varying novel viewpoints after training.\n\nThe ray bending MLP disentangles the geometry of the scene from the scene deformations. The disentanglement is an underconstrained problem, which we tackle with further innovations. Our method assigns a rigidity score to every point in the canonical volume, which allows for the deformations to not affect the static regions in the scene. This rigidity component is jointly learned without any direct supervision. We also introduce multiple regularizers as additional soft-constraints: A regularizer on the deformation magnitude of the visible deformations encourages only sparse deformations of the volume, and thus helps to constrain the canonical volume. An additional divergence regularizer preserves the local shape, thereby constraining the Figure 2. We bend straight raysr from the deformed volume using a deformation-dependent ray-bending network b and a deformation-independent rigidity network w into a single static canonical neural radiance field volume v.\n\nrepresentation of hidden (partially occluded) regions that are not visible throughout the full video.\n\nOur results show high-fidelity reconstruction and novel view synthesis for a wide range of non-rigid scenes. Fig. 2 contains an overview of our method. To summarize, our main technical contributions are as follows:\n\n\u2022 A free viewpoint rendering method, NR-NeRF, that only requires a monocular video of the dynamic scene (Sec. 3). The spatio-temporal camera trajectory for test-time novel view synthesis can differ significantly from the trajectory of the input video. Moreover, we can extract dense correspondences relating arbitrary (input or novel) frames.\n\n\u2022 A rigidity network which can segment the scene into nonrigid foreground and rigid background without being directly supervised (Sec. 3.2).\n\n\u2022 Regularizers on the estimated deformations which constrain the problem by encouraging small volume preserving deformations (Sec. 3.3).\n\n\u2022 Several extensions for handling of view dependence and multi-view data, and applications of our technique for simple scene editing (Secs. 3).\n\nWe compare NR-NeRF to several methods for neural novel view rendering (Sec. 4). See our supplementary video for visualizations and Sec. S.10 for a discussion.\n\n\nRelated Work\n\n4D Reconstruction and Novel Viewpoint Rendering. Early methods for image-based novel and free-viewpoint rendering combined traditional concepts of multi-view camera geometry, explicit vision-based 3D shape and appearance reconstruction, and classical computer graphics or image-based rendering. These methods are based on light fields [35,19,4], multi-view stereo to capture dense depth maps [99], layered depth images [70], or representations using 3D point clouds [1,40,67], meshes [44,86] or surfels [62,7,88] for dynamic scenes. Passive geometry capture often leads to artifacts in scenes with severe occlusions and view-dependent appearance. Also, capturing temporally coherent representations in this way is challenging. More recently, the combination of multi-view stereo with fusion algorithms integrating implicit geometry over short time windows lead to improved results and short-term temporal coherence [10,55,22]. By using active depth cameras and such fusion-type reconstruction, dynamic scene capture and novel viewpoint rendering from a low number of cameras or a single camera were shown [96,81,25,97]. Several algorithms use variants of shape-from-silhouette to approximate real scene geometry, such as visual hull reconstruction or visual hulls improved via multi-view photoconsistency in [34,78]. While reconstruction is fast and feasible with fewer cameras, the coarse approximate geometry introduces rendering artifacts, and the reconstruction is usually limited to the separable foreground. Accurate and temporally coherent geometry is hard to capture in this way [5,6]. Some approaches use 3D templates and combine vision-based reconstruction with appearance modelling to enable free-viewpoint video relighting, e.g., by estimating reflectance models under general lighting or under controlled light stage illumination [83,36,49,21].\n\nThe progress in RGB-D sensors has enabled depth map capture from a single camera. Such sensors can been used for 3D reconstruction and completion of rigid environments [52] and non-rigid objects [103,51,28,75]. Other method classes allow capturing deformable geometry from sets of monocular views. Dense non-rigid structure from motion requires dense point tracks over input images, which are then factorized into camera poses and non-rigid 3D states per view [17,33,72]. The correspondences are usually obtained with dense optical flow methods, which makes them prone to occlusions and inaccuracies, and which can have a detrimental effect on the reconstructions. Monocular template-based methods do not assume dense matches and rely on a known 3D state of a deformable object (a 3D template), which is then tracked across time [61,53,95,92], or a training dataset with multiple object states [18,85]. Obtaining templates for complex objects and scenes is often non-trivial and requires specialized setups.\n\nIn contrast, our approach avoids explicit image-based 3D reconstruction. Moreover, we support arbitrary backgrounds whereas the discussed methods for monocular 3D reconstruction of deformable objects ignore it. Our approach enables free-viewpoint rendering of general deformable scenes with multiple objects and complex deformations with high visual fidelity, and yet does not rely on templates, 2D correspondences and multi-view setups. Neural Scene Representations and Neural Rendering. An emerging algorithm class uses neural networks to augment or replace established graphics and vision concepts for reconstruction and novel-view rendering. Most recent work is designed for static scenes [24,13,54,45,14,73,74,47,65]; methods for dynamic scenes are in their infancy.\n\nSeveral approaches address related problems to ours, such as generating images of humans in new poses [2,42,50,66] or body reenactment from monocular videos [8]. Other methods combine explicit dynamic scene reconstruction and traditional graphics rendering with neural rerendering [43,30,29,94]. Shysheya et al. [71] proposed a neural rendering approach for human avatars with texture warping. Zhu et al. [102] leverage geometric constraints and optical flow for synthesizing novel views of humans from a single image. Thies et al. [84] combine neural textures with the classical graphics pipeline for novel view synthesis of static objects and monocular video re-rendering. Neural Volumes [41] learn object models which can be animated and rendered from novel views, given multi-view video data. In contrast to all these methods, we require only a set of monocular views of a non-rigid scene as input and are able to render the scene from novel views. Non-Peer-Reviewed Reports. Since the intersection of neural scene representations and volumetric rendering has recently become a very active area of research with quickly evolving progress, several methods for dynamic settings have been proposed concurrently to ours. We mention them only for completeness since they are not peer-reviewed and thus do not constitute prior work. Some methods extend neural radiance fields to deforming faces [87,16,15,64]. Others focus on moving human bodies [89,60,79] or more general objects [63,38,90,58,11,37]. Our method differs from these by tackling general, real-world dynamic scenes from monocular RGB observations and camera parameters only, without using any other auxiliary method to estimate, for example, optical flow or depth.\n\n\nMethod\n\nOur Non-Rigid Neural Radiance Field (NR-NeRF) approach takes as input a set of N RGB images {\u0109 i } N \u22121 i=0 of a non-rigid scene and their extrinsics {R i , t i } N \u22121 i=0 and intrinsics {K i } N \u22121 i=0 . NF-NeRF then finds a single canonical neural radiance volume that can be deformed via ray bending to correctly render each\u0109 i . Specifically, we collect appearance and geometry information in the static canonical volume v parametrized by weights \u03b8. We model deformations by bending the straight rays sent out by a camera to obtain a deformed rendering of v. This ray bending is implemented as a ray bending MLP b with weights \u03c8. It maps, conditioned on the current deformation, 3D points (e.g., sampled from the straight rays) to 3D positions in v. The deformation conditioning takes the form of auto-decoded latent codes {l i } N \u22121 i=0 for each image i.\n\n\nBackground: Neural Radiance Fields\n\nWe first recap NeRF [46] for rigid scenes. NeRF renders a 3D volume into an image by accumulating color, weighted by accumulated transmittance and density, along camera rays. The 3D volume is parametrized by an MLP v(x, d) = (c, o) that regresses an RGB color c = c(x, d) \u2208 [0, 1] 3 and an opacity o = o(x) \u2208 [0, 1] for a point x \u2208 R 3 on a ray with direction d \u2208 R 3 .\n\nConsider a pixel (u, v) of an image\u0109 i . For a pinhole camera, the associated ray r u,v (j) = o + jd(u, v) can be calculated using R i , t i , and K i , which yield the ray origin o \u2208 R 3 and ray direction d(u, v) \u2208 R 3 . We can then integrate along the ray from the near plane j n to the far plane j f of the camera frustum to obtain the final color c at (u, v): (1) with V (j) = exp(\u2212 j jn o(r u,v (s)) ds) being the accumulated transmittance along the ray from j n up to j. In practice, the integrals are approximated by discrete samples x along the ray. NeRF employs a coarse volume v c with network weights \u03b8 c and a fine volume v f with network weights \u03b8 f . Both volumes have the same architecture, but do not share weights: \u03b8 = \u03b8 c\u222a \u03b8 f . When rendering a ray, v c is accessed first at uniformly distributed samples along the ray. These coarse samples are used to estimate the transmittance distribution, from which fine samples are sampled. v f is then evaluated at the combined set of coarse and fine sample points. We refer to the original paper for more details. Adaptations for NR-NeRF. We assume Lambertian materials and thus remove the view-dependent layers of rigid NeRF, i.e., we set c = c(x). Since each image corresponds to a different deformation of the volume in our non-rigid setting, we also learn a latent code for each time step, which is then used as input for the ray bending network which parameterizes scene deformations. The weights of this network and the latent codes are shared between v c and v f .\nc(ru,v) = j f jn V (j) \u00b7 o(ru,v(j)) \u00b7 c(ru,v(j), d(u, v)) dj ,\n\nDeformation Model\n\nThe original NeRF method [46] assumes rigidity and cannot handle non-rigid scenes. A na\u00efve approach to modeling deformations in the NeRF framework would be to condition the volume on the deformation (e.g., by conditioning it on time or a deformation latent code). We explore the latter option in the experiments in Sec. S.7. As we will show, apart from not providing hard correspondences, this na\u00efve approach only leads to satisfying results when reconstructing the input camera path, but gives implausible results for novel view synthesis. Instead, we explicitly model the consistency of geometry and appearance across time by disentangling them from the deformation.\n\nWe accumulate geometry and appearance from all frames into a single, non-deforming canonical volume. We employ general space warping (or ray bending) on top of the static canonical volume to model non-rigid deformations.\n\nFor an input image\u0109 i at training time, we want to render the canonical volume such that the image is reproduced. To that end, we need to un-do the deformation of the specific time step i by mapping the camera rays to the deformationindependent canonical volume. We first send out straight rays from the input camera. To account for the deformation, we then bend the straight rays such that sampling and subsequently rendering the canonical volume along the bent rays yields\u0109 i . We choose a very unrestricted parametrization of the ray bending, namely an MLP.\n\nSpecifically, we implement ray bending as a ray bending network b(x, l i ) \u2208 R 3 . For a point x, for example lying on a straight ray, the network regresses an offset under a deformation represented by l i . The offset is then added to x, thus bending the ray. Finally, we pass the new, bent ray point to the canonical volume, that is:\n(c, o) = v(x + b(x, l i )).\nNote that v is not conditioned on l i , which leads to the disentanglement of deformation (b and l i ) from geometry and appearance (v). We denote the bent version of the straight rayr asr li (j) =r(j) + b(r(j), l i ). Rigidity Network. However, we find that rigid parts of the scene are insufficiently constrained by this formulation. We reformulate b(x, l i ) \u2208 R 3 as the product of a raw offset b (x, l i ) and a rigidity mask\nw(x) \u2208 [0, 1], i.e., b(x, l i ) = w(x)b (x, l i ).\nFor rigid objects, we want to prevent deformations and hence desire w(x) = 0, while for non-rigid objects, we want w(x) > 0. This makes it easier for b to focus on the non-rigid parts of the scene, which change over time, since rigid parts can get masked out by the rigidity network w, which is jointly trained. Because the rigidity network is not conditioned on the latent code l i , it is forced to share knowledge about the rigidity of regions in the scene across time steps, which also ensures that parts of the rigid background that can be unregularized at certain time steps are nonetheless reconstructed at all time steps without any deformation.\n\n\nLosses\n\nWith the architecture specified, we next optimize all parameters (\u03b8, \u03c8, {l i } i ) jointly. We optimize the network weights as usual but auto-decode the latent codes l i [80,57]. Notation. For ease of presentation, we consider a single time step i and a single straight rayr with coarse ray pointsC = {r(j)} j\u2208C for a set C of uniformly sampled j \u2208 [j n , j f ] and fine ray pointsF = {r(j)} j\u2208F for a set F of importance-sampled j. For a latent code l, the bent rayr l givesC = {r l (j)} j\u2208C andF = {r l (j)} j\u2208F . The actual training uses a batch of randomly chosen rays from the training images. Reconstruction Loss. We adapt the data term from NeRF to our non-rigid setting as follows:\n\nwhere\u0109(r) is the ground-truth color of the pixel and c(S) is the estimated ray color on the set S of discrete ray points.\n\nWhile this reconstruction loss yields satisfactory results along the space-time camera trajectory of the input recording, we show later in Sec. 4.2 that it leads to undesirable renderings for novel views. We thus find it necessary to regularize the bending of rays with further priors. Offsets Loss. We regularize the offsets with a loss on their magnitude. Since we want visually unoccupied space (i.e., air) to be compressible and not hinder the optimization, we weigh the loss at each point by its opacity. However, this would still apply a high weight to completely occluded points along the ray, which leads to artifacts when rendering novel views. We thus additionally weigh by transmittance:\nL na\u00efve offsets = 1 |C| j\u2208C \u03b1 j \u00b7 b(r(j), l) 2\u2212w(r(j)) 2 ,(3)\nwhere we weigh each point by transmittance and occupancy \u03b1 j = V (j) \u00b7 o(r(j)). We do not back-propagate into \u03b1 j . However, as we show in Sec. 4, we find that applying the offsets loss to the masked offsets leads to an unstable background in novel views. We hypothesize that this is due to the multiplicative ambiguity between unmasked offsets and rigidity mask. We find that applying the loss to the regressed rigidity mask and raw offsets separately works better:\nL offsets = 1 |C| j\u2208C \u03b1j\u00b7 b (r(j), l) 2\u2212w(r(j)) 2 +\u03c9 rigidity w(r(j)) ,(4)\nwhere we penalize b instead of b. The exponent of the first term is a tweak to get two desirable properties that neither an 1 nor an 2 loss fulfills: For non-rigid objects (w closer to 1), it becomes an 1 loss, which has two advantages: (1) the gradient is independent of the magnitude of the offset, so unlike with an 2 loss, small and large offsets/motions are treated equally, and (2) relative to an 2 loss, it encourages sparsity in the offsets field, which fits our scenes. For rigid objects (w closer to 0), it becomes an 2 loss, which tampers off in its gradient magnitude as the offset magnitude approaches 0, preventing noisy gradients that an 1 loss has for the tiny offsets of rigid objects. Divergence Loss. Since the offsets loss only constrains visible areas, we introduce additional regularization of hidden areas. Inspired by local, isometric shape preservation from computer graphics, like as-rigid-as-possible regularization for surfaces [77,27] or volume preservation for volumes [75], we seek to preserve the local shape after deformation. To that end, we propose to regularize the absolute value of the divergence of the offsets field. The Helmholtz decomposition [3] allows to split any twice-differentiable 3D vector field on a bounded domain into a sum of a rotation-free and a divergence-free vector fields. Thus, by penalizing the divergence, we encourage the vector field to be composed primarily of translations and rotations, effectively preserving volume. The divergence loss is:\nL divergence = 1 |C| j\u2208C w j \u00b7 |div(b(r(j), l))| 2 ,(5)\nwhere we do not back-propagate into w j = o(r(j)), and we take the divergence div of b w.r.t. the positionr(j). We employ FFJORD's [20] fast, unbiased divergence estimation, which is three times less computationally expensive than an exact computation. The divergence is defined as:\ndiv(b(x)) = Tr db(x) dx = \u2202b(x) x \u2202x + \u2202b(x) y \u2202y + \u2202b(x) z \u2202z , (6) where b(x) k \u2208 R is the k-th component of b(x), Tr(\u00b7)\nis the trace operator, and db(x) dx is the 3 \u00d7 3 Jacobian matrix. Na\u00efvely computing the divergence with PyTorch's automatic differentiation requires three backward passes, one for each term of the sum. Instead, the authors of FFJORD [20] use Hutchinson's trace estimator [26]:\nTr(A) = E e [e T Ae] .(7)\nHere, e is Gaussian-distributed. The single-sample Monte-Carlo estimator implied by this expectation can be computed with a single backward pass. Full Loss. We combine all losses to obtain the full loss:\nL = L data + \u03c9 offsets L offsets + \u03c9 divergence L divergence ,(8)\nwhere the weights \u03c9 rigidity , \u03c9 offsets , and \u03c9 offsets are scenespecific since we consider a variety of non-rigid scene types. Our implementation uses the structure-from-motion method COLMAP [67] to estimate the camera parameters. For further training and implementation details, we refer to the supplemental material. We will release our source code.\n\n\nResults\n\nWe present qualitative results of our method, including rigidity scores and correspondences, by rendering into input and novel spatio-temporal views in Sec. 4.1.Turning to the inner workings, Sec. 4.2 investigates the crucial design choices we made to improve novel view quality. We conclude the evaluation of our approach in Sec. S.7 by comparing to prior work and a baseline approach. Finally, we show simple scene-editing results in Sec. S.9. In the supplemental material, we provide information on data capture and show extensions to multi-view data and view-dependent effects.\n\n\nQualitative Results\n\nWe present qualitative results of NR-NeRF by rendering the scene from input and novel spatio-temporal views. We also visualize the additional outputs of our method. Input Reconstruction and Novel View Synthesis. Fig. 3 shows examples of input reconstruction and novel view synthesis with NR-NeRF. As the center column shows, the input is reconstructed faithfully. This enables high-quality novel view synthesis, example results can be found in the third column. We can freely move the camera in areas around the original camera paths and specify the time step. Rigidity. NR-NeRF estimates rigidity scores without supervision to improve background stability in novel-view renderings. We show examples in Fig. 4 and find that the background is consistently scored as highly rigid while foreground is correctly estimated to be rather non-rigid.\n\nCorrespondences. Another side effect of our proposed approach is the ability to estimate consistent dense 3D correspondences into the canonical model across different camera views and time steps. Fig. 4 shows examples.\n\n\nAblation Study\n\nAfter this look at the outputs of NR-NeRF, we now turn to its internal workings. Specifically, since we aim for convincing novel-view renderings, we take a closer look at the impact of some of our design choices on the foreground and background stability of our novel-view results. Setup. We investigate the necessity of all regularization losses by removing each loss individually and all of them at once. Next, we remove the rigidity network to see the impact on background stability. Finally, we determine whether Without L divergence No regularization Ours Figure 5. Ablation Study. We render the scene into novel views to determine the stability of the non-rigid part after removing the divergence loss, all regularization losses, and none of the losses.\n\napplying the offsets loss separately on both the regressed rigidity and the unmasked offsets, i.e., L offsets in our method, or directly on the masked offsets, L na\u00efve offsets , works better.\n\nResults. We find that L divergence is crucial for stable deformations of the non-rigid objects in the foreground, see Fig. 5. On the other side, the interplay of all of the remaining design choices is necessary to stabilize the rigid background as Fig. 6 shows. The supplemental video contains video examples that highlight the instability in these cases.\n\n\nComparisons\n\nHaving only considered our method in isolation so far, we next compare NR-NeRF to prior work and a baseline. In this section, we split the images into training and test sets by partitioning the temporally-ordered images into consecutive blocks of length 16 each, with the first twelve for the training set and the remaining four for the test set. Prior Work and Baseline. We start with the trivial baseline of rigid NeRF [46], which cannot handle dynamic scenes. We consider two variants: view-dependent rigid NeRF, as in the original method [46], and view-independent rigid NeRF, where we remove the view-direction conditioning. We next introduce na\u00efve NR-NeRF, which adds na\u00efve support for dynamic scenes to rigid NeRF: We condition the neural radiance fields volume on the latent code l i , i.e., (c, o) = v(x, l i ). For test images i, we backpropagate gradients into the corresponding latent code l i . We do the same for NR-NeRF in order to optimize for the test latent codes. Note that test images solely influence test latent codes, as is typical for auto-decoding [57]. Finally, we compare to Neural Volumes [41], for which we use the official code release. We consider two variants: (1) as in [41], the geometry and appearance template is conditioned on the latent code (NV), and (2) the geometry and appearance template are independent of the latent code (modified NV). Input Reconstruction. We first consider input reconstruction quality on the training set to verify the plausibility of the learned representations. See Fig. 7. We find that na\u00efve NR-NeRF and both variants of Neural Volumes perform very well on this task, similar to our method. However, rigid NeRF's not accounting for deformations leads to blur. Novel View Synthesis. We next evaluate novel-view per- formance qualitatively and quantitatively on the test sets. Fig. 7 Fig. 7 show some undesirable artifacts like blurrier or less stable results compared to ours, we refer to the supplemental video to see na\u00efve NeRF's temporal inconsistencies , especially on spatio-temporal trajectories different from the input. After the qualitative overview, we now evaluate the novel-view results of the methods considered quantitatively. We use the same three metrics as NeRF [46]. We use PSNR and SSIM [101] as conventional metrics for image similarity, where higher is better. In addition, we use a learned perceptual metric, LPIPS [100], where lower is better. Tab. 1 contains the quantitative results. Our method obtains the best SSIM and LPIPS scores, and the second-best PSNR af- ter na\u00efve NR-NeRF. As we saw in the input reconstruction results, na\u00efve NR-NeRF is competitive for settings that are close to the input spatio-temporal trajectory, as is the case for our test sets. We, therefore, next evaluate more challenging novel view scenarios with a spatio-temporal trajectory significantly different from the input. Since we do not have access to ground-truth novel view data, we focus on background stability for a spatially fixed camera. Background Stability. While a moving camera during rendering can obfuscate background instability, we found that stabilizing the background for fixed novel-view renderings matters for perceptual fidelity but is difficult to achieve. We thus quantitatively evaluate on this challenging task here. In Fig. 8, we compare the background stability of our method, na\u00efve NR-NeRF, and the Neural Volumes variants. We exclude rigid NeRF since it is static by design. We find that our method leads to significantly more stable background synthesis than the other methods, and we refer to the supplemental material for further results.\n\n\nSimple Scene Editing\n\nWe can manipulate the learned model in several simple ways: foreground removal, temporal super-sampling, deformation exaggeration and dampening, and forced background stabilization. We discuss foreground removal here and the other editing tasks in the supplemental material due to space constraints.\n\nOur representation enables us to remove a potentially occluding non-rigid object from the foreground, leaving only the unoccluded background. Assuming the rigidity network assigns higher scores to non-rigid objects than to rigid (background) objects, we can threshold them at test time to segment the canonical volume into rigid and non-rigid parts. We can then set the non-rigid part transparent, see Fig. 9.\n\n\nLimitations\n\nFor simplicity, the discrete integration along the bent ray uses the interval lengths given by the straight ray. As we build on NeRF, our method is similarly slow. All else being equal, ray bending increases runtime by about 20%. However, due to fewer rays and points sampled, we train for 6 hours. We can thus train multiple NR-NeRFs (to find appro-Input Ours\n\nNa\u00efve NR-NeRF Rigid NeRF Rigid NeRF Neural Volumes Neural Volumes (view-dep.) (not view-dep.) (modified) Figure 7. We compare input reconstruction quality (first row) and novel view synthesis quality (second row). Only our method synthesizes sharp novel views.\n\n\nOurs Na\u00efve NR-NeRF\n\nNeural Volumes Neural Volumes (modified) Figure 8. We compare background stability. See Fig. 6 for an explanation. We use all test time steps here. The results of NR-NeRF show the least instability. priate loss weights) in a time similar to other NeRF-based methods [46]. Neural Sparse Voxel Fields [39] are a promising direction to speed up NeRF-like methods. The background needs to be fairly close to the foreground, an issue we \"inherit\" from NeRF and which could be addressed similarly to NeRF++ [98]. Since we use a deformation model that does not go from the canonical space to the deformed space, we cannot obtain exact correspondences between images captured at different time steps, but instead need to use a nearest neighbor approximation. We do not account for appearance changes that are due to deformation or lighting changes. For example, temporally changing shadowing in the input images is an issue. Foreground removal can fail if a part of the foreground is entirely static. Render-ing parts of the scene barely or not at all observed in the training data would not lead to realistic results. Motion blur in input images is not modeled and would lead to artifacts. The background needs to be static and dominant enough for structure-from-motion [67] to estimate correct extrinsics. Since our problem is severely under-constrained, we employ strong regularization, which leads to a trade-off between sharpness and stability on some scenes.\n\n\nConclusion\n\nWe presented a method for free viewpoint rendering of a dynamic scene using just a monocular video as input. Several high-quality reconstruction and novel view synthesis results of general dynamic scenes, as well as unsupervised, yet plausible rigidity scores and dense 3D correspondences demonstrate the capabilities of the proposed method. Our results suggest that space warping in the form of ray bending is a promising deformation model for volumetric representations like NeRF. Furthermore, we have demonstrated that background instability, a problem also noted by concurrent work [58], can be mitigated in an unsupervised fashion by learning a rigidity mask. The extensions to multi-view data and view dependence invite future work on more constrained settings for higher quality. Although rather rudimentary, we have shown that NR-NeRF enables several scene-editing tasks, and we look forward to further work in the direction of editable neural representations. Acknowledgements. All data capture and evaluation was done at MPII and Volucap. We thank Volucap for providing the multi-view data. Research conducted by Ayush Tewari, Vladislav Golyanik and Christian Theobalt at MPII was supported in part by the ERC Consolidator Grant 4DReply (770784). This work was also supported by a Facebook Reality Labs research grant.\n\n\nNR-NeRF -Supplemental Material Overview\n\nWe provide further illustrations of the loss function in Sec. S.1. We discuss a number of training details in Sec. S.2. Next, we provide some implementation details in Sec. S.3. In Sec. S.4, we describe the capture of our data. Sec. S.5 contains details on how we visualize the additional output modalities of our method and some more results. We show an additional result of our ablation study in Sec. S.6. We provide more details on the experimental settings of our comparisons and some additional results in Sec. S.7. Then in Sec. S.8, we present the extensions to multi-view data and view-dependent effects mentioned in the main paper. Sec. S.9 contains the scene editing tasks mentioned in the main paper. Sec. S.10 contains a limitation example of our method. Finally, Sec. S.11 contains some preliminary qualitative comparisons to a concurrent, non-peer-reviewed work.  \n\n\nS.1. Loss Illustration\n\n\nS.2. Training Details\n\nWhile the network weights are optimized as usual, the latent codes l i are auto-decoded, i.e., they are treated as free variables that are directly optimized for, similar to network weights, instead of being regressed. This is based on the auto-decoding framework used in DeepSDF and earlier works [80,57].\n\nWe initialize {l i } i to zero vectors. For implementing the radiance field, we use the same architecture as in NeRF [46]. The ray bending network is a 5-layer MLP with 64 hidden dimensions and ReLU activations, the last layer of which is initialized with all weights set to zero. The rigidity network is a 3-layer MLP with 32 hidden dimensions and ReLU activations, with the last layer initialized to zeros. The output of the last layer of the rigidity network is passed though a tanh activation function and then shifted and rescaled to lie in [0, 1]. We train usually for 200k iterations with a batch of 1k randomly sampled rays. At training and at test time, we use 64 coarse and 64 fine samples per ray in most cases. We use ADAM [31] and exponentially decay the learning rate to 10% from the initial 5 \u00b7 10 \u22124 over 250k iterations. For dark scenes, we found it necessary to introduce a warm-up phase that linearly increases the learning rate starting from 1 20 th of its original value over 1000 iterations. The latent codes are of the dimension 32. We train between six and seven hours on a single Quadro RTX 8000.\n\nSince we consider a variety of types of non-rigid objects/scenes and deformations, we find it necessary to use scene-specific weights for each loss term. We have found the following ranges to be sufficient for a wide range of scenarios: \u03c9 rigidity lies in [0.01, 0.001] and typically is 0.003, \u03c9 offsets lies in [60,600] and typically is 600, and \u03c9 divergence lies in [1,30] and typically is 3 or 10. In our experience, NR-NeRF is fairly insensitive to \u03c9 offsets . Rather rigid objects benefit from higher \u03c9 divergence , while fairly non-rigid objects need lower \u03c9 divergence . Finally, we increase \u03c9 rigidity whenever we find the background to be unstable. We start the training with each weight set to 1 100 th of its value, and then exponentially increase it until it reaches its full value at the end of training.\n\n\nS.3. Implementation Details\n\nOur code is based on a faithful PyTorch [59] port [93] of the official Tensorflow NeRF code [46]. We use the official FFJORD implementation [20] to estimate Eq. 5 from the main paper. If the camera extrinsics and intrinsics are not given, we estimate them using the Structure-from-Motion (SfM) implementation of COLMAP [68,69]. We find COLMAP to be quite robust to non-rigid 'outliers'. As we are interested in estimating smooth deformations, we only apply positional encoding to the input of the canonical NeRF volume, not to the input of the ray bending network. We will make our source code available.\n\n\nS.4. Data\n\nWe show results on a variety of scenes recorded with three different cameras: the Kinect Azure, a Blackmagic, and a phone camera. Since the RGB camera of the Kinect Azure exhibits strong radial distortions along the image border, we use the manufacturer-provided intrinsics and distortion parameters to undistort the recorded RGB images beforehand. We extract frames at 5 fps from the recordings, such that scenes usually consist of 80 to 300 images, at resolutions of 480 \u00d7 270 (Blackmagic, and Sony XZ2) or 512 \u00d7 384 (Kinect Azure).\n\n\nS.5. Output Modalities\n\n\nS.5.1. Visualizations\n\nRigidity Scores In order to visualize the estimated rigidity, we need to determine the rigidity of the ray associated with a pixel. We choose to define the rigidity of such a ray as the rigidity of the point j closest to an accumulated weight j\u22121 k=0 \u03b1 k of 0.5, i.e., closest to the median. In practice, this usually gives us the rigidity at the first visible surface along the ray.\n\nCorrespondences To visualize correspondences, we treat the canonical volume as an RGB cube, i.e., we treat the xyz coordinate in canonical space as an RGB color. Since this would result in very smooth colors, we split the canonical volume into a voxel grid of 100 3 RGB cubes beforehand. We pick the ray point that determines the pixel color similar to the rigidity visualization. Canonical Volume Since the canonical volume is not supervised directly, it is conceivable that it could have bakedin deformations. The last row of Fig. S.2 contains renderings of the canonical volume without any ray bending applied. The canonical volume is a plausible state of the scene and does not show baked-in deformations. We thus find it to be sufficient to bias the optimization towards a desirable canonical volume by initializing the ray bending network to an identity map and by our regularization losses. \n\n\nS.5.2. More Results\n\n\nS.6. Ablation Study\n\n\nS.7. Comparisons\n\nIn this section, we provide more details on the experimental settings of the comparisons.\n\n\nS.7.1. Prior Work and Baseline\n\nWe start with the trivial baseline of rigid NeRF [46], which cannot handle dynamic scenes. We consider two variants: view-dependent rigid NeRF, as in the original method [46], and view-independent rigid NeRF, where we remove the view-direction conditioning.\n\nWe next introduce na\u00efve NR-NeRF, which adds na\u00efve support for dynamic scenes to rigid NeRF: We condition the neural radiance fields volume on the latent code. Thus, for latent code l i , we have (c, o) = v(x, l i ). This allows the neural radiance fields volume to output time-varying color and occupancy. Unlike NR-NeRF's ray bending, na\u00efve NR-NeRF does not have an explicit, separate deformation model. Instead, the volume needs to account for appearance, geometry and deformation at once. Note that for test images i, we do backpropagate gradients into the corresponding latent code l i .\n\nFinally, we compare to Neural Volumes [41], for which we use the official code release. We use the standard settings as a starting point, but set the number of training iterations to 100k, which leads to a training time of about two days on an RTX8000 GPU. Neural Volumes uses an image encoder to regress a latent code that conditions the geometry, appearance and deformation regression on the current time step. Since this design assumes a multi-view setup, we need to adapt it to our monocular setting. Instead of picking three fixed camera views that are always input into the encoder, we input the single image of the current time step. In particular, at test time, we input the test image. Since we do not have access to a background image, we set the estimated background image to an all-black image. We furthermore consider two variations: (1) following the original Neural Volumes method, the geometry and appearance template is conditioned on the latent code (NV), and (2) the geometry and appearance template is independent of the latent code (modified NV). In the latter case, the latent code only conditions the warp field, which is similar to our method.\n\n\nS.7.2. Training/Test Split\n\nFor quantitative evaluation, we require a test set. In the comparison section, we split the images into training and test images by partitioning the temporally-ordered images into consecutive blocks, each of length 16. The first twelve images of each block are used as training data, while the remaining four are used for testing. In our setting, test images still require corresponding latent codes to represent the deformations. Therefore, we treat test images like training images except that we do not backpropagate into the canonical volume or the ray bending network. However, we do use gradients from test images to optimize the corresponding latent codes. (Note that test images solely influence test latent codes, as is typical for auto-decoding [57]. 1 ) All other results shown in this paper, outside of the comparisons, treat all images as training images since we train scene-specific networks. \n\n\nS.8. Extensions\n\nAs mentioned in Sec. 4 from the main paper, we can extend our approach easily to work with multi-view data and view-dependent effects. We render the input scene into a novel view to determine the stability of the non-rigid objects. We show the results of removing the divergence loss, all three regularization losses, and none of the losses.\n\n\nS.8.1. Multi-View Data\n\nOur approach naturally handles multi-view data. Although we mainly work with monocular data, we can use multi-view data to investigate the upper quality bound of our approach under ideal real-world conditions.\n\nMethod Instead of each image having its own time step and hence latent code, images taken at the same time step share the same latent code. This ensures that the canonical volume deforms consistently within each time step.\n\n\nData and Settings\n\nWe use a multi-view dataset that has 16 camera pairs evenly distributed around the scene, which sufficiently constrains the optimization such that we find the training to not need any regularization losses. We train at the original resolution of 5120 \u00d7 3840 for 2 million training iterations with 4096 rays per batch and 256 coarse and 128 fine samples. These highest-quality settings lead to a training time of 11 days on 4 RTX8000 GPUs, and a rendering time of about 10 minutes per frame on the same hardware. \n\n\nResults\n\n\nS.8.2. View Dependence\n\nWe can optionally add view-dependent effects, like specularities, into our model.\n\nMethod Determining the view direction or ray direction is not as trivial as for the straight rays. Instead, we need to calculate the direction in which the bent ray passes through a point in the canonical volume. We consider two options of doing so: exact and slower, or approximate and faster.\n\nExact: We obtain the direction of the bent rayr at a point r(j) via the chain rule as \u2207 jr (j) = \u2202r(j) \u2202r(j) \u00b7 \u2202r(j) \u2202j = J \u00b7 d, where J is the 3 \u00d7 3 Jacobian and d is the direction of the straight ray. We compute J via three backward passes (one for each output dimension), which is computationally expensive.   (3) with exact view conditioning, na\u00efve NR-NeRF, rigid NeRF [46] (1) with view conditioning and (2) without view conditioning, and Neural Volumes [41] (1) without and (2) with modifications. For PSNR and SSIM [101], higher is better. For LPIPS [100], lower is better. As in Table 1 in the main document, we use 18 scenes here, with an average length of 146 frames and a minimum of 41 and a maximum of 453 frames.\n\nApproximate: To reduce computation, we can approximate the direction at the ray sample via finite differences as the normalized difference vector between the current point r(j) and the previous pointr(j \u2212 1) along the bent ray (which is closer to the camera).\n\nResults On multi-view data, conditioning on the viewing direction reduces the presence of subtle, smoke-like artifacts, which the canonical volume typically employs to model view-dependent effects without view conditioning. This is especially visible for the specularities on the face and the handle of the kettlebell. Without view-dependent effects, the reconstructed face still appears to exhibit specularities, but these are incorrectly modeled via smoke-like artifacts in the surrounding air. See For quantitative results on monocular sequences, see Tab. S.8.2. However, as Fig. S.6 shows, we find our formulation to lead to artifacts in some cases. We hypothesize that the combination of both significant motion and novel views significantly different from input views is too underconstrained for view-dependent effects. For example, non-rigid NeRF might incorrectly overfit to subtle correlations between deformation and camera position at training time. However, we want to emphasize that better formulations and regularization in future work may make viewdependent effects work in these challenging scenarios.\n\n\nS.9. Simple Scene Editing\n\nWe can manipulate the learned model in further simple ways: foreground removal, temporal super-sampling, deformation exaggeration or dampening, and forced background stabilization. Having discussed only foreground removal in the main paper due to space constraints, we here present the other editing tasks.\n\n\nInput\n\n\nOurs\n\nNa\u00efve NR-NeRF Rigid NeRF Rigid NeRF Neural Volumes Neural Volumes (view-dep.)\n\n(not view-dep.) (modified) Figure S.4. We show one time step each from each sequence and compare input reconstruction quality (first row) and novel view synthesis quality (second row).\n\n\nS.9.1. Time Interpolation\n\nWe can linearly interpolate between consecutive time steps to enable temporal super-resolution since NR-NeRF optimizes a latent code l i for every time step i. We refer to the supplemental video for results.\n\n\nS.9.2. Deformation Exaggeration/Dampening\n\nWe can manipulate the deformation even further. Specifically, We can exaggerate or dampen deformations relative to the canonical model by scaling all offsets with a constant m \u2208 R: (c, o) = v(x + mb(x, l i )). \n\n\nS.9.3. Forced Background Stabilization\n\nSince we do not require any pre-computed foregroundbackground segmentation, NR-NeRF has to assign rigidity scores without supervision. Occasionally, this insufficiently constrains the background and leads to small motion. We can fix this in some cases by enforcing a stable background at test time: we set the regressed score to 0 if it is below some threshold r min . If the rigid background has sufficiently small scores assigned to it relative to the non-rigid part of the scene, this forces the background to remain static for all time steps and views. For results, we refer to the supplemental video.\n\n\nS.10. Limitations\n\nWe do not account for appearance changes that are due to deformation or lighting changes. For example, temporally changing shadowing in the input images is an issue, as Foreground removal can fail if a part of the foreground is entirely static (e.g., the foot in Fig. S.9).\n\n\nS.11. Additional Comparisons\n\nWe show some preliminary qualitative comparisons to the concurrent, non-peer-reviewed work Neural Scene Flow Fields [38] in Fig. S.11. Figure S.5. We explore the upper quality bound of our proposed method using a highly controlled multi-view setting. We can extend our method such that it handles view-dependent effects. Results on the left are without view dependence, while those on the right are with view dependence. We show a full rendering by NR-NeRF (first row), zoom-ins thereof (second row), and input images from the two closest input cameras.     \n\nFigure 3 .Figure 4 .\n34The input (left) is reconstructed by NR-NeRF (middle) and rendered into a novel view (right). NR-NeRF can render a deformed state captured at a certain time into a novel view. We visualize here this novel-view rendering and additional modalities as seen from the novel view, namely rigidity scores and correspondences.\n\nFigure 6 .\n6term of offsets loss L offsets No second term of offsets loss L offsets . Without divergence loss L divergence Without any regularization loss .Without rigidity network w With L na\u00efve offsets . Ablation Study. We quantify the impact of our main design choices on background stability. To that end, we render the entire input sequence into a fixed novel view for all time steps and compute the standard deviation of each pixel's color across time to measure color changes and hence background stability. a) We show cumulative plots across all pixels, where our full method (left-most curve) has the most stable background. b) We then show how those instabilities are distributed in the scene. The results of NR-NeRF show the least instability in the background.\n\nFigure 9 .\n9(Left) the ground-truth input image and (right) a rendering without non-rigid foreground.\n\nFig\n. S.1 illustrates L divergence in 2D.\n\nFigure S. 1 .\n1L divergence encourages the offsets field to (b) preserve local volume rather than (c) losing it while deforming.\n\nSee\nFig. S.2 for more results of the output modalities of NR-NeRF.\n\nFig\n. S.3 contains an additional ablation study result that demonstrates the importance of the divergence regularization for foreground stability.\n\nFigure S. 2 .\n2Furthermore, qualitative baseline results in the supplemental video show both training and test time steps. NR-NeRF can render a deformed state captured at a certain time step into a novel view. We visualize here this novel-view rendering and additional output modalities as seen from the novel view, namely rigidity scores, correspondences, and the canonical volume. The canonical volume is a plausible state of the scene and does not show baked-in deformations. We refer to the supplemental video for video results.S.7.3. Additional ResultsSeeFig. S.4 for more qualitative results and Fig. S.7 for quantitative results on background stability under novel views.\n\nFigure S. 3 .\n3Ablation Study.\n\n\nSee Fig. S.5 and the supplemental video for results on five consecutive time steps.\n\n\nFig. S.5 and the supplemental video for results.\n\n\nFig. S.8 contains examples.\n\n\nFig. S.10 demonstrates.\n\nFigure S. 8 .\n8We exaggerate or dampen the motion relative to the canonical model, and render the result into a novel view.\n\nFigure S. 9 .\n9(Left) the groundtruth input image and (right) a rendering without non-rigid foreground.\n\nFigure S. 10 .\n10The input (left) is reconstructed by NR-NeRF (middle). The bottom of the image exhibits local shadowing absent at other time steps, which leads to a high reconstruction error (right\n\n\ncontains novel-view results of all methods. Both versions of Neural Volumes give implausible results that are in some cases only barely recognizable. The two rigid NeRF variants show blurry, static results similar to the training reconstruction results earlier. While the still images in\n\nTable S\nS.1. Quantitative Results Averaged Across Scenes. We evaluate our method (1) without view conditioning, (2) with approximate view conditioning, and\n\n\nno (default) approximate exact Figure S.6. While NR-NeRF extended with view-dependent effects (approximate or exact) gives similar results to the default NR-NeRF for many monocular scenes, we sometimes observe artifacts for difficult novel views. Figure S.7. Background Stability. We quantify the difference in background stability between our method, its variants with view dependence, na\u00efve NR-NeRF, and Neural Volumes. To that end, we render all test time steps of the input sequence into a fixed novel view and compute the standard deviation of each pixel's color across time to measure color changes and hence background stability. a) We show cumulative plots across all pixels, where NR-NeRF and its variants (left-most curves) have the most stable background. b) We then show how those instabilities are distributed in the scene. The results of NR-NeRF and its variants show the least instability in the background. tion-a survey. IEEE Transactions on Visualization and Computer Graphics (TVCG), 19(8):1386-1404, 2012. [4] Chris Buehler, Michael Bosse, Leonard McMillan, Steven J. Gortler, and Michael F. Cohen. Unstructured lu-Canonical Dampening 0.3\u00d7 Normal 1\u00d7 Exaggeration 2\u00d7 Exaggeration 3\u00d7 Exaggeration 5\u00d7 Canonical Dampening 0.2\u00d7Dampening 0.5\u00d7 Normal 1\u00d7 Exaggeration 1.15\u00d7 Exaggeration 2\u00d7a) \n\nLeft Scene \nRight Scene \n\nb) \n\nOurs \nOurs (approx. view) \n\n. \n\nOurs (exact view) \nNa\u00efve NR-NeRF \n\n. \n\nNeural Volumes \nNeural Volumes (modified) \n\n\n\n) .\n)migraph rendering. In SIGGRAPH, 2001. [5] Cedric Cagniart, Edmond Boyer, and Slobodan Ilic. Freeform mesh tracking: A patch-based approach. Computer Vision and Pattern Recognition (CVPR), pages 1339-1346, 2010. [6] Cedric Cagniart, Edmond Boyer, and Slobodan Ilic. Probabilistic deformable surface tracking from multiple videos. In European Conference on Computer Vision (ECCV), 2010. [7] Rodrigo L. Carceroni and Kiriakos N. Kutulakos. Multi-\nL data = c c (C) \u2212\u0109(r) 2 2 + c f (C\u222aF ) \u2212\u0109(r) 2 2 ,(2)\nThe only tweak we add is that we align the optimization landscapes of the training and test latent codes by optimizing the test latent codes jointly with the training latent codes during training. This does not lead to any information leakage from the test images to any component except for the test latent codes.\n\nBuilding rome in a day. Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, Richard Szeliski, Commun. ACM. 5410Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M. Seitz, and Richard Szeliski. Building rome in a day. Commun. ACM, 54(10):105-112, 2011.\n\nSynthesizing images of humans in unseen poses. Guha Balakrishnan, Amy Zhao, Adrian V Dalca, Fr\u00e9do Durand, John V Guttag, Computer Vision and Pattern Recognition (CVPR). Guha Balakrishnan, Amy Zhao, Adrian V. Dalca, Fr\u00e9do Durand, and John V. Guttag. Synthesizing images of hu- mans in unseen poses. Computer Vision and Pattern Recog- nition (CVPR), 2018.\n\nThe helmholtz-hodge decomposi-Input Ours NSFF. Harsh Bhatia, Gregory Norgard, Valerio Pascucci, Peer-Timo Bremer, 38Harsh Bhatia, Gregory Norgard, Valerio Pascucci, and Peer-Timo Bremer. The helmholtz-hodge decomposi- Input Ours NSFF [38]\n\n. Nr-Nerf Na\u00efve, Rigid, no view dep.) Rigid (view dep.Na\u00efve NR-NeRF Rigid (no view dep.) Rigid (view dep.)\n\nBoth the concurrent, non-peer-reviewed Neural Scene Flow Fields [38] and na\u00efve NR-NeRF however entangle deformation with geometry and appearance by conditioning the 'canonical' volume on a time-dependent deformation latent code. This makes sharing information across time more difficult, leading to blurrier results in challenging novel view scenarios compared to NR-NeRF's results. Finally, rigid NeRF shows a blurry mix of the deformations observed over the entire input sequence, which highlights the need to account for deformations in the scene. view scene capture by surfel sampling: From video streams to non-rigid 3d motion, shape and reflectance. S Figure, International Journal of Computer Vision. 492Under challenging novel view scenarios, our method benefits from the geometry and appearance information that the canonical volume has accumulated from all time steps, which allows NR-NeRF to output sharp resultsFigure S.11. Under challenging novel view scenarios, our method benefits from the geometry and appearance information that the canonical volume has accumulated from all time steps, which allows NR-NeRF to output sharp results. Both the concurrent, non-peer-reviewed Neural Scene Flow Fields [38] and na\u00efve NR-NeRF however entangle deformation with geometry and appearance by conditioning the 'canonical' volume on a time-dependent deformation latent code. This makes sharing information across time more difficult, leading to blurrier results in challenging novel view scenarios compared to NR-NeRF's results. Finally, rigid NeRF shows a blurry mix of the deformations observed over the entire input sequence, which highlights the need to account for deformations in the scene. view scene capture by surfel sampling: From video streams to non-rigid 3d motion, shape and reflectance. International Journal of Computer Vision, 49(2):175-214, 2002.\n\nEverybody dance now. Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A Efros, International Conference on Computer Vision (ICCV). Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In International Conference on Computer Vision (ICCV), 2019.\n\nHigh-quality streamable freeviewpoint video. Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk, Steve Sullivan, ACM Transactions on Graphics (ToG). 34469Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk, and Steve Sullivan. High-quality streamable free- viewpoint video. ACM Transactions on Graphics (ToG), 34(4):69, 2015.\n\nFusion4d: Real-time performance capture of challenging scenes. Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip Davidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts Escolano, Christoph Rhemann, David Kim, Jonathan Taylor, ACM Trans. Graph. 354Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip Davidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts Escolano, Christoph Rhemann, David Kim, Jonathan Tay- lor, and et al. Fusion4d: Real-time performance capture of challenging scenes. ACM Trans. Graph., 35(4), 2016.\n\nYilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenenbaum, Jiajun Wu, arXiv:2012.09790Neural radiance flow for 4d view synthesis and video processing. arXiv preprintYilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenen- baum, and Jiajun Wu. Neural radiance flow for 4d view synthesis and video processing. arXiv preprint arXiv:2012.09790, 2020.\n\n3d morphable face models -past, present and future. Bernhard Egger, A P William, Ayush Smith, Stefanie Tewari, Michael Wuhrer, Thabo Zollhoefer, Florian Beeler, Timo Bernard, Adam Bolkart, Sami Kortylewski, Christian Romdhani, Volker Theobalt, Thomas Blanz, Vetter, ACM Transactions on Graphics. 395Bernhard Egger, William A. P. Smith, Ayush Tewari, Ste- fanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romd- hani, Christian Theobalt, Volker Blanz, and Thomas Vetter. 3d morphable face models -past, present and future. ACM Transactions on Graphics, 39(5), Aug. 2020.\n\nNeural scene representation and rendering. Danilo Sm Ali Eslami, Frederic Jimenez Rezende, Fabio Besse, Ari S Viola, Marta Morcos, Avraham Garnelo, Andrei A Ruderman, Ivo Rusu, Karol Danihelka, Gregor, Science. 3606394SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Garnelo, Avraham Ru- derman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene representation and rendering. Science, 360(6394):1204-1210, 2018.\n\nDeepview: View synthesis with learned gradient descent. John Flynn, Michael Broxton, Paul Debevec, Matthew Du-Vall, Graham Fyffe, Ryan Overbeck, Noah Snavely, Richard Tucker, International Conference on Computer Vision and Pattern Recognition (CVPR). John Flynn, Michael Broxton, Paul Debevec, Matthew Du- Vall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and Richard Tucker. Deepview: View synthesis with learned gradient descent. International Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nGuy Gafni, Justus Thies, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction. arXiv e-prints. Guy Gafni, Justus Thies, Michael Zollh\u00f6fer, and Matthias Nie\u00dfner. Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction. arXiv e-prints, 2020.\n\nChen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang, Jia-Bin Huang, Portrait Neural Radiance Fields from a Single Image. arXiv e-printsChen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang, and Jia-Bin Huang. Portrait Neural Radiance Fields from a Single Image. arXiv e-prints, 2020.\n\nDense variational reconstruction of non-rigid surfaces from monocular video. Ravi Garg, Anastasios Roussos, Lourdes Agapito, Computer Vision and Pattern Recognition (CVPR). Ravi Garg, Anastasios Roussos, and Lourdes Agapito. Dense variational reconstruction of non-rigid surfaces from monocular video. Computer Vision and Pattern Recogni- tion (CVPR), pages 1272-1279, 2013.\n\nHdm-net: Monocular non-rigid 3d reconstruction with learned deformation model. Vladislav Golyanik, Soshi Shimada, Kiran Varanasi, Didier Stricker, EuroVRVladislav Golyanik, Soshi Shimada, Kiran Varanasi, and Didier Stricker. Hdm-net: Monocular non-rigid 3d re- construction with learned deformation model. In EuroVR, 2018.\n\nThe lumigraph. J Steven, Radek Gortler, Richard Grzeszczuk, Michael F Szeliski, Cohen, SIGGRAPH. Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The lumigraph. In SIGGRAPH, page 43-54, 1996.\n\nWill Grathwohl, T Q Ricky, Jesse Chen, Ilya Bettencourt, David Sutskever, Duvenaud, Ffjord, arXiv:1810.01367Free-form continuous dynamics for scalable reversible generative models. arXiv preprintWill Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form contin- uous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367, 2018.\n\nThe relightables: Volumetric performance capture of humans with realistic relighting. Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-Escolano, Rohit Pandey, Jason Dourgarian, ACM Trans. Graph. 386Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts- Escolano, Rohit Pandey, Jason Dourgarian, and et al. The relightables: Volumetric performance capture of humans with realistic relighting. ACM Trans. Graph., 38(6), 2019.\n\nReal-time geometry, albedo, and motion reconstruction using a single rgb-d camera. Kaiwen Guo, Feng Xu, Tao Yu, Xiaoyang Liu, Qionghai Dai, Yebin Liu, ACM Trans. Graph. 364Kaiwen Guo, Feng Xu, Tao Yu, Xiaoyang Liu, Qiong- hai Dai, and Yebin Liu. Real-time geometry, albedo, and motion reconstruction using a single rgb-d camera. ACM Trans. Graph., 36(4), 2017.\n\nLivecap: Real-time human performance capture from monocular video. Marc Habermann, Weipeng Xu, Michael Zollh\u00f6fer, Gerard Pons-Moll, Christian Theobalt, 14:1-14:17ACM Trans. Graph. 382Marc Habermann, Weipeng Xu, Michael Zollh\u00f6fer, Ger- ard Pons-Moll, and Christian Theobalt. Livecap: Real-time human performance capture from monocular video. ACM Trans. Graph., 38(2):14:1-14:17, Mar. 2019.\n\nDeep blending for free-viewpoint image-based rendering. Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, Gabriel Brostow, 257:1-257:15ACM Trans. Graph. 376Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Trans. Graph., 37(6):257:1-257:15, Dec. 2018.\n\nDeep volumetric video from very sparse multi-view performance capture. Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao, Jun Xing, Chloe Legendre, Linjie Luo, Chongyang Ma, Hao Li, European Conference on Computer Vision (ECCV). Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao, Jun Xing, Chloe Legendre, Linjie Luo, Chongyang Ma, and Hao Li. Deep volumetric video from very sparse multi-view performance capture. In European Conference on Com- puter Vision (ECCV), pages 351-369, 2018.\n\nA stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. F Michael, Hutchinson, Communications in Statistics-Simulation and Computation. 183Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059-1076, 1989.\n\nAsrigid-as-possible shape manipulation. Takeo Igarashi, Tomer Moscovich, John F Hughes, ACM transactions on Graphics (TOG). 243Takeo Igarashi, Tomer Moscovich, and John F Hughes. As- rigid-as-possible shape manipulation. ACM transactions on Graphics (TOG), 24(3):1134-1141, 2005.\n\nVolumedeform: Real-time volumetric non-rigid reconstruction. Matthias Innmann, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Christian Theobald, Marc Stamminger, European Conference on Computer Vision (ECCV). Matthias Innmann, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Christian Theobald, and Marc Stamminger. Volumedeform: Real-time volumetric non-rigid reconstruction. In Euro- pean Conference on Computer Vision (ECCV), 2016.\n\nNeural style-preserving visual dubbing. Hyeongwoo Kim, Mohamed Elgharib, Hans-Peter Zoll\u00f6fer, Michael Seidel, Thabo Beeler, Christian Richardt, Christian Theobalt, ACM Transactions on Graphics (TOG). 386Hyeongwoo Kim, Mohamed Elgharib, Hans-Peter Zoll\u00f6fer, Michael Seidel, Thabo Beeler, Christian Richardt, and Christian Theobalt. Neural style-preserving visual dub- bing. ACM Transactions on Graphics (TOG), 38(6):178:1- 13, 2019.\n\nDeep video portraits. Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Nie\u00dfner, Patrick P\u00e9rez, Christian Richardt, Michael Zoll\u00f6fer, Christian Theobalt, ACM Transactions on Graphics. 37Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Nie\u00dfner, Patrick P\u00e9rez, Chris- tian Richardt, Michael Zoll\u00f6fer, and Christian Theobalt. Deep video portraits. ACM Transactions on Graphics (TOG), 37, 2018.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nVIBE: Video inference for human body pose and shape estimation. Muhammed Kocabas, Nikos Athanasiou, Michael J Black, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black. VIBE: Video inference for human body pose and shape estimation. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 5252-5262.\n\n. IEEE. IEEE, June 2020.\n\nScalable dense non-rigid structure-from-motion: A grassmannian perspective. Suryansh Kumar, Anoop Cherian, Yuchao Dai, Hongdong Li, Computer Vision and Pattern Recognition (CVPR). Suryansh Kumar, Anoop Cherian, Yuchao Dai, and Hong- dong Li. Scalable dense non-rigid structure-from-motion: A grassmannian perspective. In Computer Vision and Pat- tern Recognition (CVPR), 2018.\n\nA theory of shape by space carving. N Kiriakos, Steven M Kutulakos, Seitz, International Journal of Computer Vision (IJCV). 38Kiriakos N. Kutulakos and Steven M. Seitz. A theory of shape by space carving. International Journal of Computer Vision (IJCV), 38:199-218, 2000.\n\nLight field rendering. Marc Levoy, Pat Hanrahan, SIGGRAPH. Marc Levoy and Pat Hanrahan. Light field rendering. In SIGGRAPH, page 31-42, 1996.\n\nCapturing relightable human performances under general uncontrolled illumination. Guannan Li, Chenglei Wu, Carsten Stoll, Yebin Liu, Kiran Varanasi, Qionghai Dai, Christian Theobalt, Comput. Graph. Forum. 322Guannan Li, Chenglei Wu, Carsten Stoll, Yebin Liu, Kiran Varanasi, Qionghai Dai, and Christian Theobalt. Captur- ing relightable human performances under general uncon- trolled illumination. Comput. Graph. Forum, 32(2):275- 284, 2013.\n\n. Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Zhaoyang Lv, Neural 3d video synthesisTianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, and Zhaoyang Lv. Neural 3d video synthesis, 2021.\n\nZhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang, Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes. arxiv. Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes. arxiv, 2020.\n\nNeural sparse voxel fields. Lingjie Liu, Jiatao Gu, Tat-Seng Kyaw Zaw Lin, Christian Chua, Theobalt, Advances in Neural Information Processing Systems. 33Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33, 2020.\n\nA point-cloudbased multiview stereo algorithm for free-viewpoint video. Yebin Liu, Qionghai Dai, Wenli Xu, IEEE Transactions on Visualization and Computer Graphics (TVCG). 163Yebin Liu, Qionghai Dai, and Wenli Xu. A point-cloud- based multiview stereo algorithm for free-viewpoint video. IEEE Transactions on Visualization and Computer Graph- ics (TVCG), 16(3):407-418, 2010.\n\nNeural volumes: Learning dynamic renderable volumes from images. Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh, ACM Trans. Graph. (SIGGRAPH). 384Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from im- ages. ACM Trans. Graph. (SIGGRAPH), 38(4), 2019.\n\nDisentangled person image generation. Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, Mario Fritz, Computer Vision and Pattern Recognition (CVPR). Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, and Mario Fritz. Disentangled person image generation. Computer Vision and Pattern Recogni- tion (CVPR), 2018.\n\nLookingood: Enhancing performance capture with realtime neural re-rendering. Ricardo Martin Brualla, Peter Lincoln, Adarsh Kowdle, Christoph Rhemann, Dan Goldman, Cem Keskin, Steve Seitz, Shahram Izadi, Sean Fanello, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, 37Jonathan Taylor, Julien Valentin, Sameh Khamis, Philip Davidson, and Anastasia TkachRicardo Martin Brualla, Peter Lincoln, Adarsh Kowdle, Christoph Rhemann, Dan Goldman, Cem Keskin, Steve Seitz, Shahram Izadi, Sean Fanello, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Julien Valentin, Sameh Khamis, Philip Davidson, and Anastasia Tkach. Lookingood: Enhancing performance capture with real- time neural re-rendering. volume 37, 2018.\n\nReal-time dynamic 3-d object shape reconstruction and high-fidelity texture mapping for 3-d video. Takashi Matsuyama, Xiaojun Wu, Takeshi Takai, Toshikazu Wada, IEEE Transactions on Circuits and Systems for Video Technology. 143Takashi Matsuyama, Xiaojun Wu, Takeshi Takai, and Toshikazu Wada. Real-time dynamic 3-d object shape reconstruction and high-fidelity texture mapping for 3-d video. IEEE Transactions on Circuits and Systems for Video Technology, 14(3):357-369, 2004.\n\nNeural rerendering in the wild. Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, Ricardo Martin-Brualla, Computer Vision and Pattern Recognition (CVPR). Moustafa Meshry, Dan B. Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-Brualla. Neural rerendering in the wild. In Com- puter Vision and Pattern Recognition (CVPR), 2019.\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, ECCV. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. In ECCV, 2020.\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, European Conference on Computer Vision (ECCV). 2020Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision (ECCV), 2020.\n\nInteractive free-viewpoint video. Graham Miller, Adrian Hilton, Jonathan Starck, IEEE European Conf. on Visual Media Production. Graham Miller, Adrian Hilton, and Jonathan Starck. Inter- active free-viewpoint video. In IEEE European Conf. on Visual Media Production, pages 50-59, 2005.\n\nSkin microstructure deformation with displacement map convolution. Koki Nagano, Graham Fyffe, Oleg Alexander, Jernej Barbi\u00e7, Hao Li, Abhijeet Ghosh, Paul Debevec, ACM Trans. Graph. 344Koki Nagano, Graham Fyffe, Oleg Alexander, Jernej Barbi\u00e7, Hao Li, Abhijeet Ghosh, and Paul Debevec. Skin microstructure deformation with displacement map convo- lution. ACM Trans. Graph., 34(4), 2015.\n\nNatalia Neverova, Riza Alp G\u00fcler, and Iasonas Kokkinos. Dense pose transfer. ECCV. Natalia Neverova, Riza Alp G\u00fcler, and Iasonas Kokkinos. Dense pose transfer. ECCV, 2018.\n\nDynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. Richard A Newcombe, Dieter Fox, Steven M Seitz, Computer Vision and Pattern Recognition (CVPR). Richard A. Newcombe, Dieter Fox, and Steven M. Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In Computer Vision and Pattern Recog- nition (CVPR), 2015.\n\nKinectfusion: Real-time dense surface mapping and tracking. Richard A Newcombe, Shahram Izadi, Otmar Hilliges, David Kim, Andrew J Davison, Pushmeet Kohli, Jamie Shotton, International Symposium on Mixed and Augmented Reality (ISMAR). Steve Hodges, and Andrew FitzgibbonRichard A. Newcombe, Shahram Izadi, Otmar Hilliges, David Kim, Andrew J. Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon. Kinect- fusion: Real-time dense surface mapping and tracking. In International Symposium on Mixed and Augmented Reality (ISMAR), 2011.\n\nDense image registration and deformable surface reconstruction in presence of occlusions and minimal texture. Sanghyuk Dat Tien Ngo, Anne Park, Alberto Jorstad, Chang D Crivellaro, Pascal Yoo, Fua, International Conference on Computer Vision (ICCV). Dat Tien Ngo, Sanghyuk Park, Anne Jorstad, Alberto Criv- ellaro, Chang D. Yoo, and Pascal Fua. Dense image reg- istration and deformable surface reconstruction in presence of occlusions and minimal texture. In International Confer- ence on Computer Vision (ICCV), 2015.\n\nRendernet: A deep convolutional network for differentiable rendering from 3d shapes. Chuan Thu H Nguyen-Phuoc, Stephen Li, Yongliang Balaban, Yang, Advances in Neural Information Processing Systems (NIPS). Thu H Nguyen-Phuoc, Chuan Li, Stephen Balaban, and Yongliang Yang. Rendernet: A deep convolutional network for differentiable rendering from 3d shapes. In Advances in Neural Information Processing Systems (NIPS), 2018.\n\nHoloportation: Virtual 3d teleportation in realtime. Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim, Philip L Davidson, Sameh Khamis, Mingsong Dou, Annual Symposium on User Interface Software and Technology. Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim, Philip L. Davidson, Sameh Khamis, Mingsong Dou, and et al. Holoportation: Virtual 3d teleportation in real- time. In Annual Symposium on User Interface Software and Technology, page 741-754, 2016.\n\nGeneralized connectivity constraints for spatio-temporal 3d reconstruction. Martin R Oswald, Jan St\u00fchmer, Daniel Cremers, European Conference on Computer Vision (ECCV). Martin R. Oswald, Jan St\u00fchmer, and Daniel Cremers. Gen- eralized connectivity constraints for spatio-temporal 3d re- construction. In European Conference on Computer Vision (ECCV), 2014.\n\nDeepsdf: Learning continuous signed distance functions for shape representation. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove, International Conference on Computer Vision and Pattern Recognition (CVPR). Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape represen- tation. International Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, Ricardo Martin-Brualla, arXiv:2011.12948Deformable neural radiance fields. arXiv preprintKeunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Deformable neural radiance fields. arXiv preprint arXiv:2011.12948, 2020.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-BucAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc,\n\nE Fox, R Garnett, Advances in Neural Information Processing Systems. Curran Associates, Inc32E. Fox, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems 32, pages 8024-8035. Curran Associates, Inc., 2019.\n\nSida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou, Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. arXiv eprintsSida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. arXiv e- prints, 2020.\n\nMonocular template-based reconstruction of inextensible surfaces. Mathieu Perriollat, Richard Hartley, Adrien Bartoli, Int. J. Comput. Vision (IJCV). 952Mathieu Perriollat, Richard Hartley, and Adrien Bartoli. Monocular template-based reconstruction of inextensible surfaces. Int. J. Comput. Vision (IJCV), 95(2):124-137, 2011.\n\nSurfels: Surface elements as rendering primitives. Hanspeter Pfister, Matthias Zwicker, Jeroen Van Baar, Markus Gross, Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '00. the 27th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '00USAACM Press/Addison-Wesley Publishing CoHanspeter Pfister, Matthias Zwicker, Jeroen van Baar, and Markus Gross. Surfels: Surface elements as ren- dering primitives. In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Tech- niques, SIGGRAPH '00, page 335-342, USA, 2000. ACM Press/Addison-Wesley Publishing Co.\n\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer, . D-Nerf , Neural Radiance Fields for Dynamic Scenes. arxiv. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural Radiance Fields for Dynamic Scenes. arxiv, 2020.\n\nAmit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, Pva: Pixel-aligned volumetric avatars. James Hays, and Stephen LombardiAmit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, and Stephen Lom- bardi. Pva: Pixel-aligned volumetric avatars, 2021.\n\nFree view synthesis. Gernot Riegler, Vladlen Koltun, European Conference on Computer Vision (ECCV). 2020Gernot Riegler and Vladlen Koltun. Free view synthesis. In European Conference on Computer Vision (ECCV), 2020.\n\nNeural rerendering of humans from a single image. Kripasindhu Sarkar, Dushyant Mehta, Weipeng Xu, Vladislav Golyanik, Christian Theobalt, European Conference on Computer Vision (ECCV). 2020Kripasindhu Sarkar, Dushyant Mehta, Weipeng Xu, Vladislav Golyanik, and Christian Theobalt. Neural re- rendering of humans from a single image. In European Conference on Computer Vision (ECCV), 2020.\n\nStructure-from-motion revisited. L Johannes, Jan-Michael Schonberger, Frahm, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJohannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4104-4113, 2016.\n\nStructure-from-motion revisited. Johannes Lutz Sch\u00f6nberger, Jan-Michael Frahm, Computer Vision and Pattern Recognition (CVPR). Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. Structure-from-motion revisited. In Computer Vision and Pattern Recognition (CVPR), 2016.\n\nPixelwise view selection for unstructured multi-view stereo. Johannes Lutz Sch\u00f6nberger, Enliang Zheng, Marc Pollefeys, Jan-Michael Frahm, European Conference on Computer Vision (ECCV). Johannes Lutz Sch\u00f6nberger, Enliang Zheng, Marc Polle- feys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016.\n\nLayered depth images. Jonathan Shade, Steven Gortler, Li-Wei He, Richard Szeliski, Proceedings of the 25th. the 25thJonathan Shade, Steven Gortler, Li-wei He, and Richard Szeliski. Layered depth images. In Proceedings of the 25th\n\nAnnual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '98. New York, NY, USAAssociation for Computing MachineryAnnual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '98, page 231-242, New York, NY, USA, 1998. Association for Computing Machinery.\n\nTextured neural avatars. Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, Victor Lempitsky, Computer Vision and Pattern Recognition (CVPR). Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Alek- sei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, and Victor Lempitsky. Tex- tured neural avatars. In Computer Vision and Pattern Recognition (CVPR), 2019.\n\nNeural dense nonrigid structure from motion with latent space constraints. Vikramjit Sidhu, Edgar Tretschk, Vladislav Golyanik, Antonio Agudo, Christian Theobalt, European Conference on Computer Vision (ECCV). 2020Vikramjit Sidhu, Edgar Tretschk, Vladislav Golyanik, An- tonio Agudo, and Christian Theobalt. Neural dense non- rigid structure from motion with latent space constraints. In European Conference on Computer Vision (ECCV), 2020.\n\nDeepvoxels: Learning persistent 3d feature embeddings. Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Niessner, Gordon Wetzstein, Michael Zollhofer, Computer Vision and Pattern Recognition (CVPR). Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Niessner, Gordon Wetzstein, and Michael Zollhofer. Deep- voxels: Learning persistent 3d feature embeddings. In Computer Vision and Pattern Recognition (CVPR), 2019.\n\nScene representation networks: Continuous 3d-structure-aware neural scene representations. Vincent Sitzmann, Michael Zollh\u00f6fer, Gordon Wetzstein, Advances in Neural Information Processing Systems. Vincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wet- zstein. Scene representation networks: Continuous 3d- structure-aware neural scene representations. In Advances in Neural Information Processing Systems, 2019.\n\nKillingfusion: Non-rigid 3d reconstruction without correspondences. Miroslava Slavcheva, Maximilian Baust, Daniel Cremers, Slobodan Ilic, Computer Vision and Pattern Recognition (CVPR). Miroslava Slavcheva, Maximilian Baust, Daniel Cremers, and Slobodan Ilic. Killingfusion: Non-rigid 3d reconstruc- tion without correspondences. In Computer Vision and Pat- tern Recognition (CVPR), 2017.\n\nPeter Eisert, and Thomas Wiegand. 3d video and free viewpoint video-technologies, applications and mpeg standards. Aljoscha Smolic, Karsten Mueller, Philipp Merkle, Christoph Fehn, Peter Kauff, IEEE International Conference on Multimedia and Expo. IEEEAljoscha Smolic, Karsten Mueller, Philipp Merkle, Christoph Fehn, Peter Kauff, Peter Eisert, and Thomas Wie- gand. 3d video and free viewpoint video-technologies, ap- plications and mpeg standards. In IEEE International Con- ference on Multimedia and Expo, pages 2161-2164. IEEE, 2006.\n\nAs-rigid-as-possible surface modeling. Olga Sorkine, Marc Alexa, Symposium on Geometry Processing. 4Olga Sorkine and Marc Alexa. As-rigid-as-possible sur- face modeling. In Symposium on Geometry Processing, volume 4, pages 109-116, 2007.\n\nVolumetric stereo with silhouette and feature constraints. Jonathan Starck, Gregor Miller, Adrian Hilton, British Machine Vision Conference (BMVC). Jonathan Starck, Gregor Miller, and Adrian Hilton. Vol- umetric stereo with silhouette and feature constraints. In British Machine Vision Conference (BMVC), 2006.\n\nA-nerf: Surface-free human 3d pose refinement via neural rendering. Shih-Yang Su, Frank Yu, Michael Zollhoefer, Helge Rhodin, Shih-Yang Su, Frank Yu, Michael Zollhoefer, and Helge Rhodin. A-nerf: Surface-free human 3d pose refinement via neural rendering, 2021.\n\nReducing data dimensionality through optimizing neural network inputs. Shufeng Tan, Michael L Mayrovouniotis, AIChE Journal. 416Shufeng Tan and Michael L. Mayrovouniotis. Reducing data dimensionality through optimizing neural network in- puts. AIChE Journal, 41(6):1471-1480, 1995.\n\nDoublefusion: Real-time capture of human performance with inner body shape from a depth sensor. Yu Tao, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Dai Quionhai, Hao Li, Gerard Pons-Moll, Yebin Liu, Computer Vision and Pattern Recognition (CVPR). Yu Tao, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Dai Quionhai, Hao Li, Gerard Pons-Moll, and Yebin Liu. Dou- blefusion: Real-time capture of human performance with inner body shape from a depth sensor. In Computer Vision and Pattern Recognition (CVPR), 2018.\n\n. Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nie\u00dfner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollh\u00f6fer, State of the Art on Neural Rendering. Computer Graphics Forum. 2020Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin- Brualla, Tomas Simon, Jason Saragih, Matthias Nie\u00dfner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shecht- man, Dan B Goldman, and Michael Zollh\u00f6fer. State of the Art on Neural Rendering. Computer Graphics Forum (EG STAR 2020), 2020.\n\nSeeing people in different light-joint shape, motion, and reflectance capture. Christian Theobalt, Naveed Ahmed, Hendrik Lensch, Marcus Magnor, Hans-Peter Seidel, IEEE Transactions on Visualization and Computer Graphics (TVCG). 134Christian Theobalt, Naveed Ahmed, Hendrik Lensch, Mar- cus Magnor, and Hans-Peter Seidel. Seeing people in dif- ferent light-joint shape, motion, and reflectance capture. IEEE Transactions on Visualization and Computer Graph- ics (TVCG), 13(4):663-674, 2007.\n\nDeferred neural rendering: image synthesis using neural textures. Justus Thies, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, ACM Transactions on Graphics. 38Justus Thies, Michael Zollh\u00f6fer, and Matthias Nie\u00dfner. De- ferred neural rendering: image synthesis using neural tex- tures. ACM Transactions on Graphics, 38, 2019.\n\nDEMEA: Deep mesh autoencoders for non-rigidly deforming objects. Edgar Tretschk, Ayush Tewari, Michael Zollh\u00f6fer, Vladislav Golyanik, Christian Theobalt, European Conference on Computer Vision (ECCV). 2020Edgar Tretschk, Ayush Tewari, Michael Zollh\u00f6fer, Vladislav Golyanik, and Christian Theobalt. DEMEA: Deep mesh autoencoders for non-rigidly deforming objects. In European Conference on Computer Vision (ECCV), 2020.\n\nComplete multi-view reconstruction of dynamic scenes from probabilistic fusion of narrow and wide baseline stereo. Tony Tung, Shohei Nobuhara, Takashi Matsuyama, Tony Tung, Shohei Nobuhara, and Takashi Matsuyama. Complete multi-view reconstruction of dynamic scenes from probabilistic fusion of narrow and wide baseline stereo. pages 1709-1716, 2009.\n\nZiyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, Michael Zollh\u00f6fer, Learning Compositional Radiance Fields of Dynamic Human Heads. arXiv e-prints. Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, and Michael Zollh\u00f6fer. Learning Compositional Radiance Fields of Dy- namic Human Heads. arXiv e-prints, 2020.\n\nScalable 3d video of dynamic scenes. Michael Waschb\u00fcsch, Stephan W\u00fcrmlin, Daniel Cotting, Filip Sadlo, Markus Gross, The Visual Computer. 218Michael Waschb\u00fcsch, Stephan W\u00fcrmlin, Daniel Cotting, Filip Sadlo, and Markus Gross. Scalable 3d video of dy- namic scenes. The Visual Computer, 21(8):629-638, 2005.\n\nVid2Actor: Free-viewpoint Animatable Person Synthesis from Video in the Wild. Chung-Yi Weng, Brian Curless, Ira Kemelmacher-Shlizerman, arXiv e-printsChung-Yi Weng, Brian Curless, and Ira Kemelmacher- Shlizerman. Vid2Actor: Free-viewpoint Animatable Per- son Synthesis from Video in the Wild. arXiv e-prints, 2020.\n\nWenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim, Space-time neural irradiance fields for free-viewpoint video. arXiv preprintWenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. arXiv preprint, 2020.\n\nMonocular total capture: Posing face, body, and hands in the wild. Donglai Xiang, Hanbyul Joo, Yaser Sheikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDonglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular total capture: Posing face, body, and hands in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10965-10974, 2019.\n\nMonoperfcap: Human performance capture from monocular video. Weipeng Xu, Avishek Chatterjee, Michael Zollh\u00f6fer, Helge Rhodin, Dushyant Mehta, Hans-Peter Seidel, Christian Theobalt, 27:1-27:15ACM Trans. Graph. 372Weipeng Xu, Avishek Chatterjee, Michael Zollh\u00f6fer, Helge Rhodin, Dushyant Mehta, Hans-Peter Seidel, and Christian Theobalt. Monoperfcap: Human performance capture from monocular video. ACM Trans. Graph., 37(2):27:1-27:15, 2018.\n\n. Lin Yen-Chen, Lin Yen-Chen. Nerf-pytorch. https://github.com/ yenchenlin/nerf-pytorch/, 2020.\n\nNovel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, Jan Kautz, Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. 2020.\n\nDirect, dense, and deformable: Template-based non-rigid 3d reconstruction from rgb video. Rui Yu, Chris Russell, D F Neill, Lourdes Campbell, Agapito, International Conference on Computer Vision (ICCV). Rui Yu, Chris Russell, Neill D. F. Campbell, and Lourdes Agapito. Direct, dense, and deformable: Template-based non-rigid 3d reconstruction from rgb video. In Interna- tional Conference on Computer Vision (ICCV), 2015.\n\nBodyfusion: Real-time capture of human motion and surface geometry using a single depth camera. Tao Yu, Kaiwen Guo, Feng Xu, Yuan Dong, Zhaoqi Su, Jianhui Zhao, Jianguo Li, Qionghai Dai, Yebin Liu, International Conference on Computer Vision (ICCV). Tao Yu, Kaiwen Guo, Feng Xu, Yuan Dong, Zhaoqi Su, Jianhui Zhao, Jianguo Li, Qionghai Dai, and Yebin Liu. Bodyfusion: Real-time capture of human motion and sur- face geometry using a single depth camera. In International Conference on Computer Vision (ICCV), pages 910-919, 2017.\n\nSimulcap : Single-view human performance capture with cloth simulation. Tao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Conference on Computer Vision and Pattern Recognition(CVPR). Qionghai Dai, Gerard Pons-Moll, and Yebin LiuTao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Qiong- hai Dai, Gerard Pons-Moll, and Yebin Liu. Simulcap : Single-view human performance capture with cloth simula- tion. In Conference on Computer Vision and Pattern Recog- nition(CVPR), 2019.\n\nKai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun, arXiv:2010.07492Nerf++: Analyzing and improving neural radiance fields. arXiv preprintKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492, 2020.\n\nSpacetime stereo: shape recovery for dynamic scenes. Li Zhang, Brian Curless, Steven M Seitz, Computer Vision and Pattern Recognition (CVPR). Li Zhang, Brian Curless, and Steven M. Seitz. Spacetime stereo: shape recovery for dynamic scenes. In Computer Vision and Pattern Recognition (CVPR), 2003.\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, Computer Vision and Pattern Recognition (CVPR). Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht- man, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Computer Vision and Pattern Recognition (CVPR), 2018.\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, IEEE Transactions on Image Processing. 134Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Pro- cessing, 13(4):600-612, 2004.\n\nView extrapolation of human body from a single image. Hao Zhu, Hao Su, Peng Wang, Xun Cao, Ruigang Yang, Computer Vision and Pattern Recognition (CVPR). Hao Zhu, Hao Su, Peng Wang, Xun Cao, and Ruigang Yang. View extrapolation of human body from a single im- age. In Computer Vision and Pattern Recognition (CVPR), 2018.\n\nReal-time non-rigid reconstruction using an rgb-d camera. Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rhemann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, Marc Stamminger, ACM Transactions on Graphics. Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rhemann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, and Marc Stamminger. Real-time non-rigid re- construction using an rgb-d camera. ACM Transactions on Graphics (TOG), 2014.\n\nState of the art on 3d reconstruction with rgb-d cameras. Michael Zollh\u00f6fer, Patrick Stotko, Andreas G\u00f6rlitz, Christian Theobalt, Matthias Nie\u00dfner, Reinhard Klein, Andreas Kolb, Computer graphics forum. Wiley Online Library37Michael Zollh\u00f6fer, Patrick Stotko, Andreas G\u00f6rlitz, Chris- tian Theobalt, Matthias Nie\u00dfner, Reinhard Klein, and An- dreas Kolb. State of the art on 3d reconstruction with rgb-d cameras. In Computer graphics forum, volume 37, pages 625-652. Wiley Online Library, 2018.\n", "annotations": {"author": "[{\"end\":294,\"start\":117},{\"end\":470,\"start\":295},{\"end\":652,\"start\":471},{\"end\":833,\"start\":653},{\"end\":1014,\"start\":834},{\"end\":1196,\"start\":1015}]", "publisher": null, "author_last_name": "[{\"end\":131,\"start\":123},{\"end\":307,\"start\":301},{\"end\":489,\"start\":481},{\"end\":670,\"start\":661},{\"end\":851,\"start\":844},{\"end\":1033,\"start\":1025}]", "author_first_name": "[{\"end\":122,\"start\":117},{\"end\":300,\"start\":295},{\"end\":480,\"start\":471},{\"end\":660,\"start\":653},{\"end\":843,\"start\":834},{\"end\":1024,\"start\":1015}]", "author_affiliation": "[{\"end\":293,\"start\":133},{\"end\":469,\"start\":309},{\"end\":651,\"start\":491},{\"end\":832,\"start\":672},{\"end\":1013,\"start\":853},{\"end\":1195,\"start\":1035}]", "title": "[{\"end\":114,\"start\":1},{\"end\":1310,\"start\":1197}]", "venue": null, "abstract": "[{\"end\":2937,\"start\":1312}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b76\"},\"end\":3086,\"start\":3082},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3088,\"start\":3086},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3091,\"start\":3088},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":3308,\"start\":3304},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":3311,\"start\":3308},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3314,\"start\":3311},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3714,\"start\":3710},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":3717,\"start\":3714},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3720,\"start\":3717},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3733,\"start\":3729},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":3814,\"start\":3809},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3817,\"start\":3814},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3820,\"start\":3817},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":3823,\"start\":3820},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":4218,\"start\":4214},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4423,\"start\":4419},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4593,\"start\":4589},{\"end\":7152,\"start\":7144},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8969,\"start\":8965},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8972,\"start\":8969},{\"end\":8974,\"start\":8972},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":9026,\"start\":9022},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9053,\"start\":9049},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9099,\"start\":9096},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9102,\"start\":9099},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":9105,\"start\":9102},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9118,\"start\":9114},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":9121,\"start\":9118},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":9137,\"start\":9133},{\"end\":9139,\"start\":9137},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":9142,\"start\":9139},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9549,\"start\":9545},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9552,\"start\":9549},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9555,\"start\":9552},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":9739,\"start\":9735},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":9742,\"start\":9739},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9745,\"start\":9742},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":9748,\"start\":9745},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9942,\"start\":9938},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":9945,\"start\":9942},{\"end\":10220,\"start\":10217},{\"end\":10222,\"start\":10220},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":10477,\"start\":10473},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10480,\"start\":10477},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10483,\"start\":10480},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10486,\"start\":10483},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10661,\"start\":10657},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":10689,\"start\":10684},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10692,\"start\":10689},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10695,\"start\":10692},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":10698,\"start\":10695},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10953,\"start\":10949},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10956,\"start\":10953},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":10959,\"start\":10956},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":11322,\"start\":11318},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11325,\"start\":11322},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":11328,\"start\":11325},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":11331,\"start\":11328},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11387,\"start\":11383},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":11390,\"start\":11387},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12195,\"start\":12191},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12198,\"start\":12195},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12201,\"start\":12198},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12204,\"start\":12201},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12207,\"start\":12204},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":12210,\"start\":12207},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":12213,\"start\":12210},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12216,\"start\":12213},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":12219,\"start\":12216},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12376,\"start\":12373},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12379,\"start\":12376},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12382,\"start\":12379},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":12385,\"start\":12382},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12431,\"start\":12428},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12556,\"start\":12552},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12559,\"start\":12556},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12562,\"start\":12559},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":12565,\"start\":12562},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":12587,\"start\":12583},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":12681,\"start\":12676},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":12807,\"start\":12803},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12965,\"start\":12961},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":13668,\"start\":13664},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13671,\"start\":13668},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13674,\"start\":13671},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":13677,\"start\":13674},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":13719,\"start\":13715},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13722,\"start\":13719},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":13725,\"start\":13722},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":13754,\"start\":13750},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13757,\"start\":13754},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":13760,\"start\":13757},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13763,\"start\":13760},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13766,\"start\":13763},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13769,\"start\":13766},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14931,\"start\":14927},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15645,\"start\":15642},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16923,\"start\":16919},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":20032,\"start\":20028},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":20035,\"start\":20032},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":22935,\"start\":22931},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22938,\"start\":22935},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":22978,\"start\":22974},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23163,\"start\":23160},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23676,\"start\":23672},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24184,\"start\":24180},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24222,\"start\":24218},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":24717,\"start\":24713},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28320,\"start\":28316},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28441,\"start\":28437},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":28972,\"start\":28968},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29016,\"start\":29012},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29102,\"start\":29098},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":30145,\"start\":30141},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":30173,\"start\":30168},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":30304,\"start\":30299},{\"end\":32983,\"start\":32975},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":33204,\"start\":33200},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33237,\"start\":33233},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":33439,\"start\":33435},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":34201,\"start\":34197},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":34995,\"start\":34991},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":37007,\"start\":37003},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":37010,\"start\":37007},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":37134,\"start\":37130},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":37752,\"start\":37748},{\"end\":37979,\"start\":37975},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":38452,\"start\":38448},{\"end\":38456,\"start\":38452},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38507,\"start\":38504},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38510,\"start\":38507},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38841,\"start\":38840},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":39029,\"start\":39025},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":39039,\"start\":39035},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":39081,\"start\":39077},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":39129,\"start\":39125},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":39308,\"start\":39304},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":39311,\"start\":39308},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":41713,\"start\":41709},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":41834,\"start\":41830},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":42554,\"start\":42550},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":44469,\"start\":44465},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":46766,\"start\":46762},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":46852,\"start\":46848},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":46916,\"start\":46911},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":46951,\"start\":46946},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":50699,\"start\":50695}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51480,\"start\":51138},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52254,\"start\":51481},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52357,\"start\":52255},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52400,\"start\":52358},{\"attributes\":{\"id\":\"fig_4\"},\"end\":52530,\"start\":52401},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52598,\"start\":52531},{\"attributes\":{\"id\":\"fig_6\"},\"end\":52746,\"start\":52599},{\"attributes\":{\"id\":\"fig_7\"},\"end\":53426,\"start\":52747},{\"attributes\":{\"id\":\"fig_8\"},\"end\":53458,\"start\":53427},{\"attributes\":{\"id\":\"fig_9\"},\"end\":53544,\"start\":53459},{\"attributes\":{\"id\":\"fig_10\"},\"end\":53595,\"start\":53545},{\"attributes\":{\"id\":\"fig_11\"},\"end\":53625,\"start\":53596},{\"attributes\":{\"id\":\"fig_12\"},\"end\":53651,\"start\":53626},{\"attributes\":{\"id\":\"fig_13\"},\"end\":53776,\"start\":53652},{\"attributes\":{\"id\":\"fig_14\"},\"end\":53881,\"start\":53777},{\"attributes\":{\"id\":\"fig_15\"},\"end\":54081,\"start\":53882},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54371,\"start\":54082},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":54528,\"start\":54372},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":55983,\"start\":54529},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56433,\"start\":55984}]", "paragraph": "[{\"end\":4087,\"start\":2953},{\"end\":5420,\"start\":4089},{\"end\":6395,\"start\":5422},{\"end\":7365,\"start\":6397},{\"end\":7468,\"start\":7367},{\"end\":7684,\"start\":7470},{\"end\":8028,\"start\":7686},{\"end\":8170,\"start\":8030},{\"end\":8308,\"start\":8172},{\"end\":8453,\"start\":8310},{\"end\":8613,\"start\":8455},{\"end\":10487,\"start\":8630},{\"end\":11496,\"start\":10489},{\"end\":12269,\"start\":11498},{\"end\":13997,\"start\":12271},{\"end\":14868,\"start\":14008},{\"end\":15276,\"start\":14907},{\"end\":16810,\"start\":15278},{\"end\":17562,\"start\":16894},{\"end\":17784,\"start\":17564},{\"end\":18346,\"start\":17786},{\"end\":18683,\"start\":18348},{\"end\":19142,\"start\":18712},{\"end\":19847,\"start\":19194},{\"end\":20547,\"start\":19858},{\"end\":20670,\"start\":20549},{\"end\":21370,\"start\":20672},{\"end\":21899,\"start\":21433},{\"end\":23484,\"start\":21975},{\"end\":23823,\"start\":23541},{\"end\":24223,\"start\":23947},{\"end\":24453,\"start\":24250},{\"end\":24873,\"start\":24520},{\"end\":25466,\"start\":24885},{\"end\":26331,\"start\":25490},{\"end\":26551,\"start\":26333},{\"end\":27329,\"start\":26570},{\"end\":27522,\"start\":27331},{\"end\":27879,\"start\":27524},{\"end\":31538,\"start\":27895},{\"end\":31862,\"start\":31563},{\"end\":32273,\"start\":31864},{\"end\":32649,\"start\":32289},{\"end\":32911,\"start\":32651},{\"end\":34390,\"start\":32934},{\"end\":35733,\"start\":34405},{\"end\":36654,\"start\":35777},{\"end\":37011,\"start\":36705},{\"end\":38134,\"start\":37013},{\"end\":38953,\"start\":38136},{\"end\":39589,\"start\":38985},{\"end\":40137,\"start\":39603},{\"end\":40571,\"start\":40188},{\"end\":41471,\"start\":40573},{\"end\":41625,\"start\":41536},{\"end\":41917,\"start\":41660},{\"end\":42510,\"start\":41919},{\"end\":43679,\"start\":42512},{\"end\":44618,\"start\":43710},{\"end\":44979,\"start\":44638},{\"end\":45215,\"start\":45006},{\"end\":45439,\"start\":45217},{\"end\":45973,\"start\":45461},{\"end\":46091,\"start\":46010},{\"end\":46387,\"start\":46093},{\"end\":47114,\"start\":46389},{\"end\":47375,\"start\":47116},{\"end\":48494,\"start\":47377},{\"end\":48830,\"start\":48524},{\"end\":48924,\"start\":48847},{\"end\":49110,\"start\":48926},{\"end\":49347,\"start\":49140},{\"end\":49603,\"start\":49393},{\"end\":50251,\"start\":49646},{\"end\":50546,\"start\":50273},{\"end\":51137,\"start\":50579}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16873,\"start\":16811},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18711,\"start\":18684},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19193,\"start\":19143},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21432,\"start\":21371},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21974,\"start\":21900},{\"attributes\":{\"id\":\"formula_5\"},\"end\":23540,\"start\":23485},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23946,\"start\":23824},{\"attributes\":{\"id\":\"formula_7\"},\"end\":24249,\"start\":24224},{\"attributes\":{\"id\":\"formula_8\"},\"end\":24519,\"start\":24454}]", "table_ref": "[{\"end\":46983,\"start\":46976}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2951,\"start\":2939},{\"attributes\":{\"n\":\"2.\"},\"end\":8628,\"start\":8616},{\"attributes\":{\"n\":\"3.\"},\"end\":14006,\"start\":14000},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14905,\"start\":14871},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16892,\"start\":16875},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19856,\"start\":19850},{\"attributes\":{\"n\":\"4.\"},\"end\":24883,\"start\":24876},{\"attributes\":{\"n\":\"4.1.\"},\"end\":25488,\"start\":25469},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26568,\"start\":26554},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27893,\"start\":27882},{\"attributes\":{\"n\":\"4.4.\"},\"end\":31561,\"start\":31541},{\"attributes\":{\"n\":\"5.\"},\"end\":32287,\"start\":32276},{\"end\":32932,\"start\":32914},{\"attributes\":{\"n\":\"6.\"},\"end\":34403,\"start\":34393},{\"end\":35775,\"start\":35736},{\"end\":36679,\"start\":36657},{\"end\":36703,\"start\":36682},{\"end\":38983,\"start\":38956},{\"end\":39601,\"start\":39592},{\"end\":40162,\"start\":40140},{\"end\":40186,\"start\":40165},{\"end\":41493,\"start\":41474},{\"end\":41515,\"start\":41496},{\"end\":41534,\"start\":41518},{\"end\":41658,\"start\":41628},{\"end\":43708,\"start\":43682},{\"end\":44636,\"start\":44621},{\"end\":45004,\"start\":44982},{\"end\":45459,\"start\":45442},{\"end\":45983,\"start\":45976},{\"end\":46008,\"start\":45986},{\"end\":48522,\"start\":48497},{\"end\":48838,\"start\":48833},{\"end\":48845,\"start\":48841},{\"end\":49138,\"start\":49113},{\"end\":49391,\"start\":49350},{\"end\":49644,\"start\":49606},{\"end\":50271,\"start\":50254},{\"end\":50577,\"start\":50549},{\"end\":51159,\"start\":51139},{\"end\":51492,\"start\":51482},{\"end\":52266,\"start\":52256},{\"end\":52362,\"start\":52359},{\"end\":52415,\"start\":52402},{\"end\":52535,\"start\":52532},{\"end\":52603,\"start\":52600},{\"end\":52761,\"start\":52748},{\"end\":53441,\"start\":53428},{\"end\":53666,\"start\":53653},{\"end\":53791,\"start\":53778},{\"end\":53897,\"start\":53883},{\"end\":54380,\"start\":54373},{\"end\":55988,\"start\":55985}]", "table": "[{\"end\":55983,\"start\":55832}]", "figure_caption": "[{\"end\":51480,\"start\":51162},{\"end\":52254,\"start\":51494},{\"end\":52357,\"start\":52268},{\"end\":52400,\"start\":52363},{\"end\":52530,\"start\":52417},{\"end\":52598,\"start\":52536},{\"end\":52746,\"start\":52604},{\"end\":53426,\"start\":52763},{\"end\":53458,\"start\":53443},{\"end\":53544,\"start\":53461},{\"end\":53595,\"start\":53547},{\"end\":53625,\"start\":53598},{\"end\":53651,\"start\":53628},{\"end\":53776,\"start\":53668},{\"end\":53881,\"start\":53793},{\"end\":54081,\"start\":53900},{\"end\":54371,\"start\":54084},{\"end\":54528,\"start\":54382},{\"end\":55832,\"start\":54531},{\"end\":56433,\"start\":55990}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":7585,\"start\":7579},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25708,\"start\":25702},{\"end\":26199,\"start\":26193},{\"end\":26535,\"start\":26529},{\"end\":27139,\"start\":27131},{\"end\":27648,\"start\":27642},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27778,\"start\":27772},{\"end\":29434,\"start\":29428},{\"end\":29744,\"start\":29738},{\"end\":29751,\"start\":29745},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":31219,\"start\":31213},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32272,\"start\":32266},{\"end\":32764,\"start\":32756},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33028,\"start\":33022},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":41109,\"start\":41101},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":47963,\"start\":47955},{\"end\":48961,\"start\":48953},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":50544,\"start\":50536},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":50712,\"start\":50703}]", "bib_author_first_name": "[{\"end\":56835,\"start\":56829},{\"end\":56853,\"start\":56845},{\"end\":56868,\"start\":56864},{\"end\":56881,\"start\":56878},{\"end\":56894,\"start\":56889},{\"end\":56910,\"start\":56904},{\"end\":56912,\"start\":56911},{\"end\":56927,\"start\":56920},{\"end\":57180,\"start\":57176},{\"end\":57198,\"start\":57195},{\"end\":57211,\"start\":57205},{\"end\":57213,\"start\":57212},{\"end\":57226,\"start\":57221},{\"end\":57239,\"start\":57235},{\"end\":57241,\"start\":57240},{\"end\":57536,\"start\":57531},{\"end\":57552,\"start\":57545},{\"end\":57569,\"start\":57562},{\"end\":57589,\"start\":57580},{\"end\":57733,\"start\":57726},{\"end\":58489,\"start\":58488},{\"end\":59731,\"start\":59723},{\"end\":59743,\"start\":59738},{\"end\":59760,\"start\":59753},{\"end\":59773,\"start\":59767},{\"end\":59775,\"start\":59774},{\"end\":60033,\"start\":60027},{\"end\":60046,\"start\":60042},{\"end\":60058,\"start\":60055},{\"end\":60071,\"start\":60068},{\"end\":60087,\"start\":60081},{\"end\":60101,\"start\":60096},{\"end\":60119,\"start\":60113},{\"end\":60131,\"start\":60127},{\"end\":60143,\"start\":60138},{\"end\":60497,\"start\":60489},{\"end\":60508,\"start\":60503},{\"end\":60521,\"start\":60517},{\"end\":60539,\"start\":60533},{\"end\":60554,\"start\":60550},{\"end\":60559,\"start\":60555},{\"end\":60575,\"start\":60569},{\"end\":60590,\"start\":60584},{\"end\":60595,\"start\":60591},{\"end\":60615,\"start\":60606},{\"end\":60630,\"start\":60625},{\"end\":60644,\"start\":60636},{\"end\":60952,\"start\":60947},{\"end\":60962,\"start\":60957},{\"end\":60979,\"start\":60970},{\"end\":60990,\"start\":60984},{\"end\":60992,\"start\":60991},{\"end\":61010,\"start\":61004},{\"end\":61350,\"start\":61342},{\"end\":61359,\"start\":61358},{\"end\":61361,\"start\":61360},{\"end\":61376,\"start\":61371},{\"end\":61392,\"start\":61384},{\"end\":61408,\"start\":61401},{\"end\":61422,\"start\":61417},{\"end\":61442,\"start\":61435},{\"end\":61455,\"start\":61451},{\"end\":61469,\"start\":61465},{\"end\":61483,\"start\":61479},{\"end\":61506,\"start\":61497},{\"end\":61523,\"start\":61517},{\"end\":61540,\"start\":61534},{\"end\":61962,\"start\":61956},{\"end\":61986,\"start\":61978},{\"end\":62009,\"start\":62004},{\"end\":62020,\"start\":62017},{\"end\":62022,\"start\":62021},{\"end\":62035,\"start\":62030},{\"end\":62051,\"start\":62044},{\"end\":62067,\"start\":62061},{\"end\":62069,\"start\":62068},{\"end\":62083,\"start\":62080},{\"end\":62095,\"start\":62090},{\"end\":62439,\"start\":62435},{\"end\":62454,\"start\":62447},{\"end\":62468,\"start\":62464},{\"end\":62485,\"start\":62478},{\"end\":62501,\"start\":62495},{\"end\":62513,\"start\":62509},{\"end\":62528,\"start\":62524},{\"end\":62545,\"start\":62538},{\"end\":62896,\"start\":62893},{\"end\":62910,\"start\":62904},{\"end\":62925,\"start\":62918},{\"end\":62945,\"start\":62937},{\"end\":63220,\"start\":63216},{\"end\":63233,\"start\":63226},{\"end\":63249,\"start\":63240},{\"end\":63263,\"start\":63255},{\"end\":63278,\"start\":63271},{\"end\":63584,\"start\":63580},{\"end\":63601,\"start\":63591},{\"end\":63618,\"start\":63611},{\"end\":63967,\"start\":63958},{\"end\":63983,\"start\":63978},{\"end\":63998,\"start\":63993},{\"end\":64015,\"start\":64009},{\"end\":64219,\"start\":64218},{\"end\":64233,\"start\":64228},{\"end\":64250,\"start\":64243},{\"end\":64270,\"start\":64263},{\"end\":64272,\"start\":64271},{\"end\":64428,\"start\":64424},{\"end\":64441,\"start\":64440},{\"end\":64443,\"start\":64442},{\"end\":64456,\"start\":64451},{\"end\":64467,\"start\":64463},{\"end\":64486,\"start\":64481},{\"end\":64920,\"start\":64914},{\"end\":64931,\"start\":64926},{\"end\":64947,\"start\":64941},{\"end\":64961,\"start\":64958},{\"end\":64976,\"start\":64969},{\"end\":64985,\"start\":64981},{\"end\":64999,\"start\":64994},{\"end\":65014,\"start\":65008},{\"end\":65035,\"start\":65030},{\"end\":65049,\"start\":65044},{\"end\":65451,\"start\":65445},{\"end\":65461,\"start\":65457},{\"end\":65469,\"start\":65466},{\"end\":65482,\"start\":65474},{\"end\":65496,\"start\":65488},{\"end\":65507,\"start\":65502},{\"end\":65795,\"start\":65791},{\"end\":65814,\"start\":65807},{\"end\":65826,\"start\":65819},{\"end\":65844,\"start\":65838},{\"end\":65865,\"start\":65856},{\"end\":66175,\"start\":66170},{\"end\":66190,\"start\":66184},{\"end\":66203,\"start\":66199},{\"end\":66222,\"start\":66211},{\"end\":66236,\"start\":66230},{\"end\":66255,\"start\":66248},{\"end\":66579,\"start\":66575},{\"end\":66593,\"start\":66587},{\"end\":66604,\"start\":66598},{\"end\":66616,\"start\":66611},{\"end\":66626,\"start\":66623},{\"end\":66638,\"start\":66633},{\"end\":66655,\"start\":66649},{\"end\":66670,\"start\":66661},{\"end\":66678,\"start\":66675},{\"end\":67081,\"start\":67080},{\"end\":67404,\"start\":67399},{\"end\":67420,\"start\":67415},{\"end\":67438,\"start\":67432},{\"end\":67709,\"start\":67701},{\"end\":67726,\"start\":67719},{\"end\":67746,\"start\":67738},{\"end\":67765,\"start\":67756},{\"end\":67780,\"start\":67776},{\"end\":68105,\"start\":68096},{\"end\":68118,\"start\":68111},{\"end\":68139,\"start\":68129},{\"end\":68157,\"start\":68150},{\"end\":68171,\"start\":68166},{\"end\":68189,\"start\":68180},{\"end\":68209,\"start\":68200},{\"end\":68520,\"start\":68511},{\"end\":68531,\"start\":68526},{\"end\":68546,\"start\":68541},{\"end\":68562,\"start\":68555},{\"end\":68573,\"start\":68567},{\"end\":68589,\"start\":68581},{\"end\":68606,\"start\":68599},{\"end\":68623,\"start\":68614},{\"end\":68641,\"start\":68634},{\"end\":68661,\"start\":68652},{\"end\":68985,\"start\":68984},{\"end\":69001,\"start\":68996},{\"end\":69230,\"start\":69222},{\"end\":69245,\"start\":69240},{\"end\":69265,\"start\":69258},{\"end\":69267,\"start\":69266},{\"end\":69736,\"start\":69728},{\"end\":69749,\"start\":69744},{\"end\":69765,\"start\":69759},{\"end\":69779,\"start\":69771},{\"end\":70067,\"start\":70066},{\"end\":70084,\"start\":70078},{\"end\":70086,\"start\":70085},{\"end\":70330,\"start\":70326},{\"end\":70341,\"start\":70338},{\"end\":70535,\"start\":70528},{\"end\":70548,\"start\":70540},{\"end\":70560,\"start\":70553},{\"end\":70573,\"start\":70568},{\"end\":70584,\"start\":70579},{\"end\":70603,\"start\":70595},{\"end\":70618,\"start\":70609},{\"end\":70898,\"start\":70892},{\"end\":70907,\"start\":70903},{\"end\":70926,\"start\":70919},{\"end\":70944,\"start\":70939},{\"end\":70961,\"start\":70952},{\"end\":70978,\"start\":70971},{\"end\":70990,\"start\":70984},{\"end\":71006,\"start\":71000},{\"end\":71025,\"start\":71018},{\"end\":71043,\"start\":71035},{\"end\":71274,\"start\":71267},{\"end\":71284,\"start\":71279},{\"end\":71298,\"start\":71294},{\"end\":71314,\"start\":71308},{\"end\":71583,\"start\":71576},{\"end\":71595,\"start\":71589},{\"end\":71608,\"start\":71600},{\"end\":71632,\"start\":71623},{\"end\":71946,\"start\":71941},{\"end\":71960,\"start\":71952},{\"end\":71971,\"start\":71966},{\"end\":72318,\"start\":72311},{\"end\":72334,\"start\":72329},{\"end\":72347,\"start\":72342},{\"end\":72364,\"start\":72357},{\"end\":72382,\"start\":72375},{\"end\":72398,\"start\":72393},{\"end\":72695,\"start\":72689},{\"end\":72706,\"start\":72700},{\"end\":72721,\"start\":72712},{\"end\":72737,\"start\":72734},{\"end\":72753,\"start\":72748},{\"end\":72768,\"start\":72763},{\"end\":73094,\"start\":73087},{\"end\":73101,\"start\":73095},{\"end\":73116,\"start\":73111},{\"end\":73132,\"start\":73126},{\"end\":73150,\"start\":73141},{\"end\":73163,\"start\":73160},{\"end\":73176,\"start\":73173},{\"end\":73190,\"start\":73185},{\"end\":73205,\"start\":73198},{\"end\":73217,\"start\":73213},{\"end\":73232,\"start\":73227},{\"end\":73248,\"start\":73241},{\"end\":73260,\"start\":73255},{\"end\":73838,\"start\":73831},{\"end\":73857,\"start\":73850},{\"end\":73869,\"start\":73862},{\"end\":73886,\"start\":73877},{\"end\":74251,\"start\":74243},{\"end\":74263,\"start\":74260},{\"end\":74265,\"start\":74264},{\"end\":74280,\"start\":74275},{\"end\":74295,\"start\":74289},{\"end\":74308,\"start\":74303},{\"end\":74321,\"start\":74317},{\"end\":74338,\"start\":74331},{\"end\":74687,\"start\":74684},{\"end\":74701,\"start\":74700},{\"end\":74717,\"start\":74710},{\"end\":74738,\"start\":74730},{\"end\":74740,\"start\":74739},{\"end\":74753,\"start\":74749},{\"end\":74765,\"start\":74762},{\"end\":75058,\"start\":75055},{\"end\":75072,\"start\":75071},{\"end\":75088,\"start\":75081},{\"end\":75109,\"start\":75101},{\"end\":75111,\"start\":75110},{\"end\":75124,\"start\":75120},{\"end\":75136,\"start\":75133},{\"end\":75478,\"start\":75472},{\"end\":75493,\"start\":75487},{\"end\":75510,\"start\":75502},{\"end\":75796,\"start\":75792},{\"end\":75811,\"start\":75805},{\"end\":75823,\"start\":75819},{\"end\":75841,\"start\":75835},{\"end\":75853,\"start\":75850},{\"end\":75866,\"start\":75858},{\"end\":75878,\"start\":75874},{\"end\":76118,\"start\":76111},{\"end\":76368,\"start\":76361},{\"end\":76370,\"start\":76369},{\"end\":76387,\"start\":76381},{\"end\":76399,\"start\":76393},{\"end\":76401,\"start\":76400},{\"end\":76715,\"start\":76708},{\"end\":76717,\"start\":76716},{\"end\":76735,\"start\":76728},{\"end\":76748,\"start\":76743},{\"end\":76764,\"start\":76759},{\"end\":76776,\"start\":76770},{\"end\":76778,\"start\":76777},{\"end\":76796,\"start\":76788},{\"end\":76809,\"start\":76804},{\"end\":77322,\"start\":77314},{\"end\":77341,\"start\":77337},{\"end\":77355,\"start\":77348},{\"end\":77370,\"start\":77365},{\"end\":77372,\"start\":77371},{\"end\":77391,\"start\":77385},{\"end\":77815,\"start\":77810},{\"end\":77843,\"start\":77836},{\"end\":77857,\"start\":77848},{\"end\":78210,\"start\":78204},{\"end\":78235,\"start\":78226},{\"end\":78249,\"start\":78245},{\"end\":78264,\"start\":78259},{\"end\":78278,\"start\":78272},{\"end\":78291,\"start\":78287},{\"end\":78308,\"start\":78303},{\"end\":78320,\"start\":78314},{\"end\":78322,\"start\":78321},{\"end\":78338,\"start\":78333},{\"end\":78355,\"start\":78347},{\"end\":78811,\"start\":78805},{\"end\":78813,\"start\":78812},{\"end\":78825,\"start\":78822},{\"end\":78841,\"start\":78835},{\"end\":79177,\"start\":79167},{\"end\":79189,\"start\":79184},{\"end\":79206,\"start\":79200},{\"end\":79222,\"start\":79215},{\"end\":79239,\"start\":79233},{\"end\":79589,\"start\":79581},{\"end\":79603,\"start\":79596},{\"end\":79619,\"start\":79611},{\"end\":79621,\"start\":79620},{\"end\":79636,\"start\":79630},{\"end\":79649,\"start\":79646},{\"end\":79651,\"start\":79650},{\"end\":79667,\"start\":79661},{\"end\":79669,\"start\":79668},{\"end\":79684,\"start\":79677},{\"end\":80041,\"start\":80037},{\"end\":80053,\"start\":80050},{\"end\":80070,\"start\":80061},{\"end\":80082,\"start\":80078},{\"end\":80095,\"start\":80090},{\"end\":80113,\"start\":80106},{\"end\":80128,\"start\":80122},{\"end\":80144,\"start\":80138},{\"end\":80157,\"start\":80150},{\"end\":80174,\"start\":80170},{\"end\":80188,\"start\":80183},{\"end\":80207,\"start\":80200},{\"end\":80220,\"start\":80214},{\"end\":80234,\"start\":80227},{\"end\":80753,\"start\":80752},{\"end\":80760,\"start\":80759},{\"end\":80987,\"start\":80983},{\"end\":81002,\"start\":80994},{\"end\":81017,\"start\":81010},{\"end\":81030,\"start\":81022},{\"end\":81041,\"start\":81037},{\"end\":81054,\"start\":81049},{\"end\":81067,\"start\":81060},{\"end\":81515,\"start\":81508},{\"end\":81535,\"start\":81528},{\"end\":81551,\"start\":81545},{\"end\":81831,\"start\":81822},{\"end\":81849,\"start\":81841},{\"end\":81865,\"start\":81859},{\"end\":81882,\"start\":81876},{\"end\":82431,\"start\":82425},{\"end\":82447,\"start\":82442},{\"end\":82462,\"start\":82456},{\"end\":82482,\"start\":82474},{\"end\":82506,\"start\":82498},{\"end\":82705,\"start\":82701},{\"end\":82718,\"start\":82711},{\"end\":82736,\"start\":82731},{\"end\":82749,\"start\":82744},{\"end\":82767,\"start\":82759},{\"end\":83029,\"start\":83023},{\"end\":83046,\"start\":83039},{\"end\":83280,\"start\":83269},{\"end\":83297,\"start\":83289},{\"end\":83312,\"start\":83305},{\"end\":83326,\"start\":83317},{\"end\":83346,\"start\":83337},{\"end\":83643,\"start\":83642},{\"end\":83665,\"start\":83654},{\"end\":84053,\"start\":84045},{\"end\":84083,\"start\":84072},{\"end\":84348,\"start\":84340},{\"end\":84374,\"start\":84367},{\"end\":84386,\"start\":84382},{\"end\":84409,\"start\":84398},{\"end\":84695,\"start\":84687},{\"end\":84709,\"start\":84703},{\"end\":84725,\"start\":84719},{\"end\":84737,\"start\":84730},{\"end\":85223,\"start\":85212},{\"end\":85238,\"start\":85234},{\"end\":85257,\"start\":85249},{\"end\":85270,\"start\":85265},{\"end\":85285,\"start\":85281},{\"end\":85299,\"start\":85294},{\"end\":85316,\"start\":85309},{\"end\":85333,\"start\":85329},{\"end\":85346,\"start\":85342},{\"end\":85364,\"start\":85358},{\"end\":85383,\"start\":85374},{\"end\":85400,\"start\":85394},{\"end\":85835,\"start\":85826},{\"end\":85848,\"start\":85843},{\"end\":85868,\"start\":85859},{\"end\":85886,\"start\":85879},{\"end\":85903,\"start\":85894},{\"end\":86255,\"start\":86248},{\"end\":86272,\"start\":86266},{\"end\":86285,\"start\":86280},{\"end\":86301,\"start\":86293},{\"end\":86318,\"start\":86312},{\"end\":86337,\"start\":86330},{\"end\":86715,\"start\":86708},{\"end\":86733,\"start\":86726},{\"end\":86751,\"start\":86745},{\"end\":87105,\"start\":87096},{\"end\":87127,\"start\":87117},{\"end\":87141,\"start\":87135},{\"end\":87159,\"start\":87151},{\"end\":87541,\"start\":87533},{\"end\":87557,\"start\":87550},{\"end\":87574,\"start\":87567},{\"end\":87592,\"start\":87583},{\"end\":87604,\"start\":87599},{\"end\":88000,\"start\":87996},{\"end\":88014,\"start\":88010},{\"end\":88263,\"start\":88255},{\"end\":88278,\"start\":88272},{\"end\":88293,\"start\":88287},{\"end\":88585,\"start\":88576},{\"end\":88595,\"start\":88590},{\"end\":88607,\"start\":88600},{\"end\":88625,\"start\":88620},{\"end\":88849,\"start\":88842},{\"end\":88862,\"start\":88855},{\"end\":88864,\"start\":88863},{\"end\":89152,\"start\":89150},{\"end\":89164,\"start\":89158},{\"end\":89178,\"start\":89172},{\"end\":89191,\"start\":89184},{\"end\":89201,\"start\":89198},{\"end\":89215,\"start\":89212},{\"end\":89226,\"start\":89220},{\"end\":89243,\"start\":89238},{\"end\":89563,\"start\":89558},{\"end\":89576,\"start\":89572},{\"end\":89590,\"start\":89584},{\"end\":89605,\"start\":89598},{\"end\":89623,\"start\":89616},{\"end\":89640,\"start\":89634},{\"end\":89660,\"start\":89653},{\"end\":89682,\"start\":89677},{\"end\":89695,\"start\":89690},{\"end\":89713,\"start\":89705},{\"end\":89728,\"start\":89723},{\"end\":89741,\"start\":89737},{\"end\":89757,\"start\":89751},{\"end\":89776,\"start\":89769},{\"end\":89791,\"start\":89782},{\"end\":89809,\"start\":89802},{\"end\":89823,\"start\":89820},{\"end\":89838,\"start\":89835},{\"end\":89840,\"start\":89839},{\"end\":89857,\"start\":89850},{\"end\":90427,\"start\":90418},{\"end\":90444,\"start\":90438},{\"end\":90459,\"start\":90452},{\"end\":90474,\"start\":90468},{\"end\":90493,\"start\":90483},{\"end\":90902,\"start\":90896},{\"end\":90917,\"start\":90910},{\"end\":90937,\"start\":90929},{\"end\":91215,\"start\":91210},{\"end\":91231,\"start\":91226},{\"end\":91247,\"start\":91240},{\"end\":91268,\"start\":91259},{\"end\":91288,\"start\":91279},{\"end\":91684,\"start\":91680},{\"end\":91697,\"start\":91691},{\"end\":91715,\"start\":91708},{\"end\":91922,\"start\":91917},{\"end\":91934,\"start\":91929},{\"end\":91955,\"start\":91948},{\"end\":91971,\"start\":91966},{\"end\":91984,\"start\":91979},{\"end\":92001,\"start\":91994},{\"end\":92018,\"start\":92011},{\"end\":92358,\"start\":92351},{\"end\":92378,\"start\":92371},{\"end\":92394,\"start\":92388},{\"end\":92409,\"start\":92404},{\"end\":92423,\"start\":92417},{\"end\":92707,\"start\":92699},{\"end\":92719,\"start\":92714},{\"end\":92732,\"start\":92729},{\"end\":92942,\"start\":92937},{\"end\":92956,\"start\":92949},{\"end\":92972,\"start\":92964},{\"end\":92986,\"start\":92979},{\"end\":93286,\"start\":93279},{\"end\":93301,\"start\":93294},{\"end\":93312,\"start\":93307},{\"end\":93751,\"start\":93744},{\"end\":93763,\"start\":93756},{\"end\":93783,\"start\":93776},{\"end\":93800,\"start\":93795},{\"end\":93817,\"start\":93809},{\"end\":93835,\"start\":93825},{\"end\":93853,\"start\":93844},{\"end\":94129,\"start\":94126},{\"end\":94318,\"start\":94315},{\"end\":94336,\"start\":94330},{\"end\":94348,\"start\":94342},{\"end\":94360,\"start\":94356},{\"end\":94364,\"start\":94361},{\"end\":94374,\"start\":94371},{\"end\":94647,\"start\":94644},{\"end\":94657,\"start\":94652},{\"end\":94668,\"start\":94667},{\"end\":94670,\"start\":94669},{\"end\":94685,\"start\":94678},{\"end\":95076,\"start\":95073},{\"end\":95087,\"start\":95081},{\"end\":95097,\"start\":95093},{\"end\":95106,\"start\":95102},{\"end\":95119,\"start\":95113},{\"end\":95131,\"start\":95124},{\"end\":95145,\"start\":95138},{\"end\":95158,\"start\":95150},{\"end\":95169,\"start\":95164},{\"end\":95583,\"start\":95580},{\"end\":95594,\"start\":95588},{\"end\":95606,\"start\":95602},{\"end\":95621,\"start\":95614},{\"end\":95981,\"start\":95978},{\"end\":95995,\"start\":95989},{\"end\":96009,\"start\":96005},{\"end\":96026,\"start\":96019},{\"end\":96333,\"start\":96331},{\"end\":96346,\"start\":96341},{\"end\":96362,\"start\":96356},{\"end\":96364,\"start\":96363},{\"end\":96656,\"start\":96649},{\"end\":96671,\"start\":96664},{\"end\":96685,\"start\":96679},{\"end\":96687,\"start\":96686},{\"end\":96698,\"start\":96695},{\"end\":96716,\"start\":96710},{\"end\":97059,\"start\":97055},{\"end\":97070,\"start\":97066},{\"end\":97072,\"start\":97071},{\"end\":97085,\"start\":97080},{\"end\":97087,\"start\":97086},{\"end\":97100,\"start\":97096},{\"end\":97102,\"start\":97101},{\"end\":97418,\"start\":97415},{\"end\":97427,\"start\":97424},{\"end\":97436,\"start\":97432},{\"end\":97446,\"start\":97443},{\"end\":97459,\"start\":97452},{\"end\":97748,\"start\":97741},{\"end\":97768,\"start\":97760},{\"end\":97785,\"start\":97778},{\"end\":97802,\"start\":97793},{\"end\":97823,\"start\":97812},{\"end\":97837,\"start\":97830},{\"end\":97854,\"start\":97846},{\"end\":97865,\"start\":97859},{\"end\":97885,\"start\":97878},{\"end\":97901,\"start\":97892},{\"end\":97916,\"start\":97912},{\"end\":98319,\"start\":98312},{\"end\":98338,\"start\":98331},{\"end\":98354,\"start\":98347},{\"end\":98373,\"start\":98364},{\"end\":98392,\"start\":98384},{\"end\":98410,\"start\":98402},{\"end\":98425,\"start\":98418}]", "bib_author_last_name": "[{\"end\":56843,\"start\":56836},{\"end\":56862,\"start\":56854},{\"end\":56876,\"start\":56869},{\"end\":56887,\"start\":56882},{\"end\":56902,\"start\":56895},{\"end\":56918,\"start\":56913},{\"end\":56936,\"start\":56928},{\"end\":57193,\"start\":57181},{\"end\":57203,\"start\":57199},{\"end\":57219,\"start\":57214},{\"end\":57233,\"start\":57227},{\"end\":57248,\"start\":57242},{\"end\":57543,\"start\":57537},{\"end\":57560,\"start\":57553},{\"end\":57578,\"start\":57570},{\"end\":57596,\"start\":57590},{\"end\":57739,\"start\":57734},{\"end\":57746,\"start\":57741},{\"end\":58496,\"start\":58490},{\"end\":59736,\"start\":59732},{\"end\":59751,\"start\":59744},{\"end\":59765,\"start\":59761},{\"end\":59781,\"start\":59776},{\"end\":60040,\"start\":60034},{\"end\":60053,\"start\":60047},{\"end\":60066,\"start\":60059},{\"end\":60079,\"start\":60072},{\"end\":60094,\"start\":60088},{\"end\":60111,\"start\":60102},{\"end\":60125,\"start\":60120},{\"end\":60136,\"start\":60132},{\"end\":60152,\"start\":60144},{\"end\":60501,\"start\":60498},{\"end\":60515,\"start\":60509},{\"end\":60531,\"start\":60522},{\"end\":60548,\"start\":60540},{\"end\":60567,\"start\":60560},{\"end\":60582,\"start\":60576},{\"end\":60604,\"start\":60596},{\"end\":60623,\"start\":60616},{\"end\":60634,\"start\":60631},{\"end\":60651,\"start\":60645},{\"end\":60955,\"start\":60953},{\"end\":60968,\"start\":60963},{\"end\":60982,\"start\":60980},{\"end\":61002,\"start\":60993},{\"end\":61013,\"start\":61011},{\"end\":61356,\"start\":61351},{\"end\":61369,\"start\":61362},{\"end\":61382,\"start\":61377},{\"end\":61399,\"start\":61393},{\"end\":61415,\"start\":61409},{\"end\":61433,\"start\":61423},{\"end\":61449,\"start\":61443},{\"end\":61463,\"start\":61456},{\"end\":61477,\"start\":61470},{\"end\":61495,\"start\":61484},{\"end\":61515,\"start\":61507},{\"end\":61532,\"start\":61524},{\"end\":61546,\"start\":61541},{\"end\":61554,\"start\":61548},{\"end\":61976,\"start\":61963},{\"end\":62002,\"start\":61987},{\"end\":62015,\"start\":62010},{\"end\":62028,\"start\":62023},{\"end\":62042,\"start\":62036},{\"end\":62059,\"start\":62052},{\"end\":62078,\"start\":62070},{\"end\":62088,\"start\":62084},{\"end\":62105,\"start\":62096},{\"end\":62113,\"start\":62107},{\"end\":62445,\"start\":62440},{\"end\":62462,\"start\":62455},{\"end\":62476,\"start\":62469},{\"end\":62493,\"start\":62486},{\"end\":62507,\"start\":62502},{\"end\":62522,\"start\":62514},{\"end\":62536,\"start\":62529},{\"end\":62552,\"start\":62546},{\"end\":62902,\"start\":62897},{\"end\":62916,\"start\":62911},{\"end\":62935,\"start\":62926},{\"end\":62953,\"start\":62946},{\"end\":63224,\"start\":63221},{\"end\":63238,\"start\":63234},{\"end\":63253,\"start\":63250},{\"end\":63269,\"start\":63264},{\"end\":63284,\"start\":63279},{\"end\":63589,\"start\":63585},{\"end\":63609,\"start\":63602},{\"end\":63626,\"start\":63619},{\"end\":63976,\"start\":63968},{\"end\":63991,\"start\":63984},{\"end\":64007,\"start\":63999},{\"end\":64024,\"start\":64016},{\"end\":64226,\"start\":64220},{\"end\":64241,\"start\":64234},{\"end\":64261,\"start\":64251},{\"end\":64281,\"start\":64273},{\"end\":64288,\"start\":64283},{\"end\":64438,\"start\":64429},{\"end\":64449,\"start\":64444},{\"end\":64461,\"start\":64457},{\"end\":64479,\"start\":64468},{\"end\":64496,\"start\":64487},{\"end\":64506,\"start\":64498},{\"end\":64514,\"start\":64508},{\"end\":64924,\"start\":64921},{\"end\":64939,\"start\":64932},{\"end\":64956,\"start\":64948},{\"end\":64967,\"start\":64962},{\"end\":64979,\"start\":64977},{\"end\":64992,\"start\":64986},{\"end\":65006,\"start\":65000},{\"end\":65028,\"start\":65015},{\"end\":65042,\"start\":65036},{\"end\":65060,\"start\":65050},{\"end\":65455,\"start\":65452},{\"end\":65464,\"start\":65462},{\"end\":65472,\"start\":65470},{\"end\":65486,\"start\":65483},{\"end\":65500,\"start\":65497},{\"end\":65511,\"start\":65508},{\"end\":65805,\"start\":65796},{\"end\":65817,\"start\":65815},{\"end\":65836,\"start\":65827},{\"end\":65854,\"start\":65845},{\"end\":65874,\"start\":65866},{\"end\":66182,\"start\":66176},{\"end\":66197,\"start\":66191},{\"end\":66209,\"start\":66204},{\"end\":66228,\"start\":66223},{\"end\":66246,\"start\":66237},{\"end\":66263,\"start\":66256},{\"end\":66585,\"start\":66580},{\"end\":66596,\"start\":66594},{\"end\":66609,\"start\":66605},{\"end\":66621,\"start\":66617},{\"end\":66631,\"start\":66627},{\"end\":66647,\"start\":66639},{\"end\":66659,\"start\":66656},{\"end\":66673,\"start\":66671},{\"end\":66681,\"start\":66679},{\"end\":67089,\"start\":67082},{\"end\":67101,\"start\":67091},{\"end\":67413,\"start\":67405},{\"end\":67430,\"start\":67421},{\"end\":67445,\"start\":67439},{\"end\":67717,\"start\":67710},{\"end\":67736,\"start\":67727},{\"end\":67754,\"start\":67747},{\"end\":67774,\"start\":67766},{\"end\":67791,\"start\":67781},{\"end\":68109,\"start\":68106},{\"end\":68127,\"start\":68119},{\"end\":68148,\"start\":68140},{\"end\":68164,\"start\":68158},{\"end\":68178,\"start\":68172},{\"end\":68198,\"start\":68190},{\"end\":68218,\"start\":68210},{\"end\":68524,\"start\":68521},{\"end\":68539,\"start\":68532},{\"end\":68553,\"start\":68547},{\"end\":68565,\"start\":68563},{\"end\":68579,\"start\":68574},{\"end\":68597,\"start\":68590},{\"end\":68612,\"start\":68607},{\"end\":68632,\"start\":68624},{\"end\":68650,\"start\":68642},{\"end\":68670,\"start\":68662},{\"end\":68994,\"start\":68986},{\"end\":69008,\"start\":69002},{\"end\":69012,\"start\":69010},{\"end\":69238,\"start\":69231},{\"end\":69256,\"start\":69246},{\"end\":69273,\"start\":69268},{\"end\":69742,\"start\":69737},{\"end\":69757,\"start\":69750},{\"end\":69769,\"start\":69766},{\"end\":69782,\"start\":69780},{\"end\":70076,\"start\":70068},{\"end\":70096,\"start\":70087},{\"end\":70103,\"start\":70098},{\"end\":70336,\"start\":70331},{\"end\":70350,\"start\":70342},{\"end\":70538,\"start\":70536},{\"end\":70551,\"start\":70549},{\"end\":70566,\"start\":70561},{\"end\":70577,\"start\":70574},{\"end\":70593,\"start\":70585},{\"end\":70607,\"start\":70604},{\"end\":70627,\"start\":70619},{\"end\":70901,\"start\":70899},{\"end\":70917,\"start\":70908},{\"end\":70937,\"start\":70927},{\"end\":70950,\"start\":70945},{\"end\":70969,\"start\":70962},{\"end\":70982,\"start\":70979},{\"end\":70998,\"start\":70991},{\"end\":71016,\"start\":71007},{\"end\":71033,\"start\":71026},{\"end\":71046,\"start\":71044},{\"end\":71277,\"start\":71275},{\"end\":71292,\"start\":71285},{\"end\":71306,\"start\":71299},{\"end\":71319,\"start\":71315},{\"end\":71587,\"start\":71584},{\"end\":71598,\"start\":71596},{\"end\":71621,\"start\":71609},{\"end\":71637,\"start\":71633},{\"end\":71647,\"start\":71639},{\"end\":71950,\"start\":71947},{\"end\":71964,\"start\":71961},{\"end\":71974,\"start\":71972},{\"end\":72327,\"start\":72319},{\"end\":72340,\"start\":72335},{\"end\":72355,\"start\":72348},{\"end\":72373,\"start\":72365},{\"end\":72391,\"start\":72383},{\"end\":72405,\"start\":72399},{\"end\":72698,\"start\":72696},{\"end\":72710,\"start\":72707},{\"end\":72732,\"start\":72722},{\"end\":72746,\"start\":72738},{\"end\":72761,\"start\":72754},{\"end\":72774,\"start\":72769},{\"end\":73109,\"start\":73102},{\"end\":73124,\"start\":73117},{\"end\":73139,\"start\":73133},{\"end\":73158,\"start\":73151},{\"end\":73171,\"start\":73164},{\"end\":73183,\"start\":73177},{\"end\":73196,\"start\":73191},{\"end\":73211,\"start\":73206},{\"end\":73225,\"start\":73218},{\"end\":73239,\"start\":73233},{\"end\":73253,\"start\":73249},{\"end\":73273,\"start\":73261},{\"end\":73848,\"start\":73839},{\"end\":73860,\"start\":73858},{\"end\":73875,\"start\":73870},{\"end\":73891,\"start\":73887},{\"end\":74258,\"start\":74252},{\"end\":74273,\"start\":74266},{\"end\":74287,\"start\":74281},{\"end\":74301,\"start\":74296},{\"end\":74315,\"start\":74309},{\"end\":74329,\"start\":74322},{\"end\":74353,\"start\":74339},{\"end\":74698,\"start\":74688},{\"end\":74708,\"start\":74702},{\"end\":74728,\"start\":74718},{\"end\":74747,\"start\":74741},{\"end\":74760,\"start\":74754},{\"end\":74777,\"start\":74766},{\"end\":74781,\"start\":74779},{\"end\":75069,\"start\":75059},{\"end\":75079,\"start\":75073},{\"end\":75099,\"start\":75089},{\"end\":75118,\"start\":75112},{\"end\":75131,\"start\":75125},{\"end\":75148,\"start\":75137},{\"end\":75152,\"start\":75150},{\"end\":75485,\"start\":75479},{\"end\":75500,\"start\":75494},{\"end\":75517,\"start\":75511},{\"end\":75803,\"start\":75797},{\"end\":75817,\"start\":75812},{\"end\":75833,\"start\":75824},{\"end\":75848,\"start\":75842},{\"end\":75856,\"start\":75854},{\"end\":75872,\"start\":75867},{\"end\":75886,\"start\":75879},{\"end\":76127,\"start\":76119},{\"end\":76379,\"start\":76371},{\"end\":76391,\"start\":76388},{\"end\":76407,\"start\":76402},{\"end\":76726,\"start\":76718},{\"end\":76741,\"start\":76736},{\"end\":76757,\"start\":76749},{\"end\":76768,\"start\":76765},{\"end\":76786,\"start\":76779},{\"end\":76802,\"start\":76797},{\"end\":76817,\"start\":76810},{\"end\":77335,\"start\":77323},{\"end\":77346,\"start\":77342},{\"end\":77363,\"start\":77356},{\"end\":77383,\"start\":77373},{\"end\":77395,\"start\":77392},{\"end\":77400,\"start\":77397},{\"end\":77834,\"start\":77816},{\"end\":77846,\"start\":77844},{\"end\":77865,\"start\":77858},{\"end\":77871,\"start\":77867},{\"end\":78224,\"start\":78211},{\"end\":78243,\"start\":78236},{\"end\":78257,\"start\":78250},{\"end\":78270,\"start\":78265},{\"end\":78285,\"start\":78279},{\"end\":78301,\"start\":78292},{\"end\":78312,\"start\":78309},{\"end\":78331,\"start\":78323},{\"end\":78345,\"start\":78339},{\"end\":78359,\"start\":78356},{\"end\":78820,\"start\":78814},{\"end\":78833,\"start\":78826},{\"end\":78849,\"start\":78842},{\"end\":79182,\"start\":79178},{\"end\":79198,\"start\":79190},{\"end\":79213,\"start\":79207},{\"end\":79231,\"start\":79223},{\"end\":79249,\"start\":79240},{\"end\":79594,\"start\":79590},{\"end\":79609,\"start\":79604},{\"end\":79628,\"start\":79622},{\"end\":79644,\"start\":79637},{\"end\":79659,\"start\":79652},{\"end\":79675,\"start\":79670},{\"end\":79699,\"start\":79685},{\"end\":80048,\"start\":80042},{\"end\":80059,\"start\":80054},{\"end\":80076,\"start\":80071},{\"end\":80088,\"start\":80083},{\"end\":80104,\"start\":80096},{\"end\":80120,\"start\":80114},{\"end\":80136,\"start\":80129},{\"end\":80148,\"start\":80145},{\"end\":80168,\"start\":80158},{\"end\":80181,\"start\":80175},{\"end\":80198,\"start\":80189},{\"end\":80212,\"start\":80208},{\"end\":80225,\"start\":80221},{\"end\":80241,\"start\":80235},{\"end\":80757,\"start\":80754},{\"end\":80768,\"start\":80761},{\"end\":80992,\"start\":80988},{\"end\":81008,\"start\":81003},{\"end\":81020,\"start\":81018},{\"end\":81035,\"start\":81031},{\"end\":81047,\"start\":81042},{\"end\":81058,\"start\":81055},{\"end\":81072,\"start\":81068},{\"end\":81526,\"start\":81516},{\"end\":81543,\"start\":81536},{\"end\":81559,\"start\":81552},{\"end\":81839,\"start\":81832},{\"end\":81857,\"start\":81850},{\"end\":81874,\"start\":81866},{\"end\":81888,\"start\":81883},{\"end\":82440,\"start\":82432},{\"end\":82454,\"start\":82448},{\"end\":82472,\"start\":82463},{\"end\":82496,\"start\":82483},{\"end\":82709,\"start\":82706},{\"end\":82729,\"start\":82719},{\"end\":82742,\"start\":82737},{\"end\":82757,\"start\":82750},{\"end\":82773,\"start\":82768},{\"end\":83037,\"start\":83030},{\"end\":83053,\"start\":83047},{\"end\":83287,\"start\":83281},{\"end\":83303,\"start\":83298},{\"end\":83315,\"start\":83313},{\"end\":83335,\"start\":83327},{\"end\":83355,\"start\":83347},{\"end\":83652,\"start\":83644},{\"end\":83677,\"start\":83666},{\"end\":83684,\"start\":83679},{\"end\":84070,\"start\":84054},{\"end\":84089,\"start\":84084},{\"end\":84365,\"start\":84349},{\"end\":84380,\"start\":84375},{\"end\":84396,\"start\":84387},{\"end\":84415,\"start\":84410},{\"end\":84701,\"start\":84696},{\"end\":84717,\"start\":84710},{\"end\":84728,\"start\":84726},{\"end\":84746,\"start\":84738},{\"end\":85232,\"start\":85224},{\"end\":85247,\"start\":85239},{\"end\":85263,\"start\":85258},{\"end\":85279,\"start\":85271},{\"end\":85292,\"start\":85286},{\"end\":85307,\"start\":85300},{\"end\":85327,\"start\":85317},{\"end\":85340,\"start\":85334},{\"end\":85356,\"start\":85347},{\"end\":85372,\"start\":85365},{\"end\":85392,\"start\":85384},{\"end\":85410,\"start\":85401},{\"end\":85841,\"start\":85836},{\"end\":85857,\"start\":85849},{\"end\":85877,\"start\":85869},{\"end\":85892,\"start\":85887},{\"end\":85912,\"start\":85904},{\"end\":86264,\"start\":86256},{\"end\":86278,\"start\":86273},{\"end\":86291,\"start\":86286},{\"end\":86310,\"start\":86302},{\"end\":86328,\"start\":86319},{\"end\":86347,\"start\":86338},{\"end\":86724,\"start\":86716},{\"end\":86743,\"start\":86734},{\"end\":86761,\"start\":86752},{\"end\":87115,\"start\":87106},{\"end\":87133,\"start\":87128},{\"end\":87149,\"start\":87142},{\"end\":87164,\"start\":87160},{\"end\":87548,\"start\":87542},{\"end\":87565,\"start\":87558},{\"end\":87581,\"start\":87575},{\"end\":87597,\"start\":87593},{\"end\":87610,\"start\":87605},{\"end\":88008,\"start\":88001},{\"end\":88020,\"start\":88015},{\"end\":88270,\"start\":88264},{\"end\":88285,\"start\":88279},{\"end\":88300,\"start\":88294},{\"end\":88588,\"start\":88586},{\"end\":88598,\"start\":88596},{\"end\":88618,\"start\":88608},{\"end\":88632,\"start\":88626},{\"end\":88853,\"start\":88850},{\"end\":88879,\"start\":88865},{\"end\":89156,\"start\":89153},{\"end\":89170,\"start\":89165},{\"end\":89182,\"start\":89179},{\"end\":89196,\"start\":89192},{\"end\":89210,\"start\":89202},{\"end\":89218,\"start\":89216},{\"end\":89236,\"start\":89227},{\"end\":89247,\"start\":89244},{\"end\":89570,\"start\":89564},{\"end\":89582,\"start\":89577},{\"end\":89596,\"start\":89591},{\"end\":89614,\"start\":89606},{\"end\":89632,\"start\":89624},{\"end\":89651,\"start\":89641},{\"end\":89675,\"start\":89661},{\"end\":89688,\"start\":89683},{\"end\":89703,\"start\":89696},{\"end\":89721,\"start\":89714},{\"end\":89735,\"start\":89729},{\"end\":89749,\"start\":89742},{\"end\":89767,\"start\":89758},{\"end\":89780,\"start\":89777},{\"end\":89800,\"start\":89792},{\"end\":89818,\"start\":89810},{\"end\":89833,\"start\":89824},{\"end\":89848,\"start\":89841},{\"end\":89867,\"start\":89858},{\"end\":90436,\"start\":90428},{\"end\":90450,\"start\":90445},{\"end\":90466,\"start\":90460},{\"end\":90481,\"start\":90475},{\"end\":90500,\"start\":90494},{\"end\":90908,\"start\":90903},{\"end\":90927,\"start\":90918},{\"end\":90945,\"start\":90938},{\"end\":91224,\"start\":91216},{\"end\":91238,\"start\":91232},{\"end\":91257,\"start\":91248},{\"end\":91277,\"start\":91269},{\"end\":91297,\"start\":91289},{\"end\":91689,\"start\":91685},{\"end\":91706,\"start\":91698},{\"end\":91725,\"start\":91716},{\"end\":91927,\"start\":91923},{\"end\":91946,\"start\":91935},{\"end\":91964,\"start\":91956},{\"end\":91977,\"start\":91972},{\"end\":91992,\"start\":91985},{\"end\":92009,\"start\":92002},{\"end\":92028,\"start\":92019},{\"end\":92369,\"start\":92359},{\"end\":92386,\"start\":92379},{\"end\":92402,\"start\":92395},{\"end\":92415,\"start\":92410},{\"end\":92429,\"start\":92424},{\"end\":92712,\"start\":92708},{\"end\":92727,\"start\":92720},{\"end\":92755,\"start\":92733},{\"end\":92947,\"start\":92943},{\"end\":92962,\"start\":92957},{\"end\":92977,\"start\":92973},{\"end\":92990,\"start\":92987},{\"end\":93292,\"start\":93287},{\"end\":93305,\"start\":93302},{\"end\":93319,\"start\":93313},{\"end\":93754,\"start\":93752},{\"end\":93774,\"start\":93764},{\"end\":93793,\"start\":93784},{\"end\":93807,\"start\":93801},{\"end\":93823,\"start\":93818},{\"end\":93842,\"start\":93836},{\"end\":93862,\"start\":93854},{\"end\":94138,\"start\":94130},{\"end\":94328,\"start\":94319},{\"end\":94340,\"start\":94337},{\"end\":94354,\"start\":94349},{\"end\":94369,\"start\":94365},{\"end\":94380,\"start\":94375},{\"end\":94650,\"start\":94648},{\"end\":94665,\"start\":94658},{\"end\":94676,\"start\":94671},{\"end\":94694,\"start\":94686},{\"end\":94703,\"start\":94696},{\"end\":95079,\"start\":95077},{\"end\":95091,\"start\":95088},{\"end\":95100,\"start\":95098},{\"end\":95111,\"start\":95107},{\"end\":95122,\"start\":95120},{\"end\":95136,\"start\":95132},{\"end\":95148,\"start\":95146},{\"end\":95162,\"start\":95159},{\"end\":95173,\"start\":95170},{\"end\":95586,\"start\":95584},{\"end\":95600,\"start\":95595},{\"end\":95612,\"start\":95607},{\"end\":95626,\"start\":95622},{\"end\":95987,\"start\":95982},{\"end\":96003,\"start\":95996},{\"end\":96017,\"start\":96010},{\"end\":96033,\"start\":96027},{\"end\":96339,\"start\":96334},{\"end\":96354,\"start\":96347},{\"end\":96370,\"start\":96365},{\"end\":96662,\"start\":96657},{\"end\":96677,\"start\":96672},{\"end\":96693,\"start\":96688},{\"end\":96708,\"start\":96699},{\"end\":96721,\"start\":96717},{\"end\":97064,\"start\":97060},{\"end\":97078,\"start\":97073},{\"end\":97094,\"start\":97088},{\"end\":97113,\"start\":97103},{\"end\":97422,\"start\":97419},{\"end\":97430,\"start\":97428},{\"end\":97441,\"start\":97437},{\"end\":97450,\"start\":97447},{\"end\":97464,\"start\":97460},{\"end\":97758,\"start\":97749},{\"end\":97776,\"start\":97769},{\"end\":97791,\"start\":97786},{\"end\":97810,\"start\":97803},{\"end\":97828,\"start\":97824},{\"end\":97844,\"start\":97838},{\"end\":97857,\"start\":97855},{\"end\":97876,\"start\":97866},{\"end\":97890,\"start\":97886},{\"end\":97910,\"start\":97902},{\"end\":97927,\"start\":97917},{\"end\":98329,\"start\":98320},{\"end\":98345,\"start\":98339},{\"end\":98362,\"start\":98355},{\"end\":98382,\"start\":98374},{\"end\":98400,\"start\":98393},{\"end\":98416,\"start\":98411},{\"end\":98430,\"start\":98426}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7448214},\"end\":57127,\"start\":56805},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":5038229},\"end\":57482,\"start\":57129},{\"attributes\":{\"id\":\"b2\"},\"end\":57722,\"start\":57484},{\"attributes\":{\"id\":\"b3\"},\"end\":57830,\"start\":57724},{\"attributes\":{\"id\":\"b4\"},\"end\":59700,\"start\":57832},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52070144},\"end\":59980,\"start\":59702},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":968962},\"end\":60424,\"start\":59982},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207236141},\"end\":60945,\"start\":60426},{\"attributes\":{\"doi\":\"arXiv:2012.09790\",\"id\":\"b8\"},\"end\":61288,\"start\":60947},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202538884},\"end\":61911,\"start\":61290},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49211357},\"end\":62377,\"start\":61913},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":190000375},\"end\":62891,\"start\":62379},{\"attributes\":{\"id\":\"b12\"},\"end\":63214,\"start\":62893},{\"attributes\":{\"id\":\"b13\"},\"end\":63501,\"start\":63216},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":10580200},\"end\":63877,\"start\":63503},{\"attributes\":{\"id\":\"b15\"},\"end\":64201,\"start\":63879},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2036193},\"end\":64422,\"start\":64203},{\"attributes\":{\"doi\":\"arXiv:1810.01367\",\"id\":\"b17\"},\"end\":64826,\"start\":64424},{\"attributes\":{\"id\":\"b18\"},\"end\":65360,\"start\":64828},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3248785},\"end\":65722,\"start\":65362},{\"attributes\":{\"doi\":\"14:1-14:17\",\"id\":\"b20\",\"matched_paper_id\":52931186},\"end\":66112,\"start\":65724},{\"attributes\":{\"doi\":\"257:1-257:15\",\"id\":\"b21\",\"matched_paper_id\":54151018},\"end\":66502,\"start\":66114},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52955951},\"end\":66985,\"start\":66504},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":120969358},\"end\":67357,\"start\":66987},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3394943},\"end\":67638,\"start\":67359},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":383385},\"end\":68054,\"start\":67640},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":202538634},\"end\":68487,\"start\":68056},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":44073530},\"end\":68938,\"start\":68489},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b28\"},\"end\":69156,\"start\":68940},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":209323955},\"end\":69624,\"start\":69158},{\"attributes\":{\"id\":\"b30\"},\"end\":69650,\"start\":69626},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4383513},\"end\":70028,\"start\":69652},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1538200},\"end\":70301,\"start\":70030},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1363510},\"end\":70444,\"start\":70303},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2396194},\"end\":70888,\"start\":70446},{\"attributes\":{\"id\":\"b35\"},\"end\":71265,\"start\":70890},{\"attributes\":{\"id\":\"b36\"},\"end\":71546,\"start\":71267},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":220686483},\"end\":71867,\"start\":71548},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":711929},\"end\":72244,\"start\":71869},{\"attributes\":{\"id\":\"b39\"},\"end\":72649,\"start\":72246},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":41191245},\"end\":73008,\"start\":72651},{\"attributes\":{\"id\":\"b41\"},\"end\":73730,\"start\":73010},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":16746150},\"end\":74209,\"start\":73732},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":104292390},\"end\":74610,\"start\":74211},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":213175590},\"end\":74981,\"start\":74612},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":213175590},\"end\":75436,\"start\":74983},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":14077468},\"end\":75723,\"start\":75438},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":12082265},\"end\":76109,\"start\":75725},{\"attributes\":{\"id\":\"b48\"},\"end\":76282,\"start\":76111},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":206592546},\"end\":76646,\"start\":76284},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":11830123},\"end\":77202,\"start\":76648},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":1480266},\"end\":77723,\"start\":77204},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":49306024},\"end\":78149,\"start\":77725},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":1459429},\"end\":78727,\"start\":78151},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":12500812},\"end\":79084,\"start\":78729},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":58007025},\"end\":79579,\"start\":79086},{\"attributes\":{\"doi\":\"arXiv:2011.12948\",\"id\":\"b56\"},\"end\":79965,\"start\":79581},{\"attributes\":{\"id\":\"b57\"},\"end\":80750,\"start\":79967},{\"attributes\":{\"id\":\"b58\"},\"end\":80981,\"start\":80752},{\"attributes\":{\"id\":\"b59\"},\"end\":81440,\"start\":80983},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":13947261},\"end\":81769,\"start\":81442},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":1756681},\"end\":82423,\"start\":81771},{\"attributes\":{\"id\":\"b62\"},\"end\":82699,\"start\":82425},{\"attributes\":{\"id\":\"b63\"},\"end\":83000,\"start\":82701},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":221112229},\"end\":83217,\"start\":83002},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":220825626},\"end\":83607,\"start\":83219},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":1728538},\"end\":84010,\"start\":83609},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":1728538},\"end\":84277,\"start\":84012},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":977535},\"end\":84663,\"start\":84279},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":1240104},\"end\":84894,\"start\":84665},{\"attributes\":{\"id\":\"b70\"},\"end\":85185,\"start\":84896},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":160009798},\"end\":85749,\"start\":85187},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":220702902},\"end\":86191,\"start\":85751},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":54444417},\"end\":86615,\"start\":86193},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":174798113},\"end\":87026,\"start\":86617},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":21897267},\"end\":87416,\"start\":87028},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":206818061},\"end\":87955,\"start\":87418},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":11952089},\"end\":88194,\"start\":87957},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":14654810},\"end\":88506,\"start\":88196},{\"attributes\":{\"id\":\"b79\"},\"end\":88769,\"start\":88508},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":110119024},\"end\":89052,\"start\":88771},{\"attributes\":{\"id\":\"b81\"},\"end\":89554,\"start\":89054},{\"attributes\":{\"id\":\"b82\"},\"end\":90337,\"start\":89556},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":1833660},\"end\":90828,\"start\":90339},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":219950625},\"end\":91143,\"start\":90830},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":165163970},\"end\":91563,\"start\":91145},{\"attributes\":{\"id\":\"b86\"},\"end\":91915,\"start\":91565},{\"attributes\":{\"id\":\"b87\"},\"end\":92312,\"start\":91917},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":11169014},\"end\":92619,\"start\":92314},{\"attributes\":{\"id\":\"b89\"},\"end\":92935,\"start\":92621},{\"attributes\":{\"id\":\"b90\"},\"end\":93210,\"start\":92937},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":54446451},\"end\":93681,\"start\":93212},{\"attributes\":{\"doi\":\"27:1-27:15\",\"id\":\"b92\",\"matched_paper_id\":207560964},\"end\":94122,\"start\":93683},{\"attributes\":{\"id\":\"b93\"},\"end\":94219,\"start\":94124},{\"attributes\":{\"id\":\"b94\"},\"end\":94552,\"start\":94221},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":1503186},\"end\":94975,\"start\":94554},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":4701584},\"end\":95506,\"start\":94977},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":80628373},\"end\":95976,\"start\":95508},{\"attributes\":{\"doi\":\"arXiv:2010.07492\",\"id\":\"b98\"},\"end\":96276,\"start\":95978},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":15973812},\"end\":96575,\"start\":96278},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":4766599},\"end\":96979,\"start\":96577},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":207761262},\"end\":97359,\"start\":96981},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":4811605},\"end\":97681,\"start\":97361},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":9616070},\"end\":98252,\"start\":97683},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":5028882},\"end\":98746,\"start\":98254}]", "bib_title": "[{\"end\":56827,\"start\":56805},{\"end\":57174,\"start\":57129},{\"end\":58486,\"start\":57832},{\"end\":59721,\"start\":59702},{\"end\":60025,\"start\":59982},{\"end\":60487,\"start\":60426},{\"end\":61340,\"start\":61290},{\"end\":61954,\"start\":61913},{\"end\":62433,\"start\":62379},{\"end\":63578,\"start\":63503},{\"end\":64216,\"start\":64203},{\"end\":64912,\"start\":64828},{\"end\":65443,\"start\":65362},{\"end\":65789,\"start\":65724},{\"end\":66168,\"start\":66114},{\"end\":66573,\"start\":66504},{\"end\":67078,\"start\":66987},{\"end\":67397,\"start\":67359},{\"end\":67699,\"start\":67640},{\"end\":68094,\"start\":68056},{\"end\":68509,\"start\":68489},{\"end\":69220,\"start\":69158},{\"end\":69726,\"start\":69652},{\"end\":70064,\"start\":70030},{\"end\":70324,\"start\":70303},{\"end\":70526,\"start\":70446},{\"end\":71574,\"start\":71548},{\"end\":71939,\"start\":71869},{\"end\":72309,\"start\":72246},{\"end\":72687,\"start\":72651},{\"end\":73829,\"start\":73732},{\"end\":74241,\"start\":74211},{\"end\":74682,\"start\":74612},{\"end\":75053,\"start\":74983},{\"end\":75470,\"start\":75438},{\"end\":75790,\"start\":75725},{\"end\":76359,\"start\":76284},{\"end\":76706,\"start\":76648},{\"end\":77312,\"start\":77204},{\"end\":77808,\"start\":77725},{\"end\":78202,\"start\":78151},{\"end\":78803,\"start\":78729},{\"end\":79165,\"start\":79086},{\"end\":81506,\"start\":81442},{\"end\":81820,\"start\":81771},{\"end\":83021,\"start\":83002},{\"end\":83267,\"start\":83219},{\"end\":83640,\"start\":83609},{\"end\":84043,\"start\":84012},{\"end\":84338,\"start\":84279},{\"end\":84685,\"start\":84665},{\"end\":85210,\"start\":85187},{\"end\":85824,\"start\":85751},{\"end\":86246,\"start\":86193},{\"end\":86706,\"start\":86617},{\"end\":87094,\"start\":87028},{\"end\":87531,\"start\":87418},{\"end\":87994,\"start\":87957},{\"end\":88253,\"start\":88196},{\"end\":88840,\"start\":88771},{\"end\":89148,\"start\":89054},{\"end\":90416,\"start\":90339},{\"end\":90894,\"start\":90830},{\"end\":91208,\"start\":91145},{\"end\":92349,\"start\":92314},{\"end\":93277,\"start\":93212},{\"end\":93742,\"start\":93683},{\"end\":94642,\"start\":94554},{\"end\":95071,\"start\":94977},{\"end\":95578,\"start\":95508},{\"end\":96329,\"start\":96278},{\"end\":96647,\"start\":96577},{\"end\":97053,\"start\":96981},{\"end\":97413,\"start\":97361},{\"end\":97739,\"start\":97683},{\"end\":98310,\"start\":98254}]", "bib_author": "[{\"end\":56845,\"start\":56829},{\"end\":56864,\"start\":56845},{\"end\":56878,\"start\":56864},{\"end\":56889,\"start\":56878},{\"end\":56904,\"start\":56889},{\"end\":56920,\"start\":56904},{\"end\":56938,\"start\":56920},{\"end\":57195,\"start\":57176},{\"end\":57205,\"start\":57195},{\"end\":57221,\"start\":57205},{\"end\":57235,\"start\":57221},{\"end\":57250,\"start\":57235},{\"end\":57545,\"start\":57531},{\"end\":57562,\"start\":57545},{\"end\":57580,\"start\":57562},{\"end\":57598,\"start\":57580},{\"end\":57741,\"start\":57726},{\"end\":57748,\"start\":57741},{\"end\":58498,\"start\":58488},{\"end\":59738,\"start\":59723},{\"end\":59753,\"start\":59738},{\"end\":59767,\"start\":59753},{\"end\":59783,\"start\":59767},{\"end\":60042,\"start\":60027},{\"end\":60055,\"start\":60042},{\"end\":60068,\"start\":60055},{\"end\":60081,\"start\":60068},{\"end\":60096,\"start\":60081},{\"end\":60113,\"start\":60096},{\"end\":60127,\"start\":60113},{\"end\":60138,\"start\":60127},{\"end\":60154,\"start\":60138},{\"end\":60503,\"start\":60489},{\"end\":60517,\"start\":60503},{\"end\":60533,\"start\":60517},{\"end\":60550,\"start\":60533},{\"end\":60569,\"start\":60550},{\"end\":60584,\"start\":60569},{\"end\":60606,\"start\":60584},{\"end\":60625,\"start\":60606},{\"end\":60636,\"start\":60625},{\"end\":60653,\"start\":60636},{\"end\":60957,\"start\":60947},{\"end\":60970,\"start\":60957},{\"end\":60984,\"start\":60970},{\"end\":61004,\"start\":60984},{\"end\":61015,\"start\":61004},{\"end\":61358,\"start\":61342},{\"end\":61371,\"start\":61358},{\"end\":61384,\"start\":61371},{\"end\":61401,\"start\":61384},{\"end\":61417,\"start\":61401},{\"end\":61435,\"start\":61417},{\"end\":61451,\"start\":61435},{\"end\":61465,\"start\":61451},{\"end\":61479,\"start\":61465},{\"end\":61497,\"start\":61479},{\"end\":61517,\"start\":61497},{\"end\":61534,\"start\":61517},{\"end\":61548,\"start\":61534},{\"end\":61556,\"start\":61548},{\"end\":61978,\"start\":61956},{\"end\":62004,\"start\":61978},{\"end\":62017,\"start\":62004},{\"end\":62030,\"start\":62017},{\"end\":62044,\"start\":62030},{\"end\":62061,\"start\":62044},{\"end\":62080,\"start\":62061},{\"end\":62090,\"start\":62080},{\"end\":62107,\"start\":62090},{\"end\":62115,\"start\":62107},{\"end\":62447,\"start\":62435},{\"end\":62464,\"start\":62447},{\"end\":62478,\"start\":62464},{\"end\":62495,\"start\":62478},{\"end\":62509,\"start\":62495},{\"end\":62524,\"start\":62509},{\"end\":62538,\"start\":62524},{\"end\":62554,\"start\":62538},{\"end\":62904,\"start\":62893},{\"end\":62918,\"start\":62904},{\"end\":62937,\"start\":62918},{\"end\":62955,\"start\":62937},{\"end\":63226,\"start\":63216},{\"end\":63240,\"start\":63226},{\"end\":63255,\"start\":63240},{\"end\":63271,\"start\":63255},{\"end\":63286,\"start\":63271},{\"end\":63591,\"start\":63580},{\"end\":63611,\"start\":63591},{\"end\":63628,\"start\":63611},{\"end\":63978,\"start\":63958},{\"end\":63993,\"start\":63978},{\"end\":64009,\"start\":63993},{\"end\":64026,\"start\":64009},{\"end\":64228,\"start\":64218},{\"end\":64243,\"start\":64228},{\"end\":64263,\"start\":64243},{\"end\":64283,\"start\":64263},{\"end\":64290,\"start\":64283},{\"end\":64440,\"start\":64424},{\"end\":64451,\"start\":64440},{\"end\":64463,\"start\":64451},{\"end\":64481,\"start\":64463},{\"end\":64498,\"start\":64481},{\"end\":64508,\"start\":64498},{\"end\":64516,\"start\":64508},{\"end\":64926,\"start\":64914},{\"end\":64941,\"start\":64926},{\"end\":64958,\"start\":64941},{\"end\":64969,\"start\":64958},{\"end\":64981,\"start\":64969},{\"end\":64994,\"start\":64981},{\"end\":65008,\"start\":64994},{\"end\":65030,\"start\":65008},{\"end\":65044,\"start\":65030},{\"end\":65062,\"start\":65044},{\"end\":65457,\"start\":65445},{\"end\":65466,\"start\":65457},{\"end\":65474,\"start\":65466},{\"end\":65488,\"start\":65474},{\"end\":65502,\"start\":65488},{\"end\":65513,\"start\":65502},{\"end\":65807,\"start\":65791},{\"end\":65819,\"start\":65807},{\"end\":65838,\"start\":65819},{\"end\":65856,\"start\":65838},{\"end\":65876,\"start\":65856},{\"end\":66184,\"start\":66170},{\"end\":66199,\"start\":66184},{\"end\":66211,\"start\":66199},{\"end\":66230,\"start\":66211},{\"end\":66248,\"start\":66230},{\"end\":66265,\"start\":66248},{\"end\":66587,\"start\":66575},{\"end\":66598,\"start\":66587},{\"end\":66611,\"start\":66598},{\"end\":66623,\"start\":66611},{\"end\":66633,\"start\":66623},{\"end\":66649,\"start\":66633},{\"end\":66661,\"start\":66649},{\"end\":66675,\"start\":66661},{\"end\":66683,\"start\":66675},{\"end\":67091,\"start\":67080},{\"end\":67103,\"start\":67091},{\"end\":67415,\"start\":67399},{\"end\":67432,\"start\":67415},{\"end\":67447,\"start\":67432},{\"end\":67719,\"start\":67701},{\"end\":67738,\"start\":67719},{\"end\":67756,\"start\":67738},{\"end\":67776,\"start\":67756},{\"end\":67793,\"start\":67776},{\"end\":68111,\"start\":68096},{\"end\":68129,\"start\":68111},{\"end\":68150,\"start\":68129},{\"end\":68166,\"start\":68150},{\"end\":68180,\"start\":68166},{\"end\":68200,\"start\":68180},{\"end\":68220,\"start\":68200},{\"end\":68526,\"start\":68511},{\"end\":68541,\"start\":68526},{\"end\":68555,\"start\":68541},{\"end\":68567,\"start\":68555},{\"end\":68581,\"start\":68567},{\"end\":68599,\"start\":68581},{\"end\":68614,\"start\":68599},{\"end\":68634,\"start\":68614},{\"end\":68652,\"start\":68634},{\"end\":68672,\"start\":68652},{\"end\":68996,\"start\":68984},{\"end\":69010,\"start\":68996},{\"end\":69014,\"start\":69010},{\"end\":69240,\"start\":69222},{\"end\":69258,\"start\":69240},{\"end\":69275,\"start\":69258},{\"end\":69744,\"start\":69728},{\"end\":69759,\"start\":69744},{\"end\":69771,\"start\":69759},{\"end\":69784,\"start\":69771},{\"end\":70078,\"start\":70066},{\"end\":70098,\"start\":70078},{\"end\":70105,\"start\":70098},{\"end\":70338,\"start\":70326},{\"end\":70352,\"start\":70338},{\"end\":70540,\"start\":70528},{\"end\":70553,\"start\":70540},{\"end\":70568,\"start\":70553},{\"end\":70579,\"start\":70568},{\"end\":70595,\"start\":70579},{\"end\":70609,\"start\":70595},{\"end\":70629,\"start\":70609},{\"end\":70903,\"start\":70892},{\"end\":70919,\"start\":70903},{\"end\":70939,\"start\":70919},{\"end\":70952,\"start\":70939},{\"end\":70971,\"start\":70952},{\"end\":70984,\"start\":70971},{\"end\":71000,\"start\":70984},{\"end\":71018,\"start\":71000},{\"end\":71035,\"start\":71018},{\"end\":71048,\"start\":71035},{\"end\":71279,\"start\":71267},{\"end\":71294,\"start\":71279},{\"end\":71308,\"start\":71294},{\"end\":71321,\"start\":71308},{\"end\":71589,\"start\":71576},{\"end\":71600,\"start\":71589},{\"end\":71623,\"start\":71600},{\"end\":71639,\"start\":71623},{\"end\":71649,\"start\":71639},{\"end\":71952,\"start\":71941},{\"end\":71966,\"start\":71952},{\"end\":71976,\"start\":71966},{\"end\":72329,\"start\":72311},{\"end\":72342,\"start\":72329},{\"end\":72357,\"start\":72342},{\"end\":72375,\"start\":72357},{\"end\":72393,\"start\":72375},{\"end\":72407,\"start\":72393},{\"end\":72700,\"start\":72689},{\"end\":72712,\"start\":72700},{\"end\":72734,\"start\":72712},{\"end\":72748,\"start\":72734},{\"end\":72763,\"start\":72748},{\"end\":72776,\"start\":72763},{\"end\":73111,\"start\":73087},{\"end\":73126,\"start\":73111},{\"end\":73141,\"start\":73126},{\"end\":73160,\"start\":73141},{\"end\":73173,\"start\":73160},{\"end\":73185,\"start\":73173},{\"end\":73198,\"start\":73185},{\"end\":73213,\"start\":73198},{\"end\":73227,\"start\":73213},{\"end\":73241,\"start\":73227},{\"end\":73255,\"start\":73241},{\"end\":73275,\"start\":73255},{\"end\":73850,\"start\":73831},{\"end\":73862,\"start\":73850},{\"end\":73877,\"start\":73862},{\"end\":73893,\"start\":73877},{\"end\":74260,\"start\":74243},{\"end\":74275,\"start\":74260},{\"end\":74289,\"start\":74275},{\"end\":74303,\"start\":74289},{\"end\":74317,\"start\":74303},{\"end\":74331,\"start\":74317},{\"end\":74355,\"start\":74331},{\"end\":74700,\"start\":74684},{\"end\":74710,\"start\":74700},{\"end\":74730,\"start\":74710},{\"end\":74749,\"start\":74730},{\"end\":74762,\"start\":74749},{\"end\":74779,\"start\":74762},{\"end\":74783,\"start\":74779},{\"end\":75071,\"start\":75055},{\"end\":75081,\"start\":75071},{\"end\":75101,\"start\":75081},{\"end\":75120,\"start\":75101},{\"end\":75133,\"start\":75120},{\"end\":75150,\"start\":75133},{\"end\":75154,\"start\":75150},{\"end\":75487,\"start\":75472},{\"end\":75502,\"start\":75487},{\"end\":75519,\"start\":75502},{\"end\":75805,\"start\":75792},{\"end\":75819,\"start\":75805},{\"end\":75835,\"start\":75819},{\"end\":75850,\"start\":75835},{\"end\":75858,\"start\":75850},{\"end\":75874,\"start\":75858},{\"end\":75888,\"start\":75874},{\"end\":76129,\"start\":76111},{\"end\":76381,\"start\":76361},{\"end\":76393,\"start\":76381},{\"end\":76409,\"start\":76393},{\"end\":76728,\"start\":76708},{\"end\":76743,\"start\":76728},{\"end\":76759,\"start\":76743},{\"end\":76770,\"start\":76759},{\"end\":76788,\"start\":76770},{\"end\":76804,\"start\":76788},{\"end\":76819,\"start\":76804},{\"end\":77337,\"start\":77314},{\"end\":77348,\"start\":77337},{\"end\":77365,\"start\":77348},{\"end\":77385,\"start\":77365},{\"end\":77397,\"start\":77385},{\"end\":77402,\"start\":77397},{\"end\":77836,\"start\":77810},{\"end\":77848,\"start\":77836},{\"end\":77867,\"start\":77848},{\"end\":77873,\"start\":77867},{\"end\":78226,\"start\":78204},{\"end\":78245,\"start\":78226},{\"end\":78259,\"start\":78245},{\"end\":78272,\"start\":78259},{\"end\":78287,\"start\":78272},{\"end\":78303,\"start\":78287},{\"end\":78314,\"start\":78303},{\"end\":78333,\"start\":78314},{\"end\":78347,\"start\":78333},{\"end\":78361,\"start\":78347},{\"end\":78822,\"start\":78805},{\"end\":78835,\"start\":78822},{\"end\":78851,\"start\":78835},{\"end\":79184,\"start\":79167},{\"end\":79200,\"start\":79184},{\"end\":79215,\"start\":79200},{\"end\":79233,\"start\":79215},{\"end\":79251,\"start\":79233},{\"end\":79596,\"start\":79581},{\"end\":79611,\"start\":79596},{\"end\":79630,\"start\":79611},{\"end\":79646,\"start\":79630},{\"end\":79661,\"start\":79646},{\"end\":79677,\"start\":79661},{\"end\":79701,\"start\":79677},{\"end\":80050,\"start\":80037},{\"end\":80061,\"start\":80050},{\"end\":80078,\"start\":80061},{\"end\":80090,\"start\":80078},{\"end\":80106,\"start\":80090},{\"end\":80122,\"start\":80106},{\"end\":80138,\"start\":80122},{\"end\":80150,\"start\":80138},{\"end\":80170,\"start\":80150},{\"end\":80183,\"start\":80170},{\"end\":80200,\"start\":80183},{\"end\":80214,\"start\":80200},{\"end\":80227,\"start\":80214},{\"end\":80243,\"start\":80227},{\"end\":80759,\"start\":80752},{\"end\":80770,\"start\":80759},{\"end\":80994,\"start\":80983},{\"end\":81010,\"start\":80994},{\"end\":81022,\"start\":81010},{\"end\":81037,\"start\":81022},{\"end\":81049,\"start\":81037},{\"end\":81060,\"start\":81049},{\"end\":81074,\"start\":81060},{\"end\":81528,\"start\":81508},{\"end\":81545,\"start\":81528},{\"end\":81561,\"start\":81545},{\"end\":81841,\"start\":81822},{\"end\":81859,\"start\":81841},{\"end\":81876,\"start\":81859},{\"end\":81890,\"start\":81876},{\"end\":82442,\"start\":82425},{\"end\":82456,\"start\":82442},{\"end\":82474,\"start\":82456},{\"end\":82498,\"start\":82474},{\"end\":82509,\"start\":82498},{\"end\":82711,\"start\":82701},{\"end\":82731,\"start\":82711},{\"end\":82744,\"start\":82731},{\"end\":82759,\"start\":82744},{\"end\":82775,\"start\":82759},{\"end\":83039,\"start\":83023},{\"end\":83055,\"start\":83039},{\"end\":83289,\"start\":83269},{\"end\":83305,\"start\":83289},{\"end\":83317,\"start\":83305},{\"end\":83337,\"start\":83317},{\"end\":83357,\"start\":83337},{\"end\":83654,\"start\":83642},{\"end\":83679,\"start\":83654},{\"end\":83686,\"start\":83679},{\"end\":84072,\"start\":84045},{\"end\":84091,\"start\":84072},{\"end\":84367,\"start\":84340},{\"end\":84382,\"start\":84367},{\"end\":84398,\"start\":84382},{\"end\":84417,\"start\":84398},{\"end\":84703,\"start\":84687},{\"end\":84719,\"start\":84703},{\"end\":84730,\"start\":84719},{\"end\":84748,\"start\":84730},{\"end\":85234,\"start\":85212},{\"end\":85249,\"start\":85234},{\"end\":85265,\"start\":85249},{\"end\":85281,\"start\":85265},{\"end\":85294,\"start\":85281},{\"end\":85309,\"start\":85294},{\"end\":85329,\"start\":85309},{\"end\":85342,\"start\":85329},{\"end\":85358,\"start\":85342},{\"end\":85374,\"start\":85358},{\"end\":85394,\"start\":85374},{\"end\":85412,\"start\":85394},{\"end\":85843,\"start\":85826},{\"end\":85859,\"start\":85843},{\"end\":85879,\"start\":85859},{\"end\":85894,\"start\":85879},{\"end\":85914,\"start\":85894},{\"end\":86266,\"start\":86248},{\"end\":86280,\"start\":86266},{\"end\":86293,\"start\":86280},{\"end\":86312,\"start\":86293},{\"end\":86330,\"start\":86312},{\"end\":86349,\"start\":86330},{\"end\":86726,\"start\":86708},{\"end\":86745,\"start\":86726},{\"end\":86763,\"start\":86745},{\"end\":87117,\"start\":87096},{\"end\":87135,\"start\":87117},{\"end\":87151,\"start\":87135},{\"end\":87166,\"start\":87151},{\"end\":87550,\"start\":87533},{\"end\":87567,\"start\":87550},{\"end\":87583,\"start\":87567},{\"end\":87599,\"start\":87583},{\"end\":87612,\"start\":87599},{\"end\":88010,\"start\":87996},{\"end\":88022,\"start\":88010},{\"end\":88272,\"start\":88255},{\"end\":88287,\"start\":88272},{\"end\":88302,\"start\":88287},{\"end\":88590,\"start\":88576},{\"end\":88600,\"start\":88590},{\"end\":88620,\"start\":88600},{\"end\":88634,\"start\":88620},{\"end\":88855,\"start\":88842},{\"end\":88881,\"start\":88855},{\"end\":89158,\"start\":89150},{\"end\":89172,\"start\":89158},{\"end\":89184,\"start\":89172},{\"end\":89198,\"start\":89184},{\"end\":89212,\"start\":89198},{\"end\":89220,\"start\":89212},{\"end\":89238,\"start\":89220},{\"end\":89249,\"start\":89238},{\"end\":89572,\"start\":89558},{\"end\":89584,\"start\":89572},{\"end\":89598,\"start\":89584},{\"end\":89616,\"start\":89598},{\"end\":89634,\"start\":89616},{\"end\":89653,\"start\":89634},{\"end\":89677,\"start\":89653},{\"end\":89690,\"start\":89677},{\"end\":89705,\"start\":89690},{\"end\":89723,\"start\":89705},{\"end\":89737,\"start\":89723},{\"end\":89751,\"start\":89737},{\"end\":89769,\"start\":89751},{\"end\":89782,\"start\":89769},{\"end\":89802,\"start\":89782},{\"end\":89820,\"start\":89802},{\"end\":89835,\"start\":89820},{\"end\":89850,\"start\":89835},{\"end\":89869,\"start\":89850},{\"end\":90438,\"start\":90418},{\"end\":90452,\"start\":90438},{\"end\":90468,\"start\":90452},{\"end\":90483,\"start\":90468},{\"end\":90502,\"start\":90483},{\"end\":90910,\"start\":90896},{\"end\":90929,\"start\":90910},{\"end\":90947,\"start\":90929},{\"end\":91226,\"start\":91210},{\"end\":91240,\"start\":91226},{\"end\":91259,\"start\":91240},{\"end\":91279,\"start\":91259},{\"end\":91299,\"start\":91279},{\"end\":91691,\"start\":91680},{\"end\":91708,\"start\":91691},{\"end\":91727,\"start\":91708},{\"end\":91929,\"start\":91917},{\"end\":91948,\"start\":91929},{\"end\":91966,\"start\":91948},{\"end\":91979,\"start\":91966},{\"end\":91994,\"start\":91979},{\"end\":92011,\"start\":91994},{\"end\":92030,\"start\":92011},{\"end\":92371,\"start\":92351},{\"end\":92388,\"start\":92371},{\"end\":92404,\"start\":92388},{\"end\":92417,\"start\":92404},{\"end\":92431,\"start\":92417},{\"end\":92714,\"start\":92699},{\"end\":92729,\"start\":92714},{\"end\":92757,\"start\":92729},{\"end\":92949,\"start\":92937},{\"end\":92964,\"start\":92949},{\"end\":92979,\"start\":92964},{\"end\":92992,\"start\":92979},{\"end\":93294,\"start\":93279},{\"end\":93307,\"start\":93294},{\"end\":93321,\"start\":93307},{\"end\":93756,\"start\":93744},{\"end\":93776,\"start\":93756},{\"end\":93795,\"start\":93776},{\"end\":93809,\"start\":93795},{\"end\":93825,\"start\":93809},{\"end\":93844,\"start\":93825},{\"end\":93864,\"start\":93844},{\"end\":94140,\"start\":94126},{\"end\":94330,\"start\":94315},{\"end\":94342,\"start\":94330},{\"end\":94356,\"start\":94342},{\"end\":94371,\"start\":94356},{\"end\":94382,\"start\":94371},{\"end\":94652,\"start\":94644},{\"end\":94667,\"start\":94652},{\"end\":94678,\"start\":94667},{\"end\":94696,\"start\":94678},{\"end\":94705,\"start\":94696},{\"end\":95081,\"start\":95073},{\"end\":95093,\"start\":95081},{\"end\":95102,\"start\":95093},{\"end\":95113,\"start\":95102},{\"end\":95124,\"start\":95113},{\"end\":95138,\"start\":95124},{\"end\":95150,\"start\":95138},{\"end\":95164,\"start\":95150},{\"end\":95175,\"start\":95164},{\"end\":95588,\"start\":95580},{\"end\":95602,\"start\":95588},{\"end\":95614,\"start\":95602},{\"end\":95628,\"start\":95614},{\"end\":95989,\"start\":95978},{\"end\":96005,\"start\":95989},{\"end\":96019,\"start\":96005},{\"end\":96035,\"start\":96019},{\"end\":96341,\"start\":96331},{\"end\":96356,\"start\":96341},{\"end\":96372,\"start\":96356},{\"end\":96664,\"start\":96649},{\"end\":96679,\"start\":96664},{\"end\":96695,\"start\":96679},{\"end\":96710,\"start\":96695},{\"end\":96723,\"start\":96710},{\"end\":97066,\"start\":97055},{\"end\":97080,\"start\":97066},{\"end\":97096,\"start\":97080},{\"end\":97115,\"start\":97096},{\"end\":97424,\"start\":97415},{\"end\":97432,\"start\":97424},{\"end\":97443,\"start\":97432},{\"end\":97452,\"start\":97443},{\"end\":97466,\"start\":97452},{\"end\":97760,\"start\":97741},{\"end\":97778,\"start\":97760},{\"end\":97793,\"start\":97778},{\"end\":97812,\"start\":97793},{\"end\":97830,\"start\":97812},{\"end\":97846,\"start\":97830},{\"end\":97859,\"start\":97846},{\"end\":97878,\"start\":97859},{\"end\":97892,\"start\":97878},{\"end\":97912,\"start\":97892},{\"end\":97929,\"start\":97912},{\"end\":98331,\"start\":98312},{\"end\":98347,\"start\":98331},{\"end\":98364,\"start\":98347},{\"end\":98384,\"start\":98364},{\"end\":98402,\"start\":98384},{\"end\":98418,\"start\":98402},{\"end\":98432,\"start\":98418}]", "bib_venue": "[{\"end\":69409,\"start\":69349},{\"end\":76918,\"start\":76883},{\"end\":82086,\"start\":81995},{\"end\":82846,\"start\":82814},{\"end\":83827,\"start\":83765},{\"end\":84781,\"start\":84773},{\"end\":84994,\"start\":84977},{\"end\":93462,\"start\":93400},{\"end\":56949,\"start\":56938},{\"end\":57296,\"start\":57250},{\"end\":57529,\"start\":57484},{\"end\":58538,\"start\":58498},{\"end\":59833,\"start\":59783},{\"end\":60188,\"start\":60154},{\"end\":60669,\"start\":60653},{\"end\":61094,\"start\":61031},{\"end\":61584,\"start\":61556},{\"end\":62122,\"start\":62115},{\"end\":62628,\"start\":62554},{\"end\":63047,\"start\":62955},{\"end\":63337,\"start\":63286},{\"end\":63674,\"start\":63628},{\"end\":63956,\"start\":63879},{\"end\":64298,\"start\":64290},{\"end\":64603,\"start\":64532},{\"end\":65078,\"start\":65062},{\"end\":65529,\"start\":65513},{\"end\":65902,\"start\":65886},{\"end\":66293,\"start\":66277},{\"end\":66728,\"start\":66683},{\"end\":67158,\"start\":67103},{\"end\":67481,\"start\":67447},{\"end\":67838,\"start\":67793},{\"end\":68254,\"start\":68220},{\"end\":68700,\"start\":68672},{\"end\":68982,\"start\":68940},{\"end\":69347,\"start\":69275},{\"end\":69632,\"start\":69628},{\"end\":69830,\"start\":69784},{\"end\":70152,\"start\":70105},{\"end\":70360,\"start\":70352},{\"end\":70649,\"start\":70629},{\"end\":71400,\"start\":71321},{\"end\":71698,\"start\":71649},{\"end\":72039,\"start\":71976},{\"end\":72435,\"start\":72407},{\"end\":72822,\"start\":72776},{\"end\":73085,\"start\":73010},{\"end\":73955,\"start\":73893},{\"end\":74401,\"start\":74355},{\"end\":74787,\"start\":74783},{\"end\":75199,\"start\":75154},{\"end\":75565,\"start\":75519},{\"end\":75904,\"start\":75888},{\"end\":76192,\"start\":76129},{\"end\":76455,\"start\":76409},{\"end\":76881,\"start\":76819},{\"end\":77452,\"start\":77402},{\"end\":77929,\"start\":77873},{\"end\":78419,\"start\":78361},{\"end\":78896,\"start\":78851},{\"end\":79325,\"start\":79251},{\"end\":79750,\"start\":79717},{\"end\":80035,\"start\":79967},{\"end\":80819,\"start\":80770},{\"end\":81190,\"start\":81074},{\"end\":81590,\"start\":81561},{\"end\":81993,\"start\":81890},{\"end\":82557,\"start\":82509},{\"end\":82812,\"start\":82775},{\"end\":83100,\"start\":83055},{\"end\":83402,\"start\":83357},{\"end\":83763,\"start\":83686},{\"end\":84137,\"start\":84091},{\"end\":84462,\"start\":84417},{\"end\":84771,\"start\":84748},{\"end\":84975,\"start\":84896},{\"end\":85458,\"start\":85412},{\"end\":85959,\"start\":85914},{\"end\":86395,\"start\":86349},{\"end\":86812,\"start\":86763},{\"end\":87212,\"start\":87166},{\"end\":87664,\"start\":87612},{\"end\":88054,\"start\":88022},{\"end\":88342,\"start\":88302},{\"end\":88574,\"start\":88508},{\"end\":88894,\"start\":88881},{\"end\":89295,\"start\":89249},{\"end\":89930,\"start\":89869},{\"end\":90565,\"start\":90502},{\"end\":90975,\"start\":90947},{\"end\":91344,\"start\":91299},{\"end\":91678,\"start\":91565},{\"end\":92107,\"start\":92030},{\"end\":92450,\"start\":92431},{\"end\":92697,\"start\":92621},{\"end\":93052,\"start\":92992},{\"end\":93398,\"start\":93321},{\"end\":93890,\"start\":93874},{\"end\":94313,\"start\":94221},{\"end\":94755,\"start\":94705},{\"end\":95225,\"start\":95175},{\"end\":95687,\"start\":95628},{\"end\":96105,\"start\":96051},{\"end\":96418,\"start\":96372},{\"end\":96769,\"start\":96723},{\"end\":97152,\"start\":97115},{\"end\":97512,\"start\":97466},{\"end\":97957,\"start\":97929},{\"end\":98455,\"start\":98432}]"}}}, "year": 2023, "month": 12, "day": 17}