{"id": 212633993, "updated": "2023-10-06 18:27:49.945", "metadata": {"title": "Improved Baselines with Momentum Contrastive Learning", "authors": "[{\"first\":\"Xinlei\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Haoqi\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Ross\",\"last\":\"Girshick\",\"middle\":[]},{\"first\":\"Kaiming\",\"last\":\"He\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 3, "day": 9}, "abstract": "Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2003.04297", "mag": "3009561768", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2003-04297", "doi": null}}, "content": {"source": {"pdf_hash": "a1b8a8df281bbaec148a897927a49ea47ea31515", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.04297v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "15eed5024a232646d9dfedfc1ada59a8df681f71", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a1b8a8df281bbaec148a897927a49ea47ea31515.txt", "contents": "\nImproved Baselines with Momentum Contrastive Learning\n\n\nXinlei Chen \nFacebook AI Research (FAIR)\n\n\nHaoqi Fan \nFacebook AI Research (FAIR)\n\n\nRoss Girshick \nFacebook AI Research (FAIR)\n\n\nKaiming He \nFacebook AI Research (FAIR)\n\n\nImproved Baselines with Momentum Contrastive Learning\n\nContrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoConamely, using an MLP projection head and more data augmentation-we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.\n\nIntroduction\n\nRecent studies on unsupervised representation learning from images [16,13,8,17,1,9,15,6,12,2] are converging on a central concept known as contrastive learning [5]. The results are promising: e.g., Momentum Contrast (MoCo) [6] shows that unsupervised pre-training can surpass its ImageNet-supervised counterpart in multiple detection and segmentation tasks, and SimCLR [2] further reduces the gap in linear classifier performance between unsupervised and supervised pre-training representations.\n\nThis note establishes stronger and more feasible baselines built in the MoCo framework. We report that two design improvements used in SimCLR, namely, an MLP projection head and stronger data augmentation, are orthogonal to the frameworks of MoCo and SimCLR, and when used with MoCo they lead to better image classification and object detection transfer learning results. Moreover, the MoCo framework can process a large set of negative samples without requiring large training batches (Fig. 1). In contrast to SimCLR's large 4k\u223c8k batches, which require TPU support, our \"MoCo v2\" baselines can run on a typical 8-GPU machine and achieve better results than SimCLR. We hope these improved baselines will provide a reference for future research in unsupervised learning.\n\n\nBackground\n\nContrastive learning. Contrastive learning [5] is a framework that learns similar/dissimilar representations from data that are organized into similar/dissimilar pairs. This can be formulated as a dictionary look-up problem. An ef-  fective contrastive loss function, called InfoNCE [13], is:\nL q,k + ,{k \u2212 } = \u2212 log exp(q\u00b7k + /\u03c4 ) exp(q\u00b7k + /\u03c4 ) + k \u2212 exp(q\u00b7k \u2212 /\u03c4 ) .(1)\nHere q is a query representation, k + is a representation of the positive (similar) key sample, and {k \u2212 } are representations of the negative (dissimilar) key samples. \u03c4 is a temperature hyper-parameter. In the instance discrimination pretext task [16] (used by MoCo and SimCLR), a query and a key form a positive pair if they are data-augmented versions of the same image, and otherwise form a negative pair. The contrastive loss (1) can be minimized by various mechanisms that differ in how the keys are maintained [6]. In an end-to-end mechanism (Fig. 1a) [13,8,17,1,9,2], the negative keys are from the same batch and updated endto-end by back-propagation. SimCLR [2] is based on this mechanism and requires a large batch to provide a large set of negatives. In the MoCo mechanism (Fig. 1b) [6], the negative keys are maintained in a queue, and only the queries and positive keys are encoded in each training batch. A momentum encoder is adopted to improve the representation consistency between the current and earlier keys. MoCo decouples the batch size from the number of negatives. Improved designs. SimCLR [2] improves the end-to-end variant of instance discrimination in three aspects: (i) a substantially larger batch (4k or 8k) that can provide more negative samples; (ii) replacing the output fc projection head [16] with an MLP head; (iii) stronger data augmentation.\n\nIn the MoCo framework, a large number of negative samples are readily available; the MLP head and data augmentation are orthogonal to how contrastive learning is instantiated. Next we study these improvements in MoCo.\n\n\nExperiments\n\nSettings. Unsupervised learning is conducted on the 1.28M ImageNet [3] training set. We follow two common protocols for evaluation. (i) ImageNet linear classification: features are frozen and a supervised linear classifier is trained; we report 1-crop (224\u00d7224), top-1 validation accuracy. (ii) Transferring to VOC object detection [4]: a Faster R-CNN detector [14] (C4-backbone) is fine-tuned end-to-end on the VOC 07+12 trainval set 1 and evaluated on the VOC 07 test set using the COCO suite of metrics [10]. We use the same hyper-parameters (except when noted) and codebase as MoCo [6]. All results use a standard-size ResNet-50 [7].\n\nMLP head. Following [2], we replace the fc head in MoCo with a 2-layer MLP head (hidden layer 2048-d, with ReLU). Note this only influences the unsupervised training stage; the linear classification or transferring stage does not use this MLP head. Also, following [2], we search for an optimal \u03c4 w.r.t. ImageNet linear classification accuracy:  Computational cost. In Table 3 we report the memory and time cost of our implementation. The end-to-end case reflects the SimCLR cost in GPUs (instead of TPUs in [2]). The 4k batch size is intractable even in a high-end 8-GPU machine. Also, under the same batch size of 256, the endto-end variant is still more costly in memory and time, because it back-propagates to both q and k encoders, while MoCo back-propagates to the q encoder only. Table 2 and 3 suggest that large batches are not necessary for good accuracy, and state-of-the-art results can be made more accessible. The improvements we investigate require only a few lines of code changes to MoCo v1, and we will make the code public to facilitate future research.\n\nFigure 1 .\n1A batching perspective of two optimization mechanisms for contrastive learning. Images are encoded into a representation space, in which pairwise affinities are computed.\n\n1\narXiv:2003.04297v1 [cs.CV] 9 Mar 2020unsup. pre-train \n\nImageNet \nVOC detection \n\ncase \nMLP aug+ cos epochs \nacc. \nAP 50 AP AP 75 \nsupervised \n76.5 \n81.3 53.5 58.8 \nMoCo v1 \n200 \n60.6 \n81.5 55.9 62.6 \n(a) \n200 \n66.2 \n82.0 56.4 62.6 \n(b) \n200 \n63.4 \n82.2 56.8 63.2 \n(c) \n200 \n67.3 \n82.5 57.2 63.9 \n(d) \n200 \n67.5 \n82.4 57.0 63.6 \n(e) \n800 \n71.1 \n82.5 57.4 64.0 \n\nTable 1. Ablation of MoCo baselines, evaluated by ResNet-50 for \n(i) ImageNet linear classification, and (ii) fine-tuning VOC object \ndetection (mean of 5 trials). \"MLP\": with an MLP head; \"aug+\": \nwith extra blur augmentation; \"cos\": cosine learning rate schedule. \n\n\n\nTable 3 .\n32%). This indicates that linear classification accuracy is not monotonically related to transfer performance in detection. With the MLP, the extra augmentation boosts ImageNet accuracy to 67.3%,Table 1(c). with SimCLR.Table 2compares SimCLR[2] with our results, referred to as MoCo v2. For fair comparisons, we also study a cosine (half-period) learning rate schedule[11] which SimCLR adopts. SeeTable 1(d, e). Using pre-training with 200 epochs and a batch size of 256, MoCo v2 achieves 67.5% accuracy on ImageNet: this is 5.6% higher than SimCLR under the same epochs and batch size, and better than SimCLR's large-batch result 66.6%. With 800-epoch pre-training, MoCo v2 achieves 71.1%, outperforming SimCLR's 69.3% with 1000 epochs.Memory and time cost in 8 V100 16G GPUs, imple-\nmented in PyTorch.  \u2020 : based on our estimation. \n\nstronger color distortion in [2] has diminishing gains in our \nhigher baselines). The extra augmentation alone (i.e., no \nMLP) improves the MoCo baseline on ImageNet by 2.8% \nto 63.4%, Table 1(b). Interestingly, its detection accuracy is \nhigher than that of using the MLP alone, Table 1(b) vs. (a), \ndespite much lower linear classification accuracy (63.4% \nvs. 66.Comparison \nFor all entries (including the supervised and MoCo v1 baselines), we fine-tune for 24k iterations on VOC, up from 18k in[6].\n\nLearning representations by maximizing mutual information across views. Philip Bachman, Devon Hjelm, William Buchwalter, arXiv:1906.00910Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. arXiv:1906.00910, 2019.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, arXiv:2002.05709A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. arXiv:2002.05709, 2020.\n\nImageNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.\n\nThe PASCAL Visual Object Classes (VOC) Challenge. IJCV. Mark Everingham, Luc Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. IJCV, 2010.\n\nDimensionality reduction by learning an invariant mapping. Raia Hadsell, Sumit Chopra, Yann Lecun, CVPR. Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension- ality reduction by learning an invariant mapping. In CVPR, 2006.\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, arXiv:1911.05722Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. arXiv:1911.05722, 2019.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nLearning deep representations by mutual information estimation and maximization. Alex R Devon Hjelm, Samuel Fedorov, Karan Lavoie-Marchildon, Adam Grewal, Yoshua Trischler, Bengio, ICLR. R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and Yoshua Bengio. Learn- ing deep representations by mutual information estimation and maximization. In ICLR, 2019.\n\nJ Olivier, Aravind Hnaff, Jeffrey De Srinivas, Ali Fauw, Carl Razavi, S M Doersch, Aaron Ali Eslami, Van Den Oord, arXiv:1905.09272v2Data-efficient image recognition with contrastive predictive coding. Olivier J. Hnaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, and Aaron van den Oord. Data-efficient image recognition with contrastive pre- dictive coding. arXiv:1905.09272v2, 2019.\n\nMicrosoft COCO: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV. 2014.\n\nSGDR: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradi- ent descent with warm restarts. In ICLR, 2017.\n\nSelfsupervised learning of pretext-invariant representations. Ishan Misra, Laurens Van Der Maaten, arXiv:1912.01991Ishan Misra and Laurens van der Maaten. Self- supervised learning of pretext-invariant representations. arXiv:1912.01991, 2019.\n\nAaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Rep- resentation learning with contrastive predictive coding. arXiv:1807.03748, 2018.\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, NeurIPS. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with re- gion proposal networks. In NeurIPS, 2015.\n\nYonglong Tian, Dilip Krishnan, Phillip Isola, arXiv:1906.05849Contrastive multiview coding. Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con- trastive multiview coding. arXiv:1906.05849, 2019.\n\nUnsupervised feature learning via non-parametric instance discrimination. Zhirong Wu, Yuanjun Xiong, Stella Yu, Dahua Lin, CVPR. Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Un- supervised feature learning via non-parametric instance dis- crimination. In CVPR, 2018.\n\nUnsupervised embedding learning via invariant and spreading instance feature. Mang Ye, Xu Zhang, C Pong, Shih-Fu Yuen, Chang, CVPR. Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Un- supervised embedding learning via invariant and spreading instance feature. In CVPR, 2019.\n", "annotations": {"author": "[{\"end\":99,\"start\":57},{\"end\":140,\"start\":100},{\"end\":185,\"start\":141},{\"end\":227,\"start\":186}]", "publisher": null, "author_last_name": "[{\"end\":68,\"start\":64},{\"end\":109,\"start\":106},{\"end\":154,\"start\":146},{\"end\":196,\"start\":194}]", "author_first_name": "[{\"end\":63,\"start\":57},{\"end\":105,\"start\":100},{\"end\":145,\"start\":141},{\"end\":193,\"start\":186}]", "author_affiliation": "[{\"end\":98,\"start\":70},{\"end\":139,\"start\":111},{\"end\":184,\"start\":156},{\"end\":226,\"start\":198}]", "title": "[{\"end\":54,\"start\":1},{\"end\":281,\"start\":228}]", "venue": null, "abstract": "[{\"end\":837,\"start\":283}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":924,\"start\":920},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":927,\"start\":924},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":929,\"start\":927},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":932,\"start\":929},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":934,\"start\":932},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":936,\"start\":934},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":939,\"start\":936},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":941,\"start\":939},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":944,\"start\":941},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":946,\"start\":944},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1016,\"start\":1013},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1079,\"start\":1076},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1225,\"start\":1222},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2181,\"start\":2178},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2422,\"start\":2418},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2761,\"start\":2757},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3029,\"start\":3026},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3072,\"start\":3068},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3074,\"start\":3072},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3077,\"start\":3074},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3079,\"start\":3077},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3081,\"start\":3079},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3083,\"start\":3081},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3180,\"start\":3177},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3307,\"start\":3304},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3627,\"start\":3624},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3838,\"start\":3834},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4195,\"start\":4192},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4460,\"start\":4457},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4490,\"start\":4486},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4635,\"start\":4631},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4714,\"start\":4711},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4761,\"start\":4758},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4787,\"start\":4784},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5032,\"start\":5029},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5275,\"start\":5272},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6909,\"start\":6906},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7037,\"start\":7033},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8002,\"start\":7999}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":6019,\"start\":5836},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":6653,\"start\":6020},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":7878,\"start\":6654}]", "paragraph": "[{\"end\":1348,\"start\":853},{\"end\":2120,\"start\":1350},{\"end\":2427,\"start\":2135},{\"end\":3890,\"start\":2508},{\"end\":4109,\"start\":3892},{\"end\":4762,\"start\":4125},{\"end\":5835,\"start\":4764}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":2507,\"start\":2428}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":5140,\"start\":5133},{\"end\":5558,\"start\":5551}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":851,\"start\":839},{\"attributes\":{\"n\":\"2.\"},\"end\":2133,\"start\":2123},{\"attributes\":{\"n\":\"3.\"},\"end\":4123,\"start\":4112},{\"end\":5847,\"start\":5837},{\"end\":6022,\"start\":6021},{\"end\":6664,\"start\":6655}]", "table": "[{\"end\":6653,\"start\":6060},{\"end\":7878,\"start\":7402}]", "figure_caption": "[{\"end\":6019,\"start\":5849},{\"end\":6060,\"start\":6023},{\"end\":7402,\"start\":6666}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":1844,\"start\":1836},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3067,\"start\":3058},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3303,\"start\":3294}]", "bib_author_first_name": "[{\"end\":8083,\"start\":8077},{\"end\":8098,\"start\":8093},{\"end\":8113,\"start\":8106},{\"end\":8298,\"start\":8294},{\"end\":8310,\"start\":8305},{\"end\":8330,\"start\":8322},{\"end\":8348,\"start\":8340},{\"end\":8665,\"start\":8662},{\"end\":8675,\"start\":8672},{\"end\":8689,\"start\":8682},{\"end\":8704,\"start\":8698},{\"end\":8712,\"start\":8709},{\"end\":8719,\"start\":8717},{\"end\":8935,\"start\":8931},{\"end\":8951,\"start\":8948},{\"end\":8963,\"start\":8962},{\"end\":8965,\"start\":8964},{\"end\":8983,\"start\":8979},{\"end\":9000,\"start\":8994},{\"end\":9233,\"start\":9229},{\"end\":9248,\"start\":9243},{\"end\":9261,\"start\":9257},{\"end\":9470,\"start\":9463},{\"end\":9480,\"start\":9475},{\"end\":9491,\"start\":9486},{\"end\":9503,\"start\":9496},{\"end\":9513,\"start\":9509},{\"end\":9752,\"start\":9745},{\"end\":9764,\"start\":9757},{\"end\":9780,\"start\":9772},{\"end\":9790,\"start\":9786},{\"end\":10004,\"start\":10000},{\"end\":10026,\"start\":10020},{\"end\":10041,\"start\":10036},{\"end\":10065,\"start\":10061},{\"end\":10080,\"start\":10074},{\"end\":10310,\"start\":10309},{\"end\":10327,\"start\":10320},{\"end\":10342,\"start\":10335},{\"end\":10345,\"start\":10343},{\"end\":10359,\"start\":10356},{\"end\":10370,\"start\":10366},{\"end\":10380,\"start\":10379},{\"end\":10382,\"start\":10381},{\"end\":10397,\"start\":10392},{\"end\":10781,\"start\":10773},{\"end\":10794,\"start\":10787},{\"end\":10807,\"start\":10802},{\"end\":10823,\"start\":10818},{\"end\":10836,\"start\":10830},{\"end\":10849,\"start\":10845},{\"end\":10864,\"start\":10859},{\"end\":10883,\"start\":10873},{\"end\":11140,\"start\":11136},{\"end\":11158,\"start\":11153},{\"end\":11346,\"start\":11341},{\"end\":11361,\"start\":11354},{\"end\":11528,\"start\":11523},{\"end\":11548,\"start\":11543},{\"end\":11558,\"start\":11553},{\"end\":11867,\"start\":11860},{\"end\":11886,\"start\":11882},{\"end\":11895,\"start\":11891},{\"end\":12084,\"start\":12076},{\"end\":12096,\"start\":12091},{\"end\":12114,\"start\":12107},{\"end\":12356,\"start\":12349},{\"end\":12368,\"start\":12361},{\"end\":12382,\"start\":12376},{\"end\":12392,\"start\":12387},{\"end\":12633,\"start\":12629},{\"end\":12640,\"start\":12638},{\"end\":12649,\"start\":12648},{\"end\":12663,\"start\":12656}]", "bib_author_last_name": "[{\"end\":8091,\"start\":8084},{\"end\":8104,\"start\":8099},{\"end\":8124,\"start\":8114},{\"end\":8303,\"start\":8299},{\"end\":8320,\"start\":8311},{\"end\":8338,\"start\":8331},{\"end\":8355,\"start\":8349},{\"end\":8670,\"start\":8666},{\"end\":8680,\"start\":8676},{\"end\":8696,\"start\":8690},{\"end\":8707,\"start\":8705},{\"end\":8715,\"start\":8713},{\"end\":8727,\"start\":8720},{\"end\":8946,\"start\":8936},{\"end\":8960,\"start\":8952},{\"end\":8977,\"start\":8966},{\"end\":8992,\"start\":8984},{\"end\":9005,\"start\":9001},{\"end\":9016,\"start\":9007},{\"end\":9241,\"start\":9234},{\"end\":9255,\"start\":9249},{\"end\":9267,\"start\":9262},{\"end\":9473,\"start\":9471},{\"end\":9484,\"start\":9481},{\"end\":9494,\"start\":9492},{\"end\":9507,\"start\":9504},{\"end\":9522,\"start\":9514},{\"end\":9755,\"start\":9753},{\"end\":9770,\"start\":9765},{\"end\":9784,\"start\":9781},{\"end\":9794,\"start\":9791},{\"end\":10018,\"start\":10005},{\"end\":10034,\"start\":10027},{\"end\":10059,\"start\":10042},{\"end\":10072,\"start\":10066},{\"end\":10090,\"start\":10081},{\"end\":10098,\"start\":10092},{\"end\":10318,\"start\":10311},{\"end\":10333,\"start\":10328},{\"end\":10354,\"start\":10346},{\"end\":10364,\"start\":10360},{\"end\":10377,\"start\":10371},{\"end\":10390,\"start\":10383},{\"end\":10408,\"start\":10398},{\"end\":10422,\"start\":10410},{\"end\":10785,\"start\":10782},{\"end\":10800,\"start\":10795},{\"end\":10816,\"start\":10808},{\"end\":10828,\"start\":10824},{\"end\":10843,\"start\":10837},{\"end\":10857,\"start\":10850},{\"end\":10871,\"start\":10865},{\"end\":10891,\"start\":10884},{\"end\":11151,\"start\":11141},{\"end\":11165,\"start\":11159},{\"end\":11352,\"start\":11347},{\"end\":11376,\"start\":11362},{\"end\":11541,\"start\":11529},{\"end\":11551,\"start\":11549},{\"end\":11566,\"start\":11559},{\"end\":11880,\"start\":11868},{\"end\":11889,\"start\":11887},{\"end\":11904,\"start\":11896},{\"end\":11909,\"start\":11906},{\"end\":12089,\"start\":12085},{\"end\":12105,\"start\":12097},{\"end\":12120,\"start\":12115},{\"end\":12359,\"start\":12357},{\"end\":12374,\"start\":12369},{\"end\":12385,\"start\":12383},{\"end\":12396,\"start\":12393},{\"end\":12636,\"start\":12634},{\"end\":12646,\"start\":12641},{\"end\":12654,\"start\":12650},{\"end\":12668,\"start\":12664},{\"end\":12675,\"start\":12670}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1906.00910\",\"id\":\"b0\"},\"end\":8292,\"start\":8005},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b1\"},\"end\":8607,\"start\":8294},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":57246310},\"end\":8873,\"start\":8609},{\"attributes\":{\"id\":\"b3\"},\"end\":9168,\"start\":8875},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8281592},\"end\":9394,\"start\":9170},{\"attributes\":{\"doi\":\"arXiv:1911.05722\",\"id\":\"b5\"},\"end\":9697,\"start\":9396},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206594692},\"end\":9917,\"start\":9699},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52055130},\"end\":10307,\"start\":9919},{\"attributes\":{\"doi\":\"arXiv:1905.09272v2\",\"id\":\"b8\"},\"end\":10728,\"start\":10309},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14113767},\"end\":11080,\"start\":10730},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14337532},\"end\":11277,\"start\":11082},{\"attributes\":{\"doi\":\"arXiv:1912.01991\",\"id\":\"b11\"},\"end\":11521,\"start\":11279},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b12\"},\"end\":11778,\"start\":11523},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10328909},\"end\":12074,\"start\":11780},{\"attributes\":{\"doi\":\"arXiv:1906.05849\",\"id\":\"b14\"},\"end\":12273,\"start\":12076},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4591284},\"end\":12549,\"start\":12275},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":102350974},\"end\":12828,\"start\":12551}]", "bib_title": "[{\"end\":8660,\"start\":8609},{\"end\":9227,\"start\":9170},{\"end\":9743,\"start\":9699},{\"end\":9998,\"start\":9919},{\"end\":10771,\"start\":10730},{\"end\":11134,\"start\":11082},{\"end\":11858,\"start\":11780},{\"end\":12347,\"start\":12275},{\"end\":12627,\"start\":12551}]", "bib_author": "[{\"end\":8093,\"start\":8077},{\"end\":8106,\"start\":8093},{\"end\":8126,\"start\":8106},{\"end\":8305,\"start\":8294},{\"end\":8322,\"start\":8305},{\"end\":8340,\"start\":8322},{\"end\":8357,\"start\":8340},{\"end\":8672,\"start\":8662},{\"end\":8682,\"start\":8672},{\"end\":8698,\"start\":8682},{\"end\":8709,\"start\":8698},{\"end\":8717,\"start\":8709},{\"end\":8729,\"start\":8717},{\"end\":8948,\"start\":8931},{\"end\":8962,\"start\":8948},{\"end\":8979,\"start\":8962},{\"end\":8994,\"start\":8979},{\"end\":9007,\"start\":8994},{\"end\":9018,\"start\":9007},{\"end\":9243,\"start\":9229},{\"end\":9257,\"start\":9243},{\"end\":9269,\"start\":9257},{\"end\":9475,\"start\":9463},{\"end\":9486,\"start\":9475},{\"end\":9496,\"start\":9486},{\"end\":9509,\"start\":9496},{\"end\":9524,\"start\":9509},{\"end\":9757,\"start\":9745},{\"end\":9772,\"start\":9757},{\"end\":9786,\"start\":9772},{\"end\":9796,\"start\":9786},{\"end\":10020,\"start\":10000},{\"end\":10036,\"start\":10020},{\"end\":10061,\"start\":10036},{\"end\":10074,\"start\":10061},{\"end\":10092,\"start\":10074},{\"end\":10100,\"start\":10092},{\"end\":10320,\"start\":10309},{\"end\":10335,\"start\":10320},{\"end\":10356,\"start\":10335},{\"end\":10366,\"start\":10356},{\"end\":10379,\"start\":10366},{\"end\":10392,\"start\":10379},{\"end\":10410,\"start\":10392},{\"end\":10424,\"start\":10410},{\"end\":10787,\"start\":10773},{\"end\":10802,\"start\":10787},{\"end\":10818,\"start\":10802},{\"end\":10830,\"start\":10818},{\"end\":10845,\"start\":10830},{\"end\":10859,\"start\":10845},{\"end\":10873,\"start\":10859},{\"end\":10893,\"start\":10873},{\"end\":11153,\"start\":11136},{\"end\":11167,\"start\":11153},{\"end\":11354,\"start\":11341},{\"end\":11378,\"start\":11354},{\"end\":11543,\"start\":11523},{\"end\":11553,\"start\":11543},{\"end\":11568,\"start\":11553},{\"end\":11882,\"start\":11860},{\"end\":11891,\"start\":11882},{\"end\":11906,\"start\":11891},{\"end\":11911,\"start\":11906},{\"end\":12091,\"start\":12076},{\"end\":12107,\"start\":12091},{\"end\":12122,\"start\":12107},{\"end\":12361,\"start\":12349},{\"end\":12376,\"start\":12361},{\"end\":12387,\"start\":12376},{\"end\":12398,\"start\":12387},{\"end\":12638,\"start\":12629},{\"end\":12648,\"start\":12638},{\"end\":12656,\"start\":12648},{\"end\":12670,\"start\":12656},{\"end\":12677,\"start\":12670}]", "bib_venue": "[{\"end\":8075,\"start\":8005},{\"end\":8442,\"start\":8373},{\"end\":8733,\"start\":8729},{\"end\":8929,\"start\":8875},{\"end\":9273,\"start\":9269},{\"end\":9461,\"start\":9396},{\"end\":9800,\"start\":9796},{\"end\":10104,\"start\":10100},{\"end\":10509,\"start\":10442},{\"end\":10897,\"start\":10893},{\"end\":11171,\"start\":11167},{\"end\":11339,\"start\":11279},{\"end\":11642,\"start\":11584},{\"end\":11918,\"start\":11911},{\"end\":12166,\"start\":12138},{\"end\":12402,\"start\":12398},{\"end\":12681,\"start\":12677}]"}}}, "year": 2023, "month": 12, "day": 17}