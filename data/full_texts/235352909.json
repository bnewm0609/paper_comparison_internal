{"id": 235352909, "updated": "2023-10-06 02:35:50.978", "metadata": {"title": "Few-Shot Segmentation via Cycle-Consistent Transformer", "authors": "[{\"first\":\"Gengwei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Guoliang\",\"last\":\"Kang\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Yunchao\",\"last\":\"Wei\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Few-shot segmentation aims to train a segmentation model that can fast adapt to novel classes with few exemplars. The conventional training paradigm is to learn to make predictions on query images conditioned on the features from support images. Previous methods only utilized the semantic-level prototypes of support images as conditional information. These methods cannot utilize all pixel-wise support information for the query predictions, which is however critical for the segmentation task. In this paper, we focus on utilizing pixel-wise relationships between support and query images to facilitate the few-shot segmentation task. We design a novel Cycle-Consistent TRansformer (CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR performs cross-attention between features from different images, i.e. support and query images. We observe that there may exist unexpected irrelevant pixel-level support features. Directly performing cross-attention may aggregate these features from support to query and bias the query features. Thus, we propose using a novel cycle-consistent attention mechanism to filter out possible harmful support features and encourage query features to attend to the most informative pixels from support images. Experiments on all few-shot segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable improvement compared to previous state-of-the-art methods. Specifically, on Pascal-$5^i$ and COCO-$20^i$ datasets, we achieve 67.5% and 45.6% mIoU for 5-shot segmentation, outperforming previous state-of-the-art methods by 5.6% and 7.1% respectively.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.02320", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ZhangKYW21", "doi": null}}, "content": {"source": {"pdf_hash": "d7f2a424056b43718b25aa9dc1074923bd258be6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.02320v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6a173500921c64bacc64399c889651c9832057fd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d7f2a424056b43718b25aa9dc1074923bd258be6.txt", "contents": "\nFew-Shot Segmentation via Cycle-Consistent Transformer\n\n\nGengwei Zhang \nCentre for Artificial Intelligence\nReLER\nUniversity of Technology Sydney\n\n\nGuoliang Kang \nUniversity of Texas\nAustin\n\nYi Yang yee.i.yang@gmail.com \nCollege of Computer Science and Technology\nCCAI\nZhejiang University\n\n\nYunchao Wei \nInstitute of Information Science\nBeijing Jiaotong University\n\n\nBeijing Key Laboratory of Advanced Information Science and Network\n\n\nBaidu Research \nFew-Shot Segmentation via Cycle-Consistent Transformer\n\nFew-shot segmentation aims to train a segmentation model that can fast adapt to novel classes with few exemplars. The conventional training paradigm is to learn to make predictions on query images conditioned on the features from support images. Previous methods only utilized the semantic-level prototypes of support images as the conditional information. These methods cannot utilize all pixel-wise support information for the query predictions, which is however critical for the segmentation task. In this paper, we focus on utilizing pixel-wise relationships between support and query images to facilitate the few-shot segmentation task. We design a novel Cycle-Consistent TRansformer (CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR performs cross-attention between features from different images, i.e. support and query images. We observe that there may exist unexpected irrelevant pixel-level support features. Directly performing cross-attention may aggregate these features from support to query and bias the query features. Thus, we propose using a novel cycle-consistent attention mechanism to filter out possible harmful support features and encourage query features to attend to the most informative pixels from support images. Experiments on all few-shot segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable improvement compared to previous state-of-the-art methods. Specifically, on Pascal-5 i and COCO-20 i datasets, we achieve 67.5% and 45.6% mIoU for 5-shot segmentation, outperforming previous state-of-the-art method by 5.6% and 7.1% respectively.\n\nIntroduction\n\nRecent years have witnessed great progress in semantic segmentation [19,4,47]. The success can be largely attributed to large amounts of annotated data [48,17]. However, labeling dense segmentation masks are very time-consuming [45]. Semi-supervised segmentation [15,39,38] has been broadly explored to alleviate this problem, which assumes a large amount of unlabeled data is accessible. However, semi-supervised approaches may fail to generalize to novel classes with very few exemplars. In the extreme low data regime, few-shot segmentation [26,35] is introduced to train a segmentation model that can quickly adapt to novel categories. Most few-shot segmentation methods follow a learning-to-learn paradigm where predictions of query images are made conditioned on the features and annotations of support images. The key to the success of this training paradigm lies in how to effectively utilize the information provided by support images. Previous approaches extract semantic-level prototypes from support features and follow a metric learning [29,7,35] pipeline extending from PrototypicalNet [28]. According to the granularity of utilizing support features, these methods can be categorized into two groups, as illustrated in Figure 1: 1) Class-wise mean pooling [35,46,44] (Figure 1(a)). Support features within regions of different categories are averaged to serve as prototypes to facilitate the classification of query pixels. 2) Clustering [18,41] (Figure 1(b)). Recent works attempt to generate multiple prototypes via EM algorithm or K-means clustering [41,18], in order to extract more abundant information from support images. These prototype-based methods need to \"compress\" support information into different prototypes (i.e. class-wise or cluster-wise), which may lead to various degrees of loss of beneficial support information and thus harm segmentation on query image. Rather than using prototypes to abstract the support information, [43,34] (Figure 1(c)) propose to employ the attention mechanism to extract information from support foreground pixels for segmenting query. However, such methods ignore all the background support pixels that can be beneficial for segmenting query image, and incorrectly consider partial foreground support pixels that are quite different from the query ones, leading to sub-optimal results. Many pixel-level support features are quite different from the query ones, and thus may confuse the attention. We incorporate cycle-consistency into attention to filter such confusing support features. Note that the confusing support features may come from foreground and background.\n\nIn this paper, we focus on equipping each query pixel with relevant information from support images to facilitate the query pixel classification. Inspired by the transformer architecture [32] which performs feature aggregation through attention, we design a novel Cycle-Consistent Transformer (CyCTR) module ( Figure 1(d)) to aggregate pixel-wise support features into query ones. Specifically, our CyCTR consists of two types of transformer blocks: the self-alignment block and the cross-alignment block. The selfalignment block is employed to encode the query image features by aggregating its relevant context information, while the cross-alignment aims to aggregate the pixel-wise features of support images into the pixel-wise features of query image. Different from self-alignment where Query 3 , Key and Value come from the same embedding, cross-alignment takes features from query images as Query, and those from support images as Key and Value. In this way, CyCTR provides abundant pixel-wise support information for pixel-wise features of query images to make predictions.\n\nMoreover, we observe that due to the differences between support and query images, e.g., scale, color and scene, only a small proportion of support pixels can be beneficial for the segmentation of query image. In other words, in the support image, some pixel-level information may confuse the attention in the transformer. Figure 2 provides a visual example of a support-query pair together with the label masks. The confusing support pixels may come from both foreground pixels and background pixels. For instance, point p 1 in the support image located in the plane afar, which is indicated as foreground by the support mask. However, the nearest point p 2 in the query image (i.e. p 2 has the largest feature similarity with p 1 ) belongs to a different category, i.e. background. That means, there exists no query pixel which has both high similarity and the same semantic label with p 1 . Thus, p 1 is likely to be harmful for segmenting \"plane\" and should be ignored when performing the attention. To overcome this issue, in CyCTR, we propose to equip the cross-alignment block with a novel cycle-consistent attention operation. Specifically, as shown in Figure 2, starting from the feature of one support pixel, we find its nearest neighbor in the query features. In turn, this nearest neighbor finds the most similar support feature. If the starting and the end support features come from the same category, a cycle-consistency relationship is established. We incorporate such an operation into attention to force query features only attend to cycle-consistent support features to extract information. In this way, the support pixels that are far away from query ones are not considered. Meanwhile, cycle-consistent attention enables us to more safely utilize the information from background support pixels, without introducing much bias into the query features.\n\nIn a nutshell, our contributions are summarized as follows: (1) We tackle few-shot segmentation from the perspective of providing each query pixel with relevant information from support images through pixel-wise alignment. (2) We propose a novel Cycle-Consistent TRansformer (CyCTR) to aggregate the pixel-wise support features into the query ones. In CyCTR, we observe that many support features may confuse the attention and bias pixel-level feature aggregation, and propose incorporating cycle-consistent operation into the attention to deal with this issue. 2 Related Work\n\n\nFew-Shot Segmentation\n\nFew-shot segmentation [26] is established to perform segmentation with very few exemplars. Recent approaches formulate few-shot segmentation from the view of metric learning [29,7,35]. For instance, [7] first extends PrototypicalNet [28] to perform few-shot segmentation. PANet [35] simplifies the framework with an efficient prototype learning framework. SG-One [46] leverage the cosine similarity map between the single support prototype and query features to guide the prediction. CANet [44] replaces the cosine similarity with an additive alignment module and iteratively refines the network output. PFENet [30] further designs an effective feature pyramid module and leverages a prior map to achieve better segmentation performance. Recently, [41,18,43] point out that only a single support prototype is insufficient to represent a given category. Therefore, they attempt to obtain multiple prototypes via EM algorithm to represent the support objects and the prototypes are compared with query image based on cosine similarity [18,41]. Besides, [43,34] attempt to use graph attention networks [33,40] to utilize all foreground support pixel features. However, they ignore all pixels in the background region by default. Besides, due to the large difference between support and query images, not all support pixels will benefit final query segmentation. Recently, some concurrent works propose to learn dense matching through Hypercorrelation Squeeze Networks [22] or mining latent classes [42] from the background region. Our work aims at mining information from the whole support image, but exploring to use the transformer architecture and from a different perspective, i.e., reducing the noise in the support pixel-level features.\n\n\nTransformer\n\nTransformer and self-attention were firstly introduced in the fields of machine translation and natural language processing [6,32], and are receiving increasing interests recently in the computer vision area. Previous works utilize self-attention as additional module on top of existing convolutional networks, e.g., Nonlocal [36] and CCNet [14]. ViT [8] and its following work [31] demonstrate the pure transformer architecture can achieve state-of-the-art for image recognition. On the other hand, DETR [3] builds up an end-to-end framework with a transformer encoder-decoder on top of backbone networks for object detection. And its deformable vairents [51] improves the performance and training efficiency. Besides, in natural language processing, a few works [2,5,27] have been introduced for long documents processing with sparse transformers. In these works, each Query token only attends to a pre-defined subset of Key positions.\n\n\nCycle-consistency Learning\n\nOur work is partially inspired by cycle-consistency learning [50,9] that is explored in various computer vision areas. For instance, in image translation, CycleGAN [50] uses cycle-consistency to align image pairs. It is also effective in learning 3D correspondence [49], consistency between video frames [37] and association between different domains [16]. These works typically constructs cycle-consistency loss between aligned targets (e.g., images). However, the simple training loss cannot be directly applied to few-shot segmentation because the test categories are unseen from the training process and no finetuning is involved during testing. In this work, we incorporate the idea of cycle-consistency into transformer to eliminate the negative effect of confusing or irrelevant support pixels.\n\n3 Methodology\n\n\nProblem Setting\n\nFew-shot segmentation aims at training a segmentation model that can segment novel objects with very few annotated samples. Specifically, given dataset D train and D test with category set C train and C test respectively, where C train \u2229 C test = \u2205, the model trained on D train is directly used to test on D test . In line with previous works [30,35,44], episode training is adopted in this work for few-shot segmentation. Each episode is composed of k support images I s and a query image \nI q to form a k-shot episode {{I s } k , I q },\n\nRevisiting of Transformer\n\nFollowing the general form in [32], a transformer block is composed of alternating layers of multi-head attention (MHA) and multi-layer perceptron (MLP). LayerNorm (LN) [1] and residual connection [12] are applied at the end of each block. Specially, an attention layer is formulated as\nAtten(Q, K, V ) = softmax( QK T \u221a d )V,(1)\nwhere\n[Q; K; V ] = [W q Z q ; W k Z kv ; W v Z kv ]\n, in which Z q is the input Query sequence, Z kv is the input Key/Value sequence, W q , W k , W v \u2208 R d\u00d7d denote the learnable parameters, d is the hidden dimension of the input sequences and we assume all sequences have the same dimension d by default.\n\nFor each Query element, the attention layer computes its similarities with all Key elements. Then the computed similarities are normalized via softmax, which are used to multiply the Value elements to achieve the aggregated outputs. When Z q = Z kv , it functions as self-attention mechanism.\n\nThe multi-head attention layer is an extention of attention layer, which performs h attention operations and concatenates consequences together. Specifically,\nMHA(Q, K, V ) = [head 1 , ..., head h ],(2)\nwhere\nhead m = Atten(Q m , K m , V m ) and the inputs [Q m , K m , V m ] are the m th group from [Q, K, V ] with dimension d/h.\n\nCycle-Consistent Transformer\n\nOur framework is illustrated in Figure 3(a). Generally, an encoder of our Cycle-Consistent TRansformer (CyCTR) consists of a self-alignment transformer block for encoding the query features and a cross-alignment transformer block to enable the query features to attend to the informative support features. The whole CyCTR module stacks L encoders.  Figure 3: Framework of our proposed Cycle-Consistent TRansformer (CyCTR). Each encoder of CyCTR consists of two transformers blocks, i.e., the self-alignment block for utilizing global context within the query feature map and the cross-alignment block for aggregate information from support images. In the cross-alignment block, we introduce the multi-head cycle-consistent attention (shown on the right, with the number of heads h = 1 for simplicity). The attention operation is guided by the cycle-consistency among query and support features.\n\nSpecifically, for the given query feature X q \u2208 R Hq\u00d7Wq\u00d7d and support feature X s \u2208 R Hs\u00d7Ws\u00d7d , we first flatten them into 1D sequences (with shape HW \u00d7 d) as inputs for transformer, in which a token is represented by the feature z \u2208 R d at one pixel location. The self-alignment block only takes the flattened query feature as input. As context information of each pixel has been proved beneficial for segmentation [4,47], we adopt the self-alignment block to pixel-wise features of query image to aggregate their global context information. We don't pass support images through the self-alignment block, as we mainly focus on the segmentation performance of query images. Passing through the support images which don't coordinate with the query mask may do harm to the self-alignment on query images.\n\nIn contrast, the cross-alignment block performs attention between query and support pixel-wise features to aggregate relevant support features into query ones. It takes the flattened query feature and a subset of support feature (the sampling procedure is discussed latter) with size N s \u2264 H s W s as Key/Value sequence Z kv .\n\nWith these two blocks, it is expected to better encoder the query features to facilitate the subsequent pixel-wise classification. When stacking L encoders, the output of the previous encoder is fed into the self-alignment block. The outputs of self-alignment block and the sampled support features are then fed into the cross-alignment block.\n\n\nCycle-Consistent Attention\n\nAccording to the aforementioned discussion, the pure pixel-level attention may be confused by excessive irrelevant support features. To alleviate this issue, as shown in Figure 3(b), a cycleconsistent attention operation is proposed. We first go through the proposed approach for 1-shot case for presentation simplicity and then discuss it in the multiple shot setting.\n\nFormally, an affinity map A = QK T \u221a d , A \u2208 R HqWq\u00d7Ns is first calculated to measure the correspondence between all query and support pixels. Then, for an arbitrary support pixel/token j (j \u2208 {0, 1, ..., N s \u2212 1}, N s is the number of support pixels), its most similar query pixel/token i is obtained by\ni = argmax i A (i,j) ,(3)\nwhere i \u2208 {0, 1, ..., H q W q \u2212 1} denotes the spatial index of query pixels. Since the query mask is not accessible, the label of query pixel i is unknown. However, we can in turn find its most similar support pixel j in the same way:\nj = argmax j A (i ,j) .(4)\nGiven the sampled support label M s \u2208 R Ns , cycle-consistency is satisfied if M s(j) = M s(j ) . Previous work [16] attempts to encourage the feature similarity between cycle-consistent pixels to improve the model's generalization ability within the same set of categories. However, in few-shot segmentation, the goal is to enable the model to fast adapt to novel categories rather than making the model fit better to training categories. Thus, we incorporate the cycle-consistency into the attention operation to encourage the cycle-consistent cross-attention. First, by traversing all support tokens, an additive bias B \u2208 R Ns is obtained by\nB j = 0, ifM s(j) = M s(j ) \u2212\u221e, ifM s(j) = M s(j ) ,\nwhere j \u2208 {0, 1, ..., N s }. Then, for a single query token Z q(i) \u2208 R d at location i, the support information is aggregated by\nCyCAtten(Q i , K i , V i ) = softmax(A (i) + B)V,(5)\nwhere i \u2208 {0, 1, ..., H q W q } and A is obtained by QK T \u221a d . In the forward process, B is element-wise added with the affinity A (i) for Z q(i) to aggregate support features. In this way, the attention weight for the cycle-inconsistent support features become zero, implying that these irrelevant information will not be considered. Besides, the cycle-consistent attention implicitly encourages the consistency between the most relevant query and support pixel-wise features through backpropagation. Note that our method aims at removing support pixels with certain inconsistency, rather than ensuring all support pixels to form cycle-consistency, which is impossible without knowing the query ground truth labels.\n\nWhen performing self-attention in the self-alignment block, there may also exist the same issue, i.e. the query token may attend to irrelevant or even harmful features (especially when background is complex). According to our cycle-consistent attention, each query token should receive information from more consistent pixels than aggregating from all pixels. Due to the lack of query mask M q , it is impossible to establish the cycle-consistency among query pixels/tokens. Inspired by DeformableAttention [51], the consistent pixels can be obtained via a learnable way as \u2206 = f (Q + Coord) and A = g(Q + Coord), where \u2206 \u2208 R HpWp\u00d7P is the predicted consistent pixels, in which each element \u03b4 \u2208 R P in \u2206 represents the relative offset from each pixel and P represents the number of pixels to aggregate. And A \u2208 R HqWq\u00d7P is the attention weights. Coord \u2208 R HqWq\u00d7d is the positional encoding [24] to make the prediction be aware of absolute position, and f (\u00b7) and g(\u00b7) are two fully connected layers that predict the offsets 4 and attention weights. Therefore, the self-attention within the self-alignment transformer block is represented as\nPredAtten(Q r , V r ) = P g softmax(A ) (r,g) V r+\u2206 (r,g) ,(6)\nwhere r \u2208 {0, 1, ..., H q W q } is the index of the flattened query feature, both Q and V are obtained by multiplying the flattened query feature with the learnable parameter.\n\nGenerally speaking, the cycle-consistent transformer effectively avoids the attention being biased by irrelevant features to benefit the training of few-shot segmentation.\n\nMask-guided sparse sampling and K-shot Setting: Our proposed cycle-consistency transformer can be easily extended to K-shot setting where K > 1. When multiple support feature maps are provided, all support features are flattened and concatenated together as input. As the attention is performed at the pixel-level, the computation load will be high if the number of support pixels/tokens is large, which is usually the case under K-shot setting. In this work, we apply a simple maskguided sampling strategy to reduce the computation complexity and make our method more scalable. Concretely, given the k-shot support sequence Z s \u2208 R kHsWs\u00d7d and the flattened support masks M s \u2208 R kHsWs , the support pixels/tokens are obtained by uniformly sampling N f g tokens (N f g <= Ns 2 , where N s \u2264 kH s W s ) from the foreground regions and N s \u2212 N f g tokens from the background regions in all support images. With a proper N s , the sampling operation reduces the computational complexity, and makes our algorithm more scalable with the increase of spatial size of support images. Additionally, this strategy helps balance the foreground-background ratio and also implicitly considers different sizes of various object regions in support images. \n\n\nOverall Framework\n\nFollowing previous works [30,35,44], both query and support images are first feed into a shared backbone (e.g., ResNet [12]) which is initialized with weights pretrained from ImageNet [25] to obtain general image features. Similar to [30], middle-level query features (the concatenation of query features from the 3 rd and the 4 th blocks of ResNet) are processed by a 1\u00d71 convolution to reduce the hidden dimension. The high-level query features (from the 5 th block) are used to generate a prior map (the prior map is generated by calculating the pixel-wise similarity between query and support features, details can be found in the supplementary materials) and then are concatenated with the middle-level query features. The average masked support feature is also concatenated to provide global support information. The concatenated features are processed by a 1\u00d71 convolution. The output query features are then fed into our proposed CyCTR encoders. The output of CyCTR encoders is fed into a classifier to obtain the final segmentation results. The classifier consists of a 3\u00d73 convolutional layer, a ReLU layer and a 1\u00d71 convolutional layer. More details about our network structure can be found in the supplementary materials.\n\n\nExperiments\n\n\nDataset and Evaluation Metric\n\nWe conduct experiments on two commonly used few-shot segmentation datasets, Pascal-5 i [10] (which is combined with SBD [11] dataset) and COCO-20 i [17], to evaluate our method. For Pascal-5 i , 20 classes are separated into 4 splits. For each split, 15 classes are used for training and 5 classes for test. At the test time, 1,000 pairs that belong to the testing classes are sampled from the validation set for evaluation. In COCO-20 i , we follow the data split settings in FWB [23] to divide 80 classes evenly into 4 splits, 60 classes for training and test on 20 classes, and 5,000 validation pairs from the 20 classes are sampled for evaluation. Detailed data split settings can be found in the supplementary materials. Following common practice [30,35,46], the mean intersection over union (mIoU) is adopted as the evaluation metric, which is the averaged value of IoU of all test classes. We also report the foreground-background IoU (FB-IoU) for comparison.\n\n\nImplementation Details\n\nIn our experiments, the training strategies follow the same setting in [30]: training for 50 epochs on COCO-20 i and 200 epochs on Pascal-5 i . Images are resized and cropped to 473 \u00d7 473 for both datasets and we use random rotation from \u221210 \u2022 to 10 \u2022 as data augmentation. Besides, we use ImageNet [25] pretrained ResNet [12] as the backbone network and its parameters (including BatchNorms) are freezed. For the parameters except those in the transformer layers, we use the initial learning rate 2.5 \u00d7 10 \u22123 , momentum 0.9, weight decay 1 \u00d7 10 \u22124 and SGD optimizer with poly learning rate decay [4]. The mini batch size on each gpu is set to 4. Experiments are carried out on Tesla V100 GPUs. For Pascal-5 i , one model is trained on a single GPU, while for COCO-20 i , one model is trained with 4 GPUs. We construct our baseline as follows: as stated in Section 3.4, the middle-level query features from backbone network are concatenated and merged with the global support feature and the prior map. This feature is processed by two residule blocks and input to the same classifier as our method. Dice loss [21] is used as the training objective. Besides, the middle-level query feature is averaged using the ground truth and concatenated with support feature to predict the support segmentation map, which produces an auxiliary loss for aligning features.\n\nThe same settings are also used in our method except that we use our cycle-consistent transformer to process features rather than the residule blocks. For the proposed cycle-consistent transformer, we set the number of sampled support tokens N s to 600 for 1-shot and 5 \u00d7 600 for 5-shot setting. The number of sampled tokens is obtained according to the averaged number of foreground pixels among Pascal-5 i training set. For the self-attention block, the number of points P is set to 9. For other hyper-parameters in transformer blocks, we use L = 2 transformer encoders. We set the hidden dimension of MLP layer to 3\u00d7256 and that of input to 256. The number of heads for all attention layers is set to 8 for Pascal-5 i and 1 for COCO-20 i . Parameters in the transformer blocks are optimized with AdamW [20] optimizer following other transformer works [3,8,31], with learning rate 1 \u00d7 10 \u22124 and weight decay 1 \u00d7 10 \u22122 . Besides, we use Dropout with the probability 0.1 in all attention layers.\n\n\nComparisons with State-of-the-Art Methods\n\nIn Table 1 and Table 2, we compare our method with other state-of-the-art few-shot segmentation approaches on Pascal-5 i and COCO-20 i respectively. It can be seen that our approach achieves new state-of-the-art performance on both Pascal-5 i and COCO-20 i . Specifically, on Pascal-5 i , to make fair comparisons with other methods, we report results with both ResNet-50 and ResNet-101. Our CyCTR achieves 64.0% mIoU with ResNet-50 backbone and 63.7% mIoU with ResNet-101 backbone for 1-shot segmentation, significantly outperforming previous state-ofthe-art results by 3.2% and 3.6% respectively. For 5-shot segmentation, our CyCTR can even surpass state-of-the art methods by 5.6% and 6.0% mIoU when using ResNet-50 and ResNet-101 backbones respectively. For COCO-20 i results in Table 2, our method also outperforms other methods by a large margin due to the capability of the transformer to fit more complex data. Besides, Table 3 shows the comparison using FB-IoU on PASCAL-5 i for 1-shot and 5-shot segmentation, our method also obtains the state-of-the-art performance.\n\n\nAblation Studies\n\nTo provide a deeper understanding of our proposed method, we show ablation studies in this section. The experiments are performed on Pascal-5 i 1-shot setting with ResNet-50 as the backbone network, and results are reported in terms of mIoU. \n\n\nComponent-Wise Ablations\n\nWe perform ablation studies regarding each component of our CyCTR in Table 4. The first line is the result of our baseline, where we use two residual blocks to merge features as stated in Section 4.2. For all ablations in Table 4, the hidden dimension is set to 128 and two transformer encoders are used.\n\nThe mIoU results are averaged over four splits. Firstly, we only use the self-alignment block that only encodes query features. The support information in this case comes from the concatenated global support feature and the prior map used in [44]. It can already bring decent results, showing that the transformer encoder is effective for modeling context for few-shot segmentation. Then, we utilize the cross-alignment block but only with the vanilla attention operation in Equation 1. The mIoU increases by 0.4%, indicating that pixel-level features from support can provide additional performance gain. By using our proposed cycle-consistent attention module, the performance can be further improved by a large margin, i.e. 0.6% mIoU compared to the vanilla attention. This result demonstrates our cycle-consistent attention's capability to suppress possible harmful information from support. Besides, we assume some background support features may also benefit the query segmentation and therefore use the cycle-consistent transformer to aggregate pixel-level information from background support features as well. Comparing the last two lines in Table 4, we show that our way of utilizing beneficial background pixel-level support information brings 0.5% mIoU improvement, validating our assumption and the effectiveness of our proposed cycle-consistent attention operation.\n\nBesides, one may be curious about whether the noise can also be removed by predicting the aggregation position like the way in Equation 6 for aggregating support features to query. Therefore, we use predicted aggregation instead of the cycle-consistent attention in the cross-alignment block, as denoted by CyCTR(pred) in Table 4. It does benefit the few-shot segmentation by aggregating useful information from support but is 0.9% worse than the proposed cycle-consistent attention. The reason lies in the dramatically changing support images under few-shot segmentation testing. The cycle-consistency is better than the learnable way as it can globally consider the varying conditional information from both query and support. We can stack more encoders or increase the hidden dimension of encoders to increase its capacity and validate the effectiveness of our CyCTR. The results with different numbers of encoders (denoted as L) or hidden dimensions (denoted as d) are shown in Table 5a and 5b. While increasing L or d within a certain range, CyCTR achieves better results. We chose L = 2 as our default choice for accuracy-efficiency trade-off.\n\n\nEffect of Model Capacity\n\n\nQualitative results\n\nIn Figure 4, we show some qualitative results generated by our model on Pascal-5 i . Our cycleconsistent attention can improve the segmentation quality by suppressing possible harmful information from support. For instance, without cycle-consistency, the model misclassifies trousers as \"cow\" in the first row, baby's hair as \"cat\" in the second row, and a fraction of mountain as \"car\" in the third row, while our model rectifies these part as background. However, in the first row, our CyCTR still Support Query Ours without Cycle-consistency Ours segments part of the trousers as \"cow\" and the right boundary of the segmentation mask is slightly worse than the model without cycle-consistency. The reason comes from the extreme differences between query and support, i.e. the support image shows a \"cattle\" but the query image contains a milk cow. The cycle-consistency may over-suppress the positive region in support images. Solving such issue may be a potential direction to investigate to improve our method further.\n\n\nConclusion\n\nIn this paper, we design a CyCTR module to deal with the few-shot segmentation problem. Different from previous practices that either adopt semantic-level prototype(s) from support images or only use foreground support features to encode query features, our CyCTR utilizes all pixel-level support features and can effectively eliminate aggregating confusing and harmful support features with the proposed novel cycle-consistency attention. We conduct extensive experiments on two popular benchmarks, and our CyCTR outperforms previous state-of-the-art methods by a significant margin. We hope this work can motivate researchers to utilize pixel-level support features to design more effective algorithms to advance the few-shot segmentation research.  Figure 5: The network structure used in our experiments. The backbone network first extracts features for query and support images. To enable the pixel-wise comparison in transformer, the averaged foreground support feature is expanded and concatenated with both query and support features. Our Cycle-Consistent TRansformer (CyCTR) takes the flattened query and support features as well as the flattened support mask as input and produces the encoded query feature for prediction.\n\nThe overall network architecture used in our experiments is shown in Figure 5. Following the common practice [30,35,44], query and support image are first feed into a shared backbone network to obtain general image features. Similar to [30], the backbone network is pretrained on ImageNet [25] and then completely kept fixed during few-shot segmentation training. Following [18,30,41], we use dilated version of ResNet [12] as the backbone network. Besides, middle-level features are processed by a 1x1 convolution to reduce the hidden dimension and high-level features are used to generate a prior map that concatenated with the middle-level feature. In details, the middle-level feature consists of the concatenation of features from the 3 rd and the 4 th block of ResNet (total 5 blocks including the stem block) with shape H \u00d7 W \u00d7 (512 + 1024) and is feed into a 1 \u00d7 1 convolution to reduce the dimension to H \u00d7 W \u00d7 d, where d is the hidden dimension that can be adjusted in our experiments. The high-level feature (from the 5 th block of ResNet) with shape H \u00d7 W \u00d7 2048 is used to generate the prior mask as in [30], which compute the pixel-wise similarity between the query and support high-level features and keep the maximum similarity at each pixel and normalize (using min-max normalization) the similarity map to the range of [0, 1]. To enable the pixel-wise comparison, we also concatenate the mask averaged support feature to both query and support feature and processed by a 1x1 convolution before inputting into the transformer. The final segmentation result is obtained by reshaping the output sequence back to spatial dimensions and predicted by a small convolution head that is consisted of one 3x3 convolution, one ReLU activation, and a 1x1 convolution. Dice loss [21] is used as the training objective.\n\nBaseline setup: For the baseline of our method, we use two residual blocks [12] to merge the query feature. The support information comes from the concatenated support global feature and the prior map. During training, the foreground middle-level query feature from backbone network is averaged and concatenated with the middle-level support feature to predict the support mask for feature alignment. This auxiliary supervision is included in all of our experiments.\n\n\nA.2 Dataset Settings\n\nIn this Table 6 and Table 7, we provide the detailed split settings for datasets (Pascal 5 i and COCO-20 i ) used in our experiments, which follow the split settings proposed in [23].\n\n\nSplit\n\nTest classes PASCAL-5 0 aeroplane, bicycle, bird, boat, bottle PASCAL-5 1 bus, car, cat, chair, cow PASCAL-5 2 diningtable, dog, horse, motorbike, person PASCAL-5 3 potted plant, sheep, sofa, train, tv/monitor Table 6: Data split for PASCAL-5 i , which follows the 4-fold cross-validation. Each row contains 5 classes for test and the rest 15 classes in the PASCAL dataset are used for training.  \n\n\nB More Visualizations\n\nWe provide more visualizations in Figure 6. We also provide the visualization of cycle-consistency relationships. In the first row, only a small part of the foreground region is activated while most foreground regions are valid in the second row. And in the second row, pixels on the \"person\" are shown in gray, which indicates that these pixels may have a negative impact on segmenting \"cat\".\n\n\nSupport\n\n\nQuery\n\nOurs without Cycle-consistency Ours Cycle-consistency Figure 6: More Qualitative results on Pascal-5 i . The cycle-consistency is visualized in the 2 ed column, in which red points are cycle-consistent foreground pixels, blue points are cycle-consistent background pixels, and gray points are cycle-inconsistent pixels. Best viewed in color and with zoom-in.\n\nFigure 1 :\n1Different learning frameworks for few-shot segmentation, from the perspective of ways to utilize support information. (a) Class-wise mean pooling based method. (b) Clustering based method. (c) Foreground pixel attention method. (d) Our Cycle-Consistent TRansformer (CyCTR) framework that enables all beneficial support pixel-level features (foreground and background) to be considered.\n\nFigure 2 :\n2The motivation of our proposed method.\n\n( 3 )\n3Our CyCTR achieves state-ofthe-art results on two few-shot segmentation benchmarks, i.e., Pascal-5 i and COCO-20 i . Extensive experiments validate the effectiveness of each component in our CyCTR.\n\nFigure 4 :\n4Qualitative results on Pascal-5 i . From left to right, each column shows the examples of: Support image with mask region in red; Query image with ground truth mask region in blue; Result produced by the model without cycle-consistency in CyCTR; Result produced by our method.\n\n\nin which all {I s } k and I q contain objects from the same category. Then the training set and test set are represented by D train = {{I s } k , I q } Ntrain and D test = {{I s } k , I q } Ntest , where N train and N test is the number of episodes for training and test set. During training, both support masks M s and query masks M q are available for training images, and only support masks are accessible during testing.\n\nTable 1 :\n1Comparison with other state-of-the-art methods for 1-shot and 5-shot segmentation on PASCAL-5 i using the mIoU (%) evaluation metric. Best results are shown in bold.Method \nBackbone \n1-shot \n5-shot \n5 0 \n5 1 \n5 2 \n5 3 \nMean \n5 0 \n5 1 \n5 2 \n5 3 \nMean \nPANet [35] \n\nVgg-16 \n\n42.3 58.0 51.1 41.2 \n48.1 \n51.8 64.6 59.8 46.5 \n55.7 \nFWB [23] \n47.0 59.6 52.6 48.3 \n51.9 \n50.9 62.9 56.5 50.1 \n55.1 \nSG-One [46] \n40.2 58.4 48.4 38.4 \n46.3 \n41.9 58.6 48.6 39.4 \n47.1 \nRPMM [41] \n47.1 65.8 50.6 48.5 \n53.0 \n50.0 66.5 51.9 47.6 \n54.0 \nCANet [44] \n\nRes-50 \n\n52.5 65.9 51.3 51.9 \n55.4 \n55.5 67.8 51.9 53.2 \n57.1 \nPGNet [43] \n56.0 66.9 50.6 50.4 \n56.0 \n57.7 68.7 52.9 54.6 \n58.5 \nRPMM [41] \n55.2 66.9 52.6 50.7 \n56.3 \n56.3 67.3 54.5 51.0 \n57.3 \nPPNet [18] \n47.8 58.8 53.8 45.6 \n51.5 \n58.4 67.8 64.9 56.7 \n62.0 \nPFENet [30] \n61.7 69.5 55.4 56.3 \n60.8 \n63.1 70.7 55.8 57.9 \n61.9 \nCyCTR (Ours) \nRes-50 \n65.7 71.0 59.5 59.7 \n64.0 \n69.3 73.5 63.8 63.5 \n67.5 \nFWB [23] \nRes-101 \n\n51.3 64.5 56.7 52.2 \n56.2 \n54.9 67.4 62.2 55.3 \n59.9 \nDAN [34] \n54.7 68.6 57.8 51.6 \n58.2 \n57.9 69.0 60.1 54.9 \n60.5 \nPFENet [30] \n60.5 69.4 54.4 55.9 \n60.1 \n62.8 70.4 54.9 57.6 \n61.4 \nCyCTR (Ours) \nRes-101 \n67.2 71.1 57.6 59.0 \n63.7 \n71.0 75.0 58.5 65.0 \n67.4 \n\n\n\nTable 2 :\n2Comparison with other state-of-the-art methods for 1-shot and 5-shot segmentation on COCO-20 i using the mIoU (%) evaluation metric. Best results are shown in bold.Method \nBackbone \n1-shot \n5-shot \n20 0 \n20 1 \n20 2 \n20 3 Mean 20 0 \n20 1 \n20 2 \n20 3 Mean \nFWB [23] \nRes-101 \n19.9 18.0 21.0 28.9 \n21.2 \n19.1 21.5 23.9 30.1 \n23.7 \nPPNet [18] \nRes-50 \n28.1 30.8 29.5 27.7 \n29.0 \n39.0 40.8 37.1 37.3 \n38.5 \nRPMM [41] \nRes-50 \n29.5 36.8 29.0 27.0 \n30.6 \n33.8 42.0 33.0 33.3 \n35.5 \nPFENet [30] \nRes-101 \n34.3 33.0 32.3 30.1 \n32.4 \n38.5 38.6 38.2 34.3 \n37.4 \nCyCTR (Ours) \nRes-50 \n38.9 43.0 39.6 39.8 \n40.3 \n41.1 48.9 45.2 47.0 \n45.6 \n\n\n\nTable 3 :\n3Comparison with other methods using \nFB-IoU (%) on Pascal-5 i for 1-shot and 5-shot \nsegmentation. \n\nMethod \nBackbone \nFB-IoU (%) \n1-shot 5-shot \nA-MCG [13] \nRes-101 \n61.2 \n62.2 \nDAN [34] \nRes-101 \n71.9 \n72.3 \nPFENet [30] \nRes-101 \n72.9 \n73.5 \nCyCTR (Ours) \nRes-101 \n73.0 \n75.4 \n\n\n\nTable 4 :\n4Ablation studies that validate the effectiveness of each component in our Cycle-Consistent \nTRansformer. The first result is obtained by our baseline (see Section 4.2 for details). \n\nself-alignment cross-alignment CyCTR (pred) CyCTR (fg. only) CyCTR mIoU (%) \n59.3 \n62.5 \n62.9 \n62.6 \n63.0 \n63.5 \n\n\n\nTable 5 :\n5Effect of varying (a) number of encoders \nL and (b) hidden dimensions d. When varying L, \nd is fixed to 128; while varying d, L is fixed to 2. \n\n#Encoder mIoU (%) \n1 \n62.4 \n2 \n63.5 \n3 \n63.7 \n\n(a) \n\n#Dim mIoU (%) \n128 \n63.5 \n256 \n64.0 \n384 \n63.9 \n\n(b) \n\n\n\n\nPerson, Airplane, Boat, Park meter, Dog, Elephant, Backpack, Suitcase, Sports ball, Skateboard, W. glass, Spoon, Sandwich, Hot dog, Chair, D. table, Mouse, Microwave, Fridge, Scissors, COCO-20 1 Bicycle, Bus, T.light, Bench, Horse, Bear, Umbrella, Frisbee, Kite, Surfboard, Cup, Bowl, Orange, Pizza, Couch, Toilet, Remote, Oven, Book, Teddy, COCO-20 2 Car, Train, Fire H., Bird, Sheep, Zebra, Handbag, Skis, B. bat, T. racket, Fork, Banana, Broccoli, Donut, P. plant, TV, Keyboard, Toaster, Clock, Hairdrier, COCO-20 3 Motorcycle, Truck, Stop, Cat, Cow, Giraffe, Tie, Snowboard, B. glove, Bottle, Knife, Apple, Carrot, Cake, Bed, Laptop, Cellphone, Sink, Vase, Toothbrush,Split \nTest classes \n\nCOCO-20 0 \n\n\n\nTable 7 :\n7Data split for COCO-20 i , which follows the 4-fold cross-validation. Each row contains 20 classes for test and the rest classes in the COCO dataset are used for training.\nTo distinguish from the phrase \"query\" in few-shot segmentation, we use \"Query\" with capitalization to note the query sequence in the transformer.\nThe offsets are predicted as 2d coordinates and transformed into 1d coordinates.\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\nIz Beltagy, E Matthew, Arman Peters, Cohan, Longformer, arXiv:2004.05150The long-document transformer. arXiv preprintIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.\n\nEnd-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision. SpringerNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213-229. Springer, 2020.\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE transactions on pattern analysis and machine intelligence. 40Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834-848, 2017.\n\nGenerating long sequences with sparse transformers. Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, arXiv:1904.10509arXiv preprintRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nFew-shot semantic segmentation with prototype learning. Nanqing Dong, Eric P Xing, BMVC. Nanqing Dong and Eric P Xing. Few-shot semantic segmentation with prototype learning. In BMVC, volume 3-4, 2018.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.11929An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nTemporal cycle-consistency learning. Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal cycle-consistency learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1801-1810, 2019.\n\nThe pascal visual object classes (voc) challenge. Mark Everingham, Luc Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, International journal of computer vision. 882Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303-338, 2010.\n\nSimultaneous detection and segmentation. Pablo Bharath Hariharan, Ross Arbel\u00e1ez, Jitendra Girshick, Malik, European Conference on Computer Vision. SpringerBharath Hariharan, Pablo Arbel\u00e1ez, Ross Girshick, and Jitendra Malik. Simultaneous detection and segmentation. In European Conference on Computer Vision, pages 297-312. Springer, 2014.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR). the IEEE conference on computer vision and pattern recognition (CVPR)Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 770-778, 2016.\n\nAttentionbased multi-context guiding for few-shot semantic segmentation. Tao Hu, Pengwan Yang, Chiliang Zhang, Gang Yu, Yadong Mu, G M Cees, Snoek, Association for the Advancement of Artificial Intelligence (AAAI). 33Tao Hu, Pengwan Yang, Chiliang Zhang, Gang Yu, Yadong Mu, and Cees GM Snoek. Attention- based multi-context guiding for few-shot semantic segmentation. In Association for the Ad- vancement of Artificial Intelligence (AAAI), volume 33, pages pp. 8441-8448, 2019.\n\nCcnet: Criss-cross attention for semantic segmentation. Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 603-612, 2019.\n\nWeaklysupervised semantic segmentation network with deep seeded region growing. Zilong Huang, Xinggang Wang, Jiasi Wang, Wenyu Liu, Jingdong Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZilong Huang, Xinggang Wang, Jiasi Wang, Wenyu Liu, and Jingdong Wang. Weakly- supervised semantic segmentation network with deep seeded region growing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7014-7023, 2018.\n\nPixellevel cycle association: A new perspective for domain adaptive semantic segmentation. Guoliang Kang, Yunchao Wei, Yi Yang, Yueting Zhuang, Alexander G Hauptmann, NeurIPS. Guoliang Kang, Yunchao Wei, Yi Yang, Yueting Zhuang, and Alexander G Hauptmann. Pixel- level cycle association: A new perspective for domain adaptive semantic segmentation. In NeurIPS, 2020.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision (ECCV). Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision (ECCV), 2014.\n\nSongyang Zhang, and Xuming He. Part-aware prototype network for few-shot semantic segmentation. Yongfei Liu, Xiangyi Zhang, European Conference on Computer Vision. SpringerYongfei Liu, Xiangyi Zhang, Songyang Zhang, and Xuming He. Part-aware prototype network for few-shot semantic segmentation. In European Conference on Computer Vision, pages 142-158. Springer, 2020.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for se- mantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431-3440, 2015.\n\n. Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. arXiv preprintIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nV-net: Fully convolutional neural networks for volumetric medical image segmentation. Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi, 2016 fourth international conference on 3D vision (3DV). IEEEFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565-571. IEEE, 2016.\n\nHypercorrelation squeeze for few-shot segmentation. Juhong Min, Dahyun Kang, Minsu Cho, arXiv:2104.01538arXiv preprintJuhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmenta- tion. arXiv preprint arXiv:2104.01538, 2021.\n\nFeature weighting and boosting for few-shot segmentation. Khoi Nguyen, Sinisa Todorovic, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionKhoi Nguyen and Sinisa Todorovic. Feature weighting and boosting for few-shot segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 622-631, 2019.\n\nImage transformer. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran, International Conference on Machine Learning. PMLRNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pages 4055-4064. PMLR, 2018.\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International journal of computer vision. 1153Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015.\n\nIrfan Essa, and Byron Boots. One-shot learning for semantic segmentation. Amirreza Shaban, Shray Bansal, Zhen Liu, British Machine Vision Conference (BMVC). Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation. In British Machine Vision Conference (BMVC), 2018.\n\nSparsebert: Rethinking the importance analysis in self-attention. Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, James T Kwok, arXiv:2102.12871arXiv preprintHan Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and James T Kwok. Sparsebert: Rethinking the importance analysis in self-attention. arXiv preprint arXiv:2102.12871, 2021.\n\nPrototypical networks for few-shot learning. Jake Snell, Kevin Swersky, Richard Zemel, Advances in neural information processing systems (NeurIPS). Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in neural information processing systems (NeurIPS), 2017.\n\nLearning to compare: Relation network for few-shot learning. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, H S Philip, Timothy M Torr, Hospedales, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1199-1208, 2018.\n\nPrior guided feature enrichment network for few-shot segmentation. Z Tian, M Zhao, Shu, Yang, J Li, Jia, IEEE Transactions on Pattern Analysis and Machine Intelligence. Z Tian, H Zhao, M Shu, Z Yang, R Li, and J Jia. Prior guided feature enrichment network for few-shot segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou, arXiv:2012.12877arXiv preprintHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000-6010, 2017.\n\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.10903Graph attention networks. arXiv preprintPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\nXianbin Cao, and Xiantong Zhen. Few-shot semantic segmentation with democratic attention networks. Haochen Wang, Xudong Zhang, Yutao Hu, Yandan Yang, European Conference on Computer Vision (ECCV). 2020Haochen Wang, Xudong Zhang, Yutao Hu, Yandan Yang, Xianbin Cao, and Xiantong Zhen. Few-shot semantic segmentation with democratic attention networks. In European Conference on Computer Vision (ECCV), 2020.\n\nPanet: Few-shot image semantic segmentation with prototype alignment. Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, Jiashi Feng, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionKaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot image semantic segmentation with prototype alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9197-9206, 2019.\n\nNon-local neural networks. Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7794-7803, 2018.\n\nLearning correspondence from the cycleconsistency of time. Xiaolong Wang, Allan Jabri, Alexei A Efros, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle- consistency of time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2566-2576, 2019.\n\nObject region mining with adversarial erasing: A simple classification to semantic segmentation approach. Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, and Shuicheng Yan. Object region mining with adversarial erasing: A simple classification to semantic segmentation approach. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1568-1576, 2017.\n\nRevisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation. Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi Feng, Thomas S Huang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi Feng, and Thomas S Huang. Revisiting dilated convolution: A simple approach for weakly-and semi-supervised seman- tic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7268-7277, 2018.\n\nBidirectional graph reasoning network for panoptic segmentation. Yangxin Wu, Gengwei Zhang, Yiming Gao, Xiajun Deng, Ke Gong, Xiaodan Liang, Liang Lin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYangxin Wu, Gengwei Zhang, Yiming Gao, Xiajun Deng, Ke Gong, Xiaodan Liang, and Liang Lin. Bidirectional graph reasoning network for panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9080-9089, 2020.\n\nPrototype mixture models for few-shot semantic segmentation. Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, Qixiang Ye, European Conference on Computer Vision. SpringerBoyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang Ye. Prototype mixture models for few-shot semantic segmentation. In European Conference on Computer Vision, pages 763-778. Springer, 2020.\n\nMining latent classes for few-shot segmentation. Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, Yang Gao, arXiv:2103.15402arXiv preprintLihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. Mining latent classes for few-shot segmentation. arXiv preprint arXiv:2103.15402, 2021.\n\nPyramid graph networks with connection attentions for region-based one-shot semantic segmentation. Chi Zhang, Guosheng Lin, Fayao Liu, Jiushuang Guo, Qingyao Wu, Rui Yao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionChi Zhang, Guosheng Lin, Fayao Liu, Jiushuang Guo, Qingyao Wu, and Rui Yao. Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9587-9595, 2019.\n\nCanet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, Chunhua Shen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionChi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen. Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5217-5226, 2019.\n\nInteractive object segmentation with inside-outside guidance. Shiyin Zhang, Jun Hao Liew, Yunchao Wei, Shikui Wei, Yao Zhao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionShiyin Zhang, Jun Hao Liew, Yunchao Wei, Shikui Wei, and Yao Zhao. Interactive object segmentation with inside-outside guidance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12234-12244, 2020.\n\nSg-one: Similarity guidance network for one-shot semantic segmentation. Xiaolin Zhang, Yunchao Wei, Yi Yang, Thomas S Huang, IEEE Transactions on Cybernetics. 509Xiaolin Zhang, Yunchao Wei, Yi Yang, and Thomas S Huang. Sg-one: Similarity guidance network for one-shot semantic segmentation. IEEE Transactions on Cybernetics, 50(9):3855- 3865, 2020.\n\nPyramid scene parsing network. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017.\n\nScene parsing through ade20k dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633-641, 2017.\n\nLearning dense correspondence via 3d-guided cycle consistency. Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, Alexei A Efros, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, and Alexei A Efros. Learning dense correspondence via 3d-guided cycle consistency. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 117-126, 2016.\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223-2232, 2017.\n\nDeformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, arXiv:2010.04159arXiv preprintXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n", "annotations": {"author": "[{\"end\":147,\"start\":58},{\"end\":190,\"start\":148},{\"end\":290,\"start\":191},{\"end\":435,\"start\":291},{\"end\":451,\"start\":436}]", "publisher": null, "author_last_name": "[{\"end\":71,\"start\":66},{\"end\":161,\"start\":157},{\"end\":198,\"start\":194},{\"end\":302,\"start\":299},{\"end\":450,\"start\":442}]", "author_first_name": "[{\"end\":65,\"start\":58},{\"end\":156,\"start\":148},{\"end\":193,\"start\":191},{\"end\":298,\"start\":291},{\"end\":441,\"start\":436}]", "author_affiliation": "[{\"end\":146,\"start\":73},{\"end\":189,\"start\":163},{\"end\":289,\"start\":221},{\"end\":365,\"start\":304},{\"end\":434,\"start\":367}]", "title": "[{\"end\":55,\"start\":1},{\"end\":506,\"start\":452}]", "venue": null, "abstract": "[{\"end\":2132,\"start\":508}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2220,\"start\":2216},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2222,\"start\":2220},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2225,\"start\":2222},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2304,\"start\":2300},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2307,\"start\":2304},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2380,\"start\":2376},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2415,\"start\":2411},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2418,\"start\":2415},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2421,\"start\":2418},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2696,\"start\":2692},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2699,\"start\":2696},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3202,\"start\":3198},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3204,\"start\":3202},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3207,\"start\":3204},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3252,\"start\":3248},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3423,\"start\":3419},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3426,\"start\":3423},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3429,\"start\":3426},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3605,\"start\":3601},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3608,\"start\":3605},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3720,\"start\":3716},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3723,\"start\":3720},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4111,\"start\":4107},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4114,\"start\":4111},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4974,\"start\":4970},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8367,\"start\":8363},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8519,\"start\":8515},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8521,\"start\":8519},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8524,\"start\":8521},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8543,\"start\":8540},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8578,\"start\":8574},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8623,\"start\":8619},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8708,\"start\":8704},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8835,\"start\":8831},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8956,\"start\":8952},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9093,\"start\":9089},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9096,\"start\":9093},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9099,\"start\":9096},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9378,\"start\":9374},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9381,\"start\":9378},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9396,\"start\":9392},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9399,\"start\":9396},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9444,\"start\":9440},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9447,\"start\":9444},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9810,\"start\":9806},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9840,\"start\":9836},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10223,\"start\":10220},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10226,\"start\":10223},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10426,\"start\":10422},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10441,\"start\":10437},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10450,\"start\":10447},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10478,\"start\":10474},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10604,\"start\":10601},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10756,\"start\":10752},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10863,\"start\":10860},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10865,\"start\":10863},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10868,\"start\":10865},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11129,\"start\":11125},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11131,\"start\":11129},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11232,\"start\":11228},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11333,\"start\":11329},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11372,\"start\":11368},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11419,\"start\":11415},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12248,\"start\":12244},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12251,\"start\":12248},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12254,\"start\":12251},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12502,\"start\":12498},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12640,\"start\":12637},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12669,\"start\":12665},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15076,\"start\":15073},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15079,\"start\":15076},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17244,\"start\":17240},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":19238,\"start\":19234},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19621,\"start\":19617},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21574,\"start\":21570},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21577,\"start\":21574},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21580,\"start\":21577},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21668,\"start\":21664},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21733,\"start\":21729},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21783,\"start\":21779},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22917,\"start\":22913},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22950,\"start\":22946},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22978,\"start\":22974},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23311,\"start\":23307},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23582,\"start\":23578},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23585,\"start\":23582},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":23588,\"start\":23585},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23894,\"start\":23890},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24122,\"start\":24118},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24145,\"start\":24141},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24419,\"start\":24416},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24933,\"start\":24929},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25989,\"start\":25985},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26037,\"start\":26034},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26039,\"start\":26037},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26042,\"start\":26039},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28142,\"start\":28138},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32861,\"start\":32857},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32864,\"start\":32861},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32867,\"start\":32864},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32988,\"start\":32984},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33041,\"start\":33037},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33126,\"start\":33122},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33129,\"start\":33126},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33132,\"start\":33129},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33171,\"start\":33167},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33868,\"start\":33864},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":34536,\"start\":34532},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34652,\"start\":34648},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35246,\"start\":35242}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36850,\"start\":36452},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36902,\"start\":36851},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37108,\"start\":36903},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37398,\"start\":37109},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37825,\"start\":37399},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39060,\"start\":37826},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39701,\"start\":39061},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39994,\"start\":39702},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":40304,\"start\":39995},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":40570,\"start\":40305},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":41279,\"start\":40571},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":41463,\"start\":41280}]", "paragraph": "[{\"end\":4781,\"start\":2148},{\"end\":5865,\"start\":4783},{\"end\":7737,\"start\":5867},{\"end\":8315,\"start\":7739},{\"end\":10080,\"start\":8341},{\"end\":11033,\"start\":10096},{\"end\":11865,\"start\":11064},{\"end\":11880,\"start\":11867},{\"end\":12391,\"start\":11900},{\"end\":12754,\"start\":12468},{\"end\":12803,\"start\":12798},{\"end\":13103,\"start\":12850},{\"end\":13397,\"start\":13105},{\"end\":13557,\"start\":13399},{\"end\":13607,\"start\":13602},{\"end\":14655,\"start\":13761},{\"end\":15459,\"start\":14657},{\"end\":15787,\"start\":15461},{\"end\":16132,\"start\":15789},{\"end\":16532,\"start\":16163},{\"end\":16838,\"start\":16534},{\"end\":17100,\"start\":16865},{\"end\":17772,\"start\":17128},{\"end\":17954,\"start\":17826},{\"end\":18725,\"start\":18008},{\"end\":19867,\"start\":18727},{\"end\":20106,\"start\":19931},{\"end\":20279,\"start\":20108},{\"end\":21523,\"start\":20281},{\"end\":22778,\"start\":21545},{\"end\":23792,\"start\":22826},{\"end\":25178,\"start\":23819},{\"end\":26175,\"start\":25180},{\"end\":27298,\"start\":26221},{\"end\":27561,\"start\":27319},{\"end\":27894,\"start\":27590},{\"end\":29274,\"start\":27896},{\"end\":30425,\"start\":29276},{\"end\":31499,\"start\":30476},{\"end\":32746,\"start\":31514},{\"end\":34571,\"start\":32748},{\"end\":35039,\"start\":34573},{\"end\":35247,\"start\":35064},{\"end\":35654,\"start\":35257},{\"end\":36073,\"start\":35680},{\"end\":36451,\"start\":36093}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12439,\"start\":12392},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12797,\"start\":12755},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12849,\"start\":12804},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13601,\"start\":13558},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13729,\"start\":13608},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16864,\"start\":16839},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17127,\"start\":17101},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17825,\"start\":17773},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18007,\"start\":17955},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19930,\"start\":19868}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26231,\"start\":26224},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26243,\"start\":26236},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27011,\"start\":27004},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27156,\"start\":27149},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27666,\"start\":27659},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27819,\"start\":27812},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29053,\"start\":29046},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29605,\"start\":29598},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30266,\"start\":30258},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":35091,\"start\":35072},{\"end\":35474,\"start\":35467}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2146,\"start\":2134},{\"attributes\":{\"n\":\"2.1\"},\"end\":8339,\"start\":8318},{\"attributes\":{\"n\":\"2.2\"},\"end\":10094,\"start\":10083},{\"attributes\":{\"n\":\"2.3\"},\"end\":11062,\"start\":11036},{\"attributes\":{\"n\":\"3.1\"},\"end\":11898,\"start\":11883},{\"attributes\":{\"n\":\"3.2\"},\"end\":12466,\"start\":12441},{\"attributes\":{\"n\":\"3.3\"},\"end\":13759,\"start\":13731},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":16161,\"start\":16135},{\"attributes\":{\"n\":\"3.4\"},\"end\":21543,\"start\":21526},{\"attributes\":{\"n\":\"4\"},\"end\":22792,\"start\":22781},{\"attributes\":{\"n\":\"4.1\"},\"end\":22824,\"start\":22795},{\"attributes\":{\"n\":\"4.2\"},\"end\":23817,\"start\":23795},{\"attributes\":{\"n\":\"4.3\"},\"end\":26219,\"start\":26178},{\"attributes\":{\"n\":\"4.4\"},\"end\":27317,\"start\":27301},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":27588,\"start\":27564},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":30452,\"start\":30428},{\"attributes\":{\"n\":\"4.5\"},\"end\":30474,\"start\":30455},{\"attributes\":{\"n\":\"5\"},\"end\":31512,\"start\":31502},{\"end\":35062,\"start\":35042},{\"end\":35255,\"start\":35250},{\"end\":35678,\"start\":35657},{\"end\":36083,\"start\":36076},{\"end\":36091,\"start\":36086},{\"end\":36463,\"start\":36453},{\"end\":36862,\"start\":36852},{\"end\":36909,\"start\":36904},{\"end\":37120,\"start\":37110},{\"end\":37836,\"start\":37827},{\"end\":39071,\"start\":39062},{\"end\":39712,\"start\":39703},{\"end\":40005,\"start\":39996},{\"end\":40315,\"start\":40306},{\"end\":41290,\"start\":41281}]", "table": "[{\"end\":39060,\"start\":38003},{\"end\":39701,\"start\":39237},{\"end\":39994,\"start\":39714},{\"end\":40304,\"start\":40007},{\"end\":40570,\"start\":40317},{\"end\":41279,\"start\":41245}]", "figure_caption": "[{\"end\":36850,\"start\":36465},{\"end\":36902,\"start\":36864},{\"end\":37108,\"start\":36911},{\"end\":37398,\"start\":37122},{\"end\":37825,\"start\":37401},{\"end\":38003,\"start\":37838},{\"end\":39237,\"start\":39073},{\"end\":41245,\"start\":40573},{\"end\":41463,\"start\":41292}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3390,\"start\":3382},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3442,\"start\":3430},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3621,\"start\":3609},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4124,\"start\":4115},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5101,\"start\":5093},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6198,\"start\":6190},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7036,\"start\":7028},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13801,\"start\":13793},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14118,\"start\":14110},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16341,\"start\":16333},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30487,\"start\":30479},{\"end\":32274,\"start\":32266},{\"end\":32825,\"start\":32817},{\"end\":35722,\"start\":35714},{\"end\":36155,\"start\":36147}]", "bib_author_first_name": "[{\"end\":41700,\"start\":41695},{\"end\":41704,\"start\":41701},{\"end\":41714,\"start\":41709},{\"end\":41719,\"start\":41715},{\"end\":41735,\"start\":41727},{\"end\":41737,\"start\":41736},{\"end\":41915,\"start\":41913},{\"end\":41926,\"start\":41925},{\"end\":41941,\"start\":41936},{\"end\":42214,\"start\":42207},{\"end\":42232,\"start\":42223},{\"end\":42247,\"start\":42240},{\"end\":42265,\"start\":42258},{\"end\":42284,\"start\":42275},{\"end\":42301,\"start\":42295},{\"end\":42717,\"start\":42706},{\"end\":42730,\"start\":42724},{\"end\":42750,\"start\":42743},{\"end\":42766,\"start\":42761},{\"end\":42779,\"start\":42775},{\"end\":42781,\"start\":42780},{\"end\":43200,\"start\":43195},{\"end\":43213,\"start\":43208},{\"end\":43224,\"start\":43220},{\"end\":43238,\"start\":43234},{\"end\":43436,\"start\":43431},{\"end\":43453,\"start\":43445},{\"end\":43467,\"start\":43461},{\"end\":43481,\"start\":43473},{\"end\":43491,\"start\":43482},{\"end\":43855,\"start\":43848},{\"end\":43866,\"start\":43862},{\"end\":43868,\"start\":43867},{\"end\":44001,\"start\":43995},{\"end\":44020,\"start\":44015},{\"end\":44037,\"start\":44028},{\"end\":44054,\"start\":44050},{\"end\":44075,\"start\":44068},{\"end\":44088,\"start\":44082},{\"end\":44109,\"start\":44102},{\"end\":44128,\"start\":44120},{\"end\":44144,\"start\":44139},{\"end\":44161,\"start\":44154},{\"end\":44618,\"start\":44609},{\"end\":44633,\"start\":44628},{\"end\":44649,\"start\":44641},{\"end\":44665,\"start\":44659},{\"end\":44682,\"start\":44676},{\"end\":45133,\"start\":45129},{\"end\":45149,\"start\":45146},{\"end\":45161,\"start\":45160},{\"end\":45163,\"start\":45162},{\"end\":45181,\"start\":45177},{\"end\":45198,\"start\":45192},{\"end\":45510,\"start\":45505},{\"end\":45534,\"start\":45530},{\"end\":45553,\"start\":45545},{\"end\":45858,\"start\":45851},{\"end\":45870,\"start\":45863},{\"end\":45886,\"start\":45878},{\"end\":45896,\"start\":45892},{\"end\":46345,\"start\":46342},{\"end\":46357,\"start\":46350},{\"end\":46372,\"start\":46364},{\"end\":46384,\"start\":46380},{\"end\":46395,\"start\":46389},{\"end\":46401,\"start\":46400},{\"end\":46403,\"start\":46402},{\"end\":46811,\"start\":46805},{\"end\":46827,\"start\":46819},{\"end\":46840,\"start\":46834},{\"end\":46853,\"start\":46848},{\"end\":46868,\"start\":46861},{\"end\":46879,\"start\":46874},{\"end\":47338,\"start\":47332},{\"end\":47354,\"start\":47346},{\"end\":47366,\"start\":47361},{\"end\":47378,\"start\":47373},{\"end\":47392,\"start\":47384},{\"end\":47898,\"start\":47890},{\"end\":47912,\"start\":47905},{\"end\":47920,\"start\":47918},{\"end\":47934,\"start\":47927},{\"end\":47952,\"start\":47943},{\"end\":47954,\"start\":47953},{\"end\":48218,\"start\":48210},{\"end\":48231,\"start\":48224},{\"end\":48244,\"start\":48239},{\"end\":48260,\"start\":48255},{\"end\":48273,\"start\":48267},{\"end\":48286,\"start\":48282},{\"end\":48301,\"start\":48296},{\"end\":48320,\"start\":48310},{\"end\":48704,\"start\":48697},{\"end\":48717,\"start\":48710},{\"end\":49036,\"start\":49028},{\"end\":49047,\"start\":49043},{\"end\":49065,\"start\":49059},{\"end\":49437,\"start\":49433},{\"end\":49455,\"start\":49450},{\"end\":49738,\"start\":49732},{\"end\":49756,\"start\":49750},{\"end\":49775,\"start\":49764},{\"end\":50133,\"start\":50127},{\"end\":50145,\"start\":50139},{\"end\":50157,\"start\":50152},{\"end\":50389,\"start\":50385},{\"end\":50404,\"start\":50398},{\"end\":50758,\"start\":50754},{\"end\":50773,\"start\":50767},{\"end\":50788,\"start\":50783},{\"end\":50806,\"start\":50800},{\"end\":50819,\"start\":50815},{\"end\":50838,\"start\":50829},{\"end\":50849,\"start\":50843},{\"end\":51165,\"start\":51161},{\"end\":51182,\"start\":51179},{\"end\":51192,\"start\":51189},{\"end\":51205,\"start\":51197},{\"end\":51221,\"start\":51214},{\"end\":51236,\"start\":51232},{\"end\":51248,\"start\":51241},{\"end\":51262,\"start\":51256},{\"end\":51279,\"start\":51273},{\"end\":51295,\"start\":51288},{\"end\":51704,\"start\":51696},{\"end\":51718,\"start\":51713},{\"end\":51731,\"start\":51727},{\"end\":52015,\"start\":52012},{\"end\":52027,\"start\":52021},{\"end\":52040,\"start\":52033},{\"end\":52050,\"start\":52046},{\"end\":52062,\"start\":52055},{\"end\":52077,\"start\":52070},{\"end\":52089,\"start\":52082},{\"end\":52369,\"start\":52365},{\"end\":52382,\"start\":52377},{\"end\":52399,\"start\":52392},{\"end\":52696,\"start\":52691},{\"end\":52710,\"start\":52703},{\"end\":52719,\"start\":52717},{\"end\":52730,\"start\":52727},{\"end\":52739,\"start\":52738},{\"end\":52741,\"start\":52740},{\"end\":52757,\"start\":52750},{\"end\":52759,\"start\":52758},{\"end\":53251,\"start\":53250},{\"end\":53270,\"start\":53269},{\"end\":53611,\"start\":53607},{\"end\":53629,\"start\":53621},{\"end\":53644,\"start\":53636},{\"end\":53661,\"start\":53652},{\"end\":53678,\"start\":53669},{\"end\":53698,\"start\":53693},{\"end\":53989,\"start\":53983},{\"end\":54003,\"start\":53999},{\"end\":54017,\"start\":54013},{\"end\":54031,\"start\":54026},{\"end\":54048,\"start\":54043},{\"end\":54061,\"start\":54056},{\"end\":54063,\"start\":54062},{\"end\":54077,\"start\":54071},{\"end\":54091,\"start\":54086},{\"end\":54544,\"start\":54539},{\"end\":54564,\"start\":54557},{\"end\":54582,\"start\":54575},{\"end\":54600,\"start\":54593},{\"end\":54615,\"start\":54609},{\"end\":54627,\"start\":54621},{\"end\":54965,\"start\":54958},{\"end\":54978,\"start\":54972},{\"end\":54991,\"start\":54986},{\"end\":55002,\"start\":54996},{\"end\":55343,\"start\":55337},{\"end\":55353,\"start\":55350},{\"end\":55372,\"start\":55364},{\"end\":55384,\"start\":55378},{\"end\":55397,\"start\":55391},{\"end\":55809,\"start\":55801},{\"end\":55820,\"start\":55816},{\"end\":55838,\"start\":55831},{\"end\":55853,\"start\":55846},{\"end\":56260,\"start\":56252},{\"end\":56272,\"start\":56267},{\"end\":56286,\"start\":56280},{\"end\":56288,\"start\":56287},{\"end\":56777,\"start\":56770},{\"end\":56789,\"start\":56783},{\"end\":56803,\"start\":56796},{\"end\":56820,\"start\":56811},{\"end\":56831,\"start\":56828},{\"end\":56847,\"start\":56838},{\"end\":57404,\"start\":57397},{\"end\":57416,\"start\":57410},{\"end\":57430,\"start\":57423},{\"end\":57441,\"start\":57436},{\"end\":57453,\"start\":57447},{\"end\":57468,\"start\":57460},{\"end\":57984,\"start\":57977},{\"end\":57996,\"start\":57989},{\"end\":58010,\"start\":58004},{\"end\":58022,\"start\":58016},{\"end\":58031,\"start\":58029},{\"end\":58045,\"start\":58038},{\"end\":58058,\"start\":58053},{\"end\":58544,\"start\":58540},{\"end\":58556,\"start\":58551},{\"end\":58567,\"start\":58562},{\"end\":58579,\"start\":58572},{\"end\":58593,\"start\":58586},{\"end\":58897,\"start\":58893},{\"end\":58907,\"start\":58904},{\"end\":58917,\"start\":58914},{\"end\":58930,\"start\":58922},{\"end\":58940,\"start\":58936},{\"end\":59224,\"start\":59221},{\"end\":59240,\"start\":59232},{\"end\":59251,\"start\":59246},{\"end\":59266,\"start\":59257},{\"end\":59279,\"start\":59272},{\"end\":59287,\"start\":59284},{\"end\":59803,\"start\":59800},{\"end\":59819,\"start\":59811},{\"end\":59830,\"start\":59825},{\"end\":59839,\"start\":59836},{\"end\":59852,\"start\":59845},{\"end\":60352,\"start\":60346},{\"end\":60363,\"start\":60360},{\"end\":60381,\"start\":60374},{\"end\":60393,\"start\":60387},{\"end\":60402,\"start\":60399},{\"end\":60878,\"start\":60871},{\"end\":60893,\"start\":60886},{\"end\":60901,\"start\":60899},{\"end\":60916,\"start\":60908},{\"end\":61190,\"start\":61180},{\"end\":61205,\"start\":61197},{\"end\":61219,\"start\":61211},{\"end\":61232,\"start\":61224},{\"end\":61244,\"start\":61239},{\"end\":61635,\"start\":61630},{\"end\":61646,\"start\":61642},{\"end\":61659,\"start\":61653},{\"end\":61671,\"start\":61666},{\"end\":61685,\"start\":61680},{\"end\":61703,\"start\":61696},{\"end\":62155,\"start\":62148},{\"end\":62169,\"start\":62162},{\"end\":62189,\"start\":62182},{\"end\":62203,\"start\":62197},{\"end\":62217,\"start\":62211},{\"end\":62219,\"start\":62218},{\"end\":62706,\"start\":62699},{\"end\":62719,\"start\":62712},{\"end\":62733,\"start\":62726},{\"end\":62747,\"start\":62741},{\"end\":62749,\"start\":62748},{\"end\":63197,\"start\":63191},{\"end\":63209,\"start\":63203},{\"end\":63219,\"start\":63214},{\"end\":63227,\"start\":63224},{\"end\":63240,\"start\":63232},{\"end\":63253,\"start\":63247}]", "bib_author_last_name": "[{\"end\":41707,\"start\":41705},{\"end\":41725,\"start\":41720},{\"end\":41744,\"start\":41738},{\"end\":41923,\"start\":41916},{\"end\":41934,\"start\":41927},{\"end\":41948,\"start\":41942},{\"end\":41955,\"start\":41950},{\"end\":41967,\"start\":41957},{\"end\":42221,\"start\":42215},{\"end\":42238,\"start\":42233},{\"end\":42256,\"start\":42248},{\"end\":42273,\"start\":42266},{\"end\":42293,\"start\":42285},{\"end\":42311,\"start\":42302},{\"end\":42722,\"start\":42718},{\"end\":42741,\"start\":42731},{\"end\":42759,\"start\":42751},{\"end\":42773,\"start\":42767},{\"end\":42788,\"start\":42782},{\"end\":43206,\"start\":43201},{\"end\":43218,\"start\":43214},{\"end\":43232,\"start\":43225},{\"end\":43248,\"start\":43239},{\"end\":43443,\"start\":43437},{\"end\":43459,\"start\":43454},{\"end\":43471,\"start\":43468},{\"end\":43496,\"start\":43492},{\"end\":43860,\"start\":43856},{\"end\":43873,\"start\":43869},{\"end\":44013,\"start\":44002},{\"end\":44026,\"start\":44021},{\"end\":44048,\"start\":44038},{\"end\":44066,\"start\":44055},{\"end\":44080,\"start\":44076},{\"end\":44100,\"start\":44089},{\"end\":44118,\"start\":44110},{\"end\":44137,\"start\":44129},{\"end\":44152,\"start\":44145},{\"end\":44167,\"start\":44162},{\"end\":44626,\"start\":44619},{\"end\":44639,\"start\":44634},{\"end\":44657,\"start\":44650},{\"end\":44674,\"start\":44666},{\"end\":44692,\"start\":44683},{\"end\":45144,\"start\":45134},{\"end\":45158,\"start\":45150},{\"end\":45175,\"start\":45164},{\"end\":45190,\"start\":45182},{\"end\":45203,\"start\":45199},{\"end\":45214,\"start\":45205},{\"end\":45528,\"start\":45511},{\"end\":45543,\"start\":45535},{\"end\":45562,\"start\":45554},{\"end\":45569,\"start\":45564},{\"end\":45861,\"start\":45859},{\"end\":45876,\"start\":45871},{\"end\":45890,\"start\":45887},{\"end\":45900,\"start\":45897},{\"end\":46348,\"start\":46346},{\"end\":46362,\"start\":46358},{\"end\":46378,\"start\":46373},{\"end\":46387,\"start\":46385},{\"end\":46398,\"start\":46396},{\"end\":46408,\"start\":46404},{\"end\":46415,\"start\":46410},{\"end\":46817,\"start\":46812},{\"end\":46832,\"start\":46828},{\"end\":46846,\"start\":46841},{\"end\":46859,\"start\":46854},{\"end\":46872,\"start\":46869},{\"end\":46883,\"start\":46880},{\"end\":47344,\"start\":47339},{\"end\":47359,\"start\":47355},{\"end\":47371,\"start\":47367},{\"end\":47382,\"start\":47379},{\"end\":47397,\"start\":47393},{\"end\":47903,\"start\":47899},{\"end\":47916,\"start\":47913},{\"end\":47925,\"start\":47921},{\"end\":47941,\"start\":47935},{\"end\":47964,\"start\":47955},{\"end\":48222,\"start\":48219},{\"end\":48237,\"start\":48232},{\"end\":48253,\"start\":48245},{\"end\":48265,\"start\":48261},{\"end\":48280,\"start\":48274},{\"end\":48294,\"start\":48287},{\"end\":48308,\"start\":48302},{\"end\":48328,\"start\":48321},{\"end\":48708,\"start\":48705},{\"end\":48723,\"start\":48718},{\"end\":49041,\"start\":49037},{\"end\":49057,\"start\":49048},{\"end\":49073,\"start\":49066},{\"end\":49448,\"start\":49438},{\"end\":49462,\"start\":49456},{\"end\":49748,\"start\":49739},{\"end\":49762,\"start\":49757},{\"end\":49782,\"start\":49776},{\"end\":50137,\"start\":50134},{\"end\":50150,\"start\":50146},{\"end\":50161,\"start\":50158},{\"end\":50396,\"start\":50390},{\"end\":50414,\"start\":50405},{\"end\":50765,\"start\":50759},{\"end\":50781,\"start\":50774},{\"end\":50798,\"start\":50789},{\"end\":50813,\"start\":50807},{\"end\":50827,\"start\":50820},{\"end\":50841,\"start\":50839},{\"end\":50854,\"start\":50850},{\"end\":51177,\"start\":51166},{\"end\":51187,\"start\":51183},{\"end\":51195,\"start\":51193},{\"end\":51212,\"start\":51206},{\"end\":51230,\"start\":51222},{\"end\":51239,\"start\":51237},{\"end\":51254,\"start\":51249},{\"end\":51271,\"start\":51263},{\"end\":51286,\"start\":51280},{\"end\":51305,\"start\":51296},{\"end\":51711,\"start\":51705},{\"end\":51725,\"start\":51719},{\"end\":51735,\"start\":51732},{\"end\":52019,\"start\":52016},{\"end\":52031,\"start\":52028},{\"end\":52044,\"start\":52041},{\"end\":52053,\"start\":52051},{\"end\":52068,\"start\":52063},{\"end\":52080,\"start\":52078},{\"end\":52094,\"start\":52090},{\"end\":52375,\"start\":52370},{\"end\":52390,\"start\":52383},{\"end\":52405,\"start\":52400},{\"end\":52701,\"start\":52697},{\"end\":52715,\"start\":52711},{\"end\":52725,\"start\":52720},{\"end\":52736,\"start\":52731},{\"end\":52748,\"start\":52742},{\"end\":52764,\"start\":52760},{\"end\":52776,\"start\":52766},{\"end\":53248,\"start\":53242},{\"end\":53256,\"start\":53252},{\"end\":53261,\"start\":53258},{\"end\":53267,\"start\":53263},{\"end\":53273,\"start\":53271},{\"end\":53278,\"start\":53275},{\"end\":53619,\"start\":53612},{\"end\":53634,\"start\":53630},{\"end\":53650,\"start\":53645},{\"end\":53667,\"start\":53662},{\"end\":53691,\"start\":53679},{\"end\":53704,\"start\":53699},{\"end\":53997,\"start\":53990},{\"end\":54011,\"start\":54004},{\"end\":54024,\"start\":54018},{\"end\":54041,\"start\":54032},{\"end\":54054,\"start\":54049},{\"end\":54069,\"start\":54064},{\"end\":54084,\"start\":54078},{\"end\":54102,\"start\":54092},{\"end\":54555,\"start\":54545},{\"end\":54573,\"start\":54565},{\"end\":54591,\"start\":54583},{\"end\":54607,\"start\":54601},{\"end\":54619,\"start\":54616},{\"end\":54634,\"start\":54628},{\"end\":54970,\"start\":54966},{\"end\":54984,\"start\":54979},{\"end\":54994,\"start\":54992},{\"end\":55007,\"start\":55003},{\"end\":55348,\"start\":55344},{\"end\":55362,\"start\":55354},{\"end\":55376,\"start\":55373},{\"end\":55389,\"start\":55385},{\"end\":55402,\"start\":55398},{\"end\":55814,\"start\":55810},{\"end\":55829,\"start\":55821},{\"end\":55844,\"start\":55839},{\"end\":55856,\"start\":55854},{\"end\":56265,\"start\":56261},{\"end\":56278,\"start\":56273},{\"end\":56294,\"start\":56289},{\"end\":56781,\"start\":56778},{\"end\":56794,\"start\":56790},{\"end\":56809,\"start\":56804},{\"end\":56826,\"start\":56821},{\"end\":56836,\"start\":56832},{\"end\":56851,\"start\":56848},{\"end\":57408,\"start\":57405},{\"end\":57421,\"start\":57417},{\"end\":57434,\"start\":57431},{\"end\":57445,\"start\":57442},{\"end\":57458,\"start\":57454},{\"end\":57474,\"start\":57469},{\"end\":57987,\"start\":57985},{\"end\":58002,\"start\":57997},{\"end\":58014,\"start\":58011},{\"end\":58027,\"start\":58023},{\"end\":58036,\"start\":58032},{\"end\":58051,\"start\":58046},{\"end\":58062,\"start\":58059},{\"end\":58549,\"start\":58545},{\"end\":58560,\"start\":58557},{\"end\":58570,\"start\":58568},{\"end\":58584,\"start\":58580},{\"end\":58596,\"start\":58594},{\"end\":58902,\"start\":58898},{\"end\":58912,\"start\":58908},{\"end\":58920,\"start\":58918},{\"end\":58934,\"start\":58931},{\"end\":58944,\"start\":58941},{\"end\":59230,\"start\":59225},{\"end\":59244,\"start\":59241},{\"end\":59255,\"start\":59252},{\"end\":59270,\"start\":59267},{\"end\":59282,\"start\":59280},{\"end\":59291,\"start\":59288},{\"end\":59809,\"start\":59804},{\"end\":59823,\"start\":59820},{\"end\":59834,\"start\":59831},{\"end\":59843,\"start\":59840},{\"end\":59857,\"start\":59853},{\"end\":60358,\"start\":60353},{\"end\":60372,\"start\":60364},{\"end\":60385,\"start\":60382},{\"end\":60397,\"start\":60394},{\"end\":60407,\"start\":60403},{\"end\":60884,\"start\":60879},{\"end\":60897,\"start\":60894},{\"end\":60906,\"start\":60902},{\"end\":60922,\"start\":60917},{\"end\":61195,\"start\":61191},{\"end\":61209,\"start\":61206},{\"end\":61222,\"start\":61220},{\"end\":61237,\"start\":61233},{\"end\":61248,\"start\":61245},{\"end\":61640,\"start\":61636},{\"end\":61651,\"start\":61647},{\"end\":61664,\"start\":61660},{\"end\":61678,\"start\":61672},{\"end\":61694,\"start\":61686},{\"end\":61712,\"start\":61704},{\"end\":62160,\"start\":62156},{\"end\":62180,\"start\":62170},{\"end\":62195,\"start\":62190},{\"end\":62209,\"start\":62204},{\"end\":62225,\"start\":62220},{\"end\":62710,\"start\":62707},{\"end\":62724,\"start\":62720},{\"end\":62739,\"start\":62734},{\"end\":62755,\"start\":62750},{\"end\":63201,\"start\":63198},{\"end\":63212,\"start\":63210},{\"end\":63222,\"start\":63220},{\"end\":63230,\"start\":63228},{\"end\":63245,\"start\":63241},{\"end\":63257,\"start\":63254}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b0\"},\"end\":41911,\"start\":41693},{\"attributes\":{\"doi\":\"arXiv:2004.05150\",\"id\":\"b1\"},\"end\":42158,\"start\":41913},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218889832},\"end\":42591,\"start\":42160},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3429309},\"end\":43141,\"start\":42593},{\"attributes\":{\"doi\":\"arXiv:1904.10509\",\"id\":\"b4\"},\"end\":43429,\"start\":43143},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b5\"},\"end\":43790,\"start\":43431},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52284769},\"end\":43993,\"start\":43792},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b7\"},\"end\":44570,\"start\":43995},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":118686970},\"end\":45077,\"start\":44572},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4246903},\"end\":45462,\"start\":45079},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9272368},\"end\":45803,\"start\":45464},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":46267,\"start\":45805},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":54071730},\"end\":46747,\"start\":46269},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53846561},\"end\":47250,\"start\":46749},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":51690586},\"end\":47797,\"start\":47252},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":226227144},\"end\":48165,\"start\":47799},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14113767},\"end\":48599,\"start\":48167},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":220496413},\"end\":48970,\"start\":48601},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1629541},\"end\":49429,\"start\":48972},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b19\"},\"end\":49644,\"start\":49431},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206429151},\"end\":50073,\"start\":49646},{\"attributes\":{\"doi\":\"arXiv:2104.01538\",\"id\":\"b21\"},\"end\":50325,\"start\":50075},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":203593252},\"end\":50733,\"start\":50327},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3353110},\"end\":51108,\"start\":50735},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2930547},\"end\":51620,\"start\":51110},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":23237949},\"end\":51944,\"start\":51622},{\"attributes\":{\"doi\":\"arXiv:2102.12871\",\"id\":\"b26\"},\"end\":52318,\"start\":51946},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":309759},\"end\":52628,\"start\":52320},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4412459},\"end\":53173,\"start\":52630},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":220961511},\"end\":53528,\"start\":53175},{\"attributes\":{\"doi\":\"arXiv:2012.12877\",\"id\":\"b30\"},\"end\":53954,\"start\":53530},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13756489},\"end\":54537,\"start\":53956},{\"attributes\":{\"doi\":\"arXiv:1710.10903\",\"id\":\"b32\"},\"end\":54857,\"start\":54539},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":221757772},\"end\":55265,\"start\":54859},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":201070109},\"end\":55772,\"start\":55267},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4852647},\"end\":56191,\"start\":55774},{\"attributes\":{\"id\":\"b36\"},\"end\":56662,\"start\":56193},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6793190},\"end\":57291,\"start\":56664},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":44097132},\"end\":57910,\"start\":57293},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":215754147},\"end\":58477,\"start\":57912},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":221089994},\"end\":58842,\"start\":58479},{\"attributes\":{\"doi\":\"arXiv:2103.15402\",\"id\":\"b41\"},\"end\":59120,\"start\":58844},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":204973537},\"end\":59695,\"start\":59122},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":70349904},\"end\":60282,\"start\":59697},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":219617717},\"end\":60797,\"start\":60284},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":53047277},\"end\":61147,\"start\":60799},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":5299559},\"end\":61590,\"start\":61149},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":5636055},\"end\":62083,\"start\":61592},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2812654},\"end\":62616,\"start\":62085},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":195944196},\"end\":63115,\"start\":62618},{\"attributes\":{\"doi\":\"arXiv:2010.04159\",\"id\":\"b50\"},\"end\":63473,\"start\":63117}]", "bib_title": "[{\"end\":42205,\"start\":42160},{\"end\":42704,\"start\":42593},{\"end\":43846,\"start\":43792},{\"end\":44607,\"start\":44572},{\"end\":45127,\"start\":45079},{\"end\":45503,\"start\":45464},{\"end\":45849,\"start\":45805},{\"end\":46340,\"start\":46269},{\"end\":46803,\"start\":46749},{\"end\":47330,\"start\":47252},{\"end\":47888,\"start\":47799},{\"end\":48208,\"start\":48167},{\"end\":48695,\"start\":48601},{\"end\":49026,\"start\":48972},{\"end\":49730,\"start\":49646},{\"end\":50383,\"start\":50327},{\"end\":50752,\"start\":50735},{\"end\":51159,\"start\":51110},{\"end\":51694,\"start\":51622},{\"end\":52363,\"start\":52320},{\"end\":52689,\"start\":52630},{\"end\":53240,\"start\":53175},{\"end\":53981,\"start\":53956},{\"end\":54956,\"start\":54859},{\"end\":55335,\"start\":55267},{\"end\":55799,\"start\":55774},{\"end\":56250,\"start\":56193},{\"end\":56768,\"start\":56664},{\"end\":57395,\"start\":57293},{\"end\":57975,\"start\":57912},{\"end\":58538,\"start\":58479},{\"end\":59219,\"start\":59122},{\"end\":59798,\"start\":59697},{\"end\":60344,\"start\":60284},{\"end\":60869,\"start\":60799},{\"end\":61178,\"start\":61149},{\"end\":61628,\"start\":61592},{\"end\":62146,\"start\":62085},{\"end\":62697,\"start\":62618}]", "bib_author": "[{\"end\":41709,\"start\":41695},{\"end\":41727,\"start\":41709},{\"end\":41746,\"start\":41727},{\"end\":41925,\"start\":41913},{\"end\":41936,\"start\":41925},{\"end\":41950,\"start\":41936},{\"end\":41957,\"start\":41950},{\"end\":41969,\"start\":41957},{\"end\":42223,\"start\":42207},{\"end\":42240,\"start\":42223},{\"end\":42258,\"start\":42240},{\"end\":42275,\"start\":42258},{\"end\":42295,\"start\":42275},{\"end\":42313,\"start\":42295},{\"end\":42724,\"start\":42706},{\"end\":42743,\"start\":42724},{\"end\":42761,\"start\":42743},{\"end\":42775,\"start\":42761},{\"end\":42790,\"start\":42775},{\"end\":43208,\"start\":43195},{\"end\":43220,\"start\":43208},{\"end\":43234,\"start\":43220},{\"end\":43250,\"start\":43234},{\"end\":43445,\"start\":43431},{\"end\":43461,\"start\":43445},{\"end\":43473,\"start\":43461},{\"end\":43498,\"start\":43473},{\"end\":43862,\"start\":43848},{\"end\":43875,\"start\":43862},{\"end\":44015,\"start\":43995},{\"end\":44028,\"start\":44015},{\"end\":44050,\"start\":44028},{\"end\":44068,\"start\":44050},{\"end\":44082,\"start\":44068},{\"end\":44102,\"start\":44082},{\"end\":44120,\"start\":44102},{\"end\":44139,\"start\":44120},{\"end\":44154,\"start\":44139},{\"end\":44169,\"start\":44154},{\"end\":44628,\"start\":44609},{\"end\":44641,\"start\":44628},{\"end\":44659,\"start\":44641},{\"end\":44676,\"start\":44659},{\"end\":44694,\"start\":44676},{\"end\":45146,\"start\":45129},{\"end\":45160,\"start\":45146},{\"end\":45177,\"start\":45160},{\"end\":45192,\"start\":45177},{\"end\":45205,\"start\":45192},{\"end\":45216,\"start\":45205},{\"end\":45530,\"start\":45505},{\"end\":45545,\"start\":45530},{\"end\":45564,\"start\":45545},{\"end\":45571,\"start\":45564},{\"end\":45863,\"start\":45851},{\"end\":45878,\"start\":45863},{\"end\":45892,\"start\":45878},{\"end\":45902,\"start\":45892},{\"end\":46350,\"start\":46342},{\"end\":46364,\"start\":46350},{\"end\":46380,\"start\":46364},{\"end\":46389,\"start\":46380},{\"end\":46400,\"start\":46389},{\"end\":46410,\"start\":46400},{\"end\":46417,\"start\":46410},{\"end\":46819,\"start\":46805},{\"end\":46834,\"start\":46819},{\"end\":46848,\"start\":46834},{\"end\":46861,\"start\":46848},{\"end\":46874,\"start\":46861},{\"end\":46885,\"start\":46874},{\"end\":47346,\"start\":47332},{\"end\":47361,\"start\":47346},{\"end\":47373,\"start\":47361},{\"end\":47384,\"start\":47373},{\"end\":47399,\"start\":47384},{\"end\":47905,\"start\":47890},{\"end\":47918,\"start\":47905},{\"end\":47927,\"start\":47918},{\"end\":47943,\"start\":47927},{\"end\":47966,\"start\":47943},{\"end\":48224,\"start\":48210},{\"end\":48239,\"start\":48224},{\"end\":48255,\"start\":48239},{\"end\":48267,\"start\":48255},{\"end\":48282,\"start\":48267},{\"end\":48296,\"start\":48282},{\"end\":48310,\"start\":48296},{\"end\":48330,\"start\":48310},{\"end\":48710,\"start\":48697},{\"end\":48725,\"start\":48710},{\"end\":49043,\"start\":49028},{\"end\":49059,\"start\":49043},{\"end\":49075,\"start\":49059},{\"end\":49450,\"start\":49433},{\"end\":49464,\"start\":49450},{\"end\":49750,\"start\":49732},{\"end\":49764,\"start\":49750},{\"end\":49784,\"start\":49764},{\"end\":50139,\"start\":50127},{\"end\":50152,\"start\":50139},{\"end\":50163,\"start\":50152},{\"end\":50398,\"start\":50385},{\"end\":50416,\"start\":50398},{\"end\":50767,\"start\":50754},{\"end\":50783,\"start\":50767},{\"end\":50800,\"start\":50783},{\"end\":50815,\"start\":50800},{\"end\":50829,\"start\":50815},{\"end\":50843,\"start\":50829},{\"end\":50856,\"start\":50843},{\"end\":51179,\"start\":51161},{\"end\":51189,\"start\":51179},{\"end\":51197,\"start\":51189},{\"end\":51214,\"start\":51197},{\"end\":51232,\"start\":51214},{\"end\":51241,\"start\":51232},{\"end\":51256,\"start\":51241},{\"end\":51273,\"start\":51256},{\"end\":51288,\"start\":51273},{\"end\":51307,\"start\":51288},{\"end\":51713,\"start\":51696},{\"end\":51727,\"start\":51713},{\"end\":51737,\"start\":51727},{\"end\":52021,\"start\":52012},{\"end\":52033,\"start\":52021},{\"end\":52046,\"start\":52033},{\"end\":52055,\"start\":52046},{\"end\":52070,\"start\":52055},{\"end\":52082,\"start\":52070},{\"end\":52096,\"start\":52082},{\"end\":52377,\"start\":52365},{\"end\":52392,\"start\":52377},{\"end\":52407,\"start\":52392},{\"end\":52703,\"start\":52691},{\"end\":52717,\"start\":52703},{\"end\":52727,\"start\":52717},{\"end\":52738,\"start\":52727},{\"end\":52750,\"start\":52738},{\"end\":52766,\"start\":52750},{\"end\":52778,\"start\":52766},{\"end\":53250,\"start\":53242},{\"end\":53258,\"start\":53250},{\"end\":53263,\"start\":53258},{\"end\":53269,\"start\":53263},{\"end\":53275,\"start\":53269},{\"end\":53280,\"start\":53275},{\"end\":53621,\"start\":53607},{\"end\":53636,\"start\":53621},{\"end\":53652,\"start\":53636},{\"end\":53669,\"start\":53652},{\"end\":53693,\"start\":53669},{\"end\":53706,\"start\":53693},{\"end\":53999,\"start\":53983},{\"end\":54013,\"start\":53999},{\"end\":54026,\"start\":54013},{\"end\":54043,\"start\":54026},{\"end\":54056,\"start\":54043},{\"end\":54071,\"start\":54056},{\"end\":54086,\"start\":54071},{\"end\":54104,\"start\":54086},{\"end\":54557,\"start\":54539},{\"end\":54575,\"start\":54557},{\"end\":54593,\"start\":54575},{\"end\":54609,\"start\":54593},{\"end\":54621,\"start\":54609},{\"end\":54636,\"start\":54621},{\"end\":54972,\"start\":54958},{\"end\":54986,\"start\":54972},{\"end\":54996,\"start\":54986},{\"end\":55009,\"start\":54996},{\"end\":55350,\"start\":55337},{\"end\":55364,\"start\":55350},{\"end\":55378,\"start\":55364},{\"end\":55391,\"start\":55378},{\"end\":55404,\"start\":55391},{\"end\":55816,\"start\":55801},{\"end\":55831,\"start\":55816},{\"end\":55846,\"start\":55831},{\"end\":55858,\"start\":55846},{\"end\":56267,\"start\":56252},{\"end\":56280,\"start\":56267},{\"end\":56296,\"start\":56280},{\"end\":56783,\"start\":56770},{\"end\":56796,\"start\":56783},{\"end\":56811,\"start\":56796},{\"end\":56828,\"start\":56811},{\"end\":56838,\"start\":56828},{\"end\":56853,\"start\":56838},{\"end\":57410,\"start\":57397},{\"end\":57423,\"start\":57410},{\"end\":57436,\"start\":57423},{\"end\":57447,\"start\":57436},{\"end\":57460,\"start\":57447},{\"end\":57476,\"start\":57460},{\"end\":57989,\"start\":57977},{\"end\":58004,\"start\":57989},{\"end\":58016,\"start\":58004},{\"end\":58029,\"start\":58016},{\"end\":58038,\"start\":58029},{\"end\":58053,\"start\":58038},{\"end\":58064,\"start\":58053},{\"end\":58551,\"start\":58540},{\"end\":58562,\"start\":58551},{\"end\":58572,\"start\":58562},{\"end\":58586,\"start\":58572},{\"end\":58598,\"start\":58586},{\"end\":58904,\"start\":58893},{\"end\":58914,\"start\":58904},{\"end\":58922,\"start\":58914},{\"end\":58936,\"start\":58922},{\"end\":58946,\"start\":58936},{\"end\":59232,\"start\":59221},{\"end\":59246,\"start\":59232},{\"end\":59257,\"start\":59246},{\"end\":59272,\"start\":59257},{\"end\":59284,\"start\":59272},{\"end\":59293,\"start\":59284},{\"end\":59811,\"start\":59800},{\"end\":59825,\"start\":59811},{\"end\":59836,\"start\":59825},{\"end\":59845,\"start\":59836},{\"end\":59859,\"start\":59845},{\"end\":60360,\"start\":60346},{\"end\":60374,\"start\":60360},{\"end\":60387,\"start\":60374},{\"end\":60399,\"start\":60387},{\"end\":60409,\"start\":60399},{\"end\":60886,\"start\":60871},{\"end\":60899,\"start\":60886},{\"end\":60908,\"start\":60899},{\"end\":60924,\"start\":60908},{\"end\":61197,\"start\":61180},{\"end\":61211,\"start\":61197},{\"end\":61224,\"start\":61211},{\"end\":61239,\"start\":61224},{\"end\":61250,\"start\":61239},{\"end\":61642,\"start\":61630},{\"end\":61653,\"start\":61642},{\"end\":61666,\"start\":61653},{\"end\":61680,\"start\":61666},{\"end\":61696,\"start\":61680},{\"end\":61714,\"start\":61696},{\"end\":62162,\"start\":62148},{\"end\":62182,\"start\":62162},{\"end\":62197,\"start\":62182},{\"end\":62211,\"start\":62197},{\"end\":62227,\"start\":62211},{\"end\":62712,\"start\":62699},{\"end\":62726,\"start\":62712},{\"end\":62741,\"start\":62726},{\"end\":62757,\"start\":62741},{\"end\":63203,\"start\":63191},{\"end\":63214,\"start\":63203},{\"end\":63224,\"start\":63214},{\"end\":63232,\"start\":63224},{\"end\":63247,\"start\":63232},{\"end\":63259,\"start\":63247}]", "bib_venue": "[{\"end\":42014,\"start\":41985},{\"end\":42351,\"start\":42313},{\"end\":42852,\"start\":42790},{\"end\":43193,\"start\":43143},{\"end\":43588,\"start\":43514},{\"end\":43879,\"start\":43875},{\"end\":44259,\"start\":44185},{\"end\":44775,\"start\":44694},{\"end\":45256,\"start\":45216},{\"end\":45609,\"start\":45571},{\"end\":45986,\"start\":45902},{\"end\":46482,\"start\":46417},{\"end\":46956,\"start\":46885},{\"end\":47476,\"start\":47399},{\"end\":47973,\"start\":47966},{\"end\":48375,\"start\":48330},{\"end\":48763,\"start\":48725},{\"end\":49152,\"start\":49075},{\"end\":49839,\"start\":49784},{\"end\":50125,\"start\":50075},{\"end\":50487,\"start\":50416},{\"end\":50900,\"start\":50856},{\"end\":51347,\"start\":51307},{\"end\":51777,\"start\":51737},{\"end\":52010,\"start\":51946},{\"end\":52466,\"start\":52407},{\"end\":52855,\"start\":52778},{\"end\":53342,\"start\":53280},{\"end\":53605,\"start\":53530},{\"end\":54193,\"start\":54104},{\"end\":54676,\"start\":54652},{\"end\":55054,\"start\":55009},{\"end\":55475,\"start\":55404},{\"end\":55935,\"start\":55858},{\"end\":56377,\"start\":56296},{\"end\":56930,\"start\":56853},{\"end\":57553,\"start\":57476},{\"end\":58145,\"start\":58064},{\"end\":58636,\"start\":58598},{\"end\":58891,\"start\":58844},{\"end\":59364,\"start\":59293},{\"end\":59940,\"start\":59859},{\"end\":60490,\"start\":60409},{\"end\":60956,\"start\":60924},{\"end\":61327,\"start\":61250},{\"end\":61791,\"start\":61714},{\"end\":62304,\"start\":62227},{\"end\":62824,\"start\":62757},{\"end\":63189,\"start\":63117},{\"end\":44843,\"start\":44777},{\"end\":46057,\"start\":45988},{\"end\":47014,\"start\":46958},{\"end\":47540,\"start\":47478},{\"end\":49216,\"start\":49154},{\"end\":50545,\"start\":50489},{\"end\":52919,\"start\":52857},{\"end\":54269,\"start\":54195},{\"end\":55533,\"start\":55477},{\"end\":55999,\"start\":55937},{\"end\":56445,\"start\":56379},{\"end\":56994,\"start\":56932},{\"end\":57617,\"start\":57555},{\"end\":58213,\"start\":58147},{\"end\":59422,\"start\":59366},{\"end\":60008,\"start\":59942},{\"end\":60558,\"start\":60492},{\"end\":61391,\"start\":61329},{\"end\":61855,\"start\":61793},{\"end\":62368,\"start\":62306},{\"end\":62878,\"start\":62826}]"}}}, "year": 2023, "month": 12, "day": 17}