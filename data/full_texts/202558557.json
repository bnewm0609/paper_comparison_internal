{"id": 202558557, "updated": "2023-09-28 02:40:57.924", "metadata": {"title": "SoftTriple Loss: Deep Metric Learning Without Triplet Sampling", "authors": "[{\"first\":\"Qi\",\"last\":\"Qian\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Shang\",\"middle\":[]},{\"first\":\"Baigui\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Juhua\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Tacoma\",\"last\":\"Tacoma\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Rong\",\"last\":\"Jin\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 10, "day": 1}, "abstract": "Distance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classifications, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classification loss, with DNN shows a superior performance in certain DML tasks. It inspires us to investigate the formulation of SoftMax. Our analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, we propose the SoftTriple loss to extend the SoftMax loss with multiple centers for each class. Compared with conventional deep metric learning algorithms, optimizing SoftTriple loss can learn the embeddings without the sampling phase by mildly increasing the size of the last fully connected layer. Experiments on the benchmark fine-grained data sets demonstrate the effectiveness of the proposed loss function.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1909.05235", "mag": "2991234496", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/QianSSHTLJ19", "doi": "10.1109/iccv.2019.00655"}}, "content": {"source": {"pdf_hash": "401758952e82db1a567102cdb9351db9f2c25a07", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1909.05235", "status": "GREEN"}}, "grobid": {"id": "d3c42c228a0cf267fc466a3d30cae9c70d13e206", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/401758952e82db1a567102cdb9351db9f2c25a07.txt", "contents": "\nSoftTriple Loss: Deep Metric Learning Without Triplet Sampling\n\n\nQi Qian \nAlibaba Group\n98004BellevueWAUSA\n\nLei Shang \nAlibaba Group\nHangzhouChina\n\nBaigui Sun \nAlibaba Group\nHangzhouChina\n\nJuhua Hu juhuah@uw.edu \nSchool of Engineering and Technology\nUniversity of Washington\n98402TacomaWAUSA\n\nHao Li \nAlibaba Group\nHangzhouChina\n\nRong Jin jinrong.jr@alibaba-inc.com \nAlibaba Group\n98004BellevueWAUSA\n\nSoftTriple Loss: Deep Metric Learning Without Triplet Sampling\n10.1109/ICCV.2019.00655\nDistance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classifications, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classification loss, with DNN shows a superior performance in certain DML tasks. It inspires us to investigate the formulation of SoftMax. Our analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, we propose the SoftTriple loss to extend the SoftMax loss with multiple centers for each class. Compared with conventional deep metric learning algorithms, optimizing SoftTriple loss can learn the embeddings without the sampling phase by mildly increasing the size of the last fully connected layer. Experiments on the benchmark fine-grained data sets demonstrate the effectiveness of the proposed loss function.\n\nIntroduction\n\nDistance metric learning (DML) has been extensively studied in the past decades due to its broad range of applications, e.g., k-nearest neighbor classification [29], image retrieval [24] and clustering [31]. With an appropriate distance metric, examples from the same class should be FC in SoftMax FC in SoftTriple Figure 1. Illustration of the proposed SoftTriple loss. In conventional SoftMax loss, each class has a representative center in the last fully connected layer. Examples in the same class will be collapsed to the same center. It may be inappropriate for the realworld data as illustrated. In contrast, SoftTriple loss keeps multiple centers (e.g., 2 centers per class in this example) in the fully connected layer and each image will be assigned to one of them. It is more flexible for modeling intra-class variance in real-world data sets.\n\ncloser than examples from different classes. Many algorithms have been proposed to learn a good distance metric [15,16,21,29]. In most of conventional DML methods, examples are represented by hand-crafted features, and DML is to learn a feature mapping to project examples from the original feature space to a new space. The distance can be computed as the Mahalanobis distance [11] \ndist M (x i , x j ) = (x i \u2212 x j ) M (x i \u2212 x j )\nwhere M is the learned distance metric. With this formulation, the main challenge of DML is from the dimensionality of input space. As a metric, the learned matrix M has to be positive semi-definite (PSD) while the cost of keeping the matrix PSD can be up to O(d 3 ), where d is the dimensionality of original features. The early work directly applies PCA to shrink the original space [29]. Later, various strategies are developed to reduce the computational cost [16,17].\n\nThose approaches can obtain the good metric from the input features, but the hand-crafted features are task independent and may cause the loss of information, which limits the performance of DML. With the success of deep neural networks in classification [7], researchers consider to learn the embeddings directly from deep neural networks [15,21]. Without the explicit feature extraction, deep metric learning boosts the performance by a large margin [21]. In deep metric learning, the dimensionality of input features is no longer a challenge since neural networks can learn low-dimensional features directly from raw materials, e.g., images, documents, etc. In contrast, generating appropriate constraints for optimization becomes challenging for deep metric learning. It is because most of deep neural networks are trained with the stochastic gradient descent (SGD) algorithm and only a mini-batch of examples are available at each iteration. Since embeddings are optimized with the loss defined on an anchor example and its neighbors (e.g., the active set of pairwise [31] or triplet [29] constraints), the examples in a mini-batch may not be able to capture the overall neighborhood well, especially for relatively large data sets. Moreover, a mini-batch contains O(m 2 ) pairs and O(m 3 ) triplets, where m is the size of the mini-batch. An effective sampling strategy over the mini-batch is essential even for a small batch (e.g., 32) to learn the embeddings efficiently. Many efforts have been devoted to studying sampling an informative mini-batch [19,21] and sampling triplets within a mini-batch [12,24]. Some work also tried to reduce the total number of triplets with proxies [14,18]. The sampling phase for the mini-batch and constraints not only loses the information but also makes the optimization complicated. In this work, we consider to learn embeddings without constraints sampling.\n\nRecently, researches have shown that embeddings obtained directly from optimizing SoftMax loss, which is proposed for classification, perform well on the simple distance based tasks [22,30] and face recognition [2,9,10,27,28]. It inspires us to investigate the formulation of SoftMax loss. Our Analysis demonstrates that SoftMax loss is equivalent to a smoothed triplet loss. By providing a single center for each class in the last fully connected layer, the triplet constraint derived by SoftMax loss can be defined on an original example, its corresponding center and a center from a different class. Therefore, embeddings obtained by optimizing SoftMax loss can work well as a distance metric. However, a class in real-world data can consist of multiple local clusters as illustrated in Fig. 1 and a single center is insufficient to capture the inherent structure of the data. Consequently, embeddings learned from SoftMax loss can fail in the complex scenario [22].\n\nIn this work, we propose to improve SoftMax loss by introducing multiple centers for each class and the novel loss is denoted as SoftTriple loss. Compared with a single center, multiple ones can capture the hidden distribution of the data better due to the fact that they help to reduce the intra-class variance. This property is also crucial to reserve the triplet constraints over original examples while training with multiple centers. Compared with existing deep DML methods, the number of triplets in SoftTriple is linear in the number of original examples. Since the centers are encoded in the last fully connected layer, SoftTriple loss can be optimized without sampling triplets. Fig. 1 illustrates the proposed SoftTriple loss. Apparently, SoftTriple loss has to determine the number of centers for each class. To alleviate this issue, we develop a strategy that sets a sufficiently large number of centers for each class at the beginning and then applies L 2,1 norm to obtain a compact set of centers. We demonstrate the proposed loss on the fine-grained visual categorization tasks, where capturing local clusters is essential for good performance [17].\n\nThe rest of this paper is organized as follows. Section 2 reviews the related work of conventional distance metric learning and deep metric learning. Section 3 analyzes the SoftMax loss and proposes the SoftTriple loss accordingly. Section 4 conducts comparisons on benchmark data sets. Finally, Section 5 concludes this work and discusses future directions.\n\n\nRelated Work\n\nDistance metric learning Many DML methods have been developed when input features are provided [29,31]. The dimensionality of input features is a critical challenge for those methods due to the PSD projection, and many strategies have been proposed to alleviate it. The most straightforward way is to reduce the dimension of input space by PCA [29]. However, PCA is task independent and may hurt the performance of learned embeddings. Some works try to reduce the number of valid parameters with the low-rank assumption [8]. [16] decreases the computational cost by reducing the number of PSD projections. [17] proposes to learn the dual variables in the low-dimensional space introduced by random projections and then recover the metric in the original space. After addressing the challenge from the dimensionality, the hand-crafted features become the bottleneck of performance improvement.\n\nThe forms of constraints for metric learning are also developed in these methods. Early work focuses on optimizing pairwise constraints, which require the distances between examples from the same class small while those from different classes large [31]. Later, [29] develops the triplet constraints, where given an anchor example, the distance between the anchor point and a similar example should be smaller than that between the anchor point and a dissimilar example by a large margin. It is obvious that the number of pairwise constraints is O(n 2 ) while that of triplet constraints can be up to O(n 3 ), where n is the number of original examples. Compared with the pairwise constraints, triplet constraints optimize the geometry of local cluster and are more applicable for modeling intra-class variance. In this work, we will focus on the triplet constraints.\n\nDeep metric learning Deep metric learning aims to learn the embeddings directly from the raw materials (e.g., images) by deep neural networks [15,21]. With the task dependent embeddings, the performance of metric learning has a dramatical improvement. However, most of deep models are trained with SGD that allows only a mini-batch of data at each iteration. Since the size of mini-batch is small, the information in it is limited compared to the original data. To alleviate this problem, algorithms have to develop an effective sampling strategy to generate the minibatch and then sample triplet constraints from it. A straightforward way is increasing the size of mini-batch [21]. However, the large mini-batch will suffer from the GPU memory limitation and can also increase the challenge of sampling triplets. Later, [19] proposes to generate the minibatch from neighbor classes. Besides, there are various sampling strategies for obtaining constraints [3,12,21,24]. [21] proposes to sample the semi-hard negative examples. [24] adopts all negative examples within the margin for each positive pair. [12] develops distance weighted sampling that samples examples according to the distance from the anchor example. [3] selects hard triplets with a dynamic violate margin from a hierarchical class-level tree. However, all of these strategies may fail to capture the distribution of the whole data set. Moreover, they make the optimization in deep DML complicated.\n\nLearning with proxies Recently, some researchers consider to reduce the total number of triplets to alleviate the challenge from the large number of triplets. [14] constructs the triplet loss with one original example and two proxies. Since the number of proxies is significantly less than the number of original examples, proxies can be kept in the memory that help to avoid the sampling over different batches. However, it only provides a single proxy for each class when label information is available, which is similar to SoftMax. [18] proposes a conventional DML algorithm to construct the triplet loss only with latent examples, which assigns multiple centers for each class and further reduces the number of triplets. In this work, we propose to learn the embeddings by optimizing the proposed SoftTriple loss to eliminate the sampling phase and capture the local geometry of each class simultaneously.\n\n\nSoftTriple Loss\n\nIn this section, we first introduce the SoftMax loss and the triplet loss and then study the relationship between them to derive the SoftTriple loss.\n\nDenote the embedding of the i-th example as x i and the corresponding label as y i , then the conditional probability output by a deep neural network can be estimated via the SoftMax operator\nPr(Y = y i |x i ) = exp(w yi x i ) C j exp(w j x i )\nwhere [w 1 , \u00b7 \u00b7 \u00b7 , w C ] \u2208 R d\u00d7C is the last fully connected layer. C denotes the number of classes and d is the dimension of embeddings. The corresponding SoftMax loss is\nSoftMax (x i ) = \u2212 log exp(w yi x i ) j exp(w j x i ) A deep\nmodel can be learned by minimizing losses over examples. This loss has been prevalently applied for classification task [7].\n\nGiven a triplet (x i , x j , x k ), DML aims to learn good embeddings such that examples from the same class are closer than examples from different classes, i.e., \u2200i, j, k,\nx i \u2212 x k 2 2 \u2212 x i \u2212 x j 2 2 \u2265 \u03b4 where x i\nand x j are from the same class and x k is from a different class. \u03b4 is a predefined margin. When each example has the unit length (i.e., x 2 = 1), the triplet constraint can be simplified as\n\u2200i, j, k, x i x j \u2212 x i x k \u2265 \u03b4(1)\nwhere we ignore the rescaling of \u03b4. The corresponding triplet loss can be written as\ntriplet (x i , x j , x k ) = [\u03b4 + x i x k \u2212 x i x j ] +(2)\nIt is obvious from Eqn. 1 that the number of total triplets can be cubic in the number of examples, which makes sampling inevitable for most of triplet based DML algorithms.\n\nWith the unit length for both w and x, the normalized SoftMax loss can be written as\nSoftMaxnorm (x i ) = \u2212 log exp(\u03bbw yi x i ) j exp(\u03bbw j x i )(3)\nwhere \u03bb is a scaling factor.\n\nSurprisingly, we find that minimizing the normalized SoftMax loss with the smooth term \u03bb is equivalent to optimizing a smoothed triplet loss.\nProposition 1. SoftMaxnorm (x i ) = max p\u2208\u2206 \u03bb j p j x i (w j \u2212 w yi ) + H(p)(4)\nwhere p \u2208 R C is a distribution over classes and \u2206 is the simplex as \u2206 = {p| j p j = 1, \u2200j, p j \u2265 0}. H(p) denotes the entropy of the distribution p.\n\nProof. According to the K.K.T. condition [1], the distribution p in Eqn. 4 has the closed-form solution\np j = exp(\u03bbx i (w j \u2212 w yi )) j exp(\u03bbx i (w j \u2212 w yi ))\nTherefore, we have\nSoftMaxnorm (xi) = \u03bb j pjx i (wj \u2212 wy i ) + H(p) = log( j exp(\u03bbx i (wj \u2212 wy i ))) = \u2212 log exp(\u03bbw y i xi) j exp(\u03bbw j xi)\nRemark 1 Proposition 1 indicates that the SoftMax loss optimizes the triplet constraints consisting of an original example and two centers, i.e., (x i , w yi , w j ). Compared with triplet constraints in Eqn. 1, the target of SoftMax loss is\n\u2200i, j, x i w yi \u2212 x i w j \u2265 0\nConsequently, the embeddings learned by minimizing Soft-Max loss can be applicable for the distance-based tasks while it is designed for the classification task.\n\nRemark 2 Without the entropy regularizer, the loss becomes max\np\u2208\u2206 \u03bb j p j x i w j \u2212 \u03bbx i w yi which is equivalent to max j {x i w j } \u2212 x i w yi\nExplicitly, it punishes the triplet with the most violation and becomes zero when the nearest neighbor of x i is the corresponding center w yi . The entropy regularizer reduces the influence from outliers and makes the loss more robust. \u03bb trades between the hardness of triplets and the regularizer. Moreover, minimizing the maximal entropy can make the distribution concentrated and further push the example away from irrelevant centers, which implies a large margin property.\n\n\nRemark 3\n\nApplying the similar analysis to the Prox-yNCA loss [14]:\nProxyNCA (x i ) = \u2212 log exp(w y i xi) j =y i exp(w j xi) , we have ProxyNCA (x i ) = max p\u2208\u2206 \u03bb j =yi p j x i (w j \u2212 w yi ) + H(p)\nwhere p \u2208 R C\u22121 . Compared with the SoftMax loss, it eliminates the benchmark triplet containing only the corresponding class center, which makes the loss unbounded.\n\nOur analysis suggests that the loss can be bounded as in\nEqn. 2: hinge ProxyNCA (x i ) = [\u2212 log exp(w y i xi) j =y i exp(w j xi) ] + .\nValidating the bounded loss is out of the scope of this work.\n\nDespite optimizing SoftMax loss can learn the meaningful feature embeddings, the drawback is straightforward. It assumes that there is only a single center for each class while a real-world class can contain multiple local clusters due to the large intra-class variance as in Fig. 1. The triplet constraints generated by conventional SoftMax loss is too brief to capture the complex geometry of the original data. Therefore, we introduce multiple centers for each class.\n\n\nMultiple Centers\n\nNow, we assume that each class has K centers. Then, the similarity between the example x i and the class c can be defined as\nS i,c = max k x i w k c(5)\nNote that other definitions of similarity can be applicable for this scenario (e.g.,\nmin z\u2208R K [w 1 c , \u00b7 \u00b7 \u00b7 , w K c ]z \u2212 x i 2 ).\nWe adopt a simple form to illustrate the influence of multiple centers.\n\nWith the definition of the similarity, the triplet constraint requires an example to be closer to its corresponding class than other classes \u2200j, S i,yi \u2212 S i,j \u2265 0 As we mentioned above, minimizing the entropy term H(p) can help to pull the example to the corresponding center. To break the tie explicitly, we consider to introduce a small margin as in the conventional triplet loss in Eqn. 1 and define the constraints as\n\u2200j j =yi , S i,yi \u2212 S i,j \u2265 \u03b4\nBy replacing the similarity in Eqn. 4, we can obtain the HardTriple loss as\nHardTriple (x i ) = max p\u2208\u2206 \u03bb j =yi p j (S i,j \u2212 (S i,yi \u2212 \u03b4)) + p yi (S i,yi \u2212 \u03b4 \u2212 (S i,yi \u2212 \u03b4)) + H(p) = \u2212 log exp(\u03bb(S i,yi \u2212 \u03b4)) exp(\u03bb(S i,yi \u2212 \u03b4)) + j =yi exp(\u03bbS i,j )(6)\nHardTriple loss improves the SoftMax loss by providing multiple centers for each class. However, it requires the max operator to obtain the nearest center in each class while this operator is not smooth and the assignment can be sensitive between multiple centers. Inspired by the SoftMax loss, we can improve the robustness by smoothing the max operator.\n\n\nConsider the problem\nmax k x i w k c which is equivalent to max q\u2208\u2206 k q k x i w k c(7)\nwe add the entropy regularizer to the distribution q as\nmax q\u2208\u2206 k q k x i w k c + \u03b3H(q)\nWith a similar analysis as in Proposition 1, q has the closedform solution as\nq k = exp( 1 \u03b3 x i w k c ) k exp( 1 \u03b3 x i w k c )\nTaking it back to the Eqn. 7, we define the relaxed similarity between the example x i and the class c as\nS i,c = k exp( 1 \u03b3 x i w k c ) k exp( 1 \u03b3 x i w k c ) x i w k c\nBy applying the smoothed similarity, we define the Soft-Triple loss as  Figure 2. Illustration of differences between SoftMax loss and proposed losses. Compared with the SoftMax loss, we first increase the dimension of the FC layer to include multiple centers for each class (e.g., 2 centers per class in this example). Then, we obtain the similarity for each class by different operators. Finally, the distribution over different classes is computed with the similarity obtained from each class.\nSoftTriple (x i ) = \u2212 log exp(\u03bb(S i,yi \u2212 \u03b4)) exp(\u03bb(S i,yi \u2212 \u03b4)) + j =yi exp(\u03bbS i,j )(8)\nFinally, we will show that the strategy of applying centers to construct triplet constraints can recover the constraints on original triplets.\n\n\nTheorem 1.\n\nGiven two examples x i and x j that are from the same class and have the same nearest center and x k is from a different class, if the triple constant containing centers is satisfied\nx i w yi \u2212 x i w y k \u2265 \u03b4\nand we assume \u2200i, x i \u2212 w yi 2 \u2264 , then we have\nx i x j \u2212 x i x k \u2265 \u03b4 \u2212 2\nProof.\nx i x j \u2212 x i x k = x i (x j \u2212 w yi ) + x i w yi \u2212 x i x k \u2265 x i (x j \u2212 w yi ) + x i (w y k \u2212 x k ) + \u03b4 \u2265 \u03b4 \u2212 x i 2 x j \u2212 w yi 2 \u2212 x i 2 w y k \u2212 x k 2 = \u03b4 \u2212 x j \u2212 w yi 2 \u2212 w y k \u2212 x k 2 \u2265 \u03b4 \u2212 2\nTheorem 1 demonstrates that optimizing the triplets consisting of centers with a margin \u03b4 can reserve the large margin property on the original triplet constraints. It also implies that more centers can be helpful to reduce the intraclass variance . In the extreme case that the number of centers is equal to the number of examples, becomes zero. However, adding more centers will increase the size of the last fully connected layer and make the optimization slow and computation expensive. Besides, it may incur the overfitting problem.\n\nTherefore, we have to choose an appropriate number of centers for each class that can have a small approximation error while keeping a compact set of centers. We will demonstrate the strategy in the next subsection.\n\n\nAdaptive Number of Centers\n\nFinding an appropriate number of centers for data is a challenging problem that also appears in unsupervised learning, e.g., clustering. The number of centers K trades between the efficiency and effectiveness. In conventional DML algorithms, K equals to the number of original examples. It makes the number of total triplet constraints up to cubic of the number of original examples. In SoftMax loss, K = 1 reduces the number of constraints to be linear in the number of original examples, which is efficient but can be ineffective. Without the prior knowledge about the distribution of each class, it is hard to set K precisely.\n\nDifferent from the strategy of setting the appropriate K for each class, we propose to set a sufficiently large K and then encourage similar centers to merge with each other. It can keep the diversity in the generated centers while shrinking the number of unique centers.\n\nFor each center w t j , we can generate a matrix as\nM t j = [w 1 j \u2212 w t j , \u00b7 \u00b7 \u00b7 , w K j \u2212 w t j ]\nIf w s j and w t j are similar, they can be collapsed to be the same one such that w s j \u2212 w t j 2 = 0, which is the L 2 norm of the s-th row in the matrix M t j . Therefore, we regularize the L 2 norm of rows in M t j to obtain a sparse set of centers, which can be written as the L 2,1 norm\nM t j 2,1 = K s w s j \u2212 w t j 2\nBy accumulating L 2,1 norm over multiple centers, we can have the regularizer for the j-th class as\nR(w 1 j , \u00b7 \u00b7 \u00b7 , w K j ) = K t M t j 2,1\nSince w has the unit length, the regularizer is simplified as\nR(w 1 j , \u00b7 \u00b7 \u00b7 , w K j ) = K t=1 K s=t+1 2 \u2212 2w s j w t j (9)\nWith the regularizer, our final objective becomes\nmin 1 N i SoftTriple (x i ) + \u03c4 C j R(w 1 j , \u00b7 \u00b7 \u00b7 , w K j ) CK(K \u2212 1)(10)\nwhere N is the number of total examples.\n\n\nExperiments\n\nWe conduct experiments on three benchmark finegrained visual categorization data sets: CUB-2011, Cars196 and SOP. We follow the settings in other works [3,14] for the fair comparison. Specifically, we adopt the Inception [25] with the batch normalization [5] as the backbone architecture. The parameters of the backbone are initialized with the model trained on the ImageNet ILSVRC 2012 data set [20] and then fine-tuned on the target data sets. The images are cropped to 224 \u00d7 224 as the input of the network. During training, only random horizontal mirroring and random crop are used as the data augmentation. A single center crop is taken for test. The model is optimized by Adam with the batch size as 32 and the number of epochs as 50. The initial learning rates for the backbone and centers are set to be 1e-4 and 1e-2, respectively. Then, they are divided by 10 at {20, 40} epochs. Considering that images in CUB-2011 and Cars196 are similar to those in ImageNet, we freeze BN on these two data sets and keep BN training on the rest one. Embeddings of examples and centers have the unit length in the experiments.\n\nWe compare the proposed triplet loss to the normalized SoftMax loss. The SoftMax loss in Eqn. 3 is denoted as SoftMax norm . We refer the objective in Eqn. 10 as Soft-Triple. We set \u03c4 = 0.2 and \u03b3 = 0.1 for SoftTriple. Besides, we set a small margin as \u03b4 = 0.01 to break the tie explicitly. The number of centers is set to K = 10.\n\nWe evaluate the performance of the learned embeddings from different methods on the tasks of retrieval and clustering. For retrieval task, we use the Recall@k metric as in [24]. The quality of clustering is measured by the Normalized Mutual Information (NMI) [13]. Given the clustering assignment C = {c 1 , \u00b7 \u00b7 \u00b7 , c n } and the ground-truth label \u2126 = {y 1 , \u00b7 \u00b7 \u00b7 , y n }, NMI is computed as NMI = 2I(\u2126;C) H(\u2126)+H(C) , where I(\u00b7, \u00b7) measures the mutual information and H(\u00b7) denotes the entropy.\n\n\nCUB-2011\n\nFirst, we compare the methods on a fine-grained birds data set CUB-2011 [26]. It consists of 200 species of birds and 11, 788 images. Following the common practice, we split the data set as that the first 100 classes are used for training and the rest are used for test. We note that different works report the results with different dimension of embeddings while the size of embeddings has a significant impact on the performance. For fair comparison, we report the results for the dimension of 64, which is adopted by many existing methods and the results with 512 feature embeddings, which reports the state-of-the-art results on most of data sets. Table 1 summarizes the results with 64 embeddings. Note that Npairs * applies the multi-scale test while all other methods take a single crop test. For SemiHard [21], we report the result recorded in [23]. First, it is surprising to observe that the performance of SoftMax norm surpasses that of the existing metric learning methods. It is potentially due to the fact that SoftMax loss optimizes the relations of examples as a smoothed triplet loss, which is analyzed in Proposition 1. Second, SoftTriple demonstrates the best performance among all benchmark methods. Compared to ProxyNCA, SoftTriple improves the state-of-theart performance by 10% on R@1. Besides, it is 2% better than SoftMax norm . It verifies that SoftMax loss cannot capture the complex geometry of real-world data set with a single center for each class. When increasing the number of centers, SoftTriple can depict the inherent structure of data better. Finally, both of SoftMax and SoftTriple show the superior performance compared to existing methods. It demonstrates that meaningful embeddings can be learned without a sampling phase. Table 2 compares SoftTriple with 512 embeddings to the methods with large embeddings. HDC [32] applies the dimension as 384. Margin [12] takes 128 dimension of embeddings and uses ResNet50 [4] as the backbone. HTL [3] sets the dimension of embeddings to 512 and reports the state-of-the-art result on the backbone of Inception. With the large number of embeddings, it is obvious that all methods outperform existing DML methods with 64 embeddings   [24]. Compared with other methods, the R@1 of SoftTriple improves more than 8% over HTL that has the same backbone as SoftTriple. It also increases R@1 by about 2% over Margin, which applies a stronger backbone than Inception. It shows that SoftTriple loss is applicable with large embeddings. To validate the effect of the proposed regularizer, we compare the number of unique centers for each class in Fig. 3. We set a larger number of centers as K = 20 to make the results explicit and then run SoftTriple with and without the regularizer in Eqn. 9. Fig. 3 illustrates that the one without regularizer will hold a set of similar centers. In contrast, SoftTriple with the regularizer can shrink the size of centers significantly and make the optimization effective.\n\nBesides, we demonstrate the R@1 of SoftTriple with varying the number of centers in Fig. 4. Red line denotes SoftTriple loss equipped with the regularizer while blue dashed line has no regularizer. We find that when increasing the number of centers from 1 to 10, the performance of SoftTriple is improved significantly, which confirms that with leveraging multiple centers, the learned embeddings can capture the data distribution better. If adding more centers, the performance of SoftTriple almost remains the same and it shows that the regularizer can help to learn the compact set of centers and will not be influenced by the initial number of centers. On the contrary, without the regularizer, the blue dashed line illustrates that the performance will degrade due to overfitting when the number of centers are over-parameterized. \n\n\nCars196\n\nThen, we conduct the experiments on Cars196 data set [6], which contains 196 models of cars and 16, 185 images. We use the first 98 classes for training and the rest for test. Table 3 summaries the performance with 64 embeddings. The observation is similar as for CUB-2011. SoftMax norm shows the superior performance and is 3% better than ProxyNCA on R@1. Additionally, SoftTriple can further improve the performance by about 2%, which demonstrates the effectiveness of the proposed loss function. In Table 4, we present the comparison with large dimension of embeddings. The number of embeddings for all methods in the comparison is the same as described in the experiments on CUB-2011. On this data set, HTL [3] reports the state-of-the-art result while SoftTriple outperforms it and increases R@1 by 3%. \n\n\nStanford Online Products\n\nFinally, we evaluate the performance of different methods on the Stanford Online Products (SOP) data set [24]. It contains 120, 053 product images downloaded from eBay.com and includes 22, 634 classes. We adopt the stan-dard splitting, where 11, 318 classes are used for training and the rest for test. Note that each class has about 5 images, so we set K = 2 for this data set and discard the regularizer. We also increase the initial learning rate for centers from 0.01 to 0.1.\n\nWe first report the results with 64 embeddings in Table 5. In this comparison, SoftMax norm is 2% better than Prox-yNCA on R@1. By simply increasing the number of centers from 1 to 2, we observe that SoftTriple gains another 0.4% on R@1. It confirms that multiple centers can help to capture the data structure better. Table 6 states the performance with large embeddings. We can get a similar conclusion as in Table 5. Both SoftMax norm and SoftTriple outperform the state-of-the-art methods. SoftTriple improves the state-of-the-art by more than 3% on R@1. It demonstrates the advantage of learning embeddings without sampling triplet constraints. \n\n\nConclusion\n\nSampling triplets from a mini-batch of data can degrade the performance of deep metric learning due to its poor coverage over the whole data set. To address the problem, we propose the novel SoftTriple loss to learn the embeddings without sampling. By representing each class with multiple centers, the loss can be optimized with triplets defined with the similarities between the original examples and classes. Since centers are encoded in the last fully connected layer, we can learn embeddings with the standard SGD training pipeline for classification and eliminate the sampling phase. The consistent improvement from Soft-Triple over fine-grained benchmark data sets confirms the effectiveness of the proposed loss function. Since SoftMax loss is prevalently applied for classification, SoftTriple loss can also be applicable for that. Evaluating SoftTriple on the classification task can be our future work.\n\nFigure 3 .\n3Comparison of the number of unique centers in each class on CUB-2011. The initial number of centers is set to 20.\n\nFigure 4 .\n4Illustration of SoftTriple with different number of centers and the influence of the regularizer. With the proposed regularizer as denoted by the red line, the performance is stable to the initial number of centers K when it is sufficiently large. Finally, we illustrate the examples of retrieved images in Fig. 5. The first column indicates the query image. The columns 2-4 show the most similar images retrieved according to the embeddings learned by SoftMax norm . The last four columns are the similar images returned by using the embeddings from SoftTriple. Evidently, embeddings from SoftMax norm can obtain the meaningful neighbors while the objective is for classification. Besides, SoftTriple improves the performance and can eliminate the images from different classes among the top of retrieved images, which are highlighted with red bounding boxes in SoftMax norm .\n\nFigure 5 .\n5Examples of retrieved most similar images with the learned embeddings from SoftMaxnorm and SoftTriple. The images from the classes that are different from the query image are highlighted by red bounding boxes.\n\n\nFig. 2illustrates the differences between the SoftMax loss and the proposed losses.Embedding FC layer \n\nSoftMax \n\nEmbedding FC layer \n\nSoftMax \n\nMax Operator \nEmbedding FC layer \n\nSoftMax \n\nSoftMax Operator \n\nSoftMax Loss \nHardTriple Loss \nSoftTriple Loss \n\n\n\nTable 1 .\n1Comparison on CUB-2011. The dimension of the embeddings for all methods is 64.Methods \nR@1 R@2 R@4 R@8 NMI \nSemiHard [21] \n42.6 \n55.0 \n66.4 \n77.2 \n55.4 \nLiftedStruct [24] \n43.6 \n56.6 \n68.6 \n79.6 \n56.5 \nClustering [23] \n48.2 \n61.4 \n71.8 \n81.9 \n59.2 \nNpairs  *  [22] \n51.0 \n63.3 \n74.3 \n83.2 \n60.4 \nProxyNCA [14] \n49.2 \n61.9 \n67.9 \n72.4 \n59.5 \nSoftMaxnorm \n57.8 \n70.0 \n80.1 \n87.9 \n65.3 \nSoftTriple \n60.1 \n71.9 \n81.2 \n88.5 \n66.2 \n\nin Table 1. It is as expected since the high dimensional \nspace can separate examples better, which is consistent with \nthe observation in other work \n\nTable 2 .\n2Comparison on CUB-2011 with large embeddings. \"-\" \nmeans the result is not available. \nMethods \nR@1 R@2 R@4 R@8 NMI \nHDC [32] \n53.6 \n65.7 \n77.0 \n85.6 \n-\nMargin [12] \n63.6 \n74.4 \n83.1 \n90.0 \n69.0 \nHTL [3] \n57.1 \n68.8 \n78.7 \n86.5 \n-\nSoftMaxnorm \n64.2 \n75.6 \n84.3 \n90.2 \n68.3 \nSoftTriple \n65.4 \n76.4 \n84.5 \n90.4 \n69.3 \n\n\n\nTable 3 .\n3Comparison on Cars196. The dimension is 64.Methods \nR@1 R@2 R@4 R@8 NMI \nSemiHard [21] \n51.5 \n63.8 \n73.5 \n82.4 \n53.4 \nLiftedStruct [24] \n53.0 \n65.7 \n76.0 \n84.3 \n56.9 \nClustering [23] \n58.1 \n70.6 \n80.3 \n87.8 \n59.0 \nNpairs  *  [22] \n71.1 \n79.7 \n86.5 \n91.6 \n64.0 \nProxyNCA [14] \n73.2 \n82.4 \n86.4 \n88.7 \n64.9 \nSoftMaxnorm \n76.8 \n85.6 \n91.3 \n95.2 \n66.7 \nSoftTriple \n78.6 \n86.6 \n91.8 \n95.4 \n67.0 \n\n\n\nTable 4 .\n4Comparison on Cars196 with large embeddings.Methods \nR@1 R@2 R@4 R@8 NMI \nHDC [32] \n73.7 \n83.2 \n89.5 \n93.8 \n-\nMargin [12] \n79.6 \n86.5 \n91.9 \n95.1 \n69.1 \nHTL [3] \n81.4 \n88.0 \n92.7 \n95.7 \n-\nSoftMaxnorm \n83.2 \n89.5 \n94.0 \n96.6 \n69.7 \nSoftTriple \n84.5 \n90.7 \n94.5 \n96.9 \n70.1 \n\n\n\nTable 5 .\n5Comparison on SOP. The dimension is 64.Table 6. Comparison on SOP with large embeddings.Methods \nR@1 R@10 R@100 NMI \nSemiHard [21] \n66.7 \n82.4 \n91.9 \n89.5 \nLiftedStruct [24] \n62.5 \n80.8 \n91.9 \n88.7 \nClustering [23] \n67.0 \n83.7 \n93.2 \n89.5 \nProxyNCA [14] \n73.7 \n-\n-\n90.6 \nSoftMaxnorm \n75.9 \n88.8 \n95.2 \n91.5 \nSoftTriple \n76.3 \n89.1 \n95.3 \n91.7 \n\nMethods \nR@1 R@10 R@100 NMI \nNpairs  *  [22] \n67.7 \n83.8 \n93.0 \n88.1 \nHDC [32] \n69.5 \n84.4 \n92.8 \n-\nMargin [12] \n72.7 \n86.2 \n93.8 \n90.7 \nHTL [3] \n74.8 \n88.3 \n94.8 \n-\nSoftMaxnorm \n78.0 \n90.2 \n96.0 \n91.9 \nSoftTriple \n78.3 \n90.3 \n95.9 \n92.0 \n\n\n\nConvex optimization. Stephen Boyd, Lieven Vandenberghe, Cambridge university pressStephen Boyd and Lieven Vandenberghe. Convex optimiza- tion. Cambridge university press, 2004. 4\n\nArcface: Additive angular margin loss for deep face recognition. Jiankang Deng, Jia Guo, Stefanos Zafeiriou, abs/1801.07698CoRRJiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arc- face: Additive angular margin loss for deep face recognition. CoRR, abs/1801.07698, 2018. 2\n\nDeep metric learning with hierarchical triplet loss. Weifeng Ge, Weilin Huang, Dengke Dong, Matthew R Scott, ECCV. 7Weifeng Ge, Weilin Huang, Dengke Dong, and Matthew R. Scott. Deep metric learning with hierarchical triplet loss. In ECCV, pages 272-288, 2018. 3, 6, 7, 8\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016. 6\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, ICML. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448-456, 2015. 6\n\n3d object representations for fine-grained categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei, 4th International IEEE Workshop on 3D Representation and Recognition. Sydney, AustraliaJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013. 7\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS. 23Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NIPS, pages 1106-1114, 2012. 2, 3\n\nRobust structural metric learning. Daryl Lim, Gert R G Lanckriet, Brian Mcfee, ICML. Daryl Lim, Gert R. G. Lanckriet, and Brian McFee. Robust structural metric learning. In ICML, pages 615-623, 2013. 2\n\nSphereface: Deep hypersphere embedding for face recognition. Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song, CVPR. Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In CVPR, pages 6738-6746, 2017. 2\n\nLarge-margin softmax loss for convolutional neural networks. Weiyang Liu, Yandong Wen, Zhiding Yu, Meng Yang, ICML. Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural net- works. In ICML, pages 507-516, 2016. 2\n\nOn the generalized distance in statistics. Prasanta Chandra Mahalanobis, National Institute of Science of IndiaPrasanta Chandra Mahalanobis. On the generalized distance in statistics. National Institute of Science of India, 1936. 1\n\nSampling matters in deep embedding learning. R Manmatha, Chao-Yuan, Alexander J Wu, Philipp Smola, Kr\u00e4henb\u00fchl, ICCV. 7R. Manmatha, Chao-Yuan Wu, Alexander J. Smola, and Philipp Kr\u00e4henb\u00fchl. Sampling matters in deep embedding learning. In ICCV, pages 2859-2867, 2017. 2, 3, 6, 7, 8\n\nIntroduction to information retrieval. Christopher Manning, Prabhakar Raghavan, Hinrich Sch\u00fctze, Natural Language Engineering. 161Christopher Manning, Prabhakar Raghavan, and Hinrich Sch\u00fctze. Introduction to information retrieval. Natural Lan- guage Engineering, 16(1):100-103, 2010. 6\n\nNo fuss distance metric learning using proxies. Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, Saurabh Singh, ICCV. 7Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Le- ung, Sergey Ioffe, and Saurabh Singh. No fuss distance met- ric learning using proxies. In ICCV, pages 360-368, 2017. 2, 3, 4, 6, 7, 8\n\nDeep face recognition. M Omkar, Andrea Parkhi, Andrew Vedaldi, Zisserman, BMVC. 123Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition. In BMVC, pages 41.1-41.12, 2015. 1, 2, 3\n\nEfficient distance metric learning by adaptive sampling and mini-batch stochastic gradient descent (SGD). Qi Qian, Rong Jin, Jinfeng Yi, Lijun Zhang, Shenghuo Zhu, Machine Learning. 99Qi Qian, Rong Jin, Jinfeng Yi, Lijun Zhang, and Shenghuo Zhu. Efficient distance metric learning by adaptive sampling and mini-batch stochastic gradient descent (SGD). Machine Learning, 99(3):353-372, 2015. 1, 2\n\nFinegrained visual categorization via multi-stage metric learning. Qi Qian, Rong Jin, Shenghuo Zhu, Yuanqing Lin, CVPR. 1Qi Qian, Rong Jin, Shenghuo Zhu, and Yuanqing Lin. Fine- grained visual categorization via multi-stage metric learning. In CVPR, pages 3716-3724, 2015. 1, 2\n\nLarge-scale distance metric learning with uncertainty. Qi Qian, Jiasheng Tang, Hao Li, Shenghuo Zhu, Rong Jin, CVPR. 23Qi Qian, Jiasheng Tang, Hao Li, Shenghuo Zhu, and Rong Jin. Large-scale distance metric learning with uncertainty. In CVPR, pages 8542-8550, 2018. 2, 3\n\nMetric learning with adaptive density discrimination. Oren Rippel, Manohar Paluri, Piotr Doll\u00e1r, Lubomir D Bourdev, ICLR. 23Oren Rippel, Manohar Paluri, Piotr Doll\u00e1r, and Lubomir D. Bourdev. Metric learning with adaptive density discrimina- tion. ICLR, 2016. 2, 3\n\nImageNet Large Scale Visual Recognition Challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, International Journal of Computer Vision (IJCV). 1153Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015. 6\n\nFacenet: A unified embedding for face recognition and clustering. Florian Schroff, Dmitry Kalenichenko, James Philbin, CVPR. 7Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clus- tering. In CVPR, pages 815-823, 2015. 1, 2, 3, 6, 7, 8\n\nImproved deep metric learning with multiclass n-pair loss objective. Kihyuk Sohn, NIPS. Kihyuk Sohn. Improved deep metric learning with multi- class n-pair loss objective. In NIPS, pages 1849-1857, 2016. 2, 7, 8\n\nDeep metric learning via facility location. Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, Kevin Murphy, CVPR. 7Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy. Deep metric learning via facility location. In CVPR, pages 2206-2214, 2017. 6, 7, 8\n\nDeep metric learning via lifted structured feature embedding. Hyun Oh Song, Yu Xiang, Stefanie Jegelka, Silvio Savarese, CVPR. 7Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In CVPR, pages 4004-4012, 2016. 1, 2, 3, 6, 7, 8\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pages 1-9, 2015. 6\n\nThe Caltech-UCSD Birds-200-2011 Dataset. Technical report. C Wah, S Branson, P Welinder, P Perona, S Belongie, C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical re- port, 2011. 6\n\nAdditive margin softmax for face verification. Feng Wang, Jian Cheng, Weiyang Liu, Haijun Liu, IEEE Signal Process. Lett. 257Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. Ad- ditive margin softmax for face verification. IEEE Signal Pro- cess. Lett., 25(7):926-930, 2018. 2\n\nCosface: Large margin cosine loss for deep face recognition. Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, Wei Liu, CVPR. Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In CVPR, pages 5265-5274, 2018. 2\n\nDistance metric learning for large margin nearest neighbor classification. Q Kilian, Lawrence K Weinberger, Saul, Journal of Machine Learning Research. 102Kilian Q. Weinberger and Lawrence K. Saul. Distance met- ric learning for large margin nearest neighbor classification. Journal of Machine Learning Research, 10:207-244, 2009. 1, 2\n\nA discriminative feature learning approach for deep face recognition. Yandong Wen, Kaipeng Zhang, Zhifeng Li, Yu Qiao, ECCV. Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recog- nition. In ECCV, pages 499-515, 2016. 2\n\nDistance metric learning with application to clustering with side-information. Eric P Xing, Andrew Y Ng, Michael I Jordan, Stuart J Russell, NIPS. 1Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart J. Russell. Distance metric learning with application to cluster- ing with side-information. In NIPS, pages 505-512, 2002. 1, 2\n\nHard-aware deeply cascaded embedding. Yuhui Yuan, Kuiyuan Yang, Chao Zhang, ICCV. 7Yuhui Yuan, Kuiyuan Yang, and Chao Zhang. Hard-aware deeply cascaded embedding. In ICCV, pages 814-823, 2017. 6, 7, 8\n", "annotations": {"author": "[{\"end\":108,\"start\":66},{\"end\":148,\"start\":109},{\"end\":189,\"start\":149},{\"end\":293,\"start\":190},{\"end\":330,\"start\":294},{\"end\":401,\"start\":331}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":69},{\"end\":118,\"start\":113},{\"end\":159,\"start\":156},{\"end\":198,\"start\":196},{\"end\":300,\"start\":298},{\"end\":339,\"start\":336}]", "author_first_name": "[{\"end\":68,\"start\":66},{\"end\":112,\"start\":109},{\"end\":155,\"start\":149},{\"end\":195,\"start\":190},{\"end\":297,\"start\":294},{\"end\":335,\"start\":331}]", "author_affiliation": "[{\"end\":107,\"start\":75},{\"end\":147,\"start\":120},{\"end\":188,\"start\":161},{\"end\":292,\"start\":214},{\"end\":329,\"start\":302},{\"end\":400,\"start\":368}]", "title": "[{\"end\":63,\"start\":1},{\"end\":464,\"start\":402}]", "venue": null, "abstract": "[{\"end\":2044,\"start\":489}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2224,\"start\":2220},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2246,\"start\":2242},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2266,\"start\":2262},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3032,\"start\":3028},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3035,\"start\":3032},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3038,\"start\":3035},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3041,\"start\":3038},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3298,\"start\":3294},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3739,\"start\":3735},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3818,\"start\":3814},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3821,\"start\":3818},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4082,\"start\":4079},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4168,\"start\":4164},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4171,\"start\":4168},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4280,\"start\":4276},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4901,\"start\":4897},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4917,\"start\":4913},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5386,\"start\":5382},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5389,\"start\":5386},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5436,\"start\":5432},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5439,\"start\":5436},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5518,\"start\":5514},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5521,\"start\":5518},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5916,\"start\":5912},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5919,\"start\":5916},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5944,\"start\":5941},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5946,\"start\":5944},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5949,\"start\":5946},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5952,\"start\":5949},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5955,\"start\":5952},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6698,\"start\":6694},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7864,\"start\":7860},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8341,\"start\":8337},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8344,\"start\":8341},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8590,\"start\":8586},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8765,\"start\":8762},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8771,\"start\":8767},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8852,\"start\":8848},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9389,\"start\":9385},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9402,\"start\":9398},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10151,\"start\":10147},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10154,\"start\":10151},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10686,\"start\":10682},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10830,\"start\":10826},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10965,\"start\":10962},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10968,\"start\":10965},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10971,\"start\":10968},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10974,\"start\":10971},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10980,\"start\":10976},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11037,\"start\":11033},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11113,\"start\":11109},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11226,\"start\":11223},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11636,\"start\":11632},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12012,\"start\":12008},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13156,\"start\":13153},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14518,\"start\":14515},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15900,\"start\":15896},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22724,\"start\":22721},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22727,\"start\":22724},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22794,\"start\":22790},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22827,\"start\":22824},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22969,\"start\":22965},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24198,\"start\":24194},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24285,\"start\":24281},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24606,\"start\":24602},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25347,\"start\":25343},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25386,\"start\":25382},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26388,\"start\":26384},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26430,\"start\":26426},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26486,\"start\":26483},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26511,\"start\":26508},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26747,\"start\":26743},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28416,\"start\":28413},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29074,\"start\":29071},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29306,\"start\":29302}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31383,\"start\":31257},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32274,\"start\":31384},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32497,\"start\":32275},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32758,\"start\":32498},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33348,\"start\":32759},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33678,\"start\":33349},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34083,\"start\":33679},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34370,\"start\":34084},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34968,\"start\":34371}]", "paragraph": "[{\"end\":2914,\"start\":2060},{\"end\":3299,\"start\":2916},{\"end\":3822,\"start\":3350},{\"end\":5728,\"start\":3824},{\"end\":6699,\"start\":5730},{\"end\":7865,\"start\":6701},{\"end\":8225,\"start\":7867},{\"end\":9134,\"start\":8242},{\"end\":10003,\"start\":9136},{\"end\":11471,\"start\":10005},{\"end\":12382,\"start\":11473},{\"end\":12551,\"start\":12402},{\"end\":12744,\"start\":12553},{\"end\":12971,\"start\":12798},{\"end\":13157,\"start\":13033},{\"end\":13332,\"start\":13159},{\"end\":13568,\"start\":13377},{\"end\":13688,\"start\":13604},{\"end\":13921,\"start\":13748},{\"end\":14007,\"start\":13923},{\"end\":14099,\"start\":14071},{\"end\":14242,\"start\":14101},{\"end\":14472,\"start\":14323},{\"end\":14577,\"start\":14474},{\"end\":14652,\"start\":14634},{\"end\":15014,\"start\":14773},{\"end\":15206,\"start\":15045},{\"end\":15270,\"start\":15208},{\"end\":15831,\"start\":15354},{\"end\":15901,\"start\":15844},{\"end\":16197,\"start\":16032},{\"end\":16255,\"start\":16199},{\"end\":16395,\"start\":16334},{\"end\":16867,\"start\":16397},{\"end\":17012,\"start\":16888},{\"end\":17124,\"start\":17040},{\"end\":17243,\"start\":17172},{\"end\":17667,\"start\":17245},{\"end\":17773,\"start\":17698},{\"end\":18304,\"start\":17949},{\"end\":18449,\"start\":18394},{\"end\":18559,\"start\":18482},{\"end\":18715,\"start\":18610},{\"end\":19276,\"start\":18780},{\"end\":19507,\"start\":19365},{\"end\":19704,\"start\":19522},{\"end\":19777,\"start\":19730},{\"end\":19810,\"start\":19804},{\"end\":20542,\"start\":20005},{\"end\":20759,\"start\":20544},{\"end\":21419,\"start\":20790},{\"end\":21692,\"start\":21421},{\"end\":21745,\"start\":21694},{\"end\":22087,\"start\":21795},{\"end\":22219,\"start\":22120},{\"end\":22323,\"start\":22262},{\"end\":22436,\"start\":22387},{\"end\":22553,\"start\":22513},{\"end\":23689,\"start\":22569},{\"end\":24020,\"start\":23691},{\"end\":24517,\"start\":24022},{\"end\":27510,\"start\":24530},{\"end\":28348,\"start\":27512},{\"end\":29168,\"start\":28360},{\"end\":29676,\"start\":29197},{\"end\":30328,\"start\":29678},{\"end\":31256,\"start\":30343}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3349,\"start\":3300},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12797,\"start\":12745},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13032,\"start\":12972},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13376,\"start\":13333},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13603,\"start\":13569},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13747,\"start\":13689},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14070,\"start\":14008},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14322,\"start\":14243},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14633,\"start\":14578},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14772,\"start\":14653},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15044,\"start\":15015},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15353,\"start\":15271},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16031,\"start\":15902},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16333,\"start\":16256},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17039,\"start\":17013},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17171,\"start\":17125},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17697,\"start\":17668},{\"attributes\":{\"id\":\"formula_17\"},\"end\":17948,\"start\":17774},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18393,\"start\":18328},{\"attributes\":{\"id\":\"formula_19\"},\"end\":18481,\"start\":18450},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18609,\"start\":18560},{\"attributes\":{\"id\":\"formula_21\"},\"end\":18779,\"start\":18716},{\"attributes\":{\"id\":\"formula_22\"},\"end\":19364,\"start\":19277},{\"attributes\":{\"id\":\"formula_23\"},\"end\":19729,\"start\":19705},{\"attributes\":{\"id\":\"formula_24\"},\"end\":19803,\"start\":19778},{\"attributes\":{\"id\":\"formula_25\"},\"end\":20004,\"start\":19811},{\"attributes\":{\"id\":\"formula_26\"},\"end\":21794,\"start\":21746},{\"attributes\":{\"id\":\"formula_27\"},\"end\":22119,\"start\":22088},{\"attributes\":{\"id\":\"formula_28\"},\"end\":22261,\"start\":22220},{\"attributes\":{\"id\":\"formula_29\"},\"end\":22386,\"start\":22324},{\"attributes\":{\"id\":\"formula_30\"},\"end\":22512,\"start\":22437}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25189,\"start\":25182},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26301,\"start\":26294},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28543,\"start\":28536},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28869,\"start\":28862},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29735,\"start\":29728},{\"end\":30004,\"start\":29997},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30096,\"start\":30089}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2058,\"start\":2046},{\"attributes\":{\"n\":\"2.\"},\"end\":8240,\"start\":8228},{\"attributes\":{\"n\":\"3.\"},\"end\":12400,\"start\":12385},{\"end\":15842,\"start\":15834},{\"attributes\":{\"n\":\"3.1.\"},\"end\":16886,\"start\":16870},{\"end\":18327,\"start\":18307},{\"end\":19520,\"start\":19510},{\"attributes\":{\"n\":\"3.2.\"},\"end\":20788,\"start\":20762},{\"attributes\":{\"n\":\"4.\"},\"end\":22567,\"start\":22556},{\"attributes\":{\"n\":\"4.1.\"},\"end\":24528,\"start\":24520},{\"attributes\":{\"n\":\"4.2.\"},\"end\":28358,\"start\":28351},{\"attributes\":{\"n\":\"4.3.\"},\"end\":29195,\"start\":29171},{\"attributes\":{\"n\":\"5.\"},\"end\":30341,\"start\":30331},{\"end\":31268,\"start\":31258},{\"end\":31395,\"start\":31385},{\"end\":32286,\"start\":32276},{\"end\":32769,\"start\":32760},{\"end\":33359,\"start\":33350},{\"end\":33689,\"start\":33680},{\"end\":34094,\"start\":34085},{\"end\":34381,\"start\":34372}]", "table": "[{\"end\":32758,\"start\":32583},{\"end\":33348,\"start\":32849},{\"end\":33678,\"start\":33361},{\"end\":34083,\"start\":33734},{\"end\":34370,\"start\":34140},{\"end\":34968,\"start\":34471}]", "figure_caption": "[{\"end\":31383,\"start\":31270},{\"end\":32274,\"start\":31397},{\"end\":32497,\"start\":32288},{\"end\":32583,\"start\":32500},{\"end\":32849,\"start\":32771},{\"end\":33734,\"start\":33691},{\"end\":34140,\"start\":34096},{\"end\":34471,\"start\":34383}]", "figure_ref": "[{\"end\":2383,\"start\":2375},{\"end\":6526,\"start\":6520},{\"end\":7395,\"start\":7389},{\"end\":16679,\"start\":16673},{\"end\":18860,\"start\":18852},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27153,\"start\":27147},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27302,\"start\":27296},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27602,\"start\":27596}]", "bib_author_first_name": "[{\"end\":34998,\"start\":34991},{\"end\":35011,\"start\":35005},{\"end\":35223,\"start\":35215},{\"end\":35233,\"start\":35230},{\"end\":35247,\"start\":35239},{\"end\":35483,\"start\":35476},{\"end\":35494,\"start\":35488},{\"end\":35508,\"start\":35502},{\"end\":35522,\"start\":35515},{\"end\":35524,\"start\":35523},{\"end\":35748,\"start\":35741},{\"end\":35760,\"start\":35753},{\"end\":35776,\"start\":35768},{\"end\":35786,\"start\":35782},{\"end\":36032,\"start\":36026},{\"end\":36049,\"start\":36040},{\"end\":36297,\"start\":36289},{\"end\":36313,\"start\":36306},{\"end\":36324,\"start\":36321},{\"end\":36333,\"start\":36331},{\"end\":36727,\"start\":36723},{\"end\":36744,\"start\":36740},{\"end\":36764,\"start\":36756},{\"end\":36766,\"start\":36765},{\"end\":36985,\"start\":36980},{\"end\":36995,\"start\":36991},{\"end\":36999,\"start\":36996},{\"end\":37016,\"start\":37011},{\"end\":37216,\"start\":37209},{\"end\":37229,\"start\":37222},{\"end\":37242,\"start\":37235},{\"end\":37251,\"start\":37247},{\"end\":37263,\"start\":37256},{\"end\":37271,\"start\":37269},{\"end\":37521,\"start\":37514},{\"end\":37534,\"start\":37527},{\"end\":37547,\"start\":37540},{\"end\":37556,\"start\":37552},{\"end\":37769,\"start\":37761},{\"end\":37997,\"start\":37996},{\"end\":38028,\"start\":38019},{\"end\":38030,\"start\":38029},{\"end\":38042,\"start\":38035},{\"end\":38282,\"start\":38271},{\"end\":38301,\"start\":38292},{\"end\":38319,\"start\":38312},{\"end\":38571,\"start\":38567},{\"end\":38600,\"start\":38591},{\"end\":38615,\"start\":38609},{\"end\":38617,\"start\":38616},{\"end\":38631,\"start\":38625},{\"end\":38646,\"start\":38639},{\"end\":38877,\"start\":38876},{\"end\":38891,\"start\":38885},{\"end\":38906,\"start\":38900},{\"end\":39164,\"start\":39162},{\"end\":39175,\"start\":39171},{\"end\":39188,\"start\":39181},{\"end\":39198,\"start\":39193},{\"end\":39214,\"start\":39206},{\"end\":39522,\"start\":39520},{\"end\":39533,\"start\":39529},{\"end\":39547,\"start\":39539},{\"end\":39561,\"start\":39553},{\"end\":39789,\"start\":39787},{\"end\":39804,\"start\":39796},{\"end\":39814,\"start\":39811},{\"end\":39827,\"start\":39819},{\"end\":39837,\"start\":39833},{\"end\":40062,\"start\":40058},{\"end\":40078,\"start\":40071},{\"end\":40092,\"start\":40087},{\"end\":40108,\"start\":40101},{\"end\":40110,\"start\":40109},{\"end\":40324,\"start\":40320},{\"end\":40341,\"start\":40338},{\"end\":40351,\"start\":40348},{\"end\":40364,\"start\":40356},{\"end\":40380,\"start\":40373},{\"end\":40395,\"start\":40391},{\"end\":40407,\"start\":40400},{\"end\":40421,\"start\":40415},{\"end\":40438,\"start\":40432},{\"end\":40454,\"start\":40447},{\"end\":40475,\"start\":40466},{\"end\":40477,\"start\":40476},{\"end\":40486,\"start\":40484},{\"end\":40932,\"start\":40925},{\"end\":40948,\"start\":40942},{\"end\":40968,\"start\":40963},{\"end\":41233,\"start\":41227},{\"end\":41422,\"start\":41415},{\"end\":41437,\"start\":41429},{\"end\":41452,\"start\":41447},{\"end\":41466,\"start\":41461},{\"end\":41700,\"start\":41693},{\"end\":41709,\"start\":41707},{\"end\":41725,\"start\":41717},{\"end\":41741,\"start\":41735},{\"end\":41975,\"start\":41966},{\"end\":41988,\"start\":41985},{\"end\":42002,\"start\":41994},{\"end\":42014,\"start\":42008},{\"end\":42030,\"start\":42025},{\"end\":42032,\"start\":42031},{\"end\":42047,\"start\":42039},{\"end\":42065,\"start\":42058},{\"end\":42080,\"start\":42073},{\"end\":42098,\"start\":42092},{\"end\":42388,\"start\":42387},{\"end\":42395,\"start\":42394},{\"end\":42406,\"start\":42405},{\"end\":42418,\"start\":42417},{\"end\":42428,\"start\":42427},{\"end\":42621,\"start\":42617},{\"end\":42632,\"start\":42628},{\"end\":42647,\"start\":42640},{\"end\":42659,\"start\":42653},{\"end\":42914,\"start\":42911},{\"end\":42927,\"start\":42921},{\"end\":42939,\"start\":42934},{\"end\":42950,\"start\":42946},{\"end\":42961,\"start\":42955},{\"end\":42976,\"start\":42968},{\"end\":42990,\"start\":42983},{\"end\":42998,\"start\":42995},{\"end\":43279,\"start\":43278},{\"end\":43296,\"start\":43288},{\"end\":43298,\"start\":43297},{\"end\":43617,\"start\":43610},{\"end\":43630,\"start\":43623},{\"end\":43645,\"start\":43638},{\"end\":43652,\"start\":43650},{\"end\":43906,\"start\":43902},{\"end\":43908,\"start\":43907},{\"end\":43921,\"start\":43915},{\"end\":43923,\"start\":43922},{\"end\":43935,\"start\":43928},{\"end\":43937,\"start\":43936},{\"end\":43952,\"start\":43946},{\"end\":43954,\"start\":43953},{\"end\":44201,\"start\":44196},{\"end\":44215,\"start\":44208},{\"end\":44226,\"start\":44222}]", "bib_author_last_name": "[{\"end\":35003,\"start\":34999},{\"end\":35024,\"start\":35012},{\"end\":35228,\"start\":35224},{\"end\":35237,\"start\":35234},{\"end\":35257,\"start\":35248},{\"end\":35486,\"start\":35484},{\"end\":35500,\"start\":35495},{\"end\":35513,\"start\":35509},{\"end\":35530,\"start\":35525},{\"end\":35751,\"start\":35749},{\"end\":35766,\"start\":35761},{\"end\":35780,\"start\":35777},{\"end\":35790,\"start\":35787},{\"end\":36038,\"start\":36033},{\"end\":36057,\"start\":36050},{\"end\":36304,\"start\":36298},{\"end\":36319,\"start\":36314},{\"end\":36329,\"start\":36325},{\"end\":36341,\"start\":36334},{\"end\":36738,\"start\":36728},{\"end\":36754,\"start\":36745},{\"end\":36773,\"start\":36767},{\"end\":36989,\"start\":36986},{\"end\":37009,\"start\":37000},{\"end\":37022,\"start\":37017},{\"end\":37220,\"start\":37217},{\"end\":37233,\"start\":37230},{\"end\":37245,\"start\":37243},{\"end\":37254,\"start\":37252},{\"end\":37267,\"start\":37264},{\"end\":37276,\"start\":37272},{\"end\":37525,\"start\":37522},{\"end\":37538,\"start\":37535},{\"end\":37550,\"start\":37548},{\"end\":37561,\"start\":37557},{\"end\":37789,\"start\":37770},{\"end\":38006,\"start\":37998},{\"end\":38017,\"start\":38008},{\"end\":38033,\"start\":38031},{\"end\":38048,\"start\":38043},{\"end\":38060,\"start\":38050},{\"end\":38290,\"start\":38283},{\"end\":38310,\"start\":38302},{\"end\":38327,\"start\":38320},{\"end\":38589,\"start\":38572},{\"end\":38607,\"start\":38601},{\"end\":38623,\"start\":38618},{\"end\":38637,\"start\":38632},{\"end\":38652,\"start\":38647},{\"end\":38883,\"start\":38878},{\"end\":38898,\"start\":38892},{\"end\":38914,\"start\":38907},{\"end\":38925,\"start\":38916},{\"end\":39169,\"start\":39165},{\"end\":39179,\"start\":39176},{\"end\":39191,\"start\":39189},{\"end\":39204,\"start\":39199},{\"end\":39218,\"start\":39215},{\"end\":39527,\"start\":39523},{\"end\":39537,\"start\":39534},{\"end\":39551,\"start\":39548},{\"end\":39565,\"start\":39562},{\"end\":39794,\"start\":39790},{\"end\":39809,\"start\":39805},{\"end\":39817,\"start\":39815},{\"end\":39831,\"start\":39828},{\"end\":39841,\"start\":39838},{\"end\":40069,\"start\":40063},{\"end\":40085,\"start\":40079},{\"end\":40099,\"start\":40093},{\"end\":40118,\"start\":40111},{\"end\":40336,\"start\":40325},{\"end\":40346,\"start\":40342},{\"end\":40354,\"start\":40352},{\"end\":40371,\"start\":40365},{\"end\":40389,\"start\":40381},{\"end\":40398,\"start\":40396},{\"end\":40413,\"start\":40408},{\"end\":40430,\"start\":40422},{\"end\":40445,\"start\":40439},{\"end\":40464,\"start\":40455},{\"end\":40482,\"start\":40478},{\"end\":40494,\"start\":40487},{\"end\":40940,\"start\":40933},{\"end\":40961,\"start\":40949},{\"end\":40976,\"start\":40969},{\"end\":41238,\"start\":41234},{\"end\":41427,\"start\":41423},{\"end\":41445,\"start\":41438},{\"end\":41459,\"start\":41453},{\"end\":41473,\"start\":41467},{\"end\":41705,\"start\":41701},{\"end\":41715,\"start\":41710},{\"end\":41733,\"start\":41726},{\"end\":41750,\"start\":41742},{\"end\":41983,\"start\":41976},{\"end\":41992,\"start\":41989},{\"end\":42006,\"start\":42003},{\"end\":42023,\"start\":42015},{\"end\":42037,\"start\":42033},{\"end\":42056,\"start\":42048},{\"end\":42071,\"start\":42066},{\"end\":42090,\"start\":42081},{\"end\":42109,\"start\":42099},{\"end\":42392,\"start\":42389},{\"end\":42403,\"start\":42396},{\"end\":42415,\"start\":42407},{\"end\":42425,\"start\":42419},{\"end\":42437,\"start\":42429},{\"end\":42626,\"start\":42622},{\"end\":42638,\"start\":42633},{\"end\":42651,\"start\":42648},{\"end\":42663,\"start\":42660},{\"end\":42919,\"start\":42915},{\"end\":42932,\"start\":42928},{\"end\":42944,\"start\":42940},{\"end\":42953,\"start\":42951},{\"end\":42966,\"start\":42962},{\"end\":42981,\"start\":42977},{\"end\":42993,\"start\":42991},{\"end\":43002,\"start\":42999},{\"end\":43286,\"start\":43280},{\"end\":43309,\"start\":43299},{\"end\":43315,\"start\":43311},{\"end\":43621,\"start\":43618},{\"end\":43636,\"start\":43631},{\"end\":43648,\"start\":43646},{\"end\":43657,\"start\":43653},{\"end\":43913,\"start\":43909},{\"end\":43926,\"start\":43924},{\"end\":43944,\"start\":43938},{\"end\":43962,\"start\":43955},{\"end\":44206,\"start\":44202},{\"end\":44220,\"start\":44216},{\"end\":44232,\"start\":44227}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35148,\"start\":34970},{\"attributes\":{\"doi\":\"abs/1801.07698\",\"id\":\"b1\"},\"end\":35421,\"start\":35150},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52956249},\"end\":35693,\"start\":35423},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206594692},\"end\":35930,\"start\":35695},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5808102},\"end\":36228,\"start\":35932},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14342571},\"end\":36656,\"start\":36230},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":195908774},\"end\":36943,\"start\":36658},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13965167},\"end\":37146,\"start\":36945},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206596594},\"end\":37451,\"start\":37148},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1829423},\"end\":37716,\"start\":37453},{\"attributes\":{\"id\":\"b10\"},\"end\":37949,\"start\":37718},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":24718057},\"end\":38230,\"start\":37951},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":32493971},\"end\":38517,\"start\":38232},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":17861456},\"end\":38851,\"start\":38519},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4637184},\"end\":39054,\"start\":38853},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8699669},\"end\":39451,\"start\":39056},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2473565},\"end\":39730,\"start\":39453},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":44121346},\"end\":40002,\"start\":39732},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1136006},\"end\":40267,\"start\":40004},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2930547},\"end\":40857,\"start\":40269},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206592766},\"end\":41156,\"start\":40859},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":911406},\"end\":41369,\"start\":41158},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206595144},\"end\":41629,\"start\":41371},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5726681},\"end\":41932,\"start\":41631},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206592484},\"end\":42326,\"start\":41934},{\"attributes\":{\"id\":\"b25\"},\"end\":42568,\"start\":42328},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9683805},\"end\":42848,\"start\":42570},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":68589},\"end\":43201,\"start\":42850},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":47325215},\"end\":43538,\"start\":43203},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4711865},\"end\":43821,\"start\":43540},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2643381},\"end\":44156,\"start\":43823},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7849657},\"end\":44358,\"start\":44158}]", "bib_title": "[{\"end\":35474,\"start\":35423},{\"end\":35739,\"start\":35695},{\"end\":36024,\"start\":35932},{\"end\":36287,\"start\":36230},{\"end\":36721,\"start\":36658},{\"end\":36978,\"start\":36945},{\"end\":37207,\"start\":37148},{\"end\":37512,\"start\":37453},{\"end\":37994,\"start\":37951},{\"end\":38269,\"start\":38232},{\"end\":38565,\"start\":38519},{\"end\":38874,\"start\":38853},{\"end\":39160,\"start\":39056},{\"end\":39518,\"start\":39453},{\"end\":39785,\"start\":39732},{\"end\":40056,\"start\":40004},{\"end\":40318,\"start\":40269},{\"end\":40923,\"start\":40859},{\"end\":41225,\"start\":41158},{\"end\":41413,\"start\":41371},{\"end\":41691,\"start\":41631},{\"end\":41964,\"start\":41934},{\"end\":42615,\"start\":42570},{\"end\":42909,\"start\":42850},{\"end\":43276,\"start\":43203},{\"end\":43608,\"start\":43540},{\"end\":43900,\"start\":43823},{\"end\":44194,\"start\":44158}]", "bib_author": "[{\"end\":35005,\"start\":34991},{\"end\":35026,\"start\":35005},{\"end\":35230,\"start\":35215},{\"end\":35239,\"start\":35230},{\"end\":35259,\"start\":35239},{\"end\":35488,\"start\":35476},{\"end\":35502,\"start\":35488},{\"end\":35515,\"start\":35502},{\"end\":35532,\"start\":35515},{\"end\":35753,\"start\":35741},{\"end\":35768,\"start\":35753},{\"end\":35782,\"start\":35768},{\"end\":35792,\"start\":35782},{\"end\":36040,\"start\":36026},{\"end\":36059,\"start\":36040},{\"end\":36306,\"start\":36289},{\"end\":36321,\"start\":36306},{\"end\":36331,\"start\":36321},{\"end\":36343,\"start\":36331},{\"end\":36740,\"start\":36723},{\"end\":36756,\"start\":36740},{\"end\":36775,\"start\":36756},{\"end\":36991,\"start\":36980},{\"end\":37011,\"start\":36991},{\"end\":37024,\"start\":37011},{\"end\":37222,\"start\":37209},{\"end\":37235,\"start\":37222},{\"end\":37247,\"start\":37235},{\"end\":37256,\"start\":37247},{\"end\":37269,\"start\":37256},{\"end\":37278,\"start\":37269},{\"end\":37527,\"start\":37514},{\"end\":37540,\"start\":37527},{\"end\":37552,\"start\":37540},{\"end\":37563,\"start\":37552},{\"end\":37791,\"start\":37761},{\"end\":38008,\"start\":37996},{\"end\":38019,\"start\":38008},{\"end\":38035,\"start\":38019},{\"end\":38050,\"start\":38035},{\"end\":38062,\"start\":38050},{\"end\":38292,\"start\":38271},{\"end\":38312,\"start\":38292},{\"end\":38329,\"start\":38312},{\"end\":38591,\"start\":38567},{\"end\":38609,\"start\":38591},{\"end\":38625,\"start\":38609},{\"end\":38639,\"start\":38625},{\"end\":38654,\"start\":38639},{\"end\":38885,\"start\":38876},{\"end\":38900,\"start\":38885},{\"end\":38916,\"start\":38900},{\"end\":38927,\"start\":38916},{\"end\":39171,\"start\":39162},{\"end\":39181,\"start\":39171},{\"end\":39193,\"start\":39181},{\"end\":39206,\"start\":39193},{\"end\":39220,\"start\":39206},{\"end\":39529,\"start\":39520},{\"end\":39539,\"start\":39529},{\"end\":39553,\"start\":39539},{\"end\":39567,\"start\":39553},{\"end\":39796,\"start\":39787},{\"end\":39811,\"start\":39796},{\"end\":39819,\"start\":39811},{\"end\":39833,\"start\":39819},{\"end\":39843,\"start\":39833},{\"end\":40071,\"start\":40058},{\"end\":40087,\"start\":40071},{\"end\":40101,\"start\":40087},{\"end\":40120,\"start\":40101},{\"end\":40338,\"start\":40320},{\"end\":40348,\"start\":40338},{\"end\":40356,\"start\":40348},{\"end\":40373,\"start\":40356},{\"end\":40391,\"start\":40373},{\"end\":40400,\"start\":40391},{\"end\":40415,\"start\":40400},{\"end\":40432,\"start\":40415},{\"end\":40447,\"start\":40432},{\"end\":40466,\"start\":40447},{\"end\":40484,\"start\":40466},{\"end\":40496,\"start\":40484},{\"end\":40942,\"start\":40925},{\"end\":40963,\"start\":40942},{\"end\":40978,\"start\":40963},{\"end\":41240,\"start\":41227},{\"end\":41429,\"start\":41415},{\"end\":41447,\"start\":41429},{\"end\":41461,\"start\":41447},{\"end\":41475,\"start\":41461},{\"end\":41707,\"start\":41693},{\"end\":41717,\"start\":41707},{\"end\":41735,\"start\":41717},{\"end\":41752,\"start\":41735},{\"end\":41985,\"start\":41966},{\"end\":41994,\"start\":41985},{\"end\":42008,\"start\":41994},{\"end\":42025,\"start\":42008},{\"end\":42039,\"start\":42025},{\"end\":42058,\"start\":42039},{\"end\":42073,\"start\":42058},{\"end\":42092,\"start\":42073},{\"end\":42111,\"start\":42092},{\"end\":42394,\"start\":42387},{\"end\":42405,\"start\":42394},{\"end\":42417,\"start\":42405},{\"end\":42427,\"start\":42417},{\"end\":42439,\"start\":42427},{\"end\":42628,\"start\":42617},{\"end\":42640,\"start\":42628},{\"end\":42653,\"start\":42640},{\"end\":42665,\"start\":42653},{\"end\":42921,\"start\":42911},{\"end\":42934,\"start\":42921},{\"end\":42946,\"start\":42934},{\"end\":42955,\"start\":42946},{\"end\":42968,\"start\":42955},{\"end\":42983,\"start\":42968},{\"end\":42995,\"start\":42983},{\"end\":43004,\"start\":42995},{\"end\":43288,\"start\":43278},{\"end\":43311,\"start\":43288},{\"end\":43317,\"start\":43311},{\"end\":43623,\"start\":43610},{\"end\":43638,\"start\":43623},{\"end\":43650,\"start\":43638},{\"end\":43659,\"start\":43650},{\"end\":43915,\"start\":43902},{\"end\":43928,\"start\":43915},{\"end\":43946,\"start\":43928},{\"end\":43964,\"start\":43946},{\"end\":44208,\"start\":44196},{\"end\":44222,\"start\":44208},{\"end\":44234,\"start\":44222}]", "bib_venue": "[{\"end\":36430,\"start\":36413},{\"end\":34989,\"start\":34970},{\"end\":35213,\"start\":35150},{\"end\":35536,\"start\":35532},{\"end\":35796,\"start\":35792},{\"end\":36063,\"start\":36059},{\"end\":36411,\"start\":36343},{\"end\":36779,\"start\":36775},{\"end\":37028,\"start\":37024},{\"end\":37282,\"start\":37278},{\"end\":37567,\"start\":37563},{\"end\":37759,\"start\":37718},{\"end\":38066,\"start\":38062},{\"end\":38357,\"start\":38329},{\"end\":38658,\"start\":38654},{\"end\":38931,\"start\":38927},{\"end\":39236,\"start\":39220},{\"end\":39571,\"start\":39567},{\"end\":39847,\"start\":39843},{\"end\":40124,\"start\":40120},{\"end\":40543,\"start\":40496},{\"end\":40982,\"start\":40978},{\"end\":41244,\"start\":41240},{\"end\":41479,\"start\":41475},{\"end\":41756,\"start\":41752},{\"end\":42115,\"start\":42111},{\"end\":42385,\"start\":42328},{\"end\":42690,\"start\":42665},{\"end\":43008,\"start\":43004},{\"end\":43353,\"start\":43317},{\"end\":43663,\"start\":43659},{\"end\":43968,\"start\":43964},{\"end\":44238,\"start\":44234}]"}}}, "year": 2023, "month": 12, "day": 17}