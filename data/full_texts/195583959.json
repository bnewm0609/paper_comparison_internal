{"id": 195583959, "updated": "2023-09-28 05:13:46.587", "metadata": {"title": "HoVer-Net: Simultaneous Segmentation and Classi\ufb01cation of Nuclei in Multi-Tissue Histology Images", "authors": "[{\"first\":\"Simon\",\"last\":\"Graham\",\"middle\":[]},{\"first\":\"Quoc\",\"last\":\"Vu\",\"middle\":[\"Dang\"]},{\"first\":\"Shan\",\"last\":\"Raza\",\"middle\":[\"E\",\"Ahmed\"]},{\"first\":\"Ayesha\",\"last\":\"Azam\",\"middle\":[]},{\"first\":\"Yee\",\"last\":\"Tsang\",\"middle\":[\"Wah\"]},{\"first\":\"Jin\",\"last\":\"Kwak\",\"middle\":[\"Tae\"]},{\"first\":\"Nasir\",\"last\":\"Rajpoot\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Medical image analysis", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Nuclear segmentation and classi\ufb01cation within Haematoxylin & Eosin stained histology images is a fundamental prerequisite in the digital pathology work-\ufb02ow. The development of automated methods for nuclear segmentation and classi\ufb01cation enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation and classi\ufb01cation is faced with a major challenge in that there are several dif-ferent types of nuclei, some of them exhibiting large intra-class variability such as the nuclei of tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for simultaneous nuclear segmentation and classi\ufb01cation that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. Then, for each segmented instance the network predicts the type of nucleus via a devoted up-sampling branch. We", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1812.06499", "mag": "2974825848", "acl": null, "pubmed": "31561183", "pubmedcentral": null, "dblp": "journals/mia/GrahamVRATKR19", "doi": "10.1016/j.media.2019.101563"}}, "content": {"source": {"pdf_hash": "8b1acdf7228951f81efca212876b33e0a5a222e4", "pdf_src": "Anansi", "pdf_uri": "[\"http://wrap.warwick.ac.uk/126044/1/WRAP-HoVer-Net-simultaneous-segmentation-classification-images-Graham-2019.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBYNCND", "open_access_url": "http://wrap.warwick.ac.uk/126044/1/WRAP-HoVer-Net-simultaneous-segmentation-classification-images-Graham-2019.pdf", "status": "GREEN"}}, "grobid": {"id": "3df4526f15a1ef024b679a33524fdaa1a68f56f8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8b1acdf7228951f81efca212876b33e0a5a222e4.txt", "contents": "\nHoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images\n\n\nSimon Graham \nMathematics for Real World Systems Centre for Doctoral Training\nUniversity of Warwick\nUK\n\nDepartment of Computer Science\nUniversity of Warwick\nUK\n\nQuoc Dang Vu \nDepartment of Computer Science and Engineering\nSejong University\nSouth Korea\n\nShan E \nAhmed Raza \nDepartment of Computer Science\nUniversity of Warwick\nUK\n\nCentre for Evolution and Cancer & Division of Molecular Pathology\nThe Institute of Cancer Research\nLondonUK\n\nAyesha Azam \nDepartment of Computer Science\nUniversity of Warwick\nUK\n\nUniversity Hospitals Coventry and Warwickshire\nCoventryUK\n\nYee Wah Tsang \nUniversity Hospitals Coventry and Warwickshire\nCoventryUK\n\nJin Tae Kwak \nDepartment of Computer Science and Engineering\nSejong University\nSouth Korea\n\nNasir Rajpoot \nDepartment of Computer Science\nUniversity of Warwick\nUK\n\nThe Alan Turing Institute\nLondonUK\n\nHoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images\nPreprint submitted to Medical Image Analysis September 17, 2019warwick.ac.uk/lib-publications Manuscript version: Author's Accepted Manuscript The version presented in WRAP is the author's accepted manuscript and may differ from the published version or Version of Record. Persistent WRAP URL: How to cite: Please refer to published version for the most recent bibliographic citation information. If a published version is known of, the repository item page linked to above, will contain details on accessing it. Copyright and reuse: The Warwick Research Archive Portal (WRAP) makes this work by researchers of the University of Warwick available open access under the following conditions. \u00a9 2019 Elsevier. Licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International http://creativecommons.org/licenses/by-nc-nd/4.0/. Publisher's statement: Please refer to the repository item page, publisher's statement section, for further information. For more information, please contact the WRAP Team at: wrap@warwick.ac.uk. * First authors contributed equally + Last authors contributed equallyNuclear segmentationnuclear classificationcomputational pathologydeep learning\nNuclear segmentation and classification within Haematoxylin & Eosin stainedhistology images is a fundamental prerequisite in the digital pathology workflow. The development of automated methods for nuclear segmentation and classification enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation and classification is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such as the nuclei of tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for simultaneous nuclear segmentation and classification that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. Then, for each segmented instance the network predicts the type of nucleus via a devoted up-sampling branch. WeHere, the type of nucleus refers to the cell type in which it is located. For example, nuclear features can be used to predict survival(Alsubaie et al., 2018)and also for diagnosing the grade and type of disease(Lu et al., 2018). Also, efficient and accurate detection and segmentation of nuclei can facilitate good quality tissue segmentationJaved et al., 2018), 15    which can in turn not only facilitate the quantification of WSIs but may also serve as an important step in understanding how each tissue component contributes to disease. In order to use nuclear features for downstream analysis within computational pathology, nuclear segmentation must be carried out as an initial step. However, this remains a challenge because nuclei display a high 20 level of heterogeneity and there is significant inter-and intra-instance variability in the shape, size and chromatin pattern between and within different cell 45 parative results on six independent multi-tissue histology image datasets and demonstrate state-of-the-art performance compared to other recently proposed methods. The main contributions of this work are listed as follows: \u2022 A novel network, targeted at simultaneous segmentation and classification of nuclei, where horizontal and vertical distance map predictions separate 50 1 Model code available at: https://github.com/vqdang/hover net 95 sist separation of instances. However, it often struggles to split neighbouring instances and is highly sensitive to pre-defined parameters in the weighted loss function. A more recently proposed method in Micro-Net (Raza et al., 2018) extends U-Net by utilising an enhanced network architecture with weighted loss. The network processes the input at multiple resolutions and as a result, gains 100 robustness against nuclei with varying size. In Graham and Rajpoot (2018), the authors developed a network that is robust to stain variations in H&E images by introducing a weighted loss function that is sensitive to the Haematoxylin intensity within the image. Other methods exploit information about the nuclear contour (or bound-105 155MethodsOur overall framework for automatic nuclear instance segmentation and classification can be observed inFig. 1and the proposed network inFig. 2. Here, nuclear pixels are first detected and then, a tailored post-processing pipeline is used to simultaneously segment nuclear instances and obtain the corresponding 160 nuclear types. The framework is based upon the horizontal and vertical distance maps, which can be seen inFig. 3. In the figure, each nuclear pixel denotes either the horizontal or vertical distance of pixels to their centres of mass.\n\ndemonstrate state-of-the-art performance compared to other methods on multiple independent multi-tissue histology image datasets. As part of this work, we introduce a new dataset of Haematoxylin & Eosin stained colorectal adenocarcinoma image tiles, containing 24,319 exhaustively annotated nuclei with associated class labels.\n\nKeywords: Nuclear segmentation, nuclear classification, computational pathology, deep learning\n\n\nIntroduction\n\nCurrent manual assessment of Haematoxylin and Eosin (H&E) stained histology slides suffers from low throughput and is naturally prone to intra-and inter-observer variability (Elmore et al., 2015). To overcome the difficulty in visual assessment of tissue slides, there is a growing interest in digital pathology 5 (DP), where digitised whole-slide images (WSIs) are acquired from glass histology slides using a scanning device. This permits efficient processing, analysis and management of the tissue specimens (Madabhushi and Lee, 2016). Each WSI contains tens of thousands of nuclei of various types, which can be further analysed in a systematic manner and used for predicting clinical outcome. types, disease types or even from one region to another within a single tissue sample. Tumour nuclei, in particular, tend to be present in clusters, which gives rise to many overlapping instances, providing a further challenge for automated 25 segmentation, due to the difficulty of separating neighbouring instances.\n\nAs well as extracting each individual nucleus, determining the type of each nucleus can increase the diagnostic potential of current DP pipelines. For example, accurately classifying each nucleus to be from tumour or lymphocyte enables downstream analysis of tumour infiltrating lymphocytes (TILs), which 30 have been shown to be predictive of cancer recurrence (Corredor et al., 2019).\n\nYet, similar to nuclear segmentation, classifying the type of each nucleus is difficult, due to the high variance of nuclear appearance within each WSI. Typically, nuclei are classified using two disjoint models: one for detecting each nucleus and then another for performing nuclear classification (Sharma et al., 2015;Wang 35 et al., 2016). However, it would be preferable to utilise a single unified model for nuclear instance segmentation and classification.\n\nIn this paper, we present a deep learning approach 1 for simultaneous segmentation and classification of nuclear instances in histology images. The network is based on the prediction of horizontal and vertical distances (and hence the 40 name HoVer-Net) of nuclear pixels to their centres of mass, which are subsequently leveraged to separate clustered nuclei. For each segmented instance, the nuclear type is subsequently determined via a dedicated up-sampling branch.\n\nTo the best of our knowledge, this is the first approach that achieves instance segmentation and classification within the same network. We present com-clustered nuclei.\n\n\u2022 We show that the proposed HoVer-Net achieves state-of-the-art performance on multiple H&E histology image datasets, as compared to over a dozen recently published methods.\n\n\u2022 An interpretable and reliable evaluation framework that effectively quan-55 tifies nuclear segmentation performance and overcomes the limitations of existing performance measures.\n\n\u2022 A new dataset 2 of 24,319 exhaustively annotated nuclei within 41 colorectal adenocarcinoma image tiles. \n\n\nRelated Work\n\n\nNuclear Instance Segmentation\n\nWithin the current literature, energy-based methods, in particular the watershed algorithm, have been widely utilised to segment nuclear instances.\n\nFor example, Yang et al. (2006) used thresholding to obtain the markers and the energy landscape as input for watershed to extract the nuclear instances. 65 Nonetheless, thresholding relies on a consistent difference in intensity between the nuclei and background, which does not hold for more complex images and hence often produces unreliable results. Various approaches have tried to provide an improved marker for marker-controlled watershed. Cheng et al. (2009) used active contours to obtain the markers. Veta et al. (2013) used a series 70 of morphological operations to generate the energy landscape. However, these methods rely on the predefined geometry of the nuclei to generate the markers, which determines the overall accuracy of each method. Notably, Ali and Madabhushi (2012) avoided the trouble of refining the markers for watershed by designing a method that relies solely on the energy landscape. They combined an active contour approach with nuclear shape modelling via a level-set method to obtain the nuclear instances. Despite its widespread usage, obtaining sufficiently strong markers for watershed is a non-trivial task. Some methods have departed from the energy-based approach by utilising the geometry of the nuclei. For instance, Wienert et al. (2012), LaTorre et al. (2013) and Kwak 80 et al. (2015) computed the concavity of nuclear clusters, while Liao et al. (2016) used eclipse-fitting to separate the clusters. However, this assumes a predefined shape, which does not encompass the natural diversity of the nuclei. In addition, these methods tend to be sensitive to the choice of manually selected parameters. 85 Recently, deep learning methods have received a surge of interest due to their superior performance in many computer vision tasks (Litjens et al., 2017;Shen et al., 2017;LeCun et al., 2015). These approaches are capable of automatically extracting a representative set of features, that strongly correlate with the task at hand. As a result, they are preferable to hand-crafted approaches, 90 that rely on a selection of pre-defined features. Inspired by the Fully Convolutional Network (FCN) (Long et al., 2015), U-Net (Ronneberger et al., 2015) has been successfully applied to numerous segmentation tasks in medical image analysis. The network has an encoder-decoder design with skip connections to incorporate low-level information and uses a weighted loss function to as-ary) within the network, such as DCAN  that utilised a dual architecture that outputs the nuclear cluster and the nuclear contour as two separate prediction maps. Instance segmentation is then achieved by subtracting the contour from the nuclear cluster prediction. Cui et al. (2018) proposed a network to predict the inner nuclear instance, the nuclear contour and the back-110 ground. The network utilised a customised weighted loss function based on the relative position of pixels within the image to improve and stabilise the inner nuclei and contour prediction. Some other methods have also utilised the nuclear contour to achieve instance segmentation. For example, Kumar et al. (2017) employed a deep learning technique for labelling the nuclei and the contours, 115 followed by a region growing approach to extract the final instances. Khoshdeli and Parvin (2018) used the contour predictions as input into a further network for segmentation refinement. Zhou et al. (2019) proposed CIA-Net, that utilises a multi-level information aggregation module between two task-specific decoders, where each decoder segments either the nuclei or the contours. A Deep 120 Residual Aggregation Network (DRAN) was proposed by Vu et al. (2018) that uses a multi-scale strategy, incorporating both the nuclei and nuclear contours to accurately segment nuclei.\n\nThere have been various other methods to achieve instance separation. Instead of considering the contour, Naylor et al. (2018) proposed a deep learning 125 approach to detect superior markers for watershed by regressing the nuclear distance map. Therefore, the network avoids making a prediction for areas with indistinct contours.\n\nIn line with these developments, the field of instance segmentation within natural images is also rapidly progressing and have had a significant influence 130 on nuclear instance segmentation methods. A notable example is Mask-RCNN (He et al., 2017), where instance segmentation approach is achieved by first predicting candidate regions likely to contain an object and then deep learning based segmentation within those proposed regions.\n\n\nNuclear Classification\n\nAs well as performing instance segmentation, it is desirable to determine the type of each nucleus to facilitate and improve downstream analysis. It is possible for current models to differentiate between certain nuclear types in H&E, however sub-typing of lymphocytes is an extremely hard task due to the high levels of similarity in morphological appearance between T and B lymphocytes.\n\n\n140\n\nTypically, classifying each nucleus is done via a two-stage approach, where the first step involves either nuclear segmentation or nuclear detection. When segmentation is used as the initial step, a series of morphological and textural features are extracted from each instance, which are then used within a classifier to determine the nuclei classes. For example, Nguyen et al. (2011) classified 145 nuclei within H&E stained breast cancer images as either tumour, lymphocyte or stromal based on their morphological features. Yuan et al. (2012) performed nuclear segmentation and then classified each nucleus with AdaBoost classifier, utilising the intensity, morphology and texture of nuclei as features. Otherwise, detection is performed as an initial step and a patch centred at the point of 150 detection is fed into a classifier, to predict the type of nucleus. Sirinukunwattana et al. (2016) proposed a spatially constrained CNN, that initially detects all nuclei and then for each nucleus an ensemble of associated patches are fed into a CNN to predict the type to be either epithelial, inflammatory, fibroblast or miscellaneous. \n\n\nNetwork Architecture\n\nIn order to extract a strong and representative set of features, we employ 165 a deep neural network. The feature extraction component of the network is inspired by the pre-activated residual network with 50 layers (He et al., 2016) (Preact-ResNet50), due to its excellent performance in recent computer vision tasks (Deng et al., 2009) and robustness against input perturbation (Arnab et al., 2017). Compared to the standard Preact-ResNet50 implementation, we 170 reduce the total down-sampling factor from 32 to 8 by using a stride of 1 in the first convolution and removing the subsequent max-pooling operation. This ensures that there is no immediate loss of information that is important for performing an accurate segmentation. Various residual units are applied throughout the network at different down-sampling levels. A series of consecutive resid-175 ual units is denoted as a residual block. The number of residual units within each residual block is 3, 4, 6 and 3 that are applied at down-sampling levels 1, 2, 4 and 8 respectively. For clarity, a down-sampling level of 2 means that the input has a reduction in the spatial resolution by a factor of 2.  branch. The NP branch predicts whether or not a pixel belongs to the nuclei or background, whereas the HoVer branch predicts the horizontal and vertical dis-185 tances of nuclear pixels to their centres of mass. Then, the NC branch predicts the type of nucleus for each pixel. In particular, the NP and HoVer branches jointly achieve nuclear instance segmentation by first separating nuclear pixels from the background (NP branch) and then separating touching nuclei (HoVer branch). The NC branch determines the type of each nucleus by aggregating 190 the pixel-level nuclear type predictions within each instance.\n\nAll three up-sampling branches utilise the same architectural design, which consists of a series of up-sampling operations and densely connected units (Huang et al., 2016) (or dense units). By stacking multiple and relatively cheap dense units, we build a large receptive field with minimal parameters, compared to us-195 ing a single convolution with a larger kernel size and we ensure efficient gradient propagation. We use skip connections (Ronneberger et al., 2015) to incorporate features from the encoder, but utilise summation as opposed to concatenation. The consideration of low-level information is particularly important in segmentation tasks, where we aim to precisely delineate the object boundaries. We use \n\n\nLoss Function\n\nThe proposed network design has 4 different sets of weights: w 0 , w 1 , w 2 and w 3 which refer to the weights of the Preact-ResNet50 encoder, the HoVer branch decoder, the NP branch decoder and the NC branch decoder. These 4 sets of weights are optimised jointly using the loss L defined as:\nL = \u03bbaLa + \u03bb b L b HoVer Branch + \u03bbcLc + \u03bb d L d NP Branch + \u03bbeLe + \u03bb f L f NC Branch (1)\nwhere L a and L b represent the regression loss with respect to the output of the HoVer branch, L c and L d represent the loss with respect to the output at the NP branch and and finally, L e and L f represent the loss with respect to the 230 output at the NC branch. We choose to use two different loss functions at the output of each branch for an overall superior performance. \u03bb a ...\u03bb f are scalars that give weight to each associated loss function. Specifically, we set \u03bb b to 2 and the other scalars to 1, based on empirical selection.\n\nGiven the input image I, at each pixel i we define p i (I, w 0 , w 1 ) as the re-denote the pixel-based softmax predictions of the NP and NC branches respectively. We also define \u0393 i (I), \u03a8 i (I) and \u03a6 i (I) as their corresponding ground truth At the output of the HoVer branch, we compute a multiple term regression 250 loss. We denote L a as the mean squared error between the predicted horizontal and vertical distances and the GT. We also propose a novel loss function L b that calculates the mean squared error between the horizontal and vertical gradients of the horizontal and vertical maps respectively and the corresponding gradients of the GT. We formally define L a and L b as:\n255 La = 1 n n i=1 (pi(I; w0, w1) \u2212 \u0393i(I)) 2 (2) L b = 1 m i\u2208M (\u2207x(pi,x(I; w0, w1)) \u2212 \u2207x(\u0393i,x(I))) 2 + 1 m i\u2208M (\u2207y(pi,y(I; w0, w1)) \u2212 \u2207y(\u0393i,y(I))) 2(3)\nWithin equation (3), \u2207 x and \u2207 y denote the gradient in the horizontal x and vertical y directions respectively. m denotes total number of nuclear pixels within the image and M denotes the set containing all nuclear pixels.\n\nAt the output of NP and NC branches, we calculate the cross-entropy loss 260 (L c and L e ) and the dice loss (L d and L f ). These two losses are then added together to give the overall loss of each branch. Concretely, we define the cross entropy and dice losses as:\nCE = \u2212 1 n N i=1 K k=1 X i,k (I) log Y i,k (I) (4) Dice = 1 \u2212 2 \u00d7 N i=1 (Yi(I) \u00d7 Xi(I)) + N i=1 Yi(I) + N i=1 Xi(I) + (5)\nwhere X is the ground truth, Y is the prediction, K is the number of classes 265 and is a smoothness constant which we set to 1.0e \u22123 . When calculating L c and L d for NP branch, for a given pixel i, we set X i and Y i as q i (I, w 0 , w 2 ) and \u03a8 i respectively. For L c , we set K to be 2 within equation (4) because the task of the branch is to perform binary nuclear segmentation. Similarly, for L e and L f at NC branch, for a given pixel i, we substitute X i for \u03a6 i (I) and Y i for (4) and (5). K is set as 5 within equation (4) when calculating L e , denoting the 4 types of nuclei that our model currently predicts and the background. Note, the value of K is chosen to reflect the number of nuclear types represented in the training set.\n270 r i (I, w 0 , w 3 ) in equations\nIt must be noted that the NC branch loss L e and L f are only calculated 275 when the classification labels are available. In other words, as mentioned in Section 3.1, the network performs only instance segmentation if there are no classification labels given.\n\n\nPost Processing\n\nWithin each horizontal and vertical map, pixels between separate instances 280 have a significant difference. This can be seen in Fig. 3 and is highlighted by the arrows. Therefore, calculating the gradient can inform where the nuclei should be separated because the output will give high values between neighbouring nuclei, where there is a significant difference in the pixel values. We define:\nS m = max(H x (p x ), H y (p y ))(6)\nwhere p x and p y refer to the the horizontal and vertical predictions at the output \n= \u03c3(\u03c4 (q, h) \u2212 \u03c4 (S m , k)). Here, \u03c4 (a, b) is\na threshold function that acts on a and sets values above b to 1 or 0 otherwise. Specifically, h and k were chosen such that they gave the optimal nuclear segmentation results. \u03c3 is a rectifier that sets all negative values to 0 and q 295 is the probability map output of the NP branch. We obtain the energy land-\nscape E = [1 \u2212 \u03c4 (S m , k)] * \u03c4 (q, h).\nFinally, M is used as the marker during marker-controlled watershed to determine how to split \u03c4 (q, h), given the energy landscape E. This sequence of events can be seen in Fig. 1.\n\nTo perform simultaneous nuclear instance segmentation and classification, it 300 is necessary to convert the per-pixel nuclear type prediction at the output of the NC branch to a prediction per nuclear instance. For each nuclear instance, we use majority class of the predictions made by the NC branch, i.e., the nuclear type of all pixels in an instance is assigned to be the class with the highest frequency count for that nuclear instance.\n\n\n305\n\nPlease refer to Appendix A for a full analysis on the contribution of our proposed loss function, post-processing method and devoted classification branch.\n\n\nEvaluation Metrics\n\n\nNuclear Instance Segmentation Evaluation\n\nAssessment and comparison of different methods is usually given by an over-310 all score that indicates which method is superior. However, to further investigate the method, it is preferable to break the problem into sub-tasks and measure the performance of the method on each sub-task. This enables an in depth analysis, thus facilitating a comprehensive understanding of the approach, which can help drive forward model development. For nuclear instance segmentation, the  In the current literature, two evaluation metrics have been mainly adopted to 320 quantitatively measure the performance of nuclear instance segmentation: 1)\n\nEnsemble Dice (DICE2) (Vu et al., 2018), and 2) Aggregated Jaccard Index (AJI) (Kumar et al., 2017). Given the ground truth X and prediction Y , DICE2\n\ncomputes and aggregates DICE per nucleus, where Dice coefficient (DICE) is defined as 2\u00d7(X\u2229Y )/(|X|+|Y |) and AJI computes the ratio of an aggregated 325 intersection cardinality and an aggregated union cardinality between X and Y .\n\nThese two evaluation metrics only provide an overall score for the instance segmentation quality and therefore provides no further insight into the sub-tasks at hand. In addition, these two metrics have a limitation, which we illustrate in Fig. 4. From the figure, although prediction A only differs from predic-330 tion B by a few pixels, the DICE2 and AJI scores for B are inferior. These scores are shown in Table 1. This problem arises due to over-penalisation of the overlapping regions. By overlaying the GT segment contours (red dashed line) upon the two predictions, we observe that, although the cyan-coloured instance within prediction A overlaps mostly with the cyan-coloured GT instance, it also 335 slightly overlaps with the blue-coloured GT instance. As a result, according to the DICE2 algorithm, the predicted cyan instance will be penalised by pixels not only coming from the dominant overlapping cyan-coloured GT instance, but also from the blue-coloured GT instance. The AJI also suffers from the same phenomenon. However, because AJI only uses the prediction and GT instance 340 pair with the highest intersection over union, over-penalisation is less likely compared to DICE2. Over-penalisation is likely to occur when the model completely fails to detect the neighbouring instance, such as in Fig. 4 Panoptic Quality: We propose to use another metric for accurate quantification and interpretability to assess the performance of nuclear instance seg-350 mentation. Originally proposed by Kirillov et al. (2018), panoptic quality (PQ)\n\nfor nuclear instance segmentation is defined as:\nPQ = |T P | |T P | + 1 2 |F P | + 1 2 |F N | Detection Quality(DQ) \u00d7 (x,y)\u2208T P IoU (x, y) |T P | Segmentation Quality(SQ)(7)\nwhere x denotes a GT segment, y denotes a prediction segment and IoU denotes intersection over union. Each (x,y) pair is mathematically proven to be unique (Kirillov et al., 2018) \n\n\nNuclear Classification Evaluation\n\nClassification of the type of each nucleus is performed within the nuclear in-375 stances extracted from the instance segmentation or detection tasks. Therefore, the overall measurement for nuclear type classification should also encompass these two tasks. For all nuclear instances of a particular type t from both the ground truth and the prediction, the detection task d splits the GT and predicted instances into the following subsets: correctly detected instances (TP d ), 380 misdetected GT instances (FN d ) and overdetected predicted instances (FP d ).\n\nSubsequently, the classification task c further breaks TP d into correctly classified instances of type t (TP c ), correctly classified instances of types other than type t (TN c ), incorrectly classified instances of type t (FP c ) and incorrectly classified instances of types other than type t (FN c ). We then define the F c score of each type t for combined nuclear type classification and detection as follows:\nF t c = 2(T P c + T N c ) 2(T P c + T N c ) + \u03b1 0 F P c + \u03b1 1 F N c + \u03b1 2 F P d + \u03b1 3 F N d(8)\nwhere we use \u03b1 0 = \u03b1 1 = 2 and \u03b1 2 = \u03b1 3 = 1 to give more emphasis to nuclear type classification. Moreover, using the same weighting, if we further extend t to encompass all types of nuclei T (t \u2208 T ), the classification within TP d is then 390 divided into a correctly classified set A c and an incorrectly classified set B c . We can therefore disassemble F t c into:\nF T c = 2A c 2(A c + B c ) + F P d + F N d = 2(A c + B c ) 2(A c + B c ) + F P d + F N d \u00d7 A c A c + B c = F d \u00d7 Classification Accuracy within Correctly Detected Instances (9)\nwhere F d is simply the standard detection quality like DQ while the other term is the accuracy of nuclear type classification within correctly detected instances.\n\nIn the case where the GT is not exhaustively annotated for nuclear type clas-395 sification, like in CRCHisto, an amount equal to the number of unlabelled GT instances in each set is subtracted from B c and F N c .\n\nFinally, while IoU is utilised as the criteria in DQ for selecting the TP for detection in instance segmentation, detection methods can not calculate the IoU. Therefore, to facilitate comparison of both instance segmentation and detection 400 methods for the nuclear type classification tasks, for F t c , we utilise the notion of distance to determine whether nuclei have been detected. To be precise, we define the region within a predefined radius from the annotated centre of the nucleus as the ground truth and if a prediction lies within this area, then it is considered to be a true positive. Here, we are consistent with Sirinukunwattana  \n\n\nExperimental Results\n\n\nDatasets\n\nAs part of this work, we introduce a new dataset that we term as the colorectal nuclear segmentation and phenotypes (CoNSeP) dataset 4 , consisting 410 of 41 H&E stained image tiles, each of size 1,000\u00d71,000 pixels at 40\u00d7 objective magnification. Images were extracted from 16 colorectal adenocarcinoma (CRA) WSIs, each belonging to an individual patient, and scanned with an Omnyx VL120 scanner within the department of pathology at University Hospitals Coventry and Warwickshire, UK. We chose to focus on a single cancer 415 type, so that we are able to display the true variation of tissue within colorectal adenocarcinoma WSIs, as opposed to other datasets that instead focus on using a small number of visual fields from various cancer types. Within this dataset, stroma, glandular, muscular, collagen, fat and tumour regions can be observed.\n\nBeside incorporating different tissue components, the 41 images were also cho-420 sen such that different nuclei types were present, including: normal epithelial; tumour epithelial; inflammatory; necrotic; muscle and fibroblast. Here, by type we are referring to the type of cell from which the nucleus originates from.\n\nWithin the dataset, there are many significantly overlapping nuclei with indistinct boundaries and there exists various artifacts, such as ink. As a result of the 425 diversity of the dataset, it is likely that a model trained on CoNSeP will perform well for unseen CRA cases. For each image tile, every nucleus was annotated by one of two expert pathologists (A.A, Y-W.T). After full annotation, each annotated sample was reviewed by both of the pathologists; therefore refining their own and each others' annotations. By the end of the annotation process, each 430 pathologist had fully checked every sample and consensus had been reached.\n\nAnnotating the data in this way ensured that minimal nuclei were missed in the annotation process. However, we can not avoid inevitable pixel-level differences between the annotation and the true nuclear boundary in challenging cases.\n\nIn addition to delineating the nuclear boundaries, every nucleus was labelled 435 as either: normal epithelial, malignant/dysplastic epithelial, fibroblast, muscle, inflammatory, endothelial or miscellaneous. Within the miscellaneous category, necrotic, mitotic and cells that couldn't be categorised were grouped. For our experiments, we grouped the normal and malignant/dysplastic epithelial nuclei into a single class and we grouped the fibroblast, muscle and endothelial nuclei 440 into a class named spindle-shaped nuclei.\n\nOverall, six independent datasets are utilised for this study. A full summary for each of them is provided in Table 2. Five of these datasets are used to evaluate the instance segmentation performance which we refer to as: CoNSeP;\n\nKumar (Kumar et al., 2017); CPM-15; CPM-17 (Vu et al., 2018) and TNBC 445 (Naylor et al., 2018). Example images from each of the five datasets can be seen in Fig. 7. Meanwhile, we utilise CoNSeP and a further dataset, named CRCHisto, to quantify the performance of the nuclear classification model. The\n\nCRCHisto dataset consists of the same nuclei types that are present in CoNSeP.\n\nIt is also worth noting that the CRCHisto dataset is not exhaustively annotated 450 for nuclear class labels.\n\n\nImplementation and Training Details\n\nWe implemented our framework with the open source software library Ten-sorFlow version 1.8.0 (Abadi et al., 2016) on a workstation equipped with two NVIDIA GeForce 1080 Ti GPUs. During training, data augmentation including 455 flip, rotation, Gaussian blur and median blur was applied to all methods. All networks received an input patch with a size ranging from 252\u00d7252 to 270\u00d7270.\n\nThis size difference is due to the use of valid convolutions in some architectures, such as HoVer-Net and U-Net. Regarding HoVer-Net, we initialised the model with pre-trained weights on the ImageNet dataset (Deng et al., 2009), trained 460 only the decoders for the first 50 epochs, and then fine-tuned all layers for another 50 epochs. We train stage one for around 120 minutes and stage two for around 260 minutes. Therefore, the overall training time is around 380 minutes. Stage two takes longer to train because unfreezing the encoder utilises more memory and therefore a smaller batch size needs to be used. Specifically, 465 we used a batch size of 8 and 4 on each GPU for stage one and two respectively. We used Adam optimisation with an initial learning rate of 10 \u22124 and then reduced it to a rate of 10 \u22125 after 25 epochs. This strategy was repeated for fine-tuning. On the whole, training of the network is stable, where the usage of fully independent decoders helps the network to converge each time. The 470 network was trained with an RGB input, normalised between 0 and 1.\n\n\nComparative Analysis of Segmentation Methods\n\nExperimental Setting: We evaluated our approach by employing a full independent comparison across the three largest known exhaustively labelled nuclear segmentation datasets: Kumar; CoNSeP and CPM-17 and utilised the 475 metrics as described in Section 4.1. For this experiment, because we do not have the classification labels for all datasets, we perform instance segmentation without classification. This enables us to fully leverage all data and allows us to rigorously evaluate the segmentation capability of our model. In the same way as Kumar et al. (2017), we split the Kumar dataset into two different sub-   We compared our proposed model to recent segmentation approaches used in computer vision (Long et al., 2015;Badrinarayanan et al., 2017;He et al., 2017), medical imaging (Ronneberger et al., 2015) and also to methods specifically tuned for the task of nuclear segmentation Raza et al., 2018;495 Naylor et al., 2018;Zhou et al., 2019;Vu et al., 2018). We also compared the performance of our model to two open source software applications: Cell\n\nProfiler (Carpenter et al., 2006) and QuPath (Bankhead et al., 2017). Cell\n\nProfiler is a software for cell-based analysis, with several suggested pipelines for computational pathology. The pipeline that we adopted applies a threshold to The default configuration is fine-tuned for natural images and therefore, this modification was necessary to perform a successful nuclear segmentation. DIST was implemented with the assistance of the first author of the corresponding approach in order to ensure reliability during evaluation. This also enabled 510 us to utilise DIST for further comparison in our experiments. For Micro-Net, we used the same implementation that was described by Raza et al. (2018) and was implemented by the first author of the corresponding paper (S.E.A.R).\n\nFor CNN3 and CIA-Net, we report the results on the Kumar dataset that are given in their respective original papers. The authors of CIA-Net and DRAN 515 provided their segmentation output, which meant that we were able to obtain all metrics on the datasets that the models were applied to. Therefore, we report results of CIA-Net on the Kumar dataset and results of DRAN on the CPM-17 dataset. Note, for all self-implemented approaches we are consistent with our pre-processing strategy. However, DRAN, CNN3 and CIA-Net results 520 are directly taken from their respective papers and therefore we can't guarantee the same pre-processing steps. CNN3 and CIA-Net also use stain normalisation, whereas other methods described in this paper do not.\n\nComparative Results: Table 3 and the box plots in Fig. 8a (Carpenter et al., 2006) and QuPath (Bankhead et al., 2017) achieve sub-optimal performance for all 530 datasets. In particular, both software applications consistently achieve a low DICE score, suggesting that their inability to distinguish nuclear pixels from   proposal based approach. However, Mask-RCNN is less effective than other methods at detecting nuclear pixels, which is reflected by a lower DICE score.\n\nDue to the reasoning given in Section 4, we place a larger emphasis on PQ to determine the success of different models. In particular, we consistently obtain an improved performance over DIST, which justifies the use of our proposed 555 horizontal and vertical maps as a regression target. We also report a better performance than the winners of the Computational Precision Medicine and MoNuSeg challenges (Vu et al., 2018;Zhou et al., 2019), that utlised the CPM-17 and Kumar datasets respectively. Therefore, HoVer-Net achieves state-of-the art performance for nuclear instance segmentation compared to all competing 560 methods on multiple datasets that consist of a variety of different tissue types.\n\nOur approach also outperforms methods that were fine-tuned for the task of nuclear segmentation.\n\n\nGeneralisation Study\n\nExperimental Setting: The goal of any automated method is to perform 565 well on unseen data, with high accuracy. Therefore, we conducted a large scale study to assess how all methods generalise to new H&E stained images. To analyse the generalisation capability, we assessed the ability to segment nuclei from: i) new organs (variation in nuclei shapes) and ii) different centres (variation in staining).\n\n\n570\n\nThe five instance segmentation datasets used within our experiments can be grouped into three groups according to their origin: TCGA (Kumar,, TNBC and CoNSeP. We used Kumar as the training and validation set, due to its size and diversity, whilst the combined CPM (CPM-15 and CPM-17), TNBC and CoNSeP datsets were used as three independent test sets. We 575 split the test sets in this way in accordance with their origin. Note, for this experiment we use both the training and test sets of CPM-17 and CoNSeP to form the independent test sets. Kumar was split into three subsets, as explained in Section 5.1, and Kumar-Train was used to train all models, i.e. trained with samples originating from the following organs: breast; prostate; kidney and 580 liver. Despite all samples being extracted from TCGA, CPM samples come from the brain, head & neck and lungs regions. Therefore, testing with CPM reflects the ability for the model to generalise to new organs, as mentioned above by the first generalisation criterion. TNBC contains samples from an already seen organ (breast), but the data is extracted from an independent source with 585 different specimen preservation and staining practice. Therefore, this reflects the second generalisation criterion. CoNSeP contains samples taken from colorectal tissue, which is not represented in Kumar-Train, and is also extracted from a source independent to TCGA. Therefore, this reflects both the first and second generalisation criteria. Also, as mentioned in Section 5.1, CoNSeP contains 590 challenging samples, where there exists various artifacts and there is variation in the quality of slide preparation. Therefore, the performance on this dataset also reflects the ability of a model to generalise to difficult samples.\n\nComparative Results: The results are reported in Table 4, where we only All models are initially trained on Kumar and then the Combined CPM (Vu et al., 2018), TNBC (Naylor et al., 2018) and CoNSeP datasets are processed.  Table 3, Mask-RCNN is not able to distinguish nuclear pixels from the background as well as other competing methods, which has an adverse effect on the overall segmentation performance shown by PQ. On the other hand, SegNet proves to successfully detect nuclear pixels, reporting a greater DICE score than HoVer-Net on both the TNBC and 605 CoNSeP datasets. However, the overall segmentation result for HoVer-Net is superior because it is better able to separate nuclear instances by incorporating the horizontal and vertical maps at the output of the network.\n\n\nComparative Analysis of Classification Methods\n\nExperimental Setting: We converted the top four performing nuclear in-610 stance segmentation algorithms, based on their panoptic quality on the CoNSeP dataset, such that they were able to perform simultaneous instance segmentation and classification. As mentioned in Section 5.1, the nuclear categories that we use in our experiments are: miscellaneous, inflammatory, epithelial and spindle-shaped. Specifically, we compared HoVer-Net with Micro-Net, Mask-RCNN and\n\n\n615\n\nDIST. For Micro-Net, we used an output depth of 5 rather than 2, where each channel gave the probability of a pixel being either background, miscellaneous, inflammatory, epithelial or spindle-shaped. For Mask-RCNN, there is a devoted classification branch that predicts the class of each instance and therefore is well suited to a multi-class setting. DIST performs regression at the output of 620 the network and therefore converting the model such that it is able to classify nuclei into multiple categories is non-trivial. Instead, we add an extra 1\u00d71 convolution at the output of the network that performs nuclear classification. As well as comparing to the aforementioned methods, we compared our approach to a spatially constrained CNN (SC-CNN), that achieves detection and classi-625 fication. Note, because SC-CNN does not produce a segmentation mask, we do not report the PQ for this method.\n\nComparative Results: We trained our models on the training set of the CoNSeP dataset and then we evaluated the model on both the test set of CoN-SeP and also the entire CRCHisto dataset.   In this paper, we have proposed HoVer-Net for simultaneous segmentation and classification of nuclei within multi-tissue histology images that not only detects nuclei with high accuracy, but also effectively separates clustered nuclei. Our approach has three up-sampling branches: 1) the nuclear pixel branch 665 that separates nuclear pixels from the background; 2) the HoVer branch that regresses the horizontal and vertical distances of nuclear pixels to their centres of mass and 3) the nuclear classification branch that determines the type of each nucleus. We have shown that the proposed approach achieves the state-of-theart instance segmentation performance compared to a large number of recently 670 published deep learning models across multiple datasets, including tissues that have been prepared and stained under different conditions. This makes the proposed approach likely to translate well to a practical setting due its strong generalisation capacity, which can therefore be effectively used as a prerequisite step before nuclear-based feature extraction. We have shown that utilising 675 the horizontal and vertical distances of nuclear pixels to their centres of mass provides powerful instance-rich information, leading to state-of-the-art performance in histological nuclear segmentation. When the classification labels are available, we show that our model is able to successfully segment and classify nuclei with high accuracy.\n\n\n680\n\nRegion proposal (RP) methods, such as Mask-RCNN, show great potential in dealing with overlapping instances because there is no notion of separating instances; instead nuclei are segmented independently. However, a major limitation of the RP methods is the difficulty in merging instance predictions between neigbouring tiles during processing. For example, if a sub-segment of a nucleus 685 at the boundary is assigned a label, one must ensure that the remainder of the nucleus in the neighbouring tile is also assigned the same label. To overcome this difficulty, for Mask-RCNN, we utilised an overlapping tile mechanism such that we only considered non-boundary nuclei.\n\nRegarding the processing time, the average time to process a 1,000\u00d71,000 690 image tile over 10 runs using Mask-RCNN for segmentation and classification was 106.98 seconds. Meanwhile, HoVer-Net only took an average of 11.04 seconds to complete the same operation; approximately 9.7\u00d7 faster. On the other hand, the average processing time for DIST and Micro-Net was 0.600 and 0.832 seconds respectively. Mask-RCNN inherently stores a single instance per channel, which leads to very large arrays in memory when there are many nuclei in a single image patch, which also contributes to the much longer processing time as seen above. Overall, FCN methods seem to better translate to WSI processing compared to Mask-RCNN or RPN methods in general. It must be stressed that the timing is not exact and is dependent on hardware specifications and software 700 implementation. With optimised code and sophisticated hardware, we expect these timings to be considerably different. Additionally, the inference time is also dependent on the size of the output. In particular, with a smaller output size, a smaller stride is also required during processing. For instance, if we used padded convolution in the up-sampling branches of HoVer-Net, then we observe 705 5.6\u00d7 speed up and the average processing time is 1.97 seconds per 1000\u00d71000\n\nimage tile. For fair comparison, all models were processed on a single GPU with 12GB RAM and we fixed the batch size to a size of one. Future work will explore the trade-off between the efficiency of HoVer-Net and its potential to accurately perform instance segmentation and classification.\n\n\n710\n\nA major bottleneck for the development of successful nuclear segmentation algorithms is the limitation of data; particularly with additional associated class labels. In this work, we introduce the colorectal adenocarcinoma nuclear segmentation and phenotypes (CoNSeP) dataset, containing over 24K labelled nuclei from challenging samples to reflect the true difficulty of segmenting nuclei in 715 whole-slide images. Due to the abundance of nuclei with an associated nuclear category, CoNSeP aims to help accelerate the development of further simultaneous nuclear instance segmentation and classification models to further increase the sophistication of cell-level analysis within computational pathology.\n\nWe analysed the common measurements used to assess the true performance 720 of nuclear segmentation models and discussed their limitations. Due to the fact that these measurements did not always reflect the instance segmentation performance, we proposed a set of reliable and informative statistical measures.\n\nWe encourage researchers to utilise the proposed measures to not only maximise the interpretability of their results, but also to perform a fair comparison with 725 other methods.\n\nFinally, methods have surfaced recently that explore the relationship of various nuclear types within histology images (Javed et al., 2018;Sirinukunwattana et al., 2018), yet these methods are limited to spatial analysis because the segmentation masks are not available. Utilising our model for nuclear segmentation 730 and classification enables the exploration of the spatial relationship between various nuclear types combined with nuclear morphological features and therefore may provide additional diagnostic and prognostic value. Currently, our model is trained on a single tissue type, yet due to the strong performance of our instance segmentation model across multiple tissues, we are confident that our model will 735 perform well if we were to incorporate additional tissue types. We observe a low \n\n\n745\n\nWe also acknowledge the financial support from EPSRC and MRC, provided as part of the Mathematics for Real-World Systems CDT. We thank Peter Naylor for his assistance in the implementation of the DIST network.\n\n\nAppendix A. Ablation Studies\n\nTo gain a full understanding of the contribution of our method, we investi- intuition behind our novel L b is that it enforces the correct structure of the horizontal and vertical map predictions and therefore helps to correctly separate neighbouring instances. The dice loss was introduced because it can help the network to better distinguish between background and nuclear pixels and is particularly useful when there is a class-imbalance. We present the results in Table A1, where we observe an increase in all performance measures for our proposed multi-term loss strategy. Therefore, the additional loss terms boost the network's ability to differentiate between nuclear and background pixels (DICE) and separate individual nuclei (DQ and PQ). In particular, there is a significant boost in the SQ for both Kumar and CoNSeP, which suggests that 775 our proposed loss function L b is necessary to precisely determine where nuclei should be split.\n\nPost Processing: Usually, markers obtained from applying a threshold to an energy landscape (such as the distance map) is enough to provide a competitive input for watershed, as seen by DIST in Table 3. Although HoVer-Net is 780 not directly built upon an energy landscape, we devised a Sobel-based method to derive both the energy landscape and the markers. To compare with other methods, we implemented two further techniques for obtaining the energy landscape and the markers. We then exhaustively compared all energy landscape and marker combinations to assess which post processing strategy is the best. 785 We start by linking HoVer to the distance map by calculating the square sum \u03c7 2 + \u03d5 2 , which can be seen as the distance from a pixel to its nearest nuclear centroid. In other words, this is a pseudo distance map. Additionally, \u03c7 and \u03d5 values can be interpreted as Cartesian coordinates with each nuclear centroid as the origin. By thresholding the values between a certain range, we can obtain 790 the markers. The results of all combinations are shown in Table A2. Note, our gradient-based post processing technique is specifically designed for the HoVer branch output.\n\nClassification Branch: In order to assess the importance of a devoted branch for concurrent nuclear segmentation and classification, we compared the 795 proposed three branch setup of HoVer-Net to a two branch setup. Here, the two branch setup extends the NP branch to a multi-class setting, by predicting each nuclear type at the output. Then, to obtain the binary mask, the positive channels are combined together after nuclear type prediction. Utilising three branches decouples the tasks of nuclear classification and nuclear detection,\n\n\n800\n\nwhere a separate branch is devoted to each task. For this ablation study, we train on the CoNSeP training set and then process both the CoNSeP test set and the entire CRCHisto dataset.\n\nWe report results in Table A3, where we observe that utilising a separate branch devoted to the task of nuclear classification leads to an improved overall 805 performance of simultaneous nuclear instance segmentation and classification in both the CoNSeP and CRCHisto datasets. We can see that if the classification takes place at the output of NP branch, then the network's ability to determine the nuclear type is compromised. This is because the task of nuclear classification is challenging and therefore the network benefits from the introduction of 810 a branch dedicated to the task of classification.    \n\nFigure 1 :\n1Overview of the proposed approach for simultaneous nuclear instance segmentation and classification. When no classification labels are available, the network produces the instance segmentation as shown in (a). The different colours of the nuclear boundaries represent different types of nuclei in (b).\n\nFigure 2 :\n2Overview of the proposed architecture. (a) (Pre-activated) residual unit, (b) dense unit. m indicates the number of feature maps within each residual unit. The yellow square within the input denotes the considered region at the output. When the classification labels aren't available, only the up-sampling branches in the dashed box are considered.\n\nFollowing\nPreact-ResNet50, we perform nearest neighbour up-sampling via 180 three distinct branches to simultaneously obtain accurate nuclear instance segmentation and classification. We name the corresponding branches: (i) nuclear pixel (NP) branch; (ii) HoVer branch and (iii) nuclear classification (NC)\n\n\n200 dense units after the first and second up-sampling operations, where the number of units is 4 and 8 respectively. Valid convolution is performed throughout the two up-sampling branches to prevent poor predictions at the boundary. This results in the size of the output being smaller than the size of the input. As opposed to using a dedicated network for each task, a shared encoder makes 205 it possible to train the nuclear instance segmentation and classification model end-to-end and therefore, reduce the total training time. Furthermore, a shared encoder can also take advantage of the shared information across multiple tasks and thus, help to improve the model performance on all tasks. Finally, if we do not have the classification labels of the nuclei, only the 210 NP and HoVer up-sampling branches are considered. Otherwise, we consider all three up-sampling branches and perform simultaneous nuclear instance segmentation and classification.We display an overview of the network architecture inFig. 2, where the spatial dimension of the input is 270\u00d7270 and the output dimension of each 215 branch is 80\u00d780. The dashed box withinFig. 2highlights the branches for nuclear instance segmentation. Additionally, we also show a residual unit and a dense unit withinFig. 2aandFig. 2b. We denote m as the number of feature maps within each convolution of a given residual unit. At each down sampling level, from left to right, m=256, 512, 1024, 2048 respectively. We keep a fixed 220 amount of feature maps within each dense unit throughout the two branches as shown inFig. 2c.\n\nFigure 3 :\n3Cropped image regions showing horizontal and vertical map predictions, with corresponding ground truth. Arrows highlight the strong instance information encoded within these maps, where there is a significant difference in the pixel values.\n\n(\nGT). \u03a8 i (I) is the GT of the nuclear binary map, where background pixels have the value of 0 and nuclear pixels have the value 1. On the other hand, \u03a6 i (I) is 240 the nuclear type GT where background pixels have the value 0 and any integer value larger than 0 indicates the type of nucleus. Meanwhile, \u0393 i (I) denotes the GT of the horizontal and vertical distances of nuclear pixels to their corresponding centres of mass. For \u0393 i (I), we assign values between -1 and 1 to nuclear pixels in both the horizontal and vertical directions. We assign the value of the 245 background and the line crossing the centre of mass within each nucleus to be 0. For clarity, we denote the horizontal and vertical components of the GT HoVer map as horizontal map \u0393 i,x and vertical map \u0393 i,y respectively. Visual examples of the horizontal and vertical maps can be seen in Fig. 3.\n\nFig. 1 .\n1of the HoVer branch and H x and H y refer to the horizontal and vertical components of the Sobel operator. Specifically, H x and H y compute the horizontal and vertical derivative approximations and are shown by the gradient maps in Therefore, S m highlights areas where there is a significant difference in neighbouring pixels within the horizontal and vertical maps. Therefore, ar-290 eas such as the ones shown by the arrows inFig. 3will result in high values within S m . We compute markers M\n\nFigure 4 :\n4Examples highlighting the limitations of DICE2 and AJI with slightly different predictions. For better visualisation, ground truth contours (red dash line) for each instance have been overlaid on both the predictions and original images.\n\nFigure 5 :Figure 6 :\n56Sample cropped regions extracted from each of the five nuclear instance segmentation datasets used in our experiments. From left to right: Kumar (Kumar et al., 2017); CoNSeP; CPM-15; CPM-17 (Vu et al., 2018) and TNBC (Naylor et al., 2018). The different colours of nuclear contours highlight individual instances. Sample cropped regions extracted from the CoNSeP datasets, where the colour of each nuclear boundary denotes the category.\n\n3 :\n3Comparative experiments on the Kumar(Kumar et al., 2017), CoNSeP and CPM-17(Vu et al., 2018) datasets. WS denotes watershed-based post processing. and 4 prostate) and (ii) Kumar-Test, a test set with 14 image tiles (2 breast, 2 liver, 2 kidney and 2 prostate, 2 bladder, 2 colon, 2 stomach\n\n\nthe greyscale image and then uses a series of post processing operations. QuPath is an open source software for digital pathology and whole slide image analysis. To achieve nuclear segmentation, we used the default parameters within the application. FCN, SegNet, U-Net, DCAN, Mask-RCNN and DIST have been implemented by the authors of the paper (S.G, Q.D.V). For Mask-RCNN, we 505 slightly modified the original implementation by using smaller anchor boxes.\n\nFigure 8 :\n8Box plots highlighting the performance of competing methods on the Kumar and CoNSeP datasets.\n\n\nresults of methods that employ an instance-based technique. We 595 observe that our proposed model is able to successfully generalise to unseen data in all three cases. However, some methods prove to perform poorly with unseen data, where in particular, U-Net and DIST perform worse than other competing methods on all three datasets. Both SegNet with watershed and Mask-RCNN achieve a competitive performance across all three generalisation 600 tests. However, similar to the results reported in\n\n\nof nuclei in large-scale histopathology images is an important step 650 towards automated downstream analysis for diagnosis and prognosis of cancer. Nuclear features have been often used to assess the degree of malignancy (Gurcan et al., 2009). However, visual analysis of nuclei is a very time consuming task because there are often tens of thousands of nuclei within a given whole-slide image (WSI). Performing simultaneous nuclear instance segmentation and clas-655 sification enables subsequent exploration of the role that nuclear features play in predicting clinical outcome. For example, Lu et al. (2018) utilised nuclear features from histology TMA cores to predict survival in early-stage estrogen receptor-positive breast cancer. Restricting the analysis to some specific nuclear types only may be advantageous for accurate analysis in computational 660 pathology.\n\nF 1\n1classification score for the miscellaneous category in the classification model because there are significantly less samples within this category and there exists high intra-class variability. Future work will involve obtaining more samples within this category, including necrotic and mitotic nuclei, to improve the class 740 balance of the data. Acknowledgments This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2016R1C1B2012433) and by the Ministry of Science and ICT (MSIT) (No. 2018K1A3A1A74065728).\n\n\ngated several of its components. Specifically, we performed the following ablation experiments: (i) contribution of the proposed loss strategy; (ii) Sobel-based post processing technique compared to other strategies and (iii) contribution of the dedicated classification branch. Here, we utilised the Kumar and CoNSeP datasets for (i) and (ii) due to the large number of nuclei present, whereas for 755 (iii) we use CoNSeP and CRCHisto because we do not have the classification labels for Kumar. Loss Terms: We conducted an experiment to understand the contribution of our proposed loss strategy. First, we used mean squared error (MSE) of the horizontal and vertical distances L a as the loss function of the HoVer branch and 760 binary cross entropy (BCE) loss L c as the loss function for the NP branch. We refer to this combination as the standard strategy because MSE and BCE are the two most commonly used loss functions for regression and binary classification tasks respectively. Next, we introduced the MSE of the horizontal and vertical gradients L b to the HoVer branch and the dice loss L d to the NP branch. The765\n\nTable 1 :\n1Comparison between Prediction A and Prediction B from Fig.4 across various measurements. \u2022 Separate the nuclei from the background \u2022 Detect individual nuclear instances \u2022 Segment each detected instanceDICE2 \nAJI \nPQ \n\nPrediction A \n0.6477 \n0.4790 0.6803 \n\nPrediction B \n0.9007 \n0.6414 0.6863 \n\n\n\n\n. Nonetheless, when evaluating methods across different datasets, specifically on samples containing lots of hard to recognise nuclei such as fibroblasts or nuclei with poor staining, the number of failed detections may increase and therefore may have a negative impact on the AJI measurement. Due to the limitations of DICE2 and AJI, it is clear that there is a need for an improved reliable quantitative measurement.345 \n\n\n\n\nover the entire set of prediction and GT segments if their IoU(x,y)>0.5. The unique matching splits all available segments into matched pairs (TP), unmatched GT segments (FN) and unmatched prediction segments(FP). From this, PQ can be intuitively analysed as follows: the detection quality (DQ) is the F 1 Score that is widely used to evaluate instance detection, while segmentation quality (SQ) can be interpreted as how close each correctly detected instance is to their matched GT. DQ and SQ, in a way, also provide a direct insight into the second and third sub-tasks, defined above. We believe that PQ should set the standard for measuring the performance of nuclear instance segmentation methods.Overall, to fully characterise and understand the performance of each method, we use the following three metrics: 1) DICE to measure the separation of all nuclei from the background; 2) Panoptic Quality as a unified score for comparison and 3) AJI for direct comparison with previous publications 3 . Panoptic quality is further broken down into DQ and SQ components for interpretability.Note, SQ is calculated only within true positive segments and should therefore be observed together with DQ. Throughout this study, these metrics are calculated for each image and the average of all images are reported as final values for each dataset.355 \n\n360 \n\n365 \n\n370 \n\n\n\nTable 2 :\n2Summary of the datasets used in our experiments. UHCW denotes University Hospitals Conventry and Warwickshire and TCGA denotes The Cancer Genome Atlas. Seg denotes segmentation masks and Class denotes classification labels.CoNSeP \nKumar \nCPM-15 \nCPM-17 \nTNBC \nCRCHisto \n\nTotal Number of Nuclei \n24,319 \n21,623 \n2,905 \n7,570 \n4,056 \n29,756 \n\nLabelled Nuclei \n24,319 \n0 \n0 \n0 \n0 \n22,444 \n\nNumber of Images \n41 \n30 \n15 \n32 \n50 \n100 \n\nOrigin \nUHCW \nTCGA \nTCGA \nTCGA \nCurie Institute \nUHCW \n\nMagnification \n40\u00d7 \n40\u00d7 \n40\u00d7 & 20\u00d7 \n40\u00d7 & 20\u00d7 \n40\u00d7 \n20\u00d7 \n\nSize of Images \n1000\u00d71000 \n1000\u00d71000 \n400\u00d7400 to 1000\u00d7600 \n500\u00d7500 to 600\u00d7600 \n512\u00d7512 \n500\u00d7500 \n\nSeg/Class \nBoth \nSeg \nSeg \nSeg \nSeg \nClass \n\nNumber of Cancer Types \n1 \n8 \n2 \n4 \n1 \n1 \n\n\n\nTable\n\n\n\n). Note, we utilise the exact same image split used by other recent approaches (Kumar test set into two subsets. We do this to ensure that the test set is large enough, ensuring a reliable evaluation. For CoNSeP, we devise a suitable train and test set that contains 26 and 14 images respectively. The images within the test set were selected to ensure the true diversity of nuclei types within colorectal tissue are represented. For CPM-17, we utilise the same split that had been employed for the challenge, with 32 images in both the training and test datasets.et al., 2017; Naylor et al., 2018; Zhou et al., 2019), but we do not separate the \n\n485 \n\n490 \n\n\n\n\nthe background is a major limiting factor. FCN-based approaches improve the capability of models to detect nuclear pixels, yet often fail due to their inability to separate clustered instances. For example, despite a higher DICE score than DCAN is able to better distinguish between separate instances than FCN8, which uses a very similar encoder based on the VGG16 network. Therefore, incorporating additional information at the output of the network can improve the segmentation performance. This is also exemplified by the fairly strong performances of CNN3, DIST, DRAN and CIA-Net. In a different way, Mask-RCNN is able to successfully separate clustered nuclei by utilising a region535 \n\nCell Profiler and QuPath, networks built only for semantic segmentation like \n\nFCN8 and SegNet suffer from low PQ values. Therefore, methods that incor-\n\nporate strong instance-aware techniques are favourable. Within CPM-17, there \n\nare less overlapping nuclei which explains why methods that are not instance-\n\naware are still able to achieve a satisfactory performance. We observe that \n\n540 \n\nthe weighted cross entropy loss that is used in both U-Net and Micro-Net can \n\nhelp to separate joined nuclei, but its success also depends on the capacity of \n\nthe network. This is reflected by the increased performance of Micro-Net over \n\nU-Net. \n\n545 \n\n550 \n\n\n\nTable 4 :\n4Comparative results, highlighting the generalisation capability of different models.\n\nTable 5\n5displays the results of the630 \n\nmulti-class models on the CoNSeP and the CRCHisto datasets respectively, \n\nwhere the given metrics are described in Section 4.2. For CoNSeP, along with \n\nthe classification metrics, we provide PQ as an indication of the quality of in-\n\nstance segmentation. However, in CRCHisto, only the nuclear centroids are \n\ngiven and therefore, we exclude PQ from the CRCHisto evaluation because it \n\n635 \n\ncan't be calculated without the instance segmentation masks. We observe that \n\nHoVer-Net achieves a good quality simultaneous instance segmentation and clas-\n\nsification, compared to competing methods. It must be noted, that we should \n\nexpect a lower F 1 score for the miscellaneous class because there are signifi-\n\ncantly less nuclei represented. Also, there is a high diversity of nuclei types \n\n640 \n\nthat have been grouped within this class, belonging to: mitotic; necrotic and \n\ncells that are uncategorisable. Despite this, HoVer-Net is able to achieve a sat-\n\nisfactory performance on this class, where other methods fail. Furthermore, \n\ncompared to other methods, our approach achieves the best F 1 score for epithe-\n\nlial, inflammatory and spindle classes. Therefore, due to HoVer-Net obtaining a \n\n645 \n\n\nTable 5 :\n5Comparative results for nuclear classification on the CoNSeP and CRCHisto datasets.F d denotes the F 1 score for nuclear detection, whereas F e \nc , F i \nc , F s \nc and F m \nc denote the F 1 \nclassification score for the epithelial, inflammatory, spindle-shaped and miscellaneous classes \n\nrespectively. \n\nCoNSeP \nCRCHisto \n\nMethods \nPQ \nFd \nF e \n\nc \n\nF i \n\nc \n\nF s \n\nc \n\nF m \n\nc \n\nFd \nF e \n\nc \n\n\n\nTable A1 :\nA1Ablation study highlighting the contribution of the proposed loss strategy.Kumar \nCoNSeP \n\nStrategy \nDICE AJI \nDQ \nSQ \nPQ \nDICE AJI \nDQ \nSQ \nPQ \n\nStandard Loss 0.823 0.750 0.771 0.581 0.608 \n0.846 0.685 0.774 0.532 0.557 \n\nProposed Loss 0.826 0.770 0.773 0.597 0.618 0.853 0.702 0.778 0.547 0.571 \n\n\n\nTable A2 :\nA2Ablation study for post processing techniques: Sobel-based versus thresholding to get markers and Sobel-based versus naive conversion to get energy landscapeKumar \nCoNSeP \n\nEnergy Markers DICE AJI \nDQ \nSQ \nPQ DICE AJI \nDQ \nSQ \nPQ \n\n\n\nTable A3 :\nA3Ablation study showing the contribution of the classification branch in HoVer-Net on the CoNSeP dataset. F d denotes the F 1 score for nuclear detection, whereas F e c , F i c , F s c and F m c denote the F 1 classification score for the epithelial, inflammatory, spindle-shaped and miscellaneous classes respectively. NP & HoVer & NC 0.516 0.748 0.635 0.631 0.566 0.426 0.688 0.486 0.573 0.302 0.178 arrangement of tumor-infiltrating lymphocytes for predicting likelihood ofCoNSeP \nCRCHisto \n\nBranches \nPQ \nF d \nF e \n\nc \n\nF i \n\nc \n\nF s \n\nc \n\nF m \n\nc \n\nF d \nF e \n\nc \n\nF i \n\nc \n\nF s \n\nc \n\nF m \n\nc \n\nNP & HoVer \n0.499 0.736 0.636 0.545 0.528 0.333 0.666 0.458 0.523 0.271 0.132 \n\n\nThe CoNSeP dataset for nuclear segmentation is available at https://warwick.ac.uk/ fac/sci/dcs/research/tia/data/.\ngression output of the HoVer branch, whereas q i (I, w 0 , w 2 ) and r i (I, w 0 , w 3 )\nproblem can be divided into the following three sub-tasks:\n3 Evaluation code available at: https://github.com/vqdang/hover net/src/metrics\n   et al. (2016)  and use a radius of 6 pixels at 20\u00d7 or 12 pixels at 40\u00d7.\nThis dataset is available at https://warwick.ac.uk/fac/sci/dcs/research/tia/data/.\ndatasets: (i) Kumar-Train, a training set with 16 image tiles (4 breast, 4 liver,\n\nTensorflow: A system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, OSDIAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al., 2016. Tensorflow: A system for large-scale machine learning., in: OSDI, pp. 265-283.\n\nactive contour for multiple object overlap resolution in histological imagery. IEEE transactions on medical imaging. 31active contour for multiple object overlap resolution in histological imagery. IEEE transactions on medical imaging 31, 1448-1460.\n\nA bottom-up approach for tumour differentiation in whole slide images of lung 820 adenocarcinoma. N Alsubaie, K Sirinukunwattana, S E A Raza, D Snead, N Rajpoot, Digital Pathology, International Society for Optics and Photonics. 105810Medical ImagingAlsubaie, N., Sirinukunwattana, K., Raza, S.E.A., Snead, D., Rajpoot, N., 2018. A bottom-up approach for tumour differentiation in whole slide images of lung 820 adenocarcinoma, in: Medical Imaging 2018: Digital Pathology, International Society for Optics and Photonics. p. 105810E.\n\nOn the robustness of semantic segmentation models to adversarial attacks. A Arnab, O Miksik, P H S Torr, arXiv:1711.09856Arnab, A., Miksik, O., Torr, P.H.S., 2017. On the robustness of semantic segmentation models to adversarial attacks. CoRR abs/1711.09856. URL: http://arxiv.org/abs/1711.09856, arXiv:1711.09856.\n\nSegnet: A deep convolutional encoder-decoder architecture for image segmentation. V Badrinarayanan, A Kendall, R Cipolla, IEEE transac. 39Badrinarayanan, V., Kendall, A., Cipolla, R., 2017. Segnet: A deep convolu- tional encoder-decoder architecture for image segmentation. IEEE transac- tions on pattern analysis and machine intelligence 39, 2481-2495.\n\nQupath: Open source software for digital pathology image analysis. P Bankhead, M B Loughrey, J A Fern\u00e1ndez, Y Dombrowski, D G Mcart, P D Dunne, S Mcquaid, R T Gray, L J Murray, H G Coleman, 8302017Scientific reports 7, 16878Bankhead, P., Loughrey, M.B., Fern\u00e1ndez, J.A., Dombrowski, Y., McArt, D.G., Dunne, P.D., McQuaid, S., Gray, R.T., Murray, L.J., Coleman, H.G., et al., 830 2017. Qupath: Open source software for digital pathology image analysis. Scientific reports 7, 16878.\n\nCellprofiler: image analysis software for identifying and quantifying cell pheno-835 types. A E Carpenter, T R Jones, M R Lamprecht, C Clarke, I H Kang, O Friman, D A Guertin, J H Chang, R A Lindquist, J Moffat, Genome biology. 7100Carpenter, A.E., Jones, T.R., Lamprecht, M.R., Clarke, C., Kang, I.H., Friman, O., Guertin, D.A., Chang, J.H., Lindquist, R.A., Moffat, J., et al., 2006. Cell- profiler: image analysis software for identifying and quantifying cell pheno- 835 types. Genome biology 7, R100.\n\nDcan: deep contour-aware networks for accurate gland segmentation. H Chen, X Qi, L Yu, P A Heng, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. the IEEE conference on Computer Vision and Pattern RecognitionChen, H., Qi, X., Yu, L., Heng, P.A., 2016. Dcan: deep contour-aware networks for accurate gland segmentation, in: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 2487-2496.\n\nSegmentation of clustered nuclei with 840 shape markers and marking function. J Cheng, J C Rajapakse, IEEE Transactions on Biomedical Engineering. 56Cheng, J., Rajapakse, J.C., et al., 2009. Segmentation of clustered nuclei with 840 shape markers and marking function. IEEE Transactions on Biomedical En- gineering 56, 741-748.\n\nSpatial architecture and recurrence in early-stage non-small cell lung cancer. G Corredor, X Wang, Y Zhou, C Lu, P Fu, K Syrigos, D L Rimm, M Yang, E Romero, K A Schalper, Clinical Cancer Research. 25Corredor, G., Wang, X., Zhou, Y., Lu, C., Fu, P., Syrigos, K., Rimm, D.L., Yang, M., Romero, E., Schalper, K.A., et al., 2019. Spatial architecture and recurrence in early-stage non-small cell lung cancer. Clinical Cancer Research 25, 1526-1534.\n\nA deep learning algorithm for one-step contour aware nuclei segmentation of histopathological images. Y Cui, G Zhang, Z Liu, Z Xiong, J Hu, arXiv:1803.02786arXiv preprintCui, Y., Zhang, G., Liu, Z., Xiong, Z., Hu, J., 2018. A deep learning algorithm for one-step contour aware nuclei segmentation of histopathological images. arXiv preprint arXiv:1803.02786 .\n\nImageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, 9Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. ImageNet: A Large-Scale Hierarchical Image Database, in: CVPR09.\n\nDiagnostic concordance among pathologists interpreting breast biopsy specimens. J G Elmore, G M Longton, P A Carney, B M Geller, T Onega, A N Tosteson, H D Nelson, M S Pepe, K H Allison, S J Schnitt, Elmore, J.G., Longton, G.M., Carney, P.A., Geller, B.M., Onega, T., Tosteson, A.N., Nelson, H.D., Pepe, M.S., Allison, K.H., Schnitt, S.J., et al., 2015. Diag- nostic concordance among pathologists interpreting breast biopsy specimens.\n\n. Jama. 313Jama 313, 1122-1132.\n\nSams-net: Stain-aware multi-scale network for instance-based nuclei segmentation in histology images. S Graham, N M Rajpoot, IEEE 15th International Symposium on, IEEE. Biomedical Imaging (ISBI 2018)Graham, S., Rajpoot, N.M., 2018. Sams-net: Stain-aware multi-scale network for instance-based nuclei segmentation in histology images, in: Biomedical Imaging (ISBI 2018), 2018 IEEE 15th International Symposium on, IEEE. pp. 590-594.\n\n. M N Gurcan, L E Boucheron, A Can, A Madabhushi, N M Rajpoot, Gurcan, M.N., Boucheron, L.E., Can, A., Madabhushi, A., Rajpoot, N.M.,\n\nHistopathological image analysis: A review. B Yener, IEEE reviews in biomedical engineering. 2Yener, B., 2009. Histopathological image analysis: A review. IEEE reviews in biomedical engineering 2, 147-171.\n\n. K He, G Gkioxari, P Doll\u00e1r, R Girshick, arXiv:1703.06870arXiv:1703.06870Mask R-CNN. ArXiv e-printsHe, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R., 2017. Mask R-CNN. ArXiv e-prints , arXiv:1703.06870arXiv:1703.06870.\n\nIdentity Mappings in Deep Residual Networks. K He, X Zhang, S Ren, J Sun, arXiv:1603.05027arXiv:1603.05027ArXiv e-printsHe, K., Zhang, X., Ren, S., Sun, J., 2016. Identity Mappings in Deep Residual Networks. ArXiv e-prints , arXiv:1603.05027arXiv:1603.05027.\n\nG Huang, Z Liu, L Van Der Maaten, K Q Weinberger, arXiv:1608.06993arXiv:1608.06993Densely Connected Convolutional Networks. ArXiv e-prints. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q., 2016. Densely Connected Convolutional Networks. ArXiv e-prints , arXiv:1608.06993arXiv:1608.06993.\n\nCellular community detection for tissue phenotyping in histology images. S Javed, M M Fraz, D Epstein, D Snead, N M Rajpoot, Computational Pathology and Ophthalmic Medical Image Analysis. SpringerJaved, S., Fraz, M.M., Epstein, D., Snead, D., Rajpoot, N.M., 2018. Cellular community detection for tissue phenotyping in histology images, in: Com- putational Pathology and Ophthalmic Medical Image Analysis. Springer, pp. 120-129.\n\nDeep leaning models delineates multi-875 ple nuclear phenotypes in h&e stained histology sections. M Khoshdeli, B Parvin, arXiv:1802.04427arXiv preprintKhoshdeli, M., Parvin, B., 2018. Deep leaning models delineates multi- 875 ple nuclear phenotypes in h&e stained histology sections. arXiv preprint arXiv:1802.04427 .\n\nPanoptic segmentation. A Kirillov, K He, R B Girshick, C Rother, P Doll\u00e1r, arXiv:1801.00868Kirillov, A., He, K., Girshick, R.B., Rother, C., Doll\u00e1r, P., 2018. Panoptic segmentation. CoRR abs/1801.00868. URL: http://arxiv.org/abs/1801. 00868, arXiv:1801.00868.\n\nA dataset and a technique for generalized nuclear segmentation for computational pathology. N Kumar, R Verma, S Sharma, S Bhargava, A Vahadane, A Sethi, 10.1109/TMI.2017.2677499IEEE Transactions on Medical Imaging. 36Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane, A., Sethi, A., 2017. A dataset and a technique for generalized nuclear segmentation for computational pathology. IEEE Transactions on Medical Imaging 36, 1550- 1560. doi:10.1109/TMI.2017.2677499.\n\nNucleus detec-885 tion using gradient orientation information and linear least squares regression. J T Kwak, S M Hewitt, S Xu, P A Pinto, B J Wood, Digital Pathology, International Society for Optics and Photonics. 94200Medical ImagingKwak, J.T., Hewitt, S.M., Xu, S., Pinto, P.A., Wood, B.J., 2015. Nucleus detec- 885 tion using gradient orientation information and linear least squares regression, in: Medical Imaging 2015: Digital Pathology, International Society for Optics and Photonics. p. 94200N.\n\nSegmentation of neuronal nuclei based on clump splitting and a two-890 step binarization of images. A Latorre, L Alonso-Nanclares, S Muelas, J Pea, J Defelipe, 10.1016/j.eswa.2013.06.010Expert Systems with Applications. 40LaTorre, A., Alonso-Nanclares, L., Muelas, S., Pea, J., DeFelipe, J., 2013. Segmentation of neuronal nuclei based on clump splitting and a two- 890 step binarization of images. Expert Systems with Applications 40, 6521 -6530. URL: http://www.sciencedirect.com/science/article/pii/ S0957417413003904, doi:https://doi.org/10.1016/j.eswa.2013.06.010.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, nature. 521436LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. nature 521, 436.\n\n. M Liao, Y Qian Zhao, X Hua Li, P Shan Dai, X Wen Xu, 895Liao, M., qian Zhao, Y., hua Li, X., shan Dai, P., wen Xu, X., kai 895\n\nAutomatic segmentation for cell images based on bottleneck detection and ellipse fitting. J Zhang, B Ji Zou, Neurocomputing. 173Zhang, J., ji Zou, B., 2016. Automatic segmentation for cell images based on bottleneck detection and ellipse fitting. Neurocomputing 173, 615 -622. URL: http://www.sciencedirect.com/science/article/\n\n. / S0925231215011406, Doi, 10.1016/j.neucom.2015.08.006pii/S0925231215011406, doi:https://doi.org/10.1016/j.neucom.2015.\n\nA survey on deep learning in medical image analysis. G Litjens, T Kooi, B E Bejnordi, A A A Setio, F Ciompi, M Ghafoorian, J A Van Der Laak, B Van Ginneken, C I S\u00e1nchez, Medical image analysis. 42Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi, F., Ghafoorian, M., van der Laak, J.A., Van Ginneken, B., S\u00e1nchez, C.I., 2017. A survey on deep learning in medical image analysis. Medical image analysis 42, 60-88.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE conference on computer 905 vision and pattern recognition. the IEEE conference on computer 905 vision and pattern recognitionLong, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for semantic segmentation, in: Proceedings of the IEEE conference on computer 905 vision and pattern recognition, pp. 3431-3440.\n\nNuclear shape and orientation features from h&e images predict survival in early-stage estrogen receptor-positive breast cancers. C Lu, D Romo-Bucheli, X Wang, A Janowczyk, S Ganesan, H Gilmore, D Rimm, A Madabhushi, Laboratory Investigation. 98Lu, C., Romo-Bucheli, D., Wang, X., Janowczyk, A., Ganesan, S., Gilmore, H., Rimm, D., Madabhushi, A., 2018. Nuclear shape and orientation features from h&e images predict survival in early-stage estrogen receptor-positive breast cancers. Laboratory Investigation 98, 1438.\n\nImage analysis and machine learning in digital pathology: Challenges and opportunities. A Madabhushi, G Lee, 10.1016/j.media.2016.06.037Medical Image Analysis. 3320th anniversary of the Medical Image Analysis journal (MedIAMadabhushi, A., Lee, G., 2016. Image analysis and machine learning in digital pathology: Challenges and opportunities. Medical Image Analysis 33, 170 -175. URL: http://www.sciencedirect.com/science/article/ pii/S1361841516301141, doi:https://doi.org/10.1016/j.media.2016. 06.037. 20th anniversary of the Medical Image Analysis journal (MedIA).\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. P Naylor, M La\u00e9, F Reyal, T Walter, IEEE Transactions on Medical Imaging. Naylor, P., La\u00e9, M., Reyal, F., Walter, T., 2018. Segmentation of nuclei in histopathology images by deep regression of the distance map. IEEE Trans- actions on Medical Imaging .\n\nProstate cancer detection: Fusion of cytological and textural features. K Nguyen, A K Jain, B Sabata, Journal of pathology informatics. 2Nguyen, K., Jain, A.K., Sabata, B., 2011. Prostate cancer detection: Fusion of cytological and textural features. Journal of pathology informatics 2.\n\nMicro-Net: A unified model for segmentation of various objects in microscopy images. S E A Raza, L Cheung, M Shaban, S Graham, D Epstein, S Pelengaris, M Khan, N M Rajpoot, arXiv:1804.08145arXiv:1804.08145ArXiv e-printsRaza, S.E.A., Cheung, L., Shaban, M., Graham, S., Epstein, D., Pelen- garis, S., Khan, M., Rajpoot, N.M., 2018. Micro-Net: A unified model for segmentation of various objects in microscopy images. ArXiv e-prints , arXiv:1804.08145arXiv:1804.08145.\n\nU-net: Convolutional networks 925 for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks 925 for biomedical image segmentation, in: International Conference on Medical image computing and computer-assisted intervention, Springer. pp. 234-241.\n\nA multi-resolution approach for combining visual information using nuclei segmentation and classification in histopathological images. H Sharma, N Zerbe, D Heim, S Wienert, H M Behrens, O Hellwich, P Hufnagl, 930 in: VISAPP (3)Sharma, H., Zerbe, N., Heim, D., Wienert, S., Behrens, H.M., Hellwich, O., Hufnagl, P., 2015. A multi-resolution approach for combining visual informa- tion using nuclei segmentation and classification in histopathological images., 930 in: VISAPP (3), pp. 37-46.\n\nDeep learning in medical image analysis. D Shen, G Wu, H I Suk, Annual review of biomedical engineering. 19Shen, D., Wu, G., Suk, H.I., 2017. Deep learning in medical image analysis. Annual review of biomedical engineering 19, 221-248.\n\nLocality sensitive deep learning for detection and 935 classification of nuclei in routine colon cancer histology images. K Sirinukunwattana, Ahmed Raza, S Tsang, Y W Snead, D R Cree, I A Rajpoot, N M , IEEE Trans. Med. Imaging. 35Sirinukunwattana, K., e Ahmed Raza, S., Tsang, Y.W., Snead, D.R., Cree, I.A., Rajpoot, N.M., 2016. Locality sensitive deep learning for detection and 935 classification of nuclei in routine colon cancer histology images. IEEE Trans. Med. Imaging 35, 1196-1206.\n\nNovel digital signatures of tissue phenotypes for predicting distant metastasis in colorectal cancer. K Sirinukunwattana, D Snead, D Epstein, Z Aftab, I Mujeeb, Y W Tsang, I Cree, N Rajpoot, Scientific reports. 940813692Sirinukunwattana, K., Snead, D., Epstein, D., Aftab, Z., Mujeeb, I., Tsang, Y.W., Cree, I., Rajpoot, N., 2018. Novel digital signatures of tissue pheno- types for predicting distant metastasis in colorectal cancer. Scientific reports 940 8, 13692.\n\nAutomatic nuclei segmentation in h&e stained breast cancer histopathology images. M Veta, P Van Diest, R Kornegoor, A Huisman, M Viergever, J Pluim, 10.1371/journal.pone.0070221PLoS ONE. 870221Veta, M., van Diest, P., Kornegoor, R., Huisman, A., Viergever, M., Pluim, J., 2013. Automatic nuclei segmentation in h&e stained breast cancer histopathology images. PLoS ONE 8, e70221. doi:https://doi.org/10. 1371/journal.pone.0070221.\n\nMethods for segmentation and classification of digital microscopy tissue images. Q D Vu, S Graham, M N N To, M Shaban, T Qaiser, N A Koohbanani, S A Khurram, T Kurc, K Farahani, T Zhao, arXiv:1810.13230arXiv preprintVu, Q.D., Graham, S., To, M.N.N., Shaban, M., Qaiser, T., Koohbanani, N.A., Khurram, S.A., Kurc, T., Farahani, K., Zhao, T., et al., 2018. Methods for segmentation and classification of digital microscopy tissue images. arXiv preprint arXiv:1810.13230 .\n\nAutomatic cell nuclei seg-950 mentation and classification of breast cancer histopathology images. P Wang, X Hu, Y Li, Q Liu, X Zhu, Signal Processing. 122Wang, P., Hu, X., Li, Y., Liu, Q., Zhu, X., 2016. Automatic cell nuclei seg- 950 mentation and classification of breast cancer histopathology images. Signal Processing 122, 1-13.\n\nDetection and segmentation of cell nuclei in virtual microscopy images: a minimum-model approach. S Wienert, D Heim, K Saeger, A Stenzinger, M Beil, P Hufnagl, M Dietel, C Denkert, F Klauschen, Scientific. 9552503Wienert, S., Heim, D., Saeger, K., Stenzinger, A., Beil, M., Hufnagl, P., Dietel, M., Denkert, C., Klauschen, F., 2012. Detection and segmentation of cell nuclei in virtual microscopy images: a minimum-model approach. Scientific 955 reports 2, 503.\n\nNuclei segmentation using marker-controlled watershed, tracking using mean-shift, and kalman filter in time-lapse microscopy. X Yang, H Li, X Zhou, IEEE Transactions on Circuits and Systems I: Regular Papers. 53Yang, X., Li, H., Zhou, X., 2006. Nuclei segmentation using marker-controlled watershed, tracking using mean-shift, and kalman filter in time-lapse mi- croscopy. IEEE Transactions on Circuits and Systems I: Regular Papers 53, 2405-2414.\n\nQuantitative image analysis of cellular heterogeneity in breast tumors complements genomic profiling. Y Yuan, H Failmezger, O M Rueda, H R Ali, S Gr\u00e4f, S F Chin, R F Schwarz, C Curtis, M J Dunning, H Bardwell, Science translational medicine. 4Yuan, Y., Failmezger, H., Rueda, O.M., Ali, H.R., Gr\u00e4f, S., Chin, S.F., Schwarz, R.F., Curtis, C., Dunning, M.J., Bardwell, H., et al., 2012. Quantitative im- age analysis of cellular heterogeneity in breast tumors complements genomic profiling. Science translational medicine 4, 157ra143-157ra143.\n\n. Y Zhou, O F Onder, Q Dou, E Tsougenis, H Chen, P A Heng, 965Zhou, Y., Onder, O.F., Dou, Q., Tsougenis, E., Chen, H., Heng, P.A., 2019. 965\n\nCia-net: Robust nuclei instance segmentation with contour-aware information aggregation. arXiv:1903.05358arXiv preprintCia-net: Robust nuclei instance segmentation with contour-aware information aggregation. arXiv preprint arXiv:1903.05358 .\n", "annotations": {"author": "[{\"end\":262,\"start\":102},{\"end\":354,\"start\":263},{\"end\":362,\"start\":355},{\"end\":540,\"start\":363},{\"end\":669,\"start\":541},{\"end\":743,\"start\":670},{\"end\":835,\"start\":744},{\"end\":943,\"start\":836}]", "publisher": null, "author_last_name": "[{\"end\":114,\"start\":108},{\"end\":275,\"start\":268},{\"end\":373,\"start\":369},{\"end\":552,\"start\":548},{\"end\":683,\"start\":678},{\"end\":756,\"start\":752},{\"end\":849,\"start\":842}]", "author_first_name": "[{\"end\":107,\"start\":102},{\"end\":267,\"start\":263},{\"end\":359,\"start\":355},{\"end\":361,\"start\":360},{\"end\":368,\"start\":363},{\"end\":547,\"start\":541},{\"end\":673,\"start\":670},{\"end\":677,\"start\":674},{\"end\":747,\"start\":744},{\"end\":751,\"start\":748},{\"end\":841,\"start\":836}]", "author_affiliation": "[{\"end\":204,\"start\":116},{\"end\":261,\"start\":206},{\"end\":353,\"start\":277},{\"end\":430,\"start\":375},{\"end\":539,\"start\":432},{\"end\":609,\"start\":554},{\"end\":668,\"start\":611},{\"end\":742,\"start\":685},{\"end\":834,\"start\":758},{\"end\":906,\"start\":851},{\"end\":942,\"start\":908}]", "title": "[{\"end\":99,\"start\":1},{\"end\":1042,\"start\":944}]", "venue": null, "abstract": "[{\"end\":6159,\"start\":2241}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6796,\"start\":6775},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7138,\"start\":7112},{\"end\":7542,\"start\":7540},{\"end\":7925,\"start\":7923},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8002,\"start\":7980},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8326,\"start\":8305},{\"end\":8347,\"start\":8326},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9806,\"start\":9788},{\"end\":9931,\"start\":9929},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10241,\"start\":10222},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10304,\"start\":10286},{\"end\":10566,\"start\":10549},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11056,\"start\":11035},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11079,\"start\":11058},{\"end\":11105,\"start\":11080},{\"end\":11174,\"start\":11156},{\"end\":11423,\"start\":11421},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11576,\"start\":11554},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11594,\"start\":11576},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11613,\"start\":11594},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11936,\"start\":11917},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11970,\"start\":11944},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12483,\"start\":12466},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12892,\"start\":12873},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13072,\"start\":13045},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13181,\"start\":13163},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13437,\"start\":13421},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13680,\"start\":13660},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14136,\"start\":14119},{\"end\":15148,\"start\":15113},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15293,\"start\":15275},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15646,\"start\":15616},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16143,\"start\":16126},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16247,\"start\":16228},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16310,\"start\":16290},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17865,\"start\":17845},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18163,\"start\":18137},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24328,\"start\":24311},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24388,\"start\":24368},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26208,\"start\":26186},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26586,\"start\":26563},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32147,\"start\":32127},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32181,\"start\":32164},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32216,\"start\":32195},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32767,\"start\":32747},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33265,\"start\":33246},{\"end\":33670,\"start\":33667},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34738,\"start\":34719},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34901,\"start\":34882},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34929,\"start\":34901},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34945,\"start\":34929},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34989,\"start\":34963},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":35084,\"start\":35066},{\"end\":35108,\"start\":35084},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":35126,\"start\":35108},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35142,\"start\":35126},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35271,\"start\":35247},{\"end\":35306,\"start\":35283},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":35940,\"start\":35922},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36848,\"start\":36824},{\"end\":36883,\"start\":36860},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37664,\"start\":37647},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":37682,\"start\":37664},{\"end\":38621,\"start\":38614},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":40415,\"start\":40398},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":40443,\"start\":40422},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":47753,\"start\":47733},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":47783,\"start\":47753},{\"end\":50238,\"start\":50235},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":57144,\"start\":57124},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":57180,\"start\":57163}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":52475,\"start\":52161},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52837,\"start\":52476},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53145,\"start\":52838},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54735,\"start\":53146},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54989,\"start\":54736},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55861,\"start\":54990},{\"attributes\":{\"id\":\"fig_7\"},\"end\":56369,\"start\":55862},{\"attributes\":{\"id\":\"fig_8\"},\"end\":56620,\"start\":56370},{\"attributes\":{\"id\":\"fig_9\"},\"end\":57081,\"start\":56621},{\"attributes\":{\"id\":\"fig_10\"},\"end\":57377,\"start\":57082},{\"attributes\":{\"id\":\"fig_11\"},\"end\":57837,\"start\":57378},{\"attributes\":{\"id\":\"fig_13\"},\"end\":57944,\"start\":57838},{\"attributes\":{\"id\":\"fig_14\"},\"end\":58443,\"start\":57945},{\"attributes\":{\"id\":\"fig_15\"},\"end\":59320,\"start\":58444},{\"attributes\":{\"id\":\"fig_16\"},\"end\":59905,\"start\":59321},{\"attributes\":{\"id\":\"fig_17\"},\"end\":61035,\"start\":59906},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":61342,\"start\":61036},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":61769,\"start\":61343},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":63138,\"start\":61770},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":63882,\"start\":63139},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":63890,\"start\":63883},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":64553,\"start\":63891},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":65907,\"start\":64554},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":66004,\"start\":65908},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":67258,\"start\":66005},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":67667,\"start\":67259},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":67981,\"start\":67668},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":68228,\"start\":67982},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":68921,\"start\":68229}]", "paragraph": "[{\"end\":6488,\"start\":6161},{\"end\":6584,\"start\":6490},{\"end\":7616,\"start\":6601},{\"end\":8004,\"start\":7618},{\"end\":8468,\"start\":8006},{\"end\":8939,\"start\":8470},{\"end\":9110,\"start\":8941},{\"end\":9285,\"start\":9112},{\"end\":9468,\"start\":9287},{\"end\":9577,\"start\":9470},{\"end\":9773,\"start\":9626},{\"end\":13552,\"start\":9775},{\"end\":13885,\"start\":13554},{\"end\":14325,\"start\":13887},{\"end\":14740,\"start\":14352},{\"end\":15886,\"start\":14748},{\"end\":17692,\"start\":15911},{\"end\":18415,\"start\":17694},{\"end\":18726,\"start\":18433},{\"end\":19358,\"start\":18817},{\"end\":20048,\"start\":19360},{\"end\":20424,\"start\":20201},{\"end\":20693,\"start\":20426},{\"end\":21563,\"start\":20816},{\"end\":21861,\"start\":21601},{\"end\":22277,\"start\":21881},{\"end\":22400,\"start\":22315},{\"end\":22761,\"start\":22448},{\"end\":22982,\"start\":22802},{\"end\":23426,\"start\":22984},{\"end\":23589,\"start\":23434},{\"end\":24287,\"start\":23655},{\"end\":24439,\"start\":24289},{\"end\":24673,\"start\":24441},{\"end\":26231,\"start\":24675},{\"end\":26281,\"start\":26233},{\"end\":26587,\"start\":26407},{\"end\":27185,\"start\":26625},{\"end\":27603,\"start\":27187},{\"end\":28069,\"start\":27699},{\"end\":28410,\"start\":28247},{\"end\":28626,\"start\":28412},{\"end\":29275,\"start\":28628},{\"end\":30158,\"start\":29311},{\"end\":30479,\"start\":30160},{\"end\":31122,\"start\":30481},{\"end\":31358,\"start\":31124},{\"end\":31887,\"start\":31360},{\"end\":32119,\"start\":31889},{\"end\":32423,\"start\":32121},{\"end\":32503,\"start\":32425},{\"end\":32614,\"start\":32505},{\"end\":33036,\"start\":32654},{\"end\":34126,\"start\":33038},{\"end\":35236,\"start\":34175},{\"end\":35312,\"start\":35238},{\"end\":36018,\"start\":35314},{\"end\":36764,\"start\":36020},{\"end\":37239,\"start\":36766},{\"end\":37945,\"start\":37241},{\"end\":38043,\"start\":37947},{\"end\":38473,\"start\":38068},{\"end\":40256,\"start\":38481},{\"end\":41040,\"start\":40258},{\"end\":41556,\"start\":41091},{\"end\":42464,\"start\":41564},{\"end\":44106,\"start\":42466},{\"end\":44786,\"start\":44114},{\"end\":46114,\"start\":44788},{\"end\":46407,\"start\":46116},{\"end\":47120,\"start\":46415},{\"end\":47431,\"start\":47122},{\"end\":47612,\"start\":47433},{\"end\":48423,\"start\":47614},{\"end\":48640,\"start\":48431},{\"end\":49624,\"start\":48673},{\"end\":50811,\"start\":49626},{\"end\":51353,\"start\":50813},{\"end\":51545,\"start\":51361},{\"end\":52160,\"start\":51547}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18816,\"start\":18727},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20200,\"start\":20049},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20815,\"start\":20694},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21600,\"start\":21564},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22314,\"start\":22278},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22447,\"start\":22401},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22801,\"start\":22762},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26406,\"start\":26282},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27698,\"start\":27604},{\"attributes\":{\"id\":\"formula_9\"},\"end\":28246,\"start\":28070}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25093,\"start\":25086},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32006,\"start\":31999},{\"end\":36794,\"start\":36787},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":40314,\"start\":40307},{\"end\":40487,\"start\":40480},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":49150,\"start\":49142},{\"end\":49827,\"start\":49820},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":50705,\"start\":50697},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":51576,\"start\":51568}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":6599,\"start\":6587},{\"attributes\":{\"n\":\"2.\"},\"end\":9592,\"start\":9580},{\"attributes\":{\"n\":\"2.1.\"},\"end\":9624,\"start\":9595},{\"attributes\":{\"n\":\"2.2.\"},\"end\":14350,\"start\":14328},{\"end\":14746,\"start\":14743},{\"attributes\":{\"n\":\"3.1.\"},\"end\":15909,\"start\":15889},{\"attributes\":{\"n\":\"3.1.1.\"},\"end\":18431,\"start\":18418},{\"attributes\":{\"n\":\"3.2.\"},\"end\":21879,\"start\":21864},{\"end\":23432,\"start\":23429},{\"attributes\":{\"n\":\"4.\"},\"end\":23610,\"start\":23592},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23653,\"start\":23613},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26623,\"start\":26590},{\"attributes\":{\"n\":\"5.\"},\"end\":29298,\"start\":29278},{\"attributes\":{\"n\":\"5.1.\"},\"end\":29309,\"start\":29301},{\"attributes\":{\"n\":\"5.2.\"},\"end\":32652,\"start\":32617},{\"attributes\":{\"n\":\"5.3.\"},\"end\":34173,\"start\":34129},{\"attributes\":{\"n\":\"5.4.\"},\"end\":38066,\"start\":38046},{\"end\":38479,\"start\":38476},{\"attributes\":{\"n\":\"5.5.\"},\"end\":41089,\"start\":41043},{\"end\":41562,\"start\":41559},{\"end\":44112,\"start\":44109},{\"end\":46413,\"start\":46410},{\"end\":48429,\"start\":48426},{\"end\":48671,\"start\":48643},{\"end\":51359,\"start\":51356},{\"end\":52172,\"start\":52162},{\"end\":52487,\"start\":52477},{\"end\":52848,\"start\":52839},{\"end\":54747,\"start\":54737},{\"end\":54992,\"start\":54991},{\"end\":55871,\"start\":55863},{\"end\":56381,\"start\":56371},{\"end\":56642,\"start\":56622},{\"end\":57086,\"start\":57083},{\"end\":57849,\"start\":57839},{\"end\":59325,\"start\":59322},{\"end\":61046,\"start\":61037},{\"end\":63149,\"start\":63140},{\"end\":63889,\"start\":63884},{\"end\":65918,\"start\":65909},{\"end\":66013,\"start\":66006},{\"end\":67269,\"start\":67260},{\"end\":67679,\"start\":67669},{\"end\":67993,\"start\":67983},{\"end\":68240,\"start\":68230}]", "table": "[{\"end\":61342,\"start\":61249},{\"end\":61769,\"start\":61763},{\"end\":63138,\"start\":63114},{\"end\":63882,\"start\":63374},{\"end\":64553,\"start\":64457},{\"end\":65907,\"start\":65243},{\"end\":67258,\"start\":66042},{\"end\":67667,\"start\":67354},{\"end\":67981,\"start\":67757},{\"end\":68228,\"start\":68153},{\"end\":68921,\"start\":68718}]", "figure_caption": "[{\"end\":52475,\"start\":52174},{\"end\":52837,\"start\":52489},{\"end\":53145,\"start\":52849},{\"end\":54735,\"start\":53148},{\"end\":54989,\"start\":54749},{\"end\":55861,\"start\":54993},{\"end\":56369,\"start\":55873},{\"end\":56620,\"start\":56383},{\"end\":57081,\"start\":56645},{\"end\":57377,\"start\":57088},{\"end\":57837,\"start\":57380},{\"end\":57944,\"start\":57851},{\"end\":58443,\"start\":57947},{\"end\":59320,\"start\":58446},{\"end\":59905,\"start\":59327},{\"end\":61035,\"start\":59908},{\"end\":61249,\"start\":61048},{\"end\":61763,\"start\":61345},{\"end\":63114,\"start\":61772},{\"end\":63374,\"start\":63151},{\"end\":64457,\"start\":63893},{\"end\":65243,\"start\":64556},{\"end\":66004,\"start\":65920},{\"end\":66042,\"start\":66015},{\"end\":67354,\"start\":67271},{\"end\":67757,\"start\":67682},{\"end\":68153,\"start\":67996},{\"end\":68718,\"start\":68243}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22017,\"start\":22011},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22981,\"start\":22975},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":24921,\"start\":24915},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25997,\"start\":25991},{\"end\":32285,\"start\":32279},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":36823,\"start\":36816}]", "bib_author_first_name": "[{\"end\":69562,\"start\":69561},{\"end\":69571,\"start\":69570},{\"end\":69581,\"start\":69580},{\"end\":69589,\"start\":69588},{\"end\":69597,\"start\":69596},{\"end\":69606,\"start\":69605},{\"end\":69614,\"start\":69613},{\"end\":69623,\"start\":69622},{\"end\":69635,\"start\":69634},{\"end\":69645,\"start\":69644},{\"end\":70213,\"start\":70212},{\"end\":70225,\"start\":70224},{\"end\":70245,\"start\":70244},{\"end\":70249,\"start\":70246},{\"end\":70257,\"start\":70256},{\"end\":70266,\"start\":70265},{\"end\":70723,\"start\":70722},{\"end\":70732,\"start\":70731},{\"end\":70742,\"start\":70741},{\"end\":70746,\"start\":70743},{\"end\":71047,\"start\":71046},{\"end\":71065,\"start\":71064},{\"end\":71076,\"start\":71075},{\"end\":71387,\"start\":71386},{\"end\":71399,\"start\":71398},{\"end\":71401,\"start\":71400},{\"end\":71413,\"start\":71412},{\"end\":71415,\"start\":71414},{\"end\":71428,\"start\":71427},{\"end\":71442,\"start\":71441},{\"end\":71444,\"start\":71443},{\"end\":71453,\"start\":71452},{\"end\":71455,\"start\":71454},{\"end\":71464,\"start\":71463},{\"end\":71475,\"start\":71474},{\"end\":71477,\"start\":71476},{\"end\":71485,\"start\":71484},{\"end\":71487,\"start\":71486},{\"end\":71497,\"start\":71496},{\"end\":71499,\"start\":71498},{\"end\":71894,\"start\":71893},{\"end\":71896,\"start\":71895},{\"end\":71909,\"start\":71908},{\"end\":71911,\"start\":71910},{\"end\":71920,\"start\":71919},{\"end\":71922,\"start\":71921},{\"end\":71935,\"start\":71934},{\"end\":71945,\"start\":71944},{\"end\":71947,\"start\":71946},{\"end\":71955,\"start\":71954},{\"end\":71965,\"start\":71964},{\"end\":71967,\"start\":71966},{\"end\":71978,\"start\":71977},{\"end\":71980,\"start\":71979},{\"end\":71989,\"start\":71988},{\"end\":71991,\"start\":71990},{\"end\":72004,\"start\":72003},{\"end\":72375,\"start\":72374},{\"end\":72383,\"start\":72382},{\"end\":72389,\"start\":72388},{\"end\":72395,\"start\":72394},{\"end\":72397,\"start\":72396},{\"end\":72834,\"start\":72833},{\"end\":72843,\"start\":72842},{\"end\":72845,\"start\":72844},{\"end\":73164,\"start\":73163},{\"end\":73176,\"start\":73175},{\"end\":73184,\"start\":73183},{\"end\":73192,\"start\":73191},{\"end\":73198,\"start\":73197},{\"end\":73204,\"start\":73203},{\"end\":73215,\"start\":73214},{\"end\":73217,\"start\":73216},{\"end\":73225,\"start\":73224},{\"end\":73233,\"start\":73232},{\"end\":73243,\"start\":73242},{\"end\":73245,\"start\":73244},{\"end\":73634,\"start\":73633},{\"end\":73641,\"start\":73640},{\"end\":73650,\"start\":73649},{\"end\":73657,\"start\":73656},{\"end\":73666,\"start\":73665},{\"end\":73946,\"start\":73945},{\"end\":73954,\"start\":73953},{\"end\":73962,\"start\":73961},{\"end\":73972,\"start\":73971},{\"end\":73974,\"start\":73973},{\"end\":73980,\"start\":73979},{\"end\":73986,\"start\":73985},{\"end\":74213,\"start\":74212},{\"end\":74215,\"start\":74214},{\"end\":74225,\"start\":74224},{\"end\":74227,\"start\":74226},{\"end\":74238,\"start\":74237},{\"end\":74240,\"start\":74239},{\"end\":74250,\"start\":74249},{\"end\":74252,\"start\":74251},{\"end\":74262,\"start\":74261},{\"end\":74271,\"start\":74270},{\"end\":74273,\"start\":74272},{\"end\":74285,\"start\":74284},{\"end\":74287,\"start\":74286},{\"end\":74297,\"start\":74296},{\"end\":74299,\"start\":74298},{\"end\":74307,\"start\":74306},{\"end\":74309,\"start\":74308},{\"end\":74320,\"start\":74319},{\"end\":74322,\"start\":74321},{\"end\":74705,\"start\":74704},{\"end\":74715,\"start\":74714},{\"end\":74717,\"start\":74716},{\"end\":75038,\"start\":75037},{\"end\":75040,\"start\":75039},{\"end\":75050,\"start\":75049},{\"end\":75052,\"start\":75051},{\"end\":75065,\"start\":75064},{\"end\":75072,\"start\":75071},{\"end\":75086,\"start\":75085},{\"end\":75088,\"start\":75087},{\"end\":75215,\"start\":75214},{\"end\":75380,\"start\":75379},{\"end\":75386,\"start\":75385},{\"end\":75398,\"start\":75397},{\"end\":75408,\"start\":75407},{\"end\":75641,\"start\":75640},{\"end\":75647,\"start\":75646},{\"end\":75656,\"start\":75655},{\"end\":75663,\"start\":75662},{\"end\":75856,\"start\":75855},{\"end\":75865,\"start\":75864},{\"end\":75872,\"start\":75871},{\"end\":75890,\"start\":75889},{\"end\":75892,\"start\":75891},{\"end\":76227,\"start\":76226},{\"end\":76236,\"start\":76235},{\"end\":76238,\"start\":76237},{\"end\":76246,\"start\":76245},{\"end\":76257,\"start\":76256},{\"end\":76266,\"start\":76265},{\"end\":76268,\"start\":76267},{\"end\":76683,\"start\":76682},{\"end\":76696,\"start\":76695},{\"end\":76927,\"start\":76926},{\"end\":76939,\"start\":76938},{\"end\":76945,\"start\":76944},{\"end\":76947,\"start\":76946},{\"end\":76959,\"start\":76958},{\"end\":76969,\"start\":76968},{\"end\":77257,\"start\":77256},{\"end\":77266,\"start\":77265},{\"end\":77275,\"start\":77274},{\"end\":77285,\"start\":77284},{\"end\":77297,\"start\":77296},{\"end\":77309,\"start\":77308},{\"end\":77736,\"start\":77735},{\"end\":77738,\"start\":77737},{\"end\":77746,\"start\":77745},{\"end\":77748,\"start\":77747},{\"end\":77758,\"start\":77757},{\"end\":77764,\"start\":77763},{\"end\":77766,\"start\":77765},{\"end\":77775,\"start\":77774},{\"end\":77777,\"start\":77776},{\"end\":78242,\"start\":78241},{\"end\":78253,\"start\":78252},{\"end\":78273,\"start\":78272},{\"end\":78283,\"start\":78282},{\"end\":78290,\"start\":78289},{\"end\":78728,\"start\":78727},{\"end\":78737,\"start\":78736},{\"end\":78747,\"start\":78746},{\"end\":78847,\"start\":78846},{\"end\":78855,\"start\":78854},{\"end\":78868,\"start\":78867},{\"end\":78878,\"start\":78877},{\"end\":78890,\"start\":78889},{\"end\":79065,\"start\":79064},{\"end\":79074,\"start\":79073},{\"end\":79306,\"start\":79305},{\"end\":79480,\"start\":79479},{\"end\":79491,\"start\":79490},{\"end\":79499,\"start\":79498},{\"end\":79501,\"start\":79500},{\"end\":79513,\"start\":79512},{\"end\":79517,\"start\":79514},{\"end\":79526,\"start\":79525},{\"end\":79536,\"start\":79535},{\"end\":79550,\"start\":79549},{\"end\":79552,\"start\":79551},{\"end\":79568,\"start\":79567},{\"end\":79584,\"start\":79583},{\"end\":79586,\"start\":79585},{\"end\":79908,\"start\":79907},{\"end\":79916,\"start\":79915},{\"end\":79929,\"start\":79928},{\"end\":80422,\"start\":80421},{\"end\":80428,\"start\":80427},{\"end\":80444,\"start\":80443},{\"end\":80452,\"start\":80451},{\"end\":80465,\"start\":80464},{\"end\":80476,\"start\":80475},{\"end\":80487,\"start\":80486},{\"end\":80495,\"start\":80494},{\"end\":80900,\"start\":80899},{\"end\":80914,\"start\":80913},{\"end\":81468,\"start\":81467},{\"end\":81478,\"start\":81477},{\"end\":81485,\"start\":81484},{\"end\":81494,\"start\":81493},{\"end\":81794,\"start\":81793},{\"end\":81804,\"start\":81803},{\"end\":81806,\"start\":81805},{\"end\":81814,\"start\":81813},{\"end\":82095,\"start\":82094},{\"end\":82099,\"start\":82096},{\"end\":82107,\"start\":82106},{\"end\":82117,\"start\":82116},{\"end\":82127,\"start\":82126},{\"end\":82137,\"start\":82136},{\"end\":82148,\"start\":82147},{\"end\":82162,\"start\":82161},{\"end\":82170,\"start\":82169},{\"end\":82172,\"start\":82171},{\"end\":82547,\"start\":82546},{\"end\":82562,\"start\":82561},{\"end\":82573,\"start\":82572},{\"end\":83043,\"start\":83042},{\"end\":83053,\"start\":83052},{\"end\":83062,\"start\":83061},{\"end\":83070,\"start\":83069},{\"end\":83081,\"start\":83080},{\"end\":83083,\"start\":83082},{\"end\":83094,\"start\":83093},{\"end\":83106,\"start\":83105},{\"end\":83440,\"start\":83439},{\"end\":83448,\"start\":83447},{\"end\":83454,\"start\":83453},{\"end\":83456,\"start\":83455},{\"end\":83758,\"start\":83757},{\"end\":83782,\"start\":83777},{\"end\":83790,\"start\":83789},{\"end\":83799,\"start\":83798},{\"end\":83801,\"start\":83800},{\"end\":83810,\"start\":83809},{\"end\":83812,\"start\":83811},{\"end\":83820,\"start\":83819},{\"end\":83822,\"start\":83821},{\"end\":83833,\"start\":83832},{\"end\":83835,\"start\":83834},{\"end\":84231,\"start\":84230},{\"end\":84251,\"start\":84250},{\"end\":84260,\"start\":84259},{\"end\":84271,\"start\":84270},{\"end\":84280,\"start\":84279},{\"end\":84290,\"start\":84289},{\"end\":84292,\"start\":84291},{\"end\":84301,\"start\":84300},{\"end\":84309,\"start\":84308},{\"end\":84680,\"start\":84679},{\"end\":84688,\"start\":84687},{\"end\":84701,\"start\":84700},{\"end\":84714,\"start\":84713},{\"end\":84725,\"start\":84724},{\"end\":84738,\"start\":84737},{\"end\":85111,\"start\":85110},{\"end\":85113,\"start\":85112},{\"end\":85119,\"start\":85118},{\"end\":85129,\"start\":85128},{\"end\":85133,\"start\":85130},{\"end\":85139,\"start\":85138},{\"end\":85149,\"start\":85148},{\"end\":85159,\"start\":85158},{\"end\":85161,\"start\":85160},{\"end\":85175,\"start\":85174},{\"end\":85177,\"start\":85176},{\"end\":85188,\"start\":85187},{\"end\":85196,\"start\":85195},{\"end\":85208,\"start\":85207},{\"end\":85600,\"start\":85599},{\"end\":85608,\"start\":85607},{\"end\":85614,\"start\":85613},{\"end\":85620,\"start\":85619},{\"end\":85627,\"start\":85626},{\"end\":85934,\"start\":85933},{\"end\":85945,\"start\":85944},{\"end\":85953,\"start\":85952},{\"end\":85963,\"start\":85962},{\"end\":85977,\"start\":85976},{\"end\":85985,\"start\":85984},{\"end\":85996,\"start\":85995},{\"end\":86006,\"start\":86005},{\"end\":86017,\"start\":86016},{\"end\":86425,\"start\":86424},{\"end\":86433,\"start\":86432},{\"end\":86439,\"start\":86438},{\"end\":86850,\"start\":86849},{\"end\":86858,\"start\":86857},{\"end\":86872,\"start\":86871},{\"end\":86874,\"start\":86873},{\"end\":86883,\"start\":86882},{\"end\":86885,\"start\":86884},{\"end\":86892,\"start\":86891},{\"end\":86900,\"start\":86899},{\"end\":86902,\"start\":86901},{\"end\":86910,\"start\":86909},{\"end\":86912,\"start\":86911},{\"end\":86923,\"start\":86922},{\"end\":86933,\"start\":86932},{\"end\":86935,\"start\":86934},{\"end\":86946,\"start\":86945},{\"end\":87293,\"start\":87292},{\"end\":87301,\"start\":87300},{\"end\":87303,\"start\":87302},{\"end\":87312,\"start\":87311},{\"end\":87319,\"start\":87318},{\"end\":87332,\"start\":87331},{\"end\":87340,\"start\":87339},{\"end\":87342,\"start\":87341}]", "bib_author_last_name": "[{\"end\":69568,\"start\":69563},{\"end\":69578,\"start\":69572},{\"end\":69586,\"start\":69582},{\"end\":69594,\"start\":69590},{\"end\":69603,\"start\":69598},{\"end\":69611,\"start\":69607},{\"end\":69620,\"start\":69615},{\"end\":69632,\"start\":69624},{\"end\":69642,\"start\":69636},{\"end\":69651,\"start\":69646},{\"end\":70222,\"start\":70214},{\"end\":70242,\"start\":70226},{\"end\":70254,\"start\":70250},{\"end\":70263,\"start\":70258},{\"end\":70274,\"start\":70267},{\"end\":70729,\"start\":70724},{\"end\":70739,\"start\":70733},{\"end\":70751,\"start\":70747},{\"end\":71062,\"start\":71048},{\"end\":71073,\"start\":71066},{\"end\":71084,\"start\":71077},{\"end\":71396,\"start\":71388},{\"end\":71410,\"start\":71402},{\"end\":71425,\"start\":71416},{\"end\":71439,\"start\":71429},{\"end\":71450,\"start\":71445},{\"end\":71461,\"start\":71456},{\"end\":71472,\"start\":71465},{\"end\":71482,\"start\":71478},{\"end\":71494,\"start\":71488},{\"end\":71507,\"start\":71500},{\"end\":71906,\"start\":71897},{\"end\":71917,\"start\":71912},{\"end\":71932,\"start\":71923},{\"end\":71942,\"start\":71936},{\"end\":71952,\"start\":71948},{\"end\":71962,\"start\":71956},{\"end\":71975,\"start\":71968},{\"end\":71986,\"start\":71981},{\"end\":72001,\"start\":71992},{\"end\":72011,\"start\":72005},{\"end\":72380,\"start\":72376},{\"end\":72386,\"start\":72384},{\"end\":72392,\"start\":72390},{\"end\":72402,\"start\":72398},{\"end\":72840,\"start\":72835},{\"end\":72855,\"start\":72846},{\"end\":73173,\"start\":73165},{\"end\":73181,\"start\":73177},{\"end\":73189,\"start\":73185},{\"end\":73195,\"start\":73193},{\"end\":73201,\"start\":73199},{\"end\":73212,\"start\":73205},{\"end\":73222,\"start\":73218},{\"end\":73230,\"start\":73226},{\"end\":73240,\"start\":73234},{\"end\":73254,\"start\":73246},{\"end\":73638,\"start\":73635},{\"end\":73647,\"start\":73642},{\"end\":73654,\"start\":73651},{\"end\":73663,\"start\":73658},{\"end\":73669,\"start\":73667},{\"end\":73951,\"start\":73947},{\"end\":73959,\"start\":73955},{\"end\":73969,\"start\":73963},{\"end\":73977,\"start\":73975},{\"end\":73983,\"start\":73981},{\"end\":73994,\"start\":73987},{\"end\":74222,\"start\":74216},{\"end\":74235,\"start\":74228},{\"end\":74247,\"start\":74241},{\"end\":74259,\"start\":74253},{\"end\":74268,\"start\":74263},{\"end\":74282,\"start\":74274},{\"end\":74294,\"start\":74288},{\"end\":74304,\"start\":74300},{\"end\":74317,\"start\":74310},{\"end\":74330,\"start\":74323},{\"end\":74712,\"start\":74706},{\"end\":74725,\"start\":74718},{\"end\":75047,\"start\":75041},{\"end\":75062,\"start\":75053},{\"end\":75069,\"start\":75066},{\"end\":75083,\"start\":75073},{\"end\":75096,\"start\":75089},{\"end\":75221,\"start\":75216},{\"end\":75383,\"start\":75381},{\"end\":75395,\"start\":75387},{\"end\":75405,\"start\":75399},{\"end\":75417,\"start\":75409},{\"end\":75644,\"start\":75642},{\"end\":75653,\"start\":75648},{\"end\":75660,\"start\":75657},{\"end\":75667,\"start\":75664},{\"end\":75862,\"start\":75857},{\"end\":75869,\"start\":75866},{\"end\":75887,\"start\":75873},{\"end\":75903,\"start\":75893},{\"end\":76233,\"start\":76228},{\"end\":76243,\"start\":76239},{\"end\":76254,\"start\":76247},{\"end\":76263,\"start\":76258},{\"end\":76276,\"start\":76269},{\"end\":76693,\"start\":76684},{\"end\":76703,\"start\":76697},{\"end\":76936,\"start\":76928},{\"end\":76942,\"start\":76940},{\"end\":76956,\"start\":76948},{\"end\":76966,\"start\":76960},{\"end\":76976,\"start\":76970},{\"end\":77263,\"start\":77258},{\"end\":77272,\"start\":77267},{\"end\":77282,\"start\":77276},{\"end\":77294,\"start\":77286},{\"end\":77306,\"start\":77298},{\"end\":77315,\"start\":77310},{\"end\":77743,\"start\":77739},{\"end\":77755,\"start\":77749},{\"end\":77761,\"start\":77759},{\"end\":77772,\"start\":77767},{\"end\":77782,\"start\":77778},{\"end\":78250,\"start\":78243},{\"end\":78270,\"start\":78254},{\"end\":78280,\"start\":78274},{\"end\":78287,\"start\":78284},{\"end\":78299,\"start\":78291},{\"end\":78734,\"start\":78729},{\"end\":78744,\"start\":78738},{\"end\":78754,\"start\":78748},{\"end\":78852,\"start\":78848},{\"end\":78865,\"start\":78856},{\"end\":78875,\"start\":78869},{\"end\":78887,\"start\":78879},{\"end\":78897,\"start\":78891},{\"end\":79071,\"start\":79066},{\"end\":79081,\"start\":79075},{\"end\":79324,\"start\":79307},{\"end\":79329,\"start\":79326},{\"end\":79488,\"start\":79481},{\"end\":79496,\"start\":79492},{\"end\":79510,\"start\":79502},{\"end\":79523,\"start\":79518},{\"end\":79533,\"start\":79527},{\"end\":79547,\"start\":79537},{\"end\":79565,\"start\":79553},{\"end\":79581,\"start\":79569},{\"end\":79594,\"start\":79587},{\"end\":79913,\"start\":79909},{\"end\":79926,\"start\":79917},{\"end\":79937,\"start\":79930},{\"end\":80425,\"start\":80423},{\"end\":80441,\"start\":80429},{\"end\":80449,\"start\":80445},{\"end\":80462,\"start\":80453},{\"end\":80473,\"start\":80466},{\"end\":80484,\"start\":80477},{\"end\":80492,\"start\":80488},{\"end\":80506,\"start\":80496},{\"end\":80911,\"start\":80901},{\"end\":80918,\"start\":80915},{\"end\":81475,\"start\":81469},{\"end\":81482,\"start\":81479},{\"end\":81491,\"start\":81486},{\"end\":81501,\"start\":81495},{\"end\":81801,\"start\":81795},{\"end\":81811,\"start\":81807},{\"end\":81821,\"start\":81815},{\"end\":82104,\"start\":82100},{\"end\":82114,\"start\":82108},{\"end\":82124,\"start\":82118},{\"end\":82134,\"start\":82128},{\"end\":82145,\"start\":82138},{\"end\":82159,\"start\":82149},{\"end\":82167,\"start\":82163},{\"end\":82180,\"start\":82173},{\"end\":82559,\"start\":82548},{\"end\":82570,\"start\":82563},{\"end\":82578,\"start\":82574},{\"end\":83050,\"start\":83044},{\"end\":83059,\"start\":83054},{\"end\":83067,\"start\":83063},{\"end\":83078,\"start\":83071},{\"end\":83091,\"start\":83084},{\"end\":83103,\"start\":83095},{\"end\":83114,\"start\":83107},{\"end\":83445,\"start\":83441},{\"end\":83451,\"start\":83449},{\"end\":83460,\"start\":83457},{\"end\":83775,\"start\":83759},{\"end\":83787,\"start\":83783},{\"end\":83796,\"start\":83791},{\"end\":83807,\"start\":83802},{\"end\":83817,\"start\":83813},{\"end\":83830,\"start\":83823},{\"end\":84248,\"start\":84232},{\"end\":84257,\"start\":84252},{\"end\":84268,\"start\":84261},{\"end\":84277,\"start\":84272},{\"end\":84287,\"start\":84281},{\"end\":84298,\"start\":84293},{\"end\":84306,\"start\":84302},{\"end\":84317,\"start\":84310},{\"end\":84685,\"start\":84681},{\"end\":84698,\"start\":84689},{\"end\":84711,\"start\":84702},{\"end\":84722,\"start\":84715},{\"end\":84735,\"start\":84726},{\"end\":84744,\"start\":84739},{\"end\":85116,\"start\":85114},{\"end\":85126,\"start\":85120},{\"end\":85136,\"start\":85134},{\"end\":85146,\"start\":85140},{\"end\":85156,\"start\":85150},{\"end\":85172,\"start\":85162},{\"end\":85185,\"start\":85178},{\"end\":85193,\"start\":85189},{\"end\":85205,\"start\":85197},{\"end\":85213,\"start\":85209},{\"end\":85605,\"start\":85601},{\"end\":85611,\"start\":85609},{\"end\":85617,\"start\":85615},{\"end\":85624,\"start\":85621},{\"end\":85631,\"start\":85628},{\"end\":85942,\"start\":85935},{\"end\":85950,\"start\":85946},{\"end\":85960,\"start\":85954},{\"end\":85974,\"start\":85964},{\"end\":85982,\"start\":85978},{\"end\":85993,\"start\":85986},{\"end\":86003,\"start\":85997},{\"end\":86014,\"start\":86007},{\"end\":86027,\"start\":86018},{\"end\":86430,\"start\":86426},{\"end\":86436,\"start\":86434},{\"end\":86444,\"start\":86440},{\"end\":86855,\"start\":86851},{\"end\":86869,\"start\":86859},{\"end\":86880,\"start\":86875},{\"end\":86889,\"start\":86886},{\"end\":86897,\"start\":86893},{\"end\":86907,\"start\":86903},{\"end\":86920,\"start\":86913},{\"end\":86930,\"start\":86924},{\"end\":86943,\"start\":86936},{\"end\":86955,\"start\":86947},{\"end\":87298,\"start\":87294},{\"end\":87309,\"start\":87304},{\"end\":87316,\"start\":87313},{\"end\":87329,\"start\":87320},{\"end\":87337,\"start\":87333},{\"end\":87347,\"start\":87343}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":69861,\"start\":69506},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7480362},\"end\":70112,\"start\":69863},{\"attributes\":{\"id\":\"b2\"},\"end\":70646,\"start\":70114},{\"attributes\":{\"doi\":\"arXiv:1711.09856\",\"id\":\"b3\"},\"end\":70962,\"start\":70648},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":60814714},\"end\":71317,\"start\":70964},{\"attributes\":{\"id\":\"b5\"},\"end\":71799,\"start\":71319},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":215779792},\"end\":72305,\"start\":71801},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206593732},\"end\":72753,\"start\":72307},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5315221},\"end\":73082,\"start\":72755},{\"attributes\":{\"id\":\"b9\"},\"end\":73529,\"start\":73084},{\"attributes\":{\"doi\":\"arXiv:1803.02786\",\"id\":\"b10\"},\"end\":73890,\"start\":73531},{\"attributes\":{\"id\":\"b11\"},\"end\":74130,\"start\":73892},{\"attributes\":{\"id\":\"b12\"},\"end\":74567,\"start\":74132},{\"attributes\":{\"id\":\"b13\"},\"end\":74600,\"start\":74569},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":44146054},\"end\":75033,\"start\":74602},{\"attributes\":{\"id\":\"b15\"},\"end\":75168,\"start\":75035},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8575576},\"end\":75375,\"start\":75170},{\"attributes\":{\"doi\":\"arXiv:1703.06870arXiv:1703.06870\",\"id\":\"b17\"},\"end\":75593,\"start\":75377},{\"attributes\":{\"doi\":\"arXiv:1603.05027arXiv:1603.05027\",\"id\":\"b18\"},\"end\":75853,\"start\":75595},{\"attributes\":{\"doi\":\"arXiv:1608.06993arXiv:1608.06993\",\"id\":\"b19\"},\"end\":76151,\"start\":75855},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52278924},\"end\":76581,\"start\":76153},{\"attributes\":{\"doi\":\"arXiv:1802.04427\",\"id\":\"b21\"},\"end\":76901,\"start\":76583},{\"attributes\":{\"doi\":\"arXiv:1801.00868\",\"id\":\"b22\"},\"end\":77162,\"start\":76903},{\"attributes\":{\"doi\":\"10.1109/TMI.2017.2677499\",\"id\":\"b23\",\"matched_paper_id\":5162860},\"end\":77634,\"start\":77164},{\"attributes\":{\"id\":\"b24\"},\"end\":78139,\"start\":77636},{\"attributes\":{\"doi\":\"10.1016/j.eswa.2013.06.010\",\"id\":\"b25\",\"matched_paper_id\":15703589},\"end\":78710,\"start\":78141},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1779661},\"end\":78842,\"start\":78712},{\"attributes\":{\"id\":\"b27\"},\"end\":78972,\"start\":78844},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":31125314},\"end\":79301,\"start\":78974},{\"attributes\":{\"doi\":\"10.1016/j.neucom.2015.08.006\",\"id\":\"b29\"},\"end\":79424,\"start\":79303},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2088679},\"end\":79849,\"start\":79426},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1629541},\"end\":80289,\"start\":79851},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":49617253},\"end\":80809,\"start\":80291},{\"attributes\":{\"doi\":\"10.1016/j.media.2016.06.037\",\"id\":\"b33\",\"matched_paper_id\":4773169},\"end\":81377,\"start\":80811},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":59601271},\"end\":81719,\"start\":81379},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5803415},\"end\":82007,\"start\":81721},{\"attributes\":{\"doi\":\"arXiv:1804.08145arXiv:1804.08145\",\"id\":\"b36\"},\"end\":82475,\"start\":82009},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3719281},\"end\":82905,\"start\":82477},{\"attributes\":{\"id\":\"b38\"},\"end\":83396,\"start\":82907},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":7961631},\"end\":83633,\"start\":83398},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":54556210},\"end\":84126,\"start\":83635},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":205655668},\"end\":84595,\"start\":84128},{\"attributes\":{\"doi\":\"10.1371/journal.pone.0070221\",\"id\":\"b42\",\"matched_paper_id\":2092326},\"end\":85027,\"start\":84597},{\"attributes\":{\"doi\":\"arXiv:1810.13230\",\"id\":\"b43\"},\"end\":85498,\"start\":85029},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":887323},\"end\":85833,\"start\":85500},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":9747597},\"end\":86296,\"start\":85835},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":18653283},\"end\":86745,\"start\":86298},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":206680596},\"end\":87288,\"start\":86747},{\"attributes\":{\"id\":\"b48\"},\"end\":87430,\"start\":87290},{\"attributes\":{\"doi\":\"arXiv:1903.05358\",\"id\":\"b49\"},\"end\":87673,\"start\":87432}]", "bib_title": "[{\"end\":69940,\"start\":69863},{\"end\":70210,\"start\":70114},{\"end\":71044,\"start\":70964},{\"end\":71891,\"start\":71801},{\"end\":72372,\"start\":72307},{\"end\":72831,\"start\":72755},{\"end\":73161,\"start\":73084},{\"end\":74702,\"start\":74602},{\"end\":75212,\"start\":75170},{\"end\":76224,\"start\":76153},{\"end\":77254,\"start\":77164},{\"end\":77733,\"start\":77636},{\"end\":78239,\"start\":78141},{\"end\":78725,\"start\":78712},{\"end\":79062,\"start\":78974},{\"end\":79477,\"start\":79426},{\"end\":79905,\"start\":79851},{\"end\":80419,\"start\":80291},{\"end\":80897,\"start\":80811},{\"end\":81465,\"start\":81379},{\"end\":81791,\"start\":81721},{\"end\":82544,\"start\":82477},{\"end\":83437,\"start\":83398},{\"end\":83755,\"start\":83635},{\"end\":84228,\"start\":84128},{\"end\":84677,\"start\":84597},{\"end\":85597,\"start\":85500},{\"end\":85931,\"start\":85835},{\"end\":86422,\"start\":86298},{\"end\":86847,\"start\":86747}]", "bib_author": "[{\"end\":69570,\"start\":69561},{\"end\":69580,\"start\":69570},{\"end\":69588,\"start\":69580},{\"end\":69596,\"start\":69588},{\"end\":69605,\"start\":69596},{\"end\":69613,\"start\":69605},{\"end\":69622,\"start\":69613},{\"end\":69634,\"start\":69622},{\"end\":69644,\"start\":69634},{\"end\":69653,\"start\":69644},{\"end\":70224,\"start\":70212},{\"end\":70244,\"start\":70224},{\"end\":70256,\"start\":70244},{\"end\":70265,\"start\":70256},{\"end\":70276,\"start\":70265},{\"end\":70731,\"start\":70722},{\"end\":70741,\"start\":70731},{\"end\":70753,\"start\":70741},{\"end\":71064,\"start\":71046},{\"end\":71075,\"start\":71064},{\"end\":71086,\"start\":71075},{\"end\":71398,\"start\":71386},{\"end\":71412,\"start\":71398},{\"end\":71427,\"start\":71412},{\"end\":71441,\"start\":71427},{\"end\":71452,\"start\":71441},{\"end\":71463,\"start\":71452},{\"end\":71474,\"start\":71463},{\"end\":71484,\"start\":71474},{\"end\":71496,\"start\":71484},{\"end\":71509,\"start\":71496},{\"end\":71908,\"start\":71893},{\"end\":71919,\"start\":71908},{\"end\":71934,\"start\":71919},{\"end\":71944,\"start\":71934},{\"end\":71954,\"start\":71944},{\"end\":71964,\"start\":71954},{\"end\":71977,\"start\":71964},{\"end\":71988,\"start\":71977},{\"end\":72003,\"start\":71988},{\"end\":72013,\"start\":72003},{\"end\":72382,\"start\":72374},{\"end\":72388,\"start\":72382},{\"end\":72394,\"start\":72388},{\"end\":72404,\"start\":72394},{\"end\":72842,\"start\":72833},{\"end\":72857,\"start\":72842},{\"end\":73175,\"start\":73163},{\"end\":73183,\"start\":73175},{\"end\":73191,\"start\":73183},{\"end\":73197,\"start\":73191},{\"end\":73203,\"start\":73197},{\"end\":73214,\"start\":73203},{\"end\":73224,\"start\":73214},{\"end\":73232,\"start\":73224},{\"end\":73242,\"start\":73232},{\"end\":73256,\"start\":73242},{\"end\":73640,\"start\":73633},{\"end\":73649,\"start\":73640},{\"end\":73656,\"start\":73649},{\"end\":73665,\"start\":73656},{\"end\":73671,\"start\":73665},{\"end\":73953,\"start\":73945},{\"end\":73961,\"start\":73953},{\"end\":73971,\"start\":73961},{\"end\":73979,\"start\":73971},{\"end\":73985,\"start\":73979},{\"end\":73996,\"start\":73985},{\"end\":74224,\"start\":74212},{\"end\":74237,\"start\":74224},{\"end\":74249,\"start\":74237},{\"end\":74261,\"start\":74249},{\"end\":74270,\"start\":74261},{\"end\":74284,\"start\":74270},{\"end\":74296,\"start\":74284},{\"end\":74306,\"start\":74296},{\"end\":74319,\"start\":74306},{\"end\":74332,\"start\":74319},{\"end\":74714,\"start\":74704},{\"end\":74727,\"start\":74714},{\"end\":75049,\"start\":75037},{\"end\":75064,\"start\":75049},{\"end\":75071,\"start\":75064},{\"end\":75085,\"start\":75071},{\"end\":75098,\"start\":75085},{\"end\":75223,\"start\":75214},{\"end\":75385,\"start\":75379},{\"end\":75397,\"start\":75385},{\"end\":75407,\"start\":75397},{\"end\":75419,\"start\":75407},{\"end\":75646,\"start\":75640},{\"end\":75655,\"start\":75646},{\"end\":75662,\"start\":75655},{\"end\":75669,\"start\":75662},{\"end\":75864,\"start\":75855},{\"end\":75871,\"start\":75864},{\"end\":75889,\"start\":75871},{\"end\":75905,\"start\":75889},{\"end\":76235,\"start\":76226},{\"end\":76245,\"start\":76235},{\"end\":76256,\"start\":76245},{\"end\":76265,\"start\":76256},{\"end\":76278,\"start\":76265},{\"end\":76695,\"start\":76682},{\"end\":76705,\"start\":76695},{\"end\":76938,\"start\":76926},{\"end\":76944,\"start\":76938},{\"end\":76958,\"start\":76944},{\"end\":76968,\"start\":76958},{\"end\":76978,\"start\":76968},{\"end\":77265,\"start\":77256},{\"end\":77274,\"start\":77265},{\"end\":77284,\"start\":77274},{\"end\":77296,\"start\":77284},{\"end\":77308,\"start\":77296},{\"end\":77317,\"start\":77308},{\"end\":77745,\"start\":77735},{\"end\":77757,\"start\":77745},{\"end\":77763,\"start\":77757},{\"end\":77774,\"start\":77763},{\"end\":77784,\"start\":77774},{\"end\":78252,\"start\":78241},{\"end\":78272,\"start\":78252},{\"end\":78282,\"start\":78272},{\"end\":78289,\"start\":78282},{\"end\":78301,\"start\":78289},{\"end\":78736,\"start\":78727},{\"end\":78746,\"start\":78736},{\"end\":78756,\"start\":78746},{\"end\":78854,\"start\":78846},{\"end\":78867,\"start\":78854},{\"end\":78877,\"start\":78867},{\"end\":78889,\"start\":78877},{\"end\":78899,\"start\":78889},{\"end\":79073,\"start\":79064},{\"end\":79083,\"start\":79073},{\"end\":79326,\"start\":79305},{\"end\":79331,\"start\":79326},{\"end\":79490,\"start\":79479},{\"end\":79498,\"start\":79490},{\"end\":79512,\"start\":79498},{\"end\":79525,\"start\":79512},{\"end\":79535,\"start\":79525},{\"end\":79549,\"start\":79535},{\"end\":79567,\"start\":79549},{\"end\":79583,\"start\":79567},{\"end\":79596,\"start\":79583},{\"end\":79915,\"start\":79907},{\"end\":79928,\"start\":79915},{\"end\":79939,\"start\":79928},{\"end\":80427,\"start\":80421},{\"end\":80443,\"start\":80427},{\"end\":80451,\"start\":80443},{\"end\":80464,\"start\":80451},{\"end\":80475,\"start\":80464},{\"end\":80486,\"start\":80475},{\"end\":80494,\"start\":80486},{\"end\":80508,\"start\":80494},{\"end\":80913,\"start\":80899},{\"end\":80920,\"start\":80913},{\"end\":81477,\"start\":81467},{\"end\":81484,\"start\":81477},{\"end\":81493,\"start\":81484},{\"end\":81503,\"start\":81493},{\"end\":81803,\"start\":81793},{\"end\":81813,\"start\":81803},{\"end\":81823,\"start\":81813},{\"end\":82106,\"start\":82094},{\"end\":82116,\"start\":82106},{\"end\":82126,\"start\":82116},{\"end\":82136,\"start\":82126},{\"end\":82147,\"start\":82136},{\"end\":82161,\"start\":82147},{\"end\":82169,\"start\":82161},{\"end\":82182,\"start\":82169},{\"end\":82561,\"start\":82546},{\"end\":82572,\"start\":82561},{\"end\":82580,\"start\":82572},{\"end\":83052,\"start\":83042},{\"end\":83061,\"start\":83052},{\"end\":83069,\"start\":83061},{\"end\":83080,\"start\":83069},{\"end\":83093,\"start\":83080},{\"end\":83105,\"start\":83093},{\"end\":83116,\"start\":83105},{\"end\":83447,\"start\":83439},{\"end\":83453,\"start\":83447},{\"end\":83462,\"start\":83453},{\"end\":83777,\"start\":83757},{\"end\":83789,\"start\":83777},{\"end\":83798,\"start\":83789},{\"end\":83809,\"start\":83798},{\"end\":83819,\"start\":83809},{\"end\":83832,\"start\":83819},{\"end\":83838,\"start\":83832},{\"end\":84250,\"start\":84230},{\"end\":84259,\"start\":84250},{\"end\":84270,\"start\":84259},{\"end\":84279,\"start\":84270},{\"end\":84289,\"start\":84279},{\"end\":84300,\"start\":84289},{\"end\":84308,\"start\":84300},{\"end\":84319,\"start\":84308},{\"end\":84687,\"start\":84679},{\"end\":84700,\"start\":84687},{\"end\":84713,\"start\":84700},{\"end\":84724,\"start\":84713},{\"end\":84737,\"start\":84724},{\"end\":84746,\"start\":84737},{\"end\":85118,\"start\":85110},{\"end\":85128,\"start\":85118},{\"end\":85138,\"start\":85128},{\"end\":85148,\"start\":85138},{\"end\":85158,\"start\":85148},{\"end\":85174,\"start\":85158},{\"end\":85187,\"start\":85174},{\"end\":85195,\"start\":85187},{\"end\":85207,\"start\":85195},{\"end\":85215,\"start\":85207},{\"end\":85607,\"start\":85599},{\"end\":85613,\"start\":85607},{\"end\":85619,\"start\":85613},{\"end\":85626,\"start\":85619},{\"end\":85633,\"start\":85626},{\"end\":85944,\"start\":85933},{\"end\":85952,\"start\":85944},{\"end\":85962,\"start\":85952},{\"end\":85976,\"start\":85962},{\"end\":85984,\"start\":85976},{\"end\":85995,\"start\":85984},{\"end\":86005,\"start\":85995},{\"end\":86016,\"start\":86005},{\"end\":86029,\"start\":86016},{\"end\":86432,\"start\":86424},{\"end\":86438,\"start\":86432},{\"end\":86446,\"start\":86438},{\"end\":86857,\"start\":86849},{\"end\":86871,\"start\":86857},{\"end\":86882,\"start\":86871},{\"end\":86891,\"start\":86882},{\"end\":86899,\"start\":86891},{\"end\":86909,\"start\":86899},{\"end\":86922,\"start\":86909},{\"end\":86932,\"start\":86922},{\"end\":86945,\"start\":86932},{\"end\":86957,\"start\":86945},{\"end\":87300,\"start\":87292},{\"end\":87311,\"start\":87300},{\"end\":87318,\"start\":87311},{\"end\":87331,\"start\":87318},{\"end\":87339,\"start\":87331},{\"end\":87349,\"start\":87339}]", "bib_venue": "[{\"end\":72545,\"start\":72483},{\"end\":80088,\"start\":80022},{\"end\":69559,\"start\":69506},{\"end\":69978,\"start\":69942},{\"end\":70341,\"start\":70276},{\"end\":70720,\"start\":70648},{\"end\":71098,\"start\":71086},{\"end\":71384,\"start\":71319},{\"end\":72027,\"start\":72013},{\"end\":72481,\"start\":72404},{\"end\":72900,\"start\":72857},{\"end\":73280,\"start\":73256},{\"end\":73631,\"start\":73531},{\"end\":73943,\"start\":73892},{\"end\":74210,\"start\":74132},{\"end\":74575,\"start\":74571},{\"end\":74769,\"start\":74727},{\"end\":75261,\"start\":75223},{\"end\":75638,\"start\":75595},{\"end\":75993,\"start\":75937},{\"end\":76339,\"start\":76278},{\"end\":76680,\"start\":76583},{\"end\":76924,\"start\":76903},{\"end\":77377,\"start\":77341},{\"end\":77849,\"start\":77784},{\"end\":78359,\"start\":78327},{\"end\":78762,\"start\":78756},{\"end\":79097,\"start\":79083},{\"end\":79618,\"start\":79596},{\"end\":80020,\"start\":79939},{\"end\":80532,\"start\":80508},{\"end\":80969,\"start\":80947},{\"end\":81539,\"start\":81503},{\"end\":81855,\"start\":81823},{\"end\":82092,\"start\":82009},{\"end\":82666,\"start\":82580},{\"end\":83040,\"start\":82907},{\"end\":83501,\"start\":83462},{\"end\":83862,\"start\":83838},{\"end\":84337,\"start\":84319},{\"end\":84782,\"start\":84774},{\"end\":85108,\"start\":85029},{\"end\":85650,\"start\":85633},{\"end\":86039,\"start\":86029},{\"end\":86505,\"start\":86446},{\"end\":86987,\"start\":86957},{\"end\":87519,\"start\":87432}]"}}}, "year": 2023, "month": 12, "day": 17}