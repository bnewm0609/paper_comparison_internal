{"id": 4931762, "updated": "2023-09-30 23:32:33.21", "metadata": {"title": "Learning to Navigate in Cities Without a Map", "authors": "[{\"first\":\"Piotr\",\"last\":\"Mirowski\",\"middle\":[]},{\"first\":\"Matthew\",\"last\":\"Grimes\",\"middle\":[\"Koichi\"]},{\"first\":\"Mateusz\",\"last\":\"Malinowski\",\"middle\":[]},{\"first\":\"Karl\",\"last\":\"Hermann\",\"middle\":[\"Moritz\"]},{\"first\":\"Keith\",\"last\":\"Anderson\",\"middle\":[]},{\"first\":\"Denis\",\"last\":\"Teplyashin\",\"middle\":[]},{\"first\":\"Karen\",\"last\":\"Simonyan\",\"middle\":[]},{\"first\":\"Koray\",\"last\":\"Kavukcuoglu\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Zisserman\",\"middle\":[]},{\"first\":\"Raia\",\"last\":\"Hadsell\",\"middle\":[]}]", "venue": "Neural Information Processing Systems 2018", "journal": null, "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation (\"I am here\") and a representation of the goal (\"I am going there\"). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. We present an interactive navigation environment that uses Google StreetView for its photographic content and worldwide coverage, and demonstrate that our learning method allows agents to learn to navigate multiple cities and to traverse to target destinations that may be kilometres away. The project webpage http://streetlearn.cc contains a video summarising our research and showing the trained agent in diverse city environments and on the transfer task, the form to request the StreetLearn dataset and links to further resources. The StreetLearn environment code is available at https://github.com/deepmind/streetlearn", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1804.00168", "mag": "2963946945", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/MirowskiGMHATSK18", "doi": null}}, "content": {"source": {"pdf_hash": "776170d8f851110dcaae2f43bfe11f70e59339aa", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1804.00168v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "608acb660952e5acdbafa03bb67a23a3ab08fef1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/776170d8f851110dcaae2f43bfe11f70e59339aa.txt", "contents": "\nLearning to Navigate in Cities Without a Map\n\n\nPiotr Mirowski piotrmirowski@google.com \nLondonUnited Kingdom\n\nMatthew Koichi Grimes \nLondonUnited Kingdom\n\nMateusz Malinowski mateuszm@google.com \nLondonUnited Kingdom\n\nKarl Moritz Hermann \nLondonUnited Kingdom\n\nKeith Anderson keithanderson@google.com \nLondonUnited Kingdom\n\nDenis Teplyashin teplyashin@google.com \nLondonUnited Kingdom\n\nKaren Simonyan simonyan@google.com \nLondonUnited Kingdom\n\nKoray Kavukcuoglu korayk@google.com \nLondonUnited Kingdom\n\nAndrew Zisserman zisserman@google.com \nLondonUnited Kingdom\n\nRaia Hadsell \nLondonUnited Kingdom\n\nLearning to Navigate in Cities Without a Map\n\nNavigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation (\"I am here\") and a representation of the goal (\"I am going there\"). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. A key contribution of this paper is an interactive navigation environment that uses Google Street View for its photographic content and worldwide coverage. Our baselines demonstrate that deep reinforcement learning agents can learn to navigate in multiple cities and to traverse to target destinations that may be kilometres away. The project webpage http://streetlearn.cc contains a video summarizing our research and showing the trained agent in diverse city environments and on the transfer task, the form to request the StreetLearn dataset and links to further resources. The StreetLearn environment code is available at https://github.com/deepmind/streetlearn.\n\nIntroduction\n\nThe subject of navigation is attractive to various research disciplines and technology domains alike, being at once a subject of inquiry from the point of view of neuroscientists wishing to crack the code of grid and place cells [3,13], as well as a fundamental aspect of robotics research. The majority of algorithms involve building an explicit map during an exploration phase and then planning and acting via that representation. In this work, we are interested in pushing the limits of end-to-end deep reinforcement learning for navigation by proposing new methods and demonstrating their performance in large-scale, real-world environments. Just as humans can learn to navigate a city without relying on maps, GPS localisation, or other aids, it is our aim to show that a neural network agent can learn to traverse entire cities using only visual observations. In order to realise this aim, we designed an interactive environment that uses the images and underlying connectivity information from Google Street View, and propose a dual pathway agent architecture that can navigate within the environment (see Fig. 1a).\n\nLearning to navigate directly from visual inputs has been shown to be possible in some domains, by using deep reinforcement learning (RL) approaches that can learn from task rewards -for instance, navigating to a destination. Recent research has demonstrated that RL agents can learn to navigate house scenes [47,44], mazes (e.g. [34]), and 3D games (e.g. [31]). These successes notwithstanding, deep RL approaches are notoriously data inefficient and sensitive to perturbations of the environment, and are more well-known for their successes in games and simulated environments than in real-world applications. It is therefore not obvious that they can be used for large-scale visual navigation based on real-world images, and hence this is the subject of our investigation.\n\nThe primary contributions of this paper are (a) to present a new RL challenge that features real world visual navigation through city-scale environments, and (b) to propose a modular, goal-conditional deep RL algorithm that can solve this task, thus providing a strong baseline for future research. StreetLearn 1 is a new interactive environment for reinforcement learning that features real-world images as agent observations, with real-world grounded content that is built on top of the publicly available Google Street View. Within this environment we have developed a traversal task that requires that the agent navigates from goal to goal within London, Paris and New York City. To evaluate the feasibility of learning in such an environment, we propose an agent that learns a goaldependent policy with a dual pathway, modular architecture with similarities to the interchangeable task-specific modules approach from [14], and the target-driven visual navigation approach of [47]. The approach features a recurrent neural architecture that supports both locale-specific learning as well as general, transferable navigation behaviour. Balancing these two capabilities is achieved by separating a recurrent neural pathway from the general navigation policy of the agent. This pathway addresses two needs. First, it receives and interprets the current goal given by the environment, and second, it encapsulates and memorises the features and structure of a single city region. Thus, rather than using a map, or an external memory, we propose an architecture with two recurrent pathways that can effectively address a challenging navigation task in a single city as well as transfer to new cities or regions by training only a new locale-specific pathway.\n\n\nRelated Work\n\nReward-driven navigation in a real-world environment is related to research in various areas of deep learning, reinforcement learning, navigation and planning.\n\nLearning from real-world imagery. Localising from only an image may seem impossible, but humans can integrate visual cues to geolocate a given image with surprising accuracy, motivating machine learning approaches. For instance, convolutional neural networks (CNNs) achieve competitive scores on the geolocation task [43] and CNN+LSTM architectures improve on this [16,32]. robust location-based image retrieval [2]. Several methods [6,29], including DeepNav [7], use datasets collected using Street View or Open Street Maps and solve navigation-related tasks using supervision. RatSLAM demonstrates localisation and path planning over long distances using a biologically-inspired architecture [33]. The aforementioned methods rely on supervised training with ground truth labels: with the exception of the compass, we do not provide labels in our environment.\n\nDeep RL methods for navigation. Many RL-based approaches for navigation rely on simulators which have the benefit of features like procedurally generated variations but tend to be visually simple and unrealistic [4,27,40]. To support sparse reward signals in these environments, recent navigational agents use auxiliary tasks in training [34,26,31]. Other methods learn to predict future measurements or to follow simple text instructions [17,24,23,12]; in our case, the goal is designated using proximity to local landmarks. Deep RL has also been used for active localisation [11]. Similar to our proposed architecture, [47] show goal-conditional indoor navigation with a simulated robot and environment.\n\nTo bridge the gap between simulation and reality, researchers have developed more realistic, higherfidelity simulated environments [18,30,39,44]. However, in spite of their increasing photo-realism, the inherent problems of simulated environments lie in the limited diversity of the environments and the antiseptic quality of the observations. Photographic environments have been used to train agents on short navigation problem in indoor scenes with limited scale [10,1,8,36]. Our real-world dataset is diverse and visually realistic, comprising scenes with vegetation, pedestrians or vehicles, diverse weather conditions and covering large geographic areas. However, we note that there are obvious limitations of our environment: it does not contain dynamic elements, the action space is necessarily discrete as it must jump between panoramas, and the street topology cannot be arbitrarily altered.\n\nDeep RL for path planning and mapping. Several recent approaches have used memory or other explicit neural structures to support end-to-end learning of planning or mapping. These include Neural SLAM [46] that proposes an RL agent with an external memory to represent an occupancy map and a SLAM-inspired algorithm, Neural Map [37] which proposes a structured 2D memory for navigation, Memory Augmented Control Networks [28], which uses a hierarchical control strategy, and MERLIN, a general architecture that achieves superhuman results in novel navigation tasks [42]. Other work [9,11] explicitly provides a global map that is input to the agent. The architecture in [22] uses an explicit neural mapper and planner for navigation tasks as well as registered pairs of landmark images and poses. Similar to [21,46], they use extra memory that represents the ego-centric agent position. Another recent work proposes a graph network solution [38]. The focus of our paper is to demonstrate that simpler architectures can explore and memorise very large environments using target-driven visual navigation with a goal-conditional policy.\n\n\nEnvironment\n\nThis section presents an interactive environment, named StreetLearn, constructed using Google Street View, which provides a public API 2 . Street View provides a set of geolocated 360 0 panoramic images which form the nodes of an undirected graph. We selected a number of large regions in New York City, Paris and London that contain between 7,000 and 65,500 nodes (and between 7,200 and 128,600 edges, respectively), have a mean node spacing of 10m, and cover a range of up to 5km (see Fig. 1b). We do not simplify the underlying connectivity, thus there are congested areas with complex occluded intersections, tunnels and footpaths, and other ephemera. Although the graph is used to construct the environment, the agent only sees the raw RGB images (see Fig. 1a).\n\n\nAgent Interface and the Courier Task\n\nAn RL environment needs to specify the start space, observations, and action space of the agent as well as the task reward. The agent has two inputs: the image x t , which is a cropped, 60 0 square, RGB image that is scaled to 84\u02c684 pixels (i.e. not the entire panorama), and the goal description g t . The action space is composed of five discrete actions: \"slow\" rotate left or right (\u02d822.5 0 ), \"fast\" rotate left or right (\u02d867.5 0 ), or move forward-this action becomes a noop if there is not an edge in view from the current agent pose. If there are multiple edges in the view cone of the agent, then the most central one is chosen.\n\nThere are many options for how to specify the goal to the agent, from images to agent-relative directions, to text descriptions or addresses. We choose to represent the current goal in terms of its proximity to a set L of fixed landmarks: L \" tpLat k , Long k qu k , specified using the Lat/Long (latitude and longitude) coordinate system. To represent a goal at pLat g t , Long g t q we take a softmax over the distances to the k landmarks (see Fig. 2a), thus for distances td g t,k u k the goal vector contains g t,i \" expp\u00b4\u03b1d g t,i q{ \u0159 k expp\u00b4\u03b1d g t,k q for the ith landmark with \u03b1 \" 0.002 (which we chose through cross-validation). This forms a goal description with certain desirable qualities: it is a\ngoal g i A B C D E goal code g i A B C D E\n(a) Goal description using landmarks.\n\nconv conv conv  GoalNav is a convolutional encoder plus policy LSTM with goal description input. Middle: CityNav is a single-city navigation architecture with a separate goal LSTM and optional auxiliary heading (\u03b8). Right: MultiCityNav is a multi-city architecture with individual goal LSTM pathways for each city.\nx t g t a t-1, r t-1 x t g t a t-1, r t-1 env k env j env i k j i x t g t a t\nscalable representation that extends easily to new regions, it does not rely on any arbitrary scaling of map coordinates, and it has intuitive meaning-humans and animals also navigate with respect to fixed landmarks. Note that landmarks are fixed per map and we used the same list of landmarks across all experiments; g t is computed using the distance to all landmarks, but by feeding these distances through a non-linearity, the contribution of distant landmarks is reduced to zero. In the Supplementary material, we show that the locally-continuous landmark-based representation of the goal performs as well as the linear scalar representation pLat g t , Long g t q.\n\nSince the landmark-based representation performs well while being independent of the coordinate system and thus more scalable, we use this representation as canonical. Note that the goal description is not relative to the agent's position and only changes when a new goal is sampled. Locations of the 644 manually defined landmarks in New York, London and Paris are given in the Supplementary material, where we also show that the density of landmarks does not impact the agent performance.\n\nIn the courier task, which we define as the problem of navigating to a series of random locations in a city, the agent starts each episode from a randomly sampled position and orientation. If the agent gets within 100m of the goal (approximately one city block), the next goal is randomly chosen and input to the agent. Each episode ends after 1000 agent steps. The reward that the agent gets upon reaching a goal is proportional to the shortest path between the goal and the agent's position when the goal is first assigned; much like a delivery service, the agent receives a higher reward for longer journeys. Note that we do not reward agents for taking detours, but rather that the reward in a given level is a function of the optimal distance from start to goal location. As the goals get more distant during the training curriculum, per-episode reward statistics should ideally reach and stay at a plateau performance level if the agent can equally reach closer and further goals.\n\n\nMethods\n\nWe formalise the learning problem as a Markov Decision Process, with state space S, action space A, environment E, and a set of possible goals G. The reward function depends on the current goal and state:\nR : S \u015a G \u015a A \u00d1 R.\nThe usual reinforcement learning objective is to find the policy that maximises the expected return defined as the sum of discounted rewards starting from state s 0 with discount \u03b3. In this navigation task, the expected return from a state s t also depends on the series of sampled goals tg k u k . The policy is a distribution over actions given the current state s t and the goal g t : \u03c0pa|s, gq \" P rpa t \" a|s t \" s, g t \" gq. We define the value function to be the expected return for the agent that is sampling actions from policy \u03c0 from state s t with goal g t : V \u03c0 ps, gq \" ErR t s \" Er \u0159 8 k\"0 \u03b3 k r t`k |s t \" s, g t \" gs. We hypothesise the courier task should benefit from two types of learning: general, and locale-specific. A navigating agent not only needs an internal representation that is general, to support cognitive processes such as scene understanding, but also needs to organise and remember the features and structures that are unique to a place. Therefore, to support both types of learning, we focus on neural architectures with multiple pathways.\n\n\nArchitectures\n\nThe policy and the value function are both parameterised by a neural network which shares all layers except the final linear outputs. The agent operates on raw pixel images x t , which are passed through a convolutional network as in [35]. A Long Short-Term Memory (LSTM) [25] receives the output of the convolutional encoder as well as the past reward r t\u00b41 and previous action a t\u00b41 . The three different architectures are described below. Additional architectural details are given in the Supplementary Material.\n\nThe baseline GoalNav architecture (Fig. 2ba) has a convolutional encoder and policy LSTM. The key difference from the canonical A3C agent [35] is that the goal description g t is input to the policy LSTM (along with the previous action and reward).\n\nThe CityNav architecture (Fig. 2bb) combines the previous architecture with an additional LSTM, called the goal LSTM, which receives visual features as well as the goal description. The CityNav agent also adds an auxiliary heading (\u03b8) prediction task on the outputs of the goal LSTM.\n\nThe MultiCityNav architecture (Fig. 2bc) extends the CityNav agent to learn in different cities. The remit of the goal LSTM is to encode and encapsulate locale-specific features and topology such that multiple pathways may be added, one per city or region. Moreover, after training on a number of cities, we demonstrate that the convolutional encoder and the policy LSTM become general enough that only a new goal LSTM needs to be trained for new cities, a benefit of the modular approach [14]. Figure 2b illustrates that the goal descriptor g t is not seen by the policy LSTM but only by the localespecific LSTM in the CityNav and MultiCityNav architectures (the baseline GoalNav agent has only one LSTM, so we directly input g t ). This separation forces the locale-specific LSTM to interpret the absolute goal position coordinates, with the hope that it then sends relative goal information (directions) to the policy LSTM. This hypothesis is tested in section 2.3 of the supplementary material.\n\nAs shown in [26,34,17,31], auxiliary tasks can speed up learning by providing extra gradients as well as relevant information. We employ a very natural auxiliary task: the prediction of the agent's heading \u03b8 t , defined as an angle between the north direction and the agent's pose, using a multinomial classification loss on binned angles. The optional heading prediction is an intuitive way to provide additional gradients for training the convnet. The agent can learn to navigate without it, but we believe that heading prediction helps learning the geometry of the environment; the Supplementary material provides a detailed architecture ablation analysis and agent implementation details.\n\nTo train the agents, we use IMPALA [19], an actor-critic implementation that decouples acting and learning. In our experiments, IMPALA results in similar performance to A3C [35]. We use 256 actors for CityNav and 512 actors for MultiCityNav, with batch sizes of 256 or 512 respectively, and sequences are unrolled to length 50.\n\n\nCurriculum Learning\n\nCurriculum learning gradually increases the complexity of the learning task by presenting progressively more difficult examples to the learning algorithm [5,20,45]. We use a curriculum to help the agent learn to find increasingly distant destinations. Similar to RL problems such as Montezuma's Revenge, the courier task suffers from very sparse rewards; unlike that game, we are able to define a natural curriculum scheme. We start by sampling each new goal to be within 500m of the agent's position (phase 1). In phase 2, we progressively grow the maximum range of allowed destinations to cover the full graph (3.5km in the smaller New York areas, or 5km for central London or Paris).\n\n\nResults\n\nIn this section, we demonstrate and analyse the performance of the proposed architectures on the courier task. We first show the performance of our agents in large city environments, next their generalisation capabilities on a held-out set of goals. Finally, we investigate whether the proposed approach allows transfer of an agent trained on a set of regions to a new and previously unseen region. \n\n\nCourier Navigation in Large, Diverse City Environments\n\nWe first show that the CityNav agent, trained with curriculum learning, succeeds in learning the courier task in New York, London and Paris. We replicated experiments with 5 random seeds and plot the mean and standard deviation of the reward statistic throughout the experimental results. Throughout the paper, and for ease of comparison with experiments that include reward shaping, we report only the rewards at the goal destination (goal rewards). Figure 3 compares different agents and shows that the CityNav architecture with the dual LSTM pathways and the heading prediction task attains a higher performance and is more stable than the simpler GoalNav agent. We also trained a CityNav agent without the skip connection from the vision layers to the policy LSTM. While this hurts the performance in single-city training, we consider it because of the multi-city transfer scenario (see Section 5.4) where funeling all visual information through the locale-specific LSTM seems to regularise the interface between the goal LSTM and the policy LSTM. We also consider two baselines which give lower (Heuristic) and upper (Oracle) bounds on the performance. Heuristic is a random walk on the street graph, where the agent turns in a random direction if it cannot move forward; if at an intersection it will turn with a probability p \" 0.95. Oracle uses the full graph to compute the optimal path using breath-first search.\n\nWe visualise trajectories from the trained agent over two 1000 step episodes (Fig. 4b (top row)). In London, we see that the agent crosses a bridge to get to the first goal, then travels to goal 2, and the To understand whether the agent has learned a policy over the full extent of the environment, we plot the number of steps required by the agent to get to the goal. As the number grows linearly with the straight-line distance to that goal, this result suggests that the agent has successfully learnt the navigation policy on both cities (Fig. 4a).\n\n\nImpact of Reward Shaping and Curriculum Learning\n\nTo better understand the environment, we present further experiments on reward, curriculum. Additional analysis, including architecture ablations, the robustness of the agent to the choice of goal representations, and position and goal decoding, are presented in the Supplementary Material.\n\nOur navigation task assigns a goal to the agent; once the agent successfully navigates to the goal, a new goal is given to the agent. The long distance separating the agent from the goal makes this a difficult RL problem with sparse rewards. To simplify this challenging task, we investigate giving early rewards (reward shaping) to the agent before it reaches the goal (we define goals with a 100m radius), or to add random rewards (coins) to encourage exploration [4,34]. Figure 3c suggests that coins by themselves are ineffective as our task does not benefit from wide explorations. At the same time, large radii of reward shaping help as they greatly simplify the problem. We prefer curriculum learning to reward shaping on large areas because the former approach keeps agent training consistent with its experience at test time and also reduces the risk of learning degenerate strategies such as ascending the gradient of increasing rewards to reach the goal, rather than learn to read the goal specification g t .\n\nAs a trade-off between task realism and feasibility, and guided by the results in Fig. 3c, we decide to keep a small amount of reward shaping (200m away from the goal) combined with curriculum learning. The specific reward function we use is: r t \" maxp0, minp1, pd ER\u00b4d g t q{100qq\u02c6r g , where d g t is the distance from the current position of the agent to the goal, d ER \" 200 and r g is the reward that the agent will receive if it reaches the goal. Early rewards are given only once per panorama / node, and only if the distance d g t to the goal is decreasing (in order to avoid the agent developing a behavior of harvesting early rewards around the goal rather than going directly towards the goal).\n\nWe choose a curriculum that starts by sampling the goal within a radius of 500m from the agent's location, and progressively grows that disc until it reaches the maximum distance an agent could travel within the environment (e.g., 3.5km, and 5km in the NYU and London environments respectively) by the end of the training. Note that this does not preclude the agent from going astray in the opposite direction several kilometres away from the goal, and that the goal may occasionally be sampled close to the agent. Hence, our curriculum scheme naturally combines easy with difficult cases [45], with the latter becoming more common over the period of time.   Table 1: CityNav agent generalization performance (reward and fail metrics) on a set of held-out goal locations. We also compute the half-trip time (T 1 2 ), to reach halfway to the goal.\n\n\nGeneralization on Held-out Goals\n\nNavigation agents should, ideally, be able to generalise to unseen environments [15]. While the nature of our courier task precludes zero-shot navigation in a new city without retraining, we test the CityNav agent's ability to exploit local linearities of the goal representation to handle unseen goal locations. We mask 25% of the possible goals and train on the remaining ones (Fig. 5). At test time we evaluate the agent only on its ability to reach goals in the held-out areas. Note that the agent is still able to traverse through these areas, it just never samples a goal there. More precisely, the held-out areas are squares sized 0.01 0 , 0.005 0 or 0.0025 0 of latitude and longitude (roughly 1km\u02c61km, 0.5km\u02c60.5km and 0.25km\u02c60.25km). We call these grids respectively coarse (with few and large held-out areas), medium and fine (with many small held-out areas).\n\nIn the experiments, we train the CityNav agent for 1B steps, and next freeze the weights of the agent and evaluate its performance on held-out areas for 100M steps. Table 1 shows decreasing performance of the agents as the held-out area size increases. We believe that the performance drops on the large held-out areas (medium and coarse grid size) because the model cannot process new or unseen local landmark-based goal specifications, which is due to our landmark-based goal representation: as Figure 5 shows, some coarse grid held-out areas cover multiple landmarks. To gain further understanding, in addition to the Test Reward metric, we also use missed goals (Fail) and half-trip time (T 1 2 ) metrics. The missed goals metric measures the percentage of times goals were not reached. The half-trip time measures the number of agent steps necessary to cover half the distance separating the agent from the goal. While the agent misses more goal destinations on larger held-out grids, it still manages to travel half the distance to the goal within a similar time, which suggests that the agent has an approximate held-out goal representation that enables it to head towards it until it gets close to the goal and the representation is no longer useful for the final approach.\n\n\nTransfer in Multi-city Experiments\n\nA critical test for our proposed method is to demonstrate that it can provide a mechanism for transfer to new cities. By definition, the courier task requires a degree of memorization of the map, and what we focused on was not zero-shot transfer, but rather the capability of models to generalize quickly, learning to separate general ability from local knowledge when migrating to a new map. Our motivation for transfer learning experiments comes from the goal of continual learning, which is about learning new skills without forgetting older skills. As with humans, when our agent visits a new city we would expect it to have to learn a new set of landmarks, but not have to re-learn its visual representation, its behaviours, etc. Specifically, we expect the agent to take advantage of existing visual features (convnet) and movement primitives (policy LSTM). Therefore, using the MultiCityNav agent, we train on a number of cities (actually regions in New York City), freeze both the policy LSTM and the convolutional encoder, and then train a new locale-specific pathway (the goal LSTM) on a new city. The gradient that is computed by optimising the RL loss is passed through the policy LSTM without affecting it and then applied only to the new pathway.\n\nWe compare the performance using three different training regimes, illustrated in Fig. 6a: Training on only the target city (single training); training on multiple cities, including the target city, together (joint training); and joint training on all but the target city, followed by training on the target city with the rest of the architecture frozen (pre-train and transfer). In these experiments, we use the whole Manhattan environment as shown in Figure 1b, and consisting of the following regions \"Wall Street\", \"NYU\", \"Midtown\", \"Central Park\" and \"Harlem\". The target city is always the Wall Street environment, and we evaluate the effects of pre-training on 2, 3 or 4 of the other environments. We also compare performance if the skip connection between the convolutional encoder and the policy LSTM is removed.\n\nWe can see from the results in Figure 6b that not only is transfer possible, but that its effectiveness increases with the number of the regions the network is trained on. Remarkably, the agent that is pre-trained on 4 regions and then transferred to Wall Street achieves comparable performance to an agent trained jointly on all the regions, and only slightly worse than single-city training on Wall Street alone 3 . This result supports our intuition that training on a larger set of environments results in successful transfer. We also note that in the single-city scenario it is better to train an agent with a skip-connection, but this trend is reversed in the multi-city transfer scenario. We hypothesise that isolating the locale-specific LSTM as a bottleneck is more challenging but reduces overfitting of the convolutional features and enforces a more general interface to the policy LSTM. While the transfer  learning performance of the agent is lower than the stronger agent trained jointly on all the areas, the agent significantly outperforms the baselines and demonstrates goal-dependent navigation.\n\n\nConclusion\n\nNavigation is an important cognitive task that enables humans and animals to traverse a complex world without maps. We have presented a city-scale real-world environment for training RL navigation agents, introduced and analysed a new courier task, demonstrated that deep RL algorithms can be applied to problems involving large-scale real-world data, and presented a multi-city neural network agent architecture that demonstrates transfer to new environments. A multi-city version of the Street View based RL environment, with carefully processed images provided by Google Street View (i.e., blurred faces and license plates, with a mechanism for enforcing image takedown requests) has been released for Manhattan and Pittsburgh and is accessible from http:// streetlearn.cc and https://github.com/deepmind/streetlearn. The project webpage at http://streetlearn.cc also contains resources on how to build and train an agent. Future work will involve learning landmarks from images and scaling up the navigation and path-planning thanks to hierarchical RL approaches.\n\n\nA Video of the Agent Trajectories and Observations\n\nThe video available at http://streetlearn.cc and https://youtu.be/2yjWDNXYh5s shows the performance of trained CityNav agents in the Paris Rive Gauche and Central London environments, as well as of the MultiCityNav agents trained jointly on 4 environments (Greenwich Village, Midtown, Central Park and Harlem) and then transferred to a fifth environment (Lower Manhattan). The video shows the high-resolution StreetView images (actual inputs to the network are 84\u02c684 RGB observations), overlaid with the map of the environment indicating its position and the location of the goal.  In this analysis, we focus on agents trained with a two-day learning curriculum and early rewards at 200m, in the NYU environment. Here, we study how the learning benefits from various auxiliary tasks as well as we provide additional ablation studies where we investigate various design choices. We quantify the performance in terms of average reward per episode obtained at goal acquisition. Each experiment was repeated 5 times with different seeds. We report average final reward and plot the mean and standard deviation of the reward statistic. As Fig. 7 shows, the auxiliary task of heading (HD) prediction helps in achieving better performance in the navigation task for the GoalNav architecture, and, in conjunction with a skip connection from the convnet to the policy LSTM, for the 2-LSTM architecture. The CityNav agent significantly outperforms our main baseline GoalNav (LSTM in Fig. 7), which is a goal-conditioned variant of the standard RL baseline [35]. CityNav perfoms on par with GoalNav with heading prediction, but the latter cannot adapt to new cities without re-training or adding city-specific components, whereas the MultiCityNav agent with locale-specific LSTM pathways can, as demonstrated in the paper's section on transfer learning. Our weakest baseline (CityNav no vision) performs poorly as the agent cannot exploit visual cues while performing navigation tasks. In our investigation, we do not consider other auxiliary tasks introduced in prior works [26,34] as they are either unsuitable for our task, do not perform well, or require extra information that we consider too strong. Specifically, we did not implement the reward prediction auxiliary task on the convnet from [26], as the goal is not visible from pixels, and the motion model of the agent with widely changing visual input is not appropriate for the pixel control tasks in that same paper. From [34], we kept the 2-LSTM architecture and substituted depth prediction (which we cannot perform on this dataset) by heading and neighbor traversability prediction. We did not implement loop-closure prediction as it performed poorly in the original paper and uses privileged map information. \n\n\nB.2 Goal Representation\n\nAs described in Section 3.1 of the paper, our task uses a goal description which is a vector of normalised distances to a set of fixed landmarks. Reducing the density of the landmarks to half, a quarter or an eighth (50%, 25%, 12.5%) does not significantly reduce the performance (Fig. 8).\n\nWe also investigate some alternative representations: a) latitude and longitude scalar coordinates normalised to be between 0 and 1 (Lat/long scalar in Figure 8), and b) a binned representation Lat/long binned using 35 bins for X and Y each, with each bin covering 100m. The Lat/long scalar goal representations performs best. An alternative goal representation is expressed in terms of the distances td g t,k u k from the goal position px g t , y g t q to a set of arbitrary landmarks tx k , y k u k . We defined g t,i \" expp\u00b4\u03b1d g t,k q \u0159 k expp\u00b4\u03b1d g t,k q and tuned \u03b1 \" 0.002 using grid search. We manually defined 644 landmarks covering New York, Paris and London, which we use throughout the experiments and which are illustrated on Fig.2a. We observe that reducing the density of the landmarks to half, a quarter or an eighth has a slightly detrimental effect on performance because some areas are sparsely covered by landmarks. Because the landmark representation is independent of the coordinate system, we choose it and use it in all the other experimnets in this paper.\n\nFinally, we also train a Goal-less CityNav agent by removing inputs g t . The poor performance of this agent (Fig. 8) confirms that the performance of our method cannot be attributed to clever street graph exploration alone. Goal-less CityNav learns to run in circles of increasing radii-a reasonable, greedy behaviour that is a good baseline to the other agents.\n\nSince the landmark-based representation performs well while being independent of the coordinate system and thus more scalable, we use this representation as canonical.\n\n\nB.3 Allocentric and Egocentric Goal Representation\n\nWe do an analysis of the activations of the 256 hidden units of the region-specific LSTM, by training decoders (2-layer multi-layer perceptrons, MLP, with 128 units on the hidden layer and rectified nonlinearity transfer functions) for the allocentric position of the agent and of the goal as well as for the egocentric direction towards the goal. Allocentric decoders are multinomial classifiers over the joint Lat/Long position, and have 50\u02c650 bins (London) or 35\u02c635 bins (NYU), each bin covering an area of size 100m\u02c6100m. The egocentric decoder has 16 orientation bins. Fig.9 illustrates the noisy decoding of the agent's position along 3 trajectories and the decoding of the goal (here, St Paul's), overlayed with the ground truth trajectories and goal location. The average error of the egocentric goal direction decoding is about 60 0 (as compared to 90 0 for a random predictor), suggesting some decoding but not a cartesian manifold representation in the hidden units of the LSTM. \n\n\nB.4 Reward Shaping: Goal Rewards vs. Rewards\n\nThe agent is considered to have reached the goal if it is within 100m of the goal, and the reward shaping consists in giving the agent early rewards as it is within 200m of the goal. Early rewards are shaped as following:\nr t \" max\u02c60, min\u02c61, 200\u00b4d g t 100\u02d9\u02d9\u02c6r g\nwhere d g t is the distance from the current position of the agent to the goal and r g is the reward that the agent will receive if it reaches the goal. Early rewards are given only once per panorama / node, and only if the distance d g t to the goal is decreasing (in order to avoid the agent developing a behavior of harvesting early rewards around the goal rather than going directly towards the goal). However, depending on the path taken by the agent towards the goal, it could earn slightly more rewards if it takes a longer path to the goal rather than a shorter path. Throughout the paper, and for ease of comparison with experiments that include reward shaping, we report only the rewards at the goal destination (goal rewards).\n\n\nC Implementation Details\n\n\nC.1 Neural Network Architecture\n\nFor all the experiments in the paper we use the standard vision model for Deep RL [35] with 2 convolutional layers followed by a fully connected layer. The baseline GoalNav architecture has a single recurrent layer (LSTM), from which we predict the policy and value function, similarly to [35].\n\nThe convolutional layers are as follows. The first convolutional layer has a kernel of size 8x8 and a stride of 4x4, and 16 feature maps. The second layer has a kernel of size 4x4 and a stride of 2x2, and 32 feature maps. The fully connected layer has 256 units, and outputs 256-dimensional visual features f t . Rectified nonlinearities (ReLU) separate the layers.\n\nThe convnet is connected to the policy LSTM (in case of two-LSTM architectures, we call it a Skip connection). The policy LSTM has additional inputs: past reward r t\u00b41 and previous action a t\u00b41 expressed as a one-hot vector of dimension 5 (one for each action: forward, turn left or right by 22.5 0 , turn left or right by 67.5 0 ).\n\nThe goal information g t is provided as an extra input, either to the policy LSTM (GoalNav agent) or to each goal LSTM in the CityNav and MultiCityNav agents. In case of landmark-based goals, g t is a vector of 644 elements (see Section D for the complete list of landmark locations in the New York and London environments). In the case of Lat/Long scalars, g t is a 2-dimensional vector of Lat and Long coordinates normalized to be between 0 and 1 in the environment of interest. In the case of binned Lat/Long coordinates, we bin the normalized scalar coordinates using 35 bins for Lat and 35 bins for Long in the NYU environment, each bin representing 100m, and the vector g t contains 70 elements.\n\nThe goal LSTM also takes 256-dimensional inputs from to the convnet. The goal LSTM contains 256 hidden units and is followed by a tanh nonlinearity, a dropout layer with probability p \" 0.5, then a 64-dimensional linear layer and finally a tanh layer. It is this (CityNav) or these (MultiCityNav) 64-dimensional outputs that are connected to the policy LSTM. We chose to use this bottleneck, consisting of a dropout, linear layer from 256 to 64, followed by a nonlinearity, in order to force the representations in the goal LSTM to be more robust to noise and to send only a small amount of information (possibly related to the egocentric position of the agent w.r.t. the goal) to the policy LSTM. Please note that the CityNav agent can still be trained to solve the navigation task without that layer.\n\nSimilarly to [35], the policy LSTM contains 256 hidden units, followed by two parallel layers: one linear layer going from 256 to 1 and outputing the value function, and one linear layer going from 256 to 5 (the number of actions), and a softmax nonlinearity, outputting the policy.\n\nThe heading \u03b8 t prediction auxiliary task is done using an MPL with a hidden layer of 128 units, connected to the hidden units of each goal LSTM in the CityNav and MultiCityNav agents, and outputs a softmax of 16-dimensional vectors, corresponding to 16 binned directions towards North. The auxiliary task is optimized using a multinomial loss.\n\n\nC.2 Learning Hyperparameters\n\nThe costs for all auxiliary heading prediction tasks, of the value prediction, of the entropy regularization and of the policy loss are added before being sent to the RMSProp gradient learning algorithm [41] (momentum 0, discounting factor 0.99, \" 0.1, initial learning rate 0.001). The weight of heading prediction is 1, the entropy cost is 0.004 and the value baseline weight is 0.5.\n\nIn all our experiments, we train our agent with IMPALA [19], an actor-critic implementation of deep reinforcement learning that decouples acting and learning. In our experiments, IMPALA results in similar performance to A3C [35] on a single city task, but as it has been demonstrated to handle better multi-task learning than A3C, we prefer it to A3C for our multi-city and transfer learning experiments. We use 256 actors for CityNav and 512 actors for MultiCityNav, with batch sizes of 256 or 512 respectively, and sequences are unrolled to length 50. We used a learning rate of 0.001, linearly annealed to 0 after 2B steps (NYU), 4B steps (London) or 8B steps (multi-city and transfer learning experiments). The discounting coefficient in the Bellman equation is 0.99. Rewards are clipped at 1 for the purpose of gradient calculations.\n\n\nC.3 Curriculum Learning\n\nBecause of the distributed nature of the learning algorithm, it was easier to implement the duration of phase 1 and phase 2 of curriculum learning using the Wall clock of the actors and learners rather than by sharing the total number of steps with the actors, which explains why phase durations are expressed in terms of days, rather than in a given number of steps. With our software implementation, hardware and batch size as well as number of actors, the distributed learning algorithm runs at about 6000 environment steps/sec, and a day of training corresponds to about 500M steps. In terms of gradient steps, given than we use unrolls of length 50 steps and batch sizes of 256 or 512, each gradient step corresponds to either 50\u02c6256 \" 12800 or 50\u02c6512 \" 25600 environment steps, and is taken every 2s or 4s respectively for a speed of 6000 environment steps/sec.\n\n\nD Environment\n\nFor the experiments on data from Manhattan, New York, we relied on sub-areas of a larger StreetView graph that contains 256961 nodes and 266040 edges. We defined 5 areas by selecting a starting point at a given coordinate and collecting panoramas in a panorama adjacency graph using breadth-first-search, until a given depth of the search tree. We defined areas as following: \n\nFigure 1 :\n1(a) Our environment is built of real-world places from Street View (we illustrate Times Square and Central Park in New York City and St. Paul's Cathedral in London). The green cone represents the agent's location and orientation. (b) We use large regions of London and Paris and in New York we focus on 5 different regions to show transfer.\n\n\nGoalNav agent b. CityNav agent c. MultiCityNav agent (b) Comparison of architectures.\n\nFigure 2 :\n2(a) In the illustration of the goal description, we show a set of 5 nearby landmarks and 4 distant ones; the code g i is a vector with a softmax-normalised distance to each landmark. (b) Left:\n\nFigure 3 :\n3Average per-episode rewards (y axis) are plotted vs. learning steps (x axis) for the courier task. We compare the GoalNav agent, the CityNav agent, and the CityNav agent without skip connection on the NYU environment (a), and the CityNav agent in London (b). We also give Oracle performance and a Heuristic agent. A curriculum is used in London-we indicate the end of phase 1 (up to 500m) and the end of phase 2 (5000m). (c) Results of the CityNav agent on NYU, comparing radii of early rewards (ER) vs. ER with random coins vs. curriculum with ER 200m and no coins.\n\nFigure 4 :\n4(a) Number of steps required for the CityNav agent to reach a goal from 100 start locations vs. the straight-line distance to the goal in metres. (b) CityNav performance in London (left panes) and NYU (right panes). Top: examples of the agent's trajectory during one 1000-step episode, showing successful consecutive goal acquisitions. The arrows show the direction of travel of the agent. Bottom: We visualise the agent's value function over 100 trajectories with random starting points and the same goal. Thicker and warmer colour lines correspond to higher value functions. episode ends before it can reach the third goal. Figure 4b (bottom row) shows the value function of the agent as it repeatedly navigates to a chosen destination (respectively, St Paul's Cathedral in London and Washington Square in New York).\n\nFigure 5 :\n5Illustration of medium-sized held-out grid with gray corresponding to training destinations, black corresponding to held-out test destinations. Landmark locations are marked in red.\n\nFigure 6 :\n6Left: Illustration of training regimes: (a) training on a single city (equivalent to CityNav); (b) joint training over multiple cities with a dedicated per-city pathway and shared convolutional net and policy LSTM; (c) joint pre-training on a number of cities followed by training on a target city with convolutional net and policy LSTM frozen (only the target city pathway is optimised). Right: Joint multi-city training and transfer learning performance of variants of the MultiCityNav agent, evaluated only on the target city (Wall Street).\n\nFigure 7 :\n7Learning curves of the CityNav agent (2LSTM+Skip+HD) on NYU, comparing different ablations, all they way down to GoalNav (LSTM). 2LSTM architectures have a global pathway LSTM and a policy LSTM with optional Skip connection between the convnet and the policy LSTM. HD is the heading prediction auxiliary task.\n\nFigure 8 :\n8Learning curves for CityNav agents with different goal representations: landmark-based, as well as latitude and longitude classification-based and regression-based.\n\nFig. 8\n8compares the performance of the CityNav agent for different goal representations g t on the NYU environment. The most intuitive one consists in normalized latitude and longitude coordinates, or in binned representation of latitude and longitude (we used 35 bins for X and 35 bins for Y, where each bin covers 100m, or 80 bins for each coordinate).\n\nFigure 9 :\n9Decoding of the agent position (blue dots) and goal position (cyan stars) over 3 trajectories (in red) with a goal at St Paul's Cathedral, in London (in black).\n\n\u2022\nWall Street / Lower Manhattan: 6917 nodes and 7191 edges, 200-deep search tree starting at (40.705510, -74.013589). \u2022 NYU / Greenwich Village: 17227 nodes and 17987 edges, 200-deep search tree starting at (40.731342, -73.996903). \u2022 Midtown: 16185 nodes and 16723 edges, 200-deep search tree starting at (40.756889, -73.986147).\nhttp://streetlearn.cc (dataset) and https://github.com/deepmind/streetlearn (code).\nhttps://developers.google.com/maps/documentation/streetview/\nWe observed that we could train a model jointly on 4 cities in fewer steps than when training 4 single-city models.\nAcknowledgementsThe authors wish to acknowledge Andras Banki-Horvath for open-sourcing the StreetLearn environment, Lasse Espeholt and Hubert Soyer for technical help with the IMPALA algorithm, Razvan Pascanu, Ross Goroshin, Pushmeet Kohli and Nando de Freitas for their feedback, Chloe Hillier, Razia Ahamed and Vishal Maini for help with the project, and the Google Street View team (Tilman Reinhardt, Wenfeng Li, Ben Mears, Karen Guo, Oliver Metzger, Jayanth Nayak) as well as Richard Ives and Ashwin Kakarla for their support in accessing the data.\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, arXiv:1711.07280and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. arXiv preprintPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: In- terpreting visually-grounded navigation instructions in real environments. arXiv preprint arXiv:1711.07280, 2017.\n\nNetvlad: Cnn architecture for weakly supervised place recognition. Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, Josef Sivic, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRelja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5297-5307, 2016.\n\nVector-based navigation using grid-like representations in artificial agents. Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, J Martin, Thomas Chadwick, Joseph Degris, Modayil, Nature. 1Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based navigation using grid-like representations in artificial agents. Nature, page 1, 2018.\n\nCharles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00fcttler, Andrew Lefrancq, Simon Green, arXiv:1612.03801V\u00edctor Vald\u00e9s, Amir Sadik, et al. Deepmind lab. arXiv preprintCharles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00fcttler, Andrew Lefrancq, Simon Green, V\u00edctor Vald\u00e9s, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.\n\nCurriculum learning. Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningACMYoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41-48. ACM, 2009.\n\nHeading direction estimation using deep learning with automatic large-scale data acquisition. F Rodrigo, Lucas Tabelini Berriel, Torres, B Vinicius, R\u00e2nik Cardoso, Claudine Guidolini, Alberto F De Badue, Thiago Souza, Oliveira-Santos, Rodrigo F Berriel, Lucas Tabelini Torres, Vinicius B Cardoso, R\u00e2nik Guidolini, Claudine Badue, Alberto F De Souza, and Thiago Oliveira-Santos. Heading direction estimation using deep learning with automatic large-scale data acquisition. 2018.\n\nDeepnav: Learning to navigate large cities. Samarth Brahmbhatt, James Hays, arXiv:1701.09135arXiv preprintSamarth Brahmbhatt and James Hays. Deepnav: Learning to navigate large cities. arXiv preprint arXiv:1701.09135, 2017.\n\nOne-shot reinforcement learning for robot navigation with interactive replay. Jake Bruce, Niko S\u00fcnderhauf, Piotr Mirowski, Raia Hadsell, Michael Milford, arXiv:1711.10137arXiv preprintJake Bruce, Niko S\u00fcnderhauf, Piotr Mirowski, Raia Hadsell, and Michael Milford. One-shot rein- forcement learning for robot navigation with interactive replay. arXiv preprint arXiv:1711.10137, 2017.\n\nTeaching a machine to read maps with deep reinforcement learning. Gino Brunner, Oliver Richter, Yuyi Wang, Roger Wattenhofer, arXiv:1711.07479arXiv preprintGino Brunner, Oliver Richter, Yuyi Wang, and Roger Wattenhofer. Teaching a machine to read maps with deep reinforcement learning. arXiv preprint arXiv:1711.07479, 2017.\n\nAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, arXiv:1709.06158Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprintAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.\n\nDevendra Singh Chaplot, Emilio Parisotto, Ruslan Salakhutdinov, Active neural localization. International Conference on Learning Representations. Devendra Singh Chaplot, Emilio Parisotto, and Ruslan Salakhutdinov. Active neural localization. International Conference on Learning Representations, 2018.\n\nGated-attention architectures for task-oriented language grounding. Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov, arXiv:1706.07230arXiv preprintDevendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, and Ruslan Salakhutdinov. Gated-attention architectures for task-oriented language grounding. arXiv preprint arXiv:1706.07230, 2017.\n\nEmergence of grid-like representations by training recurrent neural networks to perform spatial localization. J Christopher, Xue-Xin Cueva, Wei, arXiv:1803.07770arXiv preprintChristopher J Cueva and Xue-Xin Wei. Emergence of grid-like representations by training recurrent neural networks to perform spatial localization. arXiv preprint arXiv:1803.07770, 2018.\n\nLearning modular neural network policies for multi-task and multi-robot transfer. Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, Sergey Levine, 2017 IEEE International Conference on. IEEERobotics and Automation (ICRAColine Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning modular neural network policies for multi-task and multi-robot transfer. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 2169-2176. IEEE, 2017.\n\nA critical investigation of deep reinforcement learning for navigation. Shurjo Vikas Dhiman, Brent Banerjee, Griffin, M Jeffrey, Jason J Siskind, Corso, arXiv:1802.02274arXiv preprintVikas Dhiman, Shurjo Banerjee, Brent Griffin, Jeffrey M Siskind, and Jason J Corso. A critical investigation of deep reinforcement learning for navigation. arXiv preprint arXiv:1802.02274, 2018.\n\nLong-term recurrent convolutional networks for visual recognition and description. Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2625-2634, 2015.\n\nLearning to act by predicting the future. Alexey Dosovitskiy, Vladlen Koltun, arXiv:1611.01779arXiv preprintAlexey Dosovitskiy and Vladlen Koltun. Learning to act by predicting the future. arXiv preprint arXiv:1611.01779, 2016.\n\nAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio L\u00f3pez, Vladlen Koltun, Carla, arXiv:1711.03938An open urban driving simulator. arXiv preprintAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio L\u00f3pez, and Vladlen Koltun. Carla: An open urban driving simulator. arXiv preprint arXiv:1711.03938, 2017.\n\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu, arXiv:1802.01561Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprintLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.\n\nAutomated curriculum learning for neural networks. Alex Graves, G Marc, Jacob Bellemare, Remi Menick, Koray Munos, Kavukcuoglu, arXiv:1704.03003arXiv preprintAlex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Auto- mated curriculum learning for neural networks. arXiv preprint arXiv:1704.03003, 2017.\n\nCognitive mapping and planning for visual navigation. Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik, arXiv:1702.03920arXiv preprintSaurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cogni- tive mapping and planning for visual navigation. arXiv preprint arXiv:1702.03920, 2017.\n\nUnifying map and landmark based representations for visual navigation. Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik, arXiv:1712.08125arXiv preprintSaurabh Gupta, David Fouhey, Sergey Levine, and Jitendra Malik. Unifying map and landmark based representations for visual navigation. arXiv preprint arXiv:1712.08125, 2017.\n\nGrounded language learning in a simulated 3d world. Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojtek Czarnecki, Max Jaderberg, Denis Teplyashin, arXiv:1706.06551arXiv preprintKarl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojtek Czarnecki, Max Jaderberg, Denis Teplyashin, et al. Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551, 2017.\n\nUnderstanding grounded language learning agents. Felix Hill, Karl Moritz Hermann, Phil Blunsom, Stephen Clark, arXiv:1710.09867arXiv preprintFelix Hill, Karl Moritz Hermann, Phil Blunsom, and Stephen Clark. Understanding grounded language learning agents. arXiv preprint arXiv:1710.09867, 2017.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\n\nReinforcement learning with unsupervised auxiliary tasks. Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu, arXiv:1611.05397arXiv preprintMax Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.\n\nVizdoom: A doom-based ai research platform for visual reinforcement learning. Micha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Ja\u015bkowski, Computational Intelligence and Games (CIG), 2016 IEEE Conference on. IEEEMicha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja\u015bkowski. Viz- doom: A doom-based ai research platform for visual reinforcement learning. In Computational Intelligence and Games (CIG), 2016 IEEE Conference on, pages 1-8. IEEE, 2016.\n\n. Arbaaz Khan, Clark Zhang, Nikolay Atanasov, Konstantinos Karydis, Vijay Kumar, Daniel D Lee, arXiv:1709.05706arXiv preprintArbaaz Khan, Clark Zhang, Nikolay Atanasov, Konstantinos Karydis, Vijay Kumar, and Daniel D Lee. Memory augmented control networks. arXiv preprint arXiv:1709.05706, 2017.\n\nLooking beyond the visible scene. Aditya Khosla, Byoungkwon An An, J Joseph, Antonio Lim, Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAditya Khosla, Byoungkwon An An, Joseph J Lim, and Antonio Torralba. Looking beyond the visible scene. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3710-3717, 2014.\n\nAi2-thor: An interactive 3d environment for visual ai. Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXiv:1712.05474arXiv preprintEric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017.\n\nPlaying FPS games with deep reinforcement learning. Guillaume Lample, Devendra Singh Chaplot, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. the Thirty-First AAAI Conference on Artificial IntelligenceGuillaume Lample and Devendra Singh Chaplot. Playing FPS games with deep reinforcement learning. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 2017.\n\nAsk your neurons: A deep learning approach to visual question answering. Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, International Journal of Computer Vision. 1251-3Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. Ask your neurons: A deep learning approach to visual question answering. International Journal of Computer Vision, 125(1-3):110- 135, 2017.\n\nRatslam: a hippocampal model for simultaneous localization and mapping. J Michael, Gordon F Milford, David Wyeth, Prasser, Proceedings. ICRA'04. 2004 IEEE International Conference on. ICRA'04. 2004 IEEE International Conference onIEEE1Robotics and AutomationMichael J Milford, Gordon F Wyeth, and David Prasser. Ratslam: a hippocampal model for simultaneous localization and mapping. In Robotics and Automation, 2004. Proceedings. ICRA'04. 2004 IEEE International Conference on, volume 1, pages 403-408. IEEE, 2004.\n\nLearning to navigate in complex environments. Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, Raia Hadsell, arXiv:1611.03673arXiv preprintPiotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and Raia Hadsell. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016.\n\nAsynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International Conference on Machine Learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep rein- forcement learning. In International Conference on Machine Learning, pages 1928-1937, 2016.\n\nThe adobeindoornav dataset: Towards deep reinforcement learning based real-world indoor robot visual navigation. Kaichun Mo, Haoxiang Li, Zhe Lin, Joon-Young Lee, arXiv:1802.08824arXiv preprintKaichun Mo, Haoxiang Li, Zhe Lin, and Joon-Young Lee. The adobeindoornav dataset: Towards deep reinforcement learning based real-world indoor robot visual navigation. arXiv preprint arXiv:1802.08824, 2018.\n\nNeural map: Structured memory for deep reinforcement learning. Emilio Parisotto, Ruslan Salakhutdinov, arXiv:1702.08360arXiv preprintEmilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforce- ment learning. arXiv preprint arXiv:1702.08360, 2017.\n\nNikolay Savinov, Alexey Dosovitskiy, Vladlen Koltun, arXiv:1803.00653Semi-parametric topological memory for navigation. arXiv preprintNikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018.\n\nAirsim: High-fidelity visual and physical simulation for autonomous vehicles. Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor, Field and Service Robotics. SpringerShital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and Service Robotics, pages 621-635. Springer, 2018.\n\nA deep hierarchical approach to lifelong learning in minecraft. Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, Shie Mannor, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. the Thirty-First AAAI Conference on Artificial IntelligenceChen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, and Shie Mannor. A deep hierarchical approach to lifelong learning in minecraft. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 2017.\n\nLecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning. Tijmen Tieleman, Geoffrey Hinton, 4Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26- 31, 2012.\n\nUnsupervised predictive memory in a goal-directed agent. Greg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka Grabska-Barwinska, Jack Rae, Piotr Mirowski, Joel Z Leibo, Adam Santoro, arXiv:1803.10760arXiv preprintGreg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka Grabska- Barwinska, Jack Rae, Piotr Mirowski, Joel Z Leibo, Adam Santoro, et al. Unsupervised predictive memory in a goal-directed agent. arXiv preprint arXiv:1803.10760, 2018.\n\nPlanet-photo geolocation with convolutional neural networks. Tobias Weyand, Ilya Kostrikov, James Philbin, European Conference on Computer Vision. SpringerTobias Weyand, Ilya Kostrikov, and James Philbin. Planet-photo geolocation with convolutional neural networks. In European Conference on Computer Vision, pages 37-55. Springer, 2016.\n\nBuilding generalizable agents with a realistic and rich 3d environment. Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian, arXiv:1801.02209arXiv preprintYi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building generalizable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209, 2018.\n\nLearning to execute. Wojciech Zaremba, Ilya Sutskever, arXiv:1410.4615arXiv preprintWojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.\n\nNeural slam: Learning to explore with external memory. Jingwei Zhang, Lei Tai, Joschka Boedecker, Wolfram Burgard, Ming Liu, arXiv:1706.09520arXiv preprintJingwei Zhang, Lei Tai, Joschka Boedecker, Wolfram Burgard, and Ming Liu. Neural slam: Learning to explore with external memory. arXiv preprint arXiv:1706.09520, 2017.\n\nTarget-driven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi, 2017 IEEE International Conference on Robotics and Automation, ICRA. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In 2017 IEEE International Conference on Robotics and Automation, ICRA, pages 3357-3364, 2017.\n\n10557 nodes and 10896 edges, 200-deep search tree starting at (40.773863. Park, 73.971984\u2022 Central Park: 10557 nodes and 10896 edges, 200-deep search tree starting at (40.773863, -73.971984).\n\n14589 nodes and 15099 edges, 220-deep search tree starting at (40.806379. \u2022 Harlem, 95012473\u2022 Harlem: 14589 nodes and 15099 edges, 220-deep search tree starting at (40.806379, - 73.950124).\n\nThe Paris Rive Gauche environment contains 34026 nodes and 35475 edges, and is defined by a bounding box between Lat/Long coordinates. 0.139157) and (51.526175, -0.080043The Central London StreetView environment contains 24428 nodes and 25352 edges, and is defined by a bounding box between the following Lat/Long coordinates: (51. 50056748.839413, 2.2829247) and (48.866578, 2.3653221The Central London StreetView environment contains 24428 nodes and 25352 edges, and is defined by a bounding box between the following Lat/Long coordinates: (51.500567, -0.139157) and (51.526175, -0.080043). The Paris Rive Gauche environment contains 34026 nodes and 35475 edges, and is defined by a bounding box between Lat/Long coordinates: (48.839413, 2.2829247) and (48.866578, 2.3653221).\n\nWe provide. text file 4 , the locations of the 644 landmarks used throughout the study. 4 Available atWe provide, in a text file 4 , the locations of the 644 landmarks used throughout the study. 4 Available at http://streetlearn.cc\n", "annotations": {"author": "[{\"end\":110,\"start\":48},{\"end\":155,\"start\":111},{\"end\":217,\"start\":156},{\"end\":260,\"start\":218},{\"end\":323,\"start\":261},{\"end\":385,\"start\":324},{\"end\":443,\"start\":386},{\"end\":502,\"start\":444},{\"end\":563,\"start\":503},{\"end\":599,\"start\":564}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":54},{\"end\":132,\"start\":126},{\"end\":174,\"start\":164},{\"end\":237,\"start\":230},{\"end\":275,\"start\":267},{\"end\":340,\"start\":330},{\"end\":400,\"start\":392},{\"end\":461,\"start\":450},{\"end\":519,\"start\":510},{\"end\":576,\"start\":569}]", "author_first_name": "[{\"end\":53,\"start\":48},{\"end\":118,\"start\":111},{\"end\":125,\"start\":119},{\"end\":163,\"start\":156},{\"end\":222,\"start\":218},{\"end\":229,\"start\":223},{\"end\":266,\"start\":261},{\"end\":329,\"start\":324},{\"end\":391,\"start\":386},{\"end\":449,\"start\":444},{\"end\":509,\"start\":503},{\"end\":568,\"start\":564}]", "author_affiliation": "[{\"end\":109,\"start\":89},{\"end\":154,\"start\":134},{\"end\":216,\"start\":196},{\"end\":259,\"start\":239},{\"end\":322,\"start\":302},{\"end\":384,\"start\":364},{\"end\":442,\"start\":422},{\"end\":501,\"start\":481},{\"end\":562,\"start\":542},{\"end\":598,\"start\":578}]", "title": "[{\"end\":45,\"start\":1},{\"end\":644,\"start\":600}]", "venue": null, "abstract": "[{\"end\":2263,\"start\":646}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2511,\"start\":2508},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2514,\"start\":2511},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3716,\"start\":3712},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3719,\"start\":3716},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3737,\"start\":3733},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3763,\"start\":3759},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5106,\"start\":5102},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5164,\"start\":5160},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6435,\"start\":6431},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6483,\"start\":6479},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6486,\"start\":6483},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6529,\"start\":6526},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6550,\"start\":6547},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6553,\"start\":6550},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6576,\"start\":6573},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6812,\"start\":6808},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7191,\"start\":7188},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7194,\"start\":7191},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7197,\"start\":7194},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7318,\"start\":7314},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7321,\"start\":7318},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7324,\"start\":7321},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7419,\"start\":7415},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7422,\"start\":7419},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7425,\"start\":7422},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7428,\"start\":7425},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7557,\"start\":7553},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7601,\"start\":7597},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7818,\"start\":7814},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7821,\"start\":7818},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7824,\"start\":7821},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7827,\"start\":7824},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8152,\"start\":8148},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8154,\"start\":8152},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8156,\"start\":8154},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8159,\"start\":8156},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8788,\"start\":8784},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8915,\"start\":8911},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9008,\"start\":9004},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9152,\"start\":9148},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9168,\"start\":9165},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9171,\"start\":9168},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9257,\"start\":9253},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9395,\"start\":9391},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9398,\"start\":9395},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9528,\"start\":9524},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16078,\"start\":16074},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16116,\"start\":16112},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16499,\"start\":16495},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17385,\"start\":17381},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17908,\"start\":17904},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17911,\"start\":17908},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17914,\"start\":17911},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17917,\"start\":17914},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18625,\"start\":18621},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18763,\"start\":18759},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19094,\"start\":19091},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19097,\"start\":19094},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19100,\"start\":19097},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22883,\"start\":22880},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22886,\"start\":22883},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24737,\"start\":24733},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25111,\"start\":25107},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33103,\"start\":33099},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33621,\"start\":33617},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33624,\"start\":33621},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33844,\"start\":33840},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34030,\"start\":34026},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":38490,\"start\":38486},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":38697,\"start\":38693},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":40925,\"start\":40921},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41776,\"start\":41772},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":42015,\"start\":42011}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44437,\"start\":44084},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44525,\"start\":44438},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44731,\"start\":44526},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45311,\"start\":44732},{\"attributes\":{\"id\":\"fig_4\"},\"end\":46143,\"start\":45312},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46338,\"start\":46144},{\"attributes\":{\"id\":\"fig_7\"},\"end\":46895,\"start\":46339},{\"attributes\":{\"id\":\"fig_9\"},\"end\":47218,\"start\":46896},{\"attributes\":{\"id\":\"fig_10\"},\"end\":47396,\"start\":47219},{\"attributes\":{\"id\":\"fig_11\"},\"end\":47753,\"start\":47397},{\"attributes\":{\"id\":\"fig_12\"},\"end\":47927,\"start\":47754},{\"attributes\":{\"id\":\"fig_13\"},\"end\":48258,\"start\":47928}]", "paragraph": "[{\"end\":3401,\"start\":2279},{\"end\":4178,\"start\":3403},{\"end\":5936,\"start\":4180},{\"end\":6112,\"start\":5953},{\"end\":6974,\"start\":6114},{\"end\":7681,\"start\":6976},{\"end\":8583,\"start\":7683},{\"end\":9716,\"start\":8585},{\"end\":10498,\"start\":9732},{\"end\":11176,\"start\":10539},{\"end\":11886,\"start\":11178},{\"end\":11967,\"start\":11930},{\"end\":12283,\"start\":11969},{\"end\":13031,\"start\":12362},{\"end\":13523,\"start\":13033},{\"end\":14511,\"start\":13525},{\"end\":14727,\"start\":14523},{\"end\":15822,\"start\":14747},{\"end\":16355,\"start\":15840},{\"end\":16605,\"start\":16357},{\"end\":16890,\"start\":16607},{\"end\":17890,\"start\":16892},{\"end\":18584,\"start\":17892},{\"end\":18913,\"start\":18586},{\"end\":19623,\"start\":18937},{\"end\":20034,\"start\":19635},{\"end\":21515,\"start\":20093},{\"end\":22069,\"start\":21517},{\"end\":22412,\"start\":22122},{\"end\":23434,\"start\":22414},{\"end\":24142,\"start\":23436},{\"end\":24990,\"start\":24144},{\"end\":25896,\"start\":25027},{\"end\":27179,\"start\":25898},{\"end\":28478,\"start\":27218},{\"end\":29301,\"start\":28480},{\"end\":30416,\"start\":29303},{\"end\":31498,\"start\":30431},{\"end\":34317,\"start\":31553},{\"end\":34634,\"start\":34345},{\"end\":35714,\"start\":34636},{\"end\":36079,\"start\":35716},{\"end\":36248,\"start\":36081},{\"end\":37293,\"start\":36303},{\"end\":37563,\"start\":37342},{\"end\":38341,\"start\":37604},{\"end\":38698,\"start\":38404},{\"end\":39065,\"start\":38700},{\"end\":39399,\"start\":39067},{\"end\":40102,\"start\":39401},{\"end\":40906,\"start\":40104},{\"end\":41190,\"start\":40908},{\"end\":41536,\"start\":41192},{\"end\":41954,\"start\":41569},{\"end\":42794,\"start\":41956},{\"end\":43689,\"start\":42822},{\"end\":44083,\"start\":43707}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11929,\"start\":11887},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12361,\"start\":12284},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14746,\"start\":14728},{\"attributes\":{\"id\":\"formula_3\"},\"end\":37603,\"start\":37564}]", "table_ref": "[{\"end\":24810,\"start\":24803},{\"end\":26070,\"start\":26063}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2277,\"start\":2265},{\"attributes\":{\"n\":\"2\"},\"end\":5951,\"start\":5939},{\"attributes\":{\"n\":\"3\"},\"end\":9730,\"start\":9719},{\"attributes\":{\"n\":\"3.1\"},\"end\":10537,\"start\":10501},{\"attributes\":{\"n\":\"4\"},\"end\":14521,\"start\":14514},{\"attributes\":{\"n\":\"4.1\"},\"end\":15838,\"start\":15825},{\"attributes\":{\"n\":\"4.2\"},\"end\":18935,\"start\":18916},{\"attributes\":{\"n\":\"5\"},\"end\":19633,\"start\":19626},{\"attributes\":{\"n\":\"5.1\"},\"end\":20091,\"start\":20037},{\"attributes\":{\"n\":\"5.2\"},\"end\":22120,\"start\":22072},{\"attributes\":{\"n\":\"5.3\"},\"end\":25025,\"start\":24993},{\"attributes\":{\"n\":\"5.4\"},\"end\":27216,\"start\":27182},{\"attributes\":{\"n\":\"6\"},\"end\":30429,\"start\":30419},{\"end\":31551,\"start\":31501},{\"end\":34343,\"start\":34320},{\"end\":36301,\"start\":36251},{\"end\":37340,\"start\":37296},{\"end\":38368,\"start\":38344},{\"end\":38402,\"start\":38371},{\"end\":41567,\"start\":41539},{\"end\":42820,\"start\":42797},{\"end\":43705,\"start\":43692},{\"end\":44095,\"start\":44085},{\"end\":44537,\"start\":44527},{\"end\":44743,\"start\":44733},{\"end\":45323,\"start\":45313},{\"end\":46155,\"start\":46145},{\"end\":46350,\"start\":46340},{\"end\":46907,\"start\":46897},{\"end\":47230,\"start\":47220},{\"end\":47404,\"start\":47398},{\"end\":47765,\"start\":47755},{\"end\":47930,\"start\":47929}]", "table": null, "figure_caption": "[{\"end\":44437,\"start\":44097},{\"end\":44525,\"start\":44440},{\"end\":44731,\"start\":44539},{\"end\":45311,\"start\":44745},{\"end\":46143,\"start\":45325},{\"end\":46338,\"start\":46157},{\"end\":46895,\"start\":46352},{\"end\":47218,\"start\":46909},{\"end\":47396,\"start\":47232},{\"end\":47753,\"start\":47406},{\"end\":47927,\"start\":47767},{\"end\":48258,\"start\":47931}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3399,\"start\":3392},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10226,\"start\":10219},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10496,\"start\":10489},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11631,\"start\":11624},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16400,\"start\":16391},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16641,\"start\":16632},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16931,\"start\":16922},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17396,\"start\":17387},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20552,\"start\":20544},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21612,\"start\":21594},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22068,\"start\":22059},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22897,\"start\":22888},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23525,\"start\":23518},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25413,\"start\":25406},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26403,\"start\":26395},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28569,\"start\":28562},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28942,\"start\":28933},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29343,\"start\":29334},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":32693,\"start\":32687},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":33033,\"start\":33026},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":34633,\"start\":34625},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":34797,\"start\":34788},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35379,\"start\":35373},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":35833,\"start\":35825},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":36882,\"start\":36877}]", "bib_author_first_name": "[{\"end\":49078,\"start\":49073},{\"end\":49091,\"start\":49089},{\"end\":49102,\"start\":49096},{\"end\":49114,\"start\":49110},{\"end\":49126,\"start\":49122},{\"end\":49140,\"start\":49136},{\"end\":49156,\"start\":49153},{\"end\":49170,\"start\":49163},{\"end\":49697,\"start\":49692},{\"end\":49716,\"start\":49712},{\"end\":49732,\"start\":49725},{\"end\":49745,\"start\":49740},{\"end\":49759,\"start\":49754},{\"end\":50244,\"start\":50238},{\"end\":50260,\"start\":50253},{\"end\":50275,\"start\":50268},{\"end\":50289,\"start\":50282},{\"end\":50307,\"start\":50300},{\"end\":50324,\"start\":50319},{\"end\":50344,\"start\":50335},{\"end\":50355,\"start\":50354},{\"end\":50370,\"start\":50364},{\"end\":50387,\"start\":50381},{\"end\":50695,\"start\":50688},{\"end\":50709,\"start\":50705},{\"end\":50711,\"start\":50710},{\"end\":50724,\"start\":50719},{\"end\":50740,\"start\":50737},{\"end\":50753,\"start\":50747},{\"end\":50774,\"start\":50766},{\"end\":50790,\"start\":50784},{\"end\":50806,\"start\":50801},{\"end\":51133,\"start\":51127},{\"end\":51148,\"start\":51142},{\"end\":51165,\"start\":51160},{\"end\":51182,\"start\":51177},{\"end\":51620,\"start\":51619},{\"end\":51635,\"start\":51630},{\"end\":51644,\"start\":51636},{\"end\":51663,\"start\":51662},{\"end\":51679,\"start\":51674},{\"end\":51697,\"start\":51689},{\"end\":51721,\"start\":51709},{\"end\":51735,\"start\":51729},{\"end\":52055,\"start\":52048},{\"end\":52073,\"start\":52068},{\"end\":52311,\"start\":52307},{\"end\":52323,\"start\":52319},{\"end\":52341,\"start\":52336},{\"end\":52356,\"start\":52352},{\"end\":52373,\"start\":52366},{\"end\":52683,\"start\":52679},{\"end\":52699,\"start\":52693},{\"end\":52713,\"start\":52709},{\"end\":52725,\"start\":52720},{\"end\":52944,\"start\":52939},{\"end\":52958,\"start\":52952},{\"end\":52970,\"start\":52964},{\"end\":52989,\"start\":52983},{\"end\":53006,\"start\":52998},{\"end\":53023,\"start\":53016},{\"end\":53037,\"start\":53031},{\"end\":53048,\"start\":53044},{\"end\":53060,\"start\":53055},{\"end\":53405,\"start\":53397},{\"end\":53427,\"start\":53421},{\"end\":53445,\"start\":53439},{\"end\":53776,\"start\":53768},{\"end\":53803,\"start\":53792},{\"end\":53810,\"start\":53804},{\"end\":53827,\"start\":53823},{\"end\":53853,\"start\":53846},{\"end\":53871,\"start\":53865},{\"end\":54259,\"start\":54258},{\"end\":54280,\"start\":54273},{\"end\":54598,\"start\":54592},{\"end\":54614,\"start\":54606},{\"end\":54628,\"start\":54622},{\"end\":54644,\"start\":54638},{\"end\":54659,\"start\":54653},{\"end\":55084,\"start\":55078},{\"end\":55104,\"start\":55099},{\"end\":55125,\"start\":55124},{\"end\":55140,\"start\":55135},{\"end\":55142,\"start\":55141},{\"end\":55475,\"start\":55468},{\"end\":55489,\"start\":55485},{\"end\":55494,\"start\":55490},{\"end\":55512,\"start\":55506},{\"end\":55531,\"start\":55525},{\"end\":55552,\"start\":55542},{\"end\":55570,\"start\":55566},{\"end\":55585,\"start\":55579},{\"end\":56104,\"start\":56098},{\"end\":56125,\"start\":56118},{\"end\":56291,\"start\":56285},{\"end\":56311,\"start\":56305},{\"end\":56323,\"start\":56317},{\"end\":56342,\"start\":56335},{\"end\":56357,\"start\":56350},{\"end\":56606,\"start\":56601},{\"end\":56623,\"start\":56617},{\"end\":56635,\"start\":56631},{\"end\":56648,\"start\":56643},{\"end\":56668,\"start\":56659},{\"end\":56678,\"start\":56675},{\"end\":56690,\"start\":56685},{\"end\":56702,\"start\":56698},{\"end\":56714,\"start\":56711},{\"end\":56727,\"start\":56723},{\"end\":56742,\"start\":56737},{\"end\":56754,\"start\":56749},{\"end\":57246,\"start\":57242},{\"end\":57256,\"start\":57255},{\"end\":57268,\"start\":57263},{\"end\":57284,\"start\":57280},{\"end\":57298,\"start\":57293},{\"end\":57583,\"start\":57576},{\"end\":57596,\"start\":57591},{\"end\":57613,\"start\":57607},{\"end\":57627,\"start\":57622},{\"end\":57648,\"start\":57640},{\"end\":57944,\"start\":57937},{\"end\":57957,\"start\":57952},{\"end\":57972,\"start\":57966},{\"end\":57989,\"start\":57981},{\"end\":58265,\"start\":58254},{\"end\":58280,\"start\":58275},{\"end\":58292,\"start\":58287},{\"end\":58305,\"start\":58300},{\"end\":58316,\"start\":58312},{\"end\":58333,\"start\":58327},{\"end\":58346,\"start\":58341},{\"end\":58365,\"start\":58359},{\"end\":58380,\"start\":58377},{\"end\":58397,\"start\":58392},{\"end\":58749,\"start\":58744},{\"end\":58760,\"start\":58756},{\"end\":58767,\"start\":58761},{\"end\":58781,\"start\":58777},{\"end\":58798,\"start\":58791},{\"end\":59019,\"start\":59015},{\"end\":59038,\"start\":59032},{\"end\":59242,\"start\":59239},{\"end\":59263,\"start\":59254},{\"end\":59278,\"start\":59270},{\"end\":59285,\"start\":59279},{\"end\":59300,\"start\":59297},{\"end\":59313,\"start\":59309},{\"end\":59315,\"start\":59314},{\"end\":59328,\"start\":59323},{\"end\":59342,\"start\":59337},{\"end\":59689,\"start\":59683},{\"end\":59703,\"start\":59698},{\"end\":59721,\"start\":59713},{\"end\":59733,\"start\":59728},{\"end\":59750,\"start\":59742},{\"end\":60102,\"start\":60096},{\"end\":60114,\"start\":60109},{\"end\":60129,\"start\":60122},{\"end\":60152,\"start\":60140},{\"end\":60167,\"start\":60162},{\"end\":60183,\"start\":60175},{\"end\":60431,\"start\":60425},{\"end\":60459,\"start\":60458},{\"end\":60475,\"start\":60468},{\"end\":60900,\"start\":60896},{\"end\":60915,\"start\":60908},{\"end\":60932,\"start\":60926},{\"end\":60945,\"start\":60941},{\"end\":60958,\"start\":60951},{\"end\":60969,\"start\":60966},{\"end\":61252,\"start\":61243},{\"end\":61269,\"start\":61261},{\"end\":61683,\"start\":61676},{\"end\":61702,\"start\":61696},{\"end\":61718,\"start\":61713},{\"end\":62042,\"start\":62041},{\"end\":62058,\"start\":62052},{\"end\":62060,\"start\":62059},{\"end\":62075,\"start\":62070},{\"end\":62537,\"start\":62532},{\"end\":62554,\"start\":62548},{\"end\":62569,\"start\":62564},{\"end\":62583,\"start\":62577},{\"end\":62597,\"start\":62591},{\"end\":62613,\"start\":62607},{\"end\":62627,\"start\":62622},{\"end\":62639,\"start\":62635},{\"end\":62657,\"start\":62650},{\"end\":62670,\"start\":62665},{\"end\":62692,\"start\":62684},{\"end\":62706,\"start\":62702},{\"end\":63083,\"start\":63074},{\"end\":63095,\"start\":63090},{\"end\":63108,\"start\":63096},{\"end\":63121,\"start\":63116},{\"end\":63133,\"start\":63129},{\"end\":63149,\"start\":63142},{\"end\":63164,\"start\":63161},{\"end\":63178,\"start\":63173},{\"end\":63192,\"start\":63187},{\"end\":63637,\"start\":63630},{\"end\":63650,\"start\":63642},{\"end\":63658,\"start\":63655},{\"end\":63674,\"start\":63664},{\"end\":63986,\"start\":63980},{\"end\":64004,\"start\":63998},{\"end\":64205,\"start\":64198},{\"end\":64221,\"start\":64215},{\"end\":64242,\"start\":64235},{\"end\":64564,\"start\":64558},{\"end\":64581,\"start\":64571},{\"end\":64592,\"start\":64587},{\"end\":64607,\"start\":64601},{\"end\":64923,\"start\":64919},{\"end\":64939,\"start\":64933},{\"end\":64951,\"start\":64948},{\"end\":64966,\"start\":64960},{\"end\":64968,\"start\":64967},{\"end\":64984,\"start\":64980},{\"end\":65498,\"start\":65492},{\"end\":65517,\"start\":65509},{\"end\":65780,\"start\":65776},{\"end\":65797,\"start\":65788},{\"end\":65809,\"start\":65804},{\"end\":65821,\"start\":65816},{\"end\":65833,\"start\":65829},{\"end\":65850,\"start\":65841},{\"end\":65874,\"start\":65870},{\"end\":65885,\"start\":65880},{\"end\":65900,\"start\":65896},{\"end\":65902,\"start\":65901},{\"end\":65914,\"start\":65910},{\"end\":66274,\"start\":66268},{\"end\":66287,\"start\":66283},{\"end\":66304,\"start\":66299},{\"end\":66620,\"start\":66618},{\"end\":66630,\"start\":66625},{\"end\":66642,\"start\":66635},{\"end\":66661,\"start\":66653},{\"end\":66893,\"start\":66885},{\"end\":66907,\"start\":66903},{\"end\":67107,\"start\":67100},{\"end\":67118,\"start\":67115},{\"end\":67131,\"start\":67124},{\"end\":67150,\"start\":67143},{\"end\":67164,\"start\":67160},{\"end\":67457,\"start\":67453},{\"end\":67470,\"start\":67463},{\"end\":67485,\"start\":67481},{\"end\":67499,\"start\":67493},{\"end\":67501,\"start\":67500},{\"end\":67514,\"start\":67507},{\"end\":67524,\"start\":67522},{\"end\":67537,\"start\":67534},{\"end\":68163,\"start\":68162}]", "bib_author_last_name": "[{\"end\":49087,\"start\":49079},{\"end\":49094,\"start\":49092},{\"end\":49108,\"start\":49103},{\"end\":49120,\"start\":49115},{\"end\":49134,\"start\":49127},{\"end\":49151,\"start\":49141},{\"end\":49161,\"start\":49157},{\"end\":49176,\"start\":49171},{\"end\":49710,\"start\":49698},{\"end\":49723,\"start\":49717},{\"end\":49738,\"start\":49733},{\"end\":49752,\"start\":49746},{\"end\":49765,\"start\":49760},{\"end\":50251,\"start\":50245},{\"end\":50266,\"start\":50261},{\"end\":50280,\"start\":50276},{\"end\":50298,\"start\":50290},{\"end\":50317,\"start\":50308},{\"end\":50333,\"start\":50325},{\"end\":50352,\"start\":50345},{\"end\":50362,\"start\":50356},{\"end\":50379,\"start\":50371},{\"end\":50394,\"start\":50388},{\"end\":50403,\"start\":50396},{\"end\":50703,\"start\":50696},{\"end\":50717,\"start\":50712},{\"end\":50735,\"start\":50725},{\"end\":50745,\"start\":50741},{\"end\":50764,\"start\":50754},{\"end\":50782,\"start\":50775},{\"end\":50799,\"start\":50791},{\"end\":50812,\"start\":50807},{\"end\":51140,\"start\":51134},{\"end\":51158,\"start\":51149},{\"end\":51175,\"start\":51166},{\"end\":51189,\"start\":51183},{\"end\":51628,\"start\":51621},{\"end\":51652,\"start\":51645},{\"end\":51660,\"start\":51654},{\"end\":51672,\"start\":51664},{\"end\":51687,\"start\":51680},{\"end\":51707,\"start\":51698},{\"end\":51727,\"start\":51722},{\"end\":51741,\"start\":51736},{\"end\":51758,\"start\":51743},{\"end\":52066,\"start\":52056},{\"end\":52078,\"start\":52074},{\"end\":52317,\"start\":52312},{\"end\":52334,\"start\":52324},{\"end\":52350,\"start\":52342},{\"end\":52364,\"start\":52357},{\"end\":52381,\"start\":52374},{\"end\":52691,\"start\":52684},{\"end\":52707,\"start\":52700},{\"end\":52718,\"start\":52714},{\"end\":52737,\"start\":52726},{\"end\":52950,\"start\":52945},{\"end\":52962,\"start\":52959},{\"end\":52981,\"start\":52971},{\"end\":52996,\"start\":52990},{\"end\":53014,\"start\":53007},{\"end\":53029,\"start\":53024},{\"end\":53042,\"start\":53038},{\"end\":53053,\"start\":53049},{\"end\":53066,\"start\":53061},{\"end\":53419,\"start\":53406},{\"end\":53437,\"start\":53428},{\"end\":53459,\"start\":53446},{\"end\":53790,\"start\":53777},{\"end\":53821,\"start\":53811},{\"end\":53844,\"start\":53828},{\"end\":53863,\"start\":53854},{\"end\":53885,\"start\":53872},{\"end\":54271,\"start\":54260},{\"end\":54286,\"start\":54281},{\"end\":54291,\"start\":54288},{\"end\":54604,\"start\":54599},{\"end\":54620,\"start\":54615},{\"end\":54636,\"start\":54629},{\"end\":54651,\"start\":54645},{\"end\":54666,\"start\":54660},{\"end\":55097,\"start\":55085},{\"end\":55113,\"start\":55105},{\"end\":55122,\"start\":55115},{\"end\":55133,\"start\":55126},{\"end\":55150,\"start\":55143},{\"end\":55157,\"start\":55152},{\"end\":55483,\"start\":55476},{\"end\":55504,\"start\":55495},{\"end\":55523,\"start\":55513},{\"end\":55540,\"start\":55532},{\"end\":55564,\"start\":55553},{\"end\":55577,\"start\":55571},{\"end\":55593,\"start\":55586},{\"end\":56116,\"start\":56105},{\"end\":56132,\"start\":56126},{\"end\":56303,\"start\":56292},{\"end\":56315,\"start\":56312},{\"end\":56333,\"start\":56324},{\"end\":56348,\"start\":56343},{\"end\":56364,\"start\":56358},{\"end\":56371,\"start\":56366},{\"end\":56615,\"start\":56607},{\"end\":56629,\"start\":56624},{\"end\":56641,\"start\":56636},{\"end\":56657,\"start\":56649},{\"end\":56673,\"start\":56669},{\"end\":56683,\"start\":56679},{\"end\":56696,\"start\":56691},{\"end\":56709,\"start\":56703},{\"end\":56721,\"start\":56715},{\"end\":56735,\"start\":56728},{\"end\":56747,\"start\":56743},{\"end\":56766,\"start\":56755},{\"end\":57253,\"start\":57247},{\"end\":57261,\"start\":57257},{\"end\":57278,\"start\":57269},{\"end\":57291,\"start\":57285},{\"end\":57304,\"start\":57299},{\"end\":57317,\"start\":57306},{\"end\":57589,\"start\":57584},{\"end\":57605,\"start\":57597},{\"end\":57620,\"start\":57614},{\"end\":57638,\"start\":57628},{\"end\":57654,\"start\":57649},{\"end\":57950,\"start\":57945},{\"end\":57964,\"start\":57958},{\"end\":57979,\"start\":57973},{\"end\":57995,\"start\":57990},{\"end\":58273,\"start\":58266},{\"end\":58285,\"start\":58281},{\"end\":58298,\"start\":58293},{\"end\":58310,\"start\":58306},{\"end\":58325,\"start\":58317},{\"end\":58339,\"start\":58334},{\"end\":58357,\"start\":58347},{\"end\":58375,\"start\":58366},{\"end\":58390,\"start\":58381},{\"end\":58408,\"start\":58398},{\"end\":58754,\"start\":58750},{\"end\":58775,\"start\":58768},{\"end\":58789,\"start\":58782},{\"end\":58804,\"start\":58799},{\"end\":59030,\"start\":59020},{\"end\":59050,\"start\":59039},{\"end\":59252,\"start\":59243},{\"end\":59268,\"start\":59264},{\"end\":59295,\"start\":59286},{\"end\":59307,\"start\":59301},{\"end\":59321,\"start\":59316},{\"end\":59335,\"start\":59329},{\"end\":59354,\"start\":59343},{\"end\":59696,\"start\":59690},{\"end\":59711,\"start\":59704},{\"end\":59726,\"start\":59722},{\"end\":59740,\"start\":59734},{\"end\":59760,\"start\":59751},{\"end\":60107,\"start\":60103},{\"end\":60120,\"start\":60115},{\"end\":60138,\"start\":60130},{\"end\":60160,\"start\":60153},{\"end\":60173,\"start\":60168},{\"end\":60187,\"start\":60184},{\"end\":60438,\"start\":60432},{\"end\":60456,\"start\":60440},{\"end\":60466,\"start\":60460},{\"end\":60479,\"start\":60476},{\"end\":60489,\"start\":60481},{\"end\":60906,\"start\":60901},{\"end\":60924,\"start\":60916},{\"end\":60939,\"start\":60933},{\"end\":60949,\"start\":60946},{\"end\":60964,\"start\":60959},{\"end\":60977,\"start\":60970},{\"end\":61259,\"start\":61253},{\"end\":61283,\"start\":61270},{\"end\":61694,\"start\":61684},{\"end\":61711,\"start\":61703},{\"end\":61724,\"start\":61719},{\"end\":62050,\"start\":62043},{\"end\":62068,\"start\":62061},{\"end\":62081,\"start\":62076},{\"end\":62090,\"start\":62083},{\"end\":62546,\"start\":62538},{\"end\":62562,\"start\":62555},{\"end\":62575,\"start\":62570},{\"end\":62589,\"start\":62584},{\"end\":62605,\"start\":62598},{\"end\":62620,\"start\":62614},{\"end\":62633,\"start\":62628},{\"end\":62648,\"start\":62640},{\"end\":62663,\"start\":62658},{\"end\":62682,\"start\":62671},{\"end\":62700,\"start\":62693},{\"end\":62714,\"start\":62707},{\"end\":63088,\"start\":63084},{\"end\":63114,\"start\":63109},{\"end\":63127,\"start\":63122},{\"end\":63140,\"start\":63134},{\"end\":63159,\"start\":63150},{\"end\":63171,\"start\":63165},{\"end\":63185,\"start\":63179},{\"end\":63204,\"start\":63193},{\"end\":63640,\"start\":63638},{\"end\":63653,\"start\":63651},{\"end\":63662,\"start\":63659},{\"end\":63678,\"start\":63675},{\"end\":63996,\"start\":63987},{\"end\":64018,\"start\":64005},{\"end\":64213,\"start\":64206},{\"end\":64233,\"start\":64222},{\"end\":64249,\"start\":64243},{\"end\":64569,\"start\":64565},{\"end\":64585,\"start\":64582},{\"end\":64599,\"start\":64593},{\"end\":64614,\"start\":64608},{\"end\":64931,\"start\":64924},{\"end\":64946,\"start\":64940},{\"end\":64958,\"start\":64952},{\"end\":64978,\"start\":64969},{\"end\":64991,\"start\":64985},{\"end\":65507,\"start\":65499},{\"end\":65524,\"start\":65518},{\"end\":65786,\"start\":65781},{\"end\":65802,\"start\":65798},{\"end\":65814,\"start\":65810},{\"end\":65827,\"start\":65822},{\"end\":65839,\"start\":65834},{\"end\":65868,\"start\":65851},{\"end\":65878,\"start\":65875},{\"end\":65894,\"start\":65886},{\"end\":65908,\"start\":65903},{\"end\":65922,\"start\":65915},{\"end\":66281,\"start\":66275},{\"end\":66297,\"start\":66288},{\"end\":66312,\"start\":66305},{\"end\":66623,\"start\":66621},{\"end\":66633,\"start\":66631},{\"end\":66651,\"start\":66643},{\"end\":66666,\"start\":66662},{\"end\":66901,\"start\":66894},{\"end\":66917,\"start\":66908},{\"end\":67113,\"start\":67108},{\"end\":67122,\"start\":67119},{\"end\":67141,\"start\":67132},{\"end\":67158,\"start\":67151},{\"end\":67168,\"start\":67165},{\"end\":67461,\"start\":67458},{\"end\":67479,\"start\":67471},{\"end\":67491,\"start\":67486},{\"end\":67505,\"start\":67502},{\"end\":67520,\"start\":67515},{\"end\":67532,\"start\":67525},{\"end\":67545,\"start\":67538},{\"end\":67973,\"start\":67969},{\"end\":68170,\"start\":68164}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1711.07280\",\"id\":\"b0\"},\"end\":49623,\"start\":49073},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":44604205},\"end\":50158,\"start\":49625},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13693718},\"end\":50686,\"start\":50160},{\"attributes\":{\"doi\":\"arXiv:1612.03801\",\"id\":\"b3\"},\"end\":51104,\"start\":50688},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":873046},\"end\":51523,\"start\":51106},{\"attributes\":{\"id\":\"b5\"},\"end\":52002,\"start\":51525},{\"attributes\":{\"doi\":\"arXiv:1701.09135\",\"id\":\"b6\"},\"end\":52227,\"start\":52004},{\"attributes\":{\"doi\":\"arXiv:1711.10137\",\"id\":\"b7\"},\"end\":52611,\"start\":52229},{\"attributes\":{\"doi\":\"arXiv:1711.07479\",\"id\":\"b8\"},\"end\":52937,\"start\":52613},{\"attributes\":{\"doi\":\"arXiv:1709.06158\",\"id\":\"b9\"},\"end\":53395,\"start\":52939},{\"attributes\":{\"id\":\"b10\"},\"end\":53698,\"start\":53397},{\"attributes\":{\"doi\":\"arXiv:1706.07230\",\"id\":\"b11\"},\"end\":54146,\"start\":53700},{\"attributes\":{\"doi\":\"arXiv:1803.07770\",\"id\":\"b12\"},\"end\":54508,\"start\":54148},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":18015872},\"end\":55004,\"start\":54510},{\"attributes\":{\"doi\":\"arXiv:1802.02274\",\"id\":\"b14\"},\"end\":55383,\"start\":55006},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":5736847},\"end\":56054,\"start\":55385},{\"attributes\":{\"doi\":\"arXiv:1611.01779\",\"id\":\"b16\"},\"end\":56283,\"start\":56056},{\"attributes\":{\"doi\":\"arXiv:1711.03938\",\"id\":\"b17\"},\"end\":56599,\"start\":56285},{\"attributes\":{\"doi\":\"arXiv:1802.01561\",\"id\":\"b18\"},\"end\":57189,\"start\":56601},{\"attributes\":{\"doi\":\"arXiv:1704.03003\",\"id\":\"b19\"},\"end\":57520,\"start\":57191},{\"attributes\":{\"doi\":\"arXiv:1702.03920\",\"id\":\"b20\"},\"end\":57864,\"start\":57522},{\"attributes\":{\"doi\":\"arXiv:1712.08125\",\"id\":\"b21\"},\"end\":58200,\"start\":57866},{\"attributes\":{\"doi\":\"arXiv:1706.06551\",\"id\":\"b22\"},\"end\":58693,\"start\":58202},{\"attributes\":{\"doi\":\"arXiv:1710.09867\",\"id\":\"b23\"},\"end\":58989,\"start\":58695},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1915014},\"end\":59179,\"start\":58991},{\"attributes\":{\"doi\":\"arXiv:1611.05397\",\"id\":\"b25\"},\"end\":59603,\"start\":59181},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":430714},\"end\":60092,\"start\":59605},{\"attributes\":{\"doi\":\"arXiv:1709.05706\",\"id\":\"b27\"},\"end\":60389,\"start\":60094},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2078165},\"end\":60839,\"start\":60391},{\"attributes\":{\"doi\":\"arXiv:1712.05474\",\"id\":\"b29\"},\"end\":61189,\"start\":60841},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13973139},\"end\":61601,\"start\":61191},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":11621064},\"end\":61967,\"start\":61603},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7139556},\"end\":62484,\"start\":61969},{\"attributes\":{\"doi\":\"arXiv:1611.03673\",\"id\":\"b33\"},\"end\":63018,\"start\":62486},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6875312},\"end\":63515,\"start\":63020},{\"attributes\":{\"doi\":\"arXiv:1802.08824\",\"id\":\"b35\"},\"end\":63915,\"start\":63517},{\"attributes\":{\"doi\":\"arXiv:1702.08360\",\"id\":\"b36\"},\"end\":64196,\"start\":63917},{\"attributes\":{\"doi\":\"arXiv:1803.00653\",\"id\":\"b37\"},\"end\":64478,\"start\":64198},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":20999239},\"end\":64853,\"start\":64480},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":46900},\"end\":65355,\"start\":64855},{\"attributes\":{\"id\":\"b40\"},\"end\":65717,\"start\":65357},{\"attributes\":{\"doi\":\"arXiv:1803.10760\",\"id\":\"b41\"},\"end\":66205,\"start\":65719},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":171846},\"end\":66544,\"start\":66207},{\"attributes\":{\"doi\":\"arXiv:1801.02209\",\"id\":\"b43\"},\"end\":66862,\"start\":66546},{\"attributes\":{\"doi\":\"arXiv:1410.4615\",\"id\":\"b44\"},\"end\":67043,\"start\":66864},{\"attributes\":{\"doi\":\"arXiv:1706.09520\",\"id\":\"b45\"},\"end\":67367,\"start\":67045},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":2305273},\"end\":67893,\"start\":67369},{\"attributes\":{\"doi\":\"73.971984\",\"id\":\"b47\"},\"end\":68086,\"start\":67895},{\"attributes\":{\"id\":\"b48\"},\"end\":68277,\"start\":68088},{\"attributes\":{\"doi\":\"0.139157) and (51.526175, -0.080043\",\"id\":\"b49\"},\"end\":69057,\"start\":68279},{\"attributes\":{\"id\":\"b50\"},\"end\":69290,\"start\":69059}]", "bib_title": "[{\"end\":49690,\"start\":49625},{\"end\":50236,\"start\":50160},{\"end\":51125,\"start\":51106},{\"end\":54590,\"start\":54510},{\"end\":55466,\"start\":55385},{\"end\":59013,\"start\":58991},{\"end\":59681,\"start\":59605},{\"end\":60423,\"start\":60391},{\"end\":61241,\"start\":61191},{\"end\":61674,\"start\":61603},{\"end\":62039,\"start\":61969},{\"end\":63072,\"start\":63020},{\"end\":64556,\"start\":64480},{\"end\":64917,\"start\":64855},{\"end\":66266,\"start\":66207},{\"end\":67451,\"start\":67369},{\"end\":68412,\"start\":68279}]", "bib_author": "[{\"end\":49089,\"start\":49073},{\"end\":49096,\"start\":49089},{\"end\":49110,\"start\":49096},{\"end\":49122,\"start\":49110},{\"end\":49136,\"start\":49122},{\"end\":49153,\"start\":49136},{\"end\":49163,\"start\":49153},{\"end\":49178,\"start\":49163},{\"end\":49712,\"start\":49692},{\"end\":49725,\"start\":49712},{\"end\":49740,\"start\":49725},{\"end\":49754,\"start\":49740},{\"end\":49767,\"start\":49754},{\"end\":50253,\"start\":50238},{\"end\":50268,\"start\":50253},{\"end\":50282,\"start\":50268},{\"end\":50300,\"start\":50282},{\"end\":50319,\"start\":50300},{\"end\":50335,\"start\":50319},{\"end\":50354,\"start\":50335},{\"end\":50364,\"start\":50354},{\"end\":50381,\"start\":50364},{\"end\":50396,\"start\":50381},{\"end\":50405,\"start\":50396},{\"end\":50705,\"start\":50688},{\"end\":50719,\"start\":50705},{\"end\":50737,\"start\":50719},{\"end\":50747,\"start\":50737},{\"end\":50766,\"start\":50747},{\"end\":50784,\"start\":50766},{\"end\":50801,\"start\":50784},{\"end\":50814,\"start\":50801},{\"end\":51142,\"start\":51127},{\"end\":51160,\"start\":51142},{\"end\":51177,\"start\":51160},{\"end\":51191,\"start\":51177},{\"end\":51630,\"start\":51619},{\"end\":51654,\"start\":51630},{\"end\":51662,\"start\":51654},{\"end\":51674,\"start\":51662},{\"end\":51689,\"start\":51674},{\"end\":51709,\"start\":51689},{\"end\":51729,\"start\":51709},{\"end\":51743,\"start\":51729},{\"end\":51760,\"start\":51743},{\"end\":52068,\"start\":52048},{\"end\":52080,\"start\":52068},{\"end\":52319,\"start\":52307},{\"end\":52336,\"start\":52319},{\"end\":52352,\"start\":52336},{\"end\":52366,\"start\":52352},{\"end\":52383,\"start\":52366},{\"end\":52693,\"start\":52679},{\"end\":52709,\"start\":52693},{\"end\":52720,\"start\":52709},{\"end\":52739,\"start\":52720},{\"end\":52952,\"start\":52939},{\"end\":52964,\"start\":52952},{\"end\":52983,\"start\":52964},{\"end\":52998,\"start\":52983},{\"end\":53016,\"start\":52998},{\"end\":53031,\"start\":53016},{\"end\":53044,\"start\":53031},{\"end\":53055,\"start\":53044},{\"end\":53068,\"start\":53055},{\"end\":53421,\"start\":53397},{\"end\":53439,\"start\":53421},{\"end\":53461,\"start\":53439},{\"end\":53792,\"start\":53768},{\"end\":53823,\"start\":53792},{\"end\":53846,\"start\":53823},{\"end\":53865,\"start\":53846},{\"end\":53887,\"start\":53865},{\"end\":54273,\"start\":54258},{\"end\":54288,\"start\":54273},{\"end\":54293,\"start\":54288},{\"end\":54606,\"start\":54592},{\"end\":54622,\"start\":54606},{\"end\":54638,\"start\":54622},{\"end\":54653,\"start\":54638},{\"end\":54668,\"start\":54653},{\"end\":55099,\"start\":55078},{\"end\":55115,\"start\":55099},{\"end\":55124,\"start\":55115},{\"end\":55135,\"start\":55124},{\"end\":55152,\"start\":55135},{\"end\":55159,\"start\":55152},{\"end\":55485,\"start\":55468},{\"end\":55506,\"start\":55485},{\"end\":55525,\"start\":55506},{\"end\":55542,\"start\":55525},{\"end\":55566,\"start\":55542},{\"end\":55579,\"start\":55566},{\"end\":55595,\"start\":55579},{\"end\":56118,\"start\":56098},{\"end\":56134,\"start\":56118},{\"end\":56305,\"start\":56285},{\"end\":56317,\"start\":56305},{\"end\":56335,\"start\":56317},{\"end\":56350,\"start\":56335},{\"end\":56366,\"start\":56350},{\"end\":56373,\"start\":56366},{\"end\":56617,\"start\":56601},{\"end\":56631,\"start\":56617},{\"end\":56643,\"start\":56631},{\"end\":56659,\"start\":56643},{\"end\":56675,\"start\":56659},{\"end\":56685,\"start\":56675},{\"end\":56698,\"start\":56685},{\"end\":56711,\"start\":56698},{\"end\":56723,\"start\":56711},{\"end\":56737,\"start\":56723},{\"end\":56749,\"start\":56737},{\"end\":56768,\"start\":56749},{\"end\":57255,\"start\":57242},{\"end\":57263,\"start\":57255},{\"end\":57280,\"start\":57263},{\"end\":57293,\"start\":57280},{\"end\":57306,\"start\":57293},{\"end\":57319,\"start\":57306},{\"end\":57591,\"start\":57576},{\"end\":57607,\"start\":57591},{\"end\":57622,\"start\":57607},{\"end\":57640,\"start\":57622},{\"end\":57656,\"start\":57640},{\"end\":57952,\"start\":57937},{\"end\":57966,\"start\":57952},{\"end\":57981,\"start\":57966},{\"end\":57997,\"start\":57981},{\"end\":58275,\"start\":58254},{\"end\":58287,\"start\":58275},{\"end\":58300,\"start\":58287},{\"end\":58312,\"start\":58300},{\"end\":58327,\"start\":58312},{\"end\":58341,\"start\":58327},{\"end\":58359,\"start\":58341},{\"end\":58377,\"start\":58359},{\"end\":58392,\"start\":58377},{\"end\":58410,\"start\":58392},{\"end\":58756,\"start\":58744},{\"end\":58777,\"start\":58756},{\"end\":58791,\"start\":58777},{\"end\":58806,\"start\":58791},{\"end\":59032,\"start\":59015},{\"end\":59052,\"start\":59032},{\"end\":59254,\"start\":59239},{\"end\":59270,\"start\":59254},{\"end\":59297,\"start\":59270},{\"end\":59309,\"start\":59297},{\"end\":59323,\"start\":59309},{\"end\":59337,\"start\":59323},{\"end\":59356,\"start\":59337},{\"end\":59698,\"start\":59683},{\"end\":59713,\"start\":59698},{\"end\":59728,\"start\":59713},{\"end\":59742,\"start\":59728},{\"end\":59762,\"start\":59742},{\"end\":60109,\"start\":60096},{\"end\":60122,\"start\":60109},{\"end\":60140,\"start\":60122},{\"end\":60162,\"start\":60140},{\"end\":60175,\"start\":60162},{\"end\":60189,\"start\":60175},{\"end\":60440,\"start\":60425},{\"end\":60458,\"start\":60440},{\"end\":60468,\"start\":60458},{\"end\":60481,\"start\":60468},{\"end\":60491,\"start\":60481},{\"end\":60908,\"start\":60896},{\"end\":60926,\"start\":60908},{\"end\":60941,\"start\":60926},{\"end\":60951,\"start\":60941},{\"end\":60966,\"start\":60951},{\"end\":60979,\"start\":60966},{\"end\":61261,\"start\":61243},{\"end\":61285,\"start\":61261},{\"end\":61696,\"start\":61676},{\"end\":61713,\"start\":61696},{\"end\":61726,\"start\":61713},{\"end\":62052,\"start\":62041},{\"end\":62070,\"start\":62052},{\"end\":62083,\"start\":62070},{\"end\":62092,\"start\":62083},{\"end\":62548,\"start\":62532},{\"end\":62564,\"start\":62548},{\"end\":62577,\"start\":62564},{\"end\":62591,\"start\":62577},{\"end\":62607,\"start\":62591},{\"end\":62622,\"start\":62607},{\"end\":62635,\"start\":62622},{\"end\":62650,\"start\":62635},{\"end\":62665,\"start\":62650},{\"end\":62684,\"start\":62665},{\"end\":62702,\"start\":62684},{\"end\":62716,\"start\":62702},{\"end\":63090,\"start\":63074},{\"end\":63116,\"start\":63090},{\"end\":63129,\"start\":63116},{\"end\":63142,\"start\":63129},{\"end\":63161,\"start\":63142},{\"end\":63173,\"start\":63161},{\"end\":63187,\"start\":63173},{\"end\":63206,\"start\":63187},{\"end\":63642,\"start\":63630},{\"end\":63655,\"start\":63642},{\"end\":63664,\"start\":63655},{\"end\":63680,\"start\":63664},{\"end\":63998,\"start\":63980},{\"end\":64020,\"start\":63998},{\"end\":64215,\"start\":64198},{\"end\":64235,\"start\":64215},{\"end\":64251,\"start\":64235},{\"end\":64571,\"start\":64558},{\"end\":64587,\"start\":64571},{\"end\":64601,\"start\":64587},{\"end\":64616,\"start\":64601},{\"end\":64933,\"start\":64919},{\"end\":64948,\"start\":64933},{\"end\":64960,\"start\":64948},{\"end\":64980,\"start\":64960},{\"end\":64993,\"start\":64980},{\"end\":65509,\"start\":65492},{\"end\":65526,\"start\":65509},{\"end\":65788,\"start\":65776},{\"end\":65804,\"start\":65788},{\"end\":65816,\"start\":65804},{\"end\":65829,\"start\":65816},{\"end\":65841,\"start\":65829},{\"end\":65870,\"start\":65841},{\"end\":65880,\"start\":65870},{\"end\":65896,\"start\":65880},{\"end\":65910,\"start\":65896},{\"end\":65924,\"start\":65910},{\"end\":66283,\"start\":66268},{\"end\":66299,\"start\":66283},{\"end\":66314,\"start\":66299},{\"end\":66625,\"start\":66618},{\"end\":66635,\"start\":66625},{\"end\":66653,\"start\":66635},{\"end\":66668,\"start\":66653},{\"end\":66903,\"start\":66885},{\"end\":66919,\"start\":66903},{\"end\":67115,\"start\":67100},{\"end\":67124,\"start\":67115},{\"end\":67143,\"start\":67124},{\"end\":67160,\"start\":67143},{\"end\":67170,\"start\":67160},{\"end\":67463,\"start\":67453},{\"end\":67481,\"start\":67463},{\"end\":67493,\"start\":67481},{\"end\":67507,\"start\":67493},{\"end\":67522,\"start\":67507},{\"end\":67534,\"start\":67522},{\"end\":67547,\"start\":67534},{\"end\":67975,\"start\":67969},{\"end\":68172,\"start\":68162}]", "bib_venue": "[{\"end\":49327,\"start\":49194},{\"end\":49844,\"start\":49767},{\"end\":50411,\"start\":50405},{\"end\":50876,\"start\":50830},{\"end\":51266,\"start\":51191},{\"end\":51617,\"start\":51525},{\"end\":52046,\"start\":52004},{\"end\":52305,\"start\":52229},{\"end\":52677,\"start\":52613},{\"end\":53145,\"start\":53084},{\"end\":53541,\"start\":53461},{\"end\":53766,\"start\":53700},{\"end\":54256,\"start\":54148},{\"end\":54705,\"start\":54668},{\"end\":55076,\"start\":55006},{\"end\":55672,\"start\":55595},{\"end\":56096,\"start\":56056},{\"end\":56420,\"start\":56389},{\"end\":56873,\"start\":56784},{\"end\":57240,\"start\":57191},{\"end\":57574,\"start\":57522},{\"end\":57935,\"start\":57866},{\"end\":58252,\"start\":58202},{\"end\":58742,\"start\":58695},{\"end\":59070,\"start\":59052},{\"end\":59237,\"start\":59181},{\"end\":59829,\"start\":59762},{\"end\":60568,\"start\":60491},{\"end\":60894,\"start\":60841},{\"end\":61359,\"start\":61285},{\"end\":61766,\"start\":61726},{\"end\":62151,\"start\":62092},{\"end\":62530,\"start\":62486},{\"end\":63250,\"start\":63206},{\"end\":63628,\"start\":63517},{\"end\":63978,\"start\":63917},{\"end\":64316,\"start\":64267},{\"end\":64642,\"start\":64616},{\"end\":65067,\"start\":64993},{\"end\":65490,\"start\":65357},{\"end\":65774,\"start\":65719},{\"end\":66352,\"start\":66314},{\"end\":66616,\"start\":66546},{\"end\":66883,\"start\":66864},{\"end\":67098,\"start\":67045},{\"end\":67614,\"start\":67547},{\"end\":67967,\"start\":67895},{\"end\":68160,\"start\":68088},{\"end\":68609,\"start\":68449},{\"end\":69069,\"start\":69059},{\"end\":49908,\"start\":49846},{\"end\":51328,\"start\":51268},{\"end\":55736,\"start\":55674},{\"end\":60632,\"start\":60570},{\"end\":61420,\"start\":61361},{\"end\":62199,\"start\":62153},{\"end\":65128,\"start\":65069}]"}}}, "year": 2023, "month": 12, "day": 17}