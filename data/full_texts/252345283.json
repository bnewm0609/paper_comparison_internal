{"id": 252345283, "updated": "2023-11-14 14:35:32.71", "metadata": {"title": "Visible Light Integrated Positioning and Communication: A Multi-Task Federated Learning Framework", "authors": "[{\"first\":\"Tiankuo\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Sicong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Xiaojiang\",\"last\":\"Du\",\"middle\":[]}]", "venue": "IEEE Transactions on Mobile Computing", "journal": "IEEE Transactions on Mobile Computing", "publication_date": {"year": 2023, "month": 12, "day": 1}, "abstract": "Recently, visible light positioning and visible light communication are becoming a promising technology for integrated sensing and communication. However, the isolated design of positioning and communication has limited the system efficiency and performance. In this article, a visible light integrated positioning and communication (VIPAC) framework is formulated, in which the positioning task for the sensing service and the channel estimation task for the communication service are integrated into a unified architecture. First, a multi-task learning architecture, which is composed of a sparsity-aware shared network and two task-oriented sub-networks, is proposed to fully exploit the inherent sparse features of visible light channels, and achieve mutual benefits between the two tasks. The depth of the shared network can be adaptively adjusted to extract the optimal shared features, and the two sub-networks are further optimized for the two tasks, respectively. Moreover, the emerging federated learning technique is introduced to devise a multi-user cooperative VIPAC scheme, which further improves the generalization ability in spatiotemporally nonstationary environments while preserving data privacy. It is shown by theoretical analysis and simulation results that, the proposed scheme can significantly improve the performance of positioning and channel estimation in spatiotemporally nonstationary environments compared with existing benchmark schemes.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tmc/WeiLD23", "doi": "10.1109/tmc.2022.3207164"}}, "content": {"source": {"pdf_hash": "7010b158c15b1e0cc114889600c7f8f27ce90b67", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "218710f299d4ba33cc66bb185b46de8d4f5da768", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/7010b158c15b1e0cc114889600c7f8f27ce90b67.txt", "contents": "\nVisible Light Integrated Positioning and Communication: A Multi-Task Federated Learning Framework\n\n\nTiankuo Wei 0000-0002-1770-8836\nSenior Member, IEEESicong Liu 0000-0002-5710-0446\nFellow, IEEEXiaojiang Du 0000-0003-4235-9671\n\nDepartment of Information and Com-munication Engineering\nSchool of Informatics\nXiamen University\n361005XiamenChina\n\n\nDepartment of Electrical and Computer Engi-neering\nStevens Institute of Technology\n07030HobokenNJUSA\n\nVisible Light Integrated Positioning and Communication: A Multi-Task Federated Learning Framework\n5D788434A9B650BB7D411A55B3D34AB910.1109/TMC.2022.3207164received 21 June 2022; revised 13 August 2022; accepted 13 September 2022. Date of publication 16 September 2022; date of current version 3 November 2023.Integrated sensing and communicationvisible light positioningvisible light communicationfederated learningmulti-task learningchannel estimationsparse learning\nRecently, visible light positioning and visible light communication are becoming a promising technology for integrated sensing and communication.However, the isolated design of positioning and communication has limited the system efficiency and performance.In this article, a visible light integrated positioning and communication (VIPAC) framework is formulated, in which the positioning task for the sensing service and the channel estimation task for the communication service are integrated into a unified architecture.First, a multi-task learning architecture, which is composed of a sparsity-aware shared network and two task-oriented sub-networks, is proposed to fully exploit the inherent sparse features of visible light channels, and achieve mutual benefits between the two tasks.The depth of the shared network can be adaptively adjusted to extract the optimal shared features, and the two sub-networks are further optimized for the two tasks, respectively.Moreover, the emerging federated learning technique is introduced to devise a multi-user cooperative VIPAC scheme, which further improves the generalization ability in spatiotemporally nonstationary environments while preserving data privacy.It is shown by theoretical analysis and simulation results that, the proposed scheme can significantly improve the performance of positioning and channel estimation in spatiotemporally nonstationary environments compared with existing benchmark schemes.\n\nINTRODUCTION\n\nW ITH the explosive increase of mobile devices and the requirements of various emerging applications and services, including sensing, positioning, communication, and computing, more efficient resource utilization and stronger support of diversified services are required in the next-generation beyond 5G and 6G networks [1], [2], [3].To address this issue, an emerging wireless technique paradigm of integrated sensing and communication (ISAC), which co-designs the sensing and communication systems in order to achieve mutual benefits between them, has drawn much attention from academia and industry [4], [5].Specific ISAC techniques can be devised to improve the spectral and energy efficiency by sharing the spectrum resources and hardware implementations between sensing tasks such as target detection and navigation, and various wireless communication tasks.Meanwhile, great performance potentials can be attained from the inherent mutual benefits between the sensing and communication tasks by designing an effective ISAC mechanism [6].\n\nIn recent years, visible light positioning (VLP) and visible light communication (VLC) have been envisioned as promising candidates for the applications of indoor broadband access and high-precision positioning, because of the high positioning accuracy of VLP [7], [8], [9], [10], [11], ultrawide spectrum of VLC, as well as many attractive characteristics such as the cost-effective hardware implementation, electromagnetic-interference-free transmission, privacy protection ability, and unregulated spectrum [12], [13], [14], etc.\n\nHowever, the VLP and VLC systems are usually designed separately.The potential mutual benefits between positioning and communication have not been utilized effectively.Lack of a unified ISAC design of VLP and VLC has limited the potentials of the resource utilization efficiency and the communication and positioning performance.On the other hand, exploiting the sparse features of visible light channels with only a few dominant channel paths, some compressed sensing (CS) based methods are investigated for visible light positioning [15] or channel estimation [16], [17].However, in harsh conditions such as insufficient measurement data, intensive background noise, large sparsity level, or complicated sparse structure, the performance of existing CS-based algorithms still needs to be further improved because the inherent sparse channel features remain to be extracted and fully exploited [18], [19], [20].Thus, it is necessary to introduce the sparse learning technique, which can effectively learn the sparse characteristics from complicated distributions, to further improve the performance of channel estimation and positioning simultaneously.\n\nTherefore, in order to solve the above-mentioned problems of state-of-the-art VLC and VLP systems, it is essential to design an ISAC framework that integrates the functionalities of both accurate positioning and efficient communication for visible light networks.To this end, we will design such an ISAC framework in this paper, which intelligently learns the sparse characteristics of visible light channels, with satisfactory generalization and adaptation capability in spatiotemporally nonstationary environments.\n\nOne of the essentials in designing an ISAC framework is to achieve mutual benefits between the positioning and communication tasks.Multi-task learning (MTL) [21] is an emerging machine learning paradigm that jointly learns multiple related tasks, aiming at sharing the domain-specific knowledge between different tasks to improve the performance of all tasks [22], [23].Thus, it is very promising to introduce MTL to effectively explore the potential mutual benefits between the visible light positioning and communication tasks to improve the performance of both the two tasks.\n\nHence, in this paper, we propose an MTL-based ISAC framework, called visible light integrated positioning and communication (VIPAC), where the visible light channel state information is utilized to facilitate the location precision for the VLP task, and vice versa, the positioning information also brings about benefits to channel estimation for the VLC task.Specifically, an MTL-based deep neural network, which is composed of a sparsity-aware shared network and two task-oriented sub-task networks, is devised.The depth of the sparsity-aware shared network can be adaptively adjusted to extract the optimal shared sparse features and most mutual benefits between the two tasks of channel estimation and positioning.The structures of the two task-oriented sub-networks can be further optimized specifically for the two tasks, respectively, to further enhance the learning ability of the two tasks.Besides, the pilot subcarriers are also shared to implement the two tasks simultaneously, which greatly saves the spectrum resources and improves spectral efficiency.\n\nFurthermore, to avoid the degradation of the performance of positioning and channel estimation caused by the spatiotemporal nonstationary property of the visible light channel, the emerging technique of federated learning (FL) [24], [25] can be introduced to improve the generalization performance of the proposed MTL-based VIPAC scheme in complicated time-varying and/or spatially variant environments.FL is an emerging distributive learning paradigm that allows many data owners to train a global neural network model cooperatively without sharing local training data for the purpose of privacy protection [26], [27], [28].Thus, in the multi-task federated learning (MTFL) framework proposed in this paper, each user equipment (UE) plays the role of an intelligent agent, which can collect samples from its corresponding spot to build a local dataset and train a local model, while many UE agents can combine the weights of the local models to cooperatively train a global model for positioning and channel estimation tasks.\n\nSpecifically, in the MTFL framework, the weights of the global model at the central server are updated iteratively via multiple rounds of communications between the central server and the UE agents.The weights of the local models at the UE agents are trained by the local datasets that are updated over time, and the local weights are average combined at the central server to generate the global weights.Thus, the generalization ability towards temporal variation and the adaptability to time-varying channels can be improved for the proposed global model.Moreover, a cellular cluster architecture is devised to facilitate the training of the MTFL-based model and to improve the spatial generalization ability of the proposed VIPAC scheme in spatially nonstationary environments.The entire targe area of interest can be divided into several cellular clusters, with each cellular cluster containing a certain number of identically or similarly deployed hexagonal cells, where a light emitting diode (LED) lamp is placed in the center of each cell.Since the MTFL-based global model for the cellular cluster architecture is jointly trained using many datasets gathered from different UE agents at variant spatial locations, the trained model is more likely to be applicable in variant environments.In addition, thanks to the FL-based mechanism, the local dataset of a UE agent will not be shared with other UE agents or the central server, which protects the data and location privacy of the users very well.\n\nConsequently, a novel framework of VIPAC in the ISAC technical regime is proposed in this paper, which shares the spectrum and hardware resources for the resource-efficient joint positioning and channel estimation tasks.In the VIPAC framework, a depth-adaptive MTL-based network architecture is devised, which is used to learn the mutual beneficial features between the positioning and channel estimation tasks via a sparsity-aware shared network, and to achieve a better performance via optimizing the two task-oriented subnetworks.The MTFL framework is further formulated to improve the spatiotemporal generalization capability of the global model, and meanwhile to protect the location and data privacy of the users.Theoretical analysis and simulation results have shown that, the proposed scheme can significantly improve the performance of both the positioning and channel estimation tasks in spatiotemporally nonstationary environments, and outperforms existing benchmark schemes.To summarize, the main contributions of this paper are list as follows.\n\nAn ISAC framework called VIPAC is formulated for joint visible light positioning and communication tasks, where channel estimation and positioning are integrated into the unified framework, and a practically applicable and spatiotemporally migratable cellular cluster architecture is devised.An MTL-based neural network architecture is proposed to exploit the mutual benefits between the positioning and communication tasks, where the depth of the shared network can be adaptively adjusted to learn the optimal shared sparse features of the visible light channel, while the two task-oriented sub-networks are further optimized respectively.An MTFL framework is formulated for the multiuser cooperative VIPAC scheme to further improve the spatial and temporal generalization ability of the global model in the complicated spatiotemporally nonstationary environments, while preserving the privacy and confidentiality of the data and location of the users.The remainder of this paper is structured as follows.The system model of the proposed VIPAC framework is described in Section 2. The proposed MTL-based network architecture and the MTFL-based scheme are introduced in Sections 3 and 4, respectively.The performance bound of the proposed MTL-based scheme and the convergence of the proposed MTFL framework are theoretically analyzed in Section 5. Simulation results with discussions are reported in Section 6, followed by the conclusions in Section 7.\n\n\nVISIBLE LIGHT INTEGRATED POSITIONING AND COMMUNICATION FRAMEWORK\n\nIn this section, we first describe the typical system model and visible light channel model of the indoor positioning and communication system.Then, we introduce the VIPAC framework, including the design of the signal model for joint positioning and channel estimation, as well as the proposed multi-lamp cellular cluster architecture.\n\n\nSystem Model of Indoor Visible Light Positioning and Communication\n\nA typical multi-LED orthogonal frequency division multiplexing (OFDM)-based VLC system using intensity modulation/direct detection (IM/DD) is shown in Fig. 1a.The input data bits are first mapped to M-ary quadrature amplitude modulation (QAM) symbols, and the pilot signals for channel estimation are inserted.To obtain real-valued signals required by VLC transmission, Hermitian symmetry is implemented before imposing the inverse fast Fourier transform (IFFT) processing.After adding the cyclic prefix (CP), the resulting digital signal is converted to an analog electric signal by the digital-to-analog converter (DAC).Then, a biased direct current (DC) is added to the electric signal to generate a positive current that drives the LED light intensity with an electro-optical conversion efficiency denoted by a. Finally, the modulated LED light is emitted for simultaneous data transmission and illumination purposes.\n\nIn the indoor environments, the optical wireless propagation channel between the LED and the photodetector (PD) at the receiver is composed of the line-of-sight (LOS) component and the non-line-of-sight (NLOS) component [29].The length-L channel impulse response (CIR) vector between the tth LED located at the coordinate of a \u00f0t\u00de \u00bc \u00bda \u00f0t\u00de\n\nx ; a \u00f0t\u00de y ; a \u00f0t\u00de z and the PD at the receiver located at c \u00bc \u00bdc x ; c y ; c z can be expressed as Usually, it is assumed that the LEDs follow a Lambertian emission pattern [29], and the nth element of the LOS CIR vector can be expressed as\nh \u00f0t\u00de \u00bc h \u00f0t\u00de LOS \u00fe h \u00f0t\u00de NLOS ;(1)\u00bdh \u00f0t\u00de LOS n \u00bc \u00f0m \u00fe 1\u00deA PD cos m \u00f0' \u00f0t\u00de \u00de cos \u00f0c \u00f0t\u00de \u00degT s 2pd \u00f0t\u00de 2 rect c \u00f0t\u00de c FOV d nt s \u00c0 d \u00f0t\u00de c ;(2)\nwhere d \u00f0t\u00de is the distance between the tth LED and the PD.\n\nA PD is the effective geometrical area of the PD.T s and g are the gain of the optical filter and the optical concentrator of the PD, respectively.' \u00f0t\u00de and c \u00f0t\u00de are the angles of irradiance and incidence with respect to the normal direction, respectively.rect\u00f0\u00c1\u00de is the rectangular function, and c FOV is the field-of-view (FOV) angle of the PD.d\u00f0\u00c1\u00de is the Dirac delta function, and t s is the sampling period.c is the light speed.m is the Lambertian order given by m \u00bc \u00c0ln2=ln\u00f0cos\u00f0' 1=2 \u00de\u00de, where ' 1=2 is the half-power angle of the LED.Since most of the power of the NLOS link is concentrated on the first reflected light [29], the NLOS link is modeled as a diffusion channel of the first reflection, where the wall can be segmented into many small surfaces regarded as reflection elements.Thus, the nth element of the NLOS CIR vector is given by  r is the average diffuse reflectance of the wall.dA walls is a reflective area infinitesimal on the wall.\n\nIt is revealed that the visible light channel modeled in (1) is in fact a sparse multipath channel model [17], [19].This means that most of the energy is concentrated on only a few dominant taps in the CIR vector h \u00f0t\u00de , while the other taps are zero or relatively much smaller.This sparse property of the visible light channel can be fully exploited in the proposed MTL-based framework to facilitate shared sparse feature extraction between positioning and channel estimation tasks for a better joint performance.\n\nAt the receiver, the received visible light signal is first converted to an electric signal via the PD with a responsivity of R p .After passing through a series of the analog-to-digital converter (ADC), CP removal, FFT processing, frequencydomain equalization, and M-QAM demapping modules, the recovered output data bits can be obtained.Meanwhile, the received pilot subcarriers can be extracted for joint positioning and channel estimation using the proposed MTL network, and afterwards the estimated CIR can be utilized for equalization.\n\nIn a traditional VLP system, the position is usually estimated by some certain metrics of the received visible light signal, such as received signal strength and angle of arrival [8].In the proposed VIPAC framework, the position coordinates and the CIR can be simultaneously obtained from the received pilot signal using the proposed MTL network.The detailed signal model of VIPAC is introduced in next subsection.\n\n\nVIPAC Framework Formulation\n\nIn this subsection, the OFDM signal model and the pilot signal design for the VIPAC are first introduced, followed by the multi-lamp cellular cluster architecture devised to facilitate federated learning.\n\n\nSignal Model for VIPAC\n\nAn OFDM symbol transmitted by the tth LED is composed of the length-N CP CP sequence and the length-N OFDM data block x \u00f0t\u00de , which can be expressed as By stacking all the channel measurement vectors corresponding to the normalized pilots from all the N t LEDs, and meanwhile stacking all the CIR vectors between the N t LEDs and the receiver, the channel measurement model in the VIPAC framework can be reformulated as\nxu \u00bc F L h \u00fe wL ;(8)\nwhere u \u00bc \u00bd\u00f0u \u00f01\u00de \u00de T ; \u00f0u \u00f02\u00de \u00de T ; . . .; \u00f0u \u00f0N t \u00de \u00de T T is the stacked channel measurement vector, while h \u00bc \u00bd\u00f0h \u00f01\u00de \u00de T ; \u00f0h \u00f02\u00de \u00de T ; . . .; \u00f0h \u00f0N t \u00de \u00de T T is the stacked CIR vector, and wL is the stacked background noise vector.The matrix F L is called the observation matrix, which is a block diagonal matrix given by\nF L \u00bc F \u00f01\u00de p \u00c1 \u00c1 \u00c1 0 . . . . . . . . . 0 \u00c1 \u00c1 \u00c1 F \u00f0N t \u00de p 2 6 6 6 4 3 7 7 7 5 N t N p \u00c2N t L :(9)\nAs described above, the indoor visible light channel is usually concentrated on only a few taps in the delay domain.Therefore, the CIR vector is usually a sparse vector [17], [19].Exploiting the inherent sparsity of the visible light channel, the stacked CIR vector h can be recovered from the stacked channel measurement vector u via classical CSbased algorithms and the emerging sparsity-aware deep-learning-based methods.Moreover, according to (1), (2), and (3), the channel measurement data in u contains plenty of position-related information, so it can also be utilized to estimate the location coordinate c of the receiver.To this end, an MTL-based architecture is proposed to extract the shared features beneficial for both channel estimation and positioning, which is elaborated in detail in Section 3.\n\n\nMulti-Lamp Cellular Cluster Architecture Design for Multi-User Cooperative VIPAC\n\nTo improve the spatial generalization ability of the learnt MTL-based network, a multi-lamp cellular cluster architecture is designed in this paper, which provides a systematic and spatially transferrable VIPAC model enabling the proposed MTFL framework.Specifically, as illustrated in Fig. 1c, the VLC coverage area in an arbitrary indoor environment with many LEDs can be divided into multiple hexagonal VLC cells as is done in mobile cellular networks [30], [31], where each VLC cell contains an LED in the center on the ceiling.A certain number of VLC cells constitute a multi-lamp cellular cluster.Since the light intensity decays with the propagation distance, the coverage of an LED is mostly limited in the cell and the same subcarriers can be reused in different cellular clusters, which is like the frequency reuse technology in mobile cellular networks.As a typical example, the cluster pattern with each cellular cluster consisting of four cells is illustrated in Fig. 1c, where the orange and blue clusters are two adjacent clusters reusing the same subcarriers.With the help of the cellular cluster architecture, different clusters have a property of approximate spatial equivalence between each other, because they can be approximately transferred to each other by some simple operations such as flipping, rotating, and shifting.In other words, if you look at two different clusters from some specific spatial perspective, they look quite similar to each other.This makes it very convenient for a universally effective global model to be learnt in the MTFL framework.In fact, the original positioning problem of estimating the absolute coordinate can be transferred to another equivalent problem of estimating the relative coordinate with respect to the cluster center, which is universally applicable for different clusters.Specifically, as denoted by the orange and blue dots in Fig. 1c, the absolute coordinates of the two pairs of different positions in the two clusters are different.However, their relative coordinates to their corresponding cluster center are the same.When the relative coordinates are estimated correctly, the absolute coordinates can be easily obtained with the information of the absolute LED positions in the corresponding cluster.Hence, by converting the original absolute positioning problem to an equivalent relative positioning problem using the cellular cluster architecture, a generalized global VIPAC model effective for different clusters in various kinds of environments and scenarios can be trained in the MTFL framework, which is an effective way to improve the spatial generalization ability, as described in detail in Section 4.\n\n\nSPARSITY-AWARE MULTI-TASK LEARNING FOR ACCURATE CHANNEL AND POSITION ESTIMATION\n\nIn this section, we introduce the proposed sparsity-aware MTL-based network for accurate channel and position estimation in the VIPAC system.As illustrated in Fig. 2, it is composed of a sparsity-aware depth-adaptive shared network and two task-oriented sub-networks for channel and position estimation, respectively.\n\n\nMulti-Task Learning Based Network Architecture\n\nDifferent from the traditional single-task learning, MTL is aimed at training a joint model for multiple related tasks so that the domain-specific knowledge of each task can be harnessed to improve the generalization ability of the joint model for all the tasks [21].Data augmentation is achieved by aggregating the training data across all the tasks to learn a more accurate model for each task, which can better exploit the domain-specific knowledge and reduce the data amount required for satisfactory performance.Meanwhile, with more data from different tasks, MTL can extract the inherent mutual benefits and provide a more robust and more general representation for these tasks, which leads to a lower risk of overfitting for each task [22].\n\nHence, an MTL-based network is devised in this paper, which can extract the shared features of the channel sparsity for both the tasks of channel estimation and positioning, as illustrated in Fig. 2. By extracting the inherent sparse features of the visible light propagation channel, the accuracy of channel estimation can be improved.Meanwhile, as shown in ( 1), (2), and (3), the sparse feature of the channel reflects the locations of the dominant taps in the CIR, so it contains the information of the distance between the PD and the LED as well as the surrounding environment, which can be utilized for positioning.Therefore, the sparse features of the channel can be shared between positioning and channel estimation to improve the performance of both the two tasks.\n\nSpecifically, in the proposed MTL-based network architecture, a sparsity-aware shared network is devised to extract the shared sparse features between the two tasks, and meanwhile to obtain a coarse estimation of the CIR.To find the optimal equilibrium point of the shared sparse feature, the depth of the shared network can be flexibly and adaptively adjusted, and an optimal performance tradeoff between the two subtasks can be achieved for different scenarios and QoS requirements.Afterwards, the shared representation extracted out of the shared network is then fed into the two task-oriented sub-networks for channel and position estimation, respectively.The shared network is jointly trained to achieve the best mutual benefits between the two tasks, and the two sub-networks can be refined and further optimized respectively to improve the performance of either task, which is described in detail as follows.\n\n\nSparsity-Aware Depth-Adaptive Shared Network\n\nTo extract shared sparse features, in this paper, a sparsityaware deep-unfolding neural network inspired by the classical iterative sparse recovery algorithm of approximate message passing (AMP) is utilized as the shared network, as illustrated by the green dashed block in Fig. 2. The shared network is utilized to obtain a coarse estimation of the sparse pattern of the CIR vector, which is an important shared feature for channel estimation and positioning.In the structure of the sparsity-aware shared network, each layer mimics an iteration of the AMP algorithm, and the operations of the ith layer are given by\n\u0125S;i\u00fe1 \u00bc h\u00f0 \u0125S;i \u00fe B S;i v S;i ; s S;i ; u S;i \u00de;(10)v S;i\u00fe1 \u00bc u \u00c0 F L \u0125S;i\u00fe1 \u00fe b S;i\u00fe1 v S;i ;(11)\nwhere \u0125S;i \u00bc \u00bd\u00f0\n\u0125\u00f01\u00de S;i \u00de T ; \u00f0 \u0125\u00f02\u00de S;i \u00de T ; . . . ; \u00f0 \u0125\u00f0N t \u00de S;i \u00de T T\ndenotes the estimated CIR vector of the ith layer, and v S;i is the residual measurement error vector of the ith layer.B S;i denotes the layer-dependent learnable weights, which is a parametric matrix converted from the observation matrix F L .The soft threshold shrinkage function h\u00f0\u00c1\u00de is defined element-wise with its nth element given by h r S;i ; s S;i ; u\nS;i \u00c0 \u00c1 \u00c2 \u00c3 n , sgn r S;i \u00c2 \u00c3 n max r S;i \u00c2 \u00c3 n \u00c0 u S;i s S;i ; 0 ;(12)\nwhich accepts three parameters as the input, including the noisy measurement vector r S;i \u00bc \u0125S;i \u00fe B S;i v S;i , the standard deviation of the residual error s S;i \u00bc kv\nS;i k 2 = ffiffiffiffiffiffiffiffiffiffiffi ffi N t N p p\n, and the learnable threshold parameter u S;i .By exploiting h\u00f0\u00c1\u00de as a sparsity inducer, the noisy component with small amplitude in the noisy measurement vector r S;i can be eliminated, while the dominant nonzero elements larger than threshold are kept with an amplitude shrinkage imposed thereon.Thus, the sparse vector \u0125S;i\u00fe1 can be obtained.The Onsager correction item b S;i\u00fe1 v S;i aims to make the noise component in r S;i obey the Gaussian distribution with the standard deviation s S;i [32], where b S;i\u00fe1 is calculated by\nb S;i\u00fe1 \u00bc 1 N t N p X N t L n\u00bc0 @ h\u00f0r S;i ; s S;i ; u S;i \u00de \u00c2 \u00c3 n @ r S;i \u00c2 \u00c3 n :(13)\nLet us denote the output of the shared network as \u0125S;N I , which can be regarded as a coarse estimate of the stacked CIR vector h, with N I being the depth of the shared network.It is more important for the shared network to acquire the sparse features reflected by the dominant taps of the CIR rather than the other small-scale elements, because taking too many other redundant features into consideration might introduce noise components and interferences that cause performance degradation to positioning accuracy.The depth of the shared deep-unfolding sparsity-aware network N I can be regarded as a critical hyper-parameter that determines the extent to which the channel sparse features are extracted.Therefore, it is necessary to find the optimal depth of the shared network, so that the performance of both positioning and channel estimation can be simultaneously optimized with an optimal tradeoff.The network depth can be flexibly and adaptively adjusted in the training process, which can be determined by the loss functions of the two tasks as explained in detail in Section 3.4.\n\n\nTask-Oriented Sub-Networks for Channel Estimation and Positioning\n\nTo further improve the performance of channel estimation and positioning, two task-oriented sub-networks are specially designed for the two tasks, respectively, as illustrated by the blue and yellow dashed blocks in Fig. 2.\n\nIn the channel estimation sub-network, an essential module is introduced in the deep-unfolding network to further improve the channel estimation accuracy especially in harsh conditions, such as intensive background noise and insufficient pilot measurements.Different from the soft threshold shrinkage h\u00f0\u00c1\u00de used in the shared network, an mean squared error (MSE)-optimal denoiser D BG \u00f0\u00c1\u00de with a zero-mean Bernoulli-Gaussian (BG) prior is introduced to eliminate the noise component in the noisy measurement vector r C;l more effectively.Assuming that h is a sparse vector following a prior of an i.i.d.BG distribution, whose probability density function (PDF) of the nth elements is given by\np\u00f0h n ; K N t L ; s 2 h \u00de \u00bc 1 \u00c0 K N t L d\u00f0h n \u00de \u00fe K N t L N h n ; 0; s 2 h \u00c0 \u00c1 ;(14)\nwhere K denotes the sparsity level, and s 2 h is the variance of the nonzero elements of h, and then the noisy measurement vector r C;l of the lth layer is given by\nr C;l \u00bc h \u00fe e;(15)\nwhere e denotes the AWGN vector with the variance of s 2 C;l .To obtain the channel estimate \u0125n \u00bc E\u00bdh n j\u00bdr C;l n , the BGprior denoiser, which is MSE-optimal, is given by\n\u0125n \u00bc \u00bdr C;l n 1 \u00fe s 2 C;l s 2 h 1 \u00fe N t L\u00c0K K N \u00f0\u00bdr C;l n ;0;s 2 C;l \u00de N \u00f0\u00bdr C;l n ;0;s 2 C;l \u00fes 2 h \u00de :(16)\nTo turn the denoiser into a learnable function, we set the learnable parameters C;l \u00bc \u00bdu C;l;1 ; u C;l;2 as u C;l;1 \u00bc s 2 h and u C;l;2 \u00bc log\u00f0 N t L\u00c0K K \u00de.Then, the BG-prior denoiser is then element-wise given by\nD BG r C;l ; s C;l ; C;l \u00c0 \u00c1 \u00c2 \u00c3 n \u00bc \u00bdr C;l n 1 \u00fe s C;l 2 u C;l;1 1 \u00fe ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 \u00fe u C;l;1 s C;l 2 r exp u C;l;2 \u00c0 \u00bdr C;l 2 n 2 s C;l 2 \u00fes C;l 4 =u C;l;1 \u00f0 \u00de :\n(17) Compared with the soft threshold shrinkage function, the MSE-optimal denoiser can accurately estimate the stacked CIR vector h thanks to its robust ability of denoising, especially in harsh conditions.Since the channel sparsity is unavailable before channel estimation, the network depth should be adjustable and adaptive to variant channel conditions.Therefore, in the training stage, the number of layers of the channel estimation sub-network, i.e., N L , is also determined by the loss function, which is introduced in detail in Section 3.4.\n\n\nAlgorithm 1. Multi-Task Learning Based Joint Channel and Position Estimation Algorithm (Training Stage)\nInput: 1) Training dataset V V V V V V V \u00bc fu d ; h d ; c d g D d\u00bc1\nwith D samples (each data sample is composed of a stacked channel measurement vector u d , and the corresponding ground-truth stacked CIR vector h d and position coordinate c d ) 2) Observation matrix F L 1: Initialize i 0, v S;0 u, \u0125S;0 0 2: repeat 3: Initialize learnable parameters of ith layer: B S;i F T L , u S;i 1 4: Compute the noisy measurement vector r S;i \u00bc \u0125S;i \u00fe B S;i v S;i and the standard deviation of the residual error s S;i 5: Obtain the estimated CIR vector \u0125S;i\u00fe1 and the residual measurement error vector vS;i\u00fe1 by ( 10), ( 11), (12), and (13) 6: Perform Inner-Algorithm (a) procedure to determine the optimal number of layers for the channel estimation subnetwork 7: Go to next layer i\ni \u00fe 1 8: until L\u00f0Q Q Q Q Q Q Q \u00bdi;N L ; V V V V V V V\u00de > L\u00f0Q Q Q Q Q Q Q \u00bdi\u00c01;N L ; V V V V V V V\u00de 9:\nSet the optimal number of layers as  (18), and update fB S;i ; u S;i g, fB C;l ; C;l g and\nN I i \u00c0 1 Output: Trained parameters Q Q Q Q Q Q Q, including Q Q Q Q Q Q Q S \u00bc fB S;i ; u S;i g N I \u00c01 i\u00bc0 , Q Q Q Q Q Q Q C \u00bc fB C;l ; C;l g N L \u00c01 l\u00bc0 , and Q Q Q Q Q Q Q P 1: Inner-Algorithm (aQ Q Q Q Q Q \u00bdi;l ; V V V V V V V\u00de based onQ Q Q Q Q Q Q P via backpropagation 7: Go to next layer l l \u00fe 1 8: until L\u00f0Q Q Q Q Q Q Q \u00bdi;l ; V V V V V V V\u00de > L\u00f0Q Q Q Q Q Q Q \u00bdi;l\u00c01 ; V V V V V V V\u00de 9:\nSet the optimal number of layers as N L l \u00c0 1\n\nConsidering the outstanding performance of long shortterm memory (LSTM) in learning the long-term dependency of an input sequence [33], it is employed in the positioning sub-network in this paper.LSTM is an improved recurrent neural network containing a cell state and three gates, i.e., the input gate, forget gate, and output gate.The cell state plays a role of storing the useful information of the feature extracted from the previous entries in the input sequence, and the gates play a role of the selection and rejection of the previous information.\n\nFor the positioning sub-network, the coarsely estimated stacked CIR vector \u0125S;N I , i.e., the output of the shared network, is first unstacked and reshaped into the estimated CIR matrix \u0124 \u00bc \u00bd \u0125\u00f01\u00de S;N I ; \u0125\u00f02\u00de S;N I ; . . .; \u0125\u00f0N t \u00de S;N I T with the size of N t \u00c2 L.Then, an LSTM layer consisting of N u \u00bc N p memory units is employed to extract the position-related features in the estimated CIR matrix \u0124, of which the output is fed into a tanh activation function.Afterwards, the output data with the size of N t \u00c2 N p is flattened into a vector with length of N t N p and then fed into a fully connected network, in which two hidden layers containing N n \u00bc N t L neurons are adopted to map the features extracted by the LSTM layer into the label space of the position coordinates.The two hidden layers are both followed by a rectified linear unit (ReLu) to improve the non-linear representation ability.Finally, the last hidden layer is connected to the output layer with linear connections to generate the estimated position coordinate \u0109.\n\n\nJoint Training of the Depth-Adaptive Multi-Task Learning Network\n\nTo further improve the performance of both the two tasks and find the globally optimal point for the joint loss of the two tasks, a joint training strategy for the depth-adaptive MTL-based network consisting of the cascaded shared network and the sub-networks is proposed.The detailed procedure of the training strategy is shown in Algorithm 1. Specifically, in the training stage, the training dataset\nV V V V V V V \u00bc fu d ; h d ; c d g D\nd\u00bc1 contains D pairs of ground-truth data samples, with each data sample composed of a stacked channel measurement vector u d , and the corresponding stacked CIR vector h d and position coordinate c d .The network weights\nQ Q Q Q Q Q Q \u00bc fQ Q Q Q Q Q Q S ; Q Q Q Q Q Q Q C ; Q Q Q Q Q Q Q P g to be learnt include the N I -layer shared network weights Q Q Q Q Q Q Q S \u00bc fB S;i ; u S;i g N I i\u00bc1 , the N L -layer channel estimation sub-network weights Q Q Q Q Q Q Q C \u00bc fB C;l ; C;l g N L l\u00bc1\n, and the positioning sub-network weights\nQ Q Q Q Q Q Q P .\nA joint loss function considering the normalized mean squared error (NMSE) of both the position coordinate estimate and the CIR estimate is utilized for training, which is given by\nL\u00f0Q; V\u00de \u00bc L CE \u00f0Q; V\u00de \u00fe \u00f01 \u00c0 \u00deL PE \u00f0Q; V\u00de \u00bc D X D d\u00bc1 \u0125d \u00f0u d ; Q\u00de \u00c0 h d 2 2 h d 2 2 \u00fe 1 \u00c0 D X D d\u00bc1 \u0109d \u00f0u d ; Q\u00de \u00c0 c d 2 2 c d k k 2 2 ; (18)\nwhere \u0125d \u00f0u d ; Q\u00de and \u0109d \u00f0u d ; Q\u00de denote the output of the channel estimation sub-network and the positioning subnetwork, respectively, with the input of u d and network weights of Q.The coefficient is a tradeoff factor compromising between the two loss functions of channel estimation L CE \u00f0Q; V\u00de and positioning L PE \u00f0Q; V\u00de.\n\nFor the shared network and the channel estimation sub-network, a layer-wise training manner is utilized.Specifically, the layer numbers for both the shared network and the channel estimation sub-network can adaptively change over the training process in order to find the optimal network depth.The shared network and the channel estimation sub-network are jointly trained with the positioning sub-network to optimize the performance of both positioning and channel estimation.When training the ith layer of the shared network and the lth layer of the channel estimation sub-network, all the previous layers are utilized to calculate the loss function (18).The learnable parameters are optimized via back propagation and stochastic gradient descent by minimizing the loss function, until the loss function does not decrease with the increase of the network depths, at which the iteration terminates.Finally, when the loop halts, the total layer numbers for the shared network and the channel estimation sub-network are set as N I and N L , respectively.\nL\u00f0Q Q Q Q Q Q Q \u00bdi;l ; V V V V V V V\u00de \u00bc L\u00f0ffB S;k ; u S;k g i k\u00bc1 ; fB C;n ; C;n g l n\u00bc1 ; Q Q Q Q Q Q Q P g; V V V V V V V\u00de by\nIn the inference stage, the final estimated CIR vector \u0125C;N L and the position coordinate \u0109 can be simply obtained by performing a single-trip feed-forward operation in the trained MTL-based network.\n\n\nMULTI-TASK FEDERATED LEARNING FRAMEWORK FOR MULTI-USER COOPERATIVE VIPAC\n\nIn this section, we introduce the MTFL framework, which further improves the spatiotemporal generalization ability of the multi-user cooperative VIPAC system, as illustrated in Fig.\nc d \u00bc c d abs \u00c0 c d ref \u00c0 \u00c1 F;(19)\nwhere c d , c d abs and c d ref denote the relative coordinate, the absolute coordinate, and the coordinate of the reference point, i.e., the cluster center, respectively.F is a transfer matrix describing the flipping and rotating operations, which converts the absolute coordinate c d abs in different locations to a relative coordinate c d in the local cluster with approximate spatial equivalence between different clusters.Thus, each UE agent can constitute a local dataset\nV V V V V V V r \u00bc fu d r ; h d r ; c d r g D r d\u00bc1\nconsisting of D r samples.Unlike learningbased methods using a one-shot site survey, in the proposed MTFL framework, data collection can be implemented over time, and the local dataset is accordingly updated using the latest collected samples, making it suitable for spatiotemporally nonstationary environments.\n\nSubsequently, using the collected training data, an adaptive global neural network model can be learnt.To preserve the data privacy of the UE agents and prevent from confidential data leakage in traditional centralized learning, federated learning is utilized in the proposed MTFL framework to learn the global model in a distributive manner.The training process is divided into local model training at the N r UE agents and global model aggregation at the server.The detailed procedure is summarized in Algorithm 2, and introduced as follows.\n\n\nAlgorithm 2. Multi-Task Federated Learning Based Multi-User Cooperative VIPAC (Training Stage)\nInput: Local datasets V V V V V V V 1 ; V V V V V V V 2 ; . . . ; V V V V V V V Nr 1: Initialize the global weights Q Q Q Q Q Q Q 0 2:\nfor each communication round t \u00bc 1; 2; . . .; T do 3: At the N r UE Agents: (each UE agent performs local training independently as follows) 4: Initialize the local weights Aggregate the weights The training stage of the proposed MTFL-based VIPAC algorithm consists of a series of communication rounds indexed by t over time.Within a certain communication round, the local model is trained at each UE agent independently, and then the updated local models are aggregated at the server to update the global model.Specifically, at the rth UE agent, r \u00bc 1; 2; . . .; N r , the local dataset\nQ Q Q Q Q Q Q \u00f0r\u00de t;0 Q Q Q Q Q Q Q t\u00c01 5: for each local training step s \u00bc 1; 2; . . . ; N s do 6: Sample a mini-batch V V V V V V V t;s r of size B from the local dataset V V V V V V V r 7: Calculate the local loss function L\u00f0Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 ; V V V V V V V t;s r \u00de and update the local weights Q Q Q Q Q Q Q \u00f0r\u00de t;s via (20) 8: end for 9: Upload the learnt local weights Q Q Q Q Q Q Q\u00f0r\u00deQ Q Q Q Q Q Q\u00f01\u00det;Ns ; Q Q Q Q Q Q Q\u00f02\u00det;Ns ; . . . ; Q Q Q Q Q Q Q \u00f0Nr\u00de t;Ns by (21) and broadcast Q Q Q Q Q Q Q t toV V V V V V V r \u00bc fu d r ; h d r ; c d r g D r is usedQ Q Q Q Q \u00f0r\u00de t;0 Q Q Q Q Q Q Q t\u00c01 ,Q Q Q Q Q Q\u00f0r\u00de\nt;s are updated using stochastic gradient descent (SGD) and back propagation to minimize the local loss function\nL\u00f0Q Q Q Q Q Q Q \u00f0r\u00de t;s ; V V V V V V V t;s r \u00de calculated by (18) using the mini-batch V V V V V V V t;s r of size B sampled from the local dataset V V V V V V V r , which is given by Q Q Q Q Q Q Q \u00f0r\u00de t;s \u00bc Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 \u00c0 zG \u00f0r\u00de t;s ; (20)\nwhere z is the learning rate, and\nG \u00f0r\u00de t;s \u00bc rL\u00f0Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 ; V V V V V V V t;s r \u00deQ Q Q Q Q Q Q \u00f0r\u00de t;\nNs to the server via uplink wireless transmission such as WiFi or Bluetooth, etc.\n\nAt the server, the local weights\nQ Q Q Q Q Q Q\u00f01\u00det;N s ; Q Q Q Q Q Q Q\u00f02\u00det;N s ; . . . ; Q Q Q Q Q Q Q \u00f0N r \u00de t;N s\nreceived from UE agents are aggregated to update the global weights\nQ Q Q Q Q Q Q t , which can be expressed as Q Q Q Q Q Q Q t \u00bc 1 N r X Nr r\u00bc1 Q Q Q Q Q Q Q \u00f0r\u00de t;N s :(21)\nThen, the updated global weights\nQ Q Q Q Q Q Q t are\nbroadcasted to all the N r UE agents via visible light downlink transmission.\n\nAfter receiving the updated global weights broadcast from the server, the rth UE agent replaces its local weights\nQ Q Q Q Q Q Q \u00f0r\u00de t\u00fe1;0 with Q Q Q Q Q Q Q t ,\nand then it continues to train its local model in the next communication round.With the increase of communication rounds, a spatially generalized global model can be learnt, which can cover satisfactory VIPAC service for the whole indoor scenario illuminated by the LEDs thanks to the cooperation of multiple UE agents.\n\nAs for the inference stage of the proposed MTFL-based VIPAC algorithm at the UE agents, the tasks of channel estimation and positioning can be performed in an online manner using the currently-trained global model in any communication round of the interactive and continuous training process.Specifically, when a UE agent needs to perform the task of channel estimation or positioning, the stacked channel measurement vector u measured by the UE agent in real time can be fed into the currently-trained global model stored in the UE agent with the current global weights of\nQ Q Q Q Q Q Q t ,\nand then the output of the global model is the inference of the stacked CIR vector \u0125 and the position coordinate \u0109 of the UE agent.\n\nThe proposed MTFL-based framework is adaptive to the temporal and spatial variations of the visible light channel and the realistic environments.This is because the local datasets used for training the local models of the UE agents can be updated online over time, and thus the local models can be gradually updated to adapt to the possible spatiotemporal variations in a few communication rounds.Besides, the absolute coordinates of the training samples obtained in different clusters are standardized with approximate spatial equivalence by transferring to relative coordinates.This procedure can further improve the generalization performance of the global model in different locations and variant scenarios.Consequently, a generalized and adaptive model that can adapt to spatiotemporally non-stationary environments is learnt in the MTFL-based framework for effective VIPAC tasks.\n\nMoreover, unlike traditional distributed learning, the proposed MTFL-based framework keeps the dataset of a UE agent locally accessible only.Since the training dataset contains sensitive and private information, such as locations, trajectories, and personal data, it must be protected well and isolated from public access.Compared with encryption-based secrecy-preserving schemes, federated learning has a nature of local data privacy preservation without requiring additional dedicated computing resources for secrecy protection.In the MTFL-based scheme, only the learnt model weights are transmitted between the UE agents and the server, while the local training datasets are kept local at the UE agents only.In this way, the data privacy of the UE agents can be effectively protected.\n\n\nPERFORMANCE EVALUATION AND THEORETICAL ANALYSIS\n\n\nPerformance Bounds of Positioning and Channel Estimation Accuracy\n\nFirst, we provide a theoretical analysis for the performance bounds of positioning and channel estimation accuracy.Specifically, the Cram er-Rao lower bound (CRLB) for the channel estimation and positioning tasks will be derived, which is a widely adopted theoretical lower bound for an unbiased estimator [34].\n\nFor the channel estimation task, the CIR vector h \u00f0t\u00de is estimated from the channel measurement vector u \u00f0t\u00de contaminated by the noise w\u00f0t\u00de as given by (7).To give a lower bound of the estimation error, the CRLB of channel estimation is analyzed as follows.\n\nCorollary 1. Assume that the noise w\u00f0t\u00de follows an i.i.d.\n\nGaussian distribution of N \u00f00; s 2 w I Np \u00de.For the tth LED in the VIPAC system, the asymptotical CRLB of the CIR vector h \u00f0t\u00de with length of L for channel estimation is given by\nE \u0125\u00f0t\u00de \u00c0 h \u00f0t\u00de 2 2 ! ! L N p s 2 w :(22)\nProof.Since the noise vector w\u00f0t\u00de in (7) follows the i.i.d.\n\nGaussian distribution of N \u00f00; s 2 w I Np \u00de, the PDF of u \u00f0t\u00de conditioned by the CIR vector h \u00f0t\u00de can be expressed as p u \u00f0t\u00de j h \u00f0t\u00de u \u00f0t\u00de ; h \u00f0t\u00de \u00bc 1\n2ps 2 w \u00c0 \u00c1 N p =2 exp \u00c0 1 2s 2 w u \u00f0t\u00de \u00c0 F \u00f0t\u00de p h \u00f0t\u00de 2 2 & ' :(23)\nThen, the Fisher information matrix (FIM) [34] for channel estimation in ( 7) can be obtained by the conditional PDF in (23) as\nJ h \u00bd a;b , \u00c0 E @ 2 ln\nwhere h \u00f0t\u00de a and h \u00f0t\u00de b are the ath and bth elements in the CIR vector h \u00f0t\u00de , respectively.Then, the CRLB of the unbiased estimator \u0125\u00f0t\u00de can be derived from the inverse of the FIM [34], which is given by\nE \u0125\u00f0t\u00de \u00c0 h \u00f0t\u00de 2 2 ! ! tr J \u00c01 h \u00c0 \u00c1 \u00bc s 2 w tr F \u00f0t\u00de p H F\u00f0t\u00dep \u00c01 ! : (25)\nAccording to some properties in linear algebra [35], we have\ntr F \u00f0t\u00de p H F \u00f0t\u00de p \u00c01 ! \u00bc X L i\u00bc1 \u00c01 i \u00bc L X L i\u00bc1 \u00c01 i =L ! ! L L= X L i\u00bc1 i ! \u00bc L 2 tr F \u00f0t\u00de p H F\u00f0t\u00de\nSubstituting ( 26) and ( 27) into (25), the CRLB of the channel estimation task in ( 22) can be derived.t u\n\nRemark 1.The CRLB in Corollary 1 can be achieved only if the inequation in (26) reaches equality, i.e., the N p \u00c2 L partial DFT matrix F \u00f0t\u00de p has orthogonal columns.Fortunately, since the F \u00f0t\u00de p contains L rows and N p columns of the DFT matrix F, which is a perfectly orthogonal matrix, and the pilot subcarriers are chosen with a random pattern, F \u00f0t\u00de p has approximate orthogonality.Thus, the CRLB in ( 22) can be asymptotically approached.\n\nIn the positioning task, the position coordinate is estimated from the coarsely estimated CIR vector \u0125S;N I , where the LOS component in the CIR vector usually contains the dominant information related to the locations.Since the coordinates can be determined from the distances between the PD and more than 3 LED anchors, the CRLB of the distance estimation is usually adopted as a metric of the lower bound of positioning performance [36], which is analyzed as follows.\n\nCorollary 2. Assume that the noise vector w\u00f0t\u00de in the visible light channel follows an i.i.d.Gaussian distribution of N \u00f00; s 2 w I N p \u00de.The CRLB of the estimated distance d in the positioning task is given by\nE k d \u00c0 dk 2 2 h i ! 1 N p 2ps w \u00f0m \u00fe 1\u00de\u00f0m \u00fe 3\u00deA PD gT s z m\u00fe1 2 X N t t\u00bc1 d\u00f0t\u00de2m\u00fe8 :(28)\nProof.According to (2), the dominant taps in h corresponding to the LOS component are extracted, which constitutes a LOS CIR vector h G as given by\nh G;t \u00bc \u00f0m \u00fe 1\u00deA PD cos m \u00f0' \u00f0t\u00de \u00de cos \u00f0c \u00f0t\u00de \u00degT s 2pd \u00f0t\u00de2 ; (29)\nwhere h G;t is the tth element of h G .Assuming the PD is placed horizontally with a vertical distance z from the ceiling, Equation ( 29) can be rewritten as\nh G;t \u00bc \u00f0m \u00fe 1\u00deA PD z m\u00fe1 gT s 2p d \u00f0t\u00de \u00f0 \u00de m\u00fe3 :(30)\nNote that the vector h G contains the ground-truth dominant CIR taps without noise.In practice, the estimated LOS CIR vector denoted by h w with noise, as the output of the shared network of the MTL-based network, is given by\nh w \u00bc h G \u00fe w G ;(31)\nwhere w G is the estimation error of h G in the shared network, which can be regarded as an additive noise imposed on the real vector h G , and follows i.i.d.Gaussian\n\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.\n\nof N \u00f00; s 2 G I N t \u00de.The PDF of h w conditioned by the distance d \u00bc \u00bdd \u00f01\u00de ; d \u00f02\u00de ; . . .; d \u00f0N t \u00de can be expressed as\np h w j d h w ; d \u00f0 \u00de\u00bc 1 2ps 2 G \u00c0 \u00c1 N t =2 exp \u00c0 1 2s 2 G h w \u00c0 h G k k 2 2 & ' :(32)\nThen, the FIM [34] of the distance d can be obtained by the conditional PDF in (32) as follows:\nJ d , E @ ln p h w j d h w ; d \u00f0 \u00de \u00c0 \u00c1 @d @ ln p h w j d h w ; d \u00f0 \u00de \u00c0 \u00c1 @d T \" # \u00bc E \" @h G @d @ ln p h w j d h w ; d \u00f0 \u00de \u00c0 \u00c1 @h G @h G @d @ ln p hw j d h w ; d \u00f0 \u00de \u00c0 \u00c1 @h G T # \u00bc AJ G A T ; (33)\nwhere A is an N t \u00c2 N t matrix given by\nA \u00bc @h G;1 @d \u00f01\u00de \u00c1 \u00c1 \u00c1 @h G;N t @d \u00f01\u00de . . . . . . . . . @h G;1 @d N t \u00f0 \u00de \u00c1 \u00c1 \u00c1 @h G;N t @d N t \u00f0 \u00de 0 B B B @ 1 C C C A \u00bc \u00c0 \u00f0m \u00fe 1\u00de\u00f0m \u00fe 3\u00deA PD gT s z m\u00fe1 2p d \u00f01\u00de \u00c0 \u00c1 m\u00fe4 \u00c1 \u00c1 \u00c1 0 . . . . . . . . . 0 \u00c1 \u00c1 \u00c1 d \u00f0N t \u00de \u00c0 \u00c1 m\u00fe4 0 B B B @ 1 C C C A \u00c01 ;(34)\nand J G is given by\nJ G \u00bc E @ ln p h w j d h w ; d \u00f0 \u00de \u00c0 \u00c1 @h G @ ln p h w j d h w ; d \u00f0 \u00de \u00c0 \u00c1 @h G T \" # \u00bc s 2 G I N t \u00c0 \u00c1 \u00c01 :(35)\nThen, J d can be derived, which is a diagonal matrix with the tth diagonal elements given by\nJ d \u00bd t;t \u00bc \u00f0m \u00fe 1\u00de\u00f0m \u00fe 3\u00deA PD gT s z m\u00fe1 2ps G d \u00f0t\u00de \u00f0 \u00de m\u00fe4 ! 2 :(36)\nThus, the CRLB of the unbiased estimator d in the position estimation task can be derived from the inverse of the FIM, which is given by\nE k d \u00c0 dk 2 2 h i ! tr J \u00c01 d \u00c0 \u00c1 \u00bc 2ps G \u00f0m \u00fe 1\u00de\u00f0m \u00fe 3\u00deA PD gT s z m\u00fe1 2 X N t t\u00bc1 d\u00f0t\u00de2m\u00fe8 :(37)\nAccording to (22) in Corollary 1, the variance of the elements of the estimation error w G has an approximate lower bound given by\ns 2 G ! 1 L \u00c1 L N p s 2 w \u00bc s 2 w N p :(38)\nFinally, substituting (38) into (37), the CRLB of the position estimation task in ( 28) can be derived.t u Remark 2. The CRLB in Corollary 2 is derived as a function with an argument of the distance d.Therefore, the conclusion in Corollary 2 can be utilized as a directive metric to optimize the LED deployment pattern for improving the positioning performance.\n\n\nConvergence Analysis of the MTFL Framework\n\nThe convergence guarantee of federated learning algorithms is usually challenging because the local dataset of the UE agent may not be able to fully represent the global features in spatiotemporally varying environments [37].Specifically, the proposed MTFL framework trained by some certain datasets can be modeled as a distributed optimization problem as given by min\nQ Q Q Q Q Q Q L\u00f0Q Q Q Q Q Q Q\u00de , 1 N r X N r r\u00bc1 L r \u00f0Q Q Q Q Q Q Q\u00de;(39)\nwhere\nL r \u00f0Q Q Q Q Q Q Q\u00de , E V V V V V V V t;s r $V V V V V V V r \u00bdL\u00f0Q Q Q Q Q Q Q; V V V V V V V t;s r \u00deQ Q Q Q Q Q Q t;s \u00bc 1 N r P N r r\u00bc1 Q Q Q Q Q Q Q \u00f0r\u00de t;s . According to the SGD update in (20), we have Q Q Q Q Q Q Q t;s \u00bc Q Q Q Q Q Q Q t;s\u00c01 \u00c0 1 N r X Nr r\u00bc1 G\u00f0r\u00det;s :(40)\nAlthough the UE agent jointly trains the global model weights to fit the environment, the local SGD updates are still performed in each UE agent, which leads to a locally optimal solution.The average stochastic gradients in ( 40) is a simple average over different UE agents, which may be inaccurate for the global environment.To measure the effect caused by the local inaccuracy in gradient averaging and provide a theoretical guarantee of the proposed MTFL framework, the convergence of the cooperative training algorithm as shown in Algorithm 2 is analyzed as follows.\n\nFirst, some typical assumptions on non-convex federated optimization that ensures the convergence are given as follows [37], [38].\nAssumption 1. Each local loss function L r \u00f0Q Q Q Q Q Q Q\u00de is C-smooth, i.e., krL r \u00f0Q Q Q Q Q Q Q\u00de \u00c0 rL r \u00f0Q Q Q Q Q Q Q 0 \u00dek 2 CkQ Q Q Q Q Q Q \u00c0 Q Q Q Q Q Q Q 0 k 2 , 8Q Q Q Q Q Q Q; Q Q Q Q Q Q Q 0 , 8r2\nf1; 2; . . .; N r g.Assumption 2. The variance of the gradient of the loss function is bounded by s 2 g , i.e.,\nE V V V V V V V t;s r $V V V V V V V r \u00bdkrL\u00f0Q Q Q Q Q Q Q; V V V V V V V t;s r \u00de \u00c0 rL r \u00f0Q Q Q Q Q Q Q\u00dek 2 2 s 2 g , 8Q Q Q Q Q Q Q, 8r2\nf1; 2; . . .; N r g.\n\nAssumption 3. The second moment of the gradient of the loss\nfunction is bounded by G 2 , i.e., E V V V V V V V t;s r $V V V V V V V r \u00bdkrL\u00f0Q Q Q Q Q Q Q; V V V V V V V t;s r \u00dek 2 2 G 2 , 8Q Q Q Q Q Q Q, 8r2\nf1; 2; . . .; N r g.\n\n\nSubsequently, we bound the deviation of the local model weights\nQ Q Q Q Q Q Q \u00f0r\u00de t;s from the average weights Q Q Q Q Q Q Q t;\ns using Lemma 1 as follows.Then, we analyze the convergence rate of the proposed MTFL framework in Corollary 3 using the average of the expected squared gradient norm, which is widely adopted to characterize the convergence rate [37].\n\nLemma 1.If Assumptions 1, 2, and 3 hold, it is guaranteed by Algorithm 2 that,\nE Q Q Q Q Q Q Q t;s \u00c0 Q Q Q Q Q Q Q \u00f0r\u00de t;s 2 2 ! 4z 2 N 2 s G 2 ;\n8t; 8s; 8r 2 1; 2; . . .; N r f g ;\n\nwhere G is the constant defined in Assumptions 3.\n\nProof.Consider a certain communication round t ! 1 and local step s ! 1.Based on (20) and the initialization of\nQ Q Q Q Q Q Q\u00f0r\u00de\nt;0 in Line 4 of Algorithm 2, for r 2 f1; 2; . . .; N r g, we have\nQ Q Q Q Q Q Q \u00f0r\u00de t;s \u00bc Q Q Q Q Q Q Q \u00f0r\u00de t;0 \u00c0 z X s \u00bc1 G\u00f0r\u00det; \u00bc Q Q Q Q Q Q Q t\u00c01 \u00c0 z X s \u00bc1 G\u00f0r\u00det; :(42)\nSimilarly, based on (40) and the initialization of\nQ Q Q Q Q Q Q \u00f0r\u00de t;0 in Algorithm 2, we have Q Q Q Q Q Q Q t;s \u00bc Q Q Q Q Q Q Q t;0 \u00c0 z X s \u00bc1 1 N r X N r r\u00bc1 G\u00f0r\u00det; \u00bc Q Q Q Q Q Q Q t\u00c01 \u00c0 z X s \u00bc1 1 N r X N r r\u00bc1 G\u00f0r\u00det; :(43)\nThus, we have\nE Q Q Q Q Q Q Q t;s \u00c0 Q Q Q Q Q Q Q \u00f0r\u00de t;s 2 2 ! \u00bc E z X s \u00bc1 1 N r X N r r\u00bc1 G\u00f0r\u00det; \u00c0 z X s \u00bc1 G\u00f0r\u00de t; 2 2 2 4 3 5\u00bc z 2 E X s \u00bc1 1 N r X N r r\u00bc1 G \u00f0r\u00de t; \u00c0 X s \u00bc1 G \u00f0r\u00de t; 2 2 2 4 3 5 2z 2 E X s \u00bc1 1 N r X N r r\u00bc1 G\u00f0r\u00det; 2 2 \u00fe X s \u00bc1 G \u00f0r\u00de t; 2 2 2 4 3 5 2z 2 sE X s \u00bc1 1 N r X N r r\u00bc1 G\u00f0r\u00det; 2 2 \u00fe X s \u00bc1 G \u00f0r\u00de t; 2 2 2 4 3 5 2z 2 sE X s \u00bc1 1 N r X N r r\u00bc1 G\u00f0r\u00det; 2 2 ! \u00fe X s \u00bc1 G\u00f0r\u00det; 2 2 \" # 4z 2 N 2 s G 2 ;(44)\nwhere the first three inequalities are derived from the Jensen's inequality k\nP n i\u00bc1 1 n z i k 2 2 P n i\u00bc1 1 n kz i k 2 2 ,is z \u00bc ffiffiffiffiffiffi ffi N r N s T q 1 C\n, the average of the expected squared gradient norm is bounded by\n1 TN s X T t\u00bc1 X Ns s\u00bc1 E rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 2 2 h i O 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi N r N s T p \u00fe O 4G 2 C 2 N r N s T \u00fe O Cs 2 g ffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi N r N s T p ! :(45)\nProof.Consider a certain communication round t ! 1 and local step s ! 1.With the smoothness of the local loss function based on Assumption 1, we have\nE L Q Q Q Q Q Q Q t;s \u00c0 \u00c1 \u00c2 \u00c3 E L Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c2 \u00c3 \u00fe C 2 E Q Q Q Q Q Q Q t;s \u00c0 Q Q Q Q Q Q Q t;s\u00c01 2 2 h i |fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl} T 1 \u00fe E rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 ; Q Q Q Q Q Q Q t;s \u00c0 Q Q Q Q Q Q Q t;s\u00c01 \u00c2 \u00c3 |fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl} T 2 : (46)\nBounding the Second Term T 1 .From (40), we have\nT 1 \u00bc z 2 E 1 N r X N r r\u00bc1 G\u00f0r\u00det;s 2 2 2 4 3 5 \u00bc z 2 E 1 N r X N r r\u00bc1 G\u00f0r\u00det;s \u00c0 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 2 4 3 5 \u00fe z 2 E 1 N r X N r r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c012\nwhere the second equality is derived from the basic inequality E\u00bdkzk 2\n2 \u00bc E\u00bdkz \u00c0 E\u00bdzk 2 2 \u00fe kE\u00bdzk 2 2 . Then, since E\u00bdG \u00f0r\u00de t;s \u00c0 rL r \u00f0Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 \u00de \u00bc 0, and G \u00f0r\u00de t;s \u00c0 rL r \u00f0Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01\n\u00de are independent between the UE agents, we have\nT 1 \u00bc z 2 1 N 2 r X N r r\u00bc1 E G \u00f0r\u00de t;s \u00c0 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 ! \u00fe z 2 E 1 N 2 r X Nr r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 2 4 3 5 1 N r z 2 s 2 g \u00fe z 2 E 1 N r X N r r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c012\nwhere the inequality holds based on Assumption 2.\n\nBounding the Third Term T 2 .Note that\nT 2 \u00bc \u00c0 zE rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 ; 1 N r X N r r\u00bc1 G\u00f0r\u00det;s * + \" # \u00bc \u00c0 zE E rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 ; 1 N r X N r r\u00bc1 G\u00f0r\u00det;s * + V V V V V V V t; r \u00c8 \u00c9 s\u00c01 \u00bc1 \" # \" # \u00bc \u00c0 zE rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 ; 1 N r X N r r\u00bc1 E G \u00f0r\u00de t;s V V V V V V V t; r \u00c8 \u00c9 s\u00c01 \u00bc1 h i * + \" # \u00bc \u00c0 zE rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 ; 1 N r X N r r\u00bc1 rL r Q Q Q Q Q Q Q\u00f0r\u00det;s\u00c01 * + \" # \u00bc \u00c0 z 2 E rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 2 2 \u00fe 1 N r X Nr r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 2 4 \u00c0 rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c0 1 N r X Nr r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 3 5 ;(49)\nwhere the second equality is derived by the law of iterated expectation, since\nQ Q Q Q Q Q Q \u00f0r\u00de t;s is dependent on the selection\nof samples in the mini-batches V V V V V V V t; r ; \u00bc 1; 2; . . .; s \u00c0 1, used in the previous \u00f0s \u00c0 1\u00de local steps.The last equality holds based on the basic linear algebra property, i.e., ha; bi \u00bc 1 2 \u00f0kak 2 2 \u00fe kbk 2 2 \u00c0 ka \u00c0 bk 2 2 \u00de.Substituting (48) and ( 49) into (46) yields\nE L Q Q Q Q Q Q Q t;s \u00c0 \u00c1 \u00c2 \u00c3 E L Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c2 \u00c3 \u00c0 z 2 E rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 2 2 h i \u00c0 z \u00c0 z 2 C 2 E 1 N r X N r r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 2 4 3 5 \u00fe C 2N r z 2 s 2 g \u00fe z 2 E rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c0 1 N r X N r r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c012\nNote that\nE rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c0 1 N r X Nr r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 2 4 3 5 \u00bc E 1 N r X N r r\u00bc1 rL r Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c0 1 N r X N r r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 2 4 3 5 \u00bc E X N r r\u00bc1 1 N r rL r Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c0 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 2 4 3 5 E 1 N r X N r r\u00bc1 rL r Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c0 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 \" # C 2 N r X N r r\u00bc1 E Q Q Q Q Q Q Q t;s\u00c01 \u00c0 Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 ! 4z 2 N 2 s G 2 C 2 ;(51)\nwhere the first inequality holds based on the Jensen's\ninequality k P n i\u00bc1 1 n z i k 2 2 P n i\u00bc1 1 n kz i k 2 2\n, the second inequality comes from the smoothness in Assumption 1, and the last inequality is obtained from Lemma 1.\n\nSubstituting (51) to (50) and rearrange the terms, we have\nz 2 E rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 2 2 h i \u00c0 z \u00c0 z 2 C 2 E 1 N r X Nr r\u00bc1 rL r Q Q Q Q Q Q Q \u00f0r\u00de t;s\u00c01 2 2 2 4 3 5 \u00fe C 2N r z 2 s 2 g \u00fe E L Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c2 \u00c3 \u00c0 E L Q Q Q Q Q Q Q t;s \u00c0 \u00c1 \u00c2 \u00c3 \u00fe 2z 3 N 2 s G 2 C 2 E L Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 \u00c2 \u00c3 \u00c0 E L Q Q Q Q Q Q Q t;s \u00c0 \u00c1 \u00c2 \u00c3 \u00fe 2z 3 N 2 s G 2 C 2 \u00fe C 2N r z 2 s 2 g ;(52)\nwhere the second inequality holds since 0 < z 1 C .Subsequently, summing (52) over s 2 f1; 2; . . .; N s g and t 2 f1; 2; . . .; T g yields z 2\nX T t\u00bc1 X Ns s\u00bc1 E rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 2 2 h i E L Q Q Q Q Q Q Q 1;0 \u00c0 \u00c1 \u00c2 \u00c3 \u00c0 E L Q Q Q Q Q Q Q T;N s \u00c0 \u00c1 \u00c2 \u00c3 \u00fe 2z 3 G 2 C 2 N 3 s T \u00fe z 2 s 2 g CN s T 2N r L Q Q Q Q Q Q Q 0 \u00c0 \u00c1 \u00c0 L Q Q Q Q Q Q Q \u00c3 \u00f0 \u00de\u00fe2z 3 G 2 C 2 N 3 s T \u00fe z 2 s 2 g CN s T 2N r ;(53)\nwhere\nQ Q Q Q Q Q Q \u00c3 is\nthe globally optimal network weights over the whole environment.Dividing both sides of (53) by P T t\u00bc1 P Ns s\u00bc1 z 2 , the global convergence property of MTFL is given by\n1 TN s X T t\u00bc1 X N s s\u00bc1 E rL Q Q Q Q Q Q Q t;s\u00c01 \u00c0 \u00c1 2 2 h i 2 L Q Q Q Q Q Q Q 0 \u00c0 \u00c1 \u00c0 L Q Q Q Q Q Q Q \u00c3 \u00f0 \u00de zN s T \u00fe 4z 2 G 2 C 2 N 2 s \u00fe zCs 2 g N r :(54)\nBy setting the learning rate as z \u00bc\n\n\nffiffiffiffiffiffi ffi\n\nNr NsT q , we have (45  3\u00de.The proposed MTL-based network is trained according to Algorithm 1 using datasets with size of D \u00bc 9000 samples, and the tradeoff factor between the two subtasks is set as \u00bc 0:9.The Adam optimizer with the learning rate of 10 \u00c03 is adopted to train the parameters.\n\nFirst, the channel estimation performance of the proposed MTL-based network is evaluated, which is compared with the state-of-the-art benchmarks, such as the traditional least squares (LS) method with linear interpolation [39], the CS-based algorithms including orthogonal matching pursuit (OMP) [40] and generalized OMP (gOMP) [41], and the deep-learning-based method using deep neural networks (DNN) [9].\n\nTo investigate the accuracy of channel estimation, the NMSE of channel estimation with respect to SNR is reported in Fig. 5.It is shown that the proposed MTL-based network can achieve higher estimation accuracy at different SNRs compared with the benchmarks.At the target NMSE of 7 \u00c2 10 \u00c03 , the proposed MTL network achieves an SNR gain of greater than 10 dB compared with the CS-based algorithms of OMP and gOMP, which validates that the effectiveness of the proposed MTL-based network, especially in the harsh conditions like intensive noise.Meanwhile, since the number of pilots used in the simulation is much smaller than the length of the CIR, the LS method fails to solve the underdetermined problem.It can also be observed from Fig. 5 that, by exploiting the sparse characteristics of the visible light channel, the sparsity-aware MTL-based network can achieve higher accuracy than the deep-learning-based method using DNN.\n\nTo comprehensively investigate the channel estimation performance of the proposed MTL-based network, the successful recovery probability performance is reported in Fig. 6 as an alternative metric of channel estimation accuracy, which is defined as the probability that the NMSE is lower than -15 dB.It is shown by the results in Fig. 6 that, with the increase of the SNR, the MTL-based network can achieve a successful recovery probability of 0.99 at the SNR of 20 dB, which outperforms the CS-based algorithms by around 10 dB.Besides, the proposed scheme reaches the successful recovery probability of one at the SNR of 25 dB, which cannot be achieved by the deep-learning-based DNN even at a high SNR.\n\nOn the other hand, to evaluate the performance of the positioning task, the proposed MTL-based network is compared with some benchmark schemes, such as conventional machine-learning-based methods including the k-nearest neighbor (KNN) [20], support vector regression (SVR) [42], and random forest (RF) [43], and the deep-learning-based DNN [9].The NMSE of position estimation is reported in Fig. 7.It can be observed that the proposed MTL-based network can achieve a significantly higher positioning accuracy compared with the benchmark schemes at different SNRs.To visualize the positioning accuracy, the corresponding positioning error, which is calculated by the Euclid distance between the estimated coordinate and the ground-truth  coordinate, is reported in Fig. 8.It is shown by the results that, the proposed scheme can achieve centimeter-level accuracy at an SNR greater than 25 dB, and can reach the positioning error of 6.72 cm at the SNR of 35 dB, which significantly outperforms the machine-learning-based and deep-learning-based benchmarks.\n\nMoreover, to evaluate the overall performance of the positioning task over the environment, the cumulative distribution function (CDF) with respect to the positioning error is reported in Fig. 9.It can be seen that the CDF of the proposed MTL-based network grows much faster than the benchmark schemes, and reaches the value of 0.9 at the positioning error of 10 cm, which demonstrates that the positioning error of the proposed scheme can be controlled at centimeter-level with a probability of 0.9.\n\nFrom these simulation results, it is verified that the proposed MTL-based network greatly outperforms the benchmark methods in both the channel estimation and positioning tasks, which implies that the mutual benefits between the two tasks can be effectively extracted and exploited by the proposed scheme to improve the performance of the VIPAC system.   the NMSE of channel estimation for the proposed scheme reduces rapidly with communication rounds, and gradually converges to 1:3 \u00c2 10 \u00c03 .The successful recovery probability rapidly reaches a high value of 0.978 after the second communication round, and then gradually converges to 0.999.This indicates that the proposed scheme can achieve satisfactory performance of channel estimation and generalization ability enabled by the multi-user cooperative VIPAC mechanism.The positioning performance is shown in Fig. 13.It is observed that the NMSE and positioning error rapidly decrease thanks to the global aggregation of the learnt local models of multiple UE agents.It is shown by the results that the positioning error reaches centimeter-level after only 70 communication rounds, and finally converges to 6.77 cm, which verifies the efficiency and generalization ability of the proposed scheme in terms of positioning.\n\nTo further investigate the generalization ability of the proposed MTFL framework in spatiotemporally variant environments, we consider a dynamically changing scenario where the environmental parameters, such as room layout, LED deployment, and ambient noise, change over time and/or space.A typical demonstration is a person carrying a UE terminal is walking from one room to another.Specifically, the parameters including the room size of L \u00c2 W \u00c2 H, the average reflectance of walls r, and the SNR g, will suddenly change to a set of different values after every 200 communication rounds, which implies the person has just walked into a different room.The parameters in the three rooms are as follows: Room 1, L \u00c2 W \u00c2 H \u00bc 5 \u00c2 5 \u00c2 3m 3 , r \u00bc 0:7; Room 2, L \u00c2 W \u00c2 H \u00bc 7 \u00c2 7 \u00c2 5m 3 , r \u00bc 0:7; Room 3, L \u00c2 W \u00c2 H \u00bc 6 \u00c2 6 \u00c2 4m 3 , r \u00bc 0:3.The performance of the channel estimation task and the positioning task using the proposed MTFL framework over time are reported in Figs. 14 and 15, respectively.It is also compared with the proposed MTL-based network which is trained in a centralized rather than distributed manner.As shown in Fig. 14, the NMSE of the proposed MTFL framework is a bit greater than that of the MTL network in the initial 200 communication rounds before the environment switches.However, the NMSE of the proposed MTFL scheme can reduce rapidly again to a satisfactory level after the UE agent walks into a new room every 200 communication rounds, while the MTL scheme cannot adapt to environmental changes and thus ends up with poorer performance.Similarly, it can be observed from Fig. 15 that, the positioning error of the MTL scheme increases to 196.0 cm and 110.5 cm with the environment switching at the 201th and 401th communication round, respectively, while the MTFL scheme can converge back to centimeter-level positioning accuracy rapidly in only a few communication rounds in a new environment.This is because in the MTFL framework, the global model is updated online by the UE agents over time and space.The local datasets used for training the local models of the UE agents can be updated online, and thus the local models can be updated correspondingly to adapt to the possible spatiotemporal variations.Hence, the global model weights suitable and adaptive for VIPAC tasks in the new environment can be learnt rapidly in spatiotemporally non-stationary environments.\n\n\nCONCLUSION\n\nIn this paper, an ISAC framework called VIPAC is proposed, where the two main tasks, i.e., the positioning task for the sensing service and the channel estimation task for the communication service are integrated into a unified visible light architecture, and a spatially migratable multi-lamp cellular cluster architecture is designed.An MTL-based network architecture, which is composed of a sparsity-aware shared network and two task-oriented sub-networks, is devised to achieve mutual benefits between positioning and channel estimation.An MTFL framework is formulated to further improve the generalization ability of the global model for multi-user cooperative VIPAC in spatiotemporally nonstationary environments.The theoretical bounds of the channel estimation and positioning accuracy are derived, and the convergence rate of the proposed MTFL framework is derived.The performance of the proposed MTL-based network and the MTFL framework are evaluated by extensive simulations, which significantly outperforms the benchmark schemes in estimation accuracy and the adaptation capability in harsh and variant scenarios.Moreover, the proposed VIPAC framework to serve as an emerging ISAC solution in the next-generation mobile and wireless networks.\n\nwhere h \u00f0t\u00de LOS and h \u00f0t\u00de NLOS\n\u00f0t\u00de\nrepresent the CIR components of the LOS and NLOS links, respectively.\n\n\nFig. 1 .\n1\nFig. 1.The system model of the VIPAC framework.\n\n\n\n\ndistances between the tth LED and the surface reflection element, and between the surface reflection element and the PD, respectively.' \u00f0t\u00de in and ' \u00f0t\u00de ref are the angles of irradiance of the LED and the surface reflection element, respectively.c \u00f0t\u00de in and c \u00f0t\u00de ref are the angles of incidence of the surface reflection element and the PD, respectively.\n\n\nFig. 2 .\n2\nFig. 2. Illustration of the proposed sparsity-aware multi-task learning architecture for accurate channel and position estimation.\n\n\n3 .\n3\nIn the cooperative VIPAC system considered, N r UE agents are located in the indoor environment, and N t LEDs formulate the cellular clusters with the pattern shown in Fig. 1c.Multiple UE agents participate in the training of a global model in the framework of federated learning in order to improve the generalization ability in spatiotemporally nonstationary environments.Specifically, in the proposed MTFL framework, the UE agents participate in the collection of training samples.During a sensing interval, the rth UE agent collects the stacked CIR vector h d r and the measurement vector u d r received from the LEDs, and labels them with the corresponding position coordinate c d r \u00bc \u00bdc d r;x ; c d r;y ; c d r;z .As UE agents are moving in the indoor environment, the samples corresponding to different locations are obtained.Moreover, to exploit the multi-lamp cellular cluster architecture for cooperative VIPAC as introduced in Section 2.2.2 to improve the spatial generalization ability, the position coordinates in different cellular clusters can be standardized by transferring absolute coordinates to relative coordinates, which can be expressed as\n\n\nFig. 3 .\n3\nFig. 3.The multi-task federated learning framework.\n\n\n\n\nto train the weights of the local MTL network model, which contains the local information for the VIPAC tasks.The local weights for the local model are first initialized by the global weights of the previous communication round, i.e., Q Q\n\n\n\n\nand then updated via N s local training steps.In the sth local training step, all the local weights Q\n\n\n\n\nis the gradient of the local loss function.After N s local training steps, the rth UE agent uploads the updated local weights\n\n\n\n\np u \u00f0t\u00de j h \u00f0t\u00de u \u00f0t\u00de ; h \u00f0t\u00de\n\n\n\n\ndenotes the local loss function over the local dataset of the rth UE agent, and L\u00f0Q Q Q Q Q Q Q\u00de denotes the global loss function over all the local datasets.For the convenience of description, we define the average of the local model weights as\n\n\nFig. 4 .Fig. 5 .\n45\nFig. 4. LED deployment in the indoor environment.\n\n\nFig. 6 .Fig. 7 .\n67\nFig. 6.Successful Recovery probability of the proposed MTL-based network and the benchmark schemes for channel estimation task.\n\n\n6. 3\n3\nPerformance of the MTFL Framework for VIPAC To demonstrate the performance of the proposed MTFL framework, a cooperative VIPAC architecture is considered, where three cellular clusters with the cluster center coordi-are incorporated for joint training.Each cellular cluster contains four LED cells, which are deployed in the pattern illustrated in Fig. 10.There are N r \u00bc 10 UE agents participating in the multi-user cooperative training of the global model, and the local dataset of each UE agent includes D r \u00bc 900 samples.The local datasets are updated with newly collected samples every 50 communication rounds.The absolute coordinates in the training and testing datasets are transferred into the standardized relative coordinates via (19).The parameters in Algorithm 2 are set as follows: batch size B \u00bc 128; number of local steps N s \u00bc 5; maximum communication rounds T \u00bc 50.The average loss function over the local datasets in the training stage of the MTFL framework with respect to the communication rounds is shown in Fig. 11.We could observe that the training losses of both the positioning and channel estimation sub-tasks, and the joint loss function given by (18), decrease rapidly with the communication rounds in the cooperative training process participated by multiple UE agents.After several communication rounds, the training losses converge gradually to a relatively low level, which verifies the efficiency and the convergence capability of the proposed MTFL framework as is consistent with the theoretical analysis given in Section 5.2.Meanwhile, the testing performance of the proposed MTFL framework is evaluated with randomly moving UE agents in the cellular clusters.The global model trained cooperatively after each communication round is tested and the testing results of channel estimation and positioning are reported in Figs. 12 and 13, respectively.As shown in Fig.12,\n\n\nFig. 8 .Fig. 9 .\n89\nFig. 8. Positioning error of the proposed MTL-based network and the benchmark schemes.\n\n\nFig. 10 .Fig. 11 .\n1011\nFig. 10.The deployment of LEDs and cellular cluster layout in the proposed MTFL framework.\n\n\nFig. 12 .Fig. 13 .\n1213\nFig. 12. Performance of MTFL scheme for channel estimation task with respect to communication rounds in federated learning.\n\n\nFig. 14 .Fig. 15 .\n1415\nFig. 14.NMSE performance of channel estimation task in spatiotemporally variant environments (the environment changes every 200 communication rounds).\n\n\n\n\nall the N r UE agents via visible light downlink transmission 13: end for\n\n\n\n\nL holds, i.e., the columns in the partial DFT matrix F \u00f0t\u00de p selected from the matrix F are orthogonal to each other.Then, it is derived that the matrix \u00f0F \u00f0t\u00de p \u00de H F \u00f0t\u00de p contains identical diagonal elements of N p .Thus, we have\ntr F \u00f0t\u00de pHF \u00f0t\u00de p\u00bc N p L:\np ;(26)where 1 ; 2 ; . . .; L denote the eigenvalues of the matrix \u00f0F \u00f0t\u00de p \u00de H F \u00f0t\u00de p .It is worth noting that, the inequation in (26) reaches equality if and only if the condition 1 \u00bc 2 \u00bc \u00c1 \u00c1 \u00c1 \u00bc\n\n\n\n\nIn this section, the performance of the proposed MTL-based and MTFL-based schemes for the VIPAC system are investigated via extensive simulations.A room with size of L \u00c2 W \u00c2 H \u00bc 5 \u00c2 5 \u00c2 3m 3 deployed with the VIPAC infrastructure as illustrated in Fig.1is considered.The simulation parameters of the VIPAC system are listed in Table1.Each cellular cluster contains N t \u00bc 4 LEDs.The length of the OFDM data block and the CP are N \u00bc 1024 and N CP \u00bc 64, respectively.The number of pilot subcarriers utilized by each LED is N p \u00bc 16.The OFDM bandwidth is 20 MHz.The maximum channel length is set as L \u00bc 64, which is same as the CP length.The signal-to-noise ratio (SNR) g is defined as the ratio of the received signal power to the noise power.The training datasets are generated using the channel model in Section 2, and the position coordinates are generated randomly.The proposed MTL-based and MTFL-based networks are implemented with the TensorFlow platform and the Keras library.\n). t u6 SIMULATION RESULTS AND DISCUSSIONS6.1 Simulation Setup\n6.2 Channel and Position Estimation Performance of the MTL-Based NetworkTo evaluate the performance of the proposed MTL-based network, an LED deployment pattern in Fig.4is considered,\n\n\nTABLE 1 System\n1\nParameters in the VIPAC System\nParameterSymbolValueHalf-power angle of LEDs' 1=260Electro-optical conversion efficiencya1 W=AAverage reflectance of wallsr0.7FOV angle of PDs PD effective area Optical filter gainc FOV A PD T s90 1 cm 2 1Optical concentrator gaing1PD responsivityR p0:6 A=W\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 12, DECEMBER 2023Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nWEI ET AL.: VISIBLE LIGHT INTEGRATED POSITIONING AND COMMUNICATION: A MULTI-TASK FEDERATED LEARNING...\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 12, DECEMBER 2023\n\" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.\nThis work was supported in part by the National Natural Science Foundation of China under Grant 61901403, in part by the Science and Technology Key Project of Fujian Province, China under Grant 2019HZ020009, in part by the Youth Innovation Fund of Natural Science Foundation of Xiamen under Grant 3502Z20206039, and in part by the Xiamen Special Fund for Marine and Fishery Development under Grant 21CZB011HJ02.Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.\nA vision of 6G wireless systems: Applications, trends, technologies, and open research problems. W Saad, M Bennis, M Chen, IEEE Netw. 343May/Jun. 2020\n\nAn intelligent wireless transmission toward 6G. P Zhang, L Li, K Niu, Y Li, G Lu, Z Wang, Intell. Converged Netw. 23Sep. 2021\n\nArchitecture, mobility management, and quality of service for integrated 3G and WLAN networks. Y Xiao, K K Leung, Y Pan, X Du, Wirel. Commun. Mobile Comput. 57Nov. 2005\n\nIntegrated sensing and communications: Toward dual-functional wireless networks for 6G and beyond. F Liu, IEEE J. Sel. Areas Commun. 406Jun. 2022\n\nJoint transmit and receive beamforming design for integrated sensing and communication. N Zhao, Y Wang, Z Zhang, Q Chang, Y Shen, IEEE Commun. Lett. 263Mar. 2022\n\nIntegrating sensing and communications for ubiquitous IoT: Applications, trends, and challenges. Y Cui, F Liu, X Jing, J Mu, IEEE Netw. 355Sep./Oct. 2021\n\nIndoor positioning systems based on visible light communication: State of the art. J Luo, L Fan, H Li, IEEE Commun. Surveys Tuts. 194Oct.-Dec. 2017\n\nA survey of positioning systems using visible LED lights. Y Zhuang, IEEE Commun. Surveys Tuts. 203Jul.-Sep. 2018\n\nInvoking deep learning for joint estimation of indoor LiFi user position and orientation. M A Arfaoui, IEEE J. Sel. Areas Commun. 399Sep. 2021\n\nComputer vision-based localization with visible light communications. L Bai, IEEE Trans. Wireless Commun. 213Mar. 2022\n\nInternet of radio and light: 5G building network radio and edge architecture. Y Zhang, Intell. Converged Netw. 11Jun. 2020\n\nVisible light communication: Concepts, applications and challenges. L E M Matheus, A B Vieira, L F M Vieira, M A M Vieira, O Gnawali, IEEE Commun. Surveys Tuts. 214Oct.-Dec. 2019\n\nVisible light communication in 6G: Advances, challenges, and prospects. N Chi, Y Zhou, Y Wei, F Hu, IEEE Veh. Technol. Mag. 154Dec. 2020\n\nPerformance analysis of a lowcomplexity nonorthogonal multiple access scheme in visible light communication downlinks using pulse modulations. J Song, T Cao, H Zhang, Intell. Converged Netw. 21Mar. 2021\n\nA reversed visible light multitarget localization system via sparse matrix reconstruction. R Zhang, W.-D Zhong, K Qian, S Zhang, P Du, IEEE Internet Things J. 55Oct. 2018\n\nExperimental demonstration of compressive sensing-based channel estimation for MIMO-OFDM VLC. B Lin, Z Ghassemlooy, J Xu, Q Lai, X Shen, X Tang, IEEE Wireless Commun. Lett. 97Jul. 2020\n\nChannel estimation scheme based on compressed sensing and parameter estimation for an orthogonal frequency division multiplexing visible light communications system. J Du, H Deng, X Qian, C Zhang, Opt. Eng. 5511Nov. 2016\n\nChannel estimation for optical-OFDM-based multiuser MISO visible light communication. L Wu, J Cheng, Z Zhang, J Dang, H Liu, IEEE Photon. Technol. Lett. 2920Oct. 2017\n\nSuperimposed training-based channel estimation for MISO optical-OFDM VLC. J C Estrada-Jim Enez, B G Guzm An, M J Fern Andez-Getino Garc Ia, V P G Jim Enez, IEEE Trans. Veh. Technol. 686Jun. 2019\n\nWeighted k-nearest neighbour model for indoor VLC positioning. M T Van, N Van Tuan, T T Son, H Le-Minh, A Burton, IET Commun. 116Mar. 2017\n\nMultitask learning. R Caruana, Mach. Learn. 281Jul. 1997\n\nA survey on multi-task learning. Y Zhang, Q Yang, 10.1109/TKDE.2021.3070203IEEE Trans. Knowl. Data Eng., early access. Mar. 31, 2021\n\nDeep multi-task learning for cooperative NOMA: System design and principles. Y Lu, P Cheng, Z Chen, W H Mow, Y Li, B Vucetic, IEEE J. Sel. Areas Commun. 391Jan. 2021\n\nFederated machine learning: Concept and applications. Q Yang, Y Liu, T Chen, Y Tong, ACM Trans. Intell. Syst. Technol. 102Mar. 2019\n\nWireless communications for collaborative federated learning. M Chen, H V Poor, W Saad, S Cui, IEEE Commun. Mag. 5812Jan. 2021\n\nIntelligent task offloading and energy allocation in the UAVaided mobile edge-cloud continuum. Z Cheng, Z Gao, M Liwang, L Huang, X Du, M Guizani, IEEE Netw. 355Sep./Oct. 2021\n\nPrivacy-preserving aggregation for federated learning-based navigation in vehicular fog. Q Kong, IEEE Trans. Ind. Informat. 1712Dec. 2021\n\nExploiting unintended property leakage in blockchain-assisted federated learning for intelligent edge computing. M Shen, IEEE Internet Things J. 84Feb. 2021\n\nFundamental analysis for visiblelight communication system using LED lights. T Komine, M Nakagawa, IEEE Trans. Consum. Electron. 501Feb. 2004\n\nRadio spectrum awareness using deep learning: Identification of fading channels, signal distortions, medium access control protocols, and cellular systems. Y Zhou, Intell. Converged Netw. 21Mar. 2021\n\nMulticlass routing and medium access control for heterogeneous mobile ad hoc networks. X Du, D Wu, W Liu, Y Fang, IEEE Trans. Veh. Technol. 551Jan. 2006\n\nAMP-inspired deep networks for sparse linear inverse problems. M Borgerding, P Schniter, S Rangan, IEEE Trans. Signal Process. 6516Aug. 2017\n\nLSTM: A search space odyssey. K Greff, R K Srivastava, J Koutn Ik, B R Steunebrink, J Schmidhuber, IEEE Trans. Neural Netw. Learn. Syst. 2810Oct. 2017\n\nS M Kay, Fundamentals of Statistical Signal Processing, Volumn I: Estimation Theory. Hoboken, NJ, USAPrentice-Hall1993\n\nAn algebraic, analytic, and algorithmic investigation on the capacity and capacity-achieving input probability distributions of finite-input-finite-output discrete memoryless channels. X.-B Liang, IEEE Trans. Inf. Theory. 543Mar. 2008\n\n. IEEE TRANSACTIONS ON MOBILE COMPUTING. 2212DECEMBER 2023\n\nTheoretical accuracy analysis of indoor visible light communication positioning system based on received signal strength indicator. X Zhang, J Duan, Y Fu, A Shi, J. Lightw. Technol. 3221Nov. 2014\n\nParallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning. H Yu, S Yang, S Zhu, Proc. 33rd AAAI Conf. 33rd AAAI Conf2019\n\nCooperative SGD: A unified framework for the design and analysis of local-update SGD algorithms. J Wang, G Joshi, J. Mach. Learn. Res. 22213Sep. 2021\n\nAdaptive least squares channel estimation for visible light communications based on tap detection. X Shi, S.-H Leung, J Min, Opt. Commun. 467Jul. 2020. 125712\n\nA sharp condition for exact support recovery with orthogonal matching pursuit. J Wen, Z Zhou, J Wang, X Tang, Q Mo, IEEE Trans. Signal Process. 656Mar. 2017\n\nImproved sufficient condition for performance guarantee in generalized orthogonal matching pursuit. D Park, IEEE Signal Process. Lett. 249Sep. 2017\n\nThree-dimensional indoor visible light localization: A learning-based approach. D Su, X Liu, S Liu, Proc. ACM Int. Joint Conf. Pervasive Ubiquitous Comput. ACM Int. Joint Conf. Pervasive Ubiquitous Comput2021\n\nIndoor localization using visible light via fusion of multiple classifiers. X Guo, S Shao, N Ansari, A Khreishah, IEEE Photon. J. 96Dec. 2017\n", "annotations": {"author": "[{\"end\":132,\"start\":101},{\"end\":182,\"start\":133},{\"end\":227,\"start\":183},{\"end\":344,\"start\":228},{\"end\":447,\"start\":345}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":109},{\"end\":162,\"start\":159},{\"end\":207,\"start\":205}]", "author_first_name": "[{\"end\":108,\"start\":101},{\"end\":158,\"start\":152},{\"end\":204,\"start\":195}]", "author_affiliation": "[{\"end\":343,\"start\":229},{\"end\":446,\"start\":346}]", "title": "[{\"end\":98,\"start\":1},{\"end\":545,\"start\":448}]", "venue": null, "abstract": "[{\"end\":2378,\"start\":915}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2717,\"start\":2714},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2722,\"start\":2719},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2727,\"start\":2724},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2999,\"start\":2996},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3004,\"start\":3001},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3436,\"start\":3433},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3702,\"start\":3699},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3707,\"start\":3704},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3712,\"start\":3709},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3718,\"start\":3714},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3724,\"start\":3720},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3953,\"start\":3949},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3959,\"start\":3955},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3965,\"start\":3961},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4512,\"start\":4508},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4539,\"start\":4535},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4545,\"start\":4541},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4872,\"start\":4868},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4878,\"start\":4874},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4884,\"start\":4880},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5807,\"start\":5803},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6009,\"start\":6005},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6015,\"start\":6011},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7524,\"start\":7520},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7530,\"start\":7526},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7905,\"start\":7901},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7911,\"start\":7907},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7917,\"start\":7913},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13962,\"start\":13958},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14258,\"start\":14254},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15157,\"start\":15153},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15595,\"start\":15591},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15601,\"start\":15597},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16726,\"start\":16723},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18261,\"start\":18257},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18267,\"start\":18263},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19443,\"start\":19439},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19449,\"start\":19445},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22386,\"start\":22382},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22866,\"start\":22862},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26559,\"start\":26555},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30998,\"start\":30994},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31295,\"start\":31291},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31921,\"start\":31917},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35755,\"start\":35751},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":43902,\"start\":43898},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":44060,\"start\":44057},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":44772,\"start\":44768},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":44850,\"start\":44846},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":45064,\"start\":45060},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":45211,\"start\":45207},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":45365,\"start\":45361},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":45515,\"start\":45511},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":46322,\"start\":46318},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":47840,\"start\":47836},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":47905,\"start\":47901},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":48960,\"start\":48956},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":49144,\"start\":49140},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":49154,\"start\":49150},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":49750,\"start\":49746},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":50947,\"start\":50943},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":50953,\"start\":50949},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":52024,\"start\":52020},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":52346,\"start\":52342},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":52589,\"start\":52585},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":58804,\"start\":58800},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":58878,\"start\":58874},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":58910,\"start\":58906},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":58983,\"start\":58980},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":60863,\"start\":60859},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":60901,\"start\":60897},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":60930,\"start\":60926},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":60967,\"start\":60964}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":67231,\"start\":67125},{\"attributes\":{\"id\":\"fig_1\"},\"end\":67292,\"start\":67232},{\"attributes\":{\"id\":\"fig_2\"},\"end\":67653,\"start\":67293},{\"attributes\":{\"id\":\"fig_3\"},\"end\":67797,\"start\":67654},{\"attributes\":{\"id\":\"fig_4\"},\"end\":68968,\"start\":67798},{\"attributes\":{\"id\":\"fig_5\"},\"end\":69033,\"start\":68969},{\"attributes\":{\"id\":\"fig_6\"},\"end\":69276,\"start\":69034},{\"attributes\":{\"id\":\"fig_7\"},\"end\":69382,\"start\":69277},{\"attributes\":{\"id\":\"fig_8\"},\"end\":69512,\"start\":69383},{\"attributes\":{\"id\":\"fig_9\"},\"end\":69546,\"start\":69513},{\"attributes\":{\"id\":\"fig_10\"},\"end\":69796,\"start\":69547},{\"attributes\":{\"id\":\"fig_14\"},\"end\":69868,\"start\":69797},{\"attributes\":{\"id\":\"fig_15\"},\"end\":70018,\"start\":69869},{\"attributes\":{\"id\":\"fig_16\"},\"end\":71930,\"start\":70019},{\"attributes\":{\"id\":\"fig_17\"},\"end\":72039,\"start\":71931},{\"attributes\":{\"id\":\"fig_18\"},\"end\":72156,\"start\":72040},{\"attributes\":{\"id\":\"fig_19\"},\"end\":72306,\"start\":72157},{\"attributes\":{\"id\":\"fig_20\"},\"end\":72483,\"start\":72307},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":72561,\"start\":72484},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":73024,\"start\":72562},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":74256,\"start\":73025},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":74563,\"start\":74257}]", "paragraph": "[{\"end\":3437,\"start\":2394},{\"end\":3971,\"start\":3439},{\"end\":5126,\"start\":3973},{\"end\":5644,\"start\":5128},{\"end\":6224,\"start\":5646},{\"end\":7291,\"start\":6226},{\"end\":8319,\"start\":7293},{\"end\":9827,\"start\":8321},{\"end\":10886,\"start\":9829},{\"end\":12340,\"start\":10888},{\"end\":12744,\"start\":12409},{\"end\":13736,\"start\":12815},{\"end\":14077,\"start\":13738},{\"end\":14321,\"start\":14079},{\"end\":14524,\"start\":14465},{\"end\":15484,\"start\":14526},{\"end\":16000,\"start\":15486},{\"end\":16542,\"start\":16002},{\"end\":16958,\"start\":16544},{\"end\":17194,\"start\":16990},{\"end\":17640,\"start\":17221},{\"end\":17988,\"start\":17662},{\"end\":18899,\"start\":18088},{\"end\":21668,\"start\":18984},{\"end\":22069,\"start\":21752},{\"end\":22867,\"start\":22120},{\"end\":23642,\"start\":22869},{\"end\":24559,\"start\":23644},{\"end\":25224,\"start\":24608},{\"end\":25340,\"start\":25325},{\"end\":25761,\"start\":25401},{\"end\":26002,\"start\":25834},{\"end\":26591,\"start\":26061},{\"end\":27769,\"start\":26678},{\"end\":28062,\"start\":27839},{\"end\":28755,\"start\":28064},{\"end\":29005,\"start\":28841},{\"end\":29196,\"start\":29025},{\"end\":29518,\"start\":29306},{\"end\":30268,\"start\":29719},{\"end\":31151,\"start\":30443},{\"end\":31344,\"start\":31254},{\"end\":31785,\"start\":31740},{\"end\":32341,\"start\":31787},{\"end\":33385,\"start\":32343},{\"end\":33856,\"start\":33454},{\"end\":34115,\"start\":33894},{\"end\":34427,\"start\":34386},{\"end\":34626,\"start\":34446},{\"end\":35098,\"start\":34770},{\"end\":36152,\"start\":35100},{\"end\":36480,\"start\":36281},{\"end\":36738,\"start\":36557},{\"end\":37251,\"start\":36774},{\"end\":37614,\"start\":37303},{\"end\":38159,\"start\":37616},{\"end\":38979,\"start\":38392},{\"end\":39714,\"start\":39602},{\"end\":40001,\"start\":39968},{\"end\":40166,\"start\":40085},{\"end\":40200,\"start\":40168},{\"end\":40351,\"start\":40284},{\"end\":40491,\"start\":40459},{\"end\":40589,\"start\":40512},{\"end\":40704,\"start\":40591},{\"end\":41071,\"start\":40752},{\"end\":41646,\"start\":41073},{\"end\":41796,\"start\":41665},{\"end\":42683,\"start\":41798},{\"end\":43472,\"start\":42685},{\"end\":43903,\"start\":43592},{\"end\":44162,\"start\":43905},{\"end\":44221,\"start\":44164},{\"end\":44401,\"start\":44223},{\"end\":44502,\"start\":44443},{\"end\":44655,\"start\":44504},{\"end\":44853,\"start\":44726},{\"end\":45083,\"start\":44877},{\"end\":45220,\"start\":45160},{\"end\":45434,\"start\":45327},{\"end\":45881,\"start\":45436},{\"end\":46353,\"start\":45883},{\"end\":46565,\"start\":46355},{\"end\":46803,\"start\":46656},{\"end\":47029,\"start\":46872},{\"end\":47309,\"start\":47084},{\"end\":47498,\"start\":47332},{\"end\":47610,\"start\":47500},{\"end\":47734,\"start\":47612},{\"end\":47917,\"start\":47822},{\"end\":48154,\"start\":48115},{\"end\":48427,\"start\":48408},{\"end\":48633,\"start\":48541},{\"end\":48842,\"start\":48706},{\"end\":49073,\"start\":48943},{\"end\":49479,\"start\":49118},{\"end\":49894,\"start\":49526},{\"end\":49974,\"start\":49969},{\"end\":50822,\"start\":50251},{\"end\":50954,\"start\":50824},{\"end\":51273,\"start\":51162},{\"end\":51431,\"start\":51411},{\"end\":51492,\"start\":51433},{\"end\":51660,\"start\":51640},{\"end\":52025,\"start\":51791},{\"end\":52105,\"start\":52027},{\"end\":52208,\"start\":52173},{\"end\":52259,\"start\":52210},{\"end\":52372,\"start\":52261},{\"end\":52456,\"start\":52390},{\"end\":52615,\"start\":52565},{\"end\":52807,\"start\":52794},{\"end\":53303,\"start\":53226},{\"end\":53461,\"start\":53396},{\"end\":53836,\"start\":53687},{\"end\":54375,\"start\":54327},{\"end\":54623,\"start\":54553},{\"end\":54815,\"start\":54767},{\"end\":55078,\"start\":55029},{\"end\":55118,\"start\":55080},{\"end\":55750,\"start\":55672},{\"end\":56084,\"start\":55803},{\"end\":56374,\"start\":56365},{\"end\":56899,\"start\":56845},{\"end\":57074,\"start\":56958},{\"end\":57134,\"start\":57076},{\"end\":57607,\"start\":57464},{\"end\":57875,\"start\":57870},{\"end\":58064,\"start\":57895},{\"end\":58258,\"start\":58223},{\"end\":58576,\"start\":58285},{\"end\":58984,\"start\":58578},{\"end\":59917,\"start\":58986},{\"end\":60622,\"start\":59919},{\"end\":61678,\"start\":60624},{\"end\":62180,\"start\":61680},{\"end\":63456,\"start\":62182},{\"end\":65856,\"start\":63458},{\"end\":67124,\"start\":65871},{\"end\":67230,\"start\":67161},{\"end\":67291,\"start\":67244},{\"end\":67652,\"start\":67296},{\"end\":67796,\"start\":67666},{\"end\":68967,\"start\":67805},{\"end\":69032,\"start\":68981},{\"end\":69275,\"start\":69037},{\"end\":69381,\"start\":69280},{\"end\":69511,\"start\":69386},{\"end\":69545,\"start\":69516},{\"end\":69795,\"start\":69550},{\"end\":69867,\"start\":69818},{\"end\":70017,\"start\":69890},{\"end\":71929,\"start\":70027},{\"end\":72038,\"start\":71952},{\"end\":72155,\"start\":72065},{\"end\":72305,\"start\":72182},{\"end\":72482,\"start\":72332},{\"end\":72560,\"start\":72487},{\"end\":72797,\"start\":72565},{\"end\":73023,\"start\":72825},{\"end\":74008,\"start\":73028},{\"end\":74255,\"start\":74072},{\"end\":74305,\"start\":74275}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14357,\"start\":14322},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14464,\"start\":14357},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17642,\"start\":17641},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17661,\"start\":17642},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18087,\"start\":17989},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25278,\"start\":25225},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25324,\"start\":25278},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25400,\"start\":25341},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25833,\"start\":25762},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26060,\"start\":26003},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26677,\"start\":26592},{\"attributes\":{\"id\":\"formula_11\"},\"end\":28840,\"start\":28756},{\"attributes\":{\"id\":\"formula_12\"},\"end\":29024,\"start\":29006},{\"attributes\":{\"id\":\"formula_13\"},\"end\":29305,\"start\":29197},{\"attributes\":{\"id\":\"formula_14\"},\"end\":29718,\"start\":29519},{\"attributes\":{\"id\":\"formula_15\"},\"end\":30442,\"start\":30375},{\"attributes\":{\"id\":\"formula_16\"},\"end\":31253,\"start\":31152},{\"attributes\":{\"id\":\"formula_17\"},\"end\":31542,\"start\":31345},{\"attributes\":{\"id\":\"formula_18\"},\"end\":31584,\"start\":31542},{\"attributes\":{\"id\":\"formula_19\"},\"end\":31739,\"start\":31584},{\"attributes\":{\"id\":\"formula_20\"},\"end\":33893,\"start\":33857},{\"attributes\":{\"id\":\"formula_21\"},\"end\":34385,\"start\":34116},{\"attributes\":{\"id\":\"formula_22\"},\"end\":34445,\"start\":34428},{\"attributes\":{\"id\":\"formula_23\"},\"end\":34768,\"start\":34627},{\"attributes\":{\"id\":\"formula_24\"},\"end\":34769,\"start\":34768},{\"attributes\":{\"id\":\"formula_25\"},\"end\":36280,\"start\":36153},{\"attributes\":{\"id\":\"formula_26\"},\"end\":36773,\"start\":36739},{\"attributes\":{\"id\":\"formula_27\"},\"end\":37302,\"start\":37252},{\"attributes\":{\"id\":\"formula_28\"},\"end\":38391,\"start\":38257},{\"attributes\":{\"id\":\"formula_29\"},\"end\":39378,\"start\":38980},{\"attributes\":{\"id\":\"formula_30\"},\"end\":39394,\"start\":39378},{\"attributes\":{\"id\":\"formula_31\"},\"end\":39417,\"start\":39394},{\"attributes\":{\"id\":\"formula_32\"},\"end\":39496,\"start\":39417},{\"attributes\":{\"id\":\"formula_33\"},\"end\":39550,\"start\":39496},{\"attributes\":{\"id\":\"formula_34\"},\"end\":39587,\"start\":39550},{\"attributes\":{\"id\":\"formula_35\"},\"end\":39601,\"start\":39587},{\"attributes\":{\"id\":\"formula_36\"},\"end\":39966,\"start\":39715},{\"attributes\":{\"id\":\"formula_37\"},\"end\":39967,\"start\":39966},{\"attributes\":{\"id\":\"formula_38\"},\"end\":40064,\"start\":40002},{\"attributes\":{\"id\":\"formula_39\"},\"end\":40084,\"start\":40064},{\"attributes\":{\"id\":\"formula_40\"},\"end\":40217,\"start\":40201},{\"attributes\":{\"id\":\"formula_41\"},\"end\":40241,\"start\":40217},{\"attributes\":{\"id\":\"formula_42\"},\"end\":40283,\"start\":40241},{\"attributes\":{\"id\":\"formula_43\"},\"end\":40458,\"start\":40352},{\"attributes\":{\"id\":\"formula_44\"},\"end\":40511,\"start\":40492},{\"attributes\":{\"id\":\"formula_45\"},\"end\":40751,\"start\":40705},{\"attributes\":{\"id\":\"formula_46\"},\"end\":41664,\"start\":41647},{\"attributes\":{\"id\":\"formula_47\"},\"end\":44442,\"start\":44402},{\"attributes\":{\"id\":\"formula_48\"},\"end\":44725,\"start\":44656},{\"attributes\":{\"id\":\"formula_49\"},\"end\":44876,\"start\":44854},{\"attributes\":{\"id\":\"formula_51\"},\"end\":45146,\"start\":45084},{\"attributes\":{\"id\":\"formula_52\"},\"end\":45158,\"start\":45146},{\"attributes\":{\"id\":\"formula_53\"},\"end\":45159,\"start\":45158},{\"attributes\":{\"id\":\"formula_54\"},\"end\":45326,\"start\":45221},{\"attributes\":{\"id\":\"formula_56\"},\"end\":46645,\"start\":46566},{\"attributes\":{\"id\":\"formula_57\"},\"end\":46655,\"start\":46645},{\"attributes\":{\"id\":\"formula_58\"},\"end\":46870,\"start\":46804},{\"attributes\":{\"id\":\"formula_59\"},\"end\":46871,\"start\":46870},{\"attributes\":{\"id\":\"formula_60\"},\"end\":47083,\"start\":47030},{\"attributes\":{\"id\":\"formula_61\"},\"end\":47331,\"start\":47310},{\"attributes\":{\"id\":\"formula_62\"},\"end\":47821,\"start\":47735},{\"attributes\":{\"id\":\"formula_63\"},\"end\":48113,\"start\":47918},{\"attributes\":{\"id\":\"formula_64\"},\"end\":48114,\"start\":48113},{\"attributes\":{\"id\":\"formula_65\"},\"end\":48407,\"start\":48155},{\"attributes\":{\"id\":\"formula_66\"},\"end\":48540,\"start\":48428},{\"attributes\":{\"id\":\"formula_67\"},\"end\":48705,\"start\":48634},{\"attributes\":{\"id\":\"formula_68\"},\"end\":48932,\"start\":48843},{\"attributes\":{\"id\":\"formula_69\"},\"end\":48942,\"start\":48932},{\"attributes\":{\"id\":\"formula_70\"},\"end\":49117,\"start\":49074},{\"attributes\":{\"id\":\"formula_71\"},\"end\":49968,\"start\":49895},{\"attributes\":{\"id\":\"formula_72\"},\"end\":50075,\"start\":49975},{\"attributes\":{\"id\":\"formula_73\"},\"end\":50241,\"start\":50075},{\"attributes\":{\"id\":\"formula_74\"},\"end\":50250,\"start\":50241},{\"attributes\":{\"id\":\"formula_75\"},\"end\":51161,\"start\":50955},{\"attributes\":{\"id\":\"formula_76\"},\"end\":51410,\"start\":51274},{\"attributes\":{\"id\":\"formula_77\"},\"end\":51639,\"start\":51493},{\"attributes\":{\"id\":\"formula_78\"},\"end\":51790,\"start\":51727},{\"attributes\":{\"id\":\"formula_79\"},\"end\":52172,\"start\":52106},{\"attributes\":{\"id\":\"formula_81\"},\"end\":52389,\"start\":52373},{\"attributes\":{\"id\":\"formula_82\"},\"end\":52518,\"start\":52457},{\"attributes\":{\"id\":\"formula_83\"},\"end\":52556,\"start\":52518},{\"attributes\":{\"id\":\"formula_84\"},\"end\":52564,\"start\":52556},{\"attributes\":{\"id\":\"formula_85\"},\"end\":52731,\"start\":52616},{\"attributes\":{\"id\":\"formula_86\"},\"end\":52785,\"start\":52731},{\"attributes\":{\"id\":\"formula_87\"},\"end\":52793,\"start\":52785},{\"attributes\":{\"id\":\"formula_88\"},\"end\":52891,\"start\":52808},{\"attributes\":{\"id\":\"formula_89\"},\"end\":52924,\"start\":52891},{\"attributes\":{\"id\":\"formula_90\"},\"end\":53028,\"start\":52924},{\"attributes\":{\"id\":\"formula_91\"},\"end\":53100,\"start\":53028},{\"attributes\":{\"id\":\"formula_92\"},\"end\":53172,\"start\":53100},{\"attributes\":{\"id\":\"formula_93\"},\"end\":53194,\"start\":53172},{\"attributes\":{\"id\":\"formula_94\"},\"end\":53225,\"start\":53194},{\"attributes\":{\"id\":\"formula_95\"},\"end\":53350,\"start\":53304},{\"attributes\":{\"id\":\"formula_96\"},\"end\":53395,\"start\":53350},{\"attributes\":{\"id\":\"formula_97\"},\"end\":53686,\"start\":53462},{\"attributes\":{\"id\":\"formula_98\"},\"end\":54325,\"start\":53837},{\"attributes\":{\"id\":\"formula_99\"},\"end\":54326,\"start\":54325},{\"attributes\":{\"id\":\"formula_100\"},\"end\":54408,\"start\":54376},{\"attributes\":{\"id\":\"formula_101\"},\"end\":54452,\"start\":54408},{\"attributes\":{\"id\":\"formula_102\"},\"end\":54552,\"start\":54452},{\"attributes\":{\"id\":\"formula_104\"},\"end\":54766,\"start\":54624},{\"attributes\":{\"id\":\"formula_105\"},\"end\":55028,\"start\":54816},{\"attributes\":{\"id\":\"formula_107\"},\"end\":55179,\"start\":55119},{\"attributes\":{\"id\":\"formula_108\"},\"end\":55249,\"start\":55179},{\"attributes\":{\"id\":\"formula_109\"},\"end\":55474,\"start\":55249},{\"attributes\":{\"id\":\"formula_110\"},\"end\":55671,\"start\":55474},{\"attributes\":{\"id\":\"formula_111\"},\"end\":55802,\"start\":55751},{\"attributes\":{\"id\":\"formula_112\"},\"end\":56364,\"start\":56085},{\"attributes\":{\"id\":\"formula_114\"},\"end\":56844,\"start\":56375},{\"attributes\":{\"id\":\"formula_115\"},\"end\":56957,\"start\":56900},{\"attributes\":{\"id\":\"formula_116\"},\"end\":57463,\"start\":57135},{\"attributes\":{\"id\":\"formula_117\"},\"end\":57869,\"start\":57608},{\"attributes\":{\"id\":\"formula_118\"},\"end\":57894,\"start\":57876},{\"attributes\":{\"id\":\"formula_119\"},\"end\":58222,\"start\":58065}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2392,\"start\":2380},{\"attributes\":{\"n\":\"2\"},\"end\":12407,\"start\":12343},{\"attributes\":{\"n\":\"2.1\"},\"end\":12813,\"start\":12747},{\"attributes\":{\"n\":\"2.2\"},\"end\":16988,\"start\":16961},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":17219,\"start\":17197},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":18982,\"start\":18902},{\"attributes\":{\"n\":\"3\"},\"end\":21750,\"start\":21671},{\"attributes\":{\"n\":\"3.1\"},\"end\":22118,\"start\":22072},{\"attributes\":{\"n\":\"3.2\"},\"end\":24606,\"start\":24562},{\"attributes\":{\"n\":\"3.3\"},\"end\":27837,\"start\":27772},{\"end\":30374,\"start\":30271},{\"attributes\":{\"n\":\"3.4\"},\"end\":33452,\"start\":33388},{\"attributes\":{\"n\":\"4\"},\"end\":36555,\"start\":36483},{\"end\":38256,\"start\":38162},{\"attributes\":{\"n\":\"5\"},\"end\":43522,\"start\":43475},{\"attributes\":{\"n\":\"5.1\"},\"end\":43590,\"start\":43525},{\"attributes\":{\"n\":\"5.2\"},\"end\":49524,\"start\":49482},{\"end\":51726,\"start\":51663},{\"end\":58283,\"start\":58261},{\"attributes\":{\"n\":\"7\"},\"end\":65869,\"start\":65859},{\"end\":67156,\"start\":67126},{\"end\":67241,\"start\":67233},{\"end\":67663,\"start\":67655},{\"end\":67802,\"start\":67799},{\"end\":68978,\"start\":68970},{\"end\":69814,\"start\":69798},{\"end\":69886,\"start\":69870},{\"end\":70024,\"start\":70020},{\"end\":71948,\"start\":71932},{\"end\":72059,\"start\":72041},{\"end\":72176,\"start\":72158},{\"end\":72326,\"start\":72308},{\"end\":74272,\"start\":74258}]", "table": "[{\"end\":72824,\"start\":72798},{\"end\":74071,\"start\":74009},{\"end\":74563,\"start\":74306}]", "figure_caption": "[{\"end\":67231,\"start\":67160},{\"end\":67292,\"start\":67243},{\"end\":67653,\"start\":67295},{\"end\":67797,\"start\":67665},{\"end\":68968,\"start\":67804},{\"end\":69033,\"start\":68980},{\"end\":69276,\"start\":69036},{\"end\":69382,\"start\":69279},{\"end\":69512,\"start\":69385},{\"end\":69546,\"start\":69515},{\"end\":69796,\"start\":69549},{\"end\":69868,\"start\":69817},{\"end\":70018,\"start\":69889},{\"end\":71930,\"start\":70026},{\"end\":72039,\"start\":71951},{\"end\":72156,\"start\":72064},{\"end\":72306,\"start\":72181},{\"end\":72483,\"start\":72331},{\"end\":72561,\"start\":72486},{\"end\":72798,\"start\":72564},{\"end\":74009,\"start\":73027},{\"end\":74306,\"start\":74274}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12973,\"start\":12971},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19277,\"start\":19275},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19967,\"start\":19965},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20887,\"start\":20885},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21917,\"start\":21916},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23067,\"start\":23066},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24888,\"start\":24887},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28061,\"start\":28060},{\"end\":59109,\"start\":59108},{\"end\":59728,\"start\":59727},{\"end\":60089,\"start\":60088},{\"end\":60254,\"start\":60253},{\"end\":61021,\"start\":61020},{\"end\":61394,\"start\":61393},{\"end\":61874,\"start\":61873},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":63052,\"start\":63050},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":64439,\"start\":64430},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":64594,\"start\":64592},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":65064,\"start\":65062}]", "bib_author_first_name": "[{\"end\":75880,\"start\":75879},{\"end\":75888,\"start\":75887},{\"end\":75898,\"start\":75897},{\"end\":75983,\"start\":75982},{\"end\":75992,\"start\":75991},{\"end\":75998,\"start\":75997},{\"end\":76005,\"start\":76004},{\"end\":76011,\"start\":76010},{\"end\":76017,\"start\":76016},{\"end\":76157,\"start\":76156},{\"end\":76165,\"start\":76164},{\"end\":76167,\"start\":76166},{\"end\":76176,\"start\":76175},{\"end\":76183,\"start\":76182},{\"end\":76331,\"start\":76330},{\"end\":76467,\"start\":76466},{\"end\":76475,\"start\":76474},{\"end\":76483,\"start\":76482},{\"end\":76492,\"start\":76491},{\"end\":76501,\"start\":76500},{\"end\":76639,\"start\":76638},{\"end\":76646,\"start\":76645},{\"end\":76653,\"start\":76652},{\"end\":76661,\"start\":76660},{\"end\":76780,\"start\":76779},{\"end\":76787,\"start\":76786},{\"end\":76794,\"start\":76793},{\"end\":76904,\"start\":76903},{\"end\":77050,\"start\":77049},{\"end\":77052,\"start\":77051},{\"end\":77174,\"start\":77173},{\"end\":77302,\"start\":77301},{\"end\":77416,\"start\":77415},{\"end\":77420,\"start\":77417},{\"end\":77431,\"start\":77430},{\"end\":77433,\"start\":77432},{\"end\":77443,\"start\":77442},{\"end\":77447,\"start\":77444},{\"end\":77457,\"start\":77456},{\"end\":77461,\"start\":77458},{\"end\":77471,\"start\":77470},{\"end\":77600,\"start\":77599},{\"end\":77607,\"start\":77606},{\"end\":77615,\"start\":77614},{\"end\":77622,\"start\":77621},{\"end\":77809,\"start\":77808},{\"end\":77817,\"start\":77816},{\"end\":77824,\"start\":77823},{\"end\":77961,\"start\":77960},{\"end\":77973,\"start\":77969},{\"end\":77982,\"start\":77981},{\"end\":77990,\"start\":77989},{\"end\":77999,\"start\":77998},{\"end\":78136,\"start\":78135},{\"end\":78143,\"start\":78142},{\"end\":78158,\"start\":78157},{\"end\":78164,\"start\":78163},{\"end\":78171,\"start\":78170},{\"end\":78179,\"start\":78178},{\"end\":78394,\"start\":78393},{\"end\":78400,\"start\":78399},{\"end\":78408,\"start\":78407},{\"end\":78416,\"start\":78415},{\"end\":78536,\"start\":78535},{\"end\":78542,\"start\":78541},{\"end\":78551,\"start\":78550},{\"end\":78560,\"start\":78559},{\"end\":78568,\"start\":78567},{\"end\":78692,\"start\":78691},{\"end\":78694,\"start\":78693},{\"end\":78714,\"start\":78713},{\"end\":78716,\"start\":78715},{\"end\":78727,\"start\":78726},{\"end\":78729,\"start\":78728},{\"end\":78758,\"start\":78757},{\"end\":78762,\"start\":78759},{\"end\":78877,\"start\":78876},{\"end\":78879,\"start\":78878},{\"end\":78886,\"start\":78885},{\"end\":78898,\"start\":78897},{\"end\":78900,\"start\":78899},{\"end\":78907,\"start\":78906},{\"end\":78918,\"start\":78917},{\"end\":78974,\"start\":78973},{\"end\":79045,\"start\":79044},{\"end\":79054,\"start\":79053},{\"end\":79223,\"start\":79222},{\"end\":79229,\"start\":79228},{\"end\":79238,\"start\":79237},{\"end\":79246,\"start\":79245},{\"end\":79248,\"start\":79247},{\"end\":79255,\"start\":79254},{\"end\":79261,\"start\":79260},{\"end\":79367,\"start\":79366},{\"end\":79375,\"start\":79374},{\"end\":79382,\"start\":79381},{\"end\":79390,\"start\":79389},{\"end\":79508,\"start\":79507},{\"end\":79516,\"start\":79515},{\"end\":79518,\"start\":79517},{\"end\":79526,\"start\":79525},{\"end\":79534,\"start\":79533},{\"end\":79669,\"start\":79668},{\"end\":79678,\"start\":79677},{\"end\":79685,\"start\":79684},{\"end\":79695,\"start\":79694},{\"end\":79704,\"start\":79703},{\"end\":79710,\"start\":79709},{\"end\":79840,\"start\":79839},{\"end\":80003,\"start\":80002},{\"end\":80125,\"start\":80124},{\"end\":80135,\"start\":80134},{\"end\":80347,\"start\":80346},{\"end\":80479,\"start\":80478},{\"end\":80485,\"start\":80484},{\"end\":80491,\"start\":80490},{\"end\":80498,\"start\":80497},{\"end\":80609,\"start\":80608},{\"end\":80623,\"start\":80622},{\"end\":80635,\"start\":80634},{\"end\":80718,\"start\":80717},{\"end\":80727,\"start\":80726},{\"end\":80729,\"start\":80728},{\"end\":80743,\"start\":80742},{\"end\":80755,\"start\":80754},{\"end\":80757,\"start\":80756},{\"end\":80772,\"start\":80771},{\"end\":80840,\"start\":80839},{\"end\":80842,\"start\":80841},{\"end\":81148,\"start\":81144},{\"end\":81388,\"start\":81387},{\"end\":81397,\"start\":81396},{\"end\":81405,\"start\":81404},{\"end\":81411,\"start\":81410},{\"end\":81582,\"start\":81581},{\"end\":81588,\"start\":81587},{\"end\":81596,\"start\":81595},{\"end\":81742,\"start\":81741},{\"end\":81750,\"start\":81749},{\"end\":81895,\"start\":81894},{\"end\":81905,\"start\":81901},{\"end\":81914,\"start\":81913},{\"end\":82035,\"start\":82034},{\"end\":82042,\"start\":82041},{\"end\":82050,\"start\":82049},{\"end\":82058,\"start\":82057},{\"end\":82066,\"start\":82065},{\"end\":82214,\"start\":82213},{\"end\":82343,\"start\":82342},{\"end\":82349,\"start\":82348},{\"end\":82356,\"start\":82355},{\"end\":82549,\"start\":82548},{\"end\":82556,\"start\":82555},{\"end\":82564,\"start\":82563},{\"end\":82574,\"start\":82573}]", "bib_author_last_name": "[{\"end\":75885,\"start\":75881},{\"end\":75895,\"start\":75889},{\"end\":75903,\"start\":75899},{\"end\":75989,\"start\":75984},{\"end\":75995,\"start\":75993},{\"end\":76002,\"start\":75999},{\"end\":76008,\"start\":76006},{\"end\":76014,\"start\":76012},{\"end\":76022,\"start\":76018},{\"end\":76162,\"start\":76158},{\"end\":76173,\"start\":76168},{\"end\":76180,\"start\":76177},{\"end\":76186,\"start\":76184},{\"end\":76335,\"start\":76332},{\"end\":76472,\"start\":76468},{\"end\":76480,\"start\":76476},{\"end\":76489,\"start\":76484},{\"end\":76498,\"start\":76493},{\"end\":76506,\"start\":76502},{\"end\":76643,\"start\":76640},{\"end\":76650,\"start\":76647},{\"end\":76658,\"start\":76654},{\"end\":76664,\"start\":76662},{\"end\":76784,\"start\":76781},{\"end\":76791,\"start\":76788},{\"end\":76797,\"start\":76795},{\"end\":76911,\"start\":76905},{\"end\":77060,\"start\":77053},{\"end\":77178,\"start\":77175},{\"end\":77308,\"start\":77303},{\"end\":77428,\"start\":77421},{\"end\":77440,\"start\":77434},{\"end\":77454,\"start\":77448},{\"end\":77468,\"start\":77462},{\"end\":77479,\"start\":77472},{\"end\":77604,\"start\":77601},{\"end\":77612,\"start\":77608},{\"end\":77619,\"start\":77616},{\"end\":77625,\"start\":77623},{\"end\":77814,\"start\":77810},{\"end\":77821,\"start\":77818},{\"end\":77830,\"start\":77825},{\"end\":77967,\"start\":77962},{\"end\":77979,\"start\":77974},{\"end\":77987,\"start\":77983},{\"end\":77996,\"start\":77991},{\"end\":78002,\"start\":78000},{\"end\":78140,\"start\":78137},{\"end\":78155,\"start\":78144},{\"end\":78161,\"start\":78159},{\"end\":78168,\"start\":78165},{\"end\":78176,\"start\":78172},{\"end\":78184,\"start\":78180},{\"end\":78397,\"start\":78395},{\"end\":78405,\"start\":78401},{\"end\":78413,\"start\":78409},{\"end\":78422,\"start\":78417},{\"end\":78539,\"start\":78537},{\"end\":78548,\"start\":78543},{\"end\":78557,\"start\":78552},{\"end\":78565,\"start\":78561},{\"end\":78572,\"start\":78569},{\"end\":78711,\"start\":78695},{\"end\":78724,\"start\":78717},{\"end\":78755,\"start\":78730},{\"end\":78771,\"start\":78763},{\"end\":78883,\"start\":78880},{\"end\":78895,\"start\":78887},{\"end\":78904,\"start\":78901},{\"end\":78915,\"start\":78908},{\"end\":78925,\"start\":78919},{\"end\":78982,\"start\":78975},{\"end\":79051,\"start\":79046},{\"end\":79059,\"start\":79055},{\"end\":79226,\"start\":79224},{\"end\":79235,\"start\":79230},{\"end\":79243,\"start\":79239},{\"end\":79252,\"start\":79249},{\"end\":79258,\"start\":79256},{\"end\":79269,\"start\":79262},{\"end\":79372,\"start\":79368},{\"end\":79379,\"start\":79376},{\"end\":79387,\"start\":79383},{\"end\":79395,\"start\":79391},{\"end\":79513,\"start\":79509},{\"end\":79523,\"start\":79519},{\"end\":79531,\"start\":79527},{\"end\":79538,\"start\":79535},{\"end\":79675,\"start\":79670},{\"end\":79682,\"start\":79679},{\"end\":79692,\"start\":79686},{\"end\":79701,\"start\":79696},{\"end\":79707,\"start\":79705},{\"end\":79718,\"start\":79711},{\"end\":79845,\"start\":79841},{\"end\":80008,\"start\":80004},{\"end\":80132,\"start\":80126},{\"end\":80144,\"start\":80136},{\"end\":80352,\"start\":80348},{\"end\":80482,\"start\":80480},{\"end\":80488,\"start\":80486},{\"end\":80495,\"start\":80492},{\"end\":80503,\"start\":80499},{\"end\":80620,\"start\":80610},{\"end\":80632,\"start\":80624},{\"end\":80642,\"start\":80636},{\"end\":80724,\"start\":80719},{\"end\":80740,\"start\":80730},{\"end\":80752,\"start\":80744},{\"end\":80769,\"start\":80758},{\"end\":80784,\"start\":80773},{\"end\":80846,\"start\":80843},{\"end\":81154,\"start\":81149},{\"end\":81394,\"start\":81389},{\"end\":81402,\"start\":81398},{\"end\":81408,\"start\":81406},{\"end\":81415,\"start\":81412},{\"end\":81585,\"start\":81583},{\"end\":81593,\"start\":81589},{\"end\":81600,\"start\":81597},{\"end\":81747,\"start\":81743},{\"end\":81756,\"start\":81751},{\"end\":81899,\"start\":81896},{\"end\":81911,\"start\":81906},{\"end\":81918,\"start\":81915},{\"end\":82039,\"start\":82036},{\"end\":82047,\"start\":82043},{\"end\":82055,\"start\":82051},{\"end\":82063,\"start\":82059},{\"end\":82069,\"start\":82067},{\"end\":82219,\"start\":82215},{\"end\":82346,\"start\":82344},{\"end\":82353,\"start\":82350},{\"end\":82360,\"start\":82357},{\"end\":82553,\"start\":82550},{\"end\":82561,\"start\":82557},{\"end\":82571,\"start\":82565},{\"end\":82584,\"start\":82575}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":67856161},\"end\":75932,\"start\":75782},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":244456138},\"end\":76059,\"start\":75934},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":12825527},\"end\":76229,\"start\":76061},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":237091687},\"end\":76376,\"start\":76231},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":245738387},\"end\":76539,\"start\":76378},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":233388137},\"end\":76694,\"start\":76541},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":34564324},\"end\":76843,\"start\":76696},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52125636},\"end\":76957,\"start\":76845},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":220686914},\"end\":77101,\"start\":76959},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":240556223},\"end\":77221,\"start\":77103},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":225966087},\"end\":77345,\"start\":77223},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":150262479},\"end\":77525,\"start\":77347},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":226417548},\"end\":77663,\"start\":77527},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":236593967},\"end\":77867,\"start\":77665},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":53439920},\"end\":78039,\"start\":77869},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":216366349},\"end\":78225,\"start\":78041},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":125956010},\"end\":78447,\"start\":78227},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7100348},\"end\":78615,\"start\":78449},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":132719178},\"end\":78811,\"start\":78617},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5816789},\"end\":78951,\"start\":78813},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":45998148},\"end\":79009,\"start\":78953},{\"attributes\":{\"doi\":\"10.1109/TKDE.2021.3070203\",\"id\":\"b21\",\"matched_paper_id\":11311635},\"end\":79143,\"start\":79011},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":220793948},\"end\":79310,\"start\":79145},{\"attributes\":{\"id\":\"b23\"},\"end\":79443,\"start\":79312},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":221377355},\"end\":79571,\"start\":79445},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":243906424},\"end\":79748,\"start\":79573},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":235509562},\"end\":79887,\"start\":79750},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":226437277},\"end\":80045,\"start\":79889},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206631525},\"end\":80188,\"start\":80047},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":236765035},\"end\":80389,\"start\":80190},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":9046390},\"end\":80543,\"start\":80391},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7282331},\"end\":80685,\"start\":80545},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3356463},\"end\":80837,\"start\":80687},{\"attributes\":{\"id\":\"b33\"},\"end\":80957,\"start\":80839},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2307439},\"end\":81193,\"start\":80959},{\"attributes\":{\"id\":\"b35\"},\"end\":81253,\"start\":81195},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":21423442},\"end\":81450,\"start\":81255},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":53285773},\"end\":81642,\"start\":81452},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":238863264},\"end\":81793,\"start\":81644},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":216476666},\"end\":81953,\"start\":81795},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":177127},\"end\":82111,\"start\":81955},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":24938861},\"end\":82260,\"start\":82113},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":237619036},\"end\":82470,\"start\":82262},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":18152199},\"end\":82613,\"start\":82472}]", "bib_title": "[{\"end\":75877,\"start\":75782},{\"end\":75980,\"start\":75934},{\"end\":76154,\"start\":76061},{\"end\":76328,\"start\":76231},{\"end\":76464,\"start\":76378},{\"end\":76636,\"start\":76541},{\"end\":76777,\"start\":76696},{\"end\":76901,\"start\":76845},{\"end\":77047,\"start\":76959},{\"end\":77171,\"start\":77103},{\"end\":77299,\"start\":77223},{\"end\":77413,\"start\":77347},{\"end\":77597,\"start\":77527},{\"end\":77806,\"start\":77665},{\"end\":77958,\"start\":77869},{\"end\":78133,\"start\":78041},{\"end\":78391,\"start\":78227},{\"end\":78533,\"start\":78449},{\"end\":78689,\"start\":78617},{\"end\":78874,\"start\":78813},{\"end\":78971,\"start\":78953},{\"end\":79042,\"start\":79011},{\"end\":79220,\"start\":79145},{\"end\":79364,\"start\":79312},{\"end\":79505,\"start\":79445},{\"end\":79666,\"start\":79573},{\"end\":79837,\"start\":79750},{\"end\":80000,\"start\":79889},{\"end\":80122,\"start\":80047},{\"end\":80344,\"start\":80190},{\"end\":80476,\"start\":80391},{\"end\":80606,\"start\":80545},{\"end\":80715,\"start\":80687},{\"end\":81142,\"start\":80959},{\"end\":81385,\"start\":81255},{\"end\":81579,\"start\":81452},{\"end\":81739,\"start\":81644},{\"end\":81892,\"start\":81795},{\"end\":82032,\"start\":81955},{\"end\":82211,\"start\":82113},{\"end\":82340,\"start\":82262},{\"end\":82546,\"start\":82472}]", "bib_author": "[{\"end\":75887,\"start\":75879},{\"end\":75897,\"start\":75887},{\"end\":75905,\"start\":75897},{\"end\":75991,\"start\":75982},{\"end\":75997,\"start\":75991},{\"end\":76004,\"start\":75997},{\"end\":76010,\"start\":76004},{\"end\":76016,\"start\":76010},{\"end\":76024,\"start\":76016},{\"end\":76164,\"start\":76156},{\"end\":76175,\"start\":76164},{\"end\":76182,\"start\":76175},{\"end\":76188,\"start\":76182},{\"end\":76337,\"start\":76330},{\"end\":76474,\"start\":76466},{\"end\":76482,\"start\":76474},{\"end\":76491,\"start\":76482},{\"end\":76500,\"start\":76491},{\"end\":76508,\"start\":76500},{\"end\":76645,\"start\":76638},{\"end\":76652,\"start\":76645},{\"end\":76660,\"start\":76652},{\"end\":76666,\"start\":76660},{\"end\":76786,\"start\":76779},{\"end\":76793,\"start\":76786},{\"end\":76799,\"start\":76793},{\"end\":76913,\"start\":76903},{\"end\":77062,\"start\":77049},{\"end\":77180,\"start\":77173},{\"end\":77310,\"start\":77301},{\"end\":77430,\"start\":77415},{\"end\":77442,\"start\":77430},{\"end\":77456,\"start\":77442},{\"end\":77470,\"start\":77456},{\"end\":77481,\"start\":77470},{\"end\":77606,\"start\":77599},{\"end\":77614,\"start\":77606},{\"end\":77621,\"start\":77614},{\"end\":77627,\"start\":77621},{\"end\":77816,\"start\":77808},{\"end\":77823,\"start\":77816},{\"end\":77832,\"start\":77823},{\"end\":77969,\"start\":77960},{\"end\":77981,\"start\":77969},{\"end\":77989,\"start\":77981},{\"end\":77998,\"start\":77989},{\"end\":78004,\"start\":77998},{\"end\":78142,\"start\":78135},{\"end\":78157,\"start\":78142},{\"end\":78163,\"start\":78157},{\"end\":78170,\"start\":78163},{\"end\":78178,\"start\":78170},{\"end\":78186,\"start\":78178},{\"end\":78399,\"start\":78393},{\"end\":78407,\"start\":78399},{\"end\":78415,\"start\":78407},{\"end\":78424,\"start\":78415},{\"end\":78541,\"start\":78535},{\"end\":78550,\"start\":78541},{\"end\":78559,\"start\":78550},{\"end\":78567,\"start\":78559},{\"end\":78574,\"start\":78567},{\"end\":78713,\"start\":78691},{\"end\":78726,\"start\":78713},{\"end\":78757,\"start\":78726},{\"end\":78773,\"start\":78757},{\"end\":78885,\"start\":78876},{\"end\":78897,\"start\":78885},{\"end\":78906,\"start\":78897},{\"end\":78917,\"start\":78906},{\"end\":78927,\"start\":78917},{\"end\":78984,\"start\":78973},{\"end\":79053,\"start\":79044},{\"end\":79061,\"start\":79053},{\"end\":79228,\"start\":79222},{\"end\":79237,\"start\":79228},{\"end\":79245,\"start\":79237},{\"end\":79254,\"start\":79245},{\"end\":79260,\"start\":79254},{\"end\":79271,\"start\":79260},{\"end\":79374,\"start\":79366},{\"end\":79381,\"start\":79374},{\"end\":79389,\"start\":79381},{\"end\":79397,\"start\":79389},{\"end\":79515,\"start\":79507},{\"end\":79525,\"start\":79515},{\"end\":79533,\"start\":79525},{\"end\":79540,\"start\":79533},{\"end\":79677,\"start\":79668},{\"end\":79684,\"start\":79677},{\"end\":79694,\"start\":79684},{\"end\":79703,\"start\":79694},{\"end\":79709,\"start\":79703},{\"end\":79720,\"start\":79709},{\"end\":79847,\"start\":79839},{\"end\":80010,\"start\":80002},{\"end\":80134,\"start\":80124},{\"end\":80146,\"start\":80134},{\"end\":80354,\"start\":80346},{\"end\":80484,\"start\":80478},{\"end\":80490,\"start\":80484},{\"end\":80497,\"start\":80490},{\"end\":80505,\"start\":80497},{\"end\":80622,\"start\":80608},{\"end\":80634,\"start\":80622},{\"end\":80644,\"start\":80634},{\"end\":80726,\"start\":80717},{\"end\":80742,\"start\":80726},{\"end\":80754,\"start\":80742},{\"end\":80771,\"start\":80754},{\"end\":80786,\"start\":80771},{\"end\":80848,\"start\":80839},{\"end\":81156,\"start\":81144},{\"end\":81396,\"start\":81387},{\"end\":81404,\"start\":81396},{\"end\":81410,\"start\":81404},{\"end\":81417,\"start\":81410},{\"end\":81587,\"start\":81581},{\"end\":81595,\"start\":81587},{\"end\":81602,\"start\":81595},{\"end\":81749,\"start\":81741},{\"end\":81758,\"start\":81749},{\"end\":81901,\"start\":81894},{\"end\":81913,\"start\":81901},{\"end\":81920,\"start\":81913},{\"end\":82041,\"start\":82034},{\"end\":82049,\"start\":82041},{\"end\":82057,\"start\":82049},{\"end\":82065,\"start\":82057},{\"end\":82071,\"start\":82065},{\"end\":82221,\"start\":82213},{\"end\":82348,\"start\":82342},{\"end\":82355,\"start\":82348},{\"end\":82362,\"start\":82355},{\"end\":82555,\"start\":82548},{\"end\":82563,\"start\":82555},{\"end\":82573,\"start\":82563},{\"end\":82586,\"start\":82573}]", "bib_venue": "[{\"end\":75914,\"start\":75905},{\"end\":76046,\"start\":76024},{\"end\":76216,\"start\":76188},{\"end\":76362,\"start\":76337},{\"end\":76525,\"start\":76508},{\"end\":76675,\"start\":76666},{\"end\":76824,\"start\":76799},{\"end\":76938,\"start\":76913},{\"end\":77087,\"start\":77062},{\"end\":77207,\"start\":77180},{\"end\":77332,\"start\":77310},{\"end\":77506,\"start\":77481},{\"end\":77649,\"start\":77627},{\"end\":77854,\"start\":77832},{\"end\":78026,\"start\":78004},{\"end\":78212,\"start\":78186},{\"end\":78432,\"start\":78424},{\"end\":78600,\"start\":78574},{\"end\":78797,\"start\":78773},{\"end\":78937,\"start\":78927},{\"end\":78995,\"start\":78984},{\"end\":79128,\"start\":79086},{\"end\":79296,\"start\":79271},{\"end\":79429,\"start\":79397},{\"end\":79556,\"start\":79540},{\"end\":79729,\"start\":79720},{\"end\":79872,\"start\":79847},{\"end\":80032,\"start\":80010},{\"end\":80174,\"start\":80146},{\"end\":80376,\"start\":80354},{\"end\":80529,\"start\":80505},{\"end\":80670,\"start\":80644},{\"end\":80822,\"start\":80786},{\"end\":80922,\"start\":80848},{\"end\":81179,\"start\":81156},{\"end\":81234,\"start\":81197},{\"end\":81435,\"start\":81417},{\"end\":81622,\"start\":81602},{\"end\":81777,\"start\":81758},{\"end\":81931,\"start\":81920},{\"end\":82097,\"start\":82071},{\"end\":82246,\"start\":82221},{\"end\":82416,\"start\":82362},{\"end\":82600,\"start\":82586},{\"end\":80940,\"start\":80924},{\"end\":81638,\"start\":81624},{\"end\":82466,\"start\":82418}]"}}}, "year": 2023, "month": 12, "day": 17}