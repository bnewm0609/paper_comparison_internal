{"id": 214667019, "updated": "2023-10-06 17:11:00.239", "metadata": {"title": "In defence of metric learning for speaker recognition", "authors": "[{\"first\":\"Joon\",\"last\":\"Chung\",\"middle\":[\"Son\"]},{\"first\":\"Jaesung\",\"last\":\"Huh\",\"middle\":[]},{\"first\":\"Seongkyu\",\"last\":\"Mun\",\"middle\":[]},{\"first\":\"Minjae\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Hee\",\"last\":\"Heo\",\"middle\":[\"Soo\"]},{\"first\":\"Soyeon\",\"last\":\"Choe\",\"middle\":[]},{\"first\":\"Chiheon\",\"last\":\"Ham\",\"middle\":[]},{\"first\":\"Sunghwan\",\"last\":\"Jung\",\"middle\":[]},{\"first\":\"Bong-Jin\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Icksang\",\"last\":\"Han\",\"middle\":[]}]", "venue": "Interspeech 2020", "journal": "Interspeech 2020", "publication_date": {"year": 2020, "month": 3, "day": 26}, "abstract": "The objective of this paper is 'open-set' speaker recognition of unseen speakers, where ideal embeddings should be able to condense information into a compact utterance-level representation that has small intra-class (same speaker) and large inter-class (different speakers) distance. A popular belief in speaker recognition is that networks trained with classification objectives outperform metric learning methods. In this paper, we present an extensive evaluation of most recent loss functions for speaker recognition on the VoxCeleb dataset. We demonstrate that even the vanilla triplet loss shows competitive performance compared to classification-based losses, and those trained with our angular metric learning objective outperform state-of-the-art methods.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2003.11982", "mag": "3095028292", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/ChungHMLHCHJLH20", "doi": "10.21437/interspeech.2020-1064"}}, "content": {"source": {"pdf_hash": "e31badde1d8ffedb473358935be8711ad3141f5b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.11982v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2003.11982", "status": "GREEN"}}, "grobid": {"id": "17c42df7a67ff9e5fdeb0a53e5d267255bf4d733", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e31badde1d8ffedb473358935be8711ad3141f5b.txt", "contents": "\nIn defence of metric learning for speaker recognition\n26 Mar 2020\n\nJoon Son Chung joonson.chung@navercorp.com \nNaver Corporation\nSouth Korea\n\nJaesung Huh \nNaver Corporation\nSouth Korea\n\nSeongkyu Mun \nNaver Corporation\nSouth Korea\n\nMinjae Lee \nNaver Corporation\nSouth Korea\n\nHee Soo Heo \nNaver Corporation\nSouth Korea\n\nSoyeon Choe \nNaver Corporation\nSouth Korea\n\nChiheon Ham \nNaver Corporation\nSouth Korea\n\nSunghwan Jung \nNaver Corporation\nSouth Korea\n\nBong-Jin Lee \nNaver Corporation\nSouth Korea\n\nIcksang Han \nNaver Corporation\nSouth Korea\n\nIn defence of metric learning for speaker recognition\n26 Mar 2020Index Terms: speaker recognition, metric learning\nThe objective of this paper is 'open-set' speaker recognition of unseen speakers, where ideal embeddings should be able to condense information into a compact utterance-level representation that has small intra-class (same speaker) and large inter-class (different speakers) distance.A popular belief in speaker recognition is that networks trained with classification objectives outperform metric learning methods. In this paper, we present an extensive evaluation of most recent loss functions for speaker recognition on the VoxCeleb dataset. We demonstrate that even the vanilla triplet loss shows competitive performance compared to classification-based losses, and those trained with our angular metric learning objective outperform state-of-the-art methods.\n\nIntroduction\n\nResearch on speaker recognition has a long history and has received an increasing amount of attention in recent years. Large-scale datasets for speaker recognition such as the Vox-Celeb [1,2] and Speakers in the Wild [3] have become freely available, facilitating fast progress in the field. Speaker recognition can be categorised into closed-set or open-set settings. For closed-set setting, all testing identities are predefined in training set, therefore can be addressed as a classification problem. For open-set setting, the testing identities are not seen during training, which is close to practice. This is a metric learning problem in which voices must be mapped to a discriminative embedding space. The focus of this research, and most others, are on the latter problem.\n\nPioneering work on speaker recognition using deep neural networks have learnt speaker embeddings via the classification loss [1,4,5]. Since then, the prevailing method has been to use softmax classifiers to train the embeddings [6,7,8]. While the softmax loss can learn separable embeddings, they are not discriminative enough since it is not explicitly designed to optimise embedding similarity. Therefore, softmax-trained models have often been combined with PLDA [9] back-ends to generate scoring functions [5,10].\n\nThis weakness has been addressed by [11] who have proposed angular softmax (A-Softmax) where cosine similarity is used as logit input to the softmax layer, and a number of works have demonstrated its superiority over vanilla softmax in speaker recognition [6,7,8,12,13]. Additive margin variants, AM-Softmax [14,15] and AAM-Softmax [16], have been proposed to increase inter-class variance by introducing a cosine margin penalty to the target logit, and these have been very\n\nThe code for this paper can be found at: https://github.com/clovaai/voxceleb_trainer popular due to their ease of implementation and good performance [17,18,19,20,21,22,23,24]. However, training with AM-Softmax and AAM-Softmax has proven to be challenging since they are sensitive to the value of scale and margin in the loss function.\n\nMetric learning objectives present strong alternatives to the prevailing classification-based methods, by learning embeddings directly. Since open-set speaker recognition is essentially a metric learning problem, the key is to learn features that have small intra-class and large inter-class distance. Contrastive loss [25] and triplet loss [26] have been demonstrated promising performance on speaker recognition [27,28] by optimising the distance metrics directly, but these methods require careful pair or triplet selection which can be time consuming and performance sensitive.\n\nOf closest relevance to our work is prototypical networks [29] that learn a metric space in which open-set classification can be performed by computing distances to prototype representations of each class, with a training procedure that mimics the test scenario. The use of multiple negatives helps to stabilise learning since loss functions can enforce that an embedding is far from all negatives in a batch, rather than one particular negative in the case of triplet loss. [30,31] have adopted the prototypical framework for speaker recognition. Generalised end-to-end loss [32], originally proposed for speaker recognition, is also closely related to this setup.\n\nComparing different loss functions from prior works can be challenging and unreliable, since speaker recognition systems can vary widely in their design. Popular trunk architectures include TDNN-based systems such as x-vector [5] and its deeper counterparts [8], as well as network architectures from the computer vision community such as the ResNet [33]. A range of encoders have been proposed to aggregate frame-level informations into utterance-level embeddings, from simple averaging [1] to statistical pooling [4,7] and dictionary-based encodings [17,34]. [5] has proven that data augmentation can significantly boost speaker recognition performance, but the augmentation methods can range from adding noise [35] to room impulse response (RIR) simulation [36]. Therefore, in order to directly compare a range of loss functions, we conduct over 10,000 GPU-hours of careful experiments while keeping other training details constant. Against popular belief, we demonstrate that the networks trained with vanilla triplet loss show competitive performance compared to most AM-Softmax and AAM-Softmax trained networks, and those trained with our proposed angular objective outperform all comparable methods.\n\nThe experiments in this paper can be reproduced with the PyTorch trainer that is released with this paper.\n\n\nTraining functions\n\nThis section describes the loss functions used in our experiments and proposes a new angular variant of the prototypical loss.\n\n\nClassification objectives\n\nThe VoxCeleb2 development set contains C = 5, 994 speakers or classes. During training, each mini-batch contains N utterances each from different speakers, whose embeddings are xi and the corresponding speaker labels are yi where 1 \u2264 i \u2264 N and 1 \u2264 y \u2264 C.\n\nSoftmax. The softmax loss consists of a softmax function followed by a multi-class cross-entropy loss. It is formulated as:\nL S = \u2212 1 N N i=1 log e W T y i x i +by i C j=1 e W T j x i +b j(1)\nwhere W and b are the weights and bias of the last layer of the trunk architecture, respectively. This loss function only penalises classification error, and does not explicitly enforce intraclass compactness and inter-class separation.\n\nAM-Softmax (CosFace). By normalising the weights and the input vectors, softmax loss can be reformulated such that the posterior probability only relies on cosine of angle between the weights and the input vectors. This loss function, termed by the authors as Normalised Softmax Loss (NSL), is formulated as:\nL N = \u2212 1 N N i=1 log e cos(\u03b8 y i ,i ) j e cos(\u03b8 j,i )(2)\nwhere cos (\u03b8j,i) is the dot product of normalised vector Wj and xi. However, embeddings learned by the NSL are not sufficiently discriminative because the NSL only penalises classification error. In order to mitigate this problem, cosine margin m is incorporated into the equation:\nL C = \u2212 1 N N i=1 log e s(cos(\u03b8 y i ,i )\u2212m) e s(cos(\u03b8 y i ,i )\u2212m) + j =y i e s(cos(\u03b8 j,i ))(3)\nwhere s is a fixed scale factor to prevent gradient from getting too small in training phase.\n\nAAM-Softmax (ArcFace). This is equivalent to CosFace except that there is additive angular margin penalty m between xi and Wy i . The additive angular margin penalty is equal to the geodesic distance margin penalty in the normalised hypersphere.\nL A = \u2212 1 N N i=1 log e s(cos(\u03b8 y i ,i +m)) e s(cos(\u03b8 y i ,i +m)) + j =y i e s(cos(\u03b8 j,i ))(4)\n\nMetric learning objectives\n\nFor metric learning objectives, each mini-batch contains M utterances from each of N different speakers, whose embeddings are xj,i where 1 \u2264 j \u2264 N and 1 \u2264 i \u2264 M .\n\nTriplet. Triplet loss minimises the L2 distance between an anchor and a positive (same identity), and maximises the distance between an anchor and a negative (different identity).\nL T = 1 N N j=1 max(0, x j,0 \u2212 x j,1 2 2 \u2212 x j,0 \u2212 x k =j,1 2 2 + m) (5)\nFor our implementation, the negative utterances are sampled from different speakers within the mini-batch and the sample x k is selected by the hard negative mining function. This requires M = 2 utterances from each speaker.\n\nPrototypical. Each mini-batch contains a support set S and a query set Q. For simplicity, we will assume that the query is M -th utterance from every speaker. Then the prototype (or centroid) is:\nc j = 1 M \u2212 1 M \u22121 m=1 x j,m(6)\nSquared Euclidean distance is used as the distance metric as proposed by the original paper:\nS j,k = x j,M \u2212 c k 2 2(7)\nDuring training, each query example is classified against N speakers based on a softmax over distances to each speaker prototype:\nL P = \u2212 1 N N j=1 log e S j,j N k=1 e S j,k(8)\nHere, Sj,j is the squared Euclidean distance between the query and the prototype of the same speaker from the support set. The softmax function effectively serves the purpose of hard negative mining, since the hardest negative would most affect the gradients. The value of M is typically chosen to match the expected situation at test-time, e.g. M = 5 + 1 for 5-shot learning, so that the prototype is composed of five different utterances. In this way, the task in training exactly matches the task in test scenario.\n\nGeneralised end-to-end (GE2E). In GE2E training, every utterance in the batch except the query itself is used to form centroids. As a result, the centroid that is of the same class as the query is computed from one fewer utterance than centroids of other classes. They are defined as:\ncj = 1 M M m=1 xj,m(9)c (\u2212i) j = 1 M \u2212 1 M m=1 m =i xj,m(10)\nThe similarity matrix is defined as scaled cosine similarity between the embeddings and all centroids:\nS j,i,k = w \u00b7 cos(x j,i , c (\u2212i) j ) + b if k = j w \u00b7 cos(x j,i , c k ) + b otherwise.(11)\nwhere w > 0 and b are learnable scale and bias. The final GE2E loss is defined as:\nL G = \u2212 1 N j,i log e S j,i,j N k=1 e S j,i,k(12)\nAngular Prototypical. The angular prototypical loss uses the same batch formation as the original prototypical loss, reserving one utterance from every class as the query. This has advantages over GE2E-like formation since every centroid is made from the same number of utterances in the support set, therefore it is possible to exactly mimic the test scenario during training.\n\nWe use a cosine-based similarity metric with learnable scale and bias, as in the GE2E loss.\nS j,k = w \u00b7 cos(x j,M , c k ) + b(13)\nUsing the angular loss function introduces scale invariance, improving the robustness of objective against feature variance and demonstrating more stable convergence [37]. The resultant objective is the same as the original prototypical loss, Equation 8.\n\n\nExperiments\n\nIn this section we describe the experimental setup, which is identical across all objectives described in Section 2.\n\n\nInput representations\n\nDuring training, we use a fixed length 2-second temporal segment, extracted randomly from each utterance. Spectrograms are extracted with a hamming window of width 25ms and step 10ms. For the ResNet, the 257-dimensional raw spectrograms are used as the input to the network. For the VGG network, 40-dimensional Mel filterbanks are used as the input. Mean and variance normalisation (MVN) is performed by applying instance normalisation [38] to the network input. Since the VoxCeleb dataset consists mostly of continuous speech, voice activity detection (VAD) is not used in training.\n\n\nTrunk architecture\n\nExperiments are performed on two different trunk architectures described below. These are identical to the two models used and described in [39].\n\n\nVGG-M-40.\n\nThe VGG-M model has been proposed for image classification [40] and adapted for speaker recognition by [1]. The network is known for high efficiency and good classification performance. VGG-M-40 is a modification of the network proposed by [1] to take 40-dimensional filterbanks as inputs instead of the 513-dimensional spectrogram. The temporal average pooling (TAP) layer takes the mean of the features along the time domain in order to produce utterance-level representation.\n\nThin ResNet-34. Residual networks [33] are widely used in image recognition and have recently been applied to speaker recognition [34,2,17]. Thin ResNet-34 is the same as the original ResNet with 34 layers, except using only one-quarter of the channels in each residual block in order to reduce computational cost. The model only has 1.4 million parameters compared to 22 million of the standard ResNet-34. Self-attentive pooling (SAP) [34] is used to aggregate frame-level features into utterance-level representation while paying attention to the frames that are more informative for utterance-level speaker recognition.\n\n\nImplementation details\n\nDatasets. The network is trained on the development set of VoxCeleb2 [2] and evaluated on test set of VoxCeleb1 [1]. Note that the development set of VoxCeleb2 is completely disjoint from the VoxCeleb1 dataset (i.e. no speakers in common).\n\nTraining. Our implementation is based on the PyTorch framework [41] and trained on the NAVER Smart Machine Learning (NSML) platform [42]. The models are trained using a NVIDIA V100 GPU with 32GB memory for 500 epochs. For each epoch, we sample a maximum of 100 utterances from each of the 5,994 identities. We use the Adam optimizer with an initial learning rate of 0.001 decreasing by 10% every 10 epochs. For metric learning objectives, we use the largest batch size that fits on a GPU. For classification objectives, we use a fixed batch size of 200. The training takes approximately one day for the VGG-M-40 model and five days for the Thin ResNet-34 model.\n\nAll experiments were repeated independently three times in order to minimise the effect of random initialisation, and we report mean and standard deviation of the experiments.\n\nData augmentation. No data augmentation is performed during training, apart from the random sampling.\n\nCurriculum learning. The AAM-Softmax loss function demonstrates unstable convergence from random initialisation with larger values of m such as 0.3. Therefore, we start training the model with m = 0.1 and increase it to m = 0.3 after 100 epochs. This strategy is labelled Curriculum in Table 1.\n\nSimilarly, the triplet loss can cause models to diverge if the triplets are too difficult early in the training. We only enable hard negative mining after 100 epochs, at which point the network only sees the most difficult 1% of the negatives.\n\n\nEvaluation\n\nEvaluation protocol. The trained networks are evaluated on the VoxCeleb1 test set. We sample ten 4-second temporal crops at regular intervals from each test segment, and compute the distances between all possible combinations (10 \u00d7 10 = 100) from every pair of segments. The mean of the 100 distances is used as the score. This protocol is in line with that used by [2,39].\n\nResults. The results are given in Table 1. It can be seen that the performance of networks trained with AM-Softmax and AAM-Softmax loss functions can be very sensitive to the value of margin and scale set during training. We iterate over many combinations of m and s to find the optimal value. The model trained with the most common setting (AM-Softmax with m = 0.3 and s = 30) is outperformed by the vanilla triplet loss.\n\nGeneralised end-to-end and prototypical losses show improvements over the triplet loss by using multiple negatives in training. The prototypical networks perform best when the value of M matches the test scenario, removing the necessity for hyperparameter optimisation. The performance of the model trained with the proposed angular objective exceeds that of all classification-based and metric learning methods.\n\nThere are a substantial number of recent works on the Vox-Celeb2 dataset, but we do not compare to these in the table, since the goal of this work is to compare the performance of different loss functions under identical conditions. However, we are unaware of any work that outperforms our method with a similar number of network parameters.\n\nBatch size. The effect of batch size on various loss functions is shown in Table 2. We observe that a bigger batch size has a positive effect on performance for metric learning methods, which can be explained by the ability to sample harder negatives within the batch. We make no such observation for the network trained with classification loss.\n\n\nConclusions\n\nIn this paper, we have presented a case for metric learning in speaker recognition. Our extensive experiments indicate that the GE2E and prototypical networks show superior performance to the popular classification-based methods. We also propose an angular variant of the prototypical networks that outperforms all existing training functions. Finally, we release a flexible PyTorch trainer for large-scale speaker recognition that can be used to facilitate further research in the field.  \n\n\nTriplet [26] m = 0.1, CHNM 4.86 \u00b1 0.15 2.53 \u00b1 0.10 m = 0.2, CHNM 4.67 \u00b1 0.06 2.60 \u00b1 0.02 m = 0.3, CHNM 4.84 \u00b1 0.13 2.66 \u00b1 0.03 m = 0.4, CHNM 4.84 \u00b1 0.08 2.76 \u00b1 0.10 Table 1: Equal Error Rates (EER, %) on the VoxCeleb1 test set. We report the mean and standard deviation of the repeated experiments. CHNM: Curriculum Hard Negative Mining.Objective \n\nHyperparameters \nVGG-M-40 Thin ResNet-34 \nSoftmax \n-\n10.14 \u00b1 0.20 \n5.82 \u00b1 0.47 \n\nAM-Softmax [14] \n\nm = 0.1, s = 15 \n4.86 \u00b1 0.14 \n2.81 \u00b1 0.08 \nm = 0.2, s = 15 \n5.14 \u00b1 0.13 \n2.85 \u00b1 0.07 \nm = 0.3, s = 15 \n5.24 \u00b1 0.08 \n3.08 \u00b1 0.05 \nm = 0.4, s = 15 \n5.22 \u00b1 0.15 \n3.09 \u00b1 0.06 \nm = 0.1, s = 30 \n4.76 \u00b1 0.10 \n2.59 \u00b1 0.09 \nm = 0.2, s = 30 \n4.88 \u00b1 0.03 \n2.40 \u00b1 0.07 \nm = 0.3, s = 30 \n5.19 \u00b1 0.08 \n2.71 \u00b1 0.10 \nm = 0.4, s = 30 \n5.35 \u00b1 0.06 \n2.81 \u00b1 0.10 \nm = 0.1, s = 50 \n5.45 \u00b1 0.06 \n2.99 \u00b1 0.04 \nm = 0.2, s = 50 \n5.28 \u00b1 0.07 \n2.60 \u00b1 0.10 \nm = 0.3, s = 50 \n5.62 \u00b1 0.09 \n2.80 \u00b1 0.09 \nm = 0.4, s = 50 \n5.91 \u00b1 0.12 \n2.96 \u00b1 0.08 \n\nAAM-Softmax [16] \n\nm = 0.1, s = 15 \n4.81 \u00b1 0.03 \n2.78 \u00b1 0.04 \nm = 0.2, s = 15 \n4.88 \u00b1 0.08 \n2.88 \u00b1 0.09 \nm = 0.3, s = 15 \n14.90 \u00b1 0.16 \n3.16 \u00b1 0.05 \n\u2192 Curriculum \n5.00 \u00b1 0.05 \n2.91 \u00b1 0.08 \nm = 0.1, s = 30 \n4.67 \u00b1 0.06 \n2.60 \u00b1 0.07 \nm = 0.2, s = 30 \n4.64 \u00b1 0.04 \n2.36 \u00b1 0.04 \nm = 0.3, s = 30 \n13.25 \u00b1 0.07 \n10.55 \u00b1 0.33 \n\u2192 Curriculum \n4.69 \u00b1 0.02 \n2.39 \u00b1 0.05 \nm = 0.1, s = 50 \n5.27 \u00b1 0.03 \n2.88 \u00b1 0.05 \nm = 0.2, s = 50 \n4.96 \u00b1 0.03 \n2.50 \u00b1 0.05 \nm = 0.3, s = 50 \n10.42 \u00b1 0.12 \n8.79 \u00b1 0.21 \n\u2192 Curriculum \n4.86 \u00b1 0.11 \n2.41 \u00b1 0.08 \n\nGE2E [32] \n\nM = 2 \n4.60 \u00b1 0.04 \n2.56 \u00b1 0.08 \nM = 3 \n4.40 \u00b1 0.08 \n2.52 \u00b1 0.07 \nM = 4 \n4.49 \u00b1 0.05 \n2.59 \u00b1 0.12 \nM = 5 \n4.69 \u00b1 0.09 \n2.78 \u00b1 0.09 \nM = 10 \n5.53 \u00b1 0.04 \n3.68 \u00b1 0.08 \n\nPrototypical [29] \n\nM = 2 \n4.59 \u00b1 0.02 \n2.34 \u00b1 0.08 \nM = 3 \n4.73 \u00b1 0.11 \n2.54 \u00b1 0.07 \nM = 4 \n4.99 \u00b1 0.19 \n2.83 \u00b1 0.04 \nM = 5 \n5.34 \u00b1 0.03 \n3.33 \u00b1 0.11 \n\nAngular Prototypical \n\nM = 2 \n4.29 \u00b1 0.07 \n2.21 \u00b1 0.03 \nM = 3 \n4.30 \u00b1 0.05 \n2.45 \u00b1 0.07 \nM = 4 \n4.53 \u00b1 0.03 \n2.75 \u00b1 0.06 \nM = 5 \n4.73 \u00b1 0.01 \n3.00 \u00b1 0.11 \n\nObjective \nHyperparameters \n200 \n400 \n600 \n800 \nAM-Softmax \nm = 0.2, s = 30 \n2.40 \u00b1 0.07 2.53 \u00b1 0.08 2.49 \u00b1 0.11 2.57 \u00b1 0.07 \nPrototypical \nM = 2 \n2.42 \u00b1 0.04 2.40 \u00b1 0.07 2.34 \u00b1 0.05 2.34 \u00b1 0.08 \nAngular Prototypical M = 2 \n2.37 \u00b1 0.07 2.31 \u00b1 0.05 2.32 \u00b1 0.09 2.21 \u00b1 0.03 \n\n\n\nTable 2 :\n2Effect of training batch size on test performance. Equal Error Rates (EER, %) using the Thin ResNet-34 architecture on the VoxCeleb1 test set. We report the mean and standard deviation of the repeated experiments.\n\nVoxCeleb: a largescale speaker identification dataset. A Nagrani, J S Chung, A Zisserman, A. Nagrani, J. S. Chung, and A. Zisserman, \"VoxCeleb: a large- scale speaker identification dataset,\" in INTERSPEECH, 2017.\n\nVoxCeleb2: Deep speaker recognition. J S Chung, A Nagrani, A Zisserman, in INTERSPEECH. J. S. Chung, A. Nagrani, and A. Zisserman, \"VoxCeleb2: Deep speaker recognition,\" in INTERSPEECH, 2018.\n\nThe speakers in the wild (SITW) speaker recognition database. M Mclaren, L Ferrer, D Castan, A Lawson, INTER-SPEECH. M. McLaren, L. Ferrer, D. Castan, and A. Lawson, \"The speak- ers in the wild (SITW) speaker recognition database,\" in INTER- SPEECH, 2016.\n\nDeep neural network embeddings for text-independent speaker verification. D Snyder, D Garcia-Romero, D Povey, S Khudanpur, D. Snyder, D. Garcia-Romero, D. Povey, and S. Khudanpur, \"Deep neural network embeddings for text-independent speaker verification.\" in Interspeech, 2017, pp. 999-1003.\n\nX-vectors: Robust dnn embeddings for speaker recognition. D Snyder, D Garcia-Romero, G Sell, D Povey, S Khudanpur, Proc. ICASSP. IEEE. ICASSP. IEEED. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan- pur, \"X-vectors: Robust dnn embeddings for speaker recognition,\" in Proc. ICASSP. IEEE, 2018, pp. 5329-5333.\n\nSpeaker recognition from raw waveform with sincnet. M Ravanelli, Y Bengio, IEEE Spoken Language Technology Workshop. IEEEM. Ravanelli and Y. Bengio, \"Speaker recognition from raw wave- form with sincnet,\" in IEEE Spoken Language Technology Work- shop. IEEE, 2018, pp. 1021-1028.\n\nAttentive statistics pooling for deep speaker embedding. K Okabe, T Koshinaka, K Shinoda, in INTERSPEECH. K. Okabe, T. Koshinaka, and K. Shinoda, \"Attentive statistics pooling for deep speaker embedding,\" in INTERSPEECH, 2018.\n\nSpeaker recognition for multi-speaker conversations using x-vectors. D Snyder, D Garcia-Romero, G Sell, A Mccree, D Povey, S Khudanpur, Proc. ICASSP. IEEE. ICASSP. IEEED. Snyder, D. Garcia-Romero, G. Sell, A. McCree, D. Povey, and S. Khudanpur, \"Speaker recognition for multi-speaker conversa- tions using x-vectors,\" in Proc. ICASSP. IEEE, 2019, pp. 5796- 5800.\n\nProbabilistic linear discriminant analysis. S Ioffe, Proc. ECCV. ECCVSpringerS. Ioffe, \"Probabilistic linear discriminant analysis,\" in Proc. ECCV. Springer, 2006, pp. 531-542.\n\nPairwise discriminative neural plda for speaker verification. S Ramoji, V Krishnan, P Singh, S Ganapathy, arXiv:2001.07034arXiv preprintS. Ramoji, V. Krishnan, P. Singh, S. Ganapathy et al., \"Pairwise discriminative neural plda for speaker verification,\" arXiv preprint arXiv:2001.07034, 2020.\n\nSphereface: Deep hypersphere embedding for face recognition. W Liu, Y Wen, Z Yu, M Li, B Raj, L Song, Proc. CVPR. CVPRW. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, \"Sphereface: Deep hypersphere embedding for face recognition,\" in Proc. CVPR, 2017, pp. 212-220.\n\nState-of-the-art speaker recognition for telephone and video speech: the jhu-mit submission for nist sre18. J Villalba, N Chen, D Snyder, D Garcia-Romero, A Mc-Cree, G Sell, J Borgstrom, F Richardson, S Shon, F Grondin, InterspeechJ. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. Mc- Cree, G. Sell, J. Borgstrom, F. Richardson, S. Shon, F. Grondin et al., \"State-of-the-art speaker recognition for telephone and video speech: the jhu-mit submission for nist sre18,\" Interspeech, pp. 1488-1492, 2019.\n\nThe jhu speaker recognition system for the voices 2019 challenge. D Snyder, J Villalba, N Chen, D Povey, G Sell, N Dehak, S Khudanpur, InterspeechD. Snyder, J. Villalba, N. Chen, D. Povey, G. Sell, N. Dehak, and S. Khudanpur, \"The jhu speaker recognition system for the voices 2019 challenge,\" in Interspeech, 2019, pp. 2468-2472.\n\nAdditive margin softmax for face verification. F Wang, J Cheng, W Liu, H Liu, IEEE Signal Processing Letters. 257F. Wang, J. Cheng, W. Liu, and H. Liu, \"Additive margin softmax for face verification,\" IEEE Signal Processing Letters, vol. 25, no. 7, pp. 926-930, 2018.\n\nCosface: Large margin cosine loss for deep face recognition. H Wang, Y Wang, Z Zhou, X Ji, D Gong, J Zhou, Z Li, W Liu, Proc. CVPR. CVPRH. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu, \"Cosface: Large margin cosine loss for deep face recog- nition,\" in Proc. CVPR, 2018, pp. 5265-5274.\n\nArcface: Additive angular margin loss for deep face recognition. J Deng, J Guo, N Xue, S Zafeiriou, Proc. CVPR. CVPRJ. Deng, J. Guo, N. Xue, and S. Zafeiriou, \"Arcface: Additive angular margin loss for deep face recognition,\" in Proc. CVPR, 2019, pp. 4690-4699.\n\nUtterancelevel aggregation for speaker recognition in the wild. W Xie, A Nagrani, J S Chung, A Zisserman, Proc. ICASSP. ICASSPW. Xie, A. Nagrani, J. S. Chung, and A. Zisserman, \"Utterance- level aggregation for speaker recognition in the wild,\" in Proc. ICASSP, 2019.\n\nUnified hypersphere embedding for speaker recognition. M Hajibabaei, D Dai, arXiv:1807.08312arXiv preprintM. Hajibabaei and D. Dai, \"Unified hypersphere embedding for speaker recognition,\" arXiv preprint arXiv:1807.08312, 2018.\n\nLarge margin softmax loss for speaker verification. Y Liu, L He, J Liu, INTERSPEECH. Y. Liu, L. He, and J. Liu, \"Large margin softmax loss for speaker verification,\" in INTERSPEECH, 2019.\n\nX-vector dnn refinement with full-length recordings for speaker recognition. D Garcia-Romero, D Snyder, G Sell, A Mccree, D Povey, S Khudanpur, InterspeechD. Garcia-Romero, D. Snyder, G. Sell, A. McCree, D. Povey, and S. Khudanpur, \"X-vector dnn refinement with full-length record- ings for speaker recognition,\" in Interspeech, 2019, pp. 1493- 1496.\n\nBUT system description to VoxCeleb Speaker Recognition Challenge. H Zeinali, S Wang, A Silnova, P Mat\u011bjka, O Plchot, arXiv:1910.12592arXiv preprintH. Zeinali, S. Wang, A. Silnova, P. Mat\u011bjka, and O. Plchot, \"BUT system description to VoxCeleb Speaker Recognition Challenge 2019,\" arXiv preprint arXiv:1910.12592, 2019.\n\nDropclass and dropadapt: Dropping classes for deep speaker representation learning. C Luu, P Bell, S Renals, arXiv:1910.11643arXiv:2002.0045323arXiv preprintChannel adversarial training for speaker verification and diarizationC. Luu, P. Bell, and S. Renals, \"Channel adversarial train- ing for speaker verification and diarization,\" arXiv preprint arXiv:1910.11643, 2019. [23] --, \"Dropclass and dropadapt: Dropping classes for deep speaker representation learning,\" arXiv preprint arXiv:2002.00453, 2020.\n\nMargin matters: Towards more discriminative deep neural network embeddings for speaker recognition. X Xiang, S Wang, H Huang, Y Qian, K Yu, arXiv:1906.07317arXiv preprintX. Xiang, S. Wang, H. Huang, Y. Qian, and K. Yu, \"Margin mat- ters: Towards more discriminative deep neural network embed- dings for speaker recognition,\" arXiv preprint arXiv:1906.07317, 2019.\n\nLearning a similarity metric discriminatively, with application to face verification. S Chopra, R Hadsell, Y Lecun, Proc. CVPR. CVPRIEEE1S. Chopra, R. Hadsell, and Y. LeCun, \"Learning a similarity met- ric discriminatively, with application to face verification,\" in Proc. CVPR, vol. 1. IEEE, 2005, pp. 539-546.\n\nFacenet: A unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, Proc. CVPR. CVPRF. Schroff, D. Kalenichenko, and J. Philbin, \"Facenet: A unified embedding for face recognition and clustering,\" in Proc. CVPR, 2015.\n\nText-independent speaker verification based on triplet convolutional neural network embeddings. C Zhang, K Koishida, J H Hansen, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 269C. Zhang, K. Koishida, and J. H. Hansen, \"Text-independent speaker verification based on triplet convolutional neural network embeddings,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 9, pp. 1633-1644, 2018.\n\nAttention-based models for text-dependent speaker verification. F R Rahman Chowdhury, Q Wang, I L Moreno, L Wan, Proc. ICASSP. IEEE. ICASSP. IEEEF. R. rahman Chowdhury, Q. Wang, I. L. Moreno, and L. Wan, \"Attention-based models for text-dependent speaker verification,\" in Proc. ICASSP. IEEE, 2018, pp. 5359-5363.\n\nPrototypical networks for few-shot learning. J Snell, K Swersky, R Zemel, J. Snell, K. Swersky, and R. Zemel, \"Prototypical networks for few-shot learning,\" in NIPS, 2017, pp. 4077-4087.\n\nCentroid-based deep metric learning for speaker recognition. J Wang, K.-C Wang, M T Law, F Rudzicz, M Brudno, Proc. ICASSP. IEEE. ICASSP. IEEEJ. Wang, K.-C. Wang, M. T. Law, F. Rudzicz, and M. Brudno, \"Centroid-based deep metric learning for speaker recognition,\" in Proc. ICASSP. IEEE, 2019, pp. 3652-3656.\n\nFew shot speaker recognition using deep neural networks. P Anand, A K Singh, S Srivastava, B Lall, arXiv:1904.08775arXiv preprintP. Anand, A. K. Singh, S. Srivastava, and B. Lall, \"Few shot speaker recognition using deep neural networks,\" arXiv preprint arXiv:1904.08775, 2019.\n\nGeneralized endto-end loss for speaker verification. L Wan, Q Wang, A Papir, I L Moreno, Proc. ICASSP. IEEE. ICASSP. IEEEL. Wan, Q. Wang, A. Papir, and I. L. Moreno, \"Generalized end- to-end loss for speaker verification,\" in Proc. ICASSP. IEEE, 2018, pp. 4879-4883.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. CVPR. CVPRK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. CVPR, 2016.\n\nExploring the encoding layer and loss function in end-to-end speaker and language recognition system. W Cai, J Chen, M Li, Speaker OdysseyW. Cai, J. Chen, and M. Li, \"Exploring the encoding layer and loss function in end-to-end speaker and language recognition system,\" in Speaker Odyssey, 2018.\n\nMusan: A music, speech, and noise corpus. D Snyder, G Chen, D Povey, arXiv:1510.08484arXiv preprintD. Snyder, G. Chen, and D. Povey, \"Musan: A music, speech, and noise corpus,\" arXiv preprint arXiv:1510.08484, 2015.\n\nImage method for efficiently simulating small-room acoustics. J B Allen, D A Berkley, The Journal of the Acoustical Society of America. 654J. B. Allen and D. A. Berkley, \"Image method for efficiently sim- ulating small-room acoustics,\" The Journal of the Acoustical So- ciety of America, vol. 65, no. 4, pp. 943-950, 1979.\n\nDeep metric learning with angular loss. J Wang, F Zhou, S Wen, X Liu, Y Lin, Proc. ICCV. ICCVJ. Wang, F. Zhou, S. Wen, X. Liu, and Y. Lin, \"Deep metric learn- ing with angular loss,\" in Proc. ICCV, 2017, pp. 2593-2601.\n\nInstance normalization: The missing ingredient for fast stylization. D Ulyanov, A Vedaldi, V Lempitsky, arXiv:1607.08022arXiv preprintD. Ulyanov, A. Vedaldi, and V. Lempitsky, \"Instance normaliza- tion: The missing ingredient for fast stylization,\" arXiv preprint arXiv:1607.08022, 2016.\n\nDelving into Vox-Celeb: environment invariant speaker recognition. J S Chung, J Huh, S Mun, arXiv:1910.11238arXiv preprintJ. S. Chung, J. Huh, and S. Mun, \"Delving into Vox- Celeb: environment invariant speaker recognition,\" arXiv preprint arXiv:1910.11238, 2019.\n\nReturn of the devil in the details: Delving deep into convolutional nets. K Chatfield, K Simonyan, A Vedaldi, A Zisserman, Proc. BMVC. BMVCK. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, \"Re- turn of the devil in the details: Delving deep into convolutional nets,\" in Proc. BMVC., 2014.\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, NIPS. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \"Pytorch: An imperative style, high-performance deep learning library,\" in NIPS, 2019, pp. 8024-8035.\n\nNsml: A machine learning platform that enables you to focus on your models. N Sung, M Kim, H Jo, Y Yang, J Kim, L Lausen, Y Kim, G Lee, D Kwak, J.-W Ha, arXiv:1712.05902arXiv preprintN. Sung, M. Kim, H. Jo, Y. Yang, J. Kim, L. Lausen, Y. Kim, G. Lee, D. Kwak, J.-W. Ha et al., \"Nsml: A machine learning platform that enables you to focus on your models,\" arXiv preprint arXiv:1712.05902, 2017.\n", "annotations": {"author": "[{\"end\":142,\"start\":68},{\"end\":186,\"start\":143},{\"end\":231,\"start\":187},{\"end\":274,\"start\":232},{\"end\":318,\"start\":275},{\"end\":362,\"start\":319},{\"end\":406,\"start\":363},{\"end\":452,\"start\":407},{\"end\":497,\"start\":453},{\"end\":541,\"start\":498}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":77},{\"end\":154,\"start\":151},{\"end\":199,\"start\":196},{\"end\":242,\"start\":239},{\"end\":286,\"start\":283},{\"end\":330,\"start\":326},{\"end\":374,\"start\":371},{\"end\":420,\"start\":416},{\"end\":465,\"start\":462},{\"end\":509,\"start\":506}]", "author_first_name": "[{\"end\":72,\"start\":68},{\"end\":76,\"start\":73},{\"end\":150,\"start\":143},{\"end\":195,\"start\":187},{\"end\":238,\"start\":232},{\"end\":278,\"start\":275},{\"end\":282,\"start\":279},{\"end\":325,\"start\":319},{\"end\":370,\"start\":363},{\"end\":415,\"start\":407},{\"end\":461,\"start\":453},{\"end\":505,\"start\":498}]", "author_affiliation": "[{\"end\":141,\"start\":112},{\"end\":185,\"start\":156},{\"end\":230,\"start\":201},{\"end\":273,\"start\":244},{\"end\":317,\"start\":288},{\"end\":361,\"start\":332},{\"end\":405,\"start\":376},{\"end\":451,\"start\":422},{\"end\":496,\"start\":467},{\"end\":540,\"start\":511}]", "title": "[{\"end\":54,\"start\":1},{\"end\":595,\"start\":542}]", "venue": null, "abstract": "[{\"end\":1420,\"start\":657}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1625,\"start\":1622},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1627,\"start\":1625},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1656,\"start\":1653},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2346,\"start\":2343},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2348,\"start\":2346},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2350,\"start\":2348},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2449,\"start\":2446},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2451,\"start\":2449},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2453,\"start\":2451},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2687,\"start\":2684},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2731,\"start\":2728},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2734,\"start\":2731},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2777,\"start\":2773},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2996,\"start\":2993},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2998,\"start\":2996},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3000,\"start\":2998},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3003,\"start\":3000},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3006,\"start\":3003},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3049,\"start\":3045},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3052,\"start\":3049},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3073,\"start\":3069},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3367,\"start\":3363},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3370,\"start\":3367},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3373,\"start\":3370},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3376,\"start\":3373},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3379,\"start\":3376},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3382,\"start\":3379},{\"end\":3385,\"start\":3382},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3388,\"start\":3385},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3873,\"start\":3869},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3895,\"start\":3891},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3968,\"start\":3964},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3971,\"start\":3968},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4195,\"start\":4191},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4612,\"start\":4608},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4615,\"start\":4612},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4713,\"start\":4709},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5029,\"start\":5026},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5061,\"start\":5058},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5154,\"start\":5150},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5291,\"start\":5288},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5318,\"start\":5315},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5320,\"start\":5318},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5356,\"start\":5352},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5359,\"start\":5356},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5364,\"start\":5361},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5517,\"start\":5513},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5564,\"start\":5560},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11227,\"start\":11223},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11909,\"start\":11905},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12219,\"start\":12215},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12297,\"start\":12293},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12340,\"start\":12337},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12477,\"start\":12474},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12752,\"start\":12748},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12848,\"start\":12844},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12850,\"start\":12848},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12853,\"start\":12850},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13154,\"start\":13150},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13435,\"start\":13432},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13478,\"start\":13475},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13671,\"start\":13667},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13740,\"start\":13736},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15470,\"start\":15467},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15473,\"start\":15470}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19770,\"start\":17510},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":19996,\"start\":19771}]", "paragraph": "[{\"end\":2216,\"start\":1436},{\"end\":2735,\"start\":2218},{\"end\":3211,\"start\":2737},{\"end\":3548,\"start\":3213},{\"end\":4131,\"start\":3550},{\"end\":4798,\"start\":4133},{\"end\":6006,\"start\":4800},{\"end\":6114,\"start\":6008},{\"end\":6263,\"start\":6137},{\"end\":6547,\"start\":6293},{\"end\":6672,\"start\":6549},{\"end\":6977,\"start\":6741},{\"end\":7287,\"start\":6979},{\"end\":7627,\"start\":7346},{\"end\":7816,\"start\":7723},{\"end\":8063,\"start\":7818},{\"end\":8350,\"start\":8188},{\"end\":8531,\"start\":8352},{\"end\":8829,\"start\":8605},{\"end\":9026,\"start\":8831},{\"end\":9151,\"start\":9059},{\"end\":9308,\"start\":9179},{\"end\":9873,\"start\":9356},{\"end\":10159,\"start\":9875},{\"end\":10323,\"start\":10221},{\"end\":10497,\"start\":10415},{\"end\":10925,\"start\":10548},{\"end\":11018,\"start\":10927},{\"end\":11311,\"start\":11057},{\"end\":11443,\"start\":11327},{\"end\":12052,\"start\":11469},{\"end\":12220,\"start\":12075},{\"end\":12712,\"start\":12234},{\"end\":13336,\"start\":12714},{\"end\":13602,\"start\":13363},{\"end\":14265,\"start\":13604},{\"end\":14442,\"start\":14267},{\"end\":14545,\"start\":14444},{\"end\":14841,\"start\":14547},{\"end\":15086,\"start\":14843},{\"end\":15474,\"start\":15101},{\"end\":15898,\"start\":15476},{\"end\":16312,\"start\":15900},{\"end\":16655,\"start\":16314},{\"end\":17003,\"start\":16657},{\"end\":17509,\"start\":17019}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6740,\"start\":6673},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7345,\"start\":7288},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7722,\"start\":7628},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8158,\"start\":8064},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8604,\"start\":8532},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9058,\"start\":9027},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9178,\"start\":9152},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9355,\"start\":9309},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10182,\"start\":10160},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10220,\"start\":10182},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10414,\"start\":10324},{\"attributes\":{\"id\":\"formula_11\"},\"end\":10547,\"start\":10498},{\"attributes\":{\"id\":\"formula_12\"},\"end\":11056,\"start\":11019}]", "table_ref": "[{\"end\":14840,\"start\":14833},{\"end\":15517,\"start\":15510},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16739,\"start\":16732}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1434,\"start\":1422},{\"attributes\":{\"n\":\"2.\"},\"end\":6135,\"start\":6117},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6291,\"start\":6266},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8186,\"start\":8160},{\"attributes\":{\"n\":\"3.\"},\"end\":11325,\"start\":11314},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11467,\"start\":11446},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12073,\"start\":12055},{\"end\":12232,\"start\":12223},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13361,\"start\":13339},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15099,\"start\":15089},{\"attributes\":{\"n\":\"4.\"},\"end\":17017,\"start\":17006},{\"end\":19781,\"start\":19772}]", "table": "[{\"end\":19770,\"start\":17849}]", "figure_caption": "[{\"end\":17849,\"start\":17512},{\"end\":19996,\"start\":19783}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":20054,\"start\":20053},{\"end\":20065,\"start\":20064},{\"end\":20067,\"start\":20066},{\"end\":20076,\"start\":20075},{\"end\":20251,\"start\":20250},{\"end\":20253,\"start\":20252},{\"end\":20262,\"start\":20261},{\"end\":20273,\"start\":20272},{\"end\":20469,\"start\":20468},{\"end\":20480,\"start\":20479},{\"end\":20490,\"start\":20489},{\"end\":20500,\"start\":20499},{\"end\":20738,\"start\":20737},{\"end\":20748,\"start\":20747},{\"end\":20765,\"start\":20764},{\"end\":20774,\"start\":20773},{\"end\":21015,\"start\":21014},{\"end\":21025,\"start\":21024},{\"end\":21042,\"start\":21041},{\"end\":21050,\"start\":21049},{\"end\":21059,\"start\":21058},{\"end\":21329,\"start\":21328},{\"end\":21342,\"start\":21341},{\"end\":21614,\"start\":21613},{\"end\":21623,\"start\":21622},{\"end\":21636,\"start\":21635},{\"end\":21854,\"start\":21853},{\"end\":21864,\"start\":21863},{\"end\":21881,\"start\":21880},{\"end\":21889,\"start\":21888},{\"end\":21899,\"start\":21898},{\"end\":21908,\"start\":21907},{\"end\":22193,\"start\":22192},{\"end\":22389,\"start\":22388},{\"end\":22399,\"start\":22398},{\"end\":22411,\"start\":22410},{\"end\":22420,\"start\":22419},{\"end\":22683,\"start\":22682},{\"end\":22690,\"start\":22689},{\"end\":22697,\"start\":22696},{\"end\":22703,\"start\":22702},{\"end\":22709,\"start\":22708},{\"end\":22716,\"start\":22715},{\"end\":22997,\"start\":22996},{\"end\":23009,\"start\":23008},{\"end\":23017,\"start\":23016},{\"end\":23027,\"start\":23026},{\"end\":23044,\"start\":23043},{\"end\":23055,\"start\":23054},{\"end\":23063,\"start\":23062},{\"end\":23076,\"start\":23075},{\"end\":23090,\"start\":23089},{\"end\":23098,\"start\":23097},{\"end\":23461,\"start\":23460},{\"end\":23471,\"start\":23470},{\"end\":23483,\"start\":23482},{\"end\":23491,\"start\":23490},{\"end\":23500,\"start\":23499},{\"end\":23508,\"start\":23507},{\"end\":23517,\"start\":23516},{\"end\":23774,\"start\":23773},{\"end\":23782,\"start\":23781},{\"end\":23791,\"start\":23790},{\"end\":23798,\"start\":23797},{\"end\":24057,\"start\":24056},{\"end\":24065,\"start\":24064},{\"end\":24073,\"start\":24072},{\"end\":24081,\"start\":24080},{\"end\":24087,\"start\":24086},{\"end\":24095,\"start\":24094},{\"end\":24103,\"start\":24102},{\"end\":24109,\"start\":24108},{\"end\":24370,\"start\":24369},{\"end\":24378,\"start\":24377},{\"end\":24385,\"start\":24384},{\"end\":24392,\"start\":24391},{\"end\":24632,\"start\":24631},{\"end\":24639,\"start\":24638},{\"end\":24650,\"start\":24649},{\"end\":24652,\"start\":24651},{\"end\":24661,\"start\":24660},{\"end\":24892,\"start\":24891},{\"end\":24906,\"start\":24905},{\"end\":25118,\"start\":25117},{\"end\":25125,\"start\":25124},{\"end\":25131,\"start\":25130},{\"end\":25332,\"start\":25331},{\"end\":25349,\"start\":25348},{\"end\":25359,\"start\":25358},{\"end\":25367,\"start\":25366},{\"end\":25377,\"start\":25376},{\"end\":25386,\"start\":25385},{\"end\":25673,\"start\":25672},{\"end\":25684,\"start\":25683},{\"end\":25692,\"start\":25691},{\"end\":25703,\"start\":25702},{\"end\":25714,\"start\":25713},{\"end\":26011,\"start\":26010},{\"end\":26018,\"start\":26017},{\"end\":26026,\"start\":26025},{\"end\":26534,\"start\":26533},{\"end\":26543,\"start\":26542},{\"end\":26551,\"start\":26550},{\"end\":26560,\"start\":26559},{\"end\":26568,\"start\":26567},{\"end\":26885,\"start\":26884},{\"end\":26895,\"start\":26894},{\"end\":26906,\"start\":26905},{\"end\":27178,\"start\":27177},{\"end\":27189,\"start\":27188},{\"end\":27205,\"start\":27204},{\"end\":27463,\"start\":27462},{\"end\":27472,\"start\":27471},{\"end\":27484,\"start\":27483},{\"end\":27486,\"start\":27485},{\"end\":27870,\"start\":27869},{\"end\":27872,\"start\":27871},{\"end\":27892,\"start\":27891},{\"end\":27900,\"start\":27899},{\"end\":27902,\"start\":27901},{\"end\":27912,\"start\":27911},{\"end\":28166,\"start\":28165},{\"end\":28175,\"start\":28174},{\"end\":28186,\"start\":28185},{\"end\":28370,\"start\":28369},{\"end\":28381,\"start\":28377},{\"end\":28389,\"start\":28388},{\"end\":28391,\"start\":28390},{\"end\":28398,\"start\":28397},{\"end\":28409,\"start\":28408},{\"end\":28675,\"start\":28674},{\"end\":28684,\"start\":28683},{\"end\":28686,\"start\":28685},{\"end\":28695,\"start\":28694},{\"end\":28709,\"start\":28708},{\"end\":28950,\"start\":28949},{\"end\":28957,\"start\":28956},{\"end\":28965,\"start\":28964},{\"end\":28974,\"start\":28973},{\"end\":28976,\"start\":28975},{\"end\":29211,\"start\":29210},{\"end\":29217,\"start\":29216},{\"end\":29226,\"start\":29225},{\"end\":29233,\"start\":29232},{\"end\":29465,\"start\":29464},{\"end\":29472,\"start\":29471},{\"end\":29480,\"start\":29479},{\"end\":29702,\"start\":29701},{\"end\":29712,\"start\":29711},{\"end\":29720,\"start\":29719},{\"end\":29939,\"start\":29938},{\"end\":29941,\"start\":29940},{\"end\":29950,\"start\":29949},{\"end\":29952,\"start\":29951},{\"end\":30241,\"start\":30240},{\"end\":30249,\"start\":30248},{\"end\":30257,\"start\":30256},{\"end\":30264,\"start\":30263},{\"end\":30271,\"start\":30270},{\"end\":30490,\"start\":30489},{\"end\":30501,\"start\":30500},{\"end\":30512,\"start\":30511},{\"end\":30777,\"start\":30776},{\"end\":30779,\"start\":30778},{\"end\":30788,\"start\":30787},{\"end\":30795,\"start\":30794},{\"end\":31049,\"start\":31048},{\"end\":31062,\"start\":31061},{\"end\":31074,\"start\":31073},{\"end\":31085,\"start\":31084},{\"end\":31342,\"start\":31341},{\"end\":31352,\"start\":31351},{\"end\":31361,\"start\":31360},{\"end\":31370,\"start\":31369},{\"end\":31379,\"start\":31378},{\"end\":31391,\"start\":31390},{\"end\":31401,\"start\":31400},{\"end\":31412,\"start\":31411},{\"end\":31419,\"start\":31418},{\"end\":31433,\"start\":31432},{\"end\":31746,\"start\":31745},{\"end\":31754,\"start\":31753},{\"end\":31761,\"start\":31760},{\"end\":31767,\"start\":31766},{\"end\":31775,\"start\":31774},{\"end\":31782,\"start\":31781},{\"end\":31792,\"start\":31791},{\"end\":31799,\"start\":31798},{\"end\":31806,\"start\":31805},{\"end\":31817,\"start\":31813}]", "bib_author_last_name": "[{\"end\":20062,\"start\":20055},{\"end\":20073,\"start\":20068},{\"end\":20086,\"start\":20077},{\"end\":20259,\"start\":20254},{\"end\":20270,\"start\":20263},{\"end\":20283,\"start\":20274},{\"end\":20477,\"start\":20470},{\"end\":20487,\"start\":20481},{\"end\":20497,\"start\":20491},{\"end\":20507,\"start\":20501},{\"end\":20745,\"start\":20739},{\"end\":20762,\"start\":20749},{\"end\":20771,\"start\":20766},{\"end\":20784,\"start\":20775},{\"end\":21022,\"start\":21016},{\"end\":21039,\"start\":21026},{\"end\":21047,\"start\":21043},{\"end\":21056,\"start\":21051},{\"end\":21069,\"start\":21060},{\"end\":21339,\"start\":21330},{\"end\":21349,\"start\":21343},{\"end\":21620,\"start\":21615},{\"end\":21633,\"start\":21624},{\"end\":21644,\"start\":21637},{\"end\":21861,\"start\":21855},{\"end\":21878,\"start\":21865},{\"end\":21886,\"start\":21882},{\"end\":21896,\"start\":21890},{\"end\":21905,\"start\":21900},{\"end\":21918,\"start\":21909},{\"end\":22199,\"start\":22194},{\"end\":22396,\"start\":22390},{\"end\":22408,\"start\":22400},{\"end\":22417,\"start\":22412},{\"end\":22430,\"start\":22421},{\"end\":22687,\"start\":22684},{\"end\":22694,\"start\":22691},{\"end\":22700,\"start\":22698},{\"end\":22706,\"start\":22704},{\"end\":22713,\"start\":22710},{\"end\":22721,\"start\":22717},{\"end\":23006,\"start\":22998},{\"end\":23014,\"start\":23010},{\"end\":23024,\"start\":23018},{\"end\":23041,\"start\":23028},{\"end\":23052,\"start\":23045},{\"end\":23060,\"start\":23056},{\"end\":23073,\"start\":23064},{\"end\":23087,\"start\":23077},{\"end\":23095,\"start\":23091},{\"end\":23106,\"start\":23099},{\"end\":23468,\"start\":23462},{\"end\":23480,\"start\":23472},{\"end\":23488,\"start\":23484},{\"end\":23497,\"start\":23492},{\"end\":23505,\"start\":23501},{\"end\":23514,\"start\":23509},{\"end\":23527,\"start\":23518},{\"end\":23779,\"start\":23775},{\"end\":23788,\"start\":23783},{\"end\":23795,\"start\":23792},{\"end\":23802,\"start\":23799},{\"end\":24062,\"start\":24058},{\"end\":24070,\"start\":24066},{\"end\":24078,\"start\":24074},{\"end\":24084,\"start\":24082},{\"end\":24092,\"start\":24088},{\"end\":24100,\"start\":24096},{\"end\":24106,\"start\":24104},{\"end\":24113,\"start\":24110},{\"end\":24375,\"start\":24371},{\"end\":24382,\"start\":24379},{\"end\":24389,\"start\":24386},{\"end\":24402,\"start\":24393},{\"end\":24636,\"start\":24633},{\"end\":24647,\"start\":24640},{\"end\":24658,\"start\":24653},{\"end\":24671,\"start\":24662},{\"end\":24903,\"start\":24893},{\"end\":24910,\"start\":24907},{\"end\":25122,\"start\":25119},{\"end\":25128,\"start\":25126},{\"end\":25135,\"start\":25132},{\"end\":25346,\"start\":25333},{\"end\":25356,\"start\":25350},{\"end\":25364,\"start\":25360},{\"end\":25374,\"start\":25368},{\"end\":25383,\"start\":25378},{\"end\":25396,\"start\":25387},{\"end\":25681,\"start\":25674},{\"end\":25689,\"start\":25685},{\"end\":25700,\"start\":25693},{\"end\":25711,\"start\":25704},{\"end\":25721,\"start\":25715},{\"end\":26015,\"start\":26012},{\"end\":26023,\"start\":26019},{\"end\":26033,\"start\":26027},{\"end\":26540,\"start\":26535},{\"end\":26548,\"start\":26544},{\"end\":26557,\"start\":26552},{\"end\":26565,\"start\":26561},{\"end\":26571,\"start\":26569},{\"end\":26892,\"start\":26886},{\"end\":26903,\"start\":26896},{\"end\":26912,\"start\":26907},{\"end\":27186,\"start\":27179},{\"end\":27202,\"start\":27190},{\"end\":27213,\"start\":27206},{\"end\":27469,\"start\":27464},{\"end\":27481,\"start\":27473},{\"end\":27493,\"start\":27487},{\"end\":27889,\"start\":27873},{\"end\":27897,\"start\":27893},{\"end\":27909,\"start\":27903},{\"end\":27916,\"start\":27913},{\"end\":28172,\"start\":28167},{\"end\":28183,\"start\":28176},{\"end\":28192,\"start\":28187},{\"end\":28375,\"start\":28371},{\"end\":28386,\"start\":28382},{\"end\":28395,\"start\":28392},{\"end\":28406,\"start\":28399},{\"end\":28416,\"start\":28410},{\"end\":28681,\"start\":28676},{\"end\":28692,\"start\":28687},{\"end\":28706,\"start\":28696},{\"end\":28714,\"start\":28710},{\"end\":28954,\"start\":28951},{\"end\":28962,\"start\":28958},{\"end\":28971,\"start\":28966},{\"end\":28983,\"start\":28977},{\"end\":29214,\"start\":29212},{\"end\":29223,\"start\":29218},{\"end\":29230,\"start\":29227},{\"end\":29237,\"start\":29234},{\"end\":29469,\"start\":29466},{\"end\":29477,\"start\":29473},{\"end\":29483,\"start\":29481},{\"end\":29709,\"start\":29703},{\"end\":29717,\"start\":29713},{\"end\":29726,\"start\":29721},{\"end\":29947,\"start\":29942},{\"end\":29960,\"start\":29953},{\"end\":30246,\"start\":30242},{\"end\":30254,\"start\":30250},{\"end\":30261,\"start\":30258},{\"end\":30268,\"start\":30265},{\"end\":30275,\"start\":30272},{\"end\":30498,\"start\":30491},{\"end\":30509,\"start\":30502},{\"end\":30522,\"start\":30513},{\"end\":30785,\"start\":30780},{\"end\":30792,\"start\":30789},{\"end\":30799,\"start\":30796},{\"end\":31059,\"start\":31050},{\"end\":31071,\"start\":31063},{\"end\":31082,\"start\":31075},{\"end\":31095,\"start\":31086},{\"end\":31349,\"start\":31343},{\"end\":31358,\"start\":31353},{\"end\":31367,\"start\":31362},{\"end\":31376,\"start\":31371},{\"end\":31388,\"start\":31380},{\"end\":31398,\"start\":31392},{\"end\":31409,\"start\":31402},{\"end\":31416,\"start\":31413},{\"end\":31430,\"start\":31420},{\"end\":31440,\"start\":31434},{\"end\":31751,\"start\":31747},{\"end\":31758,\"start\":31755},{\"end\":31764,\"start\":31762},{\"end\":31772,\"start\":31768},{\"end\":31779,\"start\":31776},{\"end\":31789,\"start\":31783},{\"end\":31796,\"start\":31793},{\"end\":31803,\"start\":31800},{\"end\":31811,\"start\":31807},{\"end\":31820,\"start\":31818}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":20211,\"start\":19998},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":49211906},\"end\":20404,\"start\":20213},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":22042603},\"end\":20661,\"start\":20406},{\"attributes\":{\"id\":\"b3\"},\"end\":20954,\"start\":20663},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":46954166},\"end\":21274,\"start\":20956},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51892530},\"end\":21554,\"start\":21276},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4407761},\"end\":21782,\"start\":21556},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":146117456},\"end\":22146,\"start\":21784},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":41807584},\"end\":22324,\"start\":22148},{\"attributes\":{\"doi\":\"arXiv:2001.07034\",\"id\":\"b9\"},\"end\":22619,\"start\":22326},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206596594},\"end\":22886,\"start\":22621},{\"attributes\":{\"id\":\"b11\"},\"end\":23392,\"start\":22888},{\"attributes\":{\"id\":\"b12\"},\"end\":23724,\"start\":23394},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9683805},\"end\":23993,\"start\":23726},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":68589},\"end\":24302,\"start\":23995},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8923541},\"end\":24565,\"start\":24304},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":67856245},\"end\":24834,\"start\":24567},{\"attributes\":{\"doi\":\"arXiv:1807.08312\",\"id\":\"b17\"},\"end\":25063,\"start\":24836},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":102351511},\"end\":25252,\"start\":25065},{\"attributes\":{\"id\":\"b19\"},\"end\":25604,\"start\":25254},{\"attributes\":{\"doi\":\"arXiv:1910.12592\",\"id\":\"b20\"},\"end\":25924,\"start\":25606},{\"attributes\":{\"doi\":\"arXiv:1910.11643\",\"id\":\"b21\"},\"end\":26431,\"start\":25926},{\"attributes\":{\"id\":\"b22\"},\"end\":26796,\"start\":26433},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5555257},\"end\":27109,\"start\":26798},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206592766},\"end\":27364,\"start\":27111},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":46932213},\"end\":27803,\"start\":27366},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2148670},\"end\":28118,\"start\":27805},{\"attributes\":{\"id\":\"b27\"},\"end\":28306,\"start\":28120},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":59608822},\"end\":28615,\"start\":28308},{\"attributes\":{\"id\":\"b29\"},\"end\":28894,\"start\":28617},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":22987563},\"end\":29162,\"start\":28896},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206594692},\"end\":29360,\"start\":29164},{\"attributes\":{\"id\":\"b32\"},\"end\":29657,\"start\":29362},{\"attributes\":{\"id\":\"b33\"},\"end\":29874,\"start\":29659},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":10721495},\"end\":30198,\"start\":29876},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":23083969},\"end\":30418,\"start\":30200},{\"attributes\":{\"id\":\"b36\"},\"end\":30707,\"start\":30420},{\"attributes\":{\"id\":\"b37\"},\"end\":30972,\"start\":30709},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":7204540},\"end\":31269,\"start\":30974},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":202786778},\"end\":31667,\"start\":31271},{\"attributes\":{\"id\":\"b40\"},\"end\":32062,\"start\":31669}]", "bib_title": "[{\"end\":20248,\"start\":20213},{\"end\":20466,\"start\":20406},{\"end\":21012,\"start\":20956},{\"end\":21326,\"start\":21276},{\"end\":21611,\"start\":21556},{\"end\":21851,\"start\":21784},{\"end\":22190,\"start\":22148},{\"end\":22680,\"start\":22621},{\"end\":23771,\"start\":23726},{\"end\":24054,\"start\":23995},{\"end\":24367,\"start\":24304},{\"end\":24629,\"start\":24567},{\"end\":25115,\"start\":25065},{\"end\":26882,\"start\":26798},{\"end\":27175,\"start\":27111},{\"end\":27460,\"start\":27366},{\"end\":27867,\"start\":27805},{\"end\":28367,\"start\":28308},{\"end\":28947,\"start\":28896},{\"end\":29208,\"start\":29164},{\"end\":29936,\"start\":29876},{\"end\":30238,\"start\":30200},{\"end\":31046,\"start\":30974},{\"end\":31339,\"start\":31271}]", "bib_author": "[{\"end\":20064,\"start\":20053},{\"end\":20075,\"start\":20064},{\"end\":20088,\"start\":20075},{\"end\":20261,\"start\":20250},{\"end\":20272,\"start\":20261},{\"end\":20285,\"start\":20272},{\"end\":20479,\"start\":20468},{\"end\":20489,\"start\":20479},{\"end\":20499,\"start\":20489},{\"end\":20509,\"start\":20499},{\"end\":20747,\"start\":20737},{\"end\":20764,\"start\":20747},{\"end\":20773,\"start\":20764},{\"end\":20786,\"start\":20773},{\"end\":21024,\"start\":21014},{\"end\":21041,\"start\":21024},{\"end\":21049,\"start\":21041},{\"end\":21058,\"start\":21049},{\"end\":21071,\"start\":21058},{\"end\":21341,\"start\":21328},{\"end\":21351,\"start\":21341},{\"end\":21622,\"start\":21613},{\"end\":21635,\"start\":21622},{\"end\":21646,\"start\":21635},{\"end\":21863,\"start\":21853},{\"end\":21880,\"start\":21863},{\"end\":21888,\"start\":21880},{\"end\":21898,\"start\":21888},{\"end\":21907,\"start\":21898},{\"end\":21920,\"start\":21907},{\"end\":22201,\"start\":22192},{\"end\":22398,\"start\":22388},{\"end\":22410,\"start\":22398},{\"end\":22419,\"start\":22410},{\"end\":22432,\"start\":22419},{\"end\":22689,\"start\":22682},{\"end\":22696,\"start\":22689},{\"end\":22702,\"start\":22696},{\"end\":22708,\"start\":22702},{\"end\":22715,\"start\":22708},{\"end\":22723,\"start\":22715},{\"end\":23008,\"start\":22996},{\"end\":23016,\"start\":23008},{\"end\":23026,\"start\":23016},{\"end\":23043,\"start\":23026},{\"end\":23054,\"start\":23043},{\"end\":23062,\"start\":23054},{\"end\":23075,\"start\":23062},{\"end\":23089,\"start\":23075},{\"end\":23097,\"start\":23089},{\"end\":23108,\"start\":23097},{\"end\":23470,\"start\":23460},{\"end\":23482,\"start\":23470},{\"end\":23490,\"start\":23482},{\"end\":23499,\"start\":23490},{\"end\":23507,\"start\":23499},{\"end\":23516,\"start\":23507},{\"end\":23529,\"start\":23516},{\"end\":23781,\"start\":23773},{\"end\":23790,\"start\":23781},{\"end\":23797,\"start\":23790},{\"end\":23804,\"start\":23797},{\"end\":24064,\"start\":24056},{\"end\":24072,\"start\":24064},{\"end\":24080,\"start\":24072},{\"end\":24086,\"start\":24080},{\"end\":24094,\"start\":24086},{\"end\":24102,\"start\":24094},{\"end\":24108,\"start\":24102},{\"end\":24115,\"start\":24108},{\"end\":24377,\"start\":24369},{\"end\":24384,\"start\":24377},{\"end\":24391,\"start\":24384},{\"end\":24404,\"start\":24391},{\"end\":24638,\"start\":24631},{\"end\":24649,\"start\":24638},{\"end\":24660,\"start\":24649},{\"end\":24673,\"start\":24660},{\"end\":24905,\"start\":24891},{\"end\":24912,\"start\":24905},{\"end\":25124,\"start\":25117},{\"end\":25130,\"start\":25124},{\"end\":25137,\"start\":25130},{\"end\":25348,\"start\":25331},{\"end\":25358,\"start\":25348},{\"end\":25366,\"start\":25358},{\"end\":25376,\"start\":25366},{\"end\":25385,\"start\":25376},{\"end\":25398,\"start\":25385},{\"end\":25683,\"start\":25672},{\"end\":25691,\"start\":25683},{\"end\":25702,\"start\":25691},{\"end\":25713,\"start\":25702},{\"end\":25723,\"start\":25713},{\"end\":26017,\"start\":26010},{\"end\":26025,\"start\":26017},{\"end\":26035,\"start\":26025},{\"end\":26542,\"start\":26533},{\"end\":26550,\"start\":26542},{\"end\":26559,\"start\":26550},{\"end\":26567,\"start\":26559},{\"end\":26573,\"start\":26567},{\"end\":26894,\"start\":26884},{\"end\":26905,\"start\":26894},{\"end\":26914,\"start\":26905},{\"end\":27188,\"start\":27177},{\"end\":27204,\"start\":27188},{\"end\":27215,\"start\":27204},{\"end\":27471,\"start\":27462},{\"end\":27483,\"start\":27471},{\"end\":27495,\"start\":27483},{\"end\":27891,\"start\":27869},{\"end\":27899,\"start\":27891},{\"end\":27911,\"start\":27899},{\"end\":27918,\"start\":27911},{\"end\":28174,\"start\":28165},{\"end\":28185,\"start\":28174},{\"end\":28194,\"start\":28185},{\"end\":28377,\"start\":28369},{\"end\":28388,\"start\":28377},{\"end\":28397,\"start\":28388},{\"end\":28408,\"start\":28397},{\"end\":28418,\"start\":28408},{\"end\":28683,\"start\":28674},{\"end\":28694,\"start\":28683},{\"end\":28708,\"start\":28694},{\"end\":28716,\"start\":28708},{\"end\":28956,\"start\":28949},{\"end\":28964,\"start\":28956},{\"end\":28973,\"start\":28964},{\"end\":28985,\"start\":28973},{\"end\":29216,\"start\":29210},{\"end\":29225,\"start\":29216},{\"end\":29232,\"start\":29225},{\"end\":29239,\"start\":29232},{\"end\":29471,\"start\":29464},{\"end\":29479,\"start\":29471},{\"end\":29485,\"start\":29479},{\"end\":29711,\"start\":29701},{\"end\":29719,\"start\":29711},{\"end\":29728,\"start\":29719},{\"end\":29949,\"start\":29938},{\"end\":29962,\"start\":29949},{\"end\":30248,\"start\":30240},{\"end\":30256,\"start\":30248},{\"end\":30263,\"start\":30256},{\"end\":30270,\"start\":30263},{\"end\":30277,\"start\":30270},{\"end\":30500,\"start\":30489},{\"end\":30511,\"start\":30500},{\"end\":30524,\"start\":30511},{\"end\":30787,\"start\":30776},{\"end\":30794,\"start\":30787},{\"end\":30801,\"start\":30794},{\"end\":31061,\"start\":31048},{\"end\":31073,\"start\":31061},{\"end\":31084,\"start\":31073},{\"end\":31097,\"start\":31084},{\"end\":31351,\"start\":31341},{\"end\":31360,\"start\":31351},{\"end\":31369,\"start\":31360},{\"end\":31378,\"start\":31369},{\"end\":31390,\"start\":31378},{\"end\":31400,\"start\":31390},{\"end\":31411,\"start\":31400},{\"end\":31418,\"start\":31411},{\"end\":31432,\"start\":31418},{\"end\":31442,\"start\":31432},{\"end\":31753,\"start\":31745},{\"end\":31760,\"start\":31753},{\"end\":31766,\"start\":31760},{\"end\":31774,\"start\":31766},{\"end\":31781,\"start\":31774},{\"end\":31791,\"start\":31781},{\"end\":31798,\"start\":31791},{\"end\":31805,\"start\":31798},{\"end\":31813,\"start\":31805},{\"end\":31822,\"start\":31813}]", "bib_venue": "[{\"end\":21103,\"start\":21091},{\"end\":21952,\"start\":21940},{\"end\":22217,\"start\":22213},{\"end\":22739,\"start\":22735},{\"end\":24131,\"start\":24127},{\"end\":24420,\"start\":24416},{\"end\":24693,\"start\":24687},{\"end\":26930,\"start\":26926},{\"end\":27231,\"start\":27227},{\"end\":27950,\"start\":27938},{\"end\":28450,\"start\":28438},{\"end\":29017,\"start\":29005},{\"end\":29255,\"start\":29251},{\"end\":30293,\"start\":30289},{\"end\":31113,\"start\":31109},{\"end\":20051,\"start\":19998},{\"end\":20299,\"start\":20285},{\"end\":20521,\"start\":20509},{\"end\":20735,\"start\":20663},{\"end\":21089,\"start\":21071},{\"end\":21391,\"start\":21351},{\"end\":21660,\"start\":21646},{\"end\":21938,\"start\":21920},{\"end\":22211,\"start\":22201},{\"end\":22386,\"start\":22326},{\"end\":22733,\"start\":22723},{\"end\":22994,\"start\":22888},{\"end\":23458,\"start\":23394},{\"end\":23834,\"start\":23804},{\"end\":24125,\"start\":24115},{\"end\":24414,\"start\":24404},{\"end\":24685,\"start\":24673},{\"end\":24889,\"start\":24836},{\"end\":25148,\"start\":25137},{\"end\":25329,\"start\":25254},{\"end\":25670,\"start\":25606},{\"end\":26008,\"start\":25926},{\"end\":26531,\"start\":26433},{\"end\":26924,\"start\":26914},{\"end\":27225,\"start\":27215},{\"end\":27558,\"start\":27495},{\"end\":27936,\"start\":27918},{\"end\":28163,\"start\":28120},{\"end\":28436,\"start\":28418},{\"end\":28672,\"start\":28617},{\"end\":29003,\"start\":28985},{\"end\":29249,\"start\":29239},{\"end\":29462,\"start\":29362},{\"end\":29699,\"start\":29659},{\"end\":30010,\"start\":29962},{\"end\":30287,\"start\":30277},{\"end\":30487,\"start\":30420},{\"end\":30774,\"start\":30709},{\"end\":31107,\"start\":31097},{\"end\":31446,\"start\":31442},{\"end\":31743,\"start\":31669}]"}}}, "year": 2023, "month": 12, "day": 17}