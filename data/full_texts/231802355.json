{"id": 231802355, "updated": "2023-11-18 14:18:36.977", "metadata": {"title": "Unifying Vision-and-Language Tasks via Text Generation", "authors": "[{\"first\":\"Jaemin\",\"last\":\"Cho\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Lei\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Mohit\",\"last\":\"Bansal\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/ChoLTB21", "doi": null}}, "content": {"source": {"pdf_hash": "cb596bffc5c5042c254058b62317a57fa156fea4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2102.02779v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fbdce48e0fe59a17d863116454fbcbf3eacd799d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cb596bffc5c5042c254058b62317a57fa156fea4.txt", "contents": "\nUnifying Vision-and-Language Tasks via Text Generation\n\n\nJaemin Cho \nUNC Chapel Hill\n\n\nJie Lei jielei@cs.unc.edu \nUNC Chapel Hill\n\n\nHao Tan haotan@cs.unc.edu \nUNC Chapel Hill\n\n\nMohit Bansal mbansal@cs.unc.edu \nUNC Chapel Hill\n\n\nUnifying Vision-and-Language Tasks via Text Generation\n\nExisting methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent taskspecific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized singletask models. Our code is publicly available at: https://github.com/j-min/VL-T5\n\nIntroduction\n\nMirroring the success of the pretraining-finetuning paradigm with transformer language models (Devlin et al., 2019), recent vision-and-language transformers (Tan & Bansal (2019); Lu et al. (2019); Chen et al. (2020); Li et al. (2020b), Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).\n\n\"vqa: what is the man jumping over?\"\n\n\"image text match: A cat is lying on a bed\" \"visual grounding: yellow fire hydrant\" \"span prediction: A <text_1> is <text_2> over fire hydrant\" \"<text_1> man <text_2> jumping\" \"fire hydrant\" \"<vis_3>\"  Figure 1. Our unified framework for learning vision-and-language tasks. While existing methods require designing task-specific architectures for different tasks, our framework unifies them together as generating text labels conditioned on multimodal inputs.\n\ninter alia) have also been adopted in a wide range of visionand-language tasks. These models are firstly pretrained on large image-text corpus (e.g., COCO Caption (Chen et al., 2015)), then finetuned on downstream tasks (e.g., visual question answering  and referring expression comprehension (Mao et al., 2016)), which outperformed many previous non-pretraining-finetuning methods.\n\nFor each pretraining or downstream task, existing visionand-language transformers typically require designing taskspecific, separately-parameterized architectures on top of the transformer encoder (e.g., multi-label sigmoid classifier for visual question answering, and softmax classifier for referring expression comprehension). However, the reasoning skills required by these tasks overlap significantly. Consider the example in Fig. 1. Both answering the question \"What is the man jumping over?\" and grounding an image region corresponding to the phrase \"yellow fire hydrant\" require recognizing the object \"fire hydrant\". In addition, the labels for these tasks can be easily expressed in text. For instance, we can assign a region id (e.g., \"<vis 3>\", a special text token) to a specific region in the image, and then the referring expression comprehension task can be expressed as generating the correct region id. For visual question answering, the labels are already in text, although existing approaches (Anderson et al., 2018;Tan & Bansal, 2019;Chen et al., 2020) tackle the task as learning a multi-label classifier over a fixed set of frequent answers (See Fig. 3).\n\nHence, in order to alleviate these hassles of designing taskspecific architectures, we propose a unified framework for vision-and-language learning via generating labels in text. Specifically, we extend off-the-shelf pretrained language models T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) with visual understanding ability, named 'VL-T5' and 'VL-BART'. In contrast to existing methods that train different architectures for each pretraining and downstream task, our models tackle all tasks with the same language modeling head. To learn a new task, we can simply rewrite its input and output in text, without the need of adding extra parameters or designing new architectures and objectives. In addition, we can leverage the text generation ability of pretrained language models when making predictions. This is especially helpful when we answer open-ended questions that require non-trivial answers, where discriminative methods can only answer from a predefined set of frequent candidates, while our models can generate open-ended natural language answers.\n\nTo evaluate the effectiveness of our generative modeling approach, we compare our models against recent visionand-language transformers on a diverse set of 7 downstream benchmarks, including visual question answering on VQA  and GQA (Hudson & Manning, 2019), referring expression comprehension on Ref-COCOg (Mao et al., 2016), natural language visual reasoning on NLVR 2 (Suhr et al., 2019), visual commonsense reasoning on VCR (Zellers et al., 2019), image captioning on COCO Caption (Chen et al., 2015), and multimodal machine translation on Multi30K (Elliott et al., 2016). Our unified generative method reaches comparable performance to recent state-of-the-art vision-and-language pretraining methods. This is especially interesting because we use the same unified language modeling architecture with the same maximum likelihood estimation (MLE) objective for all the tasks, while existing approaches use task-specific architectures and objective functions. In addition, we found that our generative models have better generalization ability compared to the discriminative versions in the rare-answer scenario on visual question answering, when ground truth answers for given questions are rarely seen during training. Finally, we also experiment with our unified framework under the multi-task learning setup on all 7 downstream tasks. With a single architecture and a single set of weights, our model achieves similar performance to separately optimized single-task models.\n\n\nRelated Works\n\nVision-and-Language pretraining: Large-scale language pretraining with transformers (Vaswani et al., 2017;Devlin et al., 2019;Liu et al., 2019;Lan et al., 2020;Clark et al., 2020;Yang et al., 2019;Raffel et al., 2020) have achieved remarkable success for many natural language understanding tasks (Rajpurkar et al., 2016;Zellers et al., 2018;Wang et al., 2018;Williams et al., 2017). Following this success, image+text pretraining models Tan & Bansal, 2019;Chen et al., 2020;Huang et al., 2020;Li et al., 2020b;Cho et al., 2020;Radford et al., 2021;Zhang et al., 2021) and video+text pretraining models (Sun et al., 2019b;a;Li et al., 2020a;Zhu & Yang, 2020;Miech et al., 2020) have also shown to perform better than previous nonpretraining approaches Anderson et al., 2018;Kim et al., 2018;Yu et al., 2018b) in a wide range of discriminative Hudson & Manning, 2019;Lei et al., 2018;Mao et al., 2016;Xu et al., 2016;Zhou et al., 2018) and generative tasks (Chen et al., 2015;Xu et al., 2016;Zhou et al., 2018). In this work, we focus on image+text tasks. While existing image+text models mostly use task-specific architectures and objectives, we seek to design a unified framework across different tasks.\n\nUnified frameworks: One line of work focus on solving natural language processing tasks in a unified format, such as question answering (Mccann et al., 2018), span prediction (Keskar et al., 2019), or text generation (Raffel et al., 2020;Brown et al., 2020;Khashabi et al., 2020). These unified frameworks provide efficient knowledge sharing among different tasks and make it easy to leverage pretrained language models. In relation to these works, we propose to unify previously separately modeled vision-and-language tasks in a single unified format, via text generation, conditioned on multimodal inputs from the image and the textual context.\n\n\nModel\n\nWe propose a new framework that unifies vision-andlanguage problems as multimodal conditional text generation. We introduce VL-T5 and VL-BART based on two pretrained transformer language models: T5 Base (Raffel et al., 2020) and BART Base (Lewis et al., 2020). Specifically, we extend their text encoders to multimodal encoders by incorporating image region embeddings as additional input. The overall architecture of our framework is shown in Fig. 2. Since the architecture differences between VL-T5 and VL-BART are minor, we use VL-T5 as an example to illustrate our framework in details in the rest of this section.\n\n\nVisual Embeddings\n\nWe represent an input image v with n=36 object regions from a Faster R-CNN (Ren et al., 2015) Figure 2. An illustration of our VL-T5 and VL-BART architectures for visual grounding task. Instead of task-specific architectures, our models use text prefixes to adapt to different tasks. The green block in (a) refers to visual embeddings. (b) shows the components of visual embedding. Note that we reuse the text embeddings of visual sentinel tokens (ex. <vis 3>) as region id embeddings, which allows our models to tackle many discriminative vision-language tasks as text generation, including visual grounding.\n\nGenome (Krishna et al., 2016) for object and attribute classification (Anderson et al., 2018). As shown in Fig. 2 \n\n\nText Embeddings\n\nInstead of designing task-specific architectures, we add different prefixes to the original input text to adapt to different tasks, as shown in Table. 1 1 . This augmented input text x is then tokenized as {x 1 , . . . , x |x| } and encoded as learned embedding e x = {e x 1 , . . . , e x |x| }. The embedding parameters are shared by the encoder, decoder, and language modeling head (Press & Wolf, 2017). Since the attention layers are permutation-invariant, BART learns positional embeddings (Vaswani et al., 2017;Devlin et al., 2019) for absolute token positions and adds them to the token embeddings. In contrast, T5 adds relative position bias to each self-attention layer (Shaw et al., 2018). Our models follow the positional embedding configurations of their text backbone models.\n\nIn addition to the original vocabulary of T5 and BART, we introduce visual sentinel tokens {<vis 1>, . . . , <vis n>}, which corresponds to image regions. As illustrated in Fig. 2, we use the text embeddings of visual sentinel tokens as region id embeddings in Sec. 3.1. The embedding sharing enables our model to build the corre-1 Note that since we use simple prefixes (e.g., \"vqa:\" for VQA task), it is likely that engineering in text prompts  would improve the accuracy of our methods. As this is not the focus of this paper, we leave it as future works. spondence among query text, label text, and objects, which are useful in the grounding tasks (e.g., visual grounding and grounded captioning pretraining tasks in Sec. 4, referring expression comprehension in Sec. 5.3).\n\n\nEncoder-Decoder Architecture\n\nWe use transformer encoder-decoder architecture (Vaswani et al., 2017) to encode visual and text inputs and generate label text. Our bidirectional multimodal encoder is a stack of m transformer blocks, consisting of a self-attention layer and a fully-connected layer with residual connections. Our decoder is another stack of m transformer blocks similar to the multimodal encoder, where each block has an additional cross-attention layer. As shown in Fig. 2 (a), the encoder takes the concatenation of text and visual embeddings as input and outputs their contextualized joint representations\nh = {h x 1 , . . . , h x |x| , h v 1 , . . . , h v n } = Enc(e x , e v ).\nThen the decoder iteratively attends to previously generated tokens y <j (via self-attention) and the encoder outputs h (via crossattention), then predicts the probability of future text tokens P \u03b8 (y j |y <j , x, v) = Dec(y <j , h). We suggest readers to check Raffel et al. (2020); Lewis et al. (2020) for more details of our backbone models. For both pretraining (Sec. 4) and downstream tasks (Sec. 5), we train our model parameters \u03b8 by minimizing the negative log-likelihood of label text y tokens given input text x and image v:\nL GEN \u03b8 = \u2212 |y| j=1 log P \u03b8 (y j |y <j , x, v)(1)\n\nTask-Specific Methods vs. Our Unified Framework\n\nWe compare our unified framework with existing vision-andlanguage transformers on two popular tasks: visual question answering  and referring expression comprehension (Mao et al., 2016).\n\nVisual question answering requires a model to answer a question to a given context image. As shown in Fig.3 (a), existing methods ( Figure 3. Comparison between existing methods and our framework on visual question answering and referring expression comprehension (visual grounding) tasks. While existing methods use task-specific architectures and objectives, our models use the same language modeling architecture and maximum likelihood estimation on label text for all tasks.\n\n\n2019; Chen et al., 2020) typically introduce a multilayer perceptron (MLP) multi-label classifier head on top of h x [CLS]\n\n, which is trained together with the transformer backbone through a binary cross-entropy loss, and weighted with VQA score  \n2 : L VQA \u03b8 = \u2212 K k=1 score(a k , x, v) log P VQA \u03b8 (correct|a k , x, v).\nReferring expression comprehension requires models to localize a target region in an image that is described by a given referring expression. Previous methods tackle this task as multi-class  or binary  classification over image regions. For example, UNITER  introduces an MLP region scoring head on top of the output representations of regions, as shown in Fig. 3(b). This region scoring head is jointly trained with the encoder by minimizing negative loglikelihood of target region r * :\nL REF \u03b8 = \u2212 log P REF \u03b8 (r * |x, v)\n. In contrast to existing methods that develop task-specific architectures and objectives (e.g., equations above), our unified framework is free from extra model designs for new tasks. As shown in Fig. 3 (c,d) and Table 1, we formulate the task labels to corresponding text, and we learn these different tasks by predicting label text with the same language modeling objective (Eq. 1).\n\n\nPretraining\n\nIn this section, we describe how we pretrain our VL-T5 and VL-BART models (Sec. 3). We start with the details of the pretraining data and illustrate how we formulate diverse vision-and-language pretraining tasks as multimodal conditional text generation.\n\n\nPretraining Data\n\nWe aggregate pretraining data from MS COCO (Lin et al., 2014;Chen et al., 2015) and Visual Genome (VG; Krishna et al. (2016)) images 3 . The captioning data from these two datasets are used in the multimodal language modeling task. The COCO captions are also used in the image-text matching task to learn cross-modal alignment. Besides the captions, we also use three visual question answering datasets (VQA v2.0 , GQA balanced version (Hudson & Manning, 2019), and Visual7W ) as in Tan & Bansal (2019), but only used them for the visual question answering task. Details of these pretraining tasks are in Sec. 4.2. Overall, our pretraining dataset contains 9.18M image-text pairs on 180K distinct images. We show more details of the pretraining data in appendix.\n\n\nPretraining Tasks\n\nWe pretrain our models under a multi-task setup with diverse pretraining tasks, including multimodal language modeling, visual question answering, image-text matching, visual grounding, and grounded captioning. Table 1 shows input and output examples of our pretraining tasks. The training data for each of these tasks are summarized in appendix. In the rest of this section, we explain these tasks in detail.\n\nMultimodal language modeling: We follow Raffel et al. (2020) and Lewis et al. (2020) to construct the language modeling pretraining task. For VL-T5, we mask 15% of input text tokens and replace contiguous text span with sentinel tokens (e.g., <text 1>). For VL-BART, we mask 30% of input text tokens with <mask> tokens. Then we predict the masked text. See Table 1 for examples.\n\nVisual question answering: We include visual question answering in our pretraining tasks as in Tan & Bansal (2019). While previous methods (Tan & Bansal, 2019;Lu et al., 2019;Chen et al., 2020) tackle the task as classification over predefined answer candidates (illustrated in Fig. 3), we directly generate answers in their original text format.\n\nImage-text matching: In this task, the model needs to verify whether a text corresponds to an image. We consider an image and its captions 4 as positive pairs. With a probability Table 1. Input-output formats for pretraining (Sec. 4) and downstream tasks (Sec. 5). a We use different prefixes (\"vqa:\", \"gqa:\", \"visual7w:\") for questions from different datasets. b NLVR 2 takes two images as visual input, for brevity, we only show one here. of 50%, we randomly sample another training image's caption to create a negative pair. The model then predicts the correspondence with \"true\" or \"false\" as shown in Table 1.\n\nVisual grounding: We develop an object-text matching task to endow the model with grounding ability, which is required in several tasks (e.g., referring expression comprehension and VCR). We give the model a region description and let it predict the id of the related object region. With the help of the visual sentinel token (e.g., <vis 3> in Table 1), this task fits naturally into our text generation objective. We make the region descriptions from the predictions of the object detector that we use for visual embeddings (see Sec.\n\n3.1). Concretely, we sample an object region out of n region predictions. Then we concatenate its object name and attribute (e.g., attribute: \"yellow\" + object: \"fire hydrant\" \u2192 \"yellow fire hydrant\"). This approach does not need extra annotation and could be extended to images without dense annotations (e.g., COCO images).\n\nGrounded captioning: To teach the model with objectlevel information, we also use grounded captioning as an inverse task of visual grounding. As shown in Table 1, given a visual sentinel token (which indicates an image region) as text input, the model is asked to generate a corresponding textual description of the image region.\n\n\nPretraining Implementation Details\n\nFor both VL-T5 and VL-BART, it takes 4 days for 30epoch pretraining with mixed precision training (Narang et al., 2018) on 4 RTX 2080 Ti GPUs. We use batch size 320 and 600 for VL-T5 and VL-BART, respectively. We use AdamW (Loshchilov & Hutter, 2019) with (\u03b2 1 , \u03b2 2 ) = (0.9, 0.999) and learning rate 1e-4 with 5% linear warmup schedule. Our code is based on PyTorch (Paszke et al., 2017) and Huggingface Transformers (Wolf et al., 2019).\n\ndescriptions of an image (e.g., 'what is in the image?').\n\n\nDownstream Tasks and Results\n\nIn this section, we compare our generative architectures VL-T5 and VL-BART on a diverse set of 7 downstream tasks (details in Appendix) with existing vision-and-language pretrained transformers (Tan & Bansal, 2019;Lu et al., 2019;Chen et al., 2020;Zhou et al., 2020;Li et al., 2020b;Xia et al., 2020). As summarized in Table 2, our unified generative approach (with the input-output format in Table 1) shows performance close to the task-specific models, most of which are discriminative. In the rest of this section, we provide detailed comparisons w.r.t. the baselines.\n\n\nVisual Question Answering: VQA and GQA\n\nThe visual question answering task requires models to answer a question to a given context image. Table 2 compares our models VL-T5 and VL-BART with existing methods on VQA  and GQA (Hudson & Manning, 2019). For both tasks, our models achieve comparable performance to existing approaches.\n\nGenerative vs. Discriminative model: Modern approaches (Tan & Bansal, 2019;Lu et al., 2019;Chen et al., 2020;Zhou et al., 2020;Li et al., 2020b) are discriminative models, where they tackle visual question answering tasks as multi-label classification over a predefined set of answer candidates. This strategy achieves strong performance but not generalizes to real-world open-ended scenarios. To quantitatively compare the existing discriminative approaches and our generative approach, we break down VQA questions into in-domain and out-of-domain questions, in terms of whether the best answer a * for each question is included in the top-K (K=3, 129) answer candidates A topk . After this split, the in-domain subset contains 24,722 questions, and the out-of-domain subset contains 1,558 questions. Table 3 shows the performance. For discriminative baselines, we introduce a sigmoid MLP classifier on top of the decoder representation of start-of-sequence token <s>, following  LXMERT and UNITER. Comparing models with the same backbone, we notice the generative models improve upon the discriminative baselines across all the subsets. This improvement is more significant on the out-of-domain subset, where the generative VL-T5 and VL-BART achieve 6 and 6.2 points improvement over their discriminative counterparts, showing the effectiveness of using generative modeling. Compared to the strong discriminative baseline UNITER Base (pretrained with 4M extra images), our generative models still show comparable overall performance while significantly outperform it on the out-of-domain subset (about 3 points).\n\nDataset-specific prefixes: As shown in recent works Shin et al., 2020;Li & Liang, 2021;Radford et al., 2021), different text prompts could result in different finetuning results. We thus experiment with a single prefix 'vqa' for both VQA and GQA in VL-T5 pretraining/finetuning. Interestingly, we found slight performance increases from the original dataset-specific prefix: VQA This shows that a single model can successfully handle multiple VQA tasks without dataset-specific prefixes (similar results were observed in text QA (Khashabi et al., 2020)).\n\n\nNatural Language Visual Reasoning: NLVR 2\n\nThe task of NLVR 2 (Suhr et al., 2019) is to determine whether a natural language statement is true about two images. To apply our model to this task, we concatenate region features from the two images and use different image id embeddings to disambiguate the regions from the two images. Then our model learns to generate text labels \"true\" and \"false\". This is similar to the Triplet setting described in UNITER . In Fig. 4, we illustrate three common encoding settings for NLVR 2 .  tional attention added to Pair. UNITER shows that one can improve performance with a more complex encoding setting, i.e., Pair-biattn achieves better performance than Pair, which is again better than the simplest Triplet. Note that both the Pair and the Pair-biattn settings approximately double the computational cost compared to that of the Triplet setting. While there's the gap between our models and baselines in Pair and Pair-biattn setting, VL-T5 shows comparable performance to UNITER in Triplet setting.\n\n\nReferring Expression Comprehension: RefCOCOg\n\nReferring expression comprehension requires a model to correctly localize an object described by a given phrase (e.g., 'the car on the left'). In this work, we evaluate models on the RefCOCOg (Mao et al., 2016) dataset. Similar to the visual grounding pretraining task in Sec. 4, we give our model a referring phrase and candidate region features from the image, the model then generates the visual sentinel token (e.g., <vis 1>) of the region corresponding to the phrase. Following previous works UNITER and MAttNet , we use region detections from Mask R-CNN (He et al., 2017) as candidates and mark a selected region to be correct if its intersection over union (IoU) with the ground truth region is greater than 0.5. Table 2 compares our models with discriminative baselines. With pretraining, VL-T5 significantly outperforms the strong modular model MAttNet, and achieves a reasonable performance compared to the UNITER model that has been pretrained on a much larger corpus. While our method did not achieve state-of-the-art performance, these results suggest that referring expression comprehension can be effectively formulated as a text-generation task, rather than previously Chen et al., 2020) formulated classification task over a set of visual regions, allowing more flexible architecture design. We hope our work would inspire future works in this direction. We also observe that our experiments with VL-BART on RefCOCOg diverges. One reason might be the difference in positional encoding methods of T5 and BART. During training, BART adds learned absolute positional embedding to text token embedding, whereas T5 uses relative position biases in selfattention layers instead. We hypothesize that VL-BART found strong correspondence by memorizing the positions of each training object (we observe high training accuracy, but low validation accuracy).\n\n\nVisual Commonsense Reasoning: VCR\n\nVisual Commonsense Reasoning (VCR) (Zellers et al., 2019) is a multiple-choice question answering task that requires commonsense reasoning beyond object or action recognition. Each VCR question (Q) has 4 answers (A) and 4 rationales (R), and it can be decomposed into two multiple choice sub-tasks: question answering (Q\u2192A), and answer justification (QA\u2192R). The overall task (Q\u2192AR) requires a model to not only select the correct answer to the question, but also the correct rationale for choosing the answer. Similar to Nogueira et al. (2020) that leverages language model for document ranking, we concatenate context (image+question) with each candidate choice, and let our models generate \"true\" for the correct choice and generate \"false\" otherwise, as shown in Table 1, During inference, we use P (true) P (true)+P (f alse) to rank the choices and select the one with the highest score.\n\nUNITER  has shown that a second-stage in-domain pretraining (with the same pretraining objectives as generic-domain pretraining) on the VCR dataset would significant improve VCR task performance. This is likely due to the domain difference between VCR and the generic-domain pretraining corpus (e.g., COCO Captions), e.g., the input text (concatenation of multiple sentences:\n[Q]+[A]+[R]\n) in VCR is much longer than in genericdomain pretraining. In Table 6, we show the experiment results with second stage pretraining on VCR. On VCR val split, comparing to the base models that do not pretrain, we find that both Stage 1 generic-domain pretraining and Stage 2 in-domain pretraining help improve the VCR task performance, which is consistent with the findings in UNITER. On VCR test split, we notice that our best model VL-T5 achieves a comparable (slightly better) performance to UNITER, while significantly higher performance when compared to ViLBERT. \n\n\nImage Captioning: COCO Caption\n\nWe evaluate automatic caption generation performance on MS COCO Caption dataset (Chen et al., 2015). We use Karparthy split (Karpathy & Fei-Fei, 2015), which re-splits train2014 and val2014 images (Lin et al., 2014) into 113,287 / 5000 / 5000 for train / validation / test. While some methods use reinforcement learning-based optimization on CIDEr, we only compare with methods using cross-entropy loss. Note that image captioning is the only task in our experiments where textual context is not meaningful, which results in a notable difference in pretraining and finetuning w.r.t. the input format. Inspired by Oscar (Li et al., 2020a), we also experiment with using object tags as additional text inputs during finetuning. We use BLEU (Papineni et al., 2002), CIDEr , METEOR (Banerjee & Lavie, 2005), SPICE (Anderson et al., 2016) as evaluation In Table 7, we compare our models with baselines in different settings: use of vision-and-language pretraining and use of object tag as additional text inputs. With and without vision-and-language pretraining, our models show comparable performance to baselines. Since the use of object tags requires significant extra computation, we only use it for finetuning. Using tags gives a comparable or slightly improved performance for both models, and the improvement is significant (2.5) in CIDEr for VL-BART. We expect object tag augmentation during pretraining like Oscar would further boost the performance of our models.\n\n\nMultimodal Machine Translation: Multi30K\n\nWe evaluate multimodal machine translation performance on Multi30K dataset (Elliott et al., 2016), where a model Table 9. Single-task vs. Multi-task finetuning results on 7 tasks. With a single set of parameters, our multi-task model achieves similar performance to separately optimized single-task models. We denote the number of parameters of single VL-T5 model as P. translates English text to German text given context images. We report BLEU score using SacreBLEU (Post, 2018) 6 . We compare our method with state-of-the-art transformer models: Multimodal self-attention (MSA) (Yao & Wan, 2020), MeMAD (Gr\u00f6nroos et al., 2018). Table 8 shows that our T5-based models outperform the baselines that use strong data augmentation (e.g., back-translation) on all three test splits. Our vision-and-language models improve the textonly backbones although we did not observe improvement with vision-and-language pretraining. This might be because the source text in Multi30K contains sufficient information for translation as discussed in Caglayan et al. (2019) \n\n\nMulti-Task Finetuning\n\nSingle-task vs. Multi-task Finetuning: While our framework has unified the architecture for different downstream tasks, the parameters are separately optimized. To see whether we can go further, we finetune a single VL-T5 model for 20 epochs, where it tackles 7 different tasks with the same set of weights. At each finetuning step, we sample a mini-batch of examples from one of the 7 tasks in a round-robin fashion. For a fair comparison, we use singletask baselines without augmentations (e.g., no 2nd stage pretraining for VCR, no object tags for COCO Captioning). Table 9 shows that our multi-task model achieves comparable performance to the separately optimized single-task models on all 7 tasks with a single set of parameters.\n\nSingle shared head vs. Task-specific heads: We also experiment with the multi-task finetuning setup of ViLBERT-MT (Lu et al., 2020), where a task-specific head is finetuned for each of 7 downstream tasks while sharing backbone. The head parameters are initialized from the pretrained LM head and separately updated during finetuning. The 7 task-specific heads (7H) add 7 \u00d7 32K(vocab size) \u00d7 768(embedding size) = 172M parameters, which is 80% of original VL-T5's 220M parameters (P), resulting around 400M parameters in total. Since the increased parameters make the training slow, we compare both models by 5th epoch checkpoints. Table 10 shows that VL-T5 with single shared head achieves almost equal performance with task-6 https://github.com/mjpost/sacrebleu specific heads, while having much fewer total parameters.\n\n\nConclusion\n\nIn this work, we proposed VL-T5 and VL-BART which tackle vision-and-language tasks with a unified text generation objective. Experiments show VL-T5 and VL-BART can achieve comparable performance with state-of-the-art vision-and-language transformers on diverse vision-andlanguage tasks without hand-crafted architectures and objectives. Especially, we demonstrate our generative approach is better suited for open-ended visual question answering.\n\nIn addition, we also showed it is possible to train seven different tasks simultaneously using a single architecture with single parameters without not losing much performance. It would be an interesting future work to further explore this direction by adding even more tasks.  \n\n\nA. Comparison with Baselines\n\nIn Table 11, we compare the baseline vision-and-language transformers with our VL-T5 and VL-BART in detail, including their pretraining datasets, architecture, etc.\n\n\nB. Implementation Details\n\nIn Table 12 and Table 13, we show the detailed statistics of our pretraining and downstream datasets and tasks. In Table 14, we show the hyperparameters that we used in our pretraining and downstream task experiments. We provide the links to download pretraining and downstream datasets.\n\n\nB.1. Pretraining Data\n\nOverall, our pretraining dataset contains 9.18M image-text pairs on 180K distinct images. We carefully split our pretraining data to avoid any intersection between our training data and the validation/test sets of the downstream tasks (e.g., COCO Captioning, RefCOCOg). In this process, around 10K images are excluded from the training sets of COCO 7 and Visual Genome 8 . We use COCO Karpathy val split (Karpathy & Fei-Fei, 2015) with 5,000 images as our validation set to monitor pretraining performance.\n\n\nB.2. Downstream Tasks\n\nVQA 9 , COCO caption For both VQA and COCO captioning tasks, we follow Karparthy split (Karpathy & Fei-Fei, 2015), which re-splits train2014 and val2014 COCO images (Lin et al., 2014) into 113,287 / 5,000 / 5,000 images for train / validation / test.\n\nGQA 10 Following LXMERT (Tan & Bansal, 2019), we use GQA-balanced version. We use train and val splits for training and use test-dev split for validation. Train / val / test-  \n\nFigure 4 .\n4Different encoding settings for NLVR 2 . Pair and Pairbiattn approximately double the computational cost over Triplet, which our models are based on.\n\n\ntrained on VisualAutoregressive \nText Decoder \n\nBidirectional \nMultimodal Encoder \n\nRoI \nfeatures \n\nBox \ncoordinates \n\nImage ids \n\nRegion ids \n\nPrefix \n\n+ \n\n+ \n\n+ \n\n+ \n+ \n\n1 \n\n+ \n\n+ \n\n1 \n\n+ \n\n+ \n\n1 \n\nvisual \ngrounding \n: \nfire \nhydrant \n<s> \n<vis_3> \n\n<vis_3> \n</s> \n\n<vis_1> \n<vis_2> \n<vis_3> \n\n(a) Our vision-and-language framework \n(b) Visual embedding \n\nVisual embedding \n\n\n\n\nTan & Bansal, 2019; VL Transformer \n\nRegion scoring \nhead \n\nVQA \nhead \n\nExisting methods: N heads for N tasks \n\nOurs: LM head for all tasks \n\n[CLS] What is the man jumping over? \n[CLS] fire hydrant \n\nTop-K answer scores \n\nSigmoid \n\nMulti-label \nClassification \n\nSoftmax \n\n\"fire hydrant\" \n\nClassification \n\nVL Transformer \n\nVL Transformer \n\nvqa: What is the man jumping over? \nvisual grounding: fire hydrant \n\nVL Transformer \n\n\"fire hydrant\" \n\"<vis_3>\" \n\nLanguage \nModeling \n\n(a) \n(b) \n\n(c) \n(d) \n\n\n\n\nPretraning tasks (Sec. 4) Multimodal LM (VL-T5) span prediction: A <text 1> is <text 2> over a fire hydrant. <text 1> man <text 2> jumping Multimodal LM (VL-BART) denoise: A <mask> is <mask> over a fire hydrant. A man is jumping over a fire hydrant a Visual question answering vqa: what is the color of the man's shirt? blue Image-text matching image text match: A man with blue shirt is jumping over fire hydrant. trueTasks \nInput image \nInput text \nTarget text \n\nVisual grounding \nvisual grounding: yellow fire hydrant \n<vis 3> \nGrounded captioning \ncaption region: <vis 3> \nyellow fire hydrant \n\nDownstream tasks (Sec. 5) \nVQA \nvqa: [Q] \n[A] \nGQA \ngqa: [Q] \n[A] \nb NLVR 2 \nnlvr: [text] \ntrue/false \nVCR Q\u2192A \nvcr qa: question [Q] answer: [A] \ntrue/false \nVCR QA\u2192R \nvcr qar: question [Q] answer: [A] rationale: [R] \ntrue/false \nRefCOCOg \nvisual grounding: [referring expression] \n[region id] \nCOCO captioning \ncaption: \n[caption] \nCOCO captioning (w/ object tags) \ncaption with tags: [Tag1 Tag2 ..] \n[caption] \nMulti30K En-De translation \ntranslate English to German: [English text] \n[German text] \n\n\n\nTable 2 .\n2Single model performance on downstream tasks. Note that the baseline models adopt task-specific objectives and architectures, whereas our models tackle all tasks, including discriminative tasks (e.g., RefCOCOg), as text generation with a single architecture and objective. See our discussion in Sec.5.3.Method \n\n# \nPretrain \nImages \n\nDiscriminative tasks \nGenerative tasks \n\nVQA \nGQA \nNLVR 2 RefCOCOg VCR Q\u2192 AR \nCOCO Cap \nMulti30K En-De \ntest-std test-std \ntest-P \ntest d \ntest \nKarpathy test \ntest 2018 \nAcc \nAcc \nAcc \nAcc \nAcc \nCIDEr \nBLEU \n\nLXMERT \n180K \n72.5 \n60.3 \n74.5 \n-\n-\n-\n-\nViLBERT \n3M \n70.9 \n-\n-\n-\n54.8 \n-\n-\nUNITER Base \n4M \n72.9 \n-\n77.9 \n74.5 \n58.2 \n-\nUnified VLP \n3M \n70.7 \n-\n-\n-\n-\n117.7 \n-\nOscar Base \n4M \n73.4 \n61.6 \n78.4 \n-\n-\n123.7 \n-\nXGPT \n3M \n-\n-\n-\n-\n-\n120.1 \n-\nMeMAD \n-\n-\n-\n-\n-\n-\n-\n38.5 \n\nVL-T5 \n180K \n70.3 \n60.8 \n73.6 \n71.3 \n58.9 \n116.5 \n38.6 \nVL-BART \n180K \n71.3 \n60.5 \n70.3 \n22.4 \n48.9 \n116.6 \n28.1 \n\n\n\nTable 3 .\n3VQA Karpathy-test split accuracy using generative and discriminative methods. We break down the questions into two subsets in terms of whether the best-scoring answer a * for each question is included in the top-K answer candidates A topk . Indomain: a * \u2208 A topk , Out-of-domain: a * / \u2208 A topk .Method \nIn-domain Out-of-domain Overall \n\nDiscriminative \nUNITER Base \n74.4 \n10.0 \n70.5 \nVL-T5 \n70.2 \n7.1 \n66.4 \nVL-BART \n69.4 \n7.0 \n65.7 \n\nGenerative \nVL-T5 \n71.4 \n13.1 \n67.9 \nVL-BART \n72.1 \n13.2 \n68.6 \n\n\n\nTable 4 .\n4NLVR 2 performance comparison under different encoding settings. Note that Triplet takes lower computational cost than Pair and Pair-biattn. See alsoFig. 4.Method \nSetting \ndev test-P \n\nUNITER Base Triplet \n73.0 73.9 \nUNITER Base Pair \n75.9 75.8 \nUNITER Base Pair-biattn 77.2 77.9 \nLXMERT \nPair \n74.9 74.5 \nOscar Base \nPair \n78.1 78.4 \n\nVL-T5 \nTriplet \n74.6 73.6 \nVL-BART \nTriplet \n71.7 70.3 \n\nKarpathy-test (67.9 \u2192 69.3); GQA test-dev (60.0 \u2192 60.2). \n\n\nTable 4\n4shows the model results on NLVR 2 under differ-\nent encoding settings: (i) Triplet: joint encoding of image \npairs and text; (i) Pair: the concatenation of individual em-\nbedding of each image-text pair; (iii) Pair-biattn: bidirec-\n\n\nTable 5 .\n5RefCOCOg performance comparison.Method \nV&L PT val d test d \n\nMattNet \n66.9 67.3 \nUNITER Base \n74.3 74.5 \n\nVL-T5 \n63.4 62.9 \nVL-T5 \n71.2 71.3 \nVL-BART \n21.8 23.0 \nVL-BART \n23.6 22.4 \n\n\n\nTable 6 .\n6VCR accuracy. Stage 1 refers to the original generic-domain pretraining and Stage 2 refers to the in-domain pretraining on VCR. Stage 2 Q \u2192 A QA \u2192 R Q \u2192 AR Q \u2192 A QA \u2192 R Q \u2192 ARMethod \nV&L PT \nVCR val \nVCR test \nStage 1 ViLBERT \n69.3 \n71.0 \n49.5 \n-\n-\n-\nViLBERT \n72.4 \n74.4 \n54.0 \n73.3 \n74.6 \n54.8 \nUNITER Base \n72.4 \n73.7 \n53.5 \n-\n-\n-\nUNITER Base \n72.8 \n75.3 \n54.9 \n-\n-\n-\nUNITER Base \n74.6 \n77.0 \n57.8 \n75.0 \n77.2 \n58.2 \n\nVL-T5 \n71.1 \n73.6 \n52.5 \n-\n-\n-\nVL-T5 \n72.9 \n75.0 \n54.7 \n-\n-\n-\nVL-T5 \n74.6 \n77.0 \n57.5 \n75.3 \n77.8 \n58.9 \nVL-BART \n65.4 \n68.1 \n44.6 \n-\n-\n-\nVL-BART \n67.0 \n67.4 \n45.4 \n-\n-\n-\nVL-BART \n69.2 \n69.9 \n48.6 \n69.8 \n69.8 \n48.9 \n\nTable 7. COCO captioning scores on Karparthy-test split. All \nmodels are trained with cross-entropy loss. PT and FT refer to the \nuse of object tags during pretraining and finetuning, respectively. \n\nMethod \nV&L PT Object tags \nCOCO Captioning \nB \nC \nM \nS \n\nOscar \nPT+FT \n36.5 123.7 30.3 23.1 \nVL-T5 \nFT \n34.5 116.5 28.7 21.9 \nVL-BART \nFT \n35.1 116.6 28.7 21.5 \n\nOscar \n34.5 115.6 29.1 21.9 \nUnified VLP \n36.5 117.7 28.4 21.3 \nXGPT \n37.2 120.1 28.6 21.8 \nVL-T5 \n34.6 116.1 28.8 21.9 \nVL-BART \n34.2 114.1 28.4 21.3 \n\nUnified VLP \n35.5 114.3 28.2 21.0 \nXGPT \n34.4 113.0 27.8 20.8 \nBUTD \n36.2 113.5 27.0 20.3 \nVL-T5 \n32.6 109.4 28.2 21.0 \nVL-BART \n33.8 112.4 28.5 21.4 \n\n\n\nTable 8 .\n8Multi30KEn-De multimodal translation BLEU scores.  \u2020 \nand * refer to data augmentation and ensemble, respectively. We \nuse gray color for the ensemble model it is not fairly comparable. \n\nMethod \nV&L PT test2016 test2017 test2018 \n\nMSA \n38.7 \n-\n-\nMeMAD \n38.9 \n32.0 \n-\n\nMSA  \u2020 \n39.5 \n-\n-\nMeMAD  \u2020 \n45.1 \n40.8 \n-\nMeMAD  \u2020 *  \n45.5 \n41.8 \n38.5 \n\nT5 (text only) \n44.6 \n41.6 \n39.0 \nVL-T5 \n45.3 \n42.4 \n39.5 \nVL-T5 \n45.5 \n40.9 \n38.6 \nBART (text only) \n41.2 \n35.4 \n33.3 \nVL-BART \n41.3 \n35.9 \n33.2 \nVL-BART \n37.7 \n29.7 \n28.1 \n\nmetrics using COCOEvalCap 5 . \n\n\n\nTable 10 .\n10Multi-task finetuning with single/task-specific heads. While three tasks are included for brevity, the rest of the tasks also show the minimal differences between two setups.Method \n# Params \n\nVQA \nGQA \nCOCO Caption \nKarpathy test test-dev \nKarpathy test \nAcc \nAcc \nCIDEr \n\nSingle shared head \nP \n68.3 \n59.3 \n110.6 \nTask-specific heads P+7H=1.8P \n68.5 \n59.3 \n110.9 \n\n\n\nTable 11 .\n11Summary of baseline vision-and-language transformers. a Since not all models report exact parameter numbers, we provide rough estimates compared to BERTBase (86M; noted as P), where word embedding parameters are excluded. b LXMERT and XGPT are not initialized from pretrained language models. LXMERT authors found pretraining from scratch was more effective than initialization from BERTBase in their experiments. XGPT uses text pretraining on Conceptual captions and COCO captions with Masked LM(Devlin et al., 2019) and Masked Seq2Seq(Song et al., 2019) objectives before V&L pretraining. c LXMERT (text+visual+cross-modal) and ViLBERT (cross-modal) use dual-stream encoders. ViLBERT uses 768/1024-dim hidden states for text/visual streams respectively. XGPT uses AoA module(Huang et al., 2019) as visual encoder. Rest of the models use single-stream encoders. d For generation tasks, Unified VLP and Oscar use causal mask and reuse encoder as decoder similar to UniLM. e XGPT also uses shared parameters for encoder and decoder, but its decoder is right-shifted and predicts next tokens. f Unified VLP is initialized from UniLM, which is initialized from BERTLarge. g Oscar uses object tags as additional text inputs.V&L Pretraining \nHyperparameters \n\nDataset \n# Imgs Arch. type Backbone \n# Layers # Params a Hidden dim # Regions Position Emb \n\nLXMERT \nCOCO+VG \n180K Encoder \n-b \n9+5+5 c \n2P \n768 \n36 \nabsolute \nViLBERT \nCC \n3M \nEncoder \nBERT Base \n12 c \n2.5P \n768/1024 c \n10\u223c36 \nabsolute \nUNITER Base CC+SBU+COCO+VG \n4M \nEncoder \nBERT Base \n12 \nP \n768 \n10\u223c100 \nabsolute \nUnified VLP \nCC \n3M \nEncoder d \nUniLM f \n12 \nP \n768 \n100 \nabsolute \nOscar Base \nCC+SBU+COCO+VG+Flickr30K \n4M \nEncoder d \nBERT Base \n12 \nP \n768 \n50 g \nabsolute \nXGPT \nCC+COCO \n3M \nEnc-Dec e \n-b \n1 c +12+12 \nP \n768 \n100 \nabsolute \n\nVL-T5 \nCOCO+VG \n180K Enc-Dec \nT5 Base \n12+12 \n2P \n768 \n36 \nrelative \nVL-BART \nCOCO+VG \n180K Enc-Dec \nBART Base \n6+6 \nP \n768 \n36 \nabsolute \n\n\n\nTable 12 .\n12Pretraining tasks used in our vision-and-language pretraining. The images that have any intersection with evaluation set of downstream tasks (e.g., COCO caption, RefCOCOg) and the held-out validation set for pretraining are excluded.Multimodal language modeling COCO, VG COCO caption, VG caption 4.9M (# captions)Task \nImage source Text source \n# Examples \n\nVisual question answering \nCOCO, VG \nVQA, GQA, Visual7W \n2.5M (# questions) \nImage-text matching \nCOCO \nCOCO caption \n533K (# captions) \nVisual grounding \nCOCO, VG \nobject&attribute tags \n163K (# images) \nGrounded captioning \nCOCO, VG \nobject&attribute tags \n163K (# images) \n\n\n\nTable 13 .\n13Statistics of the datasets used in downstream tasks. The data that are not used for training/validation (e.g., COCO test2015 images) and data for leaderboard submissions (e.g., test-dev/test-std for VQA, test for GQA) are excluded.Datasets \nImage source # Images (train) \n# Text (train) \nMetric \n\nVQA \nCOCO \n123K (113K) \n658K (605K) \nVQA-score \nGQA \nVG \n82.7K (82.3K) 1.08M (1.07M) Accuracy \nNLVR 2 \nWeb Crawled \n238K (206K) \n100K (86K) \nAccuracy \nRefCOCOg \nCOCO \n26K (21K) \n95K (80K) \nAccuracy \nVCR \nMovie Clips \n110K (80K) \n290K (212K) \nAccuracy \nCOCO Caption \nCOCO \n123K (113K) \n616K (566K) \nBLEU,CIDEr,METEOR,SPICE \nMulti30K En-De Flickr30K \n31K (29K) \n31K (29K) \nBLEU \n\n\n\nTable 14 .\n14Hyperparameters for pretraining and downtream tasksModel \nTask \nLearning rate Batch size Epochs \n\nVL-T5 \n\nPretraining \n1e-4 \n320 \n30 \nVCR Pretraining \n5e-5 \n80 \n20 \nVQA \n5e-5 \n320 \n20 \nGQA \n1e-5 \n240 \n20 \nNLVR 2 \n5e-5 \n120 \n20 \nRefCOCOg \n5e-5 \n360 \n20 \nVCR \n5e-5 \n16 \n20 \nCOCO Caption \n3e-5 \n320 \n20 \nMulti30K En-De \n5e-5 \n120 \n20 \n\nVL-BART \n\nPretraining \n1e-4 \n600 \n30 \nVCR Pretraining \n5e-5 \n120 \n20 \nVQA \n5e-5 \n600 \n20 \nGQA \n1e-5 \n800 \n20 \nNLVR 2 \n5e-5 \n400 \n20 \nRefCOCOg \n5e-5 \n1200 \n20 \nVCR \n5e-5 \n48 \n20 \nCOCO Caption \n3e-5 \n520 \n20 \nMulti30K En-De \n5e-5 \n320 \n20 \n\nUNC Chapel Hill. Correspondence to: Jaemin Cho <jmin-cho@cs.unc.edu>.\nscore(a, x, v)= min((#humans that gave answer a) * 0.3, 1)\nExisting vision-and-language transformers are trained with different datasets and computational budgets, thus their results may not be directly comparable to each other. We show the number of their pretraining images inTable 2.4  We only use captions from COCO for this task, since many short captions from VG and visual questions are nondistinctive\nhttps://github.com/tylin/coco-caption\nhttps://cocodataset.org/#download 8 http://visualgenome.org/api/v0/api_home. html 9 https://visualqa.org/download.html 10 https://cs.stanford.edu/people/dorarad/ gqa/download.html\nAcknowledgmentsWe thank Hyounghun Kim, Zineng Tang, Swarnadeep Saha, Xiang Zhou, and anonymous reviewers for their comments and suggestions. This work was supported by NSF-CAREER Award 1846185, ARO-YIP Award W911NF-18-1-0336, DARPA MCS Grant N66001-19-2-4031, Google Focused Research Award, and Bloomberg Data Science Ph.D. Fellowship. The views, opinions, and/or findings contained in this article are those of the authors and not of the funding agency.\nSemantic Propositional Image Caption Evalua-Unifying Vision-and-Language Tasks via Text Generation tion. P Anderson, B Fernando, M Johnson, S Gould, Spice, ECCV. Anderson, P., Fernando, B., Johnson, M., and Gould, S. SPICE: Semantic Propositional Image Caption Evalua- Unifying Vision-and-Language Tasks via Text Generation tion. In ECCV, 2016.\n\nBottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. P Anderson, X He, C Buehler, D Teney, M Johnson, S Gould, L Zhang, CVPR. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., and Zhang, L. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question An- swering. In CVPR, 2018. URL http://arxiv.org/ abs/1707.07998.\n\nMETEOR : An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. S Banerjee, A Lavie, ACL Workshop. Banerjee, S. and Lavie, A. METEOR : An Automatic Met- ric for MT Evaluation with Improved Correlation with Human Judgments. In ACL Workshop, 2005.\n\nLanguage Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, Amodei , D , NeurIPS. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language Models are Few-Shot Learners. In NeurIPS, 2020. URL http://arxiv.org/abs/2005.14165.\n\nProbing the Need for Visual Context in Multimodal Machine Translation. O Caglayan, P Madhyastha, L Specia, L Barrault, 10.18653/v1/n19-1422NAACL, 2019. ISBN 9781950737130. Caglayan, O., Madhyastha, P., Specia, L., and Barrault, L. Probing the Need for Visual Context in Multi- modal Machine Translation. In NAACL, 2019. ISBN 9781950737130. doi: 10.18653/v1/n19-1422.\n\nMicrosoft COCO Captions: Data Collection and Evaluation Server. X Chen, H Fang, T.-Y Lin, R Vedantam, S Gupta, P Dollar, C L Zitnick, Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. Microsoft COCO Captions: Data Collection and Evaluation Server. apr 2015. URL http://arxiv.org/abs/1504.00325.\n\nUNITER: UNiversal Image-TExt Representation Learning. Y Chen, L Li, L Yu, A E Kholy, F Ahmed, Z Gan, Y Cheng, J Liu, ECCV. Chen, Y.-c., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. UNITER: UNiversal Image- TExt Representation Learning. In ECCV, 2020. URL https://arxiv.org/abs/1909.11740.\n\nX-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers. J Cho, J Lu, D Schwenk, H Hajishirzi, A Kembhavi, 10.18653/v1/2020.emnlp-main.707EMNLP. 2020Cho, J., Lu, J., Schwenk, D., Hajishirzi, H., and Kembhavi, A. X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers. In EMNLP, 2020. doi: 10.18653/v1/2020.emnlp-main.707.\n\nElectra: Pre-training text encoders as discriminators rather than generators. K Clark, M.-T Luong, Q V Le, C D Manning, ICLR. Clark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. Elec- tra: Pre-training text encoders as discriminators rather than generators. In ICLR, 2020.\n\nPre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, NAACL. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL, oct 2019. URL http://arxiv.org/abs/1810.04805.\n\nMulti30K : Multilingual English-German Image Descriptions. D Elliott, S Frank, K Sima&apos;an, L Specia, ACL Workshop. Elliott, D., Frank, S., Sima'an, K., and Specia, L. Multi30K : Multilingual English-German Image Descriptions. In ACL Workshop, pp. 70-74, 2016.\n\nMaking Pre-trained Language Models Better Few-shot Learners. T Gao, A Fisch, Chen , D , Gao, T., Fisch, A., and Chen, D. Making Pre-trained Lan- guage Models Better Few-shot Learners. 2020. URL http://arxiv.org/abs/2012.15723.\n\nMaking the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. Y Goyal, T Khot, A Agrawal, D Summers-Stay, D Batra, D Parikh, 15731405. doi: 10.1007/ s11263-018-1116-0International Journal of Computer Vision. Goyal, Y., Khot, T., Agrawal, A., Summers-Stay, D., Ba- tra, D., and Parikh, D. Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. International Journal of Com- puter Vision, 2019. ISSN 15731405. doi: 10.1007/ s11263-018-1116-0.\n\nThe MeMAD Submission to the WMT18 Multimodal Translation Task. S.-A Gr\u00f6nroos, B Huet, M Kurimo, J Laaksonen, B Merialdo, P Pham, M Sj\u00f6berg, U Sulubacak, J Tiedemann, R Troncy, R V\u00e1zquez, WMT. 2Gr\u00f6nroos, S.-A., Huet, B., Kurimo, M., Laaksonen, J., Meri- aldo, B., Pham, P., Sj\u00f6berg, M., Sulubacak, U., Tiede- mann, J., Troncy, R., and V\u00e1zquez, R. The MeMAD Submission to the WMT18 Multimodal Translation Task. In WMT, volume 2, pp. 609-617, 2018.\n\n. K He, G Gkioxari, P Dollar, R Girshick, Mask R-Cnn, Iccv, He, K., Gkioxari, G., Dollar, P., and Girshick, R. Mask R-CNN. ICCV, 2017.\n\nAttention on attention for image captioning. L Huang, W Wang, J Chen, X Y Wei, 10.1109/ICCV.2019.00473ICCV. Huang, L., Wang, W., Chen, J., and Wei, X. Y. Attention on attention for image captioning. In ICCV, pp. 4633-4642, 2019. ISBN 9781728148038. doi: 10.1109/ICCV.2019. 00473.\n\nPixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers. Z Huang, Z Zeng, B Liu, D Fu, J Fu, Huang, Z., Zeng, Z., Liu, B., Fu, D., and Fu, J. Pixel- BERT: Aligning Image Pixels with Text by Deep Multi- Modal Transformers. 2020. URL http://arxiv. org/abs/2004.00849.\n\nGQA: A new dataset for real-world visual reasoning and compositional question answering. D A Hudson, C D Manning, 10.1109/CVPR.2019.00686CVPR, 2019. ISBN 9781728132938. Hudson, D. A. and Manning, C. D. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. ISBN 9781728132938. doi: 10.1109/CVPR.2019.00686.\n\nDeep Visual-Semantic Alignments for Generating Image Descriptions. A Karpathy, L Fei-Fei, 10.1109/TPAMI.2016.2598339CVPR, 2015. ISBN 9781467369640. Karpathy, A. and Fei-Fei, L. Deep Visual-Semantic Align- ments for Generating Image Descriptions. In CVPR, 2015. ISBN 9781467369640. doi: 10.1109/TPAMI. 2016.2598339.\n\nUnifying Question Answering and Text Classification via Span Extraction. N S Keskar, B Mccann, C Xiong, R Socher, Keskar, N. S., McCann, B., Xiong, C., and Socher, R. Unify- ing Question Answering and Text Classification via Span Extraction. 2019. URL http://arxiv.org/abs/ 1904.09286.\n\nUnified QA : Crossing Format Boundaries with a Single QA System. D Khashabi, S Min, T Khot, A Sabharwal, O Tafjord, P Clark, H Hajishirzi, Findings of EMNLP. Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and Hajishirzi, H. Unified QA : Crossing Format Boundaries with a Single QA System. In Findings of EMNLP, 2020.\n\nBilinear Attention Networks. J Kim, J Jun, B Zhang, NeurIPS. Kim, J.-h., Jun, J., and Zhang, B.-t. Bilinear Attention Networks. In NeurIPS, pp. 1-12, 2018.\n\nVisual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L Jia-Li, D A Shamma, Michael Bernstein, L Fei-Fei, 10.1007/s11263-016-0981-7Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Jia-Li, L., Shamma, D. A., Michael Bernstein, and Fei-Fei, L. Visual Genome: Connecting Language and Vision Using Crowd- sourced Dense Image Annotations. International Jour- nal of Computer Vision, 2016. ISSN 15731405. doi: 10.1007/s11263-016-0981-7.\n\nAlbert: A lite bert for self-supervised learning of language representations. Z Chen, M Goodman, S Gimpel, K Sharma, P Soricut, R , ICLR. Unifying Vision-and-Language Tasks via Text Generation LanUnifying Vision-and-Language Tasks via Text Generation Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. Albert: A lite bert for self-supervised learning of language representations. In ICLR, 2020.\n\nTvqa: Localized, compositional video question answering. J Lei, L Yu, M Bansal, T L Berg, In EMNLP. Lei, J., Yu, L., Bansal, M., and Berg, T. L. Tvqa: Localized, compositional video question answering. In EMNLP, 2018.\n\nDenoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, P.-T Bart, Bart, ACL. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V., Zettlemoyer, L., and Bart, P.-t. BART: Denoising Sequence-to-Sequence Pre- training for Natural Language Generation, Translation, and Comprehension. In ACL, 2020.\n\nHERO: Hierarchical Encoder for Video+Language Omnirepresentation Pre-training. L Li, Y.-C Chen, Yu Cheng, Z G Yu, L Liu, J , EMNLP. Li, L., Chen, Y.-C., Yu Cheng, Z. G., Yu, L., and Liu, J. HERO: Hierarchical Encoder for Video+Language Omni- representation Pre-training. In EMNLP, 2020a.\n\nObject-Semantics Aligned Pre-training for Vision-Language Tasks. X Li, X Yin, C Li, P Zhang, X Hu, L Zhang, L Wang, H Hu, L Dong, F Wei, Y Choi, J Gao, Oscar, ECCV, 2020b. Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., Choi, Y., and Gao, J. Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks. In ECCV, 2020b. URL http://arxiv.org/abs/2004.06165.\n\nPrefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. 2021.\n\nCommon Objects in Context. T Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, Coco Microsoft, 10.1007/978-3-319-10602-148ECCV. Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra- manan, D., Doll\u00e1r, P., and Zitnick, C. L. Microsoft COCO: Common Objects in Context. In ECCV, 2014. ISBN 978- 3-319-10601-4. doi: 10.1007/978-3-319-10602-1 48.\n\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nDecoupled Weight Decay Regularization. I Loshchilov, F Hutter, ICLR. Loshchilov, I. and Hutter, F. Decoupled Weight De- cay Regularization. In ICLR, 2019. URL https: //openreview.net/forum?id=Bkg6RiCqY7.\n\nPretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. J Lu, D Batra, D Parikh, S Lee, Vilbert, NeurIPS. Lu, J., Batra, D., Parikh, D., and Lee, S. ViLBERT: Pre- training Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In NeurIPS, 2019. URL http://arxiv.org/abs/1908.02265.\n\n12-in-1: Multi-Task Vision and Language Representation Learning. J Lu, V Goswami, M Rohrbach, D Parikh, S Lee, CVPR. Lu, J., Goswami, V., Rohrbach, M., Parikh, D., and Lee, S. 12-in-1: Multi-Task Vision and Language Repre- sentation Learning. In CVPR, 2020. URL http: //arxiv.org/abs/1912.02315.\n\nGeneration and Comprehension of Unambiguous Object Descriptions. J Mao, J Huang, A Toshev, O Camburu, A Yuille, Murphy , K , CVPR. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A., and Murphy, K. Generation and Comprehension of Unam- biguous Object Descriptions. In CVPR, 2016.\n\nThe Natural Language Decathlon : Multitask Learning as Question Answering. B Mccann, N S Keskar, C Xiong, R Socher, Mccann, B., Keskar, N. S., Xiong, C., and Socher, R. The Natural Language Decathlon : Multitask Learning as Question Answering. 2018.\n\nEnd-to-end learning of visual representations from uncurated instructional videos. A Miech, J.-B Alayrac, L Smaira, I Laptev, J Sivic, A Zisserman, CVPR. Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J., and Zisserman, A. End-to-end learning of visual repre- sentations from uncurated instructional videos. In CVPR, 2020.\n\nMixed Precision Training. S Narang, G Diamos, E Elsen, P Micikevicius, J Alben, D Garcia, B Ginsburg, M Houston, O Kuchaiev, G Venkatesh, H Wu, ICLR. Narang, S., Diamos, G., Elsen, E., Micikevicius, P., Alben, J., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., and Wu, H. Mixed Precision Training. In ICLR, 2018. URL https://openreview.net/ forum?id=r1gs9JgRZ.\n\nDocument Ranking with a Pretrained Sequenceto-Sequence Model. R Nogueira, Z Jiang, J Lin, I R Mar, R Pradeep, Lin , J , Findings of EMNLP. Nogueira, R., Jiang, Z., Lin, J., Mar, I. R., Pradeep, R., and Lin, J. Document Ranking with a Pretrained Sequence- to-Sequence Model. In Findings of EMNLP, pp. 1-8, 2020.\n\nBLEU: a Method for Automatic Evaluation of Machine Translation. K Papineni, S Roukos, T Ward, W W Zhu, 10.3115/1073083.1073135ACL. Papineni, K., Roukos, S., Ward, T., and Zhu, W. W.-j. BLEU: a Method for Automatic Evaluation of Machine Translation. In ACL, 2002. ISBN 1- 55860-883-4. doi: 10.3115/1073083.1073135. URL http://portal.acm.org/citation.cfm? doid=1073083.1073135http://dl.acm. org/citation.cfm?id=1073135.\n\nAutomatic differentiation in PyTorch. A Paszke, S Gross, S Chintala, G Chana, E Yang, Z Devito, Z Lin, A Desmaison, L Antiga, A Lerer, NIPS Workshop. Paszke, A., Gross, S., Chintala, S., Chana, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic differentiation in PyTorch. In NIPS Workshop, 2017. URL https://openreview. net/pdf?id=BJJsrmfCZ.\n\nA Call for Clarity in Reporting BLEU Scores. M Post, WMT. 1Post, M. A Call for Clarity in Reporting BLEU Scores. In WMT, volume 1, pp. 186-191, 2018.\n\nUsing the Output Embedding to Improve Language Models. O Press, L Wolf, EACL. Press, O. and Wolf, L. Using the Output Embedding to Improve Language Models. In EACL, 2017.\n\nLearning Transferable Visual Models From Natural Language Supervision. A Radford, J Wook, K Chris, H Aditya, R Gabriel, G Sandhini, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, Radford, A., Wook, J., Chris, K., Aditya, H., Gabriel, R., Sandhini, G., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning Transferable Vi- sual Models From Natural Language Supervision. 2021.\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, JMLR. 21Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the Limits of Transfer Learning with a Unified Text-to- Text Transformer. JMLR, 21:1-67, 2020. URL http: //arxiv.org/abs/1910.10683.\n\nSquad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, EMNLP. Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions for machine comprehension of text. In EMNLP, 2016.\n\nTowards Real-Time Object Detection with Region Proposal Networks. S Ren, K He, R Girshick, J Sun, Faster R-Cnn, NIPS. Ren, S., He, K., Girshick, R., and Sun, J. Faster R- CNN: Towards Real-Time Object Detection with Re- gion Proposal Networks. In NIPS, 2015. URL https: //arxiv.org/abs/1506.01497.\n\nSelf-Attention with Relative Position Representations. P Shaw, J Uszkoreit, A Vaswani, NAACL. Shaw, P., Uszkoreit, J., and Vaswani, A. Self-Attention with Relative Position Representations. In NAACL, 2018.\n\nEliciting knowledge from language models with automatically generated prompts. T Shin, Y Razeghi, R L L Iv, E Wallace, S Singh, Autoprompt, EMNLP. Shin, T., Razeghi, Y., IV, R. L. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP, 2020.\n\nMASS: Masked Sequence to Sequence Pre-training for Language Generation. K Song, X Tan, T Qin, J Lu, T.-Y Liu, ICML. Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. MASS: Masked Sequence to Sequence Pre-training for Language Generation. In ICML, 2019. URL http://arxiv. org/abs/1905.02450.\n\nA Corpus for Reasoning About Natural Language Grounded in Photographs. A Suhr, S Zhou, A Zhang, I Zhang, H Bai, Artzi , Y , ACL. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y. A Corpus for Reasoning About Natural Language Grounded in Photographs. In ACL, 2019. URL http: //arxiv.org/abs/1811.00491.\n\nContrastive Bidirectional Transformer for Temporal Representation Learning. C Sun, F Baradel, K Murphy, C Schmid, Sun, C., Baradel, F., Murphy, K., and Schmid, C. Con- trastive Bidirectional Transformer for Temporal Represen- tation Learning. 2019a. URL http://arxiv.org/ abs/1906.05743.\n\nVideoBERT: A Joint Model for Video and Language Representation Learning. C Sun, A Myers, C Vondrick, K Murphy, C Schmid, ICCV. Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid, C. VideoBERT: A Joint Model for Video and Language Representation Learning. In ICCV, 2019b. URL http: //arxiv.org/abs/1904.01766.\n\nLearning Cross-Modality Encoder Representations from Transformers. H Tan, M Bansal, Lxmert, EMNLP. Tan, H. and Bansal, M. LXMERT: Learning Cross- Modality Encoder Representations from Transformers. In EMNLP, 2019. URL http://arxiv.org/abs/ 1908.07490.\n\nAttention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, NIPS. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polo- sukhin, I. Attention Is All You Need. In NIPS, 2017. URL https://papers.nips.cc/paper/ 7181-attention-is-all-you-need.pdf.\n\nCIDEr: Consensus-based Image Description Evaluation. R Vedantam, C L Zitnick, D Parikh, CVPR. Vedantam, R., Zitnick, C. L., and Parikh, D. CIDEr: Consensus-based Image Description Evaluation. In CVPR, nov 2015. URL http://arxiv.org/abs/ 1411.5726.\n\nGlue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, In ICLR. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analy- sis platform for natural language understanding. In ICLR, 2018.\n\nA broadcoverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S R Bowman, Williams, A., Nangia, N., and Bowman, S. R. A broad- coverage challenge corpus for sentence understanding through inference. In NAACL, 2017.\n\nHuggingFace's Transformers: Stateof-the-art Natural Language Processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Brew, Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., and Brew, J. HuggingFace's Transformers: State- of-the-art Natural Language Processing. 2019. URL http://arxiv.org/abs/1910.03771.\n\nXGPT : Cross-modal Generative Pre-Training for Image Captioning. Q Xia, H Huang, N Duan, D Zhang, Ji , L , Xia, Q., Huang, H., Duan, N., Zhang, D., and Ji, L. XGPT : Cross-modal Generative Pre-Training for Image Caption- ing. 2020. URL https://arxiv.org/abs/2003. 01473.\n\nMsr-vtt: A large video description dataset for bridging video and language. J Xu, T Mei, T Yao, Y Rui, CVPR. Xu, J., Mei, T., Yao, T., and Rui, Y. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, 2016.\n\nXlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, NeurIPS. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q. V. Xlnet: Generalized autoregres- sive pretraining for language understanding. In NeurIPS, 2019.\n\nMultimodal Transformer for Multimodal Machine Translation. S Yao, X Wan, 10.18653/v1/2020.acl-main.400ACL. Yao, S. and Wan, X. Multimodal Transformer for Mul- timodal Machine Translation. In ACL, pp. 4346-4350, 2020. doi: 10.18653/v1/2020.acl-main.400.\n\nMAttNet : Modular Attention Network for Referring Expression Comprehension. L Yu, Z Lin, X Shen, J Yang, X Lu, M Bansal, T L Berg, CVPR. Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., and Berg, T. L. MAttNet : Modular Attention Network for Referring Expression Comprehension. In CVPR, 2018a. URL https://arxiv.org/abs/1801.08186.\n\nA joint sequence fusion model for video question answering and retrieval. Y Yu, J Kim, Kim , G , ECCV. Yu, Y., Kim, J., and Kim, G. A joint sequence fusion model for video question answering and retrieval. In ECCV, 2018b.\n\nSwag: A large-scale adversarial dataset for grounded commonsense inference. R Zellers, Y Bisk, R Schwartz, Y Choi, In EMNLP. Zellers, R., Bisk, Y., Schwartz, R., and Choi, Y. Swag: A large-scale adversarial dataset for grounded common- sense inference. In EMNLP, 2018.\n\nFrom Recognition to Cognition: Visual Commonsense Reasoning. R Zellers, Y Bisk, A Farhadi, Y Choi, CVPR. Zellers, R., Bisk, Y., Farhadi, A., and Choi, Y. From Recog- nition to Cognition: Visual Commonsense Reasoning. In CVPR, 2019. URL http://arxiv.org/abs/ 1811.10830.\n\nMaking Visual Representations Matter in Vision-Language Models. P Zhang, X Li, X Hu, J Yang, L Zhang, L Wang, Y Choi, I Gao, Vinvl, Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., and Gao, I. VinVL: Making Visual Representa- tions Matter in Vision-Language Models. 2021.\n\nTowards automatic learning of procedures from web instructional videos. L Zhou, C Xu, J J Corso, AAAI. Zhou, L., Xu, C., and Corso, J. J. Towards automatic learn- ing of procedures from web instructional videos. In AAAI, 2018.\n\nUnified Vision-Language Pre-Training for Image Captioning and VQA. L Zhou, H Palangi, L Zhang, H Hu, J J Corso, J Gao, AAAI. Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J. J., and Gao, J. Unified Vision-Language Pre-Training for Image Captioning and VQA. In AAAI, 2020. URL http: //arxiv.org/abs/1909.11059.\n\nActbert: Learning global-local videotext representations. L Zhu, Y Yang, CVPR. Zhu, L. and Yang, Y. Actbert: Learning global-local video- text representations. In CVPR, 2020.\n\nVisual7W: Grounded Question Answering in Images. Y Zhu, O Groth, M Bernstein, L Fei-Fei, 10.1109/CVPR.2016.540CVPR. Zhu, Y., Groth, O., Bernstein, M., and Fei-Fei, L. Visual7W: Grounded Question Answering in Images. In CVPR, 2016. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR. 2016.540. URL http://arxiv.org/abs/1511.\n\nNLVR 2 11 Train / val / test-P splits consist of 86,373 / 6982 / 6967 sentences, respectively. We train our model on train split and use val split for validation. NLVR 2 11 Train / val / test-P splits consist of 86,373 / 6982 / 6967 sentences, respectively. We train our model on train split and use val split for validation.\n\nVCR 12 Train / val / test splits consist of 212. 923534VCR 12 Train / val / test splits consist of 212,923 / 26,534\n\n/ 25,263 questions, respectively. We train our model on train split and use val split for validation. / 25,263 questions, respectively. We train our model on train split and use val split for validation.\n\nRefCOCOg 13 We use umd split, which consists of train / val / test sets with 42,226 / 2,573 / 5,023 sentences, respectively. Following UNITER (Chen et al., 2020) and MAttNet. Yu , we use ground truth COCO boxes for training, and use the detected boxes from an off-the-shelf Mask R-CNN 14 as candidates during inference. RefCOCOg 13 We use umd split, which consists of train / val / test sets with 42,226 / 2,573 / 5,023 sentences, respec- tively. Following UNITER (Chen et al., 2020) and MAttNet (Yu et al., 2018a), we use ground truth COCO boxes for training, and use the detected boxes from an off-the-shelf Mask R-CNN 14 as candidates during inference.\n", "annotations": {"author": "[{\"end\":87,\"start\":58},{\"end\":132,\"start\":88},{\"end\":177,\"start\":133},{\"end\":228,\"start\":178}]", "publisher": null, "author_last_name": "[{\"end\":68,\"start\":65},{\"end\":95,\"start\":92},{\"end\":140,\"start\":137},{\"end\":190,\"start\":184}]", "author_first_name": "[{\"end\":64,\"start\":58},{\"end\":91,\"start\":88},{\"end\":136,\"start\":133},{\"end\":183,\"start\":178}]", "author_affiliation": "[{\"end\":86,\"start\":70},{\"end\":131,\"start\":115},{\"end\":176,\"start\":160},{\"end\":227,\"start\":211}]", "title": "[{\"end\":55,\"start\":1},{\"end\":283,\"start\":229}]", "venue": null, "abstract": "[{\"end\":1647,\"start\":285}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1778,\"start\":1757},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":1840,\"start\":1820},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1858,\"start\":1842},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1878,\"start\":1860},{\"end\":1897,\"start\":1880},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2701,\"start\":2682},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2830,\"start\":2812},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3939,\"start\":3916},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3958,\"start\":3939},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3976,\"start\":3958},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4350,\"start\":4329},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4380,\"start\":4360},{\"end\":5409,\"start\":5385},{\"end\":5477,\"start\":5449},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5542,\"start\":5523},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":5602,\"start\":5580},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5656,\"start\":5637},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5727,\"start\":5705},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6755,\"start\":6733},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6775,\"start\":6755},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6792,\"start\":6775},{\"end\":6809,\"start\":6792},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6828,\"start\":6809},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":6846,\"start\":6828},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6866,\"start\":6846},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6970,\"start\":6946},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6991,\"start\":6970},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7009,\"start\":6991},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7031,\"start\":7009},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7106,\"start\":7087},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7124,\"start\":7106},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7143,\"start\":7124},{\"end\":7160,\"start\":7143},{\"end\":7177,\"start\":7160},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7198,\"start\":7177},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":7217,\"start\":7198},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7271,\"start\":7252},{\"end\":7273,\"start\":7271},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7290,\"start\":7273},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":7307,\"start\":7290},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7326,\"start\":7307},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7423,\"start\":7401},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7440,\"start\":7423},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7457,\"start\":7440},{\"end\":7515,\"start\":7492},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7532,\"start\":7515},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7549,\"start\":7532},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7565,\"start\":7549},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7583,\"start\":7565},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7624,\"start\":7605},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7640,\"start\":7624},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7658,\"start\":7640},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8012,\"start\":7991},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8051,\"start\":8030},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8093,\"start\":8072},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8112,\"start\":8093},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8134,\"start\":8112},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8735,\"start\":8714},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8770,\"start\":8750},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9244,\"start\":9226},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9791,\"start\":9769},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9855,\"start\":9832},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10300,\"start\":10280},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10412,\"start\":10390},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10432,\"start\":10412},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10593,\"start\":10574},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":11565,\"start\":11543},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12445,\"start\":12425},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12466,\"start\":12447},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12983,\"start\":12965},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15053,\"start\":15035},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15071,\"start\":15053},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15116,\"start\":15095},{\"end\":15452,\"start\":15428},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":15494,\"start\":15475},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16247,\"start\":16227},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16271,\"start\":16252},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16681,\"start\":16662},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16726,\"start\":16706},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16742,\"start\":16726},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16760,\"start\":16742},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18881,\"start\":18860},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19012,\"start\":18985},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19151,\"start\":19130},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":19200,\"start\":19181},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":19507,\"start\":19487},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19523,\"start\":19507},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19541,\"start\":19523},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":19559,\"start\":19541},{\"end\":19576,\"start\":19559},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":19593,\"start\":19576},{\"end\":20113,\"start\":20089},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":20273,\"start\":20253},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20289,\"start\":20273},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20307,\"start\":20289},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":20325,\"start\":20307},{\"end\":20342,\"start\":20325},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21884,\"start\":21866},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21901,\"start\":21884},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21922,\"start\":21901},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22366,\"start\":22343},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23671,\"start\":23653},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24038,\"start\":24021},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24664,\"start\":24646},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":25418,\"start\":25397},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25905,\"start\":25883},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27344,\"start\":27325},{\"end\":27395,\"start\":27369},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27460,\"start\":27442},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27882,\"start\":27864},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28006,\"start\":27983},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28047,\"start\":28023},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28078,\"start\":28055},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28855,\"start\":28833},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29238,\"start\":29226},{\"end\":29240,\"start\":29239},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":29356,\"start\":29339},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29387,\"start\":29364},{\"end\":29814,\"start\":29792},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30709,\"start\":30692},{\"end\":33109,\"start\":33083},{\"end\":33324,\"start\":33298},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33394,\"start\":33376},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":33507,\"start\":33487},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":34204,\"start\":34185},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":40937,\"start\":40916},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":40975,\"start\":40956},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41216,\"start\":41196}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":33802,\"start\":33640},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34182,\"start\":33803},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34682,\"start\":34183},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35786,\"start\":34683},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36722,\"start\":35787},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":37237,\"start\":36723},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":37702,\"start\":37238},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":37945,\"start\":37703},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":38142,\"start\":37946},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":39460,\"start\":38143},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":40023,\"start\":39461},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":40405,\"start\":40024},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":42365,\"start\":40406},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":43015,\"start\":42366},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":43705,\"start\":43016},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":44291,\"start\":43706}]", "paragraph": "[{\"end\":2018,\"start\":1663},{\"end\":2056,\"start\":2020},{\"end\":2517,\"start\":2058},{\"end\":2901,\"start\":2519},{\"end\":4080,\"start\":2903},{\"end\":5150,\"start\":4082},{\"end\":6631,\"start\":5152},{\"end\":7853,\"start\":6649},{\"end\":8501,\"start\":7855},{\"end\":9129,\"start\":8511},{\"end\":9760,\"start\":9151},{\"end\":9876,\"start\":9762},{\"end\":10683,\"start\":9896},{\"end\":11462,\"start\":10685},{\"end\":12088,\"start\":11495},{\"end\":12697,\"start\":12163},{\"end\":12984,\"start\":12798},{\"end\":13464,\"start\":12986},{\"end\":13715,\"start\":13591},{\"end\":14279,\"start\":13790},{\"end\":14701,\"start\":14316},{\"end\":14971,\"start\":14717},{\"end\":15754,\"start\":14992},{\"end\":16185,\"start\":15776},{\"end\":16565,\"start\":16187},{\"end\":16913,\"start\":16567},{\"end\":17529,\"start\":16915},{\"end\":18065,\"start\":17531},{\"end\":18392,\"start\":18067},{\"end\":18723,\"start\":18394},{\"end\":19201,\"start\":18762},{\"end\":19260,\"start\":19203},{\"end\":19864,\"start\":19293},{\"end\":20196,\"start\":19907},{\"end\":21812,\"start\":20198},{\"end\":22368,\"start\":21814},{\"end\":23412,\"start\":22414},{\"end\":25324,\"start\":23461},{\"end\":26253,\"start\":25362},{\"end\":26630,\"start\":26255},{\"end\":27210,\"start\":26643},{\"end\":28713,\"start\":27245},{\"end\":29815,\"start\":28758},{\"end\":30576,\"start\":29841},{\"end\":31398,\"start\":30578},{\"end\":31859,\"start\":31413},{\"end\":32139,\"start\":31861},{\"end\":32336,\"start\":32172},{\"end\":32653,\"start\":32366},{\"end\":33185,\"start\":32679},{\"end\":33461,\"start\":33211},{\"end\":33639,\"start\":33463}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12162,\"start\":12089},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12747,\"start\":12698},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13789,\"start\":13716},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14315,\"start\":14280},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26642,\"start\":26631}]", "table_ref": "[{\"end\":10046,\"start\":10040},{\"end\":14537,\"start\":14530},{\"end\":15994,\"start\":15987},{\"end\":16551,\"start\":16544},{\"end\":17101,\"start\":17094},{\"end\":17528,\"start\":17521},{\"end\":17882,\"start\":17875},{\"end\":18555,\"start\":18548},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":19619,\"start\":19612},{\"end\":19694,\"start\":19686},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20012,\"start\":20005},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24188,\"start\":24181},{\"end\":26135,\"start\":26128},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":26712,\"start\":26705},{\"end\":28103,\"start\":28096},{\"end\":28878,\"start\":28871},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":29396,\"start\":29389},{\"end\":30417,\"start\":30410},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":31217,\"start\":31209},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":32183,\"start\":32175},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32390,\"start\":32369},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32489,\"start\":32481}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1661,\"start\":1649},{\"attributes\":{\"n\":\"2.\"},\"end\":6647,\"start\":6634},{\"attributes\":{\"n\":\"3.\"},\"end\":8509,\"start\":8504},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9149,\"start\":9132},{\"attributes\":{\"n\":\"3.2.\"},\"end\":9894,\"start\":9879},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11493,\"start\":11465},{\"attributes\":{\"n\":\"3.4.\"},\"end\":12796,\"start\":12749},{\"end\":13589,\"start\":13467},{\"attributes\":{\"n\":\"4.\"},\"end\":14715,\"start\":14704},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14990,\"start\":14974},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15774,\"start\":15757},{\"attributes\":{\"n\":\"4.3.\"},\"end\":18760,\"start\":18726},{\"attributes\":{\"n\":\"5.\"},\"end\":19291,\"start\":19263},{\"attributes\":{\"n\":\"5.1.\"},\"end\":19905,\"start\":19867},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22412,\"start\":22371},{\"attributes\":{\"n\":\"5.3.\"},\"end\":23459,\"start\":23415},{\"attributes\":{\"n\":\"5.4.\"},\"end\":25360,\"start\":25327},{\"attributes\":{\"n\":\"5.5.\"},\"end\":27243,\"start\":27213},{\"attributes\":{\"n\":\"5.6.\"},\"end\":28756,\"start\":28716},{\"attributes\":{\"n\":\"5.7.\"},\"end\":29839,\"start\":29818},{\"attributes\":{\"n\":\"6.\"},\"end\":31411,\"start\":31401},{\"end\":32170,\"start\":32142},{\"end\":32364,\"start\":32339},{\"end\":32677,\"start\":32656},{\"end\":33209,\"start\":33188},{\"end\":33651,\"start\":33641},{\"end\":35797,\"start\":35788},{\"end\":36733,\"start\":36724},{\"end\":37248,\"start\":37239},{\"end\":37711,\"start\":37704},{\"end\":37956,\"start\":37947},{\"end\":38153,\"start\":38144},{\"end\":39471,\"start\":39462},{\"end\":40035,\"start\":40025},{\"end\":40417,\"start\":40407},{\"end\":42377,\"start\":42367},{\"end\":43027,\"start\":43017},{\"end\":43717,\"start\":43707}]", "table": "[{\"end\":34182,\"start\":33822},{\"end\":34682,\"start\":34205},{\"end\":35786,\"start\":35104},{\"end\":36722,\"start\":36102},{\"end\":37237,\"start\":37032},{\"end\":37702,\"start\":37406},{\"end\":37945,\"start\":37713},{\"end\":38142,\"start\":37990},{\"end\":39460,\"start\":38330},{\"end\":40023,\"start\":39481},{\"end\":40405,\"start\":40212},{\"end\":42365,\"start\":41640},{\"end\":43015,\"start\":42693},{\"end\":43705,\"start\":43261},{\"end\":44291,\"start\":43771}]", "figure_caption": "[{\"end\":33802,\"start\":33653},{\"end\":33822,\"start\":33805},{\"end\":34205,\"start\":34185},{\"end\":35104,\"start\":34685},{\"end\":36102,\"start\":35799},{\"end\":37032,\"start\":36735},{\"end\":37406,\"start\":37250},{\"end\":37990,\"start\":37958},{\"end\":38330,\"start\":38155},{\"end\":39481,\"start\":39473},{\"end\":40212,\"start\":40038},{\"end\":41640,\"start\":40420},{\"end\":42693,\"start\":42380},{\"end\":43261,\"start\":43030},{\"end\":43771,\"start\":43720}]", "figure_ref": "[{\"end\":2268,\"start\":2260},{\"end\":3340,\"start\":3334},{\"end\":4078,\"start\":4072},{\"end\":8961,\"start\":8955},{\"end\":9253,\"start\":9245},{\"end\":9875,\"start\":9869},{\"end\":10864,\"start\":10858},{\"end\":11953,\"start\":11947},{\"end\":13093,\"start\":13088},{\"end\":13126,\"start\":13118},{\"end\":14154,\"start\":14148},{\"end\":14525,\"start\":14513},{\"end\":16852,\"start\":16845},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22839,\"start\":22833}]", "bib_author_first_name": "[{\"end\":45550,\"start\":45549},{\"end\":45562,\"start\":45561},{\"end\":45574,\"start\":45573},{\"end\":45585,\"start\":45584},{\"end\":45876,\"start\":45875},{\"end\":45888,\"start\":45887},{\"end\":45894,\"start\":45893},{\"end\":45905,\"start\":45904},{\"end\":45914,\"start\":45913},{\"end\":45925,\"start\":45924},{\"end\":45934,\"start\":45933},{\"end\":46269,\"start\":46268},{\"end\":46281,\"start\":46280},{\"end\":46491,\"start\":46490},{\"end\":46493,\"start\":46492},{\"end\":46502,\"start\":46501},{\"end\":46510,\"start\":46509},{\"end\":46519,\"start\":46518},{\"end\":46530,\"start\":46529},{\"end\":46540,\"start\":46539},{\"end\":46552,\"start\":46551},{\"end\":46567,\"start\":46566},{\"end\":46576,\"start\":46575},{\"end\":46586,\"start\":46585},{\"end\":46596,\"start\":46595},{\"end\":46607,\"start\":46606},{\"end\":46623,\"start\":46622},{\"end\":46634,\"start\":46633},{\"end\":46646,\"start\":46645},{\"end\":46655,\"start\":46654},{\"end\":46665,\"start\":46664},{\"end\":46667,\"start\":46666},{\"end\":46678,\"start\":46677},{\"end\":46684,\"start\":46683},{\"end\":46694,\"start\":46693},{\"end\":46703,\"start\":46702},{\"end\":46711,\"start\":46710},{\"end\":46721,\"start\":46720},{\"end\":46731,\"start\":46730},{\"end\":46739,\"start\":46738},{\"end\":46748,\"start\":46747},{\"end\":46757,\"start\":46756},{\"end\":46767,\"start\":46766},{\"end\":46781,\"start\":46780},{\"end\":46792,\"start\":46791},{\"end\":46810,\"start\":46804},{\"end\":46814,\"start\":46813},{\"end\":47384,\"start\":47383},{\"end\":47396,\"start\":47395},{\"end\":47410,\"start\":47409},{\"end\":47420,\"start\":47419},{\"end\":47745,\"start\":47744},{\"end\":47753,\"start\":47752},{\"end\":47764,\"start\":47760},{\"end\":47771,\"start\":47770},{\"end\":47783,\"start\":47782},{\"end\":47792,\"start\":47791},{\"end\":47802,\"start\":47801},{\"end\":47804,\"start\":47803},{\"end\":48069,\"start\":48068},{\"end\":48077,\"start\":48076},{\"end\":48083,\"start\":48082},{\"end\":48089,\"start\":48088},{\"end\":48091,\"start\":48090},{\"end\":48100,\"start\":48099},{\"end\":48109,\"start\":48108},{\"end\":48116,\"start\":48115},{\"end\":48125,\"start\":48124},{\"end\":48410,\"start\":48409},{\"end\":48417,\"start\":48416},{\"end\":48423,\"start\":48422},{\"end\":48434,\"start\":48433},{\"end\":48448,\"start\":48447},{\"end\":48775,\"start\":48774},{\"end\":48787,\"start\":48783},{\"end\":48796,\"start\":48795},{\"end\":48798,\"start\":48797},{\"end\":48804,\"start\":48803},{\"end\":48806,\"start\":48805},{\"end\":49050,\"start\":49049},{\"end\":49063,\"start\":49059},{\"end\":49072,\"start\":49071},{\"end\":49079,\"start\":49078},{\"end\":49357,\"start\":49356},{\"end\":49368,\"start\":49367},{\"end\":49377,\"start\":49376},{\"end\":49393,\"start\":49392},{\"end\":49624,\"start\":49623},{\"end\":49631,\"start\":49630},{\"end\":49643,\"start\":49639},{\"end\":49647,\"start\":49646},{\"end\":49891,\"start\":49890},{\"end\":49900,\"start\":49899},{\"end\":49908,\"start\":49907},{\"end\":49919,\"start\":49918},{\"end\":49935,\"start\":49934},{\"end\":49944,\"start\":49943},{\"end\":50382,\"start\":50378},{\"end\":50394,\"start\":50393},{\"end\":50402,\"start\":50401},{\"end\":50412,\"start\":50411},{\"end\":50425,\"start\":50424},{\"end\":50437,\"start\":50436},{\"end\":50445,\"start\":50444},{\"end\":50456,\"start\":50455},{\"end\":50469,\"start\":50468},{\"end\":50482,\"start\":50481},{\"end\":50492,\"start\":50491},{\"end\":50765,\"start\":50764},{\"end\":50771,\"start\":50770},{\"end\":50783,\"start\":50782},{\"end\":50793,\"start\":50792},{\"end\":50944,\"start\":50943},{\"end\":50953,\"start\":50952},{\"end\":50961,\"start\":50960},{\"end\":50969,\"start\":50968},{\"end\":50971,\"start\":50970},{\"end\":51258,\"start\":51257},{\"end\":51267,\"start\":51266},{\"end\":51275,\"start\":51274},{\"end\":51282,\"start\":51281},{\"end\":51288,\"start\":51287},{\"end\":51557,\"start\":51556},{\"end\":51559,\"start\":51558},{\"end\":51569,\"start\":51568},{\"end\":51571,\"start\":51570},{\"end\":51892,\"start\":51891},{\"end\":51904,\"start\":51903},{\"end\":52214,\"start\":52213},{\"end\":52216,\"start\":52215},{\"end\":52226,\"start\":52225},{\"end\":52236,\"start\":52235},{\"end\":52245,\"start\":52244},{\"end\":52493,\"start\":52492},{\"end\":52505,\"start\":52504},{\"end\":52512,\"start\":52511},{\"end\":52520,\"start\":52519},{\"end\":52533,\"start\":52532},{\"end\":52544,\"start\":52543},{\"end\":52553,\"start\":52552},{\"end\":52800,\"start\":52799},{\"end\":52807,\"start\":52806},{\"end\":52814,\"start\":52813},{\"end\":53060,\"start\":53059},{\"end\":53071,\"start\":53070},{\"end\":53078,\"start\":53077},{\"end\":53087,\"start\":53086},{\"end\":53098,\"start\":53097},{\"end\":53106,\"start\":53105},{\"end\":53117,\"start\":53116},{\"end\":53125,\"start\":53124},{\"end\":53139,\"start\":53138},{\"end\":53149,\"start\":53148},{\"end\":53151,\"start\":53150},{\"end\":53167,\"start\":53160},{\"end\":53180,\"start\":53179},{\"end\":53641,\"start\":53640},{\"end\":53649,\"start\":53648},{\"end\":53660,\"start\":53659},{\"end\":53670,\"start\":53669},{\"end\":53680,\"start\":53679},{\"end\":53691,\"start\":53690},{\"end\":54037,\"start\":54036},{\"end\":54044,\"start\":54043},{\"end\":54050,\"start\":54049},{\"end\":54060,\"start\":54059},{\"end\":54062,\"start\":54061},{\"end\":54307,\"start\":54306},{\"end\":54316,\"start\":54315},{\"end\":54323,\"start\":54322},{\"end\":54332,\"start\":54331},{\"end\":54349,\"start\":54348},{\"end\":54360,\"start\":54359},{\"end\":54368,\"start\":54367},{\"end\":54380,\"start\":54379},{\"end\":54398,\"start\":54394},{\"end\":54749,\"start\":54748},{\"end\":54758,\"start\":54754},{\"end\":54767,\"start\":54765},{\"end\":54776,\"start\":54775},{\"end\":54778,\"start\":54777},{\"end\":54784,\"start\":54783},{\"end\":54791,\"start\":54790},{\"end\":55024,\"start\":55023},{\"end\":55030,\"start\":55029},{\"end\":55037,\"start\":55036},{\"end\":55043,\"start\":55042},{\"end\":55052,\"start\":55051},{\"end\":55058,\"start\":55057},{\"end\":55067,\"start\":55066},{\"end\":55075,\"start\":55074},{\"end\":55081,\"start\":55080},{\"end\":55089,\"start\":55088},{\"end\":55096,\"start\":55095},{\"end\":55104,\"start\":55103},{\"end\":55432,\"start\":55431},{\"end\":55434,\"start\":55433},{\"end\":55440,\"start\":55439},{\"end\":55568,\"start\":55567},{\"end\":55570,\"start\":55569},{\"end\":55577,\"start\":55576},{\"end\":55586,\"start\":55585},{\"end\":55598,\"start\":55597},{\"end\":55606,\"start\":55605},{\"end\":55616,\"start\":55615},{\"end\":55627,\"start\":55626},{\"end\":55637,\"start\":55636},{\"end\":55639,\"start\":55638},{\"end\":55653,\"start\":55649},{\"end\":55923,\"start\":55922},{\"end\":55930,\"start\":55929},{\"end\":55937,\"start\":55936},{\"end\":55946,\"start\":55945},{\"end\":55952,\"start\":55951},{\"end\":55961,\"start\":55960},{\"end\":55969,\"start\":55968},{\"end\":55977,\"start\":55976},{\"end\":55986,\"start\":55985},{\"end\":56001,\"start\":56000},{\"end\":56349,\"start\":56348},{\"end\":56363,\"start\":56362},{\"end\":56604,\"start\":56603},{\"end\":56610,\"start\":56609},{\"end\":56619,\"start\":56618},{\"end\":56629,\"start\":56628},{\"end\":56918,\"start\":56917},{\"end\":56924,\"start\":56923},{\"end\":56935,\"start\":56934},{\"end\":56947,\"start\":56946},{\"end\":56957,\"start\":56956},{\"end\":57215,\"start\":57214},{\"end\":57222,\"start\":57221},{\"end\":57231,\"start\":57230},{\"end\":57241,\"start\":57240},{\"end\":57252,\"start\":57251},{\"end\":57267,\"start\":57261},{\"end\":57271,\"start\":57270},{\"end\":57511,\"start\":57510},{\"end\":57521,\"start\":57520},{\"end\":57523,\"start\":57522},{\"end\":57533,\"start\":57532},{\"end\":57542,\"start\":57541},{\"end\":57770,\"start\":57769},{\"end\":57782,\"start\":57778},{\"end\":57793,\"start\":57792},{\"end\":57803,\"start\":57802},{\"end\":57813,\"start\":57812},{\"end\":57822,\"start\":57821},{\"end\":58048,\"start\":58047},{\"end\":58058,\"start\":58057},{\"end\":58068,\"start\":58067},{\"end\":58077,\"start\":58076},{\"end\":58093,\"start\":58092},{\"end\":58102,\"start\":58101},{\"end\":58112,\"start\":58111},{\"end\":58124,\"start\":58123},{\"end\":58135,\"start\":58134},{\"end\":58147,\"start\":58146},{\"end\":58160,\"start\":58159},{\"end\":58467,\"start\":58466},{\"end\":58479,\"start\":58478},{\"end\":58488,\"start\":58487},{\"end\":58495,\"start\":58494},{\"end\":58497,\"start\":58496},{\"end\":58504,\"start\":58503},{\"end\":58517,\"start\":58514},{\"end\":58521,\"start\":58520},{\"end\":58781,\"start\":58780},{\"end\":58793,\"start\":58792},{\"end\":58803,\"start\":58802},{\"end\":58811,\"start\":58810},{\"end\":58813,\"start\":58812},{\"end\":59174,\"start\":59173},{\"end\":59184,\"start\":59183},{\"end\":59193,\"start\":59192},{\"end\":59205,\"start\":59204},{\"end\":59214,\"start\":59213},{\"end\":59222,\"start\":59221},{\"end\":59232,\"start\":59231},{\"end\":59239,\"start\":59238},{\"end\":59252,\"start\":59251},{\"end\":59262,\"start\":59261},{\"end\":59560,\"start\":59559},{\"end\":59721,\"start\":59720},{\"end\":59730,\"start\":59729},{\"end\":59909,\"start\":59908},{\"end\":59920,\"start\":59919},{\"end\":59928,\"start\":59927},{\"end\":59937,\"start\":59936},{\"end\":59947,\"start\":59946},{\"end\":59958,\"start\":59957},{\"end\":59970,\"start\":59969},{\"end\":59980,\"start\":59979},{\"end\":59990,\"start\":59989},{\"end\":60001,\"start\":60000},{\"end\":60010,\"start\":60009},{\"end\":60021,\"start\":60020},{\"end\":60349,\"start\":60348},{\"end\":60359,\"start\":60358},{\"end\":60370,\"start\":60369},{\"end\":60381,\"start\":60380},{\"end\":60388,\"start\":60387},{\"end\":60398,\"start\":60397},{\"end\":60408,\"start\":60407},{\"end\":60416,\"start\":60415},{\"end\":60422,\"start\":60421},{\"end\":60424,\"start\":60423},{\"end\":60748,\"start\":60747},{\"end\":60761,\"start\":60760},{\"end\":60770,\"start\":60769},{\"end\":60781,\"start\":60780},{\"end\":60994,\"start\":60993},{\"end\":61001,\"start\":61000},{\"end\":61007,\"start\":61006},{\"end\":61019,\"start\":61018},{\"end\":61282,\"start\":61281},{\"end\":61290,\"start\":61289},{\"end\":61303,\"start\":61302},{\"end\":61513,\"start\":61512},{\"end\":61521,\"start\":61520},{\"end\":61532,\"start\":61531},{\"end\":61536,\"start\":61533},{\"end\":61542,\"start\":61541},{\"end\":61553,\"start\":61552},{\"end\":61825,\"start\":61824},{\"end\":61833,\"start\":61832},{\"end\":61840,\"start\":61839},{\"end\":61847,\"start\":61846},{\"end\":61856,\"start\":61852},{\"end\":62117,\"start\":62116},{\"end\":62125,\"start\":62124},{\"end\":62133,\"start\":62132},{\"end\":62142,\"start\":62141},{\"end\":62151,\"start\":62150},{\"end\":62162,\"start\":62157},{\"end\":62166,\"start\":62165},{\"end\":62440,\"start\":62439},{\"end\":62447,\"start\":62446},{\"end\":62458,\"start\":62457},{\"end\":62468,\"start\":62467},{\"end\":62726,\"start\":62725},{\"end\":62733,\"start\":62732},{\"end\":62742,\"start\":62741},{\"end\":62754,\"start\":62753},{\"end\":62764,\"start\":62763},{\"end\":63036,\"start\":63035},{\"end\":63043,\"start\":63042},{\"end\":63249,\"start\":63248},{\"end\":63260,\"start\":63259},{\"end\":63271,\"start\":63270},{\"end\":63281,\"start\":63280},{\"end\":63294,\"start\":63293},{\"end\":63303,\"start\":63302},{\"end\":63305,\"start\":63304},{\"end\":63314,\"start\":63313},{\"end\":63324,\"start\":63323},{\"end\":63621,\"start\":63620},{\"end\":63633,\"start\":63632},{\"end\":63635,\"start\":63634},{\"end\":63646,\"start\":63645},{\"end\":63904,\"start\":63903},{\"end\":63912,\"start\":63911},{\"end\":63921,\"start\":63920},{\"end\":63932,\"start\":63931},{\"end\":63940,\"start\":63939},{\"end\":63948,\"start\":63947},{\"end\":63950,\"start\":63949},{\"end\":64225,\"start\":64224},{\"end\":64237,\"start\":64236},{\"end\":64247,\"start\":64246},{\"end\":64249,\"start\":64248},{\"end\":64474,\"start\":64473},{\"end\":64482,\"start\":64481},{\"end\":64491,\"start\":64490},{\"end\":64499,\"start\":64498},{\"end\":64511,\"start\":64510},{\"end\":64523,\"start\":64522},{\"end\":64530,\"start\":64529},{\"end\":64540,\"start\":64539},{\"end\":64549,\"start\":64548},{\"end\":64557,\"start\":64556},{\"end\":64570,\"start\":64569},{\"end\":64891,\"start\":64890},{\"end\":64898,\"start\":64897},{\"end\":64907,\"start\":64906},{\"end\":64915,\"start\":64914},{\"end\":64925,\"start\":64923},{\"end\":64929,\"start\":64928},{\"end\":65174,\"start\":65173},{\"end\":65180,\"start\":65179},{\"end\":65187,\"start\":65186},{\"end\":65194,\"start\":65193},{\"end\":65411,\"start\":65410},{\"end\":65419,\"start\":65418},{\"end\":65426,\"start\":65425},{\"end\":65434,\"start\":65433},{\"end\":65447,\"start\":65446},{\"end\":65449,\"start\":65448},{\"end\":65466,\"start\":65465},{\"end\":65468,\"start\":65467},{\"end\":65717,\"start\":65716},{\"end\":65724,\"start\":65723},{\"end\":65988,\"start\":65987},{\"end\":65994,\"start\":65993},{\"end\":66001,\"start\":66000},{\"end\":66009,\"start\":66008},{\"end\":66017,\"start\":66016},{\"end\":66023,\"start\":66022},{\"end\":66033,\"start\":66032},{\"end\":66035,\"start\":66034},{\"end\":66327,\"start\":66326},{\"end\":66333,\"start\":66332},{\"end\":66342,\"start\":66339},{\"end\":66346,\"start\":66345},{\"end\":66552,\"start\":66551},{\"end\":66563,\"start\":66562},{\"end\":66571,\"start\":66570},{\"end\":66583,\"start\":66582},{\"end\":66807,\"start\":66806},{\"end\":66818,\"start\":66817},{\"end\":66826,\"start\":66825},{\"end\":66837,\"start\":66836},{\"end\":67081,\"start\":67080},{\"end\":67090,\"start\":67089},{\"end\":67096,\"start\":67095},{\"end\":67102,\"start\":67101},{\"end\":67110,\"start\":67109},{\"end\":67119,\"start\":67118},{\"end\":67127,\"start\":67126},{\"end\":67135,\"start\":67134},{\"end\":67381,\"start\":67380},{\"end\":67389,\"start\":67388},{\"end\":67395,\"start\":67394},{\"end\":67397,\"start\":67396},{\"end\":67604,\"start\":67603},{\"end\":67612,\"start\":67611},{\"end\":67623,\"start\":67622},{\"end\":67632,\"start\":67631},{\"end\":67638,\"start\":67637},{\"end\":67640,\"start\":67639},{\"end\":67649,\"start\":67648},{\"end\":67909,\"start\":67908},{\"end\":67916,\"start\":67915},{\"end\":68076,\"start\":68075},{\"end\":68083,\"start\":68082},{\"end\":68092,\"start\":68091},{\"end\":68105,\"start\":68104},{\"end\":69168,\"start\":69166}]", "bib_author_last_name": "[{\"end\":45559,\"start\":45551},{\"end\":45571,\"start\":45563},{\"end\":45582,\"start\":45575},{\"end\":45591,\"start\":45586},{\"end\":45598,\"start\":45593},{\"end\":45885,\"start\":45877},{\"end\":45891,\"start\":45889},{\"end\":45902,\"start\":45895},{\"end\":45911,\"start\":45906},{\"end\":45922,\"start\":45915},{\"end\":45931,\"start\":45926},{\"end\":45940,\"start\":45935},{\"end\":46278,\"start\":46270},{\"end\":46287,\"start\":46282},{\"end\":46499,\"start\":46494},{\"end\":46507,\"start\":46503},{\"end\":46516,\"start\":46511},{\"end\":46527,\"start\":46520},{\"end\":46537,\"start\":46531},{\"end\":46549,\"start\":46541},{\"end\":46564,\"start\":46553},{\"end\":46573,\"start\":46568},{\"end\":46583,\"start\":46577},{\"end\":46593,\"start\":46587},{\"end\":46604,\"start\":46597},{\"end\":46620,\"start\":46608},{\"end\":46631,\"start\":46624},{\"end\":46643,\"start\":46635},{\"end\":46652,\"start\":46647},{\"end\":46662,\"start\":46656},{\"end\":46675,\"start\":46668},{\"end\":46681,\"start\":46679},{\"end\":46691,\"start\":46685},{\"end\":46700,\"start\":46695},{\"end\":46708,\"start\":46704},{\"end\":46718,\"start\":46712},{\"end\":46728,\"start\":46722},{\"end\":46736,\"start\":46732},{\"end\":46745,\"start\":46740},{\"end\":46754,\"start\":46749},{\"end\":46764,\"start\":46758},{\"end\":46778,\"start\":46768},{\"end\":46789,\"start\":46782},{\"end\":46802,\"start\":46793},{\"end\":47393,\"start\":47385},{\"end\":47407,\"start\":47397},{\"end\":47417,\"start\":47411},{\"end\":47429,\"start\":47421},{\"end\":47750,\"start\":47746},{\"end\":47758,\"start\":47754},{\"end\":47768,\"start\":47765},{\"end\":47780,\"start\":47772},{\"end\":47789,\"start\":47784},{\"end\":47799,\"start\":47793},{\"end\":47812,\"start\":47805},{\"end\":48074,\"start\":48070},{\"end\":48080,\"start\":48078},{\"end\":48086,\"start\":48084},{\"end\":48097,\"start\":48092},{\"end\":48106,\"start\":48101},{\"end\":48113,\"start\":48110},{\"end\":48122,\"start\":48117},{\"end\":48129,\"start\":48126},{\"end\":48414,\"start\":48411},{\"end\":48420,\"start\":48418},{\"end\":48431,\"start\":48424},{\"end\":48445,\"start\":48435},{\"end\":48457,\"start\":48449},{\"end\":48781,\"start\":48776},{\"end\":48793,\"start\":48788},{\"end\":48801,\"start\":48799},{\"end\":48814,\"start\":48807},{\"end\":49057,\"start\":49051},{\"end\":49069,\"start\":49064},{\"end\":49076,\"start\":49073},{\"end\":49089,\"start\":49080},{\"end\":49095,\"start\":49091},{\"end\":49365,\"start\":49358},{\"end\":49374,\"start\":49369},{\"end\":49390,\"start\":49378},{\"end\":49400,\"start\":49394},{\"end\":49628,\"start\":49625},{\"end\":49637,\"start\":49632},{\"end\":49897,\"start\":49892},{\"end\":49905,\"start\":49901},{\"end\":49916,\"start\":49909},{\"end\":49932,\"start\":49920},{\"end\":49941,\"start\":49936},{\"end\":49951,\"start\":49945},{\"end\":50391,\"start\":50383},{\"end\":50399,\"start\":50395},{\"end\":50409,\"start\":50403},{\"end\":50422,\"start\":50413},{\"end\":50434,\"start\":50426},{\"end\":50442,\"start\":50438},{\"end\":50453,\"start\":50446},{\"end\":50466,\"start\":50457},{\"end\":50479,\"start\":50470},{\"end\":50489,\"start\":50483},{\"end\":50500,\"start\":50493},{\"end\":50768,\"start\":50766},{\"end\":50780,\"start\":50772},{\"end\":50790,\"start\":50784},{\"end\":50802,\"start\":50794},{\"end\":50814,\"start\":50804},{\"end\":50820,\"start\":50816},{\"end\":50950,\"start\":50945},{\"end\":50958,\"start\":50954},{\"end\":50966,\"start\":50962},{\"end\":50975,\"start\":50972},{\"end\":51264,\"start\":51259},{\"end\":51272,\"start\":51268},{\"end\":51279,\"start\":51276},{\"end\":51285,\"start\":51283},{\"end\":51291,\"start\":51289},{\"end\":51566,\"start\":51560},{\"end\":51579,\"start\":51572},{\"end\":51901,\"start\":51893},{\"end\":51912,\"start\":51905},{\"end\":52223,\"start\":52217},{\"end\":52233,\"start\":52227},{\"end\":52242,\"start\":52237},{\"end\":52252,\"start\":52246},{\"end\":52502,\"start\":52494},{\"end\":52509,\"start\":52506},{\"end\":52517,\"start\":52513},{\"end\":52530,\"start\":52521},{\"end\":52541,\"start\":52534},{\"end\":52550,\"start\":52545},{\"end\":52564,\"start\":52554},{\"end\":52804,\"start\":52801},{\"end\":52811,\"start\":52808},{\"end\":52820,\"start\":52815},{\"end\":53068,\"start\":53061},{\"end\":53075,\"start\":53072},{\"end\":53084,\"start\":53079},{\"end\":53095,\"start\":53088},{\"end\":53103,\"start\":53099},{\"end\":53114,\"start\":53107},{\"end\":53122,\"start\":53118},{\"end\":53136,\"start\":53126},{\"end\":53146,\"start\":53140},{\"end\":53158,\"start\":53152},{\"end\":53177,\"start\":53168},{\"end\":53188,\"start\":53181},{\"end\":53646,\"start\":53642},{\"end\":53657,\"start\":53650},{\"end\":53667,\"start\":53661},{\"end\":53677,\"start\":53671},{\"end\":53688,\"start\":53681},{\"end\":54041,\"start\":54038},{\"end\":54047,\"start\":54045},{\"end\":54057,\"start\":54051},{\"end\":54067,\"start\":54063},{\"end\":54313,\"start\":54308},{\"end\":54320,\"start\":54317},{\"end\":54329,\"start\":54324},{\"end\":54346,\"start\":54333},{\"end\":54357,\"start\":54350},{\"end\":54365,\"start\":54361},{\"end\":54377,\"start\":54369},{\"end\":54392,\"start\":54381},{\"end\":54403,\"start\":54399},{\"end\":54409,\"start\":54405},{\"end\":54752,\"start\":54750},{\"end\":54763,\"start\":54759},{\"end\":54773,\"start\":54768},{\"end\":54781,\"start\":54779},{\"end\":54788,\"start\":54785},{\"end\":55027,\"start\":55025},{\"end\":55034,\"start\":55031},{\"end\":55040,\"start\":55038},{\"end\":55049,\"start\":55044},{\"end\":55055,\"start\":55053},{\"end\":55064,\"start\":55059},{\"end\":55072,\"start\":55068},{\"end\":55078,\"start\":55076},{\"end\":55086,\"start\":55082},{\"end\":55093,\"start\":55090},{\"end\":55101,\"start\":55097},{\"end\":55108,\"start\":55105},{\"end\":55115,\"start\":55110},{\"end\":55437,\"start\":55435},{\"end\":55446,\"start\":55441},{\"end\":55574,\"start\":55571},{\"end\":55583,\"start\":55578},{\"end\":55595,\"start\":55587},{\"end\":55603,\"start\":55599},{\"end\":55613,\"start\":55607},{\"end\":55624,\"start\":55617},{\"end\":55634,\"start\":55628},{\"end\":55647,\"start\":55640},{\"end\":55663,\"start\":55654},{\"end\":55927,\"start\":55924},{\"end\":55934,\"start\":55931},{\"end\":55943,\"start\":55938},{\"end\":55949,\"start\":55947},{\"end\":55958,\"start\":55953},{\"end\":55966,\"start\":55962},{\"end\":55974,\"start\":55970},{\"end\":55983,\"start\":55978},{\"end\":55998,\"start\":55987},{\"end\":56010,\"start\":56002},{\"end\":56019,\"start\":56012},{\"end\":56360,\"start\":56350},{\"end\":56370,\"start\":56364},{\"end\":56607,\"start\":56605},{\"end\":56616,\"start\":56611},{\"end\":56626,\"start\":56620},{\"end\":56633,\"start\":56630},{\"end\":56642,\"start\":56635},{\"end\":56921,\"start\":56919},{\"end\":56932,\"start\":56925},{\"end\":56944,\"start\":56936},{\"end\":56954,\"start\":56948},{\"end\":56961,\"start\":56958},{\"end\":57219,\"start\":57216},{\"end\":57228,\"start\":57223},{\"end\":57238,\"start\":57232},{\"end\":57249,\"start\":57242},{\"end\":57259,\"start\":57253},{\"end\":57518,\"start\":57512},{\"end\":57530,\"start\":57524},{\"end\":57539,\"start\":57534},{\"end\":57549,\"start\":57543},{\"end\":57776,\"start\":57771},{\"end\":57790,\"start\":57783},{\"end\":57800,\"start\":57794},{\"end\":57810,\"start\":57804},{\"end\":57819,\"start\":57814},{\"end\":57832,\"start\":57823},{\"end\":58055,\"start\":58049},{\"end\":58065,\"start\":58059},{\"end\":58074,\"start\":58069},{\"end\":58090,\"start\":58078},{\"end\":58099,\"start\":58094},{\"end\":58109,\"start\":58103},{\"end\":58121,\"start\":58113},{\"end\":58132,\"start\":58125},{\"end\":58144,\"start\":58136},{\"end\":58157,\"start\":58148},{\"end\":58163,\"start\":58161},{\"end\":58476,\"start\":58468},{\"end\":58485,\"start\":58480},{\"end\":58492,\"start\":58489},{\"end\":58501,\"start\":58498},{\"end\":58512,\"start\":58505},{\"end\":58790,\"start\":58782},{\"end\":58800,\"start\":58794},{\"end\":58808,\"start\":58804},{\"end\":58817,\"start\":58814},{\"end\":59181,\"start\":59175},{\"end\":59190,\"start\":59185},{\"end\":59202,\"start\":59194},{\"end\":59211,\"start\":59206},{\"end\":59219,\"start\":59215},{\"end\":59229,\"start\":59223},{\"end\":59236,\"start\":59233},{\"end\":59249,\"start\":59240},{\"end\":59259,\"start\":59253},{\"end\":59268,\"start\":59263},{\"end\":59565,\"start\":59561},{\"end\":59727,\"start\":59722},{\"end\":59735,\"start\":59731},{\"end\":59917,\"start\":59910},{\"end\":59925,\"start\":59921},{\"end\":59934,\"start\":59929},{\"end\":59944,\"start\":59938},{\"end\":59955,\"start\":59948},{\"end\":59967,\"start\":59959},{\"end\":59977,\"start\":59971},{\"end\":59987,\"start\":59981},{\"end\":59998,\"start\":59991},{\"end\":60007,\"start\":60002},{\"end\":60018,\"start\":60011},{\"end\":60031,\"start\":60022},{\"end\":60356,\"start\":60350},{\"end\":60367,\"start\":60360},{\"end\":60378,\"start\":60371},{\"end\":60385,\"start\":60382},{\"end\":60395,\"start\":60389},{\"end\":60405,\"start\":60399},{\"end\":60413,\"start\":60409},{\"end\":60419,\"start\":60417},{\"end\":60428,\"start\":60425},{\"end\":60758,\"start\":60749},{\"end\":60767,\"start\":60762},{\"end\":60778,\"start\":60771},{\"end\":60787,\"start\":60782},{\"end\":60998,\"start\":60995},{\"end\":61004,\"start\":61002},{\"end\":61016,\"start\":61008},{\"end\":61023,\"start\":61020},{\"end\":61037,\"start\":61025},{\"end\":61287,\"start\":61283},{\"end\":61300,\"start\":61291},{\"end\":61311,\"start\":61304},{\"end\":61518,\"start\":61514},{\"end\":61529,\"start\":61522},{\"end\":61539,\"start\":61537},{\"end\":61550,\"start\":61543},{\"end\":61559,\"start\":61554},{\"end\":61571,\"start\":61561},{\"end\":61830,\"start\":61826},{\"end\":61837,\"start\":61834},{\"end\":61844,\"start\":61841},{\"end\":61850,\"start\":61848},{\"end\":61860,\"start\":61857},{\"end\":62122,\"start\":62118},{\"end\":62130,\"start\":62126},{\"end\":62139,\"start\":62134},{\"end\":62148,\"start\":62143},{\"end\":62155,\"start\":62152},{\"end\":62444,\"start\":62441},{\"end\":62455,\"start\":62448},{\"end\":62465,\"start\":62459},{\"end\":62475,\"start\":62469},{\"end\":62730,\"start\":62727},{\"end\":62739,\"start\":62734},{\"end\":62751,\"start\":62743},{\"end\":62761,\"start\":62755},{\"end\":62771,\"start\":62765},{\"end\":63040,\"start\":63037},{\"end\":63050,\"start\":63044},{\"end\":63058,\"start\":63052},{\"end\":63257,\"start\":63250},{\"end\":63268,\"start\":63261},{\"end\":63278,\"start\":63272},{\"end\":63291,\"start\":63282},{\"end\":63300,\"start\":63295},{\"end\":63311,\"start\":63306},{\"end\":63321,\"start\":63315},{\"end\":63335,\"start\":63325},{\"end\":63630,\"start\":63622},{\"end\":63643,\"start\":63636},{\"end\":63653,\"start\":63647},{\"end\":63909,\"start\":63905},{\"end\":63918,\"start\":63913},{\"end\":63929,\"start\":63922},{\"end\":63937,\"start\":63933},{\"end\":63945,\"start\":63941},{\"end\":63957,\"start\":63951},{\"end\":64234,\"start\":64226},{\"end\":64244,\"start\":64238},{\"end\":64256,\"start\":64250},{\"end\":64479,\"start\":64475},{\"end\":64488,\"start\":64483},{\"end\":64496,\"start\":64492},{\"end\":64508,\"start\":64500},{\"end\":64520,\"start\":64512},{\"end\":64527,\"start\":64524},{\"end\":64537,\"start\":64531},{\"end\":64546,\"start\":64541},{\"end\":64554,\"start\":64550},{\"end\":64567,\"start\":64558},{\"end\":64575,\"start\":64571},{\"end\":64895,\"start\":64892},{\"end\":64904,\"start\":64899},{\"end\":64912,\"start\":64908},{\"end\":64921,\"start\":64916},{\"end\":65177,\"start\":65175},{\"end\":65184,\"start\":65181},{\"end\":65191,\"start\":65188},{\"end\":65198,\"start\":65195},{\"end\":65416,\"start\":65412},{\"end\":65423,\"start\":65420},{\"end\":65431,\"start\":65427},{\"end\":65444,\"start\":65435},{\"end\":65463,\"start\":65450},{\"end\":65471,\"start\":65469},{\"end\":65721,\"start\":65718},{\"end\":65728,\"start\":65725},{\"end\":65991,\"start\":65989},{\"end\":65998,\"start\":65995},{\"end\":66006,\"start\":66002},{\"end\":66014,\"start\":66010},{\"end\":66020,\"start\":66018},{\"end\":66030,\"start\":66024},{\"end\":66040,\"start\":66036},{\"end\":66330,\"start\":66328},{\"end\":66337,\"start\":66334},{\"end\":66560,\"start\":66553},{\"end\":66568,\"start\":66564},{\"end\":66580,\"start\":66572},{\"end\":66588,\"start\":66584},{\"end\":66815,\"start\":66808},{\"end\":66823,\"start\":66819},{\"end\":66834,\"start\":66827},{\"end\":66842,\"start\":66838},{\"end\":67087,\"start\":67082},{\"end\":67093,\"start\":67091},{\"end\":67099,\"start\":67097},{\"end\":67107,\"start\":67103},{\"end\":67116,\"start\":67111},{\"end\":67124,\"start\":67120},{\"end\":67132,\"start\":67128},{\"end\":67139,\"start\":67136},{\"end\":67146,\"start\":67141},{\"end\":67386,\"start\":67382},{\"end\":67392,\"start\":67390},{\"end\":67403,\"start\":67398},{\"end\":67609,\"start\":67605},{\"end\":67620,\"start\":67613},{\"end\":67629,\"start\":67624},{\"end\":67635,\"start\":67633},{\"end\":67646,\"start\":67641},{\"end\":67653,\"start\":67650},{\"end\":67913,\"start\":67910},{\"end\":67921,\"start\":67917},{\"end\":68080,\"start\":68077},{\"end\":68089,\"start\":68084},{\"end\":68102,\"start\":68093},{\"end\":68113,\"start\":68106}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":45788,\"start\":45444},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3753452},\"end\":46171,\"start\":45790},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7164502},\"end\":46449,\"start\":46173},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":47310,\"start\":46451},{\"attributes\":{\"doi\":\"10.18653/v1/n19-1422\",\"id\":\"b4\",\"matched_paper_id\":84842989},\"end\":47678,\"start\":47312},{\"attributes\":{\"id\":\"b5\"},\"end\":48012,\"start\":47680},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":216080982},\"end\":48330,\"start\":48014},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.707\",\"id\":\"b7\",\"matched_paper_id\":219964325},\"end\":48694,\"start\":48332},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":213152193},\"end\":48971,\"start\":48696},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52967399},\"end\":49295,\"start\":48973},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8622019},\"end\":49560,\"start\":49297},{\"attributes\":{\"id\":\"b11\"},\"end\":49788,\"start\":49562},{\"attributes\":{\"doi\":\"15731405. doi: 10.1007/ s11263-018-1116-0\",\"id\":\"b12\",\"matched_paper_id\":8081284},\"end\":50313,\"start\":49790},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52143791},\"end\":50760,\"start\":50315},{\"attributes\":{\"id\":\"b14\"},\"end\":50896,\"start\":50762},{\"attributes\":{\"doi\":\"10.1109/ICCV.2019.00473\",\"id\":\"b15\",\"matched_paper_id\":201070367},\"end\":51177,\"start\":50898},{\"attributes\":{\"id\":\"b16\"},\"end\":51465,\"start\":51179},{\"attributes\":{\"doi\":\"10.1109/CVPR.2019.00686\",\"id\":\"b17\",\"matched_paper_id\":152282269},\"end\":51822,\"start\":51467},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2016.2598339\",\"id\":\"b18\",\"matched_paper_id\":8517067},\"end\":52138,\"start\":51824},{\"attributes\":{\"id\":\"b19\"},\"end\":52425,\"start\":52140},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":218487109},\"end\":52768,\"start\":52427},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":29150617},\"end\":52925,\"start\":52770},{\"attributes\":{\"doi\":\"10.1007/s11263-016-0981-7\",\"id\":\"b22\"},\"end\":53560,\"start\":52927},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":202888986},\"end\":53977,\"start\":53562},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":52171684},\"end\":54196,\"start\":53979},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":204960716},\"end\":54667,\"start\":54198},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":218470055},\"end\":54956,\"start\":54669},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":215754208},\"end\":55368,\"start\":54958},{\"attributes\":{\"id\":\"b28\"},\"end\":55538,\"start\":55370},{\"attributes\":{\"doi\":\"10.1007/978-3-319-10602-148\",\"id\":\"b29\"},\"end\":55920,\"start\":55540},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b30\"},\"end\":56307,\"start\":55922},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":53592270},\"end\":56512,\"start\":56309},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":199453025},\"end\":56850,\"start\":56514},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":208637516},\"end\":57147,\"start\":56852},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8745888},\"end\":57433,\"start\":57149},{\"attributes\":{\"id\":\"b35\"},\"end\":57684,\"start\":57435},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":209370497},\"end\":58019,\"start\":57686},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3297437},\"end\":58402,\"start\":58021},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":212725651},\"end\":58714,\"start\":58404},{\"attributes\":{\"doi\":\"10.3115/1073083.1073135\",\"id\":\"b39\",\"matched_paper_id\":11080756},\"end\":59133,\"start\":58716},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":40027675},\"end\":59512,\"start\":59135},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":13751870},\"end\":59663,\"start\":59514},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":836219},\"end\":59835,\"start\":59665},{\"attributes\":{\"id\":\"b43\"},\"end\":60263,\"start\":59837},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":204838007},\"end\":60684,\"start\":60265},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":11816014},\"end\":60925,\"start\":60686},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":10328909},\"end\":61224,\"start\":60927},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3725815},\"end\":61431,\"start\":61226},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":226222232},\"end\":61750,\"start\":61433},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":146808476},\"end\":62043,\"start\":61752},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":53178856},\"end\":62361,\"start\":62045},{\"attributes\":{\"id\":\"b51\"},\"end\":62650,\"start\":62363},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":102483628},\"end\":62966,\"start\":62652},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":201103729},\"end\":63219,\"start\":62968},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":13756489},\"end\":63565,\"start\":63221},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":9026666},\"end\":63814,\"start\":63567},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":5034059},\"end\":64143,\"start\":63816},{\"attributes\":{\"id\":\"b57\"},\"end\":64398,\"start\":64145},{\"attributes\":{\"id\":\"b58\"},\"end\":64823,\"start\":64400},{\"attributes\":{\"id\":\"b59\"},\"end\":65095,\"start\":64825},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":206594535},\"end\":65334,\"start\":65097},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":195069387},\"end\":65655,\"start\":65336},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.400\",\"id\":\"b62\",\"matched_paper_id\":220045418},\"end\":65909,\"start\":65657},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":3441497},\"end\":66250,\"start\":65911},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":51939396},\"end\":66473,\"start\":66252},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":52019251},\"end\":66743,\"start\":66475},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":53734356},\"end\":67014,\"start\":66745},{\"attributes\":{\"id\":\"b67\"},\"end\":67306,\"start\":67016},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":19713015},\"end\":67534,\"start\":67308},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":202734445},\"end\":67848,\"start\":67536},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":219617394},\"end\":68024,\"start\":67850},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.540\",\"id\":\"b71\",\"matched_paper_id\":5714907},\"end\":68340,\"start\":68026},{\"attributes\":{\"id\":\"b72\"},\"end\":68667,\"start\":68342},{\"attributes\":{\"id\":\"b73\"},\"end\":68784,\"start\":68669},{\"attributes\":{\"id\":\"b74\"},\"end\":68989,\"start\":68786},{\"attributes\":{\"id\":\"b75\"},\"end\":69646,\"start\":68991}]", "bib_title": "[{\"end\":45547,\"start\":45444},{\"end\":45873,\"start\":45790},{\"end\":46266,\"start\":46173},{\"end\":46488,\"start\":46451},{\"end\":47381,\"start\":47312},{\"end\":48066,\"start\":48014},{\"end\":48407,\"start\":48332},{\"end\":48772,\"start\":48696},{\"end\":49047,\"start\":48973},{\"end\":49354,\"start\":49297},{\"end\":49888,\"start\":49790},{\"end\":50376,\"start\":50315},{\"end\":50941,\"start\":50898},{\"end\":51554,\"start\":51467},{\"end\":51889,\"start\":51824},{\"end\":52490,\"start\":52427},{\"end\":52797,\"start\":52770},{\"end\":53638,\"start\":53562},{\"end\":54034,\"start\":53979},{\"end\":54304,\"start\":54198},{\"end\":54746,\"start\":54669},{\"end\":55021,\"start\":54958},{\"end\":55565,\"start\":55540},{\"end\":56346,\"start\":56309},{\"end\":56601,\"start\":56514},{\"end\":56915,\"start\":56852},{\"end\":57212,\"start\":57149},{\"end\":57767,\"start\":57686},{\"end\":58045,\"start\":58021},{\"end\":58464,\"start\":58404},{\"end\":58778,\"start\":58716},{\"end\":59171,\"start\":59135},{\"end\":59557,\"start\":59514},{\"end\":59718,\"start\":59665},{\"end\":60346,\"start\":60265},{\"end\":60745,\"start\":60686},{\"end\":60991,\"start\":60927},{\"end\":61279,\"start\":61226},{\"end\":61510,\"start\":61433},{\"end\":61822,\"start\":61752},{\"end\":62114,\"start\":62045},{\"end\":62723,\"start\":62652},{\"end\":63033,\"start\":62968},{\"end\":63246,\"start\":63221},{\"end\":63618,\"start\":63567},{\"end\":63901,\"start\":63816},{\"end\":65171,\"start\":65097},{\"end\":65408,\"start\":65336},{\"end\":65714,\"start\":65657},{\"end\":65985,\"start\":65911},{\"end\":66324,\"start\":66252},{\"end\":66549,\"start\":66475},{\"end\":66804,\"start\":66745},{\"end\":67378,\"start\":67308},{\"end\":67601,\"start\":67536},{\"end\":67906,\"start\":67850},{\"end\":68073,\"start\":68026},{\"end\":69164,\"start\":68991}]", "bib_author": "[{\"end\":45561,\"start\":45549},{\"end\":45573,\"start\":45561},{\"end\":45584,\"start\":45573},{\"end\":45593,\"start\":45584},{\"end\":45600,\"start\":45593},{\"end\":45887,\"start\":45875},{\"end\":45893,\"start\":45887},{\"end\":45904,\"start\":45893},{\"end\":45913,\"start\":45904},{\"end\":45924,\"start\":45913},{\"end\":45933,\"start\":45924},{\"end\":45942,\"start\":45933},{\"end\":46280,\"start\":46268},{\"end\":46289,\"start\":46280},{\"end\":46501,\"start\":46490},{\"end\":46509,\"start\":46501},{\"end\":46518,\"start\":46509},{\"end\":46529,\"start\":46518},{\"end\":46539,\"start\":46529},{\"end\":46551,\"start\":46539},{\"end\":46566,\"start\":46551},{\"end\":46575,\"start\":46566},{\"end\":46585,\"start\":46575},{\"end\":46595,\"start\":46585},{\"end\":46606,\"start\":46595},{\"end\":46622,\"start\":46606},{\"end\":46633,\"start\":46622},{\"end\":46645,\"start\":46633},{\"end\":46654,\"start\":46645},{\"end\":46664,\"start\":46654},{\"end\":46677,\"start\":46664},{\"end\":46683,\"start\":46677},{\"end\":46693,\"start\":46683},{\"end\":46702,\"start\":46693},{\"end\":46710,\"start\":46702},{\"end\":46720,\"start\":46710},{\"end\":46730,\"start\":46720},{\"end\":46738,\"start\":46730},{\"end\":46747,\"start\":46738},{\"end\":46756,\"start\":46747},{\"end\":46766,\"start\":46756},{\"end\":46780,\"start\":46766},{\"end\":46791,\"start\":46780},{\"end\":46804,\"start\":46791},{\"end\":46813,\"start\":46804},{\"end\":46817,\"start\":46813},{\"end\":47395,\"start\":47383},{\"end\":47409,\"start\":47395},{\"end\":47419,\"start\":47409},{\"end\":47431,\"start\":47419},{\"end\":47752,\"start\":47744},{\"end\":47760,\"start\":47752},{\"end\":47770,\"start\":47760},{\"end\":47782,\"start\":47770},{\"end\":47791,\"start\":47782},{\"end\":47801,\"start\":47791},{\"end\":47814,\"start\":47801},{\"end\":48076,\"start\":48068},{\"end\":48082,\"start\":48076},{\"end\":48088,\"start\":48082},{\"end\":48099,\"start\":48088},{\"end\":48108,\"start\":48099},{\"end\":48115,\"start\":48108},{\"end\":48124,\"start\":48115},{\"end\":48131,\"start\":48124},{\"end\":48416,\"start\":48409},{\"end\":48422,\"start\":48416},{\"end\":48433,\"start\":48422},{\"end\":48447,\"start\":48433},{\"end\":48459,\"start\":48447},{\"end\":48783,\"start\":48774},{\"end\":48795,\"start\":48783},{\"end\":48803,\"start\":48795},{\"end\":48816,\"start\":48803},{\"end\":49059,\"start\":49049},{\"end\":49071,\"start\":49059},{\"end\":49078,\"start\":49071},{\"end\":49091,\"start\":49078},{\"end\":49097,\"start\":49091},{\"end\":49367,\"start\":49356},{\"end\":49376,\"start\":49367},{\"end\":49392,\"start\":49376},{\"end\":49402,\"start\":49392},{\"end\":49630,\"start\":49623},{\"end\":49639,\"start\":49630},{\"end\":49646,\"start\":49639},{\"end\":49650,\"start\":49646},{\"end\":49899,\"start\":49890},{\"end\":49907,\"start\":49899},{\"end\":49918,\"start\":49907},{\"end\":49934,\"start\":49918},{\"end\":49943,\"start\":49934},{\"end\":49953,\"start\":49943},{\"end\":50393,\"start\":50378},{\"end\":50401,\"start\":50393},{\"end\":50411,\"start\":50401},{\"end\":50424,\"start\":50411},{\"end\":50436,\"start\":50424},{\"end\":50444,\"start\":50436},{\"end\":50455,\"start\":50444},{\"end\":50468,\"start\":50455},{\"end\":50481,\"start\":50468},{\"end\":50491,\"start\":50481},{\"end\":50502,\"start\":50491},{\"end\":50770,\"start\":50764},{\"end\":50782,\"start\":50770},{\"end\":50792,\"start\":50782},{\"end\":50804,\"start\":50792},{\"end\":50816,\"start\":50804},{\"end\":50822,\"start\":50816},{\"end\":50952,\"start\":50943},{\"end\":50960,\"start\":50952},{\"end\":50968,\"start\":50960},{\"end\":50977,\"start\":50968},{\"end\":51266,\"start\":51257},{\"end\":51274,\"start\":51266},{\"end\":51281,\"start\":51274},{\"end\":51287,\"start\":51281},{\"end\":51293,\"start\":51287},{\"end\":51568,\"start\":51556},{\"end\":51581,\"start\":51568},{\"end\":51903,\"start\":51891},{\"end\":51914,\"start\":51903},{\"end\":52225,\"start\":52213},{\"end\":52235,\"start\":52225},{\"end\":52244,\"start\":52235},{\"end\":52254,\"start\":52244},{\"end\":52504,\"start\":52492},{\"end\":52511,\"start\":52504},{\"end\":52519,\"start\":52511},{\"end\":52532,\"start\":52519},{\"end\":52543,\"start\":52532},{\"end\":52552,\"start\":52543},{\"end\":52566,\"start\":52552},{\"end\":52806,\"start\":52799},{\"end\":52813,\"start\":52806},{\"end\":52822,\"start\":52813},{\"end\":53070,\"start\":53059},{\"end\":53077,\"start\":53070},{\"end\":53086,\"start\":53077},{\"end\":53097,\"start\":53086},{\"end\":53105,\"start\":53097},{\"end\":53116,\"start\":53105},{\"end\":53124,\"start\":53116},{\"end\":53138,\"start\":53124},{\"end\":53148,\"start\":53138},{\"end\":53160,\"start\":53148},{\"end\":53179,\"start\":53160},{\"end\":53190,\"start\":53179},{\"end\":53648,\"start\":53640},{\"end\":53659,\"start\":53648},{\"end\":53669,\"start\":53659},{\"end\":53679,\"start\":53669},{\"end\":53690,\"start\":53679},{\"end\":53694,\"start\":53690},{\"end\":54043,\"start\":54036},{\"end\":54049,\"start\":54043},{\"end\":54059,\"start\":54049},{\"end\":54069,\"start\":54059},{\"end\":54315,\"start\":54306},{\"end\":54322,\"start\":54315},{\"end\":54331,\"start\":54322},{\"end\":54348,\"start\":54331},{\"end\":54359,\"start\":54348},{\"end\":54367,\"start\":54359},{\"end\":54379,\"start\":54367},{\"end\":54394,\"start\":54379},{\"end\":54405,\"start\":54394},{\"end\":54411,\"start\":54405},{\"end\":54754,\"start\":54748},{\"end\":54765,\"start\":54754},{\"end\":54775,\"start\":54765},{\"end\":54783,\"start\":54775},{\"end\":54790,\"start\":54783},{\"end\":54794,\"start\":54790},{\"end\":55029,\"start\":55023},{\"end\":55036,\"start\":55029},{\"end\":55042,\"start\":55036},{\"end\":55051,\"start\":55042},{\"end\":55057,\"start\":55051},{\"end\":55066,\"start\":55057},{\"end\":55074,\"start\":55066},{\"end\":55080,\"start\":55074},{\"end\":55088,\"start\":55080},{\"end\":55095,\"start\":55088},{\"end\":55103,\"start\":55095},{\"end\":55110,\"start\":55103},{\"end\":55117,\"start\":55110},{\"end\":55439,\"start\":55431},{\"end\":55448,\"start\":55439},{\"end\":55576,\"start\":55567},{\"end\":55585,\"start\":55576},{\"end\":55597,\"start\":55585},{\"end\":55605,\"start\":55597},{\"end\":55615,\"start\":55605},{\"end\":55626,\"start\":55615},{\"end\":55636,\"start\":55626},{\"end\":55649,\"start\":55636},{\"end\":55665,\"start\":55649},{\"end\":55929,\"start\":55922},{\"end\":55936,\"start\":55929},{\"end\":55945,\"start\":55936},{\"end\":55951,\"start\":55945},{\"end\":55960,\"start\":55951},{\"end\":55968,\"start\":55960},{\"end\":55976,\"start\":55968},{\"end\":55985,\"start\":55976},{\"end\":56000,\"start\":55985},{\"end\":56012,\"start\":56000},{\"end\":56021,\"start\":56012},{\"end\":56362,\"start\":56348},{\"end\":56372,\"start\":56362},{\"end\":56609,\"start\":56603},{\"end\":56618,\"start\":56609},{\"end\":56628,\"start\":56618},{\"end\":56635,\"start\":56628},{\"end\":56644,\"start\":56635},{\"end\":56923,\"start\":56917},{\"end\":56934,\"start\":56923},{\"end\":56946,\"start\":56934},{\"end\":56956,\"start\":56946},{\"end\":56963,\"start\":56956},{\"end\":57221,\"start\":57214},{\"end\":57230,\"start\":57221},{\"end\":57240,\"start\":57230},{\"end\":57251,\"start\":57240},{\"end\":57261,\"start\":57251},{\"end\":57270,\"start\":57261},{\"end\":57274,\"start\":57270},{\"end\":57520,\"start\":57510},{\"end\":57532,\"start\":57520},{\"end\":57541,\"start\":57532},{\"end\":57551,\"start\":57541},{\"end\":57778,\"start\":57769},{\"end\":57792,\"start\":57778},{\"end\":57802,\"start\":57792},{\"end\":57812,\"start\":57802},{\"end\":57821,\"start\":57812},{\"end\":57834,\"start\":57821},{\"end\":58057,\"start\":58047},{\"end\":58067,\"start\":58057},{\"end\":58076,\"start\":58067},{\"end\":58092,\"start\":58076},{\"end\":58101,\"start\":58092},{\"end\":58111,\"start\":58101},{\"end\":58123,\"start\":58111},{\"end\":58134,\"start\":58123},{\"end\":58146,\"start\":58134},{\"end\":58159,\"start\":58146},{\"end\":58165,\"start\":58159},{\"end\":58478,\"start\":58466},{\"end\":58487,\"start\":58478},{\"end\":58494,\"start\":58487},{\"end\":58503,\"start\":58494},{\"end\":58514,\"start\":58503},{\"end\":58520,\"start\":58514},{\"end\":58524,\"start\":58520},{\"end\":58792,\"start\":58780},{\"end\":58802,\"start\":58792},{\"end\":58810,\"start\":58802},{\"end\":58819,\"start\":58810},{\"end\":59183,\"start\":59173},{\"end\":59192,\"start\":59183},{\"end\":59204,\"start\":59192},{\"end\":59213,\"start\":59204},{\"end\":59221,\"start\":59213},{\"end\":59231,\"start\":59221},{\"end\":59238,\"start\":59231},{\"end\":59251,\"start\":59238},{\"end\":59261,\"start\":59251},{\"end\":59270,\"start\":59261},{\"end\":59567,\"start\":59559},{\"end\":59729,\"start\":59720},{\"end\":59737,\"start\":59729},{\"end\":59919,\"start\":59908},{\"end\":59927,\"start\":59919},{\"end\":59936,\"start\":59927},{\"end\":59946,\"start\":59936},{\"end\":59957,\"start\":59946},{\"end\":59969,\"start\":59957},{\"end\":59979,\"start\":59969},{\"end\":59989,\"start\":59979},{\"end\":60000,\"start\":59989},{\"end\":60009,\"start\":60000},{\"end\":60020,\"start\":60009},{\"end\":60033,\"start\":60020},{\"end\":60358,\"start\":60348},{\"end\":60369,\"start\":60358},{\"end\":60380,\"start\":60369},{\"end\":60387,\"start\":60380},{\"end\":60397,\"start\":60387},{\"end\":60407,\"start\":60397},{\"end\":60415,\"start\":60407},{\"end\":60421,\"start\":60415},{\"end\":60430,\"start\":60421},{\"end\":60760,\"start\":60747},{\"end\":60769,\"start\":60760},{\"end\":60780,\"start\":60769},{\"end\":60789,\"start\":60780},{\"end\":61000,\"start\":60993},{\"end\":61006,\"start\":61000},{\"end\":61018,\"start\":61006},{\"end\":61025,\"start\":61018},{\"end\":61039,\"start\":61025},{\"end\":61289,\"start\":61281},{\"end\":61302,\"start\":61289},{\"end\":61313,\"start\":61302},{\"end\":61520,\"start\":61512},{\"end\":61531,\"start\":61520},{\"end\":61541,\"start\":61531},{\"end\":61552,\"start\":61541},{\"end\":61561,\"start\":61552},{\"end\":61573,\"start\":61561},{\"end\":61832,\"start\":61824},{\"end\":61839,\"start\":61832},{\"end\":61846,\"start\":61839},{\"end\":61852,\"start\":61846},{\"end\":61862,\"start\":61852},{\"end\":62124,\"start\":62116},{\"end\":62132,\"start\":62124},{\"end\":62141,\"start\":62132},{\"end\":62150,\"start\":62141},{\"end\":62157,\"start\":62150},{\"end\":62165,\"start\":62157},{\"end\":62169,\"start\":62165},{\"end\":62446,\"start\":62439},{\"end\":62457,\"start\":62446},{\"end\":62467,\"start\":62457},{\"end\":62477,\"start\":62467},{\"end\":62732,\"start\":62725},{\"end\":62741,\"start\":62732},{\"end\":62753,\"start\":62741},{\"end\":62763,\"start\":62753},{\"end\":62773,\"start\":62763},{\"end\":63042,\"start\":63035},{\"end\":63052,\"start\":63042},{\"end\":63060,\"start\":63052},{\"end\":63259,\"start\":63248},{\"end\":63270,\"start\":63259},{\"end\":63280,\"start\":63270},{\"end\":63293,\"start\":63280},{\"end\":63302,\"start\":63293},{\"end\":63313,\"start\":63302},{\"end\":63323,\"start\":63313},{\"end\":63337,\"start\":63323},{\"end\":63632,\"start\":63620},{\"end\":63645,\"start\":63632},{\"end\":63655,\"start\":63645},{\"end\":63911,\"start\":63903},{\"end\":63920,\"start\":63911},{\"end\":63931,\"start\":63920},{\"end\":63939,\"start\":63931},{\"end\":63947,\"start\":63939},{\"end\":63959,\"start\":63947},{\"end\":64236,\"start\":64224},{\"end\":64246,\"start\":64236},{\"end\":64258,\"start\":64246},{\"end\":64481,\"start\":64473},{\"end\":64490,\"start\":64481},{\"end\":64498,\"start\":64490},{\"end\":64510,\"start\":64498},{\"end\":64522,\"start\":64510},{\"end\":64529,\"start\":64522},{\"end\":64539,\"start\":64529},{\"end\":64548,\"start\":64539},{\"end\":64556,\"start\":64548},{\"end\":64569,\"start\":64556},{\"end\":64577,\"start\":64569},{\"end\":64897,\"start\":64890},{\"end\":64906,\"start\":64897},{\"end\":64914,\"start\":64906},{\"end\":64923,\"start\":64914},{\"end\":64928,\"start\":64923},{\"end\":64932,\"start\":64928},{\"end\":65179,\"start\":65173},{\"end\":65186,\"start\":65179},{\"end\":65193,\"start\":65186},{\"end\":65200,\"start\":65193},{\"end\":65418,\"start\":65410},{\"end\":65425,\"start\":65418},{\"end\":65433,\"start\":65425},{\"end\":65446,\"start\":65433},{\"end\":65465,\"start\":65446},{\"end\":65473,\"start\":65465},{\"end\":65723,\"start\":65716},{\"end\":65730,\"start\":65723},{\"end\":65993,\"start\":65987},{\"end\":66000,\"start\":65993},{\"end\":66008,\"start\":66000},{\"end\":66016,\"start\":66008},{\"end\":66022,\"start\":66016},{\"end\":66032,\"start\":66022},{\"end\":66042,\"start\":66032},{\"end\":66332,\"start\":66326},{\"end\":66339,\"start\":66332},{\"end\":66345,\"start\":66339},{\"end\":66349,\"start\":66345},{\"end\":66562,\"start\":66551},{\"end\":66570,\"start\":66562},{\"end\":66582,\"start\":66570},{\"end\":66590,\"start\":66582},{\"end\":66817,\"start\":66806},{\"end\":66825,\"start\":66817},{\"end\":66836,\"start\":66825},{\"end\":66844,\"start\":66836},{\"end\":67089,\"start\":67080},{\"end\":67095,\"start\":67089},{\"end\":67101,\"start\":67095},{\"end\":67109,\"start\":67101},{\"end\":67118,\"start\":67109},{\"end\":67126,\"start\":67118},{\"end\":67134,\"start\":67126},{\"end\":67141,\"start\":67134},{\"end\":67148,\"start\":67141},{\"end\":67388,\"start\":67380},{\"end\":67394,\"start\":67388},{\"end\":67405,\"start\":67394},{\"end\":67611,\"start\":67603},{\"end\":67622,\"start\":67611},{\"end\":67631,\"start\":67622},{\"end\":67637,\"start\":67631},{\"end\":67648,\"start\":67637},{\"end\":67655,\"start\":67648},{\"end\":67915,\"start\":67908},{\"end\":67923,\"start\":67915},{\"end\":68082,\"start\":68075},{\"end\":68091,\"start\":68082},{\"end\":68104,\"start\":68091},{\"end\":68115,\"start\":68104},{\"end\":69171,\"start\":69166}]", "bib_venue": "[{\"end\":45604,\"start\":45600},{\"end\":45946,\"start\":45942},{\"end\":46301,\"start\":46289},{\"end\":46824,\"start\":46817},{\"end\":47482,\"start\":47451},{\"end\":47742,\"start\":47680},{\"end\":48135,\"start\":48131},{\"end\":48495,\"start\":48490},{\"end\":48820,\"start\":48816},{\"end\":49102,\"start\":49097},{\"end\":49414,\"start\":49402},{\"end\":49621,\"start\":49562},{\"end\":50034,\"start\":49994},{\"end\":50505,\"start\":50502},{\"end\":51004,\"start\":51000},{\"end\":51255,\"start\":51179},{\"end\":51634,\"start\":51604},{\"end\":51970,\"start\":51940},{\"end\":52211,\"start\":52140},{\"end\":52583,\"start\":52566},{\"end\":52829,\"start\":52822},{\"end\":53057,\"start\":52927},{\"end\":53698,\"start\":53694},{\"end\":54077,\"start\":54069},{\"end\":54414,\"start\":54411},{\"end\":54799,\"start\":54794},{\"end\":55128,\"start\":55117},{\"end\":55429,\"start\":55370},{\"end\":55696,\"start\":55692},{\"end\":56083,\"start\":56037},{\"end\":56376,\"start\":56372},{\"end\":56651,\"start\":56644},{\"end\":56967,\"start\":56963},{\"end\":57278,\"start\":57274},{\"end\":57508,\"start\":57435},{\"end\":57838,\"start\":57834},{\"end\":58169,\"start\":58165},{\"end\":58541,\"start\":58524},{\"end\":58845,\"start\":58842},{\"end\":59283,\"start\":59270},{\"end\":59570,\"start\":59567},{\"end\":59741,\"start\":59737},{\"end\":59906,\"start\":59837},{\"end\":60434,\"start\":60430},{\"end\":60794,\"start\":60789},{\"end\":61043,\"start\":61039},{\"end\":61318,\"start\":61313},{\"end\":61578,\"start\":61573},{\"end\":61866,\"start\":61862},{\"end\":62172,\"start\":62169},{\"end\":62437,\"start\":62363},{\"end\":62777,\"start\":62773},{\"end\":63065,\"start\":63060},{\"end\":63341,\"start\":63337},{\"end\":63659,\"start\":63655},{\"end\":63966,\"start\":63959},{\"end\":64222,\"start\":64145},{\"end\":64471,\"start\":64400},{\"end\":64888,\"start\":64825},{\"end\":65204,\"start\":65200},{\"end\":65480,\"start\":65473},{\"end\":65762,\"start\":65759},{\"end\":66046,\"start\":66042},{\"end\":66353,\"start\":66349},{\"end\":66598,\"start\":66590},{\"end\":66848,\"start\":66844},{\"end\":67078,\"start\":67016},{\"end\":67409,\"start\":67405},{\"end\":67659,\"start\":67655},{\"end\":67927,\"start\":67923},{\"end\":68140,\"start\":68136},{\"end\":68503,\"start\":68342},{\"end\":68716,\"start\":68669},{\"end\":68886,\"start\":68786},{\"end\":69309,\"start\":69171}]"}}}, "year": 2023, "month": 12, "day": 17}