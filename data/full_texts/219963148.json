{"id": 219963148, "updated": "2023-04-05 02:39:00.034", "metadata": {"title": "Residual Feature Aggregation Network for Image Super-Resolution", "authors": "[{\"first\":\"Jie\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Wenjie\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yuting\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Gangshan\",\"last\":\"Wu\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": 6, "day": 1}, "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown great power in single image super-resolution (SISR) and achieved significant improvements against traditional methods. Among these CNN-based methods, the residual connections play a critical role in boosting the network performance. As the network depth grows, the residual features gradually focused on different aspects of the input image, which is very useful for reconstructing the spatial details. However, existing methods neglect to fully utilize the hierarchical features on the residual branches. To address this issue, we propose a novel residual feature aggregation (RFA) framework for more efficient feature extraction. The RFA framework groups several residual modules together and directly forwards the features on each local residual branch by adding skip connections. Therefore, the RFA framework is capable of aggregating these informative residual features to produce more representative features. To maximize the power of the RFA framework, we further propose an enhanced spatial attention (ESA) block to make the residual features to be more focused on critical spatial contents. The ESA block is designed to be lightweight and efficient. Our final RFANet is constructed by applying the proposed RFA framework with the ESA blocks. Comprehensive experiments demonstrate the necessity of our RFA framework and the superiority of our RFANet over state-of-the-art SISR methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3034247386", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Liu0T0W20", "doi": "10.1109/cvpr42600.2020.00243"}}, "content": {"source": {"pdf_hash": "9939a5e81546c6961b7d9f94817f3eab36e08055", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ca10c2155257b577421bffe7b18fbe41a1b49688", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9939a5e81546c6961b7d9f94817f3eab36e08055.txt", "contents": "\nResidual Feature Aggregation Network for Image Super-Resolution (Supplementary Material)\n\n\nResidual Feature Aggregation Network for Image Super-Resolution (Supplementary Material)\n\nA. Study of the strided convolution in ESA block As shown inTable 1, the strided convolution has a higher PSNR than the palin convolution, which indicates that a large receptive field is essential for image SR.PSNR strided conv 32.65 plain conv 32.61\n\nB. Study of the window size for max-pooling in ESA block As shown in Table 2, we can achieve a higher PSNR by using a pooling window size of 7 \u00d7 7, further proving the critical importance of a large receptive field for image SR.\n\nPSNR 7 \u00d7 7 32.65 3 \u00d7 3 32.59 Table 2. Investigation of the effects of different window sizes for max-pooling in the ESA block with scale factor of \u00d74 on Set5.\n\n\nC. Running time comparison\n\nIn Table 3, we compare average forward time of our proposed RFANet with RCAN [2] and SAN [1] on Urban100 with scale factor \u00d74. The forward time of all the netowrks is evaluated on the same machine with 4.3GHz Intel i7 CPU (32G RAM) and an NVIDIA 1080Ti GPU using their official codes. Our RFANet runs the fastest while achieving the best PSNR which demonstrates the effectiveness of our method.\n\n\nModel\n\nForward Time (s) PSNR RCAN [2] 0  \n\nFigure 1 .\n1Visual comparisons for \u00d74 SR with BI degradation model.\n\nTable 1 .\n1Investigation of the effect of strided convolution in the ESA block with scale factor of \u00d74 on Set5.\n\nSecond-order attention network for single image super-resolution. Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, Lei Zhang, CVPR. Computer Vision Foundation / IEEETao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single image super-resolution. In CVPR, pages 11065-11074. Computer Vision Foundation / IEEE, 2019.\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, ECCV. Springer11211Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In ECCV (7), volume 11211 of Lecture Notes in Computer Science, pages 294-310. Springer, 2018.\n", "annotations": {"author": null, "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": null, "title": "[{\"end\":89,\"start\":1},{\"end\":180,\"start\":92}]", "venue": null, "abstract": "[{\"end\":432,\"start\":182}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":933,\"start\":930},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":945,\"start\":942},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1287,\"start\":1284}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":1360,\"start\":1292},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":1473,\"start\":1361}]", "paragraph": "[{\"end\":662,\"start\":434},{\"end\":822,\"start\":664},{\"end\":1247,\"start\":853},{\"end\":1291,\"start\":1257}]", "formula": null, "table_ref": "[{\"end\":510,\"start\":503},{\"end\":700,\"start\":693},{\"end\":863,\"start\":856}]", "section_header": "[{\"end\":851,\"start\":825},{\"end\":1255,\"start\":1250},{\"end\":1303,\"start\":1293},{\"end\":1371,\"start\":1362}]", "table": null, "figure_caption": "[{\"end\":1360,\"start\":1305},{\"end\":1473,\"start\":1373}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":1544,\"start\":1541},{\"end\":1557,\"start\":1550},{\"end\":1571,\"start\":1563},{\"end\":1586,\"start\":1579},{\"end\":1595,\"start\":1592},{\"end\":1925,\"start\":1920},{\"end\":1940,\"start\":1933},{\"end\":1948,\"start\":1945},{\"end\":1959,\"start\":1953},{\"end\":1972,\"start\":1966},{\"end\":1983,\"start\":1980}]", "bib_author_last_name": "[{\"end\":1548,\"start\":1545},{\"end\":1561,\"start\":1558},{\"end\":1577,\"start\":1572},{\"end\":1590,\"start\":1587},{\"end\":1601,\"start\":1596},{\"end\":1931,\"start\":1926},{\"end\":1943,\"start\":1941},{\"end\":1951,\"start\":1949},{\"end\":1964,\"start\":1960},{\"end\":1978,\"start\":1973},{\"end\":1986,\"start\":1984}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":174788791},\"end\":1842,\"start\":1475},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":49657846},\"end\":2249,\"start\":1844}]", "bib_title": "[{\"end\":1539,\"start\":1475},{\"end\":1918,\"start\":1844}]", "bib_author": "[{\"end\":1550,\"start\":1541},{\"end\":1563,\"start\":1550},{\"end\":1579,\"start\":1563},{\"end\":1592,\"start\":1579},{\"end\":1603,\"start\":1592},{\"end\":1933,\"start\":1920},{\"end\":1945,\"start\":1933},{\"end\":1953,\"start\":1945},{\"end\":1966,\"start\":1953},{\"end\":1980,\"start\":1966},{\"end\":1988,\"start\":1980}]", "bib_venue": "[{\"end\":1607,\"start\":1603},{\"end\":1992,\"start\":1988}]"}}}, "year": 2023, "month": 12, "day": 17}