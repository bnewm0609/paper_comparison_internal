{"id": 235771722, "updated": "2022-09-29 23:27:08.782", "metadata": {"title": "Wasserstein Barycenter Transport for Acoustic Adaptation", "authors": "[{\"first\":\"Eduardo\",\"last\":\"Montesuma\",\"middle\":[\"F.\"]},{\"first\":\"Fred-Maurice\",\"last\":\"Ngol\u00e8 Mboula\",\"middle\":[]}]", "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "journal": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The recognition of music genre and the discrimination between music and speech are important components of modern digital music systems. Depending on the acquisition conditions, such as background environment, these signals may come from different probability distributions, making the learning problem complicated. In this context, domain adaptation is a key theory to improve performance. Considering data coming from various background conditions, the adaptation scenario is called multi-source. This paper proposes a multi-source domain adaptation algorithm called Wasserstein Barycenter Transport, which transports the source domains to a target domain by creating an intermediate domain using the Wasserstein barycenter. Our method outperforms other state-of-the-art algorithms, and performs better than classifiers trained with target-only data.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icassp/MontesumaM21", "doi": "10.1109/icassp39728.2021.9414199"}}, "content": {"source": {"pdf_hash": "fc9ea01a0755a8eb7decdaea004721fec954983c", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "53561725fe6f6551900ad313fd32d60ef4a78e92", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fc9ea01a0755a8eb7decdaea004721fec954983c.txt", "contents": "\nWASSERSTEIN BARYCENTER TRANSPORT FOR ACOUSTIC ADAPTATION\n\n\nEduardo F Montesuma \nUniversit\u00e9 Paris-Saclay\nCEA\nF-91120PalaiseauListFrance\n\nFred-Maurice Ngol\u00e8 \nUniversit\u00e9 Paris-Saclay\nCEA\nF-91120PalaiseauListFrance\n\nMboula \n\nDepartamento de Teleinform\u00e1tica\nUniversidade Federal do Cear\u00e1\nFortalezaBrazil\n\nWASSERSTEIN BARYCENTER TRANSPORT FOR ACOUSTIC ADAPTATION\n10.1109/ICASSP39728.2021.9414199Index Terms-Optimal Transport for Domain AdaptationMulti-source Domain AdaptationMusic Genre RecognitionMusic Speech Discrimination\nThe recognition of music genre and the discrimination between music and speech are important components of modern digital music systems. Depending on the acquisition conditions, such as background environment, these signals may come from different probability distributions, making the learning problem complicated. In this context, domain adaptation is a key theory to improve performance. Considering data coming from various background conditions, the adaptation scenario is called multi-source. This paper proposes a multi-source domain adaptation algorithm called Wasserstein Barycenter Transport, which transports the source domains to a target domain by creating an intermediate domain using the Wasserstein barycenter. Our method outperforms other state-of-the-art algorithms, and performs better than classifiers trained with targetonly data.\n\nINTRODUCTION\n\nDomain adaptation is one of the most widely studied cases of transfer learning. It often happens in various fields which machine learning is applied, such as image [1] and natural language [2] processing. The main assumption of this field is that train and test data come from different domains, respectively a source Ds and target Dt domain. These domains differ on the marginal distribution of features X, that is Ps(X) = Pt(X). Therefore the goal of domain adaptation is to adapt a predictive model that has been learned on data coming from the source domain Xs, to data coming from the target domain Xt.\n\nThis setting has multiple applications to the classification of acoustic signals. For instance, the feature distribution mismatch may be due to different recording devices, or to different background noise. An example of the first case was presented in the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge [3] where the acoustic scene classification task involved multiple recording devices. An example of the second case was presented in one of the experiments of [4], where the authors simulated various background conditions by overlaying signals with noise.\n\nIn this context, we present two contributions to the aforementioned topics: (1) the proposition of a new method for Multi-Source Domain Adaptation (MSDA), the Wassertein Barycenter Transport (WBT), and (2) the application of various domain adaptation techniques for two acoustic signal processing tasks, namely Music Genre Recognition (MGR) and Music-Speech Discrimination (MSD). The multiple domains in these tasks correspond to different background noise as mentioned before.\n\nThe rest of the paper is organized as follows. Section 2 presents the state-of-the-art in domain adaptation, MGR and MSD. Section 3 discusses the foundations of our method, as well as how to apply it. Section 4 describes the experiments and discuss its results. Finally, section 5 concludes this paper.\n\n\nRELATED WORK\n\nThe foundations of this work can be classified into two topics: transfer learning (with focus on domain adaptation) and optimal transport.\n\nTransfer learning is an old and recurrent problem in machine learning, under which one wants to use the knowledge learned by a model in other contexts. These contexts may differ, as first defined by [5], according to the nature of the data (e.g. images acquired in different poses and illumination conditions) or to the task itself (e.g. trying to learn how to play electric guitar from the knowledge of how to play guitar).\n\nBetween the various types of transfer learning, the most widely studied is the transductive setting, where one has a single task, but data coming from one or various domains [5]. The latter case has been called MSDA by the research community [6], and is viewed as a natural generalization to the theoretical framework of single-source domain adaptation.\n\nClassical single-source domain adaptation techniques include importance weighting [7], subspace techniques [8] and neural network adversarial training [9], which have shown outstanding results in visual and semantic adaptation.\n\nOptimal Transport is a field of mathematics involved with the transportation of masses, which dates the 18-th century [10]. Since this formulation, it has been recast in terms of the transportation of probability distributions. While this primarily concerns continuous distributions, the theory may be formulated for discrete ones as well, which is particularly useful for fields such as machine learning and image processing [11].\n\nThe first appearance of optimal transportation theory in the context of domain adaptation was on the seminal works of [12], where authors have used the optimal transport plan to devise a transformation between domains. This methodology aims to match source and target domain distributions, hence minimizing the divergence between them.\n\nSince then, other methods have been devised for the multisource case, such as [4] and [13]. The approach taken by [13] is particularly similar to the one proposed by this work, although focused in another problem: transportation under target shift through the Wasserstein barycenter.\n\nThe notion of Wasserstein barycenter comes from the fact that the space of probability distributions endowed with the Wasserstein distance is a metric space, as proven by [10]. As follows, the geometric notion of barycenter may be extended to this space. The mathematical definition of Wasserstein barycenter was first given by [14], and the computational procedure to compute it from finite samples was first proposed by [15].\n\nAdditionally, the classification of acoustic signals is an important problem in signal processing, being behind important services in many commercial applications, such as recommender systems. In this work, two tasks are considered: (1) MGR, for which the dataset proposed by [16] is used as benchmark and (2) MSD, for which [17] is used as benchmark.\n\nTo the best of our knowledge, our approach is the first to tackle the problem of covariate shift by using the Wasserstein barycenter, and to apply domain adaptation for MGR and MSD.\n\n\nPROPOSED METHOD\n\nIn this section we discuss how the Wasserstein barycenter may be used to perform MSDA. To do so, the Optimal Transport for Domain Adaptation (OTDA) framework is discussed in Section 3.1, then the Wasserstein barycenter is defined in Section 3.2 and finally our technique is presented in Section 3.3\n\n\nOptimal Transport for Domain Adaptation\n\nThe OTDA framework was presented first by [12] as a general procedure to perform domain adaptation under covariate shift hypothesis. Mainly, it is assumed that Ps(X, Y ) = Pt(X, Y ) due to a shift in the marginal distributions Ps(X) and Pt(X). Moreover, the shift is assumed to be caused by an unknown and possibly nonlinear transformation T .\n\nIn this case, we suppose that a dataset composed by source sam-\nples Ds = {(x s i , y s i )} ns i=1 and target samples Dt = {x t i } n t i=1 is available.\nThe goal of domain adaptation is to improve the performance on Dt of models learned on Ds. To do so through optimal transport, one defines the following empirical probability measures,\n\u00b5s(x) = 1 ns ns i=1 \u03b4(x \u2212 x s i ), \u00b5t(x) = 1 nt n t i=1 \u03b4(x \u2212 x t i ),\nwhere \u03b4 represents the Dirac delta function. Hence, the optimal transportation problem from \u00b5s to \u00b5t is given by,\n\u03b3 * = argmin \u03b3\u2208\u03a0(\u00b5s,\u00b5 t ) C, \u03b3 F ,(1)\nwhere C, \u03b3 F = i j Cij\u03b3ij is the Frobenius inner product,\nCij = C(x s i , x t j )\nis the cost associated with transporting sample x s i to x t j , and \u03a0(\u00b5s, \u00b5t) represents the set of all couplings between \u00b5s and \u00b5t. \u03b3 is called the transportation plan, and the elements \u03b3ij describe how much mass is transported from x s i to x t j . As pointed out by [11], Equation 1 represents a linear program on \u03b3, and hence may be solved using linear programming techniques. However, the size of the linear program imposed by Equation 1 is rather restrictive, motivating [18] to propose the Sinkhorn algorithm. This algorithms adds an entropic penalty,\n\u2126e(\u03b3) = i j \u03b3ij(log(\u03b3ij) \u2212 1),(2)\nregulated by \u03bbe to Equation 1. As \u03bbe \u2192 0, the solution\u03b3 of the Sinkhorn algorithm approaches \u03b3 * . The main advantage of this algorithm is its faster computation. Other kinds of penalties may be included in the optimization problem 1. For instance, [12] discusses the usage of group-sparsity regularization that a sample in the target domain only receives mass from points in the source domain. In this work we make usage of the semi-supervised penalty,\n\u2126s(\u03b3; ys, yt) = L if y s i = y t j 0 otherwise (3)\nwhere L maxi,j Cij is a constant chosen beforehand. This penalty assumes that the target labels in the target domain are available, and hence is called semi-supervised. As it will be discussed in Section 3.2, this term will be useful for the calculation of the barycenter even when y t is not available.\n\nOnce \u03b3 * is estimated, the transformation T between source and target domains needs to be defined. As [12] suggests, one may use the Barycentric mapping T\u03b3 * which has the expression,\nT\u03b3 * (x s i ) = argmin x\u2208R d n t j=1 \u03b3 * ij C(x, x t j ),(4)\nfor\nC(x s i , x t j ) = ||x s i \u2212 x t j || 2 2 , Equation 4\nhas closed form solution. By lettingXs be the matrix whose i th row corresponds to T\u03b3 * (x s i ),\nXs = ns\u03b3 * Xt.(5)\nAs consequence, instead of training a classifier on (Xs, ys), the OTDA framework proposes to transport Xs ontoXs through Equation 5, then training on (Xs, ys).\n\n\nWasserstein Barycenters\n\nDespite serving as the foundation for the estimation of the transportation plan \u03b3, Equation 1 can be used to define a distance on the space of probability measures. Indeed, suppose that \u03b3 * is the solution of the optimization problem posed by Equation 1, then the Wasserstein distance WC (\u00b5s, \u00b5t) is given by C, \u03b3 * F . As proven in [10] WC is an actual distance on the space of probability measures. Therefore, geometric notions such as the one of barycenter can be extended to this space. This latter notion was first formalized by [14]. These authors have defined the Wasserstein barycenter of a set of N measures {\u00b5 k } N k=1 through the expression,\n\u00b5 b = argmin \u00b5 N k=1 \u03bb k WC (\u00b5 k , \u00b5).(6)\nAs discussed in [15], the barycenter may be further calculated using Sinkhorn distances rather than Wasserstein distances, keeping in mind the choice of an adequate \u03bbe. Moreover, let the barycenter measure \u00b5 b be expressed as,\n\u00b5 b (x) = n b i=1 \u03b1i\u03b4(x \u2212 x b i ),\nso, the barycenter probability measure is uniquely defined by the weights \u03b1i and support points x b i . Indeed, [15] makes the distinction on whether one wants to optimize Equation 6 with respect to the weights \u03b1i (fixed-support barycenter) or with respect to the support points x b i (free-support barycenter). Accordingly to the OTDA framework proposed by [12], we suppose the weights \u03b1i fixed and equal to 1/n b . Therefore, Equation 6 may be resumed to updating the barycenter support until convergence. These updates have a closed-form solution for C as the euclidean distance,\nX b = n b N k=1 \u03bb k \u03b3 T k Xs k ,\nwhere \u03b3 k corresponds to the solution of the optimal transport problem between \u00b5 k and \u00b5 b . This procedure is detailed in Algorithm 2 of [15] and is henceforth adopted for calculating the barycenter support.\n\n\nWasserstein Barycenter Transport\n\nThe main difference between the OTDA and the MSDA setting is that, in the latter case, a set of N source domains are available with labeled samples\nDs k = {(x s k i , y s k i )} ns k i=1 .\nThe main challenge is to devise a way to transporting each domain into the target.\n\nIn this work we propose the WBT, a method which aggregates the various domains into an intermediate domain, defined through the Wasserstein barycenter. Once the data has been transported to this domain, it can be transported to the target domain using the usual single-source OTDA framework. The procedure is illustrated in Figure 1, and detailed in Algorithm 1.\n\n\nAlgorithm 1 Wassertein Barycenter Transport\n\n\nInput: Source domains samples Ds\nk = {(x s k i , y s k i )} ns k i=1 , target domain samples Dt = {x t i } n t i=1\n, initial barycenter support X 0 b , barycenter labels y b X b = barycenter({Xs k } N k=1 , X 0 b , ys, y b ) \u03b3 bt = sinkhorn(X b , Xt) Xs = ns\u03b3 bt Xt Output: Transported source samplesXs.\n\nThe first step consists on calculating the barycenter, using Algorithm 2 of [15] for a free-support with few modifications. First, notice that we may artificially label the barycenter points. Indeed, each barycenter point corresponds to a sample in a given source domain. Second, we propose solving a penalized version of Equation 6, using the penalty 3 to induce a class structure on the estimated barycenter. Despite being originally devised for semi-supervised transportation, since we have artificially labeled the barycenter, we may use this penalty with no assumptions upon the target labels. Hence, the optimization problem solved by the routine barycenter in Algorithm 1 is,\nX b = argmin X,\u03b3 k \u2208\u03a0(\u00b5s k ,\u00b5 b ) N k=1 \u03bb k C k , \u03b3 k F \u2212 \u03bbe\u2126e(\u03b3 k )+ \u2126s(\u03b3 k ; ys k , y b ) ,(7)\nThe second step consists in solving the optimal transportation problem between \u00b5 b and \u00b5t. For this step, the standard Sinkhorn algorithm [18] is used, with no additional penalties. This step gives another transportation plan \u03b3 bt . Finally, we transport the barycenter support X b into the target domain through barycentric mapping described by Equation 5. Notice that this step does not assume any labeling on the target domain.\n\n\nEXPERIMENTS AND DISCUSSION\n\nIn this section we detail the experiments made involved our proposed method, the WBT, as well as details concerning its implementation and the data preparation.\n\nOur method was implemented in Python using the POT toolbox [19]. Likewise, SinT, SinTreg, JCPOT, and JCPOT-LP are also distributed in this toolbox, corresponding to the authors code. The algorithm chosen for classification was a Random Forest classifier with 10 3 trees, and maximum depth of 13. Our code is publicly available in a Github repository 1 .\n\nFor each experiment, we compare our methods with the state of the art. We refer to Wasserstein Barycenter Transport as WBT and to its regularized version as WBTreg (using Equation 7). The comparison with other OTDA methods is established, being them: SinT, the standard OTDA framework with all sources concatenated (no source distinction) and JCPOT [13], which has as main assumption target shift. To further explore the potentiality of our method, two other methods that are not based on optimal transport are evaluated: Kernel Mean Matching (KMM) [7] and Transfer Component Analysis (TCA) [8].\n\nTo establish a comparison between the selected algorithms, two acoustic classification tasks were chosen: MGR [16] and MSD [17]. The first dataset is composed by 10 musical genres (blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, and rock), having 100 samples each. Hence, the MGR has 1000 samples per domain. The second dataset is composed by 64 music samples, and 64 speech samples, having 128 samples per domain. In both datasets, each sample is a .wav file with 30 seconds of duration. To simulate different domains, the approach taken by [4] was chosen. Specifically, each audio sample is overlaid with a specific noise, chosen from a noise dataset 2 using the Pydub library [20]. A total of 56 features were extracted on both datasets using the Librosa library [21], including the mean and variance of chromagram, root mean squared error, spectral bandwidth and rolloff, zero crossing rate, harmonic and percussive components, tempo and 20 Mel-Frequency Cepstral Coefficient (MFCC).\n\nThe comparison between the various methods is done by choosing a given domain as the target, and leaving the rest as sources. To mimic real-world conditions, each noisy domain is chosen as the target, while the original samples are always among the sources. The algorithms are evaluated using a 5-fold cross-validation procedure. Tables 1 and 2 show the mean and standard deviation for the test accuracy.\n\nConcerning Table 1, first we note the poor performance of KMM and WBT. Concerning KMM, this is mainly due the fact that it makes the assumption that the support of Pt(X) is contained in the support of Pt(X). This is not necessarily true, since the process of overlay between the signals can induce a different support on the target domain. Concerning WBT, the performance degradation happens since there is no class penalty in the barycenter's calculation. Hence, it simply calculates the barycenter of the marginals \u00b5s k and \u00b5t, mixing the classes.     Table 1: Results for the MGR task. Each column corresponds to the experiment with a given target. For each column, the best accuracy is shown in bold, while the second best is underlined.\n\nSecond, notice that JCPOT, a method designed for MSDA, under-performs other single-source methods, such as SinT or TCA. However, we remark that this method has been designed for a target shift hypothesis, which is not assumed in this work (as the classes are balanced). Moreover, this algorithm relies on the estimation of class proportions. In our experiments, we seen that JCPOT does not estimate these proportions correctly for MGR, resulting in poor performances.\n\nFinally, notice that WBTreg is the best performing algorithm among the tested methods, improving the baseline by 41.77% on average. When compared to the second best method (SinTreg), it presents an average improvement of 19.67%. Moreover, it has even improved the target-only case, where one assumes that a classifier is trained and evaluated only on labeled target data. The average improvement in accuracy is 14.25%.  Table 2: Results for the MSD task, with similar notation to Table 1.\n\n\nMethod\n\nConcerning Table 2, when using Destroyerengine and Factory2 as target domains, the baseline performs as good as a random classifier. In these cases, KMM does not manage to improve the accuracy, reinforcing our claim that the support of Pt is not contained in the support of Ps. Even worse, for F16, KMM degrades the performance. Additionally, when using Bucanneer2 as the target, the algorithm manages to improve the accuracy. Moreover, JCPOT is competitive with other domain adaptation methods. This is due the correct estimation of class proportions, unlike the previous case.\n\nFinally, as for the MGR results, in most cases the accuracy improvement promoted by WBTreg is higher than all other algorithms. The only exception is for F16 target, which TCA manages to surpass our method. Moreover, it manages to improve the baseline by a margin of 26.34% and 3.29% over the target-only case.\n\n\nCONCLUSIONS\n\nIn this work, we propose a novel technique for MSDA which uses the Wasserstein barycenter to construct an intermediate domain in the transportation from sources to target domain. This novel algorithm is used in context of domain adaptation of acoustic signals, specially MGR and MSD tasks.\n\nWhen compared to the baseline in those tasks, our method is capable of improving the classification accuracy of 41.77% on average and 26.34%, for MGR and MSD respectively. More importantly, it manages to improve accuracy over the target-only case by 14.25% and 3.29%, for MGR and MSD respectively. Moreover, our algorithm improves the state-of-the-art in most cases. We remark that this was only possible through the introduction of a class-based regularization term, which induces class-structure in the estimated barycenter. This is evidenced by the gap in performance between the regularized and non-regularized versions of our algorithm.\n\n\nstep: the Wassertein barycenter of multiple sources is calculated.\n\n\nstep: the Wasserstein barycenter is transported to the target domain.\n\nFig. 1 :\n1Illustration of the Wasserstein Barycenter Transport algorithm. The multi-source assumption is illustrated in (a). The method is composed of two steps, shown in (b) and (c).\nhttps://github.com/eddardd/WBTransport 2 http://spib.linse.ufsc.br/noise.html\n\nDeep visual domain adaptation: A survey. Mei Wang, Weihong Deng, Neurocomputing. 312Mei Wang and Weihong Deng, \"Deep visual domain adapta- tion: A survey,\" Neurocomputing, vol. 312, pp. 135-153, 2018.\n\nBiographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. John Blitzer, Mark Dredze, Fernando Pereira, Proceedings of the 45th annual meeting of the association of computational linguistics. the 45th annual meeting of the association of computational linguisticsJohn Blitzer, Mark Dredze, and Fernando Pereira, \"Biogra- phies, bollywood, boom-boxes and blenders: Domain adapta- tion for sentiment classification,\" in Proceedings of the 45th annual meeting of the association of computational linguistics, 2007, pp. 440-447.\n\nA multi-device dataset for urban acoustic scene classification. Annamaria Mesaros, Toni Heittola, Tuomas Virtanen, Scenes and Events 2018 Workshop (DCASE2018). 9Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen, \"A multi-device dataset for urban acoustic scene classification,\" in Scenes and Events 2018 Workshop (DCASE2018), 2018, p. 9.\n\nMulti-source domain adaptation via weighted joint distributions optimal transport. Rosanna Turrisi, R\u00e9mi Flamary, Alain Rakotomamonjy, Massimiliano Pontil, arXiv:2006.12938arXiv preprintRosanna Turrisi, R\u00e9mi Flamary, Alain Rakotomamonjy, and Massimiliano Pontil, \"Multi-source domain adaptation via weighted joint distributions optimal transport,\" arXiv preprint arXiv:2006.12938, 2020.\n\nA survey on transfer learning. Qiang Sinno Jialin Pan, Yang, IEEE Transactions on knowledge and data engineering. 2210Sinno Jialin Pan and Qiang Yang, \"A survey on transfer learn- ing,\" IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345-1359, 2009.\n\nDomain adaptation with multiple sources. Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh, Advances in neural information processing systems. Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh, \"Domain adaptation with multiple sources,\" in Advances in neural information processing systems, 2009, pp. 1041-1048.\n\nCorrecting sample selection bias by unlabeled data. Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Sch\u00f6lkopf, Alex J Smola, Advances in neural information processing systems. Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Sch\u00f6lkopf, and Alex J Smola, \"Correcting sample selection bias by unlabeled data,\" in Advances in neural information processing systems, 2007, pp. 601-608.\n\nDomain adaptation via transfer component analysis. Ivor W Sinno Jialin Pan, James T Tsang, Qiang Kwok, Yang, IEEE Transactions on Neural Networks. 222Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang, \"Domain adaptation via transfer component analysis,\" IEEE Transactions on Neural Networks, vol. 22, no. 2, pp. 199-210, 2010.\n\nDomain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor Lempitsky, The Journal of Machine Learning Research. 171Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Ger- main, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky, \"Domain-adversarial training of neural networks,\" The Journal of Machine Learning Research, vol. 17, no. 1, pp. 2096-2030, 2016.\n\nC\u00e9dric Villani, Optimal transport: old and new. Springer Science & Business Media338C\u00e9dric Villani, Optimal transport: old and new, vol. 338, Springer Science & Business Media, 2008.\n\nRegularized discrete optimal transport. Sira Ferradans, Nicolas Papadakis, Gabriel Peyr\u00e9, Jean-Fran\u00e7ois Aujol, SIAM Journal on Imaging Sciences. 73Sira Ferradans, Nicolas Papadakis, Gabriel Peyr\u00e9, and Jean- Fran\u00e7ois Aujol, \"Regularized discrete optimal transport,\" SIAM Journal on Imaging Sciences, vol. 7, no. 3, pp. 1853- 1882, 2014.\n\nOptimal transport for domain adaptation. N Courty, R Flamary, D Tuia, A Rakotomamonjy, IEEE Transactions on Pattern Analysis and Machine Intelligence. 399N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy, \"Op- timal transport for domain adaptation,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 9, pp. 1853-1865, 2017.\n\nOptimal transport for multi-source domain adaptation under target shift. Ievgen Redko, Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, PMLRof Proceedings of Machine Learning Research. Kamalika Chaudhuri and Masashi Sugiyama89Proceedings of Machine Learning ResearchIevgen Redko, Nicolas Courty, R\u00e9mi Flamary, and Devis Tuia, \"Optimal transport for multi-source domain adaptation under target shift,\" in Proceedings of Machine Learning Research, Kamalika Chaudhuri and Masashi Sugiyama, Eds. 16-18 Apr 2019, vol. 89 of Proceedings of Machine Learning Research, pp. 849-858, PMLR.\n\nBarycenters in the wasserstein space. Martial Agueh, Guillaume Carlier, SIAM Journal on Mathematical Analysis. 432Martial Agueh and Guillaume Carlier, \"Barycenters in the wasserstein space,\" SIAM Journal on Mathematical Analysis, vol. 43, no. 2, pp. 904-924, 2011.\n\nFast computation of wasserstein barycenters. Marco Cuturi, Arnaud Doucet, PMLRProceedings of the 31st International Conference on Machine Learning. Eric P. Xing and Tony Jebarathe 31st International Conference on Machine LearningBejing, China32Marco Cuturi and Arnaud Doucet, \"Fast computation of wasserstein barycenters,\" in Proceedings of the 31st Interna- tional Conference on Machine Learning, Eric P. Xing and Tony Jebara, Eds., Bejing, China, 22-24 Jun 2014, vol. 32 of Pro- ceedings of Machine Learning Research, pp. 685-693, PMLR.\n\nMusical genre classification of audio signals. George Tzanetakis, Perry Cook, IEEE Transactions on speech and audio processing. 105George Tzanetakis and Perry Cook, \"Musical genre classifica- tion of audio signals,\" IEEE Transactions on speech and audio processing, vol. 10, no. 5, pp. 293-302, 2002.\n\nAutomatic musical genre classification of audio signals. Tzanetakis George, Essl Georg, Cook Perry, Proceedings of the 2nd international symposium on music information retrieval. the 2nd international symposium on music information retrievalIndianaTzanetakis George, Essl Georg, and Cook Perry, \"Automatic musical genre classification of audio signals,\" in Proceedings of the 2nd international symposium on music information re- trieval, Indiana, 2001.\n\nSinkhorn distances: Lightspeed computation of optimal transport. Marco Cuturi, Advances in neural information processing systems. Marco Cuturi, \"Sinkhorn distances: Lightspeed computation of optimal transport,\" in Advances in neural information pro- cessing systems, 2013, pp. 2292-2300.\n\nPot python optimal transport library. Re\u1e3fi Flamary, Nicolas Courty, Re\u1e3fi Flamary and Nicolas Courty, \"Pot python optimal trans- port library,\" 2017.\n\n. James Robert, Marc Webbie, Pydub. GithubJames Robert, Marc Webbie, et al., \"Pydub,\" Github, 2011.\n\n. Brian Mcfee, librosa/librosa: 0.8.0Brian McFee et al., \"librosa/librosa: 0.8.0,\" July 2020.\n", "annotations": {"author": "[{\"end\":136,\"start\":60},{\"end\":212,\"start\":137},{\"end\":220,\"start\":213},{\"end\":300,\"start\":221}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":70},{\"end\":155,\"start\":150}]", "author_first_name": "[{\"end\":67,\"start\":60},{\"end\":69,\"start\":68},{\"end\":149,\"start\":137},{\"end\":219,\"start\":213}]", "author_affiliation": "[{\"end\":135,\"start\":81},{\"end\":211,\"start\":157},{\"end\":299,\"start\":222}]", "title": "[{\"end\":57,\"start\":1},{\"end\":357,\"start\":301}]", "venue": null, "abstract": "[{\"end\":1373,\"start\":522}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1556,\"start\":1553},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1581,\"start\":1578},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2335,\"start\":2332},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2494,\"start\":2491},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3729,\"start\":3726},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4130,\"start\":4127},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4198,\"start\":4195},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4393,\"start\":4390},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4418,\"start\":4415},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4462,\"start\":4459},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4659,\"start\":4655},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4967,\"start\":4963},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5092,\"start\":5088},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5388,\"start\":5385},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5397,\"start\":5393},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5425,\"start\":5421},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5767,\"start\":5763},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5924,\"start\":5920},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6018,\"start\":6014},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6301,\"start\":6297},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6350,\"start\":6346},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6963,\"start\":6959},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8181,\"start\":8177},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8389,\"start\":8385},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8754,\"start\":8750},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9417,\"start\":9413},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10256,\"start\":10252},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10457,\"start\":10453},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10635,\"start\":10631},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10993,\"start\":10989},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11239,\"start\":11235},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11635,\"start\":11631},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12807,\"start\":12803},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13649,\"start\":13645},{\"end\":13863,\"start\":13853},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14193,\"start\":14189},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14838,\"start\":14834},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15037,\"start\":15034},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15079,\"start\":15076},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15196,\"start\":15192},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15209,\"start\":15205},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15647,\"start\":15644},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15785,\"start\":15781},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15872,\"start\":15868}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20115,\"start\":20047},{\"attributes\":{\"id\":\"fig_1\"},\"end\":20187,\"start\":20116},{\"attributes\":{\"id\":\"fig_2\"},\"end\":20372,\"start\":20188}]", "paragraph": "[{\"end\":1996,\"start\":1389},{\"end\":2587,\"start\":1998},{\"end\":3066,\"start\":2589},{\"end\":3370,\"start\":3068},{\"end\":3525,\"start\":3387},{\"end\":3951,\"start\":3527},{\"end\":4306,\"start\":3953},{\"end\":4535,\"start\":4308},{\"end\":4968,\"start\":4537},{\"end\":5305,\"start\":4970},{\"end\":5590,\"start\":5307},{\"end\":6019,\"start\":5592},{\"end\":6372,\"start\":6021},{\"end\":6555,\"start\":6374},{\"end\":6873,\"start\":6575},{\"end\":7260,\"start\":6917},{\"end\":7325,\"start\":7262},{\"end\":7601,\"start\":7417},{\"end\":7786,\"start\":7673},{\"end\":7882,\"start\":7825},{\"end\":8466,\"start\":7907},{\"end\":8954,\"start\":8501},{\"end\":9309,\"start\":9006},{\"end\":9494,\"start\":9311},{\"end\":9559,\"start\":9556},{\"end\":9713,\"start\":9616},{\"end\":9891,\"start\":9732},{\"end\":10572,\"start\":9919},{\"end\":10841,\"start\":10615},{\"end\":11459,\"start\":10877},{\"end\":11701,\"start\":11493},{\"end\":11885,\"start\":11738},{\"end\":12009,\"start\":11927},{\"end\":12373,\"start\":12011},{\"end\":12725,\"start\":12537},{\"end\":13409,\"start\":12727},{\"end\":13937,\"start\":13507},{\"end\":14128,\"start\":13968},{\"end\":14483,\"start\":14130},{\"end\":15080,\"start\":14485},{\"end\":16089,\"start\":15082},{\"end\":16495,\"start\":16091},{\"end\":17238,\"start\":16497},{\"end\":17707,\"start\":17240},{\"end\":18197,\"start\":17709},{\"end\":18786,\"start\":18208},{\"end\":19098,\"start\":18788},{\"end\":19403,\"start\":19114},{\"end\":20046,\"start\":19405}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7416,\"start\":7326},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7672,\"start\":7602},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7824,\"start\":7787},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7906,\"start\":7883},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8500,\"start\":8467},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9005,\"start\":8955},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9555,\"start\":9495},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9615,\"start\":9560},{\"attributes\":{\"id\":\"formula_8\"},\"end\":9731,\"start\":9714},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10614,\"start\":10573},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10876,\"start\":10842},{\"attributes\":{\"id\":\"formula_11\"},\"end\":11492,\"start\":11460},{\"attributes\":{\"id\":\"formula_12\"},\"end\":11926,\"start\":11886},{\"attributes\":{\"id\":\"formula_13\"},\"end\":12536,\"start\":12455},{\"attributes\":{\"id\":\"formula_14\"},\"end\":13506,\"start\":13410}]", "table_ref": "[{\"end\":16435,\"start\":16421},{\"end\":16515,\"start\":16508},{\"end\":17058,\"start\":17051},{\"end\":18136,\"start\":18129},{\"end\":18196,\"start\":18189},{\"end\":18226,\"start\":18219}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1387,\"start\":1375},{\"attributes\":{\"n\":\"2.\"},\"end\":3385,\"start\":3373},{\"attributes\":{\"n\":\"3.\"},\"end\":6573,\"start\":6558},{\"attributes\":{\"n\":\"3.1.\"},\"end\":6915,\"start\":6876},{\"attributes\":{\"n\":\"3.2.\"},\"end\":9917,\"start\":9894},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11736,\"start\":11704},{\"end\":12419,\"start\":12376},{\"end\":12454,\"start\":12422},{\"attributes\":{\"n\":\"4.\"},\"end\":13966,\"start\":13940},{\"end\":18206,\"start\":18200},{\"attributes\":{\"n\":\"5.\"},\"end\":19112,\"start\":19101},{\"end\":20197,\"start\":20189}]", "table": null, "figure_caption": "[{\"end\":20115,\"start\":20049},{\"end\":20187,\"start\":20118},{\"end\":20372,\"start\":20199}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12343,\"start\":12335}]", "bib_author_first_name": "[{\"end\":20496,\"start\":20493},{\"end\":20510,\"start\":20503},{\"end\":20755,\"start\":20751},{\"end\":20769,\"start\":20765},{\"end\":20786,\"start\":20778},{\"end\":21291,\"start\":21282},{\"end\":21305,\"start\":21301},{\"end\":21322,\"start\":21316},{\"end\":21651,\"start\":21644},{\"end\":21665,\"start\":21661},{\"end\":21680,\"start\":21675},{\"end\":21708,\"start\":21696},{\"end\":21985,\"start\":21980},{\"end\":22274,\"start\":22268},{\"end\":22291,\"start\":22284},{\"end\":22305,\"start\":22299},{\"end\":22605,\"start\":22598},{\"end\":22619,\"start\":22613},{\"end\":22636,\"start\":22629},{\"end\":22656,\"start\":22648},{\"end\":22672,\"start\":22668},{\"end\":22674,\"start\":22673},{\"end\":23004,\"start\":23000},{\"end\":23006,\"start\":23005},{\"end\":23030,\"start\":23025},{\"end\":23032,\"start\":23031},{\"end\":23045,\"start\":23040},{\"end\":23344,\"start\":23336},{\"end\":23360,\"start\":23352},{\"end\":23375,\"start\":23371},{\"end\":23390,\"start\":23384},{\"end\":23404,\"start\":23400},{\"end\":23425,\"start\":23417},{\"end\":23443,\"start\":23438},{\"end\":23460,\"start\":23454},{\"end\":23795,\"start\":23789},{\"end\":24017,\"start\":24013},{\"end\":24036,\"start\":24029},{\"end\":24055,\"start\":24048},{\"end\":24076,\"start\":24063},{\"end\":24352,\"start\":24351},{\"end\":24362,\"start\":24361},{\"end\":24373,\"start\":24372},{\"end\":24381,\"start\":24380},{\"end\":24744,\"start\":24738},{\"end\":24759,\"start\":24752},{\"end\":24772,\"start\":24768},{\"end\":24787,\"start\":24782},{\"end\":25284,\"start\":25277},{\"end\":25301,\"start\":25292},{\"end\":25555,\"start\":25550},{\"end\":25570,\"start\":25564},{\"end\":26098,\"start\":26092},{\"end\":26116,\"start\":26111},{\"end\":26414,\"start\":26404},{\"end\":26427,\"start\":26423},{\"end\":26439,\"start\":26435},{\"end\":26871,\"start\":26866},{\"end\":27132,\"start\":27128},{\"end\":27149,\"start\":27142},{\"end\":27247,\"start\":27242},{\"end\":27260,\"start\":27256},{\"end\":27348,\"start\":27343}]", "bib_author_last_name": "[{\"end\":20501,\"start\":20497},{\"end\":20515,\"start\":20511},{\"end\":20763,\"start\":20756},{\"end\":20776,\"start\":20770},{\"end\":20794,\"start\":20787},{\"end\":21299,\"start\":21292},{\"end\":21314,\"start\":21306},{\"end\":21331,\"start\":21323},{\"end\":21659,\"start\":21652},{\"end\":21673,\"start\":21666},{\"end\":21694,\"start\":21681},{\"end\":21715,\"start\":21709},{\"end\":22002,\"start\":21986},{\"end\":22008,\"start\":22004},{\"end\":22282,\"start\":22275},{\"end\":22297,\"start\":22292},{\"end\":22318,\"start\":22306},{\"end\":22611,\"start\":22606},{\"end\":22627,\"start\":22620},{\"end\":22646,\"start\":22637},{\"end\":22666,\"start\":22657},{\"end\":22680,\"start\":22675},{\"end\":23023,\"start\":23007},{\"end\":23038,\"start\":23033},{\"end\":23050,\"start\":23046},{\"end\":23056,\"start\":23052},{\"end\":23350,\"start\":23345},{\"end\":23369,\"start\":23361},{\"end\":23382,\"start\":23376},{\"end\":23398,\"start\":23391},{\"end\":23415,\"start\":23405},{\"end\":23436,\"start\":23426},{\"end\":23452,\"start\":23444},{\"end\":23470,\"start\":23461},{\"end\":23803,\"start\":23796},{\"end\":24027,\"start\":24018},{\"end\":24046,\"start\":24037},{\"end\":24061,\"start\":24056},{\"end\":24082,\"start\":24077},{\"end\":24359,\"start\":24353},{\"end\":24370,\"start\":24363},{\"end\":24378,\"start\":24374},{\"end\":24395,\"start\":24382},{\"end\":24750,\"start\":24745},{\"end\":24766,\"start\":24760},{\"end\":24780,\"start\":24773},{\"end\":24792,\"start\":24788},{\"end\":25290,\"start\":25285},{\"end\":25309,\"start\":25302},{\"end\":25562,\"start\":25556},{\"end\":25577,\"start\":25571},{\"end\":26109,\"start\":26099},{\"end\":26121,\"start\":26117},{\"end\":26421,\"start\":26415},{\"end\":26433,\"start\":26428},{\"end\":26445,\"start\":26440},{\"end\":26878,\"start\":26872},{\"end\":27140,\"start\":27133},{\"end\":27156,\"start\":27150},{\"end\":27254,\"start\":27248},{\"end\":27267,\"start\":27261},{\"end\":27354,\"start\":27349}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3654323},\"end\":20652,\"start\":20452},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14688775},\"end\":21216,\"start\":20654},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":50785886},\"end\":21559,\"start\":21218},{\"attributes\":{\"doi\":\"arXiv:2006.12938\",\"id\":\"b3\"},\"end\":21947,\"start\":21561},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":740063},\"end\":22225,\"start\":21949},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3026868},\"end\":22544,\"start\":22227},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":70831},\"end\":22947,\"start\":22546},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":788838},\"end\":23286,\"start\":22949},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2871880},\"end\":23787,\"start\":23288},{\"attributes\":{\"id\":\"b9\"},\"end\":23971,\"start\":23789},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3781737},\"end\":24308,\"start\":23973},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13347901},\"end\":24663,\"start\":24310},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b12\",\"matched_paper_id\":88478543},\"end\":25237,\"start\":24665},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8592977},\"end\":25503,\"start\":25239},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b14\",\"matched_paper_id\":16786361},\"end\":26043,\"start\":25505},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3238519},\"end\":26345,\"start\":26045},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14714051},\"end\":26799,\"start\":26347},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15966283},\"end\":27088,\"start\":26801},{\"attributes\":{\"id\":\"b18\"},\"end\":27238,\"start\":27090},{\"attributes\":{\"id\":\"b19\"},\"end\":27339,\"start\":27240},{\"attributes\":{\"doi\":\"librosa/librosa: 0.8.0\",\"id\":\"b20\"},\"end\":27434,\"start\":27341}]", "bib_title": "[{\"end\":20491,\"start\":20452},{\"end\":20749,\"start\":20654},{\"end\":21280,\"start\":21218},{\"end\":21978,\"start\":21949},{\"end\":22266,\"start\":22227},{\"end\":22596,\"start\":22546},{\"end\":22998,\"start\":22949},{\"end\":23334,\"start\":23288},{\"end\":24011,\"start\":23973},{\"end\":24349,\"start\":24310},{\"end\":24736,\"start\":24665},{\"end\":25275,\"start\":25239},{\"end\":25548,\"start\":25505},{\"end\":26090,\"start\":26045},{\"end\":26402,\"start\":26347},{\"end\":26864,\"start\":26801}]", "bib_author": "[{\"end\":20503,\"start\":20493},{\"end\":20517,\"start\":20503},{\"end\":20765,\"start\":20751},{\"end\":20778,\"start\":20765},{\"end\":20796,\"start\":20778},{\"end\":21301,\"start\":21282},{\"end\":21316,\"start\":21301},{\"end\":21333,\"start\":21316},{\"end\":21661,\"start\":21644},{\"end\":21675,\"start\":21661},{\"end\":21696,\"start\":21675},{\"end\":21717,\"start\":21696},{\"end\":22004,\"start\":21980},{\"end\":22010,\"start\":22004},{\"end\":22284,\"start\":22268},{\"end\":22299,\"start\":22284},{\"end\":22320,\"start\":22299},{\"end\":22613,\"start\":22598},{\"end\":22629,\"start\":22613},{\"end\":22648,\"start\":22629},{\"end\":22668,\"start\":22648},{\"end\":22682,\"start\":22668},{\"end\":23025,\"start\":23000},{\"end\":23040,\"start\":23025},{\"end\":23052,\"start\":23040},{\"end\":23058,\"start\":23052},{\"end\":23352,\"start\":23336},{\"end\":23371,\"start\":23352},{\"end\":23384,\"start\":23371},{\"end\":23400,\"start\":23384},{\"end\":23417,\"start\":23400},{\"end\":23438,\"start\":23417},{\"end\":23454,\"start\":23438},{\"end\":23472,\"start\":23454},{\"end\":23805,\"start\":23789},{\"end\":24029,\"start\":24013},{\"end\":24048,\"start\":24029},{\"end\":24063,\"start\":24048},{\"end\":24084,\"start\":24063},{\"end\":24361,\"start\":24351},{\"end\":24372,\"start\":24361},{\"end\":24380,\"start\":24372},{\"end\":24397,\"start\":24380},{\"end\":24752,\"start\":24738},{\"end\":24768,\"start\":24752},{\"end\":24782,\"start\":24768},{\"end\":24794,\"start\":24782},{\"end\":25292,\"start\":25277},{\"end\":25311,\"start\":25292},{\"end\":25564,\"start\":25550},{\"end\":25579,\"start\":25564},{\"end\":26111,\"start\":26092},{\"end\":26123,\"start\":26111},{\"end\":26423,\"start\":26404},{\"end\":26435,\"start\":26423},{\"end\":26447,\"start\":26435},{\"end\":26880,\"start\":26866},{\"end\":27142,\"start\":27128},{\"end\":27158,\"start\":27142},{\"end\":27256,\"start\":27242},{\"end\":27269,\"start\":27256},{\"end\":27356,\"start\":27343}]", "bib_venue": "[{\"end\":20955,\"start\":20884},{\"end\":25747,\"start\":25681},{\"end\":26595,\"start\":26526},{\"end\":20531,\"start\":20517},{\"end\":20882,\"start\":20796},{\"end\":21376,\"start\":21333},{\"end\":21642,\"start\":21561},{\"end\":22061,\"start\":22010},{\"end\":22369,\"start\":22320},{\"end\":22731,\"start\":22682},{\"end\":23094,\"start\":23058},{\"end\":23512,\"start\":23472},{\"end\":23835,\"start\":23805},{\"end\":24116,\"start\":24084},{\"end\":24459,\"start\":24397},{\"end\":24841,\"start\":24798},{\"end\":25348,\"start\":25311},{\"end\":25651,\"start\":25583},{\"end\":26171,\"start\":26123},{\"end\":26524,\"start\":26447},{\"end\":26929,\"start\":26880},{\"end\":27126,\"start\":27090},{\"end\":27274,\"start\":27269}]"}}}, "year": 2023, "month": 12, "day": 17}