{"id": 2723173, "updated": "2023-09-27 19:17:11.999", "metadata": {"title": "FitNets: Hints for Thin Deep Nets", "authors": "[{\"first\":\"Adriana\",\"last\":\"Romero\",\"middle\":[]},{\"first\":\"Nicolas\",\"last\":\"Ballas\",\"middle\":[]},{\"first\":\"Samira\",\"last\":\"Kahou\",\"middle\":[\"Ebrahimi\"]},{\"first\":\"Antoine\",\"last\":\"Chassang\",\"middle\":[]},{\"first\":\"Carlo\",\"last\":\"Gatta\",\"middle\":[]},{\"first\":\"Yoshua\",\"last\":\"Bengio\",\"middle\":[]}]", "venue": "ICLR", "journal": "arXiv: Learning", "publication_date": {"year": 2014, "month": 12, "day": 19}, "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1412.6550", "mag": "2964118293", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/RomeroBKCGB14", "doi": null}}, "content": {"source": {"pdf_hash": "cd85a549add0c7c7def36aca29837efd24b24080", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1412.6550v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "62a85d7104b9e03c8cf0d5972b6deb5ff1eba014", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cd85a549add0c7c7def36aca29837efd24b24080.txt", "contents": "\nFITNETS: HINTS FOR THIN DEEP NETS\n27 Mar 2015\n\nAdriana Romero \nUniversitat de Barcelona\nBarcelonaSpain\n\nNicolas Ballas \nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alQu\u00e9becCanada\n\nSamira Ebrahimi Kahou \nAntoine Chassang \nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alQu\u00e9becCanada\n\nCarlo Gatta \nCentre de Visi\u00f3 per Computador\nBellaterraSpain\n\nYoshua Bengio \nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alQu\u00e9becCanada\n\nCIFAR Senior Fellow. 3\u00c9 cole Polytechnique de Montr\u00e9al\nMontr\u00e9alQu\u00e9becCanada\n\nFITNETS: HINTS FOR THIN DEEP NETS\n27 Mar 2015Published as a conference paper at ICLR 2015\nWhile depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.\n\nINTRODUCTION\n\nDeep networks have recently exhibited state-of-the-art performance in computer vision tasks such as image classification and object detection (Simonyan & Zisserman, 2014;Szegedy et al., 2014). However, top-performing systems usually involve very wide and deep networks, with numerous parameters. Once learned, a major drawback of such wide and deep models is that they result in very time consuming systems at inference time, since they need to perform a huge number of multiplications. Moreover, having large amounts of parameters makes the models high memory demanding. For these reasons, wide and deep top-performing networks are not well suited for applications with memory or time limitations.\n\nThere have been several attempts in the literature to tackle the problem of model compression to reduce the computational burden at inference time. In Bucila et al. (2006), authors propose to train a neural network to mimic the output of a complex and large ensemble. The method uses the ensemble to label unlabeled data and trains the neural network with the data labeled by the ensemble, thus mimicking the function learned by the ensemble and achieving similar accuracy. The idea has been recently adopted in Ba & Caruana (2014) to compress deep and wide networks into shallower but even wider ones, where the compressed model mimics the function learned by the complex model, in this case, by using data labeled by a deep (or an ensemble of deep) networks. More recently, Knowledge Distillation (KD) (Hinton & Dean, 2014) was introduced as a model compression framework, which eases the training of deep networks by following a student-teacher paradigm, in which the student is penalized according to a softened version of the teacher's output. The framework compresses an ensemble of deep networks (teacher) into a student network of similar depth. To do so, the student is trained to predict the output of the teacher, as well as the true classification labels. All previous works related to Convolutional Neural Networks focus on compressing a teacher network or an ensemble of networks into either networks of similar width and depth or into shallower and wider ones; not taking advantage of depth.\n\nDepth is a fundamental aspect of representation learning, since it encourages the re-use of features, and leads to more abstract and invariant representations at higher layers . The importance of depth has been verified (1) theoretically: deep representations are exponentially more expressive than shallow ones for some families of functions (Montufar et al., 2014); and (2) empirically: the two top-performers of ImageNet use deep convolutional networks with 19 and 22 layers, respectively (Simonyan & Zisserman, 2014) and (Szegedy et al., 2014).\n\nNevertheless, training deep architectures has proven to be challenging Erhan et al., 2009), since they are composed of successive non-linearities and, thus result in highly non-convex and non-linear functions. Significant effort has been devoted to alleviate this optimization problem. On the one hand, pre-training strategies, whether unsupervised (Hinton et al., 2006;Bengio et al., 2007) or supervised  train the network parameters in a greedy layerwise fashion in order to initialize the network parameters in a potentially good basin of attraction. The layers are trained one after the other according to an intermediate target. Similarly, semisupervised embedding (Weston et al., 2008) provides guidance to an intermediate layer to help learn very deep networks. Along this line of reasoning, (Cho et al., 2012) ease the optimization problem of DBM by borrowing the activations of another model every second layer in a purely unsupervised scenario. More recently, (Chen-Yu et al., 2014;Szegedy et al., 2014;Gulcehre & Bengio, 2013) showed that adding supervision to intermediate layers of deep architectures assists the training of deep networks. Supervision is introduced by stacking a supervised MLP with a softmax layer on top of intermediate hidden layers to ensure their discriminability w.r.t. labels. Alternatively, Curriculum Learning strategies (CL) (Bengio, 2009) tackle the optimization problem by modifying the training distribution, such that the learner network gradually receives examples of increasing and appropriate difficulty w.r.t. the already learned concepts. As a result, curriculum learning acts like a continuation method, speeds up the convergence of the training process and finds potentially better local minima of highly non-convex cost functions.\n\nIn this paper, we aim to address the network compression problem by taking advantage of depth. We propose a novel approach to train thin and deep networks, called FitNets, to compress wide and shallower (but still deep) networks. The method is rooted in the recently proposed Knowledge Distillation (KD) (Hinton & Dean, 2014) and extends the idea to allow for thinner and deeper student models. We introduce intermediate-level hints from the teacher hidden layers to guide the training process of the student, i.e., we want the student network (FitNet) to learn an intermediate representation that is predictive of the intermediate representations of the teacher network. Hints allow the training of thinner and deeper networks. Results confirm that having deeper models allow us to generalize better, whereas making these models thin help us reduce the computational burden significantly. We validate the proposed method on MNIST, CIFAR-10, CIFAR-100, SVHN and AFLW benchmark datasets and provide evidence that our method matches or outperforms the teacher's performance, while requiring notably fewer parameters and multiplications.\n\n\nMETHOD\n\nIn this section, we detail the proposed student-teacher framework to train FitNets from shallower and wider nets. First, we review the recently proposed KD. Second, we highlight the proposed hints algorithm to guide the FitNet throughout the training process. Finally, we describe how the FitNet is trained in a stage-wise fashion.\n\n\nREVIEW OF KNOWLEDGE DISTILLATION\n\nIn order to obtain a faster inference, we explore the recently proposed compression framework (Hinton & Dean, 2014), which trains a student network, from the softened output of an ensemble of wider networks, teacher network. The idea is to allow the student network to capture not only the information provided by the true labels, but also the finer structure learned by the teacher network. The framework can be summarized as follows.\n\nLet T be a teacher network with an output softmax P T = softmax(a T ) where a T is the vector of teacher pre-softmax activations, for some example. In the case where the teacher model is a single network, a T represents the weighted sums of the output layer, whereas if the teacher model is the result of an ensemble either P T or a T are obtained by averaging outputs from different networks (respectively for arithmetic or geometric averaging). Let S be a student network with parameters W S and output probability P S = softmax(a S ), where a S is the student's pre-softmax output. The student network will be trained such that its output P S is similar to the teacher's output P T , as well as to the true labels y true . Since P T might be very close to the one hot code representation of the sample's true label, a relaxation \u03c4 > 1 is introduced to soften the signal arising from the output of the teacher network, and thus, provide more information during training 1 . The same relaxation is applied to the output of the student network (P \u03c4 S ), when it is compared to the teacher's softened output (P \u03c4 T ):\nP \u03c4 T = softmax a T \u03c4 , P \u03c4 S = softmax a S \u03c4 .(1)\nThe student network is then trained to optimize the following loss function:\nL KD (W S ) = H(y true , P S ) + \u03bbH(P \u03c4 T , P \u03c4 S ),(2)\nwhere H refers to the cross-entropy and \u03bb is a tunable parameter to balance both cross-entropies. Note that the first term in Eq.\n\n(2) corresponds to the traditional cross-entropy between the output of a (student) network and labels, whereas the second term enforces the student network to learn from the softened output of the teacher network.\n\nTo the best of our knowledge, KD is designed such that student networks mimic teacher architectures of similar depth. Although we found the KD framework to achieve encouraging results even when student networks have slightly deeper architectures, as we increase the depth of the student network, KD training still suffers from the difficulty of optimizing deep nets (see Section 4.1).\n\n\nHINT-BASED TRAINING\n\nIn order to help the training of deep FitNets (deeper than their teacher), we introduce hints from the teacher network. A hint is defined as the output of a teacher's hidden layer responsible for guiding the student's learning process. Analogously, we choose a hidden layer of the FitNet, the guided layer, to learn from the teacher's hint layer. We want the guided layer to be able to predict the output of the hint layer. Note that having hints is a form of regularization and thus, the pair hint/guided layer has to be chosen such that the student network is not over-regularized. The deeper we set the guided layer, the less flexibility we give to the network and, therefore, FitNets are more likely to suffer from over-regularization. In our case, we choose the hint to be the middle layer of the teacher network. Similarly, we choose the guided layer to be the middle layer of the student network.\n\nGiven that the teacher network will usually be wider than the FitNet, the selected hint layer may have more outputs than the guided layer. For that reason, we add a regressor to the guided layer, whose output matches the size of the hint layer. Then, we train the FitNet parameters from the first layer up to the guided layer as well as the regressor parameters by minimizing the following loss function:\nL HT (W Guided , W r ) = 1 2 ||u h (x; W Hint ) \u2212 r(v g (x; W Guided ); W r )|| 2 ,(3)\nwhere u h and v g are the teacher/student deep nested functions up to their respective hint/guided layers with parameters W Hint and W Guided , r is the regressor function on top of the guided layer with parameters W r . Note that the outputs of u h and r have to be comparable, i.e., u h and r must be the same non-linearity.\n\nNevertheless, using a fully-connected regressor increases the number of parameters and the memory consumption dramatically in the case where the guided and hint layers are convolutional. Let N h,1 \u00d7 N h,2 and O h be the teacher hint's spatial size and number of channels, respectively. Similarity, let N g,1 \u00d7 N g,2 and O g be the FitNet guided layer's spatial size and number of channels. The number of parameters in the weight matrix of a fully connected regressor is N h,1 \u00d7N h,2 \u00d7O h \u00d7N g,1 \u00d7N g,2 \u00d7O g .\n\nTo mitigate this limitation, we use a convolutional regressor instead. The convolutional regressor is designed such that it considers approximately the same spatial region of the input image as the teacher hint. Therefore, the output of the regressor has the same spatial size as the teacher hint. Given a teacher hint of spatial size N h,1 \u00d7 N h,2 , the regressor takes the output of the Fitnet's guided  Figure 1: Training a student network using hints.\n\nlayer of size N g,1 \u00d7 N g,2 and adapts its kernel shape\nk 1 \u00d7 k 2 such that N g,i \u2212 k i + 1 = N h,i , where i \u2208 {1, 2}.\nThe number of parameters in the weight matrix of a the convolutional regressor is\nk 1 \u00d7 k 2 \u00d7 O h \u00d7 O g , where k 1 \u00d7 k 2 is significantly lower than N h,1 \u00d7 N h,2 \u00d7 N g,1 \u00d7 N g,2 .\n\nFITNET STAGE-WISE TRAINING\n\nWe train the FitNet in a stage-wise fashion following the student/teacher paradigm. Figure 1 summarizes the training pipeline. Starting from a trained teacher network and a randomly initialized FitNet ( Fig. 1 (a)), we add a regressor parameterized by W r on top of the FitNet guided layer and train the FitNet parameters W Guided up to the guided layer to minimize Eq. (3) (see Fig. 1 \n(b)).\nFinally, from the pre-trained parameters, we train the parameters of whole FitNet W S to minimize Eq.\n\n(2) (see Fig. 1 (c)). Algorithm 1 details the FitNet training process.\n\nAlgorithm 1 FitNet Stage-Wise Training. The algorithm receives as input the trained parameters W T of a teacher, the randomly initialized parameters W S of a FitNet, and two indices h and g corresponding to hint/guided layers, respectively. Let W Hint be the teacher's parameters up to the hint layer h. Let W Guided be the FitNet's parameters up to the guided layer g. Let W r be the regressor's parameters. The first stage consists in pre-training the student network up to the guided layer, based on the prediction error of the teacher's hint layer (line 4). The second stage is a KD training of the whole network (line 6).\nInput: WS, WT, g, h Output: W * S 1: W Hint \u2190 {WT 1 , . . . , WT h } 2: W Guided \u2190 {WS 1 , . . . , WS g } 3: Intialize Wr to small random values 4: W * Guided \u2190 argmin W Guided LHT (W Guided , Wr) 5: {WS 1 , . . . , WS g } \u2190 {W Guided * 1 , . . . , W Guided * g } 6: W * S \u2190 argmin W S LKD(WS)\n\nRELATION TO CURRICULUM LEARNING\n\nIn this section, we argue that our hint-based training with KD can be seen as a particular form of Curriculum Learning (Bengio, 2009). Curriculum learning has proven to accelerate the training convergence as well as potentially improve the model generalization by properly choosing a sequence of training distributions seen by the learner: from simple examples to more complex ones. A curriculum learning extension (Gulcehre & Bengio, 2013) has also shown that by using guidance hints on an intermediate layer during the training, one could considerably ease training. However, Bengio (2009) uses hand-defined heuristics to measure the \"simplicity\" of an example in a sequence and Gulcehre & Bengio (2013)'s guidance hints require some prior knowledge of the end-task. Both of these curriculum learning strategies tend to be problem-specific.\n\nOur approach alleviates this issue by using a teacher model. Indeed, intermediate representations learned by the teacher are used as hints to guide the FitNet optimization procedure. In addition, the teacher confidence provides a measure of example \"simplicity\" by means of teacher cross-entropy term in Eq.\n\n(2). This term ensures that examples with a high teacher confidence have a stronger impact than examples with low teacher confidence: the latter correspond to probabilities closer to the uniform distribution, which exert less of a push on the student parameters. In other words, the teacher penalizes the training examples according to its confidence. Note that parameter \u03bb in Eq.\n\n(2) controls the weight given to the teacher cross-entropy, and thus, the importance given to each example. In order to promote the learning of more complex examples (examples with lower teacher confidence), we gradually anneal \u03bb during the training with a linear decay. The curriculum can be seen as composed of two stages: first learn intermediate concepts via the hint/guided layer transfer, then train the whole student network jointly, annealing \u03bb, which allows easier examples (on which the teacher is very confident) to initially have a stronger effect, but progressively decreasing their importance as \u03bb decays. Therefore, the hint-based training introduced in the paper is a generic curriculum learning approach, where prior information about the task-at-hand is deduced purely from the teacher model.   3.1 CIFAR-10 AND CIFAR-100\n\nThe CIFAR-10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009) are composed of 32x32 pixel RGB images belonging to 10 and 100 different classes, respectively. They both contain 50K training images and 10K test images. CIFAR-10 has 1000 samples per class, whereas CIFAR-100 has 100 samples per class. Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening.\n\n\nCIFAR-10:\n\nTo validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 maxout convolutional layers, followed by a maxout fully-connected layer and a top softmax layer, with roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 1 summarizes the obtained results. Our student model outperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is crucial to achieve better representations. When compared to network compression methods, our algorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%, which is significantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requiring roughly 28 times fewer parameters. When compared to state-of-the-art methods, our algorithm matches the best performers.\n\nOne could argue the choice of hinting the inner layers with the hidden state of a wide teacher network. A straightforward alternative would be to hint them with the desired output. This could be addressed in a few different ways: (1) Stage-wise training, where stage 1 optimizes the 1st half of the network w.r.t. classification targets and stage 2 optimizes the whole network w.r.t. classification targets. In this case, stage 1 set the network parameters in a good local minima but such initialization did not seem to help stage 2 sufficiently, which failed to learn. To further assist the training of the thin and deep student network, we could add extra hints with the desired output at different hidden layers. Nevertheless, as observed in , with supervised pre-training the guided layer may discard some factors from the input, which require more layers and non-linearity before they can be exploited to predict the classes.\n\n(2) Stage-wise training with KD, where stage 1 optimizes the 1st half of the net w.r.t. classification targets and stage 2 optimizes the whole network w.r.t. Eq. (2). As in the previous case, stage 1 set the network parameters in a good local minima but such initialization did not seem to help stage 2 sufficiently, which failed to learn.\n\n(3) Jointly optimizing both stages w.r.t. the sum of the supervised hint for the guided layer and classification target for the output layer. We performed this experiment, tried different initializations and learning rates with RMSprop (Tieleman & Hinton, 2012) but we could not find any combination to make the network learn. Note that we could ease the training by adding hints to each layer and optimizing jointly as in Deeply Supervised Networks (DSN). Therefore, we built the above-mentioned 19-layer architecture and trained it by means of DSN, achieving a test performance of 88.2%, which is significantly lower than the performance obtained by the FitNets hint-based training (91.61%). Such result suggests that using a very discriminative hint w.r.t. classification at intermediate layers might be too aggressive; using a smoother hint (such as the guidance from a teacher network) offers better generalization. (4) Jointly optimizing both stages w.r.t. the sum of supervised hint for the guided layer and Eq.\n\n(2) for the output layer. Adding supervised hints to the middle layer of the network did not ease the training of such a thin and deep network, which failed to learn.\n\n\nCIFAR-100:\n\nTo validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and used the same FitNet architecture as in CIFAR-10. As in Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 2 summarizes the obtained results. As in the previous case, our FitNet outperforms the teacher model, reducing the number of parameters by a factor of 3 and, when compared to state-of-the-art methods, the FitNet provides near state-of-the-art performance.\n\n\nSVHN\n\nThe SVHN dataset (Netzer et al., 2011) is composed by 32 \u00d7 32 color images of house numbers collected by GoogleStreet View. There are 73,257 images in the training set, 26,032 images in the test set and 531,131 less difficult examples. We follow the evaluation procedure of Goodfellow et al. (2013b) and use their maxout network as teacher. We trained a 13-layer FitNet composed of 11 maxout convolutional layers, a fully-connected layer and a softmax layer.    Table 3 shows that our FitNet achieves comparable accuracy than the teacher despite using only 32% of teacher capacity. Our FitNet is comparable in terms of performance to other state-of-art methods, such as Maxout and Network in Network.\n\n\nMNIST\n\nAs a sanity check for the training procedure, we evaluated the proposed method on the MNIST dataset (LeCun et al., 1998). MNIST is a dataset of handwritten digits (from 0 to 9) composed of 28x28 pixel greyscale images, with 60K training images and 10K test images. We trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters. The 4th layer of the student network was trained to mimic the 2nd layer of the teacher network. Table 4 reports the obtained results. To verify the influence of using hints, we trained the FitNet architecture using either (1) standard backprop (w.r.t. classification labels), (2) KD or (3) Hintbased Training (HT). When training the FitNet with standard backprop from the softmax layer, the deep and thin architecture achieves 1.9% misclassification error. Using KD, the very same network achieves 0.65%, which confirms the potential of the teacher network; and when adding hints, the error still decreases to 0.51%. Furthermore, the student network achieves slightly better results than the teacher network, while requiring 12 times less parameters.\n\n\nAFLW\n\nAFLW (Koestinger et al., 2011) is a real-world face database, containing 25K annotated images. In order to evaluate the proposed framework in a face recognition setting, we extracted positive samples by re-sizing the annotated regions of the images to fit 16x16 pixels patches. Similarly, we extracted 25K 16x16 pixels patches not containing faces from ImageNet (Russakovsky et al., 2014) dataset, as negative samples. We used 90% of the extracted patches to train the network.\n\nIn this experiment, we aimed to evaluate the method on a different kind of architecture. Therefore, we trained a teacher network of 3 ReLU convolutional layers and a sigmoid output layer. We designed a first FitNet (FitNet 1) with 15 times fewer multiplications than the teacher network, and a second FitNet (FitNet 2) with 2.5 times fewer multiplications than the teacher network. Both FitNets have 7 ReLU convolutional layers and a sigmoid output layer.\n\nThe teacher network achieved 4.21% misclassification error on the validation set. We trained both FitNets by means of KD and HT. On the one hand, we report a misclassification error of 4.58% when training FitNet 1 with KD and a misclassification error of when 2.55% when training it with HT. On the other hand, we report a missclassifation error of 1.95% when training FitNet 2 with KD and a misclassification error of 1.85% when training it with HT. These results show how the method is extensible to different kind of architectures and highlight the benefits of using hints, especially when dealing with thinner architectures.\n\n\nANALYSIS OF EMPIRICAL RESULTS\n\nWe empirically investigate the benefits of our approach by comparing various networks trained using standard backpropagation (cross-entropy w.r.t. labels), KD or Hint-based Training (HT). Experiments are performed on CIFAR-10 dataset (Krizhevsky & Hinton, 2009).\n\nWe compare networks of increasing depth given a fixed computational budget. Each network is composed of successive convolutional layers of kernel size 3 \u00d7 3, followed by a maxout non-linearity and a non-overlapping 2 \u00d7 2 max-pooling. The last max-pooling takes the maximum over all remaining spatial dimensions leading to a 1 \u00d7 1 vector representation. We only change the depth and the number of channels per convolution between different networks, i.e. the number of channels per convolutional layer decreases as a network depth increases to respect a given computational budget.\n\n\nASSISTING THE TRAINING OF DEEP NETWORKS\n\nIn this section, we investigate the impact of HT. We consider two computational budgets of approximately 30M and 107M operations, corresponding to the multiplications needed in an image forward propagation. For each computational budget, we train networks composed of 3, 5, 7 and 9 convolutional layers, followed by a fully-connected layer and a softmax layer. We compare their performances when they are trained with standard backpropagation, KD and HT. Figure 2 reports test on CIFAR-10 using early stopping on the validation set, i.e. we do not retrain our models on the training plus validation sets.\n\nDue to their depth and small capacity, FitNets are hard to train. As shown in Figure 2(a), we could not train 30M multiplications networks with more than 5 layers with standard backprop. When using KD, we succesfully trained networks up to 7 layers. Adding KD's teacher cross-entropy to the training objective (Eq. (2)) gives more importance to easier examples, i.e. samples for which the teacher network is confident and, can lead to a smoother version of the training cost (Bengio, 2009). Despite some optimization benefits, it is worth noticing that KD training still suffers from   the increasing depth and reaches its limits for 7-layer networks. HT tends to ease these optimization issues and is able to train 13-layer networks of 30M multiplications. The only difference between HT and KD is the starting point in the parameter space: either random or obtained by means of the teacher's hint. On the one hand, the proliferation of local minima and especially saddle points in highly non-linear functions such as very deep networks highlights the difficulty of finding a good starting point in the parameter space at random (Dauphin et al., 2014). On the other hand, results in Figure 2(a) indicate that HT can guide the student to a better initial position in the parameter space, from which we can minimize the cost through stochastic gradient descent. Therefore, HT provides benefits from an optimization point of view. Networks trained with HT also tend to yield better test performances than the other training methods when we fix the capacity and number of layers. For instance, in Figure 2(b), the 7-layers network, trained with hints, obtains a +0.7% performance gain on the test set compared to the model that does not use any hints (the accuracy increases from 89.45% to 90.1%). As pointed by Erhan et al. (2009), pre-training strategies can act as regularizers. These results suggest that HT is a stronger regularizer than KD, since it leads to better generalization performance on the test set. Finally, Figure 2 highlights that deep models have better performances than shallower ones given a fixed computational budget. Indeed, considering networks that are trained with hints, an 11-layer network outperforms a 5-layer network by an absolute improvement of 4.11% for 107M multiplications and of 3.4% for 30M multiplications. Therefore, the experiments validate our hypothesis that given a fixed number of computations, we leverage depth in a model to achieve faster computation and better generalization.\n\nIn summary, this experiment shows that (1) using HT, we are able to train deeper models than with standard back-propagation and KD; and (2) given a fixed capacity, deeper models performed better than shallower ones.\n\n\nTRADE-OFF BETWEEN MODEL PERFORMANCE AND EFFICIENCY\n\nTo evaluate FitNets efficiency, we measure their total inference times required for processing CIFAR-10 test examples on a GPU as well as their parameter compression. Table 5 reports both the speed-up and compression rate obtained by various FitNets w.r.t. the teacher model along with their number of layers, capacity and accuracies. In this experiment, we retrain our FitNets on training plus validation sets as in Goodfellow et al. (2013b), for fair comparison with the teacher.\n\nFitNet 1, our smallest network, with 36\u00d7 less capacity than the teacher, is one order of magnitude faster than the teacher and only witnesses a minor performance decrease of 1.3%. FitNet 2, slightly increasing the capacity, outperforms the teacher by 0.9%, while still being faster by a strong 4.64 factor. By further increasing network capacity and depth in FitNets 3 and 4, we improve the performance gain, up to 1.6%, and still remain faster than the teacher. Although a trade-off between speed and accuracy is introduced by the compression rate, FitNets tend to be significantly faster, matching or outperforming their teacher, even when having low capacity.\n\nA few works such as matrix factorization (Jaderberg et al., 2014;Denton et al., 2014) focus on speeding-up deep networks' convolutional layers at the expense of slightly deteriorating their performance. Such approaches are complementary to FitNets and could be used to further speed-up the FitNet's convolutional layers.\n\nOther works related to quantization schemes (Chen et al., 2010;J\u00e9gou et al., 2011;Gong et al., 2014) aim at reducing storage requirements. Unlike FitNets, such approaches witness a little decrease in performance when compressing the network parameters. Exploiting depth allows FitNets to obtain performance improvements w.r.t. their teachers, even when reducing the number of parameters 10\u00d7. However, we believe that quantization approaches are also complementary to FitNets and could be used to further reduce the storage requirements. It would be interesting to compare how much redundancy is present in the filters of the teacher networks w.r.t. the filters of the FitNet and, therefore, how much FitNets filters could be compressed without witnessing significant performance drop. This analysis is out of the scope of the paper and is left as future work.\n\n\nCONCLUSION\n\nWe proposed a novel framework to compress wide and deep networks into thin and deeper ones, by introducing intermediate-level hints from the teacher hidden layers to guide the training process of the student. We are able to use these hints to train very deep student models with less parameters, which can generalize better and/or run faster than their teachers. We provided empirical evidence that hinting the inner layers of a thin and deep network with the hidden state of a teacher network generalizes better than hinting them with the classification targets. Our experiments on benchmark datasets emphasize that deep networks with low capacity are able to extract feature representations that are comparable or even better than networks with as much as 10 times more parameters. The hint-based training suggests that more efforts should be devoted to explore new training strategies to leverage the power of deep networks.\n\n\nA SUPPLEMENTARY MATERIAL: NETWORK ARCHITECTURES AND TRAINING PROCEDURES\n\nIn the supplementary material, we describe all network architectures and hyper-parameters used throughout the paper.\n\nA.1 CIFAR-10/CIFAR-100\n\nIn this section, we describe the teacher and FitNet architectures as well as hyper-parameters used in both CIFAR-10/CIFAR-100 experiments.\n\n\nA.1.1 TEACHERS\n\nWe used the CIFAR-10/CIFAR-100 maxout convolutional networks reported in Goodfellow et al. (2013b) as teachers. Both teachers have the same architecture, composed of 3 convolutional hidden layers of 96-192-192 units, respectively. Each convolutional layer is followed by a maxout nonlinearity (with 2 linear pieces) and a max-pooling operator with respective windows sizes of 4x4, 4x4 and 2x2 pixels. All max-pooling units have an overlap of 2x2 pixels. The third convolutional layer is followed by a fully-connected maxout layer of 500 units (with 5 linear pieces) and a top softmax layer. The CIFAR-10/CIFAR-100 teachers are trained using stochastic gradient descent and momentum. Please refer to Goodfellow et al. (2013b) for more details.\n\n\nA.1.2 FITNETS\n\nHere, we describe the FitNet architectures used in the Section 3 and Section 4. Each FitNet is composed of successive zero-padded convolutional layers of kernel size 3 \u00d7 3, followed by a maxout non-linearity with two linear pieces. A non-overlapping 2 \u00d7 2 max-pooling follows some of the convolutional layers; each network has a total of 3 max-pooling units. The last max-pooling takes the maximum over all remaining spatial dimensions, leading to a 1 \u00d7 1 vector representation. The last convolutional layer is followed by a fully-connected and a softmax layer, as the ones on CIFAR-10/100 teachers. Table 6 describes the architectures used for the depth experiment in Figure 2. Table 7 describes the architectures for the efficiency-performance trade-off experiment in Table 5. The results reported in Table 1, Table 2 and Table 3 correspond to the FitNet 4 architecture.\n\nAll FitNet parameters were initialized randomly in U(-0.005,0.005). We used stochastic gradient descent with RMSProp (Tieleman & Hinton, 2012) to train the FitNets, with an initial learning rate 0.005 and a mini-batch size of 128. Parameter \u03bb in Eq.\n\n(2) was initialized to 4 and decayed linearly during 500 epochs reaching \u03bb = 1. The relaxation term \u03c4 was set to 3.\n\nOn CIFAR-10, we divided the training set into 40K training examples and 10K validation examples. We trained stage 1 by minimizing Eq. (3) and stopped the training after 100 epochs of no validation error improvement, performing a maximum of 500 epochs. After that, we trained stage 2 by minimizing Eq.\n\n(2) using RMSprop, the same stopping criterion and the same hyper-parameters as stage 1. We picked the optimal number of epochs according to the above-mentioned stopping criterion and retrained the FitNet on the whole 50K training examples (training + validation sets).\n\nOn CIFAR-100, we trained directly on the whole training set using stochastic gradient descent with RMSprop, the same hyper-parameters as CIFAR-10 FitNets and the number of epochs determined by CIFAR-10 stopping criterion.\n\n\nA.2 MNIST\n\nIn this section, we describe the teacher and FitNet architectures as well as the hyper-parameters used in the MNIST experiments.\n\nWe trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b). The teacher architecture has three convolutional maxout hidden layers (with 2 linear pieces each) of 48-48-24 units, respectively, followed by a spatial max-pooling of 4x4-4x4-2x2 pixels, with   an overlap of 2x2 pixels. The 3rd hidden layer is followed by a fully-connected softmax layer. As is Goodfellow et al. (2013b), we added zero padding to the second convolutional layer.\n\nWe designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters. The student architecture has 6 maxout convolutional hidden layers (with 2 linear pieces each) of 16-16-16-16-12-12 units, respectively. Max-pooling is only applied every second layer in regions of 4x4-4x4-2x2 pixels, with an overlap of 2x2 pixels. The 6th convolutional hidden layer is followed by a fully-connected softmax layer.\n\nThe teacher network was trained as described in Goodfellow et al. (2013b). The FitNet was trained in a stage-wise fashion as described in Section 2. We divided the training set into a training set of 50K samples and a validation set of 10K samples.\n\nAll network parameters where initialized randomly in U(-0.005,0.005). In the first stage, the 4th layer of the FitNet was trained to mimic the 2nd layer of the teacher network, by minimizing Eq.\n\n(3) through stochastic gradient descent. We used a mini-batch size of 128 samples and fixed the learning rate to 0.0005. We initialized \u03bb to 4 and decayed it for the first 150 epochs until it reached 1. The training was stopped according to the following criterion: after 100 epochs of no validation error improvement and performning a maximum of 500 epochs. We used the same mini-batch size, learning rate and stopping criterion to train the second stage. The relaxation term \u03c4 was set to 3.\n\n\nA.3 SVHN\n\nIn this section, we describe the teacher and FitNet architectures as well as the hyper-parameters used in the SVHN experiments.\n\nWe used SVHN maxout convolutional network described in as Goodfellow et al. (2013b) teacher. The network is composed of 3 convolutional hidden layers of 64-128-128 units, respectively, followed by a fully-connected maxout layer of 400 units and a top softmax layer. The teacher training was carried out as in Goodfellow et al. (2013b).\n\nWe used the FitNet 4 architecture outlined in Table 7, initializing the network parameters randomly in U(-0.005,0.005) and training with the same hyper-parameters as in CIFAR-10. In this case, we used the same early stopping as in CIFAR-10, but we did not retrain the FitNet on the whole training set (training + validation). The same hyper-parameters where used for both stages.\n\n\nA.4 AFLW\n\nIn this section, we describe the teacher and FitNet architectures as well as the hyper-parameters used in the AFLW experiments.\n\nWe trained a teacher network of 3 ReLU convolutional layers of 128-512-512 units, respectively, followed by a sigmoid layer. Non-overlapping max-pooling of size 2 \u00d7 2 was performed after the first convolutional layer. We used receptive fields of 3-2-5 for each layer, respectively.\n\nWe designed two FitNets of 7 ReLU convolutional layers. Fitnet 1's layers have 16-32-32-32-32-32-32-32 units, respectively, followed by a sigmoid layer. Fitnet 2's layers have 32-64-64-64-64-64-64-64 units, respectively, followed by a sigmoid layer. In both cases, we used receptive fields of 3 \u00d7 3 and, due to the really small image resolution, we did not perform any max-pooling.\n\nAll network parameters of both FitNets where initialized randomly in U(-0.05,0.05). Both FitNets were trained in the stage-wise fashion described in Section 2. We used 90% of the data for training. In the first stage, the 5th layer of the FitNets were trained to mimic the 3rd layer of the teacher network, by minimizing Eq. (3) through stochastic gradient descent. We used a mini-batch size of 128 samples and initialized the learning rate to 0.001 and decayed it for the first 100 epochs until reaching 0.01. We also used momentum. We initialized momentum to 0.1 and saturated it to 0.9 at epoch 100. We picked the best validation value after a 500 epochs. We used the same mini-batch size, learning rate and stopping criterion to train the second stage. The relaxation term \u03c4 was set to 3.\n\nFigure 2 :\n2Comparison of Standard Back-Propagation, Knowledge Distillation and Hint-based Training on CIFAR-10.\n\nTable 1 :\n1Accuracy on CIFAR-10Algorithm \n# params \nAccuracy \nCompression \nFitNet \n\u223c2.5M \n64.96% \nTeacher \n\u223c9M \n63.54% \nState-of-the-art methods \nMaxout \n61.43% \nNetwork in Network \n64.32% \nDeeply-Supervised Networks \n65.43% \n\n\n\nTable 2 :\n2Accuracy on CIFAR-100 3 RESULTS ON BENCHMARK DATASETS In this section, we show the results on several benchmark datasets 2 . The architectures of all networks as well as the training details are reported in the supplementary material.\n\nTable 3 :\n3SVHN error    Algorithm \n# params Misclass \nCompression \nTeacher \n\u223c361K \n0.55% \nStandard backprop \n\u223c30K \n1.9% \nKD \n\u223c30K \n0.65% \nFitNet \n\u223c30K \n0.51% \nState-of-the-art methods \nMaxout \n0.45% \nNetwork in Network \n0.47% \nDeeply-Supervised Networks \n0.39% \n\n\n\nTable 4 :\n4MNIST error\n\nTable 5 :\n5Accuracy/Speed Trade-off on CIFAR-10.\n\nTable 6 :\n6Fitnet architectures with a computational budget of 30M (or 107M) of multiplications: conv s x \u00d7 s y \u00d7 c is a convolution of kernel size s x \u00d7 s y with c outputs channels; pool s x \u00d7 s y is a non-overlapping pooling of size s x \u00d7 s y ; fc stands for fully connected. hint: FitNet \u2190 teacher specifies the hint and guided layers used for hint-based training, respectively.FitNet 1 \nFitNet 2 \nFitNet 3 \nFitNet 4 \nconv 3x3x16 conv 3x3x16 \nconv 3x3x32 \nconv 3x3x32 \nconv 3x3x16 conv 3x3x32 \nconv 3x3x48 \nconv 3x3x32 \nconv 3x3x16 conv 3x3x32 \nconv 3x3x64 \nconv 3x3x32 \npool 2x2 \npool 2x2 \nconv 3x3x64 \nconv 3x3x48 \npool 2x2 \nconv 3x3x48 \npool 2x2 \nconv 3x3x32 conv 3x3x48 \nconv 3x3x80 \nconv 3x3x80 \nconv 3x3x32 conv 3x3x64 \nconv 3x3x80 \nconv 3x3x80 \nconv 3x3x32 conv 3x3x80 \nconv 3x3x80 \nconv 3x3x80 \npool 2x2 \npool 2x2 \nconv 3x3x80 \nconv 3x3x80 \npool 2x2 \nconv 3x3x80 \nconv 3x3x80 \npool 2x2 \nconv 3x3x48 conv 3x3x96 conv 3x3x128 conv 3x3x128 \nconv 3x3x48 conv 3x3x96 conv 3x3x128 conv 3x3x128 \nconv 3x3x64 conv 3x3x128 conv 3x3x128 conv 3x3x128 \npool 8x8 \npool 8x8 \npool 8x8 \nconv 3x3x128 \nconv 3x3x128 \nconv 3x3x128 \npool 8x8 \nfc \nfc \nfc \nfc \nsoftmax \nsoftmax \nsoftmax \nsoftmax \nhint: 6\u21902 \nhint: 6\u21902 \nhint: 8\u21902 \nhint: 11\u21902 \n\n\n\nTable 7 :\n7Performance-Efficiency FitNet architectures.\nFor example, as argued byHinton & Dean (2014), with softened outputs, more information is provided about the relative similarity of the input to classes other than the one with the highest probability.\nCode to reproduce the experiments publicly available: https://github.com/adri-romsor/FitNets\nACKNOWLEDGMENTSWe thank the developers of Theano(Bastien et al., 2012)and Pylearn2(Goodfellow et al., 2013a)and the computational resources provided by Compute Canada and Calcul Qu\u00e9bec. This work has been partially supported by NSERC, CIFAR, and Canada Research Chairs, Project TIN2013-41751, grant 2014-SGR-221 and Spanish MINECO grant TIN2012-38187-C03.\nDo deep nets really need to be deep? In NIPS. J Ba, R Caruana, Ba, J. and Caruana, R. Do deep nets really need to be deep? In NIPS, pp. 2654-2662. 2014.\n\nF Bastien, P Lamblin, R Pascanu, J Bergstra, I Goodfellow, A Bergeron, N Bouchard, D Warde-Farley, Y Bengio, Theano: new features and speed improvements. Deep Learning & Unsupervised Feature Learning Workshop, NIPS. Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley, D., and Bengio, Y. Theano: new features and speed improvements. Deep Learning & Unsupervised Feature Learning Workshop, NIPS, 2012.\n\nLearning deep architectures for AI. Foundations and trends in Machine Learning. Y Bengio, Bengio, Y. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2009.\n\nGreedy layer-wise training of deep networks. Y Bengio, P Lamblin, D Popovici, H Larochelle, NIPS. Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. Greedy layer-wise training of deep networks. In NIPS, pp. 153-160, 2007.\n\nRepresentation learning: A review and new perspectives. Y Bengio, A Courville, P Vincent, TPAMIBengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. TPAMI, 2013.\n\nModel compression. C Bucila, R Caruana, A Niculescu-Mizil, KDD. Bucila, C., Caruana, R., and Niculescu-Mizil, A. Model compression. In KDD, pp. 535-541, 2006.\n\nApproximate nearest neighbor search by residual vector quantization. Yongjian Chen, Tao Guan, Cheng Wang, Sensors. 1012Chen, Yongjian, Guan, Tao, and Wang, Cheng. Approximate nearest neighbor search by residual vector quan- tization. Sensors, 10(12):11259-11273, 2010.\n\n. L Chen-Yu, X Saining, G Patrick, Z Zhengyou, Zhuowen , abs/1409.5185Chen-Yu, L., Saining, X., Patrick, G., Zhengyou, Z., and Zhuowen, T. Deeply-supervised nets. CoRR, abs/1409.5185, 2014.\n\nA two-stage pretraining algorithm for deep Boltzmann machines. Kyunghyun Cho, Raiko, Tapani, Alexander Ilin, Juha Karhunen, NIPS Workshop on Deep Learning and Unsupervised Feature Learning. Cho, Kyunghyun, Raiko, Tapani, Ilin, Alexander, and Karhunen, Juha. A two-stage pretraining algorithm for deep Boltzmann machines. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2012.\n\nIdentifying and attacking the saddle point problem in high-dimensional non-convex optimization. Y Dauphin, R Pascanu, C Gulcehre, K Cho, S Ganguli, Y Bengio, NIPS. Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, 2014.\n\nExploiting linear structure within convolutional networks for efficient evaluation. Emily L Denton, Zaremba, Wojciech, Joan Bruna, Yann Lecun, Rob Fergus, NIPS. Denton, Emily L, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann, and Fergus, Rob. Exploiting linear structure within convolutional networks for efficient evaluation. In NIPS, pp. 1269-1277. 2014.\n\nThe difficulty of training deep architectures and the effect of unsupervised pre-training. D Erhan, P A Manzagol, Y Bengio, S Bengio, P Vincent, AISTATS. Erhan, D., Manzagol, P.A., Bengio, Y., Bengio, S., and Vincent, P. The difficulty of training deep architectures and the effect of unsupervised pre-training. In AISTATS, pp. 153-160, 2009.\n\nCompressing deep convolutional networks using vector quantization. Yunchao Gong, Liu, Yang Liu, Min Bourdev, Lubomir , abs/1412.6115CoRRGong, Yunchao, Liu, Liu, Yang, Min, and Bourdev, Lubomir. Compressing deep convolutional networks using vector quantization. CoRR, abs/1412.6115, 2014.\n\nI J Goodfellow, D Warde-Farley, P Lamblin, V Dumoulin, M Mirza, R Pascanu, J Bergstra, F Bastien, Y Bengio, arXiv:1308.4214Pylearn2: a machine learning research library. arXiv preprintGoodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013a.\n\nI J Goodfellow, D Warde-Farley, M Mirza, A Courville, Y Bengio, Maxout, Networks, ICML. Goodfellow, I.J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. Maxout networks. In ICML, 2013b.\n\nKnowledge matters: Importance of prior information for optimization. C Gulcehre, Y Bengio, ICLR. Gulcehre, C. and Bengio, Y. Knowledge matters: Importance of prior information for optimization. In ICLR, 2013.\n\nA fast learning algorithm for deep belief nets. G E Hinton, S Osindero, Y.-W Teh, Neural Computation. 187Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527-1554, 2006.\n\nDistilling knowledge in a neural network. G Hinton, O Vinyals, J Dean, Deep Learning and Representation Learning Workshop, NIPS. Hinton, G. Vinyals, O. and Dean, J. Distilling knowledge in a neural network. In Deep Learning and Repre- sentation Learning Workshop, NIPS, 2014.\n\nSpeeding up convolutional neural networks with low rank expansions. M Jaderberg, A Vedaldi, A Zisserman, BMVC. Jaderberg, M., Vedaldi, A., and Zisserman, A. Speeding up convolutional neural networks with low rank expansions. In BMVC, 2014.\n\nProduct quantization for nearest neighbor search. J\u00e9gou, Herv\u00e9, Matthijs Douze, Cordelia Schmid, IEEE TPAMI. 331J\u00e9gou, Herv\u00e9, Douze, Matthijs, and Schmid, Cordelia. Product quantization for nearest neighbor search. IEEE TPAMI, 33(1):117-128, 2011.\n\nAnnotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization. M Koestinger, P Wohlhart, P M Roth, H Bischof, First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies. Koestinger, M., Wohlhart, P., Roth, P.M., and Bischof, H. Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization. In First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies, 2011.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, Department of Computer Science, University of TorontoMaster's thesisKrizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Master's thesis, Depart- ment of Computer Science, University of Toronto, 2009.\n\nAn empirical evaluation of deep architectures on problems with many factors of variation. H Larochelle, D Erhan, A Courville, J Bergstra, Y Bengio, ICML. Larochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y. An empirical evaluation of deep architec- tures on problems with many factors of variation. In ICML, pp. 473-480, 2007.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, November 1998.\n\nOn the number of linear regions of deep neural networks. G F Montufar, R Pascanu, K Cho, Y Bengio, NIPS. Montufar, G.F., Pascanu, R., Cho, K., and Bengio, Y. On the number of linear regions of deep neural networks. In NIPS. 2014.\n\nReading digits in natural images with unsupervised feature learning. Y Netzer, T Wang, A Coates, A Bissacco, B Wu, A Ng, Deep Learning & Unsupervised Feature Learning Workshop, NIPS. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Reading digits in natural images with unsupervised feature learning. In Deep Learning & Unsupervised Feature Learning Workshop, NIPS, 2011.\n\n. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, ImageNet Large Scale Visual Recognition ChallengeRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge, 2014.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, abs/1409.1556CoRRSimonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.\n\n. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D A Erhan, D Vanhoucke, V Rabinovich, A , Going deeper with convolutions. CoRR, abs/1409.4842Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., D.A., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.\n\nLecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude. T Tieleman, G Hinton, COURSERA: Neural Networks for Machine Learning. Tieleman, T. and Hinton, G. Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.\n\nDeep learning via semi-supervised embedding. J Weston, F Ratle, R Collobert, ICML. Weston, J., Ratle, F., and Collobert, R. Deep learning via semi-supervised embedding. In ICML, 2008.\n", "annotations": {"author": "[{\"end\":104,\"start\":48},{\"end\":165,\"start\":105},{\"end\":188,\"start\":166},{\"end\":251,\"start\":189},{\"end\":312,\"start\":252},{\"end\":449,\"start\":313}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":56},{\"end\":119,\"start\":113},{\"end\":187,\"start\":182},{\"end\":205,\"start\":197},{\"end\":263,\"start\":258},{\"end\":326,\"start\":320}]", "author_first_name": "[{\"end\":55,\"start\":48},{\"end\":112,\"start\":105},{\"end\":172,\"start\":166},{\"end\":181,\"start\":173},{\"end\":196,\"start\":189},{\"end\":257,\"start\":252},{\"end\":319,\"start\":313}]", "author_affiliation": "[{\"end\":103,\"start\":64},{\"end\":164,\"start\":121},{\"end\":250,\"start\":207},{\"end\":311,\"start\":265},{\"end\":371,\"start\":328},{\"end\":448,\"start\":373}]", "title": "[{\"end\":34,\"start\":1},{\"end\":483,\"start\":450}]", "venue": null, "abstract": "[{\"end\":1736,\"start\":540}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1922,\"start\":1894},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1943,\"start\":1922},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2623,\"start\":2603},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2983,\"start\":2964},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3277,\"start\":3256},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4326,\"start\":4303},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4480,\"start\":4452},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4507,\"start\":4485},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4600,\"start\":4581},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4880,\"start\":4859},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4900,\"start\":4880},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5201,\"start\":5180},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5327,\"start\":5309},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5502,\"start\":5480},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5523,\"start\":5502},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5547,\"start\":5523},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5889,\"start\":5875},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6619,\"start\":6598},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7922,\"start\":7901},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14978,\"start\":14964},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15285,\"start\":15260},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17283,\"start\":17257},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17552,\"start\":17527},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17774,\"start\":17749},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18068,\"start\":18043},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18091,\"start\":18070},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18582,\"start\":18563},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21322,\"start\":21297},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21404,\"start\":21383},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21774,\"start\":21753},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22035,\"start\":22010},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22566,\"start\":22546},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22811,\"start\":22786},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23698,\"start\":23674},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24057,\"start\":24031},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25527,\"start\":25501},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27250,\"start\":27236},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27913,\"start\":27891},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28589,\"start\":28570},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30000,\"start\":29975},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30770,\"start\":30746},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30790,\"start\":30770},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31090,\"start\":31071},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31109,\"start\":31090},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31127,\"start\":31109},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33301,\"start\":33276},{\"end\":33432,\"start\":33402},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33927,\"start\":33902},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34979,\"start\":34954},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36243,\"start\":36218},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36566,\"start\":36541},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37128,\"start\":37103},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38218,\"start\":38193},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38469,\"start\":38444},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":42718,\"start\":42698}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40565,\"start\":40452},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40794,\"start\":40566},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41041,\"start\":40795},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":41307,\"start\":41042},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41331,\"start\":41308},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":41381,\"start\":41332},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":42615,\"start\":41382},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":42672,\"start\":42616}]", "paragraph": "[{\"end\":2450,\"start\":1752},{\"end\":3958,\"start\":2452},{\"end\":4508,\"start\":3960},{\"end\":6292,\"start\":4510},{\"end\":7428,\"start\":6294},{\"end\":7770,\"start\":7439},{\"end\":8242,\"start\":7807},{\"end\":9360,\"start\":8244},{\"end\":9488,\"start\":9412},{\"end\":9674,\"start\":9545},{\"end\":9889,\"start\":9676},{\"end\":10275,\"start\":9891},{\"end\":11202,\"start\":10299},{\"end\":11608,\"start\":11204},{\"end\":12022,\"start\":11696},{\"end\":12532,\"start\":12024},{\"end\":12989,\"start\":12534},{\"end\":13046,\"start\":12991},{\"end\":13192,\"start\":13111},{\"end\":13708,\"start\":13322},{\"end\":13816,\"start\":13715},{\"end\":13888,\"start\":13818},{\"end\":14516,\"start\":13890},{\"end\":15687,\"start\":14845},{\"end\":15996,\"start\":15689},{\"end\":16378,\"start\":15998},{\"end\":17219,\"start\":16380},{\"end\":17634,\"start\":17221},{\"end\":18720,\"start\":17648},{\"end\":19652,\"start\":18722},{\"end\":19993,\"start\":19654},{\"end\":21013,\"start\":19995},{\"end\":21181,\"start\":21015},{\"end\":21727,\"start\":21196},{\"end\":22436,\"start\":21736},{\"end\":23660,\"start\":22446},{\"end\":24146,\"start\":23669},{\"end\":24603,\"start\":24148},{\"end\":25233,\"start\":24605},{\"end\":25529,\"start\":25267},{\"end\":26111,\"start\":25531},{\"end\":26759,\"start\":26155},{\"end\":29286,\"start\":26761},{\"end\":29503,\"start\":29288},{\"end\":30039,\"start\":29558},{\"end\":30703,\"start\":30041},{\"end\":31025,\"start\":30705},{\"end\":31886,\"start\":31027},{\"end\":32828,\"start\":31901},{\"end\":33020,\"start\":32904},{\"end\":33044,\"start\":33022},{\"end\":33184,\"start\":33046},{\"end\":33945,\"start\":33203},{\"end\":34835,\"start\":33963},{\"end\":35086,\"start\":34837},{\"end\":35203,\"start\":35088},{\"end\":35505,\"start\":35205},{\"end\":35776,\"start\":35507},{\"end\":35999,\"start\":35778},{\"end\":36141,\"start\":36013},{\"end\":36624,\"start\":36143},{\"end\":37053,\"start\":36626},{\"end\":37303,\"start\":37055},{\"end\":37499,\"start\":37305},{\"end\":37993,\"start\":37501},{\"end\":38133,\"start\":38006},{\"end\":38470,\"start\":38135},{\"end\":38851,\"start\":38472},{\"end\":38991,\"start\":38864},{\"end\":39274,\"start\":38993},{\"end\":39657,\"start\":39276},{\"end\":40451,\"start\":39659}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9411,\"start\":9361},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9544,\"start\":9489},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11695,\"start\":11609},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13110,\"start\":13047},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13292,\"start\":13193},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13714,\"start\":13709},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14810,\"start\":14517}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18160,\"start\":18153},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21473,\"start\":21466},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22205,\"start\":22198},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":23013,\"start\":23006},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":29732,\"start\":29725},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":34570,\"start\":34563},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":34649,\"start\":34642},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":34740,\"start\":34733},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":34773,\"start\":34766},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34794,\"start\":34775},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":38525,\"start\":38518}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1750,\"start\":1738},{\"attributes\":{\"n\":\"2\"},\"end\":7437,\"start\":7431},{\"attributes\":{\"n\":\"2.1\"},\"end\":7805,\"start\":7773},{\"attributes\":{\"n\":\"2.2\"},\"end\":10297,\"start\":10278},{\"attributes\":{\"n\":\"2.3\"},\"end\":13320,\"start\":13294},{\"attributes\":{\"n\":\"2.4\"},\"end\":14843,\"start\":14812},{\"end\":17646,\"start\":17637},{\"end\":21194,\"start\":21184},{\"attributes\":{\"n\":\"3.2\"},\"end\":21734,\"start\":21730},{\"attributes\":{\"n\":\"3.3\"},\"end\":22444,\"start\":22439},{\"attributes\":{\"n\":\"3.4\"},\"end\":23667,\"start\":23663},{\"attributes\":{\"n\":\"4\"},\"end\":25265,\"start\":25236},{\"attributes\":{\"n\":\"4.1\"},\"end\":26153,\"start\":26114},{\"attributes\":{\"n\":\"4.2\"},\"end\":29556,\"start\":29506},{\"attributes\":{\"n\":\"5\"},\"end\":31899,\"start\":31889},{\"end\":32902,\"start\":32831},{\"end\":33201,\"start\":33187},{\"end\":33961,\"start\":33948},{\"end\":36011,\"start\":36002},{\"end\":38004,\"start\":37996},{\"end\":38862,\"start\":38854},{\"end\":40463,\"start\":40453},{\"end\":40576,\"start\":40567},{\"end\":40805,\"start\":40796},{\"end\":41052,\"start\":41043},{\"end\":41318,\"start\":41309},{\"end\":41342,\"start\":41333},{\"end\":41392,\"start\":41383},{\"end\":42626,\"start\":42617}]", "table": "[{\"end\":40794,\"start\":40598},{\"end\":41307,\"start\":41068},{\"end\":42615,\"start\":41764}]", "figure_caption": "[{\"end\":40565,\"start\":40465},{\"end\":40598,\"start\":40578},{\"end\":41041,\"start\":40807},{\"end\":41068,\"start\":41054},{\"end\":41331,\"start\":41320},{\"end\":41381,\"start\":41344},{\"end\":41764,\"start\":41394},{\"end\":42672,\"start\":42628}]", "figure_ref": "[{\"end\":12948,\"start\":12940},{\"end\":13414,\"start\":13406},{\"end\":13535,\"start\":13525},{\"end\":13707,\"start\":13701},{\"end\":13833,\"start\":13827},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26618,\"start\":26610},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26847,\"start\":26839},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27953,\"start\":27945},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28363,\"start\":28355},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28791,\"start\":28783},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34640,\"start\":34632}]", "bib_author_first_name": "[{\"end\":43371,\"start\":43370},{\"end\":43377,\"start\":43376},{\"end\":43479,\"start\":43478},{\"end\":43490,\"start\":43489},{\"end\":43501,\"start\":43500},{\"end\":43512,\"start\":43511},{\"end\":43524,\"start\":43523},{\"end\":43538,\"start\":43537},{\"end\":43550,\"start\":43549},{\"end\":43562,\"start\":43561},{\"end\":43578,\"start\":43577},{\"end\":44019,\"start\":44018},{\"end\":44172,\"start\":44171},{\"end\":44182,\"start\":44181},{\"end\":44193,\"start\":44192},{\"end\":44205,\"start\":44204},{\"end\":44413,\"start\":44412},{\"end\":44423,\"start\":44422},{\"end\":44436,\"start\":44435},{\"end\":44584,\"start\":44583},{\"end\":44594,\"start\":44593},{\"end\":44605,\"start\":44604},{\"end\":44801,\"start\":44793},{\"end\":44811,\"start\":44808},{\"end\":44823,\"start\":44818},{\"end\":44997,\"start\":44996},{\"end\":45008,\"start\":45007},{\"end\":45019,\"start\":45018},{\"end\":45030,\"start\":45029},{\"end\":45048,\"start\":45041},{\"end\":45257,\"start\":45248},{\"end\":45287,\"start\":45278},{\"end\":45298,\"start\":45294},{\"end\":45679,\"start\":45678},{\"end\":45690,\"start\":45689},{\"end\":45701,\"start\":45700},{\"end\":45713,\"start\":45712},{\"end\":45720,\"start\":45719},{\"end\":45731,\"start\":45730},{\"end\":46024,\"start\":46019},{\"end\":46026,\"start\":46025},{\"end\":46058,\"start\":46054},{\"end\":46070,\"start\":46066},{\"end\":46081,\"start\":46078},{\"end\":46382,\"start\":46381},{\"end\":46391,\"start\":46390},{\"end\":46393,\"start\":46392},{\"end\":46405,\"start\":46404},{\"end\":46415,\"start\":46414},{\"end\":46425,\"start\":46424},{\"end\":46708,\"start\":46701},{\"end\":46724,\"start\":46720},{\"end\":46733,\"start\":46730},{\"end\":46750,\"start\":46743},{\"end\":46924,\"start\":46923},{\"end\":46926,\"start\":46925},{\"end\":46940,\"start\":46939},{\"end\":46956,\"start\":46955},{\"end\":46967,\"start\":46966},{\"end\":46979,\"start\":46978},{\"end\":46988,\"start\":46987},{\"end\":46999,\"start\":46998},{\"end\":47011,\"start\":47010},{\"end\":47022,\"start\":47021},{\"end\":47325,\"start\":47324},{\"end\":47327,\"start\":47326},{\"end\":47341,\"start\":47340},{\"end\":47357,\"start\":47356},{\"end\":47366,\"start\":47365},{\"end\":47379,\"start\":47378},{\"end\":47593,\"start\":47592},{\"end\":47605,\"start\":47604},{\"end\":47782,\"start\":47781},{\"end\":47784,\"start\":47783},{\"end\":47794,\"start\":47793},{\"end\":47809,\"start\":47805},{\"end\":48017,\"start\":48016},{\"end\":48027,\"start\":48026},{\"end\":48038,\"start\":48037},{\"end\":48320,\"start\":48319},{\"end\":48333,\"start\":48332},{\"end\":48344,\"start\":48343},{\"end\":48564,\"start\":48556},{\"end\":48580,\"start\":48572},{\"end\":48851,\"start\":48850},{\"end\":48865,\"start\":48864},{\"end\":48877,\"start\":48876},{\"end\":48879,\"start\":48878},{\"end\":48887,\"start\":48886},{\"end\":49302,\"start\":49301},{\"end\":49316,\"start\":49315},{\"end\":49650,\"start\":49649},{\"end\":49664,\"start\":49663},{\"end\":49673,\"start\":49672},{\"end\":49686,\"start\":49685},{\"end\":49698,\"start\":49697},{\"end\":49963,\"start\":49962},{\"end\":49972,\"start\":49971},{\"end\":49982,\"start\":49981},{\"end\":49992,\"start\":49991},{\"end\":50256,\"start\":50255},{\"end\":50258,\"start\":50257},{\"end\":50270,\"start\":50269},{\"end\":50281,\"start\":50280},{\"end\":50288,\"start\":50287},{\"end\":50499,\"start\":50498},{\"end\":50509,\"start\":50508},{\"end\":50517,\"start\":50516},{\"end\":50527,\"start\":50526},{\"end\":50539,\"start\":50538},{\"end\":50545,\"start\":50544},{\"end\":50823,\"start\":50822},{\"end\":50838,\"start\":50837},{\"end\":50846,\"start\":50845},{\"end\":50852,\"start\":50851},{\"end\":50862,\"start\":50861},{\"end\":50874,\"start\":50873},{\"end\":50880,\"start\":50879},{\"end\":50889,\"start\":50888},{\"end\":50901,\"start\":50900},{\"end\":50911,\"start\":50910},{\"end\":50924,\"start\":50923},{\"end\":50926,\"start\":50925},{\"end\":50934,\"start\":50933},{\"end\":51270,\"start\":51269},{\"end\":51282,\"start\":51281},{\"end\":51441,\"start\":51440},{\"end\":51452,\"start\":51451},{\"end\":51459,\"start\":51458},{\"end\":51466,\"start\":51465},{\"end\":51478,\"start\":51477},{\"end\":51486,\"start\":51485},{\"end\":51488,\"start\":51487},{\"end\":51497,\"start\":51496},{\"end\":51510,\"start\":51509},{\"end\":51524,\"start\":51523},{\"end\":51832,\"start\":51831},{\"end\":51844,\"start\":51843},{\"end\":52117,\"start\":52116},{\"end\":52127,\"start\":52126},{\"end\":52136,\"start\":52135}]", "bib_author_last_name": "[{\"end\":43374,\"start\":43372},{\"end\":43385,\"start\":43378},{\"end\":43487,\"start\":43480},{\"end\":43498,\"start\":43491},{\"end\":43509,\"start\":43502},{\"end\":43521,\"start\":43513},{\"end\":43535,\"start\":43525},{\"end\":43547,\"start\":43539},{\"end\":43559,\"start\":43551},{\"end\":43575,\"start\":43563},{\"end\":43585,\"start\":43579},{\"end\":44026,\"start\":44020},{\"end\":44179,\"start\":44173},{\"end\":44190,\"start\":44183},{\"end\":44202,\"start\":44194},{\"end\":44216,\"start\":44206},{\"end\":44420,\"start\":44414},{\"end\":44433,\"start\":44424},{\"end\":44444,\"start\":44437},{\"end\":44591,\"start\":44585},{\"end\":44602,\"start\":44595},{\"end\":44621,\"start\":44606},{\"end\":44806,\"start\":44802},{\"end\":44816,\"start\":44812},{\"end\":44828,\"start\":44824},{\"end\":45005,\"start\":44998},{\"end\":45016,\"start\":45009},{\"end\":45027,\"start\":45020},{\"end\":45039,\"start\":45031},{\"end\":45261,\"start\":45258},{\"end\":45268,\"start\":45263},{\"end\":45276,\"start\":45270},{\"end\":45292,\"start\":45288},{\"end\":45307,\"start\":45299},{\"end\":45687,\"start\":45680},{\"end\":45698,\"start\":45691},{\"end\":45710,\"start\":45702},{\"end\":45717,\"start\":45714},{\"end\":45728,\"start\":45721},{\"end\":45738,\"start\":45732},{\"end\":46033,\"start\":46027},{\"end\":46042,\"start\":46035},{\"end\":46052,\"start\":46044},{\"end\":46064,\"start\":46059},{\"end\":46076,\"start\":46071},{\"end\":46088,\"start\":46082},{\"end\":46388,\"start\":46383},{\"end\":46402,\"start\":46394},{\"end\":46412,\"start\":46406},{\"end\":46422,\"start\":46416},{\"end\":46433,\"start\":46426},{\"end\":46713,\"start\":46709},{\"end\":46718,\"start\":46715},{\"end\":46728,\"start\":46725},{\"end\":46741,\"start\":46734},{\"end\":46937,\"start\":46927},{\"end\":46953,\"start\":46941},{\"end\":46964,\"start\":46957},{\"end\":46976,\"start\":46968},{\"end\":46985,\"start\":46980},{\"end\":46996,\"start\":46989},{\"end\":47008,\"start\":47000},{\"end\":47019,\"start\":47012},{\"end\":47029,\"start\":47023},{\"end\":47338,\"start\":47328},{\"end\":47354,\"start\":47342},{\"end\":47363,\"start\":47358},{\"end\":47376,\"start\":47367},{\"end\":47386,\"start\":47380},{\"end\":47394,\"start\":47388},{\"end\":47404,\"start\":47396},{\"end\":47602,\"start\":47594},{\"end\":47612,\"start\":47606},{\"end\":47791,\"start\":47785},{\"end\":47803,\"start\":47795},{\"end\":47813,\"start\":47810},{\"end\":48024,\"start\":48018},{\"end\":48035,\"start\":48028},{\"end\":48043,\"start\":48039},{\"end\":48330,\"start\":48321},{\"end\":48341,\"start\":48334},{\"end\":48354,\"start\":48345},{\"end\":48547,\"start\":48542},{\"end\":48554,\"start\":48549},{\"end\":48570,\"start\":48565},{\"end\":48587,\"start\":48581},{\"end\":48862,\"start\":48852},{\"end\":48874,\"start\":48866},{\"end\":48884,\"start\":48880},{\"end\":48895,\"start\":48888},{\"end\":49313,\"start\":49303},{\"end\":49323,\"start\":49317},{\"end\":49661,\"start\":49651},{\"end\":49670,\"start\":49665},{\"end\":49683,\"start\":49674},{\"end\":49695,\"start\":49687},{\"end\":49705,\"start\":49699},{\"end\":49969,\"start\":49964},{\"end\":49979,\"start\":49973},{\"end\":49989,\"start\":49983},{\"end\":50000,\"start\":49993},{\"end\":50267,\"start\":50259},{\"end\":50278,\"start\":50271},{\"end\":50285,\"start\":50282},{\"end\":50295,\"start\":50289},{\"end\":50506,\"start\":50500},{\"end\":50514,\"start\":50510},{\"end\":50524,\"start\":50518},{\"end\":50536,\"start\":50528},{\"end\":50542,\"start\":50540},{\"end\":50548,\"start\":50546},{\"end\":50835,\"start\":50824},{\"end\":50843,\"start\":50839},{\"end\":50849,\"start\":50847},{\"end\":50859,\"start\":50853},{\"end\":50871,\"start\":50863},{\"end\":50877,\"start\":50875},{\"end\":50886,\"start\":50881},{\"end\":50898,\"start\":50890},{\"end\":50908,\"start\":50902},{\"end\":50921,\"start\":50912},{\"end\":50931,\"start\":50927},{\"end\":50942,\"start\":50935},{\"end\":51279,\"start\":51271},{\"end\":51292,\"start\":51283},{\"end\":51449,\"start\":51442},{\"end\":51456,\"start\":51453},{\"end\":51463,\"start\":51460},{\"end\":51475,\"start\":51467},{\"end\":51483,\"start\":51479},{\"end\":51494,\"start\":51489},{\"end\":51507,\"start\":51498},{\"end\":51521,\"start\":51511},{\"end\":51841,\"start\":51833},{\"end\":51851,\"start\":51845},{\"end\":52124,\"start\":52118},{\"end\":52133,\"start\":52128},{\"end\":52146,\"start\":52137}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":43476,\"start\":43324},{\"attributes\":{\"id\":\"b1\"},\"end\":43936,\"start\":43478},{\"attributes\":{\"id\":\"b2\"},\"end\":44124,\"start\":43938},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14201947},\"end\":44354,\"start\":44126},{\"attributes\":{\"id\":\"b4\"},\"end\":44562,\"start\":44356},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11253972},\"end\":44722,\"start\":44564},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":33774240},\"end\":44992,\"start\":44724},{\"attributes\":{\"doi\":\"abs/1409.5185\",\"id\":\"b7\"},\"end\":45183,\"start\":44994},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7178848},\"end\":45580,\"start\":45185},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11657534},\"end\":45933,\"start\":45582},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7340116},\"end\":46288,\"start\":45935},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":5108612},\"end\":46632,\"start\":46290},{\"attributes\":{\"doi\":\"abs/1412.6115\",\"id\":\"b12\"},\"end\":46921,\"start\":46634},{\"attributes\":{\"doi\":\"arXiv:1308.4214\",\"id\":\"b13\"},\"end\":47322,\"start\":46923},{\"attributes\":{\"id\":\"b14\"},\"end\":47521,\"start\":47324},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2745955},\"end\":47731,\"start\":47523},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2309950},\"end\":47972,\"start\":47733},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7200347},\"end\":48249,\"start\":47974},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":17864746},\"end\":48490,\"start\":48251},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5850884},\"end\":48739,\"start\":48492},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":17432920},\"end\":49244,\"start\":48741},{\"attributes\":{\"id\":\"b21\"},\"end\":49557,\"start\":49246},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14805281},\"end\":49903,\"start\":49559},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14542261},\"end\":50196,\"start\":49905},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":5941770},\"end\":50427,\"start\":50198},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":16852518},\"end\":50818,\"start\":50429},{\"attributes\":{\"id\":\"b26\"},\"end\":51199,\"start\":50820},{\"attributes\":{\"doi\":\"abs/1409.1556\",\"id\":\"b27\"},\"end\":51436,\"start\":51201},{\"attributes\":{\"id\":\"b28\"},\"end\":51742,\"start\":51438},{\"attributes\":{\"id\":\"b29\"},\"end\":52069,\"start\":51744},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":740114},\"end\":52254,\"start\":52071}]", "bib_title": "[{\"end\":44169,\"start\":44126},{\"end\":44581,\"start\":44564},{\"end\":44791,\"start\":44724},{\"end\":45246,\"start\":45185},{\"end\":45676,\"start\":45582},{\"end\":46017,\"start\":45935},{\"end\":46379,\"start\":46290},{\"end\":47590,\"start\":47523},{\"end\":47779,\"start\":47733},{\"end\":48014,\"start\":47974},{\"end\":48317,\"start\":48251},{\"end\":48540,\"start\":48492},{\"end\":48848,\"start\":48741},{\"end\":49647,\"start\":49559},{\"end\":49960,\"start\":49905},{\"end\":50253,\"start\":50198},{\"end\":50496,\"start\":50429},{\"end\":51829,\"start\":51744},{\"end\":52114,\"start\":52071}]", "bib_author": "[{\"end\":43376,\"start\":43370},{\"end\":43387,\"start\":43376},{\"end\":43489,\"start\":43478},{\"end\":43500,\"start\":43489},{\"end\":43511,\"start\":43500},{\"end\":43523,\"start\":43511},{\"end\":43537,\"start\":43523},{\"end\":43549,\"start\":43537},{\"end\":43561,\"start\":43549},{\"end\":43577,\"start\":43561},{\"end\":43587,\"start\":43577},{\"end\":44028,\"start\":44018},{\"end\":44181,\"start\":44171},{\"end\":44192,\"start\":44181},{\"end\":44204,\"start\":44192},{\"end\":44218,\"start\":44204},{\"end\":44422,\"start\":44412},{\"end\":44435,\"start\":44422},{\"end\":44446,\"start\":44435},{\"end\":44593,\"start\":44583},{\"end\":44604,\"start\":44593},{\"end\":44623,\"start\":44604},{\"end\":44808,\"start\":44793},{\"end\":44818,\"start\":44808},{\"end\":44830,\"start\":44818},{\"end\":45007,\"start\":44996},{\"end\":45018,\"start\":45007},{\"end\":45029,\"start\":45018},{\"end\":45041,\"start\":45029},{\"end\":45051,\"start\":45041},{\"end\":45263,\"start\":45248},{\"end\":45270,\"start\":45263},{\"end\":45278,\"start\":45270},{\"end\":45294,\"start\":45278},{\"end\":45309,\"start\":45294},{\"end\":45689,\"start\":45678},{\"end\":45700,\"start\":45689},{\"end\":45712,\"start\":45700},{\"end\":45719,\"start\":45712},{\"end\":45730,\"start\":45719},{\"end\":45740,\"start\":45730},{\"end\":46035,\"start\":46019},{\"end\":46044,\"start\":46035},{\"end\":46054,\"start\":46044},{\"end\":46066,\"start\":46054},{\"end\":46078,\"start\":46066},{\"end\":46090,\"start\":46078},{\"end\":46390,\"start\":46381},{\"end\":46404,\"start\":46390},{\"end\":46414,\"start\":46404},{\"end\":46424,\"start\":46414},{\"end\":46435,\"start\":46424},{\"end\":46715,\"start\":46701},{\"end\":46720,\"start\":46715},{\"end\":46730,\"start\":46720},{\"end\":46743,\"start\":46730},{\"end\":46753,\"start\":46743},{\"end\":46939,\"start\":46923},{\"end\":46955,\"start\":46939},{\"end\":46966,\"start\":46955},{\"end\":46978,\"start\":46966},{\"end\":46987,\"start\":46978},{\"end\":46998,\"start\":46987},{\"end\":47010,\"start\":46998},{\"end\":47021,\"start\":47010},{\"end\":47031,\"start\":47021},{\"end\":47340,\"start\":47324},{\"end\":47356,\"start\":47340},{\"end\":47365,\"start\":47356},{\"end\":47378,\"start\":47365},{\"end\":47388,\"start\":47378},{\"end\":47396,\"start\":47388},{\"end\":47406,\"start\":47396},{\"end\":47604,\"start\":47592},{\"end\":47614,\"start\":47604},{\"end\":47793,\"start\":47781},{\"end\":47805,\"start\":47793},{\"end\":47815,\"start\":47805},{\"end\":48026,\"start\":48016},{\"end\":48037,\"start\":48026},{\"end\":48045,\"start\":48037},{\"end\":48332,\"start\":48319},{\"end\":48343,\"start\":48332},{\"end\":48356,\"start\":48343},{\"end\":48549,\"start\":48542},{\"end\":48556,\"start\":48549},{\"end\":48572,\"start\":48556},{\"end\":48589,\"start\":48572},{\"end\":48864,\"start\":48850},{\"end\":48876,\"start\":48864},{\"end\":48886,\"start\":48876},{\"end\":48897,\"start\":48886},{\"end\":49315,\"start\":49301},{\"end\":49325,\"start\":49315},{\"end\":49663,\"start\":49649},{\"end\":49672,\"start\":49663},{\"end\":49685,\"start\":49672},{\"end\":49697,\"start\":49685},{\"end\":49707,\"start\":49697},{\"end\":49971,\"start\":49962},{\"end\":49981,\"start\":49971},{\"end\":49991,\"start\":49981},{\"end\":50002,\"start\":49991},{\"end\":50269,\"start\":50255},{\"end\":50280,\"start\":50269},{\"end\":50287,\"start\":50280},{\"end\":50297,\"start\":50287},{\"end\":50508,\"start\":50498},{\"end\":50516,\"start\":50508},{\"end\":50526,\"start\":50516},{\"end\":50538,\"start\":50526},{\"end\":50544,\"start\":50538},{\"end\":50550,\"start\":50544},{\"end\":50837,\"start\":50822},{\"end\":50845,\"start\":50837},{\"end\":50851,\"start\":50845},{\"end\":50861,\"start\":50851},{\"end\":50873,\"start\":50861},{\"end\":50879,\"start\":50873},{\"end\":50888,\"start\":50879},{\"end\":50900,\"start\":50888},{\"end\":50910,\"start\":50900},{\"end\":50923,\"start\":50910},{\"end\":50933,\"start\":50923},{\"end\":50944,\"start\":50933},{\"end\":51281,\"start\":51269},{\"end\":51294,\"start\":51281},{\"end\":51451,\"start\":51440},{\"end\":51458,\"start\":51451},{\"end\":51465,\"start\":51458},{\"end\":51477,\"start\":51465},{\"end\":51485,\"start\":51477},{\"end\":51496,\"start\":51485},{\"end\":51509,\"start\":51496},{\"end\":51523,\"start\":51509},{\"end\":51527,\"start\":51523},{\"end\":51843,\"start\":51831},{\"end\":51853,\"start\":51843},{\"end\":52126,\"start\":52116},{\"end\":52135,\"start\":52126},{\"end\":52148,\"start\":52135}]", "bib_venue": "[{\"end\":43368,\"start\":43324},{\"end\":43692,\"start\":43587},{\"end\":44016,\"start\":43938},{\"end\":44222,\"start\":44218},{\"end\":44410,\"start\":44356},{\"end\":44626,\"start\":44623},{\"end\":44837,\"start\":44830},{\"end\":45373,\"start\":45309},{\"end\":45744,\"start\":45740},{\"end\":46094,\"start\":46090},{\"end\":46442,\"start\":46435},{\"end\":46699,\"start\":46634},{\"end\":47091,\"start\":47046},{\"end\":47410,\"start\":47406},{\"end\":47618,\"start\":47614},{\"end\":47833,\"start\":47815},{\"end\":48101,\"start\":48045},{\"end\":48360,\"start\":48356},{\"end\":48599,\"start\":48589},{\"end\":48981,\"start\":48897},{\"end\":49299,\"start\":49246},{\"end\":49711,\"start\":49707},{\"end\":50025,\"start\":50002},{\"end\":50301,\"start\":50297},{\"end\":50610,\"start\":50550},{\"end\":51267,\"start\":51201},{\"end\":51899,\"start\":51853},{\"end\":52152,\"start\":52148}]"}}}, "year": 2023, "month": 12, "day": 17}