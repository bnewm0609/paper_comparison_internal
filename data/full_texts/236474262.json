{"id": 236474262, "updated": "2023-10-06 00:38:27.344", "metadata": {"title": "Fairness-Aware Online Meta-learning", "authors": "[{\"first\":\"Chen\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Feng\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Bhavani\",\"last\":\"Thuraisingham\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining", "publication_date": {"year": 2021, "month": 8, "day": 21}, "abstract": "In contrast to offline working fashions, two research paradigms are devised for online learning: (1) Online Meta Learning (OML) learns good priors over model parameters (or learning to learn) in a sequential setting where tasks are revealed one after another. Although it provides a sub-linear regret bound, such techniques completely ignore the importance of learning with fairness which is a significant hallmark of human intelligence. (2) Online Fairness-Aware Learning. This setting captures many classification problems for which fairness is a concern. But it aims to attain zero-shot generalization without any task-specific adaptation. This therefore limits the capability of a model to adapt onto newly arrived data. To overcome such issues and bridge the gap, in this paper for the first time we proposed a novel online meta-learning algorithm, namely FFML, which is under the setting of unfairness prevention. The key part of FFML is to learn good priors of an online fair classification model's primal and dual parameters that are associated with the model's accuracy and fairness, respectively. The problem is formulated in the form of a bi-level convex-concave optimization. Theoretic analysis provides sub-linear upper bounds for loss regret and for violation of cumulative fairness constraints. Our experiments demonstrate the versatility of FFML by applying it to classification on three real-world datasets and show substantial improvements over the best prior work on the tradeoff between fairness and classification accuracy", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.09435", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2108-09435", "doi": "10.1145/3447548.3467389"}}, "content": {"source": {"pdf_hash": "d8fb4ed105bc9e6d1f738f54a0618a3cdf1a738f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.09435v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2108.09435", "status": "GREEN"}}, "grobid": {"id": "6d12e4c1c1746f569375d557768b771955dea46d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d8fb4ed105bc9e6d1f738f54a0618a3cdf1a738f.txt", "contents": "\nFairness-Aware Online Meta-learning\n2021. August 14-18, 2021\n\nChen Zhao chen.zhao@utdallas.edu \nFeng Chen feng.chen@utdallas.edu \nBhavani Thuraisingham bhavani.thuraisingham@utdallas.edu \nChen Zhao \nFeng Chen \nBhavani Thuraisingham \n\nThe University of Texas at Dallas Richardson\nTexasUSA\n\n\nThe University of Texas\nat Dallas RichardsonTexasUSA\n\n\nThe University of Texas\nat Dallas RichardsonTexasUSA\n\nFairness-Aware Online Meta-learning\n\nProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21)\nthe 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21)2021. August 14-18, 202110.1145/3447548.3467389ACM Reference Format: Virtual Event, Singapore. ACM, New York, NY, USA, 11 pages. https://doi. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '21, August 14-18, 2021, Virtual Event, SingaporeCCS CONCEPTS \u2022 Computing methodologies \u2192 Artificial intelligenceMa- chine learning\u2022 Applied computing \u2192 Law, social and behav- ioral sciences\u2022 Social and professional topics \u2192 User charac- teristics KEYWORDS deep learning, meta learning, model fairness, online learning, long- term constraints\nIn contrast to offline working fashions, two research paradigms are devised for online learning: (1) Online Meta Learning (OML)[6,20,26]learns good priors over model parameters (or learning to learn) in a sequential setting where tasks are revealed one after another. Although it provides a sub-linear regret bound, such techniques completely ignore the importance of learning with fairness which is a significant hallmark of human intelligence. (2) Online Fairness-Aware Learning[1,8,21]. This setting captures many classification problems for which fairness is a concern. But it aims to attain zero-shot generalization without any task-specific adaptation. This therefore limits the capability of a model to adapt onto newly arrived data. To overcome such issues and bridge the gap, in this paper for the first time we proposed a novel online meta-learning algorithm, namely FFML, which is under the setting of unfairness prevention. The key part of FFML is to learn good priors of an online fair classification model's primal and dual parameters that are associated with the model's accuracy and fairness, respectively. The problem is formulated in the form of a bi-level convex-concave optimization. Theoretic analysis provides sub-linear upper bounds (log ) for loss regret and ( \u221a\ufe01 log ) for violation of cumulative fairness constraints. Our experiments demonstrate the versatility of FFML by applying it to classification on three real-world datasets and show substantial improvements over the best prior work on the tradeoff between fairness and classification accuracy.\n\nINTRODUCTION\n\nIt's no secret that bias is present everywhere in our society, such as recruitment, loan qualification, recidivism, etc. The manifestation of bias can be often as fraught as race or gender. Because of this, machine learning algorithms have several examples of training models, many of which have received strong criticism for exhibiting unfair bias. Critics have voiced that human bias potentially has an influence on nowadays technology, which leads to outcomes with unfairness.\n\nWith biased input, the main goal of training an unbiased model in machine learning is to make the output fair. Group-fairness, also known as statistic parity, ensures the equality of a predictive utility across different sub-populations. In other words, the predictions are statistically independent on protected variables, e.g., race or gender. In the real world, data with bias are likely available only sequentially and also from a non-stationary task distribution. For example, a recent news [18] by New York Times reports that systematic algorithms become increasingly discriminative to African Americans in bank loan during COVID-19 pandemic. These algorithms are built up from a sequence of data batches collected one after another over time, where in each batch, decision-makings are biased on the protected attribute (e.g. race). To learn a fair model over time and make it efficiently and quickly adapt to unseen data, online learning [9] are devised to learn models incrementally from data in a sequential manner and models can be updated instantly and efficiently when new training data arrives [11].\n\nTwo distinct research paradigms in online learning have attracted attentions in recent years. Online meta-learning [6] learns priors over model parameters in a sequential setting not only to master the batch of data at hand but also the learner becomes proficient with quick adaptation at learning new arrived tasks in the future. Although such techniques achieve sub-linear loss regret, it completely ignores the significance of learning with fairness, which is a crucial hallmark of human intelligence.\n\nOn the other hand, fairness-aware online learning captures supervised learning problems for which fairness is a concern. It either compels the algorithms satisfy common fairness constraints at each round [1] or defines a fairness-aware loss regret where learning and fairness interplay with each other [21]. However, neither of these settings is ideal for studying continual lifelong learning where past experience is used to learn priors over model parameters, and hence existing methods lack adaptability to new tasks.\n\nWith the aim of connecting the fields of online fairness-aware learning and online meta-learning, we introduce a new problem statement, that is fairness-aware online meta-learning with longterm constraints, where the definition of long-term constraints [17] indicates the sum of cumulative fairness constraints. From a global perspective, we allow the learner to make decisions at some rounds which may not belong to the fairness domain due to non-stationary aspects of the problem, but the overall sequence of chosen decisions must obey the fairness constraints at the end by a vanishing convergence rate.\n\nTo this end, technically we propose a novel online learning algorithm, namely follow-the-fair-meta-leader (FFML). At each round, we determine model parameters by formulating a problem composed by two main levels: online fair task-level learning and metalevel learning. Each level of the bi-level problem is embedded within each other with two parts of parameters: primal parameters regarding model accuracy and dual parameters adjusting fairness notions. Therefore, in stead of learning primary parameters only at round \u2208 [ ], an agent learns a meta-solution pair ( +1 , +1 ) across all existing tasks by optimizing a convex-concave problem and extending the gradient based approach for variational inequality. Furthermore, when a new task arrives at +1, the primal variable +1 and the dual variable +1 are able to quickly adapted to it and the overall model regret grows sub-linearly in . We then analyze FFML with theoretic proofs demonstrating it enjoys a (log ) regret guarantee and ( \u221a\ufe01 log ) bound for violation of long-term fairness constraints when competing with the best meta-learner in hindsight. The main contributions are summarized:\n\n\u2022 To the best of our knowledge, for the first time a fairness-aware online meta-learning problem is proposed. To solve the problem efficiently, we propose a novel algorithm follow-the-fair-metaleader (FFML). Specifically, at each time, the problem is formulated as a constrained bi-level convex-concave optimization with respect to a primal-dual parameter pair for each level, where the parameter pair responds for adjusting accuracy and fairness notion adaptively. \u2022 Theoretically grounded analysis justifies the efficiency and effectiveness of the proposed method by demonstrating a (log ) bound for loss regret and ( \u221a\ufe01 log ) for violation of fairness constraints, respectively. \u2022 We validate the performance of our approach with state-of-theart techniques on real-world datasets. Our results demonstrate the proposed approach is not only capable of mitigating biases but also achieves higher efficiency compared with the state-ofthe-art algorithms.\n\n\nRELATED WORK\n\nFairness-aware online learning problems assume individuals arrive one at a time and the goal of such algorithms is to train predictive models free from biases. To require fairness guarantees at each round, [1] has partial and bandit feedback and makes distributional assumptions. Due to the the trade-off between the loss regret and fairness in terms of an unfairness tolerance parameter, a fairness-aware regret [21] is devised and it provides a fairness guarantee held uniformly over time. Besides, in contrast to group fairness, online individual bias is governed by an unknown similarity metric [8]. However, these methods are not ideal for continual lifelong learning with non-stationary task distributions, as they aim to obtain zero-shot generalization but fail to learn priors from past experience to support any task-specific adaptation. Meta-learning [23] addresses the issue of learning with fast adaptation, where a meta-learner learns knowledge transfer from history tasks onto unseen ones. Approaches could be broadly classified into offline and online paradigms. In the offline fashion, existing meta-learning methods generally assume that the tasks come from some fixed distribution [7,22,25,[31][32][33][34], whereas it is more realistic that methods are expected to work for non-stationary task distributions. To this end, FTML [6] can be considered as an application of MAML [5] in the setting of online learning. OSML [26] disentangles the whole meta-learner as a meta-hierarchical graph with multiple structured knowledge blocks. As for reinforcement learning, MOLe [20] used expectation maximization to learn mixtures of neural network models. A major drawback of aforementioned methods is that it immerses in minimizing objective functions but ignores the fairness of prediction. In our work, we deal with online meta learning subject to group fairness constraints and formulate the problem as a constrained bi-level optimization problem, in which the proposed algorithm enjoys both regret guarantee and an upper bound on violation of fairness constraints.\n\nOnline convex optimization with long term constra-ints. For online convex optimization with long-term constraints, a projection operator is typically applied to update parameters to make them feasible at each round. However, when the constraints are complex, the computational burden of the projection may be too high. To circumvent this dilemma, [17] relaxes the output through a simpler close-form projection. Thereafter, several close works aim to improve the theoretic guarantees by modifying stepsizes to a adaptive version [12], adjusting to stochastic constraints [27], and clipping constraints into a non-negative orthant [28]. Although such techniques achieve state-of-the-art theoretic guarantees, they are not directly applicable to bi-level online convex optimization with long-term constraints.\n\nIn this paper, we study the problem of online fairness-aware meta learning to deal with non-stationary task distributions. Our proposed approach is designed based on the bridging of the above three areas. In particular, we connect the first two areas to formulate the problem as a bi-level online convex optimization problem with long-term fairness constraints, and develop a novel learning algorithm based on generalization of the primal-dual optimization techniques designed in the third area.\n\n\nPRELIMINARIES 3.1 Notations\n\nAn index set of a sequence of tasks is defined as [ ] = {1, ..., }. Vectors are denoted by lower case bold face letters, e.g. the primal variables \u2208 \u0398 and the dual variables \u2208 R + where their -th entries are , . Vectors with subscripts of task indices, such as , where \u2208 [ ], indicate model parameters for the task at round . The Euclidean \u2113 2 -norm of is denoted as || ||. Given a differentiable function L ( , ) : \u0398\u00d7R + \u2192 R, the gradient at and is denoted as \u2207 L ( , ) and \u2207 L ( , ), respectively. Scalars are denoted by lower case italic letters, e.g. > 0. Matrices are denoted by capital italic letters. B is the projection operation to the set B. [ ] + denotes the projection of the vector on the nonnegative orthant in R + , namely [ ] + = (max{0, 1 }), ..., max{0, }). An example of a function (\u00b7) taking two variables , separated with a semicolon (i.e. ( ; )) indicates that is initially assigned with . Some important notations are listed in Table 1. \n\n\nFairness-Aware Constraints\n\nIntuitively, an attribute affects the target variable if one depends on the other. Strong dependency indicates strong effects. In general, group fairness criteria used for evaluating and designing machine learning models focus on the relationships between the protected attribute and the system output. The problem of group unfairness prevention can be seen as a constrained optimization problem. For simplicity, we consider one binary protected attribute (e.g. white and black) in this work. However, our ideas can be easily extended to many protected attributes with multiple levels. Let Z = X \u00d7 Y be the data space, where X = E\u222aS. Here E \u2282 R is an input space, S = {0, 1} is a protected space, and Y = {0, 1} is an output space for binary classification. Given a task (batch) of samples {e , , } =1 \u2208 (E \u00d7 Y \u00d7 S) where is the number of datapoints, a fine-grained measurement to ensure fairness in class label prediction is to design fair classifiers by controlling the decision boundary covariance (DBC) [29].\n\nDefinition 1 (Decision Boundary Covariance [16,29]). The Decision Boundary Covariance (i.e. DBC) is defined as the covariance between the protected variables s = { } =1 and the signed distance from the feature vectors to the decision boundary. A linear approximated form of DBC takes DBC = E (e, , ) \u2208Z 1\n1 (1 \u2212^1) + 1 2 \u2212^1 \u210e( , )(1)\nwhere^1 is an empirical estimate of 1 and 1 = P (e, , ) \u2208Z ( = 1) is the proportion of samples in group = 1, and \u210e : R \u00d7 \u0398 \u2192 R is a real valued function taking as parameters.\n\nTherefore, parameters in the domain of a task is feasible if it satisfies the fairness constraint ( ) \u2264 0. More concretely, ( ) is defined by DBC in Eq.(1), i.e.\n( ) = DBC \u2212(2)\nwhere |\u00b7| is the absolute function and > 0 is the fairness relaxation determined by empirical analysis. For group fairness constraints, two wildly used fair notion families are demographic parity [2] and equality of opportunity (a.k.a equal opportunity) [10]. Notice that demographic parity and equality of opportunity are quite similar from a mathematical point of view [16], and hence results and analysis on one notion can often be readily extended to the other one. According to [16], the DBC fairness notion introduced in Definition 1 is an empirical version of demographic parity. In this paper, we consider DBC as the fairnessaware constraint, but our proposed approach supports equality of opportunity and other fairness notions that are smooth functions.\n\n\nSettings and Problem Formulation\n\nThe goal of online learning is to generate a sequence of model parameters { } =1 that perform well on the loss sequence { : \u0398 \u2192 R} =1 , e.g. cross-entropy for classification problems. Here, we assume \u0398 \u2286 R is a compact and convex subset of the -dimensional Euclidean space with non-empty interior. In a general setting of online learning, there is no constraint on how the sequence of loss functions is generated. In particular, the standard objective is to minimize the regret of Eq.(3) defined as the difference between the cumulative losses that have incurred over time and the best performance achievable in hindsight. The solution to it is called Hannan consistent [3] if the upper bound on the worst case regret of an algorithm is sublinear in .\n= \u2211\ufe01 =1 ( ) \u2212 min \u2208\u0398 \u2211\ufe01 =1 ( )(3)\nTo control bias and especially ensure group fairness across different sensitive sub-populations, fairness notions are considered as constraints added on optimization problems. A projection operator is hence typically applied to the updated variables in order to make them feasible at each round [12,17,28].\n\nIn this paper, we consider a general sequential setting where an agent is faced with tasks {D } =1 one after another. Each of these tasks corresponds to a batch of samples from a fixed but unknown non-stationary distribution. The goal for the agent is to minimize the regret under the summation of fair constraints, namely long-term constraints:\nmin 1 ,..., \u2208B = \u2211\ufe01 =1 (A ( , D ), D ) (4) \u2212 min \u2208\u0398 \u2211\ufe01 =1 (A ( , D ), D ) subject to \u2211\ufe01 =1 (A ( , D ), D ) \u2264 ( ), \u2200 \u2208 [ ]\nwhere \u2208 (0, 1); D \u2282 D is the support set and D \u2286 D is a subset of task D that is used for evaluation; A (\u00b7) is the base learner which corresponds to one or multiple gradient steps [5] of a Lagrangian function, which will be introduced in the following sections. For the sake of simplicity, we will use one gradient step gradient throughout this work, but more steps are applicable. Different from traditional online learning settings, the long-term constraint violation =1 (\u00b7) : B \u2192 R, \u2200 \u2208 [ ] is required to be bounded sublinear in . In order to facilitate our analysis, at each round, is originally chosen from its domain \u0398, where it can be written as an intersection of a finite number of convex constraints that \u0398 = { \u2208 R : ( ) \u2264 0, = 1, ..., }. In order to lower the computational complexity and accelerate the online processing speed, inspired by [17], we relax the domain to B, where \u0398 \u2286 B = K with K being the unit \u2113 2 ball centered at the origin, and \u225c max{ > 0 : = ||x \u2212 y||, \u2200x, y \u2208 \u0398}. With such relaxation, we allow the learner to make decisions at some rounds which do not belong to the domain \u0398, but the overall sequence of chosen decisions must obey the constraints at the end by vanishing convergence rate. Following [28] and [6], we assume the number of rounds is known in advance.\n\nRemarks: The new regret defined in Eq.(4) differs from the settings of online learning with long-term constraints. Minimizing Eq.(4) is embeded with a bi-level optimization problem. In contrast to the regret considered in FTML [6], both loss regret and violation of long-term fairness constraints in Eq.(4) are required to be bounded sublinearly in .\n\n\nMETHODOLOGY\n\nIn order to minimize the regret constrained with fairness notions in Eq.(4), the overall protocol for the setting is:\n\n\u2022 Step 1: At round , task and model parameters defined by D , are chosen. \u2022 Step 2: The learning agent incurs loss (A ( )) and fairness (A ( )), \u2200 \u2208 [ ]. \u2022 Step 3: The update procedure is learned from prior experience, and it is used to determine model parameters +1 fairly through an optimization algorithm. \u2022 Step 4: The next predictors are updated and advance to the next round + 1.\n\n\nFollow the Fair Meta Leader (FFML)\n\nIn the protocol, the key step is to find a good meta parameters at each round (Step 3). At round , when the task D comes, the main goal incurred is to determine the meta parameters +1 for the next round. Specifically, the most intuitive way to find a good +1 is to optimize it over past seen tasks from 1 to . We hence consider a setting where the agent can perform some local taskspecific updates to the model before it is deployed and evaluated onto each task at each round.\n\nThe problem of learning meta parameters at each round, therefore, is embedded with another optimization problem of finding model-parameters in a task-specific level. Here, the base learner A (\u00b7) determines model-parameters such that the task loss : \u0398 \u2192 R is minimized subject to all constraints ( ) \u2264 0, = 1, 2, ..., , where \u2208 [ ] is the index of previous tasks.\n\nThe optimization problem is formulated with two nested levels, i.e. an outer and an inner level, and one supports another. The outer problem takes the form:\n+1 = arg min \u2208B \u2211\ufe01 =1 ( , D ; A ( , D )) (5) subject to \u2211\ufe01 =1 ( , D ; A ( , D )) \u2264 0, \u2200 \u2208 [ ]\nwhere the inner problem is defined as:\nA ( , D ) = arg min \u2208B ( , D ; ) (6) subject to ( , D ; ) \u2264 0, \u2200 \u2208 [ ]\nwhere D , D \u2282 D are support and query mini-batches, which are independently sampled without replacements, i.e. D \u2229 D = \u2205. In the following section, we introduce our proposed algorithm. In stead of optimizing primal parameters only, it efficiently deals with the bi-level optimization problem of Eq.(5)(6) by approximating a sequence of a pair of primal-dual meta parameters ( , ) where the pair respectively responds for adjusting accuracy and fairness level.\n\n\nAn Efficient Algorithm\n\nThe proposed Algorithm 1 is composed of two levels where each responds for the inner and outer problems stated in Eq.(6) and Eq.(5), respectively. The output of the outer problem are used to define the inner objective function and vice versa. A parameter pair is therefore iteratively updated between the two levels.\n\nTo solve the inner level problem stated in Eq.(6), we first consider the following Lagrangian function and omit D for the sake of brevity:\nL ( , ; , ) = ( ; ) + \u2211\ufe01 =1 , ( ; )(7)\n\nAlgorithm 1 The FFML Algorithm\n\nRequire: Learning rates 1 , 2 , some constant . Compute adapted task-level primal and dual pair ( \u2032 , \u2032 ) using Eq. (8) and (9). 14:\n\nSample datapoints D \u2282 D Update meta-level primal-dual parameter pair ( , ) using Eq. (11) and (12). 18: end while 19:\nSet meta-parameters ( , ) \u2190 ( ,) 20:\nend if 21:\n( +1 , +1 ) \u2190 ( , ) 22: end for\nwhere \u2208 B is the task-level primal variable initialized with the meta-level primal variable , and \u2208 R + is the corresponding dual variable initialized with , which is used to penalize the violation of constraints. Here, for the purpose of optimization with simplicity, constraints of Eq.(6) are approximated with the cumulative one shown in Eq. (7). To optimize, we update the task-level variables through a base learner A (\u00b7) :\n\n\u2208 B \u2192 \u2032 \u2208 R . One example for the learner is updating with one gradient step using the pre-determined stepsize 1 > 0 [6]. Notice that for multiple gradient steps, \u2032 and \u2032 interplay each other for updating.\n\u2032 = A ( ; ) \u225c \u2212 1 \u2207 L ( , ; , )(8)\u2032 = + 1 \u2207 L ( \u2032 , ; , ) +(9)\nNext, to solve the outer level problem, the intuition behind our approach stems from the observation that the constrained optimization problem is equivalent to a convex-concave optimization problem with respect to the outer-level primal variable and dual variable . We hence consider the following augmented Lagrangian function:\nL ( , ) = 1 \u2211\ufe01 =1 ( ; \u2032 ) + \u2211\ufe01 =1 ( ; \u2032 ) \u2212 2 2 2(10)\nwhere > 0 and 2 > 0 are some constant and stepsize whose values will be decided by the analysis. Besides, the augmented term on the dual variable is devised to prevent from being too large [17]. To optimize the meta-parameter pair in the outer problem, the update rule follows\n+1 = B ( \u2212 2 \u2207 L ( , )) = arg min \u2208B || \u2212 ( \u2212 2 \u2207 L ( , ))|| 2 (11) +1 = + 2 \u2207 L ( , ) +(12)\nwhere B is the projection operation to the relaxed domain B that is introduced in Sec.3.3. This approximates the true desired projection with a simpler closed-form.\n\nWe detail the iterative update procedure in Algorithm 1. At round \u2208 [ ], we first evaluate the performance on the new arrived task D using (line 5) and D is added to the task buffer U (line 6). As for the bi-level update, each task-level parameters , from the buffer are initialized with the meta-level ones (line 8). In line 13, task-level parameters are updated using support data. Query loss and fairness for each task are further computed in line 15 and they are used to optimize meta parameters in line 17. An overview of the update procedure is described in Figure 1.\n\nDifferent from techniques devised to solve online learning problems with long-term constraints, at each round FFML finds a good primal-dual parameter pair by learning from prior experience through dealing with a bi-level optimization problem. In order to ensure bias-free predictions, objective functions in both inner and outer levels subject to fairness constraints. Besides, since we generalize the traditional primal-dual update scheme onto a bi-level optimization problem, conventional theoretic guarantees cannot be applied. We hence demonstrate analysis of FFML in the following section.\n\n\nANALYSIS\n\nTo analysis, in this paper, we first make following assumptions as in [6] and [17]. These assumptions are commonly used in meta learning and online learning settings. Examples where these assumptions hold include logistic regression and 2 regression over a bounded domain. As for constraints, a family of fairness notions, such as linear relaxation based DDP (Difference of Demographic Parity) including Eq. (2), are applicable as discussed in [16]. We next present the key Theorem 2. We state that FFML enjoys sub-linear guarantee for both regret and long-term fairness constraints in the long run for Algorithm 1.  [12,28] in the case of strongly convexity, in terms of online meta-learning paradigms, for the first time we bound loss regret and cumulative fairness constraints simultaneously. For space purposes, proofs for all theorems are contained in the Appendix A and B.\n\n\nEXPERIMENTS\n\nTo corroborate our algorithm, we conduct extensive experiments comparing FFML with some popular baseline methods. We aim to answer the following questions:\n\n(1) Question 1: Can FFML achieve better performance on both fairness and classification accuracy compared with baseline methods? (2) Question 2: Can FFML be successfully applied to nonstationary learning problems and achieve a bounded fairness as increases? (3) Question 3: How efficient is FFML in task evaluation over time and how is the contribution for each component?\n\n\nDatasets\n\nWe use the following three publicly available datasets. Each dataset contains a sequence of tasks where the ordering of tasks is selected at random. (1) Bank Marketing [19] contains a total 41,188 subjects, each with 16 attributes and a binary label, which indicates whether the client has subscribed or not to a term deposit. We consider the marital status as the binary protected attribute. The dataset contains 50 tasks and each corresponds to a date when the data are collected from April to December in 2013.\n\n(2) Adult [13] is broken down into a sequence of 41 income classification tasks, each of which relates to a specific native country. The dataset totally 48,842 instances with 14 features and a binary label, which indicates whether a subject's incomes is above or below 50K dollars. We consider gender, i.e. male and female, as the protected attribute.\n\n(3) Communities and Crime [15] is split into 43 crime classification tasks where each corresponds to a state in the U.S. Following the same setting in [24], we convert the violent crime rate into binary labels based on whether the community is in the top 50% crime rate within a state. Additionally, we add a binary protected attribute that receives a protected label if African-Americans are the highest or second highest population in a community in terms of percentage racial makeup.\n\n\nEvaluation Metrics\n\nThree popular evaluation metrics are introduced that each allows quantifying the extent of bias taking into account the protected attribute.\n\nDemographic Parity (DP) [4] and Equalized Odds (EO) [10] can be formalized as DP =\n(^= 1| = 0) (^= 1| = 1) ; EO = (^= 1| = 0, = ) (^= 1| = 1, = )\nwhere \u2208 {0, 1}. Equalized odds requires that^have equal true positive rates and false positive rates between sub-groups. For both metrics, a value closer to 1 indicate fairness.\n\nDiscrimination [30] measures the bias with respect to the protected attribute in the classification:\nDisc = : =1^ : =1 1 \u2212 : =0^ : =0 1\nThis is a form of statistical parity that is applied to the binary classification decisions. We re-scale values across all baseline methods into a range of [0, 1] and = 0 indicates there is no discrimination.\n\n\nCompeting Methods\n\n(1) Train with penalty (TWP): is an intuitive approach for online fair learning where loss functions at each round is penalized by the violation of fairness constraints. We then run the standard online gradient descent (OGD) algorithm to minimize the modified loss function. (2) m-FTML [6]: the original FTML finds a sequence of meta parameters by simply applying MAML [5] at each round. To focus on fairness learning, this approach is applied to modified datasets by removing protected attributes. Notice that techniques (3)-(5) are proposed for online learning with long-term constraints and achieve state-of-the-art performance in theoretic guarantees. In order to fit bias-prevention and compare them to FFML, we specify such constraints as DBC stated in Eq. (2). (3) OGDLC [17]: updates parameters solely based on the on-going task. (4) AdpOLC [12]: improves OGDLC by modifying stepsizes to an adapted version. (5) GenOLC [28]: rectifies AdpOLC by square-clipping the constraints in place of (\u00b7), \u2200 . Although OGDLC, AdpOLC and GenOLC are devised for online learning with long-term constraints, none of these state-of-the-arts considers inner-level adaptation. We note that, among the above baseline methods, m-FTML is a state-ofthe-art one in the the area of online meta learning. The last three baselines, including OGDLC, AdpOLC, GenOLC are representative ones in the area of online optimization with long term constraints.\n\n\nSettings\n\nAs discussed in Sec.5, the performance of our proposed method has been well justified theoretically for machine learning models, such as logistic regression and L2 regression, whose objectives that are strongly convex and smooth. However, in machine learning and fairness studies, due to the non-linearity of neural networks, many problems have a non-convex landscape where theoretical analysis is challenging. Nevertheless, algorithms originally developed for convex optimization problems like gradient descent have shown promising results in practical non-convex settings [6]. Taking inspiration from these successes, we describe practical instantiations for the proposed online algorithm, and empirically evaluate the performance in Sec.7.\n\nFor each task we set the number of fairness constraints to one, i.e. = 1. For the rest, we following the same settings as used in online meta learning [6]. In particular, we meta-train with support size of 100 for each class, whereas 30% or 90% (hundreds of datapoints) of task samples for evaluation. All methods are completely online, i.e., all learning algorithms receive one task per-iteration. All the baseline models that are used to compare with our proposed approach share the same neural network architecture and parameter settings. All the experiments are repeated 10 times with the same settings and the mean and standard deviation results are reported. Details on the settings are given in Appendix C.  \n\n\nRESULTS\n\nThe following experimental results on each dataset are to answer all Questions given in Sec.6.\n\n\nEnd Task Performance\n\nIn contrast to traditional machine learning paradigms where they often run a batch learning fashion, online learning aims to learn and update the best predictor for future data at each round. In our experiments, we consider a sequential setting where a task comes one after another. To validate the effectiveness of the proposed algorithm, we first compare the end task performance. All methods stop at = after seeing all tasks. The learned parameter pair ( , ) is further fine-tuned using the support set D which is sampled from the task . The end task performance is hence evaluated on the validation set D using the adapted parameter \u2032 .  Consolidated and detailed performance of the different techniques over real-world data are listed in Table 2. We evaluate performance across all competing methods on a scale of 90% datapoints of the end task for each dataset. Best performance in each experimental unit are labeled in bold. We observe that as for bias-controlling, FFML out-performs than other baseline methods. Specifically, FFML has the highest scores in terms of the fairness metrics DP and EO, and the smallest value of Disc close to zero signifies a fair prediction. Note that although FFML returns a bit smaller predictive accuracy, this is due to the trade-off between losses and fairness.\n\n\nPerformance Through Each Round\n\nIn order to take a closer look at the performance regarding biascontrol in a non-stationary environment, at round \u2208 [ ], the parameter pair ( , ) inherited from the previous task \u2212 1 are employed to evaluate the new task . Inspired by [6], we separately record the performance based on different amount (i.e. 90% and 30%) of validation samples. Figure 2 and 3 detail evaluation results across three real-world datasets at each round with respect to two wildly used fairness metrics DP and EO, respectively. Specifically, higher is better for all plots, while shaded regions show standard error computed using various random seeds. The learning curves show that with each new task added FFML efficiently controls bias and substantially outperforms the alternative approaches in achieving the best fairness aware results represented by the highest and in final performance. GenOLC returns better results than AdpOLC and OGDLC since it applies both adaptive learning rates and squared clipped constraint term. However, two of the reasons giving rise to the performance of GenOLC inferior to FFML is that our method (1) takes task-specific adaptation with respect to primal-dual parameter pair at inner loops, which further helps the task progress better as for fairness learning, (2) FFML explicitly meta-trains and hence fully learns the structure across previous seen tasks. Although m-FTML shows an improvement in fairness, there is still substantial unfairness hidden in the data in the form of correlated attributes, which is consistent with [30] and [14]. As the most intuitive approach, theoretic analysis in [17] shown the failure of using TWP is that the weight constant is fixed and independent from the sequences of solutions obtained so far.\n\nAnother observation is when evaluation data are reduced to 30%. Although the fairness performance becomes more fluctuant, FFML remains the out-performance than other baseline methods through each round. This results from that in a limited amount of evaluated data our method stabilizes the results by learning parameters from all prior tasks so far at each round, and suggests that even better transfer can be accomplished through meta-learning.\n\n\nTask Learning Efficiency\n\nTo validate the learning efficiency of the proposed FFML, we set a proficiency threshold for all methods at each round, where = ( 1 , 2 ) corresponds to the amount of data needed in D to achieve both a loss value ( , D ) \u2264 1 and a DBC value ( , D ) \u2264 2 at the same time. We set 1 = 0.0005 and 2 = 0.0001 for all datasets. If less data is sufficient to reach the threshold, then priors learned from previous tasks are being useful and we have achieved positive transfer [6]. Through the results demonstrated in Figure  4, we observe while the baseline methods improve in efficiency over the course of learning as they see more tasks, they struggle to prevent negative transfer on each new task.\n\n\nAblation Studies\n\nWe conducted additional experiments to demonstrate the contributions of the three key technical components in FFML: the inner(task)level fairness constraints (inner FC) in Eq.(6), the outer (meta)-level fairness constraints (outer FC) in Eq. (7), and the augmented term (aug) in Eq. (10). Particularly, Inner FC and outer FC are used to regularize the task-level and metal-level loss functions, respectively, and aug is used to prevent the dual parameters being to large and hence stabilizes the learning with fairness in the outer problem.\n\nThe key findings in Figure 5 are (1) inner fairness constraints and the augmented term can enhance bias control, and (2) outer update procedure plays more important role in FFML. This is due to the close-form projection onto the relaxed domain B with respect to primal variable and clipped non-negative dual variable.\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this paper, we formulate the problem of online fairness-aware meta learning and present a novel algorithm, namely FFML. We claim that for the first time a fairness-aware online meta-learning framework is proposed. The goal of this model is to minimize both the loss regret and violation of long-term fairness constraints as increases, and to achieve sub-linear bound for them. Specifically, in stead of learning primal parameters only at each round, FFML trains a meta-parameter pair including primal and dual variables, where the primal variable determines the predictive accuracy and the dual variable controls the level of satisfaction of model fairness. To determine the parameter pair at each round, we formulate the problem to a bi-level convex-concave optimization problem. Detailed theoretic analysis and corresponding proofs justify the efficiency and effectiveness of the proposed algorithm by demonstrating upper bounds for regret and violation of fairness constraints. Experimental evaluation based on three real-world datasets shows that our method out-performs than state-of-the-art online learning techniques with long-term fairness constraints in bias-controlling. It remains interesting if one can prove that fairness constraints are satisfied at each round without approximated projections onto the relaxed domain, and if one can explore learning when environment is changing over time.\n\n\nACKNOWLEDGEMENT\n\nThis work is supported by the National Science Foundation (NSF) under Grant Number #1815696 and #1750911.\n\n\nA SKETCH OF PROOF OF THEOREM 1\n\nProof. Before giving the proof of Theorem 1, let us define a functionL ( , ) : (B \u00d7 R + ) \u2192 R with respect to the primal and dual variable and . Therefore,L ( , ) is considered as a single task function \u2208 [ ] from prior experience and it is stripped from Eq. (10).\nL ( , ) := L \u2032 A ( ), = (A ( )) + \u2211\ufe01 =1 (A ( )) \u2212 2 2 ) 2\nwhere A ( ) is defined in Eq. (8). We hence first proveL (\u00b7, ) is convex with respect to . Similar results can be easily achieved with respect to . We consider two arbitrary point , \u2208 B with respect to the primal variable: We then bound the first and second terms that: Thus,L (\u00b7, ) is 1 8 ( +\u00af)-strongly convex. SinceL (\u00b7, ) is convex, summation of convex functions with non-negative weights preserves convexity and summation of strongly convex functions is strongly convex. Therefore, we complete the proof for L (\u00b7, ). \u25a1\nFT \u2264 1 ||\u2207 \u2032 L \u2032 (A ( ))||||(\u2207 2 ( ) \u2212 \u2207 2 ( ))+ (\u2207 2 \u2211\ufe01 =1 ( ) \u2212 \u2207 2 \u2211\ufe01 =1 ( ))|| \u2264 1 ( +\u00af)( +\u00af)|| \u2212 || ST =||( \u2212 1 (\u2207 2 ( ) + \u2207 2 \u2211\ufe01 =1( ))\n\nB SKETCH OF PROOF OF THEOREM 2\n\nIn order to better understand Theorem 2, we first introduce Lemma 3 and its analysis is modified and analogous to that developed in [17]. using the inequality ( 1 + 2 + \u00b7 \u00b7 \u00b7 + ) 2 \u2264 ( 2 1 + 2 2 + \u00b7 \u00b7 \u00b7 + 2 ). By adding the inequalities of FT and ST, and using the fact || || \u2264 and \u2265 1 we complete the proof. \u25a1 By applying Lemma 3, we now prove Theorem 2.\n\nProof. By expanding Eq.(13) using Eq. (10), and in short we use RHS to substitute the right-hand side of the inequality of Eq. (13) and set = * . Following the Theorem 3.1 in [3], we have \n+ \u2211\ufe01 =1 =1 (A ( )) 2 + 2( 2 + 2 ) \u2212 \u2211\ufe01 =1 , (A ( * )) \u2264 2 2 2 \u2212 2 2 + 4 2 2 1 2 ( + 1) 2 + 2 2 + 4 ( 2 + 2 1 4 ) \u2211\ufe01 =1 2 2\nSince (A ( * )) \u2264 0 and , \u2265 0, \u2200 \u2208 [ ]. For ( ) to be strongly convex, in order to have lower upper bounds for both objective regret and the long-term constraint, we need to use timevarying stepsize as the one used in [28], that is 2 = /( + 1).\n\nDue to non-negative of In order to adapt online environment, all datasets are split into a sequence of tasks. However, numbers of data samples in each task may be small. Data augmentation is therefore used on the samples in the form of rotations of random degree. Specifically, for each data sample in a task, unprotected attributes are rotated for degrees, where is randomly selected from a range of [1,360]. Note that, for the new rotated data sample, its label and protected feature remain the same as before. Each task is enriched for a size of at least 2500. For all datasets, all the unprotected attributes are standardized to zero mean and unit variance and prepared for experiments.\n\n\nC.2 Implementation Details and Parameter Tuning\n\nOur neural network trained follows the same architecture used by [5], which contains 2 hidden layers of size of 40 with ReLU activation functions. In the training stage, each gradient is computed  Table 3. Initial primal meta parameters 1 of all baseline methods and proposed algorithms are randomly chosen, which means we train all  \n\n\nC.3 Additional Results\n\nIn order to reduce computational time in Algorithm 1, in stead of using all tasks, we approximate the results in practice with a fixed size of task buffer U. In other words, at each round, we add the new task to U if \u2264 |U|. However, when > |U|, we stochastically sample a batch of tasks from seen tasks. Figure 6 represents results based on the Bank dataset with various batch sizes. Our empirical results indicate that although better performance is able to achieved with higher batch size, on the contrary more expensive the experiments will be.  More ablation study results on Adult and Crime datasets are given in Figure 7. \n\nFigure 1 :\n1An overview of update procedure stated in Step 3. At round , new task is added in the the buffer. The parameter pair ( , ) are iteratively updated with fairness constraints through a bi-level optimization in which the inner and the outer interplay each other.\n\nAssumption 1 (\n1Convex domain). The convex set \u0398 is non-empty, closed, bounded, and can be described by convex functions as \u0398 = { : ( ) \u2264 0, \u2200 \u2208 [ ]} Assumption 2. Both the loss functions (\u00b7), \u2200 and constraint functions (\u00b7), \u2200 \u2208 [ ] satisfy the following assumptions (1) (Lipschitz continuous) ( ), \u2200 and ( ), \u2200 are Lipschitz continuous in B, that is, \u2200 , \u2208 B, || ( ) \u2212 ( )|| \u2264 || \u2212 ||, || ( ) \u2212 ( )|| \u2264 || \u2212 ||, and = max{ , }, gradient) ( ), \u2200 are -smooth and ( ), \u2200 aresmooth, that is, \u2200 , \u2208 B, ||\u2207 ( ) \u2212 \u2207 ( )|| \u2264 || \u2212 ||, ||\u2207 ( ) \u2212 \u2207 ( )|| \u2264 || \u2212 ||, and = max{ , }. (3) (Lipschitz Hessian) Twice-differentiable functions ( ), \u2200 and ( ), \u2200 have and -Lipschitz Hessian, respectively. That is, \u2200 , \u2208 B, ||\u2207 2 ( ) \u2212 \u2207 2 ( )|| \u2264 || \u2212 ||, ||\u2207 2 ( ) \u2212 \u2207 2 ( )|| \u2264 || \u2212 ||. Assumption 3 (Strongly convexity). Suppose ( ), \u2200 and ( ), \u2200 have strong convexity, that is, \u2200 , \u2208 B, ||\u2207 ( ) \u2212 \u2207 ( )|| \u2265 || \u2212 ||, ||\u2207 ( ) \u2212 \u2207 ( )|| \u2265 || \u2212 ||.We then analyze the proposed FFML algorithm and use one-step gradient update as an example. Under above assumptions, we first target Eq.(10) and state: Theorem 1. Suppose and : \u0398 \u00d7 R + \u2192 R satisfy Assumptions 1, 2 and 3. The inner level update and the augmented Lagrangian function L ( , ) are defined in Eq.(8)(7)and Eq.(10). Then, the function L ( , ) is convex-concave with respect to the arguments and , respectively. Furthermore, as for L (\u00b7, ), if stepsize 1 is selected as 1 \u2264 min{ +\u00af)-smooth and 1 8 ( +\u00af)-strongly convex, where\u00af\u2265 0 is the mean value of .\n\nTheorem 2 .\n2Set 2 = /( + 1) and choose such that \u2265 max{4 ( + 1) 2 }. If we follow the update rule in Eq.(11)(10) and * being the optimal solution for min \u2208\u0398 =1 (A ( )), we have upper bounds for both the regret on the loss and the cumulative constraint violation\u2211\ufe01 =1 (A ( )) \u2212 (A ( * )) \u2264 (log ) \u2211\ufe01 =1 (A ( )) \u2264 ( \u221a\ufe01 log ), \u2200 \u2208 [ ] Regret Discussion: Under aforementioned assumptions and provable convexity of Eq.(10) in (see Theorem 1), Algorithm 1 achieves sublinear bounds for both loss regret and violation of fairness constraints (see Theorem 2) where lim \u2192\u221e (\u00b7)/ = 0. Although such bounds are comparable with cutting-edge techniques of online learning with long-term constraints\n\nFigure 2 :\n2Evaluation using fair metric DP at each round.\n\nFigure 3 :\n3Evaluation using fair metric EO at each round.\n\nFigure 4 :Figure 5 :\n45Amount of data needed to learn each new task. Ablation study of our proposed models. (1) w/o inner FC: FFML without inner fairness constraints; (2) w/o aug: FFML without the augmented term in Eq.(10); (3) w/o aug + outer FC: FFML without the augmented term and outer fairness constraints.\n\n\n1 ( +\u00af)) 2 ( +\u00af)|| \u2212 || The inequality to bound ST is due to the Lemma 2 to 4 in [6]. Together the upper bounds for the first and the second terms and choose step size (\u00b7, ) is 9 8 ( +\u00af)-smooth. Next to achieve lower bound forL (\u00b7, ) ||\u2207L ( ) \u2212 \u2207L ( )|| \u2265 ||\u2207 A ( ) (\u2207 \u2032 L \u2032 (A ( )) \u2212 \u2207 \u2032 L \u2032 (A ( )))|| Third Term (TT) \u2212 ||\u2207 \u2032 L \u2032 (A ( ))(\u2207 A ( ) \u2212 \u2207 A ( ))||The second term in the above inequality is the same as the FT. We hence bound the TT that:TT \u2265 (1 \u2212 1 ( +\u00af)) 2 ( +\u00af)|| \u2212\n\nLemma 3 .\n3Let L (\u00b7, \u00b7) be the function defined in Eq.(10), which is convex in its first argument and concave in its second argument. Let and , \u2208 [ ] be the sequence of solution obtained by Algorithm 1. Then for any ( , ) \u2208 B \u00d7 R + , we have \u2211\ufe01 =1 L ( , ) \u2212 L ( , + 1) 2 (1 + || || 4 ) and \u2207 L | , || 2 \u2264 4 (\n\n2\n( + 1) 2 }, we can drop terms containing || || 2 and || || 4 . By taking maximization for over (0, +\u221e),\n\n\nusing a batch size of 200 examples where each binary class contains 100 examples. For each dataset, we tune the folowing hyperparameters: (1) learning rates 1 , 2 for updating inner and outer parameters in Eq.(8)(9) and (11)(12), (2) task buffer size |U|, (3) some positive constant used in the augmented term in Eq.(10), (4) inner gradient steps , and (5) the number of outer iterations . Hyperparameter configurations for all datasets are summarized in\n\n\nfrom a random point. The initial dual meta parameters 1 is chosen from {0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0}. Parameters 1 and 2 control the inner and outer learning rates are chosen from {0.0001,\n\nFigure 6 :\n6FFML performance across various batch sizes.\n\nFigure 7 :\n7Ablation study of FFML on Adult and Crime datasets.\n\nTable 1 :\n1Important notations and corresponding descriptions.Notations \nDescriptions \nTotal number of learning tasks \nIndices of tasks \nD , D , D \nSupport set, validation set, and query set of data \nD \n, \nMeta primal and dual parameters \n, \nModel primal and dual parameters of task \n(\u00b7) \nLoss function of at round \n(\u00b7) \nFairness function \nTotal number of fairness notions \nIndices of fairness notions \nA (\u00b7) \nBase learner \nU \nTask buffer \nIndices of past tasks in U \nB \nRelaxed primal domain \n\nB \n\nProjection operation to domain B \n1 , 2 \nLearning rates \nAugmented constant \n\n\n\n1 :\n1Randomly initialize primal and dual meta-parameters,( 1 , 1 ) \u2208 (\u0398 \u00d7 R + ) Append D to task buffer U \u2190 U + [D ] for each task D in U where \u2208 [ ] do2: Initialize the task buffer U \u2190 [] \n3: for = 1, 2, ..., do \n\n4: \n\nSample D \u2286 D from task \n\n5: \n\nEvaluate performance of task using D and \n\n6: \n\n7: \n\nif \u2260 1 then \n\n8: \n\nInitialize \n=0 \u2190 \nand \n=0 \u2190 \n\n9: \n\nwhile \n= 1, 2, ... do \n\n10: \n\n11: \n\n\u2190 \n\u22121 ; \n\u2190 \n\n\u22121 \n\n12: \n\nSample datapoints D \u2282 D \n\n13: \n\n\n\nTable 2 :\n2End task performance on real datasets over all baseline methods. Evaluation metrics with \"\u2191\" indicates the bigger the better and \"\u2193\" indicates the smaller the better. Best performance are labeled in bold.Dataset \n\nDP \u2191 \nEO \u2191 \nDisc \u2193 \nAcc(%) \u2191 \nTWP / m-FTML[6] / OGDLC[17] / AdpOLC[12] / GenOLC[28] / FFML(Ours) \nBank 0.09/0.68/0.36/0.76/0.81/0.97 0.11/0.65/0.35/0.72/0.78/0.96 0.19/0.72/0.23/0.22/0.11/0.07 52.14/53.69/52.36/54.89/56.78/52.55 \nAdult 0.05/0.54/0.41/0.60/0.78/0.91 0.04/0.43/0.30/0.62/0.69/0.87 0.32/0.69/0.21/0.19/0.12/0.10 51.21/67.91/52.31/48.36/47.87/61.35 \nCrime 0.40/0.38/0.48/0.68/0.70/0.74 0.38/0.29/0.39/0.43/0.64/0.69 0.23/0.78/0.25/0.31/0.15/0.17 51.23/48.69/49.10/58.89/49.43/59.57 \n\n\n\nTable 3 :\n3Hyperparameter configurations of FFML.\n\nEqual Opportunity in Online Classification with Partial Feedback. Yahav Bechavod, Katrina Ligett, Aaron Roth, Bo Waggoner, Zhiwei Wu, NeurIPS. Yahav Bechavod, Katrina Ligett, Aaron Roth, Bo Waggoner, and Zhiwei Wu. 2019. Equal Opportunity in Online Classification with Partial Feedback. NeurIPS (02 2019).\n\nBuilding Classifiers with Independency Constraints. Toon Calders, Faisal Kamiran, Mykola Pechenizkiy, 10.1109/ICDMW.2009.832009 IEEE International Conference on Data Mining Workshops. 13-18. Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. 2009. Building Classifiers with Independency Constraints. In 2009 IEEE International Conference on Data Mining Workshops. 13-18. https://doi.org/10.1109/ICDMW.2009.83\n\nNicol\u00f2 Cesa, - Bianchi, G\u00e1bor Lugosi, Prediction, Learning, and Games. Cambridge University PressNicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi. 2006. Prediction, Learning, and Games. Cambridge University Press (2006).\n\n. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Rich Zemel, Fairness Through Awareness. CoRRCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel. 2011. Fairness Through Awareness. CoRR (2011).\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta- Learning for Fast Adaptation of Deep Networks. ICML (2017).\n\n. Chelsea Finn, Aravind Rajeswaran, Sham Kakade, Sergey Levine, Online Meta-Learning. ICML. Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. 2019. Online Meta-Learning. ICML (2019).\n\nProbabilistic model-agnostic meta-learning. Chelsea Finn, Kelvin Xu, Sergey Levine, NeurIPS. Chelsea Finn, Kelvin Xu, and Sergey Levine. 2018. Probabilistic model-agnostic meta-learning. In NeurIPS. 9516-9527.\n\nOnline Learning with an Unknown Fairness Metric. Stephen Gillen, Christopher Jung, Michael Kearns, Aaron Roth, NeurIPS. Stephen Gillen, Christopher Jung, Michael Kearns, and Aaron Roth. 2018. Online Learning with an Unknown Fairness Metric. NeurIPS (2018).\n\nApproximation to bayes risk in repeated play. James Hannan, Contributions to the Theory of Games. James Hannan. 1957. Approximation to bayes risk in repeated play. Contributions to the Theory of Games (1957).\n\nEquality of opportunity in supervised learning. Moritz Hardt, Eric Price, Nathan Srebro, NeurIPS. Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of opportunity in supervised learning. NeurIPS (2016).\n\nSteven Hoi, Doyen Sahoo, Jing Lu, Peilin Zhao, arXiv:1802.02871[cs.LG](02Online Learning: A Comprehensive Survey. Steven Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. 2018. Online Learning: A Comprehensive Survey. arXiv:1802.02871 [cs.LG] (02 2018).\n\nAdaptive Algorithms for Online Convex Optimization with Long-term Constraints. Rodolphe Jenatton, Jim Huang, Cedric Archambeau, ICML. Rodolphe Jenatton, Jim Huang, and Cedric Archambeau. 2016. Adaptive Al- gorithms for Online Convex Optimization with Long-term Constraints. ICML (2016).\n\n. Ronny Kohavi, Barry Becker, UCI Machine Learning Repository. Ronny Kohavi and Barry Becker. 1994. UCI Machine Learning Repository. (1994).\n\niFair: Learning Individually Fair Data Representations for Algorithmic Decision Making. Preethi Lahoti, Gerhard Weikum, Krishna Gummadi, Preethi Lahoti, Gerhard Weikum, and Krishna Gummadi. 2019. iFair: Learning Individually Fair Data Representations for Algorithmic Decision Making. ICDE.\n\nUCI Machine Learning Repository. Moshe Lichman, Moshe Lichman. 2013. UCI Machine Learning Repository. (2013).\n\nToo Relaxed to Be Fair. Michael Lohaus, Michael Perrot, Ulrike Von Luxburg, ICML. Michael Lohaus, Michael Perrot, and Ulrike Von Luxburg. 2020. Too Relaxed to Be Fair. In ICML.\n\nTrading regret for efficiency: online convex optimization with long term constraints. Mehrdad Mahdavi, Rong Jin, Tianbao Yang, JMLR. Mehrdad Mahdavi, Rong Jin, and Tianbao Yang. 2012. Trading regret for efficiency: online convex optimization with long term constraints. JMLR (2012).\n\nIs an Algorithm Less Racist Than a Loan Officer?. Jennifer Miller, Jennifer Miller. 2020. Is an Algorithm Less Racist Than a Loan Officer? www.nytimes.com/2020/09/18/business/digital-mortgages.html (2020).\n\nA data-driven approach to predict the success of bank telemarketing. S\u00e9rgio Moro, Paulo Cortez, Paulo Rita, DSS. S\u00e9rgio Moro, Paulo Cortez, and Paulo Rita. 2014. A data-driven approach to predict the success of bank telemarketing. DSS (2014).\n\nDeep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL. Anusha Nagabandi, Chelsea Finn, Sergey Levine, ICLR. Anusha Nagabandi, Chelsea Finn, and Sergey Levine. 2019. Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL. ICLR (2019).\n\nAchieving Fairness in the Stochastic Multi-Armed Bandit Problem. Vishakha Patil, Ganesh Ghalme, Vineet Nair, Yadati Narahari, AAAIVishakha Patil, Ganesh Ghalme, Vineet Nair, and Yadati Narahari. 2020. Achieving Fairness in the Stochastic Multi-Armed Bandit Problem. AAAI (2020).\n\nMeta-learning with latent embedding optimization. Dushyant Andrei A Rusu, Jakub Rao, Oriol Sygnowski, Razvan Vinyals, Simon Pascanu, Raia Osindero, Hadsell, Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. 2019. Meta-learning with latent embedding optimization. ICLR (2019).\n\n. Jurgen Schmidhuber, Evolutionary Principles in Self-Referential Learning. Jurgen Schmidhuber. 1987. Evolutionary Principles in Self-Referential Learning. (1987).\n\nFairness Warnings and Fair-MAML: Learning Fairly with Minimal Data. Dylan Slack, Sorelle Friedler, Emile Givental, ACM FAccTDylan Slack, Sorelle Friedler, and Emile Givental. 2020. Fairness Warnings and Fair-MAML: Learning Fairly with Minimal Data. ACM FAccT (2020).\n\nCLEAR: Contrastive-Prototype Learning with Drift Estimation for Resource Constrained Stream Mining. Zhuoyi Wang, Yuqiao Chen, Chen Zhao, Yu Lin, Xujiang Zhao, Hemeng Tao, Yigong Wang, Latifur Khan, WWW. Zhuoyi Wang, Yuqiao Chen, Chen Zhao, Yu Lin, Xujiang Zhao, Hemeng Tao, Yigong Wang, and Latifur Khan. 2021. CLEAR: Contrastive-Prototype Learning with Drift Estimation for Resource Constrained Stream Mining. In WWW.\n\nOnline Structured Meta-learning. Huaxiu Yao, Yingbo Zhou, Mehrdad Mahdavi, Zhenhui Li, Richard Socher, Caiming Xiong, NeurIPS. Huaxiu Yao, Yingbo Zhou, Mehrdad Mahdavi, Zhenhui Li, Richard Socher, and Caiming Xiong. 2020. Online Structured Meta-learning. NeurIPS (2020).\n\nOnline Convex Optimization with Stochastic Constraints. Hao Yu, Michael J Neely, Xiaohan Wei, In NeurIPSHao Yu, Michael J. Neely, and Xiaohan Wei. 2017. Online Convex Optimization with Stochastic Constraints. In NeurIPS.\n\nOnline convex optimization for cumulative constraints. Jianjun Yuan, Andrew Lamperski, NeurIPS. Jianjun Yuan and Andrew Lamperski. 2018. Online convex optimization for cumulative constraints. NeurIPS (2018).\n\nFairness Constraints: Mechanisms for Fair Classification. Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P Gummadi, AIS-TATSMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. 2017. Fairness Constraints: Mechanisms for Fair Classification. AIS- TATS (2017).\n\n. Richard Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, Cynthia Dwork, Learning Fair Representations. ICML. Richard Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork. 2013. Learning Fair Representations. ICML (2013).\n\nRank-Based Multi-task Learning For Fair Regression. Chen Zhao, Feng Chen, IEEE International Conference on Data Mining (ICDM. Chen Zhao and Feng Chen. 2019. Rank-Based Multi-task Learning For Fair Regression. IEEE International Conference on Data Mining (ICDM) (2019).\n\nUnfairness Discovery and Prevention For Few-Shot Regression. Chen Zhao, Feng Chen, ICKG. Chen Zhao and Feng Chen. 2020. Unfairness Discovery and Prevention For Few-Shot Regression. ICKG (2020).\n\nA Primal-Dual Subgradient Approach for Fair Meta Learning. Chen Zhao, Feng Chen, Zhuoyi Wang, Latifur Khan, ICDM. Chen Zhao, Feng Chen, Zhuoyi Wang, and Latifur Khan. 2020. A Primal-Dual Subgradient Approach for Fair Meta Learning. In ICDM.\n\nFair Meta-Learning For Few-Shot Classification. Chen Zhao, Changbin Li, Jincheng Li, Feng Chen, ICKG. Chen Zhao, Changbin Li, Jincheng Li, and Feng Chen. 2020. Fair Meta-Learning For Few-Shot Classification. ICKG (2020).\n", "annotations": {"author": "[{\"end\":96,\"start\":63},{\"end\":130,\"start\":97},{\"end\":188,\"start\":131},{\"end\":199,\"start\":189},{\"end\":210,\"start\":200},{\"end\":233,\"start\":211},{\"end\":289,\"start\":234},{\"end\":344,\"start\":290},{\"end\":399,\"start\":345}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":68},{\"end\":106,\"start\":102},{\"end\":152,\"start\":139},{\"end\":198,\"start\":194},{\"end\":209,\"start\":205},{\"end\":232,\"start\":219}]", "author_first_name": "[{\"end\":67,\"start\":63},{\"end\":101,\"start\":97},{\"end\":138,\"start\":131},{\"end\":193,\"start\":189},{\"end\":204,\"start\":200},{\"end\":218,\"start\":211}]", "author_affiliation": "[{\"end\":288,\"start\":235},{\"end\":343,\"start\":291},{\"end\":398,\"start\":346}]", "title": "[{\"end\":36,\"start\":1},{\"end\":435,\"start\":400}]", "venue": "[{\"end\":531,\"start\":437}]", "abstract": "[{\"end\":3076,\"start\":1498}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4073,\"start\":4069},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4521,\"start\":4518},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4684,\"start\":4680},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4805,\"start\":4802},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5400,\"start\":5397},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5499,\"start\":5495},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5972,\"start\":5968},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8649,\"start\":8646},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8857,\"start\":8853},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9042,\"start\":9039},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9305,\"start\":9301},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9642,\"start\":9639},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9645,\"start\":9642},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9648,\"start\":9645},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9652,\"start\":9648},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9656,\"start\":9652},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9660,\"start\":9656},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9664,\"start\":9660},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9789,\"start\":9786},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9837,\"start\":9834},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9882,\"start\":9878},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10031,\"start\":10027},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10872,\"start\":10868},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11054,\"start\":11050},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11096,\"start\":11092},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11155,\"start\":11151},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13859,\"start\":13855},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13909,\"start\":13905},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13912,\"start\":13909},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14749,\"start\":14746},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14808,\"start\":14804},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14925,\"start\":14921},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15037,\"start\":15033},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16023,\"start\":16020},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16435,\"start\":16431},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16438,\"start\":16435},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16441,\"start\":16438},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17095,\"start\":17092},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17769,\"start\":17765},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18150,\"start\":18146},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18158,\"start\":18155},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18443,\"start\":18440},{\"end\":21577,\"start\":21574},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21590,\"start\":21588},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22020,\"start\":22017},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22222,\"start\":22219},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22947,\"start\":22943},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24545,\"start\":24542},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24554,\"start\":24550},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24920,\"start\":24916},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25093,\"start\":25089},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25096,\"start\":25093},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26080,\"start\":26076},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26437,\"start\":26433},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26806,\"start\":26802},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26931,\"start\":26927},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27454,\"start\":27451},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27483,\"start\":27479},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27771,\"start\":27767},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28407,\"start\":28404},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28490,\"start\":28487},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28884,\"start\":28881},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28900,\"start\":28896},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28971,\"start\":28967},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29049,\"start\":29045},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30139,\"start\":30136},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30460,\"start\":30457},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32729,\"start\":32726},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34039,\"start\":34035},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34048,\"start\":34044},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34108,\"start\":34104},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35189,\"start\":35186},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35718,\"start\":35714},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38151,\"start\":38147},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38244,\"start\":38241},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39046,\"start\":39042},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39309,\"start\":39305},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":39398,\"start\":39394},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":39445,\"start\":39442},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39801,\"start\":39797},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40229,\"start\":40226},{\"end\":40233,\"start\":40229},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":40635,\"start\":40632},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":43087,\"start\":43083}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41829,\"start\":41557},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43325,\"start\":41830},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44012,\"start\":43326},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44072,\"start\":44013},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44132,\"start\":44073},{\"attributes\":{\"id\":\"fig_8\"},\"end\":44445,\"start\":44133},{\"attributes\":{\"id\":\"fig_10\"},\"end\":44928,\"start\":44446},{\"attributes\":{\"id\":\"fig_11\"},\"end\":45238,\"start\":44929},{\"attributes\":{\"id\":\"fig_12\"},\"end\":45345,\"start\":45239},{\"attributes\":{\"id\":\"fig_14\"},\"end\":45802,\"start\":45346},{\"attributes\":{\"id\":\"fig_15\"},\"end\":46025,\"start\":45803},{\"attributes\":{\"id\":\"fig_17\"},\"end\":46083,\"start\":46026},{\"attributes\":{\"id\":\"fig_18\"},\"end\":46148,\"start\":46084},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46727,\"start\":46149},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":47178,\"start\":46728},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":47902,\"start\":47179},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47953,\"start\":47903}]", "paragraph": "[{\"end\":3571,\"start\":3092},{\"end\":4685,\"start\":3573},{\"end\":5191,\"start\":4687},{\"end\":5713,\"start\":5193},{\"end\":6321,\"start\":5715},{\"end\":7469,\"start\":6323},{\"end\":8423,\"start\":7471},{\"end\":10519,\"start\":8440},{\"end\":11328,\"start\":10521},{\"end\":11825,\"start\":11330},{\"end\":12817,\"start\":11857},{\"end\":13860,\"start\":12848},{\"end\":14166,\"start\":13862},{\"end\":14371,\"start\":14197},{\"end\":14534,\"start\":14373},{\"end\":15313,\"start\":14550},{\"end\":16101,\"start\":15350},{\"end\":16442,\"start\":16136},{\"end\":16789,\"start\":16444},{\"end\":18211,\"start\":16912},{\"end\":18563,\"start\":18213},{\"end\":18696,\"start\":18579},{\"end\":19083,\"start\":18698},{\"end\":19598,\"start\":19122},{\"end\":19962,\"start\":19600},{\"end\":20120,\"start\":19964},{\"end\":20253,\"start\":20215},{\"end\":20784,\"start\":20325},{\"end\":21127,\"start\":20811},{\"end\":21267,\"start\":21129},{\"end\":21472,\"start\":21340},{\"end\":21591,\"start\":21474},{\"end\":21639,\"start\":21629},{\"end\":22100,\"start\":21672},{\"end\":22307,\"start\":22102},{\"end\":22699,\"start\":22371},{\"end\":23030,\"start\":22754},{\"end\":23288,\"start\":23124},{\"end\":23863,\"start\":23290},{\"end\":24459,\"start\":23865},{\"end\":25350,\"start\":24472},{\"end\":25521,\"start\":25366},{\"end\":25895,\"start\":25523},{\"end\":26421,\"start\":25908},{\"end\":26774,\"start\":26423},{\"end\":27262,\"start\":26776},{\"end\":27425,\"start\":27285},{\"end\":27509,\"start\":27427},{\"end\":27750,\"start\":27573},{\"end\":27852,\"start\":27752},{\"end\":28096,\"start\":27888},{\"end\":29549,\"start\":28118},{\"end\":30304,\"start\":29562},{\"end\":31021,\"start\":30306},{\"end\":31127,\"start\":31033},{\"end\":32456,\"start\":31152},{\"end\":34241,\"start\":32491},{\"end\":34688,\"start\":34243},{\"end\":35410,\"start\":34717},{\"end\":35971,\"start\":35431},{\"end\":36290,\"start\":35973},{\"end\":37728,\"start\":36321},{\"end\":37853,\"start\":37748},{\"end\":38152,\"start\":37888},{\"end\":38734,\"start\":38211},{\"end\":39265,\"start\":38910},{\"end\":39455,\"start\":39267},{\"end\":39823,\"start\":39579},{\"end\":40515,\"start\":39825},{\"end\":40901,\"start\":40567},{\"end\":41556,\"start\":40928}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14196,\"start\":14167},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14549,\"start\":14535},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16135,\"start\":16102},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16911,\"start\":16790},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20214,\"start\":20121},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20324,\"start\":20254},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21306,\"start\":21268},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21628,\"start\":21592},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21671,\"start\":21640},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22342,\"start\":22308},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22370,\"start\":22342},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22753,\"start\":22700},{\"attributes\":{\"id\":\"formula_12\"},\"end\":23123,\"start\":23031},{\"attributes\":{\"id\":\"formula_13\"},\"end\":27572,\"start\":27510},{\"attributes\":{\"id\":\"formula_14\"},\"end\":27887,\"start\":27853},{\"attributes\":{\"id\":\"formula_15\"},\"end\":38210,\"start\":38153},{\"attributes\":{\"id\":\"formula_16\"},\"end\":38876,\"start\":38735},{\"attributes\":{\"id\":\"formula_17\"},\"end\":39578,\"start\":39456}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12815,\"start\":12808},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31902,\"start\":31895},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40771,\"start\":40764}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3090,\"start\":3078},{\"attributes\":{\"n\":\"2\"},\"end\":8438,\"start\":8426},{\"attributes\":{\"n\":\"3\"},\"end\":11855,\"start\":11828},{\"attributes\":{\"n\":\"3.2\"},\"end\":12846,\"start\":12820},{\"attributes\":{\"n\":\"3.3\"},\"end\":15348,\"start\":15316},{\"attributes\":{\"n\":\"4\"},\"end\":18577,\"start\":18566},{\"attributes\":{\"n\":\"4.1\"},\"end\":19120,\"start\":19086},{\"attributes\":{\"n\":\"4.2\"},\"end\":20809,\"start\":20787},{\"end\":21338,\"start\":21308},{\"attributes\":{\"n\":\"5\"},\"end\":24470,\"start\":24462},{\"attributes\":{\"n\":\"6\"},\"end\":25364,\"start\":25353},{\"attributes\":{\"n\":\"6.1\"},\"end\":25906,\"start\":25898},{\"attributes\":{\"n\":\"6.2\"},\"end\":27283,\"start\":27265},{\"attributes\":{\"n\":\"6.3\"},\"end\":28116,\"start\":28099},{\"attributes\":{\"n\":\"6.4\"},\"end\":29560,\"start\":29552},{\"attributes\":{\"n\":\"7\"},\"end\":31031,\"start\":31024},{\"attributes\":{\"n\":\"7.1\"},\"end\":31150,\"start\":31130},{\"attributes\":{\"n\":\"7.2\"},\"end\":32489,\"start\":32459},{\"attributes\":{\"n\":\"7.3\"},\"end\":34715,\"start\":34691},{\"attributes\":{\"n\":\"7.4\"},\"end\":35429,\"start\":35413},{\"attributes\":{\"n\":\"8\"},\"end\":36319,\"start\":36293},{\"end\":37746,\"start\":37731},{\"end\":37886,\"start\":37856},{\"end\":38908,\"start\":38878},{\"end\":40565,\"start\":40518},{\"end\":40926,\"start\":40904},{\"end\":41568,\"start\":41558},{\"end\":41845,\"start\":41831},{\"end\":43338,\"start\":43327},{\"end\":44024,\"start\":44014},{\"end\":44084,\"start\":44074},{\"end\":44154,\"start\":44134},{\"end\":44939,\"start\":44930},{\"end\":45241,\"start\":45240},{\"end\":46037,\"start\":46027},{\"end\":46095,\"start\":46085},{\"end\":46159,\"start\":46150},{\"end\":46732,\"start\":46729},{\"end\":47189,\"start\":47180},{\"end\":47913,\"start\":47904}]", "table": "[{\"end\":46727,\"start\":46212},{\"end\":47178,\"start\":46881},{\"end\":47902,\"start\":47395}]", "figure_caption": "[{\"end\":41829,\"start\":41570},{\"end\":43325,\"start\":41847},{\"end\":44012,\"start\":43340},{\"end\":44072,\"start\":44026},{\"end\":44132,\"start\":44086},{\"end\":44445,\"start\":44157},{\"end\":44928,\"start\":44448},{\"end\":45238,\"start\":44941},{\"end\":45345,\"start\":45242},{\"end\":45802,\"start\":45348},{\"end\":46025,\"start\":45805},{\"end\":46083,\"start\":46039},{\"end\":46148,\"start\":46097},{\"end\":46212,\"start\":46161},{\"end\":46881,\"start\":46734},{\"end\":47395,\"start\":47191},{\"end\":47953,\"start\":47915}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23862,\"start\":23854},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32844,\"start\":32836},{\"end\":35236,\"start\":35227},{\"end\":36001,\"start\":35993},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":41240,\"start\":41232},{\"attributes\":{\"ref_id\":\"fig_18\"},\"end\":41554,\"start\":41546}]", "bib_author_first_name": "[{\"end\":48026,\"start\":48021},{\"end\":48044,\"start\":48037},{\"end\":48058,\"start\":48053},{\"end\":48067,\"start\":48065},{\"end\":48084,\"start\":48078},{\"end\":48318,\"start\":48314},{\"end\":48334,\"start\":48328},{\"end\":48350,\"start\":48344},{\"end\":48681,\"start\":48675},{\"end\":48689,\"start\":48688},{\"end\":48704,\"start\":48699},{\"end\":48894,\"start\":48887},{\"end\":48908,\"start\":48902},{\"end\":48923,\"start\":48916},{\"end\":48937,\"start\":48933},{\"end\":48952,\"start\":48948},{\"end\":49191,\"start\":49184},{\"end\":49204,\"start\":49198},{\"end\":49219,\"start\":49213},{\"end\":49373,\"start\":49366},{\"end\":49387,\"start\":49380},{\"end\":49404,\"start\":49400},{\"end\":49419,\"start\":49413},{\"end\":49615,\"start\":49608},{\"end\":49628,\"start\":49622},{\"end\":49639,\"start\":49633},{\"end\":49831,\"start\":49824},{\"end\":49851,\"start\":49840},{\"end\":49865,\"start\":49858},{\"end\":49879,\"start\":49874},{\"end\":50084,\"start\":50079},{\"end\":50297,\"start\":50291},{\"end\":50309,\"start\":50305},{\"end\":50323,\"start\":50317},{\"end\":50463,\"start\":50457},{\"end\":50474,\"start\":50469},{\"end\":50486,\"start\":50482},{\"end\":50497,\"start\":50491},{\"end\":50793,\"start\":50785},{\"end\":50807,\"start\":50804},{\"end\":50821,\"start\":50815},{\"end\":51001,\"start\":50996},{\"end\":51015,\"start\":51010},{\"end\":51231,\"start\":51224},{\"end\":51247,\"start\":51240},{\"end\":51263,\"start\":51256},{\"end\":51465,\"start\":51460},{\"end\":51569,\"start\":51562},{\"end\":51585,\"start\":51578},{\"end\":51600,\"start\":51594},{\"end\":51604,\"start\":51601},{\"end\":51809,\"start\":51802},{\"end\":51823,\"start\":51819},{\"end\":51836,\"start\":51829},{\"end\":52058,\"start\":52050},{\"end\":52282,\"start\":52276},{\"end\":52294,\"start\":52289},{\"end\":52308,\"start\":52303},{\"end\":52538,\"start\":52532},{\"end\":52557,\"start\":52550},{\"end\":52570,\"start\":52564},{\"end\":52810,\"start\":52802},{\"end\":52824,\"start\":52818},{\"end\":52839,\"start\":52833},{\"end\":52852,\"start\":52846},{\"end\":53075,\"start\":53067},{\"end\":53096,\"start\":53091},{\"end\":53107,\"start\":53102},{\"end\":53125,\"start\":53119},{\"end\":53140,\"start\":53135},{\"end\":53154,\"start\":53150},{\"end\":53363,\"start\":53357},{\"end\":53593,\"start\":53588},{\"end\":53608,\"start\":53601},{\"end\":53624,\"start\":53619},{\"end\":53894,\"start\":53888},{\"end\":53907,\"start\":53901},{\"end\":53918,\"start\":53914},{\"end\":53927,\"start\":53925},{\"end\":53940,\"start\":53933},{\"end\":53953,\"start\":53947},{\"end\":53965,\"start\":53959},{\"end\":53979,\"start\":53972},{\"end\":54247,\"start\":54241},{\"end\":54259,\"start\":54253},{\"end\":54273,\"start\":54266},{\"end\":54290,\"start\":54283},{\"end\":54302,\"start\":54295},{\"end\":54318,\"start\":54311},{\"end\":54539,\"start\":54536},{\"end\":54551,\"start\":54544},{\"end\":54553,\"start\":54552},{\"end\":54568,\"start\":54561},{\"end\":54764,\"start\":54757},{\"end\":54777,\"start\":54771},{\"end\":54977,\"start\":54969},{\"end\":54997,\"start\":54991},{\"end\":55012,\"start\":55006},{\"end\":55018,\"start\":55013},{\"end\":55037,\"start\":55030},{\"end\":55039,\"start\":55038},{\"end\":55234,\"start\":55227},{\"end\":55244,\"start\":55242},{\"end\":55254,\"start\":55249},{\"end\":55271,\"start\":55264},{\"end\":55288,\"start\":55281},{\"end\":55513,\"start\":55509},{\"end\":55524,\"start\":55520},{\"end\":55792,\"start\":55788},{\"end\":55803,\"start\":55799},{\"end\":55985,\"start\":55981},{\"end\":55996,\"start\":55992},{\"end\":56009,\"start\":56003},{\"end\":56023,\"start\":56016},{\"end\":56216,\"start\":56212},{\"end\":56231,\"start\":56223},{\"end\":56244,\"start\":56236},{\"end\":56253,\"start\":56249}]", "bib_author_last_name": "[{\"end\":48035,\"start\":48027},{\"end\":48051,\"start\":48045},{\"end\":48063,\"start\":48059},{\"end\":48076,\"start\":48068},{\"end\":48087,\"start\":48085},{\"end\":48326,\"start\":48319},{\"end\":48342,\"start\":48335},{\"end\":48362,\"start\":48351},{\"end\":48686,\"start\":48682},{\"end\":48697,\"start\":48690},{\"end\":48711,\"start\":48705},{\"end\":48900,\"start\":48895},{\"end\":48914,\"start\":48909},{\"end\":48931,\"start\":48924},{\"end\":48946,\"start\":48938},{\"end\":48958,\"start\":48953},{\"end\":49196,\"start\":49192},{\"end\":49211,\"start\":49205},{\"end\":49226,\"start\":49220},{\"end\":49378,\"start\":49374},{\"end\":49398,\"start\":49388},{\"end\":49411,\"start\":49405},{\"end\":49426,\"start\":49420},{\"end\":49620,\"start\":49616},{\"end\":49631,\"start\":49629},{\"end\":49646,\"start\":49640},{\"end\":49838,\"start\":49832},{\"end\":49856,\"start\":49852},{\"end\":49872,\"start\":49866},{\"end\":49884,\"start\":49880},{\"end\":50091,\"start\":50085},{\"end\":50303,\"start\":50298},{\"end\":50315,\"start\":50310},{\"end\":50330,\"start\":50324},{\"end\":50467,\"start\":50464},{\"end\":50480,\"start\":50475},{\"end\":50489,\"start\":50487},{\"end\":50502,\"start\":50498},{\"end\":50802,\"start\":50794},{\"end\":50813,\"start\":50808},{\"end\":50832,\"start\":50822},{\"end\":51008,\"start\":51002},{\"end\":51022,\"start\":51016},{\"end\":51238,\"start\":51232},{\"end\":51254,\"start\":51248},{\"end\":51271,\"start\":51264},{\"end\":51473,\"start\":51466},{\"end\":51576,\"start\":51570},{\"end\":51592,\"start\":51586},{\"end\":51612,\"start\":51605},{\"end\":51817,\"start\":51810},{\"end\":51827,\"start\":51824},{\"end\":51841,\"start\":51837},{\"end\":52065,\"start\":52059},{\"end\":52287,\"start\":52283},{\"end\":52301,\"start\":52295},{\"end\":52313,\"start\":52309},{\"end\":52548,\"start\":52539},{\"end\":52562,\"start\":52558},{\"end\":52577,\"start\":52571},{\"end\":52816,\"start\":52811},{\"end\":52831,\"start\":52825},{\"end\":52844,\"start\":52840},{\"end\":52861,\"start\":52853},{\"end\":53089,\"start\":53076},{\"end\":53100,\"start\":53097},{\"end\":53117,\"start\":53108},{\"end\":53133,\"start\":53126},{\"end\":53148,\"start\":53141},{\"end\":53163,\"start\":53155},{\"end\":53172,\"start\":53165},{\"end\":53375,\"start\":53364},{\"end\":53599,\"start\":53594},{\"end\":53617,\"start\":53609},{\"end\":53633,\"start\":53625},{\"end\":53899,\"start\":53895},{\"end\":53912,\"start\":53908},{\"end\":53923,\"start\":53919},{\"end\":53931,\"start\":53928},{\"end\":53945,\"start\":53941},{\"end\":53957,\"start\":53954},{\"end\":53970,\"start\":53966},{\"end\":53984,\"start\":53980},{\"end\":54251,\"start\":54248},{\"end\":54264,\"start\":54260},{\"end\":54281,\"start\":54274},{\"end\":54293,\"start\":54291},{\"end\":54309,\"start\":54303},{\"end\":54324,\"start\":54319},{\"end\":54542,\"start\":54540},{\"end\":54559,\"start\":54554},{\"end\":54572,\"start\":54569},{\"end\":54769,\"start\":54765},{\"end\":54787,\"start\":54778},{\"end\":54989,\"start\":54978},{\"end\":55004,\"start\":54998},{\"end\":55028,\"start\":55019},{\"end\":55047,\"start\":55040},{\"end\":55240,\"start\":55235},{\"end\":55247,\"start\":55245},{\"end\":55262,\"start\":55255},{\"end\":55279,\"start\":55272},{\"end\":55294,\"start\":55289},{\"end\":55518,\"start\":55514},{\"end\":55529,\"start\":55525},{\"end\":55797,\"start\":55793},{\"end\":55808,\"start\":55804},{\"end\":55990,\"start\":55986},{\"end\":56001,\"start\":55997},{\"end\":56014,\"start\":56010},{\"end\":56028,\"start\":56024},{\"end\":56221,\"start\":56217},{\"end\":56234,\"start\":56232},{\"end\":56247,\"start\":56245},{\"end\":56258,\"start\":56254}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":59606250},\"end\":48260,\"start\":47955},{\"attributes\":{\"doi\":\"10.1109/ICDMW.2009.83\",\"id\":\"b1\",\"matched_paper_id\":3945595},\"end\":48673,\"start\":48262},{\"attributes\":{\"id\":\"b2\"},\"end\":48883,\"start\":48675},{\"attributes\":{\"id\":\"b3\"},\"end\":49115,\"start\":48885},{\"attributes\":{\"id\":\"b4\"},\"end\":49362,\"start\":49117},{\"attributes\":{\"id\":\"b5\"},\"end\":49562,\"start\":49364},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":46977722},\"end\":49773,\"start\":49564},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3426898},\"end\":50031,\"start\":49775},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":123624867},\"end\":50241,\"start\":50033},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7567061},\"end\":50455,\"start\":50243},{\"attributes\":{\"doi\":\"arXiv:1802.02871[cs.LG](02\",\"id\":\"b10\"},\"end\":50704,\"start\":50457},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1405098},\"end\":50992,\"start\":50706},{\"attributes\":{\"id\":\"b12\"},\"end\":51134,\"start\":50994},{\"attributes\":{\"id\":\"b13\"},\"end\":51425,\"start\":51136},{\"attributes\":{\"id\":\"b14\"},\"end\":51536,\"start\":51427},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220209240},\"end\":51714,\"start\":51538},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":104743},\"end\":51998,\"start\":51716},{\"attributes\":{\"id\":\"b17\"},\"end\":52205,\"start\":52000},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14181100},\"end\":52449,\"start\":52207},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":56475888},\"end\":52735,\"start\":52451},{\"attributes\":{\"id\":\"b20\"},\"end\":53015,\"start\":52737},{\"attributes\":{\"id\":\"b21\"},\"end\":53353,\"start\":53017},{\"attributes\":{\"id\":\"b22\"},\"end\":53518,\"start\":53355},{\"attributes\":{\"id\":\"b23\"},\"end\":53786,\"start\":53520},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":235324755},\"end\":54206,\"start\":53788},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":225041244},\"end\":54478,\"start\":54208},{\"attributes\":{\"id\":\"b26\"},\"end\":54700,\"start\":54480},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3651176},\"end\":54909,\"start\":54702},{\"attributes\":{\"id\":\"b28\"},\"end\":55223,\"start\":54911},{\"attributes\":{\"id\":\"b29\"},\"end\":55455,\"start\":55225},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":210848927},\"end\":55725,\"start\":55457},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":221716878},\"end\":55920,\"start\":55727},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":221970570},\"end\":56162,\"start\":55922},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":221716669},\"end\":56384,\"start\":56164}]", "bib_title": "[{\"end\":48019,\"start\":47955},{\"end\":48312,\"start\":48262},{\"end\":49606,\"start\":49564},{\"end\":49822,\"start\":49775},{\"end\":50077,\"start\":50033},{\"end\":50289,\"start\":50243},{\"end\":50783,\"start\":50706},{\"end\":51560,\"start\":51538},{\"end\":51800,\"start\":51716},{\"end\":52274,\"start\":52207},{\"end\":52530,\"start\":52451},{\"end\":53886,\"start\":53788},{\"end\":54239,\"start\":54208},{\"end\":54755,\"start\":54702},{\"end\":55507,\"start\":55457},{\"end\":55786,\"start\":55727},{\"end\":55979,\"start\":55922},{\"end\":56210,\"start\":56164}]", "bib_author": "[{\"end\":48037,\"start\":48021},{\"end\":48053,\"start\":48037},{\"end\":48065,\"start\":48053},{\"end\":48078,\"start\":48065},{\"end\":48089,\"start\":48078},{\"end\":48328,\"start\":48314},{\"end\":48344,\"start\":48328},{\"end\":48364,\"start\":48344},{\"end\":48688,\"start\":48675},{\"end\":48699,\"start\":48688},{\"end\":48713,\"start\":48699},{\"end\":48902,\"start\":48887},{\"end\":48916,\"start\":48902},{\"end\":48933,\"start\":48916},{\"end\":48948,\"start\":48933},{\"end\":48960,\"start\":48948},{\"end\":49198,\"start\":49184},{\"end\":49213,\"start\":49198},{\"end\":49228,\"start\":49213},{\"end\":49380,\"start\":49366},{\"end\":49400,\"start\":49380},{\"end\":49413,\"start\":49400},{\"end\":49428,\"start\":49413},{\"end\":49622,\"start\":49608},{\"end\":49633,\"start\":49622},{\"end\":49648,\"start\":49633},{\"end\":49840,\"start\":49824},{\"end\":49858,\"start\":49840},{\"end\":49874,\"start\":49858},{\"end\":49886,\"start\":49874},{\"end\":50093,\"start\":50079},{\"end\":50305,\"start\":50291},{\"end\":50317,\"start\":50305},{\"end\":50332,\"start\":50317},{\"end\":50469,\"start\":50457},{\"end\":50482,\"start\":50469},{\"end\":50491,\"start\":50482},{\"end\":50504,\"start\":50491},{\"end\":50804,\"start\":50785},{\"end\":50815,\"start\":50804},{\"end\":50834,\"start\":50815},{\"end\":51010,\"start\":50996},{\"end\":51024,\"start\":51010},{\"end\":51240,\"start\":51224},{\"end\":51256,\"start\":51240},{\"end\":51273,\"start\":51256},{\"end\":51475,\"start\":51460},{\"end\":51578,\"start\":51562},{\"end\":51594,\"start\":51578},{\"end\":51614,\"start\":51594},{\"end\":51819,\"start\":51802},{\"end\":51829,\"start\":51819},{\"end\":51843,\"start\":51829},{\"end\":52067,\"start\":52050},{\"end\":52289,\"start\":52276},{\"end\":52303,\"start\":52289},{\"end\":52315,\"start\":52303},{\"end\":52550,\"start\":52532},{\"end\":52564,\"start\":52550},{\"end\":52579,\"start\":52564},{\"end\":52818,\"start\":52802},{\"end\":52833,\"start\":52818},{\"end\":52846,\"start\":52833},{\"end\":52863,\"start\":52846},{\"end\":53091,\"start\":53067},{\"end\":53102,\"start\":53091},{\"end\":53119,\"start\":53102},{\"end\":53135,\"start\":53119},{\"end\":53150,\"start\":53135},{\"end\":53165,\"start\":53150},{\"end\":53174,\"start\":53165},{\"end\":53377,\"start\":53357},{\"end\":53601,\"start\":53588},{\"end\":53619,\"start\":53601},{\"end\":53635,\"start\":53619},{\"end\":53901,\"start\":53888},{\"end\":53914,\"start\":53901},{\"end\":53925,\"start\":53914},{\"end\":53933,\"start\":53925},{\"end\":53947,\"start\":53933},{\"end\":53959,\"start\":53947},{\"end\":53972,\"start\":53959},{\"end\":53986,\"start\":53972},{\"end\":54253,\"start\":54241},{\"end\":54266,\"start\":54253},{\"end\":54283,\"start\":54266},{\"end\":54295,\"start\":54283},{\"end\":54311,\"start\":54295},{\"end\":54326,\"start\":54311},{\"end\":54544,\"start\":54536},{\"end\":54561,\"start\":54544},{\"end\":54574,\"start\":54561},{\"end\":54771,\"start\":54757},{\"end\":54789,\"start\":54771},{\"end\":54991,\"start\":54969},{\"end\":55006,\"start\":54991},{\"end\":55030,\"start\":55006},{\"end\":55049,\"start\":55030},{\"end\":55242,\"start\":55227},{\"end\":55249,\"start\":55242},{\"end\":55264,\"start\":55249},{\"end\":55281,\"start\":55264},{\"end\":55296,\"start\":55281},{\"end\":55520,\"start\":55509},{\"end\":55531,\"start\":55520},{\"end\":55799,\"start\":55788},{\"end\":55810,\"start\":55799},{\"end\":55992,\"start\":55981},{\"end\":56003,\"start\":55992},{\"end\":56016,\"start\":56003},{\"end\":56030,\"start\":56016},{\"end\":56223,\"start\":56212},{\"end\":56236,\"start\":56223},{\"end\":56249,\"start\":56236},{\"end\":56260,\"start\":56249}]", "bib_venue": "[{\"end\":48096,\"start\":48089},{\"end\":48451,\"start\":48385},{\"end\":48744,\"start\":48713},{\"end\":49182,\"start\":49117},{\"end\":49454,\"start\":49428},{\"end\":49655,\"start\":49648},{\"end\":49893,\"start\":49886},{\"end\":50129,\"start\":50093},{\"end\":50339,\"start\":50332},{\"end\":50569,\"start\":50530},{\"end\":50838,\"start\":50834},{\"end\":51055,\"start\":51024},{\"end\":51222,\"start\":51136},{\"end\":51458,\"start\":51427},{\"end\":51618,\"start\":51614},{\"end\":51847,\"start\":51843},{\"end\":52048,\"start\":52000},{\"end\":52318,\"start\":52315},{\"end\":52583,\"start\":52579},{\"end\":52800,\"start\":52737},{\"end\":53065,\"start\":53017},{\"end\":53429,\"start\":53377},{\"end\":53586,\"start\":53520},{\"end\":53989,\"start\":53986},{\"end\":54333,\"start\":54326},{\"end\":54534,\"start\":54480},{\"end\":54796,\"start\":54789},{\"end\":54967,\"start\":54911},{\"end\":55331,\"start\":55296},{\"end\":55581,\"start\":55531},{\"end\":55814,\"start\":55810},{\"end\":56034,\"start\":56030},{\"end\":56264,\"start\":56260}]"}}}, "year": 2023, "month": 12, "day": 17}