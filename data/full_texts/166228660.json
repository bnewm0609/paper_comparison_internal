{"id": 166228660, "updated": "2023-09-27 18:11:11.245", "metadata": {"title": "On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks", "authors": "[{\"first\":\"Sunil\",\"last\":\"Thulasidasan\",\"middle\":[]},{\"first\":\"Gopinath\",\"last\":\"Chennupati\",\"middle\":[]},{\"first\":\"Jeff\",\"last\":\"Bilmes\",\"middle\":[]},{\"first\":\"Tanmoy\",\"last\":\"Bhattacharya\",\"middle\":[]},{\"first\":\"Sarah\",\"last\":\"Michalak\",\"middle\":[]}]", "venue": "ArXiv", "journal": "13888-13899", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Mixup [28] is a recently proposed method for training deep neural networks where additional samples are generated during training by convexly combining random pairs of images and their associated labels. While simple to implement, it has shown to be a surprisingly effective method of data augmentation for image classi\ufb01cation; DNNs trained with mixup show noticeable gains in classi\ufb01cation performance on a number of image classi\ufb01cation benchmarks. In this work, we discuss a hitherto untouched aspect of mixup training \u2013 the calibration and predictive uncertainty of models trained with mixup. We \ufb01nd that DNNs trained with mixup are signi\ufb01cantly better calibrated \u2013 i.e the predicted softmax scores are much better indicators of the actual likelihood of a correct prediction \u2013 than DNNs trained in the regular fashion. We conduct experiments on a number of image classi\ufb01cation architectures and datasets \u2013 including large-scale datasets like ImageNet \u2013 and \ufb01nd this to be the case. Additionally, we \ufb01nd that merely mixing features does not result in the same calibration bene\ufb01t and that the label smoothing in mixup training plays a signi\ufb01cant role in improving calibration. Finally, we also observe that mixup-trained DNNs are less prone to over-con\ufb01dent predictions on out-of-distribution and random-noise data. We conclude that the typical overcon\ufb01dence seen in neural networks, even on in-distribution data is likely a consequence of training with hard labels, suggesting that mixup training be employed for classi\ufb01cation tasks where predictive uncertainty is a signi\ufb01cant concern.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1905.11001", "mag": "2970121940", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ThulasidasanCBB19", "doi": "10.2172/1525811"}}, "content": {"source": {"pdf_hash": "a663f783b9fdabfdb39b4842e162a5c47aa1415d", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1905.11001", "status": "GREEN"}}, "grobid": {"id": "659fad52f360cc585f908a7acd7d6f792af6a401", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a663f783b9fdabfdb39b4842e162a5c47aa1415d.txt", "contents": "\nOn Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks\n\n\nSunil Thulasidasan \nLos Alamos National Laboratory\n\n\nDepartment of Electrical and Computer Engineering\nUniversity of Washington\n\n\nGopinath Chennupati \nLos Alamos National Laboratory\n\n\nJeff Bilmes \nDepartment of Electrical and Computer Engineering\nUniversity of Washington\n\n\nTanmoy Bhattacharya \nLos Alamos National Laboratory\n\n\nSarah Michalak \nLos Alamos National Laboratory\n\n\nOn Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks\n\nMixup[37] is a recently proposed method for training deep neural networks where additional samples are generated during training by convexly combining random pairs of images and their associated labels. While simple to implement, it has shown to be a surprisingly effective method of data augmentation for image classification: DNNs trained with mixup show noticeable gains in classification performance on a number of image classification benchmarks. In this work, we discuss a hitherto untouched aspect of mixup training -the calibration and predictive uncertainty of models trained with mixup. We find that DNNs trained with mixup are significantly better calibrated -i.e the predicted softmax scores are much better indicators of the actual likelihood of a correct prediction -than DNNs trained in the regular fashion. We conduct experiments on a number of image classification architectures and datasets -including large-scale datasets like ImageNet -and find this to be the case. Additionally, we find that merely mixing features does not result in the same calibration benefit and that the label smoothing in mixup training plays a significant role in improving calibration. Finally, we also observe that mixuptrained DNNs are less prone to over-confident predictions on out-of-distribution and random-noise data. We conclude that the typical overconfidence seen in neural networks, even on in-distribution data is likely a consequence of training with hard labels, suggesting that mixup training be employed for classification tasks where predictive uncertainty is a significant concern.\n\nIntroduction: Overconfidence and Uncertainty in Deep Learning\n\nMachine learning algorithms are replacing or expected to increasingly replace humans in decisionmaking pipelines. With the deployment of AI-based systems in high risk fields such as medical diagnosis [24], autonomous vehicle control [20] and the legal sector [1], the major challenges of the upcoming era are thus going to be in issues of uncertainty and trust-worthiness of a classifier. With deep neural networks having established supremacy in many pattern recognition tasks, it is the predictive uncertainty of these types of classifiers that will be of increasing importance. The DNN must not only be accurate, but also indicate when it is likely to get the wrong answer. This allows the decision-making to be routed to a human or another more accurate, but possibly more expensive, classifier, with the assumption being that the additional cost incurred is greatly surpassed by the consequences of a wrong prediction.\n\nFor this reason, quantifying the predictive uncertainty and confidence calibration for deep neural networks has seen increased attention in recent years ( [6,18,7,9,19,15,28]). One of the first works to examine the issue of calibration for modern neural networks was [9]; in a well-calibrated classifier, predictive scores should be indicative of the actual likelihood of correctness. The authors in [9] show significant empirical evidence that modern deep neural networks are poorly calibrated, with depth, weight decay and batch normalization all influencing calibration. Modern architectures, it turns out, are prone to overconfidence, meaning accuracy is likely to be lower than what is indicated by the predictive score. The top row in Figure 1 illustrates this phenomena: shown are a series of joint density plots of the average winning score and accuracy of a VGG-16 [29] network over the CIFAR-100 [17] validation set, plotted at different epochs. Both the confidence (captured by the winning score) as well as accuracy start out low and gradually increase as the network learns. However, what is interesting -and concerning -is that the confidence always leads accuracy in the later stages of training. Towards the end of training, accuracy saturates while confidence continues to increase resulting in a very sharply peaked distribution of winning scores and an overconfident model.\n\nMost modern DNNs, when trained for classification in a supervised learning setting, are trained using one-hot encoded labels that have all the probability mass in one class; the training labels are thus zero-entropy signals that admit no uncertainty about the input. The DNN is thus, in some sense, trained to become overconfident. Hence a worthwhile line of exploration is whether principled approaches to label smoothing can somehow temper overconfidence. Label smoothing and related work has been explored before [30,27]. In this work, we carry out an exploration along these lines by investigating the effect of the recently proposed mixup [37] method of training deep neural networks. In mixup, additional synthetic samples are generated during training by convexly combining random pairs of images and, importantly, their labels as well. While simple to implement, it has shown to be a surprisingly effective method of data augmentation: DNNs trained with mixup show noticeable gains in classification performance on a number of image classification benchmarks. However neither the original work nor any subsequent extensions to mixup [33,10,22] have explored the effect of mixup on predictive uncertainty and DNN calibration; this is precisely what we aim to do in this paper.\n\nOur findings are as follows: mixup trained DNNs are significantly better calibrated -i.e the predicted softmax scores are much better indicators of the actual likelihood of a correct prediction -than DNNs trained without mixup (see Figure 1 bottom row for an example). We also observe that merely mixing features does not result in the same calibration benefit and that the label smoothing in mixup training plays a significant role in improving calibration. Further, we also observe that mixup-trained DNNs are less prone to over-confident predictions on out-of-distribution and random-noise data. We note here that in this work we do not consider the calibration and uncertainty over adversarially perturbed inputs; we leave that for future exploration.\n\nThe rest of the paper is organized as follows: Section 2 provides a brief overview of the mixup training process. Section 3 discusses calibration metrics, experimental setup and mixup's calibration benefits results on image data. In Section 5 we explore in more detail the effect of mixup-based label smoothing on calibration. In Section 7 we show additional evidence of the benefit of mixup training on predictive uncertainty when dealing with unseen data. Further discussions and conclusions are in Section 8\n\n\nAn Overview of Mixup Training\n\nMixup training [37] is based on the principle of Vicinal Risk Minimization [3](VRM): the classifier is trained not only on the training data, but also in the vicinity of each training sample. The vicinal points are generated according to the following simple rule introduced in [37]:\nx = \u03bbx i + (1 \u2212 \u03bb)x j y = \u03bby i + (1 \u2212 \u03bb)y j\nwhere x i and x j are two randomly sampled input points, and y i and y j are their associated one-hot encoded labels. This has the effect of the empirical Dirac delta distribution\nP \u03b4 (x, y) = 1 n n i \u03b4(x = x i , y = y i ) centered at (x i , y i ) being replaced with the empirical vicinal distribution P \u03bd (x,\u1ef9) = 1 n n i \u03bd(x,\u1ef9|x i , y i )\nThe vicinal samples (x,\u1ef9) are generated as above, and during training minimization is performed on the empirical vicinal risk:\nR \u03bd (f ) = 1 m m i=1 L(f (x i ),\u1ef9 i )\nwhere L is the standard cross-entropy loss, but calculated on the soft-labels\u1ef9 i instead of hard labels. Training this way not only augments the feature setX, but the induced set of soft-labels also encourages the strength of the classification regions to vary linearly betweens samples. The experiments in [37] and related work in [14,33,10] show noticeable performance gains in various image classification tasks.\n\nThe linear interpolator \u03bb \u2208 [0, 1] that determines the mixing ratio is drawn from a symmetric Beta distribution, Beta(\u03b1, \u03b1), where \u03b1 is the hyper-parameter that controls the strength of the interpolation between pairs of images and the associated smoothing of the training labels. \u03b1 = 0 recovers the base case corresponding to zero-entropy training labels (one-hot encodings, in which case the resulting image is either just x i or x j ), while a high value of \u03b1 ends up in always averaging the inputs and labels. The authors in [37] remark that relatively smaller values of \u03b1 \u2208 [0.1, 0.4] gave the best performing results for classification, while high values of \u03b1 resulted in significant under-fitting. In this work, we also look at the effect of \u03b1 on calibration performance.\n\n\nExperiments\n\nWe perform numerous experiments to analyze the effect of mixup training on the calibration of the resulting trained classifiers on both image and NLP data. We experiment with various deep architectures and standard datasets, including large-scale training with ImageNet. In all the experiments in this paper, we only apply mixup to pairs of images as done in [37]. The mixup functionality was implemented using the mixup authors' code available at [36].\n\n\nSetup\n\nFor the small-scale image experiments, we use the following datasets in our experiments: STL-10 [4], CIFAR-10 and CIFAR-100 [17] and Fashion-MNIST [34]. For STL-10, we use the VGG-16 [29] network. CIFAR-10 and CIFAR-100 experiments were carried out on VGG-16 as well as ResNet-34 models. For Fashion-MNIST, we used a ResNet-18 model. For all experiments, we use batch normalization, weight decay of 5 \u00d7 10 \u22124 , trained the network using SGD with Nesterov momentum, training for 200 epochs with an initial learning rate of 0.1 halved at 2 at 60,120 and 160 epochs. Unless otherwise noted, calibration results are reported for the best performing epoch on the validation set.\n\n\nCalibration Metrics\n\nWe measure the calibration of the network as follows (and as described in [9]): predictions are grouped into M interval bins of equal size. Let B m be the set of samples whose prediction scores (the winning softmax score) fall into bin B m . The accuracy and confidence of B m are defined as\nacc(B m ) = 1 |B m | iinBm 1(\u0177 i = y i ) conf(B m ) = 1 |B m | iinBmp i wherep i is the confidence (winning score) of sample i. The Expected Calibration Error (ECE) is then defined as: ECE = M m=1 |B m | n acc(B m ) \u2212 conf(B m )\nIn high-risk applications, confident but wrong predictions can be especially harmful; thus we also define an additional calibration metric -the Overconfidence Error (OE)-as follows\nOE = M m=1 |B m | n conf(B m ) \u00d7 max conf(B m ) \u2212 acc(B m ), 0\nThis penalizes predictions by the weight of the confidence but only when confidence exceeds accuracy; thus overconfident bins incur a high penalty.\n\n\nComparison Methods\n\nSince mixup produces smoothed labels over mixtures of inputs, we compare the calibration performance of mixup to two other label smoothing techniques:\n\n\u2022 \u2212label smoothing described in [30] where the one-hot encoded training signal is smoothed by distributing an mass over the other (i.e., non ground-truth) classes.\n\n\u2022 We also compare the performance of mixup against the entropy-regularized loss (ERL) described in [27] that discourages the neural network from being over-confident by penalizing low-entropy distributions.\n\nOur baseline comparison is regular training where no label smoothing or mixing of features is applied (no-mixup). We also note that in this section we do not compare against the temperature scaling method described in [9], which is a post-training calibration method and will generally produce well-calibrated scores. Here we would like to see the effect of label smoothing while training; experiments with temperature scaling are reported in Section 7.\n\n\nResults\n\nResults on the various datasets and architectures are shown in Figure 2. While the performance gains in validation accuracy are generally consistent with the results reported in [37], it is the effects on network calibration that we focus here. The top row shows a calibration scatter plot for STL-10 and CIFAR-100, highlighting the effect of mixup training. In a well calibrated model, where the confidence matches the accuracy most of the points will be on x = y line. We see that in the base case, both for STL-10 and CIFAR-100, most of the points tend to lie in the overconfident region. The mixup case is much better calibrated, noticeably in the high-confidence regions.  The bar plots in the middle row show the results on various combinations of datasets and architectures on accuracy and calibration against comparison methods. We report the calibration error for the best performing model (in terms of validation accuracy). For label smoothing, an \u2208 [0.05, 0.1] performed best while for ERL, the best-performing confidence penalty hyper-parameter was 0.1. The trends in the comparison are clear: label smoothing either via -smoothing, ERL or mixup generally provides a calibration advantage and tempers overconfidence, with the latter generally performing the best in comparison to other methods. We also show the effect on ECE as we vary the hyperparameter \u03b1 of the mixing parameter distribution. For very low values of \u03b1, the behavior is similar to the base case (as expected), but ECE also noticeably worsens for higher values of \u03b1 due to the model being under-confident. Indeed, mixup models can be underconfident if \u03b1 is large whic is related to manifold intrusion [10]: a mixed-up sample is more likely to lie away from the original manifold and thus be affected by manifold intrusion if \u03b1 is large. Overconfidence alone decreases monotonically as we increase \u03b1 as shown in Figure 2i. We also show the accuracy of mixup models at various levels of calibration determined by \u03b1. As can be seen, a well-tuned \u03b1 can result in a better-calibrated model with very little loss in performance. Our classification results here are consistent with those reported in [37] where the best performing \u03b1 was in the [0.1, .0.4] range. Here we report the results of calibration metrics resulting form mixup training on the 1000-class version of the ImageNet [5] data comprising of over 1.2 million images. One of the advantages of mixup and its implementation is that it adds very little overhead to the training time, and thus can be easily applied to large scale datasets like ImageNet. We perform distributed parallel training using the synchronous version of stochastic gradient descent. We use the learning-rate schedule described in [8] on a 32-GPU cluster and train till 93% accuracy is reached over the top-5 predictions. We test on two modern state-of-the-art archictures: ResNet-50 [12] and ResNext-101 (32x4d) [35]. The results are shown in Figure 3. The scatter-plot showing calibration for ResNext-101 architecture suggests that mixup training provides noticeable benefits even in the large-data scenario, where the models should be less prone to over-fitting the one-hot labels. On the deeper ResNext, mixup provides better calibration than the label smoothing models, though this same effect was not visible for the ResNet-50 model. However, both calibration error and overconfidence show noticeable improvements using label smoothing over the baseline. The mixup model did however achieve a consistently higher classification performance of \u2248 0.4 percent over the other methods. While mixup was originally suggested as a method to mostly improve performance on image classification tasks, here we explore the effect of mixup training in the natural language processing (NLP) domain. A straight-forward mixing of inputs (as in pixel-mixing in images) will generally produce nonsense input since the semantics are unclear. To avoid this, we modify the mixup strategy to perform mixup on the embeddings layer rather than directly on the input documents. We note that this approach is similar to the recent work described in [11] that utilizes mixup for improving sentence classification which is among the few works, besides ours, studying the effects of mixup in the NLP domain. For our experiments, we employ mixup on NLP data for text classification using the MR [25], TREC [21] and [23] datasets.We train a CNN for sentence classification (Sentence-level CNN) [16], where we initialize all the words with pre-trained GloVe [26] embeddings, which are modified while training on each dataset. For the remaining parameters, we use the values suggested in [16]. We refrain from training the most recent NLP models [13,2,38], since our aim here is not to show state-of-art classification performance on these datasets, but to study the effect on calibration. Also, the design of the more recent NLP models makes embedding mixup less straightforward. Nevertheless, the performance benefits on calibration, shown in Figure 4 are evident where mixup provides noticeable gains for all datasets, both in terms of calibration and overconfidence. We leave further exploration of principled strategies for mixup for NLP as future work.\n\n\nLarge-scale Experiments on ImageNet\n\n\nExperiments on Natural Language Data\n\n\nEffect of Soft Labels on Calibration\n\nSo far we have seen that mixup consistently leads to better calibrated networks compared to the base case, in addition to improving classification performance as has been observed in a number of works [33,10,22]. This behavior is not surprising given that mixup is a form of data augmentation: in mixup training, due to random sampling of both images as well as the mixing parameter \u03bb, the probability that the learner sees the same image twice is small. This has a strong regularizing effect in terms of preventing memorization and over-fitting, even for high-capacity neural networks. Indeed, unlike regular training, the train loss in the mixup case is always significantly higher than the base case as observed by the mixup authors [37]. From the perspective of statistical learning theory, the improved calibration of a mixup classifier can be viewed as the classifier learning the true posteriors in the infinite data limit [32] due to the significant amount of data augmentation resulting from the random combination in mixup. However this leads to the following question: if the improved calibration is essentially an effect of data augmentation, does simply combining the images but not combining the labels provide the same calibration benefit?\n\nWe perform a series of experiments on various image datasets and architectures to explore this question. Results from the earlier sections show that existing label smoothing techniques that increase the entropy of the training signal do provide better calibration without exploiting any data augmentation effects and thus we expect to see this in play in the mixup case as well. In the latter case, the entropies of the train labels are determined by the \u03b1 parameter of the Beta(\u03b1, \u03b1) distribution from which the mixing parameter is sampled. The distribution of training entropies for a few cases of \u03b1 are shown in Figure 5. The base-case is equivalent to \u03b1 = 0 (not shown) where the entropy distribution is a point-mass at 0. To tease out the effect of full mixup versus only mixing features, we convexly combine images as before, but the resulting image assumes the hard label of the nearer class; this provides data augmentation without the label smoothing effect. Results on a number of benchmarks and architectures are shown in Figure 6. The results are clear: merely mixing features does not provide the calibration benefit seen in the full-mixup case suggesting that the point-mass distributions in hard-coded labels are contributing factors to overconfidence. As in label smoothing and entropy regularization, having (or enforcing via a loss penalty) a non-zero mass in more than one class prevents the largest pre-softmax logit from becoming much larger than the others tempering overconfidence and leading to improved calibration.\n\nIn addition to feature and label mixing, a recent extension to mixup [33] also proposes convexly combining the representations in the hidden layer of the network; we report the calibration effects of this approach in the supplementary material. As remarked in the previous section, one of the contributing factors to improved calibration in mixup is the significant data augmentation aspect of mixup training, where the model is unlikely to see the same mixed-up sample more than once. The natural question here is whether these models will eventually become overconfident if trained for much longer periods. Below, we show the training curves for a few extended training experiments where the models were trained for 1000 epochs: for the baseline (i.e when \u03b1 = 0.), the train loss and accuracy approach 0 and 100% respectively (i.e., over-fitting), while in the mixup case (non-zero \u03b1's), the strong data augmentation prevents over-fitting. This behavior is sustained over the entire duration of the training as can be seen in the corresponding values of ECE. Mixup models, even when trained for much longer, continue to have a low calibration error, suggesting that the mixing of data has a sustained inhibitive effect on over-fitting the training data (the training loss for mixup continues to be significantly higher than baseline even after extended training) and preventing the model from becoming overconfident. In this section, we explore the effect of mixup training when predicting on samples from unseen classes (out-ofdistribution) and random noise images. We first train a VGG-16 network on in-distribution data (STL-10) and then predict on classes sampled from the ImageNet database that have not been encountered during training. For the random noise images, we test on gaussian random noise with the same mean and variance as the training set.\n\nWe compare the performance of a mixup-trained model with that of the baseline, as well as a temperature calibrated pre-trained baseline as described in [9]. Since the latter is a post-training calibration method, we expect it to be well calibrated on in-distribution data. We also compare the prediction uncertainty using the Monte Carlo dropout method described in [6] where multiple forward passes using dropout are made during test-time. We average predictions over 10 runs. The distribution over prediction scores for out-of-distribution and random data for mixup and comparison methods are shown in Figure 8. The differences versus the baseline are striking; in both cases, the mixup DNN is noticeably less confident than its non-mixup counterpart, with the score distribution being nearly perfectly separable in the random noise case. While temperature scaling is more conservative than mixup on real but out-of-sample data, it is noticeably more overconfident in the random-noise case. Further, mixup performs significantly better than MC-dropout in both cases.  Table 1: Out-of-category detection results for the DAC on STL-10 and Tiny ImageNet.\n\nIn Table 1, we also show a comparison of the performance of the aforementioned models for reliably detecting of out-ofdistributon and random-noise data, using Area under the ROC curve as the metric. Mixup is the best performing model in both cases, significantly outperforming the others as a random-noise detector. Temperature scaling, while producing well-calibrated models for in-distribution data is not a reliable detector. The scaling process reduces the confidence on both in and out-ofdistribution data, significantly reducing the ability to discriminate between these two types of data. Mixup, on the other hand, does well in both cases. The results here suggest that the effect of training with interpolated samples and the resulting label smoothing tempers over-confidence in regions away from the training data.\n\nWhile these experiments were limited to two datasets and one architecture, the results indicate that training by minimizing vicinal risk can be an effective way to enhance reliability of predictions in DNNs. Note that since mixup trains the model by convexly combining pairs of images, the synthesized images all lie within the convex hull of the training data. In the suppllementary material, we provide results on the prediction confidence when images lie outside the convex hull of the training set.\n\n\nConclusion and Future Work\n\nWe presented results on an unexplored area of mixup based training -its effect on DNN calibration and predictive uncertainty. Existing empirical work has conclusively shown the benefits of mixup for boosting classification performance; in this work, we show an additional important benefitmixup trained networks turn out to be better calibrated and provide more reliable estimates both for in-sample and out-of-sample data (being under-confident in the latter case).\n\nThere are possibly multiple reasons for this: the data augmentation provided by mixup is a form of regularization that prevents over-fitting and memorization, tempering overconfidence in the process. The label smoothing resulting from mixup might be viewed as a form of entropic regularization on the training signals, again preventing the DNN from driving the training error to zero. The results in the paper provide further evidence that training with hard labels is likely one of the contributing factors leading to overconfidence seen in modern neural networks. Recent work [33] has shown how the classification regions in mixup are smoother, without sudden jumps from one high confidence region to the other suggesting that the lack of sharp boundary transitions in classification regions play an important role in producing well-calibrated classifiers.\n\nSince mixup is implemented while training, it can also be employed with post-training calibration like temperature scaling, model perturbations like the dropout method or even the ensemble models described in [18]. Further mixup based models can also be combined with rejection classifiers, both during training [31] for dealing with label noise, as well as inference [7] to improve the training and classification pipeline in modern deep learning. Indeed, the classification performance-boost coupled with the well-calibrated nature of mixup trained DNNs as studied in this paper suggest that mixup based training be employed in situations where predictive uncertainty is a significant concern.\n\nFigure 1 :\n1Joint density plot of accuracy vs confidence (captured by the winning softmax score) on the CIFAR-100 validation set at different training epochs for the VGG-16 deep neural network. Top Row: In regular training, the DNN moves from under-confidence, at the beginning of training, to overconfidence at the end. A well-calibrated classifier would have most of the density lying on the x = y gray line. Bottom Row: Training with mixup on the same architecture and dataset. At corresponding epochs, the network is much better calibrated.\n\nFigure 2 :\n2Calibration results for mixup and base-case on various image datasets and architectures. Top Row: Scatterplots for accuracy and confidence for STL-10(a,b) and CIFAR-100(c,d\n\nFigure 3 :\n3Calibration on ImageNet for ResNet architectures\n\nFigure 4 :\n4Accuracy, calibration and overconfidence on various NLP datasets\n\nFigure 5 :\n5Entropy distribution of training labels as a function of the \u03b1 parameter of the Beta(\u03b1, \u03b1) distribution from which the mixing parameter is sampled.\n\nFigure 6 :\n6Calibration performance when only features are mixed vs. full mixup, on various datasets and architectures 6 Effect of Extended Training on Mixup Calibration\n\nFigure 7 :Figure 8 :\n78Training loss and calibration error under extended training for CIFAR-10 and CIFAR-100 with mixup. Baseline train error (orange) goes to zero early on while mixup continues to have non-zero training loss even after 1000 epochs. Meanwhile, calibration error for mixup does not exhibit an upward trend even after extended training. Distribution of winning scores of various models when tested on out-of-distribution and gaussian noise samples, after being trained on the STL-10 dataset.\n\n\n). The mixup case is much better calibrated with the points lying closer to the x = y line, while in the base case, points tend to lie in the overconfident region. Middle Row: Mixup versus comparison methods. label smoothing is the -label smoothing method, while ERL is the entropy regularized loss.Bottom \nRow: Calibration Error (e) and Overconfidence error (f) on various architectures. Experiments \nsuggest best ECE is achieved in the [0.2,0.4] range for \u03b1 (h), while overconfidence error decreases \nmonotonically with \u03b1 due to underfitting (i). Accuracy behavior for differently calibrated models is \nshowin in (j). \n\n\nAcknowledgmentsWe would like to thank the anonymous referees for their valuable suggestions on improving the paper. The authors were supported in part by the Joint Design of Advanced Computing Solutions for Cancer (JDACS4C) program established by the U.S. Department of Energy (DOE) and the National Cancer Institute (NCI) of the National Institutes of Health. This work was performed under the auspices of the U.S. Department of Energy by Los Alamos National Laboratory under Contract DE-AC5206NA25396. This work was also supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.\nAn impact assessment of machine learning risk forecasts on parole board decisions and recidivism. Richard Berk, Journal of Experimental Criminology. 132Richard Berk. An impact assessment of machine learning risk forecasts on parole board decisions and recidivism. Journal of Experimental Criminology, 13(2):193-216, 2017.\n\nDaniel Cer, Yinfei Yang, Sheng-Yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, arXiv:1803.11175Chris Tar, et al. Universal sentence encoder. arXiv preprintDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175, 2018.\n\nVicinal risk minimization. Olivier Chapelle, Jason Weston, L\u00e9on Bottou, Vladimir Vapnik, Advances in neural information processing systems. Olivier Chapelle, Jason Weston, L\u00e9on Bottou, and Vladimir Vapnik. Vicinal risk minimization. In Advances in neural information processing systems, pages 416-422, 2001.\n\nAn analysis of single-layer networks in unsupervised feature learning. Adam Coates, Andrew Ng, Honglak Lee, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statisticsAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsuper- vised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215-223, 2011.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Computer Vision and Pattern Recognition. IeeeCVPR 2009. IEEE Conference onJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248-255. Ieee, 2009.\n\nDropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, international conference on machine learning. Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050-1059, 2016.\n\nSelective classification for deep neural networks. Yonatan Geifman, Ran El-Yaniv, Advances in neural information processing systems. Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In Advances in neural information processing systems, pages 4885-4894, 2017.\n\nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, arXiv:1706.02677Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprintPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\nChuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, arXiv:1706.04599On calibration of modern neural networks. arXiv preprintChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. arXiv preprint arXiv:1706.04599, 2017.\n\nHongyu Guo, Yongyi Mao, Richong Zhang, arXiv:1809.02499Mixup as locally linear out-of-manifold regularization. arXiv preprintHongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regularization. arXiv preprint arXiv:1809.02499, 2018.\n\nAugmenting data with mixup for sentence classification: An empirical study. Hongyu Guo, Yongyi Mao, Richong Zhang, arXiv:1905.08941arXiv preprintHongyu Guo, Yongyi Mao, and Richong Zhang. Augmenting data with mixup for sentence classification: An empirical study. arXiv preprint arXiv:1905.08941, 2019.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.\n\nUniversal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, arXiv:1801.06146arXiv preprintJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classifica- tion. arXiv preprint arXiv:1801.06146, 2018.\n\nData augmentation by pairing samples for images classification. Hiroshi Inoue, arXiv:1801.02929arXiv preprintHiroshi Inoue. Data augmentation by pairing samples for images classification. arXiv preprint arXiv:1801.02929, 2018.\n\nWhat uncertainties do we need in bayesian deep learning for computer vision?. Alex Kendall, Yarin Gal, Advances in neural information processing systems. Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in neural information processing systems, pages 5574-5584, 2017.\n\nConvolutional neural networks for sentence classification. Yoon Kim, arXiv:1408.5882arXiv preprintYoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, CiteseerTechnical reportAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.\n\nSimple and scalable predictive uncertainty estimation using deep ensembles. Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, Advances in Neural Information Processing Systems. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pages 6402-6413, 2017.\n\nTraining confidence-calibrated classifiers for detecting out-of-distribution samples. Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin, arXiv:1711.09325arXiv preprintKimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. arXiv preprint arXiv:1711.09325, 2017.\n\nTowards fully autonomous driving: Systems and algorithms. Jesse Levinson, Jake Askeland, Jan Becker, Jennifer Dolson, David Held, Soeren Kammel, Zico Kolter, Dirk Langer, Oliver Pink, Vaughan Pratt, Intelligent Vehicles Symposium (IV). IEEEJesse Levinson, Jake Askeland, Jan Becker, Jennifer Dolson, David Held, Soeren Kammel, J Zico Kolter, Dirk Langer, Oliver Pink, Vaughan Pratt, et al. Towards fully autonomous driving: Systems and algorithms. In Intelligent Vehicles Symposium (IV), 2011 IEEE, pages 163-168. IEEE, 2011.\n\nLearning question classifiers. Xin Li, Dan Roth, Proceedings of the 19th international conference on Computational linguistics. the 19th international conference on Computational linguisticsAssociation for Computational Linguistics1Xin Li and Dan Roth. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1-7. Association for Computational Linguistics, 2002.\n\nUnderstanding mixup training methods. Daojun Liang, Feng Yang, Tian Zhang, Peter Yang, IEEE Access. 6Daojun Liang, Feng Yang, Tian Zhang, and Peter Yang. Understanding mixup training methods. IEEE Access, 6:58774-58783, 2018.\n\nLearning word vectors for sentiment analysis. L Andrew, Raymond E Maas, Daly, T Peter, Dan Pham, Huang, Y Andrew, Christopher Ng, Potts, Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies. the 49th annual meeting of the association for computational linguistics: Human language technologiesAssociation for Computational Linguistics1Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pages 142-150. Association for Computational Linguistics, 2011.\n\nDeep patient: an unsupervised representation to predict the future of patients from the electronic health records. Riccardo Miotto, Li Li, A Brian, Joel T Kidd, Dudley, Scientific reports. 626094Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. Deep patient: an unsupervised representation to predict the future of patients from the electronic health records. Scientific reports, 6:26094, 2016.\n\nSeeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. Bo Pang, Lillian Lee, Proceedings of the 43rd annual meeting on association for computational linguistics. the 43rd annual meeting on association for computational linguisticsAssociation for Computational LinguisticsBo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 115-124. Association for Computational Linguistics, 2005.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014.\n\nGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, Geoffrey Hinton, arXiv:1701.06548Regularizing neural networks by penalizing confident output distributions. arXiv preprintGabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton. Reg- ularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, 2017.\n\nEvidential deep learning to quantify classification uncertainty. Murat Sensoy, Lance Kaplan, Melih Kandemir, Advances in Neural Information Processing Systems. Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty. In Advances in Neural Information Processing Systems, pages 3183- 3193, 2018.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re- thinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818-2826, 2016.\n\nTanmoy Sunil Thulasidasan, Jeff Bhattacharya, Gopinath Bilmes, Jamal Chennupati, Mohd-Yusof, arXiv:1905.10964Combating label noise in deep learning using abstention. arXiv preprintSunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal Mohd-Yusof. Combating label noise in deep learning using abstention. arXiv preprint arXiv:1905.10964, 2019.\n\nOn the uniform convergence of relative frequencies of events to their probabilities. N Vladimir, Vapnik, Ya Chervonenkis, Measures of complexity. SpringerVladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. In Measures of complexity, pages 11-30. Springer, 2015.\n\nVikas Verma, Alex Lamb, Christopher Beckham, Aaron Courville, arXiv:1806.05236Ioannis Mitliagkis, and Yoshua Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer. arXiv preprintVikas Verma, Alex Lamb, Christopher Beckham, Aaron Courville, Ioannis Mitliagkis, and Yoshua Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer. arXiv preprint arXiv:1806.05236, 2018.\n\nFashion-mnist: a novel image dataset for benchmarking machine learning algorithms. Han Xiao, Kashif Rasul, Roland Vollgraf, arXiv:1708.07747arXiv preprintHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492-1500, 2017.\n\n. Hongyi Zhang, Hongyi Zhang. https://github.com/hongyi-zhang/mixup.\n\nHongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. arXiv preprintHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\nText classification improved by integrating bidirectional lstm with two-dimensional max pooling. Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, Bo Xu, arXiv:1611.06639arXiv preprintPeng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. Text classifi- cation improved by integrating bidirectional lstm with two-dimensional max pooling. arXiv preprint arXiv:1611.06639, 2016.\n", "annotations": {"author": "[{\"end\":224,\"start\":95},{\"end\":278,\"start\":225},{\"end\":368,\"start\":279},{\"end\":422,\"start\":369},{\"end\":471,\"start\":423}]", "publisher": null, "author_last_name": "[{\"end\":113,\"start\":101},{\"end\":244,\"start\":234},{\"end\":290,\"start\":284},{\"end\":388,\"start\":376},{\"end\":437,\"start\":429}]", "author_first_name": "[{\"end\":100,\"start\":95},{\"end\":233,\"start\":225},{\"end\":283,\"start\":279},{\"end\":375,\"start\":369},{\"end\":428,\"start\":423}]", "author_affiliation": "[{\"end\":146,\"start\":115},{\"end\":223,\"start\":148},{\"end\":277,\"start\":246},{\"end\":367,\"start\":292},{\"end\":421,\"start\":390},{\"end\":470,\"start\":439}]", "title": "[{\"end\":92,\"start\":1},{\"end\":563,\"start\":472}]", "venue": null, "abstract": "[{\"end\":2160,\"start\":565}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2429,\"start\":2425},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2462,\"start\":2458},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2487,\"start\":2484},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3308,\"start\":3305},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3311,\"start\":3308},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3313,\"start\":3311},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3315,\"start\":3313},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3318,\"start\":3315},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3321,\"start\":3318},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3324,\"start\":3321},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3420,\"start\":3417},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3553,\"start\":3550},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4028,\"start\":4024},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4060,\"start\":4056},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5064,\"start\":5060},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5067,\"start\":5064},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5192,\"start\":5188},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5689,\"start\":5685},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5692,\"start\":5689},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5695,\"start\":5692},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7149,\"start\":7145},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7208,\"start\":7205},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8275,\"start\":8271},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8300,\"start\":8296},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8303,\"start\":8300},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8306,\"start\":8303},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8914,\"start\":8910},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9538,\"start\":9534},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9627,\"start\":9623},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9737,\"start\":9734},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9766,\"start\":9762},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9789,\"start\":9785},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9825,\"start\":9821},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10412,\"start\":10409},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11458,\"start\":11454},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11690,\"start\":11686},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12016,\"start\":12013},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12442,\"start\":12438},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13944,\"start\":13940},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14436,\"start\":14432},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14620,\"start\":14617},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15001,\"start\":14998},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15155,\"start\":15151},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15184,\"start\":15180},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16400,\"start\":16396},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16642,\"start\":16638},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16653,\"start\":16649},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16662,\"start\":16658},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16740,\"start\":16736},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16803,\"start\":16799},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16932,\"start\":16928},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16990,\"start\":16986},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16992,\"start\":16990},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16995,\"start\":16992},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17821,\"start\":17817},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17824,\"start\":17821},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17827,\"start\":17824},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18356,\"start\":18352},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18550,\"start\":18546},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20487,\"start\":20483},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22430,\"start\":22427},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22644,\"start\":22641},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25838,\"start\":25834},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26329,\"start\":26325},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26432,\"start\":26428},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26487,\"start\":26484}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27357,\"start\":26812},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27543,\"start\":27358},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27605,\"start\":27544},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27683,\"start\":27606},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27844,\"start\":27684},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28015,\"start\":27845},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28524,\"start\":28016},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29149,\"start\":28525}]", "paragraph": "[{\"end\":3148,\"start\":2225},{\"end\":4542,\"start\":3150},{\"end\":5827,\"start\":4544},{\"end\":6584,\"start\":5829},{\"end\":7096,\"start\":6586},{\"end\":7413,\"start\":7130},{\"end\":7637,\"start\":7458},{\"end\":7925,\"start\":7799},{\"end\":8379,\"start\":7964},{\"end\":9159,\"start\":8381},{\"end\":9628,\"start\":9175},{\"end\":10311,\"start\":9638},{\"end\":10626,\"start\":10335},{\"end\":11036,\"start\":10856},{\"end\":11247,\"start\":11100},{\"end\":11420,\"start\":11270},{\"end\":11585,\"start\":11422},{\"end\":11793,\"start\":11587},{\"end\":12248,\"start\":11795},{\"end\":17498,\"start\":12260},{\"end\":18870,\"start\":17616},{\"end\":20412,\"start\":18872},{\"end\":22273,\"start\":20414},{\"end\":23428,\"start\":22275},{\"end\":24253,\"start\":23430},{\"end\":24757,\"start\":24255},{\"end\":25254,\"start\":24788},{\"end\":26114,\"start\":25256},{\"end\":26811,\"start\":26116}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7457,\"start\":7414},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7798,\"start\":7638},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7963,\"start\":7926},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10855,\"start\":10627},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11099,\"start\":11037}]", "table_ref": "[{\"end\":23352,\"start\":23345},{\"end\":23440,\"start\":23433}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2223,\"start\":2162},{\"attributes\":{\"n\":\"2\"},\"end\":7128,\"start\":7099},{\"attributes\":{\"n\":\"3\"},\"end\":9173,\"start\":9162},{\"attributes\":{\"n\":\"3.1\"},\"end\":9636,\"start\":9631},{\"attributes\":{\"n\":\"3.2\"},\"end\":10333,\"start\":10314},{\"attributes\":{\"n\":\"3.3\"},\"end\":11268,\"start\":11250},{\"attributes\":{\"n\":\"3.4\"},\"end\":12258,\"start\":12251},{\"attributes\":{\"n\":\"3.4.1\"},\"end\":17536,\"start\":17501},{\"attributes\":{\"n\":\"4\"},\"end\":17575,\"start\":17539},{\"attributes\":{\"n\":\"5\"},\"end\":17614,\"start\":17578},{\"attributes\":{\"n\":\"8\"},\"end\":24786,\"start\":24760},{\"end\":26823,\"start\":26813},{\"end\":27369,\"start\":27359},{\"end\":27555,\"start\":27545},{\"end\":27617,\"start\":27607},{\"end\":27695,\"start\":27685},{\"end\":27856,\"start\":27846},{\"end\":28037,\"start\":28017}]", "table": "[{\"end\":29149,\"start\":28826}]", "figure_caption": "[{\"end\":27357,\"start\":26825},{\"end\":27543,\"start\":27371},{\"end\":27605,\"start\":27557},{\"end\":27683,\"start\":27619},{\"end\":27844,\"start\":27697},{\"end\":28015,\"start\":27858},{\"end\":28524,\"start\":28040},{\"end\":28826,\"start\":28527}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3899,\"start\":3891},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6069,\"start\":6061},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12331,\"start\":12323},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14159,\"start\":14150},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15219,\"start\":15211},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17293,\"start\":17285},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19495,\"start\":19487},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19913,\"start\":19905},{\"end\":22887,\"start\":22879}]", "bib_author_first_name": "[{\"end\":29925,\"start\":29918},{\"end\":30149,\"start\":30143},{\"end\":30161,\"start\":30155},{\"end\":30176,\"start\":30168},{\"end\":30186,\"start\":30183},{\"end\":30198,\"start\":30192},{\"end\":30215,\"start\":30209},{\"end\":30229,\"start\":30225},{\"end\":30245,\"start\":30240},{\"end\":30270,\"start\":30265},{\"end\":30607,\"start\":30600},{\"end\":30623,\"start\":30618},{\"end\":30636,\"start\":30632},{\"end\":30653,\"start\":30645},{\"end\":30957,\"start\":30953},{\"end\":30972,\"start\":30966},{\"end\":30984,\"start\":30977},{\"end\":31462,\"start\":31459},{\"end\":31472,\"start\":31469},{\"end\":31486,\"start\":31479},{\"end\":31501,\"start\":31495},{\"end\":31509,\"start\":31506},{\"end\":31516,\"start\":31514},{\"end\":31924,\"start\":31919},{\"end\":31936,\"start\":31930},{\"end\":32245,\"start\":32238},{\"end\":32258,\"start\":32255},{\"end\":32488,\"start\":32483},{\"end\":32501,\"start\":32496},{\"end\":32514,\"start\":32510},{\"end\":32531,\"start\":32525},{\"end\":32549,\"start\":32543},{\"end\":32566,\"start\":32562},{\"end\":32581,\"start\":32575},{\"end\":32954,\"start\":32949},{\"end\":32965,\"start\":32960},{\"end\":32976,\"start\":32974},{\"end\":32990,\"start\":32982},{\"end\":33221,\"start\":33215},{\"end\":33233,\"start\":33227},{\"end\":33246,\"start\":33239},{\"end\":33561,\"start\":33555},{\"end\":33573,\"start\":33567},{\"end\":33586,\"start\":33579},{\"end\":33836,\"start\":33829},{\"end\":33848,\"start\":33841},{\"end\":33864,\"start\":33856},{\"end\":33874,\"start\":33870},{\"end\":34294,\"start\":34288},{\"end\":34312,\"start\":34303},{\"end\":34560,\"start\":34553},{\"end\":34799,\"start\":34795},{\"end\":34814,\"start\":34809},{\"end\":35117,\"start\":35113},{\"end\":35319,\"start\":35315},{\"end\":35340,\"start\":35332},{\"end\":35582,\"start\":35576},{\"end\":35610,\"start\":35601},{\"end\":35627,\"start\":35620},{\"end\":36000,\"start\":35995},{\"end\":36013,\"start\":36006},{\"end\":36024,\"start\":36019},{\"end\":36036,\"start\":36030},{\"end\":36314,\"start\":36309},{\"end\":36329,\"start\":36325},{\"end\":36343,\"start\":36340},{\"end\":36360,\"start\":36352},{\"end\":36374,\"start\":36369},{\"end\":36387,\"start\":36381},{\"end\":36400,\"start\":36396},{\"end\":36413,\"start\":36409},{\"end\":36428,\"start\":36422},{\"end\":36442,\"start\":36435},{\"end\":36812,\"start\":36809},{\"end\":36820,\"start\":36817},{\"end\":37258,\"start\":37252},{\"end\":37270,\"start\":37266},{\"end\":37281,\"start\":37277},{\"end\":37294,\"start\":37289},{\"end\":37488,\"start\":37487},{\"end\":37504,\"start\":37497},{\"end\":37506,\"start\":37505},{\"end\":37520,\"start\":37519},{\"end\":37531,\"start\":37528},{\"end\":37546,\"start\":37545},{\"end\":37566,\"start\":37555},{\"end\":38295,\"start\":38287},{\"end\":38306,\"start\":38304},{\"end\":38312,\"start\":38311},{\"end\":38324,\"start\":38320},{\"end\":38326,\"start\":38325},{\"end\":38682,\"start\":38680},{\"end\":38696,\"start\":38689},{\"end\":39233,\"start\":39226},{\"end\":39253,\"start\":39246},{\"end\":39273,\"start\":39262},{\"end\":39696,\"start\":39689},{\"end\":39712,\"start\":39706},{\"end\":39724,\"start\":39721},{\"end\":39742,\"start\":39736},{\"end\":39759,\"start\":39751},{\"end\":40143,\"start\":40138},{\"end\":40157,\"start\":40152},{\"end\":40171,\"start\":40166},{\"end\":40498,\"start\":40493},{\"end\":40515,\"start\":40509},{\"end\":40768,\"start\":40759},{\"end\":40785,\"start\":40778},{\"end\":40803,\"start\":40797},{\"end\":40814,\"start\":40811},{\"end\":40831,\"start\":40823},{\"end\":41237,\"start\":41231},{\"end\":41262,\"start\":41258},{\"end\":41285,\"start\":41277},{\"end\":41299,\"start\":41294},{\"end\":41691,\"start\":41690},{\"end\":41947,\"start\":41942},{\"end\":41959,\"start\":41955},{\"end\":41977,\"start\":41966},{\"end\":41992,\"start\":41987},{\"end\":42466,\"start\":42463},{\"end\":42479,\"start\":42473},{\"end\":42493,\"start\":42487},{\"end\":42771,\"start\":42764},{\"end\":42781,\"start\":42777},{\"end\":42797,\"start\":42792},{\"end\":42813,\"start\":42806},{\"end\":42825,\"start\":42818},{\"end\":43217,\"start\":43211},{\"end\":43285,\"start\":43279},{\"end\":43302,\"start\":43293},{\"end\":43315,\"start\":43310},{\"end\":43668,\"start\":43664},{\"end\":43681,\"start\":43675},{\"end\":43693,\"start\":43686},{\"end\":43708,\"start\":43701},{\"end\":43720,\"start\":43713},{\"end\":43728,\"start\":43726}]", "bib_author_last_name": "[{\"end\":29930,\"start\":29926},{\"end\":30153,\"start\":30150},{\"end\":30166,\"start\":30162},{\"end\":30181,\"start\":30177},{\"end\":30190,\"start\":30187},{\"end\":30207,\"start\":30199},{\"end\":30223,\"start\":30216},{\"end\":30238,\"start\":30230},{\"end\":30263,\"start\":30246},{\"end\":30275,\"start\":30271},{\"end\":30616,\"start\":30608},{\"end\":30630,\"start\":30624},{\"end\":30643,\"start\":30637},{\"end\":30660,\"start\":30654},{\"end\":30964,\"start\":30958},{\"end\":30975,\"start\":30973},{\"end\":30988,\"start\":30985},{\"end\":31467,\"start\":31463},{\"end\":31477,\"start\":31473},{\"end\":31493,\"start\":31487},{\"end\":31504,\"start\":31502},{\"end\":31512,\"start\":31510},{\"end\":31524,\"start\":31517},{\"end\":31928,\"start\":31925},{\"end\":31947,\"start\":31937},{\"end\":32253,\"start\":32246},{\"end\":32267,\"start\":32259},{\"end\":32494,\"start\":32489},{\"end\":32508,\"start\":32502},{\"end\":32523,\"start\":32515},{\"end\":32541,\"start\":32532},{\"end\":32560,\"start\":32550},{\"end\":32573,\"start\":32567},{\"end\":32589,\"start\":32582},{\"end\":32958,\"start\":32955},{\"end\":32972,\"start\":32966},{\"end\":32980,\"start\":32977},{\"end\":33001,\"start\":32991},{\"end\":33225,\"start\":33222},{\"end\":33237,\"start\":33234},{\"end\":33252,\"start\":33247},{\"end\":33565,\"start\":33562},{\"end\":33577,\"start\":33574},{\"end\":33592,\"start\":33587},{\"end\":33839,\"start\":33837},{\"end\":33854,\"start\":33849},{\"end\":33868,\"start\":33865},{\"end\":33878,\"start\":33875},{\"end\":34301,\"start\":34295},{\"end\":34318,\"start\":34313},{\"end\":34566,\"start\":34561},{\"end\":34807,\"start\":34800},{\"end\":34818,\"start\":34815},{\"end\":35121,\"start\":35118},{\"end\":35330,\"start\":35320},{\"end\":35347,\"start\":35341},{\"end\":35599,\"start\":35583},{\"end\":35618,\"start\":35611},{\"end\":35636,\"start\":35628},{\"end\":36004,\"start\":36001},{\"end\":36017,\"start\":36014},{\"end\":36028,\"start\":36025},{\"end\":36041,\"start\":36037},{\"end\":36323,\"start\":36315},{\"end\":36338,\"start\":36330},{\"end\":36350,\"start\":36344},{\"end\":36367,\"start\":36361},{\"end\":36379,\"start\":36375},{\"end\":36394,\"start\":36388},{\"end\":36407,\"start\":36401},{\"end\":36420,\"start\":36414},{\"end\":36433,\"start\":36429},{\"end\":36448,\"start\":36443},{\"end\":36815,\"start\":36813},{\"end\":36825,\"start\":36821},{\"end\":37264,\"start\":37259},{\"end\":37275,\"start\":37271},{\"end\":37287,\"start\":37282},{\"end\":37299,\"start\":37295},{\"end\":37495,\"start\":37489},{\"end\":37511,\"start\":37507},{\"end\":37517,\"start\":37513},{\"end\":37526,\"start\":37521},{\"end\":37536,\"start\":37532},{\"end\":37543,\"start\":37538},{\"end\":37553,\"start\":37547},{\"end\":37569,\"start\":37567},{\"end\":37576,\"start\":37571},{\"end\":38302,\"start\":38296},{\"end\":38309,\"start\":38307},{\"end\":38318,\"start\":38313},{\"end\":38331,\"start\":38327},{\"end\":38339,\"start\":38333},{\"end\":38687,\"start\":38683},{\"end\":38700,\"start\":38697},{\"end\":39244,\"start\":39234},{\"end\":39260,\"start\":39254},{\"end\":39281,\"start\":39274},{\"end\":39704,\"start\":39697},{\"end\":39719,\"start\":39713},{\"end\":39734,\"start\":39725},{\"end\":39749,\"start\":39743},{\"end\":39766,\"start\":39760},{\"end\":40150,\"start\":40144},{\"end\":40164,\"start\":40158},{\"end\":40180,\"start\":40172},{\"end\":40507,\"start\":40499},{\"end\":40525,\"start\":40516},{\"end\":40776,\"start\":40769},{\"end\":40795,\"start\":40786},{\"end\":40809,\"start\":40804},{\"end\":40821,\"start\":40815},{\"end\":40837,\"start\":40832},{\"end\":41256,\"start\":41238},{\"end\":41275,\"start\":41263},{\"end\":41292,\"start\":41286},{\"end\":41310,\"start\":41300},{\"end\":41322,\"start\":41312},{\"end\":41700,\"start\":41692},{\"end\":41708,\"start\":41702},{\"end\":41725,\"start\":41710},{\"end\":41953,\"start\":41948},{\"end\":41964,\"start\":41960},{\"end\":41985,\"start\":41978},{\"end\":42002,\"start\":41993},{\"end\":42471,\"start\":42467},{\"end\":42485,\"start\":42480},{\"end\":42502,\"start\":42494},{\"end\":42775,\"start\":42772},{\"end\":42790,\"start\":42782},{\"end\":42804,\"start\":42798},{\"end\":42816,\"start\":42814},{\"end\":42828,\"start\":42826},{\"end\":43223,\"start\":43218},{\"end\":43291,\"start\":43286},{\"end\":43308,\"start\":43303},{\"end\":43330,\"start\":43316},{\"end\":43341,\"start\":43332},{\"end\":43673,\"start\":43669},{\"end\":43684,\"start\":43682},{\"end\":43699,\"start\":43694},{\"end\":43711,\"start\":43709},{\"end\":43724,\"start\":43721},{\"end\":43731,\"start\":43729}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":151963918},\"end\":30141,\"start\":29820},{\"attributes\":{\"doi\":\"arXiv:1803.11175\",\"id\":\"b1\"},\"end\":30571,\"start\":30143},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2316535},\"end\":30880,\"start\":30573},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":308212},\"end\":31404,\"start\":30882},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":57246310},\"end\":31831,\"start\":31406},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":160705},\"end\":32185,\"start\":31833},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":491127},\"end\":32481,\"start\":32187},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b7\"},\"end\":32947,\"start\":32483},{\"attributes\":{\"doi\":\"arXiv:1706.04599\",\"id\":\"b8\"},\"end\":33213,\"start\":32949},{\"attributes\":{\"doi\":\"arXiv:1809.02499\",\"id\":\"b9\"},\"end\":33477,\"start\":33215},{\"attributes\":{\"doi\":\"arXiv:1905.08941\",\"id\":\"b10\"},\"end\":33781,\"start\":33479},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":34224,\"start\":33783},{\"attributes\":{\"doi\":\"arXiv:1801.06146\",\"id\":\"b12\"},\"end\":34487,\"start\":34226},{\"attributes\":{\"doi\":\"arXiv:1801.02929\",\"id\":\"b13\"},\"end\":34715,\"start\":34489},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":71134},\"end\":35052,\"start\":34717},{\"attributes\":{\"doi\":\"arXiv:1408.5882\",\"id\":\"b15\"},\"end\":35258,\"start\":35054},{\"attributes\":{\"id\":\"b16\"},\"end\":35498,\"start\":35260},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6294674},\"end\":35907,\"start\":35500},{\"attributes\":{\"doi\":\"arXiv:1711.09325\",\"id\":\"b18\"},\"end\":36249,\"start\":35909},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10519397},\"end\":36776,\"start\":36251},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11039301},\"end\":37212,\"start\":36778},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53224285},\"end\":37439,\"start\":37214},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1428702},\"end\":38170,\"start\":37441},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4404566},\"end\":38573,\"start\":38172},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3264224},\"end\":39177,\"start\":38575},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1957433},\"end\":39687,\"start\":39179},{\"attributes\":{\"doi\":\"arXiv:1701.06548\",\"id\":\"b26\"},\"end\":40071,\"start\":39689},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":46939880},\"end\":40423,\"start\":40073},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b28\"},\"end\":40698,\"start\":40425},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206593880},\"end\":41229,\"start\":40700},{\"attributes\":{\"doi\":\"arXiv:1905.10964\",\"id\":\"b30\"},\"end\":41603,\"start\":41231},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8142232},\"end\":41940,\"start\":41605},{\"attributes\":{\"doi\":\"arXiv:1806.05236\",\"id\":\"b32\"},\"end\":42378,\"start\":41942},{\"attributes\":{\"doi\":\"arXiv:1708.07747\",\"id\":\"b33\"},\"end\":42700,\"start\":42380},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8485068},\"end\":43207,\"start\":42702},{\"attributes\":{\"id\":\"b35\"},\"end\":43277,\"start\":43209},{\"attributes\":{\"doi\":\"arXiv:1710.09412\",\"id\":\"b36\"},\"end\":43565,\"start\":43279},{\"attributes\":{\"doi\":\"arXiv:1611.06639\",\"id\":\"b37\"},\"end\":43973,\"start\":43567}]", "bib_title": "[{\"end\":29916,\"start\":29820},{\"end\":30598,\"start\":30573},{\"end\":30951,\"start\":30882},{\"end\":31457,\"start\":31406},{\"end\":31917,\"start\":31833},{\"end\":32236,\"start\":32187},{\"end\":33827,\"start\":33783},{\"end\":34793,\"start\":34717},{\"end\":35574,\"start\":35500},{\"end\":36307,\"start\":36251},{\"end\":36807,\"start\":36778},{\"end\":37250,\"start\":37214},{\"end\":37485,\"start\":37441},{\"end\":38285,\"start\":38172},{\"end\":38678,\"start\":38575},{\"end\":39224,\"start\":39179},{\"end\":40136,\"start\":40073},{\"end\":40757,\"start\":40700},{\"end\":41688,\"start\":41605},{\"end\":42762,\"start\":42702}]", "bib_author": "[{\"end\":29932,\"start\":29918},{\"end\":30155,\"start\":30143},{\"end\":30168,\"start\":30155},{\"end\":30183,\"start\":30168},{\"end\":30192,\"start\":30183},{\"end\":30209,\"start\":30192},{\"end\":30225,\"start\":30209},{\"end\":30240,\"start\":30225},{\"end\":30265,\"start\":30240},{\"end\":30277,\"start\":30265},{\"end\":30618,\"start\":30600},{\"end\":30632,\"start\":30618},{\"end\":30645,\"start\":30632},{\"end\":30662,\"start\":30645},{\"end\":30966,\"start\":30953},{\"end\":30977,\"start\":30966},{\"end\":30990,\"start\":30977},{\"end\":31469,\"start\":31459},{\"end\":31479,\"start\":31469},{\"end\":31495,\"start\":31479},{\"end\":31506,\"start\":31495},{\"end\":31514,\"start\":31506},{\"end\":31526,\"start\":31514},{\"end\":31930,\"start\":31919},{\"end\":31949,\"start\":31930},{\"end\":32255,\"start\":32238},{\"end\":32269,\"start\":32255},{\"end\":32496,\"start\":32483},{\"end\":32510,\"start\":32496},{\"end\":32525,\"start\":32510},{\"end\":32543,\"start\":32525},{\"end\":32562,\"start\":32543},{\"end\":32575,\"start\":32562},{\"end\":32591,\"start\":32575},{\"end\":32960,\"start\":32949},{\"end\":32974,\"start\":32960},{\"end\":32982,\"start\":32974},{\"end\":33003,\"start\":32982},{\"end\":33227,\"start\":33215},{\"end\":33239,\"start\":33227},{\"end\":33254,\"start\":33239},{\"end\":33567,\"start\":33555},{\"end\":33579,\"start\":33567},{\"end\":33594,\"start\":33579},{\"end\":33841,\"start\":33829},{\"end\":33856,\"start\":33841},{\"end\":33870,\"start\":33856},{\"end\":33880,\"start\":33870},{\"end\":34303,\"start\":34288},{\"end\":34320,\"start\":34303},{\"end\":34568,\"start\":34553},{\"end\":34809,\"start\":34795},{\"end\":34820,\"start\":34809},{\"end\":35123,\"start\":35113},{\"end\":35332,\"start\":35315},{\"end\":35349,\"start\":35332},{\"end\":35601,\"start\":35576},{\"end\":35620,\"start\":35601},{\"end\":35638,\"start\":35620},{\"end\":36006,\"start\":35995},{\"end\":36019,\"start\":36006},{\"end\":36030,\"start\":36019},{\"end\":36043,\"start\":36030},{\"end\":36325,\"start\":36309},{\"end\":36340,\"start\":36325},{\"end\":36352,\"start\":36340},{\"end\":36369,\"start\":36352},{\"end\":36381,\"start\":36369},{\"end\":36396,\"start\":36381},{\"end\":36409,\"start\":36396},{\"end\":36422,\"start\":36409},{\"end\":36435,\"start\":36422},{\"end\":36450,\"start\":36435},{\"end\":36817,\"start\":36809},{\"end\":36827,\"start\":36817},{\"end\":37266,\"start\":37252},{\"end\":37277,\"start\":37266},{\"end\":37289,\"start\":37277},{\"end\":37301,\"start\":37289},{\"end\":37497,\"start\":37487},{\"end\":37513,\"start\":37497},{\"end\":37519,\"start\":37513},{\"end\":37528,\"start\":37519},{\"end\":37538,\"start\":37528},{\"end\":37545,\"start\":37538},{\"end\":37555,\"start\":37545},{\"end\":37571,\"start\":37555},{\"end\":37578,\"start\":37571},{\"end\":38304,\"start\":38287},{\"end\":38311,\"start\":38304},{\"end\":38320,\"start\":38311},{\"end\":38333,\"start\":38320},{\"end\":38341,\"start\":38333},{\"end\":38689,\"start\":38680},{\"end\":38702,\"start\":38689},{\"end\":39246,\"start\":39226},{\"end\":39262,\"start\":39246},{\"end\":39283,\"start\":39262},{\"end\":39706,\"start\":39689},{\"end\":39721,\"start\":39706},{\"end\":39736,\"start\":39721},{\"end\":39751,\"start\":39736},{\"end\":39768,\"start\":39751},{\"end\":40152,\"start\":40138},{\"end\":40166,\"start\":40152},{\"end\":40182,\"start\":40166},{\"end\":40509,\"start\":40493},{\"end\":40527,\"start\":40509},{\"end\":40778,\"start\":40759},{\"end\":40797,\"start\":40778},{\"end\":40811,\"start\":40797},{\"end\":40823,\"start\":40811},{\"end\":40839,\"start\":40823},{\"end\":41258,\"start\":41231},{\"end\":41277,\"start\":41258},{\"end\":41294,\"start\":41277},{\"end\":41312,\"start\":41294},{\"end\":41324,\"start\":41312},{\"end\":41702,\"start\":41690},{\"end\":41710,\"start\":41702},{\"end\":41727,\"start\":41710},{\"end\":41955,\"start\":41942},{\"end\":41966,\"start\":41955},{\"end\":41987,\"start\":41966},{\"end\":42004,\"start\":41987},{\"end\":42473,\"start\":42463},{\"end\":42487,\"start\":42473},{\"end\":42504,\"start\":42487},{\"end\":42777,\"start\":42764},{\"end\":42792,\"start\":42777},{\"end\":42806,\"start\":42792},{\"end\":42818,\"start\":42806},{\"end\":42830,\"start\":42818},{\"end\":43225,\"start\":43211},{\"end\":43293,\"start\":43279},{\"end\":43310,\"start\":43293},{\"end\":43332,\"start\":43310},{\"end\":43343,\"start\":43332},{\"end\":43675,\"start\":43664},{\"end\":43686,\"start\":43675},{\"end\":43701,\"start\":43686},{\"end\":43713,\"start\":43701},{\"end\":43726,\"start\":43713},{\"end\":43733,\"start\":43726}]", "bib_venue": "[{\"end\":29967,\"start\":29932},{\"end\":30337,\"start\":30293},{\"end\":30711,\"start\":30662},{\"end\":31086,\"start\":30990},{\"end\":31565,\"start\":31526},{\"end\":31993,\"start\":31949},{\"end\":32318,\"start\":32269},{\"end\":32695,\"start\":32607},{\"end\":33059,\"start\":33019},{\"end\":33324,\"start\":33270},{\"end\":33553,\"start\":33479},{\"end\":33957,\"start\":33880},{\"end\":34286,\"start\":34226},{\"end\":34551,\"start\":34489},{\"end\":34869,\"start\":34820},{\"end\":35111,\"start\":35054},{\"end\":35313,\"start\":35260},{\"end\":35687,\"start\":35638},{\"end\":35993,\"start\":35909},{\"end\":36485,\"start\":36450},{\"end\":36904,\"start\":36827},{\"end\":37312,\"start\":37301},{\"end\":37694,\"start\":37578},{\"end\":38359,\"start\":38341},{\"end\":38785,\"start\":38702},{\"end\":39377,\"start\":39283},{\"end\":39857,\"start\":39784},{\"end\":40231,\"start\":40182},{\"end\":40491,\"start\":40425},{\"end\":40916,\"start\":40839},{\"end\":41395,\"start\":41340},{\"end\":41749,\"start\":41727},{\"end\":42140,\"start\":42020},{\"end\":42461,\"start\":42380},{\"end\":42907,\"start\":42830},{\"end\":43400,\"start\":43359},{\"end\":43662,\"start\":43567},{\"end\":31169,\"start\":31088},{\"end\":34021,\"start\":33959},{\"end\":36968,\"start\":36906},{\"end\":37797,\"start\":37696},{\"end\":38855,\"start\":38787},{\"end\":39458,\"start\":39379},{\"end\":40980,\"start\":40918},{\"end\":42971,\"start\":42909}]"}}}, "year": 2023, "month": 12, "day": 17}