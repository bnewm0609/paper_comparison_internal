{"id": 235421961, "updated": "2023-10-06 02:02:03.588", "metadata": {"title": "Graph Neural Networks with Local Graph Parameters", "authors": "[{\"first\":\"Pablo\",\"last\":\"Barcel'o\",\"middle\":[]},{\"first\":\"Floris\",\"last\":\"Geerts\",\"middle\":[]},{\"first\":\"Juan\",\"last\":\"Reutter\",\"middle\":[]},{\"first\":\"Maksimilian\",\"last\":\"Ryschkov\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 6, "day": 12}, "abstract": "Various recent proposals increase the distinguishing power of Graph Neural Networks GNNs by propagating features between $k$-tuples of vertices. The distinguishing power of these\"higher-order'' GNNs is known to be bounded by the $k$-dimensional Weisfeiler-Leman (WL) test, yet their $\\mathcal O(n^k)$ memory requirements limit their applicability. Other proposals infuse GNNs with local higher-order graph structural information from the start, hereby inheriting the desirable $\\mathcal O(n)$ memory requirement from GNNs at the cost of a one-time, possibly non-linear, preprocessing step. We propose local graph parameter enabled GNNs as a framework for studying the latter kind of approaches and precisely characterize their distinguishing power, in terms of a variant of the WL test, and in terms of the graph structural properties that they can take into account. Local graph parameters can be added to any GNN architecture, and are cheap to compute. In terms of expressive power, our proposal lies in the middle of GNNs and their higher-order counterparts. Further, we propose several techniques to aide in choosing the right local graph parameters. Our results connect GNNs with deep results in finite model theory and finite variable logics. Our experimental evaluation shows that adding local graph parameters often has a positive effect for a variety of GNNs, datasets and graph learning tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.06707", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/BarceloGRR21", "doi": null}}, "content": {"source": {"pdf_hash": "11ef232ccfb2f69d0de289d07f88e576126f2b44", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.06707v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "283455d7fa1db217b79b78865e6128e709a0de4c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/11ef232ccfb2f69d0de289d07f88e576126f2b44.txt", "contents": "\nGraph Neural Networks with Local Graph Parameters\n\n\nPablo Barcel\u00f3 \nPUC Chile and IMFD\nChile\n\nFloris Geerts \nUniversity of Antwerp\n\n\nJuan Reutter \nPUC Chile and IMFD\nChile\n\nMaksimilian Ryschkov \nUniversity of Antwerp\n\n\nGraph Neural Networks with Local Graph Parameters\n\nVarious recent proposals increase the distinguishing power of Graph Neural Networks (GNNs) by propagating features between k-tuples of vertices. The distinguishing power of these \"higher-order\" GNNs is known to be bounded by the k-dimensional Weisfeiler-Leman (WL) test, yet their O(n k ) memory requirements limit their applicability. Other proposals infuse GNNs with local higher-order graph structural information from the start, hereby inheriting the desirable O(n) memory requirement from GNNs at the cost of a one-time, possibly non-linear, preprocessing step. We propose local graph parameter enabled GNNs as a framework for studying the latter kind of approaches and precisely characterize their distinguishing power, in terms of a variant of the WL test, and in terms of the graph structural properties that they can take into account. Local graph parameters can be added to any GNN architecture, and are cheap to compute. In terms of expressive power, our proposal lies in the middle of GNNs and their higher-order counterparts. Further, we propose several techniques to aide in choosing the right local graph parameters. Our results connect GNNs with deep results in finite model theory and finite variable logics. Our experimental evaluation shows that adding local graph parameters often has a positive effect for a variety of GNNs, datasets and graph learning tasks.\n\nv\n\n(2) (2)\n\n(2) (2)\n\n(2) (2) G 1 w (0) (0) (0) (0) (0) (0) H 1 Figure 1: Two graphs that are indistinguishable by the WL-test. The numbers between round brackets indicate how many homomorphic images of the 3-clique each vertex is involved in.\n\nAs a consequence, no MPNN can detect that vertex v in Fig. 1 is part of a 3-clique, whereas w is not. Similarly, MPNNs cannot detect that w is part of a 4-cycle, whereas v is not. Further limitations of WL in terms of graph properties can be found, e.g., in F\u00fcrer (2017); Arvind et al. (2020); Chen et al. (2020); Tahmasebi & Jegelka (2020).\n\nTo remedy the weak expressive power of MPNNs, so-called higher-order MPNNs were proposed (Morris et al., 2019;Maron et al., 2019b;Morris et al., 2020), whose expressive power is measured in terms of the k-dimensional WL procedures (k-WL) (Maron et al., 2019a;Chen et al., 2019a;Azizian & Lelarge, 2021;Geerts, 2020;Sato, 2020;Damke et al., 2020). In a nutshell, k-WL operates on k-tuples of vertices and allows to distinguish vertices (graphs) based on structural information related to graphs of treewidth k (Dvorak, 2010;Dell et al., 2018). By definition, WL = 1-WL. As an example, 2-WL can detect that vertex v in Fig. 1 belongs to a 3-clique or a 4-cycle since both have treewidth two. While more expressive than WL, the GNNs based on k-WL require O(n k ) operations in each iteration, where n is the number of vertices, hereby hampering their applicability.\n\nA more practical approach is to extend the expressive power of MPNNs whilst preserving their O(n) cost in each iteration. Various such extensions (Kipf & Welling, 2017;Chen et al., 2019a;Li et al., 2019;Ishiguro et al., 2020;Bouritsas et al., 2020;Geerts et al., 2020) achieve this by infusing MPNNs with local graph structural information from the start. That is, the iterative message passing scheme of MPNNs is run on vertex labels that contain quantitative information about local graph structures. It is easy to see that such architectures can go beyond the WL test: for example, adding triangle counts to MPNNs suffices to distinguish the vertices v and w and graphs G 1 and H 1 in Fig. 1. Moreover, the cost is a single preprocessing step to count local graph parameters, thus maintaining the O(n) cost in the iterations of the MPNN. While there are some partial results showing that local graph parameters increase expressive power (Bouritsas et al., 2020;Li et al., 2019), their precise expressive power and relationship to higher-order MPNNs was unknown, and there is little guidance in terms of which local parameters do help MPNNs and which ones do not. The main contribution of this paper is a precise characterization of the expressive power of MPNNs with local graph parameters and its relationship to the hierarchy of higher-order MPNNs.\n\nOur contributions. In order to nicely formalize local graph parameters, we propose to extend vertex labels with homomorphism counts of small graph patterns. More precisely, given a graphs We recall that homomorphisms are edge-preserving mappings between the vertex sets. graph patterns has often a positive effect, and the cost for computing these counts incurs little to no overhead.\n\nAs such, we believe that F-MPNNs not only provide an elegant theoretical framework for understanding local graph parameter enabled MPNNs, they are also a valuable alternative to higher-order MPNNs as way to increase the expressive power of MPNNs. In addition, and as will be explained in Section 2, F-MPNNs provide a unifying framework for understanding the expressive power of several other existing extensions of MPNNs. Proofs of our results and further details on the relationship to existing approaches and experiments can be found in the appendix.\n\nRelated Work. Works related to the distinguishing power of the WL-test, MPNNs and their higher-order variants are cited throughout the paper. Beyond distinguishability, GNNs are analyzed in terms of universality and generalization properties (Maron et al., 2019c;Keriven & Peyr\u00e9, 2019;Chen et al., 2019b;Garg et al., 2020), local distributed algorithms (Sato et al., 2019;Loukas, 2020), randomness in features (Sato et al., 2020;Abboud et al., 2020) and using local context matrix features (Vignac et al., 2020). Other extensions of GNNs are surveyed, e.g., in Wu et al. (2021); Zhou et al. (2018) and Chami et al. (2021). Related are the Graph Homomorphism Convolutions by NT & Maehara (2020) which apply SVMs directly on the representation of vertices by homomorphism counts. Finally, our approach is reminiscent of the graph representations by means of graphlet kernels (Shervashidze et al., 2009), but then on the level of vertices.\n\n\nLocal Graph Parameter Enabled MPNNs\n\nIn this section we introduce MPNNs with local graph parameters. We start by introducing preliminary concepts.\n\nGraphs. We consider undirected vertex-labelled graphs G = (V, E, \u03c7), with V the set of vertices, E the set of edges and \u03c7 a mapping assigning a label to each vertex in V . The set of neighbours of a vertex is denoted as N G (v) = u \u2208 V {u, v} \u2208 E . A rooted graph is a graph in which one its vertices is declared as its root. We denote a rooted graph by G v , where v \u2208 V is the root and depict them as graphs in which the root is a blackened vertex. Given graphs G = (V G , E G , \u03c7 G ) and H = (H H , E H , \u03c7 H ), an homomorphism h is a mapping h : V G \u2192 V H such that (i) {h(u), h(v)} \u2208 E H for every {u, v} \u2208 E G , and (ii) \u03c7 G (u) = \u03c7 H (h(u)) for every u \u2208 V G . For rooted graphs G v and H w , an homomorphism must additionally map v to w. We denote by hom(G, H) the number of homomorphisms from G to H; similarly for rooted graphs. For simplicity of exposition we focus on vertex-labelled undirected graphs but all our results can be extended to edge-labelled directed graphs.\n\nMessage passing neural networks. The basic architecture for MPNNs (Gilmer et al., 2017), and the one used in recent studies on GNN expressibility (Morris et al., 2019;Xu et al., 2019;Barcel\u00f3 et al., 2020), consists of a sequence of rounds that update the feature vector of every vertex in the graph by combining its current feature vector with the result of an aggregation over the feature vectors of its neighbours. Formally, for a graph G = (V, E, \u03c7), let x M,G,v is a one-hot encoding of its label \u03c7(v). This feature vector is iteratively updated in a number of rounds. In particular, in round d,\nx (d) M,G,v := U (d) x (d\u22121) M,G,v , C (d) {{x (d\u22121) M,G,u | u \u2208 N G (v)}} ,\nwhere C (d) and U (d) are an aggregating and update function, respectively. Thus, the feature vectors x MPNNs with local graph parameters. The GNNs studied in this paper leverage the power of MPNNs by enhancing initial features of vertices with local graph parameters that are beyond their classification power. To illustrate the idea, consider the graphs in Fig. 1. As mentioned, these graphs cannot be distinguished by the WL-test, and therefore cannot be distinguished by the broad class of MPNNs (see e.g. (Xu et al., 2019;Morris et al., 2019)). If we allow a pre-processing stage, however, in which the initial labelling of every vertex v is extended with the number of (homomorphic images of) 3-cliques in which v participates (indicated by numbers between brackets in Fig. 1), then clearly vertices v and w (and the graphs G 1 and H 1 ) can be distinguished based on this extra structural information. In fact, the initial labelling already suffices for this purpose.\n\nLet F = {P r 1 , . . . , P r } be a set of (rooted) graphs, which we refer to as patterns. Then, F-enabled MPNNs, or just F-MPNNs, are defined in the same way as MPNNs with the crucial difference that now the initial feature vector of a vertex v is a one-hot encoding of the label \u03c7 G (v) of the vertex, and all the homomorphism counts from patterns in F. Formally, in each round d an F-MPNN M labels each vertex v in graph G with a feature vector x We note that standard MPNNs are F-MPNNs with F = \u2205. As for MPNNs, we can equip F-MPNNs with a R function that aggregates all final feature vectors into a single feature vector in order to classify or distinguish graphs.\n\nWe emphasise that any MPNN architecture can be turned in an F-MPNN by a simple homomorphism counting preprocessing step. As such, we propose a generic plug-in for a large class of GNN architectures. Better still, homomorphism counts of small graph patterns can be efficiently computed even on large datasets (Zhang et al., 2020) and they form the basis for counting (induced) subgraphs and other notions of subgraphs (Curticapean et al., 2017). Despite the simplicity of F-MPNNs, we will show that they can substantially increase the power of MPNNs by varying F, only paying the one-time cost for preprocessing. v G 2 w H 2 Figure 2: Two graphs and vertices that cannot be distinguished by the WL-test, and therefore by standard MPNNs. When considering F-MPNNs, with F = { }, one can show that both graphs cannot be distinguished just by focusing on the initial labelling, but they can be distinguished by an F-MPNN with just one aggregation layer.\n\nF -MPNNs as unifying framework. An important aspect of F-MPNNs is that they allow a principled analysis of the power of existing extensions of MPNNs. For example, taking F = { } suffices to capture degree-aware MPNNs (Geerts et al., 2020), such as the Graph Convolution Networks (GCNs) (Kipf & Welling, 2017), which use the degree of vertices; taking F = {L 1 , L 2 , . . . , L } for rooted paths L i of length i suffices to model the walk counts used in Chen et al. (2019a); and taking F as the set of labeled trees of depth one precisely corresponds to the use of the WL-labelling obtained after one round by Ishiguro et al. (2020). Furthermore, {C }-MPNNs, where C denotes the cycle of length , correspond to the extension proposed in Section 4 in Li et al. (2019).\n\nIn addition, F-MPNNs can also capture the Graph Structure Networks (GSNs) by Bouritsas et al. (2020), which use subgraph isomorphism counts of graph patterns. We recall that an isomorphism from G to H is a b\u0133ective homomorphism h from G to H which additionally satisfies (i) {h \u22121 (u), h \u22121 (v)} \u2208 E G for every {u, v} \u2208 E H , and (ii) \u03c7 G (h \u22121 (u)) = \u03c7 H (u) for every u \u2208 V H . When G and H are rooted graphs, isomorphisms should preserve the roots as well. Now, in a GSN, the feature vector of each vertex v is augmented with the the counts of every isomorphism from a rooted pattern P r to G v , for rooted patterns P in a set of patterns P, and this is followed by the execution of an MPNN, just as for our F-MPNNs. It now remains to observe that subgraph isomorphism counts can be computed in terms of homomorphism counts, and vice versa (Curticapean et al., 2017). That is, GSNs can be viewed as F-MPNNs and thus our results for F-MPNNs carry over to GSNs. We adopt homomorphism counts instead of subgraph isomorphism counts because homomorphisms counts underly existing characterizations of the expressive power of MPNNs and homomorphism counts are in general more efficient to compute. Also, Curticapean et al. (2017) indicate that all common graph counts are interchangeable in terms of expressive power.\n\nThe use of orbits in Bouritsas et al. (2020) is here ignored, but explained in the appendix.\n\n\nExpressive Power of F -MPNNs\n\nRecall that the standard WL-test (Weisfeiler & Lehman, 1968;Grohe, 2017) iteratively constructs a labelling of the vertices in a graph G = (V, E, \u03c7) as follows. In round d, for each vertex v the algorithm first collects the label of v and all of its neighbours after round d \u2212 1, and then it hashes this aggregated multiset of labels into a new label for v. The initial label of v is \u03c7(v). As shown in Xu et al. (2019) and Morris et al. (2019), the WL-test provides a bound on the classification power of MPNNs: if two vertices or two graphs are indistinguishable by the WL test, then they will not be distinguished by any MPNN.\n\nIn turn, the expressive power of the WL-test, and thus of MPNNs, can be characterised in terms of homomorphism counts of trees (Dvorak, 2010;Dell et al., 2018). This result can be seen as a characterisation of the expressiveness of the WL-test in terms of a particular infinite-dimensional graph kernel: the one defined by the number of homomorphisms from every tree T into the underlying graph G.\n\nIn this section we show that both characterisations extend in an elegant way to the setting of F-MPNNs, confirming that F-MPNNs are not just a useful, but also a well-behaved generalisation of standard MPNNs.\n\n\nCharacterisation in terms of F -WL\n\nWe bound the expressive power of F-MPNNs in terms of what we call the F-WL-test. Formally, the F-WL-test extends WL in the same way as F-MPNNs extend standard MPNNs: by including homomorphism counts of patterns in F in the initial labelling. That is, let F = {P r 1 , . . . , P r }. The F-WL-test is a vertex labelling algorithm that iteratively computes a label \u03c7 (d) F ,G,v for each vertex v of a graph G, defined as follows.\n\u03c7 (0) F ,G,v := \u03c7 G (v), hom(P r 1 , G v ), . . . , hom(P r , G v ) \u03c7 (d) F ,G,v := H \u03c7 (d\u22121) F ,G,v , { {\u03c7 (d\u22121) F ,G,u | u \u2208 N G (v)} } .\nThe F-WL-test stops in round d when no new pair of vertices are identified by means of \u03c7 (d)\n\nF ,G,v , that is, when for any two vertices v 1 and v 2 from G, \u03c7\n(d\u22121) F ,G,v 1 = \u03c7 (d\u22121) F ,G,v 2 implies \u03c7 (d) F ,G,v 1 = \u03c7 (d) F ,G,v 2 . Notice that WL = \u2205-WL.\nWe We can also construct F-MPNNs that mimic the F-WL-test: Simply adding local parameters from a set F of patterns to the GIN architecture of Xu et al. (2019) results in an F-MPNN that classifies vertices and graphs as the F-WL-test.\n\n\nCharacterisation in terms of F -pattern trees\n\nAt the core of several results about the WL-test lies a characterisation linking the test with homomorphism counts of (rooted) trees (Dvorak, 2010;Dell et al., 2018). In view of the connection to MPNNs, it tells that MPNNs only use quantitative tree-based structural information from the underlying graphs. We next extend this characterisation to F-WL by using homomorphism counts of so-called F-pattern trees. In view of the connection with F-MPNNs (Proposition 1), this reveals that F-MPNNs can use quantitative information of richer graph structures than MPNNs.\n\nTo define F-pattern trees we need the graph join operator . Given two rooted graphs G v and H w , the join graph (G H) v is obtained by taking the disjoint union of G v and H w , followed by identifying w with v. The root of the join graph is v. For example, the join of and is . Further, if G is a graph and P r is a rooted graph, then joining a vertex v in G with P r results in the disjoint union of G and P r , where r is identified with v.\n\nLet F = {P r 1 , . . . , P r }. An F-pattern tree T r is obtained from a standard rooted tree S r = (V, E, \u03c7), called the backbone of T r , followed by joining every vertex s \u2208 V with any number of copies of patterns from F.\n\nWe define the depth of an F-pattern tree as the depth of its backbone. Examples of F-pattern trees, for F = { }, are:\n\nwhere grey vertices are part of the backbones of the F-pattern trees. Standard trees are also F-pattern trees.\n\nWe next use F-pattern trees to characterise the expressive power of F-WL and thus, by Proposition 1, of F-MPNNs.\n\nTheorem 1. For any finite collection F of patterns, vertices v and w in a graph G hom(T r , G v ) = hom(T r , G w ) for every rooted F-pattern tree T r . Similarly, G and H are undistinguishable by the F-WL-test if and only if hom(T, G) = hom(T, H), for every (unrooted) F-pattern tree.\n\nThe proof of this theorem, which can be found in the appendix, requires extending techniques from Grohe (2020a,b) that were used to characterise the expressiveness of WL in terms of homomorphism counts of trees.\n\nIn fact, we can make the above theorem more precise. When F-WL is run for d rounds, then only F-patterns trees of depth d are required. This tells that increasing the number of rounds of F-WL results in that more complicated structural information is taken into account For example, consider the two graphs G 2 and H 2 and vertices v \u2208 V G 2 and w \u2208 V H 2 , shown in Fig. 2. Let F = { }. By definition, F-WL cannot distinguish v from w based on the initial labelling.\n\nIf run for one round, Theorem 1 implies that F-WL cannot distinguish v from w if and only if hom(T r , G v 2 ) = hom(T r , H w 2 ) for any F-pattern tree of depth at most 1. It is readily verified that\nhom( , G v 2 ) = 0 = 4 = hom( , H w 2 ),\nand thus F-WL distinguishes v from w after one round. Similarly, G 2 and H 2 can be distinguished by F-WL after one round. We observe that G 2 and H 2 are indistinguishable by WL. Hence, F-MPNNs are more expressive than MPNNs. Importantly, Theorem 1 discloses the boundaries of F-MPNNs. To illustrate this for some specific instances of F-MPNNs mentioned earlier, the expressive power of degree-based MPNNs (Kipf & Welling, 2017;Geerts et al., 2020) is captured by {L 1 }-pattern trees, and walk counts-MPNNs (Chen et al., 2019a) are captured by {L 1 , . . . , L }-pattern trees. These pattern trees are just trees, since joining paths to trees only results in bigger trees. Thus, Theorem 1 tells that all these extensions are still bounded by WL (albeit needing less rounds). In contrast, beyond WL, {C }-pattern trees capture cycle count MPNNs (Li et al., 2019), and for GSNs (Bouritsas et al., 2020) which use subgraph isomorphism counts of pattern P \u2208 P, their expressive power is captured by spasm(P)-pattern trees, where spasm(P) consists of all surjective homomorphic images of patterns in P (Curticapean et al., 2017).\n\n\nA Comparison with the k-WL-test\n\nWe propose F-MPNNs as an alternative and efficient way to extend the expressive power of MPNNs (and thus the WL-test) compared to the computationally intensive higher-order MPNNs based on the k-WL-test (Morris et al., 2019(Morris et al., , 2020Maron et al., 2019a). In this section we situate F-WL in the k-WL hierarchy. The definition of k-WL is deferred to the appendix.\n\nWe have seen that F-WL can distinguish graphs that WL cannot: it suffices to consider {K 3 }-WL for the 3-clique K 3 . In order to generalise this observation we need some notation. Let F and G be two sets of patterns and consider an F-MPNN M and a G-MPNN N . We say that M is upper bounded in expressive power by N if for any graph G, if N cannot distinguish vertices v and w, then neither can M . A similar notion is in place for pairs of graphs: if N cannot distinguish graphs G and H, then neither can M .\n\nMore generally, let M be a class of F-MPNN and N be a class of G-MPNN. We say that the class M is upper bounded in expressive power by N if every M \u2208 M is upper bounded in expressive power by an N \u2208 N (which may depend on M ). When M is upper bounded by N and vice versa, then M and N are said to have the same expressive power. A class N is more expressive than a class M when M is upper bounded in expressive power by N , but there exist graphs that can be distinguished by MPNNs in N but not by any MPNN in M.\n\nFinally, we use the notion of treewidth of a graph, which measures the tree-likeness of a graph. For example, trees have treewidth one, cycles have treewidth two, and the k-clique K k has treewidth Just as for the F-WL-test, an F-MPNN cannot distinguish two vertices if the label computed for both of them is the same k \u2212 1 (for k > 1). We define this standard notion in the appendix and only note that we define the treewidth of a pattern P r as the treewidth of its unrooted version P .\n\nOur first result is a consequence of the characterisation of k-WL in terms of homomorphism counts of graphs of treewidth k (Dvorak, 2010;Dell et al., 2018).\n\n\nProposition 2.\n\nFor each finite set F of patterns, the expressive power of F-WL is bounded by k-WL, where k is the largest treewidth of a pattern in F.\n\nFor example, since the treewidth of K 3 is 2, we have that {K 3 }-WL is bounded by 2-WL. Similarly, {K k+1 }-WL is bounded in expressive power by k-WL.\n\nOur second result tells how to increase the expressive power of F-WL beyond k-WL. A pattern P r is a core if any homomorphism from P to itself is injective. For example, any clique K k and cycle of odd length is a core.\n\nTheorem 2. Let F be a finite set of patterns. If F contains a pattern P r which is a core and has treewidth k, then there exist graphs that can be distinguished by F-WL but not by (k \u2212 1)-WL.\n\nIn other words, for such F, F-WL is not bounded by (k \u2212 1)-WL. For example, since K 3 is a core, {K 3 }-WL is not bounded in expressive power by WL = 1-WL. More generally, {K k }-WL is not bounded by (k \u2212 1)-WL. The proof of Theorem 2 is based on extending deep techniques developed in finite model theory, and that have been used to understand the expressive power of finite variable logics (Atserias et al., 2007;Bova & Chen, 2019). This result is stronger than the one underlying the strictness of the k-WL hierarchy (Otto, 2017), which states that k-WL is strictly more expressive than (k \u2212 1)-WL. Indeed, Otto (2017) only shows the existence of a pattern P r of treewidth k such that (k \u2212 1)-WL is not bounded by {P r }-WL. In Theorem 2 we provide an explicit recipe for finding such a pattern P r , that is, P r can be taken a core of treewidth k.\n\nIn summary, we have shown that there is a set F of patterns such that (i) F-WL can distinguish graphs which cannot be distinguished by (k \u2212 1)-WL, yet (ii) F-WL cannot distinguish more graphs than k-WL. This begs the question whether there is a finite set F such that F-WL is equivalent in expressive power to k-WL. We answer this negatively.\n\nProposition 3. For any k > 1, there does not exist a finite set F of patterns such that F-WL is equivalent in expressive power to k-WL.\n\nIn view of the connection between F-MPNNs and GSNs mentioned earlier, we thus show that no GSN can match the power of k-WL, which was a question left open in Bouritsas et al. (2020). We remark that if we allow F to consist of all (infinitely many) patterns of treewidth k, then F-WL is equivalent in expressive power to k-WL (Dvorak, 2010;Dell et al., 2018).\n\n\nWhen Do Patterns Extend Expressiveness?\n\nPatterns are not learned, but must be passed as an input to MPNNs together with the graph structure. Thus, knowing which patterns work well, and which do not, is of key importance for the power of the resulting F-MPNNs. This is a difficult question to answer since determining which patterns work well is clearly application-dependent. From a theoretical point of view, however, we can still look into interesting questions related to the problem of which patterns to choose. One such a question, and the one studied in this section, is when a pattern adds expressive power over the ones that we have already selected. More formally, we study the following problem: Given a finite set F of patterns, when does adding a new pattern P r to F extends the expressive power of the F-WL-test?\n\nTo answer this question in the positive, we need to find two graphs G and H, show that they are indistinguishable by the F-WL-test, but show that they can be distinguished by the F \u222a {P r }-WLtest. As an example of this technique we show that longer cycles always add expressive power. We use C k to represent the cycle of length k.\n\nProposition 4. For any k > 3, {C r 3 , . . . , C r k }-WL is more expressive than {C r 3 , . . . , C r k\u22121 }-WL. We also observe that, by Proposition 2, {C r 3 , . . . , C r k }-WL is bounded by 2-WL for any k \u2265 3 because cycles have treewidth two. It is often the case, however, that finding such graphs and proving that they are indistinguishable, can be rather challenging. Instead, in this section we provide two techniques that can be used to partially answer the question posed above by only looking at properties of the sets of patterns. Our first result is for establishing when a pattern does not add expressive power to a given set F of patterns, and the second one when it does.\n\n\nDetecting when patterns are superfluous\n\nOur first result is a simple recipe for choosing local features: instead of choosing complex patterns that are the joins of smaller patterns, one should opt for the smaller patterns.\n\nProposition 5. Let P r = P r 1 P r 2 be a pattern that is the join of two smaller patterns. Then for any set F of patterns, we have that F \u222a {P r } is upper bounded by F \u222a {P r 1 , P r 2 }. Stated differently, this means that adding to F any pattern which is the join of two patterns already in F does not add expressive power.\n\nThus, instead of using, for example, the pattern , one should prefer to use instead the triangle . This result is in line with other advantages of smaller patterns: their homomorphism counts are easier to compute, and, since they are less specific, they should tend to produce less over-fitting.\n\n\nDetecting when patterns add expressiveness\n\nJoining patterns into new patterns does not give extra expressive power, but what about patterns which are not joins? We provide next a useful recipe for detecting when a pattern does add expressive power. We recall that the core of a graph P is its unique (up to isomorphism) induced subgraph which is a core.\n\nTheorem 3. Let F be a finite set of patterns and let Q r be a pattern whose core has treewidth k. Then, F \u222a {Q r }-WL is more expressive than F-WL if every pattern P r \u2208 F satisfies one of the following conditions: (i) P r has treewidth < k; or (ii) P r does not map homomorphically to Q r .\n\nAs an example, {K 3 , . . . , K k }-WL is more expressive than {K 3 , . . . , K k\u22121 }-WL for any k > 3 because of the first condition. Similarly, {K 3 , . . . , K k , C }-WL is more expressive than {K 3 , . . . , K k }-WL for odd cycles C . Indeed, such cycles are cores and no clique K k with k > 2 maps homomorphically to C .\n\n\nExperiments\n\nWe next showcase that GNN architectures benefit when homomorphism counts of patterns are added as additional vertex features. For patterns where homomorphism and subgraph isomorphism counts differ (e.g., cycles) we compare with GSNs (Bouritsas et al., 2020). We use the benchmark for GNNs by Dwivedi et al. (2020), as it offers a broad choice of models, datasets and graph classification tasks. Patterns. We extend the initial features of vertices with homomorphism counts of cycles C of length \u2264 10, when molecular data (ZINC) is concerned, and with homomorphism counts of k-cliques K k for k \u2264 5, when social or collaboration data (PATTERN, CLUSTER, COLLAB) is concerned. We use the z-score of the logarithms of homomorphism counts to make them standardnormally distributed and comparable to other features. Section 5 tells us that all these patterns will increase expressive power (Theorem 3 and Proposition 4) and are \"minimal\" in the sense that they are not the join of smaller patterns (Proposition 5). Similar pattern choices were used in Bouritsas et al. (2020). We use DISC (Zhang et al., 2020) , a tool specifically built to get homomorphism counts for large datasets. Each model is trained and tested independently using combinations of patterns.\n\nHigher-order GNNs. We do not compare to higher-order GNNs since this was already done by Dwivedi et al. (2020). They included ring-GNNs (which outperform 2WL-GNNs) and 3WL-GNNs in their experiments, and these were outperformed by our selected \"linear\" architectures. Although the increased expressive power of higher-order GNNs may be beneficial for learning, scalability\n\nWe thank the authors for providing us with an executable. and learning issues (e.g., loss divergence) hamper their applicability (Dwivedi et al., 2020). Our approach thus certainly outperforms higher-order GNNs with respect to the benchmark.\n\n\nMethodology.\n\nGraphs were divided between training/test as instructed by Dwivedi et al. (2020), and all numbers reported correspond to the test set. The reported performance is the average over four runs with different random seeds for the respective combinations of patterns in F, model and dataset. Training times were comparable to the baseline of training models without any augmented features. All models for ZINC, PATTERN and COLLAB were trained on a GeForce GTX 1080 Ti GPU, for CLUSTER a Tesla V100-SXM3-32GB GPU was used. Next we summarize our results for each learning task separately.\n\nGraph regression. The first task of the benchmark is the prediction of the solubility of molecules in the ZINC dataset (Irwin et al., 2012a;Dwivedi et al., 2020), a dataset of about 12 000 graphs of small size, each of them consisting of one particular molecule. The results in Table 1 show that each of our models indeed improves by adding homomorphism counts of cycles and the best result is obtained by considering all cycles. GSNs were applied to the ZINC dataset as well (Bouritsas et al., 2020). In Table 1 we also report results by using subgraph isomorphism counts (as in GSNs): homomorphism counts generally provide better results than subgraph isomorphisms counts. Our best result (framed in Table 1) is competitive to the value of 0.139 reported in Bouritsas et al. (2020). By looking at the full results, we see that some cycles are much more important than others. Table  2 shows which cycles have greatest impact for the worst-performing baseline, GAT. Remarkably, adding homomorphism counts makes the GAT model competitive with the best performers of the benchmark.\n\nVertex classification. The next task in the benchmark corresponds to vertex classification. Here we analyze two datasets, PATTERN and CLUSTER (Dwivedi et al., 2020), both containing over 12 000 artificially generated graphs resembling social networks or communities. The task is to Code to reproduce our experiments is available at https://github.com/LGP-GNN-2021/LGP-GNN \nS (F) MAE N 0.47\u00b10.02 {C 3 } 0.45\u00b10.01 {C 4 } 0.34\u00b10.02 {C 6 } 0.31\u00b10.01 {C 5 , C 6 } 0.28\u00b10.01 {C 3 , . . . , C 6 } 0.23\u00b10.01 {C 3 , .\n. . , C 10 } 0.22\u00b10.01 Table 3: Results for the PATTERN dataset show that homomorphism counts improve all models except GatedGCN. We compare weighted accuracy of each model without any homomorphism count (baseline) against the model augmented with the counts of the set F that showed best performance (best F).\nM + F A A GAT{K 3 , K 4 , K 5 } 78.83 \u00b1 0.60 85.50 \u00b1 0.23 GCN{K 3 , K 4 , K 5 } 71.42 \u00b1 1,38 82.49 \u00b1 0.48 GraphSage {K 3 , K 4 , K 5 } 70.78 \u00b1 0,19 85,85 \u00b1 0.15 MoNet {K 3 , K 4 , K 5 } 85.90 \u00b1 0,03 86.63 \u00b1 0.03 GatedGCN {\u2205}\n86.15 \u00b1 0.08 86.15 \u00b1 0.08 predict whether a vertex belongs to a particular cluster or pattern, and all results are measured using the accuracy of the classifier. Also here, our results show that homomorphism counts, this times of cliques, tend to improve the accuracy of our models. Indeed, for the PATTERN dataset we see an improvement in all models but GatedGCN (Table 3), and three models are improved in the CLUSTER dataset (reported in the appendix). Once again, the best performer in this task is a model that uses homomorphism counts. We remark that for cliques, homomorphism counts coincide with subgraph isomorphism counts (up to a constant factor) so our extensions behave like GSNs.\n\nLink prediction In our final task we consider a single graph, COLLAB (Hu et al., 2020), with over 235 000 vertices, containing information about the collaborators in an academic network, and the task at hand is to predict future collaboration. The metric used in the benchmark is the Hits@50 evaluator (Hu et al., 2020). Here, positive collaborations are ranked among randomly sampled negative collaborations, and the metric is the ratio of positive edges that are ranked at place 50 or above. Once again, homomorphism counts of cliques improve the performance of all models, see Table 4. An interesting observation is that this time the best set of features (cliques) does depend on the model, although the best model uses all cliques again. Remarks. The best performers in each task use homomorphism counts, in accordance with our theoretical results, showing that such counts do extend the power of MPNNs. Homomorphism counts are also cheap to compute. For COLLAB, the largest graph in our experiments, the homomorphism counts of all patterns we used, for all vertices, could be computed by DISC (Zhang et al., 2020) in less than 3 minutes. One important remark is that selecting the best set of features is still a challenging endeavor. Our theoretical results help us streamline this search, but for now it is still an exploratory task. In our experiments we first looked at adding each pattern individually, and then tried with combinations of those that showed the best improvements. This feature selection strategy incurs considerable cost, both computational and environmental, and needs further investigation.\n\n\nConclusion\n\nWe propose local graph parameter enabled MPNNs as an efficient way to increase the expressive power of MPNNs. The take-away message is that enriching features with homomorphism counts of small patterns is a promising add-on to any GNN architecture, with little to no overhead. Regarding future work, the problem of which parameters to choose deserves further study. In particular, we plan to provide a complete characterisation of when adding a new pattern to F adds expressive power to the F-WL-test.\n\nChen, Z., Chen, L., Villar, S., and Bruna, J. \n\n\nAppendix\n\n\nA Proofs of Section 3\n\nWe use the following notions. Let G and H be graphs, v \u2208 V G , w \u2208 V H , and d \u2265 0. The vertices v and w are said to be indistinguishably by\nF-WL in round d, denoted by (G, v) \u2261 (d) F -WL (H, w), iff \u03c7 (d) F ,G,v = \u03c7 (d) F ,H,w . Similarly, G and H are said to be indistinguishable by F-WL in round d, denoted by G \u2261 (d) F -WL H, iff { {\u03c7 (d) F ,G,v | v \u2208 V G } } = { {\u03c7 (d) F ,H,w | w \u2208 V H } }. Along the same lines, v and w are indistinguishable by an F-MPNN M , denoted by (G, v) \u2261 (d) M,F (H, w), iff x (d) M,F ,G,v = x (d) M,F ,H,w . Similarly, G and H are said to be indistinguishable by M in round d, denoted by G \u2261 (d) M,F H, iff { {x (d) M,F ,G,v | v \u2208 V G } } = { {x (d) M,F ,H,w | w \u2208 V H } }.\n\nA.1 Proof of Proposition 1\n\nWe show that the class of F-MPNNs is upper bounded in expressive power by F-WL. The proof is analogous to the proof in Morris et al. (2019) showing that MPNNs are bounded by WL.\n\nWe show a stronger result by upper bounding F-MPNNs by F-WL-test, layer by layer. More precisely, we show that for every F-MPNN M , graphs G and H, vertices v \u2208 V G , w \u2208 V H , and d \u2265 0,\n(1) (G, v) \u2261 (d) F -WL (H, w) =\u21d2 (G, v) \u2261 (d) M,F (H, w); and (2) G \u2261 (d) F -WL H =\u21d2 G \u2261 (d) M,F H.\nClearly, these imply that F-MPNNs are bounded in expressive power by F-WL, both when vertex and graph distinguishability are concerned. Proof of implication (1). We show this implication by induction on the number of rounds. Base case. We first assume\n(G, v) \u2261 (0) F -WL (H, w). In other words, \u03c7 (0) F ,G,v = \u03c7 (0) F ,H,w and thus, \u03c7 G (v) = \u03c7 H (w) and for every P r \u2208 F we have hom(P r , G v ) = hom(P r , H w ). By definition, x (0) M,F ,G,v is a hot-one encoding of \u03c7 G (v) combined with hom(P r , G v ) for P r \u2208 F, for every MPNN M , graph G and vertex v \u2208 V G .\nSince these agree with the labelling and homomorphism counts\nfor vertex w \u2208 V H in graph H, we also have that x (0) M,F ,G,v = x (0) M,F ,H,w , as desired. Inductive step. We next assume (G, v) \u2261 (d) F -WL (H, w). By the definition of F-WL this is equivalent to (G, v) \u2261 (d\u22121) F -WL (H, w) and { {\u03c7 (d\u22121) F ,G,v | v \u2208 N G (v)} } = { {\u03c7 (d\u22121) F ,H,w | w \u2208 N H (w)} }. By the induction hypothesis, this implies (G, v) \u2261 (d\u22121) M,F (H, w) and there exists a b\u0133ection \u03b2 : N G (v) \u2192 N H (w) such that (G, v ) \u2261 (d\u22121) M,F (H, \u03b2(v )) for every v \u2208 N G (v), and every F-MPNN M . In other words, x (d\u22121) M,F ,G,v = x (d\u22121) M,F ,H,w and x (d\u22121) M,F ,G,v = x (d\u22121) M,F ,H,\u03b2(v ) for every v \u2208 N G (v). By the definition of F-MPNNs this implies that C (d) { {x (d\u22121) M,F ,G,v | v \u2208 N G (v)} } is equal to C (d) { {x (d\u22121)\nM,F ,H,w | w \u2208 N G (w)} } and hence also, after applying U\n(d) , x (d) M,F ,G,v = x (d) M,F ,H,w . That is, (G, u) \u2261 (d) M,F (H, w), as desired. Proof of implication (2). The implication G \u2261 (d) F -WL H =\u21d2 G \u2261 (d) M,F H now easily follows. Indeed, G \u2261 (d) F -WL H is equivalent to { {\u03c7 (d) F ,G,v | v \u2208 V G } } = { {\u03c7 (d) F ,H,w | w \u2208 V H } }. In other words, there exists a b\u0133ection \u03b1 : V G \u2192 V H such that \u03c7 (d) F ,G,v = \u03c7 (d) F ,H,\u03b1(v) for every v \u2208 V G . We have just shown that this implies x (d) M,F ,G,v = x (d) M,F ,H,\u03b1(v) for every v \u2208 V G and for every F-MPNN M . Hence, { {x (d) M,F ,G,v | v \u2208 V G } } = { {x (d) M,F ,H,w | w \u2208 V H } }, or G \u2261 (d)\nM,F H, as desired.\n\n\nA.2 Proof of Theorem 1\n\nWe show that for any finite collection F of patterns, graphs G and H, vertices v \u2208 V G and w \u2208 V H , and d \u2265 0:\n(G, v) \u2261 (d) F -WL (H, w) \u21d0\u21d2 hom(T r , G v ) = hom(T r , H w ),(1)\nfor every F-pattern tree T r of depth at most d. Similarly,\nG \u2261 (d) F -WL H \u21d0\u21d2 hom(T, G) = hom(T, H),(2)\nfor every (unrooted) F-pattern tree of depth at most d. For a given set F = {P r 1 , . . . , P r } of patterns and s = (s 1 , . . . , s ) \u2208 N , we denote by F s the graph pattern of the form (P s 1 1 \u00b7 \u00b7 \u00b7 P s ) r , that is, we join s 1 copies of P 1 , s 2 copies of P 2 and so on. Proof of equivalence (1). The proof is by induction on the number of rounds d.\n\n=\u21d2 We first consider the implication (G, v) \u2261 (d)\n\nF -WL (H, w) =\u21d2 hom(T r , G v ) = hom(T r , H w ) for every F-pattern tree T r of depth at most d. Base case. Let us first consider the base case, that is, d = 0. In other words, we consider F-pattern trees T r consisting of a single root r adorned with a pattern F s for some s = (s 1 , . . . , s ) \u2208 N . We note that due to the properties of the graph join operator:\nhom(T r , G v ) = i=1 hom(P r i , G v ) s i .(3)\nSince, (G, v) \u2261\n\nF -WL (H, w), we know that \u03c7 G (v) = \u03c7 H (w) = a for some a \u2208 \u03a3 and hom(P r i , G v ) = hom(P r i , H w ) for all P r i \u2208 F. This implies that the product in (3) \nis equal to i=1 hom(P r i , H w ) s i = hom(T r , H w ),\nas desired.\n\nInductive step. Suppose next that we know that the implication holds for d \u2212 1. We assume now\n(G, v) \u2261 (d)\nF -WL (H, w) and consider an F-pattern tree T r of depth at most d. Assume that in the backbone of T r , the root r has m children c 1 , . . . , c m , and denote by T c 1 1 , . . . , T c m the F-pattern trees in T r rooted at c i . Furthermore, we denote by T (r,c i ) i the F-pattern tree obtained from T c i i by attaching r to c i ; T (r,c i ) i has root r. Let F s be the pattern in T r associated with r. The following equalities are readily verified:\nhom(T r , G v ) = hom(F s , G v ) m i=1 hom(T (r,c i ) i , G v ) = hom(F s , G v ) m i=1 v \u2208N G (v) hom(T c i i , G v ) .(4)\nRecall now that we assume (G, v) \u2261 \n\nF -WL (H, w). Hence, by induction, hom(S r , G v ) = hom(S r , H w ) for every F-pattern tree S r of depth 0. In particular, this holds for S r = F s and hence\nhom(F s , G v ) = hom(F s , H w ). Furthermore, (G, v) \u2261 (d) F -WL (H, w) implies that there exists a b\u0133ection \u03b2 : N G (v) \u2192 N H (w) such that (G, v ) \u2261 (d\u22121) F -WL (H, \u03b2(v )) for every v \u2208 N G (v). By induction, for every v \u2208 N G (v) there thus exists a unique w \u2208 N H (w) such that hom(S r , G v ) = hom(S r , H w ) for every F-pattern tree S r of depth at most d \u2212 1. In particular, for every v \u2208 N G (v) there exists a w \u2208 N H (w) such that hom(T c i i , G v ) = hom(T c i i , H w ),\nfor each of the sub-trees T c i i in T r . Hence, (4) is equal to\nhom(F s , H w ) m i=1 w \u2208N H (w) hom(T c i i , H w ) ,\nwhich in turn is equal to hom(T r , H w ), as desired.\n\n\u21d0= We next consider the other direction, that is, we show that when hom(T r , G v ) = hom(S r , H w ) holds for every F-pattern tree T r of depth at most d,\nthen (G, v) \u2261 (d) F -WL (H, w)\nholds. This is again verified by induction on d. This direction is more complicated and is similar to techniques used in Grohe (2020b). In our induction hypothesis we further include that a finite number of F-pattern trees suffices to infer (G, v) \u2261 (d) F -WL (H, w) for graphs G and H and vertices v \u2208 V G and w \u2208 V H . Base case. Let us consider the base case d = 0 first. We need to show that \u03c7 G (v) = \u03c7 H (w) and hom(P r i , G v ) = hom(P r i , H w ) for every P r i \u2208 F, since this implies (G, v) \u2261\n\nF -WL (H, w). We first observe that hom(T r , G v ) = hom(T r , H w ) for every F-pattern tree T r of depth 0, implies that v and w must be assigned the same label, say a, by \u03c7 G and \u03c7 H , respectively.\n\nIndeed, if we take T r to consist of a single root r labeled with a (and thus r is associated with the pattern F 0 ), then hom(T r , G v ) = hom(T r , H w ) will be one if \u03c7 G (v) = \u03c7 H (w) = a and zero otherwise. This implies that \u03c7 G (v) = \u03c7 H (w) = a.\n\nNext, we show that hom(P r i , G v ) = hom(P r i , H w ) for every P r i \u2208 F. It suffices to consider the F-pattern tree T r i consisting of a root r joined with a single copy of P r i . We observe that we only need a finite number of F-pattern trees to infer (G, v) \u2261 (0) F -WL (H, w). Indeed, suppose that \u03c7 G and \u03c7 H assign labels a 1 , . . . , a L , then we need L single vertex trees with no patterns attached and root labeled with one of these labels. In addition, we need one F-pattern tree for each pattern P r i \u2208 F and each label a 1 , . . . , a L . That is, we need L( + 1) F-pattern trees of depth 0.\n\nInductive step. We now assume that the implication holds for d \u2212 1 and consider d. That is, we assume that if hom(T r , G v ) = hom(T r , H w ) holds for every F-pattern tree T r of depth at most\nd \u2212 1, then (G, v) \u2261 (d\u22121) F -WL (H, w) holds. Furthermore, we assume that only a finite number K of F-pattern trees S r 1 , . . . , S r K of depth at most d \u2212 1 suffice to infer (G, v) \u2261 (d\u22121)\nF -WL (H, w). So, for d, let us assume that hom(T r , G v ) = hom(T r , H w ) holds for every F-pattern tree of depth at most d. We need to show (G, v) \u2261 (d) F -WL (H, w) and that we can again assume that a finite number of F-pattern trees of depth at most d suffice to infer\n(G, v) \u2261 (d) F -WL (H, w). By definition of (G, v) \u2261 (d) F -WL (H, w), we can, equivalently, show that (G, v) \u2261 (d\u22121) F -WL (H, w)\nand that there exists a b\u0133ection \u03b2 :\nN G (v) \u2192 N H (w) such that (G, v ) \u2261 (d\u22121) F -WL (H, \u03b2(v )) for every v \u2208 N G (v). That (G, v) \u2261 (d\u22121) F -WL (H, w) holds, is by induction, since hom(T r , G v ) = hom(T r , H w )\nfor every F-pattern tree of depth at most d and thus also for every F-pattern tree of depth at most d \u2212 1. We may thus focus on showing the existence of the b\u0133ection \u03b2.\n\nLet X, Y \u2208 {G, H}, x \u2208 V X and y \u2208 V Y . We know, by induction and the proof of the previous implication, that (X, x) \u2261\n(d\u22121) F -WL (Y, y) if and only if hom(S r i , X x ) = hom(S r i , Y y ) for each i \u2208 K. Denote by R 1 , . . . , R e the equivalence class on V X \u222a V Y induced by \u2261 (d\u22121)\nF -WL . Furthermore, define N j,X (x) := N X (x) \u2229 R j and let n j = |N j,G (v)| and m j = |N j,H (w)| for v \u2208 V G and w \u2208 V H , for each j \u2208 [e]. If we can show that n j = m j for each j \u2208 [e], then this implies the existence of the desired b\u0133ection.\n\nLet T r=a i be the F-pattern tree of depth at most d obtained by attaching S r i to a new root vertex r labeled with a. We may assume that v and w both have label a, since their homomorphism counts for the single root trees with labels from \u03a3. The root vertex r is not joined with any F s (or alternatively it is joined with F 0 ). It will be convenient to denote the root of S r i by r i instead of r. Then for each i \u2208 [K]:\nhom(T r=a i , G v ) = v \u2208N G (v) hom(S r i i , G v ) = j\u2208[e] n j hom(S r i i , G v j ) = j\u2208[e] m j hom(S r i i , H w j ) = w \u2208N H (w) hom(S r i i , H w ) = hom(T r=a i , H w ),\nwhere v j and w j denote arbitrary vertices in N j,G (v) and N j,H (w), respectively. Let us denote hom(S r i i , G v j ) by a ij and observe that this is equal to hom(S r i i , H w j ). Hence, we know that for each i \u2208 [K]:\nj\u2208[e] a ij n j = j\u2208[e]\na ij m j .\n\nLet us call a set I \u2286 [K] compatible if all roots in S r i i , for i \u2208 I, have the same label. Consider a vector s = (s 1 , . . . , s K ) \u2208 N K and define its support as supp(s) := {i \u2208 [K] | s i = 0}. We say that s is compatible if its support is. For such a compatible s we now define T r=a,s to be the F-pattern tree with root r labeled with a, with one child c which is joined with (and inheriting the label from) the following F-pattern tree of depth d \u2212 1: i\u2208supp(s) S s i i . In other words, we simply join together powers of the S r i i 's that have roots with the same label. Then for every compatible s \u2208 N K :\nhom(T r=a,s , G v ) = v \u2208N G (v) i\u2208[K] hom(S r i i , G v ) s i = j\u2208[e] n j i\u2208[K] hom(S r i i , G v j ) s i = j\u2208[e] m j i\u2208[K] hom(S r i i , H w j ) s i = w \u2208N H (w) i\u2208[K] hom(S r i i , H w ) s i = hom(T r=a,s i , H w ),\nwhere, as before, v j and w j denote arbitrary vertices in N j,G (v) and N j,H (w), respectively. Hence, for any compatible s \u2208 N K :\nj\u2208[e] n j i\u2208[K] a s i ij = j\u2208[e] m j i\u2208[K] a s i ij .\nWe now continue in the same way as in the proof of Lemma 4.2 in Grohe (2020b). We repeat the argument here for completeness. Define a s\nj := K i=1 a s i ij for each j \u2208 [e]\n. We assume, for the sake of contradiction, that there exists a j \u2208 [e] such that n j = m j . We choose such a j 0 \u2208 [e] for which S = supp(a j 0 ) is inclusion-wise maximal.\n\nWe first rule out that S = \u2205. Indeed, suppose that S = \u2205. This implies that a j 0 = 0. Now observe that a j and a j are mutually distinct for all j, j \u2208 [e], j = j . Indeed, if they were equal then this would imply that R j = R j . Hence, supp(a j ) = \u2205 for any j = j 0 . We note that n j = m j for all j = j 0 by the maximality of S. Hence, n j 0 = n \u2212 j =j 0 n j = n \u2212 j =j 0 m j = m j 0 , contradicting our assumption. Hence, S = \u2205.\n\nConsider J := {j \u2208 [e] | supp(a j ) = S}. For each j \u2208 J, consider the truncated vector a j := (a ij | i \u2208 S). We note that\u00e2 j , for j \u2208 J, all have positive entries and are mutually distinct. Lemma 4.1 in Grohe (2020b) implies that we can find a vector (with non-zero entries) s = (\u015d i | i \u2208 S) such that the numbers\u00e2\u015d j for j \u2208 J are mutually distinct as well. We next consider s = (s 1 , . . . , s K ) with s i =\u015d i if i \u2208 S and s i = 0 otherwise. Then by definition of\u015d, also a s j for j \u2208 J are mutually distinct.\n\nWe next note that for every p \u2208 N, a ps j = (a s j ) p and if we define A to be the |J|\u00d7|J|-matrix such that A jj := a j s j then this will be an invertible matrix (Vandermonde). We use this invertibility to show that n j 0 = m j 0 .\n\nLet n J := (n j | j \u2208 J) and m J = (m j | j \u2208 J). If we inspect the j th entry of n J \u00b7 A, then this is equal to j\u2208J n j a j s j = n j a j s j .\n\nTo see that this holds, we verify that when S \u2286 supp(a j ) then a j s j = 0. Indeed, take an \u2208 S such that \u2208 supp(a j ). Then, a j s j contains the factor a j s j = 0 s with s =\u015d = 0. Hence, a j s j = 0. Now, by the maximality of S, for all j with S \u2282 supp(a j ) we have n j = m j and thus\nj\u2208[e] S\u2282supp(a j ) n j a j s j = j\u2208[e] S\u2282supp(a j ) m j a j s j .\nSince j\u2208[e] n j a j s j = j\u2208[e] m j a j s j , we thus also have that j\u2208J n j a j s j = j\u2208J m j a j s j .\n\nSince this holds for all j \u2208 J, we have n J \u00b7 A = m J \u00b7 A and by the invertibility of A, n J = m J . In particular, since j 0 \u2208 J, n j 0 = m j 0 contradicting our assumption. As a consequence, n j = m j for all j \u2208 [e] and thus we have our desired b\u0133ection. It remains to verify that we only need a finite number of F-pattern trees to conclude that n j = m j for all j \u2208 [e]. In fact, the above proof indicates that we just need to check test for each root label a, we need to check identities for the finite number of pattern trees used to define the matrix A. Proof of equivalence 2 This equivalence is shown just like proof of Theorem 4.4. in Grohe (2020a) with the additional techniques from Lemma 4.2 in Grohe (2020b).\n\n=\u21d2 We first show that G \u2261 (d)\n\nF -WL H implies hom(T, G) = hom(T, H) for unrooted F-pattern trees T of depth at most d.\n\nAssume that V X \u2229 V Y = \u2205 for X, Y \u2208 {G, H}. For x \u2208 V X and y \u2208 V Y , define x \u223c d y if and only if hom(T r , X x ) = hom(T r , Y y ) for all F-pattern trees T r of depth at most d. Let R 1 , . . . , R e be the \u223c d -equivalence classes and for each j \u2208 [e], let p j := |R j \u2229 V G | and q j :\n= |R j \u2229 V H |. Suppose that G \u2261 (d)\nF -WL H. This implies that p j = q j for every j \u2208 [e]. Let T be an unrooted F-pattern tree of depth at most d, let r be any vertex on the backbone of T , and let T r be the rooted F-pattern tree obtained from T by declaring r as its root. By definition, for X \u2208 {G, H}, any x \u2208 V X \u2229 R j , hom(T r , X x ) are all the same number, only dependent on j \u2208 [e]. Hence,\nhom(T, G) = v\u2208V (G) hom(T r , G v ) = j\u2208[e] p j hom(T r , G v j ) = j\u2208[e] q j hom(T r , H w j ) = w\u2208V (H) hom(T r , H w ) = hom(T, H),\nwhere v j and w j are arbitrary vertices in R j \u2229 V G and R j \u2229 V H , respectively, and where we used that hom(T r , G v j ) = hom(T r , H w j ) and p j = q j . Since this holds for any unrooted F-pattern tree T of depth at most d, we have show the desired implication.\n\n\u21d0= We next check the other direction. That is, we assume that hom(T, G) = hom(T, H) holds for any unrooted F-pattern tree T of depth at most d and verify that G \u2261 (d)\n\nF -WL H. For x \u223c d y to hold for x \u2208 V X , y \u2208 V Y and X, Y \u2208 {G, H}, we earlier showed that this corresponds to checking whether hom(T r i i , X x ) = hom(T r i i , Y y ) for a finite number K rooted F-pattern trees T r i i . By definition of the R j 's, a ij := hom(T r i i , X x ) for x \u2208 R j is well-defined (independent of the choice of X \u2208 {G, H} x \u2208 V X ). For the rooted T r i i 's we denote by T i its unrooted version. Similarly as before,\nhom(T i , G) = j\u2208[e] a ij p j = j\u2208[e]\na ij q j = hom(T i , H).\n\nWe next show that p j = q j for j \u2208 [e]. In fact, this is shown in precisely the same way as in our previous characterisation and based on Lemma 4.2 in Grohe (2020b). That is, we again consider trees obtained by joining copies of the T i 's, to obtain, for compatible s \u2208 N K ,\nj\u2208[e] a s i ij p j = j\u2208[e] a s i ij q j .\nIt now suffices to repeat the same argument as before (details omitted).\n\n\nB Proofs of Section 4 B.1 Additional details of standard concepts\n\nCore and treewidth. A graph G is a core if all homomorphisms from G to itself are injective. The treewidth of a graph G = (V, E, \u03c7) is a measure of how much G resembles a tree. This is defined in terms of the tree decompositions of G, which are pairs (T, \u03bb), for a tree T = (V T , E T ) and \u03bb a mapping that associates each vertex t of V T with a set \u03bb(t) \u2286 V , satisfying the following:\n\u2022 The union of \u03bb(t), for t \u2208 V T , is equal to V ; \u2022 The set {t \u2208 V T | v \u2208 \u03bb(t)} is connected, for all v \u2208 V ; and \u2022 For each {u, v} \u2208 E there is t \u2208 V T with {u, v} \u2208 \u03bb(t).\nThe width of (T, \u03bb) is min t\u2208T (|\u03bb(t)|) \u2212 1. The treewidth of G is the minimum width of its tree decompositions. For instance, trees have treewidth one, cycles have clique two, and the k-clique K k has treewidth k \u2212 1 (for k > 1). If P r is a pattern, then its treewidth is defined as the treewidth of the graph P . Similarly, P r is a core if P is.\n\n\nk-WL. A partial isomorphism from a graph G to a graph H is a set\n\u03c0 \u2286 V G \u00d7 V H such that all (v, w), (v , w ) \u2208 \u03c0 satisfy the equivalences v = v \u21d4 w = w , {v, v } \u2208 E G \u21d4 {w, w } \u2208 E H , \u03c7 G (v) = \u03c7 H (w) and \u03c7 G (v ) = \u03c7 H (w )\n. We may view \u03c0 as a b\u0133ective mapping from a subset X \u2286 V G to a subset of Y \u2286 V H that is an isomorphism from the induced subgraph G[X] to the induced subgraph H[Y ]. The isomorphism type isotp(G,v) of a k-tuplev = (v 1 , . . . , v k ) is a label in some alphabet \u03a3 such that isotp(G,v) = isotp(H,w) if and only if \u03c0 = {(v 1 , w 1 ), . . . , (v k , w k )} is a partial isomorphism from G to H.\n\nLet k \u2265 1 and G = (V, E, \u03c7). The k-dimensional Weisfeiler-Leman algorithm (k-WL) computes a sequence of labellings \u03c7\n(d) k,G from V k \u2192 \u03a3. We denote by \u03c7 (d)\nk,G,v the label assigned to the k-tuplev \u2208 V k in round d. The initial labelling \u03c7 (0) k,G assigns to each k-tuplev is isomorphism type isotp(G,v). Then, for round d,\n\u03c7 (d) k,G,v := \u03c7 (d\u22121) k,G,v , M (d\u22121) v , where M (d\u22121) v is the multiset isotp(v 1 , . . . , v k , w), \u03c7 (d\u22121) k,G,(v 1 ,...,v k\u22121 ,w) , \u03c7 (d\u22121) k,G,(v 1 ,...,v k\u22122 ,w,v k ) , . . . , \u03c7 (d\u22121) k,G,(w,v 2 ,...,v k ) w \u2208 V .\nAs observed in Dell et al. (2018), if k \u2265 2 holds, then we can omit the entry isotp(v 1 , . . . , v k , w) from the tuples in Mv, because all the information it contains is also contained in the entries \u03c7 (d\u22121) k,G,... of these tuples. Also, WL = 1-WL in the sense that \u03c7 (d)\nG,v = \u03c7 (d) G,v if and only if \u03c7 (d) 1,G,v = \u03c7 (d) 1,G,v for all v, v \u2208 V .\nThe k-WL algorithm is run until the labelings stabilises, i.e., if for allv,w \u2208 V k , \u03c7\n(d) k,G,v = \u03c7 (d) k,G,w if and only if \u03c7 (d+1) k,G,v = \u03c7 (d+1)\nk,G,w . We say that k-WL distinguishes two graphs G and H if the multisets of labels for all k-tuples of vertices in G and H, respectively, coincides. Similar notions as are place for distinguishing k-tuples, and for distinguishing graphs (or vertices) based on labels computed by a given number of rounds.\n\nWe remark that k-WL algorithm given here is sometimes referred to as the \"folklore\" version of the k-dimensional Weisfeiler-Leman algorithm. It is known that indistinguishability of graphs by k-WL is equivalent to indistinguishability by sentences in the the k + 1-variable fragment of first order logic with counting (Cai et al., 1992), and to hom(P, G) = hom(P, H) for every graph of treewidth k (Dvorak, 2010; Dell et al., 2018).\n\n\nB.2 Proof of Proposition 2\n\nWe show that for each finite set F of patterns, the expressive power of F-WL is bounded by k-WL, where k is the largest treewidth of a pattern in F.\n\nWe first recall the following characterisation of k-WL-equivalence (Dvorak, 2010;Dell et al., 2018). For any two graphs G and H, G \u2261 k-WL H \u21d0\u21d2 hom(P, G) = hom(P, H)\n\nfor every graph P of treewidth at most k. On the other hand, we know from Theorem 1 that G \u2261 F -WL H \u21d0\u21d2 hom(T, G) = hom(T, H) for every F-pattern tree T . Hence, we may conclude that\nG \u2261 k-WL H =\u21d2 G \u2261 F -WL H\nif we can show that any F-pattern tree has treewidth at most k.\n\nSuppose that k is the maximal treewidth of a pattern in F. To conclude the proof, we verify that the treewidth of any F-pattern tree is bounded by k.\n\nLemma 1. If k is the maximal treewidth of a pattern in F, then the treewidth of any F-pattern tree T is bounded by k.\n\nProof. The proof is by induction on the number of patterns joined at any leaf of T . Clearly, if no patterns are joined, then T is simply a tree and its treewidth is 1. Otherwise, consider a F-pattern tree T = (V, E, \u03c7) whose treewidth is at most k, and a pattern P r of treewidth k that is to be joined at vertex t of T . By the induction hypothesis, there is a decomposition (H, \u03bb) for T witnessing its bounded treewidth, that is,\n1. The union of all \u03bb(h), for h \u2208 V H , is equal to V ; 2. The set {h \u2208 V H | t \u2208 \u03bb(h)} is connected, for all t \u2208 V ; 3. For each {u, v} \u2208 E there is h \u2208 V H with {u, v} \u2208 \u03bb(h)\n; and 4. The size of each set \u03bb(h) is at most k + 1.\n\nLikewise, by assumption, for pattern P r we have such a tree decomposition, say (H P , \u03bb P ). Now consider any vertex h of the decomposition of T such that \u03bb(h) contains vertex t in T to which P r is to be joined at its root. We can create a joint tree decomposition for the join of P r and T (at node t) by merging H and H P with an edge from vertex h in H to the root of H P (recall H P is a tree by definition). It is readily verified that this decomposition maintains all necessary properties. Indeed, condition 1 is clearly satisfied since \u03bb and \u03bb p combined cover all vertices of the join of T with P r . Furthermore, since the only node shared by T and P r is the join node, and we merge H and H P by putting an edge from node h in H to the root of H P , connectivity of is guaranteed and condition 2 is satisfied. Moreover, since the operation of joining T and P r does not create any extra edges, condition 2 is immediately verified, and so is 3, because we do not create any new vertices, neither in H nor in H P , and we already know that \u03bb and \u03bb P are bounded by k + 1.\n\n\nB.3 Proof of Theorem 2\n\nWe show that if F contains a pattern P r which is a core and has treewidth k, then F-WL is not bounded by (k \u22121)-WL. In other words, we construct two graphs G and H that can be distinguished by F-WL but not by (k\u22121)-WL. It suffices to find such graphs that can be distinguished by {P r }-WL but not by (k \u2212 1)-WL. The proof relies on the characterisation of (k \u2212 1)-WL indistinguishability in terms of the k-variable fragment C k of first logic with counting and of k-pebble b\u0133ective games in particular (Cai et al., 1992;Hella, 1996). More precisely, G \u2261 (k\u22121)-WL H if and only if no sentence in C k can distinguish G from H. In other words, for any sentence \u03d5 in C k , G |= \u03d5 if and only if H |= \u03d5. We denote indistinguishability by C k by G \u2261 C k H. We heavily rely on the constructions used in Atserias et al. (2007) and Bova & Chen (2019). In fact, we show that the graphs G and H constructed in those works, suffice for our purpose, by extending their strategy for the k-pebble game to k-pebble b\u0133ective games.\n\nConstruction of the graphs G and H. Let P r be a pattern in F which is a core and has treewidth k. For a vertex v \u2208 V P , we gather all its edges in\nE v := {v, v } | {v, v } \u2208 E P . Let v 1 be one of the vertices in V P . For G, as vertex set V G we take vertices of the form (v, f ) with v \u2208 V P and f : E v \u2192 {0, 1}. We require that e\u2208Ev 1 f (e) mod 2 = 1 and e\u2208Ev f (e) mod 2 = 0 for v = v 1 , v \u2208 V P .\nFor H, as vertex set V H we take vertices of the form (v, f ) with v \u2208 V P and f :\nE v \u2192 {0, 1}.\nWe require that e\u2208Ev f (e) mod 2 = 0, for all v \u2208 V P . We observe that G and H have the same number of vertices.\n\nThe edge sets E G and E H of G and H, respectively, are defined as follows:\n\n(v, f ) and (v , f ) are adjacent if and only if v = v and furthermore,\nf ({v, v } = f ({v, v }).\nIt is known that hom(P, G) = 0 (here it is used that P is a core), hom(P, H) = 0 and G and H are indistinguishably by means of sentences in the k-variable fragment FO k of first order logic (Atserias et al., 2007;Bova & Chen, 2019). To show our theorem, we thus need to verify that G \u2261 C k H as well. Indeed, for if this holds, then G \u2261 (k\u22121)-WL H yet G \u2261 Showing C k -indistinguishability of G and H. We next show that the graphs G and H are indistinguishable by sentences in C k . This will be shown by verifying that the Duplicator has a winning strategy for the k-pebble b\u0133ective game on G and H (Hella, 1996).\n\nThe k-pebble b\u0133ective game. We recall that the k-pebble b\u0133ective game is played between two players, the Spoiler and the Duplicator, each placing at most k pebbles on the vertices of G and H, respectively. The game is played in a number of rounds. The pebbles placed after round r are typically represented by a partial function p (r) : {1, . . . , k} \u2192 V G \u00d7 V H . When p (r) (i) is defined, say, p (r) (i) = (v, w), this means that the Spoiler places the ith pebble on vertex v and the Duplicator places the ith pebble on w. Initially, no pebbles are placed on G and H and hence p (0) is undefined everywhere.\n\nThen in round r > 0, the game proceeds as follows:\n\n1. The Spoiler selects a pebble i in [k]. All other already placed pebbles are kept on the same vertices. We define p (r) (j) = p (r\u22121) (j) for all j \u2208 [k], j = i.\n\n\nThe Duplicator responds by choosing a b\u0133ection\nh : V G \u2192 V H .\nThis b\u0133ection should be consistent with the pebbles in the restriction of p (r\u22121) to [k] \\ {i}. That is, for every j \u2208 [k], j = i, if p (r\u22121) (j) = (v, w) then w = h(v). 3. Next, the Spoiler selects an element v \u2208 V G . 4. The Duplicator defines p (r) (i) = (v, h(v)). Hence, after this round, the ith pebble is placed on v by the Spoiler and on h(v) by the Duplicator.\n\nLet dom(p (r) ) be the elements in [k] for which p (r) is defined. For i \u2208 dom(p (r) ) denote by (v i , w i ) \u2208 V G \u00d7 V H the pair of vertices on which the ith pebble is placed. The Duplicator wins round r if the mapping v i \u2192 w i is partial isomorphism between G and H. More precisely, it should hold that for all edges {v i , v j } \u2208 E G if and only if (w i , w j ) \u2208 E H . In this case, the game continues to the next round. Infinite games are won by the Duplicator. A winning strategy consists of defining a b\u0133ection in step 2 in each round, allowing the game to continue, irregardless of which vertex v the Spoiler places a pebble in Step 3. Winning strategy. We will now provide a winning strategy for the k-b\u0133ective game on our constructed graphs G and H. We recall that V G and V H have the same number of vertices, so a b\u0133ection between V G and V H exists. We show how the Duplicator can select a \"good\" b\u0133ection in\n\nStep 2 of the game, by induction on the number of rounds.\n\nTo state our induction hypothesis, we first recall some notions and properties from Atserias et al. (2007) and Bova & Chen (2019).\n\nLet W be a walk in P and let e be an edge in E P . Then, occ W (e) denotes the number of occurrences of the edge e in the walk. More precisely, if W = (a 1 , . . . , a ) is a walk in P of length where M is an arbitrary bramble of P of order > k. A bramble M is a set of connected subsets of V P such that for any two elements M 1 and M 2 in M, either M 1 \u2229 M 2 = \u2205, or there exists a vertex a \u2208 M 1 and b \u2208 M 2 such that {a, b} \u2208 E P . The order of a bramble is the minimum size of a hitting set for M. It is known that P has treewidth \u2265 k if and only if it has a bramble of order > k. In what follows, we let M be any such bramble. (2019)). For any\n\n\nLemma 2 (Lemma 14 in Bova & Chen\n1 \u2264 \u2264 k, let (a 1 , f 1 ), . . . , (a , f ) be vertices in V G . Let W be a walk in P from v 1 to avoid({a 1 , . . . , a }). For all i \u2208 [ ], let f i : E a i \u2192 {0, 1} be defined by f i (e) = f i (e) + occ W (e) mod 2\nfor all e \u2208 E a i . Then, the mapping (a i , f i ) \u2192 (a i , f i ), for all i \u2208 [ ], is a partial isomorphism from G to H.\n\nWe use this lemma to show that the b\u0133ection (to be defined shortly) selected by the Duplicator induces a partial isomorphism between G and H on the pebbled vertices.\n\nWe can now state our induction hypothesis: In each round r, there exists a b\u0133ection h :\nV G \u2192 V H which is (a) consistent with the pebbles in the restriction of p (r\u22121) to [k] \\ {i} (Recall, Pebble i is selected by the Spoiler in Step 1.) (b) If p (r) (j) = (a j , f j , h(a j , f j )) for j \u2208 dom(p (r) ), then there exists a walk W (r) in P , from v to avoid({a j | j \u2208 dom(p (r) )}, such that h(a j , f j ) = (a j , f j ),\nwhere f j (e) = f j (e) + occ W (r) (e) mod 2 for every e \u2208 E a j . In other words, on the vertices in V G pebbled by p (r) , the b\u0133ection h is, by the previous Lemma, a partial isomorphism from G to H.\n\nIf this holds, then the strategy for the Duplicator is selecting that b\u0133ection h in each round. Verification of the induction hypothesis. We assume that the special vertex v 1 in P has at least two neighbours. Such a vertex exists since otherwise P consists of a single edge while we assume P to be of treewidth at least two.\n\nBase case. For the base case (r = 0) we define two walks:\nW 1 = v 1 , v 2 and W 2 = v 1 , t with v 2 = t and v 2 , t are neighhbours of v 1 . We define h(a i , f ) = (a i , f ) with f (e) = f (e) + occ W 1 (e) mod 2 if a i = t, and h(t, f ) = (t, f ) with f (e) = f (e) + occ W 2 (e) mod 2.\nThe mapping h is a b\u0133ection from V G to V H . We note that it suffices to show that h is injective since V G and V H contain the same number of vertices. Since h(a i , f i ) = h(a j , f j ) whenever a i = a j , we can focus on comparing h(a i , f ) and h(a i , g) with f = g. This implies that f (e) = g(e) for at least one edge e \u2208 N a i . Clearly, this implies that f (e) = f (e) + occ W (e) mod 2 = g (e) = g(e) + occ W (e) mod 2. In fact this, holds for any walk W and thus in particular for W 1 and W 2 . We further observe that h is consistent simply because no pebbles have been placed yet. For the same reason we can take the walk W (0) to be either W 1 or W 2 .\n\nInductive case. Assume that the induction hypothesis holds for round r and consider round r +1. Let p (r) = (a j , f j , a j , f j ) for j \u2208 dom(p (r) ). By induction, there exists a b\u0133ection h : V G \u2192 V H such that h(a j , f j ) = (a j , f j ) and furthermore, f j (e) = f j (e) + occ W (r) (e) mod 2 for every e \u2208 N a j , for some walk W (r) from v 1 to t \u2208 avoid({a j | j \u2208 dom(p (r) )}).\n\nAssume that the Spoiler selects i \u2208 [k] in Step 1 in round r + 1. We define the Duplicator's b\u0133ection h : V G \u2192 V H for round r + 1, as follows. Recall that t \u2208 V P is the vertex in which the walk W (r) ends.\n\n\u2022 For all (a, f ) \u2208 V G such that a = t, we define h(a, f ) = (a, f ) where for each e \u2208 E a :\n\nf (e) = f (e) + occ W (r) (e) mod 2.\n\n\u2022 For all (t, f ) \u2208 V G , we will extend W (r) with a walk W so that it ends in a vertex t different from t. Suppose that M \u2208 M such that t \u2208 M . We want to find an M \u2208 M such that M \u2229 ({a j | j \u2208 dom(p (r) ), j = i} \u222a {t}) = \u2205. We can then take t to be a vertex in M and since M and M are both connected, and either have a vertex in common or an edge between them, we can let W be a walk from t to t entirely in M and M . Now, such an M exists since otherwise {a j | j \u2208 dom(p (r) ), j = i} \u222a {t} would be a hitting set for M of size at most k. We know, however, that any hitting set M must be of size k + 1 because of the treewidth k assumption for P . We now define the b\u0133ection as h(t, f ) = (t, f ) where for each e \u2208 E t :\n\nf (e) = f (e) + occ W (r) ,W (e) mod 2.\n\nThis concludes the definition of h : V G \u2192 V H . We need to verify a couple of things: (i) h is b\u0133ection; (ii) h is consistent with all pebbles in p (r) except for the \"unpebbled\" one p (r) (i); and (iii) it induces a partial isomorphism on pebbled vertices.\n\n(i) h is a b\u0133ection. Since V G and V H are of the same size, it suffices to show that h is an injection.\n\nClearly, h(a 1 , f 1 ) = h(a 2 , f 2 ) whenever a 1 = a 2 . We can thus focus on h(a, f 1 ) and h(a, f 2 ) with f 1 = f 2 . Then, f 1 and f 2 differ in at least one edge e \u2208 E a and for this edge:\n\nf 1 (e) = f 1 (e) + occ W (e) mod 2 = f 2 (e) + occ W (e) mod 2 = f 2 (e).\n\nfor any walk W . In particular, this holds for both walks used in the definition of h: W (r) , used when a = t, and W (r) , W used when a = t. Hence, h is indeed a b\u0133ection.\n\n(ii) h is consistent. For each j \u2208 dom(p (r+1) ) with j = i, let p (r+1) = (a j , f j , a j , f j ). Now, by induction, W (r) ended in a vertex t distinct from any of these a j 's and thus none of these a j 's are equal to t. This implies that h(a j , f j ) = (a j , f j ) with f j (e) = f j (e) + occ W (r) (e) mod 2. But this is precisely how p (r) placed its pebbles, by induction. Hence, f j (e) = f j (e) and thus h is consistent.\n\n(iii) p (r+1) induces a partial isomorphism. After the Spoiler picked an element (a i , f i ) \u2208 V G , we now know that p (r+1) (j) = (a j , f j , h(a j , f j )) for all j \u2208 dom(p (r+1) ). We recall that h is defined in two possible ways, using two distinct walks: W (r) , for vertices in V G not involving t, or, otherwise using the walk W (r) , W , for vertices in V G involving t.\n\nHence, when all a j 's for p (r+1) are distinct from t, then h(a j , f j ) = (a j , f j ) with f j (e) = f j (e) + occ W (r) (e) mod 2 and we can simply take the new walk W (r+1) to be W (r) . Then, Lemma 2 implies that the mapping (a j , f j ) \u2192 h(a j , f j ), for j \u2208 dom(p (r+1) ) is a partial isomorphism from G to H, as desired.\n\nOtherwise, we know that a j = t for j = i but a i = t. That is, the Spoiler places the ith pebble on a vertex of the form (t, f ) in V G . We now have that h is defined in two ways for the pebbled elements using the two distinct walks. We next show that W (r) , W can be used for both types of pebbled elements in p (r+1) , those of the form (a j , f ) with a j = t and (t, f ). For the last type this is obvious since we defined h(t, f ) in terms of W (r) , W . For the former type, we note that a j \u2208 M and a j \u2208 M for j = i. If we take an edge e \u2208 N a j , then occ W r ,W (e) = occ W (r) (e) because W lies entirely in M and M . As a consequence, for (a j , f j ) with j = i, for all e \u2208 N j : f j (e) = f j (e) + occ W (r) (e) mod 2 = f j (e) + occ W (r) ,W (e) mod 2.\n\nThen, Lemma 2 implies that the mapping (a j , f j ) \u2192 h(a j , f j ), for j \u2208 dom(p (r+1) ) is a partial isomorphism from G to H, because we can use the same walk W (r),W for all pebbled vertices.\n\n\nB.4 Proof of Proposition 3\n\nWe show that no finite set F of patterns suffices for F-WL to be equivalent to k-WL, for k > 1, in terms of expressive power. The proof is by contradiction. That is, suppose that there exists a set F such that G \u2261 F -WL H \u21d4 G \u2261 k-WL H for any two graphs G and H. In particular, G \u2261 F -WL H \u21d2 G \u2261 k-WL H and thus also G \u2261 F -WL H \u21d2 G \u2261 2-WL H, since the 2-WL-test is upper bounded by any k-WL-test for k > 2. We argue that no finite set F exists satisfying\nG \u2261 F -WL H \u21d2 G \u2261 2-WL H.\nLet m denote the maximum number of vertices of any pattern in F. Furthermore, consider graphs G and H, where G is the disjoint union of m + 2 copies of the cycle C m+1 , and H is the union of m + 1 copies of the cycle C m+2 . Note that G and H have the same number of vertices.\n\nWe observe that any homomorphism from a pattern P r in F to G v or H w , for vertices v \u2208 V G and w \u2208 V H , maps P r to either a copy of C m+1 (for G) or a copy of C m+2 (for H). Furthermore, any such homomorphism maps P r in a subgraph of C m+1 or C m+2 , consisting of at most m vertices. There is, however, a unique (up to isomorphism) subgraph of m vertices in C m+1 and C m+2 . Indeed, such subgraphs will be a path of length m. This implies that hom(P r , G v ) = hom(P r , H w ) for any v \u2208 V G and w \u2208 V H . Since the argument holds for any pattern P r in F, all vertices in G and H will have the same homomorphism count for patterns in F. Furthermore, since both G and H are regular graphs (each vertex has degree two), this implies that F-WL cannot distinguish between G and H. This is formalised in the following lemma. We recall that a t-regular graph is a graph in which every vertex has degree t.\n\nLemma 3. For any set F of patterns and any two t-regular (unlabelled) graphs G and H such that hom(P r , X x ) = hom(P r , Y y ) for P r \u2208 F, X, Y \u2208 {G, H}, x \u2208 V X and y \u2208 V Y holds,\nG \u2261 F -WL H.\nProof. The lemma is readily verified by induction on the number d of rounds of F-WL. We show a stronger result in that \u03c7\n(d) F ,X,x = \u03c7 (d)\nF ,Y,y for any d, X, Y \u2208 {G, H}, x \u2208 V X and y \u2208 V Y , from which G \u2261 F -WL H follows. By our Theorem 1, it suffices to show that hom(T r , X x ) = hom(T r , Y y ) for F-pattern trees of depth at most d. Let F = {P r 1 , . . . , P r }. For the base case, let T r be a join Strictly speaking, we can use the diameter of any pattern in F instead, but it is easier to convey the proof simply by taking number of vertices. pattern F s for some s = (s 1 , . . . , s ) \u2208 N . Then,\nhom(T r , X x ) = i=1 (hom(P r i , X x )) s i = i=1 (hom(P r i , Y y )) s i = hom(T r , Y y ),\nsince hom(P r i , X x ) = hom(P r i , Y y ) for any P r i \u2208 F. Then, for the inductive case, assume that hom(S r , X x ) = hom(S r , Y y ) for any F-pattern tree S r of depth at most d \u2212 1, X, Y \u2208 {G, H}, x \u2208 V X and y \u2208 V Y , and consider an F-pattern T r of depth d. Let S c 1 1 , . . . , S cp p be the F-pattern trees of depth at most d \u2212 1 rooted at the children c 1 , . . . , c p of r in the backbone of T r . As before, let F s the pattern joined at r in T r . Then,\nhom(T r , X x ) = hom(F s , X x ) p i=1 x \u2208N X (x) hom(S c i i , X x ) = hom(F s , X x ) p i=1 t \u00b7 hom(S c i i , Xx) = hom(F s , Y y ) p i=1 t \u00b7 hom(S c i i , Y\u1ef9) = hom(F s , Y y ) p i=1 y \u2208N Y (y) hom(S c i i , Y y ) = hom(T r , Y y ),\nwhere we used that N X (x) and N Y (y) both consists of t vertices (regularity), by the induction hypothesis all vertex have the same homomorphism counts for F-patterns trees of depth at most d \u2212 1, and wherex and\u1ef9 are taken to be arbitrary vertices in N X (x) and N Y (y), respectively.\n\nHence, since G and H are 2-regular and satisfy the conditions of the lemma, we may indeed infer that G \u2261 F -WL H. We note, however, that G \u2261 2-WL H. Indeed, from Dvorak (2010) and Dell et al. (2018) we know that G \u2261 2-WL H implies that hom(P, G) = hom(P, H) for any graph P of treewidth at most two. In particular, G \u2261 2-WL H implies that hom(C , G) = hom(C , H) for all cycles C . We now conclude by observing that hom(C m+1 , G) = hom(C m+1 , H) by construction. We have thus found two graphs with cannot be distinguished by F-WL, but that can be distinguished by 2-WL, contradicting our assumption that G \u2261 F -WL H \u21d2 G \u2261 2-WL H.\n\n\nC.2 Proof of Proposition 5\n\nLet P r = P r 1 P r 2 be a pattern that is the join of two smaller patterns. We show that for any any set F of patterns, we have that F \u222a{P r }-WL is upper bounded by F \u222a{P r 1 , P r 2 }-WL. That is, for every two graphs G and H, G \u2261 F \u222a{P r\n1 ,P r 2 }-WL H implies G \u2261 F \u222a{P r }-WL H. By definition, G \u2261 F \u222a{P r 1 ,P r 2 }-WL H is equivalent to { {\u03c7 (d) F \u222a{P r 1 ,P r 2 },G,v | v \u2208 V G } } = { {\u03c7 (d) F \u222a{P r 1 ,P r 2 },H,w | w \u2208 V H } }.\nIn other words, with every v \u2208 V G we can associate a unique w \u2208 V H such that \u03c7\n(d) F \u222a{P r 1 ,P r 2 },G,v = \u03c7 (d) F \u222a{P r 1 ,P r 2 },H,w .\nWe show, by induction on d, that this implies that \u03c7 (d) H,w , which implies that hom(P r 1 , G v ) = hom(P r 1 , H w ) and hom(P r 2 , G v ) = hom(P r 2 , H w ) and hom(Q r , G v ) = hom(Q r , H w ) for every Q r \u2208 F. As a consequence, from properties of the graph join operators, since P r = P r 1 P r 2 :\nF \u222a{P r },G,v = \u03c7 (d) F \u222a{P r },H,w . This suffices to conclude that { {\u03c7 (d) F \u222a{P r },G,v | v \u2208 V G } } = { {\u03c7 (d) F \u222a{P r },H,w | w \u2208 V H } } and thus G \u2261 F \u222a{P t }-WL H. Base case. We show that { {\u03c7 (d) F \u222a{P r 1 ,P r 2 },G,v | v \u2208 V G } } = { {\u03c7 (d) F \u222a{P r 1 ,P r 2 },H,w | w \u2208 V H } } implies that with every v \u2208 V G we can associate a unique w \u2208 V H satisfying \u03c7 (0) F \u222a{P r },G,v = \u03c7 (0) F \u222a{P r },H,w . Indeed, as already observed, { {\u03c7 (d) F \u222a{P r 1 ,P r 2 },G,v | v \u2208 V G } } = { {\u03c7 (d) F \u222a{P r 1 ,P r 2 },H,w | w \u2208 V H } } implies that with every v \u2208 V G we can associate a unique w \u2208 V H such that \u03c7 (d) F \u222a{P r 1 ,P r 2 },G,v = \u03c7 (d) F \u222a{P r 1 ,P r 2 },H,w . This in turn implies that \u03c7 (0) F \u222a{P r 1 ,P r 2 },G,v = \u03c7 (0) F \u222a{P r 1 ,P r 2 },hom(P r , G v ) = hom(P r 1 , G v ) \u00b7 hom(P r 2 , G v ) = hom(P r 1 , H w ) \u00b7 hom(P r 2 , H w ) = hom(P r , H w ),\nand thus also \u03c7\n(0) F \u222a{P r },G,v = \u03c7 (0) F \u222a{P r },H,w .\nInductive case. We assume that { {\u03c7\n(d) F \u222a{P r 1 ,P r 2 },G,v | v \u2208 V G } } = { {\u03c7 (d) F \u222a{P r 1 ,P r 2 },H,w | w \u2208 V H } } implies \u03c7 (e) F \u222a{P r },G,v = \u03c7 (e)\nF \u222a{P r },H,w , and want to show that it also implies \u03c7 (e+1)\nF \u222a{P r },G,v = \u03c7 (e+1)\nF \u222a{P r },H,w . We again use the fact that we can associate with every v \u2208 V G a unique vertex w \u2208 V H such that \u03c7 \n(d) F \u222a{P r 1 ,P r 2 },G,v = \u03c7 (d) F},G,v | v \u2208 N G (v)} and {\u03c7\n(e) F \u222a{P r 1 ,P r 2 },H,w | v \u2208 N H (w)} must be equal as well, i.e., we can find a one-to-one corresponence between neighbors of v in G and neighbors of w in H that have the same label. From the induction hypothesis we then\nhave that \u03c7 (e) F \u222a{P r },G,v = \u03c7 (e)\nF \u222a{P r },H,w and also that the multisets {\u03c7\n(e) F \u222a{P r },G,v | v \u2208 N G (v)} and {\u03c7 (e) F \u222a{P r },H,w | v \u2208 N H (w)} are equal, which implies, by the definition of the WL-test, that \u03c7 (e+1) F \u222a{P r },G,v = \u03c7 (e+1)\nF \u222a{P r },H,w , as was to be shown.\n\n\nC.3 Proof of Theorem 3\n\nWe show that F \u222a {Q r }-WL, where Q r is pattern whose core has treewidth k, is more expressive than F-WL if every pattern P r \u2208 F satisfies one of the following conditions: (i) P r has treewidth < k; or (ii) P r does not map homomorphically to Q r .\n\nLet c(Q) r to denote the (rooted) core of Q, in which the root of c(Q) r is any vertex which is the image of the root of Q r in a homomorphism from Q r to c(Q) r . By assumption, c(Q) r has treewidth k.\n\nClearly, F-WL is upper bounded by F \u222a {Q r }-WL. Thus, all we need for the proof is to find two graphs that are indistinguishable by F-WL but are in fact distinguished by F \u222a {Q r }-WL.\n\nThose two graphs are, in fact, the graphs G and H constructed for c(Q) r (of treewidth k) in the proof of Theorem 2. From that proof, we know that: We note that (a) immediately implies that G and H can be distinguished by F \u222a {Q r }-WL. In fact, they are distinguished in already by the initial labelling in round 0. We next show that G and H are indistinguishable by F-WL.\n\nLet us first present a small structural result that helps us deal with patterns in F satisfying the second condition of the Theorem.\n\nLemma 4. If a rooted pattern P r does not map homomorphically to Q r , then hom(P, G) = hom(P, H) = 0\n\nProof. We use the following property of graphs G and H, which can be directly observed from their construction (and was already noted in Atserias et al. (2007) and Bova & Chen (2019)). Define G r and H r by setting as their root any vertex (a r , f ), for a r the root of c(Q) r . Then there is a homomorphism from G r to c(Q) r , and there is a homomorphism from H r to c(Q) r . Now, any homomorphism h from P r to G can be extended to a homomorphism from P r to Q r : we compose h with the homomorphism mentioned above from G to c(Q) r , which by definition again maps homomorphically to Q r . Since by definition we have that P r does not map to Q r , h cannot exist. The proof for H is analogous. Now, let F be the set of patterns obtained by removing from F all patterns which do not map homomorphically to Q r . By Lemma 4, we have that G and H are distinguished by the F-WL-test if and only if they are distinguished by F -WL.\n\nBut all patterns in F must have treewidth less than k, and by (b) G and H are indistinguishable by k-WL. Proposition 2 then implies that G and H are indistinguishable by F-WL, as desired.\n\n\nD Connections to related work\n\nWe here provide more details of how F-MPNNs connect to MPNNs from the literature which also augment the initial labelling.\n\nVertex degrees. We first consider so-called degree-aware MPNNs (Geerts et al., 2020) in which the message functions of the MPNNs may depend on the vertex degrees. The Graph Convolution Networks (GCNs) (Kipf & Welling, 2017) are an example of such MPNNs. Degree-aware MPNNs are known to be equivalent, in terms of expressive power, to standard MPNNs in which the initial labelling is extended with vertex degrees (Geerts et al., 2020). Translated to our setting, we can simply let F = { } since hom( , G v ) is equal to the degree of vertex v in G. When considering graphs without an initial vertex labelling (or a uniform labelling which assigns every vertex the same label), our characterisation (Theorem 1) implies G \u2261 So, by considering F = { }-MPNNs one gains one round of computation compared to considering standard MPNNs. To lift this to labeled graphs, instead of F = { } one has to include labeled versions of the single edge pattern, in order to count the number of neighbours of a specific label for each vertex. This is done, e.g., by Ishiguro et al. (2020), who use the WL labelling obtained after the first round to augment the initial vertex labelling. This corresponds indeed by adding hom(T r , G v ) as feature for every labeled tree of depth one. This results in that G \u2261 Isomorphism counts. Another, albeit similar, approach to add structural information to the initial labelling is taken in the paper Graph Structure Networks by Bouritsas et al. (2020). The idea there is to extend the initial features with information about how often a vertex v appears in a subgraph of G which is isomorphic to P . More precisely, Bouritsas et al. (2020) consider a connected unlabelled graph P as pattern and partition its vertex set V P orbit-wise. That is,\nV P = o P i=1 V i P\nwhere o P denotes the number of orbits of P . Here, v, v \u2208 V i P whenever there is an automorphism h in Aut(P ) mapping v to v . Next, they consider all distinct subgraphs G 1 , . . . , G k in G which are isomorphic to P , denoted by P \u223c = G j for j \u2208 [k]. We write P \u223c = f G j when P \u223c = G j using a specific isomorphism f . Then for each orbit partition i \u2208 [o P ] and vertex v \u2208 V , they define:\niso(P, G, v, i) = |{G j \u223c = P | v \u2208 V G j , and there exists an f s.t. G j \u223c = f P and f (v) \u2208 V i P , j \u2208 [k]}|.\nThat is, the number of subgraphs G j in G that can be isomorphically mapped to P are counted, provided that this can be done by an isomorphism which maps vertex v in G j (and thus G) to one of the vertices in the ith orbit partition V i P of the pattern. A similar notion is proposed for edges, which we will not consider here. Similar to our extended features, the initial features of each vertex v is then augmented with iso(P, G v , i) | P \u2208 F, i \u2208 [o P ] for some set F of patterns. Standard MPNNs are executed on these augmented initial features. We refer to Bouritsas et al. (2020) for more details.\n\nWe can view the above approach as an instance of our framework. Indeed, given a pattern P in F, for each orbit partition, we replace P by a different rooted version P r i , where r i is a vertex in V i P . Which vertex in the orbit under consideration is selected as root is not important (because they are equivalent by definition of orbit). We then see that the standard notion of subgraph isomorphism counting directly translates to the quantity used in Bouritsas et al. (2020):\nsub(P r i , G v ) := number of subgraphs in G containing v, isomorphic to P r i = iso(P, G, v, i).\nIt thus remains to express sub(P r i , G v ) in terms of homomorphism counts. This, however, follows from Curticapean et al. (2017) in which it is shown that iso(P r i , G v ) can be computed by a linear combination of hom(Q r i , G v ) where Q r i ranges over all graphs on which P r i can be mapped by means of a surjective homomorphism. For a given P r i , the finite set of such patterns is called the spasm of P r i in Curticapean et al. (2017) and can be easily computed.\n\nIn summary, given the set F of patterns in Bouritsas et al. (2020), we first replace every P \u2208 F by its rooted versions P r i , for i \u2208 [o P ], and then expand the resulting set of rooted patterns, by the spasms of each of these patterns. Let us denote by F * the final set of rooted patterns. It now follows that hom(Q r , G v ) for Q r \u2208 F * provides sufficient information to extract sub(P r i , G v ) and thus also iso(P, G, v, i) for every P \u2208 F and orbit part i \u2208 [o P ]. As a consequence, the MPNNs from Bouritsas et al. (2020) are bounded by F * -MPNNs and thus F * -WL. Conversely, given an F-MPNN one can, again using results by Curticapean et al. (2017), define a set F + of patterns, such that the subgraph isomorphism counts of patterns in F + can be used to compute the homomorphism counts of patterns in F. Hence, F-MPNNs are upper bounded by the MPNNs considered in Bouritsas et al. (2020) using patterns in F + . This is in agreement with Curticapean et al. (2017) in which it is shown that homomorphism counts, subgraph isomorphism counts and other notions of pattern counts are all interchangeable. Nevertheless, by using homomorphism counts one can gracefully extend known results about WL and MPNNs, as we have shown in the paper, and add little overhead.\n\n\nE Additional experimental information E.1 Experimental setup\n\nOne of the crucial questions when studying the effect of adding structural information to the initial vertex labels is whether these additional labels enhance the performance of graph neural networks. In order to reduce the effect of specific implementation details of GNNs and choice of hyper-parameters, we start from the GNN implementations and choices made in the benchmark by Dwivedi et al. (2020) . and only change the initial vertex labels, while leaving the GNNs themselves unchanged. This ensures that we only measure the effect of augmenting initial features with homomorphism counts. We will use the GNNs from the benchmark, without extended features, as our baselines. For the same reasons, we use datasets proposed in the benchmark for their ability to statistically separate the performance of GNNs. All other parameters are taken as in Dwivedi et al. (2020) and we refer to that paper for more details.\n\nSelected GNNs Dwivedi et al. (2020) divide the benchmarked GNNs into two classes: the MPNNs and the \"theoretically designed\" WL-GNNs. The first class is found to perform stronger and train faster. Hence, we chose to include the five following MPNN models from the benchmark: For GatedGCN we used the version in which positional encoding (Belkin & Niyogi, 2003) is added to the vertex features, as it is empirically shown to be the strongest performing version of this model by for the selected datasets (Dwivedi et al., 2020). We denote this version by GatedGCN E,P E , referring to the presence of edge features and this positional encoding. Details, background and a mathematical formalisation of the message passing layers of these models can be found in the supplementary material of Dwivedi et al. (2020).\n\nAs explained in the experimental section of the main paper, we enhance the vertex features with the log-normalised counts of the chosen patterns in every vertex of every graph of every dataset. The first layers of some models of (Dwivedi et al., 2020) are adapted to take in this variation in input size. All other layers where left identical to their original implementation as provided by Dwivedi et al. (2020).\n\nHardware, compute and resources All models for ZINC, PATTERN and COLLAB were trained on a GeForce GTX 1080 337 Ti GPU, for CLUSTER a Tesla V100-SXM3-32GB GPU was used. Tables 7, 10, 13 and 16 report the training times for all combination of models and additional feature set. A rough estimate of the CO 2 emissions based on the total computing times of reported\n\nThe original implementations can be found on https://github.com/graphdeeplearning/benchmarking-gnns experiments (2 074 hours GeForce GTX 1080, 372 hours Tesla V100-SXM3-32GB), the computing times of not-included experiments (1 037 hours GeForce GTX 1080, 181 hours Tesla V100-SXM3-32GB), the GPU types (GeForce GTX 1080, Tesla V100-SXM3-32GB) and the geographical location of our cluster results in a carbon emission of 135 kg CO 2 equivalent. This estimation was conducted using the MachineLearning Impact calculator presented in Lacoste et al. (2019).\n\n\nE.2 Graph learning tasks\n\nWe here report the full results of our experimental evaluation for graph regression (Section E.2.1), link prediction (Section E.2.2) and vertex classification (Section E.2.3) as considered in Dwivedi et al. (2020). More precisely, a full listing of the patterns and combinations used and the obtained results for the test sets can be found in Tables 5, 8, 11 and 14. Average training time (in hours) and the number of epochs are reported in Tables 7, 10, 13 and 16. Finally, the total number of model parameters are reported in Tables 6, 9, 12 and 15. All averages and standard deviations are over 4 runs with different random seeds. The main take-aways from these results are included in the main paper. Splitting. ZINC has 10 000 train, 1 000 validation and 1 000 test graphs. Training. For the learning rate strategy, an initial learning rate is set to 5 \u00d7 10 \u22125 , the reduce factor is 0.5, and the stopping learning rate is 1 \u00d7 10 \u22126 , the patience value is 25 and the maximal training time is set to 12 hours. Performance Measure The performance measure is the mean absolute error (MAE) between the predicted and the ground truth constrained solubility for each molecular graph. Number of layers 16 MPNN layers are used for every model.\n\n\nE.2.1 Graph regression with the ZINC dataset\n\n\nE.2.2 Link Prediction with the Collab dataset\n\nAnother set used in Dwivedi et al. (2020) is COLLAB, a link prediction dataset proposed by the Open Graph Benchmark (OGB) (Hu et al., 2020) corresponding to a collaboration network between approximately 235K scientists, indexed by Microsoft Academic Graph. Vertices represent scientists and edges denote collaborations between them. For vertex features, OGB provides 128-dimensional vectors, obtained by averaging the word embeddings of a scientist's papers. The year and number of co-authored papers in a given year are concatenated to form edge features. The graph can also be viewed as a dynamic multi-graph, since two vertices may have multiple temporal edges between if they collaborate over multiple years. The following are taken from Dwivedi et al. (2020):\n\nHere and in the next tasks we are using the parameters used in the code accompanying Dwivedi et al. (2020). In the paper, slightly different parameters are used. Table 5: Full results of the mean absolute error (predicted constrained solubility vs. the ground truth) for selected cycle combinations and GNNs on the ZINC data set. In the last two rows we compare between homomorphism counts (hom) and subgraph isomorphism counts (iso).  Training. All GNNs use the same learning rate strategy: an initial learning rate is set to 1 \u00d7 10 \u22123 , the reduce factor is 0.5, the patience value is 10, and the stopping learning rate is 1 \u00d7 10 \u22125 . Performance Measure. We use the evaluator provided by OGB (Hu et al., 2020), which aims to measure a model's ability to predict future collaboration relationships given past collaborations. Specifically, they rank each true collaboration among a set of 100 000 randomly-sampled negative collaborations, and count the ratio of positive edges that are ranked at K-place or above (Hits@K).\n\nThe value K = 50 as this gives the best value for statistically separating the performance of GNNs. Number of layers 3 MPNN layers are used for every model. \n\n\nE.2.3 Vertex classification with PATTERN and CLUSTER\n\nFinally, also used in Dwivedi et al. (2020) are the PATTERN and CLUSTER graph data sets, generated with the Stochastic Block Model (SBM) (Abbe, 2018), which is widely used to model communities in social networks by modulating the intra-and extra-communities connections, thereby controlling the difficulty of the task. A SBM is a random graph which assigns communities  to each vertex as follows: any two vertices are connected with probability p if they belong to the same community, or they are connected with probability q if they belong to different communities (the value of q acts as the noise level). For the PATTERN dataset, the goal of the vertex classification problem is the detection of a certain pattern P embedded in a larger graph G. The graphs in G consist of 5 communities with sizes randomly selected between [5,35]. The parameters of the SBM for each community is p = 0.5, q = 0.35, and the vertex features in G are generated using a uniform random distribution with a vocabulary of size 3, i.e., {0, 1, 2}. Randomly, 100 patterns P composed of 20 vertices with intra-probability p P = 0.5 and extra-probability q P = 0.5 are generated (i.e., 50% of vertices in P are connected to G). The vertex features for P are also generated randomly using values in {0, 1, 2}. The graphs consist of 44-188 vertices. The output vertex labels have value 1 if the vertex belongs to P and value 0 belongs to G.\n\nFor the CLUSTER dataset, the goal of the vertex classification is the detection of which cluster a vertex belongs. Here, six SBM clusters are generated with sizes randomly selected between [5,35] and probabilities p = 0.55 and q = 0.25. The graphs consist of 40-190 vertices. Each vertex can take an initial feature value in range {0, 1, 2, . . . , 6}. If the value is i then the vertex belongs to class i \u2212 1. If the value is 0, then the class of the vertex is unknown and need to be inferred. There is only one labelled vertex that is randomly assigned to each community and most vertex features are set to 0. The output vertex labels are defined as the community/cluster class labels. The following are taken from Dwivedi et al. (2020): Splitting The PATTERN dataset has 10 000 train, 2 000 validation and 2 000 test graphs. The CLUSTER dataset has 10 000 train, 1 000 validation and 1 000 test graphs. We save the generated splits and use the same sets in all models for fair comparison. Training For all GNNs, an initial learning rate is set to 1 \u00d7 10 \u22123 , the reduce factor is 0.5, the patience value is 10, and the stopping learning rate is 1 \u00d7 10 \u22125 . Performance measure The performance measure is the average vertex-level accuracy weighted with respect to the class sizes. Number of layers 16 MPNN layers are used for every model.      \n\n\ndenote the feature vector computed for vertex v \u2208 V by an MPNN M in round d. The initial feature vector x (0)\n\n\nG,u of all neighbours u of v are combined by the aggregating function C (d) into a single vector, and then this vector is used together with x applying the update function U (d) .\n\n\nF ,G,v := \u03c7 G (v), hom(P r 1 , G v ), . . . , hom(P r , G v ) F ,G,v | u \u2208 N G (v)}} .\n\nSelected\nGNNs. We select the best architectures from Dwivedi et al. (2020): Graph Attention Networks (GAT) (Velickovic et al., 2018), Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), GraphSage (Hamilton et al., 2017), Gaussian Mixture Models (MoNet) (Monti et al., 2017) and GatedGCN (Bresson & Laurent, 2017). We leave out various linear architectures such as GIN (Xu et al., 2019) as they were shown to perform poorly on the benchmark. Learning tasks and datasets. As in Dwivedi et al. (2020) we consider (i) graph regression and the ZINC dataset (Irwin et al., 2012a; Dwivedi et al., 2020); (ii) vertex classification and the PATTERN and CLUSTER datasets (Dwivedi et al., 2020); and (iii) link prediction and the COLLAB dataset (Hu et al., 2020). We omit graph classification: for this task, the graph datasets from Dwivedi et al. (2020) originate from image data and hence vertex neighborhoods carry little information.\n\nF\n-WL (H, w) and thus, in particular, (G, v) \u2261\n\n\n-WL H to hold, hom(P, G) = hom(P, H), which we know not to be true. Hence, G \u2261 {P }-WL H, as desired.\n\n\n, then occ W (e) := |{i \u2208 [ \u2212 1] | e = {a i , a i+1 }}|. Furthermore, for a subset S \u2286 V P , we define avoid(S) := {M \u2208M,M \u2229S=\u2205} M,\n\n\n(a) hom(c(Q), G) = 0 and hom(c(Q), H) = 0; and (b) G \u2261 C k H.\n\n\nand only if hom(T, G) = hom(T, H) for every { }-pattern tree of depth at most d. This in turn is equivalent to hom(T, G) = hom(T, H) for every (standard) tree of depth at most d + 1. Indeed, { }-pattern trees of depth at most d are simply trees of depth d + 1. Combining this with the characterisation of WL by Dvorak (2010) and Dell et al. (2018), we thus have for unlabelled graphs that G \u2261\n\nH\nfor labelled graphs.Walk counts. The Graph Feature Networks byChen et al. (2019a) can be regarded as a generalisation of the previous approach. Instead of simply adding vertex degrees, the number of walks of certain lengths emanating from vertices are added. Translated to our setting, this corresponds to considering {L 2 , L 3 , . . . , L }-MPNNs, where L denotes a rooted path of length . For unlabelled graphs, our characterisation (Theorem 1) implies that G \u2261(d) L 1 ,...,L -WL H is upper bounded by G \u2261 (d+ ) WL H, simply because every {L 2 , L 3 , . . . , L }-pattern tree of depth d is a standard tree of depth at most d + . Cycles. Li et al. (2019) extend MPNNs by varying the notion of neighbourhood over which is aggregated. One particular instance corresponds to an aggregation of features, weighted by the number of cycles of a certain length in each vertex (see discussion at the end of Section 4 in Li et al. (2019)). Translated to our setting, this corresponds to considering {C }-MPNNs where C denotes the cycle of length . As mentioned in the main body of the paper, these extend MPNNs and result in architectures bounded by 2-WL (Proposition 4). This is in line with Theorem 3 from Li et al. (2019) stating that their framework strictly extends MPNNs and thus 1-WL.\n\n\u2022\nGraph Attention Network (GAT) as described in Velickovic et al. (2018) \u2022 Graph Convolutional Network (GCN) as described in Kipf & Welling (2017) \u2022 GraphSage as described in Hamilton et al. (2017) \u2022 Mixed Model Convolutional Networks (MoNet) as described in Monti et al. (2017) \u2022 GatedGCN as described in Bresson & Laurent (2017).\n\n\nJust as in Dwivedi et al. (2020) we use a subset (12K) of ZINC molecular graphs (250K) dataset (Irwin et al., 2012b) to regress a molecular property known as the constrained solubility. For each molecular graph, the vertex features are the types of heavy atoms and the edge features are the types of bonds between them. The following are taken from Dwivedi et al. (2020):\n\n\n3 , . . . , C 10 } (hom) 0,22\u00b10,01 0,20\u00b10,00 0,19\u00b10,00 0,2376\u00b10,01 0,1352\u00b10,01 {C 3 , . . . , C 10 } (iso) 0,24\u00b10,01 0,22\u00b10,01 0,16\u00b10,01 0,2408\u00b10,01 0,1357 \u00b1 0,01\n\n\ncan use the F-WL-test to compare vertices of the same graphs, or different graphs. We say that the F-WL-test cannot distinguish vertices if their final labels are the same, and that the F-WL-test cannot distinguish graphs G and H if the multiset containing each label computed for G is the same as that of H. Similarly as for MPNNs and the WL-test (Xu et al., 2019; Morris et al., 2019), we obtain that the F-WL-test provides an upper bound for the expressive power of F-MPNNs. Proposition 1. If two vertices of a graph cannot be distinguished by the F-WL-test, then they cannot be distinguished by any F-MPNN either. Moreover, if two graphs cannot be distinguished by the F-WL-test, then they cannot be distinguished by any F-MPNN either.\n\nTable 1 :\n1Results for the ZINC dataset show that homomorphism (hom) counts of cycles improve every model. We compare the mean absolute error (MAE) of each model without any homomorphism count (baseline), against the model augmented with the hom count, and with subgraph isomorphism (iso) counts of C 3 -C 10 .M \nMAE ( \n) \nMAE ( \n) \nMAE ( ) \nGAT \n0.47\u00b10.02 \n0.22\u00b10.01 \n0.24\u00b10.01 \nGCN \n0.35\u00b10.01 \n0.20\u00b10.01 \n0.22\u00b10.01 \nGraphSage \n0.44\u00b10.01 \n0.24\u00b10.01 \n0.24\u00b10.01 \nMoNet \n0.25\u00b10.01 \n0.19\u00b10.01 \n0.16\u00b10.01 \nGatedGCN \n0.34\u00b10.05 \n0.1353\u00b10.01 \n0.1357\u00b10.01 \n\n\n\nTable 2 :\n2The effect of different cycles for the GAT model over the ZINC dataset, using mean absolute error.\n\nTable 4 :\n4All models improve the Hits@50 metric over the COLLAB dataset. We compare each model without any homomorphism count (baseline) against the model augmented with the counts of the set of patterns that showed best performance (best F).M \n+ \nF \nH @50 \nH @50 \nGAT {K 3 } \n50.32\u00b10.55 \n52.87\u00b10.87 \nGCN {K 3 , K 4 , K 5 } \n51.35\u00b11.30 \n54.60\u00b11.01 \nGraphSage {K 5 } \n50.33\u00b10.68 \n51.39\u00b11.23 \nMoNet {K 4 } \n49.81\u00b11.56 \n51.76\u00b11.38 \nGatedGCN {K 3 } \n51.00\u00b12.54 \n51.57\u00b10.68 \n\n\n\n\nF\u00fcrer, M. On the combinatorial power of the Weisfeiler-Lehman algorithm. In Proceedings of the 10th International Conference on Algorithms and Complexity, volume 10236 of Lecture Notes in Computer Science, pp. 260-271, 2017. URL https://doi.org/10.Geerts, F. The expressive power of kth-order invariant graph networks. arXiv, 2020. URL https://arxiv.org/abs/2007.12035. Grohe, M. Counting bounded tree depth homomorphisms. In Proceedings of the 35th Symposium on Logic in Computer Science, pp. 507-520, 2020b. URL https://doi.org/10.1145/ 3373718.3394739. Hamilton, W. L., Ying, Z., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, volume 30, pp. 1024-1034, 2017. Hella, L. Logical hierarchies in PTIME. Inf. Comput., 129(1):1-19, 1996. URL https://doi. org/10.1006/inco.1996.0070. Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. In Advances in Neural Information Processing Systems, volume 33, 2020. URL https://arxiv.org/abs/2005.00687. Irwin, J. J., Sterling, T., Mysinger, M. M., Bolstad, E. S., and Coleman, R. G. Zinc: A free tool to discover chemistry for biology. Journal of Chemical Information and Modeling, 52(7): 1757-1768, 2012a. URL https://doi.org/10.1021/ci3001277. Irwin, J. J., Sterling, T., Mysinger, M. M., Bolstad, E. S., and Coleman, R. G. Zinc: A free tool to discover chemistry for biology. Journal of Chemical Information and Modeling, 52(7): 1757-1768, 2012b. URL https://doi.org/10.1021/ci3001277. Ishiguro, K., Oono, K., and Hayashi, K. Weisfeiler-Lehman embedding for molecular graph neural networks. arXiv, 2020. URL https://arxiv.org/abs/2006.06909. Keriven, N. and Peyr\u00e9, G. Universal invariant and equivariant graph neural networks. In Advances in Neural Information Processing Systems, volume 32, pp. 7092-7101, 2019. URL https://proceedings.neurips.cc/paper/2019/file/ ea9268cb43f55d1d12380fb6ea5bf572-Paper.pdf. Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=SJU4ayYgl. Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. Li, M. L., Dong, M., Zhou, J., and Rush, A. M. A hierarchy of graph neural networks based on learnable local features. arXiv, 2019. URL https://arxiv.org/abs/1911.05256. Loukas, A. What graph neural networks cannot learn: depth vs width. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id= B1l2bp4YwS. Lov\u00e1sz, L. Operations with structures. Acta Mathematica, 18:321-328, 1967. URL https: //doi.org/10.1007/BF02280291. Maron, H., Ben-Hamu, H., Serviansky, H., and Lipman, Y. Provably powerful graph networks. In Advances in Neural Information Processing Systems, volume 32, pp. 2153-2164, 2019a. URL http://papers.nips.cc/paper/8488-provably-powerful-graph-networks. Maron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y. Invariant and equivariant graph networks. In International Conference on Learning Representations, 2019b. URL https://openreview. net/forum?id=Syx72jC9tm. Maron, H., Fetaya, E., Segol, N., and Lipman, Y. On the universality of invariant networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 4363-4371, 2019c. URL http://proceedings.mlr.press/v97/maron19a.html. Merkwirth, C. and Lengauer, T. Automatic generation of complementary descriptors with molecular graph networks. J. Chem. Inf. Model., 45(5):1159-1168, 2005. URL https://pubs.acs.org/ doi/pdf/10.1021/ci049613b. Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. M. Geometric deep learning on graphs and manifolds using mixture model CNNs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5115-5124, 2017. URL https://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_ Geometric_Deep_Learning_CVPR_2017_paper.pdf. Morris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G., and Grohe, M. Weisfeiler and Leman go neural: Higher-order graph neural networks. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence, pp. 4602-4609, 2019. URL https://doi. org/10.1609/aaai.v33i01.33014602. Morris, C., Rattan, G., and Mutzel, P. Weisfeiler and leman go sparse: Towards scalable higherorder graph embeddings. In Advances in Neural Information Processing Systems, volume 33, 2020. URL https://arxiv.org/abs/1904.01543. NT, H. and Maehara, T. Graph homomorphism convolution. arXiv, 2020. URL https://arxiv. org/abs/2005.01214. Otto, M. Bounded Variable Logics and Counting: A Study in Finite Models, volume 9 of Lecture Notes in Logic. Cambridge University Press, 2017. URL https://doi.org/10.1017/ 9781316716878. Sato, R. A survey on the expressive power of graph neural networks. arXiv, 2020. URL https: //arxiv.org/abs/2003.04078. Sato, R., Yamada, M., and Kashima, H. Approximation ratios of graph neural networks for combinatorial problems. In Advances in Neural Information Processing Systems, volume 32, pp. 4083-4092, 2019. URL https://papers.nips.cc/paper/2019/file/ 635440afdfc39fe37995fed127d7df4f-Paper.pdf. Sato, R., Yamada, M., and Kashima, H. Random features strengthen graph neural networks. arXiv, 2020. URL https://arxiv.org/abs/2002.03155. Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE Trans. Neural Networks, 20(1):61-80, 2009. URL https://doi.org/ 10.1109/TNN.2008.2005605. Shervashidze, N., Vishwanathan, S., Petri, T., Mehlhorn, K., and Borgwardt, K. Efficient graphlet kernels for large graph comparison. In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics, volume 5, pp. 488-495, 2009. URL http://proceedings. mlr.press/v5/shervashidze09a.html. Tahmasebi, B. and Jegelka, S. Counting substructures with higher-order graph neural networks: Possibility and impossibility results. arXiv, 2020. URL https://arxiv.org/abs/2012. 03174. Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Li\u00f2, P., and Bengio, Y. Graph attention networks. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=rJXMpikCZ. Vignac, C., Loukas, A., and Frossard, P. Building powerful and equivariant graph neural networks with structural message-passing. In Advances in Neural Information Processing Systems, volume 33, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ a32d7eeaae19821fd9ce317f3ce952a7-Abstract.html. Weisfeiler, B. J. and Lehman, A. A. A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informatsiya, 2(9):12-16, 1968. https: //www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf. Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P. S. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1):4-24, 2021. URL https://doi.org/10.1109/TNNLS.2020.2978386. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=ryGs6iA5Km. Zhang, H., Yu, J. X., Zhang, Y., Zhao, K., and Cheng, H. Distributed subgraph counting: A general approach. Proc. VLDB Endow., 13(11):2493-2507, 2020. URL http://www.vldb. org/pvldb/vol13/p2493-zhang.pdf.Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., and Sun, M. Graph neural networks: A review of methods and applications. ArXiv, 2018. URL http://arxiv.org/abs/1812.08434.Can graph neural networks count substructures? In \nAdvances in Neural Information Processing Systems, volume 33, 2020. URL https://arxiv. \norg/abs/2002.04025. \n\nCurticapean, R., Dell, H., and Marx, D. Homomorphisms are a good basis for counting small \nsubgraphs. In Proceedings of the 49th Symposium on Theory of Computing, pp. 210--223, \n2017. URL http://dx.doi.org/10.1145/3055399.3055502. \n\nDamke, C., Melnikov, V., and H\u00fcllermeier, E. A novel higher-order Weisfeiler-Lehman graph \nconvolution. arXiv, 2020. URL https://arxiv.org/abs/2007.00346. \n\nDell, H., Grohe, M., and Rattan, G. Lov\u00e1sz meets Weisfeiler and Leman. In Proceedings of the \n45th International Colloquium on Automata, Languages, and Programming, volume 107, pp. \n40:1-40:14, 2018. URL https://doi.org/10.4230/LIPIcs.ICALP.2018.40. \n\nDvorak, Z. On recognizing graphs by numbers of homomorphisms. Journal of Graph Theory, 64 \n(4):330-342, 2010. URL https://doi.org/10.1002/jgt.20461. \n\nDwivedi, V. P., Joshi, C. K., Laurent, T., Bengio, Y., and Bresson, X. Benchmarking graph neural \nnetworks. arXiv, 2020. URL https://arxiv.org/abs/2003.00982. \n\n1007/ \n978-3-319-57586-5_22. \n\nGarg, V. K., Jegelka, S., and Jaakkola, T. S. Generalization and representational limits of graph \nneural networks. In Proceedings of the 37th International Conference on Machine Learning, \nvolume 119, pp. 3419-3430, 2020. URL http://proceedings.mlr.press/v119/garg20c. \nhtml. \n\nGeerts, F., Mazowiecki, F., and P\u00e9rez, G. A. Let's agree to degree: Comparing graph convolutional \nnetworks in the message-passing framework. arXiv, 2020. URL https://arxiv.org/abs/ \n2004.02593. \n\nGilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for \nquantum chemistry. In Proceedings of the 34th International Conference on Machine Learning, \nvolume 70, pp. 1263-1272, 2017. URL http://proceedings.mlr.press/v70/gilmer17a/ \ngilmer17a.pdf. \n\nGrohe, M. Descriptive Complexity, Canonisation, and Definable Graph Structure Theory. Lec-\nture Notes in Logic. Cambridge University Press, 2017. URL https://doi.org/10.1017/ \n9781139028868. \nGrohe, M. word2vec, node2vec, graph2vec, x2vec: Towards a theory of vector embeddings of \nstructured data. In Proceedings of the 39th Symposium on Principles of Database Systems, pp. \n1-16, 2020a. URL https://doi.org/10.1145/3375395.3387641. \n\n\n\nTable 6 :\n6Total model parameters for selected cycle combinations and GNNs on the ZINC data set. In the last two rows we compare between homomorphism counts (hom) and subgraph isomorphism counts (iso).Splitting.We use the real-life training, validation and test edge splits provided by OGB. Specifically, they use collaborations until 2017 as training edges, those in 2018 as validation edges, and those in 2019 as test edges.Pattern set F \nGAT \nGCN \nGraphSage MoNet GatedGCN E,P E \n\nNone \n358 273 360 742 388 963 \n401 148 408 135 \n{C 3 } \n358 417 360 887 389 071 \n401 238 408 205 \n{C 4 } \n358 417 360 887 389 071 \n401 238 408 205 \n{C 5 } \n358 417 360 887 389 071 \n401 238 408 205 \n{C 6 } \n358 417 360 887 389 071 \n401 238 408 205 \n{C 3 , C 4 } \n358 561 361 032 389 179 \n401 328 408 275 \n{C 5 , C 6 } \n358 561 361 032 389 179 \n401 328 408 275 \n{C 4 , C 5 , C 6 } \n358 705 361 177 389 287 \n401 418 408 345 \n{C 3 , C 4 , C 5 , C 6 } \n358 849 361 322 389 395 \n401 508 408 415 \n{C 3 , . . . , C 10 } (hom) 359 425 361 902 389 827 \n401 868 408 695 \n{C 3 , . . . , C 10 } (iso) \n359 425 361 902 389 827 \n401 868 408 695 \n\n\n\nTable 7 :\n7Average training time in hours and number of epochs for selected cycle combinations and GNNs on the ZINC data set. In the last two rows we compare between homomorphism counts (hom) and subgraph isomorphism counts (iso).Model: \nGAT \nGCN \nGraphSage \nMoNet \nGatedGCN E,P E \nPattern set F \nTime Epochs Time Epochs Time Epochs Time Epochs Time Epochs \nNone \n2,40 377 \n10,99 463 \n2,46 420 \n1,53 345 \n12,08 136 \n{C 3 } \n2,88 444 \n12,03 363 \n2,03 500 \n0,91 298 \n12,07 148 \n{C 4 } \n2,30 351 \n11,36 324 \n2,31 396 \n1,70 382 \n12,06 139 \n{C 5 } \n2,42 375 \n12,03 333 \n1,70 444 \n1,06 370 \n12,06 202 \n{C 6 } \n2,40 369 \n9,98 421 \n2,58 446 \n1,25 288 \n12,08 136 \n{C 3 , C 4 } \n2,98 461 \n12,03 332 \n2,56 458 \n1,41 321 \n12,09 132 \n{C 5 , C 6 } \n2,76 422 \n12,04 319 \n2,67 464 \n1,53 356 \n12,06 137 \n{C 4 , C 5 , C 6 } \n2,45 381 \n10,13 419 \n1,67 463 \n1,04 382 \n12,04 229 \n{C 3 , C 4 , C 5 , C 6 } \n2,65 408 \n10,38 420 \n2,09 503 \n1,26 364 \n12,08 135 \n{C 3 , . . . , C 10 } (hom) 2,65 428 \n12,03 350 \n2,76 478 \n1,48 363 \n12,06 175 \n{C 3 , . . . , C 10 } (iso) \n2,78 497 \n11,72 419 \n2,63 547 \n1,58 440 \n11,62 148 \n\n\n\nTable 8 :\n8Full Results (Hits @50) for all selected pattern combinations and GNNs on the COLLAB data set. None 50,32\u00b10,55 51,36\u00b11,30 49,81\u00b11,56 50,33\u00b10,68 51,00\u00b12,54 {K 3 } 52,87\u00b10,87 53,57\u00b10,89 50,18\u00b11,38 51,10\u00b10,38 51,57\u00b10,68 {K 4 } 51,33\u00b11,42 52,84\u00b11,32 51,76\u00b11,38 51,13\u00b11,60 49,43\u00b11,85 {K 5 } 52,41\u00b10,89 54,60\u00b11,01 50,94\u00b11,30 51,39\u00b11,23 50,31\u00b11,59 {K 3 , K 4 } 52,68\u00b11,82 53,49\u00b11,35 50,88\u00b11,73 50,97\u00b10,68 51,36\u00b10,92 {K 3 , K 4 , K 5 } 51,81\u00b11,17 54,32\u00b11,02 49,94\u00b10,23 51,01\u00b11,00 51,11\u00b11,06Pattern set F GAT \nGCN \nGraphSage MoNet \nGatedGCN E,P E \n\n\n\nTable 9 :\n9Total number of model parameters for all selected pattern combinations and GNNs on the COLLAB data set. Pattern set F GAT GCN GraphSage MoNet GatedGCN E,P ENone \n25 992 40 479 39 751 \n26 487 27 440 \n{K 3 } \n26 049 40 553 39 804 \n26 525 27 475 \n{K 4 } \n26 049 40 553 39 804 \n26 525 27 475 \n{K 5 } \n26 049 40 553 39 804 \n26 525 27 475 \n{K 3 , K 4 } \n26 106 40 627 39 857 \n26 563 27 510 \n{K 3 , K 4 , K 5 } 26 163 40 701 39 910 \n26 601 27 545 \n\n\n\nTable 10 :\n10Average training times and number of epochs for all selected pattern combinations and GNNs on the COLLAB data set. Pattern set F Time #Epochs Time #Epochs Time #Epochs Time #Epochs Time #EpochsModel: \nGAT \nGCN \nMoNet \nGraphSage \nGatedGCN E,P E \n\nNone \n0,81 167 \n0,85 141 \n1,62 190 \n12,05 115,67 \n2,22 167 \n{K 3 } \n0,67 165 \n0,90 153 \n1,70 184 \n12,10 67,00 \n2,48 186 \n{K 4 } \n1,06 188 \n0,95 160 \n2,16 188 \n12,04 113,50 \n1,26 188 \n{K 5 } \n0,50 167 \n1,13 165 \n1,04 193 \n12,05 124,00 \n1,82 174 \n{K 3 , K 4 } \n1,20 189 \n0,86 128 \n2,15 189 \n12,05 113,25 \n1,51 183 \n{K 3 , K 4 , K 5 } 0,44 149 \n0,90 134 \n0,98 186 \n12,05 124,00 \n1,84 177 \n\n\n\nTable 11 :\n11Full results of the weighted accuracy for selected pattern combinations and GNNs on the CLUSTER data set. 64\u00b12,93 72,14\u00b10,19 72,57\u00b10,19 74,16\u00b10,24 {K 5 } 71,26\u00b10,39 66,60\u00b11,47 72,34\u00b10,09 72,60\u00b10,24 74,23\u00b10,07 {K 3 , K 4 } 71,80\u00b10,28 50,94\u00b122,98 72,32\u00b10,27 73,03\u00b10,25 74,17\u00b10,13 {K 3 , K 4 , K 5 } 71,63\u00b10,26 63,03\u00b13,72 72,32\u00b10,36 72,65\u00b10,13 74,03\u00b10,19Pattern set F GAT \nGCN \nMoNet \nGraphSage GatedGCN E,P E \n\nNone \n70,86\u00b10,06 70,64\u00b10,39 71,15\u00b10,33 72,25\u00b10,52 74,28\u00b10,15 \n{K 3 } \n71,60\u00b10,15 64,88\u00b14,16 72,21\u00b10,19 72,97\u00b10,23 74,14\u00b10,12 \n{K 4 } \n71,40\u00b10,24 60,\n\nTable 12 :\n12Total number of model parameters for all selected pattern combinations and GNNs on the CLUSTER data set.Pattern set F GAT GCN MoNet GraphSage GatedGCN E,P ENone \n395 396 362 849 399 373 386 835 \n406 755 \nNone \n395 396 362 849 399 373 386 835 \n406 755 \n{K 3 } \n395 396 362 849 399 373 386 835 \n406 755 \n{K 4 } \n395 548 362 995 399 463 386 943 \n406 825 \n{K 5 } \n395 700 363 141 399 553 387 051 \n406 895 \n{K 3 , K 4 } \n395 700 363 141 399 553 387 051 \n406 895 \n{K 3 , K 4 , K 5 } 396 004 363 433 399 733 387 267 \n407 035 \n\n\nTable 13 :\n13Training times (in hours) and number of epochs for all selected pattern combinations and GNNs on the CLUSTER data set.Pattern setF Time #Epochs Time #Epochs Time #Epochs Time #Epochs Time #EpochsModel: \nGAT \nGCN \nMoNet \nGraphSage \nGatedGCN E,P E \n\nNone \n1,62 109 \n2,83 117 \n1,54 125 \n0,95 101 \n10,40 92 \n{K 3 } \n1,52 107 \n2,67 85 \n1,72 145 \n1,08 102 \n11,01 89 \n{K 4 } \n1,18 107 \n1,94 80 \n1,62 149 \n0,90 102 \n10,23 90 \n{K 5 } \n1,23 106 \n2,30 84 \n1,68 143 \n0,92 99 \n10,68 91 \n{K 3 , K 4 } \n1,53 102 \n1,97 82 \n1,89 153 \n0,94 99 \n10,80 90 \n{K 3 , K 4 , K 5 } 1,62 105 \n1,96 82 \n1,95 157 \n0,97 100 \n10,25 91 \n\n\n\nTable 14 :\n14Full results of the weighted accuracy for selected pattern combinations and GNNs on the PATTERN data set. Pattern set F GAT GCN MoNet GraphSage GatedGCN E,P E None 78,83\u00b10,60 71,42\u00b11,38 85,90\u00b10,03 70,78\u00b10,19 86,15\u00b10,08 {K 3 } 84,34\u00b10,09 61,54\u00b12,20 86,59\u00b10,02 84,75\u00b10,11 85,02\u00b10,20 {K 4 } 84,43\u00b10,40 63,40\u00b11,55 86,60\u00b10,02 84,51\u00b10,06 85,40\u00b10,28 {K 5 } 83,47\u00b10,11 64,18\u00b13,88 86,57\u00b10,02 83,73\u00b10,10 85,63\u00b10,22 {K 3 , K 4 } 85,44\u00b10,24 81,29\u00b12,82 86,58\u00b10,02 85,85\u00b10,13 85,80\u00b10,20 {K 3 , K 4 , K 5 } 85,50\u00b10,23 82,49\u00b10,48 86,63\u00b10,03 85,88\u00b10,15 85,56\u00b10,33\n\nTable 15 :\n15Total number of model parameters for selected pattern combinations and GNNs on the PATTERN data set.Pattern set F GAT GCN MoNet GraphSage GatedGCN E,P ENone \n394 632 362 117 398 921 386 291 \n406 403 \n{K 3 } \n394 784 362 263 399 011 386 399 \n406 473 \n{K 4 } \n394 784 362 263 399 011 386 399 \n406 473 \n{K 5 } \n394 784 362 263 399 011 386 399 \n406 473 \n{K 3 , K 4 } \n394 936 362 409 399 101 386 507 \n406 543 \n{K 3 , K 4 , K 5 } 395 088 362 555 399 191 386 615 \n406 613 \n\n\nTable 16 :\n16Training times (in hours) and number of epochs for selected pattern combinations and GNNs on the PATTERN data set.Pattern set F Time Epochs Time Epochs Time Epochs Time Epochs Time EpochsModel: \nGAT \nGCN \nMoNet \nGraphSage \nGatedGCN E,P E \n\nNone \n1,96 87 \n3,41 102 \n1,68 116 \n0,77 103 \n10,32 101 \n{K 3 } \n0,97 97 \n2,58 80 \n1,42 107 \n0,69 105 \n9,12 95 \n{K 4 } \n0,90 90 \n2,68 80 \n1,46 106 \n0,67 95 \n9,47 94 \n{K 5 } \n0,89 95 \n2,36 80 \n1,26 100 \n0,58 98 \n9,14 99 \n{K 3 , K 4 } \n2,11 91 \n3,62 98 \n1,68 108 \n0,86 97 \n9,50 87 \n{K 3 , K 4 , K 5 } 1,02 91 \n3,26 94 \n1,48 109 \n0,76 102 \n8,84 88 \n\nC Proofs of Section 5 C.1 Proof of Proposition 4We show that for any k > 3, {C r 3 , . . . , C r k }-WL is more expressive than {C r 3 , . . . , C r k\u22121 }-WL. More precisely, we construct two graphs G and H such that G and H cannot be distinguished by {C r 3 , . . . , C r k\u22121 }-WL, but they can be distinguished by {C r 3 , . . . , C r k }-WL. The proof is analogous to the proof of Proposition 3. Indeed, it suffices to let G consist of k disjoint copies of C k+1 and H to consist of k + 1 disjoint copies of C k . Then, as observed in the proof of Proposition 3, G and H will be indistinguishable by {C r 3 , . . . , C r k\u22121 }-WL simply because each pattern has at most k \u2212 1 vertices. Yet, by construction, hom(C k , G) = hom(C k , H) and thus G and H are distinguishable (already by the initial labelling) by {C r 3 , . . . , C r k }-WL.\nCommunity detection and stochastic block models: Recent developments. E Abbe, Journal of Machine Learning Research. 18177Abbe, E. Community detection and stochastic block models: Recent developments. Journal of Machine Learning Research, 18(177):1-86, 2018. URL http://jmlr.org/papers/v18/ 16-480.html.\n\nThe surprising power of graph neural networks with random node initialization. R Abboud, \u0130 \u0130 Ceylan, M Grohe, T Lukasiewicz, arXivAbboud, R., Ceylan, \u0130. \u0130., Grohe, M., and Lukasiewicz, T. The surprising power of graph neural networks with random node initialization. arXiv, 2020. URL https://arxiv.org/abs/2010. 01179.\n\nOn Weisfeiler-Leman invariance: Subgraph counts and related graph properties. V Arvind, F Fuhlbr\u00fcck, J K\u00f6bler, O Verbitsky, 10.1016/j.jcss.2020.04.003Journal of Computer and System Sciences. 113Arvind, V., Fuhlbr\u00fcck, F., K\u00f6bler, J., and Verbitsky, O. On Weisfeiler-Leman invariance: Subgraph counts and related graph properties. Journal of Computer and System Sciences, 113:42 -59, 2020. URL https://doi.org/10.1016/j.jcss.2020.04.003.\n\nOn the power of k-consistency. A Atserias, A A Bulatov, V Dalmau, 10.1007/978-3-540-73420-8_26Proceedings of the 34th International Colloquium on Automata, Languages and Programming. the 34th International Colloquium on Automata, Languages and Programming4596Atserias, A., Bulatov, A. A., and Dalmau, V. On the power of k-consistency. In Proceedings of the 34th International Colloquium on Automata, Languages and Programming, volume 4596 of Lecture Notes in Computer Science, pp. 279-290, 2007. URL https://doi.org/10.1007/ 978-3-540-73420-8_26.\n\nExpressive power of invariant and equivariant graph neural networks. W Azizian, M Lelarge, International Conference on Learning Representations. Azizian, W. and Lelarge, M. Expressive power of invariant and equivariant graph neural networks. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=lxHgXYN4bwl.\n\nThe logical expressiveness of graph neural networks. P Barcel\u00f3, E V Kostylev, M Monet, J P\u00e9rez, J Reutter, J P Silva, International Conference on Learning Representations. Barcel\u00f3, P., Kostylev, E. V., Monet, M., P\u00e9rez, J., Reutter, J., and Silva, J. P. The logical expres- siveness of graph neural networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=r1lZ7AEKvB.\n\nLaplacian eigenmaps for dimensionality reduction and data representation. M Belkin, P Niyogi, Neural Computation. 156Belkin, M. and Niyogi, P. Laplacian eigenmaps for dimensionality reduction and data representa- tion. Neural Computation, 15(6):1373-1396, 2003. URL https://ieeexplore.ieee.org/ document/6789755.\n\nImproving graph neural network expressivity via subgraph isomorphism counting. G Bouritsas, F Frasca, S Zafeiriou, M M Bronstein, arXivBouritsas, G., Frasca, F., Zafeiriou, S., and Bronstein, M. M. Improving graph neural network expressivity via subgraph isomorphism counting. arXiv, 2020. URL https://arxiv.org/ abs/2006.09252.\n\nHow many variables are needed to express an existential positive query?. S Bova, H Chen, 10.1007/s00224-018-9884-zTheory Comput. Syst. 637Bova, S. and Chen, H. How many variables are needed to express an existential positive query? Theory Comput. Syst., 63(7):1573-1594, 2019. URL https://doi.org/10.1007/ s00224-018-9884-z.\n\n. X Bresson, T Laurent, Bresson, X. and Laurent, T. Residual gated graph convnets. arXiv, 2017. URL https://arxiv. org/abs/1711.07553.\n\nAn optimal lower bound on the number of variables for graph identifications. J Cai, M F\u00fcrer, N Immerman, 10.1007/BF01305232Combinatorica. 124Cai, J., F\u00fcrer, M., and Immerman, N. An optimal lower bound on the number of variables for graph identifications. Combinatorica, 12(4):389-410, 1992. URL https://doi.org/10. 1007/BF01305232.\n\nI Chami, S Abu-El-Ha\u0133a, B Perozzi, C R\u00e9, Murphy , K , Machine learning on graphs: A model and comprehensive taxonomy. arXivChami, I., Abu-El-Ha\u0133a, S., Perozzi, B., R\u00e9, C., and Murphy, K. Machine learning on graphs: A model and comprehensive taxonomy. arXiv, 2021. URL https://arxiv.org/abs/2005. 03675.\n\nAre powerful graph neural nets necessary? A dissection on graph classification. T Chen, S Bian, Y Sun, arXivChen, T., Bian, S., and Sun, Y. Are powerful graph neural nets necessary? A dissection on graph classification. arXiv, 2019a. URL https://arxiv.org/abs/1905.04579.\n\nOn the equivalence between graph isomorphism testing and function approximation with GNNs. Z Chen, S Villar, L Chen, Bruna , J , Advances in Neural Information Processing Systems. 32Chen, Z., Villar, S., Chen, L., and Bruna, J. On the equivalence between graph isomorphism testing and function approximation with GNNs. In Advances in Neural Information Processing Systems, volume 32, pp. 15894-15902, 2019b. URL https://proceedings.neurips.cc/ paper/2019/file/71ee911dd06428a96c143a0b135041a4-Paper.pdf.\n", "annotations": {"author": "[{\"end\":93,\"start\":53},{\"end\":132,\"start\":94},{\"end\":172,\"start\":133},{\"end\":218,\"start\":173}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":59},{\"end\":107,\"start\":101},{\"end\":145,\"start\":138},{\"end\":193,\"start\":185}]", "author_first_name": "[{\"end\":58,\"start\":53},{\"end\":100,\"start\":94},{\"end\":137,\"start\":133},{\"end\":184,\"start\":173}]", "author_affiliation": "[{\"end\":92,\"start\":68},{\"end\":131,\"start\":109},{\"end\":171,\"start\":147},{\"end\":217,\"start\":195}]", "title": "[{\"end\":50,\"start\":1},{\"end\":268,\"start\":219}]", "venue": null, "abstract": "[{\"end\":1650,\"start\":270}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2188,\"start\":2168},{\"end\":2208,\"start\":2190},{\"end\":2236,\"start\":2210},{\"end\":2349,\"start\":2328},{\"end\":2369,\"start\":2349},{\"end\":2389,\"start\":2369},{\"end\":2498,\"start\":2477},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2517,\"start\":2498},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2541,\"start\":2517},{\"end\":2554,\"start\":2541},{\"end\":2565,\"start\":2554},{\"end\":2584,\"start\":2565},{\"end\":2762,\"start\":2748},{\"end\":2780,\"start\":2762},{\"end\":3271,\"start\":3249},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3290,\"start\":3271},{\"end\":3306,\"start\":3290},{\"end\":3328,\"start\":3306},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3351,\"start\":3328},{\"end\":3371,\"start\":3351},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4067,\"start\":4043},{\"end\":4083,\"start\":4067},{\"end\":5661,\"start\":5640},{\"end\":5683,\"start\":5661},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5702,\"start\":5683},{\"end\":5720,\"start\":5702},{\"end\":5770,\"start\":5751},{\"end\":5783,\"start\":5770},{\"end\":5827,\"start\":5808},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5847,\"start\":5827},{\"end\":5909,\"start\":5888},{\"end\":5975,\"start\":5959},{\"end\":5995,\"start\":5977},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6019,\"start\":6000},{\"end\":6091,\"start\":6072},{\"end\":6297,\"start\":6271},{\"end\":7557,\"start\":7536},{\"end\":7637,\"start\":7616},{\"end\":7653,\"start\":7637},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7674,\"start\":7653},{\"end\":8168,\"start\":8165},{\"end\":8674,\"start\":8657},{\"end\":8694,\"start\":8674},{\"end\":10122,\"start\":10102},{\"end\":10237,\"start\":10211},{\"end\":11052,\"start\":11030},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11218,\"start\":11199},{\"end\":11377,\"start\":11355},{\"end\":11511,\"start\":11495},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11614,\"start\":11591},{\"end\":12385,\"start\":12359},{\"end\":12741,\"start\":12710},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12875,\"start\":12852},{\"end\":13016,\"start\":12989},{\"end\":13028,\"start\":13016},{\"end\":13374,\"start\":13358},{\"end\":13399,\"start\":13379},{\"end\":13727,\"start\":13713},{\"end\":13745,\"start\":13727},{\"end\":15217,\"start\":15201},{\"end\":15489,\"start\":15475},{\"end\":15507,\"start\":15489},{\"end\":18567,\"start\":18545},{\"end\":18587,\"start\":18567},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18667,\"start\":18647},{\"end\":19001,\"start\":18978},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19040,\"start\":19016},{\"end\":19522,\"start\":19502},{\"end\":19544,\"start\":19522},{\"end\":19564,\"start\":19544},{\"end\":21326,\"start\":21312},{\"end\":21344,\"start\":21326},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22483,\"start\":22460},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22500,\"start\":22483},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23585,\"start\":23562},{\"end\":23743,\"start\":23729},{\"end\":23761,\"start\":23743},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27721,\"start\":27697},{\"end\":27777,\"start\":27756},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28533,\"start\":28510},{\"end\":28833,\"start\":28812},{\"end\":30077,\"start\":30056},{\"end\":30098,\"start\":30077},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30437,\"start\":30413},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30720,\"start\":30697},{\"end\":53929,\"start\":53911},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":55043,\"start\":55025},{\"end\":55401,\"start\":55387},{\"end\":55419,\"start\":55401},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":58324,\"start\":58306},{\"end\":58336,\"start\":58324},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":58622,\"start\":58600},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":58645,\"start\":58627},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":59827,\"start\":59804},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":59845,\"start\":59827},{\"end\":60227,\"start\":60206},{\"end\":60935,\"start\":60932},{\"end\":61390,\"start\":61382},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":62586,\"start\":62564},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":62609,\"start\":62591},{\"end\":73980,\"start\":73962},{\"end\":75086,\"start\":75083},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":78645,\"start\":78623},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":78668,\"start\":78650},{\"end\":79850,\"start\":79823},{\"end\":79989,\"start\":79967},{\"end\":80199,\"start\":80178},{\"end\":80835,\"start\":80813},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":81239,\"start\":81216},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":81427,\"start\":81404},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":82653,\"start\":82630},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":83153,\"start\":83130},{\"end\":83385,\"start\":83379},{\"end\":83703,\"start\":83678},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":83799,\"start\":83776},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":84267,\"start\":84244},{\"end\":84397,\"start\":84372},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":84638,\"start\":84615},{\"end\":84714,\"start\":84708},{\"end\":85476,\"start\":85455},{\"end\":85946,\"start\":85925},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":86353,\"start\":86330},{\"end\":86518,\"start\":86496},{\"end\":86802,\"start\":86796},{\"end\":87217,\"start\":87196},{\"end\":88378,\"start\":88357},{\"end\":90375,\"start\":90354},{\"end\":90981,\"start\":90960},{\"end\":91551,\"start\":91530},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":91657,\"start\":91645},{\"end\":92338,\"start\":92335},{\"end\":92341,\"start\":92338},{\"end\":93116,\"start\":93113},{\"end\":93119,\"start\":93116},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":96418,\"start\":96399}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":94382,\"start\":94271},{\"attributes\":{\"id\":\"fig_1\"},\"end\":94564,\"start\":94383},{\"attributes\":{\"id\":\"fig_2\"},\"end\":94653,\"start\":94565},{\"attributes\":{\"id\":\"fig_3\"},\"end\":95588,\"start\":94654},{\"attributes\":{\"id\":\"fig_4\"},\"end\":95636,\"start\":95589},{\"attributes\":{\"id\":\"fig_5\"},\"end\":95740,\"start\":95637},{\"attributes\":{\"id\":\"fig_6\"},\"end\":95874,\"start\":95741},{\"attributes\":{\"id\":\"fig_7\"},\"end\":95938,\"start\":95875},{\"attributes\":{\"id\":\"fig_8\"},\"end\":96333,\"start\":95939},{\"attributes\":{\"id\":\"fig_9\"},\"end\":97621,\"start\":96334},{\"attributes\":{\"id\":\"fig_10\"},\"end\":97954,\"start\":97622},{\"attributes\":{\"id\":\"fig_11\"},\"end\":98328,\"start\":97955},{\"attributes\":{\"id\":\"fig_12\"},\"end\":98493,\"start\":98329},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":99235,\"start\":98494},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":99787,\"start\":99236},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":99898,\"start\":99788},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":100372,\"start\":99899},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":110496,\"start\":100373},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":111614,\"start\":110497},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":112715,\"start\":111615},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":113268,\"start\":112716},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":113723,\"start\":113269},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":114371,\"start\":113724},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":114943,\"start\":114372},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":115477,\"start\":114944},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":116097,\"start\":115478},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":116658,\"start\":116098},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":117140,\"start\":116659},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":117740,\"start\":117141}]", "paragraph": "[{\"end\":1662,\"start\":1655},{\"end\":1671,\"start\":1664},{\"end\":1894,\"start\":1673},{\"end\":2237,\"start\":1896},{\"end\":3101,\"start\":2239},{\"end\":4456,\"start\":3103},{\"end\":4842,\"start\":4458},{\"end\":5396,\"start\":4844},{\"end\":6334,\"start\":5398},{\"end\":6483,\"start\":6374},{\"end\":7468,\"start\":6485},{\"end\":8069,\"start\":7470},{\"end\":9121,\"start\":8147},{\"end\":9792,\"start\":9123},{\"end\":10742,\"start\":9794},{\"end\":11512,\"start\":10744},{\"end\":12829,\"start\":11514},{\"end\":12923,\"start\":12831},{\"end\":13584,\"start\":12956},{\"end\":13983,\"start\":13586},{\"end\":14193,\"start\":13985},{\"end\":14659,\"start\":14232},{\"end\":14892,\"start\":14800},{\"end\":14959,\"start\":14894},{\"end\":15292,\"start\":15059},{\"end\":15906,\"start\":15342},{\"end\":16352,\"start\":15908},{\"end\":16578,\"start\":16354},{\"end\":16697,\"start\":16580},{\"end\":16809,\"start\":16699},{\"end\":16923,\"start\":16811},{\"end\":17211,\"start\":16925},{\"end\":17424,\"start\":17213},{\"end\":17893,\"start\":17426},{\"end\":18096,\"start\":17895},{\"end\":19264,\"start\":18138},{\"end\":19672,\"start\":19300},{\"end\":20183,\"start\":19674},{\"end\":20697,\"start\":20185},{\"end\":21187,\"start\":20699},{\"end\":21345,\"start\":21189},{\"end\":21499,\"start\":21364},{\"end\":21652,\"start\":21501},{\"end\":21873,\"start\":21654},{\"end\":22066,\"start\":21875},{\"end\":22921,\"start\":22068},{\"end\":23265,\"start\":22923},{\"end\":23402,\"start\":23267},{\"end\":23762,\"start\":23404},{\"end\":24592,\"start\":23806},{\"end\":24926,\"start\":24594},{\"end\":25617,\"start\":24928},{\"end\":25843,\"start\":25661},{\"end\":26172,\"start\":25845},{\"end\":26469,\"start\":26174},{\"end\":26826,\"start\":26516},{\"end\":27119,\"start\":26828},{\"end\":27448,\"start\":27121},{\"end\":28721,\"start\":27464},{\"end\":29094,\"start\":28723},{\"end\":29337,\"start\":29096},{\"end\":29935,\"start\":29354},{\"end\":31017,\"start\":29937},{\"end\":31391,\"start\":31019},{\"end\":31838,\"start\":31528},{\"end\":32757,\"start\":32064},{\"end\":34378,\"start\":32759},{\"end\":34894,\"start\":34393},{\"end\":34942,\"start\":34896},{\"end\":35119,\"start\":34979},{\"end\":35891,\"start\":35714},{\"end\":36080,\"start\":35893},{\"end\":36432,\"start\":36181},{\"end\":36811,\"start\":36751},{\"end\":37617,\"start\":37559},{\"end\":38236,\"start\":38218},{\"end\":38374,\"start\":38263},{\"end\":38501,\"start\":38442},{\"end\":38907,\"start\":38547},{\"end\":38958,\"start\":38909},{\"end\":39328,\"start\":38960},{\"end\":39393,\"start\":39378},{\"end\":39557,\"start\":39395},{\"end\":39626,\"start\":39615},{\"end\":39721,\"start\":39628},{\"end\":40191,\"start\":39735},{\"end\":40352,\"start\":40317},{\"end\":40513,\"start\":40354},{\"end\":41067,\"start\":41002},{\"end\":41177,\"start\":41123},{\"end\":41335,\"start\":41179},{\"end\":41871,\"start\":41367},{\"end\":42075,\"start\":41873},{\"end\":42331,\"start\":42077},{\"end\":42945,\"start\":42333},{\"end\":43142,\"start\":42947},{\"end\":43612,\"start\":43337},{\"end\":43780,\"start\":43744},{\"end\":44130,\"start\":43962},{\"end\":44251,\"start\":44132},{\"end\":44673,\"start\":44422},{\"end\":45100,\"start\":44675},{\"end\":45502,\"start\":45278},{\"end\":45536,\"start\":45526},{\"end\":46158,\"start\":45538},{\"end\":46511,\"start\":46378},{\"end\":46701,\"start\":46566},{\"end\":46913,\"start\":46739},{\"end\":47350,\"start\":46915},{\"end\":47870,\"start\":47352},{\"end\":48105,\"start\":47872},{\"end\":48251,\"start\":48107},{\"end\":48542,\"start\":48253},{\"end\":48713,\"start\":48609},{\"end\":49438,\"start\":48715},{\"end\":49469,\"start\":49440},{\"end\":49559,\"start\":49471},{\"end\":49853,\"start\":49561},{\"end\":50256,\"start\":49891},{\"end\":50661,\"start\":50392},{\"end\":50829,\"start\":50663},{\"end\":51280,\"start\":50831},{\"end\":51343,\"start\":51319},{\"end\":51622,\"start\":51345},{\"end\":51737,\"start\":51665},{\"end\":52194,\"start\":51807},{\"end\":52719,\"start\":52370},{\"end\":53345,\"start\":52951},{\"end\":53463,\"start\":53347},{\"end\":53671,\"start\":53505},{\"end\":54171,\"start\":53896},{\"end\":54335,\"start\":54248},{\"end\":54705,\"start\":54399},{\"end\":55139,\"start\":54707},{\"end\":55318,\"start\":55170},{\"end\":55484,\"start\":55320},{\"end\":55668,\"start\":55486},{\"end\":55758,\"start\":55695},{\"end\":55909,\"start\":55760},{\"end\":56028,\"start\":55911},{\"end\":56462,\"start\":56030},{\"end\":56692,\"start\":56640},{\"end\":57775,\"start\":56694},{\"end\":58818,\"start\":57802},{\"end\":58968,\"start\":58820},{\"end\":59309,\"start\":59227},{\"end\":59437,\"start\":59324},{\"end\":59514,\"start\":59439},{\"end\":59587,\"start\":59516},{\"end\":60228,\"start\":59614},{\"end\":60841,\"start\":60230},{\"end\":60893,\"start\":60843},{\"end\":61058,\"start\":60895},{\"end\":61493,\"start\":61124},{\"end\":62419,\"start\":61495},{\"end\":62478,\"start\":62421},{\"end\":62610,\"start\":62480},{\"end\":63261,\"start\":62612},{\"end\":63635,\"start\":63514},{\"end\":63802,\"start\":63637},{\"end\":63891,\"start\":63804},{\"end\":64432,\"start\":64230},{\"end\":64759,\"start\":64434},{\"end\":64818,\"start\":64761},{\"end\":65722,\"start\":65052},{\"end\":66115,\"start\":65724},{\"end\":66325,\"start\":66117},{\"end\":66421,\"start\":66327},{\"end\":66459,\"start\":66423},{\"end\":67189,\"start\":66461},{\"end\":67230,\"start\":67191},{\"end\":67490,\"start\":67232},{\"end\":67596,\"start\":67492},{\"end\":67794,\"start\":67598},{\"end\":67870,\"start\":67796},{\"end\":68045,\"start\":67872},{\"end\":68482,\"start\":68047},{\"end\":68866,\"start\":68484},{\"end\":69201,\"start\":68868},{\"end\":69975,\"start\":69203},{\"end\":70172,\"start\":69977},{\"end\":70658,\"start\":70203},{\"end\":70962,\"start\":70685},{\"end\":71874,\"start\":70964},{\"end\":72059,\"start\":71876},{\"end\":72193,\"start\":72073},{\"end\":72687,\"start\":72213},{\"end\":73255,\"start\":72783},{\"end\":73780,\"start\":73493},{\"end\":74413,\"start\":73782},{\"end\":74685,\"start\":74444},{\"end\":74965,\"start\":74885},{\"end\":75333,\"start\":75026},{\"end\":76220,\"start\":76205},{\"end\":76298,\"start\":76263},{\"end\":76485,\"start\":76424},{\"end\":76625,\"start\":76510},{\"end\":76915,\"start\":76690},{\"end\":76998,\"start\":76954},{\"end\":77204,\"start\":77169},{\"end\":77481,\"start\":77231},{\"end\":77685,\"start\":77483},{\"end\":77872,\"start\":77687},{\"end\":78247,\"start\":77874},{\"end\":78381,\"start\":78249},{\"end\":78484,\"start\":78383},{\"end\":79419,\"start\":78486},{\"end\":79608,\"start\":79421},{\"end\":79764,\"start\":79642},{\"end\":81532,\"start\":79766},{\"end\":81951,\"start\":81553},{\"end\":82671,\"start\":82066},{\"end\":83154,\"start\":82673},{\"end\":83731,\"start\":83254},{\"end\":85009,\"start\":83733},{\"end\":85991,\"start\":85074},{\"end\":86803,\"start\":85993},{\"end\":87218,\"start\":86805},{\"end\":87581,\"start\":87220},{\"end\":88136,\"start\":87583},{\"end\":89406,\"start\":88165},{\"end\":90267,\"start\":89503},{\"end\":91292,\"start\":90269},{\"end\":91451,\"start\":91294},{\"end\":92922,\"start\":91508},{\"end\":94270,\"start\":92924}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8146,\"start\":8070},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14799,\"start\":14660},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15058,\"start\":14960},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18137,\"start\":18097},{\"attributes\":{\"id\":\"formula_4\"},\"end\":31527,\"start\":31392},{\"attributes\":{\"id\":\"formula_5\"},\"end\":32063,\"start\":31839},{\"attributes\":{\"id\":\"formula_6\"},\"end\":35684,\"start\":35120},{\"attributes\":{\"id\":\"formula_7\"},\"end\":36180,\"start\":36081},{\"attributes\":{\"id\":\"formula_8\"},\"end\":36750,\"start\":36433},{\"attributes\":{\"id\":\"formula_9\"},\"end\":37558,\"start\":36812},{\"attributes\":{\"id\":\"formula_10\"},\"end\":38217,\"start\":37618},{\"attributes\":{\"id\":\"formula_11\"},\"end\":38441,\"start\":38375},{\"attributes\":{\"id\":\"formula_12\"},\"end\":38546,\"start\":38502},{\"attributes\":{\"id\":\"formula_13\"},\"end\":39377,\"start\":39329},{\"attributes\":{\"id\":\"formula_15\"},\"end\":39614,\"start\":39558},{\"attributes\":{\"id\":\"formula_16\"},\"end\":39734,\"start\":39722},{\"attributes\":{\"id\":\"formula_17\"},\"end\":40316,\"start\":40192},{\"attributes\":{\"id\":\"formula_19\"},\"end\":41001,\"start\":40514},{\"attributes\":{\"id\":\"formula_20\"},\"end\":41122,\"start\":41068},{\"attributes\":{\"id\":\"formula_21\"},\"end\":41366,\"start\":41336},{\"attributes\":{\"id\":\"formula_23\"},\"end\":43336,\"start\":43143},{\"attributes\":{\"id\":\"formula_24\"},\"end\":43743,\"start\":43613},{\"attributes\":{\"id\":\"formula_25\"},\"end\":43961,\"start\":43781},{\"attributes\":{\"id\":\"formula_26\"},\"end\":44421,\"start\":44252},{\"attributes\":{\"id\":\"formula_27\"},\"end\":45277,\"start\":45101},{\"attributes\":{\"id\":\"formula_28\"},\"end\":45525,\"start\":45503},{\"attributes\":{\"id\":\"formula_29\"},\"end\":46377,\"start\":46159},{\"attributes\":{\"id\":\"formula_30\"},\"end\":46565,\"start\":46512},{\"attributes\":{\"id\":\"formula_31\"},\"end\":46738,\"start\":46702},{\"attributes\":{\"id\":\"formula_32\"},\"end\":48608,\"start\":48543},{\"attributes\":{\"id\":\"formula_33\"},\"end\":49890,\"start\":49854},{\"attributes\":{\"id\":\"formula_34\"},\"end\":50391,\"start\":50257},{\"attributes\":{\"id\":\"formula_35\"},\"end\":51318,\"start\":51281},{\"attributes\":{\"id\":\"formula_36\"},\"end\":51664,\"start\":51623},{\"attributes\":{\"id\":\"formula_37\"},\"end\":52369,\"start\":52195},{\"attributes\":{\"id\":\"formula_38\"},\"end\":52950,\"start\":52787},{\"attributes\":{\"id\":\"formula_39\"},\"end\":53504,\"start\":53464},{\"attributes\":{\"id\":\"formula_40\"},\"end\":53895,\"start\":53672},{\"attributes\":{\"id\":\"formula_41\"},\"end\":54247,\"start\":54172},{\"attributes\":{\"id\":\"formula_42\"},\"end\":54398,\"start\":54336},{\"attributes\":{\"id\":\"formula_43\"},\"end\":55694,\"start\":55669},{\"attributes\":{\"id\":\"formula_44\"},\"end\":56639,\"start\":56463},{\"attributes\":{\"id\":\"formula_45\"},\"end\":59226,\"start\":58969},{\"attributes\":{\"id\":\"formula_46\"},\"end\":59323,\"start\":59310},{\"attributes\":{\"id\":\"formula_47\"},\"end\":59613,\"start\":59588},{\"attributes\":{\"id\":\"formula_48\"},\"end\":61123,\"start\":61108},{\"attributes\":{\"id\":\"formula_49\"},\"end\":63513,\"start\":63297},{\"attributes\":{\"id\":\"formula_50\"},\"end\":64229,\"start\":63892},{\"attributes\":{\"id\":\"formula_51\"},\"end\":65051,\"start\":64819},{\"attributes\":{\"id\":\"formula_52\"},\"end\":70684,\"start\":70659},{\"attributes\":{\"id\":\"formula_53\"},\"end\":72072,\"start\":72060},{\"attributes\":{\"id\":\"formula_54\"},\"end\":72212,\"start\":72194},{\"attributes\":{\"id\":\"formula_55\"},\"end\":72782,\"start\":72688},{\"attributes\":{\"id\":\"formula_56\"},\"end\":73492,\"start\":73256},{\"attributes\":{\"id\":\"formula_57\"},\"end\":74884,\"start\":74686},{\"attributes\":{\"id\":\"formula_58\"},\"end\":75025,\"start\":74966},{\"attributes\":{\"id\":\"formula_59\"},\"end\":76090,\"start\":75334},{\"attributes\":{\"id\":\"formula_60\"},\"end\":76204,\"start\":76090},{\"attributes\":{\"id\":\"formula_61\"},\"end\":76262,\"start\":76221},{\"attributes\":{\"id\":\"formula_62\"},\"end\":76423,\"start\":76299},{\"attributes\":{\"id\":\"formula_63\"},\"end\":76509,\"start\":76486},{\"attributes\":{\"id\":\"formula_64\"},\"end\":76662,\"start\":76626},{\"attributes\":{\"id\":\"formula_65\"},\"end\":76689,\"start\":76662},{\"attributes\":{\"id\":\"formula_66\"},\"end\":76953,\"start\":76916},{\"attributes\":{\"id\":\"formula_67\"},\"end\":77168,\"start\":76999},{\"attributes\":{\"id\":\"formula_68\"},\"end\":81552,\"start\":81533},{\"attributes\":{\"id\":\"formula_69\"},\"end\":82065,\"start\":81952},{\"attributes\":{\"id\":\"formula_70\"},\"end\":83253,\"start\":83155}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30222,\"start\":30215},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30449,\"start\":30442},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30646,\"start\":30639},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30823,\"start\":30815},{\"end\":31558,\"start\":31551},{\"end\":32437,\"start\":32428},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33346,\"start\":33339},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":88519,\"start\":88508},{\"end\":90438,\"start\":90431}]", "section_header": "[{\"end\":1653,\"start\":1652},{\"attributes\":{\"n\":\"2\"},\"end\":6372,\"start\":6337},{\"attributes\":{\"n\":\"3\"},\"end\":12954,\"start\":12926},{\"attributes\":{\"n\":\"3.1\"},\"end\":14230,\"start\":14196},{\"attributes\":{\"n\":\"3.2\"},\"end\":15340,\"start\":15295},{\"attributes\":{\"n\":\"4\"},\"end\":19298,\"start\":19267},{\"end\":21362,\"start\":21348},{\"attributes\":{\"n\":\"5\"},\"end\":23804,\"start\":23765},{\"attributes\":{\"n\":\"5.1\"},\"end\":25659,\"start\":25620},{\"attributes\":{\"n\":\"5.2\"},\"end\":26514,\"start\":26472},{\"attributes\":{\"n\":\"6\"},\"end\":27462,\"start\":27451},{\"end\":29352,\"start\":29340},{\"attributes\":{\"n\":\"7\"},\"end\":34391,\"start\":34381},{\"end\":34953,\"start\":34945},{\"end\":34977,\"start\":34956},{\"end\":35712,\"start\":35686},{\"end\":38261,\"start\":38239},{\"end\":51805,\"start\":51740},{\"end\":52786,\"start\":52722},{\"end\":55168,\"start\":55142},{\"end\":57800,\"start\":57778},{\"attributes\":{\"n\":\"2.\"},\"end\":61107,\"start\":61061},{\"end\":63296,\"start\":63264},{\"end\":70201,\"start\":70175},{\"end\":74442,\"start\":74416},{\"end\":77229,\"start\":77207},{\"end\":79640,\"start\":79611},{\"end\":85072,\"start\":85012},{\"end\":88163,\"start\":88139},{\"end\":89453,\"start\":89409},{\"end\":89501,\"start\":89456},{\"end\":91506,\"start\":91454},{\"end\":94663,\"start\":94655},{\"end\":95591,\"start\":95590},{\"end\":96336,\"start\":96335},{\"end\":97624,\"start\":97623},{\"end\":99246,\"start\":99237},{\"end\":99798,\"start\":99789},{\"end\":99909,\"start\":99900},{\"end\":110507,\"start\":110498},{\"end\":111625,\"start\":111616},{\"end\":112726,\"start\":112717},{\"end\":113279,\"start\":113270},{\"end\":113735,\"start\":113725},{\"end\":114383,\"start\":114373},{\"end\":114955,\"start\":114945},{\"end\":115489,\"start\":115479},{\"end\":116109,\"start\":116099},{\"end\":116670,\"start\":116660},{\"end\":117152,\"start\":117142}]", "table": "[{\"end\":99787,\"start\":99547},{\"end\":100372,\"start\":100143},{\"end\":110496,\"start\":108145},{\"end\":111614,\"start\":110924},{\"end\":112715,\"start\":111846},{\"end\":113268,\"start\":113210},{\"end\":113723,\"start\":113437},{\"end\":114371,\"start\":113931},{\"end\":114943,\"start\":114737},{\"end\":115477,\"start\":115114},{\"end\":116097,\"start\":115687},{\"end\":117140,\"start\":116825},{\"end\":117740,\"start\":117342}]", "figure_caption": "[{\"end\":94382,\"start\":94273},{\"end\":94564,\"start\":94385},{\"end\":94653,\"start\":94567},{\"end\":95588,\"start\":94664},{\"end\":95636,\"start\":95592},{\"end\":95740,\"start\":95639},{\"end\":95874,\"start\":95743},{\"end\":95938,\"start\":95877},{\"end\":96333,\"start\":95941},{\"end\":97621,\"start\":96337},{\"end\":97954,\"start\":97625},{\"end\":98328,\"start\":97957},{\"end\":98493,\"start\":98331},{\"end\":99235,\"start\":98496},{\"end\":99547,\"start\":99248},{\"end\":99898,\"start\":99800},{\"end\":100143,\"start\":99911},{\"end\":108145,\"start\":100375},{\"end\":110924,\"start\":110509},{\"end\":111846,\"start\":111627},{\"end\":113210,\"start\":112728},{\"end\":113437,\"start\":113281},{\"end\":113931,\"start\":113738},{\"end\":114737,\"start\":114386},{\"end\":115114,\"start\":114958},{\"end\":115687,\"start\":115492},{\"end\":116658,\"start\":116112},{\"end\":116825,\"start\":116673},{\"end\":117342,\"start\":117155}]", "figure_ref": "[{\"end\":1723,\"start\":1715},{\"end\":1956,\"start\":1950},{\"end\":2862,\"start\":2856},{\"end\":3797,\"start\":3791},{\"end\":8512,\"start\":8506},{\"end\":8928,\"start\":8922},{\"end\":10426,\"start\":10418},{\"end\":17799,\"start\":17793}]", "bib_author_first_name": "[{\"end\":118655,\"start\":118654},{\"end\":118968,\"start\":118967},{\"end\":118978,\"start\":118977},{\"end\":118980,\"start\":118979},{\"end\":118990,\"start\":118989},{\"end\":118999,\"start\":118998},{\"end\":119287,\"start\":119286},{\"end\":119297,\"start\":119296},{\"end\":119310,\"start\":119309},{\"end\":119320,\"start\":119319},{\"end\":119677,\"start\":119676},{\"end\":119689,\"start\":119688},{\"end\":119691,\"start\":119690},{\"end\":119702,\"start\":119701},{\"end\":120263,\"start\":120262},{\"end\":120274,\"start\":120273},{\"end\":120603,\"start\":120602},{\"end\":120614,\"start\":120613},{\"end\":120616,\"start\":120615},{\"end\":120628,\"start\":120627},{\"end\":120637,\"start\":120636},{\"end\":120646,\"start\":120645},{\"end\":120657,\"start\":120656},{\"end\":120659,\"start\":120658},{\"end\":121045,\"start\":121044},{\"end\":121055,\"start\":121054},{\"end\":121364,\"start\":121363},{\"end\":121377,\"start\":121376},{\"end\":121387,\"start\":121386},{\"end\":121400,\"start\":121399},{\"end\":121402,\"start\":121401},{\"end\":121688,\"start\":121687},{\"end\":121696,\"start\":121695},{\"end\":121943,\"start\":121942},{\"end\":121954,\"start\":121953},{\"end\":122154,\"start\":122153},{\"end\":122161,\"start\":122160},{\"end\":122170,\"start\":122169},{\"end\":122410,\"start\":122409},{\"end\":122419,\"start\":122418},{\"end\":122434,\"start\":122433},{\"end\":122445,\"start\":122444},{\"end\":122456,\"start\":122450},{\"end\":122460,\"start\":122459},{\"end\":122794,\"start\":122793},{\"end\":122802,\"start\":122801},{\"end\":122810,\"start\":122809},{\"end\":123078,\"start\":123077},{\"end\":123086,\"start\":123085},{\"end\":123096,\"start\":123095},{\"end\":123108,\"start\":123103},{\"end\":123112,\"start\":123111}]", "bib_author_last_name": "[{\"end\":118660,\"start\":118656},{\"end\":118975,\"start\":118969},{\"end\":118987,\"start\":118981},{\"end\":118996,\"start\":118991},{\"end\":119011,\"start\":119000},{\"end\":119294,\"start\":119288},{\"end\":119307,\"start\":119298},{\"end\":119317,\"start\":119311},{\"end\":119330,\"start\":119321},{\"end\":119686,\"start\":119678},{\"end\":119699,\"start\":119692},{\"end\":119709,\"start\":119703},{\"end\":120271,\"start\":120264},{\"end\":120282,\"start\":120275},{\"end\":120611,\"start\":120604},{\"end\":120625,\"start\":120617},{\"end\":120634,\"start\":120629},{\"end\":120643,\"start\":120638},{\"end\":120654,\"start\":120647},{\"end\":120665,\"start\":120660},{\"end\":121052,\"start\":121046},{\"end\":121062,\"start\":121056},{\"end\":121374,\"start\":121365},{\"end\":121384,\"start\":121378},{\"end\":121397,\"start\":121388},{\"end\":121412,\"start\":121403},{\"end\":121693,\"start\":121689},{\"end\":121701,\"start\":121697},{\"end\":121951,\"start\":121944},{\"end\":121962,\"start\":121955},{\"end\":122158,\"start\":122155},{\"end\":122167,\"start\":122162},{\"end\":122179,\"start\":122171},{\"end\":122416,\"start\":122411},{\"end\":122431,\"start\":122420},{\"end\":122442,\"start\":122435},{\"end\":122448,\"start\":122446},{\"end\":122799,\"start\":122795},{\"end\":122807,\"start\":122803},{\"end\":122814,\"start\":122811},{\"end\":123083,\"start\":123079},{\"end\":123093,\"start\":123087},{\"end\":123101,\"start\":123097}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9134861},\"end\":118886,\"start\":118584},{\"attributes\":{\"id\":\"b1\"},\"end\":119206,\"start\":118888},{\"attributes\":{\"doi\":\"10.1016/j.jcss.2020.04.003\",\"id\":\"b2\",\"matched_paper_id\":53288302},\"end\":119643,\"start\":119208},{\"attributes\":{\"doi\":\"10.1007/978-3-540-73420-8_26\",\"id\":\"b3\",\"matched_paper_id\":18607297},\"end\":120191,\"start\":119645},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":235358624},\"end\":120547,\"start\":120193},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":209513260},\"end\":120968,\"start\":120549},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14879317},\"end\":121282,\"start\":120970},{\"attributes\":{\"id\":\"b7\"},\"end\":121612,\"start\":121284},{\"attributes\":{\"doi\":\"10.1007/s00224-018-9884-z\",\"id\":\"b8\",\"matched_paper_id\":13626607},\"end\":121938,\"start\":121614},{\"attributes\":{\"id\":\"b9\"},\"end\":122074,\"start\":121940},{\"attributes\":{\"doi\":\"10.1007/BF01305232\",\"id\":\"b10\",\"matched_paper_id\":30787182},\"end\":122407,\"start\":122076},{\"attributes\":{\"id\":\"b11\"},\"end\":122711,\"start\":122409},{\"attributes\":{\"id\":\"b12\"},\"end\":122984,\"start\":122713},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":168169990},\"end\":123489,\"start\":122986}]", "bib_title": "[{\"end\":118652,\"start\":118584},{\"end\":119284,\"start\":119208},{\"end\":119674,\"start\":119645},{\"end\":120260,\"start\":120193},{\"end\":120600,\"start\":120549},{\"end\":121042,\"start\":120970},{\"end\":121685,\"start\":121614},{\"end\":122151,\"start\":122076},{\"end\":123075,\"start\":122986}]", "bib_author": "[{\"end\":118662,\"start\":118654},{\"end\":118977,\"start\":118967},{\"end\":118989,\"start\":118977},{\"end\":118998,\"start\":118989},{\"end\":119013,\"start\":118998},{\"end\":119296,\"start\":119286},{\"end\":119309,\"start\":119296},{\"end\":119319,\"start\":119309},{\"end\":119332,\"start\":119319},{\"end\":119688,\"start\":119676},{\"end\":119701,\"start\":119688},{\"end\":119711,\"start\":119701},{\"end\":120273,\"start\":120262},{\"end\":120284,\"start\":120273},{\"end\":120613,\"start\":120602},{\"end\":120627,\"start\":120613},{\"end\":120636,\"start\":120627},{\"end\":120645,\"start\":120636},{\"end\":120656,\"start\":120645},{\"end\":120667,\"start\":120656},{\"end\":121054,\"start\":121044},{\"end\":121064,\"start\":121054},{\"end\":121376,\"start\":121363},{\"end\":121386,\"start\":121376},{\"end\":121399,\"start\":121386},{\"end\":121414,\"start\":121399},{\"end\":121695,\"start\":121687},{\"end\":121703,\"start\":121695},{\"end\":121953,\"start\":121942},{\"end\":121964,\"start\":121953},{\"end\":122160,\"start\":122153},{\"end\":122169,\"start\":122160},{\"end\":122181,\"start\":122169},{\"end\":122418,\"start\":122409},{\"end\":122433,\"start\":122418},{\"end\":122444,\"start\":122433},{\"end\":122450,\"start\":122444},{\"end\":122459,\"start\":122450},{\"end\":122463,\"start\":122459},{\"end\":122801,\"start\":122793},{\"end\":122809,\"start\":122801},{\"end\":122816,\"start\":122809},{\"end\":123085,\"start\":123077},{\"end\":123095,\"start\":123085},{\"end\":123103,\"start\":123095},{\"end\":123111,\"start\":123103},{\"end\":123115,\"start\":123111}]", "bib_venue": "[{\"end\":119900,\"start\":119828},{\"end\":118698,\"start\":118662},{\"end\":118965,\"start\":118888},{\"end\":119397,\"start\":119358},{\"end\":119826,\"start\":119739},{\"end\":120336,\"start\":120284},{\"end\":120719,\"start\":120667},{\"end\":121082,\"start\":121064},{\"end\":121361,\"start\":121284},{\"end\":121747,\"start\":121728},{\"end\":122212,\"start\":122199},{\"end\":122525,\"start\":122463},{\"end\":122791,\"start\":122713},{\"end\":123164,\"start\":123115}]"}}}, "year": 2023, "month": 12, "day": 17}