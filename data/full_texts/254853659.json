{"id": 254853659, "updated": "2023-10-05 06:29:14.609", "metadata": {"title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor", "authors": "[{\"first\":\"Or\",\"last\":\"Honovich\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Scialom\",\"middle\":[]},{\"first\":\"Omer\",\"last\":\"Levy\",\"middle\":[]},{\"first\":\"Timo\",\"last\":\"Schick\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.09689", "mag": null, "acl": "2023.acl-long.806", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/HonovichSLS23", "doi": "10.18653/v1/2023.acl-long.806"}}, "content": {"source": {"pdf_hash": "2317748e902a8dcc3e5ef22a5a69b70b0fc1e1e4", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2023.acl-long.806.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "026f0ebe3b718435a3e4cdaae7064c246ea98bc6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2317748e902a8dcc3e5ef22a5a69b70b0fc1e1e4.txt", "contents": "\nUnnatural Instructions: Tuning Language Models with (Almost) No Human Labor\nLong PapersCopyright Long PapersJuly 9-14, 2023\n\nOr Honovich \nTel Aviv University \u00b5 Meta AI\n\n\nThomas Scialom \nTel Aviv University \u00b5 Meta AI\n\n\nOmer Levy \nTel Aviv University \u00b5 Meta AI\n\n\nTimo Schick \nTel Aviv University \u00b5 Meta AI\n\n\nUnnatural Instructions: Tuning Language Models with (Almost) No Human Labor\n\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nthe 61st Annual Meeting of the Association for Computational LinguisticsLong Papers1July 9-14, 2023\nInstruction tuning enables pretrained language models to perform new tasks from inferencetime natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.\n\nIntroduction\n\nInstruction tuning enables pretrained language models to generalize to unseen tasks in a zero-shot setting (Sanh et al., 2021;Wei et al., 2021). One way to collect examples of instructions and their execution is to reformulate existing NLP datasets in an explicit instruction-input-output format via prompt engineering (Mishra et al., 2022;. However, the resulting data is limited to existing academic benchmarks, even though the instruction paradigm can describe any text-based task (Efrat and Levy, 2020). Alternatively, Ouyang et al. (2022) collect user-generated prompts and manually annotate their expected outputs, reflecting a different (and arguably more desirable) distribution of the instruction space, but requiring a live application with existing users and major investments in human annotation. Can we create a large dataset of instructions that is diverse in tasks, content, and phrasing, without human labor?\n\nWe introduce Unnatural Instructions, a dataset of natural language instructions and their corresponding inputs and outputs. Inspired by recent work on utilizing language models for data generation (Schick and Sch\u00fctze, 2021b;Lee et al., 2021;Liu et al., 2022a), we collect data in a fully automatic manner by prompting a pretrained language model with three examples from the Super-Natural Instructions 1 dataset (Mishra et al., 2022;) and asking the model to generate a fourth ( Figure 1). We repeat this process with 5 different seeds -i.e. the entire process requires only 15 instruction examples -to automatically produce 64,000 diverse triplets of instructions, inputs, and outputs. 2 We further diversify the dataset's format by generating additional natural language paraphrases of each instruction, while preserving the contents of any input arguments and outputs, expanding the dataset to approximately 240,000 examples. Although the dataset contains noise, our analysis reveals that more than 50% of generated examples are indeed correct, and that even incorrect examples typically contain valuable information for instruction tuning. At the same time, we find that Unnatural Instructions contains highly creative tasks -some of which are very different from \"classic\" NLP tasks -and has a more diverse set of instructions than Super-Natural Instructions.\n\nExperiments show that fine-tuning an 11Bparameter T5 model (Raffel et al., 2020) on Unnatural Instructions can outperform both T0++ (Sanh et al., 2021) and Tk-Instruct  across several benchmarks, including Super-Natural Instructions , BIG-bench Hard (Suzgun et al., 2022), and LMentry (Efrat et al., 2022). When controlling for all variables besides the data, we find that a model trained on Unnatural Instructions performs competitively with a baseline model trained on Super-Natural Instructions. In particular, we observe an 18-point gain on BIG-bench Hard (original task formulation) and a 16-point gain on LMentry, suggesting that Unnatural Instructions is particularly useful for generalizing to instructions that deviate from the distribution of classic NLP tasks. These improvements become even more pronounced when the cost of generating examples is amortized; in this case, training on Unnatural Instructions substantially outperforms our baseline on all benchmarks. We observe a log-linear relationship between the number of generated examples and downstream task performance, suggesting that performance of models trained on Unnatural Instructions can further be improved simply by increasing its size.\n\nBeyond the immediate implications on instruction tuning, this work demonstrates the viability of automatic dataset expansion using language models as an alternative to crowdsourcing. Unnatural Instructions highlights the ability of language models to produce creative and diverse data, a trait that is difficult to obtain with crowd workers, who lack the intrinsic motivation to create novel examples and typically collapse into predictable heuristics to form annotation artifacts (Gururangan et al., 2018). At the same time, language models are faster and cheaper than human labor, opening up new possibilities for scaling up data annotation.\n\n\nData Collection\n\nWe introduce Unnatural Instructions, a dataset of 240,670 diverse natural language instructions. Each example contains a natural language instruction as input and its expected execution as output. Table 2 displays examples from the dataset.\n\nUnnatural Instructions is collected in a completely automatic process, requiring a seed of only 15 manually-constructed examples, which can be produced in about one hour of human labor. We first collect a core set of 68,478 examples ( \u00a72.1) by prompting a pretrained language model M with a seed of 3 manually-annotated examples to produce a new (fourth) example. This phase uses a structured instruction format and filtering heuristics to ensure data quality. We then expand the core dataset by rephrasing the structured instructions in Example 1 Instruction: You are given a science question (easy-level) and four answer options (associated with \"A\", \"B\", \"C\", \"D\"). Your task is to find the correct answer based on scientific facts, knowledge, and reasoning. Do not generate anything else apart from one of the following characters: 'A', 'B, 'C', 'D'. There is only one correct answer for each question.\n\n\nInput: Which part of a bicycle BEST moves in a circle? (A) Seat (B) Frame (C) Foot pedal (D) Kickstand\n\nConstraints: The output should be one of the following characters: 'A', 'B, 'C', 'D'.\n\n\nExample 2\n\nInstruction: You are given a negative review and your task is to convert it to a positive review by one or more making minimal changes. Avoid changing the context of the review.\n\nInput: we stood there in shock, because we never expected this.\n\n\nConstraints: None.\n\nExample 3 Instruction: In this task, you are given two sentences taken from a conversation, and your job is to classify whether these given sentences are sequential or not. We will mark the given sentence pair as 'True' if it's sequential, otherwise 'False'. The two sentences are spoken by two different people.\n\nInput: Noah: When and where are we meeting? :), Madison: I thought you were busy...?\n\n\nConstraints: None.\n\nExample 4 Instruction: In this task, you will be given a profile of someone and your job is to generate a set of interesting questions that can lead to a conversation with the person.\n\nInput: Yvonne has been playing the violin since she was four years old. She loves all kinds of music, but her favorite composer is Bach.\n\n\nConstraints: None.\n\nFigure 1: Our data generation prompt. Blue: The metaprompt, which contains the number of the in-context example, as well as the constant fields of each example: instruction, input, and constraints. Black: The in-context examples. We show here one of our 5 incontext seeds. Pink: One of the model's generations for the given prompt. free-form natural language ( \u00a72.2). This expansion is performed automatically by prompting a language model with manually-constructed examples, scaling up the dataset more than 3-fold. Throughout this section, we use OpenAI's text-davinci-002 as M . See \u00a76 for experiments with other models.\n\n\nCore Dataset Generation\n\nThe core dataset consists of examples in a structured format, making it easier for the generating model M to predict and for us to filter automatically. We use stochastic decoding to generate example inputs (to promote creativity), followed by deterministic decoding to generate their outputs (for accuracy). Figure 2 illustrates the process.\n\nFormat Each example in the core dataset contains four fields: (1) An instruction describing the task. The instruction can be a generic template (e.g. \"Write whether the following review is positive or\nx 1 x 2 x 3 Instruction: \u2026 Input: \u2026 Constraints: \u2026 M M Output: \u2026 finetune\nInstruction-Tuned Model nucleus sampling greedy decoding Figure 2: The core Unnatural Instructions generation pipeline. We use a seed of three in-context demonstrations x 1 , x 2 , x 3 to create a large dataset of NLP tasks with instructions, inputs and outputs. As a first step, we sample instructions, inputs, and constraints from a language model M . In the next step, we use M to deterministically generate the corresponding outputs. Finally, the data can be used for instruction tuning.\n\nnegative\") that can be instantiated by a particular input argument (e.g. the review itself). (2) The input argument that instantiates the instruction, creating a specific example of the task. (3) Output space constraints, which detail the restrictions on the task's output space. Constraints are mainly relevant for classification tasks; for tasks with no specific output space constraints, this field is \"None.\" (4) A textual output reflecting a correct execution of the instruction given the input arguments and output space constraints. The first three fields (instruction, input argument, constraints) are the model's input, and the output field acts as the reference for training and/or evaluation. The constraints field is meant to guide M during output generation and is discarded after generating the outputs (see next). In Appendix D we provide data-driven evidence for selecting this particular format.\n\n\nInput Generation\n\nWe first generate examples of instruction-input-constraints by prompting a model with three task demonstrations x 1 , x 2 , x 3 , each presented in the structured format (without outputs). These demonstrations are wrapped by a simple meta-prompt that incentivizes the model to create a fourth example x 4 , as illustrated in Figure 1.\n\nWe use 5 seeds of 3 demonstrations each to generate the core dataset; i.e., the whole process requires only 15 examples. Demonstrations are taken from the Super-Natural Instructions   Output Generation Given a generated example x, we generate the corresponding output y by conditioning a pretrained language model with the instruction, input argument, and constraints (if not none), followed by an \"Output:\" prompt. Here we apply greedy decoding to prioritize correctness over creativity. We ignore examples for which the generated output is an empty string.\n\n\nTemplate Expansion\n\nExamples in our core dataset have a strict instruction-input-output format. To increase the format diversity and obtain tasks phrased in freeform natural language (Schick and Sch\u00fctze, 2021a; Sanh et al., 2021), we collect alternative formulations that preserve the content of the original instructions. Specifically, we prompt a language model to reformulate the core dataset tasks and collect two alternative formulations for each generated task. 3 Alternative formulations are often shorter and less formal than the original instructions. The rephrasing prompt contains two examples of instructions and their alternative formulation. We do not include inputs, constraints, and outputs in the rephrasing prompt; instead, we utilize the alreadygenerated inputs and outputs to complement the rephrased instruction. Unlike the examples in the core dataset, the input is often embedded into the task description. We achieve that by adding an \"{INPUT}\" placeholder, which marks the position for input insertion (Figure 3).\n\nIn some cases, the model generates two identical reformulations, or it copies the original instruction. Some alternative formulations may also have an invalid format -e.g., not containing the \"{INPUT}\" placeholder. When such failures occur we continue to sample reformulations, stopping after five unsuccessful attempts. Consequently, some instructions have only one alternative formulation, while others have none. Overall, more than 97.5% of the Example 1 Instruction: In this task, you are given an article. Your task is to summarize the article in a sentence.\n\n\nInput: {INPUT}\n\nAlternative formulation: My college roommate asked me what this article means: \"{INPUT}\". So I recapped it in layman's terms:\n\n\nExample 2\n\nInstruction: This task is about writing a correct answer for the reading comprehension task. Based on the information provided in a given passage\u2026\n\n\nInput: {INPUT}\n\nAlternative formulation: {INPUT} Based on the given context, the answer to the question is Example 3 Instruction: In this task, you are asked to determine whether the given recipe is for a savory or sweet dish. If it is for a savory dish, output \"SAVORY\". If the recipe is for a sweet dish, output \"SWEET\".\n\n\nInput: {INPUT}\n\nAlternative formulation: Given the following recipe, {INPUT}, is the dish savory or sweet? Your output should be \"SAVORY\" or \"SWEET\" instructions have two distinct, valid reformulations.\n\nIn fact, some instructions end up with more than two paraphrases because we generate two paraphrases per example (i.e. instruction-input-output pair) and the core dataset contains examples that share the exact same instruction but not the same input argument. Therefore, by cross-referencing each instruction's alternative phrasings with all of its input arguments, we can extend the data even further and arrive at a total of 240,670 examples without additional cost.\n\n\nData Analysis\n\nWe first demonstrate the creativity of Unnatural Instructions, and then manually analyze 200 randomly-sampled examples from our core dataset, focusing on correctness and diversity. We also compare our data's distribution to Super-Natural Instructions, and find our inputs to be more diverse.\n\nCreativity A major challenge when creating an instruction dataset is task creativity. Crowd workers typically collapse into predictable heuristics to form annotation artifacts (Gururangan et al., 2018). While the high performance of models trained on Unnatural Instructions (see \u00a75) suggests that it is indeed diverse and creative, we also present in Table 1 some cherry-picked examples, providing a glimpse at their creativity.\n\nCorrectness When evaluating correctness, we test whether (1) the generated instructions are logical and executable, (2) the input arguments correspond to the task described in the instruction, and (3) the outputs are correct, given the instruction and input. Although our data filtering process is minimal, 113 of the 200 analyzed examples (56.5%) are correct. Of the 87 incorrect examples, 9 (4.5%) had incomprehensible instructions, 35 (17.5%) had an input that did not match the task description, and 43 (21.5%) had incorrect outputs. Table 2 shows some correct and incorrect examples from our analysis.\n\nWhile the amount of noise in the data may raise concerns regarding its usability, many of the examples that were marked as incorrect can still be considered informative. For example, one erroneous example had the instruction \"In this task, you will be provided with a list of countries and their corresponding capital cities. You are also given a list of clues...For each clue, determine which country it is referring to and write down that country's name...\" The input argument was \"Clue 1: This capital city is on two different continents.\" This example is incorrect since the input does not conform with the format described by the instruction -a list of countries and their capitals is not provided, only a clue. However, the output is Istanbul, Turkey, which indeed lies in both Europe and Asia and therefore corresponds with the input clue. In \u00a75 we show that, despite being noisy, Unnatural Instructions provides a highly informative training signal.\n\nDiversity We manually cluster the instructions into tasks and measure the number of unique types.\n\nOut of the 200 examples tested, we identify 117 distinct tasks. While many tasks are classical NLP tasks, such as sentiment analysis, question answering, and summarization, others are not quite canonical, and some are very specific, such as detecting a recipe given a list of ingredients. Table 3 shows the most commonly generated tasks from the set of 200 analyzed examples. Other tasks appeared 3 times or less, with 85 tasks appearing only once.\n\nWe also analyze how similar each pair of examples is, as a general proxy for diversity. Specifically, we sample 10,000 pairs of examples from Unnatural Instructions, and compute the similarity of their inputs using BERTScore (Zhang et al.,\n\n\nInstruction Category\n\nYou need to answer the question 'Is this a good experiment design?', given an experiment scenario. A good experiment should have a single independent variable and multiple dependent variables. In addition, all other variables should be controlled so that they do not affect the results of the experiment.\n\n\nExperiment Verification\n\nYou are given a recipe for baking muffins that contains some errors. Your task is to correct the errors in the instructions by replacing each underlined word with the correct one from the options provided.\n\n\nRecipe Correction\n\nYou will be given a piece of text that contains characters, places, and objects. For each character in the text, you need to determine whether they are static or dynamic. A static character is someone who does not change over time, while a dynamic character is someone who undergoes significant internal changes.\n\n\nCharacter Categorization\n\nIn this task, you are asked to generate a limerick given two rhyming words. A limerick is a five-line poem with the following rhyme scheme: AABBA. The first, second and fifth lines must be of three beats, while the third and fourth lines must be of two beats each. Additionally, all poems should have the same meter (e.g., iambic pentameter)   2020). We repeat this process for Super-Natural Instructions, producing two empirical distributions. Figure 4 shows that the inputs of Unnatural Instructions tend to be less similar to each other than the inputs of Super-Natural Instructions. This result comes as a surprise considering the fact that the entire Unnatural Instructions dataset was constructed by conditioning only on 15 original examples.\n\n\nExperimental Setup\n\nWe describe model fine-tuning on Unnatural Instructions and our evaluation protocol.\n\n\nFine-Tuning on Unnatural Instructions\n\nWe fine-tune T5-LM, the language-model-adapted variant of T5-11B (Raffel et al., 2020;Lester et al., 2021). We follow standard practice for fine-tuning, using a batch size of 16 examples over 3 epochs. For training on our core dataset, we use the same template as  for formatting instructions and inputs. Our full set of training hyperparameters is available in Appendix A. We create a small validation set of 1,000 examples for model selection following the methodology proposed by : we randomly select 10 examples from 100 random tasks of the Super-Natural Instructions training set.\n\n\nBaselines\n\nWe measure the relative utility of Unnatural Instructions by comparing it to a variety of models, all based on T5-11B, which were fine-tuned with different types and quantities of manually-annotated instruction data.   any task that participates in the validation set. This model differs from Tk-Instruct along three aspects: the dataset subsample, the base model (T5-LM), and some training hyperparameters (batch size 16 for 3 epochs).\n\n\nEvaluation\n\nWe evaluate models on four different benchmarks, measuring a range of capabilities. All evaluations are carried out in a zero-shot setting, without fewshot demonstrations, unless explicitly provided in the instructions. See the full evaluation details in Appendix B.\n\nNatural Instructions We evaluate models on the test set of Super-Natural Instructions (Mishra et al., 2022;. As in the original papers, outputs are generated using greedy decoding, and performance is measured using Rouge-L.\n\nT0: Zero-Shot We evaluate models on the heldout set of T0 (Sanh et al., 2021), using rank classification for decoding and accuracy as a metric. For fair comparison, we remove tasks supersets of which are present in the Tk-Instruct training set. The final set contains six tasks: ANLI R1-R3, CB, COPA and RTE. We refer to this evaluation set as T0: Zero-Shot. Unlike Super-Natural Instructions, T0: Zero-Shot tasks do not have a strict format and are phrased in a rather free-form manner, including inputs that can be embedded into the task description. We therefore expect models trained on our core dataset (without instruction paraphrases) to perform poorly under these conditions, while adding the task reformulation data should boost performance on T0: Zero-Shot.\n\n\nBIG-bench: Hard\n\nThe \"hard\" subset of BIGbench (Suzgun et al., 2022) contains 23 challenging tasks from BIG-Bench (Srivastava et al., 2022). We investigate two different formats for all tasks:\n\ntheir original format in BIG-bench, and the format of Suzgun et al. (2022), who reformulate each task as question answering with manually added instructions; for the latter, we remove all few-shot demonstrations. For both formats, we use greedy decoding and exact match with the reference for evaluation.\n\nLMentry LMentry (Efrat et al., 2022) is a benchmark that tests basic language abilities, designed to complement common approaches for evaluating large language models. Outputs are generated by applying greedy decoding and evaluated using highaccuracy regular expressions. The benchmark's metric is the LMentry score, which combines accuracy with multiple aspects of robustness.\n\n\nResults\n\nOur main results are shown in Table 4, which reports the performance of each model on every benchmark. Remarkably, T5-LM finetuned on Unnatural Instructions outperforms several strong instruction-tuned baselines such as T0++ and Tk-Instruct; the only exception to this is BIG-bench: Hard (Orig), where T0++ performs better. Retraining a model on Super-Natural Instructions using our exact setup reveals a significantly better baseline than Tk-Instruct, using the same data. However, even in this direct comparison, Unnatural Instructions leads to stronger or equal performance for every dataset except Super-Natural Instructions itself. While T5-LM finetuned on Unnatural Instructions is outperformed by FLAN-T5, that model was trained on approximately 60 times more data. These results demonstrate that automated data generation with pretrained LMs is a viable and cost-effective alternative to human-curated data.\n\n\nPerformance with Template Expansion\n\nWe evaluate the contribution of template expansion ( \u00a72.2) to the performance of models trained on Unnatural Instructions. To this end, we finetune a single model on our full dataset with paraphrases; results are shown in the bottom row of Table 4. Adding instruction paraphrases boosts performance on T0: Zero-Shot (+3.3), Big-bench: Hard in its original format (+12.1) and LMentry (+8.7). We surmise that this improvement is largely because examples in our core dataset were generated based on demonstrations from Super-Natural Instructions only and therefore have their exact format and style. Accordingly, models trained on our core dataset rely too much on this specific format and cannot generalize well to different formats found in other benchmarks. Obtaining more format diversity through template expansion successfully addresses this issue. On the other hand, over-reliance on the format of Super-Natural Instructions is probably preferable when testing on this dataset itself, which explains the performance drop when adding paraphrases compared to the boost in performance on other benchmarks.\n\nWhile some of the performance gains observed may also be attributed to the fact that adding paraphrases simply increases the data, in \u00a75.2 we show that template expansion is helpful even when controlling for dataset size.\n\n\nPerformance Scaling by Dataset Size\n\nAs all of our data is generated from the same model using the same set of prompts, scaling up the amount of generated examples might lead to numerous repetitions and, as a consequence, diminishing returns in terms of downstream task performance. To investigate whether this is an issue, we analyze how the amount of training examples affects the performance of our finetuned models. To this end, we train models on subsets of both Super-Natural Instructions and Unnatural Instructions, ranging from 250 to 64,000 examples. As shown in Figure 5, our core and full data as well as Super-Natural Instructions all exhibit log-linear scaling laws, suggesting that even for subsets of Unnatural Instructions containing thousands of examples, simply generating more examples still adds a valuable signal to our training data.\n\nResults for LMentry ( Figure 5) show that our template expansion process is still beneficial when controlling for dataset size. The added value of the paraphrases is therefore likely to be in terms of format diversity rather than solely as a method for increasing the amount of data.\n\n\nPerformance Scaling by Cost\n\nIn practical scenarios with fixed annotation budgets, the actual cost associated with a certain level of performance is even more relevant than the number of required examples. We therefore measure model performance as a function of the cost for obtaining the training data. Based on OpenAI's pricing as of December 2022, the cost for generating an example is estimated at $0.02 for our core dataset, and $0.01 for the expanded dataset. Kiela et al. (2021)   example, excluding indirect costs such as task design and UX development; for comparison with our automatic data collection method, we assume the lower-bound human annotation cost of $0.50. As shown in Figure 5, Unnatural Instructions is clearly more cost-efficient than manually curated data. This is true even for the Super-Natural Instructions test set, where a model trained on Unnatural Instructions is weaker than a model trained on Super-Natural Instructions for a fixed number of examples, but better when controlling for cost, showing that our automatic approach outperforms crowdsourcing for a fixed annotation budget.\n\n\nGenerative Model Ablations\n\nAs a data generation model, we used text-davinci-002, an instruction-tuned variant of GPT-3 (Brown et al., 2020). However, our approach is not limited to this specific model. We experiment with original (untuned) GPT-3 model by using it as the model M in both the input generation and output generation phases (see \u00a72). We train models for 1,500 steps using 2,000 examples and evaluate the Super-Natural Instructions validation set performance as a proxy, averaged across three different random seeds. Table 5 shows how replacing an instructiontuned model with a vanilla model affects the quality of the data. We observe that while the quality of generated inputs does drop by 4.5 points, it is well within the range of other prompt ablations (see Appendix D). In other words, informative and diverse instructions can be generated by untuned language models. However, generating outputs does seem to require instruction tuning. A manual analysis reveals that outputs generated by GPT-3 mainly suffer from the model's inability to stop, often starting with the correct answer, but then degenerating into repetitions or tangents. While this may be reme-    Other work suggested creating datasets entirely automatically, without the need for labeled data. Schick and Sch\u00fctze (2021b) and Ye et al. (2022) propose to leverage pretrained language models to generate entire labeled datasets from scratch, for a given, predefined task. Agrawal et al. (2022) use pretrained language models to automatically construct multilingual QA data using only five examples per language.\n\n\nConclusion\n\nWe introduce Unnatural Instructions, an automatically generated dataset of natural language instructions and their corresponding inputs and out-puts. To the best of our knowledge, this is the first general-purpose NLP dataset that was automatically generated. Our experiments show that models trained on Unnatural Instructions outperforms models trained on manually annotated datasets across several benchmarks. Unnatural Instructions is not only cost-effective, we also provide evidence of enhanced diversity in the instructions produced and a high level of creativity in the tasks devised, a trait difficult to obtain with crowd workers. Ablations show that even weaker models without instruction tuning can generate useful instructions, though they may struggle with producing the corresponding outputs. However, coming up with interesting tasks and writing diverse instructions is arguably the main challenge of the data collection process, whereas given instructions and inputs, outputs are often far easier to annotate through crowdsourcing. Our findings incentivize utilizing models for general-purpose data generation, which we view as an intriguing direction for future research.\n\n\nLimitations\n\nWe point at some directions for future improvements in automatic instruction generation.\n\nFirst, as shown in \u00a73, Unnatural Instructions contains noisy examples, in which either the instruction, input, or output are invalid. Future work may focus on developing better filters for such examples -e.g., by annotating a subset of examples as either valid or not and training a classifier for determining the correctness of generated instances (West et al., 2022;Liu et al., 2022a).\n\nSecond, future work may employ a human-inthe-loop approach, where humans should recognize challenging patterns, encouraging models to gener- Finally, language models are known to sometimes reflect undesirable biases present in their training data. Automatically generated data may therefore contain such content. We note that during our manual analysis, we did not notice any harmful examples. Still, future work may consider applying a filtering mechanism to reduce the risk of having biased content. \n\n\nA Fine-Tuning Hyperparameters\n\nWe use the same set of hyperparameters for finetuning experiments with T5-LM (Raffel et al., 2020;Lester et al., 2021). All models are trained for up to max(3 epochs, 3000 steps) and the final model is chosen based on Rouge-L on our validation set, where we evaluate every 100 steps. We use a batch size of 16, a maximum learning rate of 1 \u00b7 10 \u22125 with warm-up for the first 10% of training and a weight decay of 0.01. We truncate inputs at 1,024 tokens and outputs at 128 tokens. All models are trained using DeepSpeed's ZeRO-3 (Rasley et al., 2020). Training on up to 64,000 examples is performed on 32 NVIDIA Tesla V100 16GB Volta GPUs using FP32; for bigger training datasets, we used 8 NVIDIA A100 40GB GPUs with BF16. For computing Rouge-L and exact match scores, we use the implementation of .\n\n\nB Evaluation Details\n\nFor evaluating model performance on Super-Natural Instructions, T0: Zero-Shot and LMEntry, we use their official evaluation scripts. For evaluation on BIG-bench: Hard, we lowercase outputs, remove punctuation characters and trim extra whitespace before computing exact match scores. The only exception to this is the task dyck_languages, where the target output consists entirely of punctuation characters. Table 6 presents the in-context demonstrations we used, taken from .\n\n\nC Data Generation Prompts\n\n\nIn-Context Demonstrations\n\nSeed 1\n\nExample 1 Instruction: In this task, you're given passages that contain mentions of names of people, places, or things. Some of these mentions refer to the same person, place, or thing. Your job is to write questions that evaluate one's understanding of such references. Good questions are expected to link pronouns (she, her, him, his, their, etc.) or other mentions to people, places, or things to which they may refer. Do not ask questions that can be answered correctly without understanding the paragraph or having multiple answers. Avoid questions that do not link phrases referring to the same entity. For each of your questions, the answer should be one or more phrases in the paragraph, and it should be unambiguous. Input: Passage: Nearing London, Oliver encounters Jack Dawkins, a pickpocket more commonly known by the nickname the \"Artful Dodger\", and his sidekick, a boy of a humorous nature named Charley Bates, but Oliver's innocent and trusting nature fails to see any dishonesty in their actions. The Dodger provides Oliver with a free meal and tells him of a gentleman in London who will \"give him lodgings for nothing, and never ask for change\". Grateful for the unexpected assistance, Oliver follows the Dodger to the \"old gentleman's\" residence. In this way Oliver unwittingly falls in with an infamous Jewish criminal known as Fagin, the gentleman of whom the Artful Dodger spoke. Ensnared, Oliver lives with Fagin and his gang of juvenile pickpockets in their lair at Saffron Hill for some time, unaware of their criminal occupations. He believes they make wallets and handkerchiefs. Constraints: None.\n\nExample 2 Instruction: You will be given a piece of text either about an everyday event, or a general statement. If the event seems a plausible event to you, or the general statement makes sense matches your commonsense, output 'True', otherwise output 'False'. Input: Text: The glass fell of a three-story building, so it broke into pieces. Constraints: The output should be one of the two: 'True' or 'False'.\n\nExample 3 Instruction: You need to answer the question 'Are the given steps in order?', given a set of steps describing a process. Your answer must be either Yes or No. If the answer is No, that means the steps are out of order and do not make sense in the order they are in. If the answer is Yes, that means the steps are in order and make sense in the order that they are in. A set of steps are not in order if the steps reference information that is introduced in a later step. Input: Steps: ['The seeds are dispersed by wind, animals, etc', 'The seeds reach the ground', 'Grow into new trees', 'The process repeats itself over and over', 'A tree produces seeds','These new trees produce seeds'] Constraints: The output should be one of the two: 'Yes' or 'No'.\n\n\nExample 4\n\nSeed 2\n\nExample 1 Instruction: In this task, you are given two phrases: Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether the Head is used for the Tail or not. The usage describes everyday affordances or uses of objects and includes both typical and atypical uses. For example, a popcorn bucket can typically be used to hold popcorn, but it could also serve as a hat in atypical situations. Classify your answers into \"Yes\" and \"No\". The phrase may also contain \"-\", a placeholder that can be an object, a person, and/or an action. Input: Head: floor mats<sep>Tail: wipe off one's boots Constraints: The output should be 'Yes' or 'No'.\n\nExample 2 Instruction: In this task, you're given a short story of five sentences written in natural language. However, the order of the given story is not correct. Your job is to return the correct order for the given five sentences to create a coherent short story with the new order that has the correct flow. Generate your answer using the number of sentences in the correct order, such as '23415'. Input: Sentence1: He is happy now. Sentence2: Rick grew up in a troubled household. Sentence3: It wasn't long before Rick got shot in a robbery. Sentence4: The incident caused him to turn a new leaf. Sentence5: He never found good support in family, and turned to gangs. Constraints: The output must consist of the numbers representing the sentences.\n\nExample 3 Instruction: In this task, you are given Yelp reviews. The task is to classify a review as \"POSITIVE\" if the overall sentiment of the review is positive or as \"NEGATIVE\" if the overall sentiment of the review is negative. Input: This is my go to place to get the best chicken Pad Thai! Also the price is super good, considering most places have high prices and poor quality. Love this place, its definitely a top 5 fav for take out. Constraints: The output should be 'POSITIVE' or 'NEGATIVE'.\n\n\nExample 4\n\nSeed 3\n\nExample 1 Instruction: You are given a question title. You need to classify it into one of the 10 topics: 0 -'Society & Culture', 1 -'Science & Mathematics ',. Input :What's the best way to clean a keyboard? Constraints: The output should be a number between 0 and 9.\n\nExample 2 Instruction: You are given a target person and two sentences. Your task is to determine whether the second sentence conveys either a stereotype, anti-stereotype, or is unrelated with respect to the first sentence and the target. A stereotype is a widely held but oversimplified notion of a group of people. An anti-stereotype is an idea that goes against the widely held but oversimplified notion of a group of people. Sentences and targets are unrelated if there is no relevance between them. Input: Target: mother. The mother walked into her kitchen. She pulled a beer out of the fridge and leaned against the doorway, sipping it slowly. Constraints: The output should be one of the three: 'stereotype', 'anti-stereotype' or 'unrelated'. Seed 4\n\nExample 1 Instruction: In this task, you are given Wikipedia articles on a range of topics as passages and a question from the passage. We ask you to answer the question by classifying the answer as 0 (False) or 1 (True). Input: Passage: Property tax -Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets. Question: is house tax and property tax are same. Constraints: The output should be 0 or 1.\n\nExample 2 Instruction: Rewrite each original sentence in order to make it easier to understand by non-native speakers of English. You can do so by replacing complex words with simpler synonyms (i.e. paraphrasing), deleting unimportant information (i.e. compression), and/or splitting a long complex sentence into several simpler ones. The final simplified sentences need to be grammatical, fluent, and retain the main ideas of their original counterparts without altering their meanings. Input: From its inception, it was designated a duty-free port and vied with the neighboring Sultanate of Pattani for trade. Constraints: None.\n\nExample 3 Instruction: You are provided with an arithmetic question. Your task is to compute the solution using the given arithmetic operations. The only arithmetic operators needed to answer the questions are'+'(addition) and'-'(subtraction). The answer should be correct to one decimal place. Input: Joan found 70 seashells on the beach. She gave Sam some of her seashells, after which she has 27 seashell left. How many seashells did she give to Sam? Constraints: None.\n\n\nExample 4\n\nSeed 5\n\nExample 1 Instruction: You are given a science question (easy-level) and four answer options (associated with \"A\", \"B\", \"C\", \"D\"). Your task is to find the correct answer based on scientific facts, knowledge, and reasoning. Do not generate anything else apart from one of the following characters: Example 2 Instruction: You are given a negative review and your task is to convert it to a positive review by one or more making minimal changes. Avoid changing the context of the review. Input: we stood there in shock, because we never expected this. Constraints: None.\n\nExample 3 Instruction: In this task, you are given two sentences taken from a conversation, and your job is to classify whether these given sentences are sequential or not. We will mark the given sentence pair as 'True' if it's sequential, otherwise 'False'.   Figure 6: The meta-prompts used in our ablations.\n\n\nD Structural Prompt Ablations\n\nWe explore the effect of the different components of our data collection pipeline by conducting structural prompt ablations. Throughout this section, we train models for 1,500 steps using 2,000 examples and evaluate the Super-Natural Instructions validation set performance, averaged across three different random seeds.\n\n\nD.1 Meta-Prompts\n\nLanguage models are known to be sensitive to the meta-prompt -i.e., the text wrapping the in-context demonstrations, which can include task description or additional guidance regarding the desired output. We therefore experiment with three different metaprompt styles: minimal, enumeration, and verbose ( Figure 6). Table 7 presents the results obtained from finetuning on datasets generated with different metaprompts. We observe that the simple enumeration approach elicits more informative examples than either the minimalistic or verbose approaches. Perhaps surprisingly, the verbose meta-prompt performs worse than the minimalistic one, possibly because the last line (the command) interrupts the pattern, and does not align well with patterns in the pretraining corpus. 4\n\n\nMeta-Prompt Super-Natural Instructions\n\nMinimal 47.5 \u00b1 0.6 Enumeration 48.7 \u00b1 0.3 Verbose 46.9 \u00b1 0.3  \n\n\nD.2 In-Context Examples\n\nModels such as GPT-3 are known to be sensitive to slight variations in prompt content, resulting in performance differences when provided with different demonstrations sampled from the same dataset (Liu et al., 2022b) and when permuting the in-context demonstrations (Kumar and Talukdar, 2021;Lu et al., 2022). To account for the effect of the provided demonstrations on the quality of the generated data, we experiment with each of our five demonstration sets separately. 5 Table 8 shows that the data generation pipeline is largely robust to variations in the in-context demonstrations, with one outlier (seed 4). Inspecting the differences between these groups, we find that seed 4 led to less constrained instructions: 1,376 out of 2,000 examples do not have constraints, whereas that number is between 28 and 880 for all other sets. Indeed, in seed 4, only one out of three prompt demonstrations had constraints, while in other sets, at least two demonstrations had constraints.\n\n\nD.3 Constraints\n\nAs mentioned in \u00a72, each instruction-input demonstration is accompanied by an additional constraints field, which details the task's output space restrictions (e.g., \"entailment\", \"contradiction\" or \"neutral\" for NLI). We note that, in all demonstrations, the instruction itself lists the output space  constraints. We hypothesize that adding the constraints field may emphasize these restrictions, ultimately steering the output generation model to produce outputs in the correct format. We verify our hypothesis by conducting two ablation experiments. First, we keep the constraints field when generating the instructions and inputs, but only use instructions and input arguments for the output generation step (i.e., without concatenating generated constraints). Second, we completely remove the constraints field from the data generation pipeline, leaving the instruction field as the only source of information for output space constraints. Table 9 shows that the constraints field has a positive effect both on the quality of the generated outputs and inputs. Removing constraints from the output generation step reduces performance by 3 points, and removing the field from the instructions-inputs generation phase decreases performance by an additional 2.2 points.\n\n\nD.4 Two-Step Process\n\nAn alternative to our two-step pipeline is to generate instruction-input-output triplets in one pass.\n\nTo test this approach, we provide the model with the same prompt used for the instruction-inputconstraints generation, only with an additional output field, added after the constraints field. As Table 9 shows, one-step generation obtains a score that is lower by 1.7 than the default two-step process. We suspect that this gap is a result of using stochastic decoding in the unified input-output generation phase, which is critical for obtaining diverse inputs. In contrast, when generating outputs in a separate phase, we can use deterministic decoding algorithms to maximize accuracy.  B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n\nWe verified that all the data and code used is publicly open -we verified license details for each, and we provided citation to all relevant resources, where license details can also be found.\n\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? 4\n\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? As for existing datasets we used, we didn't discuss that, but other than the fact that we used published datasets that are already used by the research community -we also sampled examples and manually verified their content. As for data we collected, we did discuss that in section 9, and additionally provided data analysis in section 3.\n\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and Left blank.\n\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\n\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.\n\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\n\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\n\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.\n\nFigure 3 :\n3Our template expansion prompt. Black: Fewshot demonstrations of instructions and alternative formulations. Blue: The instruction we wish to paraphrase. Pink: Model-generated task reformulation.\n\nFigure 4 :\n4Similarity scores distribution for Super-Natural Instructions and for Unnatural Instructions, obtained by sampling 10,000 pairs of examples from each dataset and computing their similarity.\n\nT0++\n(Sanh et al., 2021)  is an instruction-tuned variant of T5-LM, trained on tasks in the Prompt-Source(Bach et al., 2022)  prompt formats.Tk-Instruct fine-tune T5 v1.1 on Super-Natural Instructions, using a subsample Example Instruction: In this task, you are asked to come up with a design for an office lobby. The client has provided specific instructions on what they want in the lobby, and it is your job to create a floor plan that meets all of their requirements. (\u2713) Input: Client wants a welcoming environment with space for people to wait comfortably without feeling cramped. They also would like some sort of area where people can do work if they need to kill some time before their next meeting.(\u2713) Output: [...] Based on these requirements, a possible floor plan [...] The lobby would have a reception area [...] with access to outlets, and a coffee station. (\u2713) Instruction: You will be given several pieces of information about an event, and you have to determine whether or not it is a cause-and-effect relationship. If the given statements are related by cause and effect, then output 'True'. Otherwise, output 'False'. (\u2713) Input: Statements: ['The tornado damaged the city', 'Many people were left homeless']. (\u2713) Output: True (\u2713) Instruction: You are to determine if the statement is true or false. (\u2713) Input: Text: Santa Claus lives at the North Pole. (\u2713) Output: False (\u2717)Instruction: You are given a scientific experiment scenario and your job is to determine which variable(s) the scientist should control in order to test the hypothesis. (\u2713) Input: The hypothesis is \"If a plant receives more sunlight, then it will grow taller.\" Which variable(s) should the scientist control? (\u2717) Output: A (-)\n\nFigure 5 :\n5Scaling experiments comparing Unnatural Instructions with Super-Natural Instructions. Top row: Model performance when controlling for dataset size, tested on Super-Natural Instructions (left) and LMentry (right). Bottom row: Model performance when controlling for the cost of obtaining data, tested on Super-Natural Instructions (left) and LMentry (right). et al., 2020; Lee et al., 2021, inter alia). Kiela et al. (2021) suggest a human-and-model-in-theloop dataset creation; In the same manner, Nie et al. (2020) apply a process to create training data for the task of NLI (Dagan et al., 2006; Bowman et al., 2015). Liu et al. (2022a) combine human annotators and GPT-3, create challenging NLI examples.\n\n\nate more complex examples (Liu et al., 2022a). In another human-in-the-loop scenario, models trained on Unnatural Instructions can be queried by humans to find examples on which these models fail, thus collecting harder examples (Nie et al., 2020).\n\n\n'A', 'B, 'C', 'D'. There is only one correct answer for each question. Input: Which part of a bicycle BEST moves in a circle? (A) Seat (B) Frame (C) Foot pedal (D) Kickstand Constraints: The output should be one of the following characters: 'A', 'B, 'C', 'D'.\n\n\ntrain set. To obtain various examples using the same prompt, decoding is done by nucleus sampling with p = 0.99 (Holtzman et al., 2020). We apply three automatic filters to the generated examples to remove: (1) model generations that do not include the three input fields (instruction, input argument, and constraints), (2) instructions and inputs that are identical to those demonstrated in the prompt, (3) duplicate examples, i.e. two different examples that have the same instruction and input argument.Filtering \n\nTable 1 :\n1Examples of eight interesting generated instructions and their corresponding category. The first four examples are taken from the core dataset, while the last four were generated during the template expansion phase.\n\nTable 2 :\n2Examples of generated instructions, inputs and outputs in our core dataset. For the first two examples, the entire pair of instruction, input and output is valid. The third example has an incorrect output; in the fourth example, the experiment is not described in the input.of 757 tasks with 100 examples each. Tk-Instruct is trained with a batch size of 1,024 examples for 1,000 steps. Since our evaluation focuses on zero-shot instruction understanding, we use the definition-only version of Tk-Instruct. FLAN-T5 Chung et al. (2022) fine-tune T5 on a collection of tasks phrased as instructions in multiple prompting setups (zero-shot, few-shot, Chainof-Thought (Wei et al., 2022)), achieving impressive zero-shot generalization capabilities. T5-LM on Natural Instructions Our main point of comparison is the utility of the original manuallycurated instructions in Super-Natural Instructions. We therefore train a model which is identical to ours in all aspects but data. Specifically, we finetune the LM-adapted variant of T5-11B on a subsample of 64,000 examples from Super-Natural Instructions training set, excluding examples fromTask \n#Examples \n\nQuestion Answering \n11 \nSentiment Analysis \n10 \nArithmetic \n8 \nGeometry \n8 \nEvent Ordering \n7 \nFact Verification \n5 \nFill-in-the-Blank \n5 \nGeneral Math Puzzles \n4 \nIdentifying Overlapping Strings \n4 \nArray Manipulations and Puzzles \n4 \n\n\n\nTable 3 :\n3Top 10 tasks by #examples, out of the 200 manually-analyzed Unnatural Instructions examples.\n\n\nestimate human annotation cost at $0.50-$1.00 perModel \n\n#Examples \nSuper-Natural \nT0: \nBIG-bench: \nLMentry \nInstructions \nZero-Shot Hard (Orig/QA) \n\nPrior Work \nT5-LM \n0 \n24.3 \n40.2 \n0.0 / 0.7 \n20.6 \nT0++ \n12,492,800 \n40.3 \n\nNHO \n\n20.2 / 13.9 \n38.3 \nTk-Instruct \n75,417 \n45.6 \n41.4 \n5.8 / 11.8 \n35.7 \nFLAN-T5 \n14,336,000 \n\nNHO \nNHO \n\n39.3 / 40.0 \n52.2 \n\nDirect Comparison Baseline \nT5-LM on Super-Natural Instructions \n64,000 \n54.0 \n44.0 \n10.2 / 29.7 \n34.6 \nOur Approach \nT5-LM on Unnatural Instructions \n64,000 \n51.9 \n45.7 \n16.0 / 29.5 \n42.0 \n+ Instruction Paraphrases \n240,670 \n49.3 \n49.0 \n28.1 / 29.4 \n50.7 \n\n\n\nTable 4 :\n4Model performance on four benchmarks. Best results in our direct comparison setup are bold, best results overall are underlined. NHO indicates that a benchmark's data is not held out because it was used for training.\n\nTable 5 :\n5Performance of 11B T5-LM models trained on \n2,000 examples, generated with different models, on the \nSuper-Natural Instructions validation set. \n\ndied through various post-processing heuristics, we \nleave exploration of such methods to future work. \n\n7 Related Work \n\nInstruction Tuning Efrat and Levy (2020) pro-\npose the Instruction Paradigm, where models learn \nnew tasks from natural language instructions alone. \nMishra et al. (2022); Wang et al. (2022) construct \nthe first large-scale instruction benchmarks by col-\nlecting crowdsourcing instructions used to create \nNLP datasets and converting them into a uniform \nformat. Sanh et al. (2021); Wei et al. (2021) fur-\nther extend the usability of instructions by suggest-\ning instruction tuning, where a language model is \ntrained on many natural language instructions in the \nhope that it will generalize to new, unseen instruc-\ntion tasks. Chung et al. (2022) advance instruction \ntuning by scaling the number of tasks, scaling the \nmodel size, and adding chain-of-thought (Wei et al., \n2022), while Ouyang et al. (2022) propose a rein-\nforcement learning approach for instruction tuning \nfrom comparative human judgements. \n\nAutomatic Data Generation Obtaining large-\nscale supervised data can be expensive and time-\nconsuming, making automatic data generation ap-\npealing. A common approach is to automati-\ncally augment existing datasets (Anaby-Tavor et al., \n2020; Andreas, 2020; Yang et al., 2020; Kaushik \n\n\n\nComputational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Association for Computational Linguistics. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. \n2020. Learning the difference that makes a differ-\nence with counterfactually-augmented data. In Inter-\nnational Conference on Learning Representations. \n\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh \nKaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-\ngen, Grusha Prasad, Amanpreet Singh, Pratik Ring-\nshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, \nZeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit \nBansal, Christopher Potts, and Adina Williams. 2021. \nDynabench: Rethinking benchmarking in NLP. In \nProceedings of the 2021 Conference of the North \nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, \npages 4110-4124, Online. Association for Computa-\ntional Linguistics. \n\nSawan Kumar and Partha Talukdar. 2021. Reorder-\ning examples helps during priming-based few-shot \nlearning. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, pages \n4507-4518, Online. Association for Computational \nLinguistics. \n\nKenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and \nHyung Won Chung. 2021. Neural data augmentation \nvia example extrapolation. \n\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. \nThe power of scale for parameter-efficient prompt \ntuning. In Proceedings of the 2021 Conference on \nEmpirical Methods in Natural Language Processing, \npages 3045-3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics. \n\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and \nYejin Choi. 2022a. Wanli: Worker and ai collabora-\ntion for natural language inference dataset creation. \n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, \nLawrence Carin, and Weizhu Chen. 2022b. What \nmakes good in-context examples for GPT-3? In \nProceedings of Deep Learning Inside Out (DeeLIO \n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures, \npages 100-114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics. \n\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, \nand Pontus Stenetorp. 2022. Fantastically ordered \nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the \n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages \n\n8086-8098, Dublin, Ireland. Association for Compu-\ntational Linguistics. \n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and \nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions. \nIn Proceedings of the 60th Annual Meeting of the \nAssociation for Computational Linguistics (Volume \n1: Long Papers), pages 3470-3487, Dublin, Ireland. \nAssociation for Computational Linguistics. \n\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, \nJason Weston, and Douwe Kiela. 2020. Adversarial \nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, \npages 4885-4901, Online. Association for Computa-\ntional Linguistics. \n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang, \nSandhini Agarwal, Katarina Slama, Alex Ray, John \nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, \nMaddie Simens, Amanda Askell, Peter Welinder, \nPaul Christiano, Jan Leike, and Ryan Lowe. 2022. \nTraining language models to follow instructions with \nhuman feedback. \n\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi \nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the \nlimits of transfer learning with a unified text-to-text \ntransformer. Journal of Machine Learning Research, \n21(140):1-67. \n\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, \nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with \nover 100 billion parameters. In Proceedings of the \n26th ACM SIGKDD International Conference on \nKnowledge Discovery & Data Mining, KDD '20, \npage 3505-3506, New York, NY, USA. Association \nfor Computing Machinery. \n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H \nBach, Lintang Sutawika, Zaid Alyafeai, Antoine \nChaffin, Arnaud Stiegler, Teven Le Scao, Arun \nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint \narXiv:2110.08207. \n\nTimo Schick and Hinrich Sch\u00fctze. 2021a. Few-shot \ntext generation with natural language instructions. In \nProceedings of the 2021 Conference on Empirical \nMethods in Natural Language Processing, pages 390-\n402, Online and Punta Cana, Dominican Republic. \nAssociation for Computational Linguistics. \n\nTimo Schick and Hinrich Sch\u00fctze. 2021b. Generating \ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 6943-\n6951, Online and Punta Cana, Dominican Republic. \nAssociation for Computational Linguistics. \n\n\n\nThe two sentences are spoken by two different people. Input: Noah: When and where are we meeting? :), Madison: I thought you were busy...? Constraints: The output should be 'True' or 'False'.Example 4 \n\n\n\nTable 6 :\n6The in-context demonstrations used in our experiments.14424 \n\n\n\nTable 7 :\n7Performance of 11B T5-LM models trained on 2,000 examples, generated with each meta-prompt, on the Super-Natural Instructions validation set.Seed Demonstrations Super-Natural Instructions \n\n1 \n46.9 \u00b1 0.3 \n2 \n46.1 \u00b1 0.3 \n3 \n46.8 \u00b1 0.4 \n4 \n41.9 \u00b1 1.0 \n5 \n46.0 \u00b1 0.2 \n\nMix \n46.1 \u00b1 0.3 \n\n\n\nTable 8 :\n8Performance of 11B T5-LM models trained on 2,000 examples, generated with various sets of three in-context demonstrations (seeds), on the Super-Natural Instructions validation set. Mix samples 400 examples from each of the five single-seed datasets.\n\nTable 9 :\n9Performance of 11B T5-LM models trained on 2,000 examples, generated with and without the constraints field, on the Super-Natural Instructions validation set.\n\n\nData Generation Process Super-Natural InstructionsSeparate I/O Steps 46.9 \u00b1 0.3 Unified I/O Step 45.2 \u00b1 0.6\n\nTable 10 :\n10Performance of 11B T5-LM models trained on 2,000 examples, generated either using separate input and output steps or a single unified step, on the Super-Natural Instructions validation set. ACL 2023 Responsible NLP Checklist A For every submission: A1. Did you describe the limitations of your work? A2. Did you discuss any potential risks of your work? A3. Do the abstract and introduction summarize the paper's main claims? A4. Have you used AI writing assistants when working on this paper? Left blank. B Did you use or create scientific artifacts? B1. Did you cite the creators of artifacts you used?14426 \n\n\n\n\nlinguistic phenomena, demographic groups represented, etc.? 3 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? 4, Appendix A C2. Did you discuss the experimental setup, including hyperparameter search and best-found C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? 5, 6, Appendix D C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? 4, Appendix A, Appendix B D Did you use human annotators (e.g., crowdworkers) or research with human participants?2 \n\nC Did you run computational experiments? \n\n4, 5 \n\nhyperparameter values? \n4, Appendix A \n\n\nAlso known as Natural Instructions v2. 2 In practice, we collected 68,478 examples, but only used subsets of 64,000 examples for training.\nThe seed reformulations in each prompt are inspired and partially taken from PromptSource(Bach et al., 2022).\nWhile our core dataset was created using the enumeration meta-prompt, the remaining ablation experiments in this section were run using the verbose meta-prompt.\nSee Appendix C for all demonstration sets.\n\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, R Adam, Adam Brown, Aditya Santoro, Adri\u00e0 Gupta, Garriga-Alonso, arXiv:2206.04615arXiv preprintAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.\n\nChallenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won, Aakanksha Chung, Quoc V Chowdhery, Ed H Le, Denny Chi, Jason Zhou, Wei, 10.48550/ARXIV.2210.09261Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se- bastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them.\n\nSuper-naturalinstructions:generalization via declarative instructions on 1600+ tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, EMNLP. Yizhong Wang, Swaroop Mishra, Pegah Alipoor- molabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022. Super-naturalinstructions:generalization via declara- tive instructions on 1600+ tasks. In EMNLP.\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M Andrew, Quoc V Dai, Le, arXiv:2109.01652arXiv preprintJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. 2021. Finetuned lan- guage models are zero-shot learners. arXiv preprint arXiv:2109.01652.\n\nChain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models.\n\nSymbolic knowledge distillation: from general language models to commonsense models. Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ximing Ronan Le Bras, Sean Lu, Yejin Welleck, Choi, 10.18653/v1/2022.naacl-main.341Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesUnited StatesAssociation for Computational LinguisticsSeattlePeter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2022. Symbolic knowledge distillation: from general language mod- els to commonsense models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 4602-4625, Seat- tle, United States. Association for Computational Linguistics.\n\nGenerative data augmentation for commonsense reasoning. Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ji-Ping Ronan Le Bras, Chandra Wang, Yejin Bhagavatula, Doug Choi, Downey, 10.18653/v1/2020.findings-emnlp.90Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational LinguisticsYiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. 2020. Generative data augmentation for common- sense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1008-1025, Online. Association for Computational Linguistics.\n\nZeroGen: Efficient zero-shot learning via dataset generation. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. 2022. ZeroGen: Efficient zero-shot learning via dataset generation. In Proceedings of the 2022 Con- ference on Empirical Methods in Natural Language Processing, pages 11653-11669, Abu Dhabi, United Arab Emirates. Association for Computational Lin- guistics.\n\nBertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 2020Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu- ating text generation with bert. In ICLR 2020.\n", "annotations": {"author": "[{\"end\":170,\"start\":126},{\"end\":218,\"start\":171},{\"end\":261,\"start\":219},{\"end\":306,\"start\":262}]", "publisher": "[{\"end\":88,\"start\":77},{\"end\":555,\"start\":544}]", "author_last_name": "[{\"end\":137,\"start\":129},{\"end\":185,\"start\":178},{\"end\":228,\"start\":224},{\"end\":273,\"start\":267}]", "author_first_name": "[{\"end\":128,\"start\":126},{\"end\":177,\"start\":171},{\"end\":223,\"start\":219},{\"end\":266,\"start\":262}]", "author_affiliation": "[{\"end\":169,\"start\":139},{\"end\":217,\"start\":187},{\"end\":260,\"start\":230},{\"end\":305,\"start\":275}]", "title": "[{\"end\":76,\"start\":1},{\"end\":382,\"start\":307}]", "venue": "[{\"end\":471,\"start\":384}]", "abstract": "[{\"end\":1678,\"start\":572}]", "bib_ref": "[{\"end\":1820,\"start\":1801},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1837,\"start\":1820},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2034,\"start\":2013},{\"end\":2200,\"start\":2178},{\"end\":2237,\"start\":2202},{\"end\":2844,\"start\":2817},{\"end\":2861,\"start\":2844},{\"end\":2879,\"start\":2861},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3053,\"start\":3032},{\"end\":3308,\"start\":3307},{\"end\":4066,\"start\":4045},{\"end\":4137,\"start\":4118},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4257,\"start\":4236},{\"end\":4291,\"start\":4271},{\"end\":5708,\"start\":5683},{\"end\":15261,\"start\":15236},{\"end\":19753,\"start\":19732},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19773,\"start\":19753},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21092,\"start\":21071},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22048,\"start\":22027},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22119,\"start\":22094},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22248,\"start\":22228},{\"end\":27558,\"start\":27532},{\"end\":28205,\"start\":28194},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28746,\"start\":28730},{\"end\":28895,\"start\":28874},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30690,\"start\":30671},{\"end\":30708,\"start\":30690},{\"end\":31345,\"start\":31324},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31365,\"start\":31345},{\"end\":31797,\"start\":31769},{\"end\":32962,\"start\":32929},{\"end\":37744,\"start\":37742},{\"end\":43754,\"start\":43728},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":43770,\"start\":43754}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48911,\"start\":48705},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49114,\"start\":48912},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50837,\"start\":49115},{\"attributes\":{\"id\":\"fig_4\"},\"end\":51556,\"start\":50838},{\"attributes\":{\"id\":\"fig_5\"},\"end\":51807,\"start\":51557},{\"attributes\":{\"id\":\"fig_6\"},\"end\":52069,\"start\":51808},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":52588,\"start\":52070},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":52816,\"start\":52589},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":54220,\"start\":52817},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":54325,\"start\":54221},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":54941,\"start\":54326},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":55170,\"start\":54942},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":56653,\"start\":55171},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":61973,\"start\":56654},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":62179,\"start\":61974},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":62254,\"start\":62180},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":62551,\"start\":62255},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":62813,\"start\":62552},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":62984,\"start\":62814},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":63094,\"start\":62985},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":63721,\"start\":63095},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":65298,\"start\":63722}]", "paragraph": "[{\"end\":2618,\"start\":1694},{\"end\":3984,\"start\":2620},{\"end\":5200,\"start\":3986},{\"end\":5845,\"start\":5202},{\"end\":6105,\"start\":5865},{\"end\":7013,\"start\":6107},{\"end\":7205,\"start\":7120},{\"end\":7396,\"start\":7219},{\"end\":7461,\"start\":7398},{\"end\":7796,\"start\":7484},{\"end\":7882,\"start\":7798},{\"end\":8088,\"start\":7905},{\"end\":8226,\"start\":8090},{\"end\":8872,\"start\":8249},{\"end\":9242,\"start\":8900},{\"end\":9444,\"start\":9244},{\"end\":10010,\"start\":9519},{\"end\":10924,\"start\":10012},{\"end\":11279,\"start\":10945},{\"end\":11839,\"start\":11281},{\"end\":12880,\"start\":11862},{\"end\":13445,\"start\":12882},{\"end\":13589,\"start\":13464},{\"end\":13749,\"start\":13603},{\"end\":14074,\"start\":13768},{\"end\":14279,\"start\":14093},{\"end\":14749,\"start\":14281},{\"end\":15058,\"start\":14767},{\"end\":15488,\"start\":15060},{\"end\":16096,\"start\":15490},{\"end\":17055,\"start\":16098},{\"end\":17154,\"start\":17057},{\"end\":17604,\"start\":17156},{\"end\":17845,\"start\":17606},{\"end\":18174,\"start\":17870},{\"end\":18407,\"start\":18202},{\"end\":18741,\"start\":18429},{\"end\":19518,\"start\":18770},{\"end\":19625,\"start\":19541},{\"end\":20252,\"start\":19667},{\"end\":20702,\"start\":20266},{\"end\":20983,\"start\":20717},{\"end\":21208,\"start\":20985},{\"end\":21977,\"start\":21210},{\"end\":22172,\"start\":21997},{\"end\":22478,\"start\":22174},{\"end\":22857,\"start\":22480},{\"end\":23784,\"start\":22869},{\"end\":24930,\"start\":23824},{\"end\":25153,\"start\":24932},{\"end\":26011,\"start\":25193},{\"end\":26296,\"start\":26013},{\"end\":27415,\"start\":26328},{\"end\":29013,\"start\":27446},{\"end\":30216,\"start\":29028},{\"end\":30320,\"start\":30232},{\"end\":30709,\"start\":30322},{\"end\":31213,\"start\":30711},{\"end\":32047,\"start\":31247},{\"end\":32547,\"start\":32072},{\"end\":32611,\"start\":32605},{\"end\":34238,\"start\":32613},{\"end\":34650,\"start\":34240},{\"end\":35415,\"start\":34652},{\"end\":35435,\"start\":35429},{\"end\":36305,\"start\":35437},{\"end\":37060,\"start\":36307},{\"end\":37564,\"start\":37062},{\"end\":37584,\"start\":37578},{\"end\":37853,\"start\":37586},{\"end\":38611,\"start\":37855},{\"end\":40168,\"start\":38613},{\"end\":40800,\"start\":40170},{\"end\":41274,\"start\":40802},{\"end\":41294,\"start\":41288},{\"end\":41864,\"start\":41296},{\"end\":42176,\"start\":41866},{\"end\":42530,\"start\":42210},{\"end\":43328,\"start\":42551},{\"end\":43433,\"start\":43371},{\"end\":44444,\"start\":43461},{\"end\":45735,\"start\":44464},{\"end\":45861,\"start\":45760},{\"end\":46539,\"start\":45863},{\"end\":46733,\"start\":46541},{\"end\":47106,\"start\":46735},{\"end\":47680,\"start\":47108},{\"end\":47787,\"start\":47682},{\"end\":47964,\"start\":47789},{\"end\":48203,\"start\":47966},{\"end\":48448,\"start\":48205},{\"end\":48558,\"start\":48450},{\"end\":48704,\"start\":48560}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9518,\"start\":9445}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":6069,\"start\":6062},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16035,\"start\":16028},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17452,\"start\":17445},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22906,\"start\":22899},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24071,\"start\":24064},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":27955,\"start\":27948},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":32486,\"start\":32479},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":42874,\"start\":42867},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":43943,\"start\":43936},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":45417,\"start\":45410},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":46065,\"start\":46058}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1692,\"start\":1680},{\"attributes\":{\"n\":\"2\"},\"end\":5863,\"start\":5848},{\"end\":7118,\"start\":7016},{\"end\":7217,\"start\":7208},{\"end\":7482,\"start\":7464},{\"end\":7903,\"start\":7885},{\"end\":8247,\"start\":8229},{\"attributes\":{\"n\":\"2.1\"},\"end\":8898,\"start\":8875},{\"end\":10943,\"start\":10927},{\"attributes\":{\"n\":\"2.2\"},\"end\":11860,\"start\":11842},{\"end\":13462,\"start\":13448},{\"end\":13601,\"start\":13592},{\"end\":13766,\"start\":13752},{\"end\":14091,\"start\":14077},{\"attributes\":{\"n\":\"3\"},\"end\":14765,\"start\":14752},{\"end\":17868,\"start\":17848},{\"end\":18200,\"start\":18177},{\"end\":18427,\"start\":18410},{\"end\":18768,\"start\":18744},{\"attributes\":{\"n\":\"4\"},\"end\":19539,\"start\":19521},{\"attributes\":{\"n\":\"4.1\"},\"end\":19665,\"start\":19628},{\"attributes\":{\"n\":\"4.2\"},\"end\":20264,\"start\":20255},{\"attributes\":{\"n\":\"4.3\"},\"end\":20715,\"start\":20705},{\"end\":21995,\"start\":21980},{\"attributes\":{\"n\":\"5\"},\"end\":22867,\"start\":22860},{\"attributes\":{\"n\":\"5.1\"},\"end\":23822,\"start\":23787},{\"attributes\":{\"n\":\"5.2\"},\"end\":25191,\"start\":25156},{\"attributes\":{\"n\":\"5.3\"},\"end\":26326,\"start\":26299},{\"attributes\":{\"n\":\"6\"},\"end\":27444,\"start\":27418},{\"attributes\":{\"n\":\"8\"},\"end\":29026,\"start\":29016},{\"attributes\":{\"n\":\"9\"},\"end\":30230,\"start\":30219},{\"end\":31245,\"start\":31216},{\"end\":32070,\"start\":32050},{\"end\":32575,\"start\":32550},{\"end\":32603,\"start\":32578},{\"end\":35427,\"start\":35418},{\"end\":37576,\"start\":37567},{\"end\":41286,\"start\":41277},{\"end\":42208,\"start\":42179},{\"end\":42549,\"start\":42533},{\"end\":43369,\"start\":43331},{\"end\":43459,\"start\":43436},{\"end\":44462,\"start\":44447},{\"end\":45758,\"start\":45738},{\"end\":48716,\"start\":48706},{\"end\":48923,\"start\":48913},{\"end\":49120,\"start\":49116},{\"end\":50849,\"start\":50839},{\"end\":52599,\"start\":52590},{\"end\":52827,\"start\":52818},{\"end\":54231,\"start\":54222},{\"end\":54952,\"start\":54943},{\"end\":55181,\"start\":55172},{\"end\":62190,\"start\":62181},{\"end\":62265,\"start\":62256},{\"end\":62562,\"start\":62553},{\"end\":62824,\"start\":62815},{\"end\":63106,\"start\":63096}]", "table": "[{\"end\":52588,\"start\":52578},{\"end\":54220,\"start\":53965},{\"end\":54941,\"start\":54377},{\"end\":56653,\"start\":55183},{\"end\":61973,\"start\":56990},{\"end\":62179,\"start\":62167},{\"end\":62254,\"start\":62246},{\"end\":62551,\"start\":62408},{\"end\":63721,\"start\":63713},{\"end\":65298,\"start\":65204}]", "figure_caption": "[{\"end\":48911,\"start\":48718},{\"end\":49114,\"start\":48925},{\"end\":50837,\"start\":49121},{\"end\":51556,\"start\":50851},{\"end\":51807,\"start\":51559},{\"end\":52069,\"start\":51810},{\"end\":52578,\"start\":52072},{\"end\":52816,\"start\":52601},{\"end\":53965,\"start\":52829},{\"end\":54325,\"start\":54233},{\"end\":54377,\"start\":54328},{\"end\":55170,\"start\":54954},{\"end\":56990,\"start\":56656},{\"end\":62167,\"start\":61976},{\"end\":62246,\"start\":62192},{\"end\":62408,\"start\":62267},{\"end\":62813,\"start\":62564},{\"end\":62984,\"start\":62826},{\"end\":63094,\"start\":62987},{\"end\":63713,\"start\":63109},{\"end\":65204,\"start\":63724}]", "figure_ref": "[{\"end\":3107,\"start\":3099},{\"end\":9217,\"start\":9209},{\"end\":9584,\"start\":9576},{\"end\":11278,\"start\":11270},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12879,\"start\":12869},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19223,\"start\":19215},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25736,\"start\":25728},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26044,\"start\":26035},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26997,\"start\":26989},{\"end\":42135,\"start\":42127},{\"end\":42864,\"start\":42856}]", "bib_author_first_name": "[{\"end\":65853,\"start\":65847},{\"end\":65873,\"start\":65866},{\"end\":65891,\"start\":65883},{\"end\":65908,\"start\":65897},{\"end\":65924,\"start\":65916},{\"end\":65935,\"start\":65931},{\"end\":65944,\"start\":65943},{\"end\":65955,\"start\":65951},{\"end\":65969,\"start\":65963},{\"end\":65984,\"start\":65979},{\"end\":66417,\"start\":66412},{\"end\":66432,\"start\":66426},{\"end\":66450,\"start\":66441},{\"end\":66469,\"start\":66460},{\"end\":66482,\"start\":66480},{\"end\":66508,\"start\":66499},{\"end\":66520,\"start\":66516},{\"end\":66522,\"start\":66521},{\"end\":66536,\"start\":66534},{\"end\":66538,\"start\":66537},{\"end\":66548,\"start\":66543},{\"end\":66559,\"start\":66554},{\"end\":66935,\"start\":66928},{\"end\":66949,\"start\":66942},{\"end\":66963,\"start\":66958},{\"end\":66989,\"start\":66982},{\"end\":67005,\"start\":66997},{\"end\":67021,\"start\":67015},{\"end\":67038,\"start\":67033},{\"end\":67050,\"start\":67046},{\"end\":67079,\"start\":67072},{\"end\":67091,\"start\":67086},{\"end\":67444,\"start\":67439},{\"end\":67457,\"start\":67450},{\"end\":67466,\"start\":67465},{\"end\":67482,\"start\":67476},{\"end\":67494,\"start\":67489},{\"end\":67498,\"start\":67495},{\"end\":67509,\"start\":67504},{\"end\":67517,\"start\":67514},{\"end\":67531,\"start\":67530},{\"end\":67546,\"start\":67540},{\"end\":67875,\"start\":67870},{\"end\":67887,\"start\":67881},{\"end\":67898,\"start\":67894},{\"end\":67918,\"start\":67911},{\"end\":67928,\"start\":67926},{\"end\":67938,\"start\":67934},{\"end\":67948,\"start\":67943},{\"end\":68212,\"start\":68207},{\"end\":68226,\"start\":68219},{\"end\":68244,\"start\":68240},{\"end\":68257,\"start\":68253},{\"end\":68270,\"start\":68265},{\"end\":68284,\"start\":68278},{\"end\":68304,\"start\":68300},{\"end\":68314,\"start\":68309},{\"end\":69210,\"start\":69205},{\"end\":69226,\"start\":69217},{\"end\":69242,\"start\":69237},{\"end\":69260,\"start\":69254},{\"end\":69281,\"start\":69274},{\"end\":69304,\"start\":69297},{\"end\":69316,\"start\":69311},{\"end\":69334,\"start\":69330},{\"end\":69928,\"start\":69920},{\"end\":69939,\"start\":69933},{\"end\":69952,\"start\":69945},{\"end\":69961,\"start\":69957},{\"end\":69974,\"start\":69966},{\"end\":69988,\"start\":69981},{\"end\":69996,\"start\":69993},{\"end\":70009,\"start\":70001},{\"end\":70661,\"start\":70655},{\"end\":70675,\"start\":70669},{\"end\":70690,\"start\":70685},{\"end\":70701,\"start\":70695},{\"end\":70703,\"start\":70702},{\"end\":70720,\"start\":70716}]", "bib_author_last_name": "[{\"end\":65864,\"start\":65854},{\"end\":65881,\"start\":65874},{\"end\":65895,\"start\":65892},{\"end\":65914,\"start\":65909},{\"end\":65929,\"start\":65925},{\"end\":65941,\"start\":65936},{\"end\":65949,\"start\":65945},{\"end\":65961,\"start\":65956},{\"end\":65977,\"start\":65970},{\"end\":65990,\"start\":65985},{\"end\":66006,\"start\":65992},{\"end\":66424,\"start\":66418},{\"end\":66439,\"start\":66433},{\"end\":66458,\"start\":66451},{\"end\":66478,\"start\":66470},{\"end\":66486,\"start\":66483},{\"end\":66497,\"start\":66488},{\"end\":66514,\"start\":66509},{\"end\":66532,\"start\":66523},{\"end\":66541,\"start\":66539},{\"end\":66552,\"start\":66549},{\"end\":66564,\"start\":66560},{\"end\":66569,\"start\":66566},{\"end\":66940,\"start\":66936},{\"end\":66956,\"start\":66950},{\"end\":66980,\"start\":66964},{\"end\":66995,\"start\":66990},{\"end\":67013,\"start\":67006},{\"end\":67031,\"start\":67022},{\"end\":67044,\"start\":67039},{\"end\":67070,\"start\":67051},{\"end\":67084,\"start\":67080},{\"end\":67096,\"start\":67092},{\"end\":67448,\"start\":67445},{\"end\":67463,\"start\":67458},{\"end\":67474,\"start\":67467},{\"end\":67487,\"start\":67483},{\"end\":67502,\"start\":67499},{\"end\":67512,\"start\":67510},{\"end\":67524,\"start\":67518},{\"end\":67528,\"start\":67526},{\"end\":67538,\"start\":67532},{\"end\":67550,\"start\":67547},{\"end\":67554,\"start\":67552},{\"end\":67879,\"start\":67876},{\"end\":67892,\"start\":67888},{\"end\":67909,\"start\":67899},{\"end\":67924,\"start\":67919},{\"end\":67932,\"start\":67929},{\"end\":67941,\"start\":67939},{\"end\":67953,\"start\":67949},{\"end\":68217,\"start\":68213},{\"end\":68238,\"start\":68227},{\"end\":68251,\"start\":68245},{\"end\":68263,\"start\":68258},{\"end\":68276,\"start\":68271},{\"end\":68298,\"start\":68285},{\"end\":68307,\"start\":68305},{\"end\":68322,\"start\":68315},{\"end\":68328,\"start\":68324},{\"end\":69215,\"start\":69211},{\"end\":69235,\"start\":69227},{\"end\":69252,\"start\":69243},{\"end\":69272,\"start\":69261},{\"end\":69295,\"start\":69282},{\"end\":69309,\"start\":69305},{\"end\":69328,\"start\":69317},{\"end\":69339,\"start\":69335},{\"end\":69347,\"start\":69341},{\"end\":69931,\"start\":69929},{\"end\":69943,\"start\":69940},{\"end\":69955,\"start\":69953},{\"end\":69964,\"start\":69962},{\"end\":69979,\"start\":69975},{\"end\":69991,\"start\":69989},{\"end\":69999,\"start\":69997},{\"end\":70014,\"start\":70010},{\"end\":70667,\"start\":70662},{\"end\":70683,\"start\":70676},{\"end\":70693,\"start\":70691},{\"end\":70714,\"start\":70704},{\"end\":70726,\"start\":70721}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2206.04615\",\"id\":\"b0\"},\"end\":66337,\"start\":65753},{\"attributes\":{\"doi\":\"10.48550/ARXIV.2210.09261\",\"id\":\"b1\"},\"end\":66840,\"start\":66339},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":253098274},\"end\":67387,\"start\":66842},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b3\"},\"end\":67797,\"start\":67389},{\"attributes\":{\"id\":\"b4\"},\"end\":68120,\"start\":67799},{\"attributes\":{\"doi\":\"10.18653/v1/2022.naacl-main.341\",\"id\":\"b5\",\"matched_paper_id\":238857304},\"end\":69147,\"start\":68122},{\"attributes\":{\"doi\":\"10.18653/v1/2020.findings-emnlp.90\",\"id\":\"b6\",\"matched_paper_id\":222177123},\"end\":69856,\"start\":69149},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":246867045},\"end\":70604,\"start\":69858},{\"attributes\":{\"id\":\"b8\"},\"end\":70880,\"start\":70606}]", "bib_title": "[{\"end\":66926,\"start\":66842},{\"end\":68205,\"start\":68122},{\"end\":69203,\"start\":69149},{\"end\":69918,\"start\":69858}]", "bib_author": "[{\"end\":65866,\"start\":65847},{\"end\":65883,\"start\":65866},{\"end\":65897,\"start\":65883},{\"end\":65916,\"start\":65897},{\"end\":65931,\"start\":65916},{\"end\":65943,\"start\":65931},{\"end\":65951,\"start\":65943},{\"end\":65963,\"start\":65951},{\"end\":65979,\"start\":65963},{\"end\":65992,\"start\":65979},{\"end\":66008,\"start\":65992},{\"end\":66426,\"start\":66412},{\"end\":66441,\"start\":66426},{\"end\":66460,\"start\":66441},{\"end\":66480,\"start\":66460},{\"end\":66488,\"start\":66480},{\"end\":66499,\"start\":66488},{\"end\":66516,\"start\":66499},{\"end\":66534,\"start\":66516},{\"end\":66543,\"start\":66534},{\"end\":66554,\"start\":66543},{\"end\":66566,\"start\":66554},{\"end\":66571,\"start\":66566},{\"end\":66942,\"start\":66928},{\"end\":66958,\"start\":66942},{\"end\":66982,\"start\":66958},{\"end\":66997,\"start\":66982},{\"end\":67015,\"start\":66997},{\"end\":67033,\"start\":67015},{\"end\":67046,\"start\":67033},{\"end\":67072,\"start\":67046},{\"end\":67086,\"start\":67072},{\"end\":67098,\"start\":67086},{\"end\":67450,\"start\":67439},{\"end\":67465,\"start\":67450},{\"end\":67476,\"start\":67465},{\"end\":67489,\"start\":67476},{\"end\":67504,\"start\":67489},{\"end\":67514,\"start\":67504},{\"end\":67526,\"start\":67514},{\"end\":67530,\"start\":67526},{\"end\":67540,\"start\":67530},{\"end\":67552,\"start\":67540},{\"end\":67556,\"start\":67552},{\"end\":67881,\"start\":67870},{\"end\":67894,\"start\":67881},{\"end\":67911,\"start\":67894},{\"end\":67926,\"start\":67911},{\"end\":67934,\"start\":67926},{\"end\":67943,\"start\":67934},{\"end\":67955,\"start\":67943},{\"end\":68219,\"start\":68207},{\"end\":68240,\"start\":68219},{\"end\":68253,\"start\":68240},{\"end\":68265,\"start\":68253},{\"end\":68278,\"start\":68265},{\"end\":68300,\"start\":68278},{\"end\":68309,\"start\":68300},{\"end\":68324,\"start\":68309},{\"end\":68330,\"start\":68324},{\"end\":69217,\"start\":69205},{\"end\":69237,\"start\":69217},{\"end\":69254,\"start\":69237},{\"end\":69274,\"start\":69254},{\"end\":69297,\"start\":69274},{\"end\":69311,\"start\":69297},{\"end\":69330,\"start\":69311},{\"end\":69341,\"start\":69330},{\"end\":69349,\"start\":69341},{\"end\":69933,\"start\":69920},{\"end\":69945,\"start\":69933},{\"end\":69957,\"start\":69945},{\"end\":69966,\"start\":69957},{\"end\":69981,\"start\":69966},{\"end\":69993,\"start\":69981},{\"end\":70001,\"start\":69993},{\"end\":70016,\"start\":70001},{\"end\":70669,\"start\":70655},{\"end\":70685,\"start\":70669},{\"end\":70695,\"start\":70685},{\"end\":70716,\"start\":70695},{\"end\":70728,\"start\":70716}]", "bib_venue": "[{\"end\":68645,\"start\":68505},{\"end\":70206,\"start\":70104},{\"end\":65845,\"start\":65753},{\"end\":66410,\"start\":66339},{\"end\":67103,\"start\":67098},{\"end\":67437,\"start\":67389},{\"end\":67868,\"start\":67799},{\"end\":68503,\"start\":68361},{\"end\":69452,\"start\":69383},{\"end\":70102,\"start\":70016},{\"end\":70653,\"start\":70606}]"}}}, "year": 2023, "month": 12, "day": 17}