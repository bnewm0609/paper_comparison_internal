{"id": 235357262, "updated": "2023-04-05 10:00:33.727", "metadata": {"title": "A Generalized Loss Function for Crowd Counting and Localization", "authors": "[{\"first\":\"Jia\",\"last\":\"Wan\",\"middle\":[]},{\"first\":\"Ziquan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Antoni\",\"last\":\"Chan\",\"middle\":[\"B.\"]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 6, "day": 1}, "abstract": "Previous work [40] shows that a better density map representation can improve the performance of crowd counting. In this paper, we investigate learning the density map representation through an unbalanced optimal transport problem, and propose a generalized loss function to learn density maps for crowd counting and localization. We prove that pixel-wise L2 loss and Bayesian loss [29] are special cases and suboptimal solutions to our proposed loss function. A perspective-guided transport cost function is further proposed to better handle the perspective transformation in crowd images. Since the predicted density will be pushed toward annotation positions, the density map prediction will be sparse and can naturally be used for localization. Finally, the proposed loss outperforms other losses on four large-scale datasets for counting, and achieves the best localization performance on NWPU-Crowd and UCF-QNRF.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/WanLC21", "doi": "10.1109/cvpr46437.2021.00201"}}, "content": {"source": {"pdf_hash": "4c398a28d9ed728130256a702f5b70db86a0e867", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "110fecde70b33116317e164300249e4bb181670c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4c398a28d9ed728130256a702f5b70db86a0e867.txt", "contents": "\nA Generalized Loss Function for Crowd Counting and Localization\n\n\nJia Wan \nDepartment of Computer Science\nCity University of Hong Kong\n\n\nZiquan Liu ziquanliu2-c@my.cityu.edu.hk \nDepartment of Computer Science\nCity University of Hong Kong\n\n\nAntoni B Chan abchan@cityu.edu.hk \nDepartment of Computer Science\nCity University of Hong Kong\n\n\nA Generalized Loss Function for Crowd Counting and Localization\n10.1109/CVPR46437.2021.00201\nPrevious work[40]shows that a better density map representation can improve the performance of crowd counting. In this paper, we investigate learning the density map representation through an unbalanced optimal transport problem, and propose a generalized loss function to learn density maps for crowd counting and localization. We prove that pixel-wise L2 loss and Bayesian loss[29]are special cases and suboptimal solutions to our proposed loss function. A perspective-guided transport cost function is further proposed to better handle the perspective transformation in crowd images. Since the predicted density will be pushed toward annotation positions, the density map prediction will be sparse and can naturally be used for localization. Finally, the proposed loss outperforms other losses on four large-scale datasets for counting, and achieves the best localization performance on NWPU-Crowd and UCF-QNRF.\n\nIntroduction\n\nCrowd counting and localization draw increasing attention recently because of its practical usage in surveillance, transport management and business. Most of the algorithms predict a density map from a crowd image, where the summation of the density map is the crowd count [41,4]. A density map (a smooth heat map) is an intermediate representation of the crowd -one popular method to generate the ground-truth density map is to place a Gaussian kernel on each person's dot annotation. The density map estimator is then trained as a standard pixel-wise regression problem using L2 loss [12,40] (see Fig. 1a). In contrast to pixel-wise L2 loss, Bayesian loss (BL) [29] generates an aggregated dot prediction from the density map prediction, and uses a point-wise loss function between the ground-truth dot annotations and the aggregated dot prediction (see Fig. 1b).\n\nBoth L2 and BL assume a fixed ground-truth representation, either Gaussian density kernels for L2 or Gaussian likelihoods for BL. Recent works [40,43] have shown that the intermediate density map representation affects the counting performance, and a better density map representation can be learned in an end-to-end manner from the dot-  [29] computes an aggregated dot prediction and uses a point-wise loss function. We show that L2 and BL are related to an optimal transport problem using a suboptimal transport matrix. (c) Our proposed loss is based on unbalanced optimal transport, where the transport cost is fully minimized and both the pixel-wise and point-wise losses are considered.\n\nannotations. However, [40,43] still use L2 loss for training, which is not appropriate in suppressing background and improving localization. In particular, with L2 loss, a unit change in density in background regions (which is a large localization error) is equivalent to a unit change in density near a dot annotation (which is a small localization error). Thus the L2 loss function is not ideal for localization or generating compact density maps, and we should prefer a loss function that has an increased penalty for errors far from the annotations, so as to improve localization and compactness.\n\nConsidering both motivations of learning the density map representation and using localization-sensitive loss, we propose a generalized loss function based on an unbalanced optimal transport (OT) framework, which measures the transport cost between the predicted density map and the ground-truth dot annotations (see Fig. 1c). We show that the transport matrix, which is optimized to minimize the loss, is related to the intermediate density map representation. To better handle perspective changes in the image, we propose a perspective-guided transport cost function to better separate the density around people who are close together due to the camera perspective. The proposed loss function decomposes into four terms: 1) a transport loss that pushes the predicted density toward annotations; 2) a transport regularization term that prevents collapse onto a single annotation; 3) a pixel-wise loss that measures the difference between the predicted density map and the constructed density map (from the transport matrix); 4) a point-wise loss that ensures that all annotations are accounted for in the predicted density map. We further show that our proposed loss is a generalization of the traditional L2 loss with Gaussian density kernel and BL, i.e., they are special cases and suboptimal solutions to the unbalanced OT in our proposed loss.\n\nCompared to previous losses, our proposed loss function has four advantages: 1) the density map representation is learned via the optimized transport matrix; 2) it does not require any special design for background regions (such as [29,42]), and naturally pushes predicted density away from the background and towards the annotations; 3) it produces compact density maps that can be naturally used for localization; 4) it is less sensitive to the blur factor hyperparameter (which is equivalent to the Gaussian kernel variance). In summary, the contributions of the paper are four-fold:\n\n1. We propose a generalized loss function, motivated by unbalanced optimal transport theory, for crowd counting and localization. We prove L2 and BL are special cases and suboptimal solutions of our loss function. 2. To handle perspective effects in crowd images, we propose a perspective-guided transport cost, which increases transport costs of density far from the camera, thus making densities in those regions more compact. 3. In extensive experiments on crowd counting, using our loss achieves better performance than traditional loss functions on three large-scale datasets, NWPU-Crowd, JHU-CROWD++, and UCF-QNRF. 4. Our low-resolution predicted density maps (1/8 image size) achieve the best localization performance on two large benchmarks NWPU-Crowd and UCF-QNRF.\n\n\nRelated Works\n\nTraditional crowd counting Traditional methods count the number of people in an image by detecting human bodies [18] or body parts [20], which does not work well for images with high crowd density. Thus, direct regression methods are proposed based on low-level features [4,5,12]. Density map based counting Most of recent methods use a deep neural network (DNN) to predict density maps [42] from crowd images, where the sum over density map is the crowd count [19]. The DNN is trained using L2 pixel-wise loss. Various DNN structures are proposed to address scale variation [50,33,15], to refine the predicted density map [30,31,35], and to encode context information [36,47]. To improve the generalization ability, [49] proposes a crossscene crowd counting method. [46] proposes a synthetic dataset and a domain adaptation method to adapt DNNs trained from synthetic data to real images. [7] focuses on semantic consistency across different domains. Since the labeling of crowd images is time-consuming, semi-supervised and weakly-supervised methods are proposed. [26] proposes a ranking loss to utilize unlabeled data, while [38] proposes a Gaussian Process-based iterative model with limited labeled data. [28] proposes to learn generic features with self-training on surrogate tasks. Active learning is also used for crowd counting with limited supervision [51].\n\nLoss functions Although most of the crowd counting methods use L2 norm as the loss function, L2 loss is sensitive to the choice of variance in the Gaussian kernel [40]. Therefore, Bayesian loss (BL) [29] is proposed with pointwise supervision. However, BL cannot well handle false positives in the background, and requires a special design for the background region. [41] proposes a generative model for spatial noise in dot annotations, and derives a novel loss function that is robust to annotation noise. The most related work to ours is the concurrent work of DM-count [44], which considers density maps and dot maps as probability distributions, and uses balanced OT to match the shape of the two distributions. The DM-count loss is composed of three terms: the OT loss, a total variation (TV) pixel-wise loss, and a counting loss. There are four key differences between our work and DM-count. First, DM-count normalizes the density map predictions and the dot map to compute the balanced OT between them. Since normalization removes the actual count in the two maps, an additional counting loss is required to ensure that the count (i.e., the sum of the density map) is predicted correctly. However, this counting loss provides poor supervision, since its gradient adds the same constant value to all pixels (see Supp. A). In contrast, our proposed loss is based on unbalanced OT, which preserves the count of the prediction and GT dot annotations -any mismatch in counts is penalized by our pixel-wise and point-wise loss terms, which give direct pixel-wise supervision on the erroneous predictions. Second, the TV loss used in DM-count is a pixel-wise loss between the normalized density map prediction and the normalized dot map, which is prone to over-fitting especially for the localization task. In contrast, our work contains a pixel-wise loss between the predicted density map and optimized constructed density map (via the transport matrix), which is less prone to over-fit. Third, we show that our loss is a generalization of other loss functions (L2 and BL) when a sub-optimal fixed transport matrix is used. Fourth, DM-count uses the standard squared Euclidean distance as the transport cost, while our work uses a perspective-guided transport cost to increase the separation between people's density in crowded regions, which improves localization. We compare our loss with DM-count in the ablation study.\n\nCrowd Localization To perform counting, density map estimation and localization simultaneously, [13] proposes a composition loss function. [23] proposes to localize crowd locations by a recurrent zooming network, while [22] proposes a detection-based method with RGB-D data. [32] propose to count, localize, and estimate head size simultaneously. [45] proposes a large-scale benchmark for crowd counting and localization. These works use pixel-wise losses, which are sensitive to the kernel bandwidth. In contrast, our loss pushes density towards annotation and is less sensitive to the bandwidth, and thus robust for localization.\n\n\nA Generalized Loss Function for Crowd\n\nCounting and Localization\n\nRecent work [40] shows that learning the intermediate density map representation yields improved performance in counting networks, which suggests the importance of directly using the ground-truth (GT) dot annotations for supervision, rather than a fixed GT density map. However, [40] uses L2 pixel-wise loss for training, which is not appropriate since small changes in background density (which are large localization errors) are equivalent to small changes in density over a person (which are small localization errors). Thus a loss function that penalizes the distance of the error to the annotation is preferred, as in optimal transport (OT) cost between the predicted density map and the dot annotations. Note that the predicted density map and GT dot annotations may not have the same count, due to mis-predictions, or annotation noise (missing/duplicate annotations). Considering these issues, we propose to use unbalanced optimal transport (UOT) as the loss function for training. This loss function both learns the density map representation (uses dot annotations as supervision), and handles count mismatches between the prediction and GT.\n\n\nGeneralized loss function\n\nFormally, let the predicted density map be\nA = {(a i , x i )} n i=1 ,\nwhere a i is the predicted density of pixel x i \u2208 R 2 and n is the number of pixels. We denote a = [a i ] i as the predicted density map. The ground-truth dot map is\nB = {(b j , y j )} m j=1 ,\nwhere y j is the location of the j-th annotation, m is the number of annotation points, and b j is the number of people represented by the annotation. In this paper, we assume\nb = [b j ] j = 1 m .\nOur loss function is based on the entropic-regularized unbalanced optimal transport cost,\nL \u03c4 C (A, B) = min P\u2208R n\u00d7m + C, P \u2212 \u03b5H(P) + \u03c4 D 1 (P1 m |a) + \u03c4 D 2 (P 1 n |b).\n(1)\nC \u2208 R n\u00d7m +\nis the transport cost matrix, whose entry C ij measures the cost of moving the predicted density at x i to GT dot annotation y i via the cost function C ij = c(x i , y j ). P is the transport matrix, which (fractionally) assigns each each location x i from A to y i from B for measuring the cost. The optimal transport cost is obtained by minimizing the loss over P. Note that\u00e2 = P1 m is the construction Figure 2: The relationship between density map, annotations, and transport matrix: (top) optimal transport; (bottom) fixed transport from Bayesian loss [29]. (a) predicted density maps with 81 pixels and 2 annotations; (b) transport matrix P \u2208 R 81\u00d72 , where the arrow length represents the transport value, and the direction points to the assigned annotation; (c) columns of P; (d,e) the transport plan for annotation b 0 and b 1 , reshaped to a map, equivalent to the ground-truth density map for each annotation. The fixed transport (bottom) assigns false positives in the background to annotations, while the optimal transport (top) only considers nearby density. The optimal transport is more sparse, which is better for localization. of a intermediate density map representation from the GT annotations, whileb = P 1 n is the reconstruction of the GT dot annotations. See Fig. 2 (top) for an example.\n\nThe loss function decomposes into four terms. The first term C, P is the transport loss, which encourages prediction of density values near the annotations; it pushes the predicted density towards the annotation during training. The second term H(P) = \u2212 ij P ij log P ij is the entropic regularization term, which favors partial transports between locations, resulting in spread-out (less compact) density maps. Larger values of \u03b5 will yield less compact predicted density maps, and vice-versa. The third term D 1 (P1 m |a) is the pixel-wise loss between the predicted density map a and the constructed intermediate density map representation a = P1 m , i.e., the \"ground-truth\" density map. Finally, the fourth term D 2 (P 1 n |b) is the point-wise loss between the reconstructed annotationsb = P 1 n and the GT annotations. The last two terms are complementary -the pixelwise term D 1 ensures that all predicted density values have a corresponding annotation, while the point-wise term D 2 ensures that all GT annotations are accounted for (used in the transport plan). In other words, any predicted density that is not associated with an annotation is penalized, and any annotation that is not used is penalized.\n\nIn our implementation, we use squared L2 norm for the pixel-wise term and L1-norm for the point-wise term, \nD 1 (P1 m |a) = P1 m \u2212 a 2 2 ,(2)D 2 (P 1 n |b) = P 1 n \u2212 b 1 .(3)\n\nPerspective-Guided Transport Cost\n\nWe next propose a transport cost function for crowd counting. A typical cost function is the squared Euclidean distance between the two points, L 2 ij = x i \u2212 y j ever, due to the perspective effect in crowd images, people that are farther from the camera will appear closer together in the image, while those closer to the camera will be farther apart in the image. In order to keep the density of people in the \"far\" crowds from leaking together, the transport costs for those regions in the image should be higher, which will make the density for those people more compact.\n\nTo encode perspective information in crowd images, a perspective-guided cost function is proposed to have larger penalty for the transport of density far from the camera. Formally, the cost function is defined as:\nC ij = exp( 1 \u03b7(xi,yj ) x i \u2212 y j 2 ),(4)\nwhere \u03b7(x i , y j ) is an adaptive perspective factor, which is mapped between an interval based on the average height\n1 2 (h xi +h yj ),\nwhere h x \u2208 [0, 1] refers to normalized height of the pixel x in the image. We use the exponential in (4) to enhance the cost of moving densities over long distances, which makes the predicted density maps more compact.\n\n\nOptimization of transport matrix\n\nAs shown in [1], the solution to (1) for the optimization of transport matrix P is unique, and has the form\nP = diag(u)Kdiag(v), K = exp(\u2212C/\u03b5),(5)\nwhere K is the Gibbs kernel constructed from the cost matrix C, and exp is element-wise exponential. For the optimization of P in (1), we approximate D 1 and D 2 with KL divergence, since this yields an efficient algorithm. 1 The u, v are computed with the generalized Sinkhorn iterations, (6) where the division and exponent operations are elementwise. To compute network gradients, the optimal v * is considered as a constant, and u * is a function of a, i.e., P = diag(u * (a))Kdiag(v * ).\nu ( +1) = a Kv ( ) \u03c4 \u03c4 + , v ( +1) = b K u ( +1) \u03c4 \u03c4 + ,\n\nDensity map counting and localization\n\nTo apply our loss function to density map counting, we learn a density map estimator f (I), whose input is the image I, and output is the density map vector a. The predicted density map A, together with the corresponding GT annotations B, are fed into the loss function in (1). To compute the loss, the transport matrix P is optimized for each input separately using (5) and the iterations in (6). Given the test image, the density map estimator predicts the density map, which is then summed to obtain the count.\n\nWe perform localization by applying simple postprocessing to the predicted density map a. First, a is upsampled to the image size since the density map is 1 8 of the input image size (due to pooling operations). Then, a pixel is considered a candidate for a predicted location if its value is the local maximum in a 3\u00d73 window centered on the pixel. Finally, the candidates with density larger than 0.05 are the final location predictions.\n\n\nRelationship with traditional losses\n\nIn this section, we prove that the traditional L2 loss and Bayesian loss (BL) [29] are suboptimal solutions to the unbalanced OT in our loss function in (1). In particular, L2 and BL are both 2-stage approximations to solve (1), consisting of: 1) constructing a half-iteration approximate solution of the transport matrix P using entropic-regularized balanced OT with squared Euclidean transport cost; 2) substituting the approximate P into our loss in (1). Because the computed P is a half-iteration approximation to the minimization in (1), both L2 and BL are suboptimal approximations of our loss function.\n\n\nHalf-iteration approximations for P\n\nWe first derive closed-form solutions to approximate P under the entropic-regularized balanced OT problem. Removing the last two terms in (1), we obtain the entropic regularized OT problem,\nL \u03b5 C (A, B) = min P\u2208R n\u00d7m + C, P \u2212 \u03b5H(P),(7)\nwhere C, A, B are defined as before. As shown in [1], the solution to (7) is unique with P\n= [P ij ] ij , P ij = u i K ij v j , K ij = exp(\u2212C ij /\u03b5),(8)where u = [u i ] i , v = [v j ] j are from the Sinkhorn iterations, u ( +1) = a Kv ( ) , v ( +1) = b K u ( +1) ,(9)\nwhere the division is element-wise. Typically, the iterations are initialized with v = 1 m . We next obtain 2 approximate solutions to P, by substituting a half Sinkhorn iteration, u ( +1) or v ( +1) , into (8),\nP ij = a Kv i K ij v j ,P ij = u i K ij b K u j .(10)\nIf v is uniform (as in the typical initialization) and the cost function C ij is the squared Euclidean distance, then\nP ij = Kij m j=1 Kij a i =\u03c0 ij a i ,\u03c0 ij = exp(\u2212 xi\u2212yj 2 /\u03b5) m j=1 exp(\u2212 xi\u2212yj 2 /\u03b5) .\nSimilarly, assuming u is initialized as uniform and C ij is the squared Euclidean distance, then forP ij we hav\u1ebd\nP ij = Kij n i=1 Kij b j =\u03c0 ij ,\u03c0 ij = exp(\u2212 xi\u2212yj 2 /\u03b5) n i=1 exp(\u2212 xi\u2212yj 2 /\u03b5) , since b j = 1.\nNote that the difference between\u03c0 ij and\u03c0 ij is the summation in the denominator is either over GT annotation locations y j or density map pixels x i , respectively.\n\n\nRelationship with L2 Loss\n\nWe now derive the L2 loss as a special case of our loss function when using the suboptimal transport matrixP. Substituting into (1), we note that the first 2 terms, C,P and H(P) are constants w.r.t. a, and thus do not affect the loss in terms of a. Next, it is straightforward to show thatP 1 n = 1 m , and therefore the fourth term in (1) is D 2 (P 1 n |b) = 0. Only the third term (i.e, the pixel-wise loss) remains, and assuming \u03c4 = 1, we have the loss\nL(A, B) = D 1 (P1 m |a) = n i=1 (a i \u2212\u00e3 i ) 2 ,(11)a i = [P1 m ] i = m j=1\u03c0 ij = m j=1 exp(\u2212 xi\u2212yj 2 /\u03b5) n i=1 exp(\u2212 xi\u2212yj 2 /\u03b5) .(12)\nNote that\u00e3 i is equivalent to a \"ground-truth\" density map value at pixel location i, which places a Gaussian kernel with squared-bandwidth /2 at each annotation y j . The denominators in (12) are the normalization constants of each Gaussian. Thus from (11) and (12), our loss using the approximate transport matrixP is equivalent to L2 loss with traditional Gaussian-based density maps for supervision.\n\n\nRelationship with Bayesian Loss\n\nWe next derive BL [29] as a special case of our loss using approximationP. Note that\u03c0 ij is the probability of assigning the density value of the i-th pixel to the j-th annotation point, as defined in [29]\n. Since [P1 m ] i = m j=1\u03c0 ij a i = a i , then the third term in (1) is D 1 (P1 m |a) = 0.\nAssuming that is small (so that the entropy term can be ignored) and \u03c4 = 1, we have the los\u015d L(A, B) = C,P + D 2 (P 1 n |b)\n= n i=1 \u03c9 i a i + m j=1 |1 \u2212 n i=1\u03c0 ij a i |,(13)\nwhere\n\u03c9 i = m j=1 C ij\u03c0ij = j ||x i \u2212 y j || 2\u03c0\nij is a weight on the prediction a i . The second term in (14) is exactly the point-wise Bayesian loss defined in [29]. The first term in (14) can be interpreted as a background loss, which penalizes non-zero values of a i for pixels x i far from any annotation (i.e., false positives). In particular, the weight \u03c9 i on the i-th pixel is based on a weighted average of squared distances from the pixel to the annotations.\n\nWe now relate the background term in (14) with the background model used in BL [29]. The background loss in [29] is based on the nearest annotation to each point x i (details in Supp. B),\nL BG = |0 \u2212 n i=1\u03c9 i a i | = n i=1\u03c9 i a i ,(15)\nwhere the weight\u03c9 i =k \u012b ki+ m j=1 Kij andk i = exp(\u2212(d \u2212 ||x i \u2212 y \u03b7(i) ||) 2 /\u03b5), and \u03b7(i) is the index of the annotation nearest to x i . The weight can be rewritten as\u03c9 i = exp( 2d\n\u03b5 ||x i \u2212 y \u03b7(i) || \u2212 d 2 \u03b5 )\u03c0 i , where\u03c0 i = K i,\u03b7(i) ki+ m j=1\nKij is the weight contribution for the nearest neighbor \u03b7(i). Note that (15) and the first term in (14) have the same form, but use different weight values\u03c9 i or \u03c9 i . For BL, the weight\u03c9 i Figure 3: Comparison of background weight maps for Bayesian Loss (\u03c9) and approximate UOT (\u03c9) in (14).\n\nis based on the exponential distance to the nearest annotation \u03b7(i). In contrast, for the background term in (14), the weight \u03c9 i is based on a weighted average of squared distances to all annotations. Therefore, the background model used in [29] is a special case of the background term in (14). Fig. 3 shows a visualization of the weights maps for\u03c9 i and \u03c9 i . Both BL and (14) have large weights for background regions and small weights for head (annotation) regions. However, the weight map for (14) is smoother since all annotations are considered using squared distances, while the weight map for BL contains flat regions since it uses the exponential distance to only the nearest annotation.\n\nThus, from (14), BL with background model is a special case of our proposed loss in (1), where the approximate transport matrix isP and a single-neighbor approximation of the cost matrix C is used to compute the cost term (i.e.,. the background loss). If no background model is used, then the cost matrix is assumed to be 0.\n\n\nExperiments\n\nIn this section, we evaluate the counting and localization performance using the proposed general loss function.\n\n\nExperimental setups\n\nDatasets: We evaluate the performance of the proposed loss function on four datasets: ShanghaiTech [50], UCF-QNRF [13], JHU-CROWD++ [39], and NWPU-Crowd [45]. ShanghaiTech contains two parts: Part A (482/300 for training/testing) and Part B (716/400 for training/testing). UCF-QNRF is a large-scale dataset consists of 1,535 highresolution crowd images (1,201/334 for training/testing). JHU-CROWD++ contains 4,317 images (2,722/500/1,600 images are for training/validation/testing). NWPU-Crowd is the largest dataset with 3,109 training images, 500 validation images, and 1,500 testing images (whose labels are not release to public for fair comparison). We report results on the NWPU-Crowd test set.\n\nEvaluation metrics: Mean absolute error (MAE) and root mean squared error (MSE) are used as the evaluation metric for counting performance, as in previous works [16]:\nMAE = 1 N i |y i \u2212\u0177 i |, MSE = ( 1 N i (y i \u2212\u0177 i ) 2 ) 1/2 ,\nwhere y i ,\u0177 i are the GT and predicted counts. To evaluate the localization performance, we followed the protocols used in NWPU-Crowd and UCF-QNRF, respectively. For NWPU-Crowd, Precision, Recall and F-measure are used, and Precision, Recall and AUC are used for UCF-QNRF. Backbone and training: Following the experiment settings in [41], we use 3 backbone networks: VGG19 [29], CSRNet [21], and MCNN [50]. We train the counting network using our loss function in (1), where P is solved using the generalized Sinkhorn iterations in (6). We set \u03b5 = 0.005. In practice, \u03b5-scaling heuristic is used for acceleration, which needs less than 20 iterations until converge, and the computation is calculated in log-domain for numerical stability as in [6]. In preliminary studies using the exponential transport cost, we observe that \u03b7 \u2208 {0.6, 0.8} yield better performance (Fig. 4a). Thus for the perspective-guided cost, we simply map the range of image pixel y-coordinates to \u03b7 \u2208 [0.6, 0.8]. VGG19 and CSRNet are pre-trained on ImageNet, and MCNN is trained from scratch. Adam optimizer [17] is used to train the networks with learning rate 10 \u22125 for VGG19/CSRNet, and 10 \u22124 for MCNN.\n\n\nAblation studies\n\nWe first conduct ablation studies on our loss function on UCF-QNRF or JHU-CROWD++.\n\n\nComparison with different losses\n\nIn Table 1, we compare the performance of loss functions with different backbones, including L2 pixel-wise loss, Bayesian loss (BL) [29], NoiseCC [44], which models noisy annotations, and DM-count [44], which uses balanced OT as part of their loss. Our proposed loss function achieves the lowest MAE among all loss functions. Our loss function outperforms L2 and BL since we use an optimal transport plan, instead of fixed as shown in Sec. 4, and a better transport plan (i.e., density map) can achieve better performance as shown in [43]. Compared to DM-count, our loss function achieves better performance especially for MCNN trained from scratch. Our loss function is based on unbalanced OT using exponential transport cost, while DM-count is based on balanced OT and squared-Euclidean cost. We further compare the unbalanced/balanced OT frameworks and transport cost functions in the next 2 ablation studies.\n\n\nThe effect of transport cost functions\n\nWe evaluate the effectiveness of the proposed perspectiveguided transport cost by comparing with other cost functions, including Euclidean distance (L ij ), squared Euclidean (L 2 ij ), and exponential of Euclidean (e Lij ). The test results are shown in Fig. 4a. First, the standard cost function based on Euclidean distance is less effective than the exponential cost function. The perspective-guided cost achieves the best performance, which confirms that adapting the cost function to the perspective changes is effective for crowd counting. We visualize the density maps predicted with different cost functions in Fig. 6. Using exponential cost yields a more compact density map compared to the squared Euclidean cost. Furthermore, using perspectiveguided cost yields more sparsity for high-density regions, which demonstrates that its efficacy at pushing away density from background to annotations. Finally, TV loss used in [44] assumes the same smoothness for all annotations, which is incompatible with the perspective-guided (PG) cost that produces different smoothness for each annotation. To confirm this, we conduct an experiment by decreasing the weight of TV loss by 10x, and the performance using PG cost improved to MAE 66, which is still worse than using exponential cost with fixed \u03b7 = 0.8 (MAE 64). Thus, TV loss hinders the PG transport cost (with adaptive smoothness), but works with the exponential cost with fixed \u03b7 (i.e., fixed smoothness).\n\n\nThe effect of unbalanced/balanced OT\n\nWe next compare our unbalanced OT framework with the balanced OT of DM-count [44], using the same transport cost functions in Sec. 5.2.2. As seen in Fig. 4a, our proposed loss outperforms DM-Count when using different cost functions, which demonstrates the efficacy of unbalanced OT for the density map regression problem. Our proposed loss is based on the unbalanced OT problem, where extra/missing density is penalized using both point-wise and pixel-wise losses. In contrast, DM-count uses balanced OT, and requires an additional count loss, which is a map-wise loss that is less effective (see discussion in Sec. 2 \"loss functions\"). Second, the TV loss in DM-Count uses the normalized dot map for pixel-wise supervision, which is prone to overfitting. Finally, our proposed cost is more effective at pushing away density from background to annotations compare to squared Euclidean cost (see Sec. 5.2.2).\n\n\nThe effect of \u03b5\n\nWe next investigate the effect of blur factor \u03b5, which is equivalent to the Gaussian squared-bandwidth (variance) used to generate the ground-truth density maps for L2 and BL, as shown in Sec. 4.2. The results for varying \u03b5 are shown in Fig. 4b. The L2 loss is sensitive to \u03b5, with MAE increasing significantly as \u03b5 increases. In contrast, BL and our proposed loss are less sensitive, since the background model in BL and the transport loss in ours can push density towards annotations, always making the predicted density maps compact. The proposed loss function is generally better than BL because the BL background model only consid-  ers the nearest annotation, while our loss considers all annotations (see Sec. 4.3). A visualization is shown in Fig. 5. As the \u03b5 increases, the density map for L2 become more blurry and inaccurate, which demonstrates that L2 is sensitive to \u03b5. BL and our loss are generally robust to the choice of \u03b5, and the network can learn a sharper density map with the proposed loss function, which is better for localization.\n\n\nThe effect of \u03c4 and divergence function\n\nNext, we study the effect of \u03c4 and the divergence function for point-wise and pixel-wise cost functions. We try different combinations of L1 and L2 norms with \u03c4 = 0.5, and the results are presented in Fig. 4d. The best performance occurs with point-wise L1 and pixel-wise L2, which matches the common practice for the individual point-wise and pixel-wise-based losses [29,42]. Next, using L1 and L2 for the point-wise and pixel-wise costs, we vary \u03c4 (see Fig. 4c), and visualize the learned density maps in Fig. 6 As \u03c4 decreases, the density maps becomes more compact (more sparse), since the transport cost dominates and  pushes the density more towards the annotations.\n\n\nThe effect of terms in the loss function\n\nFinally, we evaluate the effect of different terms in the proposed loss function in Table 2. The most important term is entropic regularization, which controls the smoothness of the prediction to prevent over-fitting. Unbalanced OT (UOT) with either pixel-wise loss (removing D 2 ) or pointwise loss (removing D 1 ) can be effective, and is better than the corresponding approximations BL and L2 loss (85.4 vs 88.8; 85.0 vs. 98.7), which shows the effectiveness of using a better transport solution. Finally, UOT with both pixelwise and point-wise losses further improves the model.\n\n\nComparison with state-of-the-arts\n\nTo evaluate the overall counting performance, we compare VGG19 [29] trained with our loss function with stateof-the-art methods in Table 3. First, compared with the baseline method BL and DM-Count, our method achieves significantly better performance especially for the largescale datasets NWPU-Crowd, JHU-CROWD++, and UCF-QNRF. Second, our model achieves the best MAE on the   3 largest datasets and competitive performance on Shang-haiTech, without any special design to extract multi-scale features or to handle noisy annotations. The experiment confirms the effectiveness of the proposed loss function.\n\n\nLocalization\n\nFinally, since our loss function trains the model to predict compact density maps that are suitable for localization, we evaluate the localization performance on NWPU-Crowd and UCF-QNRF. We compare against other state-of-theart that have reported results on localization, and the results are presented in Tables 4 and 5. On NWPU-Crowd, our loss achieves the overall best performance as quantified by F-measure. Faster RCNN, a detector-based approach, has the highest precision but lowest recall, which shows that it cannot handle the small objects far from the camera. In contrast, TinyFace has the highest recall, but the lowest precision, showing that it has many false-positives. Our loss yields a more balanced localization result, obtaining the best F-measure, and the 2nd best precision and recall. RAZNet also achieves better performance than the detection-based methods, via its recurrent zooming mech-anism for handling small objects. However, our loss outperforms RAZNet, without any special design for predicting high-resolution density maps or zooming mechanism. One localization example from the NWPU-Crowd test set is shown in Fig. 7.\n\nOn UCF-QNRF, the proposed loss outperforms other loss functions including composition loss (CL), which is designed for localization. Our loss also outperforms its baselines L2 and BL, showing the efficacy of the proposed loss over the purely pixel-wise and point-wise losses. The experiment demonstrates that the proposed loss can be naturally used for localization, since the density is encouraged to be compact around the annotations during optimization with transport loss and exponential cost.\n\nIn Supp. D, we show a comparison of the localization results for different loss functions. For L2 loss, many false negatives appear in dense regions, and the recall is the worst, which shows that L2 loss cannot handle small objects far from the camera. BL and DM-Count have better recall, but BL has many false positives in high-density regions, and DM-Count has many false positives even in low-density regions. Our proposed loss achieves both high precision and recall, yielding the best F-measure. \n\n\nConclusion\n\nIn this paper, we propose a generalized loss function for learning density maps for crowd counting and localization, which is based on unbalanced optimal transport. We prove that traditional L2 and Bayesian loss are special cases and suboptimal solutions of our loss function. A perspective-guided cost function is proposed to handle perspective transformation in crowd images. We then conduct extensive experiments and achieve superior performance on large-scale datasets. Finally, we apply the proposed loss function to crowd localization and achieve the best performance without any special design of the architecture.\n\nFigure 1 :\n1Loss functions for counting: (a) L2 loss generates density map as the supervision and uses a pixel-wise loss function. (b) Bayesian loss (BL)\n\nFigure 4 :\n4Ablation study: (a) Comparison of different transport cost functions on JHU-CROWD++, using our approach and DM-count[44]. The base cost is Euclidean distanceL ij = x i \u2212 y j 2 . (b)The effect of \u03b5 for different loss functions. \u03b5 is the Gaussian bandwidth for density maps when using L2 and BL, or the blur factor for our unbalanced OT. (c) The effect of \u03c4 . (d) Comparison of divergences for point-wise and pixel-wise costs on UCF-QNRF.\n\nFigure 5 :\n5Visualization of density maps predicted from models trained with different loss functions and blur factors \u03b5. Note that the width and height of the trained images patches are normalized to 1. The sparsity is defined as the percentage of pixels with density less than 0.001, and the most sparse density map is shown in red bold.\n\nFigure 6 :\n6Visualization of the effect of (top) transport cost functions, and (bottom) \u03c4 . The sparsity is defined as the percentage of pixels with density less than 0.001.\n\nFigure 7 :\n7Example localization result on NWPU-Crowd test set.\n\n\nIn Sec. 4, we show that L2 and BL are suboptimal solutions to our proposed generalized loss function, which use a fixed intermediate representation (i.e., transport matrix).\n\nTable 1 :\n1Test results comparing different loss functions with different back-bones on UCF-QNRF. \nVGG19 [29] \nCSRNet [21] \nMCNN [50] \nMAE MSE MAE MSE MAE MSE \nL2 \n98.7 \n176.1 110.6 190.1 186.4 283.6 \nBL [29] \n88.8 \n154.8 107.5 184.3 190.6 272.3 \nNoiseCC [41] \n85.8 \n150.6 96.5 \n163.3 177.4 259.0 \nDM-count [44] 85.6 \n148.3 103.6 180.6 176.1 263.3 \nOurs \n84.3 \n147.5 92.0 \n165.7 142.8 227.9 \n\n\n\nTable 2 :\n2Effectiveness of terms in the loss function on UCF-QNRF.Component \nCombinations \nC, P \nH(P) \nD 1 (P1m|a) \nD 2 (P 1n|b) \nMAE \n91.1 85.4 85.0 84.3 \n\n\n\nTable 3 :\n3Comparison with state-of-the-art crowd counting methods.NWPU \nJHU++ \nUCF-QNRF \nShTech A \nShTech B \n\nMAE MSE MAE MSE MAE MSE MAE MSE MAE MSE \nMCNN [50] \nCVPR'16 232.5 714.6 188.9 483.4 277.0 426.0 110.2 173.2 26.4 41.3 \nSwitchCNN [33] CVPR'17 \n-\n-\n-\n-228.0 445.0 90.4 135.0 21.6 33.4 \nCP-CNN [36] \n\nICCV'17 \n\n-\n-\n-\n-\n-\n-73.6 106.4 20.1 30.1 \nACSCP [34] \n\nCVPR'18 \n\n-\n-\n-\n-\n-\n-75.7 102.7 17.2 27.4 \nCSRNet [21] \nCVPR'18 121.3 387.8 85.9 309.2 110.6 190.1 68.2 115.0 10.6 16.0 \nCL [13] \n\nECCV'18 \n\n-\n-\n-\n-132.0 191.0 \n-\n-\n-\n-\nSANet [3] \nECCV'18 190.6 491.4 91.1 320.4 \n-\n-67.0 104.5 \n8.4 13.6 \nDSSINet [25] \n\nICCV'19 \n\n-\n-133.5 416.5 99.1 159.2 60.6 96.0 \n6.8 10.3 \nMBTTBF [37] ICCV'19 \n-\n-81.8 299.1 97.5 165.2 60.2 94.1 \n8.0 15.5 \nBL [29] \nICCV'19 105.4 454.2 75.0 299.9 88.7 154.8 62.8 101.8 \n7.7 12.7 \nLSCCNN [32] TPAMI'20 \n-\n-112.7 454.4 120.5 218.2 66.5 101.8 \n7.7 12.7 \nKDMG [43] \nTPAMI'20 100.5 415.5 69.7 268.3 99.5 173.0 63.8 99.2 \n7.8 12.7 \nRPNet [48] \n\nCVPR'20 \n\n-\n-\n-\n-\n-\n-61.2 96.9 \n8.1 11.6 \nASNet [14] \n\nCVPR'20 \n\n-\n-\n-\n-91.6 159.7 57.8 90.1 \n-\n-\nAMSNet [10] \n\nECCV'20 \n\n-\n-\n-\n-101.8 163.2 56.7 93.4 \n6.7 10.2 \nAMRNet [27] ECCV'20 \n-\n-\n-\n-86.6 152.2 61.6 98.4 \n7.0 11.0 \nLibraNet [24] \n\nECCV'20 \n\n-\n-\n-\n-88.1 143.7 55.9 97.1 \n7.3 11.3 \nDM-count [44] NeurIPS'20 88.4 357.6 68.4 283.3 85.6 148.3 59.7 95.7 \n7.4 11.8 \nNoiseCC [41] NeurIPS'20 96.9 534.2 67.7 258.5 85.8 150.6 61.9 99.6 \n7.4 11.3 \nOurs \n79.3 346.1 59.9 259.5 84.3 147.5 61.3 95.4 \n7.3 11.7 \n\n\n\nTable 4 :\n4Localization performance on NWPU-Crowd dataset.Precision Recall F-measure \nFaster RCNN [25] 0.958 \n0.035 \n0.068 \nTinyFace [9] \n0.529 \n0.611 \n0.567 \nVGG+GPR \n0.558 \n0.496 \n0.525 \nRAZNet [23] \n0.666 \n0.543 \n0.599 \nours \n0.800 \n0.562 \n0.660 \n\n\n\nTable 5 :\n5Localization performance on UCF-QNRF dataset.Precision Recall AUC \nMCNN [50] \n0.599 \n0.635 \n0.591 \nResNet [8] \n0.616 \n0.669 \n0.612 \nDenseNet [11] \n0.702 \n0.581 \n0.637 \nEncoder-Decoder [2] 0.718 \n0.630 \n0.670 \nCL [13] \n0.758 \n0.598 \n0.714 \nDM-Count [44] \n0.731 \n0.638 \n0.692 \nVGG19+L2 \n0.605 \n0.670 \n0.623 \nVGG19+BL [29] \n0.767 \n0.654 \n0.720 \nVGG19+ours \n0.782 \n0.748 \n0.763 \n\n\n2 , which considers all distances equally throughout the image. How-\nSolving for P using D 1 and D 2 in(2)and(3)requires an inefficient nested optimization.\nAcknowledgments. This work was supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CityU 11212518).\nComputational optimal transport. Foundations and Trends in Machine Learning. 11Computational optimal transport. Foundations and Trends in Machine Learning, 11(5-6):355-607, 2019. 4\n\nSegnet: A deep convolutional encoder-decoder architecture for image segmentation. Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3912Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE Transactions on Pattern Anal- ysis and Machine Intelligence, 39(12):2481-2495, 2017. 8\n\nScale aggregation network for accurate and efficient crowd counting. Xinkun Cao, Zhipeng Wang, Yanyun Zhao, Fei Su, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale aggregation network for accurate and efficient crowd count- ing. In Proceedings of the European Conference on Com- puter Vision (ECCV), pages 734-750, 2018. 8\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. B Antoni, Zhang-Sheng John Chan, Nuno Liang, Vasconcelos, IEEE Conference on Computer Vision and Pattern Recognition. 1Antoni B Chan, Zhang-Sheng John Liang, and Nuno Vas- concelos. Privacy preserving crowd monitoring: Counting people without people models or tracking. In IEEE Con- ference on Computer Vision and Pattern Recognition, pages 1-7, 2008. 1, 2\n\nBayesian poisson regression for crowd counting. B Antoni, Nuno Chan, Vasconcelos, International Conference on Computer Vision. Antoni B Chan and Nuno Vasconcelos. Bayesian poisson regression for crowd counting. In International Conference on Computer Vision, pages 545-551, 2009. 2\n\nInterpolating between optimal transport and mmd using sinkhorn divergences. Jean Feydy, Thibault S\u00e9journ\u00e9, Fran\u00e7ois-Xavier Vialard, Alain Shun-Ichi Amari, Gabriel Trouv\u00e9, Peyr\u00e9, The 22nd International Conference on Artificial Intelligence and Statistics. Jean Feydy, Thibault S\u00e9journ\u00e9, Fran\u00e7ois-Xavier Vialard, Shun-ichi Amari, Alain Trouv\u00e9, and Gabriel Peyr\u00e9. Inter- polating between optimal transport and mmd using sinkhorn divergences. In The 22nd International Conference on Ar- tificial Intelligence and Statistics, pages 2681-2690, 2019. 6\n\nFocus on semantic consistency for cross-domain crowd understanding. Tao Han, Junyu Gao, Yuan Yuan, Qi Wang, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Tao Han, Junyu Gao, Yuan Yuan, and Qi Wang. Focus on semantic consistency for cross-domain crowd understand- ing. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1848-1852. IEEE, 2020. 2\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 8\n\nFinding tiny faces. Peiyun Hu, Deva Ramanan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionPeiyun Hu and Deva Ramanan. Finding tiny faces. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition, pages 951-959, 2017. 8\n\nNas-count: Counting-by-density with neural architecture search. Yutao Hu, Xiaolong Jiang, Xuhui Liu, Baochang Zhang, Jungong Han, Xianbin Cao, David Doermann, arXiv:2003.00217arXiv preprintYutao Hu, Xiaolong Jiang, Xuhui Liu, Baochang Zhang, Jun- gong Han, Xianbin Cao, and David Doermann. Nas-count: Counting-by-density with neural architecture search. arXiv preprint arXiv:2003.00217, 2020. 8\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil- ian Q Weinberger. Densely connected convolutional net- works. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700-4708, 2017. 8\n\nMulti-source multi-scale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, IEEE Conference on Computer Vision and Pattern Recognition. 1H. Idrees, I. Saleemi, C. Seibert, and M. Shah. Multi-source multi-scale counting in extremely dense crowd images. In IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 2547-2554, 2013. 1, 2\n\nComposition loss for counting, density map estimation and localization in dense crowds. Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, Mubarak Shah, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak Shah. Composition loss for counting, density map estimation and localization in dense crowds. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV), pages 532- 546, 2018. 2, 5, 8\n\nAttention scaling for crowd counting. Xiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu Zhang, Pei Lv, Bing Zhou, Xin Yang, Yanwei Pang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu Zhang, Pei Lv, Bing Zhou, Xin Yang, and Yanwei Pang. Attention scaling for crowd counting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4706-4715, 2020. 8\n\nCrowd counting by adaptively fusing predictions from an image pyramid. Di Kang, Antoni B Chan, British Machine Vision Conference. 89Di Kang and Antoni B. Chan. Crowd counting by adaptively fusing predictions from an image pyramid. In British Ma- chine Vision Conference, page 89, 2018. 2\n\nBeyond counting: comparisons of density maps for crowd analysis taskscounting, detection, and tracking. Di Kang, Zheng Ma, Antoni B Chan, IEEE Transactions on Circuits and Systems for Video Technology. 5Di Kang, Zheng Ma, and Antoni B Chan. Beyond count- ing: comparisons of density maps for crowd analysis tasks- counting, detection, and tracking. IEEE Transactions on Cir- cuits and Systems for Video Technology, 2018. 5\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6\n\nPedestrian detection in crowded scenes. Bastian Leibe, Edgar Seemann, Bernt Schiele, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 1Bastian Leibe, Edgar Seemann, and Bernt Schiele. Pedes- trian detection in crowded scenes. In IEEE Computer Soci- ety Conference on Computer Vision and Pattern Recognition, volume 1, pages 878-885, 2005. 2\n\nLearning to count objects in images. Victor Lempitsky, Andrew Zisserman, Advances in neural information processing systems. Victor Lempitsky and Andrew Zisserman. Learning to count objects in images. In Advances in neural information pro- cessing systems, pages 1324-1332, 2010. 2\n\nEstimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. Min Li, Zhaoxiang Zhang, Kaiqi Huang, Tieniu Tan, International Conference on Pattern Recognition. Min Li, Zhaoxiang Zhang, Kaiqi Huang, and Tieniu Tan. Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. In International Conference on Pattern Recognition, pages 1-4, 2008. 2\n\nCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes. Yuhong Li, Xiaofan Zhang, Deming Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition6Yuhong Li, Xiaofan Zhang, and Deming Chen. Csrnet: Di- lated convolutional neural networks for understanding the highly congested scenes. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 1091-1100, 2018. 6, 8\n\nDensity map regression guided detection network for rgb-d crowd counting and localization. Dongze Lian, Jing Li, Jia Zheng, Weixin Luo, Shenghua Gao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDongze Lian, Jing Li, Jia Zheng, Weixin Luo, and Shenghua Gao. Density map regression guided detection network for rgb-d crowd counting and localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 1821-1830, 2019. 3\n\nRecurrent attentive zooming for joint crowd counting and precise localization. Chenchen Liu, Xinyu Weng, Yadong Mu, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2Chenchen Liu, Xinyu Weng, and Yadong Mu. Recurrent at- tentive zooming for joint crowd counting and precise local- ization. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1217-1226. IEEE, 2019. 2, 8\n\nWeighing counts: Sequential crowd counting by reinforcement learning. Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, Chunhua Shen, arXiv:2007.08260arXiv preprintLiang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, and Chunhua Shen. Weighing counts: Sequential crowd counting by reinforcement learning. arXiv preprint arXiv:2007.08260, 2020. 8\n\nCrowd counting with deep structured scale integration network. Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, Liang Lin, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, and Liang Lin. Crowd counting with deep struc- tured scale integration network. In Proceedings of the IEEE International Conference on Computer Vision, pages 1774- 1783, 2019. 8\n\nLeveraging unlabeled data for crowd counting by learning to rank. Xialei Liu, Joost Van De Weijer, Andrew D Bagdanov, IEEE Conference on Computer Vision and Pattern Recognition. Xialei Liu, Joost van de Weijer, and Andrew D Bagdanov. Leveraging unlabeled data for crowd counting by learning to rank. In IEEE Conference on Computer Vision and Pattern Recognition, 2018. 2\n\nAdaptive mixture regression network with local counting map for crowd counting. Xiyang Liu, Jie Yang, Wenrui Ding, arXiv:2005.05776arXiv preprintXiyang Liu, Jie Yang, and Wenrui Ding. Adaptive mixture regression network with local counting map for crowd count- ing. arXiv preprint arXiv:2005.05776, 2020. 8\n\nSemi-supervised crowd counting via self-training on surrogate tasks. Yan Liu, Lingqiao Liu, Peng Wang, Pingping Zhang, Yinjie Lei, arXiv:2007.03207arXiv preprintYan Liu, Lingqiao Liu, Peng Wang, Pingping Zhang, and Yinjie Lei. Semi-supervised crowd counting via self-training on surrogate tasks. arXiv preprint arXiv:2007.03207, 2020. 2\n\nBayesian loss for crowd count estimation with point supervision. Zhiheng Ma, Xing Wei, Xiaopeng Hong, Yihong Gong, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision7Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervi- sion. In Proceedings of the IEEE International Conference on Computer Vision, pages 6142-6151, 2019. 1, 2, 3, 4, 5, 6, 7, 8\n\nIterative crowd counting. Hieu Viresh Ranjan, Minh Le, Hoai, European Conference on Computer Vision. Viresh Ranjan, Hieu Le, and Minh Hoai. Iterative crowd counting. In European Conference on Computer Vision, pages 278-293, 2018. 2\n\nTop-down feedback for crowd counting convolutional neural network. Deepak Babu Sam, R Venkatesh, Babu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceDeepak Babu Sam and R. Venkatesh Babu. Top-down feed- back for crowd counting convolutional neural network. In Proceedings of the AAAI Conference on Artificial Intelli- gence, pages 7323-7330, 2018. 2\n\nAmogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. Skand Deepak Babu Sam, Mukuntha Vishwanath Peri, Narayanan Sundararaman, IEEE Transactions on Pattern Analysis and Machine Intelligence. 38Deepak Babu Sam, Skand Vishwanath Peri, Mukun- tha Narayanan Sundararaman, Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 2020. 3, 8\n\nSwitching convolutional neural network for crowd counting. Shiv Deepak Babu Sam, R Surya, Venkatesh, Babu, IEEE Conference on Computer Vision and Pattern Recognition. 2Deepak Babu Sam, Shiv Surya, and R. Venkatesh Babu. Switching convolutional neural network for crowd counting. In IEEE Conference on Computer Vision and Pattern Recog- nition, pages 4031-4039, 2017. 2, 8\n\nCrowd counting via adversarial cross-scale consistency pursuit. Zan Shen, Yi Xu, Bingbing Ni, Minsi Wang, Jianguo Hu, Xiaokang Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZan Shen, Yi Xu, Bingbing Ni, Minsi Wang, Jianguo Hu, and Xiaokang Yang. Crowd counting via adversarial cross-scale consistency pursuit. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5245- 5254, 2018. 8\n\nCrowd counting with deep negative correlation learning. Zenglin Shi, Le Zhang, Yun Liu, Xiaofeng Cao, Yangdong Ye, Ming-Ming Cheng, Guoyan Zheng, IEEE Conference on Computer Vision and Pattern Recognition. Zenglin Shi, Le Zhang, Yun Liu, Xiaofeng Cao, Yangdong Ye, Ming-Ming Cheng, and Guoyan Zheng. Crowd counting with deep negative correlation learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5382- 5390, 2018. 2\n\nGenerating high-quality crowd density maps using contextual pyramid cnns. A Vishwanath, Sindagi, M Vishal, Patel, IEEE International Conference on Computer Vision. 2Vishwanath A Sindagi and Vishal M Patel. Generating high-quality crowd density maps using contextual pyramid cnns. In IEEE International Conference on Computer Vi- sion, pages 1879-1888, 2017. 2, 8\n\nMulti-level bottom-top and top-bottom feature fusion for crowd counting. A Vishwanath, Sindagi, M Vishal, Patel, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionVishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom feature fusion for crowd count- ing. In Proceedings of the IEEE International Conference on Computer Vision, pages 1002-1012, 2019. 8\n\nLearning to count in the crowd from limited labeled data. A Vishwanath, Rajeev Sindagi, Deepak Yasarla, Sam Babu, Venkatesh Babu, Patel, arXiv:2007.03195arXiv preprintVishwanath A Sindagi, Rajeev Yasarla, Deepak Sam Babu, R Venkatesh Babu, and Vishal M Patel. Learning to count in the crowd from limited labeled data. arXiv preprint arXiv:2007.03195, 2020. 2\n\nJhu-crowd++: Large-scale crowd counting dataset and a benchmark method. A Vishwanath, Rajeev Sindagi, Yasarla, Patel, Technical ReportVishwanath A Sindagi, Rajeev Yasarla, and Vishal M Pa- tel. Jhu-crowd++: Large-scale crowd counting dataset and a benchmark method. Technical Report, 2020. 5\n\nAdaptive density map generation for crowd counting. Jia Wan, Antoni Chan, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision13Jia Wan and Antoni Chan. Adaptive density map genera- tion for crowd counting. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 1130-1139, 2019. 1, 2, 3\n\nModeling noisy annotations for crowd counting. Jia Wan, Antoni Chan, Advances in Neural Information Processing Systems. to appearJia Wan and Antoni Chan. Modeling noisy annotations for crowd counting. In Advances in Neural Information Process- ing Systems, to appear Dec 2020. 1, 2, 6, 8\n\nResidual regression and semantic prior for crowd counting. Jia Wan, Wenhan Luo, Baoyuan Wu, Antoni B Chan, Wei Liu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition27Jia Wan, Wenhan Luo, Baoyuan Wu, Antoni B Chan, and Wei Liu. Residual regression and semantic prior for crowd counting. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, 2019. 2, 7\n\nKernelbased density map generation for dense object counting. Jia Wan, Qingzhong Wang, Antoni B Chan, IEEE Transactions on Pattern Analysis and Machine Intelligence. 6Jia Wan, Qingzhong Wang, and Antoni B Chan. Kernel- based density map generation for dense object counting. IEEE Transactions on Pattern Analysis and Machine Intel- ligence, 2020. 1, 6, 8\n\nDistribution matching for crowd counting. Boyu Wang, Huidong Liu, Dimitris Samaras, Minh Hoai, Advances in Neural Information Processing Systems. 7to appearBoyu Wang, Huidong Liu, Dimitris Samaras, and Minh Hoai. Distribution matching for crowd counting. In Ad- vances in Neural Information Processing Systems, to appear Dec 2020. 2, 6, 7, 8\n\nNwpucrowd: A large-scale benchmark for crowd counting. Qi Wang, Junyu Gao, Wei Lin, Xuelong Li, IEEE Transactions on Pattern Analysis and Machine Intelligence. 35Qi Wang, Junyu Gao, Wei Lin, and Xuelong Li. Nwpu- crowd: A large-scale benchmark for crowd counting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 3, 5\n\nLearning from synthetic data for crowd counting in the wild. Qi Wang, Junyu Gao, Wei Lin, Yuan Yuan, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionQi Wang, Junyu Gao, Wei Lin, and Yuan Yuan. Learning from synthetic data for crowd counting in the wild. In Pro- ceedings of IEEE Conference on Computer Vision and Pat- tern Recognition, pages 8198-8207, 2019. 2\n\nSpatiotemporal modeling for crowd counting in videos. Feng Xiong, Xingjian Shi, Dit-Yan Yeung, 2017 IEEE International Conference on. Computer Vision (ICCVFeng Xiong, Xingjian Shi, and Dit-Yan Yeung. Spatiotempo- ral modeling for crowd counting in videos. In Computer Vi- sion (ICCV), 2017 IEEE International Conference on, pages 5161-5169, 2017. 2\n\nReverse perspective network for perspectiveaware object counting. Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang, Nicu Sebe, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang, and Nicu Sebe. Reverse perspective network for perspective- aware object counting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4374-4383, 2020. 8\n\nCross-scene crowd counting via deep convolutional neural networks. Cong Zhang, Hongsheng Li, X Wang, Xiaokang Yang, IEEE Conference on Computer Vision and Pattern Recognition. Cong Zhang, Hongsheng Li, X. Wang, and Xiaokang Yang. Cross-scene crowd counting via deep convolutional neural networks. In IEEE Conference on Computer Vision and Pat- tern Recognition, pages 833-841, 2015. 2\n\nSingle-image crowd counting via multi-column convolutional neural network. Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, Yi Ma, IEEE Conference on Computer Vision and Pattern Recognition. 6Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In IEEE Conference on Com- puter Vision and Pattern Recognition, pages 589-597, 2016. 2, 5, 6, 8\n\nActive crowd counting with limited supervision. Zhen Zhao, Miaojing Shi, Xiaoxiao Zhao, Li Li, arXiv:2007.06334arXiv preprintZhen Zhao, Miaojing Shi, Xiaoxiao Zhao, and Li Li. Ac- tive crowd counting with limited supervision. arXiv preprint arXiv:2007.06334, 2020. 2\n", "annotations": {"author": "[{\"end\":137,\"start\":67},{\"end\":240,\"start\":138},{\"end\":337,\"start\":241}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":71},{\"end\":148,\"start\":145},{\"end\":254,\"start\":250}]", "author_first_name": "[{\"end\":70,\"start\":67},{\"end\":144,\"start\":138},{\"end\":247,\"start\":241},{\"end\":249,\"start\":248}]", "author_affiliation": "[{\"end\":136,\"start\":76},{\"end\":239,\"start\":179},{\"end\":336,\"start\":276}]", "title": "[{\"end\":64,\"start\":1},{\"end\":401,\"start\":338}]", "venue": null, "abstract": "[{\"end\":1345,\"start\":431}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1638,\"start\":1634},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1640,\"start\":1638},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1951,\"start\":1947},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1954,\"start\":1951},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2028,\"start\":2024},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2375,\"start\":2371},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2378,\"start\":2375},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2571,\"start\":2567},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2948,\"start\":2944},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2951,\"start\":2948},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5110,\"start\":5106},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5113,\"start\":5110},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6369,\"start\":6365},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6388,\"start\":6384},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6527,\"start\":6524},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6529,\"start\":6527},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6532,\"start\":6529},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6644,\"start\":6640},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6718,\"start\":6714},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6832,\"start\":6828},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6835,\"start\":6832},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6838,\"start\":6835},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6880,\"start\":6876},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6883,\"start\":6880},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6886,\"start\":6883},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6926,\"start\":6922},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6929,\"start\":6926},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6974,\"start\":6970},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7024,\"start\":7020},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7146,\"start\":7143},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7323,\"start\":7319},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7385,\"start\":7381},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7467,\"start\":7463},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7619,\"start\":7615},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7789,\"start\":7785},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7825,\"start\":7821},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7993,\"start\":7989},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8199,\"start\":8195},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10148,\"start\":10144},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10191,\"start\":10187},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10271,\"start\":10267},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10327,\"start\":10323},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10399,\"start\":10395},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10764,\"start\":10760},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11031,\"start\":11027},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13134,\"start\":13130},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16557,\"start\":16554},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16914,\"start\":16913},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16982,\"start\":16979},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17555,\"start\":17552},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17675,\"start\":17672},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18356,\"start\":18352},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18730,\"start\":18727},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19211,\"start\":19208},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21087,\"start\":21083},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21356,\"start\":21352},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21539,\"start\":21535},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21971,\"start\":21967},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21995,\"start\":21991},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22317,\"start\":22313},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22359,\"start\":22355},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22388,\"start\":22384},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22838,\"start\":22834},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22865,\"start\":22861},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23052,\"start\":23048},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23168,\"start\":23164},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23301,\"start\":23297},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23350,\"start\":23346},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24334,\"start\":24330},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24349,\"start\":24345},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24367,\"start\":24363},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24388,\"start\":24384},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25098,\"start\":25094},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25499,\"start\":25495},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25539,\"start\":25535},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25552,\"start\":25548},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25567,\"start\":25563},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25697,\"start\":25694},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25909,\"start\":25906},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26248,\"start\":26244},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26617,\"start\":26613},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26631,\"start\":26627},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26682,\"start\":26678},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27019,\"start\":27015},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28371,\"start\":28367},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29023,\"start\":29019},{\"end\":30591,\"start\":30582},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31340,\"start\":31336},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31343,\"start\":31340},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32371,\"start\":32367},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36002,\"start\":35998}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35868,\"start\":35714},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36318,\"start\":35869},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36659,\"start\":36319},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36834,\"start\":36660},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36899,\"start\":36835},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37075,\"start\":36900},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37470,\"start\":37076},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37630,\"start\":37471},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39110,\"start\":37631},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39363,\"start\":39111},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39752,\"start\":39364}]", "paragraph": "[{\"end\":2226,\"start\":1361},{\"end\":2920,\"start\":2228},{\"end\":3522,\"start\":2922},{\"end\":4872,\"start\":3524},{\"end\":5460,\"start\":4874},{\"end\":6235,\"start\":5462},{\"end\":7620,\"start\":6253},{\"end\":10046,\"start\":7622},{\"end\":10679,\"start\":10048},{\"end\":10746,\"start\":10721},{\"end\":11897,\"start\":10748},{\"end\":11969,\"start\":11927},{\"end\":12162,\"start\":11997},{\"end\":12365,\"start\":12190},{\"end\":12476,\"start\":12387},{\"end\":12560,\"start\":12557},{\"end\":13884,\"start\":12573},{\"end\":15101,\"start\":13886},{\"end\":15210,\"start\":15103},{\"end\":15890,\"start\":15314},{\"end\":16105,\"start\":15892},{\"end\":16266,\"start\":16148},{\"end\":16505,\"start\":16286},{\"end\":16649,\"start\":16542},{\"end\":17181,\"start\":16689},{\"end\":17792,\"start\":17279},{\"end\":18233,\"start\":17794},{\"end\":18883,\"start\":18274},{\"end\":19112,\"start\":18923},{\"end\":19249,\"start\":19159},{\"end\":19638,\"start\":19427},{\"end\":19810,\"start\":19693},{\"end\":20010,\"start\":19898},{\"end\":20274,\"start\":20109},{\"end\":20759,\"start\":20304},{\"end\":21298,\"start\":20895},{\"end\":21539,\"start\":21334},{\"end\":21754,\"start\":21631},{\"end\":21810,\"start\":21805},{\"end\":22274,\"start\":21853},{\"end\":22463,\"start\":22276},{\"end\":22696,\"start\":22512},{\"end\":23053,\"start\":22762},{\"end\":23753,\"start\":23055},{\"end\":24079,\"start\":23755},{\"end\":24207,\"start\":24095},{\"end\":24931,\"start\":24231},{\"end\":25099,\"start\":24933},{\"end\":26341,\"start\":25161},{\"end\":26444,\"start\":26362},{\"end\":27393,\"start\":26481},{\"end\":28901,\"start\":27436},{\"end\":29850,\"start\":28942},{\"end\":30924,\"start\":29870},{\"end\":31639,\"start\":30968},{\"end\":32266,\"start\":31684},{\"end\":32910,\"start\":32304},{\"end\":34075,\"start\":32927},{\"end\":34574,\"start\":34077},{\"end\":35077,\"start\":34576},{\"end\":35713,\"start\":35092}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11996,\"start\":11970},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12189,\"start\":12163},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12386,\"start\":12366},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12556,\"start\":12477},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12572,\"start\":12561},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15244,\"start\":15211},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15277,\"start\":15244},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16147,\"start\":16106},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16285,\"start\":16267},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16688,\"start\":16650},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17238,\"start\":17182},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19158,\"start\":19113},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19311,\"start\":19250},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19426,\"start\":19311},{\"attributes\":{\"id\":\"formula_14\"},\"end\":19692,\"start\":19639},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19897,\"start\":19811},{\"attributes\":{\"id\":\"formula_16\"},\"end\":20108,\"start\":20011},{\"attributes\":{\"id\":\"formula_17\"},\"end\":20811,\"start\":20760},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20894,\"start\":20811},{\"attributes\":{\"id\":\"formula_19\"},\"end\":21630,\"start\":21540},{\"attributes\":{\"id\":\"formula_20\"},\"end\":21804,\"start\":21755},{\"attributes\":{\"id\":\"formula_22\"},\"end\":21852,\"start\":21811},{\"attributes\":{\"id\":\"formula_23\"},\"end\":22511,\"start\":22464},{\"attributes\":{\"id\":\"formula_24\"},\"end\":22761,\"start\":22697},{\"attributes\":{\"id\":\"formula_25\"},\"end\":25160,\"start\":25100}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26491,\"start\":26484},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31775,\"start\":31768},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32442,\"start\":32435},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33246,\"start\":33232}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1359,\"start\":1347},{\"attributes\":{\"n\":\"2.\"},\"end\":6251,\"start\":6238},{\"attributes\":{\"n\":\"3.\"},\"end\":10719,\"start\":10682},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11925,\"start\":11900},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15312,\"start\":15279},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16540,\"start\":16508},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17277,\"start\":17240},{\"attributes\":{\"n\":\"4.\"},\"end\":18272,\"start\":18236},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18921,\"start\":18886},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20302,\"start\":20277},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21332,\"start\":21301},{\"attributes\":{\"n\":\"5.\"},\"end\":24093,\"start\":24082},{\"attributes\":{\"n\":\"5.1.\"},\"end\":24229,\"start\":24210},{\"attributes\":{\"n\":\"5.2.\"},\"end\":26360,\"start\":26344},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":26479,\"start\":26447},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":27434,\"start\":27396},{\"attributes\":{\"n\":\"5.2.3\"},\"end\":28940,\"start\":28904},{\"attributes\":{\"n\":\"5.2.4\"},\"end\":29868,\"start\":29853},{\"attributes\":{\"n\":\"5.2.5\"},\"end\":30966,\"start\":30927},{\"attributes\":{\"n\":\"5.2.6\"},\"end\":31682,\"start\":31642},{\"attributes\":{\"n\":\"5.3.\"},\"end\":32302,\"start\":32269},{\"attributes\":{\"n\":\"5.4.\"},\"end\":32925,\"start\":32913},{\"attributes\":{\"n\":\"6.\"},\"end\":35090,\"start\":35080},{\"end\":35725,\"start\":35715},{\"end\":35880,\"start\":35870},{\"end\":36330,\"start\":36320},{\"end\":36671,\"start\":36661},{\"end\":36846,\"start\":36836},{\"end\":37086,\"start\":37077},{\"end\":37481,\"start\":37472},{\"end\":37641,\"start\":37632},{\"end\":39121,\"start\":39112},{\"end\":39374,\"start\":39365}]", "table": "[{\"end\":37470,\"start\":37156},{\"end\":37630,\"start\":37539},{\"end\":39110,\"start\":37699},{\"end\":39363,\"start\":39170},{\"end\":39752,\"start\":39421}]", "figure_caption": "[{\"end\":35868,\"start\":35727},{\"end\":36318,\"start\":35882},{\"end\":36659,\"start\":36332},{\"end\":36834,\"start\":36673},{\"end\":36899,\"start\":36848},{\"end\":37075,\"start\":36902},{\"end\":37156,\"start\":37088},{\"end\":37539,\"start\":37483},{\"end\":37699,\"start\":37643},{\"end\":39170,\"start\":39123},{\"end\":39421,\"start\":39376}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1967,\"start\":1960},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2224,\"start\":2217},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3848,\"start\":3841},{\"end\":12986,\"start\":12978},{\"end\":13862,\"start\":13856},{\"end\":22960,\"start\":22952},{\"end\":23358,\"start\":23352},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26037,\"start\":26028},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27698,\"start\":27691},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28061,\"start\":28055},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29098,\"start\":29091},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30114,\"start\":30107},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30627,\"start\":30621},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31176,\"start\":31169},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31430,\"start\":31423},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31481,\"start\":31475},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34074,\"start\":34068}]", "bib_author_first_name": "[{\"end\":40348,\"start\":40343},{\"end\":40369,\"start\":40365},{\"end\":40386,\"start\":40379},{\"end\":40771,\"start\":40765},{\"end\":40784,\"start\":40777},{\"end\":40797,\"start\":40791},{\"end\":40807,\"start\":40804},{\"end\":41233,\"start\":41232},{\"end\":41258,\"start\":41242},{\"end\":41269,\"start\":41265},{\"end\":41639,\"start\":41638},{\"end\":41652,\"start\":41648},{\"end\":41953,\"start\":41949},{\"end\":41969,\"start\":41961},{\"end\":41995,\"start\":41980},{\"end\":42010,\"start\":42005},{\"end\":42035,\"start\":42028},{\"end\":42491,\"start\":42488},{\"end\":42502,\"start\":42497},{\"end\":42512,\"start\":42508},{\"end\":42521,\"start\":42519},{\"end\":42930,\"start\":42923},{\"end\":42942,\"start\":42935},{\"end\":42958,\"start\":42950},{\"end\":42968,\"start\":42964},{\"end\":43350,\"start\":43344},{\"end\":43359,\"start\":43355},{\"end\":43737,\"start\":43732},{\"end\":43750,\"start\":43742},{\"end\":43763,\"start\":43758},{\"end\":43777,\"start\":43769},{\"end\":43792,\"start\":43785},{\"end\":43805,\"start\":43798},{\"end\":43816,\"start\":43811},{\"end\":44109,\"start\":44106},{\"end\":44123,\"start\":44117},{\"end\":44136,\"start\":44129},{\"end\":44161,\"start\":44153},{\"end\":44609,\"start\":44608},{\"end\":44619,\"start\":44618},{\"end\":44630,\"start\":44629},{\"end\":44641,\"start\":44640},{\"end\":45012,\"start\":45006},{\"end\":45028,\"start\":45021},{\"end\":45043,\"start\":45037},{\"end\":45056,\"start\":45052},{\"end\":45070,\"start\":45064},{\"end\":45088,\"start\":45083},{\"end\":45105,\"start\":45098},{\"end\":45573,\"start\":45565},{\"end\":45583,\"start\":45581},{\"end\":45600,\"start\":45591},{\"end\":45612,\"start\":45605},{\"end\":45623,\"start\":45620},{\"end\":45632,\"start\":45628},{\"end\":45642,\"start\":45639},{\"end\":45655,\"start\":45649},{\"end\":46135,\"start\":46133},{\"end\":46148,\"start\":46142},{\"end\":46150,\"start\":46149},{\"end\":46457,\"start\":46455},{\"end\":46469,\"start\":46464},{\"end\":46480,\"start\":46474},{\"end\":46482,\"start\":46481},{\"end\":46820,\"start\":46819},{\"end\":46836,\"start\":46831},{\"end\":47042,\"start\":47035},{\"end\":47055,\"start\":47050},{\"end\":47070,\"start\":47065},{\"end\":47408,\"start\":47402},{\"end\":47426,\"start\":47420},{\"end\":47766,\"start\":47763},{\"end\":47780,\"start\":47771},{\"end\":47793,\"start\":47788},{\"end\":47807,\"start\":47801},{\"end\":48203,\"start\":48197},{\"end\":48215,\"start\":48208},{\"end\":48229,\"start\":48223},{\"end\":48726,\"start\":48720},{\"end\":48737,\"start\":48733},{\"end\":48745,\"start\":48742},{\"end\":48759,\"start\":48753},{\"end\":48773,\"start\":48765},{\"end\":49271,\"start\":49263},{\"end\":49282,\"start\":49277},{\"end\":49295,\"start\":49289},{\"end\":49690,\"start\":49685},{\"end\":49699,\"start\":49696},{\"end\":49711,\"start\":49704},{\"end\":49724,\"start\":49717},{\"end\":49738,\"start\":49732},{\"end\":49751,\"start\":49744},{\"end\":50046,\"start\":50040},{\"end\":50058,\"start\":50052},{\"end\":50071,\"start\":50064},{\"end\":50082,\"start\":50076},{\"end\":50093,\"start\":50088},{\"end\":50107,\"start\":50102},{\"end\":50547,\"start\":50541},{\"end\":50558,\"start\":50553},{\"end\":50580,\"start\":50574},{\"end\":50582,\"start\":50581},{\"end\":50933,\"start\":50927},{\"end\":50942,\"start\":50939},{\"end\":50955,\"start\":50949},{\"end\":51227,\"start\":51224},{\"end\":51241,\"start\":51233},{\"end\":51251,\"start\":51247},{\"end\":51266,\"start\":51258},{\"end\":51280,\"start\":51274},{\"end\":51565,\"start\":51558},{\"end\":51574,\"start\":51570},{\"end\":51588,\"start\":51580},{\"end\":51601,\"start\":51595},{\"end\":52000,\"start\":51996},{\"end\":52020,\"start\":52016},{\"end\":52281,\"start\":52270},{\"end\":52288,\"start\":52287},{\"end\":52753,\"start\":52748},{\"end\":52779,\"start\":52771},{\"end\":53234,\"start\":53230},{\"end\":53253,\"start\":53252},{\"end\":53611,\"start\":53608},{\"end\":53620,\"start\":53618},{\"end\":53633,\"start\":53625},{\"end\":53643,\"start\":53638},{\"end\":53657,\"start\":53650},{\"end\":53670,\"start\":53662},{\"end\":54127,\"start\":54120},{\"end\":54135,\"start\":54133},{\"end\":54146,\"start\":54143},{\"end\":54160,\"start\":54152},{\"end\":54174,\"start\":54166},{\"end\":54188,\"start\":54179},{\"end\":54202,\"start\":54196},{\"end\":54585,\"start\":54584},{\"end\":54608,\"start\":54607},{\"end\":54948,\"start\":54947},{\"end\":54971,\"start\":54970},{\"end\":55381,\"start\":55380},{\"end\":55400,\"start\":55394},{\"end\":55416,\"start\":55410},{\"end\":55755,\"start\":55754},{\"end\":55774,\"start\":55768},{\"end\":56030,\"start\":56027},{\"end\":56042,\"start\":56036},{\"end\":56407,\"start\":56404},{\"end\":56419,\"start\":56413},{\"end\":56708,\"start\":56705},{\"end\":56720,\"start\":56714},{\"end\":56733,\"start\":56726},{\"end\":56744,\"start\":56738},{\"end\":56746,\"start\":56745},{\"end\":56756,\"start\":56753},{\"end\":57186,\"start\":57183},{\"end\":57201,\"start\":57192},{\"end\":57214,\"start\":57208},{\"end\":57216,\"start\":57215},{\"end\":57523,\"start\":57519},{\"end\":57537,\"start\":57530},{\"end\":57551,\"start\":57543},{\"end\":57565,\"start\":57561},{\"end\":57877,\"start\":57875},{\"end\":57889,\"start\":57884},{\"end\":57898,\"start\":57895},{\"end\":57911,\"start\":57904},{\"end\":58223,\"start\":58221},{\"end\":58235,\"start\":58230},{\"end\":58244,\"start\":58241},{\"end\":58254,\"start\":58250},{\"end\":58665,\"start\":58661},{\"end\":58681,\"start\":58673},{\"end\":58694,\"start\":58687},{\"end\":59028,\"start\":59023},{\"end\":59042,\"start\":59035},{\"end\":59050,\"start\":59047},{\"end\":59057,\"start\":59055},{\"end\":59070,\"start\":59062},{\"end\":59082,\"start\":59078},{\"end\":59559,\"start\":59555},{\"end\":59576,\"start\":59567},{\"end\":59582,\"start\":59581},{\"end\":59597,\"start\":59589},{\"end\":59957,\"start\":59949},{\"end\":59970,\"start\":59965},{\"end\":59982,\"start\":59977},{\"end\":59997,\"start\":59989},{\"end\":60005,\"start\":60003},{\"end\":60361,\"start\":60357},{\"end\":60376,\"start\":60368},{\"end\":60390,\"start\":60382},{\"end\":60399,\"start\":60397}]", "bib_author_last_name": "[{\"end\":40363,\"start\":40349},{\"end\":40377,\"start\":40370},{\"end\":40394,\"start\":40387},{\"end\":40775,\"start\":40772},{\"end\":40789,\"start\":40785},{\"end\":40802,\"start\":40798},{\"end\":40810,\"start\":40808},{\"end\":41240,\"start\":41234},{\"end\":41263,\"start\":41259},{\"end\":41275,\"start\":41270},{\"end\":41288,\"start\":41277},{\"end\":41646,\"start\":41640},{\"end\":41657,\"start\":41653},{\"end\":41670,\"start\":41659},{\"end\":41959,\"start\":41954},{\"end\":41978,\"start\":41970},{\"end\":42003,\"start\":41996},{\"end\":42026,\"start\":42011},{\"end\":42042,\"start\":42036},{\"end\":42049,\"start\":42044},{\"end\":42495,\"start\":42492},{\"end\":42506,\"start\":42503},{\"end\":42517,\"start\":42513},{\"end\":42526,\"start\":42522},{\"end\":42933,\"start\":42931},{\"end\":42948,\"start\":42943},{\"end\":42962,\"start\":42959},{\"end\":42972,\"start\":42969},{\"end\":43353,\"start\":43351},{\"end\":43367,\"start\":43360},{\"end\":43740,\"start\":43738},{\"end\":43756,\"start\":43751},{\"end\":43767,\"start\":43764},{\"end\":43783,\"start\":43778},{\"end\":43796,\"start\":43793},{\"end\":43809,\"start\":43806},{\"end\":43825,\"start\":43817},{\"end\":44115,\"start\":44110},{\"end\":44127,\"start\":44124},{\"end\":44151,\"start\":44137},{\"end\":44172,\"start\":44162},{\"end\":44616,\"start\":44610},{\"end\":44627,\"start\":44620},{\"end\":44638,\"start\":44631},{\"end\":44646,\"start\":44642},{\"end\":45019,\"start\":45013},{\"end\":45035,\"start\":45029},{\"end\":45050,\"start\":45044},{\"end\":45062,\"start\":45057},{\"end\":45081,\"start\":45071},{\"end\":45096,\"start\":45089},{\"end\":45110,\"start\":45106},{\"end\":45579,\"start\":45574},{\"end\":45589,\"start\":45584},{\"end\":45603,\"start\":45601},{\"end\":45618,\"start\":45613},{\"end\":45626,\"start\":45624},{\"end\":45637,\"start\":45633},{\"end\":45647,\"start\":45643},{\"end\":45660,\"start\":45656},{\"end\":46140,\"start\":46136},{\"end\":46155,\"start\":46151},{\"end\":46462,\"start\":46458},{\"end\":46472,\"start\":46470},{\"end\":46487,\"start\":46483},{\"end\":46829,\"start\":46821},{\"end\":46843,\"start\":46837},{\"end\":46847,\"start\":46845},{\"end\":47048,\"start\":47043},{\"end\":47063,\"start\":47056},{\"end\":47078,\"start\":47071},{\"end\":47418,\"start\":47409},{\"end\":47436,\"start\":47427},{\"end\":47769,\"start\":47767},{\"end\":47786,\"start\":47781},{\"end\":47799,\"start\":47794},{\"end\":47811,\"start\":47808},{\"end\":48206,\"start\":48204},{\"end\":48221,\"start\":48216},{\"end\":48234,\"start\":48230},{\"end\":48731,\"start\":48727},{\"end\":48740,\"start\":48738},{\"end\":48751,\"start\":48746},{\"end\":48763,\"start\":48760},{\"end\":48777,\"start\":48774},{\"end\":49275,\"start\":49272},{\"end\":49287,\"start\":49283},{\"end\":49298,\"start\":49296},{\"end\":49694,\"start\":49691},{\"end\":49702,\"start\":49700},{\"end\":49715,\"start\":49712},{\"end\":49730,\"start\":49725},{\"end\":49742,\"start\":49739},{\"end\":49756,\"start\":49752},{\"end\":50050,\"start\":50047},{\"end\":50062,\"start\":50059},{\"end\":50074,\"start\":50072},{\"end\":50086,\"start\":50083},{\"end\":50100,\"start\":50094},{\"end\":50111,\"start\":50108},{\"end\":50551,\"start\":50548},{\"end\":50572,\"start\":50559},{\"end\":50591,\"start\":50583},{\"end\":50937,\"start\":50934},{\"end\":50947,\"start\":50943},{\"end\":50960,\"start\":50956},{\"end\":51231,\"start\":51228},{\"end\":51245,\"start\":51242},{\"end\":51256,\"start\":51252},{\"end\":51272,\"start\":51267},{\"end\":51284,\"start\":51281},{\"end\":51568,\"start\":51566},{\"end\":51578,\"start\":51575},{\"end\":51593,\"start\":51589},{\"end\":51606,\"start\":51602},{\"end\":52014,\"start\":52001},{\"end\":52023,\"start\":52021},{\"end\":52029,\"start\":52025},{\"end\":52285,\"start\":52282},{\"end\":52298,\"start\":52289},{\"end\":52304,\"start\":52300},{\"end\":52769,\"start\":52754},{\"end\":52795,\"start\":52780},{\"end\":52819,\"start\":52797},{\"end\":53250,\"start\":53235},{\"end\":53259,\"start\":53254},{\"end\":53270,\"start\":53261},{\"end\":53276,\"start\":53272},{\"end\":53616,\"start\":53612},{\"end\":53623,\"start\":53621},{\"end\":53636,\"start\":53634},{\"end\":53648,\"start\":53644},{\"end\":53660,\"start\":53658},{\"end\":53675,\"start\":53671},{\"end\":54131,\"start\":54128},{\"end\":54141,\"start\":54136},{\"end\":54150,\"start\":54147},{\"end\":54164,\"start\":54161},{\"end\":54177,\"start\":54175},{\"end\":54194,\"start\":54189},{\"end\":54208,\"start\":54203},{\"end\":54596,\"start\":54586},{\"end\":54605,\"start\":54598},{\"end\":54615,\"start\":54609},{\"end\":54622,\"start\":54617},{\"end\":54959,\"start\":54949},{\"end\":54968,\"start\":54961},{\"end\":54978,\"start\":54972},{\"end\":54985,\"start\":54980},{\"end\":55392,\"start\":55382},{\"end\":55408,\"start\":55401},{\"end\":55424,\"start\":55417},{\"end\":55434,\"start\":55426},{\"end\":55450,\"start\":55436},{\"end\":55457,\"start\":55452},{\"end\":55766,\"start\":55756},{\"end\":55782,\"start\":55775},{\"end\":55791,\"start\":55784},{\"end\":55798,\"start\":55793},{\"end\":56034,\"start\":56031},{\"end\":56047,\"start\":56043},{\"end\":56411,\"start\":56408},{\"end\":56424,\"start\":56420},{\"end\":56712,\"start\":56709},{\"end\":56724,\"start\":56721},{\"end\":56736,\"start\":56734},{\"end\":56751,\"start\":56747},{\"end\":56760,\"start\":56757},{\"end\":57190,\"start\":57187},{\"end\":57206,\"start\":57202},{\"end\":57221,\"start\":57217},{\"end\":57528,\"start\":57524},{\"end\":57541,\"start\":57538},{\"end\":57559,\"start\":57552},{\"end\":57570,\"start\":57566},{\"end\":57882,\"start\":57878},{\"end\":57893,\"start\":57890},{\"end\":57902,\"start\":57899},{\"end\":57914,\"start\":57912},{\"end\":58228,\"start\":58224},{\"end\":58239,\"start\":58236},{\"end\":58248,\"start\":58245},{\"end\":58259,\"start\":58255},{\"end\":58671,\"start\":58666},{\"end\":58685,\"start\":58682},{\"end\":58700,\"start\":58695},{\"end\":59033,\"start\":59029},{\"end\":59045,\"start\":59043},{\"end\":59053,\"start\":59051},{\"end\":59060,\"start\":59058},{\"end\":59076,\"start\":59071},{\"end\":59087,\"start\":59083},{\"end\":59565,\"start\":59560},{\"end\":59579,\"start\":59577},{\"end\":59587,\"start\":59583},{\"end\":59602,\"start\":59598},{\"end\":59963,\"start\":59958},{\"end\":59975,\"start\":59971},{\"end\":59987,\"start\":59983},{\"end\":60001,\"start\":59998},{\"end\":60008,\"start\":60006},{\"end\":60366,\"start\":60362},{\"end\":60380,\"start\":60377},{\"end\":60395,\"start\":60391},{\"end\":60402,\"start\":60400}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":40259,\"start\":40079},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":60814714},\"end\":40694,\"start\":40261},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52955811},\"end\":41142,\"start\":40696},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9059102},\"end\":41588,\"start\":41144},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":749620},\"end\":41871,\"start\":41590},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":84834062},\"end\":42418,\"start\":41873},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211205019},\"end\":42875,\"start\":42420},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206594692},\"end\":43322,\"start\":42877},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7552471},\"end\":43666,\"start\":43324},{\"attributes\":{\"doi\":\"arXiv:2003.00217\",\"id\":\"b9\"},\"end\":44062,\"start\":43668},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9433631},\"end\":44539,\"start\":44064},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9749221},\"end\":44916,\"start\":44541},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":51901514},\"end\":45525,\"start\":44918},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219964549},\"end\":46060,\"start\":45527},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":21704501},\"end\":46349,\"start\":46062},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":19706288},\"end\":46773,\"start\":46351},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b16\"},\"end\":46993,\"start\":46775},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14395688},\"end\":47363,\"start\":46995},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":18018217},\"end\":47645,\"start\":47365},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5157313},\"end\":48102,\"start\":47647},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3645757},\"end\":48627,\"start\":48104},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":196194550},\"end\":49182,\"start\":48629},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":195473446},\"end\":49613,\"start\":49184},{\"attributes\":{\"doi\":\"arXiv:2007.08260\",\"id\":\"b23\"},\"end\":49975,\"start\":49615},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":201645306},\"end\":50473,\"start\":49977},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3787969},\"end\":50845,\"start\":50475},{\"attributes\":{\"doi\":\"arXiv:2005.05776\",\"id\":\"b26\"},\"end\":51153,\"start\":50847},{\"attributes\":{\"doi\":\"arXiv:2007.03207\",\"id\":\"b27\"},\"end\":51491,\"start\":51155},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":199543622},\"end\":51968,\"start\":51493},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":50785934},\"end\":52201,\"start\":51970},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":19200040},\"end\":52615,\"start\":52203},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":189999733},\"end\":53169,\"start\":52617},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1089358},\"end\":53542,\"start\":53171},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":52860247},\"end\":54062,\"start\":53544},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52243494},\"end\":54508,\"start\":54064},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2099022},\"end\":54872,\"start\":54510},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":201668945},\"end\":55320,\"start\":54874},{\"attributes\":{\"doi\":\"arXiv:2007.03195\",\"id\":\"b37\"},\"end\":55680,\"start\":55322},{\"attributes\":{\"id\":\"b38\"},\"end\":55973,\"start\":55682},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":204958589},\"end\":56355,\"start\":55975},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":227275245},\"end\":56644,\"start\":56357},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":197639806},\"end\":57119,\"start\":56646},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":221625392},\"end\":57475,\"start\":57121},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":221970845},\"end\":57818,\"start\":57477},{\"attributes\":{\"id\":\"b44\"},\"end\":58158,\"start\":57820},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":72941021},\"end\":58605,\"start\":58160},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":24725602},\"end\":58955,\"start\":58607},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":219963357},\"end\":59486,\"start\":58957},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2131202},\"end\":59872,\"start\":59488},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":4545310},\"end\":60307,\"start\":59874},{\"attributes\":{\"doi\":\"arXiv:2007.06334\",\"id\":\"b50\"},\"end\":60575,\"start\":60309}]", "bib_title": "[{\"end\":40341,\"start\":40261},{\"end\":40763,\"start\":40696},{\"end\":41230,\"start\":41144},{\"end\":41636,\"start\":41590},{\"end\":41947,\"start\":41873},{\"end\":42486,\"start\":42420},{\"end\":42921,\"start\":42877},{\"end\":43342,\"start\":43324},{\"end\":44104,\"start\":44064},{\"end\":44606,\"start\":44541},{\"end\":45004,\"start\":44918},{\"end\":45563,\"start\":45527},{\"end\":46131,\"start\":46062},{\"end\":46453,\"start\":46351},{\"end\":47033,\"start\":46995},{\"end\":47400,\"start\":47365},{\"end\":47761,\"start\":47647},{\"end\":48195,\"start\":48104},{\"end\":48718,\"start\":48629},{\"end\":49261,\"start\":49184},{\"end\":50038,\"start\":49977},{\"end\":50539,\"start\":50475},{\"end\":51556,\"start\":51493},{\"end\":51994,\"start\":51970},{\"end\":52268,\"start\":52203},{\"end\":52746,\"start\":52617},{\"end\":53228,\"start\":53171},{\"end\":53606,\"start\":53544},{\"end\":54118,\"start\":54064},{\"end\":54582,\"start\":54510},{\"end\":54945,\"start\":54874},{\"end\":56025,\"start\":55975},{\"end\":56402,\"start\":56357},{\"end\":56703,\"start\":56646},{\"end\":57181,\"start\":57121},{\"end\":57517,\"start\":57477},{\"end\":57873,\"start\":57820},{\"end\":58219,\"start\":58160},{\"end\":58659,\"start\":58607},{\"end\":59021,\"start\":58957},{\"end\":59553,\"start\":59488},{\"end\":59947,\"start\":59874}]", "bib_author": "[{\"end\":40365,\"start\":40343},{\"end\":40379,\"start\":40365},{\"end\":40396,\"start\":40379},{\"end\":40777,\"start\":40765},{\"end\":40791,\"start\":40777},{\"end\":40804,\"start\":40791},{\"end\":40812,\"start\":40804},{\"end\":41242,\"start\":41232},{\"end\":41265,\"start\":41242},{\"end\":41277,\"start\":41265},{\"end\":41290,\"start\":41277},{\"end\":41648,\"start\":41638},{\"end\":41659,\"start\":41648},{\"end\":41672,\"start\":41659},{\"end\":41961,\"start\":41949},{\"end\":41980,\"start\":41961},{\"end\":42005,\"start\":41980},{\"end\":42028,\"start\":42005},{\"end\":42044,\"start\":42028},{\"end\":42051,\"start\":42044},{\"end\":42497,\"start\":42488},{\"end\":42508,\"start\":42497},{\"end\":42519,\"start\":42508},{\"end\":42528,\"start\":42519},{\"end\":42935,\"start\":42923},{\"end\":42950,\"start\":42935},{\"end\":42964,\"start\":42950},{\"end\":42974,\"start\":42964},{\"end\":43355,\"start\":43344},{\"end\":43369,\"start\":43355},{\"end\":43742,\"start\":43732},{\"end\":43758,\"start\":43742},{\"end\":43769,\"start\":43758},{\"end\":43785,\"start\":43769},{\"end\":43798,\"start\":43785},{\"end\":43811,\"start\":43798},{\"end\":43827,\"start\":43811},{\"end\":44117,\"start\":44106},{\"end\":44129,\"start\":44117},{\"end\":44153,\"start\":44129},{\"end\":44174,\"start\":44153},{\"end\":44618,\"start\":44608},{\"end\":44629,\"start\":44618},{\"end\":44640,\"start\":44629},{\"end\":44648,\"start\":44640},{\"end\":45021,\"start\":45006},{\"end\":45037,\"start\":45021},{\"end\":45052,\"start\":45037},{\"end\":45064,\"start\":45052},{\"end\":45083,\"start\":45064},{\"end\":45098,\"start\":45083},{\"end\":45112,\"start\":45098},{\"end\":45581,\"start\":45565},{\"end\":45591,\"start\":45581},{\"end\":45605,\"start\":45591},{\"end\":45620,\"start\":45605},{\"end\":45628,\"start\":45620},{\"end\":45639,\"start\":45628},{\"end\":45649,\"start\":45639},{\"end\":45662,\"start\":45649},{\"end\":46142,\"start\":46133},{\"end\":46157,\"start\":46142},{\"end\":46464,\"start\":46455},{\"end\":46474,\"start\":46464},{\"end\":46489,\"start\":46474},{\"end\":46831,\"start\":46819},{\"end\":46845,\"start\":46831},{\"end\":46849,\"start\":46845},{\"end\":47050,\"start\":47035},{\"end\":47065,\"start\":47050},{\"end\":47080,\"start\":47065},{\"end\":47420,\"start\":47402},{\"end\":47438,\"start\":47420},{\"end\":47771,\"start\":47763},{\"end\":47788,\"start\":47771},{\"end\":47801,\"start\":47788},{\"end\":47813,\"start\":47801},{\"end\":48208,\"start\":48197},{\"end\":48223,\"start\":48208},{\"end\":48236,\"start\":48223},{\"end\":48733,\"start\":48720},{\"end\":48742,\"start\":48733},{\"end\":48753,\"start\":48742},{\"end\":48765,\"start\":48753},{\"end\":48779,\"start\":48765},{\"end\":49277,\"start\":49263},{\"end\":49289,\"start\":49277},{\"end\":49300,\"start\":49289},{\"end\":49696,\"start\":49685},{\"end\":49704,\"start\":49696},{\"end\":49717,\"start\":49704},{\"end\":49732,\"start\":49717},{\"end\":49744,\"start\":49732},{\"end\":49758,\"start\":49744},{\"end\":50052,\"start\":50040},{\"end\":50064,\"start\":50052},{\"end\":50076,\"start\":50064},{\"end\":50088,\"start\":50076},{\"end\":50102,\"start\":50088},{\"end\":50113,\"start\":50102},{\"end\":50553,\"start\":50541},{\"end\":50574,\"start\":50553},{\"end\":50593,\"start\":50574},{\"end\":50939,\"start\":50927},{\"end\":50949,\"start\":50939},{\"end\":50962,\"start\":50949},{\"end\":51233,\"start\":51224},{\"end\":51247,\"start\":51233},{\"end\":51258,\"start\":51247},{\"end\":51274,\"start\":51258},{\"end\":51286,\"start\":51274},{\"end\":51570,\"start\":51558},{\"end\":51580,\"start\":51570},{\"end\":51595,\"start\":51580},{\"end\":51608,\"start\":51595},{\"end\":52016,\"start\":51996},{\"end\":52025,\"start\":52016},{\"end\":52031,\"start\":52025},{\"end\":52287,\"start\":52270},{\"end\":52300,\"start\":52287},{\"end\":52306,\"start\":52300},{\"end\":52771,\"start\":52748},{\"end\":52797,\"start\":52771},{\"end\":52821,\"start\":52797},{\"end\":53252,\"start\":53230},{\"end\":53261,\"start\":53252},{\"end\":53272,\"start\":53261},{\"end\":53278,\"start\":53272},{\"end\":53618,\"start\":53608},{\"end\":53625,\"start\":53618},{\"end\":53638,\"start\":53625},{\"end\":53650,\"start\":53638},{\"end\":53662,\"start\":53650},{\"end\":53677,\"start\":53662},{\"end\":54133,\"start\":54120},{\"end\":54143,\"start\":54133},{\"end\":54152,\"start\":54143},{\"end\":54166,\"start\":54152},{\"end\":54179,\"start\":54166},{\"end\":54196,\"start\":54179},{\"end\":54210,\"start\":54196},{\"end\":54598,\"start\":54584},{\"end\":54607,\"start\":54598},{\"end\":54617,\"start\":54607},{\"end\":54624,\"start\":54617},{\"end\":54961,\"start\":54947},{\"end\":54970,\"start\":54961},{\"end\":54980,\"start\":54970},{\"end\":54987,\"start\":54980},{\"end\":55394,\"start\":55380},{\"end\":55410,\"start\":55394},{\"end\":55426,\"start\":55410},{\"end\":55436,\"start\":55426},{\"end\":55452,\"start\":55436},{\"end\":55459,\"start\":55452},{\"end\":55768,\"start\":55754},{\"end\":55784,\"start\":55768},{\"end\":55793,\"start\":55784},{\"end\":55800,\"start\":55793},{\"end\":56036,\"start\":56027},{\"end\":56049,\"start\":56036},{\"end\":56413,\"start\":56404},{\"end\":56426,\"start\":56413},{\"end\":56714,\"start\":56705},{\"end\":56726,\"start\":56714},{\"end\":56738,\"start\":56726},{\"end\":56753,\"start\":56738},{\"end\":56762,\"start\":56753},{\"end\":57192,\"start\":57183},{\"end\":57208,\"start\":57192},{\"end\":57223,\"start\":57208},{\"end\":57530,\"start\":57519},{\"end\":57543,\"start\":57530},{\"end\":57561,\"start\":57543},{\"end\":57572,\"start\":57561},{\"end\":57884,\"start\":57875},{\"end\":57895,\"start\":57884},{\"end\":57904,\"start\":57895},{\"end\":57916,\"start\":57904},{\"end\":58230,\"start\":58221},{\"end\":58241,\"start\":58230},{\"end\":58250,\"start\":58241},{\"end\":58261,\"start\":58250},{\"end\":58673,\"start\":58661},{\"end\":58687,\"start\":58673},{\"end\":58702,\"start\":58687},{\"end\":59035,\"start\":59023},{\"end\":59047,\"start\":59035},{\"end\":59055,\"start\":59047},{\"end\":59062,\"start\":59055},{\"end\":59078,\"start\":59062},{\"end\":59089,\"start\":59078},{\"end\":59567,\"start\":59555},{\"end\":59581,\"start\":59567},{\"end\":59589,\"start\":59581},{\"end\":59604,\"start\":59589},{\"end\":59965,\"start\":59949},{\"end\":59977,\"start\":59965},{\"end\":59989,\"start\":59977},{\"end\":60003,\"start\":59989},{\"end\":60010,\"start\":60003},{\"end\":60368,\"start\":60357},{\"end\":60382,\"start\":60368},{\"end\":60397,\"start\":60382},{\"end\":60404,\"start\":60397}]", "bib_venue": "[{\"end\":40927,\"start\":40878},{\"end\":43115,\"start\":43053},{\"end\":43510,\"start\":43448},{\"end\":44315,\"start\":44253},{\"end\":45227,\"start\":45178},{\"end\":45811,\"start\":45745},{\"end\":48377,\"start\":48315},{\"end\":48920,\"start\":48858},{\"end\":50234,\"start\":50182},{\"end\":51729,\"start\":51677},{\"end\":52415,\"start\":52369},{\"end\":53818,\"start\":53756},{\"end\":55108,\"start\":55056},{\"end\":56170,\"start\":56118},{\"end\":56903,\"start\":56841},{\"end\":58394,\"start\":58336},{\"end\":59238,\"start\":59172},{\"end\":40154,\"start\":40079},{\"end\":40458,\"start\":40396},{\"end\":40876,\"start\":40812},{\"end\":41348,\"start\":41290},{\"end\":41715,\"start\":41672},{\"end\":42126,\"start\":42051},{\"end\":42626,\"start\":42528},{\"end\":43051,\"start\":42974},{\"end\":43446,\"start\":43369},{\"end\":43730,\"start\":43668},{\"end\":44251,\"start\":44174},{\"end\":44706,\"start\":44648},{\"end\":45176,\"start\":45112},{\"end\":45743,\"start\":45662},{\"end\":46190,\"start\":46157},{\"end\":46551,\"start\":46489},{\"end\":46817,\"start\":46775},{\"end\":47155,\"start\":47080},{\"end\":47487,\"start\":47438},{\"end\":47860,\"start\":47813},{\"end\":48313,\"start\":48236},{\"end\":48856,\"start\":48779},{\"end\":49374,\"start\":49300},{\"end\":49683,\"start\":49615},{\"end\":50180,\"start\":50113},{\"end\":50651,\"start\":50593},{\"end\":50925,\"start\":50847},{\"end\":51222,\"start\":51155},{\"end\":51675,\"start\":51608},{\"end\":52069,\"start\":52031},{\"end\":52367,\"start\":52306},{\"end\":52883,\"start\":52821},{\"end\":53336,\"start\":53278},{\"end\":53754,\"start\":53677},{\"end\":54268,\"start\":54210},{\"end\":54672,\"start\":54624},{\"end\":55054,\"start\":54987},{\"end\":55378,\"start\":55322},{\"end\":55752,\"start\":55682},{\"end\":56116,\"start\":56049},{\"end\":56475,\"start\":56426},{\"end\":56839,\"start\":56762},{\"end\":57285,\"start\":57223},{\"end\":57621,\"start\":57572},{\"end\":57978,\"start\":57916},{\"end\":58334,\"start\":58261},{\"end\":58739,\"start\":58702},{\"end\":59170,\"start\":59089},{\"end\":59662,\"start\":59604},{\"end\":60068,\"start\":60010},{\"end\":60355,\"start\":60309}]"}}}, "year": 2023, "month": 12, "day": 17}