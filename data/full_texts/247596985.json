{"id": 247596985, "updated": "2023-10-05 15:58:49.217", "metadata": {"title": "AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network", "authors": "[{\"first\":\"Wooseok\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Sanghyun\",\"last\":\"Son\",\"middle\":[]},{\"first\":\"Kyoung\",\"last\":\"Lee\",\"middle\":[\"Mu\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Blind-spot network (BSN) and its variants have made significant advances in self-supervised denoising. Nevertheless, they are still bound to synthetic noisy inputs due to less practical assumptions like pixel-wise independent noise. Hence, it is challenging to deal with spatially correlated real-world noise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has been proposed to remove the spatial correlation of real-world noise. However, it is not trivial to integrate PD and BSN directly, which prevents the fully self-supervised denoising model on real-world images. We propose an Asymmetric PD (AP) to address this issue, which introduces different PD stride factors for training and inference. We systematically demonstrate that the proposed AP can resolve inherent trade-offs caused by specific PD stride factors and make BSN applicable to practical scenarios. To this end, we develop AP-BSN, a state-of-the-art self-supervised denoising method for real-world sRGB images. We further propose random-replacing refinement, which significantly improves the performance of our AP-BSN without any additional parameters. Extensive studies demonstrate that our method outperforms the other self-supervised and even unpaired denoising methods by a large margin, without using any additional knowledge, e.g., noise level, regarding the underlying unknown noise.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2203.11799", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LeeSL22", "doi": "10.1109/cvpr52688.2022.01720"}}, "content": {"source": {"pdf_hash": "fc58866483ad8109ac4bc64108c9f539fd97506c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2203.11799v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a694559530130491786cd54e8795fce06d9f7353", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fc58866483ad8109ac4bc64108c9f539fd97506c.txt", "contents": "\nAP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network\n\n\nWooseok Lee \nDepartment of ECE\nSeoul National University\n\n\nSanghyun Son \nDepartment of ECE\nSeoul National University\n\n\nKyoung Mu kyoungmu@snu.ac.kr \nDepartment of ECE\nSeoul National University\n\n\nLee Asri \nDepartment of ECE\nSeoul National University\n\n\nAP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network\n\nBlind-spot network (BSN) and its variants have made significant advances in self-supervised denoising. Nevertheless, they are still bound to synthetic noisy inputs due to less practical assumptions like pixel-wise independent noise. Hence, it is challenging to deal with spatially correlated real-world noise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has been proposed to remove the spatial correlation of real-world noise. However, it is not trivial to integrate PD and BSN directly, which prevents the fully self-supervised denoising model on realworld images. We propose an Asymmetric PD (AP) to address this issue, which introduces different PD stride factors for training and inference. We systematically demonstrate that the proposed AP can resolve inherent trade-offs caused by specific PD stride factors and make BSN applicable to practical scenarios. To this end, we develop AP-BSN, a state-of-the-art self-supervised denoising method for realworld sRGB images. We further propose random-replacing refinement, which significantly improves the performance of our AP-BSN without any additional parameters. Extensive studies demonstrate that our method outperforms the other self-supervised and even unpaired denoising methods by a large margin, without using any additional knowledge, e.g., noise level, regarding the underlying unknown noise.\n\nIntroduction\n\nImage denoising is one of the essential topics in the computer vision area, which aims to recover a clean image from the noisy signal. Due to its practical usage in several vision-related applications, several learning-based denoising algorithms [29,37,44,45] have been proposed with the advent of convolutional neural networks (CNNs). Conventional methods usually adopt additive white Gaussian noise (AWGN) to acquire large-scale training data by synthesizing clean-noisy image pairs for supervised learning. Never-Code is available at: https://github.com/wooseoklee4/AP-BSN (a) DnCNN [44] ( Supervised ) (b) C2N [19] + DIDN [41] ( Unpaired ) (c) NAC [40] ( Self-supervised ) (d) AP-BSN + R 3 (Ours) ( Self-supervised ) Figure 1. Visual comparison between different denoising methods on the DND benchmark [35]. (a) DnCNN is trained on realworld noisy-clean pairs from the SIDD [1] dataset. (b) C2N uses clean SIDD [1] and noisy DND [35] samples to simulate realworld noise distribution in an unsupervised manner. (c-d) Selfsupervised methods can be trained on the DND [35] noisy images directly. We mark PSNR(dB) and SSIM with respect to the ground-truth clean image for the quantitative comparison.\n\ntheless, models learned on the synthetic noise do not generalize well in practice since the characteristics of real-world noise differ much from AWGN. To overcome the limitation, several attempts have been made to construct pairs of real-world datasets like SIDD [1] and NIND [4]. Using the real-world training pairs, supervised denoising methods [8,16,21,42,43] can be trained to restore clean images from the noisy real-world input. However, constructing the real-world dataset requires massive human labor, strictly controlled environments, and complicated post-processing. In addition, it is difficult to generalize the learned model toward diverse practical scenarios as the characteristic of noise varies much for the different capturing devices.\n\nRecently, several self-supervised approaches [3,17,23,25,32,39,40] have been introduced, which do not rely on paired training data. Such methods require noisy images only for training instead of clean-noisy pairs. Among them, Blind-Spot Network (BSN) [23] is one of the representative methods motivated by Noise2Noise [26]. Under the assumption that noise signals are pixel-wise independent and zero-mean, BSN reconstructs a clean pixel from the neighboring noisy pixels without referring to the corresponding input pixel. Based on BSN, several approaches [15,24,25,38] have achieved better performance on synthetic noise while ensuring strict blindness w.r.t. the center pixel. However, real-world noises are known to be spatially-correlated [6,20,33], which does not meet the basic assumption of BSN: noise is pixel-wise independent.\n\nTo break spatial correlation of real-world noise, Zhou et al. [46] utilize pixel-shuffle downsampling (PD). PD creates a mosaic by subsampling a noisy image with a fixed stride factor, and thereby increases an actual distance between noise signals. Nevertheless, integrating PD to BSN is nontrivial when handling real-world noise in a fully selfsupervised manner, where it cannot stand alone without knowledge from additional noisy-clean synthetic pairs [38]. We identify that the principal reason for such limitation is the trade-off between the pixel-wise independent assumption and reconstruction quality. For example, a large PD stride factor (> 3) ensures the strict pixel-wise independent noise assumption and benefits BSN during training. However, it also destructs detailed structures and textures from the noisy image. In contrast, a small PD stride factor (\u2264 3) preserves image structures but cannot satisfy the pixel-wise independent assumption when training BSN.\n\nInspired by these observations, we propose Asymmetric PD (AP), which uses different stride factors for training and inference. For real-world noise, we systematically validate that a specific combination of training and inference strides can compensate shortcomings of each other. Then, we integrate AP to BSN (AP-BSN), which can learn to denoise noisy real-world inputs in a fully self-supervised manner, without requiring any prior knowledge of underlying noise. Furthermore, we propose random-replacing refinement (R 3 ), a novel post-processing method that improves the performance of our AP-BSN without any additional training. To the best of our knowledge, our AP-BSN is the first attempt to introduce self-supervised BSN for realworld sRGB noisy images. Extensive studies demonstrate that our method outperforms not only the state-of-the-art self-supervised denoising methods but also several unsupervised/unpaired approaches by a large margin. We summarize our contributions as follows:\n\n\u2022 To handle spatially correlated real-world noise in a blind fashion, we propose a novel self-supervised AP-BSN. Our framework employs asymmetric PD stride factors for training and inference in conjunction with BSN. \u2022 We propose random-replacing refinement (R 3 ), a novel post-processing method that further improves our AP-BSN without any additional parameters. \u2022 Our AP-BSN is the first self-supervised BSN that covers real-world sRGB noisy inputs and outperforms the other self-supervised and even several unpaired solutions by large margins.\n\n\nRelated Work\n\nDeep image denoising for synthetic noise. Beyond the classical non-learning based approaches [2,9,12,18], DnCNN [44] has introduced a CNN-based architecture to remove AWGN from a given image. Following DnCNN, several learning-based approaches have been proposed such as FFDNet [45], RED30 [29], and MemNet [37], with advanced network architectures. Nevertheless, the methods trained on AWGN suffer from generalization toward the real-world denoising due to domain discrepancy between real and synthetic noises. Specifically, Guo et al. [13] have demonstrated that AWGN-based denoisers do not perform well when input noise signals are signal-dependent [10] or spatially-correlated [6,20,33].\n\nReal-world image denoising. To reduce the gap between synthetic and real-world denoising, CBDNet [13] simulates in-camera ISP with gamma correction and demosaicking process. Then, synthetic heteroscedastic Gaussian noise can be transformed into realistic noise signals, which can be used to generate training pairs for supervised learning. Zhou et al. [46] have proposed pixel-shuffle downsampling (PD) to cover spatially-correlated real-world noise with conventional AWGN denoisers. In contrast, there have been a few attempts to capture the noisy-clean training pairs from real-world [1,4]. Using the real-world pairs, it is straightforward to train supervised denoising methods [8,16,21,42,43], which generalize well on the corresponding real-world inputs. However, constructing realworld pairs require huge labor and is not always available. Unpaired image denoising. When sets of unpaired clean and real-world noisy images are available, several methods leverage generative approaches [11] to synthesize realistic noise from the clean samples [5,7,14,19]. Among them, GCBD [7] selectively uses plain regions from noisy images for stable learning. Recently, C2N [19] explicitly considers various noise characteristics to simulate real-world noise more accurately. Using the generated noisy-clean pairs, the following supervised denoising model [41,44] can be trained to deal with real-world noise. On the other hand, Wu et al. [38] distill knowledge from a self-supervised denoising model while adopting synthetic noisy-clean pairs. Still, it is important to match the scene statistics of clean and noisy datasets even in the unpaired configuration [19], which can be difficult in practice.\n\nSelf-supervised denoising. A major bottleneck for realworld denoising is the absence of appropriate training data. Therefore, several approaches have been proposed to train their model using noisy images only. Motivated by Noise2Noise [26], Noise2Void [23] and Noise2Self [3] have introduced novel self-supervised learning frameworks by masking a portion of noisy pixels from the input image.\n\nNotably, the concept of BSN [23] has been later extended to more efficient architectures in the form of four halved receptive fields [25] or dilated and masked convolutions [38]. While Noise2Same [39] does not use BSN, a novel loss term is used to satisfy J -invariant property [3] in the denoising network. Neighbor2Neighbor [17], on the other hand, acquires the noisy-noisy pair for self-supervision by subsampling the given input. Nevertheless, the above self-supervised methods heavily rely on assumptions that noise signals are pixel-wise independent. Therefore, they usually end up learning identity mappings when applied to real-world sRGB images as noise signals are spatiallycorrelated [6,20,33]. Recent Noisier2Noise [30], NAC [40], and R2R [32] add different synthetic noise signals to the given input to make auxiliary training pairs. However, Noisier2Noise requires prior knowledge regarding the underlying noise distribution, and Noisy-As-Clean relies on weak noise assumptions. R2R also requires several prior information such as noise level and ISP function, which may not be available in realworld scenarios.\n\n\nBSN and PD\n\nBlind-spot network. BSN [23] is a variant of the conventional CNN that does not see the center pixel in the receptive field to predict the corresponding output pixel. Several studies [3,23,24] have demonstrated that BSN B (\u00b7) can learn to denoise a noisy image I N \u2208 R H\u00d7W in a self-supervised manner. We note that the image has a resolution of H \u00d7 W , and color channels are omitted for simplicity. To train BSN, the following two assumptions must be satisfied: noise is spatially, i.e., pixel-wise, independent and zero-mean. Under such assumptions, it is known [3,39] that minimizing the self-supervised loss L self w.r.t. BSN is equivalent to conventional supervised learning as follows:\nL self = E IN \u2225B (I N ) \u2212 I N \u2225 2 2 = E IN,IC \u2225B (I N ) \u2212 I C \u2225 2 2 + c = L super + c,(1)\nwhere I C \u2208 R H\u00d7W is a clean ground-truth for the noisy input I N , L super is a supervised denoising loss function, and c is a constant, respectively.\n\nTherefore, several types of BSN [25,38] are constructed under the pixel-wise independent noise assumption. However, real-world noise is spatially correlated due to the image signal processors (ISP). Specifically, demosaicking on Bayer filter [6,20,33] involves interpolation between noisy subpixels. Fig. 2 demonstrates that in real-world, noise intensities between neighboring pixels show non-negligible correlation based on their relative distance. Since the neighboring noise signals can be clues for inferring the unseen center pixel, we have identified that BSN operates as an approximately identity mapping on real-world sRGB images.\n\nPixel-shuffle downsampling. Zhou et al. [46] have introduced a novel concept of PD to break down the spatial correlation in the real-world noise. Specifically, PD s can be regarded as an inverse operation of the pixel-shuffling [36] with a stride factor of s. Since real-world noise signals are correlated with few neighboring pixels, subsampling in PD process may break the dependency between them. Then, conventional denoising algorithms can be applied to the downsampled images, where the PD-inverse operation PD \u22121 s follows to reconstruct a full-sized output. To preserve image textures and details, Zhou et al. [46] set the stride factor to 2, i.e. PD 2 , for the best performance.\n\n\nMethod\n\nOur goal is to generalize BSN on real-world sRGB images in a self-supervised manner. To this end, we adopt PD and minimize the following loss L BSN to train BSN:\nL BSN = PD \u22121 s (B (PD s (I N ))) \u2212 I N 1 = \u2225I s BSN \u2212 I N \u2225 1 ,(2)\nwhere I s BSN is an output from PD s and BSN pipeline, namely PD s -BSN. Instead of widely-used L 2 loss, we use L 1 norm for better generalization [27]. In brief, we first decompose the given noisy image I N into s 2 sub-images. We note that PD s (I N ) is a tiling of those sub-images [46] I s sub \u2208 R H /s\u00d7 W /s , as shown in Fig. 4. Then, we apply BSN to the sub-images and reconstruct the output I s BSN using the PD-inverse operation PD \u22121 s . However, it is not straightforward to apply PD-BSN directly on real-world sRGB images. While Wu et al. [38] have also tried to integrate PD and BSN, they resort to knowledge distillation combined with additional synthetic noisy-clean pairs. We have also observed that PD-BSN is not applicable to real-world noisy images when trained with the self-supervised loss in Eq. (2). Figs. 3c and 3d demonstrate that PD 2 -BSN and PD 5 -BSN cannot restore a clean and sharp image from the given noisy input, regardless of the PD stride factor s.\n(a) Real-world noisy image IN (b) Clean image IC (c) PD2-BSN (d) PD5-BSN (e) Zhou et al. [46] (f) AP-BSN + R 3 (Ours)\n\nTrade-offs in PD-BSN\n\nWhen applying the AWGN-based denoiser on real-world images, Zhou et al. [46] use PD 2 . However, we have observed that PD exhibits different behaviors as the stride factor s varies. Therefore, we first describe two important aspects of PD-BSN regarding the stride factor s. Breaking spatial correlation. Originally, PD has been proposed to reduce spatial correlation between neighboring noise signals in real-world images. While Zhou et al. [46] resort to the stride factor of 2, our analysis in Fig. 2a demonstrates that the stride factor should be at least 5 to minimize the dependency in the given noise signal. In other words, noise signals in the sub-images I 2 sub are still spatially cor- Figure 4. Comparison between PD2 and PD5. Each operation decomposes the given image into 4 and 25 sub-images, respectively. In sub-images from PD5, we mark the aliasing artifact, i.e. a black dot, with red, which can be interpreted as noise for BSN. We note that the artifact does not appear in the blue sub-image. related, where the pixel-wise independent noise assumption for BSN does not hold. Aliasing artifacts. Nevertheless, the sub-images I s sub from PD s suffer stronger degree of aliasing as the stride factor s becomes larger. From the perspective of signal processing, it is well-known that a downsampled image suffers aliasing when the original signal is not properly bandlimited [31]. Since the PD process does not leverage a low-pass filter before subsampling, we have identified that aliasing occurs as a form of noise when applying large-stride PD, e.g., s = 5, as shown in Fig. 4.\n\n\nEffective training stride factor for PD-BSN\n\nWe next establish a strategy to train PD s -BSN. For such purpose, the correlation between noise signals in the training input images I N has to be minimized [23]. However, as discussed in Section 4.1, PD 2 is not enough to break spatial correlation of real-world noise. Since the underlying assumption of BSN is not satisfied, the model cannot learn to denoise with PD 2 . By setting s = 5 to suppress the spatial correlation between noise signals in training samples, we can train BSN on the smaller sub-images I 5 sub . We note that BSN also learns to remove the aliasing artifacts induced by the large PD stride factor. The aliasing happens when high-frequency signals are not removed before subsampling [31]. As the high-frequency components change rapidly in the original noisy image I N , we can ignore the spatial correlation of aliasing artifacts in the sub-images I 5 sub . The artifacts also satisfy the zero-mean constraint, i.e., their statistical mean is approximately the same as that of the noisy image I N , since they are random samples of the observed signal. As the aliasing artifacts satisfy two preconditions of BSN, our PD-BSN also learns to remove them.\n\n\nAsymmetric PD for BSN\n\nSeveral studies [7,19] have already identified that matching data distribution between training and test samples play a critical role in accurate image denoising. Therefore, it is Figure 5. Overview of the proposed AP-BSN and R 3 post-processing. We visualize the proposed AP5/2-BSN. To apply BSN on realworld sRGB images, we introduce AP a/b to maximize synergies of using different stride factors for training and inference. We use a large stride factor, e.g., a = 5, to ensure pixel-wise independence between noise signals for training. During the inference, we use a minimum stride factor of b = 2 to avoid aliasing artifacts while breaking down the spatial correlation of noise to some extent. Our random-replacing refinement (R 3 ) further improves the performance of AP-BSN without any additional parameters.\n\nnatural to use the same stride factor for training and inference when applying PD-BSN. However, we have found that the learned BSN recognizes aliasing artifacts from PD 5 as noise signals to be removed during inference. Since those artifacts contain necessary information to reconstruct highfrequency details, PD 5 -BSN destructs image structures during inference while removing noise as shown in Fig. 3d.\n\nInstead, we propose an asymmetric stride factor during the inference of PD-BSN, which we refer to as Asymmetric PD (AP a/b ). We note that a and b are stride factors for training and inference, respectively. Specifically, we set b = 2 so that the sub-images I 2 sub contain minimum aliasing artifacts during inference, while the correlation between neighboring noise signals can be decreased. In Section 5, we demonstrate how each trade-off, i.e., spatial correlation and aliasing artifacts, affects the denoising performance of our method. Our BSN with the proposed AP 5/2 (AP-BSN) can learn to remove real-world noise in a self-supervised manner, while preserving image structures as shown in Fig. 3f. We also note that our AP-BSN does not require any clean samples for training and is directly applicable to sRGB noisy images in practical scenarios. Fig. 5 illustrates our asymmetric training and inference schemes for AP-BSN.\n\n\nRandom-replacing refinement\n\nEven with the smallest stride factor, PD and the following denoising step may remove some informative highfrequency components from the input, resulting in visual artifacts [46]. Therefore, Zhou et al. [46] propose PDrefinement to suppress artifacts from the PD process and enhance details of the denoising result. In PD-refinement, an i-th replaced image I Mi is formulated as follows:\nI Mi = M i \u2299 I N + (1 \u2212 M i ) \u2299 I s BSN ,(3)\nwhere M i \u2208 {0, 1} H\u00d7W is a binary mask indicating pixels to be replaced and \u2299 denotes element-wise multiplication. Here, M i is a structured binary matrix where ones are placed with a fixed stride of 2 and i M i = 1. After the replacement, each image I Mi is denoised again and averaged to reconstruct the final result I DN as follows:\nI DN = 1 T T i=1 D (I Mi ),(4)\nwhere D is the denoising model targeting pixel-wise independent noise and T is the number of masks, i.e., 2 2 = 4, for the original PD-refinement.\n\nHowever, the deterministic strategy in PD-refinement leaves a non-negligible correlation between the replaced noise signals. Specifically, a replaced noisy pixel in I Mi is always correlated with some of its neighbors, as visualized in Fig. 6a. Such correlation negatively affects the performance of the following denoising method D, which assumes spatially uncorrelated noise. Therefore, we propose an advanced random-replacing refinement (R 3 ) strategy to mitigate the limitation of PD-refinement.\n\nIn our R 3 , we adopt T randomized binary masks R i instead, which are defined as follows:\nR i (x, y) = 1, with a probability of p, 0, otherwise,(5)\nwhere (x, y) denotes an index of the element in a H \u00d7 W matrix. For Eq. (3) and Eq. (4), we adopt the randomized mask R i rather than the fixed one M i to acquire the final output. Since noisy pixels are randomly placed in the i-th replaced image I Ri , an expected correlation between two noise signals is multiplied by p, as shown in Fig. 6a. Hence, our R 3 significantly reduces the expected correlation compared to the previous PD-refinement. When we combine R 3 with AP-BSN, we do not perform PD and feed the replaced image I Ri to BSN directly because spatial correlation of noise in the input is almost negligible. Fig. 6 highlights major differences between PD-refinement and our R 3 .\n\n\nExperiment\n\n\nExperimental configurations\n\nDataset. To train and evaluate our AP-BSN, we adopt widely-used real-world image denoising datasets: SIDD [1] and DND [35]. SIDD-Medium consists of 320 real-world noisy and clean image pairs for training. For validation and performance evaluation, we adopt SIDD validation and benchmark datasets, respectively. Both contain 1,280 noisy patches with a size of 256 \u00d7 256, where the corresponding clean images are also provided for the validation set. The DND dataset does not include training images and consists of 50 real-world noisy inputs only for evaluation. Rather than using the SIDD-Medium training dataset for this case, we enjoy the advantage of a fully self-supervised learning framework and use the same data for training and performance evaluation. In other words, we train our AP-BSN on 50 noisy DND images and reconstruct the final denoising results from the same inputs. Metric. To evaluate our AP-BSN and compare it with the other denoising methods, we introduce widely-used peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) metrics. For SIDD and DND benchmarks, we upload our results to the evaluation sites to calculate the metrics. On the SIDD validation dataset, we use the cor- responding functions in skimage.metrics library and RGB color space for comparison. Implementation and optimization. We use PyTorch 1.9.0 [34] for implementation. By default, we adopt AP 5/2 and set p and T to 0.16 and 8, respectively, for the proposed R 3 . For BSN, we modify the architecture from Wu et al. [38] for efficiency. AP-BSN is trained using Adam [22] optimizer, and the initial learning rate starts from 10 \u22124 . More details are described in our supplementary material.\n\n\nAnalyzing Asymmetric PD\n\nWe first validate the effect of AP for real-world sRGB denoising. To this end, we conduct an extensive study on all possible combinations of feasible stride factors, i.e., a \u2208 {2, 3, 4, 5, 6} and b \u2208 {1, 2, 3, 4, 5, 6}, in Fig. 7a. We note that BSN cannot be trained when a = 2 due to the spatial correlation of real-world noise. With larger training stride factors a, the input noise of BSN follows pixel-wise independent assumption more strictly. Therefore, the model can learn the denoising function better, where the performances are maximized with a = 5. When a = 6 is used,  Table 1. Quantitative comparison of various denoising methods on the SIDD and DND benchmarks. We note that several supervised methods leverage SIDD noisy-clean pairs for training and perform much better than our AP-BSN, while we use noisy sRGB images only for training. By default, we report official evaluation results from SIDD and DND benchmark websites. \u22c4 and R indicate that the performances are evaluated by ourselves, or reported from R2R [32], respectively. We also mark methods with * which adopt self-ensemble strategy [27].\nMethod SIDD DND PSNR \u2191 (dB) SSIM \u2191 PSNR \u2191 (dB) SSIM \u2191\n\u2020 denotes that the model is trained on SIDD benchmark images in a fully self-supervised fashion.\n\nAP 6/b -BSN performs slightly worse since the noise in the SIDD [1] dataset show increasing correlation as shown in Fig. 2a. Interestingly, a = 6 is slightly better than a = 5 on the NIND [4] dataset, as the correlation gradually decreases w.r.t. to the relative distance between pixels. More analysis on the NIND dataset is reported in our supplementary material. During the inference, BSN cannot remove real-world noise without PD, i.e., b = 1, as it is learned on pixel-wise independent noise. The performances are maximized when b = 2, as the trade-off between spatial correlation and aliasing can be optimized. With larger inference stride factors, i.e., b > 2, AP-BSN performs worse because more image details are removed in the form of aliasing artifacts.\n\nIn Fig. 7b, we justify that the existence of aliasing artifacts is a critical factor for our denoising framework. When applying AP 5/b -BSN to the plain region illustrated in Fig. 8a, the model performs better as the inference stride factor b becomes larger. Since the region does not contain high-frequency information, aliasing artifacts do not appear in Figs. 8c, 8d, and 8e. Rather, the spatial correlation of noise signals becomes smaller with a larger b, which results in better performance. For a general image in Fig. 8b, our AP 5/b -BSN shows a similar behavior to that of Fig. 7a, while the performance drop is much severe due to the stronger aliasing artifacts as shown in Fig. 8h.  Fig. 9 shows a detailed ablation study on hyperparameters for the proposed R 3 . We first set T = 2, 4, 8 to find the optimal replacement probability p. As shown in Fig. 9a, our R 3 shows a consistent behavior where the maximum performance is achieved with p \u2248 0.16. We note that a larger p increases the expected spatial correlation of noise signals which degrades the performance. Due to the stochastic behavior, the number of randomized masks T is not limited in our R 3 , while PD-refinement can only use T = 4. Fig. 9b demonstrates that the proposed R 3 performs better than PDrefinement even with T = 2, and the performance increases as the number of randomized masks T goes higher. Since the computational complexity of R 3 is proportional to T , we set T = 8 to balance the performance and runtime.\n\n\nAnalyzing Random-Replacing Refinement\n\n\nAP-BSN for real-world denoising\n\nOur AP-BSN aims to denoise real-world sRGB images in a self-supervised manner. Table 1 compares various image denoising models on widely-used SIDD and DND benchmark datasets. Using noisy images only for training, the proposed AP-BSN + R 3 achieves the best performance among several unpaired [19,38] and selfsupervised approaches. Especially, we note that selfsupervised NAC [40] and R2R [32] are constructed on less practical assumptions like noise level is weak or ISP function is known. On the other hand, our approach adopts BSN with several observations regarding the properties of PD and real-world noise. Therefore, we do not rely on specific assumptions and show better generalization on several real-world datasets. In addition, the proposed R 3 postprocessing further improves the evaluation PSNR more than 1dB on the SIDD benchmark track without any additional parameters. Fig. 10 provides visual comparisons between several methods addressed in Table 1.\n\nFurthermore, AP-BSN can be trained on noisy samples directly, without using any clean images. Since several un-/self-supervised methods are trained on auxiliary images [32] or generated noise [19], the discrepancy between training and test distributions may result in sub-optimal solutions. In contrast, our approach can use target sRGB noisy images directly during training phase. To validate the merit of our framework, we train AP-BSN on the SIDD benchmark and evaluate on the same dataset. The last row of Table 1 shows that the fully self-supervised strategy improves the denoising performance by about 1dB without making any modifications. Although SIDD-Medium contains about \u00d760 more pixels than the benchmark split, such an improvement highlights that AP-BSN can also generalize well on practical cases where there exist noisy test samples only.\n\n\nConclusion\n\nIn this paper, we first identify several trade-offs regarding different PD stride factors in perspective of BSN. Rather than directly integrate PD and BSN, we propose asymmetric PD between training and inference to satisfy pixel-wise independent assumption while preserving image details. To this end, we propose AP-BSN, a fully selfsupervised approaches for real-world denoising. We also propose random-replacing refinement R 3 , which removes visual artifacts of AP-BSN without any additional parameters. The proposed AP-BSN + R 3 does not require any prior knowledge on real-world noise and outperforms recent selfsupervised/unsupervised denoising methods. Figure S1. Visualization of our BSN architecture. We adopt 3 \u00d7 3 and 5 \u00d7 5 Centrally Masked Convolutions [38] to implement the blind-spot network. Each Dilated Convolution module (DC) contains one 3 \u00d7 3 dilated convolution with a stride s, where s = 2 and s = 3 are used for the upper and lower path of the network, respectively. For each path, we stack 9 DC modules. The number of output channels is denoted below each convolutional layer, where 128 is used by default.\n\n\nS1. Optimization\n\nTo train our AP-BSN, we randomly crop 120\u00d7120 noisy patches from the SIDD and DND datasets, respectively. We note that 24,542, 24,784, and 24,320 patches are used in one epoch for SIDD-Medium, DND, and SIDD benchmark datasets, respectively. Each sample is augmented with random 90 \u2022 rotation and horizontal/vertical flips. Our minibatch contains the 8 augmented samples. The proposed AP-BSN is optimized for 20 epochs, where the learning rate is decayed by a factor of 10 for every 8 epochs.\n\n\nS2. Network architecture\n\nOur BSN architecture is based on Wu et al. [38], while several changes are made for simplification. Instead of the MDC modules with multiple branches of the dilated convolutions, we use a sequence of dilated convolution modules (DC) that have a single branch only. Fig. S1 visualizes a detailed architecture of the BSN used to construct our AP-BSN framework.\n\nTherefore, our network has 3.7M parameters, which are fewer than 6.6M parameters from the original BSN proposed by Wu et al. [38]. We also note that recent unsupervised/unpaired methods adopt larger denoising networks than the proposed AP-BSN. Specifically, e.g. DIDN [41] and C2N [19], MWCNN [28] has \u223c16.2M in Wu et al. [38]). Our AP-BSN w/o R 3 shows comparable results with much smaller denoising networks even our AP-BSN only uses noisy images.\n\n\nS3. Effects of aliasing artifacts\n\nTo examine the effect of aliasing artifact during the training and inference, we train our AP-BSN using clean SIDD images only. Specifically, BSN is trained to reconstruct the same image from given a clean input while not seeing the center pixel in the receptive field. We suppose that the clean images contain zero-intensity noise, which follows the two basic assumptions of BSN: noise signals are spatially uncorrelated and zero-mean. Thus, PD-BSN should learn an identity mapping if sub-images from PD do not contain any noise. However, as shown in Figs. S2b and S2c, PD 5 -BSN removes high-frequency information from the given input clean image in Fig. S2a and does not operate an identity function even on the clean image while PD 2 -BSN does not. From this observation, we can assume that PD 5 -BSN learns to remove some information during the training that does not exist in PD 2 sub-images. When we apply the proposed AP 5/2 strategy, BSN does not remove high-frequency components and preserves the image structure well, as shown in Fig. S2d. Therefore, we conclude that the aliasing artifacts prevent PD 5 -BSN from being a feasible denoising model since removing the artifacts during inference can significantly degrade the performance of PD-BSN.\n\n\nS4. AP-BSN on the NIND dataset\n\nIn Fig. 2a of our main manuscript, we have demonstrated that noise signals in the NIND [4] dataset show gradually decreasing correlations between them as their relative distance d increases. Such observation implies that the proposed AP-BSN may perform better with a = 6 or larger, as the spatial correlations between noise can be further reduced. Therefore, we analyze the trade-offs of AP a/b on the NIND dataset similar to Section 5.2 in our main manuscript. To investigate the trade-off under diverse scenes, we conduct a per-sample analysis rather than calculating the performance on the entire dataset. Fig. S3a shows several noisy images in the NIND dataset. In Fig. S3b, we also visualize the denoising results of our AP 5/2 -BSN + R 3 trained on the  Figure S2. Effects of aliasing artifacts in BSN. To validate that the advantage of AP-BSN comes from the existence of aliasing artifacts, we conduct a clean-to-clean experiment. We sample a clean image from the SIDD validation dataset for visualization.\n\nNIND dataset. Since the noise property of the NIND dataset differs from SIDD, AP 6/2 may perform slightly better on some specific samples as shown in Fig. S3c. However, we note that the performance gaps are marginal, and AP 5/2 generalizes well on various real-world datasets on average.\n\n\nS5. Qualitative results\n\n\nS5.1. Additional qualitative results\n\nSince several existing methods do not provide qualitative results on specific datasets, we could not perform extensive qualitative comparisons in our main manuscript. For example, Figs. 10d (upper figure in the 3rd column) and 10e (lower figure in the 3rd column) in our main manuscript represent results of NAC [40] on the DND benchmark and R2R [32] on the SIDD benchmark, respectively, because R2R does not provide results on the DND dataset. Fig. S4 shows additional qualitative comparison between different denoising methods on the DND [35] benchmark and SIDD [1] validation dataset.\n\n\nS5.2. Results on real-world inputs\n\nOur AP-BSN is designed to handle real-world sRGB images, where appropriate training examples, i.e., noisy-real pairs for supervised, a set of clean images for unpaired learning, may not exist. One of the major advantages of the proposed fully self-supervised framework is that we can apply our model on a single noisy test image directly without any pre-trained knowledge. To this end, we capture realworld noisy images under a high ISO condition using the recent Samsung Galaxy smartphone. Modern smartphone cameras usually incorporate software-based denoising algorithms to remove unpleasing noise from the captured scene. Therefore, we first acquire RAW data and leverage the sim-ulated camera pipeline without explicit denoising stage [1] to get the corresponding sRGB images. Fig. S5 visualizes denoising results of our method on the real-world sRGB images. Compared to the hardwarespecific in-camera denoising algorithm in Fig. S5b, our approach reconstructs much sharper edges while suppressing unwanted noise signals effectively, as shown in Fig. S5d. The proposed method also outperforms DnCNN [44] trained on SIDD [1] noisy-clean pairs, while our formulation utilizes a single noisy image only for training.\n\n\nS5.3. Qualitative improvement by R 3\n\nOur R 3 post-processing strategy significantly improves the performance of the proposed denoising method. Fig. S6 provides qualitative comparisons between AP-BSN without R 3 and AP-BSN + R 3 . Without R 3 , our AP-BSN tends to generate unpleasing blocky artifacts as shown in Fig. S6b. By using the proposed R 3 , our AP-BSN can reconstruct smooth and natural image structures without requiring any additional parameters and training. train their method on synthetic AWGN and impulse noise. During the inference, PD2 is used to break the spatial correlation of real-world noise. (d) C2N generates a realistic noisy image from the clean input, where the following denoising model, i.e., DIDN, is trained on the generated pairs. (e) Our method is directly applicable to practical sRGB noisy images in a self-supervised manner, which does not require any additional data. For quantitative comparison, we mark per-sample PSNR/SSIM w.r.t. the ground-truth image at the bottom left of each patch. We also note that ground-truth images are not available for the DND dataset.  Figure S5. AP-BSN + R 3 on noisy images captured by ourselves. (a) To avoid the in-camera denoising pipeline, we first capture RAW images with ISO 3200 using a recent Samsung Galaxy smartphone. We note that no other pre/post-processing is done on the exported .dng files. Then, we render the sRGB images using the SIDD ISP pipeline [1], which does not include the denoising process. (b) The corresponding sRGB images processed by the smartphone. We note that the recent mobile devices have adopted software-based denoising algorithms, which suppress unwanted noise from the captured images. (c) Same as Fig. 10a in our main manuscript, DnCNN is trained on the real-world SIDD pairs. (d) Results of our AP-BSN + R 3 trained on a single noisy input without any external data. We note that there exist color shifts between (a) and (b) since the simulated ISP pipeline does not know the color mappings of the actual ISP. \n\nFigure 2 .\n2Analysis of spatial correlation on real-world noise. (a) As the relative distance d between two noise signals increases, their correlation decreases. We note that different camera devices, e.g., Motorola Nexus 6 (N6) or LG G4, in the SIDD [1] dataset show similar noise behaviors in terms of spatial correlation, as illustrated with dotted lines. (b) x and y axis represent a relative distance along with horizontal and vertical directions, respectively.\n\nFigure 3 .\n3Issues on PDs-BSN when handling real-world noise. (c) With a small stride factor, PD-BSN cannot remove noise from the input IN. (d) With a large stride factor, PD-BSN destructs edge structures. (e) When AWGN denoiser meets PD [46], the model cannot completely remove real-world noise. (f) Our selfsupervised approach delivers an accurate denoising result by overcoming the limitation of combining PD and BSN.\n\nFigure 6 .\n6Comparison between PD-refinement and our R 3 . While PD-refinement adopts regular binary masks Mi with a stride of 2, our R 3 uses randomized masks Ri. (a) We compare the expected spatial correlation of noise signals in the replaced image IM i and IR i . (b) Each gray box represents a pixel from the original noisy image IN, which replaces the denoised pixel in I s BSN .\n\nFigure 7 .Figure 8 .\n78Ablation study of AP a/b -BSN on the SIDD validation dataset. We note that the proposed R 3 post-processing is not applied in these ablation studies.(a) Our AP a/b -BSN consistently achieves the best performance when b = 2. (b) We validate AP 5/b -BSN on two representative images displayed in Figs. 8a and 8b. Visual comparison of the trade-off in AP a/b -BSN. (ce) For a plain region in (a), performance of AP-BSN gradually increases as the inference stride factor b becomes larger. (f-h) For a textured region in (b), AP-BSN performs the best when b = 2 but shows decreased performance for larger b. Please refer to Fig. 7b for more details.\n\nFigure 9 .\n9Ablation study of AP-BSN + R 3 on the SIDD validation dataset. We note that AP-BSN without R 3 achieves 34.86dB on the same dataset. (a) We investigate the effect of different p for T = 2, 4, 8. (b) We fix p = 0.16 to see the effect of T in our R 3 .\n\nFigure 10 .\n10Qualitative comparison between different denoising methods on DND[35] and SIDD[1] benchmarks.(a) DnCNN is trained on the paired SIDD-Medium dataset. (b) Zhou et al. train their method on synthetic AWGN and impulse noise. The learned denoising model is then combined with PD to handle real-world noise. (c) C2N generates a realistic noisy image from the clean input, where the following denoising model, i.e., DIDN, is trained on the generated pairs. (d-e) Recent self-supervised approaches are trained on noisy images only. (f) Our method is directly learnable on the practical sRGB images. We note that the DND benchmark (Upper) provides per-sample PSNR/SSIM, while SIDD benchmark (Lower) does not, i.e., Not available.\n\nFigure S4 .\nS4) Noisy images from the NIND [4] dataset (b) AP-BSN + R 3 (Ours) Additional qualitative comparison between different methods on DND [35] benchmark and SIDD [1] validation datasets. The upper two rows are examples from the DND benchmark dataset, and the lower four rows are from the SIDD validation dataset. (a) Input noisy images. (b) Same as Fig. 10a in our main manuscript, DnCNN is trained on the paired SIDD-Medium dataset. (c) Zhou et al.\n\n( a )\naReal-world sRGB images under the high ISO condition (b) In-camera processing (c) DnCNN [44] on SIDD [1] (d) AP-BSN + R 3 (Ours)\n\nFigure S6 .\nS6Visual comparison between denoising results of AP-BSN without R 3 and with R 3 on SIDD validation dataset. (b) Even with the smallest inference stride factor (b = 2), BSN leaves unpleasing artifacts on the denoised results and cannot preserve the image structures well. (c) The proposed R 3 removes artifacts from BSN and significantly improves the denoising performances. For quantitative comparison, we also provide per-sample PSNR/SSIM w.r.t. ground-truth images at the bottom left of each patch.\n\nA high-quality denoising dataset for smartphone cameras. Abdelrahman Abdelhamed, Stephen Lin, Michael S Brown, CVPR. 1213Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In CVPR, 2018. 1, 2, 3, 6, 7, 8, 10, 12, 13\n\nAn algorithm for designing overcomplete dictionaries for sparse representation. Michal Aharon, Michael Elad, Alfred Bruckstein, . K-Svd , IEEE Transactions on Signal Processing. 5411Michal Aharon, Michael Elad, and Alfred Bruckstein. K- SVD: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Pro- cessing, 54(11):4311-4322, 2006. 2\n\nNoise2Self: Blind denoising by self-supervision. Joshua Batson, Loic Royer, ICML. 7Joshua Batson and Loic Royer. Noise2Self: Blind denoising by self-supervision. In ICML, 2019. 2, 3, 7\n\nNatural image noise dataset. Benoit Brummer, Christophe De Vleeschouwer, CVPR Workshops. 11Benoit Brummer and Christophe De Vleeschouwer. Natural image noise dataset. In CVPR Workshops, 2019. 1, 2, 7, 9, 11\n\nGAN2GAN: Generative noise learning for blind image denoising with single noisy images. Sungmin Cha, Taeeon Park, Taesup Moon, ICLR. Sungmin Cha, Taeeon Park, and Taesup Moon. GAN2GAN: Generative noise learning for blind image denoising with sin- gle noisy images. In ICLR, 2021. 2\n\nSing Bing Kang, and Yasuyuki Matsushita. Noise suppression in low-light images through joint denoising and demosaicing. Priyam Chatterjee, Neel Joshi, CVPR. 23Priyam Chatterjee, Neel Joshi, Sing Bing Kang, and Ya- suyuki Matsushita. Noise suppression in low-light images through joint denoising and demosaicing. In CVPR, 2011. 2, 3\n\nImage blind denoising with generative adversarial network based noise modeling. Jingwen Chen, Jiawei Chen, Hongyang Chao, Ming Yang, CVPR. 7Jingwen Chen, Jiawei Chen, Hongyang Chao, and Ming Yang. Image blind denoising with generative adversarial net- work based noise modeling. In CVPR, 2018. 2, 4, 7\n\nNBNet: Noise basis learning for image denoising with subspace projection. Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu, Haoqiang Fan, Shuaicheng Liu, CVPR. 1Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu, Haoqiang Fan, and Shuaicheng Liu. NBNet: Noise basis learning for image denoising with subspace projection. In CVPR, 2021. 1, 2\n\nImage denoising by sparse 3-D transformdomain collaborative filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, IEEE TIP. 1687Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-D transform- domain collaborative filtering. IEEE TIP, 16(8):2080-2095, 2007. 2, 7\n\nPractical Poissonian-Gaussian noise modeling and fitting for single-image raw-data. Alessandro Foi, Mejdi Trimeche, Vladimir Katkovnik, Karen Egiazarian, IEEE TIP. 1710Alessandro Foi, Mejdi Trimeche, Vladimir Katkovnik, and Karen Egiazarian. Practical Poissonian-Gaussian noise mod- eling and fitting for single-image raw-data. IEEE TIP, 17(10):1737-1754, 2008. 2\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NIPS. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 2\n\nWeighted nuclear norm minimization with application to image denoising. Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, CVPR. 27Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with application to image denoising. In CVPR, 2014. 2, 7\n\nToward convolutional blind denoising of real photographs. Zifei Shi Guo, Kai Yan, Wangmeng Zhang, Lei Zuo, Zhang, CVPR. 27Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei Zhang. Toward convolutional blind denoising of real pho- tographs. In CVPR, 2019. 2, 7\n\nEnd-to-end unpaired image denoising with conditional adversarial networks. Zhiwei Hong, Xiaocheng Fan, Tao Jiang, Jianxing Feng, AAAI. 2020Zhiwei Hong, Xiaocheng Fan, Tao Jiang, and Jianxing Feng. End-to-end unpaired image denoising with conditional ad- versarial networks. In AAAI, 2020. 2\n\nEfficient blind-spot neural network architecture for image denoising. David Honz\u00e1tko, A Siavash, Engin Bigdeli, L Andrea T\u00fcretken, Dunbar, 2020 7th Swiss Conference on Data Science (SDS). 2020David Honz\u00e1tko, Siavash A Bigdeli, Engin T\u00fcretken, and L Andrea Dunbar. Efficient blind-spot neural network archi- tecture for image denoising. In 2020 7th Swiss Conference on Data Science (SDS), 2020. 2\n\nPseudo 3D auto-correlation network for real image denoising. Xiaowan Hu, Ruijun Ma, Zhihong Liu, Yuanhao Cai, Xiaole Zhao, Yulun Zhang, Haoqian Wang, CVPR. 1Xiaowan Hu, Ruijun Ma, Zhihong Liu, Yuanhao Cai, Xi- aole Zhao, Yulun Zhang, and Haoqian Wang. Pseudo 3D auto-correlation network for real image denoising. In CVPR, 2021. 1, 2\n\nNeighbor2Neighbor: Self-supervised denoising from single noisy images. Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, Jianzhuang Liu, CVPR. 23Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2Neighbor: Self-supervised de- noising from single noisy images. In CVPR, 2021. 2, 3\n\nEPLL: an image denoising method using a Gaussian mixture model learned on a large set of patches. Samuel Hurault, Thibaud Ehret, Pablo Arias, Image Processing On Line. 82Samuel Hurault, Thibaud Ehret, and Pablo Arias. EPLL: an image denoising method using a Gaussian mixture model learned on a large set of patches. Image Processing On Line, 8:465-489, 2018. 2\n\nC2N: Practical generative noise modeling for real-world denoising. Geonwoon Jang, Wooseok Lee, Sanghyun Son, Kyoung Mu Lee, ICCV. 912Geonwoon Jang, Wooseok Lee, Sanghyun Son, and Ky- oung Mu Lee. C2N: Practical generative noise modeling for real-world denoising. In ICCV, 2021. 1, 2, 4, 7, 8, 9, 12\n\nA review of an old dilemma: Demosaicking first, or denoising first?. Qiyu Jin, Gabriele Facciolo, Jean-Michel Morel, CVPR Workshops. 23Qiyu Jin, Gabriele Facciolo, and Jean-Michel Morel. A re- view of an old dilemma: Demosaicking first, or denoising first? In CVPR Workshops, 2020. 2, 3\n\nTransfer learning from synthetic to real-noise denoising with adaptive instance normalization. Yoonsik Kim, Jae Woong Soh, Yong Gu, Nam Ik Park, Cho, CVPR, 2020. 1. 27Yoonsik Kim, Jae Woong Soh, Gu Yong Park, and Nam Ik Cho. Transfer learning from synthetic to real-noise denoising with adaptive instance normalization. In CVPR, 2020. 1, 2, 7\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6\n\nNoise2Void-learning denoising from single noisy images. Alexander Krull, Tim-Oliver Buchholz, Florian Jug, CVPR. Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2Void-learning denoising from single noisy images. In CVPR, 2019. 2, 3, 4, 7\n\nProbabilistic Noise2Void: Unsupervised content-aware denoising. Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, Florian Jug, Frontiers in Computer Science. 253Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, and Florian Jug. Probabilistic Noise2Void: Unsuper- vised content-aware denoising. Frontiers in Computer Sci- ence, 2:5, 2020. 2, 3\n\nHigh-quality self-supervised deep image denoising. Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila, NeurIPS. 23Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. In NeurIPS, 2019. 2, 3\n\nNoise2Noise: Learning image restoration without clean data. Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, ICML. 23Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2Noise: Learning image restoration without clean data. In ICML, 2018. 2, 3\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, CVPR Workshops. 37Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPR Workshops, 2017. 3, 7\n\nMulti-level Wavelet-CNN for image restoration. Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, Wangmeng Zuo, CVPR Workshops. 79Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and Wangmeng Zuo. Multi-level Wavelet-CNN for image restoration. In CVPR Workshops, 2018. 7, 9\n\nImage restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. Xiaojiao Mao, Chunhua Shen, Yu-Bin Yang, NIPS. 1Xiaojiao Mao, Chunhua Shen, and Yu-Bin Yang. Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. In NIPS, 2016. 1, 2\n\nNoisier2Noise: Learning to denoise from unpaired noisy data. Nick Moran, Dan Schmidt, Yu Zhong, Patrick Coady, CVPR. 2020Nick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2Noise: Learning to denoise from unpaired noisy data. In CVPR, 2020. 3\n\nSignals & systems. Alan V Oppenheim, Alan S Willsky, Syed Hamid Nawab, Gloria Mata Hern\u00e1ndez, Pearson Educaci\u00f3n. 4Alan V Oppenheim, Alan S Willsky, Syed Hamid Nawab, Gloria Mata Hern\u00e1ndez, et al. Signals & systems. Pearson Educaci\u00f3n, 1997. 4\n\nRecorrupted-to-Recorrupted: Unsupervised deep learning for image denoising. Tongyao Pang, Huan Zheng, Yuhui Quan, Hui Ji, CVPR. 10Tongyao Pang, Huan Zheng, Yuhui Quan, and Hui Ji. Recorrupted-to-Recorrupted: Unsupervised deep learning for image denoising. In CVPR, 2021. 2, 3, 7, 8, 10\n\nManu Parmar, and Brian A Wandell. A case for denoising before demosaicking color filter array data. Sung Hee Park, Suk Hyung, Steven Kim, Lansel, Asilomar Conference on Signals. 23Sung Hee Park, Hyung Suk Kim, Steven Lansel, Manu Par- mar, and Brian A Wandell. A case for denoising before de- mosaicking color filter array data. In Asilomar Conference on Signals, Systems and Computers, 2009. 2, 3\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS Workshops. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NIPS Workshops, 2017. 6\n\nBenchmarking denoising algorithms with real photographs. Tobias Plotz, Stefan Roth, CVPR. 1012Tobias Plotz and Stefan Roth. Benchmarking denoising al- gorithms with real photographs. In CVPR, 2017. 1, 6, 8, 10, 12\n\nReal-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, P Andrew, Rob Aitken, Daniel Bishop, Zehan Rueckert, Wang, CVPR. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, 2016. 3\n\nMem-Net: A persistent memory network for image restoration. Ying Tai, Jian Yang, Xiaoming Liu, Chunyan Xu, ICCV. 1Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem- Net: A persistent memory network for image restoration. In ICCV, 2017. 1, 2\n\nUnpaired learning of deep image denoising. Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, Wangmeng Zuo, ECCV. 89Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, and Wang- meng Zuo. Unpaired learning of deep image denoising. In ECCV, 2020. 2, 3, 4, 6, 7, 8, 9\n\nNoise2Same: Optimizing a self-supervised bound for image denoising. Yaochen Xie, Zhengyang Wang, Shuiwang Ji, NeurIPS, 2020. 23Yaochen Xie, Zhengyang Wang, and Shuiwang Ji. Noise2Same: Optimizing a self-supervised bound for image denoising. In NeurIPS, 2020. 2, 3\n\nNoisy-As-Clean: Learning selfsupervised denoising from corrupted image. Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, Ling Shao, IEEE TIP. 2910Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, and Ling Shao. Noisy-As-Clean: Learning self- supervised denoising from corrupted image. IEEE TIP, 29:9316-9329, 2020. 1, 2, 3, 7, 8, 10\n\nDeep iterative down-up CNN for image denoising. Songhyun Yu, Bumjun Park, Jechang Jeong, CVPR Workshops. 912Songhyun Yu, Bumjun Park, and Jechang Jeong. Deep iter- ative down-up CNN for image denoising. In CVPR Work- shops, 2019. 1, 2, 7, 8, 9, 12\n\nVariational denoising network: Toward blind noise modeling and removal. Zongsheng Yue, Hongwei Yong, Qian Zhao, Lei Zhang, Deyu Meng, NeurIPS. Zongsheng Yue, Hongwei Yong, Qian Zhao, Lei Zhang, and Deyu Meng. Variational denoising network: Toward blind noise modeling and removal. In NeurIPS, 2019. 1, 2, 7\n\nDual adversarial network: Toward real-world noise removal and noise generation. Zongsheng Yue, Qian Zhao, Lei Zhang, Deyu Meng, ECCV, 2020. 1. 27Zongsheng Yue, Qian Zhao, Lei Zhang, and Deyu Meng. Dual adversarial network: Toward real-world noise removal and noise generation. In ECCV, 2020. 1, 2, 7\n\nBeyond a Gaussian denoiser: Residual learning of deep CNN for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, IEEE TIP. 26713Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising. IEEE TIP, 26(7):3142- 3155, 2017. 1, 2, 7, 8, 10, 12, 13\n\nFFDNet: Toward a fast and flexible solution for CNN-based image denoising. Kai Zhang, Wangmeng Zuo, Lei Zhang, IEEE TIP. 279Kai Zhang, Wangmeng Zuo, and Lei Zhang. FFDNet: To- ward a fast and flexible solution for CNN-based image de- noising. IEEE TIP, 27(9):4608-4622, 2018. 1, 2\n\nWhen AWGNbased denoiser meets real noises. Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, Thomas Huang, AAAI. 812Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, and Thomas Huang. When AWGN- based denoiser meets real noises. In AAAI, 2020. 2, 3, 4, 5, 7, 8, 12\n", "annotations": {"author": "[{\"end\":158,\"start\":100},{\"end\":218,\"start\":159},{\"end\":294,\"start\":219},{\"end\":350,\"start\":295}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":108},{\"end\":171,\"start\":168},{\"end\":228,\"start\":226},{\"end\":303,\"start\":299}]", "author_first_name": "[{\"end\":107,\"start\":100},{\"end\":167,\"start\":159},{\"end\":225,\"start\":219},{\"end\":298,\"start\":295}]", "author_affiliation": "[{\"end\":157,\"start\":113},{\"end\":217,\"start\":173},{\"end\":293,\"start\":249},{\"end\":349,\"start\":305}]", "title": "[{\"end\":97,\"start\":1},{\"end\":447,\"start\":351}]", "venue": null, "abstract": "[{\"end\":1826,\"start\":449}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2092,\"start\":2088},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2095,\"start\":2092},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2098,\"start\":2095},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2101,\"start\":2098},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2432,\"start\":2428},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2460,\"start\":2456},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2472,\"start\":2468},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2498,\"start\":2494},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2652,\"start\":2648},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2723,\"start\":2720},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2760,\"start\":2757},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2779,\"start\":2775},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2915,\"start\":2911},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3310,\"start\":3307},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3323,\"start\":3320},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3394,\"start\":3391},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3397,\"start\":3394},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3400,\"start\":3397},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3403,\"start\":3400},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3406,\"start\":3403},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3846,\"start\":3843},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3849,\"start\":3846},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3852,\"start\":3849},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3855,\"start\":3852},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3858,\"start\":3855},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3861,\"start\":3858},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3864,\"start\":3861},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4053,\"start\":4049},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4120,\"start\":4116},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4358,\"start\":4354},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4361,\"start\":4358},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4364,\"start\":4361},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4367,\"start\":4364},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4544,\"start\":4541},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4547,\"start\":4544},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4550,\"start\":4547},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4701,\"start\":4697},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5093,\"start\":5089},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7266,\"start\":7263},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7268,\"start\":7266},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7271,\"start\":7268},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7274,\"start\":7271},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7286,\"start\":7282},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7451,\"start\":7447},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7463,\"start\":7459},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7480,\"start\":7476},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7710,\"start\":7706},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7825,\"start\":7821},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7853,\"start\":7850},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7856,\"start\":7853},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7859,\"start\":7856},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7963,\"start\":7959},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8218,\"start\":8214},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8451,\"start\":8448},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8453,\"start\":8451},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8546,\"start\":8543},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8549,\"start\":8546},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8552,\"start\":8549},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8555,\"start\":8552},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8558,\"start\":8555},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8856,\"start\":8852},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8913,\"start\":8910},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8915,\"start\":8913},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8918,\"start\":8915},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8921,\"start\":8918},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8943,\"start\":8940},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9032,\"start\":9028},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9214,\"start\":9210},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9217,\"start\":9214},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9297,\"start\":9293},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9519,\"start\":9515},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9797,\"start\":9793},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9814,\"start\":9810},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9833,\"start\":9830},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9984,\"start\":9980},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10089,\"start\":10085},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10129,\"start\":10125},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10152,\"start\":10148},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10233,\"start\":10230},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10282,\"start\":10278},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10650,\"start\":10647},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10653,\"start\":10650},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10656,\"start\":10653},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10683,\"start\":10679},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10693,\"start\":10689},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10707,\"start\":10703},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11120,\"start\":11116},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11278,\"start\":11275},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11281,\"start\":11278},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11284,\"start\":11281},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11659,\"start\":11656},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11662,\"start\":11659},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12063,\"start\":12059},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12066,\"start\":12063},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12272,\"start\":12269},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12275,\"start\":12272},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12278,\"start\":12275},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12712,\"start\":12708},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12900,\"start\":12896},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13289,\"start\":13285},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13748,\"start\":13744},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13887,\"start\":13883},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14153,\"start\":14149},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14800,\"start\":14796},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15169,\"start\":15165},{\"end\":15428,\"start\":15420},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16117,\"start\":16113},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16528,\"start\":16524},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17078,\"start\":17074},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17588,\"start\":17585},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17591,\"start\":17588},{\"end\":17757,\"start\":17749},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19931,\"start\":19927},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19960,\"start\":19956},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22200,\"start\":22197},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22213,\"start\":22209},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23448,\"start\":23444},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23620,\"start\":23616},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23670,\"start\":23666},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24848,\"start\":24844},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24931,\"start\":24927},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25152,\"start\":25149},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25276,\"start\":25273},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27721,\"start\":27717},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27724,\"start\":27721},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27804,\"start\":27800},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27817,\"start\":27813},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28564,\"start\":28560},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28588,\"start\":28584},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":30029,\"start\":30025},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":30978,\"start\":30974},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31420,\"start\":31416},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31563,\"start\":31559},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31576,\"start\":31572},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31588,\"start\":31584},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31617,\"start\":31613},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33159,\"start\":33156},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34754,\"start\":34750},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34788,\"start\":34784},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34982,\"start\":34978},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":35005,\"start\":35002},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":35806,\"start\":35803},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36171,\"start\":36167},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36191,\"start\":36188},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37726,\"start\":37723},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":40602,\"start\":40598},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40614,\"start\":40611}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38776,\"start\":38309},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39198,\"start\":38777},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39584,\"start\":39199},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40253,\"start\":39585},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40517,\"start\":40254},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41253,\"start\":40518},{\"attributes\":{\"id\":\"fig_7\"},\"end\":41712,\"start\":41254},{\"attributes\":{\"id\":\"fig_8\"},\"end\":41848,\"start\":41713},{\"attributes\":{\"id\":\"fig_9\"},\"end\":42363,\"start\":41849}]", "paragraph": "[{\"end\":3042,\"start\":1842},{\"end\":3796,\"start\":3044},{\"end\":4633,\"start\":3798},{\"end\":5609,\"start\":4635},{\"end\":6605,\"start\":5611},{\"end\":7153,\"start\":6607},{\"end\":7860,\"start\":7170},{\"end\":9556,\"start\":7862},{\"end\":9950,\"start\":9558},{\"end\":11077,\"start\":9952},{\"end\":11783,\"start\":11092},{\"end\":12025,\"start\":11874},{\"end\":12666,\"start\":12027},{\"end\":13355,\"start\":12668},{\"end\":13527,\"start\":13366},{\"end\":14582,\"start\":13596},{\"end\":16318,\"start\":14724},{\"end\":17543,\"start\":16366},{\"end\":18384,\"start\":17569},{\"end\":18791,\"start\":18386},{\"end\":19722,\"start\":18793},{\"end\":20140,\"start\":19754},{\"end\":20522,\"start\":20186},{\"end\":20700,\"start\":20554},{\"end\":21202,\"start\":20702},{\"end\":21294,\"start\":21204},{\"end\":22046,\"start\":21353},{\"end\":23789,\"start\":22091},{\"end\":24932,\"start\":23817},{\"end\":25083,\"start\":24987},{\"end\":25847,\"start\":25085},{\"end\":27349,\"start\":25849},{\"end\":28390,\"start\":27425},{\"end\":29245,\"start\":28392},{\"end\":30390,\"start\":29260},{\"end\":30902,\"start\":30411},{\"end\":31289,\"start\":30931},{\"end\":31740,\"start\":31291},{\"end\":33034,\"start\":31778},{\"end\":34082,\"start\":33069},{\"end\":34371,\"start\":34084},{\"end\":35025,\"start\":34438},{\"end\":36281,\"start\":35064},{\"end\":38308,\"start\":36322}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11873,\"start\":11784},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13595,\"start\":13528},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14700,\"start\":14583},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20185,\"start\":20141},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20553,\"start\":20523},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21352,\"start\":21295},{\"attributes\":{\"id\":\"formula_6\"},\"end\":24986,\"start\":24933}]", "table_ref": "[{\"end\":24405,\"start\":24398},{\"end\":27511,\"start\":27504},{\"end\":28389,\"start\":28382}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1840,\"start\":1828},{\"attributes\":{\"n\":\"2.\"},\"end\":7168,\"start\":7156},{\"attributes\":{\"n\":\"3.\"},\"end\":11090,\"start\":11080},{\"attributes\":{\"n\":\"4.\"},\"end\":13364,\"start\":13358},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14722,\"start\":14702},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16364,\"start\":16321},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17567,\"start\":17546},{\"attributes\":{\"n\":\"4.4.\"},\"end\":19752,\"start\":19725},{\"attributes\":{\"n\":\"5.\"},\"end\":22059,\"start\":22049},{\"attributes\":{\"n\":\"5.1.\"},\"end\":22089,\"start\":22062},{\"attributes\":{\"n\":\"5.2.\"},\"end\":23815,\"start\":23792},{\"attributes\":{\"n\":\"5.3.\"},\"end\":27389,\"start\":27352},{\"attributes\":{\"n\":\"5.4.\"},\"end\":27423,\"start\":27392},{\"attributes\":{\"n\":\"6.\"},\"end\":29258,\"start\":29248},{\"end\":30409,\"start\":30393},{\"end\":30929,\"start\":30905},{\"end\":31776,\"start\":31743},{\"end\":33067,\"start\":33037},{\"end\":34397,\"start\":34374},{\"end\":34436,\"start\":34400},{\"end\":35062,\"start\":35028},{\"end\":36320,\"start\":36284},{\"end\":38320,\"start\":38310},{\"end\":38788,\"start\":38778},{\"end\":39210,\"start\":39200},{\"end\":39606,\"start\":39586},{\"end\":40265,\"start\":40255},{\"end\":40530,\"start\":40519},{\"end\":41266,\"start\":41255},{\"end\":41719,\"start\":41714},{\"end\":41861,\"start\":41850}]", "table": null, "figure_caption": "[{\"end\":38776,\"start\":38322},{\"end\":39198,\"start\":38790},{\"end\":39584,\"start\":39212},{\"end\":40253,\"start\":39609},{\"end\":40517,\"start\":40267},{\"end\":41253,\"start\":40533},{\"end\":41712,\"start\":41269},{\"end\":41848,\"start\":41721},{\"end\":42363,\"start\":41864}]", "figure_ref": "[{\"end\":2571,\"start\":2563},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12333,\"start\":12327},{\"end\":13931,\"start\":13925},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15227,\"start\":15220},{\"end\":16317,\"start\":16311},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18790,\"start\":18783},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19495,\"start\":19488},{\"end\":19652,\"start\":19646},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20945,\"start\":20938},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21696,\"start\":21689},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21981,\"start\":21975},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":24047,\"start\":24040},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25208,\"start\":25201},{\"end\":25859,\"start\":25852},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26031,\"start\":26024},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26226,\"start\":26206},{\"end\":26377,\"start\":26370},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26438,\"start\":26431},{\"end\":26540,\"start\":26533},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26549,\"start\":26543},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26715,\"start\":26708},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27066,\"start\":27059},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28316,\"start\":28309},{\"end\":29929,\"start\":29920},{\"end\":31203,\"start\":31196},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32438,\"start\":32430},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32827,\"start\":32819},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33079,\"start\":33072},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33686,\"start\":33678},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33746,\"start\":33738},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33838,\"start\":33829},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34242,\"start\":34234},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":34890,\"start\":34883},{\"end\":35852,\"start\":35845},{\"end\":36001,\"start\":35993},{\"end\":36122,\"start\":36114},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36435,\"start\":36428},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36606,\"start\":36598},{\"end\":37400,\"start\":37391},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38002,\"start\":37994}]", "bib_author_first_name": "[{\"end\":42433,\"start\":42422},{\"end\":42453,\"start\":42446},{\"end\":42468,\"start\":42459},{\"end\":42732,\"start\":42726},{\"end\":42748,\"start\":42741},{\"end\":42761,\"start\":42755},{\"end\":42781,\"start\":42774},{\"end\":43092,\"start\":43086},{\"end\":43105,\"start\":43101},{\"end\":43258,\"start\":43252},{\"end\":43278,\"start\":43268},{\"end\":43281,\"start\":43279},{\"end\":43525,\"start\":43518},{\"end\":43537,\"start\":43531},{\"end\":43550,\"start\":43544},{\"end\":43839,\"start\":43833},{\"end\":43856,\"start\":43852},{\"end\":44133,\"start\":44126},{\"end\":44146,\"start\":44140},{\"end\":44161,\"start\":44153},{\"end\":44172,\"start\":44168},{\"end\":44427,\"start\":44423},{\"end\":44440,\"start\":44435},{\"end\":44453,\"start\":44447},{\"end\":44468,\"start\":44461},{\"end\":44482,\"start\":44474},{\"end\":44498,\"start\":44488},{\"end\":44770,\"start\":44762},{\"end\":44788,\"start\":44778},{\"end\":44802,\"start\":44794},{\"end\":44819,\"start\":44814},{\"end\":45126,\"start\":45116},{\"end\":45137,\"start\":45132},{\"end\":45156,\"start\":45148},{\"end\":45173,\"start\":45168},{\"end\":45429,\"start\":45426},{\"end\":45446,\"start\":45442},{\"end\":45467,\"start\":45462},{\"end\":45479,\"start\":45475},{\"end\":45489,\"start\":45484},{\"end\":45511,\"start\":45504},{\"end\":45524,\"start\":45519},{\"end\":45542,\"start\":45536},{\"end\":45812,\"start\":45805},{\"end\":45820,\"start\":45817},{\"end\":45836,\"start\":45828},{\"end\":45850,\"start\":45842},{\"end\":46077,\"start\":46072},{\"end\":46090,\"start\":46087},{\"end\":46104,\"start\":46096},{\"end\":46115,\"start\":46112},{\"end\":46358,\"start\":46352},{\"end\":46374,\"start\":46365},{\"end\":46383,\"start\":46380},{\"end\":46399,\"start\":46391},{\"end\":46644,\"start\":46639},{\"end\":46656,\"start\":46655},{\"end\":46671,\"start\":46666},{\"end\":46689,\"start\":46681},{\"end\":47034,\"start\":47027},{\"end\":47045,\"start\":47039},{\"end\":47057,\"start\":47050},{\"end\":47070,\"start\":47063},{\"end\":47082,\"start\":47076},{\"end\":47094,\"start\":47089},{\"end\":47109,\"start\":47102},{\"end\":47374,\"start\":47371},{\"end\":47391,\"start\":47382},{\"end\":47398,\"start\":47396},{\"end\":47411,\"start\":47404},{\"end\":47426,\"start\":47416},{\"end\":47703,\"start\":47697},{\"end\":47720,\"start\":47713},{\"end\":47733,\"start\":47728},{\"end\":48036,\"start\":48028},{\"end\":48050,\"start\":48043},{\"end\":48064,\"start\":48056},{\"end\":48079,\"start\":48070},{\"end\":48334,\"start\":48330},{\"end\":48348,\"start\":48340},{\"end\":48370,\"start\":48359},{\"end\":48651,\"start\":48644},{\"end\":48660,\"start\":48657},{\"end\":48666,\"start\":48661},{\"end\":48676,\"start\":48672},{\"end\":48684,\"start\":48681},{\"end\":48687,\"start\":48685},{\"end\":48938,\"start\":48937},{\"end\":48954,\"start\":48949},{\"end\":49132,\"start\":49123},{\"end\":49150,\"start\":49140},{\"end\":49168,\"start\":49161},{\"end\":49391,\"start\":49382},{\"end\":49404,\"start\":49399},{\"end\":49418,\"start\":49412},{\"end\":49433,\"start\":49428},{\"end\":49448,\"start\":49441},{\"end\":49737,\"start\":49731},{\"end\":49749,\"start\":49745},{\"end\":49764,\"start\":49758},{\"end\":49779,\"start\":49775},{\"end\":49997,\"start\":49991},{\"end\":50013,\"start\":50008},{\"end\":50027,\"start\":50024},{\"end\":50046,\"start\":50040},{\"end\":50058,\"start\":50054},{\"end\":50072,\"start\":50067},{\"end\":50086,\"start\":50082},{\"end\":50358,\"start\":50355},{\"end\":50372,\"start\":50364},{\"end\":50384,\"start\":50378},{\"end\":50398,\"start\":50390},{\"end\":50413,\"start\":50404},{\"end\":50656,\"start\":50650},{\"end\":50669,\"start\":50662},{\"end\":50680,\"start\":50677},{\"end\":50693,\"start\":50688},{\"end\":50707,\"start\":50699},{\"end\":50990,\"start\":50982},{\"end\":51003,\"start\":50996},{\"end\":51016,\"start\":51010},{\"end\":51267,\"start\":51263},{\"end\":51278,\"start\":51275},{\"end\":51290,\"start\":51288},{\"end\":51305,\"start\":51298},{\"end\":51479,\"start\":51475},{\"end\":51481,\"start\":51480},{\"end\":51497,\"start\":51493},{\"end\":51499,\"start\":51498},{\"end\":51513,\"start\":51509},{\"end\":51533,\"start\":51527},{\"end\":51538,\"start\":51534},{\"end\":51782,\"start\":51775},{\"end\":51793,\"start\":51789},{\"end\":51806,\"start\":51801},{\"end\":51816,\"start\":51813},{\"end\":52090,\"start\":52086},{\"end\":52094,\"start\":52091},{\"end\":52104,\"start\":52101},{\"end\":52118,\"start\":52112},{\"end\":52427,\"start\":52423},{\"end\":52439,\"start\":52436},{\"end\":52454,\"start\":52447},{\"end\":52472,\"start\":52465},{\"end\":52487,\"start\":52481},{\"end\":52501,\"start\":52494},{\"end\":52516,\"start\":52510},{\"end\":52527,\"start\":52522},{\"end\":52543,\"start\":52539},{\"end\":52556,\"start\":52552},{\"end\":52856,\"start\":52850},{\"end\":52870,\"start\":52864},{\"end\":53123,\"start\":53117},{\"end\":53133,\"start\":53129},{\"end\":53151,\"start\":53145},{\"end\":53168,\"start\":53160},{\"end\":53176,\"start\":53175},{\"end\":53188,\"start\":53185},{\"end\":53203,\"start\":53197},{\"end\":53217,\"start\":53212},{\"end\":53551,\"start\":53547},{\"end\":53561,\"start\":53557},{\"end\":53576,\"start\":53568},{\"end\":53589,\"start\":53582},{\"end\":53783,\"start\":53777},{\"end\":53792,\"start\":53788},{\"end\":53801,\"start\":53798},{\"end\":53814,\"start\":53807},{\"end\":53828,\"start\":53820},{\"end\":54059,\"start\":54052},{\"end\":54074,\"start\":54065},{\"end\":54089,\"start\":54081},{\"end\":54324,\"start\":54321},{\"end\":54333,\"start\":54329},{\"end\":54350,\"start\":54341},{\"end\":54360,\"start\":54358},{\"end\":54369,\"start\":54366},{\"end\":54379,\"start\":54375},{\"end\":54388,\"start\":54384},{\"end\":54666,\"start\":54658},{\"end\":54677,\"start\":54671},{\"end\":54691,\"start\":54684},{\"end\":54940,\"start\":54931},{\"end\":54953,\"start\":54946},{\"end\":54964,\"start\":54960},{\"end\":54974,\"start\":54971},{\"end\":54986,\"start\":54982},{\"end\":55256,\"start\":55247},{\"end\":55266,\"start\":55262},{\"end\":55276,\"start\":55273},{\"end\":55288,\"start\":55284},{\"end\":55550,\"start\":55547},{\"end\":55566,\"start\":55558},{\"end\":55578,\"start\":55572},{\"end\":55589,\"start\":55585},{\"end\":55599,\"start\":55596},{\"end\":55901,\"start\":55898},{\"end\":55917,\"start\":55909},{\"end\":55926,\"start\":55923},{\"end\":56154,\"start\":56148},{\"end\":56167,\"start\":56161},{\"end\":56180,\"start\":56174},{\"end\":56192,\"start\":56188},{\"end\":56202,\"start\":56199},{\"end\":56216,\"start\":56209},{\"end\":56228,\"start\":56222}]", "bib_author_last_name": "[{\"end\":42444,\"start\":42434},{\"end\":42457,\"start\":42454},{\"end\":42474,\"start\":42469},{\"end\":42739,\"start\":42733},{\"end\":42753,\"start\":42749},{\"end\":42772,\"start\":42762},{\"end\":43099,\"start\":43093},{\"end\":43111,\"start\":43106},{\"end\":43266,\"start\":43259},{\"end\":43294,\"start\":43282},{\"end\":43529,\"start\":43526},{\"end\":43542,\"start\":43538},{\"end\":43555,\"start\":43551},{\"end\":43850,\"start\":43840},{\"end\":43862,\"start\":43857},{\"end\":44138,\"start\":44134},{\"end\":44151,\"start\":44147},{\"end\":44166,\"start\":44162},{\"end\":44177,\"start\":44173},{\"end\":44433,\"start\":44428},{\"end\":44445,\"start\":44441},{\"end\":44459,\"start\":44454},{\"end\":44472,\"start\":44469},{\"end\":44486,\"start\":44483},{\"end\":44502,\"start\":44499},{\"end\":44776,\"start\":44771},{\"end\":44792,\"start\":44789},{\"end\":44812,\"start\":44803},{\"end\":44830,\"start\":44820},{\"end\":45130,\"start\":45127},{\"end\":45146,\"start\":45138},{\"end\":45166,\"start\":45157},{\"end\":45184,\"start\":45174},{\"end\":45440,\"start\":45430},{\"end\":45460,\"start\":45447},{\"end\":45473,\"start\":45468},{\"end\":45482,\"start\":45480},{\"end\":45502,\"start\":45490},{\"end\":45517,\"start\":45512},{\"end\":45534,\"start\":45525},{\"end\":45549,\"start\":45543},{\"end\":45815,\"start\":45813},{\"end\":45826,\"start\":45821},{\"end\":45840,\"start\":45837},{\"end\":45855,\"start\":45851},{\"end\":46085,\"start\":46078},{\"end\":46094,\"start\":46091},{\"end\":46110,\"start\":46105},{\"end\":46119,\"start\":46116},{\"end\":46126,\"start\":46121},{\"end\":46363,\"start\":46359},{\"end\":46378,\"start\":46375},{\"end\":46389,\"start\":46384},{\"end\":46404,\"start\":46400},{\"end\":46653,\"start\":46645},{\"end\":46664,\"start\":46657},{\"end\":46679,\"start\":46672},{\"end\":46698,\"start\":46690},{\"end\":46706,\"start\":46700},{\"end\":47037,\"start\":47035},{\"end\":47048,\"start\":47046},{\"end\":47061,\"start\":47058},{\"end\":47074,\"start\":47071},{\"end\":47087,\"start\":47083},{\"end\":47100,\"start\":47095},{\"end\":47114,\"start\":47110},{\"end\":47380,\"start\":47375},{\"end\":47394,\"start\":47392},{\"end\":47402,\"start\":47399},{\"end\":47414,\"start\":47412},{\"end\":47430,\"start\":47427},{\"end\":47711,\"start\":47704},{\"end\":47726,\"start\":47721},{\"end\":47739,\"start\":47734},{\"end\":48041,\"start\":48037},{\"end\":48054,\"start\":48051},{\"end\":48068,\"start\":48065},{\"end\":48083,\"start\":48080},{\"end\":48338,\"start\":48335},{\"end\":48357,\"start\":48349},{\"end\":48376,\"start\":48371},{\"end\":48655,\"start\":48652},{\"end\":48670,\"start\":48667},{\"end\":48679,\"start\":48677},{\"end\":48692,\"start\":48688},{\"end\":48697,\"start\":48694},{\"end\":48947,\"start\":48939},{\"end\":48961,\"start\":48955},{\"end\":48965,\"start\":48963},{\"end\":49138,\"start\":49133},{\"end\":49159,\"start\":49151},{\"end\":49172,\"start\":49169},{\"end\":49397,\"start\":49392},{\"end\":49410,\"start\":49405},{\"end\":49426,\"start\":49419},{\"end\":49439,\"start\":49434},{\"end\":49452,\"start\":49449},{\"end\":49743,\"start\":49738},{\"end\":49756,\"start\":49750},{\"end\":49773,\"start\":49765},{\"end\":49784,\"start\":49780},{\"end\":50006,\"start\":49998},{\"end\":50022,\"start\":50014},{\"end\":50038,\"start\":50028},{\"end\":50052,\"start\":50047},{\"end\":50065,\"start\":50059},{\"end\":50080,\"start\":50073},{\"end\":50091,\"start\":50087},{\"end\":50362,\"start\":50359},{\"end\":50376,\"start\":50373},{\"end\":50388,\"start\":50385},{\"end\":50402,\"start\":50399},{\"end\":50417,\"start\":50414},{\"end\":50660,\"start\":50657},{\"end\":50675,\"start\":50670},{\"end\":50686,\"start\":50681},{\"end\":50697,\"start\":50694},{\"end\":50711,\"start\":50708},{\"end\":50994,\"start\":50991},{\"end\":51008,\"start\":51004},{\"end\":51021,\"start\":51017},{\"end\":51273,\"start\":51268},{\"end\":51286,\"start\":51279},{\"end\":51296,\"start\":51291},{\"end\":51311,\"start\":51306},{\"end\":51491,\"start\":51482},{\"end\":51507,\"start\":51500},{\"end\":51525,\"start\":51514},{\"end\":51548,\"start\":51539},{\"end\":51787,\"start\":51783},{\"end\":51799,\"start\":51794},{\"end\":51811,\"start\":51807},{\"end\":51819,\"start\":51817},{\"end\":52099,\"start\":52095},{\"end\":52110,\"start\":52105},{\"end\":52122,\"start\":52119},{\"end\":52130,\"start\":52124},{\"end\":52434,\"start\":52428},{\"end\":52445,\"start\":52440},{\"end\":52463,\"start\":52455},{\"end\":52479,\"start\":52473},{\"end\":52492,\"start\":52488},{\"end\":52508,\"start\":52502},{\"end\":52520,\"start\":52517},{\"end\":52537,\"start\":52528},{\"end\":52550,\"start\":52544},{\"end\":52562,\"start\":52557},{\"end\":52862,\"start\":52857},{\"end\":52875,\"start\":52871},{\"end\":53127,\"start\":53124},{\"end\":53143,\"start\":53134},{\"end\":53158,\"start\":53152},{\"end\":53173,\"start\":53169},{\"end\":53183,\"start\":53177},{\"end\":53195,\"start\":53189},{\"end\":53210,\"start\":53204},{\"end\":53226,\"start\":53218},{\"end\":53232,\"start\":53228},{\"end\":53555,\"start\":53552},{\"end\":53566,\"start\":53562},{\"end\":53580,\"start\":53577},{\"end\":53592,\"start\":53590},{\"end\":53786,\"start\":53784},{\"end\":53796,\"start\":53793},{\"end\":53805,\"start\":53802},{\"end\":53818,\"start\":53815},{\"end\":53832,\"start\":53829},{\"end\":54063,\"start\":54060},{\"end\":54079,\"start\":54075},{\"end\":54092,\"start\":54090},{\"end\":54327,\"start\":54325},{\"end\":54339,\"start\":54334},{\"end\":54356,\"start\":54351},{\"end\":54364,\"start\":54361},{\"end\":54373,\"start\":54370},{\"end\":54382,\"start\":54380},{\"end\":54393,\"start\":54389},{\"end\":54669,\"start\":54667},{\"end\":54682,\"start\":54678},{\"end\":54697,\"start\":54692},{\"end\":54944,\"start\":54941},{\"end\":54958,\"start\":54954},{\"end\":54969,\"start\":54965},{\"end\":54980,\"start\":54975},{\"end\":54991,\"start\":54987},{\"end\":55260,\"start\":55257},{\"end\":55271,\"start\":55267},{\"end\":55282,\"start\":55277},{\"end\":55293,\"start\":55289},{\"end\":55556,\"start\":55551},{\"end\":55570,\"start\":55567},{\"end\":55583,\"start\":55579},{\"end\":55594,\"start\":55590},{\"end\":55605,\"start\":55600},{\"end\":55907,\"start\":55902},{\"end\":55921,\"start\":55918},{\"end\":55932,\"start\":55927},{\"end\":56159,\"start\":56155},{\"end\":56172,\"start\":56168},{\"end\":56186,\"start\":56181},{\"end\":56197,\"start\":56193},{\"end\":56207,\"start\":56203},{\"end\":56220,\"start\":56217},{\"end\":56234,\"start\":56229}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52059988},\"end\":42644,\"start\":42365},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7477309},\"end\":43035,\"start\":42646},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":59523708},\"end\":43221,\"start\":43037},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":173990552},\"end\":43429,\"start\":43223},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":166228760},\"end\":43711,\"start\":43431},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16977475},\"end\":44044,\"start\":43713},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":51989956},\"end\":44347,\"start\":44046},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":229923112},\"end\":44689,\"start\":44349},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1475121},\"end\":45030,\"start\":44691},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9044077},\"end\":45395,\"start\":45032},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1033682},\"end\":45731,\"start\":45397},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1663191},\"end\":46012,\"start\":45733},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49672261},\"end\":46275,\"start\":46014},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":213655883},\"end\":46567,\"start\":46277},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":220734100},\"end\":46964,\"start\":46569},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235679062},\"end\":47298,\"start\":46966},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":231419143},\"end\":47597,\"start\":47300},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":127125752},\"end\":47959,\"start\":47599},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":244426949},\"end\":48259,\"start\":47961},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":216144407},\"end\":48547,\"start\":48261},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":211506288},\"end\":48891,\"start\":48549},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6628106},\"end\":49065,\"start\":48893},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":53751136},\"end\":49316,\"start\":49067},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":173990717},\"end\":49678,\"start\":49318},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":173990648},\"end\":49929,\"start\":49680},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3846544},\"end\":50286,\"start\":49931},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6540453},\"end\":50601,\"start\":50288},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":29151865},\"end\":50874,\"start\":50603},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10987457},\"end\":51200,\"start\":50876},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":204904999},\"end\":51454,\"start\":51202},{\"attributes\":{\"id\":\"b30\"},\"end\":51697,\"start\":51456},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":235719899},\"end\":51984,\"start\":51699},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":18987544},\"end\":52383,\"start\":51986},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":40027675},\"end\":52791,\"start\":52385},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9715523},\"end\":53006,\"start\":52793},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":7037846},\"end\":53485,\"start\":53008},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":8550762},\"end\":53732,\"start\":53487},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":221376798},\"end\":53982,\"start\":53734},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":225062070},\"end\":54247,\"start\":53984},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":222154570},\"end\":54608,\"start\":54249},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":198166487},\"end\":54857,\"start\":54610},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":201667906},\"end\":55165,\"start\":54859},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":220495900},\"end\":55466,\"start\":55167},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":996788},\"end\":55821,\"start\":55468},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":10514149},\"end\":56103,\"start\":55823},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":102351079},\"end\":56417,\"start\":56105}]", "bib_title": "[{\"end\":42420,\"start\":42365},{\"end\":42724,\"start\":42646},{\"end\":43084,\"start\":43037},{\"end\":43250,\"start\":43223},{\"end\":43516,\"start\":43431},{\"end\":43831,\"start\":43713},{\"end\":44124,\"start\":44046},{\"end\":44421,\"start\":44349},{\"end\":44760,\"start\":44691},{\"end\":45114,\"start\":45032},{\"end\":45424,\"start\":45397},{\"end\":45803,\"start\":45733},{\"end\":46070,\"start\":46014},{\"end\":46350,\"start\":46277},{\"end\":46637,\"start\":46569},{\"end\":47025,\"start\":46966},{\"end\":47369,\"start\":47300},{\"end\":47695,\"start\":47599},{\"end\":48026,\"start\":47961},{\"end\":48328,\"start\":48261},{\"end\":48642,\"start\":48549},{\"end\":48935,\"start\":48893},{\"end\":49121,\"start\":49067},{\"end\":49380,\"start\":49318},{\"end\":49729,\"start\":49680},{\"end\":49989,\"start\":49931},{\"end\":50353,\"start\":50288},{\"end\":50648,\"start\":50603},{\"end\":50980,\"start\":50876},{\"end\":51261,\"start\":51202},{\"end\":51473,\"start\":51456},{\"end\":51773,\"start\":51699},{\"end\":52084,\"start\":51986},{\"end\":52421,\"start\":52385},{\"end\":52848,\"start\":52793},{\"end\":53115,\"start\":53008},{\"end\":53545,\"start\":53487},{\"end\":53775,\"start\":53734},{\"end\":54050,\"start\":53984},{\"end\":54319,\"start\":54249},{\"end\":54656,\"start\":54610},{\"end\":54929,\"start\":54859},{\"end\":55245,\"start\":55167},{\"end\":55545,\"start\":55468},{\"end\":55896,\"start\":55823},{\"end\":56146,\"start\":56105}]", "bib_author": "[{\"end\":42446,\"start\":42422},{\"end\":42459,\"start\":42446},{\"end\":42476,\"start\":42459},{\"end\":42741,\"start\":42726},{\"end\":42755,\"start\":42741},{\"end\":42774,\"start\":42755},{\"end\":42784,\"start\":42774},{\"end\":43101,\"start\":43086},{\"end\":43113,\"start\":43101},{\"end\":43268,\"start\":43252},{\"end\":43296,\"start\":43268},{\"end\":43531,\"start\":43518},{\"end\":43544,\"start\":43531},{\"end\":43557,\"start\":43544},{\"end\":43852,\"start\":43833},{\"end\":43864,\"start\":43852},{\"end\":44140,\"start\":44126},{\"end\":44153,\"start\":44140},{\"end\":44168,\"start\":44153},{\"end\":44179,\"start\":44168},{\"end\":44435,\"start\":44423},{\"end\":44447,\"start\":44435},{\"end\":44461,\"start\":44447},{\"end\":44474,\"start\":44461},{\"end\":44488,\"start\":44474},{\"end\":44504,\"start\":44488},{\"end\":44778,\"start\":44762},{\"end\":44794,\"start\":44778},{\"end\":44814,\"start\":44794},{\"end\":44832,\"start\":44814},{\"end\":45132,\"start\":45116},{\"end\":45148,\"start\":45132},{\"end\":45168,\"start\":45148},{\"end\":45186,\"start\":45168},{\"end\":45442,\"start\":45426},{\"end\":45462,\"start\":45442},{\"end\":45475,\"start\":45462},{\"end\":45484,\"start\":45475},{\"end\":45504,\"start\":45484},{\"end\":45519,\"start\":45504},{\"end\":45536,\"start\":45519},{\"end\":45551,\"start\":45536},{\"end\":45817,\"start\":45805},{\"end\":45828,\"start\":45817},{\"end\":45842,\"start\":45828},{\"end\":45857,\"start\":45842},{\"end\":46087,\"start\":46072},{\"end\":46096,\"start\":46087},{\"end\":46112,\"start\":46096},{\"end\":46121,\"start\":46112},{\"end\":46128,\"start\":46121},{\"end\":46365,\"start\":46352},{\"end\":46380,\"start\":46365},{\"end\":46391,\"start\":46380},{\"end\":46406,\"start\":46391},{\"end\":46655,\"start\":46639},{\"end\":46666,\"start\":46655},{\"end\":46681,\"start\":46666},{\"end\":46700,\"start\":46681},{\"end\":46708,\"start\":46700},{\"end\":47039,\"start\":47027},{\"end\":47050,\"start\":47039},{\"end\":47063,\"start\":47050},{\"end\":47076,\"start\":47063},{\"end\":47089,\"start\":47076},{\"end\":47102,\"start\":47089},{\"end\":47116,\"start\":47102},{\"end\":47382,\"start\":47371},{\"end\":47396,\"start\":47382},{\"end\":47404,\"start\":47396},{\"end\":47416,\"start\":47404},{\"end\":47432,\"start\":47416},{\"end\":47713,\"start\":47697},{\"end\":47728,\"start\":47713},{\"end\":47741,\"start\":47728},{\"end\":48043,\"start\":48028},{\"end\":48056,\"start\":48043},{\"end\":48070,\"start\":48056},{\"end\":48085,\"start\":48070},{\"end\":48340,\"start\":48330},{\"end\":48359,\"start\":48340},{\"end\":48378,\"start\":48359},{\"end\":48657,\"start\":48644},{\"end\":48672,\"start\":48657},{\"end\":48681,\"start\":48672},{\"end\":48694,\"start\":48681},{\"end\":48699,\"start\":48694},{\"end\":48949,\"start\":48937},{\"end\":48963,\"start\":48949},{\"end\":48967,\"start\":48963},{\"end\":49140,\"start\":49123},{\"end\":49161,\"start\":49140},{\"end\":49174,\"start\":49161},{\"end\":49399,\"start\":49382},{\"end\":49412,\"start\":49399},{\"end\":49428,\"start\":49412},{\"end\":49441,\"start\":49428},{\"end\":49454,\"start\":49441},{\"end\":49745,\"start\":49731},{\"end\":49758,\"start\":49745},{\"end\":49775,\"start\":49758},{\"end\":49786,\"start\":49775},{\"end\":50008,\"start\":49991},{\"end\":50024,\"start\":50008},{\"end\":50040,\"start\":50024},{\"end\":50054,\"start\":50040},{\"end\":50067,\"start\":50054},{\"end\":50082,\"start\":50067},{\"end\":50093,\"start\":50082},{\"end\":50364,\"start\":50355},{\"end\":50378,\"start\":50364},{\"end\":50390,\"start\":50378},{\"end\":50404,\"start\":50390},{\"end\":50419,\"start\":50404},{\"end\":50662,\"start\":50650},{\"end\":50677,\"start\":50662},{\"end\":50688,\"start\":50677},{\"end\":50699,\"start\":50688},{\"end\":50713,\"start\":50699},{\"end\":50996,\"start\":50982},{\"end\":51010,\"start\":50996},{\"end\":51023,\"start\":51010},{\"end\":51275,\"start\":51263},{\"end\":51288,\"start\":51275},{\"end\":51298,\"start\":51288},{\"end\":51313,\"start\":51298},{\"end\":51493,\"start\":51475},{\"end\":51509,\"start\":51493},{\"end\":51527,\"start\":51509},{\"end\":51550,\"start\":51527},{\"end\":51789,\"start\":51775},{\"end\":51801,\"start\":51789},{\"end\":51813,\"start\":51801},{\"end\":51821,\"start\":51813},{\"end\":52101,\"start\":52086},{\"end\":52112,\"start\":52101},{\"end\":52124,\"start\":52112},{\"end\":52132,\"start\":52124},{\"end\":52436,\"start\":52423},{\"end\":52447,\"start\":52436},{\"end\":52465,\"start\":52447},{\"end\":52481,\"start\":52465},{\"end\":52494,\"start\":52481},{\"end\":52510,\"start\":52494},{\"end\":52522,\"start\":52510},{\"end\":52539,\"start\":52522},{\"end\":52552,\"start\":52539},{\"end\":52564,\"start\":52552},{\"end\":52864,\"start\":52850},{\"end\":52877,\"start\":52864},{\"end\":53129,\"start\":53117},{\"end\":53145,\"start\":53129},{\"end\":53160,\"start\":53145},{\"end\":53175,\"start\":53160},{\"end\":53185,\"start\":53175},{\"end\":53197,\"start\":53185},{\"end\":53212,\"start\":53197},{\"end\":53228,\"start\":53212},{\"end\":53234,\"start\":53228},{\"end\":53557,\"start\":53547},{\"end\":53568,\"start\":53557},{\"end\":53582,\"start\":53568},{\"end\":53594,\"start\":53582},{\"end\":53788,\"start\":53777},{\"end\":53798,\"start\":53788},{\"end\":53807,\"start\":53798},{\"end\":53820,\"start\":53807},{\"end\":53834,\"start\":53820},{\"end\":54065,\"start\":54052},{\"end\":54081,\"start\":54065},{\"end\":54094,\"start\":54081},{\"end\":54329,\"start\":54321},{\"end\":54341,\"start\":54329},{\"end\":54358,\"start\":54341},{\"end\":54366,\"start\":54358},{\"end\":54375,\"start\":54366},{\"end\":54384,\"start\":54375},{\"end\":54395,\"start\":54384},{\"end\":54671,\"start\":54658},{\"end\":54684,\"start\":54671},{\"end\":54699,\"start\":54684},{\"end\":54946,\"start\":54931},{\"end\":54960,\"start\":54946},{\"end\":54971,\"start\":54960},{\"end\":54982,\"start\":54971},{\"end\":54993,\"start\":54982},{\"end\":55262,\"start\":55247},{\"end\":55273,\"start\":55262},{\"end\":55284,\"start\":55273},{\"end\":55295,\"start\":55284},{\"end\":55558,\"start\":55547},{\"end\":55572,\"start\":55558},{\"end\":55585,\"start\":55572},{\"end\":55596,\"start\":55585},{\"end\":55607,\"start\":55596},{\"end\":55909,\"start\":55898},{\"end\":55923,\"start\":55909},{\"end\":55934,\"start\":55923},{\"end\":56161,\"start\":56148},{\"end\":56174,\"start\":56161},{\"end\":56188,\"start\":56174},{\"end\":56199,\"start\":56188},{\"end\":56209,\"start\":56199},{\"end\":56222,\"start\":56209},{\"end\":56236,\"start\":56222}]", "bib_venue": "[{\"end\":42480,\"start\":42476},{\"end\":42822,\"start\":42784},{\"end\":43117,\"start\":43113},{\"end\":43310,\"start\":43296},{\"end\":43561,\"start\":43557},{\"end\":43868,\"start\":43864},{\"end\":44183,\"start\":44179},{\"end\":44508,\"start\":44504},{\"end\":44840,\"start\":44832},{\"end\":45194,\"start\":45186},{\"end\":45555,\"start\":45551},{\"end\":45861,\"start\":45857},{\"end\":46132,\"start\":46128},{\"end\":46410,\"start\":46406},{\"end\":46755,\"start\":46708},{\"end\":47120,\"start\":47116},{\"end\":47436,\"start\":47432},{\"end\":47765,\"start\":47741},{\"end\":48089,\"start\":48085},{\"end\":48392,\"start\":48378},{\"end\":48712,\"start\":48699},{\"end\":48971,\"start\":48967},{\"end\":49178,\"start\":49174},{\"end\":49483,\"start\":49454},{\"end\":49793,\"start\":49786},{\"end\":50097,\"start\":50093},{\"end\":50433,\"start\":50419},{\"end\":50727,\"start\":50713},{\"end\":51027,\"start\":51023},{\"end\":51317,\"start\":51313},{\"end\":51567,\"start\":51550},{\"end\":51825,\"start\":51821},{\"end\":52162,\"start\":52132},{\"end\":52578,\"start\":52564},{\"end\":52881,\"start\":52877},{\"end\":53238,\"start\":53234},{\"end\":53598,\"start\":53594},{\"end\":53838,\"start\":53834},{\"end\":54107,\"start\":54094},{\"end\":54403,\"start\":54395},{\"end\":54713,\"start\":54699},{\"end\":55000,\"start\":54993},{\"end\":55308,\"start\":55295},{\"end\":55615,\"start\":55607},{\"end\":55942,\"start\":55934},{\"end\":56240,\"start\":56236}]"}}}, "year": 2023, "month": 12, "day": 17}