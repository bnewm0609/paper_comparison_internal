{"id": 234482824, "updated": "2023-10-06 03:06:24.011", "metadata": {"title": "VSR: A Unified Framework for Document Layout Analysis combining Vision, Semantics and Relations", "authors": "[{\"first\":\"Peng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Can\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Qiao\",\"middle\":[]},{\"first\":\"Zhanzhan\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Shiliang\",\"last\":\"Pu\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Niu\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Wu\",\"middle\":[]}]", "venue": "Document Analysis and Recognition \u2013 ICDAR 2021", "journal": "Document Analysis and Recognition \u2013 ICDAR 2021", "publication_date": {"year": 2021, "month": 5, "day": 13}, "abstract": "Document layout analysis is crucial for understanding document structures. On this task, vision and semantics of documents, and relations between layout components contribute to the understanding process. Though many works have been proposed to exploit the above information, they show unsatisfactory results. NLP-based methods model layout analysis as a sequence labeling task and show insufficient capabilities in layout modeling. CV-based methods model layout analysis as a detection or segmentation task, but bear limitations of inefficient modality fusion and lack of relation modeling between layout components. To address the above limitations, we propose a unified framework VSR for document layout analysis, combining vision, semantics and relations. VSR supports both NLP-based and CV-based methods. Specifically, we first introduce vision through document image and semantics through text embedding maps. Then, modality-specific visual and semantic features are extracted using a two-stream network, which are adaptively fused to make full use of complementary information. Finally, given component candidates, a relation module based on graph neural network is incorported to model relations between components and output final results. On three popular benchmarks, VSR outperforms previous models by large margins. Code will be released soon.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.06220", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icdar/ZhangLQCPN021", "doi": "10.1007/978-3-030-86549-8_8"}}, "content": {"source": {"pdf_hash": "5c5bc5f9e82ddfa26f64b919e4bac24a59bca514", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.06220v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2105.06220", "status": "GREEN"}}, "grobid": {"id": "ec3b041d370cc61cb3e7db59a2807418d4fda088", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5c5bc5f9e82ddfa26f64b919e4bac24a59bca514.txt", "contents": "\nVSR: A Unified Framework for Document Layout Analysis combining Vision, Semantics and Relations\n\n\nPeng Zhang zhangpeng23@hikvision.com \nHikvision Research Institute\nChina\n\nCan Li lican9@hikvision.com \nHikvision Research Institute\nChina\n\nLiang Qiao qiaoliang6@hikvision.com \nHikvision Research Institute\nChina\n\nZhanzhan Cheng chengzhanzhan@hikvision.com \nHikvision Research Institute\nChina\n\nZhejiang University\nChina\n\nShiliang Pu pushiliang.hri@hikvision.com \nHikvision Research Institute\nChina\n\nYi Niu niuyi@hikvision.com \nHikvision Research Institute\nChina\n\nFei Wu wufei@cs.zju.edu.cn \nZhejiang University\nChina\n\nVSR: A Unified Framework for Document Layout Analysis combining Vision, Semantics and Relations\nVision \u00b7 Semantics \u00b7 Relations \u00b7 Document layout analysis\nDocument layout analysis is crucial for understanding document structures. On this task, vision and semantics of documents, and relations between layout components contribute to the understanding process. Though many works have been proposed to exploit the above information, they show unsatisfactory results. NLP-based methods model layout analysis as a sequence labeling task and show insufficient capabilities in layout modeling. CV-based methods model layout analysis as a detection or segmentation task, but bear limitations of inefficient modality fusion and lack of relation modeling between layout components. To address the above limitations, we propose a unified framework VSR for document layout analysis, combining vision, semantics and relations. VSR supports both NLP-based and CV-based methods. Specifically, we first introduce vision through document image and semantics through text embedding maps. Then, modality-specific visual and semantic features are extracted using a two-stream network, which are adaptively fused to make full use of complementary information. Finally, given component candidates, a relation module based on graph neural network is incorported to model relations between components and output final results. On three popular benchmarks, VSR outperforms previous models by large margins. Code will be released soon.\n\nIntroduction\n\nDocument layout analysis is a crucial step in automatic document understanding and enables many important applications, such as document retrieval [4], digitization [7] and editing. Its goal is to identify the regions of interest in unstructured document and recognize the role of each region. This task is challenging due to the diversity and complexity of document layouts.\n\nMany deep learning models have been proposed on this task in both computer vision (CV) and natural language processing (NLP) communities. Most of them consider either only visual features [12,5,36,10,34,41,19,21] or only semantic features [6,17,27]. However, information from both modalities could help recognize the document layout better. Some regions (e.g., Figure, Table) can be easily identified by visual features, while semantic features are important for separating visually similar regions (e.g.,Abstract and Paragraph). Therefore, some recent efforts try to combine both modalities [1,20,39,3]. Here we summarize them into two categories.\n\nNLP-based methods (Fig 1 (a)) model layout analysis as a sequence labeling task and apply a bottom-up strategy. They first serialize texts into 1D token sequence 1 . Then, using both semantic and visual features (such as coordinates and image embedding) of each token, they determine token labels sequentially through a sequence labeling model. However, NLP-based methods show insufficient capabilities in layout modeling. For example in Fig. 1(a), all texts in a paragraph should have consistent semantic labels (Paragraph), but some of them are recognized as Figure Caption, which are the labels of adjacent texts.\n\nCV-based methods (Fig 1 (b)) model layout analysis as object detection or segmentation task, and apply a top-down strategy. They first extract visual features by convolutional neural network and introduce semantic features through text embedding maps (at sentence-level [39] or character-level [3]), which are directly concatenated as the representation of document. Then, detection or segmentation models (e.g., Mask RCNN [13]) are used to generate layout component candidates (coordinates and semantic labels). While capturing spatial information better compared to NLP-based methods, CV-based methods still have 3 limitations: (1) limited semantics. Semantic information are embedded in text at different granularities, including characters (or words) and sentences, which could help identify different document elements. For example, characterlevel features are better for recognizing components which need less context (e.g., Author) while sentence-level features are better for contextual components (e.g., Table caption). Exploiting semantics at one granularity could not achieve optimal performances. (2) simple and heuristic modality fusion strategy. Features from different modalities contribute differently to component recognition. Visual features contribute more to recognizing visually rich components (such as Figure and Table), while semantic features are better at distinguishing text-based components (Abstract and Paragraph). Simple and heuristic modality fusion by concatenation can not fully make use of complementary information between two modalities. (3) lack of relation modeling between components. Strong relations exist in documents. For example, \" Figure\" and \"Figure Caption\" often appear together, and \"Paragraph\"s have aligned bounding box coordinates. Such relations could be utilized to boost layout analysis performances.\n\nIn this paper, we propose a unified framework VSR for document layout analysis, combining Vision, Semantics and Relation modeling, as shown in Fig 1  (c). This framework can be applied to both NLP-based and CV-based methods. First, documents are fed into VSR in the form of images (vision) and text embedding maps (semantics at both character-level and sentence-level). Then, modality-specific visual and semantic features are extracted through a two-stream network, which are effectively combined later in a multi-scale adaptive aggregation module. Finally, a GNN(Graph Neural Network)-based relation module is incorporated to model relations between component candidates, and generate final results. Specifically, for NLP-based methods, text tokens serve as the component candidates and relation module predicts their semantic labels. While for CV-based methods, component candidates are proposed by detection or segmentation model (e.g., Faster RCNN/ Mask RCNN) and relation module generates their refined coordinates and semantic labels.\n\nOur work makes four key contributions:\n\n-We propose a unified framework VSR for document layout analysis, combining vision, semantics and relations in documents. -To exploit vision and semantics effectively, we propose a two-stream network to extract modality-specific visual and semantic features, and fuse them adaptively through an adaptive aggregation module. Besides, we also explore document semantics at different granularities. -A GNN-based relation module is incorporated to model relations between document components, and it supports relation modeling in both NLP-based and CV-based methods. -We perform extensive evaluations of VSR, and on three public benchmarks, VSR shows significant improvements compared to previous models.\n\n\nRelated Works\n\nDocument Layout Analysis. In this paper, we try to review layout analysis works from the perspective of modality used, namely, unimodal layout analysis and multimodal layout analysis. Unimodal layout analysis exploits either only visual features [21,19] (document image) or only semantic features (document texts) to understand document structures. Using visual features, several works [5,36] have been proposed to apply CNN to segment various objects, e.g., text blocks [10], text lines [34,18], words [41], figures or tables [12,29]. At the same time, there are also methods [6,17,27] which try to address the layout analysis problem using semantic features. However, all the above methods are strictly restricted to visual or semantic features, and thus are not able to exploit complementary information from other modalities.\n\nMultimodal layout analysis tries to combine information from both visual and semantic modalities. Related methods can be further divided into two categories, NLP-based and CV-based methods. NLP-based methods work on low-level elements (e.g., tokens) and model layout analysis as a sequence labeling task. MMPAN [1] is presented to recognize form structures. DocBank [20] is proposed as a large scale dataset of multimodal layout analysis and several NLP baselines have been released. However, the above methods show insufficient capabilities in layout modeling. CV-based methods introduce document semantics through text embedding maps, and model layout analysis as object detection or segmentation task. MFCN [39] introduces sentence granularity semantics and inserts the text embedding maps at the decision-level (end of network), while dhSegment T 2 [3] introduces character granularity semantics and inserts text embedding maps at the input-level. Though showing great success, the above methods also bear the following limitations: limited semantics used, simple modality fusion strategy and lack of relation modeling between components.\n\nTo remedy the above limitations, we propose a unified framework VSR to exploit vision, semantics and relations in documents.\n\nTwo-stream networks. Two-stream networks are widely used to combine features in different modalities or representations [2] effectively. In action recognition, two-stream networks are used to capture the complementary spatial and temporal information [9]. In RGB-D saliency detection, the complete representations are fused from the deep features of the RGB stream and depth stream [11]. Also, two-stream networks are used to fuse different features of same input sample in sound event classification and image recognition [23]. Motivated by their successes, we apply two-stream networks to capture complementary vision and semantics information in documents.\n\nRelation modeling. Relation modeling is a broad topic and has been studing for decades. In natural language processing, dependencies between sequential\nVisual Stream Semantic Stream CharGrid SentGrid Image Multi-scale Adaptive Aggregation Module S 2 S 3 S 4 V 2 V 3 V 4\n\nRelation Module\n\nHidden Layers texts are captured through RNN [15] or Transformer [32] architectures. In computer vision, non-local networks [35] and relation networks [16] are presented to model long-range dependencies between pixels and objects. Besides, in document image processing, relations between text and layout [38] or relations between document entities [40,24,42] are explored. As to multimodal layout analysis, NLP-based methods model it as a sequence labeling task and use RNN to capture component relations, while CV-based methods model it as object detection task but lack relation modeling between layout components. In this paper, we propose a GNN-based relation module, supporting relation modeling in both NLP-based or CV-based methods.\nFM 2 FM 3 FM 4 FM i [V i ,S i ] AM i Component Candidates Generation\n\nMethodology\n\n\nArchitecture Overview\n\nOur proposed framework has three parts: two-stream ConvNets, a multi-scale adaptive aggregation module and a relation module (as shown in Fig 2). First, a two-stream convolutional network extracts modality-specific visual and semantic features, where visual stream and semantic stream take images and text embedding maps as input, respectively (Sec 3.2). Next, instead of simply concatenating the visual and semantic features, we aggregate them via a multi-scale adaptive aggregation module (Sec 3.3). Then, a set of component candidates are generated. Finally, a relation module is incorporated to model relations among those candidates and generate final results (Sec 3.4). Notice that multimodal layout analysis can be modeled as sequence labeling (NLP-based methods) or object detection tasks (CV-based methods). Our framework supports both modeling types. The only difference is what the component candidates are and how to generate them. Component candidates are low-level elements (e.g., text tokens) in NLP-based methods and can be generated by parsing PDFs, while candidates are high-level elements (regions) generated by detection or segmentation model (e.g., Mask RCNN) in CV-based methods. In the rest of this paper, we will illustrate how VSR is applied to CV-based methods, and show it can be easily adapted to NLP-based methods in experiments on DocBank benchmark (Sec 4.3).\n\n\nTwo-stream ConvNets\n\nCNN is known to be good at learning deep features. However, previous multimodal layout analysis works [39,3] only apply it to extract visual features. Text embedding maps are directly used as semantic features. This single-stream network design could not make full use of document semantics. Motivated by great success of two-stream network in various multimodal applications [9,23], we apply it to extract deep visual and semantic features.\n\nVisual stream ConvNet. This stream directly takes document images as input and extracts multi-scale deep features using CNN backbones like ResNet [14]. Specifically, for an input image x \u2208 R H\u00d7W \u00d73 , multi-scale features maps (denoted by\n{V 2 , V 3 , V 4 , V 5 }) are extracted, where each V i \u2208 R H 2 i \u00d7 W 2 i \u00d7C V i . H and W are the height and width of input image x, C V i is the channel dimension of feature map V i , and V 0 = x.\nSemantic stream ConvNet. Similar to [39,3], we introduce document semantics through text embedding maps S 0 \u2208 R H\u00d7W \u00d7C S 0 , which are the input of semantic stream ConvNet. S 0 have same spatial sizes with document image x (V 0 ) and C S 0 denotes the initial channel dimension. This type of representation not only encodes text content, but also preserves the 2D layout of a document. Previously, only semantics at one granularity is used (character-level [3] or sentence-level 3 [39]). However, semantics at different granularities contribute to identification of different components. Thus, S 0 consists of both character and sentence level semantics. Next, we show how we build text embedding maps S 0 .\n\nThe characters and sentences of a document page are denoted as D c = {(c k , b c k ) |k = 0, \u00b7 \u00b7 \u00b7 , n} and D s = {(s k , b s k ) |k = 0, \u00b7 \u00b7 \u00b7 , m}, where n and m are the total number of characters and sentences. c k and b c k = (x 0 , y 0 , x 1 , y 1 ) are the k-th character and its associated box, where (x 0 , y 0 ) and (x 1 , y 1 ) are top-left and bottom-right pixel coordinates. Similarly, s k and b s k are the k-th sentence and its box location. Next, character embedding maps CharGrid \u2208 R H\u00d7W \u00d7C S 0 and sentence embedding maps SentGrid \u2208 R H\u00d7W \u00d7C S 0 can be constructed as follows.\nCharGrid ij = E c (c k ) if (i, j) \u2208 b c k 0 otherwise (1) SentGrid ij = E s (s k ) if (i, j) \u2208 b s k 0 otherwise (2)\nAll pixels in each b c k (b s k ) share the same character (sentence) embedding vector. E c and E s are the mapping functions of c k \u2192 R C S 0 and s k \u2192 R C S 0 . In our implementation, E c is a typical word embedding layer and we adopt pretrained language model BERT [8] as E s . Finally, the text embedding maps S 0 can be constructed by applying LayerNorm normalization to the summation of CharGrid and SentGrid, as shown in Eq. (3).\nS 0 = LayerN orm (CharGrid + SentGrid)(3)\nSimilar to the visual stream, semantic stream ConvNet then takes text embedding maps S 0 as input and extracts multi-scale features {S 2 , S 3 , S 4 , S 5 }, which have the same spatial sizes and channel dimension with\n{V 2 , V 3 , V 4 , V 5 }.\n\nMulti-scale Adaptive Aggregation\n\nFeatures from different modalities are important for identifying different objects. Modality fusion strategy should adaptively aggregate visual and semantic features. Thus, we design a multi-scale adaptive aggregation module that learns an attention map to combine visual features {V 2 , V 3 , V 4 , V 5 } and semantic features {S 2 , S 3 , S 4 , S 5 } adaptively. At scale i, this module first concatenates V i and S i , and then feed it into a convolutional layer to learn an attention map AM i . Finally, aggregated multi-modal features F M i is obtained. All operations in this module are formulated by:\nAM i = h (g ([V i , S i ]))(4)F M i = AM i V i + (1 \u2212 AM i ) S i(5)\nwhere [\u00b7] denotes the concatenation operation, g (\u00b7) is a convolutional layer with kernel size 1 \u00d7 1 \u00d7 C V i + C S i \u00d7 C S i and h (\u00b7) is a non-linear activation function. denotes the element-wise multiplication. Through this module, a set of fused multi-modal features F M = {F M 2 , F M 3 , F M 4 , F M 5 } are generated, which serve as the multimodal multi-scale features of a document. Then, FPN [22] (feature pyramid network) is applied on F M and provides enhanced representations.\n\n\nRelation Module\n\nGiven aggregated features F M = {F M 2 , F M 3 , F M 4 , F M 5 }, a standard object detection or segmentation model (e.g., Mask RCNN [26]) can be used to generate component candidates in a document. Previous works directly take those predictions as final results. However, strong relations exist between layout components. For example, bounding boxes of Paragraphs in the same column should be aligned; Table and Table Caption often appear together; there is no overlap between components. We find that such relations can be utilized to further refine predictions, as shown in Fig 3, i.e., adjusting regression coordinates for aligned bounding boxes, correcting wrong prediction labels based on co-occurrence of  components and removing false predictions based on non-overlapping property. Next, we show how we use GNN (graph neural network) to model component relations and how to use it to refine prediction results.\n\nWe represent a document as a graph G = (O, E), where O = {o 1 , \u00b7 \u00b7 \u00b7 , o N } is the node set and E is the edge set. Each node o j represents a component candidate generated by the object detection model previously, and each edge represents the relation between two component candidates. Since remote regions in a document may also bear close dependencies (e.g., a paragraph spans two columns), all regions constitute a neighbor relationship. Thus, the document graph is a fully-connected graph and E \u2286 O \u00d7 O. The key idea of our relation module is to update the hidden representations of each node by attending over its neighbors (z 1 , z 2 , \u00b7 \u00b7 \u00b7 , z 8 \u2192z 1 , as shown in Fig 3). With updated node features, we could predict its refined label and position coordinates.\n\nInitially, each node, denoted by o j = (b j , f j ), includes two pieces of information: position coordinates b j and deep features f j = RoIAlign(F M, b j ). In order to incorporate both of them into node representation, we construct new node feature z j as follows,\nz j = LayerN orm(f j + e pos j (b j ))(6)\nwhere e pos j (b j ) is the position embedding vectors of j-th node. Then, instead of explicitly specifying the relations between nodes, inspired by [33], we apply self-attention mechanism to automatically learn the relations, which has already shown great success in NLP and document processing [38,40,24,42]. Specifically, we adopt the popular scaled dot-product attention [32] to obtain sufficient expressive power. Scaled dot-product attention consists of queries Q and keys K of dimension d k , and values V of dimension d v . The output O is obtained by weighted sum over all values in V , where the attention weights are obtained using Q and K, as shown in Eq. (7). Please refer to [32] for details.\nO = Attention(Q, K, V ) = sof tmax( QK T \u221a d k )V(7)\nIn our context, node feature set Z = {z 1 , \u00b7 \u00b7 \u00b7 , z N } serves as K, Q and V and updated node feature set Z = {z 1 , \u00b7 \u00b7 \u00b7 , z N } is the output O. We apply multihead attention to further improve representation capacity of node features.\n\nFinally, given updated node features Z , refined detection results of j-th node (j-th layout component candidate) o j = p c j , b j is computed as,\np c j = Sof tmax(Linear cls z j ) (8) b j = Linear reg z j(9)\nwhere p c j is the probability of belonging to c-th class, b j is its refined regression coordinates. Linear cls and Linear reg are projection layers.\n\nRelation module can be easily applied to NLP-based methods. In this case, node feature z j in Eq. (6) is the representation of j-th low-level elements (e.g., tokens). Then, GNN models pairwise relations between tokens and predicts their semantic labels ( p c j ).\n\n\nOptimization\n\nSince multimodal layout analysis can be modeled as sequence labeling or object detection tasks, their optimization losses are different. Layout analysis as sequence labeling. The loss function is formulated as,\nL = \u2212 1 T T j=1 log p j (y j )(10)\nwhere, T is the number of low-level elements and y j is the groundtruth semantic label of j-th element. Layout analysis as object detection. The loss function is generated from two parts,\nL = L DET + \u03bbL RM(11)\nwhere L DET and L RM are the losses used in candidate generation process and relation module. Both L DET and L RM consist of a cross entropy loss (classification) and a smooth L 1 loss (coordinate regression), as defined in [26]. Hyperparameters \u03bb controls the trade-off between two losses.\n\n\nExperiments\n\n\nDatasets\n\nAll three benchmarks provide document images and their original PDFs. Therefore, text could be directly obtained by parsing PDFs, allowing the explorations of multi-modal techniques. To compare with existing solutions on each benchmark, we use the same evaluation metrics as used by each benchmark.\n\nArticle Regions [31] consists of 822 document samples and 9 region classes are annotated (Title, Authors, Abstract, Body, Figure, Figure Caption, Table,  Table Caption and References). The annotation is in object detection format and the evaluation metric is mean average precision (mAP).\n\nPubLayNet [43] is a large-scale document dataset recently released by IBM. It consists of 360K document samples and 5 region classes are annotated (Text, Title, List, Figure, and Table). The annotation is also in object detection format. They use the same evaluation metric as used in the COCO competition, i.e., the mean average precision (AP) @ intersection over union (IOU) [0.50:0.95].\n\nDocBank [20] is proposed by Microsoft. It contains 500K document samples with 12 region classes (Abstract, Author, Caption, Equation, Figure, Footer, List, Paragraph, Reference, Section, Table and Title). It provides token-level annotations, and use F1 score as official evaluation metric. Also, it provides object detection annotations, supporting object detection method.\n\n\nImplementation Details\n\nDocument image is directly used as input for visual stream. For semantic stream, we extract embedding maps (SentGrid and CharGrid ) from text as input, where SentGrid is generated by pretrained BERT model [8] and CharGrid is obtained from a word embedding layer. They all have the same channel dimension size (C S 0 = 64). ResNeXt-101 [37] is used as backbone to extract both visual and semantic features (unless otherwise specified), which are later fused by a multiscale adaptive aggregation and feature pyramid network.\n\nFor CV-based multimodal layout analysis methods, fused features are fed into RPN, followed by RCNN, to generate component candidates. In RPN, 7 anchor ratios (0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0) are adopted to handle document elements that vary in sizes and scales. In relation module, dimension of each candidate is set to 1024 and 2 layers of multi-head attention with 16 heads are used to model relations. We set \u03bb in Eq.(11) to be 1 in all our experiments. For NLP-based multimodal layout analysis methods, low-level elements parsed from PDFs (e.g., tokens) serve as component candidates, and relation module predicts their semantic labels.\n\nOur model is implemented under the PyTorch framework. It is trained by the SGD optimizer with batchsize=2, momentum=0.9 and weight-decay=10 \u22124 . The initial learning rate is set to 10 \u22123 , which is divided by 10 every 10 epochs on Article Regions dataset and 3 epochs on the other two benchmarks. The training of model on Article Regions lasts for 30 epochs while on the other two benchmarks lasts for 6. All the experiments are carried out on Tesla-V100 GPUs. Source code will be released in the near future.\n\n\nResults\n\nArticle Regions. We compare the performance of VSR on this dataset with two models: Faster RCNN and Faster RCNN with context [31]. Faster RCNN Title Author Abstract Body Figure  Figure Caption Table  Table  Caption Reference mAP  with context adds limited context (page numbers, region-of-interest position and size) as input in addition to document images.\n\nIn Table 1, we first show mAP as reported in their original papers [31]. For fair comparison, we reimplement those two models using the same backbone (ResNet-101) and neck configuration as used in VSR. We also report their performance after reimplementation. We can see that our reimplemented models have much higher mAP than their original models. We believe this is mainly because we use multiple anchor ratios in RPN, thus achieve better detection results on document elements with various sizes. VSR makes full use of vision, semantics and relations between components, showing highest mAP on most classes. On Figure and Table categories, VSR achieves comparable results and the slight performance drop will be further discussed in Sec 4.4.\n\nPubLayNet. In Table 2, we compare the performance of VSR on this dataset with two pure image-based methods, Faster RCNN [26] and Mask RCNN [13]. While those two models present promising results (AP>90%) on validation dataset, VSR improves the performance on all classes and increases the final AP by 4.7%. VSR shows large performance improvements on text-related classes (Text, Title and List) since it also utilizes document semantics in addition to document image. On test dataset (also known as leaderboard of ICDAR2021 layout analysis recognition competition 4 ), VSR surpasses all participating teams and ranks first, with 4.99% increase on AP compared with Mask RCNN baseline.  Table 4. Performance comparisons on DocBank dataset in mAP.\n\nModels Abstract Author Caption Equation Figure Footer List Paragraph Reference Section Table Title  DocBank. This dataset offers both token and detection annotations. Therefore, we could treat layout analysis task either as sequence labeling task or as object detection task, then compare VSR with existing solutions in both cases. Layout analysis as sequence labeling . Using token-level annotations, we compare VSR with BERT [8], RoBERTa [25], LayoutLM [38], Faster RCNN with ResNeXt-101 [37] and ensemble models (ResNeXt-101+LayoutLM) in Table 3. Even though highest F1 score of Caption and Figure are achieved by ensemble model (ResNeXt-101+LayoutLM) and LayoutLM respectively, VSR achieves comparable results with small gaps (\u2264 0.37%). More importantly, VSR gets the highest scores on all other classes. This indicates that VSR is significantly better than BERT, RoBERTa and LayoutLM architectures on document layout analysis task.\n\nLayout analysis as object detection. Since both VSR and Faster RCNN with ResNeXt-101 can provide object detection results, we further compare them in object detection format using mAP as evaluation metric. Results in Table 4 show that VSR outperforms Faster RCNN on most classes, except Equation, Footer and Title. Overall, VSR shows 1.3% gains in final mAP.\n\n\nAblation Studies\n\nVSR introduces multi-granularity semantics, two-stream network with adaptive aggregation, and relation module. Now we explore how each of them contributes to VSR's performance improvement on Article Regions dataset.\n\nEffects of multi-granularity semantic features. To understand whether multi-granularity semantic features indeed improve VSR's performance, we compare 4 versions of VSR (vision-only, vision+character, vision+sentence, vision+ character+sentence) in Table 5. Here character and sentence refer to semantic features at two different granularities. We can see that, introducing document Table 5. Effects of semantic features at different granularities.\n\n\nVision\n\nSemantics Title Author Abstract Body Figure  Figure Caption Table  Table  Caption Reference  Table 6. Effects of two-stream network with adaptive aggregation.\n\n\nMethod\n\nTitle Author Abstract Body Figure  Figure Caption Table  Table  Caption Reference mAP FPS semantics at each granularity alone can boost analysis performance while combining both of them leads to highest mAP. This is consistent with how humans comprehend documents. Humans can better recognize regions which require little context from characters/words (e.g.,Author) and those which need context from sentences (e.g., Table caption).\n\nEffects of two-stream network with adaptive aggregation. We propose a two-stream network with adaptive aggregation module to combine vision and semantics of document. To verify its effectiveness, we compare our VSR with its multimodal single-stream counterparts in Table 6. Instead of using extra stream to extract semantic features, single-stream networks directly use text embedding maps and concatenate them with visual features at input-level [3] or decision-level [39]. [3] performs concatenation fusion in the input level and shows worse performances, while [39] fuses multimodal features in the decision level and achieves impressive performances (89.8 mAP). VSR first extracts visual and semantic features separately using two-stream network, and then fuses them adaptively. This leads to highest mAP (92.3). At the same time, VSR can run at real-time (13.94 frames per second). We also experiment on larger backbone (ResNet-152) and reach consistent conclusions as shown in Table 6.\n\nEffects of relation module. To verify the effectiveness of relation module (RM ), we compare two versions of Faster RCNN and VSR in Table 7, i.e., with RM and without RM. Since both labels and position coordinates can be refined in RM, both unimodal Faster RCNN and VSR show consistent improvements after incorporating relation module, with 5.3% and 2.2% increase respectively. Visual examples are given in Fig 4. However, for Figure component, performance may slightly drop after introducting RM. The reason is that, while removing duplicate predictions, our relation module may also risk removing correct pre- Table 7. Effects of relation module.\n\n\nMethod\n\nTitle Author Abstract Body Figure Figure caption Table Table caption   dictions. But still, we see improvements on overall performances, showing the benefits of introducting relations.\n\nLimitations. As mentioned above, in addition to document images, VSR also requires the positions and contents of texts in the document. Therefor, the generalization of VSR may be not good enough compared with its unimodal counterparts, which we'll address in the future.\n\n\nConclusion\n\nIn this paper, we present a unified framework VSR for multimodal layout analysis combining vision, semantics and relations. We first introduce semantics of document at character and sentence granularities. Then, a two-stream convolutional network is used to extract modality-specific visual and semantic features, which are further fused in the adaptive aggregation module. Finally, given component candidates, a relation module is adopted to model relations between them and output final results. On three benchmarks, VSR outperforms its unimodal and multimodal single-stream counterparts significantly. In the future, we will investigate pre-training models with VSR and extend it to other tasks, such as information extraction.\n\nFig. 1 .\n1Comparison of multimodal document layout analysis frameworks. VSR supports both NLP-based and CV-based frameworks. \u00a9 and A \u25cb denote concatenation and adaptive aggregation fusion strategies. Different colored regions in the prediction results indicate different semantic labels (Paragraph, Figure, Figure Caption,Table Caption).\n\nFig. 2 .\n2The architecture overview of VSR. Best viewed in color.\n\nFig. 3 .\n3Illustration of relation module. It captures relations between component candidates, and thus improves detection results (remove false Figure prediction, correct Table Caption label and adjust Paragraph coordinates). The colors of semantic labels are: Figure, Paragraph, Figure Caption,\n\nFig. 4 .\n4Qualitative comparison between VSR w/wo RM. Introducing RM effectively removes duplicate predictions and provides more accurate detection results (both labels and coordinates). The colors of semantic labels are: Figure, Body, Figure Caption.\n\nTable , Table\n,Caption. Best viewed in color.\n\nTable 1 .\n1Performance comparisons on Article Regions datasetMethod \n\n\nTable 2 .\n2Performance comparisons on PubLayNet dataset.Method \nDataset Text Title List Table Figure AP \nFaster RCNN [43] \nval \n\n91 \n82.6 88.3 95.4 93.7 90.2 \nMask RCNN [43] \n91.6 \n84 \n88.6 \n96 \n94.9 \n91 \nVSR \n96.7 93.1 94.7 97.4 96.4 95.7 \nFaster RCNN [43] \n\ntest \n\n91.3 81.2 88.5 94.3 94.5 \n90 \nMask RCNN [43] \n91.7 82.8 88.7 94.7 95.5 90.7 \nDocInsightAI \n94.51 88.31 94.84 95.77 97.52 94.19 \nSCUT \n94.3 89.72 94.25 96.62 97.68 94.51 \nSRK \n94.65 89.98 95.14 97.16 97.95 94.98 \nSiliconMinds \n96.2 89.75 94.6 96.98 97.6 95.03 \nVSR \n96.69 92.27 94.55 97.03 97.90 95.69 \n\n\n\nTable 3 .\n3Performance comparisons on DocBank dataset in F1 Score.Method \nAbstract Author Caption Equation Figure Footer List Paragraph Reference Section Table Title \nMacro \nAverage \nBERTbase \n92.94 \n84.84 86.29 \n81.52 100.0 78.05 71.33 \n96.19 \n93.10 \n90.81 82.96 94.42 87.70 \nRoBERTabase \n92.88 \n86.18 89.44 \n82.48 100.0 80.14 73.53 \n96.46 \n93.41 \n93.37 83.89 95.11 88.91 \nLayoutLMbase \n98.16 \n85.95 95.97 \n89.47 100.0 89.57 89.48 \n97.88 \n93.38 \n95.98 86.33 95.79 93.16 \nBERTlarge \n92.86 \n85.77 86.50 \n81.77 100.0 78.14 69.60 \n96.19 \n92.84 \n90.65 83.20 94.30 87.65 \nRoBERTalarge \n94.79 \n87.24 90.81 \n83.70 100.0 83.92 74.51 \n96.65 \n93.34 \n94.07 84.94 94.61 89.88 \nLayoutLMlarge \n97.84 \n87.83 95.56 \n89.74 100.0 91.46 90.04 \n97.90 \n93.32 \n95.96 86.79 95.52 93.50 \nX101 \n97.17 \n82.27 94.35 \n89.38 88.12 90.29 90.51 \n96.82 \n87.98 \n94.12 83.53 91.58 90.51 \nX101+LayoutLMbase 98.15 \n89.07 96.69 \n94.30 99.90 92.92 93.00 \n98.43 \n94.37 \n96.64 88.18 95.75 94.78 \nX101+LayoutLMlarge 98.02 \n89.64 96.66 \n94.40 99.94 93.52 92.93 \n98.44 \n94.30 \n96.70 88.75 95.31 94.88 \nVSR \n98.29 91.19 96.32 \n95.84 99.96 95.11 94.66 98.66 \n95.05 97.11 89.24 95.63 95.59 \n\n\nIn the rest of this paper, we assume text is available. There are tools available to extract text from PDF documents (e.g., PDFMiner[28]) and document images (e.g., OCR engine[30]).\ndhSegment T means dhSegment with inputs of image and text embedding maps.\nSentence is a group of words or phrases, which usually ends with a period, question mark or exclamation point. For simplicity, we approximate it with text lines.\nhttps://icdar2021.org/competitions/competition-on-scientific-literature-parsing/\n\nMulti-modal association based grouping for form structure extraction. M Aggarwal, M Sarkar, H Gupta, B Krishnamurthy, WACVAggarwal, M., Sarkar, M., Gupta, H., Krishnamurthy, B.: Multi-modal association based grouping for form structure extraction. In: WACV. pp. 2064-2073 (2020)\n\nMultimodal machine learning: A survey and taxonomy. T Baltrusaitis, C Ahuja, L Morency, IEEE Trans. Pattern Anal. Mach. Intell. 412Baltrusaitis, T., Ahuja, C., Morency, L.: Multimodal machine learning: A survey and taxonomy. IEEE Trans. Pattern Anal. Mach. Intell. 41(2), 423-443 (2019)\n\nCombining visual and textual features for semantic segmentation of historical newspapers. R Barman, M Ehrmann, S Clematide, S A Oliveira, F Kaplan, CoRR abs/2002.06144Barman, R., Ehrmann, M., Clematide, S., Oliveira, S.A., Kaplan, F.: Combining visual and textual features for semantic segmentation of historical newspapers. CoRR abs/2002.06144 (2020)\n\nDocument layout analysis: A comprehensive survey. G M Binmakhashen, S A Mahmoud, ACM Comput. Surv. 52636BinMakhashen, G.M., Mahmoud, S.A.: Document layout analysis: A comprehen- sive survey. ACM Comput. Surv. 52(6), 109:1-109:36 (2020)\n\nPage segmentation of historical document images with convolutional autoencoders. K Chen, M Seuret, M Liwicki, J Hennebert, R Ingold, Chen, K., Seuret, M., Liwicki, M., Hennebert, J., Ingold, R.: Page segmentation of historical document images with convolutional autoencoders. In: ICDAR. pp. 1011-1015 (2015)\n\nPage grammars and page parsing. A syntactic approach to document layout recognition. A Conway, ICDAR. pp. Conway, A.: Page grammars and page parsing. A syntactic approach to document layout recognition. In: ICDAR. pp. 761-764 (1993)\n\nHistorical document digitization through layout analysis and deep content classification. A Corbelli, L Baraldi, C Grana, R Cucchiara, ICPR. Corbelli, A., Baraldi, L., Grana, C., Cucchiara, R.: Historical document digitization through layout analysis and deep content classification. In: ICPR. pp. 4077-4082 (2016)\n\nBERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, NAACL-HLT. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirec- tional transformers for language understanding. In: NAACL-HLT. pp. 4171-4186 (2019)\n\nConvolutional two-stream network fusion for video action recognition. C Feichtenhofer, A Pinz, A Zisserman, CVPR. Feichtenhofer, C., Pinz, A., Zisserman, A.: Convolutional two-stream network fu- sion for video action recognition. In: CVPR. pp. 1933-1941 (2016)\n\nSegmentation of historical handwritten documents into text zones and text lines. B Gatos, G Louloudis, N Stamatopoulos, ICFHR. pp. Gatos, B., Louloudis, G., Stamatopoulos, N.: Segmentation of historical handwrit- ten documents into text zones and text lines. In: ICFHR. pp. 464-469 (2014)\n\nCnns-based RGB-D saliency detection via cross-view transfer and multiview fusion. J Han, H Chen, N Liu, C Yan, X Li, IEEE Trans. Cybern. 4811Han, J., Chen, H., Liu, N., Yan, C., Li, X.: Cnns-based RGB-D saliency detection via cross-view transfer and multiview fusion. IEEE Trans. Cybern. 48(11), 3171- 3183 (2018)\n\nMulti-scale multi-task FCN for semantic page segmentation and table detection. D He, S Cohen, B L Price, D Kifer, C L Giles, He, D., Cohen, S., Price, B.L., Kifer, D., Giles, C.L.: Multi-scale multi-task FCN for semantic page segmentation and table detection. In: ICDAR. pp. 254-261 (2017)\n\nK He, G Gkioxari, P Doll\u00e1r, R B Girshick, Mask R-CNN. In: ICCV. He, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R.B.: Mask R-CNN. In: ICCV. pp. 2980-2988 (2017)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. pp. 770-778 (2016)\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Comput. 98Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8), 1735-1780 (1997)\n\nRelation networks for object detection. H Hu, J Gu, Z Zhang, J Dai, Y Wei, CVPR. Hu, H., Gu, J., Zhang, Z., Dai, J., Wei, Y.: Relation networks for object detection. In: CVPR. pp. 3588-3597 (2018)\n\nSyntactic segmentation and labeling of digitized pages from technical journals. M S Krishnamoorthy, G Nagy, S C Seth, M Viswanathan, IEEE Trans. Pattern Anal. Mach. Intell. 157Krishnamoorthy, M.S., Nagy, G., Seth, S.C., Viswanathan, M.: Syntactic segmen- tation and labeling of digitized pages from technical journals. IEEE Trans. Pattern Anal. Mach. Intell. 15(7), 737-747 (1993)\n\nPage segmentation using a convolutional neural network with trainable co-occurrence features. J Lee, H Hayashi, W Ohyama, S Uchida, Lee, J., Hayashi, H., Ohyama, W., Uchida, S.: Page segmentation using a con- volutional neural network with trainable co-occurrence features. In: ICDAR. pp. 1023-1028 (2019)\n\nCross-domain document object detection: Benchmark suite and method. K Li, C Wigington, C Tensmeyer, H Zhao, N Barmpalios, V I Morariu, V Manjunatha, T Sun, Y Fu, CVPRLi, K., Wigington, C., Tensmeyer, C., Zhao, H., Barmpalios, N., Morariu, V.I., Manjunatha, V., Sun, T., Fu, Y.: Cross-domain document object detection: Bench- mark suite and method. In: CVPR. pp. 12912-12921 (2020)\n\nDocbank: A benchmark dataset for document layout analysis. M Li, Y Xu, L Cui, S Huang, F Wei, Z Li, M Zhou, COLING. Li, M., Xu, Y., Cui, L., Huang, S., Wei, F., Li, Z., Zhou, M.: Docbank: A benchmark dataset for document layout analysis. In: COLING. pp. 949-960 (2020)\n\nInstance aware document image segmentation using label pyramid networks and deep watershed transformation. X Li, F Yin, T Xue, L Liu, J Ogier, C Liu, Li, X., Yin, F., Xue, T., Liu, L., Ogier, J., Liu, C.: Instance aware document image segmentation using label pyramid networks and deep watershed transformation. In: ICDAR. pp. 514-519 (2019)\n\nFeature pyramid networks for object detection. T Lin, P Doll\u00e1r, R B Girshick, K He, B Hariharan, S J Belongie, CVPR. Lin, T., Doll\u00e1r, P., Girshick, R.B., He, K., Hariharan, B., Belongie, S.J.: Feature pyramid networks for object detection. In: CVPR. pp. 936-944 (2017)\n\nBilinear CNN models for fine-grained visual recognition. T Lin, A Roychowdhury, S Maji, ICCV. Lin, T., RoyChowdhury, A., Maji, S.: Bilinear CNN models for fine-grained visual recognition. In: ICCV. pp. 1449-1457 (2015)\n\nGraph convolution for multimodal information extraction from visually rich documents. X Liu, F Gao, Q Zhang, H Zhao, NAACL-HLT. Liu, X., Gao, F., Zhang, Q., Zhao, H.: Graph convolution for multimodal informa- tion extraction from visually rich documents. In: NAACL-HLT. pp. 32-39 (2019)\n\nRoberta: A robustly optimized BERT pretraining approach. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, CoRR abs/1907.11692Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized BERT pretraining approach. CoRR abs/1907.11692 (2019)\n\nFaster R-CNN: towards real-time object detection with region proposal networks. S Ren, K He, R B Girshick, J Sun, NeurIPS. pp. Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object detection with region proposal networks. In: NeurIPS. pp. 91-99 (2015)\n\nLearning non-generative grammatical models for document analysis. M Shilman, P Liang, P A Viola, ICCV. Shilman, M., Liang, P., Viola, P.A.: Learning non-generative grammatical models for document analysis. In: ICCV. pp. 962-969 (2005)\n\nPdfminer: Python pdf parser and analyzer. Y Shinyama, 11Shinyama, Y.: Pdfminer: Python pdf parser and analyzer. Retrieved on 11 (2015)\n\nExtracting scientific figures with distantly supervised neural networks. N Siegel, N Lourie, R Power, W Ammar, JCDL. pp. Siegel, N., Lourie, N., Power, R., Ammar, W.: Extracting scientific figures with distantly supervised neural networks. In: JCDL. pp. 223-232 (2018)\n\nAn overview of the tesseract OCR engine. R Smith, ICDAR. pp. Smith, R.: An overview of the tesseract OCR engine. In: ICDAR. pp. 629-633 (2007)\n\nVisual detection with context for document layout analysis. C Soto, S Yoo, EMNLP-IJCNLP. Soto, C., Yoo, S.: Visual detection with context for document layout analysis. In: EMNLP-IJCNLP. pp. 3462-3468 (2019)\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, NeurIPS. pp. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS. pp. 5998-6008 (2017)\n\nGraph attention networks. P Velickovic, G Cucurull, A Casanova, A Romero, P Li\u00f2, Y Bengio, ICLRVelickovic, P., Cucurull, G., Casanova, A., Romero, A., Li\u00f2, P., Bengio, Y.: Graph attention networks. In: ICLR (2018)\n\nDense prediction for text line segmentation in handwritten document images. Q N Vo, G Lee, ICIP. Vo, Q.N., Lee, G.: Dense prediction for text line segmentation in handwritten document images. In: ICIP. pp. 3264-3268 (2016)\n\nX Wang, R B Girshick, A Gupta, K He, Non-local neural networks. In: CVPR. Wang, X., Girshick, R.B., Gupta, A., He, K.: Non-local neural networks. In: CVPR. pp. 7794-7803 (2018)\n\nFully convolutional neural networks for page segmentation of historical document images. C Wick, F Puppe, DASWick, C., Puppe, F.: Fully convolutional neural networks for page segmentation of historical document images. In: DAS. pp. 287-292 (2018)\n\nAggregated residual transformations for deep neural networks. S Xie, R B Girshick, P Doll\u00e1r, K He, CVPR. Xie, S., Girshick, R.B., Doll\u00e1r P., He, K.: Aggregated residual transformations for deep neural networks. In: CVPR. pp. 5987-5995 (2017)\n\nLayoutlm: Pre-training of text and layout for document image understanding. Y Xu, M Li, L Cui, S Huang, F Wei, M Zhou, Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: Layoutlm: Pre-training of text and layout for document image understanding. In: KDD. pp. 1192-1200 (2020)\n\nLearning to extract semantic structure from documents using multimodal fully convolutional neural networks. X Yang, E Yumer, P Asente, M Kraley, D Kifer, C L Giles, CVPR. Yang, X., Yumer, E., Asente, P., Kraley, M., Kifer, D., Giles, C.L.: Learning to extract semantic structure from documents using multimodal fully convolutional neural networks. In: CVPR. pp. 4342-4351 (2017)\n\nPICK: processing key information extraction from documents using improved graph learning-convolutional networks. W Yu, N Lu, X Qi, P Gong, R Xiao, ICPR. Yu, W., Lu, N., Qi, X., Gong, P., Xiao, R.: PICK: processing key information extraction from documents using improved graph learning-convolutional networks. In: ICPR. pp. 4363-4370 (2020)\n\nSegmentation-based historical handwritten word spotting using document-specific local features. K Zagoris, I Pratikakis, B Gatos, ICFHR. pp. Zagoris, K., Pratikakis, I., Gatos, B.: Segmentation-based historical handwritten word spotting using document-specific local features. In: ICFHR. pp. 9-14 (2014)\n\nTRIE: end-to-end text reading and information extraction for document understanding. P Zhang, Y Xu, Z Cheng, S Pu, J Lu, L Qiao, Y Niu, F Wu, MM. Zhang, P., Xu, Y., Cheng, Z., Pu, S., Lu, J., Qiao, L., Niu, Y., Wu, F.: TRIE: end-to-end text reading and information extraction for document understanding. In: MM. pp. 1413-1422 (2020)\n\nPublaynet: Largest dataset ever for document layout analysis. X Zhong, J Tang, A Jimeno-Yepes, Zhong, X., Tang, J., Jimeno-Yepes, A.: Publaynet: Largest dataset ever for docu- ment layout analysis. In: ICDAR. pp. 1015-1022 (2019)\n", "annotations": {"author": "[{\"end\":172,\"start\":99},{\"end\":237,\"start\":173},{\"end\":310,\"start\":238},{\"end\":417,\"start\":311},{\"end\":495,\"start\":418},{\"end\":559,\"start\":496},{\"end\":614,\"start\":560}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":104},{\"end\":179,\"start\":177},{\"end\":248,\"start\":244},{\"end\":325,\"start\":320},{\"end\":429,\"start\":427},{\"end\":502,\"start\":499},{\"end\":566,\"start\":564}]", "author_first_name": "[{\"end\":103,\"start\":99},{\"end\":176,\"start\":173},{\"end\":243,\"start\":238},{\"end\":319,\"start\":311},{\"end\":426,\"start\":418},{\"end\":498,\"start\":496},{\"end\":563,\"start\":560}]", "author_affiliation": "[{\"end\":171,\"start\":137},{\"end\":236,\"start\":202},{\"end\":309,\"start\":275},{\"end\":389,\"start\":355},{\"end\":416,\"start\":391},{\"end\":494,\"start\":460},{\"end\":558,\"start\":524},{\"end\":613,\"start\":588}]", "title": "[{\"end\":96,\"start\":1},{\"end\":710,\"start\":615}]", "venue": null, "abstract": "[{\"end\":2124,\"start\":769}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2290,\"start\":2287},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2308,\"start\":2305},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2709,\"start\":2705},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2711,\"start\":2709},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2714,\"start\":2711},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2717,\"start\":2714},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2720,\"start\":2717},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2723,\"start\":2720},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2726,\"start\":2723},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2729,\"start\":2726},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2759,\"start\":2756},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2762,\"start\":2759},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2765,\"start\":2762},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3112,\"start\":3109},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3115,\"start\":3112},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3118,\"start\":3115},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3120,\"start\":3118},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4059,\"start\":4055},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4082,\"start\":4079},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4212,\"start\":4208},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4897,\"start\":4894},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7694,\"start\":7690},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7697,\"start\":7694},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7833,\"start\":7830},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7836,\"start\":7833},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7919,\"start\":7915},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7936,\"start\":7932},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7939,\"start\":7936},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7951,\"start\":7947},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7975,\"start\":7971},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7978,\"start\":7975},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8024,\"start\":8021},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8027,\"start\":8024},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8030,\"start\":8027},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8589,\"start\":8586},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8645,\"start\":8641},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8989,\"start\":8985},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9131,\"start\":9128},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9668,\"start\":9665},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9799,\"start\":9796},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9931,\"start\":9927},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10072,\"start\":10068},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10543,\"start\":10539},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10563,\"start\":10559},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10622,\"start\":10618},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10649,\"start\":10645},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10802,\"start\":10798},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10846,\"start\":10842},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10849,\"start\":10846},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10852,\"start\":10849},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12860,\"start\":12856},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12862,\"start\":12860},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13133,\"start\":13130},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13136,\"start\":13133},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13347,\"start\":13343},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13674,\"start\":13670},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13676,\"start\":13674},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14094,\"start\":14091},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14119,\"start\":14115},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15326,\"start\":15323},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16894,\"start\":16890},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17134,\"start\":17130},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19153,\"start\":19149},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19300,\"start\":19296},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19303,\"start\":19300},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19306,\"start\":19303},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19309,\"start\":19306},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19379,\"start\":19375},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19671,\"start\":19668},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19693,\"start\":19689},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21327,\"start\":21323},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21736,\"start\":21732},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22020,\"start\":22016},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22409,\"start\":22405},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23005,\"start\":23002},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23136,\"start\":23132},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24618,\"start\":24614},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24919,\"start\":24915},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25718,\"start\":25714},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25737,\"start\":25733},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26769,\"start\":26766},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26783,\"start\":26779},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26798,\"start\":26794},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26833,\"start\":26829},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29385,\"start\":29382},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29408,\"start\":29404},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29413,\"start\":29410},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29503,\"start\":29499},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34720,\"start\":34716},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34763,\"start\":34759}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32127,\"start\":31789},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32194,\"start\":32128},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32492,\"start\":32195},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32745,\"start\":32493},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32792,\"start\":32746},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32863,\"start\":32793},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33435,\"start\":32864},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34583,\"start\":33436}]", "paragraph": "[{\"end\":2515,\"start\":2140},{\"end\":3165,\"start\":2517},{\"end\":3783,\"start\":3167},{\"end\":5641,\"start\":3785},{\"end\":6684,\"start\":5643},{\"end\":6724,\"start\":6686},{\"end\":7426,\"start\":6726},{\"end\":8273,\"start\":7444},{\"end\":9417,\"start\":8275},{\"end\":9543,\"start\":9419},{\"end\":10204,\"start\":9545},{\"end\":10357,\"start\":10206},{\"end\":11233,\"start\":10494},{\"end\":12730,\"start\":11341},{\"end\":13195,\"start\":12754},{\"end\":13434,\"start\":13197},{\"end\":14341,\"start\":13634},{\"end\":14936,\"start\":14343},{\"end\":15491,\"start\":15055},{\"end\":15752,\"start\":15534},{\"end\":16421,\"start\":15814},{\"end\":16977,\"start\":16490},{\"end\":17915,\"start\":16997},{\"end\":18688,\"start\":17917},{\"end\":18957,\"start\":18690},{\"end\":19706,\"start\":19000},{\"end\":19999,\"start\":19760},{\"end\":20148,\"start\":20001},{\"end\":20361,\"start\":20211},{\"end\":20626,\"start\":20363},{\"end\":20853,\"start\":20643},{\"end\":21076,\"start\":20889},{\"end\":21389,\"start\":21099},{\"end\":21714,\"start\":21416},{\"end\":22004,\"start\":21716},{\"end\":22395,\"start\":22006},{\"end\":22770,\"start\":22397},{\"end\":23319,\"start\":22797},{\"end\":23966,\"start\":23321},{\"end\":24477,\"start\":23968},{\"end\":24846,\"start\":24489},{\"end\":25592,\"start\":24848},{\"end\":26337,\"start\":25594},{\"end\":27275,\"start\":26339},{\"end\":27635,\"start\":27277},{\"end\":27871,\"start\":27656},{\"end\":28321,\"start\":27873},{\"end\":28490,\"start\":28332},{\"end\":28933,\"start\":28501},{\"end\":29926,\"start\":28935},{\"end\":30576,\"start\":29928},{\"end\":30771,\"start\":30587},{\"end\":31043,\"start\":30773},{\"end\":31788,\"start\":31058}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10475,\"start\":10358},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11302,\"start\":11234},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13633,\"start\":13435},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15054,\"start\":14937},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15533,\"start\":15492},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15778,\"start\":15753},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16452,\"start\":16422},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16489,\"start\":16452},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18999,\"start\":18958},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19759,\"start\":19707},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20210,\"start\":20149},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20888,\"start\":20854},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21098,\"start\":21077}]", "table_ref": "[{\"end\":2892,\"start\":2886},{\"end\":4811,\"start\":4798},{\"end\":5127,\"start\":5121},{\"end\":17423,\"start\":17400},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21883,\"start\":21862},{\"end\":22191,\"start\":22185},{\"end\":24703,\"start\":24682},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24858,\"start\":24851},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25615,\"start\":25608},{\"end\":26285,\"start\":26278},{\"end\":26437,\"start\":26426},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26887,\"start\":26880},{\"end\":27501,\"start\":27494},{\"end\":28129,\"start\":28122},{\"end\":28263,\"start\":28256},{\"end\":28413,\"start\":28392},{\"end\":28432,\"start\":28425},{\"end\":28572,\"start\":28551},{\"end\":28931,\"start\":28918},{\"end\":29207,\"start\":29200},{\"end\":29925,\"start\":29918},{\"end\":30067,\"start\":30060},{\"end\":30547,\"start\":30540},{\"end\":30655,\"start\":30636}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2138,\"start\":2126},{\"attributes\":{\"n\":\"2\"},\"end\":7442,\"start\":7429},{\"end\":10492,\"start\":10477},{\"attributes\":{\"n\":\"3\"},\"end\":11315,\"start\":11304},{\"attributes\":{\"n\":\"3.1\"},\"end\":11339,\"start\":11318},{\"attributes\":{\"n\":\"3.2\"},\"end\":12752,\"start\":12733},{\"attributes\":{\"n\":\"3.3\"},\"end\":15812,\"start\":15780},{\"attributes\":{\"n\":\"3.4\"},\"end\":16995,\"start\":16980},{\"attributes\":{\"n\":\"3.5\"},\"end\":20641,\"start\":20629},{\"attributes\":{\"n\":\"4\"},\"end\":21403,\"start\":21392},{\"attributes\":{\"n\":\"4.1\"},\"end\":21414,\"start\":21406},{\"attributes\":{\"n\":\"4.2\"},\"end\":22795,\"start\":22773},{\"attributes\":{\"n\":\"4.3\"},\"end\":24487,\"start\":24480},{\"attributes\":{\"n\":\"4.4\"},\"end\":27654,\"start\":27638},{\"end\":28330,\"start\":28324},{\"end\":28499,\"start\":28493},{\"end\":30585,\"start\":30579},{\"attributes\":{\"n\":\"5\"},\"end\":31056,\"start\":31046},{\"end\":31798,\"start\":31790},{\"end\":32137,\"start\":32129},{\"end\":32204,\"start\":32196},{\"end\":32502,\"start\":32494},{\"end\":32760,\"start\":32747},{\"end\":32803,\"start\":32794},{\"end\":32874,\"start\":32865},{\"end\":33446,\"start\":33437}]", "table": "[{\"end\":32863,\"start\":32855},{\"end\":33435,\"start\":32921},{\"end\":34583,\"start\":33503}]", "figure_caption": "[{\"end\":32127,\"start\":31800},{\"end\":32194,\"start\":32139},{\"end\":32492,\"start\":32206},{\"end\":32745,\"start\":32504},{\"end\":32792,\"start\":32762},{\"end\":32855,\"start\":32805},{\"end\":32921,\"start\":32876},{\"end\":33503,\"start\":33448}]", "figure_ref": "[{\"end\":2885,\"start\":2878},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3195,\"start\":3185},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3614,\"start\":3605},{\"end\":3742,\"start\":3728},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3812,\"start\":3802},{\"end\":5116,\"start\":5110},{\"end\":5469,\"start\":5462},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5796,\"start\":5786},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11485,\"start\":11479},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17580,\"start\":17574},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18598,\"start\":18592},{\"end\":21860,\"start\":21838},{\"end\":22180,\"start\":22173},{\"end\":22538,\"start\":22531},{\"end\":24673,\"start\":24659},{\"end\":25472,\"start\":25462},{\"end\":26392,\"start\":26379},{\"end\":28383,\"start\":28369},{\"end\":28542,\"start\":28528},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30341,\"start\":30335},{\"end\":30635,\"start\":30614}]", "bib_author_first_name": "[{\"end\":35155,\"start\":35154},{\"end\":35167,\"start\":35166},{\"end\":35177,\"start\":35176},{\"end\":35186,\"start\":35185},{\"end\":35417,\"start\":35416},{\"end\":35433,\"start\":35432},{\"end\":35442,\"start\":35441},{\"end\":35743,\"start\":35742},{\"end\":35753,\"start\":35752},{\"end\":35764,\"start\":35763},{\"end\":35777,\"start\":35776},{\"end\":35779,\"start\":35778},{\"end\":35791,\"start\":35790},{\"end\":36056,\"start\":36055},{\"end\":36058,\"start\":36057},{\"end\":36074,\"start\":36073},{\"end\":36076,\"start\":36075},{\"end\":36324,\"start\":36323},{\"end\":36332,\"start\":36331},{\"end\":36342,\"start\":36341},{\"end\":36353,\"start\":36352},{\"end\":36366,\"start\":36365},{\"end\":36637,\"start\":36636},{\"end\":36876,\"start\":36875},{\"end\":36888,\"start\":36887},{\"end\":36899,\"start\":36898},{\"end\":36908,\"start\":36907},{\"end\":37184,\"start\":37183},{\"end\":37194,\"start\":37193},{\"end\":37203,\"start\":37202},{\"end\":37210,\"start\":37209},{\"end\":37472,\"start\":37471},{\"end\":37489,\"start\":37488},{\"end\":37497,\"start\":37496},{\"end\":37745,\"start\":37744},{\"end\":37754,\"start\":37753},{\"end\":37767,\"start\":37766},{\"end\":38036,\"start\":38035},{\"end\":38043,\"start\":38042},{\"end\":38051,\"start\":38050},{\"end\":38058,\"start\":38057},{\"end\":38065,\"start\":38064},{\"end\":38348,\"start\":38347},{\"end\":38354,\"start\":38353},{\"end\":38363,\"start\":38362},{\"end\":38365,\"start\":38364},{\"end\":38374,\"start\":38373},{\"end\":38383,\"start\":38382},{\"end\":38385,\"start\":38384},{\"end\":38560,\"start\":38559},{\"end\":38566,\"start\":38565},{\"end\":38578,\"start\":38577},{\"end\":38588,\"start\":38587},{\"end\":38590,\"start\":38589},{\"end\":38764,\"start\":38763},{\"end\":38770,\"start\":38769},{\"end\":38779,\"start\":38778},{\"end\":38786,\"start\":38785},{\"end\":38936,\"start\":38935},{\"end\":38950,\"start\":38949},{\"end\":39118,\"start\":39117},{\"end\":39124,\"start\":39123},{\"end\":39130,\"start\":39129},{\"end\":39139,\"start\":39138},{\"end\":39146,\"start\":39145},{\"end\":39356,\"start\":39355},{\"end\":39358,\"start\":39357},{\"end\":39376,\"start\":39375},{\"end\":39384,\"start\":39383},{\"end\":39386,\"start\":39385},{\"end\":39394,\"start\":39393},{\"end\":39752,\"start\":39751},{\"end\":39759,\"start\":39758},{\"end\":39770,\"start\":39769},{\"end\":39780,\"start\":39779},{\"end\":40033,\"start\":40032},{\"end\":40039,\"start\":40038},{\"end\":40052,\"start\":40051},{\"end\":40065,\"start\":40064},{\"end\":40073,\"start\":40072},{\"end\":40087,\"start\":40086},{\"end\":40089,\"start\":40088},{\"end\":40100,\"start\":40099},{\"end\":40114,\"start\":40113},{\"end\":40121,\"start\":40120},{\"end\":40406,\"start\":40405},{\"end\":40412,\"start\":40411},{\"end\":40418,\"start\":40417},{\"end\":40425,\"start\":40424},{\"end\":40434,\"start\":40433},{\"end\":40441,\"start\":40440},{\"end\":40447,\"start\":40446},{\"end\":40724,\"start\":40723},{\"end\":40730,\"start\":40729},{\"end\":40737,\"start\":40736},{\"end\":40744,\"start\":40743},{\"end\":40751,\"start\":40750},{\"end\":40760,\"start\":40759},{\"end\":41007,\"start\":41006},{\"end\":41014,\"start\":41013},{\"end\":41024,\"start\":41023},{\"end\":41026,\"start\":41025},{\"end\":41038,\"start\":41037},{\"end\":41044,\"start\":41043},{\"end\":41057,\"start\":41056},{\"end\":41059,\"start\":41058},{\"end\":41287,\"start\":41286},{\"end\":41294,\"start\":41293},{\"end\":41310,\"start\":41309},{\"end\":41536,\"start\":41535},{\"end\":41543,\"start\":41542},{\"end\":41550,\"start\":41549},{\"end\":41559,\"start\":41558},{\"end\":41795,\"start\":41794},{\"end\":41802,\"start\":41801},{\"end\":41809,\"start\":41808},{\"end\":41818,\"start\":41817},{\"end\":41824,\"start\":41823},{\"end\":41833,\"start\":41832},{\"end\":41841,\"start\":41840},{\"end\":41849,\"start\":41848},{\"end\":41858,\"start\":41857},{\"end\":41873,\"start\":41872},{\"end\":42179,\"start\":42178},{\"end\":42186,\"start\":42185},{\"end\":42192,\"start\":42191},{\"end\":42194,\"start\":42193},{\"end\":42206,\"start\":42205},{\"end\":42445,\"start\":42444},{\"end\":42456,\"start\":42455},{\"end\":42465,\"start\":42464},{\"end\":42467,\"start\":42466},{\"end\":42657,\"start\":42656},{\"end\":42824,\"start\":42823},{\"end\":42834,\"start\":42833},{\"end\":42844,\"start\":42843},{\"end\":42853,\"start\":42852},{\"end\":43062,\"start\":43061},{\"end\":43225,\"start\":43224},{\"end\":43233,\"start\":43232},{\"end\":43400,\"start\":43399},{\"end\":43411,\"start\":43410},{\"end\":43422,\"start\":43421},{\"end\":43432,\"start\":43431},{\"end\":43445,\"start\":43444},{\"end\":43454,\"start\":43453},{\"end\":43456,\"start\":43455},{\"end\":43465,\"start\":43464},{\"end\":43475,\"start\":43474},{\"end\":43695,\"start\":43694},{\"end\":43709,\"start\":43708},{\"end\":43721,\"start\":43720},{\"end\":43733,\"start\":43732},{\"end\":43743,\"start\":43742},{\"end\":43750,\"start\":43749},{\"end\":43960,\"start\":43959},{\"end\":43962,\"start\":43961},{\"end\":43968,\"start\":43967},{\"end\":44108,\"start\":44107},{\"end\":44116,\"start\":44115},{\"end\":44118,\"start\":44117},{\"end\":44130,\"start\":44129},{\"end\":44139,\"start\":44138},{\"end\":44375,\"start\":44374},{\"end\":44383,\"start\":44382},{\"end\":44596,\"start\":44595},{\"end\":44603,\"start\":44602},{\"end\":44605,\"start\":44604},{\"end\":44617,\"start\":44616},{\"end\":44627,\"start\":44626},{\"end\":44853,\"start\":44852},{\"end\":44859,\"start\":44858},{\"end\":44865,\"start\":44864},{\"end\":44872,\"start\":44871},{\"end\":44881,\"start\":44880},{\"end\":44888,\"start\":44887},{\"end\":45166,\"start\":45165},{\"end\":45174,\"start\":45173},{\"end\":45183,\"start\":45182},{\"end\":45193,\"start\":45192},{\"end\":45203,\"start\":45202},{\"end\":45212,\"start\":45211},{\"end\":45214,\"start\":45213},{\"end\":45551,\"start\":45550},{\"end\":45557,\"start\":45556},{\"end\":45563,\"start\":45562},{\"end\":45569,\"start\":45568},{\"end\":45577,\"start\":45576},{\"end\":45876,\"start\":45875},{\"end\":45887,\"start\":45886},{\"end\":45901,\"start\":45900},{\"end\":46170,\"start\":46169},{\"end\":46179,\"start\":46178},{\"end\":46185,\"start\":46184},{\"end\":46194,\"start\":46193},{\"end\":46200,\"start\":46199},{\"end\":46206,\"start\":46205},{\"end\":46214,\"start\":46213},{\"end\":46221,\"start\":46220},{\"end\":46481,\"start\":46480},{\"end\":46490,\"start\":46489},{\"end\":46498,\"start\":46497}]", "bib_author_last_name": "[{\"end\":35164,\"start\":35156},{\"end\":35174,\"start\":35168},{\"end\":35183,\"start\":35178},{\"end\":35200,\"start\":35187},{\"end\":35430,\"start\":35418},{\"end\":35439,\"start\":35434},{\"end\":35450,\"start\":35443},{\"end\":35750,\"start\":35744},{\"end\":35761,\"start\":35754},{\"end\":35774,\"start\":35765},{\"end\":35788,\"start\":35780},{\"end\":35798,\"start\":35792},{\"end\":36071,\"start\":36059},{\"end\":36084,\"start\":36077},{\"end\":36329,\"start\":36325},{\"end\":36339,\"start\":36333},{\"end\":36350,\"start\":36343},{\"end\":36363,\"start\":36354},{\"end\":36373,\"start\":36367},{\"end\":36644,\"start\":36638},{\"end\":36885,\"start\":36877},{\"end\":36896,\"start\":36889},{\"end\":36905,\"start\":36900},{\"end\":36918,\"start\":36909},{\"end\":37191,\"start\":37185},{\"end\":37200,\"start\":37195},{\"end\":37207,\"start\":37204},{\"end\":37220,\"start\":37211},{\"end\":37486,\"start\":37473},{\"end\":37494,\"start\":37490},{\"end\":37507,\"start\":37498},{\"end\":37751,\"start\":37746},{\"end\":37764,\"start\":37755},{\"end\":37781,\"start\":37768},{\"end\":38040,\"start\":38037},{\"end\":38048,\"start\":38044},{\"end\":38055,\"start\":38052},{\"end\":38062,\"start\":38059},{\"end\":38068,\"start\":38066},{\"end\":38351,\"start\":38349},{\"end\":38360,\"start\":38355},{\"end\":38371,\"start\":38366},{\"end\":38380,\"start\":38375},{\"end\":38391,\"start\":38386},{\"end\":38563,\"start\":38561},{\"end\":38575,\"start\":38567},{\"end\":38585,\"start\":38579},{\"end\":38599,\"start\":38591},{\"end\":38767,\"start\":38765},{\"end\":38776,\"start\":38771},{\"end\":38783,\"start\":38780},{\"end\":38790,\"start\":38787},{\"end\":38947,\"start\":38937},{\"end\":38962,\"start\":38951},{\"end\":39121,\"start\":39119},{\"end\":39127,\"start\":39125},{\"end\":39136,\"start\":39131},{\"end\":39143,\"start\":39140},{\"end\":39150,\"start\":39147},{\"end\":39373,\"start\":39359},{\"end\":39381,\"start\":39377},{\"end\":39391,\"start\":39387},{\"end\":39406,\"start\":39395},{\"end\":39756,\"start\":39753},{\"end\":39767,\"start\":39760},{\"end\":39777,\"start\":39771},{\"end\":39787,\"start\":39781},{\"end\":40036,\"start\":40034},{\"end\":40049,\"start\":40040},{\"end\":40062,\"start\":40053},{\"end\":40070,\"start\":40066},{\"end\":40084,\"start\":40074},{\"end\":40097,\"start\":40090},{\"end\":40111,\"start\":40101},{\"end\":40118,\"start\":40115},{\"end\":40124,\"start\":40122},{\"end\":40409,\"start\":40407},{\"end\":40415,\"start\":40413},{\"end\":40422,\"start\":40419},{\"end\":40431,\"start\":40426},{\"end\":40438,\"start\":40435},{\"end\":40444,\"start\":40442},{\"end\":40452,\"start\":40448},{\"end\":40727,\"start\":40725},{\"end\":40734,\"start\":40731},{\"end\":40741,\"start\":40738},{\"end\":40748,\"start\":40745},{\"end\":40757,\"start\":40752},{\"end\":40764,\"start\":40761},{\"end\":41011,\"start\":41008},{\"end\":41021,\"start\":41015},{\"end\":41035,\"start\":41027},{\"end\":41041,\"start\":41039},{\"end\":41054,\"start\":41045},{\"end\":41068,\"start\":41060},{\"end\":41291,\"start\":41288},{\"end\":41307,\"start\":41295},{\"end\":41315,\"start\":41311},{\"end\":41540,\"start\":41537},{\"end\":41547,\"start\":41544},{\"end\":41556,\"start\":41551},{\"end\":41564,\"start\":41560},{\"end\":41799,\"start\":41796},{\"end\":41806,\"start\":41803},{\"end\":41815,\"start\":41810},{\"end\":41821,\"start\":41819},{\"end\":41830,\"start\":41825},{\"end\":41838,\"start\":41834},{\"end\":41846,\"start\":41842},{\"end\":41855,\"start\":41850},{\"end\":41870,\"start\":41859},{\"end\":41882,\"start\":41874},{\"end\":42183,\"start\":42180},{\"end\":42189,\"start\":42187},{\"end\":42203,\"start\":42195},{\"end\":42210,\"start\":42207},{\"end\":42453,\"start\":42446},{\"end\":42462,\"start\":42457},{\"end\":42473,\"start\":42468},{\"end\":42666,\"start\":42658},{\"end\":42831,\"start\":42825},{\"end\":42841,\"start\":42835},{\"end\":42850,\"start\":42845},{\"end\":42859,\"start\":42854},{\"end\":43068,\"start\":43063},{\"end\":43230,\"start\":43226},{\"end\":43237,\"start\":43234},{\"end\":43408,\"start\":43401},{\"end\":43419,\"start\":43412},{\"end\":43429,\"start\":43423},{\"end\":43442,\"start\":43433},{\"end\":43451,\"start\":43446},{\"end\":43462,\"start\":43457},{\"end\":43472,\"start\":43466},{\"end\":43486,\"start\":43476},{\"end\":43706,\"start\":43696},{\"end\":43718,\"start\":43710},{\"end\":43730,\"start\":43722},{\"end\":43740,\"start\":43734},{\"end\":43747,\"start\":43744},{\"end\":43757,\"start\":43751},{\"end\":43965,\"start\":43963},{\"end\":43972,\"start\":43969},{\"end\":44113,\"start\":44109},{\"end\":44127,\"start\":44119},{\"end\":44136,\"start\":44131},{\"end\":44142,\"start\":44140},{\"end\":44380,\"start\":44376},{\"end\":44389,\"start\":44384},{\"end\":44600,\"start\":44597},{\"end\":44614,\"start\":44606},{\"end\":44624,\"start\":44618},{\"end\":44630,\"start\":44628},{\"end\":44856,\"start\":44854},{\"end\":44862,\"start\":44860},{\"end\":44869,\"start\":44866},{\"end\":44878,\"start\":44873},{\"end\":44885,\"start\":44882},{\"end\":44893,\"start\":44889},{\"end\":45171,\"start\":45167},{\"end\":45180,\"start\":45175},{\"end\":45190,\"start\":45184},{\"end\":45200,\"start\":45194},{\"end\":45209,\"start\":45204},{\"end\":45220,\"start\":45215},{\"end\":45554,\"start\":45552},{\"end\":45560,\"start\":45558},{\"end\":45566,\"start\":45564},{\"end\":45574,\"start\":45570},{\"end\":45582,\"start\":45578},{\"end\":45884,\"start\":45877},{\"end\":45898,\"start\":45888},{\"end\":45907,\"start\":45902},{\"end\":46176,\"start\":46171},{\"end\":46182,\"start\":46180},{\"end\":46191,\"start\":46186},{\"end\":46197,\"start\":46195},{\"end\":46203,\"start\":46201},{\"end\":46211,\"start\":46207},{\"end\":46218,\"start\":46215},{\"end\":46224,\"start\":46222},{\"end\":46487,\"start\":46482},{\"end\":46495,\"start\":46491},{\"end\":46511,\"start\":46499}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35362,\"start\":35084},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10137425},\"end\":35650,\"start\":35364},{\"attributes\":{\"doi\":\"CoRR abs/2002.06144\",\"id\":\"b2\"},\"end\":36003,\"start\":35652},{\"attributes\":{\"id\":\"b3\"},\"end\":36240,\"start\":36005},{\"attributes\":{\"id\":\"b4\"},\"end\":36549,\"start\":36242},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":42019065},\"end\":36783,\"start\":36551},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4989897},\"end\":37099,\"start\":36785},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":37399,\"start\":37101},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12289712},\"end\":37661,\"start\":37401},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16958669},\"end\":37951,\"start\":37663},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":195665698},\"end\":38266,\"start\":37953},{\"attributes\":{\"id\":\"b11\"},\"end\":38557,\"start\":38268},{\"attributes\":{\"id\":\"b12\"},\"end\":38715,\"start\":38559},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206594692},\"end\":38909,\"start\":38717},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1915014},\"end\":39075,\"start\":38911},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":37158713},\"end\":39273,\"start\":39077},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16107554},\"end\":39655,\"start\":39275},{\"attributes\":{\"id\":\"b17\"},\"end\":39962,\"start\":39657},{\"attributes\":{\"id\":\"b18\"},\"end\":40344,\"start\":39964},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":219176849},\"end\":40614,\"start\":40346},{\"attributes\":{\"id\":\"b20\"},\"end\":40957,\"start\":40616},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10716717},\"end\":41227,\"start\":40959},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1331231},\"end\":41447,\"start\":41229},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":85528598},\"end\":41735,\"start\":41449},{\"attributes\":{\"doi\":\"CoRR abs/1907.11692\",\"id\":\"b24\"},\"end\":42096,\"start\":41737},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10328909},\"end\":42376,\"start\":42098},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10072745},\"end\":42612,\"start\":42378},{\"attributes\":{\"id\":\"b27\"},\"end\":42748,\"start\":42614},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4698432},\"end\":43018,\"start\":42750},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7038773},\"end\":43162,\"start\":43020},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202782172},\"end\":43370,\"start\":43164},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13756489},\"end\":43666,\"start\":43372},{\"attributes\":{\"id\":\"b32\"},\"end\":43881,\"start\":43668},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1610874},\"end\":44105,\"start\":43883},{\"attributes\":{\"id\":\"b34\"},\"end\":44283,\"start\":44107},{\"attributes\":{\"id\":\"b35\"},\"end\":44531,\"start\":44285},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":8485068},\"end\":44774,\"start\":44533},{\"attributes\":{\"id\":\"b37\"},\"end\":45055,\"start\":44776},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2272015},\"end\":45435,\"start\":45057},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":215786577},\"end\":45777,\"start\":45437},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7706265},\"end\":46082,\"start\":45779},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":218900797},\"end\":46416,\"start\":46084},{\"attributes\":{\"id\":\"b42\"},\"end\":46647,\"start\":46418}]", "bib_title": "[{\"end\":35414,\"start\":35364},{\"end\":36053,\"start\":36005},{\"end\":36634,\"start\":36551},{\"end\":36873,\"start\":36785},{\"end\":37181,\"start\":37101},{\"end\":37469,\"start\":37401},{\"end\":37742,\"start\":37663},{\"end\":38033,\"start\":37953},{\"end\":38761,\"start\":38717},{\"end\":38933,\"start\":38911},{\"end\":39115,\"start\":39077},{\"end\":39353,\"start\":39275},{\"end\":40403,\"start\":40346},{\"end\":41004,\"start\":40959},{\"end\":41284,\"start\":41229},{\"end\":41533,\"start\":41449},{\"end\":42176,\"start\":42098},{\"end\":42442,\"start\":42378},{\"end\":42821,\"start\":42750},{\"end\":43059,\"start\":43020},{\"end\":43222,\"start\":43164},{\"end\":43397,\"start\":43372},{\"end\":43957,\"start\":43883},{\"end\":44593,\"start\":44533},{\"end\":45163,\"start\":45057},{\"end\":45548,\"start\":45437},{\"end\":45873,\"start\":45779},{\"end\":46167,\"start\":46084}]", "bib_author": "[{\"end\":35166,\"start\":35154},{\"end\":35176,\"start\":35166},{\"end\":35185,\"start\":35176},{\"end\":35202,\"start\":35185},{\"end\":35432,\"start\":35416},{\"end\":35441,\"start\":35432},{\"end\":35452,\"start\":35441},{\"end\":35752,\"start\":35742},{\"end\":35763,\"start\":35752},{\"end\":35776,\"start\":35763},{\"end\":35790,\"start\":35776},{\"end\":35800,\"start\":35790},{\"end\":36073,\"start\":36055},{\"end\":36086,\"start\":36073},{\"end\":36331,\"start\":36323},{\"end\":36341,\"start\":36331},{\"end\":36352,\"start\":36341},{\"end\":36365,\"start\":36352},{\"end\":36375,\"start\":36365},{\"end\":36646,\"start\":36636},{\"end\":36887,\"start\":36875},{\"end\":36898,\"start\":36887},{\"end\":36907,\"start\":36898},{\"end\":36920,\"start\":36907},{\"end\":37193,\"start\":37183},{\"end\":37202,\"start\":37193},{\"end\":37209,\"start\":37202},{\"end\":37222,\"start\":37209},{\"end\":37488,\"start\":37471},{\"end\":37496,\"start\":37488},{\"end\":37509,\"start\":37496},{\"end\":37753,\"start\":37744},{\"end\":37766,\"start\":37753},{\"end\":37783,\"start\":37766},{\"end\":38042,\"start\":38035},{\"end\":38050,\"start\":38042},{\"end\":38057,\"start\":38050},{\"end\":38064,\"start\":38057},{\"end\":38070,\"start\":38064},{\"end\":38353,\"start\":38347},{\"end\":38362,\"start\":38353},{\"end\":38373,\"start\":38362},{\"end\":38382,\"start\":38373},{\"end\":38393,\"start\":38382},{\"end\":38565,\"start\":38559},{\"end\":38577,\"start\":38565},{\"end\":38587,\"start\":38577},{\"end\":38601,\"start\":38587},{\"end\":38769,\"start\":38763},{\"end\":38778,\"start\":38769},{\"end\":38785,\"start\":38778},{\"end\":38792,\"start\":38785},{\"end\":38949,\"start\":38935},{\"end\":38964,\"start\":38949},{\"end\":39123,\"start\":39117},{\"end\":39129,\"start\":39123},{\"end\":39138,\"start\":39129},{\"end\":39145,\"start\":39138},{\"end\":39152,\"start\":39145},{\"end\":39375,\"start\":39355},{\"end\":39383,\"start\":39375},{\"end\":39393,\"start\":39383},{\"end\":39408,\"start\":39393},{\"end\":39758,\"start\":39751},{\"end\":39769,\"start\":39758},{\"end\":39779,\"start\":39769},{\"end\":39789,\"start\":39779},{\"end\":40038,\"start\":40032},{\"end\":40051,\"start\":40038},{\"end\":40064,\"start\":40051},{\"end\":40072,\"start\":40064},{\"end\":40086,\"start\":40072},{\"end\":40099,\"start\":40086},{\"end\":40113,\"start\":40099},{\"end\":40120,\"start\":40113},{\"end\":40126,\"start\":40120},{\"end\":40411,\"start\":40405},{\"end\":40417,\"start\":40411},{\"end\":40424,\"start\":40417},{\"end\":40433,\"start\":40424},{\"end\":40440,\"start\":40433},{\"end\":40446,\"start\":40440},{\"end\":40454,\"start\":40446},{\"end\":40729,\"start\":40723},{\"end\":40736,\"start\":40729},{\"end\":40743,\"start\":40736},{\"end\":40750,\"start\":40743},{\"end\":40759,\"start\":40750},{\"end\":40766,\"start\":40759},{\"end\":41013,\"start\":41006},{\"end\":41023,\"start\":41013},{\"end\":41037,\"start\":41023},{\"end\":41043,\"start\":41037},{\"end\":41056,\"start\":41043},{\"end\":41070,\"start\":41056},{\"end\":41293,\"start\":41286},{\"end\":41309,\"start\":41293},{\"end\":41317,\"start\":41309},{\"end\":41542,\"start\":41535},{\"end\":41549,\"start\":41542},{\"end\":41558,\"start\":41549},{\"end\":41566,\"start\":41558},{\"end\":41801,\"start\":41794},{\"end\":41808,\"start\":41801},{\"end\":41817,\"start\":41808},{\"end\":41823,\"start\":41817},{\"end\":41832,\"start\":41823},{\"end\":41840,\"start\":41832},{\"end\":41848,\"start\":41840},{\"end\":41857,\"start\":41848},{\"end\":41872,\"start\":41857},{\"end\":41884,\"start\":41872},{\"end\":42185,\"start\":42178},{\"end\":42191,\"start\":42185},{\"end\":42205,\"start\":42191},{\"end\":42212,\"start\":42205},{\"end\":42455,\"start\":42444},{\"end\":42464,\"start\":42455},{\"end\":42475,\"start\":42464},{\"end\":42668,\"start\":42656},{\"end\":42833,\"start\":42823},{\"end\":42843,\"start\":42833},{\"end\":42852,\"start\":42843},{\"end\":42861,\"start\":42852},{\"end\":43070,\"start\":43061},{\"end\":43232,\"start\":43224},{\"end\":43239,\"start\":43232},{\"end\":43410,\"start\":43399},{\"end\":43421,\"start\":43410},{\"end\":43431,\"start\":43421},{\"end\":43444,\"start\":43431},{\"end\":43453,\"start\":43444},{\"end\":43464,\"start\":43453},{\"end\":43474,\"start\":43464},{\"end\":43488,\"start\":43474},{\"end\":43708,\"start\":43694},{\"end\":43720,\"start\":43708},{\"end\":43732,\"start\":43720},{\"end\":43742,\"start\":43732},{\"end\":43749,\"start\":43742},{\"end\":43759,\"start\":43749},{\"end\":43967,\"start\":43959},{\"end\":43974,\"start\":43967},{\"end\":44115,\"start\":44107},{\"end\":44129,\"start\":44115},{\"end\":44138,\"start\":44129},{\"end\":44144,\"start\":44138},{\"end\":44382,\"start\":44374},{\"end\":44391,\"start\":44382},{\"end\":44602,\"start\":44595},{\"end\":44616,\"start\":44602},{\"end\":44626,\"start\":44616},{\"end\":44632,\"start\":44626},{\"end\":44858,\"start\":44852},{\"end\":44864,\"start\":44858},{\"end\":44871,\"start\":44864},{\"end\":44880,\"start\":44871},{\"end\":44887,\"start\":44880},{\"end\":44895,\"start\":44887},{\"end\":45173,\"start\":45165},{\"end\":45182,\"start\":45173},{\"end\":45192,\"start\":45182},{\"end\":45202,\"start\":45192},{\"end\":45211,\"start\":45202},{\"end\":45222,\"start\":45211},{\"end\":45556,\"start\":45550},{\"end\":45562,\"start\":45556},{\"end\":45568,\"start\":45562},{\"end\":45576,\"start\":45568},{\"end\":45584,\"start\":45576},{\"end\":45886,\"start\":45875},{\"end\":45900,\"start\":45886},{\"end\":45909,\"start\":45900},{\"end\":46178,\"start\":46169},{\"end\":46184,\"start\":46178},{\"end\":46193,\"start\":46184},{\"end\":46199,\"start\":46193},{\"end\":46205,\"start\":46199},{\"end\":46213,\"start\":46205},{\"end\":46220,\"start\":46213},{\"end\":46226,\"start\":46220},{\"end\":46489,\"start\":46480},{\"end\":46497,\"start\":46489},{\"end\":46513,\"start\":46497}]", "bib_venue": "[{\"end\":35152,\"start\":35084},{\"end\":35490,\"start\":35452},{\"end\":35740,\"start\":35652},{\"end\":36102,\"start\":36086},{\"end\":36321,\"start\":36242},{\"end\":36655,\"start\":36646},{\"end\":36924,\"start\":36920},{\"end\":37231,\"start\":37222},{\"end\":37513,\"start\":37509},{\"end\":37792,\"start\":37783},{\"end\":38088,\"start\":38070},{\"end\":38345,\"start\":38268},{\"end\":38621,\"start\":38601},{\"end\":38796,\"start\":38792},{\"end\":38977,\"start\":38964},{\"end\":39156,\"start\":39152},{\"end\":39446,\"start\":39408},{\"end\":39749,\"start\":39657},{\"end\":40030,\"start\":39964},{\"end\":40460,\"start\":40454},{\"end\":40721,\"start\":40616},{\"end\":41074,\"start\":41070},{\"end\":41321,\"start\":41317},{\"end\":41575,\"start\":41566},{\"end\":41792,\"start\":41737},{\"end\":42223,\"start\":42212},{\"end\":42479,\"start\":42475},{\"end\":42654,\"start\":42614},{\"end\":42869,\"start\":42861},{\"end\":43079,\"start\":43070},{\"end\":43251,\"start\":43239},{\"end\":43499,\"start\":43488},{\"end\":43692,\"start\":43668},{\"end\":43978,\"start\":43974},{\"end\":44179,\"start\":44144},{\"end\":44372,\"start\":44285},{\"end\":44636,\"start\":44632},{\"end\":44850,\"start\":44776},{\"end\":45226,\"start\":45222},{\"end\":45588,\"start\":45584},{\"end\":45918,\"start\":45909},{\"end\":46228,\"start\":46226},{\"end\":46478,\"start\":46418}]"}}}, "year": 2023, "month": 12, "day": 17}